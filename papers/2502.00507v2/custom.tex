% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{remark}{Remark}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{A statistically consistent measure of Semantic Variability using Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Yi Liu \\
  Seattle, Washington, USA \\
  %\texttt{liuyi3@microsoft.com} 
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
To address the challenge of variability in the output generated by language models, we introduce a measure of semantic variability that remains statistically consistent under mild assumptions. This measure, termed semantic spectral entropy, is an easily implementable algorithm that requires only standard, pre-trained language models. Our approach imposes minimal restrictions on the choice of language models, and through rigorous simulation studies, we demonstrate that this method can produce an accurate and reliable metric despite the inherent randomness in language model outputs.
\end{abstract}

\section{Introduction}

\label{introduction}
{\color{white}..} The birth of Large Language Models (LLM) has given rise to the possibility of a wide range of industry applications \cite{touvron2023llama,chowdhery2023palm}. One of the key applications of generative models that has garnered significant interest is the development of specialized chatbots with domain-specific expertise such as legal and healthcare \cite{Lexis,mesko2023top}. These applications illustrate how generative models can improve decision-making and improve the efficiency of professional services in specialized fields.

This new LLM capability is made possible by the strong understanding of generative capabilities of the models \cite{liu2023mmc,long2023large} and the advent of Retrieval-Augmented Generation (RAG) \cite{lewis2020retrieval, gao2023retrieval}. In an RAG system, the user interacts by submitting queries, which trigger a search for relevant documents within a pre-established database. These pertinent documents are retrieved based on the query and serve as a context for the LLM to generate an appropriate response. Since the implementation of RAG does not require a custom-trained LLM, it offers a cost-effective solution. The resulting chatbot can perform tasks traditionally handled by domain experts, improving operational efficiency and driving cost reductions.

However, a critical challenge impeding the widespread deployment of generative models in industry is the inherent variability present in these models \cite{amodei2016concrete,hendrycks2021unsolved}.  Although parameters such as temperature, top-k, top-p, and repetition penalty are known to significantly influence model performance \cite{wang2020contextual,wang2023cost, song2024good}, even when these parameters are tuned to achieve deterministic output (e.g. setting temperature to 0 or top-p to 1), differences in the generated results can still occur in multiple runs. This persistent variability poses a significant barrier to the reliable and consistent application of generative models in practical settings.

Atil et al. (2024) conducted a series of experiments involving six deterministically configured large language models (LLMs), with temperature set to 0 and top p set to 1, across eight common tasks and five identical trials per task. The study aimed to assess the repeatability of model outputs by examining whether the generated strings were consistent between runs. The authors found that none of the LLMs demonstrated consistent performance in terms of generating identical outputs on all tasks \cite{atil2024llm}. 
%For complex tasks, such as college-level mathematics, the models often produced lexically different outputs for each run, leading to zero consistency in terms of exact string matching. 
However, the authors noted that when accounting for syntactical variations, the observed differences were relatively minor as many of the generated strings were semantically equivalent. 


The variability in output has been attributed to the use of GPUs in large language model (LLM) inference processes, where premature rounding during computations can lead to discrepancies \cite{nvidia2024,atil2024llm}. Given this, it is reasonable to conclude that complete elimination of variability is unfeasible in any empirical setting. Consequently, we must acknowledge that the output of LLMs is inherently uncertain. In light of this, it becomes essential, similar to practices in statistics, to assess and quantify the level of uncertainty in the text generated by LLMs for any given scenario. 

Most prior studies on uncertainty in foundation models for natural language processing (NLP) have focused primarily on the calibration of classifiers and text regressors \cite{jiang2021can, desai2020calibration, glushkova2021uncertainty}. Other research has addressed uncertainty by prompting models to evaluate their own outputs or fine-tuning generative models to predict their own uncertainty \cite{linteaching, kadavath2022language}. However, these approaches require additional training and supervision, making them difficult to reproduce, costly to implement, and sensitive to distributional shifts. 

 Our work follows from a line of work inline with the concept of semantic entropy proposed in \cite{kuhn2023semantic, nikitin2024kernel,duan-etal-2024-shifting,lin2023generating}. \cite{kuhn2023semantic} explore the entropy of the generated text by assigning semantic equivalence to the pairs of text and subsequently estimating the entropy. Similarly, \cite{nikitin2024kernel} and \cite{lin2023generating} utilize graphical spectral analysis to enhance empirical results. However, a notable limitation in the entropy estimators proposed by \cite{kuhn2023semantic} and \cite{nikitin2024kernel} is their reliance on token likelihoods when assessing semantic equivalence, which may not always be accessible. Furthermore, \cite{kuhn2023semantic} acknowledge that the clustering process employed in their framework is susceptible to the order of comparisons, introducing variability into the results. 

Moreover, previous work focuses on the empirical performance of the estimator. As such, while these methods have demonstrated favorable empirical outcomes, to the best of our knowledge, no authors have established using a theoretical analysis that their entropy estimators converge to a true entropy value as the sample size increases under an underlying generative model. Exploring the theoretical properties allows us to have a clear understanding of how the number of clusters and size of data would affect the estimator. 

Our approach seeks to address these limitations by developing a robust theoretical analysis of the clustering procedure, ensuring convergence properties, and mitigating the variability inherent in prior methodologies. We propose a theoretically analyzable metric for quantifying the variation within a collection of texts, which we refer to as semantic spectral entropy. This measure addresses the observation that many generated strings, while lexically and syntactically distinct, may convey equivalent semantic content. To identify these semantic equivalences, we advocate the use of off-the-shelf generative language models (LMs). Moreover, we acknowledge that the LM used to evaluate semantic similarity is itself a stochastic generator. In response, we employ the well-established technique of spectral clustering, which is provably consistent under minimal assumptions on the generator, thereby ensuring the robustness and reliability of the proposed metric. Specifically, we demonstrate that the measure is statistically consistent under a weak assumption on the LM. To the best of our knowledge, this is the first semantic variability measure with proven convergence properties. As an empirical evaluation studies, we also propose a simple method for constructing clusters of different lexically and syntactically distinct but semantically equivalent text using compound  propositions from \cite{wittgenstein2023tractatus}.

\section{Semantic spectral entropy}
\label{methodology}
\subsection{Semantic entropy}

{\color{white}..} We begin with a collection of textual pieces \( n \), denoted \( \mathcal{T} = (t_1, \cdots, t_n) \). Unlike that in \cite{kuhn2023semantic}, our assumption is that we have access only to $\mathcal{T}$. In fact, we do not require the existence of a generative model and is interested only in variability of the semantics in the text. To evaluate the semantic variability of these texts in the context of a specific use case, we propose a theoretically proven measure of semantic entropy which we named semantic spectral entropy. 

A key reason for opting against the use of variance as a measure of variability is that computing variance requires the definition of a mean, which is challenging to establish for semantic distributions. Although it is possible to define an arbitrary reference point, such as a standard answer in a chatbot that answers questions, evaluating the variability with respect to such a reference introduces bias. 

In contrast, entropy is a well-established measure of variation, particularly for multinomial distributions. For a distribution \( \mathcal{P}(t) \) over a set of semantic clusters \( \{C_1, \cdots, C_k\} \), the entropy \( \mathcal{E} \) is defined as:
\begin{equation}
\label{equ:entropy}
\mathcal{E}(t) = - \sum_{i} p(t \in C_i)\log p(t \in C_i).
\end{equation}
This formulation captures the uncertainty or disorder associated with assigning a given text \( t \) to one of the clusters. Consequently, it provides a quantitative measure of semantic variability that avoids the biases introduced by arbitrary reference points.

To estimate the entropy for a given data set \( t_1, \cdots, t_n \), we first calculate the number of occurrences of each text \( t_i \) in each group \( C_j \). This is achieved by computing:
\[
n_j = \sum_{i=1}^n \mathbb{I}(t_i \in C_j),
\]
where \( \mathbb{I}(t_i \in C_j) \) is an indicator function that equals 1 if \( t_i \) belongs to the cluster \( C_j \), and 0 otherwise. 

Next, the true probability \( p(t \in C_j) \) is approximated using the empirical distribution:
\[
\bar{p}(t \in C_j) = \frac{n_j}{n},
\]
which represents the fraction of texts assigned to cluster \( C_j \). Using this empirical distribution, the empirical entropy is defined as:
\[
\bar{\mathcal{E}}(\mathcal{T}) = - \sum_{j} \bar{p}(t \in C_j) \log \bar{p}(t \in C_j).
\]
This measure provides a practical estimation of semantic entropy based on observed data.


One critical step in this process is clustering the texts $t_i$ into disjoint groups. To do so, it is sufficient to define a relationship between $t_i \sim t_j$, such that they satisfy the properties of equivalence relation. Specifically, one needs to demonstrate 
\begin{enumerate}
    \item Reflexivity: For every $t_i$, we have $t_i \sim t_i$, meaning that any text is equivalent to itself.
    \item Symmetry: If $t_i \sim t_j$, then $t_j \sim t_i$, meaning that equivalence is bidirectional.
    \item Transitivity: If $t_i \sim t_j$ and $t_j \sim t_k$, then $t_i \sim t_k$, which means that equivalence is transitive.
\end{enumerate}

It turns out the existence of an equivalence equation is both a necessary and sufficient condition for a definition of a breakdown of $\mathcal{T}$ into disjoint clusters \cite{liebeck2018concise}. In light of this, defining $\sim$ should be based on the linguist properties of entropy measurement. 

 Direct string comparison, defined as \( t_i \sim t_j \) if and only if \( t_i \) and \( t_j \) share identical characters, reflects lexicon equality and constitutes an equivalence relation. However, this criterion is overly restrictive. In a question-and-response context, a more appropriate equivalence relation might be defined as \( t_i \sim t_j \) if and only if \( t_i \) and \( t_j \) yield identical scores when evaluated by a language model (LM) prompt. This criterion, however, requires an answer statement as a point of reference. We are more interested in a stand-alone metric that can capture the semantic equivalence. For example, consider the sentences \( t_1 = \text{"Water is vital to human survival"} \) and \( t_2 = \text{"Humans must have water to survive"}\). Despite differences in language, both sentences convey the same underlying meaning.

To address such challenges, \cite{kuhn2023semantic,nikitin2024kernel} propose an equivalence relation wherein \( t_i \sim t_j \) if and only if \( t_i \) is true if and only if \( t_j \) is true. This formulation ensures that two texts, \( t_i \) and \( t_j \), belong to the same equivalence class if they are logically equivalent. This broader definition allows for greater flexibility and applicability in assessing semantic equivalence beyond superficial lexical similarity.\cite{copi2016introduction}. We will present their argument as a proposition where we will put the verification in the appendix
\begin{proposition}
    \label{prop:equ}
    The relation $t_i \sim t_j$ if "$t_i$ is true if and only if $t_j$ is true" is an equiva
    
    lence relation.
\end{proposition}

% one considers segmenting the set into equivalence classes based on the following equivalence relation:  


%We first establish that this relation $\sim$ indeed defines well-defined, disjoint subsets of $t_i, i \in \{1,\dots n\}$. 



% Now, we can conclude that $\sim$ is indeed an equivalence relation, and thus, the set of texts $t_1\dots t_n$ can be partitioned into disjoint equivalence classes, where each class represents a distinct semantic group. 
%\begin{remark}
    
%\end{remark}
In light of the fact that equivalence relations can be defined arbitrarily based on the needs of the user. We propose that the determination of equivalence relations, denoted as $\sim$, is performed through a LM that generates responses independently of the specific generation of terms $t_1, \dots, t_n$. However, we do not assume that we have access to probability distribution of the tokens as proposed by \cite{kuhn2023semantic,nikitin2024kernel} which is not always available. Rather, we just require a generator LM which can generate a determination of this relationship. Therefore, this LM can be general generative language model with a crafted prompt which we will use in our simulation studies. The error in this LM will be removed in the spectral clustering algorithm at the later stage. By leveraging this LM, we can define a function $e:{\mathcal{T}, \mathcal{T}}\rightarrow {0,1}$, which is formally expressed as follows: \begin{equation} e(t_i, t_j) = \begin{cases} 1 & \text{if } t_i \sim t_j, \\ 0& \text{otherwise.} \end{cases} \end{equation}

However, since the function relies on an LM, $e(t_i, t_j)$ can be viewed as a Bernoulli random variable, whose value is dependent on the terms $t_i$ and $t_j$.\nocite{kuhn2023semantic} did not address this issue but instead offers adopting a very powerful entailment identification model which the authors trust to identify the equivalence relation perfectly. In contrast, we suggest modeling the outputs of the LM as a random graph with an underlying distribution. In this framework, $t_i$ and $t_j$ represent nodes, while $e(t_i, t_j)$ are random variables that indicate the presence of an edge between the two nodes. Specifically, when $t_i \sim t_j$, the edge existence is governed by the following probability distribution: \begin{equation} \label{eqn:equation_p} e(t_i, t_j) = \begin{cases} 1 & \text{with probability } p, \\ 0 & \text{with probability } 1-p. \end{cases} \end{equation} Conversely, when $t_i \not\sim t_j$, the edge existence follows a different probability distribution: \begin{equation} \label{eqn:equation_q} e(t_i, t_j) = \begin{cases} 1 & \text{with probability } q, \\ 0 & \text{with probability } 1-q. \end{cases} \end{equation}

To mitigate the inherent randomness introduced by the LLM, we propose leveraging spectral clustering to identify clusters of semantically similar texts.
\subsection{Spectral clustering}

{\color{white}..} To compute semantic entropy, it is crucial to identify the clusters of nodes and count the number of nodes within each cluster. Identifying these clusters in a random graph is analogous to detecting clusters in a stochastic block model \cite{holland1983stochastic}. We propose employing the spectral clustering algorithm, with the number of clusters $K$ specified in advance, as an effective approach for this task.

Spectral Clustering is a well-established algorithm for graph clustering, supported by strong theoretical foundations and efficient implementations \cite{shi2000normalized, lei2015consistency, su2019strong, scikit-learn}.  To compute semantic entropy, we aim to cluster a random graph with adjacency matrix $E$ where $E_{ij} = e(t_i, t_j)$, representing the pairwise similarity between text elements $t_i$ and $t_j$. 

We begin by computing the Laplacian matrix $L =  D-E$ where $D$ is the degree matrix.  This is followed by the decomposition of the eigenvalue of $L$. Next, we construct the matrix formed by the first $K$ eigenvectors of $L$ denoted $\hat{U} \in \mathbb{R}^{n\times K}$. This matrix serves as input to an appropriate $(1+\epsilon)-$ k-means clustering algorithm \cite{kumar2004simple,choo2020k}.

The output of this procedure is $K$ distinct clusters $C_1,\cdots C_K$. For each text element $t_i$, we assign a corresponding vector $g_{i}$ where 
$$ g_{ij} = \begin{cases} 1 \text{ if } t_i \in C_j\\
    0 \text{ otherwise }
\end{cases}$$
This binary indicator vector $g_i$ encodes the cluster membership for each text element $t_i$

Finally, we compute the estimated entropy based on the number of texts within each cluster. The entropy $\hat{\mathcal{E}}$ can be approximated using the following formula:
\begin{equation}
\hat{\mathcal{E}}(\mathcal{T}) = - \sum_{j=1}^k\hat{p}( C_j) \log(\hat{p}( C_j)),
\end{equation}
where $\hat{p}(C_j) = \frac{1}{n}\sum_{i=1}^n g_{ij}$.This expression represents the empirical entropy based on the distribution of texts among the $K$ clusters, providing a measure of the uncertainty or diversity within the semantic structure of the data.
\subsection{Full algorithm and implementation}
{\color{white}..} We merge the process of finding sermantic entropy with spectral clustering to present the full algorithm as Algorithm \ref{algo:1}: Sermantic Spectral Entropy. 
\begin{algorithm}
\begin{algorithmic}
    \STATE Begin with $\mathcal{T} = \{t_1, \cdots t_n\}$
    \FOR{$i, j \in \{1,\cdots n\} \times \{1, \cdots n\}, i\neq j$}
    \STATE Use LLM to compute $E_{i,j} = e(t_i, t_j)$. 
    \ENDFOR
    \STATE Find the Laplacian of $E$, $L = D -E$
    \STATE Compute the first $K$ eigenvectors $u_1,\dots,u_k$ of $L$ and the top $K$ eigenvalues $\lambda_1,\cdots \lambda_k$.
    \STATE Let $\hat{U} \in \mathbb{R}^{n\times k}$ be the matrix containing the vectors $u_1,\dots,u_k$ as columns.
    \STATE Use $(1+\epsilon)$ K-means clustering algorithm to cluster the rows of $U$
    \STATE Let $g_{ij}$ be an $(1+\epsilon)-$approximate solution to a $K-$means clustering algorithm
    \STATE Compute $\hat{\mathcal{E}}(\mathcal{T})$ using $ g_{ij}$
\end{algorithmic}
\caption{\label{algo:1} Sermantic Spectral Entropy }
\end{algorithm}

This polynomial-time algorithm is characterized by the largest computational cost associated with the determination of $E_{ij}$. However, computing $E_{ij}$ is embarrassingly parallel, meaning that it can be efficiently distributed across multiple processing units. Furthermore, there are well-established implementation, such as Microsoft Azure's Prompt-Flow \cite{esposito2024programming} and LangChain \cite{mavroudis2024langchain} that facilitate the implementation of parallel workflows, making it feasible to deploy such parallelized tasks with relative ease.
\subsection{Finding K}

{\color{white}..} A notable limitation of this analysis is the unavailability of $K$ in the direct computation of semantic spectral entropy. However, the determination of $K$ for stochastic block model has been well studied \cite{lei2016goodness,wang2017likelihood,chen2018network}. We will describe the cross-validation approach \cite{chen2018network} in detail. The principle behind cross-validation involves predicting the probabilities associated with inter-group connections ($p$) and intra-group connections ($q$). If the estimated value of $K$ is too small, it fails to accurately recover the true underlying probabilities; conversely, if $K$ is too large, it leads to overfitting to noisy data. This approach has the potential to recover the true cluster size under relatively mild conditions.

\section{Theoretical Results}
\label{theory}

{\color{white}..} Our theoretical analysis involves a proof that the estimator is strongly consistent, i.e. the estimator converges to true value almost surely, and an analysis of its rate with respect to the number of cluster $K$. 

We divide our analysis into two subsections. The first subsection examines a fixed set of $\mathcal{T} = {t_1, \dots, t_n}$, which is assumed to exhibit some inherent clusters $C_1, \dots, C_K$. Under the assumption of perfect knowledge of these clusters, the empirical entropy $\bar{\mathcal{E}}$ can be determined. The primary focus in this subsection is on the performance of spectral clustering algorithms. The second subsection explores a scenario in which there exists an underlying generative mechanism that allows for the infinite generation of $t_i$. In this case, we permit $K$ to increase with $n$, though at a significantly slower rate. This scenario is particularly relevant for evaluating the performance of RAG in the context of continuous generation of results in response to a given query.

\subsection{Performance of spectral clustering algorithms}
{\color{white}..}  We model the LM determination of $e(t_i,t_j)$ as a random variable, as described in Equations \ref{eqn:equation_p} and \ref{eqn:equation_q}. In the theoretical analysis presented here, we assume that the number of clusters, $K$, is known and fixed. To derive various results, we first establish the relationship between the difference $|\bar{\mathcal{E}}(\mathcal{T}) - \hat{\mathcal{E}}(\mathcal{T})|$ and the miscluster error, denoted $M_\text{error}$.
\begin{lemma}
 \label{lemma:error}
Suppose that there exists $0<c_2<1$ such that $2Kn_{\min}/n \geq c_2$, 
\begin{equation}
     |\hat{\mathcal{E}}(\mathcal{T}) - \bar{\mathcal{E}}(\mathcal{T})|\leq h\left(\frac{2K}{c_2}\right) \left|\frac{1}{n} (M_\text{error})\right| 
\end{equation}
where $h(x) = \left(x+\log\left(x\right)\right)$.
\end{lemma}
The proof is presented in the Appendix section \ref{Appendix:proofoflemma:error}. 
We begin by presenting the result of strong consistency for the spectral clustering algorithm.  

\begin{theorem}
\label{the:strongConsistensy}
Under regularity conditions, the estimated entropy empirical entropy $\hat{\mathcal{E}}(\mathcal{T})$ is strongly consistent with the empirical entropy, i.e. 
\begin{equation}
    |\bar{\mathcal{E}}(\mathcal{T}) - \hat{\mathcal{E}}(\mathcal{T}) | \rightarrow 0 \text{ almost surely }
\end{equation}
\end{theorem}
The proof is provided in the Appendix section \ref{appendix:sec:the:strongconsistency}. This establishes strong consistency result that we aim to present. At the same time, we also want to show the finite sample properties of the estimator $\hat{\mathcal{E}}(\mathcal{T})$.

\begin{theorem}
    \label{the:finite_sample}
    If there exists $0<c_2\leq1$ and $\lambda > 0$ such that $2Kn_{\min}/n \geq c_2$, and $p = \alpha_n = \alpha_n(q + \lambda) $, where $\alpha_n \geq \log(n)$ then with probability at least $1-\frac{1}{n}$

\begin{equation}
|\bar{\mathcal{E}}(\mathcal{T}) - \hat{\mathcal{E}}(\mathcal{T}) |  \leq h\left(\frac{2K}{c_2}\right) \frac{n_{\max }}{4c_2^2n_{\min }^{2} \alpha_{n}K^2} 
\end{equation}
where $h(x) = \left(x+\log\left(x\right)\right)$, $n_{\max} = \max_j\{n_j : j = 1,\dots K\}$, and $n_{\min} = \min_j\{n_j : j = 1,\dots K\}$.
\end{theorem}
The full proof is provided in the appendix section \ref{appendix:proofofthe:finite_sample}. A brief outline of the proof is as follows: we begin by using the results from \cite{lei2015consistency}, which establish the rate of convergence for the stochastic block model. Next, we relate the errors of the spectral clustering algorithm to the errors in the empirical entropy, using the lemma \ref{lemma:error} to establish this connection.

\begin{remark}
    This result is particularly relevant for computing semantic entropy, as the output generated by LMs is produced with a probability that is independent of $n$. As a result, we have $\alpha_n = O(1)$. Assuming balanced community sizes, the convergence rate is therefore $O(\frac{1}{n})$. This is formally stated in the following corollary:
\end{remark}


\begin{corollary} \label{corollary:rate} If there exists a constant $0 < c_2 \leq 1$ such that $2Kn_{\min}/n \geq c_2$ and $\alpha_n = alpha >0$, then there exists a constant $\alpha$ such that with probability at least $1 - \frac{1}{n}$, \begin{equation} |\bar{\mathcal{E}}(\mathcal{T}) - \hat{\mathcal{E}}(\mathcal{T})| \leq h\left(\frac{2K}{c_2}\right) \frac{1}{c_2^4 \alpha n}. \end{equation} \end{corollary}

The proof of this result is provided in the Appendix section \ref{appendix:proofofcorollary:rate}. 
\begin{remark}
    In particular, we observe that the convergence rate is $O\left(\frac{1}{n}\right)$. This means that the error associated with spectral clustering is small, and our estimated entropy converges to the empirically entropy quickly.
\end{remark}


\subsection{Performance under a generative model}

{\color{white}..} In practical terms, we assume the presence of a generator, specifically an RAG, that produces identically distributed independent random variables $t_i$' that collectively form semantic clusters $C_1 \dots C_K$. In essence, we have $t_i \sim G$ such that $t_i \in C_j$ with probability $p(C_j)$. In this model, there is a true value of entropy $\mathcal{E}(\mathcal{T})$ given in Equation \ref{equ:entropy}, and we want to find the convergence rate of our method. 
\begin{theorem}
    \label{the:final} If there exists a constant $\alpha $ such that $p = \alpha  = \alpha(q + \lambda) $, then with probability at least $1-\frac{3}{n}$,
    \begin{equation}
    \label{eqn:final_the}
       \begin{array}{cc}
         |\mathcal{E} - \hat{\mathcal{E}}|&  \leq h\left(\frac{1}{p_{\min}}\right)K\sqrt{\frac{1}{2n}\log\left(2Kn\right)}\\
         & +h\left(\frac{1}{m(n)p_{\min}}\right)\frac{1}{16K^4m(n)^4p_{\min}^4n}
    \end{array} 
    \end{equation}

where $m(n) = \left(1- \sqrt{2\log(nK)/np_{\min}}\right)$ and $p_{\min} = \min\{p(C_1)\dots p(C_K)\}$.
\end{theorem}
Most of the material used for this proof is presented in Corollary \ref{corollary:rate}. 
\begin{proof}
Consider the following equality
$$|\mathcal{E} - \hat{\mathcal{E}}| \leq |\mathcal{E} -\bar{\mathcal{E}} +\bar{\mathcal{E}}-  \hat{\mathcal{E}}| \leq |\mathcal{E} -\bar{\mathcal{E}}| + | \bar{\mathcal{E}}-  \hat{\mathcal{E}}|,$$  
 We know that there are three sufficient conditions for Equation \ref{eqn:final_the}. These are
\begin{enumerate}
    \item[C1:]$|\mathcal{E} -\bar{\mathcal{E}}| \leq  h\left(\frac{1}{p_{\min}}\right)K\sqrt{\frac{1}{2n}\log\left(2Kn\right)},$
    \item[C2:]$ \exists c_2 \text{ such that } 0 < c_2 \leq 1$ and $2Kn_{\min}/n \geq c_2,$ 
    \item[C3:] $ |\bar{\mathcal{E}}(\mathcal{T}) - \hat{\mathcal{E}}(\mathcal{T})| \leq h\left(\frac{2K}{c_2}\right) \frac{1}{c_2^4 n}.$
\end{enumerate}
Then, using union bound
\begin{align*}
    \mathbb{P}(\text{Not (\ref{eqn:final_the})}) &\leq \mathbb{P}( \text{Not C1 or Not C2 or Not C3})\\
    &\leq \mathbb{P}( \text{Not C1}) + \mathbb{P}( \text{Not C2})+ \mathbb{P}( \text{Not C3}).
\end{align*}
In Lemma \ref{lemma:final1} and \ref{lemma:Final2} of the appendix, we show that $|\mathcal{E} -\bar{\mathcal{E}}| \geq  h\left(\frac{1}{p_{\min}}\right)K\sqrt{\frac{1}{2n}\log\left(2Kn\right)}$ with probability at most $\frac{1}{n}$.

In Lemma \ref{Lemma:Final3} of the Appendix, we show that setting $c_2 = 2K\left(1-\sqrt{\frac{2\log(nK)}{np_{\min}}}\right)p_{\min}$, we have $2Kn_{\min}/n< c_2$ with probability at most $\frac{1}{n}$.

Finally, the corollary \ref{corollary:rate} tells us that $ |\bar{\mathcal{E}}(\mathcal{T}) - \hat{\mathcal{E}}(\mathcal{T})| > h\left(\frac{2K}{c_2}\right) \frac{1}{c_2^4 n}$ occurs with probability at most $\frac{1}{n}$.
\end{proof}
\begin{remark}
One observation is that the empirical entropy converges to true entropy at a rate slower than that of estimated entropy to the empirical entropy. This is natural since each $t_i$ has the opportunity to make a $n-1$ connection with other $t_j$s, resulting in $n(n-1)/2$ independent observations, whereas each generator generates only $n$ independent observations.
\end{remark}
\subsection{Discussion on $K$}
{\color{white}..} An intriguing question to consider is the rate at which \( K \), the number of clusters, can grow with \( n \), the number of texts, as it is natural to expect \( K \) to increase with \( n \). Focusing solely on the spectral clustering algorithm, the error is characterized as \( O((K + \log(K))/n) \). Thus, under the condition \( K = o(n^{1-\delta}) \) for some \( \delta > 0 \), we have \( |\bar{\mathcal{E}}(\mathcal{T}) - \hat{\mathcal{E}}(\mathcal{T})| \to 0 \) in probability. In contrast, when considering a scenario involving a generative model, a stricter condition is required. Specifically, \( K \) must satisfy \( K = o(n^{1/2 - \delta}) \), with \( \delta > 0 \), to ensure \( |\mathcal{E}(\mathcal{T}) - \hat{\mathcal{E}}(\mathcal{T})| \to 0 \) in probability.

\section{Simulation and data studies}
\label{simulation}

{\color{white} .. }As this paper focuses more on the theoretical analysis of semantic spectral entropy with respect to variable $n$ and $K$, we decide against using the evaluation method proposed in \cite{kuhn2023semantic,duan-etal-2024-shifting, lin2023generating} in favor of constructing a simulation where we know the true entropy $\bar{\mathcal{E}}$. This allows us to better analyze how $|\bar{\mathcal{E}} -\hat{\mathcal{E}}|$ changes with choice of generator $e$, $K$ and $n_{\min}$.

To construct a non-trivial simulation for this use case, we evaluate the performance of our algorithms within the context of an unordered set of elementary proposition statements that has no logical interconnections. This approach draws upon the philosophical framework defined by \citep{wittgenstein2023tractatus} in Tractatus Logico-Philosophicus, where each elementary proposition represents a singular atomic fact. Within this framework, texts containing an identical set of elementary propositions are deemed semantically equivalent. The primary advantage of this experimental design lies in its efficiency, as it facilitates the generation of thousands of samples with minimal generator propositions, all while maintaining knowledge of the ground truth.

For example, we can consider a list of things that a hypothetical individual "John" likes to do in his free time: 
\begin{itemize}
    \item Running/Jogging 
    \item Drone Flying/ Pilot Aerial drones
    \item jazzercise / aerobics
    \item ...
\end{itemize}

To generate a cluster of text from this set of hobbies, we begin by randomly selecting \( M \) items from a total of \( N \) items in the list to formulate the compound proportion. This selection process yields \( \binom{N}{M} \) potential subset of hobbies and we know that two subsets of hobbies are the same as long as their elements are the same. Next, to create individual text samples \( t_i \) within the group, we randomly permute the order of the \( M \) selected elements in the subset. This permutation process generates \( M! \) unique samples for each combination of hobbies. Finally, the hobbies are then placed in its permuted order in a sentence like that below. 
\begin{quote}
"In his free time, John likes hobby $1$, hobby $2$, hobby $3$, ..., and hobby $M$ as his hobbies."
\end{quote}
In order to prevent models to rely on sentence structure, a few of these sentences are being designed. 

%We replicate this simulation set-up in different 2 settings. The  setting is the 10 common hobbies that this hypothetical individual likes to do in his free time. %The second set-up is 10 events that happened on the date December 3 in history which we collect from Wikipedia \cite{wiki}. %The last setting  %need to think about how to build these algorithm
%\cite{atil2024llm}

We utilize Microsoft Phi-3.5 \cite{abdin2024phi}, OpenAI GPT3.5-turbo \cite{hurst2024gpt}, A21-Jamba 1.5 Mini \cite{lieber2021jurassic}, Cohere-command-r-08-2024 \cite{Ustun2024AyaMA},Ministral-3B \cite{jiang2023mistral} and the Llama 3.2 70B model \cite{dubey2024llama} as \( e \). These models are lightweight, off-the-shelf language models that are cost-effective to deploy and exhibit efficiency in generating outputs, thereby off-setting the computational cost of determining sermantic relationships. The exact prompt used to generate the verdict is specified in Appendix \ref{appendix_sec:prompt_engineering}.

\begin{table*}[ht]
\centering
\begin{tabular}{l|rrr|rrr|rrr|}
\toprule
ratio & \multicolumn{3}{r|}{0.2,0.3,0.5} & \multicolumn{3}{r|}{0.3,0.3,0.4} & \multicolumn{3}{r|}{0.5,0.5}\\
datasize & 30 & 50 & 70 & 30 & 50 & 70 & 30 & 50 & 70\\
\midrule
LLAMA & 0.36 & 0.49 & 0.44 & 0.34 & 0.43 & 0.46 & 0.30 & 0.27 & 0.26 \\x
MINISTRAL & 0.22 & 0.27 & 0.13 & 0.25 & 0.23 & 0.21 & 0.14 & 0.22 & 0.21 \\
COHERE & 0.04 & 0.02 & 0.06 & 0.02 & 0.03 & 0.00 & 0.00 & 0.00 & 0.00 \\
A21 & 0.05 & 0.00 & 0.00 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 \\
PHI & 0.08 & 0.07 & 0.07 & 0.03 & 0.03 & 0.00 & 0.00 & 0.00 & 0.00 \\
GPT & 0.06 & 0.02 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
\bottomrule
\end{tabular}
\caption{\label{tab:basic_simu} Average $|\bar{\mathcal{E}}- \hat{\mathcal{E}}|$ over simulation 10 iterations. We have three different ratio value run over three different data sizes. For $e$, we use Microsoft Phi-3.5 \cite{abdin2024phi}, OpenAI GPT3.5-turbo \cite{hurst2024gpt}, A21-Jamba 1.5 Mini \cite{lieber2021jurassic}, Cohere-command-r-08-2024 \cite{Ustun2024AyaMA}, Ministral-3B \cite{jiang2023mistral} and the Llama 3.2 70B model \cite{dubey2024llama}. }
\end{table*}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{LambdaExp1.pdf}
    \caption{\label{fig:dotdata} A scatter plot of $p-q$ against $|\bar{\mathcal{E}}- \hat{\mathcal{E}}|$. The different colors represents different language models used as $e$: A21 in blue, Phi in Orange, GPT in Green, Cohere in Red, Llama is Purple and Ministral in Brown. We notice that there is clear phrase change point where for $p-q <0.4$, we have that $|\bar{\mathcal{E}}- \hat{\mathcal{E}}|$ is very high most of the time, for $p-q >0.4$, $|\bar{\mathcal{E}}- \hat{\mathcal{E}}|$ is small with occasional jumps that the theory predicts.}
    
\end{figure}

\begin{table}[]
    \centering
\begin{tabular}{lrrr}
\toprule
 $e$& $p-q$ & $p$ & $q$ \\
\midrule
LLAMA & 0.17 & 0.17 & 0.00 \\
MINISTRAL & 0.22 & 0.99 & 0.77 \\
COHERE & 0.55 & 0.61 & 0.05 \\
A21 & 0.81 & 0.96 & 0.15 \\
PHI & 0.67 & 0.67 & 0.01 \\
GPT & 0.80 & 0.87 & 0.07 \\
\bottomrule
\end{tabular}
\caption{\label{tab:p-q} $p$, $q$ and $p-q$.  For $e$, we use Microsoft Phi-3.5 \cite{abdin2024phi}, OpenAI GPT3.5-turbo \cite{hurst2024gpt}, A21-Jamba 1.5 Mini \cite{lieber2021jurassic}, Cohere-command-r-08-2024 \cite{Ustun2024AyaMA}, Ministral-3B \cite{jiang2023mistral} and the Llama 3.2 70B model \cite{dubey2024llama}.}
\end{table}

We complete simulation studies for a ratio of (0.2,0.3,0.5), (0.3, 0.3,0.4), and (0.5,0.5) and a sample size of 30, 50, 70. The average $|\bar{\mathcal{E}}- \hat{\mathcal{E}}|$ over 10 iterations using different models as $e$ is recorded in table \ref{tab:basic_simu}. The performance of algorithm using Cohere, A21, Phi, and GPT is strong while the performance of the algorithm with Minstral and Llama is weak. We primary attribute this to the inability of Llama and Minstral to make correct statements. $p-q$ is small for Llama and Minstral and large for Cohere, A21, Phi, and GPT (shown in Table \ref{tab:p-q}). In fact, when we plot $p-q$ against $|\bar{\mathcal{E}}- \hat{\mathcal{E}}|$ in Figure \ref{fig:dotdata}, we notice that there is phrase change at value $p-q = 0.4$. $p-q < 0.4$ $|\bar{\mathcal{E}}- \hat{\mathcal{E}}|$ is high but  $p-q > 0.4$ implies that $|\bar{\mathcal{E}}- \hat{\mathcal{E}}|$ is generally small. This phrase change is not predicted in the theory and suggests that more work is needed. 
\section{Discussion}
\label{conclusion}
{\color{white} .. }Many natural language processing tasks exhibit a fundamental invariance: sequences of distinct tokens can convey identical meanings. This paper introduces a theoretically grounded metric for quantifying semantic variation, referred to as semantic spectral clustering. This approach reframes the challenge of measuring semantic variation as a prompt-engineering problem, which can be applied to any large language model (LLM), as demonstrated through our simulation analysis. In addition, unsupervised uncertainty can offer a solution to the issue identified in prior research, where supervised uncertainty measures face challenges in handling distributional shifts.

While we define two texts as having equivalent meaning if and only if they mutually imply one another, alternative definitions may be appropriate for specific use cases. For example, legal documents could be clustered based on the adoption of similar legal strategies, with documents grouped together if they demonstrate comparable approaches. In such scenarios, the entropy of the legal documents could also be computed to quantify their informational diversity. We have demonstrated that, provided there exists a function $e$ capable of performing the evaluation with weak accuracy, this estimator remains consistent. Given the reasoning capabilities of large language models (LLMs), we foresee numerous possibilities for extending this method to a wide range of applications.

In addition to the methodology presented, we present a theoretical analysis of the proposed algorithms by proving a theorem concerning the contraction rates of the entropy estimator and its strong consistency. Although the algorithm utilizes generative models, which are typically treated as black-boxes, we simplify the analysis by considering the outputs of these models as random variables. We demonstrate that only a few conditions on the generative are sufficient for our spectral clustering algorithm to achieve strong consistency. Our approach allows for many statistical methodologies to be applied in conjunctions with generative models to analyze text at a level previously not achievable by humans. 



\section{Limitation}
{\color{white} .. }We acknowledge that, while this research offers a theoretically consistent measurement of variation, it does not account for situations where two pieces of text may partially agree. For instance, two texts may contain points of agreement as well as points of disagreement. This is particularly common when different authors cite the same sources but reach contradictory conclusions.
%\section{Acknowledgments}



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}
\onecolumn
\appendix

\section{Theoretical Result}
\subsection{Proof of proposition  \ref{prop:equ}}
\begin{proof} 
    To prove that the relation $t_i \sim t_j$ if $t_i$ is true if and only if $t_j$ is true is an equivalence relation, we need to meet 3 key criteria, namely symmetry, reflexivity, and Transitivity. 

    First, symmetry 
    $t_i \sim t_j$ implies that $t_j$ is true $\Leftrightarrow$ $t_j$ is true, but this also means $t_j$ is true $\Leftrightarrow$ $t_i$ is true. Then we have $t_j \sim t_i$. 

    Second, reflexivity, 
    $t_i \sim t_j$ implies $t_j$ is true $\Leftrightarrow$ $t_j$ is true. But this means that $t_j$ is true  $\Leftrightarrow$ $t_i$ is true. Then we have $t_i \sim t_j$. 
    
    Third, transitivity,
    If $t_i \sim t_j$ and $t_j \sim t_k$, Then if $t_i$ is true $\Rightarrow$ $t_j$ is true $\Rightarrow$ $t_k$ is true, which means $t_i$ is true $\Rightarrow$ $t_k$ is true. On the other hand, using the same argument, $t_k$ is true $\Rightarrow$ $t_j$ is true $\Rightarrow$ $t_i$ is true. This means the $t_k$ is true $\Rightarrow$ $t_i$ is true. Therefore $t_i \sim t_k$. 

    The three points is sufficient to demonstrate that $\sim$ is a equivalence relation. 
\end{proof}
\subsection{Proof of Theorem \ref{the:strongConsistensy}}
\label{appendix:sec:the:strongconsistency}
To prove Theorem \ref{the:strongConsistensy}, we adopt notations from \cite{su2019strong}.
Consider the adjacency matrix $E$ which is determined by a Language model. 

Let $d_i = \sum_{j=1}^n E_{ij}$ denote the degree of node $i$,  $D = \text{diag}(d_1,\cdots, d_n)$, and $L = D^{-1/2}ED^{-1/2}$ be the graph Laplacian. We also define $n_k$ be the number of text in each cluster. We denote a block probability matrix $B = B_{k_1k_2}$ where $k_1,k_2 \in\{1,\cdots K\}$ be the clusters index.  i.e. 
$$ B_{k_1 k_2} = \begin{cases}
    p \quad \text{if $k_1 = k_2$}\\
    1-q \quad \text{otherwise.}
\end{cases}$$

Let $\mathbb{E}(E) = P$ i.e. the probability of edge between $i$ and $j$ is given by $P_{ij} = B_{k_1k_2}$ if text $i$ is in $C_{k_1}$ and $j$ is in $C_{k_2}$.
Denote $Z = \{Z_{ik}\}$ be a $n\times K$  binary matrix providing the cluster membership of text $t$, i.e., $Z_{ik} = 1$ if text $i$ is in $C_k$ and $Z_{ik} = 0$ otherwise. The population version of the Laplacian is given by $\mathcal{L} = \mathcal{D}^{-1/2}P\mathcal{D}^{-1/2}$ where  $\mathcal{D} = \text{diag}(d_1 \cdots d_n)$ where $d_i =\sum_{j=1}^{n}P_{ij} = p + (n-1)(q)$.

Let $\pi_{kn} = n_k/n, W_k = \sum_{l=1}^KB_{kl}\pi_{ln}$, $\mathcal{D}_B = \text{diag}(W_1,\cdots W_K)$, and $B_0=\mathcal{D}_B^{-1/2}B\mathcal{D}_B^{-1/2}  $
%C^star = 3528C_1 c_1^{-1/2}
\begin{assumption}[Assumption 1 in \cite{su2019strong}]
\label{assumption:eigenvalues}
$P$ is rank $k$ and spectral decomposition $\Pi_{n}^{1/2}P\Pi_{n}^{1/2}$ is $S_n \Omega_n S_n^T$ in which $S_n$ is a $K \times K$ matrix such that $S_n^T S_n = I_{K\times K}$  and $\Omega_n = \text{diag}(\omega_1 \cdots \omega_{K_n})$ such that $|\omega_1|\geq |\omega_2|\geq\cdots \geq|\omega_{K_n}|$
\end{assumption}
Assumption \ref{assumption:eigenvalues} implies that the spectral decomposition $$\mathcal{L} = U_n \Sigma_n U_n^T = U_{1n}\Sigma_{1n}U_{1n}^T$$

where \(\Sigma_{n}=\operatorname{diag}\left(\sigma_{1 n}, \ldots, \sigma_{K n}, 0, \ldots, 0\right)\) is a \(n \times n\) matrix that contains the eigenvalues of \(\mathcal{L}\) such that \(\left|\sigma_{1 n}\right| \geq\left|\sigma_{2 n}\right| \geq \cdots \geq\left|\sigma_{K n}\right|>0, \Sigma_{1 n}=\operatorname{diag}\left(\sigma_{1 n}, \ldots, \sigma_{K n}\right)\), the columns of \(U_{n}\) contain the 
 eigenvectors of \(\mathcal{L}\) associated with the eigenvalues in \(\Sigma_{n}, U_{n}=\left(U_{1 n}, U_{2 n}\right)\), and \(U_{n}^{T} U_{n}=I_{n}\) \cite{su2019strong}.
\begin{assumption}[Assumption 2 in \cite{su2019strong}]
\label{assumption:limits_nk}
    There exists constant $C_1 >0$ and $c_2>0$ such that
    $$C_1 \geq \lim\sup_n\sup_k n_k K/n \geq \lim \inf_n \inf_k n_k K/n \geq c_2  $$
\end{assumption}

\begin{assumption}[Assumption 3 in \cite{su2019strong}]
\label{assumption:bound_eigenvalues}
    Let $\mu_n = \min_i d_i$ and $\rho_n = \max(\sup_{k_1k_2}[B_0]_{k_1k_2},1)$. Then $n$ sufficiently large, 
    $$ 
\frac{K \rho_{n} \log ^{1 / 2}(n)}{\mu_{n}^{1 / 2} \sigma_{K n}^{2}}\left(1+\rho_{n}+\left(\frac{1}{K}+\frac{\log (5)}{\log (n)}\right)^{1 / 2} \rho_{n}^{1 / 2}\right) \leq 10^{-8} C_{1}^{-1} c_{2}^{1 / 2} .
$$
    
\end{assumption}
Let 
$$ 
\hat{O}_{n}=\bar{U} \bar{V}^{T}
$$
where \(\bar{U} \bar{\Sigma} \bar{V}^{T}\) is the singular value decomposition of \(\hat{U}_{1 n}^{T} U_{1 n}\). we also denote \(\hat{u}_{1 i}^{T}\) and \(u_{1 i}^{T}\) as the \(i\)-th rows of \(\hat{U}_{1 n}\) and \(U_{1 n}\), respectively.

Now we present the notation of the K-means algorithm. With a little abuse of notation, let \(\hat{\beta}_{\text {in }} \in \mathbb{R}^{K}\) be a generic estimator of \(\beta_{g_{i}^{0} n} \in \mathbb{R}^{K}\) for \(i=1, \ldots, n\). To recover the community membership structure (i.e., to estimate \(g_{i}^{0}\) ), it is natural to apply the  K-means clustering algorithm to \(\left\{\widehat{\beta}_{\text {in }}\right\}\). Specifically, let \(\mathcal{A}=\left\{\alpha_{1}, \ldots, \alpha_{K}\right\}\) be a set of \(K\) arbitrary  \(K \times 1\) vectors: \(\alpha_{1}, \ldots, \alpha_{K}\). Define
\[
\widehat{Q}_{n}(\mathcal{A})=\frac{1}{n} \sum_{i=1}^{n} \min _{1 \leq l \leq K}\left\|\hat{\beta}_{i n}-\alpha_{l}\right\|^{2}
\]

and \(\widehat{\mathcal{A}}_{n}=\left\{\widehat{\alpha}_{1}, \ldots, \widehat{\alpha}_{K}\right\}\), where \(\widehat{\mathcal{A}}_{n}=\arg \min _{\mathcal{A}} \widehat{Q}_{n}(\mathcal{A})\). Then we compute the estimated cluster  identity as
\[
\hat{g}_{i}=\underset{1 \leq l \leq K}{\arg \min }\left\|\hat{\beta}_{\text {in }}-\widehat{\alpha}_{l}\right\|,
\]

where if there are multiple \(l\) 's that achieve the minimum, \(\hat{g}_{i}\) takes value of the smallest one. We then state the key assumption that relates to K-means clustering algorithm. 

\begin{assumption}[Assumption 7 in \cite{su2019strong}]
\label{assumption:K-means}
     Suppose for \(n\) sufficiently large,
     \[
15 C^{*} \frac{K \rho_{n} \log ^{1 / 2}(n)}{\mu_{n}^{1 / 2} \sigma_{K n}^{2}}\left(1+\rho_{n}+\left(\frac{1}{K}+\frac{\log (5)}{\log (n)}\right)^{1 / 2} \rho_{n}^{1 / 2}\right) \leq c_{2} C_{1}^{-1 / 2} \sqrt{2}
\]
Where \(C^{*} = 3528C_1 c_2^{-1/2} \)
\end{assumption}

\begin{theorem}(Collorary 2.2)
\label{theorem:no_error}
    Corollary 2.2. Suppose that Assumptions \ref{assumption:eigenvalues},  \ref{assumption:limits_nk}, \ref{assumption:bound_eigenvalues}, and \ref{assumption:K-means} hold and the \(K\)-means algorithm is applied  to \(\hat{\beta}_{i n}=(n / K)^{1 / 2} \hat{u}_{1 i}\) and \(\beta_{g_{i}^{0} n}=(n / K)^{1 / 2} \hat{O}_{n} u_{1 i}\) Then, 
    \[
\sup _{1 \leq i \leq n} \mathbf{1}\left\{\tilde{g}_{i} \neq g_{i}^{0}\right\}=0 \quad \text { a.s. }
\]
\end{theorem}

We now have define the error of mis-classification. 

% Since there is no true $j$, we have to take all permutation of $j$ which we denote as $\sigma(j)$. 
\begin{definition}
Denote $M_\text{error} = \sum_{j} \sum_{i}\mathbb{I}(g_{ij}  \neq g^{\text{True}}_{ij})$ as the mis-classification error.
\end{definition}

\begin{lemma}
\label{lemma:error_connections}
    If $\sup_{i,j} \mathbb{I}(g_{ij}  \neq g^{\text{True}}_{ij}) = 0 \quad \text{a.s.}$, then $M_\text{error} = 0 \quad \text{a.s.}$
\end{lemma}
\begin{proof}
Notice $\mathbb{I}(g_{ij}  \neq g^{\text{True}}_{ij})$ can only takes up value $1$ or $0$. Therefore $\sum_{j} \sum_{i}\mathbb{I}(g_{ij}\neq g^{\text{True}}_{ij}) \neq 0 \Leftrightarrow \exists i, j  \text{ s.t }\mathbb{I}(g_{ij}\neq g^{\text{True}}_{ij}) \neq 0 \Leftrightarrow  \sup_{i,j}\mathbb{I}(g_{ij}\neq g^{\text{True}}_{ij}) \neq 0$  
    \begin{align*}
        \mathbb{P}(M_\text{error} \neq 0 \text{ i.o. }) &= \mathbb{P}\left( \sum_{j} \sum_{i}\mathbb{I}(g_{ij}\neq g^{\text{True}}_{ij}) \neq 0 \text{ i.o.}\right)\\
        &= \mathbb{P}\left( \exists i, j  \text{ s.t }\mathbb{I}(g_{ij}\neq g^{\text{True}}_{ij}) \neq 0 \text{ i.o. } \right)\\
        &= \mathbb{P}\left( \sup_{i,j}\mathbb{I}(g_{ij}\neq g^{\text{True}}_{ij}) \neq 0 \text{ i.o }\right)\\
        &= 0 \quad \text{ since $\sup_{i,j} \mathbb{I}(g_{ij}  \neq g^{\text{True}}_{ij}) = 0$ \text{ a.s.}}
    \end{align*}
    Here we use the classical notation i.o. as happens infinitely often. 
\end{proof}
\begin{lemma}
    \label{lemma:misclassification}
    $\sum_j \left|\sum_{i=1}^n g_{ij}-n_j\right| \leq M_\text{error}$
\end{lemma}
\begin{proof}

\begin{align*}
\sum_j \left|\sum_{i=1}^n g_{ij}-n_j\right|
&= \sum_j \Biggl| \sum_i \mathbb{I}(g_{ij} = 1, g^{\text{True}}_{ij} = 0 ) + \mathbb{I}(g_{ij} = 1, g^{\text{True}}_{ij} = 1 ) + \mathbb{I}(g_{ij} = 0, g^{\text{True}}_{ij} = 1 ) \\
&- \mathbb{I}(g_{ij} = 0, g^{\text{True}}_{ij} = 1 )- n_j \Biggr|\\
& = \sum_j \Biggl| \sum_i \mathbb{I}(g_{ij} = 1, g^{\text{True}}_{ij} = 0 ) - \mathbb{I}(g_{ij} = 0, g^{\text{True}}_{ij} = 1 ) \\
& + \sum_i \mathbb{I}(g_{ij} = 0, g^{\text{True}}_{ij} = 1 )+ \mathbb{I}(g_{ij} = 0, g^{\text{True}}_{ij} = 1 ) - n_j \Biggr|\\
& = \sum_j \Biggl| \sum_i \mathbb{I}(g_{ij} = 1, g^{\text{True}}_{ij} = 0 ) - \mathbb{I}(g_{ij} = 0, g^{\text{True}}_{ij} = 1 ) + n_j - n_j \Biggr|\\
&= \sum_j \Biggl| \sum_i \mathbb{I}(g_{ij} = 1, g^{\text{True}}_{ij} = 0 ) -\mathbb{I}(g_{ij} = 0, g^{\text{True}}_{ij} = 1 ) \Biggr|\\
&\leq \sum_j\sum_i \mathbb{I}(g_{ij} = 1, g^{\text{True}}_{ij} = 0 ) + \mathbb{I}(g_{ij} = 0, g^{\text{True}}_{ij} = 1 )\\
&=   \sum_{j} \sum_{i}\mathbb{I}(g_{ij}  \neq g^{\text{True}}_{ij}) \\
&= M_\text{error}
\end{align*}
\end{proof}
\newpage
\subsubsection{Proof of lemma \ref{lemma:error}}
\label{Appendix:proofoflemma:error}
Now we prove lemma \ref{lemma:error}.
\begin{proof}
Recall that
\begin{itemize}
    \item $\hat{p}(C_j) = \frac{1}{n}\sum_{i=1}^n g_{ij}$ and $\hat{\mathcal{E}}(\mathcal{T}) = - \sum_{j=1}^K\hat{p}( C_j) \log(\hat{p}( C_j))$
    \item $\bar{p}(C_j) = \frac{n_j}{n}$ and $\bar{\mathcal{E}}(\mathcal{T}) = - \sum_{j=1}^K\bar{p}( C_j) \log(\bar{p}( C_j))$
\end{itemize}
\begin{align*}
    |\hat{\mathcal{E}}(\mathcal{T}) - \bar{\mathcal{E}}(\mathcal{T})| &= \left| \sum_{j=1}^k\hat{p}( C_j) \log(\hat{p}( C_j)) -  \bar{p}( C_j) \log(\bar{p}( C_j)) \right|\\
    &=  \left|\sum_{j=1}^K\hat{p}( C_j)\log(\hat{p}( C_j)) -  \hat{p}( C_j)\log(\bar{p}( C_j)) + \hat{p}( C_j)\log(\bar{p}( C_j)) -  \bar{p}( C_j) \log(\bar{p}( C_j)) \right|\\
    &= \left|\sum_{j=1}^K \hat{p}( C_j)\log\left(\frac{\hat{p}( C_j)}{\bar{p}( C_j)}\right) - \left(\hat{p}( C_j) -\bar{p}( C_j)\right)\log(\bar{p}( C_j)) \right|\\
    &=  \left|\sum_{j=1}^K \hat{p}( C_j)\log\left(\frac{\hat{p}( C_j)}{p( C_j)}\right) - \left(\hat{p}( C_j) -\bar{p}( C_j)\right)\log(\bar{p}( C_j)) \right|\\
    &\leq \left|\sum_{j=1}^K \hat{p}( C_j)\log\left(\frac{\hat{p}( C_j)}{\bar{p}( C_j)}\right)\right| + \left|\sum_{j=1}^K \left(\hat{p}( C_j) -\bar{p}( C_j)\right)\log(\bar{p}( C_j)) \right|\\
    &\leq  \left|\sum_{j=1}^K \left(\frac{\hat{p}( C_j)-\bar{p}( C_j)}{\bar{p}( C_j)}\right)\right| + \left|\sum_{j=1}^K \left(\hat{p}( C_j) -\bar{p}( C_j)\right)\log(\bar{p}( C_j)) \right|\\
    &= \left|\sum_{j=1}^K \left(\frac{\frac{1}{n}\sum_{i=1}^n g_{ij}-\bar{p}( C_j)}{\bar{p}( C_j)}\right)\right| + \left|\sum_{j=1}^K \left(\frac{1}{n}\sum_{i=1}^n g_{ij} -\bar{p}( C_j)\right)\log(\bar{p}( C_j)) \right|\\
    &= \left|\sum_{j=1}^K \left(\frac{\frac{1}{n}\left(\sum_{i=1}^n g_{ij}-n_j\right)}{\bar{p}( C_j)}\right)\right| + \left|\sum_{j=1}^K \left(\frac{1}{n}\sum_{i=1}^n g_{ij} -\bar{p}( C_j)\right)\log(\bar{p}( C_j)) \right|\\
    &\leq \sum_{j=1}^K \left|\frac{\frac{1}{n}\left(\sum_{i=1}^n g_{ij}-n_j\right)}{\bar{p}( C_j)}\right| + \sum_{j=1}^K \left|\frac{1}{n}\sum_{i=1}^n (g_{ij} - n_j)\right|\left|\log(\bar{p}( C_j)) \right|\\
    &\leq \left|\frac{\frac{2K}{n}(M_\text{error})}{c_2}\right| + \log\left(\frac{2K}{c_2}\right) \left|\frac{1}{n} (M_\text{error})\right|\\
    &= h\left(\frac{2K}{c_2}\right) \left|\frac{1}{n} (M_\text{error})\right| 
\end{align*}

where $h(x) = \left(x+\log\left(x\right)\right)$. 
\end{proof}

We prove Theorem \ref{the:strongConsistensy}. To do so, we first restate Theorem \ref{the:strongConsistensy} with all the conditions required to get to the outcome.
\begin{theorem}[Theorem \ref{the:strongConsistensy} with all conditions stated]
    Assume that Assumptions \ref{assumption:eigenvalues},  \ref{assumption:limits_nk}, \ref{assumption:bound_eigenvalues}, and \ref{assumption:K-means} hold and the \(K\)-means algorithm is applied  to \(\hat{\beta}_{i n}=(n / K)^{1 / 2} \hat{u}_{1 i}\) and \(\beta_{g_{i}^{0} n}=(n / K)^{1 / 2} \hat{O}_{n} u_{1 i}\) Then 
    \[ |\bar{\mathcal{E}}(\mathcal{T}) - \hat{\mathcal{E}}(\mathcal{T}) | \rightarrow 0 \text{ almost surely }\]
\end{theorem}

\begin{proof}

Using Theorem \ref{theorem:no_error}, we know that under Assumptions \ref{assumption:eigenvalues},  \ref{assumption:limits_nk}, \ref{assumption:bound_eigenvalues}, and \ref{assumption:K-means}, we have that 
 \[
\sup _{1 \leq i \leq n} \mathbf{1}\left\{\tilde{g}_{i} \neq g_{i}^{0}\right\}=0 \quad \text { a.s. }
\]
Using Lemma \ref{lemma:error_connections}, we know that 

$$M_\text{error} = 0 \quad \text{a.s.}$$
Using results from Lemma \ref{lemma:error}, we know that $M_\text{error} \rightarrow 0 \quad a.s. \Rightarrow \hat{\mathcal{E}}(\mathcal{T}) \rightarrow \bar{\mathcal{E}}(\mathcal{T}) \quad a.s. $. 
\end{proof}
Now we try to prove Theorem \ref{the:finite_sample}. To do so, we state corollary 3.2 in \cite{lei2015consistency}.
\subsection{Proof of Theorem \ref{the:finite_sample}}
\label{appendix:proofofthe:finite_sample}
\begin{theorem}[Corollary 3.2 in \cite{lei2015consistency}]
\label{the:finite_sample_core}
 Let $E$ be an adjacency matrix from the $\operatorname{SBM}(Z, B)$, where $B=\alpha_{n} B_{0}$ for some $\alpha_{n} \geq \log n / n$ and with $B_{0}$ having minimum absolute eigenvalue $\geq \lambda>0$ and $\max _{k \ell} B_{0}(k, \ell)=1$. Let $g_{ij}$ be the output of spectral clustering using $(1+\varepsilon)$-approximate $k$-means. Then  there exists an absolute constant $c$ such that if 

\begin{equation*}
(2+\varepsilon) \frac{K n}{n_{\min }^{2} \lambda^{2} \alpha_{n}}<c
\end{equation*}
then with probability at least $1-n^{-1}$,
$$
\frac{1}{n}M_{\text{error}}\leq c^{-1}(2+\varepsilon) \frac{K n_{\max }}{n_{\min }^{2} \lambda^{2} \alpha_{n}}
$$
\end{theorem}


\begin{proof}
We now prove Theorem  \ref{the:finite_sample}.

    Under the model we have, we know that minimum eigenvalue of $B$ is $\lambda$. Use theorem \ref{the:finite_sample_core} to replace $h\left(\frac{2K}{c_2}\right) \left|\frac{1}{n} (M_\text{error})\right|$ with $ h\left(\frac{2K}{c_2}\right) c^{-1}(2+\varepsilon) \frac{K n_{\max }}{n_{\min }^{2} \lambda^{2} \alpha_{n}} $ in lemma \ref{lemma:error}.

We now have to show the existence of $c$ in Theorem \ref{the:finite_sample_core}.

\begin{align*}
    &\quad 2Kn_{\min}/n \geq c_2 \\
    &\Rightarrow 1/n_{\min}^2 \leq 4K^2/n^2c_2^2\\
    &\Rightarrow (2+\epsilon)\frac{Kn}{n_{\min}^2\lambda^2 \alpha_n} \leq (2+\epsilon)\frac{4K^3 }{n\lambda^2 \alpha_nc_2^2}\leq (2+\epsilon)\frac{4K^3}{\lambda^2c_2^2}\\
    &\text{Let $c = (2+\epsilon)\frac{4K^3}{\lambda^2}c_2^2$}
\end{align*}
substitute $c$ to $ h\left(\frac{2K}{c_2}\right) c^{-1}(2+\varepsilon) \frac{K n_{\max }}{n_{\min }^{2} \lambda^{2} \alpha_{n}} $, we have that 
\begin{equation*}
|\bar{\mathcal{E}}(\mathcal{T}) - \hat{\mathcal{E}}(\mathcal{T}) |  \leq h\left(\frac{2K}{c_2}\right) \frac{n_{\max }}{4c_2^2n_{\min }^{2} \alpha_{n}K^2}
\end{equation*}

\end{proof}
\subsubsection{Proof of Corollary \ref{corollary:rate}}
\label{appendix:proofofcorollary:rate}
\begin{proof}
Now we prove Corollary \ref{corollary:rate}. Note that $n \geq n_{\max} \geq n_{\min} \geq nc_2/2K$.

\begin{equation*}
|\bar{\mathcal{E}}(\mathcal{T}) - \hat{\mathcal{E}}(\mathcal{T}) |  \leq h\left(\frac{2K}{c_2}\right) \frac{n_{\max }}{4c_2^2n_{\min }^{2} \alpha_{n}K^2}\leq h\left(\frac{2K}{c_2}\right) \frac{1}{c_2^4 \alpha n}
\end{equation*}
\end{proof}

\newpage
\begin{lemma}
\label{lemma:final1}
    $$|\mathcal{E} - \hat{\mathcal{E}}| \leq \sum_{j=1}^K \left( \left| \frac{p(C_j) - \bar{p}(C_j)}{p(C_j)}\right| + \log\left(\frac{1}{p(C_j)}\right)\left| p(C_j) - \bar{p}(C_j)\right|\right) + h\left(\frac{2K}{c_2}\right) \left|\frac{1}{n} (M_\text{error})\right| $$
\end{lemma}
\begin{proof}
    First, we have that 
    $$|\mathcal{E} - \hat{\mathcal{E}}| \leq |\mathcal{E} -\bar{\mathcal{E}} +\bar{\mathcal{E}}-  \hat{\mathcal{E}}| \leq |\mathcal{E} -\bar{\mathcal{E}}| + | \bar{\mathcal{E}}-  \hat{\mathcal{E}}| \leq |\mathcal{E} -\bar{\mathcal{E}}| + h\left(\frac{2K}{c_2}\right) \left|\frac{1}{n} (M_\text{error})\right| $$
    Next, 
    \begin{align*}
        |\mathcal{E} -\bar{\mathcal{E}}| &\leq  \left| \sum_{j=1}^kp( C_j) \log(p( C_j)) -  \bar{p}( C_j) \log(\bar{p}( C_j)) \right|\\  
        &\leq \left| \sum_{j=1}^kp( C_j) \log(p( C_j)) -  \bar{p}( C_j) \log(p( C_j)) + \bar{p}( C_j) \log(p( C_j)) - \bar{p}( C_j) \log(\bar{p}( C_j)) \right|\\
        &\leq \sum_{j=1}^k \left|p( C_j) \log(p( C_j)) -  \bar{p}( C_j) \log(p( C_j)) \right| + \left|\bar{p}( C_j) \log(p( C_j)) - \bar{p}( C_j) \log(\bar{p}( C_j)) \right| \\
        &\leq \sum_{j=1}^k \left|p( C_j)  -  \bar{p}( C_j)\right| \log\left(\frac{1}{p( C_j)}\right)  + \left| \frac{p( C_j) -  \bar{p}( C_j)}{p( C_j)} \right|
    \end{align*}
\end{proof}

\begin{lemma}
\label{lemma:Final2}
With probability at least $1-\frac{1}{n}$,
$$\sum_{j=1}^k \left|p( C_j)  -  \bar{p}( C_j)\right| \leq K\sqrt{\frac{1}{2n}\log(2Kn)} $$
\end{lemma}
\begin{proof}

       $$ \left|p( C_j)  -  \bar{p}( C_j)\right| = \frac{1}{n}\left|np( C_j)  -  n_j\right|$$
Now use Hoeffding bound, we notice that for any $j$
$$\mathbb{P}(|n_j - np(C_j)| \geq \delta) \leq 2\exp\left(-\frac{2\delta^2}{n}\right) $$
Using union bound 
$$\mathbb{P}(\exists j \text{ such that }|n_j - np(C_j)| \geq \delta) \leq \sum_{j=1}^K\mathbb{P}(|n_j - np(C_j)| \geq \delta) \leq 2K\exp\left(-\frac{2\delta^2}{n}\right) $$

$\exists j \text{ such that }|n_j - np(C_j)| \geq \delta \Leftarrow\max |n_j - np(C_j)| \geq \delta \Leftarrow \sum_{j=1}^K |n_j - np(C_j)| \geq K\delta.$ 

Now, let $ 2K\exp\left(-\frac{2\delta^2}{n}\right) = \frac{1}{n}$, we have that $\delta = \sqrt{\frac{n}{2}\log(2Kn)}$

This gives us that with probability at least $1-\frac{1}{n}$,

$$ \sum_{j=1}^k \left|p( C_j)  -  \bar{p}( C_j)\right| \leq K\sqrt{\frac{1}{2n}\log(2Kn)} $$ 
\end{proof}
\begin{lemma}
\label{Lemma:Final3}
With probability at least $1-\frac{1}{n}$
$$n_{\min} \geq \frac{nc_2}{2K}$$
where $c_2 = 2K\left(1-\sqrt{\frac{2\log(nK)}{np_{\min}}}\right)p_{\min}$ and $p_{\min} = \min \{p(C_1) \dots p(C_K) \}$
\end{lemma}
\begin{proof}
    Using the Chernoff inequality, we have $$\mathbb{P}\left(n_j \leq (1-\delta)np(C_j)\right) \leq \exp\left(\frac{-np(C_j)}{2}\right)$$
Using the union bound
$$\mathbb{P}(n_{\min} \leq nc_2/2K) \leq \mathbb{P}\left(\exists j \text{ such that }n_j \leq (1-\delta)np(C_j)\right) \leq K\exp\left(\frac{-np_{\min}}{2}\right) $$
Let $K\exp\left(\frac{-np_{\min}}{2}\right) = \frac{1}{n}$, we get $\delta = \sqrt{\frac{2\log(nK)}{np_{\min}}}.$
Finally, we have $c_2 = 2K\left(1-\sqrt{\frac{2\log(nK)}{np_{\min}}}\right)p_{\min}$ 

\end{proof}
\newpage
\section{Simulations}
\subsection{Hobby Examples}
We can consider a list of things that a hypothetical individual "John" likes to do in his free time: 
\begin{itemize}
    \item running / jogging 
    \item Drone flying / pilot Aerial drones
    \item jazzercise / aerobics
    \item making pottery / making ceramics
    \item water gardening / aquatic gardening
    \item caving / spelunking / potholing
    \item cycling / bicycling / biking
    \item reading
    \item writing journals / journal writings/ journaling
    \item sculling / rowing
\end{itemize}
\iffalse
\subsection{Historical Examples}
On the day December 3,
\begin{itemize}
    \item 915  Pope John X crowns Berengar I of Italy as Holy Roman Emperor
    \item 1775  American Revolutionary War: USS Alfred becomes the first vessel to fly the Grand Union Flag; the flag is hoisted by John Paul Jones.
    \item 1800  War of the Second Coalition: Battle of Hohenlinden: French General Jean Victor Marie Moreau decisively defeats the Archduke John of Austria near Munich. Coupled with First Consul Napoleon Bonaparte's earlier victory at Marengo, this will force the Austrians to sign an armistice and end the war.
    \item 1818  Illinois becomes the 21st U.S. state.
    \item 1834  The Zollverein (German Customs Union) begins the first regular census in Germany.
    \item 1898  The Duquesne Country and Athletic Club defeats an all-star collection of early football players 160, in what is considered to be the first all-star game for professional American football.
    \item 1920  Following more than a month of TurkishArmenian War, the Turkish-dictated Treaty of Alexandropol is concluded.
    \item 1929  President Herbert Hoover delivers his first State of the Union message to Congress. It is presented in the form of a written message rather than a speech
    \item 1959  The current flag of Singapore is adopted, six months after Singapore became self-governing within the British Empire.
    \item 1979  In Cincinnati, 11 fans are suffocated in a crush for seats on the concourse outside Riverfront Coliseum before a Who concert.
    \item 1979  Iranian Revolution: Ayatollah Ruhollah Khomeini becomes the first Supreme Leader of Iran.
\end{itemize}
\fi
\newpage
\section{Prompt}
\label{appendix_sec:prompt_engineering}
This is the prompt we inserted for "Phi-3-mini-4k-instruct", "AI21-Jamba-1.5-Mini", "Cohere-command-r-08-2024".


\begin{verbatim}
'''
    You are a expert in logical deduction and you are given 2 piece of texts: TEXT A and TEXT B. 
    You are to identify if TEXT A implies TEXT B and TEXT B implies TEXT A at the same time. 
    
    TEXT A: 
    {text_A}
    
    TEXT B:
    {text_B}
    
    ## OUTPUT
    You are to return TRUE if TEXT A implies TEXT B and TEXT B implies TEXT A at the same time. 
    otherwise, you are to return FALSE 
'''
\end{verbatim}

This is the prompt we inserted for "Ministral-3B","Llama-3.3-70B-Instruct", "gpt-35-turbo"

\begin{verbatim}
''' 
    You are a expert in logical deduction and you are given 2 piece of texts: TEXT A and TEXT B. 
    You are to identify if TEXT A implies TEXT B and TEXT B implies TEXT A at the same time. 
    
    TEXT A: 
    {text_A}
    
    TEXT B:
    {text_B}
    
    ## OUTPUT
    You are to return TRUE if TEXT A implies TEXT B and TEXT B implies TEXT A at the same time. 
    otherwise, you are to return FALSE 
    
    ##FORMAT:
    START with either TRUE or FALSE, then detail your reasoning
'''
\end{verbatim}
\end{document}