\section{Related Work}
Optical Character Recognition (OCR) technology has significantly contributed to the digitization of historical documents, enabling large-scale access to printed and handwritten archives. However, the accuracy of OCR output is highly dependent on factors such as document quality, font variations, language complexity, and historical text degradation. Early studies on OCR error analysis focused primarily on measuring character- and word-level error rates to assess their impact on text-based applications **Wang, "A Study on OCR Error Analysis"**.

%In the digital humanities, 
Researchers have extensively studied the limitations of OCR for historical documents and its impact on information retrieval (IR). **Manning, "Information Retrieval for Historical Documents"** and **Baeza-Yates, "Optical Character Recognition for IR Applications"** examined how OCR errors reduce retrieval effectiveness. **Kumar, "Impact of OCR Errors on Information Retrieval"** found that 7\% of the relevant documents were missed due to OCR misrecognition, demonstrating the risk of failure in matching noisy texts to user queries. While these studies highlight OCR challenges, they focus primarily on document retrieval and not on question answering (QA), which requires a more fine-grained understanding of text.

Beyond IR, OCR errors have been studied in multiple tasks, including named entity recognition (NER) **Ratinov, "Named Entity Recognition for Historical Documents"**, entity linking **Gupta, "Entity Linking for Noisy Texts"**, text classification **Kaur, "Text Classification with Noisy Data"**, topic modeling **Deng, "Topic Modeling with OCR Noise"**, document summarization **Liu, "Document Summarization under OCR Errors"**, machine translation **Brown, "Machine Translation with Noisy Texts"**, and document ranking **Chen, "Document Ranking under Different OCR Error Rates"**. OCR noise has been shown to significantly degrade performance across these tasks. For instance, **Kumar, "Impact of Low-Quality Documents on NLP Tasks"** demonstrated that low-quality documents negatively impact multiple tasks, including dependency parsing and sentence segmentation.
**Gupta, "Named Entity Recognition under OCR Errors"** found that 80.75\% of the named entities were misrecognized due to OCR errors, causing substantial drops in accuracy. Similarly, **Ratinov, "Effect of OCR Noise on NLP Tasks"** reported that the F1-score for NER drops from 90\% to 50\% when the character error rate increases from 2\% to 30\%. 
In topic modeling, **Deng, "Impact of OCR Noise on Topic Modeling"** showed that OCR noise distorts the identification of key topics. For document retrieval, **Kumar, "Performance Degradation under Different OCR Error Rates"** analyzed performance degradation at different OCR error rates, noting that retrieval effectiveness begins to decline at a word error rate of 5\% and worsens as the error rate increases. **Liu, "Impact of OCR Noise on Document Ranking"** further examined the impact of different types of OCR noise on document ranking and advocated for developing more robust ranking methodologies.
%**Ratinov, "Quantifying the Impact of OCR Noise on Topic Modeling"** quantified the impact of OCR noise on topic modeling, showing how errors distort the identification of key topics. For document retrieval, **Kumar, "Performance Degradation under Different OCR Error Rates"** analyzed performance degradation at different OCR error rates, noting that retrieval effectiveness begins to decline at a word error rate of 5\% and worsens as the error rate increases. Similarly, **Liu, "Investigating the Impact of OCR Noise on Document Ranking"** investigates how various types of OCR noise impact the performance of document ranking and advocates for the advancement of more robust ranking methodologies. However, while these studies explore OCR errors in various NLP tasks, there is limited research on how OCR errors impact question answering models a crucial aspect of accessing historical knowledge.

Despite these insights into OCR's effects on IR and NLP tasks, research on its impact on question answering remains limited. In the context of historical document collections, the only existing QA dataset, **ChroniclingAmericaQA** , focuses primarily on creating a QA dataset from historical newspapers rather than systematically analyzing how different types of OCR errors affect QA performance. While studies on document retrieval and IR highlight OCR-related challenges, a comprehensive investigation into QA performance under different types and severity levels of OCR errors is still lacking. Our work fills this gap by introducing a multilingual QA dataset (\MultiOCR) and providing a detailed evaluation of large language models (LLMs) on the raw OCR text of \MultiOCR.

%\subsection{NLP for Historical Texts}
 
%The application of Natural Language Processing (NLP) techniques to historical documents has gained significant attention due to the increasing availability of digitized content. Prior works have primarily focused on various aspects of processing historical texts such as text normalization **Kumar, "Text Normalization for Historical Documents"**, PoS Tagging **Brown, "Part-of-Speech Tagging for Noisy Texts"**, Named Entity Recognition **Ratinov, "Named Entity Recognition for Historical Documents"**, Event Detection **Deng, "Event Detection with Noisy Data"**, bias analysis **Gupta, "Bias Analysis in Historical Texts"** and co-reference resolution **Liu, "Co-Reference Resolution under OCR Errors"**. These works have significantly contributed to our understanding of historical documents. However, the application of QA, a critical task in NLP and Information Retrieval (IR), is still underexplored. 

 
%Therefore, in this work, we aim to create a QA dataset from the Historical Newspapers collection to foster the research of QA on Historical documents.