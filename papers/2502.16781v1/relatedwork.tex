\section{Related Work}
Optical Character Recognition (OCR) technology has significantly contributed to the digitization of historical documents, enabling large-scale access to printed and handwritten archives. However, the accuracy of OCR output is highly dependent on factors such as document quality, font variations, language complexity, and historical text degradation. Early studies on OCR error analysis focused primarily on measuring character- and word-level error rates to assess their impact on text-based applications \cite{piotrowski2012natural, hill2019quantifying}.

%In the digital humanities, 
Researchers have extensively studied the limitations of OCR for historical documents and its impact on information retrieval (IR). \citet{croft1994evaluation} and \citet{traub2015impact} examined how OCR errors reduce retrieval effectiveness. \citet{chiron2017impact} found that 7\% of the relevant documents were missed due to OCR misrecognition, demonstrating the risk of failure in matching noisy texts to user queries. While these studies highlight OCR challenges, they focus primarily on document retrieval and not on question answering (QA), which requires a more fine-grained understanding of text.

Beyond IR, OCR errors have been studied in multiple tasks, including named entity recognition (NER) \cite{hamdi2020assessing, hamdi2023depth}, entity linking \cite{linhares2019impact}, text classification \cite{zu2004impact, vitman2022evaluating}, topic modeling \cite{mutuvi2018evaluating, zosa2021evaluating}, document summarization \cite{jing2003summarizing}, machine translation \cite{farooq12005effect, khayrallah-koehn-2018-impact}, and document ranking \cite{giamphy2023quantitative}. OCR noise has been shown to significantly degrade performance across these tasks. For instance, \citet{vanStrien2020AssessingTI} demonstrated that low-quality documents negatively impact multiple tasks, including dependency parsing and sentence segmentation.
\citet{hamadi20220cr} found that 80. 75\% of the named entities were misrecognized due to OCR errors, causing substantial drops in accuracy. Similarly, \citet{hamdi2023depth} reported that the F1-score for NER drops from 90\% to 50\% when the character error rate increases from 2\% to 30\%. 
In topic modeling, \citet{mutuvi2018evaluating} showed that OCR noise distorts the identification of key topics. For document retrieval, \citet{OCR-IR} analyzed performance degradation at different OCR error rates, noting that retrieval effectiveness begins to decline at a word error rate of 5\% and worsens as the error rate increases. \citet{giamphy2023quantitative} further examined the impact of different types of OCR noise on document ranking and advocated for developing more robust ranking methodologies.
%\citet{mutuvi2018evaluating} quantified the impact of OCR noise on topic modeling, showing how errors distort the identification of key topics. For document retrieval, \citet{OCR-IR} analyzed performance degradation at different OCR error rates, noting that retrieval effectiveness begins to decline at a word error rate of 5\% and worsens as the error rate increases. Similarly, \citet{giamphy2023quantitative} investigates how various types of OCR noise impact the performance of document ranking and advocates for the advancement of more robust ranking methodologies. However, while these studies explore OCR errors in various NLP tasks, there is limited research on how OCR errors impact question answering models a crucial aspect of accessing historical knowledge.

Despite these insights into OCR's effects on IR and NLP tasks, research on its impact on question answering remains limited. In the context of historical document collections, the only existing QA dataset, ChroniclingAmericaQA \cite{chroniclingamericaqa}, focuses primarily on creating a QA dataset from historical newspapers rather than systematically analyzing how different types of OCR errors affect QA performance. While studies on document retrieval and IR highlight OCR-related challenges, a comprehensive investigation into QA performance under different types and severity levels of OCR errors is still lacking. Our work fills this gap by introducing a multilingual QA dataset (\MultiOCR) and providing a detailed evaluation of large language models (LLMs) on the raw OCR text of \MultiOCR.

%\subsection{NLP for Historical Texts}
 
%The application of Natural Language Processing (NLP) techniques to historical documents has gained significant attention due to the increasing availability of digitized content. Prior works have primarily focused on various aspects of processing historical texts such as text normalization \cite{robertson-goldwater-2018-evaluating, bollmann-2019-large, bollmann-etal-2018-multi}, PoS Tagging \cite{hardmeier-2016-neural}, Named Entity Recognition \cite{ehrmann2020overview, ehrmann2023named}, Event Detection \cite{sprugnoli-tonelli-2019-novel, lai-etal-2021-event}, bias analysis \cite{borenstein-etal-2023-measuring} and co-reference resolution \cite{krug-etal-2015-rule}. These works have significantly contributed to our understanding of historical documents. However, the application of QA, a critical task in NLP and Information Retrieval (IR), is still underexplored. 

 
%Therefore, in this work, we aim to create a QA dataset from the Historical Newspapers collection to foster the research of QA on Historical documents.