%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf, natbib=true]{acmart}
%\documentclass[sigconf,natbib=true,review]{acmart}
\usepackage[most]{tcolorbox}
%\usepackage[dvipsnames]{xcolor}
\usepackage{makecell}

% \usepackage[table]{xcolor}
% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}
% \usepackage{lmodern}
% \usepackage[ngerman]{babel}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}
\usepackage{xspace}
\newcommand{\MultiOCR}{\texttt{MultiOCR-QA}\xspace}


%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[SIGIR '25]{Make sure to enter the correct
  conference title from your rights confirmation email}{July 13--18,
  2025}{Padova, IT}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{MultiOCR-QA: Dataset for Evaluating Robustness of LLMs in Question Answering on Multilingual OCR Texts}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Bhawna Piryani}
\email{bhawna.piryani@uibk.ac.at}
\affiliation{%
  \institution{University of Innsbruck}
  \city{Innsbruck}
  \country{Austria}
}
\author{Jamshid Mozafari}
\email{jamshid.mozafari@uibk.ac.at}
\affiliation{%
  \institution{University of Innsbruck}
  \city{Innsbruck}
  \country{Austria}
}
\author{Abdelrahman Abdallah}
\email{abdelrehman.abdallah@uibk.ac.at}
\affiliation{%
  \institution{University of Innsbruck}
  \city{Innsbruck}
  \country{Austria}
}
\author{Antoine Doucet}
\email{antoine.doucet@univ-lr.fr}
\affiliation{%
  \institution{University of La Rochelle}
  \city{La Rochelle}
  \country{France}
}
\author{Adam Jatowt}
\email{adam.jatowt@uibk.ac.at}
\affiliation{%
  \institution{University of Innsbruck}
  \city{Innsbruck}
  \country{Austria}
}





%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Piryani et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Optical Character Recognition (OCR) plays a crucial role in digitizing historical and multilingual documents, yet OCR errors -imperfect extraction of the text, including character insertion, deletion and permutation- can significantly impact downstream tasks like question-answering (QA). In this work, we introduce a multilingual QA dataset \MultiOCR, designed to analyze the effects of OCR noise on QA systems' performance. The \MultiOCR dataset comprises 60K question-answer pairs covering three languages, English, French, and German. The dataset is curated from OCR-ed old documents, allowing for the evaluation of OCR-induced challenges on question answering. We evaluate \MultiOCR on various levels and types of OCR errors to access the robustness of LLMs in handling real-world digitization errors. Our findings show that QA systems are highly prone to OCR induced errors and exhibit performance degradation on noisy OCR text.  %By comparing model performance on clean and noisy OCR text, we assess the robustness of QA models in handling real-world digitization errors. To better understand the impact of OCR noise in multilingual QA systems, we focus on three types of OCR errors: insertion, deletion, and substitution errors
  %, to varying degrees, 
  %and demonstrate the vulnerabilities of QA systems to such errors. 
  
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003347.10003348</concept_id>
       <concept_desc>Information systems~Question answering</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003318.10003321</concept_id>
       <concept_desc>Information systems~Content analysis and feature selection</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Question answering}
\ccsdesc[500]{Information systems~Content analysis and feature selection}

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Multilingual QA, OCR Text, Large Language Models}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Optical Character Recognition (OCR) technology has played a crucial role in preserving and providing access to historical texts. Over the past decade, significant advancements in OCR have improved text recognition accuracy, leading to the development of large-scale digital libraries of historical texts \cite{terras20111}. These libraries serve as valuable resources for researchers, historians, and the general public, granting access to old manuscripts, newspapers, and other archival materials. Historical documents hold a wealth of knowledge, offering insights into past events, cultures, and people. Many professionals such as historians, journalists, or sociologists rely on these collections for various research and analysis tasks.


Historical corpora have been widely used for numerous natural language processing (NLP) tasks, including Named Entity Recognition (NER), topic modeling, text classification, neural ranking, and understanding semantic changes in language over time \cite{ehrmann2020overview, ehrmann2023named, yang-etal-2011-topic, Liebeskind2020DeepLF,subramani2020survey}. Among these, question answering (QA) provides a direct approach to extracting precise and relevant information from historical texts. Unlike broader text-processing tasks, QA focuses on retrieving specific answers to user queries, making it an essential tool for efficiently accessing historical knowledge.


\begin{figure*}
    \centering
    \includegraphics[width=0.8
    \textwidth]{figures/multiocr_qa_example.pdf}
    \caption{An example of CorrectedOCR and RawOCR text from the MultiOCR dataset for the English language, highlighting different types of errors along with questions corresponding to this text. WER and CER denote Word Error Rate and Character Error Rate, respectively, indicating the level of errors in the text. The green highlights represent insertion errors, where IEO denotes Insertion Edit Operations - the number of insertions needed to transform RawOCR into CorrectedOCR. Similarly, red and blue highlights indicate deletion and substitution errors, with DEO and SEO representing Deletion Edit Operations and Substitution Edit Operations, respectively. The black boxes with numbers in the CorrectedOCR and RawOCR text correspond to the answers for each question in the paragraph.}
    \label{fig:example}
\end{figure*}



Despite the advancements in OCR technology, several challenges persist. OCR-generated text, referred as \textbf{RawOCR text} in this paper, often contains errors due to the degraded conditions of historical documents. Factors like faded ink and paper, irregular fonts, physical damage, and printing inconsistencies cause recognition errors that negatively impact downstream NLP tasks such as information retrieval, machine translation, and QA systems. Since QA models heavily depend on the quality of the input text, errors in raw OCR text can significantly affect the accuracy and reliability of generated answers.

%Common OCR Character error include misinterpretation such as  ſ (long s æ
%\textit{ſagen} (to say), \textit{cæur} (heart), \textit{Straße} (street)


For instance, in German, a passage with OCR error "\textit{Der Bericht der Lagsatzungsgesandtschaft wird verlesen undvon Hrn. Bürgermeistet Mousson als erstem Gesandten desStandes Zürich mit einigen Bemerkungen begleitet}.\footnote{English Translation: The report of the legislative mission is read out and accompanied by a few comments from Mr. Mayor Mousson, the first envoy of the Zurich state.}" contains multiple insertion, deletion, and substitution errors, such as "\textit{Lagsatzungsgesandtschaft}" (should be "Tagsatzungsgesandtschaft") and \textit{"Bürgermeistet"} (should be "Bürgermeister"). When a QA model encounters such a noisy OCR text, it may generate an incorrect answer. For example, given the question: "\textit{Wer hat den Bericht der Tagsatzungsgesandtschaft verlesen?}" (Who read the report of the parliamentary delegation?) the model incorrectly responds "\textit{Der Bericht der Tagsatzungsgesandtschaft wurde von Hrn. Bürgermeistet Mousson verlesen}." (The report of the parliamentary delegation was read out by Mr. Mayor Mousson.) which retains the OCR error and potentially misleads the QA system. This example highlights how even minor OCR errors can significantly affect QA accuracy, leading to misleading or incorrect answers. 

Although extensive research has focused on improving OCR accuracy and post-processing correction techniques, the specific impact of OCR noise on QA tasks remains underexplored. Previous studies have examined challenges related to OCR in information retrieval (IR) \cite{OCR-IR}, historical text processing \cite{piotrowski2012natural}, and entity recognition \cite{grover-etal-2008-named}. However, a systematic investigation of how OCR errors affect QA model performance is still lacking. %\citet{chroniclingamericaqa} developed a QA dataset from a historical newspapers spanning over a century, for English Language. %rather than a detailed analysis of OCR errors' impact on QA performance. 
Additionally, although large language models (LLMs) have been increasingly used to process OCR text, \cite{thomas-etal-2024-leveraging, madarasz2024ocr}, %no study has systematically examined 
their robustness in handling QA tasks with noisy OCR text remains an open research question.




In this paper, we address the critical gap in understanding how large language models (LLMs) perform in question-answering (QA) tasks when dealing with noisy OCR-generated text. Specifically, we analyze the robustness of LLMs in QA tasks using RawOCR text and introduce \MultiOCR \footnote{The dataset is freely available at: \url{https://github.com/DataScienceUIBK/MultiOCR-QA}}, a new multilingual QA dataset constructed from the ICDAR dataset, covering English, French, and German. This dataset includes both \textbf{RawOCR text} (OCR-generated text with errors) and \textbf{CorrectedOCR text} (ground truth), allowing a direct comparison of QA performance under different text quality conditions. To generate contextually relevant question-answer pairs from historical text excerpts, we leverage instruction-fine-tuned LLMs. We then systematically evaluate the impact of different types of OCR errors—insertions, deletions, and substitutions—on QA model performance, offering new insights into the strengths and limitations of LLMs when processing noisy historical data. Figure~\ref{fig:example} illustrates an example from the \MultiOCR dataset, showcasing various OCR errors and their impact on text accuracy. It presents QA pairs and responses using both CorrectedOCR and RawOCR text as context, demonstrating the effects of OCR noise on QA performance.

In summary, we make the following contributions in this work:
\begin{itemize}
    \item We introduce \MultiOCR, a new multilingual QA dataset from historical texts in English, French, and German, featuring both raw and corrected OCR text, allowing direct comparison under varying text quality conditions.
    \item Using \MultiOCR, we then conduct a comprehensive evaluation of LLM robustness against noisy OCR text analyzing their impact on QA performance.
    \item We also categorize different types and levels of OCR errors, evaluating their individual impact on QA performance, providing insights into how LLMs handle OCR-related challenges for different types of error. %\item Using \MultiOCR, we then conduct a comprehensive evaluation of LLM robustness against different OCR error types and levels analyzing their impact on QA performance.
    % \item We categorize OCR error types and evaluate their individual effects on answer correctness, providing insights into how LLMs handle OCR-related challenges in text processing.
   
\end{itemize}




%To address this gap, we analyze the robustness of LLMs in QA tasks involving raw OCR text. Specifically, we construct a multilingual QA dataset focusing on English, French, and German, derived from the ICDAR dataset, which contains historical text snippets. This dataset includes both rawOCR text (OCR-generated text with recognition errors) and Ground Truth text (referred to as CleanedOCR text in this paper), representing the corrected version of OCR output.

%To generate QA datasets for each language, we leverage instruction-fine-tuned LLMs, enabling the creation of diverse and contextually relevant question-answer pairs from historical text excerpts. Additionally, we systematically evaluate how different levels and types of OCR errors impact the performance of LLM-based QA models, shedding light on their robustness and limitations in handling noisy OCR data.



\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/multiocr_qa_pipeline.pdf}
    \caption{The Pipeline for \MultiOCR generation: Arrows represent the output quantity based on the number of documents and callouts illustrate the statistics  for English, French and German Language. Ps and QAs denote the number of paragraphs and question-answer pairs, respectively. Note, to supplement the data for English Language 10,000 questions from ChroniclingAmericaQA were included.}
    \label{fig:QA_generation_pipeline}
\end{figure*}

\section{Related Work}

Optical Character Recognition (OCR) technology has significantly contributed to the digitization of historical documents, enabling large-scale access to printed and handwritten archives. However, the accuracy of OCR output is highly dependent on factors such as document quality, font variations, language complexity, and historical text degradation. Early studies on OCR error analysis focused primarily on measuring character- and word-level error rates to assess their impact on text-based applications \cite{piotrowski2012natural, hill2019quantifying}.

%In the digital humanities, 
Researchers have extensively studied the limitations of OCR for historical documents and its impact on information retrieval (IR). \citet{croft1994evaluation} and \citet{traub2015impact} examined how OCR errors reduce retrieval effectiveness. \citet{chiron2017impact} found that 7\% of the relevant documents were missed due to OCR misrecognition, demonstrating the risk of failure in matching noisy texts to user queries. While these studies highlight OCR challenges, they focus primarily on document retrieval and not on question answering (QA), which requires a more fine-grained understanding of text.

Beyond IR, OCR errors have been studied in multiple tasks, including named entity recognition (NER) \cite{hamdi2020assessing, hamdi2023depth}, entity linking \cite{linhares2019impact}, text classification \cite{zu2004impact, vitman2022evaluating}, topic modeling \cite{mutuvi2018evaluating, zosa2021evaluating}, document summarization \cite{jing2003summarizing}, machine translation \cite{farooq12005effect, khayrallah-koehn-2018-impact}, and document ranking \cite{giamphy2023quantitative}. OCR noise has been shown to significantly degrade performance across these tasks. For instance, \citet{vanStrien2020AssessingTI} demonstrated that low-quality documents negatively impact multiple tasks, including dependency parsing and sentence segmentation.
\citet{hamadi20220cr} found that 80. 75\% of the named entities were misrecognized due to OCR errors, causing substantial drops in accuracy. Similarly, \citet{hamdi2023depth} reported that the F1-score for NER drops from 90\% to 50\% when the character error rate increases from 2\% to 30\%. 
In topic modeling, \citet{mutuvi2018evaluating} showed that OCR noise distorts the identification of key topics. For document retrieval, \citet{OCR-IR} analyzed performance degradation at different OCR error rates, noting that retrieval effectiveness begins to decline at a word error rate of 5\% and worsens as the error rate increases. \citet{giamphy2023quantitative} further examined the impact of different types of OCR noise on document ranking and advocated for developing more robust ranking methodologies.
%\citet{mutuvi2018evaluating} quantified the impact of OCR noise on topic modeling, showing how errors distort the identification of key topics. For document retrieval, \citet{OCR-IR} analyzed performance degradation at different OCR error rates, noting that retrieval effectiveness begins to decline at a word error rate of 5\% and worsens as the error rate increases. Similarly, \citet{giamphy2023quantitative} investigates how various types of OCR noise impact the performance of document ranking and advocates for the advancement of more robust ranking methodologies. However, while these studies explore OCR errors in various NLP tasks, there is limited research on how OCR errors impact question answering models a crucial aspect of accessing historical knowledge.

Despite these insights into OCR's effects on IR and NLP tasks, research on its impact on question answering remains limited. In the context of historical document collections, the only existing QA dataset, ChroniclingAmericaQA \cite{chroniclingamericaqa}, focuses primarily on creating a QA dataset from historical newspapers rather than systematically analyzing how different types of OCR errors affect QA performance. While studies on document retrieval and IR highlight OCR-related challenges, a comprehensive investigation into QA performance under different types and severity levels of OCR errors is still lacking. Our work fills this gap by introducing a multilingual QA dataset (\MultiOCR) and providing a detailed evaluation of large language models (LLMs) on the raw OCR text of \MultiOCR.

%\subsection{NLP for Historical Texts}
 
%The application of Natural Language Processing (NLP) techniques to historical documents has gained significant attention due to the increasing availability of digitized content. Prior works have primarily focused on various aspects of processing historical texts such as text normalization \cite{robertson-goldwater-2018-evaluating, bollmann-2019-large, bollmann-etal-2018-multi}, PoS Tagging \cite{hardmeier-2016-neural}, Named Entity Recognition \cite{ehrmann2020overview, ehrmann2023named}, Event Detection \cite{sprugnoli-tonelli-2019-novel, lai-etal-2021-event}, bias analysis \cite{borenstein-etal-2023-measuring} and co-reference resolution \cite{krug-etal-2015-rule}. These works have significantly contributed to our understanding of historical documents. However, the application of QA, a critical task in NLP and Information Retrieval (IR), is still underexplored. 

 
%Therefore, in this work, we aim to create a QA dataset from the Historical Newspapers collection to foster the research of QA on Historical documents.






\section{Methodology}
To systematically investigate the impact of OCR errors on QA systems, we constructed \MultiOCR, a new multilingual QA dataset derived from historical texts processed with OCR. This section details the two main stages of the dataset creation pipeline: Data Collection and Question-Answer Generation. Figure \ref{fig:QA_generation_pipeline} provides an overview of this process.

\subsection{Data Collection}
In this section, we describe the process of collecting documents to generate question-and-answer pairs to carry out our study. We utilized the ICDAR 2019 POST-OCR Text Correction dataset\footnote{\url{https://sites.google.com/view/icdar2019-postcorrectionocr}}~\cite{icdar-2019} as our primary historical text source. We selected this dataset as it contains CorrectedOCR text (Ground Truth Text) along with RawOCR text. The dataset contains 22 million OCR-processed characters and corresponding ground truth text for ten European languages (English, French German, Finish, Spanish, Dutch, Czech, Bulgarian, Slovak, and Polish). In our study, we focused on English, French, and German.

\textbf{\textit{Language Specific Data Collection}:} We extracted text in these three languages from the ICDAR 2019 dataset. The texts originally came from %extracted from 
various historical document repositories. 
\begin{itemize}
    \item \texttt{English:} The documents for the English language in the ICDAR 2019 dataset are sourced from IMPACT - British Library, comprising a total of 150 files. %As the number of files for English was relatively low, we decided to extract 10,000 QA pairs from the ChroniclingAmericaQA dataset \cite{chroniclingamericaqa}. This dataset, created from a historical newspaper collection, contains both ground-truth text and raw OCR text, making it suitable for our study.
    \item \texttt{French:} For the French language, the ICDAR 2019 dataset provides a collection from three sources: the HIMANIS\footnote{\url{https://www.himanis.org}} Project, IMPACT - National Library of France, and the RECEIPT\footnote{\url{http://findit.univ-lr.fr/}} dataset. The original dataset contained 2,800 files. %After analyzing the language of each file, we discovered that some documents labeled as French were actually in other languages, particularly Latin. As a result, we removed non-French documents, reducing the dataset to 1,713 French-language files and eliminating 1,086 Latin-language files.
    \item \texttt{German:} The German-language dataset includes the OCR-processed text from multiple sources, such as, front pages of the Swiss newspaper NZZ\footnote{\url{https://zenodo.org/records/3333627}}, IMPACT - German National Library, GT4Hist-dta19 dataset, GT4Hist - EarlyModernLatin, GT4Hist - Kallimachos, GT4Hist - RefCorpus-ENHG-Incunabula, and GT4Hist - RIDGES-Fraktur\footnote{\url{https://zenodo.org/records/1344132}} \cite{springmann2018gt4hist}. The German dataset in ICDAR 2019 originally contained 10,032 files. %After removing files in Latin or any other non-German languages, we retained a total of 9,058 German-language files.
\end{itemize}


\textbf{\textit{Language Verification and Filtering}:} Prior to QA pair generation, we preprocessed the ICDAR dataset to ensure that each document contains text in the correct target language. We applied langdetect\footnote{\url{https://github.com/Mimino666/langdetect}} library to detect the language of documents. 
%After analyzing the language of each file, we
The analysis revealed that some documents labeled as English, French and German were actually in other languages, particularly Latin. To maintain dataset integrity, we removed non-target language documents, resulting in the following reductions: We removed non-English documents, reducing the number of documents in the English dataset to 141. Similarly, we discarded non-French documents, reducing the dataset to 1,713 French-language files, and eliminating 1,086 Latin-language files. Finally, for German, we removed Latin or other non-German files and retained a total of 9,075 German-language files. 

%\textbf{\textit{Symbol Removal and Ground Truth Filtering:}} 
Furthermore, the ICDAR dataset, originally intended for post-OCR correction, contained special alignment symbols (e.g., \texttt{@}, \texttt{\#}) to map the RawOCR text to its ground-truth counterpart. We removed the alignment symbols from the ground-truth text before generating questions. We also excluded files where the ground-truth text was missing, resulting in the removal of 16 files for English, 3 files for French, and none for German. This preprocessing step ensured that all QA pairs were generated from text that had both CorrectedOCR text and RawOCR text\footnote{We will sometimes use CorrectedOCR and RawOCR to denote the text that was subject to post-OCR error correction and one without such correction, respectively.}.



\subsection{Question-Answer Generation}

To construct the multilingual QA dataset, we opted for automatic QA pair generation, as manual dataset creation would require substantial human resources. To achieve this, we instruction fine-tuned a pretrained LLM for each target language to generate QA pairs from CorrectedOCR text.

While LLMs are pretrained on diverse NLP tasks, they typically generate a variety of question types, including non-factoid and open-ended questions. Since our goal is to develop a factoid QA dataset, we fine-tuned the models using language-specific QA datasets to ensure the generation of structured, precise, and factual question-answer pairs.

Instruction fine-tuning enhances both the capabilities and controllability of LLMs \cite{zhang2023instruction}. Fine-tuning instruction-based datasets across multiple languages allows the model to generalize across different question-answering styles, ensuring that the generated questions remain relevant even when dealing with language-specific variations. We opted to finetune LLama-3.1-70B instruct model \cite{dubey2024llama} separately for each language using widely adopted QA datasets.

 For fine-tuning the model in English, we used the SQuAD v1 dataset \cite{rajpurkar-etal-2016-squad}. We randomly selected 2,067 paragraphs and 10,570 questions from the development set and 3,000 paragraphs and 13,894 questions from the test set. In total, we fine-tuned the LLaMA-3.1-70B instruct model for English QA dataset generation using 5,067 paragraphs and 24,464 questions. Similarly, for the French language, we fine-tuned the model on the FQuAD dataset \cite{dhoffschmidt-etal-2020-fquad}, utilizing both the validation and training sets, comprising 5,689 paragraphs and 23,919 questions. For the German language, we fine-tuned the model on the GermanQuAD dataset \cite{moller-etal-2021-germanquad} using the training and validation sets, which consisted of 3,014 paragraphs and 13,722 questions. This fine-tuning step ensured that the model accurately generated factoid-style QA pairs, reducing instances of open-ended or conversational questions.

After instruction fine-tuning, we used the fine-tuned model for each language to generate questions for the preprocessed dataset prepared in the initial step. To systematically generate high-quality QA pairs, we employed a \textbf{two-step prompt-based approach:} Answer  Extraction and Question Generation for the Extracted Answers. 

\textbf{\textit{Answer Extraction:}} The model was first prompted to extract multiple candidate answer spans from a given passage. These spans included entities, numbers, dates, locations, and key phrases that could serve as factual answers. The following prompt was used for the extraction of the candidate answer.  
%To fine-tune the models, we used two prompts. In the first prompt, we asked the model to extract several candidate spans from the given passage that were likely to be answers. In the second prompt, we provided the extracted candidate answers and asked the model to generate questions for which these candidates would be the answers, based on the passage. The prompts for all three languages are shown below: 

\begin{tcolorbox}[size=small,colback=blue!2!white,colframe=blue!50!black, title=English Answer Extraction Prompt ]
\begin{quote}
\footnotesize
\emph{\textit{\textbf{System:}} You are an expert at extracting key information from text. Your goal is to identify spans of text that are likely to serve as answers to potential questions based on the input passage. Focus on meaningful, distinct, and diverse snippets such as entities, nouns, verbs, adjectives, numbers, dates, and phrases. Avoid redundancy and ensure the answers are diverse, representing key information in the passage.}

\emph{\textbf{\textit{User}}: Given the passage below, extract several candidate spans that are likely to be answers to potential questions. Write only the extracted answers, separated by a semicolon (;). Passage : \{context\}}
\end{quote}
\end{tcolorbox}

After generating the candidate answers, we checked for duplicated answer spans. If the answers were duplicate, we removed them and retained only unique answers. This prompt was applied uniformly across all three languages, with translations adapted to each language.

\textbf{\textit{Question Generation:}} Following answer extraction, the extracted answer spans were re-fed into the model, and it was prompted to generate questions that align with each answer while maintaining contextual relevance. The generated questions were structured to be standalone, well-formed, and factually grounded in the passage. The following Prompt was used for Question Generation from the Extracted Answers.

\begin{tcolorbox}[size=small,colback=blue!2!white,colframe=blue!50!black, title=English Question Generation Prompt ]
\begin{quote}
\footnotesize
\emph{\textit{\textbf{System:}} You are an expert at generating standalone questions based on the provided passage. Your goal is to create a clear, relevant, and self-contained question that aligns with the information in the passage. The question should not explicitly reference the passage or require additional information to be understandable. Ensure the question is concise, well-structured, and meaningful.}

\emph{\textit{\textbf{User:}} Based on the passage below, please generate a question that is relevant to the information provided. The question should be standalone, clear, and understandable without referencing the passage directly. The answer to the question should be [{answer}]. Passage : \{context\}}
\end{quote}
\end{tcolorbox}

%After instruction fine-tuning, we used the fine-tuned model for each language to generate questions for the preprocessed dataset prepared in the initial step. For each language, we first prompted the fine-tuned model to generate candidate answer spans from passages using the \textit{Answer Generation Prompt}. We then prompted the fine-tuned model again to generate questions based on these unique answers. 

Using this approach, we generated 941 questions for English, 10,522 questions for French, and 44,607 questions for German.

\textit{\textbf{Dataset Filtering:}} After generating the dataset for each language, we applied additional filtering steps to ensure quality and consistency. Specifically, we removed questions that did not end with a question mark, duplicate questions, and questions with excessively long answers. Since LLM-generated datasets may contain hallucinated long answers, we applied this additional filtering by removing excessively long answers. %Question with excessively long answers were removed because for some instances LLM due to its hallucinating nature gave a long paragraph as answer. 

As a result, we removed 66 questions for English Language, 530 questions for French Language, and 7,210 questions for German Language. This final filtering step ensured that the \MultiOCR dataset consisted of concise, well-structured, and factually accurate question-answer pairs.

 
\begin{comment} 
    

\begin{tcolorbox}[size=small,colback=green!8!white,colframe=green!75!black, title= Answer Generation French]
\begin{quote}
\footnotesize
\emph{\textit{System:} Vous êtes un expert dans l'extraction d'informations clés à partir d'un texte. Votre objectif est d'identifier les parties de texte qui sont susceptibles de servir de réponses à des questions potentielles basées sur le passage d'entrée. Concentrez-vous sur des bribes significatives, significatives, distinctes et diverses, telles que des entités, des noms, des verbes, des adjectifs, des nombres, des dates et des phrases. Évitez la redondance et veillez à ce que les réponses soient diversifiées et représentent les informations clés du passage.}

\emph{User: Étant donné le passage ci-dessous, extrayez plusieurs segments candidats qui sont susceptibles d’être des réponses à des questions potentielles. Écrivez uniquement les réponses extraites, séparées par un point-virgule (;). Passage : \{context\}}
\end{quote}
\end{tcolorbox}
\begin{tcolorbox}[size=small,colback=green!8!white,colframe=green!75!black, title= Question Generation French]
\begin{quote}
\footnotesize
\emph{\textit{System:} Vous êtes un expert dans la génération de questions autonomes basées sur le passage fourni. Votre objectif est de créer une question claire, pertinente et autonome qui s’aligne avec les informations contenues dans le passage. La question ne doit pas explicitement faire référence au passage ou nécessiter des informations supplémentaires pour être compréhensible. Assurez-vous que la question soit concise, bien structurée et significative.}

\emph{User: "À partir du passage ci-dessous, veuillez générer une question pertinente par rapport aux informations fournies. La question doit être autonome, claire et compréhensible sans faire directement référence au passage. La réponse à la question doit être [{answer}]. Passage : {context}}
\end{quote}
\end{tcolorbox}

\begin{tcolorbox}[size=small,colback=blue!8!white,colframe=blue!75!black, title=Answer Generation  German]
\begin{quote}
\footnotesize
\emph{\textit{System:} Sie sind Experte für die Extraktion von Schlüsselinformationen aus Texten. Ihr Ziel ist es, Textabschnitte zu identifizieren, die auf der Grundlage der eingegebenen Passage als Antworten auf mögliche Fragen dienen können. Konzentrieren Sie sich auf aussagekräftige, eindeutige und vielfältige Textabschnitte wie Entitäten, Substantive, Verben, Adjektive, Zahlen, Daten und Phrasen. Vermeiden Sie Redundanzen und stellen Sie sicher, dass die Antworten vielfältig sind und die wichtigsten Informationen des Textes wiedergeben.}

\emph{User: Extrahieren Sie aus dem nachstehenden Text mehrere Kandidaten, die als Antworten auf mögliche Fragen in Frage kommen. Schreiben Sie nur die extrahierten Antworten, getrennt durch ein Semikolon (;). Passage : \{context\}}
\end{quote}
\end{tcolorbox}

\begin{tcolorbox}[size=small,colback=blue!8!white,colframe=blue!75!black, title=Question Generation  German]
\begin{quote}
\footnotesize
\emph{\textit{System:} Sie sind Experte für die Erstellung eigenständiger Fragen auf der Grundlage des vorgegebenen Textes. Ihr Ziel ist es, eine klare, relevante und in sich geschlossene Frage zu erstellen, die sich an den Informationen im Text orientiert. Die Frage sollte sich nicht explizit auf die Textstelle beziehen oder zusätzliche Informationen erfordern, um verständlich zu sein. Achten Sie darauf, dass die Frage prägnant, gut strukturiert und aussagekräftig ist.}

\emph{User: Bitte formulieren Sie auf der Grundlage des unten stehenden Textes eine Frage, die sich auf die angegebenen Informationen bezieht. Die Frage sollte für sich stehen, klar und verständlich sein, ohne sich direkt auf den Text zu beziehen. Die Antwort auf die Frage sollte [{answer}] lauten.Passage : {context}}
\end{quote}
\end{tcolorbox}
\end{comment} 








\section{Dataset Analysis}

After applying all the filtering steps, we obtained the final dataset, comprising 50,079 question-answer pairs.
%derived from the ICDAR dataset. 
Due to the relatively low number of English-language documents in the ICDAR 2019 dataset, the initial \MultiOCR dataset contained only 875 QA pairs for English. To balance the distribution of QA pairs across languages, we incorporated 10,000 additional QA pairs from the ChroniclingAmericaQA dataset \cite{chroniclingamericaqa}. That dataset, sourced from the American historical newspaper collection called Chronicling America\footnote{\url{https://chroniclingamerica.loc.gov/about/}}, spans over 120 years from 1800-1920. It includes both ground-truth and RawOCR text, making it particularly relevant to our study. Consequently, the final \MultiOCR dataset consists of documents from historical newspapers, books, and centuries-old scripts, with 10,875 QA pairs in English, 10,004 in French, and 39,200 in German. The dataset statistics, including average paragraph length, question length, and answer length, are presented in Table \ref{tab:dataset_statistics}. 

\begin{table}[t]
\centering
\caption{Basic statistics of the \MultiOCR dataset, including question-answer (QA) pair count, paragraph count, and average text lengths across languages.}
\label{tab:dataset_statistics}
\resizebox{0.85\columnwidth}{!}{%
\begin{tabular}{cccc}
\toprule
                                            & \textbf{English}   & \textbf{French}    & \textbf{German}   \\
\toprule
\#QA pairs                                          & 10,875  & 10,004   & 39,200  \\
\#Paragraphs                                        & 6,525   & 1,670    &9,075 \\
Average CorrecteddOCR paragraph length (words)      & 219.09  & 297.53   & 212.86 \\
Average RawOCR paragraph length (words)             & 233.25  & 335.73   &193.23 \\
Average question length (words)                     & 10.98   & 8.73     & 8.08 \\
Average answer length (words)                       & 2.05    & 3.12     & 5.63 \\
Average questions per paragraph                     & 1.67    & 5.99     & 4.32   \\
\bottomrule
\end{tabular}%
}
\end{table}


\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/error_type_edit_ops.pdf}
    \caption{Statistics of insertion, deletion, and substitution errors for each language, categorized into low, medium, and high noise levels.}
    \label{fig:error_type_edit_ops}
\end{figure*}

\subsection{Quantifying and Filtering OCR Noise}
\label{error_quantification}

To assess the impact of OCR noise on QA quality, we quantified the noise level in the RawOCR text using two standard metrics: Character Error Rate (CER) and Word Error Rate (WER). CER measures the proportion of character-level errors in the raw OCR text compared to the ground truth text. It is computed as the number of insertions, deletions, and substitutions (including spaces) required to transform the RawOCR text into its correct form. WER quantifies word-level discrepancies, representing the proportion of words that require modifications (insertions, deletions, or substitutions) to match ground truth text. Both CER and WER were computed using the Levenshtein distance \cite{miller-2009}, which determines the minimum number of edits needed to correct the OCR-generated text. A high CER but low WER suggests that errors are concentrated within a few words (e.g., spelling variations), whereas a high WER indicates distortions across multiple words, significantly affecting readability.



\textbf{\textit{Outlier Detection:}} To ensure a reliable analysis, we applied the Interquartile Range (IQR) method to detect and remove outliers in CER values. Outliers were defined as CER values below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR, where Q1 and Q3 represent the 33th and 66th percentiles of the CER distribution, respectively, and IQR is the difference between Q3 and Q1. This filtering resulted in the removal of 1,056 English, 351 French, and 2,423 German paragraphs.

Following outlier removal, we categorized the remaining paragraphs into three noise levels based on CER percentiles for each language. Documents with CER below the 33rd percentile were classified as "low noise," those between the 33rd and 66th percentiles as "medium noise", and those above the 66th percentile as "high noise." The specific CER thresholds for each category and language were as follows:

\begin{itemize}
    \item \texttt{English:} Low: CER $<$ 0.0618, Medium: $0.0618 \leq$ CER $<$ 0.1104, High: CER $\geq$ 0.1104
    \item \texttt{French:} Low: CER $<$ 0.0357, Medium: $0.0357 \leq$ CER $<$ 0.0558, High: CER $\geq$ 0.0558
    \item \texttt{German:} Low: CER $<$ 0.8489, Medium: $0.8489 \leq$ CER $<$ 0.8947, High: CER $\geq$ 0.8947
\end{itemize}


In addition to CER-based classification, we analyzed the distribution of three specific OCR error types: insertions, deletions, and substitutions. Each error type was categorized separately using a percentile-based approach, allowing for a more detailed examination of the nature and severity of OCR distortions. To further investigate OCR noise patterns, we classified insertion, deletion, and substitution errors into low, medium, and high noise levels.
As illustrated in Figure \ref{fig:error_type_edit_ops}, the distribution of these error types varies significantly across languages, reflecting differences in OCR quality and text processing challenges in English, French, and German. Additionally, Table \ref{tab:ocr_error_statistics} presents the statistical characteristics of the distribution of OCR error metrics across languages, including CER, WER, and edit operations, providing further insights into the OCR noise characteristics.


\begin{table}[t]
\centering
\caption{OCR Error Statistics across Languages}
\label{tab:ocr_error_statistics}
\resizebox{0.75\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{English} & \textbf{French} & \textbf{German} \\
\midrule
\multicolumn{4}{c}{\textbf{Character Error Rate (CER)}} \\
Mean   & 0.0988  & 0.0519  & 0.2816  \\
Median & 0.0826  & 0.0440  & 0.2592  \\
%33rd Percentile & 0.0618 & 0.0357 & 0.2453  \\
%66th Percentile & 0.1104 & 0.0558 & 0.2757  \\
\midrule
\multicolumn{4}{c}{\textbf{Word Error Rate (WER)}} \\
Mean   & 0.2587  & 0.1904  & 0.8713  \\
Median & 0.2319  & 0.1760  & 0.8730  \\
% 33rd Percentile & 0.1774 & 0.1483 & 0.8489  \\
% 66th Percentile & 0.2938 & 0.2062 & 0.8947  \\
\midrule
\multicolumn{4}{c}{\textbf{Edit Operations}} \\
Mean Substitutions      & 74.66  & 31.51  & 240.23  \\
Median Substitutions    & 61.00  & 21.00  & 230.50  \\
Median Deletions        & 11.74  & 41.09  & 85.09  \\
Median Deletions        & 7.00   & 25.00  & 39.00  \\
Median Insertions       & 39.02  & 20.90  & 82.60  \\
Median Insertions       & 30.00  & 11.00  & 80.00  \\
% \midrule
% \multicolumn{4}{c}{\textbf{Categorization of OCR Errors}} \\
% Low Error Count & 1805 & 435  & 2196  \\
% Medium Error Count & 1804 & 435  & 2195  \\
% High Error Count & 1860 & 449  & 2261  \\
\bottomrule
\end{tabular}
}
\end{table}


%Similarly, we categorized the documents based on their Word Error Rate (WER) into three levels: low, medium, and high noise, using the same percentile-based approach as for CER. In addition to WER classification, we also analyzed the distribution of three specific error types: insertion, deletion, and substitution errors. Each error type was categorized separately using a percentile-based approach. Table \ref{tab:ocr_error_statistics} presents the statistical distribution of OCR error metrics across languages.



\subsection{Human Evaluation}

To assess the quality of the \MultiOCR dataset, we also conducted a manual evaluation study. A total of 100 question-answer pairs were randomly selected for evaluation from each language. We asked native speakers for each language, to rate the questions. %and consistency in the evaluation. 
The evaluation followed a five-point rating scale (1 to 5), where 1 indicated very poor and 5 indicated very good. The assessment was based on four key criteria: (1) Question Readability, assessing grammatical correctness and fluency; (2) Answer Readability, ensuring coherence and grammatical correctness; (3) Relevance, verifying whether the generated question aligns with the passage; and (4) Answer correctness, confirming the accuracy and completeness of the answer. 

Each question was rated by an annotator, and the final scores for each criterion were averaged to obtain an overall evaluation. The results of the human evaluation are presented in Table \ref{tab:human_evaluation}. The evaluation showed high scores for both question and answer readability for the English and French language. The scores for relevance and answer correctness were moderate for these languages, indicating that most of the questions generated were contextually appropriate and that the answers provided were generally correct. But the results for German language were very low depicting that the CorrectedOCR text has a huge number of OCR errors so the errors are carried out further in the generated dataset. Such levels of errors are common when digitizing old historical texts due to the physical damage, irregular fonts, or faded ink of original document \cite{jatowt2019deep}.    


\begin{table}[t]
\centering
\caption{Human evaluation results of \MultiOCR dataset.}
\label{tab:human_evaluation}
\resizebox{0.95\columnwidth}{!}{
\begin{tabular}{ccccc}
\toprule
\textbf{Language} & \makecell{\textbf{Question} \\ \textbf{Readability}} 
                 & \makecell{\textbf{Answer} \\ \textbf{Readability}} 
                 & \textbf{Relevance} 
                 & \makecell{\textbf{Answer} \\ \textbf{Correctness}} \\
\midrule
\textbf{English} &4.13 &4.03 &3.33  &3.75\\
\midrule
\textbf{French}  &4.16 &4.14 &3.75  &3.30\\
\midrule
\textbf{German}  &2.80 &1.92 &1.64  &1.59\\
\bottomrule
\end{tabular}%
}
\end{table}

\section{Experiments and Results}

In this section, we conduct a comprehensive analysis of \MultiOCR from several perspectives. First, we evaluate the performance of \MultiOCR across various LLMs, comparing different model families and sizes to assess their effectiveness in handling OCR-generated text. Second, we investigate the impact of OCR errors on QA performance, focusing on three primary error types: insertion errors, deletion errors, and substitution errors. By analyzing these aspects, we aim to quantify the robustness of LLMs in processing noisy OCR text and provide insights into the challenges associated with question-answering on historical documents.

\subsection{Experimental Settings}
%We evaluate the impact of OCR errors on a QA system across three types of OCR errors: Insertion Error, Substitution Error, and Deletion Error, as well as the overall performance without focusing on any specific error type as a baseline. 
%As a baseline, 
%To evaluate \MultiOCR, w
We conducted experiments using multiple large language models (LLMs), including Qwen2.5 7B \cite{yang2024qwen2}, LLaMa 3.1-8B \cite{dubey2024llama}, Gemma-2-27B \cite{team2024gemma}, Mixtral 8×7B \cite{jiang2024mixtral}, LLaMA 3.3-70B \cite{dubey2024llama}, and Qwen2.5 72B \cite{yang2024qwen2}. These models span different architectures and parameter sizes, allowing for a comprehensive comparison on OCR text.

Traditionally, QA systems are evaluated using Exact Match (EM). However, these metrics can be insufficient for LLMs, as models often generate verbose responses, leading to low EM scores even when the correct answer is included in the response. To address this limitation, we evaluate \MultiOCR using \textbf{BERTScore} \cite{bertscore} alongside \textbf{EM}. Additionally, we introduce another evaluation metric, \textbf{Contains}, to better assess \MultiOCR performance. Contains measures the extent to which the ground truth is present in the response generated by the model, regardless of the verbosity. 
%By employing these metrics, we aim to provide a more comprehensive evaluation of LLMs ability to generate responses 
We will apply these metrics to evaluate the QA results on RawOCR texts and then on CorrectedOCR texts used as context.

\subsection{Experimental Results}
\label{experimental_results}
Tables \ref{tab:model_performance_LLM_English}, \ref{tab:model_performance_LLM_French}, and \ref{tab:model_performance_LLM_German} present the impact of OCR errors on the performance of various LLMs in question-answering tasks for English, French, and German texts. 

Table \ref{tab:model_performance_LLM_English} presents the performance of LLMs on English text using both CorrectedOCR (CP) and RawOCR (RP) paragraphs. Across all models, the transition from CP to RP negatively impacts performance. The best-performing model on CP is Gemma-2 27B with a BERTScore of 62.90, followed by LLaMA-3.3-70B at 60.71. When switching to RP, Gemma-2 still achieves the highest BERTScore 59.70, but with a 5.09\% drop, highlighting its robustness. The lowest-performing model is Mixtral 8×22B, showing the most significant impact of OCR errors. Qwen-2.5 72B model achieves the best performance in CP for the Contains metric of 59.95, indicating its strong retrieval ability in clean text. However, it experiences a 9.48\% drop in RP, suggesting moderate sensitivity to OCR errors. The lowest performing model in RP is again Mixtral 8×22B for Contains metric which sees an 11.26\% decrease, indicating its greater vulnerability to noisy text. The Exact Match (EM) metric shows the steepest decline across models, emphasizing that OCR errors severely impact the models' ability to generate precise answers.
 

Table \ref{tab:model_performance_LLM_French} reports the performance of LLMs on French text, again comparing CorrectedOCR and RawOCR paragraphs. The same as in English, the models perform better with CorrectedOCR, confirming and quantifying the negative influence of OCR errors on the QA accuracy. Gemma-2 27B model achieves the highest BERTScore of 76.51 on CP, demonstrating its strong ability to capture semantic similarity. Despite the 1.46\% drop, it still maintains the highest performance with a BERTScore of 75.39 on RP, indicating robustness to OCR noise. Mixtral 8×22B model shows the smallest drop of 1.05\%, but its overall score remains lower than the ones for the other models.  
For Contains, Qwen-2.5 72B achieves the highest score on CP 57.55, highlighting superior retrieval performance on clean text. However, it experiences a 19.90\% drop in RP, reinforcing its vulnerability to OCR errors. LLLaMA-3.1 8B also struggles, with a 19.63\% decline, showing difficulty in retrieving spans from noisy text. In terms of EM, Gemma-2 27B outperforms all models with an EM of 17.46 on CP. Although it drops by 15.23\%, it still maintains the best EM value 14.80 in RP. %In contrast, Qwen-2.5 72B experiences the largest EM drop 26.16\%, suggesting its high sensitivity to OCR distortions. 
Mixtral 8×22B model is the most affected in EM, dropping by 27.99\%, suggesting that OCR noise drastically reduces its accuracy in generating exact answers.



\begin{table}[t]
\caption{Performance of LLMs on English Language: Comparison Using CorrectedOCR Paragraphs (CP) and RawOCR Paragraphs (RP) as Context. Red numbers indicate the percentage decrease in performance with RP. Bold values highlight the highest performance for each metric with CP, while underlined values denote the best performance for each metric with RP.}
\label{tab:model_performance_LLM_English}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lclll}
\toprule
\textbf{Model} & \textbf{Parameter} & \textbf{BERTScore} & \textbf{Contains}  & \textbf{EM} \\
\midrule
Qwen-2.5 (CP) & 7B & 49.58  &  57.36 & 8.50 \\
Qwen-2.5 (RP) & 7B & 48.24 \textcolor{red}{ (2.70\%$\downarrow$) } & 48.67 \textcolor{red}{ (15.17\%$\downarrow$) }  & 7.17 \textcolor{red}{ (15.67\%$\downarrow$) } \\
\midrule
LlaMA-3.1 (CP) & 8B & 50.18 & 59.27  & 0.344 \\
LlaMA-3.1 (RP) & 8B & 08.42 \textcolor{red}{ (3.51\%$\downarrow$)}  & 52.03 \textcolor{red}{ (12.22\%$\downarrow$) }  & 0.266 \textcolor{red}{ (22.58\%$\downarrow$) } \\
\midrule
Gemma-2 (CP) & 27B & \textbf{62.90} & 53.16  & \textbf{19.05} \\
Gemma-2 (RP) & 27B & \underline{59.70} \textcolor{red}{ (5.09\%$\downarrow$) } & 49.16 \textcolor{red}{ (7.53\%$\downarrow$) }  & \underline{14.75} \textcolor{red}{ (22.57\%$\downarrow$) } \\
\midrule
Mixtral (CP) & 8x22B & 40.81 & 57.89  & 1.92 \\
Mixtral (RP) & 8x22B & 39.93 \textcolor{red}{ (2.16\%$\downarrow$) } & 51.37 \textcolor{red}{ (11.26\%$\downarrow$) }  & 1.58 \textcolor{red}{ (17.34\%$\downarrow$) } \\
\midrule
LlaMA-3.3 (CP) & 70B & 60.71 & 53.39  & 11.22 \\
LlaMA-3.3 (RP) & 70B & 59.06 \textcolor{red}{ (2.72\%$\downarrow$) }& 48.76 \textcolor{red}{ (8.67\%$\downarrow$) } & 9.15 \textcolor{red}{ (18.50\%$\downarrow$) } \\
\midrule
Qwen-2.5 (CP) & 72B & 49.64 & \textbf{59.95}  & 11.53 \\
Qwen-2.5 (RP) & 72B &47.41 \textcolor{red}{ (4.49\%$\downarrow$) } & \underline{54.26} \textcolor{red}{ (9.48\%$\downarrow$) }  & 8.79 \textcolor{red}{ (23.77\%$\downarrow$) } \\
\bottomrule
\end{tabular}
}
\end{table}




\begin{table}[t]
\caption{Performance of LLMs on French Language: Comparison Using CorrectedOCR Paragraphs (CP) and RawOCR Paragraphs (RP) as Context. Red numbers indicate the percentage decrease in performance with RP. Bold values highlight the highest performance for each metric with CP, while underlined values denote the best performance for each metric with RP.}
\label{tab:model_performance_LLM_French}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lclll}
\toprule
\textbf{Model} & \textbf{Parameter} & \textbf{BERTScore} & \textbf{Contains}  & \textbf{EM} \\
\midrule

Qwen-2.5 (CP) & 7B & 73.12 & 54.82 & 11.38 \\
Qwen-2.5 (RP) & 7B & 72.03 \textcolor{red}{ (1.49\%$\downarrow$) } & 42.99 \textcolor{red}{ (21.58\%$\downarrow$) } & 9.59 \textcolor{red}{ (15.75\%$\downarrow$) } \\
\midrule
LLaMA-3.1 (CP) &8B & 70.16 & 51.35 & 0.64 \\
LLaMA-3.1 (RP) & 8B & 69.30 \textcolor{red}{ (1.23\%$\downarrow$) } & 41.27 \textcolor{red}{ (19.63\%$\downarrow$) } & 0.61 \textcolor{red}{ (5.56\%$\downarrow$) } \\
\midrule
Gemma-2 (CP) & 27B & \textbf{76.51} & 52.58 & \textbf{17.46} \\
Gemma-2 (RP) & 27B & \underline{75.39} \textcolor{red}{ (1.46\%$\downarrow$) } & 42.05 \textcolor{red}{ (20.02\%$\downarrow$) } & \underline{14.80} \textcolor{red}{ (15.23\%$\downarrow$) } \\

\midrule
Mixtral (CP) & 8x22B & 68.65 & 48.32 & 0.30 \\
Mixtral (RP) & 8x22B & 67.93 \textcolor{red}{ (1.05\%$\downarrow$) } & 38.96 \textcolor{red}{ (19.37\%$\downarrow$) } & 0.21 \textcolor{red}{ (27.99\%$\downarrow$) } \\
\midrule
LLaMA-3.3 (CP) & 70B & 72.50 & 54.00 & 4.41 \\
LLaMA-3.3 (RP) & 70B & 71.42 \textcolor{red}{ (1.49\%$\downarrow$) } & 43.22 \textcolor{red}{ (19.96\%$\downarrow$) } & 3.89 \textcolor{red}{ (11.72\%$\downarrow$) } \\
\midrule
Qwen-2.5 (CP) & 72B & 73.26 & \textbf{57.55} & 10.62 \\
Qwen-2.5 (RP) & 72B & 71.98 \textcolor{red}{ (1.75\%$\downarrow$) } & \underline{46.10} \textcolor{red}{ (19.90\%$\downarrow$) } & 7.84 \textcolor{red}{ (26.16\%$\downarrow$) } \\
\bottomrule
\end{tabular}
}
\end{table}




\begin{table}[t]
\caption{Performance of LLMs on German Language: Comparison Using CorrectedOCR Paragraphs (CP) and RawOCR Paragraphs (RP) as Context. Red numbers indicate the percentage decrease in performance with RP. Bold values highlight the highest performance for each metric with CP, while underlined values denote the best performance for each metric with RP.}
\label{tab:model_performance_LLM_German}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lclll}
\toprule
\textbf{Model} & \textbf{Parameter} & \textbf{BERTScore} & \textbf{Contains}  & \textbf{EM} \\
\midrule
Qwen-2.5 (CP) & 7B & 62.76 & 15.88 & 0.389 \\
Qwen-2.5 (RP) & 7B & 59.52 \textcolor{red}{ (5.16\%$\downarrow$) } & 5.33 \textcolor{red}{ (66.40\%$\downarrow$) } & 0.192 \textcolor{red}{ (50.45\%$\downarrow$) } \\
\midrule
LLaMA-3.1 (CP) & 8B & 63.87 & 15.36 & 0.457 \\
LLaMA-3.1 (RP) & 8B & 58.29 \textcolor{red}{ (8.74\%$\downarrow$) } & 5.31 \textcolor{red}{ (65.37\%$\downarrow$) } & 0.135 \textcolor{red}{ (70.31\%$\downarrow$) } \\
\midrule
Gemma-2 (CP) & 27B & \textbf{67.07} & 11.56 & \textbf{2.691} \\
Gemma-2 (RP) & 27B & \underline{63.78} \textcolor{red}{ (4.91\%$\downarrow$) } & 4.47 \textcolor{red}{ (61.27\%$\downarrow$) } & \underline{0.564} \textcolor{red}{ (79.04\%$\downarrow$) } \\
\midrule
Mixtral (CP) & 8x22B & 60.87 & 11.23 & 0.078 \\
Mixtral (RP) & 8x22B & 58.14 \textcolor{red}{ (4.48\%$\downarrow$) } & 4.56 \textcolor{red}{ (59.37\%$\downarrow$) } & 0.021 \textcolor{red}{ (72.74\%$\downarrow$) } \\
\midrule

LLaMA-3.3 (CP) & 70B & 63.82 & \textbf{17.68} & 0.553 \\
LLaMA-3.3 (RP) & 70B & 58.44 \textcolor{red}{ (8.43\%$\downarrow$) } & 6.24 \textcolor{red}{ (64.72\%$\downarrow$) } & 0.203 \textcolor{red}{ (63.23\%$\downarrow$) } \\
\midrule
Qwen-2.5 (CP) & 72B & 63.25 & 16.43 & 0.699 \\
Qwen-2.5 (RP) & 72B & 60.04 \textcolor{red}{ (5.08\%$\downarrow$) } & \underline{6.50} \textcolor{red}{ (60.44\%$\downarrow$) } & 0.167 \textcolor{red}{ (76.02\%$\downarrow$) } \\
\bottomrule
\end{tabular}
}
\end{table}




\begin{figure*}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/line_plot_bertscore1.pdf}
    \caption{BERTScore of different error types  on Low, medium and High categories for each Language in \MultiOCR dataset. }
    \label{fig:f1_vs_error_type}
\end{figure*}



Table \ref{tab:model_performance_LLM_German} examines the performance of the model in the German text, revealing the most significant performance decline among the three languages. Unlike English and French, German exhibits the largest performance drop due to its lower OCR quality, and the models struggle more with its linguistic structure. 
Among the models evaluated, Gemma-2 27B achieves the highest BERTScore of 67.07 on CP, confirming its strong ability to capture semantic similarity in clean text. It also maintains the best performance on RP (63.78); however, it still experiences a 4.91\% decrease, highlighting the adverse effects of OCR errors.
In contrast, LLaMA-3.1 8B and LLaMA-3.3 70B show the biggest BERTScore drop (8.74\% and 8.43\% respectively), suggesting that these models struggle more with OCR noise. 
For the Contains metric, LLaMA-3.3 70B achieves the highest Contains score on CP, making it the most effective for retrieving relevant information in clean text. However, all models suffer a severe drop in Contains when moving to RP, with reductions exceeding 65\%. Qwen-2.5 7B has the worst drop 66.40\%, indicating that it faces challenges to retrieve information from noisy text. In terms of EM, Gemma-2 27B achieves the highest EM score on CP 2.69, while Mixtral 8×22B has the lowest EM 0.078. EM scores drop drastically across all models, the largest decrease is 79.04\% for Gemma-2, reinforcing that word-level distortions from OCR errors make exact answer matching nearly impossible. The Mixtral model drops 72.74\% in EM, making it highly unreliable for exact answers in noisy OCR text.

\textbf{Summary of findings:} OCR errors consistently degrade the performance of the models in English, French and German texts, resulting in maximum drop of 5.09\%, 1.75\%, and 8.74\% in BERTScore, respectively. %Across English, French, and German texts, OCR errors consistently degrade model performance, with drop of 5.09\%, 1.75\% and 8.74\% on BERTScore respectively. 
The most severe impact was observed in German due to the lower quality of the OCR and the complex linguistic structure. While larger models like Gemma-2 27B and LLaMA-3.3 70B demonstrate greater resilience, all models suffer substantial declines in Contains and Exact Match (EM) metrics, highlighting their weakness in retrieving and generating precise answers from noisy text. Gemma-2 27B consistently outperforms others, maintaining the highest BERTScore and EM across all languages, but still experiences notable degradation in noisy conditions. Mixtral 8×22B emerges as the most vulnerable, exhibiting the lowest absolute performance and struggling particularly with exact answer generation.



\subsection{Performance based on Different Error Types}
In this section, we conduct an in-depth analysis of the impact of different types of OCR errors. Insertion, deletion, and substitution on QA systems. We use Gemma-2 (27B) for this analysis, as it was found to consistently outperform the other models across English, French, and German. As detailed in Section \ref{error_quantification}, each error type is categorized into three levels: Low, Medium, and High, where Low represents minimal presence and High indicates the most frequent occurrence of a particular type of error. We evaluated \MultiOCR's performance across these categories, as illustrated in Figure \ref{fig:f1_vs_error_type}.

%Figure \ref{fig:f1_vs_error_type} demonstrates that while all three OCR error types degrade QA performance, their impact varies. Deletion errors have the most detrimental effect across all languages, causing the sharpest decline in BERTScore as their frequency increases. This suggests that character-level deletion significantly disrupt word meaning and sentence structure compared to insertions or substituion.

Insertion errors introduce extraneous characters or words, leading to moderate performance degradation. At low and medium insertion levels, the effect on BERTScore remains relatively minor, suggesting that small insertions do not always disrupt semantic meaning. However, at high insertion levels, performance drops sharply, indicating that excessive insertions impair both readability and semantic coherence.

Deletion errors impact sentence coherence and factual consistency, especially when they corrupt or remove key words or essential contextual phrases. Although the impact is less pronounced at lower levels, it escalates sharply as the frequency of deletions increases. At higher levels of deletion error, the degradation in BERTScore is similar to that seen with substitution errors, highlighting how missing characters or words disrupt structured text. %At high deletion levels, BERTScore degradation approaches that observed with substitution errors, reinforcing the disruptive nature of missing words in structured text.

Substitution errors exhibit the most severe impact on QA performance in English and French, causing the steepest decline in BERTScore as their frequency increases. Since these errors modify characters within words, they often alter word meaning and disrupt sentence structure, making them highly detrimental to text comprehension.

However, in German, substitution errors appear to be less disruptive than in English and French. This can be attributed to the compound word structure in German, where minor substitutions can still preserve some semantic similarity. In contrast, deletions or insertions tend to fragment meaningful lexical units, making them more impactful in German than in other languages.

\textbf{Summary of findings:} Across languages, the results reveal that English and French exhibit similar degradation patterns, with BERTScore progressively decreasing as the OCR error frequency increases. However, in German, the sharpest decline is observed across all error types, particularly for substitutions and deletions. This suggests that OCR noise in German is more detrimental, probably because the older scripts have content in old German where characters such as (long s) are used instead of "s", which can be often misread as "f" or "l".
%, so even minor errors can significantly alter meaning.

The results indicate that \textit{substitution errors are the most disruptive in English and French, while German is more affected by deletions} due to its compound word structure. \textit{Insertion errors generally cause moderate degradation}, but severe performance drops occur at high error levels. Overall, \textit{German experiences the highest performance drop}, reinforcing its greater vulnerability to OCR distortions and highlighting the need for effective OCR correction strategies. 

\subsection{Performance based on Pre-Processed and Post-Processed OCR Text}

In this section, we conduct an additional study to assess the impact of OCR on QA through pre-processing and post-processing Raw OCR text. In this experiment, we focus exclusively on paragraphs and their corresponding questions derived solely from the ICDAR dataset for the English language, comprising 83 paragraphs and 513 questions. We chose this subset of paragraphs and questions because its OCR ground truth is manually curated, offering a robust basis for comparison, unlike the CorrectedOCR text in the ChroniclingAmerica QA, which has been refined using GPT 3.5 \cite{brown2020language}. Our approach involves two strategies: pre-processing, where RawOCR text is corrected using the Gemma-2 27B model prior to answering the questions, and post-processing, where answers generated from RawOCR text are subsequently corrected. Gemma-2 27B was used in this experiment as it is identified as the best performing model among all other LLMs as demonstrated in Section \ref{experimental_results}. The results for each strategy are presented in Table \ref{tab:additional_study}.
   %For preprocessing the RawOCR text, we used the Gemma-2 27B model to correct the Raw OCR text. The corrected paragraphs were then used as context to answer the corresponding questions. Additionally, we postprocessed the answers by having the Gemma model respond to questions using the Raw OCR paragraphs as context and then correct any errors in the answers. The results for each approached is shown in the table \ref{tab:additional_study}.

\begin{table}[t]
\caption{Performance Metrics of QA Systems Using Pre-Processed and Post-Processed OCR Text. Red indicates the percentage drop in performance when  using different variants of paragraph used as context, compared to Corrected Paragraph (Ground Truth).}
\label{tab:additional_study}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{llll}
\toprule
\textbf{Approach}       & \textbf{BERTScore} & \textbf{Contains}  & \textbf{EM} \\
\midrule
Corrected Paragraph (Ground Truth)     &63.99 & 58.09 & 16.76 \\
\toprule
RawOCR Paragraph        &58.08 \textcolor{red}{(9.24\%$\downarrow$)}  &46.20 \textcolor{red}{(20.45\%$\downarrow$)}  & 9.36 \ \ \textcolor{red}{(44.15\%$\downarrow$)}  \\
LLM Corrected Paragraph &59.33 \textcolor{red}{(7.28\%$\downarrow$)}  &43.08 \textcolor{red}{(25.81\%$\downarrow$)}  & 12.28 \textcolor{red}{(26.73\%$\downarrow$)}   \\
RawOCR Corrected Answer &56.10 \textcolor{red}{(12.33\%$\downarrow$)} &41.33 \textcolor{red}{(28.86\%$\downarrow$)}  & 9.16 \ \ \textcolor{red}{(45.35\%$\downarrow$)}  \\

\bottomrule
\end{tabular}
}
\end{table}




The results in Table \ref{tab:additional_study} indicate that using ground truth paragraph as context gives better performance metrics, with a BERTScore of 63.99, Contains of 58.09, and EM of 16.76, compared to using RawOCR text or LLM corrected passage as context. The comparative analysis reveals that pre-processing the RawOCR text significantly enhances QA performance, as evidenced by higher BERTScore, Contains, and EM scores. This suggests that correcting OCR errors prior to QA processing preserves the semantic integrity of the text, thereby facilitating more accurate answer generation. However, it shows a percentage drop of 7. 28\% compared to the Ground Truth BERTScore.  In contrast, \emph{post-processing the generated answer is less effective}, resulting in the highest drop of 12.33\% and the lowest BertScore among others. This is likely because that approach does not include any contextual reference and leads to generation of corrected words that have limited relation to context. 

These findings emphasize the \emph{importance of integrating OCR error correction early in the QA pipeline} to improve the reliability of QA systems, especially when dealing with historical texts or other archival materials. However, given the huge collections of digitized content with vastly varying levels of OCR quality that the current memory institutions (archives, libraries, museums, etc.) held, the correction cost and effort would be enormous. It is also difficult to correct the queried texts at inference time as this would also introduce computational cost and latency in online systems. 
Therefore, more robust QA systems that are aware of OCR errors and capable of predicting correct answers based on contextual information are required.




% \section{Limitations and Use cases}

% The \MultiOCR dataset offers a unique resource for advancing research on OCR-aware QA and studying QA on noisy OCR text, making it useful in several ways:


% \begin{itemize}
%       \item It can be used to train LLMs to improve the error correction capabilities and enhancing robustness against OCR inaccuracies yet preserving the archaic language structure.
%       \item It can be used to expand LLMs multilingual processing abilities by training on multilingual OCR texts, improving accuracy in low resource languages.
% %     %\item While \MultiOCR includes English, French, and German, it does not cover a wider range of languages or scripts, such as Latin, Finish, Polish and many more. Future extensions could incorporate low-resource and non-Latin scripts to improve generalization.
    
% \end{itemize}
\section{Conclusion}

In this paper, we introduce \MultiOCR, a multilingual question-answering dataset for historical texts processed by OCR. Our dataset is unique as it is specifically designed to assess the impact of OCR errors on QA system performance, enabling a systematic analysis of how different error types—insertions, deletions, and substitutions- affect the accuracy and robustness of large language models. By incorporating both CorrectedOCR (clean text) and RawOCR (OCR-ed text), \MultiOCR facilitates comparative assessments that highlight the challenges posed by OCR noise in historical document processing. Our evaluation of multiple state-of-the-art LLMs demonstrates that OCR errors significantly degrade QA performance, with the most pronounced effects observed in languages with higher OCR error rates. Although larger models such as Gemma-2 27B and Qwen-2.5 72B exhibit greater resilience to OCR noise, smaller models and low-resource QA systems show a more substantial performance drop. These findings underscore the need for OCR-aware training strategies, particularly for historical document processing and multilingual QA tasks. 
%Thus future search should be focused 

\textbf{Use cases:} The \MultiOCR dataset offers a unique resource to advance research on OCR-aware QA and studying QA on noisy OCR text, making it useful in several ways; It can be used to train LLMs to improve error correction capabilities and enhancing robustness against OCR inaccuracies while preserving the archaic language structure. It can be also used to expand LLMs' multilingual processing abilities by training on multilingual OCR texts, improving accuracy in low-resourced languages.
% In future research, we will focus on the development of methodologies that effectively preserve the original language structure while simultaneously removing OCR errors.

\textbf{Limitations and Future work}: While \MultiOCR includes English, French and German, it does not encompass low resource languages or scripts such Latin, Finish and others. Future research should incorporate low-resourced languages to improve generalization and greater applicability across diverse languages. Additionally, methodologies that not only remove OCR errors but also preserve the original structure of documents could be applied. %Advancing OCR correction techniques with a strong emphasis on structural preservation will enhance the reliability of historical and multilingual document processing, ultimately improving access to valuable archival and scholarly resources.




%Our findings highlight that certain types of OCR errors have a more profound impact on answer retrieval and semantic similarity. Substitution errors, in particular, lead to the greatest performance degradation, while insertion and deletion errors contribute to information loss and reduced answerability. The dataset also provides insights into the multilingual challenges of OCR-based QA, showing that error patterns vary across languages, affecting retrieval accuracy differently.
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
