\section{Related Work}
\begin{table}[t]
\caption{
    Comparison across methods, including style adaptation during inference (w/ Style), head pose generation (w/ Pose), and real-time capability (Real-time).
    ARTalk (ours) is the only method to achieve all three features, demonstrating its comprehensive advantages over baseline methods.
}
\label{tab:comp_methods}
\vskip 0.5cm
\tablestyle{2pt}{1}
\begin{center}\begin{sc}
\scalebox{0.82}{\begin{tabular}{lccc}
\toprule
Method & w/ Style & w/ Pose & Real-time \\
\midrule
FaceFormer ____       & \rcross & \rcross & \rcross  \\
CodeTalker ____       & \rcross & \rcross & \rcross  \\
SelfTalk ____           & \rcross & \rcross & \gcheck  \\
FaceDiffuser ____   & \rcross & \rcross & \rcross  \\
MultiTalk ____         & \gcheck & \rcross & \rcross  \\
ScanTalk ____           & \rcross & \rcross & \rcross  \\
DiffPoseTalk ____   & \gcheck & \gcheck & \rcross  \\
% \midrule
ARTalk (Ours)                           & \gcheck & \gcheck & \gcheck \\
\bottomrule
\end{tabular}}
\end{sc}\end{center}
\vskip -0.1in
\end{table}


\begin{figure*}[t]
\vskip 0.5cm
\begin{center}
\centerline{\includegraphics[width=1.0\linewidth]{images/method.pdf}}
\vskip -0.2cm
\caption{
ARTalk involves two separated parts. (a) shows our temporal multi-scale VQ autoencoder. It encodes motion sequences into multi-scale token maps \([M_{k_1}, M_{k_2}, ..., M_{K}]\) using a shared codebook and causal masking on temporal. (b) shows The ARTalk Causal Transformer, where training uses ground truth tokens with a block-wise causal attention mask, and inference autoregressively predicts motion tokens conditioned on speech features and last scale tokens and last time window motions.
}
\label{fig:main_method}
\end{center}
\vskip -0.1cm
\end{figure*}


\subsection{Speech-Driven 3D Facial Animation}
Research on audio-driven 3D motion generation has been an area of interest for decades, with methods evolving significantly over time.
Early approaches ____ relied primarily on procedural methods, which segmented speech into phonemes and mapped them to predefined visemes through handcrafted rules.
Although procedural methods provide explicit control over the generated animations, they typically require complex parameter tuning and fail to capture the diversity and complexity of real-world speaking styles.
As a result, procedural methods struggled to deliver animations that appeared both natural and adaptable to varying speech dynamics.
In recent years, learning-based approaches ____ have advanced rapidly, addressing these limitations and enabling more natural and expressive facial animations.
Additionally, some approaches ____ focus on directly generating talking head videos without explicitly modeling motion.
However, this limits their ability to integrate with motion-driven downstream applications, restricting their broader applicability.

\subsection{Autoregressive 3D Facial Motion Generation}
Autoregressive (AR) methods model the temporal sequence of facial motion in a step-by-step manner.
These methods typically leverage pre-trained speech model features ____ as speech representations, which are subsequently mapped to 3D deformable model parameters or 3D meshes via neural networks.
For example, FaceFormer ____ autoregressively predicts the continuous facial motion parameters while MeshTalk ____ learns a prior over categorical expression tokens.
CodeTalker ____ and MultiTalk ____ tokenize facial motions with a VQ-VAE and train a transformer decoder to predict subsequent motion tokens based on audio features.
Learn2Talk ____ augments FaceFormer with lip-sync and lip-reading losses, improving alignment with speech.
MMHead ____ provides text descriptions as an extra condition for controlling the generated motion. 
However, these methods also show limitations.
Temporal autoregressive modeling often under represents motion within each time window, leading to overly smooth lip movements and failing to capture the complex speech-to-motion mapping.
This issue becomes more pronounced with larger and more diverse datasets.
Additionally, many approaches ____ rely on a predefined set of style labels to reduce the complexity of the mapping, which limits their ability to adapt to new individuals and styles.


\subsection{Diffusion based 3D Facial Motion Generation}
Diffusion models ____ have recently gained traction for generative tasks because of their strong modeling capacities.
FaceDiffuser ____ conditions a diffusion model on audio features to predict displacements from a neutral template.
More recently, FaceTalk ____ employs a diffusion model on expression coefficients with cross-attention to wav2vec2 ____ features, while DiffSpeaker ____ utilizes biased conditional attention for a diffusion-driven synthesis pipeline.
Scantalk ____ uses a DiffusionNet ____ structure to overcome the fixed topology limitation and perform diffusion on arbitrary meshes.
Similarly, DiffPoseTalk ____ uses a diffusion-based transformer decoder to generate both expression and pose (blendshapes) conditioned on audio and learned style features.
Going beyond motion, LetsTalk ____ adopts a spatiotemporal diffusion approach to synthesize entire video frames.
Although these methods generate high-fidelity and realistic facial motion, their iterative sampling steps can be computationally expensive and may limit real-time applicability.

In this paper, we leverage FLAME ____ as our motion representation and propose a novel autoregressive framework for speech-driven motion generation.
It not only outperform current diffusion models in generating facial motion, but it also achieves real-time speed and produce natural head poses.
A comparison of our method with existing approaches is shown in Table \ref{tab:comp_methods}.
Our method enables arbitrary style inference, allowing stylized motion generation at inference time by providing an example motion clip.
Additionally, our method generates both facial expressions and head poses while maintaining real-time performance.



% - list 2d methods
% ____: these are from KMTalk paper
% - 3d methods
%     - FLAME parameter based
%     - Mesh based
% - difference in audio features, raw audio features, ssl features (wav2vec2, hubert)
% - speaker dependent and speaker independent
% - modeling paradigm: feedforward, difffusion, autoregressive

% A broad class of data-driven methods tackle the problem of mapping audio to 3D facial expressions and head motion. These methods can be categorized in several categories depending on the facial motion representation, audio feature extraction, modeling paradigm, and speaker generalizability. 

% feedforward: VOCA, KMTalk, scantalk, sadtalker, selftalk, meshtalk, SAAS


% \paragraph{VOCA (CVPR 2019)}
% input: deepspeech features, template mesh, onehot speaker id
% output: mesh displacements from template
% speaker info: fixed to training set speakers due to using one-hot speaker id
% architecture: CNN + MLP
% method: predict the mesh displacements using the NN and trained with position and velocity losses

% \paragraph{MeshTalk (ICCV 2021) (AR)}
% input: template mesh, raw audio, expression signal
% output: animated mesh
% speaker info: not required, generalized
% architecture: 1d cnn audio encoder, MLP+LSTM expression encoder, MLP fusion, UNet decoder, PixelCNN-like AR prior
% method: train audio encoder + expression encoder + decoder with cross modality loss (output mesh reconstruction from audio and expression through a categorical latent bottleneck using gumbel softmax, which is concatedated in the middle of the unet decoder). Finally a autoregressive prior is learnt over the categorican latents from audio features.

% \paragraph{FaceFormer, CVPR 2022 (AR)}
% input: wav2vec2.0 features, facial motion (mesh?), one-hot speaker/style embedding
% output: facial motions (mesh?)
% speaker info: fixed to training set speakers due to using one-hot speaker id
% architecture: wav2vec2 encoder + learnable projection + transformer decoder with cross modal MH attention
% method: project and interpolate wav2vec2 features and combine with the transformer decoder using biased cross-modal MH attention. Decoder is trained autoregressively to predict the facial motions conditioned on the speech features.

% \paragraph{CodeTalker, CVPR 2023 (AR)}
% input: wav2vec2.0 features, tokenized facial motion with VQVAE, speaker/style embedding (unit vector, could be interpolated during inference)
% output: facial motion codes, then decoded by vqvae decoder
% speaker info: during training use unit vectors which can be interpolated to kind of generalize to new speakers
% architecture: wav2vec2 encoder + facial motion VQVAE + transformer decoder
% method: train VQVAE to tokenize facial motions, then train transformer decoder with wav2vec2 features as cross attention to predict the facial motion tokens autoregressively. Finally decoder of vqvae is used to reconstruct the output mesh

% \paragraph{SelfTalk, MM 2023 (NAR)}
% input: wav2vec2 features
% output: facial motion, mesh
% speaker info: speaker specific?
% architecture: transformer encoder as mesh decoder, and lip encoder/text decoder for training with text consistency loss CTC
% method: train the mesh decoder with consistency of ASR transcript and lip encoder/text decoder predictions from generated facial motion

% \paragraph{FaceDiffuser, SIGGRAPH MIG 2023 (Diffusion}
% input: hubert features, noise, one hot style embedding
% output: displacement from neutral template
% speaker info: speaker specific one hot
% architecture: MLP + GRU
% method: train diffusion model to predict displacement from neutral template mesh

% \paragraph{SadTalker (CVPR 2023) (NAR)}
% input: raw audio, 3dmm coefficients (exp and pose)
% output: 3dmm coeffs / rendered frames
% speaker info: generalized
% architecture: resnet audio encoder, Linear layer mapping, MLP based poseVAE
% method: map audio to expression, trained with pretrained wav2lip model and distilling features. learn residual pose (from first frame), conditioned on audio features trained with MSE and GAN

% \paragraph{MultiTalk, InterSpeech 2024 (AR)}
% same as codetalker, except they use a learnable language style embedding, and multilingual wav2vec2 (xlsr)

% \paragraph{ScanTalk, ECCV 2024 (NAR)}
% input: hubert features, neutral face and per-vertext surface features
% output: deformation/displacements from neutral mesh
% speaker info: speaker/dataset specific
% architecture: FC projection + BLSTM, DiffusionNet Encoder/Decoder (NOTE: this is not a diffusion model, but the name of the encoder/decoder is diffusion net)
% method: hubert feature passed through the blstm is combined with diffusionnet encoder features of neutral mesh, and fed to decoder for predicting displacements.

% \paragraph{DiffPoseTalk, SIGGRAPH 2024 (Diffusion)}
% input: hubert features, shape and style features
% output: motion parameters (FLAME expression + pose)
% speaker info: learnt from data using the style encoder
% architecture: transformer encoder fro style encoder, transformer decoder for diffusion model
% method: hubert features as conditioning for transformer decoder, which takes as input noisy motion features and shape feature and style feature extracted by style encoder (also trained together)

% \paragraph{Learn2Talk (TVCG 2024), (AR)}
% Extension of Faceformer, with additional lip sync loss and lip reading loss.

% \paragraph{MMHead (MM 2024), }
% Similar to CodeTalker, but with additional text description conditioning (describing expression, pose, action, scenario, etc.

% \paragraph{FaceTalk (CVPR 2024) (diffusion)}
% input: wav2vec2, noisy expressions
% output: clean expressions
% speaker info: not required, extracted from template mesh by disentangled NPHM (like FLAME)
% architecture: transformer decoder with cross attention from speech features, and diffusion timestep conditioning using FiLM MLP
% method: diffusion for expression features conditioned on wav2vec2 features

% \paragraph{LetsTalk (arXiv 2024), (diffusion)}
% input: portrait image, wav2vec features
% output: video frames
% speaker info: not required extracted from portrait image
% architecture: modified transformer with temporal attention, audio attention, spatial attention and Feedforward layer
% method: patchify and repeat portrait image  and combine with noisy frames for diffusion model training

% \paragraph{KMTalk (ECCV 2024) (NAR)}
% speaker-specific
% key motion prediction based on linguistic/phoneme based localization and key motion decoding. Followed by audio conditioned motion infilling/completion based on the initial key motion predictions. Transformer based models.

% \paragraph{SAAS (AAAI 2024),  (NAR)}
% sepaker-agnostic
% 3DMM based, either video-driven or audio-driven. extract style feature from reference video, combine with source video/audio motions and random frame 3DMM expression features and decode stylized expressions.

% \paragraph{DiffSpeaker (arXiv 2024) (diffusion)}
% speaker specific
% diffusion model (transformer decoder with audio (wav2vec2 or hubert) cross-attention. requires one hot speaker/style id.


% \subsection{Diffusion for Speech-driven Motion Generation}

% \subsection{Autoregressive Model for Motion Generation}