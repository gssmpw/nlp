\begin{abstract}
Speech-driven 3D facial animation aims to generate realistic lip movements and facial expressions for 3D head models from arbitrary audio clips.
Although existing diffusion-based methods are capable of producing natural motions, their slow generation speed limits their application potential.
In this paper, we introduce a novel autoregressive model that achieves real-time generation of highly synchronized lip movements and realistic head poses and eye blinks by learning a mapping from speech to a multi-scale motion codebook.
Furthermore, our model can adapt to unseen speaking styles using sample motion sequences, enabling the creation of 3D talking avatars with unique personal styles beyond the identities seen during training. 
Extensive evaluations and user studies demonstrate that our method outperforms existing approaches in lip synchronization accuracy and perceived quality.
Demos and codes are available at \href{https://xg-chu.site/project_artalk/}{https://xg-chu.site/project\_artalk/}.
\end{abstract}



\section{Introduction}
\label{Intro}
\begin{figure}[ht]
\vskip 0.6cm
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{images/teaser.pdf}}
\vskip -0.1cm
\caption{
We present ARTalk, a framework for speech-driven 3D facial motion generation.
Our method learns a mapping from speech to a multi-scale motion codebook, enabling the real-time generation of realistic and diverse animation sequences.
}
\label{fig:teaser}
\end{center}
\vskip -0.1cm
\end{figure}

Speech-driven 3D facial animation has attracted significant attention in both academia and industry due to its broad applications in virtual reality, game animation, film production, and human-computer interaction.
This task focuses on generating natural and synchronized facial expressions, particularly lip movements, along with realistic head motions directly from speech input. 
In recent years, many exploratory methods have significantly enhanced the performance of speech-driven motion generation tasks.
However, generating natural and human-like motions remains challenging due to the inherently complex many-to-many mapping between speech and facial or head movements.
Capturing these intricate relationships is crucial for creating realistic animations.

Some existing methods, such as \citep{richard2021meshtalk, codetalker2023, multitalk2024, scantalk2024}, employ autoregressive models to predict deformation offsets on meshes for speech-driven motion generation.
However, due to the high complexity of meshes in 3D space, these approaches often fail to fully capture the many-to-many relationships between speech and motion.
This limitation frequently results in overly smooth non-lip facial expressions that lack realism.
Recent works leveraging diffusion models \citep{ho2020denoising, tevet2023human, facediffuser2023, diffposetalk2024} have demonstrated impressive results in generative tasks, including speech-driven motion.
For instance, FaceDiffuser \citep{facediffuser2023} employs diffusion to generate motions directly on meshes, while DiffPoseTalk \citep{diffposetalk2024} generates facial expressions and head motions using diffusion on FLAME \citep{FLAME2017} blendshapes, achieving natural results.
However, these methods involve computationally expensive diffusion processes and cannot perform real-time and low-latency motion generation.
Meanwhile, multi-scale autoregressive models \citep{var2024} have shown success in image autoregressive generation tasks, offering fast and high-quality results.
However, these models are designed for single multi-scale samples instead of temporal motions, and thus cannot be directly applied to speech-to-motion generation tasks.

To address the challenges of achieving high-quality, natural, and fast speech-driven motion generation while overcoming these limitations, we propose a novel method based on sliding temporal windows and multi-scale autoregression.
Our method divides speech into time windows for generation, which is important for low-latency action generation.
We extend the VQ autoencoder to encode multi-scale motion codes across two consecutive time windows, enabling effective handling of temporal sequence data. 
Additionally, we design a novel autoregressive generator that conditionally generates multi-scale motion codes based on the speech input of the current time window and previously generated motions.
This design allows our autoregressive model to perform high-quality multi-scale generation within each time window while maintaining temporal continuity across windows.
Furthermore, we integrated a transformer-based style encoder to extract style codes from sample motion segments, enabling our model to generate stylized motions that reflect unique individual characteristics while reducing the complexity of the many-to-many mapping between speech and motion.
These innovations enable our method to achieve personalized and high-fidelity lip motions, facial expressions and head motion generation, making it suitable for a wide range of speech-driven downstream tasks.

Our main contributions are as follows:
\begin{itemize}[itemsep=5pt, parsep=0pt, topsep=0pt]
\item We propose ARTalk, a novel autoregressive framework capable of generating natural 3D facial motions with head poses in real time.
\item We design a novel encoder-decoder that encode motions from consecutive time windows to produce temporally dependent multi-scale motion representations.
\item We introduce a novel conditional autoregressive generator on temporal and feature scales, enabling motion generations tightly aligned with speech conditions and consistent across time windows.
\end{itemize}



\section{Related Work}
\begin{table}[t]
\caption{
    Comparison across methods, including style adaptation during inference (w/ Style), head pose generation (w/ Pose), and real-time capability (Real-time).
    ARTalk (ours) is the only method to achieve all three features, demonstrating its comprehensive advantages over baseline methods.
}
\label{tab:comp_methods}
\vskip 0.5cm
\tablestyle{2pt}{1}
\begin{center}\begin{sc}
\scalebox{0.82}{\begin{tabular}{lccc}
\toprule
Method & w/ Style & w/ Pose & Real-time \\
\midrule
FaceFormer \citep{faceformer2022}       & \rcross & \rcross & \rcross  \\
CodeTalker \citep{codetalker2023}       & \rcross & \rcross & \rcross  \\
SelfTalk \citep{selftalk2023}           & \rcross & \rcross & \gcheck  \\
FaceDiffuser \citep{facediffuser2023}   & \rcross & \rcross & \rcross  \\
MultiTalk \citep{multitalk2024}         & \gcheck & \rcross & \rcross  \\
ScanTalk \citep{scantalk2024}           & \rcross & \rcross & \rcross  \\
DiffPoseTalk \citep{diffposetalk2024}   & \gcheck & \gcheck & \rcross  \\
% \midrule
ARTalk (Ours)                           & \gcheck & \gcheck & \gcheck \\
\bottomrule
\end{tabular}}
\end{sc}\end{center}
\vskip -0.1in
\end{table}


\begin{figure*}[t]
\vskip 0.5cm
\begin{center}
\centerline{\includegraphics[width=1.0\linewidth]{images/method.pdf}}
\vskip -0.2cm
\caption{
ARTalk involves two separated parts. (a) shows our temporal multi-scale VQ autoencoder. It encodes motion sequences into multi-scale token maps \([M_{k_1}, M_{k_2}, ..., M_{K}]\) using a shared codebook and causal masking on temporal. (b) shows The ARTalk Causal Transformer, where training uses ground truth tokens with a block-wise causal attention mask, and inference autoregressively predicts motion tokens conditioned on speech features and last scale tokens and last time window motions.
}
\label{fig:main_method}
\end{center}
\vskip -0.1cm
\end{figure*}


\subsection{Speech-Driven 3D Facial Animation}
Research on audio-driven 3D motion generation has been an area of interest for decades, with methods evolving significantly over time.
Early approaches \citep{taylor2012dynamic, xu2013practical,edwards2016jali} relied primarily on procedural methods, which segmented speech into phonemes and mapped them to predefined visemes through handcrafted rules.
Although procedural methods provide explicit control over the generated animations, they typically require complex parameter tuning and fail to capture the diversity and complexity of real-world speaking styles.
As a result, procedural methods struggled to deliver animations that appeared both natural and adaptable to varying speech dynamics.
In recent years, learning-based approaches \citep{faceformer2022, codetalker2023, diffposetalk2024, danvevcek2023emotional, emotalk2023, yang2024probabilistic} have advanced rapidly, addressing these limitations and enabling more natural and expressive facial animations.
Additionally, some approaches \citep{yi2022predicting, ye2024real3d, zhang2023sadtalker, tan2024say} focus on directly generating talking head videos without explicitly modeling motion.
However, this limits their ability to integrate with motion-driven downstream applications, restricting their broader applicability.

\subsection{Autoregressive 3D Facial Motion Generation}
Autoregressive (AR) methods model the temporal sequence of facial motion in a step-by-step manner.
These methods typically leverage pre-trained speech model features \citep{baevski2020wav2vec, hsu2021hubert, kyutai2024moshi} as speech representations, which are subsequently mapped to 3D deformable model parameters or 3D meshes via neural networks.
For example, FaceFormer \citep{faceformer2022} autoregressively predicts the continuous facial motion parameters while MeshTalk \citep{richard2021meshtalk} learns a prior over categorical expression tokens.
CodeTalker \citep{codetalker2023} and MultiTalk \citep{multitalk2024} tokenize facial motions with a VQ-VAE and train a transformer decoder to predict subsequent motion tokens based on audio features.
Learn2Talk \citep{Zhuang2024learn2talk} augments FaceFormer with lip-sync and lip-reading losses, improving alignment with speech.
MMHead \citep{wu2024mmhead} provides text descriptions as an extra condition for controlling the generated motion. 
However, these methods also show limitations.
Temporal autoregressive modeling often under represents motion within each time window, leading to overly smooth lip movements and failing to capture the complex speech-to-motion mapping.
This issue becomes more pronounced with larger and more diverse datasets.
Additionally, many approaches \citep{faceformer2022, codetalker2023, multitalk2024} rely on a predefined set of style labels to reduce the complexity of the mapping, which limits their ability to adapt to new individuals and styles.


\subsection{Diffusion based 3D Facial Motion Generation}
Diffusion models \citep{ho2020denoising, zhu2023taming, alexanderson2023listen, facediffuser2023, ma2024diffspeaker, diffposetalk2024} have recently gained traction for generative tasks because of their strong modeling capacities.
FaceDiffuser \citep{facediffuser2023} conditions a diffusion model on audio features to predict displacements from a neutral template.
More recently, FaceTalk \citep{aneja2023facetalk} employs a diffusion model on expression coefficients with cross-attention to wav2vec2 \citep{baevski2020wav2vec} features, while DiffSpeaker \citep{ma2024diffspeaker} utilizes biased conditional attention for a diffusion-driven synthesis pipeline.
Scantalk \citep{scantalk2024} uses a DiffusionNet \citep{sharp2022diffusionnet} structure to overcome the fixed topology limitation and perform diffusion on arbitrary meshes.
Similarly, DiffPoseTalk \citep{diffposetalk2024} uses a diffusion-based transformer decoder to generate both expression and pose (blendshapes) conditioned on audio and learned style features.
Going beyond motion, LetsTalk \citep{zhang2024letstalk} adopts a spatiotemporal diffusion approach to synthesize entire video frames.
Although these methods generate high-fidelity and realistic facial motion, their iterative sampling steps can be computationally expensive and may limit real-time applicability.

In this paper, we leverage FLAME \citep{FLAME2017} as our motion representation and propose a novel autoregressive framework for speech-driven motion generation.
It not only outperform current diffusion models in generating facial motion, but it also achieves real-time speed and produce natural head poses.
A comparison of our method with existing approaches is shown in Table \ref{tab:comp_methods}.
Our method enables arbitrary style inference, allowing stylized motion generation at inference time by providing an example motion clip.
Additionally, our method generates both facial expressions and head poses while maintaining real-time performance.



% - list 2d methods
% \citep{alghamdi2022talking, chen2020talking, chung2017out, das2020speech, guo2021ad, ji2022eamm, ji2021audio, liang2022expressive, liu2022semantic, pang2023dpe, prajwal2020lip, shen2022learning, vougioukas2020realistic, wang2022one, yi2020audio, zhang2023metaportrait, zhou2021pose}: these are from KMTalk paper
% - 3d methods
%     - FLAME parameter based
%     - Mesh based
% - difference in audio features, raw audio features, ssl features (wav2vec2, hubert)
% - speaker dependent and speaker independent
% - modeling paradigm: feedforward, difffusion, autoregressive

% A broad class of data-driven methods tackle the problem of mapping audio to 3D facial expressions and head motion. These methods can be categorized in several categories depending on the facial motion representation, audio feature extraction, modeling paradigm, and speaker generalizability. 

% feedforward: VOCA, KMTalk, scantalk, sadtalker, selftalk, meshtalk, SAAS


% \paragraph{VOCA (CVPR 2019)}
% input: deepspeech features, template mesh, onehot speaker id
% output: mesh displacements from template
% speaker info: fixed to training set speakers due to using one-hot speaker id
% architecture: CNN + MLP
% method: predict the mesh displacements using the NN and trained with position and velocity losses

% \paragraph{MeshTalk (ICCV 2021) (AR)}
% input: template mesh, raw audio, expression signal
% output: animated mesh
% speaker info: not required, generalized
% architecture: 1d cnn audio encoder, MLP+LSTM expression encoder, MLP fusion, UNet decoder, PixelCNN-like AR prior
% method: train audio encoder + expression encoder + decoder with cross modality loss (output mesh reconstruction from audio and expression through a categorical latent bottleneck using gumbel softmax, which is concatedated in the middle of the unet decoder). Finally a autoregressive prior is learnt over the categorican latents from audio features.

% \paragraph{FaceFormer, CVPR 2022 (AR)}
% input: wav2vec2.0 features, facial motion (mesh?), one-hot speaker/style embedding
% output: facial motions (mesh?)
% speaker info: fixed to training set speakers due to using one-hot speaker id
% architecture: wav2vec2 encoder + learnable projection + transformer decoder with cross modal MH attention
% method: project and interpolate wav2vec2 features and combine with the transformer decoder using biased cross-modal MH attention. Decoder is trained autoregressively to predict the facial motions conditioned on the speech features.

% \paragraph{CodeTalker, CVPR 2023 (AR)}
% input: wav2vec2.0 features, tokenized facial motion with VQVAE, speaker/style embedding (unit vector, could be interpolated during inference)
% output: facial motion codes, then decoded by vqvae decoder
% speaker info: during training use unit vectors which can be interpolated to kind of generalize to new speakers
% architecture: wav2vec2 encoder + facial motion VQVAE + transformer decoder
% method: train VQVAE to tokenize facial motions, then train transformer decoder with wav2vec2 features as cross attention to predict the facial motion tokens autoregressively. Finally decoder of vqvae is used to reconstruct the output mesh

% \paragraph{SelfTalk, MM 2023 (NAR)}
% input: wav2vec2 features
% output: facial motion, mesh
% speaker info: speaker specific?
% architecture: transformer encoder as mesh decoder, and lip encoder/text decoder for training with text consistency loss CTC
% method: train the mesh decoder with consistency of ASR transcript and lip encoder/text decoder predictions from generated facial motion

% \paragraph{FaceDiffuser, SIGGRAPH MIG 2023 (Diffusion}
% input: hubert features, noise, one hot style embedding
% output: displacement from neutral template
% speaker info: speaker specific one hot
% architecture: MLP + GRU
% method: train diffusion model to predict displacement from neutral template mesh

% \paragraph{SadTalker (CVPR 2023) (NAR)}
% input: raw audio, 3dmm coefficients (exp and pose)
% output: 3dmm coeffs / rendered frames
% speaker info: generalized
% architecture: resnet audio encoder, Linear layer mapping, MLP based poseVAE
% method: map audio to expression, trained with pretrained wav2lip model and distilling features. learn residual pose (from first frame), conditioned on audio features trained with MSE and GAN

% \paragraph{MultiTalk, InterSpeech 2024 (AR)}
% same as codetalker, except they use a learnable language style embedding, and multilingual wav2vec2 (xlsr)

% \paragraph{ScanTalk, ECCV 2024 (NAR)}
% input: hubert features, neutral face and per-vertext surface features
% output: deformation/displacements from neutral mesh
% speaker info: speaker/dataset specific
% architecture: FC projection + BLSTM, DiffusionNet Encoder/Decoder (NOTE: this is not a diffusion model, but the name of the encoder/decoder is diffusion net)
% method: hubert feature passed through the blstm is combined with diffusionnet encoder features of neutral mesh, and fed to decoder for predicting displacements.

% \paragraph{DiffPoseTalk, SIGGRAPH 2024 (Diffusion)}
% input: hubert features, shape and style features
% output: motion parameters (FLAME expression + pose)
% speaker info: learnt from data using the style encoder
% architecture: transformer encoder fro style encoder, transformer decoder for diffusion model
% method: hubert features as conditioning for transformer decoder, which takes as input noisy motion features and shape feature and style feature extracted by style encoder (also trained together)

% \paragraph{Learn2Talk (TVCG 2024), (AR)}
% Extension of Faceformer, with additional lip sync loss and lip reading loss.

% \paragraph{MMHead (MM 2024), }
% Similar to CodeTalker, but with additional text description conditioning (describing expression, pose, action, scenario, etc.

% \paragraph{FaceTalk (CVPR 2024) (diffusion)}
% input: wav2vec2, noisy expressions
% output: clean expressions
% speaker info: not required, extracted from template mesh by disentangled NPHM (like FLAME)
% architecture: transformer decoder with cross attention from speech features, and diffusion timestep conditioning using FiLM MLP
% method: diffusion for expression features conditioned on wav2vec2 features

% \paragraph{LetsTalk (arXiv 2024), (diffusion)}
% input: portrait image, wav2vec features
% output: video frames
% speaker info: not required extracted from portrait image
% architecture: modified transformer with temporal attention, audio attention, spatial attention and Feedforward layer
% method: patchify and repeat portrait image  and combine with noisy frames for diffusion model training

% \paragraph{KMTalk (ECCV 2024) (NAR)}
% speaker-specific
% key motion prediction based on linguistic/phoneme based localization and key motion decoding. Followed by audio conditioned motion infilling/completion based on the initial key motion predictions. Transformer based models.

% \paragraph{SAAS (AAAI 2024),  (NAR)}
% sepaker-agnostic
% 3DMM based, either video-driven or audio-driven. extract style feature from reference video, combine with source video/audio motions and random frame 3DMM expression features and decode stylized expressions.

% \paragraph{DiffSpeaker (arXiv 2024) (diffusion)}
% speaker specific
% diffusion model (transformer decoder with audio (wav2vec2 or hubert) cross-attention. requires one hot speaker/style id.


% \subsection{Diffusion for Speech-driven Motion Generation}

% \subsection{Autoregressive Model for Motion Generation}

\section{Method}

We provide an overview of our method in Figure \ref{fig:main_method}.
We adopt the widely used 3DMM \citep{FLAME2017} as the facial representation and leverage a multi-scale VQ autoencoder model to train and obtain a multi-scale motion codebook along with its corresponding encoder-decoder, enabling a discrete representation of the motion space.
Subsequently, we train a multi-scale autoregressive model to map speech information to the discrete motion space.

In the following subsections, Section \ref{sec:31} details the problem definition and the 3DMM representation, Section \ref{sec:32} explains the multi-scale motion VQ autoencoder model, Section \ref{sec:33} introduces the multi-scale autoregressive model.
% and Section \ref{sec:34} describes the training objectives and inference process.

% \subsection{Preliminaries}
% \label{sec:31}

% We adopt the widely used 3D morphable model (3DMM) FLAME \citep{FLAME2017} to represent the facial motion of each frame.
% The FLAME geometry consists of 5,023 vertices and can be parameterized by shape parameters $\beta$, expression parameters $\psi$, and pose parameters $\theta$.

% Given the parameters $\{\beta, \psi, \theta\}$, 3D meshes can be reconstructed through blend shape and rotation operations.
% Compared to previous methods \citep{faceformer2022, codetalker2023, selftalk2023, scantalk2024} that directly learn on the mesh, FLAME blend shapes convert complex vertices in 3D space into 1D blend shape parameters, which retains rich expression and motion information while greatly reducing the difficulty and complexity of modeling.
% For more details on 3DMM, please refer to the supplementary material.

% \subsection{Temporal Multi-scale VQ Autoencoder}
% \label{sec:32}
% Although the task of predicting motion frames from speech is inherently discrete, it involves highly temporally dense sequences and diverse mapping relationships.
% Inspired by the success of existing methods \citep{codetalker2023, neuraldiscrete2017, zhou2022codeformer} in image and motion generation tasks and the effectiveness of the VAR model \citep{var2024} in multi-scale encoding and generation, we propose a novel temporal multi-scale VQ autoencoder.
% Our encoder can encode and decode temporally continuous motion sequences with both high fidelity and strong detail representation.

% Figure \ref{fig:main_method} (a) illustrates the overall architecture of our temporal multi-scale VQ autoencoder.
% To learn discrete motion features, we introduce multi-scale quantized codebook to store motion information at varying temporal resolutions. Specifically, the autoencoder processes an input motion sequence of $K$ frames, encoding it into feature maps at multiple scales $[k_1, k_2, ..., K]$, where each scale represents motion features at a different temporal resolution.
% These features are then quantized to produce discrete representations.

% During the encoding and decoding process, residual accumulation is performed at the maximum scale $K$ to minimize information loss.
% To support this, fully connected layers are added during the upsampling process to restore lost details, while downsampling is achieved using region-based interpolation to aggregate features efficiently.
% Furthermore, we employ a shared codebook across all temporal scales to unify the discrete motion space and reduce the complexity of the subsequent autoregressive modeling.
% Our experiments show that this multi-scale quantization and residual accumulation strategy not only ensures temporal consistency in generated motions but also enhances stylistic uniformity and detail richness at different temporal resolutions.

% Since motion generation is inherently a temporal task, generating motions within a single time window is insufficient to ensure the continuity and stability of longer motion sequences. To address this challenge, we propose a temporal causal reasoning mechanism to enable cross-time window encoding and decoding.
% Specifically, our encoding and quantization process considers two consecutive time windows, $T\text{-}1$ and $T$, rather than a single window.
% During training and inference, causal masks are applied to ensure that the generation process adheres to temporal causality: the generation of the current time window $T$ depends on the encoded results of the previous window $T\text{-}1$, while the actions in $T\text{-}1$ remain independent of subsequent actions.
% This design not only ensures rich details within single time windows but also improves temporal continuity through cross-window modeling.

% By combining this temporal multi-scale quantization framework with the temporal causal reasoning mechanism, our method achieves consistent modeling from fine-grained details to long-term sequences, significantly improving the stability, realism, and detail fidelity of motion generation tasks.


% \noindent\textbf{Training objectives.}
% During the training of the VQ autoencoder, we adopted a hybrid loss function to comprehensively improve the accuracy of expression and motion reconstruction as well as temporal consistency.
% First, we employed an $L_1$ loss to constrain the expression and motion vectors, ensuring an overall alignment between the predicted samples and the ground truth:
% \begin{align}
% \label{eq:vqloss1}
% L_{recon} = |\hat{M}-M|+w_{lips}||\hat{V}-V||^2.
% \end{align}
% Second, we introduced $L_2$ distances on the lip vertices and the entire facial vertices of the generated mesh to further refine the accuracy of expression reconstruction, ensuring that key regions, such as the lips, closely match the ground truth:

% Finally, to enhance the temporal smoothness of the generated sequences, we applied temporal smoothness constraints on both mesh vertices and head poses, penalizing large acceleration changes in expressions and head movements to reduce unnatural jitter and ensure continuity in the generated results:
% \begin{align}
% \label{eq:vqloss2}
% L_{vel} &= ||(\hat{V}_{[1:]}-\hat{V}_{[:\text{-}1]})-(V_{[1:]}-V_{[:\text{-}1]})||^2, \\
% L_{smooth} &= ||\hat{V}_{[2:]}-2\hat{V}_{[1:\text{-}1]}+\hat{V}_{[:\text{-}2]}||^2.
% \end{align}
% The overall training target is:
% \begin{align}
% \label{eq:vqloss2}
% L_{VQ} = L_{recon} + \lambda_{vel}L_{vel} + \lambda_{smooth}L_{smooth}.
% \end{align}

% Sec 3.1 and 3.2 rewritten by Nabarun
\subsection{Preliminaries}
\label{sec:31}

We adopt the widely used 3D morphable model (3DMM) FLAME \citep{FLAME2017} to represent facial motion, where each frame is parameterized by shape \(\beta\), expression \(\psi\), and pose \(\theta\). Given these parameters, the 3D face mesh, consisting of 5,023 vertices, is reconstructed through blend shape and rotation operations. Compared to previous methods \citep{faceformer2022, codetalker2023, selftalk2023, scantalk2024} that directly model the mesh, FLAME simplifies motion modeling by converting complex vertex movements into lower-dimensional blend shape parameters while preserving expression and motion details.

We define the motion vector \(\mathbf{M} \in \mathbb{R}^{K \times D}\) as the concatenation of per-frame expression and pose parameters over \(K\) frames, i.e., \(\mathbf{M} = [\psi \;\; \theta]\). The corresponding vertex coordinates \(\mathbf{V} \in \mathbb{R}^{K \times N \times 3}\) store the 3D positions of \(N\) mesh vertices per frame, where \(\mathbf{V}_{\mathrm{lips}} \subset \mathbf{V}\) denotes the subset corresponding to the lip region, which is particularly important for speech related expression modeling. Further details on 3DMM can be found in the supplementary material.


\subsection{Temporal Multi-scale VQ Autoencoder}
\label{sec:32}

Predicting motion frames from speech is a discrete task involving highly dense temporal sequences and complex mappings. Inspired by the success of discrete representations in image and motion generation \citep{codetalker2023, neuraldiscrete2017, zhou2022codeformer} and the effectiveness of multi-scale encoding in \citep{var2024}, we propose a temporal multi-scale VQ autoencoder to efficiently model motion dynamics.

The input motion sequence, consisting of \(K\) frames, is first processed by a Transformer encoder, which extracts temporal features and maps them into a latent space. These representations are then quantized using a multi-scale codebook that captures motion information at varying temporal resolutions \(\bigl[k_1, k_2, \ldots, K\bigr]\), producing discrete tokens. The Transformer decoder reconstructs the motion from these discrete embeddings, attending to both local and long-range dependencies.

\noindent\textbf{Multi-scale residual VQ.}
To progressively refine motion representations, we adopt a residual vector quantization approach:
\begin{equation}
\label{eq:multi_scale_residual}
\begin{aligned}
    \mathbf{h}^{(l)} & = \mathrm{Interp}\bigl(\mathrm{Quant}(\mathbf{r}^{(l-1)}),\,k_l\bigr), \\
    \mathbf{r}^{(l)} & = \mathbf{r}^{(l-1)} - \mathbf{h}^{(l)},
\end{aligned}
\end{equation}
for \(l=1,\ldots,L\), where \(\mathbf{r}^{(0)}\) is the encoder output. The function \(\mathrm{Quant}(\cdot)\) assigns features to the closest codebook entries, while \(\mathrm{Interp}(\cdot,k_l)\) adjusts resolutions to ensure smooth transitions. By accumulating the outputs \(\mathbf{h}^{(l)}\) across scales, the model retains high fidelity while removing redundancy in a structured manner.

Residual accumulation at the finest scale \(K\) mitigates information loss, while downsampling via region-based interpolation enhances efficiency. We use a \emph{shared} codebook across scales to maintain a unified motion space and simplify autoregressive modeling. This strategy improves temporal consistency and preserves stylistic coherence across different resolutions.

\noindent\textbf{Temporal causal reasoning.}
To ensure stability over extended sequences, we incorporate a causal reasoning mechanism that enforces cross-window consistency. Instead of generating frames independently, our approach considers two consecutive time windows, \(T-1\) and \(T\). During training and inference, causal masks are applied, ensuring that predictions for \(T\) depend only on previously observed information from \(T-1\), without leaking future details. This prevents temporal discontinuities and enhances the smoothness of long-term motion.

By integrating multi-scale residual quantization with causal reasoning and Transformer-based encoding/decoding, our method ensures both fine-grained expressiveness and long-term coherence in generated motion sequences.

\noindent\textbf{Training objectives.}

To train the VQ autoencoder, we use a hybrid loss function balancing motion accuracy, temporal smoothness, and codebook stability.

The reconstruction loss enforces alignment between predicted motion vectors \(M'\) and the ground truth \(M\), with additional constraints on lip and facial vertices:
\begin{align}
\label{eq:vqloss1}
L_{\text{recon}} = \| \hat{M} - M \|_1 + w_{\text{lips}} \| \hat{V}_{\text{lips}} - V_{\text{lips}} \|^2 + \| \hat{V} - V \|^2
\end{align}
To ensure smooth transitions, we penalize velocity and acceleration differences:
\begin{align}
\label{eq:vqloss2}
L_{\text{vel}} &= \| (\hat{V}_{1:} - \hat{V}_{:-1}) - (V_{1:} - V_{:-1}) \|^2, \\
L_{\text{smooth}} &= \| \hat{V}_{2:} - 2\hat{V}_{1:-1} + \hat{V}_{:-2} \|^2.
\end{align}
We also apply the standard VQ losses which we term as \(L_{\text{cb}}\) to encourage stable VQ assignments and prevent mode collapse.

The final training objective is:
\begin{align}
\label{eq:vqloss_all}
L_{\text{VQ}} = L_{\text{recon}} + \lambda_{\text{vel}} L_{\text{vel}} + \lambda_{\text{smooth}} L_{\text{smooth}} + L_{\text{cb}},
\end{align}
where \(\lambda_{\text{vel}}\) and \(\lambda_{\text{smooth}}\) control the temporal constraints.

By jointly optimizing these terms, the model learns compact yet expressive motion representations, ensuring high-fidelity synthesis with strong temporal coherence.

\subsection{Speech-to-Motion Autoregressive Model}
\label{sec:33}

After training the VQ autoencoder (Section~\ref{sec:32}), we obtain discrete codes \(\{\mathbf{z}_{T}^{(l)}\}\) at multiple resolutions \(k_l\) for each time window \(T\), where \(l=1,\dots,L\) spans from coarsest \((k_1)\) to finest \((k_L)\). To ensure long-term temporal coherence, we model these codes with a Transformer that is autoregressive \emph{across} scales \((1\!\to\!L)\) \emph{and across} adjacent windows \((T-1\!\to\!T)\).

\noindent\textbf{Architecture.}
Figure~\ref{fig:main_method}(b) illustrates the AR model, where orange arrows mark training-only components. A pre-trained HuBERT \citep{hsu2021hubert} encoder extracts speech features \(\mathbf{a}_T\), resampled to each scale \(k_l\). A style token \(\mathbf{s}\) encodes speaker identity \citep{diffposetalk2024}. Given the previous window’s finest-scale codes \(\mathbf{z}_{T-1}^{(L)}\), we predict \(\mathbf{z}_{T}^{(1)},\dots,\mathbf{z}_{T}^{(L)}\) for the current window.

\noindent\textbf{Two-level autoregression.}
Within each window \(T\), the codes are generated scale by scale:
\small
\begin{align}
&p(\{\mathbf{z}_{T}^{(l)}\}_{l=1}^L \mid \mathbf{z}_{T-1}^{(L)}, \mathbf{a}_T, \mathbf{s})
\;=\;
\prod_{l=1}^L 
p\!\bigl(\mathbf{z}_{T}^{(l)} \mid \mathbf{z}_{T}^{(<l)}, \mathbf{z}_{T-1}^{(L)}, \mathbf{a}_T, \mathbf{s}\bigr),
\end{align}
\normalsize
where \(\mathbf{z}_{T}^{(<l)}\) denotes codes at all coarser scales. Inside each scale \(l\), tokens are predicted \emph{in parallel} (blockwise) while attending causally to previously generated scales and the previous window.

\noindent\textbf{Training and inference.}
During training, ground-truth codes \(\{\mathbf{z}_{T}^{(l)\!\text{(gt)}}\}\) supervise the model via cross-entropy:
\begin{align}
L_{\mathrm{AR}}
&=\;
-\sum_{T}\sum_{l=1}^L 
\log\,
p(\mathbf{z}_{T}^{(l)\!\text{(gt)}} \mid \mathbf{z}_{T}^{(<l)\!\text{(gt)}}, \mathbf{z}_{T-1}^{(L)\!\text{(gt)}}, \mathbf{a}_T, \mathbf{s}).
\end{align}
At inference, we sample or select each scale’s codes in sequence, conditioned on \(\mathbf{z}_{T-1}^{(L)}\), \(\mathbf{a}_T\), and \(\mathbf{s}\). Repeating this process over windows yields motion sequences that remain temporally coherent and stylistically consistent throughout.

% \subsection{Speech to Motion Autoregressive Model}
% \label{sec:33}
% After training the multi-scale encoder, we obtain discrete multi-scale tokens, which serve as inputs for training the autoregressive (AR) model. 
% Unlike conventional next-scale prediction tasks, our goal involves maintaining temporal coherence in motion generation.
% Autoregression within individual time windows alone is insufficient to achieve this, so we incorporate temporal autoregression into our modeling to ensure consistency across time.

% Figure \ref{fig:main_method} (b) illustrates the overall structure of our autoregressive model, with the orange arrows indicating components that are only active during training.
% Specifically, we first use a pre-trained HuBERT speech encoder to encode speech information for the current time window. 
% This encoded information is treated as conditional tokens. To ensure strict alignment between the generated motions and speech, the speech features are rescaled to match the multi-scale frame lengths $[k_1, k_2, ..., K]$.
% This alignment is critical to maintain precise synchronization between each motion frame and its corresponding speech signal during multi-scale generation.

% The autoregressive process begins with a single token map. We use a speaker’s style, encoded as the initial token $[s]$, to generate personalized motion sequences.
% The style features are extracted using a pre-trained Transformer similar to \citep{diffposetalk2024}.
% This approach reduces the complexity of the many-to-many mapping between speech and head motion, thereby stabilizing the training process.
% The AR model incrementally predicts higher-scale token maps, eventually generating features consistent with the time window length $K$ and mapping them to codebook IDs.

% During training, block-wise causal attention masks \citep{var2024} are applied to ensure that all tokens at each scale are generated in parallel while depending on the tokens from the previous scale and the speech conditions aligned in time.
% Beyond multi-scale autoregression, we retain the multi-scale codes from the previous time window to establish temporal attention links.
% Through this combination of temporal modeling in both the encoding-decoding phase and the autoregressive phase, our method generates temporally coherent motions and expressions.

% \noindent\textbf{Training Objectives and Inference.}
% During training, the ground truth multi-scale motion codes are used as inputs to the Transformer, and the AR model is trained using cross-entropy loss.
% During inference, the generation process is autoregressive, starting from the style features.
% It predicts the next-scale codes based on the current-scale speech conditions and the predictions from the previous scale.

\begin{figure}[t]
\vskip 0.15cm
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{images/comparison.pdf}}
\vskip -0.2cm
\caption{
Comparison of runtime efficiency and performance across different methods.
}
\label{fig:comparison_speed}
\end{center}
\vskip -0.6cm
\end{figure}

\section{Experiments}
\noindent\textbf{Datasets.}
We use the TFHP dataset proposed by ~\citet{diffposetalk2024} to train our model, which consists of 1,052 video clips from 588 subjects, with a total duration of approximately 26.5 hours.
All videos are tracked at 25 fps, resulting in approximately 2,385,000 action frames.
For training and evaluation, we adopt the train/test split provided in the original paper \citep{diffposetalk2024}.
Furthermore, we evaluate the generalization performance of our model on the test split of the widely used VOCASET dataset \citep{voca2019}.
The test split consists of 80 audiovisual sequence pairs from 2 subjects.
To adapt VOCASET to our method, we track the FLAME parameters at 25 fps and conduct evaluation on our tracked data.
It is worth noting that we don't use the VOCASET training split to train or fine-tune our method.

\noindent\textbf{Implementation Details.}
Our method is implemented on the PyTorch framework \citep{pytorch2017}.
In the first stage, we train our VQ autoencoder to obtain a multi-scale motion codebook.
The size of the motion codebook is 256 and the code dimension is 64.
The motion codebook consists of 256 entries, each with a code dimension of 64.
The time window size is 100 frames (4 seconds), and the multi-scale levels are [1, 5, 25, 50, 100].
During this stage, we used the AdamW optimizer with a learning rate of 1.0e-4, a total batch size of 64, and trained for 50,000 iterations.
In the second stage, we train the multi-scale autoregressive model using the AdamW optimizer with the same learning rate of 1.0e-4 and a batch size of 64 for 50,000 iterations.
During this stage, we employ a frozen pre-trained HuBERT \citep{hsu2021hubert} backbone without fine-tuning.

All training was conducted on one NVIDIA Tesla A100 GPU, requiring a total of approximately 13 GPU hours (5 hours for the first stage and 8 hours for the second stage), demonstrating efficient training resource utilization.
During inference, our method needs only 0.01 seconds to generate motions for 1 second on an NVIDIA Tesla A100 GPU and 0.057 seconds on an Apple M2 Pro chip, showcasing high inference efficiency and low latency.
For more implementation details, please refer to the supplementary materials.


\begin{table}[t]
\caption{
    Quantitative results on the TFHP \citep{diffposetalk2024} dataset. 
    We use colors to denote the \au{first} and \ag{second} places respectively.
    ${*}$ indicates that the method was not trained on TFHP, while all other methods were trained or fine-tuned on TFHP.
}
\label{tab:main_tfhp}
\vskip 0.4cm
\tablestyle{2pt}{1.1}
\begin{center}\begin{sc}
\scalebox{0.94}{\begin{tabular}{lccc}
\toprule
Method & LVE $\downarrow$ & FFD $\downarrow$ & MOD $\downarrow$ \\
\midrule
FaceFormer \citep{faceformer2022}       & 12.72 & 22.06 & 2.73  \\
CodeTalker \citep{codetalker2023}       & 12.28 & 22.38 & 2.60  \\
SelfTalk \citep{selftalk2023}           & 12.07 & 23.74 & 2.57  \\
FaceDiffuser \citep{facediffuser2023}   & 11.92 & 22.17 & 2.55  \\
MultiTalk$^{*}$ \citep{multitalk2024}   & 12.23 & 24.42 & 2.48  \\
ScanTalk$^{*}$ \citep{scantalk2024}           & 12.14 & 21.02 & 3.20  \\
DiffPoseTalk \citep{diffposetalk2024}   & \ag{10.39} & \ag{20.15} & \ag{2.07}  \\
\midrule
ARTalk (Ours)                           & \au{9.34} & \au{18.15} & \au{1.81} \\
\bottomrule
\end{tabular}}
\end{sc}\end{center}
\vskip -0.1in
\end{table}


\begin{table}[t]
\caption{
    Quantitative results on the VOCASET-Test \citep{voca2019} dataset. 
    We use colors to denote the \au{first} and \ag{second} places respectively.
    ${*}$ indicates that the method was not trained on VOCASET.
    It is worth noting that our method is \textbf{not} trained or fine-tuned on VOCASET.
}
\label{tab:main_vocaset}
\vskip 0.4cm
\tablestyle{2pt}{1.1}
\begin{center}\begin{sc}
\scalebox{0.94}{\begin{tabular}{lccc}
\toprule
Method & LVE $\downarrow$ & FFD $\downarrow$ & MOD $\downarrow$ \\
\midrule
FaceFormer \citep{faceformer2022}       &  8.28 & \ag{15.62} & 1.89  \\
CodeTalker \citep{codetalker2023}       &  7.73 & 18.86 & 1.81  \\
SelfTalk \citep{selftalk2023}           &  7.71 & 28.70 & 1.79  \\
FaceDiffuser \citep{facediffuser2023}   &  8.00 & 21.46 & 1.84  \\
MultiTalk$^{*}$ \citep{multitalk2024}         & 12.33 & 25.11 & 2.82  \\
ScanTalk \citep{scantalk2024}           &  \au{7.15} & 15.56 & \au{1.61}  \\
DiffPoseTalk$^{*}$ \citep{diffposetalk2024}   & 10.01 & 20.64 & 2.29  \\
\midrule
ARTalk (Ours)$^{*}$                           & \ag{7.57} & \au{15.49} & \ag{1.78} \\
\bottomrule
\end{tabular}}
\end{sc}\end{center}
\vskip -0.1in
\end{table}


\subsection{Quantitative Results}
Based on previous studies \citep{richard2021meshtalk, faceformer2022, codetalker2023, scantalk2024, diffposetalk2024}, we adopted two quantitative metrics, the lip vertex error (LVE) \citep{richard2021meshtalk} and the upper face dynamic deviation (FDD) \citep{codetalker2023}, to evaluate the generated facial motions.
LVE calculates the maximum L2 error of all lip vertices for each frame, evaluating the largest deviation between predicted and ground truth lip positions.
FDD calculates the standard deviation of the motion of each upper facial vertex over time between predictions and ground truth, evaluating the consistency of upper facial motion, which is closely related to speaking styles.
Additionally, we use a similar metric as ~\citet{diffposetalk2024}, mouth opening distance (MOD), to more accurately assess the stylistic similarity of mouth opening movements.
MOD measures the average difference in mouth opening region between predictions and ground truth.
Compared to LVE, MMD focuses more on the similarity of mouth opening styles and is less sensitive to temporal lip synchronization.
For the partitioning of the lip area and upper face, we used the mask partitioning officially provided by FLAME \citep{FLAME2017}.
The lip region contains 254 points, while the upper face, including the eye region and forehead, contains 884 points.

We present the quantitative comparison on TFHP dataset \citep{diffposetalk2024} in Table \ref{tab:main_tfhp}. The baseline methods FaceFormer \citep{faceformer2022}, CodeTalker \citep{codetalker2023}, SelfTalk \citep{selftalk2023}, and FaceDiffuser \citep{facediffuser2023} are all mesh-based approaches and cannot directly generalize to arbitrary styles or new meshes.
To ensure a fair comparison, we retrained these methods on meshes generated from the TFHP dataset.
For MultiTalk \citep{multitalk2024}, which is trained on FLAME meshes and supports language-based stylization, we directly used its English style for evaluation.
ScanTalk claims to work on arbitrary meshes, so we use the officially provided pre-trained weights.
For DiffPoseTalk, we directly used its pre-trained weights on the TFHP dataset (including head pose) for evaluation.
The results in the Table \ref{tab:main_tfhp} show that our method achieving significant improvements in lip synchronization accuracy (LVE) and style alignment (FFD and MOD), indicating that our method not only achieves precise lip synchronization but also effectively captures personalized speaking styles.

To demonstrate the generalizability of our method, we evaluated it on the VOCASET dataset \citep{voca2019}.
The results are presented in Table \ref{tab:main_vocaset}.
Notably, for some baseline methods \citep{faceformer2022, codetalker2023, selftalk2023, facediffuser2023, scantalk2024}, we used their pre-trained weights on VOCASET, whereas our method was \textbf{not} trained or fine-tuned on this dataset.
Despite this, our method achieved highly competitive performance and outperformed most baseline methods specifically trained on this dataset.
For MultiTalk \citep{multitalk2024} and DiffPoseTalk \citep{diffposetalk2024} which designed with style generalization capabilities, we followed the same strategy as with our method by directly testing it on the VOCASET dataset without any additional training or fine-tuning.
The results indicate that our method surpasses them in terms of generalization ability, further verifying the robustness of our method in handling unseen styles and data.

We also present a comparison of LVE and efficiency in Figure \ref{fig:comparison_speed}. 
Although our model adopts a two-level autoregressive framework, it remains more efficient than fully autoregressive methods.
This efficiency is achieved by utilizing longer window lengths while retaining only the past 4 seconds of motion frames.
Additionally, frames within each scale are generated in parallel within the window, further improving computational efficiency.


\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\linewidth]{images/main_results.pdf}}
\caption{
Qualitative comparison with existing methods (all head poses fixed).
The first four rows are from the TFHP dataset, and the last two rows are from the VOCASET dataset.
Our method shows better alignment with the ground truth in expression style, mouth dynamics, and lip synchronization.
Additional results are available in the supplementary materials and demo videos.
}
\label{fig:main_results}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Qualitative Results}
In Figure \ref{fig:main_results}, we present a qualitative comparison between our method and other baseline approaches.
Our method demonstrates excellent lip synchronization, accurately capturing various phonetic elements.
Furthermore, the generated results exhibit realistic facial expressions and mouth opening, closely matching the style of the ground truth.
Notably, our method also generates realistic blinking and head movements, which are implicitly learned and encoded within the motion codebook.
Additional qualitative evaluation results are available in the supplementary videos.


\begin{table}[t]
\caption{User study results.}
\label{tab:user_study}
\vskip 0.2cm
\tablestyle{2pt}{1.1}
\begin{center}\begin{sc}
\scalebox{0.80}{\begin{tabular}{lcccc}
\toprule
Method & Sync (\%) & Nat-Exp (\%) & Style (\%) & Nat-Pose (\%) \\
\midrule
vs. FaceFormer      & 75.0 & 89.3 & 88.1 & - \\
vs. CodeTalker      & 84.5 & 86.9 & 92.9 & - \\
vs. SelfTalk        & 85.7 & 89.3 & 91.7 & - \\
vs. FaceDiffuser    & 88.1 & 90.4 & 86.9 & - \\
vs. MultiTalk       & 78.6 & 76.2 & 78.6 & - \\
vs. ScanTalk        & 88.1 & 90.5 & 90.5 & - \\
vs. DiffPoseTalk    & 63.1 & 59.5 & 60.7 & 58.3 \\
\bottomrule
\end{tabular}}
\end{sc}\end{center}
\vskip -0.4cm
\end{table}

\subsection{User Study}
User studies are a reliable approach for evaluating 3D talking heads.
To comprehensively compare our method with baseline methods \citep{faceformer2022, codetalker2023, selftalk2023, facediffuser2023, multitalk2024, scantalk2024, diffposetalk2024}, we conducted a user study focusing on four key metrics: lip synchronization, facial expression naturalness, style consistency and head pose naturalness.

Given that head movements can significantly influence users' evaluation of lip synchronization, the study was divided into two parts.
In the first part, videos with fixed head poses were rendered across all methods to assess lip synchronization and facial expression realism and style consistency.
In the second part, videos with dynamic head poses were rendered to evaluate style consistency and head pose naturalness.
All comparisons were conducted using pairwise comparisons, where motions generated by our method and a competing baseline were displayed side by side, along with the ground truth provided as a reference for users.
After watching the videos, users selected the animation they perceived as better based on their subjective preference.
The proportion of user selections was calculated to quantify satisfaction.

As presented in Table \ref{tab:user_study}, our method significantly outperformed the baseline methods in lip synchronization, style consistency and facial expression realism.
Furthermore, our method achieved superior head pose naturalness compared to DiffPoseTalk \citep{diffposetalk2024}, demonstrating its comprehensive advantages.

\subsection{Ablation study}
\noindent\textbf{Multi-Scale Autoregression.}
To validate the importance of multi-scale autoregressive modeling within a single time window, we conducted experiments by removing the multi-scale autoregressive mechanism entirely. The results shown in Table \ref{tab:ablation} indicate a significant drop in generation accuracy without multi-scale autoregression.
We attribute this decline to the model's inability to fully capture the complex details of the speech-to-motion mapping without the hierarchical structure provided by multi-scale modeling.

\begin{figure}[t]
% \vskip 0.5cm
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{images/headpose.pdf}}
\vskip -0.1cm
\caption{
Qualitative results of head pose.
When certain words are stressed or when accents occur, the model produces nodding motions similar to human behavior.
Additionally, during transitions between different speech styles or sentence breaks, the head naturally turns, reflecting realistic conversational dynamics.
}
\label{fig:head_pose}
\end{center}
\vskip -0.6cm
\end{figure}

\noindent\textbf{Temporal Encoder and Temporal Autoregression.}
To assess the necessity of temporal encoding and autoregression across time windows, we replaced our proposed temporal encoder and autoregressive model with a standard multi-scale encoder and a single-window multi-scale autoregressive model, respectively.
As shown in Table \ref{tab:ablation}, the absence of temporal modeling in VQ autoencoder or autoregressive both result in degraded generation quality.
Additionally, we also observed temporal discontinuities in the motion sequences, further emphasizing the critical role of our temporal design in achieving smooth and coherent outputs.

\noindent\textbf{Speaker Style Embedding.}
We also evaluated the effect of removing the speaker style feature, which serves as the starting condition for the autoregressive process.
Without this feature, the complexity of the many-to-many mapping between speech and motion increases significantly, and the generated motions lack personalized style.
As reflected in Table \ref{tab:ablation}, the removal of style features causes a substantial decline in generation quality, highlighting their importance in achieving stylistic and expressive outputs.

\noindent\textbf{Key Hyperparameter Choices.}
We explored different number of motion frames within a single time window, which is critical for our method.
Specifically, we tested window lengths of 8, 25, 50 frames.
For a window length of 8, we used a multi-scale sequence of [1,2,4,8], while for 25 and 50, we adopted [1,5,10,15,25] and [1,5,10,25,50], respectively.
The results, presented in Table \ref{tab:ablation}, demonstrate that the choice of window size significantly impacts performance.
However, the window size depends heavily on the downstream task characteristics, such as whether the input consists of streaming audio chunks.
In general, longer temporal windows generally produce better results.
In this paper, we chose a window length of 100 frames and a multi-scale sequence of [1, 5, 25, 50, 100] as it is consistent with previous work such as DiffPoseTalk \citep{diffposetalk2024} and achieves a good balance between quality and efficiency.

\begin{table}[t]
\caption{Ablation results on TFHP dataset.}
\label{tab:ablation}
\vskip 0.3cm
\tablestyle{8pt}{1.1}
\begin{center}\begin{sc}
\scalebox{0.97}{\begin{tabular}{lccc}
\toprule
Method & LVE$\downarrow$ & FFD$\downarrow$ & MOD $\downarrow$ \\
\midrule
w/o Multi-Scale AR      & 14.14 & 22.04 & 3.06 \\
w/o Temporal VQ         &  9.82 & 18.60 & 1.86 \\
w/o Temporal AR         &  9.97 & 18.71 & 1.99 \\
w/o Style Embedding     & 11.80 & 21.46 & 2.37 \\
Motion Clip Length 8    & 11.73 & 24.89 & 2.25 \\ 
Motion Clip Length 25   & 10.20 & 19.00 & 1.97 \\ 
Motion Clip Length 50   &  9.78 & \textbf{18.03} & 1.89 \\ 
\midrule
ARTalk (Full)           &  \textbf{9.34} & 18.15 & \textbf{1.81} \\
\bottomrule
\end{tabular}}
\end{sc}\end{center}
\vskip -0.3cm
\end{table}



\section{Discussion and Conclusion}
In this paper, we introduced ARTalk, a novel framework for generating 3D facial and head motions from speech.  
The core innovation of our method lies in the temporal multi-scale autoencoder and the ARTalk autoregressive Transformer, which together ensure temporal consistency and precise motion generation.
Our experimental results demonstrate that ARTalk outperforms state-of-the-art baselines in terms of lip synchronization, expression naturalness and style consistency, while maintaining real-time generation capabilities.  
We believe that the strong generalization ability of ARTalk make it a promising solution for a wide range of applications, including virtual avatars, language training, and animation production for gaming and movies.

\noindent\textbf{Limitations and future work.}
While ARTalk demonstrates strong performance in lip synchronization and expressions, a current limitation is that head motions are primarily driven by speech prosody (rhythm and emphasis) rather than semantic context.
Addressing the cultural and semantic nuances of head gestures, such as nodding or shaking to indicate affirmation, would require broader and more diverse datasets and fine-grained head pose control, which we leave as future work.
We hope that our work lays a solid foundation for the further research of 3D talking head generation.

\section*{Impact Statement}
Given that our method can generate highly realistic talking head sequences, there is a potential risk of misuse, such as deepfake creation and manipulation.
To address these concerns, we strongly advocate for watermarking or clearly annotating facial sequences generated by our method as synthetic data.  
Moreover, we will collaborate on deepfake detection research to further ensure that our proposed method is used for positive and constructive applications.
We also aim to raise awareness about these risks and encourage collaboration among governments, developers, researchers, and users to jointly prevent the misuse of synthetic video technologies.
By fostering collective responsibility and supporting regulatory efforts, we hope to promote the ethical use of AI technologies while mitigating potential harms.