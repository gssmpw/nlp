\section{Reproducibility}
\subsection{More Data Processing Details of VOCASET}
For evaluation, we also re-tracked the FLAME \citep{FLAME2017} parameters of the VOCASET \citep{voca2019} dataset.
This is necessary because the  original data of VOCASET registers head motion directly onto FLAME meshes rather than FLAME parameters, and also introduces details such as hair that cannot be represented by the FLAME parameters.
Since our method and DiffPoseTalk \citep{diffposetalk2024} works in the FLAME parameter space, we reprocessed VOCASET accordingly. 
Specifically, we obtained the original VOCASET videos, resampled them to 25 fps, and used EMICA \citep{DECA2021, MICA2022, EMOCA2021} to extract a 100-dimensional shape vector, a 50-dimensional expression vector, and a 6-dimensional pose vector for evaluation.  

For mesh-based baseline methods, we used meshes reconstructed from FLAME parameters instead of the original VOCASET meshes.
While this introduces minor differences—such as the absence of hair and subtle expression details—these variations have minimal impact on our primary evaluation, which focuses on lip movements.

\subsection{More Implementation Details}
Our model consists of two main components: a multi-scale encoder-decoder and an autoregressive module.  

The multi-scale encoder-decoder adopts a transformer-based architecture.
Encoder and decoder both consist of 8 layers, each with 8 attention heads and a hidden dimension of 512.
The encoder outputs a 64-dimensional codebook representation, while the decoder reconstructs the original expression and pose parameters.  
During multi-scale quantization, an additional fully connected layer is used in the upsampling process to recover lost information, maintaining an input and output dimension of 64.
The codebook consists of 256 vectors, each with a dimensionality of 64.  

The autoregressive module is a transformer-based autoregressive model.
It has an input and hidden dimension of 768, a conditional vector dimension of 768, and a depth of 12 layers with 12 attention heads.
At the input stage, both the codebook features and the audio encoding features are projected to a 768-dimensional space.
The audio conditions are incorporated into the transformer layers using adaptive instance normalization (AdaIN) \citep{huang2017arbitrary}.  
After generation through the transformer, a fully connected layer maps the output to a 256-dimensional space, followed by a softmax function to compute the probability distribution over the codebook entries.  

We use the AdamW \citep{loshchilov2017decoupled} optimizer with an initial learning rate of \(1 \times 10^{-4}\), trained for 50,000 iterations.
A linear decay learning rate scheduler is applied, gradually reducing the learning rate to \(1 \times 10^{-5}\) over the course of training.

\subsection{More Evaluation Details}
We generally follow the approach of CodeTalker \citep{codetalker2023} to compute LVE and FFD, and additionally compute the mean mouth opening distance (MMD).
Since existing works rarely specify the exact selection of mouth and upper face vertices, we adopt the official FLAME region masks.
\footnote{FLAME\_masks.pkl downloaded from http://s2017.siggraph.org.}
Specifically, the lip/mouth region consists of 254 vertices from the “lips” area, while the upper face region includes 884 vertices from the “eye\_region” and “forehead” areas.

\section{Preliminaries of 3DMM}
We leverage the widely used 3D morphable model (3DMM), FLAME \citep{FLAME2017}, known for its geometric accuracy and versatility.
Due to its realistic rendering capabilities and flexibility, FLAME has been widely adopted in applications such as facial animation, avatar creation, and facial recognition.
We use FLAME as our representation for facial motion, construct a multi-scale codebook using FLAME parameters and learn speech-driven autoregression within this codebook, effectively leveraging the priors embedded in FLAME. 
This approach provides two key advantages: (1) it reduces the high-dimensional complexity of modeling a large number of mesh vertices, and (2) it enables seamless integration with downstream tasks that utilize FLAME-based representations \citep{gpavatar2024, deng2024portrait4d2, chu2024gagavatar, xu2023gaussianheadavatar}.

The FLAME model represents the head shape as follows:
\begin{equation}
\label{eq:flame}
    TP(\hat{\beta}, \hat{\theta}, \hat{\psi}) = \bar{T} + BS(\hat{\beta};S) + BP(\hat{\theta};P) + BE(\hat{\psi};E),
\end{equation}
where $\bar{T}$ represents the template head avatar mesh, $BS(\hat{\beta};S)$ is the shape blend-shape function to account for identity-related shape variation, $BP(\hat{\theta};P)$ models jaw and neck pose to correct deformations that cannot be fully explained by linear blend skinning, and the expression blend-shapes $BE(\hat{\psi};E)$ capture facial expressions such as eye closure and smiling.



\section{More Discussions on ARTalk}
\subsection{The Choice of Audio Encoder}
The performance of the audio encoder has a significant impact on the overall model.
In this work, we use the HuBERT \citep{hsu2021hubert} encoder for experiments.
However, HuBERT is primarily designed for longer audio comprehension, which may not be ideal for our windowed input setting.
For instance, when the input is as short as 8 frames (320 ms), HuBERT struggles to effectively capture phonetic features.  

To address this, we experiment with Mimi \citep{kyutai2024moshi}, an audio encoder better suited for streaming input and short audio segments.
As shown in Table \ref{tab:supp_mimi}, Mimi outperforms HuBERT when the input length is short, whereas HuBERT performs better on longer audio segments.
We attribute this to Mimi’s design, which prioritizes capturing fine-grained phonetic details in short audio sequences.
However, its lower feature dimensionality limits its representational capacity for longer segments compared to HuBERT.  
Ultimately, the selection of the audio encoder should be guided by the specific requirements of the application, balancing the need for short-term phonetic precision and long-term representational capacity.

\begin{table}[ht]
\caption{Different audio encoder performance on TFHP dataset.}
\vskip 0.2cm
\label{tab:supp_mimi}
\tablestyle{6pt}{1.1}
\begin{center}\begin{sc}
\begin{tabular}{lccc}
\toprule
Method & LVE$\downarrow$ & FFD$\downarrow$ & MOD $\downarrow$ \\
\midrule
Motion Clip Length 8 + MIMI     & 11.01 & 22.79 & 2.14 \\ 
Motion Clip Length 8 + HuBERT   & 11.73 & 24.89 & 2.25 \\ 
Motion Clip Length 25 + MIMI    & 10.33 & 19.65 & 1.99 \\ 
Motion Clip Length 25 + HuBERT  & 10.20 & 19.00 & 1.97 \\ 
Motion Clip Length 50 + MIMI    & 10.10 & 19.16 & 1.95 \\ 
Motion Clip Length 50 + HuBERT  &  9.78 & 18.03 & 1.89 \\ 
Motion Clip Length 100 + MIMI   &  9.97 & 18.48 & 1.93 \\ 
Motion Clip Length 100 + HuBERT &  \textbf{9.34} & \textbf{18.15} & \textbf{1.81} \\ 
\bottomrule
\end{tabular}
\end{sc}\end{center}
\vskip -0.25cm
\end{table}

\subsection{Performance of Multi-scale VQ Autoencoder}
The overall performance of the model is inherently influenced by the VQ autoencoder, which serves as the upper bound for the final model's performance. 
Here in Table \ref{tab:supp_vq}, we present the performance of VQ autoencoder trained with different clip lengths.
We did not observe a significant impact of clip length on the performance of the VQ autoencoder.
For the autoregressive generation results under different segment length settings, please refer to the Table \ref{tab:ablation} in the main paper.

\begin{table}[ht]
\caption{VQ Autoencoder performance on TFHP dataset.}
\label{tab:supp_vq}
\vskip 0.2cm
\tablestyle{6pt}{1.1}
\begin{center}\begin{sc}
\begin{tabular}{lccc}
\toprule
Method & LVE$\downarrow$ & FFD$\downarrow$ & MOD $\downarrow$ \\
\midrule
Motion Clip Length 8    & 1.96 & 8.81 & 0.43 \\ 
Motion Clip Length 25   & 1.97 & 9.57 & 0.44 \\ 
Motion Clip Length 50   & 2.19 & 9.54 & 0.47 \\ 
Motion Clip Length 100  & 2.08 & 8.97 & 0.45 \\ 
\bottomrule
\end{tabular}
\end{sc}\end{center}
\vskip -0.25cm
\end{table}


\begin{figure}[t]
% \vskip 0.5cm
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{images/user_study.pdf}}
\caption{
The interface of our user study.
Users evaluate each video based on three criteria: lip synchronization, expression naturalness, and style consistency.
The first two are judged by comparing method A and B, while style consistency is assessed with reference to the ground truth.
One of the videos (A or B) is generated by our method, and the other by a baseline method, with their order randomized.
}
\label{fig:supp_user_study}
\end{center}
\vskip -0.5cm
\end{figure}

\section{User Study Details}
We collected a total of 28 survey responses, with each participant answering 84 questions (corresponding to 84 pairwise comparison trials), for a total of 2352 comparisons.
Among these, 63 comparisons were conducted on the TFHP dataset and 21 on the VOCASET dataset.
To mitigate potential biases caused by random or preferential selections, we randomized the display order of our method and the baseline in each trial.
Which means in each comparison, one of the videos (A or B) was generated by our method, but the assignment was randomized.

For each comparison, users were asked three questions.
The first two questions follow prior studies \citep{faceformer2022, codetalker2023}, assessing the quality of lip synchronization and the perceived naturalness of expressions.
Additionally, we introduce a third question to evaluate the consistency of the generated animation with the ground truth style.
The questions presented to users were as follows: Which lip sync performs better? Which looks more natural in expression? Which looks more consistent in style with the ground truth? All three questions are single-choice, requiring users to select either A or B.


% \section{About the Generation Diversity}
% Although our method follows an autoregressive framework, it differs from previous deterministic autoregressive approaches \citep{faceformer2022, codetalker2023, multitalk2024} by adopting a probabilistic generation process.
% Instead of directly predicting motion features, our model estimates the probability distribution over tokens at each step.
% This allows us to adopt multiple sampling strategies.
% With greedy search, the token with the highest probability is always selected, resulting in deterministic generation.
% In contrast, with top-k sampling, the next token is sampled from the top-k most likely candidates, enabling diverse generation.
% By adjusting the sampling strategy, we can control the trade-off between diversity and consistency in motion generation.
% We present the results of our sampling-based diversity evaluation in Table \ref{tab:supp_random}.

\begin{figure}[H]
\begin{center}
\centerline{\includegraphics[width=0.85\columnwidth]{images/supp_downstream.pdf}}
\vskip -0.3cm
\caption{
We reconstruct the avatar using GAGAvatar \citep{chu2024gagavatar} and drive it with our ARTalk model, enabling speech-driven dynamic avatar generation.
}
\label{fig:supp_gaga}
\end{center}
\vskip -0.5cm
\end{figure}

\begin{figure}[ht]
% \vskip 0.5cm
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{images/supp_moewresults.pdf}}
\caption{
The interface of our user study.
}
\label{fig:supp_comparison}
\end{center}
\vskip -0.3cm
\end{figure}


\section{Integration with Downstream Tasks}
Our method can be integrated into various FLAME-based downstream applications, enabling a wider range of use cases. 
While this extends beyond the primary scope of this paper, we also demonstrate its application in GAGAvatar \citep{chu2024gagavatar}.
As shown in Figure \ref{fig:supp_gaga}, our method can generate control signals for GAGAvatar, effectively transforming it into a speech-driven one-shot dynamic avatar reconstruction method.

\section{More Qualitative Results}
We present additional qualitative comparison results in Figure \ref{fig:supp_comparison}.
However, we strongly recommend referring to the supplementary video for a more comprehensive comparison of different methods.



