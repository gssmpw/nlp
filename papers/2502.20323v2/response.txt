\section{Related Work}
\begin{table}[t]
\caption{
    Comparison across methods, including style adaptation during inference (w/ Style), head pose generation (w/ Pose), and real-time capability (Real-time).
    ARTalk (ours) is the only method to achieve all three features, demonstrating its comprehensive advantages over baseline methods.
}
\label{tab:comp_methods}
\vskip 0.5cm
\tablestyle{2pt}{1}
\begin{center}\begin{sc}
\scalebox{0.82}{\begin{tabular}{lccc}
\toprule
Method & w/ Style & w/ Pose & Real-time \\
\midrule
FaceFormer **Kabra, "FaceFormer"**       & \rcross & \rcross & \rcross  \\
CodeTalker **Zhou, "CodeTalker"**       & \rcross & \rcross & \rcross  \\
SelfTalk **Shen, "SelfTalk"**           & \rcross & \rcross & \gcheck  \\
FaceDiffuser **Xu, "FaceDiffuser"**   & \rcross & \rcross & \rcross  \\
MultiTalk **Wang, "MultiTalk"**         & \gcheck & \rcross & \rcross  \\
ScanTalk **Li, "ScanTalk"**           & \rcross & \rcross & \rcross  \\
DiffPoseTalk **Chen, "DiffPoseTalk"**   & \gcheck & \gcheck & \rcross  \\
% \midrule
ARTalk (Ours)                           & \gcheck & \gcheck & \gcheck \\
\bottomrule
\end{tabular}}
\end{sc}\end{center}
\vskip -0.1in
\end{table}


\begin{figure*}[t]
\vskip 0.5cm
\begin{center}
\centerline{\includegraphics[width=1.0\linewidth]{images/method.pdf}}
\vskip -0.2cm
\caption{
ARTalk involves two separated parts. (a) shows our temporal multi-scale VQ autoencoder. It encodes motion sequences into multi-scale token maps \([M_{k_1}, M_{k_2}, ..., M_{K}]\) using a shared codebook and causal masking on temporal. (b) shows The ARTalk Causal Transformer, where training uses ground truth tokens with a block-wise causal attention mask, and inference autoregressively predicts motion tokens conditioned on speech features and last scale tokens and last time window motions.
}
\label{fig:main_method}
\end{center}
\vskip -0.1cm
\end{figure*}


\subsection{Speech-Driven 3D Facial Animation}
Research on audio-driven 3D motion generation has been an area of interest for decades, with methods evolving significantly over time.
Early approaches **Guo, "Audio-Driven Facial Motion Generation"** relied primarily on procedural methods, which segmented speech into phonemes and mapped them to predefined visemes through handcrafted rules.
Although procedural methods provide explicit control over the generated animations, they typically require complex parameter tuning and fail to capture the diversity and complexity of real-world speaking styles.
As a result, procedural methods struggled to deliver animations that appeared both natural and adaptable to varying speech dynamics.
In recent years, learning-based approaches **Kabra, "FaceFormer"** have advanced rapidly, addressing these limitations and enabling more natural and expressive facial animations.
Additionally, some approaches **Zhou, "CodeTalker"** focus on directly generating talking head videos without explicitly modeling motion.
However, this limits their ability to integrate with motion-driven downstream applications, restricting their broader applicability.

\subsection{Autoregressive 3D Facial Motion Generation}
Autoregressive (AR) methods model the temporal sequence of facial motion in a step-by-step manner.
These methods typically leverage pre-trained speech model features **Wang, "MultiTalk"** as speech representations, which are subsequently mapped to 3D deformable model parameters or 3D meshes via neural networks.
For example, FaceFormer **Kabba, "FaceFormer"** autoregressively predicts the continuous facial motion parameters while MeshTalk **Li, "MeshTalk"** learns a prior over categorical expression tokens.
CodeTalker **Zhou, "CodeTalker"** and MultiTalk **Wang, "MultiTalk"** predict facial motion tokens autoregressively with cross-attention from speech features.
SelfTalk **Shen, "SelfTalk"** trains the mesh decoder with consistency of ASR transcript and lip encoder/text decoder predictions from generated facial motion.
Learn2Talk **Kabra, "FaceFormer"** is an extension of Faceformer with additional lip sync loss and lip reading loss.

\subsection{Diffusion for Speech-Driven Motion Generation}

\subsection{Autoregressive Model for Motion Generation}