[
  {
    "index": 0,
    "papers": [
      {
        "key": "faceformer2022",
        "author": "Fan, Yingruo and Lin, Zhaojiang and Saito, Jun and Wang, Wenping and Komura, Taku",
        "title": "FaceFormer: Speech-Driven 3D Facial Animation with Transformers"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "codetalker2023",
        "author": "Xing, Jinbo and Xia, Menghan and Zhang, Yuechen and Cun, Xiaodong and Wang, Jue and Wong, Tien-Tsin",
        "title": "Codetalker: Speech-driven 3d facial animation with discrete motion prior"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "selftalk2023",
        "author": "Ziqiao Peng and Yihao Luo and Yue Shi and Hao Xu and Xiangyu Zhu and Hongyan Liu and Jun He and Zhaoxin Fan",
        "title": "SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend 3D Talking Faces"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "facediffuser2023",
        "author": "Stan, Stefan and Haque, Kazi Injamamul and Yumak, Zerrin",
        "title": "Facediffuser: Speech-driven 3d facial animation synthesis using diffusion"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "multitalk2024",
        "author": "Kim Sung-Bin and Lee Chae-Yeon and Gihun Son and Oh Hyun-Bin and Janghoon Ju and Suekyeong Nam and Tae-Hyun Oh",
        "title": "MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "scantalk2024",
        "author": "Nocentini, F. and Besnier, T. and Ferrari, C. and Arguillere, S. and Berretti, S. and Daoudi, M.",
        "title": "ScanTalk: 3D Talking Heads from Unregistered Scans"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "diffposetalk2024",
        "author": "Sun, Zhiyao and Lv, Tian and Ye, Sheng and Lin, Matthieu and Sheng, Jenny and Wen, Yu-Hui and Yu, Minjing and Liu, Yong-Jin",
        "title": "DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "taylor2012dynamic",
        "author": "Taylor, Sarah L and Mahler, Moshe and Theobald, Barry-John and Matthews, Iain",
        "title": "Dynamic units of visual speech"
      },
      {
        "key": "xu2013practical",
        "author": "Xu, Yuyu and Feng, Andrew W and Marsella, Stacy and Shapiro, Ari",
        "title": "A practical and configurable lip sync method for games"
      },
      {
        "key": "edwards2016jali",
        "author": "Edwards, Pif and Landreth, Chris and Fiume, Eugene and Singh, Karan",
        "title": "Jali: an animator-centric viseme model for expressive lip synchronization"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "faceformer2022",
        "author": "Fan, Yingruo and Lin, Zhaojiang and Saito, Jun and Wang, Wenping and Komura, Taku",
        "title": "FaceFormer: Speech-Driven 3D Facial Animation with Transformers"
      },
      {
        "key": "codetalker2023",
        "author": "Xing, Jinbo and Xia, Menghan and Zhang, Yuechen and Cun, Xiaodong and Wang, Jue and Wong, Tien-Tsin",
        "title": "Codetalker: Speech-driven 3d facial animation with discrete motion prior"
      },
      {
        "key": "diffposetalk2024",
        "author": "Sun, Zhiyao and Lv, Tian and Ye, Sheng and Lin, Matthieu and Sheng, Jenny and Wen, Yu-Hui and Yu, Minjing and Liu, Yong-Jin",
        "title": "DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models"
      },
      {
        "key": "danvevcek2023emotional",
        "author": "Dan{\\v{e}}{\\v{c}}ek, Radek and Chhatre, Kiran and Tripathi, Shashank and Wen, Yandong and Black, Michael and Bolkart, Timo",
        "title": "Emotional speech-driven animation with content-emotion disentanglement"
      },
      {
        "key": "emotalk2023",
        "author": "Peng, Ziqiao and Wu, Haoyu and Song, Zhenbo and Xu, Hao and Zhu, Xiangyu and He, Jun and Liu, Hongyan and Fan, Zhaoxin",
        "title": "EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation"
      },
      {
        "key": "yang2024probabilistic",
        "author": "Yang, Karren D and Ranjan, Anurag and Chang, Jen-Hao Rick and Vemulapalli, Raviteja and Tuzel, Oncel",
        "title": "Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks Methods and Applications"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yi2022predicting",
        "author": "Yi, Ran and Ye, Zipeng and Sun, Zhiyao and Zhang, Juyong and Zhang, Guoxin and Wan, Pengfei and Bao, Hujun and Liu, Yong-Jin",
        "title": "Predicting personalized head movement from short video and speech signal"
      },
      {
        "key": "ye2024real3d",
        "author": "Ye, Zhenhui and Zhong, Tianyun and Ren, Yi and Yang, Jiaqi and Li, Weichuang and Huang, Jiawei and Jiang, Ziyue and He, Jinzheng and Huang, Rongjie and Liu, Jinglin and others",
        "title": "Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis"
      },
      {
        "key": "zhang2023sadtalker",
        "author": "Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei",
        "title": "Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation"
      },
      {
        "key": "tan2024say",
        "author": "Tan, Shuai and Ji, Bin and Ding, Yu and Pan, Ye",
        "title": "Say Anything with Any Style"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "baevski2020wav2vec",
        "author": "Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael",
        "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
      },
      {
        "key": "hsu2021hubert",
        "author": "Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman",
        "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units"
      },
      {
        "key": "kyutai2024moshi",
        "author": "Alexandre D\\'efossez and Laurent Mazar\\'e and Manu Orsini and Am\\'elie Royer and Patrick P\\'erez and Herv\\'e J\\'egou and Edouard Grave and Neil Zeghidour",
        "title": "Moshi: a speech-text foundation model for real-time dialogue"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "faceformer2022",
        "author": "Fan, Yingruo and Lin, Zhaojiang and Saito, Jun and Wang, Wenping and Komura, Taku",
        "title": "FaceFormer: Speech-Driven 3D Facial Animation with Transformers"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "richard2021meshtalk",
        "author": "Richard, Alexander and Zollh{\\\"o}fer, Michael and Wen, Yandong and De la Torre, Fernando and Sheikh, Yaser",
        "title": "Meshtalk: 3d face animation from speech using cross-modality disentanglement"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "codetalker2023",
        "author": "Xing, Jinbo and Xia, Menghan and Zhang, Yuechen and Cun, Xiaodong and Wang, Jue and Wong, Tien-Tsin",
        "title": "Codetalker: Speech-driven 3d facial animation with discrete motion prior"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "multitalk2024",
        "author": "Kim Sung-Bin and Lee Chae-Yeon and Gihun Son and Oh Hyun-Bin and Janghoon Ju and Suekyeong Nam and Tae-Hyun Oh",
        "title": "MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "Zhuang2024learn2talk",
        "author": "Zhuang, Yixiang and Cheng, Baoping and Cheng, Yao and Jin, Yuntao and Liu, Renshuai and Li, Chengyang and Cheng, Xuan and Liao, Jing and Lin, Juncong",
        "title": "Learn2Talk: 3D Talking Face Learns from 2D Talking Face"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wu2024mmhead",
        "author": "Wu, Sijing and Li, Yunhao and Yan, Yichao and Duan, Huiyu and Liu, Ziwei and Zhai, Guangtao",
        "title": "MMHead: Towards Fine-grained Multi-modal 3D Facial Animation"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "faceformer2022",
        "author": "Fan, Yingruo and Lin, Zhaojiang and Saito, Jun and Wang, Wenping and Komura, Taku",
        "title": "FaceFormer: Speech-Driven 3D Facial Animation with Transformers"
      },
      {
        "key": "codetalker2023",
        "author": "Xing, Jinbo and Xia, Menghan and Zhang, Yuechen and Cun, Xiaodong and Wang, Jue and Wong, Tien-Tsin",
        "title": "Codetalker: Speech-driven 3d facial animation with discrete motion prior"
      },
      {
        "key": "multitalk2024",
        "author": "Kim Sung-Bin and Lee Chae-Yeon and Gihun Son and Oh Hyun-Bin and Janghoon Ju and Suekyeong Nam and Tae-Hyun Oh",
        "title": "MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "ho2020denoising",
        "author": "Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",
        "title": "Denoising diffusion probabilistic models"
      },
      {
        "key": "zhu2023taming",
        "author": "Zhu, Lingting and Liu, Xian and Liu, Xuanyu and Qian, Rui and Liu, Ziwei and Yu, Lequan",
        "title": "Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation"
      },
      {
        "key": "alexanderson2023listen",
        "author": "Alexanderson, Simon and Nagy, Rajmund and Beskow, Jonas and Henter, Gustav Eje",
        "title": "Listen, denoise, action! audio-driven motion synthesis with diffusion models"
      },
      {
        "key": "facediffuser2023",
        "author": "Stan, Stefan and Haque, Kazi Injamamul and Yumak, Zerrin",
        "title": "Facediffuser: Speech-driven 3d facial animation synthesis using diffusion"
      },
      {
        "key": "ma2024diffspeaker",
        "author": "Zhiyuan Ma and Xiangyu Zhu and Guojun Qi and Chen Qian and Zhaoxiang Zhang and Zhen Lei",
        "title": "DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer"
      },
      {
        "key": "diffposetalk2024",
        "author": "Sun, Zhiyao and Lv, Tian and Ye, Sheng and Lin, Matthieu and Sheng, Jenny and Wen, Yu-Hui and Yu, Minjing and Liu, Yong-Jin",
        "title": "DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "facediffuser2023",
        "author": "Stan, Stefan and Haque, Kazi Injamamul and Yumak, Zerrin",
        "title": "Facediffuser: Speech-driven 3d facial animation synthesis using diffusion"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "aneja2023facetalk",
        "author": "Shivangi Aneja and Justus Thies and Angela Dai and Matthias Nie\u00dfner",
        "title": "FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "baevski2020wav2vec",
        "author": "Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael",
        "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "ma2024diffspeaker",
        "author": "Zhiyuan Ma and Xiangyu Zhu and Guojun Qi and Chen Qian and Zhaoxiang Zhang and Zhen Lei",
        "title": "DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "scantalk2024",
        "author": "Nocentini, F. and Besnier, T. and Ferrari, C. and Arguillere, S. and Berretti, S. and Daoudi, M.",
        "title": "ScanTalk: 3D Talking Heads from Unregistered Scans"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "sharp2022diffusionnet",
        "author": "Sharp, Nicholas and Attaiki, Souhaib and Crane, Keenan and Ovsjanikov, Maks",
        "title": "Diffusionnet: Discretization agnostic learning on surfaces"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "diffposetalk2024",
        "author": "Sun, Zhiyao and Lv, Tian and Ye, Sheng and Lin, Matthieu and Sheng, Jenny and Wen, Yu-Hui and Yu, Minjing and Liu, Yong-Jin",
        "title": "DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "zhang2024letstalk",
        "author": "Zhang, Haojie and Liang, Zhihao and Fu, Ruibo and Wen, Zhengqi and Liu, Xuefei and Li, Chenxing and Tao, Jianhua and Liang, Yaling",
        "title": "LetsTalk: Latent Diffusion Transformer for Talking Video Synthesis"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "FLAME2017",
        "author": "Li, Tianye and Bolkart, Timo and Black, Michael. J. and Li, Hao and Romero, Javier",
        "title": "Learning a model of facial shape and expression from {4D} scans"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "alghamdi2022talking",
        "author": "Alghamdi, Mohammed M and Wang, He and Bulpitt, Andrew J and Hogg, David C",
        "title": "Talking Head from Speech Audio using a Pre-trained Image Generator"
      },
      {
        "key": "chen2020talking",
        "author": "Chen, Lele and Cui, Guofeng and Liu, Celong and Li, Zhong and Kou, Ziyi and Xu, Yi and Xu, Chenliang",
        "title": "Talking-head generation with rhythmic head motion"
      },
      {
        "key": "chung2017out",
        "author": "Chung, Joon Son and Zisserman, Andrew",
        "title": "Out of time: automated lip sync in the wild"
      },
      {
        "key": "das2020speech",
        "author": "Das, Dipanjan and Biswas, Sandika and Sinha, Sanjana and Bhowmick, Brojeshwar",
        "title": "Speech-driven facial animation using cascaded gans for learning of motion and texture"
      },
      {
        "key": "guo2021ad",
        "author": "Guo, Yudong and Chen, Keyu and Liang, Sen and Liu, Yong-Jin and Bao, Hujun and Zhang, Juyong",
        "title": "Ad-nerf: Audio driven neural radiance fields for talking head synthesis"
      },
      {
        "key": "ji2022eamm",
        "author": "Ji, Xinya and Zhou, Hang and Wang, Kaisiyuan and Wu, Qianyi and Wu, Wayne and Xu, Feng and Cao, Xun",
        "title": "Eamm: One-shot emotional talking face via audio-based emotion-aware motion model"
      },
      {
        "key": "ji2021audio",
        "author": "Ji, Xinya and Zhou, Hang and Wang, Kaisiyuan and Wu, Wayne and Loy, Chen Change and Cao, Xun and Xu, Feng",
        "title": "Audio-driven emotional video portraits"
      },
      {
        "key": "liang2022expressive",
        "author": "Liang, Borong and Pan, Yan and Guo, Zhizhi and Zhou, Hang and Hong, Zhibin and Han, Xiaoguang and Han, Junyu and Liu, Jingtuo and Ding, Errui and Wang, Jingdong",
        "title": "Expressive talking head generation with granular audio-visual control"
      },
      {
        "key": "liu2022semantic",
        "author": "Liu, Xian and Xu, Yinghao and Wu, Qianyi and Zhou, Hang and Wu, Wayne and Zhou, Bolei",
        "title": "Semantic-aware implicit neural audio-driven video portrait generation"
      },
      {
        "key": "pang2023dpe",
        "author": "Pang, Youxin and Zhang, Yong and Quan, Weize and Fan, Yanbo and Cun, Xiaodong and Shan, Ying and Yan, Dong-ming",
        "title": "Dpe: Disentanglement of pose and expression for general video portrait editing"
      },
      {
        "key": "prajwal2020lip",
        "author": "Prajwal, KR and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P and Jawahar, CV",
        "title": "A lip sync expert is all you need for speech to lip generation in the wild"
      },
      {
        "key": "shen2022learning",
        "author": "Shen, Shuai and Li, Wanhua and Zhu, Zheng and Duan, Yueqi and Zhou, Jie and Lu, Jiwen",
        "title": "Learning dynamic facial radiance fields for few-shot talking head synthesis"
      },
      {
        "key": "vougioukas2020realistic",
        "author": "Vougioukas, Konstantinos and Petridis, Stavros and Pantic, Maja",
        "title": "Realistic speech-driven facial animation with gans"
      },
      {
        "key": "wang2022one",
        "author": "Wang, Suzhen and Li, Lincheng and Ding, Yu and Yu, Xin",
        "title": "One-shot talking face generation from single-speaker audio-visual correlation learning"
      },
      {
        "key": "yi2020audio",
        "author": "Yi, Ran and Ye, Zipeng and Zhang, Juyong and Bao, Hujun and Liu, Yong-Jin",
        "title": "Audio-driven talking face video generation with learning-based personalized head pose"
      },
      {
        "key": "zhang2023metaportrait",
        "author": "Zhang, Bowen and Qi, Chenyang and Zhang, Pan and Zhang, Bo and Wu, HsiangTao and Chen, Dong and Chen, Qifeng and Wang, Yong and Wen, Fang",
        "title": "Metaportrait: Identity-preserving talking head generation with fast personalized adaptation"
      },
      {
        "key": "zhou2021pose",
        "author": "Zhou, Hang and Sun, Yasheng and Wu, Wayne and Loy, Chen Change and Wang, Xiaogang and Liu, Ziwei",
        "title": "Pose-controllable talking face generation by implicitly modularized audio-visual representation"
      }
    ]
  }
]