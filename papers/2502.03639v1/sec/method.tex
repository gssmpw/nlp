\section{Method}
As mentioned earlier, current video models primarily learn moving objects as sequences of 2D pixels, without considering their true 3D states. Their understanding of the physical world is limited to a projected plane, making it challenging to resolve changes in shape and motion that are not visible in the projected view. To address this, we propose incorporating a partial 3D representation into the generation pipeline. The inclusion of 3D information as a second modality will not only enhance the generation capability of the video model but also guide the generation of RGB videos through joint optimization.


In this section, we outline the details of our 3D-aware video generation pipeline, which consists of three essential components: First, we introduce our novel 3D-aware video dataset, PointVid, in \autoref{sec:method:dataset}, providing videos with spatial information orthogonal to color dimensions. Next, in \autoref{sec:method:training}, we present our joint training approach, enhancing the video modelâ€™s ability to incorporate 3D geometry and dynamics. Finally, we describe our 3D regularization strategy in \autoref{sec:method:regularization}, designed to resolve non-physical artifacts in the generated results using the newly incorporated modality.

\begin{figure}
\includegraphics[width=\columnwidth]{figs/data-gen}
\caption{\textbf{PointVid Dataset Generation Workflow}. Given an input video, we use the first frame as a reference frame and perform semantic segmentation to obtain masks for foreground objects. Next, we randomly sample pixels with a distribution favoring pixels inside foreground objects. We perform 3D point tracking on these queried pixels, and map these points to the input video frames. The resulting data point contains 3D coordinates of tracked foreground pixels while remaining pixels are zeroed out.}\label{fig:data-gen}
\end{figure}

\subsection{PointVid Dataset}\label{sec:method:dataset}
We build our 3D-aware video dataset upon standard 3-channel videos of shape $[T, H, W, C]$, where $C=3$ encodes the RGB channels, height $H$ and width $W$ digitize 2D images into pixels, and $T$ represents the temporal dimension that sequences the pixel data over time.

The key challenge lies in selecting a 3D representation that is suitable for learning by a video model. Explicit 3D representations, such as meshes (surface or volumetric), point clouds, or voxels, can effectively model geometry but often have variable sizes due to irregular object shapes, complicating the diffusion training process. Moreover, aligning 3D points with 2D pixels in video remains a non-trivial task. In contrast, implicit representations like NeRF \cite{mildenhall2021nerf} typically require substantial time to convert into an explicit form for direct usage, making them impractical for training purposes.


Recent advancements in 3D point tracking \cite{xiao2024spatialtracker} have made this task more feasible. Instead of representing moving objects as sequences of meshes or point clouds with arbitrary resolution, we focus exclusively on tracking the movements of pixels. Here, 2D pixels are unprojected into 3D space and tracked throughout the video. This allows the 3D locations of the pixels to be treated as three additional attributes, similar to color intensities. As a result, we can format the 3 spatial coordinates as extra channels, yielding a point tracking tensor $\mathcal{P}$ of shape $[T, H, W, C]$, matching the dimensions of the RGB video. In practice, we store the 3D tracking information using 2D pixel coordinates $(u, v)$ and depth $d$, requiring only a fixed camera unprojection to reconstruct the 3D world coordinates $(x, y, z)$. By using $(u, v, d)$, the first two channels correspond to the pixel's position projected onto the 2D plane, which aligns precisely with the moving point in the RGB video, thus simplifying the task of learning the alignment between video and point tracking.

We visualize the workflow of generating point tracking in \autoref{fig:data-gen}. For each RGB video $\mathcal{V}$, we extract the first frame as a reference image and perform semantic segmentation using Grounded-SAM-2 \cite{ren2024grounded,ravi2024sam2segmentimages}, yielding masks for the foreground objects. Next, we randomly sample pixels on the reference image with a main focus on those inside the foreground objects. These queried pixels are then tracked with trajectories of 3D Cartesian coordinates $(x, y, z)$ throughout the video using SpaTracker \cite{xiao2024spatialtracker}. For reconstruction purposes, ZoeDepth \cite{bhat2023zoedepth} is employed for depth estimation. Finally, we postprocess the tracked results to align the shape with the video: we fill out the pixels in the foreground mask to store the pixel's spatial positions, formatted as 3 channels, and discard pixels in the background (setting all background pixels to zero). Since we apply sparse queries for tracking efficiency, some foreground pixels may not have a corresponding trajectory; 
we use KDTree searching to interpolate the positions for such pixels.

One remaining issue we encounter in the tracked results is that they sometimes contain high-frequency noise, possibly due to the imprecision of the tracking. This causes problems when the video model attempts to learn from its distribution, as the backbone diffusion model relies on sampling with random noise. To address this, we apply a Kalman Filter for 3D point tracking to remove temporal high-frequency noise and recover its primary motion.


\begin{table*}[t]
    \centering
    \caption{\textbf{Quantitative Evaluation.} We evaluate various aspects of our method using the VBench~\cite{huang2024vbench} and VideoPhy~\cite{bansal2024videophy} benchmarks. The evaluated metrics are as follows: (VBench) SC: subject consistency, BC: background consistency, MS: motion smoothness, AQ: aesthetic quality, IQ: imaging quality; (VideoPhy) PC: physical commonsense. By incorporating 3D knowledge, our video model shows substantial improvement in metrics such as physical commonsense, motion smoothness, and subject/background consistency. This demonstrates that our method generates significantly more temporally consistent and physically plausible videos.} 
    \begin{tabular}{lcccccccc}
        \toprule
        Method &
        SC $\uparrow$ & BC $\uparrow$ & MS $\uparrow$ & AQ $\uparrow$ & IQ $\uparrow$ & PC $\uparrow$ % & PC2
        \\\midrule
        I2VGen-XL & 0.83247 & 0.89147 & 0.95706 & \textbf{0.44055} & 0.58532 & 0.32665 \\
        Ours & \textbf{0.95892} & \textbf{0.95202} & \textbf{0.98456} & 0.43369 & \textbf{0.60423}  & \textbf{0.37434} \\
        \bottomrule
    \end{tabular}
    \label{tab:score}
\end{table*}

\subsection{Point Augmentation}\label{sec:method:training}
With our 3D-aware video dataset, we now aim to extend the capabilities of the video model to generate not only the RGB video $\mathcal{V}$ but also the corresponding 3D representation $\mathcal{P}$. Aligning the video with the point cloud enables the model to inherently acquire 3D knowledge. The augmented video-point model learns not only how the object appears in a 2D color image but also its 3D geometry and location.

Our augmentation pipeline is visualized in \autoref{fig:pipeline}. It is worth noting that training the video-point model entirely from scratch is unnecessary, as training a video diffusion model already demands substantial computational resources and vast amounts of video data. Therefore, instead of training from scratch, it is more practical to fine-tune an existing video model to incorporate the point-tracking modality. In this work, we employ a similar architecture to the video model in I2VGen-XL \cite{zhang2023i2vgen, wang2023modelscope}, a UNet-based latent diffusion model conditioned on image and text prompts. 

To collectively learn from and generate $\mathcal{V}$ and $\mathcal{P}$, the straightforward way would be to feed them together to the diffusion model. Since we are matching the shapes of video and point tensors, we can concatenate them along the channel dimension, resulting in an point-augmented video $\mathcal{VP}$ with a shape of $[T, H, W, 2C]$. This concatnated video can be seamlessly integrated into the UNet by simply adjusting the input/output dimensions to accommodate the additional channels. 
This increase in dimensionality does not overshadow the weights of the original model; instead, only new dimensions are introduced to handle the added modality. To leverage pretrained RGB weights, we can load the pretrained model for the corresponding RGB channels and initialize the remaining weights as zeros. For the conditioning frame, we continue using the first RGB frame and repeat the condition image tensor to prompt the generation of both video and point tracking, as our goal is for the model not to rely on the initial point positions during the inference stage. This strategy is effective because the same reference frame is used for generating the point-tracking data earlier.

In addition to the pixel-wise alignment of video and point data in creating our dataset, we introduce a cross-attention layer at the entry of the UNet. This layer connects the first and second halves of the channel dimensions, corresponding to color information and spatial location in the latent dimension, to further enhance the alignment between them.

Incorporating the aforementioned changes, The training pipeline then proceed as usual: we encode $\mathcal{VP}$ to latent space $z$, noise it to $z_t$ with randomly sampled noise $\epsilon$ and diffusion step $t$. The UNet model then predict the added noise and compare with true noise, where we adopt DDIM scheduling for the model. With the modified architecture, we fine-tune the diffusion model on \textbf{PointVid} to obtain our video-point generation model. Given an RGB image and a text prompt, our model can generate both the video and the corresponding 3D trajectories.






\subsection{Point Regularization}\label{sec:method:regularization}

With the video closely aligned to 3D trajectories, we leverage this additional modality to enhance RGB video generation. To achieve this, we introduce regularization terms on the point tracking component of the diffusion output, aligning it with the ground truth, which in turn implicitly improves the RGB video output. (see \autoref{fig:pipeline}).

The first component we need is to generate directly usable point trackings for evaluation in 3D space. Since including random noise of any level in the point cloud would likely render it unusable, we employ DDIM sampling to recover a noise-free stage before applying regularization. In each training step with a latent input $z$ and corresponding noisy stage $z_t$, We apply DDIM denoising to recover $z_0$ iteratively. Following the approach of \cite{yuan2024instructvideo}, we utilize gradient checkpointing to freeze the gradient flow except during the final step, preventing memory overflow. The denoised data $z_0$ is subsequently decoded into explicit 3D point tracking $\tilde{\mathcal P}$ as the sampled 3D trajectory of the video.

\begin{figure}
\includegraphics[width=\columnwidth]{figs/point-reg}
\caption{\textbf{Point Regularization.} The reconstructed point cloud in the diffusion output often contains noise and deformations (middle). This issue is mitigated using our point regularization (right). The synthetic point cloud above (\eg \emph{box} and \emph{shoes} falling on the ground) is generated by Kubric ~\cite{kubric} and trained with our pipeline.}\label{fig:point-reg}
\end{figure}


With the explicit modeling of trajectories as a 3D representation, we introduce regularization terms to improve the quality of the generated results, focusing on motion fidelity and temporal smoothness. For simplicity, let us assume $\tilde{\mathcal P}$ is reshaped to $T \times N \times 3$, and we use $\tilde{\mathcal P}^\tau$ to denote the point cloud of shape $N \times 3$ at time $\tau$.


A common issue that arises during point tracking generation is the presence of excessive noise (see \autoref{fig:point-reg} top middle). This noise can originate from various sources, such as the variational autoencoder (VAE) used to encode $\mathcal P$ into the latent space. High-frequency noise in both spatial and temporal dimensions tends to obscure the main shape and motion, thereby impairing the video model's ability to track 3D points effectively. To mitigate noise and increase fidelity of reconstruction, we propose the reconstruction loss $\mathcal{L}_{\text{recon}}$ as follows:
\begin{multline}
    \mathcal{L}_{\text{recon}} = c_0\sum_{\tau=0}^T\|\tilde{\mathcal P}^\tau - \mathcal P^\tau\| + c_1 \sum_{\tau=1}^T \|\tilde{\mathcal P}^\tau - \tilde{\mathcal P}^{\tau-1}\| \\ + c_2 \sum_{\tau=2}^T \|\tilde{\mathcal P}^\tau - 2\tilde{\mathcal P}^{\tau-1} + \tilde{\mathcal P}^{\tau-2}\|.
\end{multline}
Here the first term penalizes the difference between generated point cloud and the noise-free ground truth $\mathcal P$, ensuring generation fidelity, while the second term penalizes the difference between consecutive time step to encourage temporal smoothness; the third term further enforeces acceleration smoothness, discouraging abrupt changes in velocity. We selected the weights so that each term are about the same scale initially. 

Apart from the microscopic high-frequency noise, we observe a macroscopic shape deformation in many of the generated point cloud (see \autoref{fig:point-reg} bottom middle). Such deformation of arbitrary degree is undesirable as most solid objects, including human body, inderently requies conservation of volumn and local rigidity. Violating this can lead to unnatural shape deformation and morphing, which is considerd undesired in our generation task. Hence we propose the rigidity loss $\mathcal{L}_{\text{rigid}}$ to enforce local shape preservation:
\begin{align}
    \mathcal{L}_{\text{rigid}} = \sum_{\tau=1}^T \sum_{i} \sum_{j \in \mathcal{N}(i)} \left(\text{dist}(\tilde{\mathcal P}_i^\tau, \tilde{\mathcal P}_j^\tau)-\text{dist}(\tilde{\mathcal P}_i^0, \tilde{\mathcal P}_j^0)\right)^2.
\end{align}
Here, we apply a kNN search on the reference frame to construct local neighbor pairs, then penalize changes in distance between these pairs.

By combining reconstruction and rigidity loss, we enhance the generation quality of the 3D point cloud, as shown in \autoref{fig:point-reg} (right). This improvement in the point space implicitly promotes shape preservation and smooth motion transitions in the RGB video, as our video-point model closely aligns the two modalities. To prevent the geometric regularizations from dominating the semantic aspects, we jointly optimize these regularization terms with diffusion losses during training, resulting in an overall loss of the form:
\begin{align}
    \mathcal{L} = \lambda_{\text{ddim}}\mathcal{L}_{\text{ddim}}  + \lambda_{\text{recon}} \mathcal{L}_{\text{recon}} + \lambda_{\text{rigid}} \mathcal{L}_{\text{rigid}}
\end{align}




