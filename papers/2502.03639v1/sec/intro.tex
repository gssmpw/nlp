\section{Introduction}
The recent development of video diffusion models has attracted significant research interests and seen tremendous progress~\cite{blattmann2023stablevideodiffusionscaling, singer2023makeavideo,ho2022imagenvideohighdefinition}. 
With vast amounts of training data and sophisticated architectures, video models have greatly improved in expressiveness and aesthetics~\cite{videoworldsimulators2024,genmo2024mochi}. Today, state-of-the-art video models can realistically represent content from the real world with high creativity, enabling various applications in the entertainment industry and scientific research~\cite{Menapace_2024_CVPR}. 
However, existing video diffusion models primarily focus on improving the appearance of content and the smoothness of motion. 
The models are trained in a way to understand the movements of 2-dimensional pixels, where the object movements in 3D space is approximated through the changing of RGB values.



As a result, the underlying 3-dimensional shapes and motions, which represent how objects truly exist in the physical world, are not learned well in the existing video generation models.
Even worse, we observe that when the video diffusion models neglect the underlying 3D information, they struggle to understand 
how objects occupy space, change shape and location, and interact with each other, which is essential for generating videos with complex motion.
For instance, imaging a situation when perceiving a scene with a human performing specific tasks, \eg, \textit{hands tying shoelaces}, viewpoint projection and occlusion make it impossible to capture the true 3D shape of the hand and the shoelaces using 2D pixels, complicating the modeling of how they make contact. 
Without understanding the underlying geometry, objects in the video may change shape arbitrarily or even appear and disappear suddenly, leading to the common issue of \textit{object morphing} (as example results from prior arts in Fig.~\ref{fig:comparison-task-all}). 
This problem is exacerbated for complex objects like humans; without 3D awareness, limbs and body parts may move unnaturally or change shape abruptly, resulting in dislocated arms or twisted bodies.

To address the above issue, we aim to improve video diffusion models towards the better understanding of physical world, thus generating more reasonable shape and motion.
Previous works have attempted to control shape and motion directly in 2D space~\cite{savantaira2024motioncraft, liu2024physgen}. Nonetheless, without a comprehensive understanding of 3D geometry, these methods are limited to directing pixel movement confined within the 2D plane, making them unable to handle out-of-plane motions or fully resolve 3D dynamics~\cite{liu2024physgen}. One straightforward approach could be to model the complete 3D geometry of objects using representations such as NeRF \cite{mildenhall2021nerf} or 3D Gaussian-Splatting \cite{kerbl20233d}, essentially leading to 4D generation \cite{bahmani20244d, zhao2023animate124}. 
While these methods provide accurate 3D understanding, they are computationally intensive and generally limited to generating simple scenes with only a few specific types of objects.


\begin{figure*}[t]
\includegraphics[width=\textwidth]{figs/pipeline-2.pdf}
\caption{\textbf{Training Pipeline Overview.} We sample video-point pairs, concatenate them in channel dimensions and used to train a UNet. In addition to standard condition and latent cross attention, we further add cross attention between video and point in corresponding channels for a better alignment between the two modalities. Furthermore, the 3D information from the points is utilized to regularize the RGB video generation by applying a misalignment penalty to the video diffusion process.
 }\label{fig:pipeline}
\end{figure*}

To bridge the gap between leveraging 3D information for better video generation, we propose a method to augment and regularize video diffusion models using 3D point cloud. Specifically, our model enriches 2D videos with out-of-dimension data, which is absent in traditional RGB video models, without requiring full-scale 3D reconstruction. Instead of generating a complete mesh or point cloud for video diffusion model, we track the movement of 2D pixels in 3D space, creating a pseudo-3D representation of objects in the video. To enable such optimization, we further develop a 3D-aware video dataset, named \textbf{PointVid}, with segmented objects in pixel space and their corresponded 3D dynamic states. We fine-tune video diffusion models on the \textbf{PointVid} dataset to extend its generation capability to 3D shape and motion, enhancing the model to better understand object behavior in the physical world. Leveraging prior knowledge of 3D space, we utilize this additional information to guide pixel movements towards more physically plausible outcomes.

We demonstrate the effectiveness of our approach through extensive experiments, primarily focusing on handling complex scenarios with interacting objects and accurately resolving dynamic contacts. Guided by 3D trajectories, our model achieves superior quality in terms of smooth transitions of shape and motion compared to existing works, generating visually plausible videos. We summarize our contributions as follows:

% \begin{enumerate} [leftmargin=18pt,nosep]
\begin{enumerate} [nosep]

    
    \item \textbf{Injecting 3D Knowledge into Video Diffusion.} We propose a novel \emph{3D point augmentation} strategy to bridge the 3D spatial domain of the physical world with the pixel space of 2D videos. By explicitly incorporating 3D information into the video diffusion process, our method significantly enhances video generation quality, especially in challenging scenarios such as \emph{solid object interactions}.

  \item \textbf{Regularization with 3D Prior.} We introduce a novel regularization strategy to guide the point cloud diffusion process, ensuring that the generated point cloud is well-structured. This structured point cloud, in turn, enhances the RGB video generation by providing robust 3D priors for better spatial coherence and alignment.
    
    \item \textbf{3D-Aware Video Dataset.} We curate a 3D-aware video dataset, \textbf{PointVid}, by tracking 3D points in the first frame of a given video. We segment the first frame to obtain foreground objects and mainly track these points in the original video. The resulting dataset contains 3D coordinates corresponding to the first frame, pixel aligned with the video dataset.
  
    
\end{enumerate}
