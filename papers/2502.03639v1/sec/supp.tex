\clearpage
% \setcounter{page}{1}
\maketitlesupplementary

\begin{appendix}
Supplementary to the main paper, we provide more details on dataset creation in \autoref{sec:supp:datagen}, additional qualitative results in \autoref{sec:supp:comparison}, additional results for ablation studies in \autoref{sec:supp:ablation}, and a human preference evaluation in \autoref{sec:supp:userstudy}.


\section{PointVid Dataset Generation}
\label{sec:supp:datagen}
In this section, we present additional details on generating our 3D-aware video dataset, \textbf{PointVid}. A visualization of the generation pipeline is provided in the main paper. Our pipeline requires no additional annotations beyond the video-text pairs in the dataset. For our experiments, we utilize Panda70M \cite{chen2024panda}.

\paragraph{Preparation}
Starting with any video-text pair, we trim the videos to a fixed length $T$ and shape $H\times W$, extracting the first frame of the clipped video as the \textit{reference frame}. We then apply semantic segmentation to the reference frame using Grounded-SAM2 \cite{ren2024grounded, ravi2024sam2segmentimages} to automatically generate masks for foreground objects. To streamline the process, we utilize a language model, LMDeploy \cite{2023lmdeploy}, to infer the main moving objects in the scene based on the video's caption. For example, the caption \textit{A man swinging a baseball bat in a studio} results in \textit{\{man, baseball bat\}}. This object-level label is subsequently fed to Grounded-SAM2 for prompt-based segmentation.

\paragraph{Point Tracking}
Having obtained a reference frame and its segmentation, we apply SpaTracker \cite{xiao2024spatialtracker} to track pixel movements accordingly. We use the same reference frame to query points during tracking so that the 3D positions align with the first frame's pixel locations initially. We set \textit{downsample} to 1 and \textit{gridsize} to 80 in \cite{xiao2024spatialtracker} to enable sparse tracking. For a 448$\times$256 image, we observe that sparse tracking at this resolution provides a sufficient number of pixels while remaining relatively efficient in terms of time. Increasing the resolution or using dense tracking could potentially enhance the results, but we leave this for future exploration. The model outputs point tracking data of shape $T \times N \times 3$, where $N$ is the number of sampled points.

\paragraph{Post-processing}
To mitigate temporal noise caused by inaccurate tracking, we first apply a Kalman Filter to the 3D point cloud to estimate a global velocity and eliminate high-frequency noise. The result is then reshaped to $T \times H \times W \times 3$, where the spatial coordinates of pixels are treated as three channels, similar to RGB. To address the difference in shape between the $N$ tracked points and the video resolution, we apply the following rules: if a tracked pixel is not within the foreground mask, we discard its value (effectively setting all background pixels to zero). Otherwise, we assign its value to the corresponding pixel entry in the tensor. Since we use sparse tracking, only a subset of foreground pixels have values. We then apply KDTree searching on the sparse pixels based on their $uv$ coordinates in the first frame. For all untracked pixels within the foreground mask, we approximate their trajectory by interpolating their three nearest tracked pixels. The pseudocode for post-processing is summarized in \autoref{alg:post_processing}.

\begin{algorithm}[H]
\caption{Post-processing for Point Tracking}
\label{alg:post_processing}
\begin{algorithmic}[1]
\Require Tracked points $\mathbf{P} \in \mathbb{R}^{T \times N \times 3}$, Foreground mask $\mathbf{M} \in \{0, 1\}^{H \times W}$, Resolution $(H, W)$
\Ensure Processed tensor $\mathcal{P} \in \mathbb{R}^{T \times H \times W \times 3}$

\State \textbf{Initialize:} $\mathcal{P} \gets 0$ \Comment{Initialize all values to zero}
\State $\mathbf{P} \gets \text{KalmanFilter3D}(\mathbf{P})$ \Comment{Reduce temporoal noise}
\State $\mathbf{P}_0 \gets \mathbf{P}[0,:,0:2]$ \Comment{Initial $uv$ of 2D pixels}
\State $\mathbf{P}_1 \gets \mathbf{P}_0 \bigcap \mathbf{M}$ \Comment{Filter foreground pixels}
\State $\mathbf{Tr} \gets \text{KDTree}(\mathbf{P}_1)$ \Comment{Use initial $uv$ for searching}
% \For{each frame $t \in [1, T]$}
    \For{each foreground pixel $M[u, v] = 1$}
        \If{found $\mathbf{P}_1[j] = (u,v)$} \Comment{Tracked pixel}
            \State $\mathcal{P}[:,u,v,:] \gets \mathbf{P}[:, j, :]$
        \Else \Comment{Untracked pixel}
            \State $\text{ind} \gets \text{Query}(\mathbf{Tr}, (u,v), k=3)$
            \State $\mathcal{P}[:,u,v,:] \gets \text{Interp}(\mathbf{P}[:,\text{ind},:])$
        \EndIf
    \EndFor
\State \Return $\mathcal{P}$
\end{algorithmic}
\end{algorithm}



\section{Additional Qualitative Results}
\label{sec:supp:comparison}
We provide additional qualitative evaluations of our model compared to three other models: I2VGen-XL \cite{zhang2023i2vgen}, SVD \cite{blattmann2023stablevideodiffusionscaling}, DynamiCrafter \cite{xing2025dynamicrafter}. These include both \textit{task-oriented} videos (see \autoref{fig:supp-task}) and videos from non-specific categories (see \autoref{fig:supp-all}), such as humans, animals, and general objects. Our results demonstrate improved shape and motion consistency of objects, effectively minimizing common non-physical artifacts like object morphing.

\begin{figure*}
\includegraphics[width=\textwidth]{figs/supp-task_compressed}
\caption{\textbf{Comparison on Task-oriented Videos.} 
 }\label{fig:supp-task}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{figs/supp-all-2_compressed}
\caption{\textbf{Comparison on General Categories.} 
 }\label{fig:supp-all}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{figs/supp-ablation-all_compressed}
\caption{\textbf{Ablation Studies on Different Components.} We compare the results from (i) our full generation pipeline with (ii) model trained with RGB only, (iii) model trained with point augmentation only (no regularization), (iv) model without channel cross-attention, and (v) model trained without diffusion loss.}\label{fig:supp-ablation}
\end{figure*}

\newpage
\section{Additional Ablation Study}


\label{sec:supp:ablation}
We conduct ablation studies to evaluate the effectiveness of our training pipeline design. The results are visualized in \autoref{fig:supp-ablation}.

As discussed earlier, our two-stage design of point augmentation and regularization progressively enhances the video model. Compared to training the model with RGB videos only (\autoref{fig:supp-ablation} (i)), our point augmentation alone (\autoref{fig:supp-ablation} (ii)) injects 3D awareness into the model and improves its ability to perceive 3D shapes, as evidenced by more consistent object shapes in the videos. Our point regularization (\autoref{fig:supp-ablation} (iii)) further improves quality by optimizing point generation and implicitly guiding RGB generation towards higher fidelity.

Although the video and point data are already pixel-aligned during generation, our channel-wise cross-attention mechanism is essential to ensure cross-dimensional and cross-modality alignment. Without the cross-attention (\autoref{fig:supp-ablation} (iv)), 3D regularization on points is not correctly transferred to 2D pixels, leading to degraded video quality. Furthermore, in the regularization stage, we adopt a joint training strategy that combines regularization loss with diffusion loss. We observe that dropping diffusion loss (\autoref{fig:supp-ablation} (v)) causes 3D information to dominate semantics, resulting in completely unusable output.


\section{User Study}
\label{sec:supp:userstudy}
Apart from the quantitative evaluation using VBench \cite{huang2024vbench} and VideoPhy \cite{bansal2024videophy}, we additionally conduct a user study on our model in comparison to the base model I2VGen-XL \cite{zhang2023i2vgen} to assess human preference. We ask 10 labelers to compare the same testing batch used in quantitative evaluation (387 video clips), focusing on identifying non-physical artifacts and evaluating overall physical plausibility. The following 5 questions are designed for the evaluation:

\begin{enumerate}
    \item Which video appears to have more physically realistic object movements and interactions? (Consider aspects like gravity, collisions, and the natural flow of objects.)
    \item (Negative) Which video contains more noticeable \textbf{non-physical artifacts}, such as object morphing, stretching, or sudden changes in shape?
    \item In which video do the objects maintain a more consistent size, shape, and appearance throughout the entire sequence?
    \item Which video better adheres to natural physics laws, such as consistent lighting, shadow behavior, and material properties (e.g., rigidity or fluidity)?
    \item Overall, which video feels more coherent and believable based on the physical interactions and behavior of objects?
\end{enumerate}

Given the evaluation questions, we ask users to compare our generated videos side by side with results from the base model and select the one that best fits each question. As shown in \autoref{tab:userstudy}, our method demonstrates significant improvement in terms of physical plausibility by incorporating 3D awareness into the video.

\begin{table}[h]
    \centering
    \caption{\textbf{User Study Results.} Our model demonstrates a significant improvement in physical plausibility, as assessed by human labelers. Here, Q2 asks the user to identify negative artifacts in the videos, while the other questions positively assess physical plausibility.} 
    \begin{tabular}{lcccccccc}
        \toprule
        Method &
        Q1 $\uparrow$ & Q2 $\downarrow$ & Q3 $\uparrow$ & Q4 $\uparrow$ & Q5 $\uparrow$ 
        \\\midrule
        I2VGen-XL & 0.138 & 0.862 & 0.135 & 0.132 & 0.137 \\
        Ours & \textbf{0.862} & \textbf{0.137} & \textbf{0.865} & \textbf{0.868} & \textbf{0.863} \\
        \bottomrule
    \end{tabular}
    \label{tab:userstudy}
\end{table}


\end{appendix}