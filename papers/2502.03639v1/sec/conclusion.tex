\section{Conclusion and Discussion}

To summarize, we propose a generation framework that incorporates 3D-awareness into video diffusion models. By tracking objects in 3D and aligning them in pixel space, we elevate traditional video datasets to a new dimension, revealing out-of-plane information previously inaccessible to video models. Through joint training, the video model learns to perceive 3D shape and motion, acquiring physical commonsense that is natural in 3D. We then apply regularization to refine the generation process and further enhance the results, eliminating artifacts such as object morphing. Our method has demonstrated a strong capability to handle content-rich scenarios, such as task-oriented videos, and shows superior visual plausibility when generating videos of general categories.

\paragraph{Limitations.}
We acknowledge a few limitations of our framework. The degree of 3D-awareness, rooted in our video-point joint training, is constrained by the resolution of our 3D points. Objects that are not sufficiently represented may lack a quality 3D prior and may still exhibit implausible artifacts. Additionally, the generation quality of our model is somewhat limited by the backend UNet model, and could potentially be improved by employing more advanced transformer-based models. We leave these aspects as directions for future exploration.