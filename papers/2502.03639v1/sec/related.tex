\section{Related Work}
\textbf{Video Diffusion Models.} Following the success of diffusion-based text-to-image (T2I) models~\cite{sd2.1,podell2023sdxl,deepfloyd,liu2024playgroundv3,flux,liu2023hyperhuman,kolors,li2024snapfusion,gao2024lumina,kag2024ascan}, many video generation models have been proposed in the literature~\cite{blattmann2023stablevideodiffusionscaling, singer2023makeavideo,Menapace_2024_CVPR,ho2022imagenvideohighdefinition}. These models allow video generation following input conditioning such as text and image. Video diffusion models can be categorized into three categories. First, auto-regressive models like VideoPoet~\cite{kondratyuk2024videopoetlargelanguagemodel} follow the causal language model paradigm of next token prediction given previous tokens. These models encode the input conditioning into latent tokens using tokenizers such as MAGVIT~\cite{yu2023magvit}.

Second, pixel-space diffusion models~\cite{Menapace_2024_CVPR,ho2022imagenvideohighdefinition,singer2023makeavideo} directly model the video synthesis on the pixel space to avoid artifacts arising in the compressed VAE latent spaces. While these models provide more photo-realism and better motion dynamics, they typically only offer low-resolution video generation due to a higher computational cost in generating high-resolution videos. These base models are augmented with super-resolution components to increase the video resolution resulting in deep cascaded diffusion models. Imagen-video~\cite{ho2022imagenvideohighdefinition}, a UNet-based architecture,  generates videos at a low-resolution and utilizes a sequence super-resolution models to increase the video resolution. Make-a-video~\cite{singer2023makeavideo} uses a T2I as prior to encode text input and trains a deep cascade of spatial and temporal layers to generate high resolution videos. SnapVideo~\cite{Menapace_2024_CVPR} replaces the popular UNet architecture design with a FIT network for improved generation efficiency.

Third, latent diffusion models transform the raw pixel space into a compressed 
spatio-temporal latent representation. It enables training a higher resolution and higher capacity base model instead of pixel-space diffusion models. These include Stable Video Diffusion~\cite{blattmann2023stablevideodiffusionscaling}, SORA~\cite{videoworldsimulators2024}, VideoCrafter2~\cite{chen2024videocrafter2}, ModelScopeT2V~\cite{wang2023modelscope}, Mochi~\cite{genmo2024mochi}, CogVideoX~\cite{yang2024cogvideox, hong2022cogvideo} etc. While latent models are typically single-stage pipelines, they can extend to deep cascade pipelines to offer even higher-quality video generations. MovieGen~\cite{polyak2024moviegencastmedia} proposed a transformer-based cascade latent diffusion model.

While our approach is applicable to different types of video diffusion models, we mainly use ModelScopeT2V~\cite{wang2023modelscope} as our base video-generation model and demonstrate how to incoporate 3D geometry and dynamics improve video generation.

\noindent\textbf{Dynamics Aware Video Generation.} There have been various works~\cite{savantaira2024motioncraft,liu2024physgen,zhang2024physdreamer} that instill physically plausible and temporally consistent dynamics in video generation. PhysGen~\cite{liu2024physgen} added simple control such as force or torque to an object in within the image, enabling simple physically plausible dynamics to the resulting video. MotionCraft~\cite{savantaira2024motioncraft} synthesized videos by incorporating optical flow in the noise diffusion process. This allows image animation with a temporally consistent video content. Physdreamer~\cite{zhang2024physdreamer} tried distilling dynamic priors of static 3D objects from the video generation models. It creates a physical material field around the 3D object, thereby easily synthesizing 3D dynamics under arbritary forces. Although these works are able to incorporate simple dynamics to a static image, they assume that the underlying diffusion model understands and captures the object dynamics. This assumption is not necessarily true, as pointed by VideoPhy~\cite{bansal2024videophy}, current video generation models suffer from many inconsistencies, both physically implausible as well as temporally inconsistent. There have been other works along the line of incorporating camera intrinsics during video generation. 3D-Aware Video-Generation~\cite{bahmani20233dawarevideogeneration}, first trains a model by jointing diffusing noise along a motion and content codes while sampling different camera poses. The model is trained using a generative adversarial network based discriminator which discriminates the resulting video, by difference between frames and time duration. This enables generating videos that are physically plausible. 


In contrast to the previous works, we incorporate 3D knowledge and dynamics by jointly diffusing the 3D geometry alongside the video modality. This helps improve the diffusion model understanding of the dynamics during training and improves the physical plausibility and temporal consistency of the generated videos. 


