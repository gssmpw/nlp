\begin{figure}[h]
\includegraphics[width=\columnwidth]{figs/compare-rgb-point-reg}
  \caption{\textbf{Ablation on Point Augmentation and Regularization.} Our point-augmented model demonstrates a degree of 3D-awareness compared to the model fine-tuned on video data alone, but it still exhibits some artifacts, which are mitigated through regularization.}\label{fig:compare-rgb-point-reg}
\end{figure}

\section{Experiments}
In this section, we demonstrate the effectiveness of our proposed method through a series of experiments. We collect around 70K videos from \cite{chen2024panda} and our proprietary dataset to form our training dataset. The videos are processed to a resolution of 448$\times$256 with 16 frames. During the augmentation stage, the video-point diffusion model is trained on 8 A100 GPUs with a batch size of 4. In the subsequent regularization stage, we train on a single A100 GPU with a batch size of 4. We evaluate the results both qualitatively and quantitatively, showcasing the model's successful handling of contact-rich scenarios and its strong shape/motion consistency.

\subsection{Qualitative Results}
Although our model is designed to generate sequences of 2D images, our point augmentation and regularization enhance its ability to handle inherently 3D motions.
To evaluate, we test our model on unseen images and visualize the corresponding RGB video. For comparison, we apply the same prompts to the baseline model I2VGen-XL \cite{zhang2023i2vgen}, as well as two additional models: SVD (stable-video-diffusion-img2vid-xt) \cite{blattmann2023stablevideodiffusionscaling} and DynamiCrafter \cite{xing2025dynamicrafter}.
For qualitative comparison, we focus primarily on the aspects of \textit{object interaction} and \textit{shape conservation}.

\paragraph{Task-oriented videos} A prevalent category of scenarios that make up a significant portion of real-world videos in the wild involves humans performing specific tasks (see \autoref{fig:comparison-task-all}), such as playing with a toy, cooking food, or handling tools. However, current video models often struggle to capture localized details, particularly how human hands interact with objects. The challenge arises from the complex 3D nature of the interacting parts, and relying solely on 2D data fails to capture how different parts shape, move, and contact. As a result, hands or objects may undergo severe distortion during interactions, leading to unrealistic artifacts. With our 3D-aware generation framework, our model effectively preserves the shape of both hands and objects, ensuring smooth transitions throughout the video, exhibiting consistent quality improvement over the compared models.

\paragraph{General videos} The strength of our model extends to generating high-fidelity videos across a variety of categories. In \autoref{fig:compare-all-2}, we showcase several categories: (a) dynamic objects, (b) static objects with a moving camera, (c/d) humans, and (e/f) animals. For objects with camera motion, our 3D-aware regularization enhances the modelâ€™s understanding of 3D shape and scene composition, effectively eliminating artifacts such as foreground object morphing and background degradation. For human subjects, our point augmentation prevents unnatural changes in shape and appearance. Additionally, our model accurately captures the shape and behavior of animals, avoiding morphing issues.

\subsection{Quantitative Results}
We quantitatively evaluate our model using batch inference on the test set. We apply the metrics to a test batch obtained from \cite{chen2024panda}, consisting of a total of 387 video clips. We compare the scores with the base model, I2VGen-XL, and summarize the results in \autoref{tab:score}.

First, we assess the general aspects of video generation quality using VBench \cite{huang2024vbench}, evaluating the content and aesthetic quality of the videos. The results show that our method outperforms in most categories. Notably, our point regularization contributes to significant improvement in content consistency.

Beyond the general aspects, we focus on how 3D-awareness enhances alignment with real-world physical principles. It is important to note that determining whether a 2D video is physically correct is largely subjective, as ground-truth 3D physical states are not available. Therefore, we use VideoPhy \cite{bansal2024videophy}, where the authors utilize human annotations to train a discriminator that provides a \textit{physical commonsense} score. VideoPhy defines physical commonsense based on various perspectives, such as solid mechanics, where solid objects are expected to retain their shape and size throughout the video. As shown in \autoref{tab:score}, our model achieves a higher score for alignment with physical principles.
\begin{figure*}[h]
\includegraphics[width=\textwidth]{figs/comparison-all-2_compressed}
\caption{\textbf{Comparison on General Videos.} We showcase the generated videos across various categories, including static and dynamic objects, humans, and animals. Our method ensures smooth transitions in object shape and motion and eliminates morphing artifacts.}\label{fig:compare-all-2}
\end{figure*}


\subsection{Ablation Study}
We conduct ablation studies to demonstrate the essential role of each component of our method, namely point augmentation and regularization (see \autoref{fig:compare-rgb-point-reg}). As a baseline, we finetune the diffusion model on our dataset using only video data to eliminate bias from training videos (\autoref{fig:compare-rgb-point-reg} left). Next, we perform inference using the jointly trained video-point model to highlight the improvement from point augmentation alone (\autoref{fig:compare-rgb-point-reg} middle). Finally, we evaluate the full model with regularization (\autoref{fig:compare-rgb-point-reg} right). We observe that the model trained exclusively with RGB video can generate videos that match the appearance and semantics of the prompts. However, it suffers from severe distortions in both hands and objects. By incorporating point tracking during training, we leverage the 3D information from \textbf{PointVid} to gain partial awareness of spatial locations. This enhancement is evident as objects in the scene (\eg, cellphone and white stars) maintain their correct 3D shapes. This partial awareness is further enhanced by our regularization steps, leading to better control of shape and motion, significantly reducing artifacts in the videos (\eg, hands now retain their shape).




