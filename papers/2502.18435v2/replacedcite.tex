\section{Related Work}
\paragraph{Reversal Curse} 
____ first investigates the "reversal curse" in LLMs, which refers to the phenomenon where models trained on forward text data struggle to perform well on inverse search tasks. ____ further discusses this issue and proposes that augmentation during the pretraining stage can help bridge the knowledge extraction performance gap in reverse entity mapping. In a similar vein, ____ suggest training a unified model that combines text data with augmented reversed or partially reversed data can mitigate the reversal curse.
These studies imply that autoregressively-trained language models tend to have a linear and unidirectional thinking process, and certain types of augmentation can faciliate the model in making complex connections between pieces of learned information to enable more intricate cross-referencing. Our research also demonstrates that the autoregressive nature of LLMs may introduce inductive biases rooted from the pretraining corpus. Instead of focusing on the "reversal curse," we suggest that knowledge extraction and reasoning may be more straightforward in the direction with lower conditional entropy.


\paragraph{Order of Reasoning} 
Previous works have also been exploring the reasoning order's impact to the reasoning performance. ____ first demonstrates that the sequence in which input and output data are organized significantly impacts the performance of sequence-to-sequence models and propose to search over possible orders during training to manage unstructured output sets.
Recently, ____ reveals a surprisingly consistent lower log-perplexity when predicting in L2R versus R2L, despite theoretical expectations of symmetry. The authors attributes this asymmetry to factors like sparsity and computational complexity. We also observe this difference yet we have another hypothesis rationale beyond theirs. 
____ shows that by reversing the digit order, prioritizing the least significant digit can improve LLMs's performance on arithmetic, which aligns with our findings in section~\ref{sec:sim}. 



Previous studies on sequence modeling have also delved into relaxing the conventional ``left-to-right'' autoregressive dependencies, primarily to facilitate rapid parallel generation ____ and non-monotonic sequential generation____. Text diffusion has recently emerged as a promising approach in terms of planning and controllability ____. It has shown to be more effective than LLM than language model (LLM), particularly for tasks that require bidirectional reasoning strategies such as sudoku and countdown games ____.


\paragraph{Multiple-Choice Questions (MCQs) for LLM evaluation}
MCQs have been widely used for evaluation LLM's reasoning and knowledge extraction abilities. 
____ demonstrates that LLMs exhibit a selection bias in MCQs, favoring certain option positions, and introduces a debiasing method to mitigate this issue.
____ examines how LLMs’ performance on MCQs is influenced by the order of answer options, finding that reordering can lead to huge performance variations. 
____ proposes reframing MCQs as a series of binary classifications, demonstrating that this approach significantly improves performance across various models and datasets.
____ highlights issues like positional biases and discrepancies compared to long-form generated responses, when using MCQs in evaluating LLMs.
____ discovers that the prediction of specific answer symbols is primarily attributed to a single middle layer’s multi-head self-attention mechanism, with subsequent layers increasing the probability of the chosen answer in the model’s vocabulary space.
In contrast to the previous work, our work first shows the connection between the preferred reasoning direction and the direction that has lower conditional entropy in MCQ evaluations.