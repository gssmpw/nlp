# Intro
@inproceedings{bengio2015scheduled,
  title={Scheduled sampling for sequence prediction with recurrent neural networks},
  author={Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  booktitle={NeurIPS},
  volume={28},
  year={2015}
}

@inproceedings{welleck2019non,
  title={Non-monotonic sequential text generation},
  author={Welleck, Sean and Brantley, Kiant{\'e} and Iii, Hal Daum{\'e} and Cho, Kyunghyun},
  booktitle={International Conference on Machine Learning},
  pages={6716--6726},
  year={2019},
  organization={PMLR}
}
@article{gu2019insertion,
  title={Insertion-based decoding with automatically inferred generation order},
  author={Gu, Jiatao and Liu, Qi and Cho, Kyunghyun},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={661--676},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}
@inproceedings{xulearning,
  title={Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation},
  author={Xu, Jin and Liu, Xiaojiang and Yan, Jianhao and Cai, Deng and Li, Huayang and Li, Jian},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{zhang2023planner,
  title={PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model},
  author={Zhang, Yizhe and Gu, Jiatao and Wu, Zhuofeng and Zhai, Shuangfei and Susskind, Josh and Jaitly, Navdeep},
  booktitle = {NeurIPS},
  year={2023}
}


# Related work
@article{golovneva2024reverse,
  title={Reverse training to nurse the reversal curse},
  author={Golovneva, Olga and Allen-Zhu, Zeyuan and Weston, Jason and Sukhbaatar, Sainbayar},
  journal={arXiv preprint arXiv:2403.13799},
  year={2024}
}

@article{allen2023physics_3_2,
  title={Physics of language models: Part 3.2, knowledge manipulation},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2309.14402},
  year={2023}
}

@article{allen2023physics_3_1,
  title={Physics of language models: Part 3.1, knowledge storage and extraction},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2309.14316},
  year={2023}
}

@article{papadopoulos2024arrows,
  title={Arrows of Time for Large Language Models},
  author={Papadopoulos, Vassilis and Wenger, J{\'e}r{\'e}mie and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:2401.17505},
  year={2024}
}

@article{berglund2023reversal,
  title={The reversal curse: Llms trained on" a is b" fail to learn" b is a"},
  author={Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  journal={arXiv preprint arXiv:2309.12288},
  year={2023}
}


@article{zhang2024reverse,
  title={Reverse That Number! Decoding Order Matters in Arithmetic Learning},
  author={Zhang-Li, Daniel and Lin, Nianyi and Yu, Jifan and Zhang, Zheyuan and Yao, Zijun and Zhang, Xiaokang and Hou, Lei and Zhang, Jing and Li, Juanzi},
  journal={arXiv preprint arXiv:2403.05845},
  year={2024}
}

@article{vinyals2015order,
  title={Order matters: Sequence to sequence for sets},
  author={Vinyals, Oriol and Bengio, Samy and Kudlur, Manjunath},
  journal={arXiv preprint arXiv:1511.06391},
  year={2015}
}


@inproceedings{zhang-etal-2020-pointer,
	title = "{POINTER}: Constrained Progressive Text Generation via Insertion-based Generative Pre-training",
	author = "Zhang, Yizhe  and
	Wang, Guoyin  and
	Li, Chunyuan  and
	Gan, Zhe  and
	Brockett, Chris  and
	Dolan, Bill",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.emnlp-main.698",
	doi = "10.18653/v1/2020.emnlp-main.698",
	pages = "8649--8670",
}
@inproceedings{gu-kong-2021-fully,
	title = "Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade",
	author = "Gu, Jiatao  and
	Kong, Xiang",
	booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.findings-acl.11",
	doi = "10.18653/v1/2021.findings-acl.11",
	pages = "120--133",
}
@inproceedings{gu2018non,
	title={Non-autoregressive neural machine translation},
	author={Gu, Jiatao and Bradbury, James and Xiong, Caiming and Li, Victor OK and Socher, Richard},
	booktitle={ICLR},
	year={2018}
}
@inproceedings{ghazvininejad-etal-2019-mask,
	title = "Mask-Predict: Parallel Decoding of Conditional Masked Language Models",
	author = "Ghazvininejad, Marjan  and
	Levy, Omer  and
	Liu, Yinhan  and
	Zettlemoyer, Luke",
	booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D19-1633",
	doi = "10.18653/v1/D19-1633",
	pages = "6112--6121",
}

@inproceedings{zhang2023planner,
  title={PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model},
  author={Zhang, Yizhe and Gu, Jiatao and Wu, Zhuofeng and Zhai, Shuangfei and Susskind, Josh and Jaitly, Navdeep},
  booktitle = {NeurIPS},
  year={2023}
}

@article{ye2024beyond,
  title={Beyond autoregression: Discrete diffusion for complex reasoning and planning},
  author={Ye, Jiacheng and Gao, Jiahui and Gong, Shansan and Zheng, Lin and Jiang, Xin and Li, Zhenguo and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2410.14157},
  year={2024}
}

@article{gong2024scaling,
  title={Scaling Diffusion Language Models via Adaptation from Autoregressive Models},
  author={Gong, Shansan and Agarwal, Shivam and Zhang, Yizhe and Ye, Jiacheng and Zheng, Lin and Li, Mukai and An, Chenxin and Zhao, Peilin and Bi, Wei and Han, Jiawei and others},
  journal={arXiv preprint arXiv:2410.17891},
  year={2024}
}

@article{Li-2022-DiffusionLM,
  title={Diffusion-LM Improves Controllable Text Generation},
  author={Xiang Lisa Li and John Thickstun and Ishaan Gulrajani and Percy Liang and Tatsunori Hashimoto},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.14217}
}

@article{li2024can,
  title={Can multiple-choice questions really be useful in detecting the abilities of LLMs?},
  author={Li, Wangyue and Li, Liangzhi and Xiang, Tong and Liu, Xiao and Deng, Wei and Garcia, Noa},
  journal={arXiv preprint arXiv:2403.17752},
  year={2024}
}

@article{ghosal2022two,
  title={Two is better than many? binary classification as an effective approach to multi-choice question answering},
  author={Ghosal, Deepanway and Majumder, Navonil and Mihalcea, Rada and Poria, Soujanya},
  journal={arXiv preprint arXiv:2210.16495},
  year={2022}
}

@article{pezeshkpour2023large,
  title={Large language models sensitivity to the order of options in multiple-choice questions},
  author={Pezeshkpour, Pouya and Hruschka, Estevam},
  journal={arXiv preprint arXiv:2308.11483},
  year={2023}
}



@inproceedings{zheng2023large,
  title={Large language models are not robust multiple choice selectors},
  author={Zheng, Chujie and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}


@article{wiegreffe2024answer,
  title={Answer, assemble, ace: Understanding how transformers answer multiple choice questions},
  author={Wiegreffe, Sarah and Tafjord, Oyvind and Belinkov, Yonatan and Hajishirzi, Hannaneh and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2407.15018},
  year={2024}
}

# Method
@article{mirzadeh2024gsm,
  title={Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2410.05229},
  year={2024}
}

@article{kambhampati2024can,
  title={Can large language models reason and plan?},
  author={Kambhampati, Subbarao},
  journal={Annals of the New York Academy of Sciences},
  volume={1534},
  number={1},
  pages={15--18},
  year={2024},
  publisher={Wiley Online Library}
}

@article{valmeekam2024llms,
  title={LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench},
  author={Valmeekam, Karthik and Stechly, Kaya and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2409.13373},
  year={2024}
}

@article{abbe2024far,
  title={How Far Can Transformers Reason? The Locality Barrier and Inductive Scratchpad},
  author={Abbe, Emmanuel and Bengio, Samy and Lotfi, Aryo and Sandon, Colin and Saremi, Omid},
  journal={arXiv preprint arXiv:2406.06467},
  year={2024}
}



# Evaluation
@inproceedings{talmor-etal-2019-commonsenseqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon and
      Herzig, Jonathan and
      Lourie, Nicholas and
      Berant, Jonathan",
    booktitle = "ACL",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1421",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158",
}

@inproceedings{zellers-etal-2019-hellaswag,
    title = "{H}ella{S}wag: Can a Machine Really Finish Your Sentence?",
    author = "Zellers, Rowan and
      Holtzman, Ari and
      Bisk, Yonatan and
      Farhadi, Ali and
      Choi, Yejin",
    booktitle = "ACL",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1472",
    doi = "10.18653/v1/P19-1472",
    pages = "4791--4800",
}

@article{bisk2019piqa,
    title={PIQA: Reasoning about Physical Commonsense in Natural Language},
    author={Bisk, Yonatan and Zellers, Rowan and Bras, Ronan Le and Gao, Jianfeng and Choi, Yejin},
    journal={arXiv preprint arXiv:1911.11641},
    year={2019}
}


@inproceedings{sap-etal-2019-social,
    title = "{S}ocial{IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
    publisher = "Association for Computational Linguistics",
}


@inproceedings{clark2018think,
    title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
    author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
    booktitle={AAAI},
    year={2018}
}

@article{hendrycks2021measuring,
    title={Measuring Massive Multitask Language Understanding},
    author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
    journal={arXiv preprint arXiv:2009.03300},
    year={2021}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}


@article{mihaylov2018openbookqa,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@inproceedings{
penedo2024the,
title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale},
author={Guilherme Penedo and Hynek Kydl{\'\i}{\v{c}}ek and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=n6SCkn2QaG}
}

@misc{lin2021truthfulqa,
    title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
    author={Stephanie Lin and Jacob Hilton and Owain Evans},
    year={2021},
    eprint={2109.07958},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{liu2020logiqa,
  title={Logiqa: A challenge dataset for machine reading comprehension with logical reasoning},
  author={Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
  journal={arXiv preprint arXiv:2007.08124},
  year={2020}
}

@inproceedings{amini-etal-2019-mathqa,
    title = "{M}ath{QA}: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
    author = "Amini, Aida  and
      Gabriel, Saadia  and
      Lin, Shanchuan  and
      Koncel-Kedziorski, Rik  and
      Choi, Yejin  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1245",
    doi = "10.18653/v1/N19-1245",
    pages = "2357--2367",
}

@inproceedings{holtzman-etal-2021-surface,
    title = "Surface Form Competition: Why the Highest Probability Answer Isn`t Always Right",
    author = "Holtzman, Ari  and
      West, Peter  and
      Shwartz, Vered  and
      Choi, Yejin  and
      Zettlemoyer, Luke",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.564/",
    doi = "10.18653/v1/2021.emnlp-main.564",
    pages = "7038--7051",
    abstract = "Large language models have shown promising results in zero-shot settings. For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition{---}wherein different surface forms compete for probability mass, even if they represent the same underlying concept in a given context, e.g. {\textquotedblleft}computer{\textquotedblright} and {\textquotedblleft}PC.{\textquotedblright} Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated and uncalibrated scoring functions on all GPT-2 and GPT-3 models on a variety of multiple choice datasets."
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}


## other
@article{shorten2017effectiveness,
  title={The Effectiveness of Data Augmentation in Image Classification using Deep Learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M.},
  journal={arXiv preprint arXiv:1712.04621},
  year={2017}
}

@article{kumar2023survey,
  title={Image Data Augmentation Approaches: A Comprehensive Survey and Future Directions},
  author={Kumar, Teerath and others},
  journal={arXiv preprint arXiv:2301.02830},
  year={2023}
}

@article{hu2024learning,
  title={Learning to Achieve Goals with Belief State Transformers},
  author={Hu, Edward S and Ahn, Kwangjun and Liu, Qinghua and Xu, Haoran and Tomar, Manan and Langford, Ada and Jayaraman, Dinesh and Lamb, Alex and Langford, John},
  journal={arXiv preprint arXiv:2410.23506},
  year={2024}
}

@article{dubey2024llama3,
  title={The Llama 3 Herd of Models},
  author={Dubey, Abhishek and Jauhri, Ayush and Pandey, Ankit and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Alexander and Mathur, Anshul and Schelten, Alan and Yang, Angela Fan and Goyal, Ankush and Hartshorn, Adam and Yang, Ailin and Mitra, Anirban and Sravankumar, Akhila and Korenev, Andrey and Hinsvark, Alex and Rao, Ananth and Zhang, Aojun and Rodriguez, Armando and Gregerson, Austin and Spataru, Bogdan and Roziere, Baptiste and Biron, Benjamin and Tang, Brian and Chern, Brian and Caucheteux, Caleb and Nayak, Chitwan and Bi, Chunting and Marra, Carlo and McConnell, Chris and Keller, Christopher and Touret, Clement and Wu, Chao and Wong, Curtis and Ferrer, Carlos and Nikolaidis, Christos and Allonsius, Daan and Song, Da and Pintz, Daniel and Livshits, Denis and Esiobu, David and Choudhary, Divyam and Mahajan, Dhruv and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{pytorch2024torchtune,
  title={torchtune: PyTorch native post-training library},
  author={PyTorch Team},
  year={2024},
  howpublished={\url{https://github.com/pytorch/torchtune}},
  note={Accessed: 2025-02-15}
}

@misc{li2024datacomplm,
      title={DataComp-LM: In search of the next generation of training sets for language models},
      author={Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Yitzhak Gadre and Hritik Bansal and Etash Kumar Guha and Sedrick Keh and Kushal Arora and Saurabh Garg and Rui Xin and Niklas Muennighoff and Reinhard Heckel and Jean Mercat and Mayee F. Chen and Suchin Gururangan and Mitchell Wortsman and Alon Albalak and Yonatan Bitton and Marianna Nezhurina and Amro Abbas and Cheng-Yu Hsieh and Dhruba Ghosh and Josh Gardner and Maciej Kilian and Hanlin Zhang and Rulin Shao and Sarah Pratt and Sunny Sanyal and Gabriel Ilharco and Giannis Daras and Kalyani Marathe and Aaron Gokaslan and Jieyu Zhang and Khyathi Chandu and Thao Nguyen and Igor Vasiljevic and Sham Kakade and Shuran Song and Sujay Sanghavi and Fartash Faghri and Sewoong Oh and Luke Zettlemoyer and Kyle Lo and Alaaeldin El-Nouby and Hadi Pouransari and Alexander Toshev and Stephanie Wang and Dirk Groeneveld and Luca Soldaini and Pang Wei Koh and Jenia Jitsev and Thomas Kollar and Alexandros G. Dimakis and Yair Carmon and Achal Dave and Ludwig Schmidt and Vaishaal Shankar},
      year={2024},
      eprint={2406.11794},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      note={\url{https://arxiv.org/abs/2406.11794}}
}