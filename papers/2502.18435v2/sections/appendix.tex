\appendix
\onecolumn
\begin{center}
    {\Large \bf Appendix}
\end{center}
\section{Model Architecture and Training Hyperparameters}
\label{app:arch}
We provide the model architecture of our 2B and 8B models in Table~\ref{tab:model_architecture}. We use no attention dropout and the Normalization $\epsilon$ is set to $1 \times 10^{-5}$.  
We employ cosine scheduling for the learning rate, with the number of warmup steps set at 2\% of the total iterations. The models are trained on 64 H100 Nvidia GPUs for 60 hours and 208 hours for 2B and 8B models, respectively. 


We used the Llama3 tokenizer \citep{dubey2024llama3}. Despite being trained from a L2R model, \citet{papadopoulos2024arrows} demonstrated through an ablation study that the direction of tokenizer being trained has minimal effect on the R2L training. Consequently, we adhere to the same reasoning and opt not to train on a R2L tokenizer, as obtaining the necessary training data for Llama3's tokenizer is practically infeasible for fair comparison.
Our code implementation is based on Torchtune \citep{pytorch2024torchtune}, which is released under the BSD 3-Clause License, a permissive open-source license that allows for broad usage and distribution with minimal restrictions. 

\begin{table}[!htp]\centering
\rowcolors{2}{gray!15}{white}
    \caption{Model architecture specifications for 2B and 8B models.}\label{tab:model_architecture}
    \begin{tabular}{lrr}\toprule
    Parameter & 2B Model & 8B Model \\\midrule
    Vocabulary Size & 128,256 & 128,256 \\
    Number of Layers & 24 & 32 \\
    Number of Heads & 16 & 32 \\
    Number of KV Heads & 8 & 8 \\
    Embedding Dimension & 2,048 & 4,096 \\
    Max Sequence Length & 2,048 & 8,192 \\
    Intermediate Dimension & 7,168 & 14,336 \\
    RoPE Base & 500,000 & 500,000 \\
    \bottomrule
    \end{tabular}
\end{table}


\section{Comparing Three Paradigms for Reverse Thinking}
\label{app:rev_comparison}
We present a comparison of the three paradigms discussed in Section~\ref{sec:rev} across all tasks, as shown in Table~\ref{tab:3variants}. Paradigm 3 showcases the highest overall performance.


\begin{table}[!htp]\centering
\rowcolors{2}{gray!15}{white}
    \caption{Downstream evaluation of three variants to compute $s_i$. }\label{tab:3variants}
    \small
    \begin{tabular}{lrrr}\toprule
    &Paradigm 1 &Paradigm 2 &Paradigm 3 \\\midrule
    LogiQA &31.49\% &27.34\% &\textbf{31.49\%} \\
    OpenbookqQA &37.00\% &24.60\% &\textbf{44.40\%} \\
    TruthfulQA &24.36\% &17.38\% &\textbf{28.76\%} \\
    CommensenseQA &41.85\% &27.19\% &\textbf{45.13\%} \\
    Social\_IQA &\textbf{43.09\%} &37.36\% &42.22\% \\
    AI2\_arc &32.19\% &45.32\% &\textbf{52.31\%} \\
    HellaSwag &33.73\% &43.38\% &\textbf{44.34\%} \\
    MathQA &23.18\% &21.07\% &\textbf{24.86\%} \\
    MMLU & 33.10\% &29.31\% &\textbf{34.35\%} \\
    PIQA &\textbf{70.76\%} &68.70\% &57.13\% \\
    Winogrande & 54.46\% &53.67\% &\textbf{54.85\%} \\
    AVG &38.66\% &35.94\% &\textbf{41.80\%} \\
    \bottomrule
    \end{tabular}
    \end{table}


\section{Experimental Details on Computing the Conditional Entropy in MCQs}
\label{app:ce}
We measure conditional entropy across various MCQs tasks by using ancestral sampling to generate sequences conditioned on each task's input. Generation length can be a confounding issue as longer generation will typically receive larger 
entropy. 
To ensure consistent and fair comparison, we constrain the generation to exactly 10 tokens in each direction (L2R and R2L). We make the generation non-stopping by removing the end-of-sentence token. The conditional entropy is first calculated for individual examples within each task, then averaged across all examples to obtain the task-level measure.






















\section{Analysis of How Reverse Thinking Reduces Surface Form Competition in MCQ}
\label{app:surface}
Consider a multiple-choice question:

\begin{center}
    Q: \textbf{Who barks?} \\
    A: \textit{dog} \quad B: \textit{cat}
\end{center}

We assume a vocabulary of only three words: \textit{dog}, \textit{cat}, and \textit{puppy}. However, the answer choices are limited to \textit{dog} and \textit{cat}.

\paragraph{Forward Thinking: Surface Form Competition}
In the forward model, suppose the model predicts the probabilities of each token in the vocabulary as in below:
\[
P_{L2R}(\text{dog} \mid q) = 0.3, \quad P_{L2R}(\text{puppy} \mid q) = 0.3, \quad P_{L2R}(\text{cat} \mid q) = 0.4.
\]
As \citet{holtzman-etal-2021-surface} shown, here, ``dog'' and ``puppy'', although referring to the same entity, compete as surface forms, potentially impacting the selection of ``dog'' as an answer.

\paragraph{Reverse Thinking: Alleviating the Surface Form Competition}
In the reverse thinking, the model should still suffer the surface competition in the prior, suppose we have:
\[
P_{R2L}(\text{dog}) = P_{R2L}(\text{puppy}) = 0.2, \quad P_{R2L}(\text{cat}) = 0.6.
\]

The reverse conditional probability would would resemble this:
\[
P_{R2L}(q \mid \text{dog}) = 0.9, \quad P_{R2L}(q \mid \text{puppy}) = 0.9, \quad P_{R2L}(q \mid \text{cat}) = 0.4.
\]

In the reverse thinking R2L paradigm 3, we enforce a uniform prior, which achieves the best performance. 
We hypothesize that this is due to the alleviation of the surface form competition issue. In this example, by enforcing a uniform prior, surface form competition in the prior distribution disappears, ensuring a fair comparison between ``dog'' and ``cat''.
Since we only compare $P_{R2L}(q \mid \text{dog})$ and $P_{R2L}(q \mid \text{cat})$, the probability $P_{R2L}(q \mid \text{puppy})$ becomes irrelevant.








\section{Additional Results for Simulation Study}
\label{app:simulation}
We think Forward X is generally more challenging because distinguishing incorrect choices from the correct one can be more challenging than in the Reverse X. Unlike Reverse X where all wrong choices are seen in the dataset, approximately 75\% of incorrect choices in Forward X are absent as they cannot be factorized into $m \times n$. 

We provide some additional results by performing free ancestral sampling generation (without topK or topP, with temperature set to 1) for the R2L(m) in Forward X. The accuracy of exact match for the generation is reported in below. For reference, the L2R free generation accuracy is 90.4\%.

\begin{enumerate}
    \item Regarding Forward X, R2L(m), free generation of $m$ using $\times n = p_{\text{correct}}$, the exact match accuracy for generative output is \textbf{82.1\%}.
    \item For Forward X, R2L(m), free generation of $m$  using $\times n = p_{\text{incorrect}}$, the generative exact match accuracy remains high at \textbf{30.2\%}, while the accuracy for random guessing is nearly 0. 
\end{enumerate}

These results suggest that the R2L model may map unseen $p_{\text{incorrect}}$ to its neighborhood $p_{\text{correct}}$ and execute the same computation as if using $p_{\text{correct}}$. Consequently, the model faces a significant challenge in distinguishing alternative incorrect answers from the correct ones, making alternative incorrect answers hard-negatives. 