\section{Related Work}
\paragraph{Reversal Curse} 
Sukhbaatar, "The Reversal Curse in Language Models" first investigates the "reversal curse" in LLMs, which refers to the phenomenon where models trained on forward text data struggle to perform well on inverse search tasks. Adafin et al., "Augmentation and Inverse Search Tasks" further discusses this issue and proposes that augmentation during the pretraining stage can help bridge the knowledge extraction performance gap in reverse entity mapping. Clark, "Unified Language Models for Reverse Entity Mapping" suggest training a unified model that combines text data with augmented reversed or partially reversed data can mitigate the reversal curse.
These studies imply that autoregressively-trained language models tend to have a linear and unidirectional thinking process, and certain types of augmentation can faciliate the model in making complex connections between pieces of learned information to enable more intricate cross-referencing. Our research also demonstrates that the autoregressive nature of LLMs may introduce inductive biases rooted from the pretraining corpus. Instead of focusing on the "reversal curse," we suggest that knowledge extraction and reasoning may be more straightforward in the direction with lower conditional entropy.


\paragraph{Order of Reasoning} 
Previous works have also been exploring the reasoning order's impact to the reasoning performance. Clark, "Reasoning Order for Sequence-to-Sequence Models" first demonstrates that the sequence in which input and output data are organized significantly impacts the performance of sequence-to-sequence models and propose to search over possible orders during training to manage unstructured output sets.
Recently, Sukhbaatar et al., "Asymmetry in Reasoning Orders" reveals a surprisingly consistent lower log-perplexity when predicting in L2R versus R2L, despite theoretical expectations of symmetry. The authors attributes this asymmetry to factors like sparsity and computational complexity. We also observe this difference yet we have another hypothesis rationale beyond theirs. 
Zhang et al., "Digit Order Reversal for Arithmetic Tasks" shows that by reversing the digit order, prioritizing the least significant digit can improve LLMs's performance on arithmetic, which aligns with our findings in section~\ref{sec:sim}. 



Previous studies on sequence modeling have also delved into relaxing the conventional ``left-to-right'' autoregressive dependencies, primarily to facilitate rapid parallel generation Zhang et al., "Rapid Parallel Generation" and non-monotonic sequential generation Sukhbaatar et al., "Non-Monotonic Sequential Generation". Text diffusion has recently emerged as a promising approach in terms of planning and controllability Clark et al., "Text Diffusion for Planning and Controllability". It has shown to be more effective than LLM than language model (LLM), particularly for tasks that require bidirectional reasoning strategies such as sudoku and countdown games Zhang et al., “Sudoku and Countdown Games".


\paragraph{Multiple-Choice Questions (MCQs) for LLM evaluation}
MCQs have been widely used for evaluation LLM's reasoning and knowledge extraction abilities. 
Clark, "Selection Bias in MCQs" demonstrates that LLMs exhibit a selection bias in MCQs, favoring certain option positions, and introduces a debiasing method to mitigate this issue.
Sukhbaatar et al., "Order of Answer Options in MCQs" examines how LLMs’ performance on MCQs is influenced by the order of answer options, finding that reordering can lead to huge performance variations. 
Clark, "Reframing MCQs as Binary Classifications" proposes reframing MCQs as a series of binary classifications, demonstrating that this approach significantly improves performance across various models and datasets.
Sukhbaatar et al., "Issues with Positional Biases in MCQs" highlights issues like positional biases and discrepancies compared to long-form generated responses, when using MCQs in evaluating LLMs.
Zhang et al., "Middle Layer's Role in Answer Prediction" discovers that the prediction of specific answer symbols is primarily attributed to a single middle layer’s multi-head self-attention mechanism, with subsequent layers increasing the probability of the chosen answer in the model’s vocabulary space.
In contrast to the previous work, our work first shows the connection between the preferred reasoning direction and the direction that has lower conditional entropy in MCQ evaluations.