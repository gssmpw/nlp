%% LaTeX Template for ISIT 2024
%%
%% by Stefan M. Moser, October 2017
%% (with some modifications by Tobias Koch, November 2023)
%% 
%% derived from bare_conf.tex, V1.4a, 2014/09/17, by Michael Shell
%% for use with IEEEtran.cls version 1.8b or later
%%
%% Support sites for IEEEtran.cls:
%%
%% http://www.michaelshell.org/tex/ieeetran/
%% http://moser-isi.ethz.ch/manuals.html#eqlatex
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%%

%\documentclass[conference,letterpaper]{IEEEtran}
\documentclass[journal,onecolumn]{IEEEtran}

%% depending on your installation, you may wish to adjust the top margin:
\addtolength{\topmargin}{9mm}

%%%%%%
%% Packages:
%% Some useful packages (and compatibility issues with the IEEE format)
%% are pointed out at the very end of this template source file (they are 
%% taken verbatim out of bare_conf.tex by Michael Shell).
%
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{ifthen}
\usepackage{cite}
\usepackage[cmex10]{amsmath} % Use the [cmex10] option to ensure complicance
                             % with IEEE Xplore (see bare_conf.tex)

%% Please note that the amsthm package must not be loaded with
%% IEEEtran.cls because IEEEtran provides its own versions of
%% theorems. Also note that IEEEXplore does not accepts submissions
%% with hyperlinks, i.e., hyperref cannot be used.
\usepackage{amsthm,mathrsfs}
\usepackage{mathcomSTEv4}
\usepackage{makecell}
\usepackage{multirow}
\usepackage[T1]{fontenc}
\usepackage{cite}
\usepackage{enumerate}

\usepackage{enumitem}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}              % provides \url{...}
%\usepackage{ifthen}          % provides \ifthenelse
\usepackage{cite}             % improves presentation of citations

\usepackage[cmex10]{amsmath}  % Use the [cmex10] option to ensure complicance
                              % with IEEEXplore (see bare_conf.tex)
\interdisplaylinepenalty=1000 % As explained in bare_conf.tex
\usepackage{mleftright}       % fix to wrong spacing of \left-,
\mleftright                   % \middle- \right-commands 

\usepackage{graphicx}         % provides \includegraphics{...} to
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
%\usepackage[toc,page]{appendix}
%\usepackage{caption}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{bbm}
\usepackage{makecell}
\usepackage{multirow}
 \usepackage{amssymb}
\usepackage{txfonts}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage[scr=dutchcal]{mathalfa}
\usepackage{ textcomp }
\usepackage{floatflt}
\usepackage{amsthm}                              % include graphics (pdf format)
\usepackage{booktabs}



\theoremstyle{definition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Proposition}{Proposition}
\newtheorem{Lemma}{Lemma}
\newtheorem{Corollary}{Corollary}
\newtheorem{Example}{Example}
%
\newtheorem{Remark}{Remark}
%
\newcommand{\ste}{ \color{orange}}
\newcommand{\Tsf}{\mathsf{T}}
\newcommand{\tsf}{\mathsf{t}}

\newtheorem{Definition}{Definition}

\newtheorem{Proof}{Proof}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newtheorem{Claim}{Claim}


%-------------------------------
\newcommand{\Esf}{\mathsf{E}}
\newcommand{\Qsf}{\mathsf{Q}}

%------------------------------- 
\interdisplaylinepenalty=2500 % As explained in bare_conf.tex


%%%%%%
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% ------------------------------------------------------------
\begin{document}
\title{The Sequential Query/Hit Learning Model} 

\author{%
  \IEEEauthorblockN{Mahshad Shariatnasab}
  \IEEEauthorblockA{  Knight Foundation School of \\Computing  and Information Sciences \\
                    Florida International University\\
                    Maimi, FL, USA\\
                    Email: mshar075@fiu.edu}
  \and
    \IEEEauthorblockN{Farhad Shirani}
  \IEEEauthorblockA{  Knight Foundation School of \\Computing  and Information Sciences \\
                    Florida International University\\
                    Maimi, FL, USA\\
                    Email: fshirani@fiu.edu}
\and
  \IEEEauthorblockN{Stefano Rini}
  \IEEEauthorblockA{Electrical and Computer Engineering Dept. \\ National Yang-Ming \\Chao-Tung University (NYCU) \\Taipei, Hsinchu, Taiwan\\
                    Email: stefano.rini@nycu.edu.tw}
}

\maketitle
\begin{abstract}
Motivated by communication, complexity, and privacy constraints, we consider a two-agent learning problem.
A streaming source $Z(t)$ is chosen according to a hypothesis $h$  from the hypothesis class $\Hcal$. 
%
A remote observer-- Alice-- has access to the source $Z(t)$, while Bob wishes to output a prediction $\hh$ of the true hypothesis $h$.
%
The interaction between Alice and Bob takes place through by sequential \emph{query/hit} pairs: (i) Bob communicates to Alice a sequence of source symbols-- a \emph{query} (ii) Alice responds to the query with the waiting time between the current time and the occurrence of the query - the \emph{hit}.
%

By sequentially designing the queries and observing the corresponding hit, Bob aims to produce the estimate of the hypotheses $h$ within a prescribed error and within the minimum time. 

\end{abstract}

\section{Introduction}
\vspace{1cm}
{\ste complete}
\vspace{1cm}

\noindent
{\bf Notation:}
 We represent random variables by capital letters such as $X, U$ and their realizations by small letters such as $x, u$. Sets are denoted by calligraphic letters such as $\mathcal{X}, \mathcal{U}$. The random variable $\mathbbm{1}_{\mathcal{E}}$ is the indicator function of the event $\mathcal{E}$.
 The set of numbers $\{n,n+1,\cdots, m\}, n,m\in \mathbb{N}$ is represented by $[n,m]$. Furthermore, for the interval $[1,m]$, we use the shorthand notation $[m]$ for brevity. 
 For a given $n\in \mathbb{N}$, the $n$-length vector $(x_1,x_2,\hdots, x_n)$ is written as $x^n$. 


\newpage
\section{Problem Formulation}

\begin{figure}
    \centering
    \includegraphics[width=.5\textwidth]{33568.jpg}
    \caption{The Sequential Query/Hit Problem (SQHP)}
    \label{fig:conceptual}
\end{figure}

Consider the problem formulation in Fig.  \ref{fig:conceptual} -- which we term the  \emph{Sequential Query/Hit Problem} (SQHP).
%
Formally, a discrete-time random process $Z_h(t)$ with alphabet $\Zcal$ defined in the interval $[T]$ is selected according to the hypothesis $h$ in the hypothesis class $\Hcal$.
%

This process is observed at a remote observer \emph{Alice} (A) in a streaming manner, meaning that Alice -- at time $t$-- cannot access the source values for $t'<t$. 
%
The central processor \emph{Bob} (B) wishes to estimate the hypothesis $\hh$ by sequentially querying Alice. 
%
The $k^{\rm th}$ \emph{query} $\Qsf_k$ is in the form of a sequence $z^m \in \Zcal^*$. 
%
Upon receiving the query $\Qsf_k$ at time $t$, Alice replies with the corresponding \emph{hit} -- $\Tsf_k$: this corresponds to the time index of the first appearance of the sequence $z^m$ in $Z_h$ for $t'>t$. 

Upon receiving the hit corresponding to the $k^{\rm th}$ query, Bob produces an estimate of the underlying hypothesis  $h$ -- $\hh_k$-- as a function of the previous hits, that is $\hh_k=\hh_k(T^k)$.
%
Bob's objective is to minimize the a certain loss between the estimate and the true hypothesis,  namely
\ea{
L_K(T) = \Lcal \lb h, \hh(\Qsf^K,\Tsf^K) \rb,
\label{eq:loss}
}
where $t$ is time at which the estimate is produced, that is
\ea{
t=\sum_{j \in k} \Tsf_k.
}
Finally, the loss minimization problem can be formulated under various constraints: 

\begin{itemize}

\item {\bf Query-constrained  setting:}
\ea{
L_K^* =\min_{ k \leq K } L_k(t)
}

\item {\bf Time-constrained  setting:}
\ea{
L^*(T) =\min_{ t \leq T } L_k(t)
}

\item {\bf Loss-constrained  setting:}
this is the dual of the previous setting, that is
\ea{
t^* (\ep) =\min_{ L_k(t) \leq \ep} t 
}
\end{itemize}

Note that the previous measures can also be taken in expectation over a certain distribution over the hypothesis class. 


\subsection{Scope of Learning Problems Modeled}

The Sequential Query/Hit Problem (SQHP) outlined  above is a rather versatile model that can encapsulate various types of learning problems. 
%
The distinctiveness of this model lies in its sequential interaction between two parties over a streaming data source, aiming to minimize a loss function related to the accuracy of hypothesis estimation under time or query constraints. 


\begin{itemize}
    \item  {\bf Binary Hypothesis Testing:}
    
    In this context, $\Hcal$ represents a set of two possible hypotheses about the distribution or characteristics of the streaming source $Z_h(t)$. 
    %
    The objective is to identify which hypothesis $h$ from $\Hcal$ best explains the observed data. This fits naturally within the SQHP as Bob seeks to estimate $\hat{h}$ that minimizes the loss function, equating to making a decision on which hypothesis is true based on the received hits. This can be formalized as testing between different statistical properties or distributions that $Z_h(t)$ may follow.
In this case, the loss function $L_k(t)$ could be defined in a Bayesian setting by minimizing the probability of error under a certain prior on the type I and type II errors. 


\item {\bf Multiclass Classification:}

 For multiclass classification, each hypothesis $h \in \Hcal$ corresponds to a class label, and the streaming data $Z_h(t)$ are features observed over time that are associated with one of these labels. Bob's task is to predict the correct class label for the sequence observed so far. 
 In this case, the loss function $L_k(t)$ could be defined as the cross entropy loss, where the loss is 0 if the predicted class $\hat{h}_k$ matches the true class $h$ and 1 otherwise.



\item {\bf Estimation:}
  The SQHP framework can also be effectively applied to estimation problems, where the goal is not to classify into discrete classes or make binary decisions, but rather to estimate continuous or discrete quantities based on the streaming data $Z_h(t)$. In this setting, each hypothesis $h \in \Hcal$ represents a possible value or a set of values (parameters) characterizing some aspect of the data stream, such as its mean, variance, rate of occurrence of certain patterns, etc.

  \item {\bf Regression:}
  
 In the regression setting, the hypothesis $h$ could represent a parameter or a set of parameters of a regression model that describes the relationship between the time $t$ and the streaming data $Z_h(t)$. The objective here is for Bob to estimate these parameters as accurately as possible. The loss function in this scenario could be the mean squared error between the predicted values based on $\hat{h}_k$ and the actual values of $Z_h(t)$.

\item {\bf Sequential Decision Making:}

 An additional learning problem that the SQHP model could encapsulate is sequential decision making. Here, the hypothesis $h$ could represent a policy or strategy that dictates the generation of $Z_h(t)$. Bob's objective is to identify this policy based on the sequence of observations and hits received. This scenario emphasizes the sequential aspect of decision making under uncertainty, where the loss function might measure the discrepancy between the chosen actions (or queries) and those that would be optimal under the true policy.

\end{itemize}



\subsection{Definitions}
Let us introduce some useful definitions for the remainder of the paper. 


\begin{Definition}{\textbf{Streaming Source}}
Let $h$ be a hypothesis an the hypothesis class $\Hcal$, then the source corresponding to the hypothesis $h$ is indicated as
\ea{
Z_h(t), \quad t \in [T] 
}
for some -- possibly random -- function mapping $h$ to the discrete time process $Z_h(t)$ with support $\Zcal$.
\end{Definition}


\begin{Definition}{\textbf{Query}}
A query of length $m$ from the source $Z_h(t)$ with support $\Zcal$ is defined as 
\ea{
\Qsf = z^m 
}
for $z^m \in \Zcal^m$.
\end{Definition}


\begin{Definition}[\textbf{Hit}]
Given a query $\Qsf=z^m$ of length $m$ at time $t$, the hit is defined as 
\ea{
\tsf(Q,t) =  \argmin_{t', t'\geq t}  \ \ \lcb t' \ | \ Z_{t'}^{t'+m}=\Qsf \rcb. 
\label{eq:query zero}
}
\end{Definition}



\begin{Definition}[\textbf{Query/Hit Set}]
A \emph{query/hit set} is defined as 
\ea{
(\Qsf^K,\Tsf^K| Z^T) = \lcb (Q_k,\tsf_k)  \ | \  Z^T \rcb_{k \in [K]}. 
\label{eq:set of qh}
}
such that $\tsf_k$ is obtained as in \eqref{eq:query zero} for the query $Q_k$ and $t=\tsf_{k-1}$ for $k>0$ and $t=0$ for $k=1$.
\end{Definition}

% Note that in \eqref{eq:set of qh} we have simplified the notation as
% \ea{
% \tsf_k= \tsf(Q_k,t)
% }

\begin{Definition}[\textbf{Prediction Function}]
Given  a query/hit set $\Qsf^K,\Tsf^K$, let the prediction function of the hypothesis $h$ be   
\ea{
\hh=\hh(\Qsf^K,\Tsf^K | Z^T)
}
\end{Definition}


% \begin{Definition}[\textbf{Query Function and Query Response]
% \label{def:time}
% Let $\Delta\in \mathbb{N}$, and define the set of admissible $\Delta$-length queries as $\mathcal{Q}_{\Delta}\triangleq \{z^{\Delta}| z_i \in \mathcal{Z}\}$. Given a 
%  random process $Z_1, Z_2, \ldots$ and a query sequence $z^{\Delta}\in \mathcal{Q}_{\Delta}$,  the query response at time $t\in \mathcal{N}$ is defined as: 
% \begin{align*}
%     &T_j(z^{\Delta},t) = \argmin_{j: t+\Delta\leq  j} \{j| Z_{j-\Delta+1}^{j}=z^{\Delta}\}. 
% \end{align*}
% %Here, $T_j$ serves as a shorthand for $T_j(Z,\sigma_j)$, with each $T_j$ corresponding to a distinct pattern or subsequent definition of length $\Delta$.
% \end{Definition}


\subsection{Discussion}

In the definition of a query hit set is implicit (i) the instantaneous communication between Alice and Bob, and (ii) the instantaneous computation of the query strategy.



\begin{itemize}
    \item {\ste the streaming source is reminiscent of the online learning approach}
\end{itemize}
\newpage
\section{Relevant Results}

In the reminder of the paper we shall be concerned with ergodic and iid sequences. 
%
Let $T_z$ me the random variable corresponding to the waiting time of the symbol $Z=z$, so that $T_z(m)$ is the probability that the waiting time for the query $Z=z$ is $m$. 
%
For an iid sequence with $P[Z=1]=\al$- for example - it holds that 
\ean{
T_1(m)  & = \al (1-\al)^{m-1}\\
\Ebb[T_1] & = \sum_{m \in \Nbb} \al (1-\al)^{m-1} = \f 1 {\al}.
}
More generally, Kac in 1947 \cite{...} showed that for any ergodic sequence, it holds that 
\ea{
\Ebb[T_z] = \f 1 {\Pr[Z=z]}.
}


\newpage
\section{The Sequential Detection Problem}
In this section, we wish to study the  SQHP for detection in which 
\begin{align*}
\Hcal_0: & \ \ Z(t) \sim P_t(\cdot) \\
\Hcal_1: & \ \ Z(t) \sim Q_t(\cdot),
\end{align*}
with a prior on the two hyphotesis as 
\ea{
P_H = \lcb \p{ 
\Hcal_0 & \pi_0  \\
\Hcal_1 & \pi_1
}
\rnone 
}
For this problem, let us define -- as costomary-- the type I and type II errors as 
\ean{
\al(h,\hh) & = P[\hh \neq h | h = 0] \\
\be(h,\hh) & = P[\hh \neq h | h = 1].
}
so that the loss function in \eqref{eq:loss} is chose an 
\ea{
\Lcal (h,\hh) = \pi_0  P[\hh \neq h | h = 0] + \pi_1 P[\hh \neq h | h = 1].
}



\subsection{Single query}
Next, let us consider the single-query SQHP 

{\ste one shot baysian: to be done }
\newpage

\section{The Tracking Problem}

Next, we consider the estimation problem of tracking the position in one dimension. 
%
Fix two hyperparameters $(p,\ep)$ and and consider a symmetric binary Markov chain with transition probability $p$ and form the hyphotesis class $\Hcal$  to contain the most likely realization of the Markov chain that contain $1-\ep$ of the probability in $T$ iterations, starting from the inital state $0$. 
%
In other words, consider we take the typical sequences of the Markov chain covering $1-\ep$ of the probability. 


% $\tau<T$ and let the hypothesis class $\Hcal$ be the integer between $1$ and  $\binom{T}{\tau}$, than $Z_h^T$ corresponds to the $h$-th permutation of a vector with $\tau$ ones and $T-\tau$ zeros, under some labelling of the permutations $\pi$ as $\pi(h)$.
% %
Next consider the problem of estimating the permutation index $h$ under a loss function defined as
\ea{
\Lcal_K(T) =  ( \| Z_h - Z_{\hh}\|^2 )
}
where $Z_{\hh}$ is the estimated Markov chain realization.
%
In this setting, $Z_{\hh}$ is obtained deterministically as follows: (i) at the position $\tsf_k$, the sequence is equalm to the query, while (ii) for the $t$ values between $\tsf_k$ and $\tsf_{k_11}$ $Z_{\hh}$ is equal to the most likely sequence in the Markov chain between the two states. 

The problem above corresponds to the problem of tracking 
\vspace{1cm}

{\ste  In this case we could also increase the memory of the markov chain to simulate markov chains with more states.}


\textbf{}


\newpage
\begin{Definition}[\textbf{Query Function and Query Response}]
\label{def:time}
Let $\Delta\in \mathbb{N}$, and define the set of admissible $\Delta$-length queries as $\mathcal{Q}_{\Delta}\triangleq \{z^{\Delta}| z_i \in \mathcal{Z}\}$. Given a 
 random process $Z_1, Z_2, \ldots$ and a query sequence $z^{\Delta}\in \mathcal{Q}_{\Delta}$,  the query response at time $t\in \mathcal{N}$ is defined as: 
\begin{align*}
    &T_j(z^{\Delta},t) = \argmin_{j: t+\Delta\leq  j} \{j| Z_{j-\Delta+1}^{j}=z^{\Delta}\}. 
\end{align*}
%Here, $T_j$ serves as a shorthand for $T_j(Z,\sigma_j)$, with each $T_j$ corresponding to a distinct pattern or subsequent definition of length $\Delta$.
\end{Definition}

The $i$th query, sent by Bob to Alice at time $t_i$, is parametrized by a finite sequence $z^{m_i}\in \mathcal{Z}^*, 
m_i, i\in \mathbb{N}$, where $\mathcal{Z}^*\triangleq \{z^m| z_i\in \mathcal{Z}, i\in [m], m\in \mathbb{N}\}$ is the set of all finite-length sequences on the set $\mathcal{Z}$. After receiving the query $z^{m_i}$, Alice sends back the time index $T_i$ of the first instance where $z^{m_i}$ is observed in the sequence $Z_{{t_i}}, Z_{t_{i+1}},\cdots$. Bob then decides to either declare one of the two hypotheses as the true hypothesis based on the received query responses $T_1,T_2,\cdots,T_i$, or to continue asking additional queries. 

Formally, ...



\textcolor{red}{explain a high-level overview of the problem, in the context of bot detection, mention Figure and high-level overview of the problem. If high-level overview is given here, then the rest of the application-related descriptions in future paragraphs can be very brief, otherwise, we can distribute the descriptions to each definition itself.}

In the IHP problem formulation the source coding is not available to the agent and the agent only can send queries $Q_i$ to the source based on the query responses $T_i$ he can decide whether the source is based on distribution $H_0$ or $H_1$. To facilitate explanation, and provide justifications for the model assumptions, we describe the model by focusing on the scenario of bot detection. 


In a typical bot detection scenario, consider Bob as a website administrator and Alice representing a device interacting with the website. Bob's objective is to ascertain whether Alice is a human user or an automated agent. To achieve this, Bob employs a web socket that monitors specific user actions, such as mouse movements (scroll events, and clipboard interactions like cut, paste, and copy and etc.).

However, due to privacy concerns, Bob's script is designed to respect Alice's privacy. It does not transmit all the collected information back to the server. Instead, Bob's system is structured to respond to specific queries about the data it collects. This selective approach allows Bob to gather relevant information without compromising the privacy of Alice.

Bob utilizes the responses to these targeted queries to conduct a binary hypothesis test. This test is designed to determine the nature of Alice's interaction with the website - whether it is human-driven or bot-driven. This setup epitomizes an IHT problem, where the full dataset is not directly accessible, but insights are drawn from specific, limited data points. This process is shown in Figure \ref{fig:Overview}. 






%\textcolor{red}{explain a high-level overview of the problem, in the context of bot detection, mention Figure and high-level overview of the problem. If high-level overview is given here, then the rest of the application-related descriptions in future paragraphs can be very brief, otherwise, we can distribute the descriptions to each definition itself.}

\vspace{1cm}
{\ste: changed this part to be more compact and general. Once we agree, we redo the figure.}

\vspace{1cm}

% \begin{Definition}[\textbf{Null and alternative hypothesis}]

% Consider two hypotheses \( H_0 \) and \( H_1 \) which correspond to the two possible underlying distributions, \( P_X \) and \( P_Y \), governing the samples. In other words, we have:
% \begin{align*}
% H_0: Z \sim P_X(\cdot), \\
% H_1: Z \sim P_Y(\cdot),
% \end{align*}
% where \( Z \) takes values in a finite set \( \mathcal{Z} \). We also assume that the distributions $P_X$ and $P_Y$ are known to both the agents.

% \end{Definition}


%\begin{Definition}[\textbf{Null and Alternative Hypothesis}]
%At each time \( n = 1, 2, \ldots \) let $Z_n \in \mathcal{Z}$ be a outcomes that are characterized by the hypothesis $H_0$.
%\begin{align*}
%    Z \sim P_1(\cdot)
%\end{align*}
    
%\end{Definition}




%\textcolor{red}{(if the high level overview is not given in the beginning, then we need more detailed description of the bot detection problem here. e.g., For instance: consider a setup where Bob is a website administrator, and Alice represents a device visiting the website. Bob wishes to determine whether Alice is controlled by a human user or an automated agent. To this end, Bob uses a web socket which collects specific user actions, such as mouses movements, scrolls, clipboard actions (cut, paste, copy, etc.). To preserve Alice's privacy and abide by regional privacy laws, Bob's script does not send back all the information that it collects, rather, it responds to specific queries that Bob sends regarding the collected data. Bob uses the query responses to perform a  binary hypothesis test and determine the nature of Alice's visit. A formal statement of this \textit{indirect hypothesis testing} problem is provided below. }



%These responses, as the sequence $T$, are received by Bob one at a time. Bob's goal is to decide between the two possible distributions, $P_X$ and $P_Y$, where $P_X$ and $P_Y$ are the probability distribution of bot and human queries.
%In the subsequent definition, we will elaborate on how Alice encodes the vector $Z$ into the vector $T$.


% \begin{Definition}[\textbf{Query Responses}]

% Consider a sequence $Z=\{Z_1, Z_2, \ldots, Z_n\}, n \in \mathbb{N}$. For a series of different combinations or permutations $\{\sigma_1, \sigma_2, \ldots, \sigma_m\}, m \in \mathbb{N}$ derived from a finite set ${\Sigma}$, we define a corresponding series of functions $\{T_j\}_{j=1}^m, j\in[m]$. Each function $T_j$ is associated with a specific permutation $\sigma_j$ and is defined as follows:


% \begin{align*}
%     &T_j(X;\sigma_j) = \argmin_{i} \{ i \mid 1 \leq i \leq n-|\sigma_j|+1, 
%     \\& (Z_i, Z_{i+1}, \ldots, Z_{i+|\sigma_j|-1})= \sigma_j \}
% \end{align*}    

% In this problem formulation without loss of generality, we use $T_j$ instead of $T_j(X,\sigma_j)$.
% \end{Definition}






%The probability of observing \( y \) after performing an experiment \( x \) under hypothesis \( h \) is denoted by \( p_{x}^{h}(y) \), that is,

%\[
%p_{x}^{h}(y) \triangleq P(Y_n = y | H = h, X_n = x).
%\]

In this problem formulation, Alice generates a random process $Z_1,Z_2,\cdots$. At any given time $n\in \mathcal{N}$, Bob may ask Alice a query parameterized by a sequence $(z_1,z_2,\cdots,z_k)$, where $k\in \mathbb{N}$ and $z_i\in \mathcal{Z}$. Having received the query, Alice sends back a variable $T$ which is equal to the number of sequence symbols $Z_n,Z_{n+1},\cdots,Z_{m}$ observed until the query sequence is observed for the first time. 

\begin{Remark}
    Consider a scenario where Alice generates a binary sequence \( z_1, z_2, z_3, z_4, z_5 = 0, 1, 1, 1, 0 \) from the set \( \mathcal{Z} = \{0, 1\} \). Bob, intending to identify a specific pattern in Alice's sequence, sends a series of queries. Suppose Bob's queries are \( q_1, q_2, q_3=  0, 1, 0 \) (a specific pattern he is looking for in the sequence). 

Alice's responses to these queries, denoted by \( T \), are determined by the earliest occurrence of each query pattern. For the query pattern \( 0 \), the earliest occurrence in Alice's sequence is at position 1 (for \( Z_1 \)). For the pattern \( 1 \), it is at position 2 (for \( Z_2 \)), and for the pattern \( 0 \) again, it occurs at position 5 (for \( Z_5 \)). Therefore, Alice's response to these queries will be \( T = 1, 2, 5 \).
\end{Remark}



Let us assume that at a given time $t \in \mathbb{N}$, Bob receives a query response $T_{k_t}$, where $k_t$ is the total number of query responses received up to time $t$. Bob has two choices, one is to declare the output of the Hypothesis test by determining either the null or alternative hypothesis as the ground truth, or to continue sending new queries by choosing a new query sequence. We model Bob's operation by two functions, namely, the query function and the identification function. The pair of functions is called an identification strategy. These are formally defined in the following. 

%When Bob receives query responses $ \{T_1, T_2, T_3, \ldots\} $, it utilizes query functions $x_t(\cdot)$ and identification function $ \text{Id}_t(\cdot)$, for $t \in \mathbb{N}$. The query function $ x_t(T^{t-1})$ takes as input the encoder output $T^{t-1}$ up to time $t-1$ and outputs the next experiment $ \sigma_t$. Upon receiving the response $T^t$, the identification function $ \text{Id}_t(T^t)$ compares the probability of $T^t$ with Alice probability distribution $P_X$ and Bob probaility distribution $P_Y$. It then either decides between hypotheses $ H_0$ and $H_1$, or indicates that the identity cannot be determined yet, suggesting that the agent should conduct the next experiment. The Identification function can take values in $\hat{H} \cup \{e\}$, where $\hat{H} \in \{H_0, H_1\}$.


\begin{Definition}[\textbf{Identification Strategy}]
 Let $\Delta\in \mathbb{N}$. An identification strategy consists of a pair of sequences of query functions and identification functions   $x_i : \mathbb{N}^{i-1} \to \mathcal{Q}_{\Delta}$ and $Id_i:\mathbb{N}^{i}\to \{0,1,c\}, i\in \mathbb{N}$, respectively. The sequence of pairs $(x_i,Id_i)_{i\in \mathbb{N}}$ is called an identification strategy. 

 The output of the (fixed) function $x_1()$ represents Bob's query sequence at time $t=1$, and the output of $x_{i}(T^{i-1})$ represents Bobs query sequence at time $t=T_{i-1}$, where $T_i$ is defined in Definition \ref{def:time}. We define $K_t\triangleq i-1$.  Having received query responses $T_1,T_2,\cdots,T_i$, if $Id_{i}(T^i)\in \{0,1\}$, then Bob outputs the hypothesis $H_{Id_{i}(T^i)}$ as his estimate of the ground-truth, otherwise, if $Id_{i}(T^i)=c$, Bob sends query $x_{i+1}(T^{i})$.  


 
% identification function is a function of the form $Id_i:  \mapsto \hat{H} \cup \{e\}$, where $x_t(T^{t-1})$ outputs the next experiment $\sigma_t$ at the time $t$ and identification $Id_t(T^t)$ either outputs the $H_0$ and $H_1$ or outputs `e' in which case the information is not enough and further sequence is needed. 
%Let $t\in \mathbb{N}$, and define $K_t$ as the random variable corresponding to the number of queries 
\end{Definition}

\begin{Remark}
In the context of the identification strategy, the nature of the queries sent by Bob can be either adaptive or non-adaptive. A non-adaptive query strategy implies that the sequence of queries is predetermined and fixed before the test begins, regardless of the responses received. For example, if Bob decides to send the queries \(0, 1, 0\) in sequence irrespective of Alice's responses, this represents a non-adaptive approach.

On the other hand, an adaptive query strategy allows Bob to adjust his queries based on the responses received from Alice. For instance, if Bob's initial query is \(0\) and, based on Alice's response, he decides to follow up with a \(1\) or another \(0\), this approach is adaptive as it adapts to the information gathered during the testing process.

In this paper, both scenarios are considered. The non-adaptive approach is useful for situations where a predefined sequence of queries is necessary, while the adaptive approach allows for more flexibility and potentially more efficient identification strategies based on the evolving nature of the responses.
\end{Remark}

\begin{Definition}[\textbf{Type-I Error Probability}]
The Type-I error probability, denoted as \( P_{1|0}(Id_{T_i}, i) \), is defined as the probability that the identification function \( Id_{T_i} \) decides in favor of \( H_1 \) when the true hypothesis is \( H_0 \) based on the observations up to time \( i \). Formally:
$$
P_{1|0}(Id_{T_i}) = P_X(Id_{T_i} = 1).
$$
\end{Definition}

\begin{Definition}[\textbf{Type-II Error Probability}]
The Type-II error probability, denoted as \( P_{0|1}(Id_{T_i}, i) \), is defined as the probability that the identification function \( Id_{T_i} \) decides in favor of \( H_0 \) when the true hypothesis is \( H_1 \) based on the observations up to time \( i \). Formally:
$$
P_{0|1}(Id_{T_i}) = P_Y(Id_{T_i} = 0).
$$
\end{Definition}


\begin{Definition}[\textbf{Objective Function}]

Let \( T_i \) be the response at the \( i \)-th time step in the identification strategy comprising sequences of query functions \( x_i : \mathbb{N}^{i-1} \to \mathcal{Q}_{\Delta} \) and identification functions \( Id_i : \mathbb{N}^i \to \{0, 1, c\} \). Define the objective function to minimize the probability of incorrect identification as:
$$
\min \quad \mathcal{E}=  P_{0|1}(Id_{T_i}) + P_{1|0}(Id_{T_i}),
$$
where \( P_{1|0}(Id_{T_i}, i) \), is the probability that the identification function \( Id_{T_i} \) decides in favor of \( H_1 \) when the true hypothesis is \( H_0 \), and \( P_{0|1}(Id_{T_i}, i) \), is the probability that the identification function \( Id_{T_i} \) decides in favor of \( H_0 \) when the true hypothesis is \( H_1 \) based on the observations up to time \( i \)
\end{Definition}


This objective function aims to find an optimal balance between minimizing the probability of falsely rejecting the null hypothesis (Type-I error) and minimizing the probability of falsely accepting it (Type-II error) within the constraints of the given identification strategy.

\vspace{1cm}
{\ste: changed from here}

\vspace{1cm}
Consider the binary hypothesis testing in which a remote observer -- Alice  -- has access to a set of observations from the process $\{Z_t \}_{t \in [T]}=Z^T$ comprised of i.i.d. entries from two possible hypothesis:
\begin{align*}
\Hcal_0: & \ \ Z \sim P(\cdot) \\
\Hcal_1: & \ \ Z \sim Q(\cdot),
\end{align*}
over the same finite alphabet $\Zcal$ with cardinality $\|\Zcal\|\subseteq \Nbb$.
%
A central detector -- Bob -- is tasked with detecting which hypothesis is correct through a set of \emph{query-hit exchanges}  with Alice.
%
A \emph{query} $Q=z^l$ is a sequence of length $l \in \Nbb$ of the element of $\Zcal$. 
%
Given the source sequence $Z^T$, the query hit-time $\Tsf(Q,Z^T)$ is  define as 
%
\ea{
\tsf(Q|Z^T) =  \argmin_{j: \ \  t+l \leq  j}  \ \ \lcb j \ | \ Z_{j-l+1}^{j}=z^{l}\rcb. 
\label{eq:query zero}
}
for $Q=z^l$.
%
%
A \emph{set} of $K$ successive query-hit exchanges, queries being produced by Bob and hits by Alice, is defined
%more compactly  denoted as
\ea{
\lcb (Q_k,\tsf_k)  \ | \  Z^T \rcb_{k \in [K]}. 
\label{eq:set of qh}
}
where $\tsf_1$ is obtained as is 
\ea{
\tsf_k = \tsf(Q_k|Z^T_{\tsf_{k-1}})
}


\begin{Remark}
    {\ste say query do not superpose}
\end{Remark}
Given the set of $K$ query-hits exchanges as in \eqref{eq:set of qh}, the detector choses an hypthosis 
\ea{
\hh = \hh \ \lb \lcb (Q_k,\tsf_k)  \ | \  Z^T \rcb_{k \in [K]}.  \rb  \in \{0,1\}.
}
For compactess, let us define a \emph{querying strategy} of length $K$ as the tuple  
\ea{
\Qsf_K = \lsb \{Q_k\}_{k\in [K]}, \hh \rsb.
}
\subsection{Performance analysis}
%
%{\ste I prefer the chernoff information approach here}
%\vspace{1cm}
Given a querying strategy $\Qsf$ let us define -- as costumary-- the type I and type II errors as 
\ea{
\al_k & = P[\hh \neq \Hcal | \Hcal = 0] \\
\be_k & = P[\hh \neq \Hcal | \Hcal = 1].
}
Bob's objective is to choose the correct hypothesis with the highest probability and the shortest time. 
To harmonize these two objectives, we take an error exponent approach, that is we consider the exponent
\ea{
\Esf(\Qsf_K) = - \f 1 {\Ebb \lsb \sum_{k \in [K]}] \tsf_{k}\rsb}  \log \be_K,
}
and the Chernoffâ€“Stein lemma-type maximization
\ea{
\Esf^*(\ep)  = \max_{\Qsf_K,  \ST \al_K\leq \ep}  \Esf(\Qsf_K),
}
that is we wish to maximize the error exponent of the type II errors, subject to the type I error to be at most $\ep$.
The maximization 

\subsection{Adaptive vs non-adaptive}

{\ste complete: we want both}

% 1111111111111111111111
% Q: 11
% H:? 1 2 3 4 5 X
% H:0 2 4 6

%\subsection*{Adaptive and Non-Adaptive \( Q \) Function}
{\ste
The function \( Q \) can be implemented in two distinct modes: Adaptive and Non-Adaptive. These modes dictate how the function \( Q \) operates based on its output and the subsequent actions of the agent. 

\begin{Definition}[\textbf{Non-Adaptive \( Q \)}]
In the Non-Adaptive implementation of \( Q \), the function is fixed and predefined before its execution. Once set, the agent does not modify or adjust the function based on the output it generates. This means that the function \( Q \) operates consistently throughout its application, without any alteration in response to the data it processes or the results it produces. 
\end{Definition}

\begin{Definition}[\textbf{Adaptive \( Q \)}]
Conversely, in the Adaptive implementation, the agent has the capability to adjust or modify the function \( Q \) based on its output. This means that the function \( Q \) is dynamic and can evolve in response to the data it encounters. This adaptability allows for a more flexible approach, potentially enhancing the efficacy of the function in various scenarios. However, in the context of binary sequences, the distinction between adaptive and non-adaptive \( Q \) is minimal. This is because, with binary sequences, the agent can reconstruct the entire sequence, making the adaptability of the function less impactful.
\end{Definition}

These definitions articulate the key differences between the two modes of operation for the function \( Q \) and provide insights into their respective utilities and limitations, particularly in the context of binary sequence analysis.
}


% \vspace{3cm}
% To measure the detection performance, we take Chernoff's approach. 
% %
% \vspace{1cm}
% {\ste must define the choice of query functions here: adaptive or not? how  do we handle it?}
% \vspace{1cm}
% %
% Assume that hypothesis $\Hcal = i$ is selected with probability $\pi_i$, than the \emph{error probability} is defined as
% \ea{
% P_e = \pi_0 P[\hh \neq \Hcal | \Hcal = 0] +  \pi_i P[\hh \neq \Hcal | \Hcal = i].
% }
% The error exponent is defined as 
% \ea{

% }

% \vspace{2cm}
% {\ste this formulation is more compact imo, since the decision is only at the end of things}
% \vspace{2cm}

% is a statistical inference problem
% concerning the decision between two distinct probability distributions of the observed data. Consider two hypotheses \( H_0 \) and \( H_1 \) which correspond to the two possible underlying distributions, \( P_X \) and \( P_Y \), governing the samples. In other words, we have:
% \begin{align*}
% H_0: Z \sim P_X(\cdot), \\
% H_1: Z \sim P_Y(\cdot),
% \end{align*}
% where \( Z \) takes values in a finite set \( \mathcal{Z} \). We also assume that the distributions $P_X$ and $P_Y$ are known to both the agents.



% %\begin{Definition}[\textbf{Null and Alternative Hypothesis}]
% %At each time \( n = 1, 2, \ldots \) let $Z_n \in \mathcal{Z}$ be a outcomes that are characterized by the hypothesis $H_0$.
% %\begin{align*}
% %    Z \sim P_1(\cdot)
% %\end{align*}
    
% %\end{Definition}


\subsection{A motivating scenario}

{\ste make this a recurring example. just it up here as a motivation but keep the setting open}
\vspace{1cm}

In a typical bot detection scenario, consider Bob as a website administrator and Alice representing a device interacting with the website. Bob's objective is to ascertain whether Alice is a human user or an automated agent, a decision critical for maintaining the integrity and security of the website. To achieve this, Bob employs a web socket that monitors specific user actions, such as mouse movements (scroll events, and clipboard interactions like cut, paste, and copy and etc.).

However, due to privacy concerns, Bob's script is designed to respect Alice's privacy. It does not transmit all the collected information back to the server. Instead, Bob's system is structured to respond to specific queries about the data it collects. This selective approach allows Bob to gather relevant information without compromising the privacy of Alice's visit.

Bob utilizes the responses to these targeted queries to conduct a binary hypothesis test. This test is designed to determine the nature of Alice's interaction with the website - whether it is human-driven or bot-driven. This setup epitomizes an \textit{indirect hypothesis testing} problem, where the full dataset is not directly accessible, but insights are drawn from specific, limited data points.

In this problem formulation, Alice generates a random process $Z_1,Z_2,\cdots$. At any given time $n\in \mathcal{N}$, Bob may ask Alice a query parameterized by a sequence $(z_1,z_2,\cdots,z_k)$, where $k\in \mathbb{N}$ and $z_i\in \mathcal{Z}$. Having received the query, Alice sends back a variable $T$ which is equal to the number of sequence symbols $Z_n,Z_{n+1},\cdots,Z_{m}$ observed until the query sequence is observed for the first time. This is formalized below:

\textcolor{red}{(if the high level overview is not given in the beginning, then we need more detailed description of the bot detection problem here. e.g., For instance: consider a setup where Bob is a website administrator, and Alice represents a device visiting the website. Bob wishes to determine whether Alice is controlled by a human user or an automated agent. To this end, Bob uses a web socket which collects specific user actions, such as mouses movements, scrolls, clipboard actions (cut, paste, copy, etc.). To preserve Alice's privacy and abide by regional privacy laws, Bob's script does not send back all the information that it collects, rather, it responds to specific queries that Bob sends regarding the collected data. Bob uses the query responses to perform a  binary hypothesis test and determine the nature of Alice's visit. A formal statement of this \textit{indirect hypothesis testing} problem is provided below. }



%These responses, as the sequence $T$, are received by Bob one at a time. Bob's goal is to decide between the two possible distributions, $P_X$ and $P_Y$, where $P_X$ and $P_Y$ are the probability distribution of bot and human queries.
%In the subsequent definition, we will elaborate on how Alice encodes the vector $Z$ into the vector $T$.



\section{Preliminaries}
{\ste update notation when we all agree}

\vspace{1cm}

\begin{Lemma}[Neyman-Pearson Lemma]
\label{eq:Neyman-Pearson Lemma}
Let \( T_1, T_2, \ldots, T_k \) be the observed responses from a sequence of queries based on the data \( Z_1, Z_2, \ldots, Z_n \), where \( Z_i \) are i.i.d. according to a probability distribution \( Q \). Consider the decision problem corresponding to the hypotheses \( Q = P_X \) (for the underlying data \( Z \)) versus an alternative distribution \( P_Y \) (representing a different hypothesis for \( Z \)). Define a decision region \( A_k(\lambda) \) for the observed query responses as:
$$
A_k(\lambda) = \left\{ t^k : \frac{\mathbb{P}(T_1, T_2, \ldots, T_k | Q = P_X)}{\mathbb{P}(T_1, T_2, \ldots, T_k | Q = P_Y)} > \lambda \right\}.
$$
Let \( \alpha^* = \mathbb{P}^k(A_k(\lambda)^c | Q = P_X) \) and \( \beta^* = \mathbb{P}^k(A_k(\lambda) | Q = P_Y) \) be the probabilities of Type-I and Type-II errors, respectively, for decision region \( A_k(\lambda) \). For any other decision region \( B_k \) with associated probabilities of error \( \alpha \) and \( \beta \), if \( \alpha \leq \alpha^* \), then \( \beta \geq \beta^* \).
\end{Lemma}


\begin{proof}
Consider decision regions \( A_k(\lambda) \) as defined in the lemma for the observed query responses, and any other acceptance region \( B_k \subseteq \mathcal{T}^k \), where \( \mathcal{T}^k \) is the space of all possible sequences of \( k \) query responses. Let \( \phi_{A_k} \) and \( \phi_{B_k} \) be the indicator functions of \( A_k \) and \( B_k \), respectively. 

For all sequences of query responses \( t^k = (t_1, t_2, \ldots, t_k) \), the likelihood ratio test asserts that:
\begin{align*}
(\phi_{A_k}(t^k) - \phi_{B_k}(t^k))\left(\mathbb{P}(t^k | Q = P_X) - \lambda \mathbb{P}(t^k | Q = P_Y)\right) \geq 0.
\end{align*}
This inequality holds true because \( \phi_{A_k}(t^k) \) is 1 when \( t^k \) favors \( Q = P_X \) more strongly than \( Q = P_Y \) by a factor of more than \( \lambda \), and 0 otherwise. The same logic applies to \( \phi_{B_k}(t^k) \).

Summing over the entire space of query responses \( \mathcal{T}^k \), we obtain:
\begin{align*}
0 \leq \sum_{\mathcal{T}^k} (\phi_{A_k} \mathbb{P}_{P_X} - \lambda \phi_{A_k} \mathbb{P}_{P_Y} - \phi_{B_k} \mathbb{P}_{P_X} + \lambda \phi_{B_k} \mathbb{P}_{P_Y}).
\end{align*}

Rearranging the terms and translating them into Type-I and Type-II error probabilities for regions \( A_k(\lambda) \) and \( B_k \), we get:
\begin{align*}
\lambda (\beta - \beta^*) - (\alpha^* - \alpha) \geq 0,
\end{align*}
where \( \alpha^* \) and \( \beta^* \) are the Type-I and Type-II error probabilities for region \( A_k(\lambda) \), and \( \alpha \) and \( \beta \) are those for region \( B_k \).

Since \( \lambda \geq 0 \), it follows that if \( \alpha \leq \alpha^* \), then \( \beta \geq \beta^* \). This proves that the likelihood ratio test for the observed query responses is the most powerful test for distinguishing between \( Q = P_X \) and \( Q = P_Y \) under the constraint of a fixed Type-I error rate.
\end{proof}

\begin{Lemma}[Stein's Lemma \cite{}]
In the context of binary hypothesis testing for two probability distributions \( P_X \) and \( P_Y \) corresponding to hypotheses \( H_0 \) and \( H_1 \) respectively, Stein's Lemma relates the exponential decay rate of the Type-II error \( \beta \) to the Kullback-Leibler divergence \( D(P_X||P_Y) \). For a fixed Type-I error \( \alpha \) and \( n \) i.i.d. observations, Stein's Lemma states that:
$$
\lim_{n \rightarrow \infty} -\frac{1}{n} \log \beta_n = D(P_X||P_Y),
$$
where \( \beta_n \) is the Type-II error probability for \( n \) observations.
\end{Lemma}


\section{A Motivating example}

{\ste Rewrote this part: make it as communicative as possible.}

 \vspace{1cm}

% \textbf{Binary Example for Function \( Q \):}

Consider the binary setting in which 
\ea{
\Hcal_0 & \sim \Bcal(p_0) \\
\Hcal_1 & \sim \Bcal(p_1),
}
for some $p_0<p_1\leq 1/2 \in \Rbb^+$ without loss of generality, and let us consider the possible querying strategies.
%

\medskip
\noindent
{\bf Single query strategy}
One first approach that one imagines is that in which Bob produces a single query that is sufficient for reliable detection. 
%
Let us assume that the sequence is the all-zero sequence of length $l$ for some $l$ to be determined. 
%


\vspace{1cm}
{\ste complete}
\vspace{1cm}



\medskip
\noindent
{\bf Sequence reconstructive strategy.}

\vspace{1cm}
{\ste complete}
\vspace{1cm}




\medskip
\noindent
{\bf Non-adaptive querying strategy.}
Given a given query length $l$ and Lem. \ref{eq:Neyman-Pearson Lemma}, one first strategy would be to select the most discriminating query across the two hypotheses and repeat the query $K$ times.
%
Formally, 
\ea{
\max_{z^l}  \| \log (P(z^l))-\log (Q(z^l)) \| 
}
which -given the symmetry of the problem - corresponds to either the whole-zero and the whole-one query.  
%
Let us assume that we choose the query to be the whole-one query. 
%
Under this strategy, we have that the RV $\tsf_{k+1}=\Tsf_{k+1}-\Tsf_{k}$ in a success run of with probability of success $p_0^l$/$p_1^l$ under hypothesis $\Hcal_0$/$\Hcal_1$.
%

{\vspace{1cm}}
{\ste complete, what's the decay of the error probability with $l$}
{\vspace{1cm}}


\noindent
{\bf Long-queries first adaptive strategy.}



\noindent
{\bf Short-queries first adaptive strategy.}




\noindent
{\bf Optimal Adaptive querying strategy.*}
%
As an alternative strategy, consider the setting in which the query length 



Taking 
\vspace{3cm}

Consider a mouse trajectory represented in a binary format, for instance, \( \mathbf{T} = 010010110 \). In this sequence, each digit represents a state at a time instance, with '0' and '1' indicating different states or conditions.

Suppose the function \( Q \) is designed to notify whenever a '1' is encountered in the trajectory. For the given sequence \( \mathbf{T} \), applying \( Q \) would yield:

\[
Q(T_1, T_2, T_3, T_4, T_5, T_6, T_7, T_8, T_9) = \{2, 5, 6, 7, 9\}
\]

Here, the function \( Q \) selectively shares the indices of the trajectory where '1' appears. The output \( \{2, 5, 6, 7, 9\} \) indicates that '1' is found at the 2nd, 5th, 6th, 7th, and 9th positions in the original mouse trajectory sequence \( \mathbf{T} \). This example illustrates how \( Q \) can be used to extract and share specific, privacy-preserving information from the trajectory data.

\section{Main Results}
The primary objective of this paper is to develop a mathematical framework that enables the distinction between humans and bots, while concurrently preserving user privacy.



\vspace{2cm}
This approach aims to maintain the confidentiality of individual user data while providing sufficient information to accurately distinguish between two users.

\begin{Theorem}
\label{th:1}
    ...

\begin{proof}
Please refer to Appendix \ref{A:1}.
\end{proof}
\end{Theorem}

\begin{Theorem}
\label{th:2}
    ...
\begin{proof}
Please refer to Appendix \ref{A:2}.
\end{proof}
\end{Theorem}







%%%%%% 
%% Appendix:
%% If needed a single appendix is created by
%%
\newpage
\appendices

\section{Proof of Theorem \ref{th:1}}
\label{A:1}

Given a sequence of queries resulting in responses \( T_1, T_2, \ldots, T_n \), now assume the agent has only received the first answer which is $T_1$, so the probability distribution of having hypothesis $H_1$ or $H_0$ after receiving the first query is like the following:
\begin{align*}
    q_1 = P_{H_i|T_1} = \frac{P_iP_{T_1|H}}{P_{T_1}}, i \in {0,1}
\end{align*}

We can continue and write this distribution for $T_1, T_2$:

\begin{align*}
    & q_2 = P_{H_i|T_1,T_2} = \frac{P_{T_2|H_iT_1}P_{H_i|T_1}}{P_{T_2|T_1}}
    \\& = \frac{P_iP_{T_1|H_i}P_{T_2|H_iT_1}}{P_{T_1}P_{T_2|T_1}}
\end{align*}

By now we can calculate the likelihood ratio:

\begin{align*}
&\Lambda = \frac{\frac{P_0P_{T_1|H_0}P_{T_2|H_0T_1}}{P_{T_1}P_{T_2|T_1}}}{\frac{P_1P_{T_1|H_1}P_{T_2|H_1T_1}}{P_{T_1}P_{T_2|T_1}}}
= \frac{P_0P_{T_1|H_0}P_{T_2|H_0T_1}}{P_1P_{T_1|H_1}P_{T_2|H_1T_1}}
\end{align*}

\begin{align*}
    \frac{P_{T_1|H_0}P_{T_2|H_0T_1}}{P_{T_1|H_1}P_{T_2|H_1T_1}}>< \frac{P_1}{P_0}
\end{align*}
 

\section{Proof of Theorem \ref{th:2}}

\section*{Theoretical Bound on the Error Exponent}

Given the error exponent 
\[
\Esf(\Qsf_K) = - \frac{1}{\mathbb{E} \left[ \sum_{k \in [K]} \mathsf{T}_{k}\right]}  \log \beta_K,
\]
we aim to find a theoretical bound by relating it to the information content of the queries and the distinguishability of the hypotheses.

Assuming the queries are optimally chosen to maximize the information gain about the hypotheses, the cumulative information gain across all queries can be approximated by the sum of Kullback-Leibler divergences between the probability distributions under the two hypotheses for each query-hit exchange. Formally, let $D(P_{X|Q_k} || P_{Y|Q_k})$ denote the KL divergence for the $k$-th query, then
\[
\mathcal{I}_{\text{cumulative}} = \sum_{k \in [K]} D(P_{X|Q_k} || P_{Y|Q_k}).
\]

Following the Chernoff-Stein Lemma, the error exponent for distinguishing between the two hypotheses is bounded by this cumulative information gain. Therefore,
\[
\Esf(\Qsf_K) \geq \frac{\mathcal{I}_{\text{cumulative}}}{\mathbb{E} \left[ \sum_{k \in [K]} \mathsf{T}_{k}\right]},
\]
where the expectation in the denominator reflects the average time required to accumulate this information.

This formulation highlights the trade-off between the speed of information acquisition and the accuracy of hypothesis discrimination.


\[
\Esf^* = \max_{\Qsf_K} \left\{ -\frac{1}{\mathbb{E}[\sum_{k=1}^K \mathsf{T}_k]} \log \beta_K \right\},
\]

subject to:

\[
\alpha_K \leq \epsilon,
\]

where \(\epsilon\) is the acceptable Type-I error rate, and the maximization is over all permissible sequences of queries \(\Qsf_K\) that adhere to the constraint.

\[
\Esf(\Qsf_K) \approx - \frac{C(H_0, H_1)}{\mathbb{E} \left[ \sum_{k \in [K]} \mathsf{T}_{k}\right]},
\]

where \(C(H_0, H_1)\) represents the Chernoff information between hypotheses \(H_0\) and \(H_1\), which serves as an approximation to the cumulative mutual information \(\mathcal{I}_{\text{cumulative}}\) gained from the sequence of \(K\) query-hit exchanges.

Given a querying strategy $\mathcal{Q}_K$ comprising $K$ queries, the objective is to maximize the error exponent $\Esf(\mathcal{Q}_K)$, defined by:

\[
\Esf(\mathcal{Q}_K) = - \frac{1}{\mathbb{E} \left[ \sum_{k=1}^{K} \mathsf{T}_k \right]} \log \beta_K,
\]

subject to:

\[
\alpha_K \leq \epsilon,
\]

where $\mathsf{T}_k$ denotes the hit-time for the $k$-th query, $\beta_K$ is the Type-II error probability, and $\alpha_K$ is the Type-I error probability with $\epsilon$ being the acceptable threshold.

The optimization problem can be further detailed as:

\[
\max_{\mathcal{Q}_K} \left\{ -\frac{1}{\mathbb{E} \left[ \sum_{k=1}^{K} \mathsf{T}_k \right]} \log \left( \sum_{k=1}^{K} I(Q_k; H | Z^{T_{k-1}}) \right) \right\},
\]

where $I(Q_k; H | Z^{T_{k-1}})$ represents the mutual information between the $k$-th query response and the hypothesis, given the prior observations.

This formulation aims to find the set of queries $\{Q_k\}_{k\in [K]}$ that optimizes the information gain about the hypotheses while minimizing the expected time to make a decision, all within the constraints of maintaining a Type-I error rate below $\epsilon$.
\begin{align*}
    \mathbb{E} \left[ \sum_{k=1}^{K} \mathsf{T}_k \right] = \sum_{k=1}^{K} \mathbb{E}[\mathsf{T}_k],
\end{align*}


    \[
\Esf(\Qsf_K) = - \frac{1}{\sum_{k=1}^{K} \mathbb{E}[\mathsf{T}_k]} \log \left( \sum_{k=1}^{K} \left( H(H | Z^{T_{k-1}}) - H(H | Q_k, Z^{T_{k-1}}) \right) \right),
\]

subject to:

\[
\alpha_K \leq \epsilon.
\]

\[
\Esf(\Qsf_K) \approx -\frac{1}{K\mu_T} \log \left( \exp\left(-\frac{\delta^2 K \bar{I}}{2\sigma_I^2}\right) \right),
\]

subject to:

\[
\Pr\left(S_K \geq (1 + \delta)K\mu_T\right) \leq \exp\left(-\frac{\delta^2 K \mu_T}{3\sigma_T^2}\right),
\]

\[
\Pr\left(G_K \leq (1 - \delta)K\bar{I}\right) \leq \exp\left(-\frac{\delta^2 K \bar{I}}{2\sigma_I^2}\right).
\]


\label{A:2}





\section{Various considerations}

SR 
\begin{enumerate}
    \item  scaling with the cardinality $\Zcal$
    \item  adding $\| \Zcal \| $ to the rate constraint scaling should allow one to compare across lengths
    \item  query/hit  or probe/echo or ping/response
\end{enumerate}

\bibliographystyle{unsrt}
\bibliography{References}
\end{document}



