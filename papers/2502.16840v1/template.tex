\documentclass{article}

\usepackage[numbers]{natbib}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{array}
\usepackage{amsmath}
% \usepackage{algorithm}
\usepackage{algorithm2e}
%\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{bbm}


\title{In-context learning of evolving data streams with tabular foundational models}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
\date{} 					% Or removing it

\author{ 
    {\hspace{1mm}Afonso Lourenço} \\
	Polytechnic of Porto \\
	\texttt{fonso@isep.ipp.pt} 
    \And
    {\hspace{1mm}João Gama} \\
	University of Porto \\
    \And
    {\hspace{1mm}Eric P. Xing} \\
	CMU, MBZUAI \\
    \And
    {\hspace{1mm}Goreti Marreiros} \\
	Polytechnic of Porto \\
}


% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{}
%\renewcommand{\undertitle}{}
\renewcommand{\shorttitle}{In-context learning of evolving data streams with tabular foundational models}

\begin{document}
\maketitle

\begin{abstract}
	State-of-the-art data stream mining in supervised classification has traditionally relied on ensembles of incremental decision trees. However, the emergence of large tabular models, i.e., transformers designed for structured numerical data, marks a significant paradigm shift. These models move beyond traditional weight updates, instead employing in-context learning through prompt tuning. By using on-the-fly sketches to summarize unbounded streaming data, one can feed this information into a pre-trained model for efficient processing. This work bridges advancements from both areas, highlighting how transformers' implicit meta-learning abilities, pre-training on drifting natural data, and reliance on context optimization directly address the core challenges of adaptive learning in dynamic environments. Exploring real-time model adaptation, this research demonstrates that TabPFN, coupled with a simple sliding memory strategy, consistently outperforms ensembles of Hoeffding trees across all non-stationary benchmarks. Several promising research directions are outlined in the paper. The authors urge the community to explore these ideas, offering valuable opportunities to advance in-context stream learning.
\end{abstract}


% keywords can be removed
\keywords{Data stream \and concept drift \and transformer \and in-context learning}

\section{Introduction}

\begin{wrapfigure}{r}{0.33\textwidth}  % "r" for right alignment, 0.5 is the width
    \vspace{-37pt}  % Optional: Adjust space above the figure
    \centering
    \begin{subfigure}[t]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Fig0.pdf}
        \caption{In-context stream mining}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Fig1.pdf}
        \caption{Conventional stream mining}
    \end{subfigure}
    \caption{A new paradigm}
    \label{fig:newparadigm}
\end{wrapfigure}

Data stream mining is an area in machine learning where the inference and training of the algorithm are performed in real-time, dealing with large volumes of ever-evolving tabular data. Differently to batch learning where all training data necessary to induce a model is available, data streams incrementally arrive at any time. Thus, making a model easily outdated due to the occurrence of concept drifts, i.e., distribution changes over time \cite{gama2010knowledge}. To circumvent these challenges, many stream mining algorithms have been proposed with different requirements being imposed: (1) be ready to predict and update the model at any point, and in limited time; (2) process an example at a time, inspecting only once with a limited sized model; (3) be able to adapt to change. 

\textbf{Decision trees.} In fulfilling these requirements, the current state-of-art for supervised learning in tabular data streams has long been incremental decision trees (IDTs), whose success can be attributed to two key factors: approximation-based splitting techniques \cite{domingos2001catching} and effective adaptation strategies \cite{gama2014survey}. Approximation-based splitting involves updating statistical summaries of entropy-based metrics, e.g. information gain, and determining whether the observed utility of a split is statistically close to its true utility when the data distribution is unknown, e.g. via the Hoeffding bound \cite{hoeffding1994probability}. However, as tree grows from the root node, the descendant nodes subsequently get fixed to covering particular sub-spaces of the space covered by their parent node. To address this, adaptation strategies allow to trigger the forgetting of old knowledge, pruning the affected parent nodes, instead of a complete replacement of the base learner \cite{bifet2009adaptive}. Alternatively, multiple trees can be combined into a high diversity ensemble, which naturally develops decision boundaries that locally adapt to changes in the data distribution \cite{gomes2019streaming,krawczyk2017ensemble}. Upon a concept drift, the diversity among trees can be exploited as some local minima are less affected than others. Affected components can be dynamically deleted, added, and combined with a voting scheme, weighting different weak learners based on their past performance.

\textbf{Lack of autonomy.} Regardless of how sophisticated these strategies are, these streaming algorithms still rely on human expertise to select the most suitable algorithm, or determine the optimal configuration for a given algorithm. Similarly to the way drift detectors identify when the specific trained model is no longer performing, the model class itself may no longer be applicable. The performance curves of different streaming algorithms, inductive biases, and configurations cross as the stream evolves. In these moments, the learning framework should autonomously select both the right algorithm \cite{van2015having} and corresponding hyperparameter settings \cite{veloso2018self}.

\textbf{Large Language Models (LLMs).} Conversely, the disruptive success and knowledge-modeling capabilities of LLMs, e.g. GPT-3 \cite{brown2020language} and LLaMA \cite{touvron2023llama} brings the opportunity to tap into prior experience, to the point that most of the required learning has already been done beforehand. While the aforementioned streaming methods follow the traditional two-stage methodology, i.e., first optimizing the model with several tuning iterations over a sliding window to then select the best algorithm or hyperparameter configuration for the next window, LLMs allow to instantaneously deploy a model, bypassing the computational and temporal costs of this traditional process. By pretraining on extremely large corpora of textual datasets via autoregressive next-token prediction, LLMs develop common sense inductive biases from the infinite amounts of prior learning experiences. Thus, driving emergent abilities that improve with scale, such as zero- and few-shot in-context learning (ICL), which allows models to perform new tasks at inference time by conditioning on a prompt of input-output examples, without any parameter updates \cite{brown2020language,raffel2020exploring}.

\textbf{Tabular prompts.} Consequently, this ICL ability pushed a new research paradigm, focused on adapting pre-trained models through prompt tuning rather than adjusting model weights, by discrete search with natural language, or optimizing the model's embedding space directly. Within this paradigm and the emerging possibilities of multimodal applications, various efforts were made for flexible prompts of non-textual information, such as structured tabular data tasks with heterogeneous feature spaces and mixed-type columns \cite{fang2024large}. As LLMs' capabilities on table understanding were validated, new efforts went towards directly applying them for supervised learning of tabular data, i.e. predicting an unseen sample by continuing its textual description on the target column \cite{dinh2022lift, hegselmann2023tabllm}. Despite the potential of these advancements to perform semantically-driven drift detection and model adaptation, several limitations are expected to persist in the near term hindering their application for data stream mining: limited context windows, quadratic complexity costs, struggle with autoregressively understanding continuous variables \cite{thawani2021representing,hopkins2023can,van2024latable}, and sensitivity to unexpected characters \cite{zhao2021calibrate, webson2021prompt}.

\textbf{Large Tabular Models (LTMs).} To alleviate all these limitations, one can shift to designing a dedicated architecture for numeric data from scratch, completely pre-trained on a wide range of tabular datasets with different data distributions, allowing it to learn relevant and general meta-features. These are referred to as LTMs \cite{van2024tabular}. While it has been known for a while that transformers can handle variable length input sequences in order to perform pretraining and cross-table knowledge transfer \cite{wang2022transtab}, it has been recently demonstrated that finetuning from a pretrained tabular transformer \cite{gorishniy2021revisiting, wu2021fastformer, somepalli2021saint} is superior to training tabular transformers from scratch \cite{zhu2023xtab}. Following these findings, various LTMs pretrained on synthetic tabular data have been proposed for classification \cite{hollmann2025accurate,bonet2024hyperfast,mueller2024mothernet,ye2023training} and time series forecasting \cite{dooley2024forecastpfn,bhethanabhotla2024mamba4cast}. For instance, the original TabPFN \cite{hollmann2022tabpfn} is pre-trained with a 12-layer Transformer for 18000 batches of 512 synthetically generated datasets generated using randomly initialized neural networks to impose the diverse interrelations, e.g. causal relationships and feature importance, that exist among the features of realistic tabular datasets \cite{muller2021transformers}. While this training step is moderately expensive, requiring a total of 20 hours on 8 GPUs (Nvidia RTX 2080 Ti), it is done offline, in advance, and only once, as part of the algorithm development. Then, in deployment TabPFN performs instant classification without fine-tuning, adapting to unseen datasets in a single forward pass at inference time, by using various training examples as context, analogously to how LLMs use the preceding tokens \cite{brown2020language}. However, in contrast to LLMs, LTMs like TabPFN allow scaling to much larger datasets, saving tuning resources during deployment, while providing consistent behavior supported by traditional statistical learning.

\textbf{This work.} Building on these advancements, this work proposes a new paradigm, using on-the-fly techniques to summarize unbounded data streams before feeding them to LTMs. While typical streaming methods rely on adapting the model parameters to deal with non-stationary data distributions, this new paradigm uses input data as learnable parameters without changing model parameters, decoupling the design of the learning algorithm from the foundational model. The main idea is to transform big data into a sketch whose size and running time has little or no dependency on the size of the data stream, as illustrated in Figure \ref{fig:newparadigm}. The learning algorithm is then applied to this dynamically contextualized sketch, with the main challenge of balancing the trade-off between sketch size and information loss. Thus, shifting from in-weights learning to in-context learning. The contributions are outlined as follows:

\begin{itemize}
\item Section~\ref{sec:background} provides an overview of the evolution of state-of-the-art methods for ensembles of IDT and transformers in tabular classification tasks.
\item Section~\ref{sec:methodology} offers insights into the drivers behind ICL ability and emphasizes the importance of locality for generalizing effectively to an evolving data stream. It discusses the management of biases introduced by both the foundational model and in-context examples, especially in the presence of concept drift.
\item Section~\ref{sec:experiments} presents experiments on several benchmark tabular datasets, demonstrating that a naive implementation of a sliding short- and long-term memory, paired with a state-of-the-art LTM, consistently outperforms all ensembles of IDT, with a reasonable processing time per instance.
\item Section~\ref{sec:discussion} highlights avenues for future research, including hypernetwork-based LTMs, on-the-fly context optimization strategies, and unique streaming challenges. The authors, too, are interested in exploring these directions further and invite collaboration.
\end{itemize}

\section{Background}
\label{sec:background}

This section describes the different advancements for decisions trees and transformers that make them now cross roads as competing solutions for data stream mining. First, describing the different adaptations needed to make ensembles of IDTs the state-of-the art for tabular data stream mining. Second, the different efforts of making transformers capable on working with tabular data and perform ICL.

\subsection{Incremental decision trees}

\textbf{Dynamic structural expansion.} To incrementally construct a IDT, using contained memory and time per sample, the fundamental algorithm design component is approximation-based splitting \cite{domingos2001catching}. As new instances arrive, they traverse the tree from the root to a leaf node, updating statistics at each node that guarantee for split decisions almost identical to those that conventional batch learning methods would make, despite lookahead and stability issues. Based on these sufficient statistics, IDTs continuously update the heuristic values for each attribute, with a distribution of potential split numeric values being approximate by a single Gaussian \cite{gama2004forest} and successfully executed according to a statistical Bound of the heuristic difference between attributes \cite{domingos2000mining}. However, while effective for incremental adaptation, this design component does not control for unnecessary tree growth. Thus, posing the risk of: (1) excessive memory consumption, due to multiple redundant splits on all features and increasing running statistics to keep updated; and (2) loss of plasticity, due to interference between concepts and descendant nodes getting subsequently fixed to the space covered by their parent node. To address this, various pre-pruning and pos-pruning techniques have been proposed. Pre-pruning strategies include splitting rules enhancements, adaptive tie breaking, adaptive grace periods, and activity-based expansion modes \cite{lourenco2025dfdt}. Pos-pruning includes various methods that estimate in each decision node whether the corresponding rooted subtree is consistent with the current data and worth pruning, e.g. checking if the current split outperforms the null attribute \cite{manapragada2018extremely} or directly monitoring the error rate in nodes, e.g. using the Page–Hinckley \cite{mouss2004test} which is designed to detect abrupt changes in the average of a Gaussian signal \cite{ikonomovska2011learning}, or the ADWIN change detector \cite{bifet2009adaptive}. To reconstruct the tree, one can simply require another attribute to become the top split attribute \cite{manapragada2018extremely}, restructure the otherwise pruned subtree 
\cite{heyden2024leveraging}, or train an alternative subtree as soon as an inconsistent node is detected, which only eventually replaces the original subtree when its accuracy is superior, and after a user-defined number of examples \cite{bifet2009adaptive}.

\textbf{Diversity.} While single IDTs need necessarily to be pruned, combining multiple trees into a high diversity ensemble allows to naturally develop decision boundaries that locally adapt to changes in the data distribution \cite{gomes2017survey,krawczyk2017ensemble}. Moreover, ensemble strategies can be combined with change detection methods to dynamically select, delete, add, and combine IDTs optimized for different regions of the solution space. For instance, Adaptive Random Forest (ARF) \cite{gomes2017adaptive} combines resampling with drift recovery strategies. It uses both online bagging and boosting to train classifiers iteratively, increasing the weight on misclassified instances \cite{oza2001experimental}. If a drift is detected, the background tree becomes the new active tree, and the old one is forgotten. Streaming Random Patches (SRP) \cite{gomes2019streaming} combines random subspaces and resampling to leverage diversity among base incremental learners, applying a global subspace strategy that increases diversity across weak learners. Streaming Gradient Boosted Trees (SGBT) \cite{gunasekara2024gradient} resets only some parts of the booster using an internal tree replacement mechanism, instead of externally monitoring each item in the boosting ensemble.

\textbf{Concept history.} However, such ensembles constantly train all active base classifiers, progressively discarding some of them when a drift is detected. Base learners evolve and the previously learned concepts are forgotten before they reoccur, if that benefits the current concept being learned. Consequently, these models might need to learn old concepts from scratch, which results in a waste of computational resources, longer training times, and significant prediction errors while models are not up-to-date with the latest state of the data stream. To circumvent this issue, meta-learning \cite{anderson2016recurring} and model-based clustering \cite{xu2016mining} allows to more explicitly model history mechanisms to retain a pool of previous active and inactive concepts. Consequently, maintaining this concept history creates the need of identifying what concept is present at each time. For this purpose, one can assume that two classifiers describe the same concept when predicting similarly during a time window \cite{anderson2016recurring}, while modeling time dependencies according to the concept drift properties \cite{almeida2018adapting}. Alternatively, one can recognize similar concepts based on cohesion and separability in the input space, e.g. euclidean distances between clusters representing different concept clusters \cite{xu2016mining}. In these, micro-clusters or latent features are used to make a synopsis of the incoming instances and reduce the computational cost of finding similarities among conceptual vectors \cite{katakis2010tracking}. Then, methods differ in how they manage this concept history.

\textbf{Algorithm selection.} Despite these sophisticated approximation-based splitting, adaptation and concept history strategies, these streaming algorithms still rely on human expertise to select the most suitable algorithm or determine the optimal configuration for a given algorithm. Indeed, combining all these building blocks in a single solution normally yields more inductive biases to assume and hyperparameters to tune. Consequently, as new concepts emerge, the performance curves of different algorithm settings cross. To address this, one can simply build heterogeneous ensembles with different algorithms, whose weights are dynamically selected based on recent performance \cite{van2015having}. Alternatively, one can calculate metafeatures over sliding windows to build a meta-model that predicts the best algorithm \cite{van2014algorithm} or the best drift detector \cite{aguiar2023enhancing} for the next interval of unseen data. Moreover, one can combine both approaches, with the meta-learner choosing an ensemble over the current data \cite{rossi2014metastream}. Regarding the hyperparameter settings, one can restart classifier hyperparameter tuning after concept drift \cite{veloso2018self}, meta-optimize drift detectors and classifier hyperparameters on-the-fly \cite{lacombe2021meta,lobo2021lightweight}, or even select the most suitable uncertainty threshold for active learning strategies \cite{martins2023meta}.

\subsection{Tabular foundational models}

\textbf{Multi-modal LLMs.} Within the discovery of ICL abilities \cite{brown2020language}, and the subsequent emerging possibilities of multimodal applications, various efforts were made for flexible prompts of tabular information \cite{fang2024large}. Initial research efforts focused on complementing multi-modal solutions with table understanding, such as Text2SQL, question answering, fact verification, and natural language inference \cite{yin2020tabert,deng2022turl,wang2021tuta,iida2021tabbie}. For instance, by extending BERT with a masked language model objective and additional embeddings designed to capture tabular structured data \cite{herzig2020tapas}. While in simpler architectures, an LLM takes in a query and serialized table as an input prompt, in more involved architectures, the system might be also connected to external spreadsheets, databases or programs, allowing for their manipulation, management, and integration \cite{zhang2023jellyfish, narayan2022can, fernandez2023large}.

\textbf{LLMs for tabular understanding.} In order for a foundational model to efficiently perform these tabular tasks it is important to compress tables, not only because large tables might not fit in the context length, but also due to the slow processing of long sentences caused by the quadratic complexity of self-attention \cite{vaswani2017attention}. Furthermore, compressing allows to manage the impact of tabular data challenges such as: noisy information in messy data and adversarial inputs \cite{cheng2022binding,zhou2023well}; random ordering of rows and table transpositions \cite{singha2023tabular}; arbitrary, sequential and shuffled column names \cite{singha2023tabular}; semi-structured content with rows as strings of key-value pairs and merged contiguous columns \cite{singha2023tabular}; and feature heterogeneity in understanding large tables. Motivated by these challenges, predefined constraints \cite{zhou2023well} and other methods \cite{herzig2020tapas,liu2021tapex} can be used to truncate the input based on a maximum sequence length. Moreover, the ability to search and retrieve information relevant to the tabular task can also be crucial to automate prompt compression. Instead of manually curating or randomly selecting k examples \cite{narayan2022can}, one can find k examples based on semantic similarity with a query, using a predefined distance metric of the question and example embedding, e.g. the Euclidean or negative cosine similarity \cite{gao2023text}. Domain-specific information, such as table and column names can be masked from this query.

\textbf{LLMs for tabular prediction.} Leveraging on these LLMs' validated capabilities on table understanding, new efforts went towards directly applying them for supervised learning of tabular data. While the same serialization and prompting strategies apply, tabular data prediction requires more sophisticated computation with numerical data, e.g. estimating threshold and densities. To address this, initial methods focused on training on large tabular metadatasets with masked reconstruction loss \cite{yin2020tabert,deng2022turl}, using text templates to convert tabular data into instruction-oriented formats \cite{borisov2022language}, fine-tuning LLMs on small collections of linearized tabular data to learn fundamental features before adapting it to a specific prediction \cite{hegselmann2023tabllm, wang2023unipredict, zhang2023towards} and fine-tuning on the specific prediction dataset to specialize its knowledge using Low-Rank Adaptation \cite{dinh2022lift}.

\textbf{LLM limitations.} Despite the potential of these advancements, several limitations hinder the application of LLMs for tabular supervised learning. Firstly, the limited context window restricts the number of examples that can be utilized for few-shot learning and the amount of additional semantic information that can be ingested, particularly in datasets that contain excessive numbers of categorical columns. Secondly, the high count of parameters, quadratic complexity cost of processing the table as string ($n$ samples · $m$ features) makes inference computationally expensive. Moreover, this computational cost is aggravated in numerical columns, as a single numerical variable is implicitly modeled as an autoregressive series of categorical variables, e.g. 1.23 might require 3 expensive forward calls to the model 1 → . → 23, plus the field separation token that follows. Thirdly, this tokenization and the multinomial training objective impact pattern formation, making LLMs struggle with autoregressively understanding whole numbers and modeling continuous variables, such as simple uniform and Gaussian distributions \cite{thawani2021representing,hopkins2023can,van2024latable}. Tokenization of integers leads to a fragmented approach where even basic mathematical operations require memorization rather than algorithmic processing. Fourthly, the performance remains sensitive to the precise details of the natural-language input, such as unexpected characters of poorly represented categorical features, inadequate metadata and confusing column names, with no assurances of consistent behavior \cite{zhao2021calibrate, webson2021prompt}.

\textbf{Tabular transformers.} To address these limitations, one can shift to designing a dedicated architecture for numeric data from scratch \cite{van2024tabular}. Indeed, the challenge of building transformer-based models for tabular data has been widely studied, using token-based mechanisms for feature selection and reasoning in heterogeneous tabular data \cite{shwartz2022tabular}. While in text and vision, data is intrinsically tied to the position of a token or pixel, the order-invariance of tabular data makes it challenging to understand the inherent relationships between features, which is further aggravated by heterogeneous and uninformative features, skewed heavy-tailed feature distributions, and extreme values \cite{grinsztajn2022tree,kadra2021well}. To circumvent these issues, one can develop a transformer-based architecture using a multi-head self-attentive neural network to automatically learn high-order feature interactions \cite{song2019autoint}, combine column descriptions and cells as input for feature encoding \cite{wang2022transtab}, use row and column attention mechanisms to capture the inter-sample interactions with self-supervised pretraining to deal with label scarcity \cite{somepalli2021saint}, use additive attention to consider the interaction between each token and the global representation, achieving a linear complexity \cite{wu2021fastformer}, transform categorical features into contextual embeddings \cite{huang2020tabtransformer,gorishniy2021revisiting}, or use sequential hierarchical subnetworks that prioritize the most significant features \cite{tabnet2019arik, gorishniy2021revisiting}.

\textbf{LTMs.} Despite the potential of these methods for pre-training, all these works solely focused on introducing various attention mechanisms to handle the tabular data structure, by attending relevant columns and performing attention across both rows and columns \cite{somepalli2021saint}, being mostly developed just by training from scratch on each target task. To evaluate the possible ICL capabilities for tabular data of such models, XTab pretrained three of the aforementioned tabular transformers \cite{gorishniy2021revisiting, wu2021fastformer, somepalli2021saint} with a diverse range of tabular datasets, separating the models into data-specific and shared components \cite{zhu2023xtab}. Thus, finding that finetuning from a pretrained tabular transformer is superior to training tabular transformers from scratch. Following these findings, various LTMs pretrained on synthetic tabular data have been proposed for classification \cite{hollmann2022tabpfn,bonet2024hyperfast,mueller2024mothernet,ye2023training} and time series forecasting \cite{dooley2024forecastpfn,bhethanabhotla2024mamba4cast}. In contrast to LLMs, LTMs allow scaling to larger contexts, saving tuning resources during deployment, while providing consistent behavior supported by traditional statistical learning. For instance, 
TabPFN \cite{hollmann2022tabpfn,hollmann2025accurate} naturally needs a single floating point number to express a single feature of decimals with two places, compared to the smallest version of LLAMA taking four tokens, i.e. 16384 floating point numbers or a sparse vector of length 128000 \cite{touvron2023llama}. 

\textbf{LTMs vs IDTs.} Revisiting the aforementioned building blocks of data stream mining with IDTs — dynamic structural expansion, diversity, concept history, and algorithm selection — it becomes evident that LTMs preclude the need to implement these components. Instead of serving as the central elements of the learning process, these building blocks should be embedded within the context provided to the LTM, complementing its already powerful capabilities. In this way, their role shifts from core components to supportive features, ultimately enhancing the LTM's effectiveness. Furthermore, their implementation aligns with the motivation behind prompt tuning itself, i.e. to improve tabular prediction and scaling with respect to context size (i.e., samples, features, and categories). For this purpose, several strategies have been proposed in batch settings, including summarization \cite{feuer2024tunetables,ma2024context}, meta-representations \cite{ye2023training}, retrieval-based localized context \cite{nejjar2024context}, ensembling \cite{xu2024mixture}, and fine-tuning \cite{breejen2023fine,xu2024mixture}. Further details on how these overlap with a streaming setting will be provided in Section \ref{sec:discussion}.

\section{In-context stream learning}
\label{sec:methodology}

This sections offers insights into the drivers behind the ICL ability, and how it addresses the core challenges of adaptive learning in dynamic environments. ICL requires a model to implicitly construct a map from in-context examples, of a new concept on which it has not been previously trained, to a predictor without any updates to the model's parameters themselves. Indeed, it has been show empirically that transformers with fixed parameters can learn linear functions comparable to the optimal least squares estimator, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context on the fly \cite{von2022what}. Furthermore, it has been demonstrated that transformers implement more complex function classes implicitly, i.e., sparse linear functions, two-layer neural networks, and decision trees, with performance that matches or exceeds task-specific learning algorithms \cite{min2022rethinking}. However, it remains unclear to what extent these models are able to learn new tasks from in-context examples alone as opposed to indexing into a vast set of known tasks from the training data. In this regard, it has been hypothesized that this emergent behavior of transformers is driven by both the distributional qualities of the training data itself \cite{chan2022data} and an architectural bias where the internal structure of the model ends up performing mesa-optimization \cite{bai2022uncovering}.

\textbf{Natural data hypothesis.} The data hypothesis was inspired by the observation that many natural data sources differ from typical batch supervised datasets due to a few notable features: (1) natural data is temporally bursty, i.e. a given concept may have a distribution that is not uniform across time, instead tending to appear in clusters \cite{altmann2009beyond}; (2) the meaning of these concepts in natural data is often dynamic rather than fixed, usually in a context-dependent way; (3) the marginal distribution across entities is highly skewed, with large numbers of rarely occurring classes \cite{chan2022data}. Indeed, these are the properties of data streams that motivate the development of incremental learning algorithms. While batch i.i.d. training typically consists of item classes that reoccur with uniform regularity, and input-output mappings that are fixed throughout training, data stream mining implies training on sequences of data with drifting concepts, akin to having many labels per item \cite{read2023multi}, a few highly reoccurring concepts \cite{anderson2016recurring}, and a large number of outliers that reoccur much more rarely, thus excluded from the concept forming process due to their disproportionately less likelihood to occur multiple times within a given context window \cite{de2016minas}. 

\begin{wrapfigure}{r}{0.4\textwidth}  % "r" for right, 0.4 is the width
    \vspace{-20pt}  % Optional: Adjust space above the figure
    \centering
    \includegraphics[width=0.4\textwidth]{figures/Fig2.pdf}
    \caption{ICL as a mesa-optimization capability}
    \label{fig:mesa}
\end{wrapfigure}

\textbf{Mesa-optimization hypothesis.} The algorithm hypothesis identifies this emergent behavior as stemming from an architectural bias where the internal structure of transformers ends up performing mesa-optimization \cite{bai2022uncovering}, which doesn't happen for other recurrent models, like LSTMs and RNNs, even if matched on number of parameters \cite{chan2022data}. Mesa-optimization occurs when a base optimizer is searching for algorithms to solve some problem and finds a model that is itself a mesa-optimizer \cite{hubinger2019risks}. Unlike the base objective, the mesa-objective is not specified directly by the programmers. Instead, a learned process running within the forward pass constructs an internal mesa-objective, and its corresponding solution through mesa-optimization, because it is instrumentally useful for solving the given task. While meta means to go above, thus stepping outside to a higher logical level, mesa means going to the other direction, to go down into the content narrowing the details into smaller and smaller pieces \cite{cheal2021what}. Many reasons have been identified for this phenomenon. Firstly, the base optimizer's incentive to find a compressed policy creates a bias towards simple solutions with low description length. Moreover, the transformer's statefulness favors mesa-optimization by decreasing the implicit penalty on time complexity imposed by enforcing all computations to end when the learned algorithm produces its output. And, the ability of the learned algorithm to save and refer back to previous state enables the caching of intermediate results. These factors, when combined with heavy pre-training on diverse tasks that are likely to be completely novel, incentivize the base optimizer to expend optimization power in learning how to adjust to a new task immediately in order to perform consistently in an diverse environment. Figure \ref{fig:mesa} illustrates this ability.

\textbf{Implicit meta-learning.} While breaking things down into bits is a mesa process, the model always returns to rebuild and re-map the actual details. In going mesa, all generalizations are deconstructed which allows to reconstruct the input-output mapping in a more resourceful manner. In this regard, training a model to perform ICL can also be viewed as an instance of the learning-to-learn or meta-learning paradigm, with learning that takes place on various time scales, fast and slow, comprising both mesa- and meta-processes.
Examples include approximating an implicit model selection procedure competitive with empirical risk minimization \cite{agrawal2022transformers}, a gradient-based few-shot learning within its forward pass \cite{von2022transformers} or a kernel-based sequence model \cite{mccoy2022recasting}. However, in these cases, researchers explicitly designed the meta-training regime to incentivize ICL, whereas in the case of transformers, the capacity for ICL is emergent. Neither the model’s transformer architecture nor its learning objective are explicitly designed with ICL in mind.

\begin{wrapfigure}{r}{0.4\textwidth}  % "r" for right, 0.4 is the width
    \vspace{-20pt}  % Optional: Adjust space above the figure
    \centering
    \includegraphics[width=0.4\textwidth]{figures/Fig3.pdf}
    \caption{ICL as bayesian inference}
    \label{fig:bayesian}
\end{wrapfigure}

\textbf{Bayesian perspective.} While the distinction between mesa- and meta-learning offer a perspective on how systems learn and adapt, a Bayesian theory can help to understand and quantify the uncertainty in the process of learning itself. In this regard, ICL can be viewed as approximate Bayesian inference, whether implicitly \cite{xie2022explanation} or explicitly \cite{garg2022transformers}. It has be shown that a Transformer directly fits the posterior predictive distribution (PPD), with a hypothesis latent concept $\phi$ which could parametrize the transitions of a Hidden Markov model \cite{baum1966statistical}, or a space of structural causal models. In this framework, the prior defines a space of hypotheses $\Phi$ on the relationship of a set of inputs $x$ to the output labels $y$. Each hypothesis $\phi \in \Phi$ can be seen as a mechanism that generates a data distribution from which one can draw context data with observed labels ${x_1,y_1,\ldots,x_n,y_n}$ and an unlabeled query sample $x_{query}$, as illustrated in Figure \ref{fig:bayesian}.

The PPD specifies the distribution of the label $p(y_{query} | x_{query}, x_1,y_1,\ldots,x_n,y_n)$ and can be obtained by integration over the space of hypotheses $\Phi$, where the weight of a hypothesis $\phi \in \Phi$ is determined by the posterior probability $p(\phi | x_1,y_1,\ldots,x_n,y_n)$ given the context data. Considering the unnormalized posterior, which corresponds to the prior probability $p(\phi)$ and the likelihood $p(x_1,y_1,\ldots,x_n,y_n | \phi)$ of the training data given $\phi$, we have:

\begin{equation}
p(y_{query}|x_1,y_1,\ldots,x_n,y_n,x_{query}) = \int_{\Phi} p(y_{query}| x_{query}, \phi) p(x_1,y_1,\ldots,x_n,y_n |\phi) p(\phi) d\phi
\end{equation}

\textbf{Frequentistic perspective.} Despite contexts being sampled from a different distribution than the pretraining distribution, the asymptotic prediction error of ICL is optimal when the signal about the latent concept in each context example is larger than the error due to the distribution mismatch. As a more informative and representative input space is provided, the learning error decreases \cite{xie2022explanation}. In this regard, one can also take a frequentistic perspective \cite{nagler2023statistical}. From a predictor's variance standpoint, being a pre-tuned, but untrained predictor with many hyperparameters and multi-head attention, a LTM has extremely high sensitivity to individual training samples, which translates in an increased ability to choose submodels and vanishing variance. From a bias standpoint, hyperparameters are pre-tuned to be optimal for a set of tasks defined by the prior. If this prior has large enough support and does not concentrate too much away from the true hypothesis, one can guarantee that the PPD converges to a close approximation of the true predictive distribution. Consequently, whether the LTM predictor can learn at inference time depends on its structural properties, with the optimal LTM approximation being characterized by a Kullback-Leibler criterion \cite{van2014renyi,nagler2023statistical}. To allow for accurate approximation of the conditional class probabilities, one needs sufficiently complex LTM models and prior. Practically, a LTM is trained on simulated data sets. The larger these data sets are, the more complex the PPD is approximated. The training set size can therefore be understood as a regularizer on the expected complexity of the network. Thus, while LTM implementations ensure vanishing predictor's variance, the bias vanishes only if the provided data is somewhat localized around the test features.

\subsection{Context optimization on evolving data streams}

Based on this intuition, the objective of in-context stream learning is to minimize bias in model predictions when faced with new data that drifts over time. This problem occurs in situations where the LTM \( f_\theta \), is applied on-the-fly to a continuous stream of real-world data. The data stream is assumed to be partitioned sequentially into non-overlapping segments, called concepts, such as \( C_i \) and \( C_{i+1} \), each corresponding to distinct joint data distributions, \( p_i(x, y) \) and \( p_{i+1}(x, y) \), respectively. Within each concept, the data distribution is assumed static, however, as the data stream progresses, drift between consecutive concepts can introduce discrepancies between past and future data. For instance, if concept \( C_i \) has a skewed label distribution \( p_i(y) \), and the following concept \( C_{i+1} \) has a uniform label distribution \( p_{i+1}(y) \), then the predictive model must adapt to account for this shift in label distribution \cite{nejjar2024context}. The goal is to select an optimal localized context $l$, composed of \( (x_1, y_1, \ldots, x_n, y_n, x_{\text{query}}) \), which helps \( f_\theta \) approximate the correct output \( y_{\text{query}} \) even for cases in the tail regions of these drifting distributions. In this case, the model’s expected prediction error for a new sample \( (x_{\text{query}}, y_{\text{query}}) \) is defined by:
\begin{equation}
\begin{split}
EPE_{f_\theta}(x_{\text{query}}) &= \mathbb{E}_l[(f_\theta(x_{\text{query}} \mid x_1, y_1, \ldots, x_n, y_n) - \mathbb{E}[y_{\text{query}} \mid x_{\text{query}}])^2] \\
&= \mathbb{E}_l[( (f_\theta(x_{\text{query}} \mid x_1, y_1, \ldots, x_n, y_n) - \mathbb{E}_l[f_\theta(x_{\text{query}} \mid x_1, y_1, \ldots, x_n, y_n)]) \\
&+ (\mathbb{E}_l[f_\theta(x_{\text{query}} \mid x_1, y_1, \ldots, x_n, y_n)] - \mathbb{E}[y_{\text{query}} \mid x_{\text{query}}]) )^2] \\
&= \text{Var}[f_\theta(x_{\text{query}} \mid x_1, y_1, \ldots, x_n, y_n)] + \left( \text{Bias}^2[f_\theta(x_{\text{query}} \mid x_1, y_1, \ldots, x_n, y_n)] \right) + \sigma^2
\end{split}
\end{equation}

\textbf{LTM's variance decreases with more in-context examples.} Assume \( f_\theta \) to be \( c \)-Lipschitz continuous with a constant \( c = (c_1, \ldots, c_n) \) where each \( c_i \) scales as \( \delta i^{-\alpha} \), \( \delta \) is a positive constant, and \( \alpha > 0.5 \). Then, for two independently sampled contexts \( (x_1, y_1, \ldots, x_n, y_n, x_{\text{query}}) \) and \((x_1', y_1', \ldots, x_n', y_n', x_{\text{query}}) \) from an unbounded \( D_{stream} \), the inequality $|f_\theta(x_{\text{query}} \mid (x_1, y_1, \ldots, x_n, y_n)) - f_\theta(x_{\text{query}} \mid (x_1', y_1', \ldots, x_n', y_n'))| \leq \sum_{i=1}^n c_i \mathbbm{1}_{\{x_i \neq x'_i\}}$ holds \cite{garg2022transformers}. Applying McDiarmid’s Inequality \cite{mcdiarmid1989method}, for any \( t > 0 \), the tail probability bound is:
\begin{equation}
Pr(|f_\theta(x_{\text{query}} \mid x_1, y_1, \ldots, x_n, y_n) - \mathbb{E}_l[f_\theta(x_{\text{query}} \mid x_1, y_1, \ldots, x_n, y_n)]| \geq t) \leq 2 \exp\left(-\frac{2t^2}{|c|^2_2}\right)
\end{equation}

where \( |c|^2_2 = \sum_{i=1}^{\infty}(\delta i^{-\alpha})^2 \), converging due to \( \alpha > 0.5 \). By the Borel-Cantelli lemma, \( f_\theta(x_{\text{query}} \mid x_1, y_1, \ldots, x_n, y_n) \) converges almost surely to the expected prediction \( \mathbb{E_s}[f_\theta(x_{\text{query}} \mid x_1, y_1, \ldots, x_n, y_n)] \) as \( n \rightarrow \infty \) \cite{nejjar2024context}. This setup implies that as more samples are added to the prompt, the model’s sensitivity to small input changes diminishes, reducing error volatility.

\textbf{LTM's biases inherited from in-context examples.} Assume \( f_\theta \) mimics gradient descent steps on examples \( (x_1, y_1, \ldots, x_n, y_n, x_{\text{query}}) \), the model’s average prediction over many samples \( \mathbb{E}_l[f_\theta(x_{\text{query}} \mid x_1, y_1, \ldots, x_n, y_n)] \) almost surely converges to the true conditional expectation \( \mathbb{E}[y_{\text{query}} \mid x_{\text{query}}] \) as \( n \to \infty \), provided there is no drift in the data distribution \cite{von2022transformers}. However, if there is a shift in the distribution, \( (x_1, y_1, \ldots, x_n, y_n, x_{\text{query}}) \) may become misaligned with the current distribution, leading to biased predictions that reflect outdated patterns. As a result, if \( (x_1, y_1, \ldots, x_n, y_n, x_{\text{query}}) \) is composed of samples that no longer represent the evolving data accurately, \( f_\theta \) ceases to be an unbiased estimator. By keeping representative samples of reoccurring concepts, while gradually incorporating data from new concepts, one can create a balanced pool of samples to created an unbiased context \( (x_1, y_1, \ldots, x_n, y_n, x_{\text{query}}) \).

\textbf{Limited context size.} Each data point in the dataset does not have the same potential, as some of the data points can be more representative or informative than others. To make the most out of in-context examples, researchers have studied a variety of data summarizing techniques, often called sketching or set cover methods, which consist of finding a small subset of data points that cover a large dataset \cite{feldman2020core}. In the context of a pretrained transformer $f_\theta$, the goal is to find a sketching function $s : \mathbb{R}^{N \times d} \rightarrow \mathbb{R}^{n \times d}$ such that: \( \mathbb{E}_s[f_\theta(x_{\text{query}} \mid s(D_{\text{stream}}))] \). This sketching function can also be viewed in terms of coreset construction. For instance, a simple yet effective summarizing policy is reservoir sampling \cite{vitter1985random}, which randomly selects a sample of $k$ items, and can be viewed as choosing a diagonal sketching matrix $S \in \mathbb{R}^{n \times n}$ where the diagonal entries are 1 if the point was picked. This is arguably the most straightforward way to reduce the size of dataset, simply picking as many points as permissible uniformly at random. Though it is rare for an algorithm to produce a coreset in this straightforward fashion, it has been shown that for class-balanced and outlier-resistant problems this approach can already provide reasonable results. Indeed, one should note that the existence of coresets is trivial, as the original data set itself is in fact a coreset. The key question is the existence of small coresets where the coreset size is sublinear in the number of data points, while at the same time being polynomial in other parameters, e.g. the dimension and desired approximation error \cite{feldman2020core}.

\textbf{Inductive biases in context optimization.} Ultimately, adaptation requires expertise and can not result only from the data. The need for a localization strategy reinforces the role of inductive biases, i.e. the set of assumptions enforced into a model to perform adaptation, putting the human expert back at the center of learning. Examples include: the smoothness assumption, where if two points reside in a high-density region are close, then so should be their corresponding outputs; the cluster assumption, where if points are in the same cluster, they are likely to be of the same concept; the manifold assumption, where high-dimensional data lies in a low-dimensional manifold; or the temporal assumption, where recent points are likely to be of the same concept. However, in any of these cases, as concepts drift and reoccur over time, abruptly or gradually, the model may rely too heavily on outdated or unrepresentative samples, leading to systematic errors in prediction. While the variance term decreases as more context examples are considered, the challenge lies in the fact that averaging over a set of \( n \) samples close to a given query point, either spatially, e.g. using cosine similarity, or temporally, e.g. using recent samples, may not necessarily correspond to a similar concept.

\section{Experimental study}
\label{sec:experiments}

This section evaluates the performance of a naive streaming implementation of a large tabular model (LTM) against state-of-art algorithms for data stream mining, namely: Adaptive Random Forest(ARF) \cite{gomes2017adaptive}, Streaming Random Patches (SRP) \cite{gomes2019streaming}, Boosting-like Online Learning Ensemble (BOLE) \cite{de2016boosting}, Leverage Bagging (LevBag) \cite{bifet2010leveraging}, Extremely Fast Decision Tree (EFDT) \cite{manapragada2018extremely}, and Very Fast Decision Tree (VFDT) \cite{domingos2000mining}. These algorithms were implemented within the Massive Online Analysis (MOA) framework\footnote{https://github.com/Waikato/moa}, and tuned for each dataset using a grid search strategy, with ensembles of up to 90 components, grace periods set to 100, 400, and 1000, and tie-splitting thresholds configured to 0.01, 0.05, and 0.1 as the hyperparameter settings.

\begin{wrapfigure}{r}{0.40\textwidth}  % "r" for right, 0.4 is the width
    \vspace{-10pt}  % Optional: Adjust space above the figure
    \centering
    \includegraphics[width=0.40\textwidth]{figures/Fig4.pdf}
    \caption{Special attention mark}
    \label{fig:attention}
\end{wrapfigure}

The LTM was implemented with the publicly available first version of TabPFN \footnote{https://github.com/PriorLabs/TabPFN} on a T4 GPU. This model relies on causal attention layers typical of a standard decoder-only Transformer architecture \cite{liu2018generating}, making it possible to process a streaming sequence where tokens become available one at a time \cite{mccoy2022recasting}. However, with a special attention mask, where in-context tokens only attend to each other, with no attention to the query instances, as illustrated in Figure \ref{fig:attention}. Instead, the target to the query instances are used as a label to minimize the cross-entropy loss \cite{muller2021transformers}. Since tabular columns are permutation invariant, the feature orderings and scalings are shuffled for an ensembled prediction. In these experiments, 4 permutations were performed.

\begin{wrapfigure}{r}{0.40\textwidth}  % "r" for right, 0.4 is the width
    \vspace{-10pt}  % Optional: Adjust space above the figure
    \centering
    \includegraphics[width=0.35\textwidth]{figures/Fig6.pdf}
    \caption{Short- and long-term memories}
    \label{fig:naive}
\end{wrapfigure}

For stream mining, a naive localization scheme was used, combining two memory strategies, as depicted in Figure \ref{fig:naive}: a First-In-First-Out (FIFO) short-term memory that stores $M_{\text{short}}$ recent samples, and a long-term memory of size $M - M_{\text{short}}$ which stores a representative subset of recent samples from each class so that the limited in-context coreset contains richer information regarding to the whole data stream. This allows to simultaneously view the data from the current and past concepts, as the model can adapt to new knowledge while reducing the catastrophic forgetting for the past. In these experiments, $M$ was set to 1000 instances, and $M_{\text{short}}$ to 750 instances. Moreover, an initial warming period $T_{\text{warm}}$ of 100 instances was considered to collect samples into the memory, without making predictions.

\begin{wrapfigure}{r}{0.40\textwidth}  % Place the algorithm on the right side
    \vspace{-10pt}  % Optional: Adjust space above the algorithm
    
    \begin{algorithm}[H]
        \caption{Naive context optimization}
        \label{alg:ltm}
        \KwIn{$D_{\text{stream}}$, $M_{\text{short}}$, $M_{\text{long}}$, $T_{\text{warm}}$, $LTM$}       
        $S, L \gets \emptyset$\;
        $C[y] \gets 0, \forall y \in \mathcal{Y}$\;
        $t_{\text{warm}} \gets 0$\;
        
        \For{each new sample $(x_t, y_t) \in D$}{
            \If{$t_{\text{warm}} > T_{\text{warm}}$}{
                $\hat{y} \gets \text{learn in-context}(LTM, S \cup L)$\;
                $A_T \gets \frac{1}{T} \sum_{t=1}^{T} \mathbb{1} (\hat{y}_t = y_t)$\;
            }

            $S \gets S \cup \{(x_t, y_t)\}$\;

            \If{$|S| > M_{\text{short}}$}{
                $(x_{\text{old}}, y_{\text{old}}) \gets$ oldest sample in $S$\;
                $S \gets S \setminus \{(x_{\text{old}}, y_{\text{old}})\}$\;
                $L \gets L \cup \{(x_{\text{old}}, y_{\text{old}})\}$\;
                $C[y_{\text{old}}] \gets C[y_{\text{old}}] + 1$\;
            }

            \If{$|L| > M_{\text{long}}$}{
                $y_{\max} \gets \arg\max_{y \in \mathcal{Y}} C[y]$\;
                $(x_{\text{old}}, y_{\text{old}}) \gets$ oldest sample in $\{ (x, y) \in L \mid y = y_{\max} \}$\;
                $L \gets L \setminus \{(x_{\text{old}}, y_{\text{old}})\}$\;
                $C[y_{\text{old}}] \gets C[y_{\text{old}}] - 1$\;
            }
        }
    \end{algorithm}
\end{wrapfigure}

The pseudocode is given in Algorithm \ref{alg:ltm}. The algorithm begins by initializing the short-term memory $S$ and long-term memory $L$, along with a class count dictionary $C[y]$ for each class $y$. The size of the long-term memory is computed as $M_{\text{long}} = M - M_{\text{short}}$. After a warming period $T_{\text{warm}}$, the algorithm enters the main loop, processing each new sample $(x_t, y_t)$ from the data stream $D_{\text{stream}}$. For each new sample, the algorithm first adds it to the short-term memory $S$. If the short-term memory exceeds its maximum size $M_{\text{short}}$, the oldest sample is removed and moved to the long-term memory $L$. The class count for the corresponding class of the removed sample is updated in the dictionary $C$. Once the long-term memory exceeds its capacity $M_{\text{long}}$, the algorithm identifies the most overrepresented class, $y_{\max}$, in long-term memory $L$. The oldest sample from this overrepresented class is then removed to maintain balance in the memory, ensuring that the system does not retain too many samples from any single class. Throughout the process, the algorithm continuously updates both memories while ensuring the long-term memory remains balanced across classes, and the system adapts to new data as it arrives. 

In evaluating the application of any foundation model, one can consider both intrinsic and extrinsic evaluation metrics \cite{bommasani2022opportunities}. Extrinsic evaluation refers to measuring the performance of the adapted foundational model on a downstream task, whereas intrinsic evaluation refers to directly measuring the foundational model's quality. Since this work is concerned with the adaptability of LTMs to drifting streams, only extrinsic performance evaluation is performed, using a prequential evaluation strategy, where each instance is first used to test then update the classifier in an online manner (instance by instance) \cite{gama2013evaluating}. The used data streams are from the USP data stream repository\footnote{https://sites.google.com/view/uspdsrepository}, encompassing both binary and multiclass classification tasks: NOAA Weather (18,159 instances), SmartMeter LADPU (22,950), Electricity (45,312), Rialto Bridge Timelapse (82,250), Posture (164,860), Forest CoverType (581,012) and PokerHand (829,201).

\begin{wrapfigure}{r}{0.4\textwidth}  % "r" for right, 0.4 is the width
    \vspace{-10pt}  % Optional: Adjust space above the figure
    \centering
    \includegraphics[width=0.4\textwidth]{figures/cdd3.pdf}
    \caption{Nemeyi test at p-value = 5\%}
    \label{fig:cdd}
\end{wrapfigure}

Table \ref{tab:results} and Figure \ref{fig:cdd} present a comparison of LTM's accuracy against the aforementioned baselines. LTM consistently outperforms, with an average accuracy of 82.83\% and optimal rank of 1. The top competing algorithms, ARF and SRP, achieved average accuracies of 77.47\% and 78.87\%, respectively. The most significant performance gains were observed on the POKER, RIALTO, METER, and NOAA datasets, where LTM outperformed the best alternative by 9.4\%, 6.81\%, 5.55\%, and 3.94\%, respectively. Moreover, it is important to highlight that LTM eliminates the need for hyperparameter tuning, as it is a pre-tuned model. In contrast, the baseline algorithms can exhibit significant variability in their results depending on the parameter settings. This issue is especially pronounced for single tree models like EFDT and HT, which show high standard deviations of up to 4.1 and 7.02, respectively. Although increasing the number of ensembles provides more stability, SRP, LevBag, BOLE, and ARF still exhibit standard deviations of up to 1.81, 5.78, 1.54, and 0.43, respectively. This underscores the advantage of LTM’s consistency in performance across varying conditions.

\begin{table}[!ht]
    \centering
    \caption{Prequential accuracy}
    \label{tab:results}
    \begin{tabular}{cccccccc}
        \toprule
        Dataset & LTM & ARF (n=90) & BOLE (n=50) & LevBag (n=45) & SRP (n=80) & EFDT & VFDT \\ \midrule
        NOAA & \textbf{81.45} & 78.36 ± 0.20 & 73.45 ± 0.51 & 75.51 ± 0.23 & 78.35 ± 0.18 & 72.34 ± 0.43 & 70.86 ± 1.56 \\ 
        METER & \textbf{73.56} & 68.02 ± 0.36 & 67.28 ± 1.54 & 63.17 ± 5.78 & 69.69 ± 1.81 & 57.94 ± 2.21 & 52.07 ± 2.63 \\
        ELEC & \textbf{91.70} & 87.89 ± 0.22 & 90.17 ± 0.98 & 85.86 ± 0.16 & 88.70 ± 0.57 & 77.96 ± 1.28 & 77.87 ± 1.29 \\
        RIALTO & \textbf{80.23} & 67.95 ± 0.36 & 47.27 ± 1.54 & 60.99 ± 5.78 & 75.12 ± 1.81 & 55.79 ± 2.21 & 28.68 ± 2.63 \\ 
        POSTURE & \textbf{59.79} & 59.14 ± 0.19 & 48.28 ± 1.09 & 55.44 ± 0.69 & 58.14 ± 0.55 & 48.91 ± 0.18 & 52.99 ± 1.05 \\ 
        COVER & \textbf{95.51} & 91.75 ± 0.14 & 93.33 ± 0.59 & 85.26 ± 0.24 & 94.54 ± 0.20 & 82.38 ± 1.08 & 76.98 ± 2.37 \\
        POKER & \textbf{97.55} & 89.17 ± 0.43 & 80.01 ± 1.54 & 88.52 ± 3.12 & 87.58 ± 0.88 & 77.38 ± 4.10 & 77.43 ± 7.02 \\ \midrule
        Average & \textbf{82.83} & 77.47 & 71.40 & 73.54 & 78.87 & 67.53 & 62.41 \\
        Rank & \textbf{1.0} & 2.86 & 4.57 & 4.29 & 2.71 & 6 & 6.57 \\
        \bottomrule
    \end{tabular}
\end{table}

Figure \ref{fig:results2} illustrates the evolution of accuracy over time for COVER, showing that LTM experiences significantly smaller performance drops at drift points compared to the baseline learners. While these baseline algorithms demonstrate rapid online convergence, LTM’s windowing approach excels in handling concept drift, maintaining a strong focus on recent instances. It is worth noting that, although LTM outperforms in drift scenarios, the competing algorithms can match LTM's performance in long sequences of stable concepts. However, LTM remains robust, delivering consistent performance that is largely unaffected by drifts. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/plot5.pdf}
    \caption{Prequential accuracy over time}
    \label{fig:results2}
\end{figure}

To further substantiate the results, Table \ref{tab:otherresults} presents the reported accuracies from the original papers, including not only their own performance but also the results of other algorithms, Online Smooth Boosting (OB) \cite{chen2012online}, Online Accuracy Updated Ensemble (OAUE) \cite{brzezinski2014combining}, Dynamic Weighted Majority (DWM) \cite{kotler2003dynamic}, and Streaming Gradient Boosted Trees (SGBT) \cite{gunasekara2024gradient}. Notably, no reported algorithm outperforms LTM.

\begin{table}[!ht]
    \centering
    \caption{Prequential accuracy reported in other benchmarks}
    \label{tab:otherresults}
    \begin{tabular}{ccccccccccc}
    \toprule
        \multirow{2}{*}{Model} & \multicolumn{2}{c}{LTM} & \multicolumn{2}{c}{SGBT$^\ast$/BOLE$^\dagger$} & \multicolumn{2}{c}{SRP (n=10)} & \multicolumn{2}{c}{SRP (n=100)} & \multicolumn{2}{c}{ARF (n=100)} \\ 
        & ELEC & COVER & ELEC & COVER & ELEC & COVER & ELEC & COVER & ELEC & COVER \\ \midrule
        ARF \cite{gomes2017adaptive} & 87.89 & 91.75 & 90.62$^\ast$ & 94.72$^\ast$ & 88.71 & 94.69 & 89.67 & 94.97 & 88.54 & 92.32 \\ 
        LevBag \cite{bifet2010leveraging} & 85.86 & 85.26 & 89.71$^\dagger$ & 88.13$^\dagger$ & 90.16 & 94.86 & 89.51 & 95.1 & 88.53 & 93.08 \\ 
        SRP \cite{gomes2019streaming} & 88.7 & 94.54 & 89.68$^\ast$ & 95.34$^\ast$ & 88.82 & 95.25 & 89.86 & 95.35 & ~ & ~ \\ 
        OB \cite{chen2012online} & ~ & ~ & 89.51$^\ast$ & 92.69$^\ast$ & 85.25 & 90.33 & 89.52 & 92.7 & 87.05 & 86.34 \\ 
        OAUE \cite{brzezinski2014combining} & ~ & ~ & ~ & ~ & 88.28 & 90.17 & 87.41 & 92.86 & 86.37 & 92.26 \\ 
        DWM \cite{kotler2003dynamic} & ~ & ~ & 88.52$^\dagger$ & 87$^\dagger$ & 87.76 & 88.29 & 87.76 & 88.52 & ~ & ~ \\ 
        SGBT \cite{gunasekara2024gradient} & ~ & ~ & 88.5$^\ast$ & 94.29$^\ast$ & ~ & ~ & ~ & ~ & ~ & ~ \\
        BOLE \cite{de2016boosting} & 90.17 & 93.33 & 90.04$^\dagger$ & 90.16$^\dagger$ & ~ & ~ & ~ & ~ & ~ & ~ \\ \midrule
        LTM & \textbf{91.70} & \textbf{95.51} & \textbf{91.70} & \textbf{95.51} & \textbf{91.70} & \textbf{95.51} & \textbf{91.70} & \textbf{95.51} & \textbf{91.70} & \textbf{95.51} \\\bottomrule
    \end{tabular}
\end{table}

\begin{wraptable}{r}{0.5\textwidth}  % "r" for right, 0.5 is the width
    \vspace{-10pt}  % Optional: Adjust space above the table
    \centering
    \caption{Processing time per instance}
    \label{fig:requirement}
    \begin{tabular}{lccc}
    \toprule
    \textbf{Name}  & \textbf{Classes} & \textbf{Features} & \textbf{Time (ms)} \\ \midrule
    NOAA     & 2              & 8            &   18.4    \\ 
    METER        & 10             & 96              &   81.4   \\ 
    ELEC     & 2              & 8                 &   16.9  \\ 
    RIALTO        & 10             & 27               &   28.7   \\ 
    POSTURE         & 11             & 3               &   20.3  \\ 
    COVER   & 7              & 54                 &  41.1  \\ 
    POKER       & 10             & 11              &     24.2   \\ \bottomrule
    \end{tabular}
\end{wraptable}

While the performance of LTM is well-established, it is essential to evaluate its adherence to key data stream mining requirements \cite{bifet2009adaptive}: (R1) processing one example at a time and inspecting it only once, (R2) using a limited amount of memory, (R3) operating within a limited time budget, and (R4) being ready to make predictions at any point. Regarding (R1), no strict constraint prevents an algorithm from temporarily storing examples internally. As long as these stored instances are eventually discarded to satisfy (R2), an algorithm remains compliant with this requirement. In terms of (R2), although LTMs may require substantial memory storage, their memory footprint remains constant, enabling them to handle data volumes far exceeding the available working memory. However, memory limitations are a physical constraint that must be balanced with adherence to (R3), i.e., ensuring feasible runtime performance. The memory in LTMs can be divided into two categories: the one allocated for running statistics of long- and short-term memory and the memory used to store the foundational model. While data stream mining algorithms typically optimize memory efficiency by maintaining running statistics that directly inform predictions, LTMs inherently separate these two types of memory. The implications of this distinction are further explored in Section~\ref{sec:discussion}. For (R3), LTMs must scale efficiently to any number of examples, which is ensured by its linear runtime complexity concerning the number of processed instances. Furthermore, the computational cost per example is upper-bounded by the number of features, classes and instances within the given context. A key constraint is that for a LTM to function in real-time, it must process instances as fast as they arrive, with failure to do so resulting in data loss. Although LTMs may be slower than Hoeffding tree-based models, several strategies exist to mitigate this issue, depending on the specific LTM architecture. For example, Table \ref{fig:requirement} shows the processing time per sample when using the TabPFN architecture to predict ten instances in parallel within a single forward pass. Alternatively, a hypernetwork-based LTM detaches the inference cost of the LTM from that of the prediction by the generated network, suggesting a different strategy. Further details on LTM architectures are provided in Section~\ref{sec:discussion}. Moreover, traditional strategies can be applied on an application-specific basis, ensuring that excessive delays do not reduce the LTM’s utility for users requiring timely predictions. For instance, instances can be offloaded to other learners, or buffered processing can be used during idle periods, especially in scenarios where absolute timing is less critical, such as when classifying large, persistent data sources. Finally, concerning (R4), while conventional stream learners continuously update their models based on evolving statistics, an LTM translates these statistics into a structured representation, incorporating a selected subset of in-context examples.

\section{Future research directions}
\label{sec:discussion}

In the early stages of machine learning, the dominant belief was that the best way to solve a task was to meticulously design a specific dataset and tailor a model exclusively for that task. Over time, it became evident that for certain types of unstructured data, such as image recognition, a general-purpose model could be fine-tuned on specific datasets, yielding effective results. Today, we stand at a point where highly versatile models, such as general-purpose transformers, can be prompted to tackle a wide range of tasks without task-specific training. With this work, we embrace this evolving paradigm. So far, we have identified LTMs as a promising candidate for streaming solutions, warranting further research. The key challenge is not merely the size of the model or processing speed but rather the dynamic interplay between data arrival, training, recovery, and inference. As technology advances, models will naturally become smaller, faster, and more affordable. In the meantime, efforts should focus on designing algorithms that align more closely with the core principles of data stream mining: processing data in its natural order, handling labels that may be correct at the time of request but outdated or even incorrect by the time they become available, minimizing interference among concepts, and so on. Building on this foundation, several promising research directions emerge within this evolving paradigm. The authors encourage the academic community to explore these opportunities, as they hold significant potential for advancing the field. The authors, too, may pursue some of these directions and welcome feedback and collaboration from those interested in contributing to this line of research.

\subsection{Developing new LTMs}

Since the pre-training stage did not represent the added value of this work, an already existing LTM for batch data was used. However, many benefits could be introduced by scaling LTMs with large-scale drifting tabular datasets. Indeed, it has been shown repeatedly that the construction of modality-specific large-scale datasets allows to significantly increase performance, while using the same model architecture and training procedure from prior work \cite{raffel2020exploring}. For instance, recent work filtered large collections of raw tabular data \cite{eggert2023tablib} with a mix heuristics and rule-based methods applied at a table, column and row level, to extract a high-quality subset for tabular prediction \cite{gardner2024large}. With this heavy modality-specific pre-training, this LLM now outperforms traditional batch tree-based methods across unseen domains, relying on a small number of representative examples for few-shot learning without any fine-tuning. This shows the performance bottleneck of previous similar efforts \cite{hegselmann2023tabllm,dinh2022lift,wang2023unipredict} lied in a lack of large-scale tabular pre-training. In this regard, it is important to note that developing state-of-the-art LTMs is still within the computational reach of many AI researchers, opposed to the economically-exclusionary cost of training modern LLMs.

\subsection{Hypernetwork-based LTMs}

Instead of predicting the target directly, hypernetwork-based LTMs predict a set of concept-specific weights for a target model. This target model then performs classification on tabular data, allowing the LTM to remain independent from the inference stage, which is particularly beneficial in memory- and time-constrained scenarios. Examples of such LTMs include HyperFast \cite{bonet2024hyperfast} and MotherNet \cite{mueller2024mothernet}, both of which generate feed-forward neural networks through a single forward pass. In HyperFast, the weights of the main network are generated layer-by-layer by hypernetwork modules with shared and layer-specific parameters. The first layer applies a random features kernel trick combined with PCA to form a fixed-size, permutation-invariant representation. Layers 2 to \(L-1\) employ MLP-based hypernetworks that process intermediate representations, incorporating residual connections and non-linear activations. For the final layer \(L\), HyperFast avoids directly predicting the weights, instead averaging per-class embeddings to create a lightweight classification weight matrix. In contrast, MotherNet adapts the TabPFN model to generate weights for a compact feed-forward network with a fixed architecture consisting of two hidden layers of 512 units each. MotherNet introduces an attention layer that compresses all activations into a single dataset embedding, based on a learned query vector \cite{mueller2024mothernet}. This query token is optimized during training and subsequently fixed, similarly to prefix tuning \cite{li2021prefix}, but is applied solely within a top-layer attention module rather than across the entire transformer. For future research, one can draw upon the traditional hypernetwork architectural paradigms, which are typically categorized as either task-conditioned or data-conditioned. Task-conditioned schemes involve providing task-specific inputs, such as task identity/embedding, hyperparameters, or other cues \cite{chauhan2024dynamic}, while data-conditioned schemes adapt based on the characteristics of the input data \cite{volk2022example,alaluf2022hyperstyle}. While hypernetwork-based LTMs have primarily focused on data-conditioned schemes, exploring task-conditioned schemes may offer better information sharing across multiple tasks \cite{chauhan2024dynamic}. However, the conditions under which to model a problem as data-conditioned or task-conditioned remain unclear, and require further exploration depending on the problem, the availability of data, and the number of tasks \cite{chauhan2024brief}. Beyond the dynamic nature of input-dependent contexts, future work can investigate alternative output-based strategies. Hypernetwork-based LTMs have primarily focused on generating weights for the entire target model simultaneously \cite{mueller2024mothernet,bonet2024hyperfast}, but generating a large number of weights for the target model can result in a larger last layer in the hypernetwork. One potential solution is to introduce multiple heads for weight generation, thereby reducing the number of weights required in the final layer by a factor corresponding to the number of heads \cite{chauhan2024dynamic}. Furthermore, adopting a chunk-wise or component-wise weight generation approach could be advantageous. In chunk-wise approaches, some weights may remain unused, with additional embeddings used to distinguish and produce different chunks, which reduces complexity and improves the scalability of hypernetworks \cite{chauhan2024dynamic}. Component-wise approaches go further, generating separate weights for each layer or channel in the target model, allowing the network to focus on specific features or patterns \cite{alaluf2022hyperstyle}. It is important to note that, unlike traditional hypernetwork architectures \cite{ha2016hypernetworks}, which use task-specific, multi-task hypernetworks to generate large target networks, hypernetwork-based LTMs utilize large, transformer-style networks to generate compact tabular classification models. Moreover, hypernetwork-based LTMs should generate all weights in a single forward pass, rather than generating weights for specific layers (e.g., the final layer) while training other parts of the feature extractor \cite{gidaris2018dynamic, qiao2018few}.

\subsection{Localization strategy}

Similar to LLMs, compressing the context for LTMs is essential for ensuring alignment with the context length. This process must account for the fact that different data points exhibit varying levels of potential, with some being more representative or informative than others. Rather than naively selecting a random subsample of the training data as context for the prompt \cite{hollmann2022tabpfn, mcelfresh2024neural}, it is more effective to retrieve information that is specifically relevant to the task at hand. Techniques such as compressing the dataset through k-means centroids \cite{feuer2024tunetables} or employing dataset distillation \cite{ma2024context} have been applied. In a streaming setting, however, synopsis techniques, such as histograms, wavelets, sketches are often required to construct geometric and statistical data descriptors as the online phase of clustering-based approaches, capturing the internal structure of the classes \cite{silva2013data}. Subsequently, these methods are followed by a offline mining process performed on the stored micro-cluster synopsis whenever a user sends a request. For example, this two-phase learning approach has been used as a temporal extension for clustering feature vectors \cite{aggarwal2003framework}, which was later extended with projection techniques for subspace clustering in high-dimensional data streams \cite{aggarwal2004projected} and temporal-dependent splitting for long-term operation \cite{zhou2008tracking}. Moreover, this synopsis strategy can be utilized to build a density profile of the data, aiding in the discovery of arbitrarily shaped clusters by extending DBSCAN \cite{ester1996density}. This includes calculating estimates for the epsilon neighborhood around each data point \cite{cao2006density}, or for micro-clusters \cite{ren2011density}. These methods not only reflect the proximity of data points but also capture their interconnectivity \cite{karypis1999chameleon}. Additionally, these techniques can be incorporated into drift detection schemes to actively adapt in non-stationary environments. These concept drifts can monitor the performance of the LTM itself, e.g. focusing on prediction errors, such as error trend \cite{gama2004learning} error distance \cite{baena2006early}, and different rates of the confusion matrix \cite{wang2015concept}, or via posterior probability estimates indicating how well the classes are separated by the classifier, incrementally testing for differences, e.g using KL divergence \cite{lindstrom2013drift}, or a Page-Hinckley test \cite{lughofer2016recognizing}. Furthermore, instead of relying solely on the LTM or a window of recent data to define drift severity, one can explicitly combine drift adaptation with the context optimization strategy. For example, dynamically maintaining a small set of short-term and long-term prototypes based on error-driven representativeness learning and synchronization-inspired constrained clustering \cite{shao2014prototype}. In clustering-driven context optimization, concept drift can be detected by tracking assignment errors \cite{sakamoto2015concept} or comparing recent and reference data within each cluster using the univariate k-sample Anderson-Darling test for each principal component of each centroid \cite{wan2021concept}. Several drift detectors rely on clustering to construct data subspaces for regional density estimation. This includes using neighbor entropy to identify similarities \cite{xu2020self}, or accumulating density discrepancies in each subspace \cite{liu2018accumulating}. These density-based approaches can also be implemented as graphs, such as detecting the origin of drift by updating the k-dist graph based on DBSCAN \cite{miyata2020concept}.

\subsection{Retrieval-based strategy}

Context optimization should not be constrained to a uniform context across all query points. While certain coresets may provide valuable insights for defining the current concept, others could limit the model's adaptability. Instead, a more flexible approach involves storing a larger pool of encoded knowledge that can be dynamically grouped into adaptive local contexts, tailored to each specific query \cite{bonet2024hyperfast}. This retrieval-based strategy facilitates LTMs, which rely on subsampling, to scale more efficiently, even when the number of samples exceeds the maximum context length. Moreover, separating new knowledge from the feature space used to revisit older knowledge reduces interference between old and new subspaces, preserving valuable information that may not contribute immediately but could be useful in future distributions. To construct the most representative context at each iteration, it is essential to define a measure of coreset affinity. One approach involves designing a short-term context-wise query mechanism, where short-term memory with labels is utilized as training data to compute the loss in prediction for each coreset within the larger pool. This key-value pair-based query strategy provides a distribution matching measure for attention, which can then be used to dynamically select the top-$k$ suitable coresets based on their alignment with the current context. Alternatively, one might assume that the most relevant information for classifying a query point is contained within its local vicinity, employing k-NN to identify the nearest domain centers \cite{thomas2024retrieval}. The number of neighbors governs the model's expressivity and the bias-variance trade-off. Naturally, the varying potentials of coresets become particularly pronounced in practical scenarios involving imbalanced or noisy instances. To address this, a second training set can be constructed in which the sample size in each region is inversely proportional to that of the original set. This allows for the retrieval of neighboring samples from both sets for a new query \cite{nejjar2024context}. When pursuing this research direction, it is essential to note that TabPFN \cite{hollmann2022tabpfn} replicates each prompt across multiple permutations of feature shufflings and scalings to ensure invariance with respect to table column order. To address the resulting inefficiencies in the forward pass, TabPFN prompting fits both the test samples and their context simultaneously, thereby accelerating inference. As a result, any retrieval-based strategy built upon TabPFN must align with this batching protocol, wherein multiple test points share the same context.

\subsection{Meta-representations}

Rather than operating directly within the original data space, an LTM can be applied in a projected space, shifting the focus to learning how to represent relationships between features and labels in an embedding space. This approach addresses the heterogeneity within attribute and class spaces across tabular datasets, which not only complicates joint training of an LTM but also hinders its direct application during inference. To achieve this, LTMs can incorporate a meta-representation module, enabling datasets to be transformed into a uniform format of consistent dimensionality while filtering out redundant or noisy attributes. For example, TabPTM standardizes heterogeneous datasets by defining instances, regardless of their original dimensionality, through their membership to a class in terms of the Euclidean or Manhattan distance to the top-$k$ nearest class-specific prototypes in the training set \cite{ye2023training}. This results in a meta-representation of instances as a set of $K$-dimensional vectors, one for each of the classes, which is then used to train the LTM across multiple tabular datasets to extract class-wise confidence scores. With this trained LTM, one can compute the meta-representation of any downstream dataset and perform generalization-free classification in few-shot scenarios.

\subsection{Conditional computation}

Rather than executing the full LTM, specific parts of the network can be activated on a per-instance basis. This strategy significantly enhances model capacity without a proportional increase in computational cost. Building on this idea, MixturePFN improves prompt effectiveness by directing new test samples to a specialized prompter context, which is fine-tuned on a cluster of training data most relevant to the query \cite{xu2024mixture}. The clusters are formed by expanding and subsampling K-Means clusters, tailored to the desired context size. The effectiveness of MixturePFN prompts depends on the number of in-context prompters (ICPs), with a larger number required as the complexity and scale of data grow to capture label entropy. MixturePFN employs a tunable hyperparameter, $\gamma$, to balance efficiency and effectiveness, where the number of ICPs is given by $\lceil\gamma N_{train}/B\rceil$. Intuitively, increasing $\gamma$ enhances effectiveness but at the cost of efficiency.

\subsection{Fine-tuning}

While the previous strategies generally improve performance, unexpected drops may occur due to distribution shifts between the true data-generating mechanism during inference and the pretraining dataset prior \cite{hollmann2022tabpfn, mcelfresh2024neural}. To address this, MixturePFN employs bootstrapping and linear adapter layers for parameter-efficient fine-tuning, minimizing the parameter count for each new expert on the assigned context, i.e., training cluster \cite{xu2024mixture}. Additionally, fine-tuning has been shown to significantly improve retrieval-based training performance \cite{breejen2023fine, thomas2024retrieval}. It is important to note, however, that methods built on top of TabPFN should recognize that fine-tuning it is nontrivial, as it has been trained on a synthetic dataset prior, rather than a pretraining dataset.

\section*{Acknowledgments}

This work was funded by the EU, through the Portuguese Republic’s Recovery and Resilience Plan, within the project PRODUTECH R3. It was also funded by the Portuguese Foundation for Science and Technology under project doi.org/10.54499/UIDP/00760/2020 and Ph.D. scholarship PRT/BD/154713/2023.

\bibliographystyle{unsrtnat}
\bibliography{references}


\end{document}
