\section{StructFlowBench}

In this section, we first introduce the structural flow framework and the constraint categories of our benchmark.
Next, we detail the data construction pipeline and present an overview of the statistics for StructFlowBench.
Finally, we outline the evaluation protocol.

\begin{figure*}[t!]
    \captionsetup{skip=0pt}
	\centering
	\includegraphics[width=\textwidth]{figures/data_construction_pipeline.pdf}
	\caption{The construction pipeline of \textsc{StructFlowBench}. First, tasks, topics, user types, and structural flow templates are defined. Then, dialogue data is generated in two steps: intermediate dialogue plans (i.e., the summarized prompts) are created from the structural flow, followed by generating complete dialogues from these plans. Finally, intra-turn constraints are extracted by GPT-4o, and structural constraints are added based on the structural flow information.}
	\label{fig:construction_pipeline}
\end{figure*}

\subsection{Structural Flow Taxonomy}

By analyzing existing LLM and real human multi-turn dialogue datasets (such as \textit{WILDCHAT}~\cite{zhao2024wildchat} and \textit{LMSYS-Chat-1M dataset}~\cite{zheng2023lmsys}), we identified and categorized six structural patterns of multi-turn dialogues to enhance the understanding and analysis of conversational structural flow.

\textbf{Follow-up:} An adjacent-turn structure where the user's next prompt builds on the content of the previous turn, incorporating details from either the user’s previous prompt or the AI’s previous response.
This is the most common structure in multi-turn dialogues, typically reflecting the user's intent to explore the topic more deeply.

\textbf{Refinement:} An adjacent-turn structure in which the user modifies or clarifies their immediate previous prompt to improve the AI’s response. 
This structure usually signals the user's dissatisfaction with the prior response, prompting them to refine the prompt while clarifying and emphasizing their concerns to obtain a more satisfactory response.

\textbf{Recall:} A long-range structure in which the user refers back to content from two or more turns ago to provide context for the current prompt (long-range follow-up) or referencing prior content for clarification (long-range refinement).

\textbf{Expansion:} A multi-turn ``fan-out'' structure where the user introduces a main theme and explores related subtopics in subsequent turns.
This structure suggests that the user's following turns are focused on specific subtopics derived from a particular point in the conversation.

\textbf{Summary:} A multi-turn ``fan-in'' structure in which the user requests a consolidation of content from multiple previous turns into a cohesive overview.
This structure acts as the counterpart to expansion, reflecting the need to summarize and condense the information discussed in earlier turns.

\textbf{Unrelatedness:} A conversational structure in which the user's prompt is entirely independent of the previous turn, with no reference to prior content or context.
This structure often occurs in everyday use of LLMs by non-experts, where a new topic is introduced within a previously unrelated dialogue, rather than starting a new conversation.

After defining the six basic dialogue structures, we can use the Structural Flow Taxonomy to analyze multi-turn dialogue data and construct the corresponding structural flows.


\subsection{Constraint Categories}

We categorize our constraints into \textbf{intra-turn constraints} and \textbf{multi-turn structural constraints}.
Details related to constraints can be found in Appendix~\ref{sec:constraint}.

For \textbf{intra-turn constraints}, we synthesize and refine constraint classification systems from several works in this field (e.g., IF-Eval~\cite{zhou2023instruction}, CFBench~\cite{zhang2024cfbench}, FollowBench~\cite{jiang-etal-2024-followbench}). Based on this synthesis, we categorize constraints into eight types: \textit{Inverse Constraint, Style Constraint, Situation Constraint, Keyword/Element Constraint, Basic Format Constraint, Quantity Format Constraint, Template Format Constraint, and Content Constraint}.


For \textbf{multi-turn structural constraints}, we define five types of structural design constraints, excluding the ``unrelatedness'' structure. 
These constraints are specifically designed to maintain logical coherence and continuity across multiple turns in a dialogue. 
They ensure that the structural relationships between turns are consistent and contextually relevant, enabling a smooth flow of conversation. 
The five types of constraints are aimed at handling key aspects such as follow-ups, refinements, recalls, expansions, and summaries, ensuring that each turn in the dialogue properly connects to the previous ones while adhering to the intended conversational structure.

\subsection{Data Construction Pipeline}

The construction pipeline of StructFlowBench, as shown in Figure~\ref{fig:construction_pipeline}, comprises three main components: parameter setting, two-step dialogue generation, and constraint extraction and addition.
All prompt templates used in the data construction process are included in Appendix~\ref{sec:prompt}, and a sample data instance is provided in Appendix~\ref{sec:case}.

\subsubsection*{Parameter Setting}

Before dialogue generation, we select parameters such as topic, task, user characteristics, and structural flow template, ensuring comprehensive coverage of the evaluation scope for multi-turn dialogue generation.
For task types, we refer to the taxonomy of ComplexBench~\cite{wen2024benchmarking}, adapting it to our evaluation framework and selecting eight task types.
For topics, we draw from the MT-Bench-101~\cite{bai-etal-2024-mt} framework, making necessary adjustments to suit our context, and ultimately select 22 topics.
For user characteristics, we consider the significant differences in questioning styles and language between experts and non-experts.
For the structural flow template, we designed multiple templates based on insights from real data and specific scenarios. 

\subsubsection*{Two-Step Dialogue Generation}
We employ a two-step process to generate a dialogue for a parameter setting. 
The first step uses the structural flow template to generate an intermediate dialogue plan (i.e., summarized prompts) via GPT-4o. 
Locally deployed mini-models perform initial screening and manual inspection of error data to ensure the dialogue plan aligns with the structural flow. 
In the second step, each intermediate dialogue plan is used to generate a complete dialogue, including user prompts and LLM responses via GPT-4o.
This approach ensures high-quality generation of both dialogue content and structure while minimizing manual effort.


\subsubsection*{Constraint Extraction and Addition}
For the complete multi-turn dialogue data, we extract intra-turn constraints using the GPT-4o, followed by manual validation to ensure accuracy. 
Based on the structural flow information, we then assign the corresponding multi-turn structural constraints to each dialogue turn.


\subsection{Benchmark Dataset Statistics}

Table~\ref{tab:benchmark-comparison} presents a comparison of related benchmark datasets, evaluating them from three perspectives: fine-grained constraints, multi-turn dialogue assessment, and structural information.
Our StructFlowBench encompasses 8 task types, 22 topics, and 13 constraint types. 
It ultimately includes 155 multi-turn dialogues, comprising a total of 643 turns and 1,775 constraints.
Detailed statistics for tasks and topics are provided in the Appendix~\ref{sec:topic-task}.

\begin{table*}[h]
    \begingroup
    \renewcommand{\arraystretch}{0.9}
    \captionsetup{skip=3pt}
    \small
    
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lc>{\centering\arraybackslash}p{1.5cm}>{\centering\arraybackslash}p{2cm}>{\centering\arraybackslash}p{2.2cm}>{\centering\arraybackslash}p{2.2cm}>{\centering\arraybackslash}p{2.2cm}}
            \toprule
            \multirow{2}{*}{\textbf{Benchmark}} & \multirow{2}{*}{\textbf{\#Dialogues}} & \textbf{Avg. \#Turns} & \textbf{\#Constraint Types} & \textbf{Fine-grained Constraint} & \textbf{Multi-turn Assessment} & \textbf{Structural Information} \\
            \midrule
            IFEval    & 541  & 1  & 4  & \cmark & \xmark & \xmark \\
            CELLO      & 523  & 1  & 4  & \cmark & \xmark & \xmark \\
            FollowBench    & 820  & 1  & 6  & \cmark & \xmark & \xmark \\
            InfoBench       & 500  & 1  & 5  & \cmark & \xmark & \xmark \\
            CFBench         & 1000  & 1  & 10  & \cmark & \xmark & \xmark \\
            ComplexBench    & 1150  & 1  & 19  & \cmark & \xmark & \xmark \\
            MT-Bench-101    & 1388  & 3.03  & -  & \xmark & \cmark & \xmark \\
            Multi-if    & 4501  & 3  & 24  & \cmark & \cmark & \xmark \\
            MT-Eval     & 168  & 6.96  & -  & \xmark & \cmark &  $\triangle$  \\
            \midrule
            \textbf{StructFlowBench} & 155  & 4.14  & 13  & \cmark & \cmark & \cmark \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparisons between \textsc{StructFlowbench} and other related benchmark datasets. $\triangle$ represents partially satisfied.}
    \label{tab:benchmark-comparison}
    \endgroup
\end{table*}

\subsection{Evaluation}

\subsubsection*{Evaluation Criteria}
Drawing on the methodology of MT-Bench-101~\cite{bai-etal-2024-mt}, we implemented the ``Golden Context'' approach in our evaluation framework. 
Instead of relying on model-generated contexts, this method uses carefully curated datasets as dialogue histories. 
By providing accurate and consistent contexts for each dialogue turn, it minimizes biases and noise, improving the reliability, fairness, and comparability of response quality assessments across different models.

To achieve a fine-grained evaluation of multi-turn user instructions, we integrate insights from prior studies~\cite{qin2024infobench,wen2024benchmarking,zhang2024cfbench,he2024can} and propose an assessment method based on constraint decomposition and binary question formulation. 
Specifically, we decompose each user instruction into multiple independent constraints and design concise binary questions for each, answered with a simple ``Yes'' or ``No'' to assess satisfaction. 
These binary questions are then aggregated into a checklist that comprehensively covers all critical constraints of the instruction.

Building on this foundation, we further adopt the approach of leveraging state-of-the-art LLMs for evaluation, as outlined in MT-Bench~\cite{zheng2023judging}. 
In our implementation, we use the advanced GPT-4o as the LLM evaluator. 
By providing the evaluator with the golden context, response of the test model, the constraint checklist, and a carefully crafted prompt template, we ensure high consistency and reliability in the evaluation process. 
The prompt template is designed to emphasize key evaluation points, effectively enhancing the accuracy and credibility of the results.

\subsubsection*{Evaluation Metrics}
We adopted several existing metrics, including Constraint Satisfaction Rate (CSR) and Instruction Satisfaction Rate (ISR) ~\cite{zhang2024cfbench}, as well as Decomposed Requirements Following Ratio (DRFR) ~\cite{qin2024infobench}.

The \textbf{Constraint Satisfaction Rate (CSR)} evaluates the average proportion of satisfied constraints across all instructions, calculated as $ \text{CSR} = \frac{1}{m} \sum_{i=1}^{m} \left( \frac{1}{n_i} \sum_{j=1}^{n_i} s_i^j \right) $, where $ m $ represents the total number of instructions, $ n_i $ denotes the number of constraints in the $ i $-th instruction, and $ s_i^j \in \{0, 1\} $ indicates whether the $ j $-th constraint in the $ i $-th instruction is satisfied. 

The \textbf{Instruction Satisfaction Rate (ISR)} measures the proportion of instructions where all constraints are fully satisfied, computed as $ \text{ISR} = \frac{1}{m} \sum_{i=1}^{m} s_i $, where $ s_i \in \{0, 1\} $ indicates whether all constraints in the $ i $-th instruction are satisfied.

The \textbf{Decomposed Requirements Following Ratio (DRFR)} evaluates the overall satisfaction of requirements across all instructions, defined as $ \text{DRFR} = \frac{\sum_{i,j} r'_{i,j}}{\sum_i m_i} $, where $ m_i $ is the number of scoring questions for the $ i $-th instruction, and $ r'_{i,j} $ denotes the result of the $ j $-th scoring question in the $ i $-th instruction.

Despite their utility, these existing metrics have limitations. 
For instance, CSR treats all constraints equally without considering their relative importance, while ISR provides a binary evaluation that may overlook partial fulfillment of constraints. 
To overcome these limitations, we introduce the \textbf{Weighted Constraint Satisfaction Rate (WCSR)}, defined as \textbf{$ \text{WCSR} = \frac{\sum_{j=1}^{n} w_j \cdot s_j}{\sum_{j=1}^{n} w_j}, $} which incorporates weighted factors to account for the varying significance of different constraint types.
Here, $ n $ denotes the total number of constraints, $ w_j $ represents the weight assigned to the $ j $-th constraint, and $ s_j \in \{0, 1\} $ indicates whether the $ j $-th constraint is satisfied. 
In our framework, intra-turn constraints are assigned a weight of $ w_r = 1 $, whereas structural constraints, which play a critical role in ensuring coherence and correctness, are given a higher weight of $ w_s = 2 $.

The introduction of WCSR provides a more nuanced evaluation by emphasizing important constraints through weighted assessments. 
This improves the precision and relevance of evaluations, enhancing the reliability of LLMs in meeting complex requirements.