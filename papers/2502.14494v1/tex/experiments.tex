\begin{table*}[htbp]
    \begingroup
    \renewcommand{\arraystretch}{0.9}
    \captionsetup{skip=3pt}

    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lccccc|cccc}
            \toprule
            \textbf{Model Name} & \textbf{follow-up} & \textbf{refinement} & \textbf{expansion} & \textbf{summary} & \textbf{recall} & \textbf{CSR} & \textbf{ISR} & \textbf{WCSR} & \textbf{DRFR} \\
            \midrule
            Deepseek-v3 & \textbf{\underline{0.99}}& \textbf{\underline{0.8}}& \textbf{\underline{0.92}}& \textbf{\underline{1.0}} & \textbf{\underline{1.0}} & \textbf{\underline{0.98}} & \textbf{\underline{0.93}}& \textbf{\underline{0.96}} & \textbf{\underline{0.98}} \\
            Gemini-1.5-Pro & 0.97& 0.78& 0.91& \textbf{\underline{1.0}} & 0.94& 0.97 & 0.91& 0.95 & 0.97 \\
            GPT-4o & 0.98 & 0.78& 0.88& 0.97& 0.91& 0.97& 0.9 & 0.95& 0.97 \\
            Claude-3.5-Sonnet & 0.98 & \textbf{\underline{0.8}}& 0.88& \textbf{\underline{1.0}} & 0.91& 0.95 & 0.88& 0.94 & 0.96\\
            GLM-4-9B-Chat & 0.95 & 0.75& 0.84& 0.97& 0.94& 0.95 & 0.86 & 0.93 & 0.95 \\
            Qwen2.5-14B-Instruct & 0.97& 0.73& 0.87& 0.97& 0.97& 0.94 & 0.84& 0.92 & 0.94 \\
            Qwen2.5-7B-Instruct & 0.95& 0.76& 0.9& 0.94& 0.97& 0.94 & 0.84& 0.92 & 0.94 \\
            Deepseek-R1-Distill-Qwen-7B & 0.91& 0.62& 0.85& 0.86& 0.78& 0.81 & 0.69& 0.8 & 0.82 \\
            DeepSeek-R1-Distill-Llama-8B & 0.94& 0.73& 0.82& 0.89& 0.84& 0.87& 0.79& 0.86& 0.87\\
            Llama-3.1-Instruct-8B & 0.96& 0.71& 0.84& 0.79& 0.94& 0.85 & 0.68& 0.83 & 0.86 \\
            Phi-3.5-mini-instruct & 0.94& 0.68& 0.87& 0.94& 0.94& 0.88 & 0.73& 0.87 & 0.88 \\
            Yi-6B-Chat & 0.98 & 0.62& 0.87 & 0.84& 0.94& 0.86 & 0.7& 0.84& 0.86 \\
            Mistral-7B-Instruct-v0.3 & 0.97& 0.59& 0.87& 0.71& 0.97& 0.77 & 0.56& 0.76 & 0.78 \\
            \bottomrule
        \end{tabular}
    }
    \caption{\textsc{StructFlowBench} rated by \textbf{GPT-4o}. The left side displays the performance of various models on the five basic structural constraints, while the right side presents their performance on the four key metrics.}
    \label{tab:main-results}
    \endgroup
\end{table*}

\section{Experiments}

\subsection{Experimental Setup}
We evaluate 13 popular LLMs on StructFlowBench, including 3 closed-source models (GPT-4o~\cite{hurst2024gpt}, Claude-3.5-Sonnet~\cite{anthropic2024claude} and Gemini-1.5-Pro~\cite{team2024gemini} ) and 10 open-source models: Llama-3.1-Instruct-8B~\cite{dubey2024llama}, Mistral-7B-Instruct-v0.3~\cite{jiang2023mistral}, Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct~\cite{yang2024qwen2}, Yi-6B-Chat~\cite{young2024yi}, Phi-3.5-mini-instruct~\cite{abdin2024phi}, GLM-4-9B-Chat~\cite{glm2024chatglm}, Deepseek-R1-Distill-Llama-8B, Deepseek-R1-Distill-Qwen-7B~\cite{guo2025deepseek} and DeepSeek-v3~\cite{liu2024deepseek}.
More details on these evaluated models can be found in Appendix~\ref{sec:model-link}.

\subsection{Main Results}
\subsubsection*{Overall Results}

Table~\ref{tab:main-results} presents a comprehensive evaluation of 13 representative LLMs on StructFlowBench, covering four key metrics as well as assessments of structural constraints. 
The detailed results, categorized by intra-turn constraints and task types, are provided in the Appendix~\ref{sec:detailed results}.

The recently released DeepSeek-v3 outperforms all other models across all metrics, demonstrating its exceptional capability in fine-grained constraint satisfaction and multi-turn dialogue structure understanding. 
Gemini-1.5-Pro and GPT-4o closely follow, achieving comparable performance in intra-turn constraints but showing slightly weaker results in adhering to structural constraints for multi-turn dialogues. 
Claude-3.5-Sonnet, GLM-4-9B-Chat, Qwen2.5-14B-Instruct, and Qwen2.5-7B-Instruct also exhibit strong instruction-following capabilities, with CSR exceeding 94\%. 
Notably, all seven of these models achieve high DRFR scores, indicating their strong ability to follow fine-grained instructions.

In contrast, mid-tier models such as Deepseek-R1-Distill-Llama-8B, Llama-3.1-8B-Instruct, Phi-3.5-Mini-Instruct, and Yi-6B-Chat perform reasonably well but exhibit greater instability, particularly in ISR and WCSR. 
While they handle simpler constraints effectively, they struggle with maintaining consistency when processing complex instructions and multi-turn dialogue structures. 
The weakest performers in multi-turn instruction following are Deepseek-R1-Distill-Qwen-7B and Mistral-7B-Instruct-v0.3, revealing significant deficiencies in natural interaction scenarios.

A particularly interesting observation is that Deepseek-R1-Distill-Llama-8B, distilled from Llama-3.1-8B, outperforms Llama-3.1-8B-Instruct across all metrics, demonstrating the effectiveness of the distillation process. 
However, Deepseek-R1-Distill-Qwen-7B, distilled from Qwen2.5-Math-7B, underperforms due to its origin from a model optimized primarily for mathematical reasoning tasks, which inherently makes it weaker in multi-turn dialogue instruction following compared to Qwen2.5-7B-Instruct.

One particularly noteworthy outcome is that DeepSeek-v3, an open-source model, surpasses its closed-source counterparts in multi-turn instruction-following evaluations. 
This result is encouraging for both the research community and the open-source ecosystem, suggesting that the theoretical advancements and training methodologies behind DeepSeek-v3 could offer valuable insights for improving LLMs in multi-turn instruction-following tasks.
 

\subsubsection*{Structural-Constraint-Categorized Performance}

The evaluated LLMs exhibit strong performance in follow-up structures, with nearly all models excelling in maintaining contextual continuity and generating coherent responses.
Additionally, most models handle recall structures well, demonstrating their ability to reference prior conversational turns effectively.
However, performance varies when dealing with more complex structures such as summary and expansion.
DeepSeek-v3 and proprietary models outperform the others, indicating their superior capability in nuanced content condensation and elaboration.
In contrast, refinement tasks pose a significant challenge across all models.  
Even the strongest model, DeepSeek-v3, achieves only 0.8 in this category, highlighting the inherent difficulty of processing refinements accurately and maintaining coherence when adapting to modified user inputs.
While LLMs exhibit strong instruction-following abilities in structured dialogue, refinement remains the most challenging task, requiring improvements in dynamic response adaptation. 
Future advancements should focus on enhancing models' flexibility in refining responses based on iterative user feedback, ensuring more robust handling of complex multi-turn interactions.

\subsubsection*{Intra-Turn-Constraint-Categorized Performance}

The evaluation of LLMs across various constraint dimensions highlights their strengths and weaknesses in following specific instructions. 
DeepSeek-v3, Gemini-1.5-Pro, and GPT-4o achieve near-perfect satisfaction rates, demonstrating strong capabilities in fine-grained instruction following.
Most other models also perform well in rule-based constraints, such as Inverse Constraint, Keyword/Element Constraint, Style Constraint, and Situation Constraint.
However, performance drops noticeably in format-related constraints, including Basic Format Constraint, Template Format Constraint, and Quantity Format Constraint, indicating that rigid format adherence remains a significant challenge, even for top-performing models.
Overall, while LLMs effectively handle intra-turn constraints, their ability to maintain format consistency remains a key limitation. 
Addressing this challenge requires further advancements in structured output generation and adherence to strict formatting requirements.

\subsubsection*{Task-Categorized Performance}

We evaluated various models across seven NLP tasks and a mixed task.
Unlike the constraint-categorized evaluation, where DeepSeek-v3 led across all metrics, the task-based analysis presents a more nuanced picture. 
DeepSeek-v3 remains the overall best-performing model but leads only in Fact-based Questions, Professional Writing, Practical Writing, and Casual Chat.
Gemini-1.5-Pro outperforms others in Open-ended Questions and Creative Writing, while Claude-3.5-Sonnet achieves the highest performance in Fact-based Questions and Task-oriented Role-playing. 
Meanwhile, GPT-4o excels in the Mixture task type, reflecting its strength in handling diverse instructions across domains. 
These results highlight the varying strengths of these top-tier models across different tasks.
Following the top four models, GLM-4-9B-Chat, Qwen2.5-14B-Instruct, and Qwen2.5-7B-Instruct maintain consistently strong performance across all tasks. 
Their stability, combined with their significantly smaller parameter sizes compared to the leading models, makes them highly cost-effective alternatives.
In contrast, the remaining models all exhibit noticeable weaknesses in at least one task category, with Mistral-7B-Instruct-v0.3 underperforming across nearly all tasks, revealing a clear performance gap. 

\begin{figure*}[htbp]
    \captionsetup{skip=0pt}
	\centering
	\includegraphics[width=\textwidth]{figures/heatmap_separate_cbar_structflow.pdf}
	\caption{The comprehensive complex scenario evaluation heatmap of five multi-turn dialogue datasets.}
	\label{fig:heatmap}
\end{figure*}

\subsection{Further Analysis}

\subsubsection{Complex Scenario Suitability Study}
This study aims to verify whether the multi-turn dialogue dataset we have constructed more closely aligns with real-world complex use cases.
To achieve this, we designed an experiment to analyze three key properties of dialogue: logical coherence, goal clarity, and transition naturalness. 
The datasets used in this experiment include our StructFlowBench, three other multi-turn dialogue evaluation datasets (MT-Bench-101, Multi-if, and MT-Eval), and a real-world dialogue dataset, WILDCHAT.

\textbf{Data Preparation:} For each dataset, we randomly selected 50 English multi-turn dialogue samples, ensuring a diverse representation of dialogue types.

\textbf{Evaluation Protocol:} To quantify how well the dialogues meet complex scenario requirements, we employed GPT-4o for automated scoring. 
Each dialogue was evaluated based on its performance in the following areas:
\vspace{-.1in}
\begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \item \textbf{Logical Coherence:} Evaluates whether the dialogue is logically consistent and free of abrupt or unreasonable shifts.
    \item \textbf{Goal Clarity:} Assesses whether the dialogue clearly communicates the task's goals and ensures both the user’s and system’s intentions are transparent.
    \item \textbf{Transition Naturalness:} Judges whether transitions between dialogue turns are smooth and natural, without awkward or forced shifts.
\end{itemize}
\vspace{-.1in}
Each property was scored on a scale from 1 to 5, where 1 indicates complete failure to meet the expected standard, and 5 represents perfect alignment with complex scenario requirements.

\textbf{Confusion Factor (CF):}
To further evaluate the datasets, we introduced the Confusion Factor (CF), which quantifies the proportion of dialogues in each dataset that scored 4 or higher, indicating they were mistakenly perceived as real-world interactions. 
The CF is calculated as follows:
$$
\text{CF} = \frac{\text{Number of dialogues with average score} \geq 4}{\text{Total number of dialogues}},
$$
By comparing the CF values of our StructFlowBench dataset with those of others, we can assess whether our dataset outperforms the others in terms of alignment with complex scenarios.

\textbf{Results and Discussion:}
The results are presented as a heatmap, as shown in Figure~\ref{fig:heatmap}.
StructFlowBench achieves the highest scores across all three evaluation dimensions, leading with a confusion factor of 0.83. 
MT-Bench-101, with its comprehensive dialogue generation process and rigorous human proofreading, also produces high-quality dialogues and ranks closely behind with strong scores.
In contrast, the WILDCHAT real multi-turn dialogue dataset, containing one million dialogues, exhibits generally low quality. 
Although we performed preliminary filtering on the WILDCHAT data, such as considering prompt length and dialogue content, the extracted dialogues still failed to meet the ideal quality standards
As a result, WILDCHAT performed the worst across the three evaluation dimensions for data-driven simulated scenarios.

\subsection{Human Verification}

We extracted 30 dialogues from the output of Qwen2-7B-Instruct and invited domain experts to conduct a comprehensive and detailed evaluation of the results.
The experts rated the outputs using a binary scoring system. 
The results showed that the Kappa coefficient between GPT-4o's evaluations and those of the experts was approximately 0.75. 
This indicates that utilizing advanced LLMs, like GPT-4o, to assess the quality of outputs from other models is a reliable approach, effectively reducing both subjective bias and the time costs associated with relying solely on human evaluation.