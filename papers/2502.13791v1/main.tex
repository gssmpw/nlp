\pdfoutput=1

\documentclass[11pt]{article}
\usepackage{tabularx}
\usepackage{arydshln} 
\usepackage[dvipsnames]{xcolor}

\usepackage[preprint]{acl}
\usepackage{xltabular}

\usepackage{times}
\usepackage{latexsym}
\usepackage{amsfonts}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}


\usepackage{lipsum} 
\usepackage{comment}

\usepackage{longtable}
\usepackage{makecell}
\usepackage{array}
\usepackage{booktabs, colortbl}
\usepackage{marginnote} 

\usepackage[normalem]{ulem} 
\setlength{\dashlinedash}{2.5pt}

\newcommand{\isep}{\mathrel{{.}\,{.}}\nobreak}

\title{From Tools to Teammates: Evaluating LLMs\\in Multi-Session Coding Interactions}



\author{
 \textbf{Nathanaël Carraz Rakotonirina\textsuperscript{1}},
 \textbf{Mohammed Hamdy\textsuperscript{5}},
 \textbf{Jon Ander Campos\textsuperscript{3}},
 \textbf{Lucas Weber\textsuperscript{1}},
\\
 \textbf{Alberto Testoni\textsuperscript{2}},
 \textbf{Marzieh Fadaee\textsuperscript{4}},
 \textbf{Sandro Pezzelle\textsuperscript{2}},
 \textbf{Marco Del Tredici}
\\
\\
 \textsuperscript{1}Universitat Pompeu Fabra,
  \textsuperscript{2}University of Amsterdam,
 \textsuperscript{3}Cohere, \\
 \textsuperscript{4}Cohere For AI,
 \textsuperscript{5}Cohere For AI Community
}

\begin{document}
\maketitle
\begin{abstract}

\let\thefootnote\relax\footnotetext{Correspondence:  \href{mailto:nathanael.rakotonirina@upf.edu}
{nathanael.rakotonirina@upf.edu} \\
Marco Del Tredici started this project while working at Cohere. Lucas Weber is now at Fraunhofer IIS and Alberto Testoni at Amsterdam UMC.
}

Large Language Models (LLMs) are increasingly used in  working environments for a wide range of tasks, excelling at solving individual problems in isolation. However, are they also able to effectively collaborate over long-term interactions? To investigate this, we introduce \textbf{\textsc{MemoryCode}}, a synthetic multi-session dataset designed to test LLMs' ability to track and execute simple coding instructions amid irrelevant information, simulating a realistic setting. 
While all the models we tested handle isolated instructions well, even the performance of state-of-the-art models like GPT-4o deteriorates when instructions are spread across sessions. Our analysis suggests this is due to their failure to retrieve and integrate information over long instruction chains. Our results highlight a fundamental limitation of current LLMs, restricting their ability to collaborate effectively in long interactions\footnote{The code and data are available at \url{https://github.com/for-ai/MemoryCode}.}.

\end{abstract}

\input{sections/introduction}

\input{sections/related}

\input{sections/dataset_creation}

\input{sections/experiments}

\input{sections/analysis}

\input{sections/discussion}

\input{sections/limitations}

\section*{Acknowledgments}
UPF was funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 101019291). During his affiliation with UvA, Alberto Testoni was funded by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 819455, PI R. Fernández). This paper reflects the authors’ view only, and the funding agency is not responsible for any use that may be made of the information it contains.


\bibliography{refs}

\newpage
\appendix
\input{sections/appendix}

\end{document}
