\section{Conclusions}
\label{sec:discussion}

In this paper, we proposed \textbf{\textsc{MemoryCode}}, 
a new benchmark to assess state-of-the-art LLMs in their ability to retrieve and reason over pieces of information in multi-session dialogue histories reflecting real-world scenarios.
Differently from many existing datasets, the tasks in \textsc{MemoryCode} do not require any complex reasoning, and are easily solved by the models when provided in isolation. 
The main challenge of \textsc{MemoryCode} lies in the ability to keep track of multiple simple instructions received throughout a multi-session interaction, and to jointly apply them to perform a task.
When the number of sessions is small (<15), SotA models like GPT-4o manage to perform the task well. However, as the number grows up to 100, even these models face a dramatic drop in performance. 
Our analysis shows that this is mainly due to their failure to reason over a long chain of simple instructions. 

Overall, our results show a severe limitation of current LLMs. 
The inability to keep track of simple information as the interaction with a human unfolds effectively hinders their adoption in real-world scenarios and restricts their usage to addressing 
single, self-contained problems. 
We argue that effective long-term collaboration cannot be achieved by further scaling model and input context window sizes.
Rather, we believe that new mechanisms to handle and retrieve from long-term memory need to be developed. 
\textsc{MemoryCode} contributes to this challenging and yet crucial goal, by providing a robust benchmark for developing and testing such methods. 





