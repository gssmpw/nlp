\section{Related work}
\label{sec:related}

\subsection{Long-Context Evaluation} 
Early approaches to evaluating long-context understanding date back to the pre-LLM era.
One such example is 
LAMBADA \citep{paperno-etal-2016-lambada}, which includes high-quality human-annotated samples with an average length of 75 tokens. 
As context lengths increased, new datasets were created by repurposing or expanding existing NLP datasets \citep{an-etal-2024-l, bai-etal-2024-longbench, dong-etal-2024-bamboo}. 
More recently, controlled-length synthetic evaluation frameworks, such as Needle-in-a-Haystack \cite{kamradt2023needle} and LTM \cite{LMT}, have been widely adopted for evaluating long-context understanding \citep{anil2023gemini, anthropic2024claude3}. 
In these frameworks, the %
models are
tasked with retrieving information from long distractor texts.
RULER \citep{hsieh2024ruler} extends Needle-in-the-Haystack by varying the types and numbers of \textit{needles} and adding new tasks like variable tracking and frequent word extraction.
LOFT \citep{lee2024can} adds many real-world tasks, such as Retrieval-Augmented Generation and SQL-like tasks, that require context up to millions of tokens. 
Similar to these approaches, in this work we evaluate long-context understanding in conversational settings. Unlike
other works, though,
we do not ask the models to retrieve a piece of information, but rather challenge them to retrieve 
the most up-to-date instructions dispersed across the dialogue history to accomplish a task.\\


\subsection{Long-term Dialogue Evaluation}
Multi-turn and multi-session interactions are the \textit{de facto} standard setup in which LLMs are used. Accordingly, several datasets have been introduced to evaluate long contexts in conversations.
\citealp{zheng2024judging} introduced MT-Bench, a high-quality, multi-turn question dataset across 8 knowledge categories, but with only two turns per session. 
Many benchmarks have been proposed to expand or improve upon MT-Bench \citep{sun2024parrot, bai-etal-2024-mt, kwan2024mt}. 
For example, MT-Eval \citep{kwan2024mt} evaluates different aspects of multi-turn dialogue such as the ability to understand follow-up questions.
MINT \citep{wang2024mint} focuses on tool use and natural language feedback evaluation, while \citealp{duan-etal-2024-botchat} introduce a framework where three different evaluation strategies are proposed: evaluating each multi-turn dialogue separately, comparing the quality of two generated dialogues, and comparing two dialogues to determine which one is 
the human
conversation. 


Most similar to our work, MMMT-IF \citep{epstein2024mmmt} extends multi-turn and multi-modal datasets to measure instruction-following abilities and shows that the main challenge for LLMs is not in following instructions, but rather in retrieving instructions.
In multi-session dialogues, many datasets were also created synthetically; for example, Conversation Chronicles~\citep{jang2023conversation}, which includes 200K conversations of about 5 sessions each, or LoCoMo~\citep{maharana-etal-2024-evaluating}, a multi-modal dataset based on a framework that leverages personas and temporal event graphs.
\citet{kim2024share} and \citet{kim2024dialsim} utilized movie scripts to construct complex multi-session dialogue datasets having, for example, multi-party conversations and shared memories between speakers. 
Most of the works mentioned above focus on expanding the number of turns and sessions or on introducing more complex tasks that are challenging for LLMs. Similarly, our work evaluates the performance of LLMs in multi-turn/session dialogues, but with very simple tasks and more focus on practical, real-world settings in which information is constantly changing.


\subsection{Synthetic Dialogue Generation}
Synthetic data generation via LLMs addresses limitations of human-based dataset construction such as high costs~\citep{doi:10.1073/pnas.2305016120} and privacy concerns~\citep{kurakin2023harnessing}. Precisely because of these advantages, we decided to adopt synthetic generation for the creation of \textsc{MemoryCode}. Examples of widely adopted synthetic datasets include SODA \cite{kim-etal-2023-soda}, an open-domain dialogue dataset grounded on commonsense knowledge, containing millions of utterances generated by GPT3.5;
DialHalu~\citep{chen2024diahalu}, a dataset to evaluate different subtypes of hallucination in language models; and 
MoralDial \citep{sun-etal-2023-moraldial}, which evaluates moral values in language models. 
\citet{wu2024hiding} proposed a dialogue generation framework that provides control over many attributes of the speakers, such as personality, age group, and profession. Finally, \citet{rakotonirina-baroni-2024-memoryprompt} introduced a synthetic dataset consisting of sequences of realistic facts that may be updated over time. Their dataset is designed to evaluate LLMs' ability to track specific pieces of information amid distractors. 
While similar in spirit to these approaches, our dataset is novel as it is composed of interactions set in practical business contexts and with a focus on coding.
Additionally, our evaluation emphasizes the model's ability to follow well-defined instructions rather than simply retrieving facts.





