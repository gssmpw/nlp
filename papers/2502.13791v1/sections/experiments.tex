\section{Results}
\label{sec:experiments}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figs/average_scores.png}
    \caption{Average \textsc{Instruction}, \textsc{Session}, and \textsc{History} scores per model. For the latter, `short' includes dialogue histories with less than 15 sessions, `long'  those with 16 to 100 sessions.}
    \label{fig:average_score}
\end{figure}



In this section, we report the performance of the models across the evaluation setups described in Section~\ref{subsec:experiments}. Figure~\ref{fig:average_score} shows the average score for each model. The exact numbers are included in Table \ref{tab:detailed_scores} of Appendix \ref{sec:detailed_scores}. 

\paragraph{\textsc{Instruction}} As shown in Figure~\ref{fig:average_score}, all models achieve high or very high accuracy on this setup. 
This aligns with our goals of having relatively easy instructions.
In particular, all the large models approach or exceed 0.9 accuracy---reported in the $[0,1]$ scale. 
While the results of Llama-8B are lower, they still show that even a small, non-specialized model achieves good performance, confirming the easiness of the task.

As a sanity check, we run a set of experiments in which we do not prompt the models with the necessary instructions (e.g., \textit{use CamelCase}), but directly run the evaluation (in this case, we check if CamelCase was used).\footnote{
Due to budget limits, we only used Command R+ for this experiment and for the analysis in Section \ref{subsec:retrieval_or_reasoning_problem}.
We expect the results to be representative of all other models' behavior.} This setup verifies that models do not solve \textsc{MemoryCode} through their default behavior. Models fail spectacularly, achieving an extremely low average accuracy (consistently lower than 0.01), confirming that the instructions we provide are crucial to executing the tasks correctly. 

\paragraph{\textsc{Session}} The performance in this setup is very similar to \textsc{Instruction} for the larger Llama models and GPT-4o, indicating that these models have no difficulties at retrieving the relevant information in a single session. Command R+ shows a larger drop of 0.22 (25\% relative drop compared to \textsc{Instruction}), while Llama-3.1-8B shows a major drop of 0.34 (48\%), which indicates its inability to retrieve relevant information across multiple turns. 


\paragraph{\textsc{History}} Things change dramatically in this setup, with a degradation in performance across the board already for `short' dialogue histories. In particular, GPT-4o shows a drop of 0.14,
a value that increases for the other models: 0.47 (67\% relative drop compared to \textsc{Instruction}) for Llama-3.1-8B and 0.48 (54\%) for Command R+. 
These results indicate that as the number of sessions increases, even the best-in-class models have difficulties in identifying and applying the relevant instructions. 

A more dramatic drop is observed in the `long' setup. Here, all the models struggle, with the best GPT-4o only achieving 0.30 accuracy,
which indicates a relative drops of 61\% from 
the `short' setup and of 
67\% 
from \textsc{Instruction}. 
The drop is even more significant for the other models: the performance of Llama-3.1-405B drops by 78\% compared to \textsc{Instruction}, Command R+ by 87\%.
Crucially, this happens even though the tasks on which models are evaluated are identical to those in the \textsc{Instruction} and \textsc{Session} setups, where the models achieved nearly-perfect accuracy. 
The difference in performance, hence, is to be ascribed to models' inability to retrieve and reason over 
relevant pieces of information present in their input. 


