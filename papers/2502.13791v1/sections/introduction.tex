\section{Introduction}
\label{sec:intro}


Current efforts to improve the performance of large language models (LLMs) mostly focus on their ability to solve increasingly harder tasks autonomously. Examples of this research include solving complex math~\cite{wangmathcoder,gao2024omni,alphageo2021}, coding~\cite{chen2021evaluating,austin2021program,tao2024crystal,puerto-etal-2024-code}, or reasoning problems~\cite{hao2023reasoning,wang2024chain,renze2024self}.
Since many of these tasks are relevant to real-world applications, LLMs are widely adopted in industry, where they have been reported to significantly enhance productivity~\cite{weber2024significant,cambon2023early}.
This extensive adoption of LLM assistants into the daily working routine is effectively converting them from mere tools to fully-fledged \textit{teammates}. 
For LLMs to behave as such, though, complementary skills related to collaboration and interaction are needed. 
One such ability is retaining relevant information from multiple interactions with human users
and leveraging it for future tasks.


\begin{figure}[t]
    \centering
    
    \includegraphics[width=1\columnwidth]{figs/intro_llm_colleagues_img.png}
    \caption{A simplified but realistic example of a long-term interaction between a human and an LLM-based `teammate'. 
    In this example, each day represents a single session. The LLM teammate must remember a piece of information---in red---learned during the session on Day 1 to correctly perform a task on Day 20, while also receiving irrelevant information---in blue---on Day 5. 
    }
    \label{fig:memoryCode_fig1_sketch}
\end{figure}

In this paper, we investigate this challenge by introducing \textbf{\textsc{MemoryCode}}, 
a synthetic dataset 
of multi-session dialogue histories 
designed to evaluate models' ability to 
track simple coding instructions provided amid irrelevant information, and execute them in future coding tasks.
Each dialogue history is a chronological sequence of dialogues, or \textit{sessions}, between a \textit{mentor} and a \textit{mentee}.
Throughout the sessions, the mentor passes critical information for solving a task to a mentee. Crucially, this information is interspersed with a substantial amount of unrelated content, thus reflecting the real-life scenario of working in an office. 
Furthermore, the information needed to perform a task can be updated multiple times throughout the dialogue history.


\textsc{MemoryCode} mimics natural interactions between coworkers. Figure \ref{fig:memoryCode_fig1_sketch} shows an example of such interactions, where various coding conventions and rules arise~\cite{convertino2008articulating,chumg2022drives} that are passed on to new team members (Day 1), often among other pieces of information irrelevant to coding tasks (Day 5). Newcomers are expected to comply with such rules when performing future tasks (Day 20), unless rules are deprecated or changed.
\textsc{MemoryCode} tests whether current models behave like new human teammates by consistently adhering to such rules across many sessions.


Similar to previous work~\cite{nelson2024needle,epstein2024mmmt,maharana-etal-2024-evaluating}, the primary goal of our benchmark is to retrieve important information from a long conversational history.  
In contrast to previous datasets, \textsc{MemoryCode} requires to \emph{use} retrieved information in practical tasks while not being explicitly cued to do so.
This is more challenging than cued retrieval of static information, as it requires prospective 
memory and spontaneous retrieval~\cite{mcdaniel2007prospective,brandimonte2014prospective}. 
Additionally, \textsc{MemoryCode} requires an integration of information retrieved from different parts of the dialogue history, as rules can be updated, with only the last update being eventually relevant.
At the same time, the rules in it (e.g., adding a date at the start of every new code) are simple to execute, which allows for disentangling a model's retrieval capabilities from other complex skills.
To the best of our knowledge, \textsc{MemoryCode} is the first multi-session dataset that tests this practically highly relevant skill.

We test several proprietary and open-source
SotA models on \textsc{MemoryCode}, and show that:
(i) Even small models succeed in executing the single coding instructions in \textsc{MemoryCode} when prompted without additional complex context, indicating that such instructions are well within the reach of current LLMs;
(ii) As we increase the complexity and provide a full mentor-mentee session, including several instructions and irrelevant information, 
only larger models continue to perform well, while the performance of smaller models drops significantly;
(iii)When we provide the full dialogue history, even strong proprietary models struggle to follow our simple instructions, with GPT-4o showing a dramatic 67\% drop in accuracy compared to its performance with instructions alone. This reveals that \textsc{MemoryCode} is a challenging benchmark even for the best available models, that struggle to retrieve and incrementally update relevant information.




We argue that solving \textsc{MemoryCode} requires more than simply scaling models even further. 
Instead, our results indicate a pressing need to develop dedicated mechanisms to enhance LLMs' abilities, such as improved long-term memory retention strategies, prospective memory, or additional reasoning mechanisms. We release the dataset under the Apache 2.0 license. 









