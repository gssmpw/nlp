\section{Analysis}
\label{sec:analysis}

In this section, we conduct an analysis aimed at understanding which factors influence model performance on the \textsc{MemoryCode} benchmark. We focus on the \textsc{History} setup, the most important and challenging one. 

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=\linewidth]{figs/per_session_score_all_models.png}
        \caption{Score per number of sessions.}
    \label{fig:per_session_score_all_models}
\end{figure}

\subsection{Effect of Number of Sessions} 
\label{subsec:effect_number_sessions}

In Figure \ref{fig:per_session_score_all_models}, we show how the performance decreases with an increasing number of sessions. 
Consistently with the aggregated patterns shown in Figure~\ref{fig:average_score}, 
relevant variations can be observed across models when the number of sessions is rather low, which reflects the differences observed in the `short' setup. 
However, all models converge to a similarly, extremely low accuracy (around 0.1) when the number of sessions approaches 100. This confirms that all models are similarly bad at handling requests involving long-context inputs. 
 
This weakness could be due to limitations in retrieving the relevant information from the dialogue history, reasoning about a chain of instructions and updates, or both.
Below, we shed light on this issue. 

\subsection{Retrieval or Reasoning Problem?}
\label{subsec:retrieval_or_reasoning_problem}

If the poor performance in the \textsc{History} setup was due to retrieval, then passing models only the full chain of instructions and updates---without any intervening irrelevant text---should solve the issue. 
\textit{Vice versa}, if the issue was about reasoning over such a chain, 
they should still perform poorly.
We test these assumptions by feeding Command R+ with only the entire chain of instructions needed to solve a task. We name this setup \textsc{Instructions-chain}. 
As shown in Figure~\ref{fig:per_session_command_r_plus}, the trend is strikingly similar to the one observed in \textsc{History}, with the model still struggling even if only the relevant information is provided, with no dialogue history.
This indicates that models' drop in performance is mainly due to their inability to reason compositionally over a sequence of instructions. 

Retrieval from the dialogue history also plays a role, as indicated by the slightly higher performance in \textsc{Instructions-chain} over \textsc{History}. To mitigate this retrieval issue, we experimented with Retrieval Augmented Generation (RAG), where instead of providing as input the whole history, we retrieve its relevant parts only and feed them to the model. However, we did not observe any improvement over \textsc{History} (see Appendix \ref{app:rag_experiments}).



\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{figs/cum_no_rag_pivot_only.png}
    \caption{Per-sessions score for \textsc{Instr.-chain}.}
    \label{fig:per_session_command_r_plus}
\end{figure}



\subsection{Effect of Instruction Updates}

After uncovering models' reasoning limitations, we now
explore whether they are due to the inability to cope with instruction updates. 
We hypothesize that the higher the number of updates an instruction goes through, 
the harder it is for the model to keep track of such updates 
and eventually adhere to the updated instruction  
when performing a task.

We define the \textbf{update rank} of an instruction as the number of times the instruction is updated throughout the dialogue history, for both `short' and `long' setups. An update rank of 0 means that the instruction was never updated. 
Figure \ref{fig:update_rank} reports for each update rank the average score, computed across all models, of the instructions with that rank. We observe that the score of an instruction decreases as its number of updates increases. This result is consistent with our hypothesis that models struggle to incrementally update their instruction representations through multiple reasoning steps.  


\subsection{Instruction Difficulty}
We finally assess if, besides the updates,
the very nature of each instruction makes it more challenging than others.
For this, we compute the average per-instruction score of the best-performing model, GPT-4o, for both the initial instruction and their updates.  
The results reveal notable variations across instructions: 
In particular, instructions and updates that are less common in practice---like including a digit in object names---tend to obtain lower scores compared to more common ones such as using docstrings or annotations. 
Figure~\ref{fig:per_pivot_score_insertion} and~\ref{fig:per_pivot_score_update} in Appendix~\ref{app:per_instruction_scores} report the results of this analysis. %
Overall, this suggests that the models' ability to retrieve and update relevant instructions is modulated by their characteristics and that familiarity is likely to play a role. 
An example (from GPT-4o) is shown in Table~\ref{tab:qual_example_2}. 
In this case, the model correctly follows the instruction on comments, but not those about attribute and variable names.









\arrayrulecolor{gray!50}
\begin{table}[t!]
\centering
\begin{tabularx}{1\columnwidth}{X}
\toprule
\textbf{DIALOGUE HISTORY} \\
\midrule
(1) \textbf{Yoon-Seo:} Alexander, it's great to finally meet you. I'm Yoon-Seo, your mentor  \dots \\
\hdashline
(2) \textbf{Yoon-Seo:} \dots One of our key practices is to \colorbox{Apricot}{always add comments to your code}\dots \\
\textbf{Yoon-Seo:} \dots Another convention we follow is to \colorbox{CornflowerBlue}{end variable names with `\_n'} \dots \\
\hdashline
(3) \textbf{Yoon-Seo:} \dots \colorbox{CornflowerBlue}{All variable names should} \colorbox{CornflowerBlue}{now end with `\_z'} \dots \\
\hdashline
(4) \textbf{Yoon-Seo:} \dots When \colorbox{YellowGreen}{naming attributes}, [...] \colorbox{YellowGreen}{include the string `chx'} in the name \dots \\
\midrule
\textbf{EVALUATION QUERY} \\
\midrule
Write a function that checks if a string is a valid palindrome \\
\midrule
\textbf{MODEL OUTPUT} \\
\midrule
\texttt{def is\_palindrome\_\colorbox{YellowGreen}{chx}(s\_\colorbox{YellowGreen}{chx}):} \\    
\texttt{    \colorbox{Apricot}{\# Initialize two pointers}} \\   
\texttt{    left\_\colorbox{YellowGreen}{chx}\colorbox{CornflowerBlue}{\_n} = 0} \\    
\texttt{    right\_\colorbox{YellowGreen}{chx}\colorbox{CornflowerBlue}{\_n} = len(s\_\colorbox{YellowGreen}{chx}) - 1} \\
\texttt{    \dots} \\
\bottomrule
\end{tabularx}
\caption{A 4-session dialogue history (session id in parenthesis). 
GPT-4o correctly applies the instruction about \colorbox{Apricot}{comments} but not the one about \colorbox{YellowGreen}{attribute names} and the update on \colorbox{CornflowerBlue}{variable names}.}
\label{tab:qual_example_2}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/update_rank_all_models.png}
    \caption{Score as a function of update rank.}
    \label{fig:update_rank}
\end{figure}


