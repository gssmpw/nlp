\section{Dataset}
\label{sec:data}
We simulate a scenario in which the model assumes the role of a new hire (henceforth, the \textit{mentee}) who undergoes an onboarding process in a given company. 
The mentee interacts with a \textit{mentor} in chronologically ordered \textit{sessions}. A session is a multi-turn dialogue in which the mentor passes the mentee various information.
In a session, the mentor can give instructions about relevant coding practices in Python that the mentee should follow when performing a task. For example, in Figure \ref{fig:memoryCode_fig1_sketch}, the instruction is the text in red on Day 1.
Once introduced, an instruction can be updated over time: in the case of Figure \ref{fig:memoryCode_fig1_sketch}, an update might be to  \textit{not} add the date anymore. 
When the mentee is asked to perform a task, it should remember and follow all the relevant instructions. 
Sessions can also include topics irrelevant to the target tasks: We refer to these topics as \textit{fillers} (in Figure \ref{fig:memoryCode_fig1_sketch}, the information in blue provided by the mentor).
Finally, a dialogue \textit{history} is the concatenation of all the sessions between the mentee and the mentor.

The dataset evaluates the models' ability to leverage the relevant instructions received throughout the history to perform the assigned tasks. 
To create dialogue histories we relied on both manual and automatic labor, thus optimizing quality and minimizing costs and effort, as described below. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/dataset_creation.png}
    \caption{Dataset generation process. First, we randomly sample from our seeds to fill the variables of the template. The LLM is then prompted with this template to generate the dialogue history.}
    \label{fig:dataset_creation_process}
\end{figure*}

\subsection{Seeds}
\label{subsec:seeds}

A dialogue history is created based on a few crucial elements, or \textit{seeds}. We use four types of seeds:  
\textbf{instructions}, 
\textbf{fillers}, mentor and mentee \textbf{personas}, and \textbf{names}. 
For each seed, we define a set of possible values, from which we sample to generate histories (see Figure~\ref{fig:dataset_creation_process}). %
The possible seed values were manually defined by the authors to ensure high quality. Below, we describe each seed.

\paragraph{Instructions}
These are the coding instructions that the mentee must follow when generating a piece of code. 
An example is: \textit{always start function names with `g\_'.} Instructions are designed to be as simple as possible---recall that
we are interested in assessing the models' ability to leverage the information that emerged during interactions, not 
their ability to perform complex tasks. 

Each instruction applies to a specific Python object (e.g., \textit{function}).
Also, for some instructions, we define \textit{updates}:
For example, the instruction above would be updated to \textit{always start function names with `h\_'}.
Then, for each instruction,
we create an evaluation query and a test function.\footnote{Note that the evaluation query is the same for an instruction and for its updates.} Queries are specifically designed to trigger code generation that is relevant to the instruction (e.g., \textit{write a function that merges two sorted lists}).
Test functions are regular expressions: they only assess if the relevant instructions were followed (e.g., if the function starts with the required letter), and not the overall quality of the generated code.
We manually crafted 51 instructions, 16 of which can be updated up to 8 times, while the remaining ones do not have updates. We report the full list of instructions in Table \ref{tab:instructions_list} of Appendix \ref{app:seeds}.


\paragraph{Fillers} In real-world scenarios, interactions between colleagues can also include topics that do not necessarily impact daily tasks. To simulate this, we use fillers, which can be of two types. 
The first type contains general information about common topics at work such as \textit{remote work policy}.
The second contains instructions similar to those introduced above, but not strictly related to code generation, like \textit{use GitHub as the main version control system}. These latter fillers are meant to be harder distractors, as a model, recognizing them as instructions, might focus on them.

Fillers can be updated throughout sessions, however---unlike instructions---they are never evaluated.\footnote{For this reason, 
from now on,
`instructions' will always refer to 
coding ones---not fillers---unless
differently specified.}
We manually gathered 80 fillers, 50 of the first type, and 30 of the second. A filler can be updated up to 4 times. The full list of fillers are in Table \ref{tab:fillers} of Appendix \ref{app:seeds}.

\paragraph{Personas} Personas define the personality traits of the mentor and the mentee. 
By having different personas and combining them, we can generate conversations that are more diverse and thus increase the variety of the dataset. We define 6 personas for the mentor and 5 for the mentee (see Table \ref{tab:mentee_personas} and Table \ref{tab:mentor_personas} of Appendix \ref{app:seeds}). 

\paragraph{Names} We define lists of fictitious names for mentors, mentees, and companies, from which we randomly sample to generate the conversations (see Table \ref{tab:names} of Appendix \ref{app:seeds}). 

\subsection{Dialogue Histories}
\label{subsec:dataset_creation}

We generate the dialogue histories in two steps: we first create templates by sampling different combinations of seeds and other parameters, and then generate the actual histories based on these templates using an LLM, as shown in Figure \ref{fig:dataset_creation_process}.


\begin{table}[]
\begin{tabular}{ll}
\toprule
\textbf{Parameter}      & \textbf{Range}                                                                   \\
\midrule
Sessions ($n$)          & $$\{1,2,3,4,5,10,15,$$ \\ & $$20,30,40,50,100\}$$ \\
Sessions with instr. (\%)               &   $[50, 70]$                                                                    \\
Instr. in a session ($n$) &  $\{1,2,3\}$                                                                     \\
Instr. updates (\%)      & $[30, 70]$                                                                       \\
Filler updates (\%)      & $[50, 70]$     \\                          
\bottomrule
\end{tabular}
\caption{Parameters for dialogue history generation.}
\label{tab:parameters_range}
\end{table}



\paragraph{Template generation}  
We initially sample a \textbf{name} and a \textbf{persona} for the mentor and mentee, and a \textbf{name} for the company from our seeds. 
We then randomly pick a value for each of the following parameters: 
(i) \textbf{sessions}: how many sessions will be included in the dialogue history;
(ii) \textbf{sessions with instructions}: the percentage of sessions that will include an instruction. Since we set the maximum value to 70\%, some sessions will only have fillers;
(iii) \textbf{instructions in session}: how many instructions a session will include (min 1; max 3);
(iv) \textbf{instructions and update ratio}: the actual instructions that will be included, and how many of them will be updated;
(v) \textbf{fillers and update ratio}: same as for instructions.
Table \ref{tab:parameters_range} presents the parameters range we used to generate the dataset. 




\begin{table}[]
    \centering
    \begin{tabularx}{1\columnwidth}{Xll}
    \toprule
    \textbf{Parameter}  & \textbf{Short dataset} & \textbf{Long dataset} \\
    & \textbf{(<15 sessions)} &  \textbf{(>15 sessions)} \\
    \midrule
    Sessions             & \normalsize{5.71} \scriptsize{($\pm$4.65)}            & \normalsize{48.00} \scriptsize{($\pm$27.85)}         \\
    Sessions\textsubscript{w/ instr.} & \normalsize{3.38} \scriptsize{($\pm$2.66)}            & \normalsize{28.13} \scriptsize{($\pm$16.56)}         \\
    Instr.               & \normalsize{4.98} \scriptsize{($\pm$4.10)}            & \normalsize{42.24} \scriptsize{($\pm$25.37)}         \\
    Instr.\textsubscript{added}           & \normalsize{3.56} \scriptsize{($\pm$2.62)}            & \normalsize{24.82} \scriptsize{($\pm$15.06)}         \\
    Instr.\textsubscript{updated}        & \normalsize{1.41} \scriptsize{($\pm$1.97)}            & \normalsize{17.42} \scriptsize{($\pm$11.93)}         \\
    Fillers              & \normalsize{5.04} \scriptsize{($\pm$4.75)}            & \normalsize{45.06} \scriptsize{($\pm$29.36)}         \\
    Filler\textsubscript{added}          & \normalsize{3.36} \scriptsize{($\pm$2.92)}            & \normalsize{24.63} \scriptsize{($\pm$12.70)}         \\
    Filler\textsubscript{updated}       & \normalsize{1.52} \scriptsize{($\pm$1.81)}            & \normalsize{18.86} \scriptsize{($\pm$13.48)}         \\
    Tokens         & \normalsize{3.20k} \scriptsize{($\pm$2.71k)}            & \normalsize{26.15k} \scriptsize{($\pm$15.50k)}         \\
    Vocabulary           & \normalsize{8.54k}                                    & \normalsize{14.24k}                                  \\
    \bottomrule
    \end{tabularx}
    \caption{Summary statistics (averages and standard deviations) for the `short' and `long' datasets.}
    \label{tab:dataset_statistics}
\end{table}


\paragraph{Dialogue history generation} For each session, we automatically construct a prompt incorporating the information from the template. The prompt introduces the company, the mentor, and the mentee, as well as the instructions and fillers of the session. We then use Command R+ \citep{cohere2024} to generate the session. We report examples of prompts in Table \ref{tab:prompt_conv_70} and \ref{tab:prompt_conv_108} of Appendix \ref{app:examples}. 

The resulting dataset contains 360 dialogue histories, 30 for each of the following number of sessions: 1, 2, 3, 4, 5, 10, 15, 20, 30, 40, 50, 100.
In what follows, we use `short' to refer to histories with fewer than 15 sessions (54\% of the total), and `long' to those with more than 15 sessions (46\%). Note that the longest history contains 63k tokens, which still fits the context window of all the models we used.
In Table \ref{tab:dataset_statistics}, we report the main statistics of the dataset. 
During the dataset creation, to ensure quality, we performed several generation rounds that we manually assessed and used to further optimize the prompting. Manual inspection of the final generated dialogue histories confirmed the overall quality and coherence of the dataset.

\section{Experiments}
\label{subsec:experiments}




We evaluate models on \textsc{MemoryCode} on three evaluation setups, each of them including a different kind of textual input. 


\paragraph{\textsc{Instruction}} The input consists of a single instruction (e.g., in Figure~\ref{fig:memoryCode_fig1_sketch}, `\textit{add the current date to the start of every function}'). This setting is included to assess how good models are at performing coding tasks without any conversational setup.

\paragraph{\textsc{Session}} The input is an entire session (in Figure~\ref{fig:memoryCode_fig1_sketch}, a whole-day mentor-mentee interaction). 
In this setup, the model output is correct only if the model simultaneously adheres to \textit{all} the instructions introduced in the session.

\paragraph{\textsc{History}} The input of the model is the whole dialogue history, i.e., the concatenation of all sessions (in Figure~\ref{fig:memoryCode_fig1_sketch}, the entire 20-day mentor-mentee interaction). This setup is the most challenging one,
as it evaluates the ability to recall information from previous sessions and to use it together with new information to correctly perform the task. As such, it 
mimics realistic working scenarios, where colleagues interact over long periods.\\
    


Given an instruction and the model output, we assess it using the corresponding regex function. 
The model receives a score of 1 only if the instruction is correctly applied to all instances of the relevant Python object and there are no syntax errors.\footnote{Additionally, if the relevant Python object is not present in the generated code (<1\% of the cases), the instruction is not taken into account when averaging the scores.} For example, if the instruction is \textit{always start function names with `g\_'}, all functions in the generated code must start with `g\_'.
The overall model's performance is computed using macro-averaged accuracy.



\subsection{Models}

We test several recent LLMs on our benchmark, namely, three versions of Llama-3.1~\cite[8B-Instruct, 70B-Instruct, and 405B-Instruct;][]{dubey2024llama},
Command R+ \citep{cohere2024}, and GPT-4o~\citep{openai2024}. Our model selection includes both proprietary and open-weights models, covering a broad range of model sizes. 
This provides us with a comprehensive overview of how various types of LLMs perform on our dataset. We note that all the models have been trained on code and tested on Python coding benchmarks such as HumanEval \citep{chen2021evaluating} and MBPP \citep{austin2021program}. The details to reproduce the results are provided in Appendix~\ref{app:hyperparameters}. %


