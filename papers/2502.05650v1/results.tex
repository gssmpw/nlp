\begin{table*}[t]
    \centering

    \begin{subtable}{0.56\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{l|ccc|ccc|ccc}
            \hline
            & \multicolumn{3}{c|}{Fine-tuned MLM} & \multicolumn{3}{c|}{MLM -- Regular prompts} & \multicolumn{3}{c}{MLM -- Question prompts}  \\
            \hline
            \textbf{Model} & \textbf{Pre} & \textbf{Rec} & \textbf{F1} & \textbf{Pre} & \textbf{Rec} & \textbf{F1} & \textbf{Pre} & \textbf{Rec} & \textbf{F1} \\
            \hline
            \bf Longformer & 0.59 & 0.59 & 0.59 & 0.63 & 0.63 & 0.62 & 0.66 & 0.63 & 0.63 \\
            \bf Big-Bird & 0.63 & 0.62 & 0.62  & 0.64 & 0.64 & 0.64 & 0.65 & 0.65 & 0.64\\
            \bf Bird-MNLI & 0.64 & 0.63 & 0.63 & 0.64 & 0.62 & 0.63 & 0.65 & 0.63 & 0.63 \\
            \hline
        \end{tabular}}
        \caption{Masked Language Models.}
        \label{table:performance:PLM}
    \end{subtable}
\hfill
    \begin{subtable}{0.42\textwidth}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|ccc|ccc}
            \hline
        & \multicolumn{3}{c|}{LLM -- Question prompts} & \multicolumn{3}{c}{\model\ -- $6Ws$ prompts} \\
            \cline{2-7}
            \textbf{Model} & \textbf{Pre} & \textbf{Rec} & \textbf{F1} & \textbf{Pre} & \textbf{Rec} & \textbf{F1} \\
            \hline
            \bf Llama-3 [8B] & 0.66 & 0.68& 0.67 & 0.62  & \bf 0.89 & 0.73 \\
            \bf Gemma [9B] & 0.62 & 0.69 & 0.65 & 0.64 &  0.76& 0.70 \\
            \bf Mistral [7B] & 0.63 & 0.83 & 0.71 & \bf 0.65  & 0.87 & \textbf{0.75} \\
            \bf Qwen [7B] & 0.61 & 0.75 & 0.67 & 0.64 & 0.74 & 0.69 \\
            \hline
        \end{tabular}}
        \caption{Instruction-tuned Large Language Models.}
        \label{table:performance:LLM}
    \end{subtable}
    \vspace{-3mm}
    \caption{Performance evaluation of incongruence detection on \dataset\ using different tuning approaches.}
    \label{table:performance}
\end{table*}

% \begin{table*}[t]
%     \centering
%     \hspace*{1cm}
%     % \small % Reduce font size
    
% \end{table*}


% \begin{table*}[t]
%     \centering
%     \hspace*{1cm}
%     % \small % Reduce font size
%     \begin{tabular}{p{4cm}|p{3cm}|p{3cm}|p{3cm}}
%         \hline
%         \multicolumn{4}{c}{Fine-tuned pre-trained MLM model} \\
%         \hline
%         \textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
%         \hline
%         Longformer & 0.59 & 0.59 & 0.591 \\
%         Big-Bird & 0.63 & 0.62 & 0.62 \\
%         Bird-MNLI & 0.64 & 0.63 & 0.63 \\
%         \hline
%         \multicolumn{4}{c}{pre-trained MLM Models with regular prompt tuning} \\
%         \hline
%         Longformer & 0.632 & 0.638 & 0.629 \\
%         Big-Bird & 0.64 & 0.64 & 0.6 \\
%         Bird-MNLI & 0.64 & 0.62 & 0.63 \\
%         \hline
%         \multicolumn{4}{c}{pre-trained MLM Models with question prompt tuning} \\
%         \hline
%         Longformer & 0.66 & 0.635 & 0.63 \\
%         Big-Bird & 0.65 & 0.65 & 0.647 \\
%         Bird-MNLI & 0.65 & 0.63 & 0.63 \\
%         \hline
%         \multicolumn{4}{c}{Large Language models instruction tuned on question prompts} \\
%         \hline
%         Llama-3 & 0.664 & 0.68& 0.676 \\
%         Gemma-7b & 0.62 & 0.69 & 0.657 \\
%         Mistral-7b & 0.63 & 0.83 & 0.71 \\
%         Qwen-7b & 0.612 & 0.75 & 0.675 \\
%         \hline
%         \hline
%         \multicolumn{4}{c}{Large Language models instruction tuned on 6W prompts} \\
%         \hline
%         Llama-3 & 0.626  & 0.89 & 0.73 \\
%         Gemma-7b & 0.644 &  0.76& 0.70 \\
%         Mistral-7b & 0.65  & 0.87 & \textbf{0.752} \\
%         Qwen-7b & 0.649 & 0.748 & 0.69 \\
%         \hline
%     \end{tabular}
%     \caption{Performance metrics of different pre-trained models with various tuning methods.}
%     \label{table:performance}
% \end{table*}