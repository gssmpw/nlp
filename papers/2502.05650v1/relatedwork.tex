\section{Related Work}
\paragraph{Existing Datasets:} One of the key areas where detecting incongruence is crucial is during interrogations, where individuals may attempt to deceive by presenting statements that contradict established evidence, facts, or the testimonies of other witnesses. Deception research primarly uses two types of dataset. Real-life and Mock. The Real Life Trial dataset \cite{perez2015verbal}\,includes misleading and honest footage extracted from courtroom footage while Mock datasets , generated under controlled conditions, include the work by \cite{CSC} with 32 hours of deceptive speech.
The Bag-of-Lies dataset by\cite{Bag-of-lies}, contains visual, audio, EEG, and eye gaze data collected in real-life situations utilizing standard mobile devices and sensors.
\vspace{-0.3cm}
\paragraph{Contradiction Detection:} A large portion of contradiction detection research falls under the scope of Natural Language Inference where the objective is to determine whether a hypothesis is entailing, contradicting or undetermined given a premise \cite{stanford-nlp-manning}. Early efforts focussed on sentence-level entailment, such as the work of \cite{Khot_Sabharwal_Clark_2018} \cite{schuster-etal-2022-stretching} extended NLI to longer documents with SeNtLI, through it retained sentence-level decomposition, which restricted context preservation.
However, no existing work effectively detects contradictions in personal narratives involving differing perspective.
\vspace{-0.3cm}
\paragraph{Large Language Models: } LLMs have recently gained popularity in various natural language processing tasks \cite{gpt}. Open-source models like Llama \cite{dubey2024llama} and Mistral \cite{jiang2023mistral} excel in handling complex tasks through natural language instructions. Instruction tuning, which fine-tunes LLMs using datasets paired with specific instructions, has also become widespread (Jiao et al., 2023a; Wang et al., 2023b; Zhang et al., 2023; Cheng et al., 2023), enabling the models to make task-specific predictions.

Additionally, recent works have started to leverage the common sense understanding of these models\cite{paranjape-etal-2021-prompting}. Research by \cite{fei2023reasoning,jiang-etal-2022-understanding} has demonstrated the value of multi-hop reasoning in enhancing model performance, especially in tasks that demand deep contextual understanding. The reasoning ability is essential for semantic understanding task like contradiction detection, where the model must grasp subtle nuances and approach the problem by mimicking human-like, step-by-step thinking. To the best of our knowledge no existing work detects contradictions in personal narratives involving multiple events or differing perspectives from various individuals, and all current approaches follow a binary method. In contrast, \model\ distinguishes itself by operating at both the binary and span levels, addressing the gap in analyzing complex, multi-perspective datasets.