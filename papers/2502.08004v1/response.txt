\section{Related Work}
\label{sec:related_work}

In BOED, we focus on the setting of myopic gradient-based experimental design. Previously, Gutmann et al., "Probabilistic Integration for Scalable Bayesian Computation" proposed various likelihood-free information bounds that could work in the SBI setting based on variational inference estimators but did not make use of a normalized generative model like a normalizing flow. This is problematic for use in sequential SBI methods that make use of a normalized likelihood or posterior functions. Zhang et al., "Deep Exploration via Bootstrapped DQN" developed MINEBED to simultaneously optimize experimental designs and a critic that could draw samples from the posterior, but relied on a differentiable simulator or using Bayesian Optimization. Fu et al., "Robust Imitation Learning with Wasserstein Loss" extended both previous works and developed policy-based experimental designs for non-myopic experimental designs, but whose critics also relied on differentiable simulators - simulators whose inputs can be connected to a differentiable computation graph, which may not be available for scientific simulators. A RL approach Mnih et al., "Human-level control through deep reinforcement learning" optimized a critic without requiring a differentiable simulator but relies on computationally expensive RL algorithms and may not be feasible for practitioners with scientific simulators that can take significant amount of time to simulate. This is exemplified in our biological experiment where each round of simulation takes about ten seconds.

Recent work connecting SBI methods to MI-based optimization studied how to stably train a discriminative and generative SBI model Zhang et al., "Stabilizing Deep Neural Networks with Weight Normalization" , and applying a nonparametric function to the selection of data points to reduce variance of the resulting MI estimate Gutmann et al., "Probabilistic Integration for Scalable Bayesian Computation" . These methods take a complimentary approach to ours but require a generative and discriminative model whereas we only require a generative model. Additionally, we study the utility of MI-based optimization of SBI models in BOED.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Experiments %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
  \centering
  \includegraphics[width=\textwidth]{figs/two_moons_sweep2.png}
  \vspace{-9pt}
  \caption{Comparison on the Two Moons task of the EIG and the validation loss $-\E \log p_\phi(y | \theta)$ across varying number of contrastive samples ($L = N - 1$) and $\lambda$ regularization. Increasing number of contrastive samples improves the information lower bound and likelihood validation metrics.  The $\lambda$ parameter helps improve the likelihood accuracy at the expense of MI estimation.
  } % We include a reference MI calculation for the single design scenario which is detailed in Appendix D. 
  \label{fig:two_moons}
  \vspace{-12pt}
\end{figure*}