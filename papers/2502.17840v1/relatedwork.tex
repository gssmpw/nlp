\section{Related Work}
Automated theorem proving has gained prominence in artificial intelligence~\cite{saxton2019analysing, wang-etal-2023-dt,DBLP:journals/corr/abs-2410-15700}, yet its application remains largely restricted to relatively simple mathematical problems and struggles with proving more complex statements. Data scarcity and its uneven distribution across different mathematical domains is a key challenge.


Among the largest publicly available datasets in AI4Maths, Numina~\cite{li2024numinamath} contains 860K competition-level math problems with solutions following the Chain of Thought (CoT)~\cite{cotgoogle} reasoning paradigm. In contrast, Mathlib~\cite{MathlibCommunity}, the community-maintained Lean formal mathematics dataset, provides a rigorously structured repository spanning algebra, number theory, and combinatorics. However, both datasets rely heavily on manual formalization, leading to limited coverage and uneven representation across specialized mathematical fields.

To address the limitations of data scarcity, researchers have explored automated theorem generation~\cite{piotrowski2018atpboost}. In recent years, theorem generation  techniques based on neural networks and LLMs have emerged as a promising direction, opening up new possibilities for both theorem generation and automated proving ~\cite{gauthier2019deep,r1}.
Existing approaches leverage various strategies for theorem generation: INT~\cite{r1} integrates inequalities to construct new theorems, Leandojo~\cite{r5} retrieves and synthesizes formal statements from Mathlib, and MetaGen~\cite{MetaGen2020} transplants proof trees while employing reinforcement learning to align theorem generation with human reasoning. Meanwhile, another class of generators relies on LLMs to assist in theorem generation. Specifically, these models are often employed as premise selectors to identify key reasoning steps or to iteratively sample from existing theorems, generating new ones that adhere to specific rules~\cite{urban2020first, ds-prover, palermo2022synthetic, polu2020, lime}. Furthermore, the PACT method~\cite{r18} extracts data from theorems and generates nine distinct language modeling tasks, enabling data augmentation within the Lean theorem prover.  
 

However, most existing theorem generators expand datasets by extracting sub-theorems from complete theorems or synthesizing new ones. Against this backdrop, we combine
LLMs’ broad knowledge coverage with RL’s exploration-driven capabilities to conduct tactic prediction research based on our manually constructed benchmark, \textit{LeanComb}.
Our goal is to generate high-quality, novel theorems that contribute to developing data resources in specialized mathematical fields while advancing the capabilities of automated theorem-proving systems.