@article{DBLP:journals/corr/abs-2410-15700,
  author       = {Zijian Wu and
                  Suozhi Huang and
                  Zhejian Zhou and
                  Huaiyuan Ying and
                  Jiayu Wang and
                  Dahua Lin and
                  Kai Chen},
  title        = {InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert
                  Iteration on Large-Scale {LEAN} Problems},
  journal      = {CoRR},
  volume       = {abs/2410.15700},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2410.15700},
  doi          = {10.48550/ARXIV.2410.15700},
  eprinttype    = {arXiv},
  eprint       = {2410.15700},
  timestamp    = {Tue, 26 Nov 2024 15:54:17 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2410-15700.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{MathlibCommunity, series={POPL ’20},
   title={The lean mathematical library},
   url={http://dx.doi.org/10.1145/3372885.3373824},
   DOI={10.1145/3372885.3373824},
   booktitle={Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs},
   publisher={ACM},
   author={The mathlib Community},
   year={2020},
   month=jan, collection={POPL ’20} }

@article{MetaGen2020,
  title={Learning to prove theorems by learning to generate theorems},
  author={Wang, Mingzhe and Deng, Jia},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18146--18157},
  year={2020}
}

@inproceedings{cotgoogle,
author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
title = {Chain-of-thought prompting elicits reasoning in large language models},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought  
            demonstrations are provided as exemplars in prompting.Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1800},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@article{ds-prover,
  title={Enhancing neural theorem proving through data augmentation and dynamic sampling method},
  author={Vishwakarma, Rahul and Mishra, Subhankar},
  journal={arXiv preprint arXiv:2312.14188},
  year={2023}
}

@article{gauthier2019deep,
  title={Deep reinforcement learning for synthesizing functions in higher-order logic},
  author={Gauthier, Thibault},
  journal={arXiv preprint arXiv:1910.11797},
  year={2019}
}

@article{li2024numinamath,
  title={Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions},
  author={Li, Jia and Beeching, Edward and Tunstall, Lewis and Lipkin, Ben and Soletskyi, Roman and Huang, Shengyi and Rasul, Kashif and Yu, Longhui and Jiang, Albert Q and Shen, Ziju and others},
  journal={Hugging Face repository},
  volume={13},
  year={2024}
}

@inproceedings{lime,
  title={Lime: Learning inductive bias for primitives of mathematical reasoning},
  author={Wu, Yuhuai and Rabe, Markus N and Li, Wenda and Ba, Jimmy and Grosse, Roger B and Szegedy, Christian},
  booktitle={International Conference on Machine Learning},
  pages={11251--11262},
  year={2021},
  organization={PMLR}
}

@inproceedings{palermo2022synthetic,
  title={Synthetic proof term data augmentation for theorem proving with language models},
  author={Palermo, Joseph and Ye, Johnny and Han, Jesse Michael},
  booktitle={Proceedings of the 7th Conference on Artificial Intelligence and Theorem Proving},
  year={2022}
}

@inproceedings{piotrowski2018atpboost,
  title={ATPboost: Learning premise selection in binary setting with ATP feedback},
  author={Piotrowski, Bartosz and Urban, Josef},
  booktitle={Automated Reasoning: 9th International Joint Conference, IJCAR 2018, Held as Part of the Federated Logic Conference, FloC 2018, Oxford, UK, July 14-17, 2018, Proceedings 9},
  pages={566--574},
  year={2018},
  organization={Springer}
}

@article{polu2020,
  title={Generative language modeling for automated theorem proving},
  author={Polu, Stanislas and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2009.03393},
  year={2020}
}

@inproceedings{r1,
  author       = {Yuhuai Wu and
                  Albert Q. Jiang and
                  Jimmy Ba and
                  Roger Baker Grosse},
  title        = {{INT:} An Inequality Benchmark for Evaluating Generalization in Theorem
                  Proving},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=O6LPudowNQm},
  timestamp    = {Mon, 30 May 2022 11:51:52 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/WuJBG21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{r18,
  title={Proof artifact co-training for theorem proving with language models},
  author={Han, Jesse Michael and Rute, Jason and Wu, Yuhuai and Ayers, Edward W and Polu, Stanislas},
  journal={arXiv preprint arXiv:2102.06203},
  year={2021}
}

@article{r5,
  title={Leandojo: Theorem proving with retrieval-augmented language models},
  author={Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan J and Anandkumar, Animashree},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{saxton2019analysing,
  title={Analysing mathematical reasoning abilities of neural models},
  author={Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1904.01557},
  year={2019}
}

@inproceedings{urban2020first,
  title={First neural conjecturing datasets and experiments},
  author={Urban, Josef and Jakub{\v{u}}v, Jan},
  booktitle={Intelligent Computer Mathematics: 13th International Conference, CICM 2020, Bertinoro, Italy, July 26--31, 2020, Proceedings 13},
  pages={315--323},
  year={2020},
  organization={Springer}
}

@inproceedings{wang-etal-2023-dt,
    title = "{DT}-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function",
    author = "Wang, Haiming  and
      Yuan, Ye  and
      Liu, Zhengying  and
      Shen, Jianhao  and
      Yin, Yichun  and
      Xiong, Jing  and
      Xie, Enze  and
      Shi, Han  and
      Li, Yujun  and
      Li, Lin  and
      Yin, Jian  and
      Li, Zhenguo  and
      Liang, Xiaodan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.706/",
    doi = "10.18653/v1/2023.acl-long.706",
    pages = "12632--12646",
    abstract = "Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does. Here, to accommodate general theorems, we propose a novel Dynamic-Tree Driven Theorem Solver (DT-Solver) by guiding the search procedure with state confidence and proof-level values. Specifically, DT-Solver introduces a dynamic-tree Monte-Carlo search algorithm, which dynamically allocates computing budgets for different state confidences, guided by a new proof-level value function to discover proof states that require substantial exploration. Experiments on two popular theorem-proving datasets, PISA and Mathlib, show significant performance gains by our DT-Solver over the state-of-the-art approaches, with a 6.65{\%} improvement on average in terms of success rate. And especially under low computing resource settings (11.03{\%} improvement on average)."
}

