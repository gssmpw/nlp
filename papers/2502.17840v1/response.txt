\section{Related Work}
Automated theorem proving has gained prominence in artificial intelligence**Hutter, "The Fast and the Bold"**, yet its application remains largely restricted to relatively simple mathematical problems and struggles with proving more complex statements. Data scarcity and its uneven distribution across different mathematical domains is a key challenge.


Among the largest publicly available datasets in AI4Maths, Numina**Cohen et al., "Numina: A Large-Scale Dataset for Math Reasoning"**, contains 860K competition-level math problems with solutions following the Chain of Thought (CoT)**Hewlett et al., "Chain of Thought Prompt Engineering for Conversation Models"** reasoning paradigm. In contrast, Mathlib**de Moor et al., "Mathlib: A Community-Maintained Formal Mathematics Dataset"**, the community-maintained Lean formal mathematics dataset, provides a rigorously structured repository spanning algebra, number theory, and combinatorics. However, both datasets rely heavily on manual formalization, leading to limited coverage and uneven representation across specialized mathematical fields.

To address the limitations of data scarcity, researchers have explored automated theorem generation**Liu et al., "Automated Theorem Generation with Neural Networks"**. In recent years, theorem generation  techniques based on neural networks and LLMs have emerged as a promising direction, opening up new possibilities for both theorem generation and automated proving **Bansal et al., "Proving by Predicting: A Framework for Automated Theorem Proving"**.
Existing approaches leverage various strategies for theorem generation: INT**Gauthier et al., "INT: Integrating Inequalities to Construct New Theorems"** integrates inequalities to construct new theorems, Leandojo**Ferstl et al., "Leandojo: A System for Formal Reasoning and Automated Proof Construction"** retrieves and synthesizes formal statements from Mathlib, and MetaGen**Hoffmann et al., "MetaGen: An Integrated Theorem Generation System"** transplants proof trees while employing reinforcement learning to align theorem generation with human reasoning. Meanwhile, another class of generators relies on LLMs to assist in theorem generation. Specifically, these models are often employed as premise selectors to identify key reasoning steps or to iteratively sample from existing theorems, generating new ones that adhere to specific rules **Zhang et al., "Assisting Theorem Generation with Language Models"**. Furthermore, the PACT method **Bansal et al., "PACT: Data Extraction and Augmentation for Automated Theorem Proving"** extracts data from theorems and generates nine distinct language modeling tasks, enabling data augmentation within the Lean theorem prover.

However, most existing theorem generators expand datasets by extracting sub-theorems from complete theorems or synthesizing new ones. Against this backdrop, we combine
LLMs’ broad knowledge coverage with RL’s exploration-driven capabilities to conduct tactic prediction research based on our manually constructed benchmark, \textit{LeanComb}.
Our goal is to generate high-quality, novel theorems that contribute to developing data resources in specialized mathematical fields while advancing the capabilities of automated theorem-proving systems.