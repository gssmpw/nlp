%\newpage
\clearpage
\appendix
%\onecolumn
%\setcounter{page}{1}

%------------------------------------------------------------------------
% Contents
%------------------------------------------------------------------------

\section{Table of Contents}

In this supplementary material, we provide details regarding our implementation and experimental setup in \cref{sec:supp-impl}, explain the shape optimization experiment and give additional results in \cref{sec:supp-optim}, show parts interpolation in \cref{sec:supp-interp}, ablate components of \PSDF{} in \cref{sec:exp-ablation}, and we provide additional experimental results in \cref{sec:supp-results}: further visualizations for shape reconstruction (\cref{sec:supp-recon}), generation (\cref{sec:supp-gen}) and manipulation (\cref{sec:supp-manip}).


%------------------------------------------------------------------------
\section{Implementation Details}
\label{sec:supp-impl}

In this section, we describe the architecture of \PSDF{} (\cref{sec:supp-net}), its training procedure (\cref{sec:supp-train}), and the point cloud encoder and diffusion models we used (\cref{sec:supp-enc}). Then, we detail our datasets and their processing (\cref{sec:supp-data}), the baselines against which we compare and any hyperparameter choices (\cref{sec:supp-baseline}), and finally the metrics used in this work and how they were computed (\cref{sec:supp-metrics}).

\subsection{Network Architecture}
\label{sec:supp-net}

The final architecture of \PSDF{}, as presented in this work, is as follow:

The dimensionality of the part latent space is $Z=256$, such that $\bz_p\in\real^{256} \; \forall p$, and the hidden size of the fully-connected layers $h_\text{fc}$ is 512, with the exception of the model trained on the \textit{Chair} dataset where it is 256. This is because there are more parts, thus we expect each part to be simpler and it helps to alleviate the memory increase. We use weight normalization~\cite{Salimans16b} on these layers, as in~\cite{Park19c}. \PSDF{} has 8 $h_\text{fc}$ layers, counting the input and output ones, with a convolutional layer $h_\text{conv}$ between each pair, as described in \cref{sec:method-dec}. All layers, except the output one, are followed by ReLU non-linearities. See \cref{fig:supp-conv} for a visualization of their arrangement in our decoder. Note that each $h_\text{conv}$ has only $P^2+P$ trainable parameters, \eg, $25+5=30$ parameters in the case of cars.

\input{fig/supp/conv}


\subsection{Training Procedure}
\label{sec:supp-train}

The decoder model of \PSDF{} is trained for 2000 epochs using a batch size of 16 and 8192 sampled points per shape. We use the Adam optimizer~\cite{Kingma15a} with default parameters in PyTorch~\cite{Paszke17}, setting a learning rate of $5 \times 10^{-4}$ for the model and $1 \times 10^{-3}$ for the latent vectors. The learning rates are reduced by a factor of 0.35 at 80\% and 90\% of the total training epochs.
The latent regularization weight is set to $\lambda = 10^{-4}$, and the non-intersection loss uses a softmax temperature parameter set to $0.02$.

To supervise the SDF, we follow the sampling strategy used in DeepSDF~\cite{Park19c}: 95\% of the sampled points are near the surface, and 5\% are uniformly distributed in space. The SDF values are computed using the IGL library~\cite{Jacobson18}, assuming watertight meshes. During training, the SDF values are clamped between $-0.1$ and $0.1$ to focus the capacity of the network around the surface.

Trainings are conducted on a single NVIDIA V100 GPU with 32GB of memory, taking 1â€“2 days to train depending on the dataset size.

\subsection{Encoder and Diffusion Models}
\label{sec:supp-enc}

\paragraph{Point Cloud Encoder.} In \cref{sec:exp-recon}, we use a point encoder to map point clouds of the full shapes to \PSDF{} parameter space, \ie{} part latents and poses. For this, we leverage the model of 3DShape2VecSet~\cite{Zhang23d} using their official implementation\footnote{\url{https://github.com/1zb/3DShape2VecSet}}. We use a smaller version of the model with 8 self-attention layers (against 24 in the original work), and without the final cross-attention layer with query positions (see Fig.~3 in their work) as we want to output part latents and poses instead of implicit values. We use learnable queries instead of point queries (see Fig.~4(a) in~\cite{Zhang23d}) to encode the point cloud, which creates an order on the latent set, allowing us to recover the parameters of specific parts. Note that we concatenate latent and pose for every part, such that the encoder outputs them both in a single vector.

\paragraph{Diffusion Model.} In order to generate composite shapes with \PSDF{}, we use a diffusion model to generate the part latents and poses (\cref{sec:exp-gen}), more specifically \SALAD~\cite{Koo23} with their official implementation\footnote{\url{https://github.com/KAIST-Visual-AI-Group/SALAD}}. For the second diffusion on the latents, we however make use of a transformer decoder instead of encoder so that each part latent can attend to all part poses, and the part latents are normalized before training. This last point was found crucial for the diffusion as our latent vectors tend to have significantly lower norms than random vectors sampled from a multivariate Gaussian distribution. Indeed, as in~\cite{Park19c}, latent vectors are initialized such that $\mathbb{E}_{\bz_p}\lVert\bz_p\rVert=1$, while Gaussian noise in $d$-dimensions has $\mathbb{E}_{\bx\sim\mathcal{N}^d}\lVert\bx\rVert=\sqrt{d}$.

\subsection{Datasets}
\label{sec:supp-data}

For all datasets, we preprocess the meshes by centering them using their bounding box and rescaling them into the cube $[-0.9, 0.9]^3$, ensuring the longest edge of the bounding box measures $1.8$. This leaves a margin around the shape to prevent the surface from leaving the meshing region, defined as the $[-1, 1]^3$ cube. 

To create the SDF supervision, we perform two types of sampling~\cite{Park19c}. First, we sample points on the surface of the mesh and perturb them with Gaussian noise, using variances of $0.005$ and $0.0005$ to generate points near the surface. Second, we uniformly sample points within the $[-1, 1]^3$ cube. The SDF values for all sampled points are computed using \textit{libigl}~\cite{Jacobson18}. The samples and their SDF are mixed in a 95/5\% ratio (near-surface to uniform samples) and stored for training. During training, a subset of these precomputed points is used, randomized at each epoch.

Additionally, each part, given as a subset of the original mesh, is fitted with a cuboid or cylinder. The orientation, translation, and scale of the primitive is then used as that part's pose.

As explained in \cref{sec:experiments}, obtaining high-quality composite shape data is challenging, with online data usually containing non-watertight meshes, or without part decomposition or an inconsistent one. Therefore, we use three curated datasets in this work.

\paragraph{Cars.} In order to perform Computational Fluid Dynamics (CFD) simulations on the cars, see \cref{sec:supp-optim} below, we need them to be correctly closed and physically plausible, \ie{} wheels and body correctly separated. We therefore hand-process a subset of ShapeNet~\cite{Chang15}'s cars: The wheels and body are manually selected and separated, and then remeshed to become watertight~\cite{Wang22}.

\paragraph{Mixers.} This dataset was proposed by~\cite{Vasu22} and consists in liquid mixers made of a central tube, an interior helix, and two attach points. It has the advantage of having an available, and consistent, part decomposition, which is suitable to learn a composite shape representation. The main challenge lies in the thinness of the tubes and helices, and the latter's complexity.

\paragraph{Chairs.} We use a clean subset of ShapeNet's chairs with semantic segmentation provided by PartNet~\cite{Mo19}, with seat, backrest, armrest, and leg classes. To enable SDF computation, we first make the chairs watertight~\cite{Stutz18}. Then, the PartNet segmentation, originally provided as a point cloud, is transferred to the mesh using a voting scheme: each face is assigned the most frequent class among its closest points. To refine this segmentation, we further divide the armrests and legs into individual parts by detecting connected components. Chairs that fail this finer segmentation are removed to ensure a consistency within our dataset. Additionally, we discard shapes at intermediate steps if they exhibit issues such as misalignment with the segmentation point cloud or a high Chamfer-Distance relative to the original shape.

Each dataset is split into train and test sets ($80\%/20\%$). Model hyperparameters are tuned using the train and validation sets, and final models, as reported below, are trained on train+validation and evaluated on the test set. Part poses are obtained by fitting each part with a cuboid or a cylinder.


\subsection{Baselines}
\label{sec:supp-baseline}

\paragraph{\DAENET.} \DAENET~\cite{Chen24b} is a recent method that learns an unsupervised part decomposition to perform co-segmentation. It creates multiple part templates that are deformed to reconstruct a shape given its voxelization as input, allowing the original shape to be segmented based on the predicted decomposition. As mentioned in our main text, unsupervised methods like \DAENET{} are typically designed to discover new shape decompositions, which is unsuitable for engineering design where parts have specific predefined meanings and uses. Additionally, as shown in our experiments, the reliance on voxelized inputs significantly limits its representation ability. We use the official implementation\footnote{\url{https://github.com/czq142857/DAE-Net}} and conduct a grid search to tune the sparsity loss weight $\gamma$ (see Eqs. (5) and (6) in their work). The optimal values found were $\gamma=0.001$ for cars and mixers, and $\gamma=0.002$ for chairs, consistent with their reported results.

\paragraph{\BAENET.} Similarly, \BAENET~\cite{Chen19i} encodes a voxelized input to generate part implicit fields, which can be used for co-segmentation. The key difference, and the reason for comparison to our approach, is that \BAENET{} also introduces a weak supervision scheme where ground truth segmentations are provided for a small subset of the training shapes (typically ranging from 1 to 8 shapes). In this setting, the model propagates the learned decomposition across the entire dataset. Similar to \DAENET{}, \BAENET{} suffers from the limitations of voxelized inputs, which restrict its reconstruction accuracy. However, as shown in \cref{fig:recon}, the weak supervision allows it to recover the desired decomposition in most cases, although certain parts, such as car wheels, are sometimes segmented together rather than individually. We use the official implementation\footnote{\url{https://github.com/czq142857/BAE-NET}} to train the models. For the weak supervision, we experiment with 1, 2, 3, and 8 shots, finding that 8-shot supervision consistently yields the best results. Therefore, we use these models.

\paragraph{\PQNET.} As a fully supervised baseline, we compare against \PQNET~\cite{Wu20c}. \PQNET{} employs a two-stage training process: first, an implicit auto-encoder is trained to reconstruct all individual parts, followed by a GRU-based RNN~\cite{Cho14b} that learns to auto-encode sequences of parts, represented by their latent vectors and bounding box parameters. For shape generation, latentGANs~\cite{Achlioptas18b} are used to sample in the RNN's latent space. Like the previous baselines, \PQNET{} relies on voxelized data and often reconstructs "inflated" shapes. While this helps against thinner parts, such as mixer helices and chair legs, the overall reconstruction accuracy remains low. Additionally, backpropagating through RNNs is notoriously difficult~\cite{Pascanu13}, and the non-continuous nature of its sequence generation makes it less suitable as a shape prior for optimization tasks. We train \PQNET{} using the author's official implementation\footnote{\url{https://github.com/ChrisWu1997/PQ-NET}}.

\paragraph{\VecSet.} In order to evaluate the reconstruction and generation quality of our shapes against a more powerful baseline, we also compare to a state-of-the-art part-agnostic method, namely \VecSet{}~\cite{Zhang23d}. The core of the architecture is a large auto-encoder transformer~\cite{Vaswani17} model that encodes a point cloud into an \textit{unordered set} of latent vectors, which are then decoded at queried positions to obtain occupancy values. For generation, a KL-divergence loss is added during training to make the auto-encoder variational and help a second-stage diffusion model. This diffusion model is trained to generate \textit{sets} of latent vectors, thus sampling \VecSet{}'s latent space. As noted in the work, the models are very heavy, requiring multiple GPUs for training. Using the author's official implementation\footnote{\url{https://github.com/1zb/3DShape2VecSet}}, we train auto-encoders \textit{without} the KL-divergence loss for reconstruction experiments and \textit{with} the KL-divergence for the generation experiments (as reference, we use the \textit{same} \PSDF{} decoder for both experiments). We use the provided hyper-parameters, except for the batch size that we limit to $256$, from $512$, because of memory constraints.


\subsection{Metrics}
\label{sec:supp-metrics}

\paragraph{Reconstruction.} As we mention in the main text, to assess reconstruction accuracy of the 3D shapes, we use three metrics: For surface accuracy, we use the Chamfer-Distance (CD), which measures the \textit{symmetric} distance between sets of points, $\mathcal{X}=\left\{\bx_i\right\}_{i=1}^{N}$ and $\mathcal{Y}=\left\{\by_i\right\}_{i=1}^{M}$, sampled from the surface of the ground truth and reconstructed surfaces respectively. Their CD is then written
\begin{equation}
	\mathrm{CD}(\mathcal{X}, \mathcal{Y})= \frac1N \sum_{\bx\in\mathcal{X}} \min_{\by\in\mathcal{Y}}\lVert \bx - \by \rVert^2 + \frac1M \sum_{\by\in\mathcal{Y}} \min_{\bx\in\mathcal{X}}\lVert \by - \bx \rVert^2.
\end{equation}
In this work, we use $N=M=30'000$, unless specified otherwise.

To evaluate the volume accuracy, we use Intersection over Union (IoU). It is defined as the ratio of volume between the intersection of the shapes over their union. In practice, we compute it using occupancy values on a grid: We find the smallest bounding box that contains the two shapes, then sample points on a $128^3$ grid. For each point $\bx$, we get the binary occupancy of each shape as $o_{\mathcal{S}_1}(\bx)$ and $o_{\mathcal{S}_2}(\bx)$, where $o_\mathcal{S}(\bx)$ is $1$ if $\bx$ is inside the shape and $0$ otherwise. The IoU is then computed as
\begin{equation}
	\mathrm{IoU}(\mathcal{S}_1, \mathcal{S}_2) = \frac{\sum_\bx o_{\mathcal{S}_1}(\bx) \cdot o_{\mathcal{S}_2}(\bx)}{\sum_\bx \max\left(1, o_{\mathcal{S}_1}(\bx) + o_{\mathcal{S}_2}(\bx)\right)}.
\end{equation}

Finally, we use Image Consistency (IC)~\cite{Guillard22b} to evaluate the appearance and surface normals, using renderings of the shapes. Let $\mathcal{K}$ be a set of 8 cameras located at the vertices of a cuboid that encompasses the shape, looking at its centroid. For each camera $k\in\mathcal{K}$, we render the binary silhouettes $S_k\in\{0, 1\}^{256\times256}$ and normal map $N_k\in\real^{256\times256\times3}$ of shape $\mathcal{S}_1$, and similarly $\tilde{S}_k$ and $\tilde{N}_k$ for shape $\mathcal{S}_2$. The IC between these shapes is defined as
\begin{equation}
	\mathrm{IC}(\mathcal{S}_1, \mathcal{S}_2) = \frac{1}{\lvert K\rvert} \sum_{k\in K} \mathrm{IoU}_{\mathrm{2D}}(S_k, \tilde{S}_k) * \mathrm{COS}(N_k, \tilde{N}_k),
\end{equation}
where $\mathrm{IoU}_{\mathrm{2D}}$ is the Intersection over Union between binary images and $\mathrm{COS}$ is the average cosine similarity between two normal maps. We refer interested readers to~\cite{Guillard22b} for more details.

We additionally report per-part average IoU (pIoU) as a way to measure part reconstruction accuracy, noting that CD and IC might have issues if the ground-truth parts are not watertight. In order to compute the occupancy of an open part, we rely on a strategy similar to our SDF supervision, see \cref{fig:method-train}. First, the occupancy of the full shape is computed on the grid. Then, for each point where the occupancy is $1$, we look for the closest part to it. This part will get an occupancy of $1$ at that point and all the others will get $0$. Therefore, part occupancy can be seen as the intersection of the full shape's occupancy with the part's "closest region" as visualized in \cref{fig:method-train}(b).
Note that for baselines without part correspondences, \ie{} \DAENET{} and \PQNET{}, we must first match the reconstructed parts to the ground truth (GT) ones: For each shape and each reconstructed part, we match it with the GT part with which it has the highest IoU.


\paragraph{Generation.} To evaluate the plausibility and diversity of the generated shapes, as compared to a held out test set, we report the Minimum Matching Distance (MMD) and Coverage Score (COV)~\cite{Achlioptas18b} respectively, using the Chamfer-Distance as the (pseudo-)distance metric.

MMD is the average distance between each shape from the test set $T$ and the generated set $G$. It is written as
\begin{equation}
	\mathrm{MMD}(G,T) = \frac{1}{|T|}\sum_{\mathcal{X}\in T} \min_{\mathcal{Y}\in G} \mathrm{CD}(\mathcal{X}, \mathcal{Y}),
\end{equation}
where we make the abuse of notation that $\mathrm{CD}(\mathcal{X}, \mathcal{Y})$ means the Chamfer Distance between samples on the surfaces of $\mathcal{X}$ and $\mathcal{Y}$.

On the other hand, COV measures the fraction of shapes in the test set that are recovered with a greedy matching of generated shapes: each shape in G is matched with the closest shape in T. Rigorously, it is computed as
\begin{equation}
	\mathrm{COV}(G,T)=\frac{\lvert\{ \arg\min_{\mathcal{X}\in T} \mathrm{CD}(\mathcal{X}, \mathcal{Y}), \forall \mathcal{Y}\in G \}\rvert}{\lvert T \rvert}.
\end{equation}
For both the baseline and our method, we generate $2000$ shapes, and use $2048$ surface samples to compute the CD, for efficiency concern.


%------------------------------------------------------------------------
%\section{Comparison to State-of-the-Art}
%\label{sec:supp-sota}
%
%\TODO{keep?}
%\NT{Explain in more depth differences with SOTA to justify our approach?}


% Moved earlier for formatting
\input{fig/supp/optim}

%------------------------------------------------------------------------
\section{Part Optimization}
\label{sec:supp-optim}

As described in \cref{sec:exp-optim}, we optimize shapes to demonstrate how \PSDF{} is used as a part-aware shape prior. In this section, we provide further experimental details and results.


\paragraph{Experimental Setup.}

The goal of this experiment is to optimize the shape of a car to minimize its aerodynamic drag, expressed as the drag coefficient ($C_d$), by modifying its main body under the constraints that wheels cannot be modified. Direct simulation of aerodynamic performance, such as Computational Fluid Dynamics (CFD)~\cite{OpenFoam}, is computationally expensive and non-differentiable without adjoint solvers~\cite{Allaire15}. To address this, we use a surrogate model to approximate drag-related metrics efficiently and differentiably~\cite{Baque18, Remelli20b}.

We use a graph convolutional neural network (GCNN), specifically with GraphSage layers~\cite{Hamilton17}, to predict the surface pressure distribution of a car. The predicted surface pressure is integrated to compute the drag force (\cref{eq:p-drag}), which is then normalized by the mass density of the fluid, its velocity squared, and the frontal surface area of the shape to obtain $C_d$. We simulate the cars in our dataset, following the well-established setup of the DrivAer model~\cite{Wieser14}, to compute the corresponding surface pressures and $C_d$ values. The dataset thus created is used to trained  the GCNN-based surrogate model.

Following this, all model parameters, those of \PSDF{} and the surrogate's, are frozen for the shape optimization. It is performed by adjusting the latents $\bZ$ and/or poses $\bP$ of the parts, starting from the initial parametrization of an existing car. In this experiment, the wheels are kept fixed and only the body is optimized, which is only possible because of the composite shape representation of our model that also ensures coherence between the parts.

The optimization minimizes the integrated pressure drag predicted by the surrogate model, backpropagating through the meshing to adjust the part parameters (\cref{eq:optim}). Regularization terms $\mathcal{L}_\text{reg}$ are applied to maintain consistency in the latent space and prevent excessive deviations: 
\begin{itemize} 
	\item A $k$-nearest neighbors (kNN) loss on the latent vectors to ensure they remain close to the training distribution, as introduced in~\cite{Remelli20b}, and
	\item an $L_2$ regularization on the deviation of latent vectors and poses from their initial values, \eg, $\lVert\bZ - \bZ_{init} \rVert_2^2$.
\end{itemize}
To summarize, during a single iteration of this optimization process:
\begin{enumerate}
	\item Current part latents and poses $(\bZ, \bP)$ are used to compute the car's SDF on a grid, as required by the Marching Cubes algorithm.
	\item Marching Cubes is applied on the SDF grid to obtain a mesh of the car surface $\mathcal{S} = \text{MC}(f_\theta, \bZ, \bP)$.
	\item The mesh is passed through the GCNN surrogate to obtain the surface pressure $\hat{p}(\mathcal{S})$, from which the drag coefficient $C_d$ is computed.
	\item The gradient of $C_d + \mathcal{L}_\text{reg}$ is computed with respect to $(\bZ, \bP)$, which are then updated through stochastic gradient descent.
\end{enumerate}

This setup allows us to efficiently optimize composite shapes for aerodynamic performance while leveraging the flexibility of \PSDF{}'s framework.


\paragraph{Results.}

With this experimental setup, we optimize multiple cars from our dataset and simulate the resulting shapes in OpenFOAM~\cite{OpenFoam} to obtain their final drag coefficient $C_d$. Because simulations are expensive ($>10$h depending on the meshing resolution of the 3D space), we must limit ourselves to a subset of 35 cars. The average drag coefficients before and after optimizations are

\vspace{2mm}
\parbox{\linewidth}{
	\centering
	\begin{tabular}{cc}
		\toprule
		Before & After \\
		\midrule
		0.378 & 0.325 \\
		\bottomrule
	\end{tabular}
}
\vspace{1mm}

with an average relative improvement of 12.7\%. We visualize some optimization results in \cref{fig:supp-optim}. As can be observed, only the body of the car has been modified while the wheels are unchanged. Nonetheless, they stay consistent with each other and no inter-penetration happens.

%\paragraph{$C_d$ for cars in \cref{fig:optim}.}
%The drag coefficients $C_d$ of the optimized cars displayed in \cref{fig:optim} of the main text are, in the same order as in the figure:
%\begin{itemize}
%	\item $C_d=0.355\rightarrow C_d=0.323$
%	\item $C_d=0.520\rightarrow C_d=0.397$
%	\item $C_d=0.473\rightarrow C_d=0.391$,
%\end{itemize}
%as obtained through CFD simulation~\cite{OpenFoam}. \NT{To remove once added to the figure.}


%------------------------------------------------------------------------

\section{Part Interpolation}
\label{sec:supp-interp}

\input{fig/supp/interp}

To demonstrate the smoothness and consistency of our latent and pose space, we perform an interpolation experiment. Given pair of shapes, we linearly interpolate between their part latent vectors and poses, with the exception of rotation quaternions for which we use spherical linear interpolation (slerp). At multiple interpolation steps, we reconstruct the intermediate shapes using our decoder and visualize such interpolation "path" in \cref{fig:supp-interp}, illustrating smooth transitions between the pair of shapes. Notably, the part-based structure remains coherent throughout the interpolation. This highlights the effectiveness of our learned latent and pose spaces in representing composite shapes.


%-------------------------------------------------------------------------
\section{Ablation Study} 
\label{sec:exp-ablation}
\input{table/ablation}

\input{fig/ablation}

We conduct an ablation study to evaluate the impact of convolutional layers, the use of pose parameters, and latent modulation in \PSDF{}, and report metrics on the Car dataset in \cref{tab:ablation}.
Pose parameters allow independent control of part transformations, which improves the shape reconstruction performance but also allows more precise part manipulation, while the addition of latent modulation is a simple modification that improves surface and part reconstruction, as measured by Chamfer-Distance and part IoU. On the other hand, convolutional layers cause a slight decrease in part accuracy, largely counterbalanced by the manipulation capability it enables. In \cref{fig:ablation}, we show that adding these layers to \PSDF{} maintains the consistency between parts when doing part manipulation. Additionally, we show in \cref{fig:ablation} the necessity of the non-intersection loss $\mathcal{L}_\text{inter}$: without it, the reconstruction losses do not prevent the parts from bleeding into one another, causing the model to learn an incorrect part decomposition with overlapping parts.

%The numerical and visual results, presented in \cref{tab:ablation} and \cref{fig:ablation}, prove the importance of these components in the design of \PSDF{}.

%%% Isn't very visible on cars, should have done it on chairs that have touching parts.
%\subsection{Ablation Studies}
%\label{sec:supp-abla}
%\NT{For non-inter loss only?}


%------------------------------------------------------------------------
\section{Additional Experimental Results}
\label{sec:supp-results}

In this section, we provide additional results and visualization on shape reconstruction (\cref{sec:supp-recon}), generation (\cref{sec:supp-gen}) and manipulation (\cref{sec:supp-manip}).

%%% No time for the following...
%\subsection{Image Encoder}
%\label{sec:supp-img}


\subsection{Shape Reconstruction}
\label{sec:supp-recon}

We provide additional examples of shape reconstruction on the test set in \cref{fig:supp-recon}. We also visualize the part poses as primitives and results using the point encoder. \DAENET{} and \BAENET{} struggle to recover thin parts and complex shapes, while \PQNET{} tend to inflates them without details. On the opposite, \PSDF{} recovers accurately the shapes and the parts. We note that with a point cloud encoder, the results may not be as accurate, \eg, the last chair where the armrest segmentation is different, though it is still sensible.

\input{fig/supp/recon}


\subsection{Shape Generation}
\label{sec:supp-gen}

In \cref{fig:supp-gen}, we visualize generation examples with \PQNET{} and \PSDF{}. We also show the part latent generation conditioned on poses for our model. The generated shapes with our method in combination of SALAD~\cite{Koo23} are more detailed than \PQNET{} + a latentGAN, with greater variety for cars and mixers (as reported in \cref{tab:gen}).

\input{fig/supp/gen}

\subsection{Shape Manipulation}
\label{sec:supp-manip}

Finally, we perform more manipulation in \cref{fig:supp-manip} with \PSDF{} on all dataset. By changing part latents, the affected parts have modified appearances but adapt to the original pose and to each others. For example, the car bodies have the same general scales but their wheel wells fit correctly the original wheels, the mixers helices fit the original tube, and the chairs keep the same overall structure while each parts are locally different. When editing their poses, the parts are moved and deformed, but also preserve their original features, \eg, the cars keep their original shapes but adapt to their new wheels, the mixers and their helices are rescaled but conserve their inside/outside relationship, and the size and height of chairs can be changed while maintaining the chairs' features.
Note that this requires the part poses to be correct and coherent. Indeed, if they are manually set to be out of contact and disorganized in 3D space, the resulting shape will be meaningless.

\input{fig/supp/manip}


%------------------------------------------------------------------------
% OLD
%------------------------------------------------------------------------
\iffalse

\section{Rationale}
\label{sec:rationale}
% 
Having the supplementary compiled together with the main paper means that:
% 
\begin{itemize}
	\item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
	\item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
	\item When submitted to arXiv, the supplementary will already included at the end of the paper.
\end{itemize}
% 
To split the supplementary pages from the main paper, you can use \href{https://support.apple.com/en-ca/guide/preview/prvw11793/mac#:~:text=Delete%20a%20page%20from%20a,or%20choose%20Edit%20%3E%20Delete).}{Preview (on macOS)}, \href{https://www.adobe.com/acrobat/how-to/delete-pages-from-pdf.html#:~:text=Choose%20%E2%80%9CTools%E2%80%9D%20%3E%20%E2%80%9COrganize,or%20pages%20from%20the%20file.}{Adobe Acrobat} (on all OSs), as well as \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.



What to include in this supplementary: (structure may change)
\begin{itemize}
	\item \textbf{Table of Contents.} Short introduction listing the sections.
	\item \textbf{Experimental details.} Explanation in greater details of the experiments. Should have hyper-parameters, data processing, metrics, baselines, etc. Details about encoder/diffusion.
	\item \textbf{Additional results.} Additional visualization for reconstruction, generation, and manipulation. \textit{Ablation.} Also ablate some losses like non-intersection.
	\item \textbf{Shape optimization.} Additional details (possibly), if can make work, additional results. Some simulation results.
	\item \textbf{Latent interpolation.} Visualize what happens when we interpolate latents, and/or latents+poses.
	\item \textbf{Image encoder.} Like for HybridSDF, train a small image encode (ResNet18) on rendering of cars and chairs (look into Choy et al. for renderings?).
	\item \textbf{Difference to SOTA.} Further details differences with SOTA methods?
\end{itemize}

They could be organized as follow. \\
\textbf{Table of Contents:}
\begin{enumerate}
	\item Experimental Details \begin{enumerate}
		\item Network architecture
		\item Training procedure
		\item Encoder and diffusion models
		\item Datasets
		\item Baselines
		\item Metrics
	\end{enumerate}
	\item \textit{Difference to SOTA?}
	\item Parts interpolation (latents and/or poses)
	\item \textit{Ablation studies}
	\item Shape optimization \begin{enumerate}
		\item Experimental details
		\item Quantitative results
		\item Additional qualitative results
	\end{enumerate}
	\item Image Encoder
	\item Additional Experimental Results \begin{enumerate}
		\item Reconstruction
		\item Generation
		\item Manipulation
	\end{enumerate}
\end{enumerate}

\fi