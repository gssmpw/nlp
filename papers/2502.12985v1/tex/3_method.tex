% !TEX root = ../top.tex
% !TEX spellcheck = en-US

\section{Method}
\label{sec:method}


% Present pipeline here
%Pipeline: part latents go to decoder model with position. Poses affect position. SDF are combined. Can have Encoder or whatever model before to generate/predict part latents and poses (encode, decode, or predict).

We propose \PSDF{}, a modular, part-based representation for composite shapes, designed for flexibility across applications that require both structured part manipulation and optimization. Given a dataset of composite shapes with known part decompositions, our model learns to represent each one independently within a compact latent space, while being able to maintain consistency when deforming them.  

As shown in \cref{fig:method}, our framework consists of two primary components.
First, the core of our model is a \textit{composite shape decoder} that outputs a signed distance function (SDF) for each part, parameterized by a latent vector and a pose (\cref{sec:method-dec}). It is trained in an auto-decoding fashion~\cite{Park19c} in which the latent vectors and the decoder's weights are learned simultaneously, with supervision applied at both the global and part levels (\cref{sec:method-train}). 
Second, our architecture supports secondary models for \textit{part encoding or generation}, allowing further adaptation of part latents and poses for tasks such as shape reconstruction or generation, while maintaining the same core part decoder for efficient inference and manipulation (\cref{sec:method-enc}).


%-------------------------------------------------------------------------
\subsection{Composite Shape Decoder}
\label{sec:method-dec}

In our composite shape representation, shown in \cref{fig:method}(a), each part is described by a latent vector $\bz_p\in\real^Z$ and a pose $\bp_p\in\real^{10}$, consisting of a rotation quaternion~$\bq_p\in\real^4$, translation~$\bt_p\in\real^3$, and scale~$\bs_p\in\real^3$. Given a query point $\mathbf{x} \in \mathbb{R}^3$, each part’s pose maps the query point into the part’s canonical space via inverse transformation $\mathcal{T}^{-1}$, resulting in the transformed query point $\hat{\bx}_p = \mathcal{T}^{-1}(\bx, \bp_p), \,\forall p$. The decoder $f$ then outputs the SDF for each part at this transformed point 
%
\begin{equation} 
	\hat{s}_p = f(\bz_p, \hat{\bx}_p).
\end{equation}
%
The SDFs for all parts can then be combined to recover the full shape representation as $\hat{s} = \min_p{\hat{s}_p}$.

In \PSDF{}, the decoder $f$ is implemented as a multi-layer perceptron (MLP) with modifications to support part-based representation. First, we perform \textit{latent modulation}~\cite{Dupont22} by injecting the part latent vectors $\bz_p$ at every fully-connected layer of the MLP. Then, we introduce a per-part learnable bias into these layers to help differentiate the individual parts. 
%This can be seen as a simplified FiLM~\cite{Perez18} or AdaIN~\cite{Huang17e} per-part conditioning, without input normalization nor scaling factor to reduce computational complexity. 
We formalize these layers $h_{\mathrm{fc}}$ as
%
\begin{equation}
	\bx_p^{l+1} = h_{\mathrm{fc}}^l(\bz_p, \bx_p^l) = \sigma\left( \mathbf{W}^l \bx_p^l + \mathbf{b}^l + \mathbf{W}_z^l \bz_p + \mathbf{b}_p^l \right) \; ,
\end{equation}
%
where $\sigma$ is an activation function, $\bx_p^l$ denotes the features of part $p$ at layer $l$, $(\mathbf{W}^l, \mathbf{b}^l)$ are the layer's parameters, $\mathbf{W}_z^l $ is the parameters of the \textit{latent modulation}, and $\mathbf{b}_p^l$ the learnable bias of part $p$ at layer $l$. 

This creates a part-specific MLP structure with extensive weight sharing across parts. However, it processes each part independently, which is not desirable if they are to adapt to each others when the composite shape is manipulated.
To remedy this, we incorporate 1D convolutions with a kernel size of 1 between each $h_{\mathrm{fc}}$ layer. These convolutions operate across parts, treating them as the "channel" dimension, with one filter $\bw_p$ per part. \nt{Hence, for each feature dimension $d$:
%
\begin{equation}
	{\bx}_{p}^{l+1}[d] = \sigma\left( \sum_{i=1}^P \bw_p^l[i] \cdot {\bx}_{i}^l[d] + b_p^l \right), \forall p, d \; .
\end{equation}
% 
}Conceptually, they function as the $h_{\mathrm{fc}}$ layers, but on another input dimension: They process all parts for each feature individually (see \cref{sec:supp-net} for a visualization). This enables the decoder to capture inter-part dependencies and improve adaptability during part manipulation, as shown in our ablation study (\cref{sec:exp-ablation}). Our final model $f_\theta$, with $\theta$ containing all trainable parameters, operates on all parts. We write
%
\begin{equation}
	\hat{\bs} = f_\theta\left( \bZ, \hat{\bX}  \right) \; ,
\end{equation}
%
where $\hat{\bs}$, $\bZ$, and $\hat{\bX}$ are all parts' SDF, latents, and transformed query points respectively.

%-------------------------------------------------------------------------
\subsection{Training}
\label{sec:method-train}

\input{fig/method_train}

Let us consider a dataset $\mathcal{D}$ where each shape $\mathcal{S}$ is decomposed in up to $P$ parts, given as the segmentation of its surface. To establish initial poses for each part, we fit simple primitives---cuboids or cylinders---to the segmented parts and use these primitives’ poses as the part poses. Therefore, each element of the dataset becomes a tuple $\mathcal{D}=\left\{\left( \mathcal{S}, \left\{\mathcal{P}_p\right\}, \left\{\bp_p\right\} \right)_i\right\}$  of the shape, its parts, and their poses, where we write $\bp_p=\left( \bq_p, \bt_p, \bs_p \right)$.

\parag{Training the Decoder.}

For watertight shapes $\mathcal{S}$, \PSDF{}'s decoder is then trained in an auto-decoding fashion~\cite{Park19c}, where model parameters $\theta$ and part latent vectors $\bz_p$ are optimized jointly. Note that we can directly use the parts poses from $\mathcal{D}$ during training.
When a part is missing from a specific shape, we assign it the average pose of that part over the dataset, allowing the model to naturally learn to output $\text{SDF} > 0$ for nonexistent parts, without having to predict an \textit{existence score}, as in ~\cite{Petrov23}.

Some parts $\mathcal{P}_p$, however, may \textit{not} be watertight as they are a subset of $\mathcal{S}$'s surface that may not be closed. Rather than computing an approximate SDF or editing the parts to make them watertight~\cite{Wu20c, Petrov23}, we adopt a different supervision strategy. Each part is supervised using the full shape's SDF, but only in the region of space where each 3D point is closest to this part and not the other. Formally, a part $\mathcal{P}_p$ is supervised in $\left\{ \bx\in\real^3 \, \vert \, p=\arg\min_i d_i(\bx) \right\}$, where $d_i(\bx)$ is the projection distance of $\bx$ to part $i$, as depicted by \cref{fig:method-train}. To further prevent overlaps, we incorporate a non-intersection loss that maintains spatial separation between parts.

\parag{Loss Function.}

For any shape from $\mathcal{D}$, we minimize 
%
\begin{equation}
	\mathcal{L} = \mathcal{L}_\text{sdf} + \mathcal{L}_\text{part} + \mathcal{L}_\text{inter} + \lambda \sum_p \lVert \bz_p \rVert^2 \; ,
\end{equation}
%
where $\lambda$ is a hyperparameter controlling the latent $L2$-regularization and the loss terms are defined below.

For the whole shape, we minimize
\begin{equation}
	\mathcal{L}_\text{sdf} = \frac{1}{\lvert \mathcal{X} \rvert} \sum_{(\bx_i, s_i) \in \mathcal{X}} \lvert \hat{s} - s_i \rvert \; ,
\end{equation}
%
where $\mathcal{X}$ is a set of sampled points in 3D and around $\mathcal{S}$, with their ground truth SDF $s_i$ from the full shape $\mathcal{S}$. $\hat{s}$ is the predicted SDF at those points. For notational simplicity,  we omit the dependency on $\theta$ and the $\bz_p$ along with the clamping of SDF values~\cite{Park19c}. 

For individual parts, we minimize 
%
\begin{equation}
	\mathcal{L}_\text{part}= \frac{1}{\lvert \mathcal{X} \rvert} \sum_{(\bx_i, s_i, p_i) \in \mathcal{X}} \lvert \hat{s}_{p_i} - s_i \rvert,
\end{equation}
%
with an additional index $p_i$ of the part closest to $\bx_i$, re-using $\mathcal{X}$ as a slight abuse of notation to explicit that we compute these losses on the same 3D samples. $\hat{s}_{p_i}$ is the predicted part SDF at those points. 

Finally, we introduce a non-intersection loss that pushes the SDF of parts to be positive at each position $\bx_i$ where at least two parts have SDF$<0$. Using $\hat{\mathcal{X}}$ as the subset of such 3D points, the loss is written
%
\begin{equation}
	\label{eq:noninter}
	\mathcal{L}_\text{inter}= \frac{1}{\lvert \hat{\mathcal{X}} \rvert} \sum_{\bx_i \in \hat{\mathcal{X}}} \left\lvert \bw_i \cdot \hat{\bs}_i \right\rvert,
\end{equation}
%
where $\hat{\bs}_i$ is the vector of predicted part SDF at point $\bx_i$ and
$\bw_i = \text{softmax}\left(\tilde{\bs}_i\right),$
defining $\tilde{\bs}_i$ as $\hat{\bs}_i$ with all positive values replaced by $-\infty$. These weights pushes predicted SDFs that are closer to 0, more strongly towards $>0$.


%-------------------------------------------------------------------------
\subsection{Part Encoding and Generation}
\label{sec:method-enc}

Once \PSDF{}'s decoder is trained, it is frozen to be used for downstream tasks, optionally in conjunction with separate specialized networks. While this requires training two networks independently, it makes the training of each one easier. Furthermore, the decoder need only been trained once and used again and again for the different tasks. 

% Doing this in a separate stage, instead of training them end-to-end serves two purposes. First, while this requires two trainings, they are easier to develop as their respective hyper-parameters can be tuned independently, \eg, as the encoder's training speed can significantly affect the decoder's performances. Second, it allows to reuse the same decoder for various tasks, for example, generating a new shape, manually editing it, then refining it through optimization, all within the same model. \NT{will try to show this in Teaser, then add ref here.}

\paragraph{Encoding}
To reconstruct a shape from a given modality, \eg, point clouds or images, encoders can be trained to directly predict part latents and poses, to then be decoded with \PSDF{}: with input data $\mathcal{I}$ corresponding to the shape of our training data $\mathcal{D}$, encoders are trained to map this new modality to the part latents and poses of our pre-trained decoder.
%
As an example, we show how to reconstruct unseen shapes for which part decomposition is unknown in \cref{sec:exp-recon}.
We do this by training a point cloud encoder to predict part latents and poses, which are then refined in an auto-decoding strategy and part-agnostic manner.


\paragraph{Generation}
For shape generation with a coherent part set, generative models can be trained to map random noise to the parts parametrization. Generative Adversarial Networks (GAN)~\cite{Goodfellow14b} have been used to generate shape latents, often dubbed \textit{latentGANs} in related work~\cite{Achlioptas18b, Chen19c} such as \PQNET~\cite{Wu20c}. Recently, diffusion models~\cite{Sohl15, Ho20a} have gained a lot of traction for their generative performances, even in shape~\cite{Zhang23d} or part-based generation~\cite{Koo23}. Generating new shapes with \PSDF{} can then be achieved by training such generative models on the part poses and latents of our pre-trained encoder and, even if not shown here, could be extended to conditional generation on images or texts~\cite{Zhang23d, Koo23}.

% Applications: Summarize how the model supports tasks like shape optimization, reconstruction, and generation, referencing the flexibility of the modular setup.

%----------------------------
% OLD
%----------------------------

\iffalse

\paragraph{Global to local decoder}
Possible to map a global latent vector to the local ones (and poses). This serves as a global prior ontop of the part decoder. \NT{Keep only if I actually make use of this in experiments}

\fi