\section{Related Works}
\label{sec:related}

Robot navigation methods can be broadly categorized into classical, end-to-end learning, and modular learning methods.

\paragraph{Classical Approaches:}
Classical methods for navigation, including Simultaneous Localization and Mapping (SLAM)-based techniques, have been extensively studied over the last three decades \cite{elfes1989using,fennema1990experiments,thrun1999minerva,yamauchi1997frontier}. 
These methods typically build geometric maps of an environment using depth sensors or monocular RGB cameras \cite{thrun2001robust,newcombe2011kinectfusion,davison2007monoslam,sattler2018benchmarking}, while simultaneously localizing the robot relative to the growing map. 
Exploration is often guided by heuristic strategies like frontier-based methods \cite{yamauchi1997frontier,freda2005frontier}, while analytical planners are employed for obstacle avoidance and path planning. 
Although effective for traditional navigation tasks, these classical methods lack the ability to integrate vision-language features, making them insufficient for human-collaboration tasks like vision-language navigation.

\paragraph{End-to-End Learning:}
End-to-end learning methods directly map sensory inputs to navigation actions using deep neural networks, typically trained with imitation learning (IL) or reinforcement learning (RL) losses \cite{mirowski2016learning,savva2017minos,codevilla2018end,singh2023scene,ramrakhya2022habitat}. 
Despite their ability to learn semantic priors for goal-directed exploration, end-to-end approaches suffer from significant limitations. 
These include high computational cost, reliance on large-scale training data, and a lack of interpretability. 
Furthermore, these methods often struggle to generalize beyond simulation due to domain gaps, as evidenced by significant performance drops when transitioning to real world \cite{gervet2023navigating}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.92\linewidth]{images/pipeline_2.pdf} 
    % \vspace{-20pt}
    \captionsetup{
    width=\textwidth,
    font=Smallfont,
    labelfont=Smallfont,
    textfont=Smallfont
    }
    \caption{An overview of the VL-Nav pipeline. VL-Nav processes inputs including prompts, RGB images, odometry poses, and LiDAR scans. The Vision-Language (VL) module conducts open-vocabulary pixel-wise detection to identify areas and objects related to the prompt, generating instance-based target points. Concurrently, the map module performs terrain analysis and manages a dynamic occupancy map. Frontier-based target points are then identified based on this occupancy map, along with the instance points, forming a candidate points pool. VL-Nav employs spatial reasoning to select the most effective goal point from this pool for path planning.}
    \label{fig:pipeline}
    % \vspace{-6pt}
\end{figure*}

\paragraph{Modular Learning:}
Modular learning approaches seek to combine the advantages of classical and end-to-end methods by replacing specific components of the classical pipeline with learned modules. This modularity allows for the integration of semantic priors while maintaining system interpretability and efficiency \cite{mcallister2017concrete,muller2018driving,chaplot2020learning,chaplot2007object}. For example, semantic maps generated from RGB-D data have been used to guide navigation toward specific objects \cite{chaplot2020object,ramakrishnan2020occupancy,ramakrishnan2022poni,hahn2021no}. Modular methods have shown promise in addressing Sim-to-Real transfer challenges by abstracting raw sensor data into higher-level representations, mitigating the impact of domain gaps \cite{muller2018driving, chaplot2007object, chaplot2020learning}. However, existing modular approaches often rely heavily on real-world training data and struggle to utilize vision-language features.

\paragraph{Foundation Models for Navigation:}
Recent advances in foundation models like VLMs and LLMs have further enhanced semantic navigation by incorporating natural language understanding into modular learning approaches \cite{gadre2023cows, yin2025sg, liu2024dynamem, yokoyama2024vlfm}. 
The Vision-Language Frontier Maps (VLFM) method \cite{yokoyama2024vlfm} exemplifies this trend by leveraging a pre-trained VLM to extract semantic values directly from RGB images, enabling zero-shot semantic prior understanding for frontier-based exploration. 
VLFM achieves SOTA performance in the simulated environment, addressing the limitations of prior frontier-based and goal-oriented modular methods \cite{yamauchi1997frontier,freda2005frontier,chaplot2020object,luo2022stubborn}. 
However, VLFM's reliance on computationally intensive VLMs restricts its deployability on resource-constrained platforms, and its focus on a single semantic similarity score for each observation does not adequately account for pixel-wise vision-language understanding and spatial reasoning, which is crucial for VLN. 
Their real-robot demonstration involves mounting a high-power-consumption RTX 4090 gaming laptop on the quadrupedal robot to handle computation-intensive modules, which is an impractical design for mobile-robot applications.



\paragraph{Strengths of VL-Nav:}
VL-Nav significantly advances vision-language navigation (VLN) by addressing the limitations of classical, end-to-end, and modular learning methods. It employs a rolling occupancy grid and partial frontier detection, reducing computational overhead and enhancing adaptability in dynamic environments on low-power platforms. The integration of instance-based target points with spatial reasoning enables pixel-wise vision-language feature interpretation and context-aware goal selection. Unlike systems reliant on heavy computational models, VL-Nav optimizes for resource-constrained platforms and maintains real-time performance at 30 Hz. This makes VL-Nav not only practical for real-world applications but also superior in navigating complex and various environments efficiently.


