\vspace{-5pt}
\section{Introduction}
\label{sec:intro}

\vspace{-5pt}
Efficient navigation following human instructions in unseen environments is critical for autonomous robots, with various applications spanning from home assistance \cite{szot2021habitat,warrier2022autonomous,xu2024naturalvlm} to planetary exploration \cite{se2004vision,yashchenko2024intelligent,surwase2024application}. In such scenarios, robots are tasked with exploring and identifying targets instructed by humans, a challenge we define as \textit{vision-language navigation} (VLN). Suppose a man wearing gray is reported missing and a command ``find a man in gray'' is given, the robot should not engage in random exploration. Instead, it should prioritize directions with visual features showing a stronger correlation to the language cues, such as detecting ``gray cloth'' within its field of view. 
Real-world VLN requires a navigation system that can: (1) interpret pixel-wise vision-language features, (2) adapt and perform robustly across various environments, and (3) operate in real-time on low-power platforms.  



However, there are still no VLN systems that can fully address the three capabilities.
Existing approaches can be broadly categorized into classical, end-to-end learning, and modular learning methods \cite{gervet2023navigating}. 
Classical approaches \cite{elfes1989using,fennema1990experiments,thrun1999minerva,yamauchi1997frontier}, while efficient, struggle to integrate vision-language (VL) features.
End-to-end learning methods \cite{mirowski2016learning,savva2017minos,codevilla2018end,singh2023scene,ramrakhya2022habitat} 
show promise but are computationally expensive, prone to overfitting in simulation, and exhibit poor generalization to out-of-distribution scenarios.
Modular learning approaches \cite{chaplot2020object,ramakrishnan2022poni,hahn2021no,chaplot2021seal,chaplot2020neural}, on the other hand, demonstrate robust real-world performance \cite{gervet2023navigating}. However, they often depend on extensive real-world robot training data and lack the capacity for human-like reasoning.
The advent of vision-language models (VLMs) and large language models (LLMs) has further enhanced modular navigation approaches \cite{gadre2023cows, yin2025sg, liu2024dynamem, yokoyama2024vlfm}. For instance, Vision-Language Frontier Maps (VLFM) \cite{yokoyama2024vlfm} utilized VLMs to extract language-grounded features directly from RGB images. This enables the creation of a semantic map, guiding exploration based on human semantic knowledge.
However, their reliance on computationally intensive models limits their deployability on low-power platforms. Moreover, VLFM relies heavily on a single image-level feature similarity for goal selection, limiting its ability to exploit fine-grained VL cues.

To bridge this gap, we present vision-language navigation (VL-Nav), a novel navigation framework optimized for low-power robots, achieving zero-shot VLN at 30 Hz with an onboard computer. VL-Nav employs the spatial reasoning on both frontier-based and instance-based target points.
VL-Nav initially generates frontier-based target points from the dynamic occupancy map using partial frontier detection. 
It confines the search to a manageable field of view, thereby reducing computational overhead. 
Additionally, instance-based target points are incorporated to emulate human search patterns, enabling the robot to approach and verify potential target objects, thereby enhancing the success rate. 
To select the most informative goal point, we introduce CVL spatial reasoning. 
This technique first transforms pixel-wise vision-language features into a spatial scoring distribution using a Gaussian mixture model. Each target point is then assigned a vision-language semantic score based on this distribution. Subsequently, a curiosity-driven weighting, which encourages the robot to explore unknown areas, is applied to adjust these scores into CVL scores. 
The target point with the highest CVL score is ultimately selected as the goal point. This CVL spatial reasoning process ensures that the selected goal point not only closely aligns with the human description but also guides the robot to explore unknown areas.

Once a goal is selected, VL-Nav utilizes a classic planner for real-time obstacle-aware path planning, enabling seamless adaptation to partially known environments. By combining pixel-wise vision-language features with curiosity-based exploration via the novel CVL spatial reasoning, VL-Nav surpasses all the baselines, achieving intelligent navigation while remaining computationally feasible for real-world deployment.

Our main contributions are as follows:
\begin{itemize}
    \item 
    % Efficient and robust navigation for resource-constrained robots: 
    We propose VL-Nav, an efficient VLN system optimized for low-power robots, achieving robust real-time performance at 30 Hz with an onboard computer.
    \item 
    % Spatially-aware human-like reasoning: 
    We empower VL-Nav with spatial reasoning by integrating pixel-wise vision-language features and curiosity-driven exploration for more efficient VLN..
    \item 
    % Extensive experimental evaluation: 
    We conduct comprehensive evaluations across four real-world settings, showing that VL-Nav outperforms prior methods by 44.15\% in diverse environments.
\end{itemize}
