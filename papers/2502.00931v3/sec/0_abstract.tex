



\begin{abstract}
\label{sec:abstract}
Vision-language navigation in unknown environments is crucial for mobile robots. In scenarios such as household assistance and rescue, mobile robots need to understand a human command, such as ``find a person wearing black''.
We present a novel vision-language navigation (VL-Nav) system that integrates efficient spatial reasoning on low-power robots.
Unlike prior methods that rely on a single image-level feature similarity to guide a robot, our method integrates pixel-wise vision-language features with curiosity-driven exploration. 
This approach enables robust navigation to human-instructed instances across diverse environments.
We deploy VL-Nav on a four-wheel mobile robot and evaluate its performance through comprehensive navigation tasks in both indoor and outdoor environments, spanning different scales and semantic complexities.
Remarkably, VL-Nav operates at a real-time frequency of 30 Hz with a Jetson Orin NX, highlighting its ability to conduct efficient vision-language navigation. 
Results show that VL-Nav achieves an overall success rate of 86.3\%, outperforming previous methods by 44.15\%.
% A detailed video demonstration can be found at \url{https://sairlab.org/vlnav/}, where the source code will also be publicly released.

\end{abstract}

\IEEEpeerreviewmaketitle
