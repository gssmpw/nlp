\section{Limitation and Conclusion}
\label{sec:limitation_conclusion}

\subsection{Limitations}
\label{sec:limitation}
Despite the promising results demonstrated by VL-Nav, several limitations remain. One major challenge is the system's ability to handle complex language descriptions. It struggles with spatial descriptions that contain hidden object references and objects described with specific textual annotations, which can affect navigation accuracy. 
Another limitation lies in the reliance on manually defined thresholds for various navigation conditions like the lightning condition. These thresholds may not generalize well across different environments and scenarios, requiring further investigation into adaptive or learning-based approaches to threshold tuning.

\subsection{Conclusion}
\label{sec:conclusion}
In this paper, we introduced \emph{VL-Nav}, a vision-language navigation framework that operates efficiently in real time on resource-constrained platforms. By combining pixel-wise vision-language features with a curiosity-based exploration strategy, our \emph{CVL} spatial reasoning method demonstrates robust performance in a variety of indoor and outdoor settings. Empirically, VL-Nav not only achieves real-time navigation at 30 Hz on a Jetson Orin NX but also surpasses existing approaches by 44.15\%, achieving an overall success rate of 86.3\%. The key to these gains lies in effectively leveraging semantic cues from vision-language embeddings to prioritize frontiers and potential object instances, thereby aligning navigation decisions more closely with human-like reasoning.

Future work will explore extending VL-Nav to handle more complex instructions that involve multi-step tasks, temporal reasoning (e.g., tracking a moving target), and dynamic environments. Additionally, further integration with large language models could enable more nuanced command parsing and open-vocabulary object detection for even broader applicability. We believe these directions will help advance the capability of VLN systems and bring us closer to deploying versatile, robust robotic assistants in real-world settings.
