
\section{Experiments}
\label{sec:experiments}



\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/Environments.pdf} 
    % \vspace{-20pt}
    \captionsetup{
    width=\textwidth,
    font=Smallfont,
    labelfont=Smallfont,
    textfont=Smallfont
    }
    \captionsetup{
    width=\textwidth,
    font=Smallfont,
    labelfont=Smallfont,
    textfont=Smallfont
    }
    \caption{Four different real-world experiment environments.}
    \label{fig:environments}
    % \vspace{-6pt}
\end{figure*}

\begin{figure*}[t]
    \centering
    \captionsetup{
    width=\textwidth,
    font=Smallfont,
    labelfont=Smallfont,
    textfont=Smallfont
    }
    % Top-left subfigure
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/fig_office.pdf}
        \caption{Office}
        \label{fig:subfig1}
    \end{subfigure}
    \hspace{0.02\textwidth}
    % Top-right subfigure
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/fig_apt.pdf}
        \caption{Apartment}
        \label{fig:subfig2}
    \end{subfigure}

    \vskip\baselineskip

    % Bottom-left subfigure
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/fig_outdoor.pdf}
        \caption{Outdoor}
        \label{fig:subfig3}
    \end{subfigure}
    \hspace{0.02\textwidth}
    % Bottom-right subfigure
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/fig_hallway.pdf}
        \caption{Hallway}
        \label{fig:subfig4}
    \end{subfigure}

    \caption{Top-down view of the trajectories comparison on the value maps with the detection results across the four different environments.}
    \label{fig:value_map}
\end{figure*}


\begin{table*}[ht]
\captionsetup{
    width=\textwidth,
    font=Smallfont,
    labelfont=Smallfont,
    textfont=Smallfont
    }
\caption{Vision-language navigation performance in 4 unseen environments (SR and SPL).}
\label{SOTAResults}
\centering
\begin{tabular}{lcccc|cccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{SR (\%)}} & \multicolumn{4}{c}{\textbf{SPL}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
 & Hallway & Office & Apartment & Outdoor & Hallway & Office & Apartment & Outdoor \\
\midrule
\textbf{Frontier Exploration}  
  & 40.0 & 41.7 & 55.6 & 33.3  
  & 0.239 & 0.317 & 0.363 & 0.189 \\

\textbf{VLFM} \cite{yokoyama2024vlfm}                 
  & 53.3 & 75.0 & 66.7 & 44.4  
  & 0.366 & 0.556 & 0.412 & 0.308 \\

\textbf{VL-Nav w/o IBTP}      
  & 66.7 & 83.3 & \underline{70.2} & \underline{55.6}  
  & 0.593 & 0.738 & 0.615 & \underline{0.573} \\

\textbf{VL-Nav w/o curiosity}      
  & \underline{73.3} & \underline{86.3} & 66.7 & \underline{55.6}  
  & \underline{0.612} & \underline{0.743} & \underline{0.631} & 0.498 \\

\textbf{VL-Nav}               
  & \textbf{86.7} & \textbf{91.7} & \textbf{88.9} & \textbf{77.8}  
  & \textbf{0.672} & \textbf{0.812} & \textbf{0.733} & \textbf{0.637} \\

\bottomrule
\end{tabular}
\end{table*}








\subsection{Experimental Setting}
\label{sec:experimental_setting}

We evaluate our approach in real-robot experiments against five methods: (1) classical frontier-based exploration, (2) VLFM \cite{yokoyama2024vlfm}, (3) VLNav without instance-based target points, (4) VLNav without curiosity terms, and (5) the full VLNav configuration. Because the original VLFM relies on BLIP-2 \cite{li2023blip}, which is too computationally heavy for real-time edge deployment, we use the YOLO-World \cite{cheng2024yolo} model instead to generate per-observation similarity scores for VLFM. Each method is tested under the same conditions to ensure a fair comparison of performance.

\paragraph{Environments:}
We consider four distinct environments (shown in \fref{fig:environments}), each with a specific combination of semantic complexity (\textit{High}, \textit{Medium}, or \textit{Low}) and size (\textit{Big}, \textit{Mid}, or \textit{Small}). Concretely, we use a Hallway (\textit{Medium \& Big}), an Office (\textit{High \& Mid}), an Outdoor area (\textit{Low \& Big}), and an Apartment (\textit{High \& Small}). In each environment, we evaluate five methods using three language prompts, yielding a diverse range of spatial layouts and semantic challenges. This setup provides a rigorous assessment of each method’s adaptability.

\paragraph{Language-Described Instance:}
We define nine distinct, uncommon human-described instances to serve as target objects or persons during navigation. Examples include phrases such as “tall white trash bin,” “there seems to be a man in white,” “find a man in gray,” “there seems to be a black chair,” “tall white board,” and “there seems to be a fold chair.” The variety in these descriptions ensures that the robot must rely on vision-language understanding to accurately locate these targets.

\noindent\textbf{Robots and Sensor Setup:} 
All experiments are conducted using a four-wheel Rover equipped with a Livox Mid-360 LiDAR. The LiDAR is tilted by approximately 23 degrees to the front to achieve a $\pm 30$ degrees vertical FOV coverage closely aligned with the forward camera’s view. An Intel RealSense D455 RGB-D camera, tilted upward by 7 degrees to detect taller objects, provides visual observation, though its depth data are not used for positioning or mapping. LiDAR measurements are a primary source of mapping and localization due to their higher accuracy. The whole VL-Nav system runs on an NVIDIA Jetson Orin NX on-board computer.




\subsection{Main Results}
\label{sec:main_results}

We validate the proposed VL-Nav system in real-robot experiments across four distinct environments (\textit{Hallway}, \textit{Office}, \textit{Apartment}, and \textit{Outdoor}), each featuring different semantic levels and sizes. Building on the motivation articulated in~\sref{sec:intro}, we focus on evaluating VL-Nav’s ability to (1) interpret fine-grained vision-language features and conduct robust VLN, (2) explore efficiently in unfamiliar spaces across various environments, and (3) run in real-time on resource-constrained platforms. \fref{fig:value_map} presents a top-down comparison of trajectories and detection results on the value map.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/result_plot.pdf} 
    % \vspace{-20pt}
    \captionsetup{
    width=\textwidth,
    font=Smallfont,
    labelfont=Smallfont,
    textfont=Smallfont
    }
    \caption{Plots of performance in different environments sizes and semantic comlexities.}
    \label{fig:results}
    % \vspace{-6pt}
\end{figure*}

\paragraph{Overall Performance:}
As reported in Table~\ref{SOTAResults}, our full \textbf{VL-Nav} consistently obtains the highest Success Rate (SR) and Success weighted by Path Length (SPL) across all four environments. In particular, VL-Nav outperforms classical exploration by a large margin, confirming the advantage of integrating CVL spatial reasoning with partial frontier-based search rather than relying solely on geometric exploration.

\paragraph{Effect of Instance-Based Target Points (IBTP):}
We note a marked improvement when enabling IBTP: the variant without IBTP lags behind, particularly in complex domains like the \textit{Apartment} and \textit{Office}. As discussed in \sref{sec:method}, IBTP allows VL-Nav to pursue and verify tentative detections with confidence above a threshold, mirroring human search behavior. This pragmatic mechanism prevents ignoring possible matches to the target description and reduces overall travel distance to confirm or discard candidate objects.

\paragraph{Curiosity Contributions:}
The \emph{curiosity Score} is also significant to VL-Nav’s performance. It merges two key components:
\begin{itemize}
    \item \textbf{Distance Weighting}: Preventing easily select very far way goals to reduce travel time and energy consumption which is extremely important for the efficiency (metrics SPL) in the large-size environments.
    \item \textbf{Unknown-Area Weighting}: Rewards navigation toward regions that yield more information.
\end{itemize}
Our ablations reveal that removing the distance-scoring element (\textit{VL-Nav w/o curiosity}) degrades both SR and SPL, particularly in the more cluttered environments. Meanwhile, dropping the instance-based target points (IBTP) similarly lowers performance, reflecting how each piece of CVL addresses a complementary aspect of semantic navigation.

\paragraph{Comparison to VLFM:}
Although the VLFM approach \cite{yokoyama2024vlfm} harnesses vision-language similarity value, it lacks the pixel-wise vision-language features, instance-based target points verification mechanism, and CVL-based spatial reasoning. Consequently, VL-Nav surpasses VLFM in both SR and SPL by effectively combining the pixel-wise vision language features and the curiosity cues via the CVL spatial reasoning. These gains are especially pronounced in semantic complex (\textit{Apartment}) and open-area (\textit{Outdoor}) environments, underscoring how our CVL spatial reasoning enhance vision-language navigation in complex settings and scenarios.



\paragraph{Summary of Findings:}
In conclusion, the experimental results confirm that VL-Nav delivers superior vision-language navigation across diverse, unseen real-world environments. By fusing frontier-based target points detection, instance-based target points, and the CVL spatial reasoning for goal selection, VL-Nav balances semantic awareness and exploration efficiency. The system’s robust performance, even in large or cluttered domains, highlights its potential as a practical solution for zero-shot vision-language navigation on low-power robots.
