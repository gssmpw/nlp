
\documentclass[conference]{IEEEtran}

% 1) Enable \thanks footnotes in conference mode
\IEEEoverridecommandlockouts
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true,hidelinks,colorlinks=true,urlcolor=blue,citecolor=blue,allcolors=blue]{hyperref}
\usepackage{caption}
% \usepackage{cuted}

% Import additional packages in the preamble file, before hyperref
\input{preamble}

\pdfinfo{
   /Author (Yi Du, Taimeng Fu, Zhuoqun Chen, Bowen Li, Shaoshu Su, Zhipeng Zhao, Chen Wang)
   /Title  (VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning)
   /CreationDate (D:202501310000)
   /Subject (Robots)
   /Keywords (Robots; Vision-language Navigation)
}
\DeclareCaptionFont{Smallfont}{\small}

\begin{document}



% paper title
\title{\LARGE VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning}





\author{\authorblockN{Yi Du\textsuperscript{1},
Taimeng Fu\textsuperscript{1},
Zhuoqun Chen\textsuperscript{1}, 
Bowen Li\textsuperscript{2},
Shaoshu Su\textsuperscript{1},
Zhipeng Zhao\textsuperscript{1},
Chen Wang\textsuperscript{1}}

\authorblockA{\textsuperscript{1}Spatial AI \& Robotics Lab, University at Buffalo, Buffalo, NY 14260, USA.\\ 
% Email: {yid@sairlab.org}
}
\authorblockA{\textsuperscript{2}Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA.\\
% Email: {bowenli2@andrew.cmu.edu}
Project Website: \textcolor{blue}{\url{https://sairlab.org/vlnav/}}
}
}






% \maketitle

% \setlength {\marginparwidth }{2cm} 

\twocolumn[{% 
    \renewcommand\twocolumn[1][]{#1}% 
    
    \maketitle 
    
    \begin{center} 
    \centering 
    \includegraphics[width=1\linewidth]{images/main_figure.pdf}
    \captionsetup{
        width=\textwidth,
        font=Smallfont,
        labelfont=Smallfont,
        textfont=Smallfont
    }
    \captionof{figure}{
        We propose VL-Nav, a real-time zero-shot vision-language navigation approach
        with spatial reasoning that integrates pixel-wise vision-language features 
        and curiosity-based exploration for mobile robots. (a) Hallway: The wheeled robot is tasked 
        with ``find a man in gray'' in a hallway. Unlike the classical frontier-based 
        method (red line) and VLFM (green line), VL-Nav (blue line) leverages pixel-wise 
        vision-language (VL) features from the ``gray cloth'' cue for spatial reasoning, 
        selecting the most VL-correlated goal point and successfully locating 
        the missing person. The value map shows that the ``gray cloth'' VL cue 
        prioritizes the right-side area, marked by yellow square points. 
        (b) Apartment: The robot is tasked with ``Go to the tall white trash bin.'' 
        It detected two different-sized white trash bins in bottom camera observation. 
        However, it assigns a higher confidence score (0.98) to the taller bin than 
        the shorter one (0.48). These pixel-wise VL features are incorporated into 
        the spatial distribution to select the correct goal point, guiding the robot 
        toward the taller bin.
    }
    \label{fig:main_figure}
    \end{center}% 
}]


\input{sec/0_abstract}
\input{sec/1_intro}
\input{sec/2_related}
\input{sec/3_method}
\input{sec/4_evaluation}
\input{sec/5_discussion}
\input{sec/6_conclusion}

% \input{sec/X_suppl}

\section*{Acknowledgments}
This work was partially supported by the Sony Faculty Innovation Award and the DARPA grant DARPA-PS-23-13. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of DARPA.


%% Use plainnat to work nicely with natbib. 
{
\small
\bibliographystyle{plainnat}
\bibliography{references}
}
\end{document}


