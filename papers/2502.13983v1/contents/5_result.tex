\section{Results}

We conducted experiments to evaluate the performance of ASR models in generating initial transcripts. These experiments are essential for assessing the quality of the ASR output, which serves as the input for our gesture-aware contextual rewriting model. Additionally, we performed a case study on selected samples to highlight the improvements made by our proposed approach compared to the initial ASR outputs.



\subsection{Speech Recognition Models}

\begin{table}[t]
\centering
\caption{The Average Word Error Rate (WER) between original transcript and ASR results.}\label{tab:wer}
\resizebox{0.7\linewidth}{!}{%
\begin{tabular}{@{}cc@{}}
\toprule
\textbf{Model}                                                         & WER                       \\ \midrule
\textbf{Whisper~\cite{radford2023robust}}        & 0.557                     \\
\textbf{Seamless~\cite{barrault2023seamlessm4t}} & 0.891                     \\
\textbf{Wave2Vec~\cite{baevski2020wav2vec}}      & 0.630                     \\
\textbf{Whisper (conf $>$ 0.2) }                   & 0.519 \\ \bottomrule
\end{tabular}%
}
\end{table}

The foundation of a successful gesture-aware ASR system lies in obtaining high-quality initial transcripts from the speech recognition stage. To this end, we compared state-of-the-art ASR models on the AphasiaBank dataset to determine their performance in generating accurate transcripts. The results, summarized in Table~\ref{tab:wer}, highlight the average Word Error Rate (WER) for each model, calculated by comparing their output with the ground truth human-annotated transcripts. WER serves as a critical metric for evaluating ASR systems, representing the proportion of errors (insertions, deletions, and substitutions) in the generated transcript relative to the total number of words in the ground truth.

Among the models tested, Whisper~\cite{radford2023robust} demonstrated the best performance, achieving the lowest WER of 0.557. This indicates that Whisper is particularly effective in handling speech from participants with aphasia, despite challenges such as disfluencies, atypical speech patterns, and background noise. Wave2Vec~\cite{baevski2020wav2vec} performed moderately well with a WER of 0.630, while Seamless~\cite{barrault2023seamlessm4t}, designed for multilingual ASR, showed a higher WER of 0.891, indicating potential limitations in its robustness to the specific speech characteristics in this dataset.

To further improve the accuracy of the initial transcripts, we applied a confidence-based filtering approach to Whisper's outputs. By utilizing Whisper’s token-level confidence scores, we excluded tokens with a confidence score below 0.2, thereby retaining only the most reliable portions of the transcript. This refinement reduced Whisper's WER from 0.557 to 0.519, further solidifying its position as the most accurate ASR model in our evaluations.

These results highlight the importance of selecting a robust ASR model for datasets featuring non-standard speech patterns. Whisper, particularly with confidence-based filtering, provides a strong baseline for generating initial transcripts. The reduced WER achieved by Whisper demonstrates its ability to capture key elements of speech even under challenging conditions, thereby enhancing the reliability of the downstream contextual rewriting process.


\subsection{A Case Study}

\begin{table}[t]
\centering
\caption{The result of a case study for comparison between Whisper and the proposed model. \textit{[gesture:]} indicates the type of iconic gestures used by speakers while speaking.}
\label{tab:case}
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Index} & \multicolumn{1}{c}{\textbf{Origianl}}                                                                    & \multicolumn{1}{c}{\textbf{Whisper}}                                   & \multicolumn{1}{c}{\textbf{Ours}}                                                               \\ \midrule
1              & \begin{tabular}[c]{@{}l@{}}w {[}gesture:layering{]} \\ is right.\end{tabular}                            & There's right there.                                                   & \begin{tabular}[c]{@{}l@{}}You put things right there, \\ doing a \textbf{layering} action.\end{tabular} \\
2              & \begin{tabular}[c]{@{}l@{}}{[}gesture:folding{]} uz@u \\ uh right yes.\end{tabular}                      & right.                                                                 & \textbf{folding} it right.                                                                               \\
3              & \begin{tabular}[c]{@{}l@{}}um {[}gesture:cutting{]} \\ {[}gesture:cutting{]} banana.\end{tabular}        & Um... Banana.                                                          & \textbf{cutting} banana.                                                                                 \\
4              & and {[}gesture:eating{]}.                                                                                & and                                                                    & and \textbf{eating}                                                                                      \\
5              & \begin{tabular}[c]{@{}l@{}}but a w knife \\ {[}gesture:spreading{]} \\ do the stuff like um\end{tabular} & \begin{tabular}[c]{@{}l@{}}night do this stuff \\ like oh\end{tabular} & \begin{tabular}[c]{@{}l@{}}do this night stuff \\ like oh \textbf{spreading}\end{tabular}                \\ \bottomrule
\end{tabular}%
}
\end{table}

To evaluate the effectiveness of the proposed multimodal model in capturing and incorporating the latent meaning conveyed by iconic gestures, we conducted a case study comparing its outputs with those of the Whisper ASR model. Table~\ref{tab:case} presents the results, showcasing the original transcripts in the dataset, the transcript generated by Whisper, and the transcript generated by our multimodal approach. The goal of this comparison was to determine whether our model can enhance transcript quality by inferring and integrating the semantic meaning of gestures observed during speech.

As shown in the table, Whisper provides a literal transcription of the spoken words but fails to capture the contextual or semantic information conveyed by the speaker's gestures. For example, in the first example, the original input includes the layering gesture, indicating an action of stacking or placing items on top of each other. Whisper transcribes this as ``\textit{There's right there}," which misses the gesture's implicit meaning. In contrast, our proposed model generates the enriched transcript, ``\textit{You put things right there, doing a layering action},'' effectively incorporating the gesture's semantic context into the spoken narrative.

In the second example, the speaker performs a folding motion, which is entirely absent from Whisper's transcription, resulting in the simple word ``\textit{right}." However, our model identifies and integrates this gesture into the transcript, producing ``\textit{folding it right}," which provides a more comprehensive description of the speaker's intention.

Similarly, in the third and fourth examples, iconic gestures like cutting and eating are completely omitted from Whisper's transcription, leading to outputs that lack essential contextual details. The proposed model enriches these outputs by explicitly including the actions, resulting in transcripts like ``\textit{cutting banana}" and ``\textit{and eating}," which more accurately reflect the speaker's message.

The fifth example highlights a limitation in handling more complex sentences where gestures provide significant contextual cues. While our model attempts to incorporate the spreading gesture into the generated transcript, the result, ``\textit{do this night stuff like oh spreading}," is not fully accurate. This inaccuracy stems from the initial transcription provided by Whisper, which included the word ``\textit{night}," leading our model to generate a sentence misaligned with the speaker's actual intent. This example underscores the importance of having accurate initial transcripts for improving the final output quality, especially in scenarios where subtle contextual nuances are critical.

It is important to note that the proposed model is capable of inferring and incorporating the speaker’s underlying intention more effectively. By leveraging gestures alongside incomplete or disfluent speech, our approach captures the full semantic meaning of the speaker’s communication. For example, when a speaker’s verbal output is fragmented or ambiguous, the model uses the accompanying gestures to infer the intended message, producing transcripts that are both richer and more contextually accurate. Unlike Whisper, which relies solely on the spoken words and often misses crucial gestural context, our method bridges the gap between speech and gesture, offering a more comprehensive representation of the speaker's intent. 
