


\begin{table}[t]
\centering
\caption{The corpora from AphasiaBank}
\vspace{-0.1in}
\label{tab:appendix_corpus}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c}
\hline
Corpus                                                                & Site                                       \\ \hline \hline
ACWT~\cite{bynek2013aphasiabank}                & Aphasia Center of West Texas               \\
Adler~\cite{adler2013aphasia}                   & Adler Aphasia Center                       \\
APROCSA~\cite{aprocsa2021aphasia}               & Vanderbilt University Medical Center       \\
BU~\cite{bu2013apha}                            & Boston University                          \\
Capilouto~\cite{capilouto2008apha}              & University of Kentucky                     \\
CC-Stark~\cite{cc2022apha}                      & file for CC                                \\
CMU~\cite{cmu2013apha}                          & Carnegie Mellon University                 \\
Elman~\cite{elman2011starting,elman2016aphasia} & Aphasia Center of California               \\
Fridriksson~\cite{Fridriksson2013apha}          & University of South Carolina               \\
Garrett~\cite{Garrett2013apha}                  & Pittsburgh, PA                             \\
Kansas~\cite{kansas2013apha}                    & University of Kansas                       \\
Kempler~\cite{kempler2013apha}                  & Emerson College                            \\
Kurland~\cite{kurland2013apha}                  & University of Massachusetts, Amherst       \\
MSU~\cite{MSU2013apha}                          & Montclair State University                 \\
NEURAL~\cite{neural2023apha}                    & NEURAL Research Lab, Indiana University    \\
Richardson~\cite{Richardson2008apha}            & University of New Mexico                   \\
SCALE~\cite{scale2013apha}                      & Snyder Center for Aphasia Life Enhancement \\
STAR~\cite{star2013apha}                        & Stroke Aphasia Recovery Program            \\
TAP~\cite{tap2013apha}                          & Triangle Aphasia Project                   \\
TCU~\cite{tcu2013apha}                          & Texas Christian University                 \\
Thompson~\cite{thompson2013apha}                & Northwestern University                    \\
Tucson~\cite{tucson2013apha}                    & University of Arizona                      \\
UCL~\cite{ucl2021apha}                          & University College London                  \\
UMD~\cite{faroqi2018comparison}                 & University of Maryland                     \\
UNH~\cite{unh2013apha}                          & University of New Hampshire                \\
Whiteside~\cite{whiteside2013apha}              & University of Central Florida              \\
Williamson~\cite{williamson2013apha}            & Stroke Comeback Center                     \\
Wozniak~\cite{wozniak2013apha}                  & InteRACT                                   \\
Wright~\cite{wright2013apha}                    & Arizona State University                   \\ \hline
\end{tabular}%
}
\end{table}

\section{Experiments}
%\subsection{Dataset}
We collected the dataset from AphasiaBank~\cite{macwhinney2011aphasiabank, forbes2012aphasiabank}, a shared database created by clinical experts for aphasia research; the corpus information is summarized in Table~\ref{tab:appendix_corpus}. The dataset includes video recordings of the language evaluation test process between a pathologist and a subject, which also contains human-annotated transcriptions and subjects' demographic information. 


\begin{table}[t]
\centering
\caption{Summary of data statistics of `Peanut Butter Task' based on gesture annotation.}
\label{tab:data_statistics}
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}cccccc@{}}
\toprule
\multirow{2}{*}{label}  & \multirow{2}{*}{\# utt} & \multirow{2}{*}{\# user} & \multicolumn{3}{c}{duration (sec)} \\ \cmidrule(l){4-6} 
                        &                         &                          & mean      & min       & max        \\ \midrule
{[}gesture:cutting{]}   & 109                     & 92                       & 3.76      & 0.21      & 18.17      \\
{[}gesture:eating{]}    & 54                      & 48                       & 2.01      & 0.12      & 20.79      \\
{[}gesture:folding{]}   & 159                     & 145                      & 2.95      & 0.09      & 21.31      \\
{[}gesture:layering{]}  & 45                      & 31                       & 3.92      & 0.39      & 16.99      \\
{[}gesture::opening{]}  & 34                      & 24                       & 4.56      & 0.31      & 16.29      \\
{[}gesture:spreading{]} & 269                     & 169                      & 4.53      & 0.14      & 26.42      \\ \midrule
total                   & 670                     & 288                      & 3.99      & 0.26      & 19.77      \\ \bottomrule
\end{tabular}%
}
\end{table}



For our study, we selected the ``Peanut Butter Sandwich Task'' as the focal activity where people are asked to explain the procedure for making a sandwich. This task has been demonstrated to associate with high rates of iconic gesturing~\cite{stark2022task,stark2023demographic,pritchard2015language,illes1989neurolinguistic}.

We performed a detailed analysis of the dataset to examine the distribution and characteristics of gestures used by participants during this task. Table~\ref{tab:data_statistics} provides a summary of the data, showing six distinct gesture types: cutting, spreading, folding, eating, layering, and opening. Each gesture corresponds to an action performed to describe or demonstrate a step in the sandwich-making process. For example, the cutting gesture involves a back-and-forth hand motion mimicking the act of slicing, while the spreading gesture typically represents the act of spreading condiments with a sweeping motion of the hand. The data highlights the natural integration of gestures with speech, making it a valuable resource for gesture-aware ASR systems.

Among the gestures, spreading was the most frequently used, appearing in 269 utterances across 169 users, with an average duration of 4.53 seconds. This gesture reflects a critical step in the sandwich-making process, as participants commonly describe spreading peanut butter or other ingredients. Following this, folding emerged as the second most frequently observed gesture, with 159 instances across 145 users, averaging 2.95 seconds in duration. The cutting gesture ranked third, appearing 109 times across 92 users, with an average duration of 3.76 seconds. These three gestures collectively account for a significant portion of the dataset, indicating their central role in describing sandwich preparation. These findings align with linguistic analysis of the same story, demonstrating cutting, spreading, and folding as core parts of the story~\cite{dalton2020compendium}.

The dataset also revealed interesting variability in gesture execution. For instance, while spreading gestures were consistently observed, their duration ranged from brief motions lasting just 0.14 seconds to extended actions of up to 26.42 seconds. Similarly, cutting gestures varied in duration, from 0.21 seconds to 18.17 seconds, highlighting differences in participantsâ€™ expressive styles and cognitive processing abilities. Gestures such as layering and opening were less frequently observed, occurring 45 and 34 times respectively, but exhibited average durations comparable to the more common gestures, suggesting that even less frequent gestures are articulated with similar complexity.

This analysis highlights the diversity in both the frequency and execution of gestures among participants, emphasizing the importance of robust recognition systems capable of capturing these variations. The prevalence of gestures like spreading, folding, and cutting also demonstrates their importance as key features for understanding communication in tasks involving procedural explanations. These findings provide a strong foundation for training and evaluating our gesture-aware ASR framework, which seeks to seamlessly integrate gesture information into speech transcription.



%\subsection{Experimental Settings}
%All experiments are conducted on a GeForce RTX 3090 Ti GPU with 24GB of memory. Speech recognition utilizes Whisper~\cite{radford2023robust}, while image recognition employs GPT-4 Vision~\cite{achiam2023gpt}. The rewriting process is powered by GPT-4~\cite{achiam2023gpt}.