\section{Introduction}

Language disorders, such as aphasia, arise from damage to brain regions responsible for language production and comprehension. Aphasia is most often caused by acquired brain injuries, like stroke, and persists chronically in at least 30\%\ of cases~\cite{broca1861remarks,wasay2014stroke}. This means that many are living with aphasia -- indeed, there are nearly two million with aphasia in the USA alone~\cite{simmons2018aphasia}. Individuals with language disorders face significant communication challenges due to difficulties in processing and understanding language, resulting in impaired social interactions and reduced quality of life~\cite{el2017screening}. As voice-assisted technologies like Siri and Alexa become integral to daily activities, the inability of individuals with language disorders to interact effectively with these systems exacerbates their communication barriers, leading to frustration and further marginalization~\cite{rohlfing2021hey}.

Automatic Speech Recognition (ASR) systems are designed to transcribe spoken language into text, serving as a cornerstone for many voice-driven technologies. While recent advancements have made ASR systems more adept at handling disfluencies such as stuttering, their performance heavily depends on clear and consistent audio input~\cite{radford2023robust}. For individuals with language disorders, speech distortions, substitutions, and disjointed delivery pose significant challenges to accurate ASR transcription~\cite{sanguedolce2023uncovering}. As a result, current ASR solutions often fall short in addressing the needs of this population.

To enhance the robustness of ASR, researchers have explored Audio-Visual Speech Recognition (AVSR) systems that combine auditory and visual information~\cite{cheng2023opensr}. These systems leverage visual features such as lip movement~\cite{hu2023hearing,cheng2023mixspeech} and facial expressions~\cite{zadeh2016multimodal,busso2008iemocap} to improve speech recognition accuracy. However, these approaches are not ideal for individuals with language disorders, who often experience concomitant motor speech disorders and facial hemiplegia, which can result in 'masked' facial expressions~\cite{multani2017emotion,duffy2012motor}.

In contrast, individuals with language disorders frequently rely on non-verbal communication, such as gestures, to compensate for their verbal limitations~\cite{stark2023demographic}. Iconic gestures, in particular, serve as powerful tools for conveying meaning when spoken language is insufficient~\cite{de2023does,van2017production,stark2022task,stark2023demographic}. Unlike speech or facial expressions, iconic gestures offer a visual representation of concepts, enabling individuals with language impairments to express ideas more effectively~\cite{lee2023learning}. However, current ASR and AVSR systems fail to consider the latent semantic information encoded in these gestures, leaving a critical gap in understanding the full context of communication for individuals with language disorders.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% method
To address the limitations of current ASR and AVSR systems, we introduce a gesture-aware zero-shot speech recognition framework specifically designed for individuals with language disorders. Our approach harnesses the capabilities of multimodal large language models (LLMs) to incorporate linguistic, acoustic, and gestural information, enabling a deeper understanding of the speaker's intended meaning. Unlike traditional systems that rely solely on spoken or visual cues like lip movements, our method emphasizes the integration of iconic hand gestures—gestures that visually represent concepts—to enrich the transcription process. This allows the system to produce transcripts that not only reflect spoken content but also capture the latent meanings conveyed through gestures, which are crucial for individuals with impaired speech.

The core of our method lies in leveraging a zero-shot framework to align and synthesize multimodal inputs. Our system processes disfluent or incomplete speech signals while simultaneously analyzing visual data to identify and interpret gestures. By fusing these streams of information, the system generates semantically enriched transcripts that bridge the gaps left by speech alone. This zero-shot design eliminates the need for task-specific training, making the approach adaptable to a wide range of scenarios and linguistic contexts. This adaptability is particularly advantageous for language-disordered populations, where speech patterns and gestures vary widely between individuals.

%  result
Our experiments show that our proposed model successfully generates transcripts by incorporating significant gestural information to ascertain the latent intent of individuals with language disorders, which is not conveyed through speech alone. This findings indicate that incorporating non-verbal information can effectively aid in advancing the creation of more inclusive and effective communication technologies designed specifically for the distinctive requirements of individuals with language impairments.








