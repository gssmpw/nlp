\section{Concluding Remarks}
In this paper, we proposed a novel approach utilizing multimodal LLMs to generate gesture-aware speech recognition transcripts for patients with language disorders. Our framework integrates verbal speech and iconic gestures, enabling the generation of enriched transcripts that capture the latent meaning conveyed through both modalities. Through extensive experimentation, we demonstrated that the proposed method effectively contextualizes incomplete or disfluent speech by incorporating gesture information, leading to more accurate and meaningful representations of the speaker's intent. These findings highlight the potential of our approach to significantly contribute to the field of speech and language therapy, offering innovative tools that can enhance the quality of life for individuals with language disorders by facilitating better communication and assessment methods.

\subsection{Ethical Statement} 
Our dataset was obtained from AphasiaBank with the approval of the Institutional Review Board (IRB) and adheres to the data sharing guidelines set by TalkBank\footnote{https://talkbank.org/share/ethics.html}. This includes complying with the Ground Rules for all TalkBank databases, which are based on the American Psychological Association Code of Ethics~\cite{american2002ethical}.

\subsection{Limitation \& Future Work} 
%This study represents a preliminary investigation into using multimodal LLMs to generate gesture-aware speech recognition transcripts. 
While the results are promising, we recognize several limitations and outline our plans to extend this work further.

One primary limitation is the absence of a definitive ground truth for quantitative evaluation. Since our model generates transcripts by synthesizing speech and gesture data from scratch, traditional benchmarks, such as comparisons with standard speech recognition outputs, are insufficient. Moreover, existing original transcripts lack gesture annotations, making direct comparisons challenging. In future work, we aim to address this gap by collaborating with certified pathologists to conduct qualitative assessments, such as A-B preference tests, to evaluate the effectiveness of gesture-enriched transcripts in accurately conveying the speaker's intentions.

To support quantitative evaluations, we plan to develop novel metrics that assess transcript quality, including grammar accuracy, semantic consistency, and the integration of multimodal information. Such metrics will provide a more objective basis for assessing our model's performance and facilitate comparisons with other multimodal and unimodal approaches.

Another limitation of this study is its focus on structured gestures from a specific task, the Peanut Butter Sandwich Task. While this task offers a controlled context for testing our approach, it does not encompass the diversity of gestures and communication patterns seen in everyday scenarios. As part of our future work, we plan to expand the scope of our model to include tasks such as the Cinderella Story Recall Task~\cite{bird1996cinderella}, which involves unstructured and complex narrative gestures. This expansion will allow us to evaluate the adaptability and robustness of our model in handling varied linguistic and gestural contexts.

In summary, while this study establishes a strong foundation for gesture-aware speech recognition, we aim to refine and extend our methods through collaborative qualitative evaluations, the development of robust quantitative metrics, and broader task applications. These efforts will ensure that our approach continues to evolve, ultimately contributing to more effective communication tools and interventions for individuals with language disorders.



