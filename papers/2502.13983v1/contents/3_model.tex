


\section{Method}
Given that the dataset provides video recordings of persons with language disorders, the proposed model aims to improve the transcription of audio from individuals with language disorders, focusing on enhancing contextual understanding, incorporating gesture, and resolving ambiguities in speech. It consists of three interconnected components: Speech Recognition, Gesture Recognition, and Contextual Rewriting. Figure~\ref{fig:framework} provides a schematic overview of the system. 

\subsubsection{Speech Recognition}
The first step in the pipeline is audio processing using an ASR system. This component takes an audio signal $\mathbf{A}$ as input and outputs a preliminary transcript $T_{\text{ASR}}$, which represents the systemâ€™s initial interpretation of spoken words. Let the ASR model be represented by $f_{\text{ASR}}$ as follows,
\begin{equation}
T_{\text{ASR}} = f_{\text{ASR}}(\mathbf{A}), \quad T_{\text{ASR}} = \{ w_1, w_2, \ldots, w_n \}
\end{equation}
where $w_i$ is the $i$-th word in the transcript.
Consider an audio input from a patient saying, ``I um... tomato.'' The ASR model may generate a raw transcript as incomplete speech. This transcript lacks clarity and completeness, as it fails to convey the full meaning intended by the speaker. The limitations arise because ASR systems rely solely on the audio signal and are unable to incorporate accompanying non-verbal cues, such as gestures, that could provide additional context.

\subsubsection{Gesture Recognition}   
The second component focuses on identifying gestures from the video frames corresponding to the speech. The input is a sequence of video frames $\mathbf{V} = \{ v_1, v_2, \ldots, v_m \}$, where each frame $v_i$ captures a snapshot of the speaker's hand or body movements. A gesture recognition model, $f_{\text{Gesture}}$, processes these frames to detect meaningful gestures as follows,
\begin{equation}
\mathbf{G} = f_{\text{Gesture}}(\mathbf{V}), \quad \mathbf{G} = \{ g_1, g_2, \ldots, g_k \}
\end{equation}
where $g_i$ represents a detected gesture and its associated meaning. Due to the scarcity of labeled datasets for iconic gestures, the framework employs a multimodal large language model to perform zero-shot gesture recognition. This approach allows the model to infer the meanings of gestures based on generalizable patterns learned from other modalities. Iconic gestures, which represent concrete referents, are of particular interest because they often clarify or complement the spoken language.

If the video captures the speaker making a back-and-forth motion with a flat hand, the gesture recognition model identifies this as a cutting motion and assigns it the label $g$=``cut''. Iconic gestures, however, can vary significantly between individuals. For instance, one person might perform a ``handling'' gesture by mimicking the act of holding and moving an invisible knife, while another might use an ``enacting'' gesture, simulating the motion of cutting without mimicking the act of holding. The proposed model is designed to handle such variability by recognizing these different representations and mapping them to a unified semantic meaning, ensuring that the gesture's intent is accurately interpreted regardless of individual differences in gesture style.

\subsubsection{Contextual Rewriting}
The third component integrates the outputs from the ASR and gesture recognition models to generate a contextually enriched and semantically accurate transcript. This process is handled by a LLM which takes the initial transcript $T_{\text{ASR}}$, the detected gestures $\mathbf{G}$ as inputs. The final transcript, $T_{\text{Final}}$, is generated as follows:
\begin{equation}
T_{\text{Final}} = f_{\text{LLM}}(T_{\text{ASR}}, \mathbf{G}), \quad T_{\text{Final}} = \{ w'_1, w'_2, \ldots, w'_p \}
\end{equation}
where $w'_i$ represents a word in the final transcript. The model directly uses the recognized gestures as the sole non-verbal input, streamlining the process while focusing on the integration of multimodal signals.

For example, if the ASR transcript reads, "I um... tomato," and the gesture recognition model identifies the label $g$=``cut''
from a cutting motion, the LLM rewrites the transcript as ``I cut tomato.'' This correction ensures that the final output reflects both the spoken and gestural communication, improving the system's ability to understand and represent the speaker's intent accurately.
This approach enables the generation of accurate and enriched transcripts, even in cases where speech alone might be ambiguous or incomplete. By leveraging the co-speech gesture input, the system enhances its interpretive power without requiring additional external context.