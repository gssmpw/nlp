\section{Related Work}

\subsection{ASR as Assistive Technologies}
Assistive technologies play a critical role in supporting clinicians to deliver assessments and therapies, as well as in facilitating communication for individuals with aphasia. In speech-language pathology, such tools are often referred to as augmentative and alternative communication (AAC) systems~\cite{beukelman1998augmentative}. ASR technology has shown promise in enhancing communication by enabling real-time, accurate feedback to individuals with aphasia~\cite{ballard2019feasibility,barbera2021nuva}. This is particularly valuable as individuals with aphasia frequently face challenges with self-monitoring~\cite{oomen2001prearticulatory,sampson2011investigation} and often benefit from external cues to improve their performance~\cite{tompkins2006communicative,conroy2009effects,schwartz2016does}. Additionally, ASR can improve communication efficiency and quality by compensating for writing or grammatical impairments through speech-to-text conversion and delivering feedback during therapy~\cite{ballard2019feasibility,barbera2021nuva}. Consequently, ASR-based applications have gained popularity in delivering speech-language services for individuals with aphasia and other neurogenic conditions, such as Parkinson’s disease~\cite{hoover2014integrating,mccrocklin2016pronunciation,strik2009comparing}.

However, despite advancements in ASR, reliance on voice-based systems alone often results in transcription errors when processing disfluent or incomplete speech, a hallmark of language impairments~\cite{jefferson2019usability,le2016automatic}. These inaccuracies can hinder communication and reduce the overall effectiveness of these tools, further exacerbating barriers faced by individuals with language disorders. To address these challenges, AVSR systems have emerged, combining visual inputs—such as lip movements and facial expressions—with audio to enhance recognition accuracy~\cite{gabeur2022avatar,afouras2018deep,dupont2000audio,ma2021end,noda2015audio,mroueh2015deep,feng2017audio}. However, AVSR systems often fall short for individuals with language and speech disorders. Speech disorders such as dysarthria and apraxia frequently disrupt articulation, rendering lip movements unreliable. Moreover, neutral or ambiguous facial expressions common in these populations provide limited contextual insight~\cite{tong2020automatic,salama2014audio}.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figure/framework.png}
    \caption{The overall process of the proposed system. Our model integrates incomplete speech and visual data (i.e., iconic gestures) and generates semantically enriched transcripts.}%\vspace{-0.15in}
    ~\label{fig:framework}
\end{figure*}

\subsection{Gesture into Assistive Technology}
Gestures play a vital role in communication, often conveying critical information that complements, enhances, or even substitutes spoken language~\cite{kita2009cross,kendon1994gestures,mcneill1990speech}. This holds true for individuals with and without language impairments, as gestures are co-produced with speech across languages and cultures, aiding both speakers and listeners~\cite{kita2009cross}. Remarkably, even congenitally blind speakers, who have never observed gestures, use gestures as frequently as sighted individuals, emphasizing their fundamental role in communication~\cite{iverson1998people}. For speakers, gestures illustrate abstract concepts, emphasize key points, and provide additional information that speech alone may struggle to convey~\cite{mcneill1990speech,kita2000representational,goldin2013gesture}. For listeners, gestures offer visual cues that complement verbal messages, enhancing understanding in noisy settings, during non-native language processing, and when speech is ambiguous~\cite{cook2009embodied,goldin1999role}. Notably, listeners often recall information received via gestures as if it were spoken, underscoring their semantic importance~\cite{cassell2000embodied,kelly1999offering}.

Among gestures, representational gestures—those that visually represent objects or actions—are particularly valuable for enhancing communication~\cite{mcneill1990speech}. Iconic gestures, a subtype of representational gestures, depict concrete referents (e.g., a kicking motion to signify “kick”)~\cite{novack2017gesture}. Other representational gestures include pantomimes, which occur without speech, and deictic gestures, such as pointing, which may emphasize or add meaning to speech\cite{kendon1994gestures}. Additionally, emblems like a “thumbs up” convey culturally specific meanings independent of speech~\cite{kendon1994gestures}. However, not all gestures are communicative. Non-communicative movements, such as tucking hair behind the ear, do not contribute to the conveyed message and can be irrelevant to the communicative process.

For assistive technologies, particularly AI-based systems, distinguishing communicative gestures from non-communicative movements is essential. Systems must accurately interpret gestures to capture the full intent of a speaker’s message, especially when assisting individuals with language impairments. Thus, effectively integrating linguistic and co-speech gesture information is critical for improving the utility and effectiveness of AI-driven assistive tools.


\subsection{Challenges in Gesture Interpretation for Assistive Technology}
Accurately interpreting the wide variety of gestures individuals use is a significant challenge, as gestures vary in form and meaning, with some being non-communicative. Even gestures conveying the same intent, such as cutting, can differ—for example, an “enacting” gesture mimics the action of cutting with a flat hand, while a “handling” gesture simulates gripping an invisible knife~\cite{poggi2008iconicity,hassemer2018decoding}. This variability complicates consistent interpretation, particularly in zero-shot settings. While recent computer vision methods focus on generating gestures from text inputs for avatars or video sequences~\cite{ginosar2019learning,ahuja2022low,liu2022beat}, they do not address gesture understanding or intent recognition, relying instead on predefined datasets. Existing ASR and AVSR systems also fall short, as they lack the ability to incorporate conversational context or personalized knowledge, both critical for interpreting gestures tied to specific topics or individual habits. For instance, one person may favor “handling” gestures while another prefers “enacting.” Additionally, task-specific training required by many systems is impractical for real-time applications, as it necessitates frequent retraining to accommodate new users and scenarios. These limitations underscore the need for adaptable systems capable of real-time, personalized gesture interpretation.

