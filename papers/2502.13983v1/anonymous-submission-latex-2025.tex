%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{multirow}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Utility-Driven Tabular Data Synthesis via Reinforcement Learning}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1101 Pennsylvania Ave, NW Suite 300\\
    Washington, DC 20004 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\input{contents/0.Abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
%
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}

\input{contents/1.Intro}

\input{contents/2.Related}

\input{contents/3.Methodology}

\input{contents/4.Experiment}

\input{contents/5.Conclusion}


\bibliography{aaai25}

\newpage
\appendix

\section*{Appendix}


% Appendix for AAAI-2025 submission
% Outline: 
% 1, additional results on augmented
% 3, baseline implementation
% 6, target correlation score?

\subsection{Datasets}

NPHA: \url{https://archive.ics.uci.edu/dataset/936/national+poll+on+healthy+aging+(npha)}

Obesity: \url{https://www.kaggle.com/code/mpwolke/obesity-levels-life-style}

Diabetes: \url{https://archive.ics.uci.edu/dataset/34/diabetes}

Churn Modeling: \url{https://www.kaggle.com/datasets/shrutimechlearn/churn-modelling}

Adult: \url{https://archive.ics.uci.edu/dataset/2/adult}

German Credit: \url{https://archive.ics.uci.edu/dataset/522/south+german+credit}


\subsection{Baseline Implementation}

\label{sec:baseline}
% Details on baseline implementations like hyperparameter
\textbf{SMOTE:} We used the implementation provided at https://github.com/yandex-research/tab-ddpm, without balancing for target class frequency. 

\textbf{CTGAN:} We use the official implementation at https://github.com/sdv-dev/CTGAN. We use embedding dimension =128, generator dimension=(256,256), discriminator dimension =(256,256), generator learning rate=0.0002, generator decay =0.000001, discriminator learning rate =0.0002, discriminator decay =0.000001, batch size=500, training epoch = 300, discriminator steps=1, pac size = 5.

\textbf{TabDDPM:} We used the official implementation at https://github.com/yandex-research/tab-ddpm. We used 2500 diffusion steps, 10000 training epochs, learning rate = 0.001, weight decay = 1e-05, batch size = 1024.

\textbf{AIM:} We use the code implementation at https://github.com/ryan112358/private-pgm, with default parameters: epsilon=3,delta=1e-9,max model size=80

\textbf{PATE-CTGAN:} We adapted the implementation posted at: https://github.com/opendp/smartnoise-sdk/blob/main/synth/snsynth, which combines the PATE~\cite{jordon2018pate} learning framework with CTGAN. We use epsilon = 3, 5 iterations for student and teacher network, and the same value for other parameters which are shared with CTGAN.

\textbf{GReaT:} We used the official implementation at \url{https://github.com/kathrinse/be_great/tree/main}. We used a batch size of 64 and save steps of 400000. We following the pre-training pipeline outlined in~\cite{zhao2023tabula}. During pre-training, we began with a randomized distilgpt2 model. For each pre-training dataset, we iteratively loaded the latest model, fitted the model on the new dataset, and saved the model to be used for the next iteration. We used 50 epochs for the health model and 200 epochs for the out-of-domain model. For finetuning, we fitted the pre-trained model on only one dataset using 200 epochs. The dataset used in finetuning was the dataset we wished to emulate during synthesis. For reference, we also fitted a newly randomized model on the data alone to serve as a base metric. During generation, we synthesized the same number of samples as the finetune dataset.

\subsection{Data Augmentation Utility}

\begin{table*}
\begin{tabular}{llllllll}
\toprule
Model & Churn & NPHA & Obesity & Adult & Diabetes & Credit & AvgRank \\
\midrule
Real Data & 0.79{(0.03)} & 0.52{(0.02)} & 0.96{(0.03)} & 0.86{(0.04)} & 0.78{(0.01)} & 0.76{(0.03)} & 2.83 \\
\hline
AIM & 0.80{(0.04)} & 0.53{(0.02)} & 0.95{(0.02)} & \textbf{0.87}{(0.03)} & 0.76{(0.04)} & 0.73{(0.02)} & 4.58 \\
SMOTE & 0.80{(0.02)} & 0.49{(0.04)} & \textbf{0.96}{(0.02)} & 0.85{(0.01)} & 0.77{(0.04)} & 0.76{(0.03)} & 4.33 \\
CTGAN & 0.77{(0.02)} & 0.51{(0.03)} & 0.96{(0.01)} & 0.86{(0.04)} & 0.76{(0.02)} & 0.74{(0.04)} & 5.92 \\
PATECTGAN & 0.74{(0.03)} & \textbf{0.54}{(0.01)} & 0.94{(0.02)} & 0.86{(0.03)} & 0.70{(0.01)} & 0.76{(0.04)} & 5.08 \\
TabDDPM & 0.79{(0.02)} & 0.51{(0.03)} & 0.95{(0.04)} & 0.86{(0.01)} & 0.77{(0.03)} & 0.75{(0.01)} & 5.42 \\
\hline
GReaT & 0.79{(0.01)} & 0.52{(0.04)} & 0.96{(0.02)} & 0.87{(0.04)} & 0.79{(0.02)} & 0.75{(0.04)} & 4.40 \\
TabSynRL & \textbf{0.80}{(0.04)} & 0.52{(0.01)} & 0.96{(0.03)} & 0.87{(0.04)} & \textbf{0.81}{(0.02)} & \textbf{0.77}{(0.01)} & 2.00 \\
\bottomrule
\end{tabular}
\caption{Average ROC AUC of classifiers trained on real data augmented with different synthetic data. The best performing score for each dataset is highlighted in bold.}
\label{tab:augmented}
\end{table*}

Table~\ref{tab:augmented} presents the average ROC AUC scores for classifiers trained on real data augmented with different synthetic data generators across six datasets, along with their average rank. Among all methods, TabSynRL achieved the highest utility, securing the top average rank across datasets and outperforming prior state-of-the-art (SOTA) models, including TabDDPM and CTGAN. SMOTE, while effective, ranked slightly lower than TabSynRL, highlighting the limitations of interpolation-based techniques when compared to advanced generative approaches. Notably, differential privacy-preserving models like AIM and PATECTGAN exhibited lower utility due to the inherent noise introduced for privacy protection. The real data model performed competitively, but the enhancement brought by TabSynRL illustrates the benefit of augmenting data with RL fine-tuned generators. Moreover, despite the focus on conditional generation, models like TabDDPM and GReaT did not demonstrate superior utility, underscoring the significance of learning \(P(y | \mathbf{X})\) rather than \(P(\mathbf{X} | y)\) for downstream ML applications.


\subsection{Similarity of Correlation}


\begin{table*}

\begin{tabular}{llllllll}
\toprule
Model & Churn & NPHA & Obesity & Adult & Diabetes & Credit & AvgRank \\
\midrule
Real Data & 0.97{(0.04)} & 0.91{(0.02)} & 0.92{(0.01)} & 0.98{(0.03)} & 0.91{(0.02)} & 0.89{(0.04)} & 1.25 \\
\hline
SMOTE & \textbf{0.96}{(0.03)} & \textbf{0.91}{(0.01)} & 0.87{(0.02)} & \textbf{0.96}{(0.02)} & 0.91{(0.03)} & \textbf{0.88}{(0.01)} & 2.25 \\
AIM & 0.88{(0.01)} & 0.87{(0.03)} & 0.65{(0.04)} & 0.80{(0.02)} & 0.80{(0.04)} & 0.68{(0.03)} & 5.67 \\
CTGAN & 0.89{(0.02)} & 0.86{(0.04)} & 0.70{(0.01)} & 0.81{(0.03)} & 0.81{(0.02)} & 0.60{(0.04)} & 5.50 \\
PATECTGAN & 0.59{(0.03)} & 0.84{(0.02)} & 0.35{(0.01)} & 0.30{(0.03)} & 0.65{(0.02)} & 0.22{(0.01)} & 7.50 \\
TabDDPM & 0.95{(0.01)} & 0.90{(0.04)} & \textbf{0.89}{(0.03)} & 0.92{(0.02)} & 0.90{(0.01)} & 0.76{(0.04)} & 3.17 \\
GReaT & 0.75{(0.03)} & 0.75{(0.02)} & 0.82{(0.01)} & 0.88{(0.04)} & \textbf{0.92}{(0.03)} & 0.70{(0.01)} & 4.83 \\
TabSynRL & 0.87{(0.02)} & 0.75{(0.04)} & 0.86{(0.03)} & 0.98{(0.04)} & 0.91{(0.01)} & 0.66{(0.02)} & 5.40 \\
\bottomrule
\end{tabular}
\caption{Pairwise column distribution similarity between real test set and synthetic data across models. Higher value indicates greater similarity. Real Data represents training sets for synthesizers.}
\label{tab:fidelityCorr}
\end{table*}

Table~\ref{tab:fidelityCorr} presents the pairwise column distribution similarity between real test sets and synthetic data generated by various models across six datasets, along with the average rank for each model. The Real Data benchmark, as expected, ranks highest with near-perfect similarity scores. SMOTE closely follows, demonstrating competitive performance with a high fidelity to real data distributions across datasets. TabDDPM, which also ranks well, shows strong alignment with real data, particularly in datasets like Obesity and Adult, reflecting its ability to maintain distributional integrity. AIM and CTGAN perform moderately, with their scores indicating some drop in fidelity, especially in complex datasets where capturing nuanced patterns is challenging. Models like PATECTGAN and GReaT rank lower, with PATECTGAN struggling significantly, likely due to the noise introduced by privacy-preserving mechanisms. Interestingly, TabSynRL, while shown superior performance in machine learning utility, did not consistent excel in preserving column correlation in real data compared to other baselines. This provides further evidence that TabSynRL`s performance gain is not simply due to improved statistical fidelity or improvement in generative modeling objective.


\subsection{Reproducibility Checklist}

This paper:

\begin{itemize}
    \item Includes a conceptual outline and/or pseudocode description of AI methods introduced \textbf{yes}
    \item Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results \textbf{Yes}
    \item Provides well marked pedagogical references for less-familiare readers to gain background necessary to replicate the paper \textbf{Yes}
\end{itemize}

Does this paper make theoretical contributions? (\textbf{yes}/no)

Does this paper rely on one or more datasets? (\textbf{yes}/no)

If yes, please complete the list below.

\begin{itemize}
    \item A motivation is given for why the experiments are conducted on the selected datasets (\textbf{yes}/partial/no/NA)
    \item All novel datasets introduced in this paper are included in a data appendix. (yes/partial/no/\textbf{NA: no novel dataset used.})
    \item All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes/partial/no/\textbf{NA: no novel dataset used.})
    \item All datasets drawn from the existing literature (potentially including authors’ own previously published work) are accompanied by appropriate citations. (\textbf{yes}/no/NA)
    \item All datasets drawn from the existing literature (potentially including authors’ own previously published work) are publicly available. (\textbf{yes}/partial/no/NA)
    \item All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. (yes/partial/no/\textbf{NA: all datasets are publically available})
\end{itemize}

 Does this paper include computational experiments? (\textbf{yes}/no)

If yes, please complete the list below.

\begin{itemize}
    \item Any code required for pre-processing data is included in the appendix. (\textbf{yes}/partial/no).
    \item All source code required for conducting and analyzing the experiments is included in a code appendix. (yes)
    \item All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes)
    \item All source code implementing new methods has comments detailing the implementation, with references to the paper where each step comes from. (yes)
    \item If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (yes)
    \item This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. (yes)
    \item This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. (yes)
    \item This paper states the number of algorithm runs used to compute each reported result. (yes)
    \item Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. (yes)
    \item The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (yes)
    \item This paper lists all final (hyper-)parameters used for each model/algorithm in the paper’s experiments. (yes)
    \item This paper states the number and range of values tried per (hyper-)parameter during the development of the paper, along with the criterion used for selecting the final parameter setting. (yes)
\end{itemize}



\end{document}
