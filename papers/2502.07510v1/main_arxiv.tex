

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{sidecap}
\usepackage{booktabs} % for professional tables
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow} % For multirow cells

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{conference_preprint}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{adjustbox}
\mathtoolsset{showonlyrefs}
\newcommand{\tT}{\mathrm{T}}

% if you use cleveref..
% \usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}

\newcommand{\highMP}[1]{{\color{cyan}#1}}
\newenvironment{HighMP}{\color{cyan}}{}

\newcommand{\highFB}[1]{{\color{magenta}#1}}
\newenvironment{HighFB}{\color{magenta}}{}

\newcommand{\highRB}[1]{{\color{olive}#1}}
\newenvironment{HighRB}{\color{olive}}{}

\newcommand{\todo}[1]{\textcolor{red}{#1}}

\usepackage{nicefrac}
\usepackage{comment}

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\XX}{\mathbb{X}}
\newcommand{\YY}{\mathbb{Y}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\Xf}{\mathfrak{X}}
\newcommand{\Yf}{\mathfrak{Y}}
\newcommand{\Zf}{\mathfrak{Z}}
\newcommand{\MM}{\mathfrak{MM}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\p}{\mathcal{P}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\dx}{\,\mathrm{d}}
\newcommand{\weakly}{\rightharpoonup}
\newcommand{\Pio}{\Pi_{\mathrm{o}}}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator{\SGW}{SGW}
\DeclareMathOperator{\GW}{GW}
\DeclareMathOperator{\EW}{EW}
\DeclareMathOperator{\wGW}{\widetilde{\GW}}
\DeclareMathOperator{\W}{W}
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\Iso}{Iso}
\DeclareMathOperator{\JMDS}{JMDS}
\DeclareMathOperator{\SO}{SO}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Joint Metric Space Embedding by Unbalanced OT with
GW Marginal Penalization}

\begin{document}

\twocolumn[
\icmltitle{Joint Metric Space Embedding by Unbalanced OT with
Gromovâ€“-Wasserstein Marginal Penalization}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Florian Beier}{equal,yyy}
\icmlauthor{Moritz Piening}{equal,yyy}
\icmlauthor{Robert Beinert}{yyy}
\icmlauthor{Gabriele Steidl}{yyy}
%\icmlauthor{Firstname5 Lastname5}{yyy}
%\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Institute of Mathematics, Technical University of Berlin, Germany}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Moritz Piening}{piening@math.tu-berlin.de}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We propose a new approach for unsupervised
alignment of heterogeneous datasets, 
which maps data from two different domains without any known correspondences 
to a common metric space. 
Our method is based on an unbalanced optimal transport problem with Gromov--Wasserstein marginal penalization.
It can be seen as a counterpart to the recently introduced joint multidimensional scaling method.
We prove that there exists a minimizer of our functional and that for penalization parameters going to infinity, the corresponding sequence of minimizers converges to a minimizer of the so-called embedded Wasserstein distance.
Our model can be reformulated as a quadratic, multi-marginal, unbalanced optimal transport problem, for which a bi-convex relaxation admits
a numerical solver via block-coordinate descent.
We provide numerical examples 
for joint embeddings in Euclidean as well as non-Euclidean spaces.
\end{abstract}

\section{Introduction}

The comparison of heterogeneous
data distributions
is a fundamental task
in computer vision, computational biology
and machine learning.
Most existing approaches
rely on using a 
suitable ground cost function
such as an available metric.
A classic example is 
the Wasserstein distance which seeks
an optimal transport (OT)
between two given distributions.
However,
often the given distributions
are in heterogeneous spaces,
where a readily available ground cost function between these spaces does not generally exist. 
Additional effort may be required to learn appropriate cost functions \cite{cuturi2014ground,heitz2021ground}. 
However,
even if
there exists a natural embedding into 
a canonical joint metric space,
this metric 
may not be suited to
accurately gauge
differences in their samples.
Examples
of such heterogeneous settings
are, e.g.,\
the comparison
of graph-
or mesh-valued data such as 
3d shapes or manifolds.
This paper introduces a novel
framework based on OT
which enables the joint comparison
and visualization
of heterogeneous datasets
by optimally transferring
them into an a-priori 
fixed metric space, see Figure~\ref{fig:sphere_torus} for an illustrative example.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/teaser/updated/Sphere1_org.pdf}
            \end{subfigure}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/teaser/updated/Sphere1_front2.pdf}
            \end{subfigure}
        \end{subfigure}
        \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/teaser/updated/Sphere2_org.pdf}
            \end{subfigure}
                     \caption*{Original}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/teaser/updated/Sphere2_front2.pdf}
            \end{subfigure}
                     \caption*{Aligned}
        \end{subfigure}
        \caption{Spherical embedding}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/teaser/updated/Torus1_org.pdf}
            \end{subfigure}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/teaser/updated/Torus1_front.pdf}
            \end{subfigure}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/teaser/updated/Torus2_org.pdf}
            \end{subfigure}
                     \caption*{Original}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/teaser/updated/Torus2_front.pdf}
            \end{subfigure}
                     \caption*{Aligned}
        \end{subfigure}
        \caption{Toroidal embedding}
        \end{subfigure}
        \vspace{-10pt}
    \caption{Joint (aligning) transfer of two metric spaces (gray surfaces
    with surface distance) to a fixed reference space, namely to the sphere and the torus by our method,
    where the color ``yellow'' corresponds to higher values, see Subsection~\ref{subsec:3d_embeddings}.}
    \label{fig:sphere_torus}
\end{figure}

\subsection{Previous Work}

As a first step, we highlight the most relevant contributions related to our study among the vast literature on OT and dimensionality reduction.
\\[1ex]
\textbf{Optimal Transport with Invariances.} 
Optimal transport and Wasserstein distances enable comparisons of point clouds in metric spaces. 
However, 
classic OT
lacks invariance to important transformations 
like rotations in Euclidean space. 
To address this,
Gromov--Wasserstein (GW) distances 
are introduced which allow 
for the comparison of
measures from distinct metric spaces 
\cite{memoli2011gromov,sturm2006}. 
In \cite{alaya2022theoretical},
an approximation of GW distances is obtained
by jointly embedding measures 
into Euclidean spaces. 
Furthermore,
projection- and
subspace-robust Wasserstein-2 distances
are introduced in 
\cite{paty2019subspace}. 
Another approach incorporates invariance 
to Euclidean isometries 
via the Wasserstein Procrustes problem \cite{grave2019unsupervised}.
This is extended to account 
for linear operators 
with bounded Schatten norms in \cite{alvarez2019towards} 
and to Gaussian mixture applications in \cite{salmona2023gromov}. 
As outlined in Section~\ref{discrete}, our paper extends such invariant OT to non-Euclidean domains.
\\[1ex]
\textbf{Joint Dimensionality Reduction.} 
Dimensionality reduction is a core topic in machine learning enabling visualization and clustering by finding optimal low-dimensional representations of data. Classical methods like principal component analysis (PCA) \cite{greenacre2022principal} and multidimensional scaling (MDS) \cite{carroll1998multidimensional} preserve large variations or pairwise distances, but fail on nonlinear manifolds \cite{alaya2022theoretical,dengneuc}. 
Nonlinear approaches, e.g., locally linear embedding \cite{roweis2000nonlinear}, 
probabilistic models, e.g., t-distributed stochastic neighbor embedding (t-SNE) \cite{van2008visualizing}, and deep learning methods, e.g., variational autoencoders (VAEs), \cite{kingma2019introduction} address these challenges. 
As an extension, several recent methods focus on joint embeddings of heterogeneous data. Here, we are given data on two incompatible domains and are interested in simultaneous embedding. The manifold-aligning generative adversarial network \cite{amodio2018magan} employs a generative adversarial network for domain alignment. Maximum mean discrepancy (MMD) manifold-alignment \cite{liu2019jointly} balances an MMD and a distortion term. UnionCom \cite{cao2020unsupervised} leverages the generalized unsupervised manifold alignment (GUMA) \cite{cui2014generalized}.  Single-cell alignment with optimal transport (SCOT) \cite{demetci2022scot} and the partial manifold alignment algorithm \cite{cao2022manifold} employ GW distances. Finally, the recently proposed joint multidimensional scaling (JMDS) \cite{chen2023unsupervised} algorithm combines MDS with OT for the joint embedding of two 
datasets into a shared Euclidean space. Complementing this and the work on non-Euclidean embeddings in \cite{mcinnes2018umap,dengneuc}, we propose a joint embedding into arbitrary metric spaces based on GW distances.


\subsection{Contribution}

Our main contributions are the following:
\\
1.  Towards the aligned  ``embedding'' of heterogeneous metric spaces 
into a fixed, not necessarily Euclidean space,
we propose to minimize 
an unbalanced OT problem with a quadratic cost function,
where the marginals are penalized by GW distances. 
This formulation seeks near-isometric joint
embeddings of the inputs into a metric space,
while enabling an optimal comparison 
in the Wasserstein distance.
\\
2. We prove that our functional has a minimizer. Further, if its regularization parameter goes to infinity, our functional approaches the so-called ``embedded Wasserstein distance'' \cite{salmona2023gromov}.
In this sense, we also refer to our model as the ``relaxed embedded Wasserstein distance''.
\\
3. For the (approximate) computation of a minimizer,
we provide an equivalent formulation of our model
as a quadratic, multi-marginal, unbalanced OT
problem.
A bi-convex relaxation enables the application of existing algorithms to solve the problem numerically.
\\
4. We recall JMDS from the point of view of our model:
while we are searching for the weight of atomic measures fixing their supports, JMDS fixes the weights and aims to find the supports of the measures.%, see Section \ref{discrete}.
\\
5. Numerical experiments demonstrate the potential of our method
for joint embeddings of heterogeneous data on the 2d
    Euclidean space, the torus, the 2-sphere and
    the space of 2d Gaussians with the Wasserstein distance.  


All proofs are given in Appendix \ref{app:A}.


\section{OT-Based Distances} \label{sec:notation}

Given a compact metric space 
$(Z, d_Z)$, we denote by
$\p(Z)$ the space of
probability measures defined
on the Borel-$\sigma$-algebra
induced by the distance $d_Z$.
For two compact metric spaces $Z_1$ and $Z_2$,
the push-forward measure
of $\mu \in \p(Z_1)$
under a measurable map $T \colon Z_1 \to Z_2$
is denoted by
$T_\sharp \mu \coloneqq 
\mu \circ T^{-1}
\in \p(Z_2)$.
The \emph{set of transport plans}
between two measures 
$\mu_1 \in \p(Z_1)$ and $\mu_2 \in \p(Z_2)$
is given by
\begin{equation}
    \Pi(\mu_1,\mu_2) \coloneqq \bigl\{\pi \in \p(Z_1 \times Z_2) :   
    P_{i,\sharp} \pi = \mu_i, \; i=1,2 \bigr\},
\end{equation}
where 
$P_i \colon Z_1 \times Z_2 \to Z_i,
(z_1,z_2) \mapsto z_i$,
$i=1,2$,
denotes the projection to the first and second component, respectively.


To lift $d_Z$
from $Z$ to $\p(Z)$,
we rely on the 
\emph{Wasserstein(-2) distance} between $\mu_1,\mu_2 \in \p(Z)$
defined by
\begin{equation} \label{w2}
    \W(\mu_1,\mu_2)
    \coloneqq \! \!
    \min_{\pi \in \Pi(\mu_1,\mu_2)}
    \Bigl(\!
    \int\limits_{Z\times Z} d_Z^2(z,z') \dx \pi(z,z')
    \!\Bigr)^\frac{1}{2}.
\end{equation}
By $\Pio(\mu_1,\mu_2)$, we denote the set of minimizers in \eqref{w2}.
The Wasserstein distance metricizes
the weak convergence 
of measures, 
where a sequence of measures 
$(\mu_n)_{n \in \mathbb{N}}$
converges weakly to a measure 
$\mu \in \p(Z)$,
written $\mu_n \weakly \mu$,
if for every (bounded) continuous function 
$\varphi\colon Z \to \mathbb{R}$, 
we have 
$\int_Z \varphi  \dx \mu_n \to \int_Z \varphi \dx \mu$ 
as $n \to \infty$. 

To compare heterogeneous
data via their internal geometry,
we can rely on 
GW distances,
    which enables us to compare measures on different metric spaces.
    To highlight the connection between measures and underlying spaces,
    we consider \emph{metric measure spaces (mm-spaces)},
    which are triples
$\XX = (X,d_X,\xi)$
such that
$(X,d_X)$ 
is a compact metric space 
and $\xi \in \p(X)$.
Two mm-spaces
$\XX_i = (X_i,d_{X_i},\xi_i)$, $i=1,2$,
are \emph{isomorphic}
if there exists
a bijective map
    $I\colon \supp \xi_1 \to \supp \xi_2$
    between the supports of the measures
    such that $I$ is \emph{measure-preserving}, i.e.,\
    $I_\sharp \xi_1 = \xi_2$,
    and $I$ is an \emph{isometry}, i.e.,\
    $d_{X_1}(x_1,x_1') 
    = d_{X_2}(I(x_1),I(x_1'))$
    for all
    $x_1,x_1' \in \supp \xi_1$.
    By $[\XX]$ we denote the equivalence class of all mm-spaces
which are isomorphic to $\XX$.  
Finally, if $I$ is an isometry between two metric spaces $X_i$, $i=1,2$, we write
$I \colon X_1 \hookrightarrow X_2$,
and just
$X_1 \hookrightarrow X_2$ if  such an isometry between $X_1$ and $X_2$ exists.

\emph{Sturm's GW
distance} \cite{sturm2006}
between two mm-spaces 
$\XX_i$, $i=1,2$,
is defined by
\begin{equation*}
    \SGW(\XX_1,\XX_2)
    \coloneqq
    \!\inf\limits_{
    \substack{
    (Z,d_Z) \text{ metric space},\\
    I_i\colon \supp \xi_i \hookrightarrow Z, \, i=1,2
    }
    }\!
    \W( I_{1,\sharp} \xi_1, I_{2,\sharp} \xi_2 ), 
\end{equation*}
where the Wasserstein distance is taken over the respective space
$(Z,d_Z)$. 
The SGW distance seeks
optimal isometric embeddings
of either mm-space 
into a joint metric space
such that their Wasserstein distance 
in the embedded space is minimal,
see Figure~\ref{subfig:SGW}. 
Indeed, 
SGW is a metric on the equivalence classes of mm-spaces. 

MÃ©moli proposed a different GW distance, which is numerically more appealing 
than Sturm's construction.
For two mm-spaces 
$\XX_i$,
$i=1,2$,
\emph{MÃ©moli's GW distance} \cite{memoli2011gromov} is defined by
\begin{align}\label{eq:GW}
&\GW
(\XX_1,\XX_2) 
\coloneqq 
\min_{\gamma \in \Pi(\xi_1,\xi_2)} G_{X_1,X_2} (\gamma)^\frac12
\end{align}
with the quadratic GW objective
\begin{align}
G_{X_1,X_2}(\gamma)
&\coloneqq
\iint\limits_{(X_1 \times X_2)^2}
\bigl( d_{X_1}(x_1,x_1') - d_{X_2}(x_2,x_2') \bigr)^2
\\
&\hspace{27pt} {\times}\dx \gamma(x_1,x_2)
\dx \gamma(x_1',x_2').
\end{align}
The problem
seeks a transport between $\XX_1$
and $\XX_2$
which minimizes the overall pairwise
distance distortion,
see Figure~\ref{subfig:GW}.
Both distances---SGW and GW---
define a metric on the equivalence classes of mm-spaces. 
Furthermore,
both distances
induce the same topology,
but SGW turns these equivalence classes into a complete metric space,
while GW does not \cite{sturm2006,memoli2011gromov}. 
The later
can be alleviated by embedding into a larger space and extending the GW definition accordingly \cite{sturm2023space}.

\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/sturm_GW_ssvm_pi.png}
    \caption{
    Sturm's GW 
    }
    \label{subfig:SGW}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/GW_ssvm.png}
    \caption{
    MÃ©moli's GW   
    }
    \label{subfig:GW}
\end{subfigure}
\caption{
Illustration
of GW formulations of Sturm and MÃ©moli. 
}
\label{fig:comparison}
\end{figure}

\section{Unbalanced OT With GW Penalization}\label{sec:main}

Throughout this section, let $(Z,d_Z)$ be a fixed compact metric space.
Furthermore,
let $\XX_i = (X_i,d_{X_i},\xi_i)$,
$i=1,2$,
be two mm-spaces,
whose measures' support can be isometrically embedded into $(Z, d_Z)$,
i.e.,\
$\supp \xi_i \hookrightarrow Z$.
For this specific setting,
we relax Sturm's GW distance to
\begin{equation}\label{eq:Z_SGW}
\EW(\XX_1,\XX_2)
\coloneqq \inf_{
\substack{
I_1\colon \supp \xi_1 \hookrightarrow Z\\ 
I_2\colon \supp \xi_2 \hookrightarrow Z
}
}
\W ( I_{1,\sharp} \xi_1, I_{2,\sharp} \xi_2).
\end{equation}
On a certain subspace of equivalence classes of mm-spaces, this defines a metric, which we call
\emph{embedded Wasserstein metric}.
For the specific case of Euclidean spaces $X_1$, $X_2$, and $Z$, this reduces to the Wasserstein Procrustes problem \cite{grave2019unsupervised} and to the ``embedded Wasserstein metric'' in \cite{salmona2023gromov}. We adopt this name.

\begin{proposition}
    \label{prop:metric}
    $\EW$ defines a metric on the subset of isomorphic classes $[(X,d_X, \xi)]$ 
    for which there exist surjective isomorphism $I \colon \supp \xi \hookrightarrow Z$.
\end{proposition}

By the following proposition, the infimum in \eqref{eq:Z_SGW} is attained.
  
\begin{proposition} \label{prop1}
Let $\XX_i$, $i=1,2$ be two mm-spaces and  $(Z,d_Z)$  a metric space such that
$\supp \xi_i \hookrightarrow Z$ for $i=1,2$.
Then the infimum in \eqref{eq:Z_SGW} is attained.
\end{proposition}
 While EW relies on appropriate isometries, the following relaxation enables to handle
arbitrary mm-spaces $\XX_i$, $i=1,2$.
For $\lambda > 0$,
we consider the following 
GW penalized unbalanced OT problem 
\begin{align}
\EW_\lambda(\XX_1,\XX_2)
&\coloneqq 
\inf_{
\pi \in \p(Z \times Z)
} \! \!
\Big(
\int_{Z \times Z} d_Z^2(z,z') \dx \pi(z,z')
\\
&+ \lambda \sum_{i=1}^2
\GW^2(\XX_i,(Z,d_Z,P_{i,\sharp} \pi))
\Big)^{\frac{1}{2}}. \label{eq:RGW}
\end{align}
By penalizing the GW terms,
the marginals
$P_{i,\sharp}\pi \in \p(Z)$
take a form
that enforces
$(Z,d_Z,P_{i,\sharp} \pi)$
to be nearly isomorphic
to the inputs $\XX_i$,
$i=1,2$.
Furthermore,
the first term 
ensures that the marginals
$P_{i,\sharp} \pi$, $i=1,2$
are close in the Wasserstein distance
on $(Z,d_Z)$. 
The penalization extends EW to non-isomorphic metric spaces by considering minimum-distortion embeddings. 
By the following proposition, the infimum in 
\eqref{eq:RGW}
is attained.

\begin{proposition}\label{thm:RGW_existence}
Let $\XX_i$, $i=1,2$ be two mm-spaces and  $(Z,d_Z)$  a metric space. Then \eqref{eq:RGW} admits a solution.
\end{proposition}

As the GW penalization enforces isometry,
EW in \eqref{eq:Z_SGW}
becomes the limit 
of EW$_\lambda$ in \eqref{eq:RGW} if $\lambda$ goes to infinity. 

\begin{proposition}\label{thm:limit}
Let $\XX_i$, $i=1,2$ be two mm-spaces and  let $(Z,d_Z)$ be a metric space such that
$\supp \xi_i \hookrightarrow Z$. 
Let $(\lambda_n)_{n \in \mathbb N}$ be a sequence
with $\lambda_n \to \infty$ as $n \to \infty$.
Then any sequence $(\pi_n)_{n \in \mathbb N}$ of minimizers of $\EW_{\lambda_n}(\XX_1,\XX_2)$
converges weakly, up to a subsequence, 
to some $\pi \in \mathcal P(Z\times Z)$.
There exist isometries
    $(I_1,I_2)$
    realizing  $\EW(\XX_1,\XX_2)$
such that
   $\pi \in \Pio(I_{1,\sharp} \xi_1, I_{2,\sharp} \xi_2)$.
\end{proposition}

In the rest of the paper, we skip the integration domains of the integrals
 for better readability,
since they are clear from the context.
Then, by definition of the GW distance,  EW$_\lambda$ in \eqref{eq:RGW} can be rewritten as
\begin{align}
&\EW_\lambda^2(\XX_1,\XX_2)
=
\inf\limits_{
\pi \in \p(Z \times Z)
}
\inf\limits_{
 \substack{
   \gamma_i \in \Pi(\xi_i , P_{Z,\sharp} \pi)
    \\i=1,2}
}
\\
&\int d_Z^2(z,z') \dx \pi(z,z')
+ \lambda 
\big(G_{X_1,Z}(\gamma_1) + G_{X_2,Z}(\gamma_2) \big)\,\,\,\,\,\label{eq:RGW_x}
\end{align}
or equivalently as
\begin{align} 
    &\EW_\lambda^2(\XX_1,\XX_2)
=
\inf\limits_{\mu_1,\mu_2 \in \p(Z)}
\inf\limits_{\pi \in \Pi(\mu_1,\mu_2)
}
\inf\limits_{
 \substack{
   \gamma_i \in \Pi(\xi_i ,\mu_i)
    \\i=1,2}
}
\\
&\int d_Z^2(z,z') \dx \pi(z,z')
+ \lambda 
\big(G_{X_1,Z}(\gamma_1) + G_{X_2,Z}(\gamma_2) \big).\,\,\,\label{help1}
\end{align}
For computing EW$_\lambda$, we will rewrite the term by 4-plans.
To this end, we use the notation 
$Z_1=Z_2\coloneqq Z$ 
and denote the projection
onto $X_i$,$Z_i$ by $P_{X_i},P_{Z_i}$,
respectively.
Now we consider 4-plans $\alpha \in \mathcal P(X_1,Z_1,Z_2,X_2)$
fulfilling
$P_{X_i,\sharp} \alpha = \xi_i$, $i=1,2$.
Let 
\begin{equation} \label{proj}
\pi \coloneqq P_{Z_1 \times Z_2,\sharp} \alpha
\quad \text{and} \quad
\gamma_i \coloneqq P_{X_i \times Z_i,\sharp} \alpha, \;
i=1,2.
\end{equation}
Clearly, such plans automatically fulfill
$$
P_{Z_i,\sharp} \pi = P_{Z_i,\sharp} \gamma_i , \; i=1,2
$$
see Figure~\ref{fig:4plan}.
Thus, \eqref{eq:RGW_x} can be reformulated as a
quadratic, multi-marginal, unbalanced OT problem
\begin{align}\label{eq:as_fused}
\EW_\lambda(\XX_1,\XX_2)=\inf_{
\substack{
\alpha \in 
\p(X_1 \times Z_1 \times Z_2 \times X_2)
\\
P_{X_i, \sharp} \alpha = \xi_i, i=1,2
}}
F_\lambda(\alpha)^\frac12
\end{align}
with the quadratic objective
\begin{align}
F_\lambda(\alpha) &\coloneqq \iint
\frac{1}{2}\bigl(d_Z^2(z_1,z_2) + d_Z^2(z_1',z_2')\bigr)
\\
&\hspace{30pt} + \lambda\sum_{i=1}^2 
(d_{X_i}(x_i,x_i') - d_Z(z_i,z_i'))^2
\\
&\quad \times
\dx \alpha(x_1,z_1,z_2,x_2)
\dx \alpha(x'_1,z'_1,z'_2,x'_2).
\end{align}
We summarize our findings in the following proposition.

\begin{proposition}\label{prop:summary}
If $\alpha$ is a solution of \eqref{eq:as_fused}, then
    its projections \eqref{proj}
are solutions of \eqref{eq:RGW_x} and in particular
$\pi$ is a solution of \eqref{eq:RGW}.
Conversely,  any solution of \eqref{eq:RGW}
can be expressed in this way. 
\end{proposition}

In Appendix \ref{app:B}, we illustrate 
relations between the Wasserstein distance, GW, EW and EW$_\lambda$
by numerical examples.

\begin{figure}[t]
\centering
    \includegraphics[width=.6\linewidth]{imgs/4plan.png}
\caption{
Illustration
of our multi-marginal transport problem.
}
    \label{fig:4plan}
\end{figure}


\section{Bi-Convex Relaxation} \label{algo}

At its core,
the computation of EW$_\lambda$ in 
\eqref{eq:as_fused}
requires the solution of a quadratic optimization problem.
Similar  formulations appear,
for instance,
in the computation of the GW distance \cite{PCS2016,SejViaPey21},
in the multi-marginal GW setting \cite{BBS2022multi},
and in CO-OT \cite{vayer_COOT}.
All of these quadratic OT problem have in common 
that they can be numerically solved
using a block-coordinate descent
on their bi-convex relaxations.
In the following,
we adapt this approach to our multi-marginal, unbalanced transport problem \eqref{eq:as_fused}.

For this,
we decouple the minimization 
with respect to the inner and outer transport plan $\gamma$
in the double integral of \eqref{eq:as_fused}.
More precisely,
denoting the inner plan by $\alpha_1$ 
and the outer plan by $\alpha_2$,
we consider the bi-convex relaxation
\begin{equation}     \label{eq:biconvex_relax}
    \inf_{
    \substack{
    \alpha_1, \alpha_2 \in 
    \p(X_1 \times Z_1 \times Z_2 \times X_2)
    \\
    (P_{X_i})_\sharp \alpha_k = \xi_i,\ i,k=1,2
    }
    } 
    \mathcal F_\lambda(\alpha_1, \alpha_2)
\end{equation}
with the bilinear objective
\begin{align}
    \mathcal F_\lambda(\alpha_1, \alpha_2)
    &\coloneqq
    \iint
    \frac{1}{2}\bigl(d_Z^2(z_1,z_2) + d_Z^2(z_1',z_2')\bigr)
    \\[-7pt]
    &\hspace{30pt} + \lambda\sum_{i=1}^2 
    (d_{X_i}(x_i,x_i') - d_Z(z_i,z_i'))^2
    \\[-3pt]
    &\hspace{15pt} {\times}
    \dx \alpha_1(x_1,z_1,z_2,x_2)
    \dx \alpha_2(x_1',z_1',z_2',x_2').
\end{align}
By construction, 
the minimizers of \eqref{eq:biconvex_relax} constitute
a lower bound to the original, quadratic problem \eqref{eq:as_fused}.
Moreover,
every bi-convex minimizer of the form $\alpha_1 = \alpha_2$
yields a minimizer of the original problem.

In order to find a numerical solution,
we apply block-coordinate descent,
which consists of alternatively  fixing
$\alpha_1$ and $\alpha_2$ in \eqref{eq:biconvex_relax}
and minimizing with respect to the other argument.
Fixing $\alpha_1$,
it remains to solve the (linear) multi-marginal, unbalanced OT problem 
\begin{align}\label{eq:biconvex_relax_OT}
    &\inf_{
\substack{
P_{X_i,\sharp} \alpha_2 = \xi_i\\ i=1,2
}}
    \int
     c_1 (x_1',z_1',z_2',x_2')
    \dx \alpha_2(x_1',z_1',z_2',x_2'),
\end{align}
where the effective cost is given by
\begin{align}
    &c_1 (x_1',z_1',z_2',x_2')    
    \coloneqq
   \frac12 d_Z^2(z_1',z_2')  + \lambda\sum_{i=1}^2
    \\[-7pt]
    &
    \int
    \bigl(d_{X_i}(x_i,x_i') - d_Z(z_i,z_i')\bigr)^2
    \dx \alpha_1(x_1,z_1,z_2,x_2).\,\,\,\,
    \label{eq:lin-cost}
\end{align}
The cost function
additively decouples into
three partial cost functions 
solely depending on two coordinates of $c_1$.
Unbalanced, multi-marginal OT formulations of this kind
are treated in \cite{BLNS2021},
where,
relying on an entropic regularization,
a multi-marginal Sinkhorn scheme is proposed.
Mathematically,
this means that,
for a regularization parameter $\varepsilon>0$,
we approximate the minimizer of \eqref{eq:biconvex_relax_OT} by
\begin{equation}
    \inf_{
\substack{
P_{X_i,\sharp} \alpha_2 = \xi_i\\ i=1,2
}}
    \int
     c_{1}
    \dx \alpha_2
    + \varepsilon
    \KL(\alpha_2,\upsilon),
\end{equation}
where $\upsilon$ denotes the uniform measure on $X_1 \times Z_1 \times Z_2 \times X_2$
and KL the Kullback--Leibler divergence.
More precisely,
with the Radon--NikodÃ½m derivative $\dx \gamma / \dx \upsilon$,
the KL divergence is given by
$\KL(\gamma, \upsilon)
\coloneqq
\int \log(\dx \gamma / \dx \upsilon) \dx \upsilon$
if $\gamma \ll \upsilon$
and $\KL(\gamma, \upsilon) \coloneqq \infty$ otherwise.
In total,
we may approximate the minimizer of 
the quadratic, unbalanced, multi-marginal OT problem \eqref{eq:as_fused} 
by applying Algorithm~\ref{alg:unbalanced_gw}.

\begin{algorithm}
       \begin{algorithmic}[1]
        \INPUT $\XX_i = (X_i, d_{X_i}, \xi_i), \ i = 1, 2.$ \hfill(mm-spaces)
        \INPUT $(Z, d_Z)$ \hfill(finite metric space)
        \INPUT $\lambda > 0$ \hfill(penalization parameter)
        \INPUT $\varepsilon > 0$ \hfill(regularization parameter)
        \STATE Initialize $\alpha_1 = \alpha_2 := \xi_1 \otimes \mu \otimes \mu \otimes \xi_2$\\
        where $\mu$ is the uniform measure on $(Z,d_Z)$
        \WHILE{not converged}
            \STATE Compute $c_1$ as in \eqref{eq:lin-cost}
            \STATE Update $\alpha_2$ using the multi-marginal Sinkhorn scheme 
            in \cite{BLNS2021} with $c_{1}$
            \STATE Compute $c_{2}$ analogous to \eqref{eq:lin-cost}
            \STATE Update $\alpha_1$ using the multi-marginal Sinkhorn scheme 
            in \cite{BLNS2021} with $c_2$
        \ENDWHILE{}
        \OUTPUT $\alpha_1$ or $\alpha_2$
    \end{algorithmic}
     \caption{Computation of EW$_\lambda$}
    \label{alg:unbalanced_gw}
\end{algorithm}


\section{Discrete EW$_{\boldsymbol{\lambda}}$ and JMDS}
\label{discrete} 

The distance EW in \eqref{eq:Z_SGW} generalizes 
Wasserstein Procrustes \cite{grave2019unsupervised} to non-Euclidean spaces, while  EW$_\lambda$ in \eqref{eq:RGW}
is closely related to the JMDS model  \cite{chen2023unsupervised}.
In this section, we explain the relation.

We consider the discrete case, where
$$X_1 \coloneqq \{x_1^1,\ldots,x_1^{n_1} \}
\quad \text{and} \quad
X_2 \coloneqq \{x_2^1,\ldots,x_2^{n_2} \}$$
are point sets equipped with dissimilarity distances $d_{X_1}$ and $d_{X_2}$ as well as measures
$$
\xi_1 \coloneqq \sum_{j=1}^{n_1} \xi_1^j \delta_{x_1^j}
,\quad
\xi_2 \coloneqq \sum_{j=1}^{n_2} \xi_2^j \delta_{x_2^j}.
$$
{
\textbf{Discrete EW}$_{\boldsymbol{\lambda}}$
fixes a discrete embedding space 
$$
Z_1 = Z_2 = Z \coloneqq \{z^1,\ldots,z^m \}.
$$
with metric $d_Z$. Accordingly, we use measures with fixed supports
\begin{equation}
    \mu_i = \smashoperator{\sum_{j=1}^m} \mu_i^j \delta_{z^j} ,
    \; \gamma_i = \smashoperator{\sum_{j,k =1}^{n_i,m}} \gamma_i^{j,k}
\delta_{x_i^j,z^k}, \;
\pi = \smashoperator{\sum_{j,k =1}^{m}} \pi_{j,k} \delta_{z^j,z^k},
\end{equation}
where $i=1,2$.
Since the supports of the measures are fixed, we can
restrict ourselves to the weight matrices 
$\boldsymbol{\mu}_i \in \Delta_m$,
$\boldsymbol{\xi}_i \in \Delta_{n_i}$, 
$\boldsymbol{\pi} \in \Pi(\boldsymbol{\mu}_1,\boldsymbol{\mu}_2)$
and 
$\boldsymbol{\gamma}_i \in  \Pi(\boldsymbol{\xi}_i,\boldsymbol{\mu}_i)$, $i=1,2$
in the corresponding probability simplicies $\Delta$. 
Then  EW$_\lambda$ in \eqref{help1} becomes the discrete minimization problem
\begin{align}
\EW_\lambda(X_1,X_2)
=
\min_{\substack{\boldsymbol{\mu_i} \in \Delta_m\\ i=1,2}}
\min_{ \boldsymbol{\pi} \in \Pi(\boldsymbol{\mu}_1, \boldsymbol{\mu}_2) } \min_{\substack{\boldsymbol{\gamma}_i \in \Pi(\boldsymbol{\xi}_i,  \boldsymbol{\mu}_i) \\ i=1,2 }}
\;&
\\
 \big\langle \boldsymbol{\pi} , \boldsymbol{D}^2_{Z,Z} \big \rangle
 +
\lambda \big(G_{X_1,Z} (\boldsymbol{\gamma}_1) + 
 G_{X_2,Z} (\boldsymbol{\gamma}_2) \big), \label{xxx}
&
\end{align}
where
$\boldsymbol{D}^2_{Z,Z} \coloneqq \big(d_Z^2(z^j,z^k)\big)_{j,k=1}^{m}$
and, for $i=1,2$,
\begin{align}
&G_{X_i,Z} (\boldsymbol{\gamma}_i)
\\
&\coloneqq 
\sum_{j,k=1}^{n_i,m} \sum_{r,s=1}^{n_i,m} \gamma_i^{s,t} \gamma_i^{j,k}
\big( d_{X_1} (x_i^j,x_i^r) - d_{Z} (z^k,z^s)
\big)^2.
\end{align}

\textbf{JMDS} aims to find point sets
\begin{equation}
\mathcal Z_1 \coloneqq \{z_1^1,\ldots,z_1^{n_1} \} \subset \R^d, \quad
\mathcal Z_2 \coloneqq \{z_2^1,\ldots,z_2^{n_2} \} \subset \R^d
\end{equation}
such that the dissimilarity relations in  $X_i$ are approximately preserved in $\mathcal Z_i$, $i=1,2$, while ensuring that the intermediate points are optimally aligned.
Here, $\mathcal Z_i$, $i=1,2$ are exclusively in $\R^d$ with the
Euclidean metric $d(x,y) = \|x-y\|$.
Instead of measures with fixed support, we use
$$
\mu_i \coloneqq \sum_{j=1}^{n_i} \frac{1}{n_i} \delta_{z_i^j},
\;
\pi = \sum\limits_{j,k =1}^{m} \pi_{j,k} \delta_{z_1^j,z_1^k},
$$
as well as
$$
\gamma_i \coloneqq \sum_{j,k=1}^{n_i}
\gamma^{j,k}_i \delta_{x_i^j,z_i^k} \quad\text{with}\quad
\gamma^{j,k}_i\coloneqq 
\left\{
\begin{array}{ll}
\tfrac{1}{n_i}&j=k,\\
0 &j \not= k,
\end{array}
\right.
$$
and will optimize over the supports $\mathcal Z_i$, $i=1,2$.
Then \eqref{help1} becomes
\begin{align}
\JMDS_\lambda (X_1, X_2)
\coloneqq 
\min_{\mathcal Z_1,\mathcal Z_2}
\min_{\boldsymbol{\pi} \in \Pi(\frac{1}{n_1} \boldsymbol 1_{n_1},
\frac{1}{n_2} \boldsymbol 1_{n_2}) } 
\;&
\\
  \big\langle \pi, \boldsymbol{D}^2_{\mathcal Z_1, \mathcal Z_2} \big\rangle  + \lambda \big( G_1(\mathcal Z_1) + G_2 (\mathcal Z_2) \big),& \label{jmds}
\end{align}
where 
$
\boldsymbol{D}^2_{\mathcal Z_1, \mathcal Z_2} \coloneqq
\big( d^2(z_1^j, z_2^k) \big)_{j,k=1}^{n_1,n_2}
$
and, for $i=1,2$,
\begin{align}
&G_i(\mathcal Z_i) 
\! \coloneqq \!
\sum_{j,k=1}^{n_i}  \sum_{r,s=1}^{n_i}  
\gamma^{j,k}_i \gamma^{r,s}_i
    \big(
    d_{X_i}(x_i^j,x_i^r) - d(z_i^k,z_i^s) \big)^2 \label{add}
    \\[-1ex]
    & \qquad \; \; =
\sum_{j,k=1}^{n_i} \frac{1}{n_i^2}
    \big(
    d_{X_i}(x_i^j,x_i^k) - d(z_i^j,z_i^k) \big)^2.
\end{align}
This is exactly the functional proposed as \emph{JMDS} in
 \cite{chen2023unsupervised}.
Note that the \emph{Wasserstein Procrustes model} is given by
\begin{equation} \label{procrustes}
\min_{\boldsymbol{\pi} \in \Pi(\frac{1}{n_1} \boldsymbol 1_{n_1},
\frac{1}{n_2} \boldsymbol 1_{n_2}) }
\min_{Q \in \SO(d)}
     \big\langle \pi, \boldsymbol{D}^2_{Q  \mathcal Z_1, \mathcal Z_2} \big\rangle .
\end{equation}
This  is exactly the first summand in JMDS$_\lambda$, where the minimization over
the special orthogonal group $\SO(d)$ can be skipped in \eqref{jmds}
since $G_{i}(\mathcal Z_i)$, $i=1,2$,
are invariant under orthogonal transforms.

To summarize:
1. Our discrete EW$_\lambda$ fixes the support of the marginals 
$\mu_i$ in \eqref{help1} which results in the optimization over the weights, where JMSD fixes the weights of the $\mu_i$ and optimizes over the supports.
2. 
Fixing the support has the advantage that we can work with arbitrary metric spaces $(Z,d_Z)$, 
while the ``free support'' approach of JMDS is restricted to the Euclidean space.
3. The minimization problems have to be tackled by completely different optimization algorithms,
namely an unbalanced multimarginal Sinkhorn algorithm for the block-coordinate descent in Algorithm~\ref{alg:unbalanced_gw} for
EW$_\lambda$
and the so-called SMACOF method combined with Wasserstein Procrustes minimizations \cite{chen2023unsupervised}  for JMDS.


\section{Numerical Results}\label{sec:numerics}

Next, we provide several proof-of-the-concept examples.\footnote{Code available upon acceptance.}

\subsection{Joint Embedding of 3d Shapes}
\label{subsec:3d_embeddings}
    In the first example, 
    we exploit
    EW$_\lambda$
    to align and embed 3d shapes%
    ---the surfaces of objects in $\R^3$---%
    into a joint space $(Z,d_Z)$. 
    For this,
    we interpret a 3d shape as mm-space $\XX \coloneqq (X, d_X, \xi)$,
    where $X$ is the surface, 
    $d_X$ is the surface (or geodesic) distance,
    and $\xi$ is the uniform measure.
    Practically, 
    $X$ is parametrized by the vertices of a triangular mesh, 
    $d_X$ is approximated using Dijkstra's algorithm \cite{dijkstra2022note}
    on the corresponding graph,
    and $\xi$ is chosen as the discrete uniform measure.
    For the joint embedding of two given (discrete) shapes $\XX_i$, $i=1,2$,
    we compute the 4-plan $\alpha$ in \eqref{eq:as_fused}
    using the discretization in Section~\ref{discrete}
    and Algorithm~\ref{alg:unbalanced_gw}.
    Since the relaxation behind EW$_\lambda$
    does not yield an isometry,
    but only a transport plan $\gamma_i = P_{X_i \times Z_i, \sharp} \alpha$,
    we visualize the computed relaxed embeddings
    by the marginals $P_{Z_i, \sharp} \gamma_i = P_{Z_i,\sharp} \alpha$.

    \paragraph{Bended Rectangles}
    
    We start by embedding 
    an S-bended rectangle 
    and a Swiss roll (with and without a hole)
    into $\R^2$.
    Intuitively,
    we expect that
    the surfaces without holes are unrolled 
    by the relaxed embedding behind EW$_\lambda$,
    since there actually exists isometric embeddings.
    For the discretization,
    we choose $Z$ as an equispaced 50$\times$50 grid on $[0, 1.3]^2 \subset \R^2$
    and $d_Z$ as the corresponding  Euclidean distance
    and apply Algorithm~\ref{alg:unbalanced_gw} 
    with $\lambda = 100$ and $\varepsilon = 10^{-3}$.
    In Figure~\ref{fig:manifolds-rolls},
    the relaxed embeddings are visualized by $P_{Z_i,\sharp} \alpha$ ,
    where the point sizes represent the underlying probability masses/weights.
    As comparison,
    we also show the results of JMDS
    with $\lambda = 10$ and $\varepsilon = 10^{-3}$
    for the incorporated regularized OT problem. 
    Both methods produce  representations 
    that unroll the shapes
    and succeed in aligning the embeddings. 
    JMDS, however, produces an unexpected hole 
    during the alignment of the S-bended shape
    and the Swiss role with hole.

   
\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
    \centering
        \begin{subfigure}[b]{0.28\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/Ex1/ex1-1.png}
            \end{subfigure}
            \caption*{Shapes}
        \end{subfigure}
        \begin{subfigure}[b]{0.28\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/Ex1/ex1-2.png}
            \end{subfigure}
            \caption*{Ours}
        \end{subfigure}
        \begin{subfigure}[b]{0.28\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/Ex1/ex1-3.png}
            \end{subfigure}
            \caption*{JMDS}
        \end{subfigure}
    \caption{S-bended rectangle and Swiss roll (without hole).
        }         
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \begin{subfigure}[b]{0.28\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/Ex1/ex12-1.png}
            \end{subfigure}
            \caption*{Shapes}
        \end{subfigure}
        \begin{subfigure}[b]{0.28\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/Ex1/ex12-2.png}
            \end{subfigure}
                \caption*{Ours}
        \end{subfigure}
        \begin{subfigure}[b]{0.28\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/Ex1/ex12-3.png}
            \end{subfigure}
                    \caption*{JMDS}
        \end{subfigure}
        \caption{S-bended rectangle and Swiss roll with hole.
                }
    \end{subfigure}
    \vspace{-10pt}
    \caption{Embedding and alignment of an S-bended rectangle and a Swiss roll into $\R^2$.
    For our method, 
    we visualize the marginals $P_{Z_i,\sharp} \alpha$ of the computed 4-plan $\alpha$ in \eqref{eq:as_fused}
    and compare them with JMDS.
    In the second example (b), JMDS produces an unexpected hole when embedding the S-bended surface.}
    \label{fig:manifolds-rolls}
\end{figure*}

    \paragraph{Human Shapes}
    
    Since we are not limited to isometries,
    we next consider an experiment
    where isometric embeddings are not possible.
    More precisely,
    we embed human shapes from the FAUST dataset \cite{faust} into $\R^2$.
    For this,
    we choose $(Z,d_Z)$ as an equispaced 60$\times$60 grid on $[0, 1.3]^2 \subset \R^2$
    and apply Algorithm~\ref{alg:unbalanced_gw}
    with $\lambda = 100$ and $\varepsilon = 4 \cdot 10^{-4}$.
    The obtained transport-based embeddings are shown in Figure~\ref{fig:manifolds-faust}.
    As comparison,
    we apply JMDS with $\lambda = 10$ and $\varepsilon = 4 \cdot 10^{-4}$.
    The 2d representations clearly resemble the 3d human shapes,
    but JMDS splits some of the extremities.


\begin{figure*}[t]
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \begin{subfigure}[b]{0.28\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/Ex2/ex22-1.png}
            \end{subfigure}
                \caption*{Shapes}
        \end{subfigure}
        \begin{subfigure}[b]{0.28\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/Ex2/ex22-2.png}
            \end{subfigure}
            \caption*{Ours}
        \end{subfigure}
        \begin{subfigure}[b]{0.28\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/Ex2/ex22-3.png}
            \end{subfigure}
            \caption*{JMDS}
        \end{subfigure}
        \caption{Same human shape in different poses. 
        }
    \end{subfigure}
    \hspace{\fill}
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \begin{subfigure}[b]{0.28\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/Ex2/ex21-1.png}
            \end{subfigure}
            \caption*{Shapes}
        \end{subfigure}
        \begin{subfigure}[b]{0.28\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/Ex2/ex21-2.png}
            \end{subfigure}
            \caption*{Ours}
        \end{subfigure}
        \begin{subfigure}[b]{0.28\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/Ex2/ex21-3.png}
            \end{subfigure}
            \caption*{JMDS}
        \end{subfigure}
    \caption{Distinct human shapes in different poses. 
        }
    \end{subfigure}
    \vspace{-10pt}
    \caption{Embedding and alignment of human shapes from the FAUST dataset into $\R^2$.
    For our method, 
    we visualize the marginals $P_{Z_i,\sharp} \alpha$ of the computed 4-plan $\alpha$ in \eqref{eq:as_fused}
    and compare them with JMDS.
    Here JMDS tends to split some of the extremities.}
    \label{fig:manifolds-faust}
\end{figure*}

    \paragraph{Spherical and Toroidal Embeddings}
Finally,
    we consider the alignment of spherical and toroidal subsets,
    i.e.,
    the joint embedding into a non-Euclidean space.
    More precisely,
    we consider the 3d shapes in the introductory Figure~\ref{fig:sphere_torus}.
    As target space $(Z, d_Z)$,
    we choose 
    a 30$\times$30 grid on the canonical parametrization 
    of the sphere $\mathbb{S}(2)$ 
    and the torus $\mathbb{T}(2) = \mathbb{S}(1) \times \mathbb{S}(1)$
    equipped with the corresponding geodesic distance. 
    Using $\lambda=10^3$ and $\varepsilon= 10^{-3}$ for Algorithm~\ref{alg:unbalanced_gw},
    we embed the spherical rectangle and a spherical cap, 
    both with a hole,
    onto the sphere,
    and the half-torus and toroidal triangle onto the torus.
    The relaxed embeddings are shown in Figure~\ref{fig:sphere_torus},
    where the color encodes the mass of the computed marginals.
    Up to a smoothing due to the incorporated entropic regularization,
    the transport plans 
    $\gamma_i = P_{X_i \times Z_i, \sharp} \alpha$
    correspond to the expected isometries.


\subsection{Alignment of Feature Spaces}\label{subsec:alignment}

    In the next example,
    we use our method to align different feature spaces 
    occurring in real-world data.
    More precisely,
    we consider the genetyping%
   ---the determination of the corresponding cell class---%
    of single cells
    from various measured modalities.
    Multimodal technologies
    as proposed in \cite{CCT+2016,CLZ2019}
    are, 
    however,
    uncommon
    such that 
    there is a rising interest in 
    embedding the corresponding features
    into a joint feature space
    \cite{demetci2022scot,liu2019jointly,chen2023unsupervised}.
    Following the experiments of \cite{demetci2022scot}
    for $n$ single cells,
    our first aim is to assign the recorded features 
    $X_1 \coloneqq \{x_1^1, \dots, x_1^n\} \subset \R^{d_1}$ 
    from one modality
    to the features 
    $X_2 \coloneqq \{x_2^1, \dots, x_2^n\} \subset \R^{d_2}$
    of another modality,
    i.e., to recover the underlying one-to-one correspondence
    between $x_1^j$ and $x_2^j$.
    Our second goal consists in 
     transferring a classifier
    from $X_1$ to $X_2$
    in an unsupervised manner.
    
    To achieve both goals,
    we apply the following methodology:
    \\
    1.~We equip the recorded, uncorrelated features 
    $X_1 \subset \R^{d_1}$
    and
    $X_2 \subset \R^{d_2}$
    with specific distances $d_{X_i}$
    based on the path length in a nearest-ten-neighbor graph,
    see \cite{cao2020unsupervised,demetci2022scot}.
    \\
    2.~We equip the constructed metric spaces with the uniform measure
    to obtain the mm-spaces $\XX_i \coloneqq (X_i, d_{X_i}, \xi_i)$.
    \\
    3.~Fixing a 20$\times$20 grid 
    $Z \coloneqq \{z^1, \dots, z^m\} \subset \R^2$
    over $[0,1]^2$
       equipped with the Euclidean metric,
    we apply Algorithm~\ref{alg:unbalanced_gw} 
    with the discretization in Section~\ref{discrete}
    to compute the 4-plan $\alpha$ in \eqref{eq:as_fused}.
    \\
    4.~Based on the transport plans $\gamma_i \coloneqq P_{X_i \times Z_i, \sharp} \alpha$,
    we pair $x_i^j$ with its barycentric projection $\tilde x_i^j \in \R^m$
    given by
    \begin{equation}
        \tilde{x}_i^j 
        \coloneqq 
        \sum_{k=1}^m \gamma_i^{jk} \, z_k 
        \biggm\slash
        \sum_{k=1}^m \gamma_i^{jk}.
    \end{equation}

    For the first aim,
    the identification of the underlying correspondence,
    we identify $x_1^j$ with $x_2^k$
    whose barycentric projection $\tilde x_2^k$
    is closest to $\tilde x_1^j$.
    To quantify the quality of the pairing,
    we rely on the FOSCTTM (fraction of samples closer than the true match)  
   score
    \cite{liu2019jointly}.
    More precisely, 
    FOSCTTM is defined by
    \begin{equation}
        \frac{1}{n^2} \,
        \sum_{j=1}^n
        \# \Bigl\{
            k \in \{1, \dots, n\}
            :
            \lVert \tilde x_1^j - \tilde x_2^k \rVert
            <
            \lVert \tilde x_1^j - \tilde x_2^j \rVert 
        \Bigr\}
    \end{equation}
    and ranges from 0 (perfect identification) to 1.

    For the second aim,
    the transfer of a classifier from $X_1$ to $X_2$,
    we exemplarily consider the k-nearest neighbor (KNN) method.  More precisely,   we use the barycentric projections $\{\tilde x_1^1, \dots, \tilde x_1^n\}$
    and the corresponding gene type labels
    of the first modality
    to classify a feature $x_2^k$ (more exact $\tilde x_2^k$)
    from the second modality,
    where we consider the closest five neighbors.
    Here,
    higher classification accuracies (KNN-Acc) indicate better class alignments.

    For the experiments,
    we employ the publicly available datasets%
    \footnote{\url{https://rsinghlab.github.io/SCOT/data/}}
    from \cite{demetci2022scot},
    where SNAREseq consists of $d_1 = 19$ and $d_2=10$ features of $n=1047$ single cells,
    and scGEM of $d_1=34$ and $d_2=27$ features of $n=177$ specimens.
        The computed embedding of $X_i$ into $\R^2$
    of our method and JMDS 
    are visualized in Figure~\ref{fig:data_em}.
    For FOSCTTM and KKN-Acc,
    we additionally compare both methods with
    the alignment techniques
    UnionCom \cite{cao2020unsupervised}
    and SCOT \cite{demetci2022scot}
    using the same distances on $X_i$.
    Note that 
    UnionCom relies on GUMA and t-SNE, 
    whereas SCOT relies on GW and MDS.
    The results are recorded in Table~\ref{tab:comparison_cell}.
    The hyperparameters of all methods are chosen 
    according to a grid search minimizing the FOSCTTM on a 10\% validation split.
    A further experiment with different feature spaces for MNIST and FashionMNIST is given in Appendix~\ref{app:C}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/DatasetEmbeddingCrossval/SNAREseq1.png}
            \end{subfigure}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/DatasetEmbeddingCrossval/SNAREseq_JMDS1.png}
            \end{subfigure}
        \end{subfigure}
        \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/DatasetEmbeddingCrossval/SNAREseq2.png}
            \end{subfigure}
                     \caption*{Ours}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/DatasetEmbeddingCrossval/SNAREseq_JMDS2.png}
            \end{subfigure}
                     \caption*{JMDS}
        \end{subfigure}
        \caption{SNAREseq dataset}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/DatasetEmbeddingCrossval/ScGEM1.png}
            \end{subfigure}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/DatasetEmbeddingCrossval/ScGEM_JMDS1.png}
            \end{subfigure}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/DatasetEmbeddingCrossval/ScGEM2.png}
            \end{subfigure}
                     \caption*{Ours}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/DatasetEmbeddingCrossval/ScGEM_JMDS2.png}
            \end{subfigure}
                     \caption*{JMDS}
        \end{subfigure}
        \caption{scGEM dataset}
        \end{subfigure}
        \vspace{-10pt}
    \caption{Joint embedding of two feature spaces into $\R^2$ using our method and JMDS.
    (Top:  first feature space, Bottom: second feature space)
    Both methods align the color-coded classes.}
    \label{fig:data_em}
\end{figure}



\begin{table}
    \centering
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{lcccc}
            \toprule
            \multirow{2}{*}{} & \multicolumn{2}{c}{SNAREseq} & \multicolumn{2}{c}{scGEM} \\
            \cmidrule(lr){2-3} \cmidrule(lr){4-5}
          & FOSCTTM$\downarrow$ & KNN-Acc$\uparrow$ & FOSCTTM$\downarrow$ & KNN-Acc$\uparrow$ \\
            \midrule
            ${\EW}_\lambda$ (Ours) &\textbf{0.000} & {0.877} & \textbf{0.000} & \textbf{0.679} \\
            JMDS  \cite{chen2023unsupervised} & 0.026 & \textbf{0.896} & 0.174 & 0.491 \\
            SCOT \cite{demetci2022scot}  & {0.596} & {0.089} & 0.248 & 0.516  \\
            UnionCom \cite{cao2020unsupervised}  & 0.257 & 0.536 & 0.373 & 0.270  \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison of the joint embedding of two feature spaces into $\R^2$ using our method, JMDS, SCOT, and UnionCom.}
    \label{tab:comparison_cell}
\end{table}

\subsection{Alignment of Gaussian Mixture Models}

    In the final example,
    inspired by \cite{salmona2023gromov},
    we want to align Gaussian mixture models (GMMs).
    For this,
    we consider the space of 2d Gaussians
    $\mathcal{N}_2 = \{\mathcal{N}(\mu, \Sigma)| \mu \in \R^2, \Sigma \in \R^{2\times 2}\text{ spd.}\}$
    equipped with the Wasserstein distance.
    A GMM can now be interpreted as mm-space $\XX = (\mathcal{N}_2, \W, \xi)$
    with discrete measure
    $\xi \coloneqq \sum_{j=1}^{n_1} \xi_j \, \delta_{\mu_j, \Sigma_j}$.
For the experiment, 
we fit two GMMs 
to affine-transformed 2d datasets using the expectation-maximization algorithm.
Here,
we employ the datasets ``Blobs'' and  ``Moons'' from scikit-learn \cite{pedregosa2011scikit},
    see Figure~\ref{fig:gmm}. 
    For the alignment of the GMMs,
    we choose $Z \subset \mathcal{N}_2$,
    where the means form a 15$\times$15 grid over $[0, 1]^2$ 
    and we use a coarse grid over suitable matrices.
    Particularly, we consider matrices
    \begin{equation}
    \Sigma = r^2
    \begin{bmatrix}
    \sigma_{1}^{2} & \sigma_{12} \\
    \sigma_{12} & \sigma_{2}^{2}
    \end{bmatrix},
    \end{equation}
    where we choose $\sigma_1^2, \sigma_2^2 \in \{0.8, 1.\}$ 
    and $\sigma_{12} \in \{-0.2, 0., 0.2\}$. 
    The parameter $r^2 > 0$ corresponds to the mean variance of the considered GMMs.
    Applying Algorithm~\ref{alg:unbalanced_gw} to compute $\alpha$ in \eqref{eq:as_fused},
    and considering the marginals $\zeta_i \coloneqq P_{Z_i, \sharp} \alpha$,
    which are again discrete measures on $\mathcal N_2$,
    we obtain the aligned GMMs in Figure~\ref{fig:gmm}.


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/gmm/blobs1.png}
            \end{subfigure}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/gmm/blob1_em.png}
            \end{subfigure}
        \end{subfigure}
        \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/gmm/blobs2.png}
            \end{subfigure}
                     \caption*{Original}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/gmm/blob2_em.png}
            \end{subfigure}
                     \caption*{Aligned}
        \end{subfigure}
        \caption{Tri-modal GMM}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/gmm/moons1.png}
            \end{subfigure}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/gmm/moon1_em.png}
            \end{subfigure}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/gmm/moons2.png}
            \end{subfigure}
                     \caption*{Original}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/gmm/moon2_em.png}
            \end{subfigure}
                     \caption*{Aligned}
        \end{subfigure}
        \caption{``Moons'' GMM}
        \end{subfigure}
        \vspace{-10pt}
    \caption{Aligenment of GMMs with respect to the Wasserstein distance.}
    \label{fig:gmm}
\end{figure}


\section{Conclusion}

We propose an unbalanced OT framework
with GW marginal penalization 
which enables the joint embedding of two datasets based on pairwise intra-dataset distances. 
While our model can handle the aligned transfer into arbitrary metric spaces, it relies on their appropriate definition.
In particular, due to computational restrictions,
working on a grid in $\R^d$ is restricted to small dimensions $d$. 
 As a future research direction, we are interested in developing 
 a non-Euclidean free-support solver 
 based on existing free-support GW barycenter algorithms \cite{PCS2016,vayer2020fused,beier2024tangential}.
 This may combine our EW$_\lambda$ approach with
 a non-Euclidean version of JMDS.
\newpage


 
\bibliography{references}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn

\section{Proofs}\label{app:A}
\paragraph{Proof of Proposition \ref{prop:metric}}
    The main idea of the proof follows the argumentation for the original embedded Wasserstein distance on Euclidean spaces in \cite{salmona2023gromov},
    which we adapt to arbitrary metric spaces. 
    The definiteness follows directly from the definition of the isomorphic equivalence classes, 
    the isometries in EW,
    and the definiteness of the Wasserstein distance.
    The symmetry is inherited from the Wasserstein distance too. 
    It remains to show the triangle inequality.
    Without loss of generality,
    we take representatives $\XX_i \coloneqq (Z, d_Z, \xi_i)$,
    $i=1,\dots,3$,
    such that $\xi_i \in \p(Z)$ possess full support.
    Since the inverse and composition of isometries are isometry, 
    we have 
    \begin{equation*}
        \EW(\XX_1, \XX_2) 
        = \inf_{I_i : Z \hookrightarrow Z} \W(I_{1,\sharp} \xi_1, I_{2,\sharp} \xi_2)
        = \inf_{I_i : Z \hookrightarrow Z} \W(\xi_1, (I_1^{-1} \circ I_2)_\sharp \xi_2)
        = \inf_{I : Z \hookrightarrow Z} \W(\xi_1, I_{\sharp}\xi_2).
    \end{equation*}
    Using the triangle inequality of the Wasserstein distance,
    we obtain
    \begin{align*}
        \EW(\XX_1, \XX_2) 
        &= 
        \inf_{I:Z \hookrightarrow Z } 
        \W(\xi_1, I_{\sharp}\xi_2) 
        \leq \inf_{I, J:Z \hookrightarrow Z} 
        \W(\xi_1, J_\sharp \xi_3) 
        + \W(J_\sharp \xi_3, I_\sharp \xi_2)
        \\
        &=  
        \inf_{J:Z \hookrightarrow Z} \Bigl(
        \W(\xi_1,J_\sharp \xi_3) 
        + \inf_{I:Z \hookrightarrow Z}
        \W(J_\sharp \xi_3, I_\sharp \xi_2) \Bigr)
        =  
        \inf_{J:Z \hookrightarrow Z} \Bigl(
        \W(\xi_1, J_\sharp \xi_3) +  
        \inf_{I:Z \hookrightarrow Z}
        \W(\xi_3, (J^{-1} \circ I)_\sharp \xi_2) \Bigr)
        \\
        &=
        \inf_{J:Z \hookrightarrow Z} 
        \W(\xi_1, J_\sharp \xi_3) +  
        \inf_{K:Z \hookrightarrow Z} 
        \W(\xi_3, K_\sharp \xi_2)
        = 
        \EW(\XX_1, \XX_3) + \EW(\XX_3, \XX_2).
        \tag*{$\Box$}
    \end{align*}

\textbf{Proof of Proposition~\ref{prop1}.}
Let $X \in \{X_i: i=1,2\}$.
We equip the space of continuous functions
$C(X,Z)$
with the topology of uniform convergence
$d(f,g) \coloneqq \max_{x \in X} d_Z(f(x),g(x))$.

1. First, we we show that 
$$
\Iso(X,Z)
\coloneqq 
\{I: X \hookrightarrow Z\}
\subset C(X,Z)
$$
is compact.
Since $(Z,d_Z)$ 
is compact, we see immediately that
$\Iso(X,Z)$ is pointwise bounded.
Moreover,
since all functions in $\Iso(X,Z)$
are isometries
the set is equicontinuous.
By the Theorem of ArzelÃ --Ascoli
\cite{munkres2000topology} (Thm.~47.1),
this gives the relative compactness 
of $\Iso$.
Further, $\Iso(X,Z)  \subset C(X,Z)$
is closed and thus compact, since we have for any sequence of isometies
$(I_n)_n$ from $X$ to $Z$
converging to some $I \in C(X,Z)$ that
\[
d_{X}(x,x')
= d_Z(I_n(x),I_n(x'))
\rightarrow d_Z(I(x),I(x'))
\]
as $n \to \infty$, so that $I \in \Iso(X,Z)$.
Then also 
$\Iso(X_1,Z) \times \Iso(X_2,Z)$
is compact.
\\
2.
Next,
we show that the functional 
$$F(I_1,I_2) \coloneqq \W(I_{1,\sharp} \xi_1,I_{2,\sharp} \xi_2)$$
in \eqref{eq:Z_SGW}
is lower semi-continuous on $\Iso(X_1,Z) \times \Iso(X_2,Z)$.
For $i=1,2$, let $(I_{i,n})_{n \in \N} \subset \Iso(X_i,Z)$
and $I_i \in \Iso(X_1,Z)$
with $I_{i,n} \to I_i$ as $n \to \infty$.
Then,
for all $\varphi \in C(Z,\R)$,
it holds by the dominated convergence theorem that
\begin{align}
&\int_{Z} \varphi \dx (I_{i,n})_\sharp \xi_i 
= \int_{X} \varphi \circ I_{i,n} \dx \xi_i 
\\
&\rightarrow
\int_{X_i} \varphi \circ I_i \dx \xi_i 
=
\int_{Z} \varphi \dx I_{i,\sharp}\xi_i \; \text{ as } \; n \to \infty.
\end{align}
Thus we obtain the weak convergence
$(I_{i,n})_\sharp \xi_i 
\weakly I_{i,\sharp}\xi_i$.
Finally,
due to the joint weak lower semi-continuity
of the Wasserstein distance 
\cite{ambrosio2005} (Lemma.~7.1.4),
we get the desired lower semi-continuity of $F$.

Finally,  application of the Weierstrass theorem
yields the assertion.
\hfill $\Box$

\textbf{Proof of Proposition~\ref{thm:RGW_existence}.}
Since  $\p(Z \times Z)$ is weakly compact,
the existence of a minimizer
follows by Weierstrass' theorem,
see, e.g., \cite{mordukhovich2022convex} (Thm.~2.164),
once we can show that 
the objective function
\begin{align}
F(\pi) 
&\coloneqq
\int_{Z \times Z} d_Z^2(z,z') \dx \pi(z,z')
+ \lambda \sum_{i=1}^2
\GW^2(\XX_i,(Z,d_Z,P_{i,\sharp} \pi))
\end{align}
in \eqref{eq:RGW}
is weakly lower semi-continuous.

Let $(\pi_n)_{n \in \N} \subset \p(Z \times Z)$
and $\pi \in \p(Z \times Z)$
be such that
$\pi_n \weakly \pi$ as
$n \to \infty$.
We handle the terms of $F$
separately.
Since $d_Z^2$ is continuous in both arguments, we obtain 
by definition of the weak convergence
that
\[
\int_{Z \times Z} d_Z^2(z,z') \dx \pi_n(z,z')
\rightarrow
\int_{Z \times Z} d_Z^2(z,z') \dx \pi(z,z')
\]
as $n \to \infty$.
We turn to the GW terms.
For $i=1,2$, set
$$\mu_{i,n} 
\coloneqq P_{i,\sharp} \pi_n
\quad \text{and} \quad 
\mu_{i} 
\coloneqq P_{i,\sharp} \pi.
$$
Then the weak lower semi-continuity
of the marginal projection operators
yields 
$\mu_{i,n} \weakly \mu_{i}$ as $n \to \infty$.
Now let $\gamma_{i,n} \in \Pi(\xi_i,\mu_{i,n})$
be a solution of 
$\GW(\XX_i,(Z,d_Z,\mu_{i,n}))$,
$n \in \N$.
As $(\gamma_{i,n})_{n \in \N} \subset \p(X_i \times Z)$
and the latter is weakly compact,
we can choose a converging subsequence
again denoted by 
$(\gamma{i,n})_{n\in\N}$,
such that 
$\gamma_{i,n} \weakly \gamma_i \in \p(X_i \times Z)$.
Using again the weak continuity 
of the marginal projection,
we get
$\gamma \in \Pi(\xi_i,\mu_i)$.
Therefore,
it holds
\begin{align*}
&\GW^2(\XX_i, (Z,d_{Z},P_{i,\sharp} \pi))
=\GW^2(\XX_i, (Z,d_{Z},\mu_i))
\leq
\iint\limits_{(X_i \times Z)^2}
(d_{X_i}(x_i,x_i') - d_Z(z,z'))^2
\dx \gamma(x_i,z)
\dx \gamma(x_i',z')
\end{align*}
and 
by
continuity of the integrand
and the fact that 
$(\gamma_{i,n} \otimes \gamma_{i,n}) 
\weakly (\gamma_i \otimes \gamma_i)$,
see, e.g., \cite{bogachev_weak} (Prop.~2.7.8) further
\begin{align*}
\GW^2(\XX_i, (Z,d_{Z},P_{i,\sharp} \pi))
&\le
\lim_{n \to \infty} \!
\iint_{(X_i \times Z)^2}
(d_{X_i}(x_i,x_i') - d_Z(z,z'))^2
 \dx \gamma_{i,n}(x_i,z)
\dx \gamma_{i,n}(x_i',z')
\\
&= 
\lim_{n \to \infty}
\GW^2(\XX_i,(Z,d_{Z},\mu_{i,n}))
=
\lim_{n \to \infty}
\GW^2(\XX_i,(Z,d_{Z}, P_{i,\sharp} \pi_n)).
\end{align*}
This gives lower semi-continuity
of the functional of $F$.
\hfill $\Box$

\textbf{Proof of Proposition~\ref{thm:limit}.}
1. First, we show that
\begin{align}\label{eq:p2}
   \EW_\lambda(\XX_1,\XX_2)
    \le    \EW(\XX_1,\XX_2), \qquad \lambda >0.
\end{align}
To this end, let 
$I_i : X_i \hookrightarrow Z_i$
be isometries realizing $\EW(\XX_1,\XX_2)$,
i.e.,
\[
\EW(\XX_1,\XX_2)
= \W (I_{1,\sharp} \xi_1, I_{2,\sharp} \xi_2).
\]
By definition of the GW distance,
we have
$\GW(\XX_i,(Z_i,d_{Z_i},I_{i,\sharp} \xi_i) = 0$, $i=1,2$.
Then we obtain for
$\tilde \pi \in \Pi_0(I_{1,\sharp} \xi_1,I_{2,\sharp} \xi_2)$
 that
\begin{align*}
    \EW_\lambda(\XX_1,\XX_2)
       &\leq 
    \Big(\int_{Z_1 \times Z_2} 
    d_Z^2(z,z')
    \dx \tilde \pi(z,z')
    \Big)^{\frac{1}{2}}
    = 
    \W(I_{1,\sharp} \xi_1, I_{2,\sharp} \xi_2)
    = 
    \EW_{(Z,d_Z)}(\XX_1,\XX_2).
\end{align*}
2. Now let $(\lambda_n)_{n \in \N}$ with
$\lambda_{n} \to \infty$ as $n \to \infty$, and
let $\pi_n \in \p(Z \times Z)$
realize
$\EW_{\lambda_n}(\XX_1,\XX_2)$.
Then we conclude by \eqref{eq:p2} that
\begin{align*}
\EW(\XX_1,\XX_2) 
\geq \EW_{\lambda_n}(\XX_1,\XX_2)
&=
\int_{Z \times Z} d_Z (z,z') \dx \pi_n(z,z') 
+ \lambda_{n} \sum_{i=1}^2 \GW(\XX_i,(Z,d_Z, P_{i,\sharp} \pi_n))
\\
&\geq 
\lambda_{n} \sum_{i=1}^2 \GW(\XX_i,(Z,d_Z,(P_i)_\# \pi_n)).
\end{align*}
Hence, we obtain for $i=1,2$ that 
$$
\GW(\XX_i,(Z,d_Z,(P_i)_\# \pi_n)) \to 0 \; \text{ as } \; n \to \infty.
$$
Since $(\pi_n)_{n \in \N}$
is contained in the weakly compact set
$\p(Z \times Z)$,
there exists a subsequence (denoted in the same way)
which weakly converges to some $\pi \in \p(Z \times Z)$.
Then also $P_{i, \sharp} \pi_n$ converges weakly to 
$P_{i, \sharp} \pi$, $i=1,2$ and we obtain
as in the proof of \eqref{thm:RGW_existence},
that 
\[
\GW(\XX_i,(Z,d_Z,P_{i,\sharp}\pi))
\leq 
\lim_{n \to \infty}
\GW(\XX_i,(Z,d_Z,P_{i,\sharp} \pi_n))
= 0.
\]
Thus, there exist isometries 
$I_i:X_i \hookrightarrow Z$
with
$P_{i,\sharp} \pi = I_{i,\sharp} \xi_i$, $i=1,2$, i.e.,
$\pi \in \Pi(I_{1,\sharp} \xi_1, I_{2,\sharp} \xi_2)$.
Finally, we obtain for any fixed $\lambda >0$ that
\begin{align*}
\EW(\XX_1,\XX_2)
&\leq
\Big(
\int_{Z_1 \times Z_2}
d_Z^2(z_1,z_2) \dx \pi(z_1,z_2)
\Big)^{\frac{1}{2}}
\\
&=
\Big(
\int_{Z_1 \times Z_2}
d_Z^2(z_1,z_2) \dx \pi(z_1,z_2)
+ \lambda \sum_{i=1}^2 
\underbrace{\GW(\XX_i,(Z,d_Z,P_{i,\sharp} \pi))}_{=0}
\Big)^{\frac{1}{2}}
\\
&\le
\lim_{n \to \infty}
\Big(
\int_{Z_1 \times Z_2}
d_Z^2(z_1,z_2) \dx \pi_n(z_1,z_2)
+ \lambda \sum_{i=1}^2 
\GW(\XX_i,(Z,d_Z, P_{i,\sharp} \pi_n))
\Big)^{\frac{1}{2}}
\\
&\le
\lim_{n \to \infty} \EW_{\lambda_n}(\XX_1,\XX_2)
\leq
\EW(\XX_1,\XX_2).
\end{align*}
Hence $\pi \in \Pi(I_{1,\sharp} \xi_1, I_{2,\sharp} \xi_2)$ and
\begin{align}
\EW(\XX_1,\XX_2) &= \inf_{
\substack{
J_i: X_i \hookrightarrow Z, i=1,2
}
} 
\W(J_{1,\sharp} \xi_1, J_{2,\sharp} \xi_2)
= 
\Big(
\int_{Z_1 \times Z_2}
d_Z^2(z_1,z_2) \dx \pi(z_1,z_2)
\Big)^{\frac{1}{2}}
\end{align}
implies that
$(I_1,I_2)$ minimizes
$\EW(\XX_1,\XX_2)$
and $\pi$
realizes
$\W(I_{1,\sharp} \xi_1,I_{2,\sharp} \xi_2)$.
\hfill $\Box$


\section{Numerical Studies on GW, EW, and EW$_{\boldsymbol{\lambda}}$}\label{app:B}

Our approach enables geometrically meaningful data comparison 
as we can approximate the EW metric on general metric spaces $Z$. 
We validate this in two experiments.
\paragraph{Approximation of $\EW$ by $\EW_\lambda$ for Synthetic Circular Data}
Consider circular mm-spaces $\XX_i = (\mathbb{S}^1, d_{\mathbb S^1}, \xi_i)$ with $(Z, d_Z) = (\mathbb{S}^1,d_{\mathbb S^1})$, 
where ($\mathbb{S}^1$, $d_{\mathbb S^1}$) denotes the 
circle equipped with a circular distance. 
We set $\xi_1$ uniformly distributed and $\xi_2$ according to the density of a von Mises distribution 
with increasing dispersion parameter $\kappa$. 
Note that the von Mises distribution takes the form of a uniform distribution for $\kappa = 0$ 
and  becomes more concentrated for $\kappa \to \infty$. 
Due to the rotational invariance of the uniform distribution on the circle, the embedded Wasserstein distance can easily be calculated in this case as it coincides with the Wasserstein distance. Thus, we can ignore the isometry in $\EW$ and directly calculate $\EW$ by solving the linear program underlying the OT problem. 
We discretize the circle into 360 bins and estimate $\EW_\lambda$ for different choices of $\kappa$ and $\lambda$. 
The results in Figure~\ref{fig:circle} show an improved approximation of $\EW$ for larger $\lambda$ as announced in Proposition \ref{thm:limit}. Indeed, we have an excellent fit for $\lambda=20$, whereas $\lambda=0.2$ leads to an underestimation.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.25\linewidth}
        \centering
        \begin{subfigure}[b]{\linewidth}
            \includegraphics[width=\textwidth]{imgs/circle_syn/EW.png}
        \end{subfigure}
        \caption*{$\EW^2$ ($\lambda=\infty$)}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\linewidth}
        \centering
        \begin{subfigure}[b]{\linewidth}
            \includegraphics[width=\textwidth]{imgs/circle_syn/EW_20.png}
        \end{subfigure}
        \caption*{$\EW^2_\lambda$ ($\lambda=20$)}
    \end{subfigure}
                    \begin{subfigure}[b]{0.25\linewidth}
        \centering
        \begin{subfigure}[b]{\linewidth}
            \includegraphics[width=\textwidth]{imgs/circle_syn/EW_0.2.png}
        \end{subfigure}
        \caption*{$\EW^2_\lambda$ ($\lambda=0.2$)}
    \end{subfigure}
    \caption{Comparison of $\EW$ with $\EW_\lambda$ for different $\lambda$. While the curves for $\EW$ (left)
    and $\EW_{20}$ (middle) are almost identical, those for $\EW_{0.2}$ (right) is consistently smaller.}
    \label{fig:circle}
\end{figure}

\paragraph{2d Shape Matching}
Next,  we compare randomly rotated gray-value images. 
We can describe such images as mm-spaces $\XX_i = (X, d, \xi_i)$ 
where $X \subset \R^2$ is a grid that describes the pixel positions 
equipped with the Euclidean metric and $\xi_i$ is the pixel intensity, see  \cite{BBS2022multi}. 
We use $(Z, d_Z) = (X, d)$.
We apply random affine transformations, i.e., translations and rotations, 
to the first 10 FashionMNIST \cite{FMNIST} training images from the classes ``Trouser'', ``Pullover'' and ``Sneaker'', respectively.  
Then we compute $\W$, $\GW$ and
$\EW_\lambda$ for $\lambda = 20$. The results are displayed in Figure~\ref{fig:shape}. 
As expected, the Wasserstein distance cannot recover the class structure due to the affine transformations, whereas
$\EW_\lambda$ and  GW  capture the class structure. 
Moreover, we see that the distance matrices of $\EW_\lambda$ and GW display almost identical patterns, up to scaling. 

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \begin{subfigure}[b]{\linewidth}
            \includegraphics[width=\textwidth]{imgs/FashionMNIST-Shapes/ew_FashionShapes.png}
        \end{subfigure}
        \caption*{$\EW_\lambda$ ($\lambda=20$)}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \begin{subfigure}[b]{\linewidth}
            \includegraphics[width=\textwidth]{imgs/FashionMNIST-Shapes/gw_FashionShapes.png}
        \end{subfigure}
        \caption*{$\GW$}
    \end{subfigure}
            \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \begin{subfigure}[b]{\linewidth}
            \includegraphics[width=\textwidth]{imgs/FashionMNIST-Shapes/w_FashionShapes.png}
        \end{subfigure}
        \caption*{$\W$}
    \end{subfigure}
    \caption{Pairwise $\W$, $\GW$ and $\EW_{20}$ distances of  10 randomly rotated and translated images from each of the ``Trouser'' (T), ``Pullover'' (P), and ``Sneaker'' (S) FashionMNIST classes. Smaller distances are darker. The class block structure is recovered with $\EW_\lambda$ and $\GW$, but not with Wasserstein.}
    \label{fig:shape}
\end{figure}


\section{Further Example for Comparing Feature Spaces} \label{app:C}

As an addition to the genetyping experiment, we present another experiment with real-world data based on latent spaces. 
We consider the task of comparing 
the 4d latent spaces of 
an auto-encoder (AE) versus a variational auto-encoder (VAE), trained 
on FashionMNIST \cite{FMNIST}. 
For this purpose, we train a simple convolutional AE with two convolutional layers and one linear layer for the encoder and the decoder. 
We train the AE with the MSE loss and the VAE with the log-likelihood loss for 10 epochs on the canonical training data splits. 
Subsequently, we embed the test data using the resulting AE and VAE into the 4d latent spaces. 
Note that such distinct AE trained on the same dataset generally produce incomparable embeddings. 
For our experiment, we consider the first 100 data points of the test split. We choose $Z$ as an equispaced 20$\times$20 grid on $[0, 1.3] \subset \R^2$ with the Euclidean distance.
We follow the same evaluation routine as in Subsection \ref{subsec:alignment} for parameter selection and visualization.
The joint embeddings are visualized in Figure \ref{fig:data_ae_em}. 
Comparing FOSCTTM and KNN,  we see again a good embedding quality for our approach, see the figure caption.

\begin{SCfigure}[4][h]
    \centering
    \begin{subfigure}[b]{0.25\linewidth}
        \centering
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/DatasetEmbeddingCrossval/FMNIST1.png}
            \end{subfigure}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/DatasetEmbeddingCrossval/FMNIST_JMDS1.png}
            \end{subfigure}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/DatasetEmbeddingCrossval/FMNIST2.png}
            \end{subfigure}
                     \caption*{Ours}
        \end{subfigure}
            \begin{subfigure}[b]{0.47\linewidth}
            \centering
            \begin{subfigure}[b]{\linewidth}
                \includegraphics[width=\textwidth]{imgs/DatasetEmbeddingCrossval/FMNIST_JMDS2.png}
            \end{subfigure}
                     \caption*{JMDS}
        \end{subfigure}
        \end{subfigure}
        \vspace{-10pt}
    \caption{Joint embedding of the FashionMNIST latent space of an AE and a VAE into $\R^2$ using our method and JMDS.
    The AE latent space is on the top and the VAE latent space is on the bottom.
    Both methods align the color-coded classes. A quantitative comparison shows that our model achieves a better FOSCTTM (0.040) than JMDS (0.042),
    whereas our KNN-Accuracy (0.589) is slightly worse than the one of JMDS (0.644).}
    \label{fig:data_ae_em}
\end{SCfigure}

\end{document}
