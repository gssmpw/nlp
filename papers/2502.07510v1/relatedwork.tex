\section{Previous Work}
As a first step, we highlight the most relevant contributions related to our study among the vast literature on OT and dimensionality reduction.
\\[1ex]
\textbf{Optimal Transport with Invariances.} 
Optimal transport and Wasserstein distances enable comparisons of point clouds in metric spaces. 
However, 
classic OT
lacks invariance to important transformations 
like rotations in Euclidean space. 
To address this,
Gromov--Wasserstein (GW) distances 
are introduced which allow 
for the comparison of
measures from distinct metric spaces 
\cite{memoli2011gromov,sturm2006}. 
In \cite{alaya2022theoretical},
an approximation of GW distances is obtained
by jointly embedding measures 
into Euclidean spaces. 
Furthermore,
projection- and
subspace-robust Wasserstein-2 distances
are introduced in 
\cite{paty2019subspace}. 
Another approach incorporates invariance 
to Euclidean isometries 
via the Wasserstein Procrustes problem \cite{grave2019unsupervised}.
This is extended to account 
for linear operators 
with bounded Schatten norms in \cite{alvarez2019towards} 
and to Gaussian mixture applications in \cite{salmona2023gromov}. 
As outlined in Section~\ref{discrete}, our paper extends such invariant OT to non-Euclidean domains.
\\[1ex]
\textbf{Joint Dimensionality Reduction.} 
Dimensionality reduction is a core topic in machine learning enabling visualization and clustering by finding optimal low-dimensional representations of data. Classical methods like principal component analysis (PCA) \cite{greenacre2022principal} and multidimensional scaling (MDS) \cite{carroll1998multidimensional} preserve large variations or pairwise distances, but fail on nonlinear manifolds \cite{alaya2022theoretical,dengneuc}. 
Nonlinear approaches, e.g., locally linear embedding \cite{roweis2000nonlinear}, 
probabilistic models, e.g., t-distributed stochastic neighbor embedding (t-SNE) \cite{van2008visualizing}, and deep learning methods, e.g., variational autoencoders (VAEs), \cite{kingma2019introduction} address these challenges. 
As an extension, several recent methods focus on joint embeddings of heterogeneous data. Here, we are given data on two incompatible domains and are interested in simultaneous embedding. The manifold-aligning generative adversarial network \cite{amodio2018magan} employs a generative adversarial network for domain alignment. Maximum mean discrepancy (MMD) manifold-alignment \cite{liu2019jointly} balances an MMD and a distortion term. UnionCom \cite{cao2020unsupervised} leverages the generalized unsupervised manifold alignment (GUMA) \cite{cui2014generalized}.  Single-cell alignment with optimal transport (SCOT) \cite{demetci2022scot} and the partial manifold alignment algorithm \cite{cao2022manifold} employ GW distances. Finally, the recently proposed joint multidimensional scaling (JMDS) \cite{chen2023unsupervised} algorithm combines MDS with OT for the joint embedding of two 
datasets into a shared Euclidean space. Complementing this and the work on non-Euclidean embeddings in \cite{mcinnes2018umap,dengneuc}, we propose a joint embedding into arbitrary metric spaces based on GW distances.