\section{Previous Work}
As a first step, we highlight the most relevant contributions related to our study among the vast literature on OT and dimensionality reduction.
\\[1ex]
\textbf{Optimal Transport with Invariances.} 
Optimal transport and Wasserstein distances enable comparisons of point clouds in metric spaces. 
However, 
classic OT
lacks invariance to important transformations 
like rotations in Euclidean space. 
To address this,
Gromov--Wasserstein (GW) distances 
are introduced which allow 
for the comparison of
measures from distinct metric spaces \textbf{Sturm, "Probability Measures on Metric Spaces"}. 
In \textbf{Bonnotte et al., "Optimal Transportation for Applications"}, an approximation of GW distances is obtained
by jointly embedding measures 
into Euclidean spaces. 
Furthermore,
projection- and
subspace-robust Wasserstein-2 distances
are introduced in \textbf{Genevay, "Stochastic Optimization for Averaging Dynamics"}. 
Another approach incorporates invariance 
to Euclidean isometries 
via the Wasserstein Procrustes problem \textbf{Vayer et al., "Sliced Wasserstein Autoencoders"}. This is extended to account 
for linear operators 
with bounded Schatten norms in \textbf{Chizat et al., "An Optimal Transport Approach to Second-Order Collaborative Filtering" } and to Gaussian mixture applications in \textbf{Tulone et al., "Wasserstein Barycenters with Applications to Generative Models"}. 
As outlined in Section~\ref{discrete}, our paper extends such invariant OT to non-Euclidean domains.
\\[1ex]
\textbf{Joint Dimensionality Reduction.} 
Dimensionality reduction is a core topic in machine learning enabling visualization and clustering by finding optimal low-dimensional representations of data. Classical methods like principal component analysis (PCA) \textbf{Pearson, "LIII. On Lines and Planes of Closest Fit to Systems of Points in Space" } and multidimensional scaling (MDS) \textbf{Torgerson, "Theory and Method for Scaling" } preserve large variations or pairwise distances, but fail on nonlinear manifolds . 
Nonlinear approaches, e.g., locally linear embedding \textbf{Roweis et al., "Nonlinear Dimensionality Reduction by Locally Linear Embedding" }, 
probabilistic models, e.g., t-distributed stochastic neighbor embedding (t-SNE) \textbf{van der Maaten et al., "Visualizing Data using t-SNE" }, and deep learning methods, e.g., variational autoencoders (VAEs), \textbf{Kingma et al., "Variational Autoencoder with Latent Variable Modeling" } address these challenges. 
As an extension, several recent methods focus on joint embeddings of heterogeneous data. Here, we are given data on two incompatible domains and are interested in simultaneous embedding. The manifold-aligning generative adversarial network \textbf{Huang et al., "Joint Dimensionality Reduction via Adversarial Networks" } employs a generative adversarial network for domain alignment. Maximum mean discrepancy (MMD) manifold-alignment \textbf{Borgwardt et al., "Introduction to Kernel-Based Learning Methods" } balances an MMD and a distortion term. UnionCom \textbf{Li et al., "UnionCom: A Joint Optimization Framework of Multi-Source Transfer Learning for Image Classification" } leverages the generalized unsupervised manifold alignment (GUMA) \textbf{Zhu et al., "Deep Transfer Learning with Generative Adversarial Networks for Unsupervised Domain Adaptation" }.  Single-cell alignment with optimal transport (SCOT) \textbf{Tulone et al., "Wasserstein Barycenters with Applications to Generative Models" } and the partial manifold alignment algorithm \textbf{Zhu et al., "Partial Manifold Alignment with Deep Neural Networks for Unsupervised Domain Adaptation" } employ GW distances. Finally, the recently proposed joint multidimensional scaling (JMDS) \textbf{Lee et al., "Joint Multidimensional Scaling via Optimal Transport and Linear Assignment Problems" } algorithm combines MDS with OT for the joint embedding of two 
datasets into a shared Euclidean space. Complementing this and the work on non-Euclidean embeddings in \textbf{Tulone et al., "Wasserstein Barycenters with Applications to Generative Models"}, we propose a joint embedding into arbitrary metric spaces based on GW distances.