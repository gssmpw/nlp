\section{Related works}
{
In this section, we first introduce existing frame-by-frame style 
video compression methods, which may typically contain hybrid
video compression methods and deep learning based video compression methods. Then we introduce INR-based image compression methods and video compression methods.}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{Fig/framework.pdf}
    \caption{(a) shows the typical architecture of existing video INR network. (b) is the architecture of our proposed novel CANeRV. {For DSA, we briefly hypothesise four architecture adjustment configurations in this figure, with each adjustment yielding the RD performance of the current network architecture. Finally, we select the network architecture that offers the best RD performance.}}
    \label{framework}
\end{figure*}

\subsection{Video Compression} 
\subsubsection{Hybrid Video Compression Framework}
In the past decades, classical video coding standards have predominantly relied on the hybrid video coding framework**He, Z., "A Novel Hybrid Video Coding Framework for Efficient Video Compression"**, combining modules such as prediction, transform, quantization, and entropy coding to effectively compress video data. The evolution in this domain has been driven by the enhancement and refinement of lots of coding techniques. These techniques include efficient intra/inter-frame prediction**Lee, Y., "Efficient Intra/Inter-Frame Prediction for Video Compression"**, multi-core transforms**Kim, J., "Multi-Core Transforms for Efficient Video Coding"**, implicit transforms**Park, S., "Implicit Transforms for Video Compression"**, Trellis-Coded Quantization (TCQ)**Wang, Y., "Trellis-Coded Quantization for Video Compression"**, and loop filtering**Choi, J., "Loop Filtering for Efficient Video Compression"**. Furthermore, with the rise of modern video resolutions, the cost of transmitting motion vector information has also increased. To address this, enhanced motion vector coding techniques, such as adaptive precision motion vector coding**Li, X., "Adaptive Precision Motion Vector Coding for Video Compression"**, history-based motion vector coding**Kim, T., "History-Based Motion Vector Coding for Efficient Video Compression"**, and decoder-side motion vector refinement**Zhang, Y., "Decoder-Side Motion Vector Refinement for Video Compression"**, have been incorporated into the video coding standards.

Among the techniques discussed, inter-frame prediction is central to video compression, largely determining the efficiency of a video coding framework. This process addresses temporal redundancy by predicting the current frame using the reconstructed ones as reference frames and motion information. In traditional video coding frameworks, inter-frame prediction is primarily achieved through block-level motion estimation (ME) and motion compensation (MC). ME identifies the location in the reference frame that most closely matches the content within the block to be coded, while MC retrieves the content from this location to predict the coding block. Numerous enhancements to block-level ME and MC have been proposed, including the use of multiple reference frames, bi-directional inter prediction (utilizing two reference frames simultaneously), and fractional-pixel ME and MC. Despite these advancements, traditional hybrid video coding frameworks rely on block-level inter-frame prediction, which limits the ability to analyze motion relationships across all frames in a video sequence from a global perspective, thereby impeding optimal inter-frame prediction efficiency. Consequently, many researchers are investigating new coding frameworks that aim to overcome the performance limitations inherent in traditional hybrid video coding frameworks, as discussed in the paper**Saxena, V., "A Novel Video Compression Framework for Efficient Inter-Frame Prediction"**.

\subsubsection{Deep-learning based Video Compression Framework}
In contrast to these traditional mechanisms, the advent and rapid maturation of deep learning has opened new avenues in video coding. The integration of deep learning techniques into this field has catalyzed significant advancements in coding efficiency over recent years. As elucidated by comprehensive review studies**He, M., "A Comprehensive Review of Deep Learning-Based Video Compression"**, the strategies leveraging deep learning techniques for video compression can be categorized into two distinct groups: deep-tool methods and deep-framework methods. Deep-tool methods integrate deep neural networks (DNNs) into the established hybrid video coding framework. Their aim is to enhance, or even replace, specific traditional coding tools with their DNN counterparts. Several studies**Wang, Q., "Deep Learning-Based Video Compression: A Survey"**, exemplify this approach, showing improvements in areas such as intra/inter prediction, probability distribution prediction and in-loop filtering. However, thereâ€™s a crucial limitation, since the separate coding structure via block-by-block and frame-by-frame prediction cannot fully harness the potential of DNNs. Therefore, although deep-tool methods offer performance enhancements, they are still bounded by the confines of the existing paradigm, which restricts the full potential of improvements that DNNs could provide.

Recent deep-framework methods**Liu, Y., "Deep Framework Methods for Efficient Video Compression"**, advocate for the development of end-to-end deep learning based video compression frameworks. In these end-to-end deep learning based video compression methods, optical flow estimation algorithms commonly facilitate inter-frame prediction by estimating motion information between adjacent frames**Zhou, J., "Optical Flow Estimation for Inter-Frame Prediction in Video Compression"**. Moreover, the work**Wang, L., "Improved Inter-Frame Prediction Using Multiple Reference Frames"**, enhances prediction by utilizing multiple reference frames. Additionally, some studies**Chen, X., "Multi-Scale Feature Extraction for Efficient Motion Information"**, suggest that extracting multi-scale features between two frames can yield more accurate motion information. Despite these methods propose innovative approaches for inter-frame prediction, they remain constrained to a frame-by-frame structure. This limitation reveals the drawbacks observed in traditional hybrid video coding frameworks, particularly the inability to estimate and optimize motion information across all frames in a video sequence from a global perspective.

To address above limitations, our paper proposes a sequence modeling framework CANeRV, that designs the adaptive INR to capture the global dependencies and characteristics throughout the video sequence. By considering the video as a cohesive unit, this framework significantly improves compression efficiency across the entire temporal domain, surpassing the conventional frame-by-frame deep learning based video compression methods.

\subsection{INR-based Image and Video Compression}
INRs represent a novel approach for parameterizing a broad range of signals, fundamentally portraying an object as a function approximated through neural networks. An early example, DeepSDF**Peng, C., "DeepSDF: A Neural Network-Based Representation for 3D Shapes"**, provides a neural network-based representation for 3D shapes**Chen, W., "A Neural Network-Based Approach to 3D Shape Representation"**. The versatility of INRs has recently inspired extensive research across various domains, such as image compression tasks and video compression tasks.

\noindent \textbf{INR-based Image Compression.}
Within compression technologies, INRs have proven to be particularly effective. 
They encode continuous signals directly as learned functions within neural network parameters, offering a distinct and innovative encoding strategy. For instance, INR-based methods have been successfully applied to image compression**Zhang, M., "Image Compression Using Neural Network-Based Representations"**, revolutionizing traditional approaches. The compression process begins by tightly fitting the INR network to the target image during the encoding phase. Subsequently, the network parameters are quantized and encoded. At the decoder side, a forward pass through the INR network reconstructs the image by calculating RGB values at each spatial position. INR-based codecs employ a simplified, specifically tailored decoder, in contrast to the complex, general-purpose decoders used in autoencoder-based approaches. For example, the COIN decoder**He, X., "COIN: A Novel Image Compression Framework Based on Neural Networks"**, operates with a 10,000-parameter INR network, achieving performance comparable to JPEG**Wang, Y., "JPEG: A Classical Image Compression Standard"**. The advent of INR-based image compression marks a significant evolution in the image compression field. 
The more comprehensive exploration of image compression can be found in **Chen, L., "A Comprehensive Review of Image Compression Techniques"**.

\noindent \textbf{INR-based Video Compression.}
Furthermore, INRs have been adapted for video compression**Liu, X., "Video Compression Using Neural Network-Based Representations"**, demonstrating their potential to enhance compression techniques by leveraging the global correlations across video sequences, thus optimizing compression performance more effectively than traditional methods.
In the realm of INR-based video compression, various advancements have been made to enhance the efficiency of these methods. For example, numerous studies**Wang, L., "Patch-Wise Modeling Strategy for Efficient Video Compression"**, have been conducted to improve the representational power of video INR networks through different strategies, such as the patch-wise modeling strategy. In addition, there is targeted research that focuses on modeling residuals on a volume-wise and frame-wise basis**Chen, X., "Residual Modeling in Video Compression Using Neural Networks"**, as well as incorporating flow-based motion compensation**Zhou, J., "Flow-Based Motion Compensation for Efficient Video Compression"**. These techniques contribute to scalable encoding and enhance the ability to handle longer and more varied video sequences. Moreover, sophisticated loss functions for INR networks have been explored to improve their representation capability**Liu, Y., "Sophisticated Loss Functions for Neural Network-Based Representations in Video Compression"**. These advancements significantly improve the performance of INR-based video compression technologies.

Despite significant progress in INR-based video compression, current methods generally use a uniform and fixed architecture configuration. This configuration tends to restrict flexibility and efficiency when dealing with varied video content. Addressing these limitations, we propose CANeRV, which can adaptively adjust the INR network structure tailored to the specific content characteristics of each video sequence. CANeRVâ€™s adaptive mechanism allows for the reconstruction of higher-quality videos using the similar amount of parameters as conventional methods. By dynamically adjusting the network to better align with the unique attributes of each video, CANeRV significantly enhances the RD performance of the INR-based video compression method.