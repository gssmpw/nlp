\section{Benchmarking MIS Algorithms}\label{sec:exp-setup}
We focus the experiment setup for benchmarking different algorithms for Maximum Independent Set problems (MIS). 
%\Cref{sec:algs-sketch} discusses the algorithms we consider, \Cref{sec:graph-sketch} contains the graph datasets where we test different MIS algorithms, and \Cref{sec:exp-details-sketch} provides more experiment details.

\subsection{Maximum Independent Set (MIS) problem}

%In the Maximum Independent Set (MIS) problem
%is formally defined as below: 
 Given an undirected graph $\gG(\gV, \gE)$ where $\gV$ is the set of nodes and $\gE$ is the set of edges, an {\em independent set} is a subset of vertices $\gI \subseteq \gV$ such that no two nodes in $\gI$ are adjacent, i.e., $(u, v) \notin \gE$ for all $u, v \in \gI$. The goal in MIS is to find the largest possible independent set, $\gI^*$. 
%We denote the size of the largest possible indepensent set as $\alpha(G)$, which is called the independence number.

%\haoyu{formally define the MIS problem here}

\subsection{MIS algorithms}\label{sec:algs-sketch}
%We sketch the MIS algorithms we consider.
We classify the algorithms we test as: (1) \textit{classical heuristics}, which includes \deggreedy and \kamis (\onlinemis and \redumis); 
%(\onlinemis~\citep{dahlum2016accelerating} and \redumis~\citep{lamm2017finding}); 
(2) \text{GPU-accelerated} non-learning algorithms, which includes \isco and \pcqo;
%\isco~\citep{sun2023revisiting} and \pcqo~\citep{alkhouri2022differentiable};
and (3) \textit{learning-based} algorithms, which includes \lwd, \gflownets, \difusco, and \diffuco.
%\lwd~\citep{ahn2020learning}, \gflownets~\citep{zhang2023let}, \difusco~\citep{sun2023difusco} and \diffuco~\citep{sanokowskidiffusion}. 
%Please refer to \Cref{sec:detail-algs} for the details of these algorithms.

\paragraph{\deggreedy}  (Degree-based greedy) Simplest heuristic: always picks a node with the smallest degree in the current graph, add to the current independent set, and delete that node and all of its neighbors from the graph. Most papers on AI-inspired methods do not  compare with this baseline.
%this simple baseline.% \deggreedy.

\paragraph{\onlinemis and \redumis} 
are two variants of the MIS solver \kamis, mainly consists of three alternating steps: greedy, local search, and graph reductions. \onlinemis~\citep{dahlum2016accelerating} only applies a simple reduction after local search, while \redumis~\cite{lamm2017finding} applies many graph reduction techniques. 

\paragraph{\isco}~\citep{sun2023revisiting} is a GPU-accelerated sampling-based method, incorporating gradient-based discrete MCMC and simulated annealing. The MCMC is designed based on the Metropolis-Hasting algorithm, which if given enough time (exponential),  can get the optimal solution.
%, but in practical settings returns sub-optimal solutions.

\paragraph{\pcqo}~\citep{alkhouri2024dataless} directly optimizes the quadratic loss function of the MIS using gradient descent. It is sensitive to optimization hyperparameters, so hyperparameter search is required for achieving good results.

\paragraph{\lwd}~\citep{ahn2020learning} is a reinforcement learning based algorithm which models the problem as a Markov Decision Process (MDP) and requires a dataset (without solutions) to train the policy. In each step, several nodes are added to the independent set and are never deleted. We call it a \emph{non-backtracking} algorithm. 
%\citet{boether_dltreesearch_2022} also benchmarked this algorithm. 
\citet{ahn2020learning} reported that it outperforms \kamis on very large but very sparse random graphs, which we do not include in our benchmark.
%, while  \citet{boether_dltreesearch_2022} found that their performance are similar on similar graphs.

\paragraph{\gflownets}~\citep{zhang2023let} is also a \emph{non-backtracking} MDP-based algorithm similar to \lwd, but it only selects one node at a time, which is decided by GFlowNets~\citep{bengio2021flow}. Thus, it has a very similar procedure to \deggreedy. The algorithm requires a dataset (without solutions) to train the neural network.
%\haoyu{greedy-like / MDP like, lots of RL algorithm, we pick gflownets because its already the best}

\paragraph{\difusco and \diffuco} 
%Different from \gflownets
\difusco~\citep{sun2023difusco} is an end-to-end \emph{one-shot} MIS solver using diffusion models and requires a supervised training dataset with solutions. \diffuco~\citep{sanokowskidiffusion} also uses diffusion model but with unsupervised learning and annealing techniques. It requires a training dataset without solutions.

\noindent{\bf Non-backtracking vs one-shot}
Among algorithms that build up the set step by step,  \deggreedy, \gflownets and \lwd are {\em non-backtracking}, meaning once a node is added to the set it is never dropped from it.
\onlinemis and \redumis are \emph{backtracking} algorithms, since as part of local search they might decide that a previously picked should be dropped from the set to allow further additions. 
%deleted by local search during the execution. 
 %are \emph{non-backtracking} algorithms, since when a node is picked into the independent set, it is never deleted. On the other hand, 
 AI-inspired methos \pcqo, \difusco, and \diffuco are \emph{one-shot} algorithms, since they work like end-to-end MIS solvers and directly return the full set.
 %solution.

%\haoyu{single pass}

\input{tables/res-er}

\subsection{Graph types}\label{sec:graph-sketch}
%In this section, we list the datasets to test the performance of different MIS algorithms. 
%We considers two random graph datasets, as %Erd\H{o}s-Reny\'i (ER) graph and Barab\'asi–Albert (BA) graph. We also consider an existing 
%well as dataset of ``real-world graphs.'' 

\looseness=-1\paragraph{Erd\H{o}s-Reny\'i (ER) graph}
~\citep{erdos59a} are random graphs where edges are connected uniformly at random (with a given probability or a fixed number of edges). We vary $2$ parameters for ER graphs, number of nodes $n$ and average degree $d$, by fixing the number of edges at $\frac{nd}{2}$. 
%ER graphs are fundamental objects in network sciences~\citep{lewis2011network} and random graph theory~\citep{bollobas1998random}. 
Previous benchmark~\citep{boether_dltreesearch_2022} and algorithms~\citep{ahn2020learning, sun2023difusco, zhang2023let, alkhouri2024dataless} used it as test graphs for MIS, though without varying parameters as we did. 
%There are also theoretical analysis and conjecture upper bound for MIS on ER graphs~\citep{coja2015independent}.



\paragraph{Barab\'asi–Albert (BA) graph}
~\citep{albert2002statistical} are random graphs generated by a probabilistic growth process,
%whereby new nodes preferentially attach to existing nodes with higher degrees, 
mimicking real-world networks such as Internet, citation networks, and social networks~\citep{albert2002statistical, radicchi2011citation}.
For BA graphs, we vary $2$ parameters: number of nodes $n$ and parameter $m$ (not number of edges). The average degree of BA graphs can be approximated as $2m$.

\paragraph{Real-world graphs}
We pick REDDIT-MULTI-5K and COLLAB~\citep{yanardag2015deep} from TUDataset website~\citep{Morris+2020}, since they have enough graphs for training and graph sizes not too small. 
REDDIT-MULTI-5K has $508.52$ average nodes and $594.87$ average edges. They are mostly very sparse graphs. COLLAB has $74.49$ average nodes and 2457.78 average edges. They are mostly small but dense graphs.

\subsection{More experiment details}\label{sec:exp-details-sketch}
%\haoyu{list the important exp details here: e.g., what is the time limit for each algorithm, etc}
For synthetic graphs, we test $8$ graphs for each parameter $(n,d)$ or $(n,m)$. We test on $100$ graphs for real-world datasets. For learning-based algorithms, we use $4000$ training graphs generated using the same parameter (in case of random graphs)  or drawn from the same real-world dataset. For algorithms requiring hyperparameters, we use default hyperparameters in most cases (Details in \Cref{sec:detail-exp-setup}).

We set $24$-hr time limit for \kamis (\onlinemis and \redumis) since it runs on a single CPU thread with 32GB memory, and our benchmark focuses on performance on solution quality. Note that AI-based methods run well only on relatively small graphs, and \redumis runs in less than one hour on small graphs $(n \leq 3000)$. Given 24 hours \redumis can handle much larger graphs (up to $n\approx 1e6$).

For \isco and learning-based algorithms, we report results within our computational limit (generally a single 80GB A100 for $96$hrs, details in \cref{sec:detail-exp-setup}.) We test \pcqo for $n\leq 10000$ because the performance degrades quickly for large graphs using default hyperparameter search domain.
%and it is not meaningful to test larger graphs. 
%Missing results are when $1$ 80GB A100 (\gflownets and \difusco since their original codebases do not support multi-GPU), or $4$ 80GB A100s (\diffuco) are not enough for the training or cannot complete the training within $96$hrs. We use best-of-20 sampling for all learning-based algorithms and \deggreedy. 
%Most learning-based algorithm by default sample several solutions and report the best one. It is an advantage of learning-based solutions due to their relatively short inference time, where \kamis and GPU-accelerated algorithms cannot. Since \deggreedy runs very fast, we also repeat it for 20 times and report the best solution.