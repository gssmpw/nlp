%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{colortbl}

\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OWN PKGS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{arydshln}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Define the macro for algorithm names
\newcommand{\kamis}{\texttt{KaMIS}\xspace}
\newcommand{\redumis}{\texttt{ReduMIS}\xspace}
\newcommand{\onlinemis}{\texttt{OnlineMIS}\xspace}
\newcommand{\deggreedy}{\texttt{Deg-Greedy}\xspace}
\newcommand{\rangreedy}{\texttt{Ran-Greedy}\xspace}
\newcommand{\isco}{\texttt{iSCO}\xspace}
\newcommand{\difusco}{\texttt{DIFUSCO}\xspace}
\newcommand{\diffuco}{\texttt{DiffUCO}\xspace}
\newcommand{\gflownets}{\texttt{LTFT}\xspace}
\newcommand{\pcqo}{\texttt{PCQO}\xspace}
\newcommand{\gurobi}{\texttt{Gurobi}\xspace}
\newcommand{\lwd}{\texttt{LwD}\xspace}

%Define the cell colors in table
\definecolor{cb_red}{RGB}{213,94,0}
\definecolor{cb_blue}{RGB}{0,114,178}
\definecolor{cb_yellow}{RGB}{240,228,66}
\definecolor{cb_gray}{RGB}{204,204,204}
\definecolor{cb_orange}{RGB}{230,159,0}
\definecolor{cb_skyblue}{RGB}{86,180,233}
\definecolor{cb_green}{RGB}{0,158,115}
\definecolor{cb_purple}{RGB}{204,121,167}

\newcommand{\hlcella}{\cellcolor{cb_gray!40}}

\input{math_commands}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\newcommand{\comm}[1]{\vspace{1mm}\todo[inline,color=orange!20,bordercolor=orange]{#1}}
\newcommand{\yikai}[1]{\comm{{\bf {\color{red} Yikai:}} #1}}
\newcommand{\haoyu}[1]{\comm{{\bf {\color{blue} Haoyu:}} #1}}
\newcommand{\sa}[1]{\comm{{\bf {\color{orange} Sanjeev:}} #1}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Unrealized Expectations: Comparing AI Methods vs Classical Algorithms for Maximum Independent Set}

\begin{document}

\twocolumn[
\icmltitle{Unrealized Expectations: Comparing AI Methods vs Classical Algorithms for Maximum Independent Set} 

%Exploring the Limits of AI in Combinatorial Optimization: A Case Study on Maximum Independent Set}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yikai Wu}{equal,pli}
\icmlauthor{Haoyu Zhao}{equal,pli}
\icmlauthor{Sanjeev Arora}{pli}
\end{icmlauthorlist}

\icmlaffiliation{pli}{Department of Computer Science \& Princeton Language and Intelligence, Princeton University}

\icmlcorrespondingauthor{}{yikai.wu@cs.princeton.edu}
\printAffiliationsAndNotice

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
\printAffiliationsAndNoticefootnote

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

%Improvements in machine learning (ML) methods, especially generative models and reinforcement learning, have led to hopes that they can help solve other difficult problems. Significant attention has gone to using them to solve combinatorial optimization (CO) problems, especially NP-complete ones. This paper uses {\sc Maximum Independent Set (MIS)} problem to  systematically  compare AI-based methods (running on GPUs) with more classical CPU-based methods. Experiments on standard graph families show that  ML-based algorithms fail to outperform and, in many cases, to even match the solution quality of classical solvers. The GPU-based methods often fail to surpass the simplest classical heuristic, {\em degree-based greedy}. Even when allowing ML-based methods to improve their solutions with post-processing techniques like local search, their performance remains inferior to that of CPU-based solvers. 

AI methods, such as generative models and reinforcement learning, have recently been applied to combinatorial optimization (CO) problems, especially NP-hard ones. This paper compares such GPU-based methods with classical CPU-based methods on \texttt{Maximum Independent Set} (MIS). Experiments on standard graph families show that AI-based algorithms fail to outperform and, in many cases, to match the solution quality of the state-of-art classical solver \kamis running on a single CPU. Some GPU-based methods even perform similarly to the simplest heuristic, {\em degree-based greedy}. Even with post-processing techniques like local search, AI-based methods still perform worse than CPU-based solvers.


We develop a new mode of analysis to reveal that \emph{non-backtracking} AI methods, e.g. \gflownets (which is  based on GFlowNets), end up reasoning similarly to the simplest degree-based greedy approach, and thus worse than \kamis.
We also find that CPU-based algorithms, notably \kamis, have strong performance on sparse random graphs, which appears to refute a well-known conjectured upper bound for efficient algorithms from \citet{coja2015independent}.
%algorithm, \gflownets, and show that its behavior closely resembles that of the degree-based greedy approach.
%Even  when ML-based methods are allowed to improve their solutions with post-processing techniques like local search the performance remains inferior to that of CPU-based solvers.
    
\end{abstract}

\input{intro}

%\input{background}

\input{exp-setup}

\input{main-exp}

\input{analysis}

\input{related-works}

\section{Conclusion and Takeaways}
Given the great interest in designing ``general purpose AI reasoners'', it is interesting to check how well recent AI-based methods have fared in combinatorial optimization, a field with a long history of ingenious hand-designed algorithms. Our careful empirical comparisons of such AI-inspired methods with classical methods on MIS problem showed that none of the new methods  outperform \redumis, the best CPU-based MIS solver, which 
builds up the independent set iteratively, sometimes backtracking (i.e. delete a vertex from the current set). As the graphs get larger or denser, the superiority of \redumis becomes
more evident, whereas several AI-inspired algorithms drop to performing no better than trivial classical algorithms such as \deggreedy.

Further analysis shows that the fact that AI-inspired algorithms like \difusco and \pcqo select the independent set in one shot may be handicapping themselves by foregoing the benefits of local search. 
Methods like \deggreedy and \gflownets select one node at a step and do not backtrack end up performing weakly. 
%Note that \kamis does full local search, meaning it builds up the set iteratively but can both add and delete  vertices from the set. 
Interestingly, the best-performing learning-based algorithm \lwd is doing something in the middle: selecting several nodes at a time, which may allow a more effective approximation to local search. 
 

Using local search as post-processing improves the solutions of AI-inspired methods more than the CPU-based algorithms such as degree-based greedy. Despite that, with the local search step, these AI-inspired methods still perform worse than \redumis. This suggests that a more promising method to use AI and machine learning techniques for combinatorial optimization might be to teach the model the value of local search instead of simply use it as a post-processing.
%combine purely end-to-end one-shot training methods (like \pcqo or \difusco) with more classical heuristics (like local search, or greedily picking the node with smallest degree).

\iffalse
\section*{Impact Statement}

This paper considers machine learning for combinatorial optimization, and this work can potentially lead to better algorithms for real-world problems. As this paper focuses on benchmarking and understanding previous methods, we do not feel there are any potentially negative societal consequences.
%There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.‚Äù
\fi

\section*{Acknowledgement}
YW, HZ, and SA acknowledge funding from DARPA. The authors would like to thank Alvaro Velasquez, Ismail Alkhouri, Kaifeng Lyu, Sadhika Malladi, and Pravesh Kothari for helpful discussions.

\bibliographystyle{arxiv}
\bibliography{ref}

\newpage
\appendix
\onecolumn

\input{appendix}

\end{document}
