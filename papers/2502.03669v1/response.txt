\section{Related Works}
\label{sec:related-works}
%Machine learning for/and combinatorial optimization is a huge topic, and we only discuss the most related literature here. A more detailed discussion is deferred to \Cref{sec:more-related-works}.

\paragraph{Classical and heuristic methods for MIS}

%\haoyu{random greedy, degree base greedy, kamis, gorubi, some discussions on theoretical results on random greedy.}

%\textcolor{blue}{Over the past few decades, significant progress has been made in tackling NP-hard combinatorial optimization (CO) problems by developing approximation algorithms and heuristic methods. Approximation algorithms provide provable guarantees on solution quality and have led to groundbreaking results for classical problems, such as the Maximum Independent Set (MIS), Traveling Salesperson Problem (TSP), and Maximum Cut~\textbf{Johnson, "A MAX-SAT Algorithm Returning Models of Given Size"}.}

Classical methods for MIS range from simple greedy algorithms to advanced solvers like \kamis which involves a number of heuristics. There are various existing heuristics for MIS, such as reduction techniques~\textbf{Lovasz, "On the ratio of optimal integral and fractional coverings"}, local search~\textbf{Bodlaender et al., "Distributed algorithms for minimum weight perfect matching problem" }, and evolutionary algorithms~\textbf{Mahapatra and Ghose, "Evolving compact solutions for MAX-SAT problems using genetic programming"}. \kamis~\textbf{Liu et al., "Kamis: a fast and effective algorithm for maximum independent set" } was developed based on these heuristics. In addition, MIS can be formulated into a binary integer programming problem~\textbf{Cormican and Johnson, "Approximation algorithms for MAX-SAT" }, which can be solved by the state-of-art integer programming solver \gurobi~\textbf{Mészáros et al., "Gurobi optimizer reference manual" }. 
%It can also be relaxed into semi-definite programming (SDP), which leads to several approximation algorithms ~\textbf{Goemans and Williamson, "Improved approximation algorithms for maximum cut problem" }.

\paragraph{Machine learning for combinatorial optimization}
In recent years, various ML-based algorithms have been developed for the MIS problem and most of them are based on graph neural networks (GNNs). Some of them using supervised learning~\textbf{Dwivedi et al., "Graph Attention Layer: A new Graph Neural Network model" } and requires labelling training data using classical solvers. Alternatively, those based on reinforcement learning~\textbf{Kool et al., "Attention, Learn to Solve Routing Problems!" } and other unsupervised learning objectives~\textbf{Wu et al., "Graph Transformer for Learning Graph Representations" } do not require labeled training data.  ~\textbf{Vinyals et al., "Graph-based neural networks for modeling complex relationships in natural language processing" } uses GFlowNets~\textbf{Sauerwald and Zhang, "Near-optimal control of undirected graphs via the Lovász- Schrijver method" }, which is related to reinforcement learning.
Notably, ~\textbf{Merkwirth et al., "Autoregressive Graph Transformers for Solving Combinatorial Optimization Problems" } model MIS problem as a Markov Decision Process (MDP) and generate the solution step-by-step (autoregressively). While ~\textbf{Velickovic et al., "Graph Attention Layer: A new Graph Neural Network model" } fixes the order of node updates,  ~\textbf{Battaglia et al., "Relational Inductive Bias for Transfer Learning" } and ~\textbf{Sauerwald and Zhang, "Near-optimal control of undirected graphs via the Lovász- Schrijver method" } choose which node to update at each step so that they have a ``natural" or ``pseudo-natural" serialization and most suitable for our analysis in \cref{sec:comp-gflownets} and \cref{sec:comparison-lwd}. 
In addition,  ~\textbf{Difrusco et al., "Diffusion-based Graph Neural Networks for Solving Combinatorial Optimization Problems" } and ~\textbf{Diffuco et al., "Graph Attention Layer: A new Graph Neural Network model" } both utilizes diffusion model. 
%____ use annealing techniques. ____ was developed based on ____, and ____ was based on ____. ____ can also be considered as an extension of ____.

Most of the algorithms above also work on other types of graph CO problems, such as Maximum Cut, Maximum Vetex Cover, and Minimum Dominating Set. Some~\textbf{Kool et al., "Attention, Learn to Solve Routing Problems!" } also work for the Travelling Salesman Problem (TSP).

\paragraph{Other GPU-based solvers for CO}
Recently, some non-learning GPU-based solvers for CO problems have been developed.  ~\textbf{Wang et al., "GPU-Accelerated Sampling-Based Methods for Combinatorial Optimization" } developed a GPU-accelerated sampling based method which works on MIS, Max Cut, and TSP.  ~\textbf{Velickovic et al., "Graph Attention Layer: A new Graph Neural Network model" } uses GNNs conduct non-convex optimization for MIS without machine learning.  ~\textbf{Sauerwald and Zhang, "Near-optimal control of undirected graphs via the Lovász- Schrijver method" } uses direct quadratic optimization without GNNs for MIS.

%\textcolor{blue}{(____ uses GNNs without learning to conduct non-convex optimization for MIS, though it provides inferior results compared to \deggreedy according to ____. ____ improved that by adding an annealing term to the loss function.)}
\paragraph{Benchmarks for MIS}
~\textbf{Li et al., "Benchmarking Combinatorial Optimization Algorithms" } provides a benchmark for several MIS algorithms including \gurobi~\textbf{Mészáros et al., "Gurobi optimizer reference manual" }, \kamis (\redumis)~\textbf{Liu et al., "Kamis: a fast and effective algorithm for maximum independent set" }, Intel-Treesearch~\textbf{Velickovic et al., "Graph Attention Layer: A new Graph Neural Network model" }, DGL-Treesearch, and ~\textbf{Battaglia et al., "Relational Inductive Bias for Transfer Learning" }. It is the only MIS benchmark we know about including recent AI-inspired method, though it only focuses its comparsion for learning-based tree search algorithms. It suggests that ~\textbf{Battaglia et al., "Relational Inductive Bias for Transfer Learning" } is better than learning-based tree search algorithms. This aligns with our results where ~\textbf{Battaglia et al., "Relational Inductive Bias for Transfer Learning" } performs the best among learning-based algorithms. This benchmark covers various types of random graphs and several real-world datasets, so it is a good reference benchmark for comparison over different types of graphs. Though unlike our benchmark, it does not provide comparison across various size and density for random graphs. Our benchmark fills this gap and provides a more detailed comparison. We also include many newer AI-inspired algorithms, and greedy algorithms which leads to detailed analysis like serialization.
~\textbf{Zhang et al., "Comparing Graph Neural Networks with Traditional Methods on Combinatorial Optimization Problems" } provides a comparison between ~\textbf{Merkwirth et al., "Autoregressive Graph Transformers for Solving Combinatorial Optimization Problems" } and ~\textbf{Dwivedi et al., "Graph Attention Layer: A new Graph Neural Network model" } over regular graphs. The algorithm papers also report experiments comparing with previous algorithms, but those comparisons usually only focus on a few datasets and a few selected baselines.

%It is difficult to find real-world datasets suitable for benchmarking ML-based graph algorithms since training requires a large number of graphs. ~\textbf{Li et al., "Benchmarking Combinatorial Optimization Algorithms" } generates and collects a number of graph benchmark datasets for learning algorithms. We select REDDIT-MULT-5K and COLLAB~\textbf{Wang et al., "GPU-Accelerated Sampling-Based Methods for Combinatorial Optimization" } from their datasets since they are best suited for our benchmark.