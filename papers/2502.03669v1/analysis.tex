%\vspace{-1mm}
\section{AI-Inspired vs  \deggreedy}\label{sec:ablations}
Our results show that the state-of-the-art AI-inspired algorithms for MIS still do not outperform the best heuristic \redumis. The surprising finding was that they also often do not outperform the  simplest classical heuristic, \deggreedy, especially on large and dense graphs. In this section, we  delve deeper into this comparison (\Cref{sec:comp-gflownets,sec:comp-other-algs}). Furthermore, we explore the impact of augmenting various algorithms with a local search as a post hoc step to enhance solution quality (\Cref{sec:add-local-search}).

\subsection{Comparison between \deggreedy and \gflownets}\label{sec:comp-gflownets}

\input{figs/toprank_gflownets.tex}

\deggreedy sequentially picks nodes for the independent set.  At each step, it picks the node  with the smallest degree in the residual graph (where the nodes in the independent set and their neighbors are removed).  It does not reverse any decisions (ie once picked, the node stays in the independent set).  As in \cref{sec:algs-sketch} we call it a \emph{non-backtracking} algorithm. \gflownets is also a non-backtracking algorithm and it often perform similarly to \deggreedy in \cref{tab:res-er,tab:res-ba}. It uses a trained policy network GFlowNets~\citep{bengio2021flow} to pick a node for the independent set at each step. Thus, we can naturally compare \gflownets with \deggreedy by investigating how close this trained policy compares to the naive policy in \deggreedy.

The results are shown in \Cref{fig:toprank-gflownets}. Overall, we observe that \gflownets frequently selects nodes with very small degrees. On ER graphs where the average degree is at least 30, \gflownets picks the node with the smallest degree over 85\% of rounds except for sparse graphs ($d=10$ and $(n,d)=(3000,30)$). On BA graphs, while the percentage is lower, it still exceeds 75\% on large and sufficiently dense graphs. Moreover, in cases where \gflownets selects the smallest degree nodes less frequently, its performance is worse than \deggreedy.

%If we relax the requirement from choosing the node with smallest degree to choosing the node with smallest 5\% degree, the percentage is much higher. This percentage goes over 95\% on most of the ER graphs. 
To conclude, our results suggest that despite using neural net to learn the policy,   \gflownets is closely aligned with \deggreedy: prioritizing nodes with small degrees at each step. The high consistency between the node selection strategies of \gflownets and \deggreedy can explain their similar performance.  


\subsection{Serialization: allows comparing to \deggreedy}\label{sec:comp-other-algs}

\input{figs/consistency_check}


In the previous part, we demonstrate that \gflownets employs a heuristic similar to \deggreedy, selecting the node with the smallest degree in the remaining graph in each round. However, other algorithms, such as \redumis, \pcqo, and \difusco, which does not select one node at a step without backtracking, cannot be analyzed based on the sequence of nodes picked. Thus, we introduce a method called \emph{degree-based solution serialization} to analyze their behavior and compare them with \deggreedy.

Given a graph $\gG(\gV,\gE)$ and an independent set solution $\gI$ (which is an independent set), process proceeds as follows: (1) Repeatedly remove the node in $\gI$ with the smallest degree. (2) After removing a node from $\gI$, also remove it and its neighbors in $\gG$. (3) Continue this process until all nodes in $\gI$ are removed.
The order in which nodes are removed forms the serialization of the solution. This procedure is detailed in \Cref{alg:serialization}.

To evaluate the algorithms, we compute the percentage of rounds in which the smallest degree node is selected during serialization, similar to our comparison between \gflownets and \deggreedy. Due to random tie-breaking in \Cref{alg:serialization}, we repeat the process 100 times and select the serialization with the highest number of rounds selecting the smallest degree node. Although \deggreedy theoretically achieves 100\% smallest-degree selections in its best serialization, random tie-breaking prevents us from perfectly recovering this with 100 repetitions. Instead of reporting the overall percentage, we divide the serialization into three equal parts and report the percentages for each part.

The results, shown in \Cref{fig:serialization}, compare the percentage of smallest-degree node selections across different algorithms. Due to space constraints, we present results only for ER and BA graphs under selected parameters; additional results are available in \Cref{sec:more-exp-serialization}.

Algorithms that often perform the best, namely \onlinemis, \redumis, and \isco,
%despite using different procedures, 
exhibit a consistent pattern after serialization. In the first one-third of the serialization, these algorithms deviate significantly from selecting the smallest-degree nodes. However, in the middle and final thirds, the percentage of smallest-degree node selections increases substantially. This suggests that while degree-based greedy heuristics may appear shortsighted initially, they are highly effective in the later stages of solution construction. Interestingly, \lwd, which performs the best among learning-based methods tested in our setting, also shares this pattern.

As for \pcqo and \difusco, they show consistently low percentages of smallest-degree node selections throughout the serialization, particularly on ER graphs.

Through serialization, we observe distinct differences in node selection patterns among algorithms. Our findings suggest that AI-based methods might fail to utilize (\pcqo and \difusco) or emphasize too much (\gflownets) on simple yet highly effective heuristics, such as greedily selecting the smallest-degree node, which may partly explain their performance limitations.

In \cref{sec:comparison-lwd}, we perform a \emph{pseudo-natural serialization} for \lwd. \lwd is also a non-backtracking algorithm like \gflownets, but it does not have a ``natural serialization" like \gflownets  (\cref{sec:comp-gflownets}) because it chooses several nodes in a step. The \emph{pseudo-natural serialization} performs serialization in each step of \lwd. The results in \cref{fig:toprank-lwd} align with our ``counterfactual" serialization results here.

%\vspace{-1mm}
\subsection{Incorporating local search to improve solution}\label{sec:add-local-search}
%\haoyu{need to change the paragraph title based on the exp results}

In the previous sections, we show that solutions generated by AI-based algorithms generally differ from those produced by degree-based greedy methods, which may explain their inferior performance on MIS problems. A natural idea is to enhance these solutions with simple heuristics, such as local search. Local search post-processing has also been used for AI-algorithms in previous works~\citep{ahn2020learning, boether_dltreesearch_2022}. 

We applied the $2$-improvement local search~\citep{andrade2012fast} (details in \ref{sec:local_search}), which is used in \kamis, as a post-processing step to all algorithms (except \onlinemis and \redumis since they already has local search), and the resulting performance improvements are presented in \Cref{tab:res-ls}.

\input{tables/res-ls}

As shown in \Cref{tab:res-ls}, algorithms like \pcqo and \difusco benefit significantly more from the local search post-processing compared to others, such as \deggreedy and \lwd. This observation aligns with our earlier findings: If the solution after serialization exhibits a high percentage of smallest-degree node selections in the later stages, there is relatively little room for improvement through local search.
Conversely, if the solution after serialization shows a low percentage of smallest-degree node selections, there is greater potential for improvement via local search.

In addition, although all algorithms except \isco have improvements after local search, they still perform worse than \redumis in most cases.

In summary, our analysis highlights a promising direction for designing machine learning-based combinatorial optimization algorithms. Rather than relying solely on end-to-end methods like \pcqo or \difusco, incorporating classical heuristics, such as greedily selecting the smallest-degree node, into the overall algorithm may yield better results. One potential approach could involve using machine learning algorithms to identify a small subset of nodes, followed by a degree-based greedy method to complete the solution.

\subsection{\kamis  refutes famous conjecture?}\label{sec:refute-conjecture}
For theoretical analysis for MIS on ER graphs and regular graphs, see~\citet{coja2015independent, wormald1999differential, barbier2013hard, gamarnik2014limits}. In ER graphs with $n$ nodes and average degree $d$, the  MIS has size $\frac{2n\ln d}{d}$ for asymptotically large $n$ and $d$, and simplest random greedy  achieves half-optimal at $\frac{n\ln d}{d}$~\citep{grimmett1975colouring}. Yet, there is no known polynomial-time algorithm which can achieve MIS size $(1+\eps)\frac{n\ln d}{d}$ for any constant $\eps$ and it is conjectured that polynomial-time algorithms cannot do better than  $(1+o(1))\frac{n\ln d}{d}$~\citep{coja2015independent}.  Thus it is natural to measure the goodness of an algorithm by the ratio of the MIS size obtained to $\frac{n\ln d}{d}$. This ratio can faciliate comparison across different $n$ and $d$'s.

\Cref{fig:er_heatmap_small} plots this ratio for several algorithms. Surprisingly  for \redumis and \onlinemis this ratio is consistently larger than $1.2$ and often larger than $1.3$ even for fairly large $n, d$.  These results seem to bring the conjecture in doubt, but not disprove per se since the conjecture concerns asymptotically large $n$ and $d$. But  our results could encourage further analysis and potential collaboration between researchers on theoretical and empirical aspects of MIS on random graphs.
\input{figs/fig_gnm_heatmap_small}