\section{Detailed Description of Algorithms}\label{sec:detail-algs}

In this section, we will describe the algorithms we mentioned above and compared in our experiments. We use random $G(n,m)$ graphs as in \cite{coja2015independent} as examples in the descriptions. We let $V$ be the set of vertices and $E$ be the set of edges.

\haoyu{maybe it is also better to put the first four in the category "heuristics". Then 3 (or 4) subsections can be "heuristics", learning based, GPU based (or optimization based and sampling based). Also remember to add Ismail's paper (optimization based).}
\subsection{Random greedy}
This is the most naive greedy algorithm. It first give a uniformly random order to the vertices and an empty set $S$. It then check each vertex in that order. If there is no edge between the vertex and all the vertices in $S$, we put that vertex into $S$. $S$ is then an independent set by definition. \cite{grimmett1975colouring} proves that this algorithm can achieve a MIS size of $(1+o(1))n\ln(d)/d$ with high probability.

\subsection{Degree based greedy}
In this algorithm, we start with empty set $S$. We then find the lowest degree vertex in $V \setminus (S\cup N(S))$, where $N(S)$ is the neighbors of the vertices in $S$. We then add this vertex to $S$ and continue. This algorithm uses a simple heuristic that low degree vertices are more likely to be in the independent set. \haoyu{maybe it is not necessary to state the experiment results here. should be deferred to later experimental result section.} In our experiments, we observe that it performs significantly better than random greedy algorithms for ER graphs.

\subsection{Local search (ARW)}
\label{sec:ARW}
\haoyu{There is no experiment results for local search (AWR), do you want to mention them in this part?}
For a graph $G$, given a existing maximal independent set $I$, we can apply local improvement to $I$ to make it a larger independent set. For example, $2$-improvement is to remove $1$ vertex from $I$ and add $2$ vertices to $I$ while the new set $I'$ is still an independent set.

Although there are various local search algorithms for MIS, most modern MIS solvers use ARW \cite{andrade2012fast} as their sub-procedure. It aims to find a $2$-improvement for a given independent set $I$: This algorithm process every vertex $x \in I$ in turn. First, it temporarily removes $x$ from $I$, creating a new set $S$. We call a vertex a free vertex of $S$ if there is no edge between it and any vertex in $S$. If $S$ has less than two free vertices, stop: there is no 2-improvement involving $x$. Otherwise, for each neighbor $v$ of $x$ that is a free vertex for $S$, insert $v$ into $S$ and check if the new set ($S'$) has a free vertex $w$. If it does, inserting $w$ leads to a $2$-improvement; if it does not, remove $v$ from $S'$ (thus restoring $S$) and process the next neighbor of $x$. If no improvement is found, reinsert $x$ into $S$ to turn it back to $I$. Every vertex is scanned $O(1)$ times in this algorithm so it can find a $2$-improvement (if there exists) in $O(m)$ time.

ARW can be directly applied to random greedy and degree based greedy. In our experiments, we observe that the solution from random greedy can be improved for relative many vertices, but the improved solution is still smaller than the solution obtained from degree based greedy. The solution from degree based greedy can be improved for much fewer vertices.
\subsection{Karlsruhe Maximum Independent Sets (KaMIS)}
KaMIS \cite{lamm2016finding} is a state-of-art MIS solver set. It contains $2$ solvers: ReduMIS \cite{lamm2016finding} and OnlineMIS \cite{dahlum2016accelerating}. Since ReduMIS generally provides better results, it is more widely used and used as a baseline in many other papers. Sometimes people simply use KaMIS to refer ReduMIS. OnlineMIS provides worse solutions but is significantly faster.

ReduMIS has $4$ sub-procedure: Exact reduction, Inexact reduction, and Evolutionary MIS (EvoMIS \cite{lamm2015graph}). \textit{Exact reduction} is a set of procedures that can reduce a graph $G$ to a smaller graph $G'$, but any independent set in $G'$ can be recovered to an independent set in $G$. It also ensures that if a MIS is found in $G'$, it can be recovered into a MIS in $G$. \textit{Inexact reduction} takes an independent set $I$ of $G$, picks $\lambda|I|$ lowest degree vertices ($\lambda$ is chosen as $0.1$), and reduce $G$ to $G'$ by removing these vertices and their neighbors.

EvoMIS first construct a solution population (a set of maximial independent set) using $3$ different greedy algorithms. It then picks 2 solutions $I_1$ and $I_2$, and then partitions the graph into 2 partitions $V_1$ and $V_2$ such that $V = V_1 \cup V_2 \cup S$, where $S$ is a separator. It thus generates 2 children solution $O_1 = (V_1 \cap I_1) \cup (V_2 \cap I_2)$ and $O_2 = (V_1 \cap I_2) \cup (V_2 \cap I_1)$. Then it uses ARW to make $O_1$ and $O_2$ maximal independent sets.

In each iteration of ReduMIS, it first performs Exact reduction, then EvoMIS, and Inexact reduction on the solution EvoMIS produced. The iterations end when the reduced graph is empty or time limit is reached (then the current solution of EvoMIS is accepted and no inexact reduction is performed).

OnlineMIS also has Exact reduction, Inexact reduction, and local search (ARW), but applied differently. It only uses exact reduction techniques that remove vertices without modifying the graph. Thus, it can perform reduction in an online manner within the ARW local search procedure. It also provides some effiency improvements on the original ARW.

ReduMIS is considered the state-of-art MIS solver and provides the best solution empirically in many experiments of various types of graphs. In our experiments on $G(n,m)$ graphs, it also gives the best results. OnlineMIS gives slightly worse results than ReduMIS but much better than greedy algorithms.

\subsection{Sampling based algorithm}
\cite{sun2023revisiting} (iSCO) uses GPU-accelerated Metropolis-Hasting sampling to sample $p_{\tau} \propto \exp\left(-f(x)/\tau\right)$, where $f(x)$ is the energy function and $\tau$ is the temperature. For MIS problem, it lets $x$ be a $0-1$ vector and  $f(x) = -c^\top x + \lambda \frac{x^\top A x}{2}$, where $A$ is the adjacency matrix. In our experiments, it provides solutions of similar quality to ReduMIS but takes significantly longer time for a single graph. However, it can process a large number of graphs in parallel and may be more efficient than ReduMIS in that case.

\subsection{Learning based algorithms}
There are various learing based algorithms for MIS problem, but according to their own experiments and benchmark from previous work \cite{boether_dltreesearch_2022}, none of them provide solution of similar quaility to KaMIS on ER graphs. The only exception is for neural network guided tree search algorithm \cite{li2018combinatorial} where reduction and local search techniques in KaMIS is also applied. However, \cite{boether_dltreesearch_2022} also observes that by changing the neural network outputs to random values, the quality of solutions of this solution do not decrease, suggesting that neural networks may not be useful if the algorithm is already using reduction and local search. 

According to experiments reported in these papers, none of these algorithms surpassing KaMIS on most types of graphs. Specifically for RB grpahs \cite{xu2000exact}, which are generally considered as a hard dataset for MIS problem, KaMIS provides better results than various learning based algorithms in experiments shown in \cite{sanokowskidiffusion}. The benefit of these algorithms are faster inference time, but they may require very long training time.
\subsubsection{DiffUCO}
One of the state-of-art learning based algorithms is DiffUCO \cite{sanokowskidiffusion}, which trains a diffusion model to sample from distribution $p(X,\beta) \propto \exp\left(-\beta H(x)\right)$, where $H(X)$ is the energy function and $\beta$ is the reverse temperature, similar to the sampling based algorithm. It utilizes variational annealing which decreases the temperature throughout the training process. According to the experiment results in their paper, it performs better than other learning based algorithms for RB graphs. We have also tested it on ER graphs and observe that it performs the best among learning based algorithms we tested.
\subsubsection{DIFUSCO}
Another diffusion model based algorithm is DIFUSCO \cite{sun2023difusco}. It uses supervised learning to train a diffusion model with traning data generated by ReduMIS. The performance for random graphs is however not as good as DiffUCO according to the experiments reported in their papers and also in our experiments. The reason according to their paper is their GNN structure may not be suitable for random graphs.
\subsubsection{GFlowNets}
\cite{zhang2023let} utilizes GFlowNets \cite{bengio2023gflownet} to find MIS solutions. It is based on Markov decision process (MDP). Given a graph $G=(V,E)$, it assigns a state in $\{\empty, 0, 1\}$ to each vertex in $V$. Then, it trains a GFlowNets to select the vertices to be put into the independent set $I$. Starting with an empty set $I$, at each step, the GFlowNet select a vertex $v$ with state $\empty$ and change its state to $1$. The state of all the neighbors of $v$ are then changed to $0$. Since it only adds vertices into the $I$, it is essentially a greedy algorithm. Compared to the degree based greedy algorithm, instead of choosing the eligible vertex of the lowest degree each time, it let the GFlowNets to pick the vertex.
