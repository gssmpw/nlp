\section{Experiment Results and Main Findings}\label{sec:exp-results}
In this section, we present our main experiment results. The performance of different algorithms on Erd\H{o}s-Reny\'i (ER) graphs, Barab\'asiâ€“Albert (BA) graphs, and real-world graphs are shown in \Cref{tab:res-er,tab:res-ba,tab:res-real} respectively. 
%We then discuss our main findings.


\paragraph{AI-inspired algorithms don't outperform \redumis.} Our first main finding is that, current AI-inspired algorithms do not outperform the best classical heuristics \redumis in terms of performance. As shown in \Cref{tab:res-er,tab:res-ba,tab:res-real}, \redumis consistently achieves superior results compared to all other methods, with the exception of \isco sometimes perform similarly, on ER, BA, and real-world graphs. 
%As shown in \Cref{tab:res-er}, \redumis performs the best or close to the best (within $\pm 1\%$ of the best) across all tested graph sizes $n$ and average degree $d$ on ER graphs. 
%Similarly, for BA graphs, \redumis outperforms all methods across different $n,m$ parameter settings (\Cref{tab:res-ba}). \redumis also performs close to the best (within $\pm 1\%$ of the best) on real-world graph datasets (\Cref{tab:res-real}).

Additionally, although learning-based algorithms are claimed to be more efficient, they require significant training time and GPU memory, which is over our resource constraint for graphs with more than 3000 nodes, while \redumis can handle graphs with up to $1\times 10^6$ nodes (See \Cref{tab:res-er-larger-graph}). Although \isco performs close to \redumis, it requires significant GPU memory and we are unable to get results for dense graphs with 10000 nodes. Its performance also become worse than \redumis for larger graphs.
%Additionally, \redumis can solve larger and denser graphs than AI-based MIS algorithms. For instance, \redumis handles graphs with over 10,000 nodes and varying densities\footnote{\redumis can tackle even much larger graphs within the resources constraints. See \Cref{tab:res-er-larger-graph} for more results.}, while all learning-based algorithms algorithms are limited to graphs with at most 3,000 nodes. Although \pcqo can process graphs with $n=10,000$, the performance lags significantly behind simple heuristics like \deggreedy. For example, in \Cref{tab:res-er} with $n=10,000$, \deggreedy outperforms \pcqo across all average degree $d$.

We also note that \lwd performs the best among learning-based algorithms, despite it being the oldest learning-based algorithm we tested.
In summary, our experiment results show that current AI-inspired algorithms still don't outperform the best classical heuristics for the MIS problem.

\input{tables/res-ba.tex}

\input{tables/res-real.tex}

\paragraph{The performance gap between \redumis and AI-based methods widens with larger and denser graphs.} While \redumis consistently outperforms AI-based methods in most cases, the performance gap is small on small or sparse graphs. 
On ER graphs when $n=100,300$ and average degree $d=10$, \lwd has results within $1\%$ gap from \redumis. On BA graphs for $n=300,1000$ and $m=5$, \lwd, \difusco, and \diffuco has less than $0.5\%$ gap from \redumis. Across real-world datasets 
%(which as mentioned involve small graphs) 
all tested algorithms produce independent sets of similar sizes on average.
However, as the graph becomes larger or denser, the performance gap between \redumis and AI-based algorithms enlarges. 
On ER graphs, when $n=1000$, there is a clear performance gap between AI-based algorithms and \redumis, with the only exception of the sampling based \isco. When $d=10$, \diffuco and \lwd still reaches 98\% of \redumis's performance. For denser graphs($d=100$), \diffuco and \lwd only reaches 96\% of \redumis's performance. This gap widens further for $n=3000$, where AI-based algorithms perform significantly worse than \redumis and sometimes fail to outperform simple heuristic \deggreedy. 
A similar trend has also been observed on BA graphs.
%: As graphs increase in size and density, the performance gap between \redumis and AI-based algorithms grows.
Classical solvers have no difficulty handling graphs with a million edges, but learning-based implementations struggle to scale up to that size.
%\vspace{-0.5mm}
\paragraph{\deggreedy serves as a strong baseline.} Another key finding is that the simplest degree-based greedy (\deggreedy) serves as a remarkably strong baseline. As shown in \Cref{tab:res-er,tab:res-ba,tab:res-real}, leveraging neural networks for node selection, \gflownets often perform comparably to \deggreedy, particularly on larger or denser graphs.
For example on ER graphs when $n=1000$ or $3000$, \gflownets gives performance within 2\% of \deggreedy (\Cref{tab:res-er}). An exception is for BA graphs where the graph is very dense ($n=3000,m=500$) and clear patterns emerge in the graph, \gflownets surpasses \deggreedy.
Additionally, \difusco and \pcqo fail to outperform \deggreedy on larger or less sparse graphs, such as  $n=3000$, $d>30$.
