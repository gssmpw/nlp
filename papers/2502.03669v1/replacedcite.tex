\section{Related Works}
\label{sec:related-works}
%Machine learning for/and combinatorial optimization is a huge topic, and we only discuss the most related literature here. A more detailed discussion is deferred to \Cref{sec:more-related-works}.

\paragraph{Classical and heuristic methods for MIS}

%\haoyu{random greedy, degree base greedy, kamis, gorubi, some discussions on theoretical results on random greedy.}

%\textcolor{blue}{Over the past few decades, significant progress has been made in tackling NP-hard combinatorial optimization (CO) problems by developing approximation algorithms and heuristic methods. Approximation algorithms provide provable guarantees on solution quality and have led to groundbreaking results for classical problems, such as the Maximum Independent Set (MIS), Traveling Salesperson Problem (TSP), and Maximum Cut____.}

Classical methods for MIS range from simple greedy algorithms to advanced solvers like \kamis which involves a number of heuristics. There are various existing heuristics for MIS, such as reduction techniques____, local search____, and evolutionary algorithms____. \kamis____ was developed based on these heuristics. In addition, MIS can be formulated into a binary integer programming problem ____, which can be solved by the state-of-art integer programming solver \gurobi____. 
%It can also be relaxed into semi-definite programming (SDP), which leads to several approximation algorithms ____.

\paragraph{Machine learning for combinatorial optimization}
In recent years, various ML-based algorithms have been developed for the MIS problem and most of them are based on graph neural networks (GNNs). Some of them using supervised learning____ and requires labelling training data using classical solvers. Alternatively, those based on reinforcement learning____ and other unsupervised learning objectives____ do not require labeled training data. ____ (\gflownets) uses GFlowNets____ which is related to reinforcement learning.
Notably, ____ model MIS problem as a Markov Decision Process (MDP) and generate the solution step-by-step (autoregressively). While ____ fixes the order of node updates, ____ (\lwd) and ____ (\gflownets) choose which node to update at each step so that they have a ``natural" or ``pseudo-natural" serialization and most suitable for our analysis in \cref{sec:comp-gflownets} and \cref{sec:comparison-lwd}. 
In addition, ____ (\difusco) and ____ (\diffuco) both utilizes diffusion model. 
%____ use annealing techniques. ____ was developed based on ____, and ____ was based on ____. ____ can also be considered as an extension of ____.

Most of the algorithms above also work on other types of graph CO problems, such as Maximum Cut, Maximum Vetex Cover, and Minimum Dominating Set. Some____ also work for the Travelling Salesman Problem (TSP).

\paragraph{Other GPU-based solvers for CO}
Recently, some non-learning GPU-based solvers for CO problems have been developed. ____ developed a GPU-accelerated sampling based method which works on MIS, Max Cut, and TSP. ____ uses GNNs conduct non-convex optimization for MIS without machine learning. ____ uses direct quadratic optimization without GNNs for MIS.

%\textcolor{blue}{(____ uses GNNs without learning to conduct non-convex optimization for MIS, though it provides inferior results compared to \deggreedy according to ____. ____ improved that by adding an annealing term to the loss function.)}
\paragraph{Benchmarks for MIS}
____ provides a benchmark for several MIS algorithms including \gurobi____, \kamis (\redumis)____, Intel-Treesearch____, DGL-Treesearch, and \lwd____. It is the only MIS benchmark we know about including recent AI-inspired method, though it only focuses its comparsion for learning-based tree search algorithms. It suggests that \lwd is better than learning-based tree search algorithms. This aligns with our results where \lwd performs the best among learning-based algorithms. This benchmark covers various types of random graphs and several real-world datasets, so it is a good reference benchmark for comparison over different types of graphs. Though unlike our benchmark, it does not provide comparison across various size and density for random graphs. Our benchmark fills this gap and provides a more detailed comparison. We also include many newer AI-inspired algorithms, and greedy algorithms which leads to detailed analysis like serialization.
____ provides a comparison between ____ and \deggreedy over regular graphs. The algorithm papers also report experiments comparing with previous algorithms, but those comparisons usually only focus on a few datasets and a few selected baselines.

%It is difficult to find real-world datasets suitable for benchmarking ML-based graph algorithms since training requires a large number of graphs. ____ generates and collects a number of graph benchmark datasets for learning algorithms. We select REDDIT-MULT-5K and COLLAB____ from their datasets since they are best suited for our benchmark.