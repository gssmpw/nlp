\section{Related Works}\label{sec:related-works}
%Machine learning for/and combinatorial optimization is a huge topic, and we only discuss the most related literature here. A more detailed discussion is deferred to \Cref{sec:more-related-works}.

\paragraph{Classical and heuristic methods for MIS}

%\haoyu{random greedy, degree base greedy, kamis, gorubi, some discussions on theoretical results on random greedy.}

%\textcolor{blue}{Over the past few decades, significant progress has been made in tackling NP-hard combinatorial optimization (CO) problems by developing approximation algorithms and heuristic methods. Approximation algorithms provide provable guarantees on solution quality and have led to groundbreaking results for classical problems, such as the Maximum Independent Set (MIS), Traveling Salesperson Problem (TSP), and Maximum Cut~\citep{boppana1992approximating, laporte1992traveling, goemans1995improved}.}

Classical methods for MIS range from simple greedy algorithms to advanced solvers like \kamis which involves a number of heuristics. There are various existing heuristics for MIS, such as reduction techniques~\citep{butenko2002finding, xiao2013confining, akiba2016branch}, local search~\citep{andrade2012fast}, and evolutionary algorithms~\citep{back1994evolutionary, borisovsky2003experimental, lamm2015graph}. \kamis~\citep{lamm2017finding, dahlum2016accelerating, hespe2019scalable} was developed based on these heuristics. In addition, MIS can be formulated into a binary integer programming problem \citep{nemhauser1975vertex}, which can be solved by the state-of-art integer programming solver \gurobi\cite{gurobi}. 
%It can also be relaxed into semi-definite programming (SDP), which leads to several approximation algorithms \citep{halperin2002improved, bansal2014approximating}.

\paragraph{Machine learning for combinatorial optimization}
In recent years, various ML-based algorithms have been developed for the MIS problem and most of them are based on graph neural networks (GNNs). Some of them using supervised learning~\citep{li2018combinatorial, sun2023difusco, li2024distribution} and requires labelling training data using classical solvers. Alternatively, those based on reinforcement learning~\citep{khalil2017learning, ahn2020learning, qiu2022dimes, sanokowski2023variational} and other unsupervised learning objectives~\citep{karalias2020erdos,sun2022annealed, zhang2023let, sanokowskidiffusion} do not require labeled training data. \citet{zhang2023let} (\gflownets) uses GFlowNets~\citep{bengio2021flow} which is related to reinforcement learning.
Notably, \citet{ahn2020learning, sanokowski2023variational, zhang2023let} model MIS problem as a Markov Decision Process (MDP) and generate the solution step-by-step (autoregressively). While \citet{sanokowski2023variational} fixes the order of node updates, \citet{ahn2020learning} (\lwd) and \citet{zhang2023let} (\gflownets) choose which node to update at each step so that they have a ``natural" or ``pseudo-natural" serialization and most suitable for our analysis in \cref{sec:comp-gflownets} and \cref{sec:comparison-lwd}. 
In addition, \citet{sun2023difusco} (\difusco) and \citet{sanokowskidiffusion} (\diffuco) both utilizes diffusion model. 
%\citet{sun2022annealed, sanokowski2023variational, sanokowskidiffusion} use annealing techniques. \citet{sun2023difusco} was developed based on \citet{qiu2022dimes}, and \citet{sanokowskidiffusion} was based on \citet{sanokowski2023variational}. \citet{sanokowskidiffusion} can also be considered as an extension of \citet{karalias2020erdos}.

Most of the algorithms above also work on other types of graph CO problems, such as Maximum Cut, Maximum Vetex Cover, and Minimum Dominating Set. Some~\citep{khalil2017learning, qiu2022dimes, sun2023difusco} also work for the Travelling Salesman Problem (TSP).

\paragraph{Other GPU-based solvers for CO}
Recently, some non-learning GPU-based solvers for CO problems have been developed. \citet{sun2023revisiting} developed a GPU-accelerated sampling based method which works on MIS, Max Cut, and TSP. \citet{schuetz2022combinatorial, ichikawa2023controlling} uses GNNs conduct non-convex optimization for MIS without machine learning. \citet{alkhouri2024dataless} uses direct quadratic optimization without GNNs for MIS.

%\textcolor{blue}{(\citet{schuetz2022combinatorial} uses GNNs without learning to conduct non-convex optimization for MIS, though it provides inferior results compared to \deggreedy according to \citet{angelini2023modern}. \citet{ichikawa2023controlling} improved that by adding an annealing term to the loss function.)}
\paragraph{Benchmarks for MIS}
\citet{boether_dltreesearch_2022} provides a benchmark for several MIS algorithms including \gurobi~\citep{gurobi}, \kamis (\redumis)~\citep{lamm2017finding}, Intel-Treesearch~\citep{li2018combinatorial}, DGL-Treesearch, and \lwd~\cite{ahn2020learning}. It is the only MIS benchmark we know about including recent AI-inspired method, though it only focuses its comparsion for learning-based tree search algorithms. It suggests that \lwd is better than learning-based tree search algorithms. This aligns with our results where \lwd performs the best among learning-based algorithms. This benchmark covers various types of random graphs and several real-world datasets, so it is a good reference benchmark for comparison over different types of graphs. Though unlike our benchmark, it does not provide comparison across various size and density for random graphs. Our benchmark fills this gap and provides a more detailed comparison. We also include many newer AI-inspired algorithms, and greedy algorithms which leads to detailed analysis like serialization.
\citet{angelini2023modern} provides a comparison between \citet{schuetz2022combinatorial} and \deggreedy over regular graphs. The algorithm papers also report experiments comparing with previous algorithms, but those comparisons usually only focus on a few datasets and a few selected baselines.

%It is difficult to find real-world datasets suitable for benchmarking ML-based graph algorithms since training requires a large number of graphs. \citet{Morris+2020} generates and collects a number of graph benchmark datasets for learning algorithms. We select REDDIT-MULT-5K and COLLAB~\citep{yanardag2015deep} from their datasets since they are best suited for our benchmark.