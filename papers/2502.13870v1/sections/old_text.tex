




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INTRO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\BY{in a trustworthy way-- it is a bit circular here -- is showing trustworthiness in the explanation the same as building trust with users? I suppose the first trust angle is provided by the developers and the latter about the users? } 
%

%A sentiment analysis model classifies the sentence ``Her acting never fails to impress" as positive by identifying the double negative interaction ``never fails''. 

 % %In a sentiment classification task, the model classifies the sentence ``Her acting never fails to impress" as positive. The double negative interaction between ``never" and ``fails", is critical for classification. Marginal attribution methods only capture the individual positive marginal  effect of ``never" and ``fails".
    %
    % In a multi-document context question-and-answer task, \SpecExp{}  determines how much each contextual document contributes to answering the question: ``What is the weather like during Rio Carnival?". Two documents, one containing information about the festival ``Rio Carnival" and the other about ``Climate of Rio" are required to determine the correct answer, but each individual document does not provide enough information to answer.  \SpecExp{}  finds a strong positive interaction between these documents. In contrast, the document containing the weather of another city named Rio pushes the model away from the correct answer. This document interacts negatively with other documents since it provides seemingly contradictory information.
    %
    %(c) Image Patch Interactions:
    %
    %We ask the model to summarize an unusual image. If the model sees only one of the two image patches containing the dog  or the basketball, the true generated response is very unlikely, however, when the model sees both the dog and the basketball, they interact, and the model becomes confident enough to output the correct but unlikely response that the dog is playing basketball.}

% We detail the \SpecExp{} , and the masking pattern motivated by concepts from signal processing, coding and information theory which exploits  \emph{sparsity} between features to effectively identify influential interactions for input sizes up to 1000 features. 
% \item \textbf{Identifying Influential Interactions}. We compare \SpecExp{}  to both marginal and interaction attribution approaches across multiple benchmarks. \SpecExp{}  more faithfully (up to 20\%) reconstructs the outputs of LLM outputs over other approaches. 
% \item \textbf{Case Studies}. We compare \SpecExp{}  to both marginal and interaction attribution approaches across multiple benchmarks. \SpecExp{}  more faithfully (up to 20\%) reconstructs the outputs of LLM outputs over other approaches. 


% \begin{itemize}
%     \item Section~\ref{sec:method}: The \SpecExp{}  algorithm, which draws on ideas from signal processing, coding and information theory and exploits \emph{sparse interactions} between features  to speed up computation. For the first time, under a realistic interaction sparsity assumption %\BY{under a realistic interaction sparsity assumption} 
%     to the best of our knowledge, \SpecExp{}  enables computation of feature-interaction-based explanations at a large scale, suitable for use in LLMs. While we focus on LLM applications in this work, our methods are agnostic to the model type, and are widely applicable to other {\bf non deep learning} based models as well.
%     \BY{"tokens" were used before. are they the same? or?}\JK{We've generally remove the use of "tokens" from the paper. We are not restricted to interpreting each feature as its own token, since we don't look inside the model. Of course there could be advantages to using the same atoms as the underlying attention, but in many cases token level is too much granularity.}
%     \item Section~\ref{sec:language}: Quantitative evaluation of \SpecExp{}  on large-scale problems with respect to its faithfulness and recall on a series of LLM tasks.
%     \item Section~\ref{sec:case-study}: Case studies of the capabilities of \SpecExp{}  to increase human understanding of LLM outputs. \BY{do we use human evaluations -- more details here}\JK{Will come back to this once that section is written}
% \end{itemize}




% feature interactions in machine learning models often struggle to scale to the high-dimensional and complex nature of LLM inputs. This paper introduces \SpecExp{} , a novel framework that leverages concepts from signal processing, coding and information theory to efficiently and effectively identify and explain significant feature interactions at the scale of LLMs \BY{with an implicit assumption that the interactions are sparse which often holds in practice.}
%Understanding the factors that drive the predictions of Large Language Models (LLMs), especially the interplay between different input features, is crucial for researchers. Understanding these factors can help build trust, enable debugging, and drive progress in LLM development through improved understanding.

% %

% \subsection{Interactions}
% \SpecExp{}  goes beyond the first order game-theoretic approach of Shapley values, and also computes interactions between inputs. \BY{We need to be very clear with what SHAP can do and what we can do.  Here is what I understand based on Fig. 1 later: SHAP-like methods can do marginal  for n, and second or higher order interactions for small n, and we can do any order of interactions under the assumption of sparsity which holds often in practice.} The core observation \BY{citations to support this core observation?} that enables this is that most functions of interest are well represented by a \emph{surrogate function} with a sparse Fourier spectrum \BY{is this the same as token interactions sparsity? Clarify?} By exploiting this observation, we can, for the first time, compute interactions at the scale needed for their application in LLMs, or other black box models with a large \BY{binary input?} space. This is in contrast to the current standard for interaction based explanations. For example, the recently released SHAP-IQ benchmark \cite{fumagalli2023shapiq} considers only small input spaces around $n < 20$, while our algorithm can handle up to $n \approx 1000$ features. \BY{Fig. 1 is great. But the writing is not consistent with info in Fig 1 -- see my comments above. pls write as indicated by Fig. 1}




%Complexity is a central challenge in explainable machine learning, with many approaches requiring costly computations for every feature and data point in a dataset. This computational hurdle can be even more significant for \emph{interaction based explanations}, where one aims to identify important interactions between subsets of features. Addressing the fundamental reasons for this computational burden is critical for scaling explanations for the future, and enabling the widespread integration of explanations into Machine Learning workflows.

%\subsection{\SpecExp{} : Built for Scale}

%The \SpecExp{}  framework are depicted in Fig.~\ref{fig:algorithm-block}. The algorithm determines a series of ``masking patterns", which tells us which features to occlude for inference. For each of these masking patterns, we occlude the relevant features of the underlying input and perform inference on our black-box model. Once we collect the output of the model over all of our masking patterns, we take the output and compute the Fourier transform of our surrogate function. Finally, to compute our explanation, we process the Fourier transform to extract the desired interaction index. 

% \begin{figure*}[t!]
%     \centering
% \includegraphics[width=0.99\linewidth]{figures/interaction_diagram.pdf}
%     \caption{Interaction Example [W.I.P.]}
%     \label{fig:cc-intro}
% \end{figure*}

%To understand why \SpecExp{}  works where other methods fail, 

% What distinguishes our proposed \SpecExp{}  algorithm from the state-of-the-art (SOTA) in the field?  
% %is that 
% %it is critical to understand that it 
% Our approach does not compute all $2^n$ interactions between inputs, or even all $O(n^{t})$ $t$-wise interactions, like other approaches. Instead, \SpecExp{}   identified and computes a much smaller set of interactions that are \emph{quantifiably important} in an efficient manner. 
% We carefully design our masking pattern, as informed by concepts from signal processing, coding theory and information theory, in order to identify these important interactions, without having to perform an exhaustive search. We show that this smaller set of interactions still captures the important information while providing a much more concise representation \BY{in a scalable manner.}



% \subsection{Main Contributions}
% This paper proposes, evaluates and demonstrates \SpecExp{} . The main contributions are given below.
% \begin{itemize}
%     \item Section~\ref{sec:method}: The \SpecExp{}  algorithm, which draws on ideas from signal processing, coding and information theory and exploits \emph{sparse interactions} between features  to speed up computation. For the first time, under a realistic interaction sparsity assumption %\BY{under a realistic interaction sparsity assumption} 
%     to the best of our knowledge, \SpecExp{}  enables computation of feature-interaction-based explanations at a large scale, suitable for use in LLMs. While we focus on LLM applications in this work, our methods are agnostic to the model type, and are widely applicable to other {\bf non deep learning} based models as well.
%     \BY{"tokens" were used before. are they the same? or?}\JK{We've generally remove the use of "tokens" from the paper. We are not restricted to interpreting each feature as its own token, since we don't look inside the model. Of course there could be advantages to using the same atoms as the underlying attention, but in many cases token level is too much granularity.}
%     \item Section~\ref{sec:language}: Quantitative evaluation of \SpecExp{}  on large-scale problems with respect to its faithfulness and recall on a series of LLM tasks.
%     \item Section~\ref{sec:case-study}: Case studies of the capabilities of \SpecExp{}  to increase human understanding of LLM outputs. \BY{do we use human evaluations -- more details here}\JK{Will come back to this once that section is written}
% \end{itemize}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figures/block_diagram.pdf}
%     \caption{Starting with an input, \SpecExp{}  beings with a masking design procedure. The masking design determines a set of different }
%     \label{fig:algorithm-block}
% \end{figure*}








%We learn this Fourier transform by carefully designing masking patterns that exploit the sparse structure and
%using efficient algorithms from channel coding as a search engine. This avoids the exhaustive search required by current approaches. \Abhi{Return to last two sentences.}
%We learn this Fourier transform using an efficient algorithm from \BY{channel coding} as a search engine via a carefully designed masking pattern on input features that captures the effect of important interactions without having to perform an exhaustive search as required by current approaches. Marginal attribution methods fail to capture the double negative interaction ``never fails'' while \SpecExp{} can identify these types of interactions. 
%







 %\BY{The CD methods do provide interactions and we should compare if possible, at least we should mention it -- or say that we concentrate on the widely used interpretation methods.}
%
%\Abhi{Added complexity analysis} %\Efe{This lower bound is correct for fixed $d$. But is it also true when $d$ grows?}
% \Abhi{Discuss \SpecExp{} complexity here?}
%\BY{CD-T works pretty well for interactions -- see my  email to Aliyah and Abhi} 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% RELATED WORK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \citet{enouen2023textgenshap, paes2024multi, miglani2023using} all consider generating explanations for generative models language models.
% %
% The first two works approach large scale problems via a hierarchical approach: Start with a small number of larger features, i.e., paragraphs and then slowly increase the feature space by considering more fined grained analysis (sentence, or word/token level) of those features found to be important. None of these works consider interactions between features.



% STILL TO CITE: \cite{Jin2020Towards, chen2018learning}

%\paragraph{Data Valuation} Though this work focuses on \emph{feature attribution} it is also possible to consider \emph{data attribution} or \emph{data valuation} \cite{wang2023}. The methods we consider in this work are also generally applicable to data valuation as well, however, the computation cost of obtaining counterfactual information could be prohibitive. 


%For example, \citet{enouen2023textgenshap} compute Shapley values \cite{shapley1952, Lundberg2017}, a utility division concept from game theory, for groups of inputs based on a \emph{scalarization} of outputs. These Shapley values indicate how important different parts of the input are in determining the output, which can help with reasoning about the model's decisions. 
%Attribution methods primarily focus on marginal importance for features, however, in many cases, \emph{interactions} between features play a critical role. 


%Specifically, the M\"obius transform amplifies small noisy variations in model outputs, while the orthonormality of the Fourier transform ensures these variations remain small in the Fourier domain. 
%
% \BY{We need to review CD-T here.}
% \Abhi{Added sentence.}
%
%See Sec.~\ref{sec:fourier_background} for a detailed comparison.

%We consider the Fourier transform, which deals with noise much better due to orthogonality, and we also implement a series of novel algorithmic ideas based on coding theory. Together these contributions vastly improve robustness, and enable us to develop SOTA algorithms that can be applied to practical models and datasets. 

%\BY{why this work is a non-trivial advance from the previous work?}
%\Abhi{Addressed above.}








%%%%%%%%%%%%%%%%%%%%%% METHOD %%%%%%%%%%%%%%%%%%%%%%%%%


% We do so by constructing a surrogate function that approximates the behavior of the underlying model locally (i.e., for any given input), from which explanations and visualizations can be easily generated. 

% \paragraph{Explanation Function} 
% We define a value function for a model with $n$ inputs across subsets $S \subseteq [n]$ denoted $f(S)$. The exact construction of this function can vary.  In Fig.~\ref{fig:cc-intro}a, there are $n$ words and any word not in $S$ might be masked with a $[\text{MASK}]$ token, in Fig.~\ref{fig:cc-intro}b, documents not in $S$ are omitted from the context, and in Fig.~\ref{fig:cc-intro}c, patches of the image can be replaced with black or blurred patches. 
% For the purposes of the mathematical development of this paper, we equivalently write $f : \bbF_2^n \rightarrow \bbR$, where $f(S) = f(\bbm)$ with $S = \{ i : m_i = 1\}$. The vector $\bbm$ is a binary vector that represents a \emph{masking pattern} if $m_i = 0$ we evaluate the model after masking the $i$th input. Furthermore, if the output of the model is not already a scalar, we consider the process of \emph{scalarization} \cite{paes2024multi}, wherein we convert output text of logits, or any other output into a scalar value.
% The relationship between $f : \bbF_2^n \rightarrow \bbR$ and its Fourier transform $F : \bbF_2^n \rightarrow \bbR$ is characterized by the transform:
% \begin{equation}\label{eq:transform}
%     f(\bbm)  = \sum_{\bk \in \bbF_2^n} (-1)^{\inp{\bbm}{\bk}} F(\bk),
% \end{equation}
% Note that each coefficient of the Fourier transform $F(\bk)$ corresponds to an \emph{interaction} between inputs $i$ such that $k_i = 1$. Appendix~\ref{apdx:fourier-interactions} describes the relationship between the interaction indicies and the coefficients $F(\bk)$.
% \BY{under what probability distribution? and where does it come from?}\JK{It was a half-baked comment. Have seen people try to estimate conditionals and then do this kind of thing with tabular data, but no need to mention it here.} 





% \textcolor{red}{SWITCH NOTATION TO SET NOTATION?}

% \emph{Fourier to Banzhaf Interaction Indices:} The Fourier coefficients themselves have a natural interpretation as a scaled Banzhaf Interaction Index, introduced by \cite{roubens1996interaction}:
% \begin{equation}
%     .
% \end{equation}
% %The connection between the two was discovered in \cite{faigle2016bases}.

% \emph{Fourier to Mobius:}
% \begin{equation}
%     .
% \end{equation}

% \emph{Fourier to Faith-Banzhaf Interaction Indices:} \cite{tsai2023faith, hammer1992approximations,grabisch_equivalent_2000}
% \begin{equation}
%     .
% \end{equation}

% We provide a high-level overview \BY{of what?} in the next section 


%Uniquely we formulate the problem of computing a surrogate function as recovering an estimate of its Fourier transform. 
%
%This is distinct form most other interaction and attribution methods, that instead consider a function over a representation of AND functions. 
%
%The orthonormality of the Fourier transform means that small noisy variations in $f$ remain small in the Fourier domain. In contrast, AND interactions, which operate under the non-orthogonal Möbius transform \cite{kang2024learning}, can amplify small noisy variations in $f$, leading to large fluctuations in computed interactions.
%
%Fortunately, nothing is lost here, as it is straightforward to generate AND interactions from $\hat{f}$ or  $\hat{F}$.  Table~\ref{tab:fourier-def-1} highlights some key relationships, and Appendix~\ref{app:interactions} provides a comprehensive list.
%Most interaction indices define AND interactions. 
%
%For example, the positive interaction in Fig.~\ref{fig:cc-intro}a indicates that when both ``never" AND ``fails" are present in a sentence, the model's output shifts in a positive direction.
%
%In contrast, the Fourier coefficients $F(\bk)$ define XOR interactions, which are less intuitive: the positive AND interaction in Fig.~\ref{fig:cc-intro}a corresponds to a negative second order XOR interaction. 
%
%Fortunately, this is not problematic, as it is straightforward to generate AND interactions from the surrogate function $\hat{f}$. Most popular interaction indices even have straightforward definitions in terms of Fourier coefficients $F$.  Table~\ref{tab:fourier-def-1} highlights some key relationships, and Appendix~\ref{app:interactions} provides a comprehensive list.  
%
%The XOR structure of the Fourier transform is critical for the success of \SpecExp. The Fourier transform is an 
%orthonormal transform, it preserves magnitude, meaning small noisy variations in $f$ remain small in the Fourier domain. In contrast, AND interactions, which operate under the non-orthogonal Möbius transform \cite{kang2024learning}, can amplify small noisy variations in $f$, leading to large fluctuations in computed interactions. 
%\JK{Maybe this section not worth the space after all.}

%When we subsample a function $f$ at only a small fraction of masking patterns $\bbm$, the Nyquist sampling theorem says that the Fourier transform of the subsampled function, denoted $u$ 
%correspond to cases where $\abs{\bk}$ is small, which we refer to as the low degree property. 
%This has been studies in \cite{ren2024towards, kang2024learning}, and is further validated by the quantitative benchmarks in Section~\ref{sec:language}. The algorithm can be divided into three parts, described in the rest of this section.
%In the first part, we design a set of \emph{masking patters} that determine sets of inputs which we mask. For each masking pattern, we take the original set of inputs, and alter certain inputs according to the designed \emph{masking pattern}. We then perform inference for each input with a different masking pattern, and collect the model outputs. In the second phase, we take these model outputs and decode them to compute a Fourier transform, which serves as a \emph{surrogate function} to captures the core behavior of the underlying neural networks. In the final phase of the algorithm, we can take our surrogate function and quickly and efficiently compute metrics for explanation.
%We refer to this phenomenon as \emph{sparsity}.
%\BY{this def needs to appear when first time sparisty is used in the paper}



%relationship between $f : \bbF_2^n \rightarrow \bbR$ and its Fourier transform $F : \bbF_2^n \rightarrow \bbR$ is characterized by the transform:



%\paragraph{Properties of the Fourier Transform} 
%We highlight two important properties of the Fourier transform that are relevant to \SpecExp.

% \textit{Aliasing Property}: A subsampled version of $f$, denoted $u : \bbF_2^b \rightarrow \bbR$ with $b < n $ defined below using $\bM \in\bbF_2^{b \times n}$, has Fourier transforms $U$ as follows:
% \begin{equation}\label{eq:alais_gen}
%     u(\bell) = f(\bM^\trans \bell) \iff U(\bj) = \sum_{\bM \bk = \bj} F(\bk).
% \end{equation}
%\Abhi{Do we need to define $M$ here?}

%\textit{Aliasing Property}: For $b < n$ and $\bM \in\bbF_2^{b \times n}$, let $u : \bbF_2^b \rightarrow \bbR$ denote a sub-sampled version of $f$. Then, $u$ has Fourier transform $U$ as follows:
%\begin{equation}\label{eq:alais_gen}
%    u(\bell) = f(\bM^\trans \bell) \iff U(\bj) = \sum_{\bM \bk = \bj} F(\bk).
%\end{equation}

%\textit{Shift Property}: For any function $f: \bbF_2^n \rightarrow \bbR$, if we shift the input by a some vector $\bp \in \bbF_2^n$, the Fourier transform changes as follows:
%\begin{equation}\label{eq:shift}
%    f_{\bp}(\bbm) = f(\bbm + \bp) \iff F_{\bp}(\bk) = (-1)^{\inp{\bp}{\bk}} F(\bk).
%\end{equation}


%\BY{i suppose we can choose 6? make a comment about the robustness of t choice?}
%
%For \SpecExp{}, given a parameter $t$  we construct a matrix $\bP \in \bbF_2^{p \times n}$, which is the parity matrix of a systematic $t$ error correcting BCH code\footnote{Such a BCH code can always be constructed using appropriate code shortening with $p \approx t \log_2(n)$.} of dimension $n$ \cite{Lin1999}. 
%
%\Abhi{Say ``parity matrix .. code shortening'' as a footnote? }
%
%The rows of $\bP$ serve as shift vectors for sampling in \SpecExp.
%
%With this, recovering $\bk$ from samples reduces to decoding a BCH code, which is a well studied problem. We employ ideas from both hard \cite{massey1969shift} and soft decoding \cite{Kamiya2001algebraic}. 

% \Abhi{refer to $t$ as max order?}\JK{$t$ is only max order in the sense that it is the max order $\bk$ our algorithm can possibly recover when using hard decoding. For soft decoding it is only a soft threshold, since I think the true is $\approx t + \sqrt{t}$. In practice we want $t$ to be bigger than our desired order, since higher $t$ increases error correction ability.}

% \Abhi{I see. I say this because we use $t$ as maximum order in the introduction.  } \JK{I changed it to $d$}

%The masks begin with a series of random linear codes: for some parameter $b$, and input size $n$  we select $C$ random matrices $\bM_c \in \bbF_2^{b \times n},\;c = 1,\dotsc, C$, where each entry in $\bM_c$, is chosen from the elements $\bbF_2$ with equal probability. We fix $C=3$, following the analysis of \citet{li2015spright}. Our base masking patterns are the rowspan of $\bM_c$.
%Note that the number of elements in $\mathrm{rowspan}(\mathbf{M})$ is $2^b$, thus, $b$ can control the number of masking patterns collected. The main purpose of the structure imparted by the matrices $\bM_c$ is to serve as \emph{hash functions} that can help isolate interactions in the spectral domain, which is important for recovering them. Secondly, given a parameter $t$ we also construct a matrix $\bP \in \bbF_2^{p \times n}$, which is the parity matrix of a systematic $t$ error correcting BCH code of dimension $n$. Such a code can always be constructed, with appropriate code shortening with $p \approx t \log_2(n)$ \cite{Lin1999}. For existing masks from the rowspan of each $\bM_c$, we generate $p$ new patterns by adding each row of $\bP$ to each of the existing patterns. The structure imparted by the $\bP$ matrix makes it possible to identify the inputs present in the important interactions, and to verify that they have been isolated by the hash functions. 

%\paragraph{Model Inference and Transformation} We view the process of performing repeated inference over masked samples as a way of \emph{subsampling} our model. From these samples, we construct a series of subsampled functions $u_{c,i}$:


%
%In the first part of the algorithm we design our masking patterns, which determine the inputs we will choose to ``mask" when we perform inference. The masking patterns are chosen according to a series of codes \cite{Lin1999}. While codes are typically used for the purpose of error correction, in this application, they allow our algorithm to be not only robust, but also vastly more computationally efficient. 


%When computing our surrogate function $\hat{f}$, we aim to estimate the largest values $F(\bk)$.
%If the output of the model is not already a scalar, we consider the process of \emph{scalarization}, wherein we convert text, or any other output into a scalar value. If output token log probabilities are available, one approach is to look at the probability of a target output, usually some likely output when the full input is used \cite{paes2024multi}. When the log probabilities are not available other approaches like BERTScore \cite{Zhang2020BERTScore} can be used. This allows us to define the function 

%The first step of the algorithm involves determining a small set of masking patterns $\bbm$ for which we collect model outputs $f(\bbm)$.
%
%Our choice of $\bbm$ is driven by the aliasing and shift properties of the Fourier transform, and designed to minimize the number of samples collected overall. 
%
% \subsection{Sentiment Analysis}
% This example considers a \emph{sentiment analysis} task, where the deep neural network is a binary classifier that maps input text to either a positive or negative sentiment class. In this task, out goal is to determine which words, or interactions between words contribute most to the overall sentiment. Fig.~\ref{fig:sentiment_analysis} shows the advantage of our approach over the other popular methods in the literature. 

% Fig.~\ref{fig:sentiment_faithfulness} compares SpectralExplain with other approaches for explanation across a range of different input lengths. We generally avoid comparing with Shapley-based methods, since they don't directly optimize for faithfulness and thus generally perform poorly when evaluated according to faithfulness. We compare against LIME, and a series of increasing order Faith-Banzhaf interaction models. We observe that LIME, which is a first order model remains computationally tractable, even for large sentences, while higher order Faith-Banzhaf models, while being more faithful, ultimately become computationally intractable. SpectralExplain is superior in both ways: it has much better scaling of computation, and maintains a high degree of faithfulness. Particularly at the scale of 1000+ input words, SpectralExplain is the only viable approach to go beyond linear models like LIME.

% SpectralExplain is a randomized algorithm and thus may produce different outputs. Fig.~\ref{fig:sentiment_recall} depicts the consistency of SpectralExplain across a set of examples. With only a few thousand queries to the model, SpectralExplain achieves a relatively consistent ranking of interactions.

% Most importantly Fig.~\ref{fig:sentiment_analysis} shows that the interactions derived from SpectralExplain can help us truly understand the behavior and reasoning behind the model's output. 
%for the sampling procedure. We choose $C=3$, informed by \citet{li2015spright} under a simplified sparse Fourier setting. We fix $t=5$, which determines the error correction capability of algorithm
%and is an approximate bound on the degree of $\bk$. 
%By default, we use $b=8$ and note when a different $b$ is used. The total collected samples are $\approx C2^bt \log(n)$. 
%
%For LASSO regression-based interaction indices, we choose the regularization parameter via $5$-fold cross-validation. 

%\Abhi{I don't think people will remember these hyper-parameters, is there a way to summarize?}
%
%In Appendix ~\ref{apdx:experiments}, we repeat the experiment as we vary the number of training masks used. We observe that \SpecExp{} continues to out-perform other approaches. 
%
%\textbf{Varying number of training masks.} 

%\SpecExp{} and all other methods are computed over a fixed number of masks. For all tasks, \SpecExp{} performs competitively with the best performing interaction index which requires enumeration of all interactions up to a given order. As seen in the run-time plot of Fig.~\ref{fig:faith}(a), the run-time of interaction indices explodes as $n$ increases. On the other hand, \SpecExp{} remains a viable approach to computing interactions for large input lengths. 



%Experiments show that \SpecExp{} is uniformly more faithful than other methods. \SpecExp{} out-continues to out-perform other approaches as we vary the number of masks used.  

%surrogate functions generated by \SpecExp{}. Experiments show that it universally outperforms all other methods in terms of faithfulness, with the number of collected samples fixed. In Appendix~\ref{apdx:experiments},  we observe that \SpecExp{} continues to out-perform other approaches as we vary the number of masks used. 

%We fix $b=8$ for experiments, and sweep over $b$ in Appendix~\ref{apdx:experiments}, where we observe similar results.  

% Fig.~\ref{fig:faith_sentiment} shows that for the sentiment task, \SpecExp{} remains a viable approach to computing interactions with even the largest input lengths. In this large $n$ regime, \SpecExp{}is computationally comparable to computing marginal approaches while meaningfully improving faithfulness. The best performing baseline, Faith-Banzhaf, is an interaction index that rapidly increases in computational complexity and suffers at scale since it enumerates all interactions.  

% On the DROP dataset we see that \SpecExp{} significantly outperforms marginal baselines in some sentence-length regimes. For larger $n$, this gain decreases. This could be a result of collecting insufficient masks for computing interactions accurately. Nonetheless, we still observe a non-trivial increase in faithfulness. For HotPot QA, \SpecExp{} significantly outperforms marginal approaches, and is competitive with high order Faith-Banzhaf methods that enumerate all possible interactions.  

% In the Hotpot QA dataset, \SpecExp{} does not uniformly outperform other interaction indices, but remains competitive, while maining low computational complexity. 

%we observe that gain compared to first order methods decreases. We hypothesis that this may be due to insufficient samples for capturing interactions, as the gap between marginal methods and \SpecExp{} is reduced.

%As discussed, \emph{HotpotQA} requires models to combine information across various sentences for question-answering. 

% We measure ability of interaction indices to recover top $k$ interactions. 
% %
% Specifically, let $I_{k}(\cdot)$ denote top $k$ interactions for the \emph{true} surrogate function $f$. 
% %
% For \textit{HotpotQA}, we have human-labeled interactions of sentences containing the information required for the correct answer, which we denote as $I_{k}^*$. 
% %
% For \emph{Sentiment}, we focus on reviews with small $n$, and run all possible $2^n$ inferences to compute the surrogate function $f$ exactly. %
% We explicitly compute the top $k$ interactions from $f$, and  select the largest as the $I_k(f) = I_k^*$. Then we define recall as:
% \begin{equation}
%     \text{Recall@}k =\frac{\abs{I_k^*\cap I_k(\hat{f})}}{k}.
% \end{equation}


%explicitly compute the true Faith-Banzhaf interaction indices and select the largest as the $I_k(f) = I_k^*$. Then we define recall as:
%Another metric for evaluation is the ability of \SpecExp{} to consistently recover the top $k$ interactions for a given input. Let $I_{k}(\cdot)$ correspond to the top $k$ interactions according to a given function $f$. For \textit{HotpotQA}, we have human-labeled interactions, which we denote as $I_{k}^*$. For \emph{Sentiment}, we explicitly compute the true Faith-Banzhaf interaction indices and select the largest as the $I_k(f) = I_k^*$. Then we define recall as:
%We evaluate \SpecExp{} against baselines across all datasets. We consider the metrics of faithfulness \cite{zhang2023trade} and top-$k$ removal \cite{cohenwang2024contextciteattributingmodelgeneration}, two well-established metrics for assessing explainability methods. 
%For HotpotQA, we also consider the metric of recall against the included labels containing features involved in interactions. 