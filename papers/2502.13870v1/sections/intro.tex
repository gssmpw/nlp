\section{Introduction}
\label{sec:intro}
Large language models (LLMs) perform remarkably well across many domains by modeling complex interactions among features\footnote{LLM features refer to inputs at any granularity, e.g, tokens, sentences in a prompt or image patches in vision-language models.}. 
%
Interactions are critical for complex tasks like protein design, drug discovery, or medical diagnosis, which require examining combinations of hundreds of features. 
%
As LLMs are increasingly used in such high-stakes applications, they require trustworthy explanations to aid in responsible decision-making.
%
Moreover, LLM explanations enable debugging and can drive development through improved understanding \cite{zhang2023tell}. 
\begin{figure*}[t!]
    \centering
\includegraphics[width=\linewidth]{figures/interaction_diagram.pdf}
    \caption{
    (a) Sentiment analysis: \SpecExp{} identifies the double negative ``never fails''. Marginal approaches assign positive attributions to ``never'' and ``fails''. 
   %
    (b) Retrieval augmented generation: \SpecExp{} explains the output of a RAG pipeline, finding a \textit{combination} of documents the LLM used to answer the question and ignoring unimportant information.
    %
    (c) Visual question answering: \SpecExp{} identifies interaction between image patches required to correctly summarize the image.} 
        \label{fig:cc-intro}
%\vspace{-8pt}
\end{figure*}

Current post-hoc explainability approaches for LLMs fall into two categories: (i) methods like Shapley values \cite{Lundberg2017} and LIME \cite{ribeiro2016should} compute \textit{marginal} feature attribution but do not consider interactions.
%
As a running example, 
consider a sentiment analysis task (see Fig.~\ref{fig:cc-intro}(a)) where the LLM classifies a review containing the sentence ``Her acting never fails to impress''.
%
Marginal attribution methods miss this interaction, and instead attribute positive sentiment to ``never'' and ``fails'' (see Fig.~\ref{fig:cc-intro}(a)).
%
(ii) \textit{Interaction indices} such as Faith-Shap \cite{tsai2023faith} attribute interactions up to a given order $d$.
%
That is, for $n$ input features, they compute attributions by considering all $O(n^d)$ interactions.
%
This becomes infeasible for small $n$ and $d$. This motivates the central question of this paper: 
\begin{center}
{\textit{Can we perform interaction attribution at scale for a large input space $n$ with reasonable computational complexity? }}
%\BY{CD-T is pretty fast and just got into ICLR 2025. }
\end{center}
We answer this question affirmatively with \emph{Spectral Explainer} (\SpecExp{}) by leveraging information-theoretic tools to efficiently identify important interactions at LLM scale. 
%algorithms from signal processing and channel coding theory to efficiently identify important interactions at LLM scale. 
%
The scale of \SpecExp{}  is enabled by the observation that LLM outputs are often driven by a small number of \emph{sparse} interactions between inputs \cite{tsui2024on, ren2024where}. 
%
See Fig.~\ref{fig:cc-intro} for examples of sparsity in various tasks. 
%depicts tasks where a small number of interactions are critical for understanding model outputs. 
%
\SpecExp{} discovers important interactions by using a sparse Fourier transform to construct a surrogate explanation function.
%
This sparse Fourier transform searches for interactions via a channel decoding algorithm, thereby avoiding the exhaustive search used in existing approaches.


%
Our experiments show we can identify a small set of interactions that effectively and concisely reconstruct LLM outputs with $n \approx 1000$.  
%
This scale is far beyond what current interaction attribution benchmarks consider, e.g., SHAP-IQ \cite{muschalik2024shapiq} only considers datasets with no more than 20 features. 
%
This is summarized in Fig~\ref{fig:phase-feasible-diagram}; marginal attribution methods scale to large $n$ but ignore crucial interactions.
%
On the other hand, existing interaction indices do not scale with $n$. \SpecExp{}  both  captures interactions \emph{and} scales to large $n$. For an $s$ sparse Fourier transform containing interactions of at most degree $d$, \SpecExp{} has computational complexity at most $\Tilde{O}(sdn)$. In contrast, popular interaction attribution approaches scale as $\Omega(n^d)$.

\textbf{Evaluation Overview.} We compare \SpecExp{} to popular feature and interaction indices across three standard datasets. Algorithms and experiments are made publicly available\footnote{\url{https://github.com/basics-lab/spectral-explain}}.
\begin{itemize}[topsep=0pt, itemsep=0pt,leftmargin=6pt]
    \item \textbf{Faithfulness.}  \SpecExp{}  more faithfully reconstructs ($\approx$ 20\% improvement) outputs of LLMs as compared to other methods across datasets. Moreover, it learns more faithful reconstructions with fewer model inferences.
    \item \textbf{Identifying Interactions.} \SpecExp{}  identifies a small number of influential interactions that significantly change model output. For one of our datasets, \textit{HotpotQA}, \SpecExp{} provides interactions that align with human annotations.
    \item \textbf{Case Studies.} We demonstrate how one might use \SpecExp{} to identify and debug reasoning errors made by closed-source LLMs (\emph{GPT-4o mini}) and for compositional reasoning in a large multi-modal model.
\end{itemize}

\begin{figure}[h]
    \centering
    %\vspace{-5pt}
    \includegraphics[width=0.8\linewidth]{figures/vignette.pdf}\vspace{-8pt}
    \caption{Marginal attribution approaches scale to large $n$, but do not capture interactions. Interaction indices only work for small $n$. \SpecExp{}  computes interactions \emph{and} scales.}
    \label{fig:phase-feasible-diagram}
    \vspace{-10pt}
\end{figure}

%\BY{what does it mean for these interactions change ground-truth human labelled interactions? clarify}\JK{Changed to human labeled}.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/block_diagram.pdf}
    \caption{\SpecExp{} utilizes channel codes to determine masking patterns.  We observe the changes in model output depending on the used mask. \SpecExp{} uses message passing to learn a surrogate function to generate interaction-based explanations.}
    \vspace{-5pt}
    \label{fig:algorithm-block}
\end{figure*}
























