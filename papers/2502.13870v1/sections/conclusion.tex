\section{Conclusion}
\label{sec:conclusion}
%We have both qualitatively and quantitatively demonstrated the capability of \SpecExp{}
Identifying feature interactions is a critical problem in machine learning. 
We have proposed \SpecExp{}, the first interaction based model-agnostic post-hoc explanation algorithm that is able to scale to over $1000$ features. 
%
\SpecExp{} achieves this by making a powerful connection to the field of channel coding. This enables \SpecExp{} to avoid the $O(n^{d})$ complexity that existing feature interaction attribution algorithms suffer from. 
%\textcolor{blue}{By embedding algebraic structure into the masking patterns and employing decoding algorithms from channel coding theory, we circumvent the $O(n^{d})$ complexity that prohibits prior state-of-the-art interaction attribution algorithms from scaling.}
%
Our experiments show \SpecExp{} is able to significantly outperform other methods across the \emph{Sentiment}, \emph{Drop} and \emph{HotpotQA} datasets in terms of faithfulness, feature removal, and interaction recovery rate. 
%

\paragraph{Limitations} Sparsity is central to our algorithm, and without an underlying sparse structure, \SpecExp{} can fail. 
%A stronger theoretical foundation of Fourier sparsity would significantly improve trust in sparse approaches like \SpecExp{}.
Furthermore, even though we make strides in terms of sample efficiency, the number of samples still might remain too high for many applications, particularly when inference cost or time is high.
%
Another consideration is the degree of human understanding we can extract from computed interactions. 
Manually parsing interactions can be slow, and useful visualizations of interactions vary by modality.
Further improvements in visualization and post-processing of interactions are needed.


\paragraph{Future Work} \SpecExp{} works in a non-adaptive fashion, pre-determining the masking patterns $\bbm$. For greater sample efficiency, \emph{adaptive} algorithms might be considered, where initial model inferences help determine future masking patterns. In addition, we have focused on \emph{model-agnostic} explanations, but future work could consider combining this with internal model structure. 
Finally, interactions are a central aspect of the \emph{attention} structures in transformers. Studying the connection between \SpecExp{} and sparse attention  \cite{chen2021scatterbrain} is another direction for future research. 


%Empirically we show that it scales to large input spaces and outperforms popular marginal attribution approaches in terms of faithfulness, removal and recall tasks. Finally, we showcase potential applications of \SpecExp{}. 
