\section{Related Work}
\label{sec:related_work}

LLMs are capable of generating rationalizations for their outputs, but such rationalizations are another form of model output, susceptible to the same limitations \cite{sarkar2024large}.
%
In contrast, this work focuses on explanations in the form of feature attributions that are \emph{grounded} in the model's inputs and outputs, and can be applied to any ML model. i.e., model-agnostic methods. 
%
Moreover, model-agnostic methods can be applied to LLMs incapable of explaining their own output such as protein language models as well as encoder-only models (see experiments in Sec.~\ \ref{sec:language})

%%
\paragraph{Model-Agnostic Feature Attributions} LIME \cite{ribeiro2016should}, SHAP \cite{Lundberg2017}, and Banzhaf values \cite{wang2023} are popular model-agnostic feature attribution approaches.
%
SHAP and Banzhaf use game-theoretic tools for feature attribution, while LIME fits a sparse linear model.
%
%
\citet{chen2018learning} utilize tools from information theory for feature attributions. 
%
Other methods 
\cite{sundararajan2017axiomatic,binder2016layerwiserelevancepropagationneural} instead utilize internal model structure to derive feature attributions. 


\vspace{-2pt}
\paragraph{Interaction Indices} \citet{tsai2023faith} and \citet{dhamdhere2019shapley} extend Shapley values to consider interactions. 
%Many notions of feature interaction are proposed in the literature \cite{tsai2023faith, dhamdhere2019shapley}.
%
\citet{fumagalli2023shapiq} provide a general framework towards interaction attribution but can only scale to at most $n \approx 20$ input features. 
%
\citet{ren2023can, ren2024towards} theoretically study sparse interactions, a widely observed phenomenon in practice. 
%consider several theoretical aspects of interactions, including a study of \emph{sparsity} among interactions, which is a widely observed phenomenon in practice \cite{tsui2024on, ren2024where}. 
%
\citet{kang2024learning} show that sparsity under the M\"obius transform \cite{harsanyi1958bargaining} can be theoretically exploited for efficient interaction attribution. 
%
In practice, the proposed algorithm fails due to noise being amplified by the non-orthogonality of the M\"obius basis.
%
Our work utilizes the \emph{orthonormal} Fourier transform, which improves robustness by preventing noise amplification. 
%
\citet{hsu2024efficientautomatedcircuitdiscovery} apply tools from 
mechanistic interpretability such as circuit discovery for interaction attribution.


\vspace{-8pt}
\paragraph{Feature Attribution in LLMs}
\citet{enouen2023textgenshap, paes2024multi} propose hierarchical feature attribution for language models that first groups features (paragraphs) and then increase the feature space via a more fine-grained analysis (sentences or words/tokens). 
%
\citet{cohenwang2024contextciteattributingmodelgeneration} provide marginal feature importances via LASSO. 
%
These works do not explicitly compute interaction attributions. 






