\section{Overview: Fourier Transform Formulation}
\label{sec:fourier_background}
We review background on the Fourier transform for \SpecExp{}. 

\vspace{-7pt}
\paragraph{Model Input} Let $\mathbf{x}$ be the input to the LLM where $\mathbf{x}$ consists of $n$ input features, e.g., words. 
%
For $\mathbf{x} = $ ``Her acting never fails to impress'', $n = 6$. 
%
In Fig.~\ref{fig:cc-intro}(b) and (c), $n$ refers to the number of documents or image patches. 
%
For $S \subseteq [n]$, we define $\mathbf{x}_{S}$ as a masked input where $S$ denotes the coordinates in $\mathbf{x}$ we replace with the $\texttt{[MASK]}$ token. 
%
For example, if $S = \{3\}$, then the masked input $\mathbf{x}_{S}$ is ``Her acting \texttt{[MASK]} fails to impress''. 
%
Masks can be more generally applied to any input. 
%
In Fig.~\ref{fig:cc-intro}(b) and (c), masks are applied over documents and image patches respectively. 
%

\vspace{-7pt}
\paragraph{Value Function} For input $\mathbf{x}$, let $f(\mathbf{x}_S) \in \mathbb{R}$ be the output of the LLM under masking pattern
$S$. 
%
In sentiment analysis, (see Fig.~\ref{fig:cc-intro}(a)) $f(\mathbf{x}_{S})$ is the logit of the positive class. 
%
If $ \mathbf{x}_{S}$ is ``Her acting \texttt{[MASK]} fails to impress", this masking pattern changes the score from positive to negative.  
%
For text generation tasks, we use the well-established practice of \textit{scalarizing} generated text using the negative log-perplexity\footnote{Other approaches to scalarization exist: text embedding similarity, BERT score, etc. See \cite{paes2024multi} for an overview.} of generating the \textbf{original} output for the unmasked input $\mathbf{x}$ \cite{paes2024multi,cohenwang2024contextciteattributingmodelgeneration}.
%\Efe{I think log-perplexity is the negative log-likelihood. We're using log-likelihood, which is negative log-perplexity.} 
%
Since we only consider sample-specific explanations for a given $\mathbf{x}$, we suppress dependence on $\mathbf{x}$ and write $f(\mathbf{x}_S)$ as $f(S)$. 

\vspace{-7pt}
\paragraph{Fourier Transform of Value Function} Let $\bbF_2^n = \{0,1\}^n$, and addition between two elements in $\bbF_2$ as XOR. Since there are $2^n$ possible masks $S$, we equivalently write $f : \bbF_2^n \rightarrow \bbR$, where $f(S) = f(\bbm)$ with $S = \{ i : m_i = 1\}$. 
%
That is, $\bbm \in \bbF_2^n$ is a binary vector representing a \emph{masking pattern}. 
%
If $m_i = 0$ we evaluate the model after masking the $i^{\text{th}}$ input. 
%
The Fourier transform $F : \bbF_2^n \rightarrow \bbR$ of $f$ is:
\begin{equation}\label{eq:transform}
    f(\bbm)  = \sum_{\bk \in \bbF_2^n} (-1)^{\inp{\bbm}{\bk}} F(\bk).
\end{equation}
The Fourier transform is an \emph{orthonormal} transform onto a parity (XOR) function basis \cite{odonnell2014analysis}.


\paragraph{Sparsity} 
$f$ is \emph{sparse} if $F(\bk) \approx 0$ for most of the $\bk \in \bbF_2^n$. Moreover, we call $f$ \emph{low degree}, if large $F(\bk)$ have small $\abs{\bk}$.
\citet{ren2024towards, kang2024learning, valle2018deep, yang2019fine} and experiments in Appendix~\ref{apdx:experiments} establish that deep-learning based value functions $f$ are sparse and low degree. 
%
See Fig.~\ref{fig:cc-intro} for examples. 
%


\vspace{-7pt}
\section{Problem Statement} 

Our goal is to compute an \textit{approximate surrogate} $\hat{f}$. 
%
\SpecExp{}  finds a small set of $\bk$ with $\abs{\bk} \ll n$ denoted $\cK$, and $\hat{F}(\bk)$ for each $\bk \in \cK$ such that
\begin{equation}\label{eq:surrogate}
    \hat{f}(\bbm) = \sum_{\bk \in \cK} (-1)^{\inp{\bbm}{\bk}} \hat{F}(\bk). 
\end{equation}
%
This goal is motivated by the Fourier sparsity that commonly occurs in real-world data and models.
%
Some current interaction indices \cite{tsai2023faith} determine $\cK$ by formulating it as a LASSO problem, and solving it via $\ell_1$-penalized regression \cite{tibshirani1996regression},
\begin{equation}
    \hat{F} = \argmin_{\hat{F}} \sum_{\bbm} \abs{f(\bbm) - \hat{f}(\bbm)}^2 + \lambda \norm{\hat{F}}_1.
\end{equation}
For given order $d$, this approach requires enumeration of all $O(n^{d})$ interactions. 
%
This leads to an explosion in computational complexity as $n$ grows, as confirmed by our experiments (see Fig~\ref{fig:faith}(a)). 
%
To resolve this problem, we need to find an efficient way to search the space of interactions.

%Our experiments (see Section~\ref{sec:language}) show this regression approach fails to scale because they enumerate all $O(n^{d})$ interactions for maximum degree of interaction $d$. This leads to an explosion in computational complexity as $n$ grows (see Fig~\ref{fig:faith}(a)). 
%\citet{tsai2023faith} utilize this approach to compute interaction attributions. 

\vspace{-5pt}
\paragraph{Ideas behind \SpecExp{}} The key to efficient search is realizing that we are not solving an \emph{arbitrary} regression problem: (i) the Fourier transform \eqref{eq:transform} imparts algebraic structure and (ii) we can design the masking patterns $\bbm$ with sparsity and that structure in mind. 
%
\SpecExp{} exploits this by embedding a BCH Code \cite{Lin1999}, a widely used algebraic channel code, into the masking patterns.
%
%Exploiting this,  
In doing so, we map the problem of searching the space of interactions onto the problem of decoding a message (the important $\bk$) from a noisy channel.
%
We decode via the Berlekamp-Massey algorithm \cite{massey1969shift}, a well-established algebraic algorithm for decoding BCH codes.

\section{\SpecExp: Algorithm Overview}
\label{sec:method}

We now provide a brief overview of \SpecExp{} (see Fig.~\ref{fig:algorithm-block}). A complete overview is provided in Appendix~\ref{apdx:algorithm}. The high-level description consists of three parts:
\begin{enumerate}[label={}, topsep=0pt, itemsep=0pt, leftmargin=*]
    \item {\bfseries Step 1}: Determine a minimal set of masking patterns $\bbm$ to use for model inference, and query $f(\bbm)$ for each $\bbm$.
    \item {\bfseries Step 2}: Efficiently learn the surrogate function $\hat{f}$ from the set of collected samples $f(\bbm)$. 
    %We do this via an iterative message passing algorithm, where each iteration, we first search for interactions via a channel decoding procedure, propagate the results to get intermediate estimates of $\hat{F}$, and then repeat.
    \item {\bfseries Step 3}: Use $\hat{f}$ and its transform $\hat{F}$ to identify important interactions for attribution.
\end{enumerate}

\subsection{Masking Pattern Design: Exploiting Structure}
We first highlight two important properties of Fourier transform related to masking design structure.

\textit{Aliasing (Coefficient Collapse) Property}: For $b \leq n$ and $\bM \in\bbF_2^{b \times n}$, let $u : \bbF_2^b \rightarrow \bbR$ denote a subsampled version of $f$. Then $u$ has Fourier transform $U$:
\begin{equation}\label{eq:alais_gen}
    u(\bell) = f(\bM^\trans \bell) \iff U(\bj) = \sum_{\bM \bk = \bj} F(\bk).
\end{equation}
\textit{Shift Property}: For any function $f: \bbF_2^n \rightarrow \bbR$, if we shift the input by some vector $\bp \in \bbF_2^n$, the Fourier transform changes as follows:
\begin{equation}\label{eq:shift}
    f_{\bp}(\bbm) = f(\bbm + \bp) \iff F_{\bp}(\bk) = (-1)^{\inp{\bp}{\bk}} F(\bk).
\end{equation}
\textbf{Designing Aliasing } The aliasing property \eqref{eq:alais_gen} dictates that when sampling according to $\bM \in \bbF_2^{b \times n}$, all $F(\bk)$ with image $\bj = \bM \bk$ are added together.
%
If only \emph{one} dominant $F(\bk)$ satisfies $\bM \bk = \bj$, which can happen due to sparsity, we call it a \emph{singleton}.
%
We want $\bM$ to maximize the number of singletons, since we ultimately use singletons to recover the dominant coefficients and estimate $\hat{F}$. 
% 
\SpecExp{} uses $\bM$ with elements chosen uniformly from $\bbF_2$. Such $\bM$ has favorable properties regarding generating singletons.

\textbf{Designing Shifts } Once we create singletons, we need to identify them, extract the dominant index $\bk$, and estimate $\hat{F}(\bk)$.
%
The shift property \eqref{eq:shift} is critical for this task since the sign of the dominant $F(\bk)$ changes depending on $\inp{\bp}{\bk}$. Thus, each time we apply a shift vector, we gather (potentially noisy) information about the dominant $\bk$. 
%
Finding $\bk$ and estimating $\hat{F}(\bk)$ can be modeled as communicating information over a noisy channel \cite{Shannon1948}, where the communication protocol is controlled by the shift vectors. We use the aforementioned BCH channel code, which requires only $\approx t \log(n)$ shifts to recover $\bk$. The parameter $t$ controls the robustness of the decoding procedure. Generally, if the maximum degree is $\abs{\bk} = d$, we choose $t \geq d$. If $t - \abs{\bk} > 0$, we use the additional shifts to improve the estimation of $\hat{F}(\bk)$. Since \emph{most} of the time $\abs{\bk}$ is less than $5$, we fix $t=5$ for experiments in this paper.   

\textbf{Combined Masking } Combining ideas from above, we construct $C = 3$ independently sampled $\bM_c$ and $p$ shifting vectors $\bp_i$, which come from rows of a BCH parity matrix.  Then, for $c \in [C]$ and $i \in [p]$, we entirely sample the function $u_{c,i}(\bell) = f(\bM_c^{\trans} \bell + \bp_i)$. The total number of samples is $\approx C2^bt\log(n)$. We note that all model inference informed by our masking pattern can be conducted in parallel.
%In Section~\ref{sec:language} we show that \SpecExp{} constantly outperforms other methods with the same number of samples. 
The Fourier transform of each $u_{c,i}$, denoted $U_{c,i}$, is connected to the transform of the original function via
\begin{equation} \label{eq:alais}
    U_{c,i}(\bj) = \sum_{\bk\; : \;\bM_c \bk = \bj} (-1)^{\inp{\bp_i}{\bk}} F(\bk).
\end{equation}

%The form of \eqref{eq:alais} is critical: $F(\bk)$ are summed together according to the linear hash $\bM_c$, and modulated according to the matrix $\bP$. Our choice of $\bM_c$ ensures that the large $F(\bk)$ are well-spread across the the different values of $\bj$, while our choice of $\bP$ ensures that each value $\bk$ leaves a sufficiently unique signature to be identified.

\subsection{Computing the Surrogate Function}
Once we have the samples, we use an iterative message passing algorithm to estimate $\hat{F}(\bk)$ for a small (a-priori unknown) set of $\bk \in \cK$. 

\vspace{-7pt}
\paragraph{Bipartite Graph} We construct a bipartite graph depicted in Fig.~\ref{fig:bipartite}. 
%
The observations $ \bU_c(\bj) = (U_{c,0}(\bj), \dotsc, U_{c,p}(\bj))$ are factor nodes, while the values $\hat{F}(\bk)$ correspond to variable nodes.
%
$\hat{F}(\bk)$ is connected to $\bU_c(\bj)$ if $\bM_c \bk = \bj$.
%
\begin{figure*}[t!]
\centering
\begin{center} 
  \includegraphics[width=.6\textwidth]{figures/faith_legend.pdf}
\end{center}

\begin{subfigure}[t]{\textwidth}
\centering
 \includegraphics[width=.91\textwidth]{figures/sentiment_faithfulness_fixed2.pdf}
 \vspace{-.2cm}
 \caption{\emph{Sentiment}}
     \label{fig:faith_sentiment}
\end{subfigure}
\begin{subfigure}[b]{.48\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/drop_faithfulness_fixed2.pdf}
   \vspace{-.2cm}
  \caption{\emph{DROP}}
  \label{fig:faith_drop}
\end{subfigure}%
\begin{subfigure}[b]{.48\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/hotpot_faithfulness_fixed2.pdf}
   \vspace{-.2cm}
  \caption{\emph{HotpotQA}}
  \label{fig:faith_hotpot}
\end{subfigure}
\vspace{-4pt}
\caption{(a) \SpecExp{} uniformly outperforms all baselines in terms of faithfulness. High order Faith-Banzhaf indices have competitive faithfulness, but rapidly increase in computational cost. (b) The DROP dataset contains only larger examples, so we primarily compare against first order methods.  (c) Our approach remains competitive in this task as well,  and still outperforms marginal approaches for large $n$. 
}
\label{fig:faith}
\vspace{-12pt}
\end{figure*}
%

\vspace{-7pt}
\paragraph{Message Passing} The messages from factor to variable are computed by attempting to decode a singleton via the Berlekamp-Massey algorithm. If a $\bk$ is successfully decoded, $\bk$ is added to $\cK$ and $F(\bk)$ is estimated and sent to factor node $\hat{F}(\bk)$. 
%
The variable nodes send back the average of their received messages to all connected factor nodes. The factor nodes then update their estimates of $\hat{F}$, and attempt decoding again.
%
The process repeats until convergence. Once complete the surrogate function is constructed from $\cK$ and $\hat{F}(\bk)$ according to \eqref{eq:surrogate}. 
%
Complete step-by-step details are in Appendix~\ref{apdx:algorithm}, Algorithm~\ref{alg:message-pass}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/bipartite.pdf}
    \caption{Depiction of the message passing algorithm for computing the surrogate function in \SpecExp{}.}% Each factor node $\bU_C(\bj)$, attempts BCH decoding via the Berlekamp-Massey algorithm to recover $\bk$. If the decoding succeeds, we estimate $F(\bk_i)$ where $\bk_i$ is the decoded singleton index. Variable nodes average received values, and send a message to connected factor nodes, which update according to the estimate, and iterate.}
    \label{fig:bipartite}
\end{figure}






