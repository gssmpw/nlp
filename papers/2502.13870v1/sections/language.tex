\begin{figure*}[t]
\centering
\begin{center} 
  \includegraphics[width=.63\textwidth]{figures/fidelity_legend.pdf}
\end{center}
\begin{subfigure}[b]{.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/sentiment_fidelity3_small.pdf}
  \caption{\emph{Sentiment} $n\in [32,63]$}
  \label{fig:fidelity_sentiment}
\end{subfigure}%
\begin{subfigure}[b]{.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/drop_fidelity_small.pdf}
  \caption{\emph{DROP} $n\in [32,63]$}
  \label{fig:fidelity_drop}
\end{subfigure}%
\begin{subfigure}[b]{.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/hotpot_fidelity_small.pdf}
  \caption{\emph{HotpotQA} $n\in [32,63]$}
  \label{fig:fidelity_hotpot}
\end{subfigure}
\begin{subfigure}[b]{.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/sentiment_fidelity3_large.pdf}
  \caption{\emph{Sentiment} $n\in [64,127]$}
  \label{fig:fidelity_sentiment2}
\end{subfigure}%
\begin{subfigure}[b]{.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/drop_fidelity_large.pdf}
  \caption{\emph{DROP} $n\in [64,127]$}
  \label{fig:fidelity_drop2}
\end{subfigure}%
\begin{subfigure}[b]{.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/hotpot_fidelity_large.pdf}
  \caption{\emph{HotpotQA} $n\in [64,127]$}
  \label{fig:fidelity_hotpot2}
\end{subfigure}
\vspace{-5pt}
\caption{On the removal task, \SpecExp{} performs competitively with 2\textsuperscript{nd} order methods on the \emph{Sentiment} dataset, and out-performs all approaches on \emph{DROP} and  \emph{HotpotQA} dataset for $n \in [32,63]$. When $n$ is too large to compute other interaction indices, we outperform marginal methods.}
\label{fig:fidelity}
\vspace{-12pt}
\end{figure*}

\vspace{-14pt}
\section{Experiments}
\label{sec:language}

\paragraph{Datasets} 
We use three popular datasets that require the LLM to understand interactions between features. 
\begin{enumerate}[ topsep=0pt, itemsep=0pt, leftmargin=*]
\item \emph{Sentiment} is primarily composed of the \emph{Large Movie Review Dataset} \cite{maas-EtAl:2011:ACL-HLT2011}, which contains both positive and negative IMDb movie reviews. The dataset is augmented with examples from the \emph{SST} dataset \cite{ socher2013recursive} to ensure coverage for small $n$. We treat the words of the reviews as the input features.
\item{\emph{HotpotQA} \cite{yang2018hotpotqa} is a question-answering dataset requiring multi-hop reasoning over multiple Wikipedia articles to answer complex questions. We use the sentences of the articles as the input features.}
\item{\emph{Discrete Reasoning Over Paragraphs} (DROP)} \cite{dua2019drop} is a comprehension benchmark requiring discrete reasoning operations like addition, counting, and sorting over paragraph-level content to answer questions. We use the words of the paragraphs as the input features. 
\end{enumerate}
%
%\emph{DROP} and \emph{HotpotQA} require , while \emph{Sentiment} is encoder-only. 
%
\vspace{-7pt}
\paragraph{Models} For \textit{DROP} and \textit{HotpotQA}, (generative question-answering tasks) we use \texttt{Llama-3.2-3B-Instruct} \cite{grattafiori2024llama3herdmodels} with $8$-bit quantization. For \emph{Sentiment} (classification), we use the encoder-only fine-tuned \texttt{DistilBERT} model \cite{Sanh2019DistilBERTAD,sentimentBert}.
%

\vspace{-7pt}
\paragraph{Baselines} We compare against popular marginal metrics LIME, SHAP, and the Banzhaf value. 
%
For interaction indices, we consider Faith-Shapley, Faith-Banzhaf, and the Shapley-Taylor Index. We compute all benchmarks where computationally feasible. That is, we always compute marginal attributions and interaction indices when $n$ is sufficiently small. In figures, we only show the best performing baselines. Results and implementation details for all baselines can be found in 
Appendix~\ref{apdx:experiments}.

\vspace{-6pt}
\paragraph{Hyperparameters} \SpecExp{} has several parameters to determine the number of model inferences (masks). We choose $C=3$, informed by \citet{li2015spright} under a simplified sparse Fourier setting. We fix $t = 5$, which is the error correction capability of \SpecExp{} and serves as an approximate bound on the maximum degree. 
%
We also set $b=8$; the total collected samples are $\approx C2^bt \log(n)$. 
%
For $\ell_1$ regression-based interaction indices, we choose the regularization parameter via $5$-fold cross-validation. 




\vspace{-3pt}
\subsection{Metrics}


We compare \SpecExp{} to other methods across a variety of well-established metrics to assess performance.
%\Efe{How about textbf rather than emph here?}

\textbf{Faithfulness}: To characterize how well the surrogate function $\hat{f}$ approximates the true function, we define \emph{faithfulness} \cite{zhang2023trade}:
\vspace{-3pt}
\begin{equation}
    R^2 = 1 -  \frac{\lVert \hat{f} - f \rVert^2}{\left\lVert f - \bar{f} \right\rVert^2},
\end{equation}
where $\left\lVert f  \right\rVert^2 = \sum_{\bbm \in \bbF_2^n}f(\bbm)^2$ and $\bar{f} = \frac{1}{2^n} \sum_{\bbm \in \bbF_2^n}f(\bbm)$.

Faithfulness measures the ability of different explanation methods to predict model output when masking random inputs. 
%
We measure faithfulness over 10,000 random \emph{test} masks per-sample, and report average $R^2$ across samples. 
%

\textbf{Top-$r$ Removal}: We measure the ability of methods to identify the top $r$ influential features to model output:
\vspace{-2pt}
\begin{align}
\begin{split}
    \mathrm{Rem}(r) = \frac{|f(\boldsymbol{1}) - f(\bbm^*)|}{|f(\boldsymbol{1})|}, \\
    \;\bbm^* = \argmax \limits_{\abs{\bbm} = n-r}|\hat{f}(\boldsymbol{1}) - \hat{f}(\bbm)|.
\end{split}
\end{align}
\vspace{-8pt}


\textbf{Recovery Rate@$r$:} 
%
Each question in \emph{HotpotQA} contains human-labeled annotations for the sentences required to correctly answer the question. 
%
We measure the ability of interaction indices to recover these human-labeled annotations. 
%
Let $S_{r^*} \subseteq [n]$ denote human-annotated sentence indices. %corresponding to the human-annotated sentences containing the answer. 
Let $S_{i}$ denote feature indices of the $i^{\text{th}}$ most important interaction for a given interaction index.
%
Define the recovery ability at $r$ for each method as follows
\vspace{-2pt}
\begin{equation}
\label{eq:recovery_k}
    \text{Recovery@}r = 
    \frac{1}{r}\sum^r_{i=1}\frac{\abs{S_r^*\cap S_i}}{|S_{i}|}.
\end{equation}
\vspace{-8pt}

Intuitively, \eqref{eq:recovery_k} measures how well interaction indices capture features that align with human-labels.   


\begin{figure*}[t]
\centering
\hfill
\begin{subfigure}[b]{.5\textwidth}
  \centering
    \hspace{0.82cm}\includegraphics[width=0.75\textwidth]{figures/recall_legend.pdf}
  \includegraphics[width=.9\linewidth]{figures/hotpot_recall.pdf}
  \caption{Recovery rate$@10$ for \emph{HotpotQA} }
  \label{fig:recovery_hotpot}
\end{subfigure}%
\hfill % To ensure space between the figures
\begin{subfigure}[b]{.46\textwidth}
  \centering
    \includegraphics[width=1\textwidth]{figures/hotpot.pdf}
  \caption{Human-labeled interaction identified by \SpecExp{}.}
  \label{fig:hotpot_additional}
\end{subfigure}
\hfill
\caption{(a) \SpecExp{} recovers more human-labeled features with significantly fewer training masks as compared to other methods. (b) For a long-context example ($n = 128$ sentences), \SpecExp{} identifies the three human-labeled sentences as the most important third order interaction while ignoring unimportant contextual information.}
\vspace{-8pt}
\end{figure*}

\vspace{-8pt}
\subsection{Faithfulness and Runtime}
\vspace{-3pt}

Fig.~\ref{fig:faith} shows the faithfulness of \SpecExp{} compared to other methods. We also plot the runtime of all approaches for the \emph{Sentiment} dataset for different values of $n$. 
%
All attribution methods are learned over a fixed number of training masks.
% 

\textbf{Comparison to Interaction Indices } \SpecExp{} maintains competitive performance with the best-performing interaction indices across datasets. 
%
Recall these indices enumerate \emph{all possible interactions}, whereas \SpecExp{} does not. 
%
This difference is reflected in the runtimes of Fig.~\ref{fig:faith}(a).
%
The runtime of other interaction indices explodes as $n$ increases while \SpecExp{} does not suffer any increase in runtime. 

\vspace{-2pt}
\textbf{Comparison to Marginal Attributions } For input lengths $n$ too large to run interaction indices, \SpecExp{} is significantly more faithful than marginal attribution approaches across all three datasets.

\vspace{-2pt}
\textbf{Varying number of training masks } Results in Appendix ~\ref{apdx:experiments} show that \SpecExp{} continues to out-perform other approaches as we vary the number of training masks. 

\vspace{-2pt}
\textbf{Sparsity of \SpecExp{} Surrogate Function} Results in Appendix ~\ref{apdx:experiments}, Table~\ref{tab:faith} show 
surrogate functions learned by \SpecExp{} have Fourier representations where only $\sim 10^{-100}$ percent of coefficients are non-zero. 


\vspace{-6pt}
\subsection{Removal}
\label{subsec:removal}

Fig.~\ref{fig:fidelity} plots the change in model output as we mask the top $r$ features for different regimes of $n$. 
%

\vspace{-2pt}
\textbf{Small $n$ } \SpecExp{} is competitive with other interaction indices for \textit{Sentiment}, and out-performs them for \textit{HotpotQA} and \textit{DROP}. 
%
Performance of \SpecExp{} in this task is particularly notable since Shapley-based methods are designed to identify a small set of influential features. 
%
On the other hand, \SpecExp{} does not optimize for this metric, but instead learns the function $f(\cdot)$ over all possible $2^n$ masks. 
%

\textbf{Large $n$ } \SpecExp{} out-performs all marginal approaches, indicating the utility of considering interactions.
%

\vspace{-10pt}
\subsection{Recovery Rate of Human-Labeled Interactions}

%
We compare the recovery rate \eqref{eq:recovery_k} for $r = 10$ of \SpecExp{} against third order Faith-Banzhaf and Faith-Shap interaction indices. 
%
We choose third order interaction indices because all examples 
are answerable with information from at most three sentences, i.e., maximum degree $d = 3$.
%
Recovery rate is measured as we vary the number of training masks. 

Results are shown in Fig.~\ref{fig:recovery_hotpot}, where \SpecExp{} has the highest recovery rate of all interaction indices across all sample sizes. 
%
Further, \SpecExp{} achieves close to its maximum performance with few samples, other approaches require many more samples to approach the recovery rate of \SpecExp{}. 

\textbf{Example of Learned Interaction by \SpecExp{}} Fig.~\ref{fig:hotpot_additional} displays a long-context example (128 sentences) from \emph{HotpotQA} whose answer is contained in the three highlighted sentences. 
%
\SpecExp{} identifies the three human-labeled sentences as the most important third order interaction while ignoring unimportant contextual information. 
%
Other third order methods are not computable at this length. 
%

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/case_studies.pdf}
    \caption{SHAP provides marginal feature attributions. Feature interaction attributions computed by SPEX provide a more comprehensive understanding of (above) words interactions that cause the model to answer incorrectly and (below) interactions between image patches that informed the model's output.}
    \label{fig:caseStudies}
\end{figure*}

