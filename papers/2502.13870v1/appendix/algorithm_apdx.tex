\subsection{Introduction}
This section provides the algorithmic details behind \SpecExp{}. The algorithm is derived from the sparse Fourier (Hadamard) transformation described in \citet{li2015spright}. Many modifications have been made to improve the algorithm and make it suitable for use in this application. Application of the original algorithm proposed in \citet{li2015spright} fails for all problems we consider in this paper. In this work, we focus on applications of \SpecExp{} and defer theoretical analysis to future work. 

\paragraph{Relevant Literature on Sparse Transforms} This work develops the literature on sparse Fourier transforms. The first of such works are \cite{Hassanieh2012, stobbe2012, Pawar2013}. The most relevant literature is that of the sparse Boolean Fourier (Hadamard) transform \cite{li2015spright,amrollahi2019efficiently}. Despite the promise of many of these algorithms, their application has remained relatively limited, being used in only a handful of prior applications. Our code base is forked from that of \cite{erginbas2023efficiently}.
In this work we introduce a series of major optimizations which specifically target properties of explanation functions. By doing so, our algorithm is made significantly more practical and robust than any prior work.

\paragraph{Importance of the Fourier Transform} 
The Fourier transform does more than just impart critical algebraic structure. 
The orthonormality of the Fourier transform means that small noisy variations in $f$ remain small in the Fourier domain. In contrast, AND interactions, which operate under the non-orthogonal MÃ¶bius transform \cite{kang2024learning}, can amplify small noisy variations, which limits practicality. Fortunately, this is not problematic, as it is straightforward to generate AND interactions from the surrogate function $\hat{f}$. Many popular interaction indices have simple definitions in terms of $F$.  Table~\ref{tab:fourier-def-1} highlights some key relationships, and Appendix~\ref{app:interactions} provides a comprehensive list. 

\begin{table}[h]
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Shapley Value} & \textbf{Banzhaf Interaction Index}         & \textbf{M\"obius Coefficient}  \\ \midrule
 $\mathrm{SV}(i) = \sum \limits_{S \ni i,\; \abs{S} \text{ odd}}F(S)/\abs{S} $& $I^{BII}(S) = (-2)^{|S|} F(S)$      &        $I^{M}(S) = (-2)^{|S|} \sum \limits_{T \supseteq S} F(T)$   \\ \bottomrule
\end{tabular}
\caption{Popular attribution scores in terms of Fourier coefficients}
\label{tab:fourier-def-1}
\end{table}

\subsection{Directly Solving the LASSO} 
Before we proceed, we remark that in cases where $n$ is not too large, and we expect the degree of nonzero $\abs{\bk} \leq d$ to be reasonably small, enumeration is actually not infeasible. In such cases, we can set up the LASSO problem directly:
\begin{equation}\label{eq:LASSO_apdx}
    \hat{F} = \argmin_{\tilde{F}} \sum_{\bbm}\abs{f(\bbm) - \sum_{\abs{\bk} \leq d}  \tilde{F}(\bk)}^2 + \lambda \norm{\tilde{F}}_1.
\end{equation}
Note that this is distinct from the \emph{Faith-Banzhaf} and \emph{Faith-Shapley} solution methods because those perform regression over the AND, M\"obius basis.
We observe that the formulation above typically outperforms these other approaches in terms of faithfulness, likely due to the properties of the Fourier transform. 

Most popular solvers use \emph{coordinate descent} to solve \eqref{eq:LASSO_apdx}, but there is a long line of research towards efficiently solving this problem. In our code, we also include an implementation of Approximate Message Passing (AMP) \cite{maleki2010approximate}, which can be much faster in many cases. Much like the final phase of \SpecExp{}, AMP is a low complexity message passing algorithm where messages are iteratively passed between factor nodes (observations) and variable nodes. 

A more refined version of \SpecExp{}, would likely examine the parameters $n$ and the maximum degree $d$ and determine whether or not to directly solve the LASSO, or to apply the full \SpecExp{}, as we describe in the following sections. 


\subsection{Masking Pattern Design and Model Inference}
The first part of the algorithm is to determine which samples we collect. All steps of this part of the algorithm are outlined in Algorithm~\ref{alg:collect}. This is governed by two structures: the random linear codes $\bM_c$ and the BCH parity matrix $\bP$. Random linear codes have been well studied as central objects in error correction and cryptography. They have previously been considered for sparse transforms in \cite{amrollahi2019efficiently}. They are suitable for this application because they roughly uniformly hash $\bk$ with low hamming weight. 

The use of the $\bP \in \bbF_2^{p\times n}$, the parity matrix of a binary BCH code is novel. These codes are well studied for the applications in error correction \cite{Lin1999}, and they were once the preeminent form of error correction in digital communications. A primitive, narrow-sense BCH code is characterized by its length, denoted $n_c$, dimension, denoted $k_c$ (which we want to be equal to our input dimension $n$) and its error correcting capability $t_c = 2d + 1$, where $d$ is the minimum distance of the code. For some integer $m > 3$ and $t_c < 2^{m-1}$, the parameters satisfy the following equations:
\begin{eqnarray}
    n_c &=& 2^m -1 \\
    p = n_c-k_c &\leq& mt.
\end{eqnarray}
Note that the above says we can bounds $p \leq t \left\lceil \log_2(n_c) \right \rceil$, and it is easy to solve for $p$ given $n=k_c$ and $t$, however, explicitly bounding $p$ in terms of $n$ and $t$ is difficult, so for the purpose of discussion, we simply write $p \approx{t\log(n)}$, since $n_c = p + n$, and we expect $n \gg p$ in nearly all cases of interest. 

We use the software package \verb|galois| \cite{Hostetter_Galois_2020} to construct a generator matrix, $\bG \in \bbF_2^{n_c \times k_c}$ in systematic form:
\begin{equation} \label{eq:sys-form}
    \bG = 
\begin{bmatrix}
\bI_{k_c \times k_c}\\
\bP
\end{bmatrix}
\end{equation}
\begin{algorithm}
   \caption{Collect Samples}
   \label{alg:collect}
\begin{algorithmic}[1]
   \State {\bfseries Input:} Parameters $(n, t, b, C=3, \gamma=0.9)$, Query function $f(\cdot)$ 
     \For{$j=1$ {\bfseries to} $n$, $i=1$ {\bfseries to} $b$, $c=1$ {\bfseries to} $C$} \Comment{Generate random linear code}
     \State $X_{ij} \sim \mathrm{Bern}(0.5)$
     \State $\left[\bM_c\right]_{i,j} \gets X_{i,j}$
   \EndFor
   \State $\mathrm{Code} \gets \mathrm{BCH}(n_c=n_c, k_c\geq n,  t_c=t)$ \Comment{Systematic BCH code with dimension $n$ and correcting capacity $t$}
   \State $p \gets n_c - n$
   \State $\bP \gets \mathrm{Code}.\bP$
   \State $\cP \gets \mathrm{rows}(\bP) = \left[ \boldsymbol{0}, \bp_1, \dotsc, \bp_p \right]$
   \ForAll{ $\bell \in \bbF_2^b$, $i \in \{0, \dotsc, p\}$, $c \in \{1, \dotsc, C\}$}
   \State $u_{c,i} (\bell) \gets f\left(\bM_c^\trans \bell + \cP[i] \right)$ \Comment{Query the model at masking patterns}
   \EndFor
      \ForAll{$i \in \{0, \dotsc, p\}$,  $c \in \{1, \dotsc, C\}$} 
   \State $U_{c,i} \gets \mathrm{FFT}(u_{c,i})$ \Comment{Compute the Boolean Fourier transform of the collected samples}
   \EndFor
   \State $\bU_c \gets \left[ U_{c,1}, \dotsc, U_{c, p}\right]$ 
    \State {\bfseries Output:} Processed Samples  $\bU_c,U_{c,0}\; c=1, \dotsc, C$
\end{algorithmic}
\end{algorithm}
Note that according to \eqref{eq:sys-form} $\bP \in \bbF_2^{ p \times k_c}$. In cases where $k_c > n$, we consider only the first $n$ rows of $\bP$. This is a process known as \emph{shortening}.
Our application of this BCH code in our application is rather unique. Instead of the typical use of a BCH code as \emph{channel correction} code, we use it as a \emph{joint source channel code}. 

Let $\bp_0 = \boldsymbol{0}$, and let $\bp_i,\;i=1,\dotsc,p$ correspond to the rows of $\bP$. We collect samples written as:
\begin{equation}\label{eq:subsample_apdx}
    u_{c,i} (\bell) \gets f\left(\bM_c^{\trans} \bell + \bp_i \right) \;\forall \bell \in \bbF_2^b,\; c = 1,\dotsc, C,\;i=0, \dotsc, p.
\end{equation}
Note that the total number of unique samples can be upper bounded by $C(p+1)2^{b}$. For large $n$ this upper bound is nearly always very close to the true number of unique samples collected. After collecting each sample, we compute the boolean Fourier transform. The forward and inverse transforms as we consider in this work are defined below.
\begin{equation}\label{eq:transform_def}
    \text{Forward:}\quad F(\bk) = \frac{1}{2^{n}} \sum_{\bbm \in \bbF_2^n} (-1)^{\inp{\bk}{\bbm}}f(\bbm) \qquad \text{Inverse:}\quad f(\bbm)  = \sum_{\bk \in \bbF_2^n} (-1)^{\inp{\bbm}{\bk}} F(\bk),
\end{equation}
When samples are collected according to \eqref{eq:subsample_apdx}, after applying the transform in \eqref{eq:transform_def}, the transform of $u_{c,i}$ can be written as:
\begin{equation} \label{eq:alais_apdx}
    U_{c,i}(\bj) = \sum_{\bk\; : \;\bM_c \bk = \bj} (-1)^{\inp{\bp_i}{\bk}} F(\bk).
\end{equation}
To ease notation, we write $\bU_c = [U_{c,1}, \dotsc, U_{c,p}]^T$. Then we can write
\begin{equation}\label{eq:factor_nodes}
    \bU_c(\bj) = \sum_{\bk\; : \;\bM_c \bk = \bj} (-1)^{\bP \bk} F(\bk),
\end{equation}
where we have used the notation $(-1)^{\bP \bk} = [(-1)^{\inp{\bp_0}{\bk}}, \dots, (-1)^{\inp{\bp_p}{\bk}}]^T$. We call the $(-1)^{\bP \bk}$ the \emph{signature} of $\bk$. This signature helps to identify the index of the largest interactions $\bk$, and is central to the next part of the algorithm. Note that we also keep track of $U_{c,0}(\bj)$, which is equal to the unmodulated sum $U_{c,0}(\bj) = \sum_{\bk\; : \;\bM_c \bk = \bj} F(\bk)$.
\subsection{Message Passing for Fourier Transform Recovery}
Using the samples \eqref{eq:factor_nodes}, we aim to recover the largest Fourier coefficients $F(\bk)$. To recover these samples we apply a message passing algorithm, described in detail in Algorithm~\ref{alg:message-pass}. The factor nodes are comprised of the $C2^b$ vectors $\bU_c(\bj) \; \forall \bj \in \bbF_2^b$. Each of these factor nodes are connected to all values $\bk$ that are comprise their sum, i.e., $\{\bk\mid \bM_c \bk = \bj\}$. Since the number of variable nodes is too great, we initialize the value of each variable node, which we call $\hat{F}(\bk)$ to zero implicitly. The values $\hat{F}(\bk)$ for each variable node indexed by $\bk$ represent our estimate of the Fourier coefficients. 




\subsubsection{The message from factor to variable} Consider an arbitrary factor node $\bU_c(\bj)$ initialized according to \eqref{eq:factor_nodes}. We want to understand if there are any large terms $F(\bk)$ involved in the sum in \eqref{eq:factor_nodes}. To do this, we can utilize the signature sequences $(-1)^{\bP \bk}$. If $\bU_c(\bj)$ is strongly correlated with the signature sequence of a given $\bk$, i.e., if $\abs{\inp{(-1)^{\bP \bk}}{\bU_c(\bj)}}$ is large, and $\bM_c \bk = \bj$, from the perspective of $\bU_c(\bj)$, it is likely that $F(\bk)$ is \emph{large}. Searching through all $\bM_c \bk = \bj$, which, for a full rank $\bM_c$ contains $2^{n-b}$ different $\bk$ is intractable, and likely to identify many spurious correlations. Instead, we rely on the structure of the BCH code from which $\bP$ is derived to solve this problem.

\paragraph{BCH Hard Decoding} The BCH decoding procedure is based on an idea known generally in signal processing as ``treating interference as noise". For the purpose of explanation, assume that there is some $\bk^*$ with large $F(\bk^*)$, and all other $\bk$ such that $\bM_c \bk = \bj$ correspond to small $F(\bk)$. For brevity let $\cA_c(\bj) = \{\bk\mid \bM_c \bk = \bj\}$. We can write:
\begin{equation}
    \bU_c(\bj) = F(\bk^*)(-1)^{\bP \bk^*} + \sum_{\cA_c(\bj) \setminus \bk^*} (-1)^{\bP \bk} F(\bk)
\end{equation}
After we normalize with respect to $U_{c,0}(\bj)$ this yields:
\begin{eqnarray}
    \frac{\bU_c(\bj)}{U_{c,0}(\bj)} &=& \left( \frac{1}{1 + \sum_{\cA_c(\bj) \setminus \bk^*}F(\bk)/F(\bk^*)}\right)(-1)^{\bP \bk^*} + \left(\frac{\sum_{\cA_c(\bj) \setminus \bk^*} (-1)^{\bP \bk}F(\bk)}{F(\bk^*) + \sum_{\cA_c(\bj) \setminus \bk^*} F(\bk)}\right) \\
    &=& A(\bj) (-1)^{\bP \bk^*} + \bw(\bj). \label{eq:ratio}
\end{eqnarray}
As we can see, the ratio \eqref{eq:ratio} is a noise-corrupted version of the signature sequence of $\bk^*$. To estimate $\bP \bk$ we apply a nearest-neighbor estimation rule outlined in Algorithm~\ref{alg:bch-hard}. In words, if the $i$th coordinate of the vector \eqref{eq:ratio} is closer to $-1$ we estimate that the corresponding element of $\bP \bk$ to be $1$, conversely, if the $i$th coordinate is closer to $1$ we estimate the corresponding entry to be $0$. This process effectively converts the multiplicative noise $A$ and additive noise $\bw$ to a noise vector in $\bbF_2$. We can write this as $\bP \bk^{*} + \bn$. According to the Lemma~\ref{lem:decoding} if the hamming weight $\bn$ is not too large, we can recover $\bk^*$. 

\begin{lemma}\label{lem:decoding}
    If $\abs{\bn} + \abs{\bk^*} \leq t$, where $\bn$ is the additive noise in $\bbF_2$ induced by the noisy process in \eqref{eq:ratio} and the estimation procedure in Algorithm~\ref{alg:bch-hard}, then we can recover $\bk^*$.
\end{lemma}
\begin{proof}
    Observe that the generator matrix of the BCH code is given by \eqref{eq:sys-form}. Thus, there exists a codeword of the form
    \begin{equation}
    \bc = \bG \bk^*= 
\begin{bmatrix}
\bk^*\\
\bP \bk^*
\end{bmatrix}
\end{equation}
Now construct the ``received codeword" as in Algorithm~\ref{alg:bch-hard}:
    \begin{equation}
    \br = 
\begin{bmatrix}
\boldsymbol{0}\\
\bP \bk^* + \bn
\end{bmatrix}
\end{equation}
Thus $\abs{\bc- \br} = \abs{\bn} + \abs{\bk^*}$. Since the BCH code was designed to be $t$ error correcting, Decoding the code will recover $\bc$, which contains $\bk^*$.
\end{proof}
For decoding we use the implementation in the python package \verb|galois| \cite{Hostetter_Galois_2020}. It implements the standard procedure of the Berlekamp-Massey Algorithm followed by the Chien Search algorithm for BCH decoding. 
\begin{algorithm}
   \caption{BCH Hard Decode}
   \label{alg:bch-hard}
\begin{algorithmic}[1]
   \State {\bfseries Input:} Observation $\bU_c(\bj)$, Decoding function $\mathrm{Dec}(\cdot)$
   \State $r_i \gets 0 \; i=1\dotsc, n$
   \ForAll{$i \in n+1, \dotsc, n+p$}
        \State $r_i \gets \mathds{1}\left\{ \frac{U_{c,i}(\bj)}{U_{c,0}(\bj) } < 0 \right\}$
   \EndFor
    \State dec, $ \hat{\bk} \gets \mathrm{Dec}(\br)$
    \State {\bfseries Output:} dec, $\hat{\bk}$ 
\end{algorithmic}
\end{algorithm}

\paragraph{BCH Soft Decoding} In practice the conversion of the real-valued noisy observations \eqref{eq:ratio} to noisy elements in $\bbF_2$ is a process that destroys valuable information. In coding theory, this is known as \emph{hard input} decoding, which is typically suboptimal. For example, certain coordinates will have values $\frac{U_{c,i}(\bj)}{U_{c,0}(\bj)} \approx 0$. For such coordinates, we have low confidence about the corresponding value of $(-1)^{\inp{\bp_i}{\bk^*}}$, since it is equally close to $+1$ and $-1$. This uncertainty information is lost in the process of producing a hard input. With this so-called \emph{soft information} it is possible to recover $\bk^*$ even in cases where there are more than $t$ errors in the hard decoding case. We use a simple soft decoding algorithm for BCH decoding known as a chase decoder. The main idea behind a chase decoder   is to perform hard decoding on the $d_{\text{chase}}$ most likely hard inputs, and return the decoder output of the most likely hard input that successfully decoded. In practical setting like the ones we consider in this work, we don't have an understanding of the noise in \eqref{eq:ratio}. A practical heuristic is to simply look at the \emph{margin} of estimation. In other words, if $\abs{\frac{U_{c,i}(\bj)}{U_{c,0}(\bj)}}$ is large, we assume it has high confidence, while if it is small, we assume the confidence is low. Interestingly, if we assume $A(\bj) = 1$ and $\bw(\bj) \sim \cN(0, \sigma^2)$ in \eqref{eq:ratio}, then the ratio corresponds exactly to the logarithm of the likelihood ratio (LLR) $\log \left( \frac{\mathrm{Pr}\left(\inp{\bp_i}{\bk^*} = 0\right)}{\mathrm{Pr}\left(\inp{\bp_i}{\bk^*} = 1\right)}\right)$. For the purposes of soft decoding we interpret these ratios as LLRs. Pseudocode can be found in Algorithm~\ref{alg:bch-soft}.


\emph{Remark: BCH soft decoding is a well-studied topic with a vast literature. Though we put significant effort into building a strong implementation of \SpecExp{}, we have used the simple Chase Decoder (described in Algorithm~\ref{alg:bch-soft} below) as a soft decoder. The computational complexity of Chase Decoding scales as $2^{d_\text{chase}}$, but other methods exist with much lower computational complexity and comparable performance.}

\begin{algorithm}
   \caption{BCH Soft Decode (Chase Decoding)}
   \label{alg:bch-soft}
\begin{algorithmic}[1]
   \State {\bfseries Input:} Observation $\bU_c(\bj)$, Decoding function $\mathrm{Dec}(\cdot)$, Chase depth $d_{\text{chase}}$.
   \State $r_i \gets 0 \; i=1\dotsc, n$
   \State $\cR \gets d_{\text{chase}}$ most likely hard inputs \Comment{Can be computed efficiently via dynamic programming}
   \State dec $\gets False$
   \State $j \gets 0$
   \While{$dec$ is $False$ and $j \leq d_{\text{chase}}$}
        \State $\br_{(n+1):(n+p)} \gets \cR [j]$
        \State $j \gets j+1$
        \State dec, $ \hat{\bk} \gets \mathrm{Dec}(\br)$
   \EndWhile
    \State {\bfseries Output:} dec, $\hat{\bk}$ 
\end{algorithmic}
\end{algorithm}

If we successfully decode some $\bk$ from the BCH decoding process via the bin $\bU_{c}(\bj)$, we construct a message to the corresponding variable node. Before we do this, we verify that the $\bk$ term satisfies $\bM_c \bk = \bj$. This acts as a final check to increase our confidence in the output of $\bk$. The message we construct is of the following form:
\begin{equation}\label{eq:check_msg}
    \mu_{(c,\bj) \rightarrow \bk} = \inp{(-1)^{\bP \bk}}
        {\bU_c(\bj)}/p
\end{equation}
To understand the structure of this message. This message can be seen as an estimate of the Fourier coefficient. Let's assume we are computing this message for some $\bk^*$:   
\begin{equation}
\mu_{(c,\bj) \rightarrow \bk^*} = F(\bk^*) + \sum_{\cA(\bj)\setminus \bk^*}\underbrace{\frac{1}{p}\inp{(-1)^{\bP \bk}}{(-1)^{\bP \bk^*}}}_{\text{typically small}}F(\bk)
\end{equation}
The inner product serves to reduce the noise from the other coefficients in the sum.
\begin{algorithm}
   \caption{Message Passing}
   \label{alg:message-pass}
\begin{algorithmic}[1]
\State {\bfseries Input:} Processed Samples  $\bU_c, c=1, \dotsc, C$
\State $\cS = \left\{ (c,\bj): \bj \in \bbF_2^b, c \in \{1, \dotsc, C\}\right\}$ \Comment{Nodes to process}
\State $\hat{F}[\bk] \gets 0 \;\forall\bk$
\State $\cK \gets \emptyset$
\While{$\abs{\cS} > 0$} \Comment{Outer Message Passing Loop}
    \State $\cS_{\text{sub}} \gets \emptyset$
    \State $\cK_{\text{sub}} \gets \emptyset$
    \For{$(c,\bj) \in \cS$}
        \State dec, $\bk$ $\gets \mathrm{DecBCH} (\bU_c(\bj))$ \Comment{Process Factor Node}
        \If{dec}
            \State corr $\gets \frac{\inp{(-1)^{\bP \bk}}{\bU_c(\bj)}}{\norm{\bU_c(\bj)}^2}$
        \Else
            \State corr $\gets 0$
        \EndIf
        \If{corr $> \gamma$} \Comment{Interaction identified}
            \State $\cS_{\text{sub}} \gets \cS_{\text{sub}} \cup \{(\bk, c, \bj)\}$
            \State $\cK_{\text{sub}} \gets \cK_{\text{sub}} \cup \{\bk\}$
        \Else
            \State $\cS \gets \cS \setminus \{ (c, \bj)\}$ \Comment{Cannot extract interaction}
        \EndIf
    \EndFor
    \For{ $\bk \in \cK_{\text{sub}}$}
        \State $\cS_{\bk} \gets \{ (\bk', c', \bj') \mid (\bk', c', \bj') \in \cS_{\text{sub}}, \bk' = \bk \}$
        \State $\mu_{(c,\bj) \rightarrow \bk} \gets \inp{(-1)^{\bP \bk}}
        {\bU_c(\bj)}/p$ 
        \State $\mu_{\bk \rightarrow \text{all}} \gets \sum_{(\bk, c, \bj) \in \cS_{\bk}} \mu_{(c,\bj) \rightarrow \bk}$
        \State $\hat{F}(\bk) \gets \hat{F}(\bk) + \mu_{\bk \rightarrow \text{all}}$ \Comment{Update variable node}
        \For{$c \in \{ 1, \dotsc, C\}$}
            \State $\bU_c(\bM_c \bk) \gets \bU_c(\bM_c \bk) - \mu_{\bk \rightarrow \text{all}}\cdot(-1)^{\bP \bk}$ \Comment{Update factor node}
            \State $\cS \gets \cS \cup \{ (c, \bM_c \bk)\}$
        \EndFor
    \EndFor
    \State $\cK \gets \cK \cup \cK_{\text{sub}}$
\EndWhile
  \State {\bfseries Output: $\left\{ \left(\bk, \hat{F}(\bk)\right) \mid \bk \in \cK\right\}$}, interactions, and scalar values corresponding to interactions.
\end{algorithmic}
\end{algorithm}
\subsubsection{The message from variable to factor}
The message from factor to variable is comparatively simple. The variable node takes the average of all the messages it receives, adding the result to its state, and then sends that average back to all connected factor nodes. These factor nodes then subtract this value from their state and then the process repeats. 


\subsection{Computational Complexity}

\paragraph{Generating masking patterns $\bbm$} Constructing each masking pattern requires $n2^b$ for each $\bM_c$. The algorithm for computing it efficiently involves a gray iteratively adding to an $n$ bit vector and keeping track of the output in a Gray code. Doing this for all $C$, and then adding all $p$ additional shifting vectors makes the cost $O(Cpn2^b)$.

\paragraph{Taking FFT} For each $u_{c,i}$ we take the Fast Fourier transform in $b2^b$ time, with a total of $O(Cpb2^b)$. This is dominated by the previous complexity since, $b \leq n$

\paragraph{Message passing} One round of BCH hard decoding is $O(n_ct + t^2)$. For soft decoding, this cost is multiplied by $2^{d_{\text{chase}}}$, which we is a constant.  Computing the correlation vector is $O(np)$, dominated by the computation of $\bP \bk$. In the worst case, we must do this for all $C 2^b$ vectors $\bU_c(\bj)$. We also check that $\bM \bk = \bj$ before sending the message, which costs $O(nb)$. Thus, processing all the factor nodes costs $O(C2^b(n_c t + t^2 + n(p+b)))$. The number of active (with messages to send) variable nodes is at most $C2^b$, and computing their factors is at most $C$. Thus, computing factor messages is at most $C^22^b$ messages. Finally, factor nodes are updated with at most $C2^b$ variable messages sending messages to at most $C$ factor nodes each, each with a cost of $O(np)$. Thus, the total cost of processing all variable nodes is $O(C^22^b + C^22^bnp)$. The total cost of message is dominated by processing the factors. 

The total complexity is then $O(2^b(n_c t + t^2 + n(p + b))$.
Note that $p = n_c - n = t \log(n_c)$. Due to the structure of the code and the relationship between $n,p$ and $n_c$, one could stop here, and it would be best to if we want to consider very large $t$. For the purposes of exposition, we will assume that $t \ll n$, which implies $n > p$, and thus $p \approx t \log(n)$. In this case, we can write:
\begin{equation}
    \text{Complexity} = O(2^b(nt\log(n)  + nb))
\end{equation}

To arrive at the stated equation in Section~\ref{sec:intro}, we take $2^b = O(s)$. Under the low degree assumption, we have $s = O(d\log(n))$. Then assuming we take $t= O(d)$, we arrive at a complexity of $O(sdn\log(n))$.