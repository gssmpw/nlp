\subsection{Implementation Details} Experiments are run on a server using Nvidia L40S GPUs and A100 GPUs. When splitting text into words or sentences, we make use of the default word and sentence tokenizer from \texttt{nltk} \cite{bird2009natural}. To fit regressions, we use the \texttt{scikit-learn} \cite{scikit-learn} implementations of \texttt{LinearRegression} and \texttt{RidgeCV}.

\subsection{Datasets and Models}

\subsubsection{Sentiment Analysis}
152 movie reviews were used from the \emph{Large Movie Review Dataset} \cite{maas-EtAl:2011:ACL-HLT2011}, supplemented with 8 movie reviews from the \emph{Stanford Sentiment Treebank} dataset \cite{socher2013recursive}. These 160 reviews were categorized using their word counts into 8 groups ([8-15, 16-32, \dots, 1024-2047]), with 20 reviews in each group.

To measure the sentiment of each movie review, we utilize a \texttt{DistilBERT} model \cite{Sanh2019DistilBERTAD} fine-tuned for sentiment analysis \cite{sentimentBert}. When masking, we replace the word with the \texttt{[UNK]} token. We construct an value function over the output logit associated with the positive class.


\subsubsection{HotpotQA}
We consider $100$ examples from the \emph{HotpotQA}\cite{yang2018hotpotqa} dataset. These examples were categorized using the number of sentences into four groups ([8-15, 16-32, 32-64, 64-127]). We use a \texttt{Llama-3.2-3B-Instruct} model with $8$-bit quantization. When masking, we replace with the \texttt{[UNK]} token, and measure the log-perplexity of generating the original output. Since \emph{HotpotQA} is a multi-document dataset, we use the following prompt format. 

\begin{tcolorbox}[colframe=black, colback=white, sharp corners]
\textbf{Title:} \{title\_1\}

\textbf{Content:} \{document\_1\}\\
\dots\\
\textbf{Title:} \{title\_m\}

\textbf{Content:} \{document\_m\}\\

\textbf{Query:} \{question\}. Keep your answers as short as possible. 
\end{tcolorbox}

\subsubsection{DROP}
We consider $100$ examples from the \emph{DROP }\cite{yang2018hotpotqa} dataset. These examples were categorized using the number of words into six groups ([8-15, 16-32, 32-64, 64-127, 128-256, 512-1024]). We use the same model as \emph{HotpotQA} and mask in a similar fashion. We use the following prompt format.

\begin{tcolorbox}[colframe=black, colback=white, sharp corners]
\textbf{Context:} \{context\}

\textbf{Query:} \{question\}. Keep your answers as short as possible. 
\end{tcolorbox} 

\subsubsection{Trolley Problem}
The simplified trolley problem was inspired by the one provided in \cite{williams2024easy}. When masking, the \texttt{[UNK]} token was used to replace words. The following prompt was given to \texttt{gpt-4o-mini-2024-07-18}:

\begin{tcolorbox}[colframe=black, colback=white, sharp corners]
\textbf{System:} Answer with the one word True or False only. Any other answer will be marked incorrect.

\textbf{User:} \{Masked Input\} True or False: You should not pull the lever.
\end{tcolorbox}

A value function was created by finding the difference between the model's logprob associated with the ``True" token minus the logprob of the ``False" token. 
\subsubsection{Visual Question Answering}
The base image was partitioned into a $6\times 8$ grid. To mask, Gaussian blur was applied to the masked cells. The masked image was input into \texttt{LLaVA-NeXT-Mistral-7B}, a large multimodal model, with the following prompt:

\begin{tcolorbox}[colframe=black, colback=white, sharp corners]
\textbf{Context:} \{masked image\}

\textbf{Query:} What is shown in this image?
\end{tcolorbox}
The original output to the unmasked image is 
``A dog playing with a basketball.'' Using the masked images, we build a value function that measures the probability of generating the original output sequence (log probability).  


\subsection{Baselines}\label{apdx:baselines}
The following marginal feature attribution baselines were run:
\begin{enumerate}
    \item \emph{LIME}: LIME (Local Interpretable Model-agnostic Explanations) \cite{ribeiro2016should} uses LASSO to build a sparse linear approximation of the value function. The approximation is weighted to be \emph{local}, using an exponential kernel to fit the function better closer to the original input (less maskings). 
    \item \emph{SHAP}: Implemented using KernelSHAP \cite{Lundberg2017}, SHAP interprets the value function as a cooperative game and attributes credit to each of the features according to the Shapley value. KernelSHAP approximates the Shapley values of this game through solving a weighted least squares problem, where the weighting function is informed by the Shapley kernel, promoting samples where very either very few or most inputs are masked. 
    \item \emph{Banzhaf}: Similar to Shapley values, Banzhaf values \cite{banzhaf1964weighted} represent another credit attribution concept from cooperative game theory. We compute the Banzhaf values by fitting a ridge regression to uniformly drawn samples, selecting the regularization parameter through cross-validation.
\end{enumerate}
Furthermore, we compared against the following interaction attribution methods:
\begin{enumerate}[resume]
    \item \emph{Faith-Banzhaf}: The Faith-Banzhaf Interaction Index \cite{tsai2023faith}, up to degree $t$, provides the most faithful $t$\textsuperscript{th} order polynomial approximation of the value function under a uniform kernel. We obtain this approximation using cross-validated ridge regression on uniformly drawn samples.
    \item \emph{Faith-Shap}: Similarly, the Faith-Shapley Interaction Index \cite{tsai2023faith}, up to degree $t$,  provides the most faithful $t$ order polynomial approximation of the value function under a Shapley kernel. As described in \cite{tsai2023faith}, the indices can be estimated through solving a weighted least squares problem. We use the implementation provided in SHAP-IQ \cite{muschalik2024shapiq}.
    
    \item \emph{Shapley-Taylor}: The Shapley-Taylor Interaction Index \cite{dhamdhere2019shapley}, up to degree $t$, provides another interaction definition based on the Taylor Series of the M\"obius transform of the value function. To estimate the interaction indices, we leverage the sample-efficient estimator SVARM-IQ \cite{kolpaczki2024svarm}, as implemented in SHAP-IQ \cite{muschalik2024shapiq}.
\end{enumerate}

\subsection{Sample Complexity}
The total number of samples needed for \SpecExp{} is  $\approx C2^bt \log(n)$. We fix $C=3$ and $t=5$. The table below presents the number of samples used in our experiments for various choices of sparsity parameter $b$ and different input sizes $n$:

\begin{table}[ht]
\centering
\begin{tabular}{c|ccccccc}
\toprule
 & \multicolumn{7}{c}{\textbf{Number of Inputs ($n$)}} \\
\textbf{Sparsity Parameter ($b$)} & 8--11 & 12--36 & 37--92 & 93--215 & 216--466 & 467--973 & 974--1992 \\
\midrule
4 & 1,008 & 1,344 & 1,728 & 1,968 & 2,208 & 2,448 & 2,688 \\
6 & 4,032 & 5,376 & 6,912 & 7,872 & 8,832 & 9,792 & 10,752 \\
8 & 16,128 & 21,504 & 27,648 & 31,488 & 35,328 & 39,168 & 43,008 \\
\bottomrule
\end{tabular}
\caption{Number of samples needed for each $b$ and $n$.}
\label{tab:numSamples}
\end{table}

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccccccccccc}\toprule
                                                           \multicolumn{1}{c|}{}       & \multicolumn{1}{c|}{\multirow{2}{*}{$\mathbf{n}$}}           & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Avg. Sparsity}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Avg. Sparsity Ratio}}}&\textbf{}                & \textbf{}     & \textbf{}        & \multicolumn{3}{c}{\textbf{Faith-Banzhaf}} & \textbf{}        & \multicolumn{3}{c}{\textbf{Faith-Shap}}    & \multicolumn{3}{c}{\textbf{Shapley-Taylor}} \\
                                                           \multicolumn{1}{c|}{}       &        \multicolumn{1}{l|}{}                         & \multicolumn{1}{l|}{} &               \multicolumn{1}{l|}{}               & \textbf{SPEX} & \textbf{LIME} & \textbf{Banzhaf} & \textbf{2nd} & \textbf{3rd} & \textbf{4th} & \textbf{SHAP} & \textbf{2nd} & \textbf{3rd} & \textbf{4th} & \textbf{2nd}  & \textbf{3rd} & \textbf{4th} \\\midrule
\multicolumn{1}{c|}{\multirow{8}{*}{\textit{\textbf{Sentiment}}}} & \multicolumn{1}{c|}{8-15}      & \multicolumn{1}{c|}{369.9}               &      \multicolumn{1}{c|}{$3.70\times 10^{-1}$}         & 1.00                     & 0.83          & 0.84             & 0.96         & 0.99         & 1.00         & 0.62             & 0.93         & 0.98         & 1.00         & 0.72          & 0.81         & 0.92         \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{16-31}     & \multicolumn{1}{c|}{208.7}           &       \multicolumn{1}{c|}{$2.31\times 10^{-4}$}            & 0.97                     & 0.75          & 0.75             & 0.93         & 0.98         &              & 0.20             & 0.89         & 0.95         &              & 0.46          & -710.85      &              \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{32-63}     & \multicolumn{1}{c|}{149.7}            &    \multicolumn{1}{c|}{$3.14\times 10^{-9}$}              & 0.93                     & 0.67          & 0.68             & 0.90         &              &              & -0.25            & 0.82         &              &              & -0.30         &              &              \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{64-127}    & \multicolumn{1}{c|}{118.4}         &        \multicolumn{1}{c|}{$2.19\times 10^{-19}$}             & 0.87                     & 0.66          & 0.66             &              &              &              & -1.17            &              &              &              &               &              &              \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{128-255}   & \multicolumn{1}{c|}{113.5}         &         \multicolumn{1}{c|}{$1.40\times 10^{-38}$}            & 0.82                     & 0.63          & 0.63             &              &              &              & -3.94            &              &              &              &               &              &              \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{256-511}   & \multicolumn{1}{c|}{100.4}         &         \multicolumn{1}{c|}{$5.48\times 10^{-78}$}            & 0.76                     & 0.62          & 0.62             &              &              &              & -6.77            &              &              &              &               &              &              \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{512-1013}  & \multicolumn{1}{c|}{86.1}         &         \multicolumn{1}{c|}{$2.02\times 10^{-155}$}            & 0.73                     & 0.61          & 0.61             &              &              &              & -4.78            &              &              &              &               &              &              \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{1024-2047} & \multicolumn{1}{c|}{81.7}       &           \multicolumn{1}{c|}{$3.78\times 10^{-302}$}            & 0.71                     & 0.59          & 0.59             &              &              &              & -17.31           &              &              &              &               &              &              \\\midrule
\multicolumn{1}{c|}{\multirow{5}{*}{\textit{\textbf{DROP}}}}      & \multicolumn{1}{c|}{32-63}     & \multicolumn{1}{c|}{77.1}         &        \multicolumn{1}{c|}{$1.97\times 10^{-14}$}             & 0.58                     & 0.29          & 0.29             & 0.53         &              &              & -0.07            & 0.17         &              &              & N/A           &              &              \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{64-127}    & \multicolumn{1}{c|}{56.3}         &          \multicolumn{1}{c|}{$2.59\times 10^{-24}$}           & 0.51                     & 0.30          & 0.30             &              &              &              & 0.02             &              &              &              &               &              &              \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{128-255}   & \multicolumn{1}{c|}{58.7}         &        \multicolumn{1}{c|}{$2.31\times 10^{-38}$}             & 0.67                     & 0.43          & 0.43             &              &              &              & 0.14             &              &              &              &               &              &              \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{256-511}   & \multicolumn{1}{c|}{56.7}        &        \multicolumn{1}{c|}{$1.29\times 10^{-76}$}              & 0.48                     & 0.43          & 0.44             &              &              &              & -5.29            &              &              &              &               &              &              \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{512-1023}  & \multicolumn{1}{c|}{36.3}          &       \multicolumn{1}{c|}{$9.63\times 10^{-155}$}             & 0.55                     & 0.46          & 0.48             &              &              &              & -0.23            &              &              &              &               &              &              \\\midrule
\multicolumn{1}{c|}{\multirow{4}{*}{\textit{\textbf{HotpotQA}}}}  & \multicolumn{1}{c|}{8-15}      & \multicolumn{1}{c|}{108.3}       &         \multicolumn{1}{c|}{$1.10\times 10^{-2}$}              & 0.87                     & 0.38          & 0.38             & 0.63         & 0.77         & 0.84         & -1.09            & -19.14       & -2.33        & 0.73         & N/A           & N/A          & N/A          \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{16-31}     & \multicolumn{1}{c|}{96.4}      &          \multicolumn{1}{c|}{$3.30\times 10^{-4}$}              & 0.67                     & 0.39          & 0.49             & 0.66         & 0.72         &              & 0.23             & -2.28        & 0.40         &              & N/A           & N/A          &              \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{32-63}     & \multicolumn{1}{c|}{79.4}        &        \multicolumn{1}{c|}{$5.86\times 10^{-9}$}              & 0.57                     & 0.44          & 0.45             & 0.59         &              &              & 0.13             & 0.32         &              &              & N/A           &              &              \\
\multicolumn{1}{c|}{}                                             & \multicolumn{1}{c|}{64-127}    & \multicolumn{1}{c|}{73.0}        &         \multicolumn{1}{c|}{$1.02\times 10^{-18}$}             & 0.63                     & 0.53          & 0.53             &              &              &              & -0.31            &              &              &              &               &              &       \\\bottomrule      
\end{tabular}
}
\caption{Faithfulness across all baseline methods for fixed $b=8$. The average recovered sparsity and the average ratio between sparsity and total possible interactions is also reported.}
\label{tab:faith}
\end{table*}





\subsection{Additional Results}
\subsubsection{Faithfulness}\label{apdx:faithfulness}
\paragraph{Faithfulness for a fixed sparsity parameter $b=8$:}
We first measure the faithfulness by scaling the number of samples logarithmically with $n$. The exact number of samples used can be found in Table~\ref{tab:numSamples}.

In Table~\ref{tab:faith}, we showcase the average faithfulness of every runnable method across every group of examples for the \emph{Sentiment}, \emph{DROP}, and \emph{HotpotQA} datasets. Among marginal attribution methods, LIME and Banzhaf achieve the best faithfulness. SHAP's faithfulness worsens as $n$ grows, though this is unsurprising, as Shapley values are only efficient (intended to sum to the unmasked output), not faithful. 

Comparing to interaction methods, \SpecExp{} is comparable to the highest order Faith-Banzhaf that can feasible be run at every size of $n$. However, due to poor computation complexity scaling of this, and other interaction methods, these methods are only able to be used for small $n$. In particular, we found Shapley-Taylor difficult to run for the \emph{DROP} and \emph{HotpotQA} tasks, unable to finish within thirty minutes.

We also report the average sparsity and the average sparsity ratio (sparsity over total interactions) discovered by SPEX for each of the groups. For \emph{Sentiment}, once reaching the $n\in[128-255]$ group, the average sparsity is found to be less than the number of inputs! Yet, SPEX is still able to achieve high faithfulness and significantly outperform linear methods like LIME and Banzhaf. 


\begin{figure}[H]
    \centering
    \begin{subfigure}{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/sentiment_faithfulness_dynamic.pdf}
        \vspace{-.6cm}
        \caption{\emph{Sentiment} $n\in[16,31]$}
    \end{subfigure}
    
    \begin{subfigure}{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/drop_faithfulness_dynamic.pdf}
        \vspace{-.6cm}
        \caption{\emph{DROP} $n\in[32,63]$}
    \end{subfigure}
    
    \begin{subfigure}{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/hotpot_faithfulness_dynamic.pdf}
        \vspace{-.6cm}
        \caption{\emph{HotpotQA $n\in[16,31]$}}
    \end{subfigure}
    
    \caption{Faithfulness across the three datasets for all methods that can be feasibly ran. Methods appearing in the legend, but not in the plot had a faithfulness below 0 for the given number of samples.}
    \label{fig:faithfulnessDyn}
\end{figure}
\paragraph{Faithfulness for varying the sparsity parameter $b$:}

$b=8$ may not be the sparsity parameter that achieves the best trade-off between samples and faithfulness. For instance, with complex generative models, the cost or time per instance may necessitate taking fewer samples. 

In Fig.~\ref{fig:faithfulnessDyn}, we showcase the faithfulness results for \SpecExp{} and all baseline methods when $b$ is $4,6,8$. Since the samples taken by the algorithm grows with $2^b$, $b=8$ takes 16 times more samples than $b=4$. Even in the low-sample regime, \SpecExp{} achieves high faithfulness, often surpassing linear models and second order models. At this scale, we find that third and fourth order models often do not have enough samples to provide a good fit. 

\subsubsection{Abstract Reasoning}\label{apdx:abstract}
We also evaluated the performance of \texttt{Llama 3.2 3B-Instruct} \cite{grattafiori2024llama3herdmodels} on the modified trolley problem. As a reminder, the modified problem is presented below:

\begin{quote}
\leftskip=.001in \rightskip=.001in
A runaway trolley is heading \textbf{away} from five people who are tied to the track and cannot move. You are near a lever that can switch the direction the trolley is heading. Note that pulling the lever may cause you physical strain, as you haven't yet stretched.

\textbf{\textcolor[HTML]{455935}{ True} or \textcolor[HTML]{6d3336}{False}: You should not pull the lever.}
\end{quote}

Across 1,000 evaluations, the model achieves an accuracy of just 11.8\%. Despite a similar accuracy to \texttt{GPT-4o mini}, the SHAP and SPEX-computed interactions indicate that the two models are lead astray by different parts of the problem. 

\begin{figure}[h]
    \centering
    \begin{minipage}{0.8\textwidth}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/trolley_shap_llama2.pdf}
        \caption{SHAP values}
        \label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/trolley_se_llama2.pdf}
        \caption{Interactions computed via \SpecExp{}}
        \label{fig:sub2}
    \end{subfigure}
    \caption{SHAP and SPEX-computed interactions computed for  \texttt{Llama 3.2 3B-Instruct}'s answering of the modified trolley problem. Words and interactions highlighted in green contribute positively to producing the correct output, while those in red lead the model toward an incorrect response.}
    \label{fig:main}
    \end{minipage}
\end{figure}


The most negative SHAP values of \texttt{Llama 3.2 3B-Instruct} appear for later terms such as \emph{pulling} and \emph{lever}, with surrounding words having positive SHAP values. The SPEX-computed interactions tell a different story; many of the words in the last sentence have a negative first order value, with a significant third order interaction between \emph{you haven't yet}. Furthermore, the first word \emph{A} possesses a strong negative second order interaction with \emph{trolley}. Although counterintuitive---since the fact about stretching should only enhance the likelihood of a correct answer---removing the non-critical final sentence unexpectedly boosts the model's accuracy to 20.8\%, a 9\% improvement.