%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{duckuments} % for more interesting ducks
\usepackage{tcolorbox}
\usepackage{todonotes}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}
\usepackage{array}
%\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{makecell} % For multiline cells
\renewcommand\cellalign{cc} % Align cells both vertically and horizontally
%\usepackage[dvipsnames]{xcolor} % For extended color support


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{color,soul}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{dsfont}
\usepackage{enumitem}
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}
\usepackage{subcaption}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\median}{median}
\DeclareMathOperator*{\mean}{mean}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\esssup}{ess\,sup}
\DeclareMathOperator{\fastmobius}{FastMobius}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\dec}{Dec}
\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator{\linspan}{span}
\DeclareMathOperator{\proj}{Proj}
\DeclareMathOperator{\sv}{SV}
\DeclareMathOperator{\stii}{STII}
\DeclareMathOperator{\bz}{BZ}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\obj}{obj}
\DeclareMathOperator{\bern}{Bern}
\DeclareMathOperator{\binomdist}{Binom}
\DeclareMathOperator\erf{erf}

\newcommand{\BY}[1]{\todo[author=BY, color=red, size=\small]{\color{white}#1}}
\newcommand{\Abhi}[1]{\todo[author=Abhi, color=orange, size=\small]{\color{white}#1}}
\newcommand{\JK}[1]{\todo[author=JK, color=blue, size=\small]{\color{white}#1}}
\newcommand{\KR}[1]{\todo[author=KR, color=green, size=\small]{\color{white}#1}}
\newcommand{\Efe}[1]{\todo[author=Efe, color=purple, size=\small]{\color{white}#1}}

\newcommand{\type}[1]{\mathrm{Type} \left( #1 \right)}
\newcommand{\detect}[1]{\mathrm{Detect} \left( #1 \right)}

\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\definecolor{lightpink}{rgb}{1,0.9,0.9}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}
\renewenvironment{quote}{%
  \list{}{%
    \leftmargin0.5cm   % this is the adjusting screw
    \rightmargin\leftmargin
  }
  \item\relax
}
{\endlist}
\newcommand{\dt}{\text{ d}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\cA}{\mathcal A}
\newcommand{\cB}{\mathcal B}
\newcommand{\cC}{\mathcal C}
\newcommand{\cD}{\mathcal D}
\newcommand{\cE}{\mathcal E}
\newcommand{\cF}{\mathcal F}
\newcommand{\cG}{\mathcal G}
\newcommand{\cH}{\mathcal H}
\newcommand{\cI}{\mathcal I}
\newcommand{\cJ}{\mathcal J}
\newcommand{\cK}{\mathcal K}
\newcommand{\cL}{\mathcal L}
\newcommand{\cM}{\mathcal M}
\newcommand{\cN}{\mathcal N}
\newcommand{\cO}{\mathcal O}
\newcommand{\cP}{\mathcal P}
\newcommand{\cQ}{\mathcal Q}
\newcommand{\cR}{\mathcal R}
\newcommand{\cS}{\mathcal S}
\newcommand{\cT}{\mathcal T}
\newcommand{\cU}{\mathcal U}
\newcommand{\cV}{\mathcal V}
\newcommand{\cW}{\mathcal W}
\newcommand{\cX}{\mathcal X}
\newcommand{\cY}{\mathcal Y}
\newcommand{\cZ}{\mathcal Z}
\newcommand{\scrA}{\mathscr A}
\newcommand{\scrB}{\mathscr B}
\newcommand{\scrC}{\mathscr C}
\newcommand{\scrD}{\mathscr D}
\newcommand{\scrE}{\mathscr E}
\newcommand{\scrF}{\mathscr F}
\newcommand{\scrG}{\mathscr G}
\newcommand{\scrH}{\mathscr H}
\newcommand{\scrI}{\mathscr I}
\newcommand{\scrJ}{\mathscr J}
\newcommand{\scrK}{\mathscr K}
\newcommand{\scrL}{\mathscr L}
\newcommand{\scrM}{\mathscr M}
\newcommand{\scrN}{\mathscr N}
\newcommand{\scrO}{\mathscr O}
\newcommand{\scrP}{\mathscr P}
\newcommand{\scrQ}{\mathscr Q}
\newcommand{\scrR}{\mathscr R}
\newcommand{\scrS}{\mathscr S}
\newcommand{\scrT}{\mathscr T}
\newcommand{\scrU}{\mathscr U}
\newcommand{\scrV}{\mathscr V}
\newcommand{\scrW}{\mathscr W}
\newcommand{\scrX}{\mathscr X}
\newcommand{\scrY}{\mathscr Y}
\newcommand{\scrZ}{\mathscr Z}
\newcommand{\bbB}{\mathbb B}
\newcommand{\bbS}{\mathbb S}
\newcommand{\bbR}{\mathbb R}
\newcommand{\bbZ}{\mathbb Z}
\newcommand{\bbI}{\mathbb I}
\newcommand{\bbQ}{\mathbb Q}
\newcommand{\bbP}{\mathbb P}
\newcommand{\bbE}{\mathbb E}
\newcommand{\bbF}{\mathbb F}
\newcommand{\bbN}{\mathbb N}
\newcommand{\sfE}{\mathsf E}
\newcommand{\sfF}{\mathsf F}
\newcommand{\sfV}{\mathsf V}
\newcommand{\one}{\mathds 1}

\newcommand{\Tr}{ \text{Tr} }
\newcommand{\trans}{\top}

\newcommand{\by}{\mathbf y}
\newcommand{\bxi}{\boldsymbol \xi}
\newcommand{\bx}{\mathbf x}
\newcommand{\bd}{\mathbf d}
\newcommand{\barbd}{\mathbf{ \overline{ d}}}
\newcommand{\barbh}{\mathbf{ \overline{ h}}}
\newcommand{\bc}{\mathbf c}
\newcommand{\be}{\mathbf e}
\newcommand{\bg}{\mathbf g}
\newcommand{\bw}{\mathbf w}
\newcommand{\bW}{\mathbf W}
\newcommand{\bP}{\mathbf P}
\newcommand{\bv}{\mathbf v}
\newcommand{\bj}{\mathbf j}
\newcommand{\bu}{\mathbf u}
\newcommand{\ba}{\mathbf a}
\newcommand{\bp}{\mathbf p}
\newcommand{\bk}{\mathbf k}
\newcommand{\bbm}{\mathbf m}
\newcommand{\br}{\mathbf r}
\newcommand{\bM}{\mathbf M}
\newcommand{\bX}{\mathbf X}
\newcommand{\bY}{\mathbf Y}
\newcommand{\bD}{\mathbf D}
\newcommand{\bG}{\mathbf G}
\newcommand{\bI}{\mathbf I}
\newcommand{\bH}{\mathbf H}
\newcommand{\bh}{\mathbf h}
\newcommand{\bn}{\mathbf n}
\newcommand{\bs}{\mathbf s}
\newcommand{\bS}{\mathbf S}
\newcommand{\bZ}{\mathbf Z}
\newcommand{\bU}{\mathbf U}
\newcommand{\bbeta}{\bm \beta}
\newcommand{\balpha}{\bm \alpha}
\newcommand{\bell}{\boldsymbol{\ell}}
\newcommand{\bZero}{\boldsymbol 0}
\newcommand{\bOne}{\boldsymbol 1}
\newcommand{\beps}{\boldsymbol{\epsilon}}
\newcommand{\indep}{\perp \!\!\! \perp}
\DeclarePairedDelimiterX{\inp}[2]{\langle}{\rangle}{#1, #2}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\SpecExp}{\textsc{SPEX}}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{SpectralExplain}

\begin{document}

\twocolumn[
\icmltitle{\SpecExp: Scaling Feature Interaction Explanations for LLMs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Justin Singh Kang}{equal,yyy}
\icmlauthor{Landon Butler}{equal,yyy}
\icmlauthor{Abhineet Agarwal}{equal,xxx}
\icmlauthor{Yigit Efe Erginbas}{yyy}
\icmlauthor{Ramtin Pedarsani}{zzz}
\icmlauthor{Kannan Ramchandran}{yyy}
\icmlauthor{Bin Yu}{yyy,xxx}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Electrical Engineering and Computer Science, UC Berkeley}
\icmlaffiliation{xxx}{Department of Statistics, UC Berkeley}
\icmlaffiliation{zzz}{Department of Electrical and Computer Engineering, UC Santa Barbara}

% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Justin Singh Kang}{justin\_kang@berkeley.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may prov$ide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution} 
% otherwise use the standard text.

\begin{abstract}

 
Large language models (LLMs) have revolutionized machine learning due to their ability to capture complex interactions between input features. Popular post-hoc explanation methods like SHAP provide \textit{marginal} feature attributions, while their extensions to interaction importances only scale to small input lengths ($\approx 20$). We propose \emph{Spectral Explainer} (\SpecExp{}), a model-agnostic interaction attribution algorithm that efficiently scales to large input lengths ($\approx 1000)$. \SpecExp{} exploits underlying natural sparsity among interactions---common in real-world data---and applies a sparse Fourier transform using a channel decoding algorithm to efficiently identify important interactions.
We perform experiments across three difficult long-context datasets that require LLMs to utilize interactions between inputs to complete the task. For large inputs, \SpecExp{} outperforms marginal attribution methods by up to 20\% in terms of faithfully reconstructing LLM outputs. Further, \SpecExp{} successfully identifies key features and interactions that strongly influence model output. For one of our datasets, \textit{HotpotQA}, \SpecExp{} provides interactions that align with human annotations. Finally, we use our model-agnostic approach to generate explanations to demonstrate abstract reasoning in closed-source LLMs (\emph{GPT-4o mini}) and compositional reasoning in vision-language models.
\end{abstract}
\input{sections/intro}
\input{sections/related_work}
\input{sections/method}
\input{sections/language}
\input{sections/case_study}
\input{sections/conclusion}

\section*{Impact Statement}
Getting insights into the decisions of deep learning models offers significant advantages, including increased trust in model outputs. By reasoning about the rationale behind a model's decisions with the help of \SpecExp{}, we can develop greater confidence in its output, and use it to aid in our own reasoning. When using analysis tools like \SpecExp{}, it's crucial to avoid over-interpretation of results.
\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Algorithm Details}\label{apdx:algorithm}
\input{appendix/algorithm_apdx}

\section{Experiment Details}\label{apdx:experiments}
\input{appendix/exp_details}

\section{Relationships between Fourier and Interaction Concepts} \label{apdx:fourier-interactions}
\input{appendix/interactions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}




% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution












































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%We use the closed-source \emph{GPT-4o mini}, highlighting the power of our model-agnostic approach. 

%, powered by error correction codes that can identify important interactions without an exhaustive search. 
%Experiments across three popular datasets, containing input sizes varying from $10$ to $1000$ demonstrate that \SpecExp{} scales to large contexts, where all competing interaction attribution approaches are infeasible for large $n$.
%For large input lengths, we outperform marginal attribution methods by up to 20\% in terms of faithfully reconstructing near state-of-art transformer based LLM outputs, on sentiment and multi-hop reasoning tasks where interactions are critical. \SpecExp{} successfully identifies key interactions that influence model outputs as well as human-labeled interactions. As a case study, we use \SpecExp{} to generate explanations to study reasoning in vision-language models and LLMs. We use the closed-source \emph{GPT-4o mini}, highlighting the power of our model-agnostic approach. 

%\BY{We need to indicate these are SOTA models and these are not easy problems, if true. Why should we trust these explanations?}
%\BY{how many? are these hard datasets? being more precise is more informative}
%\JK{Is the statement on faithfulness not enough to address the ``"why should we trust?" part}
%In both cases generating interaction based explanations at this scale is only possible because of the advancements of SpectralExplain.
 %possible $2^n$ interactions for contexts of length $n$. %\BY{This is confusing. The previous sentence is saying that SHAP can do interactions. Clarify} 

% which is commonly found in real-world data and models. Specifically, SpectralExplain applies a sparse Fourier transform, utilizing concepts from signal processing, and information and coding theory to reach this scale. Experiments across multiple benchmarks show that SpectralExplain can scale to large contexts while significantly out-performing (up to 20\%) other  attribution approaches in terms of faithfully reconstructing the outputs of LLMs and identifying interactions that influence outputs, and human-labelled interactions. As a case study we apply SpectralExplain to generate explanations for a task of identifying reasoning errors in LLMs and in question-answering for multi-modal models. In both cases generating interaction based explanations at this scale is only possible because of the advancements of SpectralExplain.

%\BY{There are extensions of SHAP that deal with interactions. A google search gives: https://[a] www.frontiersin.org/journals/nutrition/articles/ 10.3389/fnut.2022.871768/full  and [b] https://towardsdatascience.com/analysing-interactions-with-shap-8c4a2bc11c2a -- the latter implies that the SHAP package has interactions already. pls check them out} \JK{To clear this up: There are many definitions of interactions that can be thought of as extending the Shapley value - we mention Faith-Shapley. SHAP is the a software package that has evolved over time. It now features some tools to compute interactions via tree based models (work by Hugh Chen -- will add to citations) but is generally limited to the type of model. [a] and [b] use this. SHAP-IQ is a fork of the SHAP package that focuses on computing interactions for many black box models so we mention that a few times in the paper, with comparisons.}

%\BY{judged by humans?}\JK{Not human judged, based on their ability to predict changes in model output. ``True to the model"} 
%Understanding the outputs of large language models (LLMs) is a central problem in deep learning. The most popular post-hoc methods like SHAP are unable to capture complex interactions between inputs, while more expressive approaches that capture these interactions remain computationally intractable for even modest input sizes. We introduce SpectralExplain, an algorithm that can capture important interactions between inputs and scale to large input spaces. To achieve this, SpectralExplain exploits an underlying sparsity among interactions via a connection to the Fourier transform and information theoretic tools that use this structure to maintain computational efficiency. We show that SpectralExplain works at the scale of large language models, providing explanations that are more faithful to the underlying model, while maintaining a reasonable computational complexity. Thus SpectralExplain is the first to provide interaction-level explanations to large scale language tasks.