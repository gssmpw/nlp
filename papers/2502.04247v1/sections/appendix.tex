\section{Notation} \label{sec:notation}

We summarize below the preliminary tools and notations used throughout the article. Proofs of standard results are omitted, with references provided. For clarity, the content is organized into thematic sections.

\subsection{Tensors} \label{subsec:tensors}

% = splitted in 2 lines
% edit: colt template
Given $S$ finite set (e.g., $[n] \text{ or } [n_1] \times \dots \times [n_k], \text{ for some } k \geq 2$, where $[n] =$ $= \{1, \dots, n\}$) we denote with $\real^S$ the vector space of real valued functions $\bm{v}: S \to \real$ (column vectors which generalizes to multidimensional tensors). 
We adopt the notation $\bm{v}_s \coloneqq \bm{v}(s)$, $\forall s \in S$, and we introduce the following conventions: $\bm{e}_s$ is the $s$\textsuperscript{th} vector of the canonical base ($\bm{e}_s(s) = 1$ and $\bm{e}_s(r) = 0$, $\forall r \in S \setminus \{s\}$), $\bm{1}_{S} \coloneqq \sum_{s \in S} \bm{e}_s$ and $\bm{0}_{S}$ is the constant null vector. 
When $S$ is used as a subscript/superscript we also simplify the notation: $[n_1] \times \dots \times [n_k]$ becomes just $n_1 \times \dots \times n_k$. \\
Given $S$, $T$ two finite sets, we can see the vector space $\real^{S \times T}$ as the space of linear transformations $\bm{A}: \real^T \to \real^S$, defining 
\begin{equation*}
	\forall \bm{v} \in \real^T, \, \bm{A} \bm{v} \coloneqq \bm{A}(\bm{v}) = \sum_{s \in S} \left(\sum_{t \in T} \bm{A}_{s, t} \bm{v}_t\right) \bm{e}_s.
\end{equation*}
If $S = [n]$ and $T = [m]$ then $\bm{A}$ can be represented as a standard matrix $\bm{A} \in \real^{m \times n}$, where $\bm{A}_{i, j} \coloneqq (\bm{A}(\bm{e}_j))(i)$, $\forall i \in [n], j \in [m]$. 
Analogously if $S = [n_1] \times [n_2]$ and $T = [m_1] \times [m_2]$ it is possible to represent $\bm{A}$ as a $4$-dimensional tensor such that $\bm{A}_{i, j, k, l} \coloneqq \bm{A}(\bm{e}_{(k, l)})((i, j))$, $\forall (i, j) \in [n_1] \times [n_2], (k, l) \in [m_1] \times [m_2]$. \\
If $S = T$, we introduce the notation $\operatorname{Sym}^{S}$ for the set of symmetric linear transformations in $\real^{S \times S}$, i.e., $\bm{A} \in \operatorname{Sym}^{S}$ if and only if $\bm{A} \in \real^{S \times S}$, $\bm{A} = \bm{A}^T$. 
Moreover, we denote with $\operatorname{Sym}_+^{S}$ the subset of $\operatorname{Sym}^{S}$ composed by symmetric, positive definite matrices, where a matrix $\bm{A}$ is said to be positive definite if $\forall \bm{v} \in \real^S \setminus \{0\}$, $\bm{v}^T \bm{A} \bm{v} > 0$.

\smallskip

We define the outer product (or tensor product) between $\bm{v} \in \real^S$ and $\bm{w} \in \real^{T}$ as $\bm{v} \otimes \bm{w} \in \real^{S \times T}$ with $(\bm{v} \otimes \bm{w})_{s, t} \coloneqq \bm{v}_s \bm{w}_t$, $\forall s \in S, t \in T$, denoting $\bm{v} \otimes \bm{v}$ as $\bm{v}^{\otimes 2}$. 
We also define the identity map $\bm{I}_S: \real^S \to \real^S$ as $\bm{I}_S \coloneqq \sum_{s \in S} \bm{e}_s^{\otimes 2}$, observing that, $\forall \bm{v} \in \real^S$, $\bm{I}_S \bm{v} = \sum_{s \in S} (\sum_{t \in S} (\bm{I}_S)_{s, t} \bm{v}_t) \bm{e}_s =$ $= \sum_{s \in S} \bm{v}_s \bm{e}_s = \bm{v}$.
If $S = [n]$, $T = [m]$ then $\bm{v} \otimes \bm{w} = \bm{v} \bm{w}^T$. \\
Given a generic pair of elements in a real vector space, $\bm{v}, \bm{w} \in \real^S$, we define the standard scalar product between them as 
\begin{equation*}
	\langle \bm{v}, \bm{w} \rangle \coloneqq \trace{\bm{v} \otimes \bm{w}} = \sum_{s \in S} \bm{v}_s \bm{w}_s.
\end{equation*}
Analogously we can define the Euclidean norm (or $2$-norm) induced by the scalar product as
\begin{equation} \label{eq:tensornorm}
	\norm{\bm{v}} = \langle \bm{v}, \bm{v} \rangle^{1/2} = \left(\sum_{s \in S} \bm{v}_s^2\right)^{1/2}.
\end{equation}
If $S = [n]$, $n \in \natural_{> 0}$, we also define the $p$-norm of $\bm{v}$, $p \geq 1$, as $\norm{\bm{v}}_p \coloneqq \left(\sum_{i = 1}^{n} \bm{v}_i^p\right)^{1/p}$. 
If nothing is specified we always consider $\real^n$ as a normed spaced with the Euclidean norm defined in \cref{eq:tensornorm}. 
In the matrix case, $S = [n] \times [m]$, this norm is usually referred as Frobenius norm, therefore for an improved readability we denote it as $\norm{\cdot}_F$. \\
Given an operator $\bm{A} \in \operatorname{Sym}^{S}$, with $S = [n]$, we define the operator norm as
\begin{equation*}
	\norm{\bm{A}}_{\mathrm{op}} \coloneqq \sup_{\norm{\bm{x}}_2 = 1} \norm{\bm{A}\bm{x}}_2 = \max\left\{\lambda \given \lambda \in \operatorname{Sp}(\bm{A})\right\}.
\end{equation*}

\begin{lemma} \label{lem:norminequalities}
	For every $\bm{x}, \bm{y} \in \left(\real^S, \norm{\cdot}\right)$ real vector space with the Euclidean norm, the following inequalities hold:
	\begin{align}
		\norm{\bm{x} - \bm{y}}^2 & \leq 2 (\norm{\bm{x}}^2 + \norm{\bm{y}}^2), \label{eq:ineqnormsqleq} \\
		\norm{\bm{x} - \bm{y}}^2 & \geq \frac{\epsilon}{\epsilon + 1} \norm{\bm{x}}^2 -\epsilon \norm{\bm{y}}^2, \forall \epsilon \in \real^+ \label{eq:ineqnormsqgeqeps}. 
	\end{align}
\end{lemma}
\begin{proof}
	We prove both the statements using the triangle inequality and its inverse: $\forall \bm{u}, \bm{v} \in \real^S$, $\norm{\bm{u} + \bm{v}} \leq \norm{\bm{u}} + \norm{\bm{v}}$ and $\norm{\bm{u} - \bm{v}} \geq \norm{\bm{u}} - \norm{\bm{v}}$. \\
	Let us start observing that $\forall a, b, \epsilon > 0$, we have
	\begin{equation*}
		\begin{aligned}
			(a + b)^2 & = a^2 + b^2 + \epsilon 2a \frac{b}{\epsilon} \leq a^2 + b^2 + \epsilon \left(a^2 + \frac{b^2}{\epsilon^2}\right) = a^2 \left(1 + \epsilon\right) + b^2 \left(1 + \frac{1}{\epsilon}\right).
		\end{aligned}
	\end{equation*}
	With $\epsilon = 1$ we get $(a + b)^2 \leq 2(a^2 + b^2)$, and therefore by triangle inequality  follows the first result,
	\begin{equation*}
		\norm{\bm{x} - \bm{y}}^2 \leq \left(\norm{\bm{x}} + \norm{\bm{y}}\right)^2 \leq 2 \left(\norm{\bm{x}}^2 + \norm{\bm{y}}^2\right).
	\end{equation*}
	For the second inequality we use the reverse triangle inequality,
	\begin{equation*}
		\begin{aligned}
			\norm{\bm{x}}^2 & \leq \left(\norm{\bm{x} - \bm{y}} + \norm{\bm{y}}\right)^2 \leq \norm{\bm{x} - \bm{y}}^2 (1 + \epsilon) + \norm{\bm{y}} \left(1 + \frac{1}{\epsilon}\right),
		\end{aligned}
	\end{equation*}
	which implies
	\begin{equation*}
		\norm{\bm{x} - \bm{y}}^2 \geq \norm{\bm{x}}^2 \frac{1}{1 + \epsilon} - \norm{\bm{y}}^2 \frac{1}{\epsilon}.
	\end{equation*}
	Now by simply substituting $\epsilon' = \epsilon^{-1}$ in place of $\epsilon$ we get $(1 + (\epsilon')^{-1})^{-1} = \epsilon' / (\epsilon' + 1)$ and so the thesis.
\end{proof}

\subsection{Random variables} \label{subsec:rv}

Given $S$ finite set, a random variable $\bm{x}$ with values in $\real^S$ is a measurable map
\begin{equation*}
	\bm{x}: (\Omega, \mathcal{A}, \mathbb{P}) \to (\real^S, \borel{\real^S}).
\end{equation*}
It is adopted the same notation for deterministic tensors introduced in \Cref{subsec:tensors} and random tensors, i.e., random variable with values in tensor spaces. 
We denote with $\mathbb{P}_{\bm{x}}$ the distribution (or law) of $\bm{x}$, 
\begin{equation*}
	\mathbb{P}_{\bm{x}} \coloneqq \prob{\bm{x}^{-1}(A)}, \, \forall A \in \borel{\real^S}.
\end{equation*}
We write $\bm{x} \sim \bm{y}$ if two random variables share the same distribution.

\smallskip

Given $\bm{x}$ random variable with values on $\real^S$ we define its mean value, or first moment, and its variance\footnote{Often referred as covariance if $|S| > 1$.}, or second moment of $\bm{x} - \mean{\bm{x}}$, respectively as
\begin{equation*}
	\begin{aligned}
		\mean{\bm{x}} & \coloneqq \left(\mean{\bm{x}_s}\right)_{s \in S} = \left(\int_{\Omega} \bm{x}_s(\omega) d\mathbb{P}(\omega)\right)_{s \in S} \in \real^S,
	\end{aligned}
\end{equation*}
and
\begin{equation*}
	\begin{aligned}
		\var{\bm{x}} & \coloneqq \mean{\left(\bm{x} - \mean{\bm{x}}\right)^{\otimes 2}} \in \operatorname{Sym}_+^{S}.
	\end{aligned}
\end{equation*}
Given $p \geq 1$, we define the Lebesgue norm of order $p$ of $\bm{x}$ as
\begin{equation*}
	\norm{\bm{x}}_{L^p} \coloneqq \mean{\norm{\bm{x}}^p}^{1/p} \in \real.
\end{equation*}
Recalling that $\norm{\bm{x}}^2 = \trace{\bm{x}^{\otimes 2}}$, thanks to the linearity of the integral, it is possible to exchange trace and expectation so that we can write 
\begin{equation*}
	\norm{\bm{x}}_{L^2}^2 = \mean{\trace{\bm{x}^{\otimes 2}}} = \trace{\mean{\bm{x}^{\otimes 2}}}.
\end{equation*}

\subsubsection{Gaussian random variables}

Given $\bm{\mu} \in \real^S$, $\bm{\Sigma} \in \operatorname{Sym}_+^{S}$, we denote with $\normal{\bm{\mu}}{\bm{\Sigma}}$ the distribution of a Gaussian random variable $\bm{x}: (\Omega, \mathcal{A}, \mathbb{P}) \to (\real^S, \borel{\real^S})$, such that
\begin{equation*}
	\mean{\bm{x}} = \bm{\mu} \quad \text{and} \quad \var{\bm{x}} = \mean{\left(\bm{x} - \bm{\mu}\right)^{\otimes 2}} = \bm{\Sigma}.
\end{equation*}
\begin{remark} \label{rem:gaussmoments} 
	Let $\left(\real^S, \norm{\cdot}\right)$ be a normed space with a Euclidean norm and $\bm{\mu} = \bm{0}_S$. Then,
	\begin{equation*}
	\mean{\norm{\bm{x}}}^2 \leq \mean{\norm{\bm{x}}^2} = \mean{\sum_{s \in S} \bm{x}_s^2} = \sum_{s \in S} \mean{\bm{x}_s^2} = \trace{\bm{\Sigma}}.
	\end{equation*}
\end{remark}

\subsubsection{Inverse-Gamma random variables}

A random variable $s$, with values in $\left(\real^+, \borel{\real^+}\right)$, is said to be Inverse-Gamma distributed with parameters $a$ and $b$ in $\real^+$ (denoted as $s \sim \invgamma{a}{b}$), if $\mathbb{P}_{s}$ admits a density with respect to the Lebesgue measure $\lambda^+$ on $\real^+$, and in particular
\begin{equation*}
	p_s(s) \coloneqq \frac{d\mathbb{P}_{s}}{d\lambda^+}(s) = \frac{b^a}{\Gamma(a)} \left(\frac{1}{s}\right)^{a + 1} \exp{-\frac{b}{s}}.
\end{equation*}
As the name suggests, and a simple change of variables shows, one can equivalently say that the variable $1 / s$ is Gamma distributed with shape and rate parameters $(a, b)$.

\subsubsection{Multivariate \texorpdfstring{Student-$t$}{Student-t} random variables}

Given $k \in \natural_{> 0}$, a random variable $\bm{z}$, with values in $\left(\real^k, \borel{\real^k}\right)$, is said to be $k$-dimensional Student-$t$ distributed with $\nu$ degrees of freedom, location $\bm{\mu} \in \real^k$ and scale $\bm{\Sigma} \in \operatorname{Sym}_+^{k}$ (denoted as $\bm{z} \sim \tstud{\nu}{\bm{\mu}}{\bm{\Sigma}}$), if $\mathbb{P}_{\bm{z}}$ admits a density with respect to the Lebesgue measure $\lambda^k$ on $\real^k$, and in particular
\begin{equation*}
	p_{\bm{z}}(\bm{z}) \coloneqq \frac{d\mathbb{P}_{\bm{z}}}{d\lambda^k}(\bm{z}) = \frac{\Gamma((\nu + k)/2)}{\Gamma(\nu/2) (\nu \pi)^{k/2} \det{\bm{\Sigma}}^{1/2}} \left(1 + \frac{1}{\nu} (\bm{z} - \bm{\mu})^T \bm{\Sigma}^{-1} (\bm{z} - \bm{\mu})\right)^{-\frac{\nu + k}{2}}.
\end{equation*}

\subsection{Wasserstein distance} \label{subsec:wassdist}

We first recall two well-known properties (see e.g., \citet{otvillani2008}) of the Wasserstein metric, together with the main two technical tools exploited in the demonstration of our core result: \Cref{prop:wassconvexity} and \Cref{lem:nnposterior}.

\begin{proposition} \label{prop:wassmult}
	Given two random variables $\bm{x}$, $\bm{y}$ with values in $\real^S$ and a positive constant $a$, $\forall p \geq 1$, it holds
	\begin{equation} \label{eq:wasslinear}
		\wass[p]{a \bm{x}}{a \bm{y}} = a \wass[p]{\bm{x}}{\bm{y}}.
	\end{equation}
\end{proposition}

\begin{theorem}[Kantorovich duality for $\mathcal{W}_1$] \label{thm:kantorovichduality}
	Given $T$ finite set, $\mu$ and $\widetilde{\mu}$ probability measures on $\real^T$, then we have
	\begin{equation} \label{eq:kantorovichduality}
		\wass[1]{\mu}{\widetilde{\mu}} = \sup_{\substack{f: \real^T \to \real, \\ \lipschitz{f} \leq 1}} \left(\int_{\real^T} f(\bm{x}) d\mu(\bm{x}) - \int_{\real^T} f(\bm{x}) d\widetilde{\mu}(\bm{x})\right). 
	\end{equation}
\end{theorem}

The supremum above is computed over all the functions $f: \real^T \to \real$ that are Lipschitz continuous, with Lipschitz constant $\lipschitz{f} \leq 1$. Notice that one can further restrict to functions $f$ such that $f(0) = 0$ since adding constants to $f$ does not change the difference of the integrals.

\smallskip

Before we state and prove \Cref{prop:wassconvexity}, let us recall the notion of Markov kernel. 

\begin{definition}[Markov kernel]
	Let us consider $(E, \mathcal{E})$, $(F, \mathcal{F})$ measurable spaces, a Markov kernel with source $(E, \mathcal{E})$ and target $(F, \mathcal{F})$ is a map $K_{\mu}: E \times \mathcal{F} \to [0, 1]$, such that
	\begin{itemize}
		\item[-] $\forall B \in \mathcal{F}$, the map $s \to K_{\mu}(s, B)$ for $s \in E$ is measurable from $(E, \mathcal{E})$ to $([0, 1], \borel{[0, 1]})$;
		\item[-] $\forall s \in E$, the map $B \to K_{\mu}(s, B)$ for $B \in \mathcal{F}$ is a probability measure on $(F, \mathcal{F})$.
	\end{itemize}
	For any fixed $s \in E$, we denote $\mu(s) \coloneqq K_{\mu}(s, \cdot)$ and the Markov kernel as $K_{\mu} = (\mu(s))_{s \in E}$.
\end{definition}
\begin{proof}[\normalfont\bfseries Proof of \Cref{prop:wassconvexity}] \hypertarget{proof:wassconvexity}
% edit: colt template
% \begin{proofnote}[\normalfont\bfseries Proof of \Cref{prop:wassconvexity}] \hypertarget{proof:wassconvexity}
	Consider $f: \real^T \to \real$ with $\lipschitz{f} \leq 1$. Then by \cref{eq:kantorovichduality} with the measures $\mu(s)$ and $\widetilde{\mu}(s)$ it follows
	\begin{equation} \label{eq:wassconvexproof}
		\int_{\real^T} f(\bm{x}) d\mu(s)(\bm{x}) - \int_{\real^T} f(\bm{x}) d\widetilde{\mu}(s)(\bm{x}) \leq \wass[1]{\mu(s)}{\widetilde{\mu}(s)}.
	\end{equation}
	Integrating both sides in \cref{eq:wassconvexproof} with respect to $\nu$ yields
	\begin{equation*} 
		\int_{\real^T} f(\bm{x}) d\mu(\bm{x}) - \int_{\real^T} f(\bm{x}) d\widetilde{\mu}(\bm{x}) \leq \int_{\real^+} \wass[1]{\mu(s)}{\widetilde{\mu}(s)} d\nu(s).
	\end{equation*}
	We conclude by taking the supremum over the possible $f$'s and again by Kantorovich duality \cref{eq:kantorovichduality}).
\end{proof}
% edit: colt template
% \end{proofnote}

The following \Cref{lem:nnposterior} shows that if two \textit{prior} distributions are close with respect to the Wasserstein metric and the (common) Likelihood is sufficiently regular, then also the \textit{posterior} distributions will be close, in a quantitative way.

\smallskip

We use the notation
\begin{equation*}
	m_{p}(\mu) \coloneqq \int_{\real^S} \norm{\bm{z}}^{p} d \mu(\bm{z}).
\end{equation*}
for the moment of order $p \geq 1$ of a measure $\mu$.

\begin{lemma}[Lemma 5.1 of \citet{trevisan2023}] \label{lem:nnposterior}
	Let $\mu$, $\widetilde{\mu}$ be probability measures on $(\real^S, \norm{\cdot})$ for some finite set $S$ and finite moments of order $p \geq 1$.
	Fix $g: \real^S \to \real^+$ be a uniformly bounded (by $\norm{g}_\infty$) Lipschitz continuous map (with constant $\lipschitz{g}$), such that
	\begin{equation*}
		\mu(g) \coloneqq \int_{\real^S} g(\bm{z}) d\mu(\bm{z}) > 0 \quad \text{and} \quad \widetilde{\mu}(g) >0.
	\end{equation*}
	Defining the probability measures $\mu_g \ll \mu$ and $\widetilde{\mu}_g \ll \widetilde{\mu}$, with respective densities $\frac{d\mu_g}{d\mu} \coloneqq \frac{g}{\mu(g)}$ and $\frac{d\widetilde{\mu}_g}{d\widetilde{\mu}} \coloneqq \frac{g}{\widetilde{\mu}(g)}$, it holds
	\begin{equation} \label{eq:wassposterior}
		\wass[1]{\widetilde{\mu}_g}{\mu_g} \leq \frac{1}{\mu(g)} \left(\lipschitz{g} m_{p / (p - 1)}(\mu) + \left(1 + \frac{m_1(\mu) \lipschitz{g}}{\widetilde{\mu}(g)}\right) \norm{g}_{\infty}\right) \wass[p]{\widetilde{\mu}}{\mu}.
	\end{equation}
\end{lemma}
% \begin{proof}
%     Given a coupling $(\bm{u}, \bm{v})$ of the probabilities $(\mu, \widetilde{\mu})$, using that $g$ is Lipschitz and applying Jensen, it holds that 
%     \begin{equation*}
%         |\mu(g) - \widetilde{\mu}(g)| = |\mean{g(\bm{u}) - g(\bm{v})}| \leq \norm{g(\bm{u}) - g(\bm{v})}_{L^1} \leq \lipschitz{g} \norm{\bm{u} - \bm{v}}_{L^1}.
%     \end{equation*}
%     Taking the infimum over the possible couplings we get
%     \begin{equation} \label{eq:nnposterior1}
%         |\mu(g) - \widetilde{\mu}(g)| \leq \lipschitz{g} \wass[1]{\mu}{\widetilde{\mu}}.
%     \end{equation}
%     Similarly, given $f: \real^S \to \real$ with $f(0) = 0$, $\lipschitz{f} \leq 1$, so that for every $\bm{z}$ in $\real^S$, $|f(\bm{z})| \leq \norm{\bm{z}}$, by applying triangle, H{\"o}lder and Jensen inequalities we can derive the following result:
%     \begin{equation*}
%         \begin{aligned}
%             & \left|\int_{\real^S} f g d\mu - \int_{\real^S} f g d\widetilde{\mu}\right| \leq \norm{f(\bm{u}) g(\bm{u}) - f(\bm{v}) g(\bm{v})}_{L^1} \leq \\
%             & \kern20pt \leq \norm{f(\bm{u}) (g(\bm{u}) - g(\bm{v}))}_{L^1} + \norm{g(\bm{v}) (f(\bm{u}) - f(\bm{v}))}_{L^1} \leq \\
%             & \kern20pt \leq \norm{f(\bm{u})}_{L^{p'}} \norm{g(\bm{u}) - g(\bm{v})}_{L^{p}} + \norm{g}_{\infty} \norm{f(\bm{u}) - f(\bm{v})}_{L^{1}} \leq \\
%             & \kern20pt \leq m_{p'}(\mu) \lipschitz{g} \norm{\bm{u} - \bm{v}}_{L^{p}} + \norm{g}_{\infty} \norm{\bm{u} - \bm{v}}_{L^{p}},
%         \end{aligned}
%     \end{equation*}
%     with $p' = p / (p - 1)$. Taking again the infimum over the couplings $(\bm{u}, \bm{v})$ we get
%     \begin{equation} \label{eq:nnposterior2}
%         \left|\int_{\real^S} f g d\mu - \int_{\real^S} f g d\widetilde{\mu}\right| \leq \left(m_{p'}(\mu) \lipschitz{g} + \norm{g}_{\infty}\right) \wass[p]{\mu}{\widetilde{\mu}}.
%     \end{equation}
%     Next we use Kantorovich duality (\Cref{thm:kantorovichduality}) that yields
%     \begin{equation*}
%         \wass[1]{\mu_g}{\widetilde{\mu}_g} = \sup_{\substack{f: \real^S \to \real, \\ \lipschitz{f} \leq 1}} \left(\int_{\real^S} f d\mu_g - \int_{\real^S} f d\widetilde{\mu}_g\right),
%     \end{equation*}
%     and we recall that it is possible to assume $f(0) = 0$ without loss of generality.
%     Hence, using \cref{eq:nnposterior1,eq:nnposterior2} and $\wass[1]{\mu}{\widetilde{\mu}} \leq \wass[p]{\mu}{\widetilde{\mu}}$ we get
%     \begin{equation*}
%         \begin{aligned}
%             \int_{\real^S} f d\mu_g - \int_{\real^S} f d\widetilde{\mu}_g & = \frac{1}{\mu(g)} \int_{\real^S} f g d\mu - \frac{1}{\widetilde{\mu}(g)} \int_{\real^S} f g d\widetilde{\mu} \, \pm \, \frac{1}{\mu(g)} \int_{\real^S} f g d\widetilde{\mu} = \\
%             & = \frac{1}{\mu(g)} \int_{\real^S} f g d(\mu - \widetilde{\mu}) + \left(\frac{1}{\mu(g)} - \frac{1}{\widetilde{\mu}(g)}\right) \int_{\real^S} f g d\widetilde{\mu} \leq \\
%             & \leq \frac{1}{\mu(g)} \left(\int_{\real^S} f g d\mu - \int_{\real^S} f g d\widetilde{\mu}\right) + \norm{g}_{\infty} \frac{|\mu(g) - \widetilde{\mu}(g)|}{\mu(g) \widetilde{\mu}(g)} m_1(\mu) \leq \\
%             & \leq \frac{m_{p'}(\mu) \lipschitz{g} + \norm{g}_{\infty}}{\mu(g)} \wass[p]{\mu}{\widetilde{\mu}} + \frac{\lipschitz{g} \norm{g}_{\infty} m_1(\mu)}{\mu(g) \widetilde{\mu}(g)} \wass[1]{\mu}{\widetilde{\mu}} \leq \\
%             & \leq \frac{1}{\mu(g)} \left(\lipschitz{g} m_{p'}(\mu) + \left(1 + \frac{m_1(\mu) \lipschitz{g}}{\widetilde{\mu}(g)}\right) \norm{g}_{\infty}\right) \wass[p]{\mu}{\widetilde{\mu}},
%         \end{aligned}
%     \end{equation*}
%     and the thesis follows taking the supremum over $f$.
% \end{proof}
% \begin{remark}
%     \Cref{lem:nnposterior} holds for any $p \geq 1$, but for our purposes it is enough to consider $p = 2$ (and so $p / (p - 1) = 2$) because for any $p$ the result is equivalent: the posterior measures with density $g$ normalized are as close in $\mathcal{W}_1$ as their prior measures are in $\mathcal{W}_p$, up to a constant term.
% \end{remark}

\section{Posterior NNGP} \label{sec:postnngp}

Given the Bayesian framework presented in \cref{eq:hiervariancenngp,eq:hiermodel,eq:rescaledkernel}, in order to get the posterior distribution of $G(\bm{x}_{\mathcal{D}}) \given \train$ we write explicitly all the densities and apply the Bayes rule. 
In particular, assuming $\bm{K}'(\bm{x}_{\mathcal{D}}) \in \operatorname{Sym}_+^{k}$ invertible (we already know that is symmetric positive semi-definite), flattening all the random matrices by columns and defining
\begin{equation*}
	\bm{y}_{\mathrm{f}} \coloneqq \flatten{\bm{y}_{\mathcal{D}}}, \quad \bm{z}_{\mathrm{f}} \coloneqq \flatten{\bm{z}},
\end{equation*}
with $\bm{z} \coloneqq G(\bm{x}_{\mathcal{D}}) \in \real^{n_L \times k}$, we get
\begin{equation*}
	\begin{aligned}	
		p_{G(\bm{x}_{\mathcal{D}}) | \sigma^2}(\bm{z}) & = \frac{1}{\left((2\pi \sigma^2)^{n_L k} \det{\bm{K}'(\bm{x}_{\mathcal{D}}) \otimes_K \bm{I}_{n_L}}\right)^{1/2}} \cdot \\
		& \blankeq \cdot \exp{-\frac{1}{2\sigma^2} \bm{z}_{\mathrm{f}}^T \left(\bm{K}'(\bm{x}_{\mathcal{D}}) \otimes_K \bm{I}_{n_L}\right)^{-1} \bm{z}_{\mathrm{f}}}, \\
		p_{\sigma^2}(\sigma^2) & = \frac{b^a}{\Gamma(a)} \frac{1}{\left(\sigma^2\right)^{a + 1}} \exp{-\frac{b}{\sigma^2}}, \\
		p_{\bm{y}_{\mathcal{D}} | G(\bm{x}_{\mathcal{D}}), \sigma^2}(\bm{y}_{\mathcal{D}}) &= \frac{1}{\left(2\pi\sigma^2\right)^{n_L k / 2}} \exp{-\frac{1}{2 \sigma^2} (\bm{y}_{\mathrm{f}} - \bm{z}_{\mathrm{f}})^T (\bm{y}_{\mathrm{f}} - \bm{z}_{\mathrm{f}})}.
	\end{aligned}
\end{equation*}
with $\otimes_K$ representing the Kronecker product. \\
By performing explicit computation we retrieve the posterior distribution of $G(\bm{x}_{\mathcal{D}}), \sigma^2 \given \train$ as
\begin{equation*}
	\begin{aligned}
		p_{G(\bm{x}_{\mathcal{D}}), \sigma^2 | \train}(\bm{z}, \sigma^2) & \propto p_{\bm{y}_{\mathcal{D}} | G(\bm{x}_{\mathcal{D}}), \sigma^2}(\bm{y}_{\mathcal{D}}) \, p_{G(\bm{x}_{\mathcal{D}}) | \sigma^2}(\bm{z}) \, p_{\sigma^2}(\sigma^2) \propto \\
		& \propto \frac{1}{\left(\sigma^2\right)^{n_L k / 2}} \exp{-\frac{1}{2\sigma^2} (\bm{y}_{\mathrm{f}} - \bm{z}_{\mathrm{f}})^T (\bm{y}_{\mathrm{f}} - \bm{z}_{\mathrm{f}})} \cdot \\
		& \blankeq \cdot \frac{1}{\left(\sigma^2\right)^{n_L k / 2} \sqrt{\det{\bm{K}'(\bm{x}_{\mathcal{D}})}}} \exp{-\frac{1}{2 \sigma^2} \bm{z}_{\mathrm{f}}^T \left(\bm{K}'(\bm{x}_{\mathcal{D}}) \otimes_K \bm{I}_{n_L}\right)^{-1} \bm{z}_{\mathrm{f}}} \cdot \\
		& \blankeq \cdot \frac{1}{\left(\sigma^2\right)^{a + 1}} \exp{-\frac{b}{\sigma^2}}.
	\end{aligned}
\end{equation*}
Defining $\bm{N} \coloneqq \bm{K}'(\bm{x}_{\mathcal{D}}) \otimes_K \bm{I}_{n_1} \in \operatorname{Sym}_+^{n_1 k}$, $\bm{M} \coloneqq \bm{I}_{n_1 k} + \bm{N}^{-1} \in \operatorname{Sym}_+^{n_1 k}$, through simple manipulations of the exponent we get
\begin{equation*}
	\begin{aligned}
		& (\bm{y}_{\mathrm{f}} - \bm{z}_{\mathrm{f}})^T (\bm{y}_{\mathrm{f}} - \bm{z}_{\mathrm{f}}) + \bm{z}_{\mathrm{f}}^T \bm{N}^{-1} \bm{z}_{\mathrm{f}} = 
		\norm{\bm{y}_{\mathrm{f}}}_2^2 + \norm{\bm{z}_{\mathrm{f}}}_2^2 - 2 \bm{y}_{\mathrm{f}}^T \bm{z}_{\mathrm{f}} + \bm{z}_{\mathrm{f}}^T \bm{N}^{-1} \bm{z}_{\mathrm{f}} = \\
		& \kern20pt = \bm{z}_{\mathrm{f}}^T \left(\bm{I}_{n_L k} + \bm{N}^{-1}\right) \bm{z}_{\mathrm{f}} - 2 \bm{y}_{\mathrm{f}}^T \bm{z}_{\mathrm{f}} + \bm{y}_{\mathrm{f}}^T \bm{y}_{\mathrm{f}} \pm \bm{y}_{\mathrm{f}}^T \left(\bm{I}_{n_L k} + \bm{N}^{-1}\right)^{-1} \bm{y}_{\mathrm{f}} = \\
		& \kern20pt = \bm{z}_{\mathrm{f}}^T \bm{M} \bm{z}_{\mathrm{f}} - 2 \bm{y}_{\mathrm{f}}^T \bm{z}_{\mathrm{f}} + \bm{y}_{\mathrm{f}}^T \bm{y}_{\mathrm{f}} \pm \bm{y}_{\mathrm{f}}^T \bm{M}^{-1} \bm{y}_{\mathrm{f}} = \\
		& \kern20pt = \left(\bm{z}_{\mathrm{f}} - \bm{M}^{-1} \bm{y}_{\mathrm{f}}\right)^T \bm{M} \left(\bm{z}_{\mathrm{f}} - \bm{M}^{-1} \bm{y}_{\mathrm{f}}\right) + \bm{y}_{\mathrm{f}}^T \left(\bm{I}_{n_L k} - \bm{M}^{-1}\right) \bm{y}_{\mathrm{f}}.
	\end{aligned}
\end{equation*}
Substituting and multiplying for the constant term $\sqrt{\det{\bm{M}^{-1}}}$ we obtain
\begin{equation} \label{eq:marginalposteriors}
	\begin{aligned}
		p_{G(\bm{x}_{\mathcal{D}}), \sigma^2 | \train}(\bm{z}, \sigma^2) & \propto \frac{1}{\left(\sigma^2\right)^{n_L k / 2} \sqrt{\det{\bm{M}^{-1}}}} \exp{-\frac{1}{2 \sigma^2} (\bm{z}_{\mathrm{f}} - \bm{M}^{-1} \bm{y}_{\mathrm{f}})^T \bm{M} (\bm{z}_{\mathrm{f}} - \bm{M}^{-1} \bm{y}_{\mathrm{f}})} \cdot \\
		& \blankeq \cdot \frac{1}{\left(\sigma^2\right)^{(a + n_L k / 2) + 1}} \exp{-\frac{1}{\sigma^2} \left(b + \frac{1}{2} \left(\bm{y}_{\mathrm{f}}^T \left(\bm{I}_{n_L k} - \bm{M}^{-1}\right) \bm{y}_{\mathrm{f}}\right)\right)}.
	\end{aligned}
\end{equation}
From \cref{eq:marginalposteriors} it is possible to identify two kernels: one associable with a Gaussian density and the other with an Inverse-Gamma density,
\begin{equation*}
	\begin{aligned}
		\flatten{G(\bm{x}_{\mathcal{D}})} \given \sigma^2, \train & \sim \normal{\bm{M}^{-1} \flatten{\bm{y}_{\mathcal{D}}}}{\sigma^2 \bm{M}^{-1}}, \\
		\sigma^{2} \given \train & \sim \invgamma{a + \frac{n_L k}{2}}{b + \frac{1}{2} \left(\flatten{\bm{y}_{\mathcal{D}}}^T \left(\bm{I}_{n_1 k} - \bm{M}^{-1}\right) \flatten{\bm{y}_{\mathcal{D}}}\right)}.
	\end{aligned}
\end{equation*}
This result allows us to apply the following \Cref{lem:norminvgammastudent} (see \citet{bayesiantheory2009}) and state that the induced posterior distribution, $G(\bm{x}_{\mathcal{D}}) \given \train$ is a $(n_L \times k)$-dimensional Student-$t$:
\begin{equation*}
	\begin{aligned}
		\flatten{G(\bm{x}_{\mathcal{D}})} \given \train \sim \tstud{2a + n_L k}{\bm{\mu}_{\mathrm{post}}}{\bm{\Sigma}_{\mathrm{post}}},
	\end{aligned}
\end{equation*}
with
\begin{equation*}
	\begin{aligned}
		\bm{M} & \coloneqq \bm{I}_{n_L k} + \left(\bm{K}'(\bm{x}) \otimes_K \bm{I}_{n_L}\right)^{-1}, \\
		\bm{\mu}_{\mathrm{post}} & \coloneqq \bm{M}^{-1} \flatten{\bm{y}_{\mathcal{D}}}, \\
		\bm{\Sigma}_{\mathrm{post}} & \coloneqq \left(b + \frac{1}{2} \left(\flatten{\bm{y}_{\mathcal{D}}}^T \left(\bm{I}_{n_L k} - \bm{M}^{-1}\right) \flatten{\bm{y}_{\mathcal{D}}}\right)\right) \frac{2}{2a + n_L k} \bm{M}^{-1}.
	\end{aligned}
\end{equation*}

\begin{lemma} \label{lem:norminvgammastudent}
	Let $k \in \natural_{> 0}$, and $(\bm{z}, \sigma^2)$ Gaussian-Inverse-Gamma (Gaussian-IG) distributed, i.e.,
	\begin{align*}
		\bm{z} \given \sigma^2 & \sim \normal{\bm{\mu}}{\sigma^2 \bm{\Lambda}} \text{ with } \bm{\mu} \in \real^k, \, \bm{\Lambda} \in \operatorname{Sym}_+^{k} \text{ and} \\
		\sigma^2 & \sim \invgamma{\alpha}{\beta} \text{ with } \alpha, \beta > 0,
	\end{align*}
	then $\bm{z}$ is distributed as a $k$-dimensional Student-$t$ with $2 \alpha$ degrees of freedom, $\bm{z} \sim \tstud{2\alpha}{\bm{\mu}}{\frac{\beta}{\alpha} \bm{\Lambda}}$.
\end{lemma}
\begin{proof}
	We know, by hypothesis that $(\bm{z}, \sigma^2)$ is such that
	\begin{align*}
		p_{(\bm{z}, \sigma^2)}(\bm{z}, \sigma^2) & = \frac{1}{(2 \pi)^{\frac{k}{2}} \left(\sigma^2\right)^{\frac{k}{2}} \sqrt{\det{\bm{\Lambda}}}} \exp{-\frac{1}{2 \sigma^2} (\bm{z} - \bm{\mu})^T \bm{\Lambda}^{-1} (\bm{z} - \bm{\mu})} \cdot \\
		& \blankeq \cdot \frac{\beta^\alpha}{\Gamma(\alpha)} \left(\frac{1}{\sigma^2}\right)^{\alpha + 1} \exp{-\frac{\beta}{\sigma^2}}.
	\end{align*}
	Marginalizing over the variance $\sigma^2$ we get
	\begin{equation} \label{eq:ninvgstud1}
		\begin{aligned}
			p_{\bm{z}}(\bm{z}) & = \int_{0}^{\infty} p_{(\bm{z}, \sigma^2)}(\bm{z}, \sigma^2) d \sigma^2 = \\
			& \propto \int_{0}^{\infty} \exp{-\frac{1}{2 \sigma^2} \left(2 \beta + (\bm{z} - \bm{\mu})^T \bm{\Lambda}^{-1} (\bm{z} - \bm{\mu})\right)} \left(\frac{1}{\sigma^2}\right)^{\alpha + \frac{k}{2} + 1} d \sigma^2.
		\end{aligned}
	\end{equation}
	Setting $a = \alpha + \frac{k}{2}$, $b = \frac{2 \beta + (\bm{z} - \bm{\mu})^T \bm{\Lambda}^{-1} (\bm{z} - \bm{\mu})}{2}$, $s = \sigma^2$ one can rewrite the last line of \cref{eq:ninvgstud1} as 
	\begin{equation} \label{eq:ninvgstud2}
		\begin{aligned}
			p_{\bm{z}}(\bm{z}) & \propto \int_{0}^{\infty} s^{-(a + 1)} \exp{-\frac{b}{s}} d s = \int_{\infty}^{0} \left(\frac{t}{b}\right)^{a + 1} e^{-t} \left(-\frac{b}{t^2}\right) dt = \\
			& = \int_{0}^{\infty} b^{-a} t^{a - 1} e^{-t} dt = \Gamma(a) b^{-a} \propto \left(\frac{2 \beta + (\bm{z} - \bm{\mu})^T \bm{\Lambda}^{-1} (\bm{z} - \bm{\mu})}{2}\right)^{-\left(\alpha + \frac{k}{2}\right)} \propto \\
			& \propto \left(1 + \frac{1}{2 \alpha} (\bm{z} - \bm{\mu})^T \left(\frac{\beta}{\alpha} \bm{\Lambda}\right)^{-1} (\bm{z} - \bm{\mu})\right)^{-\frac{2\alpha + k}{2}},
		\end{aligned}
	\end{equation}
	where in the first equality of \cref{eq:ninvgstud2} we performed the change of variable $t = \frac{b}{s}$. In the final form of $p_{\bm{z}}(\bm{z})$ it is possible to recognize the kernel of a $k$-dimensional Student-$t$, $\bm{z} \sim \tstud{2 \alpha}{\bm{\mu}}{\frac{\beta}{\alpha} \bm{\Lambda}}$.
\end{proof}

\section{Proof of the main result} \label{sec:mainproof}

\subsection{Distance between marginal posterior of BNNs and NNGP} \label{subsec:convposteriorsigma}

\begin{proposition} \label{prop:likelihood}
	Let $\mathcal{L}$ be a Gaussian likelihood, $\mathcal{L} \sim \normal{\bm{z}}{\sigmay^2 \bm{I}_{n_L \times k}}$, it holds
	\begin{equation*}
		\norm{\mathcal{L}}_{\infty} = \frac{1}{\left(2 \pi \sigmay^2\right)^{n_L k / 2}} \quad \text{and} \quad \lipschitz{\mathcal{L}} = \frac{e^{-1/2}}{\sqrt{\sigmay^2}} \frac{1}{\left(2 \pi \sigmay^2\right)^{n_L k / 2}}.
	\end{equation*}
\end{proposition}
\begin{proof}
	We first rewrite the map in a more compact form in terms of the Frobenius norm of $(\bm{y}_{\mathcal{D}} - \bm{z})$: $\forall \bm{z} \in \real^{n_L \times k}$,
	\begin{equation*}
		\mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}}) = \frac{1}{\left(2 \pi \sigmay^2\right)^{n_L k / 2}} \exp{-\frac{1}{2\sigmay^2} \norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F^2}.
	\end{equation*}
	Then for the uniform norm it is sufficient to recall that $\mathcal{L}$ is a bell-shaped map with maximum in the mean point. 
	Therefore, it is immediate that
	\begin{equation*}
		\norm{\mathcal{L}}_{\infty} = \mathcal{L}(\bm{y}_{\mathcal{D}}; \bm{y}_{\mathcal{D}}) = \frac{1}{\left(2 \pi \sigmay^2\right)^{n_L k / 2}}.
	\end{equation*}
	For the identification of the Lipschitz constant it is necessary to recall that, as a consequence of the Mean Value Theorem, given a map $g: \Omega \to \real$ with $\Omega$ open convex subset of $\real^S$, $S$ finite set, if $\sup_{\bm{z} \in \Omega} \norm{\partial / \partial \bm{z} \, g(\bm{z})} \leq L$, then $\mathcal{L}$ is $L$-Lipschitz.
	Hence, our objective is to identify the value of $\sup_{\bm{z} \in \real^{n_L \times k}} \norm{\partial / \partial \bm{z} \, \mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}})}_F$. \\
	Let us define $c = \frac{1}{\left(2 \pi \sigmay^2\right)^{n_L k / 2}}$, then
	\begin{equation*}
		% edit: colt template
		% \setmuskip{0.9}
		\begin{aligned}
			\frac{\partial}{\partial \bm{z}} \mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}}) & = c \, \frac{\partial}{\partial \bm{z}} \exp{-\frac{1}{2\sigmay^2} \norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F^2} = \\
			& = c \, \frac{\partial}{\partial \norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F^2} \exp{-\frac{1}{2\sigmay^2} \norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F^2} \frac{\partial}{\partial (\bm{y}_{\mathcal{D}} - \bm{z})} \norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F^2 \frac{\partial}{\partial \bm{z}} (\bm{y}_{\mathcal{D}} - \bm{z}) = \\
			& = c \, \left(- \frac{1}{2 \sigmay^2} \exp{-\frac{1}{2\sigmay^2} \norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F^2}\right) 2 (\bm{y}_{\mathcal{D}} - \bm{z}) \left(- \bm{I}_{n_L \times k}\right) = \frac{\bm{y}_{\mathcal{D}} - \bm{z}}{\sigmay^2} \mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}}).
		\end{aligned}
	\end{equation*}
	To find the supremum of $h: \real^{n_L \times k} \to \real$, 
	\begin{equation*}
		h(\bm{z}) \coloneqq \norm{\frac{\partial}{\partial \bm{z}} \mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}})}_F = \frac{\norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F}{\sigmay^2} \mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}}), \ \forall \bm{z} \in \real^{n_L \times k},
	\end{equation*}
	we first have to notice that $h$ is a positive real valued map, and that its first derivative has zeros in every $\bm{z}_0$ such that $\norm{\bm{y}_{\mathcal{D}} - \bm{z}_0}_F^2 = \sigmay^2$, which, as a consequence, are critical points. Indeed, for every $\bm{z} \neq \bm{y}_{\mathcal{D}}$,
	\begin{align*}
		\frac{\partial}{\partial \bm{z}} h(\bm{z}) & = \frac{1}{\sigmay^2} \left(\frac{\partial}{\partial \bm{z}} \mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}}) \norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F + \mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}}) \frac{\partial}{\partial \bm{z}} \norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F\right) = \\
		& = \frac{1}{\sigmay^2} \left(\frac{\bm{y}_{\mathcal{D}} - \bm{z}}{\sigmay^2} \mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}}) \norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F - \mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}}) \frac{\bm{y}_{\mathcal{D}} - \bm{z}}{\norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F}\right) = \\
		& = \frac{1}{\sigmay^2} \mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}}) \frac{\bm{y}_{\mathcal{D}} - \bm{z}}{\norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F} \left(\frac{\norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F^2}{\sigmay^2} - 1\right),
	\end{align*}
	which is null if and only if $\norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F^2 / \sigmay^2 = 1$. \\
	For any such $\bm{z}_0$ we would have that
	\begin{equation*}
		h(\bm{z}_0) = \mathcal{L}\left(\bm{z}_0; \bm{y}_{\mathcal{D}}\right) = \frac{1}{\sqrt{\sigmay^2}} \frac{e^{-1/2}}{\left(2 \pi \sigmay^2\right)^{n_L k / 2}}.
	\end{equation*}
	It is also possible to define 
	\begin{equation*}
		l: \real^+ \to \real, \ l(x) \coloneqq \frac{x}{\sigmay^2} \frac{1}{\left(2 \pi \sigmay^2\right)^{n_L k / 2}} \exp{-\frac{1}{2 \sigmay^2} x^2}, \ \forall x \in \real^+,
	\end{equation*}
	which, by construction, has the following property: 
	\begin{equation*}
		\forall \bm{z} \in \real^{n_L \times k}, \ l(\norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F) = h(\bm{z}).
	\end{equation*}
	From the definition it is easy to see that $\sup_{x \in \real^+} l(x) = h(\bm{z}_0)$, just computing its first two derivatives $l'$ and $l''$. Defining $d = \sigmay^{-2} \left(2 \pi \sigmay^2\right)^{- n_L k / 2}$ positive constant we have
	\begin{align*}
		l'(x) & = d \exp{-\frac{1}{2\sigmay^2} x^2} \left(1 - x^2 \frac{1}{\sigmay^2}\right), \\
		l''(x) & = - d \exp{-\frac{1}{2\sigmay^2} x^2} \frac{1}{\sigmay^2} x \left(\left(1 - x^2 \frac{1}{\sigmay^2}\right) + 2\right) = \\
		& = - d \exp{-\frac{1}{2\sigmay^2} x^2} \frac{1}{\sigmay^2} x \left(3 - \frac{x^2}{\sigmay^2}\right),
	\end{align*}
	and so
	\begin{align*}
		l'(x) = 0 \iff x^2 = \sigmay^2, \ l''(x) \leq 0 \iff x^2 \leq 3 \sigmay^2.
	\end{align*}
	This implies that in $x = \sqrt{\sigmay^2}$ there is a local maximum and observing the asymptotic behavior, $\lim_{x \to +\infty} l(x) = 0$, we know that it is the unique global maximum. 
	Finally, evaluating $l$ in $\sqrt{\sigmay^2}$ we get 
	\begin{equation*}
		l\left(\sqrt{\sigmay^2}\right) = \frac{1}{\sqrt{\sigmay^2}} \frac{e^{- 1/2}}{\left(2 \pi \sigmay^2\right)^{n_L k / 2}}.
	\end{equation*}
	Therefore, it follows that the critical points $\bm{z}_0$ are global maxima for $h$ and so
	\begin{equation*}
		\sup_{\bm{z} \in \real^{n_L \times k}} \norm{\frac{\partial}{\partial \bm{z}} \mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}})}_F = h(\bm{z}_0).
	\end{equation*}
\end{proof}

For the sake of completeness we report below a more detailed version of the Corollary 5.3 of \citet{trevisan2023}, which we use as a starting point for the subsequent results.

\begin{corollary} \label{cor:convposterior}
	Given $f_{\bm{\theta}}$ BNN, with architecture $\bm{\alpha} = (\bm{n}, \bm{\varphi})$, $\bm{\varphi}$ collection of Lipschitz activation functions, prior distribution on $\bm{\theta}$ as in \cref{eq:nealprior}, $G$ Gaussian process as in \cref{eq:nngp}, $\bm{x}$, $\bm{y}_{\mathcal{D}}$ as in \Cref{subsec:postbnn}, and a Gaussian likelihood function $\mathcal{L} \sim \normal{\bm{z}}{\sigmay^2 \bm{I}_{n_L \times k}}$, exists a constant
	\begin{equation*}
		c\left(\train, \bm{\varphi}, \bm{\sigma}, \sigmay^2, n_L\right) > 0, \text{ independent of } \left(n_l\right)_{l = 1}^{L - 1},
	\end{equation*}
	such that,
	\begin{equation*}
		\wass[1]{f_{\bm{\theta}}(\bm{x}) \given \train}{G(\bm{x}) \given \train} \leq c \frac{1}{\sqrt{n_{min}}},
	\end{equation*}
	for all $\displaystyle n_{min} \coloneqq \min_{l = 1, \dots, L - 1} n_l$ sufficiently large.
\end{corollary}
\begin{proof}
	Let $\widetilde{\mu}$ the law of the induced prior distribution of a BNN, $\widetilde{\mu} \sim f_{\bm{\theta}}(\bm{x})$, and $\mu$ be the law of the associated NNGP, $\mu \sim G(\bm{x})$, probability measures on $\left(\real^{n_L \times k}, \norm{\cdot}\right)$. \\
	Let $g \coloneqq \mathcal{L}: \real^{n_L \times k} \to \real$, bounded Lipschitz map.
	The posterior distributions $\mathbb{P}_{f_{\bm{\theta}}(\bm{x}) | \train}$ and $\mathbb{P}_{G(\bm{x}) | \train}$ are, by construction, respectively equal to $\widetilde{\mu}_g$ and $\mu_g$ as they are defined in \Cref{lem:nnposterior}, therefore, by a direct application with $p = 2$, we get the following rewriting of \cref{eq:wassposterior}:
	\begin{equation} \label{eq:wasspostcor}
		% edit: colt template
		% \setmuskip{0.75}
		\begin{aligned}
			& \wass[1]{f_{\bm{\theta}}(\bm{x}) \given \train}{G(\bm{x}) \given \train} \leq \frac{1}{p^{(1)}} \left(\lipschitz{\mathcal{L}} p^{(3)} + \left(1 + \frac{p^{(4)} \lipschitz{\mathcal{L}}}{p^{(2)}}\right) \norm{\mathcal{L}}_{\infty}\right) \wass[2]{f_{\bm{\theta}}(\bm{x})}{G(\bm{x})},
		\end{aligned}
	\end{equation}
	where $\lipschitz{\mathcal{L}}, \norm{\mathcal{L}}_{\infty}$ are positive constants depending on $k, \sigmay^2, n_L$, as showed in \Cref{prop:likelihood}, and
	\begin{equation*}
		\begin{gathered}
			p^{(1)} = \mean[\bm{z} \sim G(\bm{x})]{\mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}})}, \ p^{(2)} = \mean[\bm{z} \sim f_{\bm{\theta}}(\bm{x})]{\mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}})}, \\
			p^{(3)} = \mean[\bm{z} \sim G(\bm{x})]{\norm{\bm{z}}_F^{2}}, \ p^{(4)} = \mean[\bm{z} \sim G(\bm{x})]{\norm{\bm{z}}_F}.
		\end{gathered}
	\end{equation*}
	It is immediate that $p^{(3)}$ and $p^{(4)}$ are finite indeed they are the second and the first moment of a multivariate centered Gaussian, $G(\bm{x}) \sim \normal{\bm{0}_{n_L \times k}}{\bm{I}_{n_L} \otimes \bm{K}(\bm{x})}$, respectively. Therefore, as we observed in \Cref{rem:gaussmoments}, it holds
	\begin{align} \label{eq:p3-4}
	(p^{(4)})^2 \leq p^{(3)} & = \trace{\bm{I}_{n_L} \otimes \bm{K}(\bm{x})} = n_L \trace{\bm{K}(\bm{x})} \leq n_L k \max_{i \in [k]} \bm{K}(\bm{x}_i, \bm{x}_i) \leq \nonumber \\
		& \leq n_L k \left(\sigma_{\bm{W}^{(L)}}^2 \max_{i \in [k]} \mean{\norm{\varphi_{L}\left(G^{(L - 1)}(\bm{x}_i)\right)}_2^2} / n_L + \sigma_{\bm{b}^{(L)}}^2\right).
	\end{align} 
	So both the terms can be bounded by expressions only dependent on $\bm{x}, \bm{\varphi}, \bm{\sigma}$ and $n_L$. \\
	For the remaining terms we still exploit the normality of $G(\bm{x})$ (see \cref{eq:nngpx}). 
	Due to the boundedness and positivity of $\mathcal{L}: \real^{d_{\mathrm{out} \times k}} \to (0, \norm{\mathcal{L}}_{\infty})$, it is clear that $p^{(1)} \in (0, \infty)$, indeed we are integrating $\mathcal{L}$ with respect to a strictly positive probability measure: $0 < p^{(1)}(\train, \bm{\varphi}, \bm{\sigma}, \sigmay^2, n_L) =$ $= \mean{\mathcal{L}(G(\bm{x}); \bm{y}_{\mathcal{D}})} \leq \norm{\mathcal{L}}_{\infty}$. \\
	Moreover, it is possible to notice that considering $\bm{v} \sim G(\bm{x})$, $\bm{w} \sim f_{\bm{\theta}}(\bm{x})$,
	\begin{align*}
		|p^{(1)} - p^{(2)}| & = \left|\mean{\mathcal{L}(\bm{v}; \bm{y}_{\mathcal{D}}) - \mathcal{L}(\bm{w}; \bm{y}_{\mathcal{D}})}\right| \leq \norm{\mathcal{L}(\bm{v}; \bm{y}_{\mathcal{D}}) - \mathcal{L}(\bm{w}; \bm{y}_{\mathcal{D}})}_{L^1} \leq \lipschitz{\mathcal{L}} \norm{\bm{v} - \bm{w}}_{L^1},
	\end{align*}
	and taking the infimum over the couplings $(\bm{v}, \bm{w})$ we get
	\begin{align} \label{eq:p1-2}
		|p^{(1)} - p^{(2)}| \leq \lipschitz{\mathcal{L}} \wass[1]{G(\bm{x})}{f_{\bm{\theta}}(\bm{x})} \leq \lipschitz{\mathcal{L}} c_1 \frac{1}{\sqrt{n_{min}}},
	\end{align}
	where for the last inequality we used a compact version of the result in \Cref{thm:priornngp}. Recalling that $c_1$ and $\lipschitz{\mathcal{L}}$ depend only on $\bm{x}, \bm{\varphi}, \bm{\sigma}, \sigmay^2$ and $n_L$, \cref{eq:p1-2} implies that, for $n_{min}$ sufficiently large, $p^{(2)}$  is also strictly positive. \\
	To conclude it is sufficient to note that \cref{eq:wasspostcor}, together with \Cref{thm:priornngp} and the observations made about $(p^{(i)})_{i = 1}^4$ lead to
	\begin{align*}
		\wass[1]{f_{\bm{\theta}}(\bm{x}) \given \train}{G(\bm{x}) \given \train} & \leq c_2 \wass[2]{f_{\bm{\theta}}(\bm{x})}{G(\bm{x})} \leq c_2 c_3 \sqrt{n_L} \sum_{l = 1}^{L} \frac{1}{\sqrt{n_k}} \leq c \frac{1}{\sqrt{n_{min}}},
	\end{align*}
	where $c_2$ and $c_3$ depends on $\train, \bm{\varphi}, \bm{\sigma}, \sigmay^2, n_L$.
\end{proof}

\begin{lemma} \label{lem:priornngpsigma}
	% = splitted in 2 lines
	% edit: colt template
	Given $f_{\bm{\theta}}$, $G$ and $\bm{x}$ as in \Cref{thm:priornngp}, $p \geq 1$, and assuming $\sigma^2 \coloneqq \sigma_{\bm{W}^{(L)}}^2 =$ $= \sigma_{\bm{b}^{(L)}}^2 = \sigmay^2$, there exists a constant
	\begin{equation*}
		c\left(p, \bm{x}, \bm{\varphi}, (\bm{\sigma}_l)_{l = 1}^{L - 1}\right) > 0, \text{ independent of } \left(n_l\right)_{l = 1}^L, \sigma^2,
	\end{equation*}
	such that
	\begin{equation*}
		\wass[p]{f_{\bm{\theta}}(\bm{x}) \given \sigma^2}{G(\bm{x}) \given \sigma^2} \leq \sigma c \sqrt{n_L} \sum_{l = 1}^{L - 1} \frac{1}{\sqrt{n_l}},
	\end{equation*}
\end{lemma}
\begin{proof}
	% edit: colt template
	% = splitted in 2 lines
	The proof is straightforward if we observe that, by construction (see \cref{eq:nn,eq:nngpx}) $f_{\bm{\theta}} = \sigma f_{\bm{\theta}'}$ and $G = \sigma  G'$, with 
	\begin{equation} \label{eq:thetaprime}
		\bm{\theta}' \text{ such that } \bm{\sigma}' = \left(\left(\left(\sigma_{\bm{W}^{(l)}}^2, \sigma_{\bm{b}^{(l)}}^2\right)\right)_{l = 1}^{L - 1}, (1, 1)\right),
	\end{equation}
	and $G'$ built with weights and bias variances as in $\bm{\sigma}'$. \\
	Indeed, applying \cref{eq:wasslinear} and \Cref{thm:priornngp} we get
	\begin{equation*}
		\wass[p]{f_{\bm{\theta}}(\bm{x}) \given \sigma^2}{G(\bm{x}) \given \sigma^2} =\sigma \wass[p]{f_{\bm{\theta}}'(\bm{x})}{G'(\bm{x})} \leq \sigma c \sqrt{n_L} \sum_{l = 1}^{L - 1} \frac{1}{\sqrt{n_L}}, 
	\end{equation*}
	with $c$ independent of $\sigma^2$.
\end{proof}

\begin{lemma} \label{lem:boundexplikelihood}
	% = splitted in 2 lines
	% edit: colt template
	Given $f_{\bm{\theta}}$, $G$, $\bm{x}$, $\bm{y}_{\mathcal{D}}$ and $\mathcal{L}$ as in \Cref{cor:convposterior}, assuming $\sigma^2 \coloneqq \sigma_{\bm{W}^{(L)}}^2 = \sigma_{\bm{b}^{(L)}}^2 =$ $= \sigmay^2$, $\bm{K}'(\bm{x}) \in \operatorname{Sym}_+^{k}$ (rescaled NNGP kernel), and 
	\begin{equation*}
		\bm{v} \sim G(\bm{x}) \given \sigma^2, \, \bm{w} \sim f_{\bm{\theta}}(\bm{x}) \given \sigma^2,
	\end{equation*}
	$\forall \epsilon < 1/\norm{\bm{K}'(\bm{x})}_{\mathrm{op}}$ it holds
	\begin{equation*}
		% edit: colt template
		% \setmuskip{0.95}
		\begin{aligned}
			c_1 \cdot \left(\sigma^2\right)^{- n_L k / 2} \exp{-\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{\sigma^2}} \leq \ & \mean{\mathcal{L}(\bm{v}; \bm{y}_{\mathcal{D}})} \leq c_2 \cdot \left(\sigma^2\right)^{- n_L k / 2} \exp{-\frac{1}{2 \sigma^2} \frac{\epsilon}{\epsilon + 1} \norm{\bm{y}_{\mathcal{D}}}_F^2}, \\
			c_3 \exp{-\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{\sigma^2}} \leq \ & \mean{\mathcal{L}(\bm{w}; \bm{y}_{\mathcal{D}})} \leq c_4 \cdot \left(\sigma^2\right)^{- n_L k / 2},
		\end{aligned}
	\end{equation*}
	where $n_{min} = \min_{l = 1, \dots, L - 1} n_l$ is sufficiently large and the constants depend on $\bm{x}, \bm{\varphi}$, $\left(\bm{\sigma}_l\right)_{l = 1}^{L - 1}, n_L$.
\end{lemma}
\begin{proof}
	We separately discuss the four bounds.
	
	\textbf{Bounds related to $G(\bm{x})$.}	It is possible to rewrite $\mean{\mathcal{L}(\bm{v}; \bm{y}_{\mathcal{D}})}$ as follows:
	\begin{align} \label{eq:nngpexplikelihood}
		\mean{\mathcal{L}(\bm{v}; \bm{y}_{\mathcal{D}})} & \propto \mean{\left(\sigma^2\right)^{- n_L k / 2} \exp{-\frac{1}{2\sigma^2} \norm{\bm{y}_{\mathcal{D}} - \bm{v}}_F^2}} = \nonumber \\
		& = \left(\sigma^2\right)^{- n_L k / 2} \int_{\real^{n_L \times k}} \exp{-\frac{1}{2\sigma^2} \norm{\bm{y}_{\mathcal{D}} - \bm{v}}_F^2} d \mathbb{P}_{G(\bm{x}) | \sigma^2}(\bm{v}).
	\end{align}
	Starting with the density of the Gaussian variable $G(\bm{x})$,
	\begin{equation*}
		\begin{aligned}	
			p_{G(\bm{x})}(\bm{z}) & = \frac{1}{\left((2\pi)^{n_L k} \det{\bm{K}(\bm{x}) \otimes_K \bm{I}_{n_L}}\right)^{1/2}} \cdot \\
			& \blankeq \cdot \exp{-\frac{1}{2} \flatten{\bm{z}}^T \left(\bm{K}(\bm{x}) \otimes_K \bm{I}_{n_L} \right)^{-1} \flatten{\bm{z}}},
		\end{aligned}
	\end{equation*}
	and recalling that, $\forall \bm{A} \in \real^{n \times n}, \bm{B} \in \real^{k \times k}$, then $(\bm{A} \otimes_K \bm{B})^{-1} = \bm{A}^{-1} \otimes_K \bm{B}^{-1}$, $\det{\bm{A} \otimes_K \bm{B}} =$ $= \det{\bm{A}}^k \det{\bm{B}}^n$, and $\bm{K}(\bm{x}) = \sigma^2 \bm{K}'(\bm{x})$, with $\bm{K}'(\bm{x})$ as in \cref{eq:rescaledkernel}, it is easy to see that 
	\begin{equation} \label{eq:densitynngp}
		\begin{aligned}	
			p_{G(\bm{x}) | \sigma^2}(\bm{v}) = \frac{1}{(2 \pi \sigma^2)^{n_L k / 2}} \frac{1}{\det{\bm{K}'(\bm{x})}^{n_L / 2}} \exp{-\frac{1}{2 \sigma^2} \sum_{i, j \in [k] \times [k]} \left(\bm{K}'(\bm{x})^{-1}\right)_{i, j} \bm{v}_i^T \bm{v}_j}.
		\end{aligned}
	\end{equation}

	\textbf{Lower bound.} Substituting \cref{eq:densitynngp} in \cref{eq:nngpexplikelihood} we get
	\begin{align} 
		& \begin{aligned} \label{eq:nngpexplikprop}
			\mean{\mathcal{L}(\bm{v}; \bm{y}_{\mathcal{D}})} & \propto \left(\sigma^2\right)^{- n_L k} \int_{\real^{n_L \times k}} \exp{-\frac{1}{2\sigma^2} \norm{\bm{y}_{\mathcal{D}} - \bm{v}}_F^2} \cdot \\
			& \blankeq \cdot \exp{-\frac{1}{2\sigma^2} \sum_{i, j \in [k] \times [k]} \left(\bm{K}'(\bm{x})^{-1}\right)_{i, j} \bm{v}_i^T \bm{v}_j} d\bm{v} \geq 
		\end{aligned} \\
		& \kern49.75pt \begin{aligned} \label{eq:nngpexplikgeq}
			& \geq \left(\sigma^2\right)^{- n_L k} \exp{-\frac{1}{\sigma^2} \norm{\bm{y}_{\mathcal{D}}}_F^2} \left(\sigma^2\right)^{n_L k /2} \cdot \\
			& \blankeq \cdot \int_{\real^{n_L \times k}} \exp{-\frac{1}{2} \left(\sum_{i, j \in [k] \times [k]} \left(\bm{K}'(\bm{x})^{-1} + 2 \cdot \bm{I}_k\right)_{i, j} \bm{u}_i^T \bm{u}_j\right)} d\bm{u},
		\end{aligned}
	\end{align}
	where the inequality follows applying $\norm{\bm{y}_{\mathcal{D}} - \bm{v}}_F^2 \leq 2(\norm{\bm{y}_{\mathcal{D}}}_F^2 + \norm{\bm{v}}_F^2)$ (see \cref{eq:ineqnormsqleq}) and performing the change of variable $\bm{u} \coloneqq \bm{v} / (\sigma^2)^{1/2}$. 
	The conclusion for the lower bound follows easily observing that $\bm{K}'(\bm{x})$ is positive definite, so it holds 
	\begin{equation*}
		(\bm{K}'(\bm{x})^{-1} + 2 \cdot \bm{I}_k) \otimes_K \bm{I}_{n_L} \in \operatorname{Sym}_+^{n_L k}, 
	\end{equation*}
	and therefore given $\bm{u}_{\mathrm{f}} = \flatten{\bm{u}}$ the integral in \cref{eq:nngpexplikgeq} is equal to a constant depending on $\bm{x}, \bm{\varphi}, \left(\bm{\sigma}_l\right)_{l = 1}^{L - 1}, n_L$: 
	\begin{equation} \label{eq:boundinfthetaprime}
		\begin{aligned}
			& \int_{\real^{n_L \times k}} \exp{-\frac{1}{2} (\bm{u}_{\mathrm{f}}^T \left((\bm{K}'(\bm{x})^{-1} + 2 \cdot \bm{I}_k) \otimes_K \bm{I}_{n_L}\right) \bm{u}_{\mathrm{f}})} d\bm{u}_{\mathrm{f}} = \\
			& \kern20pt = \left((2\pi)^{k} \det{\bm{K}'(\bm{x})^{-1} + 2 \cdot \bm{I}_k}\right)^{n_L / 2}.
		\end{aligned}
	\end{equation}   
	
	\textbf{Upper bound.} For the upper bound the procedure is similar.
	Starting from the result in \cref{eq:nngpexplikprop}, applying the inequality $\norm{\bm{y}_{\mathcal{D}} - \bm{v}}_F^2 \geq \frac{\epsilon}{1 + \epsilon} \norm{\bm{y}_{\mathcal{D}}}_F^2 - \epsilon \norm{\bm{v}}_F^2$, for a fixed $\epsilon > 0$ (see \cref{eq:ineqnormsqgeqeps}) and performing again the change of variable $\bm{u} \coloneqq \bm{v} / (\sigma^2)^{1/2}$ we get
	\begin{align*}
		\mean{\mathcal{L}(\bm{v}; \bm{y}_{\mathcal{D}})} & \leq c \cdot \left(\sigma^2\right)^{- n_L k} \exp{-\frac{1}{2 \sigma^2} \frac{\epsilon}{\epsilon + 1} \norm{\bm{y}_{\mathcal{D}}}_F^2} \left(\sigma^2\right)^{n_L k /2} \cdot \\
		& \blankeq \cdot \int_{\real^{n_L \times k}} \exp{-\frac{1}{2} \sum_{i, j \in [k] \times [k]} \left(\bm{K}'(\bm{x})^{-1} - \epsilon \bm{I}_k\right)_{i, j} \bm{v}_i^T \bm{v}_j} d\bm{v}.
	\end{align*}
	Now, in order to have a convergent integral it is sufficient to impose the matrix $\bm{K}'(\bm{x})^{-1} - \epsilon \bm{I}_k$ to be positive definite. Indeed, in that case, we could conclude as before. 
	However, this condition is obviously verified 
	\begin{equation*}
		\forall \epsilon < \min\left\{\lambda \given \lambda \in \operatorname{Sp}\left(\bm{K}'(\bm{x})^{-1}\right)\right\} = \max\left\{\lambda \given \lambda \in \operatorname{Sp}\left(\bm{K}'(\bm{x})\right)\right\}^{-1} = 1/\norm{\bm{K}'(\bm{x})}_{\mathrm{op}},
	\end{equation*}
	indeed for such $\epsilon$ one has that 
	\begin{equation*}
		\operatorname{Sp}\left(\bm{K}'(\bm{x})^{-1} - \epsilon \bm{I}_k\right) = \operatorname{Sp}\left(\bm{K}'(\bm{x})^{-1}\right) - \epsilon \subset \real^+.
	\end{equation*}
	
	\textbf{Bounds related to $f_{\bm{\theta}}(\bm{x})$.} As for the Gaussian process we can write 
	\begin{equation} \label{eq:nnexplikelihood}
		\mean{\mathcal{L}(\bm{w}; \bm{y}_{\mathcal{D}})} \propto \left(\sigma^2\right)^{- n_L k / 2} \int_{\real^{n_L \times k}} \exp{-\frac{1}{2\sigma^2} \norm{\bm{y}_{\mathcal{D}} - \bm{w}}_F^2} d \mathbb{P}_{f_{\bm{\theta}}(\bm{x}) | \sigma^2}(\bm{w}).
	\end{equation}
	We also recall the definitions of $f_{\bm{\theta}'} = (\sigma^2)^{-1/2} f_{\bm{\theta}}$ and $G' = (\sigma^2)^{-1/2} G$, respectively the rescaled BNN and NNGP, random processes independent on $\sigma^2$, as in \cref{eq:thetaprime}.

	\textbf{Lower bound.} Applying the same inequality and the same change of variable used in the lower bound related to $G(\bm{x})$ we get 
	\begin{align} \label{eq:nnexplikgeq}
		\mean{\mathcal{L}(\bm{w}; \bm{y}_{\mathcal{D}})} & \geq c \exp{-\frac{1}{\sigma^2} \norm{\bm{y}_{\mathcal{D}}}_F^2} \int_{\real^{n_L \times k}} \exp{-\norm{\bm{u}}_F^2} d \mathbb{P}_{f_{\bm{\theta}}'(\bm{x})}(\bm{u}) \nonumber = \\
		& = c \exp{-\frac{1}{\sigma^2} \norm{\bm{y}_{\mathcal{D}}}_F^2} \mean[\bm{u} \sim f_{\bm{\theta}}'(\bm{x})]{e^{-\norm{\bm{u}}_F^2}}.
	\end{align}
	Now, we can exploit the fact that we know how to integrate $e^{-\norm{\bm{\cdot}}_F^2}$ with respect to the measure $\mathbb{P}_{G'(\bm{x})}$ (we already computed this integral up to a constant depending on the usual parameters, in \cref{eq:boundinfthetaprime}) to obtain analogous results for the mean value in \cref{eq:nnexplikgeq}. 
	As in \cref{eq:p1-2}, exploiting \Cref{thm:priornngp}, and observing that
	\begin{align*}
		\lipschitz{e^{-\norm{\bm{\cdot}}_F^2}} & = \max_{\bm{u} \in \real^{n_L \times k}} \norm{\frac{\partial}{\partial \bm{u}} e^{-\norm{\bm{u}}_F^2}}_F = \max_{\bm{u} \in \real^{n_L \times k}} \norm{-2 \bm{u} e^{-\norm{\bm{u}}_F^2}}_F = \sqrt{\frac{2}{e}},
	\end{align*}
	it is possible to write the following upper bound to the difference of the mean values of $e^{-\norm{\bm{\cdot}}_F^2}$ with respect to the laws of $G'(\bm{x})$ and $f_{\bm{\theta}'}(\bm{x})$:
	\begin{equation} \label{eq:boundmeanexpscaled}
		\begin{aligned}
			& \left|\mean[\bm{u} \sim G'(\bm{x})]{e^{-\norm{\bm{u}}_F^2}} - \mean[\bm{u} \sim f_{\bm{\theta}}'(\bm{x})]{e^{-\norm{\bm{u}}_F^2}}\right| \leq \lipschitz{e^{-\norm{\bm{\cdot}}_F^2}} \wass[1]{G'(\bm{x})}{f_{\bm{\theta}'}(\bm{x})} \leq \frac{c}{\sqrt{n_{min}}}.
		\end{aligned}
	\end{equation}
	Therefore, assuming 
	\begin{equation*}
		\sqrt{n_{min}} \geq 2c / \mean[\bm{u} \sim G'(\bm{x})]{e^{-\norm{\bm{u}}_F^2}},
	\end{equation*}
	we get 
	\begin{equation*}
		\mean[\bm{u} \sim f_{\bm{\theta}}'(\bm{x})]{e^{-\norm{\bm{u}}_F^2}} \geq \mean[\bm{u} \sim G'(\bm{x})]{e^{-\norm{\bm{u}}_F^2}} / 2,
	\end{equation*}
	and therefore
	\begin{equation*}
		\mean{\mathcal{L}(\bm{w}; \bm{y}_{\mathcal{D}})} \geq c \exp{-\frac{1}{\sigma^2} \norm{\bm{y}_{\mathcal{D}}}_F^2}.
	\end{equation*}

	\textbf{Upper bound.} The upper bound can easily be obtained observing that the negative exponential in \cref{eq:nnexplikelihood} is smaller than $1$ and therefore its integral is smaller than $1$ as well\footnote{To find a sharper upper bound reproducing the result just showed for the lower bound is not trivial. We cannot apply an analogue of \cref{eq:boundmeanexpscaled} because the map $e^{\frac{\epsilon}{2} \norm{\bm{\cdot}}_F^2}$ is not Lipschitz.}.
\end{proof}

It is now possible to state and prove the following \Cref{cor:convposteriorsigma}, which is a version of \Cref{cor:convposterior} in which the dependence on $\sigma^2$ is explicit, under the assumptions of BNN built using the hierarchical model defined in \cref{eq:hiermodel}.

\begin{corollary} \label{cor:convposteriorsigma}
	% = splitted in 2 lines
	% edit: colt template
	% = splitted in 2 lines
	Given $f_{\bm{\theta}}$, $G$, $\bm{x}$, $\bm{y}_{\mathcal{D}}$ and $\mathcal{L}$ as in \Cref{cor:convposterior}, and assuming $\sigma^2 \coloneqq \sigma_{\bm{W}^{(L)}}^2 =$ $= \sigma_{\bm{b}^{(L)}}^2 = \sigmay^2$ exist some constants
	\begin{equation*}
		c_i\left(\train, \bm{\varphi}, (\bm{\sigma}_l)_{l = 1}^{L - 1}, n_L\right) > 0, \,i = 0, \dots, 4, \text{ independent of } \left(n_l\right)_{l = 1}^{L - 1}, \sigma^2,
	\end{equation*}
	such that,
	\begin{equation*}
		\wass[1]{f_{\bm{\theta}}(\bm{x}) \given (\train, \sigma^2)}{G(\bm{x}) \given (\train, \sigma^2)} \leq h(\sigma^2) \frac{c_0}{\sqrt{n_{min}}},
	\end{equation*}
	with
	\begin{equation*}
		h(\sigma^2) = c_1 \left(\sigma^2\right)^{1/2} \exp{\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{\sigma^2}} \left(c_2 + c_3 \left(\sigma^2\right)^{1/2} + c_4 \left(\sigma^2\right)^{- n_L k / 2} \exp{\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{\sigma^2}}\right).
	\end{equation*}
\end{corollary}
\begin{proof}
	In analogy with the proof of \Cref{cor:convposterior}, the idea is to apply \Cref{lem:nnposterior} with
	\begin{equation*}
		\widetilde{\mu} \sim f_{\bm{\theta}}(\bm{x}) \given \sigma^2, \, \mu \sim G(\bm{x}) \given \sigma^2, \, g \coloneqq \mathcal{L}(\bm{z}; \bm{y}_{\mathcal{D}}, \sigma^2),
	\end{equation*}
	so that
	\begin{equation*}
		\widetilde{\mu}_g = \frac{g}{\widetilde{\mu}(g)} \widetilde{\mu} = \mathbb{P}_{f_{\bm{\theta}}(\bm{x}) \given (\sigma^2, \train)} \quad \text{and} \quad \mu_g = \frac{g}{\mu(g)} \mu = \mathbb{P}_{G(\bm{x}) \given (\sigma^2, \train)}.
	\end{equation*}
	By a direct application of \cref{eq:wassposterior} we get
	\begin{equation} \label{eq:nngpsigma1}
		\begin{aligned}
			& \wass[1]{f_{\bm{\theta}}(\bm{x}) \given (\sigma^2, \train)}{G(\bm{x}) \given (\sigma^2, \train)} \leq \\
			& \kern20pt \leq \frac{1}{p^{(1)}} \left(\lipschitz{\mathcal{L}} p^{(3)} + \left(1 + \frac{p^{(4)} \lipschitz{\mathcal{L}}}{p^{(2)}}\right) \norm{\mathcal{L}}_{\infty}\right) \wass[2]{f_{\bm{\theta}}(\bm{x}) \given \sigma^2}{G(\bm{x}) \given \sigma^2},
		\end{aligned}
	\end{equation}
	where $\norm{\mathcal{L}}_{\infty}$, $\lipschitz{\mathcal{L}}$ are reported explicitly in \Cref{prop:likelihood}, whereas $1 / p^{(1)}$, $1 / p^{(2)}$ and $p^{(3)}$, $p^{(4)}$ are upper bounded respectively in \Cref{lem:boundexplikelihood} and \cref{eq:p3-4}: all the constants depends on $\bm{x}, \bm{\varphi}, (\bm{\sigma}_l)_{l = 1}^{L - 1}$ and $n_L$
	\begin{equation} \label{eq:p1-4final}
		\begin{gathered}
			\norm{\mathcal{L}}_{\infty} = c \, \left(\sigma^2\right)^{- n_L k / 2}, \ \lipschitz{\mathcal{L}} = c \, \left(\sigma^2\right)^{-n_L k / 2 - 1/2}, \\
			p^{(1)} \geq c \, \left(\sigma^2\right)^{- n_L k / 2} \exp{-\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{\sigma^2}}, \ p^{(2)} \geq c \, \exp{-\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{\sigma^2}}, \\
			p^{(3)} \leq c \, \sigma^2, \ p^{(4)} \leq c \, \left(\sigma^2\right)^{1/2}.
		\end{gathered}
	\end{equation}
	Hence, substituting the results in \cref{eq:p1-4final} inside \cref{eq:nngpsigma1} and applying \Cref{lem:priornngpsigma} we obtain
	\begin{equation*}
		\begin{gathered}
			\wass[1]{f_{\bm{\theta}}(\bm{x}) \given (\sigma^2, \train)}{G(\bm{x}) \given (\sigma^2, \train)} \leq h(\sigma^2) \frac{c}{\sqrt{n_{min}}}, \text{ with} \\
			\begin{aligned}
				h(\sigma^2) & = \left(\sigma^2\right)^{1/2} \left(\sigma^2\right)^{n_L k / 2} \exp{\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{\sigma^2}} \Bigg[c_1 \left(\sigma^2\right)^{- n_L k / 2 + 1/2} + \\
				& \blankeq + \left(1 + c_2 \left(\sigma^2\right)^{- n_L k / 2} \exp{\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{\sigma^2}} \right) c_3 \left(\sigma^2\right)^{- n_L k / 2}\Bigg] \leq \\
				& \leq c_1 \left(\sigma^2\right)^{1/2} \exp{\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{\sigma^2}} \left(c_2 + c_3 \left(\sigma^2\right)^{1/2} + c_4 \left(\sigma^2\right)^{- n_L k / 2} \exp{\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{\sigma^2}}\right).
			\end{aligned}
		\end{gathered}
	\end{equation*}
\end{proof}

\subsection{\texorpdfstring{1\textsuperscript{st}}{First} term bound} \label{subsec:firstterm}

As we already mentioned, in order to control the first term we need first to apply the convexity property presented in \cref{eq:wassconvex} to $\mu_{\mathrm{post}}$ and $\bar{\mu}$ defined respectively in \cref{eq:mupost,eq:barmu}.
Doing that we would have
\begin{equation} \label{eq:wassfirstterm}
	\wass[1]{\mu_{\mathrm{post}}}{\bar{\mu}} \leq \int_{\real^+} \wass[1]{\mu_{\sigma^2}(s)}{\widetilde{\mu}_{\sigma^2}(s)} \frac{\mathit{I}_{\sigma^2}(s)}{\mathit{I}} p_{\sigma^2}(s) ds
\end{equation}
where the probability measure $\nu$ in \cref{eq:wassconvex} here is simply
\begin{equation*}
	\nu \ll \lambda^+, \text{ with } \frac{d\nu}{d\lambda^+}(s) = \frac{\mathit{I}_{\sigma^2}(s)}{\mathit{I}} p_{\sigma^2}(s), \ \lambda^+ \text{ Lebesgue measure on } \real^+.
\end{equation*}
Hence, in order to get \cref{eq:wassfirstterm} we just need to prove that both $(\mu_{\sigma^2}(s))_{s \in \real^+}$ and $(\widetilde{\mu}_{\sigma^2}(s))_{s \in \real^S}$ are Markov kernels with source $\real^+$ and target $\real^{n_L \times k}$. \\
We first observe that $\forall s \in \real^+$, both $\mu_{\sigma^2}(s)$ and $\widetilde{\mu}_{\sigma^2}(s)$ are probability measures, which follows from $\mu_{\sigma^2}(s)(\real^{n_L \times k}) = \widetilde{\mu}_{\sigma^2}(s)(\real^{n_L \times k}) = 1$ and applying dominated convergence. \\
It remains only to check that for any $B \in \borel{\real^{n_L \times k}}$, $\mu_{\sigma^2}(\cdot)(B)$ and $\widetilde{\mu}_{\sigma^2}(\cdot)(B)$, are measurable from $(\real^+, \borel{\real^+})$ to $([0, 1], \borel{[0, 1]})$, which is again easy to observe:  the maps
\begin{equation*}
	s \to \int_B \mathcal{L}(\bm{z}, s) \mu(d\bm{z}), \quad s \to \int_B \mathcal{L}(\bm{z}, s) \widetilde{\mu}(d\bm{z}), \quad s \to \mathit{I}_{\sigma^2}(s), \quad s \to \widetilde{\mathit{I}}_{\sigma^2}(s) 
\end{equation*}
are continuous because Lebesgue integrals of the map $s \to \mathcal{L}(\bm{z}, s)$ with respect to $\bm{z}$, integration variable of a probability measure, and therefore we have the thesis.

\smallskip

So, the final bound on the first term can be found explicitly observing that the two probability measures $\mu_{\sigma^2}(s)$ and $\widetilde{\mu}_{\sigma^2}(s)$ parametrized by $s$, coincide with the laws of $G(\bm{x}) \given (\train, \sigma^2 = s)$ and $f_{\bm{\theta}}(\bm{x}) \given (\train, \sigma^2 = s)$. 
By \Cref{cor:convposteriorsigma}, 
\begin{equation*}
	\wass[1]{\mu_{\sigma^2}(s)}{\widetilde{\mu}_{\sigma^2}(s)} \leq h(s) \frac{c}{\sqrt{n_{min}}},
\end{equation*}
with $c$ and $h$ as in the statement of the result used, which implies
\begin{equation} \label{eq:boundfirstterm}
	\wass[1]{\mu_{\mathrm{post}}}{\bar{\mu}} \leq \frac{c}{\sqrt{n_{min}}} \int_{\real^+} h(s) \frac{\mathit{I}_{\sigma^2}(s)}{\mathit{I}} p_{\sigma^2}(s) ds.
\end{equation}
In \Cref{lem:boundexplikelihood} we already computed the bounds for $\mathit{I}_{\sigma^2}(s)$, therefore we can also bound $\mathit{I}_{\sigma^2}(s) / \mathit{I}$: observing 
\begin{equation*}
	\mathit{I}_{\sigma^2}(s) = \mean[\bm{z} \sim G(\bm{x}) | \sigma^2 = s]{\mathcal{L}(\bm{z}, s)} \quad \text{and} \quad \mathit{I} = \int_{\real^+} \mathit{I}_{\sigma^2}(s) p_{\sigma^2}(s) ds,
\end{equation*}
it is easy to check that, fixed $\epsilon < \left(\lambda_+\right)^{-1}$, with $\lambda_+ \coloneqq \max\{\lambda \given \lambda \in \operatorname{Sp}\left(\bm{K}'(\bm{x})\right)\}$, we have
\begin{align}	
	& \begin{aligned} \nonumber
		\mathit{I}_{\sigma^2}(s) & \leq c \, s^{-n_L k / 2} \exp{-\frac{1}{s} \frac{\epsilon}{2 (\epsilon + 1)} \norm{\bm{y}_{\mathcal{D}}}_F^2}, \text{ and} \\
	\end{aligned} \\
	& \begin{aligned} \label{eq:lowerboundI}
		\mathit{I} & \geq c \int_{\real^+} s^{- n_L k / 2} \exp{-\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{s}} s^{- a - 1} \exp{-\frac{b}{s}} ds = \\
		& = c \int_{\real^+} s^{n_L k / 2} \exp{- s \norm{\bm{y}_{\mathcal{D}}}_F^2} s^{a + 1} \exp{- s b} s^{-2} ds = \\
		& = c \int_{\real^+} s^{n_L k / 2 + a - 1} \exp{- s (\norm{\bm{y}_{\mathcal{D}}}_F^2 + b)} ds = c \, \Gamma\left(n_L k /  2 + a\right).
	\end{aligned}
\end{align}
Hence, the bound in \cref{eq:boundfirstterm} can be further simplified bounding the following integral:
\begin{align*}
	& \int_{\real^+} h(s) \frac{\mathit{I}_{\sigma^2}(s)}{\mathit{I}} p_{\sigma^2}(s) ds \leq \\
	& \kern20pt \begin{aligned}
		& \leq c \int_{\real^+} \left(c_1 s^{1/2} \exp{\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{s}} \left(c_2 + c_3 s^{1/2} + c_4 s^{- n_L k / 2} \exp{\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{s}}\right)\right) \cdot \\
		& \blankeq \cdot s^{- n_L k / 2} \exp{-\frac{1}{s} \frac{\epsilon}{2 (\epsilon + 1)} \norm{\bm{y}_{\mathcal{D}}}_F^2 } \cdot s^{- a - 1} \exp{-\frac{b}{s}} ds = \\
		& = c_1 \int_{\real^+} s^{a + n_L k / 2 - 1/2 - 1} \exp{-s \left(b + \left(\frac{\epsilon}{2\epsilon + 2} - 1\right) \norm{\bm{y}_{\mathcal{D}}}_F^2\right)} ds \, + \\
		& \blankeq + c_2 \int_{\real^+} s^{a + n_L k / 2 - 1 - 1} \exp{-s \left(b + \left(\frac{\epsilon}{2\epsilon + 2} - 1\right) \norm{\bm{y}_{\mathcal{D}}}_F^2\right)} ds \, + \\
		& \blankeq + c_3 \int_{\real^+} s^{a + n_L k - 1/2 - 1} \exp{-s \left(b + \left(\frac{\epsilon}{2\epsilon + 2} - 2\right) \norm{\bm{y}_{\mathcal{D}}}_F^2\right)} ds = \\
		& = c_1 \Gamma\left(a + n_L k / 2 - 1/2\right) + c_2 \Gamma\left(a + n_L k / 2 - 1\right) + c_3 \Gamma\left(a + n_L k - 1/2\right),
	\end{aligned}
\end{align*}
under the assumptions 
\begin{equation*}
	a > \frac{1}{2} \quad \text{and} \quad b > \left(1 + \frac{\epsilon + 2}{2\epsilon + 2}\right) \norm{\bm{y}_{\mathcal{D}}}_F^2.
\end{equation*}
Therefore, it holds
\begin{equation*}
	\wass[1]{\mu_{\mathrm{post}}}{\bar{\mu}} \leq \frac{c}{\sqrt{n_{min}}}.
\end{equation*}

\subsection{\texorpdfstring{2\textsuperscript{nd}}{Second} term bound} \label{subsec:secondterm}

Finally, to control the second term the idea is to apply the Theorem 6.15 of \citet{otvillani2008}, which in our case is simplified to \Cref{lem:boundwasstv}. 
We use the following characterization of the total variation measure (see Section 6.1 of \citet{rcarudin1987}): $\forall A \in \borel{\real^S}$,
\begin{equation*}
	|\mu - \nu|(A) = \sup_{\left(A_i\right)_{i = 1}^{\infty}, \ \bigsqcup_{i = 1}^{\infty} A_i = A} \sum_{i = 1}^{\infty} |(\mu - \nu)(A_i)|.
\end{equation*}
Now the problem is that we do not know how to measure maps with the finite measure $|\bar{\mu} - \widetilde{\mu}_\mathrm{post}|$, but we know how to bound it. \\
Indeed, introducing the following notation to improve the readability,
\begin{equation*}
	k(s) \coloneqq \frac{\mathit{I}_{\sigma^2}(s)}{\mathit{I}} - \frac{\widetilde{\mathit{I}}_{\sigma^2}(s)}{\widetilde{\mathit{I}}},
\end{equation*}
we have
\begin{equation*}
	\bar{\mu} - \widetilde{\mu}_{\mathrm{post}} = \int_{\real^+} k(s) \widetilde{\mu}_{\sigma^2}(s) p_{\sigma^2}(s) ds,
\end{equation*}
and therefore $\forall A \in \borel{\real^{n_L \times k}}$, 
\begin{equation*}
	\begin{aligned}
		|\bar{\mu} - \widetilde{\mu}_\mathrm{post}|(A) & = \sup_{\left(A_i\right)_{i = 1}^{\infty}, \ \bigsqcup_{i = 1}^{\infty} A_i = A} \sum_{i = 1}^{\infty} \left|\int_{\real^+} k(s) \widetilde{\mu}_{\sigma^2}(s)(A_i) p_{\sigma^2}(s) ds\right| \leq \\
		& \leq \sup_{\left(A_i\right)_{i = 1}^{\infty}, \ \bigsqcup_{i = 1}^{\infty} A_i = A} \sum_{i = 1}^{\infty} \int_{\real^+} |k(s)| \widetilde{\mu}_{\sigma^2}(s)(A_i) p_{\sigma^2}(s) ds = \\
		& = \sup_{\left(A_i\right)_{i = 1}^{\infty}, \ \bigsqcup_{i = 1}^{\infty} A_i = A} \lim_{j \to \infty} \int_{\real^+} |k(s)| \widetilde{\mu}_{\sigma^2}(s)\left(\bigsqcup_{i = 1}^j A_i\right) p_{\sigma^2}(s) ds.
	\end{aligned}
\end{equation*}
% dropped the "Now"
% edit: colt template
It is easy to observe that it is possible to apply Dominated Convergence Theorem: $\forall j \in \natural_{> 0}$,
\begin{equation*}
	\left||k(s)| \widetilde{\mu}_{\sigma^2}(s)\left(\bigsqcup_{i = 1}^j A_i\right) p_{\sigma^2}(s)\right| \leq |k(s)| p_{\sigma^2}(s) \quad \text{and} \quad \int_{\real^+} |k(s)| p_{\sigma^2}(s) \leq 2.
\end{equation*}
Therefore,
\begin{equation*}
	\begin{aligned}
		|\bar{\mu} - \widetilde{\mu}_\mathrm{post}|(A) & \leq \sup_{\left(A_i\right)_{i = 1}^{\infty}, \ \bigsqcup_{i = 1}^{\infty} A_i = A} \int_{\real^+} |k(s)| \widetilde{\mu}_{\sigma^2}(s)\left(\bigsqcup_{i = 1}^\infty A_i\right) p_{\sigma^2}(s) ds = \\
		& = \int_{\real^+} |k(s)| \widetilde{\mu}_{\sigma^2}(s)(A) p_{\sigma^2}(s) ds \eqcolon \nu(A).
	\end{aligned}
\end{equation*}
It is easy to observe that $\nu: \borel{\real^{n_L \times k}} \to \real^+$ is a finite measure, indeed:
\begin{itemize}
	\item[-] $\nu(\emptyset) = 0$, $\nu(\real^{n_L \times k}) = \int_{\real^+} |k(s)| p_{\sigma^2}(s) ds \leq 2$;
	\item[-] given $A = \bigsqcup_{i = 1}^{\infty} A_i$, again using dominated convergence we have
		\begin{equation*}
			\begin{aligned}
				\sum_{i = 1}^{\infty} \nu(A_i) & = \sum_{i = 1}^{\infty} \int_{\real^+} |k(s)| \widetilde{\mu}_{\sigma^2}(A_i) p_{\sigma^2}(s) ds = \int_{\real^+} |k(s)| \widetilde{\mu}_{\sigma^2}\left(\bigsqcup_{i = 1}^{\infty} A_i\right) p_{\sigma^2}(s) ds = \\
				& = \int_{\real^+} |k(s)| \widetilde{\mu}_{\sigma^2}(A) p_{\sigma^2}(s) ds = \nu(A).
			\end{aligned}
		\end{equation*}
\end{itemize}
Applying \Cref{lem:boundwasstv} and observing that if $\forall A \in \borel{\real^{n_L \times k}}$, $|\bar{\mu} - \widetilde{\mu}_\mathrm{post}|(A) \leq \nu(A)$ then $\forall f$ measurable from $\real^{n_L \times k}$ to $\real^+$, $|\bar{\mu} - \widetilde{\mu}_\mathrm{post}|(f) \leq \nu(f)$, we get
\begin{align} 
	\wass[1]{\bar{\mu}}{\widetilde{\mu}_{\mathrm{post}}} & \leq \int_{\real^{n_L \times k}} \norm{\bm{z}}_F d |\bar{\mu} - \widetilde{\mu}_\mathrm{post}|(\bm{z}) \leq \int_{\real^{n_L \times k}} \norm{\bm{z}}_F d \nu(\bm{z}) = \nonumber \\
	& = \int_{\real^+} |k(s)| p_{\sigma^2}(s) \int_{\real^{n_L \times k}} \norm{\bm{z}}_F \widetilde{\mu}_{\sigma^2}(s)(d \bm{z}) ds = \nonumber \\
	& = \int_{\real^+} |k(s)| p_{\sigma^2}(s) \frac{1}{\widetilde{\mathit{I}}_{\sigma^2}(s)} \int_{\real^{n_L \times k}} \norm{\bm{z}}_F \mathcal{L}(\bm{z}, s) \widetilde{\mu}(d\bm{z}) ds, \label{eq:wassbound1}
\end{align}
where the identity from the 1\textsuperscript{st} to the 2\textsuperscript{nd} line follows by Fubini's Theorem. \\
The inner integral is easy to compute bounding the likelihood $\mathcal{L}$ in terms of the variable $\bm{z}$, already computed in \Cref{prop:likelihood}: $\norm{\mathcal{L}}_{\infty} = c \, s^{-n_L k / 2}$. Using this result we obtain
\begin{align*}
	\int_{\real^{n_L \times k}} \norm{\bm{z}}_F \mathcal{L}(\bm{z}, s) \widetilde{\mu}(d\bm{z}) & = \mean[\bm{z} \sim f_{\bm{\theta}}(\bm{x}) \given \sigma^2 = s]{\norm{\bm{z}}_F \mathcal{L}(\bm{z}, s)} \leq c \, s^{-n_L k / 2} \mean[\bm{z} \sim f_{\bm{\theta}}(\bm{x}) \given \sigma^2 = s]{\norm{\bm{z}}_F}.
\end{align*}
Now the procedure to compute the first moment of the distribution of $ f_{\bm{\theta}}(\bm{x}) \given \sigma^2 = s$ is analogous to the one used in the proof of \Cref{lem:boundexplikelihood} for the lower bound related to $f_{\bm{\theta}}(\bm{x})$.
First we recall that $f_{\bm{\theta}'} = s^{-1/2} f_{\bm{\theta} \given \sigma^2 = s}$ and also $G' = s^{-1/2} (G \given \sigma^2 = s)$.
Then we apply the change of variable $\bm{u} = \bm{z} / s^{1/2}$, and we get
\begin{equation*}
	\begin{aligned}
		\mean[\bm{z} \sim f_{\bm{\theta}}(\bm{x}) \given \sigma^2 = s]{\norm{\bm{z}}_F} & = \int_{\real^{n_L \times k}} \norm{\bm{z}}_F d \mathbb{P}_{f_{\bm{\theta}}(\bm{x}) \given \sigma^2 = s}(\bm{z}) = \\
		& = \int_{\real^{n_L \times k}} \norm{\bm{u} s^{1/2}}_F s^{n_L k / 2} d \mathbb{P}_{f_{\bm{\theta}'}(\bm{x})}(\bm{\bm{u}}) = \\
		& = s^{n_L k / 2 + 1/2} \mean[\bm{u} \sim f_{\bm{\theta}'}(\bm{x})]{\norm{\bm{u}}_F}.
	\end{aligned}
\end{equation*}
Now that we removed the dependence in terms of $s$ it is possible to bound the moment of the rescaled BNN using the moment of the rescaled NNGP, which can be upper bounded as in \cref{eq:p3-4}:
\begin{equation*}
	\mean[\bm{u} \sim G'(\bm{x})]{\norm{\bm{u}}_F} \leq (n_L k)^{\frac{1}{2}} \cdot \left(\max_{i \in [k]} \mean{\norm{\varphi_L\left(G^{(L - 1)}(\bm{x}_i)\right)}_2^2} / n_L + 1\right)^{\frac{1}{2}},
\end{equation*}
with a right-hand side that is just a constant term depending on $\bm{x}, \bm{\varphi}, (\bm{\sigma}_l)_{l = 1}^{L - 1}, n_L$. \\
In order to do so we replicate an analogue of \cref{eq:boundmeanexpscaled}: thanks to the triangle inequality we know $\lipschitz{\norm{\bm{\cdot}}_F} = 1$, and applying \Cref{thm:priornngp} we get
\begin{equation*}
	\left|\mean[\bm{u} \sim G'(\bm{x})]{\norm{\bm{u}}_F} - \mean[\bm{u} \sim f_{\bm{\theta}}'(\bm{x})]{\norm{\bm{u}}_F}\right| \leq \lipschitz{\norm{\bm{\cdot}}_F} \wass[1]{G'(\bm{x})}{f_{\bm{\theta}'}(\bm{x})} \leq \frac{c}{\sqrt{n_{min}}}.
\end{equation*}
Finally, assuming $\sqrt{n_{min}} \geq 2 c / \mean[\bm{u} \sim G'(\bm{x})]{\norm{\bm{u}}_F}$, we derive
\begin{equation*}
	\mean[\bm{u} \sim f_{\bm{\theta}}'(\bm{x})]{\norm{\bm{u}}_F} \leq \frac{3}{2} \mean[\bm{u} \sim G'(\bm{x})]{\norm{\bm{u}}_F},
\end{equation*}
which implies 
\begin{equation*}
	\int_{\real^{n_L \times k}} \norm{\bm{z}}_F \mathcal{L}(\bm{z}, s) \widetilde{\mu}(d\bm{z}) \leq c s^{1/2}.
\end{equation*}
Restarting from the result in \cref{eq:wassbound1} and using the lower bound related to $f_{\bm{\theta}}(\bm{x})$ in \Cref{lem:boundexplikelihood} we get
\begin{align*}
	\wass[1]{\bar{\mu}}{\widetilde{\mu}_{\mathrm{post}}} & \leq c \int_{\real^+} |k(s)| \frac{s^{1/2}}{\widetilde{\mathit{I}}_{\sigma^2}(s)} p_{\sigma^2}(s) ds \leq \\
	& \leq c \int_{\real^+} |k(s)| \exp{\frac{\norm{\bm{y}_{\mathcal{D}}}_F^2}{s}} s^{1/2} s^{-a - 1} \exp{-\frac{b}{s}} ds. 
\end{align*}
Now it is sufficient to show 
\begin{equation} \label{eq:boundks}
	|k(s)| = \left|\frac{\mathit{I}_{\sigma^2}(s)}{\mathit{I}} - \frac{\widetilde{\mathit{I}}_{\sigma^2}(s)}{\widetilde{\mathit{I}}}\right| \leq s^{-n_L k / 2} \frac{c}{\sqrt{n_{min}}}
\end{equation}
to have the thesis. Indeed, we would have
\begin{align*}
	\wass[1]{\bar{\mu}}{\widetilde{\mu}_{\mathrm{post}}} & \leq \frac{c}{\sqrt{n_{min}}} \int_{\real^+} s^{-n_L k / 2 - a + 1/2 - 1} \exp{-\frac{1}{s} \left(b - \norm{\bm{y}_{\mathcal{D}}}_F^2\right)} ds = \nonumber \\
	& = \frac{c}{\sqrt{n_{min}}} \Gamma\left(n_L k / 2 - 1/2 + a\right) = \frac{c}{\sqrt{n_{min}}}, 
\end{align*}
under the assumption $b > \norm{\bm{y}_{\mathcal{D}}}_F^2$. \\
Let us thus prove \cref{eq:boundks}. We begin by observing that we already know how to bound the absolute difference $\big|\mathit{I}_{\sigma^2}(s) - \widetilde{\mathit{I}}_{\sigma^2}(s)\big|$, using the same arguments applied in the proof of \Cref{lem:boundexplikelihood}:
\begin{equation} \label{eq:absdiffIstildeIs}
	\begin{aligned}
		\left|\mathit{I}_{\sigma^2}(s) - \widetilde{\mathit{I}}_{\sigma^2}(s)\right| & \leq \lipschitz{\mathcal{L}(\cdot, s)} \wass[1]{G(\bm{x})}{f_{\bm{\theta}}(\bm{x})} \leq c s^{-n_L k / 2 - 1/2} \cdot s^{1/2} \frac{c}{\sqrt{n_{min}}} \leq \\
		& \leq s^{- n_L k /2} \frac{c}{\sqrt{n_{min}}}.
	\end{aligned}
\end{equation}
Therefore, it remains to show that $\mathit{I}^{-1} \leq c$ and $\widetilde{\mathit{I}}^{-1} \geq c$ for some $c$ depending only on the usual parameters.
Indeed, it would yield
\begin{equation*}
	\left|\frac{\mathit{I}_{\sigma^2}(s)}{\mathit{I}} - \frac{\widetilde{\mathit{I}}_{\sigma^2}(s)}{\widetilde{\mathit{I}}}\right| \leq c \left|\mathit{I}_{\sigma^2}(s) - \widetilde{\mathit{I}}_{\sigma^2}(s)\right| \leq s^{- n_L k /2} \frac{c}{\sqrt{n_{min}}}.
\end{equation*}
We already saw $\mathit{I} \geq c$ in \cref{eq:lowerboundI}, but in order to find an upper bound for $\widetilde{\mathit{I}}$ we also need an upper bound for $\mathit{I}$. 
The procedure to get it is analogous to the one used in \cref{eq:lowerboundI}: starting from \Cref{lem:boundexplikelihood} we bound the negative exponential with $1$, and we are done,
\begin{align*}
	\mathit{I} & \leq c \int_{\real^+} s^{-n_L k / 2} \exp{-\frac{1}{s} \frac{\epsilon}{2 (\epsilon + 1)} \norm{\bm{y}_{\mathcal{D}}}_F^2} p_{\sigma^2}(s) ds \leq c \int_{\real^+} s^{-n_L k / 2 - a - 1} \exp{-\frac{b}{s}} ds = \\
	& = c \Gamma\left(n_L k / 2 + a\right).
\end{align*}
To prove $\widetilde{\mathit{I}} \leq c$ it is sufficient to observe that 
\begin{align*}
	\left|\mathit{I} - \widetilde{\mathit{I}}\right| & = \left|\int_{\real^+} \left(\mathit{I}_{\sigma^2}(s) - \widetilde{\mathit{I}}_{\sigma^2}(s)\right) p_{\sigma^2}(s) ds\right| \leq \int_{\real^+} \left|\mathit{I}_{\sigma^2}(s) - \widetilde{\mathit{I}}_{\sigma^2}(s)\right| p_{\sigma^2}(s) ds \leq \\
	& \leq \frac{c}{\sqrt{n_{min}}} \int_{\real^+} s^{-n_L k / 2 - a - 1} \exp{-\frac{b}{s}} ds = \frac{c}{\sqrt{n_{min}}},
\end{align*}
where from the 1\textsuperscript{st} to the 2\textsuperscript{nd} line we used the inequality in \cref{eq:absdiffIstildeIs}.
Hence, considering $n_{min}$ sufficiently large to have $\sqrt{n_{min}} > 2c / \mathit{I}$, it follows $|\mathit{I} - \widetilde{\mathit{I}}| \leq \mathit{I} / 2$ which implies $\widetilde{\mathit{I}} \leq 3/2 \, \mathit{I} \leq c$.

\section{Simulations details} \label{sec:simulationsdetails}

Starting from the hierarchical model in \cref{eq:bnnhiermodel}, to sample from the posterior BNN using \Cref{alg:samplingpostbnn}, we only need to explicitly define a method for sampling from $\sigma^2 \given \bm{\theta}, \train$. 
To achieve this, it is sufficient to retrieve the kernel of its density, which can be written explicitly. 
Recalling that $\bm{x}_{\mathcal{D}}$ is assumed independent of $\bm{\theta}$ and is also obviously independent of $\sigma^2$, it holds
\begin{equation} \label{eq:conditionaldistsigmaprod}
	\begin{aligned}
		p_{\sigma^2 | \bm{\theta}, \bm{x}_{\mathcal{D}}, \bm{y}_{\mathcal{D}}}(\sigma^2) & = \frac{p_{\sigma^2, \bm{\theta}, \bm{x}_{\mathcal{D}}, \bm{y}_{\mathcal{D}}}(\sigma^2, \bm{\theta}, \bm{y}_{\mathcal{D}}, \bm{x}_{\mathcal{D}})}{p_{\bm{\theta}, \bm{x}_{\mathcal{D}}, \bm{y}_{\mathcal{D}}}(\bm{\theta}, \bm{x}_{\mathcal{D}}, \bm{y}_{\mathcal{D}})} \propto p_{\sigma^2}(\sigma^2) \, p_{\bm{\theta} \given \sigma^2}(\bm{\theta}) \, p_{\bm{x}_{\mathcal{D}}}(\bm{x}_{\mathcal{D}}) \, p_{\bm{y}_{\mathcal{D}} | \bm{\theta}, \bm{x}_{\mathcal{D}}, \sigma^2}(\bm{y}_{\mathcal{D}}) \propto \\
		& \propto p_{\sigma^2}(\sigma^2) \, p_{\bm{W}^{(L)} \given \sigma^2}\left(\bm{W}^{(L)}\right) \, p_{\bm{b}^{(L)} \given \sigma^2}\left(\bm{b}^{(L)}\right) \,  p_{\bm{y}_{\mathcal{D}} | \bm{\theta}, \bm{x}_{\mathcal{D}}, \sigma^2}(\bm{y}_{\mathcal{D}}).
	\end{aligned}
\end{equation}
Exploiting
\begin{equation*}
	f_{\bm{\theta}}(\bm{x}_{\mathcal{D}}) = \left(\sigma^2\right)^{1/2} \left(\frac{\bm{W}^{(L)}}{\left(\sigma^2\right)^{1/2}} \varphi\left(f^{(L - 1)}_{\bm{\theta}}(\bm{x}_{\mathcal{D}})\right) + \frac{\bm{b}^{(L)}}{\left(\sigma^2\right)^{1/2}}\right) \eqcolon \left(\sigma^2\right)^{1/2} f_{\bm{\theta}'}(\bm{x}_{\mathcal{D}}),
\end{equation*}
with $f_{\bm{\theta}'}(\bm{x}_{\mathcal{D}})$ independent of $\sigma^2$ (as in \cref{eq:thetaprime}), we have also
\begin{equation} \label{eq:densities}
	\begin{aligned}
		p_{\sigma^2}(\sigma^2) & \propto \left(\sigma^2\right)^{-(a + 1)} \exp{-\frac{b}{\sigma^2}}, \\
		p_{\bm{W}^{(L)} \given \sigma^2}(\bm{W}^{(L)}) & \propto \left(\sigma^2\right)^{-n_{L} n_{L - 1} / 2} \exp{-\frac{n_{L - 1}}{2 \sigma^2} \norm{\bm{W}^{(L)}}^2_F}, \\
		p_{\bm{b}^{(L)} \given \sigma^2}(\bm{b}^{(L)}) & \propto \left(\sigma^2\right)^{-n_{L}/2} \exp{-\frac{1}{2 \sigma^2} \norm{\bm{b}^{(L)}}^2_F}, \\
		p_{\bm{y}_{\mathcal{D}} | \bm{\theta}, \sigma^2}(\bm{y}_{\mathcal{D}}) & \propto \left(\sigma^2\right)^{-n_L k/ 2} \exp{-\frac{1}{2\sigma^2} \norm{\bm{y}_{\mathcal{D}} - f_{\bm{\theta}}(\bm{x}_{\mathcal{D}})}_F^2} = \\
		& = \left(\sigma^2\right)^{-n_L k / 2} \exp{-\frac{1}{2 \sigma^2} \norm{\bm{y}_{\mathcal{D}}}_F^2} \cdot \exp{-\frac{1}{2} \norm{f_{\bm{\theta}'}(\bm{x}_{\mathcal{D}})}_F^2} \cdot \\
		& \blankeq \cdot \exp{+\frac{1}{\left(\sigma^2\right)^{1/2}} \flatten{\bm{y}_{\mathcal{D}}}^T \flatten{f_{\bm{\theta}'}(\bm{x}_{\mathcal{D}})}}.
	\end{aligned}
\end{equation}
Hence, substituting the identities reported in \cref{eq:densities} inside \cref{eq:conditionaldistsigmaprod} we get
\begin{equation} \label{eq:conditionaldistsigma}
	\begin{aligned}
		p_{\sigma^2 | \bm{\theta}, \train}(\sigma^2) \propto \left(\sigma^2\right)^{-(a' + 1)} \exp{-\frac{b'}{\sigma^2}} \exp{+\frac{c'}{\left(\sigma^2\right)^{1/2}}},
	\end{aligned}
\end{equation}
with $a', b' \in \real^+$, $c' \in \real$ such that
\begin{equation*}
	\begin{aligned}
		a' & \coloneqq a + (n_{L - 1} + k + 1) n_L / 2, \\
		b' & \coloneqq b + \frac{1}{2} \left(n_{L - 1} \norm{\bm{W}^{(L)}}^2_F + \norm{\bm{b}^{(L)}}^2_F + \norm{\bm{y}_{\mathcal{D}}}_F^2\right), \\
		c' & \coloneqq \flatten{\bm{y}_{\mathcal{D}}}^T \flatten{f_{\bm{\theta}'}(\bm{x}_{\mathcal{D}})}.
	\end{aligned}
\end{equation*}
As mentioned in \Cref{sec:simulations}, we have used the companion library of \citet{ding2022} to sample from the density in \cref{eq:conditionaldistsigma} in the \texttt{Python} implementation of \Cref{alg:samplingpostbnn}. This has been possible because, given $x$ random variable with density
\begin{equation*}
	p_{x}(x) \propto x^{-(a + 1)} \exp{-\frac{b}{x}} \exp{\frac{c}{x^{1/2}}} \indic{\{x \geq 0\}},
\end{equation*}
the normalization constant is
\begin{align*}
	\int_{0}^{+\infty} p_{x}(x) &= \int_{0}^{+\infty} x^{-(a + 1)} \exp{-\frac{b}{x}} \exp{\frac{c}{x^{1/2}}} dx = \\
	& = 2 \int_{0}^{+\infty} y^{2a + 1} \exp{-b y^2} \exp{cy} dy = \\
	&= 2 (2b)^{-a} \Gamma(2a) \exp{\frac{c^2}{8b}} D_{-2a}\left(\frac{-c}{(2b)^{1/2}}\right),
\end{align*}
with $D$ being the parabolic cylinder function. Therefore, given $z \sim \mathcal{GIN}^+(2a + 1, c/2b, \sqrt{1/2b})$ (as defined by \citet[Appendix D]{ding2022}), applying the transformation $y \coloneqq x^2$, we get a random variable with the same distribution as our target $x$: $x \sim y$.  

\begin{remark}
	The simulations closely follow the theoretical framework developed in this work. 
	However, during the sampling of the posterior Student-$t$ process and BNNs, it is performed a rescaling of $\sigma^2$. This adjustment is applied where $\sigma^2$ is used as the variance of $\bm{y}_{\mathcal{D}} \given \bm{\theta}, \bm{x}_{\mathcal{D}}, \sigma^2$, in order to address numerical stability issues encountered during the sampling process described in \Cref{alg:samplingpostbnn}.
\end{remark}