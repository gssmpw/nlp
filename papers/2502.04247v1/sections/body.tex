\section{Introduction} \label{sec:intro}

Bayesian neural networks (BNNs), composed of multiple layers of interconnected neurons, have become a powerful tool in modern machine learning, enabling the modeling of complex data structures while quantifying predictive uncertainty \citep{neal1996}. Unlike neural networks (NNs), BNNs offer a solid probabilistic framework where model parameters are treated as random variables with associated probability distributions. In particular, such a framework allows for the incorporation of both prior knowledge and observed data through the prior distribution and likelihood function, respectively.

\subsection{Background and motivation}

The theoretical study of BNNs dates back to the foundational work of \citet{neal1996}, which, inspired by Bayesian principles, showed that wide shallow BNNs converge to Gaussian processes if initialized with independent Gaussian parameters. 
This result was later extended to deep BNNs  \citep{matthews2018,lee2018,basteri2022,favaro2024} as well as to alternative architectures \citep{novak2020,yang2021}, strengthening the connection between deep learning and Gaussian processes in machine learning \citep{gp2006}.

Building on this foundation, significant effort has been devoted to analyzing the posterior behavior of BNNs. Notably, several studies have examined their exact infinite-width limiting posterior distribution, establishing its asymptotic convergence to a Gaussian process \citep{hron2020,trevisan2023}. Parallel research has explored approximate posterior inference methods, including Variational Inference (VI) \citep{blundell2015} and Monte Carlo Markov Chain (MCMC) sampling \citep{izmailov2021,pezzetti2024}, providing an empirical validation of these theoretical results.

Despite the significant progresses in the study of posterior BNNs, existing work typically assumes a fixed variance for the Gaussian prior on the network parameters, a simplification that limits substantially the diversity of posterior behaviors that BNNs can capture. In this paper, we address this critical  limitation by introducing a more flexible model in which the variance itself follows a prior distribution.

\subsection{Our contribution}

Our main contribution is to show that relaxing the fixed-variance assumption in BNNs by using an Inverse-Gamma prior leads to a novel limiting behavior, while preserving the classic prior introduced by \citet{neal1996}. 
Specifically, we prove that while the prior distribution of a BNN converges to a Gaussian process in the infinite-width limit, the marginal posterior distribution converges to a Student-$t$ process. 
The relevance of this result is twofold. First, it provides a new representation of Student-$t$ processes, which have been widely studied and applied in machine learning and statistics \citep{shah2014,tracey2018,sellier2023}. 
Second, it suggests that modifying the prior distributions can yield a broader class of limiting elliptical processes \citep{fang1989,bankestad2020}, opening new research directions on the asymptotic behavior of posterior BNNs. 
This insight shows that a careful selection of prior distributions can enhance model flexibility and uncertainty quantification, offering practical benefits in Bayesian deep learning.

Our approach relies on optimal transport tools, specifically the Wasserstein metric, in order to establish convergence rates and gain control over the distances of the distributions under analysis. Building on prior works \citep{basteri2022,trevisan2023}, we extend the framework to a hierarchical Gaussian-Inverse-Gamma model. In this model, while the prior and likelihood are still assumed to follow multivariate Gaussian distributions with diagonal covariance, the variance of both the last hidden layer and the likelihood function is modeled using an Inverse-Gamma distribution.

\subsection{Outline}

In \Cref{sec:bnns}, we introduce the notation and key tools used in the proof of our main result. 
\Cref{sec:mainresult} presents the main result of the paper, while \Cref{sec:simulations} reports experimental results that serve as a sanity check for the developed theory.

\section{Preliminaries} \label{sec:bnns}

To clarify the discussion, we refer the reader to \Cref{subsec:tensors,subsec:rv,subsec:wassdist} for a review of tensors, random variables, and optimal transport tools.

\subsection{Wasserstein distance}

For a finite set $S$, denote by $\mu$ and $\nu$ two probability measures defined on $(\real^S, \norm{\cdot})$ with finite moment of order $p$, for some $p \geq 1$. The $p$-Wasserstein distance between $\mu$ and $\nu$ is defined as
\begin{equation*}
    \wass[p]{\mu}{\nu} \coloneqq \inf\left\{\mean{\norm{\bm{x} - \bm{y}}^p}^{1/p} \given \bm{x}, \bm{y} \text{ r.v.s with } \mathbb{P}_{\bm{x}} = \mu, \mathbb{P}_{\bm{y}} = \nu\right\},
\end{equation*}
where the infimum is taken over all the random variables $(\bm{x}, \bm{y})$, jointly defined on a probability space $(\Omega, \mathcal{A}, \mathbb{P})$, with marginal laws $\mu$ and $\nu$. 
The random variable $(\bm{x}, \bm{y})$ is referred to as a coupling of $(\mu, \nu)$ and its law, $\gamma$, is called transport plan. We introduce the following abuse of notation: if $\bm{x} \sim \mu$ and $\bm{y} \sim \nu$, $\wass[p]{\bm{x}}{\bm{y}} \coloneqq \wass[p]{\mu}{\nu}$. 

% extended version
% edit: colt template
\begin{theorem}[Theorem 6.9 of \citet{otvillani2008}] \label{thm:wassdist}
    Given $(\bm{x}_n)_{n = 1}^{\infty}$, $\bm{x}$ random variables, then $\lim_{n \to \infty} \wass[p]{\bm{x}_n}{\bm{x}} = 0$ if and only if $\bm{x}_n \xrightarrow{law} \bm{x}$, and $\lim_{n \to \infty} \mean{\norm{\bm{x}_n}^p} = \mean{\norm{\bm{x}}^p}$.
\end{theorem}

\subsection{BNNs}

Consider a supervised learning framework in a regression setting, with a given training dataset $\train \coloneqq \left\{\left(\bm{x}_{\mathcal{D}, i}, \bm{y}_{\mathcal{D}, i}\right)\right\}_{i = 1}^k$, i.e.,
\begin{equation*}
    \bm{x}_{\mathcal{D}} \coloneqq \sum_{i = 1}^k \bm{x}_{\mathcal{D}, i} \otimes \bm{e}_i \in \real^{d_{\mathrm{in}} \times k} \quad \text{and} \quad
    \bm{y}_{\mathcal{D}} \coloneqq \sum_{i = 1}^k \bm{y}_{\mathcal{D}, i} \otimes \bm{e}_i \in \real^{d_{\mathrm{out}} \times k}.
\end{equation*}

\begin{definition}[Fully connected feed-forward NN] \label{def:nn}
    A fully connected feed-forward NN is defined through an architecture $\bm{\alpha} \coloneqq (\bm{n}, \bm{\varphi})$ with:
    \begin{enumerate}
        \item $\bm{n}$ denoting the sizes of the $L + 1$ layers\footnote{An input layer, $L - 1$ hidden layers and an output layer.} (with $L \geq 2$)
        \begin{equation*}
            \bm{n} \coloneqq \big(n_0 (\eqcolon d_{\mathrm{in}}), n_1, \dots, n_{L - 1}, n_L (\eqcolon d_{\mathrm{out}})\big), \, n_l \in \natural_{> 0}, \, \forall l = 0, \dots, L;
        \end{equation*}		
        \item $\bm{\varphi}$ denoting the $L$ activation functions (applied component-wise)
        \begin{equation*}
            \bm{\varphi} \coloneqq (\varphi_1, \dots, \varphi_L), \, \varphi_l: \real \to \real, \, \forall l \in [L], \text{ with } \varphi_1(x) = x, \forall x \in \real.
        \end{equation*}
    \end{enumerate}
    In particular, $\forall \bm{x}_0 \in \real^{d_{\mathrm{in}}}$, the NN is defined as $f(\bm{x}_0) \coloneqq f^{(L)}(\bm{x}_0)$ with
	% no newline
	% edit: colt template
    \begin{equation} \label{eq:nn}
        \begin{aligned}
            & f^{(l)}: \real^{d_{\mathrm{in}}} \to \real^{n_l}, \ \forall l = 0, \dots, L, \\
            & f^{(0)}(\bm{x}_0) = \bm{x}_0, \quad f^{(l)}(\bm{x}_0) = \bm{W}^{(l)} \varphi_l\left(f^{(l - 1)}(\bm{x}_0)\right) + \bm{b}^{(l)} \text{ for } l \in [L], \\
        \end{aligned}
    \end{equation}
    where, for any $l \in [L]$, $\bm{W}^{(l)} \in \real^{n_l \times n_{l - 1}}$ and $\bm{b}^{(l)} \in \real^{n_l}$ denote weight matrices and bias vectors, respectively.
\end{definition}

BNNs exploit the power of the Bayes' rule within this supervised learning framework. 
By defining $\bm{\theta} \in \real^t$ (with $t = \sum_{l = 1}^L n_l (n_{l - 1} + 1)$) the flattened concatenation of all parameters of the NN (both weights and biases), it is possible to apply Bayes' theorem to describe the posterior distribution of a BNN:
\begin{equation*}
    \begin{aligned}
        p_{\bm{\theta} | \train}(\bm{\theta}) & = \frac{p_{\train | \bm{\theta}}(\train) \, p_{\bm{\theta}}(\bm{\theta})}{p_{\train}(\train)} 
        \propto  p_{\bm{x}_{\mathcal{D}}, \bm{y}_{\mathcal{D}} | \bm{\theta}} (\bm{x}_{\mathcal{D}}, \bm{y}_{\mathcal{D}}) \, p_{\bm{\theta}}(\bm{\theta}) 
        \propto p_{\bm{y}_{\mathcal{D}} | \bm{\theta}, \bm{x}_{\mathcal{D}}}(\bm{y}_{\mathcal{D}}) \, p_{\bm{\theta}}(\bm{\theta}),
    \end{aligned}
\end{equation*}
where we assumed that $\bm{x}_{\mathcal{D}}$ is independent of $\bm{\theta}$ and all the random variables admit a density with respect to the Lebesgue measure. In particular, if $\mathcal{L}(\bm{\theta}; \bm{y}_{\mathcal{D}}) \coloneqq p_{\bm{y}_{\mathcal{D}} | \bm{\theta}, \bm{x}_{\mathcal{D}}}(\bm{y}_{\mathcal{D}})$ is the likelihood function then
\begin{equation} \label{eq:bayesbnn}
    p_{\bm{\theta} | \train}(\bm{\theta}) \propto \mathcal{L}(\bm{\theta}; \bm{y}_{\mathcal{D}}) \, p_{\bm{\theta}}(\bm{\theta}).
\end{equation}

\begin{definition}[BNN] \label{def:bnn}
    Let $f$ be a NN (see \cref{eq:nn}) with architecture $\bm{\alpha}$. In order to define a BNN we have to put a prior distribution over $\bm{\theta}$ and a likelihood function $\mathcal{L}(\bm{\theta}; \bm{y}_{\mathcal{D}})$ for $\bm{\theta}$ associated to the training set $\train$.
\end{definition}

\begin{remark}
    Bayes' theorem, and a related notion of posterior measure, can be naturally built without the necessity of density functions.
    Let $\bm{\theta}: (\Omega, \mathcal{A}, \mathbb{P}) \to \real^S$ random variable, $S$ finite set, and an evidence $\train$ with density $\mathcal{L}(\bm{\theta}; \bm{y}_{\mathcal{D}})$, we define the posterior measure of $\bm{\theta}$ as
    \begin{equation} \label{eq:bayesbnnmeasure}
        \mathbb{P}_{\bm{\theta} | \train} \coloneqq \frac{\mathcal{L}(\cdot; \bm{y}_{\mathcal{D}})}{\int_{\real^S} \mathcal{L}(\bm{\theta}; \bm{y}_{\mathcal{D}}) d\mathbb{P}_{\bm{\theta}}(\bm{\theta})} \mathbb{P}_{\bm{\theta}},
    \end{equation}
    where, given $\nu$ measure on $(\real^S, \borel{\real^S})$ ($S$ finite set), and $f: \real^S \to \real$, we use the notation $f \nu$ to denote the measure on $(\real^S, \borel{\real^S})$ absolutely continuous with respect to $\nu$ ($f \nu \ll \nu$), with density $f$, i.e., $\forall A \in \borel{\real^S}$, $f \nu (A) = \int_{A} f(\bm{u}) d\nu(\bm{u})$. For the sake of simplicity we always work with densities (as in \cref{eq:bayesbnn}) if they are available, and we swap to the measure theoretic definition in \cref{eq:bayesbnnmeasure} otherwise.
\end{remark}

The prior distribution considered on the parameters is the Gaussian independent prior \citep{neal1996}. Specifically, given the vector of variances $\bm{\sigma} \coloneqq \smash{\big(\big(\sigma_{\bm{W}^{(l)}}^2, \sigma_{\bm{b}^{(l)}}^2\big)\big)_{l = 1}^{L}} \in \left(\real^+ \times \real^+\right)^L$, we assume that
\begin{equation} \label{eq:nealprior}
    \begin{gathered}
        \bm{W}^{(l)} \sim \normal{\bm{0}_{n_l \times n_{l - 1}}}{\sigma_{\bm{W}^{(l)}}^2 / n_{l - 1} \, \bm{I}_{n_l \times n_{l - 1}}}, \ 
        \bm{b}^{(l)} \sim \normal{\bm{0}_{n_l}}{\sigma_{\bm{b}^{(l)}}^2 \bm{I}_{n_l}}, \text{ for } l \in [L].
    \end{gathered}
\end{equation}

The focus of this paper is to study the distribution that the posterior measure of $\bm{\theta}$ induces on $f$, which requires to investigate the behavior of the induced prior. In particular, we need to retrace the well-known results which state that the asymptotic distribution of $f_{\bm{\theta}}(\bm{x}) \coloneqq \sum_{i = 1}^m f_{\bm{\theta}}(\bm{x}_i) \otimes \bm{e}_i$ converges to the neural network Gaussian process (NNGP), where $\bm{x} = \{\bm{x}_i\}_{i = 1}^m$, $m \in \natural_{> 0}$, is a generic input set.

\begin{remark} \label{rem:nncompact}
    In order to have a simpler description of the subsequent theory it is convenient to write the layers of the BNNs in a compact form: $f^{(0)}_{\bm{\theta}}: \real^{d_{\mathrm{in}} \times m} \to \real^{d_{\mathrm{in}} \times m}$, $f^{(0)}_{\bm{\theta}}(\bm{x}) = \bm{x}$ and for every $l \in [L]$, $f^{(l)}_{\bm{\theta}}: \real^{n_{l - 1} \times m} \to \real^{n_l \times m}$,
    \begin{align*}
        f^{(l)}_{\bm{\theta}}(\bm{x}) & = \sum_{i = 1}^m f^{(l)}_{\bm{\theta}}(\bm{x}_i) \otimes \bm{e}_i = \sum_{i = 1}^m \bm{W}^{(l)} \varphi_l\left(f^{(l - 1)}_{\bm{\theta}}(\bm{x}_i)\right) \otimes \bm{e}_i + \bm{b}^{(l)} \otimes \bm{1}_{m} = \\
        & = \left(\bm{W}^{(l)} \otimes \bm{I}_{m}\right) \varphi_l\left(f^{(l - 1)}_{\bm{\theta}}(\bm{x})\right) + \bm{b}^{(l)} \otimes \bm{1}_{m},
    \end{align*}
    where $\bm{W}^{(l)} \otimes \bm{I}_{m} \in \real^{(n_l \times n_{l - 1}) \times (m \times m)}$ should be thought as an element of $\real^{(n_l \times m) \times (n_{l - 1} \times m)}$. We define $f_{\bm{\theta}}(\bm{x}) \coloneqq f^{(L)}_{\bm{\theta}}(\bm{x})$.
\end{remark}

\subsection{NNGP} 

\begin{definition} \label{def:gp}
    Let $H = (H(\bm{x}_0))_{\bm{x}_0 \in \real^{d_{\mathrm{in}}}}$ be a stochastic process such that for any $\bm{x}_0 \in \real^{d_{\mathrm{in}}}$, $H(\bm{x}_0)$ is a random vector with values in $\left(\real^{d_{\mathrm{out}}}, \borel{\real^{d_{\mathrm{out}}}}\right)$. 
    Then $H$ is said to be a Gaussian process with mean function $\bm{M}: \real^{d_{\mathrm{in}}} \to \real^{d_{\mathrm{out}}}$ and covariance kernel $\bm{H}: \real^{d_{\mathrm{in}}} \times \real^{d_{\mathrm{in}}} \to \real^{d_{\mathrm{out}} \times d_{\mathrm{out}}}$, if for any $m > 0$, given $\bm{x} = (\bm{x}_i)_{i = 1}^m \in \real^{d_{\mathrm{in}} \times m}$,
    \begin{equation*}
        H(\bm{x}) \coloneqq \left(H(\bm{x}_1), \dots, H(\bm{x}_m)\right) \sim \normal{\bm{M}(\bm{x})}{\bm{H}(\bm{x})},
    \end{equation*}
    where
    \begin{equation*}
        \bm{M}(\bm{x}) = \left(\bm{M}(\bm{x}_1), \dots, \bm{M}(\bm{x}_m)\right) \in \real^{d_{\mathrm{out}} \times m},
    \end{equation*}
    and $\bm{H}(\bm{x}) \in \real^{(d_{\mathrm{out}} \times m) \times (d_{\mathrm{out}} \times m)}$ can be viewed as a block matrix with $m \times m$ blocks such that, for any $i, j \in [m]^2$, the block $(i, j)$ of $\bm{H}(\bm{x})$ is $\left(\bm{H}(\bm{x})\right)_{i, j} = \bm{H}(\bm{x}_i, \bm{x}_j) \in \real^{d_{\mathrm{out}} \times d_{\mathrm{out}}}$. 
    For such $H$ we set $H \sim \mathcal{GP}\left(\bm{M}, \bm{H}\right)$. 
\end{definition}

\begin{remark}
    In analogy with \Cref{def:gp} it is possible to define Student-$t$ processes replacing the condition on the distribution of $H(\bm{x})$ with a multivariate Student-$t$ with $a$ degrees of freedom: $H(\bm{x}) \sim \tstud{a}{\bm{M}(\bm{x})}{\bm{H}(\bm{x})}$, $a > 0$.
\end{remark}

Following \citet{matthews2018} and \citet{lee2018}, we report below the laws of the random matrices $(G^{(l)}(\bm{x}))_{l = 1}^L$ associated with the infinite-width limits of the $L$ hidden layers of a BNN, evaluated on the input set $\bm{x}$, deriving general expressions for their corresponding Gaussian processes, $(G^{(l)})_{l = 1}^L$:
\begin{equation} \label{eq:nngpx}	
    \begin{aligned}
        & G^{(0)}(\bm{x}) \coloneqq \bm{x} \in \real^{d_{\mathrm{in}} \times m} \text{ constant r.v.}, \\
        & G^{(l)}(\bm{x}) \sim \normal{\bm{0}_{n_l \times m}}{\bm{I}_{n_l} \otimes \bm{K}^{(l)}(\bm{x})}, \text{ with } \bm{K}^{(l)}(\bm{x}) \coloneqq \left(\bm{K}^{(l)}(\bm{x}_i, \bm{x}_j)\right)_{i, j \in [m] \times [m]},
    \end{aligned}
\end{equation}
and, $\forall \bm{x}_0, \bm{x}'_0 \in \real^{d_{\mathrm{in}}}$, $\forall l = 2, \dots, L$, 
\begin{equation*}
    \begin{aligned}
        & \bm{K}^{(1)}(\bm{x}_0, \bm{x}'_0) \coloneqq \sigma_{\bm{W}^{(1)}}^2 (\bm{x}_0^T \bm{x}'_0) / d_{\mathrm{in}} + \sigma_{\bm{b}^{(1)}}^2, \\ 
        & \bm{K}^{(l)}(\bm{x}_0, \bm{x}'_0) \coloneqq \sigma_{\bm{W}^{(l)}}^2 \mean{\varphi_{l}\left(G^{(l - 1)}(\bm{x}_0)_1\right) \varphi_{l}\left(G^{(l - 1)}(\bm{x}'_0)_1\right)} + \sigma_{\bm{b}^{(l)}}^2.
    \end{aligned}
\end{equation*}
Note that, $\forall l \in [L]$, $\bm{I}_{n_l} \otimes \bm{K}^{(l)}(\bm{x}) \in \real^{(n_l \times n_l) \times (m \times m)}$ should be thought reshaped, as elements of $\real^{(n_l \times m) \times (n_l \times m)}$. From \cref{eq:nngpx} it is possible to define the asymptotic Gaussian processes of each hidden layer, $l \in [L]$, as
\begin{equation} \label{eq:nngp}
    G^{(l)} = (G^{(l)}(\bm{x}_0))_{\bm{x}_0 \in \real^{d_{\mathrm{in}}}} \quad \text{and} \quad G^{(l)} \sim \mathcal{GP}\left(\bm{0}, \bm{I}_{n_l} \otimes \bm{K}^{(l)}\right).
\end{equation}
In analogy with the notation introduced for $f_{\bm{\theta}}$ we define $G(\bm{x}) \coloneqq G^{(L)}(\bm{x})$ and $\bm{K}(\bm{x}) \coloneqq \bm{K}^{(L)}(\bm{x})$. We refer to $G = (G(\bm{x}_0))_{\bm{x}_0 \in \real^{d_{\mathrm{in}}}}$ as the NNGP associated to a BNN with architecture $\bm{\alpha}$ and vector of variances $\bm{\sigma}$.

\subsection{Quantitative CLT for prior BNNs} 

\begin{theorem}[\citet{basteri2022, trevisan2023}] \label{thm:priornngp}
    % edit: colt template
    % = splitted in 2 lines
    Let $f_{\bm{\theta}}$ BNN, with architecture $\bm{\alpha} = (\bm{n}, \bm{\varphi})$, $\bm{\varphi}$ collection of Lipschitz activation functions, a prior on $\bm{\theta}$ as in \cref{eq:nealprior}, $\left(G^{(l)}\right)_{l = 1}^L$ Gaussian processes as in \cref{eq:nngp} and an input set $\bm{x} \in \real^{d_{\mathrm{in} \times m}}$. Then, $\forall l \in [L]$ exists a constant $c > 0$ independent of $\left(n_j\right)_{j = 1}^l$, such that,
    \begin{equation} \label{eq:priornngp}
        \wass[p]{f_{\bm{\theta}}^{(l)}(\bm{x})}{G^{(l)}(\bm{x})} \leq c \sqrt{n_l} \sum_{j = 1}^{l - 1} \frac{1}{\sqrt{n_j}}.
    \end{equation}
\end{theorem}

The constant $c$ in \cref{eq:priornngp} in general depends on the input set $\bm{x}$, that must be finite. We remark that quantitative functional bounds, i.e., for infinitely many inputs, have been also established by \citet{favaro2024}.

\begin{remark} \label{rem:gaussposterior}
    An immediate consequence of \Cref{thm:priornngp} (achievable applying \Cref{thm:wassdist}) is that, by letting $n_l$ grow to $\infty$, the process $f^{(l)}_{\bm{\theta}}$ associated with the $l$-th hidden layer of a BNN evaluated on $\bm{x}$ converges in distribution to the NNGP's component $G^{(l)}(\bm{x})$: given $n_{min} \coloneqq \min_{j = 1, \dots, l - 1} n_j$,
    \begin{equation*}
        f^{(l)}_{\bm{\theta}}(\bm{x}) \xrightarrow[n_{min} \to \infty]{law} \normal{\bm{0}_{n_l \times m}}{\bm{I}_{n_l} \otimes \bm{K}^{(l)}(\bm{x})}.
    \end{equation*} 
    Therefore, \Cref{thm:priornngp} yields a quantitative version of what has already been proved by \citet{matthews2018,lee2018}.	
\end{remark}

\section{\texorpdfstring{Student-$t$}{Student-t} approximation of posterior BNNs} \label{sec:mainresult}

Our goal is to extend the closeness result between the \textit{induced} prior distribution on the BNN, $f_{\bm{\theta}}$, and the corresponding NNGP, $G$, established in \Cref{thm:priornngp}, to their respective \textit{induced} posterior distributions. In particular, the main result of this section, \Cref{thm:studposterior}, provides a posterior counterpart of \Cref{thm:priornngp}.

\subsection{Law of posterior NNGP} \label{subsec:nngplaw}

It is useful to start by introducing the hierarchical model applied to the NNGP. In particular, by assuming
\begin{equation} \label{eq:hiervariancenngp}
    \begin{gathered}	
        \sigma_{\bm{W}^{(l)}}^2, \sigma_{\bm{b}^{(l)}}^2 \text{ constants, } \forall l = 1, \dots, L - 1, \\
        \sigma^2 \coloneqq \sigma_{\bm{W}^{(L)}}^2 = \sigma_{\bm{b}^{(L)}}^2 = \sigmay^2 \quad \text{and} \quad \sigma^2 \sim \invgamma{a}{b}, \text{ with } a, b > 0,
    \end{gathered}
\end{equation}
we have
\begin{equation} \label{eq:hiermodel}
    \begin{aligned}
        G(\bm{x}_{\mathcal{D}}) \given \sigma^2 & \sim \normal{\bm{0}_{n_L \times k}}{\sigma^2 \bm{I}_{n_L} \otimes \bm{K}'(\bm{x}_{\mathcal{D}})}, \\
        \sigma^2 & \sim \invgamma{a}{b}, \text{ Inverse-Gamma with } a, b > 0, \\
        \bm{y}_{\mathcal{D}} \given G(\bm{x}_{\mathcal{D}}), \sigma^2 & \sim \normal{G(\bm{x}_{\mathcal{D}})}{\sigma^2 \bm{I}_{n_L \times k}},
    \end{aligned}
\end{equation}
with
\begin{align} \label{eq:rescaledkernel}
    \bm{K}'(\bm{x}_{\mathcal{D}}) = \mean{\varphi_{L}\left(G^{(L - 1)}(\bm{x}_{\mathcal{D}})\right)^T \varphi_{L}\left(G^{(L - 1)}(\bm{x}_{\mathcal{D}})\right)} / n_L + \bm{1}_{k \times k},
\end{align}
rescaled NNGP kernel independent of $\sigma^2$. Therefore, observing that
\begin{equation*}
    p_{G(\bm{x}_{\mathcal{D}}), \sigma^2 | \train}(\bm{z}, \sigma^2) \propto p_{\bm{y}_{\mathcal{D}} | G(\bm{x}_{\mathcal{D}}), \sigma^2}(\bm{y}_{\mathcal{D}}) \, p_{G(\bm{x}_{\mathcal{D}}) | \sigma^2}(\bm{z}) \, p_{\sigma^2}(\sigma^2),
\end{equation*}
assuming $\bm{K}'(\bm{x}_{\mathcal{D}})$ to be invertible, $n_L = 1$, and applying standard tricks (see \Cref{sec:postnngp}), we obtain that
\begin{equation*}
    \begin{aligned}
        G(\bm{x}_{\mathcal{D}}) \given \sigma^2, \train & \sim \normal{\bm{y}_{\mathcal{D}} \bm{M}^{-1}}{\sigma^2 \bm{M}^{-1}}, \\
        \sigma^{2} \given \train & \sim \invgamma{a + \frac{k}{2}}{b + \frac{1}{2} \left(\bm{y}_{\mathcal{D}} \left(\bm{I}_{k} - \bm{M}^{-1}\right) (\bm{y}_{\mathcal{D}})^T\right)}.
    \end{aligned}
\end{equation*}
Hence,
\begin{equation*}
    \begin{aligned}
        G(\bm{x}_{\mathcal{D}}) \given \train \sim \tstud{2a + k}{\bm{\mu}_{\mathrm{post}}}{\bm{\Sigma}_{\mathrm{post}}},
    \end{aligned}
\end{equation*}
with
\begin{equation} \label{eq:tstudpostparams}
    \begin{aligned}
        \bm{M} & \coloneqq \bm{I}_{k} + \bm{K}'(\bm{x}_{\mathcal{D}})^{-1}, \\
        \bm{\mu}_{\mathrm{post}} & \coloneqq \bm{y}_{\mathcal{D}} \bm{M}^{-1}, \\
        \bm{\Sigma}_{\mathrm{post}} & \coloneqq \left(b + \frac{1}{2} \left(\bm{y}_{\mathcal{D}} \left(\bm{I}_{k} - \bm{M}^{-1}\right) (\bm{y}_{\mathcal{D}})^T\right)\right) \frac{2}{2a + k} \bm{M}^{-1}.
    \end{aligned}
\end{equation}

\begin{remark} \label{rem:poststudtprocess}
    In a completely analogous way, it is possible to show that, given an input test set $\bm{x}_{\mathcal{T}} \in \real^{n_0 \times k'}$, 
    \begin{equation} \label{eq:poststudtprocess}
        \begin{aligned}
            & G(\bm{x}_{\mathcal{T}}) \given \train \sim \tstud{2a + k}{\bm{\mu}'_{\mathrm{post}}}{\left(b + \frac{1}{2} \left(\bm{y}_{\mathcal{D}} \left(\bm{I}_{k} - \bm{M}^{-1}\right) (\bm{y}_{\mathcal{D}})^T\right)\right) \frac{2}{2a + k} \bm{\Sigma}'_{\mathrm{post}}}, \text{ with } \\
            & \kern20pt \bm{\mu}'_{\mathrm{post}} \coloneqq \bm{K}'(\bm{x}_{\mathcal{T}}, \bm{x}_{\mathcal{D}}) \left(\bm{K}'(\bm{x}_{\mathcal{D}}) + \sigma^2 I_{k}\right)^{-1} \bm{y}_{\mathcal{D}}, \\
            & \kern20pt \bm{\Sigma}'_{\mathrm{post}} \coloneqq \bm{K}'(\bm{x}_{\mathcal{T}}) - \bm{K}'(\bm{x}_{\mathcal{T}}, \bm{x}_{\mathcal{D}}) \left(\bm{K}'(\bm{x}_{\mathcal{D}}) + \sigma^2 I_{k}\right)^{-1} \bm{K}'(\bm{x}_{\mathcal{T}}, \bm{x}_{\mathcal{D}}). \\
        \end{aligned}
    \end{equation}
    Indeed, following the strategy adopted by \citet[eqs. (2.22) to (2.24)]{gp2006}, we can observe that
    \begin{align*}
        & G(\bm{x}_{\mathcal{T}}) \given \sigma^2, \train \sim \normal{\bm{\mu}'_{\mathrm{post}}}{\bm{\Sigma}'_{\mathrm{post}}}, \\
        & \sigma^2 \given \train \sim \invgamma{a + \frac{k}{2}}{b + \frac{1}{2} \left(\bm{y}_{\mathcal{D}} \left(\bm{I}_{k} - \bm{M}^{-1}\right) (\bm{y}_{\mathcal{D}})^T\right)},
    \end{align*}
    which in turn implies \cref{eq:poststudtprocess} by means of \Cref{lem:norminvgammastudent}.
\end{remark}

\subsection{Posterior BNNs} \label{subsec:postbnn}

We define $\widetilde{\mu} \sim f_{\bm{\theta}}(\bm{x})$, $\mu \sim G(\bm{x})$, with $\bm{x} = (\bm{x}_{\mathcal{D}}, \bm{x}_{\mathcal{T}}) \in \real^{n_0 \times (k + k')}$, fixed input set which extends the input training set with a possible input test set, and omit the dependence on $\bm{y}_{\mathcal{D}}$ in the Gaussian likelihood $\mathcal{L}$\footnote{Now $\mathcal{L}$ depends also on $s \coloneqq \sigma^2$, which is no more a parameter.}, where
\begin{equation} \label{eq:likelihood}
    \mathcal{L}: \real^{n_L \times k} \times \real^{+} \to \real, \ \mathcal{L}(\bm{z}, s) = \frac{1}{\left(2 \pi s\right)^{n_L k / 2}} \exp{-\frac{1}{2 s} \norm{\bm{y}_{\mathcal{D}} - \bm{z}}_F^2}.
\end{equation}
The objective is to bound the $1$-Wasserstein distance between the marginal posterior of the BNN, $f_{\bm{\theta}}(\bm{x})$, and the marginal posterior of the NNGP evaluated on the input set, $G(\bm{x})$. The latter can be found integrating with respect to $s$ the prior measures $\mu$ and $\widetilde{\mu}$, both multiplied by the prior density of the variance $p_{\sigma^2}(s)$ and the likelihood $\mathcal{L}(\cdot, s)$.
In formulas, we aim to find an upper bound for $\wass[1]{\widetilde{\mu}_{\mathrm{post}}}{\mu_{\mathrm{post}}}$, with $\mu_{\mathrm{post}} \sim G(\bm{x}) \given \train$, $\widetilde{\mu}_{\mathrm{post}} \sim f_{\bm{\theta}}(\bm{x}) \given \train$, probability measures defined as in the following \Cref{def:postnngpbnn}.

\begin{remark}
    The likelihood function can can be extended to the space $\real^{n_L \times (k + k')}$, by artificially making it depend on the test input set while disregarding its contribution. 
    Consequently, the entire reasoning developed below extends naturally to this more general case through a straightforward change of variables. 
    However, to maintain a simpler notation and ensure a coherent presentation, we state and prove our main result under the framework introduced in \Cref{subsec:nngplaw}, i.e., assuming $\bm{x} = \bm{x}_{\mathcal{D}}$.
\end{remark}

\begin{definition} \label{def:postnngpbnn}
    Given a BNN as in \Cref{def:bnn}, we assume to have a hierarchical model as the one described in \cref{eq:hiermodel} for the NNGP, and an analogous model for the BNN (i.e., Gaussian prior on $\bm{\theta}$ as in \cref{eq:nealprior}, prior on the variance $\sigma^2$ as in \cref{eq:hiervariancenngp} and a likelihood as in \cref{eq:likelihood}). Then, for any $A \in \borel{\real^{n_L \times k}}$, we define the posterior measures as follows: given $\mu \sim G(\bm{x})$ and $\widetilde{\mu} \sim f_{\bm{\theta}}(\bm{x})$,
    \begin{equation*}
        \begin{aligned}
            & \mu_{\mathrm{post}}(A) \coloneqq \int_A \int_{\real^+} \frac{1}{\mathit{I}} \mathcal{L}(\bm{z}, s) p_{\sigma^2}(s) ds \mu(d\bm{z}), \ \mathit{I} \coloneqq \int_{\real^{n_L \times k}} \int_{\real^+} \mathcal{L}(\bm{z}, s) p_{\sigma^2}(s) ds \mu(d\bm{z}), \\
            & \widetilde{\mu}_{\mathrm{post}}(A) \coloneqq \int_A \int_{\real^+} \frac{1}{\widetilde{\mathit{I}}} \mathcal{L}(\bm{z}, s) p_{\sigma^2}(s) ds \widetilde{\mu}(d\bm{z}), \ \widetilde{\mathit{I}} \coloneqq \int_{\real^{n_L \times k}} \int_{\real^+} \mathcal{L}(\bm{z}, s) p_{\sigma^2}(s) ds \widetilde{\mu}(d\bm{z}).
        \end{aligned}
    \end{equation*}
\end{definition}

\subsection{Main result}

Building on \Cref{def:postnngpbnn}, the main result of this work can be summarized in the following \Cref{thm:studposterior}.

\begin{theorem} \label{thm:studposterior}
    Let $f_{\bm{\theta}}$, $G$, $\bm{x}$ and $\bm{y}_{\mathcal{D}}$ as above, $\mathcal{L}$ density of a $\normal{\bm{z}}{\sigmay^2 \bm{I}_{n_L \times k}}$. Assume a common variance for the last hidden layer of the BNN and the likelihood, distributed as an Inverse-Gamma 
    \begin{equation*}
        \sigma^2 \coloneqq \sigma_{\bm{W}^{(L)}}^2 = \sigma_{\bm{b}^{(L)}}^2 = \sigmay^2, \ \sigma^2 \sim \invgamma{a}{b},
    \end{equation*}
    with
    \begin{equation} \label{eq:abconstr}
        a > \frac{1}{2}, \ b > \left(1 + \frac{\epsilon + 2}{2\epsilon + 2}\right) \norm{\bm{y}_{\mathcal{D}}}_F^2, \text{ for any } \epsilon < 1/\norm{\bm{K}'(\bm{x})}_{\mathrm{op}}. 
    \end{equation}
    Then, there exists a constant $c > 0$, independent of $\left(n_l\right)_{l = 1}^{L - 1}$, such that
    \begin{equation*}
        \wass[1]{f_{\bm{\theta}}(\bm{x}) \given \train}{G(\bm{x}) \given \train} \leq \frac{c}{\sqrt{n_{min}}}.
    \end{equation*}
\end{theorem}
\begin{proof}[Sketch of the proof] 
% edit: colt template
% \begin{proofnote}[Sketch of the proof] 
    The idea is to show the convergence of $f_{\bm{\theta}}(\bm{x}) \given \train$ to $G(\bm{x}) \given \train$ through the following steps: 
    \begin{enumerate}
        \item partially retracing the strategy introduced by \citet{trevisan2023}, we first prove that there exist some constants $c > 0$ independent of $\left(n_l\right)_{l = 1}^{L - 1}$ and $\sigma^2$, and a function $h: \real^+ \to \real^+$ independent on $\left(n_l\right)_{l = 1}^{L - 1}$ as well, such that
        \begin{equation} \label{eq:simpleconvposteriorsigma}
            \wass[1]{f_{\bm{\theta}}(\bm{x}) \given (\train, \sigma^2)}{G(\bm{x}) \given (\train, \sigma^2)} \leq h(\sigma^2) \frac{c_0}{\sqrt{n_{min}}};
        \end{equation}
		% shortened version
		% edit: colt template
        \item we try to apply the convexity property of $1$-Wasserstein distance in the following \Cref{prop:wassconvexity} (\hyperlink{proof:wassconvexity}{proof} in \Cref{subsec:wassdist}) to the families of probabilities $\left(\mathbb{P}_{f_{\bm{\theta}}(\bm{x}) \given (\train, \sigma^2)}\right)_{\sigma^2 \in \real^+}$ and $\left(\mathbb{P}_{G(\bm{x}) \given (\train, \sigma^2)}\right)_{\sigma^2 \in \real^+}$, which would lead to the thesis.
        \begin{proposition} \label{prop:wassconvexity}
            Let us consider two Markov kernels $(\mu(s))_{s \in \real^+}$, $(\widetilde{\mu}(s))_{s \in \real^+}$ with source $\real^+$ and target $\real^T$ and a measure $\nu$ on $\real^+$ ($T$ finite set).
            Defining the probability measures on $\real^T$ such that, $\forall B \in \borel{\real^T}$,
            \begin{equation*}
                \mu(B) \coloneqq \int_{\real^+} \mu(s)(B) d\nu(s), \quad \widetilde{\mu}(B) \coloneqq \int_{\real^+} \widetilde{\mu}(s)(B) d\nu(s),
            \end{equation*}
            the following convexity property for the distance $\mathcal{W}_1$ holds: 
            \begin{equation} \label{eq:wassconvex}
                \wass[1]{\mu}{\widetilde{\mu}} \leq \int_{\real^+} \wass[1]{\mu(s)}{\widetilde{\mu}(s)} d\nu(s).
            \end{equation}
        \end{proposition}
    \end{enumerate}
    Unfortunately the second step is not easy as it could seem since \Cref{prop:wassconvexity} requires two families of probability measures (in particular Markov kernels) integrated with respect to the same measure $\nu$, which is not exactly our setting. 
    Let us describe the issue before approaching the solution. For this purpose it is useful to introduce
    \begin{equation*}
        \begin{aligned}
            \mathit{I}_{\sigma^2}(s) \coloneqq \int_{\real^{n_L \times k}} \mathcal{L}(\bm{z}, s) \mu(d\bm{z}), \quad \widetilde{\mathit{I}}_{\sigma^2}(s) \coloneqq \int_{\real^{n_L \times k}} \mathcal{L}(\bm{z}, s) \widetilde{\mu}(d\bm{z}),
        \end{aligned}
    \end{equation*}
    which allow us to write, $\forall A \in \borel{\real^{n_L \times k}}$,
    \begin{equation} \label{eq:mupost}
        \begin{aligned}
            & \mu_{\mathrm{post}}(A) = \int_{\real^+} \mu_{\sigma^2}(s)(A) \frac{\mathit{I}_{\sigma^2}(s)}{\mathit{I}} p_{\sigma^2}(s) ds, \text{ with } \mu_{\sigma^2}(s)(A) \coloneqq \int_A \frac{\mathcal{L}(\bm{z}, s) \mu(d\bm{z})}{\mathit{I}_{\sigma^2}(s)}, \\
            & \widetilde{\mu}_{\mathrm{post}}(A) = \int_{\real^+} \widetilde{\mu}_{\sigma^2}(s)(A) \frac{\widetilde{\mathit{I}}_{\sigma^2}(s)}{\widetilde{\mathit{I}}} p_{\sigma^2}(s) ds, \text{ with } \widetilde{\mu}_{\sigma^2}(s)(A) \coloneqq \int_A \frac{\mathcal{L}(\bm{z}, s) \widetilde{\mu}(d\bm{z})}{\widetilde{\mathit{I}}_{\sigma^2}(s)}.
        \end{aligned}
    \end{equation}
    Eventually, we can note that the two families $(\mu_{\sigma^2}(s))_{s \in \real^+}$ and $(\widetilde{\mu}_{\sigma^2}(s))_{s \in \real^S}$, which coincide respectively with $\left(\mathbb{P}_{G(\bm{x}) \given (\train, \sigma^2)}\right)_{\sigma^2 \in \real^+}$ and $\left(\mathbb{P}_{f_{\bm{\theta}}(\bm{x}) \given (\train, \sigma^2)}\right)_{\sigma^2 \in \real^+}$, are integrated with respect to different measures.
    Hence, it is clear that to apply \cref{eq:wassconvex} it is necessary to use the triangle inequality,
    \begin{equation} \label{eq:trianstep}
        \wass[1]{\mu_{\mathrm{post}}}{\widetilde{\mu}_{\mathrm{post}}} \leq \wass[1]{\mu_{\mathrm{post}}}{\bar{\mu}} + \wass[1]{\bar{\mu}}{\widetilde{\mu}_{\mathrm{post}}},
    \end{equation}
    where we inserted a third measure,
    \begin{equation} \label{eq:barmu}
        \bar{\mu}(A) \coloneqq \int_{\real^+} \widetilde{\mu}_{\sigma^2}(s)(A) \frac{\mathit{I}_{\sigma^2}(s)}{\mathit{I}} p_{\sigma^2}(s) ds, \ \forall A \in \borel{\real^{n_L \times k}},
    \end{equation}
    specifically constructed to satisfy the hypothesis of the convexity property. \\
    Now, to conclude, we just need to control both the terms on the right-hand side of \cref{eq:trianstep}.
    \begin{itemize}[leftmargin = 2.5cm]
        \item[\textit{1\textsuperscript{st} term.}] We just apply the aforementioned convexity property getting
        \begin{equation*}
            \wass[1]{\mu_{\mathrm{post}}}{\bar{\mu}} \leq \int_{\real^+} \wass[1]{\mu_{\sigma^2}(s)}{\widetilde{\mu}_{\sigma^2}(s)} \frac{\mathit{I}_{\sigma^2}(s)}{\mathit{I}} p_{\sigma^2}(s) ds.
        \end{equation*}
        Therefore by \cref{eq:simpleconvposteriorsigma} we get 
        \begin{equation*}
            \wass[1]{\mu_{\mathrm{post}}}{\bar{\mu}} \leq \frac{c_0}{\sqrt{n_{min}}} \int_{\real^+} h(s) \frac{\mathit{I}_{\sigma^2}(s)}{\mathit{I}} p_{\sigma^2}(s) ds \leq \frac{c_1}{\sqrt{n_{min}}},
        \end{equation*}
        where, in order to bound the last integral it is necessary to introduce a constraint on $a$ and $b$, as in \cref{eq:abconstr}.
        \item[\textit{2\textsuperscript{nd} term.}] We exploit the following technical \Cref{lem:boundwasstv}, in combination with several bounds on the first moments of the considered probability measures.
        % edit: colt template
        % \vspace{-5pt} 
        \begin{lemma} \label{lem:boundwasstv}
            Let $\mu$, $\nu$ be measures on $\real^S$, then, denoting with $|\mu - \nu|$ the total variation measure, it holds
            \begin{equation*}
                \wass[1]{\mu}{\nu} \leq \int_{\real^S} \norm{\bm{u}} d |\mu - \nu|(\bm{u}).
            \end{equation*}
        \end{lemma}
    \end{itemize}
    \vskip-10pt
    % edit: colt template
    % \vskip-20pt
\end{proof}
% edit: colt template
% \end{proofnote}

% edit: colt template
% \smallskip

\begin{remark}
    All the details of the sketched part of the previous proof can be found in \Cref{sec:mainproof}: a formal statement and a proof of \cref{eq:simpleconvposteriorsigma} can be found in \Cref{subsec:convposteriorsigma}; additionally, the bounds for the two terms found using the triangle inequality (\cref{eq:trianstep}) can be found respectively in \Cref{subsec:firstterm} and \Cref{subsec:secondterm}. 
    In \Cref{fig:chartmain}, we include a chart illustrating the dependencies among the results that lead to the proof of \Cref{thm:studposterior}.
\end{remark}

\begin{figure}[H]
	\begin{centering}
		\resizebox{\textwidth}{!}{
		\begin{tikzpicture}
			% row 4
			\hypersetup{colorlinks = true, linkcolor = black}
			\node[state] (main) at (0,0) {Main result (\Cref{thm:studposterior})};
			% row 3
			\node[state] (secondbound) [above = of main] {2\textsuperscript{nd} term bound (App. \ref{subsec:secondterm})};
			\hypersetup{colorlinks = true, linkcolor = blue}
			\node[state] (firstbound) [left = of secondbound, draw = blue, thick] {\textcolor{blue}{1\textsuperscript{st} term bound (App. \ref{subsec:firstterm})}};
			\hypersetup{colorlinks = true, linkcolor = black} 
			\hypersetup{colorlinks = true, linkcolor = red}
			\node[state] (convposteriorsigma) [right = of secondbound, draw = red, thick] {\textcolor{red}{\Cref{cor:convposteriorsigma}}};
			\hypersetup{colorlinks = true, linkcolor = black} 
			\path (firstbound) edge[bend right = 15] (main);
			\path (secondbound) edge[] (main);
			\path (convposteriorsigma) edge[shorten >= 4pt, bend left = 15] (main);
			% row 2
			\node[state] (priornngpsigma) [above = of convposteriorsigma] {\Cref{lem:priornngpsigma}};
			\node[state] (boundexplikelihood) [left = of priornngpsigma] {\Cref{lem:boundexplikelihood}};
			\node[state] (boundwasstv) [left = of boundexplikelihood] {\Cref{lem:boundwasstv}};
			\hypersetup{colorlinks = true, linkcolor = red}
			\node[state] (convposterior) [right = of priornngpsigma, draw = red, thick] {\textcolor{red}{\Cref{cor:convposterior}}};
			\hypersetup{colorlinks = true, linkcolor = black}
			\path (boundwasstv) edge[bend right = 15] (secondbound);
			\path (boundexplikelihood) edge[bend right = 15] (convposteriorsigma);
			\path (priornngpsigma) edge[] (convposteriorsigma);
			% row 1
			\node[state] (priornngp) [above = of priornngpsigma] {\Cref{thm:priornngp}};
			\node[state] (likelihood) [above = of convposterior] {\Cref{prop:likelihood}};
			\node[state] (wassmult) [left = of priornngp] {\Cref{prop:wassmult}};
			\path (wassmult) edge[bend right = 10] (priornngpsigma);
			\path (priornngp) edge[] (priornngpsigma);
			\path (priornngp) edge[bend right = 10] (convposterior);
			\path (likelihood) edge[] (convposterior);
			\path (likelihood) edge[bend right = 5] (convposteriorsigma);
			% text in the top right corner
				\node[anchor = north east, text width = 6cm] at (-4, 5.75) {%
					Required technical results: \\
					\raisebox{3pt}{\tikz{\draw[red, solid, line width = 0.9pt, -](0, 0) -- (7mm, 0);}} \Cref{lem:nnposterior} \\ 
					\raisebox{3pt}{\tikz{\draw[blue, solid, line width = 0.9pt, -](0, 0) -- (7mm, 0);}} \Cref{prop:wassconvexity}
				};
		\end{tikzpicture}}
		\caption{Dependency of results for the proof of \Cref{thm:studposterior}.}
		\label{fig:chartmain}
	\end{centering}
\end{figure}

By exploiting the connection between $\mathcal{W}_1$ and weak convergence, together with what was observed in \Cref{subsec:nngplaw}, \Cref{thm:studposterior} leads us to a characterization of the asymptotic behavior of the exact posterior law of a BNN trained following the Gaussian-Inverse-Gamma model, showing convergence to a Student-t process in the infinite-width limit.

\begin{corollary} \label{cor:studposterior}
    Under the same assumptions of \Cref{thm:studposterior} and $n_L = 1$, the posterior of the BNN $f_{\bm{\theta}}$ with Gaussian-Inverse-Gamma prior and Gaussian likelihood, evaluated in the input set $\bm{x}$, converges in law to a multivariate Student-$t$ variable with $2a+k$ degrees of freedom:
    \begin{equation*}
        f_{\bm{\theta}}(\bm{x}) \given \train \xrightarrow[n_{min} \to \infty]{law} \tstud{2a + k}{\bm{\mu}_{\mathrm{post}}}{\bm{\Sigma}_{\mathrm{post}}},
    \end{equation*}
    with $M$, $\bm{\mu}_{\mathrm{post}}$ and $\bm{\Sigma}_{\mathrm{post}}$ as in \cref{eq:tstudpostparams}.
\end{corollary}

\section{Simulations} \label{sec:simulations}

We present a procedure to sample from the posterior distribution of a BNN, ensuring consistency between the theoretical results and practical implementations. 
% = splitted in 2 lines
% edit: colt template
We consider a hierarchical Bayesian model, where we place a prior on both the network parameters, $\bm{\theta} =$ $= (\bm{W}^{(1)}, \bm{b}^{(1)}, \dots, \bm{W}^{(L)}, \bm{b}^{(L)})$, and the variance $\sigma^2$:
\begin{align} \label{eq:bnnhiermodel}
    & \bm{W}^{(l)} \sim \normal{\bm{0}_{n_l \times n_{l - 1}}}{\sigma_{\bm{W}^{(l)}}^2 / n_{l - 1} \, \bm{I}_{n_l \times n_{l - 1}}}, \ && \bm{b}^{(l)} \sim \normal{\bm{0}_{n_l}}{\sigma_{\bm{b}^{(l)}}^2 \bm{I}_{n_l}},  \text{ for } l \in [L - 1], \nonumber \\
    & \bm{W}^{(L)} \given \sigma^2 \sim \normal{\bm{0}_{n_L \times n_{L - 1}}}{\sigma^2 / n_{L - 1} \, \bm{I}_{n_L \times n_{L - 1}}}, \ && \bm{b}^{(L)} \given \sigma^2 \sim \normal{\bm{0}_{n_L}}{\sigma^2 \bm{I}_{n_L}}, \\
    & \sigma^2 \sim \invgamma{a}{b}, \ && \bm{y}_{\mathcal{D}} \given \bm{\theta}, \bm{x}_{\mathcal{D}}, \sigma^2 \sim \normal{f_{\bm{\theta}}(\bm{x}_{\mathcal{D}})}{\sigma^2 \bm{I}_{n_L \times k}}. \nonumber
\end{align}
 

\begin{algorithm}
% edit: colt template
% \begin{algorithm2e}
    \SetAlgoVlined
    \LinesNumbered
    \caption{\textsc{Training of a BNN under Gaussian-Inverse-Gamma prior}}
    \label{alg:samplingpostbnn}
    \KwIn{$\bm{\alpha}$ [architecture], $\bm{\sigma}$ [BNN's variances], $(a, b)$ [Inverse-Gamma parameters], \newline
        $f$ [function]}
    \textbf{build} training set $\train$ starting from a reference function $f$ \;
    \textbf{build} test set $\test$ based on a fine partitioning of the domain of $f$ \;
    \textbf{initialize} $\smash{\sigma^2_{(0)}}$ \;
    \For{$i = 0, \dots m$}{
        \textbf{sample} $\bm{\theta}_{(i + 1)} \given \sigma^2, \train$ using \textbf{\texttt{NUTS}} \label{line:bnnsample} \;
        \textbf{sample} $\smash{\sigma^2_{(i + 1)}} \given \bm{\theta}_{(i)}, \train$ from $\smash{p_{\sigma^2 | \bm{\theta}_{(i + 1)}, \train}(\sigma^2)}$ \label{line:varsample} \;
    }
    \Return $f_{\bm{\theta}_{(m)}}(\test) \given \train$ \;
\end{algorithm}
% edit: colt template
% \end{algorithm2e}

The Monte Carlo sampling strategy is summarized in \Cref{alg:samplingpostbnn}. In particular, the idea is to sample from $\bm{\theta}, \sigma^2 \given \train$ using a Gibbs sampling scheme. 
To implement \cref{line:bnnsample} we rely on MCMC methods, which have been widely studied in the literature of Bayesian optimization for BNNs. 
Several versions of such strategies are implemented in the \texttt{Python} library \texttt{Pyro} \citep{pyro,pytorch}, and among these, we applied the No-U-turn sampler \citep{hoffman2011}. 
Whereas, in order to sample from the marginal posterior of the variance (\cref{line:varsample}) we exploit the strategy adopted by \citet[Appendix D]{ding2022}, since it is possible to show that it follows a positive Generalized Inverse Gaussian distribution (referred as $\mathcal{GIN}^+$ by the authors). 
Such a derivation, as well as additional implementation details, can be found in \Cref{sec:simulationsdetails}.

We report in \Cref{fig:normalinvgammaprior} the comparison between a sequence of BNNs trained using the strategy discussed above and the limiting Student-$t$ process discussed in \Cref{sec:postnngp} (see \Cref{rem:poststudtprocess}). 
As the (all equal) widths of the hidden layers increase, the models' output distributions become closer. 
We also consider the case in which we replace the Gaussian-Inverse-Gamma prior with the classical Gaussian prior with fixed variance (see \Cref{fig:gaussianprior}). 
In this case the limiting process to which the sequence of BNNs converges is simply the posterior NNGP (see \citet{hron2020,trevisan2023}).

\begin{figure}
    \centering
    \includegraphics[width = \textwidth]{images/1e2_2h_5dn_samples_nig_post_all.pdf}
    \captionsetup{width = .95\textwidth, font = small, skip = 2pt}
    % edit: colt template
    \caption{%
        Sequence of posterior BNNs, \textcolor{plotgray}{$(f_{\bm{\theta}_n} \given \train)_n$} (in gray), converging to the corresponding posterior Student-$t$ process, \textcolor{plotgreen}{$G \given \train$} (in green), in the infinite-width limit. 
        Given \textcolor{plotred}{$\train$} (in red), training set, we sampled $100$ values from both $G \given \train$ and $f_{\bm{\theta}_n} \given \train$ for each width $n \in \{2^0, \dots, 2^7\}$, following \Cref{rem:poststudtprocess} and \Cref{alg:samplingpostbnn}, respectively. 
        The networks used have $2$ hidden layers, \texttt{erf} activations and parameter variances set to $5$. 
        Additionally, the hyperparameters $(a, b)$ are set to $(3, 2)$. 
    }
    \label{fig:normalinvgammaprior}
\end{figure}

\begin{remark}
    We can observe that in \Cref{fig:gaussianprior}, i.e., under Gaussian prior, the convergence is much faster and more precise compared to the Gaussian-Inverse-Gamma prior case (\Cref{fig:normalinvgammaprior}). 
    This behavior, while likely influenced by our specific sampling procedure for the posterior BNNs, is also consistent with theoretical expectations.
    Indeed, although the theoretical convergence rates of the limiting processes are identical for both the Gaussian prior and the Gaussian-Inverse-Gamma prior cases (see \Cref{cor:convposterior} and \Cref{thm:studposterior}), the associated multiplicative constants differ significantly in magnitude. 
    Therefore, given a common fixed width, different distances between the posterior BNNs and their limiting processes are to be expected.
\end{remark}

\begin{figure}
    \centering
    \includegraphics[width = \textwidth]{images/1e2_2h_5dn_samples_norm_post_all.pdf}
    \captionsetup{width = .95\textwidth, font = small, skip = 2pt}
    % edit: colt template
    \caption{%
        Sequence of posterior BNNs, \textcolor{plotgray}{$(f_{\bm{\theta}_n} \given \train)_n$} (in gray), converging to the corresponding posterior Gaussian process, \textcolor{plotgreen}{$G \given \train$} (in green), in the infinite-width limit. 
        Given \textcolor{plotred}{$\train$} (in red), training set, we sampled $100$ values from both $G \given \train$ and $f_{\bm{\theta}_n} \given \train$ for each width $n \in \{2^0, \dots, 2^7\}$. 
        The sampling was performed following \citet[eqs. (2.22)-(2.24)]{gp2006} for $G \given \train$ and the built-in \texttt{NUTS} algorithm in \texttt{Pyro} for $f_{\bm{\theta}_n} \given \train$. 
        The networks used have $2$ hidden layers, \texttt{erf} activations, parameter variances set to $2$, and likelihood variance set to $0.1$.
    }
    \label{fig:gaussianprior}
\end{figure}

We conclude by comparing the asymptotic processes under both frameworks.
The posterior Student-$t$ process models the variance of the data more accurately compared to the posterior Gaussian process.
This is an expected behavior, as the Gaussian-Inverse-Gamma model explicitly estimates the data variance during the Bayesian learning, whereas no such estimation is performed when we use a Gaussian prior.
This final result also highlights that using a Gaussian-Inverse-Gamma prior provides a more accurate representation of the data, particularly in scenarios in which the dataset is relatively small.

\begin{figure}
    \centering
    \includegraphics[width = .9\textwidth]{images/1e2_2h_5dn_comparison.pdf}
    \captionsetup{width = .9\textwidth, font = small, skip = 2pt}
    % edit: colt template
    \caption{%
        Posterior Student-$t$ process (on the right) and posterior Gaussian process (on the left). 
        We followed the same strategy and used the same parameters introduced to generate \Cref{fig:normalinvgammaprior,fig:gaussianprior}.
    }
    \label{fig:comparison}
\end{figure}