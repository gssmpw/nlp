\section{Proof Overview}\label{sec:overview}
In this section, we give an overview of our proof techniques. Our starting point is the key-agreement protocol of \cite{HaitnerMST22}. As mentioned above, \cite{HaitnerMST22} showed that a $\CDP$ protocol $\Pi$ for computing the inner product functionality implies the existence of key-agreement ($\KA$). To prove that, they used $\Pi$  to construct a (weak) $\KA$ protocol, in which the parties $\Ac$ and $\Bc$ interact as follows:
\begin{enumerate}
    \item In the first step, $\Ac$ and $\Bc$ choose random inputs $x\in \set{-1,1}^n$ and $y\in \set{-1,1}^n$, respectively.
    \item The parties interact using $\Pi$ to get approximation $z$ of $\langle x,y \rangle$.
    \item Next, $\Ac$ chooses a random string $r \gets \zn$, and sends $r, x_r=\set{x_i \colon r_i=1}$ to $\Bc$. $\Bc$ replies with $y_{- r}=\set{x_i \colon r_i=0}$.
    \item Finally, $\Ac$ computes and outputs $\out_\Ac=\langle x_{-r},y_{-r} \rangle$, and $\Bc$ outputs $\out_\Bc=z-\langle x_{r},y_{r} \rangle$.
\end{enumerate}
The first observation made by \cite{HaitnerMST22} is that, by linearity of the inner product, $\langle x_{-r},y_{-r} \rangle+\langle x_{r},y_{r} \rangle=\langle x,y \rangle$, and thus $\size{\out_A-\out_B}=\size{z-\langle x,y \rangle}$, which is small assuming $\Pi$ is accurate. Moreover, it was shown that this protocol can be amplified into a full-fledged $\KA$ protocol if any efficient adversary $\Eve$ cannot  approximate $\out_A$ within small additive distance, given the transcript of the weak $\KA$ protocol. The main technical part in \cite{HaitnerMST22} is to show that this is indeed the case in the above protocol. 

In this work, we are interested in constructing a stronger primitive --- that is, oblivious transfer ($\OT$). In contrast to a $\KA$ protocol in which security must hold against external observers that only see the transcript, in $\OT$, the security needs to hold against an adversary that has access to the full view of one of the parties. %It turns out that the $\KA$ protocol of \cite{HaitnerMST22} cannot be used as is for this task.
 Our first observation is that the original $\KA$ protocol of \cite{HaitnerMST22}, as is, 
cannot be used  to construct any form of even ``weak" oblivious transfer.
In more details, it is possible to show that for some carefully chosen $\CDP$ and accurate protocol  $\Pi$, the joint view of the parties in the above protocol can be \emph{simulated} using a trivial protocol, without using $\Pi$ at all.\footnote{Here a protocol $\Pi'$ simulates $\Pi$ if $\Pi'$ generates the joint view of the parties $\Ac$, $\Bc$ in $\Pi$, without revealing any other information to the parties. We say that the simulation $\Pi'$ is trivial if it does not rely on any nontrivial protocol or  cryptographic assumptions. Importantly, the simulation may reveal new information (that is already known for the internal parties $\Ac$ and $\Bc$) to an external observer, and thus it doesn't imply that the protocol is not a $\KA$ protocol. See \cref{appendix:HaitnerMST22} for a concrete example.}
Since there is no $\OT$ without computational assumptions, this leads to the conclusion that the $\KA$ protocol of \cite{HaitnerMST22} could not be used as a subroutine (in a black-box manner) to construct $\OT$. Looking ahead, our solution is to inject carefully placed \emph{noise} to the interaction, in a manner that cannot be simulated by the parties without revealing private information. As we will see shortly,  this enables us to bypass the limitations of the original construction of \cite{HaitnerMST22} and to achieve $\OT$.

\paragraph{Weak Erasure Channel.} Before describing our construction, let us explain the properties which we need the construction to fulfill. Similarly to \cite{HaitnerMST22}, we will start with constructing a weak $\OT$ protocol, and then we will show how to amplify it into a full-fledged $\OT$ protocol. By the work of \cite{Wullschleger09}, to achieve OT, it is sufficient to construct a \emph{weak erasure channel} (\WEC).\footnote{A weak erasure channel is a weak version of Rabin's $\OT$ \cite{Rabin81}, which is known to be equivalent to ${2 \choose 1}$-$\OT$.} In this work, we construct a protocol with a weaker security guarantee, which we call \emph{approximate weak erasure channel} (\AWEC), and show that such a protocol also suffices for constructing $\OT$. Informally, an $(\ell,\alpha,p,q)$-\AWEC is a protocol between two parties $\Ac$ and $\Bc$ that have no input, in which $\Ac$ outputs a number $O_\Ac\in [-n,n]$, and $\Bc$ either outputs a number $O_\Bc\in [-n,n]$, or an erasure symbol $O_\Bc=\bot$. We additionally require that:


\begin{itemize}
		
		\item Erasure happens with probability $1/2$. Namely,   $\pr{O_\Bc = \perp} = 1/2$.
		
		\item When there is no erasure, $\Ac$ and $\Bc$ approximately agree. More formally, $$\pr{\size{O_A - O_B} > \ell \mid O_B \ne \bot}\le \alpha.$$
		
		\item When an erasure occurs, $\Bc$ cannot predict the value of $O_A$. That is, let $V_B$ be the view of $\Bc$ in the protocol. Then we require that for any efficient algorithm  
		$\widehat{\Bc}$ it holds that
        \begin{align*}
				\pr{\size{\widehat{\Bc}(V_B) - O_A} \leq 1000 \ell \mid O_B=\bot} \leq q.
			\end{align*}
			(i.e., if $O_B=\bot$, Bob can't estimate the value of $O_A$ with error $\leq 1000 \ell$).
	
        \item Lastly, we require that $\Ac$ cannot say when an erasure occurs. That is, let $V_A$ be the view of $\Ac$ in the protocol.  Then we require that for any efficient algorithm  
		$\widehat{\Ac}$ it holds that
			\begin{align*}
				%\size{\pr{\Ac(O_A,V_A) = 1 \mid O_B \neq \perp} - \pr{\Ac(O_A,V_A) = 1 \mid O_B = \perp}} \le p
				\size{\pr{\widehat{\Ac}(V_A) = 1 \mid O_B \neq \perp} - \pr{\widehat{\Ac}(V_A) = 1 \mid O_B = \perp}} \le p
			\end{align*}
	\end{itemize}
  See \cref{def:AWEC} in \cref{sec:AWEC} for the formal definition of \AWEC.  
  We show that when $p+q+\alpha \ll 1$, an \AWEC protocol can be amplified into an $\OT$. We do this by showing a reduction from \AWEC to \WEC and then applying the amplification result of \cite{Wullschleger09} to get a full-fledged $\OT$.

\paragraph{The Construction.} We next describe how to use a $\CDP$ protocol $\Pi$ computing the inner-product to construct an \AWEC. Our accuracy requirement from $\Pi$ is that on random inputs, $\Pi$ output is within distance $\ell \ll n^{1/6}$ from the inner-product with probability at least $0.999$. That is,
\[ 
\ppr{x,y\gets\oo^n,
z\gets \Pi(x,y)}{ \size{z-\langle x,y\rangle}\le \ell} \ge 0.999.
\]
The construction is similar to the construction of \cite{HaitnerMST22}, with the key difference that with probability $1/2$, $\Bc$ sends a noisy version of its vector to $\Ac$. Informally, this random noise ``erases" the information $\Bc$ has on $\Ac$'s output.



\begin{protocol}[\AWEC]\label{protocol:intro:DPIP-to-AWEC}
	%\item Oracle access: An $(\eps,\delta)$-CDP protocol $\Pi$ $(\ell,0.999)$-accurate for the \emph{inner-product} functionality.
	\item Operation:
	\begin{enumerate}
		%\item  $\Ac$ samples $x\gets \mon$, and   $\Bc$ samples $y\gets \mon$.
		
		\item $\Ac$ samples $x\gets \oo^n$, and $\Bc$ samples $y\gets \oo^n$.
        \item $\Ac$ and $\Bc$ interact according to $\Pi$ using inputs $x$ and $y$ \resp to get output $z\approx \langle x,y \rangle \pm \ell$. 
		\item  $\Ac$ samples  $r \la \zo^n$ and sends $(r,x_{r} = \set{x_i \colon r_i =1})$ to $\Bc$.
		
		\item $\Bc$ samples a random bit $b \la \zo$ and acts as follows:
		
		\begin{enumerate}
			
			\item If $b=0$, it sends $y_{-r}= \set{y_i \colon r_i =0}$ to $\Ac$ and outputs $o_B = z - \ip{x_r, y_r}$.
			
			\item Otherwise ($b=1$), it performs the following steps:
			
			\begin{itemize}
				\item Samples $k$ uniformly random indices $i_1,\ldots,i_k \la [n]$, where $n^{1/3} \gg k \gg  e^{\eps} \cdot  \ell^2$.
				\item Compute $\ty = (\ty_1,\ldots,\ty_n)$ where $\ty_i \la \begin{cases} \cU_{\oo} & i \in \set{i_1,\ldots,i_k} \\ y_i & \text{otherwise} \end{cases}$,
				\item Send $\ty_{-r}= \set{\ty_i \colon r_i =0}$ to $\Ac$, and
				\item Output $o_B = \perp$.
			\end{itemize}
			
			
		\end{enumerate}
		
		\item  Denote by $\hy_{-r}$ the message $\Ac$ received from $\Bc$. Then $\Ac$ outputs $o_A =  \ip{x_{-r}, \hy_{-r}}$.
		
		
	\end{enumerate}
\end{protocol}
That is, with probability $1/2$, $\Bc$ decides to make an erasure by changing the value of $k$ random bits of $y$ before sending $y_{-r}$ to $\Ac$. Let $\mu=\widetilde{y}-y$ be the noise vector. Then importantly, the crux of the above protocol is that the noise $\langle \mu_{-r}, x_{-r} \rangle = \langle \widetilde{y}_{-r}, x_{-r} \rangle-   \langle y_{-r}, x_{-r} \rangle$ cannot be simulated by $\Ac$ or $\Bc$ without revealing more information on the input of the other party. We next give an overview of the analysis of the above construction. 

\paragraph{Analysis.} 
In the following, let $X,Y$ be inputs of $\Ac$ and $\Bc$ to $\Pi$, respectively, and let $V_A$ and $V_B$ be their view in the interaction according to $\Pi$.  Let $R, I_1,\dots, I_k, \widetilde{Y}$ and $\widehat{Y}$ be the random variables taking the values of $r,i_1,\dots,i_k,\widetilde{y}$ and $\widehat{y}$ respectively. 
Finally, let $\widehat{V}_A =(V_A, R, \widehat{Y}_{-R})$ and $\widehat{V}_B= (V_B, R, X_R, I_1,\dots, I_k, \widetilde{Y},\widehat{Y})$ be the parties' views in \cref{protocol:intro:DPIP-to-AWEC}, and $O_A,O_B$ their outputs, respectively. 

It is clear by the definition of the protocol that erasure indeed occurs with probability $1/2$. Moreover, similarly to the correctness of the key-agreement protocol, and by the accuracy of $\Pi$, when there is no erasure, $O_A$ and $O_B$ are of distance at most $\ell$ apart with probability at least $0.999$. Thus, it remains to prove that $\Bc$ cannot approximate $O_A$ when erasure occurs and that $\Ac$ cannot learn whether it occurred. To simplify the analysis in this proof overview, in the following, we assume that $\Pi$ is a $(\eps,0)$-\CDP protocol,\footnote{While $(\eps,0)$-\CDP (that is, setting $\delta=0$) doesn't make sense in the computational setting, for the sake of simplicity, we ignore this subtly in this informal overview.}  for a small constant $\eps>0$.


\begin{remark}[Oblivious transfer v.s. key agreement]\label{remark:OTvsKA}
Before discussing the security proof of our protocol, we want to remark an important difference from the  analysis of \cite{HaitnerMST22}'s $\KA$ protocol. While the primitive we construct here, $\OT$, is a stronger primitive than $\KA$, some parts of the proof actually became easier in our analysis. This is due the fact that in $\OT$, we need to consider an internal attacker, while in $\KA$ the security needs to hold against external observer.

In more detail, \cite{HaitnerMST22} needed to show that security holds against an adversary who sees $R, X_{R}$ and $Y_{-R}$. This form of leakage creates a dependency between the seed $R$ to both the leakage on  $X$, $X_R$, and the leakage of $Y$, $Y_{-R}$. This form of dependency makes the analysis challenging. In contrast, in our setting, the attacker sees one of the vectors $X$ and $Y$ entirely. For example, $\Ac$ gets to see $X, R$ and $Y_{-R}$, and therefore there is no dependency between the leakage on $X$ and $R$. This fact helps us to bypass this dependency issue, and makes this part of our analysis easier.
\end{remark}

\paragraph{Erasures do erase.} This parts follow quite easily using ideas from \cite{McGregorMPRTV10,HaitnerMST22}.
Assume towards a contradiction that there exists an efficient  $\widehat{\Bc}$, that given $\widehat{V}_B$ and conditioned on $O_B=\bot$ (equivalently, on $b=1$)  can approximate $O_A$ within small distance. Namely, assume that
$$
\pr{\size{\widehat{\Bc}(\widehat{V}_B) - O_A} \leq 1000 \ell \mid O_B=\bot} >q 
$$
for some small constant $q$. Our goal is to  construct, using $\widehat{\Bc}$, an algorithm $\widetilde{\Bc}$ that breaks the \CDP guarantee of $\Pi$.


Building on \cite{McGregorMPRTV10,HaitnerMST22}, to break the \CDP property of the protocol $\Pi$, it is enough to construct an algorithm that approximates the inner product of (a subset of) $X$ with a random vector $S$. That is, we want to construct an algorithm $\widetilde{\Bc}$ that for a random subset $\cH\subseteq [n]$ of size $k/4$ and for a random vector $S\gets \oo^{k/4}$,  approximates  the inner product $\langle X_{\cH}, S \rangle$ within small additive  distance. More formally, we want an algorithm $\widetilde{\Bc}$ such that 
$$\pr{\size{\widetilde{\Bc}(V_B,\cH,S, X_{-\cH})-\langle X_{\cH}, S \rangle}\le \sqrt{\size{\cH}/e^\eps}}> \alpha.$$
The work of \cite{McGregorMPRTV10} implies that in the information-theoretic settings, such an algorithm contradicts the privacy guarantee of $\Pi$.\footnote{\cite{McGregorMPRTV10} showed that when $\cH=[n]$, approximating the inner product contradicts DP. Extending this result for any subset $\cH$ is not hard.} \cite{HaitnerMST22} extended this result to the computational settings, by giving a constructive proof. Thus, such an algorithm $\widetilde{\Bc}$ is enough to break the assumed \CDP property of $\Pi$.


To construct $\widetilde{\Bc}$, we first need to make an observation regarding  $\widehat{\Bc}$. 
Let $\cH=\set{I_1,\dots,I_k} \cap \set{i\colon R_i=0}$ be the set of indices that $\Bc$ sends to $\Ac$ and replace the bit with a random value, and assume for simplicity that $\size{\cH}=k/4$.\footnote{By a simple concentration bound, the size of $\cH$ is at least $k/4$ with all but negligible probability. When $\cH$ is larger, we simply consider the first $k/4$ indices in $\cH$.} Let $S=\widetilde{Y}_\cH$, and note that the distribution of $S$ is uniformly random, even given $\cH,X$ and $V_B$. Our main observation is that if $\widehat{\Bc}$  approximates $O_A=\langle X_{-R},\widetilde{Y}_{-R} \rangle$, then it must also compute a good approximation of $\langle X_{\cH}, S \rangle =\langle X_{\cH},  \widetilde{Y}_{\cH}\rangle$.

To see why, let $\widetilde{\cH}=\set{i\colon R_i=0}\setminus \cH$, and let   $T=\widehat{\Bc} (\widehat{V}_B)-\langle X_{\widetilde{\cH}},\widetilde{Y}_{\widetilde{\cH}} \rangle$. We claim that $T$ is a good approximation of $\langle X_{\cH}, S \rangle$. Indeed,
\begin{align*}
T-\langle X_{\cH}, S \rangle &= T-(\langle X_{-R},  \widetilde{Y}_{-R}\rangle - \langle X_{\widetilde{\cH}},  \widetilde{Y}_{\widetilde{\cH}}\rangle)\\
&= (T + \langle X_{\widetilde{\cH}},  \widetilde{Y}_{\widetilde{\cH}}\rangle)-\langle X_{-R},  \widetilde{Y}_{-R}\rangle \\
& =\widehat{\Bc}(V_B) -\langle X_{-R},  \widetilde{Y}_{-R}\rangle. 
\end{align*}
Therefore, $\size{T-\langle X_{\cH}, S \rangle}\le \sqrt{\size{\cH}/e^\eps}$ if $\size{\widehat{\Bc}(V_B) -\langle X_{-R},  \widetilde{Y}_{-R}\rangle}\le 1000\ell$ (note that by our choice of $k$, $\ell \ll \sqrt{\size{\cH}/e^\eps}=\sqrt{k/4e^\eps}$).

Thus we only need to show an algorithm $\widetilde{\Bc}$, that given $V_B,\cH,S,X_{-\cH}$ as input, evaluates the value of $T$. The main issue left is that while $\widetilde{\Bc}$ only gets $V_B$, and to evaluate $T$ we need to execute $\widehat{B}$ that needs to get an entire view $\widehat{V}_B$ of $\Bc$ in \cref{protocol:intro:DPIP-to-AWEC}. However, this can be easily simulated. Indeed, $\widetilde{\Bc}$ can simply sample $R,I_1,\dots,I_k$ and $\widetilde{Y}$ condition on the values of $\cH, S$ (and $O_B=\bot$), to get the exact distribution of $\widehat{V}_B$.



\paragraph{Erasures are hidden.}
This part is more involved and is our main new technical part. The goal in this part is to show that $\widehat{\Ac}$ cannot tell when an erasure happens. Equivalently, we want to show that $\widehat{\Ac}$ cannot distinguish $Y_R$ from the noisy version $\widetilde{Y}_R$ with too good probability. For this end, we show that an algorithm $\widehat{\Ac}$ that (given $V_A$) can distinguish $Y_R$ from the noisy version $\widetilde{Y}_R$ with a small but constant probability, can be used to break $\CDP$. 

The first step in the proof is to notice that, by a rather standard hybrid argument, it is enough to show that $\widehat{\Ac}$ cannot distinguish with advantage $\Theta(1/k)$ between $Y$ to a vector $Y'$ which is identical in all but one random coordinate. That is, let $I\gets [n]$ be a random index, and let  $Y^{(I)}=(Y_1,\dots,Y_{I-1},-Y_I,Y_{I+1},...,Y_n)$ be  the vector that is identical to $Y$ in all entries except the $I$-th one. Then it is enough to show that if \Jnote{Can we say instead: We show that if,}\Nnote{not sure}\Jnote{Why not? what is missing?}
\[ \pr{\widehat{\Ac}(V_A,R,Y_R)=1}-\pr{\widehat{\Ac}(V_A,R,Y^{(I)}_R)=1} \ge 1/k,\]
  then $\widehat{\Ac}$ can be used to break the $\eps$-$\CDP$ of $\Pi$.


%\Jnote{
%Note that an algorithm $\widehat{A}$ that distinguishes $Y_R$ from $Y^{(I)}_R$ with a large enough \emph{constant} advantage (proportional to $\eps$) contradicts the privacy requirement of $\Pi$ in a trivial manner. However, when the distinguishing advantage is small as is the case in our }
We highlight that the key difficulty in proving the above, lies in the fact that the distinguishing advantages is very small. Indeed, an algorithm $\widehat{A}$ that distinguishes $Y_R$ from $Y^{(I)}_R$ with a large enough \emph{constant} advantage (proportional to $\eps$) already contradicts the privacy requirement of $\Pi$. However, in our setup the distinguishing advantage is much smaller than $\eps$, which makes the task of contradicting $\eps$-\CDP challenging. Moreover, we want to emphasize the following two crucial points that makes $\widehat{\Ac}$ useful (and our design for our \AWEC protocol, \cref{protocol:intro:DPIP-to-AWEC}, was guided by these two observations).

\begin{itemize}
    \item {\sf The importance of a random index.}  We note that it is crucial that the index $I$ is chosen at random and that it is unknown to $\widehat{\Ac}$. To see that, let $\Mc$ be a \DP mechanism and $Y\gets \oo^n$ be an input. We claim that it can be the case that an adversary can distinguish between $Y$ and $Y^{(i)}$ given $\Mc(Y)$ with advantage $\approx \eps$, for any fixed index $i$. Indeed, it is possible that $\Mc(Y)$ outputs a noisy estimation of $Y_i$, which is correct with probability $\approx 1/2 + \eps$. Such an estimation will not contradict differential privacy, but will be useful to distinguish between $Y$ and $Y^{(i)}$. We resolve this issue by adding the noise in a random position.
    \item {\sf The importance of a random subset.} Next, we claim that it is crucial that $\widehat{\Ac}$ only gets a random subset of $Y$, $Y_R$. Indeed, as in the previous example, the mechanism $\Mc(Y)$ can output a noisy estimation of the (parity of) the number of ones in $Y$. As in the previous example, such an estimation does not contradict \DP, but is enough to distinguish $Y$ and $Y^{(I)}$ by considering the number of ones in the input compared to the estimation. We deal with such a \emph{global} information by revealing to $\widehat{\Ac}$ only subset of $Y$. This makes the global information not useful.
\end{itemize}

Thus, in the proof, we need to leverage the fact that $\widehat{\Ac}$ distinguishes $Y_R$ from $Y^{(I)}_R$ for a random index $I$ and a random subset $R$. We use such an algorithm $\widehat{\Ac}$ to construct an algorithm $\Gc$ that given $Y_{-I}$, predicts the value of $Y_I$. In more detail, we prove the following lemma, which is the technical core of our proof.

\begin{lemma}[\cref{lemma:property1:prediction}, informal]\label{lemma:property1:prediction:overview}
	Let $(Z,W)$ be jointly distributed random variables, let $R \la \zo^n$ and $I \la [n]$. 
	Let  $\Ac$ be an efficient algorithm that satisfy
	\begin{align*}
		\size{\ex{\Ac(R,Z_{R},W) - \Ac(R,Z^{(I)}_{R},W)}} \geq 1/k,
	\end{align*}
	for $Z^{(I)} = (Z_1,\ldots, Z_{I-1}, -Z_I, Z_{I+1},\ldots, Z_n)$. Then, there exists an efficient algorithm $\Gc$ that outputs a value in $\set{1,-1,\bot}$, such that the following holds.
		$$\pr{\Gc(I,Z_{-I}, W) = -Z_I} \leq \frac{k^2}{n},\ and,\ \pr{\Gc(I,Z_{-I}, W) = Z_I} \geq 1/k - k^2/n.$$
\end{lemma}

Back to our setting, using  \cref{lemma:property1:prediction:overview} (taking $\Ac= \widehat{\Ac}, Z=Y$ and $W=V_A$), we get an algorithm $\Gc$ such that 
$$\pr{\Gc(I,Y_{-I}, V_A) = -Y_I} \leq \frac{k^2}{n},\ and,\
		\pr{\Gc(I,Y_{-I}, V_A) = Y_I} \geq 1/k - k^2/n.$$

We highlight that the gap between the probability of $\Gc$ to output $Y_I$ to its probability to output $-Y_I$ is smaller than $1/k$, and that it is not enough to break \DP alone. The point is that we  have a bound on the probability of $\Gc$ to output $-Y_I$, and that in \DP we measure the difference in probabilities with \emph{multiplicative distance}. Thus, when 
\begin{align}\label{eq:overview:boundingK}
\pr{\Gc(I,Y_{-I}, V_A) = Y_I} > e^\eps\cdot \pr{\Gc(I,Y_{-I}, V_A) = -Y_I} 
\end{align}
the algorithm $\Gc$ does imply contradiction to \DP. To make sure that \cref{eq:overview:boundingK} holds, we need to have
\begin{align*}
 1/k - k^2/n > e^\eps\cdot k^2/n,
 \end{align*}
which is the reason we need to  take $k \ll n^{1/3}$.
In the rest of this section we give an high level proof sketch of \cref{lemma:property1:prediction:overview}.
%Yet, our end goal is to construct an algorithm $\widetilde{A}$, that can distinguish $(I, Y)$ from $(I,Y^{(I)})$ given $\Ac$'s view $V_A$ in $\Pi$.\footnote{In the proof itself, we do not construct a distinguisher, but an algorithm that given $Y_{-I}$ tries to predict $Y_I$. Such an algorithm is equivalent to distinguisher. } Moreover, our algorithm will only have  distinguishing advantage of $1/2k$, but still will be enough to contradict differential privacy. The reason this is not in contradiction to the above, is that the  probability of $\widetilde{A}$ to output $1$ on $(I,Y^{(I)})$ will be much smaller than the distinguishing advantage $1/2k$. Namely, while the  advantage we get is not enough to contradict \DP alone, we notice that if $\widetilde{A}$ outputs $1$ with a very small probability, then $1/k$ advantage is actually enough. Indeed, if $\pr{\widetilde{A}(V_A, I, Y^{(I)})=1}< (e^{\eps} -1)\cdot  1/2k$, then  we get that \[ \pr{\widetilde{A}(V_A,I,Y)=1 }\ge 1/2k + \pr{\widetilde{A}(V_A,I,Y^{(I)})=1} > e^\eps \cdot \pr{\widetilde{A}(V_A,I,Y^{(I)})=1}.\]Of course, from our assumption alone, we do not have an upper bound on the probability of $\widehat{A}$ to output $1$. In the following we will show that it is possible to construct from $\widehat{A}$ an algorithm $\widetilde{A}$, such that $\pr{\widetilde{A}(V_A,Y^{(I)}_R)=1}\le  k^2/n$, and still have a distinguishing advantage of at least $1/2k$. To break differential privacy, we will need to make sure that $k^2/n \ll (e^\eps-1)/k$, and for this we need to take $k\ll n^{1/3}$.





\subsection{Proof Overview for  \cref{lemma:property1:prediction:overview}}\label{sec:overview:prediction-lemma}
In this part we explain the ideas behind the proof of \cref{lemma:property1:prediction:overview}. We start with describing the algorithm $\Gc$. 
\paragraph{The predictor  $\Gc$.} The construction of $\Gc$ will leverage the fact that $\Ac$ succeed on a random index $I$ and while only seeing a subset of $Y$. We next describe the algorithm $\Gc$. Recall that on input $I, Z_{-I},W$, the goal of $\Gc$ is to predict the value of $Z_I$. Toward this, for $b\in \oo$, let $Z^b=(Z_1,\dots,Z_{I-1},b,Z_{I+1},\dots,Z_n)$, so that $Z\in \set{Z^{-1},Z^{1}}$. Then $\Gc$ computes three values:\footnote{In the proof itself, $\Gc$ only estimates these values by sampling $R$. For simplicity, here we assume that $\Gc$ can compute them exactly.}
\[\mu^{1} = \eex{R\gets \zn\mid R_I=1}{\Ac(R,Z^1_R,W)}\]
\[\mu^{-1} = \eex{R\gets \zn\mid R_I=1}{\Ac(R,Z^{-1}_R,W}\]
\[\mu^{*} = \eex{R\gets \zn\mid R_I=0}{\Ac(R,Z_R,W)}\]
Importantly, we note that when $R_I=0$,  $Z_R$ does not contain $Z_I$, and thus $\Gc$ can compute $\mu^*$ giving its input.
Finally, $\Gc$ outputs $b\in \oo$ if the following two conditions holds:
\[ \size{\mu^b-\mu^*}< 1/4k,\ and,\ \size{\mu^{-b}-\mu^{*}}\ge 1/4k. \] 
Otherwise, $\Gc$ outputs $\bot$.

Namely, $\Gc$ outputs $b$ if the expected value of $\Ac$'s output on $(R, Z_R)$ when $R_I=0$, is closer to the expected value of $\Ac$'s output on $(R, Z^b_R)$ when $R_I=1$ than it is to the expected value of $\Ac$'s output on $(R, Z^{-b}_R)$. We next analyze $\Gc$. 
%We next analyzing the distinguisher  $\Gc$.

\paragraph{Analyzing the predictor.} We will show that the following hold:
\begin{enumerate}
    \item\label{overview:item:1} The value of $R_I$ does not change the expectation by much. Formally,
$$ \size{\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z_R, W)}-\eex{R\gets \zn\mid R_I=0}{\Ac(R,Z_R,W)}}<1/4k$$
with  probability at least $1-k^2/n$, while 
\item\label{overview:item:2} Filliping the value of $Z_I$ does change the expectation. Namely, $$\size{\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z^1_R,W)}-\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z^{-1}_R,W)}}> 1/2k$$
with probability at least $1/k$.
\end{enumerate}
Together, the two items above imply that our distinguisher $\Gc$ has the properties we need:
\begin{itemize}
    \item The first item  above implies that $\Gc$ errs and  outputs $-Z_I$  with probability at most $k^2/n$. Indeed, for $b=-Z_I$, it holds that $\mu^{-b}=\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z_R, W)}$.  Thus, \cref{overview:item:1} implies that with all but probability $k^2/n$,  $\size{\mu^{-b}-\mu^{*}}<1/4k$. In this case, by definition of $\Gc$, it does not output $b$.
    
\item In contrast, the two items together imply that $\Gc$ outputs $Z_I$  with probability at least $1/k-k^2/n$. Indeed, for $b=Z_I$,  \cref{overview:item:1} implies that $\size{\mu^b-\mu^*}< 1/4k$ with probability at least $1/k$. Additionally, by the triangular inequality, when both \cref{overview:item:1} and \cref{overview:item:2} hold together we get  that 
$$ \size{\eex{R\gets \zn\mid R_I=0}{\Ac(R,Z_R,W)}-\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z^{-b}_R,W)}}> 1/4k,$$
which by definition  implies that 
$\size{\mu^{-b}-\mu^{*}}\ge 1/4k$ (for $b=Z_I$). By a simple union bound, the probability that both \cref{overview:item:1} and \cref{overview:item:2} hold is at least $1/k - k^2/n$.
\end{itemize}
Therefore, assuming that \cref{overview:item:1} and \cref{overview:item:2} hold, this  conclude the proof of \cref{lemma:property1:prediction:overview}. We next briefly explain why \cref{overview:item:1} and \cref{overview:item:2} hold with the claimed probability.


\paragraph{Bounding the error probability.} We first sketch the proof of \cref{overview:item:1}. We claim that 
\begin{align}\label{eq:overview:side}
    \ppr{Z,W,I}{ \size{\eex{R\gets \zn\mid R_I=0}{\Ac(R,Z_R,W)}-\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z_R,W)}}\ge 1/4k}\le k^2/n.
\end{align}
In the following we will actually prove a stronger claim. That is, that for every fixed $Z$ and $W$, it holds that 
\begin{align}\label{eq:overview:equiv}
    \ppr{I}{ \size{\eex{R\gets \zn\mid R_I=0}{\Ac(R,Z_R,W)}-\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z_R,W)}}\ge 1/4k}\le k^2/n.
\end{align}
Fix $Z$ and $W$, and let $F(R)=\Ac(R,Z_R,W)$. Using this notation, we can rewrite \cref{eq:overview:equiv} as
\begin{align}\label{eq:overview:final}
    \ppr{I}{ \size{\eex{R\gets \zn\mid R_I=0}{F(R)}-\eex{R\gets \zn\mid R_I=1}{F(R)}}\ge 1/4k}\le k^2/n.
\end{align}
Finally, \cref{eq:overview:final} follows a more general well-known fact on the uniform distribution over random strings: For a random string $R\gets \zn$, and for a random index $I\gets [n]$, the statistical distance between $R|_{R_I=0}$ and $R|_{R_I=1}$ is at most $1/\sqrt{n}$. This  fact implies that the function $F$ cannot distinguish too well between  samples from $R|_{R_I=0}$ and samples from $R|_{R_I=1}$.\footnote{Importantly, notice that $F$ does not get $I$ as input.} We prove the following stronger lemma that finishes this part of the proof.

\begin{lemma}[Informal version of \cref{lem:distance-I}]
        Let $R \la \zo^n$. 
	For any (randomized) function $F:\{0,1\}^n\rightarrow \{0,1\}$ and any $k>0$, it holds that
        \begin{align}\label{eq:f-alpha}
            \ppr{i \la [n]}{\size{\:\ex{F(R) \mid R_i = 0}-\ex{F(R) \mid R_i = 1}\:}\geq 1/4k} \leq \Theta\paren{\frac{k^2}{n}},
        \end{align}
        where the expectations are taken over $R$ and the randomness of $F$.
\end{lemma}

\paragraph{Lower bounding the distinguishing advantage.} Next we sketch the proof of \cref{overview:item:2}. We need to show that 
\begin{align}
\ppr{Z,W,I}{ \size{\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z^1_R,W)}-\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z^{-1}_R,W)}} \ge 1/2k}\ge 1/k.
\end{align}
Or equivalently,
\begin{align}\label{eq:overview:distinguish}
\ppr{Z,W,I}{ \size{\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z_R,W)}-\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z^{(I)}_R,W)}} \ge 1/2k}\ge 1/k.
\end{align}
We claim that \cref{eq:overview:distinguish} follows by the assumption that $\Ac$ is a good distinguisher. Indeed, assume towards contradiction that this is not the case. Then, by a simple  computation (and using the fact that $\Ac(R,Z_R,W)\in \set{0,1}$)  we get that 
\begin{align*}
\eex{Z,W,I}{ \size{\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z_R,W)}-\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z^{(I)}_R,W)}} }\le 2/k.
\end{align*}
and thus, using the triangular inequality we can write,
\begin{align*}
\size{\eex{Z,W,I}{\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z_R,W)}-\eex{R\gets \zn\mid R_I=1}{\Ac(R,Z^{(I)}_R,W)}} }< 2/k.
\end{align*}
Equivalently, we have that,
\begin{align*}
\size{\pr{\Ac(R,Z_R,W)=1\mid R_I=1}-\pr{\Ac(R,Z^{(I)}_R,W)=1\mid R_I=1}}<2/k.
\end{align*}
which means that $\Ac$ fails to distinguish between $(R,Z_R)$ and $(R,Z^{(I)}_R)$ when $R_I=1$. On the other hand, we notice that $\Ac$ also must fail when $R_I=0$. Indeed, when $R_I=0$ it holds that $Z_R=Z^{(I)}_R$, and thus we get  that 
\begin{align*}
\size{\pr{\Ac(R,Z_R,W)=1\mid R_I=0}-\pr{\Ac(R,Z^{(I)}_R,W)=1\mid R_I=0}}=0.
\end{align*}
Finally, since $\Ac$ fails to distinguish $Z_R$ from $Z^{(I)}_R$ both when $R_I=0$ and $R_I=1$, we conclude that $\Ac$ is not a good distinguisher. That is, 
\begin{align*}
\size{\pr{\Ac(R,Z_R,W)=1}-\pr{\Ac(R,Z^{(I)}_R,W)=1}}< 1/k,
\end{align*}
which is in contradiction to our assumption on $\Ac$.
