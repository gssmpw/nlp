\section{Related Work}
The use and judgment of vague quantifiers has been studied extensively in the psycholinguistics literature. 
Recent years have also seen a growing but relatively limited interest in studying (V)LM behavior with linguistic quantifiers. 

\paragraph{Vague quantifiers in human language}
Numbers have been shown to play a significant role in the understanding and use of vague quantifiers in humans \citep[see e.g.][]{solt_vagueness_2011}.
It has been suggested that humans make use of an \emph{approximate number system} \citep{feigenson2004-number, dehaene2011-numbersense, coventry2005-grounding}, where vague terms might not refer to exact numbers but rather approximations thereof.
However, it has also been shown that quantifier comprehension and use go beyond (approximations of) cardinality of the targeted object. 
Factors include object size \citep{hormann1983-calculating,newstead2000-context}; 
the number and proportions of \emph{other} objects in the scene \citep{coventry2005-grounding,coventry2010-space,pezzelle2018-mental};
set size (e.g.\ the answer to a question such as: ``\emph{Several} marbles from a set of 12 marbles would be \rule{0.3cm}{0.15mm} marbles''; \citealp{newstead1987-set});
the functionality of objects in the scene \citep{newstead2000-context}; and object grouping and spacing \citep{coventry2005-grounding}.

In conversations and texts, the choice of quantifier influences the (expected) rhetorical impact of a statement, and vice versa. \citet{moxey1993-prior} show that the choice of quantifier can reveal a speaker's \emph{prior expectations} regarding the frequency of the object in the scene. Moreover, several works have outlined the different perspectives that \textit{a few} and \textit{few} convey: while ``\emph{a few} people were at the party'' focuses on those who were present, ``\emph{few} people were at the party'' puts the emphasis on those who did not attend \citep{moxey2000-quantities,paterson2009-discourse}.

\paragraph{(Vague) Quantification with (V)LMs}
Most work on evaluating VLMs on quantifiers has focused on \emph{crisp} quantifiers (e.g.\ \textit{none}, \textit{all} and \textit{more than half}) rather than vague ones. \citet{sorodoc2016-quantify} show that neural networks can be trained to learn the quantifiers \textit{no}, \textit{some} and \textit{all} without the need for an explicit counting system. \citet{sorodoc2018-neural} extend this to a visual question-answering (VQA) task with natural images. They include vague expressions with \textit{few} and \textit{some}, but define these terms using specific proportions (e.g.\ {\em few} applies for predications involving less than 17\% of objects in the domain).
A similar definition is adapted by \citet{pezzelle2017-fuzzy}, who show that models require different mechanisms for learning cardinals and quantifiers.
Note that once the range of a quantifier is defined, it can no longer be considered \emph{vague} as borderline cases are excluded.

Moving beyond the gold label approach, \citet{testoni2019-sound} demonstrate that models using both audio and visual input to select appropriate quantifiers can achieve results that align with human distributions reported by \citet{pezzelle2018-mental}. 
\citet{enyan2024-quantifiers} compare human and large language model (LLM) responses on questions such as ``There are 500 balls. 234 of them are yellow. Are many balls yellow?''
They find that responses generated by LLMs align more closely with human judgments on crisp quantifiers than on vague ones. 
\citet{belem2024-uncertainty} find that LLMs are able to map uncertainty expressions such as \textit{probably} and \textit{unlikely} to probabilistic (numerical) responses in a human-like fashion.
More akin to our experiments, \citet{testoni2024-quantifying} evaluate three VLMs on their abilities to assign appropriate quantifiers to visual scenes, prompting models to select one out of nine quantifiers in response to questions such as ``How many animals are there in the image?'', with 
synthetic images generated by \citet{pezzelle2018-mental}. 
None of the models show any correlation with the distribution of responses provided by human annotators, which the authors suggest might be due to the models' poor counting abilities. 
Our approach diverges from theirs on several points. 
First, we use natural images rather than artificial ones, offering a more realistic setting for evaluating VLMs.
Additionally, we use a wider range of methods to provide a more comprehensive assessment of model behavior.