\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/human_results.pdf}
    \caption{\textbf{Average human ratings with increasing counts, segmentation area and size norms.} For each variable and each quantifier, we also report Spearman's $\rho$, which are all statistically significant ($p < 0.05$).}\label{fig:human_results}
\end{figure*}


\section{The \dataset Dataset}\label{sec:dataset}
We construct the VAQUUM dataset: \textbf{Va}gue \textbf{Qu}antifiers with H\textbf{um}an Judgments. 

\paragraph{Images}
We utilize annotated datasets used for object counting in computer vision.
FSC-147 \citep{ranjan2021-fsc147} contains 6146 images across 147 object types,
with annotated object counts ranging from 7 to 3731.
\citet{hobley2022-fsc133} refine and deduplicate this dataset to release FSC-133 (containing 133 object types).
We sample images from FSC-133 and exclude a total of 22 object categories for several reasons, 
such as their uncountable nature (e.g.\ \textit{fresh cut}), 
obscurity (e.g.\ \textit{carrom board pieces}) 
or simply because the images do not depict the object from the label. 
We also remap 37 categories to either their plural form where necessary or 
to their basic-level category \cite[e.g.\ mapping \textit{crows} to \textit{birds}; cf.][]{Rosch1976}. 
Since the lowest count in FSC-133 is 7, we complement this dataset with samples from 
the test set of TallyQA \citep{acharya2019-tallyqa}, 
which includes images and annotated counts sourced from 
Visual Genome \citep{krishna2017-visualgenome} and VQA2 \citep{antol2015-vqa,goyal2017-vqa2}. 
Here, we use images classified as ``simple'' in TallyQA, 
which have counts between 1 and 15. 
From this set, we exclude images for which the labelled object is not 
in the set of remapped FSC-133 labels. 
We discard all counts below 2 (from TallyQA) and above 100 (from FSC-133). 
We include three types of object features in our dataset:

\noindent 
{\bf 1. Count bin} To address the imbalance in object counts within the merged dataset,
we group the 99 distinct counts (ranging from 2 to 100) into bins of three (counts from 2 to 4, 5 to 7, etc).
From each bin, we randomly sample 33 images, yielding
1089 images, evenly distributed across 33 count bins, covering counts from 2 to 100. 

\noindent
{\bf 2. Segmentation area} We estimate the segmentation area of the object(s) in each image, i.e. the ratio of pixels in the objects' bounding region over the total image area. For each image, we prompt CLIPSeg \citep{luddecke2022-clipseg}, with the name of the object type (e.g \textit{birds}). The output logits are than passed through a sigmoid function, and the resulting values are thresholded. The resulting binary mask is used to compute the segmentation area, which essentially corresponds to ``object size'' in previous work.

\noindent 
{\bf 3. Size norm}
We investigate the impact of real-world object size using the object-specific norms in the THINGSplus database \citep{stoinski2024-thingsplus}, an extension of THINGS \citep{hebart2019-things}. Such norms are collected from human judges, and they reflect ``average'' or ``typical'' values for specific properties. The \emph{size} 
norm tells us something about an object's perceived real-life size, on an arbitrary scale. 
Objects that are not explicitly present in this dataset are either mapped to the closest (base) category or discarded in our size norm analyses. 

\subsection{Human Judgments}\label{sec:dataset_humans}
We recruited 203 participants, all native and primary speakers of English, through Prolific (52.2\% female; 45.8\% male; 1.5\% undisclosed). Participant ages ranged from 25 to 84, with the majority aged 25-34 (31.5\%) and 35-44 (25.6\%). 

\subsubsection{Procedure}
We presented each participant with 100 questions in a random order. 
Each of these questions consist of an image and a statement of the form ``There are \verb|[QUANT]| \verb|[OBJECT]| in the image.'' 
Here, \verb|OBJECT| is the plural form of the object depicted and
\verb|QUANT| $\in$ \{\textit{few}, \textit{a few}, \textit{some}, \textit{many}, \textit{a lot of}\} (e.g.\ ``There are \textit{a lot of apples} in the image.''). For each image, we also include the unquantified statement (omitting \verb|QUANT|).
Participants were asked to rate, using a slider, how accurate the statement is for the image (see Figure~\ref{fig:intro_example}). The slider ranges from ``Completely inaccurate'' to ``Completely accurate''. 
No participant saw the same image twice. 

\subsubsection{Analysis}\label{sec:human_analysis}
We analyze the effects of count, segmentation area and size norms on the collected appropriateness ratings of the vague quantifiers. 
We summarize the results in Figure~\ref{fig:human_results}.

We observe from Figure~\ref{fig:human_results} that an increase in count leads to an increase in the average ratings assigned to statements containing \textit{many} and \textit{a lot of}, whose trajectories are nearly identical. 
Conversely, for the complementary pair \textit{few} and \textit{a few}, we find that average ratings \emph{decrease} as object count increases. As expected, judgments for unquantified control statements are independent of count, with the exception of a slightly lower rating for the lowest counts. We also observe that \textit{few}/\textit{a few} and \textit{many}/\textit{{a lot of}} exhibit opposing trends in relation to count, again as expected.
These observations are broadly in line with findings by e.g.\ \citet{coventry2010-space}.
Average ratings for \textit{some} also decrease as count increases, though less steeply than for (\textit{a}) \textit{few}.
While signs of Spearman's coefficient are the same across all predictors, the strength of the correlation for segmentation area and size norm is noticeably lower. 
Furthermore, \textit{few}/\textit{a few} and \textit{many}/\textit{{a lot of}} do not exhibit opposing trends as a function of area or size norm, as they do with count.


\begin{table}
    \centering
    \setlength{\tabcolsep}{5pt}
    \small
    \begin{tabular}{l|ccccc|l}
        \toprule
        & \textbf{\textit{few}} & \textbf{\textit{a few}} & \textbf{\textit{some}} & \textbf{\textit{many}} & \textbf{\textit{a lot of}} & \textbf{ME}\\\midrule
        \textbf{C} & -0.37 & -0.38 & -0.20 & 0.38 & 0.42 & 0.03\\
        \textbf{SG} & -0.07 & -0.10 & -0.05 & 0.08 & 0.06 & 0.04 \\
        \textbf{SN} & -0.13 & -0.11 & -0.07 & 0.14 & 0.17 & 0.01$^*$ \\\midrule
        \textbf{ME} & -1.71 & -1.60 & -0.73 & -0.60 & -0.69
            \\\bottomrule
    \end{tabular}
    \caption{\textbf{Estimates of the linear mixed effects model fit to data in VAQUUM.} \textbf{C}=Count, \textbf{SG}=Segmentation, \textbf{SN}=Size norm, \textbf{ME}=Main effect.  All numbers are statistically significant ($p < 0.05$), except the one marked (*). For main effects, the quantifier is releveled to the unquantified case, with intercept estimated at $\beta=0.89$.}
    \label{tab:lmm_humans}
\end{table}

To gain further insights into the relations between participants ratings and object count and size, we fit a linear mixed effects model (LMM) to our data, predicting human judgments from the fixed effects of
quantifiers, count, segmentation area and size norm
and using participants and object category as random effects. 
We include interaction terms between pairs of predictors to investigate their joint influence on judgments.
For full details of the LMM, we refer to Appendix~\ref{appendix:lmms}. 

We report LMM estimates of the main effects and two-way interaction effects in Table~\ref{tab:lmm_humans}. 
All main effects except those for size norm are statistically significant.
For the two-way interactions, \textit{few}, \textit{a few} and \textit{some} consistently show negative estimates across all predictors, while \textit{many} and \textit{a lot of} are consistently positive. As expected given the trends in Figure~\ref{fig:human_results}, object count exhibits the strongest impact on each quantifier. 
Estimates for segmentation area and size norm display similar trends, but with weaker effects.
The LMM explains $50.3\%$ of the total variance in our participant data ($R^2c = 0.503$, $R^2m = 0.459$).
The random effects present moderate variability at participant level, with a variance of $0.042$
suggesting that individual differences among participants explain some of the variance in judgments.
In contrast, the object random effect accounts for minimal variance ($0.002$),
indicating that differences between objects have little influence on the judgments given by participants in our experiments. 