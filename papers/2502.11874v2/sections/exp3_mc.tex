\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/exp3_multiple_choice.pdf}
    \caption{\textbf{Log probabilities extracted for multiple-choice labels in Experiment~3.} We do not display results for BLIP-2 because that model is not instruction-tuned.}\label{fig:exp3_mc}
\end{figure*}

\section{Experiment 3: Multiple-Choice QA}
Finally, we evaluate VLM judgments in a multiple-choice question-answering (MCQA) setup using a standard MCQA template of the form 
``Question: Which statement is most accurate for the image? Select the answer from the options below. \verb|[OPTIONS]| Answer: ('', 
with \verb|OPTIONS| being the set of all statements for an image in \dataset, labeled \verb|(A)| to \verb|(F)|. 
For each image, the order of the expressions is shuffled to mitigate the effects of positional biases \cite{zong_fool_2024}.
To compare the different quantifiers and ensure that the VLMs do not produce irrelevant output, we extract the log probabilities of the labels rather than allowing VLMs to generate a response.
Note that, differently from \S\ref{sec:logprobs} and \S\ref{sec:model_judgments}, the VLMs are now presented with {\em all} statements before being prompted for a response. 

In Figure~\ref{fig:exp3_mc}, we report the predicted log probabilities of \emph{instruction-tuned} VLMs as a function of count.
Table~\ref{tab:exp3_correlations} shows the correlation of these scores with both the human judgments and the log probabilities from Experiment~1.
It is clear that in this setup, too, InstructBLIP fails to differentiate between the various quantified statements. 
However, while Molmo behaved similarly in Experiment~1, it distinguishes between quantifiers in the current setting.
For Molmo and the two LLaVA models, count influences predictions for \textit{many/a lot of} and for \textit{few/a few} in the expected direction. 
This is most pronounced in the lower count ranges. 
Patterns for \textit{some} once again differ from those found in our earlier experiments. While probabilities for {\em some} generally fell between those of \textit{few} and \textit{a few} in Experiment~1, and {\em some} was generally judged appropriate in Experiment~2, we now observe that it follows the same trend as \textit{few} and \textit{a few}, while being slightly preferred over these two by LLaVA-OneVision.

\paragraph{Interim conclusion}
The two LLaVA models and Molmo show moderate correlation with human scores in \dataset.
They also correlate with their log probabilities from Experiment~1.
These models are also the most self-consistent.
While Molmo is not self-consistent, in the multiple-choice setup it correlates better with human ratings.

\begin{table}
\centering
\small
\setlength{\tabcolsep}{5pt}

\begin{tabular}{ll|ccccc}
\toprule
&& \textbf{\textit{few}} & \textbf{\textit{a few}} & \textbf{\textit{some}} & \textbf{\textit{many}} & \textbf{\textit{a lot of}}\\\midrule

\multirow{2}{*}{\rotatebox[origin=c]{90}{\textbf{INB}}}
    & $r(\textup{VAQ})$ & 0.00 & 0.00 & 0.01 & -0.01 & 0.04 \\
    & $r(\textup{EXP1})$ & \textbf{-0.13} & \textbf{-0.14} & \textbf{-0.12} & \textbf{-0.13} & \textbf{-0.15}\\
    \midrule

\multirow{2}{*}{\rotatebox[origin=c]{90}{\textbf{LLN}}} 
    & $r(\textup{VAQ})$ & \textbf{0.32} & \textbf{0.27} & \textbf{0.14} & \textbf{0.42} & \textbf{0.33}\\
    & $r(\textup{EXP1})$ & \textbf{0.36} & \textbf{0.35} & \textbf{0.26} & \textbf{0.44} & \textbf{0.35}\\
    \midrule
\multirow{2}{*}{\rotatebox[origin=c]{90}{\textbf{LLO}}} 
    & $r(\textup{VAQ})$ & \textbf{0.45} & \textbf{0.45} & \textbf{0.19} & \textbf{0.35} & \textbf{0.43}\\
    & $r(\textup{EXP1})$ & \textbf{0.33} & \textbf{0.42} & \textbf{0.24} & \textbf{0.35} & \textbf{0.42} \\
    \midrule
\multirow{2}{*}{\rotatebox[origin=c]{90}{\textbf{MOL}}} 
    & $r(\textup{VAQ})$ & \textbf{0.26} & \textbf{0.31} & \textbf{0.15} & \textbf{0.28} & \textbf{0.35} \\
    & $r(\textup{EXP1})$ & \textbf{0.25} & \textbf{0.28} & \textbf{0.25} & \textbf{-0.07} & \textbf{-0.12}
    \vspace{1.25pt}\\
\bottomrule

\end{tabular}
\caption{\textbf{Pearson's $\bm{r}$ of log probabilities in Experiment~3 with human data (VAQ) and log probabilities from Experiment~1 (EXP1)}. Models shown are InstructBLIP (\textbf{INB}), LLaVA-NeXT (\textbf{LLN}), LLaVA-OneVision (\textbf{LLO}) and Molmo (\textbf{MOL}). Boldfaced numbers are statistically significant.}
\label{tab:exp3_correlations}
\end{table}