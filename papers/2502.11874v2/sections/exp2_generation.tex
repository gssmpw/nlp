\section{Experiment 2: Generating Judgments}\label{sec:model_judgments}
We now evaluate the instruction-tuned VLMs using an approach that is more akin to the way \dataset was constructed in \S\ref{sec:dataset}. 
That is, we prompt the models to explicitly rate the acceptability of quantified statements.
We experimented with 10 different prompts that are variations on the question shown to human participants in \S\ref{sec:dataset_humans}. 
Drawing inspiration from prompts used by \citet{belem2024-uncertainty}, we center our analyses in the remainder of this section around the following prompt: ``On a scale of 0 (completely inaccurate) to 100 (completely accurate), how accurate is the following statement for the image? Please respond with one of the following options: 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100. \verb|[Statement]|'', where \verb|Statement| is an expression from the \dataset dataset. 
We refer to Appendix~\ref{appendix:exp2} for the complete list of prompts tested.

\paragraph{For VLMs, appropriateness is not gradable}
We find that in this evaluation setup, BLIP-2 and InstructBLIP generally fail to generate numerical responses to the prompts we tested, despite some prompts explicitly encouraging them to only respond with a number. 
The two LLaVA models and Molmo consistently provide numerical responses to most of the prompt templates tested. 
However, while we construct the prompts in such a way that VLMs are  encouraged to provide a response that falls \emph{between} a certain range, the vast majority of model responses tend towards the extremes (i.e.\ on the lower or upper bound of the specified range; see Appendix~\ref{appendix:exp2} for a distribution of responses). 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/exp2_score_generation.pdf}
    \caption{\textbf{Scores generated by VLMs in Experiment~2.} Note that we do not display results for BLIP-2 and InstructBLIP, as those models generally failed to provide numerical responses to the prompt.}
    \label{fig:exp2_score_generation}
\end{figure}

\paragraph{\textit{Some} is generally appropriate}
When numeric answers to prompts tend towards the extremes of a scale, it can be informative to aggregate generated scores, which is virtually the same as calculating the \emph{relative frequency} of a VLM dis/agreeing with the statements. 
We report this in Figure~\ref{fig:exp2_score_generation} for object count and make the following observations. 
First, statements containing the quantifier \textit{few} are rarely deemed appropriate.
For the models in the LLaVA family, arguably the most interesting deviation from Figure~\ref{fig:exp1_logprobs} is that in this setting, \textit{some} is considered an accurate quantifier, regardless of object count. 
Indeed, we observe that the trajectory of \textit{some} in Figure~\ref{fig:exp2_score_generation} corresponds to that of the unquantified condition, for which the statements are, as anticipated, generally accepted. 
We hypothesize that in the case of \emph{judging} the appropriateness of \textit{some}, this vague quantifier could be interpreted as an \emph{existential} quantifier. 
That is, ``There are some apples in the image'' can be regarded as a confirmation of the existence of apples in the image. 

\paragraph{Interim conclusion}
Experiment~1 showed that object count has an influence on model predictions for \textit{many} and \textit{a lot of}.
Similar patterns emerge in Figure~\ref{fig:exp2_score_generation}, where average scores for these quantifiers increase with count.
Discrepancies between results from Experiments~1 and 2 show that in a setting where models are explicitly required to provide judgments for statements (Exp~2), the outcomes are unrelated to the models' log probabilities for the same statements (Exp~1). In Experiment~1, log probabilities are extracted using an autoregressive method compatible with the pretraining objective of the LLM backbone of a VLM. In contrast, Experiment~2 relies on model abilities acquired during post-training, which further modifies model parameters. The discrepancies we observe align with independent observations that post-training can negatively impact model calibration \cite{kalai_calibrated_2024,zhu_calibration_2023}.