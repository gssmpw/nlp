\section{Introduction}
Everyday conversations are replete with statements containing vague quantifiers, such as ``There are {\em many} horses'' (Figure~\ref{fig:intro_example}). Despite the fact that they are vague, they cause surprisingly little misunderstanding among interlocutors \citep{jucker2003-vagueness}. 
Vague quantifiers, unlike \emph{crisp} quantifiers, allow for borderline cases in which it is unclear whether the quantifier applies or not, and where we can also expect some variation in the extent to which speakers would use it. 
For example, \textit{all} does not allow for borderline cases, but it is unclear when a quantity ceases to be \textit{a few} or how many \textit{many} is.
Despite the fact that vague quantifiers have long been a subject of investigation among formal semanticists \citep[see e.g.][]{nouwen2010-quantifier} and (psycho)linguists \citep[e.g.][]{moxey1993-book,deemter2010-vagueness}, they have received relatively little attention in the field of natural language processing (NLP).


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/intro_example.pdf}
    \caption{\textbf{Experiments in this work.} We (\textbf{1}) ask human participants to rate, using a slider, the appropriateness of statements containing  vague quantifiers in relation to images. We (\textbf{2}) extract VLM generation probabilities for those same statements, (\textbf{3}) prompt the models to generate an accuracy score for them and (\textbf{4}) evaluate probabilities assigned to these statements in a multiple-choice setup. The image above is originally from the FSC-147 dataset \citep{ranjan2021-fsc147}.}
    \label{fig:intro_example}
\end{figure}

In visually grounded settings, the use of vague quantifiers can be influenced by factors related to the scene itself, such as the number of entities observed (e.g.\ \citealp{coventry2005-grounding}) as well as their sizes \citep{hormann1983-calculating,coventry2010-space}, but also by information like the speaker's and hearer's personal beliefs and attitudes \citep{moxey2000-quantities,jucker2003-vagueness}. This broad range of factors, coupled with their vagueness,
raises the question of how well computational models of language are able to capture human patterns in the comprehension and use of such expressions. In this paper, we explore this question with vision and language models (VLMs) in multimodal settings involving quantified statements about images.
The inclusion of a vision modality allows us to provide context in the form of both visual and textual information \citep{zhang2024-vlsurvey,ghosh2024-frontier}. Our work follows the spirit of recent research exploring the grounding abilities of VLMs
\cite[e.g.][]{zellers2019-visualcommonsense,thrush2022-winoground,zhang2022-commonsense,parcalabescu2022-valse,chen_bla_2023,kamath_hard_2024,wang_mementos_2024}. 
We present VAQUUM, a new dataset which pairs images with human judgments on the acceptability of quantified statements. We also examine to what extent visual cues influence state-of-the-art VLMs' understanding and production of expressions containing vague quantifiers, and how this compares to human linguistic intuitions (Figure~\ref{fig:intro_example}).

The contributions of this paper are as follows.\footnote{Code and data will be publicly available.}

\begin{itemize}
    \item We release VAQUUM (\textbf{Va}gue \textbf{Qu}antifiers with H\textbf{um}an Judgments), a new dataset pairing images of different types of objects with their counts, as well as human judgments of different quantified statements corresponding to the image. 
    \item We analyze the features of the visual context that influence both human and model judgments on the appropriateness of different vague quantifiers, including counts, the segmentation area occupied by the target objects, and aspects of world knowledge such as their normative size.
    \item We show that VLMs do, to some extent, follow human patterns in judging the appropriateness of vague quantifiers, but instruction-tuned models generally align better. However, the behavior of models and their degree of alignment with human judgments depends on the evaluation paradigm used. 
\end{itemize}