\section{Discussion}\label{sec:discussion}

\paragraph{Alignment with humans}
In this paper, we explore how vision-and-language models produce and evaluate simple expressions containing vague quantifiers. 
We constructed the \dataset dataset and used this to investigate whether object count, segmentation area and size norm affect VLMs to the same extent as they do humans. 
We showed that in particular for object count, the patterns found in some VLMs show striking similarities with the human data in \dataset. This result appears to contradict the observation that VLMs perform poorly on counting tasks \citep{parcalabescu2021-counting, parcalabescu2022-valse}. However, our findings with vague quantifiers could be accounted for in terms of an \emph{approximate number system}, which cognitive scientists have posited to account for the human ability to rapidly estimate quantities \citep{feigenson2004-number, condry2008-number, dehaene2011-numbersense,odic2018-ans,piantadosi_rational_2016}. 
In the context of vague quantifiers, it has been argued that there exists a mapping between exact and approximate number systems \citep{coventry2005-grounding, coventry2010-space}. The extent to which VLMs rely on something akin to an ANS is a topic for future work.

\paragraph{Self-consistency}
Our experiments relied on paradigms incorporating \emph{production} (Experiment~1) and \emph{judgment} (Experiments~2 and 3). We find that VLMs are \emph{not} self-consistent across these evaluation paradigms. 
That is to say, when a VLM is set to \emph{judge} the use of a quantifier---a meta-linguistic task---its judgment is not rooted in the log probabilities that govern the model's generation of the quantifier.

\paragraph{Outlook}
Psycholinguistics has shown that vague quantifiers do not depend exclusively on the count and size of target objects.
This is further confirmed by the residual variance (49.7\%) in \dataset that cannot be explained by the linear mixed effects model (LMM) on human judgments.
While the LMM analysis yields a better fit for VLM log probabilities, we find that there, too, the LMM cannot explain all the variance (leaving a residual variance of 8.8\% for LLaVA-OneVision). 
Future work could focus on other contextual factors, such as the number of \emph{other} objects present, the object density in the image, as well as the role of scene semantics and other objects in the image background. 
In combination with visual grounding capabilities, it is worthwhile to investigate the role of commonsense and world knowledge in vague quantifier usage: 
while seeing 20 people at a conference will most likely not be reason for one to exclaim that there are \emph{many}, the same amount of toddlers at such an event might be.

\section*{Limitations} 
\paragraph{Model selection}
Our experiments focus on a selection of vision-and-language models.
While this selection has allowed us to compare models from the same model family (BLIP-2 and InstructBLIP; LLaVA-NeXT and LLaVA-OneVision), as well as models that share similar language model backbones (LLaVA-OneVision and Molmo), 
conclusions drawn in this study can be better generalised with experiments on a wider range of VLMs. We hope that \dataset provides the impetus for further model comparisons.

\paragraph{Segmentation area and size norm}
Given that of the three contextual variables, the role of object count has been most prominent in literature on vague quantifiers, we focused on selecting images that balance a range of counts that we deemed representative. 
Estimating the segmentation area and extracting the size norms for these images may subsequently have yielded distributions that do not represent the full range of values that these variables can take on.
It is therefore possible that the distributions for segmentation area and size norm were too sparse to say something more meaningful about their roles in \dataset and model results. 
Thus, while we at times find statistically significant relationships between judgments and segmentation area or size norm, future work could focus on investigating the \emph{practical} significance.
Additionally, we recognize that using CLIPSeg to estimate the segmentation area can introduce inaccuracies.

\paragraph{Variance in human judgments}
By aggregating human judgments through simply taking the average and focusing on general trends, we might overlook meaningful variability that emphasize the complexity of human judgments on vague expressions. 
While the aim of this work was to investigate whether VLMs can approximate general patterns in human data, we believe that \dataset is a dataset that can contribute to the study of  disagreement among human annotators.


\section*{Ethical Considerations}
The data collection study for \dataset underwent an ethics check in our institution. The data collected via crowdsourcing does not contain any information that can be traced back to individuals. No materials were used to our knowledge which could harm or otherwise adversely affect individuals.