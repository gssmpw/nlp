\begin{figure*}[h]
    \centering
    \includegraphics[width=0.92\linewidth]{figs/logprob_results.pdf}
    \caption{\textbf{Log probabilities as functions of count, segmentation area and size norm.} The patterns reported for LLaVA-NeXT and LLaVA-OneVision are most similar to human ratings. We find that InstructBLIP and Molmo do not distinguish between the quantifiers at all, while BLIP-2 moderately correlates with humans for \textit{many}/\textit{a lot of}.}
    \label{fig:exp1_logprobs}
\end{figure*}

\section{Experiment 1: Production Probabilities}\label{sec:logprobs}
Our first series of experiments studies the predicted \emph{production probabilities} of quantified statements by SOTA VLMs. We prompt the models with 
``How would you describe the amount of \verb|[OBJECT]| in the image?''
We extract log probabilities, conditioned on this prompt and the image, 
for the quantified statements in \dataset, as well as the unquantified version.
All extracted scores are normalized by token length. We consider the following models.

\begin{description}[nolistsep, noitemsep, wide]

    \item[BLIP-2]\citep{li2023-blip2}\textbf{.} We use the checkpoint powered by OPT-6.7B \citep{zhang2022-opt} connected to a EVA-CLIP ViT-g \citep{radford2021-clip, fang2023-eva} image encoder via a lightweight Query transformer. 
    
    \item[InstructBLIP]\citep{dai2023-instructblip}\textbf{.} We use the checkpoint with a Vicuna-13B \citep{zheng2023-vicuna} language backbone, instruction-tuned on BLIP-2.

    \item[LLaVA-NeXT]\citep{liu2024-llavanext}\textbf{.} 
    We use the 7B checkpoint with a Mistral \citep{jiang2023-mistral} language backbone.

    \item[LLaVA-OneVision]\citep{li2024-llavaov}\textbf{.} We utilize the 7B checkpoint, which integrates a SigLIP \citep{zhai2023-siglip} vision encoder with a Qwen2 \citep{yang2024-qwen2} language decoder. 
     
    \item[Molmo]\citep{deitke2024-molmo}\textbf{.} We use the 7B-D checkpoint, which connects a ViT image encoder to Qwen2.7B via a connector MLP.
\end{description}

Figure \ref{fig:exp1_logprobs} displays predicted log probabilities as a function of count, segmentation area and size norm and Table~\ref{tab:exp1_correlations} reports correlations between model predictions and human judgments.


\paragraph{Alignment with humans}
Of the VLMs tested, the two LLaVA models exhibit the highest
correlation with the human data in \dataset. 
For these models, we observe in Figure~\ref{fig:exp1_logprobs} patterns similar to those of \dataset in Figure~\ref{fig:human_results}. 
Probabilities for \textit{many} and \textit{a lot of} increase as a function of count, while \textit{few} and \textit{a few} show a downward trend. Given that the question in the prompt focused explicitly on the {\em amount} of objects, the unquantified sentence is expected to be generally dispreferred. The trends in Figure~\ref{fig:exp1_logprobs} suggest
that the LLaVA models can indeed draw this distinction between quantified and unquantified statements, as the unquantified expression displays lowest-ranking log probabilities across count, segmentation and size norm. 
However, other models do not reveal that same ability. 
This is most pronounced for InstructBLIP and Molmo, which generally tend to favor the unquantified statement as a response to the question. 
These models also show the same pattern across all quantifiers, further confirming their inability to differentiate among them. 
While the behavior of BLIP-2 is seemingly random, Figure~\ref{fig:exp1_logprobs} shows an upward trend for all quantifiers as a function of count.

\begin{table}
    \centering
    \small
    \begin{tabular}{l|ccccc}
    \toprule
    \textbf{Model} & \textbf{\textit{few}} & \textbf{\textit{a few}} & \textbf{\textit{some}} & \textbf{\textit{many}} & \textbf{\textit{a lot of}} \\ \midrule
        BLIP-2          & \textbf{-0.18} & \textbf{-0.19} & \textbf{-0.06} & \textbf{0.14} & \textbf{0.13} \\
        InstBLIP    & 0.06 & 0.04 & -0.03 & -0.01 & -0.04 \\
        LLaVA-N      & \textbf{0.34} & \textbf{0.39} & \textbf{0.21} & \textbf{0.43} & \textbf{0.52} \\ 
        LLaVA-O & \textbf{0.30} & \textbf{0.40} & \textbf{0.22} & \textbf{0.52} & \textbf{0.54} \\
        Molmo           & \textbf{0.16} & \textbf{0.20} & \textbf{0.07} & \textbf{-0.17} & \textbf{-0.21} \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Pearson's correlation between human ratings and model log probabilities.} Numbers in boldface are statistically significant ($p < 0.05$).}
 \label{tab:exp1_correlations}
 \end{table}
 
\begin{table}
    \centering
    \small
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{l|ccccc|c}
    \toprule 
        & \textbf{\textit{few}} & \textbf{\textit{a few}} & \textbf{\textit{some}} & \textbf{\textit{many}} & \textbf{\textit{a lot of}} & \textbf{ME} \\\midrule
        \textbf{C}  & 0.00 & -0.01 & -0.02 & \textbf{0.22} & \textbf{0.22} & \textbf{-0.09} \\
        \textbf{SG} & -0.02 & -0.01 & 0.01 & \textbf{0.07} & \textbf{0.05} & \textbf{-0.05} \\
        \textbf{SN} & \textbf{0.04} & \textbf{0.05} & -0.03 & \textbf{0.12} & \textbf{0.09} & \textbf{-0.05} \\\midrule
        \textbf{ME} & \textbf{0.39} & \textbf{1.68} & \textbf{0.77} & \textbf{2.46} & \textbf{2.32}\\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Estimates of the LMM for log probabilities of LLaVA-OneVision}. 
    \textbf{C}=Count, \textbf{SG}=Segmentation, \textbf{SN}=Size norm, \textbf{ME}=Main effect.  Boldface indicates statistical significance ($p < 0.05$). For the main effects, the quantifier is releveled to the unquantified case and the estimate of the intercept is $\beta = \textbf{-1.25}$.}
    \label{tab:lmm_llavaov}
\end{table}


\paragraph{Linear mixed model}
In Table~\ref{tab:lmm_llavaov}, we display the estimates of a linear mixed effects model fit to log probabilities of LLaVA-OneVision (see Appendix~\ref{appendix:lmms} for details and Appendix~\ref{appendix:exp1} for the remaining models). 
Following our approach in \S\ref{sec:dataset}, we predict model probabilities from the fixed effects of quantifiers, count, segmentation area and size norm while including object category as a random effect. 
The latter shows a variance of 0.056, indicating that object category accounts for a moderate amount of variance among predicted log probability scores. 
Moreover, we see in Table~\ref{tab:lmm_llavaov} that \textit{many} and \textit{a lot of} show statistically significant interactions with all predictors, with the strongest effects observed with count, just as was the case for the human judgments. The estimates for the other quantifiers, however, are very different from what we found for humans. 
Overall, the LMM explains 91.2\% of the total variance in our data ($R^2m = 0.861$, $R^2c = 0.912$).

\paragraph{Prompts should target \emph{amounts}} 
For most models, we find that simply changing the question from 
``How would you describe \emph{the amount of} \verb|[OBJECT]| in the image?'' 
to ``How would you describe the image?'' 
yields different patterns in the results (see Appendix~\ref{appendix:exp1}). 
Most notably, we find that the observed similarity between trends in human judgments and model predictions disappear once the prompt does not focus on amounts.

\paragraph{Interim conclusion}
In \S\ref{sec:dataset_humans}, most estimates of the LMM fit to participant data were statistically significant. 
Moreover, object count made the biggest difference across all quantifiers.
For LLaVA-OneVision, the model displaying the highest Pearson's correlation with human data in Table~\ref{tab:exp1_correlations}, a similar result can be found in  Table~\ref{tab:lmm_llavaov} for \textit{many} and \textit{a lot of}: effects of interaction with object count are most pronounced, after which size norms have a slightly higher impact than segmentation area. However, these effects are absent for the other quantifiers.
BLIP-2, InstructBLIP and Molmo do not show meaningful interactions between their predicted log probabilities and the three contextual variables.
