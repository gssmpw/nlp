\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/appendix/exp1_no_amounts.pdf}
    \caption{\textbf{Log probabilities extracted for statements as a response to ``How would you describe the image?''} The most obvious deviation from Figure~\ref{fig:exp1_logprobs} in \S\ref{sec:logprobs} are the plots for the two LLaVA models, that no longer appear to distinguish between the different quantified statements.}
    \label{fig:app_exp1_no_amounts}
\end{figure}
\section{Supplementary Material Experiment 1}\label{appendix:exp1}

\subsection{Targeting amounts}
In Figure~\ref{fig:app_exp1_no_amounts} we show the patterns of the VLMs across all predictors for the prompt that does \emph{not} target the amount. The question presented to the models is ``How would you describe the image?'', and we extract log probabilities for expressions of the form ``There are \verb|[QUANT]| \verb|[OBJECT]| in the image'' (unchanged from those used in \S\ref{sec:logprobs}).

For LLaVA-NeXT and LLaVA-OneVision, the two models observed in \S\ref{sec:logprobs} to have the highest correlation with human ratings, we now find that patterns are the same across all quantifiers. We now find a ``layered'' or ``stacked'' pattern that is indicative of a bias towards a specific quantifier: while LLaVA-NeXT and LLaVA-OneVision tend towards always responding with \textit{a lot of}, InstructBLIP and Molmo favor the unquantified statement.


\subsection{LMMs for all remaining models}
In Table~\ref{tab:exp1_lmms_extra}, we report estimates of LMMs for BLIP-2, InstructBLIP, LLaVA-NeXT and Molmo.

\begin{table*}[t]
\centering
\small
\begin{tabular}{ll|c|c|ccccc}
\toprule
\multicolumn{2}{c|}{} 
    & \multirow{2}{*}{\textbf{Intercept}} 
    & \multirow{2}{*}{\textbf{Main}}
    & \multicolumn{5}{c}{\textbf{Quantifier}} \\
    &&&& \textbf{\textit{few}} & \textbf{\textit{a few}} & \textbf{\textit{some}} & \textbf{\textit{many}} & \textbf{\textit{a lot of}} \\
    \midrule

\multirow{4}{*}{\textbf{BLIP-2}}
    & \textbf{Main effect} & \multirow{4}{*}{\textbf{0.41}} & -- & \textbf{-0.89} & \textbf{-0.09} & \textbf{-0.79} & \textbf{-0.26} & \textbf{-1.37} \\
    & \textbf{Count}        && 0.03 & \textbf{0.21} & 0.02 & \textbf{-0.10} & -0.03 & 0.01 \\
    & \textbf{Segmentation} && 0.03 & \textbf{0.06} & -0.04 & -0.02 & \textbf{0.09} & -0.02 \\
    & \textbf{Size norm}    && 0.02 & 0.01 & -0.03 & \textbf{-0.07} & \textbf{-0.13} & 0.06 \\ 
    \midrule

\multirow{4}{*}{\textbf{InstructBLIP}}
    & \textbf{Main effect} & \multirow{4}{*}{\textbf{0.57}} & -- & \textbf{-0.76} & \textbf{-0.82} & \textbf{-0.86} & \textbf{-0.46} & \textbf{-1.20} \\
    & \textbf{Count}        && -0.02 & -0.02 & -0.01 & -0.01 & 0.03 & 0.00 \\
    & \textbf{Segmentation} && \textbf{-0.11} & -0.01 & 0.00 & 0.00 & 0.00 & 0.02 \\
    & \textbf{Size norm}    && \textbf{0.33} & \textbf{-0.08} & \textbf{-0.09} & \textbf{-0.06} & 0.02
        & \textbf{-0.09} \\
    \midrule

\multirow{4}{*}{\textbf{LLaVA-NeXT}}
    & \textbf{Main effect} & \multirow{4}{*}{\textbf{-0.86}} & -- & \textbf{-0.05} & \textbf{1.00} & \textbf{0.31} & \textbf{2.10} & \textbf{2.08} \\
    &\textbf{Count}         && \textbf{-0.12} & \textbf{-0.03} & \textbf{-0.07} & \textbf{-0.04} & \textbf{0.21} & \textbf{0.26} \\
    & \textbf{Segmentation} && \textbf{-0.12} & 0.00 & -0.03 & 0.02 & \textbf{0.14} & \textbf{0.13} \\
    & \textbf{Size norm}    && \textbf{-0.08} & \textbf{0.08} & \textbf{0.12} & \textbf{0.03} & \textbf{0.15} & \textbf{0.15} \\
    \midrule

\multirow{4}{*}{\textbf{LLaVA-OneVision}}
    & \textbf{Main effect} & \multirow{4}{*}{\textbf{-1.25}} & -- & \textbf{0.39} & \textbf{1.68} & \textbf{0.77} & \textbf{2.46} & \textbf{2.32} \\
    & \textbf{Count}        && \textbf{-0.09} & 0.00 & -0.01 & -0.02 & \textbf{0.22} & \textbf{0.22} \\
    & \textbf{Segmentation} && \textbf{-0.05} & -0.02 & -0.01 & 0.01 & \textbf{0.07} & \textbf{0.05} \\
    & \textbf{Size norm}    && \textbf{-0.05} & \textbf{0.04} & \textbf{0.05} & -0.03 & \textbf{0.12} & \textbf{-0.09} \\
    \midrule

\multirow{4}{*}{\textbf{Molmo}}
    & \textbf{Main effect} & \multirow{4}{*}{\textbf{0.73}} & -- & \textbf{-0.71} & \textbf{-0.97} & \textbf{-1.35} & \textbf{-0.85} & \textbf{-1.30} \\
    & \textbf{Count}        && \textbf{-0.11} & 0.03 & 0.03 & \textbf{-0.05} & 0.02 & 0.04 \\
    & \textbf{Segmentation} && \textbf{-0.19} & 0.00 & 0.00 & -0.01 & 0.02 & 0.03 \\
    & \textbf{Size norm}    && \textbf{0.22} & -0.01 & -0.04 & -0.05 & 0.01 & \textbf{-0.06} \\
    \bottomrule

\end{tabular}
\caption{\textbf{Linear Mixed Effects estimates for all VLMs tested.} We discuss the estimates for LLaVA-OneVision in \S\ref{sec:logprobs}.}
\label{tab:exp1_lmms_extra}
\end{table*}

