\section{Related Works}
\subsection{Collaboration Mechanisms}

Traditionally, many studies on human-AI collaboration focus on using a model to determine whether to apply AI or human experts to get the final decision. ____ proposed a two-stage framework containing an automated model and an external decision-maker as in rejection learning ____, and generalizes rejection learning by considering the effect of other agents in the decision-making process. ____ further explored the method to learn a predictor that can either predict or choose to defer the decision task to a downstream expert. They also developed a novel reduction to cost sensitive learning where a consistent surrogate loss is given. ____ pointed out that the multi-class framework in ____ is not calibrated with respect to expert correctness, and propose an L2D system based on one-vs-all classifiers that is able to produce calibrated probabilities of expert correctness. Although many studies focus on building calibrated predictors, ____ show that even when the original AI model is well-calibrated, AI models as more confident than they actually are can improve human-AI performance.
Moreover, ____ argue that the most accurate AI is not necessary the best teammate in AI advised decision making, and AI systems should be trained in a human-centered manner, directly optimized for team performance. ____ develop a set of methods that combine the probabilistic output of a model with the class-level output of a human. They show that the accuracy of the combination model is driven by not only the accuracies of individual human and model, but also by the model's confidence. 

% ____ investigates a two-stage scenario for learning to defer with multiple experts, where a predictor is derived in the first stage by training with a common loss function, and a deferral function is learned in the second stage to assign the most suitable expert to each input.

Some other works are dedicated in the case of ddecision makers. ____ point out that in real-world situations several human experts with varying capabilities may be available, and develop a method that trains a classifier to complement the capabilities of multiple human experts by allocating each instance to the most suitable team member. ____ show that combining multiple human labels with the model's probabilistic output can lead to significant improvement in accuracy, which is achieved by an approach to merge the predicted labels from multiple humans with the model's probabilistic output. 
____ investigates how herding behavior affects the accuracy of consensus forecasts on a crowd-based corporate earnings platform, and concludes that encouraging independent forecasting and limiting information access can enhance the wisdom of crowds. ____ conducts a comprehensive comparison of 58 prominent models of risky choice using 19 behavioral datasets, revealing that model crowds outperform individual models in predicting choice behavior, suggesting that combining multiple models provides a more accurate understanding of decision-making processes.
____ introduces a
statistical method for assessing ensembles of any size and
investigates the optimal human-machine ratio as well as the
impact of ensemble size on the performance of human-machine
collaboration. ____ presents a study of surrogate losses and algorithms for the general problem of learning to defer with multiple experts.
% , and developed a new family of surrogate losses specifically tailored for the multiple-expert setting. 
____ further explores a two-stage learning scenario for deferring decisions to multiple experts, where a predictor is trained using a common loss function, and subsequently a deferral function is learned to assign inputs to the most suitable expert. 

% ____ propose a technique that optimizes the cost of seeking the expert's advice while using the AI model to improve accuracy, where both the expert's cost for each prediction and the misclassification cost of the combined human-AI model are considered. 

% ____ point out that the popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring which makes them uncalibrated. They show that the cause of this issue is due to the symmetric nature of the surrogate losses used and not due to softmax, and propose a novel statistically consistent asymmetric softmax-based surrogate loss that can produce valid estimates without the issue of unboundedness. 

Some papers focus on theoretical analysis of collaborative decision making. ____ analyzes the optimal composition of a forecasting group based on type coherence, where forecasters of the same type share expected accuracy and higher covariance. ____ presents a Bayesian framework for integrating predictions and confidence scores from both humans and AI algorithms to enhance performance in classification tasks, which demonstrates that hybrid systems can outperform individual human or machine predictions even if their accuracy levels differ, as long as these differences are within a certain bound. ____ explores the dynamics of multi-human and multi-machine interactions within hybrid human-machine systems and introduces a novel statistical framework to assess integration accuracy. 

\subsection{Capability Modeling.}

There exists many metrics for evaluating human capabilities. 
For instance, the Intelligence Quotient (IQ) and Emotional Intelligence Quotient (EQ) are widely utilized to quantify the two major aspects of human intelligence ____. In addition to these single-value metrics, various studies have employed multi-dimensional representations to model human personality traits, such as the Big Five personality model and the Myers-Briggs Type Indicator ____. 
% There are also language tests TOFEL, IELTS, PETS, and professional competence tests such as Bar Exam and CPA exam, which also evaluate human's capabilities in different fields. 
Furthermore, language proficiency assessments like TOEFL, IELTS, and PETS, along with professional competency examinations such as the Bar Exam and CPA exam, evaluate human capabilities across different domains. Simultaneously, there is a growing body of research focused on assessing the capabilities of AI models. Task-specific models are typically evaluated using metrics relevant to their respective tasks, such as accuracy or recall for classification tasks, and BLEU or ROUGE scores for language generation tasks. For models designed to perform multiple tasks, their capabilities can be assessed through a combination of metrics derived from these various tasks.
% For task-specific models, they can generally be evaluated by corresponding task-related metrics, such as accuracy or recall for classification tasks, BLEU or Rouge for language generation tasks. For the models capable of doing multiple tasks, the capability can be evaluated by the set of multiple metrics from these tasks. 
Additionally, large language models (LLMs), which are capable of executing a wide range of language-based tasks akin to human performance, often require human evaluation for comprehensive assessment. Numerous benchmarks have been established for evaluating LLMs ____. Some studies have even explored the application of IQ as a metric for evaluating AI models ____. 

As AI models increasingly exhibit capabilities that overlap with those of humans, it becomes imperative to develop unified evaluation metrics for both entities. Such metrics should extend beyond singular indices like IQ and may take the form of learnable vectors, akin to token embeddings used in natural language processing. 
% We contend that establishing these unified evaluation frameworks is crucial for developing a robust human-AI collaborative system with enhanced generalization capabilities.
% This can be more than a single index such as IQ, but can be learnable vectors just like token embeddings for a vocabulary. 
We believe this is crucial for developing a robust human-AI collaborative system with enhanced generalization capabilities.