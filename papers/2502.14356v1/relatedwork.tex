\section{Related Work}
\paragraph{Mathematical Reasoning}
Mathematical reasoning task is one of the most challenging tasks for LLMs. Various approaches have been explored to improve or elicit the mathematical reasoning ability of LLMs. 
A number of approaches have either continually pre-trained the base model on a vast of mathematical datasets \cite{llemma, deepseekmath} or used supervised fine-tuning with substantial synthetic datasets distilled from cutting-edge models \cite{wizardmath, metamath, orcamath, chatglm}.
Another line of work focuses on enhancing test-time computation by generating multiple solutions, developing separate reward models at either the outcome or process level to rerank these solutions \cite{cobbe2021training, lightman2023let}, or employing decoding strategies guided by the reward model \cite{ovm, xie2024self, mathshepherd, wu2024enhancing}.
In addition, Reinforcement Learningâ€™s potential in general domains, demonstrated by \citet{gpt4} and \citet{llama}, some studies have explored its use in mathematical reasoning \cite{mathshepherd, orcamath, smaug}.



\paragraph{Preference Learning}
Recently, preference learning \cite{kto, mao2024don, mao2024simple} has attracted significant attention due to its ability to align with human preferences and distinguish between positive and negative examples. While these methods, like DPO \cite{dpo}, have proven effective in general domains, it offers only marginal benefits for mathematical reasoning \cite{smaug}. Some works \cite{alistep, cuhkstep} suggest that DPO's focus on coarse solution-wise preferences makes it less effective at correcting errors in multi-step reasoning, hindering reasoning improvement. Therefore, Step-DPO \cite{cuhkstep} was proposed, which first identifies the first erroneous step, and then optimizes only this erroneous step along with the corresponding correct one. Although this approach enhances mathematical reasoning capabilities, it totally overlooks the other steps in long-chain reasoning, which also provide valuable information and should not be completely disregarded. Building on this consideration, we propose Full-Step-DPO, which fully accounts for each step by dynamically optimizing all steps in the reasoning process. 


\paragraph{Step-wise Supervision}
Recent findings by \citet{lightman2023let} suggest that step-wise supervision outperforms outcome-wise, due to the provision of more detailed feedback. 
However, training a PRM requires either costly manual annotation \cite{lightman2023let} or significant computational resources \cite{grace, mathshepherd}, which hinders the advancement and practical application of PRM.
Therefore, in this paper, we aim to build a PRM for mathematical reasoning without relying on human annotation and with reduced computational resources. 
Additionally, we explore the effectiveness of the PRM in decoding and preference learning scenarios.