\section{Related Work}
\paragraph{Mathematical Reasoning}
Mathematical reasoning task is one of the most challenging tasks for LLMs. Various approaches have been explored to improve or elicit the mathematical reasoning ability of LLMs. 
A number of approaches have either continually pre-trained the base model on a vast of mathematical datasets ____ or used supervised fine-tuning with substantial synthetic datasets distilled from cutting-edge models ____.
Another line of work focuses on enhancing test-time computation by generating multiple solutions, developing separate reward models at either the outcome or process level to rerank these solutions ____, or employing decoding strategies guided by the reward model ____.
In addition, Reinforcement Learningâ€™s potential in general domains, demonstrated by ____ and ____, some studies have explored its use in mathematical reasoning ____.



\paragraph{Preference Learning}
Recently, preference learning ____ has attracted significant attention due to its ability to align with human preferences and distinguish between positive and negative examples. While these methods, like DPO ____, have proven effective in general domains, it offers only marginal benefits for mathematical reasoning ____. Some works ____ suggest that DPO's focus on coarse solution-wise preferences makes it less effective at correcting errors in multi-step reasoning, hindering reasoning improvement. Therefore, Step-DPO ____ was proposed, which first identifies the first erroneous step, and then optimizes only this erroneous step along with the corresponding correct one. Although this approach enhances mathematical reasoning capabilities, it totally overlooks the other steps in long-chain reasoning, which also provide valuable information and should not be completely disregarded. Building on this consideration, we propose Full-Step-DPO, which fully accounts for each step by dynamically optimizing all steps in the reasoning process. 


\paragraph{Step-wise Supervision}
Recent findings by ____ suggest that step-wise supervision outperforms outcome-wise, due to the provision of more detailed feedback. 
However, training a PRM requires either costly manual annotation ____ or significant computational resources ____, which hinders the advancement and practical application of PRM.
Therefore, in this paper, we aim to build a PRM for mathematical reasoning without relying on human annotation and with reduced computational resources. 
Additionally, we explore the effectiveness of the PRM in decoding and preference learning scenarios.