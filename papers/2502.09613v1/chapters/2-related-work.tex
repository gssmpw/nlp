\section{Related Work}
\textbf{Injecting 3D priors into 2D representations.}
While many existing works focus on incorporating 2D features into 3D representations, which improves performance in downstream tasks such as scene understanding~\citep{Zhi:etal:ICCV2021,ha2022semabs,qin2023langsplat,shi2023language,zhou2024feature,cen2023saga,gu2024egolifter,guo2024semantic}, less attention has been paid to the opposite direction: leveraging 3D knowledge to enhance 2D features, which benefit challenging tasks that require 3D understanding while the perceived information is limited such as monocular depth estimation \citep{stan2023ldm3d,bhat2023zoedepth,piccinelli2024unidepth,Chatterjee_2024_CVPR,moon2023ground} and semantic segmentation \citep{wang2023one,sun2023corrmatch}. Studies such as~\citep{bachmann2022multimae,zhou2024feature} utilize 3D priors from multi-view and geometric information to improve the Masked Autoencoders ~\citep{MaskedAutoencoders2021}, achieving better performance on downstream tasks of segmentation and detection. 
However, directly injecting the geometry constraints into the pre-trained feature extractors is harmful for the self-supervised 2D representation and  heavily relying on pre-trained feature extractors poses potential limitations for performance and requires significant computational resources. In contrast, our method does not require any additional per-scene refinement module, serving as an efficient and generalizable approach for injecting 3D priors into 2D representations. 

\textbf{Radiance field representations on images and features.}
% 
Neural Radiance Fields (NeRF)~\citep{mildenhall2020nerf} and 3D Gaussian Splatting (3DGS)~\citep{mildenhall2020nerf} are benchmark radiance field representation methods for the NVS task.
NeRF represents 3D scenes and renders photorealistic novel views based on the representation capacity of neural networks. 3DGS employs a set of 3D Gaussian primitives to represent 3D scenes, and a fast differentiable rasterizer to enable more efficient rendering while keeping the photorealism of novel views.
However, the distillation of the 2D features into the 3D representations remains challenging, mainly due to the significant geometric inconsistency in the feature maps caused by massive high-frequency information. Therefore, some recent literature \citep{zhou2024feature, kobayashi2022distilledfeaturefields,Siddiqui_2023_CVPR,fan2022nerf,lerf2023} propose alternative solutions by leveraging the geometry information from the RGB space to help the 3D reconstructions of 2D features. Fit3D \citep{yue2024improving} builds a huge amount of 3D representation dataset as the superivsion for the pre-trained feature extractor fine-tuning; however, without considering the compatibility of the 3D representation and 2D feature space, they also require a customized decoder to ensure the performance in the downstream tasks. All the methods mentioned above all rely on the per-scene optimization with additional modules, while our method bridging the gap between 2D feature space and 3D representation with an efficient correspondence-aware method.   

\textbf{Text-to-3D generation with 2D priors.} Despite the impressive 3D generation capabilities demonstrated by many existing 2D generative prior-guided works~\citep{tang2023dreamgaussian,poole2022dreamfusion,wu2023reconfusion,zhou2024dreamscene360,jain2021dreamfields,text2mesh}, performing back-propagation of the Score Distillation Sampling (SDS) loss~\citep{poole2022dreamfusion} on images is computationally intensive and time-consuming. Latent diffusion models (LDMs) offer more efficient solutions by operating in the latent space. However, the vastly different distribution of the latent space means that directly utilizing the latent representations for NVS leads to degraded rendering performance. To our knowledge, only a few works attempt to overcome this challenging task. Latent-NeRF~\citep{metzer2022latent} employs a per-scene refinement layer to map the rendered latent to RGB space as an additional constraint for training the NeRF representations. ED-NeRF~\citep{park2023ed} introduces a more complex refinement module by initializing from a set of specific layers in a Variational Autoencoder (VAE). Although these per-scene refinement modules effectively mitigate the artifacts in the rendering results, they require resource-consuming optimization for each scene, and lack generalization ability to novel views or scenes. Moreover, the smoothness introduced by the neural networks hinders the reconstruction of high-frequency signals on the 2D features. On the contrary, our method requires no additional efforts for lifting the 2D features to the 3D radiance field representations, such that it can be injected into any existing NVS or text-to-3D frameworks smoothly and efficiently.



  