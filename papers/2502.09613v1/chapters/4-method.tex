
\section{Method}


In this work, we propose a method to achieve 3D-aware 2D representations and enable 3D reconstruction in the latent space. We base our method on the widely used Variational Autoencoder (VAE) from Latent Diffusion models \citep{metzer2022latent}. To enhance the 3D awareness of both encoder and decoder of the VAE, we present a three-stage pipeline as illustrated in Fig. \ref{fig:pipeline}. The first stage focuses on improving the 3D awaresness of the VAE's encoder through a novel correspondence-aware constraint on the latent space, making the 2D representations follow the geometry consistency (Sec.~\ref{subsec: Epipolar-aware Autoencoding}); The second stage builds a latent radiance field (LRF) to represent 3D scenes from the 3D-aware 2D representations (Sec.~\ref{subsec: Latent Radiance Fields}); The third stage further introduces a VAE-Radiance Field (VAE-RF) alignment method to boost the reconstruction performance (Sec.~\ref{subsec: Radiance Field-Guided Image Decoding}). In together, our LRF enables 3D reconstruction on the 2D latent space instead of the image space. It can render high-quality and photorealistic novel views, even for the unbounded scenes (Sec. \ref{sec: exp}). More details of our method are discussed in the following sections.


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/method.png}
    \vspace{-1em}
    \caption{An illustration of  our pipeline for creating a latent radiance field in conjunction with 3D-aware 2D representation fine-tuning. 
    Firstly in Stage-I, we inject 3D awareness into the VAEâ€™s encoder through applying a novel correspondence consistency constraint on the latent space, making the 2D representations follow the geometry consistency. Then in Stage-II, we create the latent radiance field (LRF) to represent 3D scenes based on the 3D-aware 2D representations. Finally in Stage-III, we introduce a VAE-Radiance Field alignment method to enhance the performance of image decoding from the  rendered latent space.
}
\vspace{.5em}
    \label{fig:pipeline}
\end{figure}

\subsection{Correspondece-aware Autoencoding}
\label{subsec: Epipolar-aware Autoencoding}
The first stage of our method is incorporating the geometry-awareness into the autoencoding process. Given $K$ muilt-view images $\mathcal{I}=\left\{\boldsymbol{I}_i\right\}_{i=1}^K,\left(\boldsymbol{I}_i \in \mathbb{R}^{H \times W \times 3}\right)$, the VAE encoder extracts 2D representations $\mathcal{Z}=\left\{\boldsymbol{Z}_i\right\}_{i=1}^K,\left(\boldsymbol{Z}_i \in \mathbb{R}^{H' \times W' \times 4}\right)$ in a low-dimensional latent space while the semantic information can be preserved effectively. However, as shown in Fig. \ref{fig: exp_recon}, most of existing NVS frameworks fail to reconstruct the photo-realistic images from the rendered latents.
It is mainly because the VAE encoding process significantly damages the multi-view consistency within the original image space, since the latent space presents massive high-frequency noises to compress the original RGB space into a compact latent space (see Fig. \ref{fig: encoder}). 
This brings severe challenges for reconstructing the 2D latent representations in the 3D space. 




\noindent\textbf{Correspondence consistency on the latent space.}
To address the above issue and enable effective latent 3D reconstruction, we are inspired by the multi-view correspondence consistency which serves as the foundation principle for modeling the natural 3D world. Specifically, points $\boldsymbol{x}_i \in \mathbb{R}^{2}$ in image $\boldsymbol{I}_i$ and points $\boldsymbol{x}_j \in \mathbb{R}^{2}$ in another image $\boldsymbol{I}_j$ are considered correspondences if they are connected by the fundamental matrix $\boldsymbol{F}_{ij} \in \mathbb{R}^{3 \times 3}$, satisfying the multi-view geometry constraint~\citep{schoenberger2016sfm}:
\begin{equation}
\boldsymbol{x}_{j}^\top \boldsymbol{F}_{ij} \boldsymbol{x}_i = 0.
\label{eq:fundamental}
\end{equation}
Eq. \ref{eq:fundamental} tells that a pair of correspondence points on the image space should be close to each other, so that the consistent geometry can be ensured during the optimization in the 3D space; otherwise, the artifacts and redundant geometry representation due to the local optimal will damage the quality of the 3D reconstruction and novel view synthesize. 
Motivated by this, we propose an computationally efficient strategy that incorporates the correspondence consistency into the autoencoder training. 
Specifically, a set of multi-view images $\mathcal{I}=\left\{\boldsymbol{I}_i\right\}_{i=1}^K,\left(\boldsymbol{I}_i \in \mathbb{R}^{H \times W \times 3}\right)$ are fed into the autoencoder to extract the latent representations  $\mathcal{Z}=\left\{\boldsymbol{Z}_i\right\}_{i=1}^K,\left(\boldsymbol{Z}_i \in \mathbb{R}^{H' \times W '\times 4}\right)$, and the correspondence consistency loss on the latent space is computed by 
% \textcolor{red}{Give the defination of j and N, and this loss should be step loss instead of total images loss}
\begin{equation}
\mathcal{L}_{\text{corres}} =  \sum_{i=1}^{K} \sum_{j \in \mathcal{K}(i)} \lambda_{ij} \left\| \boldsymbol{z}_i - \boldsymbol{z}_j \right\|_1.
\end{equation}
where $\boldsymbol{z}_i$ refers to the the latent pixel in the $\boldsymbol{Z}_i$ and $\boldsymbol{z}_i$ is the corresponding latent pixel in the neighbouring latent  $\boldsymbol{Z}_j$.
$\mathcal{L}_{\text{corres}}$ ensures that the encoded features follow the correspondence consistency derived from the multi-view images, where $\lambda_{ij}$ is the weight based on the average pose error (APE) calculated from the Frobenius norm between the two camera poses of images $\boldsymbol{I}_i$ and $\boldsymbol{I}_j$ to weight the accurate pose distance to represent the view-dependant latent codes. The detail of calculating $\lambda_{ij}$ can be found in Appendix \ref{subsec: APE details}
By injecting the latent correspondence consistency into the standard VAE training, our VAE training objective is: 
\begin{equation} 
\mathcal{L}_\text{StageI} =\mathcal{L}_\text{VAE} + \lambda_{1}\mathcal{L}_{\text{corres}} + \lambda_{2}\mathcal{L}_{\text{reg}}.
\label{eq:encoder}
\end{equation}

$\mathcal{L}_\text{VAE}$ is original VAE traning objective for VAE in Eq. \ref{eq:vae}. 
$\mathcal{L}_{\text{reg}} = -\text{KL}\left( q(\boldsymbol{Z}|\boldsymbol{X}) \parallel q_{\text{original}}(\boldsymbol{Z}|\boldsymbol{X}) \right)$ enforces the fine-tuned 2D representations being close to those of the pre-trained VAE, preserving the representation capability of the finet-tuned autoencoder.  This new learning objective ensures that the compact latent space of VAE preserves the multi-view geometric consistency, such that it is compatible with existing NVS frameworks such as 3DGS.



\textbf{Insight into latent correspondence consistency.} 
The maximum degree of the spherical harmonics is always set as 3 in NVS methods for the efficiency and robustness in the modeling the view-dependant information. To be more specific, the lower degree terms is aim to mostly capture low-frequency information such as albedo for the scene while the higher degrees are tended to model the high-frequency, view dependent information such as the lightning. For the latent space, the latent code can be considered as the combination of the base value and high frequency noise. Due to such a compact representation, the amount of the noise can be greatly increase compared to the RGB space, creating more difficulties for the SH coefficients to model the information from different views. When maximum degree is fixed, it is easier for SH coefficients to reach the global optimal instead of locally over-fitting. Fortunately, with our $\mathcal{L}_{\text{corres}}$, the high frequency noise can be effectively removed while the high-quality image generative ability can still be preserved, leading to a more stable process of the optimization and consistent geometry representation. Fig. \ref{fig: encoder} shows that the correspondence-aware encoding can significantly remove the high frequency noises in the 2D latent space and the visualization of applying Fast Fourier transform also showing less high-frequency noise in latent space achieved by our encoder,  resulting an effective approach to lifting the 2D features into the 3D latent fields.

\begin{figure}[!t]
    \centering
    \begin{tikzpicture}
     

        \node[anchor=south west, inner sep=0] (image1) at (0,0) {\includegraphics[width=1.0\textwidth]{figures/fft.png}};
        
       
        \node[anchor=south] at (1.3, 2.0) {\small Image};               
        \node[anchor=south] at (4.15, 2.0) {\small VAE latent};         
        \node[anchor=south] at (7.0,  2.0) {\small Finetuned latent};               
        \node[anchor=south] at (9.8,  2.0) {\small VAE latent FFT};
         \node[anchor=south] at (12.55,   2.0) {\small Finetuned latent FFT};
    \end{tikzpicture}
    \vspace{-1em}
    \caption{A visualization of latent spaces of original and our fine-tuned VAEs. Our method ensures an accurate geometry in the latent space while removing a certain amount of high-frequency noises.}
\label{fig: encoder}
\end{figure}



\subsection{Latent Radiance Field}
\label{subsec: Latent Radiance Fields}



Based on the 3D-aware 2D representation fine-tuning discussed in Sec.~\ref{subsec: Epipolar-aware Autoencoding}, we create 3D representations directly in the 2D latent space of VAE, namely the latent radiance field (LRF). We take 3DGS \citep{kerbl3Dgaussians} as an example of radiance field representations to discuss our LRF.  

By following 3DGS, a set of latent 3D Gaussians is formulated as
\begin{equation}
    \mathcal{G} = \{(\bm{\mu}, \mathbf{s}, \mathbf{R}, \alpha, \mathbf{SH}_{f})_j)\}_{1\leq j \leq M} \textnormal{,}
\end{equation}
where $\bm{\mu} \in \mathbb{R}^3$ is the 3D mean of the Gaussian, $\mathbf{S} = \textnormal{diag}(\mathbf{s}) \in \mathbb{R}^{3\times 3}$ is the Gaussian scale, $\mathbf{R}\in \mathbb{R}^{3\times 3}$ its orientation, $\alpha \in \mathbb{R}$ a per-Gaussian opacity, and $\mathbf{SH}_{f}$ models the view-dependant latent in the 3D latent space. By following the differentiable rasterization process of 3DGS, we rasterize the 2D latent representations using point-based $\alpha$-blending as follows:
\begin{equation}
\mathbf{Z} = \sum_{i\in \mathcal{N}}\mathbf{z}_{i}\alpha _{i}\prod_{j=1}^{i-1}(1-\alpha _{i}),
\end{equation}
where $\mathcal{N}$ is a set of ordered Gaussians overlapping the pixel, $\mathbf{z}_{i}\in \mathbb{R}^{dim}$
is the view-dependent latent code of each Gaussian, where $\mathbf{dim}$ is the number of the latent dimension of the feature. and $\alpha _{i}$ is given by evaluating a
2D Gaussian with covariance $\mathbf{\Sigma}$ multiplied with a
learned per-point opacity. 
Let  $\mathcal{I}=\left\{\boldsymbol{I}_i\right\}_{i=1}^K,\left(\boldsymbol{I}_i \in \mathbb{R}^{H \times W \times 3}\right)$ be a set of multi-view images of a scene with corresponding camera parameters. Let $\mathcal{Z}=\left\{\boldsymbol{Z}_i\right\}_{i=1}^K,\left(\boldsymbol{Z}_i \in \mathbb{R}^{H \times W \times 3}\right)$ be a corresponding set of latents from the VAE encoder. The rasterization function $r$ renders a set of latent Gaussians into a 2D latent representation according to the camera pose $\mathbf{P}_{i}$. Then, we optimize the latent Gaussian parameters, to optimally represent
latent $\mathcal{Z}$:
\begin{equation}
    \hat{\mathcal{G}} = \argmin_{\{(\bm{\mu}, \mathbf{s}, \mathbf{R}, \alpha, \mathbf{SH}_{f}\}} \sum_{i=1}^N \mathcal{L}^f(r(\mathcal{G}, \mathbf{P}_{i}),\mathbf{Z}_i) \textnormal{,}
\end{equation}
where $\mathcal{L}^f$ is a pixel-wise $l_{1}$ loss combined with a D-SSIM term. Notably, we do not need to impose additional geometric consistency constraints introduced by previous literature~\citep{yue2024improving,kobayashi2022distilledfeaturefields,zhou2024feature}, as our correspondence-aware autoencoder fine-tuning ensures geometrically consistent 2D representations in the 3D space. Therefore, our LRF reconstructs the 2D latent representations as a radiance field representation directly, and enables an efficient rendering of the 2D latent representations for novel views.

\subsection{VAE-Radiance Field Alignment} \label{subsec: Radiance Field-Guided Image Decoding}
Although the correspoondence-aware autoencoding introduced in Sec.~\ref{subsec: Epipolar-aware Autoencoding} improves the 3D consistency of VAE latent space, the LRF distribution $\boldsymbol{p}(z_{\text{NVS}})$ are still shifted from the VAE latent distribution $\boldsymbol{p}(z_{\text{VAE}})$ due to the non-linearity in neural rendering, resulting in performance decrease when we decode LRF rendering results back to images through the VAE decoder. 

We further propose to fine-tune the VAE decoder under the radiance field guidance to address this issue. With the LRF built in Sec. \ref{subsec: Latent Radiance Fields}, we can reconstruct LRFs from a large amount of scenes to generate a latent-image paired dataset. This dataset consists of the 2D latent representations $\mathcal{Z}=\left\{\boldsymbol{Z}_i\right\}_{i=1}^K,\left(\boldsymbol{Z}_i \in \mathbb{R}^{H' \times W' \times 4}\right)$ rendered by LRFs and the corresponding ground truth images $\mathcal{I}=\left\{\boldsymbol{I}_i\right\}_{i=1}^K,\left(\boldsymbol{I}_i \in \mathbb{R}^{H \times W \times 3}\right)$. Notably, we also include the training views of LRFs in this dataset, since a key feature of existing NVS methods is to overfit the training views. 
The training objective of our VAE-RF alignment decoder fine-tuning is:
\begin{equation} 
\mathcal{L}_\text{StageIII}=  \lambda_{\text{train}} \left\|D(Z_{\text{train}}) - I_{\text{train}} \right\|_1 + \lambda_{\text{novel}} \left\|D(Z_{\text{novel}}) - I_{\text{novel}}\right\|_1,
\label{eq:decoder}
\end{equation} 
where $D(\cdot)$ is the decoder, $Z_{\text{train}}$ and $Z_{\text{novel}}$  are the latent codes of the training views and novel views, respectively. $I$ refer to the corresponding ground truth images. $\lambda_{\text{novel}}$ and $\lambda_{\text{novel}}$ are the weighting coefficient that balances the contributions of the training and novel views. Both of the weights are set to $0.5$ to ensure that the decoder learns not only to decode effectively from the training views but also to generalize and perform well on the novel views.
Eq. \ref{eq:decoder} effectively minimizes the distribution mismatch between the VAE latent space and the LRF rendering space. After decoder fine-tuning, high-quality images can be reconstructed from the LRF rendering of either training or novel views. The fine-tuned autoencoder enhances 3D reconstruction and generation by providing a geometry-aware 2D latent space as well as a radiance field-compatible autoencoder.




\input{figures/exp}
