\subsection{Details of calculating the weight}
\label{subsec: APE details}
To compute $\lambda_{ij}$, we first calculate the Absolute Pose Error (APE) for each pose pair using the formula: $E_{ij} = P_i^{-1} P_j$, where $P_i$ and $P_j$ are the different camera poses respectively. After obtaining $E_{ij}$, the APE is calculated as: 

\begin{equation}
    APE_{ij} = \|E_{ij} - I_{4 \times 4}\|_F,
\end{equation}
where $I_{4 \times 4}$ is the identity matrix $F$ and 
 represents the Frobenius norm. In each iteration, the APE values are normalized across all image pairs to derive the weights $\lambda_{ij}$, as: $\lambda_{ij} = \frac{APE_{ij}}{\sum_{k} {{APE}_{k}}},$ where $k$
 represents each image pair within one iterations. This normalization ensures they reflect the relative contributions of each pose error in a consistent manner. This method is implemented based on the APE computation approach in the evo library \citep{grupp2017evo}. 


\subsection{Details of Dataset}
\label{subsec: dataset}
We create a correspondence pair dataset based on the training set of DL3DV-10K \citep{ling2024dl3dv} dataset to fine-tune our VAE encoder. We randomly sample 784 scenes and extract correspondence pairs from the multi-view images by using COLMAP. The correspondence points for each scene will be pre-computed before the model fine-tuning process. We use a sequential matcher with the number of overlapping images set to 10 and the number of quadratic overlaps set to 1. Such overlapping searching strategy ensures our model not only learns from easy and dense correspondence, but also from challenging cases among far-view image pairs, adding great robutness for our model. The ability to remain consistency in large view difference is particularly necessary for the outdoor unbounded reconstruction. Moreover, we set the minimum number of inliers and minimum ratio of inliers to 15 and 0.25 with the loop detection to make sure the extracted correspondence is accurate enough. We also train the same number of latent 3D Gaussian splatting scenes from the DL3DV-10K datasets to create a paired dataset of images and rendered latents, which are used for Stage-III decoder fine-tuning. 


\subsection{Implementation details}
\label{subsec: imple details}
For Stage-I, we employ the pre-trained VAE model ($f=8$, $KL$), from LDM model zoo as the backbone VAE model. We fine-tune the VAE on 2 NVIDIA A100-80GB GPUs for around one day, by using the correspondence pair dataset with an image resolution of 512$\times$512, the base learning rate of ${4.5e-06}$, and the default optimizer. For Stage-III, we fine-tune the decoder on the image-latent dataset with 2 NVIDIA A100-80GB GPUs for around one day. 

In the implementation of LRF, we normalize the latent input to the radiance field using the scale of all input views to stabilize radiance field optimization, and apply denormalization during rendering. During the VAE encoding stage, we start the discriminator at step 501 for better image quality, and we set $KL_{\text{weight}} = 1.0 \times 10^{-6}$, and $\mathcal{D}_{\text{weight}} = 0.5$. For the decoder training, we use the same configuration as the original VAE, except $KL_{\text{weight}} = 0$ to ensure only the decoder was optimized.

\subsection{Image Reconstruction Performance}
To verify that our approach does not degrade the performance on downstream tasks, we evaluate the image reconstruction performance of our fine-tuned VAE by calculating PSNR between the original images and the reconstructed images. As shown in Table \ref{tab:encoding}, adding the correspondence consistency constraint to inject 3D awareness and applying a regularization loss to keep the latent space close to the original latent space perform minimal impact on the VAE's reconstruction performance. This ensures that our VAE model can still be effectively used in conjunction with other pre-trained models, such as the Stable Diffusion model, without any fine-tuning. 

\begin{table*}[h]
\centering
\caption{Evaluation of PSNR for images reconstructed by VAEs on NeRF-LLFF, DL3DV-10K, and Mip-NeRF360 datasets.}
\vspace{-1em}
\resizebox{0.65\textwidth}{!}{
    \begin{tabular}{ll|ccc}
    \toprule
    \textbf{Method} & \textbf{Metric} & \textbf{NeRF-LLFF} & \textbf{DL3DV-10K} & \textbf{Mip-NeRF360} \\
    \midrule
    \textbf{VAE } &PSNR $\uparrow$     & 23.47 & 24.59 & 24.54 \\
    \textbf{Our-VAE} &PSNR $\uparrow$ & 23.59 & 23.25 & 24.24 \\
    \bottomrule
    \end{tabular}
}
\label{tab:encoding}
\end{table*}

\subsection{More Image Generation Results}
Fig. \ref{fig: generation_appendix} demonstrates that our VAE model can generate 3D objects guided by text prompts without any fine-tuning of the diffusion model. Moreover, Fig. \ref{fig: generation_appendix_gsgen} shows that our VAE can also improve the GSGEN \citep{chen2024textto3dusinggaussiansplatting} to achieve better 3D generations with complicated text prompts. 


 



\subsection{Efficiency Analysis}

Table \ref{tab:computation} demonstrates that our method reduces input resolutions, model storage space, and GPU usage for photorealistic NVS, which is particularly useful in cases with limited communication bandwidth and storage. For instance, some individuals may not have GPUs with large memories, where our method is an efficient solution for them to run photorealistic NVS algorithms.

\begin{table*}[h]
\caption{Efficiency comparison of different image-space and latent-space NVS methods.}
\vspace{-1em}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Method} & \textbf{Input resolution} & \textbf{Training Time} $\downarrow$ & \textbf{GPU Usage} $\downarrow$ & \textbf{Storage} $\downarrow$ & \textbf{Rendering FPS} $\uparrow$ & \textbf{Decoding FPS} $\uparrow$ & \textbf{PSNR} $\uparrow$ & \textbf{SSIM} $\uparrow$ & \textbf{LPIPS} $\downarrow$ \\
\midrule
\textit{3DGS} & \textit{512$\times$512} & \textit{5.9 min} & \textit{3 GB} & \textit{200.41 MB} & \textit{100} & \textit{-} & \textit{26.17} & \textit{0.778} & \textit{0.009} \\
3DGS/8 & 64$\times$64 & \textbf{3.1 min} & 1 GB & \textbf{59.15 MB} & \textbf{200} & - & 14.03 & 0.352 & 0.541 \\
3DGS-VAE & 64$\times$64 & 4.8 min & 2 GB & 250.97 MB & 80 & 20 & 20.57 & 0.595 & 0.346 \\
Latent-NeRF & 64$\times$64 & 27.2 min & 10 GB & 350.50 MB & 0.09 & 20 & 18.16 & 0.530 & 0.432 \\
Ours & 64$\times$64 & 3.9 min & \textbf{1 GB} & 96.42 MB & 180 & 20 & \textbf{22.45} & \textbf{0.667} & \textbf{0.197} \\
\bottomrule
\end{tabular}
}
\label{tab:computation}
\end{table*}

\subsection{More Experimental Results}

To demonstrate the effectiveness and generalizability of our method for 3D latent reconstruction, we show more NVS and 3D generation results on four datasets covering indoor scenes, outdoor scenes, and object-level scenes. As shown in Fig. \ref{fig: DL3DV-appendix}, \ref{fig: NeRF-LLFF-appendix}, \ref{fig: Mip-NeRF360-appendix} and \ref{fig: MVImgNet-appendix}, our method yields a significant improvement in image quality.



\begin{figure}[!t]
    \centering
    \begin{tikzpicture}
        

        \node[anchor=south west, inner sep=0] (image1) at (0,0.1) {\includegraphics[width=0.9\textwidth]{figures/appendix_gene_1.png}};
        \node[anchor=south west, inner sep=0] (image2) at (0, 5.0) {\includegraphics[width=0.9\textwidth]{figures/appendix_gene_0.png}};
        
        
        \node[anchor=south] at (1.0, 9.05) {\small Ours};               
        \node[anchor=south] at (3.0, 9.05) {\small Dreamfusion};         
        \node[anchor=south] at (5.35, 9.05) {\small Ours};               
        \node[anchor=south] at (7.35, 9.05) {\small Dreamfusion};         
        \node[anchor=south] at (9.7, 9.05) {\small Ours};               
        \node[anchor=south] at (11.7, 9.05) {\small Latent-NeRF};         

        \node[anchor=south] at (2.0, -0.8) {\small \parbox{3cm}{\textit{A small saguaro cactus planted in a clay pot}}};  
        \node[anchor=south] at (6.32, -0.5) {\small \textit{A hamburger}};
        \node[anchor=south] at (10.70, -0.8) {\small \parbox{3.5cm}{\textit{ A stack of pancakes covered in maple syrup}}};                    
        
        

        \node[anchor=south] at (2.1, 4.5) {\small {\textit{An ice cream}}};  
        \node[anchor=south] at (6.32, 4.5) {\small \textit{A temple}};
        \node[anchor=south] at (10.70, 4.5) {\small \textit{A lego man}};
    \end{tikzpicture}
    \caption{Samples for text-to-3D generation on the image and latent space. }
\label{fig: generation_appendix}
\end{figure}



\begin{figure}[!t]
    \centering
    \begin{tikzpicture}

        \node[anchor=south west, inner sep=0] (image2) at (0, 5.0) {\includegraphics[width=0.9\textwidth]{figures/appendix_gsgen.png}};
        
        
        \node[anchor=south] at (1.0, 9.05) {\small Ours};               
        \node[anchor=south] at (3.0, 9.05) {\small GSGEN};         
        \node[anchor=south] at (5.35, 9.05) {\small Ours};               
        \node[anchor=south] at (7.35, 9.05) {\small GSGEN};         
        \node[anchor=south] at (9.7, 9.05) {\small Ours};               
        \node[anchor=south] at (11.7, 9.05) {\small GSGEN};         

        \node[anchor=south] at (2.1, 4.1) {\small \parbox{4.1cm} {\textit{A DSLR photo of a tray of sushi containing pugs}}};  
        \node[anchor=south] at (6.32, 4.1) {\small\parbox{4.1cm} {\textit{A zoomed out DSLR photo of a cake in the shape of a train}}};
        \node[anchor=south] at (10.70, 4.1) {\small\parbox{4.1cm} {\textit{A zoomed out DSLR photo of a plate of fried chicken and waffles}}};
    \end{tikzpicture}
    \caption{More samples for text-to-3D generation on the image space.} 
\label{fig: generation_appendix_gsgen}
\end{figure}


\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        
        \node[anchor=south west, inner sep=0] (image) at (0,0) {
        \includegraphics[width=0.9\textwidth]{figures/appendix_dl3dv.png}};

        \newcommand{\W}{16.3}
        
        \node[anchor=south] at (2.0, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Groundtruth}}};
        \node[anchor=south] at (6.3, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Ours}}};
        \node[anchor=south] at (10.5, \W) {\scriptsize \colorbox{black}{\textcolor{white}{3DGS-VAE}}};

        \renewcommand{\W}{13.95}
        \node[anchor=south] at (2.0, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Mip-Splatting}}};
        \node[anchor=south] at (6.3, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Feature-GS}}};
        \node[anchor=south] at (10.5, \W) {\scriptsize \colorbox{black}{\textcolor{white}{3DGS}}};
        
    \end{tikzpicture}
    \caption{More NVS results on the \textbf{DL3DV-10K} dataset.
    }
\label{fig: DL3DV-appendix}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{tikzpicture}
        
        \node[anchor=south west, inner sep=0] (image) at (0,0) {
        \includegraphics[width=0.9\textwidth]{figures/appendix_llff.png}};

        \newcommand{\W}{15.7}
        
        \node[anchor=south] at (2.0, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Groundtruth}}};
        \node[anchor=south] at (6.3, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Ours}}};
        \node[anchor=south] at (10.5, \W) {\scriptsize \colorbox{black}{\textcolor{white}{3DGS-VAE}}};

        \renewcommand{\W}{12.56}
        \node[anchor=south] at (2.0, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Mip-Splatting}}};
        \node[anchor=south] at (6.3, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Feature-GS}}};
        \node[anchor=south] at (10.5, \W) {\scriptsize \colorbox{black}{\textcolor{white}{3DGS}}};
        
    \end{tikzpicture}
    \caption{More NVS results on the \textbf{NeRF-LLFF} dataset.
    }
\label{fig: NeRF-LLFF-appendix}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{tikzpicture}
        
        \node[anchor=south west, inner sep=0] (image) at (0,0) {
        \includegraphics[width=0.9\textwidth]{figures/appendix_mipnerf.png}};

        \newcommand{\W}{19.65}
        
        \node[anchor=south] at (2.0, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Groundtruth}}};
        \node[anchor=south] at (6.3, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Ours}}};
        \node[anchor=south] at (10.5, \W) {\scriptsize \colorbox{black}{\textcolor{white}{3DGS-VAE}}};

        \renewcommand{\W}{16.8}
        \node[anchor=south] at (2.0, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Mip-Splatting}}};
        \node[anchor=south] at (6.3, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Feature-GS}}};
        \node[anchor=south] at (10.5, \W) {\scriptsize \colorbox{black}{\textcolor{white}{3DGS}}};
        
    \end{tikzpicture}
    \caption{More NVS results on the \textbf{Mip-NeRF360} dataset.
    }
\label{fig: Mip-NeRF360-appendix}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{tikzpicture}
        
        \node[anchor=south west, inner sep=0] (image) at (0,0) {
        \includegraphics[width=0.9\textwidth]{figures/appendix_mvimagenet.png}};

        \newcommand{\W}{11.1}
        
        \node[anchor=south] at (1.0, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Groundtruth}}};
        \node[anchor=south] at (3.1, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Ours}}};
        \node[anchor=south] at (5.2, \W) {\scriptsize \colorbox{black}{\textcolor{white}{3DGS-VAE}}};
        
        \node[anchor=south] at (7.3, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Mip-Splatting}}};
        \node[anchor=south] at (9.4, \W) {\scriptsize \colorbox{black}{\textcolor{white}{Feature-GS}}};
        \node[anchor=south] at (11.5, \W) {\scriptsize \colorbox{black}{\textcolor{white}{3DGS}}};
        
    \end{tikzpicture}
    \caption{More NVS results on the \textbf{MVImgNet} dataset.
    }
\label{fig: MVImgNet-appendix}
\end{figure}


