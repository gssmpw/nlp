

\definecolor{tabfirst}{rgb}{1, 0.7, 0.7}
\definecolor{tabsecond}{rgb}{1, 0.85, 0.7}
\definecolor{tabthird}{rgb}{1, 1, 0.7}




\section{Experiments}
\label{sec: exp}




\subsection{Latent 3D reconstruction}
We first evaluate LRF on four real-world datasets, including MVImgNet \citep{yu2023mvimgnet}, NeRF-LLFF \citep{mildenhall2019llff}, MipNeRF360 \citep{barron2022mipnerf360}, and DL3DV-10K \cite{ling2024dl3dv}, to demonstrate the effectiveness of our approach for latent 3D reconstruction. Among these datasets, DL3DV serves as an in-distribution dataset, where the training set is used for model training, and the test set is used for evaluation. In contrast, MVImgNet, LLFF, and Mip-NeRF360 are out-of-distribution datasets, as they have never been used in the training process. We follow the standard train and test split in 3DGS and Mip-Splatting\citep{kerbl3Dgaussians,Yu2024MipSplatting}. 


Fig. \ref{fig: exp_recon} shows that our method significantly improves the capability of the 2D latent representations for 3D reconstruction task. Our approach mitigates the artifacts such as ghosting, color distortion, blurring, and texture warping caused by 3D inconsistency. While the latent and image space approaches share the same input resolution, our rendering results present clearer visual details, richer textures, and more high-frequency information.


\begin{table*}[t]
\centering
\caption{Our method outperforms the image and latent space NVS baselines on most settings and metrics, from object-level to unbounded outdoor scenes. Latent-NeRF$^*$ denotes we adapt it to NVS.}
\vspace{-1em}
\resizebox{\textwidth}{!}{
    \begin{tabular}{ll|cc|cccc}
    \toprule
    &  & \multicolumn{2}{c|}{\textbf{Image Space}} & \multicolumn{4}{c}{\textbf{Latent Space}} \\
    \textbf{~~~~Dataset} & \textbf{Metric}  &3DGS/8 & Mip-Splatting/8 & 3DGS-VAE & Latent-NeRF$^*$ & Feature-GS & \textbf{3DGS-LRF (Ours)} \\
    \midrule
    \multirow{3}{*}{
    \textbf{MVImgNet}} &PSNR $\uparrow$ & 16.93 & \cellcolor{tabthird}24.89 & \cellcolor{tabsecond}25.04 & 18.50 & \cellcolor{tabthird}21.09 & \cellcolor{tabfirst}26.26 \\
    &SSIM $\uparrow$ & 0.561 & \cellcolor{tabthird}0.799 & \cellcolor{tabsecond}0.824 & 0.709 & 0.772 & \cellcolor{tabfirst}0.863 \\
    &LPIPS $\downarrow$ & 0.466 & \cellcolor{tabthird}0.328 & \cellcolor{tabsecond}0.250 & 0.403 & 0.372 & \cellcolor{tabfirst}0.178 \\
    \midrule
    \multirow{3}{*}{
    \textbf{NeRF-LLFF}} &PSNR $\uparrow$ & 9.98 & \cellcolor{tabsecond}19.68 & \cellcolor{tabthird}19.07 & 18.31 & 16.48 & \cellcolor{tabfirst}20.00 \\
    &SSIM $\uparrow$ & 0.110 & \cellcolor{tabthird}0.484 & \cellcolor{tabsecond}0.493 & 0.457 & 0.415 & \cellcolor{tabfirst}0.541 \\
    &LPIPS $\downarrow$ & 0.631 & 0.513 & \cellcolor{tabsecond}0.364 & \cellcolor{tabthird}0.387 & 0.539 & \cellcolor{tabfirst}0.289 \\
    \midrule
    \multirow{3}{*}{ 
    \textbf{DL3DV-10K}}&PSNR $\uparrow$ & 14.03 & \cellcolor{tabsecond}21.81 & \cellcolor{tabthird}20.57 & 18.16 & 16.60 & \cellcolor{tabfirst}22.45 \\
    &SSIM $\uparrow$ & 0.352 & \cellcolor{tabsecond}0.609 & \cellcolor{tabthird}0.595 & 0.530 & 0.449 & \cellcolor{tabfirst}0.667 \\
    &LPIPS $\downarrow$ & 0.541 & 0.451 & \cellcolor{tabsecond}0.346 & \cellcolor{tabthird}0.432 & 0.602 & \cellcolor{tabfirst}0.197 \\
    \midrule
    \multirow{3}{*}{ 
    \textbf{Mip-NeRF360}}&PSNR $\uparrow$ & 14.79 & \cellcolor{tabfirst}22.38 & \cellcolor{tabthird}19.44 & 15.93 & 17.13 & \cellcolor{tabsecond}20.83 \\
    &SSIM $\uparrow$ & 0.273 & \cellcolor{tabfirst}0.502 & \cellcolor{tabthird}0.404 & 0.312 & 0.337 & \cellcolor{tabsecond}0.469 \\
    &LPIPS $\downarrow$ & 0.586 & \cellcolor{tabthird}0.521 & \cellcolor{tabsecond}0.432 & 0.537 & 0.642 & \cellcolor{tabfirst}0.328 \\
    \bottomrule
    \end{tabular}
}
\vspace{.5em}
\label{tab: main}
\end{table*}

\begin{wraptable}{r}{0.4\textwidth}
    \vspace{-0pt}
    \centering
    \footnotesize
    \caption{A comparison of different methods on LLFF dataset using 3 views.}
    \vspace{-1em}
    \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        Method  & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\
        \midrule
        3DGS-VAE & 13.06 & 0.283 & 0.570 \\
        3DGS  & 13.79 & 0.331 & 0.468 \\
        Mip-Splatting  & 13.70 & 0.315 & 0.486 \\
        \textbf{Ours} & \textbf{15.51} & \textbf{0.379} & \textbf{0.465} \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:llff_comparison}
\end{wraptable}


As shown in Table \ref{tab: main}, LRF achieves the state-of-the-art performance across all datasets in terms of metrics of PSNR, SSIM, and LPIPS. These results underscore the effectiveness of our approach in fine-tuning latent space representations to support novel view synthesis. This demonstrates that our fine-tuning approach not only effectively reduces the geometry information loss caused by 3D-inconsistent 2D representations but also preserves perceptual and textural information in NVS outputs. Compared to the original VAE model, our fine-tuning approach significantly enhances 3D-consistency in the 2D latent representations by enforcing the correspondence points to be consistent, resulting in superior latent NVS performance across all metrics. ``Image Space'' means that we input images to 3DGS with the same resolution as the latent representations, then render output images with the same resolution as before latent encoding. Since we render high-resolution images from low-resolution training images, to avoid unfair comparisons caused by aliasing, we also compare our method with the Mip-Splatting \citep{Yu2024MipSplatting} which is specialized at super-resolution rendering. Compared with these image space methods, our latent reconstruction method still achieves better performance on most of the datasets, highlighting its potential for future work in efficient 3D representation learning.

We also show the generalizability by performing the synthesis of few-shot novel views in NeRF-LLFF dataset.  We follow the same experimental configurations as in the previous work \citep{li2024dngaussian,liu20243dgs}. And we keep the same input resolution for all the methods. As shown in Table \ref{tab:llff_comparison}, our method  outperforms the other image-space approaches in the sparse-view setting.
\vspace{-1em}
\begin{figure}[!t]
    \centering
    \begin{tikzpicture}
        

        \node[anchor=south west, inner sep=0] (image1) at (0,0) {\includegraphics[width=0.6\textwidth]{figures/exp_gene_0.png}};
        \node[anchor=south west, inner sep=0] (image2) at (8.62, 0,0) {\includegraphics[width=0.29\textwidth]{figures/exp_gene_1.png}};
        
        
        \node[anchor=south] at (1.0, 4.05) {\small Ours};              
        \node[anchor=south] at (3.0, 4.05) {\small Dreamfusion};         
        \node[anchor=south] at (5.35, 4.05) {\small Ours};               
        \node[anchor=south] at (7.35, 4.05) {\small Dreamfusion};         
        \node[anchor=south] at (9.7, 4.05) {\small Ours};               
        \node[anchor=south] at (11.5, 4.05) {\small GSGEN};         

        
        \node[anchor=south] at (2.0, -0.5) {\small \textit{A lego man}};  
         \node[anchor=south] at (6.32, -0.5) {\small \textit{A vase with pink flowers}};
        \node[anchor=south] at (10.70, -0.83) {\small \parbox{3.5cm}{\textit{A DSLR photo of a tray of sushi containing pugs}}};                    
    \end{tikzpicture}
    \label{fig:3D-generation}
    \vspace{-.5em}
    \caption{Visual comparison of different text-to-3D generation methods. Our model enables the generation of more view-consistent results.}
       
\label{fig: generation}
\end{figure}
\vspace{.1em}

\subsection{Text-to-3D generation}
We evaluate our method for the state of art text-to-3D generation framework in both latent and image space. We leverage the GSGEN \citep{chen2024textto3dusinggaussiansplatting} and Dreamfusion \citep{poole2022dreamfusion} as the image space generation framework, while we use Latent-NeRF \citep{metzer2022latent} as the latent space method. GSGEN is optimized in the 512\(\times\)512 image space. Dreamfusion is optimized in the 800\(\times\)800 image space. Latent-NeRF is optimized in the 128\(\times\)128 latent space and then reconstruct images to a resolution of 1024\(\times\)1024.  By following the prompts evaluated in these two works, we generate 3D objects and render them from multiple views. The text prompts fed into the GSGEN are more complicated considering it is the state of the art generation method. 

As shown in Fig. \ref{fig: generation}, our method can boost the performance under extremely complicated text prompts, achieve complex geometry while preserving the multi-view consistency. Moreover, our encoder model can significantly enhance the high-frequency details such as the texture of the fried chicken. Besides, our approach is compatible with the diffusion model operating within the original VAE latent space. Without necessitating any fine-tuning of the diffusion U-Net parameters, the diffusion process remains capable of accurately denoising the 2D latent representations provided by our fine-tuned VAE, according to the text guidance. Furthermore, the VAE-RF alignment in decoder fine-tuning also facilitates the reconstruction of rendered latent representations, improving the image quality after VAE decoding.




\subsection{Ablation Study}
We conduct ablation studies on two major components of our three-stage framework, the correspondence-aware autoencoding and the VAE-RF aligned decoder fine-tuning, to assess their contributions to overall performance. The quantitative results shown in Table \ref{tab:ablation_table} indicate that both components contribute to performance improvement. Notably, the decoder presents a more significant impact on the results, as it directly influences the reconstruction of images from the latent space, thereby leading to stronger performance gains. Although the encoder does not directly act on image reconstruction, it enhances geometric consistency of 2D representations, which also contributes to the performance improvement in 3D reconstruction.

\input{tables/abs_table}



The qualitative results are shown in Fig. \ref{fig: abs}. The encoder fine-tuning allows the 3D latent space to capture more precise geometry, reduce blurriness in the synthesized images, and recover finer details. Additionally, the decoder fine-tuning further refines the results by rectifying inaccuracies and preserving perceptual and textural fidelity. Together, these modules synergistically contribute to significant improvements in the overall pipeline.



\begin{figure}[!t]
    \centering
    \begin{tikzpicture}
        

        \node[anchor=south west, inner sep=0] (image1) at (0,0) {\includegraphics[width=0.9\textwidth]{figures/ablation.png}};
        
        
        \node[anchor=south] at (2.0, 3.5) {\small Groundtruth};               
        \node[anchor=south] at (6.32,  4.6) {\small No fine-tuning};         
        \node[anchor=south] at (10.7,  4.6) {\small Encoder fine-tuning};               

       
        \node[anchor=south] at (6.32, -0.5) {\small Decoder fine-tuning};  
        \node[anchor=south] at (10.7, -0.5) {\small Ours};                   
    \end{tikzpicture}
    \caption{A qualitative study of the effect of different fine-tuning stages for view synthesis results. }
\label{fig: abs}
\end{figure}


