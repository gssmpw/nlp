
\section{Introduction}
Recently, significant advancement in radiance field representation, such as Neural Radiance Fields (NeRF) \citep{mildenhall2020nerf} and 3D Gaussian Splatting (3DGS) \citep{kerbl3Dgaussians},  have been made for fast and high-quality 3D reconstruction and novel view synthesis (NVS). As demonstrated by Stable Diffusion Models \citep{rombach2021highresolution}, optimizing in the 2D latent space instead of the image space can significantly boost generation efficiency. Meanwhile, a 3D-consistent latent space and photorealistic decoding capability can benefit many tasks such as text-to-3D generation, latent NVS, few-shot NVS, efficient NVS, 3D latent diffusion model, and 3D semantic understanding. 
To empower 3D semantic understanding, researchers have explored latent 3D reconstruction methods, such as Feature 3DGS \citep{zhou2024feature}, to distill 2D semantic features into 3D space for novel view semantic segmentation.
However, there are significant domain gaps between the 2D feature space and 3D representations, arising from the lack of consistent 3D spatial structure information, which hinders the direct feeding of 2D features into the 3D representations. The 2D feature extractors cannot effectively perceive the 3D structures behind the inputs images since the training images are presented to the network in an unstructured way and the training objective does not include 3D consistency.
Therefore, the loss of 3D awareness is inevitable.

Few previous work attempt to bridge the gap between the 2D features and 3D representations. By focusing on better 3D scene understanding, Feature 3DGS \citep{zhou2024feature} proposes to distill a feature field from 2D semantic features by leveraging the view-independent approach while it cannot model the view-dependent visual properties. Another line of work improves the latent field in the context of 3D generation task. Latent-NeRF \citep{metzer2022latent} and ED-NeRF \citep{park2023ed} includes an additional per-scene refinement layer to enhance the latent rendering quality, while introducing more computation cost and exhibiting the generalizabilty. 

To bridge the gap between the 2D latent space and 3D representations, we observe two main challenges: Firstly, the massive view-dependent high-frequency noise in the 2D latent space causes the inconsistent geometry and unstable optimization. Moreover, data distribution shift by applying RGB-based NVS methods to latent features also prohibits the photorealistic rendering. To tackle with these two issues, our key insight is to embed 3D awareness into the latent space, while maximumly preserving the representation ability of autoencoders without introducing any additional layers. 
We propose a novel framework that builds a latent radiance field (LRF) based on the 3D-aware 2D representations. 
Specifically, it consists of three stages. Firstly, we introduce a correspondence-aware autoencoding method to improve the 3D awaresness of the VAE's latent space, making the 2D representations follow the geometry consistency. Then, we build the LRF to represent 3D scenes from the 3D-aware 2D representations, lifting the 3D-aware 2D representations into the 3D space. Finally, we introduce a VAE-Radiance Field(VAE-RF) alignment method to further mitigate the data distribution shift caused by NVS and boost the performance of image decoding from the rendered 2D representations.
In together, the created 3D-aware latent space and LRFs can be smoothly injected into existing NVS or 3D generation pipelines without further fine-tuning, achieving high-quality and photorealistic synthesis results. 

To the best of our knowledge, this is the first work demonstrating that radiance field representations constructed in the latent space, with the injection of 3D awareness,  can achieve photorealistic 3D reconstruction performance across various settings including indoor and unbounded outdoor scenes.
Extensive NVS, 3D generation, and few-shot novel view synthesis experiments show that our method outperforms existing methods with respect to its high-quality synthesis and cross-dataset generalizability, as shown in Fig. \ref{fig: teaser} and the following sections. 
In summary, main contributions of this work include:
\begin{itemize}[leftmargin=*]
    \item We introduce a novel framework to integrate 3D awareness into 2D representation learning, including a correspondence-aware autoencoding method and a VAE-Radiance (VAE-RF) field alignment to enable high-quality 3D reconstruction in latent space.
    \item We propose the latent radiance field (LRF) to effectively elevate the 3D-aware 2D representations into 3D latent fields. It represents the first step towards constructing radiance field representations directly in the latent space for 3D reconstruction tasks. 
    \item We conduct extensive experiments to show that our method achieves superior fidelity and cross-dataset generalizability across NVS, few-shot NVS, and 3D generation tasks.
\end{itemize}



