QA is one of the most widely used tasks for evaluating the performance of RAG systems. QA tasks come in various forms, such as single-hop QA, multi-hop QA, and open-domain QA~\cite{wang2022modern}. To systematically assess the effectiveness of RAG and GraphRAG in these tasks, we evaluate them on widely used QA datasets and employ standard evaluation metrics.

% Each question may be\yu{maybe is ambiguous, could be more explicit in terms of how we consider these.} answered using information from a single document or multiple documents. 

\begin{table*}[!htb]
\caption{Performance comparison (\%) on NQ and Hotpot datasets. The best results are highlighted in bold, and the second-best results are underlined.}
\label{tab:nq}
\vspace{-0.1in}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccc|cccccc}
\toprule
\multirow{4}{*}{\textbf{Method}}      & \multicolumn{6}{c|}{\textbf{NQ}}                                                         & \multicolumn{6}{c}{\textbf{Hotpot}}                                                     \\ \cmidrule{2-13}
& \multicolumn{3}{c|}{\textbf{Llama 3.1-8B}}          & \multicolumn{3}{c|}{\textbf{Llama 3.1-70B}} & \multicolumn{3}{c|}{\textbf{Llama 3.1-8B}}          & \multicolumn{3}{c}{\textbf{Llama 3.1-70B}} \\ 
\cmidrule{2-13}

& P     & R     & \multicolumn{1}{c|}{F1}    & P          & R         & F1        & P     & R     & \multicolumn{1}{c|}{F1}    & P         & R         & F1        \\
\midrule
RAG                        & \textbf{71.7}  & \textbf{63.93} & \multicolumn{1}{c|}{\textbf{64.78}} & \textbf{74.55}      & \textbf{67.82}     & \textbf{68.18}     & \underline{62.32} & \underline{60.47} & \multicolumn{1}{c|}{\underline{60.04}} & \underline{66.34}     & \underline{63.99}     & \underline{63.88}     \\
KG-GraphRAG (Triplets only) & 40.09 & 33.56 & \multicolumn{1}{c|}{34.28} & 37.84      & 31.22     & 28.50      & 26.88 & 24.81 & \multicolumn{1}{c|}{25.02} & 32.59     & 30.63     & 30.73     \\
KG-GraphRAG (Triplets+Text) & 58.36 & 48.93 & \multicolumn{1}{c|}{50.27} & 60.91      & 52.75     & 53.88     & 45.22 & 42.85 & \multicolumn{1}{c|}{42.60}  & 51.44     & 48.99     & 48.75     \\
Community-GraphRAG (Local)  & \underline{69.48} & \underline{62.54} & \multicolumn{1}{c|}{\underline{63.01}} & \underline{71.27}      & \underline{65.46}     & \underline{65.44}     & \textbf{64.14} & \textbf{62.08} & \multicolumn{1}{c|}{\textbf{61.66}} & \textbf{67.20}      & \textbf{64.89}     & \textbf{64.60}      \\
Community-GraphRAG (Global) & 60.76 & 54.99 & \multicolumn{1}{c|}{54.48} & 61.15      & 55.52     & 55.05     & 45.72 & 47.60  & \multicolumn{1}{c|}{45.16} & 48.33     & 48.56     & 46.99     \\ \bottomrule
\end{tabular}
}
\vspace{-0.1in}
\end{table*}


\begin{table*}[htb]
\caption{Performance comparison (\%) on the MultiHop-RAG dataset across different query types.}
\label{tab:multihop}
\vspace{-0.1in}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccccc|ccccc}
\toprule
\multirow{2}{*}{\textbf{Method}}      & \multicolumn{5}{c|}{\textbf{LLama 3.1-8B}}                                                & \multicolumn{5}{c}{\textbf{Llama 3.1-70B}}                                             \\ 
\cmidrule{2-11}
                     & \textbf{Inference}      & \textbf{Comparison}     & \textbf{Null}           & \textbf{Temporal}       & \textbf{Overall}        & \textbf{Inference}      & \textbf{Comparison}  & \textbf{Null}           & \textbf{Temporal}       & \textbf{Overall}        \\ 
\midrule
RAG                        & \textbf{92.16} & 57.59          & 96.01          & 30.7           & \underline{67.02}          & \textbf{94.85} & 56.31       & 91.36          & 25.73          & \underline{65.77}          \\
KG-GraphRAG (Triplets only) & 55.76          & 22.55          & \textbf{98.67} & 18.7           & 41.24          & 76.96          & 32.36       & \textbf{94.35} & 19.55          & 50.98          \\
KG-GraphRAG (Triplets+Text) & 67.4           & 34.7           & 97.34          & 17.15          & 48.51          & 85.91          & 35.98       & 86.38          & 21.61          & 54.58          \\
Community-GraphRAG (Local)  & 86.89          & \underline{60.63}          & 80.07          & \underline{50.6}           & \textbf{69.01} & 92.03          & \underline{60.16}       & \underline{88.70}           & \underline{49.06}          & \textbf{71.17} \\
Community-GraphRAG (Global) & \underline{89.34}          & \textbf{64.02} & 19.27          & \textbf{53.34} & 64.4           & \underline{89.09}          & \textbf{66.00} & 13.95          & \textbf{59.18} & 65.69          \\ 
\bottomrule
\end{tabular}
}
\vspace{-0.1in}
\end{table*}


\begin{table*}[!htb]
\caption{Performance comparison (\%) on the NovelQA dataset across different query types with LLama 3.1-8B.}
\label{tab:novelqa}
\vspace{-0.1in}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccc|ccccccccc}
\toprule
\multicolumn{1}{l}{} & \multicolumn{8}{c|}{\textbf{RAG}}                                                     & \multicolumn{9}{c}{\textbf{KG-GraphRAG (Triplets+Text)}}                                       \\ \midrule
\multicolumn{1}{l}{} & chara & mean  & plot  & relat & settg & span                 & times & avg   & \multicolumn{1}{l}{} & chara & mean  & plot  & relat & settg & span  & times & avg   \\

mh                   & 68.75 & 52.94 & 58.33 & 75.28 & 92.31 & 64.00                & 33.96 & 47.34 & mh                   & 52.08 & 52.94 & 44.44 & 55.06 & 69.23 & 64.00 & 28.61 & 38.37 \\
sh                   & 69.08 & 62.86 & 66.11 & 75.00 & 78.35 & -                    & -     & 68.73 & sh                   & 36.84 & 45.71 & 40.17 & 87.50 & 36.08 & -     & -     & 39.93 \\
dtl                  & 64.29 & 45.51 & 78.57 & 10.71 & 83.78 & -                    & -     & 55.28 & dtl                  & 38.57 & 30.90 & 42.86 & 21.43 & 32.43 & -     & -     & 33.60 \\
avg                  & 67.78 & 50.57 & 67.37 & 60.80 & 80.95 & \multicolumn{1}{l}{64.00} & 33.96 & 57.12 & avg                  & 40.00 & 36.23 & 41.09 & 49.60 & 38.10 & 64.00 & 28.61 & 37.80 \\ \midrule
\multicolumn{1}{l}{} & \multicolumn{8}{c|}{\textbf{Community-GraphRAG (Local)}}                               & \multicolumn{9}{c}{\textbf{Community-GraphRAG   (Global)}}                                       \\ \midrule
\multicolumn{1}{l}{} & chara & mean  & plot  & relat & settg & span                 & times & avg   & \multicolumn{1}{l}{} & chara & mean  & plot  & relat & settg & span  & times & avg   \\
mh                   & 68.75 & 64.71 & 55.56 & 67.42 & 92.31 & 52.00                & 35.83 & 47.01 & mh                   & 54.17 & 58.82 & 55.56 & 56.18 & 53.85 & 68.00 & 20.59 & 34.39 \\
sh                   & 59.87 & 58.57 & 65.69 & 87.50 & 64.95 & -                    & -     & 63.43 & sh                   & 45.39 & 50.00 & 55.65 & 87.50 & 38.14 & -     & -     & 49.65 \\
dtl                  & 54.29 & 37.64 & 62.50 & 25.00 & 70.27 & -                    & -     & 46.88 & dtl                  & 28.57 & 29.78 & 32.14 & 87.50 & 40.54 & -     & -     & 30.89 \\
avg                  & 60.00 & 44.91 & 64.05 & 59.20 & 68.71 & 52.00                & 35.83 & 53.03 & avg                  & 42.59 & 36.98 & 51.66 & 52.00 & 40.14 & 68.00 & 20.59 & 39.17 \\ \bottomrule
\end{tabular}
}
\vspace{-0.2in}
\end{table*}

\subsection{Datasets and Evaluation Metrics}
\vspace{-0.05in}
To comprehensively evaluate the performance of GraphRAG on general QA tasks, we select four widely used datasets that cover different perspectives. For the single-hop QA task, we select the Natural Questions (NQ) dataset~\cite{kwiatkowski2019natural}.
% , where each query corresponds to a single document.  
For the multi-hop QA task, we select HotPotQA~\cite{yang2018hotpotqa} and MultiHop-RAG~\cite{tang2024multihop} datasets. 
% In both datasets, each query requires retrieving information from multiple documents. 
The MultiHop-RAG dataset categorizes queries into four types: Inference, Comparison, Temporal, and Null queries. To further analyze the performance of RAG and GraphRAG at a finer granularity, we also include NovelQA~\cite{tang2024multihop}, which contains 21 different types of queries. For more details, please refer to Appendix~\ref{app:qa_dataset}.
% \jt{why different metrics? do we follow existing evaluations? }
We use Precision (P), Recall (R), and F1-score as evaluation metrics for the NQ and HotPotQA datasets, while accuracy is used for the MultiHop-RAG and NovelQA datasets following their original papers.

% \yu{Do we need to mention how to calculate accuracy, e.g., LLM-as-judger?}
\vspace{-0.1in}
\subsection{QA Main Results}
\label{sec:qa_result}
% \vspace{-0.05in}
The performance comparison for the NQ and HotPotQA datasets is presented in Table~\ref{tab:nq}, while that of MultiHop-RAG is shown in Table~\ref{tab:multihop}. Due to space constraints, partial results of NovelQA with the Llama 3.1-8B model are shown in Table~\ref{tab:novelqa}, with the full results available in Appendix~\ref{app:novelqa}. Based on these results, we make the following observations: 
\vspace{-0.05in}
\begin{enumerate}[leftmargin=*, itemsep=0pt, parsep=0pt]
    \item {\bf RAG excels on detailed single-hop queries}. RAG performs well on single-hop queries and queries that require detailed information. This is evident from its performance on the single-hop dataset (NQ) as well as the single-hop (sh) and detail-oriented (dtl) queries in the NovelQA dataset, as shown in Table~\ref{tab:nq} and Table~\ref{tab:novelqa}. 
    % \yu{the first time mentioning detail-oriented queries? and also do we want to refer these results back to the Table?}
    \item \textbf{GraphRAG, particularly Community-GraphRAG (Local), excels on multi-hop queries.} For instance, it achieved the best performance on both the HotPotQA and MultiHop-RAG datasets. Although its overall performance on the NovelQA dataset is lower than that of RAG, it still performs well on the multi-hop (mh) queries in NovelQA dataset.
    % as shown in Table~\ref{tab:novelqa}.    
    \item {\bf Community-GraphRAG (Global) often struggles on QA tasks}. This is due to the global search retrieves only high-level communities, leading to a loss of detailed information. This is particularly evident from its lower performance on detail-oriented queries in the NovelQA dataset. Additionally, Community-GraphRAG (Global) tends to hallucinate in QA tasks, as shown by its poor performance on Null queries in the MultiHop-RAG dataset, which should ideally be answered as `insufficient information.' However, this summarization approach may be beneficial for queries that require comparing different topics or understanding their temporal ordering, such as Comparison and Temporal queries in the MultiHop-RAG dataset, as shown in Table~\ref{tab:multihop}.
    \item {\bf KG-based GraphRAG also generally underperform on QA tasks}. This is because it retrieves information solely from the constructed knowledge graph, which contains only entities and their relations. However, the extracted entities and relations may be incomplete, leading to gaps in the retrieved information. To verify this, we calculated the ratio of answer entities present in the constructed KG. We found that only around 65.8\% of answer entities exist in the constructed KG for the Hotpot dataset and 65.5\% for the NQ dataset. These findings highlight a key limitation in KG-based retrieval and suggest the need for improved KG construction methods to enhance graph completeness for QA.
    % \yu{maybe can inspire future research on better KG construction method?}
\end{enumerate}

\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/NQ-8B-overlap.pdf}
        \caption{NQ}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/hotpot-8B-overlap.pdf}
        \caption{Hotpot}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/news-8B-overlap.pdf}
        \caption{MultiHop-RAG}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/NovelQA-8B-overlap.pdf}
        \caption{NovelQA}
    \end{subfigure}
    \caption{Confusion matrices comparing GraphRAG and RAG correctness across datasets using Llama 3.1-8B.}
    \label{fig:confusion}
    \vspace{-0.2in}
\end{figure*}

\vspace{-0.1in}
\subsection{Comparative QA Analysis}
\vspace{-0.1in}
In this section, we conduct a detailed analysis of the behavior of RAG and GraphRAG, focusing on their strengths and weaknesses. In the following discussion, we refer to Community-GraphRAG (Local) as GraphRAG, as it demonstrates performance comparable to RAG. We categorize queries into four groups: {\bf (1)} Queries correctly answered by both methods, {\bf (2)} Queries correctly answered only by RAG (RAG-only), {\bf (3)} Queries correctly answered only by GraphRAG (GraphRAG-only), and {\bf (4)} Queries  answered incorrectly by both methods. 
% Specifically, we define a query as correct if Accuracy = 1 for the MultiHop-RAG and NovelQA datasets, and Recall = 1 for the NQ and HotPotQA datasets.


% For the NQ and HotPotQA datasets, we define \jt{I did not get the following??, please rewrite} a correct sample as one where Recall = 1, meaning the ground truth answer is present in the model's response. 

The confusion matrices representing these four groups using the Llama 3.1-8B model are shown in Figure~\ref{fig:confusion}. Notably, the proportions of queries correctly answered exclusively by GraphRAG and RAG are significant. For example, 13.6\% of queries are GraphRAG-only, while 11.6\% are RAG-only on MultiHop-RAG dataset. This phenomenon highlights the complementary properties of RAG and GraphRAG, and each method has its own strengths and weaknesses. Therefore, {\it leveraging their unique advantages has the potential to improve overall performance}.
% ~\cite{lee2024hybgrag,xia2024knowledge}.

% \yu{can cite some recent hybrid method works such as "HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases", "Knowledge-Aware Query Expansion with Large Language Models for Textual and Relational Retrieval"



\vspace{-0.1in}
\subsection{Improving QA Performance}
\label{sec:improve_qa}
Building on the complementary properties of RAG and GraphRAG, we investigate the following two strategies to enhance overall QA performance.

% To verify this assumption, we use ChatGPT-4o to category and analyze the differences between RAG-only and GraphRAG-only queries. The prompt and responses are provided in Appendix~\ref{}, and a partial response on the Hotpot dataset is shown in Figure~\ref{fig:hotpot_gpt}. ChatGPT-4o categorizes the set of RAG-only queries as fact-based queries and GraphRAG-only queries as reasoning-based queries, aligning with our hypothesis. Additionally, it finds that GraphRAG-only queries require chaining multiple facts together, aligning with our finding that GraphRAG excels in multi-hop reasoning tasks. \yu{After reading the paragraph and look at the figure, I am also confused about the figure. Are you saying you want to verify whether those only answered by GraphRAG/RAG is the reasoning-based/fact-based one? In this case, would it be better to calculate the accuracy (e.g., is it really for those who can only answered by GraphRAG be successfully categorized into reasoning?) In this way, if we can really show a very good classification performance, it further motivate your next action by using query classifier to detect and then decide which retriever you would like to go to.}
% \harry{I don't really understand this figure. What exactly are you asking ChatGPT? How does this support your hypothesis? It's not very clear}
% These results indicate that the response of ChatGPT-4o aligns well with the observed behavior of RAG and GraphRAG, supporting our hypothesis.
% Building on the complementary properties of RAG and GraphRAG, as well as our analysis, we investigate the following two strategies to enhance overall QA performance.
% \begin{figure}[!htb]
% \vspace{-10 pt}
% \begin{tcolorbox}[mybox={Analysis from ChatGPT-4o}]
% \textbf{Fact-Based vs. Reasoning-Based}: \\
% Set 1: Many questions require retrieving a single fact. 
% Set 2: More questions involve implicit reasoning.

% \textbf{Multi-Hop Reasoning:} \\
% Set 1: Most questions involve a single retrieval step. 
% Set 2: More questions require chaining multiple facts together. 
% \end{tcolorbox}
% \vspace{-10 pt}
% \caption{Analysis of ChatGPT-4o on the Hotpot dataset. Set 1 represents RAG-only queries, while Set 2 corresponds to GraphRAG-only queries.}
% \label{fig:hotpot_gpt}
% \vspace{-10 pt}
% \end{figure}

\noindent {\bf Strategy 1: RAG vs. GraphRAG Selection.} \\
In Section~\ref{sec:qa_result}, we observe that RAG generally performs well on single-hop queries and those requiring detailed information, while GraphRAG (Community-GraphRAG (Local)) excels in multi-hop queries that require reasoning. Therefore, we hypothesize that RAG is well-suited for fact-based queries, which rely on direct retrieval and detailed information, whereas GraphRAG is more effective for reasoning-based queries that involve chaining multiple facts together. Therefore, given a query, we employ a classification mechanism to determine whether it is fact-based or reasoning-based. Each query is then assigned to either RAG or GraphRAG based on the classification results. Specifically, we leverage the in-context learning ability of LLMs for classification~\cite{dong2022survey, wei2023larger}. Further details and prompts can be found in Appendix~\ref{app:selection}. In this strategy, either RAG or GraphRAG is selected for each query, and we refer to this strategy as \textbf{Selection}.

\noindent \textbf{Strategy 2: RAG and GraphRAG Integration.} \\
We also explore the \textbf{Integration} strategy to leverage the complementary strengths of RAG and GraphRAG. Both RAG and GraphRAG retrieve information for a query simultaneously. The retrieved results are then concatenated and fed into the generator to produce the final output.
% \yu{do we need to mention the budget for each, would the budget for Strategy 1 and S2 be the same?} I talked it in the efficency section later

\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/qa_improvement_8B.pdf}
        \caption{Llama3.1-8B}
        \label{fig:qa_improve_8b}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/qa_improvement_70B.pdf}
        \caption{Llama3.1-70B}
        \label{fig:qa_improve_70b}
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Overall QA performance comparison of different methods.}
    \label{fig:qa_improve}
    \vspace{-0.2in}
\end{figure*}

% \jt{let us use strategies instead of methods. Basically the high-level idea is more general so it is strategy}
We conduct experiments to verify the effectiveness of the two proposed strategies. Specifically, we evaluate overall performance across all selected datasets. For the MultiHop-RAG and NovelQA datasets, we use the overall accuracy, while for the NQ and HotPotQA datasets, we use the F1 score as the evaluation metric. The results are shown in Figure~\ref{fig:qa_improve}.
From these results, we observe that {\bf both strategies generally enhance overall performance}. For example, on the MultiHop-RAG dataset with Llama 3.1-70B, Selection and Integration improve the best method by 1.1\% and 6.4\%, respectively.
% \jt{can we give some numbers to show the improvement??} 
When comparing the Selection and Integration strategies, the Integration strategy usually achieves higher performance than the Selection strategy. 
% These results indicate that categorizing queries into Fact-based and Reasoning-based categories is beneficial but may not be perfect. 
% Relying solely on the query itself without considering the context may be suboptimal\yu{I didn't quite understand this sentence. Do you mean relying soly on the query cannot help you classify whether it is RAG or GraphRAG? But why context would help here and also what does the context mean here?}. 
However, the Selection strategy processes each query using either RAG or GraphRAG, making it more efficient. In contrast, the Integration strategy yields better performance but requires each query to be processed by both RAG and GraphRAG, increasing computational cost.



