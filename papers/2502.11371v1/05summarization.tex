Query-based summarization tasks are widely used to evaluate the performance of RAG systems~\cite{ram2023context, yu2023augmentation}. GraphRAG has also demonstrated its effectiveness in summarization tasks~\cite{edge2024local}. However, \citet{edge2024local} only evaluate its effectiveness on the global summarization task and rely on LLM-as-a-Judge~\cite{zheng2023judging} for performance assessment. In Section~\ref{sec:summ_analysis}, we show that the LLM-as-a-Judge evaluation method for summarization tasks introduces position bias, which can impact the reliability of results.
% \harry{should mention limitation of this type of eval}. 
A systematic comparison of RAG and GraphRAG on general query-based summarization across widely used datasets remains unexplored.  To address this gap, we conduct a comprehensive evaluation in this section, leveraging widely used datasets and evaluation metrics.
% \yu{Since mentioning LLM-as-judget has some problem, might also mention what unique evaluation are we doing here as well.}

\subsection{Datasets and Evaluation Metrics}
We adopt two widely used single-document query-based summarization datasets, SQuALITY~\cite{wang2022squality} and QMSum~\cite{zhong2021qmsum}, and two multi-document query-based summarization datasets, ODSum-story and ODSum-meeting~\cite{zhou2023odsum},  for our evaluation.
% \jt{mention why we do not use their queries\citet{edge2024local}} 
Unlike the LLM-generated global queries used in the unreleased datasets of \citet{edge2024local}, most queries in the selected datasets focus on specific roles or events. Since these datasets contain one or more ground truth summaries for each query, we use ROUGE-2~\cite{lin2004rouge} and BERTScore~\cite{zhang2019bertscore} as evaluation metrics to measure lexical and semantic similarity between the predicted and ground truth summaries.


\begin{table*}[!htb]
\caption{The performance of query-based single document summarization task using Llama3.1-8B.}
\label{tab:summ_single}
\vspace{-0.1in}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccc|cccccc}
\toprule
\multirow{4}{*}{\textbf{Method}}      & \multicolumn{6}{c|}{\textbf{SQuALITY}}                                                                           & \multicolumn{6}{c}{\textbf{QMSum}} \\
\cmidrule{2-13}
\multicolumn{1}{l|}{}      & \multicolumn{3}{c}{\textbf{ROUGE-2}}                     & \multicolumn{3}{c|}{\textbf{BERTScore}}                        & \multicolumn{3}{c}{\textbf{ROUGE-2}}                    & \multicolumn{3}{c}{\textbf{BERTScore}} \\ 
\cmidrule{2-13}
& P              & R             & F1             & P              & R              & F1             & P              & R             & F1            & P              & R              & F1             \\
\midrule
RAG                        & 15.09          & 8.74          & 10.08          & 74.54          & 81.00          & 77.62          & \underline{21.50}          & \textbf{3.80} & \underline{6.32}          & \textbf{81.03} & \underline{84.45} & \textbf{82.69} \\
KG-GraphRAG (Triplets only) & 11.99          & 6.16          & 7.41           & 82.46          & 84.30          & 83.17          & 13.71          & 2.55          & 4.15          & 80.16          & 82.96          & 81.52          \\
KG-GraphRAG (Triplets+Text) & 15.00          & \textbf{9.48} & \underline{10.52} & \textbf{84.37} & \textbf{85.88} & \textbf{84.92} & 16.83          & 3.32          & 5.38          & \underline{80.92}          & 83.64          & 82.25          \\
Community-GraphRAG (Local)  & \textbf{15.82} & 8.64          & 10.10          & \underline{83.93}          & \underline{85.84}          & \underline{84.66}          & 20.54          & 3.35          & 5.64          & 80.63          & 84.13          & 82.34          \\
Community-GraphRAG (Global) & 10.23          & 6.21          & 6.99           & 82.68          & 84.26          & 83.30          & 10.54          & 1.97          & 3.23          & 79.79          & 82.47          & 81.10          \\
Integration                    & \underline{15.69}          & \underline{9.32}          & \textbf{10.67}          & 74.56          & 81.22          & 77.73          & \textbf{21.97} & \textbf{3.80}           & \textbf{6.34} & 80.89          & \textbf{84.47}          & \underline{82.63}          \\ 
\bottomrule
\end{tabular}
}
\vspace{-0.1in}
\end{table*}

\begin{table*}[!htb]
\caption{The performance of query-based multiple document summarization task using Llama3.1-8B.}
\label{tab:summ_multi}
\vspace{-0.1in}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccc|cccccc}
\toprule
\multirow{4}{*}{\textbf{Method}}                   & \multicolumn{6}{c|}{\textbf{ODSum-story}}                                                                        & \multicolumn{6}{c}{\textbf{ODSum-meeting}}                                                                       \\ \cmidrule{2-13}
                   & \multicolumn{3}{c}{\textbf{ROUGE-2}}                    & \multicolumn{3}{c|}{\textbf{BERTScore}}                        & \multicolumn{3}{c}{\textbf{ROUGE-2}}                    & \multicolumn{3}{c}{\textbf{BERTScore}}                         \\ \cmidrule{2-13}
     & P              & R             & F1            & P              & R              & F1             & P              & R             & F1            & P              & R              & F1             \\
\midrule
RAG                        & \textbf{15.39} & \underline{8.44} & \textbf{9.81} & \textbf{83.87} & \textbf{85.74} & \textbf{84.57} & 15.50          & \textbf{6.43} & \textbf{8.77} & \textbf{83.12} & \textbf{85.84} & \textbf{84.45} \\
KG-GraphRAG (Triplets only) & 11.02          & 5.56          & 6.62          & 82.09          & 83.91          & 82.77          & 11.64          & 4.87          & 6.58          & 81.13          & 84.32          & 82.69          \\
KG-GraphRAG (Triplets+Text) & 9.19           & 5.82          & 6.22          & 79.39          & 83.30          & 81.03          & 11.97          & 4.97          & 6.72          & 81.50          & 84.41          & 82.92          \\
Community-GraphRAG (Local)  & \underline{13.84}          & 7.19          & 8.49          & 83.19          & 85.07          & 83.90          & \underline{15.65}          & 5.66          & 8.02          & 82.44          & 85.54          & 83.96          \\
Community-GraphRAG (Global) & 9.40           & 4.47          & 5.46          & 81.46          & 83.54          & 82.30          & 11.44          & 3.89          & 5.59          & 81.20          & 84.50          & 82.81          \\
Integration                    & 14.77          & \textbf{8.55}          & \underline{9.53}          & \underline{83.73}          & \underline{85.56}          & \underline{84.40}          & \textbf{15.69} & \underline{6.15}          & \underline{8.51}         & \underline{82.87}          & \underline{85.81}          & \underline{84.31}          \\ 
\bottomrule
\end{tabular}
}
\vspace{-0.1in}
\end{table*}


\vspace{-0.1in}
\subsection{Summarization Experimental Results}
\label{sec:summ_result}

We evaluate both the KG-based and Community-based GraphRAG methods, 
% \jt{discuss why not selection?} 
along with the Integration strategy discussed in Section~\ref{sec:improve_qa}. 
The results of Llama3.1-8B model on Query-based single document summarization and multiple document summarization are shown in Table~\ref{tab:summ_single} and Table~\ref{tab:summ_multi}, respectively. The results of Llama3.1-70B are shown in Appendix~\ref{app:summ_result}. Based on these results, we can make the following observations: 
% \harry{Added titles...(old is commented out)}
\begin{enumerate}[leftmargin=*, itemsep=0pt, parsep=-1pt]
    \item {\bf RAG generally performs well on query-based summarization tasks}. This is particularly true on multi-document summarization datasets, where they are often the best method.
    \item {\bf KG-based GraphRAG benefit from combining triplets with their corresponding text}. This improves performance by incorporating more details, making predictions closer to the ground truth summaries.
    \item {\bf Community-based GraphRAG performs better with the Local search method}. Local search retrieves entities, relations, and low-level communities, while the Global search method retrieves only high-level summaries. This demonstrates the importance of detailed information in the selected datasets.
    \item {\bf The Integration strategy is often comparable to RAG only performance}. This strategy integrates retrieved content from both RAG and Community-GraphRAG (Local), resulting in performance similar to RAG alone.
    % \item RAG, which directly retrieves original text chunks, generally performs well on query-based summarization tasks, particularly on multi-document summarization datasets.
    % \item KG-based GraphRAG benefits from combining triplets with their corresponding text, which improves performance by incorporating more details, making predictions closer to the ground truth summaries.
    % \item Community-based GraphRAG performs better with the Local search method, which retrieves entities, relations, and low-level communities, compared to the Global search method, which retrieves only high-level summaries. This demonstrates the importance of detailed information in the selected datasets.
    % \item The Integration method, which integrates the retrieved content from both RAG and Community-GraphRAG (Local), generally achieves performance comparable to RAG alone.
\end{enumerate}

\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/QM_Local.pdf}
        \caption{QMSum Local}
        \label{fig:qmsum_local}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/QM_Global.pdf}
        \caption{QMSum Global}
        \label{fig:qmsum_global}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Story_Local.pdf}
        \caption{ODSum-story Local}
        \label{fig:odsum_local}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Story_Global.pdf}
        \caption{ODSum-story Global}
        \label{fig:odsum_global}
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Comparison of LLM-as-a-Judge evaluations for RAG and GraphRAG. "Local" refers to the evaluation of RAG vs. GraphRAG-Local, while "Global" refers to RAG vs. GraphRAG-Global. }
    \label{fig:summ_comparision}
    \vspace{-0.2in}
\end{figure*}

\vspace{-0.2in}
\subsection{Position Bias in Existing Evaluation}
\label{sec:summ_analysis}
% \jt{the title of this subsection can be directly "position bias in existing evaluation"}
From the results in Section~\ref{sec:summ_result}, the Community-based GraphRAG, particularly with global search, generally underperforms compared to RAG on the selected datasets. This contrasts with the findings of \citet{edge2024local}, where Community-based GraphRAG with global search outperformed both local search and RAG.  
There are two key differences between our evaluation and \citet{edge2024local}. First, their study primarily focuses on global summarization, which captures the overall information of an entire corpus, whereas the selected datasets in our evaluation contain queries related to specific roles or events. Second, \citet{edge2024local} assess performance by comparing RAG and GraphRAG outputs using LLM-as-a-Judge without ground truth, whereas we evaluate results against ground truth summaries using ROUGE and BERTScore. These metrics emphasize similarity to the reference summaries, which often contain more detailed information.  

We further conduct an evaluation following~\citet{edge2024local}, using the LLM-as-a-Judge method to compare RAG and Community-based GraphRAG from two perspectives: Comprehensiveness and Diversity. Comprehensiveness focuses on detail, addressing the question: {\it "How much detail does the answer provide to cover all aspects and details of the question?"} Meanwhile, Diversity emphasizes global information, evaluating {\it "Does the answer provide a broad and globally inclusive perspective?"}. The prompt and details are shown in Appendix~\ref{app:llmjudge_prompt}. Specifically, we input the summaries generated by RAG and GraphRAG into the prompt and ask the LLM to select the better one for each metric, following~\citet{edge2024local}. Additionally, to better account for the order in which the summaries are presented, we consider two scenarios. {\it Order 1 (O1)}: We place the RAG summary appears before the GraphRAG summary and {\it Order 2 (O2)}: GraphRAG appears before RAG. We compare the proportion of selected best samples from RAG and GraphRAG, where a higher proportion indicates better performance as predicted by the LLM.  

The results of RAG vs. GraphRAG (Local) and RAG vs. GraphRAG (Global) on the QMSum and ODSum-story datasets are presented in Figure~\ref{fig:summ_comparision}. More result can be found in Appendix~\ref{app:llmjudge_result}. We can make the following observations: (1) \textbf{Position bias~\cite{shi2024judging, wang2024eliminating} is evident in the LLM-as-a-Judge evaluations for summarization task}, as changing the order of the two methods significantly affects the predictions. This effect is particularly strong in the comparison between RAG and GraphRAG (Local), where the LLMs make completely opposite decisions depending on the order, as shown in Figures~\ref{fig:qmsum_local} and~\ref{fig:odsum_local}.
However, (2) Comparison between RAG and GraphRAG (Global): While the proportions vary, RAG consistently outperforms GraphRAG (Global) in Comprehensiveness but underperforms in Diversity as shown in Figures~\ref{fig:qmsum_global} and~\ref{fig:odsum_global}. This result suggests that \textbf{Community-based GraphRAG with Global Search focuses more on the global aspects of whole corpus, whereas RAG captures more detailed information.}