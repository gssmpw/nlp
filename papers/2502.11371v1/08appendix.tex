\subsection{Dataset}
\label{app:dataset}
In this section, we introduce the used datasets in the question answering tasks and query-based summarization tasks.

\subsubsection{Question Answering}
\label{app:qa_dataset}
In the QA tasks, we use the following four widely used datasets:
\begin{itemize}
    \item \textbf{Natural Questions (NQ)}~\cite{kwiatkowski2019natural}: The NQ dataset is a widely used benchmark for evaluating open-domain question answering systems. Introduced by Google, it consists of real user queries from Google Search with corresponding answers extracted from Wikipedia. Since it primarily contains single-hop questions, we use NQ as the representative dataset for single-hop QA. We treat NQ as a single-document QA task, where multiple questions are associated with each document. Accordingly, we build a separate RAG system for each document in the dataset.
    \item \textbf{Hotpot}~\cite{yang2018hotpotqa}: HotpotQA is a widely used multi-hop question dataset that provides 10 paragraphs per question. The dataset includes varying difficulty levels, with easier questions often solvable by LLMs. To ensure a more challenging evaluation, we randomly selected 1,000 hard bridging questions from the development set of HotpotQA. Additionally, we treat HotpotQA as a multi-document QA task and build a single RAG system to handle all questions.
    \item \textbf{MultiHop-RAG}~\cite{tang2024multihop}: MultiHop-RAG is a QA dataset designed to evaluate retrieval and reasoning across multiple documents with metadata in RAG pipelines. Constructed from English news articles, it contains 2,556 queries, with supporting evidence distributed across 2 to 4 documents. The dataset includes four query types: Inference queries, which synthesize claims about a bridge entity to identify it; Comparison queries, which compare similarities or differences and typically yield "yes" or "no" answers; Temporal queries, which examine event ordering with answers like "before" or "after"; and Null queries, where no answer can be derived from the retrieved documents. It is also a multi-document QA task.
    \item \textbf{NovelQA}~\cite{tang2024multihop}: NovelQA is a benchmark designed to evaluate the long-text understanding and retrieval ability of LLMs using manually curated questions about English novels exceeding 50,000 words. The dataset includes queries that focus on minor details or require cross-chapter reasoning, making them inherently challenging for LLMs. It covers various query types such as details, multi-hop, single-hop, character, meaning, plot, relation, setting, span, and times. Key challenges highlighted by NovelQA include grasping abstract meanings (meaning questions), understanding nuanced relationships (relation questions), and tracking temporal sequences and spatial extents (span and time questions), emphasizing the difficulty of maintaining and applying contextual information across long narratives. We use it for single-document QA task.
\end{itemize}


\subsubsection{Query-based Summarization}
\label{app:summ_dataset}
In the Query-based Summarization tasks, we adopt the following four widely used datasets:
\begin{itemize}
    \item \textbf{SQuALITY}~\cite{wang2022squality}: SQuALITY (Summary-format QUestion Answering with Long Input Texts) is a question-focused, long-document, multi-reference summarization dataset. It consists of short stories from Project Gutenberg, each ranging from 4,000 to 6,000 words. Each story is paired with five questions, and each question has four reference summaries written by Upwork writers and NYU undergraduates. SQuALITY is designed as a single-document summarization task, making it a valuable benchmark for evaluating summarization models on long-form content.
    \item \textbf{QMSum}~\cite{zhong2021qmsum}: QMSum is a human-annotated benchmark for query-based, multi-domain meeting summarization, containing 1,808 query-summary pairs from 232 meetings across multiple domains. We use QMSum as a single-document summarization task in our evaluation.
    \item \textbf{ODSum}~\cite{zhou2023odsum}: The ODSum dataset is designed to evaluate modern summarization models in multi-document contexts and consists of two subsets: ODSum-story and ODSum-meeting. ODSum-story is derived from the SQuALITY dataset, while ODSum-meeting is constructed from QMSum. We use both ODSum-story and ODSum-meeting for the multi-document summarization task in our evaluation.
\end{itemize} 

\subsection{More results on NovelQA dataset}
\label{app:novelqa}

In this section, we present the missing results for the NovelQA dataset from the main sections. These include the performance of KG-GraphRAG (Triplets) with LLaMA 3.1-8B (Table~\ref{app:tab:triple-8b}), RAG with LLaMA 3.1-70B (Table~\ref{app:tab:rag-70b}), KG-GraphRAG (Triplets) with LLaMA 3.1-70B (Table~\ref{app:tab:triple-70b}), KG-GraphRAG (Triplets+Text) with LLaMA 3.1-70B (Table~\ref{app:tab:tripletext-70b}), Community-GraphRAG (Local) with LLaMA 3.1-70B (Table~\ref{app:tab:local-70b}), and Community-GraphRAG (Global) with LLaMA 3.1-70B (Table~\ref{app:tab:global-70b}).

\begin{table}[!htb]
\caption{The performance of KG-GraphRAG (Triplets) with Llama 3.1-8B model on NovelQA dataset.}
\label{app:tab:triple-8b}
\centering
\begin{tabular}{c|cccccccc}
\hline
KG-GraphRAG(Triplet) & character & meaning & plot  & relat & settg & span & times & avg   \\ \hline
mh                   & 31.25     & 17.65   & 41.67 & 50.56 & 38.46 & 64   & 26.47 & 32.89 \\
sh                   & 35.53     & 45.71   & 30.54 & 62.5  & 27.84 & -    & -     & 33.75 \\
dtl                  & 31.43     & 24.72   & 35.71 & 17.86 & 27.03 & -    & -     & 27.37 \\
avg                  & 33.7      & 29.81   & 32.63 & 44    & 28.57 & 64   & 26.47 & 31.88 \\ \hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\caption{The performance of RAG with Llama 3.1-70B model on NovelQA dataset.}
\label{app:tab:rag-70b}
\centering
\begin{tabular}{c|cccccccc}
\hline
RAG & character & meaning & plot  & relat & settg & span & times & avg   \\ \hline
mh  & 64.58     & 82.35   & 77.78 & 69.66 & 84.62 & 36   & 36.63 & 48.5  \\
sh  & 70.39     & 70      & 76.57 & 75    & 83.51 & -    & -     & 75.27 \\
dtl & 60        & 51.12   & 76.79 & 67.86 & 83.78 & -    & -     & 61.25 \\
avg & 66.67     & 58.11   & 76.74 & 69.6  & 83.67 & 36   & 36.63 & 61.42 \\ \hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\caption{The performance of KG-GraphRAG (Triplets) with Llama 3.1-70B model on NovelQA dataset.}
\label{app:tab:triple-70b}
\centering
\begin{tabular}{c|cccccccc}
\hline
KG-GraphRAG (Triplets) & character & meaning & plot  & relat & settg & span & times & avg   \\ \hline
mh                     & 50        & 76.47   & 75    & 43.82 & 76.92 & 24   & 22.46 & 33.72 \\
sh                     & 52.63     & 62.86   & 55.23 & 12.5  & 50.52 & -    & -     & 54.06 \\
dtl                    & 35.71     & 26.97   & 39.29 & 53.57 & 37.84 & -    & -     & 33.6  \\
avg                    & 47.78     & 39.62   & 54.68 & 44    & 49.66 & 24   & 22.46 & 41.18 \\ \hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\caption{The performance of KG-GraphRAG (Triplets+Text) with Llama 3.1-70B model on NovelQA dataset.}
\label{app:tab:tripletext-70b}
\centering
\begin{tabular}{c|cccccccc}
\hline
KG-GraphRAG (Triplets+Text) & character & meaning & plot  & relat & settg & span & times & avg   \\ \hline
mh  & 56.25     & 58.82   & 63.89 & 51.69 & 84.62 & 24   & 21.39 & 33.72 \\
sh  & 51.97     & 61.43   & 55.65 & 50    & 50.52 & -    & -     & 54.42 \\
dtl & 34.29     & 25.28   & 41.07 & 50    & 37.84 & -    & -     & 32.52 \\
avg & 48.15     & 36.98   & 54.08 & 51.2  & 50.34 & 24   & 21.39 & 41.05 \\ \hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\caption{The performance of Community-GraphRAG (Local) with Llama 3.1-70B model on NovelQA dataset.}
\label{app:tab:local-70b}
\centering
\begin{tabular}{c|cccccccc}
\hline
 Community-GraphRAG (Local)   & character & meaning & plot  & relat & settg & span & times & avg   \\ \hline
mh  & 77.08     & 70.59   & 63.89 & 77.53 & 92.31 & 28   & 32.35 & 46.68 \\
sh  & 68.42     & 71.43   & 74.9  & 62.5  & 74.23 & -    & -     & 72.44 \\
dtl & 55.71     & 37.08   & 69.64 & 64.29 & 75.68 & -    & -     & 51.49 \\
avg & 66.67     & 48.3    & 72.81 & 73.6  & 76.19 & 28   & 32.35 & 57.32 \\ \hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\caption{The performance of Community-GraphRAG (Global) with Llama 3.1-70B model on NovelQA dataset.}
\label{app:tab:global-70b}
\centering
\begin{tabular}{c|cccccccc}
\hline
Community-GraphRAG (Global) & character & meaning & plot  & relat & settg & span & times & avg   \\ \hline
mh     & 47.92     & 58.82   & 55.56 & 57.3  & 61.54 & 16   & 35.83 & 41.53 \\
sh     & 42.76     & 42.86   & 54.39 & 25    & 40.21 & -    & -     & 47    \\
dtl    & 24.29     & 22.47   & 32.14 & 50    & 35.14 & -    & -     & 27.64 \\
avg    & 38.89     & 30.19   & 50.76 & 53.6  & 40.82 & 16   & 35.83 & 40.21 \\ \hline
\end{tabular}
\end{table}

\subsection{RAG vs. GraphRAG Selection}
\label{app:selection}
We classify QA queries into Fact-based and Reasoning-based queries. Fact-based queries are processed using RAG, while Reasoning-based queries are handled by GraphRAG. The Query Classification prompt is shown in Figure~\ref{app:fig:query_classification}.

\begin{figure}[!htb]
% \vspace{-10 pt}
\begin{tcolorbox}[mybox={Prompt for Query Classification}]

System Prompt: Classifying Queries into Fact-Based and Reasoning-Based Categories

You are an AI model tasked with classifying queries into one of two categories based on their complexity and reasoning requirements.



\textbf{Category Definitions}

1. \textbf{Fact-Based Queries}

   - The answer can be directly retrieved from a knowledge source or requires details.  
   
   - The query does not require multi-step reasoning, inference, or cross-referencing multiple sources.  

2. \textbf{Reasoning-Based Queries}

   - The answer cannot be found in a single lookup and requires cross-referencing multiple sources, logical inference, or multi-step reasoning.  



\textbf{Examples}

\textbf{Fact-Based Queries}

\{\{ Fact-Based Queries Examples \}\}  

\textbf{Reasoning-Based Queries}

\{\{ Reasoning-Based Queries Examples \}\}  


\end{tcolorbox}
% \vspace{-10 pt}
\caption{Prompt for Query Classification.}
\label{app:fig:query_classification}
% \vspace{-10 pt}
\end{figure}

\subsection{Query-based Summarization Results with Llama3.1-70B model}
\label{app:summ_result}
In this section, we present the results for Query-based Summarization tasks using the LLaMA 3.1-70B model. The results for single-document summarization are shown in Table~\ref{app:tab:summ_single_70B}, while the results for multi-document summarization are provided in Table~\ref{app:tab:summ_multi_70B}.

\begin{table*}[!htb]
\caption{The performance of query-based single document summarization task using Llama3.1-70B.}
\label{app:tab:summ_single_70B}
% 
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccc|cccccc}
\toprule
\multirow{4}{*}{Method}      & \multicolumn{6}{c|}{SQuALITY}                                                                           & \multicolumn{6}{c}{QMSum} \\
\cmidrule{2-13}
\multicolumn{1}{l|}{}      & \multicolumn{3}{c}{ROUGE-2}                     & \multicolumn{3}{c|}{BERTScore}                        & \multicolumn{3}{c}{ROUGE-2}                    & \multicolumn{3}{c}{BERTScore} \\ 
\cmidrule{2-13}
& P              & R             & F1             & P              & R              & F1             & P              & R             & F1            & P              & R              & F1             \\
\midrule
RAG                        & 11.85 & 14.24   & 11.00 & 85.96 & 85.76 & 85.67 & 10.42 & 10.00 & 9.53 & 86.14 & 85.92 & 86.02 \\
KG-GraphRAG(Triplets only) & 8.53  & 10.28   & 7.46  & 84.13 & 83.97 & 83.89 & 10.62 & 6.25  & 7.48 & 83.20 & 84.72 & 83.94 \\
KG-GraphRAG(Triplets+Text) & 6.57  & 10.14   & 6.00  & 80.52 & 82.23 & 81.07 & 8.64  & 7.85  & 7.29 & 84.10 & 84.55 & 84.31 \\
Community-GraphRAG(Local)  & 12.54 & 10.31   & 9.61  & 84.50 & 85.33 & 84.71 & 13.69 & 7.43  & 9.14 & 84.09 & 85.85 & 84.95 \\
Community-GraphRAG(Global) & 8.99  & 4.78    & 5.60  & 81.64 & 83.64 & 82.44 & 10.97 & 4.40  & 6.01 & 81.93 & 84.67 & 83.26 \\
Combine                    & 13.59 & 11.32   & 10.55 & 84.88 & 85.76 & 85.12 & 13.16 & 8.67  & 9.93 & 85.18 & 86.21 & 85.69 \\
\bottomrule
\end{tabular}
}
% 
\end{table*}


\begin{table*}[!htb]
\caption{The performance of query-based multiple document summarization task using Llama3.1-70B.}
\label{app:tab:summ_multi_70B}
% 
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccc|cccccc}
\toprule
\multirow{4}{*}{Method}                   & \multicolumn{6}{c|}{ODSum-story}                                                                        & \multicolumn{6}{c}{ODSum-meeting}                                                                       \\ \cmidrule{2-13}
                   & \multicolumn{3}{c}{ROUGE-2}                    & \multicolumn{3}{c|}{BERTScore}                        & \multicolumn{3}{c}{ROUGE-2}                    & \multicolumn{3}{c}{BERTScore}                         \\ \cmidrule{2-13}
     & P              & R             & F1            & P              & R              & F1             & P              & R             & F1            & P              & R              & F1             \\
\midrule
RAG                        & 15.60 & 9.98    & 11.09 & 74.80 & 81.29 & 77.89 & 18.81 & 6.41    & 8.97  & 83.56 & 85.16 & 84.34 \\
KG-GraphRAG(Triplets only) & 10.08 & 9.12    & 8.48  & 75.71 & 81.93 & 78.66 & 11.52 & 3.41    & 4.79  & 81.19 & 83.07 & 82.11 \\
KG-GraphRAG(Triplets+Text) & 10.98 & 16.67   & 11.42 & 76.74 & 81.92 & 79.21 & 13.09 & 6.31    & 7.70  & 84.07 & 84.24 & 84.14 \\
Community-GraphRAG(Local)  & 14.20 & 11.34   & 11.25 & 75.44 & 81.81 & 78.46 & 16.17 & 7.87    & 9.23  & 84.17 & 84.85 & 84.49 \\
Community-GraphRAG(Global) & 10.46 & 6.30    & 7.08  & 74.63 & 81.24 & 77.77 & 10.65 & 1.99    & 3.28  & 79.78 & 82.53 & 81.12 \\
Combine                    & 14.76 & 12.17   & 11.72 & 75.39 & 81.75 & 78.41 & 17.57 & 8.64    & 10.34 & 84.51 & 85.14 & 84.81 \\
\bottomrule
\end{tabular}
}
% 
\end{table*}

\subsection{The LLM-as-a-Judge Prompt }
\label{app:llmjudge_prompt}
The LLM-as-a-Judge prompt can be found in Figure~\ref{app:fig:llmas}. 

\begin{figure}[!htb]
% \vspace{-10 pt}
\begin{tcolorbox}[mybox={LLM-as-a-Judge Prompt}]

You are an expert evaluator assessing the quality of responses in a query-based summarization task.\\

Below is a query, followed by two LLM-generated summarization answers. Your task is to evaluate the best answer based on the given criteria. For each aspect, select the model that performs better.

\textbf{Query}

\{\{query\}\}

\textbf{Answers Section}

\textbf{The Answer of Model 1:}

\{\{answer 1\}\}

\textbf{The Answer of Model 2:}

\{\{answer 2\}\}

\textbf{Evaluation Criteria}
Assess each LLM-generated answer independently based on the following two aspects:

1. \textbf{Comprehensiveness} \\
   - Does the answer fully address the query and include all relevant information?  \\
   - A comprehensive answer should cover all key points, ensuring that no important details are missing.  \\
   - It should present a well-rounded view, incorporating relevant context when necessary. \\ 
   - The level of detail should be sufficient to fully inform the reader without unnecessary omission or excessive brevity.  \\

2. \textbf{Global Diversity} \\
   - Does the answer provide a broad and globally inclusive perspective?  \\
   - A globally diverse response should avoid narrow or region-specific biases and instead consider multiple viewpoints. \\
   - The response should be accessible and relevant to a wide, international audience rather than assuming familiarity with specific local contexts.  
\end{tcolorbox}
% \vspace{-10 pt}
\caption{LLM-as-a-Judge Prompt.}
\label{app:fig:llmas}
% \vspace{-10 pt}
\end{figure}




\subsection{The LLM-as-a-Judge Results on more datasets}
\label{app:llmjudge_result}
In the main section, we present LLM-as-a-Judge results for the OMSum and ODSum-story datasets. Here, we provide additional results on the SQuALITY and ODSum-meeting datasets, as shown in Figure~\ref{app:fig:summ_comparision1}.
\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/SQU_Local.pdf}
        \caption{SQuALITY Local}
        \label{fig:squ_local}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/SQU_Global.pdf}
        \caption{SQuALITY Global}
        \label{fig:squ_global}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Meeting_Local.pdf}
        \caption{ODSum-meeting Local}
        \label{fig:meeting_local}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Meeting_Global.pdf}
        \caption{ODSum-meeting Global}
        \label{fig:meeting_global}
    \end{subfigure}
    
    \caption{Comparison of LLM-as-a-Judge evaluations for RAG and GraphRAG. "Local" refers to the evaluation of RAG vs. GraphRAG-Local, while "Global" refers to RAG vs. GraphRAG-Global.  "Order 1" corresponds to the prompt where RAG result is presented before GraphRAG, whereas "Order 2" corresponds to the reversed order.}
    \label{app:fig:summ_comparision1}
    % \vspace{-0.2in}
\end{figure*}