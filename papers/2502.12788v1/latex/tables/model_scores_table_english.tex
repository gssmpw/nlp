




% \begin{table*}[t]
% \centering
% \resizebox{0.8\textwidth}{!}{%
% \begin{tabular}{lcccccc}
% \hline
% \multirow{2}{*}{\textbf{Model (\#parameter)}} &
%   \multicolumn{3}{c}{\textbf{Completion}} &
%   \multicolumn{3}{c}{\textbf{MCQ}} \\ \cline{2-7} 
%                              & $\ell =$ None    & $\ell =$ Region  & $\ell =$ Country + region & $\ell =$ None    & $\ell =$ Region  & $\ell =$ Country + region \\ \hline
% Human                        & $-$           & $-$           & 100           & $-$           & $-$           & 100           \\
% Random                       & 33.3          & 33.3          & 33.3          & 33.3          & 33.3          & 33.3          \\ \hline
% BLOOMZ (560M)                & 26.0          & 25.7          & 25.8          & 34.1          & 33.1          & 34.1          \\
% BLOOMZ (7B)                  & 27.3          & 27.6          & 27.4          & 65.9          & 66.3          & 67.0          \\
% \hdashline mT0$_{xxl}$ (14B) & 31.6          & 31.4          & 31.7          & 57.9          & 57.8          & 58.5          \\
% \hdashline Llama-3.2 (1B)    & 25.2          & 24.5          & 24.4          & 34.0          & 33.9          & 33.9          \\
% Llama-3.2 Instruct (1B)      & 24.8          & 25.0          & 24.9          & 34.4          & 34.6          & 34.5          \\
% Llama-3.2 (3B)               & 26.6          & 27.6          & 27.4          & 34.2          & 34.3          & 34.3          \\
% Llama-3.2 Instruct (3B)      & 28.8          & 28.0          & 27.9          & 34.8          & 36.1          & 37.1          \\
% Llama-3.1 (8B)               & 29.7          & 29.7          & 29.6          & 35.0          & 34.7          & 34.9          \\
% Llama-3.1 Instruct (8B)      & 32.2          & 31.0          & 31.3          & 47.5          & 47.0          & 49.1          \\
% Llama-3 Instruct (70B)       & 39.1          & 39.5          & 39.4          & 47.1          & 37.0          & 38.7          \\
% Llama-3.3 Instruct (70B)     & 39.9          & \textbf{40.6} & \textbf{41.1} & 78.4          & 77.8          & 78.8          \\
% \hdashline Aya-Expanse (8B)  & 34.7          & 37.2          & 38.2          & 39.6          & 40.7          & 41.8          \\
% Aya-Expanse (32B)            & 37.9          & 39.3          & 39.9          & 52.6          & 49.5          & 49.5          \\
% \hdashline Gemma-2 (2B)      & 27.2          & 27.2          & 27.6          & 34.3          & 34.3          & 34.3          \\
% Gemma-2 Instruct (2B)        & 27.7          & 27.8          & 27.5          & 38.5          & 40.3          & 40.5          \\
% Gemma-2 (9B)                 & 32.0          & 32.2          & 32.7          & 35.2          & 35.8          & 35.4          \\
% Gemma-2 Instruct (9B)        & 33.5          & 33.8          & 33.9          & 58.7          & 55.3          & 57.0          \\
% Gemma-2 (27B)                & 33.8          & 34.4          & 35.0          & 34.3          & 34.3          & 34.4          \\
% Gemma-2 Instruct (27B)       & 38.0          & 38.9          & 39.8          & 61.6          & 64.7          & 64.2          \\
% \hdashline Qwen2.5 (0.5B)    & 24.7          & 24.6          & 24.8          & 34.3          & 34.2          & 34.3          \\
% Qwen2.5 Instruct (0.5B)      & 24.7          & 25.0          & 25.1          & 40.2          & 37.7          & 37.9          \\
% Qwen2.5 (1.5B)               & 27.1          & 27.2          & 27.4          & 42.2          & 38.2          & 39.2          \\
% Qwen2.5 Instruct (1.5B)      & 27.8          & 27.5          & 27.9          & 54.8          & 50.1          & 50.5          \\
% Qwen2.5 (3B)                 & 29.0          & 28.3          & 28.9          & 39.6          & 37.6          & 37.8          \\
% Qwen2.5 Instruct (3B)        & 30.9          & 30.5          & 30.2          & 57.4          & 52.0          & 55.2          \\
% Qwen2.5 (7B)                 & 30.0          & 31.5          & 31.8          & 52.1          & 48.1          & 49.0          \\
% Qwen2.5 Instruct (7B)        & 33.2          & 33.5          & 33.8          & 53.1          & 47.9          & 48.8          \\
% Qwen2.5 (14B)                & 33.6          & 34.5          & 35.4          & 55.2          & 62.5          & 61.6          \\
% Qwen2.5 Instruct (14B)       & 36.5          & 37.7          & 37.2          & 67.7          & 67.9          & 69.3          \\
% Qwen2.5 (32B)                & 34.9          & 35.6          & 35.9          & 51.6          & 56.6          & 53.3          \\
% Qwen2.5 Instruct (32B)       & 37.6          & 37.7          & 38.6          & 75.2          & 75.8          & 76.5          \\
% Qwen2.5 (72B)                & 35.5          & 36.7          & 37.4          & 56.1          & 52.2          & 57.3          \\
% Qwen2.5 Instruct (72B) &
%   \textbf{40.1} &
%   40.2 &
%   41.0 &
%   \textbf{80.1} &
%   \textbf{79.8} &
%   \textbf{80.0} \\ \hline
% Jais (13B)                   & 39.3          & 39.0          & 39.3          & 34.1          & 34.9          & 34.8          \\
% Jais chat (13B)              & 40.8          & 40.8          & 41.9          & 58.3          & 54.1          & 54.4          \\
% Jais-v3 (30B)                & 39.9          & 40.4          & 40.6          & 34.8          & 35.8          & 35.6          \\
% Jais-v3 Chat (30B)           & 34.0          & 34.0          & 34.5          & 60.1          & 56.2          & 54.0          \\
% \hdashline SILMA Instruct (9B)          & 32.7          & 33.3          & 33.4          & 71.5          & 71.0          & 72.0          \\
% \hdashline AceGPT-v2 (8B)               & 32.4          & 34.0          & 34.6          & 35.1          & 35.0          & 35.1          \\
% AceGPT-v2 Chat (8B)          & 36.0          & 36.4          & 37.3          & 44.8          & 45.0          & 45.4          \\
% AceGPT-v2 Chat (32B)         & 38.5          & 39.2          & 40.0          & \textbf{79.7} & \textbf{79.1} & \textbf{79.8} \\
% AceGPT-v2 Chat (70B)         & \textbf{43.2} & \textbf{44.5} & \textbf{45.1} & 73.6          & 73.0          & 74.4          \\ \hline
% GPT-4o (NA) & $-$ & $-$ & $-$ & \textbf{88.5} & \textbf{91.9} & \textbf{91.1} \\ \hline
% \end{tabular}%
% }
% \caption{Zero-shot accuracy across various models and settings. “MCQ” refers to the multiple-choice question method, and $l$ denotes the location as additional context (“Region” and “Country” denote the region, and the corresponding country). The bold numbers highlight the highest score within each model group.}
% \label{tab:model_scores}
% \end{table*}


% \begin{table*}[t]
% \centering
% \resizebox{0.8\textwidth}{!}{%
% \begin{tabular}{lcccccc}
% \hline
% \multirow{2}{*}{\textbf{Model (\#parameter)}} &

%   \multicolumn{3}{c}{\textbf{Completion}} &
%   \multicolumn{3}{c}{\textbf{MCQ}} \\ \cline{2-7} 
%                              & $\ell =$ None    & $\ell =$ Region  & $\ell =$ Country + region & $\ell =$ None    & $\ell =$ Region  & $\ell =$ Country + region \\ \hline
% Human                        & $-$           & $-$           & 100.0         & $-$           & $-$           & 100.0         \\
% Random                                        & 33.3          & 33.3            & 33.3                      & 33.3          & 33.3            & 33.3                       \\ 
% \hline
% BLOOMZ (560M)                                 & 26.0         & 25.5           & 25.8                     & 32.3         & 31.7           & 31.7                      \\

% BLOOMZ (7B)                             & 31.6         & 31.4           & 31.7                     & 57.9         & 57.8           & 58.5                       \\ \hdashline
% mT0$_xxl$ (14B)                                     & 27.3          & 27.6            & 27.4                      & 65.9          & 66.3            & 67.0                         \\ \hdashline
% Llama-3.2 (1B)                                & 25.2         & 24.5           & 24.4                     & 33.6         & 33.7           & 33.9                      \\
% Llama-3.2 Instruct (1B)                       & 24.8         & 24.4           & 24.9                      & 34.4         & 34.1           & 34.4                      \\
% Llama-3.2 (3B)                                & 26.6         & 27.6           & 27.4                     & 32.3         & 32.5           & 33.4                      \\
% Llama-3.2 Instruct (3B)                       & 28.8         & 28.0           & 27.9                     & 34.8         & 36.1           & 37.1                      \\
% Llama-3.1 (8B)                                & 29.7         & 29.7            & 29.6                      & 35.0          & 34.7            & 34.9                       \\
% Llama-3.1 Instruct (8B)                       & 30.9         & 31.0             & 31.2                      & 47.5          & 47.0            & 49.1                       \\
% Llama-3 Instruct (70B)                        & 39.1         & 39.5            & 39.4                      & 34.3          & 34.3            & 34.3                       \\
% Llama-3.3 Instruct (70B)                      & 39.9         & \textbf{40.6}            & \textbf{41.1}                      & 75.4          & 74.0            & 71.2                       \\ 
% \hdashline
% Aya-Expanse (8B)                              & 33.7         & 37.2           & 38.2                      & 39.6         & 40.7            & 41.8                      \\
% Aya-Expanse (32B)                             & 37.9         & 37.9            & 39.5                      & 52.6          & 49.5            & 49.5                       \\ 
% \hdashline
% Gemma-2 (2B)                                  & 27.2         & 26.6           & 26.6                     & 34.3         & 34.3           & 34.3                      \\
% Gemma-2 Instruct (2B)                         & 27.7         & 27.8           & 27.3                     & 38.5         & 40.3           & 40.5                      \\
% Gemma-2 (9B)                                  & 31.8         & 31.7           & 31.8                     & 35.2         & 34.5           & 34.5                      \\
% Gemma-2 Instruct (9B)                         & 33.5         & 33.8            & 33.9                     & 58.7          & 55.3           & 57.0                      \\
% Gemma-2 (27B)                                 & 32.6         & 33.0            & 33.2                      & 34.3         & 34.3           & 34.3                      \\
% Gemma-2 Instruct (27B)                        & 38.0         & 38.9           & 39.8                      & 61.6         & 64.7           & 64.2                      \\ 
% \hdashline
% Qwen2.5 (0.5B)                                & 24.7         & 24.6           & 24.8                     & 32.2         & 33.2            & 32.6                      \\
% Qwen2.5 Instruct (0.5B)                       & 24.1         & 24.5           & 24.6                     & 40.2         & 37.7           & 37.9                      \\
% Qwen2.5 (1.5B)                                & 27.1         & 27.2           & 27.4                     & 40.9         & 38.2           & 39.2                      \\
% Qwen2.5 Instruct (1.5B)                       & 27.7         & 27.3           & 27.9                     & 54.8         & 50.1           & 50.5                      \\
% Qwen2.5 (7B)                                  & 29.0         & 31.5           & 31.8                     & 52.1         & 48.1           & 49.0                      \\
% Qwen2.5 Instruct (7B)                         & 33.2         & 33.5           & 33.6                     & 53.1         & 47.9           & 48.8                      \\
% Qwen2.5 (14B)                                 & 33.6         & 34.5            & 35.4                      & 55.2          & 62.5            & 61.6                       \\
% Qwen2.5 Instruct (14B)                        & 36.5         & 35.9            & 37.0                      & 67.7         & 67.9            & 69.3                       \\
% Qwen2.5 (32B)                                 & 34.9         & 35.6           & 35.9                     & 51.6         & 56.6           & 53.3                      \\
% Qwen2.5 Instruct (32B)                        & 37.6         & 37.3           & 38.6                     & 75.2         & 75.8           & 76.5                      \\
% Qwen2.5 (72B)                                 & 35.5         & 36.7           & 37.4                     & 56.1         & 51.6           & 51.8                      \\
% Qwen2.5 Instruct (72B)                        & \textbf{40.1}         & 40.2           & 40.3                     & \textbf{80.1}         & \textbf{79.8}           & \textbf{80.0}                      \\ 
% \hline
% Jais (13B)                                    & 39.3         & 39.0           & 39.3                      & 34.1         & 34.9            & 34.8                       \\
% Jais chat (13B)                               & 40.8         & 40.8           & 41.9                      & 58.3         & 54.1            & 54.4                       \\
% Jais-v3 (30B)                                 & 39.4         & 38.4           & 39.1                      & 34.3         & 34.3            & 34.3                       \\
% Jais-v3 Chat (30B)                            & 33.3         & 33.6            & 33.4                      & 60.1         & 56.2            & 54.0                       \\ 
% \hdashline
% SILMA Instruct (9B)                           & 32.7         & 33.0           & 33.2                      & 71.5         & 71.0            & 72.0                         \\ 
% \hdashline
% AceGPT-v2 (8B)                                & 32.4         & 34.0           & 34.6                     & 35.1         & 35.0           & 35.1                      \\
% AceGPT-v2 Chat (8B)                           & 36.0         & 36.4           & 37.3                     & 43.1         & 39.7           & 39.3                      \\
% AceGPT-v2 Chat (32B)                          & 38.5         & 39.2            & 40.0                     & \textbf{79.7}         & \textbf{79.1}           & \textbf{79.6}                      \\ 

% AceGPT-v2 Chat (70B)                          & \textbf{43.2}         & \textbf{44.3}           & \textbf{44.5}                     & 61.9         & 61.7           & 62.4                      \\
% \hline
% GPT-4o                                        &               &                 &                           & \textbf{88.5}         & \textbf{89.6}            & \textbf{90.0}                        \\
% \hline
% \end{tabular}%
% }
% \caption{Zero-shot accuracy across for the English prompt for various models and settings. “MCQ” refers to the multiple-choice question method, and $l$ denotes the location as additional context (“Region” and “Country” denote the region, and the corresponding country). The bold numbers highlight the highest score within each model group.}
% \label{tab:model_scores}
% \end{table*}


\begin{table*}[t]
\centering
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{lcccccc}
\hline
\multirow{2}{*}{\textbf{Model (\#parameter)}} &

  \multicolumn{3}{c}{\cellcolor{blue!7}\textbf{Completion}} &
  \multicolumn{3}{c}{\cellcolor{red!7}\textbf{MCQ}} \\ \cline{2-7} 
                             & \cellcolor{blue!7}$\ell =$ None    & \cellcolor{blue!7}$\ell =$ R  & \cellcolor{blue!7}$\ell =$ R + C & \cellcolor{red!7}$\ell =$ None    & \cellcolor{red!7}$\ell =$ R  & \cellcolor{red!7}$\ell =$ R + C \\ \hline
Human                        & $-$           & $-$           & 100.0         & $-$           & $-$           & 100.0         \\
Random                                        & 33.3          & 33.3            & 33.3                      & 33.3          & 33.3            & 33.3                       \\ 
\hline


BLOOMZ (7B)                             & 31.6         & 31.4           & 31.7                     & 57.9         & 57.8           & 58.5                       \\ \hdashline
mT0$_\text{xxl}$ (14B)                                     & 27.3          & 27.6            & 27.4                      & 65.9          & 66.3            & 67.0                         \\ \hdashline
Llama-3.1 (8B)                                & 29.7         & 29.7            & 29.6                      & 35.0          & 34.7            & 34.9                       \\
Llama-3.1 Instruct (8B)                       & 30.9         & 31.0             & 31.2                      & 47.5          & 47.0            & 49.1                       \\
Llama-3 Instruct (70B)                        & 39.1         & 39.5            & 39.4                      & 34.3          & 34.3            & 34.3                       \\
Llama-3.3 Instruct (70B)                      & 39.9         & \textbf{40.6}            & \textbf{41.1}                      & 75.4          & 74.0            & 71.2                       \\ 
\hdashline
Aya-Expanse (8B)                              & 33.7         & 37.2           & 38.2                      & 39.6         & 40.7            & 41.8                      \\
Aya-Expanse (32B)                             & 37.9         & 37.9            & 39.5                      & 52.6          & 49.5            & 49.5                       \\ 
\hdashline
Gemma-2 (9B)                                  & 31.8         & 31.7           & 31.8                     & 35.2         & 34.5           & 34.5                      \\
Gemma-2 Instruct (9B)                         & 33.5         & 33.8            & 33.9                     & 58.7          & 55.3           & 57.0                      \\
Gemma-2 (27B)                                 & 32.6         & 33.0            & 33.2                      & 34.3         & 34.3           & 34.3                      \\
Gemma-2 Instruct (27B)                        & 38.0         & 38.9           & 39.8                      & 61.6         & 64.7           & 64.2                      \\ 
\hdashline
Qwen2.5 (7B)                                  & 29.0         & 31.5           & 31.8                     & 52.1         & 48.1           & 49.0                      \\
Qwen2.5 Instruct (7B)                         & 33.2         & 33.5           & 33.6                     & 53.1         & 47.9           & 48.8                      \\
Qwen2.5 (14B)                                 & 33.6         & 34.5            & 35.4                      & 55.2          & 62.5            & 61.6                       \\
Qwen2.5 Instruct (14B)                        & 36.5         & 35.9            & 37.0                      & 67.7         & 67.9            & 69.3                       \\
Qwen2.5 (32B)                                 & 34.9         & 35.6           & 35.9                     & 51.6         & 56.6           & 53.3                      \\
Qwen2.5 Instruct (32B)                        & 37.6         & 37.3           & 38.6                     & 75.2         & 75.8           & 76.5                      \\
Qwen2.5 (72B)                                 & 35.5         & 36.7           & 37.4                     & 56.1         & 51.6           & 51.8                      \\
Qwen2.5 Instruct (72B)                        & \textbf{40.1}         & 40.2           & 40.3                     & \textbf{80.1}         & \textbf{79.8}           & \textbf{80.0}                      \\ 
\hline
Jais (13B)                                    & 39.3         & 39.0           & 39.3                      & 34.1         & 34.9            & 34.8                       \\
Jais Chat (13B)                               & 40.8         & 40.8           & 41.9                      & 58.3         & 54.1            & 54.4                       \\
Jais-v3 (30B)                                 & 39.4         & 38.4           & 39.1                      & 34.3         & 34.3            & 34.3                       \\
Jais-v3 Chat (30B)                            & 33.3         & 33.6            & 33.4                      & 60.1         & 56.2            & 54.0                       \\ 
\hdashline
SILMA Instruct (9B)                           & 32.7         & 33.0           & 33.2                      & 71.5         & 71.0            & 72.0                         \\ 
\hdashline
AceGPT-v2 (8B)                                & 32.4         & 34.0           & 34.6                     & 35.1         & 35.0           & 35.1                      \\
AceGPT-v2 Chat (8B)                           & 36.0         & 36.4           & 37.3                     & 43.1         & 39.7           & 39.3                      \\
AceGPT-v2 Chat (32B)                          & 38.5         & 39.2            & 40.0                     & \textbf{79.7}         & \textbf{79.1}           & \textbf{79.6}                      \\ 

AceGPT-v2 Chat (70B)                          & \textbf{43.2}         & \textbf{44.3}           & \textbf{44.5}                     & 61.9         & 61.7           & 62.4                      \\
\hdashline
ALLaM-Instruct-preview (7B)                           & 37.7         & 38.4           & 39.2                      & 67.4         & 72.0            & 72.6                         \\ 

\hline
GPT-4o                                        &     --          &                -- & --                          & \textbf{88.5}         & \textbf{89.6}            & \textbf{90.0}                        \\
\hline
\end{tabular}%
}
\caption{Zero-shot accuracy results for the English prompt across various models and settings. ''MCQ`` refers to the multiple-choice question evaluation method, and  $\ell$ represents the inclusion of location context (''R`` indicates the region, and ''C`` denotes the corresponding country). Bolded numbers highlight the highest score within each model group}
\label{tab:model_scores}
%\vspace{-0.5cm}
\end{table*}