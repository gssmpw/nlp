@inproceedings{Bisk2019PIQARA,
  title={PIQA: Reasoning about Physical Commonsense in Natural Language},
  author={Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:208290939}
}

@article{Tawalbeh2020IsTS,
  title={Is this sentence valid? An Arabic Dataset for Commonsense Validation},
  author={Saja Khaled Tawalbeh and Mohammad Al-Smadi},
  journal={ArXiv},
  year={2020},
  volume={abs/2008.10873},
  url={https://api.semanticscholar.org/CorpusID:221293124}
}

@inproceedings{abdelali2024larabench,
  title={Larabench: Benchmarking arabic ai with large language models},
  author={Abdelali, Ahmed and Mubarak, Hamdy and Chowdhury, Shammur and Hasanain, Maram and Mousi, Basel and Boughorbel, Sabri and Abdaljalil, Samir and El Kheir, Yassine and Izham, Daniel and Dalvi, Fahim and others},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={487--520},
  year={2024}
}

@article{acquaye2024susu,
  title={Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the US},
  author={Acquaye, Christabel and An, Haozhe and Rudinger, Rachel},
  journal={arXiv preprint arXiv:2410.16451},
  year={2024}
}

@inproceedings{akhtar-etal-2023-exploring,
    title = "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
    author = "Akhtar, Mubashara  and
      Shankarampeta, Abhilash  and
      Gupta, Vivek  and
      Patil, Arpit  and
      Cocarascu, Oana  and
      Simperl, Elena",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.1028/",
    doi = "10.18653/v1/2023.findings-emnlp.1028",
    pages = "15391--15405",
    abstract = "Numerical data plays a crucial role in various real-world domains like finance, economics, and science. Thus, understanding and reasoning with numbers are essential in these fields. Recent benchmarks have assessed the numerical reasoning abilities of language models, revealing their limitations in limited and specific numerical aspects. In this paper, we propose a complete hierarchical taxonomy for numerical reasoning skills, encompassing over ten reasoning types across four levels: representation, number sense, manipulation, and complex reasoning. We conduct a comprehensive evaluation of state-of-the-art models on all reasoning types. To identify challenging reasoning types for different model types, we develop a diverse and extensive set of numerical probes and measure performance shifts. By employing a semi-automated approach, we focus on the tabular Natural Language Inference (TNLI) task as a case study. While no single model excels in all reasoning types, FlanT5 (few-/zero-shot) and GPT3.5 (few-shot) demonstrate strong overall numerical reasoning skills compared to other models in our probes."
}

@inproceedings{al2021commonsense,
  title={Commonsense validation for Arabic sentences using deep learning},
  author={Al-Bashabsheh, Emran and Al-Khazaleh, Huthaifa and Elayan, Omar and Duwairi, Rehab},
  booktitle={2021 22nd International Arab Conference on Information Technology (ACIT)},
  pages={1--7},
  year={2021},
  organization={IEEE}
}

@inproceedings{alshanik2023commonsense,
  title={Commonsense Validation and Explanation for Arabic Sentences},
  author={Alshanik, Farah and Al-Sharif, Ibrahim and Abdullah, Mohammad W},
  booktitle={International Conference on Emerging Trends and Applications in Artificial Intelligence},
  pages={101--112},
  year={2023},
  organization={Springer}
}

@article{bari2024allam,
  title={ALLaM: Large language models for arabic and english},
  author={Bari, M Saiful and Alnumay, Yazeed and Alzahrani, Norah A and Alotaibi, Nouf M and Alyahya, Hisham A and AlRashed, Sultan and Mirza, Faisal A and Alsubaie, Shaykhah Z and Alahmed, Hassan A and Alabduljabbar, Ghadah and others},
  journal={arXiv preprint arXiv:2407.15390},
  year={2024}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{du-etal-2022-e,
    title = "e-{CARE}: a New Dataset for Exploring Explainable Causal Reasoning",
    author = "Du, Li  and
      Ding, Xiao  and
      Xiong, Kai  and
      Liu, Ting  and
      Qin, Bing",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.33/",
    doi = "10.18653/v1/2022.acl-long.33",
    pages = "432--446",
    abstract = "Understanding causality has vital importance for various Natural Language Processing (NLP) applications. Beyond the labeled instances, conceptual explanations of the causality can provide deep understanding of the causal fact to facilitate the causal reasoning process. However, such explanation information still remains absent in existing causal reasoning resources. In this paper, we fill this gap by presenting a human-annotated explainable CAusal REasoning dataset (e-CARE), which contains over 20K causal reasoning questions, together with natural language formed explanations of the causal questions. Experimental results show that generating valid explanations for causal facts still remains especially challenging for the state-of-the-art models, and the explanation information can be helpful for promoting the accuracy and stability of causal reasoning models."
}

@inproceedings{elmadany-etal-2023-orca,
    title = "{ORCA}: A Challenging Benchmark for {A}rabic Language Understanding",
    author = "Elmadany, AbdelRahim  and
      Nagoudi, ElMoatez Billah  and
      Abdul-Mageed, Muhammad",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.609/",
    doi = "10.18653/v1/2023.findings-acl.609",
    pages = "9559--9586",
    abstract = "Due to the crucial role pretrained language models play in modern NLP, several benchmarks have been proposed to evaluate their performance. In spite of these efforts, no public benchmark of diverse nature currently exists for evaluating Arabic NLU. This makes it challenging to measure progress for both Arabic and multilingual language models. This challenge is compounded by the fact that any benchmark targeting Arabic needs to take into account the fact that Arabic is not a single language but rather a collection of languages and language varieties. In this work, we introduce a publicly available benchmark for Arabic language understanding evaluation dubbed ORCA. It is carefully constructed to cover diverse Arabic varieties and a wide range of challenging Arabic understanding tasks exploiting 60 different datasets (across seven NLU task clusters). To measure current progress in Arabic NLU, we use ORCA to offer a comprehensive comparison between 18 multilingual and Arabic language models. We also provide a public leaderboard with a unified single-number evaluation metric (ORCA score) to facilitate future research."
}

@inproceedings{elmadany2023dolphin,
  title={Dolphin: A Challenging and Diverse Benchmark for Arabic NLG},
  author={Elmadany, Abdelrahim and El-Shangiti, Ahmed and Abdul-Mageed, Muhammad and others},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={1404--1422},
  year={2023}
}

@article{fanar,
  title={Fanar: An arabic-centric multimodal generative ai platform. https://fanar.qa/en.},
  author={Fanar-Team},
  journal={arXiv preprint arXiv:2409.11404},
  year={2024}
}

@article{huang2023acegpt,
  title={Acegpt, localizing large language models in arabic},
  author={Huang, Huang and Yu, Fei and Zhu, Jianqing and Sun, Xuening and Cheng, Hao and Song, Dingjie and Chen, Zhihong and Alharthi, Abdulmohsen and An, Bang and He, Juncai and others},
  journal={arXiv preprint arXiv:2309.12053},
  year={2023}
}

@inproceedings{keleg-magdy-2023-dlama,
    title = "{DLAMA}: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
    author = "Keleg, Amr  and
      Magdy, Walid",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.389/",
    doi = "10.18653/v1/2023.findings-acl.389",
    pages = "6245--6266",
    abstract = "A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western) countries respectively. Having a more balanced benchmark (DLAMA-v1) supports that mBERT performs better on Western facts than non-Western ones, while monolingual Arabic, English, and Korean models tend to perform better on their culturally proximate facts. Moreover, both monolingual and multilingual models tend to make a prediction that is culturally or geographically relevant to the correct label, even if the prediction is wrong."
}

@inproceedings{khaled2023commonsense,
  title={Commonsense Validation and Explanation in Arabic Text: A Comparative Study Using Arabic BERT Models},
  author={Khaled, M Moneb and Al Sayadi, Aghyad and Elnagar, Ashraf},
  booktitle={2023 24th International Arab Conference on Information Technology (ACIT)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}

@inproceedings{koto-etal-2024-arabicmmlu,
    title = "{A}rabic{MMLU}: Assessing Massive Multitask Language Understanding in {A}rabic",
    author = "Koto, Fajri  and
      Li, Haonan  and
      Shatnawi, Sara  and
      Doughman, Jad  and
      Sadallah, Abdelrahman  and
      Alraeesi, Aisha  and
      Almubarak, Khalid  and
      Alyafeai, Zaid  and
      Sengupta, Neha  and
      Shehata, Shady  and
      Habash, Nizar  and
      Nakov, Preslav  and
      Baldwin, Timothy",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.334/",
    doi = "10.18653/v1/2024.findings-acl.334",
    pages = "5622--5640",
    abstract = "The focus of language model evaluation has transitioned towards reasoning and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating their performance in Arabic remains challenging due to the limited availability of relevant datasets. To bridge this gap, we present ArabicMMLU, the first multi-task language understanding benchmark for the Arabic language, sourced from school exams across diverse educational levels in different countries spanning North Africa, the Levant, and the Gulf regions. Our data comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard Arabic (MSA) and is carefully constructed by collaborating with native speakers in the region. Our comprehensive evaluations of 35 models reveal substantial room for improvement, particularly among the best open-source models. Notably, BLOOMZ, mT0, LLama2, and Falcon struggle to achieve a score of 50{\%}, while even the top-performing Arabic-centric model only achieves a score of 62.3{\%}."
}

@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth international conference on the principles of knowledge representation and reasoning},
  year={2012}
}

@inproceedings{lin-etal-2020-birds,
    title = "{B}irds have four legs?! {N}umer{S}ense: {P}robing {N}umerical {C}ommonsense {K}nowledge of {P}re-{T}rained {L}anguage {M}odels",
    author = "Lin, Bill Yuchen  and
      Lee, Seyeon  and
      Khanna, Rahul  and
      Ren, Xiang",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.557/",
    doi = "10.18653/v1/2020.emnlp-main.557",
    pages = "6862--6868",
    abstract = "Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as {\textquotedblleft}neural knowledge bases{\textquotedblright} via predicting masked words. Surprisingly, we find that this may not work for numerical commonsense knowledge (e.g., a bird usually has two legs). In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. To study this, we introduce a novel probing task with a diagnostic dataset, NumerSense, containing 13.6k masked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06{\%} vs. 96.3{\%} in accuracy)."
}

@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@inproceedings{mousi-etal-2025-aradice,
    title = "{A}ra{D}i{CE}: Benchmarks for Dialectal and Cultural Capabilities in {LLM}s",
    author = "Mousi, Basel  and
      Durrani, Nadir  and
      Ahmad, Fatema  and
      Hasan, Md. Arid  and
      Hasanain, Maram  and
      Kabbani, Tameem  and
      Dalvi, Fahim  and
      Chowdhury, Shammur Absar  and
      Alam, Firoj",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.283/",
    pages = "4186--4218",
    abstract = "Arabic, with its rich diversity of dialects, remains significantly underrepresented in Large Language Models, particularly in dialectal variations. We address this gap by introducing seven synthetic datasets in dialects alongside Modern Standard Arabic (MSA), created using Machine Translation (MT) combined with human post-editing. We present AraDiCE, a benchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on dialect comprehension and generation, focusing specifically on low-resource Arabic dialects. Additionally, we introduce the first-ever fine-grained benchmark designed to evaluate cultural awareness across the Gulf, Egypt, and Levant regions, providing a novel dimension to LLM evaluation. Our findings demonstrate that while Arabic-specific models like Jais and AceGPT outperform multilingual models on dialectal tasks, significant challenges persist in dialect identification, generation, and translation. This work contributes {\ensuremath{\approx}}45K post-edited samples, a cultural benchmark, and highlights the importance of tailored training to improve LLM performance in capturing the nuances of diverse Arabic dialects and cultural contexts. We have released the dialectal translation models and benchmarks developed in this study (https://huggingface.co/datasets/QCRI/AraDiCE)"
},
}

@article{mousi2024aradice,
  title={Aradice: Benchmarks for dialectal and cultural capabilities in llms},
  author={Mousi, Basel and Durrani, Nadir and Ahmad, Fatema and Hasan, Md Arid and Hasanain, Maram and Kabbani, Tameem and Dalvi, Fahim and Chowdhury, Shammur Absar and Alam, Firoj},
  journal={arXiv preprint arXiv:2409.11404},
  year={2024}
}

@inproceedings{naous-etal-2024-beer,
    title = "Having Beer after Prayer? Measuring Cultural Bias in Large Language Models",
    author = "Naous, Tarek  and
      Ryan, Michael J  and
      Ritter, Alan  and
      Xu, Wei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.862/",
    doi = "10.18653/v1/2024.acl-long.862",
    pages = "16366--16393",
    abstract = "As the reach of large language models (LMs) expands globally, their ability to cater to diverse cultural contexts becomes crucial. Despite advancements in multilingual capabilities, models are not designed with appropriate cultural nuances. In this paper, we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture. We introduce CAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities spanning eight types that contrast Arab and Western cultures. CAMeL provides a foundation for measuring cultural biases in LMs through both extrinsic and intrinsic evaluations. Using CAMeL, we examine the cross-cultural performance in Arabic of 16 different LMs on tasks such as story generation, NER, and sentiment analysis, where we find concerning cases of stereotyping and cultural unfairness. We further test their text-infilling performance, revealing the incapability of appropriate adaptation to Arab cultural contexts. Finally, we analyze 6 Arabic pre-training corpora and find that commonly used sources such as Wikipedia may not be best suited to build culturally aware LMs, if used as they are without adjustment. We will make CAMeL publicly available at: https://github.com/tareknaous/camel"
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{roemmele2011choice,
  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={2011 AAAI spring symposium series},
  year={2011}
}

@article{romero2024cvqa,
  title={Cvqa: Culturally-diverse multilingual visual question answering benchmark},
  author={Romero, David and Lyu, Chenyang and Wibowo, Haryo Akbarianto and Lynn, Teresa and Hamed, Injy and Kishore, Aditya Nanda and Mandal, Aishik and Dragonetti, Alina and Abzaliev, Artem and Tonja, Atnafu Lambebo and others},
  journal={arXiv preprint arXiv:2406.05967},
  year={2024}
}

@article{sakaguchi2019adversarial,
  title={An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
  journal={arXiv preprint arXiv:1907.10641},
  year={2019}
}

@article{sap2019socialiqa,
  title={Socialiqa: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09728},
  year={2019}
}

@article{sengupta2023jais,
  title={Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models},
  author={Sengupta, Neha and Sahu, Sunil Kumar and Jia, Bokang and Katipomu, Satheesh and Li, Haonan and Koto, Fajri and Marshall, William and Gosal, Gurpreet and Liu, Cynthia and Chen, Zhiming and others},
  journal={arXiv preprint arXiv:2308.16149},
  year={2023}
}

@inproceedings{tan-etal-2023-towards,
    title = "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models",
    author = "Tan, Qingyu  and
      Ng, Hwee Tou  and
      Bing, Lidong",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.828/",
    doi = "10.18653/v1/2023.acl-long.828",
    pages = "14820--14835",
    abstract = "Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach."
}

@article{wang2020semeval,
  title={SemEval-2020 task 4: Commonsense validation and explanation},
  author={Wang, Cunxiang and Liang, Shuailong and Jin, Yili and Wang, Yilong and Zhu, Xiaodan and Zhang, Yue},
  journal={arXiv preprint arXiv:2007.00236},
  year={2020}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{zhang2021situatedqa,
  title={SituatedQA: Incorporating extra-linguistic contexts into QA},
  author={Zhang, Michael JQ and Choi, Eunsol},
  journal={arXiv preprint arXiv:2109.06157},
  year={2021}
}

