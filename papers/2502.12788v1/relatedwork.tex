\section{Related Work}
\subsection{Commonsense Reasoning in English} 

%Recent advancements in large language models (LLMs) have driven the creation of increasingly diverse and challenging benchmarks designed to assess their capabilities. Among these, commonsense reasoning has become a key focus, as it highlights the extent to which LLMs can demonstrate understanding beyond surface-level text patterns.

Early research on commonsense reasoning primarily focused on linguistic reasoning, as seen in the Winograd Schema Challenge~\citep{levesque2012winograd} and Winogrande~\citep{sakaguchi2019adversarial}, which evaluate pronoun coreference resolution within social and linguistic contexts. Other works have explored physical commonsense reasoning, assessing model's understanding of real-world properties and relationships~\citep{Bisk2019PIQARA}, as well as social reasoning, where models are tested on their ability to interpret human emotions, actions, and social norms \citep{sap2019socialiqa}. Research has also expanded into numerical \cite{lin-etal-2020-birds,akhtar-etal-2023-exploring}, temporal \cite{tan-etal-2023-towards}, and causal reasoning \cite{roemmele2011choice,du-etal-2022-e}, broadening the scope of commonsense evaluation. However, these benchmarks are all developed in English and shaped by Western cultural assumptions, limiting their applicability to the Arab world.

\subsection{Arabic Large Language Models and Their Evaluation on Commonsense Reasoning}
A limited number of Arabic language models have been developed with more than 7B parameters, all of which are decoder-only architectures. These include \texttt{JAIS}~\citep{sengupta2023jais}, \texttt{Fanar}~\citep{fanar}, \texttt{AceGPT}~\citep{huang2023acegpt}, and \texttt{ALLAM}~\citep{bari2024allam}. Their evaluation of commonsense reasoning has been primarily based on machine-translated datasets from English to Arabic (e.g., ~\citet{Tawalbeh2020IsTS, al2021commonsense}). Although this approach provides a useful reference, it does not offer a comprehensive assessment of how well these models capture culturally grounded commonsense knowledge.

Much of the recent Arabic-centric benchmarks have focused on classic NLP tasks, including syntax, semantics, and question answering, as seen in \texttt{LaraBench}~\citep{abdelali2024larabench}, natural language generation in \texttt{DOLPHIN}~\citep{elmadany2023dolphin}, and natural language understanding in \texttt{ORCA}~\citep{elmadany-etal-2023-orca}. Only a few studies have shifted their focus toward knowledge-intensive tasks and reasoning abilities. Among them, \texttt{ArabicMMLU}~\citep{koto-etal-2024-arabicmmlu} compiles exam questions from different education levels across Arabic-speaking countries, offering a broad knowledge assessment but placing less emphasis on cultural reasoning. 

Table~\ref{tab:dataset_comparison} compares \datasetname{} with related Arabic datasets. Most existing datasets are derived from machine translation with post-editing, prioritizing linguistic accuracy over cultural relevance. While some assess reasoning, they often lack cultural grounding, location metadata, and fine-grained topic categorization. ACVA~\citep{huang2023acegpt}, generated using ChatGPT~\citep{ouyang2022training}, is not designed for reasoning evaluation. Similarly, \texttt{AraDice-Culture}~\citep{mousi-etal-2025-aradice} consists of only 180 samples and focuses on open-ended cultural knowledge rather than structured reasoning tasks. These gaps highlight the need for larger, more diverse benchmarks that better capture Arabic cultural contexts and reasoning abilities.


%Arabic language is a rich and complex language
%A few studies have focused on commonsense validation and explanation of Arabic sentences. The first notable work in this area was proposed by \cite{tawalbeh2020sentence}, which introduced commonsense validation in Arabic by translating the SemEval-2020 Task 4 corpus, known as ComVE \cite{wang2020semeval}. This dataset comprises 12,000 pairs of sentences designed for commonsense reasoning tasks. Subsequent research efforts aimed to enhance commonsense validation in Arabic \cite{al2021commonsense}, \cite{alshanik2023commonsense}, \cite{khaled2023commonsense}, by training models capable of distinguishing between natural language statements that make sense and those that do not.

%To bridge this gap, AraDiCE \cite{mousi2024aradice} introduced a benchmark suite designed to evaluate both dialectal and cultural capabilities in LLMs. One of its key components, AraDiCE-Culture, focuses specifically on Arabic cultural understanding by including questions from different regions, such as Egypt, the Levant, and the Gulf. These questions span diverse topics, including public holidays, food, geography, history, public figures, and traditional clothing, providing a comprehensive assessment of culturally grounded knowledge. Other initiatives have also contributed to the creation of culturally diverse datasets. For example, CVQA \cite{romero2024cvqa} is a culturally enriched visual question-answering dataset that incorporates Egyptian cultural elements. 

%Additionally, cultural benchmarks like DLAMA \cite{keleg-magdy-2023-dlama} and CAMeL \cite{naous-etal-2024-beer} have focused on examining biases in LLMs toward various cultures, including Arabic, highlighting areas where these models exhibit cultural bias to the west. 




%Additionally, cultural benchmarks cite{yin2022geomlama} and \cite{acquaye2024susu} test the cultural knowledge of the LLMs.  

%More recently, Arabic-centric large language models (LLMs), such as Jais and Jais Chat \cite{sengupta2023jais}, have demonstrated superior capabilities in commonsense reasoning for Arabic. Experimental results reveal that Jais (13B) and Jais Chat (13B) outperform existing models across six commonsense reasoning datasets, including HellaSwag \cite{zellers2019hellaswag}, PIQA \cite{bisk2020piqa}, SituatedQA \cite{zhang2021situatedqa},  ARC-C \cite{clark2018think}, and OBQA \cite{mihaylov2018can}.

%\subsection{Cultural commonsense reasoning in Arabic}
%With the rapid advancement of large language models (LLMs), several efforts have been made to develop Arabic-centric LLMs, such as Jais and Jais Chat \cite{sengupta2023jais}, Fanar \cite{fanar}, AceGPT \cite{huang2023acegpt}, and ALLAM \cite{bari2024allam}. In parallel, numerous benchmarks have been proposed to evaluate the capabilities of these LLMs across various tasks \cite{mousi2024aradice}, \cite{koto-etal-2024-arabicmmlu}, \cite{abdelali2024larabench}, \cite{elmadany2023dolphin}, and \cite{elmadany2022orca}. However, only a few focused on cultural capabilities of these LLMs.