% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{hyperref}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage{arabtex}
\usepackage{utf8} % Needed by arabtex package
\usepackage[utf8]{inputenc} % Needed by LAE, T1 for inline arabic
%\usepackage[arabic,english]{babel}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
%\usepackage[utf8]{inputenc}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{times}
\usepackage{tabu}
\usepackage{latexsym}
\usepackage{stmaryrd}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{tablefootnote}
\usepackage{enumitem}  % More control
\usepackage{lipsum}
\usepackage{xspace}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{arydshln}
\usepackage{array}
\usepackage{cleveref}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}
\newcommand{\datasetname}{\texttt{ArabCulture}\xspace}
\newcommand{\vquad}{\hspace{0.3em}} 
\newcommand{\ex}[1]{\textit{#1}\xspace} 
\newcommand{\gl}[1]{``#1''\xspace} 
\newcommand{\equalsign}{\footnotemark[1]\hspace{0.1cm}}

\newcommand{\dummy}[1]{\textcolor{red}{#1}}
\newcommand{\xmark}{\ding{55}}
\newcommand{\cmark}{\ding{51}}
\definecolor{mygreen}{RGB}{217, 234, 211}
\definecolor{myred}{RGB}{244, 204, 204}

\newcommand{\ok}{\cellcolor{mygreen}}
\newcommand{\no}{\cellcolor{myred}}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Commonsense Reasoning in Arab Culture}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Abdelrahman Sadallah$^{1}$ \quad Junior Cedric Tonga$^{1}$ \quad Khalid Almubarak$^{2}$ \\  \textbf{Saeed Almheiri}$^{1}$ \quad  \textbf{Farah Atif}$^{1}$ \,   \textbf{Cahtrine Qwaider}$^{1}$ \quad \textbf{Karima Kadaoui}$^{1}$ \\   \textbf{Sara Shatnawi}$^{3}$  \quad  \textbf{Yaser Alesh}$^{4}$  \quad \textbf{Fajri Koto}$^{1}$ \\ 
$^{1}$Department of Natural Language Processing, MBZUAI \\
        $^{2}$SDAIA \, $^{3}$Al-Balqa Applied University \,  $^{4}$Khalifa University \\ 
	\texttt{\small \{abdelrahman.sadallah,fajri.koto\}@mbzuai.ac.ae 
	} 
}

\begin{document}
\setcode{utf8}

\maketitle

\begin{abstract}
Despite progress in Arabic large language models, such as Jais and AceGPT, their evaluation on commonsense reasoning has largely relied on machine-translated datasets, which lack cultural depth and may introduce Anglocentric biases. Commonsense reasoning is shaped by geographical and cultural contexts, and existing English datasets fail to capture the diversity of the Arab world. To address this, we introduce \datasetname, a commonsense reasoning dataset in Modern Standard Arabic (MSA), covering cultures of 13 countries across the Gulf, Levant, North Africa, and the Nile Valley. The dataset was built from scratch by engaging native speakers to write and validate culturally relevant questions for their respective countries. \datasetname spans 12 daily life domains with 54 fine-grained subtopics, reflecting various aspects of social norms, traditions, and everyday experiences. Zero-shot evaluations show that open-weight language models with up to 32B parameters struggle to comprehend diverse Arab cultures, with performance varying across regions. These findings highlight the need for more culturally aware models and datasets tailored to the Arabic-speaking world.\footnote{\datasetname{} can be accessed at \url{https://huggingface.co/datasets/MBZUAI/ArabCulture}}
\end{list}
\end{abstract}

%This paper introduces \datasetname, a new benchmark for evaluating the cultural commonsense knowledge embedded in large language models (LLMs) tailored specifically to the Arab world. The dataset consists of 3,482 culture-based questions spanning 13 countries from four distinct Arabic regions: the Gulf, Levant, North Africa, and Nile Valley. These questions, written in Modern Standard Arabic (MSA), cover 54 sub-topics related to 12 main cultural themes. The study benchmarks a range of LLMs, including Arabic-specific and multilingual models, and presents an extensive zero-shot evaluation across multiple settings. The goal is to assess how well LLMs can utilize cultural and regional knowledge in cultural commonsense reasoning. Results reveal key differences in model performance across countries, regions, and topics, and provide insights into the challenges faced by current LLMs in capturing Arabic cultural knowledge. This work establishes an initial baseline for Arabic cultural commonsense reasoning and provides a foundation for future research in this area.~\footnote{The code and the dataset will be published upon the acceptance of this work.}
%\end{abstract}


\section{Introduction}
\label{sec:intro}
Commonsense reasoning is the ability to make judgments and inferences based on everyday human knowledge and experiences \cite{sap-etal-2020-commonsense}. It is a fundamental aspect of human cognition and has been extensively studied in the context of large language models (LLMs) \citep{openai2024gpt4technicalreport, grattafiori2024llama3herdmodels, Liu2023LLM360TF}. However, commonsense reasoning is not universal—it is shaped by culture, which encompasses the shared knowledge, values, customs, and behaviors that define a society \citep{macionis2012sociology,giddens2014essential}.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/regions_examples_v4.pdf} 
    \caption{\datasetname covers four regions across the Middle East and North Africa, spanning 13 countries. The highlighted areas on the map represent the regions included in \datasetname. We present example questions from Saudi Arabia, Jordan, Egypt, and Morocco, each with three answer choices, with the correct answer marked in bold. English translations are provided for illustration.}
    \label{fig:region_examples}
\end{figure}

The Arab world, home to approximately 456 million people \citep{diab-etal-2017-nlp,shoufan-alameri-2015-natural}, is characterized by its linguistic unity through Modern Standard Arabic (MSA) while encompassing diverse traditions, religions, and customs \citep{arabworld}. This cultural diversity influences not only social interactions but also reasoning patterns, making it crucial to develop evaluation benchmarks that reflect these variations. However, most existing commonsense reasoning datasets are developed with Western-centric assumptions, limiting their applicability to Arabic-speaking societies.

Despite recent advancements in Arabic LLMs \citep{sengupta2023jais, huang2023acegpt,silma_01_2024,bari2024allam}, their evaluation has largely relied on machine-translated datasets originally created in English. Many commonsense reasoning benchmarks \citep{Bisk2019PIQARA, socialiqa} fail to capture Arab cultural perspectives, as simple translations do not account for region-specific knowledge, potentially introducing bias. Given the significant cultural variations across the Arab world, these datasets do not provide a holistic measure of Arabic LLMs' ability to reason within culturally specific contexts. This raises an important question: \textit{To what extent can existing LLMs accurately reason about commonsense knowledge in diverse cultural settings, particularly in the Arab world?}

To address this gap, we introduce \datasetname, a commonsense reasoning dataset specifically designed to assess the cultural knowledge of Arabic LLMs. The dataset consists of 3,482 questions written in Modern Standard Arabic (MSA), covering 13 countries across the Gulf, Levant, North Africa, and the Nile Valley (see Figure~\ref{fig:region_examples}). It spans 12 major domains and 54 fine-grained subtopics, reflecting various aspects of social norms, traditions, and daily life in the Arab world. Unlike existing benchmarks, which often rely on translated datasets, \datasetname was built from scratch by directly engaging native speakers to write culturally relevant questions for their respective countries. We carefully implemented quality control measures throughout the dataset creation process, including rigorous validation steps to maintain accuracy, relevance, and cultural sensitivity.

We evaluate a range of closed-weight and open-weight Arabic and multilingual LLMs in a zero-shot setting to assess their cultural commonsense reasoning capabilities. Inspired by~\citet{koto-etal-2024-indoculture}, we frame the task in two ways: multiple-choice questions (MCQ) and completion tasks. Additionally, we introduce three levels of location-based contextual grounding: (1) no additional location information, (2) specifying the broader region (e.g., Gulf or Levant), and (3) specifying the exact country along with its regional classification. This setup allows us to analyze how effectively LLMs incorporate geographical and cultural cues in their reasoning.

Our results show that even LLMs with up to 32B parameters struggle with cultural commonsense reasoning, with performance varying significantly across regions. We conduct a detailed analysis of the best-performing models, identifying strengths and weaknesses across different cultural contexts. We also conducted a small manual experiment to test the models' ability to explain their chosen answers. Additionally, we explore whether enriching prompts with cultural facts improves performance in smaller language models, finding that while it helps in some cases, it does not provide a universal solution.


%The Arab world is the home of around 456 million people~\citet{diab-etal-2017-nlp,shoufan-alameri-2015-natural} who share the same official language -Modern Standard Arabic (MSA)- and traditions but have a diverse set of traditions, religions, and customs~\citet{arabworld}. With the recent advances in Natural Language Processing (NLP), research shifted to focus more on building multi-modal or language-specific Large Language Models(LLMs). This was accompanied by the creation of many benchmarks that focus on evaluating these LLMs on different domains and tasks. 

%However, most of these benchmarks have been focused on English~\citet{openai2024gpt4technicalreport,sengupta2023jais,llm360}, and conducted on English datasets such as PIQA~\citet{bisk2020piqa}, and SocialIQA~\cite{socialiqa}. This poses a significant problem as it enforces the models' bias towards the Western culture that is grounded in English~\citet{ponti-etal-2020-xcopa}. This is backed by the fact that culture encompasses the way of life and shapes our thoughts and actions~\citet{macionis2012sociology,giddens2014essential}. As cultures significantly vary from one location to another, and just translating the existing benchmarks to a certain language does not help with the lack of resources for a certain culture~\citet{ramesh-etal-2023-fairness,talat-etal-2022-reap}. 


%There has been some work on creating benchmarks for the Arab world, but very limited work has focused on the cultural aspect of the Arab world. ~\citep{koto-etal-2024-arabicmmlu} introduced a benchmark to evaluate models on Arabic general knowledge and commonsense. ~\citep{khaled2023commonsense} introduced a dataset on Arabic commonsense validation. 

%In this work, we introduce \datasetname, a new benchmark that is tailored to the Arab world and focuses on measuring the cultural nuances of the different models.  Our dataset has 3,482 culture questions that span 13 different Arabic cultures from 4 different Arabic regions--Gulf, Levant, North Africa, and Nile Valley. Figure~\ref{fig:region_examples} shows the different Arab countries that constitute our dataset, with each geographic region colored differently. 

%We chose to write these questions in MSA as we focus on evaluating the cultural knowledge embedded in the different systems while eliminating the biases for the different dialects. As mentioned, our data comes from 13 different countries accounting for 82\% of the Arab world population. In choosing the countries, we chose countries with higher populations and countries that represent the different Arab regions. The dataset spans 54 different sub-topics that address 12 main topics. We carefully designed the topic subjects and decided how many examples to include in each topic to reflect the Arabic culture from different angles. 

%We benchmark a variety of different LLMs that are commonly used and of different sizes. We also benchmark both Arabic-based models and multi-lingual models. Following~\citet{indoculture}, we formulated our task in two different settings: as an MCQ task and a Completion task. As most of the models we used were open-source LLMs, we extracted the models' outputs using the logit log-likelihood instead of relying on output post-processing and matching. 

%We begin by benchmarking different LLMs in a zero-shot setting. We use three different levels of groundings for each sample. The first form asks the model to answer the question directly without any extra information. We also conduct experiments where we explicitly tell the model from which region this question came. Lastly, we also give the model the specific country for the question. We chose this setup to measure the extent to which different models can utilize the context information to answer the question. As one question can be relevant to two different countries, an answer depends on knowing which country this question comes from. 

%We do an extensive analysis of the best three models to study the variance of their cultural knowledge across different countries and regions. We also studied which topics proved to be more challenging to the models. 

%Finally, we make one attempt to improve the models' performance on the task by trying to enrich the prompt by appending some cultural facts and observing that this simple task only helps one model.

% In this work, we don't focus on improving the cultural knowledge and capabilities of the different LLMs; instead, we introduce a new benchmark to evaluate the knowledge of Arabic cultural commonsense and set up an initial baseline for this task. 

%Our contribution can be summarized as: 
%\begin{itemize}
%    \item Introduce the first Arabic benchmark to evaluate the cultural commonsense knowledge.
%    \item We produce an initial benchmark on different LLMs in a zero-shot setting. 
%    \item We do extensive analysis to study the models' performance across different countries and for different topics.
%    \item We release the dataset and the code for replicating our experiments. 
%\end{itemize}

    
\input{latex/tables/dataset_comparison}

\section{Related Work}
\subsection{Commonsense Reasoning in English} 

%Recent advancements in large language models (LLMs) have driven the creation of increasingly diverse and challenging benchmarks designed to assess their capabilities. Among these, commonsense reasoning has become a key focus, as it highlights the extent to which LLMs can demonstrate understanding beyond surface-level text patterns.

Early research on commonsense reasoning primarily focused on linguistic reasoning, as seen in the Winograd Schema Challenge~\citep{levesque2012winograd} and Winogrande~\citep{sakaguchi2019adversarial}, which evaluate pronoun coreference resolution within social and linguistic contexts. Other works have explored physical commonsense reasoning, assessing model's understanding of real-world properties and relationships~\citep{Bisk2019PIQARA}, as well as social reasoning, where models are tested on their ability to interpret human emotions, actions, and social norms \citep{sap2019socialiqa}. Research has also expanded into numerical \cite{lin-etal-2020-birds,akhtar-etal-2023-exploring}, temporal \cite{tan-etal-2023-towards}, and causal reasoning \cite{roemmele2011choice,du-etal-2022-e}, broadening the scope of commonsense evaluation. However, these benchmarks are all developed in English and shaped by Western cultural assumptions, limiting their applicability to the Arab world.

\subsection{Arabic Large Language Models and Their Evaluation on Commonsense Reasoning}
A limited number of Arabic language models have been developed with more than 7B parameters, all of which are decoder-only architectures. These include \texttt{JAIS}~\citep{sengupta2023jais}, \texttt{Fanar}~\citep{fanar}, \texttt{AceGPT}~\citep{huang2023acegpt}, and \texttt{ALLAM}~\citep{bari2024allam}. Their evaluation of commonsense reasoning has been primarily based on machine-translated datasets from English to Arabic (e.g., ~\citet{Tawalbeh2020IsTS, al2021commonsense}). Although this approach provides a useful reference, it does not offer a comprehensive assessment of how well these models capture culturally grounded commonsense knowledge.

Much of the recent Arabic-centric benchmarks have focused on classic NLP tasks, including syntax, semantics, and question answering, as seen in \texttt{LaraBench}~\citep{abdelali2024larabench}, natural language generation in \texttt{DOLPHIN}~\citep{elmadany2023dolphin}, and natural language understanding in \texttt{ORCA}~\citep{elmadany-etal-2023-orca}. Only a few studies have shifted their focus toward knowledge-intensive tasks and reasoning abilities. Among them, \texttt{ArabicMMLU}~\citep{koto-etal-2024-arabicmmlu} compiles exam questions from different education levels across Arabic-speaking countries, offering a broad knowledge assessment but placing less emphasis on cultural reasoning. 

Table~\ref{tab:dataset_comparison} compares \datasetname{} with related Arabic datasets. Most existing datasets are derived from machine translation with post-editing, prioritizing linguistic accuracy over cultural relevance. While some assess reasoning, they often lack cultural grounding, location metadata, and fine-grained topic categorization. ACVA~\citep{huang2023acegpt}, generated using ChatGPT~\citep{ouyang2022training}, is not designed for reasoning evaluation. Similarly, \texttt{AraDice-Culture}~\citep{mousi-etal-2025-aradice} consists of only 180 samples and focuses on open-ended cultural knowledge rather than structured reasoning tasks. These gaps highlight the need for larger, more diverse benchmarks that better capture Arabic cultural contexts and reasoning abilities.


%Arabic language is a rich and complex language
%A few studies have focused on commonsense validation and explanation of Arabic sentences. The first notable work in this area was proposed by \cite{tawalbeh2020sentence}, which introduced commonsense validation in Arabic by translating the SemEval-2020 Task 4 corpus, known as ComVE \cite{wang2020semeval}. This dataset comprises 12,000 pairs of sentences designed for commonsense reasoning tasks. Subsequent research efforts aimed to enhance commonsense validation in Arabic \cite{al2021commonsense}, \cite{alshanik2023commonsense}, \cite{khaled2023commonsense}, by training models capable of distinguishing between natural language statements that make sense and those that do not.

%To bridge this gap, AraDiCE \cite{mousi2024aradice} introduced a benchmark suite designed to evaluate both dialectal and cultural capabilities in LLMs. One of its key components, AraDiCE-Culture, focuses specifically on Arabic cultural understanding by including questions from different regions, such as Egypt, the Levant, and the Gulf. These questions span diverse topics, including public holidays, food, geography, history, public figures, and traditional clothing, providing a comprehensive assessment of culturally grounded knowledge. Other initiatives have also contributed to the creation of culturally diverse datasets. For example, CVQA \cite{romero2024cvqa} is a culturally enriched visual question-answering dataset that incorporates Egyptian cultural elements. 

%Additionally, cultural benchmarks like DLAMA \cite{keleg-magdy-2023-dlama} and CAMeL \cite{naous-etal-2024-beer} have focused on examining biases in LLMs toward various cultures, including Arabic, highlighting areas where these models exhibit cultural bias to the west. 




%Additionally, cultural benchmarks cite{yin2022geomlama} and \cite{acquaye2024susu} test the cultural knowledge of the LLMs.  

%More recently, Arabic-centric large language models (LLMs), such as Jais and Jais Chat \cite{sengupta2023jais}, have demonstrated superior capabilities in commonsense reasoning for Arabic. Experimental results reveal that Jais (13B) and Jais Chat (13B) outperform existing models across six commonsense reasoning datasets, including HellaSwag \cite{zellers2019hellaswag}, PIQA \cite{bisk2020piqa}, SituatedQA \cite{zhang2021situatedqa},  ARC-C \cite{clark2018think}, and OBQA \cite{mihaylov2018can}.

%\subsection{Cultural commonsense reasoning in Arabic}
%With the rapid advancement of large language models (LLMs), several efforts have been made to develop Arabic-centric LLMs, such as Jais and Jais Chat \cite{sengupta2023jais}, Fanar \cite{fanar}, AceGPT \cite{huang2023acegpt}, and ALLAM \cite{bari2024allam}. In parallel, numerous benchmarks have been proposed to evaluate the capabilities of these LLMs across various tasks \cite{mousi2024aradice}, \cite{koto-etal-2024-arabicmmlu}, \cite{abdelali2024larabench}, \cite{elmadany2023dolphin}, and \cite{elmadany2022orca}. However, only a few focused on cultural capabilities of these LLMs.


\section{ArabCulture Dataset}

\datasetname{} is a sentence completion task in MSA, comprising 3,482 unique instances. Each question consists of a one-sentence premise and three answer choices that are both logically and syntactically valid. As illustrated in Figure~\ref{fig:region_examples}, instances are drawn from various Arab regions.

Solving these questions requires cultural knowledge specific to the country referenced, as the correct answer aligns with culturally relevant context. This makes \datasetname{} a valuable benchmark for assessing an LLM’s ability to incorporate cultural understanding and knowledge in Arabic-language tasks.

\subsection{Dataset Construction}
\datasetname{} is built from scratch without relying on web-scraped text, minimizing the risk of training data leakage when evaluating LLMs. It is manually created and validated by native speakers from 13 Arabic-speaking countries. To further ensure quality, the authors conduct rigorous manual checks for lexical accuracy, semantic coherence, and contextual relevance.\footnote{The authors of this paper represent most of the studied countries, contributing diverse regional perspectives to the dataset.} 


\paragraph{Worker requirements}
We hired 26 expert workers from 13 Arab countries, with two workers per country, that fit defined eligibility criteria: (1) The worker must be a native Arabic speaker; (2) They must have lived in the country for at least 10 years; (3) They must possess a strong understanding of local culture and traditions; (4) Their parents must also be from the country and reside there; (5) They must have at least a high school diploma, while higher degrees were considered an advantage. Among the 26 workers, 14 hold a Bachelor's degree, including seven with a Master's degree, two with a PhD, and three with a high school diploma.

All workers were required to attend a one-hour online workshop or watch a recorded session. The workshop introduced the project concept, explained task guidelines, and addressed any potential questions. To ensure a clear understanding of the task, we conducted a pilot study before the main annotation phase.

Each worker was assigned two tasks: (1) Writing instances and (2) Reviewing and verifying the work of their peer from the same country. The payment was determined based on the minimum monthly salary for data entry jobs in each worker's country, and each worker was compensated for the equivalent of four full-time working days.

% The first task spanned 14 days, while the second task was completed within 3 days. 


\paragraph{Country Selection}
%chatrine: what we should write here?
We selected countries that ensured broad geographic coverage of the Arab world while remaining within budget constraints. Priority was given to countries with larger populations and land areas, resulting in a selection of 13 countries across four regions, representing approximately 82\% of the total Arab world population. These include: (1) The Gulf: Saudi Arabia, Yemen, UAE; (2) The Levant: Syria, Jordan, Palestine, Lebanon; (3) North Africa: Morocco, Algeria, Tunisia, Libya; and (4) The Nile Valley: Egypt and Sudan.

\paragraph{Topic taxonomy}
We define 12 daily life topics with 54 fine-grained subtopics to build \datasetname{}. The topic selection is based on \citet{koto-etal-2024-indoculture} and adapted to reflect Arab regional culture. Native Arabic speakers from nine countries contributed to determining these topics, ensuring cultural relevance, diversity, and regional representation (e.g., \textit{Ramadan} traditions). Additionally, we carefully balanced the dataset by assigning an appropriate number of examples to each topic. Table \ref{tab:topics} in the Appendix provides an overview of the topics and their subtopics, which include food, weddings, holiday activities, daily activities, habits, traditional games, death, art, parenting, agriculture, family relationships, and idioms.


%When selecting topics for the dataset, the goal was to choose topics that span most of the cultural aspects of human lives. However, as we target the Arabic language, we carefully chose subtopics that are relevant to the Arab world for each topic. Moreover, we choose the weight of each topic by determining how many examples should represent this topic. For example, Food has the highest number of examples, with culturally-relative subtopics like "Iftar" and "Sahoor." In Table \ref{tab:topics}, we show the topics and their corresponding subtopics.
%(Chatrine: based on what you determine the examples?, what makes topic interested or culture-heavy??)

\paragraph{Instance Writing}
In the first stage, each worker was tasked with writing short two-sentence stories. For each entry, they were provided with a pre-defined topic and instructed to write a one-sentence premise followed by three candidate completions for the second sentence. The completions had to adhere to the following rules: (1) all had to be valid syntactic continuations of the premise, (2) none could introduce logical contradictions (e.g., ensuring consistency in topic and narrative), and (3) only one of the three sentences should be culturally accurate for the specified country. This design ensures that model predictions are influenced by cultural knowledge rather than grammatical or logical inconsistencies. Each worker was required to write 150 instances with two workers assigned per country.

%4 statements in total for each data entry. They start by writing an initial statement/ premise that matches the current sub-topic. Then, they have to write 3 second-statements that act as the 3 choices. The second statements have to comply with certain rules: (1) they all have to be a valid syntactic continuation of the first statement, (2) they can not have any logical contradictions to the first statement, and (3) only one second-statement should be culturally correct for the sample country. All these rules were designed to ensure that the model prediction will only be influenced by the cultural knowledge embedded in it and that the model can not eliminate some options because they are written in an incorrect grammatical way or because they include logical contradictions.  As a first task, each worker has to add 150 data entries following the aforementioned guidelines, for a total of 300 instances covering the country. 

\paragraph{Two-stage of Quality Control}
\label{quality-control}

In stage 1, each of the 13 countries had a designated representative involved in dataset development, all of whom are also authors of this project. After workers completed their assigned instances, the respective country representatives manually reviewed their submissions to ensure adherence to the guidelines. Linguistic errors were corrected through manual editing, but if an instance did not meet the guidelines, workers were required to revise and resubmit it.

To further ensure quality, stage 2 involved a peer validation process, where each worker reviewed their colleague’s work. The data was reformatted into multiple-choice questions, with the second sentence of each instance shuffled among three options. Workers were then asked to select the correct culturally appropriate completion and were allowed to consult external sources if unsure. If the worker selected the correct answer, it indicated agreement between annotators on the cultural validity of the instance. However, if the worker selected the wrong answer, the example was discarded, as it suggested ambiguity or cultural disagreement in the instance.

%We employed multiple stages of quality control. It started by asking workers to provide a couple of examples after they finished the training to make sure that they understood the task. Secondly, all country PIs were instructed to check the worker's progress regularly to make sure they were creating correct examples. 

%For each country, after the workers finished creating all the samples, we instructed them to cross-check. In this check, each worker takes the samples created by their peer and marks each sample as if it's relevant to their country or not. PIs take the last stage of quality control. For each country, the corresponding PI goes over each sample and either fixes minor issues and typos in the sample or marks it as an invalid sample. Before using the dataset, we remove all the samples marked as invalid from the dataset. 

%chatrine: can we have statistics in this place, dataset before and after quality check, per topic or per country. 

\paragraph{Country-Specific Annotation}
\label{country-specific}
Beyond quality control, we also tasked the quality control workers with annotating whether the cultural context described in an instance could be relevant to other countries. The goal was to distinguish instances that are truly unique to the designated country from those that are shared across multiple countries. To ensure accuracy, the authors of this paper conducted a second round of annotation. If an instance was marked as culturally relevant to more than one country by at least one annotator, we flagged it as Not Country-Specific ($\neg$CS); otherwise, it was labeled as Country-Specific (CS). This categorization was used in our analysis experiments to better understand the distribution of culturally unique and widely shared knowledge.


%As mentioned in~\ref{quality-control}, we ask workers to cross-check their pees' work for relevance to their country or not. After that, each PI goes through each sample and marks if they think this sample is also relevant to other countries. We use those two flags to group samples into two main groups: \textbf{Country Specific} (CS), and \textbf{Not Country Specific} ($\neg CS$). CS examples are the samples that have been marked as only relative to their corresponding country and as not relevant to other countries. $\neg CS$ are the rest of the examples. This grouping is used in our analysis experiments to gain insight into each type. 

\subsection{Data Statistics}
During the instance writing phase, we initially aimed to collect 3,900 samples (26 workers $\times$ 150 samples each). However, the first quality-check round (\S \ref{quality-control}) resulted in 3,606 samples. Following the second quality control, we discarded 124 samples, leaving a final dataset of 3,482 instances. 

Table~\ref{tab:region-data} shows the distribution of \datasetname{} across regions and their respective countries. The overall proportion of country-specific instances is 46\%, indicating notable cultural similarities among Arab countries. In terms of word and character count, the dataset shows consistent length across countries, with an average of 32.5 words and 181 characters per instance. However, Libyan examples tend to be longer, averaging 226 characters per instance. Furthermore, Figure~\ref{fig:samples_per_topic} illustrates the total number of samples for each topic. Overall, the dataset covers a wide range of topics, with food, daily activities, and holiday activities being the most frequent, while parenting, family relationships, and agriculture are the least frequent.

%The original task was to collect 3,900 samples in total (26 workers x 150 samples). However, not all workers completed the full 150 samples. The collected dataset resulted in a total of 3,606 samples. 
%The cross-check quality control check mentioned in~\ref{quality-control} further reduced the total samples to 3,482 samples.  Table~\ref{tab:region-data} shows the distribution of samples for each region and its corresponding countries. The overall percentage of the Country-Specific examples is 46\%, which hints at the similarities between the cultures in the Arab countries. 

\begin{center}
\input{latex/tables/raw_data_stats}
\end{center}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/samples_per_topic.pdf}
    \caption{Total number of samples for each topic.}
    \label{fig:samples_per_topic}
    \vspace{-0.5cm}
\end{figure}


\input{latex/tables/model_scores_table_english}

\section{Experiment}

\subsection{Experimental Setup}

We conducted zero-shot experiments across 31 models, categorized into the following groups: (1)  20 multilingual models of various sizes, such as BLOOMZ \cite{muennighoff-etal-2023-crosslingual}, mT0-xxl \cite{muennighoff-etal-2023-crosslingual}, Llama–3 \cite{grattafiori2024llama3herdmodels}, Aya-Expanse \cite{dang2024ayaexpansecombiningresearch}, Gemma-2 \cite{Riviere2024Gemma2I}, and Qwen2.5 \cite{qwen2025qwen25technicalreport}; (2) 10 Arabic-centric models of different sizes, including Jais \cite{sengupta2023jais}, SILMA \cite{silma_01_2024}, AceGPT-v2 \cite{liang2024alignment}, and ALLaM \cite{bari2024allamlargelanguagemodels};
(3) 1 closed-weight model, GPT-4o \cite{openai2024gpt4ocard}. All experiments are done using zero temperature (greedy sampling) to enforce the models to produce factual outputs. 

We conducted experiments using both Arabic and English prompts ( Figure~\ref{fig:prompts}) and evaluated language models using two strategies: (1) sentence completion and (2) MCQ. In the sentence completion approach, we concatenate the premise with each candidate's second sentence and select the one with the highest likelihood. For MCQ, we assign alphabetical labels to the answer choices (A, B, C for English, and \<أ>\,,  \<ب>,  \<ج> for Arabic), and the selected answer corresponds to the option with the highest probability. We constructed these experiments using the LM-Evaluation-Harness Framework~\cite{eval-harness}.
Note that for the closed-weight model, we only perform MCQ-style evaluation, instructing the model to generate the answer as a JSON object containing only the answer character.

As discussed in Section~\ref{sec:intro}, cultural knowledge varies across locations, and we hypothesize that providing geographical context can enhance a model’s reasoning ability in cultural contexts. To test this, we complement our experiments with three different levels of location context \(\ell \in \{\text{none}, \text{region}, \text{region $+$ country}\} \).

%Given a premise, three candidate options (A, B, C for English, and \<أ>\,  \<ب>,  \<ج> for Arabic), and a location \(\ell\), the correct multiple-choice answer is determined by selecting the option with the highest likelihood. For sentence completion, the correct option was determined by just giving the model the premise, location \(\ell\), and then measuring the likelihood for the three options and choosing the one with the highest probability. \newline For GPT-4o, only the multiple-choice task was conducted, as obtaining overall probability scores is not possible. Using the OpenAI API, we performed a single attempt per sample. We instructed the model to generate the answer as a JSON object containing only the answer character.

%These tasks assess model performance in predicting correct options using English and Arabic prompt templates (). 

\subsection{Zero-shot Experiments} 

Our observations show that evaluation with English prompts outperforms Arabic prompts, consistent with findings from \citet{koto-etal-2024-arabicmmlu, kmainasi2024nativevsnonnativelanguage}. We speculate that this is due to the dominance of English instruction-tuning datasets in LLM development. Therefore, we present the results using English prompts in the main text and include the Arabic results in Appendix~\ref{appendix:arabic_zero_shot}.

\paragraph{Overall Observation}

The overall results presented in Table~\ref{tab:model_scores} reveal notable performance differences between open-weight and closed-weight models in understanding Arabic culture and norms. While some large-scale open-weight models achieve relatively high accuracy—such as Qwen-2.5-Instruct (72B) with 80\%, LLaMA-3.3-Instruct (70B) with 75.4\%, and AceGPT-v2-Chat (32B) with 79.7\%—, closed-wight models represented by GPT-4o demonstrate significantly stronger performance.
Arabic-centric models do not consistently outperform multilingual models. Some, such as Jais, struggle despite being tailored for Arabic, whereas certain multilingual models like Qwen2.5 and LLama-3.3 Instruct surpass them in accuracy.
Within the same model family, performance generally improves with larger model sizes—except for Jais and AceGPT. However, across different model families, scaling does not guarantee better results. This indicates that factors beyond model size, such as pretraining data, architecture, and training recipes, significantly impact cultural comprehension.
When comparing base models to instruction-tuned variants, we observe modest improvements in completion tasks but substantial gains in MCQ tasks. 
Overall, these findings highlight the need for improvements in Arabic and multilingual models to enhance their comprehension of Arabic cultural contexts. Addressing this gap can also help mitigate cultural biases, which have been identified in recent studies, such as the work by \cite{naous-etal-2024-beer}.


%For example, Qwen-2.5 (72B) improves by 4.6 points in completions but shows a striking 24-point increase in MCQ performance. This highlights the effectiveness of instruction tuning, particularly for structured question-answering tasks.


\paragraph{Multiple‐Choice (MCQ) Outperforms Completion}

In Table~\ref{tab:model_scores}, we observe that sentence completion is not as reliable as MCQ, despite being a more natural approach that aligns with the sentence completion framework of \datasetname{}. Qwen‑2.5 Instruct (32B), for example, achieves 75.2\% accuracy in MCQ but drops significantly to 37.6\% in sentence completion. Similar disparities are also evident in smaller models; for instance, BLOOMZ (7B) achieves 58.5\% in MCQ but performs at random (31.7\%) in sentence completion. Some models, such as Aya‐Expanse (8B), show only a small gap between MCQ and sentence completion, likely due to pretraining or fine-tuning gaps that hinder their ability to leverage structured prompts effectively. Interestingly, the older version of Llama-3 Instruct (70B) does not benefit from the MCQ strategy, whereas the latest version (Llama-3.3 Instruct) shows a dramatic improvement, increasing from 41\% to 71\%. Given that MCQ yields the best results, we use it for further analysis.


%Even smaller models with fewer parameters (e.g., BLOOMZ (7B)) show gains from completion to MCQ, highlighting how predefined response options reduce the ambiguity inherent in free‐form text generation. However, the degree of improvement varies. For instance, some models such as Aya‐Expanse (8B), show minimal benefits from MCQ format, likely due to pretraining or fine-tuning gaps that hamper their ability to leverage structured prompts effectively. Another model is Llama 70B Instruct. While Llama 70B 3 did not show an improvement with MCQ, Llama 70B 3.3 showed a substantial increase of over 30\% in performance.


\paragraph{Impact of Location Granularity} Adding finer-grained location information in the prompt does not produce a consistently positive or negative effect on zero-shot performance, yielding mixed results. For instance, when region and country context are included, Jais‐v3 Chat (30B) experiences a 6-point accuracy drop compared to the vanilla prompt ($\ell = \text{None}$). In contrast, Qwen‑2.5 (14B) shows a substantial improvement, increasing from 55.2\% ($\ell = \text{None}$) to 61.6\% when provided with country-level context. These fluctuations suggest that in some models, location specificity does not necessarily enhance cultural understanding.


%The incorporation of more fine‐grained location cues, ranging from $\ell = \text{None}$ tob$\ell = \text{Region}$ or $\ell = \text{Region + Country}$, does not yield a uniformly positive or negative effect on zero‐shot performance, even for the larger models. For instance, Jais‐v3 Chat (30B) shows a drop in MCQ accuracy from 60.1\% at $\ell = \text{None}$ to 54\% at $\ell = \text{Region + Country}$. In contrast, Qwen‑2.5 (14B) improves substantially in MCQ, from 55.2\% at $\ell = \text{None}$ to 61.6\% when given more detailed country‐level context. Such fluctuations highlight the nuanced interplay between model architecture, pre-training strategies, and the specific cultural knowledge being probed. In some cases, increased location specificity may help disambiguate cultural references, while in others, it might introduce additional complexity or noise that models are not adequately equipped to process.
\input{latex/tables/english_results_by_country}
\input{latex/tables/english_results_by_topic}


\section{Analysis}

\subsection{Result by Categories}
In this section, we expand on our findings based on the top three models from Table~\ref{tab:model_scores}: (1) \texttt{GPT-4o}, (2) \texttt{Qwen-2.5-72B-Instruct}, and (3) \texttt{AceGPT-v2-32B-Chat}. We provide a detailed analysis across country and topic, with each sample categorized as either country-specific (CS) or non-country-specific ($\neg$CS). 

\paragraph{Country} 

In Table~\ref{tab:english_results_by_country}, we observe significant variation in LLM performance across countries, emphasizing the need for country-specific adaptation when deploying models. Questions from Jordan are consistently predicted with high accuracy, exceeding 90\% across all models. However, performance drops significantly for Lebanon and Tunisia, where models struggle to provide correct answers. Even the Arabic-centric AceGPT-v2 achieves only 63.6\% accuracy for Lebanon and 62.7\% for Tunisia. Across the four regions, we find that the Levant is the most challenging, underscoring the difficulty of achieving reliable performance across different cultural and linguistic contexts.

More interestingly, country-specific questions prove to be more challenging than non-country-specific ones across nearly all countries and models. For instance, in the Nile Valley region, GPT-4o experiences an accuracy drop of nearly 10 points when handling country-specific questions, while in the Levant region, Qwen-2.5 sees a 17-point decline. This suggests that when cultural knowledge is not shared across multiple countries or regions, it becomes more distinct and difficult for LLMs to capture accurately.

%shows the results for the Arabic prompt. We can observe a trend of CS examples being more hard for the model to answer correctly, with the Levant as the most challenging region for the models to answer. For the country-based results, we can see that Tunisia and Lebanon are the most challenging countries for the models.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/generation_exp.pdf}
    \caption{Performance comparison between {\tt jais-30b-chat-v3}, {\tt AceGPT-v2-32B-Chat}, and {\tt SILMA-9B-Instruct-v1.0} based on text generation output. ``Answer (T)'' indicates that the generated
answer is true, while ``Exp (F)'' denotes that the answer explanation is false.}
    %\vspace{-0.5cm}
    \label{fig:generation_exp}
\end{figure}


\paragraph{Topic}
Table~\ref{tab:english_results_by_topic} shows that LLMs encode cultural knowledge differently across various aspects of Arab culture. For example, GPT-4o performs best in agriculture and family relationships, while AceGPT excels in topics related to death and idioms. Meanwhile, Qwen achieves its highest accuracy in agriculture and traditional games. The accuracy gap between the highest- and lowest-performing topics across models ranges from 10 to 20 points, highlighting the difficulty of adapting cultural knowledge in LLMs. Additionally, we observe a consistent trend with Table~\ref{tab:english_results_by_country}, where country-specific samples are more challenging than non-country-specific ones.


%The results reveal a consistent trend that CS samples are more challenging than the $\neg CS$ samples across various topics. Again, we notice that the cultural knowledge embedded in the different models is different. This is reflected in the difference between models in which topics have the highest and lowest accuracies.   For example examples, GPT-4o demonstrated superior performance in topics such as agriculture and family relationships. In contrast, AceGPT excelled in topics like death and idioms. Qwen, on the other hand, achieved its best results in agriculture and traditional games. 


\subsection{Can the Model Provide a Reasonable Explanation to Support the Answer?}
\label{sub:explanation_genration}
We focus on Arabic-centric models—Jais, AceGPT, and Silma—to evaluate their actual generation capabilities. For 200 randomly selected samples, we generate responses by appending \<مع ذكر السبب> ("with mentioning the reason") to the Arabic prompt (\S \ref{appendix:zero_shot_prompt}) to instruct the model to provide a brief explanation for its choice. We then manually assess the outputs to verify both the correctness of the answer and the validity of the explanation.

Figure~\ref{fig:generation_exp} presents the results for each model, comparing their generation accuracy with their MCQ performance from Table~\ref{tab:model_scores}. Jais demonstrated a significant improvement, increasing from 40\% in MCQ to 72\% in the manual evaluation. In contrast, Silma’s performance dropped from 73\% in MCQ to 67\% in the generation task. Notably, Silma often failed to generate explanations, instead providing only the answer key or, at times, just the answer text. Meanwhile, AceGPT maintained a consistent performance across both MCQ and generation tests, showing no significant change in accuracy.




 




\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/model_accuracies_highlighted.pdf}
    \caption{Accuracies per models before and after context augmentation. This experiment uses the Arabic prompt template for MCQ and location \(\ell \in \{\text{region + country}\}\).}
    \label{fig:augment_res}
    \vspace{-0.5cm}
\end{figure}


\subsection{Improving Small Language Model with Additional Context from GPT-4o}
% We experiment with the base versions of six models (up to 3B parameters): Qwen2.5-0.5B, Qwen2.5-1.5B, Qwen2.5-3B, Gemma-2-2B, Llama-3.2-1B, and Llama-3.2-3B, to assess the impact of augmenting the context with cultural information. Using GPT-4o, we generate five factual Arabic sentences for each premise and subtopic per country, based on the Arabic prompt shown in Figure \ref{fig:prompt_cultural_context}. The sentences corresponding to each premise were then incorporated into the multiple-choice question prompt (Arabic prompt) illustrated in Figure \ref{fig:prompts}, with location \(\ell \in \{\text{region + country}\} \).

We evaluate six base models with $\leq$3B parameters to assess the impact of cultural context augmentation. Using GPT-4o, we generate five factual Arabic sentences conditioned on the premise, subtopic, and country, following the Arabic prompt in Figure~\ref{fig:prompt_cultural_context}. These sentences are then incorporated into the Arabic multiple-choice question prompt (\S \ref{app:cultural_prompt}) with location context \(\ell \in \{\text{region + country}\} \). Results (Figure \ref{fig:augment_res}) show accuracy gains for Qwen and Llama models, except for Qwen2.5-0.5B (decline) and Gemma-2-2B (unchanged). Qwen2.5-3B achieves the highest improvement across all countries.  These variations likely stem from differences in training data: Qwen and Llama were trained on multilingual datasets, whereas Gemma-2, primarily trained in English, has limited multilingual support.

%Table \ref{tab:country_acc_aug} highlights accuracy per country for the three best-performing models, underscoring the value of cultural context. 
%The observed accuracy boost in Qwen2.5 aligns with its Arabic-language capabilities, enhancing its response to context augmentation.



% Figure \ref{fig:augment_res} presents the results before and after context augmentation. We observe a notable improvement in accuracy for the Qwen and Llama models, except for Qwen2.5-0.5B, which shows a decrease, and Gemma-2-2B, which remains unchanged. Specifically, Qwen2.5-3B demonstrates the best performance gains in all countries. 


% Table \ref{tab:country_acc_aug} further highlights these improvements, showcasing the accuracy per country for the three best-performing models after context augmentation, underscoring the value of additional cultural context to help the models choose the correct option. These changes in accuracy for Qwen and Llama can be attributed to differences in their training data. While Qwen and Llama were trained on multilingual datasets, the base version of Gemma-2 was primarily trained on English language data, with limited support for other languages.
% Specifically, for Qwen2.5 which supports Arabic language, the observed increase in accuracy after context augmentation can be explained by its multilingual training, enabling better handling of Arabic content.

\section{Conclusion}
We introduced \datasetname{}, a benchmark for evaluating cultural commonsense reasoning in the Arab world. The dataset comprises 3,482 questions across 13 countries, covering 12 daily life domains with 54 fine-grained subtopics, all authored and validated by native speakers. Evaluations on 31 LLMs show significant performance gaps, with open-weight models up to 32B struggling to capture Arab cultural contexts. Variability across countries, regions, and topics highlights the need for more culturally aware models and datasets tailored to the Arabic-speaking world.

\section*{Limitations}

% reasoning is more that just culture
\paragraph{Culture is only one side of reasoning}
While cultural knowledge plays a crucial role in shaping commonsense reasoning, it is only one of several dimensions that contribute to a model's overall reasoning capabilities~\citep{plaat2024reasoninglargelanguagemodels}. Reasoning in LLMs encompasses a broad range of cognitive skills, including logical inference, numerical reasoning, and causal understanding, among others.
% the dataset is written in MSA
\paragraph{The Influence of Dialects on Cultural Reasoning}

The Arab world is characterized by rich dialects that vary not only across countries but also within different regions of the same country. These dialects significantly shape cultural expression, influencing language use in areas such as proverbs, humor, and everyday communication. This is particularly evident in topics like "idioms," where meaning and usage are deeply tied to specific dialects and local linguistic conventions.

However, to ensure that our evaluation isolates cultural commonsense reasoning rather than a model’s proficiency in specific dialects, we constructed \datasetname{} in Modern Standard Arabic (MSA). MSA serves as a unifying linguistic medium across Arabic-speaking countries, allowing us to control for dialectal variation while still capturing essential cultural knowledge. While this approach enhances comparability across regions, it also introduces a limitation: certain cultural concepts that are best expressed through dialect-specific phrasing or context may not be fully represented in our dataset.
%^^^^%
%Location Leakage%
\paragraph{Location Leakage} Despite our efforts to systematically control the granularity of location information, some questions or corresponding multiple-choice completions inadvertently reveal the location through the inclusion of location cues (landmarks, national events, etc.) within prompts. Thus, resulting in unintended location leakage, where the model gains access to country-specific cues directly from text rather than controlled contexts, making it difficult to isolate the effect of the location granularity control.

%ALL Arab Countries%
\paragraph{Coverage of All Arab Countries}
While our study covers a significant portion of the Arab world, representing 82\% of the total population, certain unique cultures remain underrepresented. Notably, countries such as Mauritania, Somalia, and Comoros were not included, despite their distinct cultural and linguistic characteristics. These nations, located in North Africa, the Horn of Africa, and the Indian Ocean, respectively, contribute to the broader diversity of the Arab world. Their exclusion was primarily due to the difficulty in sourcing human annotators from these regions.


%\section*{Acknowledgments}



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}

\appendix

\section{Dataset Statement for \datasetname}
\subsection{General Information}
\textbf{Dataset title} \datasetname
\newline
\textbf{Dataset version} 1.0 (Feb 2025)
\subsection{Executive Summary}
\datasetname is a cultural Arabic commonsense reasoning dataset covering 13 Arabic countries in the Gulf, Levant, North Africa, and the Nile Valley. The dataset spans 12 daily life domains and 54 fine-grained subtopics. It was created from scratch by native speakers who validated culturally relevant questions.
\subsection{Curation Rationale}
\datasetname serves as a cultural benchmark to assess large language models' ability to reason within culturally specific contexts. Built from scratch by native speakers, it avoids web-scraped text and undergoes rigorous quality control to ensure lexical accuracy, semantic coherence, and cultural sensitivity.

The dataset creation process involves:
\begin{enumerate}[label=\arabic*.]
    \item \textbf{Coverage Determination:} Selecting relevant countries and topics.
    \item \textbf{Annotator Selection:} Hiring qualified native speakers with deep cultural knowledge.
    \item \textbf{Example Generation:} Each annotator produces 150 examples, with two annotators per country.
    \item \textbf{Cross-Review:} Annotators validate each other’s work by confirming correct answers.
    \item \textbf{Final Review:} Unclear samples are revised or discarded based on comprehensive quality checks.
\end{enumerate}

\subsection{Documentation for Source Datasets}

\datasetname is built entirely from scratch, without relying on web-scraped text, All data is manually created and validated by native speakers from 13 Arabic-speaking countries. 

\subsection{Country and Regional Diversity}
\datasetname covers 13 Arab countries, ensuring rich cultural perspectives.

Our country selection was guided by the goal of broad geographic representation across the Arab world. These 13 countries span four key regions:
\begin{itemize}
    \item \textbf{The Gulf:} Saudi Arabia, Yemen, UAE.
    \item \textbf{The Levant:} Syria, Jordan, Palestine, Lebanon.
    \item \textbf{North Africa:} Morocco, Algeria, Tunisia, Libya.
    \item \textbf{The Nile Valley:} Egypt, Sudan.
\end{itemize}

\subsection{Annotator Demographics}
We recruited 26 expert annotators from 13 Arab countries, with two annotators representing each country. To ensure cultural authenticity and linguistic proficiency, we enforced the following eligibility criteria:
\begin{itemize}
    \item Native Arabic speakers.
    \item Residency in the country for at least 10 years.
    \item Deep understanding of local culture and traditions.
    \item Both parents are native to and reside in the country.
    \item Minimum educational requirement of a high school diploma (higher degrees are preferred).
\end{itemize}
Among the 26 annotators, 14 hold a Bachelor's degree, seven have a Master's degree, two have a PhD, and three have a high school diploma. All annotators participated (attended or watched a record) in an initial online workshop to ensure a clear understanding of the project guidelines.

\subsection{Topic Diversity}
\datasetname features a carefully curated taxonomy of daily life topics. It includes 12 main topics with 54 fine-grained subtopics—covering areas such as food, weddings, holiday activities, daily routines, habits, traditional games, death, art, parenting, agriculture, family relationships, and idioms. This extensive range ensures that the dataset captures both common and unique cultural experiences across the Arab world.

\section{Chosen Topics Distribution}
Table~\ref{tab:topics} shows the distribution of topics and their corresponding subtopics. 



\section{Prompts}
\subsection{Zero-shot Experiment prompts}
\label{appendix:zero_shot_prompt}
Figure \ref{fig:prompts} shows the prompts in Arabic and English that we used for the zero-shot experiments. The Arabic prompt is also used to generate responses for~\ref{sub:explanation_genration}.


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/arabic_culture_prompts.pdf}
\caption{Templates for multiple-choice question prompts. Sentence completion prompt template is the same but without the options section. Location \(\ell \in \{\text{none}, \text{region}, \text{region + country}\}\).}
\label{fig:prompts}
\end{figure}



\subsection{Cultural Context prompts}
\label{app:cultural_prompt}
Figure \ref{fig:prompt_cultural_context} shows the Arabic that we used to generate the five cultural sentences for improving the small language model with additional context from GPT-4o. A translation in English is available to allow those who do not understand Arabic to comprehend the prompt.


\label{appendix:cultural_context_prompt}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/cultural_context_prompt.pdf}
    \caption{Prompts to generate culturally relevant sentences for context augmentation. The Arabic prompt was used for the generation.}
    \label{fig:prompt_cultural_context}
\end{figure}

% \label{sec:appendix}
\section{Results of the Arabic prompt in the zero-shot experiments}
Table~\ref{tab:model_scores_arabic} includes the zero-shot experiment results for the Arabic Prompt. The prompt is shown in Figure~\ref{fig:prompts}.
\label{appendix:arabic_zero_shot}

\subsection{Results by Geographic Location - Arabic Prompt}
\label{appendix:arabic_geographic_location_analysis}
\input{latex/tables/arabic_results_by_country}
Table~\ref{tab:arabic_results_by_country} includes the geographical analysis results for the Arabic Prompt. 
\subsection{Results by Topic - Arabic Prompt}
\label{appendix:arabic_topic_analysis}
\input{latex/tables/arabic_results_by_topic}
Table~\ref{tab:arabic_results_by_topic} includes the analysis by topic results for the Arabic Prompt. 
\input{latex/tables/topics}
\input{latex/tables/model_scores_table_ar}
\end{document}

\input{latex/tables/country_acc_aug}
