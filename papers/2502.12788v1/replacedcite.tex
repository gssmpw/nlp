\section{Related Work}
\subsection{Commonsense Reasoning in English} 

%Recent advancements in large language models (LLMs) have driven the creation of increasingly diverse and challenging benchmarks designed to assess their capabilities. Among these, commonsense reasoning has become a key focus, as it highlights the extent to which LLMs can demonstrate understanding beyond surface-level text patterns.

Early research on commonsense reasoning primarily focused on linguistic reasoning, as seen in the Winograd Schema Challenge____ and Winogrande____, which evaluate pronoun coreference resolution within social and linguistic contexts. Other works have explored physical commonsense reasoning, assessing model's understanding of real-world properties and relationships____, as well as social reasoning, where models are tested on their ability to interpret human emotions, actions, and social norms ____. Research has also expanded into numerical ____, temporal ____, and causal reasoning ____, broadening the scope of commonsense evaluation. However, these benchmarks are all developed in English and shaped by Western cultural assumptions, limiting their applicability to the Arab world.

\subsection{Arabic Large Language Models and Their Evaluation on Commonsense Reasoning}
A limited number of Arabic language models have been developed with more than 7B parameters, all of which are decoder-only architectures. These include \texttt{JAIS}____, \texttt{Fanar}____, \texttt{AceGPT}____, and \texttt{ALLAM}____. Their evaluation of commonsense reasoning has been primarily based on machine-translated datasets from English to Arabic (e.g., ____). Although this approach provides a useful reference, it does not offer a comprehensive assessment of how well these models capture culturally grounded commonsense knowledge.

Much of the recent Arabic-centric benchmarks have focused on classic NLP tasks, including syntax, semantics, and question answering, as seen in \texttt{LaraBench}____, natural language generation in \texttt{DOLPHIN}____, and natural language understanding in \texttt{ORCA}____. Only a few studies have shifted their focus toward knowledge-intensive tasks and reasoning abilities. Among them, \texttt{ArabicMMLU}____ compiles exam questions from different education levels across Arabic-speaking countries, offering a broad knowledge assessment but placing less emphasis on cultural reasoning. 

Table~\ref{tab:dataset_comparison} compares \datasetname{} with related Arabic datasets. Most existing datasets are derived from machine translation with post-editing, prioritizing linguistic accuracy over cultural relevance. While some assess reasoning, they often lack cultural grounding, location metadata, and fine-grained topic categorization. ACVA____, generated using ChatGPT____, is not designed for reasoning evaluation. Similarly, \texttt{AraDice-Culture}____ consists of only 180 samples and focuses on open-ended cultural knowledge rather than structured reasoning tasks. These gaps highlight the need for larger, more diverse benchmarks that better capture Arabic cultural contexts and reasoning abilities.


%Arabic language is a rich and complex language
%A few studies have focused on commonsense validation and explanation of Arabic sentences. The first notable work in this area was proposed by ____, which introduced commonsense validation in Arabic by translating the SemEval-2020 Task 4 corpus, known as ComVE ____. This dataset comprises 12,000 pairs of sentences designed for commonsense reasoning tasks. Subsequent research efforts aimed to enhance commonsense validation in Arabic ____, ____, ____, by training models capable of distinguishing between natural language statements that make sense and those that do not.

%To bridge this gap, AraDiCE ____ introduced a benchmark suite designed to evaluate both dialectal and cultural capabilities in LLMs. One of its key components, AraDiCE-Culture, focuses specifically on Arabic cultural understanding by including questions from different regions, such as Egypt, the Levant, and the Gulf. These questions span diverse topics, including public holidays, food, geography, history, public figures, and traditional clothing, providing a comprehensive assessment of culturally grounded knowledge. Other initiatives have also contributed to the creation of culturally diverse datasets. For example, CVQA ____ is a culturally enriched visual question-answering dataset that incorporates Egyptian cultural elements. 

%Additionally, cultural benchmarks like DLAMA ____ and CAMeL ____ have focused on examining biases in LLMs toward various cultures, including Arabic, highlighting areas where these models exhibit cultural bias to the west. 




%Additionally, cultural benchmarks cite{yin2022geomlama} and ____ test the cultural knowledge of the LLMs.  

%More recently, Arabic-centric large language models (LLMs), such as Jais and Jais Chat ____, have demonstrated superior capabilities in commonsense reasoning for Arabic. Experimental results reveal that Jais (13B) and Jais Chat (13B) outperform existing models across six commonsense reasoning datasets, including HellaSwag ____, PIQA ____, SituatedQA ____,  ARC-C ____, and OBQA ____.

%\subsection{Cultural commonsense reasoning in Arabic}
%With the rapid advancement of large language models (LLMs), several efforts have been made to develop Arabic-centric LLMs, such as Jais and Jais Chat ____, Fanar ____, AceGPT ____, and ALLAM ____. In parallel, numerous benchmarks have been proposed to evaluate the capabilities of these LLMs across various tasks ____, ____, ____, ____, and ____. However, only a few focused on cultural capabilities of these LLMs.