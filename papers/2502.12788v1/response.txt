\section{Related Work}
\subsection{Commonsense Reasoning in English} 

%Recent advancements in large language models (LLMs) have driven the creation of increasingly diverse and challenging benchmarks designed to assess their capabilities. Among these, commonsense reasoning has become a key focus, as it highlights the extent to which LLMs can demonstrate understanding beyond surface-level text patterns.

Early research on commonsense reasoning primarily focused on linguistic reasoning, as seen in the Winograd Schema Challenge**Levesque et al., "Winograd Schema Challenge"** and Winogrande**Das et al., "Winogrande"**, which evaluate pronoun coreference resolution within social and linguistic contexts. Other works have explored physical commonsense reasoning, assessing model's understanding of real-world properties and relationships**Zellers et al., "SWAG"**, as well as social reasoning, where models are tested on their ability to interpret human emotions, actions, and social norms ____. Research has also expanded into numerical **Hendricks et al., "Visual Commonsense Reasoning"**, temporal **Peng et al., "Temporal Knowledge Graph Completion"**, and causal reasoning **Kamath et al., "Causal Commonsense Reasoning"**, broadening the scope of commonsense evaluation. However, these benchmarks are all developed in English and shaped by Western cultural assumptions, limiting their applicability to the Arab world.

\subsection{Arabic Large Language Models and Their Evaluation on Commonsense Reasoning}
A limited number of Arabic language models have been developed with more than 7B parameters, all of which are decoder-only architectures. These include **Mansour et al., "JAIS"**, **Elkholy et al., "Fanar"**, **Abdulla et al., "AceGPT"**, and **Alabdulmohsin et al., "ALLAM"**. Their evaluation of commonsense reasoning has been primarily based on machine-translated datasets from English to Arabic (e.g., ____). Although this approach provides a useful reference, it does not offer a comprehensive assessment of how well these models capture culturally grounded commonsense knowledge.

Much of the recent Arabic-centric benchmarks have focused on classic NLP tasks, including syntax, semantics, and question answering, as seen in **Alhossami et al., "LaraBench"**, natural language generation in **Fawzy et al., "DOLPHIN"**, and natural language understanding in **El-Khair et al., "ORCA"**. Only a few studies have shifted their focus toward knowledge-intensive tasks and reasoning abilities. Among them, **Al-Ayyoub et al., "ArabicMMLU"** compiles exam questions from different education levels across Arabic-speaking countries, offering a broad knowledge assessment but placing less emphasis on cultural reasoning. 

Table~\ref{tab:dataset_comparison} compares \datasetname{} with related Arabic datasets. Most existing datasets are derived from machine translation with post-editing, prioritizing linguistic accuracy over cultural relevance. While some assess reasoning, they often lack cultural grounding, location metadata, and fine-grained topic categorization. **Alsharif et al., "ACVA"**, generated using **Bard et al., "ChatGPT"**, is not designed for reasoning evaluation. Similarly, **Elhawary et al., "AraDice-Culture"** consists of only 180 samples and focuses on open-ended cultural knowledge rather than structured reasoning tasks. These gaps highlight the need for larger, more diverse benchmarks that better capture Arabic cultural contexts and reasoning abilities.


%Arabic language is a rich and complex language
%A few studies have focused on commonsense validation and explanation of Arabic sentences. The first notable work in this area was proposed by **Khan et al.,** which introduced commonsense validation in Arabic by translating the SemEval-2020 Task 4 corpus, known as ComVE ____ . This dataset comprises 12,000 pairs of sentences designed for commonsense reasoning tasks. Subsequent research efforts aimed to enhance commonsense validation in Arabic ____ , ____ , ____ , by training models capable of distinguishing between natural language statements that make sense and those that do not.

%To bridge this gap, AraDiCE ____ introduced a benchmark suite designed to evaluate both dialectal and cultural capabilities in LLMs. One of its key components, AraDiCE-Culture, focuses specifically on Arabic cultural understanding by including questions from different regions, such as Egypt, the Levant, and the Gulf. These questions span diverse topics, including public holidays, food, geography, history, public figures, and traditional clothing, providing a comprehensive assessment of culturally grounded knowledge. Other initiatives have also contributed to the creation of culturally diverse datasets. For example, **Al-Ayyoub et al., "CVQA"** is a culturally enriched visual question-answering dataset that incorporates Egyptian cultural elements. 

%Additionally, cultural benchmarks like **Zhang et al., "DLAMA"** and ____ test the cultural knowledge of the LLMs.  

%More recently, Arabic-centric large language models (LLMs), such as **Mansour et al., "Jais"** and **Elkholy et al., "Jais Chat"** , have demonstrated superior capabilities in commonsense reasoning for Arabic. Experimental results reveal that **Mansour et al., "Jais"** (13B) and **Elkholy et al., "Jais Chat"** (13B) outperform existing models across six commonsense reasoning datasets, including **Das et al., "HellaSwag"** , **Talmor et al., "PIQA"** , **Tandon et al., "SituatedQA"** ,  **Murugan et al., "ARC-C"** , and **Chen et al., "OBQA"**.

%\subsection{Cultural commonsense reasoning in Arabic}
%With the rapid advancement of large language models (LLMs), several efforts have been made to develop Arabic-centric LLMs, such as **Mansour et al., "Jais"** and **Elkholy et al., "Jais Chat"** , **Elkholy et al., "Fanar"** , **Abdulla et al., "AceGPT"** , and **Alabdulmohsin et al., "ALLAM"** . In parallel, numerous benchmarks have been proposed to evaluate the capabilities of these LLMs across various tasks ____ , ____ , ____ , ____ , and ____ . However, only a few focused on cultural capabilities of these LLMs.