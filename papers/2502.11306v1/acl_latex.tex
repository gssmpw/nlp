% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{multirow}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Instructions for *ACL Proceedings}
\title{Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation}

\author{
Hieu Nguyen\textsuperscript{1}, Zihao He\textsuperscript{1,2},
Shoumik Atul Gandre\textsuperscript{1}, Ujjwal Pasupulety\textsuperscript{1}, \\
\textbf{Sharanya Kumari Shivakumar\textsuperscript{1}, Kristina Lerman\textsuperscript{2}}
\\
\textsuperscript{1}Department of Computer Science, University of Southern California\\
\textsuperscript{2}Information Sciences Institute, University of Southern California\\
{\texttt{\{hieutn, zihaoh, shoumika, upasupul, ss82937\}@usc.edu}},
{\texttt{lerman@isi.edu}}
  }

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\begin{abstract}
Large language models (LLMs) often suffer from hallucination, generating factually incorrect or ungrounded content, which limits their reliability in high-stakes applications. A key factor contributing to hallucination is the use of hard labels during training, which enforce deterministic supervision, encourage overconfidence, and disregard the uncertainty inherent in natural language. To address this, we propose mitigating hallucination through knowledge distillation (KD), where a teacher model provides smoothed soft labels to a student model, reducing overconfidence and improving factual grounding. We apply KD during supervised finetuning on instructional data, evaluating its effectiveness across LLMs from different families. Experimental results on summarization benchmarks demonstrate that KD reduces hallucination compared to standard finetuning while preserving performance on general NLP tasks. These findings highlight KD as a promising approach for mitigating hallucination in LLMs and improving model reliability.\footnote{Our code and data will be publicly available upon acceptance.}
\end{abstract}

\section{Introduction}

Large language models (LLMs) have demonstrated remarkable capabilities in generating fluent and contextually coherent text, achieving state-of-the-art performance in various natural language processing (NLP) tasks, including machine translation \cite{vaswani2017attention}, question answering \cite{brown2020language, chowdhery2023palm}, and summarization \cite{zhang2020pegasus, raffel2020exploring}. However, despite their impressive generative abilities, a fundamental challenge remains: hallucination—the tendency of LLMs to generate false, misleading, or unverifiable content \cite{ji2023survey, bang2023multitask}. Hallucinations in LLMs pose serious concerns, particularly in applications that demand factual accuracy, such as medical diagnosis \cite{moor2023foundation, chu2024improving}, legal document generation \cite{guha2024legalbench}, and scientific content summarization \cite{xie2023survey}. Consequently, mitigating hallucination in LLMs is a critical research direction for ensuring reliability and trustworthiness in real-world applications.


Most LLMs are trained using next-token prediction based on maximum likelihood estimation \cite{radford2019language, touvron2023llama, dubey2024llama}. During training, models are optimized using the cross-entropy loss, which compares the predicted token probabilities to the ground-truth next token. Traditionally, ground-truth tokens are represented as one-hot vectors, known as hard labels. This means that the model is forced to assign the entire probability mass to a single token while treating all alternative completions as incorrect.

Although this approach is widely adopted, it has several drawbacks that may exacerbate hallucination. First, hard labels encourage overconfidence in incorrect predictions. Since only one token is treated as correct during training, the model learns to disregard other reasonable continuations, leading to overconfident mispredictions \cite{muller2019does, guo2017calibration}. Second, hard labels violate the principle of maximum entropy \cite{jaynes1957information}, which suggests that, given partial information, the most rational probability distribution should retain as much uncertainty as possible. By artificially forcing a single correct answer, hard labels introduce arbitrary assumptions that can mislead the model, particularly in ambiguous contexts. Third, hard labels fail to capture contextual dependencies effectively. Language generation is inherently probabilistic, and multiple completions can be equally valid depending on prior context \cite{holtzman2020curious}. Hard labels, by contrast, encourage rigid decision-making, making LLMs more prone to hallucinating confident but incorrect outputs.



To address these issues, we propose an alternative training approach based on knowledge distillation (KD) \citep{hinton2015distilling, kim2016sequence}. Instead of training models with hard labels, we introduce smoothed soft labels derived from a teacher model. In this paradigm, the teacher model generates probability distributions over possible next tokens, providing a richer and more informative training signal for the student model.

Using soft labels offers several advantages over traditional hard-label training. First, soft labels introduce uncertainty-aware supervision, allowing the student model to learn from a more calibrated probability distribution rather than a binary correct/incorrect signal. This helps mitigate overconfidence and encourages more flexible decision-making. Second, soft labels better align with the principle of maximum entropy, as they retain nonzero probabilities for multiple plausible continuations, thereby reducing arbitrary assumptions in model predictions. Finally, because soft labels are generated by a highly capable teacher model, they provide contextually grounded probability distributions that naturally reinforce faithful and less hallucinatory outputs.


In this paper, we investigate how knowledge distillation with smoothed soft labels can be leveraged to reduce hallucination in LLMs. We conduct experiments on three LLM families: Llama-2, Llama-3.1, and Qwen-2.5, evaluating different student-teacher pairs to analyze the effectiveness of KD in mitigating hallucination. To systematically evaluate hallucination, we focus on faithfulness hallucination, which occurs when a model generates outputs that are not grounded in the given context \cite{huang2023survey}. We assess model performance using CNN/Daily Mail and XSUM, two widely used summarization benchmarks from the hallucination leaderboard \cite{hughes_vectara_2023}. Our evaluation leverages three complementary metrics: ROUGE-L for n-gram overlap, factual consistency for assessing context-grounding, and factual rate for measuring hallucination at the span level \cite{chuang2024lookback}.

Our key findings can be summarized as follows:

\begin{itemize}
    \item Knowledge distillation reduces hallucination: Across all models, in most cases, finetuning with soft labels outperforms standard supervised finetuning in mitigating faithfulness hallucination. This supports our hypothesis that soft labels provide a more effective training signal than hard labels.
    \item KD preserves general performance: In addition to hallucination benchmarks, we evaluate models on general NLP tasks, including OpenBookQA, ARC, and HellaSwag, to ensure that KD does not degrade broader reasoning and comprehension abilities. Our results show that KD maintains or improves general performance, indicating that it is a viable technique for enhancing LLM reliability without compromising overall capabilities.
\end{itemize}

Our findings demonstrate that knowledge distillation effectively reduces faithfulness hallucination while maintaining strong generalization across NLP tasks. By replacing hard labels with soft, uncertainty-aware training targets, KD improves model calibration and factual grounding, making LLMs more reliable. 


\begin{figure*}[ht!]
  \centering
       \includegraphics[width=\textwidth]{imgs/motiv.pdf}
  \caption{
  Comparison of cross-entropy optimization with hard labels vs. smoothed soft labels. The figure illustrates how training with (a) hard labels  differs from training with (b) contextually smoothed labels in an autoregressive language model. In (a), the hard label for the word “\textit{physics}” is represented as a one-hot encoded (OHE) vector, assigning full probability (1.0) to a single token while forcing all alternative predictions (e.g., “\textit{Maths}”, “\textit{Assignments}”, “\textit{Arts}”) to have zero probability. This OHE representation introduces zero entropy, disregarding the inherent uncertainty in natural language, and leading the model to overconfidently discard reasonable alternatives. This forced certainty can cause the model to develop spurious assumptions and hallucinate incorrect outputs when faced with ambiguous contexts.
  }
  \label{fig:ce-vs-kd}
\end{figure*}

\section{Methodology}

\subsection{Problem Formulation}
Autoregressive language models are trained using the next-token prediction task \cite{radford2019language}. Given an input sequence of tokens, the model generates a probability distribution over the vocabulary and is optimized to minimize the cross-entropy loss between the predicted probabilities and the true labels:

\begin{equation}
    \mathcal{L}_{\text{supervised}} = \mathcal{L}_{\text{CE}}(\sigma(z), y),
\end{equation}
where $z$ represents the logits from the model, $\sigma$ denotes the softmax function, and $y$ are the ground-truth labels.

However, this standard training paradigm typically relies on hard labels (Figure \ref{fig:ce-vs-kd}), which assign a probability of 1 to a single correct token in the vocabulary and 0 to all others. While this simplifies training, we argue that it introduces critical issues—particularly overconfidence and hallucination—due to its rigid assumptions.


\subsection{Hard Labels and Hallucination}
\label{sec:hard_labels}
In standard language model training, ground-truth labels are typically represented as one-hot vectors, assuming a single correct next token. However, this rigid labeling has several drawbacks.

\paragraph{Hard labels cause overconfidence} 
Neural networks trained on hard labels often exhibit poor calibration, meaning they assign excessively high confidence to incorrect predictions \cite{muller2019does, guo2017calibration}. In language modeling, consider the input: ``\textit{The student did well in ...}'', as shown in Figure \ref{fig:ce-vs-kd}. A well-calibrated model should distribute probability mass across multiple plausible completions, such as ``\textit{physics}'', ``\textit{maths}'', ``\textit{assignments}'', ``\textit{arts}''. However, when trained with hard labels, the model is forced to treat only one option (e.g., ``\textit{physics}'') as correct while disregarding all other reasonable alternatives. This results in overconfidence, which can exacerbate hallucination—the model’s tendency to generate fluent but incorrect outputs. We further observe the overconfidence of LLMs in our exploratory analysis in Appendix \ref{apx:overconf}. 


\paragraph{Hard labels introduce arbitrary assumptions}
From an information-theoretic perspective, optimizing a model with hard labels violates the principle of maximum entropy \cite{jaynes1957information}. The principle states: ``\textit{In making inferences on the basis of partial information, we must use the probability distribution with maximum entropy, subject to whatever is known.}''. Hard labels contradict this by enforcing a deterministic choice for the next token, even when the context suggests multiple valid options. This injects arbitrary assumptions into the model, leading to over-specified predictions that may not generalize well.

\paragraph{Hard labels overlook contextual dependencies}
Language models generate predictions conditionally based on prior tokens, yet hard labels do not explicitly encode these dependencies. Consider the word ``\textit{country}''. In ``\textit{America is a ...}'', the next token might be ``\textit{country}'' or ``\textit{continent}''; in ``She lives in a ...'', ``\textit{country}'' is a much stronger candidate ``\textit{continent}''. Hard labels ignore this difference by treating each token in isolation, limiting the model’s ability to adjust predictions based on context. This lack of flexibility may lead to hallucinated responses that do not align with preceding information \cite{chen2022towards, miao2021prevent}.

Given these limitations, we propose an alternative: smoothing hard labels via knowledge distillation to mitigate hallucination.

\subsection{Smoothing Hard Labels with Knowledge Distillation}
Knowledge Distillation (KD) is traditionally used to transfer knowledge from a large teacher model to a smaller student model for efficiency \cite{hinton2015distilling, kim2016sequence}. However, in this work, we leverage KD differently. Instead of hard labels, we use soft labels produced by a highly capable teacher model to provide a smoother training signal for the student model. These soft labels preserve uncertainty, allowing the student model to learn a more realistic probability distribution over possible outputs.

This approach directly addresses the issues outlined in $\S$\ref{sec:hard_labels}: (1) Mitigating overconfidence: soft labels distribute probability mass across multiple reasonable tokens, reducing extreme confidence in incorrect predictions. (2) Avoiding arbitrary assumptions: Since the teacher-generated probabilities preserve entropy, they align better with the maximum entropy principle. (3) Enhancing context awareness: the teacher model produces context-dependent probability distributions, leading to more coherent and contextually appropriate predictions.

Specifically, given an input sequence, we define the knowledge distillation loss as

\begin{equation}
    \mathcal{L}_{\text{KD}}=\mathcal{L}_{\text{CE}}(\sigma(z_s), \sigma(z_t)),
\end{equation}
where $z_s$ and $z_t$ are the logits from the student and the teacher models respectively. The overall training loss is a combination of standard supervised learning and knowledge distillation:
\begin{equation}
\label{eq:loss-kd}
    \mathcal{L} = \mathcal{L}_{\text{supervised}} + \alpha \mathcal{L}_{\text{KD}},
\end{equation}
where $\alpha$ is a hyperparameter controlling the influence of the teacher's soft labels.


\begin{table*}[ht]
    \centering
\centering
\begin{tabular}{lcccccc}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Method}}} &
  \multicolumn{3}{c}{\textbf{CNNDM}} &
  \multicolumn{3}{c}{\textbf{XSUM}} \\ \cline{2-7} 
\multicolumn{1}{c}{} &
  \textbf{ROUGE-L (\%)} &
  \textbf{FC (\%)} &
  \textbf{FR (\%)} &
  \textbf{ROUGE-L (\%)} &
  \textbf{FC (\%)} &
  \textbf{FR (\%)} \\ \hline
\textbf{Llama-2-7B} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} \\
\textbf{+SFT} &
  28.0±0.30 &
  86.3±1.2 &
  94.8±1.3 &
  17.4±0.41 &
  73.8±1.7 &
  \textbf{91.2±3.2} \\
\textbf{+KD$_{0.1}$} &
  28.4±0.35 &
  87.4±0.7 &
  \textbf{95.4±0.9} &
  17.7±0.31 &
  75.0±1.4 &
  90.4±3.5 \\
\textbf{+KD$_{1.0}$} &
  28.6±0.29 &
  87.4±1.0 &
  94.4±1.3 &
  17.8±0.33 &
  75.2±1.3 &
  89.6±3.7 \\
\textbf{+KD$_{10.0}$} &
  \textbf{28.8±0.21} &
  \textbf{87.7±0.4} &
  93.9±0.4 &
  \textbf{18.0±0.12} &
  \textbf{76.2±1.5} &
  89.7±1.5 \\ \hline
\textbf{Llama3.1-8B} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{\textbf{}} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{\textbf{}} \\
\textbf{+SFT} &
  31.4±0.32 &
  93.5±0.5 &
  \textbf{80.7±2.2} &
  20.6±0.14 &
  79.2±1.3 &
  59.1±1.4 \\
\textbf{+KD$_{0.01}$} &
  \textbf{31.7±0.14} &
  93.3±0.6 &
  79.2±3.3 &
  \textbf{20.6±0.14} &
  79.1±1.3 &
  \textbf{60.5±1.0} \\
\textbf{+KD$_{0.1}$} &
  31.6±0.16 &
  93.3±0.7 &
  78.4±4.2 &
  20.6±0.15 &
  79.2±1.3 &
  59.9±0.7 \\
\textbf{+KD$_{1.0}$} &
  31.2±0.26 &
  \textbf{93.8±0.2} &
  78.0±3.9 &
  20.2±0.12 &
  \textbf{80.9±0.1} &
  59.6±1.9 \\ \hline
\textbf{Qwen2.5-7B} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{\textbf{}} &
  \multicolumn{1}{l}{\textbf{}} \\
\textbf{+SFT} &
  27.5±1.14 &
  92.3±0.9 &
  89.5±0.7 &
  20.2±0.99 &
  76.0±0.9 &
  71.6±2.2 \\
\textbf{+KD$_{0.01}$} &
  27.8±1.39 &
  92.3±0.9 &
  89.4±0.7 &
  \textbf{20.3±1.04} &
  76.0±1.2 &
  71.9±2.1 \\
\textbf{+KD$_{0.1}$} &
  27.8±1.37 &
  92.3±0.9 &
  89.6±0.6 &
  20.2±1.00 &
  76.4±0.8 &
  72.4±1.5 \\
\textbf{+KD$_{1.0}$} &
  \textbf{27.8±1.30} &
  \textbf{92.5±1.0} &
  \textbf{90.2±1.2} &
  20.0±0.76 &
  \textbf{77.6±0.5} &
  \textbf{73.6±1.9} \\ \hline
\end{tabular}
    \caption{
    Hallucination evaluation results for student models finetuned with supervised finetuning (SFT) and knowledge distillation (KD). Models are evaluated on the CNN/Daily Mail (CNNDM) and XSUM datasets using three metrics: ROUGE-L ($\uparrow$, \%) for n-gram overlap, factual consistency (FC, $\uparrow$, \%) for context grounding, and factual rate (FR, $\uparrow$, \%) for specialized hallucination detection. Each experiment is conducted with varying learning rates and batch sizes, and results are reported as the mean and standard deviation across runs. The results suggest that in most cases KD reduces hallucination compared to SFT, as models trained with soft labels from a teacher model demonstrate improved faithfulness.
    }
    \label{tab:hallu_perf}
\end{table*}



\section{Experiments}

\subsection{Mitigating Hallucination with KD}
Training an LLM from scratch with KD would be computationally expensive and impractical. Instead, we apply KD during supervised finetuning on an instructional dataset to approximate the benefits of pretraining with smoothed labels while ensuring computational efficiency. Specifically, we finetune student models on the \textit{Dolly} dataset \citep{DatabricksBlog2023DollyV2} using knowledge distillation from a larger teacher model. This setup enables us to investigate the impact of KD without requiring full-scale pretraining.

To systematically evaluate the impact of KD, we conduct experiments on three teacher-student model pairs from different families. For the LLaMA-2 series, we use LLaMA-2-7B-chat as the student and LLaMA-2-13B-chat as the teacher \citep{touvron2023llama}. Both sequence-level and word-level KD are applied in this setting, where \textit{Dolly} is augmented using greedy decoding from the teacher model. For the LLaMA-3.1 series, we use LLaMA-3.1-8B-Instruct as the student and LLaMA-3.1-70B-Instruct as the teacher \citep{dubey2024llama}, applying only word-level KD without additional data augmentation. Similarly, for the Qwen-2.5 series, Qwen-2.5-7B-Instruct serves as the student, and Qwen-2.5-32B-Instruct is the teacher \citep{bai2023qwen}, with word-level KD being applied.

Before distillation, each teacher model is first finetuned on \textit{Dolly} to ensure alignment with the dataset. For efficient finetuning of LLaMA-3.1-70B-Instruct, we adopt Low-Rank Adaptation \cite{dettmers2023qlora}. We explore various hyperparameter settings, including learning rates of $1e-5$ and $5e-6$, batch sizes of $2$, $4$, and $8$, and KD weight coefficients $\alpha$ of $0.01$, $0.1$, $1.0$, and $10.0$. All experiments are implemented using the MiniLLM framework \citep{gu2024minillm} and run on four NVIDIA H100 GPUs. Each training session takes approximately one hour.


As a baseline, we finetune the student models directly on \textit{Dolly} without KD, denoted as model-SFT. This allows us to assess the impact of KD by comparing distilled models with those trained solely on hard labels.

\subsection{Hallucination Evaluation}
\label{sec:hallu-eval}
Hallucination in language models can be broadly classified into two types \citep{huang2023survey}. Faithfulness hallucination occurs when a model generates outputs that are not grounded in the provided context, while factuality hallucination refers to errors where the generated content contradicts real-world knowledge stored in the model’s parametric memory. In this study, we focus on faithfulness hallucination, as it directly pertains to the model’s ability to generate contextually consistent outputs.

To evaluate faithfulness hallucination, we use \textit{lm-evaluation-harness} framework \cite{eval-harness}. We select two benchmark datasets from the allucination leaderboard \cite{hughes_vectara_2023} and integrate them into the harness. The first dataset, CNN/Daily Mail (CNNDM), consists of news articles from CNN and Daily Mail paired with multi-sentence summaries. The second dataset, XSUM, contains BBC news articles with highly abstractive single-sentence summaries. Both datasets are widely used to assess the faithfulness of model-generated summaries. To ensure a fair evaluation, we test models only on the test splits of each dataset, keeping the training and validation splits untouched.

For measuring faithfulness hallucination, we employ three metrics. ROUGE-L measures the n-gram overlap between the generated and reference summaries, serving as a traditional metric for summarization performance. Factual consistency, computed using the hallucination evaluation model from \citet{vectara2023}, assesses whether a generated summary is supported by the input article. Additionally, we adopt factual rate \cite{chuang2024lookback}, which determines whether a span of text is factual or hallucinatory based on the distribution of attention weights between the context and the generated text. For LLaMA-2, we use an off-the-shelf classifier from \citet{chuang2024lookback}. For LLaMA-3.1 and Qwen-2.5, we follow the same methodology to train separate classifiers. These classifier are then used to produce the factual rates based on generated attention weights.

\subsection{Results on Hallucination}
\label{sec:res_halu}

Table \ref{tab:hallu_perf} presents the hallucination evaluation results across different models and training approaches. The results demonstrate that in most cases models finetuned with KD outperform their SFT baselines across all model families, hallucination metrics, and both datasets. This confirms that training with soft labels from a teacher model significantly mitigates hallucination compared to training with hard labels.

It is important to highlight that \textbf{our models were not finetuned on the training splits of CNN/Daily Mail or XSUM}. Instead, finetuning was performed on an entirely different dataset, \textit{Dolly}, making our experimental setup different from models specifically optimized for these summarization benchmarks. Consequently, \textbf{our results may not match those reported on the hallucination leaderboard}. However, the goal of this work is not to optimize for leaderboard performance, but rather to investigate whether knowledge distillation can reduce hallucination in a general setting where models are trained on broad instructional data.

\begin{table*}[ht]
    \centering
\begin{tabular}{lcccc}
\hline
\textbf{} &
  \multicolumn{1}{l}{\textbf{Arc\_Challenge}} &
  \multicolumn{1}{l}{\textbf{Arc\_Easy}} &
  \multicolumn{1}{l}{\textbf{HellaSwag}} &
  \multicolumn{1}{l}{\textbf{OpenbookQA}} \\ \hline
\textbf{Llama-2-7B}   &                   &                   &                   &                   \\
\textbf{+SFT}         & 38.4±0.4          & 50.1±1.0          & 66.4±2.3          & \textbf{41.7±0.5} \\
\textbf{+KD$_{0.1}$}  & 39.5±0.4          & 50.2±1.9          & 66.8±1.1          & 40.8±0.7          \\
\textbf{+KD$_{1.0}$}  & \textbf{39.6±0.4} & 51.5±1.7          & \textbf{67.5±0.8} & 40.9±1.1          \\
\textbf{+KD$_{10.0}$} & 39.4±0.8          & \textbf{53.2±1.6} & 67.1±0.7          & 40.7±0.9          \\ \hline
\textbf{Llama3.1-8B}  &                   &                   &                   &                   \\
\textbf{+SFT}         & 57.1±0.5          & \textbf{82.4±0.3} & 78.7±0.9          & 49.6±0.2          \\
\textbf{+KD$_{0.01}$} & \textbf{57.5±0.4} & 82.1±1.0          & \textbf{78.8±1.0} & \textbf{49.9±0.7} \\
\textbf{+KD$_{0.1}$}  & 57.3±0.3          & 82.4±0.7          & 78.7±0.8          & 49.8±0.4          \\
\textbf{+KD$_{1.0}$}  & 56.2±0.3          & 82.4±0.5          & 77.6±0.2          & 49.1±0.9          \\ \hline
\textbf{Qwen2.5-7B}   &                   &                   &                   &                   \\
\textbf{+SFT}         & 50.3±3.3          & 67.6±6.0          & 74.8±2.6          & 48.4±1.1          \\
\textbf{+KD$_{0.01}$} & 50.7±3.5          & 66.6±4.8          & 74.7±2.9          & 48.5±1.8          \\
\textbf{+KD$_{0.1}$}  & 50.7±3.2          & 66.7±4.9          & 74.8±2.9          & \textbf{48.5±1.5} \\
\textbf{+KD$_{1.0}$}  & \textbf{50.8±3.4} & \textbf{69.1±6.3} & \textbf{74.8±2.3} & 47.9±0.9          \\ \hline
\end{tabular}
    \caption{
    Performance evaluation of student models finetuned with supervised finetuning (SFT) and knowledge distillation (KD) on general NLP benchmarks. The models are assessed on ARC (Challenge \& Easy), HellaSwag, and OpenBookQA using length-normalized accuracy (\%). Each experiment is conducted with varying learning rates and batch sizes, and results are presented as the mean and standard deviation. The findings indicate that KD does not degrade performance on general reasoning and comprehension tasks, suggesting that knowledge distillation effectively mitigates hallucination without compromising broader model capabilities.
    }
    \label{tab:general_perf}
\end{table*}


\begin{figure*}[ht]
  \centering
       \includegraphics[width=\textwidth]{imgs/hallu-examples.pdf}
  \caption{Comparison of summaries generated by the SFT and KD models.
  The SFT summary introduces hallucinated content (highlighted in red) that is factually incorrect or not present in the input context. In contrast, the KD summary remains faithful (highlighted in blue) to the provided input, accurately conveying key details without introducing unrelated or incorrect facts. This case study illustrates the effectiveness of knowledge distillation in mitigating hallucination and improving factual consistency.
  }
  \label{fig:examples}
\end{figure*}


A deeper analysis reveals that different hallucination metrics capture different aspects of model behavior. For example, when evaluating Llama-2 on XSUM, the KD-trained model outperforms the SFT model in ROUGE-L and factual consistency but performs slightly worse in factual rate. This discrepancy arises because factual rate was explicitly designed for hallucination detection and has been shown to generalize well to XSUM, even though it was trained on CNN/Daily Mail \cite{chuang2024lookback}. In contrast, ROUGE-L and factual consistency tend to emphasize surface-level text similarity rather than deep factual grounding. These findings underscore the importance of considering multiple evaluation metrics when analyzing hallucination tendencies.



\subsection{Results on General Benchmarks}
Since knowledge distillation alters the model’s training dynamics by encouraging smoother probability distributions, it is crucial to assess whether KD affects general model performance on broader NLP tasks. To address this, we evaluate student models on a range of reasoning, comprehension, and commonsense benchmarks. The selected datasets include ARC (Challenge \& Easy) \cite{allenai:arc} for commonsense reasoning, HellaSwag \cite{zellers2019hellaswag} for story completion, and OpenBookQA \cite{OpenBookQA2018} for science and reasoning tasks. Performance is measured using length-normalized accuracy.

Table \ref{tab:general_perf} presents the results on general benchmarks. Our findings indicate that KD does not degrade performance across these tasks. In most cases, KD-trained models match or outperform their SFT counterparts, demonstrating that distillation does not compromise the model’s reasoning or comprehension abilities. This result is particularly important, as it shows that reducing hallucination via KD does not come at the expense of broader model performance.

\subsection{Case Study}
To further illustrate the impact of KD on reducing hallucination, we present a case study comparing the SFT summary (from Qwen-2.5-SFT) and the KD summary (from Qwen-2.5-KD$_{0.1}$) for a given input context (from XSUM). The example, shown in Figure \ref{fig:examples}, highlights how KD helps generate more faithful and contextually grounded summaries.

The SFT summary contains several hallucinatory details that are unrelated to the given context. Specifically, it introduces factual errors by discussing the history of Sale Sharks, including information about its founding year, stadium, team colors, and past achievements—none of which appear in the provided context. This suggests that the model, when finetuned using hard labels, tends to over-rely on parametric knowledge rather than grounding its response in the input.

In contrast, the KD summary closely follows the input text, preserving key factual details while avoiding irrelevant or fabricated content. The summary correctly states that Josh Strauss has signed with Sale Sharks and retains the correct timeline and reasoning for his transfer. Importantly, it accurately conveys that Strauss has been sidelined by a kidney injury, a critical piece of information from the original context. Additionally, the KD summary introduces minor refinements, such as specifying a “two-year contract”, demonstrating that KD can smooth output distributions while maintaining informativeness.


\section{Related Work}

\paragraph{Hallucination mitigation}
Previous works have proposed various methods to reduce hallucinations. 
\citet{radford2019language} underscore the importance of rigorous curation and filtration of training data by human experts, which includes removing misinformation and biases, data deduplication, etc. Though effective, it is hard to scale up the filtering process as data volume expands. 
\citet{meng2022locating} later proposes a model editing technique that locates "buggy" parameters and updates them to alter the model's behavior, avoiding hallucinatory predictions, which also struggles with large scale updates. 
Other model updating techniques like factuality enhanced decoding that modifies model logits \cite{lee2022factuality} or the well studied retrieval-augmented generation (RAG) \cite{shuster2021retrieval, lewis2020retrieval, guu2020retrieval} where models retrieve relevant knowledge and give answer conditioned on that knowledge, have shown positive results and gained popularity. However, these are ad-hoc methods that do not directly deal with hallucination from the foundational level. 
Similar to our work, there are methods that focus on the training process of language models. 
For example, \citet{lee2022factuality} combats chunked factual knowledge in GPU constrained training environments using the prefix token TOPICPREFIX, \cite{liu2024exposing} that sharpens attentions weights to address attention glitches, etc. While improve the training paradigm fundamentally, they overlook the discussed flaws that hard labels impose on models.

\paragraph{Hallucination benchmarks}
A variety of benchmarks have been developed to evaluate hallucinations in LLMs \cite{tonmoy2024comprehensive}. Some examples of tasks-specific benchmarks used to determine LLM hallucinations are listed as the following.
\textbf{Summarization}: CNN-DM \cite{see-etal-2017-get}, MSMARCO \cite{msmarco}, and XSUM \cite{narayan-etal-2018-dont}.
\textbf{Open QA} : TruthfulQA \cite{lin2022truthfulqa}, FalseQA \cite{hu-etal-2023-wont}, and StrategyQA \cite{geva-etal-2021-aristotle}.
\textbf{Multi-choice QA}: MMLU \cite{mmlu}, WiCE \cite{kamoi-etal-2023-wice}, and FEVER \cite{thorne-etal-2018-fever}.
In order to maintain consistency in reporting hallucination mitigation performance, several leaderboards and benchmarks have been established which allow researchers to submit their models for evaluation \cite{hong2024hallucinations, hughes_vectara_2023, li2023halueval}.


\paragraph{Hallucination detection}
Traditional \textit{n-grams} metrics like \textbf{ROUGE-L} \cite{lin-2004-rouge} and \textit{classifier-based} metrics like \textbf{factual consistency} \cite{vectara2023} are commonly used to evaluate hallucinations. The former measures n-grams overlap among pairs of prediction and ground truth, and the latter is a T-5 based classification model that predicts whether a prediction is fully supported by a context. Nonetheless, these metrics might fall short in differentiating the subtle discrepancies between the generated content and the source content \cite{huang2023survey}, since they are limited to assessing only the generated text (hence \textit{external metrics}). Other methods operate on log-probabilities \cite{yuan2021bartscore, fu2023gptscore} and entropy \cite{xiao2021hallucination}, which can be viewed as internal metrics that process data at the last softmax stage in the transformer architecture. 
Recently \citet{chuang2024lookback} proposes Lookback-Lens classifier for hallucination detection, which predicts the level of factuality, i.e., \textbf{factual rate}, based on the ratio of attention weights given to context versus those given to newly generated tokens.
Factual rate is used in our work since it addresses two main downsides of mainstream metrics:
1) it examines internal states across all attention layers excluding non-linear transformations in forward layers, offering a new perspective to understand the intricate behaviors of LLMs.
2) grounded on the task of hallucination detection, factual rate gives a direct estimation of hallucination instead of being grounded on overlaps measure like ROUGE-L.


\paragraph{Knowledge distillation}
There are a wide range of distillation techniques, from distributions divergence to hidden states similarity \cite{xu2024survey}. Divergence-based methods minimize the divergence between the probability distributions of the teacher and student models. Similarity-based methods aim to align hidden states of the models, ensuring similar manner in processing information among the models. Since distributions divergence KD is very close to the analogy in $\S$\ref{sec:hard_labels}, we argue that divergence-based KD can address the shortcomings of hard labels and reduce hallucination in LLMs. 
In particular, our work concentrates on sequence and word-level KD \cite{kim2016sequence}, a form of divergence-based KD. Through word-level KD, student models learn from teachers' prediction at each timestep. Through sequence-level KD, students learn from teachers' prediction of sequences, which does not have a close-form cross-entropy representation like word-level KD. Instead, teacher-generated text used as labels in $\mathcal{L}_{\text{CE}}$ and $\mathcal{L}_{\text{KD}}$ approximately represent sequence-level distributions. Essentially, Equation (\ref{eq:loss-kd}), when applied with token labels generated by teachers, is equivalent to sequence and word-level combined KD. In contrast, when applied with the original labels from the training dataset, the paradigm reduces to word-level KD.
In terms of KD effectiveness, recent research also shows mixed results. For instance, \citet{wang2024experimental} finds that KD produces less capable models than SFT, while distillation pretraining has produced more capable models than supervised pretraining in Gemma and Gemini \cite{team2024gemma}, Minitron \cite{sreenivas2024llm}, and AFM \cite{gunter2024apple} families. This is further discussed in the recent work about distillation scaling laws \cite{wang2024experimental}, where, among other findings, it is better to choose a smaller teacher, slightly more capable than the target student capability, rather than a large, powerful teacher. This research is helpful in understanding any inconsistencies in our results and in designing optimal KD experiments in the future.


\section{Conclusions}
This paper explores knowledge distillation (KD) as a strategy for mitigating hallucination in large language models (LLMs) by replacing hard-label training with smoothed soft labels. We demonstrate that KD reduces overconfidence and improves factual grounding by enabling models to learn from a more calibrated probability distribution. Through experiments on multiple model families and summarization benchmarks, we show that KD-trained models exhibit lower hallucination rates compared to standard finetuning while maintaining strong general NLP performance. Our findings highlight the limitations of traditional hard-label supervision and emphasize the need for more uncertainty-aware training paradigms. Future work could explore adaptive KD strategies that dynamically adjust soft-label smoothing based on context sensitivity, integrate KD with retrieval-augmented generation (RAG) for further grounding, or extend these techniques to multimodal and domain-specific LLMs to improve factual accuracy across diverse applications.


\section*{Limitations}
\paragraph{Dependence on a well-calibrated teacher model}
The effectiveness of KD relies on the quality of the teacher model. If the teacher itself exhibits hallucination or poor factual calibration, the student model may inherit these weaknesses rather than mitigating them. While KD smooths token probabilities, it does not inherently improve the correctness of the teacher’s outputs. Future work could explore selecting or adapting teacher models with explicit hallucination mitigation techniques to ensure more reliable supervision.

\paragraph{KD in instruction finetuning}
To fully avoid assumption-prone behavior, KD should ideally be integrated into pretraining rather than applied only during finetuning. For example, Llama-3.2-1B was pretrained using logits from Llama-3.1-70B as word-level targets\footnote{https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/}, although the effects on hallucination have not been explicitly documented. Due to resource constraints, our experiments focused solely on instruction finetuning, meaning our results may not capture the full potential of KD in mitigating hallucination when used at scale during pretraining. Investigating how KD influences hallucination when applied earlier in the training pipeline remains an important direction for future research

\paragraph{Limited scope in hallucination categorization}
Our study specifically targets faithfulness hallucination, where the model generates content that is inconsistent with the provided context. However, factuality hallucination, where the generated text contradicts real-world knowledge, is another critical issue that we did not examine. Since different types of hallucinations require different mitigation strategies, future work should explore whether KD has similar benefits for factuality hallucination and how it compares to other debiasing techniques.

\paragraph{Computational cost of knowledge distillation}
Although KD is more computationally feasible than pretraining from scratch, it still introduces additional overhead compared to standard finetuning. Running teacher inference and student optimization increases resource demands, especially for large teacher models. Optimizing KD efficiency, such as distilling from smaller ensembles or using precomputed soft labels, could make this approach more practical for large-scale deployment.

\paragraph{Evaluation limitations and alternative metrics}
Our evaluation primarily relies on ROUGE-L, factual consistency, and factual rate, but other relevant metrics—such as METEOR, BERTScore, and SelfCheckGPT—were not considered. These alternative metrics could provide additional insights into hallucination tendencies, particularly for assessing deeper semantic alignment and self-consistency. Additionally, we did not incorporate human evaluation, which remains the gold standard for assessing hallucination, as it can capture nuanced errors that automated metrics


\paragraph{Multi-faceted nature of hallucination}
While our study focuses on overconfidence from hard labels, hallucination arises from a broader range of factors. Exposure bias—caused by the discrepancy between teacher-forced training and autoregressive inference—can lead to hallucination when the model generates sequences unobserved during training. Data imbalance can amplify hallucination in low-resource knowledge areas. The attend-to-all mechanism in transformers may dilute attention over longer sequences, degrading faithfulness. Additionally, models can exhibit inability to reject incorrect patterns, as seen in ChatGPT’s persistent success in Tic-Tac-Toe even when instructed to lose. Given the multifaceted nature of hallucination, our work addresses only one contributing factor. A more comprehensive mitigation strategy should integrate KD with other techniques, such as reinforcement learning from human feedback (RLHF), retrieval augmentation, and uncertainty-aware decoding.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{anthology,custom}
% \bibliographystyle{acl_natbib}

\appendix

\section{Overconfidence Evaluation}
\label{apx:overconf}

To justify the use of smoothed labels in reducing overconfidence, we first verify that LLMs are overconfident when finetuned with hard labels. 

In our experiments, four LLMs, including Mistral-7B, Llama2-7B, Pythia-6.9B \cite{refinedweb} and Falcon-7B are finetuned on the multiple choice QA dataset of CommonsenseQA \cite{talmor-etal-2019-commonsenseqa} using QLoRA \cite{dettmers2023qlora}. The finetuned models are evaluated on the validation set of CommonsenseQA with zero shot prompts. For a fair comparison, they are compared to vanilla (unfinetuned) models in a few-shot setting with instances from the training set as example shots.

To measure confidence, the negative log-likelihood (NLL) of incorrect answers are used. Specifically, when the model answers incorrectly, we extract from the first prediction step the NLL of its answer, which is either ``a'', ``A'', ``b'', ``B'', ``c'', ``C'', ``d'', ``D'', ``e'', or ``E''. The generated answers are also utilized to calculate the overall accuracy of these models.


\begin{table}[ht]
\centering
\begin{tabular}{ll}
\hline
Model           & acc  \\ \hline
Llama-2-7B      & 32.8 \\
Llama-2-7B-SFT  & 48.8 \\ \hline
Mistral-7B      & 70.0 \\
Mistral-7B-SFT  & 76.4 \\ \hline
Pythia-6.9B     & 20.6 \\
Pythia-6.9B-SFT & 19.4 \\ \hline
Falcon-7B       & 21.3 \\
Falcon-7B-SFT   & 20.4 \\ \hline
\end{tabular}
\caption{Accuracy of LLama-2-7B, Mistral-7B, Falcon-7B, and Pythia-6.9B on the validation set of CommonsenseQA.}
\label{tab:overconf}
\end{table}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\textwidth]{imgs/overconf.pdf}
    \caption{Kernel density estimation of confidence levels of incorrect answers in vanilla and finetuned (a) LLama-2-7B, (b) Mistral-7B, (c) Falcon-7B, (d) Pythia-6.9B, when evaluated on the validation set of CommonsenseQA. The confidence level is measured as the NLL.}
  \label{fig:overconf}
\end{figure*}

Figure \ref{fig:overconf} presents the overconfidence of vanilla and finetuned LLMs on their incorrect predictions, and table \ref{tab:overconf} shows their accuracy. %Ideally, the curves should be as flat (lesser incorrect answers is better) and as far to the left (incorrect answers should be given with low confidence) as possible. 
For all incorrect answers, the level of confidence is very high for all models, with the curves mostly leaning towards zero. After finetuning, Mistral and Falcon become more confident in their incorrect answers, which is evident by the height increase of the orange curves from the blue curves. Falcon and Pythia, on the other hand, do not seem to perform well on the multiple choice QA task, with their accuracy worsens after finetuning. These results indicate that finetuning with hard labels may improve accuracy in a particular task, but hardly reduce, or even raise their overconfidence. This necessitates the use of label smoothing in order to mitigate overconfidence and thus hallucination.


\section{Experiments on Small Scale LMs}
As support evidence, in addition to experiments with 7B and 8B models, we also evaluate small scale LMs from 350M to 1B parameters: Bloomz-560M and MT0-580M \cite{muennighoff2022crosslingual}, OPT-350M \cite{zhang2022opt}, and Pythia-1B \cite{biderman2023pythia}. We reuse models from \cite{crosstok}\footnote{https://huggingface.co/Nicolas-BZRD}, which are finetuned under SFT and KD (with Llama-2-7B and Mistral-7B teachers) on PubMedQA question-answering dataset \cite{jin2019pubmedqa} and DialogSum summarization dataset \cite{chen-etal-2021-dialogsum}. Evaluation benchmarks used include HotpotQA \cite{yang2018hotpotqa}, TruthfulQA \cite{lin2022truthfulqa} for factuality hallucination, and CNN/Daily Mail \cite{see-etal-2017-get} for faithfulness hallucination. Metrics used include ROUGE-L \cite{lin-2004-rouge}, CHRF \cite{popovic2015chrf}, BERTSCORE \cite{zhang2019bertscore}, and METEOR \cite{banerjee-lavie-2005-meteor}.


\begin{table*}[ht]
\small
\centering
\begin{tabular}{llllllllll}
\hline
\multirow{2}{*}{Student} & \multirow{2}{*}{Version} & \multicolumn{4}{c}{TruthfulQA} & \multicolumn{4}{c}{Hotpot QA} \\ \cline{3-10} 
                             &     & rougeL & CHRF & BertScore & METEOR & rougeL & CHRF & BertScore & METEOR \\ \hline
\multirow{2}{*}{Bloomz-560M} & SFT & 11.4   & 16.3 & 80.7      & 12.1   & 16.8   & 20.8 & 83.8      & 15.3   \\
                             & KD  & 13.2   & 17.0 & 81.5      & 13.1   & 19.2   & 23.3 & 85.1      & 16.7   \\ \hline
\multirow{2}{*}{mt0-580M}    & SFT & 32.8   & 41.9 & 88.2      & 38     & 5.2    & 13.8 & 80.8      & 9.7    \\
                             & KD  & 35.4   & 42.3 & 88.5      & 38.6   & 6.1    & 15.0 & 81.3      & 10.7   \\ \hline
\multirow{2}{*}{OPT-350M}    & SFT & 17.5   & 22.3 & 46.2      & 17.9   & 6.7    & 14.8 & 80.5      & 11.1   \\
                             & KD  & 16.6   & 20.4 & 37.8      & 16.6   & 6.9    & 15.4 & 80.9      & 11.2   \\ \hline
\multirow{2}{*}{Pythia-1B}   & SFT & 25.9   & 39.2 & 86.9      & 33.6   & 6.2    & 14.3 & 80.7      & 11.8   \\
                             & KD  & 27.8   & 41.1 & 87.2      & 36.2   & 7.4    & 16.2 & 81.2      & 12.9   \\ \hline
\end{tabular}
\caption{Hallucination evaluation results for smaller scale student models with supervised finetuning (SFT) and knowledge distillation (KD). Models are evaluated on Truthful QA and Hotpot QA for question answering.}
\label{tab:table-qa}
\end{table*}


\begin{table*}[ht]
\centering
\begin{tabular}{llllll}
\hline
\multirow{2}{*}{Student}     & \multirow{2}{*}{Version} & \multicolumn{4}{c}{CNNDM}          \\ \cline{3-6} 
                             &                          & rougeL & CHRF & BertScore & METEOR \\ \hline
\multirow{2}{*}{Bloomz-560M} & SFT                      & 20.4   & 33.2 & 85.8      & 25.6   \\
                             & KD                       & 20.8   & 33.6 & 85.9      & 26.1   \\ \hline
\multirow{2}{*}{mt0-580M}    & SFT                      & 21.9   & 35.0 & 85.6      & 27.7   \\
                             & KD                       & 21.7   & 33.9 & 85.6      & 26.4   \\ \hline
\multirow{2}{*}{OPT-350M}    & SFT                      & 23.1   & 35.4 & 86.3      & 28.4   \\
                             & KD                       & 23.5   & 35.7 & 86.4      & 28.9   \\ \hline
\multirow{2}{*}{Pythia-1B}   & SFT                      & 21.5   & 34.9 & 86.1      & 27.1   \\
                             & KD                       & 21.7   & 35.0 & 86.2      & 27.6   \\ \hline
\end{tabular}
\caption{
Hallucination evaluation results for smaller scale student models with supervised finetuning (SFT) and knowledge distillation (KD). Models are evaluated on CNN/Daily Mail for summarization.
}
\label{tab:table-summ}
\end{table*}

Table \ref{tab:table-qa} illustrates the performance of student models of Bloomz-560M, OPT-350M, mt0-580M, and Pythia-1B with Llama-2 teacher on Truthful QA and Hotpot QA. KD models consistently outperform their baseline counterparts, showing enhancements in all metrics, affirming their effectiveness in dealing with complex QA tasks. Likewise, Bloomz-560M, OPT-350M, and Pythia-1B demonstrate enhancements over the baselines on CNN/Daily Mail when employing KD with Mistral as the teacher for the summarization task (Table \ref{tab:table-summ}). However, the student model for MT0-base, exhibits a minor decline in performance compared to the base model on the same dataset. These improvements are consistent with those on larger scale LMs, consolidating our hypothesis.

\end{document}
