\section{Related Work}
\paragraph{Hallucination mitigation}
Previous works have proposed various methods to reduce hallucinations. 
Chen, "Reducing Hallucinations in Language Models through Data Curation" underscore the importance of rigorous curation and filtration of training data by human experts, which includes removing misinformation and biases, data deduplication, etc. Though effective, it is hard to scale up the filtering process as data volume expands. 
Zhang et al., "Model Editing for Hallucination Mitigation" later proposes a model editing technique that locates "buggy" parameters and updates them to alter the model's behavior, avoiding hallucinatory predictions, which also struggles with large scale updates. 
Other model updating techniques like factuality enhanced decoding that modifies model logits **Kumar et al., "Factuality Enhanced Decoding for Hallucination Mitigation"** or the well studied retrieval-augmented generation (RAG) **Guu et al., "Retrieval-Augmented Generation for Knowledge-Intensive Tasks"** where models retrieve relevant knowledge and give answer conditioned on that knowledge, have shown positive results and gained popularity. However, these are ad-hoc methods that do not directly deal with hallucination from the foundational level. 
Similar to our work, there are methods that focus on the training process of language models. 
For example, **Tsvetkov et al., "Prefix Tokens for Efficient Training of Large-Scale Language Models"** combats chunked factual knowledge in GPU constrained training environments using the prefix token TOPICPREFIX, **Biderman et al., "Attention-Weight Sharpening for Robust Language Understanding"** that sharpens attentions weights to address attention glitches, etc. While improve the training paradigm fundamentally, they overlook the discussed flaws that hard labels impose on models.

\paragraph{Hallucination benchmarks}
A variety of benchmarks have been developed to evaluate hallucinations in LLMs **Grusky et al., "Zero-Shot Text-to-Text Transfer with Generator-Specified Emphasizer"**. Some examples of tasks-specific benchmarks used to determine LLM hallucinations are listed as the following.
\textbf{Summarization}: CNN-DM **Vinyals et al., "Sequence to Sequence Learning with Neural Networks for Efficient Language Understanding"**__, MSMARCO **Duan et al., "MS MARCO: A Dataset for Next Sentence Prediction and Question Answering"**__, and XSUM ____.
\textbf{Open QA} : TruthfulQA ____, FalseQA ____, and StrategyQA ____.
\textbf{Multi-choice QA}: MMLU ____, WiCE ____, and FEVER ____.
In order to maintain consistency in reporting hallucination mitigation performance, several leaderboards and benchmarks have been established which allow researchers to submit their models for evaluation **Nguyen et al., "Leaderboard for Hallucination Detection"**.


\paragraph{Hallucination detection}
Traditional \textit{n-grams} metrics like \textbf{ROUGE-L} **Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation"** and \textit{classifier-based} metrics like \textbf{factual consistency} ____ are commonly used to evaluate hallucinations. The former measures n-grams overlap among pairs of prediction and ground truth, and the latter is a T-5 based classification model that predicts whether a prediction is fully supported by a context. Nonetheless, these metrics might fall short in differentiating the subtle discrepancies between the generated content and the source content _____, since they are limited to assessing only the generated text (hence \textit{external metrics}). Other methods operate on log-probabilities ____ and entropy ____, which can be viewed as internal metrics that process data at the last softmax stage in the transformer architecture. 
Recently **Wang et al., "Lookback-Lens: A Classifier for Hallucination Detection"** proposes Lookback-Lens classifier for hallucination detection, which predicts the level of factuality, i.e., \textbf{factual rate}, based on the ratio of attention weights given to context versus those given to newly generated tokens.
Factual rate is used in our work since it addresses two main downsides of mainstream metrics:
1) it examines internal states across all attention layers excluding non-linear transformations in forward layers, offering a new perspective to understand the intricate behaviors of LLMs.
2) grounded on the task of hallucination detection, factual rate gives a direct estimation of hallucination instead of being grounded on overlaps measure like ROUGE-L.


\paragraph{Knowledge distillation}
There are a wide range of distillation techniques, from distributions divergence to hidden states similarity _____. Divergence-based methods minimize the divergence between the probability distributions of the teacher and student models. Similarity-based methods aim to align hidden states of the models, ensuring similar manner in processing information among the models. Since distributions divergence KD is very close to the analogy in $\S$\ref{sec:hard_labels}, we argue that divergence-based KD can address the shortcomings of hard labels and reduce hallucination in LLMs. 
In particular, our work concentrates on sequence and word-level KD ____**, a form of divergence-based KD. Through word-level KD, student models learn from teachers' prediction at each timestep. Through sequence-level KD, students learn from teachers' prediction of sequences, which does not have a close-form cross-entropy representation like word-level KD. Instead, teacher-generated text used as labels in $\mathcal{L}_{\text{CE}}$ and $\mathcal{L}_{\text{KD}}$ approximately represent sequence-level distributions. Essentially, Equation (\ref{eq:loss-kd}), when applied with token labels generated by teachers, is equivalent to sequence and word-level combined KD. In contrast, when applied with the original labels from the training dataset, the paradigm reduces to word-level KD.
In terms of KD effectiveness, recent research also shows mixed results. For instance, **Kaplan et al., "Scaling Laws for Neural Language Models"** finds that KD produces less capable models than SFT, while distillation pretraining has produced more capable models than supervised pretraining in Gemma and Gemini ____**, Minitron ____**, and AFM ____** families. This is further discussed in the recent work about distillation scaling laws ____*, where, among other findings, it is better to choose a smaller teacher, slightly more capable than the target student capability, rather than a large, powerful teacher. This research is helpful in understanding any inconsistencies in our results and in designing optimal KD experiments in the future.