\section{Related Work}
\paragraph{Hallucination mitigation}
Previous works have proposed various methods to reduce hallucinations. 
\citet{radford2019language} underscore the importance of rigorous curation and filtration of training data by human experts, which includes removing misinformation and biases, data deduplication, etc. Though effective, it is hard to scale up the filtering process as data volume expands. 
\citet{meng2022locating} later proposes a model editing technique that locates "buggy" parameters and updates them to alter the model's behavior, avoiding hallucinatory predictions, which also struggles with large scale updates. 
Other model updating techniques like factuality enhanced decoding that modifies model logits \cite{lee2022factuality} or the well studied retrieval-augmented generation (RAG) \cite{shuster2021retrieval, lewis2020retrieval, guu2020retrieval} where models retrieve relevant knowledge and give answer conditioned on that knowledge, have shown positive results and gained popularity. However, these are ad-hoc methods that do not directly deal with hallucination from the foundational level. 
Similar to our work, there are methods that focus on the training process of language models. 
For example, \citet{lee2022factuality} combats chunked factual knowledge in GPU constrained training environments using the prefix token TOPICPREFIX, \cite{liu2024exposing} that sharpens attentions weights to address attention glitches, etc. While improve the training paradigm fundamentally, they overlook the discussed flaws that hard labels impose on models.

\paragraph{Hallucination benchmarks}
A variety of benchmarks have been developed to evaluate hallucinations in LLMs \cite{tonmoy2024comprehensive}. Some examples of tasks-specific benchmarks used to determine LLM hallucinations are listed as the following.
\textbf{Summarization}: CNN-DM \cite{see-etal-2017-get}, MSMARCO \cite{msmarco}, and XSUM \cite{narayan-etal-2018-dont}.
\textbf{Open QA} : TruthfulQA \cite{lin2022truthfulqa}, FalseQA \cite{hu-etal-2023-wont}, and StrategyQA \cite{geva-etal-2021-aristotle}.
\textbf{Multi-choice QA}: MMLU \cite{mmlu}, WiCE \cite{kamoi-etal-2023-wice}, and FEVER \cite{thorne-etal-2018-fever}.
In order to maintain consistency in reporting hallucination mitigation performance, several leaderboards and benchmarks have been established which allow researchers to submit their models for evaluation \cite{hong2024hallucinations, hughes_vectara_2023, li2023halueval}.


\paragraph{Hallucination detection}
Traditional \textit{n-grams} metrics like \textbf{ROUGE-L} \cite{lin-2004-rouge} and \textit{classifier-based} metrics like \textbf{factual consistency} \cite{vectara2023} are commonly used to evaluate hallucinations. The former measures n-grams overlap among pairs of prediction and ground truth, and the latter is a T-5 based classification model that predicts whether a prediction is fully supported by a context. Nonetheless, these metrics might fall short in differentiating the subtle discrepancies between the generated content and the source content \cite{huang2023survey}, since they are limited to assessing only the generated text (hence \textit{external metrics}). Other methods operate on log-probabilities \cite{yuan2021bartscore, fu2023gptscore} and entropy \cite{xiao2021hallucination}, which can be viewed as internal metrics that process data at the last softmax stage in the transformer architecture. 
Recently \citet{chuang2024lookback} proposes Lookback-Lens classifier for hallucination detection, which predicts the level of factuality, i.e., \textbf{factual rate}, based on the ratio of attention weights given to context versus those given to newly generated tokens.
Factual rate is used in our work since it addresses two main downsides of mainstream metrics:
1) it examines internal states across all attention layers excluding non-linear transformations in forward layers, offering a new perspective to understand the intricate behaviors of LLMs.
2) grounded on the task of hallucination detection, factual rate gives a direct estimation of hallucination instead of being grounded on overlaps measure like ROUGE-L.


\paragraph{Knowledge distillation}
There are a wide range of distillation techniques, from distributions divergence to hidden states similarity \cite{xu2024survey}. Divergence-based methods minimize the divergence between the probability distributions of the teacher and student models. Similarity-based methods aim to align hidden states of the models, ensuring similar manner in processing information among the models. Since distributions divergence KD is very close to the analogy in $\S$\ref{sec:hard_labels}, we argue that divergence-based KD can address the shortcomings of hard labels and reduce hallucination in LLMs. 
In particular, our work concentrates on sequence and word-level KD \cite{kim2016sequence}, a form of divergence-based KD. Through word-level KD, student models learn from teachers' prediction at each timestep. Through sequence-level KD, students learn from teachers' prediction of sequences, which does not have a close-form cross-entropy representation like word-level KD. Instead, teacher-generated text used as labels in $\mathcal{L}_{\text{CE}}$ and $\mathcal{L}_{\text{KD}}$ approximately represent sequence-level distributions. Essentially, Equation (\ref{eq:loss-kd}), when applied with token labels generated by teachers, is equivalent to sequence and word-level combined KD. In contrast, when applied with the original labels from the training dataset, the paradigm reduces to word-level KD.
In terms of KD effectiveness, recent research also shows mixed results. For instance, \citet{wang2024experimental} finds that KD produces less capable models than SFT, while distillation pretraining has produced more capable models than supervised pretraining in Gemma and Gemini \cite{team2024gemma}, Minitron \cite{sreenivas2024llm}, and AFM \cite{gunter2024apple} families. This is further discussed in the recent work about distillation scaling laws \cite{wang2024experimental}, where, among other findings, it is better to choose a smaller teacher, slightly more capable than the target student capability, rather than a large, powerful teacher. This research is helpful in understanding any inconsistencies in our results and in designing optimal KD experiments in the future.