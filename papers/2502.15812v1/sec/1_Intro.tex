\section{Introduction}
\label{sec:intro}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{file/first_3.png}
    
    \caption{Several examples from the InsightVision dataset. Chinese questions and answers have been translated into English.}
    \label{fig:example}
\end{figure}

In the domain of multimodal language models\cite{achiam2023gpt, li2024llava,you2023ferretrefergroundgranularity}, grasping the subtle meanings conveyed through visual cues—such as sarcasm, insult, or criticism—remains a substantial challenge. Understanding the nuanced implications of images is indicative of advanced human intelligence, serving as a vital bridge between perceptual and cognitive intelligence\cite{gordon2019intermodulation,DEWIT2012665}. Many images cannot be fully comprehended by merely examining their surface content; instead, a genuine understanding requires integrating background knowledge and symbolic cues to discern the true intentions of the image's creator\cite{garner1987metacognition,wang2024browseconcentratecomprehendingmultimodal}.

While visual perception entails transforming visual signals into insightful conclusions, such as profound image semantics or subtle narrative tones, existing evaluation benchmarks often fall short of assessing these deeper levels of understanding\cite{Goyal_2017_CVPR, Hiippala_2020}. These benchmarks primarily emphasize superficial tasks, such as image captioning, with datasets like COCO and ImageNet\cite{Hudson_2019_CVPR, cai2019multi}. Such efforts inadequately capture the intricacies of symbolic meanings and implicit interpretations. Furthermore, comprehensive visual perception demands both high- and low-level understanding, whereby humans employ commonsense knowledge to interpret broad concepts before honing in on the details\cite{wang2024browseconcentratecomprehendingmultimodal,chow2023travlritdontbimodal}. Current large vision-language models (LVLMs), however, often show limitations in articulating this hierarchical understanding.

To address these challenges and bridge the gaps in existing research, we introduce InsightVision, a comprehensive Chinese-based benchmark designed for nuanced, multi-level image evaluation. The InsightVision is systematically divided into four subtasks: surface-level content understanding, background knowledge comprehension, symbolic meaning interpretation, and implicit meaning comprehension. Unlike traditional datasets, it aims to provide a more thorough evaluation of multimodal language models' ability to grasp the deep semantics underlying images. The dataset comprises over 2,500 samples, each consisting of an image accompanied by questions spanning the four dimensions. Additionally, we have developed a semi-automatic pipeline to construct high-quality dataset. Utilizing InsightVision, we evaluate the implicit understanding capabilities of 15 open-source LVLMs and GPT-4o. Our assessment reveals a substantial gap between existing LVLMs and human performance in comprehending implicit meanings. For instance, even the best-performing model lags behind humans by nearly 14\% in terms of understanding implicit implications. These findings highlight the significant challenges in this domain and underscore the substantial opportunity for improvement in developing models capable of deeply understanding visual semantics. We have publicly released our annotations, code, and model results. We will publicly release our InsightVision dataset, code upon acceptance of the paper.

%第1、2段需要找参考文献
 