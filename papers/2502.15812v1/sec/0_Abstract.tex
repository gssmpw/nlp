\begin{abstract}
In the evolving landscape of multimodal language models, understanding the nuanced meanings conveyed through visual cues—such as satire, insult, or critique—remains a significant challenge. Existing evaluation benchmarks primarily focus on direct tasks like image captioning or are limited to a narrow set of categories, such as humor or satire, for deep semantic understanding. To address this gap, we introduce, for the first time, a comprehensive, multi-level Chinese-based benchmark designed specifically for evaluating the understanding of implicit meanings in images. This benchmark is systematically categorized into four subtasks: surface-level content understanding, symbolic meaning interpretation, background knowledge comprehension, and implicit meaning comprehension. We propose an innovative semi-automatic method for constructing datasets, adhering to established construction protocols. Using this benchmark, we evaluate 15 open-source large vision language models (LVLMs) and GPT-4o, revealing that even the best-performing model lags behind human performance by nearly 14\% in understanding implicit meaning. Our findings underscore the intrinsic challenges current LVLMs face in grasping nuanced visual semantics, highlighting significant opportunities for future research and development in this domain. We will publicly release our InsightVision dataset, code upon acceptance of the paper.
%Vision or Visual


% 我们第一个提出了中文的深度含义理解数据集

% 在不断发展的多模态语言模型领域中，理解通过视觉线索传达的细微意义（如讽刺、侮辱或批评）仍然是一个重大挑战。现有的评估基准主要关注图像描述等直接任务，如COCO和ImageNet等众多数据集。然而，这些基准在评估更深层次的隐含理解方面有所不足。为解决这一缺口，我们引入了一个全面的多级基准，系统地分为四个子任务：背景知识评估、基本内容理解、符号意义理解和隐含意义解释。我们提出了一种创新的方法，用于自动构建数据集，并遵循既定的构建协议。通过我们的基准，我们诊断了12个开源的大型视觉语言模型（LVLMs），发现即使是最先进的模型GPT-4o在隐含意义理解方面也仅达到55%的准确率。我们的研究结果强调了当前LVLMs在把握细微视觉语义方面面临的内在挑战，表明该领域未来还有很大的发展空间。
% 修改用词，隐晦含义
% 确定核心贡献和核心解决的问题。


% 参考论文
% 这三篇论文都关注于大规模视觉语言模型（LVLMs）和多模态语言模型（LMMs）在视觉感知和语义理解上的局限性，并通过不同的基准测试分析和评价这些模型的表现。下面是对每篇论文摘要的详细总结和分析：

% 第一篇论文
% 主题：多层次视觉感知

% 研究背景：人类拥有多层次的视觉感知能力，从低层次的物体识别到高层次的语义解释，如行为理解。
% 问题陈述：微小的低层次细节变化可能会导致高层次感知上的显著变化，例如将一个购物袋换成枪，会暗示暴力行为。尽管多模态任务取得了显著进展，LVLMs在执行这些多层次视觉感知的能力上仍未被深度探索。
% 贡献：引入MVP-Bench基准，用于系统评估LVLMs的低层和高层视觉感知能力，包括自然和合成图像。通过该基准，对12种LVLMs进行诊断，发现高层次感知任务对现有模型具有显著挑战。
% 实验结果：例如，GPT-4o在Yes/No问题上的准确率仅为56%，而低层次场景的准确率为74%。此外，自然图像和合成图像间的表现差距表明现有LVLMs在理解合成图像的视觉语义上没有像人类一样泛化。
% 第二篇论文
% 主题：理解人类幽默中的叙述对比

% 研究背景：虽然大规模多模态语言模型在多种任务中表现出显著能力，但在通过并置尤其是非线性叙述理解人类幽默方面仍存在困难。
% 问题陈述：通过矛盾叙述的漫画（每个漫画包含两个形成幽默矛盾的面板）来考察这一挑战。
% 贡献：引入了YESBUT基准，包含不同难度的任务，评估AI识别和解释这些漫画的能力，从字面内容理解到深层叙述推理。
% 实验结果： extensive analysis显示，即使是最先进的模型，在该任务上仍落后于人类表现。研究结果提供了关于AI在理解人类创造性表达上的当前局限性和潜在改进的洞见。

% 第三篇论文
% 主题：图像深层语义理解

% 研究背景：在社交媒体时代，理解图像的深层语义是至关重要的。然而，当前研究主要关注图像的表层描述，缺乏对深层语义的系统性研究。
% 贡献：引入DEEPEVAL，一个全面基准，评估LMMs在视觉深层语义上的能力。包括人工标注的数据集和三个递进的子任务：细粒度描述选择、深入标题匹配和深层语义理解。
% 实验结果：对9个开源LMMs和GPT-4V(ision)的评估显示，现有LMMs在深层语义理解能力上与人类相比存在显著差距。例如，GPT-4V在深层语义理解上落后于人类30%，尽管在图像描述中表现接近人类。进一步分析显示，LMM在DEEPEVAL上的表现随探索的深层语义具体面向而变化，表明在发展LMMs时仍存在根本性的挑战。
% 总体分析
% 这些论文揭示了当前LVLMs和LMMs在处理复杂视觉及语义任务上的不足，无论是在多层次感知、理解人类幽默，还是深层语义理解方面。每篇文章通过引入新的基准测试，采取系统的方法对模型性能进行评估，并揭示了人类与AI之间显著的性能差距，指出了未来改进研究的方向。这些研究不仅强调了现有模型的局限性，还为未来的大规模模型发展提供了参考和洞见。
\end{abstract}