\section{Related Work}
\label{sec:Related Work}
\subsection{Large vision language model}
Vision-language models\cite{li2023blip,li2024llava,bai2023qwen,lu2024deepseekvlrealworldvisionlanguageunderstanding, alayrac2022flamingo,sun2024generativemultimodalmodelsincontext}have achieved remarkable advancements within the realm of multimodal intelligence. By amalgamating large language models\cite{ray2023chatgpt,achiam2023gpt,anil2023palm,touvron2023llama2openfoundation,touvron2023llamaopenefficientfoundation} with visual content, LVLMs effectively manage intricate visual and linguistic inputs, thereby executing a variety of tasks ranging from visual description to logical reasoning. Flamingo\cite{alayrac2022flamingo} and OpenFlamingo\cite{awadalla2023openflamingoopensourceframeworktraining} models incorporate visual feature processing modules into the internal strata of language models using gated cross-attention, thereby propelling the profound integration of visual data within LLMs. CLIP\cite{radford2021learning,sun2023evaclipimprovedtrainingtechniques} utilizes contrastive learning to harmonize image and text modalities and is trained on extensive, noisy web-derived image-text pairs. By integrating modules such as QFormer\cite{li2023blip} and MLP\cite{liu2024visual}, previous works\cite{bai2023qwen, dai2023instructblipgeneralpurposevisionlanguagemodels,Liu_2024_CVPR} facilitate a collaborative comprehension between visual encoders and large language models (LLMs) of multimodal inputs. LLaVA\cite{liu2024visual} stands out for its pioneering use of GPT-generated instruction-following data to amplify LVLMs' responsiveness to visual instructions. A plethora of powerful LVLM APIs, including GPT-4o\cite{achiam2023gpt} and Qwen-VL-max\cite{bai2023qwen}, are now available. Through a rigorous evaluation of these models based on our proposed benchmark, we offer insightful perspectives into the ongoing research surrounding LVLMs.
\subsection{Vision Language Benchmarks} A rapidly expanding suite of multimodal benchmarks now rigorously evaluates the capabilities of LVLMs. Established benchmarks, including COCO Caption \cite{chen2015microsoftcococaptionsdata}, VQAv2 \cite{Goyal_2017_CVPR}, and GQA \cite{Hudson_2019_CVPR}, predominantly center on image description and question-answering tasks, employing metrics such as BLEU, CIDEr, and accuracy to gauge performance. Yet, as LVLMs advance, these traditional datasets have become insufficient for fully capturing the breadth of model capabilities. In response, researchers have developed more comprehensive evaluation frameworks that test a wider range of competencies, encompassing perceptual and cognitive skills \cite{fu2024mmecomprehensiveevaluationbenchmark}, spatial-temporal reasoning \cite{li2023seedbenchbenchmarkingmultimodalllms}, and relational understanding \cite{liu2025mmbench}. For instance, MMMU \cite{Yue_2024_CVPR} curates data from college-level textbooks and lecture materials, challenging models to demonstrate expertise across six academic disciplines. Similarly, CMMU \cite{he2024cmmubenchmarkchinesemultimodal} gathers questions from primary through high school curricula to assess foundational knowledge within the Chinese educational context. Nevertheless, these benchmarks largely remain focused on basic visual tasks, without adequately addressing the complexity of multimodal understanding. This paper introduces a benchmark tailored to evaluate deep semantic comprehension of images, specifically within a Chinese cultural framework.
\subsection{Image implicit meaning comprehension}
Image implicit meaning comprehension has become an important research focus for contemporary LVLMs, especially in handling images that convey complex emotions, cultural symbolism, and social critique. Existing evaluation datasets primarily test the models' linear visual reasoning abilities, such as visual question answering for surface-level content\cite{Hudson_2019_CVPR}. However, several works \cite{cai2019multi, machajdik2010affective} have demonstrated that LVLMs’ capabilities go beyond understanding surface-level meanings. Recent works\cite{yang2024largemultimodalmodelsuncover, liu2024iibenchimageimplicationunderstanding} highlight the limitations of current models when it comes to processing nonlinear narratives and understanding cultural contexts. For example, the most relevant prior work, DEEPEVAL\cite{yang2024largemultimodalmodelsuncover}, introduces three core tasks and shows that while the most advanced models achieve near-human performance on basic visual description tasks, they still perform poorly on tasks that involve understanding implicit semantics such as social background and satire. This paper provides a more comprehensive Chinese understanding benchmark, which, compared to the six categories in DeepEval, expands to include more thematic categories, with a total of 13 major categories and 41 subcategories (Figure \ref{fig:categories}), and offers more detailed testing across four dimensions of model performance.
% Image implicit meaning comprehension has emerged as a crucial research focus for contemporary LVLMs, particularly in handling images that convey nuanced emotions, cultural symbolism, and social critique. Achieving this level of comprehension demands that models infer implicit meanings from visual content, recognizing elements like satire, humor, and philosophical nuances. The most relevant prior work DEEPEVAL\cite{yang2024largemultimodalmodelsuncover} benchmark introduces three core tasks—fine-grained description selection. However, its limited categorization—comprising only six classes—restricts the scope of implicit meaning assessment, leaving out a broader range of complex visual semantics. 

% 2.1应该还没覆盖所有用到的模型；2.2需要补充点内容并且与2.3区分，2.3内容需要再调整
% 大型视觉语言模型(Flamingo, Blip2, Visual Instruction tuning,v Qwen-VL, LLaVA-next, DeeepSeekVL)近年来在多模态智能方面（Multimodal Intelligence）取得了显著进展。通过整合大规模语言模型（如GPTs*、LlaMa*、Palm2）和视觉内容（*）, LVLMs可以处理复杂的视觉和语言输入，实现从视觉描述到逻辑推理等多种任务。Flamingo、OpenFlamingo模型通过gated cross在语言模型的内部层次中嵌入视觉特征处理模块，推动了视觉信息在LLMs中的深度整合。CLIP模型使用对比学习实现图像和文本模态的统一，并使用大规模noisy web 图像-文本对进行训练。14, 15 16,  17,通过添加QFormer和MLP等模块使视觉编码器和大型语言模型（LLMs）能够协同理解多模态输入。LLaVA则开创了通过GPT生成的instruction-following data提升LLvMs对视觉指令的响应能力。同时包括很多强大的LVLMs API公开，包括（GPT-4v*、Qwen-VL-max*) 。通过对上述模型进行全面评估\subsection{Vision Language Benchmarks} 
%为了系统地评估视觉语言模型的能力，近年来涌现了许多多模态评估基准。传统的评估基准如 COCO Caption*、VQAv2* 和 GQA* 等，主要集中在图像描述和问答任务，通过BLEU、CIDEr、准确率 等客观指标来衡量模型的性能。然而随着LVLMs的进步，这些数据集的难度已经不足以评估LVLMs的能力。研究者们进一步提出了更为全面的基准测试框架，从感知和认知能力（MME）、spatial and temporal understanding（SEED Bench），到Relation Reasoning能力（MMBench）。MMU从大学教材、讲义中收集数据，要求模型具备大学级别六大领域的专业知识。类似的，CMMU收集了小学至高中的七大学科题目，以评估模型对中文基础学科知识的理解与应用。然而，这些基准仅限于对基础视觉任务的评估，未能充分评估模型在复杂多模态任务中的表现，因此本文旨在提出一个中文背景下的评估模型深度图像含义的Benchmark。
%深层语义理解是当前LVLMs的一个重要研究方向，特别是在处理具有复杂情感、文化隐喻和社会批判的图像时尤为重要。深层语义的理解需要模型具备从视觉内容中推理出隐含意义的能力，例如理解讽刺、幽默和哲学内涵。DEEPEVAL* 提出了三种任务：细粒度描述选择、深入标题匹配和深层语义理解，通过这些任务系统性地评估了 LVLMs 在理解深层视觉语义上的表现。例如，尽管 GPT-4V* 在基础的视觉描述任务上达到了接近人类的水平，但在涉及社会背景和讽刺的语义理解任务中，仍存在显著差距。此外，

%图像隐含意义理解已成为当代大规模多模态语言模型（LVLMs）研究的一个重要方向，特别是在处理传达复杂情感、文化符号和社会批评的图像时。现有的评估数据集主要测试模型的线性视觉推理能力，例如对于浅层内容的视觉问答（VQA），。然而Machajdik的工作也证明了LVLM的能力不止于理解浅层含义。然而最近的工作（如 MVP、DeepEval 和 YESBUT Benchmark、Ii-Bench）揭示了现有模型在处理非线性叙事和文化背景理解时的局限性。例如最相关的前期工作 DEEPEVAL 引入了三个核心任务，发现当前最先进的模型在基础视觉描述任务上已接近人类水平，但在涉及社会背景和讽刺等隐含语义理解的任务中，仍表现不佳。本文提供了一个更为完备的中文理解Benchmark，相较于 DeepEval 的六大类任务，扩展了更多的主题类别，共包含13大类和41小类，并从四个维度对大模型的性能进行了更为详细的测试。

