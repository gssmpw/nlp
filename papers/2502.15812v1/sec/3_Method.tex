\section{Dataset and task overview}
\label{sec:Method}


% \usepackage{tabularray}
\begin{table}
\centering
\begin{tblr}{
  cell{2}{2} = {c},
  cell{3}{2} = {c},
  cell{4}{2} = {c},
  cell{5}{2} = {c},
  cell{6}{2} = {c},
  hline{1,7} = {-}{0.08em},
  hline{2} = {-}{0.05em},
}
Image Amount                         & 2500  \\
QA Amount                           & 16220 \\
Surface-level Content Understanding & 5713  \\
Symbolic Meaning Interpretation     & 4649  \\
Background Knowledge Comprehension  & 3548  \\
Implicit Meaning Comprehension      & 2310  
\end{tblr}
\caption{Statistics of InsightVision dataset.}
\label{tab:dataset}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{file/overview_1.png}
    
    \caption{Data distribution of major categories and subcategories in InsightVision.}
    \label{fig:categories}
\end{figure}
% 这里需要加一个表1和图1

InsightVision, a comprehensive Chinese dataset, has been meticulously developed to assess the proficiency of LVLMs in deciphering nuanced and implicit meanings within visual content. This dataset encompasses 2,500 carefully curated samples, each comprising an image coupled with a set of choice questions. These questions are strategically designed to evaluate four distinct dimensions: surface-level content understanding, symbolic meaning interpretation, background knowledge comprehension, and implicit meaning comprehension.

The structure of InsightVision reflects the complex cognitive process involved in image interpretation, where models are required to first comprehend the surface visual content, then integrate extensive background knowledge and symbolic interpretations to ultimately infer the implicit meaning. To facilitate quantitative evaluation, we have crafted one or more single-choice questions for each dimension, testing the model's understanding across various levels of complexity. Each question presents an image, a query, and four answer options, with only one correct answer and three carefully designed distractors.

This holistic design in dataset construction allows for a robust evaluation of LVLMs' capabilities in processing visual information beyond mere surface-level recognition, delving into deeper levels of contextual and cultural understanding. Table \ref{tab:dataset} provides detailed textual statistics of the dataset, while Figure \ref{fig:example} illustrates some representative examples, demonstrating the dataset's comprehensive nature. %the comprehensive nature of the InsightVision dataset and its potential for advancing research in visual-language understanding within AI systems.

The four primary subtasks in our evaluation framework are:

\textbf{Surface-level content understanding}: This subtask assesses the model's ability to accurately identify and describe visual details present in the image. It serves as a foundation for more complex interpretations and ensures that the model can process basic visual information effectively.

\textbf{Symbolic meaning interpretation}. This subtask evaluates the model's capacity to understand the symbolic or metaphorical meanings conveyed by the image content. It tests the model's ability to move beyond literal interpretation and grasp deeper, culturally-informed meanings.

\textbf{Background knowledge comprehension}. This subtask evaluates the model's ability to leverage relevant background knowledge necessary for understanding the image content. It examines the model's capacity to integrate external information and context with visual cues.  %This subtask tests the model's ability to leverage relevant contextual and cultural knowledge to fully comprehend the image's content. It assesses whether the model can draw upon a broad knowledge base to enhance its understanding of the visual input.

\textbf{Implicit meaning comprehension}. The final subtask examines the model's proficiency in grasping the overall implicit message or subtle connotations conveyed by the image. This challenges the model to synthesize information from multiple sources and levels of interpretation to arrive at a holistic understanding.

The rationale for selecting these four tasks is to provide a comprehensive assessment of LVLMs' strengths and weaknesses in interpreting implicit visual meanings. This approach evaluates models across a range of cognitive processes, from basic perception to high-level reasoning and cultural understanding. By structuring the evaluation this way, we gain insights into how well LVLMs mimic human understanding of complex visual stimuli, identify areas for improvement, and guide future research in developing more sophisticated multimodal AI systems capable of nuanced interpretation.


\section{Dataset construction}
Constructing datasets that cover a broad range of knowledge typically requires highly educated annotators, but this approach is time-consuming and costly. To address these challenges, we developed a semi-automatic pipeline for creating the InsightVision dataset, focused on images with implicit meanings. The pipeline includes the following steps (as shown in Figure \ref{fig:main}): 1) Image collection, 2) Data annotation, 3) Keypoint extraction, 4) Question and option generation, and 5) Quality control.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{file/main_3.png}
    
    \caption{InsightVision four-stage construction pipeline. Stage 1 involves data collection and pre-annotation using GPT-4o to generate rich descriptions. Stage 2 conducts keypoint extraction, categorizing information into surface-level content, symbolic meaning, background knowledge, and implicit meaning. Stage 3 utilizes Qwen2-72B for options generation. Finally, Stage 4 applies QA filtering, including consistency checks, difficulty control, and human evaluation, to ensure high-quality, multi-layered annotations.}
    \label{fig:main}
\end{figure*}
%%% 注意需要添加附录表格。
\subsection{Image collection}
The InsightVision dataset was constructed through a comprehensive web crawling process. We systematically collected approximately 100,000 images from Cartoon Movement\cite{cartoonmovement}, a reputable online platform for editorial cartoons and comics. Each image was accompanied by its associated metadata, including titles, detailed textual descriptions, and relevant keywords. Following the collection phase, we conducted a manual curation process to eliminate duplicates and images lacking implicit meanings. Unlike previous studies, which typically categorize images into a limited set of themes such as humor or satire, we aimed to design a more comprehensive classification system. Therefore, we developed a hierarchical classification system to categorize the curated images based on their primary thematic content. This classification resulted in 13 major categories, including, but not limited to: Arts and Cultural Expression, Economic Development, Social and Cultural Issues, Politics and Power Dynamics, Health and Safety Concerns, and more. These major categories were further subdivided into 41 specific subcategories, providing a granular approach to image classification (Figure \ref{fig:categories}). From these categories, we selected 2,500 images to proceed to the next phase of annotation tasks. We have included a comprehensive list of all categories and subcategories, along with detailed explanations for each, in the Appendix A of this paper.

% This methodology allowed us to construct a diverse and comprehensive dataset that captures a wide range of implicit visual meanings across various socially relevant topics. Our approach combines automated data collection with human-in-the-loop refinement to ensure both scalability and quality in the resulting dataset. %这段看情况删除

\subsection{Data pre-annotation}
To obtain high-quality image annotation data, we implemented a novel approach combining LVLM pre-annotation with human expert verification. This method ensures comprehensive and accurate image understanding, encompassing both explicit visual content and implicit meanings.

\textbf{Pre-annotation model and human annotator selection.} After extensive comparative analysis, we identified GPT-4o as the optimal pre-annotation model. GPT-4o demonstrated superior performance in interpreting nuanced image meanings when provided with textual prompts. To maintain annotation quality, we employed a dual-review process involving two postgraduate-level experts independently verifying each pre-annotation, thus minimizing potential biases and errors.

\textbf{Comprehensive image description generation.} To generate high-quality image understanding data encompassing surface-level content, background knowledge, symbolic meanings, and implicit connotations, we input the crawled images along with their corresponding titles, textual descriptions, and keywords into GPT-4o. Guided by these textual prompts, we instruct GPT-4o to provide a comprehensive description of the image, including: a) Detailed surface-level visual content; b) Implicit meanings and connotations;  c) Requisite background knowledge for understanding these implicit meanings; d) Explanation of symbolic representations and connotations. This approach results in high-quality image-description pairs, each containing a rich, multi-layered interpretation of the visual content. 

\subsection{Keypoint extraction}
After providing a complete description for each image, we extracted key points corresponding to four distinct tasks from these complete descriptions. Each task is exemplified in the Keypoint Extraction box shown in Figure \ref{fig:main}.

% a) Key point of surface-level content include descriptive elements such as "A man using a paintbrush to draw a set of red concentric circles on a wall"; b) Symbolic meaning involves identifying symbolic elements such as "The red paint and target circles symbolize goals and achievements"; c) Background knowledge tasks require the integration of relevant contextual information. For instance, "hindsight bias" refers to the tendency to adjust criteria retrospectively to make outcomes appear successful after they have occurred; d) Implicit meaning focuses on extracting the underlying message, such as "the image criticizes the manipulation of facts and standards to present a facade of success, encouraging the pursuit of genuine goals and accomplishments".

\subsection{Questions and options generation}
After obtaining image annotations, we utilize the annotated keypoints to generate questions and four answer options. Due to the high manual cost, we utilize the complete image descriptions from Section 4.2 and Qwen2-72B to assist in generating questions and options. Qwen2-72B, with 72 billion parameters, is chosen for its capability in natural language generation.

For surface-level understanding, symbolic meaning comprehension, and background knowledge tasks, multiple questions are generated based on keypoints, each with four answer choices. Detailed prompts and examples are provided in Appendix B. For implicit meaning understanding, which primarily evaluates the model’s ability to grasp implicit meanings through reasoning that involves surface-level content, background knowledge, and symbolic interpretation, the answer options tend to be lengthier. As Qwen2-72B's generated questions and answers often diverge excessively, we employ the quality assessment pipeline described in Section 4.5 to enhance the quality of the model-generated questions.


\subsection{Dataset quality}

To ensure dataset quality, we developed a comprehensive set of quality generation criteria and filtering procedures (detailed prompts are provided in the Appendix C).

\textbf{Generation criteria}:

\textbf{1. Consistency.} All options should have roughly the same word count, avoiding obvious length discrepancies. Ensure all options maintain consistency in tone, professionalism, and vocabulary style to prevent the correct answer from being identified through stylistic differences.

\textbf{2. Distractibility.} Wrong options should be designed to be misleading and seemingly reasonable, making them difficult to eliminate by common sense alone. Ensure incorrect options have a certain persuasiveness, rather than being mere assumptions or obvious errors.

\textbf{3. Avoiding image element misguiding.} Ensure that any image elements mentioned in the options match or are similar to the actual content, avoiding easy elimination of incorrect options due to incorrect image details.

\textbf{4. Preventing keyword and pattern recognition.} Avoid obvious keyword matches between the question and options to prevent easy inference.

\textbf{5. Unique correct answer.} Ensure only one correct answer, avoiding ambiguity and ensuring clarity in each option.

\textbf{6. Core assessment.} The design of the question and answer must focus on the key information in the assessment point, which refers to the information related to understanding the deeper meaning.

\textbf{Filtering procedures:}

\textbf{1. Initial filtering.} We employ Qwen2-72B to verify whether generated questions and options fully comply with the six criteria across different understanding levels (surface content, symbolic meaning, background knowledge, and implicit meaning). Questions meeting all criteria are retained; others are regenerated.

\textbf{2. Advanced filtering.} Research suggests that some benchmarks are less reliant on visual input.\cite{tong2024eyes} To ensure true visual dependency and avoid reliance on keyword or pattern recognition, we developed an innovative screening method. Questions are initially input to Qwen2-72B without accompanying images. If the model answers correctly without visual context, the question is discarded and regenerated until it genuinely requires visual input.
%To adhere to preventing keyword and pattern recognition, based on the prior information that shallow content understanding and implicit meaning understanding require visual input for accurate answers, we designed an innovative screening method. 

\textbf{3. Difficulty control.} 
We implemented a model voting system using 16 different models to evaluate question difficulty. The difficulty of each question is determined by the proportion of models that answer it correctly. Questions are categorized based on their correct rate (e.g., 100\% correct rate is classified as easy, 10\% as difficult) and are equally distributed across difficulty levels in the final dataset, excluding the easy level.


\textbf{4. Human evaluation.} Final quality assurance involves a three-person voting system, wherein questions are retained only if all three annotators unanimously agree on their validity and appropriateness. In our study, we recruited a total of nine annotators, who were grouped into teams of three to annotate the same set of questions. The educational backgrounds of the annotators were diverse, comprising two with undergraduate degrees and seven with associate degrees. The questions were broken down into highly specific components, so a high level of academic qualification was not required for annotation.

This methodology ensures a high-quality, visually-dependent dataset with controlled difficulty levels and verified accuracy. Regarding the quality assessment of automatically generated questions, the error rate for pre-annotation using GPT-4 (image pre-annotation) was found to be 2\%, while the final error rate for the generated questions was 5\%. These results suggest that the quality of the automatically generated questions was generally high, demonstrating the effectiveness of the automated process.

\subsection{License and copyright}
% In this dataset, we used the original web links of comic images without infringing their copyrights. Our annotators voluntarily participated in the annotation process and were fairly compensated.
\textbf{Ethics Statement:}
All data samples for this project are sourced from publicly accessible content on social media platforms. To ensure copyright compliance, we use direct links to the original comics to avoid any infringement. Our annotated benchmark will be open-sourced, with links provided for each comic image. We carefully review samples to exclude any content that might be offensive or harmful.
\textit{Additionally, we have obtained permission from the creators to use these public images within our benchmark.}
\newline\textbf{Data Annotation:} Our annotators voluntarily participated in the annotation process and were fairly compensated.


