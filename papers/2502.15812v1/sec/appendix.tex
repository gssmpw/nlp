\appendix

\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\section{Categories Definition}
The hierarchical classification system mentioned in Section 4.1 is detailed as follows. We first instruct GPT-4o to output the potential categories and corresponding subcategories for the comic images. Then, we provide the collected 100,000 comic images to GPT-4o to classify them into the newly formed categories. A significant portion of the images will not find a corresponding category. We then instruct GPT-4o to complete the classification based on the remaining images, and reclassify the comic images. This process is repeated until all images are classified. The final result is shown in Table \ref{tab:category}, which includes 13 categories, 41 subcategories, and their corresponding specific definitions.

\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tblr}
{
  row{1} = {c},
  cell{2}{1} = {r=6}{c},
  cell{2}{2} = {c},
  cell{3}{2} = {c},
  cell{4}{2} = {c},
  cell{5}{2} = {c},
  cell{6}{2} = {c},
  cell{7}{2} = {c},
  cell{8}{1} = {r=4}{c},
  cell{8}{2} = {c},
  cell{9}{2} = {c},
  cell{10}{2} = {c},
  cell{11}{2} = {c},
  cell{12}{1} = {r=2}{c},
  cell{12}{2} = {c},
  cell{13}{2} = {c},
  cell{14}{1} = {r=2}{c},
  cell{14}{2} = {c},
  cell{15}{2} = {c},
  cell{16}{1} = {r=3}{c},
  cell{16}{2} = {c},
  cell{17}{2} = {c},
  cell{18}{2} = {c},
  cell{19}{1} = {r=3}{c},
  cell{19}{2} = {c},
  cell{20}{2} = {c},
  cell{21}{2} = {c},
  cell{22}{1} = {r=3}{c},
  cell{22}{2} = {c},
  cell{23}{2} = {c},
  cell{24}{2} = {c},
  cell{25}{1} = {r=4}{c},
  cell{25}{2} = {c},
  cell{26}{2} = {c},
  cell{27}{2} = {c},
  cell{28}{2} = {c},
  cell{29}{1} = {r=4}{c},
  cell{29}{2} = {c},
  cell{30}{2} = {c},
  cell{31}{2} = {c},
  cell{32}{2} = {c},
  cell{33}{1} = {r=2}{c},
  cell{33}{2} = {c},
  cell{34}{2} = {c},
  cell{35}{1} = {r=3}{c},
  cell{35}{2} = {c},
  cell{36}{2} = {c},
  cell{37}{2} = {c},
  cell{38}{1} = {r=3}{c},
  cell{38}{2} = {c},
  cell{39}{2} = {c},
  cell{40}{2} = {c},
  cell{41}{1} = {r=2}{c},
  cell{41}{2} = {c},
  cell{42}{2} = {c},
  vline{2-3} = {1-42}{0.05em},
  vline{3} = {3-7,9-11,13,15,17-18,20-21,23-24,26-28,30-32,34,36-37,39-40,42}{0.05em},
  hline{1,43} = {-}{0.08em},
  hline{2,8,12,14,16,19,22,25,29,33,35,38,41} = {-}{0.05em},
  hline{3-7,9-11,13,15,17-18,20-21,23-24,26-28,30-32,34,36-37,39-40,42} = {2-3}{dashed},
}
\textbf{Category} & \textbf{Subcategory} & \textbf{Definition}\\
Politics and Power & Political Games & Policy disputes, party struggles, concentration of power.\\
 & Political Corruption & Corruption, abuse of power, electoral fraud.\\
 & Political Figures & Behaviors of leaders, public images, personal scandals.\\
 & National Situation & {International relations, national divisions, territorial disputes, independence movements, ethnic \\conflicts.}\\
 & National Symbols and Dignity & Actions like damaging the national flag, emblem, or offending national symbols.\\
 & Freedom of Speech and Media & Issues related to freedom of speech, press freedom, censorship, and information control.\\
Society and Culture & Social Phenomena & {Racism and sexism, consumerism, celebrity scandals, cults, extremist religious groups, celebrity \\worship, superheroes as cultural symbols, conflicts in sports competitions.}\\
 & Cultural Phenomena & Modern lifestyles, technological dependency, pop culture, and media commentary.\\
 & Social Inequality & Wealth gap, labor rights, social stratification.\\
 & Protection of Minors & Issues like harmful animations, violence, and soft-pornography involving minors.\\
Economy and Development & Economic Issues & Economic crises, wealth inequality, impacts of globalization.\\
 & Technological Development & {Privacy concerns, tech monopolies, ethics in technology, future technologies, cybersecurity, tech \\and space exploration, innovation.}\\
History and Education & Historical Events & Wars, revolutions, significant historical events.\\
 & Educational Issues & Education equity, academic misconduct, reforms, and pressures.\\
Daily Life & Family Relationships & Family conflicts, generational differences, marriage issues, family humor, and reconciliation.\\
 & Work Environment & Workplace issues, corporate culture, job stress.\\
 & Leisure and Celebrations & Festivals, celebrations, summer leisure, and reading.\\
Health and Safety & Public Health & Pandemics, healthcare systems, vaccinations.\\
 & Food Safety & GMOs, food additives.\\
 & Mental Health & Issues like suicide, self-harm, depression, and anxiety.\\
Morality and Ethics & Social Morality & Hypocrisy, greed, selfishness.\\
 & Sex and Morality & Sexual behaviors, innuendos, gender discrimination, sex scandals.\\
 & Tech Ethics & Artificial intelligence, genetic editing, comparison between science and pseudoscience.\\
Environmental Protection & Environmental Pollution & Air, water, and soil pollution.\\
 & Ecological Damage & Deforestation, ocean pollution, loss of biodiversity.\\
 & Climate Change & Global warming, extreme weather events.\\
 & Sustainable Development & Resource management, green technologies, environmental policies.\\
Arts and Culture & Artistic Creation and Expression & Artistic techniques, symbolic meanings, art and technology, visual language of art and symbols.\\
 & Art and Philosophy & Surrealism, philosophy and art, existentialism in art.\\
 & Art and Culture & Modern art, geometric abstraction, cultural commemorations, semiotics in art.\\
 & Art and Entertainment & Music, films, games as forms of celebration and artistic entertainment.\\
Sports and Competition & Sports Events & Sports competitions, achievements, and glory.\\
 & Sports and Culture & Team spirit, artistic and entertaining aspects of sports.\\
Science and Exploration & Scientific Research & Unknowns of scientific exploration, satire on pseudoscience, cognition and science.\\
 & Exploration and Mysteries & {Adventures, harmony of nature and urban civilizations, space exploration, and international coop-\\eration.}\\
 & Philosophy and Science & Philosophy of time, cosmology, intersections of science and philosophy.\\
Philosophy and Life~ & Existence and Reflection & Wisdom and loneliness, symbols of creativity, existentialism.\\
 & Psychology and Emotion & Emotions and remembrance, perseverance in adversity, happiness and contentment.\\
 & Time and Life & Time and life, philosophy of time, time management and psychological adaptation.\\
Personal Grow & Personal Growth & Challenges, effort and success, self-care.\\
 & Creativity and Inspiration & Creative thinking, capturing inspiration, art and creativity, overcoming challenges with wisdom.
\end{tblr}
}
\caption{The names and detailed definitions of the categories and subcategories in InsightVision}
\label{tab:category}
\end{table*}


\section{Prompt}
The \{Comprehensive image description\} in the prompt refers to the process in Section 4.2 where we instruct GPT-4o to provide a comprehensive description of the image, including:a) Detailed surface-level visual content;b) Implicit meanings and connotations;c) Requisite background knowledge for understanding these implicit meanings; d) Explanation of symbolic representations and connotations
\subsection{Implicit meaning summarization}
\label{subsection:B.1}

\begin{figure}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1.0 \linewidth]{file/example1.png}
   \caption{Example of Implicit meaning description and Key point extraction}
   \label{fig:example1}
\end{figure}
To enable the LLM to summarize the implicit meaning of an image, we use the following prompt (originally in Chinese, but shown here in English).The example of output is illustrated in Figure \ref{fig:example1}.
\begin{tcolorbox}[colback=gray!5,colframe=gray!80, breakable,title=Prompt]
\textbf{\# Task Description}
\newline You are a master of understanding the implicit meaning of images and need to accurately summarize the profound implications of the input image.
\newline \textbf{\# Specific Requirements}
\newline Based on the elements and related details in the image, identify and accurately summarize the deep meaning. Output only the summary without any additional symbols.
\newline \textbf{\# Image Content }(Although you cannot see the image, I will describe it in the following text)
\newline \{Comprehensive image description\}
\end{tcolorbox}

\subsection{Key point extraction}
\label{subsection:B.2}
Based on the implicit meaning concluded in \ref{subsection:B.1} (referred to as \{Implicit meaning\} in the prompt) , we extract key point by the following prompt(originally in Chinese, but shown here in English).The model's output example is illustrated in the lower part of Figure \ref{fig:example1}.

\begin{tcolorbox}[colback=gray!5,colframe=gray!80, breakable,title=Prompt]
\textbf{\# Task Description}
\newline You are a master of logical reasoning and can infer the deep meaning of an image based on its content. Now, given the image content and the deep meaning, analyze step by step to identify the key elements needed to infer the deep meaning from the image content.
\newline\textbf{\# Specific Steps}
\newline  Follow these steps for the analysis:
\newline [Surface-level Content Understanding] Understand the image content that is necessary to grasp the deep meaning, mainly including the facts depicted in the image.
\newline [Symbolic Meaning Interpretation] Understand the symbolic, implicit, metaphorical, suggestive, or potential meanings that are abstract and related to the deep meaning of the image.
\newline [Background Knowledge Comprehension] Identify the specific historical knowledge or relevant common sense required to understand the deep meaning of the image, without abstract concepts.
\newline [Implicit Meaning Comprehension] Summarize the deep meaning of the image in a short phrase or sentence.
\newline \textbf{\# Output Format}
\newline[Surface-level Content Understanding] 1.xxx; 2.xxx; ...
\newline[Symbolic Meaning Interpretation] 1.xxx; 2.xxx; ...
\newline[Background Knowledge Comprehension] 1.xxx; 2.xxx; ...
\newline[Implicit Meaning Comprehension] xxx
\newline \textbf{ \# Image Content}(Although you cannot see the image, I will describe it in the following text)
\newline \{Comprehensive image description\}
\newline \textbf{\# Deep Meaning}
\newline \{Implicit meaning\}

\end{tcolorbox}

\subsection{QA Generation}
\label{subsection:B.3}
To generate high-quality QA, we explicitly inform the LLM of six requirements in the prompt and specify the analysis steps and output format. The complete prompt is shown below(originally in Chinese, but shown here in English), and the corresponding output is illustrated in Figure \ref{fig:example2}.
The \{Key points\} in the prompt refers to the output of section \ref{subsection:B.2} with examples shown in the lower part of Figure \ref{fig:example1}.The Implicit Meaning in the prompt refers to the output from B.1, as shown in the upper part of Figure \ref{fig:example1}.
\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.9 \linewidth]{file/example2.png}
   \caption{Examples of QA Generation}
   \label{fig:example2}
\end{figure*}

\begin{tcolorbox}[colback=gray!5,colframe=gray!80, breakable,title=Prompt]
\textbf{\# Task Description}
\newline You are an evaluation master, skilled in designing questions based on image content and assessment points, specifically to test others' understanding of the deeper meaning of images.
\newline\textbf{\# Requirements }(All questions and options must meet the following requirements)
\newline These requirements ensure that the designed questions and options can effectively assess whether others understand the key points without being influenced by formal differences.
\newline[Consistency] Ensure that the four options under the same question are approximately the same length to avoid obvious differences in length. Maintain a consistent tone and style across the four options, ensuring similar word choices to prevent identifying the correct option through stylistic differences.
\newline[Distractibility] Wrong options should be confusing and seemingly reasonable, making them not easily ruled out by common sense. Ensure that wrong options are somewhat persuasive, not just hypothetical or obviously incorrect.
\newline[Avoiding Image Element Misguiding] Ensure that the image elements mentioned in the four options match or are similar to the actual content, to avoid easily ruling out wrong options due to incorrect image details.
\newline[Preventing Keyword and Pattern Recognition] Avoid obvious keyword matches between questions and options. Ensure there is no direct verbal association between the question and the correct option to prevent easy inference.
\newline[Unique Correct Option] Among the four options, ensure that only one is the correct option. Avoid ambiguity or vagueness, allowing each option to have a clear judgment.
\newline[Core Assessment] The design of questions and options must be based on the "key points."
\newline\textbf{\# Specific Steps}
\newline Analyze according to the following steps:
\newline[Surface-level Content Understanding] To understand the deeper meaning of the image, extract key points from each image content understanding assessment point. Design a question and four options for each assessment point, with only one correct option per question.
\newline[Symbolic Meaning Interpretation] To understand the deeper meaning of the image, extract key points from each symbolic meaning understanding assessment point. Design a question and four options for each assessment point, with only one correct option per question.
\newline[Background Knowledge Comprehension] To understand the deeper meaning of the image, extract key points from each background knowledge involvement assessment point. Design a question and four options for each assessment point, with only one correct option per question.
\newline[Implicit Meaning Comprehension] Design a question and four options based on the deep meaning assessment point of the image, with only one correct option.
\newline\textbf{\# Output Format} (Please strictly follow the format below)
\newline\{ "Surface-level Content Understanding":
\newline[\{"Question":"xxx","A":"xxx","B":"xxx","C":"xxx",\newline"D":"xxx","Correct Option":"x"\}, \{...\}], 
\newline"Symbolic Meaning Interpretation":
\newline[\{"Question":"xxx","A":"xxx","B":"xxx","C":"xxx",\newline"D":"xxx","Correct Option":"x"\}, \{...\}], 
\newline"Background Knowledge Comprehension":
\newline[\{"Question":"xxx","A":"xxx","B":"xxx","C":"xxx",\newline"D":"xxx","Correct Option":"x"\}, \{...\}], 
\newline"Implicit Meaning Comprehension":
\newline[\{"Question":"xxx","A":"xxx","B":"xxx","C":"xxx",\newline"D":"xxx","Correct Option":"x"\}] \}
\newline \textbf{\#Image Content} (although you cannot see the image, I will describe the image with the following text)
\newline \{Comprehensive image description\}
\newline \textbf{\# Assessment Points List}
\newline \{Key points\}
\newline \textbf{\# Deep Meaning}
\newline \{Implicit meaning\}
\end{tcolorbox}

\section{QA Filtering}
\subsection{Initial screening}
\label{subsection:C.1}
\begin{figure}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1.0 \linewidth]{file/example3.png}
   \caption{Example of Initial screening}
   \label{fig:example3}
\end{figure}
For the QA generated according to the specified requirements in \ref{subsection:B.3}, we need to verify whether the output truly meets the six requirements. Therefore, we further perform a quality assessment of the Q\&A. The specific prompt is shown below(originally in Chinese, but shown here in English), and an example of the model's output is illustrated in Figure \ref{fig:example3}.
The \{Key points\} in the prompt refers to the output of section \ref{subsection:B.2} with examples shown in Figure \ref{fig:example1}. The {Questions and answers} in the prompt refer to the output of Section \ref{subsection:B.3}, with examples shown in Figure \ref{fig:example2}.

\begin{tcolorbox}[colback=gray!5,colframe=gray!80, breakable,title=Prompt]
\textbf{\# Task Description}
\newline You are an evaluation expert, skilled in assessing the rationality of a question and answer design based on certain criteria. A Q\&A consists of a question and four options.
\textbf{\newline\# Specific Steps}
\newline Analyze according to the following steps: Ensure that the question design effectively tests others' understanding of the content without being influenced by formal differences.
\newline[Consistency] All options should be approximately the same length to avoid obvious differences in length. Ensure all options maintain consistency in tone and professionalism, with similar wording styles to prevent identifying the correct answer through stylistic differences.
\newline[Distractibility] Incorrect options should be designed to be confusing and seemingly reasonable, making them not easy to eliminate through common sense. Ensure that incorrect options are somewhat persuasive, rather than just hypothetical or obviously wrong.
\newline[Avoid Image Element Misguiding] Ensure that all options mentioning image elements align with or are similar to the actual content, to avoid easily eliminating incorrect options due to errors in image details.
\newline[Prevent Keyword and Pattern Recognition] Avoid explicit keyword matches between the question and the options. Ensure that there is no direct verbal association between the way the question is asked and the correct answer, to prevent easy inference.
\newline[Unique Correct Answer] Among all the options, ensure that only one is the correct answer. Avoid ambiguity or vagueness that allows each option to have only one clear judgment.
\newline[Core Assessment] The design of questions and answers must revolve around the key information in the assessment points.
\newline[Comprehensive Judgment] Determine whether the Q\&A meets all the above requirements, and directly output yes/no without any additional characters.
\textbf{\newline\# Output Format }(Please strictly follow the format below)
\newline[Consistency] One sentence judging whether consistency is met and briefly explaining the reason.
\newline[Distractibility] One sentence judging whether confusion is met and briefly explaining the reason.
\newline[Avoid Image Element Misguiding] One sentence judging whether avoiding misleading image elements is met and briefly explaining the reason.
\newline[Prevent Keyword and Pattern Recognition] One sentence judging whether preventing keyword and pattern recognition is met and briefly explaining the reason.
\newline[Unique Correct Answer] One sentence judging whether only one correct answer is met and briefly explaining the reason.
\newline[Core Assessment] One sentence judging whether core assessment is met and briefly explaining the reason.
\newline[Comprehensive Judgment] Yes/No
\textbf{\newline\# Assessment Points}
\newline\{Key points\}
\textbf{\newline\# Q\&A}
\newline \{Questions and answers\}
\end{tcolorbox}
\subsection{Advanced Filtering}
After completing the data quality assessment in \ref{subsection:C.1}, to ensure visual dependency and prevent keyword and pattern recognition, we use the following prompt to filter the collected high-quality QA. If the model can correctly answer without accompanying images, the question is discarded and regenerated until true visual dependency is achieved.

\begin{tcolorbox}[colback=gray!5,colframe=gray!80, breakable,title=Prompt]
Question: \{Question\}
 \newline A. \{A\}
 \newline B. \{B\}
 \newline C. \{C\}
 \newline D. \{D\}
 \newline Directly output the number of the correct answer, do not output any other extra characters.
\end{tcolorbox}

\section{Large Vision Language Models}
% \begin{itemize}
%     \item \textbf{InternVL2}\cite{chen2024fargpt4vclosinggap}is an extension of InternLM2, using InternViT-6B as vision encoder, while also featuring MLP projector sandwiched between them.
%     \item \textbf{Qwen2-VL}\cite{wang2024qwen2vlenhancingvisionlanguagemodels}is an extension of Qwen2-7B, incorporating a vision encoder and a vision-language fusion module to enhance multi-modal capabilities.
%     \item \textbf{MiniCPM-V-2.6}\cite{yao2024minicpmvgpt4vlevelmllm} is also an extension of Qwen2-7B, using SigLip-400M as the vision encoder, and introducing a adapter between them.
%     \item \textbf{LLaVA-OneVision}\cite{li2024llavaonevisioneasyvisualtask} also employs SigLip as the vision encoder, selects Qwen-2 as the LLM, and uses a two-layer MLP to project image features into the word embedding space.
%     \item \textbf{DeepSeek-VL}\cite{lu2024deepseekvlrealworldvisionlanguageunderstanding} employs two different vision encoders and uses DeepSeek LLM as the language decoder, utilizing a two-layer MLP as adapter.
%     \item \textbf{GPT4o}\cite{achiam2023gpt} is an cutting-edge large multimodal model from OpenAI that builds on the success of previous versions to deliver even more accurate, coherent, and contextually aware text generation by leveraging a larger dataset and refined transformer architecture.
% \end{itemize}
\begin{itemize}
    \item \textbf{InternVL2} is an extension of InternLM2, using InternViT as vision encoder, while also featuring MLP projector sandwiched between them.
    \item \textbf{Qwen2-VL} is an extension of Qwen2-7B, incorporating a vision encoder and a vision-language fusion module to enhance multi-modal capabilities.
    \item \textbf{MiniCPM-V-2.6} is also an extension of Qwen2-7B, using SigLip-400M as the vision encoder, and introducing a adapter between them.
    \item \textbf{LLaVA-OneVision} also employs SigLip as the vision encoder, selects Qwen-2 as the LLM, and uses a two-layer MLP to project image features into the word embedding space.
    \item \textbf{DeepSeek-VL} employs two different vision encoders and uses DeepSeek LLM as the language decoder, utilizing a two-layer MLP as adapter.
    \item \textbf{GPT4o} is an cutting-edge large multimodal model from OpenAI that builds on the success of previous versions to deliver even more accurate, coherent, and contextually aware text generation by leveraging a larger dataset and refined transformer architecture.
\end{itemize}

\section{Model Hyper-parameter Details}
\begin{table}
\centering
\begin{tblr}{
  column{2} = {c},
  column{3} = {c},
  hline{1,16} = {-}{0.08em},
  hline{2} = {-}{0.05em},
}
 & Temperature & Top\_k\\
InternVL2-1B & 1.0 & 50\\
InternVL2-2B & 1.0 & 50\\
InternVL2-4B & 1.0 & 50\\
InternVL2-8B & 1.0 & 50\\
InternVL2-26B & 1.0 & 50\\
InternVL2-40B & 1.0 & 50\\
InternVL1.5-26B & 1.0 & 50\\
Qwen2-VL-2B-Instruct & 0.01 & 1\\
Qwen2-VL-7B-Instruct & 0.01 & 1\\
Qwen2-VL-72B-Instruct & 1.0 & 1\\
DeepSeek-VL-7B-chat & 1.0 & \\
llava-onevision-qwen2-7b & 0.7 & 20\\
llava-onevision-qwen2-0.5b & 0.7 & 20\\
MiniCPM-V 2.6 (8B) & 0.7 & 100
\end{tblr}
\caption{The hyper-parameter of all models evaluated in this work.}
\label{tab:param}
\end{table}
The specific parameters used by all models in this paper are shown in Table \ref{tab:param}.

\begin{figure}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1.0 \linewidth]{file/model_accuracy_plot.png}
   \caption{Impact of Key Points from Different Levels on Implicit Meaning Comprehension. Baseline indicates no additional information, Surface, Symbolic, and Background represent the injection of key points from Surface-level content, Symbolic meaning, and Background knowledge, respectively. All indicates the simultaneous injection of key points from all three levels.}
   \label{fig:infl}
\end{figure}

\section{Can Image Descriptions Help the Model Understand Implicit Meaning?}
To investigate the relationship between the key points of image descriptions at different levels (as shown in Figure \ref{fig:example1}) and the understanding of deep semantic meanings in images, we selected models with varying parameter scales from InternVL2 and Qwen2-VL for our study. This is because these two model frameworks are relatively classical and possess a broad range of parameter sizes, which makes them suitable for comprehensive analysis. We then evaluated the impact of providing supplementary information on the implicit meaning comprehension task by using key points extracted from Surface-level content, Symbolic meaning, and Background knowledge (as shown in the lower part of Figure \ref{fig:example1}) individually and in combination. The results are shown in Figure \ref{fig:infl}.

The results indicate that adding key points from the image descriptions of the three other levels significantly improves the model's accuracy in the implicit meaning comprehension task. Injecting Symbolic Meaning alone can enhance the performance of each model by approximately 20-30\%. When all three levels of key points are injected simultaneously, the accuracy of each model is further improved, with larger-scale models achieving an accuracy rate exceeding 80\%, which surpasses human performance. Therefore, we can hypothesize that when models receive information from various levels, they learn useful knowledge that aids in understanding implicit meaning. However, it is worth noting that due to the correlation between key points and the final QA, we cannot entirely rule out the possibility that some of the improvement is attributed to this correlation. Nonetheless, we can conclude that the injection of shallow-level information helps models understand implicit meaning.

\section{What Strategies Augment a Model's Comprehension of Implicit Meanings in Images?}
In order to enhance our model's ability to comprehend implicit meanings, we propose for the first time an innovative method involving the construction of multi-turn dialogues. This approach leverages both images with implicit meanings and artificially constructed virtual images (with virtual images being textual descriptions elaborating a specific image). Specifically, we employ Qwen2-VL-7B as the foundational model. For an image that conveys implicit meaning, our training data is structured into multi-turn dialogues composed of three modules.

The first module involves an input of the image paired with text, where the text comprises various questions, such as "How would you describe this image to a stranger?" This prompts the pre-trained Qwen2-VL-7B to generate a caption of the image. The text input and the caption output form the dialogue in this module, primarily serving the purpose of maintaining the original general capabilities of Qwen2-VL-7B.

In the second module, we utilize Qwen2-VL-72B and a large-scale model like GPT4o to construct multi-turn dialogues from three dimensions: Surface-level content, Symbolic meaning, and Background knowledge. This is designed to infuse the model with knowledge related to surface visual elements, symbolic meanings, and background information.

The third module focuses on extracting the reasoning processes associated with the Implicit Meaning from the images using Qwen2-VL-72B and GPT4o. It generates high-quality chain-of-thought (CoT) data. The CoT data and text inputs form the dialogue in this module, aimed at enhancing the model's inferential capabilities in understanding implicit meanings within a multimodal context.

Simultaneously, to address the insufficiency of real images with implicit meanings, we generate a substantial number of virtual images with implicit meanings via large language models (LLMs). These virtual images are used in place of real ones and are subjected to the same multi-turn dialogue construction for training the model. This approach effectively compensates for the lack of real images with implicit meanings. 

As evidenced in Table \ref{tab:new benchmark}, our proposed method enables the Qwen2-VL-7B model to achieve an accuracy of 62.5\% on the Implicit Meaning Comprehension Task, surpassing the 59.3\% accuracy of GPT4o and the 60.1\% accuracy of Qwen2-VL-72B-Instruct. This demonstrates the effectiveness and superiority of our approach.

\begin{table*}
\centering
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[300]Q[94]Q[133]Q[92]Q[121]Q[63]Q[140]},
  column{even} = {c},
  column{3} = {c},
  column{5} = {c},
  column{7} = {c},
  hline{1,17} = {-}{0.08em},
  hline{2} = {-}{0.05em},
  hline{14} = {1}{l},
  hline{14} = {2-6}{},
  hline{14} = {7}{r},
  hline{15} = {1}{l},
  hline{15} = {2-6}{},
  hline{15} = {7}{r},
  hline{16} = {1}{l},
  hline{16} = {2-6}{},
  hline{16} = {7}{r},
  % hline{17} = {1}{l},
  % hline{17} = {2-6}{},
  % hline{17} = {7}{r},
  % hline{16-17} = {1}{l},
  % hline{16-17} = {2-6}{},
  % hline{16-17} = {7}{r},
  %hline{1,19} = {-}{0.08em},
  %hline{2} = {-}{0.05em},
  %hline{17-18} = {1}{l},
  %hline{17-18} = {2-6}{},
  %hline{17-18} = {7}{r},
}
\textbf{Model}                 & \textbf{\# Params} & \textbf{Surface} & \textbf{Symbolic} & \textbf{Background} & \textbf{Mean} & \textbf{Implicit} \\
InternVL2-Llama3-76B\cite{chen2024fargpt4vclosinggap}           & 76B                & 74.7                   & 71.1              & 75.4                 & 73.7          & 53.8                      \\
Qwen2-VL-72B-Instruct\cite{wang2024qwen2vlenhancingvisionlanguagemodels}            & 72B                & 79.3                   & 82.6     & 81.6        & 81.2 & 60.1             \\
InternVL2-40B\cite{chen2024fargpt4vclosinggap}                  & 40B                & 79.5                   & 79.8              & 80.7                 & 80.0          & 58.7                      \\
InternVL1.5-26B\cite{chen2024fargpt4vclosinggap}                & 26B                & 74.1                   & 70.5              & 74.4                 & 73.0          & 54.7                      \\
InternVL2-26B\cite{chen2024fargpt4vclosinggap}                  & 26B                & 75.2                   & 71.8              & 73.9                 & 73.6          & 50.7                      \\
InternVL2-8B\cite{chen2024fargpt4vclosinggap}                   & 8B                 & 70.7                   & 73.6              & 73.7                 & 72.7          & 46.5                      \\
MiniCPM-V-2\_6\cite{yao2024minicpmvgpt4vlevelmllm}                   & 8B                 & 74.0                   & 74.1              & 79.2                 & 75.8          & 50.0                      \\
Qwen2-VL-7B-Instruct\cite{wang2024qwen2vlenhancingvisionlanguagemodels}             & 7B                 & 75.1                   & 81.1              & 79.3                 & 78.5          & 51.7                      \\
llava-onevision-qwen2-7b\cite{li2024llavaonevisioneasyvisualtask}  & 7B                 & 74.2                   & 72.9              & 76.2                 & 74.4          & 50.0                      \\
v2\_deepseek-vl-7b-chat\cite{lu2024deepseekvlrealworldvisionlanguageunderstanding}        & 7B                 & 58.8                   & 57.3              & 65.6                 & 60.6          & 38.1                      \\
% InternVL2-4B\cite{chen2024fargpt4vclosinggap}                   & 4B                 & 63.3                   & 68.9              & 69.8                 & 67.3          & 41.3                      \\
% InternVL2-2B\cite{chen2024fargpt4vclosinggap}                   & 2B                 & 58.2                   & 56.0              & 64.4                 & 59.5          & 37.6                      \\
Qwen2-VL-2B-Instruct\cite{wang2024qwen2vlenhancingvisionlanguagemodels}           & 2B                 & 70.3                   & 73.8              & 74.5                 & 72.9          & 45.2                      \\
llava-onevision-qwen2-0.5b\cite{li2024llavaonevisioneasyvisualtask} & 0.5B                 & 44.4                   & 45.0              & 33.3                 & 40.9          & 23.2                      \\
% InternVL2-1B\cite{chen2024fargpt4vclosinggap}                   & 1B                 & 49.9                   & 56.5              & 57.9                 & 54.8          & 40.3                      \\
GPT4o                          & -                  & \textbf{82.0}          & 80.8              & 79.8                 & 80.9          & 59.3                      \\
Ours                          & 7B                  & 78.4          & \textbf{84.6}              & \textbf{83.9}                 & \textbf{82.3}          & \textbf{62.5}                      \\

Human                          & -                  & 98.0                   & 88.0              & 86.0                 & 90.7          & 74.0                      
\end{tblr}
\caption{The benchmark includes the average accuracy (in percentages (\%)) on four tasks. Surface, Symbolic, Background, and Implicit represent Surface-level Content Understanding Task, Symbolic Meaning Interpretation Task, Background Knowledge Comprehension Task, and Implicit Meaning Comprehension Task, respectively. The Mean represents the average accuracy of the first three tasks.}
\label{tab:new benchmark}
\end{table*}



%为了探究图片不同level的相关知识与理解图像深度语义之间的关系，我们选取InternVL, Qwen2-VL的不同参数规模的模型作为研究对象，因为InternVL2及Qwen2-VL-72B在implict meaning comprehension task上表现最佳，与GPT4o旗鼓相当。然后，我们将Surface-level content, symbolic meaning, Background Knowledge中提取出来的关键点（如图1下半部分所示）分别以及同时作为implict meaning comprehension task的补充信息来评估其影响。结果如图4所示。
%结果表明，在进行implict meaning comprehension task加入三个其他Level的Keypoint信息，可以有效提升模型的准确率，单独Symbolic Meaning的注入能给各模型带来约20%-30%的提升，而将三个Level的Keypoint信息同时注入时，各模型的准确率得到了进一步提升，对于参数规模较大的模型，其效果已经超过了Human74%的准确率，因此我们可以假定，模型获取到各Level的信息后，会学习到有用的知识，对理解Implicit meaning带来帮助，但值得一提的是，由于Keypoint和最后的QA中存在一定的相关性，我们无法排除一部分提高是由于这种相关性导致的。但从整体而言，我们还是可以得出浅层信息的注入可以帮助模型进行Implicit meaning的理解


% 为了探究图片的相关信息与理解图像深度语义之间的关系
% 其中baseline表示无任何额外信息，Surface，symbolic，background，分别表示单独注入Surface-level content, symbolic meaning, Background Knowledge的Key points，all表示将三个信息同时注入
%分类：GPT4o写了隐晦表达 可能的分类，爬的10w数据按这个分类去分，一部分无法分类，剩下的让gpt继续分类，以此循环


