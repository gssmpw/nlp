\section{Experiments}
Given the impressive performance of LVLMs in tackling image understanding challenges, we evaluated the following LVLMs: InternVL2\cite{chen2024fargpt4vclosinggap}, Qwen2-VL\cite{wang2024qwen2vlenhancingvisionlanguagemodels}, MiniCPM-V-2\_6\cite{yao2024minicpmvgpt4vlevelmllm}, DeepSeek-VL\cite{lu2024deepseekvlrealworldvisionlanguageunderstanding}, LLaVA-OneVision\cite{li2024llavaonevisioneasyvisualtask}, and GPT4o\cite{achiam2023gpt}. These models were selected based on their top-ranking performance in the OpenCompass leaderboard\cite{contributorsopencompass}. Notably, Qwen2-VL-72B\cite{wang2024qwen2vlenhancingvisionlanguagemodels} stands out as the leading open-source LVLMs, while GPT-4o\cite{achiam2023gpt} is widely regarded as one of the excellent closed-source LVLM. Detailed descriptions of these models are provided in the Appendix D.

\subsection{Evaluation}
For evaluating task performance, accuracy was considered the primary metric. A model's answer was deemed correct if it matched the ground truth. Accuracy was computed as the ratio of the number of correct answers ($N_{r}$) to the total number of questions ($N$), i.e.,$N_{r}/N$].

Our task prompts were determined based on each image and task type (referring to the four tasks), followed by choice options: A, B, C, D. The specific parameter settings, including temperature and top-k values, used for each model in the experiments are detailed in the Appendix E. Furthermore, to assess human performance on these tasks, we randomly selected 100 questions from each task in the dataset and had human evaluators provide answers. This allowed us to benchmark human participants' performance against our models, providing a comprehensive comparison of human and machine capabilities on these specific tasks. Detailed experimental results are shown in Table \ref{tab:benchmark}.


% \subsection{Main Results}
% \usepackage{tabularray}
\begin{table*}
\centering
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[300]Q[94]Q[133]Q[92]Q[121]Q[63]Q[140]},
  column{even} = {c},
  column{3} = {c},
  column{5} = {c},
  column{7} = {c},
  hline{1,17} = {-}{0.08em},
  hline{2} = {-}{0.05em},
  hline{14} = {1}{l},
  hline{14} = {2-6}{},
  hline{14} = {7}{r},
  hline{15} = {1}{l},
  hline{15} = {2-6}{},
  hline{15} = {7}{r},
  hline{16} = {1}{l},
  hline{16} = {2-6}{},
  hline{16} = {7}{r},
  % hline{17} = {1}{l},
  % hline{17} = {2-6}{},
  % hline{17} = {7}{r},
  % hline{16-17} = {1}{l},
  % hline{16-17} = {2-6}{},
  % hline{16-17} = {7}{r},
  %hline{1,19} = {-}{0.08em},
  %hline{2} = {-}{0.05em},
  %hline{17-18} = {1}{l},
  %hline{17-18} = {2-6}{},
  %hline{17-18} = {7}{r},
}
\textbf{Model}                 & \textbf{\# Params} & \textbf{Surface} & \textbf{Symbolic} & \textbf{Background} & \textbf{Mean} & \textbf{Implicit} \\
InternVL2-Llama3-76B\cite{chen2024fargpt4vclosinggap}           & 76B                & 74.7                   & 71.1              & 75.4                 & 73.7          & 53.8                      \\
Qwen2-VL-72B-Instruct\cite{wang2024qwen2vlenhancingvisionlanguagemodels}            & 72B                & 79.3                   & \textbf{82.6}     & \textbf{81.6}        & \textbf{81.2} & \textbf{60.1}             \\
InternVL2-40B\cite{chen2024fargpt4vclosinggap}                  & 40B                & 79.5                   & 79.8              & 80.7                 & 80.0          & 58.7                      \\
InternVL1.5-26B\cite{chen2024fargpt4vclosinggap}                & 26B                & 74.1                   & 70.5              & 74.4                 & 73.0          & 54.7                      \\
InternVL2-26B\cite{chen2024fargpt4vclosinggap}                  & 26B                & 75.2                   & 71.8              & 73.9                 & 73.6          & 50.7                      \\
InternVL2-8B\cite{chen2024fargpt4vclosinggap}                   & 8B                 & 70.7                   & 73.6              & 73.7                 & 72.7          & 46.5                      \\
MiniCPM-V-2\_6\cite{yao2024minicpmvgpt4vlevelmllm}                   & 8B                 & 74.0                   & 74.1              & 79.2                 & 75.8          & 50.0                      \\
Qwen2-VL-7B-Instruct\cite{wang2024qwen2vlenhancingvisionlanguagemodels}             & 7B                 & 75.1                   & 81.1              & 79.3                 & 78.5          & 51.7                      \\
llava-onevision-qwen2-7b\cite{li2024llavaonevisioneasyvisualtask}  & 7B                 & 74.2                   & 72.9              & 76.2                 & 74.4          & 50.0                      \\
v2\_deepseek-vl-7b-chat\cite{lu2024deepseekvlrealworldvisionlanguageunderstanding}        & 7B                 & 58.8                   & 57.3              & 65.6                 & 60.6          & 38.1                      \\
% InternVL2-4B\cite{chen2024fargpt4vclosinggap}                   & 4B                 & 63.3                   & 68.9              & 69.8                 & 67.3          & 41.3                      \\
% InternVL2-2B\cite{chen2024fargpt4vclosinggap}                   & 2B                 & 58.2                   & 56.0              & 64.4                 & 59.5          & 37.6                      \\
Qwen2-VL-2B-Instruct\cite{wang2024qwen2vlenhancingvisionlanguagemodels}           & 2B                 & 70.3                   & 73.8              & 74.5                 & 72.9          & 45.2                      \\
llava-onevision-qwen2-0.5b\cite{li2024llavaonevisioneasyvisualtask} & 0.5B                 & 44.4                   & 45.0              & 33.3                 & 40.9          & 23.2                      \\
% InternVL2-1B\cite{chen2024fargpt4vclosinggap}                   & 1B                 & 49.9                   & 56.5              & 57.9                 & 54.8          & 40.3                      \\
GPT4o                          & -                  & \textbf{82.0}          & 80.8              & 79.8                 & 80.9          & 59.3                      \\
% Ours                          & 7B                  & 78.4          & \textbf{84.6}              & \textbf{83.9}                 & \textbf{82.3}          & \textbf{62.5}                      \\

Human                          & -                  & 98.0                   & 88.0              & 86.0                 & 90.7          & 74.0                      
\end{tblr}
\caption{The benchmark includes the average accuracy (in percentages (\%)) on four tasks. Surface, Symbolic, Background, and Implicit represent Surface-level Content Understanding Task, Symbolic Meaning Interpretation Task, Background Knowledge Comprehension Task, and Implicit Meaning Comprehension Task, respectively. The Mean represents the average accuracy of the first three tasks.}
\label{tab:benchmark}
\end{table*}





\subsection{Main results}
\textbf{Surface-level content understanding.} Among the open-source models, Qwen2-VL-72B-Instruct and InternVL2-40B performed best on the surface-level content understanding task, with accuracies of 79.3\% and 79.5\%, respectively, close to GPT-4o (82.0\%). Performance generally correlated with model size, ranging from 44.4\% for the 0.5B llava-onevision-qwen2 to 79.3\% for the 72B Qwen2-VL. However, all models showed a substantial gap compared to human performance (98.0\%), highlighting room for improvement.

\textbf{Symbolic meaning interpretation.} Qwen2-VL-72B-Instruct performed optimally, achieving an accuracy of 82.6\%, slightly surpassing GPT4o's 80.8\%. Smaller models like llava-onevision-qwen2-0.5b-ov-hf achieved only 45.0\%, suggesting that model scale significantly impacts symbolic understanding capabilities. Most models' performance on this task was similar to the surface-level content understanding task, indicating comparable difficulty levels for symbolic meaning interpretation and surface-level content understanding.

\textbf{Background knowledge comprehension.} InternVL2-40B and Qwen2-VL-72B-Instruct exhibited the best performance, with accuracies of 80.7\% and 81.6\%, respectively. The relatively small gap compared to human performance (86.0\%) indicates that models have made significant progress in background understanding. %Even smaller models, such as InternVL2-1B, achieved 58\% accuracy, suggesting that background knowledge comprehension is not entirely dependent on model scale.

\textbf{Implicit meaning comprehension.} All models performed significantly worse on the implicit meaning comprehension task compared to the other tasks. The best performance was achieved by Qwen2-VL-72B-Instruct at 60.1\%, comparable to GPT4o (59.3\%). Smaller models like llava-onevision-qwen2-0.5b-ov-hf achieved only 23\%, revealing a substantial gap compared to human performance (74.0\%). This task appears to be the most challenging for current LVLMs.


\section{Analysis}


\subsection{How do the models perform across different categories of visual perception?}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.65\linewidth,height=0.45\linewidth]{file/radar_5.png}
    
    \caption{The radar charts illustrate the performance of various representative models in interpreting images across different categories within our four tasks.}
    \label{fig:radar}
\end{figure*}
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{file/radar_3.png}
    
%     \caption{The radar charts illustrate the performance of various representative models in interpreting images across different categories within our four tasks.}
%     \label{fig:datasetoverview}
% \end{figure*}
% How does the model parameter scale affect deep semantic understanding?}
% Due to the scaling law, the number of parameters typically has a positive impact on model performance. In this context, we also discuss the relationship between model parameter scale and deep semantic understanding. We examined two pairs of models: the InternVL2 series and the Qwen2-VL sequence. Models within the same series have a consistent architecture and training process, differing only in parameter scale. Figure \ref{fig:bar} provides the accuracy of the models from the two series on four tasks. It can be observed that as the number of model parameters increases, there is a consistent improvement in performance across all four tasks. Therefore, an increase in the number of parameters has a positive impact on the model's deep semantic understanding capability.
Figure \ref{fig:radar} illustrates model performance across four key tasks: surface-level content understanding, symbolic meaning interpretation, background knowledge comprehension, and implicit meaning comprehension, spanning various categories. Accuracy varies significantly across categories. In simpler categories like history and environment, models achieve higher accuracy by effectively capturing direct information. However, performance drops in categories involving deep cultural symbols or metaphors, such as philosophy and personal growth, highlighting current models' limitations in handling complex semantics and cultural nuances.

Larger models (40B+) consistently outperform smaller ones, especially on complex tasks. For simpler tasks like surface-level content, all models perform well, though larger models still have an edge. As task complexity increases, performance gaps widen, with top models significantly surpassing smaller ones but still facing challenges. The Qwen2-VL and InternVL2 series excel in symbolic meaning and background knowledge but show varying stability in implicit meaning comprehension, highlighting ongoing challenges in complex semantic interpretation. These results suggest that while scaling improves performance, implicit meaning comprehension requires further architectural or training optimizations for substantial progress.

\subsection{Can Image Descriptions Help the Model Understand Implicit Meaning?}
\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth, height=0.7 \linewidth]{file/fig5.pdf}
   \caption{Relationship between implicit meaning comprehension and other tasks.}
   \label{fig:6.2}
\end{figure}
%Our analysis shows that providing detailed image descriptions enhances LVLMs' ability to understand deeper semantics. Figure \ref{fig:6.2} illustrates that performance in implicit meaning comprehension is closely linked to the performance in the first three tasks: surface-level content, symbolic meaning, and background knowledge comprehension. The results demonstrate that models provided with additional descriptive information consistently achieve higher accuracy compared to those without, confirming that added context facilitates deeper understanding(see Appendix F for detailed results). However, while descriptions aid models, they still fall short of human-level understanding, particularly in implicit meaning comprehension tasks. This suggests that while auxiliary descriptions help, further advancements are needed for true human-like comprehension.
We believe that, like humans, models need to combine surface-level content, symbolic meaning, and background knowledge to understand implicit meanings. Figure \ref{fig:6.2} shows that performance in implicit meaning comprehension is closely related to the first three tasks. To further validate this, we added key information from these tasks to the reasoning prompts. Experimental results (see Appendix F) show significant improvement, with the optimal model's accuracy surpassing human performance. We reasonably assume that adding this information enables the model to capture most of the foundational content and background knowledge required for implicit meaning comprehension.  However, despite these benefits, there remains room for improvement, suggesting that capturing key information alone is insufficient for fully understanding implicit meanings. To achieve human-like comprehension, models need not only the ability to capture key information but also the reasoning ability to process it effectively.

\subsection{How Does Model Parameter Scale Affect Implicit Meaning Comprehension?}
\begin{figure}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1.0 \linewidth]{file/fig6.pdf}
   \caption{Comparison of accuracy across tasks for InternVL2 models (1B to 40B) and Qwen2-VL-Instruct models (2B to 72B)}
   \label{fig:bar}
\end{figure}

According to scaling laws, increasing model parameters generally improves performance. To evaluate this relationship, we selected models of different scales from two distinct series, InternVL2 and Qwen2-VL. Within each series, the models share the same architecture but differ in scale. InternVL2 and Qwen2-VL, which share architecture but vary in size. Figure \ref{fig:bar} shows that larger models perform better across all four tasks, with models in the 40B-72B range balancing performance and computational cost. However, deeper semantic tasks may need further architectural optimizations, indicating that enhancing deep semantic comprehension requires more than scaling—it also needs specialized strategies.




%由于漫画通常具有高度浓缩的视觉信息，忽略细节往往会导致理解与作者意图之间的巨大差异。同时，漫画中的符号和象征意义常常依赖特定的文化和历史背景，存在一定的理解门槛。此外，一些漫画的含义过于抽象，要求更高的推理和解读能力，降低了人工测试的准确率。
%table need adjustment


