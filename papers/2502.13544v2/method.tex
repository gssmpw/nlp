\section{Methodology}

Based on the analyses and findings above, we propose \textsc{MarkerGen}, a simple-yet-effective plug-and-play method to help LLMs attain better LCTG performance, as shown in Figure~\ref{fig:Figure5}. This method consists of two key modules: (1) \textbf{Auxiliary Marker Insertion Decoding} mechanism, which explicitly enhances length modeling during generation; (2) \textbf{Three-Stage Decoupled Generation} scheme, which decouples length constraints from semantic content generation to further improve LCTG performance.

\subsection{Auxiliary Marker Insertion Decoding}
\label{sec:Auxiliary Length Marker Insertion Decoding}

\paragraph{External Tool Invocation.} 
Our analysis in \S\ref{sec:Preliminaries} reveals that LLMs exhibit significant identifying and counting errors, which directly contribute to inaccuracies in length modeling.
To mitigate these fundamental deficiencies, we introduce external tokenizer and counter for unit recognition and counting, respectively. 
As Finding \ref{find1} indicates that LLMs perceive words better than tokens, we select words as the length unit.
% Theoretically, these errors can be completely removed.
% 为了弥补这两个基础能力，我们考虑引入external tokenizer and counter来分别识别word和计数。理论上，这两个error可以被完全消除。
% 
\paragraph{Length Information Injection.}
With precise length information, we consider feeding it into the model for length modeling. 
Since Finding \ref{find3} indicates that LLMs' inherent implicit length modeling leads to significant errors and is inconvenient for incorporating external length information, we actively insert precise length markers during generation to enable explicit length modeling:
\begin{equation}
\small
\begin{gathered}
    \text{Len}(x) = \text{Counter}(\text{Tokenizer}(x))\\
    x_{t+1} = \begin{cases}
        \text{Marker}(\text{Len}(x_{\leq t})),
         & \text{if} \  \mathcal{S}(\text{Len}(x_{\leq t}),N)   \\
         \text{Sampling}(P(x_{t+1} | x_{\leq t})),
        & \text{else}
    \end{cases}
\end{gathered}
\end{equation}
where $P(x_{t+1} | x_{\leq t})$ is the LLM's probability distribution for next token, $\text{Marker}$ defines the marker format (e.g., [20 words], we discuss the effects of varied marker formats in Appendix \ref{sec:LM_form}), $\mathcal{S}$ is the strategy that determines whether to insert a marker based on current length $\text{Len}(x)$ and target length $N$.
By treating the inserted markers as anchors, LLMs can continuously adjust the expected length of content  to be generated during the generation process, thereby reducing the final LCTG error.
% 得到这些准确的长度信息后，我们需要将其传送给模型帮他进行长度建模。模型原本是自己在隐空间进行长度建模，我们考虑在生成的过程中，间隔地插入marker来帮助模型进行显示长度建模。不同于第二节模型自行插入的是，此时maker插入的时机和所显示的长度信息由外部工具决定，因此是


\paragraph{Decaying Interval Marker Insertion Strategy.}
% 最naive的insertion strategy 是等间隔地插入marker，我们记作$\mathcal{S}_$
The most naive insertion strategy involves placing markers at uniform intervals, which we denote as $\mathcal{S}_{uni}$.
% 然而，根据Finding 4和5说明，较小的insertion interval $n$ 会带来较好的长度建模，但会破坏semantic modeling，而$n$较大时情况相反。
However, according to Findings \ref{find4} and \ref{find7}, a smaller insertion interval $n$ improves length modeling but compromises semantic modeling, whereas a larger $n$ exhibits the opposite effect.
Considering this, we propose a strategy $\mathcal{S}_{dec}$, where $n$ decays exponentially during the generation process:
\begin{equation}
    \small
    \mathcal{S}_{dec}(x,N)= \begin{cases}
        \text{True}, & \text{if} \ x \in\{N-\text{int}(2^{-i}\times N)\}_{i\in \mathbb{N}}   \\
        \text{False},& \text{else}
    \end{cases}
\end{equation}
Taking $N=200$ as an example, the maker will be inserted behind the 100th, 150th, 175th, ... words.
% 这样，在前期模型可以专注于semantic modeling。随着生成的进行，模型越来越关注对长度的控制，最终导向较小的LCTG误差。因此$\mathcal{S}_{dec}$能较好地平衡semantic 和 length modeling。
At the early stage of generation, the model primarily focuses on semantic modeling. As the generation progresses, it increasingly emphasizes length control, ultimately leading to a smaller LCTG error. 
% Considering that aligning error stems from semantic modeling 被干扰，
Consequently, $\mathcal{S}_{dec}$ effectively balances semantic modeling and length modeling.


% Our analysis in \S\ref{sec:Preliminaries} reveals that LLMs exhibit significant identifying and counting errors, which directly contribute to inaccuracies in length modeling. Since standard LLM decoding lacks explicit length control mechanisms, it primarily relies on learned implicit patterns, leading to uncontrolled variations in output length. To mitigate this issue, we introduce an auxiliary length marker insertion strategy, which dynamically integrates explicit length signals into the decoding process in real time.

% \begin{equation}
%     x_{t+1} \sim P(X | x_{\leq t}, m_k),
% \end{equation}
% \begin{equation}
%     m_k =
%     \begin{cases}
%         \mathrm{LM}(k), & \text{if } k \in \mathcal{I}_{\mathrm{insert}} \text{ and } k \leq N_{\mathrm{target}}, \\
%         \emptyset, & \text{otherwise}.
%     \end{cases}
% \end{equation}

% Here, \( \mathcal{I}_{\mathrm{insert}} \) is the set of insertion positions computed based on \( N_{\mathrm{target}} \). The function \( \mathrm{LM}(k) \) determines the length marker corresponding to the \( k \)-th generated word, ensuring explicit tracking of length progression. A length marker \( m_k \) is inserted if \( k \) matches an insertion position and does not exceed \( N_{\mathrm{target}} \), reinforcing length control throughout the decoding process.

% \paragraph{Insertion Strategy}  
% The insertion positions are dynamically determined based on the target length \( N_{\mathrm{target}} \), forming the set:
% \begin{equation}
%     \mathcal{I}_{\mathrm{insert}} = f(N_{\mathrm{target}}, n),
% \end{equation}
% where \( n \) is the insertion interval. (e.g., \( n = 1, 2, 4, 8, ... \))

% However, as shown in \ref{finding:finding8}, small \( n \) can cause the "early stopping" issue by overly constraining semantic expansion, while uniformly dense insertion (Figure \ref{fig:Figure7}) disrupts text coherence. To balance control and fluency, we propose an \textbf{adaptive insertion strategy}: sparse markers are used in the early stages to maintain semantic flow, while denser markers are applied later for precise alignment of lengths.


% \paragraph{Length Marker Form}  
%  We explore different format of marker, including \texttt{"[$k$ words]"}, \texttt{"[$k$]"}, and \texttt{"[$(N_{\mathrm{target}} - k)$ words]"}, and evaluate their effectiveness in aiding length modeling and semantic expansion. Experimental results confirm that \texttt{"[$k$ words]"} achieves the best balance between precise length control and fluent text generation.


% \begin{figure}[t]
%     %\vspace{-1.0cm}
%   \includegraphics[width=\columnwidth,height=4cm]{latex/fig/Figure7.png}
%   \caption{A figure that spans across both columns in a two-column layout with limited height.}
%   \label{fig:Figure7}
% \end{figure}

\vspace{-5pt}
\subsection{Three-Stage Decoupled Generation}
\label{sec:Two-Stage High-Quality Text Generation}
% Finding 6已经证明了Planning before generation的两阶段方法相比于直接生成能导向更好的LCTG results。
% Finding 7证实了aligning error主要源自于语义建模被影响导致的生成过程早停止问题
% 为了减小aligning error同时增强文本quality，我们将原有的planning before generation scheme进一步结耦为三阶段。
% According to Finding 6, the two-step method of planning before generation yields improved LCTG results compared to generating directly. 
% 为了
Finding \ref{find7} validates that aligning error primarily arises from the inferior semantic modeling, which causes premature termination of the generation process.
% the original planning before generation scheme虽然通过将plan进行结耦取得了比direct generation更好的效果（Finding 6），但长度建模过程和语义建模过程仍然耦合。
% 为此，我们提出Three-Stage Decoupled Generation scheme，to further reduce aligning error and improve text quality。
% To reduce aligning error and improve text quality, we further decouple the original planning before generation scheme into three distinct stages.
%
While the planning before generation scheme alleviates interference in semantic modeling by decoupling the planning process (Finding \ref{find6}), it still entangles length modeling with semantic modeling.
To mitigate this, we introduce a three-stage decoupled generation scheme to further reduce the alignment error and improve the text quality, as illustrated in Figure~\ref{fig:Figure5}.
\paragraph{Stage One: Planning.} The model generates a reasonable plan based on the input query and length constraints, including the content of each section and the word allocation.
% 模型根据plan中的内容规划进行高质量的语义建模和生成，without enforcing strict length constraints.
\paragraph{Stage Two: Ensuring Semantic Integrity.} The model focuses on semantic modeling to generate a high-quality response per the plan without being strictly required to adhere to length constraints.
\paragraph{Stage Three: Aligning Length Constraints.} Responses generated in stage two are usually of high quality but may not meet length restrictions. To refine them, we use these non-compliant responses as input and apply the Auxiliary Marker Insertion Decoding mechanism for rewriting. The \textbf{rewriting requirements} include: (1) Retaining the high-quality semantic modeling of the input content. (2) Strictly adhering to the specified length constraints.
In terms of \textbf{workflow}, the model is required to: (1) Firstly analyze the previous stage's response for potential improvements; (2) If its output does not meet the length constraints, it will be regenerated up to $T$ times or until the constraints are met.

% A single-stage approach requires the model to simultaneously satisfy both length constraints and semantic quality, making the task highly challenging. We examine two variants:  
% (1) \textbf{Implicit}, where the model relies on implicit length control without explicit markers, adjusting length based on learned patterns.  
% (2) \textbf{Insert}, which enhances length awareness by incorporating explicit length markers during decoding (Section \ref{sec:Auxiliary Length Marker Insertion Decoding}).

% \paragraph{Two-Stage}  
% To reduce the difficulty of simultaneous optimization, we adopt a two-stage strategy to decouple length control from semantic generation. In the first stage, the model generates text with a focus on semantic quality, without enforcing strict length constraints. In the second stage, non-conforming outputs undergo controlled rewriting using the auxiliary length marker insertion strategy to ensure precise length alignment while preserving coherence.
% Our default setting for MAKERGEN incorporates the Three-Stage Decoupled Generation along with the adaptive auxiliary length marker insertion strategy, as illustrated in Figure~\ref{fig:Figure5}.
% 通过三阶段的
See Appendix 2 for prompts of each stage.


