
\section{Preliminaries}
\label{sec:Preliminaries}

\begin{figure*}[t]
    \centering
    % 主图布局容器
    % \vspace{-1.0cm}
    
    % 子图排列 (使用subcaptionbox实现专业标注)
    \begin{minipage}[b]{0.46\textwidth}
        \centering        \includegraphics[ width=0.9\textwidth]{latex/fig/Figure2.pdf}
        \subcaption{Identifying error analyses\label{fig:figure2-a}}
        \vspace*{-5pt} % 微调标注间距
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.53\textwidth}
        \centering
        \includegraphics[height=4.5cm, width=\textwidth]{latex/fig/Figure2.2.pdf}
        \subcaption{Counting error analyses\label{fig:figure2-b}}
        \vspace*{-5pt}
    \end{minipage}
    
    \caption{Error analyses of fundamental abilities in LCTG across LLMs. }
    \label{fig:figure2}
    \vspace{-3mm} % 标题间距优化
\end{figure*}



% \subsection{Decomposing LCTG Sub-Capabilities}
\label{sec:Decomposing Length Control Errors}
%我们将 LLM 的长度可控文本生成过程类比于人类在此任务中的认知机制进行建模。具体而言，模型首先根据任务需求和长度指令进行语义与长度规划。在此规划的指导下，语义空间在生成过程中以 word 级别逐步扩展，并伴随隐式的计数过程。同时，长度估计作为实时约束项，动态调控进一步的扩展。最终，模型在保证语义完整性的前提下，尽可能对齐目标长度要求。从这一视角出发，LLM 的最终生成误差可自底向上系统性地分解为识别误差（Identifying error）、计数误差（Counting error）、规划误差（Planning error） 和 对齐误差（Aligning error），它们分别影响模型对目标长度约束的对齐能力。
We model the LCTG process of LLMs by drawing an analogy to human patterns in this task. Specifically, the model first performs content and length planning based on task requirements and length constraints. Under this plan, the semantic space expands progressively at the word level during generation, accompanied by an implicit counting process. Meanwhile, length estimation acts as a real-time constraint, dynamically regulating further extension. Ultimately, the model strives to align the length constraints while preserving semantic integrity. From this perspective, the overall \textbf{LCTG} ability of LLMs can be systematically decomposed in a bottom-up manner into \textbf{Identifying}, \textbf{Counting}, \textbf{Planning}, and \textbf{Aligning} sub-capabilities (Figure \ref{fig:Figure0}). 
% 以下我们通过error分析来探究LLMs这些子能力的掌握情况。
Below we explore LLMs' mastery of these abilities through detailed error 
analysis on TruthfulQA dataset \citep{TruthfulQA}.

\subsection{Identifying Error}
\label{sec:identifying_error}
%识别误差指的是模型在判断基本长度单位（如单词）时的错误识别，导致其估算的文本长度与实际长度之间存在偏差。为系统性分析该误差，我们设计了一项实验，要求模型逐个识别并累加长度单位，并将其预测的计数结果与真实值进行比较。实验结果验证，在逐一累加的设定下，计数误差不会发生，这意味着最终的长度误差完全来源于识别误差。

% To systematically analyze this error, we instruct the model to recognize the length units of given text one by one in format:  
% Identifying error refers to the misidentification of fundamental length units (e.g., words), leading to discrepancies between the model’s estimated and actual text length. 
% To systematically analyze this error, we design an experiment where the model is prompted to recognize and accumulate length units sequentially and compare its predicted count against the ground truth. Experimental results confirm that in the one-by-one accumulation setting, counting errors do not occur, meaning that the final length error entirely stems from identifying error. The identifying error \( e_2 \) is computed as:

Identifying error refers to the misidentification of fundamental length units (e.g., words), leading to discrepancies between the model’s estimated and actual text length.
To systematically analyze this error, we instruct the model to recognize the length units of given text one by one. If we define a word as the length unit, the model should output like: ``\texttt{The [1 word] quick [2 words] fox [3 words] ...}''.
On this basis, we calculate the identifying error rate $e_{\text{I}}$ as follows:
\begin{equation}
e_{\text{I}} = \frac{|N_{\mathrm{pred}}^{\mathrm{1}} - N_{\mathrm{true}}|}{N_{\mathrm{true}}} 
\label{eq:e1}
\end{equation}
% 其中，\( N_{\mathrm{pred}}^{\mathrm{1p}} \) 是模型在逐个预测（one-by-one）设置下的预测计数，\( N_{\mathrm{true}} \) 是实际计数。
where \( N_{\mathrm{pred}}^{\mathrm{1}} \) is the model’s predicted final count with 1 as count interval, and \( N_{\mathrm{true}} \) is the actual count.
% 如图 \ref{fig:figure2} 所示，我们在 TruthfulQA 数据集上进行错误识别实验，分别在词（word）、子词（token）和字母（letter）级别上进行评估，其中参考文本作为计数依据。为了进行受控的消融研究，我们定义字母级计数，使得计数的字母数与参考文本的词数相匹配。实验结果表明，大规模语言模型（LLMs）在识别过程中存在显著错误，并且其误差幅度呈现出明确的模式：\textit{字母 $\ll$ 词 $<$ 子词}。总体而言，我们的发现如下：
% \vspace{5pt}  
% \noindent\textbf{发现 1：} LLMs 存在显著的识别误差，这是其长度建模不准确的根本原因。  
% \vspace{3pt}  
% \noindent\textbf{发现 2：} 语言模型感知长度的基本单位是词（word），这表明 LLMs 主要通过\textbf{语义感知}（semantic perception）建模长度，而非\textbf{解码机制}（decoding mechanics）。
% 
% 我们将$e_1$减去把每个word替换成letter ``\texttt{A}''时得到的error rate(此时并不考验identifying能力)，作为修正后的$e_1$。
We subtract the error rate obtained when replacing each word with the letter ``\texttt{A}'' (which barely assess the identifying ability) from $e_{\text{I}}$ to further eliminate the influence of other potential factors.
We explore the word and token as length unit respectively,
as shown in Figure~\ref{fig:figure2-a}.

% the results indicate that LLMs exhibit substantial identifying errors, with the magnitude following a clear pattern: \textit{letter $\ll$ word $<$ token}. Overall, we find that:
% \vspace{2pt}  
\begin{finding}
\label{find1}
LLMs exhibit notable $e_{\text{I}}$ with both word and token as unit, showcasing their deficiencies in fundamental identifying ability.
\end{finding}
\begin{finding}
\label{find2}
Word yields lower $e_{\text{I}}$ than token, indicating that LLMs conduct length modeling primarily based on \textbf{semantic perception} rather than \textbf{decoding mechanics}.
\end{finding}
% \begin{mdframed}
% \noindent\textbf{Finding 1:} LLMs exhibit notable error rates under both settings, showcasing their deficiencies in fundamental identifying ability.

% \noindent\textbf{Finding 2:}  Word yields lower $e_{\text{I}}$ than token, indicating that LLMs model length primarily based on \textbf{semantic perception} rather than \textbf{decoding mechanics}.  
% \end{mdframed}

% \vspace{1pt}  
% \noindent\textbf{Finding 2:} Word yields lower $e_{\text{I}}$ than token, indicating that LLMs model length primarily based on \textbf{semantic perception} rather than \textbf{decoding mechanics}.  


\begin{figure*}[t]
    % 下面两张图并排
    \begin{minipage}{0.51\textwidth}
        \centering
        \includegraphics[height=4.8cm]{latex/fig/Figure3-1.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}{0.51\textwidth}
        \centering
        \includegraphics[height=4.8cm]{latex/fig/Figure3-2.pdf}
    \end{minipage}

    \caption{Absolute contribution of LCTG sub-capability deficiency on overall LCTG error across LLMs.}
    \vspace{-12pt}
    \label{fig:Figure3}
\end{figure*}

\subsection{Counting Error}
\label{sec:counting_error}
% 计数误差（Counting error）指的是对给定序列中元素的不准确枚举，导致其长度偏离预期。我们通过引导 LLMs 计算按 \( n \) 个元素分组的序列来分析这种误差，其中 \( n = 1, 2, 4, 8, \dots \)。当 \( n = 1 \) 时，对应于识别误差（Identifying error）（详见 \ref{sec:identifying_error}）。由于 LLMs 在词感知上表现出识别误差，我们将观察到的计数误差 \( e_{23} \) 分解为识别误差 \( e_2 \)（见公式 \ref{eq:e2}）和纯计数误差 \( e_3 \)，其表达式如下：
% , where \( n = 1, 2, 4, 8, \dots \)
% n越大，越考验counting能力
Counting error refers to the inaccurate enumeration of length units in a given sequence, leading to deviations from the intended length. 
We analyze this error by prompting LLMs to count sequences with varied interval $n$. 
The case of \( n = 1 \) corresponds to identifying error (see \S\ref{sec:identifying_error}). 
A larger $n$ poses a greater challenge for counting accuracy.
To decompose counting error from identifying error, we calculate $e_{\text{C}}^n$ as follows:
\begin{equation}
e_{\text{IC}}^n = \frac{|N_{\mathrm{pred}}^{n} - N_{\mathrm{true}}|}{N_{\mathrm{true}}} 
\label{eq:e12}
\end{equation}
\begin{equation}
e_{\text{C}}^n = e_{\text{IC}}^n - e_{\text{I}}
\label{eq:e2}
\end{equation}

% 如图 \ref{fig:figure2} 所示，\ref{sec:identifying_error} 中的研究结果证实，LLMs 在字母级别上的识别误差可以忽略不计，因此字母级的计数误差可以作为衡量纯计数能力的直接指标。我们还引入了一个基线（letters），其中 LLM 在未被明确要求一次计数 \( n \) 个元素的情况下执行隐式计数。基于实验结果和观察到的模式，我们得出以下关键发现：
% \vspace{5pt}  
% \noindent\textbf{发现 3：} **LLMs 存在计数误差，并且误差随着 \( n \) 的增大而增加。** 无论是计数单词（\( e_2 + e_3 \)）还是计数字母（\( e_3 \)），当 \( n \) 较大时，对计数精度的要求更高，导致误差迅速增加。
% \vspace{3pt}  
% \noindent\textbf{发现 4：} **显式计数结合更精细的计数间隔可以提高 LLMs 的长度建模精度。** 在较小的 \( n \) 下，$\mathbf{e_{\mathit{letters}}}$ 明显低于 $\mathbf{e_{\mathit{letters}}}$（基线），表明通过较小单位的逐步显式计数有助于减少隐式长度建模中的误差。
% As shown in Figure \ref{fig:figure2}, 
Since LLMs exhibit negligible identifying error at the letter level (Figure~\ref{fig:figure2-a}), error of counting letter serves as a direct measure of pure counting ability. We also include a commonly used baseline where the LLMs conduct implicit counting (directly output the length of the entire given text). The results are shown in Figure~\ref{fig:figure2-b}.
% direct count without being explicitly prompted to count \( n \) items at a time. Based on the experimental results and observed patterns, we derive the following key findings:

\begin{finding}
\label{find3}
% Naive的隐式地counting会导致明显的error
\textbf{Naive implicit counting can lead to significant errors.} 
\end{finding}
% Whether counting words (\( e_1 + e_2 \)) or letters (\( e_2 \)), the higher demands on counting ability at larger \( n \) lead to a rapid increase in errors.

\begin{finding}
\label{find4}
\textbf{Explicit counting combined with fine-grained intervals leads to better length modeling.}  At smaller \( n \), the error of explicit counting is significantly lower than that of implicit counting.
\end{finding}


\begin{table}[t]
  \centering
   \renewcommand\arraystretch{1.3}
  \setlength{\tabcolsep}{0.3em} 
  \small
  % \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccc}
    \toprule
      \textbf{Models} & $e_{\text{P}}$  & $s_{\text{P}}$ & $\Delta E$ ($\downarrow$) & $\Delta S$ ($\uparrow$) \\
      \midrule
      % \hline
      GPT-4o                 & 0.06 & 4.28 & -5.31  & 0.05 \\
      GPT-4o mini           & 0.33 & 3.90 & +2.11  & 0.03 \\
      Llama-3.1-70B-Instruct & 0.00 & 3.90 & -0.63 & 0.04 \\
      Qwen2.5-32B-Instruct   & 0.04 & 4.22 & -8.93  & 0.02 \\
      \bottomrule
    \end{tabular}  \caption{\label{tab:planning_error}$e_\text{P}$ and $s_\text{P}$ denote planning error and planning quality score of LLMs. $\Delta E$ and $\Delta S$ quantify the LCTG error reduction and text quality gain from two-stage generation over one-stage generation.}
    \vspace{-10pt}
\end{table}

\vspace{-2pt}

\subsection{Planning Error}
\vspace{-2pt}
\label{sec:planning_error}
%规划误差指的是在长度约束下，不同文本部分的字数分配不当，导致最终生成的总长度未能满足目标要求。为研究该误差，我们计算各部分规划长度之和与目标长度之间的偏差，即e1:

% Planning error refers to the misallocation of word counts across different sections of the text under a given length constraint, resulting in a total length that deviates from the target requirement. To quantify this error, we measure the discrepancy between the sum of the planned segment lengths and the target length, defined as:

% \begin{equation}
% e_{\text{P}}= \frac{|N_{\mathrm{plan}} - N_{\mathrm{target}}|}{N_{\mathrm{target}}} 
% \label{eq:e3}
% \end{equation}

% where \( N_{\mathrm{plan}} \) denotes the total word count allocated by the model, and \( N_{\mathrm{target}} \) represents the specified target length.
Planning error refers to the misallocation of word counts across different sections, leading to a discrepancy from target length. For given query and precise length constraint \( N_{\mathrm{target}} \), 
we prompt LLMs to explicitly plan both content and length for each part of the response.
% we prompt LLMs to generate explicit planning statements regarding their responses, including segment-wise length allocations and content structuring. 
We assess the quality \footnote{We use Qwen-Plus \citep{qwen25} as the judge with a scoring range of [1, 5]. See corresponding prompts in Appendix \ref{sec:appen_planning}} of the plan $s_{\text{P}}$, and calculate the planning error rate $e_{\text{P}}$ as:
\begin{equation}
e_{\text{P}}= \frac{|N_{\mathrm{plan}} - N_{\mathrm{target}}|}{N_{\mathrm{target}}} 
\label{eq:e3}
\end{equation}
% We additionally assess the quality \footnote{All quality evaluations use Qwen-Plus \citep{qwen25} as the judge with a scoring range of [1, 5]. See corresponding prompts in Appendix 1.} of the plan $s_{\text{P}}$.
% Additionally, we compare the effects of \textbf{planning followed by generation} versus \textbf{direct generation} on both length accuracy and content quality. 
where \( N_{\mathrm{plan}} \) denotes the total word count allocated by the model.
Meanwhile, we calculate the reduction in final length error (\(\Delta E\)) and the improvement in content quality (\(\Delta S\)) achieved by \textbf{planning followed by generation} compared to \textbf{direct generation}.
The results are shown in Table~\ref{tab:planning_error}.
% Through these analyses, we derive two key findings:

\begin{finding}
\label{find5}
\textbf{LLMs exhibit strong planning ability.} 
The generated plan effectively meets the length constraints while achieving a quality score of around 4, demonstrating well-structured content allocation.
\end{finding}
\begin{finding}
\label{find6}
\textbf{Planning before generation brings better results.} Compared to direct generation, executing planning and generating sequentially for decomposition reduces length deviations while enhancing semantic quality.
\end{finding}


\subsection{Aligning Error}  
\label{sec:aligning_error}
Aligning error refers to the discrepancy between the model’s perceived length and the target length, arising from the challenge of maintaining semantic integrity while adhering to length constraints. We calculate aligning error as follows:  
\begin{equation}
e^n_{\text{A}} = \frac{|N^n_{\mathrm{pred}} - N_{\mathrm{target}}|}{N_{\mathrm{target}}} 
\label{eq:e4}
\end{equation}
where \( N^n_{\mathrm{pred}} \) represents the model’s perceived length with counting interval $n$, i.e., the length the model assumes it has generated. 
We calculate and show the $e_A^{n}$ in Figure \ref{fig:Figure4}.
% 
% , which is affected by identifying and counting errors.



\begin{finding}
\label{find7}
\textbf{Smaller counting intervals introduce greater aligning error.}
By closely analyzing cases, we find that frequent explicit counting interferes with semantic modeling, causing early termination of generation and poor alignment. In contrast, larger length intervals approximate implicit counting, preserving a more natural generation process.
\end{finding}

\begin{figure}[ht]
    %\vspace{-1.0cm}
  \includegraphics[width=\columnwidth,height=4cm]{latex/fig/Figure4.pdf}
  \vspace{-20pt}
  \caption{Aligning Error across varied length intervals.}
  \vspace{-12pt}
  \label{fig:Figure4}
\end{figure}

\vspace{-4pt}
\subsection{LCTG Error}  
LCTG error refers to the discrepancy between the actual length of generated text and the target length:
\begin{equation}
E = \frac{|N_{\mathrm{true}} - N_{\mathrm{target}}|}{N_{\mathrm{target}}} 
\label{eq:E}
\end{equation}
As established above, this error is systematically composed of four components: \textbf{Identifying Error} (\S\ref{sec:identifying_error}), \textbf{Counting Error} (\S\ref{sec:counting_error}), \textbf{Planning Error} (\S\ref{sec:planning_error}), and \textbf{Aligning Error} (\S\ref{sec:aligning_error}). To investigate the key factors influencing LLMs' LCTG error, we calculate their absolute contributions $\dot{e}_i^{n}$ under different length interval \( n \) as follows:  
\begin{equation}
\dot{e}_i^{n} = \frac{e_i^{n}}{e_\text{I} + e_\text{C}^{n} + e_\text{P} + e^n_\text{A}} \times E^{n} ,\ \ i\in[\text{I,C,P,A}]
\label{eq:ei}
\end{equation}
% \begin{equation}
% e_i^{n} = \frac{e_i}{e_1 + e_2 + e_3 + e_4} \times E^{n} 
% \end{equation}
The results are shown in Figure~\ref{fig:Figure3}. Further details can be found in \ref{sec:Sub-ability Decomposition}. 
% presents an error proportion analysis of Llama-3.1-70B-Instruct and Qwen2.5-32B-Instruct across four representative settings. Figure \ref{fig:Figure4} illustrates the proportion of early-stopped items caused by Aligning Error as \( n \) varies across four LLMs. Further details can be found in \ref{sec:appendix}. Based on these results, we summarize the following key findings:


\begin{finding}
\label{find8}
\textbf{LCTG error is primarily attributed to fundamental deficiencies in length modeling, following the order of}  
 \textbf{Counting Error} $>$ \textbf{Perception Error} $>$ \textbf{Aligning Error} $\gg$ \textbf{Planning Error}.  
Thus, as counting interval increases, the accumulation of counting errors leads to a corresponding rise in LCTG error.
\end{finding}

 \begin{figure*}[t]
    \centering 
\includegraphics[width=0.95\textwidth]{latex/fig/Figure5.pdf}
\vspace{-5pt}
  \caption{Overview of \textsc{MarkerGen}.}
    \vspace{-12pt}
  \label{fig:Figure5}
\end{figure*}

% \vspace{-5pt}  
% \subsection{LLM Decoding Mechanism}
% LLMs generate text in an autoregressive manner, predicting each token sequentially based on previously generated tokens. Given an input sequence $X = \{x_1, x_2, ..., x_n\}$, the generation process follows:

% \begin{equation}
%     x_{t+1} \sim P(X | x_{\leq t}),
% \end{equation}

% where each token $x_{t+1}$ is sampled from the model’s probability distribution conditioned on prior context. This iterative decoding continues until a predefined stopping criterion is met, such as reaching a maximum length or generating an end-of-sequence (EOS) token.