\section{Experiments}
We conduct comprehensive experiments to examine \textsc{MarkerGen}. Specifically, we validate its effectiveness in \S\ref{sec:performance}, analyze its generalizability in \S\ref{sec:Generalizability}, explore the impact of its key components in \S\ref{sec:ablation}, 
and provide further insights into its mechanism in \S\ref{sec:analysis}.
Hyperparameter choices and additional analyses are provided in Appendix~\ref{sec:exp details}.

\begin{table}[t]
    \renewcommand\arraystretch{1.1}
    \small
    \centering
    \setlength{\tabcolsep}{0.20em} 
    \begin{tabular}{lcc}
    \toprule
    Benchmarks&Ability Tested&Length (words)\\
     \midrule
     CNN/DailyMail&\multirow{2}{*}{Summarization} & \multirow{2}{*}{18-165}\\
     \citep{CNN/DailyMail}&&\\
    \cdashline{1-3}
    HANNA&\multirow{2}{*}{Story Generation} & \multirow{2}{*}{139-995}\\
     \citep{HANNA}&&\\
     \cdashline{1-3}
     TruthfulQA&Question & \multirow{2}{*}{101-294}\\
     \citep{TruthfulQA}&Answering&\\
     \cdashline{1-3}
    HelloBench&Heuristic LCTG\&& \multirow{2}{*}{489-1450}\\
     \citep{HelloBench}&Open-ended QA&\\
     \cdashline{1-3}
    GAOKAO&History& \multirow{2}{*}{71-901}\\
     \citep{gaokao}&Open-ended QA&\\
    \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
    \caption{Benchmarks Introduction.}
    \label{tab: benchmark}
    \vspace{-0.5cm}
\end{table}

\subsection{Experimental Settings}


%  \begin{table*}[t]
%   \centering
%     \renewcommand{\arraystretch}{0.9}
%     \footnotesize
%   \begin{tabular}{l c c *{12}{cc} c}
%     \toprule
%     \multirow{4}{*}{\textbf{Benchmark}} & \multirow{4}{*}{\textbf{Method}} & \multirow{4}{*}{\textbf{Scheme}} 
%     & \multicolumn{6}{c}{\textbf{Qwen2.5 Series}} & \multicolumn{4}{c}{\textbf{Llama3.1 Series}} & \multirow{4}{*}{\textbf{Cost}} \\
%     \cmidrule(lr){4-9} \cmidrule(lr){10-13}
%     & & & \multicolumn{2}{c}{7B} & \multicolumn{2}{c}{14B} & \multicolumn{2}{c}{32B} & \multicolumn{2}{c}{8B} & \multicolumn{2}{c}{70B}  & \\
%     \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
%     & & & $E$ ($\downarrow$) & $S$ ($\uparrow$) & $E$ ($\downarrow$) & $S$ ($\uparrow$) & $E$ ($\downarrow$) & $S$ ($\uparrow$) & $E$ ($\downarrow$) & $S$ ($\uparrow$) & $E$ ($\downarrow$) & $S$ ($\uparrow$) & \\
%     \midrule
%     \multirow{3}{*}{CNN/DailyMail} 
%       & \multirow{1}{*}{Implicit}    & \textit{-} & 30.31 & 3.04 & 12.54 & 3.15 & 11.05 & 3.21 & 15.12 & 3.04 & 11.07 & 3.09  & \dots\\
%       & \multirow{2}{*}{\textsc{MarkerGen}} & \small\textit{Stage Two} & 39.7 & 3.19 & 14.85 & 3.30 & 12.07 & 3.36 & 19.7 & 3.18 & 11.98 & 3.24 & \dots\\
%       &             & \small\textit{Stage Three} & \textbf{9.92} & 3.07 & \textbf{6.06} & 3.16 & \textbf{4.82} & 3.25 & \textbf{3.36} & 3.18 & \textbf{3.18} & 3.36 & \dots \\
%     \midrule
%     \multirow{3}{*}{HANNA} 
%       & \multirow{1}{*}{Implicit}    & \textit{-} & 28.55 & 3.47 & 14.86 & 3.55 & 12.03 & 3.67 & 16.68 & 3.54 & 10.44 & 3.61  & \dots\\
%       & \multirow{2}{*}{\textsc{MarkerGen}} & \small\textit{Stage Two} & 31.6 & 3.60 & 17.99 & 3.76 & 14.82 & 3.81 & 19.51 & 3.68 & 10.94 & 3.74 & \dots \\
%       &             & \small\textit{Stage Three} & 8.49 & 3.50 & 5.22 & 3.55  & 3.57 & 3.72 & 2.98 & 3.60 & 2.58 & 3.63 & \dots \\
%     \midrule
%     \multirow{2}{*}{TruthfulQA}
%       & \multirow{1}{*}{Implicit}    & \textit{-} & 16.7 & 4.29 & 17.9 & 4.44 & 8.7 & 4.45 & 7.21 & 4.22 & 7.64 & 4.46 & \dots\\
%       & \textsc{MarkerGen}   & \small\textit{Stage Three} & 9.08 & 4.33 & 7.59 & 4.43 & 4.48 & 4.54 & 3.82 & 4.25 & 2.80 & 4.48  & \dots \\
%     \midrule
%     \multirow{2}{*}{Heuristic Generation}
%       & \multirow{1}{*}{Implicit}    & \textit{-} & 35.69 & 3.42 & 21.34 & 3.80 & 12.02 & 3.80 & 21.91 & 3.72 & 27.89 & 3.74 & \dots \\
%       & \textsc{MarkerGen}   & \small\textit{Stage Three} & 8.51 & 4.13 & 6.35 & 4.00 & 5.34 & 4.14 & 6.03 & 4.03 & 5.03 & 3.98 & \dots \\
%     \bottomrule
%   \end{tabular}%
%   \vspace{-2pt}
%   \caption{\label{tab:Overall}Overall Performance of \textsc{MarkerGen} on Various Benchmarks. \textbf{E} denotes LCTG error rate (\%) and \textbf{S} denotes the text quality ([1, 5]) given by LLM judge.}
% \end{table*}

\begin{table*}[t]
  \centering
  \small
    \setlength{\tabcolsep}{0.40em} 
    \renewcommand{\arraystretch}{1.3}
    \footnotesize
  \begin{tabular}{l c *{12}{cc} c}
    \toprule
    \multirow{4}{*}{\textbf{Benchmarks}} & \multirow{4}{*}{\textbf{Methods}} 
    & \multicolumn{6}{c}{\textbf{Qwen2.5 Series}} & \multicolumn{4}{c}{\textbf{Llama3.1 Series}} & \multirow{4}{*}{\textbf{Costs}} \\
    \cmidrule(lr){3-8} \cmidrule(lr){9-12}
    & & \multicolumn{2}{c}{7B} & \multicolumn{2}{c}{14B} & \multicolumn{2}{c}{32B} & \multicolumn{2}{c}{8B} & \multicolumn{2}{c}{70B}  & \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
    & & $E$ ($\downarrow$) & $S$ ($\uparrow$) & $E$ ($\downarrow$) & $S$ ($\uparrow$) & $E$ ($\downarrow$) & $S$ ($\uparrow$) & $E$ ($\downarrow$) & $S$ ($\uparrow$) & $E$ ($\downarrow$) & $S$ ($\uparrow$) & \\
    \midrule
    \multirow{2}{*}{CNN/DailyMail} 
      & Implicit    & 30.31 & 3.04 & 12.54 & 3.15 & 11.05 & 3.21 & 15.12 & 3.04 & 11.07 & 3.09  & $1.30 \times \delta$\\
      & \textsc{MarkerGen}  & \textbf{9.92} & \textbf{3.07} & \textbf{6.06} & \textbf{3.16} & \textbf{4.82} & \textbf{3.25} & \textbf{3.36} & \textbf{3.18} & \textbf{3.18} & \textbf{3.36} & $\delta$ \\
    \cdashline{1-13}
    \multirow{3}{*}{HANNA} 
      & Implicit  & 28.55 & 3.47 & 14.86 & 3.55 & 12.03 & 3.67 & 16.68 & 3.54 & 10.44 & 3.61  & $2.37 \times \delta$\\
      & \textsc{MarkerGen}  & \textbf{8.49} & \textbf{3.50} & \textbf{5.22} & 3.55  & \textbf{3.57} & \textbf{3.72} & \textbf{2.98} & \textbf{3.60} & \textbf{2.58} & \textbf{3.63} & $\delta$ \\
    \cdashline{1-13}
    \multirow{2}{*}{TruthfulQA}
      & \multirow{1}{*}{Implicit}    & 16.7 & 4.29 & 17.9 & 4.44 & 8.7 & 4.45 & 7.21 & 4.22 & 7.64 & 4.46 & $1.75 \times \delta$\\
      & \textsc{MarkerGen}    & \textbf{9.08} & \textbf{4.33} & \textbf{7.59} & 4.43 & \textbf{4.48} & \textbf{4.54} & \textbf{3.82} & \textbf{4.25} & \textbf{2.80} & \textbf{4.48}  & $\delta$ \\
    \cdashline{1-13}
    \multirow{2}{*}{Heuristic Generation}
      & \multirow{1}{*}{Implicit}    & 35.69 & 3.42 & 21.34 & 3.80 & 12.02 & 3.80 & 21.91 & 3.72 & 27.89 & 3.74 & $1.06 \times \delta$ \\
      & \textsc{MarkerGen}& \textbf{8.51} & \textbf{4.13} & \textbf{6.35} & \textbf{4.00} & \textbf{5.34} & \textbf{4.14} & \textbf{6.03} & \textbf{4.03} & \textbf{5.03} & \textbf{3.98} & $\delta$ \\
    \bottomrule
  \end{tabular}%
  \vspace{-2pt}
\caption{\label{tab:Overall}Overall Performance of \textsc{MarkerGen} on Various Benchmarks. $E$ denotes LCTG error rate (\%) and $S$ denotes the text quality ([1, 5]) given by LLM judge. $\delta$ denotes the token cost of \textsc{MarkerGen} under each setting.}
\end{table*}


\begin{table*}[t]
  \centering
  \small
    \setlength{\tabcolsep}{0.6em} 
  \renewcommand{\arraystretch}{1}
 \footnotesize
  \begin{tabular}{ll *{8}{cc} c}
    \toprule
    \multirow{4}{*}{\textbf{Model}} & \multirow{4}{*}{\textbf{Methods}} & \multicolumn{8}{c}{\textbf{Target Length Scales}} &\multirow{4}{*}{\textbf{Costs}} \\
    \cmidrule(lr){3-10}
    & & \multicolumn{2}{c}{100} & \multicolumn{2}{c}{200} & \multicolumn{2}{c}{300} & \multicolumn{2}{c}{400} &\\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} 
     & & $E$ ($\downarrow$) & $S$ ($\uparrow$) & $E$ ($\downarrow$)& $S$ ($\uparrow$) & $E$ ($\downarrow$)& $S$ ($\uparrow$) & $E$ ($\downarrow$)& $S$ ($\uparrow$) &\\
    \midrule
    \multirow{2}{*}{\textbf{Qwen2.5-7B-Instruct}} 
    & \multirow{1}{*}{Implicit} 
      & 30.97 & 3.45 & 22.91  & 3.53 & 26.12 &3.28  &29.63 &3.08  & $1.26 \times \delta$ \\
    % \cmidrule(lr){3-11}
    & \multirow{1}{*}{\textsc{MarkerGen}} 
      & 8.26  &3.92  &9.06  &4.00  &7.67  &3.75 &5.10   &3.55  & $\delta$  \\
    \midrule
    \multirow{4}{*}{\textbf{Model}} & \multirow{4}{*}{\textbf{Methods}} & \multicolumn{8}{c}{\textbf{Length Constraint Types}} & \multirow{4}{*}{\textbf{Costs}}\\
    \cmidrule(lr){3-10}
    & & \multicolumn{2}{c}{<100} & \multicolumn{2}{c}{100-150}  & \multicolumn{2}{c}{160-200} & \multicolumn{2}{c}{>500}\\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} 
    & & $E_r$ ($\downarrow$)& $S$ ($\uparrow$) & $E_r$ ($\downarrow$)& $S$ ($\uparrow$) & $E_r$ ($\downarrow$)& $S$ ($\uparrow$) & $E_r$ ($\downarrow$)& $S$ ($\uparrow$) \\
    \midrule
    \multirow{2}{*}{\textbf{Qwen2.5-7B-Instruct}} 
    & \multirow{1}{*}{Implicit} 
      & 7.50  &3.47  & 63.00 & 4.03 & 66.00  & 4.06  & 29.50 & 2.65 & $1.07\times \delta$   \\
    % \cmidrule(lr){3-11}
    & \multirow{1}{*}{\textsc{MarkerGen}} 
      & 0.00   & 3.94  & 0.50  & 4.50 & 3.00  &4.53  & 0.00  &3.13  & $\delta$  \\
    \bottomrule
  \end{tabular}%
\caption{\label{tab:openqa}Experiments with varied length scales and constraint types on Open-ended QA subset of HelloBench.}
\vspace{-10pt}
\end{table*}




\paragraph{Benchmarks} 
We choose five benchmarks for experiments, where HelloBench includes two subsets, as shown in Table~\ref{tab: benchmark}. See details in Appendix \ref{sec:bench_detail}.
% Ruler是我们唯一能获取code并运行的training-based的baseline。

\paragraph{Baselines} 
\begin{itemize}[leftmargin=20pt]
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\vspace{-5pt}
\item \textbf{Ruler} \citep{ruler}: 
A training-based\footnote{Ruler is the only training-based baseline for which we can find that releases the code and training set.} method that defines length control templates to regulate generation at the range level.
% A training-based\footnote{Ruler is the only training-based baseline for which we can obtain the code and run it.} method that defines length control templates controlling at the range level.
\item \textbf{Implicit} \citep{longwriter}: 
Conduct a plan-and-generate process  without explicit counting. 
To ensure a fair comparison, the model generates multiple responses until token count outperforms \textsc{MarkerGen} and the candidate with the smallest LCTG error is selected.
% 为了比较的公平性，模型会生成多次responses直到generated tokens数量和\textsc{MarkerGen}对齐，然后选择LCTG error最小的候选。
\end{itemize}

\paragraph{Details}  
% \vspace{-5pt} 
We conduct extensive experiments 
using Qwen2.5 series (Qwen2.5-7B/14B/32B-Instruct) \citep{qwen25} and the Llama3.1 series (Llama-3.1-8B/70B-Instruct) \citep{llama31}, with sampling temperature as 0.5.
% 我们在HelloBench的Open-ended QA subset上测试在 coarse-grained length constraints下的性能，在其他benchmarks上测试precise length constraints下的LCTG error rate following Eq.~\eqref{eq:E}。
We experiment under coarse-grained length constraints on the Open-ended QA subset of HelloBench and assess the LCTG error rate under precise length constraints on other benchmarks, following Eq.~\eqref{eq:E}.
To evaluate the text quality, we use GPT-4o mini \citep{gpt4o} as the judge, with a calibration algorithm to mitigate the length bias \citep{llmasjudge} (See details in Appendix~\ref{sec:exp details}).
For precise constraints, we set the length of ground truth response as desired target length.
We run each setting for three times and report the average results.

\subsection{Main Results} \label{sec:performance}
As shown in Table \ref{tab:Overall}, the commonly used two-stage implicit counting baseline results in a substantial LCTG error rate $E$ of 18.32\% on average, even if the best response  is chosen across multiple attempts.
This intuitively demonstrates the impact of the inherent limitations of LLM's LCTG sub-capability.
    The training-based baseline Ruler, as observed in our preliminary experiments (Appendix \ref{sec:Training-based methods performance}), benefits from training on test sets that matches the training domain, while performs poorly on our evaluated benchmarks, highlighting its limited generalizability.
In comparison, under strict length constraints, \textsc{MarkerGen} achieves an absolute reduction of 12.57\% in $E$ relative to the implicit baseline, bringing the final error down to just 5.57\%.
In terms of text quality, by decoupling length modeling and semantic modeling during the generation process and employing the decaying insertion strategy to minimize the damage caused by length constraints to semantic integrity, \textsc{MarkerGen} achieves a higher $S$ in average.
Meanwhile, this performance is achieved with only 64\% of the tokens used by the baseline. 





% (E) and 40.63 points (absolute avg rejection rate of 0.88\%) for range-based length constraints (E$_r$), demonstrating significant improvements in performance. Additionally, \textsc{MarkerGen} outperforms Two-stage Implicit methods with an average quality score greater than 0.22 across five models and four benchmarks.

% In terms of computational cost, \textsc{MarkerGen} consumes only 64\% of the tokens used by the baselines. This efficiency arises because the method not only enables precise length modeling but also decouples length and semantic requirements, reducing the overall complexity. As a result, \textsc{MarkerGen} achieves better performance with lower resource consumption.
% As shown in Tables \ref{tab:Overall}, \textsc{MarkerGen} achieves a reduction of 12.57 points (absolute avg error of 5.75\%) for precise length constraints (E) and 40.63 points (absolute avg rejection rate of 0.88\%) for range-based length constraints (E$_r$), demonstrating significant improvements in performance. Additionally, \textsc{MarkerGen} outperforms Two-stage Implicit methods with an average quality score greater than 0.22 across five models and four benchmarks.

% In terms of computational cost, \textsc{MarkerGen} consumes only 64\% of the tokens used by the baselines. This efficiency arises because the method not only enables precise length modeling but also decouples length and semantic requirements, reducing the overall complexity. As a result, \textsc{MarkerGen} achieves better performance with lower resource consumption.


% across various tasks, length scales, control granularity, and models, utilizing the Qwen2.5 series (Qwen2.5-7B/14B/32B-Instruct) \citep{qwen25} and the Llama3.1 series (Llama-3.1-8B/70B-Instruct) \citep{llama31}. For the first four tasks and the sixth tasks, the reference's length is treated as the precise target. In the case of \textit{open-ended QA}, we introduce frequency length constraints and range-based constraints to assess generalization and practical applicability. Additionally, we test the performance of our method on a Chinese benchmark, further demonstrating its generalization capability.
% The evaluation metrics include Equation \ref{eq:E} for precise length control and Equation \ref{eq:Er} for range-based control.
%  To evaluate generation quality, we use GPT-4o mini \citep{gpt4o} as the judge, applying length bias correction (details in Appendix~\ref{sec:appendix}). For training-based baselines, we select Ruler, the method with publicly available code, and analyze its limitations in Appendix~\ref{sec:appendix}. Cost alignment is ensured by retrying multiple times and selecting the item with the smallest length error.

\begin{figure*}[t]
  % \vspace{-1cm} % 负间距让图片靠近顶部
  \includegraphics[width=\textwidth, height=0.25\textheight]{latex/fig/FIgure7.pdf}
  \caption{Attention matrices of the first (left) and last (right) layers.}
 % 负间距让图片靠近顶部
  \label{fig:Figure7} 
  \vspace{-4pt}
\end{figure*}



\begin{table*}[t]
  \centering
  \small
    % \setlength{\tabcolsep}{0.5em} 
  \renewcommand{\arraystretch}{0.9}
   % \normalsize
  \begin{tabular}{l *{6}{cc} }
  % \small
    \toprule
\multirow{3}{*}{Variants} & \multicolumn{12}{c}{Marker Insertion Interval $n$}  \\
    \cmidrule(lr){2-13}
      & \multicolumn{2}{c}{1} & \multicolumn{2}{c}{4} & \multicolumn{2}{c}{16} & \multicolumn{2}{c}{32} & \multicolumn{2}{c}{64} & \multicolumn{2}{c}{Decaying} \\
    \cmidrule(lr){2-13}
      & $E$ ($\downarrow$)& \textcolor{gray}{$S$ ($\uparrow$)} & $E$ ($\downarrow$)& \textcolor{gray}{$S$ ($\uparrow$)}& $E$ & \textcolor{gray}{$S$ ($\uparrow$)} & $E$ ($\downarrow$)& \textcolor{gray}{$S$ ($\uparrow$)} & $E$ ($\downarrow$)& \textcolor{gray}{$S$ ($\uparrow$)} & $E$ ($\downarrow$)& \textcolor{gray}{$S$ ($\uparrow$)} \\
    \midrule
        \textit{w/o Tool}  & 15.53 & \textcolor{gray}{4.28} & 32.50 & \textcolor{gray}{4.29} & 34.64   & \textcolor{gray}{4.46}   & 32.50 & \textcolor{gray}{4.48} & 20.44 & \textcolor{gray}{4.58} & --   & -- \\
      \textit{Two Stage} & 3.10 & \textcolor{gray}{4.03} & 1.49  & \textcolor{gray}{4.03} & 4.04 & \textcolor{gray}{4.20} & 3.26  & \textcolor{gray}{4.23} & 3.93  & \textcolor{gray}{4.32} & 2.66 & \textcolor{gray}{4.28} \\
      \textit{Three Stage}     & 4.84 & \textcolor{gray}{4.28} & 4.20 & \textcolor{gray}{4.29} & 4.89 & \textcolor{gray}{4.45} & 5.45  & \textcolor{gray}{4.48} & 5.18  & \textcolor{gray}{4.57} & 4.48 & \textcolor{gray}{4.54} \\
    \bottomrule
  \end{tabular}%
  \caption{\label{tab:ablation}Ablation studies on key components.}
  \vspace{-10pt}
\end{table*}



\subsection{Generalizability}  
\label{sec:Generalizability}
\paragraph{Across LLMs and Tasks.}
Table~\ref{tab:Overall} demonstrates the strong generalizability of \textsc{MarkerGen} to LLMs and generation tasks.
% Table~\ref{tab:Overall} 展示了在precise length constraints下\textsc{MarkerGen}对于LLMs和generation task的strong generalizability。
% 我们进一步探究其在
\vspace{-5pt}
\paragraph{Across Length Scale.}
% Table~\ref{tab:Overall} 展示了在具有不同长度需求(18-1450)的benchmarks上\textsc{MarkerGen}都能表现良好。我们进一步探查在一个benchmark上，扩展长度需求（从100扩大到400），\textsc{MarkerGen} 的error rate整体呈现下降趋势。这是因为auxiliary marker insertion decoding strategy 能够避免模型隐式建模导致的误差累积。因此target 长度越大，相对误差越小。
Table~\ref{tab:Overall} also shows \textsc{MarkerGen}’s strong performance across benchmarks with varying length scale (18–1450). To further investigate, we analyze progressively increasing the target length from 100 to 400. The results in Table~\ref{tab:openqa} show a declining trend in \textsc{MarkerGen}’s error rate, which can be attributed to the auxiliary marker insertion decoding mechanism that prevents error accumulation from implicit modeling.
\vspace{-5pt}
\paragraph{Across Constraint Types.}
In addition to exact length constraints, users may impose range-based limits. We evaluate $E_r$, the proportion of responses violating these constraints. Table~\ref{tab:openqa} shows that \textsc{MarkerGen} maintains an $E_r$ below 3\% in all cases, significantly lower than the baseline.
\vspace{-5pt}
\paragraph{Across Lingual.}
We further validate the effectiveness of \textsc{MarkerGen} in Chinese setting on GAOKAO benchmark, as shown in Table~\ref{tab:chinese}.
% 我们进一步在中文benchmark 上验证了\textsc{MarkerGen}在不同语言上的有效性。
% 除了精准的长度需求，有时用户会给出range-based constraints。
% 我们统计该类粗粒度长度限制下，生成的response不满足区间要求的比率$E_r$. 如表所示，在所有的settings下，\textsc{MarkerGen}都展现了低于3\%的$E_r$，显著低于baseline。
% We define the rejection rate \( E_r \), which is the proportion of items that do not meet the specified range requirements:
% \begin{equation}
% E_r = 1 - \frac{|\{i \in I \mid l_{\text{min}} \leq \text{len}(i) \leq l_{\text{max}}\}|}{|I|}
% \label{eq:Er}
% \end{equation}

% We evaluate the generalizability of \textsc{MarkerGen} across 4 tasks with varying length scales ranging from 18 to 1450 words for fine-grained length-controlled settings, but also in more practical and widely applicable scenarios, such as high-frequency length constraints (e.g., "Please answer in 200/300 words") and range-based length constraint settings (e.g., "Answer length should be between 100-150 words"). In addition, we test \textsc{MarkerGen} on Chinese language generation tasks to further assess its practical utility and generalization ability. For range-based constraints, we define the rejection rate \( E_r \), which is the proportion of items that do not meet the specified range requirements:

% \begin{equation}
% E_r = 1 - \frac{|\{i \in I \mid l_{\text{min}} \leq \text{len}(i) \leq l_{\text{max}}\}|}{|I|}
% \label{eq:Er}
% \end{equation}

% As shown in Tables \ref{tab:openqa} and \ref{tab:chinese}, our method performs well across all tasks, effectively demonstrating its generalizability and practicality.


\subsection{Ablation Studies} \label{sec:ablation}
In this section, we validate the effectiveness of each module in \textsc{MarkerGen} with Qwen2.5-32B-Instruct on TruthfulQA, as shown in Table \ref{tab:ablation}.
% , we replace each step of our default \textsc{MarkerGen} settings with specific variants to measure their individual impact.
\paragraph{Tool Invocation.} 
% 当模型不依靠external tokenizer and counter而被要求自行插入marker时，由于fundamental能力的不足，会导致巨大的（超过15%）error rate。
When the model is required to insert markers independently without relying on an external tokenizer and counter, its fundamental limitations lead to a significant increase in the error rate, exceeding 15\%.
\paragraph{Decaying Interval Marker Insertion.} 
% 在使用固定的marker insertion interval时，由于length modeling和$n$成正比而semantic modeling与$n$成反比（which会引发alignment error），我们观察到LCTG error非常不稳定。相比之下，Decaying Interval Marker Insertion strategy 通过先疏后密的插入策略，在保证了长度显示建模的同时，最大程度上保证了语义完整性，导向了更低的$E$和良好的$S$。
When using a fixed marker insertion interval $n$, since length control is inversely proportional to $n$, while semantic modeling is directly proportional to $n$ (which induces alignment errors), we observe unstable LCTG error rate. In contrast, by adopting a sparse-to-dense insertion approach, the Decaying Interval Marker Insertion strategy ensures explicit length modeling while maximizing semantic integrity, leading to lower $E$ and superior $S$.
\vspace{-5pt}
\paragraph{Three-Stage Decoupled Generation.}
% 两阶段的scheme通过引入插入的length marker相比于implicit baseline能带来更低的LCTG error(8.7-2.66)，但也由于更偏重长度建模而导致了text quality的下降(4.45-4.28)。相比之下，三阶段的scheme通过结耦较好地平衡了semantic和length modeling，带来了both 长度控制和文本质量的提升。
The introduction of explicit length markers in the two-stage scheme leads to a substantial reduction in LCTG error relative to the implicit baseline (8.7 → 2.66). However, this scheme places greater emphasis on length modeling, which consequently diminishes text quality (4.45 → 4.28). In comparison, the three-stage scheme achieves a better balance by decoupling semantic and length modeling, thereby improving both length control and text quality.
% 三阶段能带来更低的LCTG error, 但也会导向worse text quality。
% \paragraph{Auxiliary Marker Insertion.} 


% \paragraph{Marker Insertion Strategy}  
% Regardless of whether we compare it to implicit modeling or explicit modeling approaches, the use of Auxiliary Marker Insertion Decoding substantially reduces LCTG errors. This finding validates the effectiveness of \ref{sec:Auxiliary Length Marker Insertion Decoding} in assisting LLMs to overcome identification and counting errors, thus enabling precise length modeling. Moreover, the Decaying Intervals strategy demonstrates superior performance over the uniform intervals strategy. By facilitating fine-grained length control while maintaining high semantic quality, the Decaying Intervals strategy highlights the efficacy of $\mathcal{S}_{dec}$.


% \paragraph{Importance of Each Stage}  
% The results support the critical role of each stage in our Three-Stage Decoupled Generation (refer to \ref{sec:Two-Stage High-Quality Text Generation}). In the first stage, the LLM’s great planning capabilities (as demonstrated in \ref{find6}) contribute to more effective length control and enhanced semantic quality, as shown in Table \ref{tab:planning_error} and \ref{tab:ablation}. In the second stage, the focus shifts to semantic modeling, where the model generates high-quality text (Table \ref{tab:ablation} and \ref{tab:Overall}). Finally, the third stage ensures precise length control, aligning the generated text with the length requirements. The synergistic operation of these stages enables the model to generate text that meets both semantic and length constraints effectively.






\subsection{Working Mechanism of \textsc{MarkerGen}} \label{sec:analysis}
% \paragraph{Working Mechanism of \textsc{MarkerGen}}
To better understand how LLMs leverage the inserted length markers in \textsc{MarkerGen}, we visualize the attention matrices of the first and last layers of Llama-3.1-8b-Instruct (Figure \ref{fig:Figure7}). In the shallow layers, the attention distribution reveals a clear focus on the length information represented by the length markers (in the red box). As the model progresses to the deeper layers, attention shifts from the length information to the adjacent semantic content (in the orange box). 
% This pattern 说明模型在浅层通过marker进行长度建模，并形成准确的长度信息。而在深层，模型在长度信息的指导下进行语义建模，并最终输出符合长度需求和保障语义完整性的token。
This pattern demonstrates that at shallow layers, the model uses markers to establish length modeling and encode precise length information. At deeper layers, it relies on this length information for semantic modeling, producing tokens that align with the length constraints while maintaining semantic integrity. 
% See more details in Appendix \ref{sec:exp details}.

% This pattern demonstrates the effectiveness of the Marker Insertion Strategy in precise length modeling. Additionally, cross-layer attention analysis reveals a consistent trend: shallow layers prioritize length modeling, while deeper layers focus on semantic generation, specifically targeting relevant output (detailed in Appendix \ref{sec:exp details}). This finding offers valuable insights for future research.

