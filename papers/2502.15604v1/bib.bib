@inproceedings{10.1145/3613904.3642320,
author = {Cai, Runze and Janaka, Nuwan and Chen, Yang and Wang, Lucia and Zhao, Shengdong and Liu, Can},
title = {PANDALens: Towards AI-Assisted In-Context Writing on OHMD During Travels},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642320},
doi = {10.1145/3613904.3642320},
abstract = {While effective for recording and sharing experiences, traditional in-context writing tools are relatively passive and unintelligent, serving more like instruments rather than companions. This reduces primary task (e.g., travel) enjoyment and hinders high-quality writing. Through formative study and iterative development, we introduce PANDALens, a Proactive AI Narrative Documentation Assistant built on an Optical See-Through Head Mounted Display that supports personalized documentation in everyday activities. PANDALens observes multimodal contextual information from user behaviors and environment to confirm interests and elicit contemplation, and employs Large Language Models to transform such multimodal information into coherent narratives with significantly reduced user effort. A real-world travel scenario comparing PANDALens with a smartphone alternative confirmed its effectiveness in improving writing quality and travel enjoyment while minimizing user effort. Accordingly, we propose design guidelines for AI-assisted in-context writing, highlighting the potential of transforming them from tools to intelligent companions.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {1053},
numpages = {24},
keywords = {AI, HMD, Human-AI collaborative writing, in-context writing, large language model, multimodal information, smart glasses, travel blog},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3544548.3580651,
author = {Bennett, Dan and Metatla, Oussama and Roudaut, Anne and Mekler, Elisa D.},
title = {How does HCI Understand Human Agency and Autonomy?},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580651},
doi = {10.1145/3544548.3580651},
abstract = {Human agency and autonomy have always been fundamental concepts in HCI. New developments, including ubiquitous AI and the growing integration of technologies into our lives, make these issues ever pressing, as technologies increase their ability to influence our behaviours and values. However, in HCI understandings of autonomy and agency remain ambiguous. Both concepts are used to describe a wide range of phenomena pertaining to sense-of-control, material independence, and identity. It is unclear to what degree these understandings are compatible, and how they support the development of research programs and practical interventions. We address this by reviewing 30 years of HCI research on autonomy and agency to identify current understandings, open issues, and future directions. From this analysis, we identify ethical issues, and outline key themes to guide future work. We also articulate avenues for advancing clarity and specificity around these concepts, and for coordinating integrative work across different HCI communities.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {375},
numpages = {18},
keywords = {Autonomy, Self Determination Theory, agency, boundary objects, delegation, mixed initiative, theory, user experience},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    editor = "Goldstein, Jade  and
      Lavie, Alon  and
      Lin, Chin-Yew  and
      Voss, Clare",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}


@inproceedings{10.3115/1073083.1073135,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {BLEU: a method for automatic evaluation of machine translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}

@misc{yuan2024ragdrivergeneralisabledrivingexplanations,
      title={RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model}, 
      author={Jianhao Yuan and Shuyang Sun and Daniel Omeiza and Bo Zhao and Paul Newman and Lars Kunze and Matthew Gadd},
      year={2024},
      eprint={2402.10828},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2402.10828}, 
}

@inproceedings{10.1145/3613904.3642230,
author = {Lee, Jaewook and Wang, Jun and Brown, Elizabeth and Chu, Liam and Rodriguez, Sebastian S. and Froehlich, Jon E.},
title = {GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation in Wearable Augmented Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642230},
doi = {10.1145/3613904.3642230},
abstract = {Voice assistants (VAs) like Siri and Alexa are transforming human-computer interaction; however, they lack awareness of users’ spatiotemporal context, resulting in limited performance and unnatural dialogue. We introduce GazePointAR, a fully-functional context-aware VA for wearable augmented reality that leverages eye gaze, pointing gestures, and conversation history to disambiguate speech queries. With GazePointAR, users can ask “what’s over there?” or “how do I solve this math problem?” simply by looking and/or pointing. We evaluated GazePointAR in a three-part lab study (N=12): (1) comparing GazePointAR to two commercial systems, (2) examining GazePointAR’s pronoun disambiguation across three tasks; (3) and an open-ended phase where participants could suggest and try their own context-sensitive queries. Participants appreciated the naturalness and human-like nature of pronoun-driven queries, although sometimes pronoun use was counter-intuitive. We then iterated on GazePointAR and conducted a first-person diary study examining how GazePointAR performs in-the-wild. We conclude by enumerating limitations and design considerations for future context-aware VAs.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {408},
numpages = {20},
keywords = {LLM, augmented reality, gaze tracking, multimodal input, pointing gesture recognition, voice assistants},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3313831.3376479,
author = {Mayer, Sven and Laput, Gierad and Harrison, Chris},
title = {Enhancing Mobile Voice Assistants with WorldGaze},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376479},
doi = {10.1145/3313831.3376479},
abstract = {Contemporary voice assistants require that objects of inter-est be specified in spoken commands. Of course, users are often looking directly at the object or place of interest ? fine-grained, contextual information that is currently unused. We present WorldGaze, a software-only method for smartphones that provides the real-world gaze location of a user that voice agents can utilize for rapid, natural, and precise interactions. We achieve this by simultaneously opening the front and rear cameras of a smartphone. The front-facing camera is used to track the head in 3D, including estimating its direction vector. As the geometry of the front and back cameras are fixed and known, we can raycast the head vector into the 3D world scene as captured by the rear-facing camera. This allows the user to intuitively define an object or region of interest using their head gaze. We started our investigations with a qualitative exploration of competing methods, before developing a functional, real-time implementation. We conclude with an evaluation that shows WorldGaze can be quick and accurate, opening new multimodal gaze+voice interactions for mobile voice agents.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–10},
numpages = {10},
keywords = {interaction techniques, mobile interaction, worldgaze},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@misc{spyridis2024empoweringpriorcourtlegal,
      title={Empowering Prior to Court Legal Analysis: A Transparent and Accessible Dataset for Defensive Statement Classification and Interpretation}, 
      author={Yannis Spyridis and Jean-Paul and Haneen Deeb and Vasileios Argyriou},
      year={2024},
      eprint={2405.10702},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.10702}, 
}


@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI et al. },
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{qin2024largelanguagemodelsmeet,
      title={Large Language Models Meet NLP: A Survey}, 
      author={Libo Qin and Qiguang Chen and Xiachong Feng and Yang Wu and Yongheng Zhang and Yinghui Li and Min Li and Wanxiang Che and Philip S. Yu},
      year={2024},
      eprint={2405.12819},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.12819}, 
}

@misc{gao2024retrievalaugmentedgenerationlargelanguage,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10997}, 
}

@article{articleRamachandran,
author = {Ramachandran, Anand},
year = {2024},
month = {08},
pages = {},
title = {Retrieval Augmented Generation (RAG) for Large Language Models Leveraging Enterprise Data (SAP, Salesforce, Workday)}
}

@misc{sundar2022multimodalconversationalaisurvey,
      title={Multimodal Conversational AI: A Survey of Datasets and Approaches}, 
      author={Anirudh Sundar and Larry Heck},
      year={2022},
      eprint={2205.06907},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.06907}, 
}

@inproceedings{10.1145/3589335.3651905,
author = {Tayal, Anuja and Tyagi, Aman},
title = {Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651905},
doi = {10.1145/3589335.3651905},
abstract = {When interacting with Retrieval-Augmented Generation (RAG)-based conversational agents, the users must carefully craft their queries to be understood correctly. Yet, understanding the system's capabilities can be challenging for the users, leading to ambiguous questions that necessitate further clarification. This work aims to bridge the gap by developing a suggestion question generator. To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts. Through experiments, we show that the dynamic contexts approach can generate better suggestion questions as compared to other prompting approaches.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1338–1341},
numpages = {4},
keywords = {conversational systems, few-shot, prompting, question generation, rag},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3627673.3679078,
author = {Merkelbach, Kilian and Riabinova, Ksenia and Dutta, Arnab},
title = {AI-safe Autocompletion with RAG and Relevance Curation},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679078},
doi = {10.1145/3627673.3679078},
abstract = {In search, autocomplete (AC) is an essential tool that provides suggestions for each keystroke, functioning well with token-based queries. However, it is challenging to handle at scale when input queries are conversational and semantically rich. Identifying relevant queries for sub-tokens requires efficient lookup strategies, real-time ranking, and relevance in the results. This work integrates Retrieval-Augmented Generation (RAG), AI safety, and relevance ranking to produce autocomplete suggestions for conversational queries in a production system. RAG-based responses ensure a high hit ratio for popular AC inputs and maintain a very low risk category by not triggering any critical AI safety concerns.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5562–5563},
numpages = {2},
keywords = {ai safety, e-commerce, large language models, llms, rag, retrieval-augmented generation, search},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3627673.3680087,
author = {Joshi, Ashutosh and Sarwar, Sheikh Muhammad and Varshney, Samarth and Nag, Sreyashi and Agrawal, Shrivats and Naik, Juhi},
title = {REAPER: Reasoning based Retrieval Planning for Complex RAG Systems},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3680087},
doi = {10.1145/3627673.3680087},
abstract = {Complex dialog systems often use retrieved evidence to facilitate factual responses. Such RAG (Retrieval Augmented Generation) systems retrieve from heterogeneous data stores that are architected as multiple indexes or APIs instead of a single monolithic source. For a given query, relevant evidence needs to be retrieved from one (or few) retrieval source. Complex queries can even require multi-step retrieval. For example, a conversational agent on a retail site answering customer questions about past orders need to retrieve the appropriate customer order first and then the evidence relevant to the customer's question in the context of the ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by interleaving reasoning and retrieval steps. However, each reasoning step directly adds to the latency of the system. For large models this latency cost is significant -- in the order of multiple seconds. Multi-agent systems may classify the query to a single Agent associated with a retrieval source, which means that a (small) classification model dictates the performance of a large language model. To address this problem, we present REAPER (REAsoning-based PlannER), an LLM-based retrieval planner that we evaluate on a conversational shopping assistant, which shows significant gains in latency over Agent-based systems and scalability to new and unseen use cases when compared to classification-based planning.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {4621–4628},
numpages = {8},
keywords = {chain-of-thought, multi-hop reasoning, rag},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3637528.3671470,
author = {Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
title = {A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671470},
doi = {10.1145/3637528.3671470},
abstract = {As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6491–6501},
numpages = {11},
keywords = {fine-tuning, in-context learning, large language model (llm), pre-training, prompting, retrieval augmented generation (rag)},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3626772.3657660,
author = {Sudhi, Viju and Bhat, Sinchana Ramakanth and Rudat, Max and Teucher, Roman},
title = {RAG-Ex: A Generic Framework for Explaining Retrieval Augmented Generation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657660},
doi = {10.1145/3626772.3657660},
abstract = {Owing to their size and complexity, large language models (LLMs) hardly explain why they generate a response. This effectively reduces the trust and confidence of end users in LLM-based applications, including Retrieval Augmented Generation (RAG) for Question Answering (QA) tasks. In this work, we introduce RAG-Ex, a model- and language-agnostic explanation framework that presents approximate explanations to the users revealing why the LLMs possibly generated a piece of text as a response, given the user input. Our framework is compatible with both open-source and proprietary LLMs. We report the significance scores of the approximated explanations from our generic explainer in both English and German QA tasks and also study their correlation with the downstream performance of LLMs. In the extensive user studies, our explainer yields an F1-score of 76.9\% against the end user annotations and attains almost on-par performance with model-intrinsic approaches.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2776–2780},
numpages = {5},
keywords = {explainability, large language models, retrieval augmented generation},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@misc{mior2024largelanguagemodelsjson,
      title={Large Language Models for JSON Schema Discovery}, 
      author={Michael J. Mior},
      year={2024},
      eprint={2407.03286},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2407.03286}, 
}

@article{escarda2024llms,
  title={LLMs on the Fly: Text-to-JSON for Custom API Calling},
  author={Escarda-Fern{\'a}ndez, Miguel and L{\'o}pez-Riob{\'o}o-Botana, I{\~n}igo and Barro-Tojeiro, Santiago and Padr{\'o}n-Cousillas, Lara and Gonzalez-V{\'a}zquez, Sonia and Carreiro-Alonso, Antonio and G{\'o}mez-Area, Pablo},
  year={2024}
}

@inproceedings{10.1145/3299869.3320212,
author = {Raasveldt, Mark and M\"{u}hleisen, Hannes},
title = {DuckDB: an Embeddable Analytical Database},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3320212},
doi = {10.1145/3299869.3320212},
abstract = {The immense popularity of SQLite shows that there is a need for unobtrusive in-process data management solutions. However, there is no such system yet geared towards analytical workloads. We demonstrate DuckDB, a novel data management system designed to execute analytical SQL queries while embedded in another process. In our demonstration, we pit DuckDB against other data management solutions to showcase its performance in the embedded analytics scenario. DuckDB is available as Open Source software under a permissive license.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1981–1984},
numpages = {4},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@misc{adhikari2024comparativestudypdfparsing,
      title={A Comparative Study of PDF Parsing Tools Across Diverse Document Categories}, 
      author={Narayan S. Adhikari and Shradha Agarwal},
      year={2024},
      eprint={2410.09871},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2410.09871}, 
}


@incollection{BOUR2019289,
title = {Chapter 14 - Crowd behavior analysis from fixed and moving cameras},
booktitle = {Multimodal Behavior Analysis in the Wild},
publisher = {Academic Press},
pages = {289-322},
year = {2019},
series = {Computer Vision and Pattern Recognition},
isbn = {978-0-12-814601-9},
author = {Pierre Bour and Emile Cribelier and Vasileios Argyriou}}

@INPROCEEDINGS{6977392,
  author={Bloom, Victoria and Makris, Dimitrios and Argyriou, Vasileios},
  booktitle={2014 22nd International Conference on Pattern Recognition}, 
  title={Clustered Spatio-temporal Manifolds for Online Action Recognition}, 
  year={2014},
  volume={},
  number={},
  pages={3963-3968},
  doi={10.1109/ICPR.2014.679}}


@INPROCEEDINGS{4587762,
  author={Argyriou, Vasileios and Petrou, Maria},
  booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Recursive photometric stereo when multiple shadows and highlights are present}, 
  year={2008},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/CVPR.2008.4587762}}

@incollection{ARGYRIOU20091,
title = {Photometric Stereo: An Overview},
series = {Advances in Imaging and Electron Physics},
publisher = {Elsevier},
volume = {156},
pages = {1-54},
year = {2009},
booktitle = {Advances in IMAGING AND ELECTRON PHYSICS},
issn = {1076-5670},
author = {Vasileios Argyriou and Maria Petrou}}

@INPROCEEDINGS{10181189,
  author={Nagy, Akos and Amponis, George and Kyranou, Konstantinos and Lagkas, Thomas and Boulogeorgos, Alexandros Apostolos and Sarigiannidis, Panagiotis and Argyriou, Vasileios},
  booktitle={2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing Workshops (CCGridW)}, 
  title={AI-Powered Interfaces for Extended Reality to Support Remote Maintenance}, 
  year={2023},
  volume={},
  number={},
  pages={214-222},
  keywords={Measurement;Extended reality;Green products;Production;Maintenance engineering;Predictive models;Real-time systems;Augmented Reality;HCI;Remote Maintenance;Predictive AI;hands-free interfaces},
  doi={10.1109/CCGridW59191.2023.00045}}

@INPROCEEDINGS{10257282,
  author={Nagy, Akos and Lagkas, Thomas and Sarigiannidis, Panagiotis and Argyriou, Vasileios},
  booktitle={2023 19th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)}, 
  title={Evaluation of AI-Supported Input Methods in Augmented Reality Environment}, 
  year={2023},
  volume={},
  number={},
  pages={496-503},
  keywords={Training;Visualization;Microcomputers;Mice;Safety;Reliability;Artificial intelligence;Augmented Reality;HCI;hands-free interfaces;AI -guided interaction},
  doi={10.1109/DCOSS-IoT58021.2023.00083}}


@misc{nagy2024userexperienceevaluationar,
      title={User Experience Evaluation of AR Assisted Industrial Maintenance and Support Applications}, 
      author={Akos Nagy and Yannis Spyridis and Gregory J Mills and Vasileios Argyriou},
      year={2024},
      eprint={2410.17348},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2410.17348}, 
}