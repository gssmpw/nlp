@inproceedings{10.1145/3589335.3651905,
author = {Tayal, Anuja and Tyagi, Aman},
title = {Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651905},
doi = {10.1145/3589335.3651905},
abstract = {When interacting with Retrieval-Augmented Generation (RAG)-based conversational agents, the users must carefully craft their queries to be understood correctly. Yet, understanding the system's capabilities can be challenging for the users, leading to ambiguous questions that necessitate further clarification. This work aims to bridge the gap by developing a suggestion question generator. To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts. Through experiments, we show that the dynamic contexts approach can generate better suggestion questions as compared to other prompting approaches.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1338–1341},
numpages = {4},
keywords = {conversational systems, few-shot, prompting, question generation, rag},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3613904.3642230,
author = {Lee, Jaewook and Wang, Jun and Brown, Elizabeth and Chu, Liam and Rodriguez, Sebastian S. and Froehlich, Jon E.},
title = {GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation in Wearable Augmented Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642230},
doi = {10.1145/3613904.3642230},
abstract = {Voice assistants (VAs) like Siri and Alexa are transforming human-computer interaction; however, they lack awareness of users’ spatiotemporal context, resulting in limited performance and unnatural dialogue. We introduce GazePointAR, a fully-functional context-aware VA for wearable augmented reality that leverages eye gaze, pointing gestures, and conversation history to disambiguate speech queries. With GazePointAR, users can ask “what’s over there?” or “how do I solve this math problem?” simply by looking and/or pointing. We evaluated GazePointAR in a three-part lab study (N=12): (1) comparing GazePointAR to two commercial systems, (2) examining GazePointAR’s pronoun disambiguation across three tasks; (3) and an open-ended phase where participants could suggest and try their own context-sensitive queries. Participants appreciated the naturalness and human-like nature of pronoun-driven queries, although sometimes pronoun use was counter-intuitive. We then iterated on GazePointAR and conducted a first-person diary study examining how GazePointAR performs in-the-wild. We conclude by enumerating limitations and design considerations for future context-aware VAs.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {408},
numpages = {20},
keywords = {LLM, augmented reality, gaze tracking, multimodal input, pointing gesture recognition, voice assistants},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3613904.3642320,
author = {Cai, Runze and Janaka, Nuwan and Chen, Yang and Wang, Lucia and Zhao, Shengdong and Liu, Can},
title = {PANDALens: Towards AI-Assisted In-Context Writing on OHMD During Travels},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642320},
doi = {10.1145/3613904.3642320},
abstract = {While effective for recording and sharing experiences, traditional in-context writing tools are relatively passive and unintelligent, serving more like instruments rather than companions. This reduces primary task (e.g., travel) enjoyment and hinders high-quality writing. Through formative study and iterative development, we introduce PANDALens, a Proactive AI Narrative Documentation Assistant built on an Optical See-Through Head Mounted Display that supports personalized documentation in everyday activities. PANDALens observes multimodal contextual information from user behaviors and environment to confirm interests and elicit contemplation, and employs Large Language Models to transform such multimodal information into coherent narratives with significantly reduced user effort. A real-world travel scenario comparing PANDALens with a smartphone alternative confirmed its effectiveness in improving writing quality and travel enjoyment while minimizing user effort. Accordingly, we propose design guidelines for AI-assisted in-context writing, highlighting the potential of transforming them from tools to intelligent companions.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {1053},
numpages = {24},
keywords = {AI, HMD, Human-AI collaborative writing, in-context writing, large language model, multimodal information, smart glasses, travel blog},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3626772.3657660,
author = {Sudhi, Viju and Bhat, Sinchana Ramakanth and Rudat, Max and Teucher, Roman},
title = {RAG-Ex: A Generic Framework for Explaining Retrieval Augmented Generation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657660},
doi = {10.1145/3626772.3657660},
abstract = {Owing to their size and complexity, large language models (LLMs) hardly explain why they generate a response. This effectively reduces the trust and confidence of end users in LLM-based applications, including Retrieval Augmented Generation (RAG) for Question Answering (QA) tasks. In this work, we introduce RAG-Ex, a model- and language-agnostic explanation framework that presents approximate explanations to the users revealing why the LLMs possibly generated a piece of text as a response, given the user input. Our framework is compatible with both open-source and proprietary LLMs. We report the significance scores of the approximated explanations from our generic explainer in both English and German QA tasks and also study their correlation with the downstream performance of LLMs. In the extensive user studies, our explainer yields an F1-score of 76.9\% against the end user annotations and attains almost on-par performance with model-intrinsic approaches.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2776–2780},
numpages = {5},
keywords = {explainability, large language models, retrieval augmented generation},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3627673.3679078,
author = {Merkelbach, Kilian and Riabinova, Ksenia and Dutta, Arnab},
title = {AI-safe Autocompletion with RAG and Relevance Curation},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679078},
doi = {10.1145/3627673.3679078},
abstract = {In search, autocomplete (AC) is an essential tool that provides suggestions for each keystroke, functioning well with token-based queries. However, it is challenging to handle at scale when input queries are conversational and semantically rich. Identifying relevant queries for sub-tokens requires efficient lookup strategies, real-time ranking, and relevance in the results. This work integrates Retrieval-Augmented Generation (RAG), AI safety, and relevance ranking to produce autocomplete suggestions for conversational queries in a production system. RAG-based responses ensure a high hit ratio for popular AC inputs and maintain a very low risk category by not triggering any critical AI safety concerns.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5562–5563},
numpages = {2},
keywords = {ai safety, e-commerce, large language models, llms, rag, retrieval-augmented generation, search},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3627673.3680087,
author = {Joshi, Ashutosh and Sarwar, Sheikh Muhammad and Varshney, Samarth and Nag, Sreyashi and Agrawal, Shrivats and Naik, Juhi},
title = {REAPER: Reasoning based Retrieval Planning for Complex RAG Systems},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3680087},
doi = {10.1145/3627673.3680087},
abstract = {Complex dialog systems often use retrieved evidence to facilitate factual responses. Such RAG (Retrieval Augmented Generation) systems retrieve from heterogeneous data stores that are architected as multiple indexes or APIs instead of a single monolithic source. For a given query, relevant evidence needs to be retrieved from one (or few) retrieval source. Complex queries can even require multi-step retrieval. For example, a conversational agent on a retail site answering customer questions about past orders need to retrieve the appropriate customer order first and then the evidence relevant to the customer's question in the context of the ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by interleaving reasoning and retrieval steps. However, each reasoning step directly adds to the latency of the system. For large models this latency cost is significant -- in the order of multiple seconds. Multi-agent systems may classify the query to a single Agent associated with a retrieval source, which means that a (small) classification model dictates the performance of a large language model. To address this problem, we present REAPER (REAsoning-based PlannER), an LLM-based retrieval planner that we evaluate on a conversational shopping assistant, which shows significant gains in latency over Agent-based systems and scalability to new and unseen use cases when compared to classification-based planning.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {4621–4628},
numpages = {8},
keywords = {chain-of-thought, multi-hop reasoning, rag},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3637528.3671470,
author = {Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
title = {A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671470},
doi = {10.1145/3637528.3671470},
abstract = {As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6491–6501},
numpages = {11},
keywords = {fine-tuning, in-context learning, large language model (llm), pre-training, prompting, retrieval augmented generation (rag)},
location = {Barcelona, Spain},
series = {KDD '24}
}

@INPROCEEDINGS{10181189,
  author={Nagy, Akos and Amponis, George and Kyranou, Konstantinos and Lagkas, Thomas and Boulogeorgos, Alexandros Apostolos and Sarigiannidis, Panagiotis and Argyriou, Vasileios},
  booktitle={2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing Workshops (CCGridW)}, 
  title={AI-Powered Interfaces for Extended Reality to Support Remote Maintenance}, 
  year={2023},
  volume={},
  number={},
  pages={214-222},
  keywords={Measurement;Extended reality;Green products;Production;Maintenance engineering;Predictive models;Real-time systems;Augmented Reality;HCI;Remote Maintenance;Predictive AI;hands-free interfaces},
  doi={10.1109/CCGridW59191.2023.00045}}

@INPROCEEDINGS{10257282,
  author={Nagy, Akos and Lagkas, Thomas and Sarigiannidis, Panagiotis and Argyriou, Vasileios},
  booktitle={2023 19th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)}, 
  title={Evaluation of AI-Supported Input Methods in Augmented Reality Environment}, 
  year={2023},
  volume={},
  number={},
  pages={496-503},
  keywords={Training;Visualization;Microcomputers;Mice;Safety;Reliability;Artificial intelligence;Augmented Reality;HCI;hands-free interfaces;AI -guided interaction},
  doi={10.1109/DCOSS-IoT58021.2023.00083}}

@INPROCEEDINGS{4587762,
  author={Argyriou, Vasileios and Petrou, Maria},
  booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Recursive photometric stereo when multiple shadows and highlights are present}, 
  year={2008},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/CVPR.2008.4587762}}

@INPROCEEDINGS{6977392,
  author={Bloom, Victoria and Makris, Dimitrios and Argyriou, Vasileios},
  booktitle={2014 22nd International Conference on Pattern Recognition}, 
  title={Clustered Spatio-temporal Manifolds for Online Action Recognition}, 
  year={2014},
  volume={},
  number={},
  pages={3963-3968},
  doi={10.1109/ICPR.2014.679}}

@incollection{ARGYRIOU20091,
title = {Photometric Stereo: An Overview},
series = {Advances in Imaging and Electron Physics},
publisher = {Elsevier},
volume = {156},
pages = {1-54},
year = {2009},
booktitle = {Advances in IMAGING AND ELECTRON PHYSICS},
issn = {1076-5670},
author = {Vasileios Argyriou and Maria Petrou}}

@incollection{BOUR2019289,
title = {Chapter 14 - Crowd behavior analysis from fixed and moving cameras},
booktitle = {Multimodal Behavior Analysis in the Wild},
publisher = {Academic Press},
pages = {289-322},
year = {2019},
series = {Computer Vision and Pattern Recognition},
isbn = {978-0-12-814601-9},
author = {Pierre Bour and Emile Cribelier and Vasileios Argyriou}}

@misc{yuan2024ragdrivergeneralisabledrivingexplanations,
      title={RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model}, 
      author={Jianhao Yuan and Shuyang Sun and Daniel Omeiza and Bo Zhao and Paul Newman and Lars Kunze and Matthew Gadd},
      year={2024},
      eprint={2402.10828},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2402.10828}, 
}

