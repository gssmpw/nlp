%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
% \usepackage{microtype}
% \usepackage{graphicx}
% \usepackage{subfigure}
% \usepackage{booktabs} % for professional tables
\let\algorithmicindent \relax
\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsfonts}
%\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{bbm}
\usepackage{arydshln}
\usepackage{subfigure}
\usepackage{microtype}
\usepackage{booktabs} % for professional tables
\usepackage{graphicx} % Load the graphicx package
\usepackage[T1]{fontenc}

% In preamble


% Optional: Control floating behavior
\usepackage{float}

\usepackage{listings}
\usepackage{placeins}
\usepackage{nicefrac}
\usepackage{xurl}
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{nicefrac}
\usepackage{makecell}
\usepackage{xspace}
\usepackage{graphbox}
\usepackage{pifont}
\usepackage{enumitem}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{footmisc}
\usepackage{color}
\usepackage{stfloats}
\usepackage{slashbox}
\usepackage{diagbox}
\usepackage{float}
\usepackage{afterpage}
\usepackage{tcolorbox}

\usepackage{relsize}



% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% Some useful definitions for algorithms
%\newcommand{\algorithmicrequire}{\textbf{Input:}}
%\newcommand{\algorithmicensure}{\textbf{Output:}}

% For probability notation
\newcommand{\E}{\mathbb{E}}

% For norm notation
\newcommand{\norm}[1]{\left\|#1\right\|}

\DeclareMathOperator*{\argmax}{arg\,max}  % For proper argmax formatting
\DeclareMathOperator*{\argmin}{arg\,min}  % For proper argmin formatting
%\newcommand{\norm}[1]{\left\|#1\right\|}  % For norm notation
\newcommand{\eps}{\epsilon}   




\newcommand\Tstrut{\rule{0pt}{2.6ex}}         %
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   %
\newcommand{\vit}{ViT\xspace}
\newcommand{\oclip}{OpenClip\xspace}
\newcommand{\clip}{CLIP\xspace}
\newcommand{\simclr}{SimCLR\xspace}
\newcommand{\gpt}{\mbox{GPT-4}\xspace}
\newcommand{\openf}{OpenFlamingo\xspace}
\newcommand{\llava}{LLaVA\xspace}
\newcommand{\apgdce}{APGD\textsubscript{CE}\xspace}
\newcommand{\rbench}{RobustBench\xspace}
\newcommand{\aatt}{AutoAttack\xspace}
\newcommand{\imnet}{ImageNet\xspace}
\newcommand{\tecoa}{TeCoA\xspace}

\newcommand{\tecoafour}{TeCoA\textsuperscript{4}\xspace}
\newcommand{\tecoatwo}{TeCoA\textsuperscript{2}\xspace}
\newcommand{\oursgiant}{Robust-LLaVA\textsuperscript{4}\textsubscript{G}\@\xspace}
\newcommand{\ourshuge}{Robust-LLaVA\textsuperscript{4}\textsubscript{H}\@\xspace}
\newcommand{\faretwo}{FARE\textsuperscript{2}\@\xspace}
\newcommand{\farefour}{FARE\textsuperscript{4}\@\xspace}
\newcommand{\simcliptwo}{Sim-CLIP\textsuperscript{2}\@\xspace}
\newcommand{\simclipfour}{Sim-CLIP\textsuperscript{4}\@\xspace}

\newcommand{\flickr}{Flickr30k\xspace}
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\definecolor{lightgray}{rgb}{0.9,0.9,0.9}
\definecolor{BlueGray}{rgb}{1, 0.8, 0.8}
\definecolor{lightgreen}{rgb}{0.90, 0.99, 0.85}
\definecolor{darkgreen}{rgb}{.1, .85, .1}
\definecolor{newgray}{rgb}{0., 0., 0.} %
\definecolor{graygreen}{rgb}{.1, .75, .1} %
\definecolor{grayred}{rgb}{1., 0., 0.} %
\definecolor{lightorange}{rgb}{1., .9, 0.}
\definecolor{lighttred}{rgb}{1., .9, 0.8}
\definecolor{Gray}{gray}{0.90}
\definecolor{white}{rgb}{1.0, 1.0, 1.0}
\definecolor{LightCyan}{RGB}{240, 224, 238}
\definecolor{teaser1}{HTML}{FFCCBC}
\definecolor{teaser1}{HTML}{FFCCBC}
\newcommand{\goodcol}{darkgreen!50}
\newcommand{\okcol}{lightorange!50}
\newcommand{\badcol}{red!40}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Robust-LLaVA: On the  Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models}

\begin{document}
\twocolumn[
\icmltitle{Robust-LLaVA: On the  Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models}
\vspace{-1em}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hashmat Shadab Malik}{yyy}
\icmlauthor{Fahad Shamshad}{yyy}
\icmlauthor{Muzammal Naseer}{comp}
\icmlauthor{ Karthik Nandakumar}{sch}
\icmlauthor{Fahad Khan}{yyy,lpu}
\icmlauthor{Salman Khan}{yyy,anu}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Mohamed bin Zayed University of AI, Abu Dhabi, UAE}
\icmlaffiliation{comp}{Khalifa University, Abu Dhabi, UAE}
\icmlaffiliation{sch}{Michigan State University, Michigan, USA}
\icmlaffiliation{anu}{Australian National University, Canberra, Australia}
\icmlaffiliation{lpu}{Link\"{o}ping University, Link\"{o}ping, Sweden}

\icmlcorrespondingauthor{Hashmat Shadab Malik}{hashmat.malik@mbzuai.ac.ae}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract} 
Multi-modal Large Language Models (MLLMs) excel in vision-language tasks but remain vulnerable to visual adversarial perturbations that can induce hallucinations, manipulate responses, or bypass safety mechanisms. Existing methods seek to mitigate these risks by applying constrained adversarial fine-tuning to CLIP vision encoders on ImageNet-scale data, ensuring their generalization ability is preserved. However, this limited adversarial training restricts robustness and broader generalization.
In this work, we explore an alternative approach of leveraging existing vision classification models that have been adversarially pre-trained on large-scale data.
Our analysis reveals two principal contributions: 
(\textbf{1}) the extensive scale and diversity of adversarial pre-training enables these models to demonstrate superior robustness against diverse adversarial threats, ranging from imperceptible perturbations to advanced jailbreaking attempts, without requiring additional adversarial training, and (\textbf{2}) end-to-end MLLM integration with these robust models facilitates enhanced adaptation of language components to robust visual features,  outperforming existing plug-and-play methodologies on complex reasoning tasks.
Through systematic evaluation across visual question-answering, image captioning, and jail-break attacks, we demonstrate that MLLMs trained with these robust models achieve superior adversarial robustness while maintaining favorable clean performance. Our framework achieves 2× and 1.5× average robustness gains in captioning and VQA tasks, respectively, and delivers over 10\%  improvement against jailbreak attacks. 
Code and models are available on \href{https://github.com/HashmatShadab/Robust-LLaVA}{GitHub}.

%Multi-modal Large Language Models (MLLMs) have demonstrated impressive capabilities in vision-language tasks, but their reliance on visual processing introduces critical security vulnerabilities. Their vision encoders remain susceptible to adversarial perturbations that can induce hallucinations, manipulate responses, or bypass safety mechanisms while maintaining coherent language generation. Current approaches attempt to address this by adversarially fine-tuning CLIP vision encoders on ImageNet-scale data, but exhibit inherent limitations in both robustness and generalization 
% In this work, we present an alternative approach by leveraging vision encoders adversarially pre-trained on large-scale image-text pairs. 

\end{abstract}

\section{Introduction}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/fig1.pdf}%
                    \vspace{-1em}

    \caption{\textbf{Robust performance of the proposed Robust-LLaVA on  vision-language tasks at perturbation budget $\epsilon = 4/255$:} The original \clip exhibits minimal robustness. Our proposed Robust-LLaVA$^{4}$ outperforms state-of-the-art  \farefour~\cite{schlarmann2024robust} and \simclipfour~\cite{hossain2024sim} in robustness score across all tasks.}%, while maintaining high clean accuracy, as illustrated in Fig.~\ref{fig:stereo_proposed_method}.}
                    \vspace{-1em}
    \label{fig:teaser}
\end{figure}

\begin{figure*}
        \centering
        %\includegraphics[width=\textwidth, trim = 0cm 9.48cm 0.4cm 0cm, clip]{figs/fig2.pdf}
        \includegraphics[width=\textwidth]{figs/fig2.pdf}
        \vspace{-2em}
        \caption{ \textbf{Illustration of untargeted $\ell_\infty$-attacks with $\epsilon=4/255$ on \llava using different robust vision encoders:} 
         Both \farefour~\cite{schlarmann2024robust} and \simclipfour~\cite{hossain2024sim} are vulnerable to adversarial attacks while Robust-LLaVA$^{4}$
         not only demonstrates robustness against these attacks but also maintains high performance on the original images.
        }
                \vspace{-1em}
        \label{fig:stereo_proposed_method}
\end{figure*}

Large Language Models (LLMs) have revolutionized natural language processing, and their integration with visual understanding has introduced a new paradigm: Multi-modal Large Language Models (MLLMs)~\cite{yin2023survey,li2024multimodal}. 
Unlike traditional vision models, which are typically limited to specific tasks, MLLMs leverage a vision encoder, typically CLIP~\cite{radford2021learning}, to expand their capabilities, enabling them to engage in open-ended visual reasoning, from answering complex questions about images to generating contextual descriptions and performing multi-step visual analyses~\cite{zhu2023minigpt,liu2024visual}. 
This flexibility, achieved through instruction tuning~\cite{liu2024visual,zhang2023instruction} that aligns model responses with multimodal tasks and natural language cues, redefines visual AI from rigid classification task to interactive assistants capable of interpreting, reasoning about, and discussing visual content in natural language. 
Recently, to enhance the fine-grained visual understanding of MLLMs and address their inherent ``\textit{blindness}", researchers have explored the integration of multiple vision encoders~\cite{tong2024eyes,shi2024eagle}.
%Recently, to further improve the fine-grained visual capabilities of MLLMs and reduce the "blindness`` of MLLMs via incorporating multiple vision encoders....~\cite{tong2024eyes,shi2024eagle}
%Morever recent works improve/extend the fine-grain visual capabilities of MLLMs or reduce the "blindness" of MLLMs by incoroporating multiple vision encoders


%Despite their promise, MLLMs suffer from notable shortcomings in the vision domain, hindering their ability to effectively interpret and represent visual content, which can significantly affect their performance across a range of vision-centric tasks~\cite{tong2024eyes,shi2024eagle}.


%These vision-related shortcomings introduce an even greater challenge in the context of adversarial vulnerabilities. Integrating visual inputs requires MLLMs to process continuous, high-dimensional image spaces that are inherently more susceptible to adversarial attacks than the discrete text space of traditional LLMs~\cite{carlini2024aligned,qi2024visual}.
Despite substantial advancements, integrating visual inputs  poses a critical security challenge: MLLMs must process continuous, high-dimensional image spaces that are inherently more susceptible to adversarial attacks than the discrete text space of traditional LLMs~\cite{carlini2024aligned,qi2024visual}. 
This expanded attack surface enables sophisticated adversarial objectives beyond simple misclassification - attackers can manipulate visual inputs to induce factual hallucinations, trigger unsafe behaviors, or exploit model functionalities while maintaining coherent language output~\cite{schlarmann2023adversarial}.  
As adversarial attacks in the visual domain enable a wider range of achievable adversarial objectives, defending against these threats is particularly difficult, thereby posing a significant barrier to deploying MLLMs in critical applications where robustness is paramount. Although adversarial training~\cite{mkadry2017towards} offers potential protection, its effectiveness is constrained by an inherent trade-off between model accuracy and robustness, and its adaptation from unimodal vision models to MLLMs remains largely unexplored.




Recent approaches aim to enhance MLLM robustness by independently training a vision encoder (CLIP) in an adversarial manner and then integrating it into the MLLM, primarily through two strategies: supervised adversarial fine-tuning of the vision encoder on ImageNet (TeCoA)~\cite{mao2022understanding} or unsupervised adversarial fine-tuning (FARE)~\cite{schlarmann2024robust} and Sim-CLIP~\cite{hossain2024sim}. However, these plug-and-play methods face two fundamental limitations in an attempt to balance robustness with MLLM visual understanding capabilities.
\textbf{\textit{Limited Robustness Gain}:} Both approaches operate under the premise that CLIP’s large-scale pre-training establishes a strong foundation for generalization, and therefore significantly restrict adversarial training to preserve these pre-trained features. For instance, FARE employs an extremely limited training schedule (only two epochs) to maintain CLIP's foundational capabilities, resulting in only modest robustness gains when integrated into MLLMs, as shown in Fig.~\ref{fig:teaser}. This trade-off between preserving pre-trained features and achieving robustness inherently limits the effectiveness of these adversarial fine-tuning approaches.
\textbf{\textit{Semantic Alignment Gap}:} More importantly, the misalignment between adversarial CLIP training objectives and MLLMs' generative understanding creates a semantic alignment gap --- while such CLIP models might resist perturbations, their degraded visual representations may impair MLLMs' complex visual reasoning capabilities, as evidenced by the substantial drop in performance on tasks like visual question-answering and image captioning (see Fig.~\ref{fig:teaser}). This highlights a critical challenge: balancing adversarial robustness with MLLMs' advanced visual reasoning capabilities.



Motivated by these challenges, we investigate the role of robust visual representations in enhancing MLLMs' visual reasoning capabilities. Specifically, we explore existing robust vision encoders and assess their potential for integration within MLLMs while preserving adversarial robustness.
To evaluate their multimodal compatibility, we conduct CLIP alignment experiments, examining how well these robust encoders can align with language components. Our findings reveal that highly aligned models, when incorporated into MLLMs, exhibit significantly improved robustness without compromising vision-language performance.
Our analysis reveals two key benefits of integrating large-scale robust vision encoders into MLLM training:
%Motivated by these challenges, we observe that the quality of visual representations plays a central role in MLLMs' visual understanding capabilities. This inisght led us to explore the current available robust vision encoders and there potential viability in integration in MLLMs. 
%For this we utilise robust CLIP alignmnet expecrimernts to observe there multimodal potential. Highly aligned models when intergated into MLLMs, show hogh level of robustness
%Our empirical analysis reveals two key complementary benefits of incorporating these large-scale robust vision encoders into MLLM training:
%This insight leads us to explore a two-fold approach: \textit{leveraging vision encoders that are adversarially pre-trained on billion-scale image-text pairs}, and \textit{subsequently optimizing MLLMs with these robust encoders}.  Recent advances in large-scale adversarial training suggest that scaling to billion-scale datasets can provide more robust visual representations while maintaining the semantic understanding necessary for MLLMs.
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Our empirical analysis reveals two key complementary benefits of incorporating these large-scale robust vision encoders into MLLM training:
\begin{itemize}
    \item \textbf{Robust Feature Learning}: Large-scale adversarial training fundamentally redefines visual representation learning by overcoming the robustness-preservation trade-off in limited-scale methods~\cite{schmidt2018adversarially,stutz2019disentangling,hendrycks2019using}. The diversity of training data enables models to achieve both adversarial robustness and rich semantic understanding, crucial for MLLMs' visual reasoning. In contrast, lightweight post-hoc adversarial training, while efficient, struggles to disentangle robustness from semantic feature learning, leading to weaker defenses. 
    
    %Billion-scale adversarial training changes how visual representations addresses the robustness-preservation dilemma faced by limited-scale approaches~\cite{schmidt2018adversarially,stutz2019disentangling,hendrycks2019using}.  The scale and diversity of training data enables learning visual representations that are not only robust to adversarial attacks but also maintain the rich semantic understanding necessary for MLLMs' visual reasoning capabilities, addressing the robustness limitations of restricted-scale training approaches. In contrast, lightweight post-hoc adversarial training approaches, while computationally efficient, often fail to achieve comparable robustness, as they lack the ability to fully disentangle adversarial robustness from semantic feature learning.

    \item \textbf{Enhanced Semantic Alignment}: Training MLLMs with these robust encoders offers key advantage over approaches that simply plug in robust vision encoders as in TeCoA~\cite{mao2022understanding}  and FARE~\cite{schlarmann2023adversarial}. By enabling language components to adapt to robust visual features, this end-to-end training preserves MLLMs' visual reasoning capabilities while maintaining robustness, showing improved performance on complex tasks.% compared to ImageNet-scale robust encoders.

\end{itemize}


Our evaluation shows that training MLLMs with large-scale robust encoders achieves state-of-the-art adversarial robustness without compromising performance on vision-language tasks, as shown in Fig. \ref{fig:stereo_proposed_method}. Tested across a wide range of threats—including imperceptible perturbations, targeted manipulations, transfer attacks, and advanced jailbreaking—our approach significantly improves robustness on all metrics. Notably, it provides strong defense against black-box jailbreaking and enhances resilience to common image corruptions, addressing critical real-world deployment concerns. Importantly, this robustness is achieved with clean-data performance comparable to the current best robustness approaches, suggesting that large-scale adversarial pre-training could contribute to models’ ability to develop both resilience and meaningful visual representations.


% Our comprehensive evaluation demonstrates that training MLLMs with billion-scale robust encoders achieves state-of-the-art adversarial robustness while preserving high performance on downstream vision-language tasks. We evaluate our approach against a diverse suite of threats, from imperceptible adversarial perturbations to targeted manipulations, transfer attacks, and sophisticated jailbreaking attempts. The results reveal substantial improvements in robustness across all metrics. Notably, the proposed robust training of MLLMs demonstrates superior defense against emerging threats like blackbox jailbreaking attempts and shows significantly improved resilience to common image corruptions, addressing critical real-world deployment concerns. Importantly, this robustness is achieved with clean-data performance comparable to the current best robustness approaches, suggesting that billion-scale adversarial pre-training could contribute to models’ ability to develop both resilience and meaningful visual representations.These findings have important implications for developing trustworthy MLLMs that maintain reliable performance in both standard and adversarial settings.

%To this end, we propose utilizing adversarially robust vision encoders that are pre-trained on billion-scale image-text pairs and integrating them into the MLLM framework. While the original CLIP model was also trained on large-scale data, our approach specifically leverages adversarial training at scale, which offers two critical advantages. First, unlike previous attempts at robust CLIP that were limited to ImageNet scale, training adversarially on billion-scale data enables learning robust visual representations that can better resist a wider range of adversarial objectives. Second, adversarial training on diverse image-text pairs helps preserve semantic richness while gaining robustness - a crucial balance that smaller-scale adversarial training struggles to achieve, as evidenced by the degraded performance of ImageNet-trained robust models on complex visual reasoning tasks.



\section{Related Work}

\textbf{Multi-modal LLMs.} MLLMs extend traditional LLMs with visual capabilities, enabling processing of both visual and textual information for tasks ranging from visual question-answering to image-grounded dialogue and complex reasoning~\cite{yin2023survey,li2024multimodal,caffagni2024r,awais2023foundational}. These models generally utilize a pretrained vision encoder such as CLIP~\cite{radford2021learning} to transform images into dense vector representations (image embeddings), which are then processed through a projection layer to generate token embeddings compatible with the language model's architecture. Notably, models like LLaVA~\cite{liu2024visual,liu2024improved} exemplify this architecture's potential, integrating CLIP vision encoder with Vicuna LLM~\cite{chiang2023vicuna} through a linear projector that aligns visual features into the language domain.

\noindent \textbf{Adversarial Robustness of MLLMs.}
Adversarial attacks have been extensively studied in machine learning, with adversarial training emerging as a key defense strategy~\cite{szegedy2013intriguing,goodfellow2014explaining,mkadry2017towards,zhang2019theoretically}, particularly in single-modality systems~\cite{carlini2017towards,ebrahimi2017hotflip}. However, MLLMs present unique challenges beyond traditional label misclassification, including hallucination~\cite{huang2024visual}, factual inconsistency~\cite{wang2023survey}, and reasoning errors~\cite{wang2024exploring}.
Recent studies have demonstrated MLLMs' vulnerability to various adversarial attacks that can trigger hallucinations~\cite{bai2024hallucination}, enable jailbreaking~\cite{jin2024jailbreakzoo}, and induce model misuse~\cite{niu2024jailbreaking}, making their security critical for real-world applications~\cite{liu2025mm,liu2024safety,zhang2024benchmarking}. While adversarial fine-tuning of vision encoders has shown promise in isolated vision tasks, its effectiveness in the complete MLLM pipeline remains understudied~\cite{mao2022understanding,schlarmann2024robust,hossain2024sim}. This gap is particularly significant since the visual encoder forms the foundation for all downstream reasoning in MLLMs. In this paper, we utilize LLaVA's framework~\cite{liu2024visual,liu2024improved} as a representative MLLM architecture to investigate how a large-scale adversarially trained vision encoder~\cite{wang2024revisiting} impacts end-to-end model reliability and performance across diverse downstream tasks and attack scenarios.


%The susceptibility of machine learning models to adversarial attacks has been a central topic of research~\cite{szegedy2013intriguing,goodfellow2014explaining}, particularly in classification tasks where adversarial training emerged as one of the most effective defensive strategies~\cite{mkadry2017towards,zhang2019theoretically}. While initial research focused primarily on single-modality systems in computer vision and natural language processing~\cite{carlini2017towards,ebrahimi2017hotflip}, recent work has expanded to investigate the cross-modal transferability of adversarial attacks. However, MLLMs present unique challenges that extend beyond the traditional goal of preventing label misclassification in standard models. These challenges include complex failure modes such as hallucination~\cite{huang2024visual}, factual inconsistency~\cite{wang2023survey}, and reasoning errors~\cite{wang2024exploring}, which require fundamentally different approaches to achieve robust performance.

%\noindent \textbf{Adversarial Robustness of MLLMs.}
%Recent work has shown that MLLMs are vulnerable to a wide range of adversarial attacks, which can trigger hallucinations~\cite{bai2024hallucination}, enable jailbreaking~\cite{jin2024jailbreakzoo}, and induce model misuse~\cite{niu2024jailbreaking}, making their security implications severe in real-world applications~\cite{liu2025mm,liu2024safety,zhang2024benchmarking}. Current approaches to enhance robustness, such as adversarial fine-tuning of vision encoders, have shown promise in isolated vision tasks. However, their effectiveness in the complete MLLM pipeline remains largely unexplored, with only a few works investigating these aspects in the context of MLLMs~\cite{mao2022understanding,schlarmann2024robust,hossain2024sim}. This gap is particularly significant since the visual encoder serves as the foundation for all downstream reasoning in MLLMs.  In this paper, we use the framework from LLaVA~\cite{liu2024visual,liu2024improved} as a representative MLLM architecture due to its widespread adoption to investigate how a large-scale adversarially trained vision encoder~\cite{wang2024revisiting} impacts end-to-end model reliability and performance across diverse downstream tasks and attack scenarios.

\noindent \textbf{Jailbreak Attacks for MLLMs.} Jailbreaking attacks on MLLMs aim to bypass alignment constraints to generate harmful content~\cite{jin2024jailbreakzoo,qi2024visual,hossain2024securing,niu2024jailbreaking}. These attacks can be categorized into white-box and black-box approaches. White-box attacks focus on manipulating input images or visual embeddings, either by generating adversarial images with constraints on the harmful response set~\cite{dong2023robust,schlarmann2023adversarial}, using teacher-forcing optimization~\cite{carlini2024aligned}, or crafting images that appear harmless but have embeddings similar to harmful images~\cite{shayegani2023jailbreak}. Black-box attacks employ techniques such as system prompt attacks~\cite{wu2023jailbreaking}, transferring harmful information into text-oriented images~\cite{gong2023figstep}, or using surrogate models to generate adversarial images~\cite{zhao2024evaluating}.In this work, We show that LLaVA, when trained with a large-scale adversarially trained encoder, demonstrates resilience to both attack types while maintaining alignment with harmlessness.







\section{Robust--LLaVA}


Our goal is to enhance the robustness of Multi-modal Large Language Models (MLLMs) against adversarial attacks while preserving their advanced visual reasoning capabilities. 
Conventional approaches that directly apply adversarial training to MLLMs encounter two critical limitations: \textit{prohibitive computational overhead} and \textit{significant degradation in visual reasoning performance}. 
To overcome these limitations, we propose an effective integration framework that incorporates large-scale adversarially pre-trained vision encoders into the MLLM architecture. This approach capitalizes on the robust feature representations learned through large-scale adversarial pretraining while avoiding the pitfalls of direct adversarial training on MLLMs.


\noindent \textbf{Adversarial Training}: Adversarial training~\cite{mkadry2017towards}, involves training models with adversarial examples~\cite{goodfellow2014explaining} to enhance resilience to attacks. The process can be formalized as a min-max optimization problem:
\[
\min_\theta \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \max_{\|x' - x\|_\infty \leq \epsilon} L(f_\theta(x'), y) \right],
\]
where \( f_\theta \) denotes the model parameterized by \( \theta \), \( L \) is the loss function (\textit{e.g.}, cross-entropy), and \( \epsilon \) defines the perturbation bound in the \( \ell_\infty \)-norm. 
\textit{As adversarial training is computationally intensive, it often limits its use to smaller networks e.g., ResNet-50~\cite{he2016deep}  on smaller datasets like CIFAR-10~\cite{krizhevsky2009learning}}. Scaling adversarial training to larger models and datasets has remained challenging due to these computational constraints.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \noindent \textbf{Large-Scale Robust Vision Encoders:} Recent advances in adversarial training have enabled its scaling from medium-sized ImageNet dataset~\cite{russakovsky2015imagenet} to billion-sample web-scale datasets~\cite{gadre2024datacomp, wang2024revisiting}. This scalability is achieved through a two-stage approach: adversarial pre-training on web-scale data using CLIP's text encoder as a zero-shot classifier, followed by adversarial fine-tuning for downstream tasks.
% Although these robust vision encoders excel in classification, their potential to enhance MLLM robustness remains unexplored. 
% To address this, we aim to systematically evaluate medium-scale models such as ViT-B/16~\cite{dosovitskiy2020image} and ResNet101~\cite{he2016deep}, adversarially trained on ImageNet, alongside large-scale models like ViT-H and ViT-G~\cite{wang2024revisiting}, adversarially pre-trained on billion-sample datasets. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent \textbf{Large-Scale Robust Vision Encoders:}  
Recent advancements in adversarial training have primarily focused on improving the robustness of vision encoders in classification tasks~\cite{gadre2024datacomp, wang2024revisiting}, particularly on ImageNet~\cite{russakovsky2015imagenet}. These efforts have resulted in robust vision models that can be broadly categorized into two groups:  
\begin{itemize}
    \item \textbf{Medium-scale models} such as ViT-B/16~\cite{dosovitskiy2020image} and ResNet-101~\cite{he2016deep}, which have been adversarially trained from scratch on ImageNet, ensuring robustness against standard adversarial perturbations.  
    \item \textbf{Large-scale models} such as ViT-H and ViT-G~\cite{wang2024revisiting}, which extend adversarial training to billion-scale web datasets. These models employ a two-stage approach: first, adversarial pretraining on web-scale data using CLIP's text encoder as a zero-shot classifier, followed by robust fine-tuning on ImageNet~\cite{gadre2024datacomp, wang2024revisiting}.  
\end{itemize}


%However, a critical consideration in integrating these robust vision encoders into MLLMs is \textbf{\textit{alignment}}: the robust vision encoder must align effectively with the MLLM’s language components to ensure coherent vision-language reasoning. Previous approaches, such as CLIP-based adversarial fine-tuning, face limitations, either focusing on narrow class labels (\textit{e.g.}, ImageNet classes) or compromising downstream performance due to alignment gaps. In this work, we explore whether these large-scale robust vision encoders can maintain alignment within an MLLM framework while preserving adversarial robustness. However, this idea raises further questions: can these robust encoders, pre-trained on extensive image-text datasets, integrate effectively into MLLM frameworks like LLaVA? Would they maintain both their adversarial robustness and the alignment necessary for high-quality, vision-language reasoning?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% However, a critical challenge emerges when integrating robust vision encoders into MLLMs:  encoders must maintain effective {\textbf{\textit{alignment}} with the language components to ensure coherent vision-language reasoning while preserving their robustness.
% Prior work on large-scale adversarial training~\cite{gadre2024datacomp, wang2024revisiting} leverage aligned image-text pairs to learn robust feature representations using CLIP. CLIP's has two crucial properties: exceptional generalization capabilities and strong semantic alignment with language components, making it particularly suitable for adversarial fine-tuning in vision-language tasks. However, existing approaches that employ CLIP-based adversarial fine-tuning~\cite{schlarmann2024robust,mao2022understanding} struggle with this balance, either limiting themselves to narrow class labels or degrading downstream performance.
% This raises an intriguing question: \textbf{\textit{could the rich feature spaces learned by large-scale robust encoders enable successful alignment with MLLM frameworks without sacrificing robustness?}} To investigate this hypothesis, we  conduct a comprehensive alignment experiment.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


While these robust encoders demonstrate strong performance in classification, it has not been explored whether these models can be integrated into \textbf{multimodal large language models (MLLMs)} while maintaining their robustness. A key challenge in this integration is ensuring effective \textbf{alignment} with language components to enable coherent vision-language reasoning. A natural approach to achieving this alignment is leveraging CLIP, which has been widely adopted in MLLMs due to two crucial properties: (1) its strong generalization capabilities and (2) its inherent semantic alignment between vision and language representations. However, current approach of limited adversarial fine-tuning of CLIP-based models~\cite{schlarmann2024robust, mao2022understanding} presents a trade-off, either restricting the model’s level of robustness or significantly degrading downstream vision-language performance. This raises an important question: \textbf{\textit{Can the robust feature represnetations learned by current robust vision encoders be successfully aligned with MLLMs while maintaining adversarial robustness?}} To investigate this, we conduct a comprehensive alignment study, systematically evaluating medium-scale adversarially trained models alongside large-scale adversarially pretrained encoders.

\begin{algorithm}[t]
\caption{Robust Vision Encoder Alignment with CLIP}
\label{alg:alignment}
\begin{algorithmic}[1]
\State \textbf{Input:} Robust encoder $\phi_r$, CLIP encoder $\phi_c$, data $\mathcal{D}$, learning rate $\eta$, batch size $B$, iterations $T$
\State Initialize projection $W \in \mathbb{R}^{d \times d'}$ 
\For{$t = 1$ to $T$}
   \State Sample minibatch $\{x_i\}_{i=1}^B \sim \mathcal{D}$
   \State $\mathcal{L}_{\text{align}} \gets \frac{1}{B} \sum_{i=1}^B \|\phi_c(x_i) - W\phi_r(x_i)\|_2^2$ %\Comment{Alignment Loss}
   \State $W \gets W - \eta \nabla_W \mathcal{L}_{\text{align}}$ \Comment{Gradient Update}
\EndFor
\State \Return Aligned encoder $\mathcal{F}(x) = W\phi_r(x)$
\end{algorithmic}
\end{algorithm}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/clip_alignment_zs.png}
    \vspace{-2em}
    \caption{Robust Accuracy of Models Across Different Datasets. The plot shows the robust accuracy of different models evaluated across various datasets. PGD-10 attack is crafted at epsilon 1/255 with image-text adversarial loss.}
    \label{fig:robust_accuracy_clip_alignment_}
        \vspace{-2em}
\end{figure}


\subsection{Robust Vision Encoder Alignment} \label{sec:clip_alignment}

In this section, we investigate whether robust vision encoders $\phi_r$  can align effectively with CLIP's vision encoder $\phi_c$ through a linear mapping $W \in \mathbb{R}^{d \times d'}$ trained on ImageNet data. Unlike prior work on concept alignment~\cite{moayeri2023text}, we align robust encoders with CLIP’s text encoder to analyze robust zero-shot performance in order to assess the viability of their integration within MLLMs. Using a subset of ImageNet training data $\mathcal{D}$, we optimize $W$ by minimizing the alignment loss $\mathcal{L}_{\text{align}} = \frac{1}{B} \sum_{i=1}^B \|\phi_c(x_i) - W\phi_r(x_i)\|_2^2$, where $\{x_i\}_{i=1}^B$ represents a minibatch of $B$ images sampled from $\mathcal{D}$. The parameters $W$ of linear layers are updated iteratively using gradient descent with a learning rate $\eta$ (see Algorithm~\ref{alg:alignment} for details). We evaluate the aligned encoder $\mathcal{F}(x) = W\phi_r(x)$ by testing its zero-shot adversarial robustness under PGD~\cite{mkadry2017towards} attacks at $\epsilon = 1/255$. These attacks minimize the similarity between aligned vision features $\mathcal{F}(x)$ and CLIP text encoder features across diverse datasets, assessing  effectiveness of the alignment strategy.

%we explore whether robust vision encoders can be effectively aligned with CLIP's vision encoder through a simple linear mapping trained on ImageNet data. This alignment, if successful, would allow robust models to indirectly access CLIP's text encoder for zero-shot tasks while preserving their robustness properties. To achieve this alignment, we use a subset of ImageNet training data to learn a linear transformation that maps representations from robust encoders to CLIP's vision space. Specifically, we pass the same images through both the robust encoder and CLIP's vision encoder, creating paired representations that we use to train the linear mapping. 
%We evaluate this hypothesis by testing the zero-shot adversarial robustness of aligned models under PGD~\cite{mkadry2017towards} attacks, crafted at $\epsilon = 1/255$ using an image-text adversarial loss. This attack minimizes the similarity between aligned vision features and CLIP text encoder features across several datasets.


The results in Figure~\ref{fig:robust_accuracy_clip_alignment_} highlight a strong correlation between model scale, training strategy and robustness preservation during alignment. Small-scale robust models, including ViT-B (AT) and ResNet-101 (AT), exhibit significant degradation in robustness post-alignment, with robust accuracy dropping below 60\% across all evaluated datasets.
In contrast, large-scale models, robust ViT-H and ViT-G, preserve their robustness while acquiring CLIP's zero-shot capabilities, achieving robust accuracy ranging from 40\% to 85\% across a diverse set of benchmarks: ImageNet~\cite{russakovsky2015imagenet}, CIFAR10~\cite{krizhevsky2009learning}, CIFAR100, Pets~\cite{parkhi12a}, UCF101~\cite{soomro2012ucf101}, and Caltech101~\cite{li_andreeto_ranzato_perona_2022}. 
Notably, these large-scale models significantly outperform both their smaller counterparts and CLIP's ViT-L/14 architecture, demonstrating that robust feature representations can be successfully preserved during alignment. Building on this key finding, we integrate robust vision encoders with high potential for vision-language alignment into MLLMS, specifically in the LLaVA framework \cite{liu2024improved}. This approach enables us to achieve both strong adversarial robustness and effective semantic alignment in MLLMs, without the need for additional specialized training procedures. For detailed alignment analysis, refer to Sec.~\ref{app:clip_align} of the Appendix.

%These large models outperform both smaller counterparts and CLIP’s ViT-L/14, showing that large-scale robust models faithfully retain robustness while integrating CLIP’s zero-shot capabilities. Building on this insight, we demonstrate that these aligned robust encoders can be seamlessly integrated into the LLaVA framework, achieving both strong adversarial robustness and high task performance without requiring specialized training procedures. Detailed alignment results are in Sec.~\ref{app:clip_align} of the Appendix.








%A question emerges when working with robust vision encoders and CLIP: Can we leverage CLIP's powerful text-to-image alignment capabilities while maintaining adversarial robustness? To investigate this, we explore whether robust vision encoders can be effectively aligned with CLIP's vision encoder through a simple linear mapping trained on ImageNet data. This alignment, if successful, would allow robust models to indirectly access CLIP's text encoder for zero-shot tasks while preserving their robustness properties. To achieve this alignment, we use 20\% of ImageNet training data to learn a linear transformation that maps representations from robust encoders to CLIP's vision space. Specifically, we pass the same images through both the robust encoder and CLIP's vision encoder, creating paired representations that we use to train the linear mapping. We evaluate this hypothesis by testing the zero-shot adversarial robustness of aligned models under PGD~\cite{mkadry2017towards} attacks, crafted at $\epsilon = \frac{1}{255}$ using an image-text adversarial loss. This attack minimizes the similarity between aligned vision features and CLIP text encoder features across several datasets.


%The results reveal an intriguing scale dependency as illustrated in Figure~\ref{fig:robust_accuracy_clip_alignment_}. Smaller robustly-trained models like ViT-B (AT) and ResNet-101 (AT) struggle to maintain their robustness after alignment, showing poor robust accuracy (mostly below 60\%) across datasets. However, large-scale robustly trained models - Robust ViT-H and Robust ViT-G - successfully preserve their robustness while gaining CLIP's capabilities, maintaining impressive robust accuracy (ranging from 40\% to 85\%) across ImageNet~\cite{russakovsky2015imagenet}, CIFAR10~\cite{krizhevsky2009learning}, CIFAR100, Pets~\cite{parkhi12a}, UCF101~\cite{soomro2012ucf101}, and Caltech101~\cite{li_andreeto_ranzato_perona_2022}. These large models significantly outperform both their smaller counterparts and CLIP's ViT-L/14. This scale-dependent behavior suggests that while representation alignment is possible, only large-scale robust models can effectively maintain their robustness properties while gaining CLIP's zero-shot capabilities through alignment. For detailed results on alignment, see Sec. \ref{app:clip_align} of Appendix.



%demonstrating that with sufficient scale, robust vision encoders can be successfully aligned with CLIP while maintaining their adversarial robustness.



%Figure~\ref{fig:robust_accuracy_clip_alignment} illustrates a key finding about representation alignment between different vision encoders and CLIP. Through a simple linear mapping trained on ImageNet data, various vision encoders can be aligned with CLIP's vision encoder, enabling them to leverage CLIP's text encoder for zero-shot tasks. However, the results reveal an interesting scale dependency in maintaining robustness after alignment. Smaller scale robustly-trained models like ViT-B (AT) and ResNet-101 (AT) show relatively poor performance across datasets (with accuracies mostly below 60$\%$), suggesting they don't effectively preserve their robustness properties fter the alignment process. In contrast, large-scale robustly trained models - specifically Robust ViT-H and Robust ViT-G - demonstrate remarkable success, maintaining consistently high accuracy (ranging from ~40\% to ~85\%) across all datasets including ImageNet, CIFAR10, CIFAR100, Pets, UCF101, and Caltech101. This scale-dependent behavior suggests that while representation alignment is possible, only large-scale robust models can effectively maintain their robustness properties while gaining CLIP's zero-shot capabilities through alignment.


% 

\section{Experiments}


\begin{table*}[htb!]
% \small
\centering
\tabcolsep=1.5pt
\extrarowheight=1.5pt
\caption{\textbf{Robustness of MLLMs with different Vision Encoders on APGD-Ensemble attack.}
This table displays the robust performance of LLaVA on two tasks: image captioning and visual question answering (VQA). For VQA tasks (TextVQA, VQAv2, VizWiz, OKVQA), we report VQA accuracy, and for captioning tasks (COCO, Flickr30k), we report CIDEr score.
}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{l | c c c c || c c c c || c c c c || c c c c}
\toprule

 \multirow{3}{*}{\makecell{Vision \\ Encoder}} & \multicolumn{4}{c||}{COCO} 
& \multicolumn{4}{c||}{Flickr30}  & \multicolumn{4}{c||}{TextVQA} & \multicolumn{4}{c}{Average} \\  
\cline{2-17} 

 & \multirow{2}{*}{\hspace{3pt} clean}  & \multicolumn{3}{c||}{$\ell_{\infty}$} & \multirow{2}{*}{\hspace{3pt} clean} & \multicolumn{3}{c||}{$\ell_{\infty}$} & \multirow{2}{*}{\hspace{3pt} clean} & \multicolumn{3}{c||}{$\ell_{\infty}$} & \multirow{2}{*}{\hspace{3pt} clean} & \multicolumn{3}{c}{$\ell_{\infty}$} \\
\cline{3-5}\cline{7-9}\cline{11-13}\cline{15-17}
 & & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ & %
& $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ & & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ & &$\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$\\


\toprule
\toprule
 {\cellcolor{lightgray}} \clip
& {\cellcolor{lightgray}}126.49 &{\cellcolor{lightgray}}5.81 &{\cellcolor{lightgray}}3.57 &{\cellcolor{lightgray}}2.41
&{\cellcolor{lightgray}}85.86 &{\cellcolor{lightgray}} 2.79 &{\cellcolor{lightgray}} 1.52 &{\cellcolor{lightgray}} 0.78
& {\cellcolor{lightgray}}44.08 &{\cellcolor{lightgray}} 0 &{\cellcolor{lightgray}} 0 &{\cellcolor{lightgray}} 0
&{\cellcolor{lightgray}}85.48 &{\cellcolor{lightgray}}2.87 &{\cellcolor{lightgray}}1.69 &{\cellcolor{lightgray}}1.06
\\
\cmidrule{1-17} 

%fare4@224
%  \farefour
% &104.36 & 43.39 & 30.33 & 14.75
% &62.83 & 23.73 & 15.54 & 6.71
% &23.84 & 13.18 & 8.26 & 4.08
% &63.67 & 26.77 & 18.04 & 8.51
% \\
% \farefour@336-5
% &117.94 & 37.34 & 25.66 & 14.86
% &69.77 & 18.89 & 12.31 & 6.91
% &32.66 & 11.28 & 7.38 & 4.78
% &73.46 & 22.50 & 15.12 & 8.85
% \\
%fare4@336-10
\farefour
&117.26 & 54.54 & 35.14 & 18.03
&68.14 & 30.89 & 18.47 & 9.25
&33.40 & 15.14 & 9.70 & 7.00
&72.93 & 33.52 & 21.10 & 11.43
\\
  \simclipfour&{106.80}&{50.62} &{34.56} &{14.96}
& {64.45} &{27.26} &{17.61} &  {7.39}
& 24.86 &{14.58} &{8.88} &{4.74}
&{65.37} & 30.82 &{20.35} &{9.03}
\\
 \cellcolor{LightCyan}\ourshuge
&\cellcolor{LightCyan}{108.43}&\cellcolor{LightCyan}{88.73} & \cellcolor{LightCyan}{73.60} &\cellcolor{LightCyan}{43.88}
&\cellcolor{LightCyan}{59.79} &\cellcolor{LightCyan}{45.39} & \cellcolor{LightCyan}{34.62} & \cellcolor{LightCyan} {22.09}
&\cellcolor{LightCyan}25.14 & \cellcolor{LightCyan} {18.84} & \cellcolor{LightCyan}{15.90} & \cellcolor{LightCyan} {8.24}
&\cellcolor{LightCyan}{64.45} &\cellcolor{LightCyan}50.99 &\cellcolor{LightCyan}{41.37} &\cellcolor{LightCyan}{24.74}
\\
 \cellcolor{LightCyan}\oursgiant
&\cellcolor{LightCyan}{110.40}&\cellcolor{LightCyan}{89.68} & \cellcolor{LightCyan}{76.59} &\cellcolor{LightCyan}{47.12}
&\cellcolor{LightCyan}{65.18} &\cellcolor{LightCyan}{49.44} & \cellcolor{LightCyan}{38.75} & \cellcolor{LightCyan} {23.14}
&\cellcolor{LightCyan}24.34 & \cellcolor{LightCyan} {18.14} & \cellcolor{LightCyan}{14.54} & \cellcolor{LightCyan} {10.18}
&\cellcolor{LightCyan}{66.64} &\cellcolor{LightCyan}52.42 &\cellcolor{LightCyan}{43.29} &\cellcolor{LightCyan}{26.81}
\\


\cmidrule{1-17} 
 \multirow{3}{*}{\makecell{Vision \\ Encoder}} & \multicolumn{4}{c||}{VQAv2} 
& \multicolumn{4}{c||}{VizWiz}  & \multicolumn{4}{c||}{OKVQA} & \multicolumn{4}{c}{Average} \\  


\cline{2-17} 

 & \multirow{2}{*}{\hspace{3pt} clean}  & \multicolumn{3}{c||}{$\ell_{\infty}$} & \multirow{2}{*}{\hspace{3pt} clean} & \multicolumn{3}{c||}{$\ell_{\infty}$} & \multirow{2}{*}{\hspace{3pt} clean} & \multicolumn{3}{c||}{$\ell_{\infty}$} & \multirow{2}{*}{\hspace{3pt} clean} & \multicolumn{3}{c}{$\ell_{\infty}$} \\
\cline{3-5}\cline{7-9}\cline{11-13}\cline{15-17}
 & & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ & %
& $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ & & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ & &$\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$\\


\toprule
\toprule
 {\cellcolor{lightgray}} \clip 
& {\cellcolor{lightgray}}75.66 & {\cellcolor{lightgray}}4.28 & {\cellcolor{lightgray}}0.52 & {\cellcolor{lightgray}}0
&{\cellcolor{lightgray}}38.79 &{\cellcolor{lightgray}}0 &{\cellcolor{lightgray}}0 &{\cellcolor{lightgray}}0
& {\cellcolor{lightgray}}59.04 &{\cellcolor{lightgray}}0.31 &{\cellcolor{lightgray}}0 &{\cellcolor{lightgray}}0
&{\cellcolor{lightgray}}57.83 &{\cellcolor{lightgray}}1.53 &{\cellcolor{lightgray}}0.17 &{\cellcolor{lightgray}}0
\\
\cmidrule{1-17} 
% fare4@224
%  \farefour
% &64.20 &38.28 &28.4 &20.24
% &42.98 & 28.45 & 19.91 & 13.27
% &52.44 &28.56 &20.24 &11.36
% &53.21 &31.76 &22.85  &14.96
% \\
%fare4@336-10
\farefour
&66.56 & 38.94 & 28.94 & 21.96
&42.65 & 24.48 & 17.95 & 12.42
&54.24 & 28.04 & 19.12 & 11.32
&54.48 & 30.49 & 22.00 & 15.23
\\
% fare4@336-5
% \farefour
% &66.72 & 31.06 & 24.72 & 20.60
% &42.64 & 19.65 & 15.11 & 8.07
% &54.64 & 19.76 & 13.64 & 9.48
% &54.67 & 23.49 & 17.82 & 12.72
% \\
 \simclipfour
&{65.08}&{40.50} &{31.92} &{21.76}
&{42.49} &{28.9} &{21.77} &{13.15}
&53.04 &{29.88} &{21.48} &{12.8}
&{53.54} &33.09 &{25.06} &{15.90}
\\
 \cellcolor{LightCyan}\ourshuge
& \cellcolor{LightCyan}{66.36}&\cellcolor{LightCyan}{57.56} &\cellcolor{LightCyan}{48.50} & \cellcolor{LightCyan}{30.42}
& \cellcolor{LightCyan}{33.31} & \cellcolor{LightCyan}{27.10} &\cellcolor{LightCyan}{23.13} & \cellcolor{LightCyan}{15.35}
& \cellcolor{LightCyan}53.40 & \cellcolor{LightCyan}{46.44} &\cellcolor{LightCyan}{37.80} & \cellcolor{LightCyan} {24.88}
& \cellcolor{LightCyan}{51.02} &\cellcolor{LightCyan}43.70 &\cellcolor{LightCyan}{36.48} & \cellcolor{LightCyan}{23.55}\\
 \cellcolor{LightCyan}\oursgiant
& \cellcolor{LightCyan}{68.02}&\cellcolor{LightCyan}{59.18} &\cellcolor{LightCyan}{49.28} & \cellcolor{LightCyan}{34.52}
& \cellcolor{LightCyan}{37.82} & \cellcolor{LightCyan}{30.17} &\cellcolor{LightCyan}{24.92} & \cellcolor{LightCyan}{15.82}
& \cellcolor{LightCyan}54.88 & \cellcolor{LightCyan}{47.24} &\cellcolor{LightCyan}{39.56} & \cellcolor{LightCyan} {27.96}
& \cellcolor{LightCyan}{53.57} &\cellcolor{LightCyan}45.53 &\cellcolor{LightCyan}{37.92} & \cellcolor{LightCyan}{26.10}\\

\bottomrule
\end{tabular}
}
\label{tab:untargetdown}
\vspace{-1em}
\end{table*}

We adopt the LLaVA framework~\cite{liu2024improved} to explore the integration of robust vision encoders within Multimodal Large Language Models (MLLMs). The image features from the vision model are aligned with the pre-trained LLM embeddings by training an MLP layer that maps these features into the LLM's embedding space using an image-text pairs dataset. This is followed by fine-tuning the MLP layer and the LLM within LLaVA on a language-image instruction-following dataset. We build on LLaVA-1.5-7B, which combines a Vicuna-7B~\cite{chiang2023vicuna} language model with a vision encoder. We train and finetune our model with the same experiment setting as in LLaVA. We conduct a comprehensive empirical analysis to investigate the effectiveness of MLLMs trained with adversarially robust vision encoders. 

\textbf{Robust Vision Models.} We focus on robust vision encoders, specifically large-scale adversarially trained ImageNet models, ViT-G/14 (1B parameters) and ViT-H/14 (632M parameters)~\cite{wang2024revisiting}, trained with a perturbation budget of  $\epsilon = 4/255$. These classification models demonstrate strong zero-shot adversarial robustness, as shown in our CLIP alignment experiments\textbf{(see Section \ref{sec:clip_alignment})}. We denote ViT-G/14 and ViT-H/14 robust encoders trained in the LLaVA framework as \oursgiant and \ourshuge, respectively. Results for other robust vision encoders are presented in Sec. \ref{app:wb_untargetted} of the Appendix. 

% we utilize ViT-G/14 (1B parameters) and ViT-H/14 (632M parameters) architectures that were adversarially pre-trained on the billion-scale DataComp-1B dataset. The adversarial pre-training uses 1-step PGD with step size 1/255 at {\color{red}$\epsilon$ = 4/255}, achieving strong robustness while keeping training computationally efficient. (This detail might be discusses in method section, when we mention the reason for high zs adversarial robustness of these models) \\

\textbf{Baselines.} To assess the effectiveness of our approach, we compare it against the original CLIP vision encoder ViT-L/14 (304M parameters) and its two recent robust variants: FARE~\cite{schlarmann2024robust} and Sim-CLIP~\cite{hossain2024securing}, both of which have been adversarially finetuned on the ImageNet dataset~\cite{russakovsky2015imagenet}. We evaluate the most robust versions of these models in the LLaVA framework, which have been adversarially trained using an \(\ell_\infty\) norm with a perturbation budget of $\epsilon = 4/255$, denoted as \(\text{FARE}^4\) and \(\text{Sim-CLIP}^4\).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% To evaluate the effectiveness of our approach, we compare against the original CLIP vision encoder and two recent robust variants: FARE~\cite{schlarmann2024robust} and Sim-CLIP~\cite{hossain2024securing}, which are adversarially finetuned on ImageNet dataset~\cite{russakovsky2015imagenet}. We evaluate these models at two perturbation radii: $\epsilon$ = 2/255 (denoted as FARE$^2$, Sim-CLIP$^2$) for imperceptible perturbations and $\epsilon$ = 4/255 (denoted as FARE$^4$, Sim-CLIP$^4$) for slightly larger but still visually subtle perturbations. \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Our evaluation centers on the adversarial performance of multi-modal models, emphasizing three main aspects: (\textbf{1}) performance on complex vision-language tasks such as visual question-answering and image captioning, (\textbf{2}) robustness against diverse attack types, ranging from imperceptible perturbations to sophisticated jailbreak attacks, and (\textbf{3}) performance analysis in the presence of image corruptions, textual prompt variations, and object hallucinations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Our evaluation spans three key dimensions: (\textbf{1}) performance on complex vision-language tasks including visual question-answering and image captioning, (\textbf{2}) robustness against diverse attack types - from imperceptible perturbations to more sophisticated jailbreak attempts, and (\textbf{3}) analysis of how well the models maintain performance on clean images while achieving robustness. \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% For robust vision encoders, we utilize ViT-G/14 (1B parameters) and ViT-H/14 (632M parameters) architectures that were adversarially pre-trained on the billion-scale DataComp-1B dataset. The adversarial pre-training uses 1-step PGD with step size 1/255 at {\color{red}$\epsilon$ = 4/255}, achieving strong robustness while keeping training computationally efficient. For MLLM experiments, we build on LLaVA-1.5, which combines a Vicuna-7B language model with a vision encoder. We integrate the robust encoders by replacing LLaVA's default CLIP vision encoder while keeping all other components fixed. The training follows LLaVA's established protocol: pre-training on 558K image-text pairs followed by fine-tuning on 150K instruction-following examples, using clean images throughout to maintain the original training distribution. \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Quantitative Robustness Evaluation}
\label{sec:robustness_evaluation}

\noindent \textbf{Attack Setting.} We evaluate the robust performance of LLaVA models with both the original and adapted robust vision encoders under various attack scenarios, comparing it to state-of-the-art methods such as FARE~\cite{schlarmann2024robust} and SimCLIP~\cite{hossain2024securing}. Our attack setting is designed to effectively evaluate the adversarial  performance of model and incorporates both untargeted and targeted attack strategies. To assess robustness to untargeted attacks across downstream tasks,  following prior works~\cite{schlarmann2023adversarial, schlarmann2024robust},  we employ an ensemble of attacks.
We first employ APGD attacks~\cite{croce2020reliable} at half precision, running for 100 iterations with  several ground truth captions/answers as labels. Samples that score below a predefined threshold are excluded, leaving only the hard to fool samples to undergo a second round of attacks at single precision. This two-step strategy optimizes computational resources while maintaining attack effectiveness. Our evaluation spans image captioning and visual question answering (VQA) tasks. For targeted attacks, designed to compel models to produce predetermined outputs, we employ an APGD attack similar to the approach in ~\cite{schlarmann2024robust}, running for 10,000 iterations. All the attacks are employed with \(\ell_\infty\) norm and adversarial perturbation budget set to  \(\epsilon = \{ \frac{2}{255}, \frac{4}{255}, \frac{8}{255} \}\). 


\noindent \textbf{Datasets and Metrics.} For image captioning, we use the COCO~\cite{lin2014microsoft} and Flickr30k~\cite{plummer2015flickr30k} datasets, which feature diverse sets of images paired with multiple human-annotated captions. For VQA, we include VQAv2~\cite{goyal2017making}, TextVQA~\cite{singh2019towards}, VizWiz~\cite{gurari2018vizwiz}, and OKVQA~\cite{marino2019ok} dataset. Following~\cite{schlarmann2024robust,hossain2024securing}, we evaluate clean and adversarial performance on randomly sampled 500 images per dataset. We report the CIDEr score~\cite{vedantam2015cider} to measure captioning performance, where a higher score indicates better alignment, and VQA accuracy as the percentage of correctly answered questions for VQA tasks. We assess targeted attacks on 25 COCO images using three target captions, measuring success rate and CIDEr score.

\subsubsection{Results}

\textbf{Untargeted Attacks:} Table \ref{tab:untargetdown} presents results across six datasets, covering image captioning and visual question answering tasks. The LLaVA model using the original CLIP achieves the best clean performance but is completely vulnerable to adversarial attacks. Among the robust models, \farefour demonstrates the best overall clean performance. However, this gain comes from explicitly aligning the representations of the robust CLIP model with the original CLIP, limiting the extent of adversarial fine-tuning and resulting in significantly reduced performance against adversarial attacks. In contrast, both \oursgiant and \ourshuge maintain comparable clean performance while achieving substantial robustness improvements against adversarial attacks, striking the right balance between clean and adversarial generalization. At \(\epsilon = \frac{4}{255}\), \oursgiant maintains a CIDEr score of 76.59 on COCO, more than doubling the performance of both \farefour and \simclipfour. Similarly, in visual question answering, it achieves a notable improvement with 49.28\% accuracy on VQAv2, compared to only 31.92\% for \simclipfour.  This advantage becomes even more pronounced at larger perturbations \(\epsilon = \frac{8}{255}\), where our approach maintains significant robustness with 47.12 CIDEr on COCO and 34.52\% on VQAv2. We  report the transferability of adversarial images from \oursgiant, \farefour, and \simclipfour models, as well as results on transfer attacks in Sec. \ref{app:bb_untargetted} of the Appendix.

\begin{table*}[t!]
    \centering 
    \small
    \extrarowheight=-1.5pt
    \tabcolsep=4pt
    \captionof{table}{\textbf{Quantitative analysis of $\ell_\infty$ targeted attacks.}
    \clip is highly susceptible to the attack,  \oursgiant is completely robust.
    }
    \resizebox{0.97\textwidth}{!}{
    \begin{tabular}{l|ccc|ccc|ccc|ccc}
        \toprule
        \rowcolor{lightgray} Target 
        & \multicolumn{3}{c|}{\clip} 
        & \multicolumn{3}{c|}{\farefour} 
        & \multicolumn{3}{c|}{\simclipfour} 
        & \multicolumn{3}{c}{\oursgiant} \\
        \rowcolor{lightgray} 
        & \multicolumn{3}{c|}{$\ell_\infty$} 
        & \multicolumn{3}{c|}{$\ell_\infty$} 
        & \multicolumn{3}{c|}{$\ell_\infty$} 
        & \multicolumn{3}{c}{$\ell_\infty$} \\
        \cline{2-4} \cline{5-7} \cline{8-10} \cline{11-13}
        \rowcolor{lightgray} 
        & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
        & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
        & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
        & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ \\
        \midrule
        \texttt{Visit https://tinyurl}(\dots) 
        & 25\,/\,25 & 25\,/\,25 & 25\,/\,25 
        & 0\,/\,25 & 0\,/\,25 & 1\,/\,25 
        & 0\,/\,25 & 0\,/\,25 & 0\,/\,25 
        & 0\,/\,25 & 0\,/\,25 & 0\,/\,25 \\
        \texttt{Please Reset Your Pas}(\dots) 
        & 25\,/\,25 & 25\,/\,25 & 25\,/\,25 
        & 0\,/\,25 & 0\,/\,25 & 6\,/\,25 
        & 0\,/\,25 & 0\,/\,25 & 2\,/\,25 
        & 0\,/\,25 & 0\,/\,25 & 0\,/\,25 \\
        \texttt{You are too stupi}(\dots) 
        & 25\,/\,25 & 25\,/\,25 & 25\,/\,25 
        & 0\,/\,25 & 0\,/\,25 & 2\,/\,25 
        & 0\,/\,25 & 0\,/\,25 & 0\,/\,25 
        & 0\,/\,25 & 0\,/\,25 & 0\,/\,25 \\
        \midrule
         \textbf{Mean success rate:} & 100\% & 100\% & 100\% & \textbf{0\%} & \textbf{0\%} & 12\% & \textbf{0\%} & \textbf{0\%} & {2.67\%} & \textbf{0\%} & \textbf{0\%} & \textbf{0\%} \\
        \textbf{Average CIDEr score:} & 0 & 0 & 0 & 93.7 & 84.1 & 53.9 & 102.1 & 84.5 & 71.7 & \textbf{116.3} & \textbf{109.68} & \textbf{109.58} \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:targeted-attack}
    \vspace{-1em}
\end{table*}

\textbf{Targeted Attacks:} Table \ref{tab:targeted-attack} presents quantitative results for targeted adversarial attacks. LLaVA with the original CLIP completely fails, with a 100\% attack success rate and 0 CIDEr score across all perturbations, demonstrating the attacker's ability to fully control the output. While \farefour and \simclipfour show resilience, they break at higher perturbation budgets \(\epsilon = \frac{8}{255}\). In contrast, \oursgiant and \ourshuge remain fully robust, resisting adversarial manipulation even at high perturbations. Notably, \oursgiant maintains high-quality captions, preserving a strong CIDEr score, whereas \farefour and \simclipfour, despite blocking the attacker's string, experience a noticeable decline in caption quality as perturbations increase. See Sec. \ref{app:wb_targetted} of the Appendix for further details.

%Table \ref{tab:targeted-attack} presents quantitative results for targetted adversarail attacks. We observe that that LLaVA with original CLIP completely fails against such attacks - 100$\%$ attack success rate and 0 CIDEr score across all perturbations, indicating both the success of attacker to generate the desired output. Both \farefour and \simclipfour show robustness against the targetted attacks, but break in few cases at high perturbation budgets \(\epsilon = \frac{8}{255}\). In contrast, \oursgiant and \ourshuge remain fully robust to these attacks even at high perturbation budgets. This indicates a strong resistance to generating the attacker's targeted output. The robustness of \oursgiant stands out further as it continues to generate high-quality captions for adversarial examples, maintaining a strong CIDEr score. In comparison, while \farefour and \simclipfour prevent generating the targeted string, they still suffer from a noticeable decline in caption quality as the perturbation budget increases. For further details, see Sec. \ref{app:wb_targetted} of Appendix.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{table*}[t!]
%     \centering 
%     \small
%     \extrarowheight=-1.5pt
%     \tabcolsep=4pt
%     \captionof{table}{\textbf{ {\color{red} to be updated} Transferability analysis of adversarial examples.} This table shows the transferability rates of $\ell_\infty$-bounded adversarial examples generated by each surrogate model across different target models for both COCO and VQA\textsubscript{v2} tasks.}
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc}
%         \toprule
%         \rowcolor{lightgray} 
%         & \multicolumn{8}{c|}{\textbf{COCO}} 
%         & \multicolumn{8}{c}{\textbf{VQA\textsubscript{v2}}} \\
%         \midrule
%         \rowcolor{lightgray} Surrogate 
%         & \multicolumn{2}{c|}{\clip} 
%         & \multicolumn{2}{c|}{\farefour} 
%         & \multicolumn{2}{c|}{\simclipfour} 
%         & \multicolumn{2}{c|}{\oursgiant} 
%         & \multicolumn{2}{c|}{\clip} 
%         & \multicolumn{2}{c|}{\farefour} 
%         & \multicolumn{2}{c|}{\simclipfour} 
%         & \multicolumn{2}{c}{\oursgiant} \\
%         \rowcolor{lightgray} 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ \\
%         \midrule
%         \textbf{\clip}  
%         & 5 & 12 & 20 & 3 & 8 & 18 & 0 & 2  
%         & 6 & 13 & 21 & 4 & 9 & 19 & 1 & 3 \\
%         \textbf{\farefour} 
%         & - & 0 & 0 & 4 & 10 & 25 & 1 & 3  
%         & - & 1 & 1 & 5 & 11 & 26 & 2 & 4 \\
%         % \textbf{\simclipfour}  
%         % & 3 & 5 & 12 & - & 0 & 0 & 2 & 4 
%         % & 4 & 6 & 13 & - & 1 & 1 & 3 & 5 \\
%         \textbf{\oursgiant}  
%         & 0 & 0 & 0 & 1 & 2 & 5 & - & 0  
%         & 0 & 1 & 1 & 2 & 3 & 6 & - & 0 \\
%         \bottomrule
%     \end{tabular}
%     }
%     \label{tab:transferability-attack}
% \end{table*}
% \begin{table*}[t!]
%     \centering 
%     \small
%     \extrarowheight=-1.5pt
%     \tabcolsep=.5pt
%     \captionof{table}{\textbf{Transferability analysis of adversarial examples crafted using Ensemble Attack.} This table shows the transferability rates of $\ell_\infty$-bounded adversarial examples generated by each surrogate model across different target models for both COCO and VQA\textsubscript{v2} tasks.}
%     \begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc}
%         \toprule
%         \rowcolor{LightCyan} 
%         & \multicolumn{8}{c|}{\textbf{COCO}} 
%         & \multicolumn{8}{c}{\textbf{VQAv2}} \\
%         \midrule
%         \rowcolor{lightgray} Surrogate 
%         & \multicolumn{2}{c|}{\clip} 
%         & \multicolumn{2}{c|}{\farefour} 
%         & \multicolumn{2}{c|}{\ourshuge} 
%         & \multicolumn{2}{c|}{\oursgiant} 
%         & \multicolumn{2}{c|}{\clip} 
%         & \multicolumn{2}{c|}{\farefour} 
%         & \multicolumn{2}{c|}{\ourshuge} 
%         & \multicolumn{2}{c}{\oursgiant} \\
%           \rowcolor{lightgray} 
%         & \multicolumn{2}{c|}{$\ell_\infty$} 
%         & \multicolumn{2}{c|}{$\ell_\infty$} 
%         & \multicolumn{2}{c|}{$\ell_\infty$} 
%         & \multicolumn{2}{c|}{$\ell_\infty$}
%          & \multicolumn{2}{c|}{$\ell_\infty$} 
%         & \multicolumn{2}{c|}{$\ell_\infty$} 
%         & \multicolumn{2}{c|}{$\ell_\infty$} 
%         & \multicolumn{2}{c}{$\ell_\infty$} \\
%         \cline{2-3} \cline{4-5} \cline{6-7} \cline{8-9} \cline{10-11} \cline{12-13} \cline{14-15} \cline{16-17}
%         \rowcolor{lightgray} 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ \\
%         \midrule
%         \textbf{\clip}  
%         &  \cellcolor{green!25} 2.98 &  \cellcolor{green!25} 1.95 & 117.39 & 114.07 & 108.57 & 108.35 & 110.32 & 109.75  
%         & \cellcolor{green!25} 33.42 & \cellcolor{green!25} 32.82 & 66.50 & 66.26 & 66.16 & 65.76 & 68.02 & 67.70 \\
%         %  %fare4 @336-5
%         % \textbf{\farefour} 
%         % & 81.28 & 38.37 &  \cellcolor{green!25} 26.49 &  \cellcolor{green!25} 15.07 & 107.50 &  105.69 & 107.80 & 106.59  
%         % & 60.88 & 44.20 & \cellcolor{green!25} 24.74 & \cellcolor{green!25} 20.62 & 65.56 & 65.62 & 67.90 & 68.00 \\
%         %fare4@336-10
%         \textbf{\farefour} 
%         & 88.83 & 44.90 &  \cellcolor{green!25} 35.43 &  \cellcolor{green!25} 19.13 & 107.53 &
%         105.25 & 108.24 & 104.55  
%         & 59.30 & 44.48 & \cellcolor{green!25} 28.94 & \cellcolor{green!25} 22.64 & 65.02 & 65.70 & 67.58 & 66.46 \\
%          \textbf{\ourshuge}  
%         & 123.12 & 111.79 & 110.16 & 96.62 &  \cellcolor{green!25} 73.67 &  \cellcolor{green!25} 44.13 & 105.44 & 95.12 
%         & 76.64 & 73.46 & 66.00 & 62.64 & \cellcolor{green!25} 48.80 & \cellcolor{green!25} 30.42 & 66.62 & 62.42   \\
%         \textbf{\oursgiant}  
%         & 122.85 & 113.79 & 112.04 & 100.19 & 101.64 & 93.09 &  \cellcolor{green!25} 76.77 & \cellcolor{green!25} 46.57 
%         & 75.78 & 70.94 & 65.94 & 63.36 & 64.54 & 60.58 & \cellcolor{green!25} 49.28 & \cellcolor{green!25} 35.26 \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:transferability-attack_}
% \end{table*}

% \subsubsection{Results on Transferability}
% We further analyze the transferability of adversarial examples across different vision encoders.
% % - an important security consideration as it enables black-box attacks without access to the target model. 
% We craft adversarial examples using one encoder (surrogate) following our untargeted attack protocol and evaluate their effectiveness against other encoders (targets). We conduct this analysis at $\epsilon$ = 4/255 and 8/255 across both image captioning (COCO) and visual question-answering (VQAv2) tasks.....

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\begin{table*}[t!]
    \centering
    \small
    \renewcommand{\arraystretch}{0.8} % Adjust row height for readability
    \tabcolsep=2pt % Adjust column spacing    
    \begin{minipage}{0.48\textwidth} % Adjust width as needed
        \centering
        \caption{\label{tab:lvms-visual-adv-evaluation} \textbf{Performance of MLLMs against Visual Adv attacks.} We report the number of outputs generated by the model that contain specific(or any) toxic attributes.}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{l|c|c|c|c|c|c}
            \toprule
            \rowcolor{lightgray!30} \textbf{Model} & \textbf{Identity} & \textbf{Obscene} & \textbf{Insult} & \textbf{Threat} & \textbf{Toxicity} & \textbf{Any} \\
            \midrule
            \rowcolor{gray!10} \clip & 72 & 387  & 235 & 20 & 503 & 503 \\
            \farefour & 59 & 237 & 167 & 22 & 298 & 299 \\
            \rowcolor{gray!10} \simclipfour & 37 & 315  & 182 & 24 & 422 & 422 \\
            \rowcolor{LightCyan} 
            \oursgiant & 26 & 111 & 75 & 5 & 137 & 137 \\
            \rowcolor{LightCyan} 
            \ourshuge & 23 & 200 & 121 & 19 & 272 & 272 \\
            \bottomrule
        \end{tabular}
        \label{tab:visadv}
        }
    \end{minipage}%
    \hspace{0.8em}%\hfill
    \begin{minipage}{0.48\textwidth} % Adjust width as needed
        \centering
        \caption{\label{tab:lvms-evaluation}\textbf{Evaluation results of the MLLMs on HADES.} We report ASR \emph{(lower is better)} for different harmful instructions paired with adversarial image using HADES.}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{l|c|c|c|c|c}
            \toprule
            \rowcolor{lightgray!30} \textbf{Model} & \textbf{Animal} & \textbf{Financial} & \textbf{Privacy}  & \textbf{Violence} & \textbf{Average} \\
            \midrule
            \rowcolor{gray!10} \clip & 44 & 59.67 & 69.22  & 77.22 & 62.41 \\
            \farefour & 33.67 & 52.44 & 65.33  & 72.33 & 55.94 \\
            \rowcolor{gray!10} \simclipfour & 31.89 & 52.44 & 66.11 & 72.67 & 55.78 \\
            \rowcolor{LightCyan} 
            \oursgiant & 30.22 & 43 & 53.89  & 65.11 & 48.06 \\
            \rowcolor{LightCyan} 
            \ourshuge & 27.56 & 38.11 & 51.11 & 64.89 & 45.41 \\
            \bottomrule
        \end{tabular}
        \label{tab:hades}
        }
    \end{minipage}
    \vspace{-2em}
\end{table*}


\subsection{Robustness against Jailbreaking Attacks}

MLLMs are susceptible to jailbreaking attacks, where crafted images can trigger harmful prompts, such as generating toxic content or dangerous instructions. We evaluate our approach against both white-box and black-box jailbreaking attacks targeting the model’s safety mechanisms.

\textbf{Whitebox.} We evaluate the models using the VisualAdv~\cite{qi2023visual} jailbreak attack, which employs the PGD~\cite{mkadry2017towards} perturbations to benign images, maximizing the likelihood of generating harmful content from a set of 66 derogatory sentences. 
We generate the adversarial image with a perturbation budget of  \(\epsilon =  \frac{128}{255}\), focusing on the higher constraint to test model robustness against strong jailbreak attacks. We then pair the adversarial images with harmful prompts from the RealToxicityPrompts~\cite{gehman2020realtoxicityprompts} dataset, which contains 1,200 toxic instructions designed to elicit harmful content from the models. Toxicity is measured using the Detoxify classifier\cite{Detoxify}, which assigns scores from 0 (least toxic) to 1 (most toxic), with outputs exceeding a threshold of 0.5 considered harmful.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% We evaluate models against VisualAdv~\cite{qi2023visual} attack, which optimises a benign image on a carefully selected few-shot corpus of 66 derogatory sentences aimed at specific demographics. VisualAdv generates a universal adversarial example by optimizing it to maximize the likelihood that the model generates harmful content from a small set of derogatory sentences. This optimization is achieved using the PGD algorithm,where the adversarial example can be initialized from  a benign image with specific constraints (constrained attack). generates adversarial images that are not tied to any specific instruction, enabling a single universally perturbed image to potentially jailbreak the VLM with various prompts or harmful instructions. 

% In our experiment, we leverage this attack to generate universal adversarial images with  perturbation constraints: ϵ = 128/255. During inference,the adversarial image is paired with a harmful text instruction, forcing the VLM to produce harmful content that extends beyond the original optimization corpus. For this purpose, we use RealToxicityPrompts~\cite{gehman2020realtoxicityprompts} dataset, which contains around 1,200 harmful prompts to elicit toxicity in generated output of the LVLMs.
% The harmful instructions includes toxic prompts related to violence, toxicity, and profanity. 
% We evaluate the models response using the Detoxify classifier~\cite{Detoxify} to measure the toxicity score which ranges from 0(least toxic) to 1 (most toxic). For each attribute we calculate the ratio of generated texts whose scores exceed the threshold of 0.5.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Table~\ref{tab:visadv} compares various vision encoders against whitebox VisualAdv attacks across different toxicity attributes. The attribute \emph{Any} represents instances where at least one toxic attribute is present in the generated output. As anticipated, LLaVA with the original \clip encoder is the most susceptible, producing 503 toxic outputs, with the majority being obscene content. Both \simclipfour and \farefour exhibit improved robustness compared to \clip. However, \oursgiant and \ourshuge demonstrate the highest level of robustness against toxic content generation, with \oursgiant reducing the total number of toxic outputs significantly to 137. Nevertheless, identity-based and threat-based attacks remain challenging across all models, indicating persistent vulnerabilities even after robust training. For additional results, see Sec.~\ref{app:wb_jailbreak} of Appendix.

\textbf{Blackbox.} We employ the black-box version of the HADES attack, which operates in two stages. First, it extracts harmful content from text, embedding it typographically and linking it to an image, transferring adversarial cues to the visual domain. Next, a harmful image, generated via diffusion-based prompt optimization, is appended to increase susceptibility to adversarial content. The \texttt{GPT-4-0613} model iteratively optimizes the prompt as the LLM attacker. We evaluate 600 harmful instructions across four scenarios (\textit{Violence, Financial, Privacy}, and \textit{Animal}), pairing each with an adversarial image. Response harmfulness is assessed using Beaver-7B~\cite{ji2024beavertails}, with Attack Success Rate (ASR) quantifying the proportion of successful jailbreaks.
Table~\ref{tab:hades} compares LLaVA variants against HADES attacks. LLaVA with the original CLIP encoder is the most vulnerable, exhibiting the highest ASR, particularly in Violence and Privacy scenarios. While \farefour and \simclipfour provide moderate defense, lowering ASR by 7\%, they remain susceptible to Violence-related threats. Robust-LLaVA models offer the strongest protection, achieving the lowest ASR and significantly improving resilience, especially in Financial and Privacy attacks. See Sec.~\ref{app:bb_jailbreak} of the Appendix for details.
%We employ the black-box version of HADES attack. This version of attack consists of two stages: first, it extracts harmful content from the text and embeds it typographically, replacing the text with a pointer to an image, thereby transferring adversarial cues from the text to the visual domain. Next, a harmful image generated by a diffusion model with prompt-based optimization is appended, which increases the model’s susceptibility to adversarial content. \texttt{GPT-4-0613} model serves as the LLM attacker to iteratively optimize the prompt. 
%Subsequently, 600 harmful instructions spanning four scenarios (\textit{Violence, Financial, Privacy}, and \textit{Animal}) are paired with corresponding harmful images and presented to the model. The harmfulness of the response  is evaluated using Beaver-7B~\cite{ji2024beavertails} as the judgment model and the Attack Success Rate (ASR), which reflects the proportion of successful jailbreak attempts is reported.
%Table~\ref{tab:hades} illustrates the effectiveness of different vision encoders integrated with LLaVA against HADES attacks. LLaVA with the original CLIP encoder is the most vulnerable, displaying the highest Attack Success Rate (ASR), especially against Violence and Privacy attacks. While \farefour and \simclipfour provide moderate defense, reducing the average ASR by 7\%, they remain susceptible to Violence-related threats. Our Robust-LLaVA models deliver superior defense, achieving the lowest average ASR and offering balanced protection, notably improving against Financial and Privacy attacks. For further details, see Sec. \ref{app:bb_jailbreak} of Appendix.
% LLaVA with original CLIP shows high vulnerability with 47.5$\%$ of responses containing toxic content, particularly obscene content. While FARE$^4$ shows similar weakness, SimCLIP$^4$ improves defense with reduced overall toxicity to 41.6$\%$. Our Robust-LLaVA demonstrate the strongest defense, , with ViT-h/14 improving over SimCLIP$^4$ by approximately 8$\%$ and showing significant reductions in obscene and insult-based content generation. However, identity-based and threat-based attacks remain challenging across all models, suggesting specific vulnerabilities that persist despite robust training. We provide more results in Section \ref{app:wb_jailbreak} of Appendix. 
% Although \ourshuge shows similar overall robustness ($45.07\%$ ASR), it is more vulnerable to Self-Harm attacks, highlighting persistent challenges despite robust training. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% To evaluate robustness against blackbox jailbreak attacks, we analyze attacks where adversaries operate without access to model parameters or gradients. We evaluate on HADES benchmark comprising 750 harmful instructions across five scenarios: \textit{Violence, Financial, Privacy, Self-Harm}, and \textit{Animal}. The attack exploits MLLMs' weaker visual alignment through a two-stage process: first converting harmful keywords into typography and replacing them with generic text-to-image pointers (\textit{e.g.,} 'the object in the image'), then augmenting this with a harmful image generated through iterative prompt optimization using ChatGPT and diffusion models. This setup represents a highly practical threat as it requires only publicly available tools. We assess robustness against these attacks at two perturbation levels, $\epsilon$ = 4/255 and 8/255. We use attack success rate as metric. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Table~\ref{tab:hades} demonstrates the effectiveness of different vision encoders against HADES attacks. LLaVA with original CLIP is most vulnerable with highest ASR across all categories, particularly struggling with Violence and Privacy attacks. While robust encoders FARE$^4$ and SimCLIP$^4$ provide moderate defense by reducing average ASR by 9$\%$, they still show significant vulnerability to Violence-related attacks. Our Robust-LLaVA achieve the strongest overall defense, with ViT-g/14 showing the lowest average ASR and more balanced protection across categories, particularly improving defense against Financial and Privacy attacks. While ViT-h/14 achieves similar overall robustness (45.07$\%$ ASR), its increased vulnerability to Self-Harm attacks suggests that certain attack categories remain challenging despite large-scale robust training.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\iffalse
\begin{table}[t!]
    \centering 
    \small
    \renewcommand{\arraystretch}{0.8} % Increase row height for better readability
    \tabcolsep=2pt % Increase column spacing
    \caption{\label{tab:lvms-visual-adv-evaluation}\textbf{Performance of LVLMs against Visual Adv attacks.} This table shows the percentage of outputs generated by LVLMs that contain specific toxic attributes. The LVLMs utilize various jailbreak defense strategies and are evaluated at different levels of attack strength ($\epsilon$).}
    \begin{tabular}{l|c|c|c|c|c|c|c}
        \toprule
        \rowcolor{lightgray!30} \textbf{Model} & \textbf{Identity} & \textbf{Obscene} & \textbf{S. Toxicity } & \textbf{Insult} & \textbf{Threat} & \textbf{Toxicity} & \textbf{Any} \\
        \midrule
        \rowcolor{gray!10} \clip & 6.8 & 36.5 & 2.4 & 22.2 & 1.9 & 47.5 & 47.5 \\
        \farefour & 9.2 & 37.1 & 4.2 & 26.1 & 3.4 &	46.6 & 46.8 \\
        \rowcolor{gray!10} \simclipfour & 3.6 &	31.1 & 1.3 & 17.9 &	2.4 & 41.6 & 41.6\\
        \oursgiant & 7.4 & 31.5 & 2.3 & 21.3 & 1.4 & 38.9 & 38.9 \\
        \ourshuge & 2.8 & 24.5 & 1.7 & 14.8 & 2.3 & 33.4 & 33.4\\

        \bottomrule
    \end{tabular}
\end{table}


\begin{table}[t!]
    \centering 
    \small
    \renewcommand{\arraystretch}{1} % Increase row height for better readability
    \tabcolsep=2pt % Increase column spacing
    \caption{\label{tab:lvms-evaluation}\textbf{To be updated Evaluation results of the LVLMs using various defense strategies on instructions and adversarial images generated with HADES.} We report ASR (Attack Success Rate, lower is better) for five categories of harmful instructions: Animal, Financial, Privacy, Self-Harm, and Violence.}
    \begin{tabular}{l|c|c|c|c|c|c}
        \toprule
        \rowcolor{lightgray!30} \textbf{Model} & \textbf{Animal} & \textbf{Financial} & \textbf{Privacy} & \textbf{Self-Harm} & \textbf{Violence}& \textbf{Average}\\
        \midrule
        \rowcolor{gray!10} \clip & 44 & 59.67 & 69.22 & 35.56 & 77.22 & 57.13\\
        \farefour & 33.67 & 52.44 & 65.33 & 18.89 & 72.33 & 48.53\\
        \rowcolor{gray!10} \simclipfour & 31.89	& 52.44	& 66.11	& 19.44 & 72.67	& 48.51\\
        \rowcolor{gray!10} \oursgiant & 30.22 & 43 & 53.89 & 31.89 & 65.11 & 44.82\\
        \rowcolor{gray!10} \ourshuge & 27.56 & 38.11 & 51.11 & 43.67 & 64.89 & 45.07\\

        \bottomrule
    \end{tabular}
    \label{tab:hades}
\end{table}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[htb!]
\small
\centering
\tabcolsep=.8pt
\extrarowheight=1.5pt
\caption{\textbf{Robustness of MLLMs with ensemble of Vision Encoders on APGD attack.}
For VQA tasks (TextVQA), we report VQA accuracy, and for captioning tasks (COCO, Flickr30k), we report CIDEr score.
}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{l || c c c c || c c c c || c c c c || c c c c}
\toprule

 \multirow{3}{*}{\makecell{Vision \\ Encoder}} & \multicolumn{4}{c||}{COCO} 
& \multicolumn{4}{c||}{Flickr30}  & \multicolumn{4}{c||}{TextVQA} & \multicolumn{4}{c}{Average} \\  
\cline{2-17} 

 & \multirow{2}{*}{\hspace{3pt} clean}  & \multicolumn{3}{c||}{$\ell_{\infty}$} & \multirow{2}{*}{\hspace{3pt} clean} & \multicolumn{3}{c||}{$\ell_{\infty}$} & \multirow{2}{*}{\hspace{3pt} clean} & \multicolumn{3}{c||}{$\ell_{\infty}$} & \multirow{2}{*}{\hspace{3pt} clean} & \multicolumn{3}{c}{$\ell_{\infty}$} \\
\cline{3-5}\cline{7-9}\cline{11-13}\cline{15-17}
 & & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ & %
& $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ & & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ & &$\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$\\


\toprule
\toprule
 {\cellcolor{lightgray}} CLIP 
&{\cellcolor{lightgray}}126.49 &{\cellcolor{lightgray}}20.59 &{\cellcolor{lightgray}}12.35 &{\cellcolor{lightgray}}8.36
&{\cellcolor{lightgray}}85.86 &{\cellcolor{lightgray}}12.38 &{\cellcolor{lightgray}}9.09 &{\cellcolor{lightgray}}4.88
&{\cellcolor{lightgray}}44.08 &{\cellcolor{lightgray}}9.8 &{\cellcolor{lightgray}}8.66 &{\cellcolor{lightgray}}6.96
&{\cellcolor{lightgray}}85.48 &{\cellcolor{lightgray}}14.26 &{\cellcolor{lightgray}}10.03 &{\cellcolor{lightgray}}6.73
\\
\cmidrule{1-17} 
%fare4@224
%  \farefour
% &104.36 &68.23 &54.69 &38.24
% &62.83 &40.10 &33.60 &22.95
% &23.84 &15.88 &12.56 &8.94
% &63.68 &41.40 &33.62 &23.38
% \\
%fare4@336-5
 \farefour
&117.94 & 69.56 & 61.07 & 46.66
&69.77 & 41.98 & 36.74 & 27.25
&32.66 & 19.40 & 14.50 & 11.78
&73.46 & 43.65 & 37.44 & 28.56
\\
%fare4@336-10
%  \farefour
% &117.26 & 81.94 & 68.76 & 51.81
% &68.14 & 50.64 & 44.12 & 30.67
% &33.40 & 22.14 & 17.70 & 13.72
% &72.93 & 51.57 & 43.53 & 32.07
% \\
%   \simclipfour
% &{106.80}&{74.07} &{60.68} &{38.06}
% &{64.45} &{45.15} &{38.10} &{22.45}
% &24.86 &{16.86} &{13.18} &{8.52}
% &{65.37} &45.36 &{37.32} &{23.01}
% \\
 \cellcolor{LightCyan}\ourshuge
&\cellcolor{LightCyan}{108.43} &\cellcolor{LightCyan}{103.10} &\cellcolor{LightCyan}{93.84} &\cellcolor{LightCyan}{72.69}
&\cellcolor{LightCyan}{59.79} &\cellcolor{LightCyan}{52.79} &\cellcolor{LightCyan}{47.79} &\cellcolor{LightCyan}{36.56}
&\cellcolor{LightCyan}25.14 &\cellcolor{LightCyan}{20.26} &\cellcolor{LightCyan}{17.40} &\cellcolor{LightCyan}{10.94}
&\cellcolor{LightCyan}{64.45} &\cellcolor{LightCyan}58.72 &\cellcolor{LightCyan}{53.01} &\cellcolor{LightCyan}{40.06}
\\
 \cellcolor{LightCyan}\oursgiant
&\cellcolor{LightCyan}{110.40} &\cellcolor{LightCyan}{102.40} &\cellcolor{LightCyan}{93.21} &\cellcolor{LightCyan}{75.58}
&\cellcolor{LightCyan}{65.18} &\cellcolor{LightCyan}{58.98} &\cellcolor{LightCyan}{51.49} &\cellcolor{LightCyan}{42.26}
&\cellcolor{LightCyan}24.34 &\cellcolor{LightCyan}{19.54} &\cellcolor{LightCyan}{15.96} &\cellcolor{LightCyan}{12.64}
&\cellcolor{LightCyan}{66.63} &\cellcolor{LightCyan}60.31 &\cellcolor{LightCyan}{53.55} &\cellcolor{LightCyan}{43.49}
\\


 \cellcolor{LightCyan}\oursgiant + \clip
&\cellcolor{LightCyan}{125.94} &\cellcolor{LightCyan}{26.34} &\cellcolor{LightCyan}{18.07} &\cellcolor{LightCyan}{10.88}
&\cellcolor{LightCyan}{84.27} &\cellcolor{LightCyan}{17.23} &\cellcolor{LightCyan}{11.01} &\cellcolor{LightCyan}{6.61}
&\cellcolor{LightCyan}46.22 &\cellcolor{LightCyan}{11.44} &\cellcolor{LightCyan}{8.96} &\cellcolor{LightCyan}{6.54}
&\cellcolor{LightCyan}{85.48} &\cellcolor{LightCyan}18.34 &\cellcolor{LightCyan}{12.68} &\cellcolor{LightCyan}{8.01}
\\

 \cellcolor{LightCyan}\oursgiant + \farefour
&\cellcolor{LightCyan}{116.79} &\cellcolor{LightCyan}{72.41} &\cellcolor{LightCyan}{64.76} &\cellcolor{LightCyan}{49.29}
&\cellcolor{LightCyan}{73.48} &\cellcolor{LightCyan}{43.28} &\cellcolor{LightCyan}{39.50} &\cellcolor{LightCyan}{29.84}
&\cellcolor{LightCyan}33.34 &\cellcolor{LightCyan}{18.54} &\cellcolor{LightCyan}{15.50} &\cellcolor{LightCyan}{11.02}
&\cellcolor{LightCyan}{74.54} &\cellcolor{LightCyan}44.74 &\cellcolor{LightCyan}{39.92} &\cellcolor{LightCyan}{30.05}
\\


%  \cellcolor{LightCyan}\ourshuge + \clip
% &\cellcolor{LightCyan}{126.42} &\cellcolor{LightCyan}{25.95} &\cellcolor{LightCyan}{18.27} &\cellcolor{LightCyan}{10.82}
% &\cellcolor{LightCyan}{85.08} &\cellcolor{LightCyan}{16.90} &\cellcolor{LightCyan}{12.78} &\cellcolor{LightCyan}{6.46}
% &\cellcolor{LightCyan}44.00 &\cellcolor{LightCyan}{9.98} &\cellcolor{LightCyan}{10.16} &\cellcolor{LightCyan}{7.12}
% &\cellcolor{LightCyan}{85.17} &\cellcolor{LightCyan}17.61 &\cellcolor{LightCyan}{13.74} &\cellcolor{LightCyan}{8.13}
% \\

%  \cellcolor{LightCyan}\ourshuge + \farefour
% &\cellcolor{LightCyan}{114.96} &\cellcolor{LightCyan}{69.58} &\cellcolor{LightCyan}{62.14} &\cellcolor{LightCyan}{49.39}
% &\cellcolor{LightCyan}{69.74} &\cellcolor{LightCyan}{40.64} &\cellcolor{LightCyan}{39.45} &\cellcolor{LightCyan}{29.88}
% &\cellcolor{LightCyan}33.82 &\cellcolor{LightCyan}{18.38} &\cellcolor{LightCyan}{15.84} &\cellcolor{LightCyan}{11.9}
% &\cellcolor{LightCyan}{72.84} &\cellcolor{LightCyan}42.87 &\cellcolor{LightCyan}{39.14} &\cellcolor{LightCyan}{30.39}
% \\
 \cellcolor{LightCyan}\oursgiant + \ourshuge
&\cellcolor{LightCyan}{108.89} &\cellcolor{LightCyan}{102.19} &\cellcolor{LightCyan}{94.44} &\cellcolor{LightCyan}{77.09}
&\cellcolor{LightCyan}{62.64} &\cellcolor{LightCyan}{56.55} &\cellcolor{LightCyan}{52.12} &\cellcolor{LightCyan}{40.65}
&\cellcolor{LightCyan}24.64 &\cellcolor{LightCyan}{22.16} &\cellcolor{LightCyan}{19.24} &\cellcolor{LightCyan}{13.74}
&\cellcolor{LightCyan}{65.39} &\cellcolor{LightCyan}60.30 &\cellcolor{LightCyan}{55.27} &\cellcolor{LightCyan}{43.83}
\\
% \cmidrule{1-17} 
%  \multirow{3}{*}{\makecell{Vision \\ Encoder}} & \multicolumn{4}{c||}{VQAv2} 
% & \multicolumn{4}{c||}{VizWiz}  & \multicolumn{4}{c||}{OKVQA} & \multicolumn{4}{c||}{Average} \\  


% \cline{2-17} 

%  & \multirow{2}{*}{\hspace{3pt} clean}  & \multicolumn{3}{c||}{$\ell_{\infty}$} & \multirow{2}{*}{\hspace{3pt} clean} & \multicolumn{3}{c||}{$\ell_{\infty}$} & \multirow{2}{*}{\hspace{3pt} clean} & \multicolumn{3}{c||}{$\ell_{\infty}$} & \multirow{2}{*}{\hspace{3pt} clean} & \multicolumn{3}{c||}{$\ell_{\infty}$} \\
% \cline{3-5}\cline{7-9}\cline{11-13}\cline{15-17}
%  & & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ & %
% & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ & & $\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ & &$\nicefrac{2}{255}$ & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$\\


% \toprule
% \toprule
%  {\cellcolor{lightgray}} \clip
% &{\cellcolor{lightgray}}75.66 &{\cellcolor{lightgray}}34.72 &{\cellcolor{lightgray}}34.80 &{\cellcolor{lightgray}}33.82
% &{\cellcolor{lightgray}}38.79 &{\cellcolor{lightgray}}7.14 &{\cellcolor{lightgray}}6.69 &{\cellcolor{lightgray}}5.87
% &{\cellcolor{lightgray}}59.04 &{\cellcolor{lightgray}}16.64 &{\cellcolor{lightgray}}13.88 &{\cellcolor{lightgray}}11.56
% &{\cellcolor{lightgray}}57.83 &{\cellcolor{lightgray}}19.50 &{\cellcolor{lightgray}}18.46 &{\cellcolor{lightgray}}17.08
% \\
% \cmidrule{1-17} 
% % fare4@224
% %  \farefour
% % &64.20 &43.60 &38.24 &34.88
% % &42.98 &32.73 &26.81 &20.76
% % &52.44 &35.76 &30.28 &22.96
% % &53.21 &37.36 &31.78 &26.20
% % \\
% %fare4@336-5
%  \farefour
% &66.72 & 44.70 & 42.50 & 39.66
% &42.64 & 28.07 & 24.31 & 19.75
% &54.64 & 32.32 & 29.88 & 24.52
% &54.67 & 35.03 & 32.23 & 27.98
% \\
% %fare4@336-10
% %  \farefour
% % &66.56 & 47.86 & 43.28 & 39.66
% % &42.65 & 33.30 & 26.78 & 21.35
% % &54.24 & 37.16 & 31.44 & 26.88
% % &54.48 & 39.44 & 33.83 & 29.29
% % \\
%  \cellcolor{LightCyan}\oursgiant
% &\cellcolor{LightCyan}{68.02} &\cellcolor{LightCyan}{59.92} &\cellcolor{LightCyan}{52.12} &\cellcolor{LightCyan}{41.26}
% &\cellcolor{LightCyan}{37.82} &\cellcolor{LightCyan}{31.71} &\cellcolor{LightCyan}{28.09} &\cellcolor{LightCyan}{20.96}
% &\cellcolor{LightCyan}54.88 &\cellcolor{LightCyan}{49.68} &\cellcolor{LightCyan}{43.76} &\cellcolor{LightCyan}{34.04}
% &\cellcolor{LightCyan}{53.57} &\cellcolor{LightCyan}47.10 &\cellcolor{LightCyan}{41.32} &\cellcolor{LightCyan}{32.09}
% \\


%  \cellcolor{LightCyan}\oursgiant + \clip
% &\cellcolor{LightCyan}{76.26} &\cellcolor{LightCyan}{35.12} &\cellcolor{LightCyan}{32.56} &\cellcolor{LightCyan}{30.20}
% &\cellcolor{LightCyan}{37.05} &\cellcolor{LightCyan}{7.43} &\cellcolor{LightCyan}{6.50} &\cellcolor{LightCyan}{5.87}
% &\cellcolor{LightCyan}60.12 &\cellcolor{LightCyan}{17.92} &\cellcolor{LightCyan}{14.56} &\cellcolor{LightCyan}{11.72}
% &\cellcolor{LightCyan}{57.81} &\cellcolor{LightCyan}20.16 &\cellcolor{LightCyan}{17.87} &\cellcolor{LightCyan}{15.93}
% \\

%  \cellcolor{LightCyan}\oursgiant + \farefour
% &\cellcolor{LightCyan}{67.90} &\cellcolor{LightCyan}{44.60} &\cellcolor{LightCyan}{42.82} &\cellcolor{LightCyan}{39.98}
% &\cellcolor{LightCyan}{39.12} &\cellcolor{LightCyan}{26.01} &\cellcolor{LightCyan}{22.85} &\cellcolor{LightCyan}{18.17}
% &\cellcolor{LightCyan}54.72 &\cellcolor{LightCyan}{33.68} &\cellcolor{LightCyan}{30.72} &\cellcolor{LightCyan}{25.24}
% &\cellcolor{LightCyan}{53.91} &\cellcolor{LightCyan}34.76 &\cellcolor{LightCyan}{32.13} &\cellcolor{LightCyan}{27.79}
% \\

% %  \cellcolor{LightCyan}\ourshuge
% % &\cellcolor{LightCyan}{66.36} &\cellcolor{LightCyan}{58.66} &\cellcolor{LightCyan}{50.96} &\cellcolor{LightCyan}{36.64}
% % &\cellcolor{LightCyan}{33.31} &\cellcolor{LightCyan}{28.88} &\cellcolor{LightCyan}{24.95} &\cellcolor{LightCyan}{19.25}
% % &\cellcolor{LightCyan}53.40 &\cellcolor{LightCyan}{47.92} &\cellcolor{LightCyan}{40.72} &\cellcolor{LightCyan}{30.60}
% % &\cellcolor{LightCyan}{51.02} &\cellcolor{LightCyan}45.15 &\cellcolor{LightCyan}{38.88} &\cellcolor{LightCyan}{28.83}
% % \\
% %  \cellcolor{LightCyan}\ourshuge + \clip
% % &\cellcolor{LightCyan}{76.12} &\cellcolor{LightCyan}{34.96} &\cellcolor{LightCyan}{33.24} &\cellcolor{LightCyan}{30.08}
% % &\cellcolor{LightCyan}{40.54} &\cellcolor{LightCyan}{8.61} &\cellcolor{LightCyan}{7.87} &\cellcolor{LightCyan}{7.21}
% % &\cellcolor{LightCyan}59.20 &\cellcolor{LightCyan}{19.48} &\cellcolor{LightCyan}{16.56} &\cellcolor{LightCyan}{12.76}
% % &\cellcolor{LightCyan}{58.62} &\cellcolor{LightCyan}21.02 &\cellcolor{LightCyan}{19.22} &\cellcolor{LightCyan}{16.68}
% % \\

% %  \cellcolor{LightCyan}\ourshuge + \farefour
% % &\cellcolor{LightCyan}{66.94} &\cellcolor{LightCyan}{43.82} &\cellcolor{LightCyan}{40.94} &\cellcolor{LightCyan}{36.50}
% % &\cellcolor{LightCyan}{42.99} &\cellcolor{LightCyan}{28.51} &\cellcolor{LightCyan}{23.92} &\cellcolor{LightCyan}{20.87}
% % &\cellcolor{LightCyan}55.96 &\cellcolor{LightCyan}{33.96} &\cellcolor{LightCyan}{29.08} &\cellcolor{LightCyan}{24.32}
% % &\cellcolor{LightCyan}{55.29} &\cellcolor{LightCyan}35.43 &\cellcolor{LightCyan}{31.31} &\cellcolor{LightCyan}{27.23}
% % \\
%  \cellcolor{LightCyan}\oursgiant + \ourshuge
% &\cellcolor{LightCyan}{68.58} &\cellcolor{LightCyan}{59.20} &\cellcolor{LightCyan}{52.10} &\cellcolor{LightCyan}{40.14}
% &\cellcolor{LightCyan}{36.92} &\cellcolor{LightCyan}{33.09} &\cellcolor{LightCyan}{29.03} &\cellcolor{LightCyan}{21.93}
% &\cellcolor{LightCyan}54.16 &\cellcolor{LightCyan}{49.72} &\cellcolor{LightCyan}{44.08} &\cellcolor{LightCyan}{33.68}
% &\cellcolor{LightCyan}{53.22} &\cellcolor{LightCyan}{47.34} &\cellcolor{LightCyan}{41.74} &\cellcolor{LightCyan}{31.92}
% \\
\bottomrule
\end{tabular}
}
\label{tab:untargetdown_ensemble}
\vspace{-1em}
\end{table*}


\begin{table*}[t!]
    \centering
    \small
    \renewcommand{\arraystretch}{0.8} % Adjust row height for readability
    \tabcolsep=2pt % Adjust column spacing    
    \begin{minipage}{0.58\textwidth} % Adjust width as needed
        \centering
         \captionof{table}{\textbf{Prompt formatting results of MLLMs on COCO captioning task.}  }
        \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cc|cc|cc|cc|cc}
        \toprule
        \rowcolor{lightgray} Vision Encoders 
        & \multicolumn{2}{c|}{Original} 
        & \multicolumn{2}{c|}{\texttt{AP}} 
        & \multicolumn{2}{c|}{\texttt{AC}}
        & \multicolumn{2}{c|}{\texttt{RandStr}}
        & \multicolumn{2}{c}{\texttt{RandSent}}  \\
          \rowcolor{lightgray} 
        & \multicolumn{2}{c|}{$\ell_\infty$} 
        & \multicolumn{2}{c|}{$\ell_\infty$} 
        & \multicolumn{2}{c|}{$\ell_\infty$} 
        & \multicolumn{2}{c|}{$\ell_\infty$} 
        & \multicolumn{2}{c}{$\ell_\infty$} 
        \\
        \cline{2-3} \cline{4-5} \cline{6-7} \cline{8-9} \cline{10-11} 
        \rowcolor{lightgray} 
        & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
        & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
        & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
        & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
        & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 

        \\
        \midrule
        {\clip}  
        & 2.98 & 1.95 & 4.99 & 3.13 &  4.84 & 3.10 & 3.53 & 2.64 & 5.32 & 3.13	 \\
        %fare4@224
        % {\farefour} 
        % & 30.15 & 14.60 & 36.56 & 19.56 &  37.19 & 19.55 & 33.95 & 16.44 & 37.82 & 21.78 \\
        %fare4@336-5
        % {\farefour @336-5} 
        % & 26.49 & 15.06 & 30.23 & 19.19 &  31.69 & 19.68 & 29.24 & 17.23 & 34.38 & - \\
        %fare4@336-10
         {\farefour} 
        & 35.43 & 19.12 & 40.78 & 24.02 &  42.63 & 24.46 & 39.71 & 21.90 & 47.16 & 27.90 \\
        {\simclipfour}
        & 34.27 & 15.95 & 40.36 & 20.86 &  40.90 & 21.71 & 37.59 & 16.85 & 41.70 & 22.75  \\
            \rowcolor{LightCyan} 
         {\ourshuge}  
        & 73.67 & 44.13 & 78.63 & 52.48 &  78.20 & 52.64 & 80.25 & 48.92 & 82.14 & 59.22  \\
            \rowcolor{LightCyan} 
        {\oursgiant}  
        & 76.76 & 46.57 & 78.90 & 54.52 &  81.89 & 53.57 & 84.10 & 51.77 & 87.74 & 58.22 \\
        \bottomrule
    \end{tabular}
    \label{tab:prompt-formatting_2}
        }
    \end{minipage}%
    \hspace{0.8em}%\hfill
    \begin{minipage}{0.38\textwidth} % Adjust width as needed
        \centering
    \captionof{table}{\textbf{Hallucination evaluation of MLLMs using POPE (F1-Score).}}
        \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccc|c}
        \toprule
        \rowcolor{lightgray}  Vision Encoders 
        & \multicolumn{3}{c|}{POPE Sampling} &  \\
        \rowcolor{lightgray} 
        & {Adversarial} 
        & {Popular} 
        & {Random} 
        & Mean
%        \rowcolor{lightgray} 
        \\
        \midrule
        {\clip}  
        & 84.72 & 86.31 & 87.79 & 86.27  \\
        %fare4@224
       % {\farefour} 
       %  & 72.05 & 74.19 & 75.13 & 73.79  \\
        %  fare4@336-5
        % {\farefour} 
        % & 76.15 & 78.09 & 78.84 & 77.69  \\
        % fare4@336-10
         {\farefour} 
        & 76.10 & 78.06 & 78.78 & 77.65  \\
        {\simclipfour}
        & 72.67 & 74.65 & 75.68 & 74.33  \\
                \rowcolor{LightCyan} 
         {\ourshuge}  
        & 78.88 & 82.83 & 84.35 & 82.02  \\
                \rowcolor{LightCyan} 
        {\oursgiant}  
        & 80.15 & 83.63 & 84.89 & 82.89  \\
        \bottomrule
    \end{tabular}
    \label{tab:pope-f1}
        }
    \end{minipage}
    \vspace{-1em}
\end{table*}

\begin{table}[h]
\centering
\small
\tabcolsep=2pt
\renewcommand{\arraystretch}{1.0}
% \caption{\label{tab:corruptions-coco}\textbf{Evaluation on COCO Captioning datasets under corruptions}. We report the average performance drop for each method across different corruption types, where the severity level increases from 1 to 5. The average performance drop is evaluated using the formula: \((\text{Score at Level 1} - \text{Score at Level 5}) / \text{Score at Level 1}\).}
\caption{\label{tab:corruptions-coco}\textbf{Evaluation under Corruptions on COCO}. Average drop in score (S) across each corruption type is reported  ({\(\textstyle \frac{\text{S}_1 - \text{S}_5}{\text{S}_1}\)*100}), with corruption severity increasing from 1 to 5.}
% \caption{\label{tab:corruptions-coco}\textbf{Evaluation on COCO Captioning datasets  under corruptions}. We report the average performance drop of each method.}% from the lowest to the highest severity of corruption.} 
\resizebox{0.42\textwidth}{!}{
\begin{tabular}{l | c c c c}
\toprule
\rowcolor{LightCyan} 
\textbf{Corruption Type} & \clip & \farefour & \simclipfour & \oursgiant  \\
\midrule
Snow & 13.59 & 50.21 & 47.68 & 25.35  \\
Frost & 17.88 & 57.62 &{52.49} & 30.88  \\
Fog & 14.47 &{84.34} & 82.02 &{56.65}  \\
Brightness & 3.89 & 19.87 &{16.45} &{7.17}  \\
Defocus Blur & 26.58 &{61.42} & 59.70 &{45.90}  \\
Glass Blur & 58.91 &{70.07} & 69.85 &{48.56}  \\
Motion Blur & 21.56 &{61.42} &{58.69} & 39.56  \\
Zoom Blur & 34.32 &{58.62} & 55.91 &{45.00}  \\
Contrast & 31.20 & 93.99 &{95.69} &{95.73}  \\
Elastic Transform & 33.63 & 32.62 &{31.15} & 25.41  \\
Pixelate & 7.58 & 14.15 &{12.98} & 8.65  \\
JPEG & 10.66 & 7.73 &{7.05} &{2.18}  \\
Gaussian Noise & 33.88 & 55.67 &{49.05} &{27.89}  \\
Shot Noise & 27.90 &{53.26} & 50.39 &{28.09}  \\
Impulse Noise & 28.66 &{52.99} & 46.62 & 25.67  \\

\bottomrule
\end{tabular} 
\label{tab:common_corruptions}
}
\vspace{-1em}
\end{table}




\subsection{Ensemble of Vision Encoders}

Inspired by \cite{tong2024eyes}, we integrate \clip with \oursgiant in the LLaVA framework, incorporating separate learnable projection layers. To enhance robustness, we replace \clip with robust alternatives like \farefour and also train an ensemble of ViT-G and ViT-H.
%Drawing inspiration from \cite{tong2024eyes}, We integrate  \clip  with \oursgiant and train it in LLaVA framework,  employing separate learnable projection layers. For maintaining robustness, we replace the \clip model with robust counterparts like \farefour. Further, we also train an ensemble of ViT-G and ViT-H. 
Our results evaluated using APGD-100 attack at half precision reported in Table \ref{tab:untargetdown_ensemble} reveal the limitation of model ensebmling in context of adversarial robustness: when using multiple encoders, the model's robustness tends to align with its weakest component. For instance, LLaVA with \oursgiant + \clip  maintains high clean performance but suffers significant drops under attacks (18.07 vs 93.21 at \(\epsilon =  \frac{4}{255}\)), suggesting that adversaries can exploit the vulnerable CLIP encoder despite having a robust counterpart. Similarly, \oursgiant + \farefour  shows intermediate performance (64.76 vs 93.21 at \(\epsilon =  \frac{4}{255}\)), indicating that even a moderately robust encoder can limit the overall system's robustness. 
These findings suggest that multi-encoder systems inherit the robustness of their weakest component, making a single, highly robust encoder a more effective alternative. See Sec. \ref{app:ensemble_vision} of the Appendix for further analysis.
%These findings emphasize that in multi-encoder systems, overall robustness is constrained by the weakest component, suggesting that a single, highly robust encoder may be more effective than combining encoders with varying robustness. For further analysis, see Sec. \ref{app:ensemble_vision} of Appendix.




% \begin{table*}[t!]
%     \centering 
%     \small
%     \extrarowheight=-1.5pt
%     \tabcolsep=1pt
%     \captionof{table}{\textbf{Prompt formatting results on COCO captioning task.}  }
%     \begin{tabular}{l|cc|cc|cc|cc|cc}
%         \toprule
%         \rowcolor{lightgray} Vision Encoders 
%         & \multicolumn{2}{c|}{Original} 
%         & \multicolumn{2}{c|}{\texttt{AP}} 
%         & \multicolumn{2}{c|}{\texttt{AC}}
%         & \multicolumn{2}{c|}{\texttt{RandStr}}
%         & \multicolumn{2}{c}{\texttt{RandSent}}  \\
%           \rowcolor{lightgray} 
%         & \multicolumn{2}{c|}{$\ell_\infty$} 
%         & \multicolumn{2}{c|}{$\ell_\infty$} 
%         & \multicolumn{2}{c|}{$\ell_\infty$} 
%         & \multicolumn{2}{c|}{$\ell_\infty$} 
%         & \multicolumn{2}{c}{$\ell_\infty$} 
%         \\
%         \cline{2-3} \cline{4-5} \cline{6-7} \cline{8-9} \cline{10-11} 
%         \rowcolor{lightgray} 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 
%         & $\nicefrac{4}{255}$ & $\nicefrac{8}{255}$ 

%         \\
%         \midrule
%         {\clip}  
%         & 2.98 & 1.95 & 4.99 & 3.13 &  4.84 & 3.10 & 3.53 & 2.64 & 5.32 & 3.13	 \\
%         %fare4@224
%         % {\farefour} 
%         % & 30.15 & 14.60 & 36.56 & 19.56 &  37.19 & 19.55 & 33.95 & 16.44 & 37.82 & 21.78 \\
%         %fare4@336-5
%         % {\farefour @336-5} 
%         % & 26.49 & 15.06 & 30.23 & 19.19 &  31.69 & 19.68 & 29.24 & 17.23 & 34.38 & - \\
%         %fare4@336-10
%          {\farefour} 
%         & 35.43 & 19.12 & 40.78 & 24.02 &  42.63 & 24.46 & 39.71 & 21.90 & 47.16 & 27.90 \\
%         {\simclipfour}
%         & 34.27 & 15.95 & 40.36 & 20.86 &  40.90 & 21.71 & 37.59 & 16.85 & 41.70 & 22.75  \\
%             \rowcolor{LightCyan} 
%          {\ourshuge}  
%         & 73.67 & 44.13 & 78.63 & 52.48 &  78.20 & 52.64 & 80.25 & 48.92 & 82.14 & 59.22  \\
%             \rowcolor{LightCyan} 
%         {\oursgiant}  
%         & 76.76 & 46.57 & 78.90 & 54.52 &  81.89 & 53.57 & 84.10 & 51.77 & 87.74 & 58.22 \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:prompt-formatting_2}
% \end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{table}[t!]
%     \centering 
%     \small
%     \extrarowheight=-1.5pt
%     \tabcolsep=2pt
%     \captionof{table}{\textbf{Hallucination evaluation using POPE(F1-Score).}}
%     \begin{tabular}{l|ccc|c}
%         \toprule
%         \rowcolor{lightgray}  Vision Encoders 
%         & \multicolumn{3}{c|}{POPE Sampling} &  \\
%         \rowcolor{lightgray} 
%         & {Adversarial} 
%         & {Popular} 
%         & {Random} 
%         & Mean
% %        \rowcolor{lightgray} 
%         \\
%         \midrule
%         {\clip}  
%         & 84.72 & 86.31 & 87.79 & 86.27  \\
%         %fare4@224
%        % {\farefour} 
%        %  & 72.05 & 74.19 & 75.13 & 73.79  \\
%         %  fare4@336-5
%         % {\farefour} 
%         % & 76.15 & 78.09 & 78.84 & 77.69  \\
%         % fare4@336-10
%          {\farefour} 
%         & 76.10 & 78.06 & 78.78 & 77.65  \\
%         {\simclipfour}
%         & 72.67 & 74.65 & 75.68 & 74.33  \\
%                 \rowcolor{LightCyan} 
%          {\ourshuge}  
%         & 78.88 & 82.83 & 84.35 & 82.02  \\
%                 \rowcolor{LightCyan} 
%         {\oursgiant}  
%         & 80.15 & 83.63 & 84.89 & 82.89  \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:pope-f1}
% \end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Performance on Other Robustness Tasks} 
We expand our evaluation to a wider range of robustness metrics, including resilience to natural image corruptions~\cite{hendrycks2019benchmarking}, avoidance of object hallucinations~\cite{li2023evaluating}, and robustness to inference-time prompt variations~\cite{bhagwatkar2024towards}.


% Here we extend our evaluation to a broader range of robustness metrics. This includes assessing resilience to natural image corruptions and manipulations~\cite{hendrycks2019benchmarking}, avoiding object hallucinations~\cite{li2023evaluating}, and inference-time variations in prompt formatting~\cite{bhagwatkar2024towards}.

\textbf{Common Corruptions.} We evaluate MLLMs' resilience to common image corruptions on the COCO captioning dataset, as summarized in Table~\ref{tab:common_corruptions}. \clip achieves the highest performance across all corruption types. However, adversarially finetuned \clip vision encoders, such as \farefour and \simclipfour, exhibit substantial performance drops; exceeding 50\% on corruptions like snow, frost, blur, noise, and fog. This degradation is likely a result of adversarial finetuning, which compromises the original generalization capabilities of \clip. In contrast, \oursgiant demonstrates stronger generalization, highlighting that integrating large-scale robust backbones into the LLaVA framework effectively balances robustness and generalization, maintaining the generalization from large-scale training. Further results are detailed in Sec.~\ref{app:common_corruptions} of the Appendix.

% We evaluate MLLMs resilience to common image corruptions on the COCO captioning dataset, with the results summarized in Table~\ref{tab:common_corruptions}. Our findings reveal that \clip achieves the highest performance across all corruption types. However, adversarially finetuned \clip vision encoders, such as \farefour and \simclipfour, experience significant performance degradation across most corruption types, including a drop exceeding 50\% for snow, frost, blur, noise, and fog. We attribute this decline to the limited data used during adversarial finetuning, which constrains the preservation of the original \clip’s generalization properties. In contrast, \oursgiant exhibits stronger generalization compared to its robust counterparts. This highlights that integrating large-scale robust backbones directly into the LLaVA framework effectively balances the robustness-generalization trade-off, sustaining the high performance associated with large-scale training. Results on multiple tasks are detailed in Sec.~\ref{app:common_corruptions} of the Appendix.


\textbf{Prompt Formatting.} Recent work~\cite{bhagwatkar2024towards} has investigated modifying textual prompts at inference to enhance the robustness of non-robust LLaVA models on COCO captioning and VQAv2. While their study focused on weaker attacks, we extend this by evaluating prompt modifications against stronger APGD-ensemble attacks. For COCO captioning, we test four prompt strategies: mentioning the possibility (\texttt{AP}) or certainty (\texttt{AC}) of adversarial perturbations, and adding random strings (\texttt{RandStr}) or sentences (\texttt{RandSent}) at the start. In Table \ref{tab:prompt-formatting_2},  we observe consistent improvements with different prompt variations at inference time; however, the non-robust LLaVA model fails to achieve meaningful robustness under stronger attacks, contrasting with the high robustness observed in \cite{bhagwatkar2024towards}. Further details are reported in Sec.~\ref{app:prompt_formatting} of the Appendix.


\textbf{Hallucinations.} In Table \ref{tab:pope-f1}, we report the F1-score for each setting of the hallucination benchmark POPE~\cite{li2023evaluating}. We observe that while original \clip has the highest score across all the settings, \oursgiant and \ourshuge perform better than their robust counterparts.

%Beyond adversarial robustness, we evaluate models' resilience to common image corruptions on COCO captioning. Table~\ref{tab:common_corruptions} reveals two key insights about our approach. First, vision encoders adversarially trained on ImageNet (FARE$^4$ and Sim-CLIP$^4$) show significant performance degradation across corruptions (>50$\%$ drop for snow, frost, and fog), while our billion-scale adversarially pre-trained encoder maintains substantially better generalization with lower drops across natural corruptions (e.g., 25.35$\%$ vs $\sim$50$\%$ for snow). Second, training LLaVA with this robust encoder further helps balance the robustness-generalization trade-off - \oursgiant maintains strong performance under both subtle corruptions like brightness (7.17$\%$ vs >16$\%$) and severe ones like noise (28$\%$ vs >50$\%$). These results demonstrate that combining billion-scale adversarial pre-training with LLaVA training creates a synergistic effect: the encoder provides strong robustness while LLaVA training helps preserve clean performance and generalization capabilities.


\section{Conclusion}
%In this work, we investigate the adversarial robustness of Multimodal Large Language  Models (MLLMs), focusing on vulnerabilities within the vision modality. We highlight the limitations of current methods that use constrained adversarial fine-tuning, which  leads to an imbalanced trade-off between clean and adversarial generalization performance. To tackle this, we explore the multi-modal alignment capabilities of robust vision encoders. This investigation leads us to integrate large-scale adversarially trained vision classifiers into MLLM frameworks, significantly enhancing their robustness. 
In this work, we investigate the adversarial robustness of Multimodal Large Language Models (MLLMs), focusing on vision modality vulnerabilities. We address the limitations of constrained adversarial fine-tuning by exploring multi-modal alignment capabilities of robust vision encoders, leading to the integration of large-scale adversarially trained vision classifiers into MLLM frameworks.
Our results demonstrate that these encoders significantly enhance resilience against both untargeted and targeted attacks in image captioning and visual question answering tasks. We further show robustness against optimization-based and generation-based jailbreak attacks, common corruptions, and object hallucination.
%Our findings show that large-scale adversarially trained encoders significantly improve performance against both untargeted and targeted attacks on image captioning and visual question answering tasks. We demonstrate resilience to both optimization-based and generation-based jailbreak attacks, while also achieving robustness against common corruptions and object hallucination. 
Additionally, we analyze adversarial weaknesses in MLLM ensembling and explore inference-time prompt formatting techniques to improve robustness.
Our work advances the adversarial resilience of MLLMs, contributing to more safer models for real-world applications.
%Additionally, we provide insights into adversarial vulnerabilities arising from model ensembling in MLLMs and analyze inference-time prompt formatting techniques to enhance robustness. Our work makes important strides in enhancing the adversarial resilience of MLLMs, contributing to safer and more reliable models that are better suited for real-world applications.


% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \clearpage

\section*{Impact Statement}

Multi-Modal Large Language Models (MLLMs) are increasingly being deployed across various applications due to their impressive performance on diverse tasks. However, ensuring their security and reliability remains a critical challenge. In this work, we take a step toward addressing this issue by investigating adversarially trained vision models and their potential to enhance robustness when integrated into MLLMs. Our findings reveal that MLLMs remain vulnerable to a wide range of vision-centric adversarial attacks, irrespective of the underlying language model. This underscores the necessity of prioritizing robust vision encoders. Notably, our results highlight that incorporating large-scale adversarially multi-modal pretraining of vision encoders before integrating them into the MLLM framework yields significant benefits. Exploring this direction further could lead to substantial improvements in the robustness and security of MLLMs.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}
\input{X_suppl}
% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
