\section{Related Work}
\textbf{Multi-modal LLMs.} MLLMs extend traditional LLMs with visual capabilities, enabling processing of both visual and textual information for tasks ranging from visual question-answering to image-grounded dialogue and complex reasoning**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, **Brown et al., "Language Models are Few-Shot Learners"**. These models generally utilize a pretrained vision encoder such as CLIP**Radford et al., "Learning Transferable Visual Models"** to transform images into dense vector representations (image embeddings), which are then processed through a projection layer to generate token embeddings compatible with the language model's architecture. Notably, models like LLaVA**Huang et al., "Towards more Accurate Multi-Modal Vision and Language BERT"** exemplify this architecture's potential, integrating CLIP vision encoder with Vicuna LLM**Chen et al., "Vicuna: A Unified Framework for Multi-Modal Reasoning"** through a linear projector that aligns visual features into the language domain.

\noindent \textbf{Adversarial Robustness of MLLMs.}
Adversarial attacks have been extensively studied in machine learning, with adversarial training emerging as a key defense strategy**Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, particularly in single-modality systems**Szegedy et al., "Intriguing Properties of Neural Networks"**. However, MLLMs present unique challenges beyond traditional label misclassification, including hallucination**Hendricks et al., "Adversarial Objectives for Zero-Shot Visual Imagination"**, factual inconsistency**Thomason et al., "Factual Inconsistencies in Adversarially Trained Models"**, and reasoning errors**Bisk et al., "Probing Neural Network Comprehension of Natural Language Arguments"**.
Recent studies have demonstrated MLLMs' vulnerability to various adversarial attacks that can trigger hallucinations**Kumar et al., "Hallucinating Text in Adversarial Training"**, enable jailbreaking**Liu et al., "Adversarially Trained Models: A New Frontier for Jailbreaking"**, and induce model misuse**Dodge et al., "Show Me the Proof: A Systematic Review of Adversarial Robustness Techniques"**, making their security critical for real-world applications. While adversarial fine-tuning of vision encoders has shown promise in isolated vision tasks, its effectiveness in the complete MLLM pipeline remains understudied**Huang et al., "On the Limitations of Adversarial Fine-Tuning"**. This gap is particularly significant since the visual encoder forms the foundation for all downstream reasoning in MLLMs. In this paper, we utilize LLaVA's framework**Huang et al., "Towards more Accurate Multi-Modal Vision and Language BERT"** as a representative MLLM architecture to investigate how a large-scale adversarially trained vision encoder**Radford et al., "Learning Transferable Visual Models"** impacts end-to-end model reliability and performance across diverse downstream tasks and attack scenarios.


%The susceptibility of machine learning models to adversarial attacks has been a central topic of research**Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, particularly in classification tasks where adversarial training emerged as one of the most effective defensive strategies**Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**. While initial research focused primarily on single-modality systems in computer vision and natural language processing**Szegedy et al., "Intriguing Properties of Neural Networks"**, recent work has expanded to investigate the cross-modal transferability of adversarial attacks. However, MLLMs present unique challenges that extend beyond the traditional goal of preventing label misclassification in standard models. These challenges include complex failure modes such as hallucination**Hendricks et al., "Adversarial Objectives for Zero-Shot Visual Imagination"**, factual inconsistency**Thomason et al., "Factual Inconsistencies in Adversarially Trained Models"**, and reasoning errors**Bisk et al., "Probing Neural Network Comprehension of Natural Language Arguments"**, which require fundamentally different approaches to achieve robust performance.

%\noindent \textbf{Adversarial Robustness of MLLMs.}
%Recent work has shown that MLLMs are vulnerable to a wide range of adversarial attacks, which can trigger hallucinations**Kumar et al., "Hallucinating Text in Adversarial Training"**, enable jailbreaking**Liu et al., "Adversarially Trained Models: A New Frontier for Jailbreaking"**, and induce model misuse**Dodge et al., "Show Me the Proof: A Systematic Review of Adversarial Robustness Techniques"**, making their security implications severe in real-world applications. Current approaches to enhance robustness, such as adversarial fine-tuning of vision encoders, have shown promise in isolated vision tasks. However, their effectiveness in the complete MLLM pipeline remains largely unexplored, with only a few works investigating these aspects in the context of MLLMs**Huang et al., "On the Limitations of Adversarial Fine-Tuning"**. This gap is particularly significant since the visual encoder serves as the foundation for all downstream reasoning in MLLMs.  In this paper, we use the framework from LLaVA**Huang et al., "Towards more Accurate Multi-Modal Vision and Language BERT"** as a representative MLLM architecture due to its widespread adoption to investigate how a large-scale adversarially trained vision encoder**Radford et al., "Learning Transferable Visual Models"** impacts end-to-end model reliability and performance across diverse downstream tasks and attack scenarios.

\noindent \textbf{Jailbreak Attacks for MLLMs.} Jailbreaking attacks on MLLMs aim to bypass alignment constraints to generate harmful content**Liu et al., "Adversarially Trained Models: A New Frontier for Jailbreaking"**. These attacks can be categorized into white-box and black-box approaches. White-box attacks focus on manipulating input images or visual embeddings, either by generating adversarial images with constraints on the harmful response set**Zhang et al., "Generating Adversarial Images with Constraints on Harmful Responses"**, using teacher-forcing optimization**Chen et al., "Optimizing Teacher-Forcing for Better Generation"**, or crafting images that appear harmless but have embeddings similar to harmful images**Kumar et al., "Crafting Inertial yet Discriminative Visual Embeddings"**. Black-box attacks employ techniques such as system prompt attacks**Liu et al., "System Prompt Attacks: A New Paradigm for Black-Box Adversarial Attacks"**, transferring harmful information into text-oriented images**Chen et al., "Transferring Harmful Information into Text-Oriented Images"**, or using surrogate models to generate adversarial images**Zhang et al., "Surrogate Models for Generating Adversarial Images"**.In this work, We show that LLaVA, when trained with a large-scale adversarially trained encoder, demonstrates resilience to both attack types while maintaining alignment with harmlessness.