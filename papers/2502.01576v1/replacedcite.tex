\section{Related Work}
\textbf{Multi-modal LLMs.} MLLMs extend traditional LLMs with visual capabilities, enabling processing of both visual and textual information for tasks ranging from visual question-answering to image-grounded dialogue and complex reasoning____. These models generally utilize a pretrained vision encoder such as CLIP____ to transform images into dense vector representations (image embeddings), which are then processed through a projection layer to generate token embeddings compatible with the language model's architecture. Notably, models like LLaVA____ exemplify this architecture's potential, integrating CLIP vision encoder with Vicuna LLM____ through a linear projector that aligns visual features into the language domain.

\noindent \textbf{Adversarial Robustness of MLLMs.}
Adversarial attacks have been extensively studied in machine learning, with adversarial training emerging as a key defense strategy____, particularly in single-modality systems____. However, MLLMs present unique challenges beyond traditional label misclassification, including hallucination____, factual inconsistency____, and reasoning errors____.
Recent studies have demonstrated MLLMs' vulnerability to various adversarial attacks that can trigger hallucinations____, enable jailbreaking____, and induce model misuse____, making their security critical for real-world applications____. While adversarial fine-tuning of vision encoders has shown promise in isolated vision tasks, its effectiveness in the complete MLLM pipeline remains understudied____. This gap is particularly significant since the visual encoder forms the foundation for all downstream reasoning in MLLMs. In this paper, we utilize LLaVA's framework____ as a representative MLLM architecture to investigate how a large-scale adversarially trained vision encoder____ impacts end-to-end model reliability and performance across diverse downstream tasks and attack scenarios.


%The susceptibility of machine learning models to adversarial attacks has been a central topic of research____, particularly in classification tasks where adversarial training emerged as one of the most effective defensive strategies____. While initial research focused primarily on single-modality systems in computer vision and natural language processing____, recent work has expanded to investigate the cross-modal transferability of adversarial attacks. However, MLLMs present unique challenges that extend beyond the traditional goal of preventing label misclassification in standard models. These challenges include complex failure modes such as hallucination____, factual inconsistency____, and reasoning errors____, which require fundamentally different approaches to achieve robust performance.

%\noindent \textbf{Adversarial Robustness of MLLMs.}
%Recent work has shown that MLLMs are vulnerable to a wide range of adversarial attacks, which can trigger hallucinations____, enable jailbreaking____, and induce model misuse____, making their security implications severe in real-world applications____. Current approaches to enhance robustness, such as adversarial fine-tuning of vision encoders, have shown promise in isolated vision tasks. However, their effectiveness in the complete MLLM pipeline remains largely unexplored, with only a few works investigating these aspects in the context of MLLMs____. This gap is particularly significant since the visual encoder serves as the foundation for all downstream reasoning in MLLMs.  In this paper, we use the framework from LLaVA____ as a representative MLLM architecture due to its widespread adoption to investigate how a large-scale adversarially trained vision encoder____ impacts end-to-end model reliability and performance across diverse downstream tasks and attack scenarios.

\noindent \textbf{Jailbreak Attacks for MLLMs.} Jailbreaking attacks on MLLMs aim to bypass alignment constraints to generate harmful content____. These attacks can be categorized into white-box and black-box approaches. White-box attacks focus on manipulating input images or visual embeddings, either by generating adversarial images with constraints on the harmful response set____, using teacher-forcing optimization____, or crafting images that appear harmless but have embeddings similar to harmful images____. Black-box attacks employ techniques such as system prompt attacks____, transferring harmful information into text-oriented images____, or using surrogate models to generate adversarial images____.In this work, We show that LLaVA, when trained with a large-scale adversarially trained encoder, demonstrates resilience to both attack types while maintaining alignment with harmlessness.