\section{Related Work}
\textbf{Multi-modal LLMs.} MLLMs extend traditional LLMs with visual capabilities, enabling processing of both visual and textual information for tasks ranging from visual question-answering to image-grounded dialogue and complex reasoning~\cite{yin2023survey,li2024multimodal,caffagni2024r,awais2023foundational}. These models generally utilize a pretrained vision encoder such as CLIP~\cite{radford2021learning} to transform images into dense vector representations (image embeddings), which are then processed through a projection layer to generate token embeddings compatible with the language model's architecture. Notably, models like LLaVA~\cite{liu2024visual,liu2024improved} exemplify this architecture's potential, integrating CLIP vision encoder with Vicuna LLM~\cite{chiang2023vicuna} through a linear projector that aligns visual features into the language domain.

\noindent \textbf{Adversarial Robustness of MLLMs.}
Adversarial attacks have been extensively studied in machine learning, with adversarial training emerging as a key defense strategy~\cite{szegedy2013intriguing,goodfellow2014explaining,mkadry2017towards,zhang2019theoretically}, particularly in single-modality systems~\cite{carlini2017towards,ebrahimi2017hotflip}. However, MLLMs present unique challenges beyond traditional label misclassification, including hallucination~\cite{huang2024visual}, factual inconsistency~\cite{wang2023survey}, and reasoning errors~\cite{wang2024exploring}.
Recent studies have demonstrated MLLMs' vulnerability to various adversarial attacks that can trigger hallucinations~\cite{bai2024hallucination}, enable jailbreaking~\cite{jin2024jailbreakzoo}, and induce model misuse~\cite{niu2024jailbreaking}, making their security critical for real-world applications~\cite{liu2025mm,liu2024safety,zhang2024benchmarking}. While adversarial fine-tuning of vision encoders has shown promise in isolated vision tasks, its effectiveness in the complete MLLM pipeline remains understudied~\cite{mao2022understanding,schlarmann2024robust,hossain2024sim}. This gap is particularly significant since the visual encoder forms the foundation for all downstream reasoning in MLLMs. In this paper, we utilize LLaVA's framework~\cite{liu2024visual,liu2024improved} as a representative MLLM architecture to investigate how a large-scale adversarially trained vision encoder~\cite{wang2024revisiting} impacts end-to-end model reliability and performance across diverse downstream tasks and attack scenarios.


%The susceptibility of machine learning models to adversarial attacks has been a central topic of research~\cite{szegedy2013intriguing,goodfellow2014explaining}, particularly in classification tasks where adversarial training emerged as one of the most effective defensive strategies~\cite{mkadry2017towards,zhang2019theoretically}. While initial research focused primarily on single-modality systems in computer vision and natural language processing~\cite{carlini2017towards,ebrahimi2017hotflip}, recent work has expanded to investigate the cross-modal transferability of adversarial attacks. However, MLLMs present unique challenges that extend beyond the traditional goal of preventing label misclassification in standard models. These challenges include complex failure modes such as hallucination~\cite{huang2024visual}, factual inconsistency~\cite{wang2023survey}, and reasoning errors~\cite{wang2024exploring}, which require fundamentally different approaches to achieve robust performance.

%\noindent \textbf{Adversarial Robustness of MLLMs.}
%Recent work has shown that MLLMs are vulnerable to a wide range of adversarial attacks, which can trigger hallucinations~\cite{bai2024hallucination}, enable jailbreaking~\cite{jin2024jailbreakzoo}, and induce model misuse~\cite{niu2024jailbreaking}, making their security implications severe in real-world applications~\cite{liu2025mm,liu2024safety,zhang2024benchmarking}. Current approaches to enhance robustness, such as adversarial fine-tuning of vision encoders, have shown promise in isolated vision tasks. However, their effectiveness in the complete MLLM pipeline remains largely unexplored, with only a few works investigating these aspects in the context of MLLMs~\cite{mao2022understanding,schlarmann2024robust,hossain2024sim}. This gap is particularly significant since the visual encoder serves as the foundation for all downstream reasoning in MLLMs.  In this paper, we use the framework from LLaVA~\cite{liu2024visual,liu2024improved} as a representative MLLM architecture due to its widespread adoption to investigate how a large-scale adversarially trained vision encoder~\cite{wang2024revisiting} impacts end-to-end model reliability and performance across diverse downstream tasks and attack scenarios.

\noindent \textbf{Jailbreak Attacks for MLLMs.} Jailbreaking attacks on MLLMs aim to bypass alignment constraints to generate harmful content~\cite{jin2024jailbreakzoo,qi2024visual,hossain2024securing,niu2024jailbreaking}. These attacks can be categorized into white-box and black-box approaches. White-box attacks focus on manipulating input images or visual embeddings, either by generating adversarial images with constraints on the harmful response set~\cite{dong2023robust,schlarmann2023adversarial}, using teacher-forcing optimization~\cite{carlini2024aligned}, or crafting images that appear harmless but have embeddings similar to harmful images~\cite{shayegani2023jailbreak}. Black-box attacks employ techniques such as system prompt attacks~\cite{wu2023jailbreaking}, transferring harmful information into text-oriented images~\cite{gong2023figstep}, or using surrogate models to generate adversarial images~\cite{zhao2024evaluating}.In this work, We show that LLaVA, when trained with a large-scale adversarially trained encoder, demonstrates resilience to both attack types while maintaining alignment with harmlessness.