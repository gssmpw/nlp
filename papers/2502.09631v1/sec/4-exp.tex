
\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{img/nst-3.pdf}
  \caption{\textbf{Between-frame Coherence Comparison with TNST and LNST.} VNCA synthesizes more coherent stylization across frames, compared to per-frame optimization methods. }
  \label{fig:test}
\end{figure}


\input{sec/table-stat}

\section{Experiments}

\subsection{Qualitative Results} 
To demonstrate that our method works for a variety of style images, we show the stylization results on three smoke data \textit{Smoke Jet}, \textit{Bunny}, and \textit{Billowy} in \Cref{fig:res}. 
VNCA can synthesize RGB colors for the input density sequence and modify the input density field to match the style image, as shown on the boundaries of each volume dataset that matches the pattern given by the style image.  
Our method supports different style images and can capture the key features in the reference image for aesthetic stylization. 

In the stylization of \textit{Mammatus} and \textit{Wave} on \textit{Billowy}, the synthesized color is brighter at the bottom of the simulation. 
We empirically find that regions with more concentrated density lead to brighter synthesized style colors.
By changing the transmittance of the density field, we can exhibit control over the brightness of stylized smoke without additional training. 


\subsection{Comparison with Prior Works}
\paragraph{Baseline Methods}
To our knowledge, there is no other work on the same problem, i.e. volumetric temporal texture synthesis to stylize 3D smoke data. We thus compare VNCA with 3D smoke stylization methods \citet{kim19c} and \citet{kimlnst}. 


\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{img/multi-view-2.pdf}
  \caption{\textbf{Ablation Studies on Multiview Consistency. } (i) front view supervision; (ii) randomized view supervision. If VNCA conditions on a single view during training, we cannot achieve omniview consistent stylization. }
  \label{fig:multiview}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{img/de.pdf}
  \caption{\textbf{Ablation Studies on Density Encoding. } Without density encoding, the synthesized appearance is porous and inconsistent with input reference. With density encoding, the output is much improved. }
  \label{fig:de}
\end{figure}



\paragraph{Stylization Comparison}
Our work focuses on 3D volumetric temporal texture. 
Since TNST and LNST do not support color stylization on \textbf{3D smoke data}, we use grayscale reference images for all methods. 
We run experiments on the same RTX3090Ti for fairness. 
\Cref{fig:comp} shows the qualitative comparison of stylization matching quality. VNCA can synthesize features that match the reference pattern and turbulence effect on par with TNST and LNST. 



\Cref{fig:test} compares the preservation of stylization coherence between frames. 
We highlight the circled regions for inconsistent stylization between consecutive frames in prior works,
as they stylize each density field independently, inevitably leading to inter-frame inconsistencies. 
In contrast, the volumetric temporal texture synthesized by VNCA is naturally dynamic from the emerging motion of the underlying NCA model. 


\Cref{tab:stat} shows the results of our user study on the stylization quality, inter-frame consistency, and preservation of smoke motion comparison for TNST, LNST, and VNCA. 
We received 50 valid user responses for stylization quality evaluation ("Which image has the texture pattern that best matches the reference image?"), 44 for inter-frame consistency ("Which video has a smoother transition between frames?"),
and 49 for motion preservation evaluation ("Which video is most plausible as a natural smoke sequence?"). 
TNST is not compared for inter-frame coherence as LNST is shown to be better~\cite{kimlnst}. 
Each entry shows the percentage of users that prefer the stylization of corresponding methods. 
We see a preference for our methods over the baseline methods. 




\paragraph{Runtime Comparison}
In \Cref{tab:stat}, we provide a quantitative comparison with TNST and LNST on the time required to stylize one frame of the input sequence. 
We measure this data by stylizing 20 consecutive frames and taking the average time to take into account the inter-frame smoothing time which impedes the performance of TNST and LNST. 
We can see that our average inference time for a single frame is faster than prior methods, while our training already includes omni-view consistency.




\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{img/OF.pdf}
  \caption{\textbf{Ablation Studies on Velocity Encoding.} The use of velocity encoding better aligns the estimated optical flow of our synthesized texture motion with the input velocity field. }
  \label{fig:OF}
\end{figure}




\subsection{Ablation Studies}
\label{sec:abl}
We hereby evaluate the design choice of VNCA on encoding priors and training strategies in our ablation studies, and we further demonstrate that VNCA is not a trivial adaptation of prior works. 

\paragraph{Multi-view Style Consistency. } 
During training, we randomly select one of the camera viewpoints to render and compare it with the reference style image. 
This approach ensures that the stylized volume contains consistent style features from different viewpoints. 
\Cref{fig:multiview} shows the visual comparison of rendered viewpoints from the front, right (front $+90\degree$), and back (front $+180\degree$) between a stylized volume supervised by (i) only frontal view, and (ii) from a randomly selected viewpoint. 
We can see that only conditioning on one viewpoint cannot generalize to omni-view style consistency, demonstrating the importance of our multi-view supervision. 

\paragraph{Density encoding.} 
We conduct an ablation study on the density encoding in the Volumetric NCA framework. 
By appending one of the input density frames to the perception vector, we provide our NCA model with information about the global structure of the density distribution in the input data. 
This global information is crucial for appearance stylization since each cell in the NCA's cell state is only aware of its immediate neighborhood. 
As shown in~\Cref{fig:de}, the stylization without density encoding results in an overall color that is somewhat close to the reference but is abundantly porous and lacks detailed structures, such as stroke patterns.
Including density encoding significantly enhances the quality of stylization in terms of matching the given style image.

\paragraph{Velocity encoding} 
The use of velocity encoding facilitates the alignment of our volumetric temporal texture and guiding velocity field. 
We compare the estimated optical flow between the synthesized texture motion and the projected velocity field on the smoke datasets in \Cref{fig:OF}. 
We fix the density frame to not bias the network with fluid motion. 
The synthesized result with velocity encoding shows a closer alignment to the guiding motion flow. 

\Cref{tab:ablation} shows a quantitative comparison of the impact of velocity encoding on the alignment of motion direction and magnitude, and the impact of density encoding on the appearance supervision. 


\paragraph{Comparison on weights for loss functions} \Cref{fig:param} shows the ratio $\lambda = \lambda_\text{motion}/\lambda_\text{app}$ between the weight of $\mathcal{L}_\text{motion}$ to that of $\mathcal{L}_\text{app}$. 
The stylized appearance can be corrupted when the weight for the motion loss is too high compared to the appearance loss. 



\input{sec/table-ablation}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{img/param-ablation.pdf}
  \caption{\textbf{Comparison of different weights for loss functions. } $\lambda = \lambda_\text{motion} / \lambda_\text{app}$. When the weight for the motion loss is too high w.r.t appearance loss, the stylization degenerates. (Style Image: \textit{Ink})  }
  \label{fig:param}
\end{figure}






\subsection{Generalization Ability}
\label{sec:gen}
A key feature of VNCA stems from the robust generalizability of NCA models. 
At each epoch, we condition VNCA on the current density field of the training frame. 
This allows the synthesized volumetric temporal texture to generalize to other unseen density fields in the input smoke sequence at inference time.
As we condition the density field to be stylized during inference and supply the velocity field, VNCA does not require re-training to synthesize matching appearance and corresponding visually plausible motion. 



In this way, a converged volumetric temporal texture can stylize the entire density sequence. We empirically find that trained VNCA can even extrapolate to smoke datasets unseen at training time. 

In \Cref{fig:gen}, we illustrate the generalizability of our volumetric temporal texture. The converged VNCA is conditioned on the 60th frame of the \textit{bunny} dataset in (i). 
At inference time (ii), the trained texture volume can stylize not only the 60th frame but also all other frames of the bunny scene.
Additionally, (iii) it is capable of stylizing a new dataset \textit{Smoke Jet} unseen during training, as demonstrated by the results on the 70th frame. 
This versatility allows our model to reduce training time compared to baseline methods.





\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{img/generalization.pdf}
  \caption{\textbf{Generalization Ability of VNCA. } During training, we condition VNCA on the 60th frame of \textit{Bunny}. At inference, the trained VNCA can stylize the full sequence of \textit{Bunny}, and \textit{Smoke Jet} which is unseen in training.  }
  \label{fig:gen}
\end{figure}



