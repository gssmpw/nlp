\section{Related Works}

\subsection{Texture Synthesis with Neural Cellular Automata. }
Cellular Automata (CA) consists of a regular grid of cells that are updated based on their neighboring cell states. 
One famous example is \textit{Game of Life}~\cite{gardner1970fantastic}. 
All cells follow the same update rule asynchronously. 
\citet{gilpin2019cellular} show that the update rule of CA can be parameterized with neural nets, resulting in Neural Cellular Automata (NCA). 
\citet{niklasson2021selforganising} first use NCA to synthesize 2D textures with self-emerging, temporally varying motion with a static image exemplar as input. 
\citet{diff-program-rdsystem} train a 2D reaction-diffusion system that can be extended to 3D at inference time. 
However, it cannot control the synthesized motion due to Laplacian's isotropic nature. 
\citet{pajouheshgar2023dynca} modify the NCA model for controllable 2D dynamic texture through a pre-trained optical-flow network~\cite{tesfaldet2018two} and vector field supervision.
Despite the improvements to NCA's performance and generative abilities for texture synthesis, none use the self-emerging motion to its full advantage.

Here, we introduce a variation of NCA on 3D volume grids.
Our \MethodName{} exploits the movement that naturally emerges from NCAs post-training. These motion patterns are suitable replacements for the costly advection in fluid simulation and stylization. 
Additionally, VNCA conditioned on a specific density field during training can stylize unseen density fields at inference time. 


\subsection{Solid Texture Synthesis.}
Solid texturing, introduced by \citet{solid_textures} and \citet{perlin_noise}, enables texturing meshes without UV maps. 
While the early solid texturing methods \cite{solid_textures, perlin_noise} require hand-designed functions for each texture, \citet{kopf2007solid} propose a non-parametric, optimization-based algorithm for an explicit solid texture volume from a 2D exemplar texture. 
Several works leverage neural nets to create a solid texture block and optimize its parameters based on the target exemplar texture \cite{gramgan, on-demand-solid-texture, neural-texture-space}. 
These methods evaluate the 2D slices of the synthesized solid texture using a texture loss \cite{gatys2015texture} for optimization. 

Our approach to the stylization of smoke simulation sequences can be seen as spatiotemporal modeling of volumetric texture. 
It assigns texture to a bounded region around the smoke sequence, similar to the process of solid texture synthesis.
The synthesized texture volume is naturally dynamic and is trained to align with the input density sequence's motion from fluid advection governed by the Navier-Stokes Equations~\cite{bridson2015fluid}. 


\subsection{Volumetric Neural Style Transfer.}
Prior works on stylizing volumetric smoke data guided by exemplar images can be categorized into optimization-based methods and feed-forward methods. 
\textbf{Optimization based} algorithms \cite{kim19c, kimlnst, aurand2022efficient} extract style features using the pre-trained VGG \cite{vgg16} from both the exemplar image and the rendered smoke appearance, and iteratively optimize the density using style-based loss functions \cite{gatys2016image}. 
Transport-based style transfer (TNST)~\cite{kim19c} synthesizes complex stylization on Eulerian-based smoke, but suffers from lengthy optimization and temporal inconsistency due to its costly temporal smoothing scheme. 
The Eulerian framework observes fluid flow at fixed points in space over time.
To resolve these two drawbacks, \citet{kimlnst} reformulate TNST in a Lagrangian framework at the cost of protracted conversion time from smoke density to a particle-based representation.
Meanwhile, ~\citet{aurand2022efficient} enhance the performance by using lighter advection and temporal smoothing algorithms. 
However, These methods still suffer from the slow explicit modeling of advection, and the optimization time scales up with the number of input frames and the number of views used for training. 
Finally, these methods do not support color style transfer on 3D volume data. 
\textbf{Feed-forward} methods~\cite{guo2021volumetric, aurand2022efficient} train 3D CNNs to directly output stylized smoke density, bypassing iterative optimizations. 
These methods ensure multi-view style consistency by directly processing 3D data.
\citet{guo2021volumetric} use a 3D volume autoencoder and trains a model for arbitrary style images while incorporating a regularization for temporal consistency. 
However, it requires 70+ hours of training, and in practice, not all images can produce multiview consistent stylization for volume data.
\citet{aurand2022efficient} train separate networks for each style and smoke pair to improve the multi-view consistency, taking 18 hours to train one network. 
However, it has no explicit control on temporal coherence except implicit translational equivariance. The use of tiling due to the memory requirement to output the full stylized smoke at inference further harms temporal consistency.  

Our volume stylization technique presents a novel interpretation by framing volumetric stylization as a texturing problem, synthesizing 3D textures that are naturally dynamic. 
VNCA significantly speeds up training compared to the prior works. At the same time, our method maintains multiview stylization coherence and temporal smoothness across each stylized frame of 3D smoke data. 
Inspired by advances in 2D dynamic texture~\cite{pajouheshgar2023dynca}, we imitate fluid advection by controlling the self-emerging motion pattern of NCA to preserve a natural temporal consistency. 





\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{img/update.pdf}
  \caption{\textbf{Update Rule.} We use the update rule to determine the cell state at the next time step. We obtain the perception vector with neighboring cell state and concatenate density and positional encoding as priors. The stochastic update processes this vector for an update to each cell.}
  \label{fig:update}
\end{figure}


