
\input{figures/attention}

\section{Empirical Analysis of Unified Transformers}
In this section, we first introduce the unified transformers~\cite{showo,emu3} and the Mixture of Depths (MoD)\cite{mod} (Sec.\ref{sec:preliminary}). We then separately analyze the attention weights (Sec.\ref{sec:attention_map}), evaluate token redundancy in unified transformers (Sec.\ref{sec:topk_selection}), and analyze the interactions across different tasks (Sec.\ref{sec:SH_gumbel}). While Show-o serves as the main model for our experiments, we also investigate variations in other unified transformers~\cite{janusflow,luminamgpt,emu3} to gain a more comprehensive insight.



\subsection{Preliminary}
\label{sec:preliminary}

% \begin{table}[htbp]
% \centering
% \begin{tabular}{l|l}
% \hline
% \textbf{Task Type}                        & \textbf{Example Models}              \\ \hline
% AR + Diffusion/Flow Matching              & Show-o, Transfusion, JanusFlow       \\
% Both AR                                    & Chameleon, Emu3                     \\ 
% \end{tabular}
% \caption{Types of unified multimodal transformer models and their representative examples.}
% \label{tab:unified_transformers}
% \end{table}



\textbf{Unified Multimodal Transformers}. Unified transformers typically come in two main types: one where both the generation and understanding tasks use AR-based methods, and the other where the generation task uses diffusion or flow matching while the understanding task employs autoregressive methods. We consider two representative unified multimodal transformer models: \textbf{Show-o}~\cite{showo} and \textbf{Emu3}~\cite{emu3}.

\textbf{Show-o} is a unified vision and language foundation model capable of both generation and comprehension. Based on the Phi language model~\cite{phi1.5}, Show-o adopts a discrete diffusion mode for generation and an autoregressive mode for understanding. To support both modeling approaches, Show-o employs two learning objectives:
Next Token Prediction (NTP): For generative tasks, given $M$ image tokens $\mathbf{u} = \{u_1, \ldots, u_M\}$ and $N$ text tokens $\mathbf{v} = \{v_1, \ldots, v_N\}$, the model maximizes the likelihood of the text tokens:
\begin{equation}
\mathcal{L}_{\text{NTP}} = \sum_{i=1}^{N} \log p_\theta(v_i \mid v_{1}, \ldots, v_{i-1}, u_{1}, \ldots, u_{M}).
\end{equation}
Mask Token Prediction (MTP): For comprehension tasks, given a sequence with masked image tokens $u_j^*$, the model maximizes the likelihood of the masked tokens:

% \begin{equation}
% \mathcal{L}_{\text{MTP}} = \sum_{j=1}^{M} \log p_\theta(u_j^* \mid u_{1}, \ldots, u_{j-1}, u_{j+1}, \ldots, u_{M}, v_{1}, \ldots, v_{N})
% \end{equation}

\begin{equation}
\mathcal{L}_{\text{MTP}} = \sum_{j=1}^{M} \log p_\theta(u_j^* \mid \mathbf{u}_{\setminus j}, \mathbf{v}).
\end{equation}

\textbf{Emu3}, on the other hand, employs a fully autoregressive approach for both generation and multimodal understanding tasks. Its training objective is the standard next-token prediction across both modalities:

\begin{equation}
\mathcal{L}_{\text{AR}} = \sum_{i=1}^{N+M} \log p_\theta(x_i \mid x_{1}, \ldots, x_{i-1}),
\end{equation}
where $x_i$ represents the $i$-th token in the combined sequence of image and text tokens.




\textbf{Mixture of Depths (MoD)}~\cite{mod}. MoD is an efficient training method that reduces the number of tokens processed in each layer of a Transformer. Each layer employs a router that decides which tokens will be processed by that block. Tokens that are not chosen will skip the layer and proceed to the next layer. Thus, MoDs can be written as 


\begin{equation}
x_i^{*} =
\begin{cases}
x_i + D(x_i) R(x_i), & \text{if } R(x_i) \geq \delta_s \\
x_i, & \text{if } R(x_i) < \delta_s
\end{cases}
.
\end{equation}

Formally, let $x_i \in \mathbb{R}^d$ denote the token vector in $x$, and let $\delta_s$ be the routing threshold at each layer. $D_i$ represents the $i$-th layer of the Transformer. $R_j(\cdot)$ is the corresponding routing function. Tokens that are not chosen will skip the layer, reducing computational cost. While Mixture of Experts (MoE)~\cite{deepseekmoe} is a scaling method that increases model capacity by activating only a subset of experts per token, it does not directly reduce training cost. On the other hand, MoD focuses on reducing the number of tokens processed at each layer by routing unnecessary tokens away, leading to a reduction in computational cost. Therefore, in our work we adopt MoD to train the unified transformers efficiently.


% Sequently, we analyze the unified transformers in attention weights, layer importance and interaction across tasks. Anaylzing attention weights is to explore whether tokens of different modalities are influenced by different tasks. Exploring layer importance is to decide to select which layer should be pruned. Exploring the interaction across tasks is to 




% inter-task; intra-task
\subsection{Attention Weights}
\label{sec:attention_map}
We analyze the differences in attention weight patterns across tasks and modalities in unified transformers. We choose four unified models—Show-o~\cite{showo}, JanusFlow~\cite{janusflow}, Emu3~\cite{emu3} and Lumina-mgpt~\cite{luminamgpt}—to calculate the average attention weights received by image and text tokens across various layers. Based on the experimental results in Fig.~\ref{fig:attention_map_weights}, we draw an observation regarding the impact of different tasks on attention weight distribution.


\textbf{Observation 1}: \textit{Attention weight patterns of different modalities show significant differences depending on the task.}



As shown in Fig.~\ref{fig:attention_map_weights}, for Show-o, JanusFlow and Emu3, the attention weight patterns differ significantly between tasks. However, in Lumina-mgpt, the attention weight patterns are very similar across both tasks.


Different tasks lead to distinct modeling approaches and sequence organizations in unified transformers. Specifically, in the Show-o model, generation tasks utilize diffusion-based methods, while understanding tasks rely on autoregressive methods. Similarly, JanusFlow employs different modeling strategies for each task, resulting in varying attention weight patterns. Sequence organization refers to the relative positioning and arrangement of different modalities (e.g., text and image tokens) within the input sequence. In Emu3, although both tasks use autoregressive methods, differences in sequence organization lead to variations in modality performance. In contrast, Lumina-mgpt adopts an interleaved data training approach with consistent modeling and sequence design for both tasks, resulting in similar attention weight patterns across tasks.

% Different tasks lead to distinct modeling approaches and sequence organizations in unified transformers. For example, the Show-o model utilizes diffusion-based methods for generation tasks and autoregressive methods for understanding tasks, while JanusFlow employs varying strategies for each task, resulting in different attention weight patterns. In Emu3, both tasks use autoregressive methods, but differing sequence organizations cause variations in modality performance. Conversely, Lumina-mgpt adopts an interleaved training approach with consistent modeling and sequence design for both tasks, leading to similar attention weight patterns across tasks.



Through observing the attention weight patterns, we realize that the importance of image and text tokens varies across tasks. Therefore, during pruning, we consider redundancy in tokens from all modalities, making the goal to prune tokens across both image and text modalities.




% We speculate that these differences are due to the modes chosen for different tasks in Show-o and JanusFlow. In contrast, Lumina-mgpt employs an autoregressive mode for both tasks, leading to more similar attention weight patterns.


% \textbf{Observation 2}: \textit{Special tokens receive significantly higher attention weights than other token types in Show-o and JanusFlow, indicating their importance in the final results. In contrast, for Lumina-mgpt, special tokens do not differ as much from other token types.}

% As shown in Fig.~\ref{fig:attention_map_weights}, special tokens in Show-o and JanusFlow consistently have higher attention weights compared to other token types, regardless of the layer or task. Image and text tokens remain relatively low and stable throughout. However, in Lumina-mgpt, special tokens do not show such a high degree of difference in attention weights compared to other token types.





% \subsection{Analyzing Token Redundancy Across Layers and Tasks in the Unified Model}
% \subsection{Analyzing Layer Importance and Token Redundancy Across Tasks in Unified Transformers}
% \input{figures/arank}



\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/arank_fig.pdf}
    % \caption{ARank variations across different layers and tasks for four unified transformers: Show-o, JanusFlow, Emu3 and Lumina-mgpt.}
    \vspace{-15pt}
    \caption{ARank variations across different layers for four unified transformers: Show-o, JanusFlow, Emu3, and Lumina-mgpt. ARank, defined as the rank of the attention map, represents sequence redundancy within each layer. Higher ARank values indicate lower sequence redundancy within each layer. }
    \label{fig:arank_showo}
    \vspace{-10pt}
\end{figure*}

%  (\textit{x-axis: layer index, y-axis: ARank value})}




\subsection{Layer Importance and Token Redundancy}
\label{sec:topk_selection}

% We explore the significant variation in the importance of different layers and token redundancy for each task from two perspectives. First, we conduct simple training and inference experiments to analyze the benchmark performance in the Show-o model. Second, we use the ARank metric~\cite{gamma-mod} to evaluate token redundancy across layers in different unified transformers.




We explore the significant variation in the importance of different layers and token redundancy for each task from two perspectives. First, we conduct simple inference experiments to analyze the benchmark performance in the Show-o model. Second, we use the ARank metric~\cite{gamma-mod} to evaluate token redundancy across layers in different unified transformers.

% In the Show-o model, during the inference stage, we skip the odd-numbered layers and evaluate its performance on the GQA benchmark, as shown in Tab.\ref{tab:inference_skip_layer}. Then, we apply the straightforward MoD method during training, as illustrated in Tab.\ref{tab:topk_selection}. We compare the results of two token pruning strategies: one where 40\% of the tokens are uniformly removed, and another where the token capacity is dynamically decreased as the layer index increases. To evaluate performance, we utilize major understanding benchmarks—POPE~\cite{pope}, MME~\cite{mme}, GQA~\cite{gqa}, MMMU~\cite{mmmu}, VQAv2~\cite{vqa_v2}—and the GenEval~\cite{geneval} benchmark for generation tasks. 


In the Show-o model, during the inference stage, we skip the odd-numbered layers and evaluate its performance on the GQA benchmark~\cite{gqa}, as shown in Tab.\ref{tab:inference_skip_layer} and we draw one observation.

\input{tables/table_inference}

\textbf{Observation 2}: \textit{The contribution of each layer to the final outcome is different.}

% As shown in Tab.~\ref{tab:inference_skip_layer}, GQA performance declines more significantly when tokens are skipped in the early layers compared to the late layers. This indicates that early layers are more critical for achieving optimal results. Additionally, as shown in Tab.~\ref{tab:topk_selection}, retaining more tokens in the early layers leads to higher benchmark scores for both tasks when tokens are skipped on average across layers. These findings further confirm that early layers contribute more substantially to the final outcomes in the Show-o model.
As shown in Tab.~\ref{tab:inference_skip_layer}, GQA performance declines more significantly when tokens are skipped in the early layers compared to the late layers. This indicates that early layers are more critical for achieving optimal results.

Furthermore, we quantitatively assess the redundancy of tokens within each layer. Drawing inspiration from $\gamma$-mod~\cite{gamma-mod}, which utilizes the Attention Map Rank (ARank) metric to evaluate token-level redundancy in a layer, we apply this metric to analyze the unified transformers. ARank is calculated as the mean rank of the attention matrices:

% \begin{equation}
% \tau(x_i, D_i) = \frac{1}{n_h} \sum_{h=1}^{n_h} \text{rank}(A_h),
% A_h = (W_{Qh} x_i)(W_{Kh} x_i)^\top 
% \end{equation}

\begin{equation}
\begin{aligned}
\tau(x_i, D_i) = \frac{1}{n_h} \sum_{h=1}^{n_h} \text{rank}(A_h),  
A_h = (W_{Qh} x_i)(W_{Kh} x_i)^\top.
\end{aligned}
\end{equation}


In these equations, \( \text{rank}(\cdot) \) denotes the matrix rank operation, \( n_h \) represents the number of attention heads, \( A_h \in \mathbb{R}^{l \times l} \) is the attention map for the \( h \)-th head, and \( W_{Qh} \in \mathbb{R}^{d \times d_h} \) and \( W_{Kh} \in \mathbb{R}^{d \times d_h} \) are the weight matrices for the query and key projections, respectively. This metric allows us to quantify the redundancy of tokens in each layer, providing insights into how different tasks and layers contribute to overall model efficiency. A layer with a low ARank means that most of its tokens are less informative. Based on the experimental results as shown in Fig.~\ref{fig:arank_showo}, we draw two observations regarding the token redundancy between different tasks.

% \textbf{Observation 3}:\textit{The number of redundant tokens varies between the t2i and mmu tasks.}

% \textbf{Observation 3}: \textit{The number of redundant tokens varies between the T2I and MMU tasks.}

\textbf{Observation 3}: \textit{The number of redundant tokens differs significantly between the generation and understanding tasks.}

As illustrated in Fig.~\ref{fig:arank_showo}, ARank values differ between tasks in the Show-o and JanusFlow models. In Show-o, the Text-to-Image(T2I) generation sequence has significantly higher ARank values than the Multi-Modal Understanding(MMU) sequence, indicating more redundant tokens in the MMU task. Conversely, Lumina-mgpt and Emu3 exhibit similar redundancy levels across both tasks. We attribute this difference to their modeling approaches: Show-o and JanusFlow use diffusion or flow matching for generation and autoregressive models for understanding, while Lumina-mgpt and Emu3 employ autoregressive methods for both tasks.
% This discrepancy arises from the distinct objectives of the two tasks: The understanding task primarily utilizes image tokens to generate text tokens, whereas the generation task leverages image tokens to compute loss and optimize the model. Consequently, image tokens in the generation task hold greater significance, resulting in higher ARank values.


\textbf{Observation 4}: \textit{The redundancy of token sequences differs significantly across layers for different tasks.}

As shown in Figs.\ref{fig:arank_showo}, ARank values are significantly higher in the early layers compared to the later layers. In the Show-o model, the ARank value for the MMU task decreases as the layer index increases. In the T2I task, ARank values also show a substantial decrease in the later layers. All three models—JanusFlow, Lumina-mgpt, and Emu3—exhibit variations in ARank values across layers, indicating differing levels of token redundancy within each layer. We speculate these variations across layers may be related to the language models used.

Based on the analysis of layer importance and token redundancy, we conclude that pruning should focus on tokens in layers with high redundancy. The ARank metric can be used to identify such layers and determine the proportion of tokens to prune for both tasks.


% In the understanding task, the ARank value of each layer decreases as the layer ID increases. In the generation task, the same phenomenon with understanding does not happen. The ARank value is generally average in the first two-thirds of the layers, and it drops significantly in the last one-third of the layers. According to the phenomenon, we realize that we cannot simply use a unified strategy to prune tokens of all layers in different tasks.

% \input{tables/table_relation}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/pipeline2.pdf}
    

    \caption{Pipeline of \method{}. The Layer Switch Module transforms dense transformer layers into three specialized types: T2I MoD layers for Text-to-Image (T2I) generation, MMU MoD layers for Multi-Modal Understanding (MMU), and Shared MoD layers for both tasks. For each task, task-aware routers with distinct capacities prune tokens of different modalities, thereby enhancing computational efficiency and maintaining performance across tasks.}

    \label{fig:pipeline}
    \vspace{-10pt}
\end{figure*}



\begin{figure}[h] % 使用标准的 figure 环境
    \centering
    \includegraphics[width=0.48\textwidth]{figures/arank_plot_gumbel4.pdf}
    \vspace{-15pt}
    \caption{Token weight assignment using Gumbel Softmax. A higher number of tokens assigned a weight of 1 across layers indicates greater importance of generation task tokens compared to understanding task tokens.}
    \label{fig:gumbel_softmax}
    \vspace{-5pt}
\end{figure}

% \input{tables/table_relation}






% \subsection{Analyzing Token Importance Variations Across Tasks through Competitive Pruning}
\subsection{Interactions Between Tasks}
\label{sec:SH_gumbel}



% \textbf{Task Interactions in Unified Transformers.} To investigate whether simultaneous training of multiple tasks affects their performance, we conduct experiments in the Show-o model. Specifically, we evaluate the generation task and the understanding task under three training configurations: training each task independently and training both tasks concurrently. As shown in Tab.~\ref{tab:comparison_showo}, the benchmark results for both tasks remained largely consistent across all training conditions. The performance of each task was unaffected whether trained alone or in conjunction with the other task, indicating minimal mutual influence during training.

\textbf{Task Interactions in Unified Transformers.} We examine whether training multiple tasks simultaneously impacts performance using the Show-o model. Specifically, we assess the generation task and the understanding task under three configurations: training each task independently and training both concurrently. To evaluate performance, we utilize major understanding benchmarks—POPE~\cite{pope}, MME~\cite{mme}, GQA~\cite{gqa}, MMMU~\cite{mmmu}, VQAv2~\cite{vqav2}—and the GenEval~\cite{geneval} benchmark for generation tasks. As shown in Tab.~\ref{tab:comparison_showo}, the benchmark results for both tasks remained consistent across all training conditions. From these observations, we draw a key conclusion: \textbf{A large language model can effectively accommodate both tasks, as simultaneous training yields results comparable to training each task individually in the Show-o model.} The absence of a mutual enhancement effect may be attributed to Show-o modeling approach, which employs a discrete diffusion pipeline for generation and an autoregressive mode for understanding. 


\input{tables/table_relation}

% \textbf{Competitive Token Pruning Between Two Different Tasks.} We introduce a competitive setting where tokens from different tasks compete for selection, allowing us to assess their relative importance across tasks. We utilize the Straight-Through Gumbel-Softmax method for token pruning by assigning binary weights (0 or 1) to each token. To manage token redundancy effectively, we set the router capacity to 0.5 and introduce an auxiliary loss term \( \mathcal{L}_{\text{aux}} = P \sum_{i}(r_i - P)^2 \), where \( r_i \) is the capacity of the \( i \)-th layer (\( i \leq L \)), ensuring that each layer respects the budget constraint. The formula of Straight-Through Gumbel-Softmax method will be provided in the appendix. By incorporating this loss, we introduce competition between tasks, ensuring that only half of the tokens (capacity = 0.5) from both the T2I and MMU tasks are processed by each layer. Based on the experiment results in Fig.~\ref{fig:gumbel_softmax}, we draw one observation.

 % \( \mathcal{L}_{\text{aux}} = P \sum_{i}(r_i - P)^2 \), where \( r_i \) is the capacity of the \( i \)-th layer (\( i \leq L \))
 
\textbf{Competitive Token Pruning Between Two Tasks.} We establish a competitive framework where tokens from Text-to-Image (T2I) and Multi-Modal Understanding (MMU) tasks vie for selection, allowing us to evaluate their relative importance. Utilizing the Straight-Through Gumbel-Softmax method, we assign binary weights (0 or 1) for pruning. To manage token redundancy, we set the router capacity to 0.5 and introduce an auxiliary loss to enforce budget constraints, ensuring each layer processes only half of the tokens from both tasks. The formula for the Straight-Through Gumbel-Softmax method is provided in the Appendix~\ref{gumbel_softmax_formula}. Based on Fig.~\ref{fig:gumbel_softmax}, we draw the following observation.

% \textbf{Observation 5}:\textit{The higher weights assigned to the generation tokens indicate that they are more important to the final results compared to the understanding tokens in the Show-o model.}


\textbf{Observation 5}: \textit{In the Show-o model, generation tokens are assigned higher weights, indicating that they contribute more significantly to the final results compared to understanding tokens.}

As shown in Fig.~\ref{fig:gumbel_softmax}, T2I sequence tokens are mostly assigned weights of 1, indicating they are largely retained. In contrast, MMU tokens receive lower weights, leading to higher pruning rates. This suggests that T2I tokens are more critical for loss optimization. We believe that uniformly processing tokens leads to an imbalance, causing one task to retain excessive tokens and potentially degrading the performance of the other. The interaction results in the Show-o model show mutual enhancement between tasks is minimal. These findings suggest pruning tokens separately for each task may result in more balanced and effective performance.


% We find in the Show-o model, the mutual enhancement is not obvious. At the same time, we find the token importance is different across two tasks. We speculate that uniformly processing tokens across tasks creates an imbalance, causing one task to retain too many tokens and potentially degrading the other task's performance. To address this, we propose pruning tokens separately for each task.




% \textbf{Observation 6}:\textit{As the number of layers increases, the importance of T2I tokens continues to rise, while the MMU tokens become less influential in the Show-o model.}

% Furthermore, in deeper layers, the importance of T2I tokens increases while that of MMU tokens decreases. More T2I tokens are assigned a weight of 1 with each layer, indicating that the model increasingly focuses on T2I tokens to optimize the final loss and enhance the quality of generated outputs.




