

\section{Experiment}

\subsection{Implementation Details}

We choose Show-o~\cite{showo} and Emu-3~\cite{emu3} as representative models for our approach to token pruning, as they cover different types of unified transformers. Show-o integrates the diffusion pipeline with an autoregressive mode, addressing both generative and understanding tasks using distinct mechanisms. In contrast, Emu-3 employs the autoregressive mode for both tasks. By selecting these two models, we demonstrate that our method is applicable to a wide range of unified models.

In the Show-o model, when selecting layers to convert into MoD (Mixture of Depths) layers, we transform the last 12 layers into MoD layers for both tasks. For the Multi-Modal Understanding (MMU) task, we scale the capacity from 1 down to 0.2. For the Text-to-Image (T2I) task, we prune 20\% of the tokens in the later layers. We use a batch size of 10 for both the MMU and T2I tasks. For the T2I task, we use the image datasets from the original Show-o model, and for MMU, we employ the Cambrian dataset~\cite{cambrian}. Emu3 does not provide the training code or data required for MMU tasks. To validate our method, we employ the LLaVA-v1.5-mix-665K dataset to represent MMU capabilities and augment the training pipeline with additional MMU-specific code. For the T2I task, we use the same image data as the Show-o model. We finetune the model using 8 H100 GPUs, pruning 80\% of the tokens in each of the last 16 layers. Further details regarding the datasets and implementation are provided in the Appendix~\ref{showo_dataset}.




\subsection{Quantitative Results}

To evaluate multimodal understanding, we use the POPE\cite{pope}, MME~\cite{mme}, VQAv2~\cite{vqav2}, GQA~\cite{gqa}, and MMMU~\cite{mmmu} benchmarks. For generation capabilities, we rely on the GenEval~\cite{geneval} benchmark. To assess efficiency, we measure TFLOPs and training speed. Following the practice in DiT~\cite{dit}, we estimate the training compute as model TFLOPs × batch size × 3. The factor of 3 approximates the backward pass as requiring twice the compute of the forward pass. 


\input{tables/table_baselines4}
% \input{tables/table_emu3}
\input{tables/table_cost}


\textbf{Baselines.} We compare our approach with various baselines, as shown in Tab.\ref{tab:table_baselines3}. \textit{Full Computation.} Using the vanilla training pipeline, all visual tokens and text tokens are passed through all transformer layers without any pruning. \textit{Early Exit.} Building on studies of language-only LLMs~\cite{earlyexit1}, we adopt a similar method for our unified models by performing an early exit at the 12th layer. \textit{LayerSkip.} Following the approach introduced in previous LLM studies~\cite{skiplayer}, we skip all tokens in the interleaved layers. This is equivalent to our method with a capacity of 1 in odd-numbered layers and a capacity of 0 in even-numbered layers. 

As shown in Tab.~\ref{tab:table_baselines3}, although LayerSkip and EarlyExit consume fewer TFLOPs, their performance is significantly inferior to the full computation model, especially for the T2I task. In contrast, our method achieves the best balance between performance and efficiency. We reduce the training FLOPs while maintaining comparable performance and even attaining better results on some benchmarks in the Show-o model. As shown in Tab.~\ref{tab:table_baselines3}, in the Emu model, our method reduces FLOPs by 40\%, while achieving comparable or better results compared to the full computation model in both T2I and MMU tasks. Our full computation results of Emu3 differ from the original paper because we use different training datasets due to the absence of relevant code and data in the public repository.




% \textbf{Training Cost.} As shown in Tab.~\ref{tab:training_costs}, our method not only reduces the FLOPs but also improves memory usage and training speed. Compared to the Show-o model, our approach applied to the Emu3 model is more efficient, particularly in terms of training speed. We speculate that this difference stems from the design of their image tokenizers. Emu3 represents an image with 4096 tokens, while Show-o employs 1024 tokens. This increased number of tokens introduces more redundancy, thereby enhancing the efficiency of our method.


\textbf{Training Cost.} As shown in Tab.~\ref{tab:training_costs}, our method not only reduces FLOPs but also improves memory usage and training speed. In the Emu3 model, our approach is more efficient than Show-o in terms of FLOPs and training speed. We attribute this difference to the design of their image tokenizers: Emu3 uses 4096 tokens per image, while Show-o uses 1024 tokens. More tokens in Emu3 introduces more redundancy, thereby enhancing the efficiency of our method. The improvement in memory usage is less significant, likely due to Emu3's larger model size. Specific reasons are further discussed in the Appendix.~\ref{discussion_costs}.

\input{tables/table_ablation}
\subsection{Ablation Studies}

% \textit{Interleaved Layers.} Pruning interleaved layers instead of using ARank-based selection caused performance declines in both tasks. \textit{Single Router for Unified Models.} Using a single router to prune tokens from both tasks also led to suboptimal results. These findings highlight the effectiveness of ARank-based layer selection and the necessity of task-specific routing mechanisms.

% To evaluate the contribution of each design component in \method{}, we conduct an ablation study in the Show-o model, as shown in Tab.~\ref{tab:table_ablation}. The results demonstrate that pruning interleaved layers or employing a single router degrades performance. To ensure fairness, each ablation experiment maintains the same pruning rate as our method.

To evaluate the contribution of each design component in \method{}, we conduct an ablation study using the Show-o model, as shown in Tab.~\ref{tab:table_ablation}. We examine three configurations: (1) Basic MoD, which directly integrates MoD into the unified transformer; (2) Pruning Interleaved Layers, which selects interleaved layers for token pruning; and (3) Single Router, employing a single router to prune tokens for both tasks while selecting specific layers. Applying basic MoDs results in the poorest performance on both tasks. Using separate routers at interleaved layers slightly improves generation compared to basic MoDs, but outcomes remain suboptimal. Employing a single router for both tasks at specific layers slightly worsens understanding results and severely degrades generation performance. In contrast, our method achieves superior performance by effectively balancing efficiency and effectiveness. To ensure fairness, each ablation experiment maintains the same pruning rate as our method.








\subsection{Adaptation to Diffusion Models}

While our method is primarily designed for unified transformers, we also demonstrate its effectiveness in training and fine-tuning generation models such as DiT~\cite{dit} and PixArt~\cite{pixart}. Experimental results and implementation details are provided in the Appendix~\ref{adaption_to_diffusion}.









% \input{tables/table_pixart}
% \input{tables/table_dit}


% \subsection{Adaptation to Diffusion Models}

% \textbf{MoD for Generation Models.} While our method is primarily designed for unified transformers, we also validate its effectiveness for training or fine-tuning generation models such as DiT~\cite{dit} and PixArt~\cite{pixart}. For the DiT model, we select the checkpoint trained for 50k iterations as the base model to calculate the ARank. For the PixArt model, we use the final model as the base and fine-tune it using our internal high-quality data (used in the Show-o fine-tuning stage). We evaluate the DiT model using the ImageNet~\cite{imagenet} FID metric, and for PixArt, we use the GenEval benchmark to assess generation quality. Both models process images at a resolution of 256×256 pixels. 

% For PixArt, we conduct three experiments. First, we present the results using full computation. Second, we prune 40\% of the tokens in the interleaved layers. Finally, we calculate the ARank value for each layer and select the 14 layers with the lowest ARank values to prune 40\% of the tokens. As shown in Tab.~\ref{tab:table_pixart}, our ARank-based MoD method achieves better performance compared to standard pruning approaches at the same computational cost, with only a slight reduction in performance relative to full computation.


% For DiT, we select the checkpoint trained for 50k iterations as the base model to calculate the ARank. Then we choose the least value  14 layers to prune tokens. In practice, we gradually scale the token capacity from 1 to 0.2 over the course of 500k training iterations. Finally, we evaluate the model using a capacity of 0.4, as shown in Tab.~\ref{tab:table_dit}.

% From these experiments, we observe that the token sequences in both DiT and PixArt exhibit higher redundancy compared to those in Show-o. We attribute this difference to the design of their image tokenizers. The VAEs used in DiT and PixArt downsample images by a factor of 8, whereas Show-o's tokenizer downsamples by a factor of 16. Consequently, for images of the same resolution, DiT and PixArt require more tokens to represent the image than Show-o.


