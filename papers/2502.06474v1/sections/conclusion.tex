\section{Conclusion}

% In this work, we explore an efficient training method for unified transformers. Through examining attention map weights, computing ARank values, and evaluating token importance across different tasks, we analyze the redundancy present in the sequences of unified transformers in detail. Building on these insights, we introduce a task-aware token pruning method for training unified transformers. Our approach achieves a balance between performance and efficiency, reducing FLOPs while maintaining comparable results and even improving performance on certain benchmarks.



% In this work, we propose an efficient training method for unified transformers by analyzing attention map weights, layer importance, and task interactions to identify sequence redundancy. Using these insights, we introduce a task-aware token pruning approach that reduces FLOPs while maintaining or improving performance across various benchmarks. Our method strikes an optimal balance between efficiency and performance, demonstrating its applicability to unified transformers.



In this work, we present an efficient training method for unified transformers by analyzing attention weights, layer importance, and task interactions to identify sequence redundancy. Using these insights, we introduce a task-aware token pruning approach that reduces FLOPs while maintaining or enhancing performance across various benchmarks. Our method effectively balances efficiency and performance, demonstrating its applicability to unified transformers.