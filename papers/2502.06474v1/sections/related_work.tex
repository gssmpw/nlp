\section{Related Work}
\label{sec:related_work}

% \subsection{Unified Multi-modal Foundation Model}
\subsection{Unified Multi-modal Transformers}
Recently, there are several research work~\cite{seed-x,wu2023next,team2024chameleon,showo,lwm,sun2023emu,transfusion,CoDI,dreamllm,emu3,janusflow,luminamgpt,liquid,llamafusion,anil2023gemini,tokenflow,synergen,Orthus,omniflow} focusing on unified transformers that are capable of both generation and comprehension. Chameleon~\cite{team2024chameleon} and Emu3~\cite{emu3} employs an autoregressive approach for both generation and understanding tasks. SEED-X~\cite{seed-x} introduces a unified system for multimodal understanding and generation, incorporating a diffusion model alongside an LLM for generation tasks. Transfusion~\cite{transfusion} utilizes discrete tokens to represent texts and continuous embeddings to represent images. It integrates continuous diffusion methods with autoregressive approaches to handle generation and understanding tasks effectively. Show-o~\cite{showo} adopts discrete diffusion for generation and employs an autoregressive mode for understanding within a single model. JanusFlow~\cite{janusflow} combines the flow matching method for generation with the autoregressive method for understanding. However, all of these models require substantial resources for training. 




% In our work, we choose Show-o~\cite{showo} as a base model to explore efficient training methods.


\subsection{Sparse Computation for Transformers}
\textbf{Language Only.} Recently, Large Language Models (LLMs) have developed rapidly, resulting in the need for ever-increasing computational resources~\cite{llama,phi1.5,phi3,gpt3,gpt1}. As a result, much research~\cite{kvcache,layerskip,imageworth,attentionsink} focuses on sparse computation in LLMs. Mixture of Experts (MoE)\cite{surveymoe,openmoe,deepseekmoe,mixturalmoe} is a popular method that replaces the feed-forward (FFN) layers of transformer blocks with MoE layers, where input tokens are dynamically processed by the top-K experts via a router. However, MoE does not reduce training costs and is only efficient during inference. Mixture of Depths (MoD)\cite{mod} adopts a router at interleaved layer to decide whether tokens bypass the entire layer, thereby enhancing computational efficiency. Besides, there are several work using skip layers or early exit for sparse computation for LLMs~\cite{skiplayer,skipdecoder,earlyexit1}


\textbf{Multimodal Understanding and Generation.} 
MoE-LLaVA~\cite{moellava} and $\gamma$-MoD~\cite{gamma-mod} investigate Mixture of Experts (MoE) and Mixture of Depths (MoD) in multimodal large language models (MLLMs)\cite{llava,llava1.5,qwenvl,llava-next}, with $\gamma$-MoD introducing the ARank metric to assess token redundancy per layer. VideoLLM-MoD\cite{videollmmod} applies MoD to video LLMs~\cite{videollm}. MoMa~\cite{moma} integrates MoE and MoD into the Chameleon model~\cite{team2024chameleon}. However, it lacks results on generation tasks and most understanding benchmarks. Additionally, its application of MoD involves only a simplistic combination, without a design tailored for unified transformers. In the generation domain, concurrently, several studies~\cite{layermoddit,lazydit} have explored MoD methods for continous diffusion transformer models~\cite{dit}. In our work, we apply MoD to various unified transformers, demonstrating comparable or even improving results with fewer computational resources.















