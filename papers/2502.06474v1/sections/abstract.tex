\begin{abstract}


Unified multimodal transformers, which handle both generation and understanding tasks within a shared parameter space, have received increasing attention in recent research. Although various unified transformers have been proposed, training these models is costly due to redundant tokens and heavy attention computation. In the past, studies on large language models have demonstrated that token pruning methods, such as Mixture of Depths (MoD), can significantly improve computational efficiency. MoD employs a router to select the most important ones for processing within a transformer layer. However, directly applying MoD-based token pruning to unified transformers will result in suboptimal performance because different tasks exhibit varying levels of token redundancy. In our work, we analyze the unified transformers by (1) examining attention weight patterns, (2) evaluating the layer importance and token redundancy, and (3) analyzing task interactions. Our findings reveal that token redundancy is primarily influenced by different tasks and layers. Building on these findings, we introduce UniMoD, a \textbf{task-aware token pruning} method that employs a separate router for each task to determine which tokens should be pruned. We apply our method to Show-o and Emu3, reducing training FLOPs by approximately 15\% in Show-o and 40\% in Emu3, while maintaining or improving performance on several benchmarks. Code will be released at \url{https://github.com/showlab/UniMoD}.



% Unified multimodal transformers\cite{showo,emu3}, which handle both generation and understanding tasks within a shared parameter space, have gained attention in recent research. Despite various proposals, training these models is costly due to heavy attention computation and redundant tokens. Token pruning methods, such as Mixture of Depths (MoD)\cite{mod}, have shown potential to improve efficiency. However, directly applying MoD pruning to unified transformers results in suboptimal performance because different tasks use different modeling approaches. We analyze unified transformers by (1) examining attention patterns, (2) evaluating layer importance and token redundancy, and (3) analyzing task interactions. Our findings show that each task has distinct token importance and redundancy patterns. Based on these insights, we introduce task-aware token pruning, using a separate router for each task to selectively prune tokens. We apply this method to Show-o and Emu3, reducing training FLOPs by approximately 20\% in Show-o and 40\% in Emu3, while maintaining or improving performance on several benchmarks.

\end{abstract}






