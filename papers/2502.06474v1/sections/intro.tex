\section{Introduction}
\label{sec:intro}

Unified multimodal transformers have received growing attention due to their ability to handle both generation and understanding tasks within a shared parameter space. Recent studies~\cite{showo,team2024chameleon,seed-x,emu3,sun2023emu,x-vila,janusflow,transfusion,luminamgpt} have explored different approaches for unified transformers. These models can be broadly categorized into two types: one where both generation and understanding tasks are handled using a fully autoregressive method~\cite{seed-x,team2024chameleon,emu3}, and another where generation tasks are addressed using diffusion or flow matching techniques, while autoregressive methods are used for understanding tasks~\cite{showo,transfusion,janusflow}. However, regardless of the approach, training these unified transformers remains time and memory intensive, primarily due to token redundancy and the computationally heavy attention mechanisms. In the past, few studies have explored efficient training strategies for these models. Therefore, developing efficient methods for training unified transformers remains a significant challenge.

% In previous works on LLMs~\cite{mod} and MLLMs~\cite{gamma-mod,videollm_mod}, methods like Mixture of Depths (MoD) employ a router to estimate token importance and improve efficiency through targeted token pruning. $\gamma$-MoD~\cite{gamma-mod} introduces the ARank metric to measure token redundancy in each layer’s attention maps, helping to identify layers suitable for pruning in MLLMs~\cite{llava,llava1.5}. However, unlike language-only or large multimodal models, unified transformers must handle multiple tasks with distinct training targets, which complicates the management of token redundancy. Tokens of the same type (e.g., image or text) may vary in importance depending on whether the model is engaged in a generation or understanding task. As a result, simply transferring previous pruning approaches that perform well in simpler settings may fail when applied directly to unified models. In addition, it is not yet clear how different tasks interact when they share the same model parameters. Their importance may vary across layers, and the level of token redundancy may shift as the model processes information. Without careful analysis, it is challenging to design a token pruning strategy that adapts to these variations.


% In previous works on LLMs~\cite{mod} and MLLMs~\cite{gamma-mod,videollm_mod}, methods like Mixture of Depths (MoD) employ a router to estimate token importance and improve efficiency through targeted token pruning. $\gamma$-MoD~\cite{gamma-mod} introduces the ARank metric to measure token redundancy in each layer’s attention maps, helping to identify layers suitable for pruning in MLLMs~\cite{llava,llava1.5}. However, simply transferring previous pruning approaches that perform well in simpler settings may fail when applied directly to unified models. There are two main reasons. (1) Unified transformers must handle multiple tasks with distinct training targets. Tokens of the same type (e.g., image or text) may vary in importance depending on whether the model is engaged in a generation or understanding task. (2) Different layer has different importance. Sequence importance  and token redundancy may vary across layers. Without careful analysis, it is challenging to design a token pruning strategy that adapts to these variations.

% Specifically, $\gamma$-MoD~\cite{gamma-mod} introduces the ARank metric to measure token redundancy within each layer’s attention maps, facilitating the identification of layers suitable for pruning in MLLMs~\cite{llava,llava1.5}.


% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figures/intro2.pdf}
%     % \caption{Challenges of Directly Applying Mixture of Depths (MoD) to Unified Transformers. This straightforward approach leads to suboptimal performance.}
%     \caption{Challenges of Directly Applying Mixture of Depths (MoD) to Unified Transformers. Using a single router to uniformly prune tokens across different tasks and interleaved layers results in suboptimal performance due to inconsistent token redundancy across tasks and layers.}
%     \label{fig:intro}
%     \vspace{-10pt} 
% \end{figure}


\begin{figure*}
    \centering
    % \includegraphics[width=1\linewidth]{figures/intro3.pdf}
    \includegraphics[width=\textwidth]{figures/intro3.pdf}
    \vspace{-15pt} 
    \caption{
        (a) Pipeline and challenges of applying Mixture of Depths (MoD) to unified transformers. A single router prunes tokens across tasks and layers, leading to suboptimal performance due to inconsistent token redundancy.
        (b) Two key observations from our experiments on unified transformers, providing critical insights for our proposed method.
    }
    \label{fig:observation}
    \vspace{-12pt} 
\end{figure*}


The Mixture of Depths (MoD) approach has been employed in previous studies on large language models (LLMs)\cite{mod} and multimodal large language models (MLLMs)\cite{gamma-mod,videollmmod,pmod} to prune tokens. MoD employs a router to assign weights to tokens based on their significance, enabling the model to selectively prune less important tokens and reduce computational overhead. However, applying MoD to unified transformers presents unique challenges that are worth exploring. A straightforward application of MoD~\cite{moma} to unified transformers involves pruning tokens from sequences of any task, selecting and removing a fixed proportion of tokens at interleaved layer. 

% However, as shown in Fig.~\ref{fig:observation}(a), this straightforward approach results in suboptimal performance, due to two main factors. (1) Different modeling methods and training targets are required for unified transformers to handle multiple tasks. For example, in the Show-o model, generation tasks use diffusion-based approaches, while understanding tasks rely on autoregressive methods. These differences lead to varying levels of redundancy and importance in tokens of the same modality (e.g., image or text). (2) Different layers within unified transformers exhibit varying levels of importance, with sequence importance and token redundancy fluctuating across layers. This variability makes it challenging to design a uniform token pruning strategy that effectively adapts to the unique demands of each layer and task. Therefore, without a careful analysis of these factors, it is difficult to develop a token pruning strategy that accommodates the diverse requirements of unified transformer models.

However, as shown in Fig.~\ref{fig:observation}(a), this straightforward approach leads to suboptimal performance. A single router struggles to retain all important tokens and eliminate all redundant ones of all tasks. This limitation arises because different tasks exhibit varying levels of token redundancy at different layers. Consequently, employing a single router to uniformly prune tokens for both tasks is ineffective, as it cannot accommodate the distinct redundancy patterns inherent to each task. Therefore, without a thorough analysis of these factors, developing a token pruning strategy that meets the diverse requirements of unified transformer models remains challenging.

In this work, we conduct an empirical analysis of unified transformers from three different perspectives and present an effective solution. (1) We analyze attention weights in several unified transformers to explore whether different tasks and modalities influence attention weight patterns. (2) We evaluate layer importance and token redundancy to determine which layers should be pruned through a series of simple experiments. Firstly, our inference experiments demonstrate that different layers exhibit varying levels of importance. We then use the ARank metric~\cite{gamma-mod}, which refers to Attention Map Rank, to analyze the differences in token redundancy across layers and tasks. Higher values of ARank indicate lower token redundancy. (3) We explore the interactions between different tasks through two experiments. Firstly, we conduct experiments to assess how removing one task affects the benchmark performance of the other task. Additionally, we introduce a competitive setting where tokens from different tasks compete for selection, allowing us to compare their relative importance.

 % Using the straight-through Gumbel Softmax method, we assign binary selection weights to each token at every layer, allowing us to control the number of tokens processed and identify which task’s tokens are more dominant.

Based on our experimental results and analysis, we draw two important observations as shown in Fig.~\ref{fig:observation}(b). (1) Different layers within unified transformers exhibit varying levels of importance, with sequence importance and token redundancy fluctuating across layers.  (2) The token redundancy is different across different tasks due to distinct modeling methods. For example, in the Show-o model, generation tasks use diffusion-based approaches, while understanding tasks rely on autoregressive methods. These differences lead to varying levels of redundancy and importance in tokens of the same modality (e.g., image or text). Therefore, from these observations, we propose \method{}, a task-aware token pruning method for unified transformers. We transform several transformer layers into MoD blocks specialized for the generation task or the understanding task. Each task has its own router that assigns token weights, allowing the pruning process to adapt to each task. 

% In our experiments, we select Show-o and Emu3 as representative unified models. Show-o serves as an example of a model where generation and understanding tasks are handled using different modeling approaches, while Emu3 represents a model that applies a fully autoregressive approach to both tasks. The experiments demonstrate that our method is general and applicable to various unified transformer architectures, regardless of how tasks are modeled. Experiments show that our approach reduces FLOPs by 15\% for Show-o and by 40\% for Emu3, while maintaining or even improving performance on certain benchmarks. Furthermore, we find that this method is also applicable to pure diffusion-based generation models, such as PixArt~\cite{pixart} and DiT~\cite{dit}, further highlighting the versatility of our task-aware token pruning approach and its ability to be integrated into a variety of generation frameworks.

In our experiments, we select Show-o and Emu3 as representative unified transformers. Show-o handles generation and understanding tasks with distinct modeling approaches, while Emu3 employs a fully autoregressive approach for both tasks. Our method is general and applicable to various unified transformer architectures, regardless of how tasks are modeled. It reduces FLOPs by 15\% for Show-o and 40\% for Emu3, while maintaining or improving performance on certain benchmarks. Additionally, our approach extends to pure diffusion-based generation models like PixArt~\cite{pixart} and DiT~\cite{dit}, demonstrating the versatility and integration capability of our method across diverse generation frameworks.

Our main contributions are as follows.
\begin{itemize}[ itemsep=0em, topsep=0em]
% \begin{itemize}
    \itemsep0em 
    \item We conduct an empirical analysis of examining attention weights, layer importance and token redundancy, and the interactions between different tasks in the unified transformers.
    
    \item We identify that different modeling methods for different tasks lead to significant differences in token redundancy and importance across tasks. Furthermore, within the same task, different layers exhibit substantial variations in token redundancy and importance.
    
    \item To the best of our knowledge, we are the first work to propose a task-aware token pruning method for unified transformers, effectively reducing computation and memory usage while maintaining or improving performance.
    
\end{itemize}



% \begin{itemize}
%     \itemsep0em 
%     \item We conduct a detailed analysis of how different tasks interact in the same parameter space, examining attention weights, token redundancy at different layers, and each task’s token importance.

%     \item We find because of the modeling method is different for different task, token redundancy is different for different task. 
    
%     \item To the best of our knowledge, we are the first work to propose a task-aware token pruning method for the unified transformers, reducing computation and memory usage while maintaining stable or improved performance. We also confirm its effectiveness for continuous diffusion models.
% \end{itemize}

















