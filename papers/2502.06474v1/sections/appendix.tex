\section{Appendix}


% In this appendix, we present more implementation details of our experiments in Sec.\ref{showo_dataset}. Then we show the results of MoD adaptation to diffusion models in Sec.\ref{adaption_to_diffusion}. Furthermore, we present  the formula for the Straight-Through Gumbel Softmax in Sec.\ref{gumbel_softmax_formula}.

In this appendix, we provide additional implementation details of our experiments (Sec.~\ref{showo_dataset}), discussion about the training cost(Sec.~\ref{discussion_costs}), the results of adapting MoD to diffusion models (Sec.~\ref{adaption_to_diffusion}), and the formula for the Straight-Through Gumbel Softmax (Sec.~\ref{gumbel_softmax_formula}).



\subsection{More Implementation Details} 
\label{showo_dataset}
\textbf{Dataset Enhancement and Streamlined Training Workflow in the Show-o Model.} The original Show-o model consists of multiple training stages. In the first two stages, it is trained on the ImageNet~\cite{imagenet} dataset and large-scale text-image paired data to achieve effective text-image alignment. The third stage leverages high-quality data to develop generation capabilities, while the final two stages utilize the LLaVA dataset~\cite{llava} to enhance understanding capabilities.

In this work, we improve the Show-o training pipeline by incorporating additional understanding datasets and reducing the training process to two stages. The original Show-o exclusively used the LLaVA dataset~\cite{llava} for training its understanding component. However, data imbalance caused the model to develop generation capabilities before understanding capabilities, which adversely affected generation quality. To resolve this, we introduce the Cambrian dataset~\cite{cambrian} and internal high-quality data to fine-tune the Show-o model within a two-stage training framework. The model processes images at a resolution of 512×512 pixels. Our revised pipeline achieves comparable results while reducing computational resource usage. We reevaluate the primary benchmarks and compare them with the original Show-o results.


\textbf{Emu3.} The Emu3 public repository does not include the training code or data for MMU tasks. To address this, we utilize the LLaVA-v1.5-mix-665K dataset to represent MMU capabilities and enhance the training pipeline with additional MMU-specific code. We employ the same generation data as the Show-o model and fine-tune Emu3 for 2 epochs with a learning rate of 2e-5, processing images at a resolution of 512×512 pixels.

In our experiments, we use the Show-o and Emu3 checkpoints from the first training stage, which involves low-quality text-to-image generation and limited captioning capabilities, as base models. To validate our method, we fine-tune these models with high-quality image data and understanding QA data. The router utilizes a single linear network.


\subsection{Discussion on Training Costs}
\label{discussion_costs}

As shown in Tab.~\ref{tab:training_costs}, although Emu3 prunes more tokens and achieves greater reductions in FLOPs and training speed, its memory usage remains largely unchanged compared to the full computation model. This discrepancy arises from the significant size difference between Emu3 and Show-o, with Emu3 having 8.5B parameters compared to Show-o's 1.4B. In Emu3, the large number of parameters and their corresponding gradients dominate memory consumption, making token pruning's impact on memory minimal. Instead, pruning primarily enhances training speed because the extensive parameter count leads to substantial attention computation per layer, making token reduction more impactful on speed. In contrast, the smaller Show-o model allocates a substantial portion of memory to activation variables during token computation. Consequently, token pruning in Show-o leads to a more significant reduction in memory usage. 





\input{tables/table_pixart}


\subsection{Adaptation to Diffusion Models} 
\label{adaption_to_diffusion}
\textbf{MoD for Generation Models.} While our method is primarily designed for unified transformers, we also validate its effectiveness for training or fine-tuning generation models such as DiT~\cite{dit} and PixArt~\cite{pixart}. For the DiT model, we select the checkpoint trained for 50k iterations as the base model to calculate the ARank. For the PixArt model, we use the final model as the base and fine-tune it using our internal high-quality data (used in the Show-o fine-tuning stage). We evaluate the DiT model using the ImageNet~\cite{imagenet} FID metric, and for PixArt, we use the GenEval benchmark~\cite{geneval} to assess generation quality. Both models process images at a resolution of 256×256 pixels. 

For PixArt, we conduct three experiments. First, we present the results using full computation. Second, we prune 40\% of the tokens in the interleaved layers. Finally, we calculate the ARank value for each layer and select the 14 layers with the lowest ARank values to prune 40\% of the tokens. As shown in Tab.~\ref{tab:table_pixart}, our ARank-based MoD method achieves better performance compared to standard pruning approaches at the same computational cost, with only a slight reduction in performance relative to full computation.

\input{tables/table_dit}


For DiT, we select the checkpoint trained for 50k iterations as the base model to calculate the ARank. Then we choose the least value  14 layers to prune tokens. In practice, we gradually scale the token capacity from 1 to 0.2 over the course of 500k training iterations. Finally, we evaluate the model using a capacity of 0.4, as shown in Tab.~\ref{tab:table_dit}.

From these experiments, we observe that the token sequences in both DiT and PixArt exhibit higher redundancy compared to those in Show-o. We attribute this difference to the design of their image tokenizers. The VAEs used in DiT and PixArt downsample images by a factor of 8, whereas Show-o's tokenizer downsamples by a factor of 16. Consequently, for images of the same resolution, DiT and PixArt require more tokens to represent the image than Show-o.




\subsection{Straight-Through Gumbel Softmax}
\label{gumbel_softmax_formula}

The Straight-Through Gumbel-Softmax method assigns binary weights to tokens, allowing discrete sampling while preserving differentiability through a straight-through estimator essential for gradient-based optimization. In the forward pass, tokens with the highest probability are selected for retention or pruning. During the backward pass, soft probabilities are used to compute gradients, allowing smooth updates.

The formula for the Gumbel Softmax is:
\begin{equation}
y_i = \frac{\exp\left( \dfrac{\log(\pi_i) + g_i}{\tau} \right)}{\sum\limits_{j=1}^{K} \exp\left( \dfrac{\log(\pi_j) + g_j}{\tau} \right)}.
\end{equation}
$\pi_i$ is the original probability of the $i$-th category, $g_i$ is noise sampled from the $\text{Gumbel}(0,1)$ distribution, $\tau$ is the temperature parameter, and $K$ is the number of categories.

In the straight-through version, we obtain a hard one-hot vector z during the forward pass by taking the index with the highest y:
\begin{equation}
z_i = \begin{cases}
1, & \text{if } i = \arg\max\limits_j \, y_j \\
0, & \text{otherwise}
\end{cases}
\end{equation}

In this work, we employ this method to assess the importance of tokens across different tasks. The auxiliary loss is defined as \( \mathcal{L}_{\text{aux}} = P \sum_{i}(r_i - P)^2 \), where \( r_i \) is the capacity of the \( i \)-th layer (\( i \leq L \)). This loss function is utilized to ensure that approximately half of the tokens are processed by each layer.