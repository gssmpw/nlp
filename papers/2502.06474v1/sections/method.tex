
% \begin{figure*}
%     \centering
%     \includegraphics[width=1\linewidth]{figures/pipeline2.pdf}
    

%     \caption{Pipeline of \method{}. The Layer Switch Module transforms dense transformer layers into three specialized types: T2I MoD layers for Text-to-Image (T2I) generation, MMU MoD layers for Multi-Modal Understanding (MMU), and Shared MoD layers for both tasks. For each task, task-aware routers with distinct capacities prune tokens of different modalities, thereby enhancing computational efficiency and maintaining performance across tasks.}


%     \label{fig:pipeline}
%     \vspace{-10pt}
% \end{figure*}


\section{Method}




\subsection{Task-Aware Mixture-of-Depths}

Based on our experimental results and analyses, we observe that token redundancy varies across tasks and layers. By analyzing the attention weights of unified transformers (see Sec.~\ref{sec:attention_map}), we find that the importance of image and text tokens varies across tasks. It suggests that token pruning should target tokens from all modalities. Our experiments with ARank values (Sec.\ref{sec:topk_selection}) across various layers and tasks show that redundancy levels depend on both the task and the specific layer. It suggests using the ARank metric to identify layers for pruning. Additionally, our task competition experiments (Sec.~\ref{sec:SH_gumbel}) reveal that token importance varies across tasks in terms of final loss optimization. It suggests tokens should be pruned separately for each task. Consequently, we introduce a task-aware token pruning method. The whole pipeline of \method{} is shown in Fig.~\ref{fig:pipeline}

% We choose the TopK selection method for token pruning over the Straight-Through Gumbel-Softmax approach because the latter still requires processing all tokens during training. Consequently, adopting the Straight-Through Gumbel-Softmax method does not lead to a substantial decrease in training costs.

% Based on our experimental results and analyses, we observe that token redundancy varies across tasks and layers. In response, we propose a task-aware token pruning method. Although the Straight-Through Gumbel-Softmax approach can assign different weights to each token, it still requires processing all tokens during training. Consequently, we still employ a TopK-based strategy for token pruning.

% \textbf{Task-Aware MoD Layer.} We transform the dense transformer blocks into specialized MoD blocks: the T2I MoD block for pruning tokens in the Text-to-Image (T2I) generation task, the MMU MoD block for pruning tokens in the Multi-Modal Understanding (MMU) task and the shared MoD block for both tasks as shown in Figure~\ref{fig:pipeline}. For each task, we design a dedicated router with a specific capacity, enabling the model to adaptively prune tokens based on task requirements. These routers aim to prune tokens from different modalities, ensuring efficient multi-modal processing and enhancing the model's overall computational efficiency.


\textbf{Task-Aware MoD Layer.} We transform dense transformer blocks into three specialized MoD blocks: the T2I MoD block for pruning tokens in the Text-to-Image (T2I) generation task, the MMU MoD block for pruning tokens in the Multi-Modal Understanding (MMU) task, and the Shared MoD block for pruning tokens in both tasks, as shown in Figure~\ref{fig:pipeline}. Specifically, the T2I MoD block only prunes T2I tokens while processing all MMU tokens, and the MMU MoD block only prunes MMU tokens while processing all T2I tokens. The Shared MoD block simultaneously prunes tokens from both tasks. For each task, we design dedicated routers with specific capacities, enabling the model to adaptively prune tokens based on task requirements. These routers target tokens from different modalities, ensuring efficient multi-modal processing and enhancing the model's overall computational efficiency.




\textbf{Layer Switch Module.} In this module, we utilize the ARank metric to identify which transformer layers should be transformed into MoD blocks. Specifically, during the training process, we calculate the ARank metric for each layer across different tasks using 50 samples per task. Based on the resulting ARank line chart, we select the top 12 layers with the highest ARank values for each respective task. The token pruning ratio is then determined proportionally based on the ARank values relative to the total sequence length. This approach allows us to dynamically prune tokens for each task, effectively managing token redundancy by leveraging the variance in ARank values and ensuring that pruning targets tokens from all modalities.



To accommodate different tasks, we design task-specific routers for each task (e.g., T2I and MMU) to perform different token pruning strategies. We also set task-specific thresholds. The formula is 

\begin{equation}
x_i^{*} =
\begin{cases}
x_i + D_t(x_i) R_t(x_i), & \text{if } R_t(x_i) \geq \delta_t \\
x_i, & \text{if } R_t(x_i) < \delta_t
\end{cases}
.
\end{equation}
\textbf{$x_i$} is the original token before pruning and \textbf{$x_i^{*}$} is the updated token after applying the pruning strategy. \textbf{$t$} represents the specific task (e.g., T2I, MMU). \textbf{$D_t(x_i)$} is the task-specific router function for task \( t \), determining how the token \( x_i \) should be adjusted. \textbf{$R_t(\cdot)$} is the corresponding routing function and \textbf{$R_t(x_i)$} is the task-specific weight for token \( x_i \) in task \( t \), indicating the token's relevance or significance. \textbf{$\delta_t$} is the task-specific threshold for task \( t \), deciding whether the token \( x_i \) should undergo pruning based on its importance score. This allows us to apply different token pruning criteria for each task, enabling dynamic adjustment of token pruning based on the specific requirements and ARank values of each task.

% \footnote{Although our experiments are based on two tasks, MMU and T2I, our method can also adapt to additional tasks.}



% \input{tables/table_comparsion_new_data}




















