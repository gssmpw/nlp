\section{Preliminaries}
\label{sec:preliminaries}

A graph-structured data can be represented as $G=\{V, E, \vect{X}\}$, where $V$ is the node set, $E$ is the edge set, and $\vect{X}\in \mathbb{R}^{|V|\times d}$ is the node feature matrix. The $i$-th row $\vect{x}_i\in \mathbb{R}^d$ of $\vect{X}$ denotes the features of node $i \in V$. For a labeled node $i$, let $\vect{Y}_i\in \mathbb{R}^C$ denote the one-hot label vector, where $\vect{Y}_{ic}=1$ if and only if node $i$ belongs to class $c$. 

{\bf Node Anomaly Detection (NAD).} NAD can be seen as a binary classification task, where nodes in the graph are categorized into two different categories: normal and anomalous. Specifically, $C=2$ with label $0$ representing normal class and label $1$ representing anomalous class. Typically, the number of normal nodes is way larger than that of anomalous nodes, leading to an imbalanced dataset. Due to the sparse nature, there exists a thorny problem in the real NAD applications, e.g., the number of labeled nodes is extremely limited. Consequently, effectively leveraging the limited labels in datasets becomes a key challenge in NAD. 

{\bf Graph Neural Network (GNN).} A GNN consists of a sequence of fundamental operations, such as message passing through linear transformations and pointwise non-linear functions, which are performed on a set of nodes embedded in a given space. GNNs have been widely applied to various tasks related to graph-structured data. While these operations are well-understood in Euclidean space, extending them to non-Euclidean spaces presents challenges. After the concept of GNNs being generalized to operate on spaces with different curvature $\kappa$, allowing the network to be agnostic to the underlying geometry of the space. The propagation process for a node $i$ is as follows:
\begin{equation*}
    \vect{H}_i^{l+1}=\sigma(\exp^\kappa_{\vect{x}'}(\frac{1}{|N(i)|}\sum_{j\in N(i)}g_\theta(\log^\kappa_{\vect{x}'}(\vect{H}_i^l), \log^\kappa_{\vect{x}'}(\vect{H}_j^l)))), 
\end{equation*}
where $\vect{H}_i^{l}$ is the $i$-th row of node representation matrix at the $l$-th layer, $\exp^\kappa_{\vect{x}'}(\cdot)$ and $\log^\kappa_{\vect{x}'}(\cdot)$ are projection functions specific to different spaces (Equations \ref{eqn:exp}-\ref{eqn:log} show a possible choice for these two functions), $g_\theta(\cdot)$ is the aggregation function, $N(i)$ is the nodes within the one-hop neighborhood of node $i$, and $\sigma(\cdot)$ is the activation function. For GNNs in Euclidean space, where $\kappa=0$, the projection functions act as identical mapping. 
In contrast, for GNNs operating in non-Euclidean space, where $\kappa \neq 0$ (as in \citep{hgcn19chami}), two common choices for projection functions are the Poincar√© Ball model and the Lorentz model, both characterized by $\kappa =-1$. Further details of these two models can be found in Appendix \ref{subsec:alternative}. 


{\bf $\boldsymbol{\kappa}$-stereographic model.} To further investigate the properties of NAD tasks across different spaces, the $\kappa$-stereographic model is introduced. This model can represent spaces with distinct curvature $\kappa$ that is not limited to $-1$. For a curvature $\kappa\in \mathbb{R}$ and a dimension $d\geq 2$, the model is defined as $M_\kappa^d=\{\vect{x}\in \mathbb{R}^d | -\kappa||\vect{x}||_2^2 < 1\}$. Note that when $\kappa\geq 0$, $M_\kappa^d$ is $\mathbb{R}^d$, while for $\kappa<0$, $M_\kappa^d$ represents the open ball of radius $\frac{1}{\sqrt{-\kappa}}$. Following the extension presented by \cite{kappa20bachmann}, the $\kappa$-addition for $\vect{x}, \vect{y} \in M_\kappa^d$ is defined as: 
\begin{equation*}
    \vect{x}\oplus_\kappa\vect{y}=\frac{(1-2\kappa\vect{x^T}\vect{y}-\kappa||\vect{y}||^2)\vect{x}+(1+\kappa||\vect{x}||^2)\vect{y}}{1-2\kappa\vect{x}^T\vect{y}+\kappa^2||\vect{x}||^2||\vect{y}||^2} \in M_\kappa^d. 
\end{equation*}
The projection functions $\exp_{\vect{x}'}^\kappa(\cdot)$ and $\log_{\vect{x}'}^\kappa(\cdot)$ can be defined as: 
\begin{equation}\label{eqn:exp}
    \exp_{\vect{x}'}^\kappa(\vect{x})=\vect{x}'\oplus_\kappa(\tan_\kappa(|\kappa|^\frac{1}{2}\frac{\lambda_{\vect{x}'}^\kappa||\vect{x}||}{2})\frac{\vect{x}}{||\vect{x}||}), 
\end{equation}
\begin{equation}\label{eqn:log}
    \log_{\vect{x}'}^\kappa(\vect{x})=\frac{2|\kappa|^{-\frac{1}{2}}}{\lambda_{\vect{x}'}^\kappa}\tan_\kappa^{-1}||(-\vect{x}')\oplus_\kappa\vect{x}||\frac{(-\vect{x}')\oplus_\kappa\vect{x}}{||(-\vect{x}')\oplus_\kappa\vect{x}||}, 
\end{equation}
where $\vect{x}'$ can be chosen as the origin of the specific space, $\lambda_{\vect{x}'}^\kappa = \frac{2}{1+\kappa||\vect{x}'||^2}$, and $\tan_\kappa$ is the curvature-dependent trigonometric function defined as follows: 
\begin{equation*}
\begin{aligned}
    \tan_\kappa(\vect{x})=
    \begin{cases}
    \frac{1}{\sqrt{-\kappa}}\tanh(\sqrt{-\kappa}\vect{x}), &\kappa < 0,\\
    \vect{x}, &\kappa=0,\\
    \frac{1}{\sqrt{\kappa}}\tan(\sqrt{\kappa}\vect{x}), &\kappa>0. 
    \end{cases}
\end{aligned}
\end{equation*}
With the detailed definition of the $\kappa$-stereographic model established, a straightforward approach to design a GNN is to replace the projection functions by selecting a specific $\kappa$. However, this common design may not be suitable for NAD, as will be demonstrated in Section \ref{subsec:LSP}, where the usefulness of applying learnable projection functions to NAD will be highlighted. 
