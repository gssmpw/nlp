\section{Related Work}
\label{sec:relatedwork}

{\bf Generalized GNNs.} GNNs have gained popularity for processing graph-structured data due to their outstanding ability to capture both structure and attribute information. For example, Euclidean GNNs, such as GCN \citep{gcn17kipf}, extend convolution operations to graphs by aggregating information from neighboring nodes. Following this, GraphSAGE \citep{graphsage17hamilton} introduces a sampling technique to enhance node representations. Subsequently, models like GAT \citep{gat18velickovic} and GIN \citep{gin19xu} are developed to increase the expressiveness of GNNs. In addition to Euclidean GNNs, non-Euclidean representations for nodes have drawn attention in the graph learning community due to their ability to encode special structures in graphs. For instance, HNN \citep{hnn18ganea} embeds node features into Hyperbolic space, which effectively captures hierarchical information. Based on this, HGCN \citep{hgcn19chami} extends GNN principles to Hyperbolic space, leveraging its capability to represent hierarchical relationships more effectively. Recently, HYLA \citep{hyla23yu} combines Hyperbolic space with Laplacian-based graph learning, further enhancing the ability to capture hierarchical features in graph data. Despite the successes of generalized GNNs in some graph-related tasks, they often struggle to effectively address NAD problems due to their lack of consideration for specific properties within NAD tasks. In contrast, our proposed SpaceGNN leverages the combination of multi-space to collect comprehensive information, specifically targeting to solve the issues of NAD that generalized GNNs find difficult to process. 

{\bf Specialized GNNs.} Recognizing the limitations of generalized GNNs in NAD tasks, researchers have proposed several approaches specifically designed for this purpose. For instance, GDN \citep{gdn23gao} is designed to resist high heterogeneity in anomalous nodes while benefiting the learning of normal nodes through homogeneity. Similarly, SparseGAD \citep{sparsegad23gong} introduces sparsity to the adjacency matrix to mitigate the negative influence of heterogeneity. GAGA \citep{gaga23wang} employs a group aggregation technique to address low homogeneity issues. Additionally, XGBGraph \citep{gadbench23tang} combines XGBoost with GIN to deal with tree-like data contained in NAD tasks. Beyond these spatial GNNs, several studies have explored NAD from a spectral view within Euclidean space. For example, AMNet \citep{amnet22chai} adaptively integrates signals of varying frequencies to extract more information. Following this, BWGNN \citep{bwgnn22tang} applies a Beta kernel to detect higher frequency anomalies. GHRN \citep{ghrn23gao} combines solutions from spectral space and homogeneity to boost performance in NAD tasks. Other than these studies, the most recent work, CONSISGAD \citep{consisgad24chen}, is proposed to tackle the limited supervision issue in NAD by generating pseudo labels. Despite the improvements in performance achieved by these specialized GNNs for NAD tasks, they lack a unified framework that integrates empirical and theoretical analysis related to NAD. This gap highlights the need for our proposed SpaceGNN, which aims to unify these valuable designs while providing a comprehensive understanding of their effectiveness in addressing NAD challenges.