\section{Our Method: SpaceGNN}
\label{sec:method}


\subsection{Overview of SpaceGNN}
\label{subsec:overview}

The proposed SpaceGNN consists of three key components%, as illustrated in Figure \ref{fig:ovewview}
, which will be discussed in detail in Sections \ref{subsec:LSP}-\ref{subsec:MSE}. In Section \ref{subsec:LSP}, we begin by defining the expansion rate of different spaces and then utilize this concept to empirically demonstrate the advantages of incorporating learnable curvature in NAD tasks, which serves as the foundation for designing the LSP module. Next, in Section \ref{subsec:DAP}, we revisit the concept of homogeneity and introduce weighted homogeneity to highlight the effectiveness of distance-aware attention across various spaces for NAD tasks. We also provide a theoretical analysis to justify the inclusion of distance-based propagation in diverse spaces, leading to the design of the DAP module. Finally, in Section \ref{subsec:MSE}, we first show the potential drawbacks of data augmentation techniques and then demonstrate the advantages of employing an ensemble of multiple spaces instead of relying on a single one. This motivates the design of the {\update MulSE} module, a robust solution for NAD tasks with limited supervision. 

%\input{figures/overview/overview}

\subsection{Learnable Space Projection}
\label{subsec:LSP}

Before delving into the design of LSP, we first formulate the distance between two vectors $\vect{x}, \vect{y}\in M_\kappa^d$ as follows: 
\begin{equation}\label{eqn:distancek}
    d_\kappa(\vect{x}, \vect{y})=2\tan_\kappa^{-1}||(-\vect{x})\oplus_\kappa\vect{y}||. 
\end{equation}
Notice that this distance is not applicable when $\kappa=0$. In such cases, we leverage the Euclidean distance, denoted as $d_0(\vect{x}, \vect{y})$. 

Recap from Section \ref{sec:preliminaries} that NAD tasks can be viewed as binary classification tasks. Consider three data points: $\vect{x}_0$ and $\vect{x}_1$ belonging to class 0, and $\vect{y}$ belonging to class 1. An optimal model trained on such a dataset should minimize the distances between data points in the same class while maximizing the distances between data points from different classes. The ideal scenario is $d_\kappa(\vect{x}_0, \vect{x}_1)\approx0$, $d_\kappa(\vect{x}_0, \vect{y})\approx\infty$, and $d_\kappa(\vect{x}_1, \vect{y})\approx\infty$. In such a case, even rule-based techniques can classify the data points correctly. To quantify the advantages of the projections in spaces with curvature $\kappa$, we propose a measure that reflects the scenario, which is stated in the following definition. %\ref{def:expension}. 
\begin{definition}[Expansion Rate]
\label{def:expension}
    For three data points, $\vect{x}_0$ and $\vect{x}_1$ in class 0 and $\vect{y}$ in class 1, let the inter-distance be $d_\kappa(\vect{x}_0, \vect{y})$ and intra-distance be $d_\kappa(\vect{x}_0, \vect{x}_1)$, then we can denote the ratio between them as $r_\kappa(\vect{x}_0, \vect{x}_1, \vect{y}) = \frac{d_\kappa(\vect{x}_0, \vect{y})}{d_\kappa(\vect{x}_0, \vect{x}_1)}$. Based on this, the Expansion Rate can be further defined as $ER_\kappa(\vect{x}_0, \vect{x}_1, \vect{y}) = \frac{r_\kappa(\vect{x}_0, \vect{x}_1, \vect{y})}{r_0(\vect{x}_0, \vect{x}_1, \vect{y})}$. 
\end{definition}

\input{figures/curvature/curvature}

As for the ratio $r_\kappa(\vect{x}_0, \vect{x}_1, \vect{y})$, an effective projection into space with curvature $\kappa$ should maximize the separation between data points from
different classes, making their distances sufficiently distinct for accurate detection. Since data points are originally embedded in Euclidean space, it is natural to utilize the ratio in Euclidean space as the base to investigate the changes that occur when data points are projected into different spaces. Hence, the Expansion Rate quantifies the extent to which this ratio is expanded by the projection. To be specific, if $ER_\kappa(\vect{x}_0, \vect{x}_1, \vect{y}) > 1$, it indicates that the projection into the space with curvature $\kappa$ will have positive effects on NAD tasks, otherwise, it suggests that staying within Euclidean space may be more beneficial to the task. 



As shown in Figure \ref{fig:curvature}, we investigate several node triplets within real datasets and plot the $ER_\kappa$ for representatives, varying based on $\kappa$. Specifically, for the blue line, the maximum value of $ER_\kappa$ is achieved when $\kappa$ is negative; for the orange line, the maximum value of $ER_\kappa$ is gained when $\kappa$ is positive; and for the green line, the maximum value of $ER_\kappa$ is obtained when $\kappa$ is 0. Based on these observations, it is desirable to design a GNN framework with learnable curvature, enabling the model to capture the optimal curvatures for nodes. To this end, we propose our base model architecture $f^L_{\vect{\kappa}}$ as follows: 
\begin{equation}\label{eqn:architecture}
\begin{aligned}
    \vect{{\update E}}^l&=\text{CLAMP}_{\kappa^{l}}\left( \text{TRANS}(\exp^{\kappa^l}_{\vect{o}}(\vect{H}^l))\right), \\
    \vect{H}_i^{l+1}&=\phi(\log^{\kappa^l}_{\vect{o}}(\vect{{\update E}}_i^l) + \sum_{j\in N(i)}\omega_{ij}^{\kappa^l}\log^{\kappa^l}_{\vect{o}}(\vect{{\update E}}_j^l)), \\
    \vect{Z}&=\sigma(\text{MLP}(\text{CONCAT}(\vect{H}^{0}, \vect{H}^{1}, ..., \vect{H}^{L}))), 
\end{aligned}
\end{equation}
where $\vect{H}^0=\vect{X}$, $\text{TRANS}(\cdot)$ is the transformation function of the feature matrix built on two-layer linear projection and non-linear activation, $\text{CLAMP}_{\kappa^l}(\cdot)$ is the clamp function to restrict the node representations to a valid space, $\phi(\cdot)$ and $\sigma(\cdot)$ are the activation functions, $\exp^{\kappa^l}_{\vect{o}}(\cdot)$ and $\log^{\kappa^l}_{\vect{o}}(\cdot)$ are projection functions (Equations \ref{eqn:exp}-\ref{eqn:log}) based on the original point $\vect{o}$ of the corresponding space, $\omega_{ij}^{\kappa^l}$ is the coefficient based on the distances between node $i$ and $j$, which will be detailed in Section \ref{subsec:DAP}, and $\vect{Z}\in \mathbb{R}^{n\times2}$ is the probability matrix under spaces with learnable curvatures $\vect{\kappa} \in \mathbb{R}^L$. 

Based on the empirical analysis in this section, the importance of distances within different spaces stands out, which motivates us to further explore the effectiveness of incorporating the properties of these distances in our GNN framework. In the next subsection, we will first introduce the concept of weighted homogeneity and then provide both empirical and theoretical analysis to substantiate the rationale for designing a distance-aware attention mechanism for information propagation. 

\subsection{Distance Aware Propagation}
\label{subsec:DAP}

To elucidate the intuition behind weighted homogeneity, we first define the homogeneity for a node $i$ as $\frac{|\{j:j\in N(i), \vect{Y}_i=\vect{Y}_j\}|}{|N(i)|}$, which reflects the ratio of neighbors in the same category of $i$. Similarly, the homogeneity for a graph $G$ is defined as $\frac{\sum_{(i,j)\in E}\mathbb{I}[\vect{Y}_i=\vect{Y}_j]}{|E|}$, where $\mathbb{I}[\cdot]$ is the indicator function. This definition indicates the ratio of intra-edges within this graph. Hence, if we consider the information of each node as 1, the homogeneity metric can be interpreted as a measure of the information a node can gain from its neighbors with the same label during propagation. However, as will be demonstrated later, homogeneity alone is not an effective measure for guiding the message passing. Therefore, we introduce weighted homogeneity to enhance the passed information along intra-edges: 
\begin{definition}[Weighted Homogeneity]
\label{def:homo}
    Let $\sigma$ denote the sigmoid function, $d_\kappa(\cdot,\cdot)$ denote the distance between two vectors in the space with curvature $\kappa$, then we can define the similarity vector for node $i$ as $\vect{s}_i^\kappa=\vect{1}-\sigma([d_\kappa(\vect{X}_i, \vect{X}_j): j\in N(i)])$. The weighted homogeneity of a node $i$ can be defined as $WH^\kappa_i=\frac{\sum_{j\in N(i)}\vect{s}_{ij}^\kappa\mathbb{I}[\vect{Y}_i=\vect{Y}_j]}{\sum_{j\in N(i)}\vect{s}_{ij}^\kappa}$, and that of a graph $G$ can be defined as $WH^\kappa=\frac{\sum_{i\in V}\sum_{j\in N(i)}\vect{s}_{ij}^\kappa\mathbb{I}[\vect{Y}_i=\vect{Y}_j]}{\sum_{i\in V}\sum_{j\in N(i)}\vect{s}_{ij}^\kappa}$, {\update where $\vect{s}_{ij}^\kappa$ is the $j$-th entry of $\vect{s}_{i}^\kappa$}. 
\end{definition}

\input{figures/homogeneity/homogeneity}

In the above definition, $\vect{s}_i^\kappa$ represents the similarities between a node $i$ and its neighbors based on the distances of their representation in a space with curvature $\kappa$. This similarity vector is used as the weights in weighted homogeneity, allowing us to measure the information that a node can derive from its neighbors with the same label. If $\vect{s}_i^\kappa$ accurately reflects the similarities between nodes, it will assign larger weights to intra-edges, leading to the increased value of $WH^\kappa$. Its desirable benefits as coefficients in the propagation process will be shown in Theorem \ref{thm:gaussian}, that is, a higher $WH^\kappa$ correlates with improved performance of GNN. Next, we present a comparison between homogeneity and weighted homogeneity $WH^\kappa$ across 9 real-world datasets within different spaces to show the effectiveness of weighted homogeneity. 

As shown in Figure \ref{fig:homo}, in most cases, $WH^\kappa$ with different $\kappa$ have higher values than homogeneity, which indicates that $WH^\kappa$ is beneficial to information from intra-edges. Besides, the results suggest that the optimal projection functions vary across different datasets, which further validates the motivation for integrating information from multiple spaces to enhance the performance of NAD. 

Building on the above empirical findings, we present Theorem \ref{thm:gaussian} to elucidate why $WH^\kappa$ serves as an effective measure for the propagation process. Detailed proof can be found in Appendix \ref{subsec:proof}. 
\begin{theorem}
\label{thm:gaussian}
    Assume features of normal and anomalous nodes follow independent Gaussian distributions $\mathcal{N}(\vect{\mu}_n, \vect{\Sigma}_n)$ and $\mathcal{N}(\vect{\mu}_a, \vect{\Sigma}_a)$, respectively, and let $WH_\kappa$ (resp.\ $1-WH_\kappa$) denote the coefficients of intra-edges (resp.\ inter-edges), then the probability of a node following its original distribution after a propagation process increases as $WH_\kappa$ increases. 
\end{theorem}

Theorem \ref{thm:gaussian} emphasizes the importance of utilizing weighted homogeneity for effective information propagation. In particular, it mitigates the impact of noisy information passed from inter-edges while augmenting valuable information passed from intra-edges. Based on both empirical and theoretical results, we propose the DAP as follows: 
\begin{equation}\label{eqn:weighthomo}
    \omega_{ij}^\kappa=\text{MLP}(\text{CONCAT}(\vect{X}_i, \vect{\hat{s}}_{ij}\vect{X}_j)), 
\end{equation}
where $\vect{\hat{s}}_{ij}^\kappa$ is the $j$-th entry of $\vect{\hat{s}}_{i}^\kappa$, which is an approximation of the similarity vector $\vect{s}_{i}^\kappa$ in Definition \ref{def:homo}. The rationale behind approximating $\vect{s}_{i}^\kappa$ in DAP is to prevent the occurrence of invalid values for $d_\kappa(\cdot, \cdot)$ during the learning process. The details of this approximation are shown in Theorem \ref{thm:distance}: 

\begin{theorem}
\label{thm:distance}
    Assume $\vect{x}$, $\vect{y}$ $\in \mathbb{R}^d$ such that $\vect{x}\neq \vect{y} (\vect{x}\neq \frac{-\vect{y}}{\kappa||\vect{y}||^2}$ if $\kappa > 0)$, and $|\kappa|<\frac{1}{\min(||\vect{x}||^2, ||\vect{y}||^2)}$, then $d_\kappa(\vect{x}, \vect{y})\approx 2||\vect{x}-\vect{y}||-2\kappa((\vect{x}^T\vect{y})||\vect{x}-\vect{y}||^2+\frac{||\vect{x}-\vect{y}||^3}{3})$. 
\end{theorem}

The proof of Theorem \ref{thm:distance} can be found in Appendix \ref{subsec:proof}. With this simple yet powerful design, our proposed SpaceGNN effectively harnesses information from intra-edges to boost the performance. 

So far, we have presented the basic design of our model architecture $f_{\vect{\kappa}}^L$, which effectively captures information from spaces characterized by curvatures $\vect{\kappa}\in \mathbb{R}^L$. We have improved the expressiveness of GNN and explored properties underlying different spaces, tackling most problems existing in previous works. However, another thorny issue still remains: how to extract sufficient information when only extremely limited number of labels are available. In Section \ref{subsec:MSE}, we will empirically and theoretically demonstrate the advantages of utilizing an ensemble of GNNs from multiple spaces over data augmentation techniques for collecting auxiliary information in NAD with limited supervision. 

\subsection{Multiple Space Ensemble}
\label{subsec:MSE}

\input{figures/far/far}

In this section, we first provide an empirical analysis of CONSISGAD \citep{consisgad24chen}, the most recent work for the supervised NAD task aimed at tackling the issue of limited labels. CONSISGAD leverages a learnable framework to generate pseudo labels, thereby increasing the number of labeled nodes for training. However, the quality of these pseudo labels may not always be sufficient to provide valuable information. Moreover, as shown in a previous study \citep{dataaug23wang}, the noise introduced by training samples with inaccurate pseudo labels will harm the final performance. Specifically, we investigate the well-trained CONSISGAD framework and present the false rate of pseudo labels it generates on 9 real-world datasets in Figure \ref{fig:far}. 
As we can observe, even with a fully-trained model, most generated anomalous labels are false across nearly all datasets. This situation negatively impacts the primary objective of NAD tasks, which is to detect anomalous nodes. Besides, due to the imbalanced nature of NAD tasks, the model tends to prioritize learning features of normal nodes. Consequently, with more normal labels and false anomalous labels, the model can finally degrade to a state of inferior performance. 

Other than pseudo-label techniques, previous studies \citep{poordataaug23kirichenko, badaug24lin} also point out the potential negative effects of other popular data augmentation techniques, such as increasing the bias of models and inducing the distribution shift between training and test data. Thus, we need to explore a more suitable way to tackle the issue of limited supervision existing in real NAD tasks. As shown in previous works \citep{modelaug22xia, modelaug24liu}, model augmentation can be an alternative way to enhance the limited information. The ensemble technique is one of the most important ways to conduct model augmentation, as it brings enough benefits when there exist several independent views of the data. Such a characteristic provides explanations for combining our multiple GNNs within different spaces for NAD problems. Specifically, we have the following {\update Proposition} \ref{thm:ensemble} to show the effectiveness of the ensemble of our multi-space framework on NAD tasks. The corresponding proof can be found in Appendix \ref{subsec:proof}. 

%\begin{theorem}
\begin{proposition}
    \label{thm:ensemble}
Consider there exists a single node with label vector $\vect{p}\in \mathbb{R}^C$, and the corresponding probability vector $\vect{q}_i$ generated by our base model $f_{\vect{\kappa}_i}^L$, where $i=1,\cdots,m$. Let $\vect{\bar{q}}=\sum_{i=1}^m\alpha_i\vect{q}_i$, where $\sum_{i=1}^m\alpha_i=1$, and $\mathcal{L}(\cdot, \cdot)$ denote cross-entropy loss, then the ensemble cross-entropy loss, $\mathcal{L}(\vect{p}, \vect{\bar{q}})$, is upper bounded by weighted cross-entropy loss of $m$ single models, $\sum_{i=1}^m\alpha_i\mathcal{L}(\vect{p}, \vect{q}_i)$, with a gap term related to a label-dependent value $\Omega(\vect{p})$. 
\end{proposition}

%\end{theorem}

{\update Proposition} \ref{thm:ensemble} shows that, for every node in the graph, the combination of independent models of different spaces can reduce the loss during training, which can lead to better performance, indicating its superiority over the single-model framework. Further, building on the above {\update Proposition} \ref{thm:ensemble}, we provide the expected loss over graph $G$ in {\update Proposition} \ref{thm:expectation}, whose proof can be found in Appendix \ref{subsec:proof}: 

%\begin{theorem}
\begin{proposition}
    \label{thm:expectation}
Let $\vect{\hat{q}}$ denote $\argmin_{\vect{y}\in\vect{Y}}\mathbb{E}_G[\mathcal{L}(\vect{y}, \vect{q})]$, the centroid of model distribution with respect to $G$, then the ensemble cross-entropy loss over entire graph $G$, $\mathbb{E}_G[\mathcal{L}(\vect{p}, \vect{\bar{q}})]$, is upper bounded by weighted cross-entropy loss of $m$ single models, $\sum_{i=1}^m\alpha_i\mathcal{L}(\vect{p}, \vect{\hat{q}}_i)$, with a gap term related to a label-dependent value $\Theta(\vect{p})$. 
\end{proposition}

%\end{theorem}

With the above empirical and theoretical analysis, we conclude that a safer and more effective way to tackle the limited supervision issues for supervised NAD tasks is the combination of views from multiple independent spaces, and thus we present our framework as follows: 
\begin{equation}\label{eqn:ensemble}
\begin{aligned}
    f=\alpha f_{\vect{0}}^L+\sum_{i=1}^H\beta_i f_{\vect{\kappa}^-_i}^{L}+\sum_{j=1}^S\gamma_j f_{\vect{\kappa}^+_j}^{L}
\end{aligned}
\end{equation}
where $f_{\vect{0}}^L$, $f_{\vect{\kappa}^-_i}^{L}$, and $f_{\vect{\kappa}^+_j}^{L}$ represent the Euclidean GNN, the $i$-th Hyperbolic GNN and the $j$-th Spherical GNN, respectively. 

With empirical and theoretical analysis in Sections \ref{subsec:LSP}, \ref{subsec:DAP}, and \ref{subsec:MSE}, we present our SpaceGNN with a solid foundation, targeting to solve thorny issues in supervised NAD tasks, such as the existence of complex architectures, the requirement of the high-level expressive ability of the framework, and limited supervision. In the following Section \ref{sec:experiments}, we will further provide the experimental results to show how effective our proposed framework is. 