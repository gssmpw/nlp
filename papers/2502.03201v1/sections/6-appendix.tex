\appendix
\newpage

\section*{Appendix}
\label{sec:appendix}

\section{Proofs}
\label{subsec:proof}
{\bf Proof of Theorem 1.}
Let $p$ denote $WH^\kappa$, then the information a normal node can gain within its neighborhood during a propagation process follows $\mathcal{N}(p\vect{\mu}_n+(1-p)\vect{\mu}_a, p^2\vect{\Sigma}_n+(1-p)^2\vect{\Sigma}_a)$ according to the linear properties of independent Gaussian variables. 

Let $\vect{X}$ and $\vect{Y}$ denote the distribution of the normal node and the information over $\mathbb{R}^d$, respectively. We then use Fréchet inception distance \citep{fid17heusel} to describe the distance between two distributions as follows: 
\begin{equation*}
\begin{aligned}
    F(\vect{X}, \vect{Y})^2&=(\inf_{\gamma\in \Gamma(\vect{X}, \vect{Y})}\int_{\mathbb{R}^d \times \mathbb{R}^d}||\vect{x}-\vect{y}||^2d\gamma(\vect{x}, \vect{y})), \\
    &=(\inf_{\gamma\in \Gamma(\vect{X}, \vect{Y})}\mathbb{E}_{(\vect{x}, \vect{y})\sim\gamma}[||\vect{x}-\vect{y}||^2]),
\end{aligned}
\end{equation*}
where $\Gamma(\vect{X}, \vect{Y})$ is the set of all measures on $\mathbb{R}^d\times\mathbb{R}^d$ with marginals $\vect{X}$ and $\vect{Y}$ on the first and second factors, separately. Hence, we have the following equation: 
\begin{equation*}
\begin{aligned}
    &\mathbb{E}_{(\vect{x}, \vect{y})\sim\gamma}[||\vect{x}-\vect{y}||^2]\\
    =&\mathbb{E}_{(\tilde{\vect{x}}, \tilde{\vect{y}})\sim\tilde{\gamma}}[||(\tilde{\vect{x}}+\vect{\mu_x})-(\tilde{\vect{y}}+\vect{\mu_y})||^2]\\
    =&\mathbb{E}_{(\tilde{\vect{x}}, \tilde{\vect{y}})\sim\tilde{\gamma}}[||\tilde{\vect{x}}-\tilde{\vect{y}}||^2+||\vect{\mu_x}-\vect{\mu_y}||^2+2\langle \tilde{\vect{x}}-\tilde{\vect{y}}, \vect{\mu_x}-\vect{\mu_y}\rangle]\\
    =&||\vect{\mu_x}-\vect{\mu_y}||^2+\mathbb{E}_{(\tilde{\vect{x}}, \tilde{\vect{y}})\sim\tilde{\gamma}}[||\tilde{\vect{x}}-\tilde{\vect{y}}||^2]
\end{aligned}
\end{equation*}
where $\vect{\mu_x}$ and $\vect{\mu_y}$ represent the mean value of distributions $\vect{X}$ and $\vect{Y}$, and $\tilde{\vect{x}}$ and $\tilde{\vect{y}}$ represent vectors following distribution $\tilde{\vect{X}}$ and $\tilde{\vect{Y}}$, which have 0 mean value and the same variance value as $\vect{X}$ and $\vect{Y}$, respectively. Hence, the Fréchet inception distance can be decomposed as:
\begin{equation*}
F(\vect{X}, \vect{Y})^2=||\vect{\mu_x}-\vect{\mu_y}||^2+F(\tilde{\vect{X}}, \tilde{\vect{Y}})^2
\end{equation*}
This result shows the distance between the distribution of the normal node and the information is determined by two parts, the mean value and the variance value. Specifically, we can assume $\vect{\Sigma}_n\approx\vect{\Sigma}_a\approx c\vect{I}$ in real NAD tasks, where $c$ is a small constant, due to the independent similar behaviors of nodes in the same category. Thus, we have $F(\tilde{\vect{X}}, \tilde{\vect{Y}})^2\approx0$ and $F(\vect{X}, \vect{Y})^2=||\vect{\mu_x}-\vect{\mu_y}||^2$. 

Then, we check the distance between mean values of $\vect{X}$ and $\vect{Y}$. Specifically, it can be written as: 
\begin{equation*}
||\vect{\mu_x}-\vect{\mu_y}||^2=(1-p)^2||\vect{\mu}_n-\vect{\mu}_a||^2
\end{equation*}
which concludes that if $||\vect{\mu}_n-\vect{\mu}_a||^2$ remains the same, as $p$ increases, the distance between the distribution of the normal node and the information will decrease, and thus the probability of a normal node following its original distribution after a propagation process increases as $WH_\kappa$ increases. 

The situation of an anomalous node can be analyzed accordingly. This solution concludes that weighted homogeneity can benefit the propagation procedure for NAD tasks. {\hfill \qedsymbol}

{\bf Proof of Theorem 2.} First, we apply Taylor expansion on $\tan_\kappa^{-1}(t)$ for a fixed $t$ when $\kappa\rightarrow 0^+$: 
\begin{equation*}
\begin{aligned}
    \tan_\kappa^{-1}(t)=&\kappa^{-\frac{1}{2}}\tan(\kappa^\frac{1}{2}t)\\
    =&\kappa^{-\frac{1}{2}}(\kappa^\frac{1}{2}t+\kappa^\frac{3}{2}\frac{t^3}{3}+\mathcal{O}(\kappa^\frac{5}{2}))\\
    =&t+\kappa\frac{t^3}{3} + \mathcal{O}(\kappa^2)
\end{aligned}
\end{equation*}
When $\kappa\rightarrow 0^-$:
\begin{equation*}
\begin{aligned}
    \tan_\kappa^{-1}(t)=&(-\kappa)^{-\frac{1}{2}}\tanh((-\kappa)^\frac{1}{2}t)\\
    =&(-\kappa)^{-\frac{1}{2}}((-\kappa)^\frac{1}{2}t-(-\kappa)^\frac{3}{2}\frac{t^3}{3}+\mathcal{O}(\kappa^\frac{5}{2}))\\
    =&t+\kappa\frac{t^3}{3}+\mathcal{O}(\kappa^2)
\end{aligned}
\end{equation*}
When $\kappa\rightarrow 0$, we also have $\tan_\kappa^{-1}(t)=t-\kappa\frac{t^3}{3}+\mathcal{O}(\kappa^2)$. Hence, we conclude that near 0, $\tan_\kappa^{-1}(t)=t-\kappa\frac{t^3}{3}+\mathcal{O}(\kappa^2)$. 

Then, we need to use the Tayler expansion for $||\cdot||$. Specifically, $||\vect{x}+\vect{o}||=||\vect{x}||+\langle \vect{x}, \vect{o}\rangle+\mathcal{O}(||\vect{o}||^2)$ when $\vect{o}\rightarrow \vect{0}$. 

After that, we derive the Tayler expansion for $\vect{x}\oplus_\kappa\vect{y}$ when $\kappa$ near 0: 
\begin{equation*}
\begin{aligned}
\vect{x}\oplus\vect{y}=&\frac{(1-2\kappa\vect{x^T}\vect{y}-\kappa||\vect{y}||^2)\vect{x}+(1+\kappa||\vect{x}||^2)\vect{y}}{1-2\kappa\vect{x}^T\vect{y}+\kappa^2||\vect{x}||^2||\vect{y}||^2}\\
=&((1-2\kappa\vect{x^T}\vect{y}-\kappa||\vect{y}||^2)\vect{x}+(1+\kappa||\vect{x}||^2)\vect{y})(1+2\kappa\vect{x^T}\vect{y}+\mathcal{O}(\kappa^2))\\
=&(1-2\kappa\vect{x^T}\vect{y}-\kappa||\vect{y}||^2)\vect{x}+(1+\kappa||\vect{x}||^2)\vect{y}+2\kappa\vect{x^T}\vect{y}(\vect{x}+\vect{y})+\mathcal{O}(\kappa^2)\\
=&(1-\kappa||\vect{y}||^2)\vect{x}+(1+\kappa||\vect{x}||^2)\vect{y}+2\kappa(\vect{x^T}\vect{y})\vect{y}+\mathcal{O}(\kappa^2)\\
=&\vect{x}+\vect{y}+\kappa(||\vect{x}||^2\vect{y}-||\vect{y}||^2\vect{x}+2(\vect{x^T}\vect{y})\vect{y})+\mathcal{O}(\kappa^2)
\end{aligned}
\end{equation*}

By combining the above three Tayler expansions and ignore $\mathcal{O}(\kappa^2)$, we have the following equation: 
\begin{equation*}
\begin{aligned}
d_\kappa(\vect{x}, \vect{y})=&2\tan_\kappa^{-1}(||(-\vect{x})\oplus_\kappa\vect{y}||)\\
=&2(||\vect{x}-\vect{y}||+\kappa((-\vect{x})^T\vect{y})||\vect{x}-\vect{y}||^2)(1-\frac{\kappa}{3}(||\vect{x}-\vect{y}||^2))\\
=&2||\vect{x}-\vect{y}||-2\kappa((\vect{x^T}\vect{y})||\vect{x}-\vect{y}||^2+\frac{||\vect{x}-\vect{y}||^3}{3})
\end{aligned}
\end{equation*}
which concludes our theorem. {\hfill \qedsymbol}

{\bf Proof of {\update Proposition} \ref{thm:ensemble}.} We use the weighted cross-entropy loss of $m$ single models to subtract the ensemble cross-entropy loss: 
\begin{equation*}
\begin{aligned}
\sum_{i=1}^m\alpha_i\mathcal{L}(\vect{p}, \vect{q}_i)-\mathcal{L}(\vect{p}, \bar{\vect{q}})=&\sum_{c=1}^C\vect{p}^c\log\bar{\vect{q}}^c-\sum_{i=1}^m\sum_{c=1}^C\alpha_i\vect{p}^c\log\vect{q}^c_i\\
=&\sum_{c=1}^C\vect{p}^c\log\bar{\vect{q}}^c-\sum_{c=1}^C\vect{p}^c\log(\prod_i(\vect{q}_i^c)^{\alpha_i})\\
=&\sum_{c=1}^C\vect{p}^c\log(\frac{\bar{\vect{q}}^c}{\prod_i(\vect{q}_i^c)^{\alpha_i}})\\
=&\Omega(\vect{p})
\end{aligned}
\end{equation*}

By applying the weighted AM–GM inequality, we have $\bar{\vect{q}}\geq\prod_i(\vect{q}_i^c)^{\alpha_i}$, which means the term inside the log function is greater or equal to 1, and so the $\Omega(\vect{p})$ is non-negative. Thus, it finishes the proof of the {\update Proposition}. {\hfill \qedsymbol}

{\bf Proof of {\update Proposition} \ref{thm:expectation}.} We take the expectation of the equation in {\update Proposition} \ref{thm:ensemble} over entire graph $G$ and apply KL bias-variance decomposition in previous study \citep{enstheo23wood}, then we have: 
\begin{equation*}
\begin{aligned}
\mathbb{E}_G[\mathcal{L}(\vect{p}, \bar{\vect{q}})]=&\mathbb{E}_G[\sum_{i=1}^m\alpha_i\mathcal{L}(\vect{p}, \vect{q}_i)]-\mathbb{E}_G[\Omega(\vect{p})]\\
=&\sum_{i=1}^m\alpha_i\mathcal{L}(\vect{p}, \vect{\hat{q}}_i)+\sum_{i=1}^m\alpha_i\mathbb{E}_G[\KL(\vect{\hat{q}}_i||\vect{q}_i)]-\mathbb{E}_G[\Omega(\vect{p})]\\
=&\sum_{i=1}^m\alpha_i\mathcal{L}(\vect{p}, \vect{\hat{q}}_i)+\Theta(\vect{p}), 
\end{aligned}
\end{equation*}
where $\Theta(\vect{p})$ is demonstrated as non-negative in previous study \citep{enstheo23wood}, and thus this {\update Proposition} is proven. {\hfill \qedsymbol}

\section{Datasets and Baselines}
\label{subsec:dataset-baseline}
\textbf{Datasets.} The datasets used in our experiments are from the most recent benchmark paper \citep{gadbench23tang}, according to which, Weibo, Reddit, Questions, and T-Social aim to detect anomalous accounts on social media, Tolokers, Amazon, and YelpChi are proposed for malicious comments detection in review platforms, and T-Finance and DGraph-Fin focus on fraud detection in financial networks. The statistics of these 9 real-world datasets are shown in Table \ref{tab:datasets}. 

\input{tables/datasets}

\textbf{Baselines.}  The first group is generalized models:
\begin{itemize}[topsep=0.5mm, partopsep=0pt, itemsep=0pt, leftmargin=10pt]
    \item MLP \citep{mlp58f}: A type of neural network with multiple layers of fully connected artificial neurons;
    \item GCN \citep{gcn17kipf}: A type of GNN that leverages convolution function on a graph to propagate information within the neighborhood of each node;
    \item GraphSAGE \citep{graphsage17hamilton}: A type of GNN that uses sampling technique to aggregate features from the neighborhood;
    \item GAT \citep{gat18velickovic}: A type of GNN that adopts an attention mechanism to assign different importance to different nodes within the neighborhood of each node;
    \item GIN \citep{gin19xu}: A type of GNN that captures the properties of a graph while following graph isomorphism;
    \item HNN \citep{hnn18ganea}: A type of neural network that projects data features into non-Euclidean space;
    \item HGCN \citep{hgcn19chami}: A type of GNN that embeds node representations into non-Euclidean space and propagates accordingly;
    \item HYLA \citep{hyla23yu}: A type of GNN combines both laplacian characteristics within a graph and the information from non-Euclidean space.
\end{itemize}
The second group is specialized models:
\begin{itemize}[topsep=0.5mm, partopsep=0pt, itemsep=0pt, leftmargin=10pt]
    \item AMMNet \citep{amnet22chai}: A method proposed to capture both low- and high-frequency spectral information to detect anomalies;
    \item BWGNN \citep{bwgnn22tang}: A method designed to handle the 'right-shift' phenomenon of graph anomalies in spectral space;
    \item GDN \citep{gdn23gao}: A method that aims to learn information from a graph of the dependence relationships between sensors;
    \item SparseGAD \citep{sparsegad23gong}: A method that leverages sparsification to mitigate the heterophily issues within the neighborhood of each node;
    \item GHRN \citep{ghrn23gao}: A method that tackles the heterophily problem in the spectral space of graph anomaly detection;
    \item GAGA \citep{gaga23wang}: A method that uses group aggregation to reduce the influence of low homophily;
    \item XGBGraph \citep{gadbench23tang}: A method that combines XGB and GIN to boost the expressiveness;
    \item CONSISGAD \citep{consisgad24chen}: A method that applies a pseudo-label generation technique to solve the limited supervision problem;
\end{itemize}

\section{Algorithm}
\label{subsec:algorithm}
\input{algorithm/explog}
\input{algorithm/clamp}
\input{algorithm/base}
\input{algorithm/spacegnn}
We provide the detailed algorithm in this Section. In Algorithm \ref{alg:exp-log}, we calculate $exp_{\vect{o}}^\kappa(\cdot)$ and $log_{\vect{o}}^\kappa(\cdot)$ based on the original point $\vect{o}$ of the space with curvature $\kappa$. Besides, to satisfy the range of $log_{\vect{o}}^\kappa(\cdot)$, we utilized Algorithm \ref{alg:clamp} to prune the node representations. Moreover, we construct Algorithm \ref{alg:base} by utilizing Algorithms \ref{alg:exp-log} and \ref{alg:clamp}. Specifically, we use the approximated distance to calculate the similarities between nodes and their neighbors, and then leverage them as the corresponding coefficients during the propagation process. Notice, for each layer $l$ during the propagation, we assign a different learnable $\kappa^l$ to capture comprehensive information from different spaces. To simplify our architecture, we only use three base models, $f_{\vect{\kappa}^+}^L$, $f_{\vect{\kappa}^-}^L$, and $f_{\vect{0}}^L$, for constructing SpaceGNN. This simplification can reduce the running time cost, and allow us to investigate the effectiveness of different spaces on different datasets easily through the corresponding hyperparameters. After obtaining probability matrix $\vect{Z}$ from Algorithm \ref{alg:spacegnn}, we use the cross-entropy loss to update the framework. 

\section{Experimental Settings}
\label{subsec:setting}
\input{tables/setting}
\input{figures/hyperparameter1/hyperparameter1}
\input{figures/hyperparameter2/hyperparameter2}
Table \ref{tab:setting} provides a comprehensive list of our hyperparameters. We use grid search to train the model that yields the best F1 score on the validation set and report the corresponding test performance. Specifically, Learning Rate is searched from the set $\{0.001, 0.0001\}$, Hidden Dimension is chosen from the set $\{32, 64, 128, 256\}$, Layer ranges from $1$ to $6$, Dropout is obtained from the set $\{0, 0.05, 0.1\}$, Batch Size is fixed based on the size of the training set, and $\alpha$ and $\beta$ are from the set $\{0, 0.5, 1\}$, respectively. In the following Section \ref{subsec:parameter}, we will further analyze the influence of Hidden Dimension, Layer, and $\alpha$ and $\beta$ on the F1 scores of different datasets. 

\section{Parameter Analysis}
\label{subsec:parameter}


In this Section, we investigate the impact of Hidden Dimension, Layer, and $\alpha$ and $\beta$ on three different datasets, and present their F1 scores. 

Figure \ref{fig:hyperparameter1} reports the F1 score of SpaceGNN as we vary the Hidden Dimension from $32$ to $256$, and the Layer from $1$ to $6$. As we can observe, when we set the Hidden Dimension to $128$, SpaceGNN achieves relatively satisfactory performances on these three datasets. When we vary the Layer, we find for different datasets, the optimal value can be different. Specifically, we set it to $3$ for Amazon and T-Finance, and $6$ for T-Social to get the best performance. 

Figure \ref{fig:hyperparameter2} reports the F1 score of SpaceGNN as we vary $\alpha$ and $\beta$ from $0$ to $1$. These two hyperparameters represent the influence of diverse spaces on the datasets, so for different datasets, the optimal value will be distinct. Specifically, we set $\alpha$ to $0$ for Amazon, $1$ for T-Finance, and $0.5$ for T-Social, and we set $\beta$ to $0$ for Amazon, $1$ for T-Finance and T-Social to get the satisfactory performance. 

\section{Ablation Study}
\label{subsec:ablation}
\input{tables/abaltion}
To investigate the usefulness of the LSP and DAP components, we provide the ablation study of them on 9 datasets in Table \ref{tab:ablation}. Specifically, we set the $\kappa$ as fixed values for different spaces during the w/o LSP experiment and set the $\hat{\vect{s}}_i$ as $\vect{1}$ for each node $i$ during the w/o DAP experiment. As shown in Table \ref{tab:ablation}, SpaceGNN consistently outperforms w/o LSP and w/o DAP by a large margin, which demonstrates the benefits of these two components. 

\section{Additional Experimental Results}
\label{subsec:Additional}
\input{tables/general10}
\input{tables/gad10}
In addition to the experiments in Section \ref{sec:experiments}, we further compare our SpaceGNN with baseline models on datasets with different sizes of training/validation/testing sets. Specifically, in experiments of Tables \ref{tab:general10} and \ref{tab:gad10}, we randomly divide each dataset into 10/10 for training/validation, and the rest of the nodes for testing, and in experiments of Tables \ref{tab:general100} and \ref{tab:gad100}, we randomly divide each dataset into 100/100 for training/validation, and the rest of the nodes for testing. 

\input{tables/general100}
\input{tables/gad100}

In Tables \ref{tab:general10} and \ref{tab:gad10}, we can observe that our SpaceGNN can consistently surpass both generalized and specialized models on 9 datasets. In short, SpaceGNN outperforms the best rival 10.84\% and 5.46\% on average in terms of AUC and F1 scores, respectively. 

Similarly, in Tables \ref{tab:general100} and \ref{tab:gad100}, it is easy to find out that SpaceGNN is able to beat both generalized and specialized models on 9 datasets. In summary, compared with the best rival, SpaceGNN takes a lead by 4.98\% and 3.02\% on average in terms of AUC and F1 scores, separately. 

\section{Alternative Model}
\label{subsec:alternative}
\input{tables/alternative}
The most common non-Euclidean GNN is based on either the Poincaré Ball model \citep{hgnn19liu} or the Lorentz model \citep{lorentz18nickle}. We discover that the Poincaré Ball model can be a special form of $\kappa$-stereographic model when setting the $\kappa$ to $-1$, which inspires us to investigate the general form of the Lorentz model. Following the definition of the $\kappa$-stereographic model, we generalize the Lorentz model as the $\kappa$-Lorentz model. Notice that we only provide a similar form to the $\kappa$-stereographic model, serving as the projection functions without considering the physical meaning. The $exp_{\vect{o}}^\kappa(\cdot)$ and $log_{\vect{o}}^\kappa(\cdot)$ for $\vect{x}\in \mathbb{R}^d$ are defined as follows: 
\begin{equation*}
exp_{\vect{x}'}^\kappa(\vect{x})=cos_\kappa(||\vect{x}||_L)\vect{x}'+sin_\kappa(||\vect{x}||_L)\frac{\vect{x}}{||\vect{x}||_L}
\end{equation*}
\begin{equation*}
log_{\vect{x}'}^\kappa(\vect{x})=d_\kappa(\vect{x}, \vect{x}')\frac{\vect{x}+\frac{1}{\kappa}\langle\vect{x}, \vect{x}'\rangle_L\vect{x}'}{||\vect{x}+\frac{1}{\kappa}\langle\vect{x}, \vect{x}'\rangle_L\vect{x}'||_L}
\end{equation*}
where $\langle\vect{x}, \vect{x}'\rangle_L=-x_0x_0'+x_1x_1'+...+x_dx_d'$, $||\vect{x}||_L=\sqrt{\langle\vect{x}, \vect{x}'\rangle_L}$, $d_\kappa(\vect{x}, \vect{x}')=cos_\kappa^{-1}(-\langle\vect{x}, \vect{x}'\rangle_L)$, and $cos_\kappa$ and $sin_\kappa$ are defined as: 
\begin{equation*}
\begin{aligned}
    \cos_\kappa(\vect{x})=
    \begin{cases}
    \frac{1}{\sqrt{-\kappa}}\cosh(\sqrt{-\kappa}\vect{x}), &\kappa < 0,\\
    \vect{x}, &\kappa=0,\\
    \frac{1}{\sqrt{\kappa}}\cos(\sqrt{\kappa}\vect{x}), &\kappa>0. 
    \end{cases}
\end{aligned}
\end{equation*}
\begin{equation*}
\begin{aligned}
    \sin_\kappa(\vect{x})=
    \begin{cases}
    \frac{1}{\sqrt{-\kappa}}\sinh(\sqrt{-\kappa}\vect{x}), &\kappa < 0,\\
    \vect{x}, &\kappa=0,\\
    \frac{1}{\sqrt{\kappa}}\sin(\sqrt{\kappa}\vect{x}), &\kappa>0. 
    \end{cases}
\end{aligned}
\end{equation*}
We replace the corresponding functions in our SpaceGNN framework to get SpaceGNN-L. As shown in Table \ref{tab:alternative}, SpaceGNN and SpaceGNN-L can have similar performance in terms of all the 9 datasets, which shows SpaceGNN-L can also outperform other baselines. These results demonstrate that our framework can be generalized to other base models. 

\section{Learned $\kappa$}
\label{subsec:learnedk}
\input{tables/kappa}

{\update In this section, we report the learned $\vect{\kappa}$ of the experiments in Tables \ref{tab:general50} and \ref{tab:gad50}. Notice that, for simplicity, we only include 1 Euclidean GNN, 1 Hyperbolic GNN, and 1 Spherical GNN in our framework. Recap from Section \ref{sec:method}, in our final architecture, we utilize a hyperparameter $L$ to control the number of layers of all three GNNs, and the number of entries in $\vect{\kappa}$ for each GNN is the same as the number of layers of it. Specifically, if $L$ is set to be 6, then there will be 6 entries in $\vect{\kappa}^0$ for Euclidean GNN, 6 entries in $\vect{\kappa}^-$ for Hyperbolic GNN, and 6 entries in $\vect{\kappa}^+$ for Spherical GNN. For $\vect{\kappa}^0$, we want the GNN to stay in the Euclidean space, so we set each entry in it to 0. For $\vect{\kappa}^-$ and $\vect{\kappa}^+$, we want the Hyperbolic GNN and Spherical GNN to search for the optimal curvatures for different datasets, so we set these two as learnable curvatures. Notice that, according to Table \ref{tab:setting}, the optimal $L$ varies by datasets, so the number of entries in learned $\vect{\kappa}^-$ and $\vect{\kappa}^+$ will also be different, as shown in Table \ref{tab:kappa}, where $"-"$ represents no such entry in the vector. 

As we can see from Table \ref{tab:kappa}, the learned values stay close to 0 after the learning process, which is aligned with the analysis of Section \ref{subsec:LSP}. As shown in Figure \ref{fig:curvature}, the largest $ER_\kappa$ will be obtained around 0, which further demonstrates our findings are effective for graph anomaly detection tasks. 
}


\section{Time Complexity Analysis}
\label{subsec:time}
{\update As shown in Section \ref{subsec:MSE}, our framework is composed of 1 Euclidean GNN, $H$ Hyperbolic GNN, and $S$ Spherical GNN. The differences between these GNNs are the projection functions and the Distance Aware
Propagation (DAP) component, but the time complexity of them is the same for different GNNs. Hence, We only need to analyze one of the GNNs. 

Our analysis of the GNN time complexity is primarily based on the Algorithm \ref{alg:base} in Appendix \ref{subsec:algorithm}, which illustrates the base architecture of each GNN. For simplicity, we focus on a single layer in the base architecture (Lines 3-7). 

First, in Line 3, we apply a two-layer MLP with time complexity of $O(|V|dd_1+|V|d_1d_2)$ followed by a projection function with time complexity of $O(|V|d_2)$, where $|V|$ is the total number of nodes in the graph, $d$ is the dimension of the node feature, and $d_1, d_2$ are the output dimension of the two MLPs, respectively. Thus, the total time complexity of Line 3 is $O(|V|dd_1+|V|d_1d_2)$. 

Then, in Line 5, we have to calculate the $\vect{\hat{s}}_{i}^{\kappa^l}$ for each node $i$. Specifically, for each edge connected to node $i$, we have a time complexity of $O(d_2)$ to get the corresponding coefficient. Thus, the total time complexity for all nodes in Line 5 would be $O(|E|d_2)$, where $|E|$ is the total number of edges in this graph. 

Afterward, in Line 6, for each edge between nodes $i$ and $j$, we need to calculate the $\omega_{ij}^{\kappa^l}$ with time complexity of $O(d_2^2)$, where the input dimension of the MLP is $2d_2$ and the output dimension of it is $d_2$, so the total complexity for all edges in Line 6 would be $O(|E|d_2^2)$.

Next, in Line 7, we also have to propagate the node embeddings for each edge in the graph, so the total time complexity of Line 7 is $O(|E|d_2)$. 

Finally, we combine the results before to get the time complexity of a single layer in the base architecture, i.e., $O(|V|dd_1+|V|d_1d_2+|E|d_2^2)$. 

According to the time analysis of GAT \citep{gat18velickovic}, one of the most popular architectures in the area of graph learning, the time complexity of a single GAT attention head computing $F_0$ features can be expressed as O($|V|FF_0 + |E|F_0$), where $F$ is the number of input features, and $|V|$ and $|E|$ are the numbers of nodes and edges in the graph, respectively. 

Hence, each layer of our proposed GNN has a similar time complexity to GAT by choosing the proper hyperparameters $d_1$ and $d_2$ in our architecture. In the experiments, we find that combining 1 Euclidean GNN, 1 Hyperbolic GNN, and 1 Spherical GNN in our framework is enough to achieve superior performance over all the other baselines, so the increase of time complexity by the Multiple Space Ensemble component will not be the limitation of our models in real applications. 
}

\section{Performance with More Training Data}
\label{subsec:moredata}

{ 
\input{figures/trainsz/reddit}
\input{figures/trainsz/questions}
\input{figures/trainsz/dgraphfin}

\update To further demonstrate the superior ability of our proposed framework, we provide the performance on Reddit, Questions, and DGraph-Fin varying by the size of the training set, as shown in Figures \ref{fig:redditsz}, \ref{fig:questionssz}, and \ref{fig:dgraphfinsz}. Note that, since HYLA and GAGA can not successfully run on DGraph-Fin, we only report the performance of the other 14 baselines and our proposed SpaceGNN in Figure \ref{fig:dgraphfinsz}. 

As we can see, the red lines, which represent the performance of our SpaceGNN, are always on the top of the figures, which demonstrates that with more training data, our SpaceGNN can still outperform all the other baselines consistently in terms of both AUC and F1. In summary, such experiments further make our SpaceGNN a more general and practical algorithm.
}

\section{Performance on GADBench \citep{gadbench23tang} semi-supervised setting}
\label{subsec:gadbench}
{\update 

\input{tables/gadbench1}
\input{tables/gadbench2}

For a fair comparison, we also provide AUC, AUPRC, and Rec@K scores on 9 datasets with data split of the semi-supervised setting in GADBench \citep{gadbench23tang}. Specifically, in this setting, we use 20 positive labels (anomalous nodes) and 80 negative labels (normal nodes) for both the training set and the validation set in each dataset, separately. Note that, for the baselines in GADBench, we use the reported performance in it, and for baselines not in GADBench, we obtain the source code of all competitors from GitHub and execute these models using the default parameter settings suggested by their authors. The hyperparameters of SpaceGNN are set based on the same setting in GADBench, i.e., random search.

As we can see from Tables \ref{tab:gadbench1} and \ref{tab:gadbench2}, our proposed model can still outperform all the baselines on almost all the datasets consistently using AUC, AUPRC, and Rec@K scores as metrics, which demonstrates the effectiveness of our SpaceGNN. 
}

