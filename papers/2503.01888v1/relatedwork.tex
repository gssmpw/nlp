\section{Related Work}
\subsection{Graph Neural Networks (GNNs)}


The review proposes a new taxonomy that categorizes existing GNNs into four types: recurrent GNNs, convolutional GNNs, graph autoencoders, and spatiotemporal GNNs\cite{Comprehensive}.It systematically discusses various variants of GNNs, such as Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and Graph Recurrent Networks (GRN), and proposes a general framework for designing GNN models\cite{Graph}.The over-smoothing problem in Graph Neural Networks is systematically analyzed, and a new deep Graph Neural Network model (DAGNN) is proposed, effectively addressing the performance degradation issue in deep network training\cite{Towards}.The GraphPrompt framework integrates pretraining and downstream tasks into a unified task template and employs learnable prompts to bridge the gap between pretraining and downstream task objectives\cite{GraphPrompt}.The process of decoupled Graph Neural Networks (decoupled GNNs) is improved by introducing a scalable attention mechanism to effectively utilize multi-hop information. By regarding label propagation as a special case of decoupled GNNs, a decoupled label technique (DecLT) is proposed to integrate label information\cite{Scalable}. 

\subsection{Graph Knowledge Distillation}
Recently, knowledge distillation has been proven to be effective in the field of graph learning.A new knowledge distillation framework named MuGSI is proposed for graph classification. It addresses two main challenges in the application of existing knowledge distillation frameworks to graph classification, namely the inherent sparsity of learning signals and the expressive limitations of student MLPs, through multi-granular structural information \cite{MuGSI}. A full-frequency band GNN-to-MLP distillation framework is proposed. It extracts low-frequency and high-frequency knowledge from GNNs and injects it into MLPs, solving the problem of information loss in traditional GNN-to-MLP distillation \cite{GNN-to-MLP}. By measuring the invariance of the information entropy of GNNs to noise perturbations, the reliability of knowledge points is quantified, and a method of Knowledge Heuristic Reliable Distillation (KRD) is proposed. This method can identify and utilize reliable knowledge points for the training of student MLPs \cite{Quantifying}. A multi-task self-distillation framework is proposed. By introducing self-supervised learning and self-distillation techniques, it addresses the mismatch problem of graph convolutional networks in graph-based semi-supervised learning from the graph structure side and the label side respectively \cite{Multi-task}. A novel adaptive knowledge distillation framework BGNN is proposed. By sequentially transferring knowledge from multiple GNNs to a student GNN, and using an adaptive temperature module and a weight enhancement module to guide the student to learn effectively, it achieves improvements in both node classification and graph classification tasks \cite{Boosting}. 

Unlike prior works, this paper proposes a novel knowledge distillation framework that systematically transfers the multiscale structural knowledge of Graph Neural Networks (GNNs) to Transformer student models. By bridging the architectural gap through micro-macro distillation losses and multiscale feature alignment, it establishes a new approach for distilling graph structural information into Transformer architectures.