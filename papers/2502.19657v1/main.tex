\documentclass[10pt]{article} % For LaTeX2e
% \usepackage[accepted]{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}


\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}


\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\algnewcommand{\LineCommentCont}[1]{\State /* #1 */}


% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\DeclareMathOperator{\Top}{Top}
\DeclareMathOperator{\sg}{sg}

\usepackage{scalerel}
\newcommand*{\pdot}{\mathbin{\scalerel*{\boldsymbol\odot}{\circ}}}

\usepackage{multirow}
\usepackage{multicol}

\usepackage{hyperref}
\usepackage{url}

\usepackage{todonotes}
% \todo{default to-do}This is text that needs some attention.\\
% \todo[inline]{Default inline to-do}
% \todo[inline, color=green]{Green inline to-do}
\colorlet{lightred}{red!25}
\newcommand{\MJC}[1]{\todo[inline, color=lightred]{Mark : #1}}
\newcommand{\Pavel}[1]{\todo[inline, color=green]{Pavel : #1}}

\definecolor{lightblue}{HTML}{267cfc}
\newcommand{\bluechange}[1]{\textcolor{lightblue}{#1}}


% \title{Boosting zero-shot ranking function performance with its own variation}
% \title{Variation is the spice of zero-shot neural architecture search}
\title{Variation Matters: from Mitigating to Embracing Zero-Shot NAS Ranking Function Variation}


% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Pavel Rumiantsev \email pavel.rumiantsev@mail.mcgill.ca \\
      \addr The Department of Electrical and Computer Engineering \\
      McGill University
      \AND
      \name Mark Coates \email mark.coates@mcgill.ca \\
      \addr The Department of Electrical and Computer Engineering \\
      McGill University}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\def\month{02}  % Insert correct month for camera-ready version
\def\year{2025} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=SbGt90dxdp}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}

Neural Architecture Search~(NAS) is a powerful automatic alternative to manual design of a neural network.
In the zero-shot version, a fast ranking function is used to compare architectures without training them.
The outputs of the ranking functions often vary significantly due to different sources of randomness, including the evaluated architecture's weights' initialization or the batch of data used for calculations.
A common approach to addressing the variation is to average a ranking function output over several evaluations.
We propose taking into account the variation in a different manner, by viewing the ranking function output as a random variable representing a proxy performance metric.
During the search process, we strive to construct a stochastic ordering of the performance metrics to determine the best architecture. 
% Our experiments show that the proposed procedure is effective for ten different ranking functions over four different search spaces.
Our experiments show that the proposed stochastic ordering can effectively boost performance of a search on standard benchmark search spaces.

\end{abstract}

\section{Introduction}

Zero-shot Neural Architecture Search~(NAS) is a way to perform selection of a neural architecture without training every candidate architecture.
Recently, zero-shot NAS has been applied to a variety of practical tasks, including medical image processing~\citep{wang2024mednas}, sequence processing~\citep{serianni2023training}, language model architecture enhancements~\citep{javaheripi2022litetransformersearch} and general image classification~\citep{jiang2024meco,li2022zico}.

In general, a zero-shot approach consists of two parts: a ranking function and a search algorithm~\citep{xu2021knas,rumiantsev2023performing,cavagnero2023freerea,li2022zico,jiang2024meco}.
The ranking function aims to capture the quality of a particular candidate architecture in the form of a scalar value, which allows two architectures to be easily compared~\citep{abdelfattah2021zerocost}.
A search algorithm uses a ranking function to determine the best architecture over the given {\em architectural search space}. 
For this work, we view a search space as a combination of a feasible architecture set and a dataset of labelled training, validation, and test samples.

While the zero-shot approach is the fastest way to find an architecture that performs well, it suffers from variations in the output of the ranking function. 
In order to assess an architecture as quickly as possible, most zero-shot techniques sample a single batch of data, and randomly initialize the architecture.
The ranking function then uses the batch of data and the initialized network to derive a scalar score.
The batch selection and initialization thus induce considerable randomness in the output.
There are several data-agnostic ranking functions, including SynFlow~\citep{abdelfattah2021zerocost} and LogSynFlow~\citep{cavagnero2023freerea}, which use a batch of {\em constant unit inputs}, and Zen-Score~\citep{lin2021zen}, which uses a batch of {\em randomly generated} data as an input. Thus, there is still randomness, and the same problem remains. 
Typically, zero-shot techniques address this issue by averaging the outputs of the ranking function over multiple batches of data.

A zero-shot search procedure can counter some of the variation of the preferred ranking function by employing a robust search algorithm.
For example, \citet{chen2021tenas} use prune-based search to increase the stability of the search.
In general, the performance of a search algorithm can vary considerably, depending on which ranking function is paired with it~\citep{akhauri2022evolving,rumiantsev2023performing}.
Since it is very challenging to meaningfully analyse a ranking function in isolation (independently of a search algorithm), we base our experimental conclusions on pairs of ranking function and search algorithm.
We focus on the most popular ranking functions and search algorithms.
Specifically, we select Eigenvalue score~\citep{mellor2020neural}, ReLU Hamming distance~\citep{mellor2021neural} and condition number of the neural tangent kernel (NTK)~\citep{chen2021tenas} to demonstrate how random search~\citep{mellor2020neural} and evolutionary search (REA~\citep{real2019regularized} and FreeREA~\citep{cavagnero2023freerea}) achieve better performance in the majority of cases (>80 percent), when enhanced with our statistical evaluation approach, for standard architectural search spaces including NAS-Bench-101, NAS-Bench-201, and TransNAS-Bench-101.


In this paper, we assess the variation of different ranking functions, explore how common ways to minimize the ranking function variation help to increase the zero-shot performance, and introduce a way to incorporate variation into the assessment of an architecture.
Our main contributions are:
\begin{itemize}[leftmargin=*]
    \item We introduce a ranking function variance analysis framework for zero-shot NAS researchers to analyse existing and newly designed ranking functions.
    \item We use the variance of a ranking function to statistically compare architectures, and show that this enhances the performance of random and evolutionary search algorithms for zero-shot NAS for a variety of commonly-used ranking functions and architectural spaces.
\end{itemize}



\section{Background}
\subsection{Search Algorithms}

There are three common choices of search algorithms that are used in Zero-Shot NAS: random search, prune-based search and evolutionary search.
\textbf{Random search} is the search algorithm with the least computation overhead.
It randomly samples a subset of architectures from the search space, and then selects the one with the highest rank according to the chosen ranking function. The size of the sampled subset is the main hyperparameter that controls the trade-off between search speed and efficacy.
Due to its simplicity, it is a common choice \citep{lopes2021epe,xu2021knas,mellor2020neural,mellor2021neural,mok2022demystifying}.
% Random search suffers the most from variability in the ranking function outputs. 

\textbf{Evolutionary search} procedures are based on genetic algorithms.
They start with randomly selected architectures, then  generate mutations, and then select a subset that are ranked the highest according to the ranking function output. The procedure is then iterated.
Mutations can be based on replacement of architectural operators or involve direct manipulations of the architecture encoding.
Evolutionary search is slower and more challenging to implement than random search, but the results it produces are more stable. 
Variations of this search technique are employed more often in recent works~ \citep{lin2021zen,akhauri2022evolving,li2022zico,mok2022demystifying,cavagnero2023freerea,cavagnero2023entropic,yang2023sweet,wang2024mednas}.
Often, the ranking function is applied to multiple batches of data, with the results averaged, in order to provide a more stable score. This procedure is usually performed once, when the architecture is first encountered during the search, with the calculated score cached for later use.

The third type of search algorithm is \textbf{prune-based search}.
It iteratively removes edges from the hypergraph that can be used to represent all architectures.
By estimating the difference in ranking function output for the architectures represented by the hypergraph with and without a given edge, the algorithm can choose the best edge to prune. This process is repeated until the hypergraph represents a single architecture. 
This type of search introduces significant computational overhead as it requires to evaluate the entire hypergraph for every unpruned edge~\citep{chen2021tenas}.
It is the most challenging to implement and, therefore, is rarely adopted~\citep{chen2021tenas,rumiantsev2023performing,jiang2024meco}.

\subsection{Zero-shot ranking functions}

Many zero-shot ranking functions have been proposed, and we now review some of the most prominent. \citet{abdelfattah2021zerocost} repurposed metrics previously employed in neural
network pruning. The saliency metrics \texttt{Snip}~\citep{lee2019}, \texttt{Grasp}~\citep{wang2020},
\texttt{Fisher}~\citep{turner2020} and \texttt{SynFlow}~\citep{tanaka2020pruning} measure the impact of removing a specific parameter or operation from the network. In adapting these to construct a zero-shot ranking function, \citet{abdelfattah2021zerocost} sum over all of the architectural parameters, and thus construct a  measure of the fitness of the entire architecture.
\citet{cavagnero2023freerea} introduced a logarithmic version of \texttt{SynFlow} named \texttt{LogSynFlow}. 
Applying a logarithm suppresses some of the gradient explosions that can unduly influence the score.


The first two ranking functions designed specifically for the zero-shot setting were the
\texttt{Eigenvalue score}~\citep{mellor2020neural} and the Hamming distance between the ReLU regions~\citep{mellor2021neural}.
Based on the Neural Tangent Kernel~(NTK)~\citep{jacot2018neural}, the \texttt{Eigenvalue score} uses the eigenvalues of the gradient correlation matrix to predict the learning capabilities of an architecture.
The \texttt{ReLU Hamming distance} is constructed as a determinant of the kernel of Hamming distances betweent activation patterns of ReLU, where a pattern is represented as a string of zeros and ones.
An alternative measure is the number of activated ReLU regions for a sampled batch of data~\citep{xiong2020number}.
The underlying conjecture is that activation of more regions allows the network to learn more complex data dependencies.
Similarly, \citet{peng2024swapnas} proposes to count the number of unique ReLU activation patterns across the data dimension.
The \texttt{Entropic Score}~\citep{cavagnero2023entropic} is also based on ReLU activations.

Several other ranking functions are  based on the NTK, including (i) the mean value of the NTK~\citep{xu2021knas};
(ii) the Frobenius norm of the NTK~\citep{xu2021knas};
and (iii) the negative condition number of the NTK~\citep{chen2021tenas}.
The NTK can introduce undesirable computational overhead, so other researchers have explored alternative metrics. \citet{rumiantsev2023performing} show that the Frobenius norm of the Neural Network Gaussian Process~(NNGP) kernel can act as an effective ranking function. \citet{jiang2024meco} focus on the Pearson correlation kernels of the internal representations of an architecture, and compute the sum of the smallest eigenvalues of these kernels. For all of these proposed ranking functions, the required kernels are estimated using a random batch of data. 

Since most benchmark search spaces focus on the classification task,
some ranking functions incorporate the available label information.
Label-Gradient Alignment~(\texttt{LGA})~\citep{mok2022demystifying} constructs a metric based on both the class similarity matrix and the NTK. \citet{lopes2021epe} propose \texttt{EPE-NAS}, which uses the gradient correlation matrix, and is thus similar to the \texttt{Eigenvalue score}, but is computed class-wise. For practical reasons, both \texttt{LGA} and \texttt{EPE-NAS} are computed using a random batch of data.

Some ranking functions are based on the gradients of the trainable weights. \texttt{SweetGrad} counts the number of gradient norms in a predefined interval~\citep{yang2023sweet}.
\citet{li2022zico} sum the coefficients of variations associated with the weight gradients to compute \texttt{ZiCo}.

\section{Problem statement}

A neural architecture search space can be represented as a set of feasible architecutres $\{arch_i\}_{[i=1..N_{SS}]}$ of size $N_{SS}$.
% graph $G_X$, where each node corresponds to an architecture and an edge represents a single architectural change.
Associated with the architecture space is a dataset $\mathcal{D}$ consisting of pairs of features $x\in\mathcal{X}$ and labels $y\in\mathcal{Y}$.
In this work we consider computer vision tasks where an architecture $arch_i$ is associated with a final accuracy after training, labelled $acc_i$.
A zero-shot ranking function assigns a scalar score to each architecture: $s_i = r(arch_i, \mathcal{D})$.
A search is then conducted using these scalar scores, with the goal of selecting the most accurate architecture. 

As discussed above, almost every zero-shot ranking function has an element that is evaluated over a batch of input data (e.g., the NTK, gradient coefficients of variations, or logit similarity matrix).
To improve the stability of the output, most ranking functions are averaged over multiple random batches.
Our goal is to use the multiple ranking function outputs in a more effective way.

In subsequent sections, we explore how sensitive different ranking functions are to the random selection of the input data used in their calculation.
We point out how commonly used statistical methods allow us to overcome such sensitivity in order to improve the performance of a zero-shot approach. 

\section{Methodology}

This section describes our study into the variability of ranking functions.
First, we introduce a variation metric for assessing the variability of a ranking function for a particular search space and batch size. 
Second, we present our zero-shot search setup and explain how the variation metric can be used to improve performance.

\subsection{Ranking function variation}
\label{sec:ranking_function_variation}
It is common for ranking functions to use a real training dataset.
In order to assign a score to an architecture, a batch of data is randomly drawn from the dataset.
The ranking function scores vary from batch to batch, with the extent of the variation being potentially dependent on the batch size, the dataset, and the task.

We measure ranking function variation over the entire search space by computing, for each architecture, an empirical coefficient of variation for the ranking function output over randomly sampled batches of input data. We then average over the architectures.
To ensure the coefficient of variation is an appropriate metric, we have specified a ``meaningful zero’’ for the ranking functions. 
We discuss this in detail in Appendix~\ref{sec:meaningful_zero}

Denoting the batch size by $B$ and the number of sampled batches by $V$, and recalling that there are $N_{SS}$ architectures in the search space, we define the ranking function variation as:
\begin{align}
\label{eq:variation}
    Var_{SS}(r,B,V) \triangleq \frac{1}{N_{SS}}\sum_{i=1}^{N_{SS}}
    CV\left(\mathcal{M}_i(B,V)\right) \quad \mathrm{for} \quad \mathcal{M}_i(B,V) = \{r(arch_i, d_v)\}_{v=1}^V\,, 
\end{align}
where $r$ is a ranking function, $d_v$ is a batch of data of size $B$, uniformly sampled from the dataset associated with the search space $SS$, and $CV(\mathcal{X})=\frac{Var(\mathcal{X})}{Mean(\mathcal{X})}$.
A higher average coefficient of variation indicates greater variability with respect to the selection of input data.

Typically, if we know that a ranking function is highly variable with respect to the input data, then this motivates us to enhance the robustness by averaging over multiple random batches.
On the other hand, if a ranking function exhibits less variability, then a score can be taken from a reduced number of randomly drawn batches. This leads to a faster search, because ranking function evaluation tends to be the most computationally expensive element of zero-shot NAS.

\subsection{Search algorithm}
\label{sec:search_algo}
Zero-shot architecture search uses a ranking function as a proxy to evaluate the quality of an architecture.
In order to compare two architectures, a selected ranking function is evaluated on both of them and the outputs of the ranking function are compared.
Assuming positive correlation between accuracy and the adopted metric, the architecture with the higher corresponding metric is considered to be better.
The ranking function outputs are subject to randomness, due, for example, to random architecture initialization or random selected input batch.
A common technique to mitigate randomness is averaging: a ranking function is evaluated multiple times on the same architecture and the output is averaged over the evaluations.

Instead of averaging, the key component of our approach is a statistical comparison of the architectures. We treat multiple evaluations of the ranking metric as multiple observations of a random variable. We then test for {\em stochastic dominance}. Viewing the ranking function metrics for two compared architectures as random variables, $X$ and $Y$, we test whether $p(X>k) \geq p(Y>k)$ for all $k$ (and $p(X>k) > p(Y>k)$ for some $k$).
This test differs from comparing the mean performance, $\bar{X}>\bar{Y}$. Stochastic dominance implies $\bar{X}>\bar{Y}$, but the converse is not true. Our testing process thus targets a stricter performance discrepancy. This makes the search more effective and efficient, because we navigate towards architectures that are more likely to be genuinely superior. 

To perform the comparison, we use the U-value from the Mann-Whitney U-test~\citep{mann1947test}.
This statistical test assesses the null hypothesis that both sets of observations are drawn from the same distribution.
The alternative hypothesis (for a one-sided test) is that the random variable X is stochastically greater than the random variable Y. 
By comparing the Mann-Whitney U values for the two samples, we can identify one as a superior architecture.
We declare such superiority only if the null hypothesis can be rejected at a predetermined significance level (e.g., 0.05).
If this is not the case, we cannot meaningfully differentiate between the architectures in terms of predicted performance.
\begin{wrapfigure}{r}{0.6\textwidth}
    \begin{minipage}{0.6\textwidth}
    \vspace{-2em}
      \begin{algorithm}[H]
        \caption{Statistical MAX and TOP-K pseudocode} \label{alg:stat_max_topk}
        \begin{algorithmic}
            \Ensure Each $r_i$ is a list of ranking function evaluations
            \Function{STAT-MAX}{$r_1,\ldots,r_T$}
                \State $max\gets r_1$
                \For{$i\gets 2, T$}
                    \LineCommentCont{Mann-Whitney U-test}
                    \State $p\gets$ \Call{MannWhitneyU}{$max, r_i$}
                    \LineCommentCont{$Threshold$ is a significance level}
                    \If{$p<Threshold$} 
                        \State $max\gets r_i$
                    \EndIf
                \EndFor
                \State \textbf{return} $max$
            \EndFunction
            \Statex
            \Function{STAT-TOPK}{$k,r_1,\ldots,r_T$}
            \State $top\gets \emptyset$
            \State $rs\gets {r_1,\ldots,r_T}$
            \For{$i\gets 1, k$}
                \State $m\gets$ \Call{STAT-MAX}{$rs_1,rs_2,\ldots$}
                \State $top\gets top \cup \{m\}$
                \State $rs\gets rs \setminus \{m\}$
            \EndFor
            \State \textbf{return} $top$
            \EndFunction
        \end{algorithmic}
      \end{algorithm}
    \vspace{-3em}
    \end{minipage}
\end{wrapfigure}
We summarise this approach as a statistical maximum and statistical top-k (see~\Algref{alg:stat_max_topk}).
Those procedures iteratively search for a stochastically dominating maximum over the given sets of ranking functions outputs. 
Given that we are checking for the stochastically dominating maximum, the output of our procedure is not permutation invariant in general.
As the first example, let us consider two sets of scores, A and B, with A stochastically dominating B.
The output of \Algref{alg:stat_max_topk} will be A for both inputs [A,B] and [B,A].
As a second example, let us consider two sets of scores, A and B, where neither stochastic dominates the other.
For input [A, B], the output will be A. 
For input [B, A], the output will be B.
Practically, this is equivalent to a random tie breaker. As random tie breakers are common in search functions, we do not view permutation invariance as a necessary requirement.

Pairwise search is not the only viable design option
For example, Dunnett’s test could be used to perform multiple comparisons. 
However, our experience is that search algorithms based on multiple comparison tests are much more  computationally expensive. The statistical tests also tend to impose stronger assumptions. Thus, we use pairwise search to minimise computational overhead and extend the applicability of the procedure.

In our work, we use two popular search algorithms: {\em random} search and {\em evolutionary} search.
Both algorithms are easy to implement for an arbitrary search space and require few additional considerations when paired with a ranking function in a zero-shot setting.
For \textbf{random search}~\citep{mellor2020neural}, we sample $N$ architectures randomly from the search space.
Then we evaluate a ranking function on each sampled architecture $V$ times.
We use the Mann-Whitney U-test to rank the candidates.
In the case of ties, we select an architecture randomly from the tied candidates.

The REA~\citep{real2019regularized} \textbf{evolutionary search} starts by sampling a {\em population} of $N$ architectures randomly from the search space.  During each iteration, a subset of $n$ exemplars~(architectures) is randomly sampled from the population. We order the exemplars using the Mann-Whitney U-test, with random tie-breaking.
The exemplar that is first in this ordering is selected and mutated.
In our experiments, we use an operator replacement mutation, so that the original exemplar and the mutated exemplar have an architectural edit distance of one. 
We evaluate a ranking function on the mutant $V$ times and include the mutated architecture in the population, replacing the oldest ranked exemplar.
After $T$ iterations of this procedure, we use the Mann-Whitney U-test to order the population and select the highest ranked.

The FreeREA~\citep{cavagnero2023freerea} evolutionary search procedure introduces a small modification to the REA algorithm.
After a subset of $n$ exemplars is randomly sampled, two exemplars with the statistically highest rank are selected.
Both exemplars are mutated and the two mutants are included in the population. {\em Crossover} is performed by splicing together parts of the two selected exemplars.
Both mutated exemplars and the resultant crossover exemplar are included into the population replacing the three oldest exemplars, which increases the exploration capabilities of the search process.
We provide pseudocode for the evolutionary algorithms in Appendix~\ref{sec:greedy_evo_search}.


\section{Experiments}

\subsection{Ranking function variation}

\begin{figure}
    \begin{subfigure}[t]{0.56\linewidth}
    \includegraphics[width=1.0\linewidth, keepaspectratio]{imgs/coefficient_of_variation_nasbench.pdf}
    \caption{}
    \label{fig:cv:nasbench}
  \end{subfigure}
  \begin{subfigure}[t]{0.44\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth, keepaspectratio]{imgs/coefficient_of_variation_transnas.pdf}
    \caption{}
    \label{fig:cv:transnas}
  \end{subfigure}
  \caption{Ranking functions variation $Var_{SS}(r,B,V)$ for ranking function $r$, batch size $B$ = 64, $V$=10 and search space $SS$)  on (\subref{fig:cv:nasbench}) NAS-Bench and (\subref{fig:cv:transnas}) TransNAS-Bench-101 demonstrate that different ranking functions have considerably different variations within the search space, but a specific ranking function's variation remains similar for the different search spaces.}
  \label{fig:cv}
\end{figure}

In the first experiment, we show how different ranking functions are influenced by a randomly chosen batch of data and examine how consistent this behaviour is over a variety of search spaces. A lower coefficient of variation means that a ranking function is generally more reliable, can provide more consistent output, and requires fewer batches.

We measure the variation $Var_{SS}(r,B,V)$ by evaluating~\eqref{eq:variation}. Recall that $SS$ denotes the search space, $r$ denotes the ranking function, $B$ is the batch size, and $V$ is the number of evaluations. In this experiment, in order to focus on the most influential source of variability -- the input batch -- we initialize each network once, before the first batch is selected. We set $V=10$ and set $B=64$. An output of a ranking function is averaged over 10 runs for different random batches given the same initialisation for the network.

We present results for the NAS-Bench search spaces in Fig.~\ref{fig:cv:nasbench} and results for the TransNAS search spaces in Fig.~\ref{fig:cv:transnas}.
Overall, Eigenvalue score and ReLU Hamming distance are by far the most stable ranking functions. Snip is more stable than the others.
Fisher exhibits stability similar to that of Snip on the NAS-Bench search spaces, which are considered easy, but performs worse on TransNAS-Bench.
The performance of Fisher and NTK-based kernels are highly dependent on the dataset used by a search space. We emphasize that low variation is a desirable criterion for the ranking function selection, but it does not imply high performance. 

For all ranking functions, the coefficient of variation of an architecture tends to decrease as the average accuracy increases, although this effect is more pronounced for some ranking function and search space combinations. 
In general, this is a result of the ranking function output varying less for the higher quality architectures. 
Fig.~\ref{fig:acc_vs_cv} presents example results for ReLU Hamming distance on NAS-Bench-101~(CIFAR-10) and Eigenvalue Score on NAS-Bench-201~(ImageNet16-120).
Due to the very high variability often exhibited by poorly performing architectures, we exclude the worst-performing decile from the analysis, and present box plots of the coefficients of variation (for individual architectures) versus accuracy on the test set.
We observe a negative trend in each case, although it is much more evident for Eigenvalue Score.
% We observed similar behaviour for almost all other combinations of ranking functions and search spaces, and additional results are presented in Appendix **.
% \Pavel{Why do we need this appendix?}

\begin{figure}
  \begin{subfigure}[t]{0.49\linewidth}
  \includegraphics[width=1.0\linewidth, keepaspectratio]{imgs/acc_vs_score_nb101_cifar10_nwot.pdf}
    \caption{}
    \label{fig:acc_vs_cv:nasbench101}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\linewidth}
  \includegraphics[width=1.0\linewidth, keepaspectratio]{imgs/acc_vs_score_nb201_imagenet_jacob.pdf}
    \caption{}
    \label{fig:acc_vs_cv:nasbench201}
  \end{subfigure}

    \caption{Box plot (versus accuracy) of coefficients of variation for individual architectures ($CV(\mathcal{M}_i(B,V))=\frac{Var(\mathcal{M}_i(B,V)}{Mean(\mathcal{M}_i(B,V)}$ where $\mathcal{M}_i(B,V) = \{r(arch_i, d_v)\}_{v=1}^V$ is a set of $V$=10 ranking function outcomes for architecture $arch_i$ derived from random batches of data $d_k$, each of size $B = 64$. The depicted ranking functions are ReLU Hamming distance on NAS-Bench-101~(CIFAR-10)~(\subref{fig:acc_vs_cv:nasbench101}) and Eigenvalue Score on NAS-Bench-201~(ImageNet16-120)~(\subref{fig:acc_vs_cv:nasbench201}). For visualization purposes, we exclude ten percent of the architectures-those that exhibit lowest accuracy.}
    \label{fig:acc_vs_cv}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth, keepaspectratio]{imgs/cv_correaltion.pdf}
    \caption{Kendall-$\tau$ correlation coefficient between coefficient of variation values and validation accuracy, for each ranking function and search space. EPE-NAS and LGA require classification labels and therefore cannot be computed for TransNAS-Bench-101~(Autoencoder) and TransNAS-Bench-101~(Normal).}
    \label{fig:cv_correaltion}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth, keepaspectratio]{imgs/cv_mean_score_correaltion.pdf}
    \caption{Kendall-$\tau$ correlation coefficient between a ranking function coefficent of variation values and ranking function mean value taken over 10 evaluations, for each ranking function and search space. EPE-NAS and LGA require classification labels and therefore can not be computed for TransNAS-Bench-101~(Autoencoder) and TransNAS-Bench-101~(Normal).}
    \label{fig:cv_mean_score_correaltion}
\end{figure}

The negative trend suggests that we could consider directly using lower coefficient of variation as an indicative variable during the search. To further investigate this possibility, we measured Kendall-$\tau$ correlation between ranking function variation and architecture accuracy (see Fig.~\ref{fig:cv_correaltion}). In many cases, there is a strong negative correlation, but there are exceptions. For example, there is very weak correlation between accuracy and the coefficient of variation of the Fisher metric. On some search spaces, LGA even exhibits a positive correlation, meaning that it is, on average, more unstable for the better-performing architectures.
We conjecture that this behaviour arises due to the use of labels in both the LGA ranking function and the accuracy evaluation. EPE-NAS, the other ranking function that uses labels, exhibits close-to-zero (and occasionally slightly positive) correlation.

We can consider naive approaches to incorporate the coefficient of variation into the search process. For example, after appropriate normalization, we could sum a ranking function output with its coefficient of variation. Fig.~\ref{fig:cv_mean_score_correaltion} depicts the Kendall-$\tau$ correlation between the coefficient of variation of a ranking function and its mean value for different search spaces. For some ranking functions such as Eigenvalue score and ReLU distance, the magnitude of the correlation is high, meaning that the coefficient of variation does not provide much additional useful information for the search.
By contrast, for other ranking functions such as Frobenius norm of the NTK and the condition number of NTK, the magnitude of the correlation is low. 
In experiments (see Table~\ref{tab:cv_as_ranker} in Appendix~\ref{sec:cv_as_ranker}) we observe that the search performance improvement of our
approach over this naive scheme is indeed considerably greater for these ranking functions.
Although a naive incorporation of the coefficient of variation can improve results of the search, its impact is inconsistent; the statistical comparison approach we propose in~\Secref{sec:search_algo} exhibits more consistent improvement and is effective for all ranking functions.


\subsection{Random search}
\label{sec:random_search}
\begin{table}
    \centering
    \caption{Random search on $N$ sampled architectures with averaging (Avg) and statistic (Stat) evaluation. Mann-Whitney U-test is used for statistical evaluation. Each ranking function is evaluated 10 times for the architecture. We provide mean values and standard deviations over 100 runs. For each pair of evaluations, the best score is highlighted in \textbf{bold}; if the higher score is statistically significant with p-value below 0.05, it is highlighted in \textbf{\underline{bold underlined}}.}\label{tab:random_search}
    \footnotesize\begin{tabular}{l|c|c|c|c|c|c} \hline
    \multirow{2}{*}{Search Space} & \multicolumn{2}{c|}{Eigenvalue score} & \multicolumn{2}{c|}{ReLU dist} & \multicolumn{2}{c}{NTK(cond)} \\ \cline{2-7}
     & Avg & Stat & Avg & Stat & Avg & Stat \\ 
    \hline \multicolumn{7}{c}{\textbf{N$ = 10$}} \\ \hline
NAS-Bench-101 (CIFAR-10) & 91.51$\pm$1.44 & \textbf{91.71$\pm$2.22} & 91.28$\pm$1.80 & \textbf{91.63$\pm$1.51} & 90.94$\pm$1.42 & \textbf{91.24$\pm$1.63} \\
NAS-Bench-201 (CIFAR-10) & 91.33$\pm$1.57 & \textbf{\underline{92.21$\pm$1.66}} & 91.80$\pm$1.33 & \textbf{\underline{92.34$\pm$1.14}} & 91.61$\pm$1.96 & \textbf{91.92$\pm$2.15} \\
NAS-Bench-201 (CIFAR-100) & 68.61$\pm$3.55 & \textbf{69.13$\pm$2.93} & \textbf{71.76$\pm$1.92} & 71.68$\pm$2.03 & 68.05$\pm$4.42 & \textbf{69.47$\pm$3.17} \\
NAS-Bench-201 (ImageNet16-120) & 40.59$\pm$5.05 & \textbf{41.22$\pm$4.56} & 40.69$\pm$4.09 & \textbf{41.24$\pm$3.33} & 40.91$\pm$5.45 & \textbf{41.64$\pm$4.76} \\
NAS-Bench-201 (NinaPro) & 90.13$\pm$1.30 & \textbf{90.40$\pm$1.40} & 90.34$\pm$1.83 & \textbf{90.76$\pm$1.89} & 90.77$\pm$1.35 & \textbf{90.99$\pm$1.38} \\
TransNAS-Bench-101 (Jigsaw) & 90.72$\pm$4.55 & \textbf{91.88$\pm$4.76} & 87.28$\pm$15.74 & \textbf{89.11$\pm$11.73} & 90.97$\pm$4.63 & \textbf{\underline{92.48$\pm$3.18}} \\
TransNAS-Bench-101 (Class object) & 47.33$\pm$2.47 & \textbf{\underline{48.26$\pm$2.84}} & 47.65$\pm$2.50 & \textbf{48.71$\pm$2.69} & 47.91$\pm$2.11 & \textbf{48.48$\pm$2.46} \\
TransNAS-Bench-101 (Class scene) & 59.94$\pm$5.30 & \textbf{60.52$\pm$5.14} & 60.87$\pm$5.09 & \textbf{61.70$\pm$4.11} & 60.90$\pm$3.53 & \textbf{\underline{61.58$\pm$1.73}} \\
\hline \multicolumn{7}{c}{\textbf{N$ = 100$}} \\ \hline
NAS-Bench-101 (CIFAR-10) & 91.71$\pm$1.32 & \textbf{\underline{92.06$\pm$1.15}} & 91.98$\pm$2.08 & \textbf{91.98$\pm$2.01} & 89.67$\pm$2.25 & \textbf{90.06$\pm$1.98} \\
NAS-Bench-201 (CIFAR-10) & 91.10$\pm$1.61 & \textbf{91.73$\pm$1.43} & 92.11$\pm$0.98 & \textbf{92.52$\pm$1.10} & 91.09$\pm$1.84 & \textbf{91.75$\pm$2.38} \\
NAS-Bench-201 (CIFAR-100) & 68.43$\pm$2.82 & \textbf{69.05$\pm$2.64} & 70.38$\pm$1.77 & \textbf{70.75$\pm$1.79} & 69.39$\pm$4.50 & \textbf{70.01$\pm$4.36} \\
NAS-Bench-201 (ImageNet16-120) & 40.34$\pm$4.66 & \textbf{40.96$\pm$4.98} & 42.54$\pm$3.93 & \textbf{43.34$\pm$2.47} & 41.04$\pm$4.49 & \textbf{41.50$\pm$4.42} \\
NAS-Bench-201 (NinaPro) & 90.00$\pm$1.91 & \textbf{90.22$\pm$1.86 }& 89.18$\pm$1.72 & \textbf{\underline{89.53$\pm$1.55}} & 91.29$\pm$1.00 & \textbf{91.36$\pm$1.05} \\
TransNAS-Bench-101 (Jigsaw) & 89.95$\pm$5.57 & \textbf{90.59$\pm$4.10} & \textbf{\underline{90.58$\pm$3.55}} & 85.33$\pm$2.21 & \textbf{92.00$\pm$2.78} & 91.99$\pm$2.96 \\
TransNAS-Bench-101 (Class object) & 47.04$\pm$4.06 & \textbf{49.46$\pm$4.56} & 48.25$\pm$3.11 & \textbf{48.87$\pm$2.96} & 49.48$\pm$0.97 & \textbf{49.81$\pm$1.77} \\
TransNAS-Bench-101 (Class scene) & 60.94$\pm$3.23 & \textbf{61.12$\pm$4.14} & 61.19$\pm$3.10 & \textbf{61.50$\pm$4.32} & \textbf{61.56$\pm$0.56} & 61.29$\pm$2.85 \\
\hline \multicolumn{7}{c}{\textbf{N$ = 1000$}} \\ \hline
NAS-Bench-101 (CIFAR-10) & 91.47$\pm$1.36 & \textbf{91.57$\pm$1.31} & 92.12$\pm$2.45 & \textbf{92.60$\pm$1.73} & 90.54$\pm$2.35 & \textbf{90.92$\pm$2.40} \\
NAS-Bench-201 (CIFAR-10) & 91.25$\pm$1.46 & \textbf{\underline{92.08$\pm$1.64}} & 92.04$\pm$1.22 & \textbf{\underline{92.69$\pm$1.08}} & 91.00$\pm$2.07 & \textbf{91.88$\pm$2.16} \\
NAS-Bench-201 (CIFAR-100) & 69.32$\pm$3.17 & \textbf{70.63$\pm$2.68} & 70.78$\pm$1.66 & \textbf{71.52$\pm$1.14} & 69.83$\pm$2.62 & \textbf{70.40$\pm$2.58} \\
NAS-Bench-201 (ImageNet16-120) & 41.42$\pm$4.56 & \textbf{42.17$\pm$4.11} & 42.96$\pm$2.78 & \textbf{43.79$\pm$2.92} & 41.59$\pm$3.41 & \textbf{42.24$\pm$2.03} \\
NAS-Bench-201 (NinaPro) & 90.59$\pm$1.73 & \textbf{90.61$\pm$2.19} & \textbf{90.47$\pm$1.25} & 90.21$\pm$1.34 & 91.49$\pm$0.65 & \textbf{91.57$\pm$0.89} \\
TransNAS-Bench-101 (Jigsaw) & 88.63$\pm$4.32 & \textbf{\underline{90.81$\pm$3.93}} & \textbf{90.60$\pm$3.00} & 88.52$\pm$11.35 & 92.40$\pm$0.98 & \textbf{92.57$\pm$1.51} \\
TransNAS-Bench-101 (Class object) & 44.64$\pm$3.26 & \textbf{45.46$\pm$4.21} & 44.99$\pm$3.31 & \textbf{45.82$\pm$5.97} & 44.98$\pm$1.15 & \textbf{\underline{45.36$\pm$0.89}} \\
TransNAS-Bench-101 (Class scene) & 61.42$\pm$3.37 & \textbf{61.50$\pm$3.51} & 61.47$\pm$3.09 & \textbf{61.76$\pm$4.32} & 61.74$\pm$0.43 & \textbf{\underline{62.29$\pm$0.45}} \\
\hline
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Evolutionary search results. Results with averaging (Avg) and statistical (Stat) evaluation are provided. Mann-Whitney U-test is used for the statistical evaluation. Each ranking function is evaluated 10 times for the architecture. For each evaluation pair, the best score is highlighted in \textbf{bold}; if the higher score is statistically significant with p-value below 0.05, it is highlighted in \textbf{\underline{bold underlined}}.}\label{tab:evosearch_results}
    \footnotesize\begin{tabular}{l|l|c|c|c|c|c|c} \hline
    \multirow{2}{*}{Search Space} & \multirow{2}{*}{Search Alg.} & \multicolumn{2}{c|}{Eigenvalue score} & \multicolumn{2}{c|}{ReLU dist} & \multicolumn{2}{c}{Ensemble} \\ \cline{3-8}
     & & Avg & Stat & Avg & Stat & Avg & Stat \\ \hline

\multirow{3}{*}{\shortstack[l]{NAS-Bench-101 \\(CIFAR-10)}} & Greedy & 92.76$\pm$0.90 & \textbf{92.86$\pm$0.66} & 92.36$\pm$0.31 & \textbf{92.54$\pm$0.59} & 92.54$\pm$1.01 & \textbf{92.82$\pm$0.77} \\
 & REA & 93.02$\pm$0.32 & \textbf{93.11$\pm$0.30} & 93.15$\pm$0.15 & \textbf{\underline{93.31$\pm$0.09}} & 92.99$\pm$0.42 & \textbf{93.28$\pm$0.25} \\
 & FreeREA & 93.23$\pm$0.47 & \textbf{\underline{93.51$\pm$0.35}} & 93.29$\pm$0.58 & \textbf{93.46$\pm$0.33} & 93.32$\pm$0.53 & \textbf{93.52$\pm$0.38} \\
 \hline
\multirow{3}{*}{\shortstack[l]{NAS-Bench-201 \\(CIFAR-100)}} & Greedy & 69.54$\pm$0.13 & \textbf{\underline{70.01$\pm$0.42}} & 70.00$\pm$0.03 & \textbf{\underline{70.06$\pm$0.02}} & 71.03$\pm$0.46 & \textbf{71.12$\pm$0.58} \\ 
 & REA & 71.64$\pm$0.80 & \textbf{72.14$\pm$0.69} & 71.83$\pm$0.61 & \textbf{72.25$\pm$0.44} & 72.06$\pm$0.64 & \textbf{72.40$\pm$0.75} \\ 
 & FreeREA & 71.98$\pm$0.26 & \textbf{\underline{73.13$\pm$0.07}} & 71.63$\pm$1.35 & \textbf{72.43$\pm$1.04} & 71.89$\pm$0.97 & \textbf{72.15$\pm$0.76} \\
 \hline
\multirow{3}{*}{\shortstack[l]{NAS-Bench-201 \\(ImageNet16-120)}} & Greedy & 42.68$\pm$1.06 & \textbf{\underline{44.08$\pm$0.57}} & 44.78$\pm$1.34 & \textbf{\underline{45.90$\pm$0.71}} & 44.11$\pm$1.79 & \textbf{45.21$\pm$0.80} \\
 & REA & 44.90$\pm$0.76 & \textbf{45.18$\pm$0.56} & 43.82$\pm$1.53 & \textbf{44.45$\pm$1.06} & 44.23$\pm$0.84 & \textbf{44.76$\pm$0.94} \\
 & FreeREA & 44.84$\pm$1.17 & \textbf{\underline{45.52$\pm$0.70}} & 43.82$\pm$1.23 & \textbf{44.38$\pm$0.92} & 44.26$\pm$1.48 & \textbf{\underline{45.22$\pm$0.74}} \\
 \hline
\multirow{3}{*}{\shortstack[l]{TransNAS-Bench-101 \\(Jigsaw)}} & Greedy & 92.49$\pm$1.11 & \textbf{\underline{93.68$\pm$0.80}} & 92.23$\pm$0.94 & \textbf{\underline{92.98$\pm$0.69}} & 93.06$\pm$0.85 & \textbf{94.29$\pm$0.41}  \\
 & REA & 91.88$\pm$0.31 & \textbf{\underline{93.47$\pm$0.23}} & 92.12$\pm$0.39 & \textbf{\underline{93.29$\pm$0.29}} & 93.80$\pm$0.41 & \textbf{\underline{95.06$\pm$0.60}} \\ 
 & FreeREA & 91.79$\pm$0.18 & \textbf{\underline{93.26$\pm$0.11}} & 92.03$\pm$0.17 & \textbf{\underline{92.97$\pm$0.18}}& 93.42$\pm$0.58 & \textbf{\underline{94.99$\pm$0.58}}  \\ 
 \hline
\multirow{3}{*}{\shortstack[l]{TransNAS-Bench-101 \\(Class object)}} & Greedy & 43.89$\pm$1.73 & \textbf{\underline{45.16$\pm$1.44}} & \textbf{46.04$\pm$0.07} & 46.01$\pm$0.04 & 46.29$\pm$3.34 & \textbf{\underline{47.19$\pm$0.12}}  \\
 & REA & 46.10$\pm$0.09 & \textbf{\underline{46.44$\pm$0.07}}& 45.99$\pm$0.09 & \textbf{\underline{46.86$\pm$0.14}} & 46.26$\pm$0.08 & \textbf{\underline{46.52$\pm$0.20}}  \\ 
 & FreeREA & 46.73$\pm$0.09 & \textbf{46.81$\pm$0.08} & \textbf{46.97$\pm$0.06} & 46.97$\pm$0.07 & 47.09$\pm$0.11 & \textbf{47.14$\pm$0.09} \\ 
 \hline
    \end{tabular}
\end{table}


In order to demonstrate our statistical approach, we first use a random search proposed by \citet{mellor2020neural}.
Results are presented in Table~\ref{tab:random_search}.
We selected Eigenvalue score, ReLU Hamming distance and condition number of NTK for this experiment.
The first two serve as examples of ranking functions with low variation, while the final one exhibits high variation.
To evaluate an architecture we randomly sample a batch of 64 and randomly initialise architecture weights. 
This evaluation simulates a typical case of ranking function usage.
A ranking function is evaluated ten times on every queried architecture.
In order to reduce the number of experiments we do not test combinations of ranking function.
% Here and in the following evolutionary search experiments we adopt the same experimental procedure as \citet{krishnakumar2022bench} and evaluate individual ranking functions rather than ensembles. 
% While it has been demonstrated that combining several ranking functions into an ensemble does lead to more effective search~\cite{chen2021tenas,rumiantsev2023performing,abdelfattah2021zerocost}, we do not test ensembles as the number of required experiments increases dramatically.
We provide results for averaged rank value and for the proposed statistical ranking approach.
The 5\% significance level is used for rejecting the null hypothesis when conducting the Mann-Whitney U-test.
We repeat each experiment 100 times, computing the mean value of the accuracy for the selected architecture and its variation.
Our findings demonstrate that our statistical approach is a better way to use the multiple outcomes than simple averaging.
As we can see in Table~\ref{tab:random_search}, the proposed statistical method leads to improved performance for almost every ranking function and search space.
The average accuracy improvement is 0.49. 
The improvement is small, but there is a negligible additional computational overhead.
Our experiments do not expose a significant difference between the improvement for low- versus high-variance ranking functions.
We observed that the largest search improvement is achieved on the NAS-Bench-201 search spaces, but that could be due to the fact that those search spaces are small.
Less improvement is observed for the largest search space NAS-Bench-101~(CIFAR-10).



\subsection{Evolutionary search}



\begin{table}
    \centering
    \caption{Experiemental results on SWAPNAS, MeCo, SynFlow and LogSynFlow for evolutionary search. Results with averaging (Avg) and statistical (Stat) evaluation are provided. Mann-Whitney U-test is used for the statistical evaluation. Each ranking function is evaluated 10 times for the architecture. For each evaluation pair, the best score is highlighted in \textbf{bold}; if the higher score is statistically significant with p-value below 0.05, it is highlighted in \textbf{\underline{bold underlined}}.}
    \label{tab:add_evo_small}
\begin{tabular}{l|l|c|c|c|c}
\hline
\multirow{2}{*}{\shortstack[l]{Ranking \\ function}} & \multirow{2}{*}{Method} & \multicolumn{2}{c|}{\shortstack[c]{NAS-Bench-101 \\(CIFAR-10)}} & \multicolumn{2}{c}{\shortstack[c]{NAS-Bench-201 \\(ImageNet16-120)}} \\ \cline{3-6} 
 &  & Avg & Stat & Avg & Stat \\ \hline
\multirow{2}{*}{SWAPNAS} & REA & 93.05$\pm$0.12 & \textbf{\underline{93.37$\pm$0.11}} & 44.39$\pm$0.98 & \textbf{45.74$\pm$1.22} \\
    & FreeREA & 93.42$\pm$0.36 & \textbf{93.71$\pm$0.32} & 44.75$\pm$1.15 & \underline{\textbf{45.89$\pm$1.01}}  \\ \hline
\multirow{2}{*}{MeCo} & REA & 93.13$\pm$0.54 & \textbf{93.52$\pm$0.44} & 42.47$\pm$0.84 & \textbf{42.81$\pm$0.81} \\
    & FreeREA & 93.21$\pm$0.45 & \textbf{93.51$\pm$0.39} & 42.88$\pm$0.79 & \textbf{43.23$\pm$0.85} \\ \hline
\multirow{2}{*}{SynFlow} & REA & \textbf{93.69$\pm$0.38} & 93.56$\pm$0.31 & 44.29$\pm$1.02 & \textbf{44.57$\pm$0.99}  \\
    & FreeREA & \textbf{93.80$\pm$0.24} & 93.74$\pm$0.32 & \textbf{44.98$\pm$0.89} & 44.74$\pm$1.21  \\ \hline
\multirow{2}{*}{LogSynFlow} & REA & \textbf{93.76$\pm$0.36} & 93.62$\pm$0.28 & \textbf{45.03$\pm$0.82} & 44.91$\pm$1.04  \\
    & FreeREA & 93.76$\pm$0.25 & \textbf{93.77$\pm$0.36} & 45.06$\pm$0.90 & \textbf{45.16$\pm$0.95}  \\ \hline
\end{tabular}
\end{table}


We also compare the performance of the standard averaging approach with that of the proposed statistical evaluation when each is used in conjunction with evolutionary search.
As before, the 5\% significance level is used for rejecting the null hypothesis of the Mann-Whitney U-test.
We apply caching for both approaches.
In the case of averaging, we cache the average of the ranking function output over 10 evaluations.
For the statistical evaluation we cache the individual outcomes of the 10 evaluations.
Whenever two architectures are statistically compared, we store the outcome, so that the procedure does not need to be repeated.

Table~\ref{tab:evosearch_results} presents the evolutionary search results on the NAS-Bench and TransNAS-Bench search spaces.
We demonstrate results on Eigenvalue score and ReLU Hamming distance. 
We also include an ensemble of those that is formulated as a sum with components individually MinMax normalized over the observed architecture ranks.
We observe that statistical evaluation consistently improves the search quality.
Although the particular improvement depends on the combination of the search space and ranking function, we did not observe a case where applying a statistical evaluation leads to a poorer search result.
We do not observe a significant difference between the improvement for low- versus high-variance ranking functions.
In our experiments with evolutionary search, Eigenvalue score (low variance) has the average improvement of 0.73, whereas for RELU dist (high variance) the average improvement is 0.60.
We observed that FreeREA demonstrates the best results in most cases, thus emphasising the importance of crossover in evolutionary search.
However, in the averaging case the difference between FreeREA and REA is typically less than one standard deviation.
This may make REA more suitable for some practical applications as it requires only to define a mutation operation.
FreeREA experiences the largest performance increase with statistical evaluation, and clearly outperforms REA.
In some cases, we observed that the greedy algorithm outperforms the  others. It is not an expected outcome, but we assume it is due to randomness of the experimental setup.
Similar to other zero-shot works~\citep{cavagnero2023freerea,wang2024mednas}, our experiments demonstrate that evolutionary search is more stable and efficient than random search.
The only search space where random and evolutionary search exhibit similar performance is TransNAS-Bench-101, and we attribute this to the extremely small size of this search space (see Appendix~\ref{sec:benchmarks}).


In Table~\ref{tab:add_evo_small} we demonstrate that our approach is generalisable to more recently proposed ranking functions.
SWAPNAS~\citep{peng2024swapnas} and MeCo~\citep{jiang2024meco} are Zero-Shot ranking functions that aim to achieve the highest correlation with the accuracy of the architecture in the NAS-Bench search spaces.
Table~\ref{tab:add_evo_small} shows that the proposed methodology can easily be adapted to the new ranking functions.
In addition, we provide results on data-agnostic SynFlow~\citep{abdelfattah2021zerocost} and LogSynFlow~\citep{cavagnero2023freerea}.
Unlike any other ranking functions presented in this paper, the variation of SynFlow and LogSynFlow is induced only by random initialisation of the evaluated architecture.
The data-agnostic approaches present a challenge for our statistical evaluation.
Averaging demonstrates performance comparable or higher to that of our statistical evaluation on them.
More results for SWAPNAS, MeCo, SynFlow and LogSynFlow are demonstrated in the Appendix~\ref{sec:additional_experiments}.




\section{Conclusion and Future work}

In this work, we proposed a statistical approach for comparing architectures during a NAS zero-shot search procedure.
Through experiments using both random and evolutionary search, we provided evidence that the proposed approach leads to better search results than simply averaging the outcomes of a batch of evaluations. 
We also investigated the variation of a wide selection of zero-shot ranking functions for several search spaces and observed that those which exhibit less variability tend to produce better and more stable search outcomes.
% Our results also illustrated that caching is beneficial to search, likely because it leads to committing to decisions, allowing for a more efficient search over more architectures.
Given these observations, we can recommend that zero-shot search is conducted using statistical architecture comparison.

Every ranking function evaluation for a given architecture can be considered equivalent to sampling from the distribution.
In our work, we handled such sampling with statistical testing in order to retain compatibility with the existing pipelines and search agorithms.
Formulating a zero-shot neural architecture search as a stochastic optimisation problem that interprets the output of a ranking function not as a score, but as a distribution could be a potential research direction.

We have observed that the statistical method does not consistently benefitial for data-agnostic SynFlow and LogSynFlow, while their performance is competitive with other ranking functions discussed in the paper.
This suggests that an alternative strategy to the statistical approach is to develop techniques that are minimally affected by the data batch and initialisation.

An additional important source of performance variability that we have not discussed is the search space.
Our experiments show that the suitability of a ranking function is highly dependent on the search space.
For one search space, a particular ranking function might be the best choice; on a different search space, it might perform poorly.
Due to this variability, we cannot predict how our conclusions will generalise to a new search space.

% Some of the previous works have demonstrated that ensembles of ranking functions can lead to improved performance over a single ranking function.
% We restrict our experimentation to single ranking functions decrease the amount of experiments we have to conduct.
% Typically an ensemble of ranking functions is implemented as a weighted sum. 
% Therefore, while we did not perform experiments on the ensembles, our statistical evaluation framework is fully compatible with ensembles of ranking functions.




% Limitations: Previous work has demonstrated that ensembles of ranking functions can lead to improved performance. 
% We restrict our experimentation to single ranking functions so that the ensembling effect does not interfere with our investigation of the impact of variability and the value of statistical comparison. 
% However, in practice, it is likely that we may choose to use an ensemble, and this places a limit on our conclusions.
% Our approach is readily extendible to an ensemble if the ranking functions are combined into a single
% scalar output. 
% \MJC{Pavel: do you have a reference for this - how are
% multiple ranking functions usually combined?}
% \Pavel{I don't have a good reference for that. I have not seen any papers focused on a combination of rankers. Also no paper calls it "ensemble". But this extension itself is very trivial, because a sum of normalised ranking functions can be used as a ranking function.}
% \Pavel{I don't see it as a major novelty point.}

% There is an additional important source of performance variability that we have not discussed -- the search space.
% Our experiments show that the suitability of a ranking function is highly dependent on the search space.
% For one search space, a particular ranking function might be the best choice; on a different search space, it might perform poorly.
% Due to this variability, we cannot be sure that our results and conclusions will generalize to a new search space.
% \Pavel{Maybe this is not the best sentence to end the conclusion. It reads like all the previous experiments are wrong.}
% \Pavel{This paragraph should be switched with the previous and be the main limitation.}

\bibliography{biblio}
\bibliographystyle{tmlr}

\appendix

\section{NAS search space benchmarks}
\label{sec:benchmarks}

\textbf{NAS-Bench-101}~\cite{ying2019bench} includes the performance and training statistic of the 423k architectures on CIFAR-10.
It has three cells.
Each cell containing 7 nodes each with a pool of operations consists of: ``3x3 conv'', ``1x1 conv'' and ``3x3 max-pooling''.

\textbf{NAS-Bench-201}~\citep{dong2019bench} has 15625 architectures in the search space and provides performance for CIFAR-10, CIFAR-100, and ImageNet-16-120. 
It has 5 cells.
Every cell has 4 nodes.
The list of operations includes ``zeroize'', which is equivalent to cutting the connection, ``skip connection'', ``3x3 conv'', ``1x1 conv'' and ``3x3 avg-pooling''.
This is the most popular benchmark as it is relatively small and contains measurements for several datasets.
\textbf{NAS-Bench-360}~\citep{tu2022bench} extends this benchmark by additional datasets.
In this work, we are using the NinaPro DB5 dataset from NAS-Bench-360.
For consistency, we refer to it as NAS-Bench-201~(NinaPro).

% \textbf{NAS-Bench-301}~\cite{siems2020bench} is a surrogate search space approximating some regions of DARTS~\cite{liu2018darts} search space around approximately 60k anchor architectures.
% It is defined for CIFAR-10 dataset.
% As this benchmark does not provide information on most of the architectures in the search space, we use it only in experiments that does not require information on neighbourhood.

\textbf{TransNAS-Bench-101}~\citep{duan2021transnas} is a tabular benchmark that consists of two search spaces evaluated on seven tasks.
The architecture of the neural network in the benchmark is a composition of a searchable encoder and a decoder tailored to specific tasks.
The macro search space suggests searching for the number of blocks, the size for downsampling, and the count of output channels per block.
The micro search space is composed of four nodes, similarly to NAS-Bench-201.
Four operations are used: ``zeroize'', ``skip connection'', ``3x3 conv block'' and ``1x1 conv block'', where every convolution block is a sequence of ReLU, convolution, and batch normalisation.
In our project, we are only using the micro search space that consists of 4096 graphs.


\section{Evolutionary search algorithms}
\label{sec:greedy_evo_search}

\begin{algorithm}
\caption{Regularised evolutionary algorithm~(REA)} \label{alg:rea_search}
\begin{algorithmic}[1]
    \State $population\gets \emptyset$
    \State $history\gets \emptyset$
    \While{$\lvert population\rvert < P$}
        \Comment{Initialize population of size $P$}
        \State $arch\gets $ \Call{ArchSamle}{$ $}
        \Comment{Randomly sample architecture}
        \State $rank\gets $ \Call{EvalArch}{$arch$}
        \Comment{Evaluate ranking function for architecture}
        \State $history\gets history \cup \{(arch, rank)\}$
        \State $population\gets population \cup \{(arch, rank)\}$
    \EndWhile
    \While{$\lvert history\rvert < C$} 
        \Comment{Evolve for $C$ cycles} \label{alg:rea_search:budget_line}
        \State $random\_candidates\gets \emptyset$
        \For{$i\gets 1, S$}
            \State $candidate\gets $ \Call{Random}{population}
            \Comment{Sample random element from population}
            \State $random\_candidates\gets random\_candidates \cup \{candidate\}$
        \EndFor
        \State $parent\gets $ highest rank candidate out of $random\_candidates$
        \State $child\gets $ \Call{Mutate}{$parent$}
        \Comment{Sample one of all possible mutations}
        \State $rank\gets $ \Call{EvalArch}{$child$}
        \Comment{Evaluate ranking function for the child}
        \State $history\gets history \cup \{(child, rank)\}$
        \State $population\gets population \cup \{(child, rank)\}$\label{alg:rea_search:history_line}
        \State remove the oldest member of the population
    \EndWhile \label{alg:rea_search:while_line}
    \State $best\gets $ highest rank candidate out of $history$
    \State \textbf{return} $best$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Greedy evolutionary search algorithm difference with~\Algref{alg:rea_search}} \label{alg:greedy_evo_search}
\begin{algorithmic}[1]
\makeatletter
\setcounter{ALG@line}{14}
\makeatother
    \Statex $\ldots$
    \State $parent\gets $ highest rank candidate out of $random\_candidates$
    \State $children\gets $ \Call{MutateAll}{$parent$}
    \Comment{Get list of all possible mutations}
    \State $child\gets $ highest rank candidate out of $children$
    \State $rank\gets $ \Call{EvalArch}{$child$} 
    \Comment{Evaluate all children and get the highest rank one}
    \State add all mutations and their respective ranks to the $history$
    \Statex /* Proceed to line~\ref{alg:rea_search:history_line} of~\Algref{alg:rea_search} */
    \Statex $\ldots$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Free regularised evolutionary algorithm~(FreeREA) difference with~\Algref{alg:rea_search}} \label{alg:freerea_search}
\begin{algorithmic}[1]
\makeatletter
\setcounter{ALG@line}{14}
\makeatother
    \Statex $\ldots$
    \State $parent_1, parent_2\gets $ top-2 highest rank candidates out of $random\_candidates$
    \State $child_1\gets $ \Call{Mutate}{$parent_1$}
    \State $rank_1\gets $ \Call{EvalArch}{$child_1$}
    \State $child_2\gets $ \Call{Mutate}{$parent_2$}
    \State $rank_2\gets $ \Call{EvalArch}{$child_2$}
    \State $child_3\gets $ \Call{Crossover}{$parent_1, parent_2$}
    \Comment{Perform crossover between two parents' genes}
    \State $rank_3\gets $ \Call{EvalArch}{$child_3$}
    \State $history\gets history \cup \{(child_1, rank_1), (child_2, rank_2), (child_3, rank_3)\}$
    \State $population\gets population \cup \{(child_1, rank_1), (child_2, rank_2), (child_3, rank_3)\}$
    \State remove top-3 the oldest member of the population
    \Statex /* Proceed to line~\ref{alg:rea_search:while_line} of~\Algref{alg:rea_search} */
    \Statex $\ldots$
\end{algorithmic}
\end{algorithm}


Here we present pseudocode for all evolutionary algorithms used in this project.
Since the greedy evolutionary algorithm and FreeREA are primarily based on REA, we provide full pseudocode for REA~(see \Algref{alg:rea_search}) and parts that are different for FreeREA~(see \Algref{alg:freerea_search}) and greedy evolutionary search~(see \Algref{alg:greedy_evo_search}).
The core notation for all the algorithms is chosen to be close to REA notation, as presented by \citet{real2019regularized}.
For the standard implementation, the highest ranked candidate and the top-2 are computed by maximizing over the averaged ranking function output.
For our implementation, we use statistical comparison with stat-max and stat-top-2 from~\Algref{alg:stat_max_topk}.

We designed a greedy evolutionary search algorithm in order to further demonstrate how ranking function variance influences evolutionary search algorithm performance.
The primary difference between greedy evolutionary search and REA lies in the mutation operation.
Instead of randomly mutating a parent candidate as in REA, it greedily explores a list of all possible mutations.
Therefore, it is expected to experience a greater impact from the variance introduced by a ranking function.

In all our experiments, we are utilising an evaluation budget ($C$ at line~\ref{alg:rea_search:budget_line} of \Algref{alg:rea_search}) that defines how many architectures can be evaluated in total.
As one can see, the three evolutionary algorithms are spending the budget at different rates.
REA evaluates a single candidate per iteration.
FreeREA evaluates three and greedy search evaluates all the mutations (i.e., the neighbourhood on the search hypergraph).


\section{Additional experiments with ranking functions}
\label{sec:additional_experiments}

\begin{table}
    \centering
    \caption{Evolutionary search on SWAPNAS and MeCo. Results with averaging (Avg) and statistical (Stat) evaluation are provided. Mann-Whitney U-test is used for the statistical evaluation. Each ranking function is evaluated 10 times for the architecture. For each evaluation pair, the best score is highlighted in \textbf{bold}; if the higher score is statistically significant with p-value below 0.05, it is highlighted in \textbf{\underline{bold underlined}}.}
    \label{tab:add_evo_swapnas_meco}
\begin{tabular}{l|l|c|c|c|c}
\hline
\multirow{2}{*}{Search Space} & \multirow{2}{*}{Search Alg.} & \multicolumn{2}{c|}{SWAPNAS} & \multicolumn{2}{c}{MeCo} \\ \cline{3-6} 
 &  & Avg & Stat & Avg & Stat \\ \hline
\multirow{2}{*}{\shortstack[l]{NAS-Bench-101 \\(CIFAR-10)}} & REA & \multicolumn{1}{c|}{93.05$\pm$0.12} & \textbf{\underline{93.37$\pm$0.11}} & \multicolumn{1}{c|}{93.13$\pm$0.54} & \textbf{93.52$\pm$0.44} \\
 & FreeREA & 93.42$\pm$0.36 & \textbf{93.71$\pm$0.32} & 93.21$\pm$0.45 & \textbf{93.51$\pm$0.39} \\ \hline
\multirow{2}{*}{\shortstack[l]{NAS-Bench-201 \\(CIFAR-100)}} & REA & 71.77$\pm$0.49 & \textbf{72.54$\pm$0.56} & 71.31$\pm$1.37 & \textbf{71.54$\pm$1.11} \\
 & FreeREA & 71.70$\pm$1.02 & \textbf{72.81$\pm$0.98} & \textbf{71.86$\pm$0.87} & 71.86$\pm$0.99 \\ \hline
\multirow{2}{*}{\shortstack[l]{NAS-Bench-201 \\(ImageNet16-120)}} & REA & 44.39$\pm$0.98 & \textbf{45.74$\pm$1.22} & 42.47$\pm$0.84 & \textbf{42.81$\pm$0.81} \\
 & FreeREA & 44.75$\pm$1.15 & \underline{\textbf{45.89$\pm$1.01}} & 42.88$\pm$0.79 & \textbf{43.23$\pm$0.85} \\ \hline
\multirow{2}{*}{\shortstack[l]{TransNAS-Bench-101 \\(Jigsaw)}} & REA & 92.36$\pm$0.44 & \underline{\textbf{93.17$\pm$0.30}} & 92.43$\pm$0.14 & \underline{\textbf{93.64$\pm$0.13}}  \\ 
 & FreeREA & 92.29$\pm$0.18 & \underline{\textbf{93.04$\pm$0.21}} & 91.68$\pm$0.20 & \underline{\textbf{93.55$\pm$0.09}} \\ \hline
\multirow{2}{*}{\shortstack[l]{TransNAS-Bench-101 \\(Class object)}} & REA & 45.96$\pm$0.11 & \underline{\textbf{46.80$\pm$0.12}} & 46.73$\pm$0.09 & \underline{\textbf{46.99$\pm$0.08}} \\
 & FreeREA & 47.01$\pm$0.07 & \textbf{47.09$\pm$0.05} & 46.60$\pm$0.10 & \underline{\textbf{47.11$\pm$0.12}} \\ \hline
\end{tabular}
\end{table}
 

\begin{table}
    \centering
    \caption{Evolutionary search on SynFlow and LogSynFlow. Results with averaging (Avg) and statistical (Stat) evaluation are provided. Mann-Whitney U-test is used for the statistical evaluation. Each ranking function is evaluated 10 times for the architecture. For each evaluation pair, the best score is highlighted in \textbf{bold}; if the higher score is statistically significant with p-value below 0.05, it is highlighted in \textbf{\underline{bold underlined}}.}
    \label{tab:add_evo_synflow}
\begin{tabular}{l|l|c|c|c|c}
\hline
\multirow{2}{*}{Search Space} & \multirow{2}{*}{Search Alg.} & \multicolumn{2}{c|}{SynFlow} & \multicolumn{2}{c|}{LogSynFlow} \\ \cline{3-6} 
 &  & \multicolumn{1}{c|}{Avg} & \multicolumn{1}{c|}{Stat} & \multicolumn{1}{c|}{Avg} & \multicolumn{1}{c}{Stat} \\ \hline
\multirow{2}{*}{\shortstack[l]{NAS-Bench-101 \\(CIFAR-10)}} & REA & \textbf{93.69$\pm$0.38} & 93.56$\pm$0.31 & \textbf{93.76$\pm$0.36} & 93.62$\pm$0.28 \\
 & FreeREA & \textbf{93.80$\pm$0.24} & 93.74$\pm$0.32 & 93.76$\pm$0.25 & \textbf{93.77$\pm$0.36} \\ \hline
\multirow{2}{*}{\shortstack[l]{NAS-Bench-201 \\(CIFAR-100)}} & REA & \textbf{71.52$\pm$0.50} & 71.34$\pm$0.47 & \textbf{71.94$\pm$0.50} & 71.53$\pm$0.62 \\
 & FreeREA & \textbf{71.62$\pm$0.73} & 72.46$\pm$1.04 & \textbf{72.23$\pm$0.68} & 73.14$\pm$0.39 \\ \hline
\multirow{2}{*}{\shortstack[l]{NAS-Bench-201 \\(ImageNet16-120)}} & REA & 44.29$\pm$1.02 & \textbf{44.57$\pm$0.99} & \textbf{45.03$\pm$0.82} & 44.91$\pm$1.04 \\
 & FreeREA & \textbf{44.98$\pm$0.89} & 44.74$\pm$1.21 & 45.06$\pm$0.90 & \textbf{45.16$\pm$0.95} \\ \hline
\multirow{2}{*}{\shortstack[l]{TransNAS-Bench-101 \\(Jigsaw)}} & REA & 91.84$\pm$0.17 & \textbf{\underline{92.71$\pm$0.41}} & 91.92$\pm$0.33 & \textbf{\underline{92.49$\pm$0.28}} \\
 & FreeREA & 91.46$\pm$0.25 & \textbf{\underline{92.22$\pm$0.53}} & 91.46$\pm$0.20 & \textbf{91.89$\pm$0.62} \\ \hline
\multirow{2}{*}{\shortstack[l]{TransNAS-Bench-101 \\(Class object)}} & REA & 46.51$\pm$0.03 & \textbf{\underline{46.82$\pm$0.10}} & 46.50$\pm$0.06 & \textbf{\underline{46.82$\pm$0.10}} \\
 & FreeREA & 46.75$\pm$0.07 & \textbf{\underline{47.12$\pm$0.05}} & 46.66$\pm$0.04 & \textbf{\underline{47.03$\pm$0.09}} \\ \hline
\end{tabular}
\end{table}

In this section, we present an evaluation with SWAPNAS~\citep{peng2024swapnas}, MeCo~\citep{jiang2024meco}, SynFlow~\citep{abdelfattah2021zerocost} and LogSynFlow~\citep{cavagnero2023freerea}.
SWAPNAS and MeCo are recent Zero-Shot ranking functions that aim for the highest correlation with the architecture accuracy on the NAS-Bench search spaces.
SynFlow and LogSynFlow are data-agnostic ranking functions.
Due to constant input, SynFlow and LogSynFlow have no randomness induced by the input, therefore, resulting in less overall variation.
Note, however, that there is still variation due to the initialization of the architecture.
We present evolutionary search results for averaging over $V=10$ evaluations and the batch size $B=64$ for SWAPNAS and MeCo.
We used REA and FreeREA evolutionary search algorithms for these experiments.
For statistical evaluation with \Algref{alg:stat_max_topk} the Mann-Whitney U-test is used with the confidence threshold set to 0.05.
In Table~\ref{tab:add_evo_swapnas_meco} we present the means and standard deviations for SWAPNAS and MeCo calculated over 10 runs.
These results show that the proposed methodology can easily be adapted to recently published ranking functions that exhibit high correlation with architectural accuracy. The proposed statistical approach outperforms in all but one setting, with statistical significance in several cases.
In Table~\ref{tab:add_evo_synflow} we present the means and standard deviations for SynFlow and LogSynFlow calculated over 10 runs.

We see that the statistical approach is not consistently advantageous for these data-agnostic approaches.
This is in part due to the reduction in variability by the removal of dependence on the data batch.
A secondary factor is that with the existing (author-provided) implementations of SynFlow and LogSynFlow we observe occasional dramatic outliers due to gradient explosion for some initializations.
This could potentially be addressed by an alternative implementation.
When these outliers arise, both the statistical and averaging approaches can become equivalent to a random architecture selection, inhibiting the effectiveness of the search.
Since SynFlow and LogSynFlow are competitive with other ranking functions that we present in the paper, this suggests that an alternative strategy to the one we present in this paper is to develop techniques that are minimally affected (or even not affected) by the data batch and initialisation.

\section{Coefficient of variations as a ranking function}
\label{sec:cv_as_ranker}

\begin{table}
    \centering
    \caption{Random search on $N$ sampled architectures; coefficient of variation~(CV) is used as a ranking function. Statistical evaluation of the ranking function (Stat) is provided for the reference. Each ranking function is evaluated 10 times for the architecture on a batch of 64. We provide mean values and standard deviations over 100 runs. Each ranking function is evaluated 10 times for the architecture. For each pair of results, the best score is highlighted in \textbf{bold}; if the higher score is statistically significant with p-value below 0.05, it is highlighted in \textbf{\underline{bold underlined}}.}\label{tab:cv_as_ranker}
    \footnotesize\begin{tabular}{l|c|c|c|c|c|c} \hline
    \multirow{2}{*}{Search Space} & \multicolumn{2}{c|}{Eigenvalue score} & \multicolumn{2}{c|}{ReLU dist} & \multicolumn{2}{c}{NTK(cond)} \\ \cline{2-7}
     & CV & Stat & CV & Stat & CV & Stat \\ 
\hline \multicolumn{7}{c}{\textbf{N$ = 10$}} \\ \hline
NAS-Bench-101 (CIFAR-10) & 91.30$\pm$3.04 & \textbf{91.71$\pm$2.22} & \textbf{91.74$\pm$1.74} & 91.63$\pm$1.51 & 91.15$\pm$1.68 & \textbf{91.24$\pm$1.63} \\
NAS-Bench-201 (ImageNet16-120) & 39.58$\pm$6.11 & \textbf{41.22$\pm$4.56} & 37.02$\pm$5.80 & \textbf{\underline{41.24$\pm$3.33}} & \textbf{42.59$\pm$4.73} & 41.64$\pm$4.76 \\
TransNAS-Bench-101 (Class object) & 47.24$\pm$5.00 & \textbf{48.26$\pm$2.84} & \textbf{48.72$\pm$2.32} & 48.71$\pm$2.69 & 46.72$\pm$5.44 & \textbf{\underline{48.48$\pm$2.46}} \\
\hline \multicolumn{7}{c}{\textbf{N$ = 100$}} \\ \hline
NAS-Bench-101 (CIFAR-10) & 91.36$\pm$1.56 & \textbf{92.06$\pm$1.15} & 91.61$\pm$2.68 & \textbf{91.98$\pm$2.01} & \textbf{90.18$\pm$1.83} & 90.06$\pm$1.98 \\
NAS-Bench-201 (ImageNet16-120) & 35.98$\pm$5.59 & \textbf{\underline{40.96$\pm$4.98}} & 35.97$\pm$5.92 & \textbf{\underline{43.34$\pm$2.47}} & 38.21$\pm$5.29 & \textbf{41.50$\pm$4.42} \\
TransNAS-Bench-101 (Class object) & 45.27$\pm$6.20 & \textbf{49.46$\pm$4.56} & 45.30$\pm$6.37 & \textbf{\underline{48.87$\pm$2.96}} & 48.24$\pm$2.77 & \textbf{49.81$\pm$1.77} \\
\hline \multicolumn{7}{c}{\textbf{N$ = 1000$}} \\ \hline
NAS-Bench-101 (CIFAR-10) & 91.21$\pm$1.51 & \textbf{91.57$\pm$1.31} & \textbf{92.64$\pm$1.51 }& 92.60$\pm$1.73 & 89.04$\pm$1.89 & \textbf{\underline{90.92$\pm$2.40}} \\
NAS-Bench-201 (ImageNet16-120) & 35.26$\pm$5.50 & \textbf{\underline{42.17$\pm$4.11}} & 35.51$\pm$5.35 & \textbf{\underline{43.79$\pm$2.92}} & 35.20$\pm$5.69 & \textbf{\underline{42.24$\pm$2.03}} \\
TransNAS-Bench-101 (Class object) & 44.79$\pm$6.72 & \textbf{46.46$\pm$4.21} & 47.39$\pm$3.83 & \textbf{47.82$\pm$5.97} & 45.20$\pm$6.40 & \textbf{\underline{49.36$\pm$0.89}} \\
\hline
        \end{tabular}
\end{table}

% \begin{table}
%     \centering
%     \caption{Random search on $N$ sampled architectures, coefficient of variation~(CV) is used as a ranking function. Averaged output of the ranking function (Avg) is provided for the reference. Each ranking function is evaluated 10 times for the architecture on a batch of 64. We provide mean values and standard deviations over 100 runs.}\label{tab:cv_as_ranker}
%     \footnotesize\begin{tabular}{l|l|c|c|c} \hline
%      \multirow{2}{*}{Ranking function} &  & NAS-Bench-101 & NAS-Bench-201 & NAS-Bench-201 \\
%       & & (CIFAR-10) & (CIFAR-10) & (ImageNet16-120) \\ \hline
     
%     \multirow{2}{*}{Eigenvalue score } & Avg & & & \\
%      & CV & & & \\ \hline

%     \multirow{2}{*}{ReLU dist} & Avg & & & \\
%      & CV & & & \\ \hline
     
%     \multirow{2}{*}{NTK(cond)} & Avg & & & \\
%      & CV & & & \\ \hline

%     \end{tabular}
% \end{table}

A naive way to incorporate a coefficient of variation into the search process is to utilise it as a ranking function.
Given a ranking function $r(arch, d)$ we construct a coefficient of variation estimator as
\begin{equation}
     r_{CV}(arch,\mathcal{D}) = Mean(\mathcal{M}) + CV(\mathcal{M}) \quad \mathrm{for} \quad \mathcal{M} = \{r(arch, d_v)\}_{v=1}^V\,, 
\label{eq:rcv}
\end{equation}
where $d_v \in \mathcal{D}$ is a batch of data and $CV(\mathcal{X})=\frac{Var(\mathcal{X})}{Mean(\mathcal{X})}$.

As shown in Fig.~\ref{fig:cv_correaltion}, the ranking function coefficient of variation exhibits a considerable Kendall-$\tau$ correlation with accuracy. 
Moreover, in almost all cases this correlation is negative, which makes coefficient of variation a suitable ranking function for many metrics.

To explore this further, we conduct an experiment for several search spaces where we use random search and sample $N$ architectures.
To stabilize search performance, we use Min-Max normalization individually for the mean term and the CV term when computing $r_{CV}$ in~\eqref{eq:rcv}.

We demonstrate some results in Table~\ref{tab:cv_as_ranker}.
Although in some cases on NAS-Bench-101 $r_{CV}$ exhibits performance comparable to statistical evaluation, its performance is consistently worse on other search spaces, in some cases by a considerable margin.
In many cases, modification of the ranking function is harmful to performance compared to simply using the mean.
We attribute this to the correlation between the coefficient of variation and a ranking function averaged value (see  Fig.~\ref{fig:cv_mean_score_correaltion}).
We have noticed that the Kendall-$\tau$ correlation between the coefficient of variation of a ranking function and its mean value is the highest for the low-variance ranking functions that are the most suitable for zero-shot search (see  Fig.~\ref{fig:cv_mean_score_correaltion}).
For those ranking functions, the coefficient of variation does not provide much additional useful information for the search, but can introduce additional variability, thus making the results inconsistent.

% Fig.~\ref{fig:cv_mean_score_correaltion} depicts the the Kendall-$\tau$ correlation between the coefficient of variation of a ranking function and its mean value for different search spaces.
% For some ranking functions such as Eigenvalue score and ReLU distance, the magnitude of the correlation is high, meaning that the coefficient of variation does not provide much additional useful information for the search.
% Although a naive incorporation of the coefficient of variation can improve results of the search, its impact is inconsistent;


\section{Significance level ablation}

\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth, keepaspectratio]{imgs/plot_p_vals.pdf}
    \caption{Significance level ablation of the p-value threshold. The Eigenvalue score is used with FreeREA search algorithm. The accuracy is presented with respect to the accuracy of 0.05 threshold (see Table~\ref{tab:evosearch_results}).}
    \label{fig:p_vals_threshold}
\end{figure}

In the main body of the article we use the significance level of 0.05 for the \Algref{alg:stat_max_topk} threshold.
This value was selected as the most commonly used for statistical testing.
However, this threshold is a hyperparameter that can be optimised.
Fig.~\ref{fig:p_vals_threshold} shows that the optimal threshold lies within the range from 0.025 to 0.075.
The lower threshold seems to be excessively stringent, resulting in reduced performance.
Conversely, the higher threshold is overly lenient, permitting false cases of stochastic dominance.

\section{Caching in the evolutionary search}

\begin{table}
    \centering
    \caption{Accuracy comparison between cached rank mapping (Cached) and computation on-the-fly (On-the-fly). 
    Variation of ranking function influence on NAS-Bench-101/201. 
    We provide mean values and standard deviations over 10 runs. For each comparison pair, the best score is highlighted in \textbf{bold}; if the higher score is statistically significant with p-value below 0.05, it is highlighted in \textbf{\underline{bold underlined}}.}\label{tab:evosearch_caching}
    \footnotesize
    \begin{tabular}{l|l|c|c|c|c|c|c} \hline
    \multirow{2}{*}{Search Space} & \multirow{2}{*}{Search Alg.} & \multicolumn{3}{c|}{Eigenvalue score} & \multicolumn{3}{c}{ReLU dist} \\ \cline{3-8}
     & & Cached & Hybrid & On-the-fly & Cached & Hybrid & On-the-fly \\ \hline
\multirow{3}{*}{\shortstack[l]{NAS-Bench-201 \\(CIFAR-10)}} & Greedy & \textbf{92.76$\pm$0.90} & 92.12$\pm$1.18 & 92.30$\pm$0.90 & \textbf{92.36$\pm$0.31} & 91.65$\pm$4.29 & 91.91$\pm$3.85 \\
 & REA & \textbf{\underline{93.02$\pm$0.32}} & 92.56$\pm$2.86 & 91.98$\pm$4.11 & \textbf{\underline{93.15$\pm$0.15}} & 91.77$\pm$1.45 & 90.74$\pm$4.24 \\
 & FreeREA & \textbf{\underline{93.23$\pm$0.47}} & 92.51$\pm$1.43 & 92.43$\pm$2.09 & \textbf{93.29$\pm$0.58} & 91.94$\pm$4.02 & 91.27$\pm$7.50 \\ \hline
 
 
\multirow{3}{*}{\shortstack[l]{NAS-Bench-201 \\(CIFAR-100)}} & Greedy & \textbf{\underline{69.54$\pm$1.01}} & 67.21$\pm$1.48 & 65.42$\pm$1.37 & \textbf{\underline{70.00$\pm$0.03}} & 66.90$\pm$1.78 & 65.83$\pm$2.11 \\
 & REA & \textbf{\underline{71.54$\pm$0.80}} & 67.64$\pm$1.09 & 67.42$\pm$1.37 & \textbf{\underline{71.83$\pm$0.61}} & 67.95$\pm$1.07 & 68.02$\pm$1.50 \\
 & FreeREA & \textbf{\underline{71.98$\pm$0.26}} & 69.43$\pm$1.03 & 69.44$\pm$1.10 & \textbf{\underline{71.63$\pm$1.35}} & 68.44$\pm$1.40 & 68.89$\pm$1.42 \\ \hline
 
\multirow{3}{*}{\shortstack[l]{NAS-Bench-201 \\(ImageNet16-120)}} & Greedy & \textbf{\underline{42.68$\pm$1.06}} & 39.68$\pm$4.40 & 41.14$\pm$3.29 & \textbf{\underline{44.78$\pm$1.34}} & 41.75$\pm$5.62 & 42.54$\pm$4.75 \\
 & REA & \textbf{\underline{44.90$\pm$0.76}} & 43.51$\pm$4.22 & 43.55$\pm$4.71 & \textbf{43.82$\pm$1.53} & 42.12$\pm$2.48 & 41.34$\pm$7.19 \\
 & FreeREA & \textbf{\underline{44.84$\pm$3.17}} & 42.33$\pm$3.52 & 40.62$\pm$5.28 & \textbf{43.82$\pm$1.23} & 42.71$\pm$2.70 & 42.58$\pm$5.24 \\ \hline


%%%%%%%%%%%%% OLD TABLE %%%%%%%%%%%%%
    % \begin{tabular}{l|l|c|c|c|c|c|c} \hline
    %  \multirow{2}{*}{Search Space} & \multirow{2}{*}{Search Alg.} & \multicolumn{2}{c|}{Eigenvalue score} & \multicolumn{2}{c|}{ReLU dist} & \multicolumn{2}{c}{NTK(cond)} \\ \cline{3-8}
    %  & & Cached & On-the-fly & Cached & On-the-fly & Cached & On-the-fly \\ \hline
    % \multirow{3}{*}{\shortstack[l]{NAS-Bench-201 \\(CIFAR-10)}} & Greedy & \textbf{92.76$\pm$0.90} & 92.30$\pm$0.90  & \textbf{92.36$\pm$0.31} & 91.91$\pm$3.85 & \textbf{\underline{88.28$\pm$1.30}} & 86.85$\pm$0.26  \\
    %     & REA & \textbf{\underline{93.02$\pm$0.32}} & 91.98$\pm$4.11 & \textbf{\underline{93.15$\pm$0.15}} & 90.74$\pm$4.24 & \textbf{86.07$\pm$2.56} & 85.06$\pm$3.13 \\
    %     & FreeREA & \textbf{\underline{93.23$\pm$0.47}} & 92.43$\pm$2.09 & \textbf{93.29$\pm$0.58} & 91.27$\pm$7.50 & \textbf{\underline{87.65$\pm$1.80}} & 86.15$\pm$1.79 \\ \hline
    % \multirow{3}{*}{\shortstack[l]{NAS-Bench-201 \\(CIFAR-100)}} & Greedy & \textbf{\underline{69.54$\pm$1.01}} & 65.42$\pm$1.37 & \textbf{\underline{70.00$\pm$0.03}} & 65.83$\pm$2.11 & 63.32$\pm$1.89 &  \textbf{64.18$\pm$4.28} \\
    %     & REA & \textbf{\underline{71.54$\pm$0.80}} & 67.42$\pm$1.37 & \textbf{\underline{71.83$\pm$0.61}} & 68.02$\pm$1.50 & \textbf{\underline{68.75$\pm$4.71}} & 65.61$\pm$5.19 \\
    %     & FreeREA & \textbf{\underline{71.98$\pm$0.26}} & 69.44$\pm$1.10 & \textbf{\underline{71.63$\pm$1.35}} & 68.89$\pm$1.42 & \textbf{\underline{66.75$\pm$4.71}} & 63.43$\pm$4.65 \\ \hline
    % \multirow{3}{*}{\shortstack[l]{NAS-Bench-201 \\(ImageNet16-120)}} & Greedy & \textbf{\underline{42.68$\pm$1.06}} & 41.14$\pm$3.29 & \textbf{\underline{44.78$\pm$1.34}} & 42.54$\pm$4.75 & \textbf{\underline{40.83$\pm$3.30}} & 35.79$\pm$7.46 \\
    %     & REA & \textbf{\underline{44.90$\pm$0.76}} & 43.55$\pm$4.71 & \textbf{\underline{43.82$\pm$1.53}} & 41.34$\pm$7.19 & \textbf{\underline{41.95$\pm$10.23}} & 34.97$\pm$10.94 \\
    %     & FreeREA & \textbf{\underline{44.84$\pm$3.17}} & 40.62$\pm$5.28 & \textbf{43.82$\pm$1.23} & 42.58$\pm$5.24 & \textbf{\underline{39.51$\pm$8.03}} & 36.60$\pm$10.34\\ \hline

    \end{tabular}
\end{table}


We would like to demonstrate the importance of caching.
Unlike random search, where all architectures are sampled once, in evolutionary search, the same architecture can be encountered multiple times due to mutation or crossover. 
Due to the variations in the ranking function output, reevaluation of a previously encountered architecture can lead to a performance degradation in the search.
We demonstrate this in Table~\ref{tab:evosearch_caching}.
We report mean values and standard deviations for accuracies over 10 runs.
A randomly sampled batch of 64 with randomly initialised architecture weights is used to evaluate an architecture.
In the case of caching, we evaluate the architecture only once, when it is first encountered, and then save the ranking function output.
Subsequently, if the architecture is encountered again, the cached value of the ranking function is used in comparisons.
In the `on-the-fly' alternative, the architecture is reevaluated every time it is encountered.
We evaluate an architecture with a ranking function 10 times and average the outputs in both cases.
In addition, we introduce a 'hybrid' method where evaluations are accumulated.
The first time we encounter an architecture, we add 10 evaluations. 
Every subsequent time an architecture is encountered, three new evaluations are added.
Thus, our setup is similar to the most typical case of an evolutionary search.
To consistently compare evolutionary algorithms, we use an evaluation budget and cap it at 1000 evolution cycles.
We would like to highlight that we limit the number of evolution cycles and not the number of architecture evaluations (see line~\ref{alg:rea_search:while_line} of~\Algref{alg:rea_search}).
In this setting, REA has no exploration advantage over Greedy search, which evaluates significantly more architectures per cycle.
Both methods explore a similar number of architectures, because the search process of REA is more effective.

In addition to REA and FreeREA evolutionary searches, we also report results for a greedy evolutionary search.
It is similar to REA, but evaluates all possible mutations of the sampled exemplar architecture and selects the best according to the ranking function value.
We specially designed it to exclude the randomness introduced by the mutation process.
Greedy evolutionary search is only used for benchmarking purposes and is impractical in a real-world setting. See Appendix~\ref{sec:greedy_evo_search} for details.

We observe that caching, which is typically applied primarily with the motivation of reducing computational overhead, always improves the search performance.
Moreover, caching makes the results more stable in the majority of the cases, i.e., reduces the standard deviation.
We conjecture that this performance improvement arises because the use of caching means that previous search decisions are not overturned.
In the case of `on-the-fly' evaluation, the search can initially decide that an architecture is inferior, but then reverse that decision when the architecture is encountered a second time.
That can be clearly seen for a greedy evolutionary search, which is designed to maximise the number of re-evaluations.
The `hybrid' approach improves the stability, but results in poorer performance compared to the `cached' approach.
The efficiency of the search is improved by committing to a decision. The observed performance improvement is less for low-variance ranking functions, as their output is more consistent, leading to fewer changed decisions.
Table~\ref{tab:evosearch_caching} also shows how the selection of low variance ranking functions can significantly improve the quality of search results.


\section{Discussion on meaningful zero for ranking functions}
\label{sec:meaningful_zero}
In our work, a coefficient of variation (see \eqref{eq:variation}) was used to compare the variability of the commonly used ranking functions (see Fig.~\ref{fig:cv}).
The coefficient of variation requires that each ranking function has a meaningful zero.
In our work, we use the following ranking functions: Frobenius norm of NTK, mean value of NTK, Label-Gradient Alignment, Eigenvalue score, ReLU Hamming distance, condition number of NTK, EPE-NAS, Fisher, Snip, MeCo, SynFlow, and LogSynFlow.

We have assessed the concept of a meaningful zero for each ranking function listed above.
Here are our findings.
\begin{itemize}[leftmargin=*]

\item The Frobenius norm of NTK is always bigger than zero, and since it is a norm, it clearly has a meaningful zero.
\item The mean value of NTK is a mean of all elements for a positive semidefinite matrix. Since the NTK is positive-semidefinite~\citep{jacot2018neural}, the mean value is always non-negative, implying that there is a meaningful zero.

\item Label-Gradient Alignment is computed from the NTK $H \in \mathbb{R}^{B\times B}$ and label-alignment matrix $L \in \{0,1\}^{B\times B}$ (see~\citep{mok2022demystifying} for details). Since the NTK is positive-semidefinite, LGA is bounded from below. The metric has a meaningful zero.
%\begin{equation*}
%    \label{eq:nas_ntk_metrics:lga}
%    \text{LGA}(H) \!=\! \sum \frac{[H - \mu (H)] \cdot [L %- \mu(L)]}{\lVert  H - \mu (H)  \rVert_2 \lVert L - %\mu(L) \rVert_2}\,,
%\end{equation*}

\item The Eigenvalue score is computed using the eigenvalues of NTK $H$, which are all non-negative. The Eigenvalue score has a meaningful zero (corresponding to the extreme case of a zero-valued kernel).

%\begin{equation*}
%    \text{Eigenvalue score}(H) = -\sum_{\lambda \in %\text{eig}(H)} \log (\lambda) + \lambda^{-1}
%\end{equation*}
\item The ReLU Hamming distance is based on the comparison of ReLU activation patterns for different inputs. Since it is evaluated based on distances, it has a meaningful zero corresponding to the case when the activations match exactly for all inputs.

\item EPE-NAS score is the class-wise sum of absolute values, which is non-negative. It has a meaningful zero. 

\item The condition number of NTK is always positive and greater than or equal to one. Although the zero value cannot be achieved, it is a meaningful value in the sense that the zero point is not arbitrary. We can shift or scale the condition number and retain its fundamental meaning.

\item Fisher is a layer-wise sum over squares. It is non-negative and has a meaningful zero.

\item Snip is a layer-wise sum over absolute values that is non-negative and has a meaningful zero.

\item The implementations of both SynFlow and LogSynFlow use the layer-wise summation of absolute values. These values are non-negative, and they both have a meaningful zero.

\item MeCo is a layerwise sum of the smallest eigenvalues of Pearson correlation matrices for feature maps; as the matrices are positive-semidefinite by design (see~\citep{jiang2024meco} for details), MeCo is non-negative and has a meaningful zero.
\end{itemize}

\section{Assets}
\label{sec:asset}

In this work, we used the following software:
\begin{itemize}
    \item \textit{automl/NASLib}~\citep{mehta2022bench} available on GitHub under Apache 2.0 Licence
    \item \textit{PyTorch}~\citep{paszke2019pytorch} available via PyPI under a custom BSD licence
    \item \textit{NumPy}~\citep{harris2020array} available via PyPI under a custom BSD licence
    \item \textit{SciPy}~\citep{virtanen2020scipy} available via PyPI under a custom BSD licence
\end{itemize}

The following dataset were used:
\begin{itemize}
    \item \textit{CIFAR-10/100}~\citep{krizhevsky2009learning} under CC BY 4.0 Licence
    \item \textit{ImageNet-16-120}~\citep{chrabaszcz2017downsampled} under CC BY 4.0 Licence
    \item \textit{NinaPro}~\citep{atzori2012building} under CC BY-ND  Licence
    \item Five datasets from \textit{Taskonomy collection}~\citep{zamir2018taskonomy} under CC BY 4.0 Licence
\end{itemize}


%\section{Multiple Hypothe correction}

%\begin{table}
%    \centering
%    \caption{Statistical corrections. Mann-Whitney U-test is used for statistical evaluation. Each ranking function is evaluated 10 times for the architecture. We provide mean values and standard deviations over 1000 runs. ReLU Hamming distance is used for all experiments.}\label{tab:stat_correction}
%    \footnotesize\begin{tabular}{l|c|c|c|c} \hline
%    \multirow{2}{*}{Correction} & \multicolumn{2}{c|}{NAS-Bench-101~(CIFAR-10)} & \multicolumn{2}{c}{NAS-Bench-201~(ImageNet16-120) } \\ \cline{2-5}
%     & Acc & FER & Acc & FER  \\ 
%\hline \multicolumn{5}{c}{\textbf{N$ = 10$}} \\ \hline
%Avg & 91.28$\pm$1.80 & 0.117 & 40.69$\pm$4.09 & 0.086 \\
%No correction & 91.63$\pm$1.51 & 0.121 & 41.24$\pm$3.33 & 0.089 \\
%Bonferroni & 91.63$\pm$1.53 & 0.123 & 41.22$\pm$3.33 & 0.090 \\
%Benjamini–Hochberg & 91.63$\pm$1.53 & 0.122 & 41.25$\pm$3.33 & 0.090 \\
%Sidak-Holm & 91.61$\pm$1.53 & 0.122 & 41.24$\pm$3.33 & 0.090 \\
%Sidak & 91.64$\pm$1.52 & 0.123 & 41.25$\pm$3.34 & 0.090 \\

%\hline \multicolumn{5}{c}{\textbf{N$ = 100$}} \\ \hline
%Avg & 91.98$\pm$2.08 & 0.012 & 42.54$\pm$3.93 & 0.009 \\
%No correction & 91.98$\pm$2.01 & 0.013 & 43.34$\pm$2.47 & 0.013 \\
%Bonferroni & 91.99$\pm$2.00 & 0.015 & 43.40$\pm$2.41 & 0.013 \\
%Benjamini–Hochberg & 91.98$\pm$2.01 & 0.013 & 43.38$\pm$2.44 & 0.013 \\
%Sidak-Holm & 91.98$\pm$2.02 & 0.015 & 43.38$\pm$2.43 & 0.013 \\
%Sidak & 91.98$\pm$2.01 & 0.015 & 43.36$\pm$2.47 & 0.013 \\
%\hline
%    \end{tabular}
%\end{table}


%One can notice that our approach in \Algref{alg:stat_max_topk} is a typical representor of a multiple comparisons problem, where multiple statistical inferences occur simultaneously.
%Such problems require a threshold correction to reduce the number of false-positive errors.
%In our experiments, we did not observe any significant discrepancies between applying any form of correction and no correction at all; consequently, the correction is not included in \Algref{alg:stat_max_topk}. In a conventional hypothesis testing setting, there is usually a strong need to avoid false positives, because it can lead to incorrect scientific conclusions (e.g., this medication has a beneficial treatment effect). By contrast, in the search procedure, the effect of a false positive is not severe. If the search algorithm fails to detect a significant difference between the two architectures, it chooses one randomly anyway. 

%To illustrate this point, we present Table~\ref{tab:stat_correction} with random search results for the ReLU Hamming distance on NAS-Bench-101~(CIFAR-10) and NAS-Bench-201~(ImageNet16-120).
%The setup is similar to the random search experiment from \Secref{sec:random_search}, except that we run each experiment 1000 times and implement a threshold correction.
%We compare four candidate multi-hypothesis testing correction procedures: Bonferroni, Benjamini-Hochberg, Sidak-Holm, and Sidak. 
%Since the correction decreases the threshold, we also measure a first element rate~(FER), which measures the fraction of cases when the first architecture is selected (this is the default when the test fails for all competing architectures).
%Due to design of \Algref{alg:stat_max_topk}, a high FER would indicate that the corrected threshold is too small, and the overall procedure is too close to random sampling of an architecture from a given subset.

\end{document}