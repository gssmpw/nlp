\section{Introduction}
\label{sec:introduction}
%
Constitutive material modeling plays a crucial role in understanding the behavior of history-dependent materials, as it allows for accurate predictions of their response under various loading conditions. The selection of an appropriate material model requires extensive expertise, particularly when addressing both elastic and inelastic behaviors which are often strongly coupled. A flawed choice can lead to a focus on optimizing material parameters rather than identifying the most suitable model for the material's behavior.

It is important to recognize that the expertise of professionals is often related to their practical experience, which can lead to biases in the modeling process. This subjectivity has the potential to influence the selection of models and parameters, thereby constraining the exploration of alternative approaches.

In this context, neural networks which are grounded in physical principles -- whether strongly or weakly -- can provide significant assistance, besides model-free approaches \cite{EggersmannKirchdoerferEtAl2019,PRUME2023115704}. Such neural networks are particularly valuable in the field of finite deformations, where complexity is a major challenge. By using advanced computational methods, we can improve our ability to capture the complex responses of materials under different conditions, ultimately leading to more reliable and effective engineering solutions and paving the way for novel advanced materials.
%
%============================================
\subsection{State-of-the-art}
\label{sec:state_art}
%
\textbf{Neural networks for inelastic materials.} In recent years, the use of neural networks has surged across various scientific fields, particularly for analyzing large data sets beyond human analytical capabilities. For instance, \citet{kepner2019} demonstrate the societal impact of the internet using hypersparse neural networks on a data set of 50 billion packets.

In medical science, neural networks significantly enhance the understanding of complex brain connectivity essential for identifying neurodegenerative diseases, as shown in \cite{xu2023datadrivennetworkneurosciencedata} with data from 2,702 subjects. Additionally, \citet{girardi2018patientriskassessmentwarning} utilize an attention-based neural network trained on 600,000 medical notes to detect critical warning symptoms.
Moreover, \citet{ferle2024predictingprogressioneventsmultiple} combine Long Short-Term Memory networks with Conditional Restricted Boltzmann Machines to predict multiple myeloma events up to twelve months in advance, improving patient care.
In the field of mechanics, \citet{Nagle2024} trained a neural network on 1,000 individual virtual subjects to predict skin growth.

In continuum mechanics, access to extensive experimental data is often limited, prompting the question of how neural networks can be beneficial in this field. Expert-based neural network architectures offer distinct advantages, particularly when modeling the diverse behaviors of inelastic materials. The phenomena of interest, such as visco-elasticity and plasticity, exhibit variability based on factors like pressure-sensitivity.

Typically, experts evaluate experimental data to determine both the inelastic phenomenon and the corresponding constitutive model equations. These decisions are often influenced by prior experience, introducing bias. Given the complexity of constitutive equations, it is challenging for humans to formulate highly nonlinear relationships accurately.

Neural networks address this limitation by accommodating a wide range of material behaviors and approximating complex nonlinear functions with high accuracy through scaling their depth and width. Recent research has focused on integrating neural networks with machine learning to enhance modeling capabilities in continuum mechanics.

One of the pioneering thermodynamics-based networks was introduced in \cite{masi2021}, focusing specifically on inelastic multiscale modeling \cite{masi2022} and the inelastic evolution process \cite{masi2023}. Further advancements by \cite{Piunno2025} employed proper orthogonal decomposition to extract macroscopic internal state variables, thereby enriching the data set for training the network.
A hierarchical discovery framework that aligns with thermodynamic principles has been proposed in \cite{ZHANG2025106049}.

\citet{asad2023cmame} proposed a mechanics-informed neural network \cite{asad2022} aimed at elucidating the behavior of visco-elastic solids. Physics-augmented neural networks \cite{Klein_Roth_Valizadeh_Weeger_2023,KALINA2024116739} exploit fundamental principles from continuum mechanics, such as polyconvexity of the Helmholtz free energy, facilitating discoveries related to visco-elasticity \cite{rosenkranz2024} and thermo-elasticity \cite{fuhg2024polyconvexneuralnetworkmodels}. In this context, \cite{TAC2022115248} proposed a methodology that integrates deformation invariants with the architecture of neural ordinary differential equations.

To uncover inelastic materials characterized by an inelastic potential, the idea of input convex neural networks \cite{Amos2017} is particularly helpful due to their ability to ensure thermodynamically consistent designs. For example, physics-informed neural networks developed for elasto-viscoplasticity discovery align with this network architecture \cite{EGHTESAD2024104072}.

The unsupervised learning framework EUCLID incorporates Generalized Standard Materials (GSM) -- a method also used by \cite{Flaschel2024convex} -- into its architecture to autonomously identify plasticity under small strain conditions \cite{flaschel2022,flaschel2023}. This framework has recently been extended to address non-associative pressure-sensitivity under small strains \cite{Xu2025}. Additionally, \cite{upadhyay2024} utilized a dissipation potential based on the rate of the right Cauchy-Green tensor to characterize finite visco-elastic behavior within a GSM framework.

In scenarios involving damage, a built-in physics neural network was proposed by \cite{TAC2024102220}, while fracture problems were addressed through generalizable symbolic regression techniques in \cite{YI2025105916}. Moreover, a deep neural network capable of automatically locating and inserting regularized discontinuities for modeling brittle fractures was introduced in \cite{Baek2024}.

We adopt the approach established by Constitutive Artificial Neural Networks (CANNs) \cite{LinkaHillgaertnerEtAl2021,LinkaKuhl2023}, which were extended to visco-elasticity based on a Prony series in \cite{AbdolaziziLinkaEtAl2023}.
Recently, the idea of CANNs served as foundation for constitutive Kolmogorov Arnold networks \cite{abdolazizi2025constitutivekolmogorovarnoldnetworksckans}.
Its generalization to inelastic behavior (iCANN) under finite strains was proposed in \cite{holthusen2023,holthusen2023PAMM} and investigated in studies on visco-elasticity and elasto-plasticity with kinematic hardening \cite{boes2024arxiv}, as well as applications to biological tissue growth \cite{holthusen2025growth}. Although this custom-designed architecture inherently satisfies thermodynamic consistency for inelastic materials, it has certain limitations regarding scalability concerning depth and width and fails to incorporate pressure sensitivity effectively. 

Therefore, in this study, we explore methodologies for transitioning traditional feed-forward architectures onto the iCANN framework. Our novel architecture has similarities to recent contributions from \citet{JADOON2025117653}, who pertain to finite elasto-plasticity and share conceptual foundations with iCANNs. However, unlike  \citet{JADOON2025117653}, we fully custom-design our feed-forward network based on comprehensive discussions aimed at achieving a thermodynamically consistent yet generalized potential.

This overview of neural networks applied within the realm of thermodynamically consistent discovery of inelastic materials is not exhaustive; indeed, data-driven mechanics is an expanding field. Therefore, interested readers are encouraged to consult recent survey articles such as \cite{linden2023,watson2024arxiv,Fuhg2024}, which discuss methodologies for integrating physics into neural network architectures.\newline

\textbf{Discovery of inelastic neural networks.} As mentioned by \cite{Battalgazy2025}, different constitutive models can describe specific mechanical behaviors depending on parameter variations. 
The authors proposed a Bayesian-based procedure that combines model selection with parameter identification. 
Similarly, neural networks in continuum mechanics face challenges; as their architectures become more generalized, encompassing various inelastic phenomena, the complexity of the discovery process increases.

While highly generalizable, dense networks offer numerous advantages, they pose challenges in uniquely identifying the network's weights.
In this context, obtaining a sparse network during the discovery process is preferred over a dense one, as it can be considered `unique' to a certain extent. 
Regularizing the network's weights in thermodynamics-based architectures has proven to be a valuable tool in this regard \cite{mcculloch2024}.
This emphasis on sparsity not only enhances the modelâ€™s interpretability but also aligns with the objective of extracting meaningful insights from the underlying physical processes.

This consideration becomes increasingly critical in scenarios where data is subject to uncertainties \cite{LINKA2025117517}, as establishing a deterministic relationship between stresses and strains may often be unrealistic. 
Strategically, incrementally increasing the network's complexity could facilitate the identification of a unique network configuration \cite{linka2024} .

The challenge of creating a `rich' data set that enables optimal training of neural networks is gaining importance. 
On a structural level, this raises questions about how to design test specimens that maximize information extraction, which has been explored for one-shot identification in \cite{ghouli2025topologyoptimisationframeworkdesign}. 
Additionally, microstructural simulations serve as powerful tools for enriching macroscopic data sets \cite{PRUME2025117525}. 
In this regard, the work presented in \cite{ZHANG2024112661} provides methodologies for reconstructing microstructures from extremely limited data sets.

%============================================
\subsection{Hypothesis}
\label{sec:hypothesis}
%
We hypothesize that leveraging the depth and width of neural networks within a dual potential framework enables the modeling and discovery of a broad spectrum of inelastic material behaviors, including pressure-sensitive inelastic flow. By scaling the complexity of the neural network architecture, we aim to capture increasingly sophisticated material responses. Furthermore, we anticipate that utilizing sufficiently `rich' data sets will lead to accurate model development and reliable training outcomes.

In this initial phase of our work, we simplify the problem by neglecting both intrinsic and induced directional influences. Consequently, we do not incorporate preferred directions (i.e.\ anisotropy) in the formulation of the Helmholtz free energy, we disregard hardening effects, and we assume that the potential depends solely on the driving force.

To ensure full accessibility of the source code and to facilitate reproducibility, we implement the entire framework and training scheme using JAX \cite{jax2018github}.
%
%============================================
\subsection{Outline}
\label{sec:outline}
%
We begin with a description of the constitutive framework for inelastic materials at finite strains, applicable to a wide range of materials, in Section~\ref{sec:constitutive}.
There, we formulate the dual/pseudo potential in terms of stress invariants and comment on its connection to Generalized Standard Materials in Section~\ref{sec:GSM}.
In Section~\ref{sec:network}, we present the novel neural network architecture of the generalized dual potential embedded in a recurrent context, discussing time discretization schemes and our regularization approach.
Therefore, in Section~\ref{sec:convex_potential}, we briefly discuss mathematical properties to obtain a potential that satisfies thermodynamics a priori.
In Section~\ref{sec:resembling}, we show how well-known models, such as the \textit{von Mises} or \textit{Drucker-Prager} models, are included in the architecture.
Afterwards, in Section~\ref{sec:results}, we train our network on artificial and experimentally obtained data and evaluate its performance.
Our results are critically assessed in  Section~\ref{sec:discussion}, including a discussion on currently observed limitations of the approach.
Finally, in Section~\ref{sec:conclusion}, we conclude on the proposed method and outline possible future investigations.