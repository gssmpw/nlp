\section{Architecture of generalized iCANN}
\label{sec:network}
%
\textbf{Recurrent architecture.} History-dependent materials require sequential data processing. 
The iCANN framework integrates two independent feed-forward networks -- one for dual potential discovery and another for the Helmholtz free energy -- within a recurrent architecture. Although distinct, these networks are strongly coupled via their derivatives, governing inelastic stretch and thermodynamic driving force. 
Their architectures are detailed in Sections~\ref{sec:FFN_potential} and \ref{sec:FFN_energy}.
Schematic~\ref{fig:RNN} depicts the overall structure. 
As typical in recurrent networks, the current output, $\bm{S}$, depends not only on the current input, $\bm{C}$, but also on propagated states, $\bm{U}_i$, across sequential time data. 
The time integration scheme, whether implicit or explicit, relies on the time increment $\Delta t$. Explicit methods require propagating the previous stepâ€™s right Cauchy-Green tensor, $\bm{C}_n$, while implicit methods solve for inelastic stretch iteratively via an equality constraint.\newline

\textbf{Loss function.} Defining the loss function is critical for accurate neural network training. While neural networks effectively learn complex input-output relationships, their flexibility can lead to overfitting. To mitigate this, we apply weight regularization, a well-established method also effective in training Constitutive Artificial Neural Networks (CANNs) \cite{mcculloch2024,linka2024}.
For finite elasticity, studies have demonstrated successful regularization strategies for CANNs.
For inelasticity, this still poses an open research question.
In contrast to elasticity, where sparsity leads to a reasonably `unique' solution, inelasticity demands not only a sparse network but also the discovery of the underlying inelastic phenomena hidden in the data. 
Ideally, if the material behaves for instance purely elastically, the network should assign zero weights to the dual potential.

We define the loss as follows
%
\begin{equation}
    L(\bm{S};\mathbf{W},\mathbf{b}) = \frac{1}{n_{\text{exp}}} \sum_{\alpha=1}^{n_{\text{exp}}} \frac{1}{n_{\text{data}}} \sum_{\beta=1}^{n_{\text{data}}} \| \bm{S}_\beta'(\bm{C}_\beta,\bm{U}_{i_\beta};\mathbf{W},\mathbf{b}) - \hat{\bm{S}}_\beta' \|_2^2 + R_\psi(\mathbf{W}_\psi,\mathbf{b}_\psi) + R_{\omega^*}(\mathbf{W}_{\omega^*},\mathbf{b}_{\omega^*})
\label{eq:loss}
\end{equation}
%
where $(\bullet)'$ refers to Voigt's notation and $\| (\bullet) \|_2^2$ refers to computing the mean of the sum of squared element-wise differences, known as L\textsubscript{2} loss.
Further, $n_{\text{exp}}$ denotes the number of experiments used for training, while $n_{\text{data}}$ is the number of data points per experiment.
The experimentally measured stress is denoted by $\hat{\bm{S}}$.
In addition, $R_\psi$ and $R_{\omega^*}$ account for the regularization of the weights and biases of the feed-forward networks of the Helmholtz free energy and the dual potential, respectively.
These weights and biases are summarized in $\mathbf{W}$ and $\mathbf{b}$.\newline

\textbf{Time integration schemes.} To solve the evolution Equation~\eqref{eq:EvolutionEquationInvars} numerically, we employ two different discretization schemes for the interval $t\in [t_{n},t_{n+1}]$\footnote{For simplicity, the subscript $n+1$ is omitted in the following.}.
The exponential integrator map is well suited in the finite strain regime as it, for instance, preserves the volume in case of an absence of hydrostatic pressure.
Moreover, as shown in \cite{holthusen2025growth}, the exponential integrator satisfies that the isochoric invariants of $\bm{U}_i$ are preserved if the deviator of $\bar{\bm{\Sigma}}$ is equal to zero.

First, we employ an explicit time integration scheme \cite{holthusen2023} 
%
\begin{equation}
    \bm{C}_i = \bm{U}_{i_n}\,\mathrm{exp}\left(  2\, \Delta t\, \bar{\bm{D}}_{i_n} \right)\,\bm{U}_{i_n}, \quad \bm{U}_i = + \sqrt{\bm{C}_i}
\label{eq:explicit_integration}
\end{equation}
%
with $\Delta t := t_{n+1}-t_n$ denoting the time increment.
Although an explicit integration is numerically efficient as no iterative solution is required, it might be less stable than an implicit scheme.
Further, we have to compute the square root of $\bm{C}_i$.
As JAX does not provide an implementation of the matrix square root at the current time, we use the generating function proposed in \cite{hudobivnik2016}.
Due to these reasons, we additionally introduce the following residual of an implicit integration scheme, cf. \cite{arghavani2011a}, \cite{holthusen2023}
%
\begin{equation}
    \mathrm{log}\left(\bm{U}_i^{-1}\,\bm{C}_{i_n}\,\bm{U}_i^{-1}\right) + 2\,\Delta t\, \bar{\bm{D}}_i \overset{!}{=} \bm{0}
\label{eq:implicit_integration}
\end{equation}
%
where $\mathrm{log}\left( \bullet \right)$ refers to the matrix logarithm, which is again computed using a generating function \cite{hudobivnik2016}.
To solve Equation~\eqref{eq:implicit_integration} in an iterative manner, we employ Broyden's method (see \ref{app:broyden}).
%
\begin{figure}[h]
    \centering
    \input{figures/NN_rnn}
    \caption{Schematic illustrating two feed-forward networks (blue and orange) integrated within a recurrent architecture. The evolution equation for $\bm{U}_i$  can be solved either explicitly or implicitly. In the explicit approach, the state variable $\bm{C}_n$ is needed alongside $\bm{U}_{i_n}$. Conversely, the implicit integration method requires satisfying the evolution equation (equality constraint) but does not involve $\bm{C}_n$. The current stretch and time increment inputs $\bm{C}_{n+1}$ and $\Delta t$ yield the current stress $\bm{S}_{n+1}$, while the state variables are propagated through time.}
    \label{fig:RNN}
\end{figure}
%
%============================================================
\subsection{Feed-forward network: Dual potential}
\label{sec:FFN_potential}
%
Deep feed-forward neural networks, i.e.\ multiple hidden layers between in- and outputs, serve as universal approximators \cite{hornik1989} and can approximate functions to any desired degree of accuracy if complexity of the network is increased.
Multilayer networks achieve this by recursively applying linear transformations
%
\begin{equation}
    \mathbf{y}_{k+1} = \mathbf{W}_{k}^{k+1}\,\mathbf{x}_k + \mathbf{b}^{k+1}
\end{equation}
%
from one layer, $k$, to the next layer $k+1$.
Subsequently, a nonlinear activation function, $f$, is applied on the linear transformation
%
\begin{equation}
    \mathbf{x}_{k+1} = f(\mathbf{y}_{k+1}).
\end{equation}
%
This activation function can vary between layers and can even differ between neurons in the same layer, although this is not common.
In the following, we discuss the architecture of a generalized dual potential resulting in a convex, zero-valued, and non-negative function.
Specifically, we constrain the layer stacking, permissible activation functions per layer, and the domains of weights, $\mathbf{W}$, and biases, $\mathbf{b}$, between layers.
To this end, we briefly recapture some mathematical properties of convexity, zero-valueness, and non-negativity of composed functions in Section~\ref{sec:convex_potential}.
Our discussions inspire the design of the iCANN's dual potential in Section~\ref{sec:architecture_potential}, which is capable of recovering some classical potentials from the literature, see Section~\ref{sec:resembling}.
%
%============================================================
\subsubsection{Convex, zero-valued, and non-negative dual potential}
\label{sec:convex_potential}
%
To ensure a thermodynamically admissible evolution of inelastic stretches, the dual potential, $\omega^*$, must be convex, non-negative, and zero-valued with respect to its arguments under any arbitrary loading.
Since we aim to discover this potential using a multilayer feed-forward neural network, we mathematically outline how such a network can be constructed. Notably, the following considerations are not `if and only if' conditions -- alternative function compositions, for example, may also yield convexity but may probably not align with the proposed framework. However, designing the network according to these principles strictly ensures thermodynamic consistency.\newline

\textbf{\textit{Convexity:} Composition of a convex function and linear combination.} Suppose we have a function $g: \mathbb{R}^n \rightarrow \mathbb{R}$ (linear combination)
%
\begin{equation}
	g(x_1,\hdots,x_n) = \mathbf{a}^T \mathbf{x}, \quad \mathbf{x} = \begin{pmatrix}
	x_1 \\
	\vdots \\
	x_n
\end{pmatrix},\ \mathbf{a} \in \mathbb{R}^n
\label{eq:convex_linear_1}
\end{equation}
%
as well as $f: \mathbb{R} \rightarrow \mathbb{R}$ which is convex.
The composition $h(\mathbf{x}) = (f \circ g)(\mathbf{x})$ is convex, since the Hessian, $H_h(\mathbf{x})$, is a rank-1 matrix of the outer product
%
\begin{equation}
	H_h(\mathbf{x}) = \frac{\partial^2 f}{\partial g^2}\ \mathbf{a} \mathbf{a}^T.
\label{eq:convex_linear_2}
\end{equation}
%
Thus, its only non-zero eigenvalue is $\frac{\partial^2 f}{\partial g^2}\mathbf{a}^T \mathbf{a}$, which is greater or equal to zero since $\frac{\partial^2 f}{\partial g^2}\geq 0$ for a convex function $f$.\newline

\textbf{\textit{Convexity:} Positive sum of convex functions.} Let us introduce a family of convex functions $f_1,f_2,\hdots,f_n:\mathbb{R}^m \rightarrow \mathbb{R}$ and $g:\mathbb{R}^m\rightarrow \mathbb{R}$ be the positive sum of these functions, i.e.,
%
\begin{equation}
    g(\mathbf{x}) = \sum_{i=1}^n b_i\ f_i(\mathbf{x}), \quad \mathbf{x}\in\mathbb{R}^m, \quad b_i \geq 0
\label{eq:convex_positive_sum}
\end{equation}
%
then $g(\mathbf{x})$ is also convex, as each $f_i$ is convex and the positive scaling of convex functions preserves their convexity. 
Noteworthy, this includes the special case where $n=1$.\newline

\textbf{\textit{Convexity:} Composition of convex functions.} We introduce two convex functions $f:\mathbb{R}\rightarrow\mathbb{R}$ and $g:\mathbb{R}^n\rightarrow\mathbb{R}$. The composition of these functions, $h = (f \circ g)(\mathbf{x})$ with $\mathbf{x}\in\mathbb{R}^n$ is convex if the outer function, $f$, is monotonically increasing.
As $g$ is itself convex, we know that $g(\theta\,\mathbf{x}_1+(1-\theta)\,\mathbf{x}_2) \leq \theta\, g(\mathbf{x}_1) + (1-\theta)\, g(\mathbf{x}_2)\ \forall\,\mathbf{x}_1,\mathbf{x}_2 \in \mathbb{R}^n$ and $\theta\in[0,1]$.
Further, since $f$ is non-decreasing, we observe the following
%
\begin{equation}
    f\left(g(\theta\,\mathbf{x}_1+(1-\theta)\,\mathbf{x}_2)\right) \leq f\left(\theta\, g(\mathbf{x}_1) + (1-\theta)\, g(\mathbf{x}_2)\right).
\label{eq:Composition_1}
\end{equation}
%
Having in mind that $f$ itself is convex, we can further conclude that
%
\begin{equation}
    \underbrace{f\left(\theta\, g(\mathbf{x}_1) + (1-\theta)\, g(\mathbf{x}_2)\right)}_{=(f\circ g)(\theta\,\mathbf{x}_1+(1-\theta)\,\mathbf{x}_2)} \leq \underbrace{\theta\, f\left(g(\mathbf{x}_1)\right) + (1-\theta)\, f\left(g(\mathbf{x}_2)\right)}_{\theta(f\circ g)(\mathbf{x}_1) + (1-\theta)(f\circ g)(\mathbf{x}_2)} \quad \forall\,\mathbf{x}_1,\mathbf{x}_2 \in \mathbb{R}^n,\ \theta\in[0,1]
\label{eq:Composition_2}
\end{equation}
%
which proves that it is sufficient to state that the composition of convex functions is convex if the outer function is non-decreasing.\newline

\textbf{\textit{Zero-valued:} Composition of zero-valued functions.} We introduce two zero-valued functions $f:\mathbb{R}^n\rightarrow\mathbb{R}$ and $g:\mathbb{R}^m\rightarrow\mathbb{R}^n$ with $f(\mathbf{0})=0$ and $g(\mathbf{0})=\mathbf{0}$. The composition of these functions is also zero-valued, i.e.
%
\begin{equation}
    (f \circ g)(\mathbf{0})=0.
\label{eq:zero-valued}
\end{equation}
%

\textbf{\textit{Non-negative:} Composition with non-negative function.} Lastly, let us define two functions $f:\mathbb{R}\rightarrow\mathbb{R}^+$ and $g:\mathbb{R}^n\rightarrow\mathbb{R}$. 
The composition of these functions is always greater than or equal to zero, i.e., 
%
\begin{equation}
    (f \circ g)(\mathbf{x})\geq 0  \quad \forall\,\mathbf{x} \in \mathbb{R}^n
\label{eq:non-negative}
\end{equation}
%
since the outer function, $f$, is always greater than or equal to zero.
%
\begin{remark}
As noted, a network designed according to these properties may not encompass all possible constructions of a consistent dual potential. 
However, given that neural networks are universal approximators \cite{hornik1989}, we hypothesize that increasing the number of layers and neurons enables us to discover a broad range of inelastic material behaviors.
\end{remark}
%
%============================================================
\subsubsection{Custom-designed architecture of feed-forward network}
\label{sec:architecture_potential}
%
\textbf{General architecture.} The general architecture, whose design is deduced from Section~\ref{sec:convex_potential}, is illustrated in Figure~\ref{fig:NN_potential_general}.
We begin by refining the computation methods for the square root of $J_2^{\bar{\bm{\Sigma}}}$ and the cubic root of $J_3^{\bar{\bm{\Sigma}}}$
%
\begin{equation}
    \sqrt{x} := \frac{x}{(x+\epsilon_1)^{1/2}}, \quad \sqrt[3]{x} := \frac{x}{(\mathrm{abs}(x)+\epsilon_2)^{2/3}}, \quad \epsilon_1,\,\epsilon_2 \geq 0
\end{equation}
%
in order to be differentiable at zero.
In the numerical implementation, we choose $\epsilon_1=\epsilon_2=0.01$.

Next, let us discuss the domain of the weights and biases connecting the different layers.
According to Equations~\eqref{eq:convex_linear_1}-\eqref{eq:convex_linear_2}, the weights between the inputs and the first hidden layer are real-valued $\mathbf{W}_0^{\tightI} \in \mathbb{R}^{n_\tightI \times n_0}$. 
Here, $n_0$ refers to the number of input neurons, while $n_\tightI$ represents the total number of neurons in the first hidden layer.
For the weights between the first and second hidden layers, the matrix is given as $\mathbf{W}_{\tightI}^{\tightII} \in \mathbb{R}^{n_\tightII \times n_\tightI} \setminus S$, where
%
\begin{equation}
    S = \left\{ \mathbf{W}_{\tightI}^{\tightII} \in \mathbb{R}^{n_\tightII \times n_\tightI} \, : \, \exists i,j \text{ such that } W_{\tightI_{ij}}^\tightII < 0 \right\},
\end{equation}
%
ensuring compliance with the non-negativity constraint specified in Equation~\eqref{eq:convex_positive_sum}.
The weight matrices for subsequent layers, up to $\mathbf{W}_{\star}^{K}$, follow a similar definition. 
Further, we introduce the row vector $\mathbf{w}_{\omega^*} \in \mathbb{R}^{n_{K+1}} \setminus P$ with
%
\begin{equation}
    P = \left\{ \mathbf{w}_{\omega^*} \in \mathbb{R}^{n_{K+1}} \, : \, \exists i \text{ such that } w_{\omega^*_i} < 0 \right\},
\end{equation}
%
ensuring all weights remain non-negative.
Finally, the biases per activation function, $\mathbf{b}_\star^K$, up the second hidden layer are generally real-valued.

Following the constraints on the weights and biases of the network, we introduce how the choice of activation functions per layer and neuron is limited.
Due to Equations~\eqref{eq:convex_linear_2}-\eqref{eq:Composition_2}, we conclude that only the first hidden layer $\tightI$ permits generally convex activation functions, i.e., these functions, $f^\tightI$, might be decreasing.
Using a single activation function per layer, as is common in neural networks, severely restricts the choice of decreasing convex functions. 
To overcome this limitation, we incorporate a diverse set of activation functions in each layer, adhering to our design strategy.
The activation functions of each subsequent layer, $f^\tightII,\hdots,f^K$, are only allowed to be non-decreasing to result in a convex function, cf. Equations~\eqref{eq:convex_positive_sum} and \eqref{eq:Composition_2}.
Additionally, per Equation~\eqref{eq:zero-valued}, all activation functions must be zero-valued to achieve a zero-valued potential.

Lastly, let us consider the case where $K=\tightII$, cf. Figure~\ref{fig:NN_potential_general}.
Constructing the network with a single activation function and one neuron per layer, e.g., $f^\tightI \in \{x\}$ and $f^\tightII \in \{\mathrm{exp}(x)-1\}$, results in the potential $\omega^* = (f^\tightII \circ f^\tightI)(x) = \mathrm{exp}(w_0^\tightI \, x)-1$.
This potential is convex and zero-valued, but violates the non-negativity constraint.
To address this, we introduce an additional hidden layer, $K+1$, with the same number of neurons as layer $K$.
This final layer employs a single activation function $f^{K+1} \in \{ \mathrm{max}(x,0) \}$, cf. Equation~\eqref{eq:non-negative}.
No weights or biases are introduced between the last two hidden layers\footnote{This can be interpreted as a non-trainable identity weight matrix and a zero bias vector.}, as indicated by dashed lines in Figure~\ref{fig:NN_potential_general}.\newline

\textbf{Specific architecture.} With the general architecture at hand, we need to specify the number of layers, the choice of activation functions per layer, and the number of neurons per activation function.
In this contribution, we employ the specific architecture shown in Figure~\ref{fig:NN_potential}.
First of all, we enhance the network's inputs by the stress invariants $I_2^{\bar{\bm{\Sigma}}} := 1/2\, \mathrm{tr}(\bar{\bm{\Sigma}}^2)$ and $I_3^{\bar{\bm{\Sigma}}} := 1/3\, \mathrm{tr}(\bar{\bm{\Sigma}}^3)$.
This is an easy method of broadening the space of potentials included in our network, see also Section~\ref{sec:resembling}.
Note that this does not affect our findings on the potential, see \ref{app:stress_invars}.

In line with the design of the architecture introduced above, we choose the following set of activation functions per hidden layer
%
\begin{equation}
    \begin{aligned}
        f^{\tightI} &\in \{x,|x|^{p_1+1},\mathrm{ln}(\mathrm{cosh}(|x|^{p_2+1}))\} \\
        f^{\tightII} &\in \{\mathrm{max}(x,0),\mathrm{exp}(x)-1\} \\
        f^{\tightIII} &\in \{\mathrm{max}(x,0),\mathrm{exp}(x)-1\}.
    \end{aligned}
\end{equation}
%
Here, we introduce two additional weights, $p_1$ and $p_2$, which are discovered during training.
These weights are constrained to be greater than or equal to zero, i.e.\ $p_1,\, p_2 \geq 0$, to obtain convex activation functions.
Moreover, we modify the computation of $|x|^{\alpha+1}$ to guarantee that the function is finite for $x=0$ and differentiable at $x=0$
%
\begin{equation}
    |x|^{\alpha+1} := \mathrm{exp}\left(\left(\alpha + 1\right) \ln\left(\mathrm{abs}(u(x)) + \epsilon_1\right)\right),\quad u(x) = x + \mathrm{sgn}(x)\,\epsilon_2, \quad \epsilon_1,\,\epsilon_2 \geq 0
\end{equation}
%
where $\mathrm{sgn}(x)$ refers to the sign function.
In the numerical implementation, we choose $\epsilon_1 = \epsilon_2 = 0.0001$.
In addition, we introduce biases, $\mathbf{b}_1^{\tightII}$ and $\mathbf{b}_1^{\tightIII}$, for the $\mathrm{max}(x,0)$ activation functions in the second and third layers, respectively.
Noteworthy, the entries of theses biases must be non-positive to satisfy that the activation function remains zero-valued.

It remains to choose the number of neurons per activation function.
For simplicity, we choose the same number of neurons per activation function in each layer.
In the first layer, we employ six neurons per function resulting in eighteen neurons in total.
For all subsequent layers, we choose four neurons per activation function.
Consequently, our specific architecture consist of $90\,(\mathbf{W}_0^\tightI) + 144\,(\mathbf{W}_\tightI^\tightII) + 64\,(\mathbf{W}_\tightII^\tightIII) + 8\,(\mathbf{w}_{\omega^*}) + 4\,(\mathbf{b}_1^\tightII) + 4\,(\mathbf{b}_1^\tightIII) + 1\, (p_1) + 1\, (p_2) = 316$ weights in total.\newline

\textbf{Regularization.} In Section~\ref{sec:network}, we introduced the training loss with the regularization $R_{\omega^*}$ for the potential.
Furthermore, we stated that the regularization should discover a relatively sparse network in order to both be considered as a `unique' material model and to reveal the inelastic phenomena.
Thus, in line with the experiences made in \cite{mcculloch2024,linka2024}, we regularize the weights of the last layer, $\mathbf{w}_{\omega^*}$, by a lasso (L\textsubscript{1}) regularization to promote sparsity.
Additionally, we regularize the weights of the very first hidden layer by an elastic net, which linearly combines lasso and ridge (L\textsubscript{2}) regularization, to promote sparsity and mitigate multicollinearity.
Lastly, the biases are regularized by lasso.
Thus, the overall regularization of the potential reads
%
\begin{equation}
    R_{\omega^*} = \sum_{i=1}^{18}\sum_{j=1}^{5}\left[\lambda_1\,\left(W_{0_{ij}}^\tightI\right)^2 + \lambda_2\,\mathrm{abs}\left(W_{0_{ij}}^\tightI\right)\right] 
    +\sum_{i=1}^{8} \lambda_3\,\mathrm{abs}\left(w_{\omega^*_{i}}\right) +\sum_{i=1}^{4} \lambda_4\,\left[\mathrm{abs}\left(b_{1_i}^\tightII\right) + \mathrm{abs}\left(b_{1_i}^\tightIII\right) \right]
\label{eq:regularization_potential}
\end{equation}
%
where we set the regularization parameters to $\lambda_1=\lambda_2=\lambda_3=0.0001$ and $\lambda_4=0.01$ for all training sessions.
%
\begin{figure}[h]
    \centering
    \input{figures/NN_potential_general}
    \caption{Schematic of our feed-forward architecture for the dual potential, $\omega^*$, designed to ensure thermodynamic consistency. The network enforces convexity, non-negativity, and a zero-valued potential. Dashed lines indicate neurons without associated weights. Activation functions are denoted as $f_{\alpha,\beta}^{\gamma}$, where $\gamma$ is the layer index, $\alpha$ specifies the function type, and $\beta$ indexes the neuron within $\alpha$. The final layer ($K+1$) exclusively employs the $\max(x,0)$ activation.
    The weights are denoted by $\mathbf{W}$ and $\mathbf{w}$, respectively, while $\mathbf{b}$ represents the network's biases.
}
    \label{fig:NN_potential_general}
\end{figure}
%
\begin{figure}[h]
    \centering
    \input{figures/NN_potential}
    \caption{Specification of our general architecture presented in Figure~\ref{fig:NN_potential_general}. The inputs are extended by two invariants, while biases are only associated with the maximum activation function in the second and third layer.}
    \label{fig:NN_potential}
\end{figure}

%============================================================
\subsubsection{Reproduction of classical dual potentials}
\label{sec:resembling}
%
In the following, we demonstrate that the generalized iCANN architecture, as previously introduced, is capable of recovering a variety of established dual potentials known within the continuum mechanics community. 
To this end, we streamline the architecture outlined in Section~\ref{sec:architecture_potential} to its essential neurons, thereby enabling the integration of all these models into a unified architecture. 
The resulting reduced network is illustrated in Figure~\ref{fig:NN_resembling}. 
For each of the classical dual potentials examined, we link weight matrices to the material parameters.

The \textit{von Mises} criterion \cite{vonmises1913} can be expressed as
%
\begin{equation}
    \omega^*_{vM} = \sqrt{3\, J_2^{\bar{\bm{\Sigma}}} }, \quad \mathbf{W}_0^\tightI = \begin{pmatrix}
        0 & 1 & 0 \\
        0 & 0 & 0 \\
        0 & 0 & 0
    \end{pmatrix}, \quad \mathbf{w}_\tightI^\tightII = \begin{pmatrix}
        \sqrt{3} & 0 & 0
    \end{pmatrix}
\end{equation}
%
which is mostly applied for ductile materials.
At this point, it is already important to note that due to the dense structure of the generalized architecture, the weight matrices are not unique.
For instance, for the \textit{von Mises} potential one may exchange the non-zero entries in $\mathbf{W}_0^\tightI$ with those of $\mathbf{w}_\tightI^\tightII$.

Next, we investigate the pressure-sensitive \textit{Drucker-Prager} model \cite{drucker1952}
%
\begin{equation}
    \omega^*_{DP} = \sqrt{3\, J_2^{\bar{\bm{\Sigma}}} } + \underbrace{\frac{\sigma_c-\sigma_t}{\sigma_c+\sigma_t}}_{=:\xi}\, I_1^{\bar{\bm{\Sigma}}}, \quad \mathbf{W}_0^\tightI = \begin{pmatrix}
        \xi & \sqrt{3} & 0 \\
        0 & 0 & 0 \\
        0 & 0 & 0
    \end{pmatrix}, \quad \mathbf{w}_\tightI^\tightII = \begin{pmatrix}
        1 & 0 & 0
    \end{pmatrix}
\end{equation}
%
which represents a cone within the principal stress space.
The yield stresses, $\sigma_c$ and $\sigma_t$, denote the uniaxial yield stress in compression and tension, respectively.
It is important to note that $\xi$ might be negative.
Note that using the `original' iCANN architecture \cite{holthusen2024}, we were not able to discover the \textit{Drucker-Prager} model, cf. \cite{boes2024arxiv}.

The \textit{Bresler-Pister} \cite{bresler1985} model extends the former potential by a dependence on the quadratic hydrostatic pressure $I_1^{\bar{\bm{\Sigma}}}$.
Usually, this additional dependency is associated with the yield stress under biaxial compression.
The potential reads as follows
%
\begin{equation}
    \omega^*_{BP} = \sqrt{3\, J_2^{\bar{\bm{\Sigma}}} } + \zeta_1\,I_1^{\bar{\bm{\Sigma}}} + \zeta_2\,\left( I_1^{\bar{\bm{\Sigma}}} \right)^2, \quad \mathbf{W}_0^\tightI = \begin{pmatrix}
        \zeta_1 & \sqrt{3} & 0 \\
        1 & 0 & 0 \\
        0 & 0 & 0
    \end{pmatrix}, \quad \mathbf{w}_\tightI^\tightII = \begin{pmatrix}
        1 & \zeta_2 & 0
    \end{pmatrix}
\end{equation}
%
with the material constants $\zeta_1$ and $\zeta_2$.
As for the \textit{Drucker-Prager} model, $\zeta_1$ can be any real number.
In contrast, $\zeta_2$ must be positive to be discoverable by the generalized iCANN.

The potential proposed by \cite{stassi1967,tschoegl1971} represents a paraboloid within the principal stress space
%
\begin{equation}
    \omega^*_{ST} = 3\, J_2^{\bar{\bm{\Sigma}}} + (\sigma_c-\sigma_t)\, I_1^{\bar{\bm{\Sigma}}}, \quad \mathbf{W}_0^\tightI = \begin{pmatrix}
        \sigma_c-\sigma_t & 0 & 0 \\
        0 & \sqrt{3} & 0 \\
        0 & 0 & 0
    \end{pmatrix}, \quad \mathbf{w}_\tightI^\tightII = \begin{pmatrix}
        1 & 1 & 0
    \end{pmatrix}
\end{equation}
%
where the difference, $\sigma_c-\sigma_t$, can be negative.
Again, this potential cannot be discovered by the original iCANN architecture, see \cite{boes2024arxiv}.

A quadratic potential, which is often used to model visco-elasticity at finite strains \cite{reese1998}, is expressed in terms of stress invariants as follows
%
\begin{equation}
    \omega^*_{RG} = 0.25\,\mu^{-1}\, J_2^{\bar{\bm{\Sigma}}} + \frac{\kappa^{-1}}{18}\, \left( I_1^{\bar{\bm{\Sigma}}} \right)^2, \quad \mathbf{W}_0^\tightI = \begin{pmatrix}
        0 & 0 & 0 \\
        1 & 0 & 0 \\
        0 & 1 & 0
    \end{pmatrix}, \quad \mathbf{w}_\tightI^\tightII = \begin{pmatrix}
        0 & \kappa^{-1}/18 & 0.25\,\mu^{-1} 
    \end{pmatrix}
\end{equation}
%
where $\mu$ denotes the material's shear modulus, while $\kappa$ refers to the bulk modulus.

Lastly, we investigate a smoothed version \cite{holthusen2023} of the maximum principal stress theory, also known as Rankine's theory \cite{collins1993book},
%
\begin{equation}
    \omega^*_{sPs} = I_1^{\bar{\bm{\Sigma}}} + \sqrt{ I_2^{\bar{\bm{\Sigma}}} }, \quad \mathbf{W}_0^\tightI = \begin{pmatrix}
        1 & 0 & 1 \\
        0 & 0 & 0 \\
        0 & 0 & 0
    \end{pmatrix}, \quad \mathbf{w}_\tightI^\tightII = \begin{pmatrix}
        1 & 0 & 0
    \end{pmatrix}
\end{equation}
%
where it becomes clear why we include also the invariant $I_2^{\bar{\bm{\Sigma}}}$ into the specific architecture of our proposed generalized dual potential.
%
\begin{figure}[h]
    \centering
    \input{figures/NN_resembling}
    \caption{Reduced architecture of the specific network shown in Figure~\ref{fig:NN_potential}. For simplicity, the weights between the second and third layers as well as the third layer to the output are set equal to one. The activation functions are chosen as $f^\tightI \in \{x,|x|^{p_1+1}\}$ and $f^\tightII,\, f^\tightIII \in \{\mathrm{max}(x,0)\}$, where the weight $p_1$ is set equal to one.}
    \label{fig:NN_resembling}
\end{figure}
%
%============================================================
\subsection{Feed-forward network: Helmholtz free energy}
\label{sec:FFN_energy}
%
\textbf{Architecture.} As the scope of this manuscript lies on the development of a generalized dual potential, we employ the feed-forward network of the Helmholtz free energy proposed in \cite{holthusen2024,holthusen2024PAMM}.
Figure~\ref{fig:NN_energy} illustrates the custom-designed feed-forward network.
In contrast to our new proposed network of the potential, the energy network is sparse.
As $\psi$ is an isotropic function of $\bar{\bm{C}}_e$, we express it in terms of invariants.
To this end, the determinant, $I_3^{\bar{\bm{C}}_e}:=\mathrm{det}(\bar{\bm{C}}_e)$, as well as the isochoric invariants, $\tilde{I}_1^{\bar{\bm{C}}_e} := \mathrm{tr}(\bar{\bm{C}}_e)/(I_3^{\bar{\bm{C}}_e})^{1/3}$ and $\tilde{I}_2^{\bar{\bm{C}}_e} := \mathrm{tr}(\mathrm{cof}(\bar{\bm{C}}_e))/(I_3^{\bar{\bm{C}}_e})^{2/3}$, serve as the network's inputs.
The latter invariant is raised to the power of $3/2$ to satisfy polyconvexity \cite{hartmann2003}.

For the network, we choose the following set of activation functions per layer
%
\begin{equation}
    \begin{aligned}
        f^{\tightI} &\in \{x^2,x\} \\
        f^{\tightII} &\in \{x,\mathrm{exp}(x)-1,x^{p_3}-1-\mathrm{ln}(x^{p_3})\}
    \end{aligned}
\end{equation}
%
where we introduce the weight $p_3$ in addition to the weights of the network, see Figure~\ref{fig:NN_energy}.
Thus, the total number of weights is $4\, (\mathbf{W}_\tightI^\tightII) + 9\, (\mathbf{w}_\psi) + 1\, (p_3) = 14$ for the network of the energy.\newline

\textbf{Regularization.} As for the dual potential, we regularize the weights introduced for the energy (cf. Equations~\eqref{eq:loss} and \eqref{eq:regularization_potential}).
According to the experiences made in \cite{holthusen2023PAMM,linka2024,holthusen2024}, we only employ lasso regularization for the weights of the very last layer, and only regularize the weights associated with the isochoric part of the energy, i.e.,
%
\begin{equation}
    R_\psi = \sum_{i=1}^8 \lambda_5\, \mathrm{abs}\left( w_{\psi_i} \right)
\end{equation}
%
where $\lambda_5$ denotes the regularization parameter which is set to $0.001$ within this contribution.
%
\begin{figure}[h]
    \centering
    \input{figures/NN_energy}
    \caption{Schematic of our feed-forward architecture for the Helmholtz free energy, $\psi$. Dashed lines indicate neurons without associated weights, respectively, being equal to one and non-trainable. The biases are constant and are introduced to ensure a zero-valued energy.}
    \label{fig:NN_energy}
\end{figure}
