\section{Discussion and current limitations}
\label{sec:discussion}
%
The results from the previous section demonstrated that the proposed iCANN architecture, whose complexity can be recursively increased along the lines of classical feed-forward networks, successfully discovers visco-elasticity at finite strains, and further, automatically uncovers the inelastic phenomenon hidden in the data. 
However, as with any approach that is still in its early stages, we faced some challenges during the discovery process, took some further steps towards effective discovery of inelastic materials, and were left with some unanswered questions, which we would like to share in the following.\newline

\textbf{Time integration scheme.} Our findings highlight challenges in the integration scheme. 
Training with an implicit scheme led to an undesired convergence of the weights of the potential towards zero.
Based on Section~\ref{sec:results_artificial_1}, we attribute this to inaccuracies in the iterative solver rather than the implicit integration itself. 
Broyden's method seems to be inadequate for solving the nonlinear evolution equation, requiring numerous local iterations (see \ref{app:broyden}), which complicates neural network backpropagation.

Initially, we employed Newton’s method, but automatic differentiation of the residual~\eqref{eq:implicit_integration} to get its Jacobian introduced issues in the backpropagation, prompting us to switch to the derivative-free Broyden method.

Further research is needed on combining neural network training with efficient iterative solvers to ensure stable training. 
The implicit function theorem, applied to the residual to calculate the derivative of the inelastic stretch with respect to the network's weights, may enhance stability. 
However, our studies show that explicit integration is computationally more efficient, though smaller time steps are required.
In real-world experiments, the step size can usually be controlled.\newline

\textbf{Regularization.} We implemented elastic net regularization for the weights connecting the inputs to the first hidden layer, while applying lasso regularization to the final layer. 
This decision aligns with empirical findings from CANNs. 
While our rationale for regularizing the last layer is somewhat justified, it remains a heuristic choice.

The use of elastic net is also primarily heuristic; however, it has demonstrated effective results and -- at least in our experience -- enhanced stability during training. 
It is important to note that heuristic strategies may not represent the optimal solution. 
The question of whether a superior regularization method exists for every inelastic phenomenon warrants further investigation.

In contrast to the Helmholtz free energy, we allow the potential to be zero. 
A zero energy implies no stresses would arise, which contradicts experimental data. 
A zero potential, on the other hand, leads to a hyperelastic model that could find an `optimal solution' for monotonic loading with constant stretch rates at different maximum stretch levels applied to the same material.

Interestingly, our chosen regularization approach resulted in sparse networks and appeared capable of identifying the degree of inelasticity within the data. 
Earlier training sessions conducted without regularization on the feed-forward network produced dense networks -- none of which reduced from iCANNs to CANNs, thereby failing to accurately represent visco-elastic solids.\newline

\textbf{Initialization.} We initialized the network weights using a uniform distribution, \( U(a,b) \), with heuristically determined bounds
\begin{equation}
\mathbf{W}_0^\tightI\sim U(-1,1),\quad 
\mathbf{W}_\tightI^\tightII,\,\mathbf{W}_\tightII^\tightIII\sim U(0,0.01),\quad 
\mathbf{w}_{\omega^*}\sim U(0,0.001),\quad 
p_1,\,p_2\sim U(0,1)
\end{equation}
Biases were initialized to zero to prevent symmetry-breaking effects.

Our training results indicate that performance is highly dependent on the amount of data used (see Section~\ref{sec:results_artificial_2}). 
When the data set is too large in combination with a low degree of inelasticity, the information entropy may become insufficient for discovering the optimal potential. 
Since we have not systematically explored alternative initialization bounds, we cannot exclude the possibility that different choices might enhance the network’s ability to generalize across the full data set.

Furthermore, in our experience, the ratio between the weights of energy and potential plays a crucial role. 
If energy weights are too large, the potential rapidly tends towards infinity regardless of its own weights. 
Conversely, if energy weights are too small, the potential may be significantly underestimated, leading to an overly elastic response. 
In extreme cases, the model may learn a purely elastic behavior that simply balances the data set rather than capturing the underlying material response.

Given the significant impact of weight initialization on learning dynamics, we recognize the need for systematic investigation. 
An evolutionary optimization approach, such as a genetic algorithm, could provide an effective strategy for weight distributions and mitigating undesirable energy-to-potential ratios.\newline

\textbf{Hyperparameters.} Across all examples, we employed the same set of hyperparameters, including learning rate, regularization parameters, number of hidden layers, number of neurons per activation function, choice of activation functions, and the clipping norm for gradient clipping. 
While some of these define the network architecture itself, such as depth and width, others regulate the training process.

Notably, model performance is influenced not only by the number of neurons, which likely correlates with the number of hidden layers, but also by training-specific parameters such as the clipping norm. 
The interplay among these factors renders hyperparameter selection a highly nonlinear optimization problem.

We observed that gradient clipping is essential for stabilizing the inelastic potential. 
Similar to weight initialization, where an improper energy-to-potential ratio can lead to an unbounded potential, the absence of gradient clipping can cause excessive growth of the potential’s weights between epochs, ultimately leading to divergence.

The chosen hyperparameters yielded satisfactory results for the examples considered in this study. 
However, we acknowledge that this may be due to a fortuitous selection of parameters and the specific network complexity in terms of depth and neuron count.

For artificial data, the training procedure performed well, producing results consistent with the data set. 
In contrast, performance deteriorated when applied to experimentally obtained data. 
This discrepancy may stem from the network's complexity, which introduces unnecessary flexibility in the learned weights. 
Furthermore, the information content of artificial and experimental data sets likely differs significantly.

Across all examples, the final state of training consistently produced a sparse weight vector \( \mathbf{w}_{\omega^*} \). 
Whether performance could be further improved through systematic hyperparameter tuning and reduced network complexity remains an open question.

We anticipate that hyperparameter optimization will be particularly critical for modeling more complex inelastic phenomena such as plasticity and damage.
Fortunately, hyperparameter optimization is an extensively studied problem in the neural network community. 
Established techniques such as reinforcement learning~\cite{wu2020}, grid search~\cite{bergstra2011}, genetic algorithms~\cite{sun2019}, and gradient-based optimization~\cite{pedregosa2016} offer promising strategies.
Recently, agent-based hyperparameter optimization \cite{esmaeili2023} has shown the potential to outperform these established techniques and may also serve as a valuable tool in physics-embedded neural networks.
For a comprehensive overview, we refer the reader to \citet{bischl2023}.\newline

\textbf{Richness of data sets.} Ultimately, the key question underlying our discussion is: 
How `rich' is the data? 
If the information entropy is sufficiently high, the iCANN architecture is likely capable of uncovering the inelastic phenomena hidden in the data and of accurately identifying the underlying potential governing the material’s response. 
However, the concept of `richness' in the context of material science requires further clarification.

Consider a previously discussed example: 
Suppose we conduct a series of uniaxial tension experiments on a visco-elastic solid, varying only the maximum stretch level while keeping the deformation rate constant across all tests.
Despite the large number of data points, the information entropy remains low. 
Consequently, the network will most likely identify a purely hyperelastic response. 
In contrast, if the deformation rate is also varied, the observed stress differences between experiments can only be attributed to the presence of an inelastic potential.

We encountered this issue with the VHB 4910 polymer (see Section~\ref{sec:Hossain2012}). 
While the network accurately reproduced the training data, its predictive performance was poor. 
This suggests that the `richness' of uniaxial tension data alone was insufficient to capture an accurate material model given the chosen network’s complexity. 
To illustrate this further, we consider an experiment in which a material undergoes hydrostatic relaxation, ensuring that no shear stresses arise. 
Since the second and third stress invariants remain zero throughout the experiment, the network would be unable to infer any material dependence on these invariants.

These challenges align with long-standing questions in experimental mechanics. 
Traditionally, experiments are designed to isolate specific material parameters, such as relaxation time in relaxation tests. 
However, the combination of neural networks and advanced optimization techniques -- both in training and hyperparameter tuning -- may enable us to replace numerous experiments with a single, highly complex experiment that captures comprehensive material behavior.

This raises an important question: 
How to uniquely design a `rich' experimental setup that captures not only elasticity and visco-elasticity but also phenomena such as plasticity, damage, and even multiphysics interactions?
We assume that leveraging structural discovery in complex boundary value problems will be instrumental in achieving this goal, as demonstrated by EUCLID~\cite{flaschel2022}.
%
%\FloatBarrier