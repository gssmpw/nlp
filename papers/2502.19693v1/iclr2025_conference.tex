
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{subfigure}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{wrapfig}


\usepackage{algorithm}
\usepackage{enumitem}
% \usepackage{algorithmic}
\usepackage{algorithmicx}
\usepackage{algpseudocode}



\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{bm}

\usepackage{stmaryrd}


\newcommand{\acc}[2]{#1\textcolor{black!70!white}{\scriptsize{$\pm$#2}}}
\newcommand{\mr}[2]{\multirow{#1}{*}{#2}}
\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\udfsection}[1]{\noindent\textbf{#1}\, }

\newcommand{\gongshi}[1]{{\small #1}}
\newcommand{\update}{u}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\diag}{\text{diag}}
\newcommand{\gnn}{\text{GNN}}
\newcommand{\gcn}{\text{GCN}}
\newcommand{\norm}{\text{Norm}}
\newcommand{\aggregate}{\oplus}


% \newif\ifupdate\updatetrue
\newif\ifupdate\updatefalse 
\newcommand{\modify}[2]{\ifupdate{#1}\else{\color{red}#2}\fi}

\newcommand{\todo}[2]{\ifupdate{#1}\else{\color{blue}#2}\fi}

\title{Accurate and Scalable Graph Neural Networks via Message Invariance}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{
Zhihao Shi$\,\,$\textsuperscript{1} ,
Jie Wang\thanks{Corresponding author: jiewangx@ustc.edu.cn}$\,\,$\textsuperscript{1},
Zhiwei Zhuang$\,\,$\textsuperscript{1}, 
Xize Liang$\,\,$\textsuperscript{1}, 
Bin Li$\,\,$\textsuperscript{1}, 
Feng Wu$\,\,$\textsuperscript{1}, \\
\textsuperscript{1} MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition,
\\
University of Science and Technology of China
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\iclrfinalcopy

\maketitle

\begin{abstract}
    Message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications.
    For a sampled mini-batch of target nodes, the message passing process is divided into two parts: \textbf{m}essage \textbf{p}assing between nodes with\textbf{i}n the \textbf{b}atch ($\text{MP}_{\text{IB}}$) and \textbf{m}essage \textbf{p}assing from nodes \textbf{o}utside the \textbf{b}atch to those within it ($\text{MP}_{\text{OB}}$).
    However, $\text{MP}_{\text{OB}}$ recursively relies on higher-order out-of-batch neighbors, leading to an exponentially growing computational cost with respect to the number of layers.
    Due to the \textit{neighbor explosion}, the whole message passing stores most nodes and edges on the GPU such that many GNNs are infeasible to large-scale graphs.
    To address this challenge, we propose an accurate and fast mini-batch approach for large graph transductive learning, namely \textbf{t}op\textbf{o}logical com\textbf{p}ensation (TOP), which obtains the outputs of the whole message passing solely through $\text{MP}_{\text{IB}}$, without the costly $\text{MP}_{\text{OB}}$.
    The major pillar of TOP is a novel concept of \textit{message invariance},
    which defines \textit{message-invariant transformations} to convert costly $\text{MP}_{\text{OB}}$ into fast $\text{MP}_{\text{IB}}$.
    This ensures that the modified $\text{MP}_{\text{IB}}$ has the same output as the whole message passing.
    Experiments demonstrate that TOP is significantly faster than existing mini-batch methods by order of magnitude on vast graphs (millions of nodes and billions of edges) with limited accuracy degradation.
\end{abstract}



\section{Introduction}\label{sec:intro}


% GNNs
Message passing-based graph neural networks (GNNs) have been successfully applied to many practical applications involving graph-structured data, such as social network prediction \citep{graphsage, gcn, social2}, chip design \cite{chip_design1,chip_design2,chip_design3,chip_design4}, combinatorial optimization \cite{combinatorial_optimization1,combinatorial_optimization2,combinatorial_optimization3,combinatorial_optimization4,combinatorial_optimization5,combinatorial_optimization6,combinatorial_optimization7,combinatorial_optimization8}, drug reaction \citep{gnn_reaction1, gnn_reaction2, durg_design1}, and recommendation systems \citep{gnn_recommandation2, gnn_social}.
The key idea of GNNs is to iteratively update the embeddings of each node based on its local neighborhood.
Thus, as these iterations progress, each node embedding encodes more and more information from further reaches of the graph \citep[Chap. 5]{grl}.



However, training GNNs on a large-scale graph is challenging due to the well-known \textit{neighbor explosion} problem.
Specifically, the embedding of a node at the \gongshi{$l$}-th GNN layer depends on the embeddings of its local neighborhood at the \gongshi{$(l-1)$}-th GNN layer. Thus, around the target mini-batch nodes, these message passing iterations of an \gongshi{$L$}-layer GNN form a tree structure by unfolding their \gongshi{$L$}-hop neighborhoods \citep[Chap. 5]{grl}, whose size exponentially increases with the GNN depth \gongshi{$L$} (see Figure \ref{fig:origin_gnn}).
The exploded source neighborhoods may contain most nodes in the large-scale graph, leading to expensive computational costs.



To alleviate this problem, recent graph sampling techniques approximate the whole message passing with the small size of the source neighborhoods \citep[Chap. 7]{dlg}. 
For example, node-wise \citep{graphsage, vrgcn, labor} and layer-wise \citep{fastgcn, ladies, adapt} sampling recursively sample a small set of local neighbors over message passing layers.
The expectation of the recursive sampling obtains the whole message passing and thus the recursive sampling is accurate and provably convergent \citep{vrgcn}.
Different from the recursive fashion, subgraph sampling \citep{cluster_gcn, graphsaint, gas, shadow_gnn} adopts a cheap and simple one-shot sampling fashion, i.e., sampling the same subgraph induced by a mini-batch for different GNN layers.
It preserves message passing between in-batch nodes ($\text{MP}_{\text{IB}}$) and eliminates message passing from out-of-batch neighbors to in-batch nodes ($\text{MP}_{\text{OB}}$), achieving a linear complexity with respect to the number of GNN layers.





Nonetheless, accuracy and efficiency are two important but conflicting factors for existing graph sampling techniques. Specifically, accurate recursive sampling maintains the whole message passing at the expense of efficiency, while fast one-shot sampling eliminates $\text{MP}_{\text{OB}}$ at the expense of accuracy.
This motivates us to develop an accurate and fast mini-batch method for GNNs to approximate the outputs of the whole message passing solely through $\text{MP}_{\text{IB}}$ with marginal errors.




\begin{figure*}[t]
    \vspace{-30pt}
    \centering
    \subfigure[Original GNNs]{
        \includegraphics[width = 0.55\textwidth]{Figure/Fig1/new_Original_GNNs.pdf} \label{fig:origin_gnn}
    }
    \subfigure[Subgraph Sampling]{
        \includegraphics[width = 0.40\textwidth]{Figure/Fig1/new_CLUSTER.pdf}
    }
    \subfigure[TOP]{
        \includegraphics[width = 0.55\textwidth]{Figure/Fig1/new_TOP.pdf}
    }
    \subfigure{
        \includegraphics[width = 0.40\textwidth]{Figure/Fig1/New_Legends.pdf}
    }
    \vspace{-6pt}
    \caption{\textbf{Mini-batch processing of original GNNs, subgraph sampling, and TOP.} Given a mini-batch, the computational costs of original GNNs exponentially increase with GNN depth (a). To address this challenge, many subgraph sampling methods preserve message passing between the in-batch nodes ($\text{MP}_{\text{IB}}$) and eliminate message passing from out-of-batch neighbors to the in-batch nodes ($\text{MP}_{\text{OB}}$) to reduce the computational costs (b). However, the final embeddings of subgraph sampling are usually different from the result of the original GNNs.
    By noticing the message invariance \gongshi{$\mathbf{h}_{4} = 0 \cdot \mathbf{h}_{1} + 0 \cdot \mathbf{h}_{2} + 1 \cdot \mathbf{h}_{3}$},
    TOP converts $\text{MP}_{\text{OB}}$ \gongshi{$v_4\rightarrow v_3$} into $\text{MP}_{\text{IB}}$ \gongshi{$v_3\rightarrow v_3$} without approximation errors in the example (c).
    }
    \label{fig:top}
\end{figure*}


In this paper, we first propose a novel concept of \textit{message invariance}, which defines message-invariant transformations to convert $\text{MP}_{\text{OB}}$ into $\text{MP}_{\text{IB}}$, ensuring that the modified $\text{MP}_{\text{IB}}$ has the same output as the whole message passing.
Figure \ref{fig:top} shows a motivating example for message invariance, where converting $\text{MP}_{\text{OB}}$ \gongshi{$v_4\rightarrow v_3$} to $\text{MP}_{\text{IB}}$ \gongshi{$v_3\rightarrow v_3$} (the red edge) does not affect the output of GNNs.
Although the resulting subgraphs are different from the original graph, the in-batch embeddings and corresponding computation graphs are always the same.
We conduct extensive experiments to show the approximation of message invariance is effective in various real-world datasets (see Section \ref{subsec:sampling_bias_of_different_methods})






Inspired by the message-invariant transformations, we propose a fast subgraph sampling method, namely \textbf{t}op\textbf{o}logical com\textbf{p}ensation (TOP), which is applicable to various real-world graphs.
Specifically, TOP estimates the message invariance using a linear transformation, which learns the linear independence between embeddings of the in-batch nodes and their out-of-batch neighbors.
In Figure \ref{fig:top}, the out-of-batch embedding of \gongshi{$v_4$} is a linear combination of the in-batch embeddings of \gongshi{$(v_1, v_2, v_3)$} with coefficients \gongshi{$(0,0,1)$}.
We estimate the coefficients using a simple and efficient linear regression on sampled basic embeddings (e.g. the embeddings in GNNs with random initialization).
We further show that TOP achieves the convergence rate of \gongshi{$\mathcal{O}(\varepsilon^{-4})$} to reach an \gongshi{$\varepsilon$}-approximate stationary point (see Theorem \ref{thm:convergence_conv}), which is significantly faster than \gongshi{$\mathcal{O}(\varepsilon^{-6})$} of existing subgraph sampling methods \citep{lmc}.
We conduct extensive experiments on graphs with various sizes to demonstrate that TOP is significantly faster than existing mini-batch methods with limited accuracy degradation (see Figures \ref{fig:runtime} and \ref{fig:labor}).






\section{Related Work}\label{sec:related_work}



In this section, we discuss some works related to our proposed method.


\udfsection{Node-wise sampling.} Node-wise sampling \citep{graphsage, vrgcn, graphfm} aggregates messages from a subset of uniformly sampled neighborhoods at each GNN layer, which decreases the bases in the exponentially increasing dependencies.
The idea is originally proposed in GraphSAGE \citep{graphsage}. VR-GCN \citep{vrgcn} further alleviates the bias and variance by historical embeddings, and then shows that its convergence rate to reach an \gongshi{$\varepsilon$}-approximate stationary point is \gongshi{$N=\mathcal{O}(\varepsilon^{-4})$}, where \gongshi{$N$} denotes the number of iterations in Theorem 2 in \citep{vrgcn}.
GraphFM-IB further alleviates the staleness of the historical embeddings based on the idea of feature momentum.
Although the node-wise sampling methods achieve the convergence rate of \gongshi{$\mathcal{O}(\varepsilon^{-4})$}, their computational complexity at each step is still exponentially increasing due to the neighborhood explosion issue.



\udfsection{Layer-wise sampling.} To avoid the exponentially growing computation of node-wise sampling, layer-wise sampling \citep{fastgcn, ladies, adapt} samples a fixed number of nodes for each GNN layer and then uses importance sampling (IS) to reduce variance.
However, the optimal distribution of IS depends on the up-to-date embeddings, which are expensive.
To tackle this problem, FastGCN \citep{fastgcn} proposes to approximate the optimal distribution of IS by the normalized adjacency matrix.
Adapt \citep{adapt} proposes a learnable sampled distribution to further alleviate the variance.
Nevertheless, as the above-mentioned methods sample nodes independently in each GNN layer, the sampled nodes from two consecutive layers may be connected \citep{ladies}.
Thus, LADIES \citep{ladies} consider the dependency of sampled nodes between layers by one step forward.
By combining the advantages of node-wise and layer-wise sampling approaches using Poisson sampling, LABOR \citep{labor} significantly accelerates convergence under the same node sampling budget constraints.
% without sacrificing accuracy.
% Although layer-wise sampling significantly alleviates the memory costs, they suffer from high sampling overhead due to extensive sampling during training.

%  faster than node-wise and layer-wise sampling approaches

\udfsection{Subgraph sampling.} Subgraph sampling methods sample a mini-batch and then construct the subgraph based on the mini-batch \citep[Chap. 7]{dlg}.
Thus, we can directly run GNNs on the subgraphs.
% Therefore, subgraph sampling is simple and it is applicable to a wide range of GNN architectures.
% 1
One of the major challenges is to efficiently encode neighborhood information of the subgraph.
To tackle this problem, one line of subgraph sampling is to design subgraph samplers to alleviate the inter-connectivity between subgraphs.
For example, CLUSTER-GCN \citep{cluster_gcn} propose subgraph samplers based on graph clustering methods (e.g., METIS \citep{metis1} and Graclus \citep{graclus}) and GRAPHSAINT propose edge, node, or random-walk based samplers.
SHADOW \citep{shadow_gnn} proposes to extract the \gongshi{$L$}-hop neighbors of a mini-batch and then select an important subset from the \gongshi{$L$}-hop neighbors.
% The subgraph sampler simultaneously avoids the neighbor explosion issue and the over-smoothing issue of GNNs.
% 2
IBMB \citep{ibmb} proposes a novel subgraph sampler where the subgraphs are induced by the mini-batches with high influence scores, such as personalized PageRank scores.
Another line of subgraph sampling is to design efficient compensation for the messages from the neighborhood based on existing subgraph samplers.
For example, GAS \citep{gas} proposes historical embeddings to compensate for messages in forward passes and LMC \citep{lmc} further proposes historical gradients to compensate for messages in backward passes.
GraphFM-OB \citep{graphfm} alleviates the staleness of the historical embeddings based on the idea of feature momentum.
Besides the traditional optimization algorithm, SubMix \citep{submix} proposes a novel learning-to-optimize method for subgraph sampling, which parameterizes subgraph sampling as a convex combination of several heuristics and then learns to accelerate the training of subgraph sampling.
% TOP can be viewed as a learning-to-optimize idea, where TOP learns message-invariant transformation by linear regression to accelerate the optimization process.




\section{Preliminaries}

We first introduce notations in Section \ref{sec:notations}. Then, we introduce graph neural networks and the neighbor explosion issue in Section \ref{sec:gnn}.
% Finally, we introduce subgraph sampling to tackle the neighbor explosion issue in Section  \ref{sec:gcm}.
% We discuss the related works in Appendix \ref{sec:related_work}.

% Notation
\subsection{Notations}\label{sec:notations}
A graph  \gongshi{$\mathcal{G}=(\mathcal{V}, \mathcal{E})$} is defined by a set of nodes \gongshi{$\mathcal{V}=\{1,2,\dots,n\}$} and a set of edges  \gongshi{$\mathcal{E}$} among these nodes.
% The set of nodes consists of labeled nodes \gongshi{ $\mathcal{V}_{L}$} and unlabeled nodes \gongshi{ $\mathcal{V}_{U}:=\mathcal{V} \setminus \mathcal{V}_{L}$}. The label of a node $v_i \in \mathcal{V}_L$ is $y_i$.
Let  \gongshi{$(i,j)\in\mathcal{E}$} denote an edge going from node  \gongshi{$i\in\mathcal{V}$} to node  \gongshi{$j\in\mathcal{V}$}.
Let \gongshi{$(\mathcal{B}_1 \rightarrow \mathcal{B}_2)$} denote the set of edges \gongshi{$\{(i,j)|i \in \mathcal{B}_1, j \in \mathcal{B}_2, (i,j) \in \mathcal{E} \}$} from \gongshi{$\mathcal{B}_1$} to \gongshi{$\mathcal{B}_2$}.
Let \gongshi{$\mathcal{N}_i=\{j\in\mathcal{V}| (i,j)\in\mathcal{E}\}$} denote the neighborhood of node \gongshi{$i$}.
% , and \gongshi{ $\overline{\mathcal{N}}(v_i)$} denote \gongshi{$\mathcal{N}(v_i) \cup \{v_i\}$}.
% We assume that \gongshi{ $\mathcal{G}$} is undirected, i.e., \gongshi{ $v_j \in \mathcal{N}(v_i) \Leftrightarrow v_i \in \mathcal{N}(v_j)$}.
Let \gongshi{$\mathcal{N}_{\mathcal{B}} = (\cup_{i \in \mathcal{B}} \mathcal{N}_i) \cup \mathcal{B}$} denote the neighborhoods of a mini-batch \gongshi{$\mathcal{B}$} with itself.
Let \gongshi{$\mathcal{N}_{\mathcal{B}}^c = \mathcal{N}_{\mathcal{B}} - \mathcal{B}$} denote the out-of-batch neighbors of the mini-batch \gongshi{$\mathcal{B}$}.
% We also denote the neighborhood including itself by \gongshi{$\overline{\mathcal{N}_{\mathcal{B}}} = \mathcal{N}_{\mathcal{B}} \cup \mathcal{B} $}
We recursively define the set of \gongshi{$k$}-hop neighborhoods as \gongshi{$\mathcal{N}^k_\mathcal{B} = \mathcal{N}_{\mathcal{N}^{k-1}_{\mathcal{B}}}$} with \gongshi{$\mathcal{N}^1_{\mathcal{B}} = \mathcal{N}_{\mathcal{B}}$}.
% and \gongshi{ $\overline{\mathcal{N}}(\mathcal{S})=\mathcal{N}(\mathcal{S}) \cup \mathcal{S}$}.
The adjacency matrix is  \gongshi{$\mathbf{A} \in \mathbb{R}^{n \times n}$} with  \gongshi{$\mathbf{A}_{ij}=1$} if \gongshi{$(j,i)$} and \gongshi{$\mathbf{A}_{ij}=0$} otherwise.
% Let \gongshi{$\mathbf{D}$} be the diagonal degree matrix of $\mathbf{A}$.
% The normalized adjacency matrix is \gongshi{$\widetilde{\mathbf{A}} = (\mathbf{D}+\mathbf{I})^{-1/2}(\mathbf{A}+\mathbf{I})(\mathbf{D}+\mathbf{I})^{-1/2}$}.
Given sets \gongshi{$\mathcal{S}_1=(i_p)_{p=1}^{|\mathcal{S}_1|}, \mathcal{S}_2=(j_q)_{q=1}^{|\mathcal{S}_2|}$}, the submatrix \gongshi{$\mathbf{A}_{\mathcal{S}_1, \mathcal{S}_2}$} satisfies \gongshi{$[\mathbf{A}_{\mathcal{S}_1, \mathcal{S}_2}]_{p,q}=\mathbf{A}_{i_{p}, j_{q}}$}.
For a positive integer \gongshi{$L$}, \gongshi{$\llbracket L \rrbracket$} denotes \gongshi{$\{1,\ldots,L\}$}.


Let the boldface character \gongshi{$\mathbf{x}_{i} \in \mathbb{R}^{d_x}$} denote the feature of node \gongshi{$i$} with dimension \gongshi{$d_x$}. Let \gongshi{$\mathbf{h}_i\in\mathbb{R}^d$} be the \gongshi{$d$}-dimensional embedding of the node \gongshi{$i$}. Let \gongshi{$\mathbf{X} = (\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_n)^{\top} \in \mathbb{R}^{ n \times d_x }$} and \gongshi{$\mathbf{H}  = (\mathbf{h}_1,\mathbf{h}_2,\dots,\mathbf{h}_n)^{\top} \in \mathbb{R}^{ n \times d }$}.
We also denote the node features and embeddings of a mini-batch \gongshi{$\mathcal{B}=(i_k)_{k=1}^{|\mathcal{B}|}$} by \gongshi{$\mathbf{X}_{\mathcal{B}}  = (\mathbf{x}_{i_1}, \mathbf{x}_{i_2}, \dots, \mathbf{x}_{i_{|\mathcal{B}|}})^{\top} \in \mathbb{R}^{|\mathcal{B}| \times d_x}$} and \gongshi{$\mathbf{H}_{\mathcal{B}} \in \mathbb{R}^{|\mathcal{B}| \times d}  $} respectively.

% we denote the \gongshi{$j$}-th columns of \gongshi{$\mathbf{A}$} by \gongshi{ $\mathbf{A}_{j}$}.



% We also denote the embeddings of a set of nodes \gongshi{ $\mathcal{S}=\{v_{i_k}\}_{k=1}^{|\mathcal{S}|}$} by \gongshi{ $\mathbf{H}_{\mathcal{S}}  = (\mathbf{h}_{i_1}, \mathbf{h}_{i_2}, \dots, \mathbf{h}_{i_{|\mathcal{S}|}}) \in \mathbb{R}^{d \times |\mathcal{S}|}$}. For a \gongshi{ $p \times q$} matrix \gongshi{ $\mathbf{A}\in\mathbb{R}^{p\times q}$}, \gongshi{ $\Vec{\mathbf{A}} \in \mathbb{R}^{pq}$} denotes the vectorization of \gongshi{ $\mathbf{A}$}, i.e., \gongshi{ $\Vec{\mathbf{A}}_{i+(j-1)p} = \mathbf{A}_{ij}$}. We denote the \gongshi{$j$}-th columns of \gongshi{$\mathbf{A}$} by \gongshi{ $\mathbf{A}_{j}$}.

% Mini-batch Training via Historical Embeddings: GAS, GraphFM, LMC

% Opt: Expressiveness theory

\vspace{-2mm}

\subsection{Graph Convolutional Networks} \label{sec:gnn}


For simplicity of the derivation, we present our algorithm with graph convolutional networks (GCNs) \citep{gcn}. However, our algorithm is also applicable to arbitrary message passing-based GNNs (see Appendix \ref{sec:TOP_for_GNNs}).




A graph convolution layer is defined as
\begin{align}
     \mathbf{H}^{(l+1)} = f^{(l+1)}(\mathbf{H}^{(l)}, \widetilde{\mathbf{A}}) =\sigma(\mathbf{Z}^{(l+1)}  \mathbf{W}^{(l)}) =  \sigma(\widetilde{\mathbf{A}}\mathbf{H}^{(l)} \mathbf{W}^{(l)})
    ,\,\,(l+1)\in \llbracket L \rrbracket, \label{eqn:transformation_conv}
\end{align}
where \gongshi{$\widetilde{\mathbf{A}} = (\mathbf{D}+\mathbf{I})^{-1/2}(\mathbf{A}+\mathbf{I})(\mathbf{D}+\mathbf{I})^{-1/2}$} is the normalized adjacency matrix and \gongshi{$\mathbf{D}$} is the in-degree matrix (\gongshi{$\mathbf{D}_{uu}=\sum_{v}\mathbf{A}_{uv}$}).
The initial node feature is \gongshi{$\mathbf{H}^{(0)}=\mathbf{X}$}, \gongshi{$\sigma $} is an activation function, and \gongshi{$\mathbf{W}^{(l)}$} is a trainable weight matrix.
For simplicity, we denote the GNN parameters \gongshi{$\{\mathbf{W}^{(l)}\}_{l=0}^{L-1}$} by \gongshi{$\mathcal{W}$}.
Thus, GCNs take node features and the normalized adjacency matrix \gongshi{$(\mathbf{X}, \widetilde{\mathbf{A}})$} as input
\begin{align*}
    \mathbf{H}^{(L)} = \gcn(\mathbf{X}, \widetilde{\mathbf{A}}),
\end{align*}
where \gongshi{$\gcn = f^{(L)} \circ f^{(L-1)} \dots \circ f^{(1)}$}.



% and \gongshi{$\mathbf{C}^{(l,s)} \in \mathbb{R}^{n \times n}$} is the $s$-th convolution support at the $l$-th layer that defines how the node features and embeddings are propagated to the neighbors. For example, graph convolutional networks (GCNs) \citep{gcn} use \gongshi{$\mathbf{C}^{(l,s)} = \widetilde{\mathbf{A}}$} as the convolution support, where 
% Appendix x summarizes many popular GNN models satisfying the framework.



% \subsection{GNNs defined as Graph Convolution} \label{sec:gnn}



% Graph neural networks (GNNs) follow a \textit{neural message passing framework} \citep{mpnn}, which exchanges vector messages between nodes and their neighbors using neural networks.
% Thus, the {\small$l$}-th layer of GNNs takes the node embeddings and the adjacency matrix as input, i.e.,
% \begin{align}
%     \mathbf{H}^{(l+1)}=f^{(l+1)}(\mathbf{H}^{(l-1)};\mathbf{A}),\,\,l\in[L]. \label{eqn:transformation_conv}
% \end{align}
% The message passing function $f^{(l+1)}$ consists of three stages: the message generation $g^{(l+1)}$, the message aggregation step $\aggregate^{(l+1)}$, and the update step $\update^{(l)}$ as follows
% \begin{align}
%     \mathbf{h}_i^{(l+1)}  =\update^{(l+1)}\left(\mathbf{h}_i^{(l)}, \mathbf{m}^{(l+1)}_{\mathcal{N}(i)}\right); \quad \mathbf{m}^{(l+1)}_{\mathcal{N}(i)}  =\aggregate^{(l+1)}\left( {\left\{g^{(l)}(\mathbf{h}^{(l)}_j) \mid j\in\mathcal{N}(i)\right\}}\right),\,\,l\in[L],\label{eqn:mp}
% \end{align}
% For simplicity of the derivation, we mainly focus on graph convolutional networks






% As pointed out by \citep{spectral_gnn, sgc}, 
% % % https://proceedings.neurips.cc/paper/2021/file/3569df159ec477451530c4455b2a9e86-Paper.pdf
% an $L$-layer graph neural network (GNN) iteratively updates node representations in three stages: feature propagation \gongshi{$\mathbf{Z}^{(l,s)} = \mathbf{C}^{(l,s)}\mathbf{H}^{(l)}$}, feature transformation \gongshi{$\sum_s \mathbf{Z}^{(l,s)}\mathbf{W}^{(l,s)}$}, and nonlinear transition \gongshi{$f$} as follows
% \begin{align}
%      \mathbf{H}^{(l+1)} =  f^{(l+1)}(\sum_{s}\mathbf{C}^{(l,s)}\mathbf{H}^{(l)} \mathbf{W}^{(l,s)})
%     ,\,\,(l+1)\in[L], \label{eqn:transformation_conv}
% \end{align}
% where \gongshi{$\mathbf{H}^{(0)}=\mathbf{X}$} and \gongshi{$\mathbf{C}^{(l,s)} \in \mathbb{R}^{n \times n}$} is the $s$-th convolution support at the $l$-th layer that defines how the node features and embeddings are propagated to the neighbors. For example, graph convolutional networks (GCNs) \citep{gcn} use \gongshi{$\mathbf{C}^{(l,s)} = \widetilde{\mathbf{A}}$} as the convolution support, where \gongshi{$\widetilde{\mathbf{A}} = (\mathbf{D}+\mathbf{I})^{-1/2}(\mathbf{A}+\mathbf{I})(\mathbf{D}+\mathbf{I})^{-1/2}$} is the normalized adjacency matrix and \gongshi{$\mathbf{D}$} is the diagonal degree matrix.
% Appendix x summarizes many popular GNN models satisfying the framework.
% the general graph convolution framework and refer readers to the appendix for more details

% Let \gongshi{$\mathbf{D}$} be the diagonal degree matrix of $\mathbf{A}$.
% The normalized adjacency matrix is \gongshi{$\widetilde{\mathbf{A}} = (\mathbf{D}+\mathbf{I})^{-1/2}(\mathbf{A}+\mathbf{I})(\mathbf{D}+\mathbf{I})^{-1/2}$}.






% The neighbor explosion issue is mainly due to feature propagation \gongshi{$\mathbf{Z}^{(l,s)} = \mathbf{C}^{(l,s)}\mathbf{H}^{(l)}$}. Specifically,  the mini-batch embeddings at the $(l+1)$-th layer \gongshi{$\mathbf{H}^{(l+1)}_{\mathcal{B}}=f^{(l+1)}(\sum_s \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{N}({\mathcal{B}})}\mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})} \mathbf{W}^{(l,s)})$} recursively depend on \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})}$} at the $l$-th layer.
% Thus, the dependencies of nodes (i.e., \gongshi{$\mathbf{H}^{(L)}_{\mathcal{B}}$} depends on \gongshi{$\mathbf{H}^{(0)}_{\mathcal{N}^L(\mathcal{B})}$}) are exponentially increasing with the number of layers \gongshi{$L$} due to \gongshi{$\mathcal{O}(|\mathcal{N}^L(\mathcal{B})|)= \mathcal{O}(|\mathcal{B}| deg_{\max}^L)$} with the maximum degree \gongshi{$deg_{\max}$}.

The neighbor explosion issue is mainly due to feature propagation \gongshi{$\mathbf{Z}^{(l+1)} = \widetilde{\mathbf{A}}\mathbf{H}^{(l)}$}. Specifically,  the mini-batch embeddings at the \gongshi{$(l+1)$}-th layer 
\begin{align}\label{eqn:mp_all}
    \mathbf{H}^{(l+1)}_{\mathcal{B}}&= \sigma\left(\mathbf{Z}^{(l+1)}_{\mathcal{B}}  \mathbf{W}^{(l)}\right) =\sigma\left(\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}}\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}} \mathbf{W}^{(l)}\right) % \nonumber \\
    % &=\sigma\left(( \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}} \mathbf{H}^{(l)}_{\mathcal{B}} + \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}^c}\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c}) \mathbf{W}^{(l)}  \right) \label{eqn:mp_all}
\end{align}
recursively depend on \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}}$} at the \gongshi{$l$}-th layer.
Thus, the dependencies of nodes (i.e., \gongshi{$\mathbf{H}^{(L)}_{\mathcal{B}}$} depends on \gongshi{$\mathbf{H}^{(0)}_{\mathcal{N}^L_{\mathcal{B}}}$}\footnote{\gongshi{$\mathcal{N}^L_{\mathcal{B}}=\| [\widetilde{\mathbf{A}}^L]_{\mathcal{B}} \|_0$}.}) are exponentially increasing with respect to the number of layers \gongshi{$L$} due to \gongshi{$\mathcal{O}(|\mathcal{N}^L_{\mathcal{B}}|)= \mathcal{O}(|\mathcal{B}| deg_{\max}^L)$} with the maximum degree \gongshi{$deg_{\max}$}.




% the mini-batch embeddings at the $L$-th layer \gongshi{$\mathbf{H}^{(L)}_{\mathcal{B}}$} grows exponentially with the number of MP layers.
%  follow the message passing framework in which vector messages are exchanged between nodes and updated using neural networks.
% % An \gongshi{$L$}-layer GNN performs \gongshi{$L$} message passing iterations to generate the final node embeddings \gongshi{$\mathbf{H}^{(L)}$} as
% \begin{align}
%     \mathbf{H}^{(l)}=f_{\theta^{l}}(\widetilde{\mathbf{A}}\mathbf{H}^{(l-1)}, \mathbf{H}^{(l-1)}; \mathbf{X}),\,\,l\in[L], \label{eqn:transformation_conv}
% \end{align}
% where \gongshi{$\mathbf{H}^{0}=\mathbf{X}$} and \gongshi{$f_{\theta^l}$} is the \gongshi{$l$}-th message passing layer with parameters \gongshi{$\theta^{l}$}.


% \subsection{Subgraph Sampling}  \label{sec:gcm}

% % Subgraph sampling is a general mini-batch framework to accelerate the training of variant GNNs.
% % % During the training phase of the model, subgraph sampling is a general framework for a wide range of GNN architectures.
% Subgraph sampling is a general mini-batch framework for a wide range of GNN architectures.
% For example, subgraph sampling  directly runs a GCN on the subgraph induced by a mini-batch \gongshi{$\mathcal{B}$}
% \begin{align*}
%     \mathbf{H}^{(L)}_{\mathcal{B}} \approx  \gcn(\mathbf{X}_{\mathcal{B}}, \norm(\mathbf{A}_{\mathcal{B},\mathcal{B}})),
% \end{align*}
% where \gongshi{$\norm (\cdot)$} normalizes the adjacency matrix of the subgraph \gongshi{$\mathbf{A}_{\mathcal{B},\mathcal{B}}$}.
% For example, CLUSTER-GCN \citep{cluster_gcn} and GraphSAINT \citep{graphsaint} use \gongshi{$\norm(\mathbf{A}_{\mathcal{B},\mathcal{B}}) = \widetilde{\mathbf{A}_{\mathcal{B},\mathcal{B}}}$} and \gongshi{$\norm(\mathbf{A}_{\mathcal{B},\mathcal{B}}) = \mathbf{D}^{-1}_{\mathcal{B},\mathcal{B}}\mathbf{A}_{\mathcal{B},\mathcal{B}}$} respectively.






% % We define the sampling bias as
% % \begin{align*}
% %     % \mathbb{E}_{\mathcal{B}}[ \| \mathbf{H}^{(L)}_{\mathcal{B}} - \mathbf{H}^{(L)}_{\mathcal{B}} \|_F]
% %     \mathbb{E}_{\mathcal{B}} \left[ \| \gcn(\mathbf{X}_{\mathcal{B}}, \norm(\mathbf{A}_{\mathcal{B},\mathcal{B}}))  - \gcn(\mathbf{X}, \widetilde{\mathbf{A}})_{\mathcal{B}} \|_F \right].
% % \end{align*}



% % The mini-batch selection of subgraph sampling is motivated by an observation that, if the neighbors of \gongshi{$\mathcal{B}$} are as well belong to \gongshi{$\mathcal{B}$}, i.e., \gongshi{$\mathcal{N}_{\mathcal{B}}= \mathcal{B}$}, then the feature propagation on the subgraph \gongshi{$\widetilde{\mathbf{A}_{\mathcal{B},\mathcal{B}}}\mathbf{H}^{(l)}_{\mathcal{B}} = \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}}\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}}$} is exact. 


% % \udfsection{Mini-batch selection of subgraph sampling.} 
% % The subgraph \gongshi{$\mathbf{A}_{\mathcal{B},\mathcal{B}}$}
% Compared with the whole message passing in Equation \eqref{eqn:mp_all}, subgraph sampling drops the edges \gongshi{$\mathcal{N}_{\mathcal{B}}^c \rightarrow \mathcal{B}$} from the original graph \gongshi{$\mathbf{A}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}}$}, leading to significant approximation errors. Thus, the mini-batch selection of subgraph sampling aims to \textit{minimize the graph cut} from a topological similarity perspective, i.e.,
% \begin{align}
%     \min_{ \mathcal{B}} \| (\mathbf{A}_{\mathcal{B},\mathcal{B}}, \mathbf{O}) -  \mathbf{A}_{\mathcal{B},\mathcal{N}_{\mathcal{B}} } \|_0=\min_{ \mathcal{B}}|\mathcal{N}_{\mathcal{B}}^c|,\label{eqn:cut_minimization}
%     % \min_{ \mathcal{B}_1, \mathcal{B}_2,\dots, \mathcal{B}_n} \| \diag(\mathbf{A}_{\mathcal{B}_1,\mathcal{B}_1}, \mathbf{A}_{\mathcal{B}_2,\mathcal{B}_2}, \dots, \mathbf{A}_{\mathcal{B}_n,\mathcal{B}_n}) - \mathbf{A}_{\mathcal{V}',\mathcal{V}'} \|_0, \label{eqn:cut_minimization}
% \end{align}
% where \gongshi{$\mathbf{O} \in \mathbb{R}^{|\mathcal{B}|, |\mathcal{N}_{\mathcal{B}}^c|}$} is a zero matrix.
% %  between the sampled subgraphs \gongshi{$\mathbf{A}_{\mathcal{B},\mathcal{B}}$} and the original graph \gongshi{$\mathbf{A}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}}$}
% Notably, as a connected graph cannot be divided into two disjointed subgraphs without dropping edges, the optimal value of \eqref{eqn:cut_minimization} is always positive in the connected graph.

% % where \gongshi{$\gnn = f^{(L)} \circ f^{(L-1)} \dots \circ f^{(1)}$}.
% % As the optimal value of \eqref{eqn:cut_minimization} is always positive in the connected graph, existing subgraph sampling methods suffer from the sampling bias due to the dropped neighborhood embeddings $\mathbf{H}_{\mathcal{N}^{(l)}(\mathcal{B})-\mathcal{B}}$.



% % As the feature propagation discards the edges to the out-of-batch neighbors \gongshi{$\mathcal{N}({\mathcal{B}})-\mathcal{B}$}, the selection for the mini-batch \gongshi{$\mathcal{B}$} aims to minimize the graph cut \gongshi{$|\mathcal{N}({\mathcal{B}})-\mathcal{B}|$} \citep{cluster_gcn, graphsaint, gas, lmc, graphfm}, 
% % % \gongshi{$\mathcal{V}' = (\mathcal{B}_1, \mathcal{B}_2,\dots, \mathcal{B}_n)$} is a node ordering and \gongshi{$\diag$} merges the diagonal blocks \gongshi{$\mathbf{A}_{\mathcal{B}_i}$} into a modified adjacency matrix in $\mathbb{R}^{n \times n}$.



% To minimize the graph cut \gongshi{$|\mathcal{N}_{\mathcal{B}} - \mathcal{B}|$}, the cluster-based samplers \citep{cluster_gcn, gas, lmc, graphfm} first adopt graph clustering (e.g., METIS \citep{metis1} and Graclus \citep{graclus}) to partition the large-scale graph into \gongshi{$\{\mathcal{B}_1, \mathcal{B}_2, \dots, \mathcal{B}_n\}$} with small \gongshi{$|\mathcal{N}_{{\mathcal{B}_i}}-\mathcal{B}_i|$} and then sample a subgraph induced by $\mathcal{B}_i$.
% Besides, the random-walk based  sampler \citep{graphsaint} first uniformly samples root nodes and then generates random walks $\mathcal{B}$ starting from the root nodes, which decreases the graph cut \citep{dlg}. %  \gongshi{$|\mathcal{N}_{\mathcal{B}} - \mathcal{B}|$}


% % subgraphs and the original graph，不太行
% % One of the major challenges in subgraph sampling is to reduce a significant sampling bias due to a large number of discarded messages outside the mini-batch.
% % % to decrease the number of discarded messages 
% % One straightforward idea is to sample subgraphs based on graph cut minimization, i.e., minimizing the number of discarded messages outside the mini-batch).
% % For example, cluster-based subgraph sampling adopts graph clustering (e.g., METIS \citep{metis1} and Graclus \citep{graclus}) to partition the large-scale graph into a set of small subgraphs with low inter-connectivity.
% % Besides, \citep{graphsaint} proposes a random-walk based  sampler to decrease the inter-connectivity of subgraphs based on uniformly sampled root nodes.





% % 子图采样需要构建子图
% % 子图构建包含两个环节，一个是mini-batch of nodes选择，一个是邻接矩阵构建



% % \udfsection{Feature propagation of subgraph sampling.} % 删掉，或者往后放,GNN改成GCN

% % To tackle the neighbor explosion issue, subgraph sampling propagates features on the subgraph induced by the mini-batch \gongshi{$\mathcal{B}$}, e.g. \gongshi{$\mathbf{Z}^{(l+1)}_{\mathcal{B}} \approx \widetilde{\mathbf{A}_{\mathcal{B},\mathcal{B}}}\mathbf{H}^{(l)}_{\mathcal{B}}$} \citep{cluster_gcn,graphsaint} or \gongshi{$\mathbf{Z}^{(l+1)}_{\mathcal{B}} \approx \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}}\mathbf{H}^{(l)}_{\mathcal{B}} + \mathbf{B}$} \citep{gas, lmc}, where \gongshi{$\mathbf{B}=\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}({\mathcal{B}})-\mathcal{B}}\overline{\mathbf{H}}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}$} ignores the out-of-batch gradients \gongshi{$\nabla_{\overline{\mathbf{H}}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}} \mathbf{Z}^{(l+1)}_{\mathcal{B}}$} by nondifferentiable historical embeddings \gongshi{$\overline{\mathbf{H}}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}$}. 
% % % The convolution support \gongshi{$\widetilde{\mathbf{A}_{\mathcal{B},\mathcal{B}}}$} normalizes the submatrix \gongshi{$\mathbf{A}_{\mathcal{B},\mathcal{B}}$}.
% % We also introduce more related works to address the neighbor explosion issue in Appendix X. % \ref{sec:related_work}.


% % % Subgraph sampling has two steps: the mini-batch selection and the subgraph construction.










% % Given a mini-batch \gongshi{$\mathcal{B}$}, 


% % \subsection{Historical Embeddings}\label{sec:hist_emb}

% % To tackle the neighbor explosion issue, GAS \citep{gas} propose historical embeddings of out-of-batch nodes \gongshi{$\overline{\mathbf{H}}^{(l)}_{\mathcal{N}(\mathcal{B})-\mathcal{B}}$} from prior training iterations to avoid the recursive dependencies. Specifically, GAS approximates the feature propagation stage by
% % \begin{align}  \label{eqn:mini_batch} 
% %     \mathbf{Z}^{(l,s)}_{\mathcal{B}} = \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{N}({\mathcal{B}})}\mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})} &= \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{B}}\mathbf{H}^{(l)}_{\mathcal{B}} + \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{N}({\mathcal{B}})-\mathcal{B}}\mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}} \\  \label{eqn:mini_batch_gas}
% %     &\approx \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{B}}\mathbf{H}^{(l)}_{\mathcal{B}} + \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{N}({\mathcal{B}})-\mathcal{B}}\overline{\mathbf{H}}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}.
% % \end{align}
% % % The required historical embeddings are pulled from an offline storage, instead of being re-computed in each iteration, which keeps the required information for each batch local.
% % As GAS pulls the historical embeddings \gongshi{$\overline{\mathbf{H}}^{(l)}_{\mathcal{N}(\mathcal{B})-\mathcal{B}}$} from an offline storage (e.g. CPU memory) instead of re-computing them in each iteration, the dependencies of nodes are limited in the one-hop neighbors of the mini-batch \gongshi{$\mathcal{N}(\mathcal{B})$} with a constant size \gongshi{$|\mathcal{N}_{\mathcal{B}}| \leq |\mathcal{B}|deg_{\max}$}.



% % % Weaknesses of historical embeddings
% % However, the historical embeddings suffer from large approximation errors due to the staleness issue \citep{gas, graphfm}.
% % First, GAS updates the historical embeddings in each mini-batch average once per epoch and keeps their values between two consecutive updates of the mini-batch historical embeddings.
% % Thus, on large graphs, the low update frequency \gongshi{$|\mathcal{B}|/|\mathcal{V}|$} of \gongshi{$\overline{\mathbf{H}}^{(l)}$} leads to large approximation errors \gongshi{$\|\overline{\mathbf{H}}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}} - \mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}} \|_F$}.
% % Second, batch normalization \citep{batch_norm} in the nonlinear transition $f$---a widely used technique to accelerate training---uses different mean and variance statistics of \gongshi{$\mathbf{H}^{(l)}_{\mathcal{B}}$ and $\overline{\mathbf{H}}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}$} \citep{ineffectiveness_vr}. Specifically, the mean and variance statistics of \gongshi{$\mathbf{H}^{(l)}_{\mathcal{B}}$} are calculated within the mini-batch \gongshi{ $\mathcal{B}$} and those of \gongshi{$\overline{\mathbf{H}}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}$} are calculated within other mini-batches due to % \gongshi{$\mathcal{N}({\mathcal{B}})-\mathcal{B} \nsubseteq \mathcal{B}$}.
% % % differing from \gongshi{ $\mathcal{B}$}.
% % % from the update rule of 


% % GNNs need to compute the embeddings for all nodes within a graph during the evaluation phase, not merely for those nodes present in the evaluation dataset. Although the nodes in the evaluation dataset constitute only a fraction of the entire graph, computing embeddings of all nodes ensures exact results during the model evaluation process.

% % Bias may occur if the computation during the evaluation phase is restricted only to specific, directly relevant nodes, while excluding others. Such excluded nodes might hold essential information that affects the overall structure of the graph and the nodes' decision-making processes. For instance, a node's features may rely on those of its neighboring nodes, which themselves may depend on their respective neighbors. Overlooking any nodes, therefore, can result in the loss of critical information, thereby compromising the model’s accuracy.

% % If the computation during the evaluation phase is limited only to the nodes in the evaluation dataset and excludes other nodes, bias will occur. These excluded nodes may contain important information that affects the graph's overall structure and neighboring nodes' features. For example, the features of a node may depend on the features of its neighbor nodes, which in turn may depend on their respective neighbors. Therefore, ignoring any nodes can lead to the loss of crucial information, thus compromising the accuracy of the model.

% % When GNNs calculate embeddings for all nodes within the graph during the evaluation phase, this comprehensive approach ensures the acquisition and learning of global information in an unbiased manner. Every node in the graph, potentially connected through edges to others, has a role in directly or indirectly shaping the feature representations of adjacent nodes. Omitting any node from the computation introduces biases, distorting the prediction outcomes. By including every node, GNNs guarantee that the computations for each node are grounded in the most accurate and unbiased information available. This ensures that the neighbor information utilized in each message passing step is precise and reliable, thereby preserving the integrity and accuracy of the model’s predictions.


% % Jumping Knowledge Networks is a GNN architecture that utilizes feature representations of different hierarchical neighbors. JK-Net aggregate embeddings from different layers:
% % \begin{align}
% %       \mathbf{H} = \text{Concat}(\mathbf{H}^{(1)}, \mathbf{H}^{(2)}, \ldots, \mathbf{H}^{(K)}), 
% % \end{align}
% % where K is the is the total number of layers.

% % During the training phase, JK-Net requires the complete graph representation, denoted as \gongshi{$\mathbf{H}$}. JK-Net only needs the embeddings of the subgraph on the mini-batch, which is recorded as \gongshi{$\mathbf{H}_\mathcal{B}$}, corresponding to the mini-batch. Specifically, 

% % \begin{align}
% %       \mathbf{H}_B = \text{Concat}(\mathbf{H}_B^{(1)}, \mathbf{H}_B^{(2)}, \ldots, \mathbf{H}_B^{(K)}), 
% % \end{align}
% % where \gongshi{$\mathbf{H}_B^{(i)}$} is the embedding of the i-th layer in the mini-batch.

\vspace{-2mm}

\section{Message Invariance}


In this section, we elaborate on message invariance in detail. 
We first present the definition of message invariance in Section \ref{subsec:ms}.
% the topological compensation by a novel embedding discrepancy minimization. 
We then provide a case study for message invariance in Section \ref{sec:case_study}.
% Finally, we introduce local message invariance in Section \ref{subsec:local_mi}, which is more easily satisfied in practice than global message invariance.
% Next, we give an accurate estimation of TOP in Sections \ref{subsec:embedding_discrepancy_minimization} and \ref{subsec:basic_embeddings}.
% Finally, we present extensive experiments to demonstrate the existence of message invariance in various real-world datasets in Section \ref{subsec:sampling_bias_of_different_methods}.

% estimate the sampling bias of TOP by embedding discrepancy minimization in Section \ref{subsec:embedding_discrepancy_minimization}. 
% Then in Section \ref{subsec:basic_embeddings}, we study how to select basic embeddings, which are important to reduce the sampling bias.
% Finally, we empirically compare the sampling bias of different methods in Section \ref{subsec:sampling_bias_of_different_methods}.

% First, in Section 3.1, we describe the approach to representing MILP instances as bipartite graphs. Then, in Section 3.2, we derive the masked variational autoencoder (VAE) generative paradigm. In Section 3.3, we provide details on the implementation of the model framework. Finally, in Section 3.4, we explain the training and inference processes. The model overview is in Figure 1. More implementation details can be found in Appendix A. 

\vspace{-2mm}

\subsection{Message Invariance} \label{subsec:ms}


We first separate the mini-batch feature propagation in Equation \eqref{eqn:mp_all} into two parts, i.e.,
% \begin{align}
%     \mathbf{Z}^{(l+1)}_{\mathcal{B}}
%     = \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}} \mathbf{H}^{(l)}_{\mathcal{B}} + \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}}\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}}. 
% \end{align}
% \begin{align}
%     \mathbf{Z}^{(l+1)}_{\mathcal{B}}
%     = \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}} \mathbf{H}^{(l)}_{\mathcal{B}} + \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{OB}}\mathbf{H}^{(l)}_{\mathcal{OB}}
% \end{align}
\begin{align}
    \mathbf{Z}^{(l+1)}_{\mathcal{B}}
    = \underbrace{\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}} \mathbf{H}^{(l)}_{\mathcal{B}}}_{\textrm{\footnotesize $\text{MP}_{\text{IB}}$}} + \underbrace{\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}^c}\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c}}_{\textrm{\footnotesize $\text{MP}_{\text{OB}}$}},  \label{eqn:mini_batch}
\end{align}
where $\text{MP}_{\text{IB}}$ and $\text{MP}_{\text{OB}}$ denote message passing between the in-batch nodes and message passing from their out-of-batch neighbors to the in-batch nodes respectively.


To avoid the recursive dependencies induced by $\text{MP}_{\text{OB}}$, we first introduce a novel concept of (global) message invariance, which bridges the gap between costly $\text{MP}_{\text{OB}}$ and fast $\text{MP}_{\text{IB}}$.
\begin{definition}[Message invariance]
    We say that a transformation \gongshi{$g: \mathbb{R}^{|B| \times d} \rightarrow  \mathbb{R}^{|\mathcal{N}_{\mathcal{B}}^c| \times d}$} is message-invariant if it satisfies
    \begin{align}\label{eqn:nonlinear_extrapolation}
        \mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} = g( \mathbf{H}^{(l)}_{\mathcal{B}} ).
    \end{align}
    for any GNN parameters \gongshi{$\mathcal{W}$}.
\end{definition}
Given the message invariance, the composition of the original {$\text{MP}_{\text{OB}}$} operator \gongshi{$\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}^c}: \mathbb{R}^{|\mathcal{N}_{\mathcal{B}}^c|\times d} \rightarrow \mathbb{R}^{|\mathcal{B}|\times d} $} and the transformation \gongshi{$g: \mathbb{R}^{|\mathcal{B}|\times d} \rightarrow \mathbb{R}^{|\mathcal{N}_{\mathcal{B}}^c|\times d}$} leads to a new $\text{MP}_{\text{IB}}$ operator \gongshi{$(\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}^c} g): \mathbb{R}^{|\mathcal{B}|\times d} \rightarrow \mathbb{R}^{|\mathcal{B}|\times d} $}. Thus, the mini-batch feature propagation \eqref{eqn:mini_batch} becomes
\begin{align} \label{eqn:top}
    \mathbf{Z}^{(l+1)}_{\mathcal{B}} &= \underbrace{\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}} 
 \mathbf{H}^{(l)}_{\mathcal{B}}}_{\textrm{\footnotesize $\text{MP}_{\text{IB}}$}} + \underbrace{\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}^c} g(\mathbf{H}^{(l)}_{\mathcal{B}})}_{\textrm{\footnotesize $\text{MP}_{\text{IB}}$}},
\end{align}
which is independent of the neighborhood embeddings \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}_{{\mathcal{B}}}^c}$}.
Therefore, the message-invariant transformation \gongshi{$g$} avoids the recursive dependencies and expensive costs of out-of-batch neighbors.




\subsection{A Case Study for Message Invariance}
\label{sec:case_study}

% Next, we discuss the existence of the message invariance. 
Due to the arbitrariness of graph structures and the nonlinearity of GNNs, the formula of the message-invariant transformation \gongshi{$g$} is usually unknown. Here we provide a case study for a specific form of \gongshi{$g$} by simplifying the graph structures or the GNN architectures. 
The case study will motivate us to estimate the message-invariant transformation \gongshi{$g$} in Section \ref{subsec:formulation_of_TOP}.






% by a case study.

% The practical conditions of 

% Practical conditions


% the iterative methods solving Equation (1), while the convergence analysis is generally intractable due to the arbitrariness of F(v) (usually nonlinear by involving vv?), here we provide a case study for a specific form of F(v) whose analytical ground truth solutions are available.


% 接下来，我们讨论Global message invariance的存在性通过a case study.
% 分析的难点主要在于图结构和模型结构的任意性，因此我们先提供两个例子，他们分别对图结构和GNN结构做一定简化，使得消息对称性成立。


% 再以上的case study中，我们固定了图结构或者模型结构。然而，只要图结构和模型结构都可以任意给定，我们总是很容易找到例子message invariance不成立。为了处理这种情况，我们展示局部消息对称性。这种弱的条件能够保证消息对称性的存在



\subsubsection{Message Invariance on Graph with Symmetry}\label{sec:mi_symmetry}

% 在图1中，节点特征是有限的，GNN结构可以任意的。由于排序对称性，图中只有两类节点S1={1,2,5,6}, S2={3,4}，其中同一集合中的节点都是两两同构的cite。因此，消息不变的变换为xxx。


% 值得注意的是，在图1中，构建mini-batch事实上并不需要对称的。如果我们构建的mini-batch是v1, v3，g同样很容易得到，因为这一mini-batch已经包含所有具有代表性的节点，分别来自S1和S2。
% 启发于这一观察，我们在section x中证明，随着mini-batch采样节点的增多，代表性节点出现在mini-batch的比例更大，也因此更可能找到消息不变的transformation




The first example is shown in Figure \ref{fig:top}, where the node features are finite and the GNN architectures are arbitrary. Due to the permutation equivariance of GNNs, the nodes in the graph are categorized into two sets \gongshi{$S_1=\{v_1,v_2,v_5,v_6\}$} and \gongshi{$S_2=\{v_3,v_4\}$}, where the nodes in the same set are isomorphic to each other. The embeddings of isomorphic nodes are always the same, regardless of the GNN architectures. Therefore, the message-invariant transformation is
\begin{align*}
    \mathbf{h}_4^{(l)} = g(\mathbf{h}_1^{(l)}, \mathbf{h}_2^{(l)}, \mathbf{h}_3^{(l)}) = 0\cdot \mathbf{h}_1^{(l)} + 0\cdot\mathbf{h}_2^{(l)} + 1\cdot\mathbf{h}_3^{(l)}.
\end{align*}


Notably, the selection of mini-batches does not require considering the symmetry of the graph in Figure \ref{fig:top}. If the mini-batch \gongshi{$\mathcal{B}$} consists of two nodes \gongshi{$v_2$} and \gongshi{$v_3$} from \gongshi{$S_1$} and \gongshi{$S_2$} respectively, then finding \gongshi{$g$} is still easy by \gongshi{$\mathbf{h}_1^{(l)}=1\cdot\mathbf{h}_2^{(l)}+0\cdot\mathbf{h}_3^{(l)}$} and \gongshi{$\mathbf{h}_4^{(l)}=0\cdot\mathbf{h}_2^{(l)}+1\cdot\mathbf{h}_3^{(l)}$}. In the example, the condition for the existence of the message-invariant transformation is that the mini-batch \gongshi{$\mathcal{B}$} contains at least one node from each of \gongshi{$S_1$} and \gongshi{$S_2$}.


The example discusses a small graph with six nodes, while many real-world graphs contain millions of nodes. From a probabilistic perspective, the sets \gongshi{$S_1$} and \gongshi{$S_2$} represent two peaks of the data distribution. Then, the condition becomes that the mini-batch \gongshi{$\mathcal{B}$} contains the most frequent node inputs (the node features and their neighborhood structures). These frequent node inputs are also sampled with a high probability under a large enough batch size. Thus, the message-invariant transformation is easy to find in large-scale graphs. We provide the detailed formulation and theory in Appendix \ref{subsubsec:selection_and_isomorphic}.
% We also empirically demonstrate the existence of message invariance in many real-world datasets in Section \ref{subsec:sampling_bias_of_different_methods}.





\subsubsection{Message Invariance for Linear GNNs}


We use linear GNNs  \citep{acsc, linear_gnn} as the second example, which simplifies the GNN architectures without restricting the graph structures. 
% \udfsection{Example for linear GNNs \citep{acsc, linear_gnn}.}
Linear GNNs use an identity mapping \gongshi{$\sigma$} as the activation function.
For linear GNNs \gongshi{$\mathbf{H}^{(l)}=\widetilde{\mathbf{A}}^l \mathbf{X}\mathbf{W}^{(0)} \dots \mathbf{W}^{(l-1)}$}, the linear dependence between embeddings \gongshi{$\mathbf{H}^{(l)}$} is equal to the linear dependence between the corresponding parameter-free features $\widetilde{\mathbf{A}}^l \mathbf{X}$. Specifically, if the $l$-hop features \gongshi{$\mathbf{X}^{(l)}_{\mathcal{B}} = (\widetilde{\mathbf{A}}^l \mathbf{X})_{\mathcal{B}}$} is a full-column-rank matrix, then there exists a coefficient matrix \gongshi{$\mathbf{R}$} such that \gongshi{$ \mathbf{X}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} = \mathbf{R} \mathbf{X}^{(l)}_{\mathcal{B}}$}. Then, the linear dependence between embeddings is
\begin{align*}
    \mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} 
 = \mathbf{X}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} \mathbf{W}^{(0)} \dots \mathbf{W}^{(l-1)}= \mathbf{R} \mathbf{X}^{(l)}_{\mathcal{B}} \mathbf{W}^{(0)} \dots \mathbf{W}^{(l-1)}=\mathbf{R} \mathbf{H}^{(l)}_{\mathcal{B}}.
\end{align*}
Thus, the message-invariant transformation $g$ in Equation \eqref{eqn:nonlinear_extrapolation} is a linear transformation for the coefficient matrix \gongshi{$\mathbf{R}$}.


% If the \gongshi{$\mathbf{X}^{(l)}_{\mathcal{B}} = (\widetilde{\mathbf{A}}^l \mathbf{X})_{\mathcal{B}}$} is a full-column-rank matrix, then the message-invariant transformation is a linear transformation for the coefficient matrix \gongshi{$\mathbf{R}$}, i.e.,
% \begin{align*}
%     \mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} = g( \mathbf{H}^{(l)}_{\mathcal{B}}) = \mathbf{R} \mathbf{H}^{(l)}_{\mathcal{B}},
% \end{align*}
% where the coefficient matrix \gongshi{$\mathbf{R}$} is given by \gongshi{$\underset{{\mathbf{R}}}{\min}\| \mathbf{X}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} - \mathbf{R} \mathbf{X}^{(l)}_{\mathcal{B}} \|_F$}.
% Equation \eqref{eqn:nonlinear_extrapolation} always holds due to
% \begin{align*}
%     \mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} 
%  = \mathbf{X}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} \mathbf{W}^{(0)} \dots \mathbf{W}^{(l-1)}= \mathbf{R} \mathbf{X}^{(l)}_{\mathcal{B}} \mathbf{W}^{(0)} \dots \mathbf{W}^{(l-1)}=\mathbf{R} \mathbf{H}^{(l)}_{\mathcal{B}}.
% \end{align*}



% In the example, the linearity of model architectures leads to the linear independence between embeddings of the in-batch nodes and their out-of-batch neighbors.
For non-linear GNNs, the relation between embeddings of the in-batch nodes and their out-of-batch neighbors may be non-linear. Nonetheless, on the real-world datasets, the linear message-invariant transformation has achieved marginal approximation errors in practice as shown in Section \ref{subsec:sampling_bias_of_different_methods}.




% \subsection{Local Message Invariance}\label{subsec:local_mi}
 
% Although we have provided a case study in Section \ref{sec:case_study} to discuss the formula of message-invariant transformation, finding an efficient and effective formula on any graphs with non-linear GNNs is still challenging. To address the challenge, we introduce local message invariance, which is more easily satisfied in practice than global message invariance.
% \begin{definition}[Local message invariance]
%     We say that a transformation $g: \mathbb{R}^{|B| \rightarrow \mathcal{N}_{\mathcal{B}}^c}$ is locally message-invariant if it satisfies
%     \begin{align*}
%         \mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} = g( \mathbf{H}^{(l)}_{\mathcal{B}} ).
%     \end{align*}
%     for GNN parameters $\mathcal{W}$ in an $\epsilon$-ball neighborhood of $\mathcal{W}^{*}$, i.e., $B_{\epsilon}(\mathcal{W}^{*}) = \{(\mathbf{W}^{(l)})_{l=0}^{L-1} | \| \mathbf{W}^{(l,*)} - \mathbf{W}^{(l)}\|_F \leq \epsilon  \}$.
%     If $\epsilon=+\infty$, then the local message invariance is equal to the global message invariance.
% \end{definition}





% % The local message invariance has the following appealing features. First, the locally message-invariant transformation does not affect the original GNN gradients in the $\epsilon$ ball as shown in Theorem x, , 
% Compared with global message invariance, the transformation $g$ induced by local message invariance is linear as shown in Theorem \ref{thm:lmi_top} and thus it is easy to estimate. Once the parameters $\mathcal{W}^{+}$ escape from the $\epsilon$ ball $B_{\epsilon}(\mathcal{W}^*)$, then we can rapidly estimate a new transformation $g^{+}$ which satisfies the local message invariance in $B_{\epsilon}(\mathcal{W}^{+})$. Notably, the update of message-invariant transformation is infrequently in practice, as the linear message-invariant transformation is very accurate on the real-world datasets (see Section \ref{subsec:sampling_bias_of_different_methods}).



% % \udfsection{Local message invariance of TOP.}
% \begin{theorem}[Local message invariance of linear transformation]\label{thm:lmi_top}
%     % The activation function $\sigma$ is 1-positively homogeneous: σ(λx) = λσ(x) for nonnegative λ (e.g. ReLU and LeakyReLU).
%     Suppose that $(\mathbf{H}^{(i,t)}\mathcal{W}^{(t)})_{pq} \neq 0$. Then, there exists $\epsilon>0$ such that the local message invariance holds for $\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} = g( \mathbf{H}^{(l)}_{\mathcal{B}} ) = \mathbf{R}\mathbf{H}^{(l)}_{\mathcal{B}}$ in the $B_{\epsilon}(\mathcal{W}^{(t)})$.
%     % Let $\epsilon = \max_{p,q}|(\mathbf{H}^{(i,rand)}\mathbf{W}^{(rand)})_{pq}|$. 
% \end{theorem}
% % By the theorem, we reuse the transformation $g$ to optimize GNNs over multiple training steps if the GNN parameters are in the $\epsilon$ ball. Once the new parameters $\mathbf{W}^{(t+1)}$ escape from the $\epsilon$ ball $B_{\epsilon}(\mathbf{W})$, then we can rapidly estimate a new transformation $g^{+}$ which satisfies the local message invariance in $B_{\epsilon}(\mathbf{W}^{(t+1)})$.
% % %  The following theorem shows that TOP satisfies the local message invariance in the $B_{\epsilon}(\mathbf{W}^{rand})$.


% % 线性引理


% % 




% % 第一 the gradients准确，第二，由于他更容易满足，得到的g更加简单。当训练过程中，GNN的参数跳出这一epsilon球时，我们可以对g做一个更新，使得消息不变性继续满足







\section{Topological Compensation}\label{sec:TOP}

In this section, we present the details of the proposed topological compensation framework (TOP). First, we introduce the formulation of TOP inspired by the case study of message invariance in Section \ref{subsec:formulation_of_TOP}. Then, based on the linear estimation of TOP, we conduct experiments to demonstrate that the message invariance significantly reduces the discrepancy between $\text{MP}_{\text{IB}}$ and the whole message passing in Section \ref{subsec:sampling_bias_of_different_methods}. Finally, we analyze the convergence of TOP in Section \ref{subsec:convergence}.

% 在这一章节中，我们基于消息不变性介绍我们提出TOP算法。我们先介绍TOP的formulation在章节x，然后我们介绍TOP的几个重要性质在Section x。接下来，基于TOP的线性估计，我们验证消息不变性在实际中的成立情况。最后，我们给出TOP的收敛性分析


% 受到case study的启发，我们提出使用线性的transformation
% 在这一章节，我们提出线性的transformation能够，受到章节xxx的启发。我们将证明线性xxx的各种性质在xxx


% Motivated by the example in Figure \ref{fig:top}, the key idea of TOP is to replace the topological dependence between neighbors with the linear dependence between vector embeddings.
% % TOP proposes compensation edges by the linear dependence between embeddings to compensate the dropped edges of strutral dependence
% % linear dependence between embeddings to neighborhood dependence
% % avoid the


\subsection{Formulation of Topological Compensation}\label{subsec:formulation_of_TOP}

% Exact message symmetries may be unknown for many GNNs in many real-world datasets. 
\udfsection{Formulation.} Inspired by the linear message-invariant transformation in the case study in Section \ref{sec:case_study}, 
% we approximate the message invariance in real-world datasets by a linear transformation.
% We empirically evaluate the approximate message invariance in many real-world datasets in Figure \ref{fig:inference_gap}.
we propose to model message invariance \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c}$} by \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} \approx \mathbf{R} \mathbf{H}^{(l)}_{\mathcal{B}}$},
where the coefficient matrix \gongshi{$\mathbf{R} \in \mathbb{R}^{ |\mathcal{N}_{\mathcal{B}}^c| \times |\mathcal{B}|}$} % represents that the out-of-batch embeddings \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}-\mathcal{B}}$} are approximated by
are the weights of
linear combinations of the in-batch embeddings of \gongshi{$\mathbf{H}^{(l)}_{\mathcal{B}}$}.
Combining the approximation and the mini-batch feature propagation \eqref{eqn:mini_batch} leads to
\begin{align} 
    \mathbf{Z}^{(l+1)}_{\mathcal{B}}  \approx \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}}\mathbf{H}^{(l)}_{\mathcal{B}} + \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}^c}\mathbf{R} \mathbf{H}^{(l)}_{\mathcal{B}}
    %\nonumber &= (\mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{B}}+\mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}-\mathcal{B}}\mathbf{R}^{(l)}_{\mathcal{N}_{\mathcal{B}}-\mathcal{B}, \mathcal{B}}) \mathbf{H}^{(l)}_{\mathcal{B}}\\
    =  \underbrace{(\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}}+ \partial \mathbf{A}_{\mathcal{B},\mathcal{B}}) \mathbf{H}^{(l)}_{\mathcal{B}}}_{\textrm{\footnotesize $\text{MP}_{\text{IB}}$}},\label{eqn:mini_batch_mn}
\end{align}
where we call \gongshi{$\partial \mathbf{A}_{\mathcal{B},\mathcal{B}} \triangleq \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}^c} \mathbf{R} $} \textit{the topological compensation} (TOP). The topological compensation implements the message invariance by adding weighted edges to the induced subgraph \gongshi{$\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}}$}.
% The approximation \eqref{eqn:mini_batch_mn} is independent of the neighborhood embeddings \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}-\mathcal{B}}$}.
% Therefore, the general extrapolation avoids the recurse
% dependencies and expensive costs of a large number of neighborhood embeddings.
Then, TOP directly runs a GCN on the modified subgraph as follows
\begin{align*}
    \mathbf{H}^{(L)}_{\mathcal{B}} =  \gcn(\mathbf{X}_{\mathcal{B}}, \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}}+\partial \mathbf{A}_{\mathcal{B},\mathcal{B}}).
\end{align*}
The formulation of TOP makes it easy to incorporate the existing subgraph sampling methods.



%  \subsection{Estimation of Topological Compensation}

% \subsection{Estimation of Topological Compensation}\label{subsec:embedding_discrepancy_minimization}


\udfsection{Estimation of topological compensation.} To {reduce the discrepancy between the modified $\text{MP}_{\text{IB}}$ in Equation \eqref{eqn:mini_batch_mn} and the whole message passing \eqref{eqn:mini_batch}}, we estimate $\mathbf{R}$ by
% \begin{align*}
%     \min_{ \mathbf{R} } \| \mathbf{R}  \overline{\mathbf{H}}_{\mathcal{B}} - \overline{\mathbf{H}}_{\mathcal{N}_{\mathcal{B}}} \|_F,
% \end{align*}
{\begin{align*}
    \min_{ \mathbf{R} } \| \mathbf{R}  \overline{\mathbf{H}}_{\mathcal{B}} - \overline{\mathbf{H}}_{\mathcal{N}_{\mathcal{B}}^{c}} \|_F,
\end{align*}}
% such that \gongshi{$\partial \mathbf{A}_{\mathcal{B},\mathcal{B}} = \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}({\mathcal{B}})-\mathcal{B}} \mathbf{R} $}.
% We simplify the minimization problem into
% \begin{align}\label{eqn:embedding_minimization}
%     \min_{ \partial \mathbf{A}_{\mathcal{B},\mathcal{B}}  } \| \partial \mathbf{A}_{\mathcal{B},\mathcal{B}} \overline{\mathbf{H}}_{\mathcal{B}} - \mathbf{Y}_{\mathcal{B}}(\overline{\mathbf{H}}) \|_F, \quad \text{s.t. }  \partial \mathbf{A}_{\mathcal{B},\mathcal{B}}=\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}^c} \mathbf{R},
% \end{align}
where \gongshi{$\overline{\mathbf{H}}$} denotes the basic embeddings {and $\|\cdot\|_F$ is the Frobenius norm}.
% and \gongshi{$ \mathbf{Y}_{\mathcal{B}}(\overline{\mathbf{H}}) = (\widetilde{\mathbf{A}} \overline{\mathbf{H}})_{\mathcal{B}} - \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}} \overline{\mathbf{H}}_{\mathcal{B}} $}.
The basic embeddings reflect the similarity between nodes.



\udfsection{Selection of basic embeddings.} Before the training, we select the basic embeddings of a GNN at random initialization by \gongshi{$\overline{\mathbf{H}}(\mathcal{W}^{(rand)}) = (\mathbf{H}^{(0,rand)},\mathbf{H}^{(1,rand)}, \dots,\mathbf{H}^{(T,rand)}) \in \mathbb{R}^{n \times (T+1)d}$}, where \gongshi{$\mathcal{W}^{(rand)}$} are the randomly initialized parameters and \gongshi{$\mathbf{H}^{(j,rand)}$} are the corresponding embeddings at the \gongshi{$j$}-th layer. The basic embeddings are the concatenation of all embeddings at different layers.



An appealing feature of $\overline{\mathbf{H}}(\mathcal{W}^{(rand)})$ is that they can identify the 1-WL indistinguishable node pairs by Theorem \ref{theorem:GNN_Injective} in Appendix \ref{subsubsec:selection_and_isomorphic}. The property ensures that the learned $g$ is message-invariant on graphs with symmetry or large-scale graphs like the first motivating example in Section \ref{sec:mi_symmetry}. 



The linear message-invariant transformation with the basic embeddings \gongshi{$\overline{\mathbf{H}}(\mathcal{W}^{(rand)})$} is very accurate on real-world datasets as shown in Section \ref{subsec:sampling_bias_of_different_methods}. Thus, we estimate TOP in the pre-processing phase and then reuse it during the training phase for efficiency in our experiments. When TOP based on \gongshi{$\overline{\mathbf{H}}(\mathcal{W}^{(rand)})$} suffers from high errors, a solution is to update $g$ using the up-to-date embeddings \gongshi{$\overline{\mathbf{H}}(\mathcal{W}^{(t)})$} at the $t$-th training step.

% Notably, the update of message-invariant transformation is infrequently in practice, as the linear message-invariant transformation with the basic embeddings \gongshi{$\overline{\mathbf{H}}(\mathcal{W}^{(rand)})$} is very accurate on the real-world datasets (see Section \ref{subsec:sampling_bias_of_different_methods}).
% At the training step \gongshi{$t$}, if the validation accuracy has stopped improving for \gongshi{$p$} training steps, then we update \gongshi{$g$} with the basic embeddings \gongshi{$\overline{\mathbf{H}}(\mathcal{W}^{(t)})$}. 
% % According to Theorem \ref{thm:lmi_top}, the message invariance holds in $B_{\epsilon}(\mathcal{W}^{(t)})$ such that we can reuse $g$ to optimize GNNs over multiple training steps.




% TOP aims to find a coefficient matrix \gongshi{$\mathbf{R}$} such that 
% \begin{align}\label{eqn:minimization_condition} \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}^c} \mathbf{R} \mathbf{H}^{(l)}_{\mathcal{B}} = \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}^c} \mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c}  , \, l=0,1,\dots, L-1
% \end{align}
% for any embeddings \gongshi{$\mathbf{H}^{(l)}$} in the GCN.




% % 选择拼接，不提随机初始化
% Thus, we propose to select the basic embeddings of a GCN at random initialization by
% \begin{align*}
%     \overline{\mathbf{H}}(\mathbf{W}^{(rand)}) = (\mathbf{H}^{(0,rand)},\mathbf{H}^{(1,rand)}, \dots,\mathbf{H}^{(T,rand)}) \in \mathbb{R}^{n \times (T+1)d},
% \end{align*}
% where  \gongshi{$\mathbf{W}^{(rand)}$} are the random initialized parameters and \gongshi{$\mathbf{H}^{(j,rand)}$} are the corresponding embeddings at the $j$-th layer. The basic embeddings are the concatenation of all embeddings at different layers.


% % 选择random
% % 我们发现简单地采用随机初始化GCN得到的嵌入就能很好地反映graph中的节点相似关系
% % 例如，xxx


% % 特别是当图同构性存在时，随机初始化GCN能够获得R使得对任意参数都xxx


% The advantages of the random basic embeddings are two-fold. First, the embeddings at the random initialization can reflect the linear dependence between nodes well, especially the isomorphic node pairs, a specific linear dependence (see Appendix \ref{subsubsec:selection_and_isomorphic}).
% Second, the random basic embeddings are easy to obtain in the pre-processing phase. Thus, we can estimate the topological compensation in the pre-processing phase and then reuse it during the training phase.


% % The motivation is that the embeddings at the random initialization can reflect the linear dependence between nodes well, especially the isomorphic node pairs, a specific linear dependence.
% % Intuitively, a neural network at random initialization is likely to be a hash function, as it maps different inputs to different vectors in the high dimensional space.
% % They 


% % % Especially, GCNs at random initialization can find the accurate coefficient matrix \gongshi{$\mathbf{R}$} to detect isomorphic node pairs, which reflect a specific node similarity.
% % For example, given a mini-batch \gongshi{$\mathcal{B} = \{v_{1},v_{2},v_{3}\}$} in Figure \ref{fig:top}, we have \gongshi{$\mathbf{h}_{4} = \mathbf{h}_{3} = 0 \cdot \mathbf{h}_{1} + 0 \cdot \mathbf{h}_{2} + 1 \cdot \mathbf{h}_{3}$} for all GNN parameter due to the symmetry.
% % Thus, \gongshi{$\mathbf{R} = (0, 0, 1)$} satisfies Equation \eqref{eqn:minimization_condition} for any embeddings \gongshi{$\mathbf{H}^{(l)}$}.
% % % and we can easily find it by solving Equation \eqref{eqn:minimization_condition} with \gongshi{$\mathbf{H}^{(l)}$} computed by randomly initialized GCN.
% % We provide theoretical and empirical analysis in Sections \ref{subsubsec:selection_and_isomorphic} and \ref{subsec:sampling_bias_of_different_methods} respectively.



% % {We formulate our idea of selecting the basic embeddings \gongshi{$\overline{\mathbf{H}}$} from graph isomorphic perspective in Section \ref{subsubsec:selection_and_isomorphic}.
% % Then in Section x, we study the number of layers \gongshi{$T$} to construct the basic embeddings.}

% % Intuitively, for \gongshi{$\overline{\mathbf{H}}$} fixed, if the batch size \gongshi{$| \mathcal{B} |$} is large enough, the optimal value of Problem (\ref{eqn:embedding_minimization}) can be zero. However, the closed-form solution to the problem is dependent on the basic embeddings \gongshi{$\overline{\mathbf{H}}$}, which means
% % % For Problem (\ref{eqn:embedding_minimization}), 
% % it is difficult to find a coefficient matrix \gongshi{$\mathbf{R}$} such that 
% % \begin{align}\label{eqn:minimization_condition} \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}}= (\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}}+ \partial \mathbf{A}_{\mathcal{B},\mathcal{B}}) \mathbf{H}^{(l)}_{\mathcal{B}}
% % \end{align}
% % for all basic embeddings \gongshi{$\overline{\mathbf{H}}$}.
% % % However, the coefficient matrix \gongshi{$\mathbf{R}$} pre-computed with certain basic embeddings \gongshi{$\overline{\mathbf{H}}$} can help approximate the minimization problem (\ref{eqn:embedding_minimization}) with little bias. 

% % Thus, we change our mind to find a coefficient matrix \gongshi{$\mathbf{R}$} such that Equation \ref{eqn:minimization_condition}
% %  holds for a certain many basic embeddings \gongshi{$\overline{\mathbf{H}}$}. We show that the basic embeddings under GCNs at random initialization can reflect the linear dependence between nodes. For example, the basic embeddings can reveal the symmetry between nodes \gongshi{$v_{3}$} and \gongshi{$v_{4}$} in Figure \ref{fig:top}, i.e. \gongshi{$h_{3} = h_{4}$} for all GNN parameters.
 
% % However, the basic embeddings \gongshi{$\overline{\mathbf{H}}$} reflect the linear dependence between nodes. 
% % For example, the basic embeddings can reveal the symmetry between nodes \gongshi{$v_{3}$} and \gongshi{$v_{4}$} in Figure \ref{fig:top}, i.e. \gongshi{$h_{3} = h_{4}$} for all GNN parameters.

% % Thus, selecting proper basic embeddings is important for TOP. 


% % {In this section, we first discuss the selection of  \gongshi{$\mathbf{H}^{(j,i)}$} from the graph isomorphism perspective and then consider the number of the layers (\gongshi{$T$}) for constructing the basic embeddings.}





{
\subsection{Measuring message invariance in real-world datasets.}\label{subsec:sampling_bias_of_different_methods}
}

\begin{figure*}[t!]
    \centering  
    \subfigure[Ogbn-arxiv \& GCN]{
        \label{subfig:inference_gcn_arxiv}
        \includegraphics[width=0.32\textwidth]{Figure/message_symmetries/gcn_arxiv.pdf}}
    \subfigure[Ogbn-arxiv \& GAT]{
        \label{subfig:inference_gat_arxiv}
        \includegraphics[width=0.32\textwidth]{Figure/message_symmetries/gat_arxiv.pdf}}
    \subfigure[Ogbn-products \& SAGE]{
        \label{subfig:inference_sage_products}
        \includegraphics[width=0.32\textwidth]{Figure/message_symmetries/sage_products.pdf}}
    \subfigure[Ogbn-arxiv \& SAGE]{
        \label{subfig:inference_sage_arxiv}
        \includegraphics[width=0.32\textwidth]{Figure/message_symmetries/sage_arxiv.pdf}}
    \subfigure[Reddit \& GCNII]{
        \label{subfig:inference_gcnii_reddit}
        \includegraphics[width=0.32\textwidth]{Figure/message_symmetries/gcnii_reddit.pdf}}
    \subfigure[Yelp \& GCNII]{
        \label{subfig:inference_gcnii_yelp}
        \includegraphics[width=0.32\textwidth]{Figure/message_symmetries/gcnii_yelp.pdf}}
% \vspace{-10pt}
    \caption{
\textbf{Measuring the message invariance in real-world datasets.} The output of TOP is very close to the whole message passing (denoted by Full-batch). %, supporting the existence of message invariance.
Please refer to Table \ref{tab:ams} in Appendix \ref{sec:ams_exp} for more results.
    }\label{fig:inference_gap}

\vspace{-4mm}
    
\end{figure*}


In this section, we conduct experiments to demonstrate that the message invariance significantly reduces the discrepancy between {$\text{MP}_{\text{IB}}$} and the whole message passing in many real-world datasets. To ensure the robustness and generalizability of TOP in practice, we provide more results in Tables \ref{tab:ams}, \ref{table_G2}, and \ref{Table_G3}, including more experiments on heterophilous graphs and experiments under various subgraph samplers. The whole experiments are conducted on five GNN models (GCN, GAT, SAGE, GCNII, and PNA) and eight datasets (Ogbn-arxiv, Reddit, Yelp, Ogbn-products, amazon-ratings, minesweeper, questions, and questions).



\udfsection{Measuring message invariance in real-world datasets.} We first train GNNs by the full-batch gradient descent for each dataset. 
Then, we measure the discrepancy between {$\text{MP}_{\text{IB}}$} and the whole message passing (denoted by Full-batch) by relative approximation errors and accuracy degradation. The relative approximation errors and accuracy degradation are defined by
\begin{align*}
    \frac{\sqrt{(\sum_{i=1}^{b} \| \mathbf{H}^{(L,*)}_{\mathcal{B}_i} - \mathbf{H}^{(L)}_{\mathcal{B}_i} \|_F^2)}}{\| \mathbf{H}^{(L,*)} \|_F} \quad \text{ and } \quad\frac{1}{b}\sum_{i=1}^{b} \text{acc}(\mathbf{H}^{(L,*)}_{\mathcal{B}_i}, \mathbf{Y}_{\mathcal{B}_i}) - \text{acc}(\mathbf{H}^{(L)}_{\mathcal{B}_i}, \mathbf{Y}_{\mathcal{B}_i}),
\end{align*}
where we run the whole message passing (i.e., Full-batch) to obtain \gongshi{$\mathbf{H}^{(L,*)}_{\mathcal{B}_i}$} and \gongshi{$\mathbf{Y}$} is the matrix consisting of the node labels. We partition the graph into \gongshi{$200$} clusters and then sample \gongshi{$b$} in \gongshi{$\{20, 40, 60, 80, 100\}$} clusters to construct subgraphs. If we decrease the batch size \gongshi{$b$}, then the ratio of messages in {$\text{MP}_{\text{OB}}$} increases and thus {$\text{MP}_{\text{OB}}$} becomes important.


Our baselines include two subgraph sampling methods using {$\text{MP}_{\text{IB}}$} (i.e., CLUSTER \citep{cluster_gcn} and GAS \citep{gas}). We introduce these baselines in Appendix \ref{sec:gcm}. 
We report the test accuracy vs. subgraph ratio in Figure \ref{fig:inference_gap}.
% The prediction performance of TOP is robust in terms of the batch size and it resembles the full-batch performance.
The relative approximation errors of TOP are less than 5\% and the test accuracy of TOP is very close to Full-batch under different batch sizes.
% The results show that the message invariance holds in practice.





% and then predict the test accuracy and compute the sampling bias on the whole dataset based on TOP, CLUSTER \citep{cluster_gcn}, and GAS \citep{gas}.
% For a fair comparison, we generate the historical embeddings for GAS by the randomly initialized GCN, as the coefficient matrix of TOP is also computed by the randomly initialized GCN.
% We partition the graph into two hundred clusters and then sample \gongshi{$b$} clusters to construct subgraphs.
% {As CLUSTER \citep{cluster_gcn} does not estimate the neighborhood embeddings  \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}$}, we directly evaluate the final sampling bias by the accuracy degradation
% \begin{align*}
%     \frac{1}{b}\sum_{i=1}^{b} \text{accuracy}(\mathbf{H}^{(L,*)}_{\mathcal{B}_i}, \mathbf{Y}_{\mathcal{B}_i}) - \text{accuracy}(\mathbf{H}^{(L)}_{\mathcal{B}_i}, \mathbf{Y}_{\mathcal{B}_i}),
% \end{align*}

% Intuitively, if we decrease the batch size \gongshi{$b$}, then the number of discarded messages from neighbors increases.
% We report the test accuracy vs. \gongshi{$b$} in Figure \ref{fig:inference_gap}.
% The prediction performance of TOP is robust in terms of the batch size and it resembles the full-batch performance.}
% % Therefore, on the real-world datasets, TOP can accelerate the convergence of subgraph sampling 






% \udfsection{Comparison of single basic embeddings.} We first compare \gongshi{$\overline{\mathbf{H}} = \overline{\mathbf{H}}(\mathbf{W}^{(rand)})$} and \gongshi{$\overline{\mathbf{H}} = \overline{\mathbf{H}}(\mathbf{W}^{(i)})$} for $i=1, 10, 50, 100$.


% \udfsection{Comparison of the concatenation of basic embeddings.} We then compare the concatenation of embeddings including
% \begin{align*}
%     \overline{\mathbf{H}} &= (\overline{\mathbf{H}}(\mathbf{W}^{(rand_1)}), \overline{\mathbf{H}}(\mathbf{W}^{(rand_2)}), \dots, \overline{\mathbf{H}}(\mathbf{W}^{(rand_j)})).
%     % \overline{\mathbf{H}} &= (\overline{\mathbf{H}}(\mathbf{W}^{(rand)}), \overline{\mathbf{H}}(\mathbf{W}^{(i)})),\\
%     % \overline{\mathbf{H}} &= (\overline{\mathbf{H}}(\mathbf{W}^{(rand)}), \overline{\mathbf{H}}(\mathbf{W}^{(i_1)}), , \overline{\mathbf{H}}(\mathbf{W}^{(i_2)}), , \overline{\mathbf{H}}(\mathbf{W}^{(i_j)})).
% \end{align*}


% We evaluate the proposed basic embeddings as follows.
% \begin{enumerate}
%     \item The embeddings of GNNs at random initialization, i.e., \gongshi{$\overline{\mathbf{H}} = \overline{\mathbf{H}}(\mathbf{W}^{(rand)})$}.
%     \item The embeddings of GNNs at the $i$-th training step, i.e., \gongshi{$\overline{\mathbf{H}} = \overline{\mathbf{H}}(\mathbf{W}^{(i)})$} for $i=1, 10, 50, 100$.
%     \item The concatenation of embeddings  at random initialization \gongshi{$\overline{\mathbf{H}} = (\overline{\mathbf{H}}(\mathbf{W}^{(rand_1)}), \overline{\mathbf{H}}(\mathbf{W}^{(rand_2)}), \dots, \overline{\mathbf{H}}(\mathbf{W}^{(rand_i)}))$}.
%     \item The concatenation of embeddings \gongshi{$\overline{\mathbf{H}} = (\overline{\mathbf{H}}(\mathbf{W}^{(rand)}), \overline{\mathbf{H}}(\mathbf{W}^{(i)}))$}.
% \end{enumerate}







% We have proved the high probability of the existence of isomorphic node pairs if batch size \gongshi{$|\mathcal{B}|$} is large enough. Intuitively, for an out-of-batch node \gongshi{$j \in \mathcal{N(B)-B}$}, even if there does not exist the isomorphic in-batch node, we can still find an in-batch node \gongshi{$i$} with its embedding similar to \gongshi{$j$}, so that the extrapolation error is under control. However, how large should the batch size be? Is TOP really applicable to real world dataset? The answer is yes, and we demonstrate some experiments here to validate the fact.

% Section x gives a linear extrapolation \eqref{eqn:vq} of TOP to estimate the neighborhood embeddings  \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}$} based on the in-batch embeddings \gongshi{$\mathbf{H}^{(l)}_{\mathcal{B}}$}.
% Although the approximation errors become zero under the conditions of a large enough batch size and discrete features, the conditions may fail in practice.
% Thus, we conduct experiments on the real-world datasets in this section to verify that the approximation of the linear extrapolation \eqref{eqn:vq} is accurate.

% As some subgraph sampling methods (e.g. CLUSTER \citep{cluster_gcn}) do not estimate the neighborhood embeddings  \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}$}, we directly evaluate the final approximation errors by the accuracy degradation
% \begin{align*}
%     \frac{1}{b}\sum_{i=1}^{b} \text{accuracy}(\mathbf{H}^{(L,*)}_{\mathcal{B}_i}, \mathbf{Y}_{\mathcal{B}_i}) - \text{accuracy}(\mathbf{H}^{(L)}_{\mathcal{B}_i}, \mathbf{Y}_{\mathcal{B}_i}),
% \end{align*}
% where we run the full-batch evaluation to compute the exact node embeddings \gongshi{$\mathbf{H}^{(L,*)}_{\mathcal{B}_i}$} and \gongshi{$\mathbf{Y}$} is the matrix consisting of the node labels.




% % \mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}} \approx \mathbf{R}^{(l)}_{\mathcal{N}(\mathcal{B})-\mathcal{B}, \mathcal{B}} \mathbf{H}^{(l)}_{\mathcal{B}}
% % The analysis in 
% % Is TOP applicable to real world dataset with continuous features? How large should the batch size be to motivate TOP? In this section, we provide some empirical results to demonstrate the applicability and scalability of TOP. The experimental settings are introduced in Section \ref{sec:setting1}.


\vspace{-2mm}

\subsection{Convergence of TOP}\label{subsec:convergence}

% Figure \ref{fig:inference_gap} and Table \ref{tab:ams} show the existence of message invariance in many real-world datasets.
Based on message invariance \eqref{eqn:nonlinear_extrapolation}, we develop the convergence analysis of TOP in this section.  The assumption of Theorem \ref{thm:convergence_conv} is widely used in convergence analysis \citep{lmc, vrgcn, graphfm}.
All proofs are provided in Appendix \ref{sec:proof_convergence}.




% We first show that TOP based on Equation \eqref{eqn:top} provides unbiased gradients. We assume that subgraph \gongshi{$\mathcal{B}$} is uniformly sampled from \gongshi{$\mathcal{V}$}.  When the sampling is not uniform, we use the normalization technique \citep{graphsaint} to enforce the assumption.
% \begin{theorem}\label{thm:unbiased}
%         Suppose that the message invariant transformations \eqref{eqn:nonlinear_extrapolation} exist and the subgraph \gongshi{ $\mathcal{B}$} is uniformly sampled from \gongshi{ $\mathcal{V}$}. The iterative message passing of Equations \eqref{eqn:top} and \gongshi{$\mathbf{H}_{\mathcal{B}}^{(l+1)} = \sigma(\mathbf{Z}^{(l+1)}_{\mathcal{B}}\mathbf{W}^{(l)}) $} leads to unbiased mini-batch gradients \gongshi{$\mathbf{d}_{\mathbf{W}} \triangleq \nabla_{\mathbf{W}} \sum_{i \in \mathcal{B}} \ell (\mathbf{h}^{(L)}_{i}, y_i) / |\mathcal{B}|$} such that
%         \begin{align*}
%             \mathbb{E}[\mathbf{d}_{\mathbf{W}}]
%             &=
%             \nabla_{\mathbf{W}} \mathcal{L}
%         \end{align*}
%         where \gongshi{$\mathbf{W} = (\mathbf{W}^{(l)})_{l}$} is the collection of the GNN parameters, \gongshi{$\nabla_{\mathbf{W}} \mathcal{L}$} is the full-batch gradients, \gongshi{$\ell$} is the loss function, and $y_i$ is the label of node $i$.
% \end{theorem}





\begin{theorem}\label{thm:convergence_conv}
    Let \gongshi{$\mathcal{L}(\mathcal{W}) = \sum_{i \in \mathcal{V}} \ell (\mathbf{h}^{(L)}_{i}, y_i) / |\mathcal{B}|$} and \gongshi{$\mathbf{d}_{\mathcal{W}} = \nabla_{\mathcal{W}} \sum_{i \in \mathcal{B}} \ell (\mathbf{h}^{(L,TOP)}_{i}, y_i) / |\mathcal{B}|$} be the loss of the full-batch method and the gradient of TOP respectively, where \gongshi{$\ell$} is the loss function and \gongshi{$y_i$} is the label of node \gongshi{$i$}.
    Assume that (1) the optimal value \gongshi{$\mathcal{L}^{*} = \mathrm{inf}\ \mathcal{L}(\mathcal{W})$} is finite (2) at the \gongshi{$k$}-th iteration, a batch of nodes \gongshi{$\mathcal{V}^{k}_{\mathcal{B}}$} is uniformly sampled from \gongshi{$\mathcal{V}$} (3) function \gongshi{$\nabla_{\mathcal{W}} \mathcal{L}$} is \gongshi{$\gamma$}-Lipschitz with \gongshi{$\gamma > 1$} (4) norms \gongshi{$\|\nabla_{\mathcal{W}} \mathcal{L}\|_2$} and \gongshi{$\|\mathbf{d}_{\mathcal{W}}\|_2$} are bounded by \gongshi{$G > 1$}. With the learning rate \gongshi{$\eta=\mathcal{O}(\varepsilon^{2})$} and the training step \gongshi{$N=\mathcal{O}(\varepsilon^{-4})$}, TOP then finds an \gongshi{$\varepsilon$}-stationary solution such that \gongshi{$\mathbb{E}[\|\nabla_{\mathcal{W}} \mathcal{L}(\mathcal{W}^{(R)}) \|_2] \leq \varepsilon$} after running for \gongshi{$N$} iterations, where \gongshi{$R$} is uniformly selected from \gongshi{$\llbracket N \rrbracket$}.
\end{theorem}

The convergence rate \gongshi{$N=\mathcal{O}(\varepsilon^{-4})$} is the same as the standard SGD \citep{sgd, spider}.
Notably, the convergence rate of TOP is faster than that of LMC \citep{lmc} (i.e., \gongshi{$N=\mathcal{O}(\varepsilon^{-6})$}), as TOP avoids the staleness issue of the historical embeddings and gradients of LMC.

% \vspace{-2mm}

\begin{table*}[t!]
    \vspace{-8mm}
  \begin{center}
    \caption{\textbf{Statistics of the datasets in our experiments}. ``\#" denotes the number and ``Avg. degree" denotes the average degree. The task is node classification, which is a standard task to evaluate the scalability on the large-scale graph \citep{cluster_gcn, graphsaint, gas}.
    }\label{tab:datasets}
    % \vspace{5pt}
  \resizebox{1.0\linewidth}{!}{%
    \begin{tabular}{ccccccc}
    \toprule
    \textbf{Dataset} & \textbf{\#Classes} &\textbf{Total \#Nodes} & \textbf{Total \#Edges} & \textbf{Avg. degree} & \textbf{Train/Val/Test}  \\
      % \midrule
      \midrule
      Reddit  & 41 & 232,965 & 11,606,919 & 49.8 & 0.660/0.100/0.240   \\
    %   AMAZON & 1 & 58 & 334,863 & 925,872   \\
      Yelp  & 50 & 716,847 & 6,997,410 &  9.8  & 0.750/0.150/0.100   \\
      Ogbn-arxiv  & 40  & 169,343 & 1,157,799 & 6.9 & 0.537/0.176/0.287 \\
      Ogbn-products  & 47  & 2,449,029 & 61,859,076 & 25.3 & 0.100/0.020/0.880  \\
      Ogbn-papers100M  & 172  & 111,059,956 & 1,615,685,872 & 14.6 & 0.780/0.080/0.140 \\ % 61,859,076
      \bottomrule
    \end{tabular}
    }
  \end{center}
    \vspace{-4mm}
\end{table*} 

\vspace{-1mm}

\section{Experiments}
\label{sec:exp}

\vspace{-2mm}

We first compare the convergence and efficiency of TOP with the state-of-the-art subgraph sampling methods---which are the most related baselines---in Section \ref{sec:convergence_curve}.
Then, we compare the convergence and efficiency of TOP with the state-of-the-art  node/layer-wise sampling methods in Section \ref{sec:comp_nslabor}.
More experiments are provided in Appendix \ref{sec:more_exp}.

\vspace{-2mm}
% Finally, we evaluate the prediction performance in Section \ref{sec:prediction}.

% We first introduce the experimental settings in Section \ref{sec:setting1}. We then evaluate the convergence and efficiency of TOP in Sections \ref{sec:convergence_curve} and \ref{sec:memory_runtime}.
% Finally, we evaluate the prediction performance in Section \ref{sec:prediction}.


% \subsection{Experimental Settings}

% \subsection{Scaling GNNs to Large graphs}



\subsection{Comparison with Subgraph Sampling}
\label{sec:convergence_curve}

% \subsection{Experimental Settings} 
% \label{sec:setting1}

% \udfsection{Datasets.} Some recent works \citep{ogb} have indicated that many frequently-used graph datasets are too small compared with graphs in real-world applications.
\udfsection{Datasets.} We evaluate TOP on five datasets with various sizes (i.e., Reddit \citep{graphsage}, Yelp \citep{graphsaint}, Ogbn-arxiv, Ogbn-products, and Ogbn-papers \citep{ogb}).
These datasets contain at least 100 thousand nodes and one million edges.
Notably, Ogbn-papers is very large, containing 100 million nodes and 1.6 billion edges.
They have been widely used in previous works \citep{gas, graphsaint, graphsage, cluster_gcn, vrgcn, fastgcn}.
% For more details, please refer to Appendix \ref{sec:dataset}.
Table \ref{tab:datasets} summarizes the statistics of the datasets. We also conduct experiments on heterophilous graphs in Appendix \ref{sec:exp_heterophilous}.
% 仿照GraphFM: Improving Large-Scale GNN Training via Feature Momentum改一下，用他的表头









% 运行五次

% for GCN \citep{gcn} and GCNII \citep{gcn2} 


\udfsection{Subgraph samplers.} On the small and medium datasets (i.e., Ogbn-arxiv, Reddit, and Yelp), we follow CLUSTER \citep{cluster_gcn} and GAS \citep{gas} to sample subgraphs based on METIS (see Appendix \ref{sec:metis}). Specifically, we first use METIS to partition the original graph into many clusters and then sample a cluster of nodes to generate a subgraph.
On the large datasets  (i.e., Ogbn-products and Ogbn-papers), as the METIS algorithm is too time-consuming \citep{graphsaint}, we uniformly sample nodes to construct subgraphs.
More experiments under various subgraph samplers are provided in Appendix \ref{sec:samplers}.


\udfsection{Baselines and implementation details.} Our baselines include subgraph sampling (CLUSTER \citep{cluster_gcn}, SAINT \citep{graphsaint}, and GAS \citep{gas}). 
We also compare TOP with IBMB \citep{ibmb} in Appendix \ref{sec:top_av}, a recent subgraph sampling method focused on the design of subgraph samplers, which is orthogonal to TOP (see Section \ref{sec:related_work}).
% Many recent subgraph sampling methods (e.g. IBMB \citep{ibmb} and SHADOW \citep{shadow_gnn}) focus on the design of subgraph samplers, which is orthogonal to TOP (see Section \ref{sec:related_work}). We also compare TOP built on the subgraph samplers in Appendix \ref{sec:samplers}.
We implement TOP, CLUSTER, SAINT, and GAS based on the codes and toolkits of GAS \citep{gas} to ensure a fair comparison. 
We introduce these baselines in Appendix \ref{sec:gcm}.
%  and node/layer-wise sampling (NS \citep{graphsage} and LABOR \citep{labor}).
% We include the comparison with NS and LABOR in Appendix \ref{sec:labor}, as their best implementation uses the advanced GPU acceleration technique and machine.
We evaluate CLUSTER, GAS, SAINT, and TOP based on the same GNN backbone, including the widely used GCN \citep{gcn} and GCNII \citep{gcnii}.
We implement GCN and GCNII following \citep{gas} and \citep{graphsage}.
Due to space limitation, we present the results with more GNN backbones (e.g. SAGE \citep{graphsage}, and GAT \citep{gat}) in Appendix \ref{sec:top_av}.
 We run all experiments in this section on a single GeForce RTX 2080 Ti (11 GB), and Intel Xeon CPU E5-2640 v4.
% Moreover, we use the jumping knowledge connection \citep{jknet} to enhance GCNII, denoted by GCNII+JK.
% The hyperparameters are provided by \ref{sec:hyperparameters}
% (e.g. widely used GCN \citep{gcn} or GCNII \citep{gcnii}).
% (e.g. data splits, optimizer, etc.)
 % We run the experiments on a single GeForce RTX 2080 Ti (11 GB).
For other implementation details, please refer to Appendix \ref{sec:implementation}.


 
% To ensure a fair comparison, we follow the data splits, training pipeline, and most hyperparameters in \citep{gas}
% except for the additional hyperparameters in TOP such as $\beta$. We use the grid search to find the best $\beta$.





Figure \ref{fig:runtime} shows the convergence curves (test accuracy vs. runtime (s)) of TOP, CLUSTER, GAS, SAINT, and Full-batch (i.e. full-batch gradient descent with the whole message passing).
We provide the convergence curves (test accuracy vs. epochs) in Appendix \ref{sec:more_exp}.
We use a sliding window to smooth the curves in Figure \ref{fig:runtime} as the test accuracy is unstable.
We ran each experiment five times. The solid curves correspond to the mean, and the shaded regions correspond to values within plus or minus one standard deviation of the mean.
The convergence curves consider the runtime of pre-processing.
% We report the detailed cost of Pre-processing and training in Appendx \ref{sec:pretime}.


\begin{figure*}[t!]
    \vspace{-8mm}
    \centering  
    \subfigure[GCN on small datasets]{
        \label{subfig:small_datasets}
        \includegraphics[width=0.31\textwidth]{Figure/exp_sec1/accuracy_runtime_gcn_small.pdf}}
    \subfigure[GCNII on medium datasets]{
        \label{subfig:medium_datasets}
        \includegraphics[width=0.31\textwidth]{Figure/exp_sec1/accuracy_runtime_gcnii_medium.pdf}}
    \subfigure[GCNII on large datasets]{
        \label{subfig:large_datasets}
        \includegraphics[width=0.31\textwidth]{Figure/exp_sec1/accuracy_runtime_gcnii_large.pdf}}
\vspace{-5pt}
    \caption{
        \textbf{Convergence curves (test accuracy vs. runtime (s)) of subgraph sampling}. We use the default \gongshi{$|\mathcal{B}|$ and $|\mathcal{V}|$}---which denote the sizes of subgraphs and the whole graph respectively---provided in GAS \cite{gas}.
    }\label{fig:runtime}

\vspace{-3mm}
\end{figure*}


\begin{figure*}[t]
    \centering
    \subfigure[Relative runtime per epoch]{
        \includegraphics[width=0.48\textwidth]{Figure/1.pdf}
    }
    \subfigure[Relative memory consumption]{
		\includegraphics[width=0.48\textwidth]{Figure/2.pdf}
    }
    \vspace{-2mm}
    \caption{
    \textbf{Relative runtime per epoch and relative memory consumption}. Please refer to Table \ref{tab:memory_consumption} in Appendix \ref{sec:relative_mc} for more results.
    }
    \label{fig:0}
    \vspace{-4mm}
\end{figure*}

\udfsection{Results on small datasets.} On the small datasets (i.e., Ogbn-arxiv and Reddit), the subgraph ratio \gongshi{$|\mathcal{B}|/|\mathcal{V}|$} is up to 50\%, where \gongshi{$|\mathcal{B}|$} and \gongshi{$|\mathcal{V}|$} denote the sizes of subgraphs and the whole graph respectively. The large ratio shows that the subgraph contains much information about the whole graph.
According to Figure \ref{subfig:small_datasets}, TOP is significantly faster than Full-batch, CLUSTER, GAS, and SAINT without sacrificing accuracy.
Further, TOP stably resembles the full-batch performance on the Ogbn-arxiv and Reddit datasets, while CLUSTER, GAS, and SAINT are unstable. The standard deviation of CLUSTER, GAS, and SAINT is large such that the mean test accuracy is lower than the full-batch performance, as they are difficult to encode all available neighborhood information of the subgraph.
Specifically, CLUSTER and SAINT do not take {$\text{MP}_{\text{OB}}$} into consideration and GAS uses stale historical embeddings to approximate {$\text{MP}_{\text{OB}}$}.



\udfsection{Results on medium datasets.} On the medium datasets (i.e., Reddit, and Yelp), the subgraph ratio \gongshi{$|\mathcal{B}|/|\mathcal{V}|$} decreases from 50\% to 12.5\% due to GPU memory limitations. Thus, Full-batch runs out of GPU memory on the Yelp dataset.
Compared with GCN, the nonlinearity of GCNII becomes strong due to the large model capacity of GCNII.
Under the strong nonlinearity, TOP is still significantly faster than CLUSTER, GAS, and SAINT on the Yelp dataset with a low subgraph ratio \gongshi{$|\mathcal{B}|/|\mathcal{V}|$} according to Figure \ref{subfig:medium_datasets}.
Moreover, TOP is significantly faster than GAS and Full-batch on the Reddit dataset.
Although the mean convergence curses of TOP and CLUSTER are similar on the Reddit dataset, the low standard deviation demonstrates that TOP is more stable than CLUSTER.


\udfsection{Results on large datasets.} On the large datasets (i.e., Ogbn-products, and Ogbn-papers), the subgraph ratio \gongshi{$|\mathcal{B}|/|\mathcal{V}|$} is very low due to GPU memory limitations.
By noticing that the large number of valid and test nodes in the large datasets is useless for TOP, we remove the valid and test nodes from sampled subgraphs.
For an ablation study, we report CLUSTER without valid and test nodes in the sampled subgraphs.
We do not remove the valid and test nodes for GAS, as GAS requires updating the historical embeddings on the valid and test nodes to alleviate the staleness issue.
Due to a large number of historical embeddings, GAS runs out of CPU memory on the Ogbn-papers dataset.
According to Figure \ref{subfig:large_datasets}, TOP is significantly faster than CLUSTER and GAS by several orders of magnitude.
Moreover, the valid and test nodes in subgraphs are important for CLUSTER, as these valid and test nodes are likely to be the neighbors of the training nodes.
The valid and test nodes in subgraphs increase the ratio of messages in {$\text{MP}_{\text{IB}}$} for CLUSTER.
TOP does not depend on the valid and test nodes due to its effective topological compensation.


% \begin{figure*}[ht]
% \centering % <-- added
% \begin{subfigure}{1.0\textwidth}
%     \includegraphics[width=\linewidth]{Figure/exp_sec1/accuracy_runtime_gcn_small.pdf}
%     \caption{Small datasets}\label{subfig:small_datasets}
% \end{subfigure}\hfil
% \begin{subfigure}{1.0\textwidth}
%     \includegraphics[width=\linewidth]{Figure/exp_sec1/accuracy_runtime_gcnii_medium.pdf}
%     \caption{Medium datasets}\label{subfig:meidum_datasets}
% \end{subfigure}\hfil
% \begin{subfigure}{1.0\textwidth}
%     \includegraphics[width=\linewidth]{Figure/exp_sec1/accuracy_runtime_gcnii_large.pdf}
%     \caption{Large datasets}\label{subfig:large_datasets}
% \end{subfigure}\hfil
% \caption{
% Convergence curves (test accuracy vs. runtime (s)).} \label{fig:runtime}
% \end{figure*}

%\

% \begin{figure*}[t!]
%     \centering  
%     \subfigure[Ogbn-arxiv]{
%         \label{subfig:inference_gap_arxiv}
%         \includegraphics[width=0.48\textwidth]{Figure/exp_sec1/inferencegap_arxiv.pdf}}
%     \subfigure[Reddit]{
%         \label{subfig:inference_gap_reddit}
%         \includegraphics[width=0.48\textwidth]{Figure/exp_sec1/inferencegap_reddit.pdf}}
% \vspace{-10pt}
%     \caption{
% Prediction accuracy between the mini-batch inference of different methods and the full-batch inference.
%     }\label{fig:inference_gap}
% \end{figure*}

% \label{sec:memory_runtime}

%  the batch size $|\mathcal{B}|$, and the average proportion of the mini-batch $|\mathcal{B}|/|\mathcal{V}|$
% the batch size $|\mathcal{B}|$, and the average proportion of the mini-batch $|\mathcal{B}|/|\mathcal{V}|$ 
% , where $|\mathcal{V}|$ is the total number of nodes in the whole graph



\udfsection{Memory and runtime.} We report the GPU memory consumption and the runtime per epoch in Figure \ref{fig:0}. TOP is significantly faster and more memory-efficient than GAS on all datasets, as TOP does not require pulling and pushing historical embeddings frequently.
Especially, the speedup of TOP against GAS is up to 11x on the Ogbn-product dataset, which is one order of magnitude.
We analyze the computational complexity of TOP in Appendix \ref{sec:complexity} and give the detailed costs of pre-processing and training in Appendix \ref{sec:pretime}.

\vspace{-2mm}





% Moreover, if the subgraph ratio $|\mathcal{B}|/|\mathcal{V}|$ is small, the advantage of TOP 
% % , and the size of the subgraph
% TOP is also faster than CLUSTER, as TOP uses the subgraph inference rather than the full-graph inference.
% We demonstrate that the subgraph inference of TOP nearly resembles the full-graph inference in Figure x.
% Although TOP is slightly more memory-consuming than CLUSTER, TOP signficantly


% % Although TOP is slightly more memory-consuming than CLUSTER, TOP achieves more
% \begin{table*}[t]\centering
%     \caption{GPU memory consumption (MB), the runtime per epoch (s), and the size of the subgraph. }\label{tab:memory_consumption}
%     \setlength{\tabcolsep}{1.9mm}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{ccccc}
%     \toprule
%     \textbf{Methods} & Ogbn-arxiv & Reddit & Ogbn-products & Ogbn-papers\\
%     \midrule
%     GD & 681 MB/0.5 s/169,343 & 681 MB/0.5 s/232,965 & OOM/OOM/OOM & OOM/OOM/OOM\\
%     \midrule
%     CLUSTER & x MB/0.5 s/84,672 & 681 MB/0.5 s/116,483 & 681 MB/0.5 s/ 10000 & 681 MB/0.5 s/ 10000\\
%     GAS & x MB/0.5 s/x & 681 MB/0.5 s/x & 681 MB/0.5 s/116,483 & 681 MB/0.5 s/ 10000\\
%     {\bf TOP} & x MB/0.5 s/84,672 & 681 MB/0.5 s/116,483 & 681 MB/0.5 s/ 10000 & 681 MB/0.5 s/ 10000\\
%     \bottomrule
%   \end{tabular}
%     }
% \end{table*}





% \subsection{Inference Gap} \label{sec:inference_gap}

% We compare the prediction accuracy between the mini-batch inference of different methods and the full-batch inference.
% We first train GCN by GD on the Ogbn-arxiv and Reddit datasets, and then predict the test accuracy based on TOP, CLUSTER, and GAS. We generate the historical embeddings by the initial GCN, whose weights are randomly initialized.
% We first partition the graph into two hundred clusters and then sample \gongshi{$b$} clusters to construct subgraphs.
% If we decrease \gongshi{$b$}, then the number of discarded messages from neighbors increases.
% We report the test accuracy vs. \gongshi{$b$} in Figure \ref{fig:inference_gap}.
% The prediction performance of TOP is robust in terms of the batch size and it resembles the full-batch performance.
% The result demonstrates the superior ability of TOP to compensate for neighborhood messages.

% As the performance of CLUSTER and GAS is significantly lower than the full-batch performance, they use full-batch inference in practice, leading to expensive computational costs.

% \begin{figure}[t]
% \centering % <-- added
% % \begin{subfigure}{1.0\linewidth}
% %   \includegraphics[width=\linewidth]{imgs/convgnn/exp_sec2/trainloss_runtime.pdf}
% %   \caption{Training loss}\label{subfig:train_loss_runtime}
% % \end{subfigure}\hfil % <-- added
% % \begin{subfigure}{1.0\linewidth}
% %   \includegraphics[width=\linewidth]{imgs/convgnn/exp_sec2/testacc_runtime.pdf}
% %     \caption{Testing accuracy}\label{subfig:test_acc_runtime}
% % \end{subfigure}\hfil
% \includegraphics[width=\linewidth]{Figure/exp_sec1/inferencegap_arxiv.pdf}
% \caption{
% Prediction accuracy between the mini-batch inference of different methods and the full-batch inference.} \label{fig:inference_gap}
% \end{figure}




% \begin{wraptable}{r}{6.0cm}
%   \caption{Performance under different batch sizes on the Ogbn-arxiv dataset.
%   }\label{tab:batch size}
%   \setlength{\tabcolsep}{3pt}
%   \begin{tabular}{crrrr}
%     \toprule
%     \mr{2}{Batch size} & \mc{2}{c}{GCN} & \mc{2}{c}{GCNII} \\
%      & GAS & LMC & GAS & LMC \\
%     \midrule
%     1 & 70.56 & \textbf{71.65} & 71.34 & \textbf{72.11} \\
%     2 & 71.11 & \textbf{71.89} & 72.25 & \textbf{72.55} \\
%     5 & \textbf{71.99} & 71.84 & 72.23 & \textbf{72.87}   \\
%     10& 71.60 & \textbf{72.14} & \textbf{72.82} & 72.80 \\
%     \bottomrule
%   \end{tabular}
% \end{wraptable}


%  Average Subgraph Size, Subgraph Size




\begin{figure*}[t]
    \vspace{-8mm}
    \centering
    \subfigure[Memory.]{
		\includegraphics[width=0.21\textwidth]{Figure/rebuttal/labor/products_memory.pdf}  \label{fig:labor_memory}
    }
    \subfigure[Runtime on Products.]{
        \includegraphics[width=0.24\textwidth]{Figure/rebuttal/labor/accuracy_runtime_gcnii_labor.pdf} \label{fig:labor_convergence}
    }
    \subfigure[Runtime on Reddit.]{
        \includegraphics[width=0.24\textwidth]{Figure/rebuttal/labor/accuracy_runtime_gcnii_labor_reddit.pdf} \label{fig:labor_convergence_reddit}
    }
    \subfigure[Runtime on Arxiv.]{
        \includegraphics[width=0.24\textwidth]{Figure/rebuttal/labor/accuracy_runtime_gcnii_labor_arxiv.pdf} \label{fig:labor_convergence_arxiv}
    }
    \vspace{-3mm}
    \caption{
    \textbf{Memory consumption and convergence curves of TOP and node/layer-wise sampling}.
    }
    \label{fig:labor}

    \vspace{-6mm}
\end{figure*}

\subsection{Comparison with Node/Layer-wise Sampling} \label{sec:comp_nslabor}


\udfsection{Baselines and implementation details} We compare TOP with node-wise and layer-wise sampling methods including neighbor sampling (NS) \citep{graphsage} and LABOR \citep{labor} in Figure \ref{fig:labor}, where LABOR combines the advantages of node-wise and layer-wise sampling to accelerate convergence.
Unlike subgraph sampling, node/layer-wise sampling mainly focuses on the certain SAGE model \citep{graphsage}.
We run NS and LABOR by the official implementation of LABOR \citep{labor}.
The reported runtime includes the runtime of pre-processing.
% \footnote{https://github.com/dmlc/dgl/tree/master/examples/pytorch/labor}
% We also implement TOP with the same SAGE model based on the codes and toolkits of GAS \citep{gas}. 
% with the hyperparameters \textit{--fan-out=10} and \textit{--lad-out="16000,11000,5000,2500,1500,500"} in the official implementation.
We run experiments in this section on a single A800 card.

\udfsection{Hyperparameters.} 
For TOP, we uniformly sample nodes to construct subgraphs. To ensure a fair comparison, TOP follows the GNN architectures, data splits, training pipeline, learning rate, and hyperparameters of LABOR \citep{labor}.
We adjust the batch size of TOP such that the memory consumption of TOP is similar to LABOR.




\udfsection{Memory.} We first evaluate the GPU memory consumption in terms of the number of GNN layers in Figure \ref{fig:labor_memory}. We increase the number of GNN layers from two to seven.
The GPU memory of NS and LABOR increases exponentially with the number of GNN layers, and thus they are difficult to apply to deep GNNs (e.g. GCNII with six layers in Figure \ref{fig:runtime}).
The GPU memory of both CLUSTER \citep{cluster_gcn} and TOP increases linearly with the number of GNN layers, corresponding with the computational complexity in Table \ref{tab:complexity}.
The GPU memory of CLUSTER is slightly larger than TOP, as CLUSTER uses the layer-wise inference (like GAS, see Appendix \ref{sec:gcm2}) in the evaluation phase while TOP only uses the mini-batch information (see Equation \eqref{eqn:mini_batch_mn}).




\udfsection{Convergence curves.} We further report the convergence curves of TOP, NS, and LABOR in Figures \ref{fig:labor_convergence}, \ref{fig:labor_convergence_reddit}, and \ref{fig:labor_convergence_arxiv}. We have included the pre-processing time of TOP in the figures.
% We do not include CLUSTER as its METIS partition takes more than 250 seconds in the experiments.
Although NS and LABOR do not require pre-processing, TOP finally outperforms NS and LABOR due to its powerful convergence.
The speedup of TOP against NS and LABOR is more than 2x on all datasets.

\vspace{-2mm}

% % alg:cap

% We implement NS and LABOR z  

% % the convergence curves (test accuracy vs. epochs) 

% Notably, we report the test accuracy of the full-batch gradient descent (GD) every two steps rather than per epoch, as GD performs backward backpropagation once per epoch while other methods perform backward backpropagation twice per epoch on the Ogbn-arxiv and Reddit datasets.
% The convergence curves of TOP are close to GD on the Ogbn-arxiv and Reddit datasets, while other subgraph sampling methods fail to resemble the full-batch performance on the Ogbn-arxiv dataset.
% Moreover, TOP significantly accelerates the convergence on the medium and large datasets, e.g., Yelp, Ogbn-products, and Ogbn-papers.





\vspace{-2mm}


\section{Conclusion}

In this paper, we propose an accurate and fast subgraph sampling method, namely topological compensation (TOP), based on a novel concept of message invariance. Message invariance defines message-invariant transformations that convert expensive message passing acted on out-of-batch neighbors ({$\text{MP}_{\text{OB}}$}) into efficient message passing acted on in-batch nodes ({$\text{MP}_{\text{IB}}$}).
Based on the message invariance, the proposed TOP uses efficient {$\text{MP}_{\text{IB}}$} with limited performance degradation.
% We conduct extensive experiments to demonstrate that the message invariance holds in practice.
% Another appealing feature is that TOP is easy to implement for various message passing-based GNNs.
% , while many graph sampling techniques are designed only for GNN models with mean and sum aggregation such as SAGE \citep{graphsage}.
Experiments demonstrate that TOP is significantly faster than existing mini-batch methods by order of magnitude on vast graphs (millions of nodes and billions of edges) with limited performance degradation.
{While our experiments focus on message-invariant transformation for some common and simple GNNs, non-linear message-invariant transformation needs to be empirically evaluated for more GNNs with more complex aggregation.}
In the future, we plan to generalize our ideas to more GNNs or graph transformers with global communication.


\section*{Reproducibility Statement}

To ensure reproducibility, we provide key information from the main text and Appendix as follows.
\begin{enumerate}
    \item \textbf{Algorithm.}
    We provide the pseudocode of TOP in Algorithms \ref{alg:cap} and \ref{alg:top}. We also provide the detailed implementation of TOP in Appendix \ref{sec:implementation}.
    See Appendix \ref{sec:hyperparameters} for the hyperparameters of TOP.

    \item \textbf{Theoretical Proofs.}
    We provide all proofs in Appendix~\ref{sec:proof_convergence}.

    \item \textbf{Source Code.} The code of LMC is available on GitHub at \url{https://github.com/MIRALab-USTC/TOP}.

    \item \textbf{Experimental Details.}
    We provide the detailed experimental settings in Section~\ref{sec:exp} and Appendix~\ref{sec:implementation}.
\end{enumerate}

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
The authors would like to thank all the anonymous reviewers for their valuable suggestions.
This work was supported by the National Key R\&D Program of China under contract 2022ZD0119801 and the National Nature Science Foundations of China grants U23A20388 and 62021001.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix

% \section{Appendix / supplemental material}


% Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
% All such materials \textbf{SHOULD be included in the main submission.}

\newpage


% \section{More Related Work}\label{sec:add_related_work}





\section{Background of Subgraph Sampling}  \label{sec:gcm}

% Subgraph sampling is a general mini-batch framework to accelerate the training of variant GNNs.
% % During the training phase of the model, subgraph sampling is a general framework for a wide range of GNN architectures.
Subgraph sampling is a general mini-batch framework for a wide range of GNN architectures.
For example, subgraph sampling  directly runs a GCN on the subgraph induced by a mini-batch \gongshi{$\mathcal{B}$}
\begin{align*}
    \mathbf{H}^{(L)}_{\mathcal{B}} \approx  \gcn(\mathbf{X}_{\mathcal{B}}, \norm(\mathbf{A}_{\mathcal{B},\mathcal{B}})),
\end{align*}
where \gongshi{$\norm (\cdot)$} normalizes the adjacency matrix of the subgraph \gongshi{$\mathbf{A}_{\mathcal{B},\mathcal{B}}$}.
For example, CLUSTER-GCN \citep{cluster_gcn} and GraphSAINT \citep{graphsaint} use \gongshi{$\norm(\mathbf{A}_{\mathcal{B},\mathcal{B}}) = \widetilde{\mathbf{A}_{\mathcal{B},\mathcal{B}}}$} and \gongshi{$\norm(\mathbf{A}_{\mathcal{B},\mathcal{B}}) = \mathbf{D}^{-1}_{\mathcal{B},\mathcal{B}}\mathbf{A}_{\mathcal{B},\mathcal{B}}$} respectively.






% We define the sampling bias as
% \begin{align*}
%     % \mathbb{E}_{\mathcal{B}}[ \| \mathbf{H}^{(L)}_{\mathcal{B}} - \mathbf{H}^{(L)}_{\mathcal{B}} \|_F]
%     \mathbb{E}_{\mathcal{B}} \left[ \| \gcn(\mathbf{X}_{\mathcal{B}}, \norm(\mathbf{A}_{\mathcal{B},\mathcal{B}}))  - \gcn(\mathbf{X}, \widetilde{\mathbf{A}})_{\mathcal{B}} \|_F \right].
% \end{align*}



% The mini-batch selection of subgraph sampling is motivated by an observation that, if the neighbors of \gongshi{$\mathcal{B}$} are as well belong to \gongshi{$\mathcal{B}$}, i.e., \gongshi{$\mathcal{N}_{\mathcal{B}}= \mathcal{B}$}, then the feature propagation on the subgraph \gongshi{$\widetilde{\mathbf{A}_{\mathcal{B},\mathcal{B}}}\mathbf{H}^{(l)}_{\mathcal{B}} = \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}}\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}}$} is exact. 


% \udfsection{Mini-batch selection of subgraph sampling.} 
% The subgraph \gongshi{$\mathbf{A}_{\mathcal{B},\mathcal{B}}$}
Compared with the whole message passing in Equation \eqref{eqn:transformation_conv}, subgraph sampling drops the edges \gongshi{$\mathcal{N}_{\mathcal{B}}^c \rightarrow \mathcal{B}$} from the original graph \gongshi{$\mathbf{A}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}}$}, leading to significant approximation errors. Thus, the mini-batch selection of subgraph sampling aims to \textit{minimize the graph cut} from a topological similarity perspective, i.e.,
\begin{align}
    \min_{ \mathcal{B}} \| (\mathbf{A}_{\mathcal{B},\mathcal{B}}, \mathbf{O}) -  \mathbf{A}_{\mathcal{B},\mathcal{N}_{\mathcal{B}} } \|_0=\min_{ \mathcal{B}}|\mathcal{N}_{\mathcal{B}}^c|,\label{eqn:cut_minimization}
    % \min_{ \mathcal{B}_1, \mathcal{B}_2,\dots, \mathcal{B}_n} \| \diag(\mathbf{A}_{\mathcal{B}_1,\mathcal{B}_1}, \mathbf{A}_{\mathcal{B}_2,\mathcal{B}_2}, \dots, \mathbf{A}_{\mathcal{B}_n,\mathcal{B}_n}) - \mathbf{A}_{\mathcal{V}',\mathcal{V}'} \|_0, \label{eqn:cut_minimization}
\end{align}
where \gongshi{$\mathbf{O} \in \mathbb{R}^{|\mathcal{B}| \times |\mathcal{N}_{\mathcal{B}}^c|}$} is a zero matrix.
%  between the sampled subgraphs \gongshi{$\mathbf{A}_{\mathcal{B},\mathcal{B}}$} and the original graph \gongshi{$\mathbf{A}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}}$}
Notably, as a connected graph cannot be divided into two disjointed subgraphs without dropping edges, the optimal value of \eqref{eqn:cut_minimization} is always positive in the connected graph.

% where \gongshi{$\gnn = f^{(L)} \circ f^{(L-1)} \dots \circ f^{(1)}$}.
% As the optimal value of \eqref{eqn:cut_minimization} is always positive in the connected graph, existing subgraph sampling methods suffer from the sampling bias due to the dropped neighborhood embeddings $\mathbf{H}_{\mathcal{N}^{(l)}(\mathcal{B})-\mathcal{B}}$.



% As the feature propagation discards the edges to the out-of-batch neighbors \gongshi{$\mathcal{N}({\mathcal{B}})-\mathcal{B}$}, the selection for the mini-batch \gongshi{$\mathcal{B}$} aims to minimize the graph cut \gongshi{$|\mathcal{N}({\mathcal{B}})-\mathcal{B}|$} \citep{cluster_gcn, graphsaint, gas, lmc, graphfm}, 
% % \gongshi{$\mathcal{V}' = (\mathcal{B}_1, \mathcal{B}_2,\dots, \mathcal{B}_n)$} is a node ordering and \gongshi{$\diag$} merges the diagonal blocks \gongshi{$\mathbf{A}_{\mathcal{B}_i}$} into a modified adjacency matrix in $\mathbb{R}^{n \times n}$.



To minimize the graph cut \gongshi{$|\mathcal{N}_{\mathcal{B}}^c|$}, the cluster-based samplers \citep{cluster_gcn, gas, lmc, graphfm} first adopt graph clustering (e.g., METIS \citep{metis1} and Graclus \citep{graclus}) to partition the large-scale graph into \gongshi{$\{\mathcal{B}_1, \mathcal{B}_2, \dots, \mathcal{B}_n\}$} with small \gongshi{$|\mathcal{N}_{{\mathcal{B}_i}}^c|$} and then sample a subgraph induced by \gongshi{$\mathcal{B}_i$}.
Besides, the random-walk based  sampler \citep{graphsaint} first uniformly samples root nodes and then generates random walks \gongshi{$\mathcal{B}$} starting from the root nodes, which decreases the graph cut \citep[Chap. 7]{dlg}. %  \gongshi{$|\mathcal{N}_{\mathcal{B}} - \mathcal{B}|$}



\subsection{METIS}\label{sec:metis}
METIS is a widely used graph clustering technique \citep{cluster_gcn,gas}. 
Graph clustering aims to construct partitions over the nodes in a graph such that intra-links within clusters occur much more frequently than inter-links between different clusters \citep{metis1}. 
Intuitively, this results that neighbors of a node are located in the same cluster with high probability. 
METIS minimizes the graph cut from a topological similarity perspective, i.e. Equation \eqref{eqn:cut_minimization}, to maintain enough information in the original graph, thus reducing the accesses of inaccurate compensation made by the subgraph sampling method, making the computation faster and more accurate.

However, METIS algorithm is too time-consuming \citep{graphsaint} on large datasets (e.g. Ogbn-products and Ogbn-papers). Thus, we uniformly sample nodes to construct subgraphs of large datasets.


\subsection{Historical Embeddings}

% \subsection{GAS}\label{sec:gas}
GAS \citep{gas} further compensates for the messages from the out-of-batch neighbors by historical embeddings, which are defined by
\begin{align} 
    \mathbf{Z}^{(l+1)}_{\mathcal{B}}  \approx \underbrace{\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}}\mathbf{H}^{(l)}_{\mathcal{B}}}_{\textrm{\footnotesize {$\text{MP}_{\text{IB}}$}}} + \underbrace{\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}^c}
    \overline{\mathbf{H}}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c}}_{\textrm{\footnotesize Bias}},
\end{align}
where \gongshi{$\overline{\mathbf{H}}^{(l)}$} are historical embeedings.
GAS pulls historical embeddings from RAM or hard drive storage, making it significantly faster and more memory-efficient than the methods computing real up-to-date embeddings.
% \begin{align} 
%     \mathbf{Z}^{(l+1)}_{\mathcal{B}}  \approx \underbrace{\widetilde{\mathbf{A}_{\mathcal{B},\mathcal{B}}}\mathbf{H}^{(l)}_{\mathcal{B}}}_{\textrm{\footnotesize {$\text{MP}_{\text{IB}}$}}}
% \end{align}


However, the historical embeddings suffer from large approximation errors due to the staleness issue \citep{gas, graphfm, lmc}.
Specifically, GAS updates the historical embeddings in each mini-batch average once per epoch and keeps their values between two consecutive updates of the mini-batch historical embeddings.
Thus, if the size of the sampled subgraphs is significantly smaller than the whole graph, the update of historical embeddings is infrequent due to very low node sampling probability, leading to large approximation errors of GAS.
Moreover, as the number of the out-of-batch neighbors is more than that of the nodes in the mini-batch subgraph on large-scale graphs (see Table 6 in \citep{gas}), pulling a large number of historical embeddings is still expensive.


% subgraphs and the original graph，不太行
% One of the major challenges in subgraph sampling is to reduce a significant sampling bias due to a large number of discarded messages outside the mini-batch.
% % to decrease the number of discarded messages 
% One straightforward idea is to sample subgraphs based on graph cut minimization, i.e., minimizing the number of discarded messages outside the mini-batch).
% For example, cluster-based subgraph sampling adopts graph clustering (e.g., METIS \citep{metis1} and Graclus \citep{graclus}) to partition the large-scale graph into a set of small subgraphs with low inter-connectivity.
% Besides, \citep{graphsaint} proposes a random-walk based  sampler to decrease the inter-connectivity of subgraphs based on uniformly sampled root nodes.





% 子图采样需要构建子图
% 子图构建包含两个环节，一个是mini-batch of nodes选择，一个是邻接矩阵构建



% \udfsection{Feature propagation of subgraph sampling.} % 删掉，或者往后放,GNN改成GCN

% To tackle the neighbor explosion issue, subgraph sampling propagates features on the subgraph induced by the mini-batch \gongshi{$\mathcal{B}$}, e.g. \gongshi{$\mathbf{Z}^{(l+1)}_{\mathcal{B}} \approx \widetilde{\mathbf{A}_{\mathcal{B},\mathcal{B}}}\mathbf{H}^{(l)}_{\mathcal{B}}$} \citep{cluster_gcn,graphsaint} or \gongshi{$\mathbf{Z}^{(l+1)}_{\mathcal{B}} \approx \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}}\mathbf{H}^{(l)}_{\mathcal{B}} + \mathbf{B}$} \citep{gas, lmc}, where \gongshi{$\mathbf{B}=\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}({\mathcal{B}})-\mathcal{B}}\overline{\mathbf{H}}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}$} ignores the out-of-batch gradients \gongshi{$\nabla_{\overline{\mathbf{H}}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}} \mathbf{Z}^{(l+1)}_{\mathcal{B}}$} by nondifferentiable historical embeddings \gongshi{$\overline{\mathbf{H}}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}$}. 
% % The convolution support \gongshi{$\widetilde{\mathbf{A}_{\mathcal{B},\mathcal{B}}}$} normalizes the submatrix \gongshi{$\mathbf{A}_{\mathcal{B},\mathcal{B}}$}.
% We also introduce more related works to address the neighbor explosion issue in Appendix x. % \ref{sec:related_work}.


% % Subgraph sampling has two steps: the mini-batch selection and the subgraph construction.










% % Given a mini-batch \gongshi{$\mathcal{B}$}, 


\subsection{Layer-wise Inference in Evaluation Phase}  \label{sec:gcm2}

To ensure the exact inference results on the large graphs, graph sampling usually adapts layer-wise inference, which iteratively updates all node embeddings at each layer without dropping edges. 
Specifically, the nodes are partitioned into \gongshi{$n$} mini-batches with batch size \gongshi{$|\mathcal{B}$|}, denoted as \gongshi{$\mathcal{B}_1,\mathcal{B}_2,...,\mathcal{B}_n$}.
At the \gongshi{$l$}-th layer, layer-wise inference traverses all mini-baches by
\begin{align*}
    \mathbf{H}^{(l+1)}_{\mathcal{B}_i} =  \gcn(\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}_i}}, \widetilde{\mathbf{A}}_{\mathcal{B}_i,\mathcal{N}_{\mathcal{B}_i}}),\text{ for }i\in\{1,2,...,n\}.
\end{align*}


Then, layer-wise inference iteratively updates \gongshi{$\mathbf{H}^{(l+1)}$} on the entire graph based on the previous embeddings \gongshi{$\mathbf{H}^{(l)}$}.
% After obtaining \gongshi{$\mathbf{H}^{(l+1)}$}  on the entire graph, the model starts the inference of obtaining \gongshi{$\mathbf{H}^{(l+2)}$} .


For each computation within a batch, the input \gongshi{$ \mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}_i}}$} is exact, and the adjacency matrix \gongshi{$\widetilde{\mathbf{A}}_{\mathcal{B}_i,\mathcal{N}_{\mathcal{B}_i}}$} aggregates all the neighbor information. Therefore, \gongshi{$ \mathbf{H}^{(l+1)}_{\mathcal{N}_{\mathcal{B}_i}}$}  is also exact. Since the model is computed layer-wise, each layer's \gongshi{$ \mathbf{H}^{(l+1)}$} is exact. As a result, the final output of the model is exact inference results.

Due to the fact that this layer-wise inference requires the computation of a large amount of data beyond the evolution dataset, it can lead to potential computational redundancy, resulting in significant computational overhead.
TOP does not use this layer-wise inference in experiments, which significantly saves computational costs. 
% If Equation 3.3 could be applied during the evolution phase, it would substantially reduce this potential redundancy. However, current methods using Equation 3.3 result in data loss, which in turn leads to less exact inference results. Therefore, we hope to propose a method that allows Equation 3.3 to obtain dropout node information, thereby enabling the model to improve computational efficiency while maintaining exactness in inference.

% However, not using edge dropout will cause 
%  high computational overhead. In large graphs, the number of edges can be very high, and not droping edge out can slow down the model's training speed. During the evaluation phase of the model, the dataset being evaluated often represents only a part of the entire graph's nodes. However, without edge dropout, the model needs to iteratively updates all node embeddings at each layer, leading to computational redundancy.


% \section{1-WL Test}\label{sec:1-WL test}

% Given initial node feature/representation \gongshi{$h_{u}^{(0)}$}, at the $l$-th iterations, 1-WL test for GCNs updates the node representation \gongshi{$h_{i}^{(l-1)}$} based on the local neighborhood by
% \begin{align}\label{eqn:hash}
%     h_{i}^{(l)} = Hash(\{\{ (h_{u}^{(l-1)},\widetilde{A}_{iu}),u\in \overline{\mathcal{N}}_{i} \triangleq \mathcal{N}_{i} \cup \{i\} \}\}).
% \end{align}

% Following \citep{gin}, we show the connections between GNNs and 1-WL test in Lemma \ref{lemma:pro_of_isomor_nodes}.

% \begin{lemma}\label{lemma:pro_of_isomor_nodes}
%     Given a graph \gongshi{$\mathcal{G}=(\mathcal{V},\mathcal{E})$} and a GNN, if nodes \gongshi{$i,j \in \mathcal{V}$} are indistinguishable under \gongshi{$l$} iterations of the 1-WL test, then there holds
%     \begin{align*}
%         \mathbf{H}_{i}^{(l)} = \mathbf{H}_{j}^{(l)},
%     \end{align*}
%     for all GNN parameters.
% \end{lemma}

% \begin{proof} 
%     As \gongshi{$i,j$} are indistinguishable under \gongshi{$l$} iterations of 1-WL test, we have \gongshi{$h_{i}^{(l)}=h_{j}^{(l)}$}. Notice that the function \gongshi{$Hash$} is injective, we have
%     \begin{align*}
%         \{\{ (h_{u}^{(l-1)},\widetilde{A}_{iu}),u \in \overline{\mathcal{N}}_{i} \}\}=\{\{ (h_{v}^{(l-1)},\widetilde{A}_{jv}),v \in \overline{\mathcal{N}}_{j} \}\}.
%     \end{align*}
%     Then, we can know that
%     \begin{align*}
%         \{\{ (h_{p}^{(l-2)},\widetilde{A}_{up}),u \in \overline{\mathcal{N}}_{i}, p \in \overline{\mathcal{N}}_{u}\}\}=\{\{ (h_{q}^{(l-2)},\widetilde{A}_{vq}),v \in \overline{\mathcal{N}}_{j},q \in \overline{\mathcal{N}}_{v} \}\},
%     \end{align*}
%     which is equivalent to
%     \begin{align*}
%         \{\{ (h_{p}^{(l-2)},\widetilde{A}_{up}),u \in \overline{\mathcal{N}}_{i}, p \in \overline{\mathcal{N}}_{i}^{2}\}\}=\{\{ (h_{q}^{(l-2)},\widetilde{A}_{vq}),v \in \overline{\mathcal{N}}_{j},q \in \overline{\mathcal{N}}_{j}^{2} \}\}.
%     \end{align*}
%     Recursively, we have
%     \begin{gather*}
%         \{\{ (h_{p}^{(l-3)},\widetilde{A}_{up}),u \in \overline{\mathcal{N}}_{i}^{2}, p \in \overline{\mathcal{N}}_{i}^{3}\}\}=\{\{ (h_{q}^{(l-3)},\widetilde{A}_{vq}),v \in \overline{\mathcal{N}}_{j}^{2},q \in \overline{\mathcal{N}}_{j}^{3} \}\} \\
%         \vdots\\
%         \{\{ (h_{p}^{(0)},\widetilde{A}_{up}),u \in \overline{\mathcal{N}}_{i}^{l-1}, p \in \overline{\mathcal{N}}_{i}^{l}\}\}=\{\{ (h_{q}^{(0)},\widetilde{A}_{vq}),v \in \overline{\mathcal{N}}_{j}^{l-1},q \in \overline{\mathcal{N}}_{j}^{l} \}\}.
%     \end{gather*}
%     As \gongshi{$\mathbf{x}_{k}=h_{k}^{(0)}$}, we have
%     \begin{align*}
%         \{\{ (\mathbf{x}_{p},\widetilde{A}_{up}),u \in \overline{\mathcal{N}}_{i}^{l-1}, p \in \overline{\mathcal{N}}_{i}^{l}\}\}=\{\{ (\mathbf{x}_{q},\widetilde{A}_{vq}),v \in \overline{\mathcal{N}}_{j}^{l-1},q \in \overline{\mathcal{N}}_{j}^{l} \}\}.
%     \end{align*}
%     Thus, we have
%     \begin{gather*}
%         \mathbf{H}_{\overline{\mathcal{N}}^{l-1}_{i}}^{(1)} = \sigma(\widetilde{A}_{\overline{\mathcal{N}}^{l-1}_{i},\overline{\mathcal{N}}^{l}_{i}} \mathbf{X}_{\overline{\mathcal{N}}^{l}_{i}} \mathbf{W}^{(0)}) = \sigma(\widetilde{A}_{\overline{\mathcal{N}}^{l-1}_{j},\overline{\mathcal{N}}^{l}_{j}} \mathbf{X}_{\overline{\mathcal{N}}^{l}_{j}} \mathbf{W}^{(0)}) = \mathbf{H}_{\overline{\mathcal{N}}^{l-1}_{j}}^{(1)}\\
%         \vdots\\
%         \mathbf{H}_{\overline{\mathcal{N}}_{i}}^{(l-1)} = \sigma(\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}^{2}_{i}} \mathbf{H}^{(l-2)}_{\overline{\mathcal{N}}^{2}_{i}} \mathbf{W}^{(l-2)}) = \sigma(\widetilde{A}_{\overline{\mathcal{N}}_{j},\overline{\mathcal{N}}^{2}_{j}} \mathbf{H}^{(l-2)}_{\overline{\mathcal{N}}^{2}_{j}} \mathbf{W}^{(l-2)}) = \mathbf{H}_{\overline{\mathcal{N}}_{j}}^{(l-1)}\\
%         \mathbf{H}_{i}^{(l)} = \sigma(\widetilde{A}_{i,\overline{\mathcal{N}}_{i}} \mathbf{H}^{(l-1)}_{\overline{\mathcal{N}}_{i}} \mathbf{W}^{(l-1)}) = \sigma(\widetilde{A}_{j,\overline{\mathcal{N}}_{j}} \mathbf{H}^{(l-1)}_{\overline{\mathcal{N}}_{j}} \mathbf{W}^{(l-1)}) = \mathbf{H}_{j}^{(l)}
%     \end{gather*}
%     for all GNN parameters. 
% \end{proof}



\section{Implementation Details}\label{sec:implementation}

\subsection{TOP for Variant GNNs}\label{sec:TOP_for_GNNs}

% Equation \eqref{eqn:mini_batch_mn} is easy to extend to the GNNs with sum and mean aggregation by different normalizations of the adjacency matrix.


We also extend TOP to the message passing framework for variant message passing-based GNNs. The \gongshi{$l$}-th layer of GNNs is defined as
\begin{align}\label{eqn:mp}
    \mathbf{h}_i^{(l+1)} = f^{(l+1)}\left(\mathbf{h}_i^{(l)}, \left\{\left\{\mathbf{h}_j^{(l)} \right\}\right\}_{j \in \mathcal{N}_i} \right),
\end{align}
where \gongshi{$\left\{\left\{ \dots \right\}\right\}$} denotes the multiset. We separate the neighborhood information in Equation \eqref{eqn:mp} of the multiset into two parts
\begin{align} \nonumber
    \mathbf{h}_i^{(l+1)} &= f^{(l+1)}\left(\mathbf{h}_i^{(l)}, \left\{\left\{\mathbf{h}_j^{(l)} \right\}\right\}_{j \in \mathcal{N}_i \cap \mathcal{B}}    \cup  \left\{\left\{\mathbf{h}_j^{(l)} \right\}\right\}_{j \in \mathcal{N}_i- \mathcal{B}} \right)\\
    &\approx f^{(l+1)}\left(\mathbf{h}_i^{(l)}, \left\{\left\{\mathbf{h}_j^{(l)} \right\}\right\}_{j \in \mathcal{N}_i \cap \mathcal{B}}    \cup  \left\{\left\{ \mathbf{r}_{j} \mathbf{H}_{\mathcal{B}} \right\}\right\}_{j \in \mathcal{N}_i- \mathcal{B}} \right), \label{eqn:mp_top}
\end{align}
where \gongshi{$\mathbf{r}_j$} is the \gongshi{$j$}-th row of the coefficient matrix \gongshi{$\mathbf{R}$}. Equation \eqref{eqn:mp_top} does not depend on the out-of-batch neighborhood information, achieving a linear computational complexity. We estimate the coefficient matrix \gongshi{$\mathbf{R}$} by
\begin{align}\label{eqn:min_mp}
    \min_{\mathbf{R}} \| \overline{\mathbf{H}}_{\mathcal{N}_{\mathcal{B}}^c} - \mathbf{R} \overline{\mathbf{H}}_{\mathcal{B}} \|_F,
\end{align}
We provide more details for the estimation of \gongshi{$\mathbf{R}$} in Appendix \ref{sec:fecm}.

\subsection{Implementation of GCNII}\label{sec:gcn_jk}

We follow the implementation\footnote{https://github.com/rusty1s/pyg\_autoscale} of GAS \citep{gas}, which introduces the jumping knowledge connection \citep{jknet} to accelerate the convergence \citep{acsc} for some GNN models.


We first run GCNII \citep{gcnii} to generate embeddings \gongshi{$\mathbf{H}^{(l)}_{\mathcal{B}}$} for each GNN layer \gongshi{$l$}.
Then, we compute the final embeddings by the jumping knowledge connection \citep{jknet}
\begin{align*}
   \mathbf{H}^{final}_{\mathcal{B}} = MLP^{output}( \frac{1}{L+1}\sum_{l=0}^{L} MLP^{(l)}(\mathbf{H}^{(l)}_{\mathcal{B}})),
\end{align*}
where MLP is a multi-layer perceptron.
We find the best hyperparameters \gongshi{$\alpha,\lambda$} of GCNII by grid search on the Ogbn-products and Ogbn-papers dataset.




\subsection{Hyperparameters} \label{sec:hyperparameters}


\udfsection{Comparison with subgraph sampling.} To ensure a fair comparison, we follow the GNN architectures, the data splits, training pipeline, and hyperparameters of GCN and PNA in \citep{gas}.
We search the best hyperparameters of GCNII, GAT, and SAGE for TOP, CLUSTER, and GAS in the same set.


\udfsection{Comparison with node/layer-wise sampling.} 
We run NS and LABOR by the official implementation\footnote{https://github.com/dmlc/dgl/tree/master/examples/pytorch/labor} of LABOR \citep{labor} and corresponding hyperparameters. For TOP, we uniformly sample nodes to construct subgraphs. To ensure a fair comparison, TOP follows the data splits, training pipeline, learning rate, and hyperparameters of LABOR \citep{labor}.
We adapt the batch size of TOP such that the memory consumption of TOP is similar to LABOR.
% \footnote{https://github.com/dmlc/dgl/tree/master/examples/pytorch/labor}
% We also implement TOP with the same SAGE model based on the codes and toolkits of GAS \citep{gas}. 
% with the hyperparameters \textit{--fan-out=10} and \textit{--lad-out="16000,11000,5000,2500,1500,500"} in the official implementation.
%  We also implement TOP with the same SAGE model based on the codes and toolkits of GAS \citep{labor}. 
% with the hyperparameters \textit{--fan-out=10} and \textit{--lad-out="16000,11000,5000,2500,1500,500"} in the official implementation.
% with the hyperparameters \textit{--fan-out=10} and \textit{--lad-out="16000,11000,5000,2500,1500,500"} in the official implementation.


\begin{table*}[t]
    \centering
    \caption{
    \textbf{Time and space complexity per gradient update of full-batch gradient descent with whole message passing (Full-batch), CLUSTER \citep{cluster_gcn}, GAS \citep{gas}, LMC \citep{lmc}, and TOP.} 
    }
    \label{tab:complexity}
    % \vspace{-0.5mm}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lccc}
    \toprule
        \textbf{Method} & \textbf{Time complexity}  &\textbf{GPU Memory}  & \textbf{Neighborhood Compensation} \\
        \midrule
        Full-batch  & $\mathcal{O}(L(|\mathcal{E}|d+|\mathcal{V}| d^2))$ & $\mathcal{O}(L|\mathcal{V}| d)$ &  $\checkmark$\\
        CLUSTER  & $\mathcal{O}( L(deg_{\max}|\mathcal{B}|d+|\mathcal{B}| d^2) )$ & $\mathcal{O}(L|\mathcal{B}| d)$ &  $\times$\\
        GAS and LMC & $\mathcal{O}( L(deg_{\max}|\mathcal{B}|d+|\mathcal{B}| d^2) )$ & $\mathcal{O}( deg_{\max} L|\mathcal{B}| d)$ &  $\checkmark$\\
        \midrule
        TOP & $\mathcal{O}( L(deg_{\max}|\mathcal{B}|d+|\mathcal{B}| (d^2 +k^2)) )$ & $\mathcal{O}(L|\mathcal{B}| d)$ &  $\checkmark$\\
    \bottomrule
    \end{tabular}
    }
    % \vspace{-3mm}
\end{table*}


\subsection{Fast Estimation of Coefficient Matrix}\label{sec:fecm}

We compute the coefficient matrix \gongshi{$\mathbf{R}$} by solving Equation \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} = \mathbf{R} \mathbf{H}^{(l)}_{\mathcal{B}}$}.
If the size of the subgraph \gongshi{$|\mathcal{B}|$} is large, then solving the linear equation \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^c} = \mathbf{R} \mathbf{H}^{(l)}_{\mathcal{B}}$} is expensive. As the rank of  \gongshi{$\overline{\mathbf{H}}_{\mathcal{B}}$} is less than the hidden dimension \gongshi{$d << |\mathcal{B}|$}, there exists a low-rank matrix decomposition such that
\begin{align*}
    \overline{\mathbf{H}}_{\mathcal{B}} = \mathbf{Q}\mathbf{Q}^{\top}\overline{\mathbf{H}}_{\mathcal{B}},
\end{align*}
where \gongshi{$\mathbf{Q} \in \mathbb{R}^{|\mathcal{B}| \times k}$} has orthogonal columns.
\gongshi{$k \geq d$} is a {hyperparameter}. We use \gongshi{$k=d$} in all experiments.
We use the proto-algorithm \citep{rsvd} to efficiently compute \gongshi{$\mathbf{Q}$}.
By letting \gongshi{$ \hat{\mathbf{R}} = \mathbf{R} \mathbf{Q} \in \mathbb{R}^{|\mathcal{N}_{\mathcal{B}}^c| \times d}$}, Equation \eqref{eqn:min_mp} becomes
\begin{align}\label{eqn:solver}
    \min_{\hat{\mathbf{R}}} \|\mathbf{Y}_{\mathcal{B}}(\overline{\mathbf{H}}) - \widetilde{\mathbf{A}}_{\mathcal{B}, \mathcal{N}_{\mathcal{B}}^c}\hat{\mathbf{R}} (\mathbf{Q}^{\top} \overline{\mathbf{H}}_{\mathcal{B}})  \|_F.
\end{align}
Further, we uniformly sample a small set \gongshi{$\mathcal{S}$} with \gongshi{$|\mathcal{S}| = k$} from \gongshi{$\mathcal{B}$} to reduce the costs by
\begin{align*}
     \min_{\hat{\mathbf{R}}} \| \mathbf{Y}_{\mathcal{B}}(\overline{\mathbf{H}}) - \widetilde{\mathbf{A}}_{\mathcal{B}, \mathcal{N}_{\mathcal{B}}^c} \hat{\mathbf{R}} (\mathbf{Q}_{\mathcal{S}}^{\top} \overline{\mathbf{H}}_{\mathcal{S}})  \|_F,
\end{align*}
which is equivalent to
\begin{align*}
    \min_{\hat{\mathbf{R}}} \| \widetilde{\mathbf{A}}_{\mathcal{B}, \mathcal{N}_{\mathcal{B}}^c} ( \overline{\mathbf{H}}_{\mathcal{N}_{\mathcal{B}}^{c}} -  \hat{\mathbf{R}} (\mathbf{Q}_{\mathcal{S}}^{\top} \overline{\mathbf{H}}_{\mathcal{S}}) )  \|_F.
\end{align*}


Since \gongshi{$\overline{\mathbf{H}}_{\mathcal{S}}$} is usually the full-column-rank matrix, we can compute \gongshi{$\hat{\mathbf{R}}$} by \gongshi{$\hat{\mathbf{R}} = \mathbf{H}_{\mathcal{N}_{\mathcal{B}}^{c}} (\mathbf{Q}_{\mathcal{S}}^{\top} \overline{\mathbf{H}}_{\mathcal{S}})^{\dagger}$} and then save \gongshi{$\partial \hat{\mathbf{A}}_{\mathcal{B}} = \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{N}_{\mathcal{B}}^c}\hat{\mathbf{R}}$} in the pre-processing phase, where \gongshi{$(\mathbf{Q}_{\mathcal{S}}^{\top} \overline{\mathbf{H}}_{\mathcal{S}})^{\dagger}$} is the Moore-Penrose inverse of \gongshi{$\mathbf{Q}_{\mathcal{S}}^{\top} \overline{\mathbf{H}}_{\mathcal{S}}$}.
At the training phase, Equation \eqref{eqn:mini_batch_mn} becomes
\begin{align}
    \mathbf{Z}^{(l+1)}_{\mathcal{B}}=\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}}\mathbf{H}^{(l)}_{\mathcal{B}} + \partial \hat{\mathbf{A}}_{\mathcal{B}}(\mathbf{Q}^{\top}_{\mathcal{S}} \mathbf{H}^{(l)}_{\mathcal{S}}),\label{eqn:mini_batch_top_efficient}
\end{align}
where \gongshi{$\partial \hat{\mathbf{A}}_{\mathcal{B}} \in \mathbb{R}^{|\mathcal{B}| \times k}$}, \gongshi{$\hat{\mathbf{Q}}_{{\mathcal{S}}} \in \mathbb{R}^{k \times k}$}, and \gongshi{$\mathbf{H}^{(l)}_{\mathcal{S}} \in \mathbb{R}^{k \times d}$}.
The time complexity of the second term in Equation \ref{eqn:mini_batch_top_efficient} is \gongshi{$\mathcal{O}(|\mathcal{B}|k^2+k^2d)$}, which is significantly lower than that in Equation \eqref{eqn:mini_batch_mn}, i.e., \gongshi{$\mathcal{O}(|\mathcal{B}|^2d)$}, as \gongshi{$|\mathcal{B}|>>d$}.



The analysis for message passing-based GNNs is similar.

\subsection{Decrease Approximation Errors by More Basic Embeddings}


By concatenating $N$ basic embeddings at different random initialization, we can estimate $\mathbf{R}$ by $(\overline{\mathbf{H}}(\mathcal{W}^{(rand)}_1),\overline{\mathbf{H}}(\mathcal{W}^{(rand)}_2),\dots,\overline{\mathbf{H}}(\mathcal{W}^{(rand)}_N)) \in \mathbb{R}^{n \times (T+1)dN}$, which decreases the approximation errors as $N$ increases.




\subsection{Complexity Analysis}\label{sec:complexity}

We summarize TOP in Algorithms \ref{alg:cap} and \ref{alg:top}.
TOP first pre-processes the topological compensation by Algorithm \ref{alg:cap} and then reuses the topological compensation during the training phase.




% At the \gongshi{$k$}-th training step, TOP first randomly samples a subgraph constructed by \gongshi{$\mathcal{B}_{i}$}.
% Then, TOP performs the message passing based on the in-batch nodes and the pre-processed topological compensation.
% Then, LMC updates the stored historical node embeddings \gongshi{$\overline{\embH}^{l,k}_{\mathcal{V}_{b_k}}$} in the order of \gongshi{$l=1,\ldots,L$} by Equations (\ref{eqn:mini_mpeq_update})--(\ref{eqn:subexact_compute}), and the stored historical auxiliary variables \gongshi{$\overline{\embV}^{l,k}_{\mathcal{V}_{b_k}}$} in the order of \gongshi{$l=L-1,\ldots,1$} by Equations (\ref{eqn:mini_mpeq_auxiliary})--(\ref{eqn:subexact_compute_auxiliary}). By the randomly update, the historical values get close to the exact up-to-date values. Finally, for \gongshi{$l\in[L]$} and \gongshi{$v_j\in\mathcal{V}_{b_k}$}, by replacing \gongshi{$\embh^{l,k}_j$}, \gongshi{$\embm^{l,k}_{\neighbor{v_j}}$} and \gongshi{$\embV^{l,k}_j$} in Equations (\ref{eqn:mini-batch_grad_w}) and (\ref{eqn:mini-batch_grad_theta}) with \gongshi{$\overline{\embh}^{l,k}_j$}, \gongshi{$\overline{\embm}^{l,k}_{\neighbor{v_j}}$}, and \gongshi{$\overline{\embV}^{l,k}_j$}, respectively, LMC computes mini-batch gradients \gongshi{$\widetilde{\mathbf{g}}_{w},\widetilde{\mathbf{g}}_{\theta^1},\ldots,\widetilde{\mathbf{g}}_{\theta^L}$} to update parameters \gongshi{$w,\theta^1,\ldots,\theta^L$}.





% \begin{algorithm}
% \caption{Pre-processing phase of TOP}\label{alg:cap}
% \begin{algorithmic}[1]
%     \STATE {\bfseries Input:} Mini-batches $\{\mathcal{B}_i\}_{i=1}^m$
%     \FOR{$l=0,...,L-1$}
%     \STATE Compute $\mathbf{H}^{(l)}$ with a model at random initialization.
%     \FOR{$i=1,...,m$}
%     \STATE Compute $\mathbf{Q}_{\mathcal{S}_i}^{(l)}$ by the proto-algorithm.
%     \STATE Compute $\hat{\mathbf{R}}$ by solving Equation \eqref{eqn:solver}.
%     \STATE Compute $\hat{\mathbf{C}}_{\mathcal{B}_i}^{(l,s)}=\mathbf{C}^{(l,s)}_{\mathcal{B}_i,\mathcal{N}({\mathcal{B}_i})-\mathcal{B}_i}\hat{\mathbf{R}}$ 
%     \ENDFOR
%     \STATE Save $\{\mathbf{Q}_{\mathcal{S}_i}^{(l)}\}_{i=1}^m$ and $\{\hat{\mathbf{C}}_{\mathcal{B}_i}^{(l,s)}\}_{i=1}^m$
%     \ENDFOR
%     \STATE {\bfseries Output:} $\{\{\mathbf{Q}_{\mathcal{S}_i}^{(l)}\}_{i=1}^m\}_{l=0}^{L-1}$ and $\{\{\hat{\mathbf{C}}_{\mathcal{B}_i}^{(l,s)}\}_{i=1}^m\}_{l=0}^{L-1}$
% \end{algorithmic}
% \end{algorithm}


\begin{algorithm}
\caption{Pre-processing phase of TOP}\label{alg:cap}
\begin{algorithmic}[1]
    \State {\bfseries Input:} Mini-batches $\{\mathcal{B}_i\}_{i=1}^m$
    % \For{$l=0,...,L-1$}
    \State Compute $\mathbf{H}^{(l)}$ with a model at random initialization.
    \For{$i=1,...,m$}
    \State Compute $\mathbf{Q}_{\mathcal{S}_i}$ by the proto-algorithm.
    \State Compute $\hat{\mathbf{R}}$ by solving Equation \eqref{eqn:solver}.
    \State Compute $\partial \hat{\mathbf{A}}_{\mathcal{B}_i}=\widetilde{\mathbf{A}}_{\mathcal{B}_i,\mathcal{N}_{\mathcal{B}_i}^c}\hat{\mathbf{R}}$ 
    \EndFor
    \State Save $\{\mathbf{Q}_{\mathcal{S}_i}\}_{i=1}^m$ and $\{\partial \hat{\mathbf{A}}_{\mathcal{B}_i}\}_{i=1}^m$
    % \EndFor
    \State {\bfseries Output:} $\{\mathbf{Q}_{\mathcal{S}_i}\}_{i=1}^m$ and $\{\partial \hat{\mathbf{A}}_{\mathcal{B}_i}\}_{i=1}^m$
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\caption{Training phase of TOP}\label{alg:top}
\begin{algorithmic}[1]
    \State {\bfseries Input:} Mini-batches $\{\mathcal{B}_i\}_{i=1}^m$, $\{\mathbf{Q}_{\mathcal{S}_i}\}_{i=1}^m$, and $\{\partial \hat{\mathbf{A}}_{\mathcal{B}_i}\}_{i=1}^m$
    \For{\gongshi{$i = 1, \dots, N$}}
            \State Randomly sample \gongshi{$\mathcal{B}_{i}$} from \gongshi{$\{\mathcal{B}_i\}_{i=1}^m$}
            \State Initialize \gongshi{$\mathbf{H}^{(0)}_{\mathcal{B}_i}=\mathbf{X}_{\mathcal{B}_i}$}
            \For{\gongshi{$l=0,\dots,L-1$}}
                \State Compute $\mathbf{H}^{(l+1)}_{\mathcal{B}_i} = \sigma( \widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}} \mathbf{H}^{(l)}_{\mathcal{B}_i} +(\partial \hat{\mathbf{A}}_{\mathcal{B}_i} (\mathbf{Q}_{\mathcal{S}_i}^T \mathbf{H}^{(l)}_{\mathcal{B}_i}) )  \mathbf{W}^{(l)}) $
            \EndFor
            \State Compute the mini-batch loss
            \State Update parameters by backward propagation
    \EndFor
\end{algorithmic}
\end{algorithm}


As the costs of pre-processing are marginal, we compare the computational complexity of the training phase in Table \ref{tab:complexity}. TOP compensates for the neighborhood messages with the least time and memory complexity among existing subgraph sampling methods.




\section{More Experiments}\label{sec:more_exp}

\subsection{Measuring Message Invariance in Real-world Datasets.}\label{sec:ams_exp}


We conduct extensive experiments on four real-world datasets with five GNN backbones to demonstrate that the message invariance holds in real-world datasets.
Table \ref{tab:ams} shows that the relative approximation errors of TOP are less than 5\% and the test accuracy of TOP is very close to the whole message passing.


\begin{table}
  \centering
  \caption{%
    \textbf{Message invariance in real-world datasets.} TOP approximates the whole message passing solely through {$\text{MP}_{\text{IB}}$} with marginal approximation errors.
  }\label{tab:ams}
  \setlength{\tabcolsep}{2pt}
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{c|c|c|rrrrr|rrrrr}
    \toprule
    % \\
 \mr{2}{\textbf{Dataset}}    &   \mr{2}{\textbf{GNN}}      & \mr{2}{\textbf{Methods}}      & 
 \multicolumn{5}{c|}{{\textbf{Relative approximation errors} $\downarrow$}}
  & \mc{5}{c}{\textbf{Test accuracy degradation $\downarrow$}} \\
     &                    &       & 10\%  & 20\%  & 30\%  & 40\%  & 50\%  & 10\%  & 20\%  & 30\%  & 40\%  & 50\% \\
    \midrule
\multirow{12}{*}{Ogbn-arxiv}   & \multirow{3}{*}{GCN}   & CLUSTER & 23.8\%  & 20.2\%  & 18.0\%  & 15.8\%  & 12.1\%  & 5.03\%  & 3.67\%  & 2.99\% & 2.66\% & 1.40\% \\
                          &                        & GAS     & 17.1\%  & 15.3\%  & 14.1\%  & 12.8\%  & 9.9\%   & 0.80\%  & 0.78\%  & 0.58\% & 0.61\% & 0.33\% \\
                          &                        & TOP     & \textbf{3.5}\%   & \textbf{2.8}\%   & \textbf{2.4}\%   & \textbf{2.2}\%   & \textbf{1.6}\%   & \textbf{0.15}\%  & \textbf{0.10 }\% & \textbf{0.15}\% & \textbf{0.12}\% & \textbf{0.13}\% \\ \cmidrule{2-13}
                          & \multirow{3}{*}{GCNII} & CLUSTER & 13.5\%  & 11.5\%  & 10.2\%  & 9.0\%   & 6.9\%   & 4.72\%  & 3.60\%  & 2.99\% & 2.38\% & 1.90\% \\ 
                          &                        & GAS     & 9.6\%   & 8.8\%   & 8.1\%   & 7.3\%   & 6.0\%   & 2.37\%  & 2.06\%  & 1.88\% & 1.67\% & 1.46\% \\
                          &                        & TOP     & \textbf{2.5}\%   & \textbf{2.3}\%   & \textbf{2.0}\%   & \textbf{1.8}\%   & \textbf{1.4 }\%  & \textbf{0.42}\%  & \textbf{0.28}\%  & \textbf{0.35}\% & \textbf{0.17}\% & \textbf{0.18}\% \\ \cmidrule{2-13}
                          & \multirow{3}{*}{SAGE}  & CLUSTER & 15.58\% & 13.08\% & 11.08\% & 9.91\%  & 7.06\%  & 4.37\%  & 3.69\%  & 3.04\% & 2.80\% & 1.91\% \\
                          &                        & GAS     & 24.77\% & 20.97\% & 16.80\% & 16.02\% & 10.47\% & 6.84\%  & 4.83\%  & 3.98\% & 3.75\% & 1.77\% \\
                          &                        & TOP     & \textbf{4.38}\%  & \textbf{3.69}\%  & \textbf{3.21}\%  & \textbf{2.89}\%  & \textbf{2.03}\%  & \textbf{0.08}\%  & \textbf{0.11}\%  & \textbf{0.01}\% & \textbf{0.10 }\%& \textbf{0.00}\% \\ \cmidrule{2-13}
                          & \multirow{3}{*}{GAT}   & CLUSTER & 23.37\% & 20.22\% & 18.00\% & 16.27\% & 12.67\% & 5.99\%  & 4.34\%  & 3.73\% & 3.09\% & 2.04\% \\
                          &                        & GAS     & 14.96\% & 13.47\% & 12.51\% & 11.53\% & 9.08\%  & 1.17\%  & 0.97\%  & 0.89\% & 0.80\% & 0.67\% \\
                          &                        & TOP     & \textbf{3.41}\%  & \textbf{2.99}\%  & \textbf{2.56}\%  & \textbf{2.30 }\% & \textbf{1.63}\%  & \textbf{0.15}\%  & \textbf{0.16}\%  & \textbf{0.10}\% & \textbf{0.08}\% & \textbf{0.08}\% \\ \midrule
\multirow{12}{*}{Reddit}  & \multirow{3}{*}{GCN}   & CLUSTER & 29.20\% & 22.10\% & 18.10\% & 16.02\% & 11.53\% & 3.85\%  & 3.05\%  & 2.34\% & 1.88\% & 1.14\% \\
                          &                        & GAS     & 27.13\% & 23.87\% & 21.07\% & 18.69\% & 15.00\% & 1.27\%  & 1.22\%  & 0.82\% & 0.68\% & 0.49\% \\
                          &                        & TOP     & \textbf{0.78}\%  & \textbf{0.65}\%  & \textbf{0.55}\%  & \textbf{0.53}\%  & \textbf{0.39}\%  & \textbf{0.09}\%  & \textbf{0.09}\%  & \textbf{0.08}\% & \textbf{0.06}\% & \textbf{0.03}\% \\ \cmidrule{2-13}
                          & \multirow{3}{*}{GCNII} & CLUSTER & 25.32\% & 19.62\% & 16.45\% & 14.94\% & 11.29\% & 4.79\%  & 3.59\%  & 2.45\% & 2.78\% & 1.46\% \\
                          &                        & GAS     & 32.87\% & 30.74\% & 27.52\% & 26.00\% & 23.05\% & 7.76\%  & 5.82\%  & 4.41\% & 4.51\% & 2.77\% \\
                          &                        & TOP     & \textbf{4.54}\%  & \textbf{4.75}\%  & \textbf{5.22}\%  & \textbf{3.62}\%  & \textbf{2.58}\%  & \textbf{0.33}\%  & \textbf{0.41}\%  & \textbf{0.54}\% & \textbf{0.31}\% & \textbf{0.30}\% \\ \cmidrule{2-13}
                          & \multirow{3}{*}{SAGE}  & CLUSTER & 10.74\% & 7.58\%  & 6.80\%  & 5.35\%  & 3.44\%  & 3.84\% & 2.85\% & 2.60\% & 2.13\% & 1.37\% \\
                          &                        & GAS     & 7.03\%  & 5.95\%  & 5.64\%  & 3.43\%  & 1.90\%  & 0.94\% & 0.60\% & 0.79\% & 0.41\% & 0.43\% \\
                          &                        & TOP     & \textbf{1.31}\%  & \textbf{1.10}\%  & \textbf{1.05}\%  & \textbf{0.85}\%  & \textbf{0.61}\%  & \textbf{0.31}\% & \textbf{0.27}\% & \textbf{0.21}\% & \textbf{0.19}\% & \textbf{0.11}\% \\ \cmidrule{2-13}
                          & \multirow{3}{*}{PNA}   & CLUSTER & 24.13\% & 21.40\% & 18.60\% & 16.70\% & 13.60\% & 4.16\%  & 3.20\%  & 1.76\% & 1.43\% & 1.13\% \\
                          &                        & GAS     & 22.39\% & 20.16\% & 18.77\% & 16.54\% & 13.38\% & 2.60\%  & 2.49\%  & 1.74\% & 1.64\% & 0.72\% \\
                          &                        & TOP     & \textbf{13.01}\% & \textbf{11.18}\% & \textbf{10.48}\% & \textbf{9.35}\%  & \textbf{7.42}\%  & \textbf{1.48}\%  & \textbf{1.28}\%  & \textbf{0.76}\% & \textbf{0.97}\% & \textbf{0.62}\% \\\midrule
\multirow{6}{*}{Yelp}     & \multirow{3}{*}{GCNII} & CLUSTER & 5.74\%  & 4.48\%  & 3.82\%  & 3.25\%  & 2.38\%  & 0.89\%  & 0.57\%  & 0.45\% & 0.34\% & 0.21\% \\
                          &                        & GAS     & 7.36\%  & 6.52\%  & 5.84\%  & 5.15\%  & 4.01\%  & 1.08\%  & 0.93\%  & 0.77\% & 0.64\% & 0.50\% \\
                          &                        & TOP     & \textbf{1.36}\%  & \textbf{1.24}\%  & \textbf{1.14}\%  & \textbf{1.05}\%  & \textbf{0.86}\%  & \textbf{0.13}\%  & \textbf{0.11}\%  & \textbf{0.09}\% & \textbf{0.08}\% & \textbf{0.05}\% \\ \cmidrule{2-13}
                          & \multirow{3}{*}{SAGE}  & CLUSTER & 15.55\% & 12.53\% & 10.85\% & 9.35\%  & 6.99\%  & 1.13\%  & 0.87\%  & 0.66\% & 0.56\% & 0.35\% \\
                          &                        & GAS     & 5.21\%  & 4.65\%  & 4.19\%  & 3.85\%  & 2.96\%  & 0.77\%  & 0.62\%  & 0.59\% & 0.53\% & 0.40\% \\
                          &                        & TOP     & \textbf{2.41}\%  & \textbf{2.18}\%  & \textbf{2.02}\%  & \textbf{1.84}\%  & \textbf{1.54}\%  & \textbf{0.07}\%  & \textbf{0.07}\%  & \textbf{0.05}\% & \textbf{0.04}\% & \textbf{0.04}\% \\\midrule
\multirow{3}{*}{Ogbn-products} & \multirow{3}{*}{SAGE}  & CLUSTER & 9.55\%  & 8.50\%  & 7.43\%  & 6.91\%  & 5.34\%  & 1.67\%  & 1.67\%  & 1.67\% & 1.67\% & 1.67\% \\
                          &                        & GAS     & 2.44\%  & 2.18\%  & 1.91\%  & 1.80\%  & 1.37\%  & 0.37\%  & 0.34\%  & 0.31\% & 0.27\% & 0.19\% \\
                          &                        & TOP     & \textbf{0.86}\%  & \textbf{0.73}\%  & \textbf{0.63}\%  & \textbf{0.58}\%  & \textbf{0.44}\%  & \textbf{0.17}\%  & \textbf{0.14}\%  & \textbf{0.12}\% & \textbf{0.11}\% & \textbf{0.11}\%\\\midrule
\multicolumn{2}{c|}{\multirow{3}{*}{\textbf{Average}}}       & CLUSTER & 17.86\% & 14.66\% & 12.67\% & 11.22\% & 8.48\%  & 3.68\% & 2.83\% & 2.24\% & 1.97\% & 1.33\% \\
\multicolumn{2}{l|}{}                               & GAS     & 15.54\% & 13.88\% & 12.40\% & 11.19\% & 8.83\%  & 2.36\% & 1.88\% & 1.52\% & 1.41\% & 0.88\% \\
\multicolumn{2}{l|}{}                               & TOP     & \textbf{3.46}\%  & \textbf{3.05}\%  & \textbf{2.85}\%  & \textbf{2.45}\%  & \textbf{1.86}\%  & \textbf{0.31}\% & \textbf{0.27}\% & \textbf{0.22}\% & \textbf{0.20}\% & \textbf{0.15}\%\\
    \bottomrule
  \end{tabular}
  }
\end{table}


\subsection{Convergence Curves (Test Accuracy vs. Epochs)}

We provide the convergence curves (test accuracy vs. epochs) in Figure \ref{fig:epoch}.
Notably, we report the test accuracy of the full-batch gradient descent (GD) every two steps rather than per epoch, as GD performs backward backpropagation once per epoch while other methods perform backward backpropagation twice per epoch on the Ogbn-arxiv and Reddit datasets.
The convergence curves of TOP are close to GD on the Ogbn-arxiv and Reddit datasets, while other subgraph sampling methods fail to resemble the full-batch performance on the Ogbn-arxiv dataset.
Moreover, TOP significantly accelerates the convergence on the medium and large datasets, e.g., Yelp, Ogbn-products, and Ogbn-papers.

\begin{figure*}[t!]
    \centering  
    \subfigure[GCN on small datasets]{
        \label{subfig:small_datasets_epoch}
        \includegraphics[width=0.31\textwidth]{Figure/exp_sec1/accuracy_epoch_gcn_small.pdf}}
    \subfigure[GCNII on medium datasets]{
        \label{subfig:medium_datasets_epoch}
        \includegraphics[width=0.31\textwidth]{Figure/exp_sec1/accuracy_epoch_gcnii_medium.pdf}}
    \subfigure[GCNII+JK on large datasets]{
        \label{subfig:large_datasets_epoch}
        \includegraphics[width=0.31\textwidth]{Figure/exp_sec1/accuracy_epoch_gcnii_large.pdf}}
    \caption{
        \textbf{Convergence curves (test accuracy vs. epoch).} \gongshi{$|\mathcal{B}|$} and \gongshi{$|\mathcal{V}|$} denote the sizes of  subgraphs and the whole graph respectively.
    }\label{fig:epoch}
\end{figure*}



\subsection{TOP on Architecture Variants}\label{sec:top_av}


We compare subgraph sampling methods (including TOP, CLUSTER \citep{cluster_gcn}, GAS \citep{gas}, SAINT \citep{graphsaint}, and IBMB \citep{ibmb}) on more GNN architectures (i.e., GAT \citep{gat} and SAGE \citep{graphsage}) in Figure \ref{fig:gat_sage}.
TOP is faster than the existing subgraph sampling methods on GAT and SAGE architectures due to its powerful convergence and high efficiency.
The experiments demonstrate that TOP is a general framework for different GNN architectures.


\begin{figure*}[h]
    \centering
    \subfigure[TOP with GCN]{
		\includegraphics[width=0.31\textwidth]{Figure/neurips_rebuttal/accuracy_runtime_gcn_ibmb.pdf}
    }
    \subfigure[TOP with GAT]{
        \includegraphics[width=0.31\textwidth]{Figure/neurips_rebuttal/accuracy_runtime_gat_ibmb.pdf}
    }
    \subfigure[TOP with SAGE]{
		\includegraphics[width=0.31\textwidth]{Figure/neurips_rebuttal/accuracy_runtime_sage_ibmb.pdf}
    }
    \caption{
    \textbf{Convergence curves (test accuracy vs. runtime (s)) on more GNN architectures (i.e., GAT \citep{gat} and SAGE \citep{graphsage}).}
    }
    \label{fig:gat_sage}
\end{figure*}


\subsection{Relative Runtime per Epoch and Relative Memory Consumption}\label{sec:relative_mc}

We report the relative runtime per Epoch and relative Memory Consumption in Table \ref{tab:memory_consumption}.
As the graph size increases, the subgraph ratio \gongshi{$|\mathcal{B}|/|\mathcal{V}|$} decreases.
TOP enjoys the least runtime and memory consumption among the baselines.

\begin{table*}
  \centering
  \caption{
  \textbf{ Efficiency of the full-batch gradient descent (Full-batch), GAS, TOP}. $R_{*}$ and $M_{*}$ denote the runtime per epoch and memory consumption of the algorithm $*$ respectively.
  }\label{tab:memory_consumption}
  \setlength{\tabcolsep}{10pt}
  \resizebox{1.0\linewidth}{!}{%
  \begin{tabular}{cccc|cccc|cccc}
    \toprule
    \mr{2}{\textbf{Dataset}} & \mr{2}{\textbf{Model}} & \mr{2}{$\frac{|\mathcal{B}|}{|\mathcal{V}|}$} & \mr{2}{$|\mathcal{B}|$} & \mc{3}{c}{\textbf{Runtime} (s)$\downarrow$} & \mr{2}{{\Large$\frac{R_{GAS}}{R_{TOP}}$}} & \mc{3}{c}{\textbf{Memory} (MB)$\downarrow$} & \mr{2}{{\Large$\frac{M_{GAS}}{M_{TOP}}$}} \\
    && & & Full-batch & GAS & \textbf{TOP} &  & Full-batch & GAS & \textbf{TOP} & \\
    \midrule
Ogbn-arxiv    & GCN      & 50.0\%    & 84672  & 0.63 & 0.35  & \textbf{0.33} & 1.07  & 2463.03 & 1566.89 & \textbf{1071.27} & 1.46 \\
Reddit        & GCN      & 50.0\%    & 116483 & 2.05 & 1.33  & \textbf{0.91} & 1.46  & 2796.30 & 2444.25 & \textbf{1334.78} & 1.83 \\
Reddit        & GCNII    & 50.0\%    & 116483 & 3.21 & 1.94  & \textbf{1.49} & 1.30  & 8240.29 & 5509.85 & \textbf{3837.94} & 1.44 \\
Yelp          & GCNII    & 12.5\% & 89606  & OOM  & 4.29  & \textbf{3.98} & 1.08  & OOM     & 6752.89 & \textbf{2231.76} & 3.03 \\
Ogbn-products & GCNII+JK & 4.0\%     & 97961  & OOM  & 70.68 & \textbf{6.04} & 11.70 & OOM     & 6574.37 & \textbf{1406.60} & 4.67 \\
    % \textsc{Ogbn-papers \& GCNII+JK} & 1.178 & \textbf{0.007} & 325.97 & \textbf{16.32} \\
    \bottomrule
  \end{tabular}
  }
\end{table*}



\subsection{Cost of Pre-processing and Training}
\label{sec:pretime}


We report the cost of pre-processing and training in experiments in Table \ref{tab:cost_preprocessing}. On the small and medium datasets (i.e., Ogbn-arxiv, REDDIT, and YELP), the total time of different methods is similar and TOP achieves the least GPU consumption in most experiments.
On the large datasets (i.e. Ogbn-products), TOP is significantly faster and more memory-efficient than existing subgraph sampling methods, as it can remove the valid and test nodes from the sampled subgraph without significant performance degradation. Specifically,  Equation \eqref{eqn:mini_batch_mn} compensates the neighborhood information from the valid and test nodes based on the mini-batch training nodes.
However, the valid and test nodes in subgraphs are important for CLUSTER, as these valid and test nodes are likely to be the neighbors of the training nodes.
Directly removing these nodes without any compensation results in significant performance degradation (see Figures \ref{fig:runtime} and \ref{fig:epoch}).
Besides, GAS needs to update the historical embeddings of valid and test nodes many times, leading to expensive computational costs.


\begin{table}[h]\centering
      \caption{%
      \textbf{The cost of pre-processing and training.}
      }\label{tab:cost_preprocessing}
    \setlength{\tabcolsep}{1.9mm}
    \resizebox{1.0\linewidth}{!}{
\begin{tabular}{ll|cccc}
    \toprule
 \textbf{GNN}  \& \textbf{Dataset}         & \textbf{Methods}        & \textbf{Pre-processing time (s)} & \textbf{Training time (s)} & \textbf{Total Time (s)} & \textbf{Memory (MB)} \\
    \midrule
\multirow{4}{*}{GCN \& arxiv}         & GraphSAINT & 0.0  & 122.0  & 122.0  & \underline{1144.1} \\
                                      & CLUSTER    & 1.7  & 92.8   & \textbf{94.5}   & 1312.0 \\
                                      & GAS        & 3.0  & 105.0  & 108.0  & 1566.9 \\
                                      & TOP        & 5.0  & 99.0   & \underline{104.0}  & \textbf{1071.3} \\\midrule
\multirow{4}{*}{SAGE \& arxiv}        & GraphSAINT & 0.0  & 114.5  & 114.5  & 1716.2 \\
                                      & CLUSTER    & 1.6  & 93.3   & \textbf{94.9 }  & \underline{1450.1} \\
                                      & GAS        & 3.0  & 107.8  & 110.7  & 1616.1 \\
                                      & TOP        & 5.3  & 98.9   & \underline{104.1}  & \textbf{1110.2} \\\midrule
\multirow{4}{*}{GAT \& arxiv}         & GraphSAINT & 0.0  & 63.3   & 63.3   & \textbf{2060.9} \\
                                      & CLUSTER    & 1.7  & 43.7   & \textbf{45.4}   & \underline{2253.6} \\
                                      & GAS        & 3.0  & 52.8   & \underline{55.7}   & 3025.9 \\
                                      & TOP        & 4.4  & 58.7   & 63.1   & 3177.9 \\\midrule
\multirow{4}{*}{GCN \& REDDIT}        & GraphSAINT & 0.0  & 387.6  & 387.6  & \underline{1398.2}\\
                                      & CLUSTER    & 14.9 & 351.7  & \textbf{366.7}  & 1955.2 \\
                                      & GAS        & 16.6 & 532.0  & 548.6  & 2444.3 \\
                                      & TOP        & 20.1 & 364.0  & \underline{384.1}  & \textbf{1334.8} \\\midrule
\multirow{4}{*}{GCNII \& REDDIT}      & GraphSAINT & 0.0  & 672.0  & 672.0  & \underline{3935.2} \\
                                      & CLUSTER    & 14.7 & 595.0  & \textbf{609.7}  & 4242.2 \\
                                      & GAS        & 17.4 & 776.0  & 793.4  & 5509.9 \\
                                      & TOP        & 21.2 & 596.0  & \underline{617.2}  & \textbf{3837.9} \\\midrule
\multirow{4}{*}{GCNII \& YELP}        & GraphSAINT & 0.0  & 1648.6 & \textbf{1648.6} & 6011.7 \\
                                      & CLUSTER    & 12.6 & 1871.6 & \underline{1884.2} & \underline{5940.4} \\
                                      & GAS        & 17.3 & 2145.0 & 2162.3 & 6752.9 \\
                                      & TOP        & 25.3 & 1990.0 & 2015.3 & \textbf{2231.8} \\\midrule
\multirow{3}{*}{GCNII+JK \& products} & CLUSTER    & 35.5 & 3964.4 & \underline{3999.9} & \underline{2048.7} \\
                                      & GAS        & 45.5 & 7068.0 & 7113.5 & 6574.4 \\
                                      & TOP        & 35.8 & 604.0  & \textbf{639.8}  & \textbf{1406.6}  \\\midrule
\multirow{2}{*}{GCNII+JK \& papers} & CLUSTER    & 0.00 & 1007.42 & {\textbf{1007.42}} & {\textbf{1526.91}} \\
                                      & TOP        & 144.53 & 1094.01  & {\underline{1238.54}} & {\underline{1560.34}}  \\
    \bottomrule
\end{tabular}
    }
\end{table}

\subsection{Prediction Performance on Various Graphs} \label{sec:prediction}



% \subsubsection{Experimental Settings}

% Under a single GeForce RTX 2080 Ti (11 GB), the prediction performance

\udfsection{Datasets.} 
% The prediction performance of many graph sampling methods is close to the full-batch gradient descents on the small datasets.
We report the prediction performance of TOP on four datasets, i.e., Flickr \citep{graphsaint}, Ogbn-arxiv, Ogbn-products and Ogbn-papers \citep{ogb}, where the two challenging large datasets (i.e., Ogbn-products and Ogbn-papers \citep{ogb}) contain at least 100 thousand nodes and one million edges.
% Moreover, OGB provides an official leaderboard\footnote{\url{https://ogb.stanford.edu/docs/leader_nodeprop/}} for a fair comparison of different methods.
As shown by Table \ref{tab:memory_consumption}, as the batch size is significantly lower than the size of the whole graph, the convergence of mini-batch methods under small batch sizes becomes very important.

% These datasets contain at least 100 thousand nodes and one million edges.
% Notably, Ogbn-papers100M is very large, containing 100 million nodes and 1.6 billion edges.


\udfsection{Baselines and implementation details.} Our baselines are from the OGB leaderboards \citep{ogb}, including node-wise sampling methods (GraphSAGE \citep{graphsage}, subgraph-wise sampling methods (CLUSTER-GCN in the original paper \citep{cluster_gcn}, GraphSAINT \citep{graphsaint}, SHADOW \citep{shadow_gnn} and GAS \citep{gas}), precomputing methods (SGC \citep{sgc} and SIGN \citep{sign}).
The GNN backbones of these baselines are different, as more scalable methods usually use more advanced but more memory-consuming GNN backbones.
Due to the differences in GNN backbones, frameworks, weight initialization, and optimizers in the baselines, we report CLUSTER-GCNII and GAS-GCNII for ablation studies. %  implement TOP based on the codes of GAS
The hyperparameter settings are the same as Section \ref{sec:convergence_curve}.
The results of the baselines are taken from the referred papers and the OGB leaderboards.


% \begin{table}
%     \centering
%       \caption{%
%       \textbf{Accuracy on large graph datasets.} Bold font indicates the best result and underline indicates the second best result. % CLUSTER$\uparrow$ denotes the improvements of TOP-GCNII+JK over CLUSTER-GCNII+JK. 
%       % $^{\ast}$ indicates a statistically significant difference against the second-best result from the two-sample one-tailed t-test.
%         }\label{tab:largegraph}
%     \begin{tabular}{lcc}
%     \toprule
%     {\textbf{\#\,nodes}} &\mc{1}{c}{{2.4M}} & \mc{1}{c}{{111M}}  \\[-0.1cm]
%     {\textbf{\#\,edges}} & \mc{1}{c}{{61.9M}} & \mc{1}{c}{{1.6B}} \\[-0.05cm]
%     \mr{2}{\textbf{Method}} & \mc{1}{c}{Ogbn-products}    & \mc{1}{c}{Ogbn-papers} \\
%     & test$\uparrow$ &  test$\uparrow$ \\
%     \midrule
%     {NS-SAGE}     & 78.70           & 67.06          \\
%     {CLUSTER-GCN}        & 78.97           & ---            \\
%     {GraphSAINT}       & 79.08                & ---            \\
%     {SHADOW-GAT}      & \underline{80.71}            & \underline{67.08}            \\
%     {SGC}              & ---              & 63.29            \\
%     {SIGN}             & 80.52          & 66.06            \\
%     % \mc{2}{l}{{\textsc{DROPEDGE}}}        & { \textbf{97.02}  }         & ---          & ---          & ---           \\
%      {GAS-PNA}               & 79.91 & ---\\
%      {FM-PNA}               & 80.47 & ---\\
%     \midrule
%      {NS-GCNII}               & OOM & OOM\\
%      {CLUSTER-GCNII}               & 79.62 & 51.73\\
%      {GAS-GCNII}               & 79.99 & OOM \\
%     \midrule
%      % {TOP-GCNII+JK}   & \underline{92.74} & \textbf{82.21} & \textbf{71.04} & \textbf{67.32}\\
%      % \mr{2}{TOP-GCNII+JK}   & \underline{92.34} & \textbf{82.21} & \textbf{71.04} & \textbf{67.21}\\
%      % & $\pm$0.40 & $\pm$0.24 & $\pm$0.03 & $\pm$0.12 \\
%     % \midrule
%     %  % CLUSTER$\uparrow$   & 4.01 & 2.58 & 16.56 & 15.59\\
%     %  Improvement$\uparrow$   & -0.25 & 1.50 & 0.31 & 0.24\\
%     {TOP-GCNII}   & \textbf{81.96} {\tiny $\pm$ 0.24} & \textbf{67.21} {\tiny $\pm$ 0.12}\\
%     \bottomrule
%   \end{tabular}
% \end{table}

\begin{table}
    \centering
      \caption{%
      \textbf{Prediction Performance.} Bold font indicates the best result and underline indicates the second best result. % CLUSTER$\uparrow$ denotes the improvements of TOP-GCNII+JK over CLUSTER-GCNII+JK. 
      % $^{\ast}$ indicates a statistically significant difference against the second-best result from the two-sample one-tailed t-test.
        }\label{tab:largegraph}
    \begin{tabular}{lcccc}
    \toprule
    {\textbf{\#\,nodes}} &\mc{1}{c}{{169K}} &\mc{1}{c}{{89K}} &\mc{1}{c}{{2.4M}} & \mc{1}{c}{{111M}}  \\[-0.1cm]
    {\textbf{\#\,edges}} & \mc{1}{c}{{1.2M}} & \mc{1}{c}{{450K}} & \mc{1}{c}{{61.9M}} & \mc{1}{c}{{1.6B}} \\[-0.05cm]
    \mr{2}{\textbf{Method}} & \mc{1}{c}{Ogbn-arxiv} & \mc{1}{c}{Flickr} & \mc{1}{c}{Ogbn-products}    & \mc{1}{c}{Ogbn-papers} \\
    & acc$\uparrow$ &  acc$\uparrow$   & acc$\uparrow$ &  acc$\uparrow$\\
    \midrule
    {NS-SAGE}    & 71.49    & 50.10     & 78.70           & 67.06          \\
    {CLUSTER-GCN}     & ---     & 48.10      & 78.97           & ---            \\
    {GraphSAINT}     & ---     & 51.10     & 79.08                & ---            \\
    {SHADOW-GAT}     & \underline{72.74}    & 53.52    & \underline{80.71}            & \underline{67.08}            \\
    {SGC}    & ---    & 48.20      & ---              & 63.29            \\
    {SIGN}   & ---    & 51.40      & 80.52          & 66.06            \\
    % \mc{2}{l}{{\textsc{DROPEDGE}}}        & { \textbf{97.02}  }         & ---          & ---          & ---           \\
     % {GAS-PNA}               & 79.91 & ---\\
     % {FM-PNA}               & 80.47 & ---\\
    \midrule
     {GD-GCNII}    & \textbf{72.83}    & 55.28    & OOM    & OOM\\
     % {NS-GCNII}    & ---     & ---      & OOM & OOM\\
     {CLUSTER-GCNII}      & 72.39    & \underline{55.33}         & 79.62 & 51.73\\
     {GAS-GCNII}     & 72.50     & \textbf{55.42}             & 79.99 & OOM \\
    \midrule
     % {TOP-GCNII+JK}   & \underline{92.74} & \textbf{82.21} & \textbf{71.04} & \textbf{67.32}\\
     % \mr{2}{TOP-GCNII+JK}   & \underline{92.34} & \textbf{82.21} & \textbf{71.04} & \textbf{67.21}\\
     % & $\pm$0.40 & $\pm$0.24 & $\pm$0.03 & $\pm$0.12 \\
    % \midrule
    %  % CLUSTER$\uparrow$   & 4.01 & 2.58 & 16.56 & 15.59\\
    %  Improvement$\uparrow$   & -0.25 & 1.50 & 0.31 & 0.24\\
    {TOP-GCNII}   & 72.52 {\tiny $\pm$ 0.34}    & 55.21 {\tiny $\pm$ 0.46}    & \textbf{81.96} {\tiny $\pm$ 0.24} & \textbf{67.21} {\tiny $\pm$ 0.12}\\
    \bottomrule
  \end{tabular}
\end{table}

\udfsection{Prediction performance.} We report the prediction performance of TOP in Table \ref{tab:largegraph}. 
On the small datasets (i.e., the ogbn-arxiv and flickr datasets), which have fewer than 170k nodes, the prediction performance of several subgraph sampling methods (CLUSTER, GAS, and TOP) is comparable to gradient descent (GD), as they can use a large subgraph ratio \gongshi{$\mathcal{|B|}/\mathcal{|V|}$} on the small dataset (e.g. 50\% used in GAS), such that the sampled subgraphs are close to the whole graph.
On the large datasets (i.e., the ogbn-products and ogbn-papers datasets),  as the large subgraph ratio may suffer from the out-of-GPU memory issue, we use a small subgraph ratio (less than 4\%). However, the small subgraph ratio increases the ratio of missing messages in {$\text{MP}_{\text{OB}}$} for CLUSTER and results in the severe staleness issue for GAS (the update of historical embeddings in GAS is infrequent due to very low node sampling probability). The accuracy of TOP is larger than other baselines, as it compensates the messages in {$\text{MP}_{\text{OB}}$} by the message-invarant trasformation \gongshi{$g$} and relies solely on up-to-date embeddings, thus avoiding the staleness issue of the historical embeddings.

% TOP-GCNII significantly outperforms the state-of-the-art performance by 1.25\% and 0.13\% accuracy on Ogbn-products and Ogbn-papers respectively.
% Compared with subgraph sampling methods with the same GCNII+JK backbone (i.e., CLUSTER-GCNII and GAS-GCNII), TOP-GCNII improves accuracy by a large margin, i.e., 1.97\% and 15.48\% accuracy improvement on Ogbn-products and Ogbn-papers respectively.



\subsection{Experiments on Heterophilous Graphs}\label{sec:exp_heterophilous}


The message invariance still holds on heterophilous graphs. To verify our claim, we conduct experiments on five heterophilous graphs (i.e., roman-empire, amazon-ratings, minesweeper, tolokers, and questions) provided by the recent heterophilous benchmark \citep{heterophily}, as shown in Table \ref{table_G2}. We set the subgraph ratio to be 50\%, as the heterophilous graphs (10k-50k nodes) are significantly smaller than the homophilic graphs (200k-112000k nodes) in Section \ref{sec:exp}.  On the heterophilous graphs, although a node may be very different from its neighbors, the neighbors may be similar to other nodes in the subgraph. Notably, the message-invariant transformation in Equation \ref{eqn:nonlinear_extrapolation} does not restrict that the embedding of an in-batch node should be similar to its neighborhood embeddings, and thus the message-invariant transformation is able to approximate the neighborhood embeddings by other nodes in the subgraph.



\begin{table}[h]
  \centering
  \caption{\textbf{Approximation errors of TOP, CLUSTER, and TOP on heterophilous graphs.}}
  \label{table_G2}
  \resizebox{0.7\textwidth}{!}{%
  \renewcommand{\arraystretch}{0.5}
    \begin{tabular}{c|rrr}
    \toprule
       \textbf{Heterophilous Graph} & CLUSTER & GAS & TOP  \\ 
    \midrule
        amazon-ratings & 4.14\% & 1.36\% & \textbf{1.02\%}  \\ 
        minesweeper & 54.53\% & 19.68\% & \textbf{3.12\%}  \\ 
        questions & 20.25\% & 9.54\% & \textbf{4.90\%}  \\ 
        roman-empire & 5.42\% & 1.65\% & \textbf{0.88\%}  \\ 
    \bottomrule
    \end{tabular}%
  }
\end{table}


Figure \ref{figure_G1} further reports the convergence curves of TOP, CLUSTER, and GAS on the homophily datasets. From Table \ref{table_G2}, the approximation errors of TOP are significantly lower than CLUSTER and GAS on minesweeper, tolokers, and questions. Accordingly, TOP is also significantly faster than CLUSTER and GAS on the three datasets.



\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figure/neurips_rebuttal/accuracy_runtime_gcn_heterophily.pdf}
    \caption{\textbf{Convergence curves of TOP, CLUSTER, and GAS on real-world heterophilous graphs.}}
    \label{figure_G1}
    % \vskip -0.1in
\end{figure*}


\subsection{Experiments under Various Subgraph Samplers}\label{sec:samplers}

We conduct experiments to demonstrate that TOP consistently brings performance improvement for various subgraph samplers. We first evaluate the relative approximation errors of TOP, CLUSTER, and GAS under METIS, Random, GraphSAINT \citep{graphsaint}, and SHADOW \citep{shadow_gnn} sampling in Table \ref{Table_G3}. The results demonstrate that TOP significantly alleviates approximation errors by integrating different subgraph sampling techniques. Specifically, different subgraph sampling techniques are designed to encourage the connections between the sampled nodes with a trade-off for efficiency. METIS aims to directly achieve this goal, while it may be more time-consuming than other sampling techniques. Random sampling is the fastest sampling baseline among them, while it does not consider the connections between the sampled nodes. Thus, Random sampling significantly amplifies the approximation errors of CLUSTER and GAS, while TOP is robust under Random, GraphSAINT, and SHADOW sampling.


\begin{table}[h]
  \centering
  \caption{%
    \textbf{Message invariance in real-world datasets with various subgraph samplers.}
  }
    \label{Table_G3}
  \setlength{\tabcolsep}{2pt}
  \resizebox{0.7\linewidth}{!}{%
    \begin{tabular}{c|c|c|cccc}
    \toprule
    % \\
 \mr{2}{\textbf{Dataset}}    &   \mr{2}{\textbf{GNN}}      & \mr{2}{\textbf{Methods}}      & 
 \multicolumn{4}{c}{{\textbf{Relative Approximation Errors} $\downarrow$}}
  \\
     &                    &       & Random  & SAINT  & SHADOW  & METIS   \\
    \midrule
\multirow{3}{*}{Ogbn-arxiv}   & \multirow{3}{*}{GCN}   & CLUSTER & 30.02\%  & 15.79\% & 12.49\% & 12.10\%   \\
                          &                        & GAS     & 45.29\%  & --- & --- & 9.89\%    \\
                          &                        & TOP     & \textbf{7.61}\% & \textbf{5.94}\% & \textbf{3.41}\%   & \textbf{1.58}\%   \\ \midrule
\multirow{3}{*}{Reddit}  & \multirow{3}{*}{SAGE}   & CLUSTER  & 25.91\% & 22.22\% & 21.32\% & 3.44\%   \\
                          &                        & GAS      & 13.91\% & --- & --- & 1.90\%  \\
                          &                        & TOP      & \textbf{3.10}\%  & \textbf{1.45}\% & \textbf{1.27}\% & \textbf{0.61}\%   \\\midrule
\multirow{3}{*}{Yelp}     & \multirow{3}{*}{GCNII} & CLUSTER  & 4.87\% & 1.13\% & 0.91\%  & 2.38\%  \\
                          &                        & GAS      & 7.86\%  & --- & --- & 4.01\%  \\
                          &                        & TOP       & \textbf{2.68}\% & \textbf{0.74}\%  & \textbf{0.64}\%  & \textbf{0.86}\% \\
    \bottomrule
  \end{tabular}
  }
\end{table}




Figure \ref{figure_G4} further shows the convergence curves of TOP, CLUSTER, and GAS under Random sampling. Due to the accurate and fast message passing of TOP, TOP significantly outperforms CLUSTER and GAS in terms of accuracy and converge speeds. By integrating the results with Figures \ref{subfig:small_datasets} and \ref{subfig:medium_datasets}, the performance improvement of TOP is consistent for various subgraph sampling methods.


% Figure G1
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Figure/neurips_rebuttal/accuracy_runtime_gcn_random.pdf}
    \caption{\textbf{Convergence curves of TOP, CLUSTER, and GAS under the Random sampler.}}
    \label{figure_G4}
    % \vskip -0.1in
\end{figure}



\section{Proof for Convergence} \label{sec:proof_convergence}


We first show that TOP based on Equation \eqref{eqn:top} provides unbiased gradients. We assume that subgraph \gongshi{$\mathcal{B}$} is uniformly sampled from \gongshi{$\mathcal{V}$}.  When the sampling is not uniform, we use the normalization technique \citep{graphsaint} to enforce the assumption. 
\begin{theorem}\label{thm:unbiased}
        Suppose that the message invariant transformations \eqref{eqn:nonlinear_extrapolation} exist and the subgraph \gongshi{ $\mathcal{B}$} is uniformly sampled from \gongshi{ $\mathcal{V}$}. The iterative message passing of Equations \eqref{eqn:top} and \gongshi{$\mathbf{H}_{\mathcal{B}}^{(l+1)} = \sigma(\mathbf{Z}^{(l+1)}_{\mathcal{B}}\mathbf{W}^{(l)}) $} leads to unbiased mini-batch gradients \gongshi{$\mathbf{d}_{\mathcal{W}}$} such that
        \begin{align*}
            \mathbb{E}[\mathbf{d}_{\mathcal{W}}]
            &=
            \nabla_{\mathcal{W}} \mathcal{L}.
        \end{align*}
\end{theorem}

% \subsection{Proof of Theorem \ref{thm:unbiased}: Unbiased Mini-batch Gradients of TOP}

% In this section, we give the proof of Theorem \ref{thm:unbiased} to show that the mini-batch gradients computed by TOP are unbiased.
% % \begin{theorem}\label{D:thm:unbiased}
% %     Suppose that a subgraph \gongshi{ $\mathcal{B}$} is uniformly sampled from \gongshi{ $\mathcal{V}$}. The iterative message passing of Equations \eqref{eqn:top} and $\mathbf{H}_{\mathcal{B}}^{(l+1)} = f(\sum_s \mathbf{Z}^{(l,s)}_{\mathcal{B}} \mathbf{W}^{(l,s)}) $ leads to unbiased mini-batch gradients $\mathbf{d}_{\mathbf{W}} \triangleq \nabla_{\mathbf{W}} \sum_{i \in \mathcal{B}} \ell (\mathbf{H}^{(L)}_{i}, y_i) / |\mathcal{B}|$ such that
% %     \begin{align*}
% %         \mathbb{E}[\mathbf{d}_{\mathbf{W}}]
% %         &=
% %         \nabla_{\mathbf{W}} \mathcal{L}
% %     \end{align*}
% %     where $\mathbf{W} = (\mathbf{W}^{(l,s)})_{l,s}$ is the collection of the GNN parameters, $\mathcal{L}$ is the full-batch gradients, $\ell$ is the loss function and $y_i$ is the label of node $i$.
% % \end{theorem}

% % \vspace{-10mm}

\begin{proof}
    % As $\mathcal{B}$ uniformly sampled from $\mathcal{V}$, $\mathbb{E}[\mathbf{d}_{\mathbf{W}}]  = \nabla_{\mathbf{W}} \mathcal{L}$ holds.
    % \begin{align*}
    %     \nabla_{\mathbf{W}} g^{(l)}(\mathbf{H}^{(l)}_{\mathcal{B}}) &= 0\\
    %     \Rightarrow 
    %     \nabla_{\mathbf{W}} \mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}& = \nabla_{\mathbf{W}} g^{(l)}(\mathbf{H}^{(l)}_{\mathcal{B}}) +  \nabla_{\mathbf{W}} \mathbf{H}^{(l)}_{\mathcal{B}}  \circ\nabla_{\mathbf{H}^{(l)}_{\mathcal{B}}}  g^{(l)}(\mathbf{H}^{(l)}_{\mathcal{B}})\\
    %     &= \nabla_{\mathbf{W}} \mathbf{H}^{(l)}_{\mathcal{B}} \circ \nabla_{\mathbf{H}^{(l)}_{\mathcal{B}}} g^{(l)}(\mathbf{H}^{(l)}_{\mathcal{B}})
    % \end{align*}
    % where $\mathbf{W}$ are the parameters of GNN.

    % The gradients of SGD are given by the gradients of Equation \ref{eqn:mini_batch_gas}
    % \begin{align*}
    %     \nabla_{\mathbf{W}} \mathbf{Z}^{(l,s)}_{\mathcal{B}} &= \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{B}}  \circ \nabla_{\mathbf{W}} \mathbf{H}^{(l)}_{\mathcal{B}} + \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{N}({\mathcal{B}})-\mathcal{B}} \circ  \nabla_{\mathbf{W}} \mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}}  \\
    %     &= \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{B}} \circ  \nabla_{\mathbf{W}} \mathbf{H}^{(l)}_{\mathcal{B}} + \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{N}({\mathcal{B}})-\mathcal{B}} \circ \nabla_{\mathbf{W}} \mathbf{H}^{(l)}_{\mathcal{B}} \circ \nabla_{\mathbf{H}^{(l)}_{\mathcal{B}}} g^{(l)}(\mathbf{H}^{(l)}_{\mathcal{B}})
    % \end{align*}

    % % \mathbf{Z}^{(l,s)}_{\mathcal{B}} &= \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{N}({\mathcal{B}})}\mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})} \\ \label{eqn:mini_batch}
    % % &= \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{B}}\mathbf{H}^{(l)}_{\mathcal{B}} + \mathbf{C}^{(l,s)}_{\mathcal{B},\mathcal{N}({\mathcal{B}})-\mathcal{B}}\mathbf{H}^{(l)}_{\mathcal{N}({\mathcal{B}})-\mathcal{B}} \\  \label{eqn:mini_batch_gas}
    % the expectation of the mini-batch gradients is
    % \begin{align*}
    %     \mathbb{E}[\mathbf{d}_{\mathbf{W}}]
    %     &=
    %     \mathbb{E}[\nabla_{\mathbf{W}} \frac{\sum_{i \in \mathcal{B}}\ell(\mathbf{H}^{(L)}_{i}, y_i)}{|\mathcal{B}|}]\\
    %     &=
    %     \mathbb{E}_{\mathcal{B}}[\nabla_{\mathbf{W}} \frac{\sum_{i \in \mathcal{B}}\ell(\mathbf{H}^{(L)}_{i}, y_i)}{|\mathcal{B}|}]\\
    %     &=
    %     \mathbb{E}[\nabla_{\mathbf{W}}\ \ell(\mathbf{H}^{(L)}_{i}, y_i)]\\
    %     &=
    %     \nabla_{\mathbf{W}}\frac{\sum_{i \in \mathcal{V}}\ell(\mathbf{H}^{(L)}_{i}, y_i)}{|\mathcal{V}|}\\
    %     &=
    %     \nabla_{\mathbf{W}} \mathcal{L}
    % \end{align*}
    Given any mini-batch \gongshi{$\mathcal{B}$}, the embeddings \gongshi{$\mathbf{H}^{(l)}_{\mathcal{B}}$} of TOP are the same as that of SGD due to \gongshi{$\mathbf{H}^{(l)}_{\mathcal{N}_{\mathcal{B}}^{c}} = g( \mathbf{H}^{(l)}_{\mathcal{B}} )$} for any GNN parameters \gongshi{$\mathbf{W}^{(l)}$}.
    Thus, their total objective functions of \gongshi{$\mathcal{L} = \mathcal{L}_{TOP}$} are the same.

    
    If TOP is biased, then the expected gradient \gongshi{$\nabla_{\mathcal{W}} \mathcal{L}_{TOP} \neq \nabla_{\mathcal{W}} \mathcal{L}$}.
    Thus, there exists $\epsilon>0$ and \gongshi{$\mathcal{W}_0$} such that \gongshi{$\mathcal{L}_{TOP}(\mathcal{W}_0) = \mathcal{L}(\mathcal{W}_0)$} while \gongshi{$\mathcal{L}_{TOP}(\mathcal{W}_0 - \epsilon \nabla_{\mathcal{W}_{0}} \mathcal{L}) \neq \mathcal{L}(\mathcal{W}_{0} - \epsilon \nabla_{\mathcal{W}_{0}} \mathcal{L})$} due to different directional derivatives \gongshi{$\langle \nabla_{\mathcal{W}_{0}} \mathcal{L}, \nabla_{\mathcal{W}_{0}} \mathcal{L}_{TOP} \rangle \neq \| \nabla_{\mathcal{W}_{0}} \mathcal{L} \|^2_F$}, which contradicts to \gongshi{$\mathcal{L} = \mathcal{L}_{TOP}$}. The unbiasedness holds immediately.
\end{proof}

\subsection{Proof of Theorem \ref{thm:convergence_conv}: Convergence Guarantees of TOP}
In this subsection, we give the convergence guarantees of TOP.
The proof follows the proof of Theorem 2 in Appendix C.4 of \citep{vrgcn}.
% \begin{assumption}\label{D:assumption:2}
%     Assume that (1) the optimal value $\mathcal{L}^{*} = \mathrm{inf}\ \mathcal{L}(\mathbf{W})$ is finite (2) at the $k$-th iteration, a batch of nodes $\mathcal{V}^{k}_{\mathcal{B}}$ is uniformly sampled from $\mathcal{V}$ (3) function $\nabla_{\mathbf{W}} \mathcal{L}$ is $\gamma$-Lipschitz with $\gamma > 1$ (4) norms $\|\nabla_{\mathbf{W}} \mathcal{L}\|_2$ and $\|\mathbf{d}_{\mathbf{W}}\|_2$ are bounded by $G > 1$ 
% \end{assumption}

% \begin{theorem}\label{D:thm:convergence_conv}
%     Suppose that Assumption \ref{D:assumption:2} holds. Then, with \gongshi{$\eta=\mathcal{O}(\varepsilon^{2})$} and \gongshi{$N=\mathcal{O}(\varepsilon^{-4})$}, TOP ensures to find an \gongshi{$\varepsilon$}-stationary solution such that $\mathbb{E}[\|\nabla_{\mathbf{W}} \mathcal{L}(\mathbf{W}^{R}) \|_2] \leq \varepsilon$ after running for $N$ iterations, where $R$ is uniformly selected from $[N]$ and $\mathbf{W}^{R}$ is the value of the GNN parameters at the $R$-th iteration.
% \end{theorem}


\begin{proof}
     % Let's abbreviate $\nabla_{\mathbf{W}} \mathcal{L}$ as $\nabla \mathcal{L}$ at first for some simplification.\\
     As \gongshi{$\nabla \mathcal{L}$} is \gongshi{$\gamma$}-Lipschitz, we have
    \begin{align*}
        &\ \ \ \ \mathcal{L}(\mathcal{W}^{(k+1)})\\
        &=
        \mathcal{L}(\mathcal{W}^{(k)}) + \int_{0}^{1} \langle \nabla \mathcal{L}(\mathcal{W}^{(k)}+t(\mathcal{W}^{(k+1)}-\mathcal{W}^{(k)})) , \mathcal{W}^{(k+1)}-\mathcal{W}^{(k)} \rangle \ dt\\
        &=
        \mathcal{L}(\mathcal{W}^{(k)}) + \langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \mathcal{W}^{(k+1)}-\mathcal{W}^{(k)} \rangle\\ 
        & 
        \ \ \ + \int_{0}^{1} \langle \nabla \mathcal{L}(\mathcal{W}^{(k)}+t(\mathcal{W}^{(k+1)}-\mathcal{W}^{(k)})) - \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \mathcal{W}^{(k+1)}-\mathcal{W}^{(k)} \rangle \ dt\\
        & \leq
        \mathcal{L}(\mathcal{W}^{(k)}) + \langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \mathcal{W}^{(k+1)}-\mathcal{W}^{(k)} \rangle\\
        &
        \ \ \ + \int_{0}^{1} \| \nabla \mathcal{L}(\mathcal{W}^{(k)}+t(\mathcal{W}^{(k+1)}-\mathcal{W}^{(k)})) - \nabla \mathcal{L}(\mathcal{W}^{(k)}) \|_{2}\| \mathcal{W}^{(k+1)}-\mathcal{W}^{(k)} \|_{2} \ dt\\
        & \leq
        \mathcal{L}(\mathcal{W}^{(k)}) + \langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \mathcal{W}^{(k+1)}-\mathcal{W}^{(k)} \rangle + \int_{0}^{1} \gamma t \| \mathcal{W}^{(k+1)}-\mathcal{W}^{(k)} \|^{2}_{2} \ dt\\
        &=
        \mathcal{L}(\mathcal{W}^{(k)}) + \langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \mathcal{W}^{(k+1)}-\mathcal{W}^{(k)} \rangle +  \frac{\gamma}{2} \| \mathcal{W}^{(k+1)}-\mathcal{W}^{(k)} \|^{2}_{2}.
    \end{align*}
    Notice that the update formula of \gongshi{$\mathcal{W}^{(k)}$} is
    \begin{align*}
        \mathcal{W}^{(k+1)} = \mathcal{W}^{(k)} - \eta \mathbf{d}_{\mathcal{W}}^{(k)},
    \end{align*}
    {where \gongshi{$\mathbf{d}_{\mathcal{W}}^{(k)}$} is the gradient of TOP at the \gongshi{$k$}-th iteration and we select \gongshi{$\eta < \frac{2}{\gamma}$}}.
    Let \gongshi{$\Delta^{(k)} \triangleq \mathbf{d}_{\mathcal{W}}^{(k)}-\nabla\mathcal{L}(\mathcal{W}^{(k)})$}, then
    \begin{align*}
        &\ \ \ \ \mathcal{L}(\mathcal{W}^{(k+1)})\\
        & \leq
        \mathcal{L}(\mathcal{W}^{(k)}) + \langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \mathcal{W}^{(k+1)}-\mathcal{W}^{(k)} \rangle +  \frac{\gamma}{2} \| \mathcal{W}^{(k+1)}-\mathcal{W}^{(k)} \|^{2}_{2}\\
        &=
        \mathcal{L}(\mathcal{W}^{(k)}) - \eta\langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \mathbf{d}_{\mathcal{W}}^{(k)} \rangle + \frac{\eta^{2}\gamma}{2} \| \mathbf{d}^{(k)}_{\mathcal{W}} \|^{2}_{2}\\
        &=
        \mathcal{L}(\mathcal{W}^{(k)}) - \eta(1-\eta\gamma) \langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \Delta^{(k)} \rangle - \eta(1-\frac{\eta\gamma}{2}) \| \nabla\mathcal{L}(\mathcal{W}^{(k)}) \|_{2}^{2} + \frac{\eta^{2}\gamma}{2} \| \Delta^{(k)} \|_{2}^{2}.
    \end{align*}
    % Notice that
    % \begin{align*}
    %     \mathbb{E}[\langle \nabla \mathcal{L}(\mathbf{W}^{k}) , \Delta^{k} \rangle] = \mathbb{E}[\mathbb{E}[\langle \nabla \mathcal{L}(\mathbf{W}^{k}) , \Delta^{k} \rangle | \nabla \mathcal{L}(\mathbf{W}^{k})]] = 0
    % \end{align*}
    By taking the expectations of both sides, we have
    \begin{align*}
        & \mathbb{E}[\mathcal{L}(\mathcal{W}^{(k+1)})]\\
        & \leq
        \mathbb{E}[\mathcal{L}(\mathcal{W}^{(k)})] - \eta(1-\eta \gamma)\mathbb{E}[\langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \Delta^{(k)} \rangle] - \eta(1-\frac{\eta\gamma}{2}) \mathbb{E}[\| \nabla\mathcal{L}(\mathcal{W}^{(k)}) \|_{2}^{2}] + \frac{\eta^{2}\gamma}{2} \mathbb{E}[\| \Delta^{(k)} \|_{2}^{2}].
    \end{align*}
    {By the properties of the expectations and Theorem \ref{thm:unbiased}, we have
    \begin{align*}
        \mathbb{E}[\langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \Delta^{(k)} \rangle]
        & = \mathbb{E}[\mathbb{E}[\langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \Delta^{(k)} \rangle | \nabla \mathcal{L}(\mathcal{W}^{(k)})]]
        \\
        & = \mathbb{E}[\langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \mathbb{E}[\Delta^{(k)} | \nabla \mathcal{L}(\mathcal{W}^{(k)})] \rangle ] 
        \\
        & = \mathbb{E}[\langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \mathbb{E}[\mathbf{d}_{\mathcal{W}}^{(k)}-\nabla\mathcal{L}(\mathcal{W}^{(k)}) | \nabla \mathcal{L}(\mathcal{W}^{(k)})] \rangle ]
        \\
        & = \mathbb{E}[\langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \mathbb{E}[\mathbf{d}_{\mathcal{W}}^{(k)} | \nabla \mathcal{L}(\mathcal{W}^{(k)})]-\nabla\mathcal{L}(\mathcal{W}^{(k)})  \rangle ]
        \\
        & = \mathbb{E}[\langle \nabla \mathcal{L}(\mathcal{W}^{(k)}) , \nabla \mathcal{L}(\mathcal{W}^{(k)}) -\nabla\mathcal{L}(\mathcal{W}^{(k)})  \rangle ]
        \\
        & = 0,
    \end{align*}
    }
    which leads to
    \begin{align*}
        &\mathbb{E}[\mathcal{L}(\mathcal{W}^{(k+1)})] \leq
        \mathbb{E}[\mathcal{L}(\mathcal{W}^{(k)})] - \eta(1-\frac{\eta\gamma}{2}) \mathbb{E}[\| \nabla\mathcal{L}(\mathcal{W}^{(k)}) \|_{2}^{2}] + \frac{\eta^{2}\gamma}{2} \mathbb{E}[\| \Delta^{(k)} \|_{2}^{2}]\\
        \Rightarrow &{\eta(1-\frac{\eta\gamma}{2}) \mathbb{E}[\| \nabla\mathcal{L}(\mathcal{W}^{(k)}) \|_{2}^{2}] \leq \mathbb{E}[\mathcal{L}(\mathcal{W}^{(k)})] - \mathbb{E}[\mathcal{L}(\mathcal{W}^{(k+1)})] + \frac{\eta^{2}\gamma}{2} \mathbb{E}[\| \Delta^{(k)} \|_{2}^{2}]}.
    \end{align*}
    
    By summing up the above inequalities for \gongshi{$k \in \llbracket N \rrbracket$} and dividing both sides by \gongshi{$N\eta(1-\frac{\eta\gamma}{2})$}, we have
    {\begin{align*}
        \frac{\sum_{k=1}^{N} \mathbb{E}[\| \nabla\mathcal{L}(\mathcal{W}^{(k)}) \|_{2}^{2}]}{N}
        & \leq
        \frac{\mathcal{L}(\mathcal{W}^{(1)})-\mathbb{E}[\mathcal{L}(\mathcal{W}^{(N+1)})]}{N\eta(1-\frac{\eta\gamma}{2})} + \frac{\eta\gamma}{2-\eta\gamma}\frac{\sum_{k=1}^{N} \mathbb{E}[\| \Delta^{(k)} \|_{2}^{2}]}{N}\\
        & \leq
        \frac{\mathcal{L}(\mathcal{W}^{(1)})-\mathcal{L}^{*}}{N\eta(1-\frac{\eta\gamma}{2})} + \frac{\eta\gamma}{2-\eta\gamma}\frac{\sum_{k=1}^{N} \mathbb{E}[\| \Delta^{(k)} \|_{2}^{2}]}{N}.
    \end{align*}}
    By noticing that
    \begin{align*}
        \mathbb{E}[\|\nabla \mathcal{L}(\mathcal{W}^{(R)}) \|_2^{2}] = \mathbb{E}[\mathbb{E}[\|\nabla_{\mathcal{W}} \mathcal{L}(\mathcal{W}^{(R)}) \|_2^{2}\ |\ R]] = \frac{\sum_{k=1}^{N} \mathbb{E}[\| \nabla\mathcal{L}(\mathcal{W}^{(k)}) \|_{2}^{2}]}{N}
    \end{align*}
    and
    \begin{align*}
        \mathbb{E}[\| \Delta^{(k)} \|_{2}^{2}] 
        &= 
        \mathbb{E}[\| \mathbf{d}_{\mathcal{W}}^{(k)}-\nabla\mathcal{L}(\mathcal{W}^{(k)}) \|_{2}^{2}]\\
        & \leq
        2(\mathbb{E}[\| \mathbf{d}_{\mathcal{W}}^{(k)} \|_{2}^{2}] + \mathbb{E}[\| \nabla\mathcal{L}(\mathcal{W}^{(k)}) \|_{2}^{2}])\\
        & \leq
        4G^{2},
    \end{align*}
    we have
    \begin{align*}
        \mathbb{E}[\|\nabla \mathcal{L}(\mathcal{W}^{(R)}) \|_2^{2}] \leq \frac{\mathcal{L}(\mathcal{W}^{(1)})-\mathcal{L}^{*}}{N\eta(1-\frac{\eta\gamma}{2})} + \frac{4G^{2}\eta\gamma}{2-\eta\gamma}.
    \end{align*}
    If \gongshi{$\eta < \frac{1}{\gamma},\ \eta=\mathcal{O}(N^{-\frac{1}{2}})$}, we have
    \begin{align*}
        \mathbb{E}[\|\nabla \mathcal{L}(\mathcal{W}^{(R)}) \|_2^{2}] \leq \frac{2(\mathcal{L}(\mathcal{W}^{(1)})-\mathcal{L}^{*})}{N\eta} + 8G^{2}\eta\gamma = \mathcal{O}(N^{-\frac{1}{2}}).
    \end{align*}
    Therefore, by letting \gongshi{$\varepsilon = (\frac{2(\mathcal{L}(\mathcal{W}^{(1)})-\mathcal{L}^{*})}{N\eta} + 8G^{2}\eta\gamma)^{\frac{1}{2}} = \mathcal{O}(N^{-\frac{1}{4}})$}, Theorem \ref{thm:convergence_conv} follows immediately.
\end{proof}





\section{Encoding Symmetry in Graphs via TOP}\label{subsubsec:selection_and_isomorphic}







In this section, we show that the embeddings of GNNs at random initialization can encode the symmetry in the original graph, which is a specific node similarity.

\udfsection{Notations} For brevity, \gongshi{$\overline{\mathcal{N}}_{i} = \mathcal{N}_{i} \cup \{ i \}$} denotes the neighborhood of node \gongshi{$i$} with itself.
We recursively define the set of neighborhoods within  \gongshi{$k$}-hops as \gongshi{$\overline{\mathcal{N}}_{i}^{k} = \overline{\mathcal{N}}_{\overline{\mathcal{N}}_{i}^{k-1}}$} with \gongshi{$\overline{\mathcal{N}}_{i}^{1} = \overline{\mathcal{N}}_{i}$}.
For Theorem \ref{thm:high_approx_of_linear_extra}, we denote all the possible embeddings at the \gongshi{$l$}-th layer by \gongshi{$E^{(l)}=\{ \mathbf{h}_{1}^{(l)},\ \mathbf{h}_{2}^{(l)},\dots,\mathbf{h}_{t^{(l)}}^{(l)} \}$},  where \gongshi{$t^{(l)} \leq t$} is the number of different embeddings at the \gongshi{$l$}-th layer, \gongshi{$l \in \llbracket L \rrbracket$}.


\udfsection{Motivation for the symmetry.} We first motivate the basic embeddings from the graph isomorphism perspective. 
The 1-dimensional Weisfeiler-Lehman test (i.e., 1-WL test) \citep{wl_test} is widely used to distinguish whether two nodes or graphs are isomorphic.

Given initial node feature/representation \gongshi{$h_{u}^{(0)}$}, at the $l$-th iterations, 1-WL test for GCNs updates the node representation \gongshi{$h_{i}^{(l-1)}$} based on the local neighborhood by
\begin{align}\label{eqn:hash}
    h_{i}^{(l)} = Hash(\{\{ h_{u}^{(l-1)}, u\in \overline{\mathcal{N}}_{i} \}\}).
\end{align}

Following \citep{gin}, we show the connections between GNNs and 1-WL test in Lemma \ref{lemma:pro_of_isomor_nodes}.

Without loss of generality, we present the theories for the GCN version.
Extending them to other GNNs is easy.

\begin{lemma}\label{lemma:pro_of_isomor_nodes}
    Given a graph \gongshi{$\mathcal{G}=(\mathcal{V},\mathcal{E})$} and a GNN, if nodes \gongshi{$i,j \in \mathcal{V}$} are indistinguishable under \gongshi{$l$} iterations of the 1-WL test, then there holds
    \begin{align*}
        \mathbf{H}_{i}^{(l)} = \mathbf{H}_{j}^{(l)},
    \end{align*}
    for all GNN parameters.
\end{lemma}

\begin{proof} 
    As \gongshi{$i,j$} are indistinguishable under \gongshi{$l$} iterations of 1-WL test, we have \gongshi{$h_{i}^{(l)}=h_{j}^{(l)}$}. Notice that the function \gongshi{$Hash$} is injective, we have
    \begin{align*}
        \{\{ h_{u}^{(l-1)}, u \in \overline{\mathcal{N}}_{i} \}\}=\{\{ h_{v}^{(l-1)}, v \in \overline{\mathcal{N}}_{j} \}\}\ \mathrm{and}\ |\overline{\mathcal{N}}_{i}| = |\overline{\mathcal{N}}_{j}|.
    \end{align*}
    Then, we can know that
    \begin{align*}
        \{\{ h_{p}^{(l-2)}, p \in \overline{\mathcal{N}}_{u}, u \in \overline{\mathcal{N}}_{i}\}\} 
        & = \{\{ h_{q}^{(l-2)}, q \in \overline{\mathcal{N}}_{v}, v \in \overline{\mathcal{N}}_{j} \}\}
        \\
        \mathrm{and}\ \{\{|\overline{\mathcal{N}}_{u}|, u \in \overline{\mathcal{N}}_{i}\}\} 
        & = \{\{|\overline{\mathcal{N}}_{v}|, v \in \overline{\mathcal{N}}_{j}\}\}.
    \end{align*}
    which is equivalent to
    \begin{align*}
        \{\{ h_{p}^{(l-2)}, p \in \overline{\mathcal{N}}_{i}^{2}\}\}=\{\{ h_{q}^{(l-2)}, q \in \overline{\mathcal{N}}_{j}^{2} \}\}\ \mathrm{and}\ \{\{|\overline{\mathcal{N}}_{u}|, u \in \overline{\mathcal{N}}_{i}\}\} 
        & = \{\{|\overline{\mathcal{N}}_{v}|, v \in \overline{\mathcal{N}}_{j}\}\}.
    \end{align*}
    Recursively, we have
    \begin{gather*}
        \{\{ h_{p}^{(l-3)}, p \in \overline{\mathcal{N}}_{i}^{3}\}\}=\{\{ h_{q}^{(l-3)}, q \in \overline{\mathcal{N}}_{j}^{3} \}\}\ \mathrm{and}\ \{\{|\overline{\mathcal{N}}_{u}|, u \in \overline{\mathcal{N}}_{i}^{2}\}\}
        = \{\{|\overline{\mathcal{N}}_{v}|, v \in \overline{\mathcal{N}}_{j}^{2}\}\} \\
        \vdots\\
        \{\{ h_{p}^{(0)}, p \in \overline{\mathcal{N}}_{i}^{l}\}\}=\{\{ h_{q}^{(0)}, q \in \overline{\mathcal{N}}_{j}^{l} \}\}\ \mathrm{and}\ \{\{|\overline{\mathcal{N}}_{u}|, u \in \overline{\mathcal{N}}_{i}^{l-1}\}\}
        = \{\{|\overline{\mathcal{N}}_{v}|, v \in \overline{\mathcal{N}}_{j}^{l-1}\}\}.
    \end{gather*}
    By \gongshi{$\mathbf{x}_{k}=h_{k}^{(0)}$} and incorporating the equations above, we can know that
    \begin{align*}
        \{\{ (\mathbf{x}_{p},\widetilde{A}_{up}),u \in \overline{\mathcal{N}}_{i}^{l-1}, p \in \overline{\mathcal{N}}_{i}^{l}\}\}=\{\{ (\mathbf{x}_{q},\widetilde{A}_{vq}),v \in \overline{\mathcal{N}}_{j}^{l-1},q \in \overline{\mathcal{N}}_{j}^{l} \}\}.
    \end{align*}
    Thus, we have
    \begin{gather*}
        \mathbf{H}_{\overline{\mathcal{N}}^{l-1}_{i}}^{(1)} = \sigma(\widetilde{A}_{\overline{\mathcal{N}}^{l-1}_{i},\overline{\mathcal{N}}^{l}_{i}} \mathbf{X}_{\overline{\mathcal{N}}^{l}_{i}} \mathbf{W}^{(0)}) = \sigma(\widetilde{A}_{\overline{\mathcal{N}}^{l-1}_{j},\overline{\mathcal{N}}^{l}_{j}} \mathbf{X}_{\overline{\mathcal{N}}^{l}_{j}} \mathbf{W}^{(0)}) = \mathbf{H}_{\overline{\mathcal{N}}^{l-1}_{j}}^{(1)}\\
        \vdots\\
        \mathbf{H}_{\overline{\mathcal{N}}_{i}}^{(l-1)} = \sigma(\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}^{2}_{i}} \mathbf{H}^{(l-2)}_{\overline{\mathcal{N}}^{2}_{i}} \mathbf{W}^{(l-2)}) = \sigma(\widetilde{A}_{\overline{\mathcal{N}}_{j},\overline{\mathcal{N}}^{2}_{j}} \mathbf{H}^{(l-2)}_{\overline{\mathcal{N}}^{2}_{j}} \mathbf{W}^{(l-2)}) = \mathbf{H}_{\overline{\mathcal{N}}_{j}}^{(l-1)}\\
        \mathbf{H}_{i}^{(l)} = \sigma(\widetilde{A}_{i,\overline{\mathcal{N}}_{i}} \mathbf{H}^{(l-1)}_{\overline{\mathcal{N}}_{i}} \mathbf{W}^{(l-1)}) = \sigma(\widetilde{A}_{j,\overline{\mathcal{N}}_{j}} \mathbf{H}^{(l-1)}_{\overline{\mathcal{N}}_{j}} \mathbf{W}^{(l-1)}) = \mathbf{H}_{j}^{(l)}
    \end{gather*}
    for all GNN parameters. 
\end{proof}


% {Without loss of generality,} given initial node feature/representation \gongshi{$h_{u}^{(0)}$}, at the $l$-th iterations, 1-WL test {of the GCN version} updates the node representation \gongshi{$h_{i}^{(l-1)}$} based on {the} local neighborhood by
% \begin{align}\label{eqn:hash}
%     h_{i}^{(l)} = Hash(\{\{ {(h_{u}^{(l-1)},deg(u))},u\in \mathcal{N}(i) \cup \{i\} \}\}).
% \end{align}
% Following \citep{gin}, we show the connections between GNNs and 1-WL test in Lemma \ref{lemma:pro_of_isomor_nodes}.
% {
% For simplicity, we only show the following theories in the GCN structure.
% However, for other GNN structures, we can also easily generalize the definition of the 1-WL test and the corresponding theories. 
% }

% 挪到附录？
% Given initial node feature/representation \gongshi{$h_{u}^{(0)}$}, at the $l$-th iterations, 1-WL test for GCNs updates the node representation \gongshi{$h_{i}^{(l-1)}$} based on the local neighborhood by
% \begin{align}\label{eqn:hash}
%     h_{i}^{(l)} = Hash(\{\{ (h_{u}^{(l-1)},\widetilde{A}_{iu}),u\in \mathcal{N}_{i} \cup \{i\} \}\}).
% \end{align}

However, as many GNNs are less expressive than the 1-WL test, it is difficult to find 1-WL isomorphic node pairs by detecting the embeddings in GNNs. Fortunately, TOP does not require as strong expressiveness as the 1-WL test. For two nodes, we do not need to identify whether they are 1-WL isomorphic, but only need to identify whether they are indistinguishable by GNNs.

\begin{definition}\label{def:isomorphic}
(Isomorphism under GNNs). Given initial node feature/representation \gongshi{$\mathbf{H}^{(0)}$}, at the \gongshi{$l$}-th iteration, node pairs \gongshi{$(i,j)$} are \textit{isomorphic} if they are indistinguishable under \gongshi{$l$} iterations of GNNs, i.e.  \gongshi{$\mathbf{H}_{i}^{(l)} = \mathbf{H}_{j}^{(l)}$} for all GNN parameters. 
\end{definition}



% Following \citep{gin}, we show the connections between GNNs and 1-WL test in Lemma \ref{lemma:pro_of_isomor_nodes}.

% \begin{lemma}\label{lemma:pro_of_isomor_nodes}
%     Given a graph \gongshi{$\mathcal{G}=(\mathcal{V},\mathcal{E})$} and a GNN, if nodes \gongshi{$i,j \in \mathcal{V}$} are indistinguishable under \gongshi{$l$} iterations of the 1-WL test, then there holds
%     \begin{align*}
%         \mathbf{H}_{i}^{(l)} = \mathbf{H}_{j}^{(l)},
%     \end{align*}
%     for all GNN parameters.
% \end{lemma}


% Intuitively, if two nodes are indistinguishable under 1 iteration of the 1-WL test, then they have the same initial features and their 1-hop neighbors have the same initial features.
% Thus, their embeddings at the first layer of GNNs are the same.
% Similarly, if two nodes are indistinguishable under \gongshi{$l$} iteration of the 1-WL test, then they have the same initial features and their $i$-hop neighbors with all \gongshi{$i \leq l$} have the same initial features. Thus, their embeddings at the $l$-th layer are the same. 
% Therefore, given two indistinguishable nodes under \gongshi{$l$} iterations of the 1-WL test, we can use one to extrapolate the other without any bias.
From the definition \ref{def:isomorphic}, if two nodes are isomorphic under \gongshi{$l$} iterations of GNNs, then their embeddings at the \gongshi{$l$}-th layer are the same for all GNN parameters. Therefore, given two indistinguishable nodes under \gongshi{$l$} iterations of GNNs, we can use one to extrapolate the other without any bias.




\udfsection{Finding isomorphic node pairs.} We estimate the coefficient matrix \gongshi{$\mathbf{R}$} by solving Problem \eqref{eqn:solver} with \gongshi{$\overline{\mathbf{H}}(\mathcal{W}^{rand})$} are the embeddings of GNNs at random initialization.
Intuitively, a neural network at random initialization is likely to be a hash function, as it maps different inputs to different vectors in the high dimensional space.
The hash function can detect isomorphic node pairs with the same embeddings.
We show this by the following theorem.

% \begin{theorem}\label{theorem:GNN_Injective} 
%     Assume that the activation function \gongshi{$\sigma$} is an identity mapping and GCNs are randomly initialized. If node pairs \gongshi{$(i,j)$} are not isomorphic, then \gongshi{$\mathbf{H}^{(l,rand)}_i \neq \mathbf{H}^{(l,rand)}_j$} with probability one.
%     % . Denote $U_{0}$ as the event: GNN is an injective function of the input features $\mathbf{X}$. Then, $p(U_{0}) = 1$.
% \end{theorem}

% \begin{proof}
%     % For simplicity, we present the proof for the case where \gongshi{$\sigma = Id$}. For other injective functions, the proof is analogous.
%     % We state some details of the conditions here.
%     % We denote the finite set that \gongshi{$\mathbf{X}$} sampled from by \gongshi{$\mathbf{F} = \{ e_{1},e_{2},\dots, e_{t_{0}} \}$}. 
%     % The GNN parameters \gongshi{$\mathbf{W}^{(l)} \sim \mathcal{N}(0,\mathcal{I})$} are independent random variables. 
    
%     % Suppose node pairs \gongshi{$(i,j)$} are indistinguishable under \gongshi{$l$} iterations of 1-WL test. 
%     % By Lemma \ref{lemma:pro_of_isomor_nodes}, we have \gongshi{$\mathbf{H}_{i}^{(l)}=\mathbf{H}_{j}^{(l)}$} for all GNN parameters.
%     % Thus, \gongshi{$\mathbf{P}(\mathbf{H}_{i}^{(l,rand)}=\mathbf{H}_{j}^{(l,rand)}) = 1$}.

%     % On the other hand, suppose \gongshi{$\mathbf{P}(\mathbf{H}_{i}^{(l,rand)}=\mathbf{H}_{j}^{(l,rand)}) = 1$}. 
%     % As \gongshi{$\sigma$} is injective, for \gongshi{$l=1$}, we have
%     % \begin{align*}
%     %     \mathbf{P}(\widetilde{A}_{i,\overline{\mathcal{N}}_{i}} \mathbf{X}_{\overline{\mathcal{N}}_{i}} \mathbf{W}^{(0)} = \widetilde{A}_{j,\overline{\mathcal{N}}_{j}} \mathbf{X}_{\overline{\mathcal{N}}_{j}} \mathbf{W}^{(0)})=1
%     % \end{align*}
%     % As \gongshi{$\mathbf{W}^{(0)} \sim \mathcal{N}(0,\mathcal{I})$},  \gongshi{$det(\mathbf{W^{(0)}})$} is a continuous random variable with zero probability at any value. So \gongshi{$\mathbf{P}(det(\mathbf{W}^{(0)})=0)=0$}, which means \gongshi{$\mathbf{W}^{(0)}$} is invertible with probability 1. Thus we have
%     % \begin{align*}
%     %     \widetilde{A}_{i,\overline{\mathcal{N}}_{i}} \mathbf{X}_{\overline{\mathcal{N}}_{i}} = \widetilde{A}_{j,\overline{\mathcal{N}}_{j}} \mathbf{X}_{\overline{\mathcal{N}}_{j}}.
%     % \end{align*}
%     % {It is equivalent to 
%     % \begin{align*}
%     %     \{\{ (\mathbf{x}_{u}, \widetilde{A}_{iu}), u\in \overline{\mathcal{N}}_{i} \}\} = \{\{ (\mathbf{x}_{v}, \widetilde{A}_{jv}), v\in \overline{\mathcal{N}}_{j} \}\},
%     % \end{align*}
%     % since the initial features are sampled from the finite set \gongshi{$\mathbf{F}$}. Thus, the node pairs \gongshi{$(i,j)$} are indistinguishable under the first iteration of the 1-WL test. For \gongshi{$l \ge 2$}, the proof is the same.}

%     Suppose node pairs \gongshi{$(i,j)$} are not isomorphic.
%     For \gongshi{$l=1$}, we have
%     % \begin{align*}
%     %     \sigma(\widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\mathbf{X}_{\overline{\mathcal{N}}_{i}}\mathbf{W}^{(0)}) \not\equiv \sigma(\widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\mathbf{X}_{\overline{\mathcal{N}}_{j}}\mathbf{W}^{(0)}).
%     % \end{align*}
%     % As \gongshi{$\sigma$} is injective, we have
%     \begin{align*}
%         \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\mathbf{X}_{\overline{\mathcal{N}}_{i}}\mathbf{W}^{(0)} \not\equiv \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\mathbf{X}_{\overline{\mathcal{N}}_{j}}\mathbf{W}^{(0)}.
%     \end{align*}
%     % Since GCNs are randomly initialized, \gongshi{$\mathbf{W}^{(0)} \sim \mathcal{N}(0,\mathcal{I})$}, then \gongshi{$det(\mathbf{W^{(0)}})$} is a continuous random variable with zero probability of any single value. Thus, \gongshi{$p(det(\mathbf{W}^{(0)})=0)=0$}, which means \gongshi{$\mathbf{W}^{(0)}$} is invertible with probability 1.
%     Thus, we have
%     \begin{align*}
%         \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\mathbf{X}_{\overline{\mathcal{N}}_{i}} \neq \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\mathbf{X}_{\overline{\mathcal{N}}_{j}},
%     \end{align*}
%     leading to
%     \begin{align*}
%         \mathbf{H}_{i}^{(1,rand)} = \sigma(\widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\mathbf{X}_{\overline{\mathcal{N}}_{i}}\mathbf{W}^{(0,rand)}) \neq \sigma(\widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\mathbf{X}_{\overline{\mathcal{N}}_{j}}\mathbf{W}^{(0,rand)}) = \mathbf{H}_{j}^{(1,rand)}
%     \end{align*}
%     for all GCN parameters.
%     Therefore, \gongshi{$\mathbf{H}^{(1,rand)}_i \neq \mathbf{H}^{(1,rand)}_j$} with probability one.
%     % if GCNs detect that \gongshi{$\mathbf{H}^{(1,rand)}_i = \mathbf{H}^{(1,rand)}_j$}, then \gongshi{$\widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\mathbf{X}_{\overline{\mathcal{N}}_{i}} = \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\mathbf{X}_{\overline{\mathcal{N}}_{j}}$} with probability one. Thus, \gongshi{$\mathbf{H}^{(1)}_i = \mathbf{H}^{(1)}_j$} holds for all GCN parameters, which means node pairs \gongshi{$(i,j)$} are isomorphic with probability one.

%     For \gongshi{$l=2$}, similar to the situation of \gongshi{$l=1$}, we have
%     \begin{align*}
%         \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\mathbf{H}_{\overline{\mathcal{N}}_{i}}^{(1)} \not\equiv \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\mathbf{H}_{\overline{\mathcal{N}}_{j}}^{(1)}.
%     \end{align*}
%     Specifically, it is
%     \begin{align*}
%         % \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\sigma(\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}}\mathbf{W}^{(0)}) \not\equiv \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\sigma(\widetilde{A}_{\overline{\mathcal{N}}_{j},\overline{\mathcal{N}}_{j}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{j}^{2}}\mathbf{W}^{(0)}).
%         \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}}\mathbf{W}^{(0)} \not\equiv \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\widetilde{A}_{\overline{\mathcal{N}}_{j},\overline{\mathcal{N}}_{j}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{j}^{2}}\mathbf{W}^{(0)}.
%     \end{align*}
%     Thus, we have
%     \begin{align*}
%         \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}} \neq \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\widetilde{A}_{\overline{\mathcal{N}}_{j},\overline{\mathcal{N}}_{j}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{j}^{2}},
%     \end{align*}
%     leading to
%     \begin{align*}
%         \mathbf{H}_{i}^{(2,rand)}
%         &= \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}(\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}}\mathbf{W}^{(0,rand)})\mathbf{W}^{(1,rand)}\\
%         &\neq \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}(\widetilde{A}_{\overline{\mathcal{N}}_{j},\overline{\mathcal{N}}_{j}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{j}^{2}}\mathbf{W}^{(0,rand)})\mathbf{W}^{(1,rand)} = \mathbf{H}_{j}^{(2,rand)}
%     \end{align*}
%     for all GCN parameters.
%     Therefore, \gongshi{$\mathbf{H}^{(2,rand)}_i \neq \mathbf{H}^{(2,rand)}_j$} with probability one.
    
%     % Notice that, if \gongshi{$\mathbf{W}^{(0)}$} satisfies \gongshi{$\widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\sigma(\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}}\mathbf{W}^{(0)}) = \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\sigma(\widetilde{A}_{\overline{\mathcal{N}}_{j},\overline{\mathcal{N}}_{j}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{j}^{2}}\mathbf{W}^{(0)})$}, then \gongshi{$\sigma(\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}}\mathbf{W}^{(0)}) \in diag(\frac{(\widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\sigma(\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}}\mathbf{W}^{(0)}))_{1}}{\widetilde{A}_{j,\overline{\mathcal{N}}_{j}}_{1}}, \dots, )$}
    
    
%     For \gongshi{$l \ge 3$}, the proof is similar.
% \end{proof}

\begin{theorem}\label{theorem:GNN_Injective} 
    Assume that the activation function \gongshi{$\sigma$} is the \textrm{LeakyReLU} function and GCNs are randomly initialized. If node pairs \gongshi{$(i,j)$} are not isomorphic, then \gongshi{$\mathbf{H}^{(l,rand)}_i \neq \mathbf{H}^{(l,rand)}_j$} with probability one.
    % . Denote $U_{0}$ as the event: GNN is an injective function of the input features $\mathbf{X}$. Then, $p(U_{0}) = 1$.
\end{theorem}

\begin{proof}
    Suppose node pairs \gongshi{$(i,j)$} are not isomorphic.
    For \gongshi{$l=1$}, we have 
    \begin{align*}
        \sigma(\widetilde{A}_{i}\mathbf{X}\mathbf{W}^{(0)}) = \mathbf{H}_{i}^{(1)} \not\equiv \mathbf{H}_{j}^{(1)} = \sigma(\widetilde{A}_{j}\mathbf{X}\mathbf{W}^{(0)}).
    \end{align*}
    Since the activation function \gongshi{$\sigma = \mathrm{LeakyReLU}$} is injective, we have
    \begin{align*}
        \widetilde{A}_{i}\mathbf{X}\mathbf{W}^{(0)}  \not\equiv \widetilde{A}_{j}\mathbf{X}\mathbf{W}^{(0)},
    \end{align*}
    leading to
    \begin{align*}
        \widetilde{A}_{i}\mathbf{X} \neq \widetilde{A}_{j}\mathbf{X}.
    \end{align*}
    Thus, we have 
    \begin{align*}
         \widetilde{A}_{i}\mathbf{X}\mathbf{W}^{(0,rand)} \neq & \ \widetilde{A}_{j}\mathbf{X}\mathbf{W}^{(0,rand)}
        \\
        \mathbf{H}_{i}^{(1,rand)} = \sigma(\widetilde{A}_{i}\mathbf{X}\mathbf{W}^{(0,rand)}) \neq & \  \sigma(\widetilde{A}_{j}\mathbf{X}\mathbf{W}^{(0,rand)}) = \mathbf{H}_{j}^{(1,rand)}
    \end{align*}
    for all GCN parameters.
    
    For \gongshi{$l \ge 2$}, similar to the case of \gongshi{$l = 1$}, we have
    \begin{align*}
        \widetilde{A}_{i}\ \sigma(A\mathbf{H}^{(l-2)}\mathbf{W}^{(l-2)}) = \widetilde{A}_{i}\mathbf{H}^{(l-1)} \not\equiv \widetilde{A}_{j}\mathbf{H}^{(l-1)} = \widetilde{A}_{j}\ \sigma(A\mathbf{H}^{(l-2)}\mathbf{W}^{(l-2)}).
    \end{align*}
    We only need to prove that \gongshi{$\{ \mathbf{W}^{(l-2)}\in \mathbb{R}^{d\times d}\ |\ \widetilde{A}_{i}\ \sigma(A\mathbf{H}^{(l-2)}\mathbf{W}^{(l-2)}) = \widetilde{A}_{j}\ \sigma(A\mathbf{H}^{(l-2)}\mathbf{W}^{(l-2)}) \}$} is a Null set in \gongshi{$\mathbb{R}^{d\times d}$}.
    For simplicity, we denote \gongshi{$\alpha^{\top} = \widetilde{A}_{i}$}, \gongshi{$\beta^{\top} = \widetilde{A}_{j}$} and \gongshi{$B = A\mathbf{H}^{(l-2)}$}.
    Thus, \gongshi{$\widetilde{A}_{i}\ \sigma(A\mathbf{H}^{(l-2)}\mathbf{W}^{(l-2)}) = \widetilde{A}_{j}\ \sigma(A\mathbf{H}^{(l-2)}\mathbf{W}^{(l-2)})$} is equivalent to \gongshi{$(\alpha - \beta)^{\top} \sigma(B\mathbf{W}^{(l-2)}) = \mathbf{0}^{\top}$}.
    
    Notice that
    \begin{align*}
        \sigma(x) = \mathrm{LeakyReLU}(x) = 
        \begin{cases}
            x, & \mathrm{if}\ x \ge 0 \\
            kx, & \mathrm{if}\ x < 0
        \end{cases},
    \end{align*}
    where \gongshi{$k\in\mathbb{R}$} is the negative slope with the default value 1e-2.
    
    Then we can know that
    \begin{align*}
        (\sigma(B\mathbf{W}^{(l-2)}))_{uv} = \sigma(\sum_{s=1}^{d} B_{us}\mathbf{W}^{(l-2)}_{sv}) = \sum_{s=1}^{d} \sigma_{uv} B_{us}\mathbf{W}^{(l-2)}_{sv},
    \end{align*}
    where \gongshi{$u\in \llbracket n \rrbracket,\ v\in \llbracket d \rrbracket$, $\sigma_{uv} = 1\ \mathrm{or}\ k$}.
    
    Therefore, 
    \begin{align*}
        ((\alpha - \beta)^{\top} \sigma(B\mathbf{W}^{(l-2)}))_{v} 
        & = \sum_{u=1}^{n}(\alpha_{u}-\beta_{u}) \sum_{s=1}^{d} \sigma_{uv} B_{us}\mathbf{W}^{(l-2)}_{sv}
        \\
        & = \sum_{s=1}^{d} \mathbf{W}^{(l-2)}_{sv} \sum_{u=1}^{n} \sigma_{uv}B_{us}(\alpha_{u}-\beta_{u}).
    \end{align*}

    Let \gongshi{$\gamma_{sv} = \sum_{u=1}^{n} \sigma_{uv}B_{us}(\alpha_{u}-\beta_{u}) \in \mathbb{R}$} and \gongshi{$\gamma = (\gamma_{sv}) \in \mathbb{R}^{d\times d}$}. Then
    \begin{align*}
        ((\alpha - \beta)^{\top} \sigma(B\mathbf{W}^{(l-2)})) = \mathbf{0}^{\top}
    \end{align*}
    is equivalent to
    \begin{align*}
        \sum_{s=1}^{d} \mathbf{W}^{(l-2)}_{sv}\gamma_{sv} = ((\alpha - \beta)^{\top} \sigma(B\mathbf{W}^{(l-2)}))_{v} = 0
    \end{align*}
    for all \gongshi{$v\in \llbracket d \rrbracket$}.

    However, \gongshi{$\gamma \neq \mathbf{0}$} for all value of \gongshi{$\sigma_{uv}$} since the isomorphism of node pairs \gongshi{$(i,j)$}. 
    As a result, for \gongshi{$\sigma_{uv}$} fixed, the solution to \gongshi{$\sum_{s=1}^{d} \mathbf{W}^{(l-2)}_{sv}\gamma_{sv} = 0,\ v \in \llbracket d \rrbracket$} forms a subspace in \gongshi{$\mathbb{R}^{d\times d}$} with the dimension \gongshi{${d \times d - 1}$} at most.
    
    Thus, the set \gongshi{$\{ \mathbf{W}^{(l-2)}\in \mathbb{R}^{d\times d}\ |\ \widetilde{A}_{i}\ \sigma(A\mathbf{H}^{(l-2)}\mathbf{W}^{(l-2)}) = \widetilde{A}_{j}\ \sigma(A\mathbf{H}^{(l-2)}\mathbf{W}^{(l-2)}) \}$} is contained by the union of several subspaces in \gongshi{$\mathbb{R}^{d\times d}$} with the dimension \gongshi{${d \times d - 1}$} at most, which means it is a Null set in \gongshi{$\mathbb{R}^{d\times d}$}.
    
    Therefore, \gongshi{$\mathbf{H}^{(l,rand)}_i \neq \mathbf{H}^{(l,rand)}_j$} with probability one.
\end{proof}



















% From Theorem \ref{theorem:GNN_Injective}, for an out-of-batch node \gongshi{$j \in \mathcal{N(B)-B}$}, if there exists a 1-WL indistinguishable node \gongshi{$i \in \mathcal{B}$}, then the solution \gongshi{$\mathbf{R}$} to Problem \eqref{eqn:embedding_minimization} satisfies \gongshi{$\mathbf{R}_{j} = \mathbf{e}_i^{\top}$}, where the $i$-th entry of \gongshi{$\mathbf{e}_i$} is one and other entries are zero.
% Moreover, the approximation error at node \gongshi{$j$} is zero due to \gongshi{$\mathbf{H}^{(l)}_j =  \mathbf{e}_i^{\top}  \mathbf{H}^{(l)}_{\mathcal{B}} = \mathbf{H}^{(l)}_i$}.
From Theorem \ref{theorem:GNN_Injective}, for an out-of-batch node \gongshi{$j \in \mathcal{N}_{\mathcal{B}}^c$}, if randomly initialized GCNs detect \gongshi{$i \in \mathcal{B}$} such that {$\mathbf{H}^{(l,rand)}_i = \mathbf{H}^{(l,rand)}_j$}, then node pairs \gongshi{$(i,j)$} is probably isomorphic. Thus, we can estimate the solution \gongshi{$\mathbf{R}$} to Problem \eqref{eqn:solver} that \gongshi{$\mathbf{R}_{j} = \mathbf{e}_i^{\top}$}. Moreover, the estimation probably leads to a zero approximation error at node \gongshi{$j$} since \gongshi{$\mathbf{H}^{(l)}_j =  \mathbf{e}_i^{\top}  \mathbf{H}^{(l)}_{\mathcal{B}} = \mathbf{H}^{(l)}_i$} holds for all GCN parameters.


In practice, the ratio of the indistinguishable node pairs increases as the batch size \gongshi{$|\mathcal{B}|$} increases.
The following theorem shows that the approximation error of TOP decreases to zero if the batch size \gongshi{$|\mathcal{B}|$} is large enough.
\begin{theorem}\label{thm:high_approx_of_linear_extra}
    Assume that \gongshi{$\mathcal{B}$} is uniformly selected from \gongshi{$\mathcal{V}$}, the initial features $\mathbf{X}$ are sampled from a finite set, the number of different embeddings is bounded by \gongshi{$t$}, and \gongshi{$|\mathcal{B}| \ge B_{0} \triangleq t \log (Lt\varepsilon^{-1})$}. Then, there exists the coefficient matrix \gongshi{$\mathbf{R}$} such that \gongshi{$\overline{\mathbf{H}}_{\mathcal{N}_{\mathcal{B}}^c}=\mathbf{R}\overline{\mathbf{H}}_{\mathcal{B}}$} with probability \gongshi{$1-\mathcal{O}(\varepsilon)$} for any GCN parameters.
\end{theorem}

\begin{proof}

    By Theorem \ref{theorem:GNN_Injective}, if for any out-of-batch node \gongshi{$j$}, there exists an isomorphic in-batch node \gongshi{$i \in \mathcal{B}$}, then we can easily find the coefficient matrix \gongshi{$\mathbf{R}$} with \gongshi{$\mathbf{R}_{i}=e_{j}$} such that \gongshi{$\overline{\mathbf{H}}_{\mathcal{N}_{\mathcal{B}}^{c}}=\mathbf{R}\overline{\mathbf{H}}_{\mathcal{B}}$}.
    
    As a result, we only need to estimate the probability of the existence of such an in-batch node \gongshi{$i$}. Notice that, if for all \gongshi{$l \in \llbracket L \rrbracket$}, \gongshi{$\{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}$} contains all the embeddings in \gongshi{$E^{(l)}$}, then the existence follows immediately.
    Thus, we estimate the probability of \gongshi{$E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\},\ \forall l \in \llbracket L \rrbracket$} as a lower bound. 

    By considering the contrary, for \gongshi{$|\mathcal{B}|$} fixed, we have
    \begin{align*}
        p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|)
        & =
        1 - p(E^{(l)} \not\subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|)\\
        & = 
        1 - p(\exists\ \mathbf{h}_{u}^{(l)} \notin \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|)\\
        & \ge
        1 - \sum_{u=1}^{t^{(l)}} p(\mathbf{h}_{u}^{(l)} \notin \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|).
    \end{align*}
    Notice that \gongshi{$t^{(l)} \leq t$} and \gongshi{$p(\mathbf{h}_{u}^{(l)} \notin \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|) = (1-\frac{1}{t^{(l)}})^{|\mathcal{B}|} \leq (1-\frac{1}{t})^{|\mathcal{B}|}$}, we have
    \begin{align*}
        p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|)
        & \ge 
        1 - \sum_{u=1}^{t^{(l)}} p(\mathbf{h}_{u}^{(l)} \notin \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|)\\
        & \ge
        1 - \sum_{u=1}^{t^{(l)}} (1-\frac{1}{t})^{|\mathcal{B}|}\\
        & \ge
        1 - t(1-\frac{1}{t})^{|\mathcal{B}|},
    \end{align*}
    which leads to
    \begin{align*}
        p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\},\ \forall l \in \llbracket L \rrbracket\ |\ |\mathcal{B}|)
        & =
        1 - p(\exists E^{(l)} \not\subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|)\\
        & \ge
        1 - \sum_{l=1}^{L}p(E^{(l)} \not\subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\})\\
        & \ge
        1 - Lt(1-\frac{1}{t})^{|\mathcal{B}|}.
    \end{align*}
    By the condition of the batch size \gongshi{$|\mathcal{B}|$}, we know that
    \begin{align*}
        |\mathcal{B}|
        & \ge 
        t\log (Lt\varepsilon^{-1})\\
        & =
        \frac{\log (Lt\varepsilon^{-1})}{\frac{1}{t}}\\
        & \ge
        \frac{\log (Lt\varepsilon^{-1})}{-\log (1-\frac{1}{t})}\\
        & =
        -\log_{1-\frac{1}{t}} (Lt\varepsilon^{-1}),
    \end{align*}
    leading to
    \begin{align*}
        p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\},\ \forall l \in \llbracket L 
        \rrbracket\ |\ |\mathcal{B}|)
        & \ge 
        1 - Lt (1-\frac{1}{t})^{|\mathcal{B}|}\\
        & \ge
        1 - Lt (1-\frac{1}{t})^{-\log_{1-\frac{1}{t}} (Lt\varepsilon^{-1})}\\
        & = 
        1 - \varepsilon.
    \end{align*}
    Thus, we have
    \begin{align*}
        p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\},\ \forall l \in \llbracket L \rrbracket) 
        &= \sum_{|\mathcal{B}| = B_{0}}^{|\mathcal{V}|} p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\},\ \forall l \in \llbracket L \rrbracket\ |\ |\mathcal{B}|)p(|\mathcal{B}|)\\ 
        & \ge
        \sum_{|\mathcal{B}| = B_{0}}^{|\mathcal{V}|} (1 - \varepsilon)p(|\mathcal{B}|)\\
        & =
        1 - \varepsilon.
    \end{align*}
    Therefore, the probability of the existence of the coefficient matrix \gongshi{$\mathbf{R}$} is
    \begin{align*}
        p \ge p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\},\ \forall l \in \llbracket L \rrbracket) \ge 1 - \varepsilon,
    \end{align*}
    which means \gongshi{$p=1-\mathcal{O}(\varepsilon)$}.
\end{proof}

\begin{remark}
    The assumption of discrete features in Theorem \ref{thm:high_approx_of_linear_extra} is widely used to analyze expressiveness \citep{gin, relation_pooling}. In real-world graphs with continuous features, finding the exactly indistinguishable node pairs is difficult. For example, the probability of sampling two points with the same value from the Gaussian distribution is zero.
    Fortunately, Equation \eqref{eqn:solver} still achieves small approximation errors in practice.
    To verify this claim, we empirically demonstrate that TOP compensates for neighborhood messages well in Figure \ref{fig:inference_gap} in Section \ref{subsec:sampling_bias_of_different_methods}.
    % We leave the theoretical study of the continuous features as future work. % , where complex polynomial extrapolation may perform better than simple linear extrapolation
\end{remark}

% {\subsubsection{Number of layers to construct $\mathbf{R}$}
% While selecting the basic embeddings \gongshi{$\overline{\mathbf{H}}$}, the number of layers to construct \gongshi{$\overline{\mathbf{H}}$} needs to be considered.
% Intuitively, if we use \gongshi{$T$} layers to construct \gongshi{$\overline{\mathbf{H}}$}, the computed coefficient matrix \gongshi{$\mathbf{R}$} can only be used in GCNs with \gongshi{$T$} layers.
% However, the fact is that we can use a deep GCN at random initialization to construct the basic embeddings \gongshi{$\overline{\mathbf{H}}$}, while the corresponding coefficient matrix \gongshi{$\mathbf{R}$} can be used to train a shallow GCN with great efficiency.

% As shown in Figure x, the number of GCN layers for constructing the basic embeddings is 10, while the number of GCN layers to train is 3. The empirical results demonstrate that the accuracy rises if we increase the number of constructing layers.

% }



% \subsubsection{Basic Embeddings Encoding Optimization Dynamics}

% If the basic embeddings have full column rank, the optimal value of Problem \eqref{eqn:embedding_minimization} is zero. The full column rank usually holds in many real-world datasets with continuous node features due to \gongshi{$|\mathcal{B}| >> (T+1)d$}.
% Therefore, Equation \eqref{eqn:mini_batch_mn} is exact at the basic embeddings \gongshi{$\overline{\mathbf{H}}(\mathbf{W}^{(i)})$} with weights \gongshi{$\mathbf{W}^{(i)}$}.


% Theorem \ref{thm:linear_span} shows Equation \eqref{eqn:mini_batch_mn} is exact in a linear span of several basic embeddings rather than several isolated points.
% \begin{theorem}\label{thm:linear_span}
%     If the topological compensation \gongshi{$\Delta \mathbf{A}_{\mathcal{B},\mathcal{B}}$} satisfies  that
%     \begin{align*}
%         \Delta \mathbf{A}_{\mathcal{B},\mathcal{B}} \overline{\mathbf{H}}(\mathbf{W}^{(i)})_{\mathcal{B}} = \mathbf{Y}_{\mathcal{B}}(\overline{\mathbf{H}}(\mathbf{W}^{(i)})), \, i=1,2,\dots,N
%     \end{align*}
%     then for any input embeddings \gongshi{$\overline{\mathbf{H}}$} in the linear span of \gongshi{$\{\overline{\mathbf{H}}(\mathbf{W}^{(1)}), \overline{\mathbf{H}}(\mathbf{W}^{(2)}), \dots, \overline{\mathbf{H}}(\mathbf{W}^{(n)}) \}$}, i.e.,
%     \begin{align*}
%         \overline{\mathbf{H}} = \sum_{i=1}^n \lambda_i \overline{\mathbf{H}}(\mathbf{W}^{(i)}), \, \exists \lambda_i \in \mathbb{R},
%     \end{align*}
%     we have
%     \begin{align*}
%         \Delta \mathbf{A}_{\mathcal{B},\mathcal{B}} \overline{\mathbf{H}} = \mathbf{Y}_{\mathcal{B}}(\overline{\mathbf{H}}).
%     \end{align*}
%     Notably, we consider the arbitrary weights \gongshi{$\mathbf{W}^{(i)}$} rather than the weights at the $i$-th training step in the theorem.
%     % Thus, \gongshi{$\mathbf{Z}^{(l+1)}_{\mathcal{B}}
%     % =  (\widetilde{\mathbf{A}}_{\mathcal{B},\mathcal{B}}+ \Delta \mathbf{A}_{\mathcal{B},\mathcal{B}}) \overline{\mathbf{H}}^{(l)}_{\mathcal{B}}$}
% \end{theorem}


% % % \udfsection{Motivation for the optimization dynamics.} 
% % % \gongshi{$\mathbf{W}^{(i)}$} are the weights at the $i$-th training step.
% % As shown in \citep{l2o}, the gradients with momentum are
% % \begin{align*}
% %      \sum_{j=0}^{i-1} \alpha^{i-1-j} \nabla_{\overline{\mathbf{H}}(\mathbf{W}^{(j)})} \loss \approx \sum_{j=0}^{i-1} \alpha^{i-1-j} (\overline{\mathbf{H}}(\mathbf{W}^{(j+1)}) - \overline{\mathbf{H}}(\mathbf{W}^{(j)})) / \gamma,
% % \end{align*}
% % where $\gamma$ is the learning rate. The right side is the linear combination of the basic embeddings \gongshi{$\{\overline{\mathbf{H}}(\mathbf{W}^{(0)}), \overline{\mathbf{H}}(\mathbf{W}^{(1)}), \overline{\mathbf{H}}(\mathbf{W}^{(2)}), \dots, \overline{\mathbf{H}}(\mathbf{W}^{(i)}) \}$}.
% % From Theorem \ref{thm:linear_span}, the input embeddings at the next training step are close to 
% As shown in Figure x, the linear span of the basic embeddings at the $i$-th training step \gongshi{$\overline{\mathbf{H}}(\mathbf{W}^{(i)})$} and the starting training step  \gongshi{$\overline{\mathbf{H}}(\mathbf{W}^{(rand)})$} can approximate the training trajectory well.
% The intuition suggests the basic embeddings at the $i$-th training step in practice.
% We empirically analyze how to select these promising basic embeddings in real-world datasets in Section x.







% Problem \eqref{eqn:embedding_minimization}
% As formulated in Equation \eqref{eqn:vq}, the extrapolated out-of-batch embeddings a
% Problem \eqref{eqn:embedding_minimization}
% 寻找basic embedding能够编码所有embedding是具有挑战的，但是能够编码一部分



% Our theoretical results are as follows.
% \begin{enumerate}
%     \item The sampling bias is zero with a high probability under the assumption of discrete features.
%     \item The sampling bias is zero for the input embeddings in a large linear space under continuous features in practice. The linear space is a span of several basic embeddings. Therefore, we can select appropriate basic embeddings to approximate the training trajectory of input embeddings.
% \end{enumerate}



% In this section, we show the existence of the extrapolation from the graph isomorphism perspective and find the extrapolation based on a GNN at random initialization.
% We first introduce the graph isomorphism, which is a graph property and hence independent of GNN parameters.
% Then, we propose to use a GNN at random initialization to find the isomorphic node pairs, resulting in a linear extrapolation.
% Finally, we show the convergence of TOP based on the linear extrapolation.

% \section{Detailed Proofs}\label{sec:proof}

% \udfsection{Notations} For brevity, \gongshi{$\overline{\mathcal{N}}_{i} = \mathcal{N}_{i} \cup \{ i \}$} denotes the neighborhood of node \gongshi{$i$} with itself.
% We recursively define the set of neighborhoods within  \gongshi{$k$}-hops as \gongshi{$\overline{\mathcal{N}}_{i}^{k} = \overline{\mathcal{N}}_{\overline{\mathcal{N}}_{i}^{k-1}}$} with \gongshi{$\overline{\mathcal{N}}_{i}^{1} = \overline{\mathcal{N}}_{i}$}.
% For Theorem \ref{thm:high_approx_of_linear_extra}, we denote all the possible embeddings at the \gongshi{$l$}-th layer by \gongshi{$E^{(l)}=\{ \mathbf{h}_{1}^{(l)},\ \mathbf{h}_{2}^{(l)},\dots,\mathbf{h}_{t^{(l)}}^{(l)} \}$},  where \gongshi{$t^{(l)} \leq t$} is the number of different embeddings at the \gongshi{$l$}-th layer, \gongshi{$l \in [L]$}.

% \subsection{Proof of Theorem \ref{theorem:GNN_Injective}: Identification of the 1-WL Indistinguishable Node Pairs}
% In this subsection, we give the proof of Theorem \ref{theorem:GNN_Injective} to show that GNNs at random initialization can identify the 1-WL indistinguishable node pairs.

% \begin{theorem}
%     Assume that the activation function \gongshi{$\sigma$} is an identity mapping and GCNs are randomly initialized. If node pairs \gongshi{$(i,j)$} are not isomorphic, then \gongshi{$\mathbf{H}^{(l,rand)}_i \neq \mathbf{H}^{(l,rand)}_j$} with probability one.
% \end{theorem}

% \begin{proof}
%     % For simplicity, we present the proof for the case where \gongshi{$\sigma = Id$}. For other injective functions, the proof is analogous.
%     % We state some details of the conditions here.
%     % We denote the finite set that \gongshi{$\mathbf{X}$} sampled from by \gongshi{$\mathbf{F} = \{ e_{1},e_{2},\dots, e_{t_{0}} \}$}. 
%     % The GNN parameters \gongshi{$\mathbf{W}^{(l)} \sim \mathcal{N}(0,\mathcal{I})$} are independent random variables. 
    
%     % Suppose node pairs \gongshi{$(i,j)$} are indistinguishable under \gongshi{$l$} iterations of 1-WL test. 
%     % By Lemma \ref{lemma:pro_of_isomor_nodes}, we have \gongshi{$\mathbf{H}_{i}^{(l)}=\mathbf{H}_{j}^{(l)}$} for all GNN parameters.
%     % Thus, \gongshi{$\mathbf{P}(\mathbf{H}_{i}^{(l,rand)}=\mathbf{H}_{j}^{(l,rand)}) = 1$}.

%     % On the other hand, suppose \gongshi{$\mathbf{P}(\mathbf{H}_{i}^{(l,rand)}=\mathbf{H}_{j}^{(l,rand)}) = 1$}. 
%     % As \gongshi{$\sigma$} is injective, for \gongshi{$l=1$}, we have
%     % \begin{align*}
%     %     \mathbf{P}(\widetilde{A}_{i,\overline{\mathcal{N}}_{i}} \mathbf{X}_{\overline{\mathcal{N}}_{i}} \mathbf{W}^{(0)} = \widetilde{A}_{j,\overline{\mathcal{N}}_{j}} \mathbf{X}_{\overline{\mathcal{N}}_{j}} \mathbf{W}^{(0)})=1
%     % \end{align*}
%     % As \gongshi{$\mathbf{W}^{(0)} \sim \mathcal{N}(0,\mathcal{I})$},  \gongshi{$det(\mathbf{W^{(0)}})$} is a continuous random variable with zero probability at any value. So \gongshi{$\mathbf{P}(det(\mathbf{W}^{(0)})=0)=0$}, which means \gongshi{$\mathbf{W}^{(0)}$} is invertible with probability 1. Thus we have
%     % \begin{align*}
%     %     \widetilde{A}_{i,\overline{\mathcal{N}}_{i}} \mathbf{X}_{\overline{\mathcal{N}}_{i}} = \widetilde{A}_{j,\overline{\mathcal{N}}_{j}} \mathbf{X}_{\overline{\mathcal{N}}_{j}}.
%     % \end{align*}
%     % {It is equivalent to 
%     % \begin{align*}
%     %     \{\{ (\mathbf{x}_{u}, \widetilde{A}_{iu}), u\in \overline{\mathcal{N}}_{i} \}\} = \{\{ (\mathbf{x}_{v}, \widetilde{A}_{jv}), v\in \overline{\mathcal{N}}_{j} \}\},
%     % \end{align*}
%     % since the initial features are sampled from the finite set \gongshi{$\mathbf{F}$}. Thus, the node pairs \gongshi{$(i,j)$} are indistinguishable under the first iteration of the 1-WL test. For \gongshi{$l \ge 2$}, the proof is the same.}

%     Suppose node pairs \gongshi{$(i,j)$} are not isomorphic.
%     For \gongshi{$l=1$}, we have
%     % \begin{align*}
%     %     \sigma(\widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\mathbf{X}_{\overline{\mathcal{N}}_{i}}\mathbf{W}^{(0)}) \not\equiv \sigma(\widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\mathbf{X}_{\overline{\mathcal{N}}_{j}}\mathbf{W}^{(0)}).
%     % \end{align*}
%     % As \gongshi{$\sigma$} is injective, we have
%     \begin{align*}
%         \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\mathbf{X}_{\overline{\mathcal{N}}_{i}}\mathbf{W}^{(0)} \not\equiv \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\mathbf{X}_{\overline{\mathcal{N}}_{j}}\mathbf{W}^{(0)}.
%     \end{align*}
%     % Since GCNs are randomly initialized, \gongshi{$\mathbf{W}^{(0)} \sim \mathcal{N}(0,\mathcal{I})$}, then \gongshi{$det(\mathbf{W^{(0)}})$} is a continuous random variable with zero probability of any single value. Thus, \gongshi{$p(det(\mathbf{W}^{(0)})=0)=0$}, which means \gongshi{$\mathbf{W}^{(0)}$} is invertible with probability 1.
%     Thus, we have
%     \begin{align*}
%         \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\mathbf{X}_{\overline{\mathcal{N}}_{i}} \neq \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\mathbf{X}_{\overline{\mathcal{N}}_{j}},
%     \end{align*}
%     leading to
%     \begin{align*}
%         \mathbf{H}_{i}^{(1,rand)} = \sigma(\widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\mathbf{X}_{\overline{\mathcal{N}}_{i}}\mathbf{W}^{(0,rand)}) \neq \sigma(\widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\mathbf{X}_{\overline{\mathcal{N}}_{j}}\mathbf{W}^{(0,rand)}) = \mathbf{H}_{j}^{(1,rand)}
%     \end{align*}
%     for all GCN parameters.
%     Therefore, \gongshi{$\mathbf{H}^{(1,rand)}_i \neq \mathbf{H}^{(1,rand)}_j$} with probability one.
%     % if GCNs detect that \gongshi{$\mathbf{H}^{(1,rand)}_i = \mathbf{H}^{(1,rand)}_j$}, then \gongshi{$\widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\mathbf{X}_{\overline{\mathcal{N}}_{i}} = \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\mathbf{X}_{\overline{\mathcal{N}}_{j}}$} with probability one. Thus, \gongshi{$\mathbf{H}^{(1)}_i = \mathbf{H}^{(1)}_j$} holds for all GCN parameters, which means node pairs \gongshi{$(i,j)$} are isomorphic with probability one.

%     For \gongshi{$l=2$}, similar to the situation of \gongshi{$l=1$}, we have
%     \begin{align*}
%         \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\mathbf{H}_{\overline{\mathcal{N}}_{i}}^{(1)} \not\equiv \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\mathbf{H}_{\overline{\mathcal{N}}_{j}}^{(1)}.
%     \end{align*}
%     Specifically, it is
%     \begin{align*}
%         % \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\sigma(\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}}\mathbf{W}^{(0)}) \not\equiv \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\sigma(\widetilde{A}_{\overline{\mathcal{N}}_{j},\overline{\mathcal{N}}_{j}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{j}^{2}}\mathbf{W}^{(0)}).
%         \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}}\mathbf{W}^{(0)} \not\equiv \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\widetilde{A}_{\overline{\mathcal{N}}_{j},\overline{\mathcal{N}}_{j}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{j}^{2}}\mathbf{W}^{(0)}.
%     \end{align*}
%     Thus, we have
%     \begin{align*}
%         \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}} \neq \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\widetilde{A}_{\overline{\mathcal{N}}_{j},\overline{\mathcal{N}}_{j}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{j}^{2}},
%     \end{align*}
%     leading to
%     \begin{align*}
%         \mathbf{H}_{i}^{(2,rand)}
%         &= \widetilde{A}_{i,\overline{\mathcal{N}}_{i}}(\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}}\mathbf{W}^{(0,rand)})\mathbf{W}^{(1,rand)}\\
%         &\neq \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}(\widetilde{A}_{\overline{\mathcal{N}}_{j},\overline{\mathcal{N}}_{j}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{j}^{2}}\mathbf{W}^{(0,rand)})\mathbf{W}^{(1,rand)} = \mathbf{H}_{j}^{(2,rand)}
%     \end{align*}
%     for all GCN parameters.
%     Therefore, \gongshi{$\mathbf{H}^{(2,rand)}_i \neq \mathbf{H}^{(2,rand)}_j$} with probability one.
    
%     % Notice that, if \gongshi{$\mathbf{W}^{(0)}$} satisfies \gongshi{$\widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\sigma(\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}}\mathbf{W}^{(0)}) = \widetilde{A}_{j,\overline{\mathcal{N}}_{j}}\sigma(\widetilde{A}_{\overline{\mathcal{N}}_{j},\overline{\mathcal{N}}_{j}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{j}^{2}}\mathbf{W}^{(0)})$}, then \gongshi{$\sigma(\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}}\mathbf{W}^{(0)}) \in diag(\frac{(\widetilde{A}_{i,\overline{\mathcal{N}}_{i}}\sigma(\widetilde{A}_{\overline{\mathcal{N}}_{i},\overline{\mathcal{N}}_{i}^{2}}\mathbf{X}_{\overline{\mathcal{N}}_{i}^{2}}\mathbf{W}^{(0)}))_{1}}{\widetilde{A}_{j,\overline{\mathcal{N}}_{j}}_{1}}, \dots, )$}
    
    
%     For \gongshi{$l \ge 3$}, the proof is similar.
% \end{proof}

% \subsection{Proof of Theorem \ref{thm:high_approx_of_linear_extra}: Convergence Guarantees of TOP }
% In this subsection, we give the proof of Theorem \ref{thm:high_approx_of_linear_extra} to show that the
% sampling bias of TOP decreases to zero if the batch size $|\mathcal{B}|$ is large enough.

% \begin{theorem}
%     Assume that \gongshi{$\mathcal{B}$} is uniformly selected from \gongshi{$\mathcal{V}$}, the initial features $\mathbf{X}$ are sampled from a finite set, the number of different embeddings is bounded by \gongshi{$t$}, and \gongshi{$|\mathcal{B}| \ge B_{0} \triangleq t \log (Lt\varepsilon^{-1})$}. Then, there exists the coefficient matrix \gongshi{$\mathbf{R}$} such that \gongshi{$\overline{\mathbf{H}}_{\mathcal{N}_{\mathcal{B}}^c}=\mathbf{R}\overline{\mathbf{H}}_{\mathcal{B}}$} with probability \gongshi{$1-\mathcal{O}(\varepsilon)$} for any GNN parameters.
% \end{theorem}

% \begin{proof}

%     By Theorem \ref{theorem:GNN_Injective}, if for any out-of-batch node \gongshi{$j$}, there exists an isomorphic in-batch node \gongshi{$i \in \mathcal{B}$}, then we can easily find the coefficient matrix \gongshi{$\mathbf{R}$} with \gongshi{$\mathbf{R}_{i}=e_{j}$} such that \gongshi{$\overline{\mathbf{H}}_{\mathcal{N}_{\mathcal{B}}^{c}}=\mathbf{R}\overline{\mathbf{H}}_{\mathcal{B}}$}.
    
%     As a result, we only need to estimate the probability of the existence of such an in-batch node \gongshi{$i$}. Notice that, if for all \gongshi{$l \in [L]$}, \gongshi{$\{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}$} contains all the embeddings in \gongshi{$E^{(l)}$}, then the existence follows immediately.
%     Thus, we estimate the probability of \gongshi{$E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\},\ \forall l \in [L]$} as a lower bound. 

%     By considering the contrary, for \gongshi{$|\mathcal{B}|$} fixed, we have
%     \begin{align*}
%         p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|)
%         & =
%         1 - p(E^{(l)} \not\subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|)\\
%         & = 
%         1 - p(\exists\ \mathbf{h}_{u}^{(l)} \notin \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|)\\
%         & \ge
%         1 - \sum_{u=1}^{t^{(l)}} p(\mathbf{h}_{u}^{(l)} \notin \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|).
%     \end{align*}
%     Notice that \gongshi{$t^{(l)} \leq t$} and \gongshi{$p(\mathbf{h}_{u}^{(l)} \notin \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|) = (1-\frac{1}{t^{(l)}})^{|\mathcal{B}|} \leq (1-\frac{1}{t})^{|\mathcal{B}|}$}, we have
%     \begin{align*}
%         p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|)
%         & \ge 
%         1 - \sum_{u=1}^{t^{(l)}} p(\mathbf{h}_{u}^{(l)} \notin \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|)\\
%         & \ge
%         1 - \sum_{u=1}^{t^{(l)}} (1-\frac{1}{t})^{|\mathcal{B}|}\\
%         & \ge
%         1 - t(1-\frac{1}{t})^{|\mathcal{B}|},
%     \end{align*}
%     which leads to
%     \begin{align*}
%         p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\},\ \forall l \in [L]\ |\ |\mathcal{B}|)
%         & =
%         1 - p(\exists E^{(l)} \not\subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\}\ |\ |\mathcal{B}|)\\
%         & \ge
%         1 - \sum_{l=1}^{L}p(E^{(l)} \not\subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\})\\
%         & \ge
%         1 - Lt(1-\frac{1}{t})^{|\mathcal{B}|}.
%     \end{align*}
%     By the condition of the batch size \gongshi{$|\mathcal{B}|$}, we know that
%     \begin{align*}
%         |\mathcal{B}|
%         & \ge 
%         t\log (Lt\varepsilon^{-1})\\
%         & =
%         \frac{\log (Lt\varepsilon^{-1})}{\frac{1}{t}}\\
%         & \ge
%         \frac{\log (Lt\varepsilon^{-1})}{-\log (1-\frac{1}{t})}\\
%         & =
%         -\log_{1-\frac{1}{t}} (Lt\varepsilon^{-1}),
%     \end{align*}
%     leading to
%     \begin{align*}
%         p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\},\ \forall l \in [L]\ |\ |\mathcal{B}|)
%         & \ge 
%         1 - Lt (1-\frac{1}{t})^{|\mathcal{B}|}\\
%         & \ge
%         1 - Lt (1-\frac{1}{t})^{-\log_{1-\frac{1}{t}} (Lt\varepsilon^{-1})}\\
%         & = 
%         1 - \varepsilon.
%     \end{align*}
%     Thus, we have
%     \begin{align*}
%         p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\},\ \forall l \in [L]) 
%         &= \sum_{|\mathcal{B}| = B_{0}}^{|\mathcal{V}|} p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\},\ \forall l \in [L]\ |\ |\mathcal{B}|)p(|\mathcal{B}|)\\ 
%         & \ge
%         \sum_{|\mathcal{B}| = B_{0}}^{|\mathcal{V}|} (1 - \varepsilon)p(|\mathcal{B}|)\\
%         & =
%         1 - \varepsilon.
%     \end{align*}
%     Therefore, the probability of the existence of the coefficient matrix \gongshi{$\mathbf{R}$} is
%     \begin{align*}
%         p \ge p(E^{(l)} \subset \{\{ \mathbf{H}_{v}^{(l)},\ v\in \mathcal{B} \}\},\ \forall l \in [L]) \ge 1 - \varepsilon,
%     \end{align*}
%     which means \gongshi{$p=1-\mathcal{O}(\varepsilon)$}.
% \end{proof}


\section{Limitations and Broader Impacts}\label{sec:limitations}

In this paper, we propose a novel subgraph-wise sampling method to accelerate the training of GNNs on large-scale graphs, i.e., TOP.
The acceleration of TOP is due to the assumption of message invariance.
We have conducted extensive experiments to demonstrate that the message invariance holds in various datasets.
However, it is still possible that the message invariance assumption does not hold in certain datasets and complex GNN models.




Moreover, this work is promising in many practical and important scenarios such as search engines, recommendation systems, biological networks, and molecular property prediction.
Nonetheless, this work may have some potential risks. For example, using this work in search engine and recommendation systems to over-mine the behavior of users may cause undesirable privacy disclosure. 



\end{document}
