# chip design：

@inproceedings{
chip_design1,
title={La{MP}lace: Learning to Optimize Cross-Stage Metrics in Macro Placement},
author={Zijie Geng and Jie Wang and Ziyan Liu and Siyuan Xu and Zhentao Tang and Shixiong Kai and Mingxuan Yuan and Jianye HAO and Feng Wu},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=YLIsIzC74j}
}

@inproceedings{
chip_design2,
title={A Graph Enhanced Symbolic Discovery Framework For Efficient Circuit Synthesis},
author={Yinqi Bai and Jie Wang and Lei Chen and Zhihai Wang and Yufei Kuang and Mingxuan Yuan and Jianye HAO and Feng Wu},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=EG9nDN3eGB}
}


@inproceedings{
chip_design3,
title={A Circuit Domain Generalization Framework for Efficient Logic Synthesis in Chip Design},
author={Zhihai Wang and Lei Chen and Jie Wang and Yinqi Bai and Xing Li and Xijun Li and Mingxuan Yuan and Jianye Hao and Yongdong Zhang and Feng Wu},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
organization={PMLR}
}

@inproceedings{
chip_design4,
title={Towards Next-Generation Logic Synthesis: A Scalable Neural Circuit Generation Framework},
author={Zhihai Wang and Jie Wang and Qingyue Yang and Yinqi Bai and Xing Li and Lei Chen and Jianye HAO and Mingxuan Yuan and Bin Li and Yongdong Zhang and Feng Wu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=ZYNYhh3ocW}
}

# drug design
@inproceedings{
durg_design1,
title={De Novo Molecular Generation via Connection-aware Motif Mining},
author={Zijie Geng and Shufang Xie and Yingce Xia and Lijun Wu and Tao Qin and Jie Wang and Yongdong Zhang and Feng Wu and Tie-Yan Liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Q_Jexl8-qDi}
}

# combinatorial optimization

@inproceedings{combinatorial_optimization1,
 author = {Geng, Zijie and Li, Xijun and Wang, Jie and Li, Xiao and Zhang, Yongdong and Wu, Feng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {26025--26047},
 publisher = {Curran Associates, Inc.},
 title = {A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/5297e56ac65ba2bfa70ee9fc4818c042-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{
combinatorial_optimization2,
title={Differentiable Integer Linear Programming},
author={Zijie Geng and Jie Wang and Xijun Li and Fangzhou Zhu and Jianye HAO and Bin Li and Feng Wu},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=FPfCUJTsCn}
}

@inproceedings{
combinatorial_optimization3,
title={Apollo-{MILP}: An Alternating Prediction-Correction Neural Solving Framework for Mixed-Integer Linear Programming},
author={Haoyang Liu and Jie Wang and Zijie Geng and Xijun Li and Yuxuan Zong and Fangzhou Zhu and Jianye HAO and Feng Wu},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=mFY0tPDWK8}
}

@misc{combinatorial_optimization4,
      title={Promoting Generalization for Exact Solvers via Adversarial Instance Augmentation}, 
      author={Haoyang Liu and Yufei Kuang and Jie Wang and Xijun Li and Yongdong Zhang and Feng Wu},
      year={2023},
      eprint={2310.14161},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.14161}, 
}

@inproceedings{
combinatorial_optimization5,
title={{MILP}-StuDio: {MILP} Instance Generation via Block Structure Decomposition},
author={Haoyang Liu and Jie Wang and Wanbo Zhang and Zijie Geng and Yufei Kuang and Xijun Li and Bin Li and Yongdong Zhang and Feng Wu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=W433RI0VU4}
}

@inproceedings{
combinatorial_optimization6,
title={Learning Cut Selection for Mixed-Integer Linear Programming via Hierarchical Sequence Model},
author={Zhihai Wang and Xijun Li and Jie Wang and Yufei Kuang and Mingxuan Yuan and Jia Zeng and Yongdong Zhang and Feng Wu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Zob4P9bRNcK}
}

@ARTICLE{combinatorial_optimization7,
  author={Wang, Jie and Wang, Zhihai and Li, Xijun and Kuang, Yufei and Shi, Zhihao and Zhu, Fangzhou and Yuan, Mingxuan and Zeng, Jia and Zhang, Yongdong and Wu, Feng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Learning to Cut via Hierarchical Sequence/Set Model for Efficient Mixed-Integer Programming}, 
  year={2024},
  volume={},
  number={},
  pages={1-17},
  doi={10.1109/TPAMI.2024.3432716}}

@article{combinatorial_optimization8, title={Learning to Stop Cut Generation for Efficient Mixed-Integer Linear Programming}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/30064}, DOI={10.1609/aaai.v38i18.30064}, number={18}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Ling, Haotian and Wang, Zhihai and Wang, Jie}, year={2024}, month={Mar.}, pages={20759-20767} }


@InProceedings{submix,
  title = 	 {SubMix: Learning to Mix Graph Sampling Heuristics},
  author =       {Abu-El-Haija, Sami and Dillon, Joshua V. and Fatemi, Bahare and Axiotis, Kyriakos and Bulut, Neslihan and Gasteiger, Johannes and Perozzi, Bryan and Bateni, Mohammadhossein},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1--10},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/abu-el-haija23a/abu-el-haija23a.pdf},
  url = 	 {https://proceedings.mlr.press/v216/abu-el-haija23a.html},
  abstract = 	 {Sampling subgraphs for training Graph Neural Networks (GNNs) is receiving much attention from the GNN community. While a variety of methods have been proposed, each method samples the graph according to its own heuristic. However, there has been little work in mixing these heuristics in an end-to-end trainable manner. In this work, we design a generative framework for graph sampling. Our method, SubMix, parameterizes subgraph sampling as a convex combination of heuristics. We show that a continuous relaxation of the discrete sampling process allows us to efficiently obtain analytical gradients for training the sampling parameters. Our experimental results illustrate the usefulness of learning graph sampling in three scenarios: (1) robust training of GNNs by automatically learning to discard noisy edge sources; (2) improving model performance by trainable and online edge subset selection; and (3) by integrating our framework into decoupled GNN models improves their performance on standard benchmarks.}
}


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{
heterophily,
title={A critical look at the evaluation of {GNN}s under heterophily: Are we really making progress?},
author={Oleg Platonov and Denis Kuznedelev and Michael Diskin and Artem Babenko and Liudmila Prokhorenkova},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=tJbbQfw-5wv}
}


@inproceedings{
ibmb,
title={Influence-Based Mini-Batching for Graph Neural Networks},
author={Johannes Gasteiger and Chendi Qian and Stephan G{\"u}nnemann},
booktitle={The First Learning on Graphs Conference},
year={2022},
url={https://openreview.net/forum?id=b9g0vxzYa_}
}


@InProceedings{linear_gnn,
  title = 	 {How Powerful are Spectral Graph Neural Networks},
  author =       {Wang, Xiyuan and Zhang, Muhan},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {23341--23362},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/wang22am/wang22am.pdf},
  url = 	 {https://proceedings.mlr.press/v162/wang22am.html},
  abstract = 	 {Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based on graph signal filters. Some models able to learn arbitrary spectral filters have emerged recently. However, few works analyze the expressive power of spectral GNNs. This paper studies spectral GNNs’ expressive power theoretically. We first prove that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2) no missing frequency components in node features. We also establish a connection between the expressive power of spectral GNNs and Graph Isomorphism (GI) testing, the latter of which is often used to characterize spatial GNNs’ expressive power. Moreover, we study the difference in empirical performance among different spectral GNNs with the same expressive power from an optimization perspective, and motivate the use of an orthogonal basis whose weight function corresponds to the graph signal density in the spectrum. Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions. JacobiConv deserts nonlinearity while outperforming all baselines on both synthetic and real-world datasets.}
}



@InProceedings{acsc,
  title = 	 {Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth},
  author =       {Xu, Keyulu and Zhang, Mozhi and Jegelka, Stefanie and Kawaguchi, Kenji},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11592--11602},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/xu21k/xu21k.pdf},
  url = 	 {https://proceedings.mlr.press/v139/xu21k.html},
  abstract = 	 {Graph Neural Networks (GNNs) have been studied through the lens of expressive power and generalization. However, their optimization properties are less well understood. We take the first step towards analyzing GNN training by studying the gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that despite the non-convexity of training, convergence to a global minimum at a linear rate is guaranteed under mild assumptions that we validate on real-world graphs. Second, we study what may affect the GNNs’ training speed. Our results show that the training of GNNs is implicitly accelerated by skip connections, more depth, and/or a good label distribution. Empirical results confirm that our theoretical results for linearized GNNs align with the training behavior of nonlinear GNNs. Our results provide the first theoretical support for the success of GNNs with skip connections in terms of optimization, and suggest that deep GNNs with skip connections would be promising in practice.}
}


@inproceedings{
l2o,
title={Learning to Optimize},
author={Ke Li and Jitendra Malik},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=ry4Vrt5gl}
}

@inproceedings{
labor,
title={Layer-Neighbor Sampling --- Defusing Neighborhood Explosion in {GNN}s},
author={Muhammed Fatih Balin and Umit Catalyurek},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=Kd5W4JRsfV}
}


@InProceedings{relation_pooling,
  title = 	 {Relational Pooling for Graph Representations},
  author =       {Murphy, Ryan and Srinivasan, Balasubramaniam and Rao, Vinayak and Ribeiro, Bruno},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4663--4673},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/murphy19a/murphy19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/murphy19a.html},
  abstract = 	 {This work generalizes graph neural networks (GNNs) beyond those based on the Weisfeiler-Lehman (WL) algorithm, graph Laplacians, and diffusions. Our approach, denoted Relational Pooling (RP), draws from the theory of finite partial exchangeability to provide a framework with maximal representation power for graphs. RP can work with existing graph representation models and, somewhat counterintuitively, can make them even more powerful than the original WL isomorphism test. Additionally, RP allows architectures like Recurrent Neural Networks and Convolutional Neural Networks to be used in a theoretically sound approach for graph classification. We demonstrate improved performance of RP-based graph representations over state-of-the-art methods on a number of tasks.}
}



@article{rsvd,
author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
title = {Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions},
journal = {SIAM Review},
volume = {53},
number = {2},
pages = {217-288},
year = {2011},
doi = {10.1137/090771806},

URL = { 
    
        https://doi.org/10.1137/090771806
    
    

},
eprint = { 
    
        https://doi.org/10.1137/090771806
    
    

}
,
    abstract = { Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed—either explicitly or implicitly—to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the k dominant components of the singular value decomposition of an \$m \times n\$ matrix. (i) For a dense input matrix, randomized algorithms require \$\bigO(mn \log(k))\$ floating-point operations (flops) in contrast to \$ \bigO(mnk)\$ for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to \$\bigO(k)\$ passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data. }
}




@book{sgd,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}


@ARTICLE{vq,
  author={Gray, R.M. and Neuhoff, D.L.},
  journal={IEEE Transactions on Information Theory}, 
  title={Quantization}, 
  year={1998},
  volume={44},
  number={6},
  pages={2325-2383},
  doi={10.1109/18.720541}}


@inproceedings{ineffectiveness_vr,
 author = {Defazio, Aaron and Bottou, Leon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On the Ineffectiveness of Variance Reduced Optimization for Deep Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf},
 volume = {32},
 year = {2019}
}


@InProceedings{batch_norm,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Ioffe, Sergey and Szegedy, Christian},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {448--456},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/ioffe15.html},
  abstract = 	 {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.}
}


@InProceedings{sgc,
  title = 	 {Simplifying Graph Convolutional Networks},
  author =       {Wu, Felix and Souza, Amauri and Zhang, Tianyi and Fifty, Christopher and Yu, Tao and Weinberger, Kilian},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6861--6871},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/wu19e/wu19e.pdf},
  url = 	 {https://proceedings.mlr.press/v97/wu19e.html},
  abstract = 	 {Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.}
}


@inproceedings{
spectral_gnn,
title={Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective},
author={Muhammet Balcilar and Guillaume Renton and Pierre H{\'e}roux and Benoit Ga{\"u}z{\`e}re and S{\'e}bastien Adam and Paul Honeine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=-qh0M9XWxnv}
}

@inproceedings{spider,
 author = {Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path-Integrated Differential Estimator},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf},
 volume = {31},
 year = {2018}
}


@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}




@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@Article{string,
   Author="Szklarczyk, D.  and Kirsch, R.  and Koutrouli, M.  and Nastou, K.  and Mehryary, F.  and Hachilif, R.  and Gable, A. L.  and Fang, T.  and Doncheva, N. T.  and Pyysalo, S.  and Bork, P.  and Jensen, L. J.  and von Mering, C. ",
   Title="{{T}he {S}{T}{R}{I}{N}{G} database in 2023: protein-protein association networks and functional enrichment analyses for any sequenced genome of interest}",
   Journal="Nucleic Acids Res",
   Year="2023",
   Volume="51",
   Number="D1",
   Pages="D638-D646",
   Month="Jan"
}


@article{gustafson,
  title={Reevaluating Amdahl's law},
  author={Gustafson, John L},
  journal={Communications of the ACM},
  volume={31},
  number={5},
  pages={532--533},
  year={1988},
  publisher={ACM New York, NY, USA}
}

@article{idl,
  title={Implicit deep learning},
  author={El Ghaoui, Laurent and Gu, Fangda and Travacca, Bertrand and Askari, Armin and Tsai, Alicia},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={3},
  number={3},
  pages={930--958},
  year={2021},
  publisher={SIAM}
}

@article{gdl,
  title={Geometric deep learning: going beyond euclidean data},
  author={Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={4},
  pages={18--42},
  year={2017},
  publisher={IEEE}
}


@InProceedings{jr,
  title = 	 {Stabilizing Equilibrium Models by Jacobian Regularization},
  author =       {Bai, Shaojie and Koltun, Vladlen and Kolter, Zico},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {554--565},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/bai21b/bai21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/bai21b.html},
  abstract = 	 {Deep equilibrium networks (DEQs) are a new class of models that eschews traditional depth in favor of finding the fixed point of a single non-linear layer. These models have been shown to achieve performance competitive with the state-of-the-art deep networks while using significantly less memory. Yet they are also slower, brittle to architectural choices, and introduce potential instability to the model. In this paper, we propose a regularization scheme for DEQ models that explicitly regularizes the Jacobian of the fixed-point update equations to stabilize the learning of equilibrium models. We show that this regularization adds only minimal computational cost, significantly stabilizes the fixed-point convergence in both forward and backward passes, and scales well to high-dimensional, realistic domains (e.g., WikiText-103 language modeling and ImageNet classification). Using this method, we demonstrate, for the first time, an implicit-depth model that runs with approximately the same speed and level of performance as popular conventional deep networks such as ResNet-101, while still maintaining the constant memory footprint and architectural simplicity of DEQs. Code is available https://github.com/locuslab/deq.}
}

@inproceedings{
lmc,
title={{LMC}: Fast Training of {GNN}s via Subgraph Sampling with Provable Convergence},
author={Zhihao Shi and Xize Liang and Jie Wang},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=5VBBA91N6n}
}


@article{amazon,
  title={Defining and evaluating network communities based on ground-truth},
  author={Yang, Jaewon and Leskovec, Jure},
  journal={Knowledge and Information Systems},
  volume={42},
  number={1},
  pages={181--213},
  year={2015},
  publisher={Springer}
}

@inproceedings{recommender,
  title={Graph convolutional neural networks for web-scale recommender systems},
  author={Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L and Leskovec, Jure},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={974--983},
  year={2018}
}

@TECHREPORT{label_propagation,
    author = {Xiaojin Zhu and Zoubin Ghahramani},
    title = {Learning from Labeled and Unlabeled Data with Label Propagation},
    institution = {},
    year = {2002}
}

@inproceedings{jknet,
  title={Representation learning on graphs with jumping knowledge networks},
  author={Xu, Keyulu and Li, Chengtao and Tian, Yonglong and Sonobe, Tomohiro and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  booktitle={International Conference on Machine Learning},
  pages={5453--5462},
  year={2018},
  organization={PMLR}
}

@inproceedings{gpt,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}

@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}



@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@article{comprehensive,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Philip, S Yu},
  journal={IEEE transactions on neural networks and learning systems},
  volume={32},
  number={1},
  pages={4--24},
  year={2020},
  publisher={IEEE}
}

@article{sign,
  publtype={informal},
  author={Emanuele Rossi and Fabrizio Frasca and Ben Chamberlain and Davide Eynard and Michael M. Bronstein and Federico Monti},
  title={SIGN: Scalable Inception Graph Neural Networks},
  year={2020},
  cdate={1577836800000},
  journal={CoRR},
  volume={abs/2004.11198},
  url={https://arxiv.org/abs/2004.11198}
}

@inproceedings{
dropedge,
title={DropEdge: Towards Deep Graph Convolutional Networks on Node Classification},
author={Yu Rong and Wenbing Huang and Tingyang Xu and Junzhou Huang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hkx1qkrKPr}
}


@InProceedings{sgru,
  title = 	 {Improving Breadth-Wise Backpropagation in Graph Neural Networks Helps Learning Long-Range Dependencies.},
  author =       {Lukovnikov, Denis and Fischer, Asja},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {7180--7191},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/lukovnikov21a/lukovnikov21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/lukovnikov21a.html},
  abstract = 	 {In this work, we focus on the ability of graph neural networks (GNNs) to learn long-range patterns in graphs with edge features. Learning patterns that involve longer paths in the graph, requires using deeper GNNs. However, GNNs suffer from a drop in performance with increasing network depth. To improve the performance of deeper GNNs, previous works have investigated normalization techniques and various types of skip connections. While they are designed to improve depth-wise backpropagation between the representations of the same node in successive layers, they do not improve breadth-wise backpropagation between representations of neighbouring nodes. To analyse the consequences, we design synthetic datasets serving as a testbed for the ability of GNNs to learn long-range patterns. Our analysis shows that several commonly used GNN variants with only depth-wise skip connections indeed have problems learning long-range patterns. They are clearly outperformed by an attention-based GNN architecture that we propose for improving both depth- and breadth-wise backpropagation. We also verify that the presented architecture is competitive on real-world data.}
}


@InProceedings{lipschitznorm,
  title = 	 {Lipschitz normalization for self-attention layers with application to graph neural networks},
  author =       {Dasoulas, George and Scaman, Kevin and Virmaux, Aladin},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2456--2466},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/dasoulas21a/dasoulas21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/dasoulas21a.html},
  abstract = 	 {Attention based neural networks are state of the art in a large range of applications. However, their performance tends to degrade when the number of layers increases. In this work, we show that enforcing Lipschitz continuity by normalizing the attention scores can significantly improve the performance of deep attention models. First, we show that, for deep graph attention networks (GAT), gradient explosion appears during training, leading to poor performance of gradient-based training algorithms. To address this issue, we derive a theoretical analysis of the Lipschitz continuity of attention modules and introduce LipschitzNorm, a simple and parameter-free normalization for self-attention mechanisms that enforces the model to be Lipschitz continuous. We then apply LipschitzNorm to GAT and Graph Transformers and show that their performance is substantially improved in the deep setting (10 to 30 layers). More specifically, we show that a deep GAT model with LipschitzNorm achieves state of the art results for node label prediction tasks that exhibit long-range dependencies, while showing consistent improvements over their unnormalized counterparts in benchmark node classification tasks.}
}


@inproceedings{pairnorm,
  author    = {Lingxiao Zhao and
               Leman Akoglu},
  title     = {PairNorm: Tackling Oversmoothing in GNNs},
  booktitle = {{ICLR}},
  publisher = {OpenReview.net},
  year      = {2020}
}


@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}


@InProceedings{planetoid,
  title = 	 {Revisiting Semi-Supervised Learning with Graph Embeddings},
  author = 	 {Yang, Zhilin and Cohen, William and Salakhudinov, Ruslan},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {40--48},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/yanga16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/yanga16.html},
  abstract = 	 {We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.}
}

@inproceedings{adapt,
 author = {Huang, Wenbing and Zhang, Tong and Rong, Yu and Huang, Junzhou},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Sampling Towards Fast Graph Representation Learning},
 url = {https://proceedings.neurips.cc/paper/2018/file/01eee509ee2f68dc6014898c309e86bf-Paper.pdf},
 volume = {31},
 year = {2018}
}



@inproceedings{ladies,
 author = {Zou, Difan and Hu, Ziniu and Wang, Yewen and Jiang, Song and Sun, Yizhou and Gu, Quanquan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/91ba4a4478a66bee9812b0804b6f9d1b-Paper.pdf},
 volume = {32},
 year = {2019}
}



@inproceedings{
fastgcn,
title={Fast{GCN}: Fast Learning with Graph Convolutional Networks via Importance Sampling},
author={Jie Chen and Tengfei Ma and Cao Xiao},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rytstxWAW},
}


@InProceedings{vrgcn,
  title = 	 {Stochastic Training of Graph Convolutional Networks with Variance Reduction},
  author =       {Chen, Jianfei and Zhu, Jun and Song, Le},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {942--950},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
}


@InProceedings{graphfm,
  title = 	 {{G}raph{FM}: Improving Large-Scale {GNN} Training via Feature Momentum},
  author =       {Yu, Haiyang and Wang, Limei and Wang, Bokun and Liu, Meng and Yang, Tianbao and Ji, Shuiwang},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {25684--25701},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/yu22g/yu22g.pdf},
  url = 	 {https://proceedings.mlr.press/v162/yu22g.html},
  abstract = 	 {Training of graph neural networks (GNNs) for large-scale node classification is challenging. A key difficulty lies in obtaining accurate hidden node representations while avoiding the neighborhood explosion problem. Here, we propose a new technique, named feature momentum (FM), that uses a momentum step to incorporate historical embeddings when updating feature representations. We develop two specific algorithms, known as GraphFM-IB and GraphFM-OB, that consider in-batch and out-of-batch data, respectively. GraphFM-IB applies FM to in-batch sampled data, while GraphFM-OB applies FM to out-of-batch data that are 1-hop neighborhood of in-batch data. We provide a convergence analysis for GraphFM-IB and some theoretical insight for GraphFM-OB. Empirically, we observe that GraphFM-IB can effectively alleviate the neighborhood explosion problem of existing methods. In addition, GraphFM-OB achieves promising performance on multiple large-scale graph datasets.}
}


@inproceedings{mvs,
author = {Cong, Weilin and Forsati, Rana and Kandemir, Mahmut and Mahdavi, Mehrdad},
title = {Minimal Variance Sampling with Provable Guarantees for Fast Training of Graph Neural Networks},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403192},
doi = {10.1145/3394486.3403192},
abstract = {Sampling methods (e.g., node-wise, layer-wise, or subgraph) has become an indispensable strategy to speed up training large-scale Graph Neural Networks (GNNs). However, existing sampling methods are mostly based on the graph structural information and ignore the dynamicity of optimization, which leads to high variance in estimating the stochastic gradients. The high variance issue can be very pronounced in extremely large graphs, where it results in slow convergence and poor generalization. In this paper, we theoretically analyze the variance of sampling methods and show that, due to the composite structure of empirical risk, the variance of any sampling method can be decomposed intoembedding approximation variance in the forward stage andstochastic gradient variance in the backward stage that necessities mitigating both types of variance to obtain faster convergence rate. We propose a decoupled variance reduction strategy that employs (approximate) gradient information to adaptively sample nodes with minimal variance, and explicitly reduces the variance introduced by embedding approximation. We show theoretically and empirically that the proposed method, even with smaller mini-batch sizes, enjoys a faster convergence rate and entails a better generalization compared to the existing methods.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1393–1403},
numpages = {11},
keywords = {graph neural networks, minimal variance sampling},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{graclus,
author = {Dhillon, Inderjit S. and Guan, Yuqiang and Kulis, Brian},
title = {Weighted Graph Cuts without Eigenvectors A Multilevel Approach},
year = {2007},
issue_date = {November 2007},
publisher = {IEEE Computer Society},
address = {USA},
volume = {29},
number = {11},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2007.1115},
doi = {10.1109/TPAMI.2007.1115},
abstract = {A variety of clustering algorithms have recently been proposed to handle data that is not linearly separable; spectral clustering and kernel k-means are two of the main methods. In this paper, we discuss an equivalence between the objective functions used in these seemingly different methods--in particular, a general weighted kernel k-means objective is mathematically equivalent to a weighted graph clustering objective. We exploit this equivalence to develop a fast, high-quality multilevel algorithm that directly optimizes various weighted graph clustering objectives, such as the popular ratio cut, normalized cut, and ratio association criteria. This eliminates the need for any eigenvector computation for graph clustering problems, which can be prohibitive for very large graphs. Previous multilevel graph partitioning methods, such as Metis, have suffered from the restriction of equal-sized clusters; our multilevel algorithm removes this restriction by using kernel k-means to optimize weighted graph cuts. Experimental results show that our multilevel algorithm outperforms a state-of-the-art spectral clustering algorithm in terms of speed, memory usage, and quality. We demonstrate that our algorithm is applicable to large-scale clustering tasks such as image segmentation, social network analysis and gene network analysis.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = {nov},
pages = {1944–1957},
numpages = {14},
keywords = {Spectral Clustering, Graph Partitioning, Clustering, k-means, Data Mining, Segmentation, Kernel}
}




@article{metis1,
  title={A fast and high quality multilevel scheme for partitioning irregular graphs},
  author={Karypis, George and Kumar, Vipin},
  journal={SIAM Journal on scientific Computing},
  volume={20},
  number={1},
  pages={359--392},
  year={1998},
  publisher={SIAM}
}



@inproceedings{fdgnn,
  title={Fast and deep graph neural networks},
  author={Gallicchio, Claudio and Micheli, Alessio},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={3898--3905},
  year={2020}
}

@INPROCEEDINGS{1555942,
  author={Gori, M. and Monfardini, G. and Scarselli, F.},
  booktitle={Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.}, 
  title={A new model for learning in graph domains}, 
  year={2005},
  volume={2},
  number={},
  pages={729-734 vol. 2},
  doi={10.1109/IJCNN.2005.1555942}}

@inproceedings{mdeq,
 author = {Bai, Shaojie and Koltun, Vladlen and Kolter, J. Zico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {5238--5250},
 publisher = {Curran Associates, Inc.},
 title = {Multiscale Deep Equilibrium Models},
 url = {https://proceedings.neurips.cc/paper/2020/file/3812f9a59b634c2a9c574610eaba5bed-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{deq,
 author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Equilibrium Models},
 url = {https://proceedings.neurips.cc/paper/2019/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{
asgc,
title={Simplified Graph Convolution with Heterophily},
author={Sudhanshu Chanpuriya and Cameron N Musco},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=jRrpiqxtrWm}
}

@book{banach,
  title={An introduction to metric spaces and fixed point theory},
  author={Khamsi, Mohamed A and Kirk, William A},
  volume={53},
  year={2011},
  publisher={John Wiley \& Sons}
}

@article{contraction,
  title={The graph neural network model},
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE transactions on neural networks},
  volume={20},
  number={1},
  pages={61--80},
  year={2008},
  publisher={IEEE}
}




@InProceedings{gcnii,
  title = 	 {Simple and Deep Graph Convolutional Networks},
  author =       {Chen, Ming and Wei, Zhewei and Huang, Zengfeng and Ding, Bolin and Li, Yaliang},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1725--1735},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20v/chen20v.pdf},
  url = 	 {http://proceedings.mlr.press/v119/chen20v.html},
}


@inproceedings{ignn,
 author = {Gu, Fangda and Chang, Heng and Zhu, Wenwu and Sojoudi, Somayeh and El Ghaoui, Laurent},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {11984--11995},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Graph Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/8b5c8441a8ff8e151b191c53c1842a38-Paper.pdf},
 volume = {33},
 year = {2020}
}



@InProceedings{gas,
  title = 	 {GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings},
  author =       {Fey, Matthias and Lenssen, Jan E. and Weichert, Frank and Leskovec, Jure},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3294--3304},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/fey21a/fey21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/fey21a.html},
}




@article{rph,
  title={Recurrently Predicting Hypergraphs},
  author={Zhang, David W and Burghouts, Gertjan J and Snoek, Cees GM},
  journal={arXiv preprint arXiv:2106.13919},
  year={2021}
}

@inproceedings{
cgs,
title={Convergent Graph Solvers},
author={Junyoung Park and Jinhyun Choo and Jinkyoo Park},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=ItkxLQU01lD}
}


@inproceedings{
eignn,
title={{EIGNN}: Efficient Infinite-Depth Graph Neural Networks},
author={Juncheng Liu and Kenji Kawaguchi and Bryan Hooi and Yiwei Wang and Xiaokui Xiao},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=blzTEKKRIcV}
}

@InProceedings{deq_gcn,
  title = 	 {Training Graph Neural Networks with 1000 Layers},
  author =       {Li, Guohao and M{\"u}ller, Matthias and Ghanem, Bernard and Koltun, Vladlen},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6437--6449},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/li21o/li21o.pdf},
  url = 	 {https://proceedings.mlr.press/v139/li21o.html},
}



@InProceedings{sse,
  title = 	 {Learning Steady-States of Iterative Algorithms over Graphs},
  author =       {Dai, Hanjun and Kozareva, Zornitsa and Dai, Bo and Smola, Alex and Song, Le},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1106--1114},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/dai18a/dai18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/dai18a.html},
}



@article{sbm,
  author  = {Emmanuel Abbe},
  title   = {Community Detection and Stochastic Block Models: Recent Developments},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {177},
  pages   = {1-86}
}

@article{dgl,
    title={Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks},
    author={Minjie Wang and Da Zheng and Zihao Ye and Quan Gan and Mufei Li and Xiang Song and Jinjing Zhou and Chao Ma and Lingfan Yu and Yu Gai and Tianjun Xiao and Tong He and George Karypis and Jinyang Li and Zheng Zhang},
    year={2019},
    journal={arXiv preprint arXiv:1909.01315}
}

@article{mag,
  title={Microsoft academic graph: When experts are not enough},
  author={Wang, Kuansan and Shen, Zhihong and Huang, Chiyuan and Wu, Chieh-Han and Dong, Yuxiao and Kanakia, Anshul},
  journal={Quantitative Science Studies},
  volume={1},
  number={1},
  pages={396--413},
  year={2020},
  publisher={MIT Press}
}

@article{virtual_node,
  Title={Learning Graph-Level Representation for Drug Discoveryk},
  Journal={arXiv preprint arXiv:1709.03741},
  Author={Junying Li, Deng Cai, Xiaofei He},
  Year={2017},
}

@inproceedings{hgnn,
 author = {Liu, Qi and Nickel, Maximilian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {Hyperbolic Graph Neural Networks},
 year = {2019}
}

@inproceedings{hgcn,
 author = {Chami, Ines and Ying, Zhitao and R\'{e}, Christopher and Leskovec, Jure},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Hyperbolic Graph Convolutional Neural Networks},
 year = {2019}
}
@article{ugnn,
  author    = {Yongyi Yang and
               Yangkun Wang and
               Zengfeng Huang and
               David Wipf},
  title     = {Implicit vs Unfolded Graph Neural Networks},
  journal   = {CoRR},
  volume    = {abs/2111.06592},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.06592},
  eprinttype = {arXiv},
  eprint    = {2111.06592},
  timestamp = {Tue, 16 Nov 2021 12:12:31 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-06592.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ggnn,
  author    = {Yujia Li and
               Daniel Tarlow and
               Marc Brockschmidt and
               Richard S. Zemel},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Gated Graph Sequence Neural Networks},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.05493},
  timestamp = {Thu, 25 Jul 2019 14:25:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LiTBZ15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{dlg,
title={Deep Learning on Graphs},
author={Yao Ma and Jiliang Tang},
publisher={Cambridge University Press},
year={2021}
}


@inproceedings{monet,
  title={Geometric deep learning on graphs and manifolds using mixture model cnns},
  author={Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodola, Emanuele and Svoboda, Jan and Bronstein, Michael M},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5115--5124},
  year={2017}
}

@inproceedings{c&s,
title={Combining Label Propagation and Simple Models out-performs Graph Neural Networks},
author={Qian Huang and Horace He and Abhay Singh and Ser-Nam Lim and Austin Benson},
booktitle={International Conference on Learning Representations},
year={2021},
}

@inproceedings{lrp,
 author = {Chen, Zhengdao and Chen, Lei and Villar, Soledad and Bruna, Joan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {10383--10395},
 title = {Can Graph Neural Networks Count Substructures?},
 year = {2020}
}

@article{cora,
  title={Automating the construction of internet portals with machine learning},
  author={McCallum, Andrew Kachites and Nigam, Kamal and Rennie, Jason and Seymore, Kristie},
  journal={Information Retrieval},
  volume={3},
  number={2},
  pages={127--163},
  year={2000},
  publisher={Springer}
}

@inproceedings{graphsage,
 author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Inductive Representation Learning on Large Graphs},
 pages = {1025–1035},
 year = {2017}
}

@inproceedings{eigenpool,
author = {Ma, Yao and Wang, Suhang and Aggarwal, Charu C. and Tang, Jiliang},
title = {Graph Convolutional Networks with EigenPooling},
year = {2019},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {723–731},
}

@inproceedings{
graphsaint,
title={GraphSAINT: Graph Sampling Based Inductive Learning Method},
author={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJe8pkHFwS}
}

@inproceedings{
shadow_gnn,
title={Decoupling the Depth and Scope of Graph Neural Networks},
author={Hanqing Zeng and Muhan Zhang and Yinglong Xia and Ajitesh Srivastava and Andrey Malevich and Rajgopal Kannan and Viktor Prasanna and Long Jin and Ren Chen},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=_IY3_4psXuf}
}

@inproceedings{cluster_gcn,
  title={Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks},
  author={Chiang, Wei-Lin and Liu, Xuanqing and Si, Si and Li, Yang and Bengio, Samy and Hsieh, Cho-Jui},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={257--266},
  year={2019}
}

@InProceedings{g-unet,
  title = 	 {Graph U-Nets},
  author =       {Gao, Hongyang and Ji, Shuiwang},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2083--2092},
  year = 	 {2019},
}


@inproceedings{diffpool,
 author = {Ying, Zhitao and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, Will and Leskovec, Jure},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Hierarchical Graph Representation Learning with Differentiable Pooling},
 pages = {4805–4815},
 year = {2018}
}



@article{tsne,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579-2605},
}

@inproceedings{ogb,
 author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {22118--22133},
 title = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},
 year = {2020}
}



@article{hecn,
  title = {Hyperbolic geometry of complex networks},
  author = {Krioukov, Dmitri and Papadopoulos, Fragkiskos and Kitsak, Maksim and Vahdat, Amin and Bogu\~n\'a, Mari\'an},
  journal = {Phys. Rev. E},
  volume = {82},
  issue = {3},
  pages = {036106},
  numpages = {18},
  year = {2010},
}


@inproceedings{gen,
  title={Graph Structure Estimation Neural Networks},
  author={Wang, Ruijia and Mou, Shuai and Wang, Xiao and Xiao, Wanpeng and Ju, Qi and Shi, Chuan and Xie, Xing},
  year={2021},
booktitle = {Proceedings of The Web Conference},
}




@inproceedings{
geom-gcn,
title={Geom-GCN: Geometric Graph Convolutional Networks},
author={Hongbin Pei and Bingzhe Wei and Kevin Chen-Chuan Chang and Yu Lei and Bo Yang},
booktitle={International Conference on Learning Representations},
year={2020},
}

@article{mlp_approx2,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
}

@article{mlp_approx1,
title = {Approximation capabilities of multilayer feedforward networks},
journal = {Neural Networks},
volume = {4},
number = {2},
pages = {251-257},
year = {1991},
issn = {0893-6080},
author = {Kurt Hornik},
}

@inproceedings{betae,
 author = {Ren, Hongyu and Leskovec, Jure},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {19716--19726},
 title = {Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs},
 year = {2020}
}

@inproceedings{query2box,
  title={Query2box: Reasoning over Knowledge Graphs in Vector Space using Box Embeddings},
  author={Ren, Hongyu and Hu, Weihua and Leskovec, Jure},
  booktitle={International Conference on Learning Representations},
  year={2020},
}

@inproceedings{kg-pe-box,
    title = "Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures",
    author = "Vilnis, Luke  and
      Li, Xiang  and
      Murty, Shikhar  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2018",
    pages = "263--272",
}

@inproceedings{g2gauss,
title={Deep Gaussian Embedding of Graphs:  Unsupervised Inductive Learning via Ranking},
author={Aleksandar Bojchevski and Stephan Günnemann},
booktitle={International Conference on Learning Representations},
year={2018},
}


@book{ig,
  title={Information geometry and its applications},
  author={Amari, Shun-ichi},
  volume={194},
  year={2016},
  publisher={Springer}
}

@book{stat_inference,
  author = {Casella, George and Berger, Roger},
  keywords = {methodology probability statistics},
  month = {June},
  publisher = {{Duxbury Resource Center}},
  title = {Statistical Inference},
  year = 2001
}


@article{gatedgcn,
  title={Residual gated graph convnets},
  author={Bresson, Xavier and Laurent, Thomas},
  journal={arXiv preprint arXiv:1711.07553},
  year={2017}
}

@article{igse,
  title={Information-geometric set embeddings (igse): From sets to probability distributions},
  author={Sun, Ke and Nielsen, Frank},
  journal={arXiv preprint arXiv:1911.12463},
  year={2019}
}

@ARTICLE{ipool,
  author={Gao, Xing and Dai, Wenrui and Li, Chenglin and Xiong, Hongkai and Frossard, Pascal},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={iPool--Information-Based Pooling in Hierarchical Graph Neural Networks}, 
  year={2021},
  volume={},
  number={},
  pages={1-13},
  }

@article{wl_test,
  title={The reduction of a graph to canonical form and the algebra which appears therein},
  author={Weisfeiler, Boris and Leman, Andrei},
  journal={NTI, Series},
  volume={2},
  number={9},
  pages={12--16},
  year={1968}
}

@inproceedings{factorgcn,
 author = {Yang, Yiding and Feng, Zunlei and Song, Mingli and Wang, Xinchao},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {20286--20296},
 title = {Factorizable Graph Convolutional Networks},
 year = {2020}
}

@inproceedings{wls,
 author = {Ok, Seongmin},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1--12},
 title = {A graph similarity for deep learning},
 year = {2020}
}

@inproceedings{pna,
 author = {Corso, Gabriele and Cavalleri, Luca and Beaini, Dominique and Li\`{o}, Pietro and Veli\v{c}kovi\'{c}, Petar},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {13260--13271},
 title = {Principal Neighbourhood Aggregation for Graph Nets},
 year = {2020}
}

@article{benchmark_gnns,
  title={Benchmarking Graph Neural Networks},
  author={Dwivedi, Vijay Prakash and Joshi, Chaitanya K and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  journal={arXiv preprint arXiv:2003.00982},
  year={2020}
}

@inproceedings{gin,
title={How Powerful are Graph Neural Networks?},
author={Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{gat,
  title="{Graph Attention Networks}",
  author={Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2018},
}




@inproceedings{gcn,
  author    = {Thomas N. Kipf and
               Max Welling},
  title     = {Semi-Supervised Classification with Graph Convolutional Networks},
  booktitle = {{ICLR} (Poster)},
  publisher = {OpenReview.net},
  year      = {2017}
}


@techreport{pagerank,
  title={The PageRank citation ranking: Bringing order to the web.},
  author={Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
  year={1999},
  institution={Stanford InfoLab}
}

@inproceedings{oversmoothing,
        title = "{Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning}",
       author = {{Li}, Q. and {Han}, Z. and {Wu}, X.-M.},
    booktitle = {The Thirty-Second AAAI Conference on Artificial Intelligence},
         year = {2018},
 organization = {AAAI},
}

@article{grl,
author={Hamilton, William L.},
title={Graph Representation Learning},
journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
volume={14},
number={3},
pages={1-159},
publisher={Morgan and Claypool},
year = 2020
}

@inproceedings{smp,
 author = {Vignac, Cl\'{e}ment and Loukas, Andreas and Frossard, Pascal},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {14143--14155},
 title = {Building powerful and equivariant graph neural networks with structural message-passing},
 year = {2020}
}


@InProceedings{mpnn,
  title = 	 {Neural Message Passing for Quantum Chemistry},
  author =       {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1263--1272},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/gilmer17a.html},
  abstract = 	 {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
}



@InProceedings{gnn_image2,
author = {Kim, Jongmin and Kim, Taesup and Kim, Sungwoong and Yoo, Chang D.},
title = {Edge-Labeling Graph Neural Network for Few-Shot Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2019}
}

@InProceedings{gnn_image1,
author = {Shen, Yantao and Li, Hongsheng and Yi, Shuai and Chen, Dapeng and Wang, Xiaogang},
title = {Person Re-identification with Deep Similarity-Guided Graph Neural Network},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
year = {2018}
}


@article{gnn_mol1,
  author    = {N. P. Moloi and
               M. M. Ali},
  title     = {An Iterative Global Optimization Algorithm for Potential Energy Minimization},
  journal   = {Comput. Optim. Appl.},
  volume    = {30},
  number    = {2},
  pages     = {119--132},
  year      = {2005}
}


@article{gnn_mol2,
author = {Kearnes, Steven and McCloskey, Kevin and Berndl, Marc and Pande, Vijay and Riley, Patrick},
year = {2016},
month = {08},
pages = {},
title = {Molecular Graph Convolutions: Moving Beyond Fingerprints},
volume = {30},
journal = {Journal of Computer-Aided Molecular Design},
}


@inproceedings{gnn_mol3,
author = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and G\'{o}mez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al\'{a}n and Adams, Ryan P.},
title = {Convolutional Networks on Graphs for Learning Molecular Fingerprints},
year = {2015},
booktitle = {Advances in Neural Information Processing Systems},
pages = {2224–2232},
}


@ARTICLE {gnn_material,
author = {J. Gostick and M. Aghighi and J. Hinebaugh and T. Tranter and M. A. Hoeh and H. Day and B. Spellacy and M. H. Sharqawy and A. Bazylak and A. Burns and W. Lehnert and A. Putz},
journal = {Computing in Science \& Engineering},
title = {OpenPNM: A Pore Network Modeling Package},
year = {2016},
volume = {18},
number = {04},
issn = {1558-366X},
pages = {60-74},
keywords = {computational modeling;mathematical model;object recognition;modeling;open source software;network topology},
doi = {10.1109/MCSE.2016.49},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jul}
}


@inproceedings{gnn_recommendation,
author = {Brin, Sergey and Page, Lawrence},
title = {The Anatomy of a Large-Scale Hypertextual Web Search Engine},
year = {1998},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
booktitle = {Proceedings of the Seventh International Conference on World Wide Web 7},
pages = {107–117},
numpages = {11},
keywords = {World Wide Web, information retrieval, Google, PageRank, search engines},
location = {Brisbane, Australia},
series = {WWW7}
}



@INPROCEEDINGS{recgnn1,
  author={Gori, M. and Monfardini, G. and Scarselli, F.},
  booktitle={Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.}, 
  title={A new model for learning in graph domains}, 
  year={2005},
  volume={2},
  number={},
  pages={729-734 vol. 2},
  doi={10.1109/IJCNN.2005.1555942}}

@article{gnn_bio,
author = {Huang, Kexin and Xiao, Cao and Glass, Lucas and Zitnik, Marinka and Sun, J.},
year = {2020},
month = {12},
pages = {},
title = {SkipGNN: predicting molecular interactions with skip-graph networks},
volume = {10},
journal = {Scientific Reports},
}

@inproceedings{gnn_recommandation2,
author = {Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
title = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219890},
doi = {10.1145/3219819.3219890},
abstract = {Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains an unsolved challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. Overall, we can train on and embed graphs that are four orders of magnitude larger than typical GCN implementations. We show how GCN embeddings can be used to make high-quality recommendations in various settings at Pinterest, which has a massive underlying graph with 3 billion nodes representing pins and boards, and 17 billion edges. According to offline metrics, user studies, as well as A/B tests, our approach generates higher-quality recommendations than comparable deep learning based systems. To our knowledge, this is by far the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {974–983},
numpages = {10},
keywords = {scalability, graph convolutional networks, recommender systems, deep learning},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{gnn_reaction2,
 author = {Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alan and Adams, Ryan P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Convolutional Networks on Graphs for Learning Molecular Fingerprints},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf},
 volume = {28},
 year = {2015}
}

@inproceedings{gnn_reaction1,
  author       = {Kien Do and
                  Truyen Tran and
                  Svetha Venkatesh},
  editor       = {Ankur Teredesai and
                  Vipin Kumar and
                  Ying Li and
                  R{\'{o}}mer Rosales and
                  Evimaria Terzi and
                  George Karypis},
  title        = {Graph Transformation Policy Network for Chemical Reaction Prediction},
  booktitle    = {Proceedings of the 25th {ACM} {SIGKDD} International Conference on
                  Knowledge Discovery {\&} Data Mining, {KDD} 2019, Anchorage, AK,
                  USA, August 4-8, 2019},
  pages        = {750--760},
  publisher    = {{ACM}},
  year         = {2019},
  url          = {https://doi.org/10.1145/3292500.3330958},
  doi          = {10.1145/3292500.3330958},
  timestamp    = {Tue, 16 Aug 2022 23:04:27 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/Do0V19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{social2,
  title={Learning Dynamic Context Graphs for Predicting Social Events},
  author={Deng, Songgaojun and Rangwala, Huzefa and Ning, Yue},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1007--1016},
  year={2019}
}


@inproceedings{gnn_social,
author = {Fan, Wenqi and Ma, Yao and Li, Qing and He, Yuan and Zhao, Eric and Tang, Jiliang and Yin, Dawei},
title = {Graph Neural Networks for Social Recommendation},
year = {2019},
booktitle = {The World Wide Web Conference},
pages = {417–426},
series = {WWW '19}
}

@inproceedings{lsh,
author = {Indyk, Piotr and Motwani, Rajeev},
title = {Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality},
year = {1998},
booktitle = {Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing},
pages = {604–613},
}

@inproceedings{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
}

@article{lemma_sym,
author = {Seeger, Alberto and Torki, Mounir},
year = {2018},
pages = {983-1011},
title = {Measuring axial symmetry in convex cones},
volume = {25},
journal = {Journal of Convex Analysis}
}

@inproceedings{murp,
 author = {Balazevic, Ivana and Allen, Carl and Hospedales, Timothy},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {4463--4473},
 title = {Multi-relational Poincar\'{e} Graph Embeddings},
 volume = {32},
 year = {2019}
}



@inproceedings{quate,
 author = {Zhang, Shuai and Tay, Yi and Yao, Lina and Liu, Qi},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2735--2745},
 title = {Quaternion Knowledge Graph Embeddings},
 year = {2019}
}


@inproceedings{card_sigmod,
author = {Ting, Daniel},
title = {Approximate Distinct Counts for Billions of Datasets},
year = {2019},
pages = {69–86},
numpages = {18},
}

@inproceedings{kdd_card2,
author = {Cohen, Reuven and Katzir, Liran and Yehezkel, Aviv},
title = {A Minimal Variance Estimator for the Cardinality of Big Data Set Intersection},
year = {2017},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {95–103},
numpages = {9},
}

@inproceedings{kdd_card,
author = {Ting, Daniel},
title = {Towards Optimal Cardinality Estimation of Unions and Intersections with Sketches},
year = {2016},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1195–1204},
numpages = {10},
}

@inproceedings{quantum,
 author = {Garg, Dinesh and Ikbal, Shajith and Srivastava, Santosh K. and Vishwakarma, Harit and Karanam, Hima and Subramaniam, L Venkata},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Quantum Embedding of Knowledge for Reasoning},
 year = {2019}
}



@inproceedings{emb_kgqa,
    title = "Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings",
    author = "Saxena, Apoorv  and
      Tripathi, Aditay  and
      Talukdar, Partha",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    year = "2020",
    pages = "4498--4507",
}

@inproceedings{pullnet,
    title = "{P}ull{N}et: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text",
    author = "Sun, Haitian  and
      Bedrax-Weiss, Tania  and
      Cohen, William",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
    pages = "2380--2390",
}

@inproceedings{alime,
author = {Li, Feng-Lin and Qiu, Minghui and Chen, Haiqing and Wang, Xiongwei and Gao, Xing and Huang, Jun and Ren, Juwei and Zhao, Zhongzhou and Zhao, Weipeng and Wang, Lei and Jin, Guwei and Chu, Wei},
title = {AliMe Assist: An Intelligent Assistant for Creating an Innovative E-Commerce Experience},
year = {2017},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2495–2498},
numpages = {4},
}

@inproceedings{chatbot,
    title = "Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots",
    author = "Yuan, Chunyuan  and
      Zhou, Wei  and
      Li, Mingming  and
      Lv, Shangwen  and
      Zhu, Fuqing  and
      Han, Jizhong  and
      Hu, Songlin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    pages = "111--120",
}

@inproceedings{subg_emb,
    title = "Question Answering with Subgraph Embeddings",
    author = "Bordes, Antoine  and
      Chopra, Sumit  and
      Weston, Jason",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    pages = "615--620",
}

@inproceedings{sp_qa,
    title = "Semantic Parsing on {F}reebase from Question-Answer Pairs",
    author = "Berant, Jonathan  and
      Chou, Andrew  and
      Frostig, Roy  and
      Liang, Percy",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    pages = "1533--1544",
}

@inproceedings{xiong2017explicit,
  title={Explicit semantic ranking for academic search via knowledge graph embedding},
  author={Xiong, Chenyan and Power, Russell and Callan, Jamie},
  booktitle={Proceedings of the 26th international conference on world wide web},
  pages={1271--1279},
  year={2017}
}

@inproceedings{yago,
  author = {Suchanek, Fabian M. and Kasneci, Gjergji and Weikum, Gerhard},
  booktitle = {Proceedings of the 16th International Conference on World Wide Web},
  numpages = {10},
  pages = {697--706},
  publisher = {ACM},
  series = {WWW '07},
  title = {Yago: A Core of Semantic Knowledge},
  year = 2007
}




@inproceedings{faith,
  title={Faithful Embeddings for Knowledge Base Queries},
  author={Haitian Sun and Andrew O. Arnold and Tania Bedrax-Weiss and F. Pereira and William W. Cohen},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@book{dnf,
  title={Introduction to lattices and order},
  author={Davey, Brian A and Priestley, Hilary A},
  year={2002},
  publisher={Cambridge university press}
}

@book{boyd,
author = {Boyd, Stephen and Vandenberghe, Lieven},
title = {Convex Optimization},
year = {2004},
isbn = {0521833787},
publisher = {Cambridge University Press},
address = {USA}
}


@InProceedings{ent_cone,
  title = 	 {Hyperbolic Entailment Cones for Learning Hierarchical Embeddings},
  author =       {Ganea, Octavian and Becigneul, Gary and Hofmann, Thomas},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1646--1655},
  year = 	 {2018},
  volume = 	 {80},
}



@inproceedings{imbox,
  author    = {Shib Sankar Dasgupta and
               Michael Boratko and
               Dongxu Zhang and
               Luke Vilnis and
               Xiang Li and
               Andrew McCallum},
  title     = {Improving Local Identifiability in Probabilistic Box Embeddings},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
}

@inproceedings{chains,
    title = "Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks",
    author = "Das, Rajarshi  and
      Neelakantan, Arvind  and
      Belanger, David  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    pages = "132--141",
}

@inproceedings{variational,
  title={Variational reasoning for question answering with knowledge graph},
  author={Zhang, Yuyu and Dai, Hanjun and Kozareva, Zornitsa and Smola, Alexander J and Song, Le},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{ptranse,
    title = "Modeling Relation Paths for Representation Learning of Knowledge Bases",
    author = "Lin, Yankai  and
      Liu, Zhiyuan  and
      Luan, Huanbo  and
      Sun, Maosong  and
      Rao, Siwei  and
      Liu, Song",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    pages = "705--714",
}


@inproceedings{dura,
  title={Duality-Induced Regularizer for Tensor Factorization Based Knowledge Graph Completion},
  author={Zhang, Zhanqiu and Cai, Jianyu and Wang, Jie},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{deepsets,
 author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3391--3401},
 title = {Deep Sets},
 volume = {30},
 year = {2017}
}



@inproceedings{ruge,
title = {Knowledge Graph Embedding with Iterative Guidance from Soft Rules},
author = {Shu Guo and Quan Wang and Lihong Wang and Bin Wang and Li Guo},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence},
year = {2018}
}

@inproceedings{rl,
    title = "Multi-Hop Knowledge Graph Reasoning with Reward Shaping",
    author = "Lin, Xi Victoria  and
      Socher, Richard  and
      Xiong, Caiming",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    year = "2018",
    pages = "3243--3253",
}

@inproceedings{deeppath,
    title = "{D}eep{P}ath: A Reinforcement Learning Method for Knowledge Graph Reasoning",
    author = "Xiong, Wenhan  and
      Hoang, Thien  and
      Wang, William Yang",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    year = "2017",
    pages = "564--573",
}

@inproceedings{beta,
 title={Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs},
 author={Hongyu Ren and Jure Leskovec},
 booktitle={Neural Information Processing Systems},
 year={2020}
}

@inproceedings{traversing,
    title = "Traversing Knowledge Graphs in Vector Space",
    author = "Guu, Kelvin  and
      Miller, John  and
      Liang, Percy",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/D15-1038",
    pages = "318--327",
}

@inproceedings{q2b,
  title={Query2box: Reasoning over Knowledge Graphs in Vector Space using Box Embeddings},
  author={Ren, Hongyu and Hu, Weihua and Leskovec, Jure},
  booktitle={International Conference on Learning Representations},
  year={2020},
}

@inproceedings{gqe,
author = {Hamilton, William L. and Bajaj, Payal and Zitnik, Marinka and Jurafsky, Dan and Leskovec, Jure},
title = {Embedding Logical Queries on Knowledge Graphs},
year = {2018},
abstract = {Learning low-dimensional embeddings of knowledge graphs is a powerful approach used to predict unobserved or missing edges between entities. However, an open challenge in this area is developing techniques that can go beyond simple edge prediction and handle more complex logical queries, which might involve multiple unobserved edges, entities, and variables. For instance, given an incomplete biological knowledge graph, we might want to predict what drugs are likely to target proteins involved with both diseases X and Y?—a query that requires reasoning about all possible proteins that might interact with diseases X and Y. Here we introduce a framework to efficiently make predictions about conjunctive logical queries—a flexible but tractable subset of first-order logic—on incomplete knowledge graphs. In our approach, we embed graph nodes in a low-dimensional space and represent logical operators as learned geometric operations (e.g., translation, rotation) in this embedding space. By performing logical operations within a low-dimensional embedding space, our approach achieves a time complexity that is linear in the number of query variables, compared to the exponential complexity required by a naive enumeration-based approach. We demonstrate the utility of this framework in two application studies on real-world datasets with millions of relations: predicting logical relationships in a network of drug-gene-disease interactions and in a graph-based representation of social interactions derived from a popular web forum.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {2030–2041},
numpages = {12},
}



@inproceedings{tucker,
    title = "{T}uck{ER}: Tensor Factorization for Knowledge Graph Completion",
    author = "Balazevic, Ivana  and
      Allen, Carl  and
      Hospedales, Timothy",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1522",
    doi = "10.18653/v1/D19-1522",
    pages = "5185--5194",
    abstract = "Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively straightforward but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms previous state-of-the-art models across standard link prediction datasets, acting as a strong baseline for more elaborate models. We show that TuckER is a fully expressive model, derive sufficient bounds on its embedding dimensionalities and demonstrate that several previously introduced linear models can be viewed as special cases of TuckER.",
}

@inproceedings{sparse,
  title={Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding},
  author={Song Han and Huizi Mao and William J. Dally},
  booktitle={International Conference on Learning Representations},
  year={2015},
}

@InProceedings{bg_reg,
    author="Minervini, Pasquale
    and Costabello, Luca
    and Mu{\~{n}}oz, Emir
    and Nov{\'a}{\v{c}}ek, V{\'i}t
    and Vandenbussche, Pierre-Yves",
    editor="Ceci, Michelangelo
    and Hollm{\'e}n, Jaakko
    and Todorovski, Ljup{\v{c}}o
    and Vens, Celine
    and D{\v{z}}eroski, Sa{\v{s}}o",
    title="Regularizing Knowledge Graph Embeddings via Equivalence and Inversion Axioms",
    booktitle="Machine Learning and Knowledge Discovery in Databases",
    year="2017",
    publisher="Springer International Publishing",
    pages="668--683",
}

@inproceedings{simp_cons,
    title = "Improving Knowledge Graph Embedding Using Simple Constraints",
    author = "Ding, Boyang  and
      Wang, Quan  and
      Wang, Bin  and
      Guo, Li",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2018",
    pages = "110--121",
}

@article{kg_cv,
  title={The More You Know: Using Knowledge Graphs for Image Classification},
  author={Kenneth Marino and Ruslan Salakhutdinov and Abhinav Gupta},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={20-28}
}

@Article{trace_norm2,
    author="Cand{\`e}s, Emmanuel J.
    and Recht, Benjamin",
    title="Exact Matrix Completion via Convex Optimization",
    journal="Foundations of Computational Mathematics",
    year="2009",
    month="Apr",
    day="03",
    volume="9",
    number="6",
    pages="717",
}



@incollection{trace_norm,
    title = {Maximum-Margin Matrix Factorization},
    author = {Nathan Srebro and Jason Rennie and Tommi S. Jaakkola},
    booktitle = {Advances in Neural Information Processing Systems 17},
    editor = {L. K. Saul and Y. Weiss and L. Bottou},
    pages = {1329--1336},
    year = {2005},
    publisher = {MIT Press},
}


@inproceedings{old_dog,
    title={You {\{}CAN{\}} Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings},
    author={Daniel Ruffinelli and Samuel Broscheit and Rainer Gemulla},
    booktitle={International Conference on Learning Representations},
    year={2020},
}

@incollection{simple,
    title = {SimplE Embedding for Link Prediction in Knowledge Graphs},
    author = {Kazemi, Seyed Mehran and Poole, David},
    booktitle = {Advances in Neural Information Processing Systems 31},
    pages = {4284--4295},
    year = {2018},
    publisher = {Curran Associates, Inc.},
}


@incollection{weighted_reg,
    title = {Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm},
    author = {Nathan Srebro and Salakhutdinov, Russ R},
    booktitle = {Advances in Neural Information Processing Systems 23},
    pages = {2056--2064},
    year = {2010},
    publisher = {Curran Associates, Inc.},
}


@article{uni-form,
  title={A Relational Tucker Decomposition for Multi-Relational Link Prediction},
  author={Wang, Yanjie and Broscheit, Samuel and Gemulla, Rainer},
  journal={arXiv preprint arXiv:1902.00898},
  year={2019}
}

@article{adagrad,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}

@article{cp,
  title={The expression of a tensor or a polyadic as a sum of products},
  author={Hitchcock, Frank L},
  journal={Journal of Mathematics and Physics},
  volume={6},
  number={1-4},
  pages={164--189},
  year={1927},
  publisher={Wiley Online Library}
}

@article{nunorm,
  title={Nuclear norm of higher-order tensors},
  author={Friedland, Shmuel and Lim, Lek-Heng},
  journal={Mathematics of Computation},
  volume={87},
  number={311},
  pages={1255--1281},
  year={2018}
}

@inproceedings{se,
    author = {Bordes, Antoine and Weston, Jason and Collobert, Ronan and Bengio, Yoshua},
    title = {Learning Structured Embeddings of Knowledge Bases},
    year = {2011},
    publisher = {AAAI Press},
    booktitle = {Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence},
    pages = {301–306},
    numpages = {6},

}

@InProceedings{n3,
  title = 	 {Canonical Tensor Decomposition for Knowledge Base Completion},
  author = 	 {Lacroix, Timothee and Usunier, Nicolas and Obozinski, Guillaume},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2863--2872},
  year = 	 {2018},
  publisher = 	 {PMLR},
}

@inproceedings{hake,
  title={Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction},
  author={Zhang, Zhanqiu and Cai, Jianyu and Zhang, Yongdong and Wang, Jie},
  booktitle = {Proceedings of the Thirty-Fourth {AAAI} Conference on Artificial Intelligence},
  year={2020},

}

@inproceedings{ernie,
    title = "{ERNIE}: Enhanced Language Representation with Informative Entities",
    author = "Zhang, Zhengyan  and
      Han, Xu  and
      Liu, Zhiyuan  and
      Jiang, Xin  and
      Sun, Maosong  and
      Liu, Qun",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    pages = "1441--1451",
}

@inproceedings{glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}


@inproceedings{word2vec,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
 title = {Distributed Representations of Words and Phrases and Their Compositionality},
 booktitle = {Advances in Neural Information Processing Systems 26},
 year = {2013},

} 


@inproceedings{KGRS,
     author = {Wang, Hongwei and Zhang, Fuzheng and Wang, Jialin and Zhao, Miao and Li, Wenjie and Xie, Xing and Guo, Minyi},
     title = {RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems},
     year = {2018},
     publisher = {Association for Computing Machinery},
     booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
     pages = {417–426},
}


@InProceedings{KGCV,
    author = {Lee, Chung-Wei and Fang, Wei and Yeh, Chih-Kuan and Frank Wang, Yu-Chiang},
    title = {Multi-Label Zero-Shot Learning With Structured Knowledge Graphs},
    booktitle = {CVPR},
    year = {2018}
}

@inproceedings{KGQA,
 author = {Huang, Xiao and Zhang, Jingyuan and Li, Dingcheng and Li, Ping},
 title = {Knowledge Graph Embedding Based Question Answering},
 year = {2019},
 publisher = {Association for Computing Machinery},
 booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
 pages = {105–113},
}

@inproceedings{pyg,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}


@inproceedings{RSN,
    Author = {Lingbing Guo and Zequn Sun and Wei Hu},
    Title = {Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs},
    Booktitle = {ICML},
    Year = {2019}
}

@article{KGE_survey,
  title={Knowledge graph embedding: A survey of approaches and applications},
  author={Wang, Quan and Mao, Zhendong and Wang, Bin and Guo, Li},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={29},
  number={12},
  pages={2724--2743},
  year={2017},
  publisher={IEEE}
}

@InProceedings{capse,
    author={Dai Quoc Nguyen and Thanh Vu and Tu Dinh Nguyen and Dat Quoc Nguyen and Dinh Phung},
    title={{A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization}},
    booktitle={NAACL},
    year={2019}
}

@inproceedings{transg,
    author = {Xiao, Han  and
      Huang, Minlie  and
      Zhu, Xiaoyan},
    title = {{T}rans{G} : A Generative Model for Knowledge Graph Embedding},
    booktitle = {ACL},
    year = {2016}
}

  
@inproceedings{transd,
    title = "Knowledge Graph Embedding via Dynamic Mapping Matrix",
    author = "Ji, Guoliang  and
      He, Shizhu  and
      Xu, Liheng  and
      Liu, Kang  and
      Zhao, Jun",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    year = "2015",
    publisher = "Association for Computational Linguistics",
    pages = "687--696",
}
      
@inproceedings{transr,
    author = {Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
    title = {Learning Entity and Relation Embeddings for Knowledge Graph Completion},
    year = {2015},
    publisher = {AAAI Press},
    booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
    pages = {2181–2187},
} 

@inproceedings{transe,
    title = {Translating Embeddings for Modeling Multi-relational Data},
    author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
    booktitle = {Advances in Neural Information Processing Systems 26},
    pages = {2787--2795},
    year = {2013},
} 


@inproceedings{transh,
     author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
     title = {Knowledge Graph Embedding by Translating on Hyperplanes},
     year = {2014},
     publisher = {AAAI Press},
     booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
     pages = {1112–1119},
}

@inproceedings{manifolde,
    author = {Xiao, Han and Huang, Minlie and Zhu, Xiaoyan},
    title = {From One Point to a Manifold: Knowledge Graph Embedding for Precise Link Prediction},
    booktitle = {IJCAI},
    year = {2016}
} 

@inproceedings{rgcn,
    author={Schlichtkrull, Michael and Kipf, Thomas N and Bloem, Peter and Van Den Berg, Rianne and Titov, Ivan and Welling, Max},
    title={Modeling relational data with graph convolutional networks},
    booktitle={ESWC},
    year={2018}
}

@inproceedings{kg2e,
author = {He, Shizhu and Liu, Kang and Ji, Guoliang and Zhao, Jun},
title = {Learning to Represent Knowledge Graphs with Gaussian Embedding},
year = {2015},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {623–632},
}

@inproceedings{hole,
    author = {Nickel, Maximilian and Rosasco, Lorenzo and Poggio, Tomaso},
    title = {Holographic Embeddings of Knowledge Graphs},
    booktitle = {AAAI},
    year = {2016}
} 


@inproceedings{mlp,
    author = {Dong, Xin and Gabrilovich, Evgeniy and Heitz, Geremy and Horn, Wilko and Lao, Ni and Murphy, Kevin and Strohmann, Thomas and Sun, Shaohua and Zhang, Wei},
    title = {Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion},
    booktitle = {SIGKDD},
    year = {2014}
} 


@inproceedings{ntn,
 author = {Socher, Richard and Chen, Danqi and Manning, Christopher D. and Ng, Andrew Y.},
 title = {Reasoning with Neural Tensor Networks for Knowledge Base Completion},
 booktitle = {NeurIPS},
 year = {2013},
} 





@inproceedings{complex,
  title = 	 {Complex Embeddings for Simple Link Prediction},
  author = 	 {Théo Trouillon and Johannes Welbl and Sebastian Riedel and Eric Gaussier and Guillaume Bouchard},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2071--2080},
  year = 	 {2016},
  volume = 	 {48},
} 


@InProceedings{analogy,
  title = 	 {Analogical Inference for Multi-relational Embeddings},
  author = 	 {Hanxiao Liu and Yuexin Wu and Yiming Yang},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2168--2178},
  year = 	 {2017},
  volume = 	 {70},
}

@InProceedings{distmult,
    author = {Yang, Bishan and Yih, Scott Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
    title = {Embedding Entities and Relations for Learning and Inference in Knowledge Bases},
    booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
    year = {2015},
}

@inproceedings{rescal,
    author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
    title = {A Three-way Model for Collective Learning on Multi-relational Data},
    booktitle = {Proceedings of the 28th International Conference on Machine Learning},
    year = {2011},
    pages = 	 {809--816},
    volume = 	 {11},
} 

@article{rmsprop,
  title={Generating sequences with recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1308.0850},
  year={2013}
}

@InProceedings{adam,
    author = {Kingma, Diederick P and Ba, Jimmy},
    title = {Adam: A method for stochastic optimization},
    booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
    year = {2015}
}

@inproceedings{freebase,
    author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
    title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
    year = {2008},
    booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
    pages = {1247–1250},
} 


@article{wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM}
} 


@inproceedings{yago3,
author    = {Farzaneh Mahdisoltani and
               Joanna Biega and
               Fabian M. Suchanek},
  title     = {{YAGO3:} {A} Knowledge Base from Multilingual Wikipedias},
  booktitle = {Seventh Biennial Conference on Innovative Data Systems
               Research},
  year      = {2015},
}

@inproceedings{fb237,
    author = {Toutanova, Kristina and Chen, Danqi},
    title = {Observed Versus Latent Features for Knowledge Base and Text Inference},
    booktitle = {3rd Workshop on Continuous Vector Space Models and Their Compositionality},
    year = {2015},
}

@inproceedings{rotate,
    title={RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space},
    author={Zhiqing Sun and Zhi-Hong Deng and Jian-Yun Nie and Jian Tang},
    booktitle={International Conference on Learning Representations},
    year={2019},
}

@inproceedings{conve,
    author = {Tim Dettmers and Pasquale Minervini and Pontus Stenetorp and Sebastian Riedel},
	title = {Convolutional 2D Knowledge Graph Embeddings},
	Booktitle = {Proceedings of the 32th AAAI Conference on Artificial Intelligence},
	year = {2018},
	pages  = {1811--1818},
}

@inproceedings{convkb,
    author = "Nguyen, Dai Quoc  and
      Nguyen, Tu Dinh  and
      Nguyen, Dat Quoc  and
      Phung, Dinh",
    title = "A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network",
    booktitle = "NAACL",
    year = "2018"
}


@book{srl,
  title={Introduction to statistical relational learning},
  author={Getoor, Lise and Taskar, Ben},
  year={2007},
  publisher={MIT press}
}

@inproceedings{tact,
  title={Topology-aware correlations between relations for inductive link prediction in knowledge graphs},
  author={Chen, Jiajun and He, Huarui and Wu, Feng and Wang, Jie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={7},
  pages={6271--6278},
  year={2021}
}