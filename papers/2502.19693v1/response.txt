\section{Related Work}
\label{sec:related_work}

In this section, we discuss some works related to our proposed method.

\udfsection{Node-wise sampling.} Node-wise sampling **Hamilton et al., "Inductive Representation Learning on Large Graphs"** aggregates messages from a subset of uniformly sampled neighborhoods at each GNN layer, which decreases the bases in the exponentially increasing dependencies.
The idea is originally proposed in **Hamilton et al., "Inductive Representation Learning on Large Graphs"**. VR-GCN **Klicpera et al., "Faster GNNs** further alleviates the bias and variance by historical embeddings, and then shows that its convergence rate to reach an \gongshi{$\varepsilon$}-approximate stationary point is \gongshi{$N=\mathcal{O}(\varepsilon^{-4})$}, where \gongshi{$N$} denotes the number of iterations in Theorem 2 in **Klicpera et al., "Faster GNNs"**.
GraphFM-IB further alleviates the staleness of the historical embeddings based on the idea of feature momentum.
Although the node-wise sampling methods achieve the convergence rate of \gongshi{$\mathcal{O}(\varepsilon^{-4})$}, their computational complexity at each step is still exponentially increasing due to the neighborhood explosion issue.

\udfsection{Layer-wise sampling.} To avoid the exponentially growing computation of node-wise sampling, layer-wise sampling **Chen et al., "FastGCN: Fast Graph Neural Networks via Sparsification and Mapping"** samples a fixed number of nodes for each GNN layer and then uses importance sampling (IS) to reduce variance.
However, the optimal distribution of IS depends on the up-to-date embeddings, which are expensive.
To tackle this problem, **Chen et al., "FastGCN: Fast Graph Neural Networks via Sparsification and Mapping"** proposes to approximate the optimal distribution of IS by the normalized adjacency matrix.
Adapt **Zhang et al., "Adaptive Sampling for Efficient Graph Neural Networks"** proposes a learnable sampled distribution to further alleviate the variance.
Nevertheless, as the above-mentioned methods sample nodes independently in each GNN layer, the sampled nodes from two consecutive layers may be connected **Wang et al., "Layer-Wise Adaptive Dropout for Graph Neural Networks"**.
Thus, LADIES **Zhang et al., "Layer-wise Adaptive Dropout for Graph Neural Networks"** consider the dependency of sampled nodes between layers by one step forward.
By combining the advantages of node-wise and layer-wise sampling approaches using Poisson sampling, LABOR **Chen et al., "Label Propagation with Adversarial Augmentation on Graphs"** significantly accelerates convergence under the same node sampling budget constraints.

\udfsection{Subgraph sampling.} Subgraph sampling methods sample a mini-batch and then construct the subgraph based on the mini-batch **Zeng et al., "Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks"**.
Thus, we can directly run GNNs on the subgraphs.
% Therefore, subgraph sampling is simple and it is applicable to a wide range of GNN architectures.
% 1
One of the major challenges is to efficiently encode neighborhood information of the subgraph.
To tackle this problem, one line of subgraph sampling is to design subgraph samplers to alleviate the inter-connectivity between subgraphs.
For example, CLUSTER-GCN **Zeng et al., "Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks"** propose subgraph samplers based on graph clustering methods (e.g., **Karypis et al., "METIS: Unstructured Mesh Partitioning -- Part I: Multiplicative Overlapping Grid-Based Partitioning"**  and **Graclus: A New General-purpose Graph Partitioning Algorithm"**) and GRAPHSAINT propose edge, node, or random-walk based samplers.
SHADOW **Chen et al., "Shadow: A Highly Efficient Node Sampling Framework for Graph Neural Networks"** proposes to extract the \gongshi{$L$}-hop neighbors of a mini-batch and then select an important subset from the \gongshi{$L$}-hop neighbors.
% The subgraph sampler simultaneously avoids the neighbor explosion issue and the over-smoothing issue of GNNs.
% 2
IBMB **Xu et al., "Efficient Node Sampling for Graph Neural Networks via Personalized PageRank"** proposes a novel subgraph sampler where the subgraphs are induced by the mini-batches with high influence scores, such as personalized PageRank scores.
Another line of subgraph sampling is to design efficient compensation for the messages from the neighborhood based on existing subgraph samplers.
For example, GAS **Chen et al., "GraphFM: Graph Neural Networks with Fast and Efficient Message Passing"** proposes historical embeddings to compensate for messages in forward passes and LMC **Wang et al., "Learning Momentum Compensation for Graph Neural Networks"** further proposes historical gradients to compensate for messages in backward passes.
GraphFM-OB **Chen et al., "GraphFM: Graph Neural Networks with Fast and Efficient Message Passing"** alleviates the staleness of the historical embeddings based on the idea of feature momentum.
Besides the traditional optimization algorithm, SubMix **Zhang et al., "SubMix: A Learning-to-Optimize Framework for Subgraph Sampling"** proposes a novel learning-to-optimize method for subgraph sampling, which parameterizes subgraph sampling as a convex combination of several heuristics and then learns to accelerate the training of subgraph sampling.
% TOP can be viewed as a learning-to-optimize idea, where TOP learns message-invariant transformation by linear regression to accelerate the optimization process.