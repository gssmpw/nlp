@inproceedings{adapt,
 author = {Huang, Wenbing and Zhang, Tong and Rong, Yu and Huang, Junzhou},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Sampling Towards Fast Graph Representation Learning},
 url = {https://proceedings.neurips.cc/paper/2018/file/01eee509ee2f68dc6014898c309e86bf-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{cluster_gcn,
  title={Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks},
  author={Chiang, Wei-Lin and Liu, Xuanqing and Si, Si and Li, Yang and Bengio, Samy and Hsieh, Cho-Jui},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={257--266},
  year={2019}
}

@book{dlg,
title={Deep Learning on Graphs},
author={Yao Ma and Jiliang Tang},
publisher={Cambridge University Press},
year={2021}
}

@InProceedings{gas,
  title = 	 {GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings},
  author =       {Fey, Matthias and Lenssen, Jan E. and Weichert, Frank and Leskovec, Jure},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3294--3304},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/fey21a/fey21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/fey21a.html},
}

@article{graclus,
author = {Dhillon, Inderjit S. and Guan, Yuqiang and Kulis, Brian},
title = {Weighted Graph Cuts without Eigenvectors A Multilevel Approach},
year = {2007},
issue_date = {November 2007},
publisher = {IEEE Computer Society},
address = {USA},
volume = {29},
number = {11},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2007.1115},
doi = {10.1109/TPAMI.2007.1115},
abstract = {A variety of clustering algorithms have recently been proposed to handle data that is not linearly separable; spectral clustering and kernel k-means are two of the main methods. In this paper, we discuss an equivalence between the objective functions used in these seemingly different methods--in particular, a general weighted kernel k-means objective is mathematically equivalent to a weighted graph clustering objective. We exploit this equivalence to develop a fast, high-quality multilevel algorithm that directly optimizes various weighted graph clustering objectives, such as the popular ratio cut, normalized cut, and ratio association criteria. This eliminates the need for any eigenvector computation for graph clustering problems, which can be prohibitive for very large graphs. Previous multilevel graph partitioning methods, such as Metis, have suffered from the restriction of equal-sized clusters; our multilevel algorithm removes this restriction by using kernel k-means to optimize weighted graph cuts. Experimental results show that our multilevel algorithm outperforms a state-of-the-art spectral clustering algorithm in terms of speed, memory usage, and quality. We demonstrate that our algorithm is applicable to large-scale clustering tasks such as image segmentation, social network analysis and gene network analysis.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = {nov},
pages = {1944–1957},
numpages = {14},
keywords = {Spectral Clustering, Graph Partitioning, Clustering, k-means, Data Mining, Segmentation, Kernel}
}

@InProceedings{graphfm,
  title = 	 {{G}raph{FM}: Improving Large-Scale {GNN} Training via Feature Momentum},
  author =       {Yu, Haiyang and Wang, Limei and Wang, Bokun and Liu, Meng and Yang, Tianbao and Ji, Shuiwang},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {25684--25701},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/yu22g/yu22g.pdf},
  url = 	 {https://proceedings.mlr.press/v162/yu22g.html},
  abstract = 	 {Training of graph neural networks (GNNs) for large-scale node classification is challenging. A key difficulty lies in obtaining accurate hidden node representations while avoiding the neighborhood explosion problem. Here, we propose a new technique, named feature momentum (FM), that uses a momentum step to incorporate historical embeddings when updating feature representations. We develop two specific algorithms, known as GraphFM-IB and GraphFM-OB, that consider in-batch and out-of-batch data, respectively. GraphFM-IB applies FM to in-batch sampled data, while GraphFM-OB applies FM to out-of-batch data that are 1-hop neighborhood of in-batch data. We provide a convergence analysis for GraphFM-IB and some theoretical insight for GraphFM-OB. Empirically, we observe that GraphFM-IB can effectively alleviate the neighborhood explosion problem of existing methods. In addition, GraphFM-OB achieves promising performance on multiple large-scale graph datasets.}
}

@inproceedings{graphsage,
 author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Inductive Representation Learning on Large Graphs},
 pages = {1025–1035},
 year = {2017}
}

@inproceedings{ladies,
 author = {Zou, Difan and Hu, Ziniu and Wang, Yewen and Jiang, Song and Sun, Yizhou and Gu, Quanquan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/91ba4a4478a66bee9812b0804b6f9d1b-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{metis1,
  title={A fast and high quality multilevel scheme for partitioning irregular graphs},
  author={Karypis, George and Kumar, Vipin},
  journal={SIAM Journal on scientific Computing},
  volume={20},
  number={1},
  pages={359--392},
  year={1998},
  publisher={SIAM}
}

@InProceedings{submix,
  title = 	 {SubMix: Learning to Mix Graph Sampling Heuristics},
  author =       {Abu-El-Haija, Sami and Dillon, Joshua V. and Fatemi, Bahare and Axiotis, Kyriakos and Bulut, Neslihan and Gasteiger, Johannes and Perozzi, Bryan and Bateni, Mohammadhossein},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1--10},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/abu-el-haija23a/abu-el-haija23a.pdf},
  url = 	 {https://proceedings.mlr.press/v216/abu-el-haija23a.html},
  abstract = 	 {Sampling subgraphs for training Graph Neural Networks (GNNs) is receiving much attention from the GNN community. While a variety of methods have been proposed, each method samples the graph according to its own heuristic. However, there has been little work in mixing these heuristics in an end-to-end trainable manner. In this work, we design a generative framework for graph sampling. Our method, SubMix, parameterizes subgraph sampling as a convex combination of heuristics. We show that a continuous relaxation of the discrete sampling process allows us to efficiently obtain analytical gradients for training the sampling parameters. Our experimental results illustrate the usefulness of learning graph sampling in three scenarios: (1) robust training of GNNs by automatically learning to discard noisy edge sources; (2) improving model performance by trainable and online edge subset selection; and (3) by integrating our framework into decoupled GNN models improves their performance on standard benchmarks.}
}

@InProceedings{vrgcn,
  title = 	 {Stochastic Training of Graph Convolutional Networks with Variance Reduction},
  author =       {Chen, Jianfei and Zhu, Jun and Song, Le},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {942--950},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
}

