\section{Related Work}
\label{sec:related_work}



In this section, we discuss some works related to our proposed method.


\udfsection{Node-wise sampling.} Node-wise sampling ____ aggregates messages from a subset of uniformly sampled neighborhoods at each GNN layer, which decreases the bases in the exponentially increasing dependencies.
The idea is originally proposed in GraphSAGE ____. VR-GCN ____ further alleviates the bias and variance by historical embeddings, and then shows that its convergence rate to reach an \gongshi{$\varepsilon$}-approximate stationary point is \gongshi{$N=\mathcal{O}(\varepsilon^{-4})$}, where \gongshi{$N$} denotes the number of iterations in Theorem 2 in ____.
GraphFM-IB further alleviates the staleness of the historical embeddings based on the idea of feature momentum.
Although the node-wise sampling methods achieve the convergence rate of \gongshi{$\mathcal{O}(\varepsilon^{-4})$}, their computational complexity at each step is still exponentially increasing due to the neighborhood explosion issue.



\udfsection{Layer-wise sampling.} To avoid the exponentially growing computation of node-wise sampling, layer-wise sampling ____ samples a fixed number of nodes for each GNN layer and then uses importance sampling (IS) to reduce variance.
However, the optimal distribution of IS depends on the up-to-date embeddings, which are expensive.
To tackle this problem, FastGCN ____ proposes to approximate the optimal distribution of IS by the normalized adjacency matrix.
Adapt ____ proposes a learnable sampled distribution to further alleviate the variance.
Nevertheless, as the above-mentioned methods sample nodes independently in each GNN layer, the sampled nodes from two consecutive layers may be connected ____.
Thus, LADIES ____ consider the dependency of sampled nodes between layers by one step forward.
By combining the advantages of node-wise and layer-wise sampling approaches using Poisson sampling, LABOR ____ significantly accelerates convergence under the same node sampling budget constraints.
% without sacrificing accuracy.
% Although layer-wise sampling significantly alleviates the memory costs, they suffer from high sampling overhead due to extensive sampling during training.

%  faster than node-wise and layer-wise sampling approaches

\udfsection{Subgraph sampling.} Subgraph sampling methods sample a mini-batch and then construct the subgraph based on the mini-batch ____.
Thus, we can directly run GNNs on the subgraphs.
% Therefore, subgraph sampling is simple and it is applicable to a wide range of GNN architectures.
% 1
One of the major challenges is to efficiently encode neighborhood information of the subgraph.
To tackle this problem, one line of subgraph sampling is to design subgraph samplers to alleviate the inter-connectivity between subgraphs.
For example, CLUSTER-GCN ____ propose subgraph samplers based on graph clustering methods (e.g., METIS ____ and Graclus ____) and GRAPHSAINT propose edge, node, or random-walk based samplers.
SHADOW ____ proposes to extract the \gongshi{$L$}-hop neighbors of a mini-batch and then select an important subset from the \gongshi{$L$}-hop neighbors.
% The subgraph sampler simultaneously avoids the neighbor explosion issue and the over-smoothing issue of GNNs.
% 2
IBMB ____ proposes a novel subgraph sampler where the subgraphs are induced by the mini-batches with high influence scores, such as personalized PageRank scores.
Another line of subgraph sampling is to design efficient compensation for the messages from the neighborhood based on existing subgraph samplers.
For example, GAS ____ proposes historical embeddings to compensate for messages in forward passes and LMC ____ further proposes historical gradients to compensate for messages in backward passes.
GraphFM-OB ____ alleviates the staleness of the historical embeddings based on the idea of feature momentum.
Besides the traditional optimization algorithm, SubMix ____ proposes a novel learning-to-optimize method for subgraph sampling, which parameterizes subgraph sampling as a convex combination of several heuristics and then learns to accelerate the training of subgraph sampling.
% TOP can be viewed as a learning-to-optimize idea, where TOP learns message-invariant transformation by linear regression to accelerate the optimization process.