[
  {
    "index": 0,
    "papers": [
      {
        "key": "Christiano2017Deep",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "Ziegler2019Fine",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "Ouyang2022TrainingLM",
        "author": "Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Francis Christiano and Jan Leike and Ryan J. Lowe",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "Bradley1952RankAO",
        "author": "Ralph Allan Bradley and Milton E. Terry",
        "title": "Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "Schulman2017Proximal",
        "author": "John Schulman and\nFilip Wolski and\nPrafulla Dhariwal and\nAlec Radford and\nOleg Klimov",
        "title": "Proximal Policy Optimization Algorithms"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "Korbak2022RLWK",
        "author": "Tomasz Korbak and Ethan Perez and Christopher L. Buckley",
        "title": "RL with KL penalties is better viewed as Bayesian inference"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "Ouyang2022TrainingLM",
        "author": "Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Francis Christiano and Jan Leike and Ryan J. Lowe",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "Gao2022ScalingLF",
        "author": "Leo Gao and John Schulman and Jacob Hilton",
        "title": "Scaling Laws for Reward Model Overoptimization"
      },
      {
        "key": "Dubois2023Alice",
        "author": "Yann Dubois and\nChen Xuechen Li and\nRohan Taori and\nTianyi Zhang and\nIshaan Gulrajani and\nJimmy Ba and\nCarlos Guestrin and\nPercy Liang and\nTatsunori B. Hashimoto",
        "title": "AlpacaFarm: {A} Simulation Framework for Methods that Learn from Human\nFeedback"
      },
      {
        "key": "wang-etal-2024-hybrid",
        "author": "Wang, Chenglong  and\nZhou, Hang  and\nChang, Kaiyan  and\nLi, Bei  and\nMu, Yongyu  and\nXiao, Tong  and\nLiu, Tongran  and\nZhu, JingBo",
        "title": "Hybrid Alignment Training for Large Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "Wu2023PairwisePP",
        "author": "Tianhao Wu and Banghua Zhu and Ruoyu Zhang and Zhaojin Wen and Kannan Ramchandran and Jiantao Jiao",
        "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2024esrl",
        "author": "Chenglong Wang and\nHang Zhou and\nYimin Hu and\nYifu Huo and\nBei Li and\nTongran Liu and\nTong Xiao and\nJingbo Zhu",
        "title": "{ESRL:} Efficient Sampling-Based Reinforcement Learning for Sequence\nGeneration"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "Rafailov2023DirectPO",
        "author": "Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Ethayarajh2024KTOMA",
        "author": "Kawin Ethayarajh and\nWinnie Xu and\nNiklas Muennighoff and\nDan Jurafsky and\nDouwe Kiela",
        "title": "Model Alignment as Prospect Theoretic Optimization"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "Azar2023AGT",
        "author": "Mohammad Gheshlaghi Azar and\nZhaohan Daniel Guo and\nBilal Piot and\nR{\\'{e}}mi Munos and\nMark Rowland and\nMichal Valko and\nDaniele Calandriello",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "Park2024DisentanglingLF",
        "author": "Ryan Park and\nRafael Rafailov and\nStefano Ermon and\nChelsea Finn",
        "title": "Disentangling Length from Quality in Direct Preference Optimization"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "Xu2024ContrastivePO",
        "author": "Haoran Xu and\nAmr Sharaf and\nYunmo Chen and\nWeiting Tan and\nLingfeng Shen and\nBenjamin Van Durme and\nKenton Murray and\nYoung Jin Kim",
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of {LLM}\nPerformance in Machine Translation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "Hong2024ORPOMP",
        "author": "Jiwoo Hong and\nNoah Lee and\nJames Thorne",
        "title": "{ORPO:} Monolithic Preference Optimization without Reference Model"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "Meng2024SimPO",
        "author": "Yu Meng and\nMengzhou Xia and\nDanqi Chen",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      }
    ]
  }
]