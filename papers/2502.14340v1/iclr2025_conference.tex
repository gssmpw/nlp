
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{booktabs}

\usepackage{tikz}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=newest}
\usepackage{amsmath}
\usepackage{amssymb}
\usetikzlibrary{positioning,shapes,arrows,shadows,patterns}
\usepackage{subfig}
% \usepackage{subfigure}
% \usepackage{subcaption}
\usepackage{caption}
\usepackage{soul}
\usepackage{ragged2e}

\newcommand{\revised}[1]{{\color{blue}#1}}

\usepackage{enumitem}
\newcommand{\basicalert}[2]{\fbox{\bfseries\sffamily\scriptsize\color{red} #1}{\sf\small$\blacktriangleright$\textit{\color{blue} #2}$\blacktriangleleft$}}
\newcommand{\bei}[1]{\basicalert{From Bei}{#1}}
\newcommand{\gangao}[1]{\basicalert{From Gangao Liu}{#1}}
\newcommand{\src}[1]{\basicalert{From SRC}{#1}}
\newcommand{\cy}[1]{\basicalert{From Chen Yang}{#1}}

\definecolor{citecolor}{HTML}{0071BC}
\definecolor{linkcolor}{HTML}{ED1C24}
\hypersetup{pagebackref=true, breaklinks=true, letterpaper=true,colorlinks=true, citecolor=citecolor, linkcolor=linkcolor, urlcolor=purple, bookmarksopen=true, bookmarksopenlevel=2, bookmarksnumbered=true} % hyperlinks


\newcommand{\method}{$\textrm{D}^2\textrm{PO}$ }


\usepackage{wrapfig}
% \usepackage{warptable}

\title{Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
  Ruichen Shao${}^{1}\thanks{Equal Contributions.}$ \ \ \ \ Bei Li${}^{1*}\thanks{Corresponding Author.}$ \ \ \ \  Gangao Liu${}^{2}$ \ \ \ \ Yang Chen${}^1$ \ \ \ \ Xiang Zhou${}^1$ \\ \textbf{Jingang Wang}${}^{1\dag}$ \ \ \ \ \textbf{Xunliang Cai}${}^1$ \ \ \ \ \textbf{Peng Li}${}^2$ \\ 
  ${}^1$Meituan Inc. \ \ \ \ ${}^2$University of Chinese Academy of Sciences \ \ \ \  \\ 
\texttt{\{shaoruichen,libei17,chenyang108,wangjingang02\}@meituan.com} \\
\texttt{\{liugangao2023,lipeng\}@iscas.ac.cn}}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy
% Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Direct Preference Optimization (DPO) has gained attention as an efficient alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with human preferences. Despite its advantages, DPO suffers from a length bias, generating responses longer than those from the reference model. Existing solutions like SimPO and SamPO address this issue but uniformly treat the contribution of rewards across sequences, overlooking temporal dynamics. To this end, we propose an enhanced preference optimization method that incorporates a temporal decay factor controlled by a gamma parameter. This dynamic weighting mechanism adjusts the influence of each reward based on its position in the sequence, prioritizing earlier tokens that are more critical for alignment. By adaptively focusing on more relevant feedback, our approach mitigates overfitting to less pertinent data and remains responsive to evolving human preferences. Experimental results on several benchmarks show that our approach consistently outperforms vanilla DPO by 5.9-8.8 points on AlpacaEval 2 and 3.3-9.7 points on Arena-Hard across different model architectures and sizes. 
Furthermore, additional experiments on mathematical and reasoning benchmarks (MMLU, GSM8K, and MATH) confirm that our method enhances performance without compromising general capabilities. Our codebase would be available at \url{https://github.com/LotuSrc/D2PO}.

\end{abstract}



\section{Introduction}

Direct Preference Optimization (DPO)~\citep{Rafailov2023DirectPO} has recently emerged as a highly efficient alternative for aligning large language models (LLMs) with human preferences ~\citep{Askell2021AGL, Ouyang2022TrainingLM}. Unlike reinforcement learning from human feedback (RLHF), which involves training a reward model followed by iterative policy updates, DPO reframes the problem as a binary classification task directly over human preference data. 
Compared to supervised fine-tuning, DPO enables the model not only to learn what is good but also to be aware of what is bad.
This formulation allows DPO to optimize preference alignment in a single-stage training process, bypassing the complexities of reinforcement learning, such as policy sampling or extensive hyperparameter tuning. By leveraging an analytical mapping between reward functions and optimal policies, DPO fine-tunes LLMs efficiently and stably, offering superior performance in tasks like sentiment control, summarization, and dialogue generation while reducing computational overhead.

% Despite its advantages, DPO suffers from a length bias problem, which is caused by the unbalanced length preference due to the non-uniform length distribution of chosen and rejected responses. This leads to generated responses tending to be longer than those of the reference model if the majority of chosen responses are longer than the rejected ones. Several approaches have emerged to address this issue. One such method is SimPO~\citep{Meng2024SimPO}, which introduces a more streamlined framework by eliminating the need for a reference model. Instead of relying on a pre-trained reference model for comparison, SimPO uses the average log probability of a generated sequence as the implicit reward signal. This innovation reduces computational complexity and memory usage, making SimPO a more efficient alternative to DPO. However, our experiments have revealed that SimPO suffers from unexpected performance issues when applied to data not generated through self-sampling. Similarly, \citet{lu2024sampo} proposed SamPO to address the length bias inherent in DPO. By constraining the reward computation to the shorter time-series range between the chosen and rejected responses, SamPO mitigates biases arising from sequence length disparities, thereby refining the preference optimization process. 

Despite its advantages, DPO suffers from a length bias problem, which is caused by the unbalanced length preference due to the non-uniform length distribution of chosen and rejected responses. This leads to generated responses tending to be longer than those of the reference model if the majority of chosen responses are longer than the rejected ones. To address this, SimPO~\citep{Meng2024SimPO} introduces a more streamlined framework by eliminating the need for a reference model. Instead of relying on a pre-trained reference model for comparison, SimPO uses the average log probability of a generated sequence as the implicit reward signal. This innovation reduces computational complexity and memory usage, making SimPO a more efficient alternative to DPO. However, our experiments have revealed that SimPO suffers from unexpected performance issues when applied to data not generated through self-sampling. Similarly, SamPO~\citep{lu2024sampo} addresses DPO's length bias by limiting reward computation to the shorter time-series range between chosen and rejected responses, thereby the refining preference optimization.

\input{intro_earlier}
Both of these studies, however, treat the contribution of each reward across the entire sequence as uniform. We posit that this uniform treatment cannot fully capture the nuances of preference optimization. Specifically, the temporal dynamics within a sequence may influence the importance of certain tokens or segments over others. To validate this conjecture, we plot the KL divergence between the instruct models and their DPO variants on three widely used open-source models, where the results are shown in Figure \ref{fig:self_sampling}. We notice that the KL divergence remains larger at earlier tokens but gradually decreases along the positions, which indicates earlier tokens' distributions are more likely affected by DPO. This observation aligns with the findings of previous studies~\citep{lin2024unlocking,yang2023tokenguidance} that alignment is more critical for earlier tokens. This is also consistent with the nature of next-token prediction, where an accurate prefix allows subsequent tokens to be generated on a more reliable foundation, thereby improving the overall quality~\citep{Edunov2018backtranslation}. In other words, the uncertainty of earlier tokens is much lower, and the calibration for more recent tokens is higher than that for earlier ones~\citep{Wang2020calibration}. 

Building on this observation, we propose an enhanced version of DPO, namely temporal decay based DPO (short for \method), that integrates a temporal decay factor, controlled by a gamma parameter, to further refine the influence of preference data during training. Our method introduces a dynamic weighting mechanism that modulates the contribution of each reward based on its temporal relevance, allowing the model to prioritize earlier feedback over more recent tokens.
To this end, when the coefficient is slightly less than 1, it gradually reduces the influence of more recent rewards, which are inherently dependent on past rewards. Surprisingly, this temporal decay strategy has also been validated in the text-to-image synthesis task~\citep{yang2024denseReward}, where they emphasize the earlier steps in the reverse chain of the diffusion process. Our work complements theirs by showcasing the effectiveness of this approach in an autoregressive context, particularly in standard RLHF tasks. We provide a theoretical analysis on how earlier tokens can contribute more significantly from a token-level Markov Decision Process (MDP) perspective. Further discussion on the differences can be found in Appendix \ref{appendxi:comparison_with_related_work}.


By incorporating this adaptive temporal decay mechanism, \method not only facilitates earlier tokens to contribute more but also maintains the computational efficiency that makes DPO such a compelling approach for preference optimization. Experimental results on several widely used benchmarks, including AlpacaEval2, Arena-Hard and MT-bench, demonstrate the effectiveness on both off-policy and on-policy configurations. For example, in on-policy setups, \method outperforms DPO by up to 5.9-8.8 performance gains in terms of win rate on AlpacaEval2 and 3.3-9.7 points on Arena-Hard, respectively. Similarly, in off-policy setups, our method also demonstrates performance improvements. As a bonus of this decay mechanism which helps in reducing the overestimation of rewards caused by length bias in preference pairs, our method could be easily extended to reference-free mode, and it also can beat SimPO~\citep{Meng2024SimPO} by a large margin. Specifically, our best reference-free \method model can achieve 62.4 LC win rate on AlpacaEval 2 and 63.6 win rate on Arena Hard, which is competitive with the reference-based model. We also conducted additional experiments on mathematical and reasoning benchmarks, such as MMLU, GSM8K, and Math, indicating that our method enhances performance without compromising general capabilities.



\section{Related Work}
\subsection{Reinforcement Learning from Human Feedback(RLHF)}

The classical RLHF pipeline~\citep{Christiano2017Deep,Ziegler2019Fine,Ouyang2022TrainingLM} consists of two distinct stages: The reward modeling phase and the RL phase.

\vspace{-0.25cm}
\paragraph{Reward Modeling Phase.}The reward modeling is a binary classification task. Given a prompt, the comparison pair ($y_1, y_2$) is collected by querying the supervised fine-tuning (SFT) model. Then, the preference $y_w \succ y_l$ is labeled by human which is used to train a reward model. Typically, Bradley-Terry model~\citep{Bradley1952RankAO} which quantifies the likelihood of one action being preferred over another is usually used to modeling the preference relations:
\begin{equation}
    p\left(y_1 \succ y_2 \mid x\right)=\frac{\exp \left(r\left(x, y_1\right)\right)}{\exp \left(r\left(x, y_1\right)\right)+\exp \left(r\left(x, y_2\right)\right)}=\sigma\left(r\left(x, y_1\right)-r\left(x, y_2\right)\right)
\end{equation}

\vspace{-0.25cm}
\paragraph{Reinforcement Learning Phase.} With the reward model in place, the second phase involves optimizing a policy through reinforcement learning, such as proximal policy optimization (PPO)~\citep{Schulman2017Proximal}, aiming to maximize the learned reward while ensuring the policy remains close to a predefined reference policy~\citep{Korbak2022RLWK}. This optimization is crucial for preventing model drift and maintaining alignment with human judgments, which is typically formulated as:
\begin{equation}
    \max _\theta \mathbb{E}_{x \sim D, y \sim \pi_\theta(\cdot \mid x)}\left[r_\phi(x, y)\right]-\beta \mathbb{E}_{x \sim D}\left[\mathrm{KL}\left(\pi_\theta(\cdot \mid x) \| \pi_{\operatorname{ref}}(\cdot \mid x)\right)\right]
\end{equation}

\subsection{Direct Alignment Algorithms (DAAs)}
RLHF has become a cornerstone in the training of LLMs, facilitating their alignment with human preferences. However, the classical RLHF framework~\citep{Ouyang2022TrainingLM} is characterized by a two-stage training process, which includes reward modeling, and reinforcement learning. This complexity introduces several challenges and limitations, including reward over-optimization~\citep{Gao2022ScalingLF,Dubois2023Alice,wang-etal-2024-hybrid}, training instability~\citep{Wu2023PairwisePP} and efficiency~\citep{wang2024esrl}. 
Nowadays, DAAs have emerged as a promising alternative, which can be divided into two major categories based on whether to consider a reference model.

\vspace{-0.25cm}
\paragraph{Reference-based Methods.}
The reference-based methods in DAAs utilize a pre-existing model, often a supervised fine-tuned (SFT) model, as a reference point during the optimization process. This reference model serves as a baseline to which the updated model is compared, ensuring that updates do not deviate excessively from the initial, presumably safe and aligned, model configuration. DPO~\citep{Rafailov2023DirectPO} is the most popular reference-based alignment algorithm and after its appearance, more researchers attempt to modify objective function for better performance. KTO~\citep{Ethayarajh2024KTOMA} distinguishes itself by its capability to train from non-paired preference data, providing a unique angle on optimization. IPO~\citep{Azar2023AGT} learns directly from preferences without relying on the Bradley-Terry model assumption that assumes that pairwise preferences can be substituted with pointwise rewards. R-DPO~\citep{Park2024DisentanglingLF} is an enhanced derivative of DPO, fortified with an additional regularization term designed to mitigate the tendency to exploit length biases, thus ensuring more balanced and diverse response generation.

\vspace{-0.25cm}
\paragraph{Reference-free Methods.}
In contrast to reference-based methods that depend on a pre-existing model for guidance, reference-free methods forgo the need for such a reference. They directly optimize the model parameters in response to human feedback, which can enhance the flexibility of the optimization process. However, this freedom also presents challenges in controlling the extent of updates. CPO~\citep{Xu2024ContrastivePO} leverages sequence likelihood as a reward signal and is trained in conjunction with an SFT objective. ORPO~\citep{Hong2024ORPOMP} is a novel alignment method that integrates an odds ratio-based penalty into the supervised fine-tuning process. SimPO~\citep{Meng2024SimPO} uses an average log probability as an implicit reward and introduces a target reward margin to enhance performance without relying on a reference model.

\section{Methodology}
\subsection{Direct Preference Optimization (DPO).}
Direct Preference Optimization (DPO) is a pivotal advancement in the field of offline preference-based training for language models. Traditional RLHF involves a complex, multi-stage process that includes training a reward model to align with human preferences and subsequently optimizing a policy model to maximize this reward while staying close to the original model's distribution. DPO simplifies this process by reparameterizing the reward function directly in terms of the policy model, eliminating the need for an explicit reward model:
\begin{equation}
    r(x, y)=\beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}+\beta \log Z(x),
\end{equation}

where $\pi_\theta$, $\pi_{ref}$ denotes the policy model and reference model, respectively. Z(x) is the partition function, and $\beta$ is a hyperparameter to control the deviation from the reference model. Substituting this reward into the Bradley-Terry (BT) ranking objective yields the DPO loss:
\begin{equation}
\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(\mathbf{y}_w \mid \mathbf{x}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_w \mid \mathbf{x}\right)}-\beta \log \frac{\pi_\theta\left(\mathbf{y}_l \mid \mathbf{x}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_l \mid \mathbf{x}\right)}\right)\right]
\end{equation}

DPO operates by formulating an implicit reward using the log ratio of the likelihood of a response between the current policy model and a supervised fine-tuned (SFT) model. This reward is then incorporated into the Bradley-Terry ranking objective to directly optimize the policy model for preference data. The effectiveness of DPO lies in its ability to simplify the preference optimization process, making it more accessible and efficient for practical applications.

\input{overall_method}
\subsection{Temporal decay matters in preference learning.} 

\input{pred_prob}

\paragraph{Motivation.}
Preference learning plays a pivotal role in optimizing LLMs, especially when leveraging user feedback to align model outputs with human preferences. While methods like DPO~\citep{Rafailov2023DirectPO} and its successors~\citep{Meng2024SimPO,lu2024sampo} have demonstrated significant potential in this domain, they exhibit a critical oversight: the uniform treatment of tokens across the sequence. As illustrated in Figure \ref{fig:decay_mechanisms}, DPO, SimPO, and SamPO assign identical coefficients to all tokens within the chosen response $y_w$ and the rejected response $y_l$. We argue that optimizing each token equally, without considering their positional importance or temporal dependence, is suboptimal for most scenarios.

Our observations indicate that earlier tokens receive greater optimization during the preference learning process compared to later ones (see Figure \ref{fig:self_sampling}
). This suggests that the benefits derived from the alignment phase over SFT are predominantly due to the optimization of initial tokens. Additionally, when plotting the prediction probability across different response positions in Figure \ref{fig:pred_prob}, we find that more recent tokens have higher probabilities than earlier tokens. This indicates that the model becomes increasingly confident in predicting tokens as it progresses through the sequence, likely due to the accumulating contextual information from previous tokens. However, since the accuracy of these later tokens is already high—reaching up to 0.9, further improvements are more likely to come from enhancing the accuracy of the earlier tokens. Therefore, a natural approach is to focus on improving the accuracy of the prefix: \textit{the more accurate the earlier tokens are, the better the overall quality of the sequence will be.}

\vspace{-0.25cm}
\paragraph{Temporal Decay Mechanism.} Inspired by the success of \citet{yang2024denseReward}, where earlier steps are crucial in the reverse chain of the diffusion denoising process, we propose a temporal decay mechanism to highlight the importance of earlier tokens in LLM scenarios. Considering the original DPO loss formulation, the most direct way to prioritize earlier tokens is to incorporate a position-dependent coefficient that decays over time. Multiple decay mechanisms can achieve this, including linear, polynomial, step, and cosine decay functions. After evaluating these options, we chose exponential decay due to its simplicity and effectiveness.

Exponential decay applies a coefficient that decreases exponentially with the token position, represented as $\gamma^t$, where $\gamma$ is the decay rate ($0 < \gamma < 1$) and $t$ is the token's position. This approach provides a smooth and gradual reduction in the influence of later tokens, ensuring that earlier tokens have a more significant impact on the loss calculation. To this end, we adapt this concept to the DPO loss function which defined as:
% Drawing inspiration from reinforcement learning, where rewards are discounted over time using an exponential function to model time preference, 
\sethlcolor{red}
\begin{equation}
\label{eq:d^2po}
\mathcal{L}_{\textrm{D}^2\textrm{PO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)
    =-\log \sigma\left(\sum\limits_{t=0}^{T_w} \textcolor{red}{\gamma^t} \beta \log \frac{\pi_\theta\left(\mathbf{y}_w^t \mid \mathbf{x},\mathbf{y}_w^{<t}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_w^t \mid \mathbf{x},\mathbf{y}_w^{<t}\right)}-\sum\limits_{t=0}^{T_l} \textcolor{red}{\gamma^t} \beta \log \frac{\pi_\theta\left(\mathbf{y}_l^t \mid \mathbf{x},\mathbf{y}_l^{<t}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_l^t \mid \mathbf{x},\mathbf{y}_l^{<t}\right)}\right)
\end{equation}
\noindent In this formulation, the exponential decay factor $\gamma^t$ adjusts the contribution of each token based on its position in the sequence. As is shown in Figure \ref{fig:decay_mechanisms} (d), coefficients of each token in \method gradually decrease along the position (the color from dark to light), placing greater emphasis on earlier tokens. This modification aligns the optimization process with the observed pattern of optimization in preference learning, where initial tokens benefit more from the alignment phase. Similar to the derivation of the vanilla DPO, we provide the detailed derivation of \method in the Appendix \ref{sec:derivation}.

    
\subsection{Reference-free is consistent with on-policy setups.} 
\label{sec:reference-free}

\input{density}
Reference-based methods often incorporate a KL divergence constraint to prevent the policy model from deviating significantly from its initial state, which adds computational and memory overhead. In the context of DPO, this constraint appears as an adaptive margin term $\log \frac{\pi_{\text{ref}}(y_l|x)}{\pi_{\text{ref}}(y_w|x)}$ in the pairwise loss function. This term quantifies the reference model's preference difference between less-preferred ($y_l$) and preferred ($y_w$) responses. 
We note that the DPO loss can be viewed as a specific case of contrastive loss, where $\log \pi_\theta(y)$ measures the relevance between the prompt $x$ and the response $y$. The adaptive margin ensures that loss values remain moderate, contributing to training stability. However, if the reference model assigns similar probabilities to both $y_w$ and $y_l$ (i.e., the margin approaches zero), the impact of the reference model diminishes, suggesting that it can be easily excluded.


To validate this, we analyze the margin distributions in the UltraFeedback dataset under off-policy and on-policy settings. In the off-policy setting, we use original responses, while in the on-policy setting, responses are regenerated by the same model. As illustrated in Figure~\ref{fig:ref_margin_density}, the on-policy dataset exhibits smaller variance in margins and an average closer to zero compared to the off-policy dataset. This indicates a higher proportion of semi-hard samples in the on-policy data. From this perspective, we can discard the KL divergence constraint under on-policy settings and easily derive the reference-free version loss function:
\begin{equation}
\mathcal{L}_{\textrm{D}^2\textrm{PO}}\left(\pi_\theta\right)
    =-\log \sigma\left(\sum\limits_{t=0}^{T_w} \gamma^t \beta \log \pi_\theta\left(\mathbf{y}_w^t \mid \mathbf{x,y_w^{<t}}\right)-\sum\limits_{t=0}^{T_l} \gamma^t \beta \log \pi_\theta\left(\mathbf{y}_l^t \mid \mathbf{x,y_l^{<t}}\right)\right)
\end{equation}
\section{theoretical analysis}
In this section, we analyze the influence of the discount factor $\gamma$ on the performance of our method compared to DPO. Both DPO and our method can be considered as a token-level MDP that satisfies the Bellman equation. Here, we define the suboptimality as the performance difference between the optimal policy $\pi^{*}$ and a given policy $\pi$ under specific discount factors, which has been widely discussed in offline RL~\citep{rashidinejad2021bridging,jin2021pessimism}.

\subsection{Suboptimality Decomposition}
\textbf{Definition 1 (Suboptimality).} The suboptimality of a policy $\pi$ with respect to the optimal policy $\pi^*$, starting from an initial state $s$ under discount factor $\gamma$, is defined as:
\begin{equation}
\text{SubOpt}(\pi, s; \gamma) = V_{\gamma}^{\pi^*}(s) - V_{\gamma}^{\pi}(s),
\end{equation}
where $V_{\gamma}^{\pi}(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{H-1} \gamma^{t} r(s_t, a_t) \mid s_0 = s\right]$ is the expected return of policy $\pi$ under discount factor $\gamma$, and $H$ is the finite horizon. To analyze the influence of the discount factor $\gamma$, we consider the suboptimality of our method evaluated with an evaluation discount factor $\gamma_e = 1.0$. We decompose the suboptimality into three terms that separately capture the differences due to the discount factors and the policy discrepancies. In this way, we can rewrite the suboptimality as below:
\begin{equation}
\begin{aligned}
\text{SubOpt}(\pi, s; \gamma_e) &= V_{\gamma_e}^{\pi^*}(s) - V_{\gamma_e}^{\pi}(s) \\
&= \underbrace{\left[V_{\gamma_e}^{\pi^*}(s) - V_{\gamma}^{\pi^*}(s)\right]}_{\Delta_1} + \underbrace{\left[V_{\gamma}^{\pi^*}(s) - V_{\gamma}^{\pi}(s)\right]}_{\Delta_2} + \underbrace{\left[V_{\gamma}^{\pi}(s) - V_{\gamma_e}^{\pi}(s)\right]}_{\Delta_3}
\end{aligned}
\end{equation}

This decomposition allows us to separately analyze the impact of the discount factors and the policy differences.

\subsection{Suboptimality Analysis}
\textbf{Theorem 1(Suboptimality Upper Bound).} 
\emph{Let $\pi^*$ denote the optimal policy, and $\pi$ be the policy under a discount factor $\gamma \in [0, 1)$. Assume that rewards are bounded such that $\left| r(s, a) \right| \leq R$ for all states $s$ and actions $a$, and consider a finite horizon $H$. Then, the suboptimality of $\pi$ compared to $\pi^*$ when evaluated with an evaluation discount factor $\gamma_e = 1.0$ satisfies the following upper bound:}
\begin{equation}
\begin{aligned}
\text{SubOpt}(\pi,s; \gamma_e) \leq 2(H - \frac{1-\gamma^H}{1-\gamma})R + \frac{2(1-\gamma^H)^2}{(1-\gamma)^2}E_{s \sim d^{\pi^{*}}}\left[\mathbb{TV}(\pi^*(a|s)||\pi(a|s)\right]R
\end{aligned}
\end{equation}

The complete proof is included in Appendix \ref{sec:theorem_proof}. This upper bound reveals that the suboptimality depends on both the discount factor $\gamma$ and the mismatch between $\pi$ and $\pi^*$. Specifically, the first term $H - \frac{1 - \gamma^{H}}{1 - \gamma}$ decreases as $\gamma$ increases, while the second term $\left(\frac{1 - \gamma^{H}}{1 - \gamma}\right)^2$ increases, highlighting a trade-off in the choice of $\gamma$. As both terms vary monotonically with the discount factor $\gamma$ but in opposing directions, there exists an optimal value $\gamma^*$ within the interval $(0, 1)$ that balances these effects to minimize the overall suboptimality.

\section{Experiments}

\subsection{Experimental Setups}
\label{sec:baselines}
Due to page limitations, we briefly describe the model setting, training data, and hyperparameters in the following section. Expanded details on evaluation benchmarks and baselines are available in Appendix \ref{appendix:exp_setup}.
\paragraph{Model Setting.} We conducted preference optimization experiments using three model families: Llama3-8B~\citep{llama3modelcard}, Gemma2-9B~\citep{gemmateam2024gemma2improvingopen} and Mistral-12B~\citep{Jiang2023Mistral7}. Here, we mainly focused on building our systems upon the instruct models.
Thus, we utilized pre-trained instruction-tuned models (e.g., \href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{meta-llama/Meta-Llama-3-8B-Instruct}, \href{https://huggingface.co/google/gemma-2-9b-it}{google/gemma-2-9b-it}, and \href{https://huggingface.co/nvidia/Mistral-NeMo-12B-Instruct}{nvidia/Mistral-NeMo-12B-Instruct}) as the SFT models.\footnote{The exact nature of the instruction-tuning (whether it includes SFT or the complete RLHF pipeline) of these models is not fully disclosed. For simplicity, we refer to these as SFT models.} 

\vspace{-0.25cm}
\paragraph{Training Data}
Our experiments were carried out using the UltraFeedback dataset. Specifically, We categorize the preference data into two types: 1) off-policy data (original response pairs from the UltraFeedback dataset), and 2) on-policy data generated using the SFT models. Similar to SimPO~\citep{Meng2024SimPO}, for each prompt $x$, we generated 5 responses using the SFT model with a sampling temperature of 0.8. To validate these responses, we employed \href{https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1}{{RLHFlow/ArmoRM-Llama3-8B-v0.1}}~\citep{ArmoRM} to assign scores to each response, allowing us to select the highest-scoring response as $y_w$ and the lowest-scoring one as $y_l$.

\vspace{-0.25cm}
\paragraph{Hyperparameters}
For all models, we set the maximum response length to 2,048 tokens and used a batch size of 128. Optimization was performed using the AdamW optimizer~\citep{kingma2014adam} with a learning rate of $5e-7$ and a cosine learning rate schedule featuring a 10\% warmup period. In preference optimization methods, including DPO and its variants such as our method \method{} and SamPO, we set $\beta$ to 0.1 to ensure a fair comparison.


\begin{table*}[!t]
\setlength{\tabcolsep}{2pt}
\centering
\small 
\caption{We report AlpacaEval 2~\citep{AlpacaEval} (denoted by AE2), Arena-Hard~\citep{arenahard2024} (denoted by AH), and MT-Bench~\citep{zheng2023judging} (denoted by MB) results under three settings using standard provided samples. Note that LC and WR denote length-controlled and raw win rate, respectively. We used off-the-shelf models as the SFT model. And our judge model is GPT-4-Turbo.}

% \resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccccc}
% \begin{tabular}{lrrrrrrrrrr}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Llama3-Instruct (8B)}} & \multicolumn{4}{c}{\textbf{Gemma2-Instruct (9B)}} & \multicolumn{4}{c}{\textbf{Mistral-NeMo-Instruct (12B)}} \\ 
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
& \multicolumn{2}{c}{\textbf{AE2}} & \multicolumn{1}{c}{\textbf{AH}} & \multicolumn{1}{c}{\textbf{MB}} & \multicolumn{2}{c}{\textbf{AE2}} & \multicolumn{1}{c}{\textbf{AH}} & \multicolumn{1}{c}{\textbf{MB}} & \multicolumn{2}{c}{\textbf{AE2}} & \multicolumn{1}{c}{\textbf{AH}} & \multicolumn{1}{c}{\textbf{MB}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9} \cmidrule(lr){10-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13} 
& {\scriptsize \bf WR (\%)} & {\scriptsize \bf LC (\%)} & {\scriptsize \bf WR (\%)} & {\scriptsize \bf G4-T} & {\scriptsize \bf WR (\%)}  & {\scriptsize \bf LC (\%)} & {\scriptsize \bf WR (\%)} & {\scriptsize \bf G4-T} & {\scriptsize \bf WR (\%)}  & {\scriptsize \bf LC (\%)} & {\scriptsize \bf WR (\%)} & {\scriptsize \bf G4-T} \\
\midrule
SFT          & 39.1  & 40.1  & 27.6  & 7.5 & 37.6 & 47.2 & 44.1 & 8.3 & 44.6 & 47.7 & 46.5 & 8.1 \\
\midrule
DPO          & 37.4 & 40.3 & 27.7 & \textbf{7.7} & 38.8 & 48.8 & 42.5 & 8.1 & 44.4 & 49.3 & 48.5 & 8.3 \\
KTO          & 33.3 & 38.1 & 21.0 & 7.5 & 39.1 & 50.0 & 43.8 & 8.3 & 37.4 & 48.7 & 35.8 & 8.2 \\
IPO          & 42.2 & \textbf{45.7} & 31.9 & 7.6 & 41.0 & 50.0 & 48.2 & 8.0 & 39.8 & 48.9 & 39.8 & 8.2 \\
SamPO          & 40.7 & 43.1 & 26.1 & 7.5 & 39.9 & 50.1 & 46.9 & 8.2 & 43.5 & 49.5 & 50.1 & 8.1 \\
$\textrm{D}^2$PO (ours) & \textbf{43.5} & 43.0 & \textbf{37.0} & \textbf{7.7} & \textbf{45.5} & \textbf{51.0} & \textbf{50.2} & \textbf{8.3} & \textbf{51.3} & \textbf{54.4} & \textbf{51.8} & \textbf{8.4} \\
\midrule
ORPO         & 10.6 & 15.3 & 6.8 & 6.3 & 11.3 & 21.6 & 10.2 & 7.1 & 9.6 & 17.0 & 9.8 & 6.9 \\
SimPO        & 0.3$^*$ & 0.8$^*$ & 1.4$^*$ & 1.6$^*$ & 38.8 & 50.0 & 31.6 & 8.0 & 46.8 & \textbf{53.3} & 46.6 & 8.0 \\
\bottomrule
\end{tabular}
% }
\label{tab:main_res_offline_turbo}
\vspace{-.5em}
\end{table*}


\begin{table*}[!t]
\setlength{\tabcolsep}{2pt}
\centering
\small 
\caption{Following the setting in \citet{Meng2024SimPO}, we used the on-policy data to obtain the chosen and rejected and applied a stronger reward model. $\dagger$ denotes our reference-free version.}
% \resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccccc}
% \begin{tabular}{lrrrrrrrrrr}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Llama3-Instruct (8B)}} & \multicolumn{4}{c}{\textbf{Gemma2-Instruct (9B)}} & \multicolumn{4}{c}{\textbf{Mistral-NeMo-Instruct (12B)}} \\ 
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
& \multicolumn{2}{c}{\textbf{AE2}} & \multicolumn{1}{c}{\textbf{AH}} & \multicolumn{1}{c}{\textbf{MB}} & \multicolumn{2}{c}{\textbf{AE2}} & \multicolumn{1}{c}{\textbf{AH}} & \multicolumn{1}{c}{\textbf{MB}} & \multicolumn{2}{c}{\textbf{AE2}} & \multicolumn{1}{c}{\textbf{AH}} & \multicolumn{1}{c}{\textbf{MB}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-7}\cmidrule(lr){8-8}\cmidrule(lr){9-9} \cmidrule(lr){10-11}\cmidrule(lr){12-12}\cmidrule(lr){13-13} 
& {\scriptsize \bf WR (\%)} & {\scriptsize \bf LC (\%)} & {\scriptsize \bf WR (\%)} & {\scriptsize \bf G4-T} & {\scriptsize \bf WR (\%)}  & {\scriptsize \bf LC (\%)} & {\scriptsize \bf WR (\%)} & {\scriptsize \bf G4-T} & {\scriptsize \bf WR (\%)}  & {\scriptsize \bf LC (\%)} & {\scriptsize \bf WR (\%)} & {\scriptsize \bf G4-T} \\
\midrule
SFT          & 39.1 & 40.1 & 27.6 & 7.5 & 37.6 & 47.2 & 44.1 & 8.3 & 44.6 & 47.7 & 46.5 & 8.1 \\
\midrule
DPO          & 46.2 & 47.6 & 42.4 & 7.8 & 47.0 & 53.4 & 56.7 & \textbf{8.4} & 53.5 & 53.3 & 59.0 & 8.4 \\
KTO          & 42.4 & 44.8 & 32.1 & 7.7 & 48.3 & 53.4 & 57.1 & 8.3 & 48.9 & 51.9 & 53.2 & 8.4 \\
IPO          & 42.9 & 46.0 & 34.5 & 7.9 & 50.9 & 50.0 & 59.7 & 8.3 & 53.6 & 54.4 & 59.7 & 8.4 \\
SamPO        & 44.4 & 47.2 & 35.8 & \textbf{8.0} & 45.8 & 55.2 & 55.2 & 8.2 & 51.1 & 53.0 & 58.3 & 8.3 \\
$\textrm{D}^2$PO (ours) & \textbf{47.4} & \textbf{53.5} & \textbf{47.3} & 7.8 & \textbf{57.2} & \textbf{59.7} & \textbf{66.4} & 8.3 & \textbf{57.3} & \textbf{62.1} & \textbf{62.3} & \textbf{8.6} \\
\midrule
ORPO         & 37.8 & 39.3 & 25.5 & 7.7 & 41.9 & 51.1 & 45.3 & 8.2 & 43.8 & 47.5 & 46.0 & 8.2 \\
SimPO        & 44.4 & 50.3 & 41.9 & \textbf{7.8} & 54.5 & 58.4 & 65.0 & \textbf{8.3} & 51.3 & 55.0 & 61.9 & \textbf{8.3} \\
$\textrm{D}^2$PO$^\dagger$ (ours)    & \textbf{48.0} & \textbf{53.9} & \textbf{46.1} & 7.7 & \textbf{56.7} & \textbf{60.8} & \textbf{65.7} & \textbf{8.3} & \textbf{58.3} & \textbf{62.4} & \textbf{63.6} & \textbf{8.3}\\
\bottomrule
\end{tabular}
% }
\label{tab:main_res_online_turbo}
\vspace{-.5em}
\end{table*}

\subsection{Experimental Results}
In our experiments, we provide a comprehensive comparison of our proposed method against DPO and its variants on both off-policy and on-policy data respectively, along with the baselines introduced in Section \ref{sec:baselines}. The baselines are categorized into two broad paradigms: reference-based and reference-free (SimPO and ORPO). Notably, as discussed in Section \ref{sec:reference-free}, our method can be seamlessly integrated into the reference-free paradigm using on-policy data.
We ensure fair comparisons by maintaining consistency in the codebase and experimental settings across all methods evaluated.

% ############Figure############
\input{Figure/gamma}
% ############Figure############

\vspace{-0.25cm}
\paragraph{Off-policy Setups.}
Table \ref{tab:main_res_offline_turbo} clearly demonstrates that our method delivers significant improvements in win rates across all configurations. Specifically, when applied to the Llama3, our method outperforms DPO by margins of 6.1\% and 2.9\% in standard and length-controlled evaluation scenarios, respectively. Similarly, for the Mistral-NeMo model, our method surpasses DPO by margins of 6.9\% and 5.1\% in standard and length-controlled scenarios, respectively. We observed that reference-free methods exhibited instability when applied to off-policy data, often leading to a degradation in model performance. This issue is particularly evident with SimPO, where previous work observed similar findings~\citep{lu2024sampo}. This phenomenon highlights the challenges associated with reference-free methods in preference optimization on off-policy data.

\vspace{-0.25cm}
\paragraph{On-policy Setups.}
As shown in Table \ref{tab:main_res_online_turbo}, our proposed method, along with all baselines, achieves better results compared to off-policy settings. Notably, our method consistently demonstrates improvements across different setups. Due to the reward model's length preference when selecting on-policy data, models trained on this data are more prone to verbosity. A critical observation in standard evaluations is the inherent bias favoring models that generate longer responses, which tend to achieve higher win rates. However, our method not only achieves superior win rates but also produces significantly shorter responses, showcasing its efficiency in generating concise and relevant outputs. Additionally, when the reference model is omitted, our method outperforms SimPO by 2.4–7.4 in LC win rate and 0.7–4.2 in win rate on AlpacaEval 2 and Arena-Hard, respectively. These findings further underscore the robustness and effectiveness of our approach. This superiority in both reference-based and reference-free contexts emphasizes the versatility and reliability of our method in preference optimization.

\section{Analyses}

\paragraph{$\gamma$ Plays an Important Role.} 
The temporal decay is one of the main contributions of this work, and we would like to show how the $\gamma$ affects the performance. We conducted ablation studies on three open-source models for robust conclusions. Through results as shown in Figure \ref{fig:gamma_vs_winrate}, we see that nearly all variants with $\gamma$ lower than 1.0 consistently outperform DPO\footnote{DPO is a special case of ours where $\gamma$ equals to 1.0.}. Also, $\gamma=0.98$  achieves the highest performance across three benchmarks for these strong open-source models. This indicates that our method is robust to the choice of $\gamma$, reducing the need for extensive hyperparameter tuning.

\setlength{\tabcolsep}{4pt}
\begin{table*}[!t]
\centering
\small 
\caption{Results on OpenLLM Benchmark, including reasoning and mathematical testsets. Note that Hella. denotes Hellaswag, Truth. denotes TruthfulQA and Wino. denotes Winogrande.}

% \resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Method}}  & \textbf{MMLU} & \textbf{GSM8K} & \textbf{Math} & \textbf{IFEval} & \textbf{ARC-C} & \textbf{Hella.} & \textbf{Truth.} & \textbf{Wino.} \\ 
\cmidrule(lr){2-9}
& 0-shot & 0-shot & 0-shot & 0-shot & 25-shot & 10-shot & 0-shot & 5-shot \\ 
\midrule
\multicolumn{9}{c}{\bf (a) Llama3-Instruct (8B)} \\
SFT          & 61.7  & 78.5  & 7.9 & 68.6 & 62.0 & 78.8 & 51.6 & 75.5  \\
\midrule
DPO          & 56.7  & 70.5  & 7.8 & 65.1 & 65.1 & \textbf{79.9} & 56.4 & 74.5  \\
SimPO        & 55.2  & 57.5  & 5.3 & 60.8 & \textbf{67.6} & 78.8 & \textbf{63.8} & 74.3  \\
\method (ours) & \textbf{61.4} & \textbf{72.0} & \textbf{8.5} & \textbf{65.6} & 65.8 & 79.0 & 57.6 & \textbf{75.1} \\
\midrule
\multicolumn{9}{c}{\bf (b) Gemma2-Instruct (9B)} \\
SFT          & 72.8 & 87.4  & 19.4 & 71.9 & 71.8 & 81.7 & 60.2	& 77.9  \\
\midrule
DPO          & 72.2	& 88.5 & 19.4 & 60.1 & 69.9	& 71.5 & 57.7 & 72.7  \\
SimPO        & 72.4 & 88.2 & 19.0 & \textbf{71.5} & 68.3	& 66.5 & 58.9 & 73.7  \\
\method (ours) & \textbf{72.7} & \textbf{88.9} & \textbf{21.2} & 71.2 & \textbf{71.4} & \textbf{81.0} & \textbf{61.3} & \textbf{76.0} \\
\bottomrule
\end{tabular}
% }
\label{tab:openllm_benchmark}
\vspace{-.5em}
\end{table*}

\vspace{-0.25cm}
\paragraph{$\gamma$ Larger than 1 is Harmful.} 
As highlighted in the previous section, we prioritize earlier feedback over more recent feedback, aligning with the next-token prediction paradigm. We conducted an experiment where the decay factor $\gamma$ was set to slightly greater than 1.0 to observe the effects. The results could also be observed in Figure \ref{fig:gamma_vs_winrate}. When $\gamma$ exceeds 1.0, rewards linked to later tokens in the sequence receive larger coefficients than those for earlier tokens. However, this adjustment was detrimental to preference optimization, resulting in performance that lagged behind the standard DPO on both the AlpacaEval 2 and Arena-Hard benchmarks. This finding demonstrates the crucial role of earlier tokens in the alignment process and indicates that overemphasizing later tokens can degrade model performance.

\vspace{-0.25cm}
\paragraph{Evaluations on OpenLLM Benchmark.}

To verify whether the improvements of \method on the aforementioned RLHF benchmarks, such as Alpaca Eval2, Arena Hard, and MT-bench, come at the expense of general language modeling ability, we conducted a comprehensive evaluation of downstream tasks on the Open LLM leaderboard\footnote{Open LLM leaderboard is created by huggingface to provide a standardized evaluation setup for LLMs, which includes several popular benchmarks encompassing a wide range of capabilities across multiple domains.}. Specifically, we employed zero-shot evaluations on MMLU~\citep{hendrycks2021measuring}, GSM8K~\citep{cobbe2021training}, MATH~\citep{hendrycks2021measuring}, IFEval~\citep{zhou2023instruction}, and TruthfulQA~\citep{lin2022truthfulqa}. Additionally, we performed few-shot evaluations on ARC-C~\citep{clark2018think}, Hellaswag~\citep{zellers2019hellaswag}, and Winogrande~\citep{levesque2012winograd} according to the official settings in the Open LLM leaderboard. The results are summarized in Table \ref{tab:openllm_benchmark}, and we observe that:

\begin{itemize}[leftmargin=*]
    \item In the Llama3-8B configuration, our \method method significantly outperforms both DPO and SimPO, particularly on the MMLU and MATH benchmarks. Notably, \method exhibits less performance degradation on GSM8K compared to SimPO, despite both methods effectively controlling output length. \method achieves substantial performance gains on the MATH dataset, surpassing the Instruct model by 0.55 points, while the other two methods show a noticeable decline.
    \item In the Gemma2-9B configuration, we observe a similar pattern, with \method demonstrating a significant performance advantage on the MATH benchmark. These results suggest that \method effectively enhances reasoning and mathematical problem-solving abilities in LLMs across different models. Furthermore, these additional evaluations on specialized datasets confirm that \method maintains its effectiveness across various contexts and task types.
\end{itemize}



\begin{table*}[!t]
\centering
\small 
\caption{Comparison of different decay mechanisms in terms of performance and response length.}
\label{tab:decay_methods}
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{\textbf{Decay Strategy}} & \multirow{2}{*}{\textbf{Rewards}} & \multicolumn{3}{c}{\textbf{AE2}} & \multicolumn{2}{c}{\textbf{AH}} & \multicolumn{1}{c}{\textbf{MB}} \\
\cmidrule(lr){3-5}\cmidrule(lr){6-7}\cmidrule(lr){8-8}
& & {\scriptsize  \bf WR (\%)} & {\scriptsize \bf LC (\%)} & {\scriptsize \bf Len.} & {\scriptsize \bf WR (\%)} & {\scriptsize \bf Len.} & {\scriptsize \bf G4-T} \\
\midrule
Exponential & $\sum\limits_{t=0}^{T} \gamma^t \beta \log \frac{p_\theta\left(\mathbf{y}_t \mid \mathbf{x,y_{<t}}\right)}{p_{ref}\left(\mathbf{y}_t \mid \mathbf{x,y_{<t}}\right)}$ & 57.2 & 59.7 & 1950 & 66.4 & 724 & 8.3 \\

Head & $\sum\limits_{t=0}^{\gamma T} \beta \log \frac{p_\theta\left(\mathbf{y}_t \mid \mathbf{x,y_{<t}}\right)}{p_{ref}\left(\mathbf{y}_t \mid \mathbf{x,y_{<t}}\right)}$ & 48.6 & 54.7 & 1762 & 57.4 & 680 & 8.2 \\

Linear & $\sum\limits_{t=0}^{\gamma T} \left(1-\frac{t}{\gamma T}\right) \beta \log \frac{p_\theta\left(\mathbf{y}_t \mid \mathbf{x,y_{<t}}\right)}{p_{ref}\left(\mathbf{y}_t \mid \mathbf{x,y_{<t}}\right)}$ & 48.3 & 54.5 & 1713 & 59.4 & 661 & 8.3 \\

Power-Law & $\sum\limits_{t=0}^{T} \frac{1}{t^{\gamma}} \beta \log \frac{p_\theta\left(\mathbf{y}_t \mid \mathbf{x,y_{<t}}\right)}{p_{ref}\left(\mathbf{y}_t \mid \mathbf{x,y_{<t}}\right)}$ & 56.8 & 57.7 & 2011 & 71.2 & 823 & 8.5 \\
\bottomrule
\end{tabular}
\end{table*}

\vspace{-0.25cm}
\paragraph{Comparisons of Various Decay Strategies}
We have proven the importance of temporal decay. Following the classic Markov Decision Process, we use exponential decay as our default decay strategy. Meanwhile, We also consider several variants of decay strategies, including Head decay, Linear decay and Power-Law decay. The detailed decay mechanism are summarized in Table \ref{tab:decay_methods}. We observe 1-0 decay and Linear Deacy show inferior results to the tenporal decay, and even underperforms with the vanilla DPO. While though Power-Law method also shows promising results, but it cannot properly control the response length competitive results with exponential decay.

\vspace{-0.25cm}
\paragraph{Lengthy Debias.}
Previous studies~\citep{Park2024DisentanglingLF,lu2024sampo,Meng2024SimPO} have demonstrated that DPO is susceptible to length exploitation, as it tends to amplify verbosity biases present in the preference datasets. This propensity can lead to suboptimal outcomes where the model's decisions are disproportionately influenced by the length of the responses rather than their quality or relevance. 
To investigate the relationship between the length bias of training data and the output length of the model, we visualized the DPO and \method loss of 1000 random samples based on the length gap between the chosen and rejected responses.
For simplicity, verbosity-biased data refers to pairs in which the chosen response must be longer than the reject response and brevity-biased data refers to the opposite type of data.

\begin{wrapfigure}[9]{r}{0.40\textwidth}
    \centering
    \vspace{-1.5em}
    \includegraphics[width=\linewidth]{Figure/Loss_with_decay.pdf}
    \vspace{-1.7em}
    \captionsetup{justification=centering}
    \caption{Loss vs. length diff.}
    \label{fig:loss_with_decay}
\end{wrapfigure}
From Figure \ref{fig:loss_with_decay}, we can see that during the DPO training process, the loss of verbosity-biased data is large, while the loss of brevity-biased data is small. Consequently, DPO prioritizes the optimization of verbosity-biased data, increasing likelihood of longer chosen responses and decreasing likelihood of shorter ones. This kind of imbalance loss can easily cause model verbosity. Meanwhile, \method reduce the loss imbalance between verbosity-biased data and brevity-biased data, thereby controlling the output length of the model.

\vspace{-0.25cm}
\paragraph{Human Evaluations}
\begin{wraptable}[7]{r}{5.0cm}
\vspace{-0.40cm}
  \setlength{\tabcolsep}{2.5pt}
  \small
  \centering
  \caption{Human evaluation results on two benchmarks.}
  \begin{tabular}{lccc}
    \toprule
    \bf Benchmark  & \bf Win & \bf Tie & \bf Lose  \\
    \midrule
    AlpacaEval 2         & 116 & 36  & 48   \\
    Arena-Hard           & 107 & 62  & 31   \\
    \bottomrule
    \end{tabular}
  \label{tab:human_evaluation}
\end{wraptable}
To further validate our results, we conducted human evaluations on the AlpacaEval2 and Arena-Hard datasets using the Gemma2-9B model. We enlisted four evaluators, with each person evaluating 50 samples for each benchmark. For each instruction, we randomized the order of the outputs from DPO and \method to prevent bias. The evaluators assessed the responses based on three criteria: accuracy, completeness, and relevance, determining which response was better for each sample. If both responses were equally correct or incorrect, the result was considered a tie. As shown in Table \ref{tab:human_evaluation}, our comparison between \method and DPO indicates that \method achieved a significantly higher win rate than DPO, with an overall win rate of 67\% in Arena-Hard and 69\% in AlpacaEval 2 (calculated as (win + tie/2) / total).


\section{Conclusions}

In this work, we revisited the loss objectives of DPO and its variants, introducing a temporal decay mechanism governed by a parameter~$\gamma$. Motivated by the observation that earlier tokens contribute more significantly during preference optimization, our dynamic weighting scheme prioritizes these initial tokens, aligning naturally with the next-token prediction paradigm. Extensive experiments demonstrate that our approach consistently outperforms vanilla DPO, achieving notable improvements across diverse benchmarks and model architectures. By enabling DPO to focus more on short-term rewards while retaining its simplicity and stability, our method offers a compelling solution for preference-based fine-tuning of large-scale models. Furthermore, we showed that our method can be extended to a reference-free, on-policy setting, outperforming existing approaches.


%\section*{Limitations}
%Our current implementation does not fully exploit the potential of the temporal decay method, largely because we have not conducted an extensive search for the optimal decay factor. Future work should involve a thorough exploration of decay parameters to enhance performance. Additionally, it remains an open question whether the temporal decay factor could be made sample-specific, adapting to different prompts and queries. For instance, employing a decay factor $\gamma$ closer to 1.0 might benefit mathematical or logical reasoning tasks, where the chain-of-thought process is crucial for achieving high accuracy.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\input{appendix}

\end{document}
