\section{Related Work}
\subsection{Reinforcement Learning from Human Feedback(RLHF)}

The classical RLHF pipeline~\citep{Christiano2017Deep,Ziegler2019Fine,Ouyang2022TrainingLM} consists of two distinct stages: The reward modeling phase and the RL phase.

\vspace{-0.25cm}
\paragraph{Reward Modeling Phase.}The reward modeling is a binary classification task. Given a prompt, the comparison pair ($y_1, y_2$) is collected by querying the supervised fine-tuning (SFT) model. Then, the preference $y_w \succ y_l$ is labeled by human which is used to train a reward model. Typically, Bradley-Terry model~\citep{Bradley1952RankAO} which quantifies the likelihood of one action being preferred over another is usually used to modeling the preference relations:
\begin{equation}
    p\left(y_1 \succ y_2 \mid x\right)=\frac{\exp \left(r\left(x, y_1\right)\right)}{\exp \left(r\left(x, y_1\right)\right)+\exp \left(r\left(x, y_2\right)\right)}=\sigma\left(r\left(x, y_1\right)-r\left(x, y_2\right)\right)
\end{equation}

\vspace{-0.25cm}
\paragraph{Reinforcement Learning Phase.} With the reward model in place, the second phase involves optimizing a policy through reinforcement learning, such as proximal policy optimization (PPO)~\citep{Schulman2017Proximal}, aiming to maximize the learned reward while ensuring the policy remains close to a predefined reference policy~\citep{Korbak2022RLWK}. This optimization is crucial for preventing model drift and maintaining alignment with human judgments, which is typically formulated as:
\begin{equation}
    \max _\theta \mathbb{E}_{x \sim D, y \sim \pi_\theta(\cdot \mid x)}\left[r_\phi(x, y)\right]-\beta \mathbb{E}_{x \sim D}\left[\mathrm{KL}\left(\pi_\theta(\cdot \mid x) \| \pi_{\operatorname{ref}}(\cdot \mid x)\right)\right]
\end{equation}

\subsection{Direct Alignment Algorithms (DAAs)}
RLHF has become a cornerstone in the training of LLMs, facilitating their alignment with human preferences. However, the classical RLHF framework~\citep{Ouyang2022TrainingLM} is characterized by a two-stage training process, which includes reward modeling, and reinforcement learning. This complexity introduces several challenges and limitations, including reward over-optimization~\citep{Gao2022ScalingLF,Dubois2023Alice,wang-etal-2024-hybrid}, training instability~\citep{Wu2023PairwisePP} and efficiency~\citep{wang2024esrl}. 
Nowadays, DAAs have emerged as a promising alternative, which can be divided into two major categories based on whether to consider a reference model.

\vspace{-0.25cm}
\paragraph{Reference-based Methods.}
The reference-based methods in DAAs utilize a pre-existing model, often a supervised fine-tuned (SFT) model, as a reference point during the optimization process. This reference model serves as a baseline to which the updated model is compared, ensuring that updates do not deviate excessively from the initial, presumably safe and aligned, model configuration. DPO~\citep{Rafailov2023DirectPO} is the most popular reference-based alignment algorithm and after its appearance, more researchers attempt to modify objective function for better performance. KTO~\citep{Ethayarajh2024KTOMA} distinguishes itself by its capability to train from non-paired preference data, providing a unique angle on optimization. IPO~\citep{Azar2023AGT} learns directly from preferences without relying on the Bradley-Terry model assumption that assumes that pairwise preferences can be substituted with pointwise rewards. R-DPO~\citep{Park2024DisentanglingLF} is an enhanced derivative of DPO, fortified with an additional regularization term designed to mitigate the tendency to exploit length biases, thus ensuring more balanced and diverse response generation.

\vspace{-0.25cm}
\paragraph{Reference-free Methods.}
In contrast to reference-based methods that depend on a pre-existing model for guidance, reference-free methods forgo the need for such a reference. They directly optimize the model parameters in response to human feedback, which can enhance the flexibility of the optimization process. However, this freedom also presents challenges in controlling the extent of updates. CPO~\citep{Xu2024ContrastivePO} leverages sequence likelihood as a reward signal and is trained in conjunction with an SFT objective. ORPO~\citep{Hong2024ORPOMP} is a novel alignment method that integrates an odds ratio-based penalty into the supervised fine-tuning process. SimPO~\citep{Meng2024SimPO} uses an average log probability as an implicit reward and introduces a target reward margin to enhance performance without relying on a reference model.