\newpage
\appendix

\section{Experimental Setups}
\label{appendix:exp_setup}

% \vspace{-0.25cm}
\paragraph{Evaluation Benchmarks.} We primarily evaluated our models using three widely used open-ended instruction-following benchmarks: MT-Bench~\citep{zheng2023judging}, AlpacaEval 2~\citep{AlpacaEval}\footnote{\url{https://tatsu-lab.github.io/alpaca_eval/}}, and Arena-Hard v0.1~\citep{arenahard2024}. These benchmarks assess the models' versatile conversational capabilities across a diverse set of queries and are widely adopted by the research community. Concretely, AlpacaEval 2 comprises 805 questions from 5 datasets, while MT-Bench encompasses 8 categories with 80 questions. Arena-Hard, an enhanced version of MT-Bench\footnote{\citet{Adler2024Nemotron} discussed the existence of incorrect reference answers in MT-Bench, therefore a corrected version of MT-Bench was used.}, includes 500 rigorously defined technical problem-solving queries. 
For AlpacaEval 2, we used \texttt{alpaca\_eval\_gpt4\_turbo\_fn} as the annotator which has a higher human agreement and report both the raw win rate (WR) and the length-controlled win rate (LC)~\citep{dubois2024length}, with the LC metric designed to be robust against model verbosity. For Arena-Hard, we reported the WR against the baseline model. For MT-Bench, we report the average MT-Bench score, using \texttt{GPT-4-Turbo-2024-04-09} as the judge model\footnote{\texttt{GPT-4-Turbo-2024-04-09} provides more accurate reference answers and judgments compared to GPT-4.}.

\vspace{-0.25cm}
\paragraph{Baselines.} We selected several advanced preference optimization baselines, including: 
IPO~\citep{Azar2023AGT} is a theoretically grounded method that avoids DPO's assumption that pairwise preferences can be replaced with pointwise rewards. 
%CPO~\citep{xu2024contrastive} uses sequence likelihood as a reward and trains alongside an SFT objective. 
KTO~\citep{Ethayarajh2024KTOMA} learns from non-paired preference data. ORPO~\citep{Hong2024ORPOMP} introduces a reference-model-free odds ratio term to directly contrast winning and losing responses with the policy model, jointly training with the SFT objective. SimPO~\citep{Meng2024SimPO} and SamPO~\citep{lu2024sampo} are both designed to address the issue of model verbosity by applying length normalization. 
We meticulously tune the hyperparameters for each baseline and report the best performance.

\vspace{-0.25cm}
\paragraph{AlpacaEval 2 Annotator Choice}
AlpacaEval 2 provides various evaluation templates and in the official readme recommends using \texttt{weighted\_alpaca\_eval\_gpt4\_turbo} as well as \texttt{alpaca\_eval\_gpt4\_turbo\_fn}. The former is the default annotator in AlpacaEval 2 with a human agreement rate of 65.7\% and much cheaper price. In all of our evaluations, we used the latter as the annotator which has a higher agreement rate of 68.1\% with human annotation data.

\section{Experimental Details}
\label{sec:exp_details}
Considering DPO is a special case of \method when $\gamma=1.0$, to compare the effects of different decay coefficients, we conduct experiments with $\beta$ fixed at 0.1. In addition, we follow the optimal hyperparameters claimed in SimPO and our code is built on \href{https://github.com/hiyouga/LLaMA-Factory}{LlamaFactory~\citep{zheng2024llamafactory}}. Across all DAAs run, the models were trained on 32 A100 with a global batch size of 128 (4 gradient accumulation steps). The hyperparameter search range of all methods are displayed in the Table \ref{tab:all_hyperparameter}.



\section{More Analyses}
\paragraph{When to Decay}
In our default setting, we apply decay from the beginning of the prompt rather than from the first generated tokens. Here, we investigate the implications of these two approaches. Theoretically, during the loss computation, both the chosen token $y_w$ and the rejected token $y_l$ share the same prompt prefix, which results in distinct initial scaling coefficients for the reward at the first generated position. For illustration, if the prompt length is $l$, then in our default setting, the reward for the first generated token is scaled by $y_l$ while in the alternate setting, the scaling factor would be 1. Through results in Figure \ref{fig:alpaca_eval2_lc_wr}, we can see that both two settings achieve better results than DPO, and our default setting is much better than the other one. This indicates that proper scaling factor is very important during preference optimization.

% ############Figure############
\input{when_to_decay}
% ############Figure############


% \setlength{\tabcolsep}{4pt}
\begin{table*}[!h]
%\vspace{-1em}
\caption{Various preference optimization objectives and hyperparameter search range.}
\label{tab:hyperparams_baseline}
\centering
\resizebox{\textwidth}{!}{
\small
\begin{tabular}{lll}
\toprule 
\textbf{Method} & \textbf{Objective} & \textbf{Hyperparameter} \\ \midrule
% \midrule 
DPO~\citep{Rafailov2023DirectPO} & $-\log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)$ & $\beta \in [0.01, 0.1]$ \\ \midrule 
IPO~\citep{Azar2023AGT} & $ \left( \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} - \frac{1}{2\tau} \right)^2$ & $\tau \in [0.01, 0.1, 0.5, 1.0]$ \\  \midrule 
\multirow{2}{*}{KTO~\citep{Ethayarajh2024KTOMA}} & $-\lambda_w \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - z_{\text{ref}} \right) +  \lambda_l \sigma \left( z_{\text{ref}} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right),\,$ & $\lambda_l = \lambda_w = 1.0$ \\ 
& $\text{where} \,\, z_{\text{ref}} = \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[\beta \text{KL}\left( \pi_\theta(y|x) || \pi_{\text{ref}}(y|x) \right)  \right]$ & $\beta \in [0.01, 0.05, 0.1]$ \\ \midrule
\multirow{2}{*}{SamPO~\citep{lu2024sampo}} & \multirow{2}{*}{$-\log \sigma\left(\sum\limits_{t=0}^{T_m} \beta \log \frac{\pi_\theta\left(\mathbf{y}_w^t \mid \mathbf{x},\mathbf{y}_w^{<t}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_w^t \mid \mathbf{x},\mathbf{y}_w^{<t}\right)}-\sum\limits_{t=0}^{T_m} \beta \log \frac{\pi_\theta\left(\mathbf{y}_l^t \mid \mathbf{x},\mathbf{y}_l^{<t}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_l^t \mid \mathbf{x},\mathbf{y}_l^{<t}\right)}\right)$} & $\beta \in [0.01,0.1]$ \\ [1ex]
& & \\ 
& $\text{where} \,\, T_m=min(T_w,T_l),y^t \sim Uniform(T_m, {y}^T)$ & \\ \midrule
\multirow{2}{*}{ORPO~\citep{Hong2024ORPOMP}} & $-\log p_\theta(y_w|x) - \lambda  \log \sigma \left(\log \frac{p_\theta(y_w|x)}{1 - p_\theta(y_w|x)} - \log \frac{p_\theta(y_l|x)}{1 - p_\theta(y_l|x)}  \right),\,$ & \multirow{2}{*}{$\lambda \in [0.1, 0.5, 1.0, 2.0]$} \\  
& $\text{where} \,\, p_\theta(y|x) = \exp\left( \frac{1}{|y|} \log \pi_\theta(y|x) \right)$ \\  
& & \\
\midrule 
\multirow{2}{*}{SimPO~\citep{Meng2024SimPO}} & \multirow{2}{*}{$-\log \sigma  \left( \frac{\beta}{|y_w|} \log \pi_\theta(y_w|x) - \frac{\beta}{|y_l|} \log \pi_\theta(y_l|x) - \gamma \right)$} & $\beta \in [2.5, 10]$ \\
& & $\gamma \in [0.3, 0.5, 1.0]$ \\
\midrule 
\multirow{2}{*}{\method} & \multirow{2}{*}{$-\log \sigma\left(\sum\limits_{t=0}^{T_w} \textcolor{red}{\gamma^t} \beta \log \frac{\pi_\theta\left(\mathbf{y}_w^t \mid \mathbf{x},\mathbf{y}_w^{<t}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_w^t \mid \mathbf{x},\mathbf{y}_w^{<t}\right)}-\sum\limits_{t=0}^{T_l} \textcolor{red}{\gamma^t} \beta \log \frac{\pi_\theta\left(\mathbf{y}_l^t \mid \mathbf{x},\mathbf{y}_l^{<t}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_l^t \mid \mathbf{x},\mathbf{y}_l^{<t}\right)}\right)$} & $\beta \in [0.1]$ \\ [1ex]
& & $\gamma \in [0.95,0.97,0.98,0.99]$ \\ [1ex]
\bottomrule
\end{tabular}
}
\label{tab:all_hyperparameter}
\end{table*}

\vspace{-0.25cm}
\paragraph{Effect of Different Judge Models}
Here, we mainly evaluated our generated results via \texttt{ GPT-4-Turbo-0409}, while previous work mainly used \texttt{GPT-4-preview-1106} instead. Results in Table \ref{tab:main_res_online_1106} show that \method delivers consistent performance gains in both two judge models.



\paragraph{Full evaluation results}
We present the full evaluation of AlpacaEval 2, Arena-Hard and MT-Bench in Table \ref{tab:full_off_policy_res} and Table \ref{tab:full_on_policy_res}. The former is an off-policy setup, while the latter is an on-policy setup. For on-policy setups, we found that DPO can achieve better performance when $beta=0.01$ and reported this result for fair comparison. Specifically, ``-'' indicates that the model suffered a collapse during training.


\setlength{\tabcolsep}{2pt}
\begin{table*}[!t]
\centering
\small 
\caption{Three benchmarks results with on-policy setups, using gpt-4-1106-preview as the judge model. $\dagger$ denotes our reference-free version.}
\begin{tabular}{lcccccccccccc}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{Llama3-Instruct (8B)}} & \multicolumn{3}{c}{\textbf{Gemma2-Instruct (9B)}} & \multicolumn{3}{c}{\textbf{Mistral-NeMo-Instruct (12B)}}\\ 
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
& \multicolumn{2}{c}{\textbf{AE2}} & \multicolumn{1}{c}{\textbf{AH}} & \multicolumn{2}{c}{\textbf{AE2}} & \multicolumn{1}{c}{\textbf{AH}} & \multicolumn{2}{c}{\textbf{AE2}} & \multicolumn{1}{c}{\textbf{AH}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-4}\cmidrule(lr){5-6}\cmidrule(lr){7-7}\cmidrule(lr){8-9}\cmidrule(lr){10-10}
& {\scriptsize \bf WR (\%)} & {\scriptsize \bf LC (\%)} & {\scriptsize \bf WR (\%)} & {\scriptsize \bf WR (\%)}  & {\scriptsize \bf LC (\%)} & {\scriptsize \bf WR (\%)} & {\scriptsize \bf WR (\%)}  & {\scriptsize \bf LC (\%)} & {\scriptsize \bf WR (\%)} \\
\midrule
SFT          & 31.6  & 31.7  & 19.7 & 37.7 & 48.2 & 39.9 & 40.8 & 44.2 & 39.7 \\
\midrule
DPO          & 41.7 & 42.9 & 31.2 & 46.4 & 53.1 & 47.4 & 53.4 & 52.6 & 47.4 \\
KTO          & 35.7 & 37.7 & 25.2 & 46.5 & 52.3 & 49.2 & 45.9 & 49.7 & 45.4 \\
IPO          & 40.1 & 43.2 & 25.2 & 49.1 & 48.3 & 49.5 & 51.7 & 52.9 & \textbf{51.8} \\
SamPO        & 39.4 & 41.9 & 28.7 & 46.9 & 56.7 & 50.4 & 49.8 & 52.5 & 50.4 \\
\method (ours)       & \textbf{44.5} & \textbf{50.1} & \textbf{34.1} & \textbf{59.3} & \textbf{62.3} & \textbf{58.4} & \textbf{55.0} & \textbf{60.6} & 50.6 \\
\midrule
ORPO          & 31.5 & 32.5 & 20.9 & 39.8 & 48.2 & 41.3 & 40.1 & 44.5 & 41.2 \\
SimPO        & 44.5 & 49.1 & \textbf{33.1} & 55.1 & 59.4 & 56.5 & 50.7 & 53.8 & \textbf{51.0} \\
$\textrm{D}^2$PO$^\dagger$ (ours)    & \textbf{45.0} & \textbf{51.9} & 33.0 & \textbf{58.0} & \textbf{61.5} & \textbf{56.9}& \textbf{56.8} & \textbf{59.9} & 49.4 \\
\bottomrule
\end{tabular}
% }
\label{tab:main_res_online_1106}
\vspace{-.5em}
\end{table*}

\paragraph{Arena-Hard Style Control Evaluation}
\begin{wraptable}[9]{r}{7.0cm}
% \vspace{0cm}
  \setlength{\tabcolsep}{2.5pt}
  \small
  \centering
  \caption{Style Control evaluation on the Arena-Hard benchmark.}
  \begin{tabular}{lcc}
    \toprule
    \bf Model  & \bf AH & \bf AH (Style-Control)  \\
    \midrule
    \method               & 66.4 & 67.2     \\
    DPO ($\beta=0.1$)     & 56.7 & 57.2  \\
    DPO ($\beta=0.01$)    & 65.2 & 66.4  \\
    SimPO                 & 65.0 & 66.3  \\
    \bottomrule
    \end{tabular}
  \label{tab:style_control}
\end{wraptable}
We have conducted evaluations using the Arena-Hard benchmark, focusing on style control capabilities based on Gemma2-9B. Through results in Table \ref{tab:style_control}, we can see that \method consistently achieves superior performance, even when style control is a key factor, highlighting our method's effectiveness in style-controlled settings.


\setlength{\tabcolsep}{4pt}
\begin{table*}[!t]
\centering
\small
\caption{Full results on benchmarks under off-policy setups.}

% \resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccccc}
% \begin{tabular}{lrrrrrrrrrr}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{AlpacaEval 2}} & \multicolumn{2}{c}{\textbf{Arena Hard}} & \multicolumn{1}{c}{\textbf{MT-Bench}} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-6} \cmidrule(lr){7-7}
& {\scriptsize \bf Win Rate (\%)} & {\scriptsize \bf LC Win Rate (\%)} & {\scriptsize \bf Len.} & {\scriptsize \bf Win Rate (\%)} & {\scriptsize \bf Len.} & {\scriptsize \bf G4-Turbo} \\
\midrule
%\midrule
\multicolumn{7}{c}{\textbf{Llama3-Instruct (8B)}} \\
\midrule
SFT            & 39.05 & 40.13 & 1971 & 27.6 & 581 & 7.5 \\
DPO            & 37.38 & 40.28 & 1880 & 27.7 & 546 & \textbf{7.7} \\
KTO            & 33.29 & 38.06 & 1765 & 21.0 & 525 & 7.5 \\
IPO            & 42.16 & 45.66 & 1845 & 31.9 & 542 & 7.6 \\
SamPO          & 40.68 & 43.11 & 1891 & 26.1 & 550 & 7.5 \\
\method ($\gamma$=0.95)       & 45.90 & 44.78 & 2113 & 40.0 & 636 & 8.0 \\
\method ($\gamma$=0.97)       & 43.46 & 43.04 & 1994 & 37.0 & 602 & 7.7 \\
\method ($\gamma$=0.98)       & 42.73 & 44.10 & 1954 & 35.1 & 578 & 7.9 \\
\method ($\gamma$=0.99)       & 41.74 & 44.03 & 1912 & 30.4 & 560 & 8.0 \\
\method ($\gamma$=1.01)       & 38.50 & 40.21 & 1928 & 26.1 & 569 & 7.5 \\
\method ($\gamma$=1.02)       & 37.69 & 38.63 & 1955 & 26.8 & 572 & 7.4 \\
\midrule
ORPO           & 10.62 & 15.32 & 1386 & 6.8 & 764 & 6.3 \\
SimPO          & 0.25 & 0.80 & 27 & 1.4 & 15 & 1.6 \\
%\method $\gamma$=0.97       & 44.97 & 44.58 & 2089 & 29.1 & 608 & 8.1 \\

\midrule
%\midrule

\multicolumn{7}{c}{\textbf{Gemma2-Instruct (9B)}} \\
\midrule
%\midrule
SFT            & 37.58 & 47.23 & 1566 & 44.1 & 608 & 8.3 \\
DPO            & 38.81 & 48.83 & 1546 & 42.5 & 595 & 8.1 \\
KTO            & 39.07 & 50.00 & 1530 & 43.8 & 540 & 8.3 \\
IPO            & 41.04 & 50.03 & 1630 & 48.2 & 608 & 8.1 \\
SamPO          & 39.86 & 50.06 & 1574 & 46.9 & 596 & 8.2 \\
\method ($\gamma$=0.95)      & 48.07 & 50.05 & 1929 & 53.4 & 657 & 8.5 \\
\method ($\gamma$=0.97)      & 45.34 & 49.70 & 1824 & 50.7 & 636 & 8.3 \\
\method ($\gamma$=0.98)      & 45.46 & 50.99 & 1746 & 50.2 & 625 & 8.3 \\
\method ($\gamma$=0.99)       & 42.10 & 50.05 & 1636 & 50.2 & 609 & 8.4 \\
\method ($\gamma$=1.01)       & 38.57 & 47.45 & 1577 & 43.7 & 612 & 8.3 \\
\method $(\gamma$=1.02)       & - & - & - & - & - & - \\
\midrule
ORPO           & 11.30 & 21.55 & 1182 & 10.2 & 641 & 7.1 \\
SimPO          & 38.76 & 50.00 & 1508 & 31.6 & 475 & 8.0 \\
%\method $\gamma$=0.97       & 46.96 & 49.84 & 1903 & 45.2 & 615 & 8.2 \\

\midrule
%\midrule

\multicolumn{7}{c}{\textbf{Mistral-NeMo-Instruct (12B)}} \\
\midrule
%\midrule
SFT            & 44.60 & 47.71 & 1879 & 46.5 & 575 & 8.1 \\
DPO            & 44.41 & 49.25 & 1821 & 48.5 & 569 & 8.3 \\
KTO            & 37.39 & 48.68 & 1620 & 35.8 & 501 & 8.2 \\
IPO            & 39.75 & 48.85 & 1634 & 39.8 & 506 & 8.2 \\
SamPO          & 43.54 & 49.47 & 1784 & 50.1 & 562 & 8.1 \\
\method ($\gamma$=0.95)       & 52.17& 52.42 & 2017 & 54.2 & 590 & 8.3 \\
\method ($\gamma$=0.97)       & 51.30 & 54.43 & 1879 & 51.8 & 562 & 8.4 \\
\method ($\gamma$=0.98)       & 49.57 & 55.43 & 1778 & 47.8 & 534 & 8.3 \\
\method ($\gamma$=0.99)       & 46.96 & 53.86 & 1770 & 45.9 & 538& 8.0 \\
\method ($\gamma$=1.01)       & 43.65 & 47.60 & 1829 & 46.8 & 564 & 8.1 \\
\method ($\gamma$=1.02)       & 43.84 & 47.91 & 1840 & 45.6 & 572 & 8.0 \\
\midrule
ORPO           & 9.64 & 17.00 & 1185 & 9.8 & 640 & 6.9 \\
SimPO          & 46.77 & 53.28 & 1704 & 46.6 & 500 & 8.0 \\
%\method  $\gamma$=0.97      & 53.42 & 52.35 & 2128 & 47.9 & 612 & 8.3 \\

\bottomrule
\end{tabular}
% }
\label{tab:full_off_policy_res}
\vspace{-.5em}
\end{table*}


\clearpage
\setlength{\tabcolsep}{4pt}
\begin{table*}[!t]
\centering
\small
\caption{Full results on benchmarks under on-policy setups. $\dagger$ denotes our reference-free version.}

% \resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccccc}
% \begin{tabular}{lrrrrrrrrrr}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{AlpacaEval2}} & \multicolumn{2}{c}{\textbf{Arena Hard}} & \multicolumn{1}{c}{\textbf{MT-Bench}} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-6} \cmidrule(lr){7-7}
& {\scriptsize \bf Win Rate (\%)} & {\scriptsize \bf LC Win Rate (\%)} & {\scriptsize \bf Len.} & {\scriptsize \bf Win Rate (\%)} & {\scriptsize \bf Len.} & {\scriptsize \bf G4-Turbo} \\
\midrule
\multicolumn{7}{c}{\textbf{Llama3-Instruct (8B)}} \\
\midrule
SFT            & 39.05 & 40.13 & 1971 & 27.6 & 581 & 7.5 \\
DPO ($\beta$=0.01)           & 48.26 & 49.93 & 1937 & 45.2 & 568 & 7.8 \\
KTO            & 42.36 & 44.77 & 1901 & 32.1 & 545 & 7.7 \\
IPO            & 42.92 & 45.99 & 1889 & 34.5 & 553 & 7.9 \\
SamPO          & 44.35 & 47.17 & 1890 & 35.8 & 536 & 8.0 \\
\method ($\gamma$=0.95)       & 48.13 & 51.53 & 1832 & 42.5 & 578 & 7.7 \\
\method ($\gamma$=0.97)       & 46.15 & 50.52 & 1739 & 41.6 & 549 & 7.7 \\
\method ($\gamma$=0.98)       & 47.39 & 53.54 & 1705 & 47.3 & 518 & 7.8 \\
\method ($\gamma$=0.99)       & 48.01 & 52.97 & 1739 & 44.0 & 514 & 7.8 \\
DPO ($\beta$=0.1)          & 46.21 & 47.60 & 1971 & 42.4 & 627 & 7.9 \\
\method ($\gamma$=1.01)       & 37.25 & 38.32 & 1948 & 28.1 & 578 & 7.6 \\
\method ($\gamma$=1.02)       & 37.75 & 39.30 & 1942 & 26.2 & 566 & 7.6 \\
\midrule
ORPO           & 37.75 & 39.29 & 1934 & 25.5 & 615 & 7.7 \\
SimPO          & 44.41 & 50.34 & 1704 & 41.9 & 477 & 7.8 \\
$\textrm{D}^2$PO$^\dagger$ ($\gamma$=0.98)       & 48.01 & 53.87 & 1726 & 46.1 & 526 & 7.7 \\
\midrule
%\midrule

\multicolumn{7}{c}{\textbf{Gemma2-Instruct (9B)}} \\
\midrule
%\midrule
SFT            & 37.58 & 47.23 & 1566 & 44.1 & 608 & 8.3 \\
DPO ($\beta$=0.01)           & 54.53 & 57.05 & 1948 & 65.2 & 768 & 8.3 \\
KTO            & 48.26 & 53.39 & 1775 & 57.1 & 705 & 8.3 \\
IPO            & 50.86 & 50.00 & 2129 & 59.7 & 759 & 8.3 \\
SamPO          & 45.78 & 55.21 & 1662 & 55.2 & 668 & 8.2 \\
\method ($\gamma$=0.95)       & 58.39 & 59.03 & 2034 & 65.5 & 739 & 8.5 \\
\method ($\gamma$=0.97)       & 56.83 & 59.25 & 1949 & 64.8 & 715 & 8.3 \\
\method ($\gamma$=0.98)       & 57.20 & 59.71 & 1950 & 66.4 & 724 & 8.3 \\
\method ($\gamma$=0.99)       & 53.98 & 57.38 & 1843 & 63.9 & 693 & 8.2 \\
DPO ($\beta$=0.1)          & 47.02 & 53.43 & 1737 & 56.7 & 682 & 8.3 \\
\method ($\gamma$=1.01)       & 38.70 & 48.06 & 1592 & 43.7 & 610 & 8.2 \\
\method ($\gamma$=1.02)       & - & - & - & - & - & - \\
\midrule
ORPO           & 41.93 & 51.14 & 1647 & 45.3 & 641 & 8.2 \\
SimPO          & 54.47 & 58.42 & 1871 & 65.0 & 744 & 8.3 \\
$\textrm{D}^2$PO$^\dagger$ ($\gamma$=0.98)       & 56.71 & 60.76 & 1894 & 65.7 & 687 & 8.3 \\
\midrule
%\midrule

\multicolumn{7}{c}{\textbf{Mistral-Nemo-Instruct (12B)}} \\
\midrule
%\midrule
SFT            & 44.60 & 47.71 & 1879 & 46.5 & 575 & 8.1 \\
DPO  ($\beta$=0.01)          & 58.76 & 57.29 & 2160 & 63.6 & 659 & 8.3 \\
KTO            & 48.26 & 53.39 & 1775 & 57.1 & 705 & 8.3 \\
IPO            & 50.86 & 50.00 & 2129 & 59.7 & 759 & 8.3 \\
SamPO          & 45.78 & 55.21 & 1662 & 55.2 & 668 & 8.2 \\
\method  ($\gamma$=0.95)      & 59.25 & 57.85 & 2167 & 60.7 & 665 & 8.6 \\
\method  ($\gamma$=0.97)      & 56.77 & 58.65 & 1969 & 57.5 & 586 & 8.6 \\
\method  ($\gamma$=0.98)      & 57.34 & 62.07 & 1853 & 62.3 & 546 & 8.6 \\
\method  ($\gamma$=0.99)      & 54.29 & 58.83 & 1816 & 59.3 & 532 & 8.4 \\
DPO ($\beta$=0.1)           & 53.48 & 53.32 & 2081 & 59.0 & 624 & 8.4 \\
\method  ($\gamma$=1.01)      & 45.34 & 48.06 & 1908 & 44.2 & 581 & 8.0 \\
\method  ($\gamma$=1.02)      & - & - & - & - & - & - \\
\midrule
ORPO           & 41.93 & 51.14 & 1647 & 45.3 & 641 & 8.2 \\
SimPO          & 54.47 & 58.42 & 1871 & 65.0 & 744 & 8.3 \\
$\textrm{D}^2$PO$^\dagger$ ($\gamma$=0.98)      & 56.71 & 60.76 & 1894 & 65.7 & 687 & 8.3 \\
\bottomrule
\end{tabular}
% }
\label{tab:full_on_policy_res}
\vspace{-.5em}
\end{table*}

\clearpage
\paragraph{Stronger Instruct Model Evaluation}
To verify the robustness properties of our method, we conducted experiments under on-policy setups based on a stronger Instruct model, Gemma2-Instruct (27B). Considering the limited computing resources, we only compared \method with DPO and SimPO. Table \ref{tab:gemma2_27b_result} shows the evaluation results on three benchmarks, demonstrating that our method maintains a certain advantage over stronger models. 
\setlength{\tabcolsep}{4pt}
\begin{table*}[!htbp]
\centering
\small
\caption{Gemma2-Instruct (27B) results under on-policy setups}

% \resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccccc}
% \begin{tabular}{lrrrrrrrrrr}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{AlpacaEval2}} & \multicolumn{2}{c}{\textbf{Arena Hard}} & \multicolumn{1}{c}{\textbf{MT-Bench}} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-6} \cmidrule(lr){7-7}
& {\scriptsize \bf Win Rate (\%)} & {\scriptsize \bf LC Win Rate (\%)} & {\scriptsize \bf Len.} & {\scriptsize \bf Win Rate (\%)} & {\scriptsize \bf Len.} & {\scriptsize \bf G4-Turbo} \\
\midrule
SFT            & 39.68 & 48.13 & 1633 & 60.7 & 648 & 8.5 \\
DPO ($\beta$=0.1)           & 48.45 & 53.66 & 1786 & 71.2 & 715 & 8.5 \\
DPO ($\beta$=0.01)           & 57.39 & 59.05 & 1990 & 78.6 & 811 & \textbf{8.6} \\
SimPO            & 54.29 & \textbf{61.30} & 1748 & 75.3 & 682 & 8.5 \\
\method ($\gamma$=0.98)       & \textbf{57.52} & 60.52 & 1893 & \textbf{79.8} & 708 & \textbf{8.6} \\
\bottomrule
\end{tabular}
\label{tab:gemma2_27b_result}
\end{table*}



\section{Case Analysis}
We list two concrete cases from AlpacaEval2 to compare \method with DPO. Regarding the first case in Figure \ref{tab:gemma_case_study}, the DPO model's output overthinks and mistakenly assumes that the stopover time is already included in the total travel time in the second step of its analysis, resulting in the omission of this time in the final calculation. In contrast, the model trained with our method succinctly lists the times and correctly sums them to obtain the total travel time. Figure \ref{tab:mistral_case_study} shows the second case: given the question, ``What is the largest ocean in the world?'', the DPO model provides a response without explanation. In contrast, the \method model produces a well-structured answer with detailed explanations, making the answer more credible.

\begin{figure}[!h]
    \caption{A case examples of AlpacaEval2, generated by Gemma2-Instruct (9B).}\label{tab:gemma_case_study}
    { \tt \footnotesize \begin{tabular}{>{\raggedleft\arraybackslash}p{0.3\linewidth} p{0.7\linewidth}} \toprule
    \textbf{Input} $\to$ & Q: What is the largest ocean in the world? \\
    \midrule \textbf{Gemma2-9b-it DPO} $\to$ & The largest ocean in the world is the \textbf{Pacific Ocean}.  \\
    \midrule \textbf{Gemma2-9b-it \method} $\to$ & The \textbf{Pacific Ocean} is the largest ocean in the world. \\ \\ 
    & Here's why:\\ \\
    & * \textbf{Surface Area:} It covers over 30\% of the Earth's surface, roughly 63.8 million square miles (165.25 million square kilometers).\\ 
    & * \textbf{Volume:} It holds about half of the Earth's total water.\\
    & * \textbf{Depth:}  The Pacific Ocean boasts the deepest point on Earth, the Challenger Deep in the Mariana Trench, which plunges over 36,000 feet (10,900 meters) below sea level. \\ \\
    & Let me know if you'd like to know more about the Pacific Ocean or other oceans! \\
    \bottomrule
    \end{tabular} }  
\end{figure}

\begin{figure}[!t] 
    \caption{A case examples of AlpacaEval2, generated by Mistral-Nemo-Instruct (12B).}
    \label{tab:mistral_case_study}
    { \tt \footnotesize \begin{tabular}{>{\raggedleft\arraybackslash}p{0.3\linewidth} p{0.7\linewidth}} \toprule
    \textbf{Input} $\to$ & Q: if it takes 8 hours to fly to Singapore, stop over for an hour and a half and then fly 14 hours to London then what is the total travel time if the plane is delayed in Singapore? \\
    \midrule \textbf{Mistral-Nemo DPO} $\to$ & To calculate the total travel time including the delay in Singapore, we need to add up all the components of the journey: \\
    %&  \\
    & 1. Flight to Singapore: 8 hours \\
    & 2. Stopover in Singapore: 1.5 hours (which is already included in the total time as it's part of the journey, not a delay)\\
    & 3. Delay in Singapore: Let's assume the delay is 'D' hours.\\ 
    & 4. Flight from Singapore to London: 14 hours \\ \\ 
    & The total travel time including the delay would be:\\
    & \\
    & Total travel time = Flight to Singapore + Delay in Singapore + Flight to London\\ 
    & Total travel time = 8 hours + D hours + 14 hours\\
    & Without knowing the exact duration of the delay (D), we can express the total travel time as:\\
    & Total travel time = 22 hours + D\\ \\
    & For example, if the delay in Singapore is 2 hours, the total travel time would be:\\
    & \\
    & Total travel time = 22 hours + 2 hours = 24 hours \\
    & So, the total travel time including the delay is 22 hours plus the duration of the delay (D). \\
    \midrule \textbf{Mistral-Nemo \method} $\to$ & To calculate the total travel time including the delay in Singapore, let's break down the journey and add the delay: \\ \\
    & Total initial travel time without delay: \\
    & - Flight to Singapore: 8 hours \\
    & - Stopover in Singapore: 1.5 hours \\
    & - Flight from Singapore to London: 14 hours \\ \\
    & Total without delay = 8 + 1.5 + 14 = 23.5 hours \\ \\
    & Now, let's assume the plane is delayed in Singapore by 'D' hours. The delay will only affect the stopover time, so we add 'D' to the stopover duration: \\ \\
    & Adjusted stopover time with delay = 1.5 hours + D hours\\
    & Now, the total travel time with the delay is: \\
    & Total with delay = 8 hours (flight to Singapore) + (1.5 + D) hours (stopover with delay) + 14 hours (flight to London) \\
    & Total with delay = 8 + (1.5 + D) + 14 \\
    & Total with delay = 23.5 + D hours \\
    & Without knowing the specific duration of the delay (D), we can express the total travel time with the delay as: \\
    & Total travel time with delay = 23.5 hours + Delay in Singapore (D hours) \\
    \bottomrule
    \end{tabular} }  
\end{figure}

\clearpage
\newpage
\section{Derivation of \method}
\label{sec:derivation}
In the reinforcement learning scenario, two fundamental concepts are the state-value function V and the action-value function Q. The former represents the expected cumulative reward from taking action a in state s, meanwhile the latter represents the reward under state s. We extend the relation between the Q-function and the V-function under a KL divergence constraint, as proposed in \citet{rafailov2024fromr}, by incorporating the temporal decay mechanism:
\begin{equation}
    Q^*\left(\mathbf{s}_t, \mathbf{a}_t\right)= \begin{cases}r\left(\mathbf{s}_t, \mathbf{a}_t\right)+\beta \log \pi_{\mathrm{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_t\right)+\gamma V^*\left(\mathbf{s}_{t+1}\right), & \text { if } \mathbf{s}_{t+1} \text { is not terminal } \\ r\left(\mathbf{s}_t, \mathbf{a}_t\right)+\beta \log \pi_{\mathrm{ref}}\left(\mathbf{a}_t \mid \mathbf{s}_t\right), & \text { if } \mathbf{s}_{t+1} \text { is terminal }\end{cases}
\end{equation}
where $\gamma \in (0,1]$ represents the discount factor. In the assumption of DPO and its subsequent variants, $\gamma$ is typically set to 1 which indicates long-term returns do not need to decay. However, in auto-regressive scenarios such as language models, the longer the context provided, the lower the uncertainty of the model's predictions (see Figure \ref{fig:pred_prob}). Therefore, the tokens at the beginning of the response make greater contributions to the total return. Based on this assumption, we can get the formulation of the reward over a trajectory $\tau = \{s_1, a_1, . . . , a_{T-1}, s_T \}$:
\begin{align}
\sum\limits_{t=0}^{T-1}\gamma^tr(s_t, a_t)&=\sum\limits_{t=0}^{T-1}\gamma^tQ^*(s_t, a_t)-\gamma^t\beta \log \pi_{ref}(a_t, s_t) - \gamma^{t+1} V^*(s_{t+1})
\label{eq:decay_reward1}
\end{align}
Noting that, in the general maximum entropy RL setting\revised{~\citep{ziebart2008maximum,ziebart2010maximum}}, the optimal policy is given by Boltzmann probability distribution as:
\begin{equation}
\pi^*\left(\mathbf{a}_t \mid \mathbf{s}_t\right)=e^{\left(Q^*\left(\mathbf{s}_t, \mathbf{a}_t\right)-V^*\left(\mathbf{s}_t\right)\right) / \beta}\label{optimal_policy}
\end{equation}

By taking the logarithm of the Eq. (\ref{optimal_policy}), we can simplified Eq. (\ref{eq:decay_reward1}):
\begin{align}
\sum\limits_{t=0}^{T-1}\gamma^tr(s_t, a_t)
&=Q^*(s_0, a_0) - \beta \log \pi_{ref}(a_0, s_0) + \sum\limits_{t=1}^{T-1}(\gamma^t\beta \log \frac{\pi_{\theta}(a_t, s_t)}{\pi_{ref}(a_t,s_t)}) \\
&=V^*(s_0) + \sum\limits_{t=1}^{T-1}(\gamma^t\beta \log \frac{\pi_{\theta}(a_t, s_t)}{\pi_{ref}(a_t,s_t)})\label{eq:decay_reward2}
\end{align}

We can then plug Eq. (\ref{eq:decay_reward2}) into the Bradley-Terry ranking objective, which yields our final loss formation:
\begin{align}
\mathcal{L}_{\textrm{D}^2\textrm{PO}}\left(\pi_\theta\right)
    &= -\log \sigma\left(\sum\limits_{t=0}^{T_w} \gamma^tr(s_t, a_t) - \sum\limits_{t=0}^{T_l} \gamma^tr(s_t, a_t)\right) \\
    &=-\log \sigma\left(\sum\limits_{t=0}^{T_w} \textcolor{red}{\gamma^t} \beta \log \frac{\pi_\theta\left(\mathbf{y}_w^t \mid \mathbf{x},\mathbf{y}_w^{<t}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_w^t \mid \mathbf{x},\mathbf{y}_w^{<t}\right)}-\sum\limits_{t=0}^{T_l} \textcolor{red}{\gamma^t} \beta \log \frac{\pi_\theta\left(\mathbf{y}_l^t \mid \mathbf{x},\mathbf{y}_l^{<t}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_l^t \mid \mathbf{x},\mathbf{y}_l^{<t}\right)}\right)
\end{align}

%as discussed in Eq. (\ref{eq:d^2po}).
This is similar to the standard DPO objective, except for an additional temporay decay term $\gamma$. We empirically set $\gamma < 1$ to focus more on short term return rather than long term return.

\clearpage
\newpage

\section{theoretical analysis}
\label{sec:theorem_proof}
In this section, we provide the detailed derivation of the upper bound for

\begin{equation}
\begin{aligned}
\text{SubOpt}(\pi, s; \gamma_e) &= V_{\gamma_e}^{\pi^*}(s) - V_{\gamma_e}^{\pi}(s) \\
&= \underbrace{\left[V_{\gamma_e}^{\pi^*}(s) - V_{\gamma}^{\pi^*}(s)\right]}_{\Delta_1} + \underbrace{\left[V_{\gamma}^{\pi^*}(s) - V_{\gamma}^{\pi}(s)\right]}_{\Delta_2} + \underbrace{\left[V_{\gamma}^{\pi}(s) - V_{\gamma_e}^{\pi}(s)\right]}_{\Delta_3}
\end{aligned}
\end{equation}

%DPO and our method can both be considered as token-level MDP which satisfies the Bellman equation. In order to analysis the influence of temporary decay factor $\gamma$, we define the suboptimality as the performance difference
% of the optimal policy $\pi^*$ and the policy $\pi$ with discount factor $\gamma$ and initial state $s$:
%\begin{equation}
%\text { SubOpt }\left(\pi,s ; \gamma\right)=V_{\gamma}^{\pi^{*}}(s)-V_{\gamma}^{\pi}(s)
%\end{equation}
%The SubOpt of \method evaluated with $\gamma_e=1.0$ can be written as:
%\begin{equation}
%    \text { SubOpt } \left(\pi,s ; \gamma_e\right) = V_{\gamma_e}^{\pi^{*}}(s) - V_{\gamma}^{\pi^{*}}(s) \\
%    + V_{\gamma}^{\pi^{*}}(s) - V_{\gamma}^{\pi}(s) \\
%    + V_{\gamma}^{\pi}(s) - V_{\gamma_e}^{\pi}(s)
%\end{equation}

%where $V_{\gamma} = \mathbb{E}[\sum_{t=0}^{H-1}\gamma^t r(s_t, a_t)]$. 
%The first term and the third term capture the difference in the expected returns of the same policy when evaluated under different $\gamma$. The second term represents the difference in performance between the optimal policy $\pi^*$ and our policy $\pi$ when both are evaluated under the same $\gamma$.

%where \(V_{\gamma_e, M}(\pi)\) denotes the expected return of policy \(\pi\) in MDP \(M\) with discount factor \(\gamma_e\), and \(\pi^*\) is the optimal policy under discount factor \(\gamma\).

%We represent the three terms as follows:

%   \[
%   \Delta_1 = V_{\gamma_e}^{\pi^{*}}(s) - V_{\gamma}^{\pi^{*}}(s).
%   \]
%   \[
%   \Delta_2 = V_{\gamma}^{\pi^{*}}(s) - V_{\gamma}^{\pi}(s).
%   \]

%   \[
%   \Delta_3 = V_{\gamma}^{\pi}(s) - V_{\gamma_e}^{\pi}(s).
%   \]

%Thus, the suboptimality can be expressed as:

%\begin{equation}
%    \text{SubOpt}(\pi,s; \gamma_e) = \Delta_1 + \Delta_2 + \Delta_3.
%\end{equation}

%This decomposition allows us to separately analyze the impact of the discount factors and the policy differences.

\subsection{The upper bound of $\Delta_1$ and $\Delta3$}
Noting that $\Delta_1$ and $\Delta_3$ both capture the difference in the expected returns of the same policy when evaluated under different $\gamma$, we can analyze the upper bound of these two items together.

The term $\Delta_1$ is given by with $\gamma^e=1.0$:

\[
\begin{aligned}
\Delta_1 &= V_{\gamma_e}^{\pi^{*}}(s) - V_{\gamma}^{\pi^{*}}(s) \\
&= \mathbb{E}_{\pi^*} \left[ \sum_{t=0}^{H-1} \gamma_e^t r(s_t, a_t) - \sum_{t=0}^{H-1} \gamma^t r(s_t, a_t) \right] \\
&= \mathbb{E}_{\pi^*} \left[ \sum_{t=0}^{H-1} (1 - \gamma^t) r(s_t, a_t) \right].
\end{aligned}
\]

Assuming the rewards are bounded, i.e., $|r(s, a)| \leq R$, we have:

\begin{equation}
    \Delta_1 \leq \sum_{t=0}^{H-1} (1 - \gamma^t)R=(H - \frac{1-\gamma^H}{1-\gamma}) R.
\end{equation}

%Specifically, when $\gamma \to 1$, the upper bound of $\Delta_1$ is close to 0.

Similarly, we can obtain the upper bound of $\Delta_3$:

\begin{equation}
    \Delta_3 \leq \sum_{t=0}^{H-1} (1 - \gamma^t)R=(H - \frac{1-\gamma^H}{1-\gamma}) R.
\end{equation}

\subsection{The upper bound of $\Delta_2$}
\textbf{Lemma 1 Performance Difference Lemma with finite horizon H~\citep{kakade2002appro}} 
\begin{equation}
V^{\pi^{*}}_{\gamma}(s)-V^\pi_{\gamma}(s)=\frac{1-\gamma^H}{1-\gamma} E_{s \sim d^{\pi^{*}}}\left[\Sigma_{a \in A}(\pi^{*}(a \mid s)-\pi(a \mid s)) Q^\pi(s, a)\right]
\end{equation}
where $\pi^*$ represents optimal policy and $\pi$ represents policy.

Based on the assumption that the rewards are bounded, i.e., $|r(s, a)| \leq R$, we have:
\begin{equation}
Q^\pi(s, a) \leq \sum_{t=0}^{H-1}\gamma^t R = \frac{1-\gamma^H}{1-\gamma}R
\end{equation}

Finally, we can get
\begin{align}
    \Delta_2 &= V^{\pi^{*}}_{\gamma}(s)-V^\pi_{\gamma}(s) \\
    &\leq \frac{(1-\gamma^H)^2}{(1-\gamma)^2}E_{s \sim d^{\pi^{*}}}\left[\sum_{a \in A}(\pi^*(a \mid s)-\pi(a \mid s)\right] R \\
    &= \frac{2(1-\gamma^H)^2}{(1-\gamma)^2}E_{s \sim d^{\pi^{*}}}\left[\mathbb{TV}(\pi^{*}(a|s)||\pi(a|s)\right]R
\end{align}

%## A.4 Bounding \(\Delta_2\): Suboptimality Under Training Discount Factor and Finite Horizon

%We bound $\Delta_2$ using a finite-horizon version of the Performance Difference Lemma, considering the discount factor $\gamma$ over horizon $H$.

%The first term is obvious and the second term is proved by performance difference lemma.(Need more detailed info)

\subsection{SubOptimal Analysis}

Adding the bounds on $\Delta_1$, $\Delta_2$, and $\Delta_3$, we obtain:
\begin{align}
\text{SubOpt}(\pi,s; \gamma_e) \leq 2(H - \frac{1-\gamma^H}{1-\gamma})R + \frac{2(1-\gamma^H)^2}{(1-\gamma)^2}E_{s \sim d^{\pi^{*}}}\left[\mathbb{TV}(\pi^*(a|s)||\pi(a|s)\right]R
\end{align}

Since both terms vary monotonically with the temporary decay factor $\gamma$ but in opposing directions, this implies the existence of an optimal trade-off value, denoted as $\gamma^{*}$, within the interval (0, 1).




\section{Comparison with Strongly Related Work}
\label{appendxi:comparison_with_related_work}

We noticed that \citet{yang2024denseReward} also emphasized the importance of focusing on the contribution of earlier steps, but within the reverse chain of a diffusion denoising process, rather than in an autoregressive LLM scenario. They employed a $\gamma$ parameter to control the contribution of earlier steps, ensuring the quality of generation. We would like to highlight how our approach differs from previous studies in the following perspectives:

\begin{itemize}
    \item Different Perspectives and Tasks: While our work and the referenced prior work both involve preference optimization, they are derived from fundamentally different perspectives and are applied to different downstream tasks. The prior work focuses on text-to-image tasks, which involve fixed-length generation through a non-autoregressive diffusion process. In contrast, our research is centered on LLMs in an autoregressive context, where sequence generation dynamics are inherently different.
    \item Motivation Differences: As illustrated in Figures \ref{fig:self_sampling} and \ref{fig:pred_prob}, our motivation diverges significantly from prior work. Our temporal decay mechanism is designed to address specific challenges in LLMs, such as length bias and the need for alignment with human preferences across varying sequence lengths.
    \item Flexible Decay Mechanism: Our approach is not limited to exponential decay. As shown in Table \ref{tab:decay_methods}, we explore multiple decay strategies, demonstrating the flexibility and adaptability of our method to different scenarios and tasks.
    \item Theoretical Insights: We have provided a theoretical analysis based on the token-level MDP, suggesting the existence of an optimal gamma value for enhancing preference optimization. This theoretical foundation supports the practical effectiveness of our approach.
    \item Extension and Complementarity: Our work serves as both an extension and a complement to the referenced study. While the prior work has not validated its approach on standard RLHF benchmarks, our method has been tested and shown to be effective in these contexts, as detailed in our experimental results.
\end{itemize}
