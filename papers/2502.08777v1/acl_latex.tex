\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[final]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{lingmac}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{times}
\usepackage{xspace}
\newcommand{\tsc}[1]{\textsc{#1}\xspace}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tss}[1]{\textsuperscript{#1}}
\newcommand{\tn}[1]{\textnormal{#1}}

\usepackage{enumitem}  %
\usepackage{xcolor}    %
\usepackage{tcolorbox} %
\usepackage{soul}   
\newcommand{\hlc}[2][yellow]{{%
    \colorlet{foo}{#1}%
    \sethlcolor{foo}\hl{#2}}%
}

\usepackage{todonotes}
\newcommand{\sidenote}[3]{\todo[color=#2,size=scriptsize,fancyline]{\textbf{#1:} #3}}
\newcommand{\john}[1]{\sidenote{John}{green!20}{#1}}
\newcommand{\owen}[1]{\sidenote{Owen}{orange!20}{#1}}


\definecolor{ForestGreen}{RGB}{20,150,20}
\usepackage{fontawesome5} %
\newcommand{\reasoning}{\faBrain}   %
\newcommand{\openmodel}{\faUnlock}    %


\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}

\usepackage{graphicx}

\title{Zero-Shot Belief: A Hard Problem for LLMs}

\author{
    John Murzaku\tss{\tn{$\blacklozenge\spadesuit$}},
    Owen Rambow\tss{\tn{$\clubsuit\spadesuit$}} \\
    \tss{$\blacklozenge$}Department of Computer Science
    \tss{$\clubsuit$}Department of Linguistics\\
    \tss{$\spadesuit$}Institute for Advanced Computational Science\\
    Stony Brook University\\
    \texttt{jmurzaku@cs.stonybrook.edu}
}

\begin{document}
\maketitle

\begin{abstract}
We present two LLM-based approaches to zero-shot source-and-target belief prediction on FactBank:  a unified system that identifies events, sources, and belief labels in a single pass, and a hybrid approach that uses a fine-tuned DeBERTa tagger for event detection.
We show that multiple open-sourced, closed-source, and reasoning-based LLMs struggle with the task.
Using the hybrid approach, we achieve new state-of-the-art results on FactBank and offer a detailed error analysis. Our approach is then tested on the Italian belief corpus ModaFact. 
\end{abstract}
\section{Introduction}
The term “belief''
(interchangeably referred to as ``event factuality'' in NLP) refers to the extent an event mentioned by the author or by sources in a text is presented as being factual.  While this task has received attention over the years, no zero-shot experiments have been performed.  We show that this task remains a hard task for LLMs.

Our major contributions are as follows:
\newline (1) We present unified and hybrid zero-shot frameworks 
for the source-and-target belief prediction task (i.e., who has what belief towards what). We test our approach on various LLMs.
\newline (2) Our hybrid approach achieves new state-of-the-art results (SOTA) on the FactBank corpus, but the problem is far from solved.
\newline (3) We are the first to evaluate FactBank on Nested belief, revealing that LLMs perform particularly poorly on this task.  We perform an error analysis showcasing where LLMs fail. 
\newline (4) We validate the transferability of our approach by testing on the Italian ModaFact belief corpus. 

This paper is organized as follows: we provide an  overview of the belief detection task in Section~\ref{sec:relwork}. We follow by detailing our methodology in Section~\ref{sec:methods}
and discuss our results and analysis for FactBank and ModaFact in Section~\ref{sec:results}. 

\section{Related Work}
\label{sec:relwork}
\noindent\textbf{Corpora } Many corpora explore the notion of belief on the sentence level. FactBank is one of the first corpora to do this, annotating source-and-target belief: both the belief presented by the author towards an event and the belief towards events by sources mentioned inside of the text \citep{sauri2009factbank}.
Other corpora annotate only the author's belief towards events: these corpora include LU \cite{diab-etal-2009-committed}, UW \cite{lee-etal-2015-event}, LDCCB \citep{prabhakaran-etal-2015-new}, MEANTIME \citep{minard-etal-2016-meantime}, MegaVeridicality \cite{white-etal-2018-lexicosyntactic},  UDS-IH2 \citep{rudinger-etal-2018-neural-models}, CommitmentBank \citep{de2019commitmentbank}, and RP \citep{ross-pavlick-2019-well}. Two recent corpora for event factuality are Maven-Fact \citep{li-etal-2024-maven} which contains a large-scale corpus of event and supporting evidence annotations, 
and ModaFact \citep{rovera-etal-2025-modafact}, which is an Italian author belief corpus that annotates in a similar style and inspiration as FactBank. 

\noindent\textbf{Methods} Previous methods for author belief prediction mainly involve fine-tuning: \citet{rudinger-etal-2018-neural-models} fine-tune multi-task LSTMs; \citet{pouran-ben-veyseh-etal-2019-graph} fine-tune a graph convolutional network with BERT \citep{devlin-etal-2019-bert} representations; \citet{jiang-de-marneffe-2021-thinks, murzaku-etal-2022-examining} fine-tune RoBERTa \citep{liu2019roberta} with span representations; \citet{li-etal-2024-maven} fine-tune RoBERTa and Flan-T5 \citep{chung2024scaling}, and also explore four LLMs predictions using few-shot learning; \citet{rovera-etal-2025-modafact} fine-tune BERT, mT5-XXL \citep{xue-etal-2021-mt5}, Aya23-8B \citep{aryabumi2024aya}, and Minerva-3B \citep{orlando2024minerva}.

There has been much less focus on the complete source-and-target belief task: \citet{murzaku-etal-2023-towards, murzaku-rambow-2024-beleaf} both fine-tune a Flan-T5 model, with the latter optimizing for the structure of belief represented as a tree. 


\section{Preliminaries} 
\label{sec:prelims}
Consider this sentence: 
{\em Trurit Inc. said it is phasing out legacy routers.} This sentence reports on two events: a
``said'' event and a ``phasing'' event. 

\noindent\textbf{Author Belief } The definition of author belief (also called event factuality) %
is how committed is the author of the text (the source) to the truth (or factuality) of an event. In this sentence, the author is presenting the ``said'' event as
factual, i.e., they are committed to the ``said''
event having happened. On the other hand, the author is presenting the ``phasing'' event as having an unknown factuality; the author is not directly committing to the truth of the event, rather they are reporting on what ``Trurit Inc.'' said.

\noindent\textbf{Nested } In nested belief, we report the belief towards events according to nested sources inside of a text. 
The task can be split into three steps: (i) identifying the nested or attributed source in the text; (ii) linking the source to the events (i.e., which events does the source commit to); (iii) labelling the belief of the event according to that source. In our example, the source is ``Trurit Inc.''
Once the source is introduced (i), we then link the source to the events in the text (ii): in this case, Trurit Inc. is reportedly committing to the event ``phasing'', and asserting it as true (iii). Since the source is reporting about this event, and directly committing to the event happening, it is therefore true in Trurit Inc.'s perspective as reported by the author, and unknown in the author's perspective.




\section{Methodology}
\label{sec:methods}
We use the test set of the source-and-target (author and nested sources) projection of FactBank %
released by \citet{murzaku-rambow-2024-beleaf}. 
Further dataset details are in Appendix~\ref{sec:appendix-data}.



\subsection{Zero-Shot}
\noindent\textbf{Unified} Our \textbf{Unified} approach provides a single end-to-end zero-shot prompt to the LLM with the input text, a high-level descriptions of the task, the three main steps in the annotation process in detail, special cases guidelines, and the output format. We end the prompt with a summary of the specific steps on how to produce the final answer in a chain-of-thought format (CoT) \citep{wei2022chain}, which has proven to work well for author event factuality \citep{li-etal-2024-maven}. The three steps are: 
(1) Label all events according to the FactBank annotation guidelines, which we provide.
(2) Identify all nested sources in the text. 
(3) Assign factuality labels for each event, according to that source.
We leave all model details, API parameters, and our exact prompts in Appendix~\ref{sec:appendix-unified}. 


\noindent\textbf{Hybrid}
For our \textbf{Hybrid} zero-shot approach, we first extract events in a sentence using a DeBERTa \citep{he2021debertav3} based tagger. After extracting the events, we prompt an LLM with the sentence and the list of events. We then follow the exact steps (minus event detection, since we provide events) as our \textbf{Unified} prompt: we instruct the LLM to identify all nested sources, ask the LLM to assign factuality labels for the events, according to the identified sources, and finish with instructions for answering with CoT.  See Appendix~\ref{sec:appendix-hybrid} for further details on our \textbf{Hybrid} experiments and our exact prompts. 

\noindent\textbf{Event Tagger} The FactBank corpus has a complex definition of what exactly is an annotatable event. \citet{murzaku-etal-2023-towards} found that annotating FactBank events is non-trivial, even with a specialized, generative fine-tuned model achieving only 85.4\% F1 on event identification. We therefore choose to fine-tune a DeBERTa model for event token detection, and then pass the events to our \textbf{Hybrid} prompts. 


\subsection{Models}
We perform experiments on a variety of LLM types: open LLMs, specifically LLaMA-3.3-70B \cite{llama}, DeepSeek-v3 \citep{liu2024deepseek}, and DeepSeek-r1; closed LLMs, specifically GPT-4o \citep{gpt4o}, newly released reasoning models o1 \citep{o1} and o3-mini \citep{o3-mini}, and Claude 3.5 Sonnet \citep{claude}; and reasoning LLMs, DeepSeek r1 (henceforth \textsc{r1}), o1, and o3-mini.



\input{tables/fb_table}

\subsection{Evaluation: Metrics} 
\label{sec:methodology-evals}
We evaluate on three F1 metrics: \texttt{Full} where we perform an exact match evaluation on all generated (source, event, label) annotations; \texttt{Author} where we perform an evaluation on all generated annotations where the source is the author of the text; and \texttt{Nest} where we perform an exact match evaluation on all generated annotations where the source is a nested source.



 \subsection{Evaluation: FactBank Sources}
\label{methodology:normalization}
FactBank has specific conventions about annotating sources. Consider the example ``Trurit Inc. shares rose by 5\% today''. FactBank annotates on the token level, and the source is ``Inc.''. We do not wish to penalize LLMs for not knowing this conversion, and therefore propose a few-shot normalization technique for postprocessing.
We perform all source normalization experiments with GPT-4o \citep{gpt4o}.  
Exact prompts for our source normalization methods and a detailed ablation study are shown in Appendix~\ref{sec:appendix-source}.

Our task setup is as follows: Given a predicted source, we prompt GPT-4o to transform the predicted source
into a FactBank-compliant version.




\section{Results and Analysis}
\label{sec:results}

\subsection{FactBank} 
\noindent\textbf{Main Results} Our main results for FactBank are shown in Table~\ref{tab:fb-results}. We compare all our results to the previous fine-tuned SOTA from \citet{murzaku-rambow-2024-beleaf},
evaluating on exact match F1 (\texttt{Full}) and author exact match F1 (\texttt{Author}) as described in Section~\ref{sec:methodology-evals}. We add one more metric: nested exact match F1 (\texttt{Nest}), where we evaluate on nested sources only. 

Our \textbf{Unified} zero-shot results (column \textbf{Unified}) achieve competitive performance compared to fully fine-tuned models, with \textsc{r1} (66.1\% for \texttt{Full}) and o1 (73.2\% for \texttt{Author}). We outperform the fine-tuned GPT-3 model from \citet{murzaku-rambow-2024-beleaf} on \texttt{Full}, but do not outperform the Flan-T5-XL system. 

We achieve new SOTA
on FactBank with our \textbf{Hybrid} systems. Our \textsc{r1} \textbf{Hybrid} system achieves \texttt{Full} of 72.0\%, outperforming  the previous state of the art by 2.5\% (column $\Delta$ vs. SOTA). Similar to the \textbf{Unified} results, o1 excels in \texttt{Author}, achieving 78.9\% \texttt{Author} and outperforming the previous SOTA by 2.3\%. We also note that GPT-4o and Claude-3.5 also achieve competitive performance, with Claude-3.5 outperforming the previous SOTA on \texttt{Full} and \texttt{Author} by 0.9\% and 1.0\% respectively. We hypothesize that these models excel due to CoT prompting.


\noindent\textbf{Nest F1} We are the first to provide \texttt{Nest} F1 metrics on FactBank. Our top performing model is r1, which achieves a nested F1 of 25.3\%. For reasoning models o1, o3-mini, and r1, we notice that going from \textbf{Unified} to \textbf{Hybrid} does not increase \texttt{Nest} F1 dramatically (0.3\% for o1, 1.4\% for o3-mini, and 1.2\% for r1), showcasing the models' lack of capabilities
for nested belief predictions. We note that these results are low, and believe modelling of nested beliefs is essential future work and a challenging task for reasoning LLMs.

\noindent\textbf{Zero vs. Hybrid} We quantify the exact difference (in \%) between our \textbf{Unified} and \textbf{Hybrid} models in Table~\ref{tab:fb-results} (column \(\Delta\)~(Hyb.-Unif.)). 
We see improvements in every model, with the greatest improvements occuring in GPT-4o and Claude-3.5 for \texttt{Full} and \texttt{Author}.
On average, our \textbf{Hybrid} models outperform our \textbf{Unified} models by 5.7\% for \texttt{Full}, 5.9\% for \texttt{Author}, and 2.0\% for \texttt{Nest}. Our results emphasize the need for a specialized event tagger and hybrid approach, allowing the LLM to focus on linking sources and tagging belief labels. 
\input{tables/table_event_tagger}

\noindent\textbf{Event Detection} 
We investigate how LLMs perform on event tagging.  We show these results in Table~\ref{tab:event-detection}. We compare three LLMs (r1, GPT-4o, and Claude-3.5) to the fine-tuned DeBERTa event tagger used in the {\bf Hybrid} system.
For our LLMs, we try two configurations: zero-shot and few-shot (5 examples). We find that a fine-tuned DeBERTa outperforms all LLMs in all settings, emphasizing that event detection is 
still a difficult task. We leave further experimental details and prompts in Appendix~\ref{sec:appendix-event-tagger}.



\noindent\textbf{Error Analysis} We perform an error analysis on the top-performing model (\textsc{r1}, \textbf{Hybrid}) on nested beliefs (F1 of only 25.3\%). We categorize errors as follows: \textbf{(1) Source} mismatch, often labeling the author instead of the nested source or failing to classify pronoun sources such as “it” correctly (123 errors); \textbf{(2) FN} (false negatives on events), where context-dependent event nouns or verbs are missed (e.g., \emph{“acquisition,” “construction”}) (77 errors); \textbf{(3) FP} (false positives on events), over-predicting event nouns (73 errors); \textbf{(4) Label} errors, notably predicting \emph{True} or \emph{Probable} instead of \emph{Unknown} for future/uncommitted events (e.g., “Mary offered to \textbf{buy} an apple”, where the \textbf{buy} event should be \emph{Unknown}
) (53 errors). We note that the FN errors are consistent with findings from prior FactBank studies: \citet{murzaku-etal-2022-examining} also found similar errors. More detailed results and analysis of our error analysis are in Appendix~\ref{sec:appendix-ea}.




\input{tables/moda_table}

\subsection{Multilingual Belief} 

The ModaFact Italian corpus \citep{rovera-etal-2025-modafact} annotates the author's belief, polarity, and modality towards events and temporal information.
We only use the belief and polarity annotations and combine these to tags similar to those of FactBank (and perform an exact match evaluation on Belief+Polarity F1).  This is different from how \citet{rovera-etal-2025-modafact} evaluate, but they kindly shared their raw results so that we could apply our evaluation.

\noindent\textbf{Results} We perform our ModaFact experiments with three cost-effective models that performed well for FactBank:
GPT-4o, o3-mini, and \textsc{r1}. Our results are shown in Table~\ref{tab:moda-results}. Unlike our FactBank results, we fall short of the fine-tuned SOTA for our \textbf{Hybrid} system (by 0.8\%).  Similar to our FactBank results, \textbf{Hybrid} strongly outperforms \textbf{Unified} in all settings.
Finally, we see that \textsc{r1}
and o3-mini (both reasoning models) come very close to the fine-tuned SOTA. GPT-4o also proves competitive, but falls short of the reasoning models by 2.4\%. We note that while we do not beat the SOTA, 
the LLMs we use are not explicitly trained for multilingual data (in contrast with mT5-XXL).  For example, \textsc{r1} is specifically optimized for English and Chinese data 
\citep{guo2025deepseek}. We hypothesize that future multilingual optimizations for these reasoning LLMs %
would in fact lead to a new SOTA for the ModaFact corpus.

\section{Conclusion}
\label{sec:conclusion}
We show that belief detection from text remains a challenging problem for LLMs.  This is particularly true for nested beliefs, which the author ascribes to other sources.  Our new SOTA system includes a distinct fine-tuned event detection component.






\section*{Limitations} 
While our model achieves a new state-of-the-art on the English only FactBank, our results, while still competitive, do not perform as well for the Italian ModaFact corpus. We acknowledge this as a shortcoming and aim to work towards broader multilingual generalization for this task.

We note that our LLM approach yields poor results on the nested F1 metric, indicating a large gap and potential for future improvement. We will explore improving these results in future work and believe this to be a gap for all major open, closed, and reasoning LLMs.

Finally, we note that our top performing LLM approach, while using the open DeepSeek r1 model, is reliant on API calls for the source normalization technique. We attempt to minimize costs by using GPT-4o, but note that we can (i) achieve better performance using a larger, reasoning model (more cost) or (ii) switch to an open model. We will explore both techniques.

\section*{Ethics Statement}
We note that our paper is foundational research and we are not tied to any direct applications. We do not foresee any potential risks with our work. We do not perform any annotations or human evaluation as we use the already existing FactBank dataset and ModaFact dataset.  















\bibliography{anthology,custom}

\appendix

\section{Unified Experiments}
\label{sec:appendix-unified}
\paragraph{Details} For all \textbf{Unified} zero-shot experiments, we use a temperature of 0.0 where applicable (all models besides o1 and o3-mini). For o1 and o3-mini, we use the default reasoning setting (Medium). To prompt all other models (LLaMA-3.3, DeepSeek-v3, DeepSeek-r1, and Claude-3.5-Sonnet), we use the OpenRouter API \citep{openrouter}. The open models are ran at full precision (henceforth why we used the OpenRouter API and external providers). 

\paragraph{Prompt} Our zero-shot \textbf{Unified} prompt is shown in Figure~\ref{fig:zero-shot-prompt}.


\section{Hybrid Experiments}
\label{sec:appendix-hybrid}
\paragraph{Details} For all \textbf{Hybrid} zero-shot experiments, we use a temperature of 0.0 where applicable (all models besides o1 and o3-mini). For o1 and o3-mini, we use the default reasoning setting (Medium). To prompt all other models (LLaMA-3.3, DeepSeek-v3, DeepSeek-r1, and Claude-3.5-Sonnet), we use the OpenRouter API \citep{openrouter}. 

\paragraph{Prompt} Our \textbf{Hybrid} zero-shot prompt is shown in Figure~\ref{fig:hybrid-prompt}.


\section{LLM Experiment Details} For all our FactBank experiments, we report a single run, especially due to cost. We note that o1 experiments cost up to \$75 per run on the FactBank test set. To minimize randomness, we set the temperature to 0.0 where applicable (besides o1 and o3-mini). For o1 and o3-mini, we use the default reasoning setting (Medium). For our ModaFact experiments, we report the average over all five folds. Due to API costs and performing five-fold cross validation, we limit all ModaFact experiments to GPT-4o, o3-mini, and DeepSeek r1, which are the most cost effective models. 

\section{Dataset Details}
\label{sec:appendix-data}
\paragraph{FactBank} We use the author and source-and-target projection of FactBank from \citet{murzaku-rambow-2024-beleaf}, who follow the article split from \citet{murzaku-etal-2022-examining}. We use their provided code for data extraction and follow their exact article split. The release of the FactBank corpus that we use can be found at the Linguistic Data Consortium, catalog number LDC2009T23. The test set contains 280 sentences and 1,326 examples. 


\paragraph{ModaFact} We use all five-folds of the test set of the ModaFact corpus from \citet{rovera-etal-2025-modafact}, which is publicly available. All results we report are averages over the five folds. To get the events from ModaFact for our \textbf{Hybrid} zero-shot experiments, we use the author's provided prediction files and inference script with mT5-XXL. 
\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\hline
Fold   & Sentences & Examples \\
\hline
Fold 1 & 646       & 2098     \\
Fold 2 & 605       & 2097     \\
Fold 3 & 606       & 2096     \\
Fold 4 & 626       & 2094     \\
Fold 5 & 601       & 2090     \\
\hline
\end{tabular}
\caption{Dataset details for the ModaFact test set.}
\label{tab:modafact-appdx}
\end{table}

\section{Source Normalization}
\label{sec:appendix-source}
We propose two source normalization prompts: a few shot source normalization prompt and an oracle source normalization prompt. For these prompts, we use GPT-4o, with temperature 0.0. We prompt GPT-4o using the OpenAI API. Our exact few shot source normalization prompt is shown in Figure~\ref{fig:few-shot-norm}. Our exact oracle normalization prompt is shown in Figure~\ref{fig:source_normalization_prompt}.

\input{tables/table_normalization_ablation}
\begin{table}[t]
\setlength\tabcolsep{4pt}
\centering
\small
\begin{tabular}{lr|lr}
\toprule
\textbf{Category} & \textbf{Count} & \textbf{Breakdown} & \textbf{Count} \\
\midrule
Source & 123 & Gold=AUTHOR & 50 \\
                  &     & Gold=``it''   & 13 \\
\midrule
FN   & 77  & Missed Noun     & 38 \\
                  &     & Missed Verb     & 30 \\
\midrule
Label  & 73  & Pred:CT+ $\rightarrow$ Gold:UU  & 28 \\
                  &     & Pred:PR+ $\rightarrow$ Gold:UU & 22 \\
\midrule
FP   & 53  & Predicted Noun & 33 \\
                  &     & Predicted Verb & 10 \\
\bottomrule
\end{tabular}
\caption{Error analysis for our \textbf{Hybrid} DeepSeek r1 system on nested predictions, showing counts of each error type relative to its category total count.}
\label{tab:error-analysis-percentages-only}
\end{table}
We perform an ablation analysis of our few-shot and oracle normalization techniques described in Section~\ref{methodology:normalization}. We showcase these results for our top performing system (DeepSeek r1, \textbf{Hybrid}) in Table~\ref{tab:deepseek-ablation}. Without any normalization, we achieve a Full F1 of 68.9\% and Nest F1 of 17.5\%. Our few shot normalization technique improves us 2.1\% for Full F1, and more notably by 7.8\% for Nest F1. Our oracle method, as expected (since we provide gold sources), performs even better than our few shot method, achieving a Full F1 of 72.7\% and 27.1\%. However, we choose to perform all experiments with our few shot normalization method instead of our oracle method to truly showcase LLMs capabilities for belief detection without any gold sources as input. 



\section{Event Tagger}
\label{sec:appendix-event-tagger}
\paragraph{DeBERTa Tagger} We use DeBERTa-large for token classification, setting the number of labels to 2 (O vs. EVENT). We use the following hyperparameters: Epochs: 5; Batch Size: 16; Learning Rate: 1e-4; Max Sequence Length: 128. We do not perform any hyperparameter optimization or tuning. The model is trained using the HuggingFace Transformers library \citep{wolf-etal-2020-transformers}.

\paragraph{LLM Event Tagging} We perform event tagging on multiple LLMs. We set the temperature to 0.0. We use the OpenRouter API \citep{openrouter} for DeepSeek r1 and Claude 3.5, and the OpenAI API for GPT-4o. We do not perform experiments with o1 to avoid high costs. Our zero-shot event detection prompt is shown in Figure~\ref{fig:factbank_event_prompt_zs}. Our few-shot event detection prompt is shown in Figure~\ref{fig:factbank_few_shot_prompt_events}.


\section{Nested Error Analysis}
\label{sec:appendix-ea}


We expand our error analysis on the nested sources. 
Table~\ref{tab:error-analysis-percentages-only} shows our error counts and error types.

We specifically analyze the errors for nested beliefs, which is where all LLMs fail on (our top performing model achieving F1 of only 25.3\%). We showcase the error category, and then the top two error types by count. We use the following labels: {\bf Source} indicates a source mismatch error; {\bf FN} indicates a false negative, where the LLM did not generated a certain event type; {\bf Label} indicates a label error where the LLM had the source and event correct, but inorrectly labeled the event. {\bf FP} indicates a false positive, where the LLM overpredicted (that is, it generated an event that was not actually an event). 

Our most notable error is {\bf Source}, where 123 errors are made. The most common error is the model predicts the author is the source instead of the correct nested source. Another notable error is where the model does not classify the source as it, but rather predicts the name of the entity. Next, we see a repeating of similar errors that \citet{murzaku-etal-2022-examining} discovered. Specifically, event nouns can be hard to determine (e.g. nouns like ``concerns'', ``acquisition'', ``construction''). Our {\bf FN}  and {\bf FP} errors showcase that LLMs simultaneously over predict event nouns, while also missing both event nouns and verbs. Finally, we notice two notable label flips for our {\bf Label} error category: the LLM predicts CT+ (the event happened/is true) when the gold label is UU (unknown), and PR+ (possibly true) when the gold is UU. This is due to FactBank's definitions of nested sources and future events: when a reporting of a future event happening (e.g. ``Mary said it will happen''), the factuality of the event according to the source is UU (the source is not committing to the event; rather, the author is commiting to it). 

Our analysis emphasizes that despite our source normalization method and use of strong reasoning LLMs, there is much room for improvement. Our error analysis findings are further supported with similar errors that have been reported in previous works on FactBank \citep{murzaku-etal-2022-examining}.

\section{Code Release} We will release all of our code. We will provide the full pipelines, datasets, and model checkpoints where applicable. 

\input{appendix/prompt_zero}
\input{appendix/prompt_hybrid}

\input{appendix/prompt_oracle_source}
\input{appendix/prompt_fewshot_source}

\input{appendix/prompt_events_zeroshot}
\input{appendix/prompt_events_fewshot}
\end{document}
