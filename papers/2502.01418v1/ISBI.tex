% Template for ISBI paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{comment}
% It's fine to compress itemized lists if you used them in the
% manuscript
\usepackage{enumitem}
\usepackage{xcolor}   
\setlist{nosep, leftmargin=14pt}
\usepackage{amsfonts}
\usepackage{mwe} % to get dummy images
\usepackage{hyperref}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}
\newcommand{\ms}[1]{{\color{orange} #1}}
\newcommand{\pa}[1]{{\color{pink} #1}}
\newcommand{\vpp}[1]{{\color{blue} #1}}
\newcommand{\gray}[1]{{\color{gray} #1}}

% Title.
% ------
\title{Assessing the use of Diffusion models for motion artifact correction in brain MRI}
% \title{Are diffusion models reliable for 2D Brain MRI motion artifact correction?}
%.
\usepackage{algorithm}
\usepackage{algpseudocode}

% Diffusion models and MRI motion artifact correction: a critic assessment
% Diffusion models and MRI motion artifact correction: trick or treat
%Are diffusion models reliable for MRI motion artifact correction?
% Single address.
% ---------------
\name{Paolo Angella\textsuperscript{1}, Vito Paolo Pastore\textsuperscript{1*†}\thanks{*Correspondence to Vito.Paolo.Pastore@unige.it}, Matteo Santacesaria\textsuperscript{1†}\thanks{†These authors contributed equally to this work.}}
\address{\textsuperscript{1} MaLGa Center,
University of Genoa, Italy %\\ \textsuperscript{2} MaLGa Center, DIBRIS,
%Università degli studi di Genova, Genoa, Italy
}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Some author footnote.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
% More than two addresses
% -----------------------
% \name{Author Name$^{\star \dagger}$ \qquad Author Name$^{\star}$ \qquad Author Name$^{\dagger}$}
%is a widely used non-invasive diagnostic imaging technique that provides excellent soft tissue contrast. However, it 
% \address{$^{\star}$ Affiliation Number One \\
%     $^{\dagger}$}Affiliation Number Two
\begin{document}
\maketitle
\begin{abstract}
Magnetic Resonance Imaging generally requires long exposure times, while being sensitive to patient motion, resulting in artifacts in the acquired images, which may hinder their diagnostic relevance. 
Despite research efforts to decrease the acquisition time, and designing efficient acquisition sequences, motion artifacts are still a persistent problem, pushing toward the need for the development of automatic motion artifact correction techniques. 
Recently, diffusion models have been proposed as a solution for the task at hand. While diffusion models can produce high-quality reconstructions, they are also susceptible to hallucination, which poses risks in diagnostic applications. In this study, we critically evaluate the use of diffusion models for correcting motion artifacts in 2D brain MRI scans. Using a popular benchmark dataset, we compare a diffusion model-based approach with state-of-the-art methods consisting of Unets trained in a supervised fashion on motion-affected images to reconstruct ground truth motion-free images. %Our results are controversial, suggesting that diffusion models can provide accurate predictions or can be prone to harmful hallucination in this context, 
Our findings reveal mixed results: diffusion models can produce accurate predictions or generate harmful hallucinations in this context, depending on data heterogeneity and the acquisition planes considered as input.
\end{abstract}
%
\begin{keywords}
MRI, motion artifact correction, Diffusion models, Deep learning
\end{keywords}
%

\section{Introduction}
Magnetic Resonance Imaging (MRI) \cite{review1} has a significant limitation: its lengthy scan time, during which patients must remain completely still. This requirement poses particular challenges for young children, claustrophobic individuals, and elderly patients. Moreover, certain movements, such as those from breathing or cardiac activity, are unavoidable.

Motion-induced artifacts in MRI have distinctive characteristics. MRI images are generated by sampling points from the Fourier transform of the target object and then reconstructing the image through an inverse Fourier transform with incomplete data. As this constitutes an inverse problem, motion-induced errors can be substantially amplified. The process of working in Fourier space produces artifacts that are fundamentally different from those in conventional radiography, where subject movement typically results in localized blur. In MRI, even minor, localized movements can affect the entire image \cite{zaitsev2015motion}.

Recent years have seen increasing efforts to use deep learning methods for retrospective motion artifact removal in MRI—techniques applied after scan completion \cite{review1}. These approaches typically rely on supervised training of deep neural networks. Notable work in this area includes \cite{al2022stacked, levac2023accelerated, oh2023annealed}. A common challenge these methods face is the reliance on simulated data for training, as real-world paired data—scans of the same patient with and without motion artifacts—are neither widely available nor simple to collect. To address this limitation, several studies \cite{liu2021learning, ghodrati2021retrospective,oh2021unpaired} have developed deep learning frameworks that perform motion correction using unpaired data.

In this work, we investigate the potential of applying 
a generative model approach based on diffusion models to the task of MRI motion artifact correction. We compare this generative method's performance against a more traditional approach using a U-Net architecture \cite{ronneberger2015u}, trained on images with simulated motion artifacts, using a widely-used benchmark dataset of brain MRI scans. This comparison aims to evaluate diffusion models' effectiveness in motion correction relative to established methods. Notably, while the U-Net architecture follows a supervised learning approach, diffusion models are unsupervised, requiring no motion-affected images during training. To thoroughly evaluate these approaches—particularly the trade-off between diffusion models' accuracy in motion-artifact correction and potential harmful hallucinations—we conduct experiments across all three available planes (sagittal, coronal, and transverse). Our results indicate that diffusion models' performance varies significantly based on data heterogeneity and the investigated scan planes, with hallucinations potentially affecting reliable clinical usage. 

\begin{comment}
\section{Introduction}
Magnetic Resonance Imaging (MRI) \cite{review1} \cite{review2} has the significant drawback of having a lengthy scan time, during which the patient must remain perfectly still. This can be particularly challenging in certain cases, such as with very young, claustrophobic, or elderly patients. Additionally, some movements, such as those caused by breathing or cardiac activity, are unavoidable.
The artifacts resulting from movement in MRI are particularly distinctive. This is because MRI images are produced by sampling points from the Fourier transform of the object of interest and then reconstructing the image using an inverse Fourier transform with incomplete data, as that is an inverse problem, movement-induced errors can be significantly amplified. The process of passing through Fourier space (referred to as K-space in the context of MRI) causes artifacts that differ greatly from those in conventional photography, where capturing a moving subject leads to more localized distortions. In MRI, even small, localized movements can affect the entire image \cite{zaitsev2015motion}.
Recently, there has been a growing effort to use deep learning methods to retrospectively remove motion artifacts in MRI, meaning these methods are applied after the scan is completed \cite{review1} \cite{review2}. These techniques typically rely on training machine learning models (often deep neural networks) in a supervised manner. Notable approaches in this area include \cite{duffy2018retrospective, johnson2019conditional, liu2020motion, al2022stacked, kuzmina2022autofocusing, levac2023accelerated, oh2023annealed}. A common aspect of these methods is the use of simulated data to train neural networks for motion correction, as real-world paired data, consisting of scans of the same patient with and without motion artifacts, are not widely available, nor simple to collect. To address this limitation, several studies \cite{liu2021learning, ghodrati2021retrospective, oh2021unpaired, armanious2020unsupervised} have developed deep learning frameworks that perform motion correction using unpaired data.
In this work, we explore the feasibility of applying a generative modeling approach, specifically a Denoising Diffusion Probabilistic Model (DDPM) \cite{NEURIPS2020_4c5bcfec}, to perform the same task of motion artifact correction in MRI. We compare the performance of this generative method with a more traditional approach that utilizes a U-Net architecture \cite{ronneberger2015u}, trained on images with simulated motion artifacts, on a popular benchmark dataset including brain MRI scans. This comparison aims to assess the effectiveness of DDPM in handling motion correction relative to established methods. It is important to note that the U-Net architecture follows a supervised learning approach, whereas DDPMs are unsupervised, so no motion-affected images are employed during their training. To thoroughly evaluate the different approaches, and especially the trade-off between diffusion models' accuracy in motion-artifact correction and potentially harmful hallucinations, we conduct experiments along all three axes available in the considered dataset (sagittal, coronal, and axial). Our results suggest that diffusion models' performance varies widely with respect to the heterogeneity of data, and hence, to the investigated axial plane. 
% We believe this work highlights that despite the encouraging performance, further research is still necessary to avoid harmful hallucinations when using diffusion models for motion-artifact correction. 

\end{comment}
\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{imgs/ISBI}
\caption{Our implementation follows a two-phase approach. In the first phase, a diffusion model (DDPM) is trained on a dataset of motion artifact-affected images. In the second phase, the trained model is used to introduce motion artifacts into clean images, generating paired datasets. These pairs enable supervised training in the subsequent step. See Algorithm \ref{alg:motion_correction} for more details.}
\label{All}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.475\textwidth]{imgs/progressione.png}
\caption{DDPM motion artifact correction with different time steps $n$.}
\label{prog}
\end{figure}

\section{Methods}
In this work, we focused on comparing two different methods for motion correction, both of which operate under the assumption that paired data of motion-affected and motion-free images are unavailable. This assumption holds because such paired scans can only be obtained through dedicated experiments, which are not feasible in standard clinical practice. The first method involves a U-Net trained in a supervised manner using synthetically generated motion-affected images. The second method employs an unconditional diffusion model, specifically a Denoising Diffusion Probabilistic Model (DDPM) \cite{NEURIPS2020_4c5bcfec}, trained only on motion-free images, to accomplish the same task (see Fig. \ref{All} for a schematic overview).

DDPMs are generative models that generate data by reversing a diffusion process. In the forward process, Gaussian noise is gradually added to a data point $\mathbf{x}_0$ (in our case, an MRI image) over $T$ timesteps, producing a sequence of latents $\mathbf{x}_1, \dots, \mathbf{x}_T$. Each transition follows:
\begin{equation}
    q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t} \mathbf{x}_{t-1}, (1-\alpha_t) \mathbf{I}),
\end{equation}
where $\alpha_t$ are predefined noise variance schedule parameters. The reverse process, parameterized by a neural network, learns to denoise each $\mathbf{x}_t$ to recover $\mathbf{x}_{t-1}$:
\begin{equation}
    p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \Sigma(\mathbf{x}_t, t)).
\end{equation}
This formulation enables DDPMs to generate high-quality samples by progressive denoising starting from pure Gaussian noise.

The key challenge lies in adapting the DDPM for motion correction, as it typically generates images by starting from random noise and performing $T$ denoising steps. In our approach, described in Algorithm \ref{alg:motion_correction}, we instead begin with the motion-affected image, add Gaussian noise up to a certain step $n$ (where $n < T$), and then allow the model to denoise for the remaining $n$ steps. The choice of $n$ is crucial: if too high, the model may hallucinate details; if too low, the correction will be insufficient (see Fig. \ref{prog} for a qualitative example).

To evaluate the performance of the various approaches, we employ standard metrics for motion artifact correction assessment: the Structural Similarity Index Measure (SSIM), Normalized Mean Squared Error (NMSE), and Peak Signal-to-Noise Ratio (PSNR).

\begin{comment}
\section{Methods}
In this work, we focused on comparing two different methods for motion correction, both of which operate under the assumption that paired data of motion-affected and motion-free images are unavailable. This assumption holds because such paired scans can only be obtained through dedicated experiments, which are not feasible in standard clinical practice. The first method involves a U-Net trained in a supervised manner using synthetically generated motion-affected images. The second method employs an unconditional diffusion model, specifically a Denoising Diffusion Probabilistic Model (DDPM) \cite{NEURIPS2020_4c5bcfec}, trained only on motion-free images, to accomplish the same task.\\
DDPMs are generative models that generate data by reversing a diffusion process. In the forward process, Gaussian noise is gradually added to a data point $\mathbf{x}_0$, that in our case is an MRI image, over $T$ timesteps, producing a sequence of latents $\mathbf{x}_1, \dots, \mathbf{x}_T$. Each transition follows:
\begin{equation}
    q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t} \mathbf{x}_{t-1}, (1-\alpha_t) \mathbf{I}),
\end{equation}
where $\alpha_t$ are predefined noise variance schedule parameters. The reverse process, parameterized by a neural network, learns to denoise each $\mathbf{x}_t$ to recover $\mathbf{x}_{t-1}$:
\begin{equation}
    p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \Sigma(\mathbf{x}_t, t)).
\end{equation}
This formulation enables DDPMs to generate high-quality samples by progressive denoising starting from pure Gaussian noise.
\\
The DDPM doesn't require paired data for training, it learns solely from motion-free images. The challenge, however, is how to use this generative model for motion correction. Typically, a DDPM starts from random noise and performs $T$ denoising steps to create a new image, in its reverse process. In our approach, described in terms of pseudocode in Algorithm \ref{alg:motion_correction}, instead of starting from pure noise, we begin with the motion-affected image, add Gaussian noise up to a certain step $n$ (where $n < T$), and then allow the model to denoise for the remaining $n$ steps. Choosing the right $n$ is crucial: if it is too high, the model may hallucinate details; if it's too low, the correction won't be sufficient (see Fig. \ref{prog} for a qualitative example).
To quantify the performance of the various approaches we employ standard metrics for motion artifact correction evaluation, including the Structural Similarity Index Measure (SSIM), Normalized Mean Squared Error (NMSE), and Peak Signal-to-Noise Ratio (PSNR).
\end{comment}
\begin{algorithm}
\caption{Diffusion-based motion correction.}
\label{alg:motion_correction}  % Label for referencing
\begin{algorithmic}[1]
\State \textbf{input} $\mathbf{Y},n, \left\{ \alpha_t, \bar \alpha_t, \sigma_t \right\}_{t=1}^{n}, \mathbf{\epsilon}_\theta$
\State $\mathbf{z} \sim \mathcal{N}(0, \mathbf{I})$
\State $\mathbf{x}_n =  \sqrt{\bar{\alpha}_n} \mathbf{Y} + (1 - \bar{\alpha}_n) \mathbf{z}$

\For{$t = n, \dots, 1$}
\State if {$t > 1$} $\mathbf{z} \sim \mathcal{N}(0, \mathbf{I})$, else $\mathbf{z} = 0$
    \State $\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \mathbf{\epsilon}_\theta (\mathbf{x}_t, t) \right) + \sigma_t \mathbf{z}$
\EndFor
\State \textbf{return} $\mathbf{x}_0$
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\subsection{Dataset}
All experiments are conducted using the MR-ART dataset \cite{MR-ART}, which contains brain MRI scans of 144 patients, captured both while they remained still and while moving their heads to introduce motion artifacts. As mentioned earlier, having paired data like this is uncommon. Therefore, we employ data from Olsson et al. \cite{olsson2024simulating}, which includes simulated motion artifacts. The dataset consists of 3D volumes, from which sagittal, coronal, and transverse views were extracted for use in the experiments. Our preprocessing step includes normalization and registration between the motion-free and motion-affected images from MR-ART, as the volumes are often misaligned.
\subsection{Experiment Details}
All experiments are conducted on an NVIDIA A30 GPU using the PyTorch framework. The architecture of the U-Net follows the original of \cite{ronneberger2015u}, while the diffusion model is based on the architecture of \cite{ddpm-github}. Specifically, 500 timesteps were used. For DDPM training, the learning rate was set to $5 \times 10^{-5}$, and for the U-Net at $1 \times 10^{-3}$, in both cases the batch size was $6$. Early stopping was applied to all models to prevent overfitting on the training set. A subset of 30 patients was used for training, 10 for validation, and 25 for testing. This approach was adopted to ensure efficient use of computational resources while maintaining robust performance.
\subsection{Results}
We first describe our parameter-tuning procedure, and then report the obtained results on the three available anatomical planes. Our analysis compares three approaches: (1) our diffusion model approach, (2) a U-Net trained on synthetic images from \cite{olsson2024simulating}, and (3) a benchmark U-Net trained directly on paired motion-affected and motion-free images from the ground truth dataset. This last approach serves as an upper bound for performance, as it uses real paired data that would typically be unavailable in clinical settings.

\subsubsection{Parameter Tuning}
In the original MR-ART dataset, each clean image is paired with two corresponding images featuring induced motion. Therefore, for each U-Net training session, we utilize two synthetic motion-affected images for every motion-free image. In \cite{olsson2024simulating}, four sets of synthetic images were produced for each reference image. For a fair comparison, we select the best two synthetic images by training a U-Net on each possible combination (referred to as A, B, C, and D) and selecting the one yielding the best results on the validation set. All sets produce very similar outcomes, with slightly better performance for the BC set. Thus, we select this set to compare with our diffusion model-based approach.
An important parameter is the timestep at which to start the denoising process. Here, there is a trade-off between artifact removal and hallucination by the model, as shown in Fig. \ref{prog}. We selected 150 timesteps as it provided an optimal balance between reconstruction quality and the extent of hallucinations.

\begin{figure}
\centering
\includegraphics[width=0.475\textwidth]{imgs/compazio_alt.png}
\caption{Comparison of the different approaches on all three views.}
\label{confronto}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.475\textwidth]{imgs/allucinazioni_alt.png}
\caption{Example of hallucination of the DDPM.}
\label{immaginie}
\end{figure}
\subsubsection{Transverse view}
\begin{table}[!ht]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llll}
\hline
Label & SSIM & NMSE & PSNR \\
\hline
UNet Real & $\textbf{0.858} \pm 0.079$ & $0.059 \pm 0.054$ & $22.9 \pm 2.7$ \\
\hline
UNet Synth & $\textit{0.844} \pm 0.088$ & $\textit{0.055} \pm 0.062$ & $\textit{24.5} \pm 2.8$ \\
Diffusion & $0.839 \pm 0.073$ & $\textbf{0.047} \pm 0.053$ & $\textbf{24.9} \pm 2.4$ \\
\hline
GAN \cite{safari23}  & $0.670$ & $0.225 $ & $21.7$ \\
Pix2Pix \cite{safari23}  & $0.640$ & $0.242$ & $21.4$ \\
% UNet from \cite{levac2023accelerated}  & $0.730$ & $0.195 $ & $22.5 $ \\
Diffusion from \cite{safari23}  & $0.680$ & $0.191 $ & $21.9 $ \\
\hline
\end{tabular}}
\caption{Transverse view metrics.}
\end{table}
In the transverse view, the Diffusion model performs best for NMSE and PSNR, while the UNet achieves the highest SSIM. The UNet Synth model performs decently but slightly trails behind the Diffusion model in NMSE and PSNR, despite being close to the UNet in SSIM. See Fig. \ref{confronto} (middle) for a qualitative comparison of the motion-free images predicted with the investigated approaches. In this view, DDPM is not prone to hallucination. To better frame the obtained results in the state-of-the-art, we report the performance on the transverse view available from \cite{safari23}, exploiting popular generative models including CycleGAN \cite{zhu2017unpaired},  Pix2Pix \cite{isola2017image}, and a conditional diffusion probabilistic model. Our investigated methods outperform the available state-of-the-art results for all the evaluation metrics. Notably, the transverse is the only view employed in the literature existing on motion-artifact correction with diffusion models for the MR-ART dataset \cite{safari23}.   
\subsubsection{Sagittal view}
\begin{table}[!ht]
\centering
\begin{tabular}{llll}
\hline
Label & SSIM & NMSE & PSNR \\
\hline
UNet Real & $\textbf{0.782} \pm 0.055$ & $\textbf{0.084} \pm 0.119$ & $21.9 \pm 2.1$ \\
\hline
UNet Synth & $\textit{0.741} \pm 0.060$ & $\textit{0.098} \pm 0.124$ & $\textbf{22.2} \pm 1.9$ \\
Diffusion & $0.666 \pm 0.180$ & $0.132 \pm 0.182$ & $\textit{22.0} \pm 3.4$ \\
\hline
\end{tabular}
\caption{Sagittal view metrics.}
\end{table}
For the sagittal view, the UNet performs the best in terms of SSIM and NMSE, though the UNet Synth slightly surpasses it in PSNR. The Diffusion model performs poorly in this view, with significantly higher NMSE and a much lower SSIM (See Fig. \ref{confronto} (top) for a qualitative comparison) Furthermore, DDPM is prone to hallucination, affecting predicted motion-free images (see Fig. \ref{immaginie} for an example). 
% indicating a struggle in maintaining visual accuracy and noise reduction for this orientation. Furthermore, the predicted motion- See Fig. \ref{fi}
\subsubsection{Coronal view}
\begin{table}[!ht]
\centering
\begin{tabular}{llll}
\hline
Label & SSIM & NMSE & PSNR \\
\hline
UNet Real & $\textit{0.806} \pm 0.103$ & $\textbf{0.064} \pm 0.054$ & $22.6 \pm 2.9$ \\
\hline
UNet Synth & $\textbf{0.810} \pm 0.121$ & $0.074 \pm 0.065$ & $\textbf{23.5} \pm 2.8$ \\
Diffusion & $0.765 \pm 0.108$ & $\textit{0.068} \pm 0.051$ & $\textbf{23.5} \pm 2.5$ \\
\hline
\end{tabular}
\caption{Coronal view metrics.}
\end{table}
In the coronal view, the UNet Synth performs the best overall, with the highest SSIM and PSNR, though the UNet has a lower NMSE. The Diffusion model does well in NMSE and PSNR but falls behind in SSIM compared to the UNet models, with predicted motion-free images affected by hallucination. See Fig. \ref{confronto} (bottom) for a qualitative comparison. 
% This view highlights the balance between models, as no single model outperforms across all metrics, indicating that each one has its strengths depending on the view and metric.

\section{Conclusions}
This study critically evaluates the use of diffusion models for correcting motion artifacts in 2D brain MRI scans. Our findings reveal that while diffusion models can achieve competitive performance with supervised methods in certain scenarios, their effectiveness varies significantly with data heterogeneity and anatomical planes. The primary challenge lies in balancing reconstruction quality against the risk of hallucinations, which could be particularly problematic in diagnostic settings. Our results suggest that diffusion models hold promise for MRI motion artifact correction, but their clinical application requires careful consideration of specific use cases and potential risks. Future work should focus on developing methods to detect and mitigate hallucinations, exploring hybrid approaches that combine the strengths of both diffusion and supervised methods, and conducting clinical validation studies to assess the impact on diagnostic accuracy.

\section{Compliance with Ethical Standards}

This study was conducted retrospectively using ethically acquired publicly
available human subject data. The authors have no interests to disclose.
\section{Acknowledgments}
\label{sec:acknowledgments}
This material is supported by the Air Force Office of Scientific Research (a.n. FA8655-23-1-7083). Co-funded by Regional Problem FSE+ (point 1.2 of attach. IX of Reg. (UE) 1060/2021) and European Union - Next Generation EU.
\bibliographystyle{IEEEbib}
%\bibliography{strings,refs}
\begin{thebibliography}{10}

\bibitem{review1}
Chang et~al.,
\newblock ``Deep learning-based rigid motion correction for magnetic resonance
  imaging: a survey,''
\newblock {\em Meta-Radiol}, p. 100001, 2023.

\bibitem{zaitsev2015motion}
Zaitsev et~al.,
\newblock ``Motion artifacts in mri: A complex problem with many partial
  solutions,''
\newblock {\em J Magn Reson Imaging}, vol. 42, no. 4, pp. 887--901, 2015.

\bibitem{al2022stacked}
Al-Masni et~al.,
\newblock ``Stacked u-nets with self-assisted priors towards robust correction
  of rigid motion artifact in brain mri,''
\newblock {\em Neuroimage}, vol. 259, pp. 119411, 2022.

\bibitem{levac2023accelerated}
Levac et~al.,
\newblock ``Accelerated motion correction for mri using score-based generative
  models,''
\newblock in {\em IEEE ISBI}. IEEE, 2023, pp. 1--5.

\bibitem{oh2023annealed}
Oh~et~al.,
\newblock ``Annealed score-based diffusion model for mr motion artifact
  reduction,''
\newblock {\em IEEE Trans Comput Imaging}, 2023.

\bibitem{liu2021learning}
Liu et~al.,
\newblock ``Learning mri artefact removal with unpaired data,''
\newblock {\em Nat Mach Intell}, vol. 3, no. 1, pp. 60--67, 2021.

\bibitem{ghodrati2021retrospective}
Ghodrati et~al.,
\newblock ``Retrospective respiratory motion correction in cardiac cine mri
  reconstruction using adversarial autoencoder and unsupervised learning,''
\newblock {\em NMR Biomed}, vol. 34, no. 2, pp. e4433, 2021.

\bibitem{oh2021unpaired}
Oh~et~al.,
\newblock ``Unpaired mr motion artifact deep learning using outlier-rejecting
  bootstrap aggregation,''
\newblock {\em IEEE Trans Med Imaging}, vol. 40, no. 11, pp. 3125--3139, 2021.

\bibitem{ronneberger2015u}
Ronnebergeret al.,
\newblock ``U-net: Convolutional networks for biomedical image segmentation,''
\newblock in {\em MICCAI}. Springer, 2015, pp. 234--241.

\bibitem{NEURIPS2020_4c5bcfec}
Ho~et~al.,
\newblock ``Denoising diffusion probabilistic models,''
\newblock in {\em NeurIPS}, 2020, vol.~33, pp. 6840--6851.

\bibitem{MR-ART}
Nárai et~al.,
\newblock ``Movement-related artefacts (mr-art) dataset of matched
  motion-corrupted and clean structural mri brain scans,''
\newblock {\em Sci Data}, vol. 9, no. 1, Oct 2022.

\bibitem{olsson2024simulating}
Olsson et~al.,
\newblock ``Simulating rigid head motion artifacts on brain magnitude mri
  data--outcome on image quality and segmentation of the cerebral cortex,''
\newblock {\em PLoS One}, vol. 19, no. 4, pp. e0301132, 2024.

\bibitem{ddpm-github}
Higuera et~al.,
\newblock ``Learning to read braille: Bridging the tactile reality gap with
  diffusion models,''
\newblock 2023.

\bibitem{safari23}
Safari et~al.,
\newblock ``Mri motion artifact reduction using a conditional diffusion
  probabilistic model (mar-cdpm),''
\newblock {\em Med Phys}, vol. 51, no. 4, pp. 2598--2610, 2024.

\bibitem{zhu2017unpaired}
Zhu et~al.,
\newblock ``Unpaired image-to-image translation using cycle-consistent
  adversarial networks,''
\newblock in {\em ICCV}, 2017, pp. 2223--2232.

\bibitem{isola2017image}
Isola et~al.,
\newblock ``Image-to-image translation with conditional adversarial networks,''
\newblock in {\em CVPR}, 2017, pp. 1125--1134.

\end{thebibliography}


\end{document}
