@inproceedings{abbasi-yadkoriRegretBoundsAdaptive2011,
  title = {Regret {{Bounds}} for the {{Adaptive Control}} of {{Linear Quadratic Systems}}},
  author = {Abbasi-Yadkori, Yasin and Szepesvari, Csaba},
  date = {2011-12-21},
  url = {https://www.semanticscholar.org/paper/Regret-Bounds-for-the-Adaptive-Control-of-Linear-Abbasi-Yadkori-Szepesvari/c5d98a94db43a07d8e2f9a8f30cacc29224fec10},
  urldate = {2023-08-06},
  abstract = {We study the average cost Linear Quadratic (LQ) control problem with unknown model parameters, also known as the adaptive control problem in the control community. We design an algorithm and prove that apart from logarithmic factors its regret up to time T is O( p T ). Unlike previous approaches that use a forced-exploration scheme, we construct a high-probability condence set around the model parameters and design an algorithm that plays optimistically with respect to this condence set. The construction of the condence set is based on the recent results from online least-squares estimation and leads to improved worst-case regret bound for the proposed algorithm. To the best of our knowledge this is the the rst time that a regret bound is derived for the LQ control problem.},
  eventtitle = {Annual {{Conference Computational Learning Theory}}},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/abbasi-yadkoriRegretBoundsAdaptive2011.pdf}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@inproceedings{airsim2017fsr,
  author = {Shital Shah and Debadeepta Dey and Chris Lovett and Ashish Kapoor},
  title = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles},
  year = {2017},
  booktitle = {Field and Service Robotics},
  eprint = {arXiv:1705.05065},
  url = {https://arxiv.org/abs/1705.05065}
}
@inproceedings{adolfssonCFEARRadarodometryConservative2021,
  title = {{{CFEAR Radarodometry}} - {{Conservative Filtering}} for {{Efficient}} and {{Accurate Radar Odometry}}},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Adolfsson, Daniel and Magnusson, Martin and Alhashimi, Anas and Lilienthal, Achim J. and Andreasson, Henrik},
  date = {2021-09-27},
  pages = {5462--5469},
  publisher = {IEEE},
  location = {Prague, Czech Republic},
  doi = {10.1109/IROS51168.2021.9636253},
  url = {https://ieeexplore.ieee.org/document/9636253/},
  urldate = {2023-04-13},
  eventtitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  isbn = {978-1-66541-714-3},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/adolfssonCFEARRadarodometryConservative2021.pdf}
}

@article{adolfssonLidarLevelLocalizationRadar2023,
  title = {Lidar-{{Level Localization With Radar}}? {{The CFEAR Approach}} to {{Accurate}}, {{Fast}}, and {{Robust Large-Scale Radar Odometry}} in {{Diverse Environments}}},
  shorttitle = {Lidar-{{Level Localization With Radar}}?},
  author = {Adolfsson, Daniel and Magnusson, Martin and Alhashimi, Anas and Lilienthal, Achim J. and Andreasson, Henrik},
  date = {2023-04},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {39},
  number = {2},
  pages = {1476--1495},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2022.3221302},
  url = {https://ieeexplore.ieee.org/document/9969174/},
  urldate = {2023-04-16},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/adolfssonLidarLevelLocalizationRadar2023.pdf}
}

@online{agarwalTransformersReinforcementLearning2023,
  title = {Transformers in {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Transformers in {{Reinforcement Learning}}},
  author = {Agarwal, Pranav and Rahman, Aamer Abdul and St-Charles, Pierre-Luc and Prince, Simon J. D. and Kahou, Samira Ebrahimi},
  date = {2023-07-12},
  eprint = {2307.05979},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.05979},
  urldate = {2023-07-18},
  abstract = {Transformers have significantly impacted domains like natural language processing, computer vision, and robotics, where they improve performance compared to other neural networks. This survey explores how transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability. We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical RL algorithms. Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL. We examine the application of transformers to various aspects of RL, including representation learning, transition and reward function modeling, and policy optimization. We also discuss recent research that aims to enhance the interpretability and efficiency of transformers in RL, using visualization techniques and efficient training strategies. Often, the transformer architecture must be tailored to the specific needs of a given application. We present a broad overview of how transformers have been adapted for several applications, including robotics, medicine, language modeling, cloud computing, and combinatorial optimization. We conclude by discussing the limitations of using transformers in RL and assess their potential for catalyzing future breakthroughs in this field.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/agarwalTransformersReinforcementLearning2023.pdf;/Users/akhilnagariya/Zotero/storage/RVM7PNKX/2307.html}
}

@article{ahnCanNotSay2022,
  title = {Do {{As I Can}}, {{Not As I Say}}: {{Grounding Language}} in {{Robotic Affordances}}},
  shorttitle = {Do {{As I Can}}, {{Not As I Say}}},
  author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan and Zeng, Andy},
  date = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2204.01691},
  url = {https://arxiv.org/abs/2204.01691},
  urldate = {2023-10-26},
  abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/.},
  version = {2},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG),Robotics (cs.RO)},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/ahnCanNotSay2022.pdf}
}

@inproceedings{alderaWhatCouldGo2019,
  title = {What {{Could Go Wrong}}? {{Introspective Radar Odometry}} in {{Challenging Environments}}},
  shorttitle = {What {{Could Go Wrong}}?},
  booktitle = {2019 {{IEEE Intelligent Transportation Systems Conference}} ({{ITSC}})},
  author = {Aldera, Roberto and Martini, Daniele De and Gadd, Matthew and Newman, Paul},
  date = {2019-10},
  pages = {2835--2842},
  publisher = {IEEE},
  location = {Auckland, New Zealand},
  doi = {10.1109/ITSC.2019.8917111},
  url = {https://ieeexplore.ieee.org/document/8917111/},
  urldate = {2023-04-13},
  eventtitle = {2019 {{IEEE Intelligent Transportation Systems Conference}} - {{ITSC}}},
  isbn = {978-1-5386-7024-8},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/alderaWhatCouldGo2019.pdf}
}

@article{annaswamyAdaptiveControlIntersections2023,
  title = {Adaptive {{Control}} and {{Intersections}} with {{Reinforcement Learning}}},
  author = {Annaswamy, Anuradha M.},
  date = {2023-05-03},
  journaltitle = {Annual Review of Control, Robotics, and Autonomous Systems},
  shortjournal = {Annu. Rev. Control Robot. Auton. Syst.},
  volume = {6},
  number = {1},
  pages = {65--93},
  issn = {2573-5144, 2573-5144},
  doi = {10.1146/annurev-control-062922-090153},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-control-062922-090153},
  urldate = {2023-07-26},
  abstract = {This article provides an exposition of the field of adaptive control and its intersections with reinforcement learning. Adaptive control and reinforcement learning are two different methods that are both commonly employed for the control of uncertain systems. Historically, adaptive control has excelled at real-time control of systems with specific model structures through adaptive rules that learn the underlying parameters while providing strict guarantees on stability, asymptotic performance, and learning. Reinforcement learning methods are applicable to a broad class of systems and are able to produce near-optimal policies for highly complex control tasks. This is often enabled by significant offline training via simulation or the collection of large input-state datasets. This article attempts to compare adaptive control and reinforcement learning using a common framework. The problem statement in each field and highlights of their results are outlined. Two specific examples of dynamic systems are used to illustrate the details of the two methods, their advantages, and their deficiencies. The need for real-time control methods that leverage tools from both approaches is motivated through the lens of this common framework.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/annaswamyAdaptiveControlIntersections2023.pdf}
}

@article{annaswamyHistoricalPerspectiveAdaptive2021,
  title = {A Historical Perspective of Adaptive Control and Learning},
  author = {Annaswamy, Anuradha M. and Fradkov, Alexander L.},
  date = {2021},
  journaltitle = {Annual Reviews in Control},
  shortjournal = {Annual Reviews in Control},
  volume = {52},
  pages = {18--41},
  issn = {13675788},
  doi = {10.1016/j.arcontrol.2021.10.014},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1367578821000894},
  urldate = {2023-07-26},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/annaswamyHistoricalPerspectiveAdaptive2021.pdf}
}

@online{asadiLipschitzContinuityModelbased2018,
  title = {Lipschitz {{Continuity}} in {{Model-based Reinforcement Learning}}},
  author = {Asadi, Kavosh and Misra, Dipendra and Littman, Michael L.},
  date = {2018-07-27},
  eprint = {1804.07193},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1804.07193},
  url = {http://arxiv.org/abs/1804.07193},
  urldate = {2023-08-09},
  abstract = {We examine the impact of learning Lipschitz continuous models in the context of model-based reinforcement learning. We provide a novel bound on multi-step prediction error of Lipschitz models where we quantify the error using the Wasserstein metric. We go on to prove an error bound for the value-function estimate arising from Lipschitz models and show that the estimated value function is itself Lipschitz. We conclude with empirical results that show the benefits of controlling the Lipschitz constant of neural-network models.},
  pubstate = {preprint},
  version = {3},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/asadiLipschitzContinuityModelbased2018.pdf;/Users/akhilnagariya/Zotero/storage/USMLNIYK/1804.html}
}

@inproceedings{atkesonLearningTasksSingle1997,
  title = {Learning Tasks from a Single Demonstration},
  booktitle = {Proceedings of {{International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Atkeson, C.G. and Schaal, S.},
  date = {1997},
  volume = {2},
  pages = {1706--1712},
  publisher = {IEEE},
  location = {Albuquerque, NM, USA},
  doi = {10.1109/ROBOT.1997.614389},
  url = {http://ieeexplore.ieee.org/document/614389/},
  urldate = {2023-08-08},
  abstract = {Learning a complex dynamic robot maneuver from a single human demonstration is difficult. This paper explores an approach to learning from demonstration based on learning an optimization criterion from the demonstration and a task model from repeated attempts to perform the task, and using the learned criterion and model to compute an appropriate robot movement. A preliminary version of the approach has been implemented on an anthropomorphic robot arm using a pendulum swing up task as an example.},
  eventtitle = {International {{Conference}} on {{Robotics}} and {{Automation}}},
  isbn = {978-0-7803-3612-4},
  langid = {english},
  file = {/Users/akhilnagariya/Zotero/storage/Q7NGM32N/Atkeson and Schaal - 1997 - Learning tasks from a single demonstration.pdf}
}

@inproceedings{bagnellAutonomousHelicopterControl2001,
  title = {Autonomous {{Helicopter Control}} Using {{Reinforcement Learning}}},
  author = {Bagnell, J. and Schneider, J.},
  date = {2001},
  url = {https://www.semanticscholar.org/paper/Autonomous-Helicopter-Control-using-Reinforcement-Bagnell-Schneider/a6822f5ec7d08b7351bb301ef5e6f855a9c52a4a},
  urldate = {2023-08-08},
  abstract = {| Many control problems in the robotics eld can be cast as Partially Observed Markovian Decision Problems (POMDPs), an optimal control formalism. Finding optimal solutions to such problems in general, however is known to be intractable. It has often been observed that in practice, simple structured controllers su ce for good sub-optimal control, and recent research in the arti cial intelligence community has focused on policy search methods as techniques for nding sub-optimal controllers when such structured controllers do exist. Traditional model-based reinforcement learning algorithms make a certainty equivalence assumption on their learned models and calculate optimal policies for a maximumlikelihood Markovian model. In this work, we consider algorithms that evaluate and synthesize controllers under distributions of Markovian models. Previous work has demonstrated that algorithms that maximizemean reward with respect to model uncertainty leads to safer and more robust controllers. We consider brie y other performance criterion that emphasize robustness and exploration in the search for controllers, and note the relation with experiment design and active learning. To validate the power of the approach on a robotic application we demonstrate the presented learning control algorithm by ying an autonomous helicopter. We show that the controller learned is robust and delivers good performance in this real-world domain.},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/bagnellAutonomousHelicopterControl2001.pdf}
}

@article{bajracharyaAutonomousRoadNavigation2009,
  title = {Autonomous Off‐road Navigation with End‐to‐end Learning for the {{LAGR}} Program},
  author = {Bajracharya, Max and Howard, Andrew and Matthies, Larry H. and Tang, Benyang and Turmon, Michael},
  date = {2009-01},
  journaltitle = {Journal of Field Robotics},
  shortjournal = {Journal of Field Robotics},
  volume = {26},
  number = {1},
  pages = {3--25},
  issn = {1556-4959, 1556-4967},
  doi = {10.1002/rob.20269},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/rob.20269},
  urldate = {2024-03-06},
  abstract = {Abstract             We describe a fully integrated real‐time system for autonomous off‐road navigation that uses end‐to‐end learning from onboard proprioceptive sensors, operator input, and stereo cameras to adapt to local terrain and extend terrain classification into the far field to avoid myopic behavior. The system consists of two learning algorithms: a short‐range, geometry‐based local terrain classifier that learns from very few proprioceptive examples and is robust in many off‐road environments; and a long‐range, image‐based classifier that learns from geometry‐based classification and continuously generalizes geometry to appearance, making it effective even in complex terrain and varying lighting conditions. In addition to presenting the learning algorithms, we describe the system architecture and results from the Learning Applied to Ground Robots (LAGR) program's field tests. © 2008 Wiley Periodicals, Inc.},
  langid = {english},
  file = {/Users/akhilnagariya/Downloads/Journal of Field Robotics - 2008 - Bajracharya - Autonomous off‐road navigation with end‐to‐end learning for the LAGR.pdf}
}

@article{barfootExactlySparseGaussian2020,
  title = {Exactly Sparse {{Gaussian}} Variational Inference with Application to Derivative-Free Batch Nonlinear State Estimation},
  author = {Barfoot, Timothy D and Forbes, James R and Yoon, David J},
  date = {2020-11},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {39},
  number = {13},
  pages = {1473--1502},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364920937608},
  url = {http://journals.sagepub.com/doi/10.1177/0278364920937608},
  urldate = {2023-04-13},
  abstract = {We present a Gaussian variational inference (GVI) technique that can be applied to large-scale nonlinear batch state estimation problems. The main contribution is to show how to fit both the mean and (inverse) covariance of a Gaussian to the posterior efficiently, by exploiting factorization of the joint likelihood of the state and data, as is common in practical problems. This is different than maximum a posteriori (MAP) estimation, which seeks the point estimate for the state that maximizes the posterior (i.e., the mode). The proposed exactly sparse Gaussian variational inference (ESGVI) technique stores the inverse covariance matrix, which is typically very sparse (e.g., block-tridiagonal for classic state estimation). We show that the only blocks of the (dense) covariance matrix that are required during the calculations correspond to the non-zero blocks of the inverse covariance matrix, and further show how to calculate these blocks efficiently in the general GVI problem. ESGVI operates iteratively, and while we can use analytical derivatives at each iteration, Gaussian cubature can be substituted, thereby producing an efficient derivative-free batch formulation. ESGVI simplifies to precisely the Rauch–Tung–Striebel (RTS) smoother in the batch linear estimation case, but goes beyond the ‘extended’ RTS smoother in the nonlinear case because it finds the best-fit Gaussian (mean and covariance), not the MAP point estimate. We demonstrate the technique on controlled simulation problems and a batch nonlinear simultaneous localization and mapping problem with an experimental dataset.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/barfootExactlySparseGaussian2020.pdf}
}

@article{barnesMaskingMovingLearning2019,
  title = {Masking by {{Moving}}: {{Learning Distraction-Free Radar Odometry}} from {{Pose Information}}},
  shorttitle = {Masking by {{Moving}}},
  author = {Barnes, Dan and Weston, Rob and Posner, Ingmar},
  date = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1909.03752},
  url = {https://arxiv.org/abs/1909.03752},
  urldate = {2023-04-13},
  abstract = {This paper presents an end-to-end radar odometry system which delivers robust, real-time pose estimates based on a learned embedding space free of sensing artefacts and distractor objects. The system deploys a fully differentiable, correlation-based radar matching approach. This provides the same level of interpretability as established scan-matching methods and allows for a principled derivation of uncertainty estimates. The system is trained in a (self-)supervised way using only previously obtained pose information as a training signal. Using 280km of urban driving data, we demonstrate that our approach outperforms the previous state-of-the-art in radar odometry by reducing errors by up 68\% whilst running an order of magnitude faster.},
  version = {4},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Robotics (cs.RO)}
}

@online{barnesMaskingMovingLearning2020,
  title = {Masking by {{Moving}}: {{Learning Distraction-Free Radar Odometry}} from {{Pose Information}}},
  shorttitle = {Masking by {{Moving}}},
  author = {Barnes, Dan and Weston, Rob and Posner, Ingmar},
  date = {2020-01-17},
  eprint = {1909.03752},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1909.03752},
  urldate = {2023-04-13},
  abstract = {This paper presents an end-to-end radar odometry system which delivers robust, real-time pose estimates based on a learned embedding space free of sensing artefacts and distractor objects. The system deploys a fully differentiable, correlation-based radar matching approach. This provides the same level of interpretability as established scan-matching methods and allows for a principled derivation of uncertainty estimates. The system is trained in a (self-)supervised way using only previously obtained pose information as a training signal. Using 280km of urban driving data, we demonstrate that our approach outperforms the previous state-of-the-art in radar odometry by reducing errors by up 68\% whilst running an order of magnitude faster.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/barnesMaskingMovingLearning2020.pdf}
}

@inproceedings{barnesOxfordRadarRobotCar2020,
  title = {The {{Oxford Radar RobotCar Dataset}}: {{A Radar Extension}} to the {{Oxford RobotCar Dataset}}},
  shorttitle = {The {{Oxford Radar RobotCar Dataset}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Barnes, Dan and Gadd, Matthew and Murcutt, Paul and Newman, Paul and Posner, Ingmar},
  date = {2020-05},
  pages = {6433--6438},
  publisher = {IEEE},
  location = {Paris, France},
  doi = {10.1109/ICRA40945.2020.9196884},
  url = {https://ieeexplore.ieee.org/document/9196884/},
  urldate = {2023-04-12},
  eventtitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-72817-395-5},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/barnesOxfordRadarRobotCar2020.pdf}
}

@inproceedings{barnesRadarLearningPredict2020,
  title = {Under the {{Radar}}: {{Learning}} to {{Predict Robust Keypoints}} for {{Odometry Estimation}} and {{Metric Localisation}} in {{Radar}}},
  shorttitle = {Under the {{Radar}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Barnes, Dan and Posner, Ingmar},
  date = {2020-05},
  pages = {9484--9490},
  publisher = {IEEE},
  location = {Paris, France},
  doi = {10.1109/ICRA40945.2020.9196835},
  url = {https://ieeexplore.ieee.org/document/9196835/},
  urldate = {2023-04-13},
  eventtitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-72817-395-5},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/barnesRadarLearningPredict2020.pdf}
}

@inproceedings{bartlettREGALRegularizationBased2009,
  title = {{{REGAL}}: A Regularization Based Algorithm for Reinforcement Learning in Weakly Communicating {{MDPs}}},
  shorttitle = {{{REGAL}}},
  booktitle = {Proceedings of the {{Twenty-Fifth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Bartlett, Peter L. and Tewari, Ambuj},
  date = {2009-06-18},
  series = {{{UAI}} '09},
  pages = {35--42},
  publisher = {AUAI Press},
  location = {Arlington, Virginia, USA},
  abstract = {We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of Õ(HS√AT). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds.},
  isbn = {978-0-9749039-5-8},
  keywords = {Finit State MDP},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/bartlettREGALRegularizationBased2009.pdf}
}

@online{biggieTellMeWhere2023,
  title = {Tell {{Me Where}} to {{Go}}: {{A Composable Framework}} for {{Context-Aware Embodied Robot Navigation}}},
  shorttitle = {Tell {{Me Where}} to {{Go}}},
  author = {Biggie, Harel and Mopidevi, Ajay Narasimha and Woods, Dusty and Heckman, Christoffer},
  date = {2023-06-19},
  eprint = {2306.09523},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2306.09523},
  urldate = {2023-11-07},
  abstract = {Humans have the remarkable ability to navigate through unfamiliar environments by solely relying on our prior knowledge and descriptions of the environment. For robots to perform the same type of navigation, they need to be able to associate natural language descriptions with their associated physical environment with a limited amount of prior knowledge. Recently, Large Language Models (LLMs) have been able to reason over billions of parameters and utilize them in multi-modal chat-based natural language responses. However, LLMs lack real-world awareness and their outputs are not always predictable. In this work, we develop NavCon, a low-bandwidth framework that solves this lack of real-world generalization by creating an intermediate layer between an LLM and a robot navigation framework in the form of Python code. Our intermediate shoehorns the vast prior knowledge inherent in an LLM model into a series of input and output API instructions that a mobile robot can understand. We evaluate our method across four different environments and command classes on a mobile robot and highlight our NavCon's ability to interpret contextual commands.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/biggieTellMeWhere2023.pdf;/Users/akhilnagariya/Zotero/storage/KMJDXCHW/2306.html}
}

@article{bonciHumanRobotPerceptionIndustrial2021,
  title = {Human-{{Robot Perception}} in {{Industrial Environments}}: {{A Survey}}},
  shorttitle = {Human-{{Robot Perception}} in {{Industrial Environments}}},
  author = {Bonci, Andrea and Cen Cheng, Pangcheng David and Indri, Marina and Nabissi, Giacomo and Sibona, Fiorella},
  date = {2021-02-24},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {21},
  number = {5},
  pages = {1571},
  issn = {1424-8220},
  doi = {10.3390/s21051571},
  url = {https://www.mdpi.com/1424-8220/21/5/1571},
  urldate = {2024-01-23},
  abstract = {Perception capability assumes significant importance for human–robot interaction. The forthcoming industrial environments will require a high level of automation to be flexible and adaptive enough to comply with the increasingly faster and low-cost market demands. Autonomous and collaborative robots able to adapt to varying and dynamic conditions of the environment, including the presence of human beings, will have an ever-greater role in this context. However, if the robot is not aware of the human position and intention, a shared workspace between robots and humans may decrease productivity and lead to human safety issues. This paper presents a survey on sensory equipment useful for human detection and action recognition in industrial environments. An overview of different sensors and perception techniques is presented. Various types of robotic systems commonly used in industry, such as fixed-base manipulators, collaborative robots, mobile robots and mobile manipulators, are considered, analyzing the most useful sensors and methods to perceive and react to the presence of human operators in industrial cooperative and collaborative applications. The paper also introduces two proofs of concept, developed by the authors for future collaborative robotic applications that benefit from enhanced capabilities of human perception and interaction. The first one concerns fixed-base collaborative robots, and proposes a solution for human safety in tasks requiring human collision avoidance or moving obstacles detection. The second one proposes a collaborative behavior implementable upon autonomous mobile robots, pursuing assigned tasks within an industrial space shared with human operators.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/bonciHumanRobotPerceptionIndustrial2021.pdf}
}

@article{brohanRT1RoboticsTransformer2022,
  title = {{{RT-1}}: {{Robotics Transformer}} for {{Real-World Control}} at {{Scale}}},
  shorttitle = {{{RT-1}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil J and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Malla, Utsav and Manjunath, Deeksha and Mordatch, Igor and Nachum, Ofir and Parada, Carolina and Peralta, Jodilyn and Perez, Emily and Pertsch, Karl and Quiambao, Jornell and Rao, Kanishka and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sayed, Kevin and Singh, Jaspiar and Sontakke, Sumedh and Stone, Austin and Tan, Clayton and Tran, Huong and Vanhoucke, Vincent and Vega, Steve and Vuong, Quan and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  date = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2212.06817},
  url = {https://arxiv.org/abs/2212.06817},
  urldate = {2023-10-25},
  abstract = {By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io},
  version = {2},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Robotics (cs.RO)},
  file = {/Users/akhilnagariya/Zotero/storage/VRU2DHAS/brohanRT1RoboticsTransformer2022.pdf}
}

@article{brohanRT2VisionLanguageActionModels2023,
  title = {{{RT-2}}: {{Vision-Language-Action Models Transfer Web Knowledge}} to {{Robotic Control}}},
  shorttitle = {{{RT-2}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and Florence, Pete and Fu, Chuyuan and Arenas, Montse Gonzalez and Gopalakrishnan, Keerthana and Han, Kehang and Hausman, Karol and Herzog, Alexander and Hsu, Jasmine and Ichter, Brian and Irpan, Alex and Joshi, Nikhil and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Lisa and Lee, Tsang-Wei Edward and Levine, Sergey and Lu, Yao and Michalewski, Henryk and Mordatch, Igor and Pertsch, Karl and Rao, Kanishka and Reymann, Krista and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sermanet, Pierre and Singh, Jaspiar and Singh, Anikait and Soricut, Radu and Tran, Huong and Vanhoucke, Vincent and Vuong, Quan and Wahid, Ayzaan and Welker, Stefan and Wohlhart, Paul and Wu, Jialin and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  date = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2307.15818},
  url = {https://arxiv.org/abs/2307.15818},
  urldate = {2023-10-25},
  abstract = {We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).},
  version = {1},
  keywords = {Computation and Language (cs.CL),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Robotics (cs.RO)},
  file = {/Users/akhilnagariya/Zotero/storage/AMR67UK3/brohanRT2VisionLanguageActionModels2023.pdf}
}

@article{buckmanSampleEfficientReinforcementLearning2018,
  title = {Sample-{{Efficient Reinforcement Learning}} with {{Stochastic Ensemble Value Expansion}}},
  author = {Buckman, Jacob and Hafner, Danijar and Tucker, G. and Brevdo, E. and Lee, Honglak},
  date = {2018-07-04},
  journaltitle = {ArXiv},
  url = {https://www.semanticscholar.org/paper/Sample-Efficient-Reinforcement-Learning-with-Value-Buckman-Hafner/08555bf7d6a483f783b7508ed2df5b1a4d29661c},
  urldate = {2023-08-07},
  abstract = {Integrating model-free and model-based approaches in reinforcement learning has the potential to achieve the high performance of model-free algorithms with low sample complexity. However, this is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will almost always be imperfect. As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance. We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon lengths for each individual example, STEVE ensures that the model is only utilized when doing so does not introduce significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency, and in contrast to previous model-based approaches, performance does not degrade in complex environments.},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/buckmanSampleEfficientReinforcementLearning2018.pdf}
}

@inproceedings{burnettRadarOdometryCombining2021,
  title = {Radar {{Odometry Combining Probabilistic Estimation}} and {{Unsupervised Feature Learning}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVII}}},
  author = {Burnett*, Keenan and Yoon*, David and Schoellig, Angela and Barfoot, Tim},
  date = {2021-07-12},
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2021.XVII.029},
  url = {http://www.roboticsproceedings.org/rss17/p029.pdf},
  urldate = {2023-04-12},
  eventtitle = {Robotics: {{Science}} and {{Systems}} 2021},
  isbn = {978-0-9923747-7-8},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/burnettRadarOdometryCombining2021.pdf}
}

@article{burnettWeNeedCompensate2021,
  title = {Do {{We Need}} to {{Compensate}} for {{Motion Distortion}} and {{Doppler Effects}} in {{Spinning Radar Navigation}}?},
  author = {Burnett, Keenan and Schoellig, Angela P. and Barfoot, Timothy D.},
  date = {2021-04},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {6},
  number = {2},
  pages = {771--778},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2021.3052439},
  url = {https://ieeexplore.ieee.org/document/9327473/},
  urldate = {2023-04-13},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/burnettWeNeedCompensate2021.pdf}
}

@inproceedings{byravanNeRF2RealSim2realTransfer2023,
  title = {{{NeRF2Real}}: {{Sim2real Transfer}} of {{Vision-guided Bipedal Motion Skills}} Using {{Neural Radiance Fields}}},
  shorttitle = {{{NeRF2Real}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Byravan, Arunkumar and Humplik, Jan and Hasenclever, Leonard and Brussee, Arthur and Nori, Francesco and Haarnoja, Tuomas and Moran, Ben and Bohez, Steven and Sadeghi, Fereshteh and Vujatovic, Bojan and Heess, Nicolas},
  date = {2023-05-29},
  pages = {9362--9369},
  publisher = {IEEE},
  location = {London, United Kingdom},
  doi = {10.1109/ICRA48891.2023.10161544},
  url = {https://ieeexplore.ieee.org/document/10161544/},
  urldate = {2023-08-17},
  eventtitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {9798350323658},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/byravanNeRF2RealSim2realTransfer2023.pdf}
}

@online{caiProbabilisticTraversabilityModel2023a,
  title = {Probabilistic {{Traversability Model}} for {{Risk-Aware Motion Planning}} in {{Off-Road Environments}}},
  author = {Cai, Xiaoyi and Everett, Michael and Sharma, Lakshay and Osteen, Philip R. and How, Jonathan P.},
  date = {2023-07-31},
  eprint = {2210.00153},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2210.00153},
  urldate = {2024-02-25},
  abstract = {A key challenge in off-road navigation is that even visually similar terrains or ones from the same semantic class may have substantially different traction properties. Existing work typically assumes no wheel slip or uses the expected traction for motion planning, where the predicted trajectories provide a poor indication of the actual performance if the terrain traction has high uncertainty. In contrast, this work proposes to analyze terrain traversability with the empirical distribution of traction parameters in unicycle dynamics, which can be learned by a neural network in a self-supervised fashion. The probabilistic traction model leads to two risk-aware cost formulations that account for the worst-case expected cost and traction. To help the learned model generalize to unseen environment, terrains with features that lead to unreliable predictions are detected via a density estimator fit to the trained network's latent space and avoided via auxiliary penalties during planning. Simulation results demonstrate that the proposed approach outperforms existing work that assumes no slip or uses the expected traction in both navigation success rate and completion time. Furthermore, avoiding terrains with low density-based confidence score achieves up to 30\% improvement in success rate when the learned traction model is used in a novel environment.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/caiProbabilisticTraversabilityModel2023a.pdf;/Users/akhilnagariya/Zotero/storage/LVUQ66SC/2210.html}
}

@article{callmerRadarSLAMUsing2011,
  title = {Radar {{SLAM}} Using Visual Features},
  author = {Callmer, Jonas and Törnqvist, David and Gustafsson, Fredrik and Svensson, Henrik and Carlbom, Pelle},
  date = {2011-12},
  journaltitle = {EURASIP Journal on Advances in Signal Processing},
  shortjournal = {EURASIP J. Adv. Signal Process.},
  volume = {2011},
  number = {1},
  pages = {71},
  issn = {1687-6180},
  doi = {10.1186/1687-6180-2011-71},
  url = {https://asp-eurasipjournals.springeropen.com/articles/10.1186/1687-6180-2011-71},
  urldate = {2023-04-13},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/callmerRadarSLAMUsing2011.pdf}
}

@inproceedings{cenPreciseEgoMotionEstimation2018,
  title = {Precise {{Ego-Motion Estimation}} with {{Millimeter-Wave Radar Under Diverse}} and {{Challenging Conditions}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Cen, Sarah H. and Newman, Paul},
  date = {2018-05},
  pages = {1--8},
  publisher = {IEEE},
  location = {Brisbane, QLD},
  doi = {10.1109/ICRA.2018.8460687},
  url = {https://ieeexplore.ieee.org/document/8460687/},
  urldate = {2023-04-13},
  eventtitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-5386-3081-5},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/cenPreciseEgoMotionEstimation2018.pdf}
}

@inproceedings{cenRadaronlyEgomotionEstimation2019,
  title = {Radar-Only Ego-Motion Estimation in Difficult Settings via Graph Matching},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Cen, Sarah H. and Newman, Paul},
  date = {2019-05},
  pages = {298--304},
  publisher = {IEEE},
  location = {Montreal, QC, Canada},
  doi = {10.1109/ICRA.2019.8793990},
  url = {https://ieeexplore.ieee.org/document/8793990/},
  urldate = {2023-04-13},
  eventtitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-5386-6027-0},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/cenRadaronlyEgomotionEstimation2019.pdf}
}

@online{chenDirectLiDAROdometry2022,
  title = {Direct {{LiDAR Odometry}}: {{Fast Localization}} with {{Dense Point Clouds}}},
  shorttitle = {Direct {{LiDAR Odometry}}},
  author = {Chen, Kenny and Lopez, Brett T. and Agha-mohammadi, Ali-akbar and Mehta, Ankur},
  date = {2022-01-07},
  eprint = {2110.00605},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2110.00605},
  urldate = {2024-02-22},
  abstract = {Field robotics in perceptually-challenging environments require fast and accurate state estimation, but modern LiDAR sensors quickly overwhelm current odometry algorithms. To this end, this paper presents a lightweight frontend LiDAR odometry solution with consistent and accurate localization for computationally-limited robotic platforms. Our Direct LiDAR Odometry (DLO) method includes several key algorithmic innovations which prioritize computational efficiency and enables the use of dense, minimally-preprocessed point clouds to provide accurate pose estimates in real-time. This is achieved through a novel keyframing system which efficiently manages historical map information, in addition to a custom iterative closest point solver for fast point cloud registration with data structure recycling. Our method is more accurate with lower computational overhead than the current state-of-the-art and has been extensively evaluated in multiple perceptually-challenging environments on aerial and legged robots as part of NASA JPL Team CoSTAR's research and development efforts for the DARPA Subterranean Challenge.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/chenDirectLiDAROdometry22.pdf;/Users/akhilnagariya/Zotero/storage/7P8MD2RW/2110.html}
}

@article{chenDSGNDeepStereo2020,
  title = {{{DSGN}}: {{Deep Stereo Geometry Network}} for {{3D Object Detection}}},
  shorttitle = {{{DSGN}}},
  author = {Chen, Yilun and Liu, Shu and Shen, Xiaoyong and Jia, Jiaya},
  date = {2020-06},
  journaltitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {12533--12542},
  publisher = {IEEE},
  location = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.01255},
  url = {https://ieeexplore.ieee.org/document/9157793/},
  urldate = {2023-04-20},
  abstract = {Most state-of-the-art 3D object detectors rely heavily on LiDAR sensors and there remains a large gap in terms of performance between image-based and LiDAR-based methods, caused by inappropriate representation for the prediction in 3D scenarios. Our method, called Deep Stereo Geometry Network (DSGN), reduces this gap significantly by detecting 3D objects on a differentiable volumetric representation -- 3D geometric volume, which effectively encodes 3D geometric structure for 3D regular space. With this representation, we learn depth information and semantic cues simultaneously. For the first time, we provide a simple and effective one-stage stereo-based 3D detection pipeline that jointly estimates the depth and detects 3D objects in an end-to-end learning manner. Our approach outperforms previous stereo-based 3D detectors (about 10 higher in terms of AP) and even achieves comparable performance with a few LiDAR-based methods on the KITTI 3D object detection leaderboard. Code will be made publicly available at https://github.com/chenyilun95/DSGN.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {9781728171685},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/chenDSGNDeepStereo2020.pdf}
}

@inproceedings{chenEProPnPGeneralizedEndtoEnd2022,
  title = {{{EPro-PnP}}: {{Generalized End-to-End Probabilistic Perspective-n-Points}} for {{Monocular Object Pose Estimation}}},
  shorttitle = {{{EPro-PnP}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chen, Hansheng and Wang, Pichao and Wang, Fan and Tian, Wei and Xiong, Lu and Li, Hao},
  date = {2022-06},
  pages = {2771--2780},
  publisher = {IEEE},
  location = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.00280},
  url = {https://ieeexplore.ieee.org/document/9879345/},
  urldate = {2023-04-18},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/chenEProPnPGeneralizedEndtoEnd2022.pdf}
}

@incollection{chenPersFormer3DLane2022,
  title = {{{PersFormer}}: {{3D Lane Detection}} via {{Perspective Transformer}} and the {{OpenLane Benchmark}}},
  shorttitle = {{{PersFormer}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2022},
  author = {Chen, Li and Sima, Chonghao and Li, Yang and Zheng, Zehan and Xu, Jiajie and Geng, Xiangwei and Li, Hongyang and He, Conghui and Shi, Jianping and Qiao, Yu and Yan, Junchi},
  editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  date = {2022},
  volume = {13698},
  pages = {550--567},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-19839-7_32},
  url = {https://link.springer.com/10.1007/978-3-031-19839-7_32},
  urldate = {2023-04-20},
  isbn = {978-3-031-19838-0 978-3-031-19839-7},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/chenPersFormer3DLane2022.pdf}
}

@online{chenRandomizedEnsembledDouble2021,
  title = {Randomized {{Ensembled Double Q-Learning}}: {{Learning Fast Without}} a {{Model}}},
  shorttitle = {Randomized {{Ensembled Double Q-Learning}}},
  author = {Chen, Xinyue and Wang, Che and Zhou, Zijian and Ross, Keith},
  date = {2021-03-17},
  eprint = {2101.05982},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2101.05982},
  urldate = {2023-04-29},
  abstract = {Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio {$>>$} 1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio {$>>$} 1.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/chenRandomizedEnsembledDouble2021.pdf;/Users/akhilnagariya/Zotero/storage/UY39EYGB/2101.html}
}

@online{chiDiffusionPolicyVisuomotor2023,
  title = {Diffusion {{Policy}}: {{Visuomotor Policy Learning}} via {{Action Diffusion}}},
  shorttitle = {Diffusion {{Policy}}},
  author = {Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin and Song, Shuran},
  date = {2023-06-01},
  eprint = {2303.04137},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.04137},
  urldate = {2023-11-10},
  abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details will be publicly available.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/chiDiffusionPolicyVisuomotor2023.pdf;/Users/akhilnagariya/Zotero/storage/A3P27QYL/2303.html}
}

@online{chuaDeepReinforcementLearning2018,
  title = {Deep {{Reinforcement Learning}} in a {{Handful}} of {{Trials}} Using {{Probabilistic Dynamics Models}}},
  author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  date = {2018-11-02},
  eprint = {1805.12114},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.12114},
  urldate = {2023-05-06},
  abstract = {Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Ensemble Probabilistic Networks for Dynamics,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/chuaDeepReinforcementLearning2018.pdf;/Users/akhilnagariya/Zotero/storage/SVG528EB/1805.html}
}

@online{claveraModelBasedReinforcementLearning2018,
  title = {Model-{{Based Reinforcement Learning}} via {{Meta-Policy Optimization}}},
  author = {Clavera, Ignasi and Rothfuss, Jonas and Schulman, John and Fujita, Yasuhiro and Asfour, Tamim and Abbeel, Pieter},
  date = {2018-09-13},
  eprint = {1809.05214},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1809.05214},
  url = {http://arxiv.org/abs/1809.05214},
  urldate = {2023-08-08},
  abstract = {Model-based reinforcement learning approaches carry the promise of being data efficient. However, due to challenges in learning dynamics models that sufficiently match the real-world dynamics, they struggle to achieve the same asymptotic performance as model-free methods. We propose Model-Based Meta-Policy-Optimization (MB-MPO), an approach that foregoes the strong reliance on accurate learned dynamics models. Using an ensemble of learned dynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model in the ensemble with one policy gradient step. This steers the meta-policy towards internalizing consistent dynamics predictions among the ensemble while shifting the burden of behaving optimally w.r.t. the model discrepancies towards the adaptation step. Our experiments show that MB-MPO is more robust to model imperfections than previous model-based approaches. Finally, we demonstrate that our approach is able to match the asymptotic performance of model-free methods while requiring significantly less experience.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/claveraModelBasedReinforcementLearning2018.pdf;/Users/akhilnagariya/Zotero/storage/IYUVRAZJ/1809.html}
}

@online{csomay-shanklinNonlinearModelPredictive2023,
  title = {Nonlinear {{Model Predictive Control}} of a {{3D Hopping Robot}}: {{Leveraging Lie Group Integrators}} for {{Dynamically Stable Behaviors}}},
  shorttitle = {Nonlinear {{Model Predictive Control}} of a {{3D Hopping Robot}}},
  author = {Csomay-Shanklin, Noel and Dorobantu, Victor D. and Ames, Aaron D.},
  date = {2023-06-06},
  eprint = {2209.11808},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.11808},
  url = {http://arxiv.org/abs/2209.11808},
  urldate = {2023-08-17},
  abstract = {Achieving stable hopping has been a hallmark challenge in the field of dynamic legged locomotion. Controlled hopping is notably difficult due to extended periods of underactuation combined with very short ground phases wherein ground interactions must be modulated to regulate global state. In this work, we explore the use of hybrid nonlinear model predictive control paired with a low-level feedback controller in a multi-rate hierarchy to achieve dynamically stable motions on a 3D hopping robot. In order to demonstrate richer behaviors on the manifold of rotations, both the planning and feedback layers must be designed in a geometrically consistent fashion; therefore, we develop the necessary tools to employ Lie group integrators and appropriate feedback controllers. We experimentally demonstrate stable 3D hopping, as well as trajectory tracking and flipping in simulation.},
  pubstate = {preprint},
  version = {4},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/csomay-shanklinNonlinearModelPredictive2023.pdf;/Users/akhilnagariya/Zotero/storage/BJ6QQAGT/2209.html}
}

@online{czarneckiDistillingPolicyDistillation2019,
  title = {Distilling {{Policy Distillation}}},
  author = {Czarnecki, Wojciech Marian and Pascanu, Razvan and Osindero, Simon and Jayakumar, Siddhant M. and Swirszcz, Grzegorz and Jaderberg, Max},
  date = {2019-02-06},
  eprint = {1902.02186},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1902.02186},
  url = {http://arxiv.org/abs/1902.02186},
  urldate = {2023-10-05},
  abstract = {The transfer of knowledge from one policy to another is an important tool in Deep Reinforcement Learning. This process, referred to as distillation, has been used to great success, for example, by enhancing the optimisation of agents, leading to stronger performance faster, on harder domains [26, 32, 5, 8]. Despite the widespread use and conceptual simplicity of distillation, many different formulations are used in practice, and the subtle variations between them can often drastically change the performance and the resulting objective that is being optimised. In this work, we rigorously explore the entire landscape of policy distillation, comparing the motivations and strengths of each variant through theoretical and empirical analysis. Our results point to three distillation techniques, that are preferred depending on specifics of the task. Specifically a newly proposed expected entropy regularised distillation allows for quicker learning in a wide range of situations, while still guaranteeing convergence.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/czarneckiDistillingPolicyDistillation2019.pdf;/Users/akhilnagariya/Zotero/storage/FD9K26AT/1902.html}
}

@online{daiOptimalSceneGraph2023,
  title = {Optimal {{Scene Graph Planning}} with {{Large Language Model Guidance}}},
  author = {Dai, Zhirui and Asgharivaskasi, Arash and Duong, Thai and Lin, Shusen and Tzes, Maria-Elizabeth and Pappas, George and Atanasov, Nikolay},
  date = {2023-09-17},
  eprint = {2309.09182},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.09182},
  urldate = {2023-11-07},
  abstract = {Recent advances in metric, semantic, and topological mapping have equipped autonomous robots with semantic concept grounding capabilities to interpret natural language tasks. This work aims to leverage these new capabilities with an efficient task planning algorithm for hierarchical metric-semantic models. We consider a scene graph representation of the environment and utilize a large language model (LLM) to convert a natural language task into a linear temporal logic (LTL) automaton. Our main contribution is to enable optimal hierarchical LTL planning with LLM guidance over scene graphs. To achieve efficiency, we construct a hierarchical planning domain that captures the attributes and connectivity of the scene graph and the task automaton, and provide semantic guidance via an LLM heuristic function. To guarantee optimality, we design an LTL heuristic function that is provably consistent and supplements the potentially inadmissible LLM guidance in multi-heuristic planning. We demonstrate efficient planning of complex natural language tasks in scene graphs of virtualized real environments.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/daiOptimalSceneGraph2023.pdf;/Users/akhilnagariya/Zotero/storage/TAADDH8P/2309.html}
}

@inproceedings{daoSimtoRealLearningBipedal2022,
  title = {Sim-to-{{Real Learning}} for {{Bipedal Locomotion Under Unsensed Dynamic Loads}}},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Dao, Jeremy and Green, Kevin and Duan, Helei and Fern, Alan and Hurst, Jonathan},
  date = {2022-05},
  pages = {10449--10455},
  doi = {10.1109/ICRA46639.2022.9811783},
  url = {https://ieeexplore.ieee.org/document/9811783},
  urldate = {2023-10-05},
  abstract = {Recent work on sim-to-real learning for bipedal locomotion has demonstrated new levels of robustness and agility over a variety of terrains. However, that work, and most prior bipedal locomotion work, have not considered locomotion under a variety of external loads that can significantly influence the overall system dynamics. In many applications, robots will need to maintain robust locomotion under a wide range of potential dynamic loads, such as pulling a cart or carrying a large container of sloshing liquid, ideally without requiring additional load-sensing capabilities. In this work, we explore the capabilities of reinforcement learning (RL) and sim-to-real transfer for bipedal locomotion under dynamic loads using only proprioceptive feedback. We show that prior RL policies trained for unloaded locomotion fail for some loads and that simply training in the context of loads is enough to result in successful and improved policies. We also compare training specialized policies for each load versus a single policy for all considered loads and analyze how the resulting gaits change to accommodate different loads. Finally, we demonstrate sim-to-real transfer, which is successful but shows a wider sim-to-real gap than prior unloaded work, which points to interesting future research.},
  eventtitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/daoSimtoRealLearningBipedal2022.pdf;/Users/akhilnagariya/Zotero/storage/T8JUE85X/9811783.html}
}

@inproceedings{dasilvalubancoComparisonReflectivityDifferent2020,
  title = {A {{Comparison}} about the {{Reflectivity}} of {{Different Materials}} with {{Active Sensors}}},
  booktitle = {2020 5th {{International Conference}} on {{Robotics}} and {{Automation Engineering}} ({{ICRAE}})},
  author = {family=Silva Lubanco, given=Daniel Louback, prefix=da, useprefix=true and Kaineder, Gerhard and Scherhaufl, Martin and Schlechter, Thomas and Salmen, Daniel},
  date = {2020-11-20},
  pages = {59--63},
  publisher = {IEEE},
  location = {Singapore, Singapore},
  doi = {10.1109/ICRAE50850.2020.9310883},
  url = {https://ieeexplore.ieee.org/document/9310883/},
  urldate = {2023-04-13},
  eventtitle = {2020 5th {{International Conference}} on {{Robotics}} and {{Automation Engineering}} ({{ICRAE}})},
  isbn = {978-1-72818-981-9}
}

@online{deanSampleComplexityLinear2018,
  title = {On the {{Sample Complexity}} of the {{Linear Quadratic Regulator}}},
  author = {Dean, Sarah and Mania, Horia and Matni, Nikolai and Recht, Benjamin and Tu, Stephen},
  date = {2018-12-13},
  eprint = {1710.01688},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1710.01688},
  urldate = {2023-08-06},
  abstract = {This paper addresses the optimal control problem known as the Linear Quadratic Regulator in the case when the dynamics are unknown. We propose a multi-stage procedure, called Coarse-ID control, that estimates a model from a few experimental trials, estimates the error in that model with respect to the truth, and then designs a controller using both the model and uncertainty estimate. Our technique uses contemporary tools from random matrix theory to bound the error in the estimation procedure. We also employ a recently developed approach to control synthesis called System Level Synthesis that enables robust control design by solving a convex optimization problem. We provide end-to-end bounds on the relative error in control cost that are nearly optimal in the number of parameters and that highlight salient properties of the system to be controlled such as closed-loop sensitivity and optimal control magnitude. We show experimentally that the Coarse-ID approach enables efficient computation of a stabilizing controller in regimes where simple control schemes that do not take the model uncertainty into account fail to stabilize the true system.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/deanSampleComplexityLinear2018.pdf;/Users/akhilnagariya/Zotero/storage/TC27FIYH/1710.html}
}

@inproceedings{deisenrothLearningControlLowCost2011,
  title = {Learning to {{Control}} a {{Low-Cost Manipulator}} Using {{Data-Efficient Reinforcement Learning}}},
  booktitle = {Robotics: {{Science}} and {{Systems VII}}},
  author = {Deisenroth, Marc and Rasmussen, Carl and Fox, Dieter},
  date = {2011-06-27},
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2011.VII.008},
  url = {http://www.roboticsproceedings.org/rss07/p08.pdf},
  urldate = {2023-08-08},
  abstract = {Over the last years, there has been substantial progress in robust manipulation in unstructured environments. The long-term goal of our work is to get away from precise, but very expensive robotic systems and to develop affordable, potentially imprecise, self-adaptive manipulator systems that can interactively perform tasks such as playing with children. In this paper, we demonstrate how a low-cost off-the-shelf robotic system can learn closed-loop policies for a stacking task in only a handful of trials-from scratch. Our manipulator is inaccurate and provides no pose feedback. For learning a controller in the work space of a Kinect-style depth camera, we use a model-based reinforcement learning technique. Our learning method is data efficient, reduces model bias, and deals with several noise sources in a principled way during long-term planning. We present a way of incorporating state-space constraints into the learning process and analyze the learning gain by exploiting the sequential structure of the stacking task.},
  eventtitle = {Robotics: {{Science}} and {{Systems}} 2011},
  isbn = {978-0-262-51779-9},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/deisenrothLearningControlLowCost2011.pdf}
}

@inproceedings{deisenrothPILCOModelBasedDataEfficient2011,
  title = {{{PILCO}}: {{A Model-Based}} and {{Data-Efficient Approach}} to {{Policy Search}}},
  shorttitle = {{{PILCO}}},
  author = {Deisenroth, M. and Rasmussen, C.},
  date = {2011-06-28},
  url = {https://www.semanticscholar.org/paper/PILCO%3A-A-Model-Based-and-Data-Efficient-Approach-to-Deisenroth-Rasmussen/60b7d47758a71978e74edff6dd8dea4d9c791d7a},
  urldate = {2023-08-06},
  abstract = {In this paper, we introduce PILCO, a practical, data-efficient model-based policy search method. PILCO reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, PILCO can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  keywords = {Gaussian Process},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/deisenrothPILCOModelBasedDataEfficient2011.pdf}
}

@article{deisenrothSurveyPolicySearch2013,
  title = {A {{Survey}} on {{Policy Search}} for {{Robotics}}},
  author = {Deisenroth, Marc Peter},
  date = {2013},
  journaltitle = {Foundations and Trends in Robotics},
  shortjournal = {FNT in Robotics},
  volume = {2},
  number = {1-2},
  pages = {1--142},
  issn = {1935-8253, 1935-8261},
  doi = {10.1561/2300000021},
  url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-robotics/ROB-021},
  urldate = {2023-08-06},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/deisenrothSurveyPolicySearch2011.pdf}
}

@online{depewegLearningPolicySearch2016,
  title = {Learning and {{Policy Search}} in {{Stochastic Dynamical Systems}} with {{Bayesian Neural Networks}}},
  author = {Depeweg, Stefan and Hernández-Lobato, José Miguel and Doshi-Velez, Finale and Udluft, Steffen},
  date = {2016-03-07},
  eprint = {1605.07127},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1605.07127},
  urldate = {2023-05-06},
  abstract = {We present an algorithm for model-based reinforcement learning that combines Bayesian neural networks (BNNs) with random roll-outs and stochastic optimization for policy learning. The BNNs are trained by minimizing \$\textbackslash alpha\$-divergences, allowing us to capture complicated statistical patterns in the transition dynamics, e.g. multi-modality and heteroskedasticity, which are usually missed by other common modeling approaches. We illustrate the performance of our method by solving a challenging benchmark where model-based approaches usually fail and by obtaining promising results in a real-world scenario for controlling a gas turbine.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,NN based dynamic model,Statistics - Machine Learning},
  annotation = {most recent 2017},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/depewegLearningPolicySearch2017.pdf;/Users/akhilnagariya/Zotero/storage/48XE5W69/1605.html}
}

@online{djeumouAutonomousDriftingMinutes2023,
  title = {Autonomous {{Drifting}} with 3 {{Minutes}} of {{Data}} via {{Learned Tire Models}}},
  author = {Djeumou, Franck and Goh, Jonathan Y. M. and Topcu, Ufuk and Balachandran, Avinash},
  date = {2023-06-09},
  eprint = {2306.06330},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2306.06330},
  url = {http://arxiv.org/abs/2306.06330},
  urldate = {2023-08-17},
  abstract = {Near the limits of adhesion, the forces generated by a tire are nonlinear and intricately coupled. Efficient and accurate modelling in this region could improve safety, especially in emergency situations where high forces are required. To this end, we propose a novel family of tire force models based on neural ordinary differential equations and a neural-ExpTanh parameterization. These models are designed to satisfy physically insightful assumptions while also having sufficient fidelity to capture higher-order effects directly from vehicle state measurements. They are used as drop-in replacements for an analytical brush tire model in an existing nonlinear model predictive control framework. Experiments with a customized Toyota Supra show that scarce amounts of driving data -- less than three minutes -- is sufficient to achieve high-performance autonomous drifting on various trajectories with speeds up to 45mph. Comparisons with the benchmark model show a \$4 \textbackslash times\$ improvement in tracking performance, smoother control inputs, and faster and more consistent computation time.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/djeumouAutonomousDriftingMinutes2023.pdf;/Users/akhilnagariya/Zotero/storage/Q36XRI9G/2306.html}
}

@inproceedings{dolgovPracticalSearchTechniques2008,
  title = {Practical {{Search Techniques}} in {{Path Planning}} for {{Autonomous Driving}}},
  author = {Dolgov, D.},
  date = {2008},
  url = {https://www.semanticscholar.org/paper/Practical-Search-Techniques-in-Path-Planning-for-Dolgov/62a7cf939e24bf542958489ea75bb7551f16e43f},
  urldate = {2024-02-27},
  abstract = {We describe a practical path-planning algorithm that generates smooth paths for an autonomous vehicle operating in an unknown environment, where obstacles are detected online by the robot’s sensors. This work was motivated by and experimentally validated in the 2007 DARPA Urban Challenge, where robotic vehicles had to autonomously navigate parking lots. Our approach has two main steps. The first step uses a variant of the well-known A* search algorithm, applied to the 3D kinematic state space of the vehicle, but with a modified state-update rule that captures the continuous state of the vehicle in the discrete nodes of A* (thus guaranteeing kinematic feasibility of the path). The second step then improves the quality of the solution via numeric non-linear optimization, leading to a local (and frequently global) optimum. The path-planning algorithm described in this paper was used by the Stanford Racing Teams robot, Junior, in the Urban Challenge. Junior demonstrated flawless performance in complex general path-planning tasks such as navigating parking lots and executing U-turns on blocked roads, with typical fullcycle replaning times of 50–300ms. Introduction and Related Work We address the problem of path planning for an autonomous vehicle operating in an unknown environment. We assume the robot has adequate sensing and localization capability and must replan online while incrementally building an obstacle map. This scenario was motivated, in part, by the DARPA Urban Challenge, in which vehicles had to freely navigate parking lots. The path-planning algorithm described below was used by the Stanford Racing Team’s robot, Junior in the Urban Challenge (DARPA 2007). Junior (Figure 1) demonstrated flawless performance in complex general path-planning tasks—many involving driving in reverse—such as navigating parking lots, executing Uturns, and dealing with blocked roads and intersections with typical full-cycle replanning times of 50–300ms on a modern PC. One of the main challenges in developing a practical path planner for free navigation zones arises from the fact that the space of all robot controls—and hence trajectories—is continuous, leading to a complex continuous-variable optimization landscape. Much of prior work on search algorithms for Copyright c © 2008, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Junior, our entry in the DARPA Urban Challenge, was used in all experiments. Junior is equipped with several LIDAR and RADAR units, and a high-accuracy inertial measurement system. path planning (Ersson and Hu 2001; Koenig and Likhachev 2002; Ferguson and Stentz 2005; Nash et al. 2007) yields fast algorithms for discrete state spaces, but those algorithms tend to produce paths that are non-smooth and do not generally satisfy the non-holonomic constraints of the vehicle. An alternative approach that guarantees kinematic feasibility is forward search in continuous coordinates, e.g., using rapidly exploring random trees (RRTs) (Kavraki et al. 1996; LaValle 1998; Plaku, Kavraki, and Vardi 2007). The key to making such continuous search algorithms practical for online implementations lies in an efficient guiding heuristic. Another approach is to directly formulate the path-planning problem as a non-linear optimization problem in the space of controls or parametrized curves (Cremean et al. 2006), but in practice guaranteeing fast convergence of such programs is difficult due to local minima. Our algorithm builds on the existing work discussed above, and consists of two main phases. The first step uses a heuristic search in continuous coordinates that guarantees kinematic feasibility of computed trajectories. While lacking theoretical optimality guarantees, in practice this first Figure 2: Graphical comparison of search algorithms. Left: A* associates costs with centers of cells and only visits states that correspond to grid-cell centers. Center: Field D* (Ferguson and Stentz 2005) and Theta* (Nash et al. 2007) associate costs with cell corners and allow arbitrary linear paths from cell to cell. Right: Hybrid A* associates a continuous state with each cell and the score of the cell is the cost of its associated continuous state. step typically produces a trajectory that lies in a neighborhood of the global optimum. The second step uses conjugate gradient (CG) descent to locally improve the quality of the solution, producing a path that is at least locally optimal, but usually attains the global optimum as well. Another practical challenge is the design of a cost function over paths that yields the desired driving behavior. The difficulty stems from the fact that we would like to obtain paths that are near-optimal in length, but at the same time are smooth and keep a comfortable distance to obstacles. A common way of penalizing proximity to obstacles is to use a potential field (Andrews and Hogan 1983; Khatib 1986; Pavlov and Voronin 1984; Miyazaki and Arimoto 1985). However, as has been observed by many researchers (Tilove 1990; Koren and Borenstein 1991), one of the drawbacks of potential fields is that they create high-potential areas in narrow passages, thereby making those passages effectively untraversable. To address this issues, we introduce a potential that rescales the field based on the geometry of the workspace, allowing precise navigation in narrow passages while also effectively pushing the robot away from obstacles in wider-open areas. Hybrid-State A* Search The first phase of our approach uses a variant of the wellknown A* algorithm applied to the 3D kinematic state space of the vehicle, but with a modified state-update rule that captures continuous-state data in the discrete search nodes of A*. Just as in conventional A*, the search space (x, y, θ) is discretized, but unlike traditional A* which only allows visiting centers of cells, our hybrid-state A* associates with each grid cell a continuous 3D state of the vehicle, as illustrated in Figure 2. As noted above, our hybrid-state A* is not guaranteed to find the minimal-cost solution, due to its merging of continuous-coordinate states that occupy the same cell in the discretized space. However, the resulting path is guaranteed to be drivable (rather than being piecewise-linear as in the case of standard A*). Also, in practice, the hybrid-A* solution typically lies in the neighborhood of the global optimum, allowing us to frequently arrive at the globally optimal solution via the second phase of our algorithm (which uses gradient descent to locally improve the path, as described below). The main advantage of hybrid-state A* manifests itself in maneuvers in tight spaces, where the discretization errors become critical. Our algorithm plans forward and reverse motion, with penalties for driving in reverse as well as switching the direction of motion. Heuristics Our search algorithm is guided by two heuristics, illustrated in Figure 3. These heuristics do not rely on any properties of hybrid-state A* and are also applicable to other search methods (e.g., discrete A*). The first heuristic—which we call “non-holonomicwithout-obstacles”—ignores obstacles but takes into account the non-holonomic nature of the car. To compute it, we assume a goal state of (xg, yg, θg) = (0, 0, 0) and compute the shortest path to the goal from every point (x, y, θ) in some discretized neighborhood of the goal, assuming complete absence of obstacles.1 Clearly, this cost is an admissible heuristic. We then use a max of the non-holonomicwithout-obstacles cost and 2D Euclidean distance as our heuristic. The effect of this heuristic is that it prunes search branches that approach the goal with the wrong headings. Notice that because this heuristic does not depend on runtime sensor information, it can be fully pre-computed offline and then simply translated and rotated to match the current goal. In our experiments in real driving scenarios, this heuristic provided close to an order-of-magnitude improvement in the number of nodes expanded over the straightforward 2D Euclidean-distance cost. The second heuristic is a dual of the first in that it ignores the non-holonomic nature of the car, but uses the obstacle map to compute the shortest distance to the goal by performing dynamic programming in 2D. The benefit of this heuristic is that it discovers all U-shaped obstacles and dead-ends in 2D and then guides the more expensive 3D search away from these areas. Both heuristics are mathematically admissible in the A* sense, so the maximum of the two can be used. Analytic Expansions The forward search described above uses a discretized space of control actions (steering). This means that the search will never reach the exact continuouscoordinate goal state (the accuracy depends on the resolution of the grid in A*). To address this precision issue, and to further improve search speed, we augment the search with analytic expansions based on the Reed-Shepp model (Reeds and Shepp 1990). In the search described above, a node in the tree is expanded by simulating a kinematic model of the car—using a particular control action—for a small period of time (corresponding to the resolution of the grid). In addition to children generated in such a way, for some nodes, an additional child is generated by computing an optimal Reed-and-Shepp path from the current state to the goal (assuming an obstacle-free environment). The Reed-andShepp path is then checked for collisions against the current obstacle map, and the children node is only added to We used a 160x160 grid with 1m resolution in x-y and 5◦ angular resolution.},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/dolgovPracticalSearchTechniques2008.pdf}
}

@article{draegerModelPredictiveControl1995,
  title = {Model Predictive Control Using Neural Networks},
  author = {Draeger, A. and Engell, S. and Ranke, H.},
  date = {1995-10},
  journaltitle = {IEEE Control Systems Magazine},
  volume = {15},
  number = {5},
  pages = {61--66},
  issn = {1941-000X},
  doi = {10.1109/37.466261},
  abstract = {In this article, we present the application of a neural-network-based model predictive control scheme to control pH in a laboratory-scale neutralization reactor. We use a feedforward neural network as the nonlinear prediction model in an extended DMC-algorithm to control the pH-value. The training data set for the neural network was obtained from measurements of the inputs and outputs of the real plant operating with a PI-controller. Thus, no a priori information about the dynamics of the plant and no special operating conditions of the plant were needed to design the controller. The training algorithm used is a combination of an adaptive backpropagation algorithm that tunes the connection weights with a genetic algorithm to modify the slopes of the activation function of each neuron. This combination turned out to be very robust against getting caught in local minima and it is very insensitive to the initial settings of the weights of the network. Experimental results show that the resulting control algorithm performs much better than the conventional PI-controller which was used for the generation of the training data set.{$<>$}},
  eventtitle = {{{IEEE Control Systems Magazine}}},
  keywords = {Backpropagation algorithms,Feedforward neural networks,Genetic algorithms,Inductors,Laboratories,Neural networks,Neurons,Predictive control,Predictive models,Training data},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/draegerModelPredictiveControl1995.pdf;/Users/akhilnagariya/Zotero/storage/69PN6NJI/466261.html}
}

@article{duTaskAgnosticDynamicsPriors2019,
  title = {Task-{{Agnostic Dynamics Priors}} for {{Deep Reinforcement Learning}}},
  author = {Du, Yilun and Narasimhan, Karthik},
  date = {2019-05-13},
  journaltitle = {ArXiv},
  url = {https://www.semanticscholar.org/paper/Task-Agnostic-Dynamics-Priors-for-Deep-Learning-Du-Narasimhan/c31f9302a48b8c40fe89650a154ad58e5103894b},
  urldate = {2023-08-07},
  abstract = {While model-based deep reinforcement learning (RL) holds great promise for sample efficiency and generalization, learning an accurate dynamics model is often challenging and requires substantial interaction with the environment. A wide variety of domains have dynamics that share common foundations like the laws of classical mechanics, which are rarely exploited by existing algorithms. In fact, humans continuously acquire and use such dynamics priors to easily adapt to operating in new environments. In this work, we propose an approach to learn task-agnostic dynamics priors from videos and incorporate them into an RL agent. Our method involves pre-training a frame predictor on task-agnostic physics videos to initialize dynamics models (and fine-tune them) for unseen target environments. Our frame prediction architecture, SpatialNet, is designed specifically to capture localized physical phenomena and interactions. Our approach allows for both faster policy learning and convergence to better policies, outperforming competitive approaches on several different environments. We also demonstrate that incorporating this prior allows for more effective transfer between environments.},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/duTaskAgnosticDynamicsPriors2019.pdf}
}

@online{ebertVisualForesightModelBased2018,
  title = {Visual {{Foresight}}: {{Model-Based Deep Reinforcement Learning}} for {{Vision-Based Robotic Control}}},
  shorttitle = {Visual {{Foresight}}},
  author = {Ebert, Frederik and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Lee, Alex and Levine, Sergey},
  date = {2018-12-03},
  eprint = {1812.00568},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1812.00568},
  urldate = {2023-05-06},
  abstract = {Deep reinforcement learning (RL) algorithms can learn complex robotic skills from raw sensory inputs, but have yet to achieve the kind of broad generalization and applicability demonstrated by deep learning methods in supervised domains. We present a deep RL method that is practical for real-world robotics tasks, such as robotic manipulation, and generalizes effectively to never-before-seen tasks and objects. In these settings, ground truth reward signals are typically unavailable, and we therefore propose a self-supervised model-based approach, where a predictive model learns to directly predict the future from raw sensory readings, such as camera images. At test time, we explore three distinct goal specification methods: designated pixels, where a user specifies desired object manipulation tasks by selecting particular pixels in an image and corresponding goal positions, goal images, where the desired goal state is specified with an image, and image classifiers, which define spaces of goal states. Our deep predictive models are trained using data collected autonomously and continuously by a robot interacting with hundreds of objects, without human supervision. We demonstrate that visual MPC can generalize to never-before-seen objects---both rigid and deformable---and solve a range of user-defined object manipulation tasks using the same model.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,NN based dynamic model},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/ebertVisualForesightModelBased2018.pdf;/Users/akhilnagariya/Zotero/storage/EXQEAGWX/1812.html}
}

@inproceedings{farahmandValueAwareLossFunction2017,
  title = {Value-{{Aware Loss Function}} for {{Model-based Reinforcement Learning}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Farahmand, Amir-Massoud and Barreto, Andre and Nikovski, Daniel},
  date = {2017-04-10},
  pages = {1486--1494},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v54/farahmand17a.html},
  urldate = {2023-08-09},
  abstract = {We consider the problem of estimating the transition probability kernel to be used by a model-based reinforcement learning (RL) algorithm. We argue that estimating a generative model that minimizes a probabilistic loss, such as the log-loss, is an overkill because it does not take into account the underlying structure of decision problem and the RL algorithm that intends to solve it. We introduce a loss function that takes the structure of the value function into account. We provide a finite-sample upper bound for the loss function showing the dependence of the error on model approximation error, number of samples, and the complexity of the model space. We also empirically compare the method with the maximum likelihood estimator on a simple problem.},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/farahmandValueAwareLossFunction2017.pdf;/Users/akhilnagariya/bibliography/bibtex-pdf/farahmandValueAwareLossFunction22.pdf}
}

@article{feinbergModelBasedValueEstimation2018,
  title = {Model-{{Based Value Estimation}} for {{Efficient Model-Free Reinforcement Learning}}},
  author = {Feinberg, Vladimir and Wan, Alvin and Stoica, I. and Jordan, Michael I. and Gonzalez, Joseph E. and Levine, S.},
  date = {2018-02-28},
  journaltitle = {ArXiv},
  url = {https://www.semanticscholar.org/paper/Model-Based-Value-Estimation-for-Efficient-Learning-Feinberg-Wan/2cdf553c4d3e68b9189217cc1f2d6b90e3f6eb7e},
  urldate = {2023-08-07},
  abstract = {Recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity. Such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks. Unfortunately, they rely on heuristics that limit usage of the dynamics model. We present model-based value expansion, which controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, we improve value estimation, which, in turn, reduces the sample complexity of learning.},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/feinbergModelBasedValueEstimation2018.pdf}
}

@article{ferragutiMyWelderCollaborativeSystem2023,
  title = {{{MyWelder}}: {{A}} Collaborative System for Intuitive Robot-Assisted Welding},
  shorttitle = {{{MyWelder}}},
  author = {Ferraguti, Federica and Villani, Valeria and Storchi, Chiara},
  date = {2023-02},
  journaltitle = {Mechatronics},
  shortjournal = {Mechatronics},
  volume = {89},
  pages = {102920},
  issn = {09574158},
  doi = {10.1016/j.mechatronics.2022.102920},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957415822001386},
  urldate = {2024-01-23},
  langid = {english}
}

@online{finnModelAgnosticMetaLearningFast2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  date = {2017-07-18},
  eprint = {1703.03400},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.03400},
  urldate = {2023-09-20},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/finnModelAgnosticMetaLearningFast2017.pdf;/Users/akhilnagariya/Zotero/storage/7BH56IBN/1703.html}
}

@online{fruitEfficientBiasSpanConstrainedExplorationExploitation2018,
  title = {Efficient {{Bias-Span-Constrained Exploration-Exploitation}} in {{Reinforcement Learning}}},
  author = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Ortner, Ronald},
  date = {2018-07-06},
  eprint = {1802.04020},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1802.04020},
  url = {http://arxiv.org/abs/1802.04020},
  urldate = {2023-08-06},
  abstract = {We introduce SCAL, an algorithm designed to perform efficient exploration-exploitation in any unknown weakly-communicating Markov decision process (MDP) for which an upper bound \$c\$ on the span of the optimal bias function is known. For an MDP with \$S\$ states, \$A\$ actions and \$\textbackslash Gamma \textbackslash leq S\$ possible next states, we prove a regret bound of \$\textbackslash widetilde\{O\}(c\textbackslash sqrt\{\textbackslash Gamma SAT\})\$, which significantly improves over existing algorithms (e.g., UCRL and PSRL), whose regret scales linearly with the MDP diameter \$D\$. In fact, the optimal bias span is finite and often much smaller than \$D\$ (e.g., \$D=\textbackslash infty\$ in non-communicating MDPs). A similar result was originally derived by Bartlett and Tewari (2009) for REGAL.C, for which no tractable algorithm is available. In this paper, we relax the optimization problem at the core of REGAL.C, we carefully analyze its properties, and we provide the first computationally efficient algorithm to solve it. Finally, we report numerical simulations supporting our theoretical findings and showing how SCAL significantly outperforms UCRL in MDPs with large diameter and small span.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Machine Learning,Finit State MDP,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/fruitEfficientBiasSpanConstrainedExplorationExploitation2018.pdf;/Users/akhilnagariya/Zotero/storage/BW8BA7TF/1802.html}
}

@inproceedings{galImprovingPILCOBayesian,
  title = {Improving {{PILCO}} with {{Bayesian Neural Network Dynamics Models}}},
  author = {Gal, Yarin and McAllister, Rowan Thomas and Rasmussen, Carl Edward},
  abstract = {Model-based reinforcement learning (RL) allows an agent to discover good policies with a small number of trials by generalising observed transitions. Data efficiency can be further improved with a probabilistic model of the agent’s ignorance about the world, allowing it to choose actions under uncertainty. Bayesian modelling offers tools for this task, with PILCO [1] being a prominent example, achieving state-of-theart data efficiency on low dimensional RL benchmarks. But PILCO relies on Gaussian processes (GPs), which prohibits its applicability to problems that require a larger number of trials to be solved. Further, PILCO does not consider temporal correlation in model uncertainty between successive state transitions, which results in PILCO underestimating state uncertainty at future time steps [2]. In this paper we extend PILCO’s framework to use Bayesian deep dynamics models with approximate variational inference, allowing PILCO to scale linearly with number of trials and observation space dimensionality. Using particle methods we sample dynamics function realisations, and obtain lower cumulative cost than PILCO. We give insights into the modelling assumptions made in PILCO, and show that moment matching is a crucial simplifying assumption made by the model. Our implementation can leverage GPU architectures, offering faster running time than PILCO, and will allow structured observation spaces to be modelled (images or higher dimensional inputs) in the future.},
  eventtitle = {Data-{{Efficient Machine Learning}} Workshop, {{International Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {NN based dynamic model},
  file = {/Users/akhilnagariya/Zotero/storage/WSX9I7JS/Gal et al. - Improving PILCO with Bayesian Neural Network Dynam.pdf}
}

@inproceedings{gaoObstacleAwareTopologicalPlanning2023,
  title = {Obstacle-{{Aware Topological Planning}} over {{Polyhedral Representation}} for {{Quadrotors}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Gao, Junjie and He, Fenghua and Zhang, Wei and Yao, Yu},
  date = {2023-05-29},
  pages = {10097--10103},
  publisher = {IEEE},
  location = {London, United Kingdom},
  doi = {10.1109/ICRA48891.2023.10161295},
  url = {https://ieeexplore.ieee.org/document/10161295/},
  urldate = {2023-08-17},
  eventtitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {9798350323658},
  file = {/Users/akhilnagariya/Zotero/storage/7ZRISJIK/gaoObstacleAwareTopologicalPlanning2023.pdf}
}

@article{garnett3DLaneNetEndtoEnd3D2019,
  title = {{{3D-LaneNet}}: {{End-to-End 3D Multiple Lane Detection}}},
  shorttitle = {{{3D-LaneNet}}},
  author = {Garnett, Noa and Cohen, Rafi and Pe'er, Tomer and Lahav, Roee and Levi, Dan},
  date = {2019-10},
  journaltitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages = {2921--2930},
  publisher = {IEEE},
  location = {Seoul, Korea (South)},
  doi = {10.1109/ICCV.2019.00301},
  url = {https://ieeexplore.ieee.org/document/9008811/},
  urldate = {2023-04-20},
  abstract = {We introduce a network that directly predicts the 3D layout of lanes in a road scene from a single image. This work marks a first attempt to address this task with on-board sensing without assuming a known constant lane width or relying on pre-mapped environments. Our network architecture, 3D-LaneNet, applies two new concepts: intra-network inverse-perspective mapping (IPM) and anchor-based lane representation. The intra-network IPM projection facilitates a dual-representation information flow in both regular image-view and top-view. An anchor-per-column output representation enables our end-to-end approach which replaces common heuristics such as clustering and outlier rejection, casting lane estimation as an object detection problem. In addition, our approach explicitly handles complex situations such as lane merges and splits. Results are shown on two new 3D lane datasets, a synthetic and a real one. For comparison with existing methods, we test our approach on the image-only tuSimple lane detection benchmark, achieving performance competitive with state-of-the-art.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {9781728148038},
  file = {/Users/akhilnagariya/Zotero/storage/YX2DQCCI/garnett3DLaneNetEndtoEnd3D2019a.pdf}
}

@inproceedings{garnett3DLaneNetEndtoEnd3D2019a,
  title = {{{3D-LaneNet}}: {{End-to-End 3D Multiple Lane Detection}}},
  shorttitle = {{{3D-LaneNet}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Garnett, Noa and Cohen, Rafi and Pe'er, Tomer and Lahav, Roee and Levi, Dan},
  date = {2019-10},
  pages = {2921--2930},
  publisher = {IEEE},
  location = {Seoul, Korea (South)},
  doi = {10.1109/ICCV.2019.00301},
  url = {https://ieeexplore.ieee.org/document/9008811/},
  urldate = {2023-04-20},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-72814-803-8},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/garnett3DLaneNetEndtoEnd3D2019.pdf}
}

@article{gasparinoWayFASTNavigationPredictive2022,
  title = {{{WayFAST}}: {{Navigation}} with {{Predictive Traversability}} in the {{Field}}},
  shorttitle = {{{WayFAST}}},
  author = {Gasparino, Mateus Valverde and Sivakumar, Arun Narenthiran and Liu, Yixiao and Velasquez, Andres Eduardo Baquero and Higuti, Vitor Akihiro Hisano and Rogers, John and Tran, Huy and Chowdhary, Girish},
  date = {2022-10},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {7},
  number = {4},
  eprint = {2203.12071},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  pages = {10651--10658},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2022.3193464},
  url = {http://arxiv.org/abs/2203.12071},
  urldate = {2024-02-27},
  abstract = {We present a self-supervised approach for learning to predict traversable paths for wheeled mobile robots that require good traction to navigate. Our algorithm, termed WayFAST (Waypoint Free Autonomous Systems for Traversability), uses RGB and depth data, along with navigation experience, to autonomously generate traversable paths in outdoor unstructured environments. Our key inspiration is that traction can be estimated for rolling robots using kinodynamic models. Using traction estimates provided by an online receding horizon estimator, we are able to train a traversability prediction neural network in a self-supervised manner, without requiring heuristics utilized by previous methods. We demonstrate the effectiveness of WayFAST through extensive field testing in varying environments, ranging from sandy dry beaches to forest canopies and snow covered grass fields. Our results clearly demonstrate that WayFAST can learn to avoid geometric obstacles as well as untraversable terrain, such as snow, which would be difficult to avoid with sensors that provide only geometric data, such as LiDAR. Furthermore, we show that our training pipeline based on online traction estimates is more data-efficient than other heuristic-based methods.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control,I.2.10,I.2.6,I.2.9},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/gasparinoWayFASTNavigationPredictive2022.pdf;/Users/akhilnagariya/Zotero/storage/CQTY88BK/2203.html}
}

@inproceedings{geigerAreWeReady2012,
  title = {Are We Ready for Autonomous Driving? {{The KITTI}} Vision Benchmark Suite},
  shorttitle = {Are We Ready for Autonomous Driving?},
  booktitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Geiger, A. and Lenz, P. and Urtasun, R.},
  date = {2012-06},
  pages = {3354--3361},
  publisher = {IEEE},
  location = {Providence, RI},
  doi = {10.1109/CVPR.2012.6248074},
  url = {http://ieeexplore.ieee.org/document/6248074/},
  urldate = {2023-04-13},
  eventtitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
  file = {/Users/akhilnagariya/Zotero/storage/EB5WZTJC/geigerAreWeReady2012.pdf}
}

@online{gibsonMultistepDynamicsModeling2023,
  title = {A {{Multi-step Dynamics Modeling Framework For Autonomous Driving In Multiple Environments}}},
  author = {Gibson, Jason and Vlahov, Bogdan and Fan, David and Spieler, Patrick and Pastor, Daniel and Agha-mohammadi, Ali-akbar and Theodorou, Evangelos A.},
  date = {2023-05-03},
  eprint = {2305.02241},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2305.02241},
  urldate = {2023-08-17},
  abstract = {Modeling dynamics is often the first step to making a vehicle autonomous. While on-road autonomous vehicles have been extensively studied, off-road vehicles pose many challenging modeling problems. An off-road vehicle encounters highly complex and difficult-to-model terrain/vehicle interactions, as well as having complex vehicle dynamics of its own. These complexities can create challenges for effective high-speed control and planning. In this paper, we introduce a framework for multistep dynamics prediction that explicitly handles the accumulation of modeling error and remains scalable for sampling-based controllers. Our method uses a specially-initialized Long Short-Term Memory (LSTM) over a limited time horizon as the learned component in a hybrid model to predict the dynamics of a 4-person seating all-terrain vehicle (Polaris S4 1000 RZR) in two distinct environments. By only having the LSTM predict over a fixed time horizon, we negate the need for long term stability that is often a challenge when training recurrent neural networks. Our framework is flexible as it only requires odometry information for labels. Through extensive experimentation, we show that our method is able to predict millions of possible trajectories in real-time, with a time horizon of five seconds in challenging off road driving scenarios.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/akhilnagariya/Zotero/storage/GJXUSSMZ/gibsonMultistepDynamicsModeling2023.pdf;/Users/akhilnagariya/Zotero/storage/KHAQLHQ5/2305.html}
}

@inproceedings{guContinuousDeepQLearning2016,
  title = {Continuous {{Deep Q-Learning}} with {{Model-based Acceleration}}},
  author = {Gu, S. and Lillicrap, T. and Sutskever, Ilya and Levine, S.},
  date = {2016-03-02},
  url = {https://www.semanticscholar.org/paper/Continuous-Deep-Q-Learning-with-Model-based-Gu-Lillicrap/d358d41c69450b171327ebd99462b6afef687269},
  urldate = {2023-08-07},
  abstract = {Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of modelfree algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized advantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/guContinuousDeepQLearning2016.pdf}
}

@article{guoLIGAStereoLearningLiDAR2021,
  title = {{{LIGA-Stereo}}: {{Learning LiDAR Geometry Aware Representations}} for {{Stereo-based 3D Detector}}},
  shorttitle = {{{LIGA-Stereo}}},
  author = {Guo, Xiaoyang and Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
  date = {2021-10},
  journaltitle = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages = {3133--3143},
  publisher = {IEEE},
  location = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.00314},
  url = {https://ieeexplore.ieee.org/document/9710742/},
  urldate = {2023-04-20},
  abstract = {Stereo-based 3D detection aims at detecting 3D objects from stereo images, which provides a low-cost solution for 3D perception. However, its performance is still inferior compared with LiDAR-based detection algorithms. To detect and localize accurate 3D bounding boxes, LiDAR-based detectors encode high-level representations from Li-DAR point clouds, such as accurate object boundaries and surface normal directions. In contrast, high-level features learned by stereo-based detectors are easily affected by the erroneous depth estimation due to the limitation of stereo matching. To solve the problem, we propose LIGA-Stereo (LiDAR Geometry Aware Stereo Detector) to learn stereo-based 3D detectors under the guidance of high-level geometry-aware representations of LiDAR-based detection models. In addition, we found existing voxel-based stereo detectors failed to learn semantic features effectively from indirect 3D supervisions. We attach an auxiliary 2D detection head to provide direct 2D semantic supervisions. Experiment results show that the above two strategies improved the geometric and semantic representation capabilities. Compared with the state-of-the-art stereo detector, our method has improved the 3D detection performance of cars, pedestrians, cyclists by 10.44\%, 5.69\%, 5.97\% mAP respectively on the official KITTI benchmark. The gap between stereo-based and LiDAR-based 3D detectors is further narrowed. The code is available at https://xy-guo.github.io/liga/.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {9781665428125},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/guoLIGAStereoLearningLiDAR2021.pdf}
}

@online{haarnojaSoftActorCriticOffPolicy2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  date = {2018-08-08},
  eprint = {1801.01290},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1801.01290},
  urldate = {2023-05-06},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/haarnojaSoftActorCriticOffPolicy2018.pdf;/Users/akhilnagariya/Zotero/storage/MBK4F9EN/1801.html}
}

@online{hasanbeigDeepSynthAutomataSynthesis2021,
  title = {{{DeepSynth}}: {{Automata Synthesis}} for {{Automatic Task Segmentation}} in {{Deep Reinforcement Learning}}},
  shorttitle = {{{DeepSynth}}},
  author = {Hasanbeig, Mohammadhosein and Jeppu, Natasha Yogananda and Abate, Alessandro and Melham, Tom and Kroening, Daniel},
  date = {2021-03-06},
  eprint = {1911.10244},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1911.10244},
  url = {http://arxiv.org/abs/1911.10244},
  urldate = {2023-10-05},
  abstract = {This paper proposes DeepSynth, a method for effective training of deep Reinforcement Learning (RL) agents when the reward is sparse and non-Markovian, but at the same time progress towards the reward requires achieving an unknown sequence of high-level objectives. Our method employs a novel algorithm for synthesis of compact automata to uncover this sequential structure automatically. We synthesise a human-interpretable automaton from trace data collected by exploring the environment. The state space of the environment is then enriched with the synthesised automaton so that the generation of a control policy by deep RL is guided by the discovered structure encoded in the automaton. The proposed approach is able to cope with both high-dimensional, low-level features and unknown sparse non-Markovian rewards. We have evaluated DeepSynth's performance in a set of experiments that includes the Atari game Montezuma's Revenge. Compared to existing approaches, we obtain a reduction of two orders of magnitude in the number of iterations required for policy synthesis, and also a significant improvement in scalability.},
  pubstate = {preprint},
  version = {5},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/hasanbeigDeepSynthAutomataSynthesis2021.pdf;/Users/akhilnagariya/Zotero/storage/H3WYWZ78/1911.html}
}

@online{hawkeUrbanDrivingConditional2019,
  title = {Urban {{Driving}} with {{Conditional Imitation Learning}}},
  author = {Hawke, Jeffrey and Shen, Richard and Gurau, Corina and Sharma, Siddharth and Reda, Daniele and Nikolov, Nikolay and Mazur, Przemyslaw and Micklethwaite, Sean and Griffiths, Nicolas and Shah, Amar and Kendall, Alex},
  date = {2019-12-05},
  eprint = {1912.00177},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1912.00177},
  urldate = {2024-03-01},
  abstract = {Hand-crafting generalised decision-making rules for real-world urban autonomous driving is hard. Alternatively, learning behaviour from easy-to-collect human driving demonstrations is appealing. Prior work has studied imitation learning (IL) for autonomous driving with a number of limitations. Examples include only performing lane-following rather than following a user-defined route, only using a single camera view or heavily cropped frames lacking state observability, only lateral (steering) control, but not longitudinal (speed) control and a lack of interaction with traffic. Importantly, the majority of such systems have been primarily evaluated in simulation - a simple domain, which lacks real-world complexities. Motivated by these challenges, we focus on learning representations of semantics, geometry and motion with computer vision for IL from human driving demonstrations. As our main contribution, we present an end-to-end conditional imitation learning approach, combining both lateral and longitudinal control on a real vehicle for following urban routes with simple traffic. We address inherent dataset bias by data balancing, training our final policy on approximately 30 hours of demonstrations gathered over six months. We evaluate our method on an autonomous vehicle by driving 35km of novel routes in European urban streets.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/hawkeUrbanDrivingConditional2019.pdf;/Users/akhilnagariya/Zotero/storage/GAZMJJLZ/1912.html}
}

@inproceedings{heessLearningContinuousControl2015,
  title = {Learning {{Continuous Control Policies}} by {{Stochastic Value Gradients}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
  date = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2015/hash/148510031349642de5ca0c544f31b2ef-Abstract.html},
  urldate = {2023-08-07},
  abstract = {We present a unified framework for learning continuous control policies usingbackpropagation. It supports stochastic control by treating stochasticity in theBellman equation as a deterministic function of exogenous noise. The productis a spectrum of general policy gradient algorithms that range from model-freemethods with value functions to model-based methods without value functions.We use learned models but only require observations from the environment insteadof observations from model-predicted trajectories, minimizing the impactof compounded model errors. We apply these algorithms first to a toy stochasticcontrol problem and then to several physics-based control problems in simulation.One of these variants, SVG(1), shows the effectiveness of learning models, valuefunctions, and policies simultaneously in continuous domains.},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/heessLearningContinuousControl2015.pdf}
}

@inproceedings{heRethinkingImageNetPreTraining2019,
  title = {Rethinking {{ImageNet Pre-Training}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {He, Kaiming and Girshick, Ross and Dollar, Piotr},
  date = {2019-10},
  pages = {4917--4926},
  publisher = {IEEE},
  location = {Seoul, Korea (South)},
  doi = {10.1109/ICCV.2019.00502},
  url = {https://ieeexplore.ieee.org/document/9010930/},
  urldate = {2023-04-20},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-72814-803-8},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/heRethinkingImageNetPreTraining2019.pdf}
}

@article{hindererLipschitzContinuityValue2005,
  title = {Lipschitz {{Continuity}} of {{Value Functions}} in {{Markovian Decision Processes}}},
  author = {Hinderer, K.},
  date = {2005-09-01},
  journaltitle = {Mathematical Methods of Operations Research},
  shortjournal = {Math Meth Oper Res},
  volume = {62},
  number = {1},
  pages = {3--22},
  issn = {1432-5217},
  doi = {10.1007/s00186-005-0438-1},
  url = {https://doi.org/10.1007/s00186-005-0438-1},
  urldate = {2023-08-06},
  abstract = {We present tools and guidelines for investigating Lipschitz continuity of the value functions in MDP’s, using the Hausdorff metric and the Kantorovich metric for measuring the influence of the constraint set and the transition law, respectively. The methods are explained by examples. Additional topics include an application to the the discretization algorithm of Bertsekas (1975).},
  langid = {english},
  keywords = {47J10,90C40,approximate solution of MDP’s by discretization,Finit State MDP,Lipschitz continuity,Markovian decision processes},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/hindererLipschitzContinuityValue2005.pdf}
}

@online{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  eprint = {2006.11239},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2023-07-16},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/hoDenoisingDiffusionProbabilistic2020.pdf;/Users/akhilnagariya/Zotero/storage/U5B2BBS2/2006.html}
}

@article{hoferSim2RealRoboticsAutomation2021,
  title = {{{Sim2Real}} in {{Robotics}} and {{Automation}}: {{Applications}} and {{Challenges}}},
  shorttitle = {{{Sim2Real}} in {{Robotics}} and {{Automation}}},
  author = {Hofer, Sebastian and Bekris, Kostas and Handa, Ankur and Gamboa, Juan Camilo and Mozifian, Melissa and Golemo, Florian and Atkeson, Chris and Fox, Dieter and Goldberg, Ken and Leonard, John and Karen Liu, C. and Peters, Jan and Song, Shuran and Welinder, Peter and White, Martha},
  date = {2021-04},
  journaltitle = {IEEE Transactions on Automation Science and Engineering},
  shortjournal = {IEEE Trans. Automat. Sci. Eng.},
  volume = {18},
  number = {2},
  pages = {398--400},
  issn = {1545-5955, 1558-3783},
  doi = {10.1109/TASE.2021.3064065},
  url = {https://ieeexplore.ieee.org/document/9398246/},
  urldate = {2023-10-05},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/hoferSim2RealRoboticsAutomation2021.pdf}
}

@online{hollandEffectPlanningShape2019,
  title = {The {{Effect}} of {{Planning Shape}} on {{Dyna-style Planning}} in {{High-dimensional State Spaces}}},
  author = {Holland, G. Zacharias and Talvitie, Erin J. and Bowling, Michael},
  date = {2019-03-28},
  eprint = {1806.01825},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1806.01825},
  urldate = {2023-08-08},
  abstract = {Dyna is a fundamental approach to model-based reinforcement learning (MBRL) that interleaves planning, acting, and learning in an online setting. In the most typical application of Dyna, the dynamics model is used to generate one-step transitions from selected start states from the agent's history, which are used to update the agent's value function or policy as if they were real experiences. In this work, one-step Dyna was applied to several games from the Arcade Learning Environment (ALE). We found that the model-based updates offered surprisingly little benefit over simply performing more updates with the agent's existing experience, even when using a perfect model. We hypothesize that to get the most from planning, the model must be used to generate unfamiliar experience. To test this, we experimented with the "shape" of planning in multiple different concrete instantiations of Dyna, performing fewer, longer rollouts, rather than many short rollouts. We found that planning shape has a profound impact on the efficacy of Dyna for both perfect and learned models. In addition to these findings regarding Dyna in general, our results represent, to our knowledge, the first time that a learned dynamics model has been successfully used for planning in the ALE, suggesting that Dyna may be a viable approach to MBRL in the ALE and other high-dimensional problems.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/hollandEffectPlanningShape2019.pdf;/Users/akhilnagariya/Zotero/storage/VE8BT6A5/1806.html}
}

@article{hongRadarSLAMRobustSimultaneous2022,
  title = {{{RadarSLAM}}: {{A}} Robust Simultaneous Localization and Mapping System for All Weather Conditions},
  shorttitle = {{{RadarSLAM}}},
  author = {Hong, Ziyang and Petillot, Yvan and Wallace, Andrew and Wang, Sen},
  date = {2022-04},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {41},
  number = {5},
  pages = {519--542},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/02783649221080483},
  url = {http://journals.sagepub.com/doi/10.1177/02783649221080483},
  urldate = {2023-04-13},
  abstract = {A Simultaneous Localization and Mapping (SLAM) system must be robust to support long-term mobile vehicle and robot applications. However, camera and LiDAR based SLAM systems can be fragile when facing challenging illumination or weather conditions which degrade the utility of imagery and point cloud data. Radar, whose operating electromagnetic spectrum is less affected by environmental changes, is promising although its distinct sensor model and noise characteristics bring open challenges when being exploited for SLAM. This paper studies the use of a Frequency Modulated Continuous Wave radar for SLAM in large-scale outdoor environments. We propose a full radar SLAM system, including a novel radar motion estimation algorithm that leverages radar geometry for reliable feature tracking. It also optimally compensates motion distortion and estimates pose by joint optimization. Its loop closure component is designed to be simple yet efficient for radar imagery by capturing and exploiting structural information of the surrounding environment. Extensive experiments on three public radar datasets, ranging from city streets and residential areas to countryside and highways, show competitive accuracy and reliability performance of the proposed radar SLAM system compared to the state-of-the-art LiDAR, vision and radar methods. The results show that our system is technically viable in achieving reliable SLAM in extreme weather conditions on the RADIATE Dataset, for example, heavy snow and dense fog, demonstrating the promising potential of using radar for all-weather localization and mapping.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/hongRadarSLAMRobustSimultaneous2022.pdf}
}

@online{huangBEVDet4DExploitTemporal2022,
  title = {{{BEVDet4D}}: {{Exploit Temporal Cues}} in {{Multi-camera 3D Object Detection}}},
  shorttitle = {{{BEVDet4D}}},
  author = {Huang, Junjie and Huang, Guan},
  date = {2022-06-16},
  eprint = {2203.17054},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.17054},
  url = {http://arxiv.org/abs/2203.17054},
  urldate = {2023-04-20},
  abstract = {Single frame data contains finite information which limits the performance of the existing vision-based multi-camera 3D object detection paradigms. For fundamentally pushing the performance boundary in this area, a novel paradigm dubbed BEVDet4D is proposed to lift the scalable BEVDet paradigm from the spatial-only 3D space to the spatial-temporal 4D space. We upgrade the naive BEVDet framework with a few modifications just for fusing the feature from the previous frame with the corresponding one in the current frame. In this way, with negligible additional computing budget, we enable BEVDet4D to access the temporal cues by querying and comparing the two candidate features. Beyond this, we simplify the task of velocity prediction by removing the factors of ego-motion and time in the learning target. As a result, BEVDet4D with robust generalization performance reduces the velocity error by up to -62.9\%. This makes the vision-based methods, for the first time, become comparable with those relied on LiDAR or radar in this aspect. On challenge benchmark nuScenes, we report a new record of 54.5\% NDS with the high-performance configuration dubbed BEVDet4D-Base, which surpasses the previous leading method BEVDet-Base by +7.3\% NDS. The source code is publicly available for further research at https://github.com/HuangJunJie2017/BEVDet .},
  pubstate = {preprint},
  version = {3},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/huangBEVDet4DExploitTemporal2022.pdf;/Users/akhilnagariya/Zotero/storage/YWH2KZ7B/2203.html}
}

@online{huangBEVDetHighperformanceMulticamera2022,
  title = {{{BEVDet}}: {{High-performance Multi-camera 3D Object Detection}} in {{Bird-Eye-View}}},
  shorttitle = {{{BEVDet}}},
  author = {Huang, Junjie and Huang, Guan and Zhu, Zheng and Ye, Yun and Du, Dalong},
  date = {2022-06-16},
  eprint = {2112.11790},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.11790},
  urldate = {2023-04-20},
  abstract = {Autonomous driving perceives its surroundings for decision making, which is one of the most complex scenarios in visual perception. The success of paradigm innovation in solving the 2D object detection task inspires us to seek an elegant, feasible, and scalable paradigm for fundamentally pushing the performance boundary in this area. To this end, we contribute the BEVDet paradigm in this paper. BEVDet performs 3D object detection in Bird-Eye-View (BEV), where most target values are defined and route planning can be handily performed. We merely reuse existing modules to build its framework but substantially develop its performance by constructing an exclusive data augmentation strategy and upgrading the Non-Maximum Suppression strategy. In the experiment, BEVDet offers an excellent trade-off between accuracy and time-efficiency. As a fast version, BEVDet-Tiny scores 31.2\% mAP and 39.2\% NDS on the nuScenes val set. It is comparable with FCOS3D, but requires just 11\% computational budget of 215.3 GFLOPs and runs 9.2 times faster at 15.6 FPS. Another high-precision version dubbed BEVDet-Base scores 39.3\% mAP and 47.2\% NDS, significantly exceeding all published results. With a comparable inference speed, it surpasses FCOS3D by a large margin of +9.8\% mAP and +10.0\% NDS. The source code is publicly available for further research at https://github.com/HuangJunJie2017/BEVDet .},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/huangBEVDetHighperformanceMulticamera2022.pdf;/Users/akhilnagariya/Zotero/storage/AJHQXH6H/2112.html}
}

@article{husseinImitationLearningSurvey2018,
  title = {Imitation {{Learning}}: {{A Survey}} of {{Learning Methods}}},
  shorttitle = {Imitation {{Learning}}},
  author = {Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
  date = {2018-03-31},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {50},
  number = {2},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3054912},
  url = {https://dl.acm.org/doi/10.1145/3054912},
  urldate = {2023-10-05},
  abstract = {Imitation learning techniques aim to mimic human behavior in a given task. An agent (a learning machine) is trained to perform a task from demonstrations by learning a mapping between observations and actions. The idea of teaching by imitation has been around for many years; however, the field is gaining attention recently due to advances in computing and sensing as well as rising demand for intelligent applications. The paradigm of learning by imitation is gaining popularity because it facilitates teaching complex tasks with minimal expert knowledge of the tasks. Generic imitation learning methods could potentially reduce the problem of teaching a task to that of providing demonstrations, without the need for explicit programming or designing reward functions specific to the task. Modern sensors are able to collect and transmit high volumes of data rapidly, and processors with high computational power allow fast processing that maps the sensory data to actions in a timely manner. This opens the door for many potential AI applications that require real-time perception and reaction such as humanoid robots, self-driving vehicles, human computer interaction, and computer games, to name a few. However, specialized algorithms are needed to effectively and robustly learn models as learning by imitation poses its own set of challenges. In this article, we survey imitation learning methods and present design options in different steps of the learning process. We introduce a background and motivation for the field as well as highlight challenges specific to the imitation problem. Methods for designing and evaluating imitation learning tasks are categorized and reviewed. Special attention is given to learning methods in robotics and games as these domains are the most popular in the literature and provide a wide array of problems and methodologies. We extensively discuss combining imitation learning approaches using different sources and methods, as well as incorporating other motion learning methods to enhance imitation. We also discuss the potential impact on industry, present major applications, and highlight current and future research directions.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/husseinImitationLearningSurvey2018.pdf}
}

@online{innesElaboratingLearnedDemonstrations2020,
  title = {Elaborating on {{Learned Demonstrations}} with {{Temporal Logic Specifications}}},
  author = {Innes, Craig and Ramamoorthy, Subramanian},
  date = {2020-05-22},
  eprint = {2002.00784},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2002.00784},
  url = {http://arxiv.org/abs/2002.00784},
  urldate = {2023-10-05},
  abstract = {Most current methods for learning from demonstrations assume that those demonstrations alone are sufficient to learn the underlying task. This is often untrue, especially if extra safety specifications exist which were not present in the original demonstrations. In this paper, we allow an expert to elaborate on their original demonstration with additional specification information using linear temporal logic (LTL). Our system converts LTL specifications into a differentiable loss. This loss is then used to learn a dynamic movement primitive that satisfies the underlying specification, while remaining close to the original demonstration. Further, by leveraging adversarial training, our system learns to robustly satisfy the given LTL specification on unseen inputs, not just those seen in training. We show that our method is expressive enough to work across a variety of common movement specification patterns such as obstacle avoidance, patrolling, keeping steady, and speed limitation. In addition, we show that our system can modify a base demonstration with complex specifications by incrementally composing multiple simpler specifications. We also implement our system on a PR-2 robot to show how a demonstrator can start with an initial (sub-optimal) demonstration, then interactively improve task success by including additional specifications enforced with our differentiable LTL loss.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/innesElaboratingLearnedDemonstrations2020.pdf;/Users/akhilnagariya/Zotero/storage/65C32QY5/2002.html}
}

@article{jakschNearoptimalRegretBounds2010,
  title = {Near-Optimal {{Regret Bounds}} for {{Reinforcement Learning}}},
  author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  date = {2010-08-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {11},
  pages = {1563--1600},
  issn = {1532-4435},
  abstract = {For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s,s' there is a policy which moves from s to s' in at most D steps (on average). We present a reinforcement learning algorithm with total regret Õ(DS√AT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of Ω(√DSAT) on the total regret of any learning algorithm is given as well. These results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T. Finally, we also consider a setting where the MDP is allowed to change a fixed number of l times. We present a modification of our algorithm that is able to deal with this setting and show a regret bound of Õ(l1/3T2/3DS√A).},
  keywords = {Finit State MDP},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/jakschNearoptimalRegretBounds2010.pdf}
}

@online{jangBCZZeroShotTask2022,
  title = {{{BC-Z}}: {{Zero-Shot Task Generalization}} with {{Robotic Imitation Learning}}},
  shorttitle = {{{BC-Z}}},
  author = {Jang, Eric and Irpan, Alex and Khansari, Mohi and Kappler, Daniel and Ebert, Frederik and Lynch, Corey and Levine, Sergey and Finn, Chelsea},
  date = {2022-02-04},
  eprint = {2202.02005},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.02005},
  urldate = {2023-10-28},
  abstract = {In this paper, we study the problem of enabling a vision-based robotic manipulation system to generalize to novel tasks, a long-standing challenge in robot learning. We approach the challenge from an imitation learning perspective, aiming to study how scaling and broadening the data collected can facilitate such generalization. To that end, we develop an interactive and flexible imitation learning system that can learn from both demonstrations and interventions and can be conditioned on different forms of information that convey the task, including pre-trained embeddings of natural language or videos of humans performing the task. When scaling data collection on a real robot to more than 100 distinct tasks, we find that this system can perform 24 unseen manipulation tasks with an average success rate of 44\%, without any robot demonstrations for those tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/jangBCZZeroShotTask2022.pdf;/Users/akhilnagariya/Zotero/storage/ITRCQANG/2202.html}
}

@online{jannerDeepGenerativeModels2023,
  title = {Deep {{Generative Models}} for {{Decision-Making}} and {{Control}}},
  author = {Janner, Michael},
  date = {2023-07-08},
  eprint = {2306.08810},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2306.08810},
  urldate = {2023-08-07},
  abstract = {Deep model-based reinforcement learning methods offer a conceptually simple approach to the decision-making and control problem: use learning for the purpose of estimating an approximate dynamics model, and offload the rest of the work to classical trajectory optimization. However, this combination has a number of empirical shortcomings, limiting the usefulness of model-based methods in practice. The dual purpose of this thesis is to study the reasons for these shortcomings and to propose solutions for the uncovered problems. Along the way, we highlight how inference techniques from the contemporary generative modeling toolbox, including beam search, classifier-guided sampling, and image inpainting, can be reinterpreted as viable planning strategies for reinforcement learning problems.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/jannerDeepGenerativeModels2023.pdf;/Users/akhilnagariya/Zotero/storage/QDWVP4YC/2306.html}
}

@inproceedings{jannerOfflineReinforcementLearning2021,
  title = {Offline {{Reinforcement Learning}} as {{One Big Sequence Modeling Problem}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
  date = {2021},
  volume = {34},
  pages = {1273--1286},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2021/hash/099fe6b0b444c23836c4a5d07346082b-Abstract.html},
  urldate = {2023-08-07},
  abstract = {Reinforcement learning (RL) is typically viewed as the problem of estimating single-step policies (for model-free RL) or single-step models (for model-based RL), leveraging the Markov property to factorize the problem in time. However, we can also view RL as a sequence modeling problem: predict a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether powerful, high-capacity sequence prediction models that work well in other supervised learning domains, such as natural-language processing, can also provide simple and effective solutions to the RL problem. To this end, we explore how RL can be reframed as "one big sequence modeling" problem, using state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. Addressing RL as a sequence modeling problem significantly simplifies a range of design decisions: we no longer require separate behavior policy constraints, as is common in prior work on offline model-free RL, and we no longer require ensembles or other epistemic uncertainty estimators, as is common in prior work on model-based RL. All of these roles are filled by the same Transformer sequence model. In our experiments, we demonstrate the flexibility of this approach across imitation learning, goal-conditioned RL, and offline RL.},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/jannerOfflineReinforcementLearning2021.pdf}
}

@online{jannerWhenTrustYour2021,
  title = {When to {{Trust Your Model}}: {{Model-Based Policy Optimization}}},
  shorttitle = {When to {{Trust Your Model}}},
  author = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  date = {2021-11-28},
  eprint = {1906.08253},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.08253},
  urldate = {2023-04-29},
  abstract = {Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Ensemble Probabilistic Networks for Dynamics,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/jannerWhenTrustYour2021.pdf;/Users/akhilnagariya/Zotero/storage/IRBJ76ZE/1906.html}
}

@article{jiangEfficientPlanningCompact2022,
  title = {Efficient {{Planning}} in a {{Compact Latent Action Space}}},
  author = {Jiang, Zhengyao and Zhang, Tianjun and Janner, Michael and Li, Yueying and Rocktäschel, Tim and Grefenstette, Edward and Tian, Yuandong},
  date = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2208.10291},
  url = {https://arxiv.org/abs/2208.10291},
  urldate = {2023-08-07},
  abstract = {Planning-based reinforcement learning has shown strong performance in tasks in discrete and low-dimensional continuous action spaces. However, planning usually brings significant computational overhead for decision-making, and scaling such methods to high-dimensional action spaces remains challenging. To advance efficient planning for high-dimensional continuous control, we propose Trajectory Autoencoding Planner (TAP), which learns low-dimensional latent action codes with a state-conditional VQ-VAE. The decoder of the VQ-VAE thus serves as a novel dynamics model that takes latent actions and current state as input and reconstructs long-horizon trajectories. During inference time, given a starting state, TAP searches over discrete latent actions to find trajectories that have both high probability under the training distribution and high predicted cumulative reward. Empirical evaluation in the offline RL setting demonstrates low decision latency which is indifferent to the growing raw action dimensionality. For Adroit robotic hand manipulation tasks with high-dimensional continuous action space, TAP surpasses existing model-based methods by a large margin and also beats strong model-free actor-critic baselines.},
  version = {3},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/jiangEfficientPlanningCompact2022.pdf}
}

@online{jiangPolarFormerMulticamera3D2023,
  title = {{{PolarFormer}}: {{Multi-camera 3D Object Detection}} with {{Polar Transformer}}},
  shorttitle = {{{PolarFormer}}},
  author = {Jiang, Yanqin and Zhang, Li and Miao, Zhenwei and Zhu, Xiatian and Gao, Jin and Hu, Weiming and Jiang, Yu-Gang},
  date = {2023-01-15},
  eprint = {2206.15398},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2206.15398},
  urldate = {2023-04-20},
  abstract = {3D object detection in autonomous driving aims to reason "what" and "where" the objects of interest present in a 3D world. Following the conventional wisdom of previous 2D object detection, existing methods often adopt the canonical Cartesian coordinate system with perpendicular axis. However, we conjugate that this does not fit the nature of the ego car's perspective, as each onboard camera perceives the world in shape of wedge intrinsic to the imaging geometry with radical (non-perpendicular) axis. Hence, in this paper we advocate the exploitation of the Polar coordinate system and propose a new Polar Transformer (PolarFormer) for more accurate 3D object detection in the bird's-eye-view (BEV) taking as input only multi-camera 2D images. Specifically, we design a cross attention based Polar detection head without restriction to the shape of input structure to deal with irregular Polar grids. For tackling the unconstrained object scale variations along Polar's distance dimension, we further introduce a multi-scalePolar representation learning strategy. As a result, our model can make best use of the Polar representation rasterized via attending to the corresponding image observation in a sequence-to-sequence fashion subject to the geometric constraints. Thorough experiments on the nuScenes dataset demonstrate that our PolarFormer outperforms significantly state-of-the-art 3D object detection alternatives.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/jiangPolarFormerMulticamera3D2023.pdf;/Users/akhilnagariya/Zotero/storage/9ZMPNC37/2206.html}
}

@online{kahnBADGRAutonomousSelfSupervised2020,
  title = {{{BADGR}}: {{An Autonomous Self-Supervised Learning-Based Navigation System}}},
  shorttitle = {{{BADGR}}},
  author = {Kahn, Gregory and Abbeel, Pieter and Levine, Sergey},
  date = {2020-04-15},
  eprint = {2002.05700},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2002.05700},
  urldate = {2023-10-30},
  abstract = {Mobile robot navigation is typically regarded as a geometric problem, in which the robot's objective is to perceive the geometry of the environment in order to plan collision-free paths towards a desired goal. However, a purely geometric view of the world can can be insufficient for many navigation problems. For example, a robot navigating based on geometry may avoid a field of tall grass because it believes it is untraversable, and will therefore fail to reach its desired goal. In this work, we investigate how to move beyond these purely geometric-based approaches using a method that learns about physical navigational affordances from experience. Our approach, which we call BADGR, is an end-to-end learning-based mobile robot navigation system that can be trained with self-supervised off-policy data gathered in real-world environments, without any simulation or human supervision. BADGR can navigate in real-world urban and off-road environments with geometrically distracting obstacles. It can also incorporate terrain preferences, generalize to novel environments, and continue to improve autonomously by gathering more data. Videos, code, and other supplemental material are available on our website https://sites.google.com/view/badgr},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/Zotero/storage/V8VTHIQL/kahnBADGRAutonomousSelfSupervised2020.pdf;/Users/akhilnagariya/Zotero/storage/2BVZFNTP/2002.html}
}

@article{kaiserModelBasedReinforcementLearning2019,
  title = {Model-{{Based Reinforcement Learning}} for {{Atari}}},
  author = {Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Mohiuddin, Afroz and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
  date = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1903.00374},
  url = {https://arxiv.org/abs/1903.00374},
  urldate = {2023-08-07},
  abstract = {Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.},
  version = {4},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML),NN based dynamic model},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/kaiserModelBasedReinforcementLearning2019a.pdf}
}

@article{kalashnikovMTOptContinuousMultiTask2021,
  title = {{{MT-Opt}}: {{Continuous Multi-Task Robotic Reinforcement Learning}} at {{Scale}}},
  shorttitle = {{{MT-Opt}}},
  author = {Kalashnikov, Dmitry and Varley, Jacob and Chebotar, Yevgen and Swanson, Benjamin and Jonschkowski, Rico and Finn, Chelsea and Levine, Sergey and Hausman, Karol},
  date = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2104.08212},
  url = {https://arxiv.org/abs/2104.08212},
  urldate = {2023-10-28},
  abstract = {General-purpose robotic systems must master a large repertoire of diverse skills to be useful in a range of daily tasks. While reinforcement learning provides a powerful framework for acquiring individual behaviors, the time needed to acquire each skill makes the prospect of a generalist robot trained with RL daunting. In this paper, we study how a large-scale collective robotic learning system can acquire a repertoire of behaviors simultaneously, sharing exploration, experience, and representations across tasks. In this framework new tasks can be continuously instantiated from previously learned tasks improving overall performance and capabilities of the system. To instantiate this system, we develop a scalable and intuitive framework for specifying new tasks through user-provided examples of desired outcomes, devise a multi-robot collective learning system for data collection that simultaneously collects experience for multiple tasks, and develop a scalable and generalizable multi-task deep reinforcement learning method, which we call MT-Opt. We demonstrate how MT-Opt can learn a wide range of skills, including semantic picking (i.e., picking an object from a particular category), placing into various fixtures (e.g., placing a food item onto a plate), covering, aligning, and rearranging. We train and evaluate our system on a set of 12 real-world tasks with data collected from 7 robots, and demonstrate the performance of our system both in terms of its ability to generalize to structurally similar new tasks, and acquire distinct new tasks more quickly by leveraging past experience. We recommend viewing the videos at https://karolhausman.github.io/mt-opt/},
  version = {2},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Robotics (cs.RO)},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/kalashnikovMTOptContinuousMultiTask2021.pdf}
}

@inproceedings{kalweitUncertaintydrivenImaginationContinuous2017,
  title = {Uncertainty-Driven {{Imagination}} for {{Continuous Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 1st {{Annual Conference}} on {{Robot Learning}}},
  author = {Kalweit, Gabriel and Boedecker, Joschka},
  date = {2017-10-18},
  pages = {195--206},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v78/kalweit17a.html},
  urldate = {2023-08-07},
  abstract = {Continuous control of high-dimensional systems can be achieved by current state-of-the-art reinforcement learning methods such as the Deep Deterministic Policy Gradient algorithm, but needs a significant amount of data samples. For real-world systems, this can be an obstacle since excessive data collection can be expensive, tedious or lead to physical damage. The main incentive of this work is to keep the advantages of model-free Q-learning while minimizing real-world interaction by the employment of a dynamics model learned in parallel. To counteract adverse effects of imaginary rollouts with an inaccurate model, a notion of uncertainty is introduced, to make use of artificial data only in cases of high uncertainty. We evaluate our approach on three simulated robot tasks and achieve faster learning by at least 40 per cent in comparison to vanilla DDPG with multiple updates.},
  eventtitle = {Conference on {{Robot Learning}}},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/kalweitUncertaintydrivenImaginationContinuous22.pdf}
}

@article{kellyReliableRoadAutonomous2006,
  title = {Toward {{Reliable Off Road Autonomous Vehicles Operating}} in {{Challenging Environments}}},
  author = {Kelly, Alonzo and Stentz, Anthony and Amidi, Omead and Bode, Mike and Bradley, David and Diaz-Calderon, Antonio and Happold, Mike and Herman, Herman and Mandelbaum, Robert and Pilarski, Tom and Rander, Pete and Thayer, Scott and Vallidis, Nick and Warner, Randy},
  date = {2006-05},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {25},
  number = {5-6},
  pages = {449--483},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364906065543},
  url = {http://journals.sagepub.com/doi/10.1177/0278364906065543},
  urldate = {2024-03-06},
  abstract = {The DARPA PerceptOR program has implemented a rigorous evaluative test program which fosters the development of field relevant outdoor mobile robots. Autonomous ground vehicles were deployed on diverse test courses throughout the USA and quantitatively evaluated on such factors as autonomy level, waypoint acquisition, failure rate, speed, and communications bandwidth. Our efforts over the three year program have produced new approaches in planning, perception, localization, and control which have been driven by the quest for reliable operation in challenging environments. This paper focuses on some of the most unique aspects of the systems developed by the CMU PerceptOR team, the lessons learned during the effort, and the most immediate challenges that remain to be addressed.},
  langid = {english},
  file = {/Users/akhilnagariya/Downloads/kelly-et-al-2006-toward-reliable-off-road-autonomous-vehicles-operating-in-challenging-environments.pdf}
}

@online{kendallLearningDriveDay2018,
  title = {Learning to {{Drive}} in a {{Day}}},
  author = {Kendall, Alex and Hawke, Jeffrey and Janz, David and Mazur, Przemyslaw and Reda, Daniele and Allen, John-Mark and Lam, Vinh-Dieu and Bewley, Alex and Shah, Amar},
  date = {2018-09-11},
  eprint = {1807.00412},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1807.00412},
  urldate = {2023-07-17},
  abstract = {We demonstrate the first application of deep reinforcement learning to autonomous driving. From randomly initialised parameters, our model is able to learn a policy for lane following in a handful of training episodes using a single monocular image as input. We provide a general and easy to obtain reward: the distance travelled by the vehicle without the safety driver taking control. We use a continuous, model-free deep reinforcement learning algorithm, with all exploration and optimisation performed on-vehicle. This demonstrates a new framework for autonomous driving which moves away from reliance on defined logical rules, mapping, and direct supervision. We discuss the challenges and opportunities to scale this approach to a broader range of autonomous driving tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/kendallLearningDriveDay2018.pdf;/Users/akhilnagariya/Zotero/storage/F8EDH5YY/1807.html}
}

@inproceedings{kenkHumanawareRobotNavigation2019,
  title = {Human-Aware {{Robot Navigation}} in {{Logistics Warehouses}}:},
  shorttitle = {Human-Aware {{Robot Navigation}} in {{Logistics Warehouses}}},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Informatics}} in {{Control}}, {{Automation}} and {{Robotics}}},
  author = {Kenk, Mourad and Hassaballah, M. and Brethé, Jean-François},
  date = {2019},
  pages = {371--378},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  location = {Prague, Czech Republic},
  doi = {10.5220/0007920903710378},
  url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0007920903710378},
  urldate = {2024-01-24},
  eventtitle = {16th {{International Conference}} on {{Informatics}} in {{Control}}, {{Automation}} and {{Robotics}}},
  isbn = {978-989-758-380-3},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/kenkHumanawareRobotNavigation2019.pdf}
}

@online{kimDynaConDynamicRobot2023,
  title = {{{DynaCon}}: {{Dynamic Robot Planner}} with {{Contextual Awareness}} via {{LLMs}}},
  shorttitle = {{{DynaCon}}},
  author = {Kim, Gyeongmin and Kim, Taehyeon and Kannan, Shyam Sundar and Venkatesh, Vishnunandan L. N. and Kim, Donghan and Min, Byung-Cheol},
  date = {2023-09-27},
  eprint = {2309.16031},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.16031},
  urldate = {2023-11-07},
  abstract = {Mobile robots often rely on pre-existing maps for effective path planning and navigation. However, when these maps are unavailable, particularly in unfamiliar environments, a different approach become essential. This paper introduces DynaCon, a novel system designed to provide mobile robots with contextual awareness and dynamic adaptability during navigation, eliminating the reliance of traditional maps. DynaCon integrates real-time feedback with an object server, prompt engineering, and navigation modules. By harnessing the capabilities of Large Language Models (LLMs), DynaCon not only understands patterns within given numeric series but also excels at categorizing objects into matched spaces. This facilitates dynamic path planner imbued with contextual awareness. We validated the effectiveness of DynaCon through an experiment where a robot successfully navigated to its goal using reasoning. Source code and experiment videos for this work can be found at: https://sites.google.com/view/dynacon.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/kimDynaConDynamicRobot2023.pdf;/Users/akhilnagariya/Zotero/storage/ATETLGY8/2309.html}
}

@inproceedings{kimTraversabilityClassificationUGV2007,
  title = {Traversability Classification for {{UGV}} Navigation: A Comparison of Patch and Superpixel Representations},
  shorttitle = {Traversability Classification for {{UGV}} Navigation},
  booktitle = {2007 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Kim, Dongshin and Oh, Sang Min and Rehg, James M.},
  date = {2007-10},
  pages = {3166--3173},
  issn = {2153-0866},
  doi = {10.1109/IROS.2007.4399610},
  url = {https://ieeexplore.ieee.org/document/4399610},
  urldate = {2024-03-06},
  abstract = {Robot navigation in complex outdoor terrain can benefit from accurate traversability classification. Appearancebased traversability estimation can provide a long-range sensing capability which complements the traditional use of stereo or LIDAR ranging. In the standard approach to traversability classification, each image frame is decomposed into patches or pixels for further analysis. However, classification at the pixel level is prone to noise and complicates the task of identifying homogeneous regions for navigation. Fixed-sized patches aggregate pixel information, resulting in better noise properties, but they can span multiple distinct image regions, which can degrade the classification performance and make thin obstacles difficult to detect. We address the use of superpixels as the visual primitives for traversability estimation. Superpixels are obtained from an over-segmentation of the image and they aggregate visually homogeneous pixels while respecting natural terrain boundaries. We show that superpixels are superior to patches in classification accuracy and result in more effective navigation in complex terrain environments. Our experimental results include a study of the effect of patch and superpixel size on classification accuracy. We demonstrate that superpixels can be computed on-line on a real robot at a sufficient frame rate to support long-range sensing and planning.},
  eventtitle = {2007 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  keywords = {Aggregates,Degradation,Image analysis,Intelligent robots,Layout,Navigation,Pixel,Robot sensing systems,Shape,Stereo vision},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/kimTraversabilityClassificationUGV2007.pdf}
}

@online{kiranDeepReinforcementLearning2021,
  title = {Deep {{Reinforcement Learning}} for {{Autonomous Driving}}: {{A Survey}}},
  shorttitle = {Deep {{Reinforcement Learning}} for {{Autonomous Driving}}},
  author = {Kiran, B. Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Sallab, Ahmad A. Al and Yogamani, Senthil and Pérez, Patrick},
  date = {2021-01-23},
  eprint = {2002.00444},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2002.00444},
  url = {http://arxiv.org/abs/2002.00444},
  urldate = {2023-10-05},
  abstract = {With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/kiranDeepReinforcementLearning2021.pdf;/Users/akhilnagariya/Zotero/storage/UIRPIITY/2002.html}
}

@online{kontogianniInteractiveObjectSegmentation2023,
  title = {Interactive {{Object Segmentation}} in {{3D Point Clouds}}},
  author = {Kontogianni, Theodora and Celikkan, Ekin and Tang, Siyu and Schindler, Konrad},
  date = {2023-01-23},
  eprint = {2204.07183},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.07183},
  urldate = {2023-08-17},
  abstract = {We propose an interactive approach for 3D instance segmentation, where users can iteratively collaborate with a deep learning model to segment objects in a 3D point cloud directly. Current methods for 3D instance segmentation are generally trained in a fully-supervised fashion, which requires large amounts of costly training labels, and does not generalize well to classes unseen during training. Few works have attempted to obtain 3D segmentation masks using human interactions. Existing methods rely on user feedback in the 2D image domain. As a consequence, users are required to constantly switch between 2D images and 3D representations, and custom architectures are employed to combine multiple input modalities. Therefore, integration with existing standard 3D models is not straightforward. The core idea of this work is to enable users to interact directly with 3D point clouds by clicking on desired 3D objects of interest\textasciitilde (or their background) to interactively segment the scene in an open-world setting. Specifically, our method does not require training data from any target domain, and can adapt to new environments where no appropriate training sets are available. Our system continuously adjusts the object segmentation based on the user feedback and achieves accurate dense 3D segmentation masks with minimal human effort (few clicks per object). Besides its potential for efficient labeling of large-scale and varied 3D datasets, our approach, where the user directly interacts with the 3D environment, enables new applications in AR/VR and human-robot interaction.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/kontogianniInteractiveObjectSegmentation2023.pdf;/Users/akhilnagariya/Zotero/storage/6P4CXAXV/2204.html}
}

@online{krantzNavigatingObjectsSpecified2023,
  title = {Navigating to {{Objects Specified}} by {{Images}}},
  author = {Krantz, Jacob and Gervet, Theophile and Yadav, Karmesh and Wang, Austin and Paxton, Chris and Mottaghi, Roozbeh and Batra, Dhruv and Malik, Jitendra and Lee, Stefan and Chaplot, Devendra Singh},
  date = {2023-04-03},
  eprint = {2304.01192},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2304.01192},
  urldate = {2023-11-07},
  abstract = {Images are a convenient way to specify which particular object instance an embodied agent should navigate to. Solving this task requires semantic visual reasoning and exploration of unknown environments. We present a system that can perform this task in both simulation and the real world. Our modular method solves sub-tasks of exploration, goal instance re-identification, goal localization, and local navigation. We re-identify the goal instance in egocentric vision using feature-matching and localize the goal instance by projecting matched features to a map. Each sub-task is solved using off-the-shelf components requiring zero fine-tuning. On the HM3D InstanceImageNav benchmark, this system outperforms a baseline end-to-end RL policy 7x and a state-of-the-art ImageNav model 2.3x (56\% vs 25\% success). We deploy this system to a mobile robot platform and demonstrate effective real-world performance, achieving an 88\% success rate across a home and an office environment.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/krantzNavigatingObjectsSpecified2023.pdf;/Users/akhilnagariya/Zotero/storage/XVAYGV6Q/2304.html}
}

@online{krantzSim2SimTransferVisionandLanguage2022,
  title = {Sim-2-{{Sim Transfer}} for {{Vision-and-Language Navigation}} in {{Continuous Environments}}},
  author = {Krantz, Jacob and Lee, Stefan},
  date = {2022-04-24},
  eprint = {2204.09667},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.09667},
  url = {http://arxiv.org/abs/2204.09667},
  urldate = {2023-10-05},
  abstract = {Recent work in Vision-and-Language Navigation (VLN) has presented two environmental paradigms with differing realism -- the standard VLN setting built on topological environments where navigation is abstracted away, and the VLN-CE setting where agents must navigate continuous 3D environments using low-level actions. Despite sharing the high-level task and even the underlying instruction-path data, performance on VLN-CE lags behind VLN significantly. In this work, we explore this gap by transferring an agent from the abstract environment of VLN to the continuous environment of VLN-CE. We find that this sim-2-sim transfer is highly effective, improving over the prior state of the art in VLN-CE by +12\% success rate. While this demonstrates the potential for this direction, the transfer does not fully retain the original performance of the agent in the abstract setting. We present a sequence of experiments to identify what differences result in performance degradation, providing clear directions for further improvement.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/krantzSim2SimTransferVisionandLanguage2022.pdf;/Users/akhilnagariya/Zotero/storage/F26DL3K9/2204.html}
}

@inproceedings{kumarOptimalControlLearned2016,
  title = {Optimal Control with Learned Local Models: {{Application}} to Dexterous Manipulation},
  shorttitle = {Optimal Control with Learned Local Models},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Kumar, Vikash and Todorov, Emanuel and Levine, Sergey},
  date = {2016-05},
  pages = {378--383},
  publisher = {IEEE},
  location = {Stockholm, Sweden},
  doi = {10.1109/ICRA.2016.7487156},
  url = {http://ieeexplore.ieee.org/document/7487156/},
  urldate = {2023-05-06},
  eventtitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-4673-8026-3},
  keywords = {Time Varying Linear Dynamics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/kumarOptimalControlLearned2016.pdf}
}

@online{kurutachModelEnsembleTrustRegionPolicy2018,
  title = {Model-{{Ensemble Trust-Region Policy Optimization}}},
  author = {Kurutach, Thanard and Clavera, Ignasi and Duan, Yan and Tamar, Aviv and Abbeel, Pieter},
  date = {2018-10-05},
  eprint = {1802.10592},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.10592},
  urldate = {2023-08-06},
  abstract = {Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity, which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/kurutachModelEnsembleTrustRegionPolicy2018.pdf;/Users/akhilnagariya/Zotero/storage/MIQMJHJZ/1802.html}
}

@inproceedings{lakshmananImprovedRegretBounds2015,
  title = {Improved {{Regret Bounds}} for {{Undiscounted Continuous Reinforcement Learning}}},
  author = {Lakshmanan, K. and Ortner, R. and Ryabko, D.},
  date = {2015-07-06},
  url = {https://www.semanticscholar.org/paper/Improved-Regret-Bounds-for-Undiscounted-Continuous-Lakshmanan-Ortner/d3aaa9dd1c1a3b07c613a69466c86e6443babac4},
  urldate = {2023-08-06},
  abstract = {We consider the problem of undiscounted reinforcement learning in continuous state space. Regret bounds in this setting usually hold under various assumptions on the structure of the reward and transition function. Under the assumption that the rewards and transition probabilities are Lipschitz, for 1-dimensional state space a regret bound of O(T3/4 after any T steps has been given by Ortner and Ryabko (2012). Here we improve upon this result by using non-parametric kernel density estimation for estimating the transition probability distributions, and obtain regret bounds that depend on the smoothness of the transition probability distributions. In particular, under the assumption that the transition probability functions are smoothly differentiable, the regret bound is shown to be O(T2/3 asymptotically for reinforcement learning in 1-dimensional state space. Finally, we also derive improved regret bounds for higher dimensional state space.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  keywords = {Finit State MDP},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/lakshmananImprovedRegretBounds2015.pdf}
}

@online{lanCriticBenchEvaluatingLarge2024,
  title = {{{CriticBench}}: {{Evaluating Large Language Models}} as {{Critic}}},
  shorttitle = {{{CriticBench}}},
  author = {Lan, Tian and Zhang, Wenwei and Xu, Chen and Huang, Heyan and Lin, Dahua and Chen, Kai and Mao, Xian-ling},
  date = {2024-02-21},
  eprint = {2402.13764},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2402.13764},
  urldate = {2024-02-22},
  abstract = {Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \textbackslash shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \textbackslash shortname\textasciitilde encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \textbackslash shortname\textasciitilde will be publicly released at \textbackslash url\{https://github.com/gmftbyGMFTBY/CriticBench\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/lanCriticBenchEvaluatingLarge2024.pdf;/Users/akhilnagariya/Zotero/storage/DDTMG48C/2402.html}
}

@inproceedings{langPointPillarsFastEncoders2019,
  title = {{{PointPillars}}: {{Fast Encoders}} for {{Object Detection From Point Clouds}}},
  shorttitle = {{{PointPillars}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar},
  date = {2019-06},
  pages = {12689--12697},
  publisher = {IEEE},
  location = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.01298},
  url = {https://ieeexplore.ieee.org/document/8954311/},
  urldate = {2023-04-20},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/langPointPillarsFastEncoders2019.pdf}
}

@article{leeUserPreferenceOptimization2023,
  title = {User Preference Optimization for Control of Ankle Exoskeletons Using Sample Efficient Active Learning},
  author = {Lee, Ung Hee and Shetty, Varun S. and Franks, Patrick W. and Tan, Jie and Evangelopoulos, Georgios and Ha, Sehoon and Rouse, Elliott J.},
  date = {2023-10-25},
  journaltitle = {Science Robotics},
  shortjournal = {Sci. Robot.},
  volume = {8},
  number = {83},
  pages = {eadg3705},
  issn = {2470-9476},
  doi = {10.1126/scirobotics.adg3705},
  url = {https://www.science.org/doi/10.1126/scirobotics.adg3705},
  urldate = {2023-10-20},
  abstract = {One challenge to achieving widespread success of augmentative exoskeletons is accurately adjusting the controller to provide cooperative assistance with their wearer. Often, the controller parameters are “tuned” to optimize a physiological or biomechanical objective. However, these approaches are resource intensive, while typically only enabling optimization of a single objective. In reality, the exoskeleton user experience is likely derived from many factors, including comfort, fatigue, and stability, among others. This work introduces an approach to conveniently tune the four parameters of an exoskeleton controller to maximize user preference. Our overarching strategy is to leverage the wearer to internally balance the experiential factors of wearing the system. We used an evolutionary algorithm to recommend potential parameters, which were ranked by a neural network that was pretrained with previously collected user preference data. The controller parameters that had the highest preference ranking were provided to the exoskeleton, and the wearer responded with real-time feedback as a forced-choice comparison. Our approach was able to converge on controller parameters preferred by the wearer with an accuracy of 88\% on average when compared with randomly generated parameters. User-preferred settings stabilized in 43 ± 7 queries. This work demonstrates that user preference can be leveraged to tune a partial-assist ankle exoskeleton in real time using a simple, intuitive interface, highlighting the potential for translating lower-limb wearable technologies into our daily lives.           ,              The proposed algorithm reliably identifies multiple users’ preferred settings of an exoskeleton controller in real time.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/leeUserPreferenceOptimization2023.pdf}
}

@online{levineEndtoEndTrainingDeep2016,
  title = {End-to-{{End Training}} of {{Deep Visuomotor Policies}}},
  author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  date = {2016-04-18},
  eprint = {1504.00702},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1504.00702},
  urldate = {2023-08-06},
  abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/levineEndtoEndTrainingDeep2016.pdf;/Users/akhilnagariya/Zotero/storage/X5Z2MZE7/1504.html}
}

@inproceedings{levineGuidedPolicySearch2013,
  title = {Guided {{Policy Search}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Levine, Sergey and Koltun, Vladlen},
  date = {2013-05-26},
  pages = {1--9},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v28/levine13.html},
  urldate = {2023-08-07},
  abstract = {Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {Time Varying Linear Dynamics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/levineGuidedPolicySearch2013.pdf}
}

@article{levineLearningRoboticNavigation2023,
  title = {Learning {{Robotic Navigation}} from {{Experience}}: {{Principles}}, {{Methods}}, and {{Recent Results}}},
  shorttitle = {Learning {{Robotic Navigation}} from {{Experience}}},
  author = {Levine, Sergey and Shah, Dhruv},
  date = {2023-01-30},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Phil. Trans. R. Soc. B},
  volume = {378},
  number = {1869},
  eprint = {2212.06759},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {20210447},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2021.0447},
  url = {http://arxiv.org/abs/2212.06759},
  urldate = {2023-11-07},
  abstract = {Navigation is one of the most heavily studied problems in robotics, and is conventionally approached as a geometric mapping and planning problem. However, real-world navigation presents a complex set of physical challenges that defies simple geometric abstractions. Machine learning offers a promising way to go beyond geometry and conventional planning, allowing for navigational systems that make decisions based on actual prior experience. Such systems can reason about traversability in ways that go beyond geometry, accounting for the physical outcomes of their actions and exploiting patterns in real-world environments. They can also improve as more data is collected, potentially providing a powerful network effect. In this article, we present a general toolkit for experiential learning of robotic navigation skills that unifies several recent approaches, describe the underlying design principles, summarize experimental results from several of our recent papers, and discuss open problems and directions for future work.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/levineLearningRoboticNavigation2023.pdf;/Users/akhilnagariya/Zotero/storage/CQXA3R9Y/2212.html}
}

@online{liangBEVFusionSimpleRobust2022,
  title = {{{BEVFusion}}: {{A Simple}} and {{Robust LiDAR-Camera Fusion Framework}}},
  shorttitle = {{{BEVFusion}}},
  author = {Liang, Tingting and Xie, Hongwei and Yu, Kaicheng and Xia, Zhongyu and Lin, Zhiwei and Wang, Yongtao and Tang, Tao and Wang, Bing and Tang, Zhi},
  date = {2022-11-11},
  eprint = {2205.13790},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.13790},
  url = {http://arxiv.org/abs/2205.13790},
  urldate = {2023-04-20},
  abstract = {Fusing the camera and LiDAR information has become a de-facto standard for 3D object detection tasks. Current methods rely on point clouds from the LiDAR sensor as queries to leverage the feature from the image space. However, people discovered that this underlying assumption makes the current fusion framework infeasible to produce any prediction when there is a LiDAR malfunction, regardless of minor or major. This fundamentally limits the deployment capability to realistic autonomous driving scenarios. In contrast, we propose a surprisingly simple yet novel fusion framework, dubbed BEVFusion, whose camera stream does not depend on the input of LiDAR data, thus addressing the downside of previous methods. We empirically show that our framework surpasses the state-of-the-art methods under the normal training settings. Under the robustness training settings that simulate various LiDAR malfunctions, our framework significantly surpasses the state-of-the-art methods by 15.7\% to 28.9\% mAP. To the best of our knowledge, we are the first to handle realistic LiDAR malfunction and can be deployed to realistic scenarios without any post-processing procedure. The code is available at https://github.com/ADLab-AutoDrive/BEVFusion.},
  pubstate = {preprint},
  version = {3},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/liangBEVFusionSimpleRobust2022.pdf;/Users/akhilnagariya/Zotero/storage/Z5XVVMR7/2205.html}
}

@online{liangCodePoliciesLanguage2023,
  title = {Code as {{Policies}}: {{Language Model Programs}} for {{Embodied Control}}},
  shorttitle = {Code as {{Policies}}},
  author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  date = {2023-05-24},
  eprint = {2209.07753},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2209.07753},
  urldate = {2023-08-17},
  abstract = {Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/liangCodePoliciesLanguage2023.pdf;/Users/akhilnagariya/Zotero/storage/I8XAGV3S/2209.html}
}

@online{liBEVDepthAcquisitionReliable2022,
  title = {{{BEVDepth}}: {{Acquisition}} of {{Reliable Depth}} for {{Multi-view 3D Object Detection}}},
  shorttitle = {{{BEVDepth}}},
  author = {Li, Yinhao and Ge, Zheng and Yu, Guanyi and Yang, Jinrong and Wang, Zengran and Shi, Yukang and Sun, Jianjian and Li, Zeming},
  date = {2022-11-30},
  eprint = {2206.10092},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2206.10092},
  urldate = {2023-04-20},
  abstract = {In this research, we propose a new 3D object detector with a trustworthy depth estimation, dubbed BEVDepth, for camera-based Bird's-Eye-View (BEV) 3D object detection. Our work is based on a key observation -- depth estimation in recent approaches is surprisingly inadequate given the fact that depth is essential to camera 3D detection. Our BEVDepth resolves this by leveraging explicit depth supervision. A camera-awareness depth estimation module is also introduced to facilitate the depth predicting capability. Besides, we design a novel Depth Refinement Module to counter the side effects carried by imprecise feature unprojection. Aided by customized Efficient Voxel Pooling and multi-frame mechanism, BEVDepth achieves the new state-of-the-art 60.9\% NDS on the challenging nuScenes test set while maintaining high efficiency. For the first time, the NDS score of a camera model reaches 60\%.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/liBEVDepthAcquisitionReliable2022.pdf;/Users/akhilnagariya/Zotero/storage/L8D4TPLH/2206.html}
}

@article{liBEVFormerLearningBird2022,
  title = {{{BEVFormer}}: {{Learning Bird}}'s-{{Eye-View Representation}} from {{Multi-Camera Images}} via {{Spatiotemporal Transformers}}},
  shorttitle = {{{BEVFormer}}},
  author = {Li, Zhiqi and Wang, Wenhai and Li, Hongyang and Xie, Enze and Sima, Chonghao and Lu, Tong and Yu, Qiao and Dai, Jifeng},
  date = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2203.17270},
  url = {https://arxiv.org/abs/2203.17270},
  urldate = {2023-04-20},
  abstract = {3D visual perception tasks, including 3D detection and map segmentation based on multi-camera images, are essential for autonomous driving systems. In this work, we present a new framework termed BEVFormer, which learns unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, we design spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. For temporal information, we propose temporal self-attention to recurrently fuse the history BEV information. Our approach achieves the new state-of-the-art 56.9\textbackslash\% in terms of NDS metric on the nuScenes \textbackslash texttt\{test\} set, which is 9.0 points higher than previous best arts and on par with the performance of LiDAR-based baselines. We further show that BEVFormer remarkably improves the accuracy of velocity estimation and recall of objects under low visibility conditions. The code is available at \textbackslash url\{https://github.com/zhiqi-li/BEVFormer\}.},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/liBEVFormerLearningBird2022.pdf}
}

@article{liDelvingDevilsBird2022,
  title = {Delving into the {{Devils}} of {{Bird}}'s-Eye-View {{Perception}}: {{A Review}}, {{Evaluation}} and {{Recipe}}},
  shorttitle = {Delving into the {{Devils}} of {{Bird}}'s-Eye-View {{Perception}}},
  author = {Li, Hongyang and Sima, Chonghao and Dai, Jifeng and Wang, Wenhai and Lu, Lewei and Wang, Huijie and Xie, Enze and Li, Zhiqi and Deng, Hanming and Tian, Hao and Zhu, Xizhou and Chen, Li and Gao, Yulu and Geng, Xiangwei and Zeng, Jia and Li, Yang and Yang, Jiazhi and Jia, Xiaosong and Yu, Bohan and Qiao, Yu and Lin, Dahua and Liu, Si and Yan, Junchi and Shi, Jianping and Luo, Ping},
  date = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2209.05324},
  url = {https://arxiv.org/abs/2209.05324},
  urldate = {2023-04-18},
  abstract = {Learning powerful representations in bird's-eye-view (BEV) for perception tasks is trending and drawing extensive attention both from industry and academia. Conventional approaches for most autonomous driving algorithms perform detection, segmentation, tracking, etc., in a front or perspective view. As sensor configurations get more complex, integrating multi-source information from different sensors and representing features in a unified view come of vital importance. BEV perception inherits several advantages, as representing surrounding scenes in BEV is intuitive and fusion-friendly; and representing objects in BEV is most desirable for subsequent modules as in planning and/or control. The core problems for BEV perception lie in (a) how to reconstruct the lost 3D information via view transformation from perspective view to BEV; (b) how to acquire ground truth annotations in BEV grid; (c) how to formulate the pipeline to incorporate features from different sources and views; and (d) how to adapt and generalize algorithms as sensor configurations vary across different scenarios. In this survey, we review the most recent work on BEV perception and provide an in-depth analysis of different solutions. Moreover, several systematic designs of BEV approach from the industry are depicted as well. Furthermore, we introduce a full suite of practical guidebook to improve the performance of BEV perception tasks, including camera, LiDAR and fusion inputs. At last, we point out the future research directions in this area. We hope this report would shed some light on the community and encourage more research effort on BEV perception. We keep an active repository to collect the most recent work and provide a toolbox for bag of tricks at https://github.com/OpenPerceptionX/BEVPerception-Survey-Recipe.},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Robotics (cs.RO)},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/liDelvingDevilsBird2022.pdf}
}

@inproceedings{liDNDETRAccelerateDETR2022,
  title = {{{DN-DETR}}: {{Accelerate DETR Training}} by {{Introducing Query DeNoising}}},
  shorttitle = {{{DN-DETR}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Feng and Zhang, Hao and Liu, Shilong and Guo, Jian and Ni, Lionel M. and Zhang, Lei},
  date = {2022-06},
  pages = {13609--13617},
  publisher = {IEEE},
  location = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.01325},
  url = {https://ieeexplore.ieee.org/document/9879250/},
  urldate = {2023-04-20},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/liDNDETRAccelerateDETR2022.pdf}
}

@online{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  date = {2019-07-05},
  eprint = {1509.02971},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1509.02971},
  urldate = {2023-05-06},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/lillicrapContinuousControlDeep2019.pdf;/Users/akhilnagariya/Zotero/storage/9VQLZVF4/1509.html}
}

@inproceedings{linFeaturePyramidNetworks2017,
  title = {Feature {{Pyramid Networks}} for {{Object Detection}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  date = {2017-07},
  pages = {936--944},
  publisher = {IEEE},
  location = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.106},
  url = {http://ieeexplore.ieee.org/document/8099589/},
  urldate = {2023-04-20},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/linFeaturePyramidNetworks2017.pdf}
}

@article{liuOKRobotWhatReally2024,
  title = {{{OK-Robot}}: {{What Really Matters}} in {{Integrating Open-Knowledge Models}} for {{Robotics}}},
  shorttitle = {{{OK-Robot}}},
  author = {Liu, Peiqi and Orru, Yaswanth and Paxton, Chris and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
  date = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2401.12202},
  url = {https://arxiv.org/abs/2401.12202},
  urldate = {2024-02-02},
  abstract = {Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5\% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered environments, OK-Robot's performance increases to 82\%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules. Videos of our experiments are available on our website: https://ok-robot.github.io},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Robotics (cs.RO)},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/liuOKRobotWhatReally2024.pdf}
}

@inproceedings{liuSMOKESingleStageMonocular2020,
  title = {{{SMOKE}}: {{Single-Stage Monocular 3D Object Detection}} via {{Keypoint Estimation}}},
  shorttitle = {{{SMOKE}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Liu, Zechen and Wu, Zizhang and Toth, Roland},
  date = {2020-06},
  pages = {4289--4298},
  publisher = {IEEE},
  location = {Seattle, WA, USA},
  doi = {10.1109/CVPRW50498.2020.00506},
  url = {https://ieeexplore.ieee.org/document/9150775/},
  urldate = {2023-04-20},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  isbn = {978-1-72819-360-1},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/liuSMOKESingleStageMonocular2020.pdf}
}

@incollection{loubackdasilvalubancoSurveyRadarOdometry2022,
  title = {Survey on {{Radar Odometry}}},
  booktitle = {Computer {{Aided Systems Theory}} – {{EUROCAST}} 2022},
  author = {Louback da Silva Lubanco, Daniel and Schlechter, Thomas and Pichler-Scheder, Markus and Kastl, Christian},
  editor = {Moreno-Díaz, Roberto and Pichler, Franz and Quesada-Arencibia, Alexis},
  date = {2022},
  volume = {13789},
  pages = {619--625},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-25312-6_73},
  url = {https://link.springer.com/10.1007/978-3-031-25312-6_73},
  urldate = {2023-04-13},
  isbn = {978-3-031-25311-9 978-3-031-25312-6},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/loubackdasilvalubancoSurveyRadarOdometry2022a.pdf}
}

@incollection{loubackdasilvalubancoSurveyRadarOdometry2022a,
  title = {Survey on {{Radar Odometry}}},
  booktitle = {Computer {{Aided Systems Theory}} – {{EUROCAST}} 2022},
  author = {Louback da Silva Lubanco, Daniel and Schlechter, Thomas and Pichler-Scheder, Markus and Kastl, Christian},
  editor = {Moreno-Díaz, Roberto and Pichler, Franz and Quesada-Arencibia, Alexis},
  date = {2022},
  volume = {13789},
  pages = {619--625},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-25312-6_73},
  url = {https://link.springer.com/10.1007/978-3-031-25312-6_73},
  urldate = {2023-04-13},
  isbn = {978-3-031-25311-9 978-3-031-25312-6},
  langid = {english}
}

@online{luoAlgorithmicFrameworkModelbased2019,
  title = {Algorithmic {{Framework}} for {{Model-based Deep Reinforcement Learning}} with {{Theoretical Guarantees}}},
  author = {Luo, Yuping and Xu, Huazhe and Li, Yuanzhi and Tian, Yuandong and Darrell, Trevor and Ma, Tengyu},
  date = {2019-02-15},
  eprint = {1807.03858},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1807.03858},
  urldate = {2023-05-06},
  abstract = {Model-based reinforcement learning (RL) is considered to be a promising approach to reduce the sample complexity that hinders model-free RL. However, the theoretical understanding of such methods has been rather limited. This paper introduces a novel algorithmic framework for designing and analyzing model-based RL algorithms with theoretical guarantees. We design a meta-algorithm with a theoretical guarantee of monotone improvement to a local maximum of the expected reward. The meta-algorithm iteratively builds a lower bound of the expected reward based on the estimated dynamical model and sample trajectories, and then maximizes the lower bound jointly over the policy and the model. The framework extends the optimism-in-face-of-uncertainty principle to non-linear dynamical models in a way that requires \textbackslash textit\{no explicit\} uncertainty quantification. Instantiating our framework with simplification gives a variant of model-based RL algorithms Stochastic Lower Bounds Optimization (SLBO). Experiments demonstrate that SLBO achieves state-of-the-art performance when only one million or fewer samples are permitted on a range of continuous control benchmark tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {most recent 2021},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/luoAlgorithmicFrameworkModelbased2021.pdf;/Users/akhilnagariya/Zotero/storage/DUJH87PT/1807.html}
}

@online{luoUnderstandingDiffusionModels2022,
  title = {Understanding {{Diffusion Models}}: {{A Unified Perspective}}},
  shorttitle = {Understanding {{Diffusion Models}}},
  author = {Luo, Calvin},
  date = {2022-08-25},
  eprint = {2208.11970},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2208.11970},
  urldate = {2023-07-16},
  abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/luoUnderstandingDiffusionModels2022.pdf;/Users/akhilnagariya/Zotero/storage/FB7KRS9D/2208.html}
}

@article{manduchiObstacleDetectionTerrain2005,
  title = {Obstacle {{Detection}} and {{Terrain Classification}} for {{Autonomous Off-Road Navigation}}},
  author = {Manduchi, R. and Castano, A. and Talukder, A. and Matthies, L.},
  date = {2005-01},
  journaltitle = {Autonomous Robots},
  shortjournal = {Autonomous Robots},
  volume = {18},
  number = {1},
  pages = {81--102},
  issn = {0929-5593},
  doi = {10.1023/B:AURO.0000047286.62481.1d},
  url = {http://link.springer.com/10.1023/B:AURO.0000047286.62481.1d},
  urldate = {2024-03-06},
  abstract = {Autonomous navigation in cross-country environments presents many new challenges with respect to more traditional, urban environments. The lack of highly structured components in the scene complicates the design of even basic functionalities such as obstacle detection. In addition to the geometric description of the scene, terrain typing is also an important component of the perceptual system. Recognizing the different classes of terrain and obstacles enables the path planner to choose the most efficient route toward the desired goal.This paper presents new sensor processing algorithms that are suitable for cross-country autonomous navigation. We consider two sensor systems that complement each other in an ideal sensor suite: a color stereo camera, and a single axis ladar. We propose an obstacle detection technique, based on stereo range measurements, that does not rely on typical structural assumption on the scene (such as the presence of a visible ground plane); a color-based classification system to label the detected obstacles according to a set of terrain classes; and an algorithm for the analysis of ladar data that allows one to discriminate between grass and obstacles (such as tree trunks or rocks), even when such obstacles are partially hidden in the grass. These algorithms have been developed and implemented by the Jet Propulsion Laboratory (JPL) as part of its involvement in a number of projects sponsored by the US Department of Defense, and have enabled safe autonomous navigation in high-vegetated, off-road terrain.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/manduchiObstacleDetectionTerrain2005.pdf}
}

@online{margolisLearningSeePhysical2023,
  title = {Learning to {{See Physical Properties}} with {{Active Sensing Motor Policies}}},
  author = {Margolis, Gabriel B. and Fu, Xiang and Ji, Yandong and Agrawal, Pulkit},
  date = {2023-11-02},
  eprint = {2311.01405},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.01405},
  urldate = {2023-11-07},
  abstract = {Knowledge of terrain's physical properties inferred from color images can aid in making efficient robotic locomotion plans. However, unlike image classification, it is unintuitive for humans to label image patches with physical properties. Without labeled data, building a vision system that takes as input the observed terrain and predicts physical properties remains challenging. We present a method that overcomes this challenge by self-supervised labeling of images captured by robots during real-world traversal with physical property estimators trained in simulation. To ensure accurate labeling, we introduce Active Sensing Motor Policies (ASMP), which are trained to explore locomotion behaviors that increase the accuracy of estimating physical parameters. For instance, the quadruped robot learns to swipe its foot against the ground to estimate the friction coefficient accurately. We show that the visual system trained with a small amount of real-world traversal data accurately predicts physical parameters. The trained system is robust and works even with overhead images captured by a drone despite being trained on data collected by cameras attached to a quadruped robot walking on the ground.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/margolisLearningSeePhysical2023.pdf;/Users/akhilnagariya/Zotero/storage/XQWMQIKE/2311.html}
}

@inproceedings{matsukiIMODERealTimeIncremental2023,
  title = {{{iMODE}}:{{Real-Time Incremental Monocular Dense Mapping Using Neural Field}}},
  shorttitle = {{{iMODE}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Matsuki, Hidenobu and Sucar, Edgar and Laidow, Tristan and Wada, Kentaro and Scona, Raluca and Davison, Andrew J.},
  date = {2023-05-29},
  pages = {4171--4177},
  publisher = {IEEE},
  location = {London, United Kingdom},
  doi = {10.1109/ICRA48891.2023.10161538},
  url = {https://ieeexplore.ieee.org/document/10161538/},
  urldate = {2023-08-17},
  eventtitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {9798350323658},
  file = {/Users/akhilnagariya/Zotero/storage/94FCK3CP/matsukiIMODERealTimeIncremental2023.pdf}
}

@incollection{maturanaRealTimeSemanticMapping2018,
  title = {Real-{{Time Semantic Mapping}} for {{Autonomous Off-Road Navigation}}},
  booktitle = {Field and {{Service Robotics}}},
  author = {Maturana, Daniel and Chou, Po-Wei and Uenoyama, Masashi and Scherer, Sebastian},
  editor = {Hutter, Marco and Siegwart, Roland},
  date = {2018},
  volume = {5},
  pages = {335--350},
  publisher = {Springer International Publishing},
  location = {Cham},
  urldate = {2024-03-06},
  isbn = {978-3-319-67360-8 978-3-319-67361-5},
  file = {/Users/akhilnagariya/Downloads/978-3-319-67361-5.pdf}
}

@online{meesGroundingLanguageVisual2023,
  title = {Grounding {{Language}} with {{Visual Affordances}} over {{Unstructured Data}}},
  author = {Mees, Oier and Borja-Diaz, Jessica and Burgard, Wolfram},
  date = {2023-03-08},
  eprint = {2210.01911},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.01911},
  urldate = {2023-08-17},
  abstract = {Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task, language-conditioned robotic skills typically requires large-scale data collection and frequent human intervention to reset the environment or help correcting the current policies. In this work, we propose a novel approach to efficiently learn general-purpose language-conditioned robot skills from unstructured, offline and reset-free data in the real world by exploiting a self-supervised visuo-lingual affordance model, which requires annotating as little as 1\% of the total data with language. We evaluate our method in extensive experiments both in simulated and real-world robotic tasks, achieving state-of-the-art performance on the challenging CALVIN benchmark and learning over 25 distinct visuomotor manipulation tasks with a single policy in the real world. We find that when paired with LLMs to break down abstract natural language instructions into subgoals via few-shot prompting, our method is capable of completing long-horizon, multi-tier tasks in the real world, while requiring an order of magnitude less data than previous approaches. Code and videos are available at http://hulc2.cs.uni-freiburg.de},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/meesGroundingLanguageVisual2023.pdf;/Users/akhilnagariya/Zotero/storage/ZFLER246/2210.html}
}

@inproceedings{mengConditionalDETRFast2021,
  title = {Conditional {{DETR}} for {{Fast Training Convergence}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Meng, Depu and Chen, Xiaokang and Fan, Zejia and Zeng, Gang and Li, Houqiang and Yuan, Yuhui and Sun, Lei and Wang, Jingdong},
  date = {2021-10},
  pages = {3631--3640},
  publisher = {IEEE},
  location = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.00363},
  url = {https://ieeexplore.ieee.org/document/9710523/},
  urldate = {2023-04-20},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/mengConditionalDETRFast2021.pdf}
}

@online{mirjaliliFMLocUsingFoundation2023,
  title = {{{FM-Loc}}: {{Using Foundation Models}} for {{Improved Vision-based Localization}}},
  shorttitle = {{{FM-Loc}}},
  author = {Mirjalili, Reihaneh and Krawez, Michael and Burgard, Wolfram},
  date = {2023-04-14},
  eprint = {2304.07058},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2304.07058},
  urldate = {2023-11-07},
  abstract = {Visual place recognition is essential for vision-based robot localization and SLAM. Despite the tremendous progress made in recent years, place recognition in changing environments remains challenging. A promising approach to cope with appearance variations is to leverage high-level semantic features like objects or place categories. In this paper, we propose FM-Loc which is a novel image-based localization approach based on Foundation Models that uses the Large Language Model GPT-3 in combination with the Visual-Language Model CLIP to construct a semantic image descriptor that is robust to severe changes in scene geometry and camera viewpoint. We deploy CLIP to detect objects in an image, GPT-3 to suggest potential room labels based on the detected objects, and CLIP again to propose the most likely location label. The object labels and the scene label constitute an image descriptor that we use to calculate a similarity score between the query and database images. We validate our approach on real-world data that exhibit significant changes in camera viewpoints and object placement between the database and query trajectories. The experimental results demonstrate that our method is applicable to a wide range of indoor scenarios without the need for training or fine-tuning.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/mirjaliliFMLocUsingFoundation2023.pdf;/Users/akhilnagariya/Zotero/storage/GRXBK3HM/2304.html}
}

@inproceedings{moldovanOptimismdrivenExplorationNonlinear2015,
  title = {Optimism-Driven Exploration for Nonlinear Systems},
  booktitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Moldovan, Teodor Mihai and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
  date = {2015-05},
  pages = {3239--3246},
  publisher = {IEEE},
  location = {Seattle, WA, USA},
  doi = {10.1109/ICRA.2015.7139645},
  url = {http://ieeexplore.ieee.org/document/7139645/},
  urldate = {2023-08-06},
  eventtitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-4799-6923-4},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/moldovanOptimismdrivenExplorationNonlinear2015.pdf}
}

@online{nagabandiLearningAdaptDynamic2019,
  title = {Learning to {{Adapt}} in {{Dynamic}}, {{Real-World Environments Through Meta-Reinforcement Learning}}},
  author = {Nagabandi, Anusha and Clavera, Ignasi and Liu, Simin and Fearing, Ronald S. and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
  date = {2019-02-27},
  eprint = {1803.11347},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.11347},
  urldate = {2023-07-18},
  abstract = {Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/nagabandiLearningAdaptDynamic2019.pdf;/Users/akhilnagariya/Zotero/storage/7PZ2JHGA/1803.html}
}

@online{nagabandiNeuralNetworkDynamics2017,
  title = {Neural {{Network Dynamics}} for {{Model-Based Deep Reinforcement Learning}} with {{Model-Free Fine-Tuning}}},
  author = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S. and Levine, Sergey},
  date = {2017-12-01},
  eprint = {1708.02596},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1708.02596},
  urldate = {2023-05-06},
  abstract = {Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,NN based dynamic model},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/nagabandiNeuralNetworkDynamics2017.pdf;/Users/akhilnagariya/Zotero/storage/RR4QBSB9/1708.html}
}

@online{nageshraoAutonomousHighwayDriving2019,
  title = {Autonomous {{Highway Driving}} Using {{Deep Reinforcement Learning}}},
  author = {Nageshrao, Subramanya and Tseng, Eric and Filev, Dimitar},
  date = {2019-03-29},
  eprint = {1904.00035},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1904.00035},
  urldate = {2024-03-01},
  abstract = {The operational space of an autonomous vehicle (AV) can be diverse and vary significantly. This may lead to a scenario that was not postulated in the design phase. Due to this, formulating a rule based decision maker for selecting maneuvers may not be ideal. Similarly, it may not be effective to design an a-priori cost function and then solve the optimal control problem in real-time. In order to address these issues and to avoid peculiar behaviors when encountering unforeseen scenario, we propose a reinforcement learning (RL) based method, where the ego car, i.e., an autonomous vehicle, learns to make decisions by directly interacting with simulated traffic. The decision maker for AV is implemented as a deep neural network providing an action choice for a given system state. In a critical application such as driving, an RL agent without explicit notion of safety may not converge or it may need extremely large number of samples before finding a reliable policy. To best address the issue, this paper incorporates reinforcement learning with an additional short horizon safety check (SC). In a critical scenario, the safety check will also provide an alternate safe action to the agent provided if it exists. This leads to two novel contributions. First, it generalizes the states that could lead to undesirable "near-misses" or "collisions ". Second, inclusion of safety check can provide a safe and stable training environment. This significantly enhances learning efficiency without inhibiting meaningful exploration to ensure safe and optimal learned behavior. We demonstrate the performance of the developed algorithm in highway driving scenario where the trained AV encounters varying traffic density in a highway setting.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/nageshraoAutonomousHighwayDriving2019.pdf;/Users/akhilnagariya/Zotero/storage/K33HNT8F/1904.html}
}

@article{ohActionConditionalVideoPrediction2015,
  title = {Action-{{Conditional Video Prediction}} Using {{Deep Networks}} in {{Atari Games}}},
  author = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard and Singh, Satinder},
  date = {2015},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1507.08750},
  url = {https://arxiv.org/abs/1507.08750},
  urldate = {2023-08-07},
  abstract = {Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.},
  version = {2},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),NN based dynamic model},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/ohActionConditionalVideoPrediction2015c.pdf}
}

@online{ohValuePredictionNetwork2017,
  title = {Value {{Prediction Network}}},
  author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak},
  date = {2017-11-06},
  eprint = {1707.03497},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1707.03497},
  url = {http://arxiv.org/abs/1707.03497},
  urldate = {2023-08-08},
  abstract = {This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/ohValuePredictionNetwork2017.pdf;/Users/akhilnagariya/Zotero/storage/GNIVBF2G/1707.html}
}

@online{overbyeGVOMGPUAccelerated2021,
  title = {G-{{VOM}}: {{A GPU Accelerated Voxel Off-Road Mapping System}}},
  shorttitle = {G-{{VOM}}},
  author = {Overbye, Timothy and Saripalli, Srikanth},
  date = {2021-09-27},
  eprint = {2109.13176},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2109.13176},
  urldate = {2024-02-22},
  abstract = {We present a local 3D voxel mapping framework for off-road path planning and navigation. Our method provides both hard and soft positive obstacle detection, negative obstacle detection, slope estimation, and roughness estimation. By using a 3D array lookup table data structure and by leveraging the GPU it can provide online performance. We then demonstrate the system working on three vehicles, a Clearpath Robotics Warthog, Moose, and a Polaris Ranger, and compare against a set of pre-recorded waypoints. This was done at 4.5 m/s in autonomous operation and 12 m/s in manual operation with a map update rate of 10 Hz. Finally, an open-source ROS implementation is provided. https://github.com/unmannedlab/G-VOM},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/overbyeGVOMGPUAccelerated2021.pdf;/Users/akhilnagariya/Zotero/storage/X478HAKY/2109.html}
}

@online{panAgileAutonomousDriving2019,
  title = {Agile {{Autonomous Driving}} Using {{End-to-End Deep Imitation Learning}}},
  author = {Pan, Yunpeng and Cheng, Ching-An and Saigol, Kamil and Lee, Keuntaek and Yan, Xinyan and Theodorou, Evangelos and Boots, Byron},
  date = {2019-08-09},
  eprint = {1709.07174},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1709.07174},
  urldate = {2023-07-18},
  abstract = {We present an end-to-end imitation learning system for agile, off-road autonomous driving using only low-cost sensors. By imitating a model predictive controller equipped with advanced sensors, we train a deep neural network control policy to map raw, high-dimensional observations to continuous steering and throttle commands. Compared with recent approaches to similar tasks, our method requires neither state estimation nor on-the-fly planning to navigate the vehicle. Our approach relies on, and experimentally validates, recent imitation learning theory. Empirically, we show that policies trained with online imitation learning overcome well-known challenges related to covariate shift and generalize better than policies trained with batch imitation learning. Built on these insights, our autonomous driving system demonstrates successful high-speed off-road driving, matching the state-of-the-art performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/panAgileAutonomousDriving2019.pdf;/Users/akhilnagariya/Zotero/storage/3FBE3YEH/1709.html}
}

@article{panImitationLearningAgile2020,
  title = {Imitation Learning for Agile Autonomous Driving},
  author = {Pan, Yunpeng and Cheng, Ching-An and Saigol, Kamil and Lee, Keuntaek and Yan, Xinyan and Theodorou, Evangelos A and Boots, Byron},
  date = {2020-03},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {39},
  number = {2-3},
  pages = {286--302},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364919880273},
  url = {http://journals.sagepub.com/doi/10.1177/0278364919880273},
  urldate = {2023-07-18},
  abstract = {We present an end-to-end imitation learning system for agile, off-road autonomous driving using only low-cost on-board sensors. By imitating a model predictive controller equipped with advanced sensors, we train a deep neural network control policy to map raw, high-dimensional observations to continuous steering and throttle commands. Compared with recent approaches to similar tasks, our method requires neither state estimation nor on-the-fly planning to navigate the vehicle. Our approach relies on, and experimentally validates, recent imitation learning theory. Empirically, we show that policies trained with online imitation learning overcome well-known challenges related to covariate shift and generalize better than policies trained with batch imitation learning. Built on these insights, our autonomous driving system demonstrates successful high-speed off-road driving, matching the state-of-the-art performance.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/panImitationLearningAgile2020.pdf}
}

@inproceedings{parkPhaRaODirectRadar2020,
  title = {{{PhaRaO}}: {{Direct Radar Odometry}} Using {{Phase Correlation}}},
  shorttitle = {{{PhaRaO}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Park, Yeong Sang and Shin, Young-Sik and Kim, Ayoung},
  date = {2020-05},
  pages = {2617--2623},
  publisher = {IEEE},
  location = {Paris, France},
  doi = {10.1109/ICRA40945.2020.9197231},
  url = {https://ieeexplore.ieee.org/document/9197231/},
  urldate = {2023-04-13},
  eventtitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-72817-395-5},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/parkPhaRaODirectRadar2020.pdf}
}

@inproceedings{parkPseudoLidarNeededMonocular2021,
  title = {Is {{Pseudo-Lidar}} Needed for {{Monocular 3D Object}} Detection?},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Park, Dennis and Ambrus, Rares and Guizilini, Vitor and Li, Jie and Gaidon, Adrien},
  date = {2021-10},
  pages = {3122--3132},
  publisher = {IEEE},
  location = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.00313},
  url = {https://ieeexplore.ieee.org/document/9710908/},
  urldate = {2023-04-20},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/parkPseudoLidarNeededMonocular2021.pdf}
}

@article{pedrocchiSafeHumanRobotCooperation2013,
  title = {Safe {{Human-Robot Cooperation}} in an {{Industrial Environment}}},
  author = {Pedrocchi, Nicola and Vicentini, Federico and Matteo, Malosio and Tosatti, Lorenzo Molinari},
  date = {2013-01-01},
  journaltitle = {International Journal of Advanced Robotic Systems},
  shortjournal = {International Journal of Advanced Robotic Systems},
  volume = {10},
  number = {1},
  pages = {27},
  issn = {1729-8814, 1729-8814},
  doi = {10.5772/53939},
  url = {http://journals.sagepub.com/doi/10.5772/53939},
  urldate = {2024-01-23},
  abstract = {The standard EN ISO10218 is fostering the implementation of hybrid production systems, i.e., production systems characterized by a close relationship among human operators and robots in cooperative tasks. Human-robot hybrid systems could have a big economic benefit in small and medium sized production, even if this new paradigm introduces mandatory, challenging safety aspects. Among various requirements for collaborative workspaces, safety-assurance involves two different application layers; the algorithms enabling safe space-sharing between humans and robots and the enabling technologies allowing acquisition data from sensor fusion and environmental data analysing. This paper addresses both the problems: a collision avoidance strategy allowing on-line re-planning of robot motion and a safe network of unsafe devices as a suggested infrastructure for functional safety achievement.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/pedrocchiSafeHumanRobotCooperation2013.pdf}
}

@article{philionLiftSplatShoot2020,
  title = {Lift, {{Splat}}, {{Shoot}}: {{Encoding Images}} from {{Arbitrary Camera Rigs}} by {{Implicitly Unprojecting}} to {{3D}}},
  shorttitle = {Lift, {{Splat}}, {{Shoot}}},
  author = {Philion, Jonah and Fidler, Sanja},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  journaltitle = {Computer Vision – ECCV 2020},
  volume = {12359},
  pages = {194--210},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-58568-6_12},
  url = {https://link.springer.com/10.1007/978-3-030-58568-6_12},
  urldate = {2023-04-20},
  abstract = {The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single "bird's-eye-view" coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird's-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to "lift" each image individually into a frustum of features for each camera, then "splat" all frustums into a rasterized bird's-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird's-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by "shooting" template trajectories into a bird's-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: this https URL .},
  isbn = {9783030585679 9783030585686},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/philionLiftSplatShoot2020.pdf}
}

@inproceedings{picheProbabilisticPlanningSequential2018,
  title = {Probabilistic {{Planning}} with {{Sequential Monte Carlo}} Methods},
  author = {Piche, Alexandre and Thomas, Valentin and Ibrahim, Cyril and Bengio, Yoshua and Pal, Chris},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=ByetGn0cYX},
  urldate = {2023-08-07},
  abstract = {In this work, we propose a novel formulation of planning which views it as a probabilistic inference problem over future optimal trajectories. This enables us to use sampling methods, and thus, tackle planning in continuous domains using a fixed computational budget. We design a new algorithm, Sequential Monte Carlo Planning, by leveraging classical methods in Sequential Monte Carlo and Bayesian smoothing in the context of control as inference. Furthermore, we show that Sequential Monte Carlo Planning can capture multimodal policies and can quickly learn continuous control tasks.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/picheProbabilisticPlanningSequential2018.pdf}
}

@inproceedings{pirottaAdaptiveStepSizePolicy2013,
  title = {Adaptive {{Step-Size}} for {{Policy Gradient Methods}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pirotta, Matteo and Restelli, Marcello and Bascetta, Luca},
  date = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2013/hash/f64eac11f2cd8f0efa196f8ad173178e-Abstract.html},
  urldate = {2023-08-06},
  abstract = {In the last decade, policy gradient methods have significantly grown in popularity in the reinforcement--learning field. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identification of effective gradient directions and the proposal of efficient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly influenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step--size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second--order polynomial of the step size, and we show how a simplified version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear--quadratic regulator problem.},
  keywords = {Finit State MDP},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/pirottaAdaptiveStepSizePolicy2013.pdf}
}

@article{pirottaPolicyGradientLipschitz2015,
  title = {Policy Gradient in {{Lipschitz Markov Decision Processes}}},
  author = {Pirotta, Matteo and Restelli, Marcello and Bascetta, Luca},
  date = {2015-09},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {100},
  number = {2-3},
  pages = {255--283},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-015-5484-1},
  url = {http://link.springer.com/10.1007/s10994-015-5484-1},
  urldate = {2023-08-06},
  langid = {english},
  keywords = {Finit State MDP},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/pirottaPolicyGradientLipschitz2015.pdf}
}

@inproceedings{pomerleauALVINNAutonomousLand1988,
  title = {{{ALVINN}}: {{An Autonomous Land Vehicle}} in a {{Neural Network}}},
  shorttitle = {{{ALVINN}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pomerleau, Dean A.},
  date = {1988},
  volume = {1},
  publisher = {Morgan-Kaufmann},
  url = {https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html},
  urldate = {2024-03-01},
  abstract = {ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer  back-propagation network designed for the task of road following. Cur(cid:173) rently ALVINN takes images from a camera and a laser range finder as input  and produces as output the direction the vehicle should travel in order to  follow the road. Training has been conducted using simulated road images.  Successful tests on the Carnegie Mellon autonomous navigation test vehicle  indicate that the network can effectively follow real roads under certain field  conditions. The representation developed to perfOIm the task differs dra(cid:173) matically when the networlc is trained under various conditions, suggesting  the possibility of a novel adaptive autonomous navigation system capable of  tailoring its processing to the conditions at hand.},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/pomerleauALVINNAutonomousLand1988.pdf}
}

@online{pongTemporalDifferenceModels2020,
  title = {Temporal {{Difference Models}}: {{Model-Free Deep RL}} for {{Model-Based Control}}},
  shorttitle = {Temporal {{Difference Models}}},
  author = {Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
  date = {2020-02-24},
  eprint = {1802.09081},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.09081},
  urldate = {2023-08-06},
  abstract = {Model-free reinforcement learning (RL) is a powerful, general tool for learning complex behaviors. However, its sample efficiency is often impractically large for solving challenging real-world problems, even with off-policy algorithms such as Q-learning. A limiting factor in classic model-free RL is that the learning signal consists only of scalar rewards, ignoring much of the rich information contained in state transition tuples. Model-based RL uses this information, by training a predictive model, but often does not achieve the same asymptotic performance as model-free RL due to model bias. We introduce temporal difference models (TDMs), a family of goal-conditioned value functions that can be trained with model-free learning and used for model-based control. TDMs combine the benefits of model-free and model-based RL: they leverage the rich information in state transitions to learn very efficiently, while still attaining asymptotic performance that exceeds that of direct model-based RL methods. Our experimental results show that, on a range of continuous control tasks, TDMs provide a substantial improvement in efficiency compared to state-of-the-art model-based and model-free methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/pongTemporalDifferenceModels2020.pdf;/Users/akhilnagariya/Zotero/storage/9JB27FQB/1802.html}
}

@article{pratiUseInteractionDesign2022,
  title = {Use of {{Interaction Design Methodologies}} for {{Human}}–{{Robot Collaboration}} in {{Industrial Scenarios}}},
  author = {Prati, Elisa and Villani, Valeria and Grandi, Fabio and Peruzzini, Margherita and Sabattini, Lorenzo},
  date = {2022-10},
  journaltitle = {IEEE Transactions on Automation Science and Engineering},
  shortjournal = {IEEE Trans. Automat. Sci. Eng.},
  volume = {19},
  number = {4},
  pages = {3126--3138},
  issn = {1545-5955, 1558-3783},
  doi = {10.1109/TASE.2021.3107583},
  url = {https://ieeexplore.ieee.org/document/9537740/},
  urldate = {2024-01-23},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/pratiUseInteractionDesign2022.pdf}
}

@incollection{procopioTerrainSegmentationOnLine2009,
  title = {Terrain {{Segmentation}} with {{On-Line Mixtures}} of {{Experts}} for {{Autonomous Robot Navigation}}},
  booktitle = {Multiple {{Classifier Systems}}},
  author = {Procopio, Michael J. and Kegelmeyer, W. Philip and Grudic, Greg and Mulligan, Jane},
  editor = {Benediktsson, Jón Atli and Kittler, Josef and Roli, Fabio},
  date = {2009},
  volume = {5519},
  pages = {385--397},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  urldate = {2024-03-06},
  isbn = {978-3-642-02325-5 978-3-642-02326-2},
  file = {/Users/akhilnagariya/Documents/978-3-642-02326-2_39.pdf}
}

@article{pupaDynamicPlannerSafe2024,
  title = {A {{Dynamic Planner}} for {{Safe}} and {{Predictable Human-Robot Collaboration}}},
  author = {Pupa, Andrea and Minelli, Marco and Secchi, Cristian},
  date = {2024-01},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {9},
  number = {1},
  pages = {507--514},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2023.3334977},
  url = {https://ieeexplore.ieee.org/document/10323521/},
  urldate = {2024-01-23},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/pupaDynamicPlannerSafe2024.pdf}
}

@online{rajeswaranEPOptLearningRobust2017,
  title = {{{EPOpt}}: {{Learning Robust Neural Network Policies Using Model Ensembles}}},
  shorttitle = {{{EPOpt}}},
  author = {Rajeswaran, Aravind and Ghotra, Sarvjeet and Ravindran, Balaraman and Levine, Sergey},
  date = {2017-03-03},
  eprint = {1610.01283},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1610.01283},
  urldate = {2023-08-08},
  abstract = {Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/rajeswaranEPOptLearningRobust2017.pdf;/Users/akhilnagariya/Zotero/storage/SSCN6B24/1610.html}
}

@online{rajvanshiSayNavGroundingLarge2023,
  title = {{{SayNav}}: {{Grounding Large Language Models}} for {{Dynamic Planning}} to {{Navigation}} in {{New Environments}}},
  shorttitle = {{{SayNav}}},
  author = {Rajvanshi, Abhinav and Sikka, Karan and Lin, Xiao and Lee, Bhoram and Chiu, Han-Pang and Velasquez, Alvaro},
  date = {2023-09-22},
  eprint = {2309.04077},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.04077},
  url = {http://arxiv.org/abs/2309.04077},
  urldate = {2023-10-05},
  abstract = {Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on a new multi-object navigation task, that requires the agent to utilize a massive amount of human knowledge to efficiently search multiple different objects in an unknown environment. SayNav outperforms an oracle based Point-nav baseline, achieving a success rate of 95.35\% (vs 56.06\% for the baseline), under the ideal settings on this task, highlighting its ability to generate dynamic plans for successfully locating objects in large-scale new environments. In addition, SayNav also enables efficient generalization of learning to navigate from simulation to real novel environments.},
  pubstate = {preprint},
  version = {3},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/rajvanshiSayNavGroundingLarge2023.pdf;/Users/akhilnagariya/Zotero/storage/VTVP57J6/2309.html}
}

@online{ramakrishnanHabitatMatterport3DDataset2021,
  title = {Habitat-{{Matterport 3D Dataset}} ({{HM3D}}): 1000 {{Large-scale 3D Environments}} for {{Embodied AI}}},
  shorttitle = {Habitat-{{Matterport 3D Dataset}} ({{HM3D}})},
  author = {Ramakrishnan, Santhosh K. and Gokaslan, Aaron and Wijmans, Erik and Maksymets, Oleksandr and Clegg, Alex and Turner, John and Undersander, Eric and Galuba, Wojciech and Westbury, Andrew and Chang, Angel X. and Savva, Manolis and Zhao, Yili and Batra, Dhruv},
  date = {2021-09-16},
  eprint = {2109.08238},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2109.08238},
  urldate = {2023-11-08},
  abstract = {We present the Habitat-Matterport 3D (HM3D) dataset. HM3D is a large-scale dataset of 1,000 building-scale 3D reconstructions from a diverse set of real-world locations. Each scene in the dataset consists of a textured 3D mesh reconstruction of interiors such as multi-floor residences, stores, and other private indoor spaces. HM3D surpasses existing datasets available for academic research in terms of physical scale, completeness of the reconstruction, and visual fidelity. HM3D contains 112.5k m\^{}2 of navigable space, which is 1.4 - 3.7x larger than other building-scale datasets such as MP3D and Gibson. When compared to existing photorealistic 3D datasets such as Replica, MP3D, Gibson, and ScanNet, images rendered from HM3D have 20 - 85\% higher visual fidelity w.r.t. counterpart images captured with real cameras, and HM3D meshes have 34 - 91\% fewer artifacts due to incomplete surface reconstruction. The increased scale, fidelity, and diversity of HM3D directly impacts the performance of embodied AI agents trained using it. In fact, we find that HM3D is `pareto optimal' in the following sense -- agents trained to perform PointGoal navigation on HM3D achieve the highest performance regardless of whether they are evaluated on HM3D, Gibson, or MP3D. No similar claim can be made about training on other datasets. HM3D-trained PointNav agents achieve 100\% performance on Gibson-test dataset, suggesting that it might be time to retire that episode dataset.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/ramakrishnanHabitatMatterport3DDataset2021.pdf;/Users/akhilnagariya/Zotero/storage/F93BXKC6/2109.html}
}

@article{readingCategoricalDepthDistribution2021,
  title = {Categorical {{Depth Distribution Network}} for {{Monocular 3D Object Detection}}},
  author = {Reading, Cody and Harakeh, Ali and Chae, Julia and Waslander, Steven L.},
  date = {2021-06},
  journaltitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {8551--8560},
  publisher = {IEEE},
  location = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.00845},
  url = {https://ieeexplore.ieee.org/document/9578221/},
  urldate = {2023-04-20},
  abstract = {Monocular 3D object detection is a key problem for autonomous vehicles, as it provides a solution with simple configuration compared to typical multi-sensor systems. The main challenge in monocular 3D detection lies in accurately predicting object depth, which must be inferred from object and scene cues due to the lack of direct range measurement. Many methods attempt to directly estimate depth to assist in 3D detection, but show limited performance as a result of depth inaccuracy. Our proposed solution, Categorical Depth Distribution Network (CaDDN), uses a predicted categorical depth distribution for each pixel to project rich contextual feature information to the appropriate depth interval in 3D space. We then use the computationally efficient bird’s-eye-view projection and single-stage detector to produce the final output detections. We design CaDDN as a fully differentiable end-to-end approach for joint depth estimation and object detection. We validate our approach on the KITTI 3D object detection benchmark, where we rank 1st among published monocular methods. We also provide the first monocular 3D detection results on the newly released Waymo Open Dataset. We provide a code release for CaDDN which is made available.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {9781665445092},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/readingCategoricalDepthDistribution2021.pdf}
}

@article{rombachHighResolutionImageSynthesis2021,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  date = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2112.10752},
  url = {https://arxiv.org/abs/2112.10752},
  urldate = {2023-07-16},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/rombachHighResolutionImageSynthesis2021.pdf}
}

@online{ROSHome,
  title = {{{ROS}}: {{Home}}},
  url = {https://www.ros.org/},
  urldate = {2024-02-24},
  file = {/Users/akhilnagariya/Zotero/storage/HS3EQ6QK/www.ros.org.html}
}

@inproceedings{sahaTranslatingImagesMaps2022,
  title = {Translating {{Images}} into {{Maps}}},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Saha, Avishkar and Mendez, Oscar and Russell, Chris and Bowden, Richard},
  date = {2022-05-23},
  pages = {9200--9206},
  publisher = {IEEE},
  location = {Philadelphia, PA, USA},
  doi = {10.1109/ICRA46639.2022.9811901},
  url = {https://ieeexplore.ieee.org/document/9811901/},
  urldate = {2023-04-20},
  eventtitle = {2022 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-72819-681-7},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/sahaTranslatingImagesMaps2022.pdf}
}

@online{sathyamoorthyTerraPNUnstructuredTerrain2022,
  title = {{{TerraPN}}: {{Unstructured Terrain Navigation}} Using {{Online Self-Supervised Learning}}},
  shorttitle = {{{TerraPN}}},
  author = {Sathyamoorthy, Adarsh Jagan and Weerakoon, Kasun and Guan, Tianrui and Liang, Jing and Manocha, Dinesh},
  date = {2022-06-22},
  eprint = {2202.12873},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.12873},
  urldate = {2024-02-25},
  abstract = {We present TerraPN, a novel method that learns the surface properties (traction, bumpiness, deformability, etc.) of complex outdoor terrains directly from robot-terrain interactions through self-supervised learning, and uses it for autonomous robot navigation. Our method uses RGB images of terrain surfaces and the robot's velocities as inputs, and the IMU vibrations and odometry errors experienced by the robot as labels for self-supervision. Our method computes a surface cost map that differentiates smooth, high-traction surfaces (low navigation costs) from bumpy, slippery, deformable surfaces (high navigation costs). We compute the cost map by non-uniformly sampling patches from the input RGB image by detecting boundaries between surfaces resulting in low inference times (47.27\% lower) compared to uniform sampling and existing segmentation methods. We present a novel navigation algorithm that accounts for a surface's cost, computes cost-based acceleration limits for the robot, and dynamically feasible, collision-free trajectories. TerraPN's surface cost prediction can be trained in \textasciitilde 25 minutes for five different surfaces, compared to several hours for previous learning-based segmentation methods. In terms of navigation, our method outperforms previous works in terms of success rates (up to 35.84\% higher), vibration cost of the trajectories (up to 21.52\% lower), and slowing the robot on bumpy, deformable surfaces (up to 46.76\% slower) in different scenarios.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/sathyamoorthyTerraPNUnstructuredTerrain2022.pdf;/Users/akhilnagariya/Zotero/storage/YPNXCG4H/2202.html}
}

@online{schaldenbrandFRIDACollaborativeRobot2022,
  title = {{{FRIDA}}: {{A Collaborative Robot Painter}} with a {{Differentiable}}, {{Real2Sim2Real Planning Environment}}},
  shorttitle = {{{FRIDA}}},
  author = {Schaldenbrand, Peter and McCann, James and Oh, Jean},
  date = {2022-10-02},
  eprint = {2210.00664},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.00664},
  url = {http://arxiv.org/abs/2210.00664},
  urldate = {2023-08-17},
  abstract = {Painting is an artistic process of rendering visual content that achieves the high-level communication goals of an artist that may change dynamically throughout the creative process. In this paper, we present a Framework and Robotics Initiative for Developing Arts (FRIDA) that enables humans to produce paintings on canvases by collaborating with a painter robot using simple inputs such as language descriptions or images. FRIDA introduces several technical innovations for computationally modeling a creative painting process. First, we develop a fully differentiable simulation environment for painting, adopting the idea of real to simulation to real (real2sim2real). We show that our proposed simulated painting environment is higher fidelity to reality than existing simulation environments used for robot painting. Second, to model the evolving dynamics of a creative process, we develop a planning approach that can continuously optimize the painting plan based on the evolving canvas with respect to the high-level goals. In contrast to existing approaches where the content generation process and action planning are performed independently and sequentially, FRIDA adapts to the stochastic nature of using paint and a brush by continually re-planning and re-assessing its semantic goals based on its visual perception of the painting progress. We describe the details on the technical approach as well as the system integration.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/schaldenbrandFRIDACollaborativeRobot2022.pdf;/Users/akhilnagariya/Zotero/storage/UTDRGWX4/2210.html}
}

@inproceedings{schneiderExploitingModelUncertainty1996,
  title = {Exploiting {{Model Uncertainty Estimates}} for {{Safe Dynamic Control Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schneider, Jeff},
  date = {1996},
  volume = {9},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper/1996/hash/93fb9d4b16aa750c7475b6d601c35c2c-Abstract.html},
  urldate = {2023-08-08},
  abstract = {Model  learning combined  with  dynamic  programming has  been  shown  to  be effective  for  learning control  of continuous state dynamic systems.  The  simplest method assumes the learned model is correct  and applies dynamic  programming to it, but many approximators provide uncertainty estimates  on  the  fit.  How  can  they  be  exploited?  This  paper  addresses  the  case  where the system must be prevented from having catastrophic failures dur(cid:173) ing learning.  We  propose  a  new  algorithm adapted  from  the  dual  control  literature  and  use  Bayesian  locally  weighted  regression  models  with  dy(cid:173) namic programming.  A common reinforcement learning assumption is that  aggressive exploration should be encouraged.  This paper addresses the con(cid:173) verse  case  in which  the system  has  to  reign  in exploration.  The algorithm  is illustrated on a 4 dimensional simulated control  problem.},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/schneiderExploitingModelUncertainty1996.pdf}
}

@inproceedings{schumannSemanticSegmentationRadar2018,
  title = {Semantic {{Segmentation}} on {{Radar Point Clouds}}},
  booktitle = {2018 21st {{International Conference}} on {{Information Fusion}} ({{FUSION}})},
  author = {Schumann, Ole and Hahn, Markus and Dickmann, Jurgen and Wohler, Christian},
  date = {2018-07},
  pages = {2179--2186},
  publisher = {IEEE},
  location = {Cambridge},
  doi = {10.23919/ICIF.2018.8455344},
  url = {https://ieeexplore.ieee.org/document/8455344/},
  urldate = {2023-04-13},
  eventtitle = {2018 21st {{International Conference}} on {{Information Fusion}} ({{FUSION}} 2018)},
  isbn = {978-0-9964527-6-2},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/schumannSemanticSegmentationRadar2018.pdf}
}

@inproceedings{schusterLandmarkBasedRadar2016,
  title = {Landmark Based Radar {{SLAM}} Using Graph Optimization},
  booktitle = {2016 {{IEEE}} 19th {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Schuster, F. and Keller, C. G. and Rapp, M. and Haueis, M. and Curio, C.},
  date = {2016-11},
  pages = {2559--2564},
  publisher = {IEEE},
  location = {Rio de Janeiro, Brazil},
  doi = {10.1109/ITSC.2016.7795967},
  url = {http://ieeexplore.ieee.org/document/7795967/},
  urldate = {2023-04-13},
  eventtitle = {2016 {{IEEE}} 19th {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  isbn = {978-1-5090-1889-5},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/schusterLandmarkBasedRadar2016.pdf}
}

@online{shahGNMGeneralNavigation2023,
  title = {{{GNM}}: {{A General Navigation Model}} to {{Drive Any Robot}}},
  shorttitle = {{{GNM}}},
  author = {Shah, Dhruv and Sridhar, Ajay and Bhorkar, Arjun and Hirose, Noriaki and Levine, Sergey},
  date = {2023-05-22},
  eprint = {2210.03370},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.03370},
  urldate = {2023-10-31},
  abstract = {Learning provides a powerful tool for vision-based navigation, but the capabilities of learning-based policies are constrained by limited training data. If we could combine data from all available sources, including multiple kinds of robots, we could train more powerful navigation models. In this paper, we study how a general goal-conditioned model for vision-based navigation can be trained on data obtained from many distinct but structurally similar robots, and enable broad generalization across environments and embodiments. We analyze the necessary design decisions for effective data sharing across robots, including the use of temporal context and standardized action spaces, and demonstrate that an omnipolicy trained from heterogeneous datasets outperforms policies trained on any single dataset. We curate 60 hours of navigation trajectories from 6 distinct robots, and deploy the trained GNM on a range of new robots, including an underactuated quadrotor. We find that training on diverse data leads to robustness against degradation in sensing and actuation. Using a pre-trained navigation model with broad generalization capabilities can bootstrap applications on novel robots going forward, and we hope that the GNM represents a step in that direction. For more information on the datasets, code, and videos, please check out our project page https://sites.google.com/view/drive-any-robot.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/Zotero/storage/F3UC9KZF/shahGNMGeneralNavigation2023.pdf;/Users/akhilnagariya/Zotero/storage/2P4XX3CD/2210.html}
}

@online{shahLMNavRoboticNavigation2022,
  title = {{{LM-Nav}}: {{Robotic Navigation}} with {{Large Pre-Trained Models}} of {{Language}}, {{Vision}}, and {{Action}}},
  shorttitle = {{{LM-Nav}}},
  author = {Shah, Dhruv and Osinski, Blazej and Ichter, Brian and Levine, Sergey},
  date = {2022-07-26},
  eprint = {2207.04429},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2207.04429},
  urldate = {2023-08-22},
  abstract = {Goal-conditioned policies for robotic navigation can be trained on large, unannotated datasets, providing for good generalization to real-world settings. However, particularly in vision-based settings where specifying goals requires an image, this makes for an unnatural interface. Language provides a more convenient modality for communication with robots, but contemporary methods typically require expensive supervision, in the form of trajectories annotated with language descriptions. We present a system, LM-Nav, for robotic navigation that enjoys the benefits of training on unannotated large datasets of trajectories, while still providing a high-level interface to the user. Instead of utilizing a labeled instruction following dataset, we show that such a system can be constructed entirely out of pre-trained models for navigation (ViNG), image-language association (CLIP), and language modeling (GPT-3), without requiring any fine-tuning or language-annotated robot data. We instantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon navigation through complex, outdoor environments from natural language instructions. For videos of our experiments, code release, and an interactive Colab notebook that runs in your browser, please check out our project page https://sites.google.com/view/lmnav},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/Zotero/storage/JW7JX2YY/shahLMNavRoboticNavigation2022.pdf;/Users/akhilnagariya/Zotero/storage/VBA6W4L6/2207.html}
}

@online{shahLMNavRoboticNavigation2022a,
  title = {{{LM-Nav}}: {{Robotic Navigation}} with {{Large Pre-Trained Models}} of {{Language}}, {{Vision}}, and {{Action}}},
  shorttitle = {{{LM-Nav}}},
  author = {Shah, Dhruv and Osinski, Blazej and Ichter, Brian and Levine, Sergey},
  date = {2022-07-26},
  eprint = {2207.04429},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2207.04429},
  urldate = {2023-10-30},
  abstract = {Goal-conditioned policies for robotic navigation can be trained on large, unannotated datasets, providing for good generalization to real-world settings. However, particularly in vision-based settings where specifying goals requires an image, this makes for an unnatural interface. Language provides a more convenient modality for communication with robots, but contemporary methods typically require expensive supervision, in the form of trajectories annotated with language descriptions. We present a system, LM-Nav, for robotic navigation that enjoys the benefits of training on unannotated large datasets of trajectories, while still providing a high-level interface to the user. Instead of utilizing a labeled instruction following dataset, we show that such a system can be constructed entirely out of pre-trained models for navigation (ViNG), image-language association (CLIP), and language modeling (GPT-3), without requiring any fine-tuning or language-annotated robot data. We instantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon navigation through complex, outdoor environments from natural language instructions. For videos of our experiments, code release, and an interactive Colab notebook that runs in your browser, please check out our project page https://sites.google.com/view/lmnav},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/shahLMNavRoboticNavigation2022a.pdf;/Users/akhilnagariya/Zotero/storage/DW4X3QVN/2207.html}
}

@online{shahNavigationLargeLanguage2023,
  title = {Navigation with {{Large Language Models}}: {{Semantic Guesswork}} as a {{Heuristic}} for {{Planning}}},
  shorttitle = {Navigation with {{Large Language Models}}},
  author = {Shah, Dhruv and Equi, Michael and Osinski, Blazej and Xia, Fei and Ichter, Brian and Levine, Sergey},
  date = {2023-10-16},
  eprint = {2310.10103},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.10103},
  urldate = {2023-10-30},
  abstract = {Navigation in unfamiliar environments presents a major challenge for robots: while mapping and planning techniques can be used to build up a representation of the world, quickly discovering a path to a desired goal in unfamiliar settings with such methods often requires lengthy mapping and exploration. Humans can rapidly navigate new environments, particularly indoor environments that are laid out logically, by leveraging semantics -- e.g., a kitchen often adjoins a living room, an exit sign indicates the way out, and so forth. Language models can provide robots with such knowledge, but directly using language models to instruct a robot how to reach some destination can also be impractical: while language models might produce a narrative about how to reach some goal, because they are not grounded in real-world observations, this narrative might be arbitrarily wrong. Therefore, in this paper we study how the ``semantic guesswork'' produced by language models can be utilized as a guiding heuristic for planning algorithms. Our method, Language Frontier Guide (LFG), uses the language model to bias exploration of novel real-world environments by incorporating the semantic knowledge stored in language models as a search heuristic for planning with either topological or metric maps. We evaluate LFG in challenging real-world environments and simulated benchmarks, outperforming uninformed exploration and other ways of using language models.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/Zotero/storage/RLB64ZAU/shahNavigationLargeLanguage2023.pdf;/Users/akhilnagariya/Zotero/storage/I5Q72INJ/2310.html}
}

@inproceedings{shahViNGLearningOpenWorld2021,
  title = {{{ViNG}}: {{Learning Open-World Navigation}} with {{Visual Goals}}},
  shorttitle = {{{ViNG}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Shah, Dhruv and Eysenbach, Benjamin and Kahn, Gregory and Rhinehart, Nicholas and Levine, Sergey},
  date = {2021-05-30},
  eprint = {2012.09812},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {13215--13222},
  doi = {10.1109/ICRA48506.2021.9561936},
  url = {http://arxiv.org/abs/2012.09812},
  urldate = {2023-10-30},
  abstract = {We propose a learning-based navigation system for reaching visually indicated goals and demonstrate this system on a real mobile robot platform. Learning provides an appealing alternative to conventional methods for robotic navigation: instead of reasoning about environments in terms of geometry and maps, learning can enable a robot to learn about navigational affordances, understand what types of obstacles are traversable (e.g., tall grass) or not (e.g., walls), and generalize over patterns in the environment. However, unlike conventional planning algorithms, it is harder to change the goal for a learned policy during deployment. We propose a method for learning to navigate towards a goal image of the desired destination. By combining a learned policy with a topological graph constructed out of previously observed data, our system can determine how to reach this visually indicated goal even in the presence of variable appearance and lighting. Three key insights, waypoint proposal, graph pruning and negative mining, enable our method to learn to navigate in real-world environments using only offline data, a setting where prior methods struggle. We instantiate our method on a real outdoor ground robot and show that our system, which we call ViNG, outperforms previously-proposed methods for goal-conditioned reinforcement learning, including other methods that incorporate reinforcement learning and search. We also study how \textbackslash sysName generalizes to unseen environments and evaluate its ability to adapt to such an environment with growing experience. Finally, we demonstrate ViNG on a number of real-world applications, such as last-mile delivery and warehouse inspection. We encourage the reader to visit the project website for videos of our experiments and demonstrations sites.google.com/view/ving-robot.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/Zotero/storage/N4KVSPIZ/shahViNGLearningOpenWorld2021.pdf;/Users/akhilnagariya/Zotero/storage/8XCQ7D35/2012.html}
}

@online{shahViNTFoundationModel2023,
  title = {{{ViNT}}: {{A Foundation Model}} for {{Visual Navigation}}},
  shorttitle = {{{ViNT}}},
  author = {Shah, Dhruv and Sridhar, Ajay and Dashora, Nitish and Stachowicz, Kyle and Black, Kevin and Hirose, Noriaki and Levine, Sergey},
  date = {2023-10-24},
  eprint = {2306.14846},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2306.14846},
  urldate = {2023-10-31},
  abstract = {General-purpose pre-trained models ("foundation models") have enabled practitioners to produce generalizable solutions for individual machine learning problems with datasets that are significantly smaller than those required for learning from scratch. Such models are typically trained on large and diverse datasets with weak supervision, consuming much more training data than is available for any individual downstream application. In this paper, we describe the Visual Navigation Transformer (ViNT), a foundation model that aims to bring the success of general-purpose pre-trained models to vision-based robotic navigation. ViNT is trained with a general goal-reaching objective that can be used with any navigation dataset, and employs a flexible Transformer-based architecture to learn navigational affordances and enable efficient adaptation to a variety of downstream navigational tasks. ViNT is trained on a number of existing navigation datasets, comprising hundreds of hours of robotic navigation from a variety of different robotic platforms, and exhibits positive transfer, outperforming specialist models trained on singular datasets. ViNT can be augmented with diffusion-based subgoal proposals to explore novel environments, and can solve kilometer-scale navigation problems when equipped with long-range heuristics. ViNT can also be adapted to novel task specifications with a technique inspired by prompt-tuning, where the goal encoder is replaced by an encoding of another task modality (e.g., GPS waypoints or routing commands) embedded into the same space of goal tokens. This flexibility and ability to accommodate a variety of downstream problem domains establishes ViNT as an effective foundation model for mobile robotics. For videos, code, and model checkpoints, see our project page at https://visualnav-transformer.github.io.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/Zotero/storage/VT44RQBZ/shahViNTFoundationModel2023.pdf;/Users/akhilnagariya/Zotero/storage/TYF9T6BG/2306.html}
}

@online{shekLearningPhysicalHuman2023,
  title = {Learning from {{Physical Human Feedback}}: {{An Object-Centric One-Shot Adaptation Method}}},
  shorttitle = {Learning from {{Physical Human Feedback}}},
  author = {Shek, Alvin and Su, Bo Ying and Chen, Rui and Liu, Changliu},
  date = {2023-06-02},
  eprint = {2203.04951},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.04951},
  urldate = {2023-08-17},
  abstract = {For robots to be effectively deployed in novel environments and tasks, they must be able to understand the feedback expressed by humans during intervention. This can either correct undesirable behavior or indicate additional preferences. Existing methods either require repeated episodes of interactions or assume prior known reward features, which is data-inefficient and can hardly transfer to new tasks. We relax these assumptions by describing human tasks in terms of object-centric sub-tasks and interpreting physical interventions in relation to specific objects. Our method, Object Preference Adaptation (OPA), is composed of two key stages: 1) pre-training a base policy to produce a wide variety of behaviors, and 2) online-updating according to human feedback. The key to our fast, yet simple adaptation is that general interaction dynamics between agents and objects are fixed, and only object-specific preferences are updated. Our adaptation occurs online, requires only one human intervention (one-shot), and produces new behaviors never seen during training. Trained on cheap synthetic data instead of expensive human demonstrations, our policy correctly adapts to human perturbations on realistic tasks on a physical 7DOF robot. Videos, code, and supplementary material are provided.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/shekLearningPhysicalHuman2023.pdf;/Users/akhilnagariya/Zotero/storage/ZMXR3CA4/2203.html}
}

@online{silverPredictronEndToEndLearning2017,
  title = {The {{Predictron}}: {{End-To-End Learning}} and {{Planning}}},
  shorttitle = {The {{Predictron}}},
  author = {Silver, David and family=Hasselt, given=Hado, prefix=van, useprefix=true and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and Dulac-Arnold, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and Degris, Thomas},
  date = {2017-07-20},
  eprint = {1612.08810},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1612.08810},
  url = {http://arxiv.org/abs/1612.08810},
  urldate = {2023-08-08},
  abstract = {One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple "imagined" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.},
  pubstate = {preprint},
  version = {3},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/silverPredictronEndToEndLearning2017.pdf;/Users/akhilnagariya/Zotero/storage/QMC454EL/1612.html}
}

@inproceedings{simchowitzLearningMixingSharp2018,
  title = {Learning {{Without Mixing}}: {{Towards A Sharp Analysis}} of {{Linear System Identification}}},
  shorttitle = {Learning {{Without Mixing}}},
  author = {Simchowitz, Max and Mania, Horia and Tu, Stephen and Jordan, Michael I. and Recht, B.},
  date = {2018-02-22},
  url = {https://www.semanticscholar.org/paper/Learning-Without-Mixing%3A-Towards-A-Sharp-Analysis-Simchowitz-Mania/88141e44d400bd1b38f6420358d17e621d6472ee},
  urldate = {2023-08-06},
  abstract = {We prove that the ordinary least-squares (OLS) estimator attains nearly minimax optimal performance for the identification of linear dynamical systems from a single observed trajectory. Our upper bound relies on a generalization of Mendelson's small-ball method to dependent data, eschewing the use of standard mixing-time arguments. Our lower bounds reveal that these upper bounds match up to logarithmic factors. In particular, we capture the correct signal-to-noise behavior of the problem, showing that more unstable linear systems are easier to estimate. This behavior is qualitatively different from arguments which rely on mixing-time calculations that suggest that unstable systems are more difficult to estimate. We generalize our technique to provide bounds for a more general class of linear response time-series.},
  eventtitle = {Annual {{Conference Computational Learning Theory}}},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/simchowitzLearningMixingSharp2018.pdf}
}

@article{singireddyAutomatonDistillationNeuroSymbolic2022,
  title = {Automaton {{Distillation}}: {{A Neuro-Symbolic Transfer Learning Approach}} for {{Deep RL}}},
  shorttitle = {Automaton {{Distillation}}},
  author = {Singireddy, Suraj and Jha, Sumit Kumar and Velasquez, Alvaro},
  date = {2022-09-29},
  url = {https://openreview.net/forum?id=C6CEY8xiA7v},
  urldate = {2023-10-05},
  abstract = {Reinforcement learning is a powerful tool for finding optimal policies in sequential decision processes. However, deep learning methods suffer from two weaknesses: collecting the amount of agent experience required for practical RL problems is prohibitively expensive, and the learned policies exhibit poor generalization on tasks outside the training distribution. To mitigate these issues, we introduce automaton distillation, a form of neuro-symbolic transfer learning in which Q-value estimates from a teacher are distilled into a low-dimensional representation in the form of an automaton. We then propose two methods for generating Q-value estimates: static transfer, which reasons over an abstract MDP constructed based on prior knowledge, and dynamic transfer, where symbolic information is extracted from a DQN teacher. The resulting Q-value estimates from either method are used to bootstrap learning in the target environment via a modified DQN loss function. We list several failure modes of existing automaton-based transfer methods and demonstrate that both static and dynamic automaton distillation decrease the time required to find optimal policies for various decision tasks.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/singireddyAutomatonDistillationNeuroSymbolic2022.pdf}
}

@inproceedings{sivaprakasamImprovingOffroadPlanning2021,
  title = {Improving {{Off-road Planning Techniques}} with {{Learned Costs}} from {{Physical Interactions}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Sivaprakasam, Matthew and Triest, Samuel and Wang, Wenshan and Yin, Peng and Scherer, Sebastian},
  date = {2021-05-30},
  pages = {4844--4850},
  publisher = {IEEE},
  location = {Xi'an, China},
  doi = {10.1109/ICRA48506.2021.9561881},
  url = {https://ieeexplore.ieee.org/document/9561881/},
  urldate = {2024-02-27},
  eventtitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-72819-077-8},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/sivaprakasamImprovingOffroadPlanning2021.pdf}
}

@online{sridharNoMaDGoalMasked2023,
  title = {{{NoMaD}}: {{Goal Masked Diffusion Policies}} for {{Navigation}} and {{Exploration}}},
  shorttitle = {{{NoMaD}}},
  author = {Sridhar, Ajay and Shah, Dhruv and Glossop, Catherine and Levine, Sergey},
  date = {2023-10-11},
  eprint = {2310.07896},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.07896},
  urldate = {2023-11-07},
  abstract = {Robotic learning for navigation in unfamiliar environments needs to provide policies for both task-oriented navigation (i.e., reaching a goal that the robot has located), and task-agnostic exploration (i.e., searching for a goal in a novel setting). Typically, these roles are handled by separate models, for example by using subgoal proposals, planning, or separate navigation strategies. In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the latter providing the ability to search novel environments, and the former providing the ability to reach a user-specified goal once it has been located. We show that this unified policy results in better overall performance when navigating to visually indicated goals in novel environments, as compared to approaches that use subgoal proposals from generative models, or prior methods based on latent variable models. We instantiate our method by using a large-scale Transformer-based policy trained on data from multiple ground robots, with a diffusion model decoder to flexibly handle both goal-conditioned and goal-agnostic navigation. Our experiments, conducted on a real-world mobile robot platform, show effective navigation in unseen environments in comparison with five alternative methods, and demonstrate significant improvements in performance and lower collision rates, despite utilizing smaller models than state-of-the-art approaches. For more videos, code, and pre-trained model checkpoints, see https://general-navigation-models.github.io/nomad/},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/sridharNoMaDGoalMasked2023.pdf;/Users/akhilnagariya/Zotero/storage/9A4VYHUF/2310.html}
}

@online{sunDualPolicyIteration2018,
  title = {Dual {{Policy Iteration}}},
  author = {Sun, Wen and Gordon, Geoffrey J. and Boots, Byron and Bagnell, J. Andrew},
  date = {2018-04-05},
  eprint = {1805.10755},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.10755},
  urldate = {2023-05-06},
  abstract = {Recently, a novel class of Approximate Policy Iteration (API) algorithms have demonstrated impressive practical performance (e.g., ExIt from [2], AlphaGo-Zero from [27]). This new family of algorithms maintains, and alternately optimizes, two policies: a fast, reactive policy (e.g., a deep neural network) deployed at test time, and a slow, non-reactive policy (e.g., Tree Search), that can plan multiple steps ahead. The reactive policy is updated under supervision from the non-reactive policy, while the non-reactive policy is improved with guidance from the reactive policy. In this work we study this Dual Policy Iteration (DPI) strategy in an alternating optimization framework and provide a convergence analysis that extends existing API theory. We also develop a special instance of this framework which reduces the update of non-reactive policies to model-based optimal control using learned local models, and provides a theoretically sound way of unifying model-free and model-based RL approaches with unknown dynamics. We demonstrate the efficacy of our approach on various continuous control Markov Decision Processes.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/sunDualPolicyIteration2019.pdf;/Users/akhilnagariya/Zotero/storage/XJ5NVEMB/1805.html}
}

@online{suomelaPlaceNavTopologicalNavigation2023,
  title = {{{PlaceNav}}: {{Topological Navigation}} through {{Place Recognition}}},
  shorttitle = {{{PlaceNav}}},
  author = {Suomela, Lauri and Kalliola, Jussi and Edelman, Harry and Kämäräinen, Joni-Kristian},
  date = {2023-10-05},
  eprint = {2309.17260},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.17260},
  urldate = {2023-11-07},
  abstract = {Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present PlaceNav, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayesian filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76\% higher success rate in indoor and 23\% higher in outdoor navigation tasks with higher computational efficiency.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/suomelaPlaceNavTopologicalNavigation2023.pdf;/Users/akhilnagariya/Zotero/storage/ETS4PBEX/2309.html}
}

@online{suttonDynaStylePlanningLinear2012,
  title = {Dyna-{{Style Planning}} with {{Linear Function Approximation}} and {{Prioritized Sweeping}}},
  author = {Sutton, Richard S. and Szepesvari, Csaba and Geramifard, Alborz and Bowling, Michael P.},
  date = {2012-06-13},
  eprint = {1206.3285},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1206.3285},
  url = {http://arxiv.org/abs/1206.3285},
  urldate = {2023-08-06},
  abstract = {We consider the problem of efficiently learning optimal control policies and value functions over large state spaces in an online setting in which estimates must be available after each interaction with the world. This paper develops an explicitly model-based approach extending the Dyna architecture to linear function approximation. Dynastyle planning proceeds by generating imaginary experience from the world model and then applying model-free reinforcement learning algorithms to the imagined state transitions. Our main results are to prove that linear Dyna-style planning converges to a unique solution independent of the generating distribution, under natural conditions. In the policy evaluation setting, we prove that the limit point is the least-squares (LSTD) solution. An implication of our results is that prioritized-sweeping can be soundly extended to the linear approximation case, backing up to preceding features rather than to preceding states. We introduce two versions of prioritized sweeping with linear Dyna and briefly illustrate their performance empirically on the Mountain Car and Boyan Chain problems.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/suttonDynaStylePlanningLinear2012.pdf;/Users/akhilnagariya/Zotero/storage/NQMRDGYB/1206.html}
}

@inproceedings{szitaModelbasedReinforcementLearning2010,
  title = {Model-Based Reinforcement Learning with Nearly Tight Exploration Complexity Bounds},
  author = {Szita, I. and Szepesvari, Csaba},
  date = {2010-06-21},
  url = {https://www.semanticscholar.org/paper/Model-based-reinforcement-learning-with-nearly-Szita-Szepesvari/75c827ed14c8794b6babf05b944751877e7d2b77},
  urldate = {2023-08-09},
  abstract = {One might believe that model-based algorithms of reinforcement learning can propagate the obtained experience more quickly, and are able to direct exploration better. As a consequence, fewer exploratory actions should be enough to learn a good policy. Strangely enough, current theoretical results for model-based algorithms do not support this claim: In a finite Markov decision process with N states, the best bounds on the number of exploratory steps necessary are of order O(N2 log N), in contrast to the O(N log N) bound available for the model-free, delayed Q-learning algorithm. In this paper we show that Mormax, a modified version of the Rmax algorithm needs to make at most O(N log N) exploratory steps. This matches the lower bound up to logarithmic factors, as well as the upper bound of the state-of-the-art model-free algorithm, while our new bound improves the dependence on other problem parameters.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/szitaModelbasedReinforcementLearning2010.pdf}
}

@online{tagliabueRobustHighRateTrajectory2022,
  title = {Robust, {{High-Rate Trajectory Tracking}} on {{Insect-Scale Soft-Actuated Aerial Robots}} with {{Deep-Learned Tube MPC}}},
  author = {Tagliabue, Andrea and Hsiao, Yi-Hsuan and Fasel, Urban and Kutz, J. Nathan and Brunton, Steven L. and Chen, YuFeng and How, Jonathan P.},
  date = {2022-09-26},
  eprint = {2209.10007},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.10007},
  url = {http://arxiv.org/abs/2209.10007},
  urldate = {2023-08-17},
  abstract = {Accurate and agile trajectory tracking in sub-gram Micro Aerial Vehicles (MAVs) is challenging, as the small scale of the robot induces large model uncertainties, demanding robust feedback controllers, while the fast dynamics and computational constraints prevent the deployment of computationally expensive strategies. In this work, we present an approach for agile and computationally efficient trajectory tracking on the MIT SoftFly, a sub-gram MAV (0.7 grams). Our strategy employs a cascaded control scheme, where an adaptive attitude controller is combined with a neural network policy trained to imitate a trajectory tracking robust tube model predictive controller (RTMPC). The neural network policy is obtained using our recent work, which enables the policy to preserve the robustness of RTMPC, but at a fraction of its computational cost. We experimentally evaluate our approach, achieving position Root Mean Square Errors lower than 1.8 cm even in the more challenging maneuvers, obtaining a 60\% reduction in maximum position error compared to our previous work, and demonstrating robustness to large external disturbances},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/tagliabueRobustHighRateTrajectory2022.pdf;/Users/akhilnagariya/Zotero/storage/PQD2MN9L/2209.html}
}

@inproceedings{talvitieModelRegularizationStable2014,
  title = {Model {{Regularization}} for {{Stable Sample Rollouts}}},
  author = {Talvitie, Erik},
  date = {2014-07-23},
  url = {https://www.semanticscholar.org/paper/Model-Regularization-for-Stable-Sample-Rollouts-Talvitie/48aaaa1e59c58856315d814363f431b93f76e668},
  urldate = {2023-08-08},
  abstract = {When an imperfect model is used to generate sample rollouts, its errors tend to compound - a flawed sample is given as input to the model, which causes more errors, and so on. This presents a barrier to applying rollout-based planning algorithms to learned models. To address this issue, a training methodology called "hallucinated replay" is introduced, which adds samples from the model into the training data, thereby training the model to produce sensible predictions when its own samples are given as input. Capabilities and limitations of this approach are studied empirically. In several examples hallucinated replay allows effective planning with imperfect models while models trained using only real experience fail dramatically.},
  eventtitle = {Conference on {{Uncertainty}} in {{Artificial Intelligence}}},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/talvitieModelRegularizationStable2014.pdf}
}

@article{tamarIntegratingPartialModel2012,
  title = {Integrating a {{Partial Model}} into {{Model Free Reinforcement Learning}}},
  author = {Tamar, Aviv and Castro, Dotan Di and Meir, Ron},
  date = {2012},
  journaltitle = {Journal of Machine Learning Research},
  volume = {13},
  number = {61},
  pages = {1927--1966},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v13/tamar12a.html},
  urldate = {2023-08-06},
  abstract = {In reinforcement learning an agent uses online feedback from the environment in order to adaptively select an effective policy. Model free approaches address this task by directly mapping environmental states to actions, while model based methods attempt to construct a model of the environment, followed by a selection of optimal actions based on that model. Given the complementary advantages of both approaches, we suggest a novel procedure which augments a model free algorithm with a partial model. The resulting hybrid algorithm switches between a model based and a model free mode, depending on the current state and the agent's knowledge. Our method relies on a novel definition for a partially known model, and an estimator that incorporates such knowledge in order to reduce uncertainty in stochastic approximation iterations. We prove that such an approach leads to improved policy evaluation whenever environmental knowledge is available, without compromising performance when such knowledge is absent. Numerical simulations demonstrate the effectiveness of the approach on policy gradient and Q-learning algorithms, and its usefulness in solving a call admission control problem.},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/tamarIntegratingPartialModel2012.pdf}
}

@online{tamarValueIterationNetworks2017,
  title = {Value {{Iteration Networks}}},
  author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
  date = {2017-03-20},
  eprint = {1602.02867},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1602.02867},
  url = {http://arxiv.org/abs/1602.02867},
  urldate = {2023-08-08},
  abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
  pubstate = {preprint},
  version = {4},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/tamarValueIterationNetworks2017.pdf;/Users/akhilnagariya/Zotero/storage/64VEJWGD/1602.html}
}

@online{tanEfficientNetRethinkingModel2020,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  date = {2020-09-11},
  eprint = {1905.11946},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.11946},
  urldate = {2023-11-07},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/tanEfficientNetRethinkingModel2020.pdf;/Users/akhilnagariya/Zotero/storage/8NSHGC9P/1905.html}
}

@incollection{tangSearchingEfficient3D2020,
  title = {Searching {{Efficient 3D Architectures}} with {{Sparse Point-Voxel Convolution}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Tang, Haotian and Liu, Zhijian and Zhao, Shengyu and Lin, Yujun and Lin, Ji and Wang, Hanrui and Han, Song},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  volume = {12373},
  pages = {685--702},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-58604-1_41},
  url = {https://link.springer.com/10.1007/978-3-030-58604-1_41},
  urldate = {2023-04-20},
  isbn = {978-3-030-58603-4 978-3-030-58604-1},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/tangSearchingEfficient3D2020.pdf}
}

@article{thakarSurveyWheeledMobile2023,
  title = {A {{Survey}} of {{Wheeled Mobile Manipulation}}: {{A Decision-Making Perspective}}},
  shorttitle = {A {{Survey}} of {{Wheeled Mobile Manipulation}}},
  author = {Thakar, Shantanu and Srinivasan, Srivatsan and Al-Hussaini, Sarah and Bhatt, Prahar M. and Rajendran, Pradeep and Jung Yoon, Yeo and Dhanaraj, Neel and Malhan, Rishi K. and Schmid, Matthias and Krovi, Venkat N. and Gupta, Satyandra K.},
  date = {2023-04-01},
  journaltitle = {Journal of Mechanisms and Robotics},
  volume = {15},
  number = {2},
  pages = {020801},
  issn = {1942-4302, 1942-4310},
  doi = {10.1115/1.4054611},
  url = {https://asmedigitalcollection.asme.org/mechanismsrobotics/article/15/2/020801/1141071/A-Survey-of-Wheeled-Mobile-Manipulation-A-Decision},
  urldate = {2024-01-23},
  abstract = {Abstract             Mobile manipulators that combine base mobility with the dexterity of an articulated manipulator have gained popularity in numerous applications ranging from manufacturing and infrastructure inspection to domestic service. Deployments span a range of interaction tasks with the operational environment comprising minimal interaction tasks such as inspection and complex interaction tasks such as logistics resupply and assembly. This flexibility, offered by the redundancy, needs to be carefully orchestrated to realize enhanced performance. Thus, advanced decision-support methodologies and frameworks are crucial for successful mobile manipulation in (semi-) autonomous and teleoperation contexts. Given the enormous scope of the literature, we restrict our attention to decision-support frameworks specifically in the context of wheeled mobile manipulation. Hence, here, we present a classification of wheeled mobile manipulation literature while accounting for its diversity. The intertwining of the deployment tasks, application arenas, and decision-making methodologies are discussed with an eye for future avenues for research.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/thakarSurveyWheeledMobile2023.pdf}
}

@incollection{thrunWinningDARPAGrand2006,
  title = {Winning the {{DARPA Grand Challenge}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2006},
  author = {Thrun, Sebastian},
  editor = {Fürnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  date = {2006},
  volume = {4212},
  pages = {4--4},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11871842_4},
  isbn = {978-3-540-45375-8 978-3-540-46056-5},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/thrunWinningDARPAGrand2006.pdf}
}

@inproceedings{tianFCOSFullyConvolutional2019,
  title = {{{FCOS}}: {{Fully Convolutional One-Stage Object Detection}}},
  shorttitle = {{{FCOS}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
  date = {2019-10},
  pages = {9626--9635},
  publisher = {IEEE},
  location = {Seoul, Korea (South)},
  doi = {10.1109/ICCV.2019.00972},
  url = {https://ieeexplore.ieee.org/document/9010746/},
  urldate = {2023-04-20},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-72814-803-8},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/tianFCOSFullyConvolutional2019.pdf;/Users/akhilnagariya/bibliography/bibtex-pdf/tianFCOSFullyConvolutional22.pdf;/Users/akhilnagariya/bibliography/bibtex-pdf/tianFCOSFullyConvolutional3.pdf}
}

@inproceedings{tobinDomainRandomizationTransferring2017,
  title = {Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World},
  booktitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  date = {2017-09},
  pages = {23--30},
  issn = {2153-0866},
  doi = {10.1109/IROS.2017.8202133},
  url = {https://ieeexplore.ieee.org/document/8202133},
  urldate = {2023-10-05},
  abstract = {Bridging the `reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  eventtitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/tobinDomainRandomizationTransferring2017.pdf;/Users/akhilnagariya/Zotero/storage/XHXDRPCL/8202133.html}
}

@online{triestLearningRiskAwareCostmaps2023,
  title = {Learning {{Risk-Aware Costmaps}} via {{Inverse Reinforcement Learning}} for {{Off-Road Navigation}}},
  author = {Triest, Samuel and Castro, Mateo Guaman and Maheshwari, Parv and Sivaprakasam, Matthew and Wang, Wenshan and Scherer, Sebastian},
  date = {2023-01-31},
  eprint = {2302.00134},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.00134},
  urldate = {2024-02-27},
  abstract = {The process of designing costmaps for off-road driving tasks is often a challenging and engineering-intensive task. Recent work in costmap design for off-road driving focuses on training deep neural networks to predict costmaps from sensory observations using corpora of expert driving data. However, such approaches are generally subject to over-confident mispredictions and are rarely evaluated in-the-loop on physical hardware. We present an inverse reinforcement learning-based method of efficiently training deep cost functions that are uncertainty-aware. We do so by leveraging recent advances in highly parallel model-predictive control and robotic risk estimation. In addition to demonstrating improvement at reproducing expert trajectories, we also evaluate the efficacy of these methods in challenging off-road navigation scenarios. We observe that our method significantly outperforms a geometric baseline, resulting in 44\% improvement in expert path reconstruction and 57\% fewer interventions in practice. We also observe that varying the risk tolerance of the vehicle results in qualitatively different navigation behaviors, especially with respect to higher-risk scenarios such as slopes and tall grass.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/triestLearningRiskAwareCostmaps2023.pdf;/Users/akhilnagariya/Zotero/storage/G7NY9LDW/2302.html}
}

@online{truongIndoorSimtoOutdoorRealLearningNavigate2023,
  title = {{{IndoorSim-to-OutdoorReal}}: {{Learning}} to {{Navigate Outdoors}} without Any {{Outdoor Experience}}},
  shorttitle = {{{IndoorSim-to-OutdoorReal}}},
  author = {Truong, Joanne and Zitkovich, April and Chernova, Sonia and Batra, Dhruv and Zhang, Tingnan and Tan, Jie and Yu, Wenhao},
  date = {2023-05-09},
  eprint = {2305.01098},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.01098},
  urldate = {2023-11-07},
  abstract = {We present IndoorSim-to-OutdoorReal (I2O), an end-to-end learned visual navigation approach, trained solely in simulated short-range indoor environments, and demonstrates zero-shot sim-to-real transfer to the outdoors for long-range navigation on the Spot robot. Our method uses zero real-world experience (indoor or outdoor), and requires the simulator to model no predominantly-outdoor phenomenon (sloped grounds, sidewalks, etc). The key to I2O transfer is in providing the robot with additional context of the environment (i.e., a satellite map, a rough sketch of a map by a human, etc.) to guide the robot's navigation in the real-world. The provided context-maps do not need to be accurate or complete -- real-world obstacles (e.g., trees, bushes, pedestrians, etc.) are not drawn on the map, and openings are not aligned with where they are in the real-world. Crucially, these inaccurate context-maps provide a hint to the robot about a route to take to the goal. We find that our method that leverages Context-Maps is able to successfully navigate hundreds of meters in novel environments, avoiding novel obstacles on its path, to a distant goal without a single collision or human intervention. In comparison, policies without the additional context fail completely. Lastly, we test the robustness of the Context-Map policy by adding varying degrees of noise to the map in simulation. We find that the Context-Map policy is surprisingly robust to noise in the provided context-map. In the presence of significantly inaccurate maps (corrupted with 50\% noise, or entirely blank maps), the policy gracefully regresses to the behavior of a policy with no context. Videos are available at https://www.joannetruong.com/projects/i2o.html},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/truongIndoorSimtoOutdoorRealLearningNavigate2023.pdf;/Users/akhilnagariya/Zotero/storage/W5TEYJ47/2305.html}
}

@incollection{valadaDeepMultispectralSemantic2017,
  title = {Deep {{Multispectral Semantic Scene Understanding}} of {{Forested Environments Using Multimodal Fusion}}},
  booktitle = {2016 {{International Symposium}} on {{Experimental Robotics}}},
  author = {Valada, Abhinav and Oliveira, Gabriel L. and Brox, Thomas and Burgard, Wolfram},
  editor = {Kulić, Dana and Nakamura, Yoshihiko and Khatib, Oussama and Venture, Gentiane},
  date = {2017},
  volume = {1},
  pages = {465--477},
  publisher = {Springer International Publishing},
  location = {Cham},
  urldate = {2024-03-06},
  isbn = {978-3-319-50114-7 978-3-319-50115-4}
}

@article{velasquezDynamicAutomatonGuidedReward2021,
  title = {Dynamic {{Automaton-Guided Reward Shaping}} for {{Monte Carlo Tree Search}}},
  author = {Velasquez, Alvaro and Bissey, Brett and Barak, Lior and Beckus, Andre and Alkhouri, Ismail and Melcer, Daniel and Atia, George},
  date = {2021-05-18},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {13},
  pages = {12015--12023},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i13.17427},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17427},
  urldate = {2023-10-05},
  abstract = {Reinforcement learning and planning have been revolutionized in recent years, due in part to the mass adoption of deep convolutional neural networks and the resurgence of powerful methods to refine decision-making policies. However, the problem of sparse reward signals and their representation remains pervasive in many domains. While various rewardshaping mechanisms and imitation learning approaches have been proposed to mitigate this problem, the use of humanaided artificial rewards introduces human error, sub-optimal behavior, and a greater propensity for reward hacking. In this paper, we mitigate this by representing objectives as automata in order to define novel reward shaping functions over this structured representation. In doing so, we address the sparse rewards problem within a novel implementation of Monte Carlo Tree Search (MCTS) by proposing a reward shaping function which is updated dynamically to capture statistics on the utility of each automaton transition as it pertains to satisfying the goal of the agent. We further demonstrate that such automaton-guided reward shaping can be utilized to facilitate transfer learning between different environments when the objective is the same.},
  issue = {13},
  langid = {english},
  keywords = {Neuro-Symbolic AI (NSAI)},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/velasquezDynamicAutomatonGuidedReward2021.pdf}
}

@article{villaniSurveyHumanRobot2018,
  title = {Survey on Human–Robot Collaboration in Industrial Settings: {{Safety}}, Intuitive Interfaces and Applications},
  shorttitle = {Survey on Human–Robot Collaboration in Industrial Settings},
  author = {Villani, Valeria and Pini, Fabio and Leali, Francesco and Secchi, Cristian},
  date = {2018-11},
  journaltitle = {Mechatronics},
  shortjournal = {Mechatronics},
  volume = {55},
  pages = {248--266},
  issn = {09574158},
  doi = {10.1016/j.mechatronics.2018.02.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957415818300321},
  urldate = {2024-01-23},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/villaniSurveyHumanRobot2018.pdf}
}

@online{viswanathOFFSEGSemanticSegmentation2021,
  title = {{{OFFSEG}}: {{A Semantic Segmentation Framework For Off-Road Driving}}},
  shorttitle = {{{OFFSEG}}},
  author = {Viswanath, Kasi and Singh, Kartikeya and Jiang, Peng and B., Sujit P. and Saripalli, Srikanth},
  date = {2021-03-23},
  eprint = {2103.12417},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.12417},
  urldate = {2024-02-22},
  abstract = {Off-road image semantic segmentation is challenging due to the presence of uneven terrains, unstructured class boundaries, irregular features and strong textures. These aspects affect the perception of the vehicle from which the information is used for path planning. Current off-road datasets exhibit difficulties like class imbalance and understanding of varying environmental topography. To overcome these issues we propose a framework for off-road semantic segmentation called as OFFSEG that involves (i) a pooled class semantic segmentation with four classes (sky, traversable region, non-traversable region and obstacle) using state-of-the-art deep learning architectures (ii) a colour segmentation methodology to segment out specific sub-classes (grass, puddle, dirt, gravel, etc.) from the traversable region for better scene understanding. The evaluation of the framework is carried out on two off-road driving datasets, namely, RELLIS-3D and RUGD. We have also tested proposed framework in IISERB campus frames. The results show that OFFSEG achieves good performance and also provides detailed information on the traversable region.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/viswanathOFFSEGSemanticSegmentation2021.pdf;/Users/akhilnagariya/Zotero/storage/5TGLZFSL/2103.html}
}

@article{wangDETR3D3DObject2021,
  title = {{{DETR3D}}: {{3D Object Detection}} from {{Multi-view Images}} via {{3D-to-2D Queries}}},
  shorttitle = {{{DETR3D}}},
  author = {Wang, Yue and Guizilini, Vitor and Zhang, Tianyuan and Wang, Yilun and Zhao, Hang and Solomon, Justin},
  date = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2110.06922},
  url = {https://arxiv.org/abs/2110.06922},
  urldate = {2023-04-20},
  abstract = {We introduce a framework for multi-camera 3D object detection. In contrast to existing works, which estimate 3D bounding boxes directly from monocular images or use depth prediction networks to generate input for 3D object detection from 2D information, our method manipulates predictions directly in 3D space. Our architecture extracts 2D features from multiple camera images and then uses a sparse set of 3D object queries to index into these 2D features, linking 3D positions to multi-view images using camera transformation matrices. Finally, our model makes a bounding box prediction per object query, using a set-to-set loss to measure the discrepancy between the ground-truth and the prediction. This top-down approach outperforms its bottom-up counterpart in which object bounding box prediction follows per-pixel depth estimation, since it does not suffer from the compounding error introduced by a depth prediction model. Moreover, our method does not require post-processing such as non-maximum suppression, dramatically improving inference speed. We achieve state-of-the-art performance on the nuScenes autonomous driving benchmark.},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Robotics (cs.RO)},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/wangDETR3D3DObject2021.pdf}
}

@inproceedings{wangFCOS3DFullyConvolutional2021,
  title = {{{FCOS3D}}: {{Fully Convolutional One-Stage Monocular 3D Object Detection}}},
  shorttitle = {{{FCOS3D}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Wang, Tai and Zhu, Xinge and Pang, Jiangmiao and Lin, Dahua},
  date = {2021-10},
  pages = {913--922},
  publisher = {IEEE},
  location = {Montreal, BC, Canada},
  doi = {10.1109/ICCVW54120.2021.00107},
  url = {https://ieeexplore.ieee.org/document/9607436/},
  urldate = {2023-04-20},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshops}} ({{ICCVW}})},
  isbn = {978-1-66540-191-3},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/wangFCOS3DFullyConvolutional2021.pdf}
}

@article{wangMonocular3DObject2022,
  title = {Monocular {{3D Object Detection}} with {{Depth}} from {{Motion}}},
  author = {Wang, Tai and Pang, Jiangmiao and Lin, Dahua},
  date = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2207.12988},
  url = {https://arxiv.org/abs/2207.12988},
  urldate = {2023-04-18},
  abstract = {Perceiving 3D objects from monocular inputs is crucial for robotic systems, given its economy compared to multi-sensor settings. It is notably difficult as a single image can not provide any clues for predicting absolute depth values. Motivated by binocular methods for 3D object detection, we take advantage of the strong geometry structure provided by camera ego-motion for accurate object depth estimation and detection. We first make a theoretical analysis on this general two-view case and notice two challenges: 1) Cumulative errors from multiple estimations that make the direct prediction intractable; 2) Inherent dilemmas caused by static cameras and matching ambiguity. Accordingly, we establish the stereo correspondence with a geometry-aware cost volume as the alternative for depth estimation and further compensate it with monocular understanding to address the second problem. Our framework, named Depth from Motion (DfM), then uses the established geometry to lift 2D image features to the 3D space and detects 3D objects thereon. We also present a pose-free DfM to make it usable when the camera pose is unavailable. Our framework outperforms state-of-the-art methods by a large margin on the KITTI benchmark. Detailed quantitative and qualitative analyses also validate our theoretical conclusions. The code will be released at https://github.com/Tai-Wang/Depth-from-Motion.},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Robotics (cs.RO)}
}

@article{wangMonocular3DObject2022a,
  title = {Monocular {{3D Object Detection}} with {{Depth}} from {{Motion}}},
  author = {Wang, Tai and Pang, Jiangmiao and Lin, Dahua},
  date = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2207.12988},
  url = {https://arxiv.org/abs/2207.12988},
  urldate = {2023-04-18},
  abstract = {Perceiving 3D objects from monocular inputs is crucial for robotic systems, given its economy compared to multi-sensor settings. It is notably difficult as a single image can not provide any clues for predicting absolute depth values. Motivated by binocular methods for 3D object detection, we take advantage of the strong geometry structure provided by camera ego-motion for accurate object depth estimation and detection. We first make a theoretical analysis on this general two-view case and notice two challenges: 1) Cumulative errors from multiple estimations that make the direct prediction intractable; 2) Inherent dilemmas caused by static cameras and matching ambiguity. Accordingly, we establish the stereo correspondence with a geometry-aware cost volume as the alternative for depth estimation and further compensate it with monocular understanding to address the second problem. Our framework, named Depth from Motion (DfM), then uses the established geometry to lift 2D image features to the 3D space and detects 3D objects thereon. We also present a pose-free DfM to make it usable when the camera pose is unavailable. Our framework outperforms state-of-the-art methods by a large margin on the KITTI benchmark. Detailed quantitative and qualitative analyses also validate our theoretical conclusions. The code will be released at https://github.com/Tai-Wang/Depth-from-Motion.},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Robotics (cs.RO)},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/wangMonocular3DObject2022.pdf}
}

@article{wangPseudoLiDARVisualDepth2019,
  title = {Pseudo-{{LiDAR From Visual Depth Estimation}}: {{Bridging}} the {{Gap}} in {{3D Object Detection}} for {{Autonomous Driving}}},
  shorttitle = {Pseudo-{{LiDAR From Visual Depth Estimation}}},
  author = {Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
  date = {2019-06},
  journaltitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {8437--8445},
  publisher = {IEEE},
  location = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.00864},
  url = {https://ieeexplore.ieee.org/document/8954293/},
  urldate = {2023-04-20},
  abstract = {3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies --- a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that it is not the quality of the data but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations --- essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance --- raising the detection accuracy of objects within the 30m range from the previous state-of-the-art of 22\% to an unprecedented 74\%. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo-image-based approaches.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {9781728132938},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/wangPseudoLiDARVisualDepth2019.pdf}
}

@online{weberImaginationAugmentedAgentsDeep2018,
  title = {Imagination-{{Augmented Agents}} for {{Deep Reinforcement Learning}}},
  author = {Weber, Théophane and Racanière, Sébastien and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo Jimenez and Badia, Adria Puigdomènech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
  date = {2018-02-14},
  eprint = {1707.06203},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1707.06203},
  url = {http://arxiv.org/abs/1707.06203},
  urldate = {2023-08-08},
  abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/weberImaginationAugmentedAgentsDeep2018.pdf;/Users/akhilnagariya/Zotero/storage/FLB6GNIU/1707.html}
}

@inproceedings{westonProbablyUnknownDeep2019,
  title = {Probably {{Unknown}}: {{Deep Inverse Sensor Modelling Radar}}},
  shorttitle = {Probably {{Unknown}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Weston, Rob and Cen, Sarah and Newman, Paul and Posner, Ingmar},
  date = {2019-05},
  pages = {5446--5452},
  publisher = {IEEE},
  location = {Montreal, QC, Canada},
  doi = {10.1109/ICRA.2019.8793263},
  url = {https://ieeexplore.ieee.org/document/8793263/},
  urldate = {2023-04-13},
  eventtitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-5386-6027-0},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/westonProbablyUnknownDeep2019.pdf}
}

@article{whitneyUnderstandingAsymptoticPerformance2018,
  title = {Understanding the {{Asymptotic Performance}} of {{Model-Based RL Methods}}},
  author = {Whitney, William and Fergus, Rob},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=B1g29oAqtm},
  urldate = {2023-08-08},
  abstract = {In complex simulated environments, model-based reinforcement learning methods typically lag the asymptotic performance of model-free approaches. This paper uses two MuJoCo environments to understand this gap through a series of ablation experiments designed to separate the contributions of the dynamics model and planner. These reveal the importance of long planning horizons, beyond those typically used. A dynamics model that directly predicts distant states, based on current state and a long sequence of actions, is introduced. This avoids the need for many recursions during long-range planning, and thus is able to yield more accurate state estimates. These accurate predictions allow us to uncover the relationship between model accuracy and performance, and translate to higher task reward that matches or exceeds current state-of-the-art model-free approaches.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/whitneyUnderstandingAsymptoticPerformance2018.pdf}
}

@online{xiaGibsonEnvRealWorld2018,
  title = {Gibson {{Env}}: {{Real-World Perception}} for {{Embodied Agents}}},
  shorttitle = {Gibson {{Env}}},
  author = {Xia, Fei and Zamir, Amir and He, Zhi-Yang and Sax, Alexander and Malik, Jitendra and Savarese, Silvio},
  date = {2018-08-31},
  eprint = {1808.10654},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1808.10654},
  urldate = {2023-11-08},
  abstract = {Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, "Goggles", enabling deploying the trained models in real-world without needing further domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/xiaGibsonEnvRealWorld2018.pdf;/Users/akhilnagariya/Zotero/storage/NJ3ZGQY3/1808.html}
}

@online{xiaoThinkingMovingDeep2020,
  title = {Thinking {{While Moving}}: {{Deep Reinforcement Learning}} with {{Concurrent Control}}},
  shorttitle = {Thinking {{While Moving}}},
  author = {Xiao, Ted and Jang, Eric and Kalashnikov, Dmitry and Levine, Sergey and Ibarz, Julian and Hausman, Karol and Herzog, Alexander},
  date = {2020-04-25},
  eprint = {2004.06089},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2004.06089},
  urldate = {2023-10-28},
  abstract = {We study reinforcement learning in settings where sampling an action from the policy must be done concurrently with the time evolution of the controlled system, such as when a robot must decide on the next action while still performing the previous action. Much like a person or an animal, the robot must think and move at the same time, deciding on its next action before the previous one has completed. In order to develop an algorithmic framework for such concurrent control problems, we start with a continuous-time formulation of the Bellman equations, and then discretize them in a way that is aware of system delays. We instantiate this new class of approximate dynamic programming methods via a simple architectural extension to existing value-based deep reinforcement learning algorithms. We evaluate our methods on simulated benchmark tasks and a large-scale robotic grasping task where the robot must "think while moving".},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,I.2.9,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/xiaoThinkingMovingDeep2020.pdf;/Users/akhilnagariya/Zotero/storage/I9FBDXKQ/2004.html}
}

@article{xieBEVMultiCameraJoint2022,
  title = {M\$\^{}2\${{BEV}}: {{Multi-Camera Joint 3D Detection}} and {{Segmentation}} with {{Unified Birds-Eye View Representation}}},
  shorttitle = {M\$\^{}2\${{BEV}}},
  author = {Xie, Enze and Yu, Zhiding and Zhou, Daquan and Philion, Jonah and Anandkumar, Anima and Fidler, Sanja and Luo, Ping and Alvarez, Jose M.},
  date = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2204.05088},
  url = {https://arxiv.org/abs/2204.05088},
  urldate = {2023-04-20},
  abstract = {In this paper, we propose M\$\^{}2\$BEV, a unified framework that jointly performs 3D object detection and map segmentation in the Birds Eye View\textasciitilde (BEV) space with multi-camera image inputs. Unlike the majority of previous works which separately process detection and segmentation, M\$\^{}2\$BEV infers both tasks with a unified model and improves efficiency. M\$\^{}2\$BEV efficiently transforms multi-view 2D image features into the 3D BEV feature in ego-car coordinates. Such BEV representation is important as it enables different tasks to share a single encoder. Our framework further contains four important designs that benefit both accuracy and efficiency: (1) An efficient BEV encoder design that reduces the spatial dimension of a voxel feature map. (2) A dynamic box assignment strategy that uses learning-to-match to assign ground-truth 3D boxes with anchors. (3) A BEV centerness re-weighting that reinforces with larger weights for more distant predictions, and (4) Large-scale 2D detection pre-training and auxiliary supervision. We show that these designs significantly benefit the ill-posed camera-based 3D perception tasks where depth information is missing. M\$\^{}2\$BEV is memory efficient, allowing significantly higher resolution images as input, with faster inference speed. Experiments on nuScenes show that M\$\^{}2\$BEV achieves state-of-the-art results in both 3D object detection and BEV segmentation, with the best single model achieving 42.5 mAP and 57.0 mIoU in these two tasks, respectively.},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/xieBEVMultiCameraJoint2022.pdf}
}

@online{xieModelbasedReinforcementLearning2016,
  title = {Model-Based {{Reinforcement Learning}} with {{Parametrized Physical Models}} and {{Optimism-Driven Exploration}}},
  author = {Xie, Christopher and Patil, Sachin and Moldovan, Teodor and Levine, Sergey and Abbeel, Pieter},
  date = {2016-03-15},
  eprint = {1509.06824},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1509.06824},
  url = {http://arxiv.org/abs/1509.06824},
  urldate = {2023-08-06},
  abstract = {In this paper, we present a robotic model-based reinforcement learning method that combines ideas from model identification and model predictive control. We use a feature-based representation of the dynamics that allows the dynamics model to be fitted with a simple least squares procedure, and the features are identified from a high-level specification of the robot's morphology, consisting of the number and connectivity structure of its links. Model predictive control is then used to choose the actions under an optimistic model of the dynamics, which produces an efficient and goal-directed exploration strategy. We present real time experimental results on standard benchmark problems involving the pendulum, cartpole, and double pendulum systems. Experiments indicate that our method is able to learn a range of benchmark tasks substantially faster than the previous best methods. To evaluate our approach on a realistic robotic control task, we also demonstrate real time control of a simulated 7 degree of freedom arm.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/xieModelbasedReinforcementLearning2016.pdf;/Users/akhilnagariya/Zotero/storage/N33H8JVT/1509.html}
}

@online{xiongNaturalLanguageBased2023,
  title = {Natural {{Language}} Based {{Context Modeling}} and {{Reasoning}} with {{LLMs}}: {{A Tutorial}}},
  shorttitle = {Natural {{Language}} Based {{Context Modeling}} and {{Reasoning}} with {{LLMs}}},
  author = {Xiong, Haoyi and Bian, Jiang and Yang, Sijia and Zhang, Xiaofei and Kong, Linghe and Zhang, Daqing},
  date = {2023-09-23},
  eprint = {2309.15074},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.15074},
  urldate = {2023-11-07},
  abstract = {Large language models (LLMs) have become phenomenally surging, since 2018--two decades after introducing context-awareness into computing systems. Through taking into account the situations of ubiquitous devices, users and the societies, context-aware computing has enabled a wide spectrum of innovative applications, such as assisted living, location-based social network services and so on. To recognize contexts and make decisions for actions accordingly, various artificial intelligence technologies, such as Ontology and OWL, have been adopted as representations for context modeling and reasoning. Recently, with the rise of LLMs and their improved natural language understanding and reasoning capabilities, it has become feasible to model contexts using natural language and perform context reasoning by interacting with LLMs such as ChatGPT and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and autonomous agents (AutoAgents) that enable LLMs to perform context modeling and reasoning without requiring fine-tuning of the model. We organize and introduce works in the related field, and name this computing paradigm as the LLM-driven Context-aware Computing (LCaC). In the LCaC paradigm, users' requests, sensors reading data, and the command to actuators are supposed to be represented as texts. Given the text of users' request and sensor data, the AutoAgent models the context by prompting and sends to the LLM for context reasoning. LLM generates a plan of actions and responds to the AutoAgent, which later follows the action plan to foster context-awareness. To prove the concepts, we use two showcases--(1) operating a mobile z-arm in an apartment for assisted living, and (2) planning a trip and scheduling the itinerary in a context-aware and personalized manner.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Networking and Internet Architecture},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/xiongNaturalLanguageBased2023.pdf;/Users/akhilnagariya/Zotero/storage/SKJQKQCK/2309.html}
}

@online{xiRisePotentialLarge2023,
  title = {The {{Rise}} and {{Potential}} of {{Large Language Model Based Agents}}: {{A Survey}}},
  shorttitle = {The {{Rise}} and {{Potential}} of {{Large Language Model Based Agents}}},
  author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and Gui, Tao},
  date = {2023-09-19},
  eprint = {2309.07864},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.07864},
  urldate = {2023-11-07},
  abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/xiRisePotentialLarge2023.pdf;/Users/akhilnagariya/Zotero/storage/PWD23HWJ/2309.html}
}

@inproceedings{xuUniversalDomainAdaptation2022,
  title = {Universal {{Domain Adaptation}} without {{Source Data}} for {{Remote Sensing Image Scene Classification}}},
  booktitle = {{{IGARSS}} 2022 - 2022 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Xu, Qingsong and Shi, Yilei and Zhu, Xiaoxiang},
  date = {2022-07},
  pages = {5341--5344},
  issn = {2153-7003},
  doi = {10.1109/IGARSS46834.2022.9884889},
  url = {https://ieeexplore.ieee.org/document/9884889},
  urldate = {2023-10-05},
  abstract = {Existing domain adaptation (DA) approaches are usually not well suited for practical DA scenarios of remote sensing image classification, since these methods (such as unsupervised DA) rely on rich prior knowledge about the relationship between label sets of source and target domains, and source data are usually not accessible in many cases due to the privacy or confidentiality issues. To this end, we propose a novel source data generation-based universal domain adaptation (SDG-UniDA) model, which includes two parts, i.e., the stage of source data generation and the stage of model adaptation. The first stage is to estimate the conditional distribution of source data from the pre-trained model using the knowledge of class-separability in the source domain and then to synthesize the source data. With this synthetic source data in hand, it becomes a universal DA task that requires no prior knowledge on the label sets. A novel transferable weight is proposed to distinguish the shared and private label sets to each domain, thereby promoting the adaptation in the automatically discovered shared label set and recognizing the “unknown” samples successfully. Empirical results show that SDG-UniDA is effective and practical in this challenging setting for remote sensing image scene classification.},
  eventtitle = {{{IGARSS}} 2022 - 2022 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/xuUniversalDomainAdaptation2022.pdf;/Users/akhilnagariya/Zotero/storage/TTNZQTNG/9884889.html}
}

@article{yanSECONDSparselyEmbedded2018,
  title = {{{SECOND}}: {{Sparsely Embedded Convolutional Detection}}},
  shorttitle = {{{SECOND}}},
  author = {Yan, Yan and Mao, Yuxing and Li, Bo},
  date = {2018-10-06},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {18},
  number = {10},
  pages = {3337},
  issn = {1424-8220},
  doi = {10.3390/s18103337},
  url = {http://www.mdpi.com/1424-8220/18/10/3337},
  urldate = {2023-04-20},
  abstract = {LiDAR-based or RGB-D-based object detection is used in numerous applications, ranging from autonomous driving to robot vision. Voxel-based 3D convolutional networks have been used for some time to enhance the retention of information when processing point cloud LiDAR data. However, problems remain, including a slow inference speed and low orientation estimation performance. We therefore investigate an improved sparse convolution method for such networks, which significantly increases the speed of both training and inference. We also introduce a new form of angle loss regression to improve the orientation estimation performance and a new data augmentation approach that can enhance the convergence speed and performance. The proposed network produces state-of-the-art results on the KITTI 3D object detection benchmarks while maintaining a fast inference speed.},
  langid = {english},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/yanSECONDSparselyEmbedded2018.pdf}
}

@inproceedings{yinCenterbased3DObject2021,
  title = {Center-Based {{3D Object Detection}} and {{Tracking}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yin, Tianwei and Zhou, Xingyi and Krahenbuhl, Philipp},
  date = {2021-06},
  pages = {11779--11788},
  publisher = {IEEE},
  location = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.01161},
  url = {https://ieeexplore.ieee.org/document/9578166/},
  urldate = {2023-04-20},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/yinCenterbased3DObject2021.pdf}
}

@inproceedings{yoonLearningbasedInitializationTrajectory2023,
  title = {Learning-Based {{Initialization}} of {{Trajectory Optimization}} for {{Path-following Problems}} of {{Redundant Manipulators}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Yoon, Minsung and Kang, Mincheul and Park, Daehyung and Yoon, Sung-Eui},
  date = {2023-05-29},
  pages = {9686--9692},
  publisher = {IEEE},
  location = {London, United Kingdom},
  doi = {10.1109/ICRA48891.2023.10161426},
  url = {https://ieeexplore.ieee.org/document/10161426/},
  urldate = {2023-08-17},
  eventtitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {9798350323658},
  file = {/Users/akhilnagariya/Zotero/storage/ZLRLLPGY/yoonLearningbasedInitializationTrajectory2023.pdf}
}

@online{youPseudoLiDARAccurateDepth2020,
  title = {Pseudo-{{LiDAR}}++: {{Accurate Depth}} for {{3D Object Detection}} in {{Autonomous Driving}}},
  shorttitle = {Pseudo-{{LiDAR}}++},
  author = {You, Yurong and Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Pleiss, Geoff and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
  date = {2020-02-15},
  eprint = {1906.06310},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1906.06310},
  url = {http://arxiv.org/abs/1906.06310},
  urldate = {2023-04-20},
  abstract = {Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving. Existing approaches largely rely on expensive LiDAR sensors for accurate depth information. While recently pseudo-LiDAR has been introduced as a promising alternative, at a much lower cost based solely on stereo images, there is still a notable performance gap. In this paper we provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation. Concretely, we adapt the stereo network architecture and loss function to be more aligned with accurate depth estimation of faraway objects --- currently the primary weakness of pseudo-LiDAR. Further, we explore the idea to leverage cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias our depth estimation. We propose a depth-propagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map. We show on the KITTI object detection benchmark that our combined approach yields substantial improvements in depth estimation and stereo-based 3D object detection --- outperforming the previous state-of-the-art detection accuracy for faraway objects by 40\%. Our code is available at https://github.com/mileyan/Pseudo\_Lidar\_V2.},
  pubstate = {preprint},
  version = {3},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/youPseudoLiDARAccurateDepth2020.pdf;/Users/akhilnagariya/Zotero/storage/53BSFJHK/1906.html}
}

@online{yuMetaWorldBenchmarkEvaluation2021,
  title = {Meta-{{World}}: {{A Benchmark}} and {{Evaluation}} for {{Multi-Task}} and {{Meta Reinforcement Learning}}},
  shorttitle = {Meta-{{World}}},
  author = {Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Narayan, Avnish and Shively, Hayden and Bellathur, Adithya and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  date = {2021-06-14},
  eprint = {1910.10897},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1910.10897},
  url = {http://arxiv.org/abs/1910.10897},
  urldate = {2023-10-05},
  abstract = {Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/yuMetaWorldBenchmarkEvaluation2021.pdf;/Users/akhilnagariya/Zotero/storage/Z2L9UT36/1910.html}
}

@online{zhangInteractiveNavigationEnvironments2023,
  title = {Interactive {{Navigation}} in {{Environments}} with {{Traversable Obstacles Using Large Language}} and {{Vision-Language Models}}},
  author = {Zhang, Zhen and Lin, Anran and Wong, Chun Wai and Chu, Xiangyu and Dou, Qi and Au, K. W. Samuel},
  date = {2023-10-13},
  eprint = {2310.08873},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.08873},
  urldate = {2023-11-07},
  abstract = {This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like "Can you pass through the curtains to deliver medicines to me?", to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable objects such as curtains and grasses for verification by instructing the robot to traverse them. Besides, traversing curtains in a medical scenario was tested. All experimental results demonstrated the proposed framework's effectiveness and adaptability to diverse environments.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/zhangInteractiveNavigationEnvironments2023.pdf;/Users/akhilnagariya/Zotero/storage/GAQXYQ3B/2310.html}
}

@article{zhangScanDenoisingNormal2023,
  title = {Scan {{Denoising}} and {{Normal Distribution Transform}} for {{Accurate Radar Odometry}} and {{Positioning}}},
  author = {Zhang, Rongxi and Zhang, Yuanhui and Fu, Duo and Liu, Kang},
  date = {2023-03},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {8},
  number = {3},
  pages = {1199--1206},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2023.3236570},
  url = {https://ieeexplore.ieee.org/document/10015883/},
  urldate = {2023-04-12},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/zhangScanDenoisingNormal2023.pdf}
}

@inproceedings{zhangSmartRainNetUncertaintyEstimation2023,
  title = {{{SmartRainNet}}: {{Uncertainty Estimation For Laser Measurement}} in {{Rain}}},
  shorttitle = {{{SmartRainNet}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Zhang, Chen and Huang, Zefan and Lin Tung, Beatrix Xue and Ang, Marcelo H. and Rus, Daniela},
  date = {2023-05},
  pages = {10567--10573},
  doi = {10.1109/ICRA48891.2023.10160874},
  abstract = {Adverse weather has raised a big challenge for autonomous vehicles. Unreliable measurements due to sensor degradation could seriously affect the performance of autonomous driving tasks, such as perception and localization. In this work, we study sensor degradation in rainy weather and present a novel method that evaluates the uncertainty for each laser measurement from a 3D LiDAR. With uncertainty estimation, downstream tasks that rely on LiDAR input (e.g., perception or localization) can increase their reliability by adjusting their reliance on laser measurements with varying fidelity. Alternatively, uncertainty estimation can be used for sensor performance evaluation. Our proposed method, SmartRainNet, uses an attention-based Mixture Density Network to model the dependence between neighboring laser measurements and then calculate the probability density for each laser measurement as an uncertainty score. We evaluate SmartRainNet on synthetic and naturalistic sensor degradation datasets and provide qualitative and quantitative results to demonstrate the effectiveness of our method in evaluating uncertainty. Finally, we demonstrate three practical applications of uncertainty estimation to address autonomous driving challenges in rainy weather.},
  eventtitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Degradation,Density measurement,Estimation,Measurement by laser beam,Measurement uncertainty,Robot sensing systems,Uncertainty},
  file = {/Users/akhilnagariya/Zotero/storage/I3JD8GFJ/zhangSmartRainNetUncertaintyEstimation2023.pdf;/Users/akhilnagariya/Zotero/storage/MD5IC8QP/10160874.html}
}

@inproceedings{zhouVoxelNetEndtoEndLearning2018,
  title = {{{VoxelNet}}: {{End-to-End Learning}} for {{Point Cloud Based 3D Object Detection}}},
  shorttitle = {{{VoxelNet}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhou, Yin and Tuzel, Oncel},
  date = {2018-06},
  pages = {4490--4499},
  publisher = {IEEE},
  location = {Salt Lake City, UT, USA},
  doi = {10.1109/CVPR.2018.00472},
  url = {https://ieeexplore.ieee.org/document/8578570/},
  urldate = {2023-04-20},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  file = {/Users/akhilnagariya/bibliography/bibtex-pdf/zhouVoxelNetEndtoEndLearning2018.pdf}
}
