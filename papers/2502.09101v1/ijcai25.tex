%%%% ijcai25.tex

\typeout{Bridging the Gap Between LLMs and Human Intentions: Progresses and Challenges in Instruction Understanding, Intention Reasoning, and Reliable Generation}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\urlstyle{same}
\usepackage{blindtext}
\usepackage{makecell}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{color}
\usepackage{tikz}
\usepackage[edges]{forest}
\usepackage{multirow}
\definecolor{hidden-draw}{RGB}{205, 44, 36}
\definecolor{hidden-blue}{RGB}{194,232,247}
\definecolor{hidden-orange}{RGB}{243,202,120}
\definecolor{hidden-yellow}{RGB}{242,244,193}
\usepackage{cleveref}
\usepackage{tcolorbox}

\usepackage{xcolor}
\usepackage{ifthen}
\usepackage[normalem]{ulem}

\definecolor{myorange}{HTML}{FEAE03}
\definecolor{myturquois}{HTML}{01AB8F}
\definecolor{mypink}{HTML}{D31876}

\definecolor{brightred}{HTML}{E55347} % Darker Tomato
\definecolor{orange}{HTML}{FF8C00} % Dark Orange
% \definecolor{gold}{HTML}{FFD700} % Gold, kept as is for distinction
\definecolor{yellowgreen}{HTML}{6B8E23} % Olive Drab, darker for better contrast
\definecolor{green}{HTML}{228B22} % Forest Green, darker and richer
\newtcolorbox{bluebox}[1][]{
	float,
  	title=#1,
	colback=myturquois!4,
	colframe=myturquois,
        top=1pt,           % 控制顶部空白
        bottom=1pt,        % 控制底部空白
        left=0pt,          % 控制左边空白
        right=0pt,          % 控制右边空白
        % before skip=0pt,        % 与前一段之间的距离
        % after skip=0pt,          % 与后一段之间的距离
        before skip=0.65em, after skip=0.75em,
}

\newcommand{\hypbox}[2]{%
\begin{tcolorbox}[colback=white!98!black,colframe=white!30!black,boxsep=1.1pt,top=6.75pt]%
\vspace{1.75pt}%
\textbf{#1}\\[-0.575em]
\noindent\makebox[\textwidth]{\rule{\textwidth}{0.4pt}}
\\[0.25em]
#2
\end{tcolorbox}
}


\definecolor{darkgrey}{rgb}{0.53,0.53,0.53}
\definecolor{mygrey}{rgb}{0.9,0.9,0.9}
\definecolor{color1}{HTML}{006EB8}
% \hypersetup{
%   colorlinks   = true,
%   urlcolor     = black,
%   linkcolor    = black,
%   citecolor   = black
% }


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


% tree
\usepackage{tikz}
\usepackage[edges]{forest}
% \usepackage{natbib}
\usetikzlibrary{shapes, arrows.meta, positioning}
\usepackage{graphicx}
\usepackage{forest}
\usetikzlibrary{trees,positioning,shapes,shadows,arrows.meta}


\definecolor{myblue}{RGB}{159, 192, 230}
\definecolor{myblueline}{RGB}{87, 127, 185}
\definecolor{bluelight1}{RGB}{185, 211, 237}
\definecolor{bluelight2}{RGB}{213, 222, 239}
\definecolor{mygreen}{RGB}{168, 209, 201}
% \definecolor{mygreen}{rgb}{0.56, 0.93, 0.56}
\definecolor{greenlight}{RGB}{220, 235, 234}
% \definecolor{hidden-draw}{RGB}{205, 44, 36}
\definecolor{hidden-draw}{RGB}{177, 177, 177}
\definecolor{mygray}{RGB}{185, 185, 185}







\definecolor{lightcoral}{rgb}{0.94, 0.5, 0.5}
\definecolor{lightgreen}{rgb}{0.56, 0.93, 0.56}
\definecolor{harvestgold}{rgb}{0.98, 0.85, 0.40}
\definecolor{brightlavender}{rgb}{0.75, 0.58, 0.89}
\definecolor{capri}{rgb}{0.0, 0.75, 1.0}
\definecolor{carminepink}{rgb}{0.92, 0.3, 0.26}
\definecolor{celadon}{rgb}{0.67, 0.88, 0.69}
\definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24}
\definecolor{lightgoldenrodyellow}{rgb}{0.98, 0.98, 0.82}
\definecolor{jonquil}{rgb}{0.98, 0.85, 0.37}
\definecolor{lightkhaki}{rgb}{0.94, 0.9, 0.55}
\definecolor{lemonchiffon}{rgb}{1.0, 0.98, 0.8}
\definecolor{schoolbusyellow}{rgb}{1.0, 0.85, 0.0}
% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Bridging the Gap Between LLMs and Human Intentions: \\Progresses and Challenges in Instruction Understanding, Intention Reasoning, and Reliable Generation}


% Single author syntax
\author{
    Author Name
    \affiliations
    Affiliation
    \emails
    email@example.com
}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)

\author{
Zongyu Chang$^1*$
\and
Feihong Lu$^2*$\and
Ziqin Zhu$^2$\and
Qian Li$^1$ \and
Cheng Ji$^2$ \and
Zhuo Chen$^1$ \and
Yang Liu$^5$ \and \\
Ruifeng Xu$^3$ \and
Yangqiu Song$^4$ \and
Shangguang Wang$^1$\And
Jianxin Li$^2$\\
\affiliations
$^1$ Beijing University of Posts and Telecommunications\\
$^2$Beihang University
$^3$Harbin Institute of Technology, Shenzhen\\
$^4$Hong Kong University of Science and Technology
$^5$Chinese Academy of Sciences\\
\emails
\{changzongyu, li.qian, sgwang\}@bupt.edu.cn,
\{lufeihong, zhuziqin, jicheng, lijx\}@buaa.edu.cn,
xuruifeng@hit.edu.cn,
yqsong@cse.ust.hk
\thanks{These authors contributed equally to this work.}
}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) have demonstrated exceptional capabilities in understanding and generation.
% , driving their widespread adoption across various domains. 
However, when interacting with human instructions in real-world scenarios, LLMs still face significant challenges, particularly in accurately capturing and comprehending human instructions and intentions. 
This paper focuses on three challenges in LLM-based text generation tasks: instruction understanding, intention reasoning, and reliable generation. 
Regarding human complex instruction, LLMs have deficiencies in understanding long contexts and instructions in multi-round conversations. For intention reasoning, LLMs may have inconsistent command reasoning, difficulty reasoning about commands containing incorrect information, difficulty understanding user ambiguous language commands, and a weak understanding of user intention in commands. Besides, In terms of reliable generation, LLMs may have unstable generated content and unethical generation. To this end, we classify and analyze the performance of LLMs in challenging scenarios and conduct a comprehensive evaluation of existing solutions.
Furthermore, we introduce benchmarks and categorize them based on the aforementioned three core challenges. 
Finally, we explore potential directions for future research to enhance the reliability and adaptability of LLMs in real-world applications.

\end{abstract}


\section{Introduction}

Rapid advancements with the development of large language models (LLMs) have been experienced in the field of artificial intelligence. 
These models, built upon massive amounts of data and extensive computing resources, have shown impressive capabilities in understanding and generating human language. 
Recent advancements in LLMs, including the use of scaling laws~\cite{kaplan2020scaling}, supervised fine-tuning (SFT)~\cite{wu2021recursively}, and reinforcement learning with human feedback (RLHF)~\cite{ouyang2022training}, have propelled these models to new heights. Researchers have explored innovative strategies like chain-of-thought reasoning (COT)~\cite{wei2022chain}, aiming to enhance their performance in processing and generating accurate responses. 
However, they still struggle with more complex interactions, especially when the input data is ambiguous, incomplete, or inconsistent. Despite improvements, issues such as content hallucination~\cite{DBLP:conf/emnlp/LiCZNW23} and logical misinterpretations remain prevalent. Consequently, while LLMs show promise, they are far from flawless and require further refinement to address the challenges posed by more unpredictable and complex human instructions as follows.


\begin{figure}[t]
  \centering
  \setlength{\belowcaptionskip}{-0.4cm}
  \includegraphics[width=1
  \linewidth]{picture/figure1.pdf}
  \caption{Example of large language model generation.
  }\label{fig:intro}
  % \vspace{-4mm}
\end{figure}

\textbf{I. Challenge of Instruction Understanding.}
One of the most pressing challenges that LLMs face is instruction understanding as \Cref{fig:intro}(a), particularly when the user input involves complex or multi-step instructions. While models have improved in parsing relatively simple queries, they continue to encounter significant difficulties when dealing with long, context-rich instructions or when instructions are spread across multiple conversational turns. LLMs often fail to grasp subtle nuances or interpret implicit meanings embedded within the text, which leads to inaccurate or incomplete responses. Existing approaches to instruction understanding have introduced techniques like optimizing the model's parsing abilities~\cite{teng2024fine}, and context-aware optimization~\cite{sun2024parrot}. While these methods show promise, they often fall short when addressing the complexities and ambiguities present in real-world instructions.


\textbf{II. Challenge of Intention Reasoning.}
Another critical area is intention reasoning as illustrated in \Cref{fig:intro}(b), where they struggle to align the generated responses with the user’s underlying intention. Ambiguities in language, conflicting instructions, and implicit requirements often result in models producing outputs that diverge from the user's expectations. LLMs also face difficulties when instructions are inconsistent or contain incorrect information, which challenges the model’s ability to make accurate inferences. Various strategies, including retrieval-enhanced generation and fine-tuning techniques, have been proposed to enhance reasoning capabilities, enabling the models to better handle inconsistent or incomplete instructions. However, these methods often introduce new challenges related to bias and the inability to fully resolve conflicts in user input, further complicating the alignment between generated content and user expectations.


\textbf{III. Challenge of Reliable Generation.}
The final major challenge is the reliable generation, which pertains to the accuracy, ethical considerations, and stability of the content they produce, such as \Cref{fig:intro}(c). While LLMs are generally capable of generating coherent and contextually relevant outputs, they sometimes exhibit instability, generating content that is factually incorrect, logically inconsistent, or ethically questionable. This challenge is exacerbated by the model’s inability to recognize uncertainty, which can lead to overconfident but inaccurate outputs. Recent efforts to address this issue involve techniques like uncertainty-aware fine-tuning and using external tools to evaluate output credibility. However, these approaches struggle to provide a comprehensive and reliable solution, especially in complex or dynamic contexts.



\input{taxonomy}

\paragraph{Present Survey.}
Facing these challenges, there is an increasing need for focused research on LLMs and their interaction with human instructions and intentions. This paper systematically analyzes LLMs' performance in processing human instructions, highlighting three key areas: user instruction understanding, intention comprehension and reasoning, and reliable content generation. While existing review papers address model training, fine-tuning, and specific aspects of LLMs' capabilities~\cite{lou2024large,plaat2024reasoning,huang2024survey}, our focus is on the LLMs' ability to understand and reason about user intentions. Specifically, we explore how well LLMs understand user input, reason about the user's intention, and generate content that aligns with human intentions, minimizing hallucinations and ensuring credibility and ethical consistency.



\paragraph{Comparison with Previous Surveys.} 
While the gap between human intention and LLMs is a core challenge in generative AI, many studies focus on specific aspects of the issue, lacking a comprehensive overview. These works offer valuable insights but do not provide a systematic summary of the field. Lou et al.~\cite{lou2024large} primarily address instruction following challenges in LLMs without delving into the reasoning capabilities for complex user instructions. Gao et al. analyze the four stages of human-machine LLM interaction (planning, facilitation, iteration, and testing) but overlook LLM's understanding of user instructions. Xu et al.~\cite{xu2024knowledge} examine the impact of various memory conflicts on LLM-generated content credibility and performance, yet do not consider reasoning or intention comprehension. Plaat et al.~\cite{plaat2024reasoning} focus on LLM Reasoning for basic mathematical problems, without exploring its applicability to broader fields. Shorinwa et al.~\cite{shorinwa2024survey} provide an initial analysis of LLMs uncertainty quantification, but exclude user input instructions. In contrast, our survey offers a more comprehensive perspective, with a unique classification and systematic analysis of instruction processing, addressing current solutions to key challenges. 



\paragraph{Survey Organization.} 
As in Figure \ref{fig:texonomy}, we begin by exploring the capability of user instruction understanding (\S\ref{sec:challenge1}). Next, we analyze intention comprehension and reasoning, focusing on how models infer implicit intentions, incorporate contextual information for logical reasoning, and address inconsistencies or incomplete instructions (\S\ref{sec:challenge2}). We then examine reliable content generation, assessing the quality and credibility of model-generated outputs (\S\ref{sec:challenge3}). Following this, we review benchmarks that address these core challenges (\S\ref{sec:Benchmark}). Finally, we propose potential research directions (\S\ref{sec:Directions}) and summarize the key findings (\S\ref{sec:Conclusion}).



\begin{figure}[h]
	% \setlength{\abovecaptionskip}{-0.2cm}
	\setlength{\belowcaptionskip}{-0.3cm}
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{1\linewidth}{0pt}}
		\includegraphics[width=1\linewidth]{picture/understanding.pdf}
	\end{center}
	\caption{Common understanding issues in LLMs include: (A) \textbf{Remote Information Failure (\S\ref{sec:long})}, where the model forgets relevant information over long distances in long context (A.1). \textbf{(B) Incorrect Relevance Judgment(\S\ref{sec:multi})}, such as the model incorrectly associates wrong content (B.1-2.nd) from the previous turn (B.1-1.st).}
	\label{fig:understanding}
\end{figure}


\section{Instruction Understanding}\label{sec:challenge1}

LLMs excel at single-turn dialogues, but struggle to understand multi-turn dialogues and long-contexts, which are commonly used by users. LLMs may forget prior information, be influenced by irrelevant data, and overlook key inputs. 
% These issues arise from the complexity of multi-round interactions, involving sparse information, variable intents, and frequent pronouns.

\subsection{Long-Text Comprehension}\label{sec:long}

Challenges remain in understanding long context and this paper classifies the factors into three categories as follows:\textbf{1) Information Sparsity and Redundancy}. Long texts often contain redundant or irrelevant information that can obscure the task-relevant content, leading to difficulties in information extraction and potential hallucinations. \textbf{2) Remote Information Failure} (Figure \ref{fig:understanding} A.1). Long contexts may cause models to forget relevant information that is distant within the text. Additionally, links between remote information across paragraphs or sentences can be difficult for models to identify, diminishing their understanding of contextual connections. \textbf{3) Attention Dilution}. As context length increases, the model’s attention mechanism faces greater computational demands and struggles to assign appropriate weights to each token, making it harder to prioritize key information, particularly with complex, multi-level relationships in longer texts. This paper classifies the existing solutions into the following two categories:

\paragraph{Information Focusing.}
Improving LLM's ability to focus on important information in long texts involves several methods: 1) Sparsifying attention to concentrate on critical information~\cite{beltagy2020longformer}. 
% 2) Dynamically adjusting attention based on the current task~\cite{wu2021smart}. 
2) Optimizing attention to minimize redundancy and emphasize core content~\cite{chen2024core}. 
3) Training with location-independent tasks to enhance the ability to search and react to relevant information in long contexts~\cite{he2024never}.

\paragraph{Multipath Optimization.}
Various methods can enhance LLMs on long-context tasks: 1) Pre-training with extended context windows and reinforcement learning for fine-tuning to optimize long-context understanding~\cite{zhang2024longreward}. 
2) Combining retrieval-based models with generative models on long-context tasks~\cite{li2024retrieval}. 
3) Leveraging cyclic sequence models' linear scaling property for better inference efficiency~\cite{gu2023mamba}. 
% 4) Using external memory to store and retrieve long-context information~\cite{liu2024memlong}. 
% 5) Mimicking brain memory hierarchies to improve long-context processing efficiency~\cite{he2024hmt}.


\subsection{Multi-Turn Conversation Handling}\label{sec:multi}

This paper categorizes the challenges faced by existing LLMs when understanding multi-turn conversations into three categories, as follows: \textbf{1) Capability Weakening.} 
% LLMs exhibit a reduced understanding of user intent in multi-turn scenarios. 
Current supervised instruction fine-tuning (SIFT) and RLHF may even impair multi-turn capabilities\cite{wang2023mint}, with models struggling on complex reasoning tasks that span multiple rounds, such as those requiring evidence collection and conclusions \cite{banatt2024wilt}. 
Additionally, multi-turn dialogs increase the vulnerability of LLMs to adversarial attacks, 
where malicious users can mask harmful intentions across multiple rounds, 
leading to the generation of misleading or harmful content \cite{agarwal2024prompt}. \textbf{2) Error Propagation.} Instruction comprehension errors accumulate across rounds, leading to an escalating failure rate in subsequent responses \cite{he2024multi}, which may snowball into larger issues such as biased or incorrect outputs \cite{fan2024fairmt}. \textbf{3) Incorrect Relevance Judgment} (Figure \ref{fig:understanding} B.1) LLMs often struggle to identify relevant content in multi-turn dialogs, failing to properly link content from previous rounds or to discern ellipsis and implicit meaning inherent in user commands \cite{sun2024parrot}.


% To solve above challenges, we categorize existing solutions into the following two types: supervised fine-tuning methods based on multi-turn dialogue data, with further improvements such as optimizing instruction parsing \cite{teng2024fine} and context-aware preference strategies \cite{sun2024parrot}. Reinforcement learning methods specifically improved for multi-turn dialogue, with targeted improvements such as hierarchical reinforcement learning strategies \cite{DBLP:conf/icml/ZhouZPLK24}.
To solve above challenges, This paper categorizes existing solutions into two types: supervised fine-tuning methods using multi-turn dialogue data, enhanced by techniques like optimized instruction parsing \cite{teng2024fine} and context-aware preference strategies \cite{sun2024parrot}; and reinforcement learning methods tailored for multi-turn dialogue, with improvements such as hierarchical reinforcement learning \cite{DBLP:conf/icml/ZhouZPLK24}.

\section{Intention Reasoning}\label{sec:challenge2}

User instructions often lack clarity due to language ambiguities. While humans can infer intention, LLMs struggle with misinterpreting ambiguous inputs, leading to errors. 
% These challenges arise from knowledge gaps and the need for models to understand underlying meanings. 
We explore causes and solutions for intention errors, focusing on inconsistent instructions, misinformation, fuzzy language, and intention clarification.

\begin{figure*}[h]
	% \setlength{\abovecaptionskip}{-0.05cm}
	\setlength{\belowcaptionskip}{-0.3cm}
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{1\linewidth}{0pt}}
		\includegraphics[width=1\linewidth]{picture/figure3.pdf}
	\end{center}
	\caption{Common reasoning issues in LLMs include: \textbf{(A) Inconsistent Instruction Reasoning (\S\ref{sec:inconsistent})}, where LLMs fail to detect conflicting inputs (A.1) or overlooking logical inconsistencies (A.2), \textbf{(B) Misinformation Reasoning (\S\ref{sec:misinformation})}, caused by temporal misalignment leading to outdated responses (B.1) or data contamination resulting in misleading outputs (B.2). \textbf{(C) Fuzzy Language Interpretation (\S\ref{sec:fuzzy})}, where the model relies on biases for fuzzy queries (C.1) or defaults to a response without seeking clarification (C.2). and \textbf{(D) Intention Clarification Failure (\S\ref{sec:intention})} , where it misinterprets sarcasm (D.1) or ignores prior emotional context (D.2).}
	\label{fig:reason}
\end{figure*}

\subsection{Inconsistent Instruction Reasoning}\label{sec:inconsistent}
In natural language communication, humans easily identify inconsistencies using context and prior knowledge, whereas LLMs struggle, often accept contradictory inputs, and generate unreliable answers.
\textit{This phenomenon has been observed across multiple question-answering generation tasks~\cite{li2023contradoc,zheng2022cdconv}, and we categorize the causes of this problem according to the scenarios in which it occurs as follows:} \textbf{1) Ignoring input errors} (Figure \ref{fig:reason} A.1). The model ignores the input errors and gives an answer, resulting in the model assigning the same weight to each context given by the user, which in turn affects the generation of the answer. \textbf{2) Inability to detect user inconsistencies} (Figure \ref{fig:reason} A.2). In the premise that the model has learned the knowledge, the model still has difficulty detecting user inconsistencies. 
 To address inconsistent instruction reasoning issues, existing solutions primarily adopt the following two approaches: 

\paragraph{Knowledge Updating.} SituatedQA~\cite{DBLP:conf/emnlp/ZhangC21} attempts to enhance model performance by updating the knowledge base. 
% ContraDoc~\cite{li2023contradoc} trains models to identify contradictions in long texts using human-annotated datasets. 
Additionally, CDConv~\cite{zheng2022cdconv} simulates common user behaviors to trigger chatbots through an automated dialogue generation method, generating contradictions for training purposes. 

\paragraph{Confidence Calibration.} Given the high cost of data annotation and model fine-tuning, some researchers have sought alternative approaches by introducing additional processing techniques. CD2~\cite{jin2024tug} maximizes probabilistic output and calibrates model confidence under knowledge conflicts using conflict decoupling and comparison decoding methods. MacNoise~\cite{hong2023so} enhances contradiction retrieval in an augmented generative system by explicitly fine-tuning a discriminator or prompting LLMs to improve contradiction discrimination. 


\subsection{Misinformation Reasoning}\label{sec:misinformation}
Erroneous instructions mislead model outputs more severely than inconsistent ones, as they lack obvious contradictions, requiring the model to comprehend, reason, compare input knowledge with its parameterized knowledge, and make objective judgments~\cite{DBLP:conf/emnlp/CheangCW0LS0C23,DBLP:conf/acl/XuLYZS0FX024}. \textit{From the input perspective, this paper classifies the sources of erroneous information into two categories as follows:}
\textbf{1) Temporal Alignment Failure} (Figure \ref{fig:reason} B.1). arises when the knowledge provided by the user and the model is temporally misaligned due to updates occurring at different times, leading to inconsistent responses. Such discrepancies typically originate during the training process.  
\textbf{2) Information Contamination} (Figure \ref{fig:reason} B.2). refers to the degradation of model quality caused by the intentional distortion of input data.  

To solve the above problems, existing methods mainly focus on improving model susceptibility in the face of internal and external knowledge conflicts through targeted fine-tuning and processing. CKL~\cite{DBLP:conf/iclr/JangYYSHKCS22} ensures that the model's knowledge is updated in a timely manner through an online approach, although this approach is slightly weaker than re-training in terms of effectiveness~\cite{DBLP:conf/icml/LiskaKGTSAdSZYG22}.
% , and RKC-LLMs~\cite{DBLP:journals/corr/abs-2310-00935} allows a large model to recognize knowledge conflicts by means of instructional fine-tuning that identifying specific passages of conflicting information. 
BIPIA~\cite{yi2023benchmarking} used adversarial training to combat the effects of information pollution and improve model robustness. CAR~\cite{DBLP:conf/eacl/WellerKWLD24} achieved nearly 20\% improvement by discriminating external knowledge that may not be contaminated in the RAG system.


\subsection{Fuzzy Language Interpretation}
\label{sec:fuzzy}
% When the user's instructions contain ambiguous words (polysemy, ambiguity, etc.), the large language model may choose the wrong interpretation among multiple possible interpretations, and thus fail to accurately identify the user's true intention. Because the large language model is trained to maximize the prediction accuracy of the training data, it generates the most likely next word or sentence based on the input text. The large model does not realize the need to clarify or take proactive actions to obtain more information. Betty et al.~\cite{hou2024large} also pointed out that LLMs may inadvertently provide misleading answers, especially when dealing with ambiguous or polysemous instructions.
When user instructions contain fuzzy terms (e.g., polysemy or vagueness), LLMs may select an incorrect interpretation from multiple possibilities, potentially leading to misleading responses.
\textit{This phenomenon has been observed across multiple information-seeking tasks~\cite{kim2024aligning}, and we categorize the causes of this problem according to the scenarios in which it occurs as follows:}
\textbf{1) Self-defined problem} (Figure \ref{fig:reason} C.1).  When the user inputs content with fuzzy sentences, LLMs may choose to generate content based on the preferences of its own training data. 
\textbf{2) Select data based on fuzzy input} (Figure \ref{fig:reason} C.2). In response to ambiguous user input, LLMs may select a default explanation without actively asking the user to clarify.


To solve the problem, researchers have started to experiment with cue engineering and relation alignment. Folkscope~\cite{DBLP:conf/acl/YuWLBSLG0Y23} proposed the FolkScope framework, which uses a large language model to analyze and discriminate users' fuzzy purchasing intention.
% Miko~\cite{lu2024miko} proposes a multimodal hierarchical intention generation framework that analyzes the potential intentions behind users' posting behaviors based on the fuzzt information they post on social platforms.
Miko~\cite{lu2024miko} introduces a multimodal hierarchical intention generation framework that interprets users' posting behaviors by analyzing the fuzzy information they share on social platforms.
% Zhang et al.~\cite{DBLP:conf/acl/0002LJ24} have employed behavioral cloning by using demonstration data from a strong model to train a weaker model so that the weaker model can perform better on a similar task. 
ATC~\cite{DBLP:conf/emnlp/DengLC0LC23} utilizes the Active Thinking Chain cueing scheme, which enhances the proactivity of a biglanguage model by adding goal-planning capabilities to the descriptive reasoning chain.


\subsection{Intention Clarification Failure}\label{sec:intention}
% The reasoning of a LLMs relies on the training data it receives and the internal structure of the model itself. Although a LLMs can learn from the data, this is not the same as “real world” common sense and contextual understanding. Real human beings can reason based on their real-life experience and common sense, but LLMs lack this experience-based reasoning ability, and it is difficult to speculate on complex contexts outside of the input data. Meanwhile the LLMs is likely to fail to maintain a consistent reasoning trajectory all the time when spanning longer texts, complex contexts, or multiple rounds of conversations, especially when complex intentions or changes in sentiment are involved, the LLMs may fail to effectively memorize the sentiment contexts of the previous text, and may not be able to correctly reason out the user's implicit needs. 
Unlike humans, who reason based on experience, LLMs lack real-world common sense and thus struggle to infer complex contexts beyond their input data. Moreover, they often fail to maintain a consistent reasoning trajectory across long texts, complex contexts, or multi-turn conversations. When handling intricate intentions or sentiment shifts, LLMs may struggle to retain prior context, leading to errors in inferring implicit user needs.
\textit{We categorize the causes of this problem according to the scenarios in which it occurs as follows:} \textbf{1) Fails to detect sarcasm} (Figure \ref{fig:reason} D.1), when LLMs fails to understand the sarcastic intention of the user. \textbf{2) Ignores prior emotional context} (Figure \ref{fig:reason} D.2),  when LLMs focus only on the second half of the sentence and ignore the emotions in the previous round of dialog.

To solve above problems, researchers have started to try to construct multi-domain datasets containing implicit intentions to strengthen the ability of the LLMs to reason about complex intentions and user emotions in multi-round interaction scenarios. 
DeepSeek-R1~\cite{guo2025deepseek} enhances its understanding of human intention through a structured process with two RL stages for refining reasoning patterns and aligning with human preferences.
S1~\cite{muennighoff2025s1} uses budget forcing to control the number of thinking tokens. The upper limit is terminated early by a delimiter, while the lower limit prohibits delimiters and adds "wait" to guide in-depth reasoning and optimize the quality of the answer.
SoulChat~\cite{chen2023soulchat} fine-tuned LLMs by constructing a dataset containing more than 2 million samples of multi-round empathic conversations. 
MoChat~\cite{DBLP:journals/corr/abs-2410-11404} constructs multi-round dialogues for spatial localization by using joint grouping spatio-temporal localization.
% LARA~\cite{DBLP:journals/corr/abs-2403-16504} combines a fine-tuned smaller model with retrieval enhancement mechanisms and integrates it into the architecture of LLM. 

\section{Realiable Generation}\label{sec:challenge3}

Despite strong performance, LLMs struggle with output reliability. Trained on large corpora using maximum likelihood estimation, they generate deterministic responses. While effective on familiar data, they often produce unstable or incomplete responses to unseen inputs, undermining reliability. We analyze LLM response stability and alignment in depth.


\subsection{Response Stability}\label{sec:response}
The knowledge acquired by the LLMs is generally determined in the pre-training stage and stored in a parameterized form. For data in a specific field, the current model is generally optimized by fine-tuning instructions so that it outputs what humans want~\cite{radford2019language}. If knowledge samples that the model has not seen are used in instruction-tuning, it will inevitably cause the model to give a definite response to unknown inputs, and there is a high probability that an answer will be fabricated. \textit{This is the over-confidence of the model that causes the model to output unreliable answers, and we categorize the causes of this problem according to the scenarios in which it occurs as follows:}
\textbf{1) Fabricated incorrect information} (Figure \ref{fig:gene} A.1). When the model’s knowledge did not match the input question, it fabricated information that did not match the facts. 
\textbf{2) Incorrect context output} (Figure \ref{fig:gene} A.2). When the model’s knowledge did match the input question, it output incorrect context information.
To address these issues, researchers have explored uncertainty, which quantifies the credibility and stability of model outputs as follows.

\begin{figure}[t]
    \setlength{\belowcaptionskip}{-0.2cm}
	\begin{center}
        \includegraphics[width=1\linewidth]{picture/figure4.pdf}
	\end{center}
	\caption{Common generation issues in LLMs include: (A) \textbf{Unstable Content Generation (\S\ref{sec:response})}, where the model fabricates details when it lacks relevant knowledge (A.1) or produces incorrect contextual information despite having relevant knowledge (A.2). \textbf{(B) Misalignment with Human Values (\S\ref{sec:ailgn})}, where the model generates harmful or offensive content (B.1) or provides responses that conflict with moral and ethical standards (B.2).}
	\label{fig:gene}
\end{figure}

\paragraph{Fine-tuning LLMs.} To make LLMs more accurate in estimating uncertainty, existing methods fine-tune  models~\cite{sensoy2018evidential,amini2020deep}. LUQ~\cite{zhang2024luq} is a novel sampling-based uncertainty quantification method specifically designed for long texts. 
% UaIT~\cite{kuhn2023semantic} using semantic entropy to assess output uncertainty. 
ConformalFactuality~\cite{mohri2024language} defines the associated uncertainties for each possible output.

\paragraph{External Tools.} Fine-tuning LLMs typically demands substantial computing resources and slow training; therefore, reducing computational overhead is crucial for improving efficiency. Researcher has proposed methods to evaluate the uncertainty of model outputs through external tools~\cite{liu2024uncertainty}. 
% ConfidenceElicitation~\cite{xiong2023can} is a new uncertainty measurement tool for large model outputs. 
CalibrateMath~\cite{lin2022teaching} assesses uncertainty by requiring models to generate numerical answers with confidence levels, evaluating their reliability.

\subsection{Alignment}\label{sec:ailgn}
Despite the impressive capabilities of large language models (LLMs), they have raised significant concerns regarding the unsafe or harmful content they may generate. LLMs are typically trained on vast datasets scraped from the internet, including inappropriate or harmful content~\cite{10.1145/3442188.3445922}.\textit{ This means that the models may inadvertently produce outputs misaligned with human values as follows:} \textbf{1) Generation of Toxic Content} (Figure \ref{fig:gene} B.1). LLMs may generate toxic content, such as hate speech or offensive comments, when asked to respond to sensitive topics~\cite{DBLP:conf/acl/LuongLNN24,DBLP:conf/ijcai/DuttaKDK24}.
\textbf{2) Conflicts with Moral/Ethical Standards} (Figure \ref{fig:gene} B.2). LLMs might produce outputs that conflict with moral or ethical standards, such as guiding illegal activities~\cite{ramezani-xu-2023-knowledge,abdulhai-etal-2024-moral}. 
To tackle the previously mentioned concerns regarding unsafe or harmful content produced by LLMs, researchers have focused on various stages:

\paragraph{Pretraining Data Cleaning and Curation.} To minimize the risks associated with harmful or inappropriate content, LLM training datasets should undergo rigorous cleaning processes~\cite{10.1145/3442188.3445922}, such as filtering out toxic language, hate speech, and harmful stereotypes. 
% Tools like the Perspective API~\cite{DBLP:conf/coling/ChengK022} and word embedding debiasing methods~\cite{rakshit-etal-2025-prejudice} can help identify and remove toxic and biased content.
Tools like word embedding debiasing methods~\cite{rakshit-etal-2025-prejudice} can help identify and remove toxic and biased content.


\begin{table*}[]
\caption{A selection of widely used benchmark datasets for evaluating LLMs. The 'Type' column categorizes the benchmarks based on task types, while 'Lang.' indicates the language of the dataset, and 'Citation' denotes the number of citations each benchmark has received.}
\label{tab:benchmark}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}llcclr@{}}
\toprule
\textbf{Type} & \textbf{Benchmark} & \textbf{Year} & \textbf{Lang.} & \textbf{Description} & \textbf{Citation} \\ \midrule
\multirow{2}{*}{LLM Understanding} 
& $\infty${B}ench~\cite{zhang-etal-2024-bench} & 2024 & Zh\&En & Evaluation of long-context handling in LLMs. & 1 \\
& MT-BENCH-101~\cite{bai-etal-2024-mt} & 2024 & En & Evaluation of multi-turn dialogue capability in LLMs. & 39 \\ \midrule

\multirow{4}{*}{LLM Reasoning} 
& BIPIA~\cite{yi2023benchmarking} & 2023 & En & Assesses LLMs vulnerability to hint injection attacks. & 60 \\
& Miko~\cite{lu2024miko} & 2024 & En & Evaluates multimodal LLMs in understanding social intention. & 2 \\
& CONTRADOC~\cite{li2023contradoc} & 2023 & En & Detects self-contradictions in long documents. & 7 \\
& CDCONV~\cite{zheng2022cdconv} & 2022 & Zh & Detects contradictions in Chinese dialogues. & 12 \\ \midrule

\multirow{3}{*}{LLM Generation} 
& Open-LLM-Leaderboard~\cite{ye2024benchmarking} & 2024 & En & Assesses LLMs uncertainty in generated content. & 19 \\
& ETHICS Dataset~\cite{hendrycks2020aligning} & 2020 & En & Evaluates LLMs on moral reasoning and ethics. & 468 \\
& FACTOR~\cite{muhlgay2023generating} & 2023 & En & Measures factual accuracy in LLM-generated text. & 71 \\ 
\bottomrule
\end{tabular}
}
\end{table*}


\paragraph{Reinforcement Learning-based Alignment.} To further align LLMs with human values and societal norms, reinforcement learning methods, such as RLHF and its advanced variants, such as PPO~\cite{ouyang2022training}, DPO~\cite{DBLP:conf/icml/ZengLMYZW24}, and GRPO~\cite{DBLP:journals/corr/abs-2402-03300}, are very essential. As an extension of RLHF, RLAIF~\cite{DBLP:conf/icml/0001PMMFLBHCRP24} leverages AI systems to assist in the feedback process, making evaluation and fine-tuning more scalable.
% and enabling continuous monitoring and refinement of the model’s behavior~\cite{DBLP:conf/icml/0001PMMFLBHCRP24}.

\paragraph{In-context Alignment.} In-context alignment leverages the ability of LLMs to adapt their responses based on a few examples provided in the prompt.~\cite{DBLP:conf/iclr/LinRLDSCB024} demonstrates that effective alignment can be achieved purely through ICL with just a few stylistic examples and a system prompt.~\cite{huang-etal-2024-far} explored the effectiveness of different components of In-context alignment, and found that examples within the context are crucial for enhancing alignment capabilities.
%~\cite{liu-etal-2024-take} introduced PICA, which uses a two-stage approach to improve alignment efficiency.


\section{Benchmark}\label{sec:Benchmark}

This section covers benchmarks for LLMs in instruction understanding, reasoning, and generation (\Cref{tab:benchmark}).


\subsection{Benchmarking Instruction Understanding}

Instruction understanding in LLMs involves extracting key information, maintaining coherence, and adapting to dynamic conversation changes, especially in long or multi-round dialogues.
Zhang et al. \cite{zhang-etal-2024-bench} provides a benchmark consisting of 12 tasks, with an average data length exceeding 100K tokens, designed to evaluate LLMs' ability to handle long contexts. The results show that the performance of LLMs significantly declines when dealing with long contexts, suggesting further improvements.
MT-BENCH-101 \cite{bai-etal-2024-mt} constructs a three-layer hierarchical capability classification and 1,388 dialogue pairs across 13 tasks, providing a comprehensive evaluation of LLMs' fine-grained capabilities in multi-turn dialogue. Experimental results show that commonly used techniques do not significantly enhance multi-turn capabilities.


\subsection{Benchmarking LLM Reasoning}

LLM Reasoning involves inferring user intentions by interpreting both explicit and implicit language cues.
BIPIA~\cite{yi2023benchmarking} evaluates LLMs under indirect hint injection attacks across five scenarios and 250 targets, revealing vulnerabilities in all models, with GPT-3.5-turbo and GPT-4 exhibiting notably higher susceptibilities.
Miko~\cite{lu2024miko} assesses multimodal models in understanding social media user intentions. The benchmark, which includes 979 social media entries, shows that multimodal LLMs outperform text-only models like LLama2-7B and GLM4. Incorporating image data enhances the model’s ability to interpret user intentions, improving accuracy.
CONTRADOC~\cite{li2023contradoc} is the first dataset for analyzing self-contradictions in long documents. Evaluation of GPT-4, PaLM2, and other LLMs on this dataset reveals that while GPT-4 outperforms others and even surpasses human performance, it still struggles with complex contradictions requiring nuanced reasoning.
CDCONV~\cite{zheng2022cdconv} focuses on contradiction detection in Chinese dialogues, containing over 12,000 dialogue rounds. The study shows that the Hierarchical method consistently outperforms others in detecting contradictions, highlighting the importance of accurate contextual modeling in dialogue understanding.



\subsection{Benchmarking LLM Generation}
LLM Generation assesses a model's ability to understand user instructions, avoid fabricating false information, and generate accurate, contextually appropriate responses.
Open-LLM-Leaderboard~\cite{ye2024benchmarking} introduces a novel benchmark that integrates uncertainty quantification to evaluate the reliability of content generation across tasks like QA, comprehension, and dialogue. Results show that larger models often exhibit greater uncertainty, and fine-tuned models tend to have higher uncertainty despite higher accuracy.
ETHICS~\cite{hendrycks2020aligning} evaluates whether generated content aligns with human ethical values, such as justice and well-being. The study finds that while models like GPT-3 show promise in predicting human moral judgments, they still need improvement in this domain.
FACTOR~\cite{muhlgay2023generating} addresses the evaluation of factuality in LLMs by providing a scalable method that ensures diverse and rare facts are considered. Testing models such as GPT-2 and GPT-Neo show that, while benchmark scores correlate with perplexity, they better reflect factuality in open-ended generation, especially when retrieval augmentation is applied.

\section{Future Directions}\label{sec:Directions}
This section summarizes ongoing challenges in instruction understanding, reasoning, and reliable generation with LLMs and outlines potential future research directions.

\paragraph{Automated Annotation Framework.} Although LLMs excel in general-domain tasks, they often produce hallucinated or incomplete content in specialized fields due to limited domain-specific training data. While contextual learning and instruction fine-tuning methods have been explored to address this issue, manual data annotation remains labor-intensive and prone to quality inconsistencies. An automated annotation framework could streamline data labeling, enhancing model performance in specialized fields by ensuring higher quality and scalability of domain-specific training datasets.

\paragraph{GraphRAG.} 
LLMs have shown impressive language generation capabilities through pre-training on large datasets, but their reliance on static data often results in inaccurate or fictional content, particularly in domain-specific tasks. The Graph-enhanced generation approach aims to tackle this by leveraging KGs and GNNs for precise knowledge retrieval. Despite its advantages, GraphRAG faces challenges in capturing structural information during graph reasoning tasks and struggles with multi-hop retrieval accuracy and conflict resolution between external and internal knowledge. Future work should focus on refining retrieval strategies and improving the stability and accuracy of GraphRAG in complex tasks.

\paragraph{Quantifying Uncertainty in LLMs.} 
As LLMs like LLama and ChatGPT have revolutionized content generation, ensuring the reliability of their outputs remains a critical challenge. Uncertainty quantification is a promising approach to address this, enabling models to provide a confidence assessment alongside their responses. Evidence learning~\cite{sensoy2018evidential,amini2020deep}, an emerging method in uncertainty representation, offers a reliable approach for quantifying uncertainty directly from data. However, its application to LLMs is computationally intensive, and most existing work focuses on small-scale models. Future research should aim to optimize uncertainty quantification for large-scale models efficiently.


\paragraph{Balancing Safety and Performance.} Although advancements in alignment techniques have improved factual accuracy and safety, they often come at the cost of the model's creativity and fluency. Striking a balance between safety and performance is crucial. Future research should explore new alignment methods that ensure both the safety and usability of LLMs, optimizing the trade-off between generating reliable, safe content and maintaining the model's creative and contextual capabilities.


\section{Conclusion}\label{sec:Conclusion}
This paper analyzes LLMs' performance in processing user instructions. Despite progress in natural language understanding, LLMs struggle with complex, inconsistent instructions, often resulting in biases, errors, and hallucinations. Improvements through prompt engineering, knowledge retrieval, model expansion, fine-tuning, and RLHF have not fully addressed LLMs' limitations in reasoning and comprehension, limiting their real-world applicability.
We identify three key bottlenecks: instruction comprehension, reasoning ability, and reliable content generation. Future research should focus on enhancing reasoning for complex instructions and aligning outputs with user intent, to improve LLMs' adaptability and reliability in real-world tasks.

% \clearpage

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

