\section{Conclusion}
\label{sec:conclusion}


Our work demonstrates that multilingual PRMs significantly enhance the ability to perform complex, multi-step reasoning tasks in various languages, consistently outperforming both monolingual and cross-lingual counterparts. This conclusion is supported by comprehensive evaluations spanning 11 languages. Furthermore, our findings highlight that performance is sensitive to the number of languages and the volume of English training data. However, it also benefits substantially from more candidate responses and model parameters. These results underscore the importance of diverse language training in providing fine-grained rewards and open up promising avenues for multilingual reasoning.
