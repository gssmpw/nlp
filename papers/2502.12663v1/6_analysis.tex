\section{Analysis}
\label{sec:analysis}


In this section, we present a comprehensive analysis of our multilingual PRM, focusing on five critical aspects: error positions (\autoref{sec:analysis_error}), number of solutions (\autoref{sec:analysis_samplecounts}), integration of LoRA with PRM (\autoref{sec:analysis_lora}), comparative evaluation with multilingual ORM (see \autoref{sec:analysis_prmorm}), and implement PPO with multilingual PRM (see \autoref{sec:analysis_ppo}). 


\subsection{Which Steps Are More Prone to Errors?}
\label{sec:analysis_error}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.55]{figures/error_position-ru2.pdf}
    \caption{Percentage distribution of the first error positions corresponding to the step in the reasoning on the PRM800K testset.}
    \label{fig:error_ru}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.6]{figures/sample_cnt-prm-math-llama.pdf}
    \caption{Best-of-N sampling performance of \llama using different verification strategies across distinct numbers of solutions on \mathset.}
    \label{fig:sample-numbers}
\end{figure}


PRMs provide fine-grained feedback on each intermediate step of a model's chain-of-thought reasoning process. The errors made at intermediate steps can propagate through the reasoning chain, ultimately affecting the final answer. Therefore, in this section, we investigate the earliest errors made by PRMs during the reasoning process, following \citet{processbench}.


\input{tables/baseline}


\paragraph{Setup}
We select a subset of instances from the PRM800K Russian test set where the final answers made by \mono, \en, and \mix are incorrect. For these instances, we identify the first occurrences of incorrect predictions from these PRMs. 
We classify the first error positions into three groups: \textit{early} (steps 1 to 5), \textit{middle} (steps 6 to 10), and \textit{later} (steps 11 to 15).



\paragraph{Multilingual PRMs produce fewer errors at early steps.}

The distribution of the earliest error positions, visualized in \autoref{fig:error_ru}, reveals a clear distinction between the three PRM configurations. In both \mono and \en, a significant proportion of errors occurs within the early steps. In contrast, \mix demonstrates fewer errors within this range and exhibits a slightly higher number of errors in later steps. These observations suggest that \mix may be less prone to error propagation in the reasoning process, enabling it to maintain a more reliable reasoning trajectory. Consequently, \mix can effectively achieve better overall performance.

\subsection{Do More Candidates Drive Better Performance?}
\label{sec:analysis_samplecounts}
Recent research suggests that providing more candidate solutions can significantly boost the performance of PRM \citep{prm800k,shepherd,openr}. To investigate whether this trend extends to multilingual settings, we examine the impact of varying the number of candidates on \mono, \en, and \mix.

\paragraph{Setup}
We conduct experiments on the \mathset benchmark using the \llama generator to compare the performance of multilingual PRM (\mix), cross-lingual PRM (\en), and monolingual PRM (\mono). For each approach, we vary the number of candidates N from 2 to 64. This allows us to assess how the number of candidate solutions influences performance across different PRM strategies in a multilingual context.




\paragraph{Multilingual PRMs yield better performance with more candidate solutions.}

\autoref{fig:sample-numbers} illustrates that \mix consistently outperforms both \en and \mono, with its advantage growing more pronounced as the number of candidates (N) increases. This finding underscores the scalability of multilingual PRM in diverse linguistic scenarios. 
Overall, these observations reinforce the conclusion that multilingual PRM not only maintains superior performance but also scales well as more candidates are introduced.


\subsection{Are Multilingual PRMs Compatible with Parameter-Efficient Finetuning?}
\label{sec:analysis_lora}


Recent research has demonstrated the effectiveness of parameter-efficient finetuning (PEFT) across a variety of tasks \citep{peft1,peft2}. Therefore, we explore whether the PEFT approaches, such as LoRA \citep{lora}, also perform well on multilingual PRMs.

\paragraph{Setup}
To investigate this question, we employ LoRA on the key, query, and value attention matrices. Specifically, we use a rank of 8 and a dropout rate of 0.05 for both multilingual and cross-lingual PRMs. We train for three epochs with a batch size of 64 and a learning rate of $1e^{-5}$.


\paragraph{LoRA is computationally efficient, but not as good as its fully-finetuning counterpart in multilingual PRMs.}


\autoref{fig:lora-fft} demonstrates that fully fine-tuning (FFT) consistently outperforms LoRA in both cross-lingual and multilingual settings. The performance gap becomes larger on the \mathset dataset, which contains more complex questions compared to \mgsmset, suggesting that FFT is better suited for tasks requiring deeper reasoning and understanding. These findings align with prior research, which indicates that while PEFT methods may fall short of FFT when tasks demand higher complexity or reasoning capabilities \citep{lora-fall}. Interestingly, although LoRA-based methods generally lag behind FFT, multilingual LoRA achieves stronger results than cross-lingual LoRA. This highlights the benefits of leveraging multilingual data during parameter-efficient fine-tuning, as multilingual data likely provides richer data diversity and linguistic coverage.


\begin{figure}[t]
    \centering
    \includegraphics[scale=0.6]{figures/lora_fft_llama.pdf}
    \caption{Comparison between parameter-efficient finetuning (LoRA) PRM and fully fine-tuning (FFT) PRM with \llama generator.}
    \label{fig:lora-fft}
\end{figure}





\subsection{Does PRM Surpass ORM in the Multilingual Scenario?}
\label{sec:analysis_prmorm}


In this section, we explore whether PRM also outperforms Outcome Reward Model (ORM) and self-consistency (SC) \citep{sc} in multilingual settings.

\paragraph{Setup}

Following \citet{prm800k,shepherd}, we evaluate the performance of \mix by comparing it with other \textit{verifier} methods, including: Direct prediction (\baseline), Self-consistency (majority voting) (\scmethod), and ORM. The accuracy of the best-of-N solution is used as the evaluation metric. Specifically, we train a multilingual ORM using uniform example budgets across seven seen languages. Then we assess the performance of verifiers on seven seen languages as well as on four additional unseen languages.


\paragraph{Multilingual \prm outperforms \scmethod and \orm across all languages and generators.} 




The results presented in \autoref{tab:baseline-sc-orm-prm} confirm that \prm consistently achieves higher accuracy on two benchmarks across multiple languages. Specifically, when using the \llama as the generator, \prm improves average accuracy by +19.64 points on the \mathset dataset and by +15.75 points on the \mgsmset dataset in terms of $\avgall$, compared to the \baseline of direct prediction. These substantial gains suggest \prm's potential to enhance reasoning performance in a multilingual setting. Furthermore, \prm also surpasses both \scmethod and \orm. For example, \prm exceeds SC and \orm by margins of up to +8.80 and +6.73 points on \mgsmset, respectively, when using \llama as the generator. Additionally, \prm demonstrates performance improvements for both seen and unseen languages. With the \deepseek generator on \mgsmset, \prm achieves respective gains of +17.49 and +31.20 for the seen and unseen language sets, compared to the \baseline. 



\subsection{Can Multilingual PRM Enhance LLMs?}
\label{sec:analysis_ppo}
\input{tables/ppo}

We have previously shown that multilingual PRM can bolster model performance under a best-of-N selection framework. In this section, we demonstrate that the multilingual PRM can be used as the reward model for finetuning the LLMs under a reinforcement learning paradigm.

\paragraph{Setup}
We design experiments to improve \llama using RL where we adopt the PPO strategy \citep{ppo} on the MetaMathQA training set \citep{metamath}. We then evaluate the resulting policy models on \mgsmset using top-1 accuracy in a zero-shot setting. Due to the computational constraints, we only generate one response during the fine-tuning process.

\paragraph{Reinforcement learning with multilingual PRM further improves the performance of LLMs.}

The results shown in \autoref{tab:ppo} indicate that step-by-step PPO with \mix (\model{PPO-PRM}) consistently outperforms a standard supervised fine-tuned \model{Baseline} and PPO with ORM (\model{PPO-ORM}). \llama with PPO-PRM achieves average boosts of +1.86 and +2.19 across 11 languages, compared to \model{Baseline} and \model{PPO-ORM}, respectively. These findings highlight the importance of fine-grained multilingual reward signals. These gains demonstrate that process rewards can refine policy decisions for both reasoning steps and final outputs with reinforcement learning.
