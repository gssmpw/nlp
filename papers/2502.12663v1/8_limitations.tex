\section{Limitations}
\label{sec:limitations}


While we have demonstrated the effectiveness of multilingual PRMs, our study has not comprehensively explored the wide range of reward optimization methods \citep{limit1,limit2}, some of which may not benefit from cross-lingual reward model transfer. Nevertheless, best-of-N and PPO, the two techniques leveraged in this paper, are highly representative of current practices, particularly given the consistently strong performance of best-of-N \citep{limit3,limit1,limit4}.
Furthermore, while our results show that multilingual PRMs outperform both cross-lingual and monolingual PRMs, our experiments are limited to 11 languages. Extending this approach to a broader set of languages and evaluating its impact across diverse linguistic families is an important avenue for future work.