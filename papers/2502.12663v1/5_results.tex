\input{tables/mix-mono-en-math}
\section{Recipes for Multilingual PRM Training}
\label{sec:results}

In this section, we conduct a series of experiments to investigate the performance of multilingual PRM. We examine how \mix compares to \mono and \en (\autoref{sec:results_multi_mono}), the impact of the number of training languages (\autoref{sec:results_languages_number}), and the effect of varying the proportion of English in the training data (\autoref{sec:results_english_percentage}).

\subsection{Monolingual, Cross-lingual, or Multilingual PRMs?}
\label{sec:results_multi_mono}


Building upon the findings of \citet{multiorm}, who demonstrated that cross-lingual ORMs outperform monolingual ones, we investigate the impact of multilingualism on PRMs. Specifically, we compare \mono, \en, and \mix to determine which setup offers best performance across languages.


\paragraph{Setup}


We include three setups in this work. The \mono is trained and evaluated on each individual language from the set of seen languages. The \en is trained exclusively on an English dataset and evaluated on all 11 test languages. Finally, the \mix is trained on all seen languages and tested on all 11 test languages.


\paragraph{Multilingual PRMs perform best, followed by cross-lingual PRMs, while monolingual PRMs achieve the worst performance, on the seen languages.}

As shown in \autoref{tab:mix-mono-en_math}, \mix consistently achieves the highest performance across multiple language generators on the seen languages, surpassing \mono and \en by +1.5 and +1.2 with \llama generator, respectively. This indicates that incorporating data from multiple languages for PRM training significantly enhances the model's ability  across different languages. When comparing \mono and \en, we observe that \en outperforms the \mono for the English-centric \mistral and \llama generators. We hypothesize that this advantage stems from the pre-training phase: these generators are predominantly trained on English data but have limited exposure to multilingual corpora. As a result, fine-tuning on English PRM data enhances the reasoning capabilities of PRMs, facilitating greater cross-lingual transfer.
More monolingual results are in \autoref{sec:appendix_crosslingualprm}.




\paragraph{Multilingual PRMs generalize better on the unseen languages.}
Both \en and \mix are evaluated on four additional unseen languages. As shown in \autoref{tab:mix-mono-en_math}, \mix demonstrates superior overall performance on the unseen languages in terms of $\avgunseen$. These results suggest that training PRMs on multilingual datasets can effectively enhance model generalization to the unseen languages.


In conclusion, these findings demonstrate that training a single multilingual PRM is an effective strategy for broad cross-lingual coverage, outperforming models trained either on a target language or on English alone. This outcome supports that \mix is particularly advantageous for expanding the capabilities of PRMs in multilingual settings. More results on \mgsmset are in \autoref{sec:appendix_mix_mono_en_mgsm}.








\subsection{Does More Languages Lead to Better Multilingual PRMs?}
\label{sec:results_languages_number}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.5]{figures/language_counts-llama-mgsm.pdf}
    \caption{Best-of-N Performance on \mgsmset of PRMs trained using various subsets of English, German, Spanish, French, Russian, Swahili, and Chinese, with the generator of \llama. The averages scores across all 11 languages.}
    \label{fig:train-counts}
\end{figure}


While multilingual PRMs have demonstrated significant improvements, the question of how many languages are needed to achieve the best performance remains an open research problem. In this section, we address this research question by exploring the relationship between the number of training languages and the resulting performance.


\paragraph{Setup}
We conduct experiments by training PRMs on datasets ranging from a single language up to all seven languages. 
In this section, the number of total training examples of all PRMs are fixed.
When the number of languages exceeds one, the total training examples are evenly distributed across all the selected languages. For evaluation, we test all PRMs on 11 different languages. The evaluation scores are averaged for each test language across all PRMs trained with the same number of languages.




\paragraph{More languages do not result in better multilingual PRMs.}
As shown in \autoref{fig:train-counts}, the overall performance (\texttt{AVG}) improves as the number of training languages increases up to five languages. Beyond this point, adding more languages does not lead to further gains. Additionally, results from five individual languages (four seen languages and one unseen language) demonstrate that, although the optimal number of training languages varies across these languages, increasing the number of languages never leads to better performance. These findings suggest that increasing the number of training languages does not necessarily enhance multilingual PRMs. A key reason for this is the fixed amount of training data: as the number of languages grows, the training examples per language decrease. This reduction hinders sufficient training for seen languages and negatively impacts cross-lingual transfer to unseen languages.

\subsection{How Much English Data Do We Need for Multilingual PRMs?}
\label{sec:results_english_percentage}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.7]{figures/english_percentage3.pdf}
    \caption{
    Best-of-N sampling performance of \llama 
    with PRMs finetuned on a training set where P\% of the data is in English and (100 - P)\% is uniformly distributed across six other languages. Each tick on the X-axis represents a specific tuning set configuration. 
    The dash lines in {\color{blue}blue}, {\color{red}red}, and {\color{green}green}, indicate the average scores of all the languages, the seen languages, and the unseen languages, respectively.
}
    \label{fig:english-percentage}
\end{figure}

While multilingual training with equal number of training examples in each language (\mix) generally improves performance compared to English-only training (\en), we observe some exceptions on certain languages, as shown in \autoref{tab:mix-mono-en_math}. This observation prompts us to investigate how varying the number of English examples can affect the multilingual PRMs.


\paragraph{Setup}

To explore this, we create data mixtures with varying percentages of English examples ($P\%$), with the remaining $(100 - P)\%$ examples evenly distributed among six languages: German, Spanish, French, Russian, Swahili, and Chinese. 
Each PRM trained on these mixtures is then evaluated across all 11 languages.



\paragraph{Moderate amount of English data can lead to better multilingual PRMs.}
As shown in \autoref{fig:english-percentage}, incorporating a small amount of English data into the training mixture can lead to notable performance improvements across languages. Specifically, even as little as 1\% of English examples significantly enhances performance, particularly for unseen languages. Interestingly, the majority of performance gains occur when English data constitutes less than 50\% of the training mixture. However, when the proportion of English data exceeds 50\%, performance begins to decline slightly across languages. Furthermore, training on 70\% English data outperforms training solely on English (100\%), suggesting that retaining some multilingual data introduces valuable variation and enhances the generalization capacity of multilingual PRMs. These findings indicate that as the proportion of English data increases, the PRMs may not be adequately trained on other seen languages, and unseen languages may benefit less from cross-lingual transfer. This highlights the importance of maintaining diverse and balanced language representation in multilingual training for optimal performance.

