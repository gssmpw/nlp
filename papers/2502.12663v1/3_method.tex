
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.3]{figures/PRM.png}
    \caption{Framework of PRM.}
    \label{fig:PRM}
\end{figure}


\section{Process Reward Modeling}
\label{sec:prm}

\subsection{PRM Training}

Given a question $p$ and its solution $s$, the ORM assigns a single value to $s$ to indicate whether $s$ is correct. 
We stack a binary classifier on top of the LLM and train the ORM with the binary cross-entropy loss:
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\text{ORM}}& = \\
    & -{(y_s \log(r_s) + (1 - y_s)\log(1 - r_s))}
\end{aligned}
\end{equation}
where $y_s$ is the ground truth label for the solution $s$ ($y_s = 1$ if $s$ is correct, otherwise $y_s = 0$), and $r_s$ is the probability score that $s$ is correct.


In contrast, the PRM evaluates each reasoning step of the solution $s$. The PRM is trained using the following loss function:
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\text{PRM}} & = \\
    -\sum_{i=1}^{K}& y_{s_i}\log(r_{s_i}) + (1 - y_{s_i})\log(1 - r_{s_i})
\end{aligned}
\end{equation}
where $s_i$ is the $i$-th step of the solution $s$, $y_{s_i}$ is the ground truth label for step $s_i$, $r_{s_i}$ is the score assigned to $s_i$ by the PRM, and $K$ is the total number of reasoning steps in the solution $s$. Compared to ORM, PRM provides more detailed and reliable feedback by evaluating individual steps.

\subsection{Ranking for Verification}
\label{sec:prm_verify}

Following \citet{multiorm,prm800k,shepherd}, we evaluate the performance of PRM using the \textit{best-of-N} selection evaluation paradigm \citep{bestofn1,bestofn2}. Specifically, given a question, multiple solutions are sampled from an LLM (referred to as the \textit{generator}) and re-ranked using a reward model (referred to as the \textit{verifier}). For each solution, as shown in \autoref{fig:PRM}, PRM assesses the correctness of each reasoning step. The scores for all steps are averaged to compute an overall score for the solution. The highest-scoring solution is then selected as the final output. This approach enhances the likelihood of selecting solutions containing correct answers, thereby improving the success rate of solving mathematical problems with LLMs.

\subsection{Reinforcement Learning with Process Supervision}

Using the trained PRM, we fine-tune LLMs with Policy Optimization (PPO) \citep{ppo} in a step-by-step manner. This method differs from the conventional strategy that uses PPO with an ORM, that only gives a reward at the end of the response. Conversely, our step-by-step PPO offers rewards at the end of each reasoning step. 

While we analyse PRM both intrinsically (using best-of-N), and extrinsically (using PPO), we focus on best-of-N for a clean testbed without confounders from reinforcement learning.