\section{Experimental Setups}
\label{sec:setting}



\paragraph{Training Datasets}
We combine the PRM800K \citep{prm800k} and Math-Shepherd \citep{shepherd} as training data to finetune PRMs, and translate the combined dataset from English (en) to six languages: German (de), Spanish (es), French (fr), Russian (ru), Swahili (sw), and Chinese (zh) with using NLLB 3.3B \citep{nllb}. The reasoning step statistics are presented in \autoref{tab:data_stat} (\autoref{sec:appendix_statistics}), and 
the parallel examples across seven languages have the same number of reasoning steps.

\paragraph{Test Dataset}
We evaluate the performance of LLMs using two widely used math reasoning datasets, \mgsmset \citep{mgsm} and \mathset \citep{shepherd}. For the \mathset datset, we translate it from English to ten languages: Bengali (bn), German (de), Spanish (es), French (fr), Japanese (ja), Russian (ru), Swahili (sw), Telugu (te), Thai (th), and Chinese (zh) with Google Translate, which is consistent with the languages included in the \mgsmset dataset. 
Furthermore, we also categorize the languages involved in the downstream tasks into two groups based on the training data of \prm: \textit{seen languages} (en, de, es, fr, ru, sw, and zh) and \textit{unseen languages} (bn, ja, te, and th).

\paragraph{Multilingual PRM Setups}
To better understand PRMs in the context of multilingual research, we define three setups: \mono, \en, and \mix. The \mono setup is trained and evaluated on the same single language, serving as the baseline for monolingual PRMs. The \en setup is trained on one language but evaluated on all 11 test languages. Specifically, in this work, we train \en on the English PRM dataset unless otherwise specified. Finally, the \mix setup represents the multilingual PRM, which is both trained on all the seen languages and evaluated on all 11 test languages. To enhance the reliability and generalizability of our study, we train our multilingual PRM (\textbf{\textit{verifier}}) based on the \qwen \citep{qwen}, 
and leverage three diverse LLMs as the \textbf{\textit{generator}}: \mistral \citep{metamath}, 
\llama (fine-tuned with the MetaMath dataset \citep{llama}),\footnote{\url{https://huggingface.co/gohsyi/Meta-Llama-3.1-8B-sft-metamath}}
and \deepseek \citep{deepseek}. 
The details of training these PRMs are presented in \autoref{sec:appendix_training}.

