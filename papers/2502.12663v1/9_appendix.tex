% \section{Appendix}

\section{Data Statistics}
\label{sec:appendix_statistics}
\input{tables/data-statistics}
The dataset statistics are summarized in \autoref{tab:data_stat}. These include the total number of examples, as well as the maximum, minimum, and average number of reasoning steps in the answers across all examples.

\section{Training Details}
\label{sec:appendix_training}
We train the PRMs by fine-tuning all parameters of \qwen using the AdamW optimizer with a learning rate of $10^{-5}$ and a batch size of 8. This process is conducted over two epochs on 4 NVIDIA A100 GPUs (80GB). During training, we use a linear learning rate schedule with a warm-up phase that constitutes 10\% of the total training steps.

\section{Cross-lingual Transfer of PRMs}
\label{sec:appendix_crosslingualprm}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.5]{figures/crosslingual-math500-llama3.1_8b_math.pdf}
    \caption{Performance of \mono trained on seven seen languages and evaluated on all 11 languages based on the \mathset with \llama generator.}
    \label{fig:cross-lingual PRM}
\end{figure}

Following \citet{multiorm}, we assess the performance of cross-lingual PRMs to inspect if language similarity like the script or mutual intelligibility might affect the levels of reasoning verification cross-lingual transfer.

\paragraph{Setup}
We train PRMs on monolingual versions of the data in German, Spanish, French, Russian, Swahili, and Chinese, and evaluate their transfer to other languages.


\paragraph{No clear signal indicates that language similarity strongly correlates with cross-lingual transfer.}

We present the cross-lingual transfer results in \autoref{fig:cross-lingual PRM} and observe that there is no clear conclusion regarding the factors that impact cross-lingual transfer. For instance, the PRM trained on Russian data achieves the highest accuracy when evaluating French, Swahili, Chinese, Telugu, and Thai. Notably, these languages neither share the same script nor belong to the same language family as Russian. This observation suggests that linguistic similarity, in terms of script or language family, may not be a decisive factor in cross-lingual transfer. These findings underscore the uncertainty in predicting cross-lingual transfer performance based solely on language similarity. In practice, selecting a diverse set of representative languages for training a multilingual PRM may be a more effective strategy to address this uncertainty and improve performance across a wide range of target languages.


\section{Breakdown Results of \mgsmset for \mono, \en, and \mix}
\label{sec:appendix_mix_mono_en_mgsm}
\input{tables/mix-mono-en-mgsm}

We present the breakdown of results for each language on the \mgsmset in \autoref{tab:mix-mono-en_mgsm}. The results indicate that the \mix consistently outperforms both the \mono and \en models across languages. This observation aligns with the conclusion drawn in \autoref{sec:results_multi_mono}, highlighting the advantages of multilingual training for PRMs.
