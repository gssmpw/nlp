\section{Introduction}
\label{sec:intro}


Aligning large language models (LLMs) with human preferences can significantly improve the model performance across various downstream tasks \citep{christiano2017deep,ziegler2019fine}. This requires a reward model that is trained on human preference data \citep{ziegler2019fine,stiennon2020learning,shen2021generate,ouyang2022training}. Typically, reward models are trained based on the final outcome of the LLMs' response, and we refer to these as outcome reward models (ORMs) \citep{orm1,solving,orm2}. However, most of recent work demonstrates that ORMs fall short on complex multi-step reasoning tasks \citep{solving,deepseek}. To overcome this limitation, process reward models (PRMs) are introduced, providing fine-grained rewards at each step of the LLMs' chain-of-thought \citep{prm800k,making,shepherd,let}. Previous research has shown that LLMs supervised by PRMs can effectively produce better responses \citep{shepherd,deepseek}.



Despite these significant advances, recent research on ORMs and PRMs has predominantly focused on monolingual settings, particularly English \citep{prm800k,openr,shepherd}. However, the exploration of multilingual PRMs remains relatively limited. Therefore, with the advent of multilingual LLMs, a natural research question arises: \textit{How can we effectively train multilingual PRMs for complex, multi-step reasoning tasks?}



To address this research question, we translate the existing PRM datasets, PRM800K \citep{prm800k} and Math-Shepherd \citep{shepherd}, from English into six additional languages, resulting in a total of seven seen languages for training. We then train multilingual PRMs using the collection of these translated datasets. We define three PRM setups: \mono, \en, and \mix. The \mono setup is trained and evaluated solely on a single language, the \en setup is trained on one language but evaluated on all test languages, and the \mix setup is trained on seven seen languages and evaluated on all test languages. Finally, we conduct a comprehensive evaluation on two popular reasoning tasks (\mathset and \mgsmset) across 11 languages (seven seen languages and four unseen languages) using three LLMs (\mistral, \llama, and \deepseek).


In this work, our main takeaways can be summarized as follows:
\begin{itemize}


    \item \textbf{Multilingual PRM consistently outperforms monolingual and cross-lingual PRMs across all three LLMs.} Our results demonstrate that \mix significantly improves model performance, boosting average accuracy by up to +1.2 and +1.5 points compared to \en and \mono, respectively (see \autoref{sec:results_multi_mono}).

    \item \textbf{Multilingual PRM is sensitive to both the number of languages and the amount of English training data.} Our experiment shows that training an optimal multilingual PRM requires careful consideration of how many languages to include (see \autoref{sec:results_languages_number}) and how much English data to use (see \autoref{sec:results_english_percentage}).

    \item \textbf{Multilingual PRM produces fewer errors in the early steps.} We identify the first occurrences of wrong predictions made by PRMs and observe that \mix produces fewer errors in the early steps compared to \mono and \en (see \autoref{sec:analysis_error}).

    \item \textbf{Multilingual PRM can benefit even more from more candidate responses and trainable parameters.} Our analysis demonstrates that \mix becomes even more advantageous when there is a larger number of candidate responses (see \autoref{sec:analysis_samplecounts}) and when more trainable parameters are introduced (see \autoref{sec:analysis_lora}).
    
\end{itemize}







