\section{Evaluation}

\subsection{Dataset}


The Splunk Security Content dataset is a comprehensive repository of security resources dedicated to improving the detection and mitigation of threats, maintained by Splunk Inc.'s research team. This dataset features over 1,600 analytic rules, all written in Splunk’s Search Processing Language (SPL), to effectively identify malicious behaviors.

These analytic rules are systematically organized into distinct domains such as endpoint, network, application, etc., based on their intended scope of applicability. For instance, the endpoint domain comprises rules specifically crafted to detect malicious activities occurring on endpoint devices. Moreover, each rule is annotated with corresponding MITRE ATT\&CK technique identifiers, thereby offering a ground-truth framework for our experiments. We chose the endpoint domain as it presents a higher diversity of MITRE ATT\&CK technique labels than other domains in the dataset.

To ensure experimental integrity, we carefully evaluated the knowledge cut-off dates of the models utilized. Among the hosted models, GPT-4-Turbo had the most recent cut-off date of December 2023,\footnote{\url{https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard\%2Cstandard-chat-completions\#gpt-4-and-gpt-4-turbo-models}} while Granite, the latest local model, was released in October 2024. 
To prevent any potential data leakage, we exclusively analyzed rules from the Splunk Security Content dataset that were created or modified on or after November 2024. 
During our review of the Splunk Security Content repository in December 2024, we identified 360 rules within the endpoint domain that met our criteria. 
None that none of the hosted or local models employed in our experiments were capable of inherently performing web searches since they were used via an API~\cite{chen2024}. 
% We have provided distribution of SIEM rules in the test set based on their labels in Appendix~\ref{appendixB}.


\subsection{Evaluation Setup}

We implemented our method \methodName in Python 3.9, leveraging multiple libraries and frameworks for model interaction. We utilized the transformers library~\cite{transformers} for local models and for hosted models, we employed the LangChain and LangGraph frameworks.  The hosted models used in our experiments are GPT-4 Turbo, GPT-4o, and GPT-4o-mini, whereas  the local models are Mistral-7B, IBM Granite-3.0-8B, and Qwen-2.5-7B. Table~\ref{tab:models} provides a summary of all the models used in our experiments.

We configured our experimental setup with an Intel Core i7 processor, 32 GB RAM, and NVIDIA RTX 4090 GPU (24GB VRAM). Given the GPU memory constraints, we specifically selected local models with up to 8 billion parameters that could be run at full precision without quantization, ensuring optimal model performance and reliable comparison baseline. This hardware configuration allowed us to maintain consistent computational precision across all local model experiments.

With the technical infrastructure in place, we next focused on developing a standardized approach to model interaction through careful prompt engineering. Our approach follows a carefully designed template that maximizes model performance while maintaining consistency across different models. The prompts used in all steps of \methodName follow a four-component structure described below:

\begin{itemize}[nosep,leftmargin=*]
	\item \textit{Context}: This provides the background or relevant details that set the stage for the task. It includes information on the topic, scenario, or purpose, ensuring that the LLM understands the larger situation.
	\item \textit{Instruction}: This part specifies the exact task the LLM is expected to perform. It provides a clear, concise, and actionable explanation to effectively guide the LLM.
	\item \textit{Guidelines}: These are the rules or constraints that the LLM must follow when completing the task. They ensure that the output is aligned with the desired tone, format, or style.
	\item \textit{Input}: This includes any user-provided data, queries, or material required for the LLM to complete the task. It serves as the starting point or raw material for generating the output.
\end{itemize}

An example of a prompt used in the experiments is illustrated in Figure~\ref{fig:prompt}. 
In this example, the prompt is structured into four key components to effectively guide the LLM. 
The \textit{context} %provides the background, 
specifies that the task belongs to the cybersecurity domain and that the LLM should approach it as a cybersecurity specialist. 
Following this, the \textit{instruction} clearly defines the task, requiring the LLM to identify and output the relevant attack techniques/sub-techniques corresponding to the provided rule description. 
The \textit{guidelines} set additional constraints, specifying that the response must be formatted in JSON. 
Lastly, the actual rule description is presented as the \textit{input} for the LLM to process and analyze.

 % [bb=0 0 1440 900,width=1.43\linewidth,height=0.9\textwidth]
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.40\textwidth]{Images/prompt.jpg}
    %\vspace{-0.2cm}
    \caption{Overview of prompt structure used in all steps of the pipeline.}
    \label{fig:prompt}
\end{figure} 



%\subsection{Experiments}

\begin{table*}[h]
\centering
\caption{Summary of the models and their configurations.}

\begin{adjustbox}{width=0.9\textwidth,center=\textwidth}
\begin{tabular}{|c|l|l|l|l|l|}
\hline
\multicolumn{1}{|l|}{\textbf{Model Type}} & \textbf{Model Name}  & \textbf{Parameter Size} & \textbf{Context Window} & \textbf{Max Tokens} & \textbf{Knowledge Cut-off} \\ \hline
\multirow{3}{*}{\textbf{Hosted}}          & \textbf{GPT-4-Turbo} & $\sim$1.8 Trillion      & 128,000                  & 4096                & December 2023            \\ \cline{2-6} 
         & \textbf{GPT-4o}      & $\sim$2 Trillion        & 128,000                  & 16,384               & October 2023              \\ \cline{2-6} 
           & \textbf{GPT-4o-mini} & $\sim$8 Billion         & 128,000                  & 16,384               & October 2023              \\ \hline
\multirow{3}{*}{\textbf{Local}}           & \textbf{IBM Granite} & $\sim$8 Billion         & 128,000                  & 8,192     & Not available               \\ \cline{2-6} 
             & \textbf{Mistral}     & $\sim$7 Billion         & 32,000                   & 4,096                & Not available                       \\ \cline{2-6} 
       & \textbf{Qwen}        & $\sim$3 Billion         & 128,000                  & 8,192                & Not available                       \\ \hline
\end{tabular}
\end{adjustbox}

\label{tab:models}
\end{table*}

\subsection{Baselines}

We used three baseline methods to compare \methodName with:
\begin{enumerate}[nosep,leftmargin=*]
    \item \textbf{A single GPT-4-Turbo LLM with zero-shot prompting}.
    The input to the LLM was the rule as-is, and the LLM was asked to analyze the rule and output all the MITRE ATT\&CK techniques or sub-techniques relevant to the rule.
%The results of the baseline are summarized in Table~\ref{tab:results1}.
    \item \textbf{BERT-based classifiers.} Building on the research by Mărmureanu et al.~\cite{10398612}, we trained a BERT model~\cite{devlin2018bert} as a baseline for comparison our experiments. 
    BERT's ability to capture bidirectional context through its masked language modeling (MLM) and next sentence prediction (NSP) objectives makes it particularly effective for classification tasks~\cite{sun2019fine}. 
    Additionally, we implemented CodeBERT~\cite{feng2020codebert} and an adaptation of the BERT model, as baseline models for comparison in our experiments.
    \item \textbf{TTPxHunter.} By implementing the publicly available code from the research proposed by Rani et al.~\cite{rani2024ttpxhunter}, we established an additional baseline for comparison with our method. 
    This approach utilizes SecureBERT to generate embeddings for the input data, which are then processed by a linear classifier to produce the final output.


\end{enumerate}

\subsection{Evaluation Metrics}

To evaluate our framework, we employed evaluation metrics commonly used in classic recommender systems. 
Specifically, we computed the average recall (AR) and average precision (AP) across the entire test dataset. 
In recommender systems, relevance is inherently user-specific, and both AP and AR metrics effectively capture the varying importance of individual recommendations. This characteristic makes these metrics particularly well-suited for multi-class classification problems like MITRE ATT\&CK technique prediction, where a single rule can be associated with multiple valid labels (techniques or sub-techniques).
For instance, the rule illustrated in Figure~\ref{fig:stages}(a), has multiple techniques and sub-techniques mapped to it, including: T1087.002, T1069.001, T1482, T1087.001, T1087, T1069.002, and T1069. 



For computing \textit{AR}, we first calculated the recall for each sample (rule) in the dataset. 
The AR was then calculated by averaging the recall values across all samples in the dataset.
Similarly, to compute the \textit{AP}, we first calculated the precision for each sample in the dataset, and then, the AP was determined by averaging the precision values across all samples in the dataset.


In addition, the task of mapping a SIEM rule to the MITRE ATT\&CK framework is inherently a multi-class classification problem, where the number of techniques associated with each sample varies. 
Appendix~\ref{appendixA} provides an analysis of the distribution of samples based on the number of techniques they contain. 
Therefore, we also compute the Weighted Average Precision (WAP) and Weighted Average Recall (WAR) metrics. 
These metrics apply weights based on the relative number of techniques for each sample. 