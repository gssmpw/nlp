\section{\label{sec:related}Related Work}

Table~\ref{tab:related_work} summarizes the related work on mapping cybersecurity data to attack tactics and techniques.
As can be seen, previous works mainly focused on mapping unstructured data, such as threat intelligence reports and semi-structured data, such as event logs, to MITRE ATT\&CK techniques. 
These efforts proposed both rule-based methods~\cite{kryukov2022mapping} and machine learning (ML)-based approaches~\cite{alam2023looking,alves2022leveraging,fayyazi2023advancing,rani2024ttpxhunter,you2022tim,liu2022threat,fengrui2024few,zhang2024attackgboosting}.

The rule-based method proposed by Kryukov et al.~\cite{kryukov2022mapping} aimed to map security events in SIEM to MITRE ATT\&CK framework using pre-defined rules based on threat patterns.
A key limitation of their method lies in its heavy reliability on rule (or patterns) database. 
While these patterns are essential to carry out accurate mapping, the method's reliance on them constrains its adaptability to newly emerging threats within the dynamic and ever-evolving cybersecurity landscape.
As a result, increased false positives (misidentification of benign activities as threats) and false negatives (failure to detect actual threats) are produced, particularly when new attack patterns or techniques have not been added to the database.

Recent advancements have marked a significant shift from traditional rule-based methods to the adoption of ML-based approaches, particularly the use of language models, for this task.
Language models, such as BERT and GPT, offer the ability to process unstructured text with minimal feature engineering due to their powerful contextual understanding and pre-trained embeddings. 


You et al.~\cite{you2022tim} introduced a classification-based model for mapping unstructured data in the form of CTI reports to the MITRE ATT\&CK framework. 
Their approach utilized a combination of a bi-directional LSTM model and a CNN model to classify unstructured data into just six technique classes, significantly simplifying the task. 
However, the complexity of mapping unstructured data increases substantially when the model must predict across the entire set of technique classes in the MITRE ATT\&CK framework, which includes approximately 670 classes. 
Additionally, it is important to note that an attack pattern or rule can be mapped to multiple techniques, and a single technique may fall under multiple tactics, further complicating the mapping process.
This limitation may restrict the method’s applicability in real-world scenarios where comprehensive coverage of TTP classes is essential.

Liu et al.~\cite{liu2022threat} proposed a novel approach to map unstructured CTI to MITRE ATT\&CK framework. 
Their methodology, referred to as ATHRNN (attention-based transformer hierarchical recurrent neural network), employs a two-step classification process: first, classifying the unstructured text into MITRE ATT\&CK tactics and then further classifying it into MITRE ATT\&CK techniques.
In other work, Alves et al.~\cite{alves2022leveraging} explored the application of BERT models for the classification of unstructured text into MITRE ATT\&CK techniques.
The study uses eleven different BERT models to map unstructured texts to the MITRE ATT\&CK framework, aiming to enhance automation in cyber threat intelligence.

Similarly, Alam et al.~\cite{alam2023looking} proposed LADDER, a framework designed to enhance cybersecurity by automatically extracting attack patterns from CTI sources.
LADDER uses different BERT-based models for extracting attack patterns from unstructured texts and then mapping these patterns to MITRE ATT\&CK framework.
Rani et al.~\cite{rani2024ttpxhunter} proposed TTPXHunter, a method designed for the automated mapping of attack patterns extracted from cyber threat reports to MITRE ATT\&CK framework. 
This method is an extension of TTPHunter~\cite{ttp_hunter}, improving its ability to cover a broader range of techniques from MITRE ATT\&CK framework and precision with the help of a cyber domain-specific language model called SecureBERT.
Sentences are transformed into embeddings using SecureBERT and then sent to a linear classifier for TTP prediction.

Fayyazi et al.~\cite{fayyazi2023advancing} evaluated how well LLMs, specifically encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5) models, can summarize and map cyberattack procedures to the appropriate ATT\&CK tactics.
The authors compared various mapping approaches. 
Howerver, they focused on mapping cyberattack procedures to MITRE ATT\&CK tactics (which represent higher-level categorizations in MITRE ATT\&CK framework). 
While this is useful, it is comparatively easier than mapping cyberattack procedures to MITRE ATT\&CK techniques and sub-techniques, which are more granular and detailed, offering deeper insights into the specific actions and methods employed in an attack.

Fengrui et al.~\cite{fengrui2024few} introduced a method combining data augmentation and instruction supervised fine-tuning using LLMs to classify TTPs effectively in scenarios with limited data.
Similarly, Zhang et al.~\cite{zhang2024attackgboosting} introduced a novel framework for constructing attack knowledge graphs (KGs) from CTI reports, by leveraging LLMs. 

While these methods demonstrate remarkable progress in handling unstructured data, their applicability to structured data use cases, such as mapping SIEM rules, is limited. 
These approaches are specifically designed for unstructured data, where relationships between entities and contextual information are often explicitly defined, simplifying the mapping process to the MITRE ATT\&CK framework. 
Adapting these methods to structured formats like SIEM rules would require extensive modifications, reducing their effectiveness and suitability for such scenarios.

To the best of our knowledge, only two studies have specifically focused on mapping structured data, such as IDS rules and SIEM rules, to the relevant MITRE ATT\&CK technique (or sub-techniques) using language models.

Nir et al.~\cite{daniel2023labeling} investigated the integration of LLMs, specifically ChatGPT, into cybersecurity workflows to automate the association of network intrusion detection system (NIDS) rules with corresponding MITRE ATT\&CK techniques. 
While their method represents one of the first applications of LLMs for this purpose, their findings highlight the necessity of incorporating additional contextual information to enhance the accuracy and reliability of LLM predictions.
Mărmureanu et al.~\cite{10398612} proposed method to map structured data in the form of Splunk rules to MITRE ATT\&CK framework.
The authors proposed the use of ML classifiers to map the Splunk rules to tactics specified in MITRE ATT\&CK framework.
A significant limitation of these methods is their dependence on supervised learning approaches to train the machine learning models within their frameworks. 
These models are unable to dynamically adapt to evolving threat landscapes or newly introduced MITRE ATT\&CK techniques without undergoing retraining with updated datasets. 
This retraining process is not only time-consuming but also resource-intensive, substantially restricting the methods' ability to keep pace with the rapid evolution of cyber threats.

In summary, the limitations of prior studies can be categorized as follows: (1) reliance on supervised learning tasks, (2) inability to adapt to structured texts, and (3) dependence on additional contextual information to effectively interpret rules. To address these shortcomings, we propose \methodName, a novel LLM-based approach that eliminates the need for training data, coherently processes structured text into natural language, and employs LLM agents to retrieve supplementary contextual information, enabling reliable and accurate predictions.


\begin{table*}[h]
\renewcommand{\arraystretch}{1.5} % Adjust row height for vertical centering
\centering
\footnotesize
\caption{Summary of previous work.}

\begin{tabular}{|>{\raggedright\arraybackslash}p{2cm}|>{\raggedright\arraybackslash}p{1.4cm}|>{\raggedright\arraybackslash}p{2.5cm}|>{\raggedright\arraybackslash}p{4.7cm}|>{\raggedright\arraybackslash}p{5.8cm}|}
\hline
\textbf{Paper name} & \textbf{Mapping} & \textbf{Input data (\& type)} & \textbf{Method} & \textbf{Comments}\\ \hline
Kryukov et al.~\cite{kryukov2022mapping} (2022) & Techniques & SIEM alerts log (semi-structured) & Rule-based mapping & Coverage of all technique classes. No quantitative metric was provided.\\ \hline

Alves et al.~\cite{alves2022leveraging} (2022) & Techniques & MITRE Procedures (unstructured) & Train BERT model as classifier & Achieved classification accuracy of 0.82 on test dataset. Coverage of only 253 techniques in their evaluation. \\ \hline

You et al.~\cite{you2022tim} (2022) & Techniques & CTI reports (unstructured) & Use pre-trained Sentence-BERT for embeddings; train bi-LSTM with attention coupled with CNN for classification & Evaluation performed on only six techniques which extremely simplifies the classification task. Classification accuracy on six techniques is 0.94. \\ \hline

Liu et al.~\cite{liu2022threat} (2022) & Techniques & CTI reports (unstructured) & Train transformer and RNN-based model as classifier & Coverage of all technique classes. Achieved an AUC score of 0.76 during the classification task.\\ \hline

Fayyazi et al.~\cite{fayyazi2023advancing} (2023) & Tactics & MITRE Procedures (unstructured) & RAG-based approach to improve LLM performance & Only 14 classes available for classification. Achived a high F1 score of 0.95 when using RAG to fetch external data.\\ \hline

Alam et al.~\cite{alam2023looking} (2023) & Techniques & CTI reports (unstructured) & Train BERT model as classifier & Achieved TTP classification recall of 0.63. \\ \hline

Rani et al.~\cite{rani2024ttpxhunter} (2024) & Techniques & CTI reports (unstructured) & Use SecureBERT for embeddings and train a linear classifier & The dataset consisted of only 193 Technique classes. Achieved a recall of 0.96 on augmented test dataset. \\ \hline

Fengrui et al.~\cite{fengrui2024few} (2024) & Techniques & MITRE Procedures (unstructured) & LLM fine-tuning with MITRE data & Achieved a recall of 0.89 when the number of samples in the fine-tuning dataset is more than 33. 
When the sample size is less, the recall achieved is 0.43. \\ \hline

Zhang et al.~\cite{zhang2024attackgboosting} (2024) & Techniques & CTI reports (unstructured) & Use LLM for similarity matching & Overall recall achieved for technique identification is 0.59. \\ \hline

Nir et al.~\cite{daniel2023labeling} (2024) & Techniques & NIDS rules (structured) & Use LLM's implicit knowledge & Maximum recall achieved with ChatGPT-4 is 0.68. \\ \hline

Mărmureanu et al.~\cite{10398612} (2023) & Tactics & SIEM rules (structured) & Train BERT model as classifier & Only 14 classes available for classification. With weight based ensemble learning strategy, achieved a recall of 0.72\\ \hline\hline

\textbf{Our method (\methodName)} & Techniques & SIEM rules (structured) & Use prompt engineering techniques and implement LLM agents to enhance LLM performance & Coverage of all technique level classes. Achieved a recall of 0.75 on the test rules. \\ \hline
\end{tabular}
%\smallskip
\label{tab:related_work}
\end{table*}

