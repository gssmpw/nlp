
\section{\label{sec:method}Methodology}

Each SIEM system uses its own RDL to define threat detection rules, and each RDL has its own schema.
For example, the Splunk SIEM uses the SPL to define its threat detection rules.
The task of understanding threat detection rules and recommending relevant MITRE ATT\&CK techniques (or sub-techniques) requires complex reasoning skills.
In the case of LLMs, this can be achieved with a technique called prompt chaining in which each task is divided into multiple sub-tasks in order to understand the complex reasoning behind the task.
Therefore, we employ a multi-phase architecture based on prompt chaining that leverages the power of LLMs to take a SIEM rule defined in any RDL and map it to relevant MITRE ATT\&CK techniques using the power of LLMs.
Our approach is based on the following intuitions:
\begin{itemize}[nosep,leftmargin=*]
    \item \textit{LLMs' implicit knowledge}: LLMs possess deep understanding of diverse RDLs. This enables them to interpret any rule, regardless of the RDL it is defined in, and convert it into comprehensible natural language text.
    \item \textit{LLMs' similarity comparison capability}: LLMs are adept at analyzing and comparing textual descriptions. 
    They can intelligently assess the similarity between two textual inputs to establish a meaningful connection.
\end{itemize}

\methodName has two main phases: (1) the rule to text translation phase, and (2) the MITRE ATT\&CK techniques recommendation phase.
These two phases in the pipeline include six key steps to determine relevant TTPs, as illustrated in Figure~\ref{fig:r2t}.

Although LLMs excel at translating SIEM rules into natural language, they often lack critical domain-specific contextual information related to IoCs in the rules.
To overcome this limitation, the \textit{rule to text translation} phase includes three steps: IoC extraction, contextual information retrieval, and natural language translation.

The workflow begins with the extraction of IoCs from the rules (for example, processes, log source, event codes, and file names) that the rule searches for in the logs (step (1)).In the next sstep a web search agent performs the task of obtaining additional contextual information about the IoCs discovered ((step 2)).
By incorporating this additional domain-specific information, the pipeline enhances the language translation, resulting in a more accurate and meaningful interpretation of SIEM rules.
The rule itself and the IoCs' contextual additional information from the previous stage are then used to translate the rule from RDL to natural language (step (3)).

The \textit{MITRE ATT\&CK techniques} recommendation phase of the pipeline includes the following three steps.
The rule in processed in data source identification step in which the probable origin of the data is identified (step (4)).
The description of the rule is then used to determine probable MITRE ATT\&CK techniques based on the implicit knowledge of the LLM (step (5)).
Finally, using chain-of-thought~\cite{wei2022chain} prompting, the most relevant techniques are extracted from the list of probable techniques (step (6)).
Each step of our method is further described in detail below.


% [bb=0 0 1440 900,width=1.43\linewidth,height=0.9\textwidth]
\begin{figure*}[htbp]
   \includegraphics[width=\textwidth]{Images/stages.jpg}
    
   \caption{An illustration of the different steps in \methodName.}
   \label{fig:stages}
\end{figure*} 

\subsection{IoC Extraction}
The context associated with a SIEM detection rule is crucial for its accurate interpretation and effective application. 
Obtaining this contextual understanding requires comprehensive analysis of the embedded IoCs in the SIEM rule.
In the first step, \methodName systematically identifies and extracts all IoCs, identifying the types of IoCs and their corresponding values that form the foundational elements of the detection rules. 
Leveraging the LLM's inherent understanding of rule structures and IoCs, we employ a zero-shot prompting approach for this task. 
Zero-shot prompting enables the direct extraction of IoCs from the rules without requiring extensive pre-training on specific datasets.

\noindent The result of this stage is a dictionary structure, where:
\begin{itemize}[nosep,leftmargin=*]
    \item Keys represent types of IoC, such as processes, files, IP addresses, and log sources.
    \item Values are lists containing specific IoC details, such as process names, file names, IP addresses, and log source identifiers.
\end{itemize}

In the example depicted in Figure~\ref{fig:stages}(a), the pipeline processes a rule for which relevant MITRE ATT\&CK techniques need to be recommended. 
The IoC extractor LLM produces a dictionary structure as output, organizing the IoCs in a structured format to support subsequent stages in the analysis pipeline. 



\subsection{Contextual Information Retrieval}
In this step, an LLM agent is employed to retrieve relevant information pertaining to the IoCs extracted from the rule.
A REACT agent~\cite{react} was used in this case to generate both reasoning traces and task-specific actions in an interleaved manner.
REACT agents interact with external tools to retrieve additional information that leads to more factual and reliable responses.
The LLM agent conducts a systematic search across web resources to gather additional contextual information for each IoC value present in the rule. 
This step addresses LLMS' lack of up-to-date knowledge or specialized domain expertise (which is critical to understanding the role and significance of the IoCs in the rule), without the need for retraining or fine-tuning.
Figure~\ref{fig:stages}(b) presents an example in which the rule includes the process name \texttt{soaphound.exe} as an IoC.
As can be seen, the web search results indicate that \texttt{soaphound.exe} is being used for active directory (AD) enumeration, which is important for the understanding of the attack. 

\subsection{Natural Language Translation}

The translation of detection rules into natural language textual descriptions fulfills three key objectives:
\begin{enumerate}[nosep,leftmargin=*]
    \item \textbf{Ensures that \methodName is format-agnostic}: It converts rules defined in various RDL formats into a generic, unstructured text format, ensuring compatibility with different SIEM systems, regardless of the specific rule format.
    \item \textbf{Provides contextual explanation}: It includes all relevant contextual information to produce a concise and comprehensible explanation of the rule.
    \item \textbf{Enhances the comprehension for LLMs}: It enables LLMs to more effectively compare the translated rule with descriptions in the MITRE ATT\&CK framework by providing a unified textual representation.
\end{enumerate}
To achieve these objectives, a zero-shot prompting technique is employed. 
The input to the LLM comprises two components:
\begin{itemize}
    \item \textbf{Syntactical information}: The rule itself, providing the structural and operational details.
    \item \textbf{Contextual information}: Details of the IoCs extracted from the rule, providing semantic insights into the rule's intent and function.
\end{itemize}
The LLM utilizes these inputs to generate a natural language textual description of the rule. 
This transformation not only ensures a more interpretable representation but also facilitates further steps of analysis and comparison, particularly in aligning the rule with MITRE ATT\&CK techniques and sub-techniques.



\subsection{Data Source or Mitigation Identification}
Identifying the most relevant data component or mitigation associated with the rule description in this step is critical for filtering out irrelevant MITRE ATT\&CK techniques (or sub-techniques) in subsequent steps of the pipeline.
In the MITRE ATT\&CK framework, data sources represent various categories of information that can be gathered from sensors or logs. 
These data sources include \textit{data components}, which are specific attributes or properties within a data source that are directly relevant to detecting a particular technique or sub-technique~. 
For example, in the context of the rule described in Figure~\ref{fig:stages}(a), the term \texttt{Endpoint.Processes} indicates that the activity is happening on an endpoint. 
Presence of the terms such as, \texttt{soaphound.exe}, \texttt{--buildcache}, \texttt{--certdump} and etc. indicate that the rule searches for command line execution of an executable named \texttt{soaphound.exe} with specific parameters. 
Therefore, the appropriate data source in this example is \textit{Command}, with the corresponding data component being \textit{Command Execution}.
Additionally, \textit{mitigations} are defined as categories of technologies or strategies that can prevent or reduce the impact of specific techniques or sub-techniques. 
The MITRE ATT\&CK framework explicitly establishes relationships between data components, mitigations, and techniques (or sub-techniques), enabling a systematic approach for identifying relevant elements.

To identify the most relevant data component or mitigation associated with a given rule description, we utilize agentic retrieval augmented generation (RAG), which incorporates an AI Agent-based implementation of the RAG framework.
Data from the MITRE ATT\&CK framework, specifically related to data components and mitigations, is stored in a vector database (e.g., ChromaDB). 
The process begins with the rule description from the previous stage, which serves as the input to the AI Agent. 
The LLM-powered agent automatically generates a search query tailored to retrieve relevant information from the RAG database.

For each query, the system retrieves the five most similar documents from the database, each containing contextual information about data components or mitigations. 
These documents are then utilized by the LLM agent to contextualize the rule description. 
By comparing the content of these retrieved documents with the rule description, the LLM agent determines and outputs the most relevant data component or mitigation along with a chain-of-thought as to why the data component or mitigation is related to the rule.


\subsection{Probable Technique Recommendation}

In this step, an LM agent is utilized to propose probable MITRE ATT\&CK techniques (and sub-techniques) that may be relevant to the description of the provided rule.
We used a REACT agent in this step as well to utilize both implicit and explicit knowledge during reasoning.
For explicit knowledge, the agent searches the MITRE ATT\&CK framework to obtain the list of probable techniques (and sub-techniques).
The natural language description of the rule from the previous step serves as input to the LLM agent.
The output of this stage consists of a list of JSON objects, each containing the MITRE technique ID, technique name, and technique description as seen in Figure~\ref{fig:stages}(c).

Throughout our experiments, we observed that as the number of recommendations ($k$) increases, both the framework's average recall and precision initially improve, however beyond a certain threshold of $k$, the %average 
precision begins to decline.
Based on these observations(please refer Table~\ref{tab:results3}), we selected a $k$-value of 11 to ensure a high recall.



\subsection{Relevant Technique Extraction}
In this step, \methodName refines the set of probable MITRE ATT\&CK techniques identified in the previous stage by eliminating irrelevant entries.
This step in the pipeline serves two primary purposes: (1) to enhance precision while maintaining recall achieved in previous step, and (2) to provide a clear rationale for the selection of the labels, ensuring transparency and interpretability of the mapping process.
This refinement process is grounded in the assumption that LLMs are effective for text similarity matching tasks.

The process comprises two key steps:
\begin{itemize}
    \item \textit{Rule-technique comparison}: The description of each technique in the set of probable techniques is compared with the rule description. 
    A chain-of-thought technique is then applied to elucidate the reasoning behind the association of each technique with the rule.
    \item \textit{Confidence calculation}: The generated chain-of-thought rationale for each technique (or sub-technique) is compared with the rule description to compute a relevance (or confidence) score, as done in prior work~\cite{freitas2024ai}.
    % \item \textbf{Reasoning}: \new{Add here the reasoning that it provides - explaining in NLP why it was selected...}
\end{itemize}

Techniques with higher confidence scores are deemed more relevant to the rule. 
Conversely, techniques with scores falling below a predefined threshold are excluded.
The techniques retained after this filtering step represent the most relevant techniques corresponding to the given rule's description. 


The chain-of-thought (CoT) rationale generated during the comparison of each rule to its probable technique is also provided as an output in this step.
This rationale offers a detailed natural language explanation, articulating why a particular technique is relevant to the given rule. 
Such explanations are highly valuable for security analysts, as they provide clear and transparent reasoning behind the mapping, enabling analysts to better understand and validate the association between the rule and the technique.
Other classification models proposed in previous works within this domain also suffer from the limitation of being black-box models, which lack the ability to provide clear reasoning or explanations. 
Unlike \methodName, these models fail to generate transparent, CoT rationales that explain why a particular rule is mapped to a specific technique, making them less interpretable and less useful for security analysts.