\subsection{Results}

\paragraph{\textbf{Comparison of \methodName with baselines.}}
We compared \methodName with the baseline methods described in the previous section, and the results of this comparison are presented in Table~\ref{tab:Results2}. 
As evident from the table, \methodName outperformed the baseline methods when utilizing GPT-4-Turbo and GPT-4o as the underlying models. 
These findings underscore the critical role of both implicit and explicit domain-specific knowledge in enabling LLMs to deliver optimal results for the task of mapping SIEM rules to the MITRE ATT\&CK framework.

\paragraph{\textbf{\methodName's performance with different LLMs.}}
\methodName's pipeline  was executed using several LLMs to evaluate its ability to recommend MITRE ATT\&CK techniques (or sub-techniques) for a given SIEM rule. 
As can be seen in Table~\ref{tab:Results2}, GPT-4-Turbo demonstrated superior performance, delivering the most accurate and relevant recommendations when compared to other hosted and local models. 
These results provided insights into how the model configuration, including model size and architectural differences, influence the quality of recommendations generated by \methodName.


\paragraph{\textbf{Effect of model's size and context window length on performance.}}
As can be seen in Table~\ref{tab:Results2}, \methodName's performance is significantly influenced by the size of the LLMs used in its pipeline (see Table~\ref{tab:models} for different models used and their sizes and context window lengths used).
Larger models, such as GPT-4-Turbo and GPT-4o, demonstrated superior performance compared to their smaller counterparts like GPT-4o-mini or local models with fewer parameters.
This performance disparity highlights the ability of larger models to better understand complex relationships and patterns in the input data. 
The large number of parameters and greater context window allow them to capture nuanced information that smaller models might overlook, leading to more accurate and reliable recommendations.

The context window of an LLM refers to the maximum amount of text, measured in tokens, that the model can process at a time. 
This parameter plays a crucial role in determining the model's ability to handle tasks requiring extensive context.
In the case of \methodName, the performance using the Mistral model was poorer compared to other models. 
This can be attributed to the significantly smaller context window of the Mistral model, which limited its ability to process and utilize the full breadth of information required for effective recommendations.
In contrast, GPT-4o-mini, a similar model with a larger context window, performed better as it could handle more comprehensive inputs, leading to improved accuracy and reliability in mapping SIEM rules to MITRE ATT\&CK techniques



\begin{table}[h]
\centering
\caption{\methodName's performance using various hosted and local models.}
\begin{tabular}{|c|l|c|c|}
\hline
\textbf{Model Type}              & \multicolumn{1}{c|}{\textbf{Model Name}} & \textbf{AR (WAR)} & \textbf{AP (WAP)} \\ \hline
\multirow{3}{*}{\textbf{Hosted}} & \textbf{GPT-4-Turbo}                     & \textbf{0.75 (0.724)}           & \textbf{0.52 (0.51)}                \\ \cline{2-4} 
       & \textbf{GPT-4o}                          & 0.71 (0.69)                  & 0.49 (0.47)                         \\ \cline{2-4} 
         & \textbf{GPT-4o-mini}                     & 0.38 (0.36)                    & 0.29 (0.275)                        \\ \hline
\multirow{3}{*}{\textbf{Local}}  & \textbf{IBM Granite}                     & 0.34 (0.31)                    & 0.28 (0.24)                         \\ \cline{2-4} 
       & \textbf{Mistral}                         & 0.12 (0.09)                   & 0.08 (0.05)                        \\ \cline{2-4} 
    & \textbf{Qwen}                            & 0.23 (0.19)                    & 0.16 (0.15)                        \\ \hline\hline

\multirow{4}{*}{\textbf{Baselines}} & \textbf{Zero-shot LLM}                     & 0.46 (0.43)           & 0.31 (0.30)                \\ \cline{2-4} 
       & \textbf{BERT}                          &  0.68 (0.65)                   &    0.39 (0.38)                     \\ \cline{2-4} 
        & \textbf{CodeBERT}                  &  0.65 (0.63)                   &  0.47 (0.45)                      \\ \cline{2-4}
        & \textbf{TTPxHunter}~\cite{rani2024ttpxhunter}                     & 0.59 (0.56)                    & 0.42 (0.41)                        \\ \hline
\end{tabular}
\label{tab:Results2}
\end{table}


\paragraph{\textbf{Ablation study.}}
In order to analyze the importance of the different components and steps in \methodName, we performed an ablation study, mainly focusing on the impact of language translation and additional contextual information on the recommendations made by the LLM.
We performed the ablation study by running the experiment in three different scenario.
In the first scenario, the rule was provided as-is to the next stage of the pipeline; in this case, \methodName achieved an AR of approximately 0.46 and an AP of around 0.39.
In the second scenario, the rule was first translated into a natural language description, without adding any contextual information, before being passed to subsequent stages of the pipeline. 
This improved the AR to around 0.54 and the AP to approximately 0.42.
Finally, in the third scenario, the rule was translated into a natural language description enriched with contextual information before being processed by the later stages of the pipeline. 
This setup yielded the best results, with an AR of around 0.75 and an AP of approximately 0.51.
In these experiments GPT-4-Turbo was used, given its superior performance in the previous experiments performed.
A summary of the results is presented in Table \ref{tab:results1}.



These findings highlight the importance of contextual information in enhancing \methodName's performance, demonstrating that enriching natural language translations with domain-specific insights leads to improved recall and precision.

\begin{table}[h]
\centering
\caption{Impact of contextual information that enriches natural language translation with domain-specific insights on \methodName's performance.}
\begin{tabular}{|c|l|c|c|}
\hline
\textbf{S.No.} & \multicolumn{1}{c|}{\textbf{Experiment}}                                                           & \textbf{AR (WAR)} & \textbf{AP (WAP)} \\ \hline
\textbf{1}     & \textbf{Rule as-is}                                                                                & 0.46 (0.42)                   & 0.39 (0.36)                      \\ \hline
\textbf{2}     & \textbf{\begin{tabular}[c]{@{}l@{}}Rule description w/o \\ contextual   information\end{tabular}}  & 0.54 (0.49) & 0.42 (0.40)                      \\ \hline
\textbf{3}     & \textbf{\begin{tabular}[c]{@{}l@{}}Rule description with \\ contextual   information\end{tabular}} & \textbf{0.75 (0.724) }           & \textbf{0.52 (0.51) }              \\ \hline
\end{tabular}

\label{tab:results1}
\end{table}
 

\paragraph{\textbf{Effect of $k$ on relevant recommendations.}}

Our preliminary experiments demonstrated that limiting $k$ substantially influenced \methodName's performance. 
Larger $k$ values  improved recall by capturing a broader range of potentially relevant recommendations, but this came at the cost of reduced precision. 
This trade-off highlights the necessity of carefully selecting $k$ to balance precision and recall according to the specific needs of the application.

To address this, we replaced the hard limit on $k$ with a filtering mechanism based on the confidence (relevance) score generated in the final stage of the pipeline. Recommendations with scores below a predefined threshold were excluded. We used a threshold of 0.8, which effectively filtered low-confidence recommendations while retaining the most relevant results. This approach improved the overall performance of the model by dynamically adjusting the number of recommendations based on their relevance. \methodName's results for different $k$ values, using the abovementioned filtering mechanism, are presented in Table~\ref{tab:results3}.


\begin{table}[h]
\centering
\caption{\methodName's performance  for various values of $k$.}

\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{S.No.} & \textbf{k}          & \textbf{AR (WAR)} & \textbf{AP (WAP)} & \textbf{F1-Score} \\ \hline
\textbf{1}     & 1                                         & 0.45 (0.44) & 0.29 (0.26)                   & 0.35              \\ \hline
\textbf{2}     & 3                                         & 0.62 (0.58)   & 0.34 (0.3)                & 0.44              \\ \hline
\textbf{3}     & 5                                       & 0.66 (0.636)       & 0.45 (0.43)                & 0.53               \\ \hline
\textbf{4}     & 7                                        & 0.71 (0.68)     & 0.42 (0.40)                & 0.527               \\ \hline\textbf{5}     & 9                                       & 0.74 (0.72)     & 0.40 (0.35)           & 0.52              \\ \hline
\textbf{6}     & 11                                       & 0.75 (0.728)  & 0.39 (0.38)                   & 0.51              \\ \hline
\textbf{7}     & 13                                         & 0.75 (0.72)      & 0.35 (0.31)              & 0.48              \\ \hline
\textbf{7}     & \textbf{dynamic-k}               & \textbf{0.75 (0.724)}   & \textbf{0.52 (0.51)}         & \textbf{0.62}     \\ \hline
\end{tabular}
\label{tab:results3}
\end{table}

% [bb=0 0 1440 900,width=1.43\linewidth,height=0.9\textwidth]
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{Images/output.jpg}
    %\vspace{-0.2cm}
    \caption{Average precision vs average recall curve.}
    \label{fig:pr-curve}
\end{figure}

\paragraph{\textbf{Reasoning.}} 
One major advantage of using LLMs over other language models like BERT or CodeBERT is their inherent ability to provide reasoning for the tasks they perform. 
In \methodName's pipeline, the Rule-Technique Comparer step plays a crucial role by comparing the rule to the technique in question and generating a chain-of-thought explanation. 
This explanation, expressed in natural language, provides the relationship between the rule and the technique. 
Such transparency allows security analysts to interpret the rationale behind the mapping, enabling them to assess whether \methodName's output can be trusted or requires further scrutiny.
For example, refer to SIEM rule mentioned in the Figure~\ref{fig:stages}(a) as input.
This rule relates to the execution of an executable file called \texttt{soaphound.exe} on a system.
One of the labels for this rule is "T1482 - Domain Trust Discovery".
Figure~\ref{fig:cot} provides the CoT explanation as to why the technique T1482 is relevant to the SIEM rule.


\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]%, height=0.8\linewidth]
    {Images/cot.jpg}
    %\vspace{-0.2cm}
    \caption{Chain-of-thought reasoning provided by \methodName.}
    \label{fig:cot}
\end{figure}

\section{Discussion}



The use of \methodName for mapping SIEM rules to the MITRE ATT\&CK framework offers several distinct advantages. 
Unlike traditional approaches, \methodName does not require any training data, making it particularly well-suited for the cybersecurity domain, where publicly available data is often scarce. 
Additionally, the incorporation of LLMs within the pipeline enables the generation of natural language reasoning, facilitating easier interpretation of results. 
This transparency enhances the trust of security analysts on the framework, as they can better understand and validate the mapping process.

While \methodName demonstrated promising results, our study has certain limitations. 
One of the primary concerns is confidentiality, as \methodName's pipeline relies on hosted models, which may involve sending sensitive data to external servers. 
Additionally, the use of prompt chaining in the pipeline leads to longer response times compared to baseline methods. 
The duration of processing varies based on the resources available when employing local models, while for hosted models, it depends on factors such as network bandwidth and the tokens-per-second (TPS) rate of the model.


The results of our study also revealed several challenges associated with the research, which are discussed below:

\noindent \textbf{Insufficient Rule-Specific Information}: The information embedded within a rule alone was found to be inadequate for fully understanding its purpose. 
Additional contextual data such as the associated common vulnerabilities and exposures (CVE) \footnote{\url{https://cve.mitre.org/}} ID often proved necessary to interpret the rule accurately.
For example, consider the following rule:
\begin{verbatim}
| tstats `security_content_summariesonly` count FROM
datamodel=Endpoint.Registry
where Registry.registry_path="*\InProcServer32\*" 
Registry.registry_value_data=*\FORMS\* 
by Registry.registry_path Registry.registry_key_name
Registry.registry_value_name 
Registry.registry_value_data Registry.dest
Registry.process_guid Registry.user 
  | `drop_dm_object_name(Registry)` 
  | `security_content_ctime(firstTime)` 
  | `security_content_ctime(lastTime)`
\end{verbatim}
\vspace{0.2cm}
\noindent From the information provided in the rule, it is impossible to infer that this rule searches for phishing activity through \texttt{outlook.exe} process unless the CVE ID related to it is known which is "CVE-2024-21378".

\noindent \textbf{Impact of Textual Similarities on LLM Accuracy}: The high degree of similarity in textual descriptions of various MITRE ATT\&CK techniques and sub-techniques led to instances of hallucination by the language models. 
This overlap in descriptions posed a significant challenge to ensuring accurate predictions.

\noindent \textbf{Dataset Mislabeling}: The importance of a technique (or sub-technique) to a specific rule differed based on the user's perspective. 
This led to inconsistencies in how the dataset was labeled, with several cases being mislabeled. 
These errors highlight that determining relevance is often subjective.
As an example, consider the following rule,\\

\begin{verbatim}
    | tstats `security_content_summariesonly` 
    count min(_time) as firstTime max(_time) 
    as lastTime from datamodel=Change.All_Changes 
    where  All_Changes.result="*locked out*" by 
    All_Changes.user All_Changes.result 
    |`drop_dm_object_name("All_Changes")` 
    |`drop_dm_object_name("Account_Management")`
    | `security_content_ctime(firstTime)` 
    | `security_content_ctime(lastTime)` 
    | search count > 5 
\end{verbatim}
\vspace{0.2cm}
This rule aims to identify instances where excessive user account lockouts are being recorded. The label assigned to this rule in the dataset is "T1078 - Valid Accounts," which is an accurate label. 
However, the dataset does not include all the relevant labels for this rule. 
In this case, the technique "T1110 - Brute Force" is also a relevant technique that should have been mapped to the rule but was not included in the dataset.
Despite these imperfections in the labeling, the Splunk Security Content dataset provided the most reliable ground truth available for our study.
% \new{1. show an example here as well for a case that a TTP in the groundtruth was missing and one that was irrelevant. 2. mention that although we this that the label is not perfect, still this is the best ground truth that we could use.}

\noindent \textbf{Technique vs. Sub-Technique Prediction Challenges}: In some cases, the LLM predicted a technique instead of a sub-technique (or vice versa). 
Under stricter evaluation criteria that was used to evaluate our experiments, such discrepancies were categorized as mismatches, which negatively influenced \methodName's performance metrics of \methodName. 
These observations underline the need for more precise distinction mechanisms during prediction tasks.

