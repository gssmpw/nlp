\section{Related Work}
LLMs~\cite{openai2023gpt4, touvron2023llama, abdin2024phi, guo2025deepseek} have demonstrated remarkable performance across diverse tasks. Chain-of-Thought (CoT)~\cite{wei2022chain, zhou2022least} prompting has emerged as an effective strategy for generating step-by-step rationale to derive answers. However, for complex multi-step reasoning problems, LLMs often underutilize critic information from earlier steps as rationale getting longer due to their tendency to lose focus on middle-context information~\cite{peysakhovich2023attention, junqing2023never, hsieh2024found}. Additionally, they frequently generate redundant steps with repeated conclusions, leading to repetitive reasoning loops and error accumulation~\cite{dziri2024faith, furuta2024exposing}. These difficulties are especially pronounced in smaller-scale LLMs with limited reasoning capacity~\cite{fu2023specializing}.
An intuitive method is to prompt LLMs for more concise outputs. However, LLMs often struggle to maintain output quality under length constraints, and simple prompting alone fails to resolve grounding and redundancy issue~\cite{nayab2024concise, han2024token}. 

This inspires generating multiple rationales and determine the most likely solution using majority voting~\cite{wang2022self} or best-of-N~\cite{wang2024math}. However, they are computationally expensive due to the exponentially growing search space when integrating diverse solutions.
To reduce the search space, recent studies have applied tree search techniques with scoring mechanisms to prioritize promising candidates at each step, such as stepwise beam search~\cite{xie2024self}, Tree-of-Thought~\cite{yao2024tree}, and Monte Carlo Tree Search~\cite{jiang2024technical, feng2023alphazero, zhang2024rest}. While effective, they face practical limitations, relying on extensive rollouts~\cite{wang2024math, wang2024q} and intensive annotations~\cite{lightman2023let} for training specialized reward models. Additionally, they introduce latency due to interactions with external or self-evaluators during autoregressive decoding~\cite{xie2024self, yao2024tree}, and fail to address the grounding and redundancy issues we focus on in this work.