[
  {
    "index": 0,
    "papers": [
      {
        "key": "openai2023gpt4",
        "author": "OpenAI",
        "title": "GPT-4 Technical Report"
      },
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      },
      {
        "key": "abdin2024phi",
        "author": "Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others",
        "title": "Phi-3 technical report: A highly capable language model locally on your phone"
      },
      {
        "key": "guo2025deepseek",
        "author": "Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others",
        "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      },
      {
        "key": "zhou2022least",
        "author": "Zhou, Denny and Sch{\\\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others",
        "title": "Least-to-most prompting enables complex reasoning in large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "peysakhovich2023attention",
        "author": "Peysakhovich, Alexander and Lerer, Adam",
        "title": "Attention sorting combats recency bias in long context language models"
      },
      {
        "key": "junqing2023never",
        "author": "Junqing, He and Kunhao, Pan and Xiaoqun, Dong and Zhuoyang, Song and Yibo, Liu and Yuxin, Liang and Hao, Wang and Qianguo, Sun and Songxin, Zhang and Zejian, Xie and others",
        "title": "Never lost in the middle: Improving large language models via attention strengthening question answering"
      },
      {
        "key": "hsieh2024found",
        "author": "Hsieh, Cheng-Yu and Chuang, Yung-Sung and Li, Chun-Liang and Wang, Zifeng and Le, Long T and Kumar, Abhishek and Glass, James and Ratner, Alexander and Lee, Chen-Yu and Krishna, Ranjay and others",
        "title": "Found in the middle: Calibrating positional attention bias improves long context utilization"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dziri2024faith",
        "author": "Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and others",
        "title": "Faith and fate: Limits of transformers on compositionality"
      },
      {
        "key": "furuta2024exposing",
        "author": "Furuta, Hiroki and Matsuo, Yutaka and Faust, Aleksandra and Gur, Izzeddin",
        "title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "fu2023specializing",
        "author": "Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and Khot, Tushar",
        "title": "Specializing smaller language models towards multi-step reasoning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "nayab2024concise",
        "author": "Nayab, Sania and Rossolini, Giulio and Buttazzo, Giorgio and Manes, Nicolamaria and Giacomelli, Fabrizio",
        "title": "Concise thoughts: Impact of output length on llm reasoning and cost"
      },
      {
        "key": "han2024token",
        "author": "Han, Tingxu and Fang, Chunrong and Zhao, Shiyu and Ma, Shiqing and Chen, Zhenyu and Wang, Zhenting",
        "title": "Token-Budget-Aware LLM Reasoning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2024math",
        "author": "Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang",
        "title": "Math-shepherd: Verify and reinforce llms step-by-step without human annotations"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "xie2024self",
        "author": "Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, James Xu and Kan, Min-Yen and He, Junxian and Xie, Michael",
        "title": "Self-evaluation guided beam search for reasoning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yao2024tree",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "jiang2024technical",
        "author": "Jiang, Jinhao and Chen, Zhipeng and Min, Yingqian and Chen, Jie and Cheng, Xiaoxue and Wang, Jiapeng and Tang, Yiru and Sun, Haoxiang and Deng, Jia and Zhao, Wayne Xin and others",
        "title": "Technical report: Enhancing llm reasoning with reward-guided tree search"
      },
      {
        "key": "feng2023alphazero",
        "author": "Feng, Xidong and Wan, Ziyu and Wen, Muning and McAleer, Stephen Marcus and Wen, Ying and Zhang, Weinan and Wang, Jun",
        "title": "Alphazero-like tree-search can guide large language model decoding and training"
      },
      {
        "key": "zhang2024rest",
        "author": "Zhang, Dan and Zhoubian, Sining and Hu, Ziniu and Yue, Yisong and Dong, Yuxiao and Tang, Jie",
        "title": "Rest-mcts*: Llm self-training via process reward guided tree search"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wang2024math",
        "author": "Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang",
        "title": "Math-shepherd: Verify and reinforce llms step-by-step without human annotations"
      },
      {
        "key": "wang2024q",
        "author": "Wang, Chaojie and Deng, Yanchen and Lyu, Zhiyi and Zeng, Liang and He, Jujie and Yan, Shuicheng and An, Bo",
        "title": "Q*: Improving multi-step reasoning for llms with deliberative planning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "lightman2023let",
        "author": "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
        "title": "Let's verify step by step"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "xie2024self",
        "author": "Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, James Xu and Kan, Min-Yen and He, Junxian and Xie, Michael",
        "title": "Self-evaluation guided beam search for reasoning"
      },
      {
        "key": "yao2024tree",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      }
    ]
  }
]