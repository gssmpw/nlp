\section{Related Work}
LLMs____ have demonstrated remarkable performance across diverse tasks. Chain-of-Thought (CoT)____ prompting has emerged as an effective strategy for generating step-by-step rationale to derive answers. However, for complex multi-step reasoning problems, LLMs often underutilize critic information from earlier steps as rationale getting longer due to their tendency to lose focus on middle-context information____. Additionally, they frequently generate redundant steps with repeated conclusions, leading to repetitive reasoning loops and error accumulation____. These difficulties are especially pronounced in smaller-scale LLMs with limited reasoning capacity____.
An intuitive method is to prompt LLMs for more concise outputs. However, LLMs often struggle to maintain output quality under length constraints, and simple prompting alone fails to resolve grounding and redundancy issue____. 

This inspires generating multiple rationales and determine the most likely solution using majority voting____ or best-of-N____. However, they are computationally expensive due to the exponentially growing search space when integrating diverse solutions.
To reduce the search space, recent studies have applied tree search techniques with scoring mechanisms to prioritize promising candidates at each step, such as stepwise beam search____, Tree-of-Thought____, and Monte Carlo Tree Search____. While effective, they face practical limitations, relying on extensive rollouts____ and intensive annotations____ for training specialized reward models. Additionally, they introduce latency due to interactions with external or self-evaluators during autoregressive decoding____, and fail to address the grounding and redundancy issues we focus on in this work.