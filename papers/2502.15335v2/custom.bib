% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{boghossian2014inference,
  title={What is inference?},
  author={Boghossian, Paul},
  journal={Philosophical studies},
  volume={169},
  number={1},
  pages={1--18},
  year={2014},
  publisher={Springer}
}

@article{wang2024can,
  title={Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs},
  author={Wang, Siyuan and Wei, Zhongyu and Choi, Yejin and Ren, Xiang},
  journal={arXiv preprint arXiv:2402.11442},
  year={2024}
}

@article{chen2024premise,
  title={Premise Order Matters in Reasoning with Large Language Models},
  author={Chen, Xinyun and Chi, Ryan A and Wang, Xuezhi and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.08939},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{creswell2022selection,
  title={Selection-inference: Exploiting large language models for interpretable logical reasoning},
  author={Creswell, Antonia and Shanahan, Murray and Higgins, Irina},
  journal={arXiv preprint arXiv:2205.09712},
  year={2022}
}

@article{sinha2019clutrr,
  title={CLUTRR: A diagnostic benchmark for inductive reasoning from text},
  author={Sinha, Koustuv and Sodhani, Shagun and Dong, Jin and Pineau, Joelle and Hamilton, William L},
  journal={arXiv preprint arXiv:1908.06177},
  year={2019}
}

@article{hardman2016reasoning,
  title={Reasoning and memory: People make varied use of the information available in working memory.},
  author={Hardman, Kyle O and Cowan, Nelson},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={42},
  number={5},
  pages={700},
  year={2016},
  publisher={American Psychological Association}
}

@article{kim2023entity,
  title={Entity tracking in language models},
  author={Kim, Najoung and Schuster, Sebastian},
  journal={arXiv preprint arXiv:2305.02363},
  year={2023}
}

@article{tafjord2020proofwriter,
  title={ProofWriter: Generating implications, proofs, and abductive statements over natural language},
  author={Tafjord, Oyvind and Mishra, Bhavana Dalvi and Clark, Peter},
  journal={arXiv preprint arXiv:2012.13048},
  year={2020}
}

@article{zhong2021ar,
  title={Ar-lsat: Investigating analytical reasoning of text},
  author={Zhong, Wanjun and Wang, Siyuan and Tang, Duyu and Xu, Zenan and Guo, Daya and Wang, Jiahai and Yin, Jian and Zhou, Ming and Duan, Nan},
  journal={arXiv preprint arXiv:2104.06598},
  year={2021}
}

@book{apt1997logic,
  title={From logic programming to Prolog},
  author={Apt, Krzysztof R and others},
  volume={362},
  year={1997},
  publisher={Prentice Hall London}
}

@article{lanchantin2024learning,
  title={Learning to reason and memorize with self-notes},
  author={Lanchantin, Jack and Toshniwal, Shubham and Weston, Jason and Sukhbaatar, Sainbayar and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{saparov2022language,
  title={Language models are greedy reasoners: A systematic formal analysis of chain-of-thought},
  author={Saparov, Abulhair and He, He},
  journal={arXiv preprint arXiv:2210.01240},
  year={2022}
}

@article{tamari2021dyna,
  title={Dyna-bAbI: unlocking bAbI's potential with dynamic synthetic benchmarking},
  author={Tamari, Ronen and Richardson, Kyle and Sar-Shalom, Aviad and Kahlon, Noam and Liu, Nelson and Tsarfaty, Reut and Shahaf, Dafna},
  journal={arXiv preprint arXiv:2112.00086},
  year={2021}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{pan2023logic,
  title={Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning},
  author={Pan, Liangming and Albalak, Alon and Wang, Xinyi and Wang, William Yang},
  journal={arXiv preprint arXiv:2305.12295},
  year={2023}
}

@misc{zhang2024cumulative,
      title={Cumulative Reasoning with Large Language Models}, 
      author={Yifan Zhang and Jingqin Yang and Yang Yuan and Andrew Chi-Chih Yao},
      year={2024},
      eprint={2308.04371},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{de2008z3,
  title={Z3: An efficient SMT solver},
  author={De Moura, Leonardo and Bj{\o}rner, Nikolaj},
  booktitle={International conference on Tools and Algorithms for the Construction and Analysis of Systems},
  pages={337--340},
  year={2008},
  organization={Springer}
}

@article{xu2024faithful,
  title={Faithful Logical Reasoning via Symbolic Chain-of-Thought},
  author={Xu, Jundong and Fei, Hao and Pan, Liangming and Liu, Qian and Lee, Mong-Li and Hsu, Wynne},
  journal={arXiv preprint arXiv:2405.18357},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@inproceedings{park2023generative,
  title={Generative agents: Interactive simulacra of human behavior},
  author={Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  booktitle={Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--22},
  year={2023}
}

@article{li2022large,
  title={Large language models with controllable working memory},
  author={Li, Daliang and Rawat, Ankit Singh and Zaheer, Manzil and Wang, Xin and Lukasik, Michal and Veit, Andreas and Yu, Felix and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2211.05110},
  year={2022}
}

@article{guo2023empowering,
  title={Empowering Working Memory for Large Language Model Agents},
  author={Guo, Jing and Li, Nan and Qi, Jianchuan and Yang, Hang and Li, Ruiqiao and Feng, Yuzhen and Zhang, Si and Xu, Ming},
  journal={arXiv preprint arXiv:2312.17259},
  year={2023}
}

@article{yue2024fragrel,
  title={FragRel: Exploiting Fragment-level Relations in the External Memory of Large Language Models},
  author={Yue, Xihang and Zhu, Linchao and Yang, Yi},
  journal={arXiv preprint arXiv:2406.03092},
  year={2024}
}

@article{wang2024augmenting,
  title={Augmenting language models with long-term memory},
  author={Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{lee2024human,
  title={A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts},
  author={Lee, Kuang-Huei and Chen, Xinyun and Furuta, Hiroki and Canny, John and Fischer, Ian},
  journal={arXiv preprint arXiv:2402.09727},
  year={2024}
}

@article{lu2024longheads,
  title={LongHeads: Multi-Head Attention is Secretly a Long Context Processor},
  author={Lu, Yi and Zhou, Xin and He, Wei and Zhao, Jun and Ji, Tao and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2402.10685},
  year={2024}
}

@article{hu2023chatdb,
  title={Chatdb: Augmenting llms with databases as their symbolic memory},
  author={Hu, Chenxu and Fu, Jie and Du, Chenzhuang and Luo, Simian and Zhao, Junbo and Zhao, Hang},
  journal={arXiv preprint arXiv:2306.03901},
  year={2023}
}

@article{yoneda2023statler,
  title={Statler: State-maintaining language models for embodied reasoning},
  author={Yoneda, Takuma and Fang, Jiading and Li, Peng and Zhang, Huanyu and Jiang, Tianchong and Lin, Shengjie and Picker, Ben and Yunis, David and Mei, Hongyuan and Walter, Matthew R},
  journal={arXiv preprint arXiv:2306.17840},
  year={2023}
}

@article{liu2020logiqa,
  title={Logiqa: A challenge dataset for machine reading comprehension with logical reasoning},
  author={Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
  journal={arXiv preprint arXiv:2007.08124},
  year={2020}
}

@article{wang2021logic,
  title={Logic-driven context extension and data augmentation for logical reasoning of text},
  author={Wang, Siyuan and Zhong, Wanjun and Tang, Duyu and Wei, Zhongyu and Fan, Zhihao and Jiang, Daxin and Zhou, Ming and Duan, Nan},
  journal={arXiv preprint arXiv:2105.03659},
  year={2021}
}

@article{zhu2023large,
  title={Large language models can learn rules},
  author={Zhu, Zhaocheng and Xue, Yuan and Chen, Xinyun and Zhou, Denny and Tang, Jian and Schuurmans, Dale and Dai, Hanjun},
  journal={arXiv preprint arXiv:2310.07064},
  year={2023}
}

@article{qiu2023phenomenal,
  title={Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement},
  author={Qiu, Linlu and Jiang, Liwei and Lu, Ximing and Sclar, Melanie and Pyatkin, Valentina and Bhagavatula, Chandra and Wang, Bailin and Kim, Yoon and Choi, Yejin and Dziri, Nouha and others},
  journal={arXiv preprint arXiv:2310.08559},
  year={2023}
}

@article{wang2022lsat,
  title={From lsat: The progress and challenges of complex reasoning},
  author={Wang, Siyuan and Liu, Zhongkun and Zhong, Wanjun and Zhou, Ming and Wei, Zhongyu and Chen, Zhumin and Duan, Nan},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={30},
  pages={2201--2216},
  year={2022},
  publisher={IEEE}
}

@article{sanyal2022fairr,
  title={Fairr: Faithful and robust deductive reasoning over natural language},
  author={Sanyal, Soumya and Singh, Harman and Ren, Xiang},
  journal={arXiv preprint arXiv:2203.10261},
  year={2022}
}

@article{goswami2010inductive,
  title={Inductive and deductive reasoning},
  author={Goswami, Usha},
  journal={The Wiley-Blackwell handbook of childhood cognitive development},
  pages={399--419},
  year={2010},
  publisher={Wiley Online Library}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{ling2024deductive,
  title={Deductive verification of chain-of-thought reasoning},
  author={Ling, Zhan and Fang, Yunhao and Li, Xuanlin and Huang, Zhiao and Lee, Mingu and Memisevic, Roland and Su, Hao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{berglund2023reversal,
  title={The Reversal Curse: LLMs trained on" A is B" fail to learn" B is A"},
  author={Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  journal={arXiv preprint arXiv:2309.12288},
  year={2023}
}

@article{lee2024symba,
  title={SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning},
  author={Lee, Jinu and Hwang, Wonseok},
  journal={arXiv preprint arXiv:2402.12806},
  year={2024}
}

@article{chen2023learning,
  title={Learning to teach large language models logical reasoning},
  author={Chen, Meiqi and Ma, Yubo and Song, Kaitao and Cao, Yixin and Zhang, Yan and Li, Dongsheng},
  journal={arXiv preprint arXiv:2310.09158},
  year={2023}
}

@inproceedings{olausson-etal-2023-linc,
    title = "{LINC}: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
    author = "Olausson, Theo  and
      Gu, Alex  and
      Lipkin, Ben  and
      Zhang, Cedegao  and
      Solar-Lezama, Armando  and
      Tenenbaum, Joshua  and
      Levy, Roger",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
}

@article{sun2023indeterminacy,
  title={From Indeterminacy to Determinacy: Augmenting Logical Reasoning Capabilities with Large Language Models},
  author={Sun, Hongda and Xu, Weikai and Liu, Wei and Luan, Jian and Wang, Bin and Shang, Shuo and Wen, Ji-Rong and Yan, Rui},
  journal={arXiv preprint arXiv:2310.18659},
  year={2023}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@article{dziri2024faith,
  title={Faith and fate: Limits of transformers on compositionality},
  author={Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{fu2023specializing,
  title={Specializing smaller language models towards multi-step reasoning},
  author={Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and Khot, Tushar},
  booktitle={International Conference on Machine Learning},
  pages={10421--10430},
  year={2023},
  organization={PMLR}
}

@article{wang2024q,
  title={Q*: Improving multi-step reasoning for llms with deliberative planning},
  author={Wang, Chaojie and Deng, Yanchen and Lyu, Zhiyi and Zeng, Liang and He, Jujie and Yan, Shuicheng and An, Bo},
  journal={arXiv preprint arXiv:2406.14283},
  year={2024}
}

@inproceedings{wang2024math,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9426--9439},
  year={2024}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{xie2024self,
  title={Self-evaluation guided beam search for reasoning},
  author={Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, James Xu and Kan, Min-Yen and He, Junxian and Xie, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{furuta2024exposing,
  title={Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web},
  author={Furuta, Hiroki and Matsuo, Yutaka and Faust, Aleksandra and Gur, Izzeddin},
  booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
  year={2024}
}

@article{mccoy2023embers,
  title={Embers of autoregression: Understanding large language models through the problem they are trained to solve},
  author={McCoy, R Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Matthew and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2309.13638},
  year={2023}
}

@article{nayab2024concise,
  title={Concise thoughts: Impact of output length on llm reasoning and cost},
  author={Nayab, Sania and Rossolini, Giulio and Buttazzo, Giorgio and Manes, Nicolamaria and Giacomelli, Fabrizio},
  journal={arXiv preprint arXiv:2407.19825},
  year={2024}
}

@article{zhu2024deductive,
  title={Deductive beam search: Decoding deducible rationale for chain-of-thought reasoning},
  author={Zhu, Tinghui and Zhang, Kai and Xie, Jian and Su, Yu},
  journal={arXiv preprint arXiv:2401.17686},
  year={2024}
}

@article{zhang2024accessing,
  title={Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b},
  author={Zhang, Di and Huang, Xiaoshui and Zhou, Dongzhan and Li, Yuqiang and Ouyang, Wanli},
  journal={arXiv preprint arXiv:2406.07394},
  year={2024}
}

@article{han2024token,
  title={Token-Budget-Aware LLM Reasoning},
  author={Han, Tingxu and Fang, Chunrong and Zhao, Shiyu and Ma, Shiqing and Chen, Zhenyu and Wang, Zhenting},
  journal={arXiv preprint arXiv:2412.18547},
  year={2024}
}

@article{jaech2024openai,
  title={OpenAI o1 System Card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}

@article{chen2024not,
  title={Do NOT Think That Much for 2+ 3=? On the Overthinking of o1-Like LLMs},
  author={Chen, Xingyu and Xu, Jiahao and Liang, Tian and He, Zhiwei and Pang, Jianhui and Yu, Dian and Song, Linfeng and Liu, Qiuzhi and Zhou, Mengfei and Zhang, Zhuosheng and others},
  journal={arXiv preprint arXiv:2412.21187},
  year={2024}
}

@article{zhang2023tell,
  title={Tell your model where to attend: Post-hoc attention steering for llms},
  author={Zhang, Qingru and Singh, Chandan and Liu, Liyuan and Liu, Xiaodong and Yu, Bin and Gao, Jianfeng and Zhao, Tuo},
  journal={arXiv preprint arXiv:2311.02262},
  year={2023}
}

@article{han2022folio,
  title={Folio: Natural language reasoning with first-order logic},
  author={Han, Simeng and Schoelkopf, Hailey and Zhao, Yilun and Qi, Zhenting and Riddell, Martin and Zhou, Wenfei and Coady, James and Peng, David and Qiao, Yujie and Benson, Luke and others},
  journal={arXiv preprint arXiv:2209.00840},
  year={2022}
}

@article{wang2024mmlu,
  title={Mmlu-pro: A more robust and challenging multi-task language understanding benchmark},
  author={Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others},
  journal={arXiv preprint arXiv:2406.01574},
  year={2024}
}

@article{rein2023gpqa,
  title={Gpqa: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2311.12022},
  year={2023}
}


@article{hsieh2024found,
  title={Found in the middle: Calibrating positional attention bias improves long context utilization},
  author={Hsieh, Cheng-Yu and Chuang, Yung-Sung and Li, Chun-Liang and Wang, Zifeng and Le, Long T and Kumar, Abhishek and Glass, James and Ratner, Alexander and Lee, Chen-Yu and Krishna, Ranjay and others},
  journal={arXiv preprint arXiv:2406.16008},
  year={2024}
}

@article{feng2023alphazero,
  title={Alphazero-like tree-search can guide large language model decoding and training},
  author={Feng, Xidong and Wan, Ziyu and Wen, Muning and McAleer, Stephen Marcus and Wen, Ying and Zhang, Weinan and Wang, Jun},
  journal={arXiv preprint arXiv:2309.17179},
  year={2023}
}

@article{zhang2024rest,
  title={Rest-mcts*: Llm self-training via process reward guided tree search},
  author={Zhang, Dan and Zhoubian, Sining and Hu, Ziniu and Yue, Yisong and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2406.03816},
  year={2024}
}

@article{luo2024improve,
  title={Improve Mathematical Reasoning in Language Models by Automated Process Supervision},
  author={Luo, Liangchen and Liu, Yinxiao and Liu, Rosanne and Phatale, Samrat and Lara, Harsh and Li, Yunxuan and Shu, Lei and Zhu, Yun and Meng, Lei and Sun, Jiao and others},
  journal={arXiv preprint arXiv:2406.06592},
  year={2024}
}

@article{zhang2025lessons,
  title={The lessons of developing process reward models in mathematical reasoning},
  author={Zhang, Zhenru and Zheng, Chujie and Wu, Yangzhen and Zhang, Beichen and Lin, Runji and Yu, Bowen and Liu, Dayiheng and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2501.07301},
  year={2025}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@misc{cuadron2025dangeroverthinkingexaminingreasoningaction,
      title={The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks}, 
      author={Alejandro Cuadron and Dacheng Li and Wenjie Ma and Xingyao Wang and Yichuan Wang and Siyuan Zhuang and Shu Liu and Luis Gaspar Schroeder and Tian Xia and Huanzhi Mao and Nicholas Thumiger and Aditya Desai and Ion Stoica and Ana Klimovic and Graham Neubig and Joseph E. Gonzalez},
      year={2025},
      eprint={2502.08235},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.08235}, 
}

@article{peysakhovich2023attention,
  title={Attention sorting combats recency bias in long context language models},
  author={Peysakhovich, Alexander and Lerer, Adam},
  journal={arXiv preprint arXiv:2310.01427},
  year={2023}
}

@article{junqing2023never,
  title={Never lost in the middle: Improving large language models via attention strengthening question answering},
  author={Junqing, He and Kunhao, Pan and Xiaoqun, Dong and Zhuoyang, Song and Yibo, Liu and Yuxin, Liang and Hao, Wang and Qianguo, Sun and Songxin, Zhang and Zejian, Xie and others},
  journal={arXiv preprint arXiv:2311.09198},
  year={2023}
}

@article{jiang2024technical,
  title={Technical report: Enhancing llm reasoning with reward-guided tree search},
  author={Jiang, Jinhao and Chen, Zhipeng and Min, Yingqian and Chen, Jie and Cheng, Xiaoxue and Wang, Jiapeng and Tang, Yiru and Sun, Haoxiang and Deng, Jia and Zhao, Wayne Xin and others},
  journal={arXiv preprint arXiv:2411.11694},
  year={2024}
}