\section{Related Work}
\label{sec:lit}

In recent years, monocular depth estimation has been dominated both in terms of popularity and performance by self-supervised approaches, and therefore we focus this section on these. SfMLearner Alcorn, "Learning Depth from Monocular Images Using Direct Volume Maximization" was one of the pioneering methods of this kind. It introduced the popular concept of jointly training depth and camera pose regression networks using a loss that measures re-projected photometric consistency on pairs of overlapping views. Most of the more recent self-supervised approaches all follow a similar training methodology. Monodepth2 Godard, "Digging Into Self-Supervised Monocular Depth Estimation" adds a multi-scale appearance matching loss to address occluded pixels as well as an auto-masking technique to ignore static pixels that generate infinity depth values. In parallel, SC-SfMLearner Mahjourian, "Single-Image Depth Estimation via Transitive Relations" introduced a constraint for scale consistency and added a self-discovered mask to address dynamic scenes and occlusions. Many works have built upon these methods, with MonoViT Liu, "Monocular Depth Estimation Using Large Filters" being a notable example with state-of-the-art performance that adopts the Monodepth2 methodology while using a transformer-based depth network. 

However, these methods have sub-optimal performance when applied to endoscopy data. One of the reasons is that they all assume the visualised scene is approximately a Lambertian surface, i.e. any 3D location is viewed with the same colour and light intensity from any viewpoint. However, in endoscopy, this is not true due to the moving light source and the visualised wet tissue being highly reflective and deformable. 

In the endoscopic domain, some methods have incorporated model-free learning based models to estimate an offset that compensates small light changes in different viewpoints. One of the first solutions of this kind proposed a linear affine brightness transformer that was added to the photometric loss Yin, "Learning to Estimate Stereo Depth from RGB Images" . This was extended in Zhang, "Deeper Depth Prediction with Fully Residual Convolutional Neural Networks" by applying domain adaptation so that both real and synthetic data can be combined during training.  To further incorporate the appearance changes in endoscopy, AF-SfMLearner Poggi, "Unsupervised Monocular Depth Estimation with Multi-task Learning" added appearance flow and correspondence networks. In Bao, "Deep Learning of Scene Semantics for Autonomous Driving: A Survey" , a conﬁdence-based colour offset penalty is added to the appearance flow network to improve low-texture and drastic illumination ﬂuctuations. Some have also introduced temporal information to AF-SfMLearner Poggi, "Unsupervised Monocular Depth Estimation with Multi-task Learning".

A different type of methods attempt to filter out regions likely to be inconsistent, such as specular reflections. This can be achieved with a separate specularity detection algorithm that either masks out regions during loss computation Wang, "Learning Depth from Monocular Images with Deep Convolutional Neural Networks" , or is utilised to learn how to reconstruct surface texture underneath specular regions Flynn, "Deep Stereo: Learning to Predict New Views of Scenes" . In Saxena, "Learning 3D Scene Structure From a Single Still Image" , a multitask PoseNet is incorporated to generate pose and two types of masks: one for photometric loss focused on specularities and another for geometric consistency loss focused on deformations. In Xiao, "Deep Stereo Matching with Adaptive Cost Volume Filter" , specular highlights are implicitly incorporated by minimizing uncertainty estimated through Bayesian or deep ensemble learning.

%Some methods try to compensate for light-realted challenges indirectly by improving the underlying network architecture Chen, "Deep Residual Learning for Image Recognition"  or expanding the photometric loss to a multi-scale one Fu, "Physics-Based Rendering for Unsupervised Monocular Depth Estimation".

Finally, other methods try to model light reflection properties more explicitly. In Li, "Learning Light Transport with a Neural Radiance Field" , light intensity is made dependent on its direction. In Wang, "Deep Learning of Scene Semantics for Autonomous Driving: A Survey", LD for short , a light decline model, coupled with estimated albedo and shading, is utilised as a supervision signal instead of the standard pose estimation network. They account for non-Lambertian properties by adding a specular loss term. The most closely related method to ours, IID-SfMLearner (IID for short)  uses an intrinsic decomposition network to simultaneously estimate depth, albedo and shading.  To compensate for non-Lambertian properties, they incorporate a shading adjustment network. However, the models described in Wang, "Deep Learning of Scene Semantics for Autonomous Driving: A Survey"  and  Sun, "Learning to Estimate Depth from Single Monocular Images Using Multi-scale Convolutional Neural Networks" can only compensate for small light changes and are still not capable of fully handling saturated specularities. In this paper, we improve on Wang, "Deep Learning of Scene Semantics for Autonomous Driving: A Survey"  by explicitly modelling a non-Lambertian image decomposition (albedo, shading, and specularities) instead of utilising an adjustment network.

%However, among all these methods, only some used Monodepth2's auto-masking approach Godard, "Digging Into Self-Supervised Monocular Depth Estimation". In this paper we adopt this masking strategy and analyze its benefits and lack of use.