\section{Introduction}

The Holy Grail of mechanism design is the \emph{truthful mechanism} --- a mechanism in which the (weakly) dominant strategy of each agent is truthfully reporting her type.
But in most settings, there is provably no truthful mechanism that satisfies other desirable properties such as budget-balance, efficiency or fairness. 
% Practical mechanisms are thus \emph{manipulable} in the sense that, for some agent~$a_i$, and \emph{some} combination of reports by the other agents, agent~$a_i$ can induce the mechanism to yield an outcome (strictly) better for her by reporting  non-truthfully.
% 
Practical mechanisms are thus \emph{manipulable} in the sense that some agent~$a_i$ has a \emph{profitable} manipulation -- for \emph{some} combination of reports by the other agents, agent~$a_i$ can induce the mechanism to yield an outcome (strictly) better for her by reporting non-truthfully.


% some agent $i$ has an untruthful report with which she can cause the mechanism to yield a (strictly) better outcome for her (will cause that for \emph{some} combination of reports by the other agents). 
% 
% 


This notion of manipulability implicitly assumes that the manipulating agent either knows the reports made by all other agents, or is willing to take the risk and act \emph{as-if} she knows their reports. 
Without knowledge of the others' reports, most manipulations are \emph{risky} -- they might decrease the manipulator's utility for some other combinations of reports by the other agents.
In practice, many agents are \emph{risk-avoiding} and will not manipulate in such cases. 
This highlights a gap between the standard definition and the nature of such agents. 
%
% The standard definition does not fully capture the nature of such agents.
%
% does not fully capture scenarios involving such agents
% knowing the reports of all other agents is very challenging.

To illustrate, consider a simple example in the context of voting. Under the Plurality rule, agents vote for their favorite candidate, and the candidate with the most votes wins. If an agent knows that her preferred candidate has no chance of winning, she may find it beneficial to vote for her second-choice candidate to prevent an even less preferred candidate from winning. However, if the agent lacks precise knowledge of the other votes and decides to vote for her second choice, it may backfire -- she might inadvertently cause an outcome worse than if she had voted truthfully. For instance, if the other agents vote in a way that makes the agent the tie-breaker.


Indeed, various papers on cake-cutting
(e.g. \cite{brams2006better,BU2023Rat}),
voting (e.g. \cite{slinko2008nondictatorial,slinko2014ever,hazon2010complexity})
 stable matching (e.g. \citet{regret2018Fernandez, chen2024regret})
 and coalition formation \citep{waxman2021manipulation}
studies truthfulness among such agents.
% \footnote{In these papers, they are called risk-averse -- see \ref{} for more details.} e.g., \cite{brams2006better,BU2023Rat}  \eden{to add more papers about truthfulness for risk averse agents}
%
In particular, \citet{BU2023Rat} introduced a weaker notion of truthfulness, suitable for risk-avoiding agents, for cake-cutting. 
Their definition can be adapted to any problem as follows.

% Recently, \cite{BU2023Rat} laid the foundation for this research.
% Indeed, recent work by \citet{BU2023Rat} introduces a weaker notion of truthfulness, suitable for risk-averse agents, for cake cutting. 
% Their definition forms the foundation for this research and can be easily adapted to any problem as follows.

Let us first define a \emph{safe manipulation} as a non-truthful report that 
% is a dominant strategy for an agent; that is, it may improve, and 
may never harm the agent's utility. 
% Based on this, a mechanism is Risk-Avoiding Truthful (RAT) if no agent has a manipulation that is both profitable and safe.
Based on that, a mechanism is \emph{safely manipulable} if some agent~$a_i$ has a manipulation that is both profitable and safe; otherwise, the mechanism is Risk-Avoiding Truthful (RAT).

Standard truthfulness and RAT can be seen as two extremes with respect to safe-and-profitable manipulations: the former considers manipulators with complete knowledge of
others, whereas the latter considers manipulators with no knowledge at all. 
In reality, agents often know about some — but not all — of the other agents.

This paper introduces the \emph{RAT-Degree} — a new measurement that quantifies how robust a mechanism is to such safe-and-profitable manipulations.
The RAT-Degree of a mechanism is an integer $d \in \{0, \ldots, n\}$, which represents --- roughly --- 
the smallest number of agents whose reports, if known, may allow another agent to safely manipulate; or $n$ if there is no such number.   (See \Cref{sec:RAT-degree} for formal definition).


This measure allows us to position mechanisms along a spectrum. A higher degree implies that an agent has to work harder in order to collect the information required for a successful manipulation; therefore it is less likely that mechanism will be manipulated.
%
% that reflects how robust they are to manipulations by risk-avoiding agents. 
%
On one end of the spectrum are truthful mechanisms -- where no agent can safely manipulate even with complete knowledge of all the other agents. The RAT-degree of such mechanisms is $n$.
While on the other end are mechanisms that are safely manipulable -- no knowledge about other agents is required to safely manipulate. The RAT-degree of such mechanisms is $0$.

Importantly, the RAT-degree is determined by the worst-case scenario for the mechanism designer, which corresponds to the best-case scenario for the manipulating agents.
The way we measure the amount of knowledge is based on a general objective applicable to all social choice settings.


\paragraph{Contributions.}
Our main contribution is the definition of the RAT-degree.


To illustrate the generality and usefulness of this concept, we selected several different social choice domains, and analyzed the RAT-degree of some prominent mechanisms in each domain.
As our goal is mainly to illustrate the new concepts, we did not attempt to analyze all mechanisms and all special case of each mechanism, but rather focused on some cases that allowed for a more simple analysis. 
%
To prove an upper bound on the RAT-degree, we need to show a profitable manipulation. However, in contrast to usual proofs of manipulability, we also have to analyze more carefully, how much knowledge on other agents is sufficient in order to guarantee the safety of the manipulation.
\er{This analysis gives more insight on the kind of manipulations possible in each mechanism, and on potential ways to avoid them.}

\eden{TODO: decide if we want to write a short description of each problem.\\
We start by considering various auctions for a single good, and show that there exists a spectrum of different degrees between the two well-known first-price and second-price auctions.
\\
Next, we focus on allocations of indivisible goods between agents with additive utilities \Cref{sec:RAT-degree}.
\\
Then, }

\paragraph{Organization.} \Cref{sec:preliminaries} introduces the model and required definitions. \Cref{sec:RAT-degree} presents the definition of RAT-degree. 
%
\Cref{sec:single-item-auction} explores auctions for a single good. \Cref{sec:indivisible-good-aloc} examines indivisible goods allocations. 
\Cref{sec:indivisible-good-aloc} focuses on cake cutting.  
\Cref{sec:single-winner-voting} addresses single-winner ranked voting.
\Cref{sec:matching} considers stable matchings.
\Cref{sec:discussion} concludes with some future work directions.

Due to space constraints, most proofs are delegated to appendices, but we have attempted to provide intuitive proof sketches in the paper body.

