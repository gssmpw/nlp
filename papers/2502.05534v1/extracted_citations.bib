@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{ahuja2019language2pose,
  title={Language2pose: Natural language grounded pose forecasting},
  author={Ahuja, Chaitanya and Morency, Louis-Philippe},
  booktitle={2019 International Conference on 3D Vision (3DV)},
  pages={719--728},
  year={2019},
  organization={IEEE}
}

@article{athanasiou2023sinc,
  title={SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation},
  author={Athanasiou, Nikos and Petrovich, Mathis and Black, Michael J and Varol, G{\"u}l},
  journal={arXiv preprint arXiv:2304.10417},
  year={2023}
}

@inproceedings{chen2023executing,
  title={Executing your Commands via Motion Diffusion in Latent Space},
  author={Chen, Xin and Jiang, Biao and Liu, Wen and Huang, Zilong and Fu, Bin and Chen, Tao and Yu, Gang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18000--18010},
  year={2023}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{ghosh2021synthesis,
  title={Synthesis of compositional animations from textual descriptions},
  author={Ghosh, Anindita and Cheema, Noshaba and Oguz, Cennet and Theobalt, Christian and Slusallek, Philipp},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1396--1406},
  year={2021}
}

@article{gilardi2023chatgpt,
  title={Chatgpt outperforms crowd-workers for text-annotation tasks},
  author={Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  journal={arXiv preprint arXiv:2303.15056},
  year={2023}
}

@inproceedings{guo2022generating,
  title={Generating diverse and natural 3d human motions from text},
  author={Guo, Chuan and Zou, Shihao and Zuo, Xinxin and Wang, Sen and Ji, Wei and Li, Xingyu and Cheng, Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5152--5161},
  year={2022}
}

@inproceedings{guo2022tm2t,
  title={Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts},
  author={Guo, Chuan and Zuo, Xinxin and Wang, Sen and Cheng, Li},
  booktitle={European Conference on Computer Vision},
  pages={580--597},
  year={2022},
  organization={Springer}
}

@article{jiang2024motiongpt,
  title={Motiongpt: Human motion as a foreign language},
  author={Jiang, Biao and Chen, Xin and Liu, Wen and Yu, Jingyi and Yu, Gang and Chen, Tao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{kalakonda2023action,
  title={Action-gpt: Leveraging large-scale language models for improved and generalized action generation},
  author={Kalakonda, Sai Shashank and Maheshwari, Shubh and Sarvadevabhatla, Ravi Kiran},
  booktitle={2023 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={31--36},
  year={2023},
  organization={IEEE}
}

@inproceedings{kim2023flame,
  title={Flame: Free-form language-based motion synthesis \& editing},
  author={Kim, Jihoon and Kim, Jiseob and Choi, Sungjoon},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={7},
  pages={8255--8263},
  year={2023}
}

@inproceedings{petrovich2022temos,
  title={TEMOS: Generating diverse human motions from textual descriptions},
  author={Petrovich, Mathis and Black, Michael J and Varol, G{\"u}l},
  booktitle={European Conference on Computer Vision},
  pages={480--497},
  year={2022},
  organization={Springer}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@inproceedings{tevet2022motionclip,
  title={Motionclip: Exposing human motion generation to clip space},
  author={Tevet, Guy and Gordon, Brian and Hertz, Amir and Bermano, Amit H and Cohen-Or, Daniel},
  booktitle={European Conference on Computer Vision},
  pages={358--374},
  year={2022},
  organization={Springer}
}

@ARTICLE{zhang2022motiondiffuse,
  author={Zhang, Mingyuan and Cai, Zhongang and Pan, Liang and Hong, Fangzhou and Guo, Xinying and Yang, Lei and Liu, Ziwei},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={MotionDiffuse: Text-Driven Human Motion Generation With Diffusion Model}, 
  year={2024},
  volume={},
  number={},
  pages={1-15},
  keywords={Pipelines;Task analysis;Noise reduction;Transformers;Training;Probabilistic logic;Decoding;Conditional motion generation;diffusion model;motion synthesis;text-driven generation},
  doi={10.1109/TPAMI.2024.3355414}
}

@inproceedings{zhang2023generating,
  title={Generating Human Motion From Textual Descriptions With Discrete Representations},
  author={Zhang, Jianrong and Zhang, Yangsong and Cun, Xiaodong and Zhang, Yong and Zhao, Hongwei and Lu, Hongtao and Shen, Xi and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14730--14740},
  year={2023}
}

@article{zhang2023remodiffuse,
  title={ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model},
  author={Zhang, Mingyuan and Guo, Xinying and Pan, Liang and Cai, Zhongang and Hong, Fangzhou and Li, Huirong and Yang, Lei and Liu, Ziwei},
  journal={arXiv preprint arXiv:2304.01116},
  year={2023}
}

@article{zhang2024finemogen,
  title={FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing},
  author={Zhang, Mingyuan and Li, Huirong and Cai, Zhongang and Ren, Jiawei and Yang, Lei and Liu, Ziwei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{zhong2023attt2m,
  title={Attt2m: Text-driven human motion generation with multi-perspective attention mechanism},
  author={Zhong, Chongyang and Hu, Lei and Zhang, Zihao and Xia, Shihong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={509--519},
  year={2023}
}

