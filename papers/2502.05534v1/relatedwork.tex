\section{Related Work}
\label{sec2}

\subsection{Text Driven Human Motion Generation}\label{sec2_1}

% While existing works such as JL2P \citep{ahuja2019language2pose} and Ghosh et al. \citep{ghosh2021synthesis} achieved progress utilizing joint embeddings and hierarchical encoders capturing coarse relationships, as well as techniques like MotionCLIP \citep{tevet2022motionclip} that generates stylized motions by projecting into a shared space learned via CLIP \citep{radford2021learning}, TEMOS \citep{petrovich2022temos} combining motion and text VAEs, and temporal VAE \citep{guo2022generating} for sequence generation, their limitation is loss of fine-grained details when encoding independently. Specifically, projecting ``walk while stumbling to the left" will easily lose unique details such as ``stumbling'' and ``left''. This inability to capture precise syntax-kinematic associations at multiple granularities hinders the generation of motions fully specified by rich descriptions.

% Autoregressive models such as TM2T \citep{guo2022tm2t}, which learns mutual mappings of motion and tokens via vector quantized VAEs, T2M-GPT \citep{zhang2023generating}, which enhances performance with EMA and code resetting, and AttT2M \citep{zhong2023attt2m} mapping to refined codes via body part attention, have achieved progress in representing motion as discrete tokens. However, their limitation lies in the unidirectional prediction, which hinders modeling bidirectional motion dependencies. For example, when predicting ``walk with arms swinging", future arm swings depend on past poses. The inability to capture such temporal correlations increases training/inference cost and reduces motion adaptability to conditional inputs.

% Recent diffusion-based models, such as MotionDiffuse \citep{zhang2022motiondiffuse}, MDM \citep{tevet2023human}, and FLAME \citep{kim2023flame}, have shown promising performance in T2M tasks by leveraging conditional diffusion to learn probabilistic text-motion mappings. Also, MLD \citep{chen2023executing} employs latent diffusion to enhance efficiency, while ReMoDiffuse \citep{zhang2023remodiffuse} incorporates sample retrieval for contextual understanding. However, they may suffer from a lack of fidelity in generating motions that precisely align with conditional inputs, particularly in capturing complex multi-modal relationships. For instance, directly diffusing phrases like ``jump while clapping" might struggle to accurately depict synchronized hand-arm-body motions. This limitation underscores the need for approaches with stronger conditioning capabilities.

While latent space alignment works such as JL2P \citep{ahuja2019language2pose} and Ghosh et al. \citep{ghosh2021synthesis} achieved progress utilizing joint embeddings and hierarchical encoders capturing coarse relationships, as well as techniques like MotionCLIP \citep{tevet2022motionclip} that generates stylized motions by projecting into a shared space learned via CLIP \citep{radford2021learning}, TEMOS \citep{petrovich2022temos} combining motion and text VAEs, and temporal VAE \citep{guo2022generating} for sequence generation, their limitation is loss of fine-grained details when encoding independently. 

Autoregressive models such as TM2T \citep{guo2022tm2t}, which learns mutual mappings of motion and tokens via vector quantized VAEs, T2M-GPT \citep{zhang2023generating}, which enhances performance with EMA and code resetting, and AttT2M \citep{zhong2023attt2m} mapping to refined codes via body part attention, have achieved progress in representing motion as discrete tokens. However, the unidirectional nature of autoregressive models limits their ability to capture future context, affecting motion quality. Incorporating bidirectional dependencies could improve this but increase training and inference costs due to the additional computational complexity involved.

Recent diffusion-based models, such as MotionDiffuse \citep{zhang2022motiondiffuse}, MDM \citep{tevet2023human}, and FLAME \citep{kim2023flame}, have shown promising performance in T2M tasks by leveraging conditional diffusion to learn probabilistic text-motion mappings. Also, MLD \citep{chen2023executing} employs latent diffusion to enhance efficiency, while ReMoDiffuse \citep{zhang2023remodiffuse} incorporates sample retrieval for contextual understanding. However, they may suffer from a lack of fidelity in generating motions that precisely align with conditional inputs, particularly in capturing complex multi-modal relationships. 

% \textcolor{blue}{Existing methods perform relatively well on coarse-grained text, such as ``a person is walking." However, they still struggle with fine-grained text. For example, ``a person is walking with the right hand raising while stumbling to the left." The latent space alignment method loses a significant feature detail during feature projection, and may only process the common ``walking" motion. While autoregressive models, due to the unidirectional prediction feature, may overlook the influence of subsequent movements, such as ``stumbling," when generating the earlier movement of ``walking," leading to motion incoherence. Existing diffusion-based models, due to insufficient feature extraction and fusion between the denoising sequence and the text, may ignore the finer details, such as ``right hand raising'' or ``stumbling to the left".}

Existing methods perform relatively well on coarse-grained text, such as ``a person is walking.'' However, they struggle with fine-grained text that involves complex syntax-kinematic associations, like ``a person is walking with the right hand raising while stumbling to the left.'' Latent space alignment methods can lose significant feature details during feature projection, often processing only the common ``walking" motion. Autoregressive models, due to their unidirectional prediction nature, may overlook subsequent movements like ``stumbling'', leading to motion incoherence. Diffusion-based models face challenges in feature extraction and fusion between the denoising sequence and text, which can result in ignoring finer details such as ``right hand raising'' or ``stumbling to the left.''

Despite advances in text-to-motion generation, challenges remain regarding fine-grained modeling. The sparsity of current datasets limits learning precise textual cue-motion correspondences. Insufficient use of linguistic cues also restricts comprehending fine-grained semantics from prompts. Addressing these issues, we created detailed annotations of different body parts' actions and words explanations, enabling more intricate understanding of part-specific details. Furthermore, leveraging linguistic structures assists semantic parsing of prompts. This enables our model to generate human motions closely aligned with the semantic content of input text, exhibiting realistic movements.

\subsection{LLMs-Assisted Motion Generation}\label{sec2_2}

While large language models such as BERT \citep{devlin2018bert}, GPT-4 \citep{achiam2023gpt} and T5 \citep{raffel2020exploring} have demonstrated strong capabilities in language tasks as evidenced by their human-level performance in certain domains \citep{gilardi2023chatgpt}, their application to human motion generation has strengths and limitations. Recent works including ActionGPT \citep{kalakonda2023action}, SINC \citep{athanasiou2023sinc}, FineMoGen \citep{zhang2024finemogen}, and MotionGPT \citep{jiang2024motiongpt} have explored leveraging LLMs' language generation and zero-shot transfer abilities to enrich prompts, identify body parts, facilitate human-AI interaction, and support various motion-related tasks. However, directly generating coherent human motions from language remains challenging due to the complex grounding problem between language and bodily motion. Their suitability ultimately depends on how effectively language representations can condition low-dimensional movement sequences.

Previous works utilizing LLMs have achieved good results, yet challenges for improving fine-grained analysis and modeling persist. Urgently needed are datasets with precise, fine-grained text representations that are sensitive to subtle motion details. To address this, we introduce the use of LLMs for parsing text prompts at a fine level to obtain specific descriptions of individual body parts. We also provide detailed explanations of nouns, adjectives, and adverbs in sentences to address challenging vocabulary in complex texts. By training models with these fine-grained linguistic details regarding all body parts and words, we can generate high-fidelity, fine-level human motion sequences.