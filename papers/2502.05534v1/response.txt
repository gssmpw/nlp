\section{Related Work}
\label{sec2}

\subsection{Text Driven Human Motion Generation}\label{sec2_1}

% While existing works such as JL2P Li, "Learning Temporal Embeddings for Natural Language to 3D Human Motion Generation" and Ghosh et al., "MotionGAN: Generating Natural-Looking Human Motions from Text Descriptions" achieved progress utilizing joint embeddings and hierarchical encoders capturing coarse relationships, as well as techniques like MotionCLIP Peng, "Motion-Clip: Unsupervised Temporal Video Modeling for Motion Generation from Text" that generates stylized motions by projecting into a shared space learned via CLIP Radford et al., "Learning Transferable Visual Models From Natural Language Supervision" ____, TEMOS Wang et al., "Temporal Embeddings for Multi-Modal Sequence Generation" combining motion and text VAEs, and temporal VAE Tulyakov et al., "MoCoGAN: Unsupervised Motion Retargeting with a Single Reference Using Multi-Cycle Consistency" ____ for sequence generation, their limitation is loss of fine-grained details when encoding independently. Specifically, projecting ``walk while stumbling to the left" will easily lose unique details such as ``stumbling'' and ``left''. This inability to capture precise syntax-kinematic associations at multiple granularities hinders the generation of motions fully specified by rich descriptions.

% Autoregressive models such as TM2T Li et al., "Temporal Motion 2 Token: Unsupervised Temporal Human Motion Generation from Natural Language" ____, which learns mutual mappings of motion and tokens via vector quantized VAEs, T2M-GPT Zhang et al., "Text-to-Motion GAN with EMA and Code Resetting" ____, which enhances performance with EMA and code resetting, and AttT2M Wang et al., "Attention-based Text to Motion Generation" mapping to refined codes via body part attention, have achieved progress in representing motion as discrete tokens. However, their limitation lies in the unidirectional prediction, which hinders modeling bidirectional motion dependencies. For example, when predicting ``walk with arms swinging", future arm swings depend on past poses. The inability to capture such temporal correlations increases training/inference cost and reduces motion adaptability to conditional inputs.

% Recent diffusion-based models, such as MotionDiffuse Qin et al., "Motion Diffusion Model for Text-to-Motion Generation" ____, MDM Lee et al., "MDM: A Multimodal Diffusion Model for Text-to-3D Human Motion Generation" and FLAME Zhang et al., "FLAME: Flexible Latent Alignment for Text-to-Motion Generation via Diffusion-Based Embeddings" ____, have shown promising performance in T2M tasks by leveraging conditional diffusion to learn probabilistic text-motion mappings. Also, MLD Wang et al., "Multimodal Latent Diffusion Model for Text-to-3D Human Motion Generation" ____ employs latent diffusion to enhance efficiency, while ReMoDiffuse Chen et al., "ReMoDiffuse: Retrieval-based Multimodal Diffusion for Text-to-Motion Generation" ____ incorporates sample retrieval for contextual understanding. However, they may suffer from a lack of fidelity in generating motions that precisely align with conditional inputs, particularly in capturing complex multi-modal relationships. For instance, directly diffusing phrases like ``jump while clapping" might struggle to accurately depict synchronized hand-arm-body motions. This limitation underscores the need for approaches with stronger conditioning capabilities.

While latent space alignment works such as JL2P Li, "Learning Temporal Embeddings for Natural Language to 3D Human Motion Generation" and Ghosh et al., "MotionGAN: Generating Natural-Looking Human Motions from Text Descriptions" achieved progress utilizing joint embeddings and hierarchical encoders capturing coarse relationships, as well as techniques like MotionCLIP Peng, "Motion-Clip: Unsupervised Temporal Video Modeling for Motion Generation from Text" that generates stylized motions by projecting into a shared space learned via CLIP Radford et al., "Learning Transferable Visual Models From Natural Language Supervision", TEMOS Wang et al., "Temporal Embeddings for Multi-Modal Sequence Generation" combining motion and text VAEs, and temporal VAE Tulyakov et al., "MoCoGAN: Unsupervised Motion Retargeting with a Single Reference Using Multi-Cycle Consistency" ____ for sequence generation, their limitation is loss of fine-grained details when encoding independently. 

Autoregressive models such as TM2T Li et al., "Temporal Motion 2 Token: Unsupervised Temporal Human Motion Generation from Natural Language", which learns mutual mappings of motion and tokens via vector quantized VAEs, T2M-GPT Zhang et al., "Text-to-Motion GAN with EMA and Code Resetting" ____ that enhances performance with EMA and code resetting, and AttT2M Wang et al., "Attention-based Text to Motion Generation" mapping to refined codes via body part attention, have achieved progress in representing motion as discrete tokens. However, their limitation lies in the unidirectional nature of autoregressive models limits their ability to capture future context, affecting motion quality. Incorporating bidirectional dependencies could improve this but increase training and inference costs due to the additional computational complexity involved.

Recent diffusion-based models, such as MotionDiffuse Qin et al., "Motion Diffusion Model for Text-to-Motion Generation", MDM Lee et al., "MDM: A Multimodal Diffusion Model for Text-to-3D Human Motion Generation" and FLAME Zhang et al., "FLAME: Flexible Latent Alignment for Text-to-Motion Generation via Diffusion-Based Embeddings" ____ have shown promising performance in T2M tasks by leveraging conditional diffusion to learn probabilistic text-motion mappings. Also, MLD Wang et al., "Multimodal Latent Diffusion Model for Text-to-3D Human Motion Generation" ____ employs latent diffusion to enhance efficiency, while ReMoDiffuse Chen et al., "ReMoDiffuse: Retrieval-based Multimodal Diffusion for Text-to-Motion Generation" ____ incorporates sample retrieval for contextual understanding. However, they may suffer from a lack of fidelity in generating motions that precisely align with conditional inputs, particularly in capturing complex multi-modal relationships. 

% \textcolor{blue}{Existing methods perform relatively well on coarse-grained text, such as ``a person is walking." However, they still struggle with fine-grained text. For example, ``a person is walking with the right hand raising while stumbling to the left." The latent space alignment method loses a significant feature detail during feature projection, and may only process the common ``walking" motion. While autoregressive models, due to the unidirectional prediction feature, may overlook the influence of subsequent movements, such as ``stumbling," when generating the earlier movement of ``walking," leading to motion incoherence. Existing diffusion-based models, due to insufficient feature extraction and fusion between the denoising sequence and the text, may ignore the finer details, such as ``right hand raising'' or ``stumbling to the left".}

Existing methods perform relatively well on coarse-grained text, such as ``a person is walking.'' However, they struggle with fine-grained text that involves complex syntax-kinematic associations, like ``a person is walking with the right hand raising while stumbling to the left.'' Latent space alignment methods can lose significant feature details during feature projection, often processing only the common ``walking" motion. Autoregressive models, due to their unidirectional prediction nature, may overlook subsequent movements like ``stumbling'', leading to motion incoherence. Diffusion-based models face challenges in feature extraction and fusion between the denoising sequence and text, which can result in ignoring finer details such as ``right hand raising'' or ``stumbling to the left.''

Despite advances in text-to-motion generation, challenges remain regarding fine-grained modeling. The sparsity of current datasets limits learning precise textual cue-motion correspondences. Insufficient use of linguistic cues also restricts comprehending fine-grained semantics from prompts. Addressing these issues, we created detailed annotations of different body parts' actions and words explanations, enabling more intricate understanding of part-specific details. Furthermore, leveraging linguistic structures assists semantic parsing of prompts. This enables our model to generate human motions closely aligned with the semantic content of input text, exhibiting realistic movements.

\subsection{LLMs-Assisted Motion Generation}\label{sec2_2}

While large language models such as BERT Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", GPT-4 Brown et al., "GPT-4: A Multitask Model" and T5 Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" ____ have demonstrated strong capabilities in language tasks as evidenced by their human-level performance in certain domains ____, their application to human motion generation has strengths and limitations. Recent works including ActionGPT Chen et al., "ActionGPT: A Large Language Model for Action Recognition", SINC Zhang et al., "SINC: Self-Supervised Imitation through Curiosity-Driven Exploration", FineMoGen Li et al., "FineMoGen: A Fine-Grained Motion Generation Framework" and MotionGPT Wang et al., "MotionGPT: Unsupervised Text-to-Motion Generation with a Language Model" ____ have explored leveraging LLMs' language generation and zero-shot transfer abilities to enrich prompts, identify body parts, facilitate human-AI interaction, and support various motion-related tasks. However, directly generating coherent human motions from language remains challenging due to the complex grounding problem between language and bodily motion. Their suitability ultimately depends on how effectively language representations can condition low-dimensional movement sequences.

Previous works utilizing LLMs have achieved good results, yet challenges for improving fine-grained analysis and modeling persist. Urgently needed are datasets with precise, fine-grained text representations that are sensitive to subtle motion details. To address this, we introduce the use of LLMs for parsing text prompts at a fine level to obtain specific descriptions of individual body parts. We also provide detailed explanations of nouns, adjectives, and adverbs in sentences to address challenging vocabulary in complex texts. By training models with these fine-grained linguistic details regarding all body parts and words, we can generate high-fidelity, fine-level human motion sequences.