\section{Data}
\label{data}

Each data set used in this paper consists of a set of permutations $X$ and a corresponding set of numerical values $Y$, where $x_i \in X$ relates to $y_i \in Y$. Some statistics of the different data sets are shown in Table \ref{tab:datasets}. We list the number of permutations in the data set as size together with the sizes of the train, validation and test splits, the number of items in each underlying set as features, the number of unique permutations in the data set, and the mean and standard deviation of the target variables.


\begin{table}
\begin{center}
\caption{Statistics of the different data sets.}
\label{tab:datasets}
\resizebox{\linewidth}{!}{\begin{tabular}{l|ccc|cc}
\toprule
Data set & Size (train/validation/test) & Features & Unique permutations & Mean($y$) & Std($y$) \\
\midrule
$edm_5\_small$ & 750(450/50/250) & 5 & 74 & 0.716 & 0.282 \\
$edm_9\_small$ & 750(450/50/250) & 9 & 332 & 0.665 & 0.258 \\
$edm_{18}\_small$ & 750(450/50/250) & 18 & 630 & 0.651 & 0.249 \\
$cc_{enron}$ & 3000(1800/200/1000) & 53 & 3000 & 0.427 & 0.005 \\
$cc_{yeast}\_small$ & 7500(4500/500/2500) & 14 & 7500 & 0.484 & 0.011 \\
\hline
$edm_5\_large$ & 2928(1728/200/1000) & 5 & 116 & 0.718 & 0.280 \\
$edm_9\_large$ & 2907(1707/200/1000) & 9 & 962 & 0.667 & 0.258 \\
$edm_{18}\_large$ & 2644(1444/200/1000) & 18 & 2095 & 0.644 & 0.255 \\
$cc_{yeast}\_large$ & 15000(9000/1000/5000) & 14 & 15000 & 0.484 & 0.011 \\
        \bottomrule
\end{tabular}}
\end{center}


\end{table}


\subsection{Classifier Chain Orderings}

As the first real-world problem for permutation regression, we chose the ordering of classifer chains \cite{classifierchains}. Classifier chains is a problem transformation method for multi-label classification. For a set of $n$ labels $L = \{l_1, \dots, l_n \}$ a chain of $n$ classifiers $CC = \{C_1, \dots, C_n \}$ are trained, where $C_i$ predicts $l_i$ given the instance features $x$ and the previously predicted labels $l_{<i}$. Since each label has to be predicted exactly once by the classifier chain, the ordering of the chain is a permutation of the classifiers. The classification performance of a specific ordering can be measured and therefore be used as regression target.


The first data set used to generate classifier chain orderings is the yeast  \cite{yeast} multi-label data set, which has 14 different labels. Additionally we generated classifier chain orderings for the enron \cite{enron} multi-label data set, which has 53 different labels. Both data sets were downloaded from the OpenML \cite{openml} data repository.


The classifier chains are trained and evaluated using scikit-learn \cite{scikit-learn} implementations. For the classifer chain, we used \textit{sklearn.multioutput.ClassifierChain} with \textit{sklearn.linear\_model.LogisticRegression} as their elements. The performance of the chains is measured by the \textit{sklearn.metrics.jaccard\_score}, which calculates the Jaccard index \cite{jaccard} of the true and the predicted label set. For each data set we randomly generated 100000 classifier chain orderings and computed the corresponding scores as target values.

In order to be able to run the data sets with our online survey setup, we only use a small randomly sampled fraction of the data sets in this work. We sampled 3000 instances for $CC_{enron}$, 7500 instances for $cc_{yeast}\_small$ and 15000 instances for $cc_{yeast}\_large$.



\subsection{Educational Domain}

The second real-world area of application for permutation regression we chose is learning analytics. The ordering of different courses in a study, the order of different tasks in a course, or the order of different questions in a test are permutations of an item set and the performance of a student can be used as corresponding target value.


As base for this data sets we used the NeurIPS 2020 Education Challenge data set for task 3 and 4 \cite{neurips_data}. Using FP-growth \cite{fpgrowth}, we select three differently sized sets of questions with a sufficiently large minimum support as the base item sets for the permutations. For each user that answered all selected question, we order the questions by the answer timestamp and use the fraction of correctly answered questions as target value.


This yields the $edm_5$, $edm_9$ and $edm_{18}$ data sets, which are each built on a set of five, nine and 18 questions. The large versions of those data sets comprise all available instances of those specific sets of questions, and for the small versions, we use a random sampling of 750 instances.