\section{Problem Definition and Methods}
\label{theory}
\subsection{Permutation Regression}

Permutation regression describes the task of estimating the relation between different orderings of a set of items and the respective dependent numerical variable derived by an unknown function. 
\par

\begin{definition}
Given a set of permutations $X$ of the same item set $S$, with $x:S \leftrightarrow S$ being a bijection $\forall x \in X$, and a function $f:X \mapsto Y \subset \mathbb{R}$, \textbf{permutation regression} is the task of finding parameters $\theta \in \Theta$ for a prediction function $f'_{\theta} : X \mapsto \tilde{Y}$ such that it minimizes a given loss function $\mathcal{L}$:
\begin{equation*}
    \min_{\forall \theta \in \Theta} \epsilon_{\theta} = \sum_{x \in X}\mathcal{L}(f(x),f'_{\theta}(x))
\end{equation*} 
\label{def:permutationregression}
\end{definition}

\subsection{Transparent Permutation Regression}
\label{transpermreg}
In order to obtain a transparent model for permutation regression, we base our approach on a gradient boosted regression model on simple binary features, derived from human-understandable constraints, as inputs. Given a set of permutations $X$, we generate a set of $l$ constraints $\rho$ and a function $g_{\rho} : X \mapsto Z \subset \{0,1\}^l$,  which maps each permutation to a binary vector of length $l$ with:
\begin{equation*}
    g_{\rho} : x \rightarrow z \text{ where} \begin{cases}
        z_i = 1&  \text{if } \varrho_i \subset x \\
        z_i = 0& \text{if } \varrho_i \not\subset x
    \end{cases} \forall i \in \{1,\dots,l\}
\end{equation*}
The notation $\varrho_i \subset x$ denotes that the constraint $\varrho_i \in \rho$ is fulfilled by permutation $x$ and analogously the notation $\varrho_i \not\subset x$ denotes that the constraint is not fulfilled.

A constraint $\varrho_i$ is an ordered subset of the permutations' underlying item set $S$. A permutation $x$ fulfills a constraint $\varrho_i$ if the elements of $\varrho_i$ occur in the same order in $x$. For instance, the permutation $x=[ \, 3, 2, 1, 4] \,$ fulfills the constraint $\varrho_i=(2,4)$ but not the constraint $\varrho_j=(1,2,3)$.

The final part of the model are the coefficients $\beta$ for each feature and the baseline prediction $\mu$. We set $\mu$ as the average target value of the training set provided. We calculate the coefficients $\beta$ by minimizing the predictive error $\epsilon = \sum_{i=1}^m |y_i - \tilde{y}_i|$, where $y_i$ is the true target value of permutation $x_i$, and $\tilde{y}_i$ is the predicted target for $x_i$. This yields the linear model:
\begin{equation*}
    \tilde{y}_j = \mu + \sum_{i=1}^{l}\beta_i g_{\rho}(x_j)_i
\end{equation*}

In order to find the constraints that greatly improve the predictive performance of our model, we employ an iterative greedy gradient boosting \cite{gradboost} based approach incorporating a fast Gauss-Southwell selection \cite{gauss_southwell,seq_reg}, similar to related methods for graph \cite{graph_reg} and sequence \cite{seq_reg} regression. Hereby we explore the space of all possible constraints with a breadth-first search, calculate the gradient for each visited constraint and add the constraint with the highest absolute gradient to our constraint set. We prune unpromising branches in the search tree to speed up the search by employing an upper bound, that exploits the hierarchical structure of the constraints similar to the exploitation of the structure of the subsequence space\cite{seq_reg}.


After each iteration, we calculate the coefficient $\beta^*$ for the new constraint $\varrho^*$ using the constraint's gradient $\tau$ and calculate the new residuals $\delta$, where each element $\delta_i = y_i - \tilde{y}_i$ is the difference between the real target $y_i$ and the predicted target $\tilde{y}_i$ of a permutation $x_i$. We continue this iterative process of selecting a constraint and calculating its coefficient until we have generated $l$ constraints, where $l$ is a selectable hyperparameter.





\subsection{Baseline models}

In order to use a permutation of an item set as input for a conventional machine learning algorithm, the permutation has to be transformed. In this section, we introduce multiple possible encodings for permutations. We start with the simplest possible baseline.

\subsubsection{Naive baseline}

This simple model always predicts the average target value over the provided training set.




\subsubsection{One-Hot Encoding}

The first method we use to encode the permutations is a binary one-hot encoding of the items' indices. For each permutation $x = \left[x_1, \dots, x_n\right]$, all integer entries $x_i$ are replaced by a zero-vector with a singular one at position $x_i$, and concatenated to a single vector.

\begin{equation*}
 [ \, 3, 2, 1, 4] \, \Rightarrow [ \,0010\ 0100\ 1000\ 0001] \,
\end{equation*}


\subsubsection{Graph Encoding}

For the directed graph $G = (V,E)$, where $V$ describes the set of all nodes and $E$ describes the set of all edges, each item of the underlying item set corresponds to a node $v \in V$ and each edge $e \in E$ describes a possible succession in the permutation. Since every item can succeed every other item in at least one unique permutation, the graph is complete. Each permutation can be represented by a path in the graph using all nodes. This structure can be used to encode a permutation.



A permutation of $n$ items is encoded in a binary vector of length $(n + 2) \times n$, where the first $n\times n$ values are a one-hot encoding of the edges present in the path representation of the permutation and the next $2n$ values are the one-hot encoding of the first and last item of the permutation.
\begin{equation*}
 [ \, 3, 2, 1, 4] \, \Rightarrow [ \, \underbrace{0001\ 1000\ 0100\ 0000}_{\text{\makebox[0pt][c]{encoding of path edges}}}\ \overbrace{0010\ 0001}^{\text{\makebox[0pt][c]{encoding of start and end}}}] \,
\end{equation*}

In the example, the first block of four binary values represents an edge from 1 to 4, the second block of four binary values an edge from 2 to 1, and so forth.


\subsubsection{Sequence Encoding}
A permutation of $n$ items can be viewed as a sequence of length $n$, where each item appears exactly once in the sequence. Similarly to the first method, we use a binary one-hot encoding, but instead of concatenating the vectors we keep them as a sequence of one-hot encoded items. Therefore, we can incorporate a recurrent neural network architecture, such as long short-term memory (LSTM), to connect the permutation data with a simple multilayer perceptron (MLP). 

\begin{equation*}
 [ \, 3, 2, 1, 4] \, \Rightarrow \begin{bmatrix}
 0010 \\
 0100 \\
 1000 \\
 0001
 \end{bmatrix}
\end{equation*}
\par


\subsection{Human Guided Regression Model}
\label{sec:HIL}

To further increase the comprehensibility of the model from Section \ref{transpermreg} and enable potential users to be able to benefit from their own background knowledge, we want to incorporate their input in the creation process of the model\footnote{An interactive demo can be found on: hugur.pensel.eu}. Figure \ref{fig:interface} shows multiple screenshots of the HuGuR interface and exemplifies its use.

\begin{algorithm}[t!]
\caption{Select $l$ constraints}\label{alg:selconstraints}
\KwData{Permutations $X$, set of constraints $\rho$, residuals $\delta$, number of selected constraints $l$}
\KwResult{most promising constraints $\rho^*$}
$\rho^* \gets \{\}$\;
$\rho' \gets \{\}$\;
$T \gets \{\}$\;
\For{$\varrho \in \rho$}{
    $Z \gets g_{\varrho}(X)$\;
    $\tau \gets |\sum \delta Z|$\;
    $\rho'.append(\varrho)$\;
    $T.append(\tau)$\;
}
$I \gets argsort(T)$\Comment*[r]{$argsort(T)_i:=\{j|(\tau_i < \tau_j) \text{ or } (\tau_i = \tau_j, i<j) \}$}
\For{$i \in \{I_{|I|-l},\dots,I_{|I|}\}$}{
$\rho^*.append(\varrho_i \in \rho')$
}
\end{algorithm}

The HIL model HuGuR initially generates $l$ minimal constraints, which are constraints consisting of only two elements, by using the set of all possible minimal constraints as input for Algorithm \ref{alg:selconstraints}. We use the average target value $\mu$ of the provided training set as base prediction to obtain the first residuals and calculate the coefficients $\beta$ for the constraints one after another by the gradient boosting method introduced in Section \ref{transpermreg}. Then, we present the selected constraints and their corresponding coefficients, as well as the current prediction error on a validation set, to the user. To improve the comprehensibility, we provide the coefficients by the hue and saturation of the color for each constraint. The saturation corresponds to the absolute value of the coefficient and the hue indicates a positive or negative impact on the predicted target, with blue corresponding to positive and red to negative. For instance, if we predict the difference from an optimal permutation, negative coefficients are blue and positive red, since a smaller difference is better.

\begin{figure}[!htbp]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=\linewidth]{hugur-use2_b.pdf}}
    \caption{Exemplary use of HuGuR. The first step \textbf{i} is to select the hyperparameters, which are the number of constraints added in each step and the learning rate of the model. The number of constraints $l$ is an integer between 1 and 20 and the learning rate is a float between 0.000001 and 1. After the generation, we obtain the model view. Here, we see the constraints the model encompasses, and we can interact with them. Blue constraints indicate a positive impact on the target value and red constraints indicate a negative impact. By clicking on an active constraint \textbf{ii}, we deactivate the constraint and replace it by the $l$ most promising child constraints. And vice versa, by clicking on an inactive constraint \textbf{iii}, we activate it and remove all of its child constraints. We can also use additional controls \textbf{iv} to restart with other hyperparameters or reset to a previous iteration. The error history \textbf{v} lets us track our progress over the iterations. Also, one can always jump back to the best model found so far \textbf{vi}.}
    \label{fig:interface}
\end{figure}

\begin{algorithm}[t!]
\caption{Generate all child constraints}\label{alg:genchild}
\KwData{Item set $S$, prior constraint $\varrho'$}
\KwResult{All child constraints $\rho'$}
$\rho' \gets []$\;
\For{$c \in S$}{
\If{$c \notin \varrho'$}{
\For{$i \in \{1,\dots,|\varrho'|+1\}$}{
$\varrho^+ \gets (\varrho'_1,\dots,\varrho'_{i-1},c,\varrho'_i,\dots,\varrho'_{|\varrho'|})$\;
$\rho'[] \gets \varrho^+$\;
}
}
}
\end{algorithm}

The user can further refine the model by replacing a single constraint $\varrho'$ with $l$ more specific child constraints. First, we generate all possible child constraints $\rho'$ as in Algorithm \ref{alg:genchild}, and then we select the most promising $l$ constraints of those by Algorithm \ref{alg:selconstraints}. To calculate the coefficients,  we remove $\varrho$ from the constraints of the model $\rho$ and use the altered prediction to obtain the residuals. Alternatively, the user can generate a new model with different hyperparameters or simplify the current model. To simplify a model, we choose the $l$ constraints with the highest absolute coefficients and use them as the foundation of a new model. Additionally, the user can reverse this by adding $\varrho$ back to $\rho$ and removing all of its children. During the whole process we track the mean absolute error on the validation set and present the histories to the user. The user can always reload any previous model, in particular the model with the lowest error.

