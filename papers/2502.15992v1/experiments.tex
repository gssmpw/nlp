\section{Experiments}
\label{experiments}

To measure the performance of HuGuR in its designated field of application, we conduct a user study with 21 participants. The participants consist of students, mostly computer science, with a varying degree of experience in the field of machine learning.

\subsection{User Study Setup}
We split the nine data sets presented in Section \ref{data} each in single independent train, validation and test sets. Each participant is assigned to work on a random and not previously seen data set. The participants receive no further information about the data sets, apart from the information conveyed by the HuGuR interface. In this way, we leveled
the playing field for the experiment for all participants.
After each finalization of their model, the participant can choose to continue with a new random data set, until all nine data sets are finished. We encourage the participants to finish at least four data sets and to go through at least 20 model iterations during the training phase. Therefore, we only consider models with a minimum exploration of at least 20 model iteration steps in our presented results. This yields at least 12 different models for each of the nine data sets. To ensure a performance-oriented participation, we gave a monetary bonus to those participants who produced the highest performing models for each data set. We assume that users have a similar incentive to build good models when they construct models for commercial or research purposes.

\subsection{User Study Results}

\begin{table}[]
    \centering
    \caption{Hyperparameter grid for the neural network based comparison methods. The $layers$ parameter describes the structure of the dense prediction part of the networks. The number of units in the sequence encoding LSTM part of the model is given by $encoder\_units$ parameter and the $alpha$ parameter sets the strength of the L2 regularization term.}
    \label{tab:hyperparams}
    \begin{tabular}{c|ll}
    \toprule
        Algorithm & Parameter & Values \\
        \midrule
        $ENC_{one-hot}$ & $layers$ & (50,), (100,), (100,50), (100,100,50), (200,100,50) \\
        $ENC_{graph}$ & $alpha$ & 0.001, 0.0001, 0.00001 \\
        \hline
        \multirow{ 2}{*}{$LSTM$} & $layers$ & (50,), (100,), (100,50), (100,100,50), (200,100,50) \\
        & $encoder\_units$ & 10, 25, 50, 100\\
        \bottomrule
    \end{tabular}
    
    
\end{table}

We calculate the mean absolute error, mean squared error, and the coefficient of determination ($R^2$ score) for all user-built HuGuR models. We also train multiple models with our comparison methods on the train splits of our data sets. For the simple one-hot and graph encoding based approaches, we employ the scikit-learn \cite{scikit-learn} multilayer perceptron (MLP) as regressor, and for the sequence based encoding,  we build a more complex neural network regressor using tensorflow \cite{tensorflow} and keras. We also incorporate a small hyperparameter search over the parameter grid provided in Table \ref{tab:hyperparams}, and evaluate the solution candidates on a dedicated validation set. The comparison models are similarly evaluated on the test splits of the data sets\footnote{The source code and the data sets are available on github.com/lpensel/HuGuR}. 

In addition to the average performance of all HuGuR models on the data sets, we also compare the average performance of the five HuGuR models ($HuGuR_5$), i.e., the models of the best five users, which yield the best performance on the validation sets to all the other methods.

First, we compare the performance of HuGuR with the naive baseline and the two simpler encoding methods. Since we have multiple HuGuR models for each data set, we provide the average performance as well as its standard deviation as result. The performance measures are compiled in Table \ref{tab:mae1}. Considering the smaller data sets, which are always ordered to the top of the tables, HuGuR outperforms the comparison methods on almost all of those data sets. With an average rank over all metrics of 1.4 for $HuGuR_5$ and 2.13 for $HuGuR$ compared to a rank of 3.53 for $ENC_{graph}$ and 3.73 for $ENC_{one-hot}$. Adding the larger data sets back into the consideration, the average ranks change to 1.7 and 2.67 for $HuGuR_5$ and $HuGuR$ as well as to 2.96 for $ENC_{one-hot}$ and 3.11 for $ENC_{graph}$. Using an adapted Friedman test \cite{ranking:signific}, it shows that there are significant ($p << 0.001$) differences in the distribution of ranks, i.e. the quality of the methods. To determine significant differences between the algorithms, we use the Bonferroni-Dunn test \cite{ranking:signific} with a significance threshold of $\alpha = 0.05$ and present the results in Table \ref{tab:rank_difference} under $Set_1$.



\begin{table}[]
    \centering
    \caption{Performance measures for the naive baseline $NB$, one-hot encoding with MLP regressor $ENC_{one-hot}$, graph encoding with MLP regressor $ENC_{graph}$, all user-built models $HuGuR$ and the top five user-built models $HuGuR_5$. The best performing method for each data set is highlighted. Additionally, we provide the average rank for each method based on these results.}
    \label{tab:mae1}
    \begin{tabular}{l|ccccc}
    \toprule
    \multicolumn{6}{c}{\textbf{Mean absolute errors} } \\
    \midrule
          Data set & $NB$ & $ENC_{one-hot}$ & $ENC_{graph}$ & $HuGuR$ & $HuGuR_5$ \\ 
\midrule 
$edm_5\_small$ & $2.506e^{-1} $& $\mathbf{ 2.394e^{-1} } $& $2.468e^{-1} $& $2.409e^{-1}(4.422e^{-3}) $& $2.395e^{-1}(4.422e^{-3}) $\\ 
$edm_9\_small$ & $2.197e^{-1} $& $2.163e^{-1} $& $2.183e^{-1} $& $2.110e^{-1}(1.953e^{-3}) $& $\mathbf{ 2.109e^{-1}(1.953e^{-3}) } $\\ 
$edm_{18}\_small$ & $2.086e^{-1} $& $2.001e^{-1} $& $\mathbf{ 1.686e^{-1} } $& $1.815e^{-1}(8.729e^{-3}) $& $1.758e^{-1}(8.729e^{-3}) $\\ 
$cc_{enron}$ & $4.270e^{-3} $& $7.384e^{-3} $& $4.720e^{-3} $& $3.468e^{-3}(1.204e^{-4}) $& $\mathbf{ 3.426e^{-3}(1.204e^{-4}) } $\\ 
$cc_{yeast}\_small$ & $9.105e^{-3} $& $7.563e^{-3} $& $8.749e^{-3} $& $6.948e^{-3}(6.816e^{-4}) $& $\mathbf{ 6.315e^{-3}(6.816e^{-4}) } $\\ 
\hline
$edm_5\_large$ & $2.368e^{-1} $& $\mathbf{ 2.144e^{-1} } $& $2.180e^{-1} $& $2.205e^{-1}(7.126e^{-3}) $& $2.145e^{-1}(7.126e^{-3}) $\\ 
$edm_9\_large$ & $2.116e^{-1} $& $2.013e^{-1} $& $\mathbf{ 1.964e^{-1} } $& $1.992e^{-1}(5.124e^{-3}) $& $1.967e^{-1}(5.124e^{-3}) $\\ 
$edm_{18}\_large$ & $2.173e^{-1} $& $1.753e^{-1} $& $\mathbf{ 1.734e^{-1} } $& $1.873e^{-1}(1.054e^{-2}) $& $1.812e^{-1}(1.054e^{-2}) $\\ 
$cc_{yeast}\_large$ & $8.903e^{-3} $& $\mathbf{ 5.458e^{-3} } $& $8.425e^{-3} $& $6.901e^{-3}(7.311e^{-4}) $& $6.158e^{-3}(7.311e^{-4}) $\\ 
\hline \hline 
 Average rank & 4.78 & 2.67 & 2.89 & 2.89 & 1.78 \\

    \toprule
    \multicolumn{6}{c}{\textbf{Mean squared errors} } \\
    \midrule
          Data set & $NB$ & $ENC_{one-hot}$ & $ENC_{graph}$ & $HuGuR$ & $HuGuR_5$ \\ 
 \midrule
$edm_5\_small$ & $8.921e^{-2} $& $9.015e^{-2} $& $9.041e^{-2} $& $8.610e^{-2}(2.533e^{-3}) $& $\mathbf{ 8.606e^{-2}(2.533e^{-3}) } $\\ 
$edm_9\_small$ & $6.915e^{-2} $& $6.653e^{-2} $& $6.741e^{-2} $& $\mathbf{ 6.246e^{-2}(1.279e^{-3}) } $& $6.289e^{-2}(1.279e^{-3}) $\\ 
$edm_{18}\_small$ & $5.804e^{-2} $& $6.466e^{-2} $& $\mathbf{ 4.577e^{-2} } $& $4.968e^{-2}(2.734e^{-3}) $& $4.853e^{-2}(2.734e^{-3}) $\\ 
$cc_{enron}$ & $2.842e^{-5} $& $8.736e^{-5} $& $3.612e^{-5} $& $1.912e^{-5}(1.413e^{-6}) $& $\mathbf{ 1.868e^{-5}(1.413e^{-6}) } $\\ 
$cc_{yeast}\_small$ & $1.283e^{-4} $& $9.012e^{-5} $& $1.174e^{-4} $& $7.705e^{-5}(1.463e^{-5}) $& $\mathbf{ 6.363e^{-5}(1.463e^{-5}) } $\\ 
\hline
$edm_5\_large$ & $8.007e^{-2} $& $\mathbf{ 7.321e^{-2} } $& $7.522e^{-2} $& $7.448e^{-2}(1.951e^{-3}) $& $7.344e^{-2}(1.951e^{-3}) $\\ 
$edm_9\_large$ & $6.610e^{-2} $& $6.321e^{-2} $& $5.825e^{-2} $& $5.852e^{-2}(2.906e^{-3}) $& $\mathbf{ 5.745e^{-2}(2.906e^{-3}) } $\\ 
$edm_{18}\_large$ & $6.398e^{-2} $& $4.724e^{-2} $& $\mathbf{ 4.485e^{-2} } $& $5.167e^{-2}(4.357e^{-3}) $& $4.953e^{-2}(4.357e^{-3}) $\\ 
$cc_{yeast}\_large$ & $1.242e^{-4} $& $\mathbf{ 4.770e^{-5} } $& $1.109e^{-4} $& $7.653e^{-5}(1.559e^{-5}) $& $6.034e^{-5}(1.559e^{-5}) $\\  
\hline \hline 
 Average rank & 4.44 & 3.11 & 3.22 & 2.56 & 1.67\\

    \toprule
    \multicolumn{6}{c}{\textbf{$R^2$ scores} } \\
    \midrule
         Data set & $NB$ & $ENC_{one-hot}$ & $ENC_{graph}$ & $HuGuR$ & $HuGuR_5$ \\ 
\midrule 
$edm_5\_small$ & $-1.282e^{-2} $& $-2.355e^{-2} $& $-2.647e^{-2} $& $2.246e^{-2}(2.876e^{-2}) $& $\mathbf{ 2.285e^{-2}(2.876e^{-2}) } $\\ 
$edm_9\_small$ & $-8.047e^{-4} $& $3.713e^{-2} $& $2.441e^{-2} $& $\mathbf{ 9.608e^{-2}(1.852e^{-2}) } $& $8.979e^{-2}(1.852e^{-2}) $\\ 
$edm_{18}\_small$ & $-1.283e^{-2} $& $-1.283e^{-1} $& $\mathbf{ 2.013e^{-1} } $& $1.331e^{-1}(4.770e^{-2}) $& $1.531e^{-1}(4.770e^{-2}) $\\ 
$cc_{enron}$ & $-8.550e^{-4} $& $-2.076e+00 $& $-2.720e^{-1} $& $3.266e^{-1}(4.976e^{-2}) $& $\mathbf{ 3.420e^{-1}(4.976e^{-2}) } $\\ 
$cc_{yeast}\_small$ & $-1.538e^{-4} $& $2.977e^{-1} $& $8.509e^{-2} $& $3.995e^{-1}(1.140e^{-1}) $& $\mathbf{ 5.041e^{-1}(1.140e^{-1}) } $\\ 
\hline
$edm_5\_large$ & $-3.862e^{-4} $& $\mathbf{ 8.533e^{-2} } $& $6.029e^{-2} $& $6.947e^{-2}(2.437e^{-2}) $& $8.253e^{-2}(2.437e^{-2}) $\\ 
$edm_9\_large$ & $-4.697e^{-5} $& $4.365e^{-2} $& $1.187e^{-1} $& $1.146e^{-1}(4.396e^{-2}) $& $\mathbf{ 1.309e^{-1}(4.396e^{-2}) } $\\ 
$edm_{18}\_large$ & $-1.639e^{-5} $& $2.617e^{-1} $& $\mathbf{ 2.990e^{-1} } $& $1.924e^{-1}(6.810e^{-2}) $& $2.258e^{-1}(6.810e^{-2}) $\\ 
$cc_{yeast}\_large$ & $-8.051e^{-5} $& $\mathbf{ 6.160e^{-1} } $& $1.074e^{-1} $& $3.840e^{-1}(1.255e^{-1}) $& $5.143e^{-1}(1.255e^{-1}) $\\ 
\hline \hline 
 Average rank & 4.44 & 3.11 & 3.22 & 2.56 & 1.67   \\

 \hline \hline
 Overall rank & 4.56 & 2.96 & 3.11 & 2.67 & 1.7 \\
        \bottomrule
    \end{tabular}
    
    
\end{table}




The comparisons of HuGuR with the more complex sequence based encoding method are gathered in Table \ref{tab:mae2}. While $HuGuR_5$ slightly outperforms $LSTM$ on the small data sets, rank 1.7 opposed to rank 1.9, on all data sets combined, $LSTM$ achieves an average rank of 1.67 and $HuGuR_5$ only an average rank of 1.74. The adapted Friedman test still shows a significant ($p < 0.001$) difference between the rankings of $LSTM$, $HuGuR$ and $HuGuR_5$. Table \ref{tab:rank_difference} lists the significant differences under $Set_2$, based on a Bonferroni-Dunn test with a threshold of $\alpha = 0.05$.



\begin{table}[]
    \centering
    \caption{Performance measures for the sequence encoding with LSTM based neural network architecture $LSTM$, all user-built models $HuGuR$ and the top five user build models $HuGuR_5$. The best performing method for each data set is highlighted. Additionally, we provide the average rank for each method based on these results.}
    \label{tab:mae2}
    \begin{tabular}{l|ccc}
    \toprule
    \multicolumn{4}{c}{\textbf{Mean absolute errors} } \\
    \midrule
         Data set & $LSTM$ & $HuGuR$ & $HuGuR_5$ \\ 
\midrule
$edm_5\_small$ & $\mathbf{ 2.364e^{-1} } $& $2.409e^{-1}(4.422e^{-3}) $& $2.395e^{-1}(4.422e^{-3}) $\\ 
$edm_9\_small$ & $\mathbf{ 2.005e^{-1} } $& $2.110e^{-1}(1.953e^{-3}) $& $2.109e^{-1}(1.953e^{-3}) $\\ 
$edm_{18}\_small$ & $\mathbf{ 1.709e^{-1} } $& $1.815e^{-1}(8.729e^{-3}) $& $1.758e^{-1}(8.729e^{-3}) $\\ 
$cc_{enron}$ & $4.251e^{-3} $& $3.468e^{-3}(1.204e^{-4}) $& $\mathbf{ 3.426e^{-3}(1.204e^{-4}) } $\\ 
$cc_{yeast}\_small$ & $\mathbf{ 4.671e^{-3} } $& $6.948e^{-3}(6.816e^{-4}) $& $6.315e^{-3}(6.816e^{-4}) $\\ 
\hline
$edm_5\_large$ & $\mathbf{ 2.084e^{-1} } $& $2.205e^{-1}(7.126e^{-3}) $& $2.145e^{-1}(7.126e^{-3}) $\\ 
$edm_9\_large$ & $\mathbf{ 1.823e^{-1} } $& $1.992e^{-1}(5.124e^{-3}) $& $1.967e^{-1}(5.124e^{-3}) $\\ 
$edm_{18}\_large$ & $\mathbf{ 1.594e^{-1} } $& $1.873e^{-1}(1.054e^{-2}) $& $1.812e^{-1}(1.054e^{-2}) $\\ 
$cc_{yeast}\_large$ & $\mathbf{ 3.737e^{-3} } $& $6.901e^{-3}(7.311e^{-4}) $& $6.158e^{-3}(7.311e^{-4}) $\\ 
\hline \hline 
 Average rank & 1.22 & 2.89 & 1.89   \\
        
    \toprule
    \multicolumn{4}{c}{\textbf{Mean squared errors} } \\
    \midrule
          Data set & $LSTM$ & $HuGuR$ & $HuGuR_5$ \\ 
\midrule
$edm_5\_small$ & $8.904e^{-2} $& $8.610e^{-2}(2.533e^{-3}) $& $\mathbf{ 8.606e^{-2}(2.533e^{-3}) } $\\ 
$edm_9\_small$ & $\mathbf{ 5.838e^{-2} } $& $6.246e^{-2}(1.279e^{-3}) $& $6.289e^{-2}(1.279e^{-3}) $\\ 
$edm_{18}\_small$ & $5.090e^{-2} $& $4.968e^{-2}(2.734e^{-3}) $& $\mathbf{ 4.853e^{-2}(2.734e^{-3}) } $\\ 
$cc_{enron}$ & $2.841e^{-5} $& $1.912e^{-5}(1.413e^{-6}) $& $\mathbf{ 1.868e^{-5}(1.413e^{-6}) } $\\ 
$cc_{yeast}\_small$ & $\mathbf{ 3.391e^{-5} } $& $7.705e^{-5}(1.463e^{-5}) $& $6.363e^{-5}(1.463e^{-5}) $\\ 
\hline
$edm_5\_large$ & $7.590e^{-2} $& $7.448e^{-2}(1.951e^{-3}) $& $\mathbf{ 7.344e^{-2}(1.951e^{-3}) } $\\ 
$edm_9\_large$ & $\mathbf{ 5.314e^{-2} } $& $5.852e^{-2}(2.906e^{-3}) $& $5.745e^{-2}(2.906e^{-3}) $\\ 
$edm_{18}\_large$ & $\mathbf{ 4.194e^{-2} } $& $5.167e^{-2}(4.357e^{-3}) $& $4.953e^{-2}(4.357e^{-3}) $\\ 
$cc_{yeast}\_large$ & $\mathbf{ 2.223e^{-5} } $& $7.653e^{-5}(1.559e^{-5}) $& $6.034e^{-5}(1.559e^{-5}) $\\ 
\hline \hline 
 Average rank & 1.89 & 2.44 & 1.67 \\
        
    \toprule
    \multicolumn{4}{c}{\textbf{$R^2$ scores} } \\
    \midrule
         Data set & $LSTM$ & $HuGuR$ & $HuGuR_5$ \\ 
\midrule
$edm_5\_small$ & $-1.098e^{-2} $& $2.246e^{-2}(2.876e^{-2}) $& $\mathbf{ 2.285e^{-2}(2.876e^{-2}) } $\\ 
$edm_9\_small$ & $\mathbf{ 1.551e^{-1} } $& $9.608e^{-2}(1.852e^{-2}) $& $8.979e^{-2}(1.852e^{-2}) $\\ 
$edm_{18}\_small$ & $1.119e^{-1} $& $1.331e^{-1}(4.770e^{-2}) $& $\mathbf{ 1.531e^{-1}(4.770e^{-2}) } $\\ 
$cc_{enron}$ & $-4.006e^{-4} $& $3.266e^{-1}(4.976e^{-2}) $& $\mathbf{ 3.420e^{-1}(4.976e^{-2}) } $\\ 
$cc_{yeast}\_small$ & $\mathbf{ 7.357e^{-1} } $& $3.995e^{-1}(1.140e^{-1}) $& $5.041e^{-1}(1.140e^{-1}) $\\ 
\hline
$edm_5\_large$ & $5.183e^{-2} $& $6.947e^{-2}(2.437e^{-2}) $& $\mathbf{ 8.253e^{-2}(2.437e^{-2}) } $\\ 
$edm_9\_large$ & $\mathbf{ 1.960e^{-1} } $& $1.146e^{-1}(4.396e^{-2}) $& $1.309e^{-1}(4.396e^{-2}) $\\ 
$edm_{18}\_large$ & $\mathbf{ 3.444e^{-1} } $& $1.924e^{-1}(6.810e^{-2}) $& $2.258e^{-1}(6.810e^{-2}) $\\ 
$cc_{yeast}\_large$ & $\mathbf{ 8.211e^{-1} } $& $3.840e^{-1}(1.255e^{-1}) $& $5.143e^{-1}(1.255e^{-1}) $\\ 
\hline \hline 
 Average rank & 1.89 & 2.44 & 1.67  \\

 \hline \hline
 Overall rank & 1.67 & 2.59 & 1.74\\
        \bottomrule
    \end{tabular}
    
    
\end{table}

Comparing all methods with each other gives again significant ($p << 0.001$) differences in the rank distributions and using the Bonferroni-Dunn test with a threshold $\alpha = 0.05$. Table \ref{tab:rank_difference} under $Set_3$ shows that $LSTM$ performs significantly better than all other methods except $HuGuR_5$. Similarly, $HuGuR_5$ performs significantly better than all other methods but $LSMT$ and $HuGuR$.

Overall, HuGuR achieves a similar or better performance than the baseline methods, while not being a black-box neural network based model but a linear model with human-understandable constraints. Additionally, if we compare the complexity of the trained models, all HuGuR models considered in our study have at least one order of magnitude fewer trainable parameters than the neural network based methods for the same data set. The biggest HuGuR model comprises 460 coefficients, while the smallest $LSTM$ model comprises 1401 weights and biases.

Table \ref{tab:model_comp} compares the number of trainable parameters of all models. In all cases HuGuR has at least one order of magnitude fewer trainable parameters than the other methods and often even multiple orders of magnitude fewer parameters.


\begin{table}[t!]
    \centering
    \caption{Statistical significant differences of the algorithms based on their ranks for the performance measures. We use the Bonferroni-Dunn test with $\alpha = 0.05$ to show the significance. $Set_1$ compares $NB$, $ENC_{one-hot}$, $ENC_{graph}$, $HuGuR$ and $HuGuR_5$. $Set_2$ compares $LSTM$, $HuGuR$ and $HuGuR_5$. $Set_3$ compares all methods.
    }
    \label{tab:rank_difference}
    \begin{tabular}{rcl|rcl|rcl}
    \toprule
       \multicolumn{3}{c|}{$Set_1$}  & \multicolumn{3}{c|}{$Set_2$} & \multicolumn{3}{c}{$Set_3$} \\
       \midrule
         $ENC_{one-hot}$ &  $>$  & $NB$ & $LSTM$ &  $>$  & $HuGuR$  & \hspace{5pt} $ENC_{one-hot}$ &  $>$  & $NB$ \\
$ENC_{graph}$ & $>$ & $NB$ & \hspace{5pt} $HuGuR_5$ & $>$ & $HuGuR$ \hspace{5pt} & $ENC_{graph}$ &  $>$  & $NB$ \\
$HuGuR$ & $>$ & $NB$ & & &  & $LSTM$ &  $>$  & $NB$ \\
$HuGuR_5$ & $>$ & $NB$ & & & & $HuGuR$ &  $>$  & $NB$ \\
$HuGuR_5$ & $>$ & $ENC_{one-hot}$ \hspace{5pt} & & & & $HuGuR_5$ &  $>$  & $NB$ \\
$HuGuR_5$ & $>$ & $ENC_{graph}$ & & & & $LSTM$ &  $>$  & $ENC_{one-hot}$ \\
& & & & & & $HuGuR_5$ &  $>$  & $ENC_{one-hot}$ \\
& & & & & & $LSTM$ &  $>$  & $ENC_{graph}$ \\
& & & & & & $HuGuR_5$ &  $>$  & $ENC_{graph}$ \\
& & & & & & $LSTM$ &  $>$  & $HuGuR$\\
        \bottomrule
    \end{tabular}
    
    
\end{table}

\begin{table}[]
    \centering
    \caption{Number of trainable parameters, i.e. the model complexity, for each model compared in our experiments. For $HuGuR$ and $HuGuR_5$ we provide the average and standard deviation for the number of parameters of all used models.}
    \label{tab:model_comp}
    \begin{tabular}{l|cccccc}
    \toprule
         Data set  & $ENC_{one-hot}$ & $ENC_{graph}$ & $LSTM$ & $HuGuR$ & $HuGuR_5$  \\
         \midrule
$edm_5\_small$   &  2701  &  1851  &  1241  & 90.25(54.49) & 112.40(22.84) \\ 
$edm_9\_small$   &  41601  &  45201  &  17101  & 65.75(48.72) & 47.60(16.32) \\ 
$edm_{18}\_small$   &  37601  &  51301  &  2361  & 147.14(171.09) & 234.80(236.92) \\ 
$cc_{enron}$ &   296201  &  608401  &  23401  & 98.00(49.46) & 67.20(24.03) \\ 
$cc_{yeast}\_small$   &  64601  &  70201  &  56201  & 171.83(114.31) & 246.40(112.02) \\ 
\hline 
$edm_5\_large$   &  17801  &  1851  &  57601  & 64.62(40.10) & 92.60(37.40) \\ 
$edm_9\_large$   &  4151  &  15101  &  1401  & 108.42(55.39) & 152.00(39.70) \\ 
$edm_{18}\_large$   &  90201  &  97401  &  2361  & 146.77(157.55) & 222.20(221.48) \\ 
$cc_{yeast}\_large$   &  64601  &  70201  &  51101  & 162.17(138.96) &  262.40(140.57)\\
        \bottomrule
    \end{tabular}
    
    
\end{table}






\subsection{User Study Evaluation}

In addition to comparing the performance of the human-built models with the baseline models, we also evaluated some potential influences of different model properties on the performance of a model. In Figure \ref{fig:vals_to_r2}, we scatter the $R^2$ score of a model against one of its properties and plot the line of best linear fit. Figure \ref{fig:lr_to_r2} indicates that overall a higher learning rate leads to a better performance, and Figure \ref{fig:nr_to_r2} shows that for the most data sets a higher number of constraints $l$ added in each step increases the performance. In Figure \ref{fig:ni_to_r2}, the relation of the $R^2$ score to the number of iterations of refinement by a user is presented and Figure \ref{fig:np_to_r2} depicts the relation between performance and the number of coefficients in the model (i.e. the complexity of the model). 


\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \resizebox{\linewidth}{!}{\input{nr_to_r2.pgf}}
       \caption{Relation to the hyperparameter number of constraints $l$.}
        \label{fig:nr_to_r2}
    \end{subfigure}
    \hfill
  \begin{subfigure}{0.49\textwidth}
        \resizebox{\linewidth}{!}{\input{lr_to_r2.pgf}}
        \caption{Relation to the hyperparameter learning rate.}
        \label{fig:lr_to_r2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \resizebox{\linewidth}{!}{\input{ni_to_r2.pgf}}
        \caption{Relation to the number of iteration steps during the training.}
        \label{fig:ni_to_r2}
    \end{subfigure}
    \hfill
   \begin{subfigure}{0.49\textwidth}
        \resizebox{\linewidth}{!}{\input{np_to_r2.pgf}}
        \caption{Relation to the complexity of the model.}
        \label{fig:np_to_r2}
    \end{subfigure}

    \caption{For each data set -- data sets with small and large versions such as $edm_5$ are combined into one plot -- we examine the relation between the measured performance, here the $R^2$ score, and multiple properties of the user-built models. Each point represents one human-built model. A dotted line represents a negative slope of the line of best fit, while a solid line represents a positive slope.}
    \label{fig:vals_to_r2}
\end{figure}

We also studied those relations on the level of the users. We calculated the average properties and the normalized average performance of all the models built by each user and scattered them against each other. These plots together with their lines of best fit are presented in Figure \ref{fig:user_behave}. Here we can again see a positive correlation between the performance and the learning rate as well as the number of constraints $l$. Table \ref{tab:user_stats} reveals that the top five performers employed higher learning rates and more constraints in their models. In addition, they frequently adjusted hyperparameters, suggesting that this tuning, along with higher learning rates and more constraints, is associated with better performance.


\begin{figure}[!ht]
    \centering
    \resizebox{\linewidth}{!}{\input{user_to_r2.pgf}}
    \caption{Average model properties in relation to the average %
performance for each participant. A solid line represents a positively sloped line of best fit.}
    \label{fig:user_behave}
\end{figure}


\begin{table}[!ht]
    \centering
    \caption{Use statistics for all users, the five users with the best performance and the five users with the worst performance. For each group, the average number of iterations of model building, the average rate of reset operations, the average rate of hyperparameter changes, the average learning rate and, the average number of constraints $l$ are compared with each other.}
    \begin{tabular}{r|c|c|c}

        & All users & Top 5 users & Bottom 5 users \\ 
        \hline\hline
        Number of Iterations & 52.9919(51.3835) & 44.7111(5.7157) & 28.4008(9.8094) \\ \hline
        Rate of resets & 0.0901(0.0631) & 0.1308(0.0816) & 0.0785(0.0435) \\ \hline
        Rate of new parameters & 0.0867(0.1038) & 0.1444(0.1565) & 0.0673(0.0466) \\ \hline
        Learning Rate & 0.5421(0.2714) & 0.8113(0.1090) & 0.2828(0.1598) \\ \hline
        Number of constraints & 11.0382(3.7507) & 14.1333(3.8505) & 9.5008(2.4914)
    \end{tabular}
    \label{tab:user_stats}
\end{table}
