\section{Related Work}
\label{sec:rel_work}
Remote Sensing Image Captioning (RSIC) has seen significant advancements in recent years, using deep learning techniques to generate textual descriptions of satellite and aerial imagery. Although natural image capture (NIC) has been extensively studied**Qu et al., "Deep Multimodal Neural Network for Generating Descriptions from Remote Sensing Images"**, RSIC remains a more specialized field with unique challenges due to its domain-specific characteristics**Wang et al., "Global-Local Captioning Model"**. The most widely adopted framework in this domain is the encoder-decoder architecture**Ren et al., "Mask-Guided Transformer Network"**, which forms the foundation of many state-of-the-art models.

Early approaches to RSIC relied on handcrafted feature extraction and traditional machine learning techniques. Qu et al.**introduced a deep multimodal neural network for generating descriptions from remote sensing images**. Shi et al.**leveraged fully convolutional networks for feature extraction, improving interpretability**. Lu et al.**performed a comparative analysis on existing models using datasets such as SYDNEY and UCM**, leading to the development of the RSICD dataset to address training data limitations. Support Vector Machines (SVMs) were also explored as classification-based captioning approaches**Ye et al., "Joint-Training Two-Stage Captioning Framework"**, demonstrating initial improvements but failing to capture complex spatial relationships effectively.

To overcome these limitations, attention mechanisms were introduced, significantly improving feature extraction and sequence modeling capabilities**Wu et al., "Dual-Transformer Attention Mechanism"**. Lu et al.**proposed a sound active attention framework**, while Li et al.**designed a multilevel attention model for RSIC**. Wang et al.**incorporated retrieval-based memory networks to improve context retention**. Ma et al.**and Sumbul et al.**explored multiscale and summarization-driven attention strategies**. Additionally, Huang et al.**introduced a denoising-based multiscale feature fusion method to mitigate noise in remote sensing data**. These methods significantly improved captioning accuracy but still struggled with long-term dependencies and contextual consistency.

The introduction of transformer-based architectures marked a major shift in RSIC**Liu et al., "Multilayer Aggregated Transformer Model"**. Ren et al.**introduced a mask-guided transformer network**, while Liu et al.**developed a multilayer aggregated transformer model**. Wang et al.**proposed a global-local captioning model to enhance feature representation, and Ye et al.**explored a joint-training two-stage captioning framework**. Wu et al.**further refined transformer-based approaches with a dual-transformer attention mechanism, integrating Swin transformer for multiscale feature extraction**. These advancements significantly improved sequence modeling and linguistic coherence in generated captions.

Despite these improvements, existing models still suffer from inefficiencies in encoder utilization and suboptimal feature aggregation in the decoder. Many studies focus on refining the decoder**Ma et al., "Multiscale and Summarization-Driven Attention Strategies"**, but optimal feature extraction remains an open challenge. To address these issues, our approach leverages a fusion-based encoding strategy that integrates multiple CNN architectures for enhanced feature representation. Additionally, we incorporate locally weighted stacking of decoders to retain hierarchical linguistic patterns effectively.