\section{Related Work}
\label{sec:rel_work}
Remote Sensing Image Captioning (RSIC) has seen significant advancements in recent years, using deep learning techniques to generate textual descriptions of satellite and aerial imagery. Although natural image capture (NIC) has been extensively studied____, RSIC remains a more specialized field with unique challenges due to its domain-specific characteristics____. The most widely adopted framework in this domain is the encoder-decoder architecture____, which forms the foundation of many state-of-the-art models.

Early approaches to RSIC relied on handcrafted feature extraction and traditional machine learning techniques. Qu et al.____ introduced a deep multimodal neural network for generating descriptions from remote sensing images. Shi et al.____ leveraged fully convolutional networks for feature extraction, improving interpretability. Lu et al.____ performed a comparative analysis on existing models using datasets such as SYDNEY____ and UCM____, leading to the development of the RSICD dataset to address training data limitations. Support Vector Machines (SVMs) were also explored as classification-based captioning approaches____, demonstrating initial improvements but failing to capture complex spatial relationships effectively.

To overcome these limitations, attention mechanisms were introduced, significantly improving feature extraction and sequence modeling capabilities____. Lu et al.____ proposed a sound active attention framework, while Li et al.____ designed a multilevel attention model for RSIC. Wang et al.____ incorporated retrieval-based memory networks to improve context retention. Ma et al.____ and Sumbul et al.____ explored multiscale and summarization-driven attention strategies. Additionally, Huang et al.____ introduced a denoising-based multiscale feature fusion method to mitigate noise in remote sensing data. These methods significantly improved captioning accuracy but still struggled with long-term dependencies and contextual consistency.

The introduction of transformer-based architectures marked a major shift in RSIC____. Ren et al.____ introduced a mask-guided transformer network, while Liu et al.____ developed a multilayer aggregated transformer model. Wang et al.____ proposed a global-local captioning model to enhance feature representation, and Ye et al.____ explored a joint-training two-stage captioning framework. Wu et al.____ further refined transformer-based approaches with a dual-transformer attention mechanism, integrating Swin transformer for multiscale feature extraction. These advancements significantly improved sequence modeling and linguistic coherence in generated captions.

Despite these improvements, existing models still suffer from inefficiencies in encoder utilization and suboptimal feature aggregation in the decoder. Many studies focus on refining the decoder____, but optimal feature extraction remains an open challenge. To address these issues, our approach leverages a fusion-based encoding strategy that integrates multiple CNN architectures for enhanced feature representation. Additionally, we incorporate locally weighted stacking of decoders to retain hierarchical linguistic patterns effectively.