%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage[title]{appendix}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{manyfoot}
\usepackage{booktabs}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{lmodern}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{arydshln}

\journal{Information Fusion}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{FE-LWS: Refined Image-Text Representations via Decoder Stacking and Fused Encodings for Remote Sensing Image Captioning} %% Article title

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[label1]{Swadhin Das}
\author[label1]{Raksha Sharma}

%% Author affiliation
\affiliation[label1]{organization={Dept. of CSE, Indian Institute of Technology, Roorkee},%Department and Organization
            addressline={Roorkee}, 
            city={Haridwar},
            postcode={247667}, 
            state={Uttarakhand},
            country={India}}

%% Abstract
\begin{abstract}
Remote sensing image captioning aims to generate descriptive text from remote sensing images, typically employing an encoder-decoder framework. In this setup, a convolutional neural network (CNN) extracts feature representations from the input image, which then guide the decoder in a sequence-to-sequence caption generation process. Although much research has focused on refining the decoder, the quality of image representations from the encoder remains crucial for accurate captioning. This paper introduces a novel approach that integrates features from two distinct CNN based encoders, capturing complementary information to enhance caption generation. Additionally, we propose a weighted averaging technique to combine the outputs of all GRUs in the stacked decoder. Furthermore, a comparison-based beam search strategy is incorporated to refine caption selection. The results demonstrate that our fusion-based approach, along with the enhanced stacked decoder, significantly outperforms both the transformer-based state-of-the-art model and other LSTM-based baselines.
\end{abstract}

% %%Graphical abstract
% \begin{graphicalabstract}
% %\includegraphics{grabs}
% \end{graphicalabstract}

% %%Research highlights
% \begin{highlights}
% \item Research highlight 1
% \item Research highlight 2
% \end{highlights}

%% Keywords
\begin{keyword}
Remote Sensing Image Captioning (RSIC) \sep Encoder-Decoder model \sep Convolutional Neural Network (CNN) \sep Gated Recurrent Unit (GRU) \sep Weighted Average \sep Beam Search Method \sep Feature Fusion \sep Comparison-based Beam Search (CBS) \sep Fusion-based Encoders (FE) \sep Local Weighted-based Stacking (LWS).
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{Introduction}
Image captioning is a task in the machine learning domain that aims to extract meaningful information from an image. Unlike other machine learning tasks such as classification, where object detection plays a crucial role, image captioning requires not only identifying objects but also understanding their relationships to generate coherent descriptions. 

There are two primary types of image captioning. In Natural Image Captioning (NIC), the objective is to create textual descriptions of images taken in real world settings~\cite{singh2021image,wang2023controllable}. These images often feature people, animals, and everyday objects. These captions often describe the scene in a natural and context-aware manner.  Remote Sensing Image Captioning (RSIC) focuses on generating descriptions for satellite or aerial imagery~\cite{qu2016deep,lu2017exploring}. This involves interpreting high-resolution geographical and environmental data, identifying land use patterns, and describing structures such as roads, rivers, forests, and urban areas. RSIC has gained significant attention in recent years due to its wide-ranging applications, including land use classification, disaster monitoring, environmental surveillance, and urban planning.

RSIC presents several challenges, making it a complex task~\cite{lu2017exploring}. First, models trained in natural images often struggle to generalize to remote sensing data~\cite{qu2016deep,lu2017exploring}. This is because remote sensing images exhibit distinct patterns, such as land use structures and topographical features. As a result, specialized techniques are required for accurate interpretation. Second, the scarcity of large-scale annotated datasets poses a major hurdle. Unlike natural image captioning, where extensive human-labeled datasets exist, RSIC datasets are limited and expensive to annotate, leading to issues such as overfitting and poor generalization. Another critical challenge is the extraction of meaningful image features, as RSIC demands analyzing intricate spatial structures, distinguishing between visually similar objects such as~\emph{roads} vs.~\emph{rivers},~\emph{farmlands} vs.~\emph{barren land}, and understanding positional relationships without absolute directional signals, where relative terms such as~\emph{near} or~\emph{far} replace absolute directions such as~\emph{north} or~\emph{south}~\cite{lu2017exploring}. These complexities make feature representation a crucial aspect, where minor spatial variations can significantly influence the generated captions. Furthermore, RSIC images are captured under diverse environmental conditions across different geographical locations, where factors such as seasonal changes, varying illumination, cloud cover, and atmospheric distortions impact the consistency and reliability of the extracted features, making accurate caption generation more challenging. Hence, overcoming domain-specific constraints, data limitations, and environmental variations is essential to ensure robust and high-quality captioning in RSIC.

The encoder-decoder model~\cite{hoxha2020new,hoxha2021novel,das2024textgcn} is the most widely used framework in RSIC. In this model, the input image is first processed by an encoder, typically a Convolutional Neural Network (CNN), to extract feature representations. These encoded features are then passed to a decoder, such as an RNN, LSTM, or GRU, which generates the corresponding caption sequentially. Existing research focuses predominantly on improving the decoder, while relatively little attention has been paid to optimizing the encoder~\cite{hoxha2021novel,das2024textgcn}. However, the encoder plays a crucial role in determining the quality of extracted features and directly influencing the captioning performance. To address this gap, we employ a fusion strategy that integrates two different encoders to extract image features more effectively. This approach ensures a richer and more diverse feature representation, allowing the model to capture fine-grained spatial structures, differentiate visually similar objects more accurately, and enhance robustness against environmental variations. Using complementary strengths of multiple encoders, our method significantly improves the quality of feature extraction, leading to more precise and contextually relevant captions.

The existing works employ a single decoder for caption generation~\cite{li2018multi,das2024unveiling,vinyals2015show}. However, using a single decoder limits the model's ability to capture hierarchical linguistic patterns and long-term dependencies, as all sequential processing relies on a single layer of transformation. This often leads to the loss of intermediate contextual information, making it difficult for the model to generate coherent and detailed captions. The stacking of multiple decoders improves the capacity of the model to process sequential data more effectively ~\cite{alabdulkreem2023sustainable,vakharia2023estimation}. By passing information through multiple decoding layers, the model can refine feature representations at each stage, leading to improved contextual understanding and more accurate word predictions. This approach also helps in mitigating vanishing-gradient issues, which commonly affect deep sequential models, thereby improving overall performance. However, simply using only the final layer's output in stacked decoders can lead to information loss, as deeper layers may override useful features. Additionally, deeper architectures risk an imbalanced information flow, causing redundancy or feature dilution. To address these issues, we utilized the outputs of all stacked decoders instead of relying solely on the final layer. By applying weighted averaging to the output of all GRUs, the model effectively integrates information from multiple layers. This approach captures both low-level and high-level feature representations, improving detail retention, and enhancing overall captioning quality. 

In image captioning, search strategies play a crucial role in generating meaningful textual descriptions by determining the most probable sequence of words. Two common approaches for this task are greedy search and beam search~\cite{das2024unveiling}. Greedy search selects the highest-probability word at each step, but it often produces suboptimal captions, as it does not consider long-term dependencies. The beam search enhances this process by maintaining multiple candidate sequences (beams) simultaneously, expanding them iteratively, and ultimately selecting the sequence with the highest cumulative probability. However, traditional beam search has its drawbacks, as it relies exclusively on log-likelihood scores, often resulting in overly generic or less meaningful captions. To address these challenges, Comparison-based Beam Search (CBS)~\cite{hoxha2020new} incorporates an additional evaluation step. Rather than relying solely on likelihood scores, CBS compares candidate captions with reference captions obtained from visually similar images, using metrics such as BLEU, METEOR, and ROUGE-L. This approach ensures that the final caption is not only statistically likely, but also contextually relevant. By incorporating external references, CBS mitigates the tendency of traditional beam search to favor excessively long or generic captions, ultimately enhancing both the accuracy and relevance of RSIC-generated descriptions. Unlike conventional methods that emphasize only numerical probabilities, CBS integrates both quantitative and qualitative assessments, leading to more precise and semantically appropriate captions.

The major contributions of this paper are as follows.  
\begin{itemize}  
    \item For enhanced image representation and better understanding, we propose a fusion-based encoding strategy.  
    \item For more effective and improved sequence modeling, we propose a two-layer stacked GRU decoder with weighted averaging.  
    \item We conduct a thorough comparison against the state-of-the-art transformer-based model and various baseline methods, showcasing the clear superiority of our model.
\end{itemize}

The structure of the paper is as follows:~\Cref{sec:rel_work} reviews existing RSIC models, highlighting their strengths and limitations.~\Cref{sec:prop_method} introduces our proposed approach, focusing on fusion-based encoding and stacking-based decoding techniques.~\Cref{sec:exp_setup} describes the experimental setup, covering datasets, model configurations, and evaluation metrics.~\Cref{sec:results} presents the results, providing a comparative analysis with baseline models using both quantitative and qualitative evaluations. Finally,~\Cref{sec:conclusion}  summarizes the key findings of the paper and highlights potential avenues for future research.
\section{Related Work}
\label{sec:rel_work}
Remote Sensing Image Captioning (RSIC) has seen significant advancements in recent years, using deep learning techniques to generate textual descriptions of satellite and aerial imagery. Although natural image capture (NIC) has been extensively studied~\cite{you2016image,li2018multi,vinyals2015show}, RSIC remains a more specialized field with unique challenges due to its domain-specific characteristics~\cite{hoxha2020new,hoxha2021novel,qu2016deep}. The most widely adopted framework in this domain is the encoder-decoder architecture~\cite{qu2016deep,hoxha2021novel,hoxha2020new,lu2017exploring}, which forms the foundation of many state-of-the-art models.

Early approaches to RSIC relied on handcrafted feature extraction and traditional machine learning techniques. Qu et al.~\cite{qu2016deep} introduced a deep multimodal neural network for generating descriptions from remote sensing images. Shi et al.~\cite{shi2017can} leveraged fully convolutional networks for feature extraction, improving interpretability. Lu et al.~\cite{lu2017exploring} performed a comparative analysis on existing models using datasets such as SYDNEY~\cite{qu2016deep} and UCM~\cite{qu2016deep}, leading to the development of the RSICD dataset to address training data limitations. Support Vector Machines (SVMs) were also explored as classification-based captioning approaches~\cite{hoxha2021novel}, demonstrating initial improvements but failing to capture complex spatial relationships effectively.

To overcome these limitations, attention mechanisms were introduced, significantly improving feature extraction and sequence modeling capabilities~\cite{xu2015show}. Lu et al.~\cite{lu2019sound} proposed a sound active attention framework, while Li et al.~\cite{li2020multi} designed a multilevel attention model for RSIC. Wang et al.~\cite{wang2020retrieval} incorporated retrieval-based memory networks to improve context retention. Ma et al.~\cite{ma2020multiscale} and Sumbul et al.~\cite{sumbul2020sd} explored multiscale and summarization-driven attention strategies. Additionally, Huang et al.~\cite{huang2020denoising} introduced a denoising-based multiscale feature fusion method to mitigate noise in remote sensing data. These methods significantly improved captioning accuracy but still struggled with long-term dependencies and contextual consistency.

The introduction of transformer-based architectures marked a major shift in RSIC~\cite{vaswani2017attention}. Ren et al.~\cite{ren2022mask} introduced a mask-guided transformer network, while Liu et al.~\cite{liu2022remote} developed a multilayer aggregated transformer model. Wang et al.~\cite{wang2022glcm} proposed a global-local captioning model to enhance feature representation, and Ye et al.~\cite{ye2022joint} explored a joint-training two-stage captioning framework. Wu et al.~\cite{wu2024trtr} further refined transformer-based approaches with a dual-transformer attention mechanism, integrating Swin transformer for multiscale feature extraction. These advancements significantly improved sequence modeling and linguistic coherence in generated captions.

Despite these improvements, existing models still suffer from inefficiencies in encoder utilization and suboptimal feature aggregation in the decoder. Many studies focus on refining the decoder~\cite{hoxha2020new,hoxha2021novel,das2024textgcn}, but optimal feature extraction remains an open challenge. To address these issues, our approach leverages a fusion-based encoding strategy that integrates multiple CNN architectures for enhanced feature representation. Additionally, we incorporate locally weighted stacking of decoders to retain hierarchical linguistic patterns effectively.

\section{Proposed Method}
\label{sec:prop_method}
\begin{figure}[!ht]
    \centering
    \includegraphics[width = \textwidth]{Architecture.png}
    \caption{Architecture of the Proposed Model}
    \label{architecture}
\end{figure}
The architecture of our model is depicted in \Cref{architecture}. In this paper, we employ an encoder-decoder framework as the core of our design. The encoder is composed of two distinct Convolutional Neural Networks (CNNs), which are utilized to capture and process input features. The output of these CNNs is concatenated and passed through a linear transformation layer ($L_1$) for further processing.

For the decoder, we adopt a sequence-to-sequence approach. Initially, at the first timestamp, a starting token ($sseq$) is selected as the input sequence and passed through a tokenizer ($T$), which converts the sequence into a tokenized vector. This vector is then fed into an embedding layer ($E$), and the resulting embedded representation is subsequently input into the decoder stack ($G$). The decoder consists of two stacked Gated Recurrent Units (GRUs), which handle temporal dependencies within the sequence. The output of the linear encoder layer ($L1$) and the decoder stack ($G$) is concatenated and then passed through a series of linear layers ($L2$ and $L3$). The final layer, $L3$, is a softmax layer that produces the probability distribution for each word in the vocabulary. Then, according to the searching technique, the decision is taken. For example, in greedy search, the word with the highest probability is selected for the next position in the sequence. In beam search, instead of selecting only the most probable word at each step, the algorithm keeps track of multiple candidate sequences (beams) simultaneously. Expands all possible sequences by considering the top $k$ words in each step, where $k$ is the beam width. The algorithm then retains the
$k$ most probable sequences based on their cumulative probability scores.
\subsection{Encoder: Fusion of Different CNNs}
\begin{figure}[!ht]
    \centering
    \includegraphics[width = \textwidth]{Fusion.png}
    \caption{Architecture of the Fusion Technique Used in Our Model}
    \label{fusion}
\end{figure}
Our method involves combining features extracted from two prominent CNN architectures. In this paper, we consider three widely used CNN models: ConvNext, ResNet, and ResNext. We explore different pairwise combinations to determine the most effective feature fusion strategy.
\begin{itemize}
    \item\textbf{ConvNext~\cite{liu2022convnet}:} It improves the extraction of features due to some structural changes incorporated in the modified version of the ResNet architecture. Instead of normal convolutions, more efficient depth separable convolutions are used. The use of an inverted bottleneck structure allows of capturing a wider and stronger range of features using higher kernel sizes. Coupled with these changes, GELU non-linearities are incorporated to enhance the expressiveness of ConvNext, as well as layer normalization instead of batch normalization for improvement in training stability. Thus, these changes lead to the competitive performance of ConvNext in image classification and object detection tasks within the framework of convolutional neural networks.
    \item\textbf{ResNet~\cite{he2016deep}:} It addresses the problem of vanishing gradients commonly encountered in deep neural networks. It employs skip connections, allowing the model to bypass several layers and preserve essential information, even in very deep networks. This characteristic makes ResNet particularly effective for feature extraction. As a result, the decoder retains a clearer understanding of the features, reducing the risk of losing critical data during the extraction process.
    \item\textbf{ResNext~\cite{xie2017aggregated}:} It improves deep neural network architectures by introducing a novel dimension called $cardinality$, which refers to the number of parallel paths within a network layer. This approach involves repeating a building block that aggregates a set of transformations with identical topology, effectively expanding the network's capacity beyond traditional depth and width dimensions. By increasing cardinality, ResNext achieves improved performance and efficiency in image classification tasks.
\end{itemize}
The architecture of the fusion strategy is shown in~\Cref{fusion}.~\Cref{SYDNEY_CNN,UCM_CNN,RSICD_CNN} indicates that the integration of these CNNs further enhances their effectiveness, leading to improved performance. In particular, this fusion approach does not introduce a significant computational burden, as CNNs are used only for feature extraction and remain frozen during training. Since encoder networks are not updated, the training process maintains efficiency while benefiting from complementary features extracted from multiple CNNs. These results clearly show that the combination of ConvNext and ResNet yields the best performance. Therefore, we have chosen this pair for the remainder of the paper.
\subsection{Decoder: Stacking of two GRUs}
Our model uses the Gated Recurrent Unit (GRU)~\cite{khan2022deep} as the decoder. The GRU effectively manages the flow of information through its gating mechanisms, allowing it to maintain crucial temporal dependencies while minimizing computational complexity. Furthermore, it mitigates the vanishing gradient problem often encountered in sequential models, enhancing its capability to capture long-term dependencies.

Conventional encoder-decoder architectures~\cite{qu2016deep, hoxha2020new, hoxha2021novel,das2024unveiling} typically employ a single decoder. This decoder processes the input sequence and generates the final output, following a sequence-to-sequence transformation. Recent studies~\cite{alghamdi2024predicting, liang2024clustering, ren2024network} have explored stacked decoder architectures. In this approach, multiple decoders are arranged in succession, with the output of one feeding into the next. The final output comes from the last decoder in the stack. This approach enables deeper feature extraction and hierarchical learning, refining the input sequence at each stage. Essentially, stacked decoders are particularly effective in modeling long-term dependencies and enhancing contextual understanding due to their iterative processing of data. By incorporating multiple decoding layers, these models significantly improve their capacity to manage complex temporal patterns.

In our approach, we employ a two-layer stacked GRU structure in the decoder to enhance the model’s ability to process sequential data and improve its robustness in RSIC. However, simply stacking GRUs sequentially and relying solely on the final layer’s output does not always yield the optimal result. This is because deeper networks can suffer from gradient vanishing, and important intermediate features might be lost if only the last GRU's output is considered. Furthermore, the information flow between stacking layers may not be efficiently balanced, leading to suboptimal learning. To address these challenges, we explore four different stacking strategies.
\subsubsection{Simple Stacking}  
\begin{table}[!ht]
\centering
\caption{Experimental Results of Different CNNs and Decoders on SYDNEY Dataset}
\label{SYDNEY_CNN}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
CNN & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
ConvNext & 0.7531 & 0.6645 & 0.5982 & 0.5376 & 0.3750 & 0.6951 & 2.3954 \\
ResNet & 0.7363 & 0.6502 & 0.5785 & 0.5138 & 0.3621 & 0.6809 & 2.2526 \\
ResNext & 0.7415 & 0.6556 & 0.5845 & 0.5209 & 0.3625 & 0.6706 & 2.2912 \\
ConvNext-ResNet &~\textbf{0.7679} &~\textbf{0.6814} &~\textbf{0.6084} &~\textbf{0.5458} & 0.3801 &~\textbf{0.7263} &~\textbf{2.4515} \\
ConvNext-ResNext & 0.7582 & 0.6727 & 0.6039 & 0.5438 &~\textbf{0.3868} & 0.7164 & 2.3882 \\
ResNet-ResNext & 0.7517 & 0.6664 & 0.5924 & 0.5243 & 0.3789 & 0.6963 & 2.3515 \\
\hline
\end{tabular}%
}
\end{table}
\begin{table}[!ht]
\centering
\caption{Experimental Results of Different CNNs and Decoders on UCM Dataset}
\label{UCM_CNN}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
CNN & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
ConvNext & 0.8369 & 0.7804 & 0.7312 & 0.6853 & 0.4689 & 0.8037 & 3.3990 \\
ResNet & 0.8297 & 0.7705 & 0.7178 & 0.6683 & 0.4506 & 0.7903 & 3.3329 \\
ResNext & 0.8170 & 0.7607 & 0.7118 & 0.6658 & 0.4583 & 0.7736 & 3.3165 \\
ConvNext-ResNet &~\textbf{0.8457} &~\textbf{0.7919} &~\textbf{0.7533} &~\textbf{0.7166} &~\textbf{0.4833} &~\textbf{0.8163} &~\textbf{3.4255} \\
ConvNext-ResNext & 0.8406 & 0.7796 & 0.7289 & 0.6813 & 0.4656 & 0.7996 & 3.4132 \\
ResNet-ResNext & 0.8334 & 0.7739 & 0.7236 & 0.6723 & 0.4580 & 0.7944 & 3.4108 \\
\hline
\end{tabular}%
}
\end{table}
\begin{table}[!ht]
\centering
\caption{Experimental Results of Different CNNs and Decoders on RSICD Dataset}
\label{RSICD_CNN}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
CNN & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
ConvNext & 0.6327 & 0.4636 & 0.3602 & 0.2903 & 0.2487 & 0.4648 & 0.8272 \\
ResNet & 0.6322 & 0.4621 & 0.3557 & 0.2835 & 0.2489 & 0.4649 & 0.8206 \\
ResNext & 0.6246 & 0.4525 & 0.3463 & 0.2639 & 0.2294 & 0.4448 & 0.8117 \\
ConvNext-ResNet &~\textbf{0.6452} &~\textbf{0.4767} &~\textbf{0.3732} &~\textbf{0.3087} &~\textbf{0.2609} &~\textbf{0.4827} &~\textbf{0.8594} \\
ConvNext-ResNext & 0.6414 & 0.4717 & 0.3676 & 0.2970 & 0.2551 & 0.4741 & 0.8429 \\
ResNet-ResNext & 0.6427 & 0.4734 & 0.3684 & 0.2974 & 0.2558 & 0.4743 & 0.8431 \\
\hline
\end{tabular}%
}
\end{table}
The traditional stacking approach, called Simple Stacking (SS), directly utilizes the output of the final decoder layer as the final output of the system. The overall output of the SS system is given below.  
    \begin{equation}  
        O = B  
    \end{equation}
\subsubsection{Concatenation-based Stacking} 
In Concatenation-based Stacking (CS), the outputs of both decoder layers are concatenated to form the final output, thus preserving a richer set of feature representations. Then the overall output $O$ of the CS system is given below.
    \begin{equation}
        O = A|B
    \end{equation}
\subsubsection{Global Weighted-based Stacking}  
In Global Weighted-based Stacking (GWS), a weighted average of the output of both decoder layers is calculated using a single global weight, ensuring a balanced contribution from each layer. The output of the GWS system is given below. 
    \begin{equation}
    O = \alpha\times A+(1-\alpha)\times B.    
    \end{equation}
\subsubsection{Local Weighted-based Stacking}  
In Local Weighted-based Stacking (LWS), a weighted average of the decoder output is calculated using element-specific weights, allowing for finer control over the contribution of each feature to the final output. The output of the LWS system is given below.
    \begin{equation}
     O = \sum_{i=1}^{n} \left(W[i] \times A[i] + (1 - W[i]) \times B[i] \right)   
    \end{equation}
      
Here, $A$ and $B$ represent the first and second decoder outputs, respectively, while $O$ is the final output of the stacked system. The parameter $n$ denotes the output size of each decoder. The symbol $|$ represents the concatenation operation. Furthermore, $0\leq\alpha\leq 1$ and $0\leq x\leq 1,~\forall x \in W$, where both $\alpha$ and $W$ are learnable parameters.

\Cref{SYDNEY_STACK,UCM_STACK,RSICD_STACK} presents the comparative performance of different stacking methods (SS, CS, GWS, LWS) alongside a single GRU (no stacking or NS), clearly demonstrating the superiority of LWS. SS and CS do not fully utilize the decoder output, as SS depends only on the output of the final layer, leaving useless intermediate information, while CS concatenates the outputs without differentiating their relative importance, leading to redundancy. To overcome these limitations, weighted average-based methods provide a more balanced approach by blending information from multiple decoder layers. GWS improves on SS and CS by assigning a single global weight to regulate the contributions of each layer. However, GWS treats all elements uniformly, ignoring variations in significance across different time steps, which can lead to suboptimal feature selection. LWS addresses this limitation by assigning element-specific weights, allowing the model to dynamically adjust the influence of each decoder layer based on local dependencies. This ensures better retention of crucial intermediate features, prevents information loss, and enhances gradient flow, resulting in improved sequence modeling. Thus, LWS effectively balances information preservation and adaptability, making it the most robust stacking strategy.
\begin{table}[!ht]
\centering
\caption{Experimental Results of Different Stacking Techniques of Decoders on SYDNEY Dataset}
\label{SYDNEY_STACK}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
CNN & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
NS & 0.7679 & 0.6814 & 0.6084 & 0.5458 & 0.3801 & 0.7263 & 2.4515 \\
SS & 0.7691 & 0.6817 & 0.6110 & 0.5572 & 0.3884 & 0.7286 & 2.4348 \\
CS & 0.7643 & 0.6803 & 0.6245 & 0.5732 & 0.4284 & 0.7146 & 2.4248 \\
GWS & 0.7708 & 0.6963 & 0.6360 & 0.5808 & 0.4348 & 0.7415 & 2.4413 \\
LWS &~\textbf{0.7935} &~\textbf{0.7197} &~\textbf{0.6575} &~\textbf{0.6029} &~\textbf{0.4391} &~\textbf{0.7491} &~\textbf{2.5812} \\
\hline
\end{tabular}%
}
\end{table}
\begin{table}[!ht]
\centering
\caption{Experimental Results of Different Stacking Techniques of Decoders on UCM Dataset}
\label{UCM_STACK}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
CNN & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
NS & 0.8457 & 0.7919 & 0.7533 & 0.7166 & 0.4833 & 0.8163 & 3.4255 \\
SS & 0.8437 & 0.7876 & 0.7408 & 0.6956 & 0.4703 & 0.8098 & 3.4873 \\
CS & 0.8425 & 0.7852 & 0.7379 & 0.6923 & 0.4740 & 0.8104 & 3.4828 \\ GWS & 0.8553 & 0.8010 & 0.7542 & 0.7093 & 0.4835 & 0.8089 & 3.5066 \\
LWS &~\textbf{0.8647} &~\textbf{0.8126} &~\textbf{0.7744} &~\textbf{0.7291} &~\textbf{0.4929} &~\textbf{0.8216} &~\textbf{3.6943} \\
\hline
\end{tabular}%
}
\end{table}
\begin{table}[!ht]
\centering
\caption{Experimental Results of Different Stacking Techniques of Decoders on RSICD Dataset}
\label{RSICD_STACK}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
CNN & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
NS & 0.6452 & 0.4767 & 0.3732 & 0.3087 & 0.2609 & 0.4827 & 0.8594 \\
SS & 0.6524 & 0.4807 & 0.3819 & 0.3104 & 0.2701 & 0.4891 & 0.8579 \\
CS & 0.6571 & 0.4880 & 0.3834 & 0.3124 & 0.2704 & 0.4882 & 0.8669 \\
GWS & 0.6589 & 0.4936 & 0.3897 & 0.3192 & 0.2709 & 0.4885 & 0.8613 \\
LWS &~\textbf{0.6647} &~\textbf{0.5071} &~\textbf{0.4068} &~\textbf{0.3317} &~\textbf{0.2965} &~\textbf{0.4979} &~\textbf{0.8779} \\
\hline
\end{tabular}%
}
\end{table}
\subsection{Comparison-based Beam Search Method}
\label{compare_beam}
The search methods is essential for generating text sequentially, especially in RSIC. Two traditional search methods are commonly used for sequence prediction: greedy search and beam search. In greedy search, the model predicts the next word based on its output, selecting the word with the highest probability at each step. The beam search, on the other hand, selects the top $B$ most probable words at each timestamp. However, as the number of timestamps increases, the number of possible sentence sequences grows exponentially (specifically, $B^i$ at the $i^{th}$ timestamp), leading to high computational costs. To mitigate this, only the top $B$ sequences are retained at each timestamp, which helps reduce the computational burden. Ultimately, the best caption is chosen on the basis of its log-likelihood value. Despite their utility, these traditional search methods often produce suboptimal captions.

To overcome these limitations, Hoxha et al.~\cite{hoxha2020new} introduced Comparison-based Beam Search (CBS). Although the core architecture remains unchanged, the final selection process is different. Instead of solely relying on log-likelihood for caption selection, all generated captions are compared with those of similar images. The best caption is chosen on the basis of this comparative analysis. In this approach, k-nearest neighbor (k-NN)~\cite{hoxha2020new} is used to identify similar images, and BLEU-2 is used to evaluate candidate captions against those of similar images. Das et al.~\cite{das2024textgcn} show that only BLEU-2 is not sufficient for this. Hence, they have considered the arithmetic mean of BLEU-2, METEOR and ROUGE-L score to balance the precision (BLEU), recall (METEOR), and the longest common subsequences (ROUGE-L). In addition, to make searching more versatile~\cite{das2024unveiling}, they have included the caption generated by greedy search if it is not part of the
search space. In this paper, we have considered this approach of CBS.

\section{Experimental Setup}
\label{sec:exp_setup}
In our work, we have utilized a conventional encoder-decoder architecture. The decoder stack of two GRUs ($G$) has an output size of $256$. Linear layers, denoted as $L1$, $L2$, and $L3$, have output sizes of $256$, $512$, and one more than the size of the vocabulary, respectively. The activation functions used are GELU for $L1$ and $L2$, and SoftMax for $L3$. A dropout of $0.5$ is applied three times: once before $L1$, once after $E$, and once before $SL$ to reduce the chance of overfitting. The size of the embedding layer is set to $256$. For model training, a batch size of $64$ is chosen. An early stopping criterion is employed instead of running the model for a fixed number of epochs, halting training when the model does not improve for consecutive $n$ epochs. We have set $n$ as eight in our work. The ROUGE-L score is tracked during each epoch for validation to assess the model's improvement over time. The categorical cross-entropy is the loss function, optimized using the Adam optimizer with learning rate $10^{-4}$. 

The archive is constructed from training and validation data from the dataset of interest. In the CBS method, we have chosen a beam size of five. To select reference captions, we have identified four similar images using the k-nearest neighbor algorithm~\cite{hoxha2020new}. We have measured the Euclidean distances between the input image and the images in our archive to find similar ones. Subsequently, we have extracted captions from these similar images to use as reference captions.

The experiments were carried out using TensorFlow within a Docker image with GPU support on a host machine equipped with a NVIDIA RTX A6000 GPU with $50$ GB of memory. The host machine itself had a total RAM capacity of $132$ GB.
\subsection{Datasets Used}
We assessed the performance of our model on three widely recognized RSIC datasets. The specifics of these datasets are outlined below.
\begin{itemize}
    \item\textbf{SYDNEY:} SYDNEY~\cite{qu2016deep} dataset consists of $613$ images, with 497 allocated for training, $58$ for testing, and $58$ for validation. It is derived from the Sydney dataset~\cite{zhang2014saliency} and has been carefully curated to include seven distinct categories: \emph{airport, industrial, meadow, ocean, residential, river,} and \emph{runway}, through thoughtful cropping and selection.
    \item\textbf{UCM:} UCM~\cite{qu2016deep} dataset consists of $2,100$ images, with $1,680$ allocated for training, $210$ for testing, and $21$0 for validation. It is derived from the \emph{UC Merced Land Use} dataset~\cite{yang2010bag}, which features $21$ land use image classes, each containing $100$ images. These classes include \emph{agriculture, airport, baseball diamond, beach, buildings, chaparral, denseresidential, forest, freeway, golfcourse, harbour, intersection, mediumresidential, mobilehomepark, overpass, parking, river, runway, sparseresidential, storagetanks,} and \emph{tenniscourt}.
    \item\textbf{RSICD:} RSICD~\cite{lu2017exploring} dataset includes a substantial collection of $10,921$ images, with $8,034$ designated for training, $1,093$ for testing, and $1,094$ for validation. Sourced from a variety of platforms such as Google Earth~\cite{xia2017aid}, Baidu Map, MapABC, and Tianditu, this dataset encompasses $31$ distinct image classes, including \emph{airport, bareland, baseballfield, beach, bridge, center, church, commercial, denseresidential, desert, farmland, forest, industrial, meadow, mediumresidential, mountain, park, parking, playground, pond, port, railwaystation, resort, river, school, sparseresidential, square, stadium, storagetanks,} and \emph{viaduct}.
\end{itemize}
In this study, we used the corrected versions of these datasets~\cite{das2024textgcn}, which address common issues such as spelling mistakes, grammatical errors, and inconsistent dialect variations of English. We used the same train-validation-test split as defined in these datasets.
\subsection{Performance Metrices}
The evaluation metrics employed in our model are discussed below. These metrics are widely used in RSIC~\cite{qu2016deep, lu2017exploring, das2024textgcn, das2024unveiling}.
\begin{itemize}
    \item\textbf{BLEU:} The Bilingual Evaluation Understudy (BLEU) metric~\cite{papineni-etal-2002-bleu} assesses the quality of the generated text by measuring the n-gram overlap between a generated caption and its reference captions. It calculates the geometric mean of n-gram precision scores and applies a brevity penalty to discourage excessively short outputs. BLEU is widely used in tasks such as machine translation and image captioning. In this work, we evaluate the performance using BLEU-1 through BLEU-4.
    \item\textbf{METEOR:} The Metric for Evaluation of Translation with Explicit Ordering (METEOR)~\cite{lavie-agarwal-2007-meteor} is an evaluation metric that accounts for stemming, synonymy and word order to assess the similarity between generated and reference captions. Unlike BLEU, which is based purely on precision, METEOR incorporates both precision and recall, with a stronger emphasis on accuracy.
    \item\textbf{ROUGE:} ROUGE~\cite{lin-2004-ROUGE} stands for~\emph{Recall-Oriented Understudy for Gisting Evaluation}. It is a recall-based metric that measures the overlap in n-grams between a generated caption and reference captions. In this paper, we use ROUGE-L, which is based on the longest common subsequence (LCS).
    \item\textbf{CIDEr:} CIDEr~\cite{vedantam2015cider} stands for~\emph{Consensus-based Image Description Evaluation}. It is a metric used to evaluate the quality of generated captions for images by comparing them with a set of human-written reference captions. CIDEr measures how well a generated caption aligns with the collective understanding of the image across multiple human descriptions, aiming to capture the similarity between a generated caption and the general human perspective on the image content.
\end{itemize}

\section{Results}
\label{sec:results}
Since captioning is a language generation task, which requires extensive evaluation. We evaluate our system in two modes, namely numerical and subjective. Numerical evaluation involves computation of BLEU, METEOR, ROUGE-L, and CIDEr. However, subjective evaluation is performed by a human annotator\footnote{The annotator is an experienced professional with extensive knowledge and years of expertise in RSIC.}.
 \begin{table}[!ht]
\centering
\caption{Experimental Results of Baseline Methods and Different Searching Techniques on SYDNEY Dataset}
\label{SYDNEY_SEARCH}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
METHOD & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
R-BOW~\cite{lu2017exploring} & 0.5310 & 0.4076 & 0.3319 & 0.2788 & 0.2490 & 0.4922 & 0.7019 \\
L-FV~\cite{lu2017exploring} & 0.6331 & 0.5332 & 0.4735 & 0.4303 & 0.2967 & 0.5794 & 1.4760 \\
CSMLF~\cite{wang2019semantic} & 0.4441 & 0.3369 & 0.2815 & 0.2408 & 0.1576 & 0.4018 & 0.9378 \\ 
CSMLF-FT~\cite{wang2019semantic} & 0.5998 & 0.4583 & 0.3869 & 0.3433 & 0.2475 & 0.5018 & 0.7555 \\
SVM-DBOW~\cite{hoxha2021novel} & 0.7787 & 0.6835 & 0.6023 & 0.5305 & 0.3797 & 0.6992 & 2.2722 \\
SVM-DCONC~\cite{hoxha2021novel} & 0.7547 & 0.6711 & 0.5970 & 0.5308 & 0.3643 & 0.6746 & 2.2222 \\
TrTr-CMR~\cite{wu2024trtr} & \textbf{0.8270} & 0.6994 & 0.6002 & 0.5199 & 0.3803 & 0.7220 & 2.2728 \\
TextGCN~\cite{das2024textgcn} & 0.7680 & 0.6892 & 0.6261 & 0.5786 & 0.4009 & 0.7314 &~\textbf{2.8595} \\
FE-LWS [proposed] & 0.7935 &~\textbf{0.7197} &~\textbf{0.6575} &~\textbf{0.6029} &~\textbf{0.4391} &~\textbf{0.7491} & 2.5812 \\
\hline
\end{tabular}%
}
\end{table}
\begin{table}[!ht]
\centering
\caption{Experimental Results of Baseline Methods and Different Searching Techniques on UCM Dataset}
\label{UCM_SEARCH}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
METHOD & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
R-BOW~\cite{lu2017exploring} & 0.4107 & 0.2249 & 0.1452 & 0.1095 & 0.1098 & 0.3439 & 0.3071 \\
L-FV~\cite{lu2017exploring} & 0.5897 & 0.4668 & 0.4080 & 0.3683 & 0.2698 & 0.5595 & 1.8438 \\
CSMLF~\cite{wang2019semantic} & 0.3874 & 0.2145 & 0.1253 & 0.0915 & 0.0954 & 0.3599 & 0.3703 \\
CSMLF-FT~\cite{wang2019semantic} & 0.3671 & 0.1485 & 0.0763 & 0.0505 & 0.0944 & 0.2986 & 0.1351 \\
SVM-DBOW~\cite{hoxha2021novel} & 0.7635 & 0.6664 & 0.5869 & 0.5195 & 0.3654 & 0.6801 & 2.7142 \\
SVM-DCONC~\cite{hoxha2021novel} & 0.7653 & 0.6947 & 0.6417 & 0.5942 & 0.3702 & 0.6877 & 2.9228 \\
TrTr-CMR~\cite{wu2024trtr} & 0.8156 & 0.7091 & 0.6220 & 0.5469 & 0.3978 & 0.7442 & 2.4742 \\
TextGCN~\cite{das2024textgcn} & 0.8461 & 0.7844 & 0.7386 & 0.6930 & 0.4868 & 0.8071 & 3.4077 \\
FE-LWS [proposed] &~\textbf{0.8647} &~\textbf{0.8126} &~\textbf{0.7744} &~\textbf{0.7291} &~\textbf{0.4929} &~\textbf{0.8216} &~\textbf{3.6943} \\
\hline
\end{tabular}%
}
\end{table}
\begin{table}[!ht]
\centering
\caption{Experimental Results of Baseline Methods and Different Searching Techniques on RSICD Dataset}
\label{RSICD_SEARCH}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
METHOD & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & METEOR & ROUGE-L & CIDEr \\
\hline
R-BOW~\cite{lu2017exploring} & 0.4401 & 0.2383 & 0.1514 & 0.1041 & 0.1684 & 0.3605 & 0.4667 \\
L-FV~\cite{lu2017exploring} & 0.4342 & 0.2453 & 0.1634 & 0.1175 & 0.1712 & 0.3818 & 0.6531 \\
CSMLF~\cite{wang2019semantic} & 0.5759 & 0.3859 & 0.2832 & 0.2217 & 0.2128 & 0.4455 & 0.5297 \\
CSMLF-FT~\cite{wang2019semantic} & 0.5106 & 0.2911 & 0.1903 & 0.1352 & 0.1693 & 0.3789 & 0.3388 \\
SVM-DBOW~\cite{hoxha2021novel} & 0.6112 & 0.4277 & 0.3153 & 0.2411 & 0.2303 & 0.4588 & 0.6825 \\
SVM-DCONC~\cite{hoxha2021novel} & 0.5999 & 0.4347 & 0.3355 & 0.2689 & 0.2299 & 0.4577 & 0.6854 \\
TrTr-CMR~\cite{wu2024trtr} & 0.6201 & 0.3937 & 0.2671 & 0.1932 & 0.2399 & 0.4895 & 0.7518 \\
TextGCN~\cite{das2024textgcn} & 0.6513 & 0.4819 & 0.3747 & 0.3085 & 0.2752 & 0.4804 & 0.8266 \\
FE-LWS [proposed]&~\textbf{0.6647} &~\textbf{0.5071} &~\textbf{0.4068} &~\textbf{0.3317} &~\textbf{0.2965} &~\textbf{0.4979} &~\textbf{0.8779} \\
\hline
\end{tabular}%
}
\end{table}
\subsection{Numerical Evaluation}
\Cref{SYDNEY_SEARCH,UCM_SEARCH,RSICD_SEARCH} presents a comparative performance analysis of various baseline methods. The first approach is an encoder-decoder model with handcrafted sentence representations~\cite{lu2017exploring}. Specifically, R-BOW employs an RNN decoder with a bag-of-words sentence representation, while L-FV utilizes an LSTM decoder with a Fisher vector-based sentence representation. Another approach is the Collective Semantic Metric Learning Framework (CSMLF)\cite{wang2019semantic}, along with its fine-tuned variant, CSMLF-FT, designed for multisentence captioning. The subsequent methods integrate support vector machines (SVM) as classifiers \cite{hoxha2021novel}, specifically SVM-DBOW and SVM-DCONC, where DBOW and DCONC represent sentence representations using bag-of-words and concatenation techniques, respectively. The TrTr-CMR model~\cite{wu2024trtr} represents a dual transformer RSIC model that uses cross-modal reasoning. It integrates a Swin Transformer encoder, which employs a shifted window partitioning mechanism to extract multiscale visual features. Additionally, a Transformer-based language model (TLM) is utilized, which incorporates both self-attention and cross-attention mechanisms within its decoder structure. The TextGCN model~\cite{das2024textgcn} adopts an encoder-decoder architecture, aligning the decoder embeddings with the encoded output of TextGCN. During training, this layer remains frozen, preserving the distinct features of each word representation. For caption generation, the model employs the CBS technique. Our approach, Fusion-based Encoder and Local Weighted-based Stacking (FE-LWS) of decoders, combines a fusion-based encoder with locally weighted stacking of decoders. This method improves performance by effectively integrating the encoding and decoding strategies. The results show that our model outperforms the state-of-the-art transformer-based model (TrTr-CMR~\cite{wu2024trtr}) and other baseline models based on LSTM by a significant margin.

\begin{figure*}[!ht]
  \centering
  \subfloat[\label{example1}]{\includegraphics[width=0.25\textwidth,height=125px]{Example1.png}}
  \subfloat[\label{example2}]{\includegraphics[width=0.25\textwidth,height=125px]{Example2.png}}
  \subfloat[\label{example3}]{\includegraphics[width=0.25\textwidth,height=125px]{Example3.png}}
  
  \subfloat[\label{example4}]{\includegraphics[width=0.25\textwidth,height=125px]{Example4.png}}
  \subfloat[\label{example5}]{\includegraphics[width=0.25\textwidth,height=125px]{Example5.png}}
  \subfloat[\label{example6}]{\includegraphics[width=0.25\textwidth,height=125px]{Example6.png}}

  \subfloat[\label{example7}]{\includegraphics[width=0.25\textwidth,height=125px]{Example7.png}}
  \subfloat[\label{example8}]{\includegraphics[width=0.25\textwidth,height=125px]{Example8.png}}
  \subfloat[\label{example9}]{\includegraphics[width=0.25\textwidth,height=125px]{Example9.png}}
   \caption{Examples of RS Image Captioning by Different Methods}
   \label{fig_visual}
\end{figure*}

\subsection{Subjective Evaluation}
\begin{table}[!ht]
\centering
\caption{Subjective Evaluation of Different CNNs on Three Datasets (in \%)}
\label{subjective}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{SYDNEY} & \multicolumn{3}{c|}{UCM} & \multicolumn{3}{c|}{RSICD} \\ 
\cline{2-10}
& R & W & U & R & W & U & R & W & U \\
\hline
ConvNext & 89.66 & 1.72 & 8.62 & 89.04 & 4.29 & 6.67 & 82.25 & 5.31 & 12.44 \\
ResNet & 87.93 & 3.45 & 8.62 & 88.09 & 5.24 & 6.67 & 81.43 & 5.03 & 13.54 \\
FE-LWS & 93.10 & 3.45 & 3.45 & 91.43 & 3.33 & 5.24 & 83.26 & 6.77 & 9.97 \\
\hline
\end{tabular}%
}
\end{table}

Unlike classification, multiple captions can describe the same image with similar effectiveness. Hence, relying solely on the test captions (five for all three data sets) to evaluate the predicted captions is not justified \cite{lu2017exploring,das2024unveiling}. For this reason, we perform a subjective evaluation. In this process, we assign a human annotator with expertise in this field to assess the quality of the captions generated. This evaluation provides insights into caption quality from the perspective of human psychology.  In our work, we use three labels to assess the quality of the generated captions, as discussed below.  
\begin{itemize}
    \item \textbf{Related:} The generated caption accurately describes the input image.  
    \item \textbf{Partially Related:} The generated caption describes the input image but contains major errors or flaws.  
    \item \textbf{Unrelated:} The generated caption is completely incorrect and does not describe the input image in any meaningful way.  
\end{itemize}
\Cref{subjective} presents the subjective evaluation results, comparing our model (FE-LWS) with conventional encoder-decoder models, where ConvNext and ResNet are used as encoders, respectively. The results indicate that our model outperforms other models. 

\subsection{Visual Interpretation of Different Models}
We present examples from the test set of three datasets in~\Cref{fig_visual}, comparing our model (FE-LWS) with the gold caption (Gold) and conventional encoder-decoder models (ConvNext and ResNet). In~\Cref{example1}, ConvNext failed to detect the main object~\emph{runway}, while ResNet failed to detect~\emph{river}, but our model successfully detects both. In~\Cref{example2}, ConvNext failed to detect the main object~\emph{church}, and ResNet completely misclassified the image, introducing~\emph{river}, which is not present, while our model successfully generates a meaningful caption. In~\Cref{example3}, the other models misclassified the image as~\emph{school}, but our model successfully generates a meaningful caption. In~\Cref{example4}, the other models misclassified the image as~\emph{railway station}, but our model correctly predicts it. In~\Cref{example5}, all models successfully detect the main object~\emph{meadow}, but our model also detects~\emph{houses}. In~\Cref{example6}, ConvNext incorrectly detects the image as~\emph{baseball field}, and ResNet as~\emph{pond}, but our model correctly detects it in the caption. In~\Cref{example7}, ConvNext failed to detect the main object~\emph{industrial area}, while ResNet failed to detect~\emph{river}, but our model successfully detects both. In~\Cref{example8}, ResNet misclassified the image as~\emph{industrial area}, but ConvNext and our model correctly describe it. In~\Cref{example9}, ConvNext failed to detect the main object~\emph{center building}, and ResNet misclassified it as~\emph{viaduct}, but our model successfully describes it. Visual examples clearly demonstrate that fusing CNNs and efficiently utilizing a stack of decoders significantly enhances caption quality compared to using a single CNN and a single decoder.
% \subsection{Error Analysis}
% \label{error analysis}
% During the visual interpretation of different models, we identified several common issues in the generated captions. The most prevalent problem is misclassification~\cite{lu2017exploring}, where one type of object is incorrectly identified as another due to high similarity between them. Examples include~\emph{farmland} being confused with~\emph{bareland},~\emph{storage tanks} with~\emph{buildings}, and~\emph{river} with~\emph{pond}. Another issue involves the misidentification of patterns in objects. This occurs when a specific object frequently appears in a particular class of images in the training dataset. For instance, a commonly occurring object in the training set may be absent in the test data but still gets incorrectly identified in the caption. A notable example is~\emph{swimming pool}, which often appears in images of the~\emph{sparse residential} class in the RSICD dataset. In some test images, despite the absence of a swimming pool, it is mistakenly detected. A similar scenario arises when a specific object frequently appears in the training dataset. In this case, a similar-looking object may be present in the test data, leading to incorrect identification. For example, in images of~\emph{forest} from the RSICD dataset, a~\emph{road} is sometimes misclassified as a~\emph{river}. Additionally, there is a problem where objects are correctly identified, but their attributes (such as count, color,~\emph{etc.}) are not accurately described. For example, inaccuracies is noticed regarding the color of a~\emph{building's roof}, the color of a~\emph{swimming pool}, the number of~\emph{football fields} in a~\emph{playground},~\emph{etc.} Another significant issue is the word correlation problem~\cite{das2024unveiling}, where two words frequently co-occur in the training data. In some test images, even if one object is absent, the generated caption still includes both objects. A common example involves~\emph{building} and~\emph{trees}.
\section{Conclusion}
\label{sec:conclusion}
Most state-of-the-art methods in RSIC face a notable limitation, as they place insufficient emphasis on the encoding process. Although significant advances have been made in improving the decoding phase, the optimization of the encoder remains an underexplored area. This lack of attention to the encoder can lead to suboptimal feature extraction from complex remote sensing images. Our proposed approach aims to address this imbalance by focusing on optimizing the performance of image encoders, ensuring more accurate and detailed representations of the input data, which ultimately improves the overall captioning performance. We illustrate that fusing features from two distinct Convolutional Neural Networks (CNNs) provides a more robust and informative feature representation than relying on a singular source. Furthermore, the implementation of an efficient stacked architecture of two GRU decoders enhances the model's ability to comprehend the current input sequence, thereby improving the generation of subsequent words compared to a traditional single-decoder setup. In addition, the incorporation of a comparison-based beam search method addresses several shortcomings associated with conventional search techniques, facilitating a more effective exploration of potential caption outputs. As a result, our approach demonstrates markedly superior performance against various baseline methods, showcasing the effectiveness of our encoding optimization and innovative search strategies in advancing the state of RSIC. This comprehensive focus on both the encoding and decoding phases ultimately improves the overall accuracy and robustness of the model in generating meaningful image captions.
\section*{Limitations}  
Although our approach achieves strong performance, it has certain limitations. First, the effectiveness of the beam search method depends on the quality and diversity of the image archives used for reference. Second, due to dataset constraints, the model may struggle to accurately generate specific object attributes, such as the number of occurrences of an object in the image. Additionally, reliance on manually curated dataset limits scalability, as adapting the model to new domains requires additional effort and retraining. Similarly, images that do not belong to the training classes may result in suboptimal captions. In the future, our goal is to address these challenges and improve the adaptability of the model.

\bibliographystyle{elsarticle-num-names} 
\bibliography{Mybibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num-names.tex'.
