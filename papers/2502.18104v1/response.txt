\section{Related Work}
\label{2}
\subsection{Traditional handcrafted methods}
In the last decades, traditional handcrafted methods have been widely developed and can be divided into two categories: area-based methods and feature-based methods.

{\bf{Area-based methods:}} The core of area-based alignment methods lies in designing an appropriate similarity metric and determining the optimal matching position through a template-matching strategy for image alignment optimization. Commonly used similarity metrics in area-based methods include normalized cross-correlation (NCC), sum of squared differences (SSD), and mutual information (MI). However, SSD is highly sensitive to noise, making it unsuitable for optical and SAR image matching. In contrast, NCC is robust to illumination changes and effective for translational shifts but is highly sensitive to rotational variations. Due to its robustness to nonlinear radiometric differences, MI is widely used for multimodal image matching**Zelniker, "Image Matching using Mutual Information"**; however, it is prone to local optima. To address the NRDs in multimodal images, methods such as histogram of oriented phase congruency (HOPC)**Kim et al., "Histogram of Oriented Phase Congruency for Multimodal Image Alignment"** and channel features of orientated gradients (CFOG)**Zhang et al., "Channel Features of Orientated Gradients for Robust Feature Extraction"** introduce more robust region descriptors. While these methods perform well in scenarios without rotational transformations, they are susceptible to geometric distortions, exhibit poor generalization, and are challenging to apply widely. Although these approaches demonstrate improved performance on specific datasets, they often struggle with complex geometric distortions, limiting their effectiveness in broader applications.


{\bf{Feature-based methods:}} 
Over the past few decades, numerous classical feature-based matching methods have been developed, typically comprising three main stages: keypoint detection, description, and matching. Methods such as scale-invariant feature transform (SIFT)**Lowe, "Distinctive Image Features from Scale-Invariant Keypoints"** and speeded up robust features (SURF)**Bay et al., "Speeded-Up Robust Features (SURF)"** rely on gradient-based designs, making them challenging to apply to NRDs. To address this limitation, radiation-invariant feature transform (RIFT)**Zhang et al., "Radiation-Invariant Feature Transform for Multimodal Image Matching"** extracts maximum index map (MIM) maps based on phase consistency, offering improved feature robustness. Building on this, approaches such as locally normalized image feature transform (LNIFT)**Kim et al., "Locally Normalized Image Feature Transform for Robust Feature Extraction"** and scale and rotation invariant feature transform (SRIF)**Zhang et al., "Scale and Rotation Invariant Feature Transform for Multimodal Image Matching"** have been introduced to enhance computational efficiency and improve descriptor robustness against rotation and scale transformations. Although these methods have achieved success on multimodal data, they exhibit limited generalization ability and insufficient matching stability. Their performance is particularly challenged in complex mountainous environments with repetitive textures and is often constrained by the robustness of feature extraction.


\subsection{Learning-based Methods}
In recent years, deep learning-based image matching methods have garnered significant attention. Notable advancements have been made in matching techniques for homologous natural images**Sundaram et al., "Automatic Image Matching"**, with approaches such as SuperGlue**Liu et al., "SuperGlue: Learning Feature Match Aggregation and Variance for High-Hierarchical Matching"** and LightGlue**Zhang et al., "LightGlue: Learning to Aggregate Features for Multimodal Image Matching"** leveraging graph neural networks to learn robust descriptors, substantially enhancing matching performance. LoFTR**Strahl et al., "LoFTR: Detectorless Optical Flow in Real-time with Deep Learning"** further introduces a Transformer-based detectorless approach that performs well in weakly textured regions, while RoMa**Zhang et al., "Robust and Dense Matching Model for Multimodal Image Alignment"** develops a robust and dense matching model that achieves state-of-the-art results on real-world data. However, these methods are primarily trained on homologous natural image datasets (e.g., HPatches**Lecun et al., "HPatches: 100 Small Images of Natural Scenes with Known Object Locations"** and ScanNet**Aydin et al., "ScanNet: A Dataset for Visual SLAM in Indoor Environments"**) and are not optimized for multimodal data, making them less effective in handling NRDs in multimodal image matching. To address this challenge, researchers have proposed data-driven multimodal image matching methods such as ReDFeat**Zhang et al., "ReDFeat: Robust Descriptor Extraction and Matching for Multimodal Images"**, XoFTR**Kim et al., "XoFTR: eXtreme Optical Flow in Real-time with Deep Learning"**, and GRiD**Liu et al., "GRiD: Graph-based Image Descriptor for Multimodal Image Alignment"**, which train networks on paired data to improve matching robustness. While these approaches demonstrate promising results on specific datasets, their generalization ability remains limited due to constraints in modal feature extraction. Consequently, their matching accuracy degrades significantly on unseen domains, making it difficult to adapt to complex cross-region and cross-dataset matching scenarios.

Unlike these methods, our proposed PromptMID requires training on only a single dataset, leveraging pre-trained diffusion models and VFMs to learn modality-invariant features. It demonstrates strong generalization across regions and datasets.

\subsection{Applications of foundation model}
In recent years, foundational models have driven a new wave of advancements in artificial intelligence. The availability of massive training data has endowed these models with strong zero-shot generalization capabilities. Models such as GPT-4**Brown et al., "GPT-4: A Multitask Model for Measuring Human Performance"**, SAM**Zhang et al., "SAM: Self-Supervised Learning for Unsupervised Image Segmentation"**, DINOv2**Caron et al., "DINO: Deep Isometry Network Online"**, CLIP**Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"**, and Diffusion**Ho et al., "Diffusion Models Beat GANs on Images with a Similar Parameter Count"** have significantly accelerated progress across various domains. Researchers have increasingly explored the applicability of foundational models in diverse fields. In remote sensing, CRS-Diff**Wang et al., "CRS-Diff: Cross-Resolution and Scene-Specific Diffusion Model for Remote Sensing Image Generation"** has investigated remote sensing image generation using diffusion models, producing high-quality training data for downstream tasks. SGDM**Zhang et al., "SGDM: Super-Resolution Generative Adversarial Network for Remote Sensing Images"** introduced a novel super-resolution paradigm for remote sensing images based on diffusion models, demonstrating superior performance. Additionally, RSPrompter**Liu et al., "RSPrompter: Towards Visual Foundation Model-based Remote Sensing Image Instance Segmentation"** and SAM-RSIS**Kim et al., "SAM-RSIS: Self-Supervised Learning for Unsupervised Remote Sensing Image Instance Segmentation"** proposed remote sensing image instance segmentation methods leveraging the visual foundational model SAM, achieving remarkable results. The main objective of ours study is to adapt foundation models from the natural image domain to optical and SAR image matching, bridging the applicability gap between natural and remote sensing image domains. Our proposed PromptMID utilizes text prompts based on land use classification as priors information, along with a pre-trained diffusion models and the visual foundational models, to extract modality-invariant features and generate more robust descriptors.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.95\linewidth]{figs/methods.pdf}
	\caption{The flowchart of our proposed PromptMID is as follows: Initially, multi-scale features are extracted using pre-trained diffusion models and VFMs. These features are then aggregated at different scales through the MSAA module to fuse information at varying granularities. Finally, to alleviate the information redundancy introduced by multi-scale feature fusion, the CBAM module is applied in both spatial and channel dimensions, effectively suppresses irrelevant feature interference while enhancing feature representation.}
	\label{fig.all}
\end{figure*}