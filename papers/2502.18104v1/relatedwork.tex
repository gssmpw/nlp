\section{Related Work}
\label{2}
\subsection{Traditional handcrafted methods}
In the last decades, traditional handcrafted methods have been widely developed and can be divided into two categories: area-based methods and feature-based methods.

{\bf{Area-based methods:}} The core of area-based alignment methods lies in designing an appropriate similarity metric and determining the optimal matching position through a template-matching strategy for image alignment optimization. Commonly used similarity metrics in area-based methods include normalized cross-correlation (NCC), sum of squared differences (SSD), and mutual information (MI). However, SSD is highly sensitive to noise, making it unsuitable for optical and SAR image matching. In contrast, NCC is robust to illumination changes and effective for translational shifts but is highly sensitive to rotational variations. Due to its robustness to nonlinear radiometric differences, MI is widely used for multimodal image matching~\cite{OFVERSTEDT2022196}; however, it is prone to local optima. To address the NRDs in multimodal images, methods such as histogram of oriented phase congruency (HOPC)~\cite{ye2017robust} and channel features of orientated gradients (CFOG)~\cite{ye2019fast} introduce more robust region descriptors. While these methods perform well in scenarios without rotational transformations, they are susceptible to geometric distortions, exhibit poor generalization, and are challenging to apply widely. Although these approaches demonstrate improved performance on specific datasets, they often struggle with complex geometric distortions, limiting their effectiveness in broader applications.


{\bf{Feature-based methods:}} 
Over the past few decades, numerous classical feature-based matching methods have been developed, typically comprising three main stages: keypoint detection, description, and matching. Methods such as scale-invariant feature transform (SIFT)~\cite{lowe2004distinctive} and speeded up robust features (SURF)~\cite{bay2008speeded} rely on gradient-based designs, making them challenging to apply to NRDs. To address this limitation, radiation-invariant feature transform (RIFT)~\cite{8935498} extracts maximum index map (MIM) maps based on phase consistency, offering improved feature robustness. Building on this, approaches such as locally normalized image feature transform (LNIFT)~\cite{li2022lnift} and scale and rotation invariant feature transform (SRIF)~\cite{li2023multimodal} have been introduced to enhance computational efficiency and improve descriptor robustness against rotation and scale transformations. Although these methods have achieved success on multimodal data, they exhibit limited generalization ability and insufficient matching stability. Their performance is particularly challenged in complex mountainous environments with repetitive textures and is often constrained by the robustness of feature extraction.


\subsection{Learning-based Methods}
In recent years, deep learning-based image matching methods have garnered significant attention. Notable advancements have been made in matching techniques for homologous natural images~\cite{mishchuk2017working,tian2020hynet,tian2019sosnet,liu2019gift}, with approaches such as SuperGlue~\cite{2019SuperGlue} and LightGlue~\cite{10377620} leveraging graph neural networks to learn robust descriptors, substantially enhancing matching performance. LoFTR~\cite{sun2021loftr} further introduces a Transformer-based detectorless approach that performs well in weakly textured regions, while RoMa~\cite{edstedt2024roma} develops a robust and dense matching model that achieves state-of-the-art results on real-world data. However, these methods are primarily trained on homologous natural image datasets (e.g., HPatches~\cite{8099893} and ScanNet~\cite{8099744}) and are not optimized for multimodal data, making them less effective in handling NRDs in multimodal image matching. To address this challenge, researchers have proposed data-driven multimodal image matching methods such as ReDFeat~\cite{9999700}, XoFTR~\cite{tuzcuouglu2024xoftr}, and GRiD~\cite{10715536}, which train networks on paired data to improve matching robustness. While these approaches demonstrate promising results on specific datasets, their generalization ability remains limited due to constraints in modal feature extraction. Consequently, their matching accuracy degrades significantly on unseen domains, making it difficult to adapt to complex cross-region and cross-dataset matching scenarios.

Unlike these methods, our proposed PromptMID requires training on only a single dataset, leveraging pre-trained diffusion models and VFMs to learn modality-invariant features. It demonstrates strong generalization across regions and datasets.

\subsection{Applications of foundation model}
In recent years, foundational models have driven a new wave of advancements in artificial intelligence. The availability of massive training data has endowed these models with strong zero-shot generalization capabilities. Models such as GPT-4~\cite{achiam2023gpt}, SAM~\cite{kirillov2023segany}, DINOv2~\cite{oquab2023dinov2}, CLIP~\cite{radford2021learning}, and Diffusion~\cite{SD} have significantly accelerated progress across various domains. Researchers have increasingly explored the applicability of foundational models in diverse fields. In remote sensing, CRS-Diff~\cite{10663449} has investigated remote sensing image generation using diffusion models, producing high-quality training data for downstream tasks. SGDM~\cite{WANG2025125} introduced a novel super-resolution paradigm for remote sensing images based on diffusion models, demonstrating superior performance. Additionally, RSPrompter~\cite{10409216} and SAM-RSIS~\cite{10680168} proposed remote sensing image instance segmentation methods leveraging the visual foundational model SAM, achieving remarkable results. The main objective of ours study is to adapt foundation models from the natural image domain to optical and SAR image matching, bridging the applicability gap between natural and remote sensing image domains. Our proposed PromptMID utilizes text prompts based on land use classification as priors information, along with a pre-trained diffusion models and the visual foundational models, to extract modality-invariant features and generate more robust descriptors.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.95\linewidth]{figs/methods.pdf}
	\caption{The flowchart of our proposed PromptMID is as follows: Initially, multi-scale features are extracted using pre-trained diffusion models and VFMs. These features are then aggregated at different scales through the MSAA module to fuse information at varying granularities. Finally, to alleviate the information redundancy introduced by multi-scale feature fusion, the CBAM module is applied in both spatial and channel dimensions, effectively suppresses irrelevant feature interference while enhancing feature representation.}
	\label{fig.all}
\end{figure*}