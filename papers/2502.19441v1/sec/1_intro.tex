\section{Introduction}
\label{sec:Introduction}
%Creating high-fidelity clothed human models holds significant applications in virtual reality, telepresence, and movie production. Traditional methods involve either complex capture systems or tedious manual work from 3D artists, making them time-consuming and expensive, thus limiting scalability for novice users. Recently, there has been a growing focus on automatically reconstructing clothed human models from single RGB images or monocular videos.

%Mesh-based methods ~\cite{kolotouros2019learning, kocabas2020vibe, sun2021monocular, feng2021collaborative} are initially introduced to recover human body shapes by regressing on parametric models such as \textit{SMPL}~\cite{SMPL:2015}, \textit{SMPL-X} ~\cite{pavlakos2019expressive}, these methods could generate instant for using but lack of detail. To reconstruct the surface of cloth or hair, which is hard to present with mesh, the implicit method has a surprising performance. Implicit methods based on occupancy fields ~\cite{saito2019pifu, saito2020pifuhd}, signed distance fields (SDF)~\cite{xiu2022icon}, and neural radiance fields (NeRFs)~\cite{mildenhall2021nerf, peng2021neural, chen2021animatable, weng2022humannerf, li2022tava, li2023posevocab, isik2023humanrf} have been developed to learn the clothed human body using volume rendering techniques. However, due to the large consumption of rendering, these methods could not achieve train in a quick time or interface for a high quality.

%% 以下两段可以合并简化，引出animatable avatar reconstruction这个问题即可

%% 然后讲基于3DGS的方法，辩证讨论优缺点，引出我们要解决的两大关键问题：1 如何高效利用SMPL提供的local geometry information作为pose guidance； 2 如何提高GS本身对高清人体图像的生成质量
% 修改前
% With its commendable performance in both speed and quality, 3D Gaussian Splatting\cite{kerbl20233d} has emerged as the new standard for reconstructing avatar models. In the realm of multi-view video reconstruction, Animatable Gaussian\cite{li2023animatable} and D3GA\cite{zielonka2023drivable} leverage implicit models for geometry and deformation priors, enabling reconstruction across diverse motion scenarios. Meanwhile, concerning monocular inputs, HUGS\cite{kocabas2023hugs}, Gart\cite{lei2023gart}, and Gauhuman\cite{hu2023gauhuman} employ learnable skinning weights to guide Gaussian deformation. Their utilization of a low spherical harmonic degree facilitates rapid training and inference speeds. Nonetheless, these monocular methods often underutilize the SMPL model as a prior, resulting in a deficiency in pose-dependent deformation and detail fidelity.

% 修改后
Creating high-fidelity clothed human models holds significant applications in virtual reality, telepresence, and movie production. Implicit methods based on occupancy fields ~\cite{saito2019pifu, saito2020pifuhd}, signed distance fields (SDF)~\cite{xiu2022icon}, and neural radiance fields (NeRFs)~\cite{mildenhall2021nerf, peng2021neural, chen2021animatable, weng2022humannerf, li2022tava, li2023posevocab, isik2023humanrf} have been developed to learn the clothed human body using volume rendering techniques. However, due to the large consumption of the volumetric learning process, these methods could not balance well the training efficiency and visual quality.

Recent advances in 3D Gaussian Splatting~\cite{kerbl20233d} based methods have shown promising performances and less time consumption in this area, covering both single-view~\cite{li2024gaussianbody, kocabas2023hugs, lei2023gart, hu2023gauhuman} and multi-view~\cite{li2023animatable, zielonka2023drivable} avatar reconstruction settings. Beyond all these works, two main ongoing challenges still need to be resolved. The first one is efficiently training the Gaussian Splatting models across different poses and the second is improving the visual quality for dynamic details. 

For the dynamic pose learning problem, there are several existing works~\cite{ye2023animatable,qian20243dgsavatar} that have already adopted the pose-dependent deformation from SMPL~\cite{SMPL:2015} prior. Unfortunately, they are all limited by the global pose vectors and neural skinning weights learning and hence lack the local geometry correspondence for clothed human details. To address this limitation, our insight is to enable the point-level SMPL deformation prior to training 3D Gaussian Splatting avatar with local pose guidance. Specifically, we take inspiration from \textit{SCARF}~\cite{feng2022capturing} by deforming the avatar with SMPL-KNN strategy and \textit{Deformable-GS}~\cite{yang2023deformable3dgs} by incorporating position and deformation codes into a Multilayer Perceptron (MLP). This approach enables the learning of locally non-rigid deformations, which are subsequently transformed using rigid deformation to align the adjusted model with the observed space. In this way, our model can efficiently learn the local geometric prior information from SMPL deformation and maintain correspondence consistency for cloth details across all the frames.

For the visual quality problem, we observe that the current 3D Gaussian Splatting model is struggling to render the non-rigidly deformed human avatars in high fidelity. We decouple the visual quality issue into two parts and propose two technical solutions correspondingly. The first issue is the unstable rendering results caused by complex non-rigid deformation between different pose spaces and the canonical space. To overcome that, we optimize a physically-based prior for the Gaussians in the observation space to mitigate the risk of overfitting Gaussian parameters. We transform the local rigid loss \cite{luiten2023dynamic} to regularize over-rotation across the canonical and observation space. The second issue is that the original Gaussian Splatting sampling strategy could not well handle the rich texture details like complicated clothes. We tackle this problem by introducing a split-with-scale strategy to further enhance the geometry expressiveness of the Gaussian Splatting model and resolve the visual artifacts in texture-rich areas.

%The inherent limitations of 3D-Gaussian Splatting in accurately reconstructing human motion scenes underscore the complexity of managing model deformations between pose space and canonical space. In light of this challenge, we embark on a concerted effort to address this issue by incorporating the physics-based prior inspired by \cite{luiten2023dynamic}. Additionally, we undertake an extensive exploration of scaling methodologies, aimed at refining Gaussian splats and thereby enhancing their effectiveness in capturing intricate motion dynamics.  
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{pic_draft/teaser.png}
    \caption{AniGaussian takes monocular RGB video as input, reconstructing an animatable avatar model in around 30 minutes and rendering with 45 FPS on a single NVIDIA RTX 4090 GPU. The resulting human model can present subtitle texture and generate non-rigid deformation of clothes details. Performance in novel views and animation with unseen poses. Furthermore, we gain the highest reconstruction quality in current works which is evident in our picture metrics.}
    \label{fig:first}
\end{figure*}
Based on the above analysis of the current limitations for Gaussian based animatable avatar models, we combine our insights and propose another novel framework called \textit{AniGaussian}. Our framework extends the 3D-GS representation to animatable avatar reconstruction, with an emphasis on enabling local pose-dependent guidance and visual quality refinement. Given a monocular human avatar video as input, \textit{AniGaussian} can efficiently train an animatable Gaussian model for the full-body avatar in 30 minutes as shown in Figure~\ref{fig:first}. In the experiment, we evaluate our proposed framework on monocular videos of animatable avatars on the task of novel view synthesis and novel pose synthesis. By comparing it with other works, our method achieves superior reconstruction quality in rendering details and geometry recovery, while requiring much less training time and real-time rendering speed. We conduct ablation studies to validate the effectiveness of each component in our method.


%In this work, we present the AniGaussian, which extended the 3D-GS representation to animatable avatar reconstruction by utilizing an articulated human model as guidance. Specifically, we decomposed the pose-guided deformation into non-rigid and rigid deformation. The non-rigid deformation presents the cloth's dynamic details and deforms the 3D Gaussians into motion space with the control of the corresponding SMPL. To avoid the uncertain SMPL parameter influencing the model train, we jointly optimize the SMPL parameters. Secondly, we optimize a physically-based prior for the Gaussians in the observation space to mitigate the risk of overfitting Gaussian parameters. We transform the local rigid loss \cite{luiten2023dynamic} to regularize over-rotation across the canonical and observation space and propose a split-with-scale strategy to enhance point cloud density and avoid artifacts. In the experiment, we evaluate our proposed framework on monocular videos of animatable avatars on the task of novel view synthesis and novel pose synthesis. By comparing it with other works, our method achieves superior reconstruction quality in rendering details and geometry recovery, while requiring much less training time and almost real-time rendering speed. We also conduct ablation studies to validate the effectiveness of each component in our method.

% In summary, Our work has the following contributions:
% \item We propose a method to reconstruct dynamic human motion scene with pose-guided deformation based on 3D-GS
In summary, our contributions are as follows:
\begin{itemize}
    \item A pose-guided deformation framework that includes both non-rigid and rigid deformation to extend the 3D Gausssian Splatting to animatable avatar reconstruction.
    \item We advanced Gaussian Splatting with the rigid-based prior restricting the canonical model and Split with scale strategy to achieve more accuracy and robustness.
    \item Our approach has yielded the best results on the PeopleSnapshot dataset, demonstrating superior rendering quality compared to other methods.
\end{itemize}
%


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{pic_draft/pipeline.png}
    \caption{\textbf{Overview of AniGaussian.}
        At first, we initialize the point cloud using SMPL vertices. In the train processing, we find the nearest vertex as the deformation-guider of the Gaussian. We input the position of Gaussian after position encoding and the nearest vertex as the deformation code to the MLP to gain the non-rigid deformation. Then with the transformation of the SMPL vertex, the Gaussians are transformed to the pose space. In the tour of transformation, we use the rigid-based prior $L_{rot}$ and $L_{iso}$ to rule the deformation. After Gaussian splatting, we could refine the SMPL parameters and the canonical model.
    }
    \label{fig:pipeline}
\end{figure*}
