\section{Conclusion}
In this paper, we present AniGaussian, a novel method for reconstructing dynamic animatable avatar models from monocular videos using the 3D Gaussians Splatting representation. By incorporating pose guidance deformation with rigid and non-rigid deformation, we extend the 3D-GS representation to animatable avatar reconstruction. Then, we incorporate pose refinement to ensure clear textures. To mitigate between the observation space and the canonical space, we employ a rigid-based prior to regularizing the canonical space Gaussians and a split-with-scale strategy to enhance both the quality and robustness of the reconstruction. Our method is capable of synthesizing an animatable avatar that can be controlled by a novel motion sequences. Experiments on the PeopleSnapShot and ZJU-MoCap datasets, our method achieves superior quality metrics with the benchmark, demonstrating competitive performance.

\noindent \textbf{Future Work}
While our method is capable of producing high-fidelity animatable avatar and partially restoring clothing wrinkles and expressions, we have identified certain challenges. During training, Gaussians hard to assimilate texture colors into their internal representations, leading to difficulties in accurately learning surface details and textures with monocular input. Moreover, due to the inherent sensitivity of spherical harmonic functions to lighting, diffuse color and lighting would be baked into the spherical harmonic functions. In our future endeavors, we aim to decouple lighting and color expressions by leveraging different dimensions of spherical harmonic functions for separate learning. This approach will facilitate the generation of lighting decoupled and animatable avatar within a single learning stage.
