%
%
\begin{table}
\centering
    \begin{tabular}{lccc}
    \hline
             & \textbf{PSNR↑} & \textbf{SSIM↑} & \textbf{LPIPS↓} \\ 
             \hline
    3DGS-Avatar\hfill &30.61& 0.9703 &29.58    \\
    GauHuman\hfill &31.34& 0.9647&30.51    \\
    GART\hfill & \textbf{32.22} &\textbf{0.9773} & \textbf{29.21}    \\
    \textbf{ours} & 30.00 & 0.9597 & 35.06   \\
            \hline  
    \end{tabular}
    \caption{\textbf{Metrics of novel view synthesis on ZJU-MoCap.} }
    \label{table:zju_metric}
\end{table}
%
%
\begin{table}
\centering
    \begin{tabular}{lccc}
    \hline
             & \textbf{PSNR↑} & \textbf{SSIM↑} & \textbf{LPIPS↓} \\ 
        \hline		
    Full-model &\textbf{34.18}&\textbf{0.9769}&\textbf{0.015}    \\
    w/o SMPL refine &32.31 &0.9724 &0.027   \\
    w/o $\mathcal{L}_{iso}$ &33.86 &0.9753 &0.021\\
    w/o $\mathcal{L}_{rot}$ &33.13 &0.9767 & 0.022    \\
    w/o split with scale &33.46 &0.9683 &0.020     \\
        \hline
    \end{tabular}
    \caption{\textbf{Metrics of ablation study on PeopleSnapshot.} We could gain the best picture rendering quality which has more details and is more evident in the quality of our full model.}
    \label{table:ablition study}
\end{table}
%
\begin{figure}
   \centering
    \includegraphics[width=1\linewidth]{pic_sup/zju_dance.png}
    \captionof{figure}{\textbf{Novel poses of ZJU-MoCap~\cite{peng2021neural}.}}
  \label{fig:ad_np}
\end{figure}
%
%
\section{Experiment}
\label{sec:exp}
In this section, we evaluate our method on monocular training videos and compare the novel view synthesized result with the other benchmark. We also conduct ablation studies to verify the effectiveness of each component in our method.

% \subsection{Setup}
% %
% \noindent\textbf{Dataset and Benchmark.}
% We evaluate our method and compare with other methods on the PeopleSnapshot Dataset~\cite{alldieck2018video} and ZJU-MoCap Dataset~\cite{peng2023implicit}. Compare to the Nerf based model: InstantAvatar~\cite{jiang2023instantavatar} and Anim-NeRF~\cite{chen2021animatable}, and the Gaussian based model: 3DGS-Avatar~\cite{qian20243dgsavatar}, Gart~\cite{lei2023gart} and GauHuman~\cite{hu2023gauhuman}. The metrics settings are same as Anim-NeRF~\cite{chen2021animatable}.

\noindent \textbf{PeopleSnapshot Dataset}~\cite{alldieck2018video}  contains eight sequences of dynamic humans wearing different outfits.
The actors rotate in front of a fixed camera, maintaining an A-pose during the recording in an environment filled with stable and uniform light.
The dataset provides the shape and the pose of the human model estimate from the images.
We train the model with the frames split from Anim-nerf~\cite{chen2021animatable} and use the poses after refinement.

\noindent \textbf{ZJU-MoCap Dataset}~\cite{peng2023implicit} contains several multi-view video captures around the motion humans. This dataset has various motions and complex cloth deform. We pick 6 sequences (377, 386, 387, 392, 393, 394) from the ZJU-MoCap dataset and follow the training/test split of 3DGS-Avatar~\cite{qian20243dgsavatar}. We train the model with 100 frames captured from a stable camera view and test results with other views to measure the metrics of novel view synthesis.

\noindent \textbf{Benchmark.} On the PeopleSnapshot dataset, we compare the metrics of novel view synthesis with the original 3D-GS~\cite{kerbl20233d}, the Nerf based model: InstantAvatar~\cite{jiang2023instantavatar} and Anim-NeRF~\cite{chen2021animatable}, and the Gaussian based model: 3DGS-Avatar~\cite{qian20243dgsavatar}, Gart~\cite{lei2023gart} and GauHuman~\cite{hu2023gauhuman}. To evaluate the quality of the novel view synthesis on ZJU-MoCap, we compare it with the Gaussian-based model~\cite{qian20243dgsavatar,lei2023gart,hu2023gauhuman} on the qualitative and quantitative results.

\noindent\textbf{Performance Metrics.}
We evaluate the novel view synthesis quality with frame size in $540\times 540$ with the quantitative metrics including Peak Signal-to-Noise Ratio(PSNR)~\cite{sara2019image}, Structural SIMilarity index (SSIM)~\cite{wang2004image}, and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018unreasonable}. These metrics serve as indicators of the reconstruction quality. PSNR primarily gauges picture quality, where higher PSNR values signify demonstration illustrates the clarity of the images. SSIM measures the similarity between the ground truth and the reconstructed result, serving as an indicator of accuracy of reconstruct result. LPIPS primarily evaluates the perception of perceptual image distortion. Lower LPIPS values imply a more realistic generated images, reflecting the fidelity of the reconstruction.
%

\noindent\textbf{Implementation Details.}
AniGaussian is implemented in PyTorch and optimized with the Adam~\cite{2014Adam}. 
We optimize the full model in 23k steps following the learning rate setting of official implementation, while the learning rate of non-rigid deformation MLP and SMPL parameters is $2e^{-3}$. We set the hyper-parameters as $\lambda_{rot}=1$, $\lambda_{iso}=1$ and follow the original setting from 3D-GS.

 \noindent \textbf{Non-rigid deformation network. }
 We describe the network architecture of our non-rigid deformation network in Fig.\ref{fig:netArch}. We use an MLP with 8 hidden layers of 256 dimensions which takes $x_c \in R^3 $ and deformation codes $V^{p}_{nn(x)}$ with positional encoding . Our MLP $F$ initially processes the input through eight fully connected layers that employ ReLU activations, and outputs a 256-dimensional feature vector. This vector is subsequently passed through three additional fully connected layers to separately output the offsets of position, rotation, and scaling for different pose.It should be noted that similar to NeRF~\cite{mildenhall2021nerf}, we concatenate the feature vector and the input in the fourth layer.

\subsection{Results of Novel View Synthesis}
\noindent \textbf{Quantitative analysis.}
As shown in Table~\ref{table:compare}, our method consistently outperforms other approaches in almost all metrics in the PeopleSnapshot~\cite{alldieck2018video} dataset, highlighting its superior performance in capturing detailed reconstructions. This is because our method presents the texture with high-order spherical harmonic functions and gains more accurate features by learning from the local geometry information from pose-guided deformation. Also the split-with-scale strategy benefits our model to capture more details on some challenging cases. This indicates the our model's superior performance in reconstructing intricate cloth textures and human body details. The NeRF-based methods \cite{chen2021animatable, jiang2023instantavatar} imposed by the volume rendering hardly to achieving higher quality. original 3D-GS~\cite{kerbl20233d} struggles with dynamic scenes due to violations of multi-view consistency, resulting in partial and blurred reconstructions. The test set contains variations in both viewpoints and poses, Gauhuman~\cite{hu2023gauhuman} as a method primarily focused on generating novel view synthesis, exhibits significant distortions. The deficiency in details within Gart~\cite{lei2023gart} significantly exacerbates the sense of unreality by reflecting on the higher LPIPS. We have better performance with 3DGS-Avatar~\cite{qian20243dgsavatar} in a similar training time.

In Table \ref{table:zju_metric}, AniGaussian gains comparable performance with other competitive approaches. Because our method are intentionally designed to capture the high fidelity image features and adopts a high-dimensional spherical harmonic function, which is pretty sensitive to local lighting change. However, ZJU-MoCap~\cite{peng2021neural} dataset unfortunately were not captured in a stable lighting environment. After transforming the the light directions to canonical space, the unstable lighting change would affect the training stability and thus produce some mismatching artifacts with the groundtruth. Even though, we show the rendering results of our method still demonstrate much more clear/sharp details. We argue that the quantitative metrics might not be able to reflect the model's visual quality, the compasion figure could be found in Supplementary material.

\noindent \textbf{Qualitative analysis.} 
In the comparative analysis presented in Figure~\ref{fig:ps_compare}, our method demonstrates superior performance in faithfully restoring intricate clothing details and capturing high-frequency information on the body. Unlike Gart~\cite{lei2023gart} and GauHuman~\cite{hu2023gauhuman}, which struggle to accurately reproduce texture mappings, resulting in blurry outputs almost without representation of clothing wrinkles and details, our approach excels in preserving these fine-grained features. Additionally, while 3DGS-Avatar~\cite{qian20243dgsavatar} manages to generate enough texture details, it falls short in providing high-frequency information to enhance the realism of the avatar. 

As shown in Figure \ref{fig:arps}, realistic rendering results from different views, featuring individuals with diverse clothing and hairstyles. These results underscore the applicability and robustness of our method in real-world scenarios. Additionally, it features clear and comprehensive textures, showcasing the details of both the clothing and the human body. Our method could support different types of clothes in the free-view render while maintaining a strong consistency in viewpoints.
% More Qualitative results can be found in the Supplementary material.

\subsection{Results of Novel Pose Synthesis}
\noindent \textbf{Qualitative analysis.} 
We provide the rendered result of the novel pose with the trained model in Figure~\ref{fig:novalpose}. Our reconstructed animatble avatar could perform in out-of-distribute poses that prove high-fidelity texture, such as the button on the shirt and the highlight on the belt. Artifacts in joint transformations are scarcely observed, and the reconstruction process can effectively accommodate loose-fitting garments, such as loose shorts. Benefiting from the non-rigid deformation, the complex cloth details could be preserve.

As shown in the Fig.\ref{fig:arzju}, our method demonstrates superior performance in faithfully restoring intricate clothing details and capturing high-frequency information on the body. What's more, we are also capable of generating realistic new pose effects on this dataset in Fig.\ref{fig:ad_np}.

In the comparative analysis presented in Figure~\ref{fig:arzju}, our method demonstrates superior performance in faithfully restoring intricate clothing details and capturing high-frequency information on the body. Unlike Gart~\cite{lei2023gart} and GauHuman~\cite{hu2023gauhuman}, which struggle to accurately reproduce texture mappings, resulting in blurry outputs almost without representation of clothing wrinkles and details, our approach excels in preserving these fine-grained features. Additionally, while 3DGS-Avatar~\cite{qian20243dgsavatar} manages to generate enough texture details, it falls short in providing additional high-frequency information to further enhance the realism of the avatar.
%
\begin{figure}[htb]
   \centering
    \includegraphics[width=1\linewidth]{pic_sup/NetworkArchitect.png}
    \captionof{figure}{\textbf{Architecture of Non-rigid Deformation Network.}}
  \label{fig:netArch}
\end{figure}
%
\begin{figure}[htb]
   \centering
    \includegraphics[width=0.8\linewidth]{pic_sup/time.png}
    \captionof{figure}{\textbf{Initialization with SMPL and Efficient Reconstruct} Benefiting from the initial SMPL vertices, we can reconstruct the model in a short time. After reconstructing the basic model, our method focuses on non-rigid transformations and the details of the model.}
  \label{fig:time}
\end{figure}
%
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{pic_draft/loss.png}
    \caption{\textbf{Effect of rigid-based prior.} The distortions occurring under changes in viewpoint or motion. The $\mathcal{L}_{rot}$ restricts the Gaussian motion between different observation spaces, and the $\mathcal{L}_{iso}$ reduces the unexpected floating artifact. }
    \label{fig:rigid-loss}
\end{figure}
%
\begin{figure}
        \centering
        \includegraphics[width=1\linewidth]{pic_draft/SWS.png}
        \caption{\textbf{Compare to the original 3D-GS split strategy} The original approach enhances geometric details by reducing the gradient threshold. The visualization of the point cloud demonstrates that our method generates denser and smoother results while effectively preventing the incorporation of texture information into the geometric domain.}
        \label{fig:split_grad}
\end{figure}
%
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{pic_draft/posemodel.png}
    \caption{\textbf{Effect of pose refinement.} We compared the novel view synthesis results. The results without pose refinement would lead to floating artifacts and inexact texture. Training with the joint optimization would decrease the artifact on the sleeve and the blur texture on the collar.}
    \label{fig:refine}
\end{figure}
\begin{figure}
   \centering
    \includegraphics[width=0.8\linewidth]{pic_sup/RotateViewDir.png}
    \captionof{figure}{\textbf{Effect of Rotating the view direction.} We compared the novel view synthesis results. The results without rotate the view direct would lead to wrong color expression.In test viewpoints, the spherical harmonic functions appear devoid of color, underscoring the importance of rotating view director with the rotation and aligning with the camera coordinate.}
  \label{fig:abrot}
\end{figure}
\subsection{Ablation Study}
We study the effect of various components of our method on the PeopleSnapshot dataset and the ZJU-MoCap dataset, including SMPL parameter refinement, the Rigid-based prior, and split with the scale. The average metrics over 4 sequences are reported in Tab.\ref{table:ablition study}. All proposed techniques are required to reach optimal performance.
% \noindent \textbf{Effective of Pose-guided deformation.}
% Our method extends the 3d Gaussian Splatting into the animatable avatar reconstruction, as shown in Table\ref{table:compare}, we modify the original 3D-GS\cite{kerbl20233d} with rotate the scene space with the global orient of SMPL parameter to suit the frozen camera, after such operate, gaussian still could not reconstruct the scene without the pose guided-deformation, which do not recover the 3d model, just recover a texture in the picture space, even 

\noindent \textbf{Effective of Rigid-based prior.}
To evaluate the impact of the Rigid-based prior, we conducted experiments by training models with part-specific Rigid-based priors. As shown in Table~\ref{table:ablition study}, our full model would gain the best performance among the recent approaches. The absence of prior challenges for the model in maintaining consistency across various viewpoints and movements in Fig.~\ref{fig:rigid-loss}. This is attributed to overfitting during training, the Gaussians would only fit part of the views in the canonical space, resulting in an inadequate representation of Gaussians during changes in pose and viewpoint to the observation space.

\noindent \textbf{Effective of rotate view direction}
As shown in the Fig.~\ref{fig:abrot}, it becomes apparent that without rotating the implementation direction, correct colors and results cannot be attained when rendering from alternative viewpoints. In test viewpoints, the spherical harmonic functions appear devoid of color, underscoring the importance of rotating view director with the rotation and aligning with the camera coordinate. This approach facilitates the accurate learning of colors and expressions by the Gaussian model.
% \begin{figure}
%         \centering
%         \includegraphics[width=1\linewidth]{pic_draft/sps.png}
%         \caption{\textbf{Effect of splitting with the scale.} The results without this refinement might generate artifacts in novel pose synthesis.}
%         \label{fig:split}
% \end{figure}
%

\noindent \textbf{Effective of Split-with-scale.}
% We compare it with the original 3D-GS split strategy with the threshold of gradient, constraining the gradient threshold still could not generate as good (dense\&smooth) results as our split-with-scale strategy in Figure.~\ref{fig:split_grad}. As monocular datasets lack sufficient variability in viewpoints, fewer and larger Gaussians are utilized in regions with minimal changes in motion and limited texture diversity. Consequently, the model tends to incorporate texture information into the geometric domain. More geometric details would be preserved in a more dense point cloud which is reflected in the metrics presented in Table~\ref{table:ablition study}. By employing additional splitting, the point cloud on the surface becomes enriched to present more geometry details. 
We compare our method with the original 3D-GS split strategy, which relies on a gradient threshold. Even when constraining the gradient threshold, the original strategy does not generate results as dense and smooth as our strategy, as shown in Figure~\ref{fig:split_grad}. In monocular datasets, which lack sufficient variability in viewpoints, fewer and larger Gaussians are used in regions with minimal motion changes and limited texture diversity. This causes the model to incorporate texture information into the geometric domain. A denser point cloud preserves more geometric details, as reflected in the metrics in Table~\ref{table:ablition study}. By employing additional splitting, our approach enriches the point cloud on the surface, capturing more geometric details. Despite our  strategy increasing the training parameters, our model still achieves comparable fast training and rendering speed.
% while improving the rendering quality by a large margin.

\noindent \textbf{Effective of Joint optimization of SMPL parameters.}
We utilize the SMPL model as the guide for rigid and non-rigid deformations, but inaccurate SMPL estimation could lead to inconsistencies of body parts in spatial positions under different viewpoints, resulting in blurred textures and floating artifacts, as shown in the metrics decrease in the Tab.\ref{table:ablition study} and worse visual in the Figure.~\ref{fig:refine}, such as the floating artifact on the sleeve and the blur texture on the collar.

\noindent \textbf{Effective of initial with SMPL vertex.} We initialize the canonical 3D Gaussians with vertex(N = 6890) of SMPL mesh in canonical pose. This enables us to generate suitable models in a relatively short time, allowing more time for subsequent texture mapping and pose optimization. We are able to produce models of decent quality in approximately 11 minutes and generate high-quality models within 20 to 30 minutes as shown in the Fig.\ref{fig:time}.