\section{Method}
In this section, we first describe our framework pipeline for 3D-GS based animatable avatar reconstruction. Then we elaborate on pose-guided local deformation to train the dynamic Gaussian. Finally, we introduce the advanced gaussian splatting to regularize the 3D Gaussians across the canonical and observation spaces. 
\subsection{Overview}
\label{subsec:overview}
As shown in Figure. \ref{fig:pipeline}, we initialize the point cloud with the SMPL vertex in the star-pose and define the template 3D Gaussians in the canonical space as $G(\bar{x},\bar{r},\bar{s},\bar{\alpha},\bar{f})$. We decompose the animatable avatar modeling problem into the canonical space and the pose space. To learn the template 3D Gaussians, we employ pose-guidance deformation fields to transform them into the pose space and render the scene using differentiable rendering. In order to reduce the artifacts of 3D Gaussian with invalid rotations or unexpected movements in canonical space, we constrain the 3D Gaussians with the rigid-based Prior. Finally, to handle the rich texture details like complicated clothes, we further refine the naive gaussian splatting approach with a split-with-scale strategy to enhance the expressiveness of our model and resolve the visual artifacts in texture-rich areas.
% \subsection{Preliminary}
% \label{subsec:pre}
% 3D-GS~\cite{kerbl20233d} is a trending 3D scene representation method originally designed for multi-view image modelling. The static model comprises a list of gaussian splats with a point cloud at the center of each. Gaussians are defined by a covariance matrix $\Sigma$ and a center point $X$, representing the mean value of the Gaussian:
% \begin{equation}
%     G(X) = e^{-\frac{1}{2}X^T\Sigma^{-1}X}.
%     \label{eq:Gaussian}
% \end{equation}
% For differentiable optimization, the covariance matrix $\Sigma$ can be decomposed into two matrixes $S$ and $R$:
% \begin{equation}
%     \Sigma = RSS^TR^T.
%     \label{eq:RandS}
% \end{equation}
% To simplify the expression, the matrices $R$ and $S$ are preserved as rotation parameter $r$ and scaling parameter $s$. To render the scene, the regressed Gaussians can be projected into camera space with the covariance matrix $\Sigma'$:
% \begin{equation}
%     \Sigma' = JW\Sigma W^TJ^T.
%     \label{eq:jacobi}
% \end{equation}
% Here, $J$ is the Jacobian of the affine approximation of the projective transformation, and $W$ is the world-to-camera matrix. After projecting the 3D Gaussians to 2D, the pixel color $C$ can be computed based on the overlapping Gaussians, sorted by depth:

% \begin{equation}
%     C = \sum_{\substack{i\in \mathbf{N}}}c_i(\alpha_i\prod_{j=1}^{i-1}1-\alpha_j).
%     \label{eq:color}
% \end{equation}

% The color $c$ is computed using the spherical harmonics coefficient $f$ and the canonicalized view direction $\bar{d}$. Collectively, the 3D Gaussians are denoted as $G(x,r,s,\alpha,f)$.
%


% due to the special presence of gaussian that occupies part of the space, it would generate artifacts with deformation. So we use the split-with-scale strategy to solve such a problem. 
% 不涉及方法，方法框架的介绍
\subsection{Pose-guided Deformation}
\label{subsec:pgd}
We utilize the parametric body model SMPL \cite{SMPL:2015} as pose guidance. 
The articulated SMPL model $M(\beta,\theta)$ is defined with pose parameters $\theta \in R^{69}$ and shape parameters $\beta \in R^{10}$ that outputs a 3D human body mesh with vertices $V \in R^{6890\times3}$, and vertex transform $T(\beta,\theta)$ from the T-pose.
To gain the transformation from the SMPL model, we find the nearest vertex of canonical 3D Gaussians, register it as the agent, on the template model $V^{c} = M(\beta,\theta_c)$ that in the star-pose as shown in Figure.\ref{fig:pipeline}. 

In order to fully utilize the local correspondence information provided by SMPL prior, we take inspirations from SelfRecon~\cite{jiang2022selfrecon} and SCARF~\cite{feng2022capturing} and decompose the pose-guided deformation fields into non-rigid transformation for the cloth movement and rigid transformation for the body movement. 

\noindent \textbf{Non-rigid transformation. }
%
First we implement a MLP $F$ to learn the non-rigid deformation of the cloth details, 
\begin{equation}
    F(x, V^{p}_{nn(x)}) = \delta x, \delta r, \delta s,
    \label{eq:non_rigid mlp}
\end{equation}
this MLP takes as input the position of the 3D Gaussian $x$ and the position of posed SMPL model vertex $V^{p}_{nn(x)}$, and output the $\delta x$, $\delta r$, $\delta s$ as gaussian parameters. 
The $V^{p}_{nn(x)}$ is the vertex on the posed SMPL model that contains the same index of the template model $V^{c}$.
And our canonical model after non-rigid deformation is $G(\bar{x}',\bar{r}',\bar{s}',\bar{\alpha},\bar{f})$.

\noindent \textbf{Rigid transformation. }
% 
The rigid transformation from canonical space to observation space of 3D Gaussians is defined by the transformation of SMPL vertex as: 
\begin{equation}
    D(\bar{x},\beta,\theta_t,\theta_c) = \sum_{\substack{v^{c}_i \in nn(\bar{x})}}\frac{\mathbf{w}_i}{\mathbf{w}}T_i(\beta,\theta_{c})^{-1}T_i(\beta,\theta_{t}),
    \label{eq:Gaussian_rigid_deform}
\end{equation}
where $v^{c}_i$ is one of the $k$ nearest vertex of template model and $T_i$ is the transformation of the vertex. $\theta_c$ is the predefined canonical pose parameter, so we omit it in Eq.~\ref{eq:Gaussiandeform}. $\theta_t$ is the pose of current frame. We set $k=3$ to maintain the 3D Gaussians transformation stability across multiple joints, and further weigh the transformations with:
\begin{equation}
    \begin{split}
    \mathbf{w}_i(x) = exp(-&\frac{||x-vi||_2||w_{nn(x)}-w_i||_2}{2\sigma^2}),\\
    \mathbf{w}(x) &= \sum_{\substack{v^{c}_i \in nn(x)}}\mathbf{w}_i(x),
    \end{split}
    \label{eq:Gaussian_rigid_weight}
\end{equation}
where $\sigma = 0.1$, $w_{nn(x)}$ is the skinning weight of the $k$ nearest vertex, $w_i$ is the blend weight of nearest vertex.

For each frame, we transform the position $\bar{x}'$ and rotation $\bar{r}'$ of the canonical Gaussians after non-rigid deformation to the observation space, with the guided of pose parameter $\theta_t$ of current frame and the global shape parameter $\beta$:
\begin{equation}
\begin{split}
    x &= \mathcal{D}(\bar{x},\theta_t,\beta)\bar{x}',\\
    r &= \mathcal{D}(\bar{x},\theta_t,\beta)\bar{r}',
\end{split}
    \label{eq:Gaussiandeform}
\end{equation}
where $\mathcal{D}$ is the deformation function defined in Eq.\ref{eq:Gaussian_rigid_deform}. 

In this way, we obtain the deformed Gaussians in the observation space. After differentiable rendering and image loss calculation, the gradients will be passed through the inverse of the deformation field $\mathcal{D}$ and optimized parameters of the Gaussians in canonical space.

Additionally, because the monocular input is hard to provide sufficient view information, it is noteworthy to mention that we opt to transform the direction of light into the canonical space to ensure the view consistent. The light direction transformation can be formulated as:
\begin{equation}
    \bar{d} = (T_{c2w} r)^T d,
    \label{eq:lighttransf}
\end{equation}
where $d$ is the light direction in the world coordinate system, $r$ is the rotation in camera coordinate system, and $T_{c2w}$ is the coordinate transformation matrix from the camera to the world coordinate system. At last we evaluate the spherical harmonics coefficients with the canonical light direction $\bar{d}$.

\noindent \textbf{Joint optimization of SMPL parameters. }
%
Since our 3D-GS training pipeline is built upon the local pose-dependent deformation from SMPL prior, it is crucial to obtain accurate SMPL shapes to guarantee the pose guidance effectiveness. Unfortunately, the regression of SMPL parameters from images would be affected by a lot of reasons like false landmark detection or uncertain camera pose estimation. 

Therefore, we propose a joint optimization idea for refining the SMPL parameters including the pose and shape during training our entire pipeline. Specifically, the SMPL shape parameter $\beta$ and pose parameters $\theta$ would be optimized regarding the image loss and get updated to match the exact body shapes and poses in training frames.
%, there is a risk of generating a s. The inaccurate SMPL parameters may impact the non-rigid deformation to align with the images, leading to blurred textures. To address this issue, we propose an optimization approach for the SMPL parameters. Specifically, we designate the SMPL shape parameter $\beta$ and pose parameters $\theta$ as the optimized parameters and refine them through the optimization process. As part of pose-guided deformation, they could be guided by defined losses.

\subsection{Advance Gaussian Splatting}
\label{subsec:PBP}
%
Since we define the Gaussians in the canonical space and deform them to the observation space for differentiable rendering, the optimization process is still an ill-posed problem. Because multiple canonical positions will be mapped to the same observation position, there are inevitably overfitting in the observation space and visual artifacts in the canonical space. To address this problem, we propose an advanced gaussian splatting to enhance the visual performance.

\begin{figure}
        \centering
        \includegraphics[width=1\linewidth]{pic_draft/physical-based.png}
        \caption{\textbf{Visual of the Rigid-based prior.} With the deformation between the canonical space and the observation space, we hope the neighbour Gaussian could have a similar rotation and keep a property distance.}
        \label{fig:physical-based}
\end{figure}

\noindent \textbf{Rigid-based prior. }
In the experiment, we also observed that this optimization approach might easily result in the novel view synthesis showcasing numerous Gaussians in incorrect rotations, consequently generating unexpected glitches.
Thus we follow \cite{luiten2023dynamic} to regularize the movement of 3D Gaussians by their local information. Particularly we employ two regularization losses to maintain the local geometry property of the deformed 3D Gaussians, including local-rotation loss $\mathcal{L}_{rot}$ and a local-isometry loss $\mathcal{L}_{iso}$.
Different from \cite{luiten2023dynamic} that attempts to track the Gaussians frame by frame, we regularize the Gaussian transformation from the canonical space to the observation space. And we do not set the rigid loss because of it would conflict with the non-rigid deformations.

Given the set of Gaussians $j$ with the k-nearest-neighbors of $i$ in canonical space (k=5), the isotropic weighting factor between the nearby Gaussians is calculated as:
\begin{equation}
    w_{i,j} = exp(-\lambda_{w}||x_{j,c}-x_{i,c}||^{2}_{2}),
    \label{eq:local-weight}
\end{equation}
where $||x_{j,c}-x_{i,c}||$ is the distance between the Gasussians $i$ and $j$ in canonical space, set $\lambda_{w} = 2000$ that gives a standard deviation. The rotation loss could enhance convergence to explicitly enforce identical rotations among neighboring Gaussians in both spaces:
\begin{equation}
    \mathcal{L}_{rot} =  \frac{1}{k|{G}|}\sum_{i \in {G}}\sum_{j \in knn_{i;k}}w_{i,j}||q_{j,o}q^{-1}_{j,c}-q_{i,o}q^{-1}_{i,c}||_{2},
    \label{eq:local-rot}
\end{equation}
where $G$ is the whole Gaussian model, $q$ is the normalized Quaternion representation of each Gaussian's rotation, the $q_{o}q^{-1}_{c}$ demonstrates the rotation of the Gaussians from the canonical space to the observation space. The $w_{i,j}$ is the weighting factor as mentioned in Eq.~\ref{eq:local-weight}.

We use an isometric constraint to make two Gaussians in different spaces in a property distance to avoid floating artifacts, which enforces the distances $\Delta x = x_i-x_j$ in different spaces between their neighbors:
\begin{equation}
    \mathcal{L}_{iso}=\frac{1}{k|{G}|}\sum_{i \in {G}}\sum_{j \in knn_{i;k}}w_{i,j}\{||\Delta x_o||_{2}-||\Delta x_c||_{2}\},
    \label{eq:iso-rigidity}
\end{equation}
after adding the above objectives, our objective is :
\begin{equation}
    \mathcal{L}=\mathcal{L}_{L1}+\lambda_{SSIM}\mathcal{L}_{SSIM}+\lambda_{rot}\mathcal{L}_{rot}+\lambda_{iso}\mathcal{L}_{iso}.
    \label{eq:losses}
\end{equation}
where $\mathcal{L}_{L1}$ and $\mathcal{L}_{SSIM}$ are the images losses from original 3D-GS~\cite{kerbl20233d}, which regular the model from the image space to optimize the Gaussians and the other models in our method, the $\lambda_{SSIM}$, $\lambda_{rot}$ and $\lambda_{iso}$ are loss weight.
%

\noindent\textbf{Split-with-scale. }
%
After adjusting to utilize monocular video input, the model lacks some of the geometric information obtained from multi-view sources. A portion of the reconstructed point cloud (3D Gaussians) may become excessively sparse, leading to oversized Gaussians and to generate blurring artifacts with novel motions. To address this, we propose a strategy to split large Gaussians using a scale threshold $\epsilon_{scale}$ after the regular split and densify. If a Gaussian has scale $s$ larger than $\epsilon_{scale}$, we decompose it into two identical Gaussians, each with half the size. With such operation, we could gain a more compact Gaussian model. The compact Gaussian would preserve more geometry information to avoid confusion by the texture.

\noindent\textbf{Initial with SMPL vertex. }
%
For the reconstruction of the 3D Gaussian model, point clouds are required as the basis input. The original 3D-GS used COLMAP to initialize multi-view images to generate the basic point clouds. However, for monocular image input, it is not possible to use COLMAP to generate the basic point clouds. But based on prior knowledge of the human body, we can use the vertices of human mesh as the basic point clouds for the reconstruction.
%
\begin{table*}
\centering
\resizebox{\textwidth}{!}{   
    \begin{tabular}{lcccccccccccccc}
        \toprule
                    &&&\multicolumn{3}{c}{male-3-casual} & \multicolumn{3}{c}{male-4-casual} & \multicolumn{3}{c}{female-3-casual} & \multicolumn{3}{c}{female-4-casual}\\
                    &\textbf{time↓}&\textbf{FPS↑}&\textbf{PSNR↑} & \textbf{SSIM↑} & \textbf{LPIPS↓}  & \textbf{PSNR↑} & \textbf{SSIM↑} & \textbf{LPIPS↓} & \textbf{PSNR↑} & \textbf{SSIM↑} & \textbf{LPIPS↓} & \textbf{PSNR↑} & \textbf{SSIM↑} & \textbf{LPIPS↓} \\
        \midrule
             3D-GS\hfill        & 0.5h& 70&  \colorbox{lightgray}{26.60} &\colorbox{lightgray}{0.9393} &\colorbox{lightgray}{0.082} &\colorbox{lightgray}{24.54} & \colorbox{lightgray}{0.9469} &\colorbox{lightgray}{0.088} &\colorbox{lightgray}{24.73} &\colorbox{lightgray}{0.9297} & \colorbox{lightgray}{0.093} &\colorbox{lightgray}{25.74} &\colorbox{lightgray}{0.9364} &\colorbox{lightgray}{0.075} \\
             Anim-NeRF\hfill      &26h& 1 & 29.37 &0.9703 &0.017 &28.37 &0.9605 & 0.027 &28.91 & {0.9743}&  {0.022} &28.90 &0.9678 &0.017 \\
             InstantAvatar\hfill&\underline{0.1h}& 15  & 29.64 &0.9719 &0.019 &28.03 &0.9647 &0.038 &28.27&0.9723& 0.025&29.58 &0.9713 &0.020 \\
             GauHuman*\hfill &\underline{0.1h}& \underline{300} &30.95 &0.9504&0.046&28.28 &0.9523&0.055& {32.52}&0.9627&0.058&30.02 &0.9494&0.044 \\
             GART\hfil     &\underline{0.1h}&90& 30.40 & \colorbox{pink}{0.9769} &0.037 &27.57 & {0.9657} &0.060 &26.26 &0.9656 &0.049 &29.23 & {0.9720} &0.037 \\
        \midrule
             3DGS-Avatar\hfill &0.75h&\underline{50}& {34.28}& 0.9724 & {0.014} & {30.22} &0.9653 & {0.023} &30.57& 0.9581 &0.020 & \colorbox{pink}{33.16}& 0.9678 &0.016  \\
             \textbf{Ours}  &\underline{0.5h}&45& \colorbox{pink}{35.35}& {0.9762}& \colorbox{pink}{0.012} & \colorbox{pink}{33.35} & \colorbox{pink}{0.9765}& \colorbox{pink}{0.018} & \colorbox{pink}{35.01}& \colorbox{pink}{0.9752}& \colorbox{pink}{0.017}& {33.02}& \colorbox{pink}{0.9798}& \colorbox{pink}{0.014} \\
        \bottomrule
    \end{tabular}}
    \caption{\textbf{Quantitative comparison of novel view synthesis on PeopleSnapshot dataset.} 
    Our approach exhibits a significant advantage in metric comparisons, showing substantial improvements in all metrics due to its superior restoration of image details.\colorbox{pink}{NUM}= The Best, \colorbox{lightgray}{NUM} = The Worst. ``*'' denotes the results trained by the official codes.}
    \label{table:compare}
\end{table*}
%
%
%
%
\begin{figure*}
        \centering
        \includegraphics[width=1\linewidth]{pic_draft/ps_compare.png}
        \caption{\textbf{Qualitative comparison of novel view synthesis on PeopleSnapshot dataset.} Compare to other methods, our method effectively restores details on the animatable avatar, including intricate details in the hair and folds in the clothes. These results underscore the applicability and robustness in real-world scenarios. }
        \label{fig:ps_compare}
\end{figure*}
% \begin{figure*}
%     \centering
%     \includegraphics[width=1\linewidth]{pic_draft/pai.png}
%     \caption{\textbf{Visual comparison of different methods about novel view synthesis on ZJU-MoCap\cite{peng2021neural}}. Our method achieves high fidelity results, especially in the texture of the clothes and the wrinkles in the garment. Compared with other methods, we have preserved more high-frequency detail from the pictures.}
%     \label{fig:zjuvisualcomapre}
% \end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{pic_draft/novelpose.png}
    \caption{\textbf{Novel pose synthesis on PeopleSnapshot~\cite{alldieck2018video}}. Our method could drive the reconstruction animatable avatar in novel poses with fewer artifacts and present cloth details and render in 45FPS. 
    }
    \label{fig:novalpose}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{pic_sup/ar_ps.png}
    \caption{\textbf{Results of novel views synthesis on PeopleSnapshot \cite{alldieck2018video} dataset.} Our method effectively gene restores details on the human body, including intricate details in the hair and folds on the clothes. Moreover, the model exhibits strong consistency across different viewpoints. }
    \label{fig:arps}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{pic_sup/adition_result_zju_compare.png}
    \caption{\textbf{Visual comparison of different methods about novel view synthesis on ZJU-MoCap\cite{peng2021neural}}. Our method achieves high fidelity results, especially in the texture of the clothes and the wrinkles in the garment. Compared with other methods, we have preserved more high-frequency detail from the pictures.}
    \label{fig:arzju}
\end{figure*}