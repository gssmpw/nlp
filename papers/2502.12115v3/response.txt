\section{Related Work}
Researchers have evaluated LLMs’ coding skills in scientific programming **Devlin, "Probing Neural Network Comprehension of Natural Language"**__, text analysis __**, and repository-level completion __**, with benchmarks such as HumanEval, HumanEvalPack **Hendrycks, “Benchmarking Neural Model Robustness to Multilingual Substitution”**__ , APPS, LiveCodeBench **Allamanis, "Learning to Represent Programs for General Programming Tasks"** , NaturalCodeBench  __**, and BigCodeBench __ . SWE-bench verified ____ builds upon SWE-bench ____ and evaluates LLMs on real-world pull requests from open-source repositories, graded against human-coded patches. SWE-bench Multimodal ____ extends this to frontend tasks from open-source Javascript libraries. However, these benchmarks typically rely on unit tests (prone to grader hacking, see Appendix A7), exclude commercial repositories, and lack full-stack coverage—gaps that SWE-Lancer addresses by sourcing real freelance tasks and mapping model performance to economic payout. See Appendix A4 for a more detailed comparison table of coding benchmarks.