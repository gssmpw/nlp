\section{Related Work}
Researchers have evaluated LLMs’ coding skills in scientific programming \cite{Tian24}, text analysis \cite{Zhong23, Lam24}, and repository-level completion \cite{Zhang24, Liu23}, with benchmarks such as HumanEval, HumanEvalPack \cite{Muennighoff24}, APPS, LiveCodeBench \cite{Jain24}, NaturalCodeBench \cite{Zhang24}, and BigCodeBench \cite{Zhuo24}. SWE-bench verified \cite{Chowdhury24} builds upon SWE-bench \cite{Jiminez24} and evaluates LLMs on real-world pull requests from open-source repositories, graded against human-coded patches. SWE-bench Multimodal \cite{Yang24} extends this to frontend tasks from open-source Javascript libraries. However, these benchmarks typically rely on unit tests (prone to grader hacking, see Appendix A7), exclude commercial repositories, and lack full-stack coverage—gaps that SWE-Lancer addresses by sourcing real freelance tasks and mapping model performance to economic payout. See Appendix A4 for a more detailed comparison table of coding benchmarks.