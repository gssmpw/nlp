\section{Related Work}
Researchers have evaluated LLMs’ coding skills in scientific programming ____, text analysis ____, and repository-level completion ____, with benchmarks such as HumanEval, HumanEvalPack ____, APPS, LiveCodeBench ____, NaturalCodeBench ____, and BigCodeBench ____. SWE-bench verified ____ builds upon SWE-bench ____ and evaluates LLMs on real-world pull requests from open-source repositories, graded against human-coded patches. SWE-bench Multimodal ____ extends this to frontend tasks from open-source Javascript libraries. However, these benchmarks typically rely on unit tests (prone to grader hacking, see Appendix A7), exclude commercial repositories, and lack full-stack coverage—gaps that SWE-Lancer addresses by sourcing real freelance tasks and mapping model performance to economic payout. See Appendix A4 for a more detailed comparison table of coding benchmarks.