\section{Related Work}
\vspace{-0.05in}
{\bf Interpretability and Reasoning}
Ideas from Mechanistic Interpretability have been applied to models trained on algorithmic data to understand the types of algorithms models may discover. Solutions learned by transformers trained on modular arithmetic have been fully reverse-engineered~\cite{nanda2023} and the dynamics of feedforward networks have been theoretically characterized~\cite{gromov2023grokking, tian2024, he2024learning}. In transformers trained on data generated by iterative algorithms, the model learns a corresponding attention head which applies this iterative scheme~\cite{cabannes2024}. Circuits in transformers exist which can compose atomic subject-object relations into multi-hop relations across entities \cite{wang2024grokked} as well as compose variable relationships and assignments in propositional logic~\cite{Hong2024}.

%Through a different line of experiments it was discovered that data diversity plays a crucial role in determining whether in-context learning emerges from training \cite{raventos2024pretraining, singh2024transient}, as well as whether the model generalizes out-of-distribution \cite{he2024learning}. Finally, it is possible to make a direct map between transformers and RASP programs\cite{weiss2021thinking, zhou2023algorithms}.

{\bf Graph Problems with Language Models}
Graph problems have served as a useful testbed for understanding the capabilities of language models with benchmarks such as GraphQA~\cite{fatemi2024talk}, CLRS-text~\cite{deepmind2024clrstext} and NLGraph~\cite{wang2023can}. Graph problems also have been used to illuminate numerous token representation limitations for solving combinatorial problems with language models~\cite{ying2021, perozzi2024, deepmind2024clrstext, bachmann2024}.

{\bf Path-finding with Language Models} 
Interpreting language models trained on path finding tasks is of great interest as it is a problem that fundamentally requires planning and reasoning~\cite{wang2024}. Models trained on directed acyclic graphs~\cite{khona2024towards} to find any path, not necessarily the shortest, learn an algorithm that is very similar to \ours\ however it relies on node embedding distances instead of edge embedding distances, possibly due to differences in graph representation. There is no connection to the Graph or Line Graph Laplacian and we do not have any constraints on cycles in our setting.
6-layer decoder-only models trained to predict the path between source and target nodes in binary trees learn an iterative mechanism applied per layer whereas our model learns a embedding distance-based mechanism~\cite{brinkmann2024}. Exploring the dynamic between depth and algorithm one can interpret is a very interesting direction for future work.
The SearchFormer line of work shows that encoder-decoder transformers can be trained to generate $A^*$ search traces in order to navigate mazes and often generate smaller search trees than $A^*$ itself~\cite{lehnert2024beyondastar, su2024dualformer}. %Given the results in the current work, it is possible that the SearchFormer models are able to prune the search tree {\it in-weights} using a learned heuristic such as embedding distances.
Finally, there exist many alternate lines of work such as Graph Neural Networks~\cite{wu2021gnn} and Looped Transformers~\cite{deluca2024}.

\vspace{-0.1in}