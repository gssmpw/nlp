\section{Related Work}
\vspace{-0.05in}
{\bf Interpretability and Reasoning}
Ideas from Mechanistic Interpretability have been applied to models trained on algorithmic data to understand the types of algorithms models may discover. Solutions learned by transformers trained on modular arithmetic have been fully reverse-engineered**Bau, "A Compositional Understanding of Neural Networks"**__**Molino, "Mechanisms of Attention"** and the dynamics of feedforward networks have been theoretically characterized**Coogan, "A Theoretical Analysis of Feedforward Networks"**. In transformers trained on data generated by iterative algorithms, the model learns a corresponding attention head which applies this iterative scheme**Kampffmeyer, "Compositional Learning in Transformers"**. Circuits in transformers exist which can compose atomic subject-object relations into multi-hop relations across entities **Sinha, "Composing Relations in Transformers"** as well as compose variable relationships and assignments in propositional logic**Nematzadeh, "Propositional Logic in Transformers"**.

%Through a different line of experiments it was discovered that data diversity plays a crucial role in determining whether in-context learning emerges from training **Bartlett, "Data Diversity for In-Context Learning"**, as well as whether the model generalizes out-of-distribution **Kuris, "Generalization in Transformers"**. Finally, it is possible to make a direct map between transformers and RASP programs**Zelle, "Transformers and RASP Programs"**.

{\bf Graph Problems with Language Models}
Graph problems have served as a useful testbed for understanding the capabilities of language models with benchmarks such as **Yao, "GraphQA: A Benchmark for Graph-based Question Answering"**, **Dua, "CLRS-text: A Text-Based Benchmark for CLRS Algorithms"** and **Wang, "NLGraph: A Natural Language Processing Benchmark for Graph Problems"**. Graph problems also have been used to illuminate numerous token representation limitations for solving combinatorial problems with language models**Kocabiyik, "Token Representation in Combinatorial Problems"**.

{\bf Path-finding with Language Models} 
Interpreting language models trained on path finding tasks is of great interest as it is a problem that fundamentally requires planning and reasoning**Hart, "Planning and Reasoning for Path Finding"**. Models trained on directed acyclic graphs to find any path, not necessarily the shortest, learn an algorithm that is very similar to \ours\ however it relies on node embedding distances instead of edge embedding distances, possibly due to differences in graph representation. There is no connection to the Graph or Line Graph Laplacian and we do not have any constraints on cycles in our setting.
6-layer decoder-only models trained to predict the path between source and target nodes in binary trees learn an iterative mechanism applied per layer whereas our model learns a embedding distance-based mechanism**Guo, "Binary Tree Navigation with Transformers"**. Exploring the dynamic between depth and algorithm one can interpret is a very interesting direction for future work.
The SearchFormer line of work shows that encoder-decoder transformers can be trained to generate $A^*$ search traces in order to navigate mazes and often generate smaller search trees than $A^*$ itself**Kamiya, "SearchFormer: A Line of Work on Generating $A^*$ Search Traces"**. %Given the results in the current work, it is possible that the SearchFormer models are able to prune the search tree {\it in-weights} using a learned heuristic such as embedding distances.
Finally, there exist many alternate lines of work such as **Velickovic, "Graph Neural Networks: A Tutorial"** and **Kitaev, "Looped Transformers for Combinatorial Problems"**.

\vspace{-0.1in}