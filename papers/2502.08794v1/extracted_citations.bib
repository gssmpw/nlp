@article{Hong2024,
  title={How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis},
  author={Hong, Guan Zhe and Dikkala, Nishanth and Luo, Enming and Rashtchian, Cyrus and Wang, Xin and Panigrahy, Rina},
  journal={arXiv preprint arXiv:2411.04105},
  year={2024}
}

@article{bachmann2024,
  title={The Pitfalls of Next-Token Prediction},
  author={Bachmann, Gregor and Nagarajan, Vaishnavh },
  journal={International Conference on Machine Learning},
  year={2024}
}

@article{brinkmann2024,
  title={A Mechanistic Analysis of a Transformer Trained on a Symbolic Mulit-Step Reasoning Task},
  author={Brinkmann, Jannik and Sheshadri, Abhay and Levoso, Victor and Swoboda, Paul and Bartelt, Christian},
  journal={Association for Computational Linguistics},
  year={2024}
}

@article{cabannes2024,
  title={Iteration Head: A Mechanistic Study of Chain-of-Thought},
  author={Cabannes, Viven and Arnbal, Charles and Bouaziz, Wassim and Yang, Alice and Charton, Francois and Kempe, Julia},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{deepmind2024clrstext,
  title={The CLRS-Text Algorithmic Reasoning Language Benchmark},
  author={Larisa Markeeva and Sean McLeish and Borja Ibarz and Wilfried Bounsi
    and Olga Kozlova and Alex Vitvitskyi and Charles Blundell and
    Tom Goldstein and Avi Schwarzschild and Petar Veli\v{c}kovi\'{c}},
  journal={arXiv preprint arXiv:2406.04229},
  year={2024}
}

@article{deluca2024,
  title={Simulation of Graph Algorithms with Looped Transformers},
  author={De Luca, Artur Back and Fountoulakis, Kimon},
  journal={arXiv preprint arXiv:2402.01107},
  year={2024}
}

@inproceedigs{fatemi2024talk,
  title={Talk like a Graph: Encoding Graphs for Large Language Models},
  author={Bahare Fatemi and Jonathan Halcrow and Bryan Perozzi},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@article{gromov2023grokking,
  title={Grokking modular arithmetic},
  author={Gromov, Andrey},
  journal={arXiv preprint arXiv:2301.02679},
  year={2023}
}

@article{he2024learning,
  title={Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks},
  author={He, Tianyu and Doshi, Darshil and Das, Aritra and Gromov, Andrey},
  journal={arXiv preprint arXiv:2406.02550},
  year={2024}
}

@article{khona2024towards,
  title={Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model},
  author={Khona, Mikail and Okawa, Maya and Hula, Jan and Ramesh, Rahul and Nishi, Kento and Dick, Robert and Lubana, Ekdeep Singh and Tanaka, Hidenori},
  journal={arXiv preprint arXiv:2402.07757},
  year={2024}
}

@misc{lehnert2024beyondastar,
      title={Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping}, 
      author={Lucas Lehnert and Sainbayar Sukhbaatar and DiJia Su and Qinqing Zheng and Paul Mcvay and Michael Rabbat and Yuandong Tian},
      year={2024},
      eprint={2402.14083},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{nanda2023,
  title={Progress Meausres for Grokking via Mechanistic Interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={Internation Conference on Learning Representations},
  year={2023}
}

@misc{perozzi2024,
      title={Let Your Graph Do the Talking: Encoding Structured Data for LLMs}, 
      author={Bryan Perozzi and Bahare Fatemi and Dustin Zelle and Anton Tsitsulin and Mehran Kazemi and Rami Al-Rfou and Jonathan Halcrow},
      year={2024},
      eprint={2402.05862},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05862}, 
}

@article{raventos2024pretraining,
  title={Pretraining task diversity and the emergence of non-bayesian in-context learning for regression},
  author={Ravent{\'o}s, Allan and Paul, Mansheej and Chen, Feng and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{singh2024transient,
  title={The transient nature of emergent in-context learning in transformers},
  author={Singh, Aaditya and Chan, Stephanie and Moskovitz, Ted and Grant, Erin and Saxe, Andrew and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{su2024dualformer,
  title={Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces},
  author={Su, Di Jia and Sukhbaatar, Sainbayar and Rabbat, Michael and Tian, Yuandong and Zheng, Qinqing},
  journal={arXiv preprint arXiv:2410.01779},
  year={2024}
}

@article{tian2024,
  title={Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in Neural Nets},
  author={Tian, Yuandong},
  journal={arXiv preprint arXiv:2410.01779},
  year={2024}
}

@article{wang2024,
  title={ALPINE: Unveiling The Planning Capability of Autoregressive Learning in Language Models},
  author={Wang, Siwei and Shen, Yifei and Feng, Shi and Sun, Haoran and Teng, Shang-Hua and Chen, Wei},
  journal={Conference on Neural Information Proessing Systems},
  year={2024}
}

@article{wang2024grokked,
  title={Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization},
  author={Wang, Boshi and Yue, Xiang and Su, Yu and Sun, Huan},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}
}

@article{wu2021gnn,
  title={A Comprehensive Survey on Graph Neural Newtorks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and S. Yu, Philip},
  journal={IEE Transactions on Neural Networks and Learning Systems},
  volume={32},
  pages={4-24},
  year={2021}
}

@inproceedings{ying2021,
 author = {Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28877--28888},
 publisher = {Curran Associates, Inc.},
 title = {Do Transformers Really Perform Badly for Graph Representation?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/f1c1592588411002af340cbaedd6fc33-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{zhou2023algorithms,
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

