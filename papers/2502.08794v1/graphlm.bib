@article{oren2023,
  title={Proving Test Set Contamination in Black Box Language Models},
  author={Oren, Yonatan and Meister, Nicole and Chatterji, Niladri and Ladhak, Faisal and Hashimoto, Tatsunoria},
  journal={arXiv preprint arXiv:2310.17623},
  year={2023}
}

@article{mirzadeh2024,
  title={GSM-Symbolic: Understanding the Limitations of Mathemetical Reasoning in Large Language Models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2410.05229},
  year={2024}
}

@article{kambhampati2024,
  title={Position: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks},
  author={Kambhampati, Subbarao and Valmeekam, Karthik and Guan, Lin and Verma, Mudit and Stechly, Kaya and Bhambri, Siddhant and Saldyt, Lucas and Murthy, Anil},
  journal={arXiv preprint arXiv:2402.01817},
  year={2024}
}
@article{nx,
  title={Exploring network structure, dynamics, and function using NetworkX},
  author={Hagberg, Aric and Schult, Daniel and Swart, Pieter},
  journal={Python in Science Conference},
  year={2008},
  pages={11-15}

}

@article{chung1997,
  title={Spectral Graph Theory},
  author={Chung, Fan},
  journal={American Mathemetical Society},
  year={1997},
}


@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{Steinerberger2020,
  title={A spectral approach to the shortest path problem},
  author={Steinerberger, Stefan},
  journal={Linear Algebra and its Applications},
  year={2020}
}

@article{goyal2024,
  title={Think before you speak: Training Language Models with Pause Tokens},
  author={Goyal, Sachin and Ji, Ziwei and Singh Rawat, Ankit and Krishna Menon, Aditya and Kumar, Sanjiv and Vaishnavh, Nagarajan},
  journal={International Conference on Learning Representations},
  year={2024}
}

@article{darcet2024,
  title={Vision Transformers Need Registers},
  author={Darcet, Timothee and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  journal={International Conference on Learning Representations},
  year={2024}
}

@article{tian2024,
  title={Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in Neural Nets},
  author={Tian, Yuandong},
  journal={arXiv preprint arXiv:2410.01779},
  year={2024}
}

@article{su2024dualformer,
  title={Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces},
  author={Su, Di Jia and Sukhbaatar, Sainbayar and Rabbat, Michael and Tian, Yuandong and Zheng, Qinqing},
  journal={arXiv preprint arXiv:2410.01779},
  year={2024}
}

@article{wang2024,
  title={ALPINE: Unveiling The Planning Capability of Autoregressive Learning in Language Models},
  author={Wang, Siwei and Shen, Yifei and Feng, Shi and Sun, Haoran and Teng, Shang-Hua and Chen, Wei},
  journal={Conference on Neural Information Proessing Systems},
  year={2024}
}

@article{bachmann2024,
  title={The Pitfalls of Next-Token Prediction},
  author={Bachmann, Gregor and Nagarajan, Vaishnavh },
  journal={International Conference on Machine Learning},
  year={2024}
}

@article{cherkassky1996,
  title={Shortest paths algorithms: theory and experimental evaluation},
  author={Cherkassky, Boris and Goldberg, Andrew and Radzik, Tomasz},
  journal={Mathematical Programming},
  year={1996}
}

@article{harary1960,
  title={Some properties of line digraphs},
  author={Harary, Frank and Norman, Robert},
  journal={Rendiconti del Circolo Matematico di Palermo},
  year={1960}
}

@article{bellman,
  title={On a routing problem},
  author={Bellman, Richard},
  journal={Quarterly of Applied Mathematics},
  pages={87-90},
  year={1960}
}

@inproceedigs{fatemi2024talk,
  title={Talk like a Graph: Encoding Graphs for Large Language Models},
  author={Bahare Fatemi and Jonathan Halcrow and Bryan Perozzi},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@inproceedings{ying2021,
 author = {Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28877--28888},
 publisher = {Curran Associates, Inc.},
 title = {Do Transformers Really Perform Badly for Graph Representation?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/f1c1592588411002af340cbaedd6fc33-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{deepmind2024clrstext,
  title={The CLRS-Text Algorithmic Reasoning Language Benchmark},
  author={Larisa Markeeva and Sean McLeish and Borja Ibarz and Wilfried Bounsi
    and Olga Kozlova and Alex Vitvitskyi and Charles Blundell and
    Tom Goldstein and Avi Schwarzschild and Petar Veli\v{c}kovi\'{c}},
  journal={arXiv preprint arXiv:2406.04229},
  year={2024}
}

@article{deluca2024,
  title={Simulation of Graph Algorithms with Looped Transformers},
  author={De Luca, Artur Back and Fountoulakis, Kimon},
  journal={arXiv preprint arXiv:2402.01107},
  year={2024}
}

@phdthesis{edelman2024,
    author = {Edelman, Benjamin},
    title = {Combinatorial Tasks as Model Systems of Deep Learning},
    school = {Harvard University},
    year = {2024}
}

@misc{perozzi2024,
      title={Let Your Graph Do the Talking: Encoding Structured Data for LLMs}, 
      author={Bryan Perozzi and Bahare Fatemi and Dustin Zelle and Anton Tsitsulin and Mehran Kazemi and Rami Al-Rfou and Jonathan Halcrow},
      year={2024},
      eprint={2402.05862},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05862}, 
}

@inproceedings{
wang2023can,
title={Can Language Models Solve Graph Problems in Natural Language?},
author={Heng Wang and Shangbin Feng and Tianxing He and Zhaoxuan Tan and Xiaochuang Han and Yulia Tsvetkov},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=UDqHhbqYJV}
}

@article{dijkstras,
  title={A note on two problems in connexion with graphs},
  author={Dijkstra, Edsger W.},
  journal={Numerische Mathematik},
  pages={269-271},
  year={1959}
}

@article{cabannes2024,
  title={Iteration Head: A Mechanistic Study of Chain-of-Thought},
  author={Cabannes, Viven and Arnbal, Charles and Bouaziz, Wassim and Yang, Alice and Charton, Francois and Kempe, Julia},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@misc{lehnert2024beyondastar,
      title={Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping}, 
      author={Lucas Lehnert and Sainbayar Sukhbaatar and DiJia Su and Qinqing Zheng and Paul Mcvay and Michael Rabbat and Yuandong Tian},
      year={2024},
      eprint={2402.14083},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{RoPe,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv e-prints, art},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@article{mckay83,
  title={Applications of a Technique for Labelled Enumeration},
  author={Mckay, B. D.},
  journal={Congressus Numerantium},
  pages={207-221},
  year={1983}
}

@online{mckaygraphs,
  author = {Mckay, B. D.},
  title = {Simple Graphs},
  url = {https://users.cecs.anu.edu.au/~bdm/data/graphs.html},
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{achiam2023,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}
}

@article{zhou2023algorithms,
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

@article{claude2024,
  title={The claude 3 model family: Opus, sonnet, haiku.},
  author={Anthropic},
  year={2024}
}

@article{Hong2024,
  title={How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis},
  author={Hong, Guan Zhe and Dikkala, Nishanth and Luo, Enming and Rashtchian, Cyrus and Wang, Xin and Panigrahy, Rina},
  journal={arXiv preprint arXiv:2411.04105},
  year={2024}
}


@article{nanda2023,
  title={Progress Meausres for Grokking via Mechanistic Interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={Internation Conference on Learning Representations},
  year={2023}
}

@article{wang2024grokked,
  title={Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization},
  author={Wang, Boshi and Yue, Xiang and Su, Yu and Sun, Huan},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{yang2024twohop,
  title={Do Large Language Models Latently Perform Multi-Hop Reasoning},
  author={Yang, Sohee and Griboskaya, Elena and Kassner, Nora and Geva, Mor and Riedel, Sebastion},
  journal={Association for Computational Linguistics},
  year={2024}
}

@article{dubey2024,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{singh2024transient,
  title={The transient nature of emergent in-context learning in transformers},
  author={Singh, Aaditya and Chan, Stephanie and Moskovitz, Ted and Grant, Erin and Saxe, Andrew and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{brinkmann2024,
  title={A Mechanistic Analysis of a Transformer Trained on a Symbolic Mulit-Step Reasoning Task},
  author={Brinkmann, Jannik and Sheshadri, Abhay and Levoso, Victor and Swoboda, Paul and Bartelt, Christian},
  journal={Association for Computational Linguistics},
  year={2024}
}

@article{nichani2024transformers,
  title={How transformers learn causal structure with gradient descent},
  author={Nichani, Eshaan and Damian, Alex and Lee, Jason D},
  journal={arXiv preprint arXiv:2402.14735},
  year={2024}
}

@article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
    }

@article{raventos2024pretraining,
  title={Pretraining task diversity and the emergence of non-bayesian in-context learning for regression},
  author={Ravent{\'o}s, Allan and Paul, Mansheej and Chen, Feng and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{liu2022omnigrok,
  title={Omnigrok: Grokking beyond algorithmic data},
  author={Liu, Ziming and Michaud, Eric J and Tegmark, Max},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{wu2021gnn,
  title={A Comprehensive Survey on Graph Neural Newtorks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and S. Yu, Philip},
  journal={IEE Transactions on Neural Networks and Learning Systems},
  volume={32},
  pages={4-24},
  year={2021}
}

@article{okawa2024compositional,
  title={Compositional abilities emerge multiplicatively: Exploring diffusion models on a synthetic task},
  author={Okawa, Maya and Lubana, Ekdeep S and Dick, Robert and Tanaka, Hidenori},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{khona2024towards,
  title={Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model},
  author={Khona, Mikail and Okawa, Maya and Hula, Jan and Ramesh, Rahul and Nishi, Kento and Dick, Robert and Lubana, Ekdeep Singh and Tanaka, Hidenori},
  journal={arXiv preprint arXiv:2402.07757},
  year={2024}
}

@article{liu2022towards,
  title={Towards understanding grokking: An effective theory of representation learning},
  author={Liu, Ziming and Kitouni, Ouail and Nolte, Niklas S and Michaud, Eric and Tegmark, Max and Williams, Mike},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34651--34663},
  year={2022}
}

@article{he2024learning,
  title={Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks},
  author={He, Tianyu and Doshi, Darshil and Das, Aritra and Gromov, Andrey},
  journal={arXiv preprint arXiv:2406.02550},
  year={2024}
}

@article{touvron2023,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{gromov2023grokking,
  title={Grokking modular arithmetic},
  author={Gromov, Andrey},
  journal={arXiv preprint arXiv:2301.02679},
  year={2023}
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@misc{nanda2022transformerlens,
    title = {TransformerLens},
    author = {Neel Nanda and Joseph Bloom},
    year = {2022},
    howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
}

@article{YXLA2024-gsm1,
  author = {Ye, Tian and Xu, Zicheng and Li, Yuanzhi and {Allen-Zhu}, Zeyuan},
  title = {{Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process}},
  journal = {ArXiv e-prints},
  year = 2024,
  month = jul,
  volume = {abs/2407.20311},
  note = {Full version available at \url{http://arxiv.org/abs/2407.20311}}
}