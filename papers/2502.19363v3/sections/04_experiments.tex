\vspace{-3pt}
\section{Experiments}
\vspace{-3pt}
We empirically validate the \ourmethod{} method by training the language model from scratch.

\vspace{-3pt}
\subsection{Experimental Setup}
\vspace{-5pt}
\paragraph{\ourdata{}.} 
We used the Llama tokenizer \citep{touvron2023llama} to segment Slimpajama corpus~\citep{cerebras2023slimpajama}, a cleaned and deduplicated version of the RedPajama~\citep{together2023redpajama}, into 1024-tokens documents. The \ourmethod{} model then annotated these documents with 14 quality ratings and domain types, creating the 447B token \ourdata{} for pre-training.
Although the annotation process is expensive, it can be reduced via large-scale parallelization and cost-effective \ourmethod{} models.
The quality ratings and domain types in \ourdata{} can serve various purposes, such as data selection, data mixing, or continued pre-training in specific domains. 
Detailed statistics of \ourdata{} can be found in Table \ref{tab:sources_stats} of Appendix \ref{app:training_details}.

\paragraph{Data selection methods.}
For each baseline, we select a 30B token training dataset from \ourdata{} with one of the following methods, while retaining the original source proportion as the overall dataset. 
We leave it as future work to explore combinations of quality criteria to broaden our method.
% \footnote{All baselines were fairly reproduced using model checkpoints from Qurating's code \url{https://github.com/princeton-nlp/QuRating}}.
\begin{itemize}

\item \textit{Uniform}: We select randomly with a uniform probability across documents. For comparison's sake, we train an additional model on 45B tokens, requiring 50\% more compute.
% , referred to as \textit{Uniform +50\% data}.

\item \textit{DSIR}: We apply data selection with importance resampling (DSIR) \citep{xie2023data} and select examples that resemble either English Wikipedia or the Book domain \citep{together2023redpajama}---commonly used as proxies for quality \citep{brown2020language, touvron2023llama, xie2023data}. We follow \cite{xie2023data} and train hashed bigram models on \ourdata{} and the target data.

\item \textit{Perplexity Filtering}: We implement perplexity filtering \citep{wenzek2019ccnet, marion2023less} and select the documents with the lowest/highest perplexity scores, as computed by a pre-trained Sheared-Llama-2.7B model \citep{xia2023sheared}---2$\times$ the size of our \ourmethod{} model.

\item \textit{Sample with Qurating}:
We sample 30B tokens according to each of the four criteria described in \cite{wettig2024qurating}: \textit{writing style, facts and trivia, educational value, and required expertise}. 
Specifically, we normalize the variance of the quality ratings to be $1$ and then sample with temperature $\tau \in \{0.0 \text{ (i.e., top-$k$ selection)}, 2.0\}$. 
Additionally, we merge the Qurating-sampled data for $\tau=2.0$ of the four criteria as \textit{criteria mix}, and subsampling is as randomly to 30B tokens, ensuring that we exclude duplicate documents.

\item \textit{Sample with \ourmethod{} (Ours)}: For each of 13 quality criteria, we perform top-k sampling based on the quality ratings and domain type described in \cref{sec:manage_data_by_dataman}. Additionally, we explore sampling 30B tokens with overall scores $l \in \{1, 2, 3, 4, 5\}$  to demonstrate the effectiveness of all criteria.
\end{itemize}

\vspace{-8pt}
\paragraph{Training settings.}
Using different data selection methods, we select a 30B token subset from \ourdata{} and train a randomly initialized Sheared-Llama-1.3B language model \citep{xia2023sheared} for one epoch in a randomly shuffled order.
We train on slightly more data than the compute-optimal ratio \citep{hoffmann2022training} because more training tokens better reflect the performance gains attributed to data quality.
Further details are in \cref{app:training_details}. 

\vspace{-3pt}
\paragraph{Evaluation metrics.}
We provide a holistic evaluation of the language models trained on 30B tokens:
\begin{itemize}[topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=1em]
    \item We measure the perplexity over SlimPajama's validation set and test set, 500M tokens each.
    
    \item  We evaluate the in-context learning (ICL) performance using \texttt{lm-evaluation-harness} \citep{eval-harness}.
    We study 10 tasks, comprising 5 reading comprehension tasks (ARC-easy/challenge \citep{clark2018think}, SciQA \citep{welbl2017crowdsourcing}, LogiQA \citep{liu2020logiqa}, BoolQ \citep{clark2019boolq}), 3 commonsense reasoning tasks (HellaSwag \citep{zellers2019hellaswag}, PIQA \citep{bisk2020piqa}, WinoGrande \citep{sakaguchi2021winogrande}) and 2 knowledge-intensive tasks (NQ \citep{kwiatkowski2019natural}, MMLU \citep{hendrycks2021measuring}). We report the detailed settings in \cref{app:training_details}.

    \item We evaluate the instruction-following capabilities of our models, borrowing the setting used by \cite{xia2023sheared}. We perform supervised fine-tuning on 10,000 instruction-response pairs from the ShareGPT dataset. We evaluate another 1,000 instructions and use the AlpacaFarm codebase \citep{dubois2024alpacafarm} to judge the responses from two models with \texttt{GPT-4o}.
\end{itemize}


\subsection{Results}
We report the model's perplexity and ICL results in Table~\ref{tab:main_results}, the instruction-following evaluation in Figure~\ref{fig:instruction_ft_winrates}, the domain-specific continue pre-training performance in Table~\ref{tab:domain_specific_results}. 
Appendix~\ref{app:training_details} provides comprehensive results for all models, including validation and test perplexity across sources in Tables~\ref{tab:val_ppl_results},~\ref{tab:test_ppl_results}, and ICL results for individual task in Table~\ref{tab:icl_results}.

\vskip -2pt
\paragraph{Traditional methods perform poorly.} Table~\ref{tab:main_results} indicates that DSIR and perplexity filtering perform worse than random uniform sampling. This indicates that model output–based methods, despite their widespread use, are ineffective for data selection.

\vskip -2pt
\paragraph{Clear quality criteria work, but Qurating’s criteria mix fails.} Table~\ref{tab:main_results} shows that selecting data with Qurating’s four criteria \citep{wettig2024qurating} improves performance compared to uniform sampling, highlighting the importance of clear LLM quality signals. 
Educational value $\tau=2.0$ is the current SOTA baseline. 
However, the criteria mix of Qurating's four criterion did not perform well, possibly due to a lack of complementarity among the Qurating’s criteria.

\vskip -2pt
\paragraph{\ourmethod{}'s criteria surpasses SOTA baseline and \emph{Overall Score} works best.}
In Table~\ref{tab:main_results}, compared to the SOTA baseline (Educational value $\tau=2.0$), our 13 individual quality criteria improved ICL performance by 0.4\% to 4.3\%. 
For example, the sample-with-creativity model achieved an impressive score of 60.6 in commonsense reasoning tasks.
As the comprehensive criterion \emph{Overall Score} increased from 1 to 5, performance gains significantly, highlighting the necessity of quality ranking.
Our \emph{Overall Score l=5} works best, even exceeding the \textit{Uniform +50\% data} baseline, validating the feasibility of combining the 13 quality criteria into a composite criterion via LLM weighting. 
This approach not only avoids the disruption of manual adjustments but also achieves optimal results.
The correlations among all criteria in Figure~\ref{fig:quality_criteria_corr} further confirm this point.

\vskip -2pt
\paragraph{PPL and ICL are not strictly aligned.}
Our results in Table~\ref{tab:main_results} reveal a trend where PPL and ICL metrics correlate to a degree, increasing or decreasing simultaneously, but they are not aligned strictly. This meets with the intuition that PPL implies general understanding, while ICL focuses more on downstream generalization. 
Further analysis see Figure~\ref{fig:icl_vs_ppl} in Appendix~\ref{app:training_details}.
Notably, \ourmethod{}'s high-rated \emph{Overall Score} achieves an optimal trade-off between understanding and generalization.
\input{tables/main_results}

\vskip -2pt
\input{figures/instruction_ft_winrates}
\paragraph{\ourmethod{}'s instruction-following abilities also well.}
As shown in Figure~\ref{fig:instruction_ft_winrates}, we compare the instruction-following win rates between the Sample-with-\ourmethod{} model and the SOTA baseline (Educational value $\tau=2.0$).
The results indicate that, under the same supervised fine-tuning conditions, our model consistently surpasses the SOTA Baseline, with the \emph{Overall Score l=5} reaching an impressive win rate at 78.5\%, further validating the superior performance of our method.

\vskip -2pt
\input{tables/specific_domain_cpt_results}
\paragraph{Training domain-specific models.}
While the \emph{Overall Score l=5} achieves the best performance, its performance in specific domains can still be improved, as shown in Table~\ref{tab:domain_specific_results}. 
To address this, we applied \ourmethod{}'s domain recognition to filter data with high \emph{Overall Score} in the \textit{medical, law, and financial} domains, and continued pre-training domain-specific models, achieving ICL performance gains in the corresponding MMLU subtasks. This validates \ourmethod{}'s capability for domain mixing.
\vskip -2pt