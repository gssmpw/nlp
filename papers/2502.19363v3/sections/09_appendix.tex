\section{Prompt Curation}\label{app:detailed_prompts}
\input{figures/full_prompts}
Our full prompt is shown above, where \text{\{text\}} represents the text to be annotated. Next, we will elaborate on the entire prompt curation process, including obtaining all quality criteria, domain types, and system prompts, which were conducted by experts with the assistance of Super LLM:

\paragraph{Initializing quality criteria.} 
Inspired by ``\emph{reverse thinking}''—\emph{prompting the LLM to self-identify which criteria benefit its performance}, as its pre-training capabilities are closely related to perplexity (PPL)~\citep{muennighoff2024scaling,marion2023mdatapruning}. To this end, we devised an analytical prompt for Super LLM to investigate the reasons behind textual PPL anomalies (in the top and bottom 2\%) from each source, and extract the initial quality criteria below. For clarity, Table~\ref{tab:initial_quality_criteria_case} provides several examples illustrating how these initial quality criteria are derived.

\paragraph{Enhancing quality criteria.} 
Next, we utilize Super LLM to rank the initial quality criteria by importance, eliminating, merging, and supplementing them. The resulting second-step criteria include: \textit{[1] Text accuracy (grammar, references, spelling), [2] Semantic coherence and consistency, [3] Language consistency, [4] Effective semantic content ratio, [5] Knowledge novelty, [6] Topic focus, [7] Creative expression ratio, [8] Proportion of technical terms, [9] Style variability, [10] Complexity of grammatical structures, [11] Content regularity, [12] Content redundancy, [13] Proportion of sensitive topics}. 
We then revised the prompts under the principle that higher scores indicate better criteria.
With Super LLM's assistance, we simplified the criterion names, detailed the criteria for each rating level, and ultimately derived 14 quality criteria in the paper.

\input{figures/initial_quality_criteria}

\paragraph{Identifying domains.} 
We identified the 15 domain types that require assessment, based on factors such as typical application LLM industries~\citep{naveed2023comprehensive}, the number of existing industry LLMs, and the level of attention they have received, as indicated by metrics like GitHub stars\footnote{https://github.com/HqWu-HITCS/Awesome-Chinese-LLM}.


\paragraph{Chain-of-though and system prompts.}
We incorporated the chain-of-thought prompting\citep{wei2022chain}—\textit{``let's think step by step''}—while avoiding using few-shot example prompting, as the diverse sources of text could introduce rating biases. We allowed the Super LLM to generate system prompts to enhance the accuracy and confidence of quality ratings, \textit{``You are an expert to evaluate the text quality with high accuracy and confidence. Don't hesitate to use the full range of the score scale, including extreme scores if the text warrants it.''}  Results indicated that this prompt facilitated more nuanced text annotations compared to the typical prompt, \textit{``You are a helpful assistant.''}. To ensure stable output from Super LLM, we set the temperature to 0.

\input{tables/initial_quality_criteria}
\input{tables/prompt_validation_sources}
\input{tables/ten_wriqual_texts}


\FloatBarrier
\section{\ourmethod{} Model} \label{app:dataman_model}
\subsection{Fine-tuning dataset}
To create the fine-tuning dataset of DataMan, we collected documents from both in-source and out-of-source within the large pre-training corpus SlimPajama \citep{cerebras2023slimpajama}. For each document, we used the full prompt to instruct the Super LLM to generate scalar scores ($l \sim [1-5]$) across 14 quality criteria, along with an $[A-O]$ letter grade to indicate its domain type. 
The fine-tuning dataset has 357k documents. 
Each document was limited to 2,048 tokens, averaging 810 tokens. It outperforms the [256, 512] token range of \cite{wettig2024qurating} when handling documents with broader length variations. 
Table~\ref{tab:sft_data_stats} lists the number and proportion of documents categorized by domain, \emph{Overall Score}, and source in the fine-tuning dataset.


\paragraph{Domain analysis.} Domain \emph{Other} accounts for nearly 25\% indicating that the fine-tuning dataset encompasses domains outside the existing 15 domains, providing \ourmethod{} with rich domain-specific prior knowledge.
Domains that account for between 15\% and 3\% involve texts related to web crawling (such as \emph{entertainment} and \emph{culture}) as well as typical vertical domains (like \emph{medicine} and \emph{coding}), enabling \ourmethod{} to better address both general and specialized knowledge.
Finally, the collection of data with high barriers to entry, such as \emph{mathematics} and long-tail \emph{telecom} data, remains a challenge for data management.

\paragraph{Overall Score analysis.} Considering the imbalance in the collected documents between high and low scores, we performed up-sampling on low-scoring documents ($< 3$) to avoid biases in the quality ratings for \ourmethod{}.
In practice, we divided the sources into five equal parts based on the difference between high- and low-scoring documents and performed a fourfold up-sampling on low-scoring documents, ultimately reaching a total dataset size of 425,794.

\paragraph{Source analysis.} While ensuring adequate data within the SlimPajama domain, we also introduced 19\% of out-of-domain data (\emph{Other}) to enhance \ourmethod{}'s source generalization capability.
\input{tables/sft_data_stats}


We present the average score for quality criteria across each domain in Table \ref{tab:sft_avgscore_domains}, with analyses below:

\begin{itemize}
\item \textbf{Knowledge Novelty} excels in \emph{mathematics} and \emph{medicine}, closely linked to cutting-edge scientific research, enhancing the model’s scientific abilities.

\item \textbf{Creativity} is highest in \emph{culture} and lowest in \emph{legal} domains, reflecting the openness of literary works versus the stability of legal texts, thereby improving the model's literary skills.

\item High \textbf{professionalism} indicates data from specialized fields like\emph{mathematics}, \emph{law}, \emph{medicine}, and \emph{finance}, enhancing the model’s performance in these areas.

\item \textbf{Coding} exhibits the least \emph{grammatical diversity} and high \emph{structural standardization} due to its fixed grammatical formats. In contrast, low values in \emph{retail e-commerce} for these two criteria suggest that they lack correlation.

\item Data from specialized domains showcases strong \emph{originality} and \emph{semantic density}, with low content redundancy and meaningful content, improving the model's performance in vertical fields.

\item The \textbf{government} and \textbf{entertainment} domains exhibit lower \emph{sensitivity}, likely related to free speech on social media and politically sensitive topics, aiding the model in filtering harmful speech and sensitive content.

\item Other criteria perform well across all domains, ensuring basic requirements are met and enhancing the model’s general capabilities.

\item In general, specialized domains tend to achieve higher \emph{Overall Score}, while long-tail and general domains are relatively lower.
\end{itemize}
\input{tables/sft_avgscore_domains}

\input{figures/quality_criteria_corr}
Figure~\ref{fig:quality_criteria_corr} shows the Pearson correlation heatmap among various quality criteria. All criteria are positively correlated, with Pearson correlation coefficients generally below 0.8, except for \emph{Style Consistency} and \emph{Structural Standardization}, which we speculate may adapt as basic requirements with other criteria. Notably, since the \emph{Overall Score} is derived from the remaining 13 quality criteria, it highly correlates to each individual criterion. 





\FloatBarrier 
\subsection{\ourmethod{} Training}
We fine-tune the DataMan model using Qwen2-1.5B \citep{yang2024qwen2}, an advanced open-source 1.5B parameter language model, based on text generation loss. To meet diverse application needs, we offer three DataMan model versions, named according to their chat prompts and applicable scenarios:

\input{figures/chat_prompt}
\begin{itemize}
    \item \textbf{Score-only DataMan} rates the \emph{Overall Score} of text (1 token), ideal for large-scale dataset filtering.
    \item \textbf{Domain-only DataMan} identifies text domain (1 token), ideal for large-scale data mixing.
    \item \textbf{All-rating DataMan} rates the 14 quality criteria of text and identifies the text domain (15 tokens), ideal for refined data selection and mixing.
\end{itemize}

\paragraph{Hyperparameter search.} Next, we conduct a hyperparameter search using a validation set of 8.6k documents. The search grid included: seed $\epsilon \{42, 1024, 3407\}$, learning rate $\epsilon \{1 \times 10^{-6}, 7 \times 10^{-6}, 1 \times 10^{-5}, 2 \times 10^{-5}, 5 \times 10^{-5}\}$, number of epochs $\epsilon \{2, 3, 4, 5\}$, batch size $\epsilon \{256, 512, 1024\}$, data size $\epsilon \{82k, 164k, 246k, 312k, 357k\}$, up-sampling fold $\epsilon \{1, 2, 3, 4, 5\}$, model size $\epsilon \{0.5B, 1.5B\}$, and inference temperature $\epsilon \{0.0 \text{(greedy decoding)}, 0.1, 0.3, 0.5, 0.8, 1.0\}$. 
Model selection was based on which model achieves the best accuracy on the criterion of \emph{Overall Score}.
The selected model parameters were: seed 1024, learning rate $1 \times 10^{-5}$, trained for 5 epochs, batch size 512, data size 357k, 4-fold up-sampling ratio, 1.5B model size, and utilizing greedy decoding for inference.

\paragraph{Inference accuracy.} Subsequently, we evaluated the accuracy of three DataMan model versions on a test set comprising 8.6k documents, as shown in Table~\ref{tab:sft_model_eval}.
Leveraging gold-labeled fine-tuning data from Super LLM, all model versions exhibited excellent performance. 
All-rating DataMan achieved nearly 80\% accuracy across quality criteria, with \emph{grammatical diversity} and \emph{structural standardization} being the most challenging criteria to predict.
We found performance limitations in quality rating using the \emph{Overall Score} criterion, which showed a five-class accuracy of 81.3\% and a binary accuracy of 97.5\%. The accuracy for high-quality documents reached 98.5\%, but for low-quality documents, it was only 81.6\%, due to insufficient samples. We aim to collect more low-quality documents to improve DataMan's accuracy in this area.  

\paragraph{Misclassification analysis.} In Table~\ref{tab:detailed_analysis_overall_score}, we analyze the test accuracy of the \emph{Overall Score} criterion to verify that DataMan rarely makes unreasonable decisions, making it unlikely to cause a ``snowball effect''. 
In addition to the 5-level classification accuracy (5-level Acc) used in the paper, we classify samples with a score of 3 or above as positive samples, and vice versa as negative samples, thus obtaining 2-level classification accuracy: (<3, $\geq$3 Acc). We detail the accuracy for positive samples ($\geq$3 Acc) and negative samples (<3 Acc), as well as error rates for specific misclassification cases: extreme false negative samples (<2 but $\geq$3 Error Rate), moderate false negative samples ($\geq$2, <3 but >3 Error Rate), marginal false negative samples ($\geq$2, <3 but =3 Error Rate), extreme false positive samples ($\geq$4 but <3 Error Rate), and marginal false positive samples ($\geq$3, <4 but <3 Error Rate). Results show the error rate for DataMan in the two extreme misclassification cases of the \emph{Overall Score} is very low, at just 0.2\%. This indicates that DataMan rarely mistakes poor-quality documents for high-quality ones, and vice versa. Considering the strong fault tolerance of pre-training, the snowball effect will not become a bottleneck.

\input{tables/sft_model_eval}
\input{tables/detailed_analysis_overall_score}
\input{tables/inference_FLOPS}

\paragraph{Inference efficiency.} Finally, Table~\ref{tab:inference_FLOPS} presents the inference FLOPs and memory usage for three DataMan model versions evaluated on a single A800 GPU using vLLM~\citep{kwon2023efficient}. To reduce DataMan annotation costs, we recommend cost-effective models like Score-only DataMan or fine-tuned Qwen2-0.5B and suggest changing the training objective from text generation to multi-task classification. 
However, to avoid parameter transfer issues due to the different learning paradigms between pre-training and fine-tuning \citep{wang2019characterizing}, we continue using text generation loss. Furthermore, heuristic pre-processing, such as deduplication using Fuzzydedup~\citep{jiang2022fuzzydedup} and Semdedup~\citep{abbas2023semdedup}, or rule-based and model-based selection methods like C4 filter~\citep{raffel2020exploring}, Gopher rules~\citep{rae2022gopher}, and binary grammar discriminators~\citep{chowdhery2023palm}, can pre-reduce data annotation needs.
\FloatBarrier



\section{Experimental Details} \label{app:training_details}
\paragraph{\ourdata{} statistics.} Table~\ref{tab:sources_stats} shows the domain, \emph{Overall Score}, and source statistics of the 447B \ourdata{} token corpus, from which we select 30B tokens using different data selection methods.
Firstly, from a domain perspective, the proportion of the mathematics domain in \ourdata{} has significantly increased compared to the fine-tuning dataset, while the coding domain has seen a slight rise. 
In contrast, the proportions of all long-tail domains (such as Transportation, Agriculture, Retail E-commerce, and Telecommunications) still remain at the lowest levels. 
Secondly, regarding overall scores, \ourdata{}, as a subset of Slimpajama, has undergone extensive cleaning and deduplication, resulting in a high proportion of samples rated 5 and 4. 
Conversely, low-quality texts (rated below 3) account for only 7.86\%. We chose to retain these low-quality texts to allow researchers for in-depth analysis.
\textit{\ourdata{} is a curated subset of SlimPajama, which is itself a subset of RedPajama. Both SlimPajama and RedPajama are released on HuggingFace under the Apache 2.0 License.}



\paragraph{Training details.} 
Using different data selection methods, we select the 30B token subset from 
\ourdata{} and train a language model from scratch for one epoch in a randomly shuffled order.
We employ a Sheared-Llama-1.3B transformer architecture with RoPE embedding \citep{su2024roformer} and SwiGLU activations \citep{shazeer2020glu}.
This model is trained using a global batch size of 2048 sequences and a learning rate of $5\times10^{-4}$ with a cosine learning rate decay to $5\times10^{-5}$ and a linear warmup for the first $5\%$ of training steps.
We use a weight decay of $0.1$ and train with Adam \citep{kingma2014adam} with hyperparameters $\beta = (0.9, 0.95)$.
Finally, we save a checkpoint every 1,000 steps and merge the last three using mergekit \citep{goddard2024arcee} as the desired LLM, eliminating biases from step fluctuations.
Each model is trained on 32x NVIDIA A800 over 228 GPU hours. 
\input{tables/pretrain_data_stats}

\paragraph{In-context learning settings.}
We choose a different number of few-shot examples per task to ensure that all demonstrations fit within the context window of 1024 tokens.
We use the following number of demonstrations (given in parentheses): ARC-easy (15), ARC-challenge (15), SciQA (2), LogiQA (2), BoolQ (0), HellaSwag (6), PIQA (6), WinoGrande (15), NQ (10), MMLU (10). We report accuracy for all tasks, except for NQ, where we report EM.
When available, we use the normalized accuracy metric provided by \texttt{lm-evaluation-harness}.


\paragraph{Full results of perplexity and ICL.} 
In Tables \ref{tab:val_ppl_results} and \ref{tab:test_ppl_results}, we report the full validation and test perplexity results for all models, including those for each RedPajama source. Table \ref{tab:icl_results} contains the ICL performance of all models across 10 downstream tasks.
The lowest perplexity reveals how quality criteria enhance LLM performance in specific data sources: 
i)-\emph{Sensitivity} excelled in web domains (CommonCrawl, C4), emphasizing the importance of avoiding sensitive topics to improve web content adaptability.
ii)-\emph{Semantic Density}, \emph{Originality}, and \emph{Topic Focus} showed superior performance in Wikipedia, suggesting the benefit of informative, original content for world knowledge absorption.
iii)-\emph{Creativity} scored lowest in book sources, indicating its role in enhancing the understanding of literature.
In ICL tasks, the criteria's influence is as follows:
i)-High \emph{Semantic Density} improved performance in elementary science tasks (ARC-E, ARC-C) and, alongside high \emph{Professionalism}, in more advanced questions (SciQ).
ii)-High \emph{Creativity} aided in summarization and subtitle tasks (HellaSw., W.G.), while complex reasoning tasks (PIQA) benefited from a mix of high \emph{Semantic Density}.
iii)-High \emph{Originality} was effective for Wikipedia-related tasks, where redundant knowledge was counterproductive.

\input{tables/val_ppl_results}
\input{tables/test_ppl_results}
\input{tables/icl_results}

\input{tables/main_results_60B}
\input{tables/val_ppl_results_60B}
\input{tables/test_ppl_results_60B}
\input{tables/icl_results_60B}

\paragraph{Results on larger 60B tokens.}
Despite the 30B token subset exceeding the compute-optimal ratio of data-to-model suggested by \citep{hoffmann2022training}, to solidify our method, we used the strongest DataMan variant \emph{Overall Score l=5}, the existing SOTA baseline (education value $\tau=2.0$), and uniform sampling to select a larger 60B subset to train the 1.3B language model from scratch. 
The model's PPL and ICL performance of 60B subset in Table~\ref{tab:main_results_60B}, the full validation and test PPL across sources in Tables~\ref{tab:val_ppl_results_60B}, ~\ref{tab:test_ppl_results_60B}, and the full ICL performance of ten downstream tasks in Table~ \ref{tab:icl_results_60B}. The Sample-with-DataMan model significantly outperformed the SOTA baseline in ICL performance tasks and showed modest improvement in the validation and test PPL. This further confirms the effectiveness of the DataMan approach.


\paragraph{Misalignment between PPL and ICL.} 
In Figure~\ref{fig:icl_vs_ppl}, we plot the relationship between perplexity and ICL performance for all models across 10 downstream tasks, including the Pearson and Spearman correlation coefficients, to investigate the misalignment between PPL and ICL. 
The results indicate that the misalignment is most pronounced in the LogiQA and MMLU tasks. Deeper analysis identifies two main causes:
\textit{i)-domain mismatch:} pre-training often uses extensive general corpora, which enables the model to exhibit lower perplexity on a common text. However, tasks like MMLU, which span 57 distinct specialized domains (such as abstract algebra and anatomy), may suffer in ICL performance due to domain mismatch;
\textit{ii)-ICL task complexity:} Many ICL tasks require complex reasoning rather than simple text generation, which perplexity assessment struggles to capture. This is particularly evident in LogiQA, where the task assesses human logical reasoning skills through expert-written questions from Civil Servants' Exams.

\input{figures/icl_vs_ppl}


\FloatBarrier
\section{Inspecting Raw Documents and Ratings} \label{app:raw_documents}

Finally, we present snippets from the raw documents of Wikipedia, Books, Stack Exchange, Github, ArXiv, CommonCrawl, and C4 subsets of \ourdata{}. 
These documents correspond to samples with quality ratings of 1, 2, 3, 4, and 5 across 14 quality criteria, as shown in Figure~\ref{fig:quality_rating_distribution}. 
Notably, although these samples represent just a small random snippet, they display significant quality differences. We believe it is essential to provide an unfiltered view of the training data; therefore, we have not applied any filtering to these documents.
\textbf{\textcolor{red!90!black}{A small number of documents contain \%potentially sensitive content.}}

From the visual examples, we found a notable distinction between the data rated 1 and 2 and those rated 3, 4, and 5.  For instance, the score of 1 corresponds to an example like \textit{``...83 510 l s 311 548 m 305 546 l 301 540 l 299 530 l 299 ...''}, whereas the score of 5 reflects \textit{``...system recognizes a hierarchy of events from the measurements, not exactly in the sense of physical reality...''}  However, the discrepancy between scores of 4 and 5 is not as pronounced, as seen in the example \textit{``...have been augmented with terms that quantify the user satisfaction or the ad relevance...,''} which corresponds to a score of 4.  This further supports our rationale for choosing pointwise evaluation over pairwise, as humans also find it challenging to determine superiority based on subtle differences.

In terms of domain adaptability, most of the evaluation criteria we established are semantic-focused, allowing for effective differentiation of documents within the C4 domain.  However, we also observed that our criteria have some relevance in the code domain (e.g., GitHub).  Specifically, code that features more detailed comments and follows structural conventions tends to receive higher scores, while disordered code is typically rated lower.
\input{figures/examples}
% \input{tables/vis_table}

