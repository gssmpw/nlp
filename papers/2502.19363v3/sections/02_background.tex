\vspace{-3pt}
\section{Related Work}
\vspace{-10pt}
\paragraph{Deduplication.}
Deduplicating training data is now standard in managing pre-training data for LLMs, as it greatly impacts performance~\citep{lee2022deduplicating}. While~\cite{kaplan2020scaling} and~\cite{hoffmann2022empirical} examine scaling laws with unique data trained for one epoch, some studies~\citep{hernandez2022scaling,muennighoff2024scaling,xue2024repeat} suggest that repeated data can harm performance, particularly as repetition epochs and model size grow. Additionally, fuzzy or semantic deduplication improves LLM performance~\citep{jiang2022fuzzydedup,abbas2023semdedup,tirumala2023d4}. But deduplication should precede quality-based selection and domain mixing, and it is unsuitable for sampling fixed-size subsets.

\paragraph{Heuristic-based Selection.} This selection approach includes two methods: \textit{Rule-based heuristics}, which apply manually crafted rules—such as mean word length, stop word fraction, and word repetition thresholds~\citep{laurenccon2022bigscience,TogetherAI,penedo2023refinedweb,soldaini2024dolma}—to filter data, exemplified by the C4 filter~\citep{raffel2020exploring} and Gopher rules~\citep{rae2022gopher}. These reduce noise effectively but need complex manual design. In contrast, \textit{Model-based heuristics} use models like binary grammar discriminators~\citep{chowdhery2023palm,touvron2023llama} to select data resembling the target domain, alongside techniques like importance resampling~\citep{xie2023data} and perplexity filtering~\citep{wenzek2019ccnet, muennighoff2024scaling, marion2023less}. However, they require more precise quality ratings for optimal selection.

\paragraph{Domain mixture.}
Most pre-training datasets, like the Pile~\citep{gao2020pile}, comprise mixed data from various sources and domains~\citep{nijkamp2023codegen2,zhang2023llama,yang2024gpt4tools,maini2024rephrasing,li2024synthetic}. As LLMs gain traction, domain-specific data for improved functionalities are increasingly used in model training~\citep{du2022glam,gao2023llama}. Identifying the optimal domain mixture ratio is essential for effective LLM pre-training~\citep{wang2023data}. Early attempts to define this relied on experiments and intuition~\citep{gao2020pile,thoppilan2022lamda}. Recent studies have begun to use automatic methods, such as domain generalization~\citep{xie2023data,xie2024doremi}, domain gradients~\citep{fan2023doge}, and loss evaluation~\citep{xia2023sheared}, to assign domain weights and assess model performance across various mixtures. This inspires our work to introduce domain types to assist in pre-training data mixture.

\paragraph{LLM quality signals.}
Appropriate LLM quality signals are valuable for selecting pre-training data~\citep{gunasekar2023textbooks}. Research shows that data enriched with facts and trivia aids LLMs in accurately addressing niche topics and fictional worlds~\citep{petroni2019language,korbak2023pretraining}. Other studies have emphasized the educational value of data as a key for enhancing LLMs' reasoning capabilities~\citep{wei2022chain,kojima2022large}. Recent efforts have integrated these insights, proposing four quality criteria—writing style, facts and trivia, educational value, and required expertise—to assess pre-training data and enhance LLM capabilities~\citep{wettigqurating}. However, these criteria depend heavily on human intuition and thus lack generality and comprehensiveness.