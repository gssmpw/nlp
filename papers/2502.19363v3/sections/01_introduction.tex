\vspace{-5pt}
\section{Introduction}
\vspace{-3pt}
As large language models (LLMs) demonstrate performance emergence driven by data scaling laws, the significance of data has become increasingly evident~\citep{kaplan2020scaling, brown2020fewshotlearners, chowdhery2023palm}. This trend prompts researchers to explore how to select pre-training data, including deduplication~\citep{lee2022deduplicating}, domain mixing~\citep{gao2020pile, shen2023slimpajama}, heuristic-based data selection~\citep{rae2022gopher, wenzek2019ccnet}, and data sampling using LLM quality signals~\citep{gunasekar2023textbooks, wettig2024qurating}.
Although these efforts aim to enhance data quality and diversity, deduplication and domain mixing are solely used as a priori or post-hoc steps in the data selection process. Furthermore, existing data selection methods typically rely on limited heuristics and human intuition, lacking comprehensive and clear criteria for data selection, thus making the selection of ideal pre-training data for LLMs an unresolved challenge.

Following this line, this paper provides guidelines for selecting pre-training data, including quality criteria, application domains, and a \textbf{Data} \textbf{Man}ager (\textbf{DataMan}) with a comprehensive quality rating and domain recognition, equipped by data sampling strategies to enhance the LLM performances.
Firstly, we believe that excellent quality criteria must: 1)-apply to a wide variety of texts; 2)-demonstrate a deep understanding of content, capturing semantic levels; and 3)-complement each other.
However, existing studies~\citep{rae2022gopher, wettig2024qurating} rely on limited heuristics and human intuition, while grounded in empirical findings, lack generality and comprehensiveness.
To address this issue, we are inspired by ``\emph{reverse thinking}'' â€” \textbf{prompting LLM to self-identify which criteria benefit its performance}, as its pre-training capability is related to perplexity (PPL)~\citep{muennighoff2024scaling,marion2023mdatapruning}.
We extracted documents with the top 2\% and bottom 2\% PPL from different sources and used Super LLM to \textbf{identify the reasons for anomalous in document perplexity}.
Through iterative refinement, we derived 13 quality criteria related to LLM performance: \emph{Accuracy, Coherence, Creativity, Grammatical Diversity, Knowledge Novelty, Language Consistency, Originality, Professionalism, Semantic Density, Sensitivity, Structural Standardization, Style Consistency, and Topic Focus}, and formed comprehensive criterion called \emph{Overall Score}.
Further, we introduced 15 common domain types in the LLM application industry~\citep {naveed2023comprehensive} for domain mixing and developed a full prompt to curate a fine-tuning dataset.
We employ a small-scale LLM to fine-tune DataMan via text generation and apply pointwise rating over pairwise rating~\citep{liu2007letor} for more applicable, cost-effective inference on vast datasets. 
Using DataMan, we annotated 447B tokens in the Slimpajama corpus~\citep{cerebras2023slimpajama} with quality ratings and domain types to create the DataPajama dataset.
While ensuring the diversity of sources and domains, we maximized the representativeness of quality criteria, sampling a 30B token subset from the DataPajama and trained the Sheared-LLaMA-1.3B language model~\citep{xia2023sheared} from scratch.

\begin{figure*}[t]
    % \vskip 0.1in
    \centering
    \centerline{\includegraphics[width=0.92\linewidth]{figures/pipeline.pdf}}
    % \vskip -0.1in
    \caption{
The pipeline of the Sample-with-DataMan model: We derived 14 quality criteria from LLMs' \emph{reverse thinking} and used DataMan to annotate the quality rating and domain type of pre-training data. By employing data sampling strategies to select subsets, the performance of trained LLMs outperforms the state-of-the-art data sampling baseline.
    }
    \label{fig:pipeline}
    \vskip -10pt
\end{figure*}

In our experiment, we initially validated the feasibility of using prompt as it achieved over 95\% agreement with human preferences.
We then revisited the limitation of pairwise rating~\citep{wettig2024qurating}, bounded pointwise rating error by its model loss, and fairly ranked the annotation results from both ratings to explain, from three perspectives, why we chose pointwise rating.
While training the DataMan model, we thoroughly analyzed its fine-tuning datasets, hyperparameter search, inference accuracy and efficiency of three model versions, and misclassification issues.\
Further, We evaluated language models trained on 30B tokens sampled with different data selection methods from the DataPajama dataset.
In ten downstream tasks, the Sample-with-DataMan model, trained on data sampled with Dataman's 13 quality criteria, outperformed the existing state-of-the-art (SOTA) baseline (Educational value $\tau=2$) in in-context learning performance by 0.4\% to 4.3\%, showcasing the effectiveness of these criteria.
As the \emph{Overall Score} rose from 1 to 5, both ICL performance and PPL improved significantly, confirming the necessity of quality ranking. 
Our strongest model was achieved at \emph{Overall Score l=5}, even surpassing the model trained on uniform sampling with 50\% more data, as the \emph{Overall Score} encompasses attributes of all criteria, supported by its high correlation with other quality criteria.
To solidify our results, we sampled a larger 60B data subset and compared the strongest Sample-with-DataMan model \emph{Overall Score l=5} against the existing SOTA baseline.
Meanwhile, we found PPL and ICL performance are not aligned strictly. 
Through correlation analysis and visualization, we reveal that PPL reflects general understanding, whereas ICL captures downstream generalization ability.
In the instruction following tasks, all Sample-with-DataMan models overwhelmingly surpassed the existing SOTA baseline with a win rate between 67.1\% and 78.5\%.
Additionally, We continued pre-training the strongest \emph{Overall Score l=5} model using high-rated, domain-specific data annotated by DataMan, achieving superior ICL performance in specific domains, thereby verifying DataMan's capability for domain mixing.
We also conducted an in-depth analysis of the DataPajama dataset, explored the distribution of DataMan's quality ratings from various sources, and inspected the original documents corresponding to each quality rating=1,2,3,4,5.
we also identified complementary relationships among quality criteria, while their low correlation with perplexity further confirms the novelty of DataMan's quality criteria.

Our contributions are summarized as follows:

\thickspace\thinspace 1. Based on the relationship between LLM performance and PPL, ``\emph{reverse thinking}'' allows LLMs to self-identify which criteria benefit its
performance, assemble with typical application domains to guide data selection.

\thickspace\thinspace 2. We introduce DataMan, offering comprehensive quality ratings and domain recognition, along with data sampling strategies to optimize LLM pre-training. Vast experiments validate its efficacy, setting new performance records.

\thickspace\thinspace 3. We will release the code, all models, and the annotated DataPajama dataset, paving the way for the community to explore the guidelines between data and LLMs further.