\vspace{-3pt}
\section{Annotating Text by DataMan}
\vspace{-3pt}
We developed \textbf{DataMan} (see Figure \ref{fig:pipeline}), a data manager that comprehensively annotates 14 quality criteria and domain types, enabling ideal data selection and mixing.

\subsection{Overview of the Annotation}
Let $\mathbf{t} = \{ t_1, \ldots, t_n \}$ represent all documents to be annotated. 
The annotation results of DataMan correspond to querying the ratings of document $t_n$ across all quality criteria and its domain type.
Assume that the quality ratings and domain recognition are in a multi-level annotation format as $\mathcal{L} = \{(l_1^{1}, \ldots, l_1^{C}), \ldots, (l_n^{1},\ldots, l_n^{C})\}$, where $l_n^{C} \in \{1, \ldots, K\}$ is the label for the $C$-th criterion or domain of document $t_n$. 
For this paper, $K$ is 5 for quality ratings and 15 for domain recognition.
Let $F$ be a class of functions, and $f \in F$ be the annotation function. 
We use a super LLM as the annotation function, recording the annotation results for each criterion and domain, expressed as: $ f(t, \mathcal{L}) = \left\{ (t_1, l_1^{1}, \ldots, l_1^{C}), \ldots, (t_n, l_n^{1}, \ldots, l_n^{C}) \right\}. $
Thus, we can quickly create a fine-tuning dataset for DataMan, $\mathcal{S} = \left\{ (t_i, l_i^{1}, \ldots, l_i^{C}) \right\}$. 
Essentially, our method uses pointwise rating by pointwise learning to rank (L2R) model \citep{liu2007letor, liu2009learning}. 
We minimize the loss function based on the document, annotation labels, and function:
\vspace{-5pt}
\begin{equation}\label{eq:1}
L(f; t, \mathcal{L}) = \sum_{i=1}^{n} \left( f(t_i) - l_i \right)^{2}.
\vspace{-3pt}
\end{equation}
This trains DataMan to learn the annotation functions for each quality criterion and domain type.


\subsection{Prompt Curation}
How to define the quality criteria and domain types of texts? We believe that excellent quality criteria should: 1)-apply to a wide variety of texts, 2)-demonstrate a deep understanding of content, capturing semantic levels, and 3)- complement each other. However, prior studies largely relied on human intuition, such as educational value in \citet{gunasekar2023textbooks}, writing style in \citet{wettig2024qurating}, and toxicity and privacy in \citet{korbak2023pretraining}, which lack generality and comprehensiveness.
To address this issue, we undertook an in-depth exploration of text quality, motivated by ``\emph{reverse thinking}''—\emph{prompting the LLM to self-identify which criteria benefit its performance}. Specifically, since LLMs' pre-training abilities are closely related to their PPL~\citep{muennighoff2024scaling,marion2023mdatapruning}, where ``high PPL indicates data is difficult to learn, and vice versa'', we focused on the training text from various sources with the top 2\% and bottom 2\% of PPL. We devised an analytical prompt for a Super LLM\footnote{
The Super LLM can independently refer to any advanced LLM. In this paper, We use the state-of-the-art LLM available at that time, \texttt{GPT-4-0125-preview}.} to investigate the reason behind these perplexity anomalies in documents, aiming to analyze the traits of both easy-to-learn and difficult-to-learn data. Through iterative refinement, we derived 14 quality criteria as shown in Figure~\ref{fig:pipeline}. Additionally, we identified the 15 domain types that need to be assessed from typical LLM application industries~\citep{naveed2023comprehensive}, integrating the \emph{"let's think step by step"} chain-of-thought prompting strategy \citep{wei2022chain} and a thoughtfully designed system prompt. 
Further details of the entire process are in Appendix \ref{app:detailed_prompts}.

\paragraph{Prompt validation.}
We validate prompt effectiveness using clear cases before prompt use. Initially, we gathered a pool of documents preliminarily rated by an independent group, splitting them into two sets of ten documents each—high-rated and low-rated—to ensure a distinct quality gap. Table \ref{tab:prompt_validation_sources} of Appendix \ref{app:detailed_prompts} details these document sources. These 20 documents were randomly shuffled and then rated on a scale of 1-5 based on each quality criterion by five independent human annotators who had not seen them before. Subsequently, using the super LLM with our prompt, we evaluated the same documents and compared super LLM ratings with human majority votes, finding over 95\% agreement. Also, we ensured inter-rater reliability among human annotators by calculating the Kappa coefficient~\citep{mchugh2012interrater}, which validated the consistency of human ratings. However, this human validation remains subjective, we hope the community develops a more rigorous method.

\subsection{Why Use Pointwise Rating?}

Using LLMs to rate text falls into two categories: pointwise rating and pairwise rating. Here, we explain why we chose the pointwise rating from the following three aspects:

\paragraph{Pairwise rating limitations.} 
We revisited pairwise rating~\citep{wettig2024qurating} and observed minimal quality differences among the top-3 documents ranked by \emph{writing style}, as even humans struggle to discern differences (see Table \ref{tab: ten_wriqual_texts}). 
This highlights the limitations of pairwise ratings when quality differences are marginal, as documents meeting an ``acceptable'' quality threshold should be accepted, where pointwise rating aligns well with human judgment.
Appendix B.1 notes that creating a fine-tuning dataset requires Super LLM to make 40 predictions per criterion and document pairs, then use LLM preferences as labels.
Table 5 shows that document pairs need a $\geq$50\% selection probability difference for quality criteria, also required for inference in Table 6, thus questioning the practicality of pointwise ratings.
For $N$ documents, pointwise ratings only need $N$ ratings, whereas pairwise ratings demands $N*(N-1)$ comparisons with 40 predictions each, greatly increasing costs.
Despite both ratings capture different aspects, pointwise rating's broader applicability and cost-effectiveness in fine-tuning dataset curation or pre-training data annotation make it our preferred choice.


\textbf{Bound pointwise rating error.} 
We present the mathematical connection between rating errors and loss in the pointwise rating model~\citep{chen2009ranking}.
\vspace{-5pt}
\begin{gather}~\label{eq:2}
1 - NDCG(f; t, \mathcal{L}) \leq \frac{15\sqrt{2}}{N_n} \left( \left( \sum_{i=1}^{n} D(t_i)^2 \right) - n \prod_{i=1}^{n} D(t_i)^{2/n} \right)^{1/2} \left( L(f; t, \mathcal{L}) \right)^{1/2}, \\
NDCG(f; t, \mathcal{L}) = \frac{1}{N_n} \sum_{i=1}^{n} G(l(\pi_f(t_i))) D(t_i), \\
N_n = \max_{\pi} \sum_{i=1}^{n} G(l(\pi(t_i))) D(t_i),
\vspace{-5pt}
\end{gather}
where NDCG is rating measures defined with respect to \( K \)-level ratings \( \mathcal{L} \), \( G \) is the gain function, \( \pi_f \) is the rating list produced by the rating function \( f \), and \( D \) is the position discount function.
One usually sets \( G(z) = 2^z - 1, D(z) = \frac{1}{\log_2(1+z)} \) if \( z \leq M \), and \( D(z) = 0 \) if \( z > M \) ($M$ is a fixed integer).
Thus, as the pointwise rating model minimizes loss via training, the rating error also reduces, validating the feasibility of pointwise ratings theoretically.

\textbf{Fairly compare both rating results.}
To ensure a fair comparison despite differing evaluation metrics, we ranked annotations from Qurating's pairwise and DataMan's pointwise ratings on the same 58,824 documents from the first split of Qurating's 1B analysis set\footnote{https://huggingface.co/datasets/princeton-nlp/QuRatedPajama-1B\_tokens\_for\_analysis}. We then presented the top, median, and bottom ten documents based on Qurating's four criteria—writing style, facts and trivia, educational value, and required expertise—and DataMan's Overall Score with sum of the remaining criteria. All results are provided here\footnote{https://github.com/pengr/DataMan/blob/main/README.md}. Our conclusions are:
\textit{i)}-Higher Qurating criteria correlate with higher DataMan's Overall Scores for the top and bottom 10 documents, and vice versa.
\textit{ii)}-Median 10 documents show minimal changes in Qurating's criteria, yet their Overall Scores vary from 3 to 5, supporting that ``Pairwise rating struggles with minimal quality differences, while pointwise rating matches human judgment.''
\textit{iii)}-Rankings by Overall Score highlight top-quality documents, especially in STEM fields.
\textit{iv)}-The ``\emph{application\_domain}'' field accurately categorizes domains, aiding in domain-specific continue pre-training, as shown in Table~\ref{tab:domain_specific_results}.


\subsection{Training the DataMan Model} \label{sec:DataMan Rater Training}
After developing the prompt and rating method, we collected documents from both in-source and out-of-source of the SlimPajama corpus~\citep{cerebras2023slimpajama} to train DataMan.
By prompting the Super LLM, we obtained 14 quality ratings and domain types for each document, creating a fine-tuning dataset of 35,700 documents at a cost of \$13,858.
Documents were limited to 2,048 tokens (averaging 810 tokens), surpassing the [256, 512] token range of~\cite{wettig2024qurating} when handling sentences with broader length variations.
Subsequently, we fine-tuned the DataMan model with Qwen2-1.5B \citep{yang2024qwen2} using text generation loss. 
In Table \ref{tab:sft_model_eval}, the DataMan model achieves near 80\% average test accuracy across all criteria, with an \emph{Overall Score} test accuracy of 81.3\% and a \emph{domain recognition} test accuracy of 86\%.
Appendix~\ref{app:dataman_model} also provides details on the fine-tuning dataset statistics, average scores for each quality criterion per domain, correlation among quality criteria, three DataMan model versions, the chat prompt for text generation, the hyperparameter search, reports on inference accuracy, misclassification analysis and inference efficiency.

\vspace{-5pt}
\section{Managing data by DataMan}\label{sec:manage_data_by_dataman}
\vspace{-3pt}
In this section, we apply the DataMan model to manage data by selecting a high-quality, diverse document subset from the pre-training corpus.
Each document \( d_i \) in the corpus \( D \), with source \( s_i \), is annotated by the DataMan model with 14 quality ratings and domain types as \( \mathcal{L} = \{(l_i^{1}, \ldots, l_i^{C-1}, q)\} \), where \( q \) is the domain type. 
Given the source and domain distribution probabilities \( P(s) \) and \( P(q) \) respectively, we perform top-k (k is the selected subset size) sampling without replacement for each quality criterion across source and domain distribution, using the probability:
% \vspace{-3pt}
\begin{equation}~\label{eq:3}
P(d_i) = \frac{P(d_i \mid l^j, s, q) \cdot P(s,q)}{\sum_{d_j \in \text{top-k}(l^{j})} P(d_j \mid l^j, s, q) \cdot P(s,q)}, \text{and} \ P(d_i | l^j) = \frac{l_i^j}{\sum_{d_j \in D} l_i^j}.
\vspace{-5pt}
\end{equation}

This method maximizes sample representativeness based on quality rating while ensuring diversity in source and domain distributions, and sampling without replacement prevents duplicate data. 
To assess if the \emph{Overall Score} covers all criteria, we substitute top-k with uniform sampling based on fixed \emph{Overall Score} ratings. 
These techniques implicitly steer language modeling toward reward-weighted regression~\citep{peters2007reinforcement, korbak2023pretraining}, expanding maximum likelihood estimation loss with a data reward mechanism.