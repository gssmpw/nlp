\section{Related Work}
The development of object recognition, segmentation, and multimodal AI has been significantly shaped by pioneering models like AlexNet \cite{krizhevsky2012imagenet}, VGGNet \cite{simonyan2014very} and ResNet \cite{he2016deep}, which introduced deep architectures capable of capturing intricate visual patterns. VGGNet, known for its deep and simple structure, uses small convolution filters to achieve high accuracy, while ResNet introduced residual learning, enabling deeper networks without the vanishing gradient problem. These advancements in convolutional neural networks (CNNs) have laid the groundwork for more sophisticated models that are crucial for tasks such as object manipulation in complex environments like convenience stores.

Object detection has also evolved with the introduction of models such as Faster R-CNN \cite{ren2015faster} and YOLO \cite{redmon2016you}, both of which have significantly influenced the field. Faster R-CNN integrates region proposal networks with CNNs to enhance detection accuracy, making it a preferred choice for tasks requiring precision. In contrast, YOLO redefined object detection by framing it as a single regression problem, allowing for real-time performance. The progression of object detection models has been further supported by attention mechanisms, particularly through the Transformer architecture and its adaptation to visual tasks with Vision Transformers (ViT) \cite{dosovitskiy2021image}. ViT demonstrated that self-attention could effectively capture global dependencies in images, offering a new approach to image recognition that reduces reliance on convolutional operations.

The integration of language and vision has opened new possibilities in multimodal AI, with models like CLIP, GLIP 
\cite{yu2022glip}, and DINO \cite{caron2021emerging} playing a central role. CLIP, developed by OpenAI, showed the power of aligning text and image representations, enabling robust zero-shot learning across various vision tasks. Building on this, GLIP enhanced the grounding of language in visual contexts, making it more effective for object detection in multimodal scenarios. DINO and its extension, Grounding DINO \cite{li2023grounding}, explored self-supervised learning for visual representations, emphasizing the importance of grounding visual concepts in multimodal inputs. These advancements have significantly enhanced the capabilities of systems that require a deep understanding of both visual and textual information.

Segmentation, a crucial aspect of object manipulation, has advanced significantly with models like the SAM by Meta AI. SAM represents a major leap in segmentation technology, delivering high-quality results across a wide range of objects and scenes. Trained using a vast and diverse dataset, SAM enhances its ability to generalize across various environments. These advanced segmentation models are particularly relevant in environments like convenience stores, where precise object detection and manipulation are critical.

To ensure the reliability and interpretability of these complex systems, techniques like Grad-CAM \cite{selvaraju2017grad} have been developed to provide visual explanations of model decisions. Grad-CAM highlights the regions of an image that influence a model’s output, offering insights into the model’s decision-making process. This explainability is essential in multimodal AI systems, where understanding the model’s focus areas can lead to more trustworthy and effective applications.