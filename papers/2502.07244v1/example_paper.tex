%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{threeparttable}
\usepackage{multirow}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Linear Transformers as VAR Models}

\begin{document}

\twocolumn[
\icmltitle{Linear Transformers as VAR Models:  Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jiecheng Lu}{gatech}
\icmlauthor{Shihao Yang}{gatech}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{gatech}{Georgia Institute of Technology, Atlanta, US}

\icmlcorrespondingauthor{Jiecheng Lu}{jlu414@gatech.edu}
\icmlcorrespondingauthor{Shihao Yang}{shihao.yang@isye.gatech.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
Autoregressive attention-based time series forecasting (TSF) has drawn increasing interest, with mechanisms like linear attention sometimes outperforming vanilla attention. However, deeper Transformer architectures frequently misalign with autoregressive objectives, obscuring the underlying VAR structure embedded within linear attention and hindering their ability to capture the data generative processes in TSF. In this work, we first show that a single linear attention layer can be interpreted as a dynamic vector autoregressive (VAR) structure. We then explain that existing multi-layer Transformers have structural mismatches with the autoregressive forecasting objective, which impair interpretability and generalization ability. To address this, we show that by rearranging the MLP, attention, and input-output flow, multi-layer linear attention can also be aligned as a VAR model. Then, we propose Structural Aligned Mixture of VAR (SAMoVAR), a linear Transformer variant that integrates interpretable dynamic VAR weights for multivariate TSF. By aligning the Transformer architecture with autoregressive objectives, SAMoVAR delivers improved performance, interpretability, and computational efficiency, comparing to SOTA TSF models. The code implementation is available at this  \href{https://github.com/LJC-FVNR/Structural-Aligned-Mixture-of-VAR}{\underline{link}}.

\end{abstract}

\section{Introduction}
In recent years, autoregressive decoder-only Transformers have made significant strides \citep{vaswani2017attention, radford2018gpt}, powering Large Language Models (LLMs) \citep{gpt3, touvron2023llamaopenefficientfoundation} capable of handling complex sequential data. Their core mechanism, autoregressive self-attention, computes attention weights between the current token and all preceding tokens during prediction. However, using softmax to compute the \( N \times N \) attention map results in large \(O(N^2)\) time complexity as the sequence length grows. To address the efficiency bottleneck, researchers have developed efficient variants like Linear Transformers \citep{katharopoulos2020transformers, hua2022transformer}. By replacing softmax with a linearizable kernel, linear attention reduces complexity from \(O(N^2)\) to \(O(N)\) by maintaining a \(2d\)-dimensional hidden state instead of forming the full \(N \times N\) attention map \citep{ choromanski2021rethinking, hua2022transformer, sun2023retentive}. Although it often underperforms vanilla attention in complex tasks like NLP, studies show that in simpler tasks, such as time series forecasting (TSF), linear attention can outperform vanilla attention \citep{ patro2024simba, lu2024autoregressive, behrouz2024titans}.

Autoregressive modeling has a long history in time series forecasting. Traditional methods like ARIMA handle univariate series through autoregression, differencing, and moving averages \citep{winters1960forecasting, holt2004forecasting}, while Vector Autoregression (VAR) \citep{stock2001vector, zivot2006vector} extends this to multivariate settings by capturing cross-variable lag dependencies. Although widely used in fields like economics and climate due to their interpretability and theoretical guarantees \citep{burbidge1984testing, pretis2020econometric}, these models' linear assumptions and fixed lag orders limit their ability to capture complex patterns. With growing data scale and complexity, deep learning-based TSF models, especially attention-based approaches, have outperformed traditional AR/VAR methods \citep{li2019logtrans, zhou2021informer, nie2023time, liu2024itransformer}.

Previous research offers various perspectives on linear attention, viewing it as an RNN with linear state updates, a dynamic temporal projection, or fast weight programming \citep{katharopoulos2020transformers, schlag2021linear}. In this paper, we show that linear attention naturally contains a VAR structure. While a single-layer linear attention module can be directly interpreted as a VAR model, stacking multiple layers introduces structural mismatches with the time series generative process, reducing its effectiveness for TSF. We demonstrate that by reorganizing the input-output flow, multi-layer linear attention can fully align with a VAR model. Further, we propose Structural Aligned Mixture of VAR (SAMoVAR), which enhances linear Transformers for TSF, improving both accuracy and interpretability.

The main contributions are summarized as follows:

1) We provide a new perspective by interpreting a single-layer linear attention module as a VAR structure, where the \textit{key} represents the observation and the outer product of value and query forms dynamic VAR weights.

2) We analyze how the designs of existing Transformers lead to misalignments with a VAR model’s time series generative objective, including mismatched losses, inconsistent residual streams, and unbalanced observation weighting.

3) We show that properly arranging the input-output flow in a linear Transformer allows multi-layer linear attention to act as a expressive dynamic VAR model. With \(l\) layers, each past step’s influence on future steps is captured through a “temporal influence path” involving up to \(l-1\) intermediate nodes, enhancing interpretability.

4) Based on this aligned structure, we propose SAMoVAR for TSF. Experiments demonstrate that it surpasses previous TSF models in accuracy, interpretability, and efficiency.

\section{Background}

\subsection{Time Series Forecasting} 

Time Series Forecasting (TSF) aims to predict future values in a multivariate sequence \(\mathbf{S}\in\mathbb{R}^{L\times C}\), split into a historical part \(\mathbf{S}_I\in\mathbb{R}^{L_I\times C}\) and a future part \(\mathbf{S}_P\in\mathbb{R}^{L_P\times C}\), where \(L=L_I+L_P\) are the series lengths, and \(C\) is the number of channels. The task is to learn a function \(f: \mathbb{R}^{L_I\times C}\to\mathbb{R}^{L_P\times C}\) that generates \(\widehat{\mathbf{S}}_P=f(\mathbf{S}_I)\), given the input \(\mathbf{S}_I\).

\subsection{Preliminaries: Attention Mechanisms}


Previous studies have examined autoregressive attention mechanisms from various angles, emphasizing their common feature of dynamic weights \citep{katharopoulos2020transformers, hua2022transformer, sun2023retentive}. For an input sequence of length \( N \) and dimension \( d \), represented as \( \mathbf{X} \in \mathbb{R}^{N \times d} \), with each token denoted by \( \bm{x}_t \in \mathbb{R}^{1 \times d} \), a single-head autoregressive attention layer is formulated as:
\begin{equation}
\label{eq:attention_layer}
\begin{gathered}
\text{Attn}(\mathbf{X}) = \sigma \left( \mathbf{M} \odot (\mathbf{Q}\mathbf{K}^\top) \right)\mathbf{V} \mathbf{W}_o, \\ \text{with} \ \ \mathbf{Q},\mathbf{K},\mathbf{V} = \mathbf{X}\mathbf{W}_q,\mathbf{X}\mathbf{W}_k,\mathbf{X}\mathbf{W}_v, \\ 
\mathbf{X} := \mathbf{X} + \text{Attn}(\text{LN}(\mathbf{X}))
\end{gathered}
\end{equation}
Here, \( \mathbf{W}_q, \mathbf{W}_k, \mathbf{W}_v, \mathbf{W}_o \in \mathbb{R}^{d \times d} \) are the projection matrices for query, key, value, and output. The causal mask \( \mathbf{M} \in \mathbb{R}^{N \times N} \) ensures autoregressive behavior, with \( \mathbf{M}_{ij} = 1\{i \geq j\} - \infty \cdot 1\{i < j\} \), allowing only current and past positions. \( \text{Attn}(\cdot) \) and \( \text{LN}(\cdot) \) denote the attention and layer normalization functions. When \( \sigma \) is softmax, the mechanism becomes vanilla attention. Replacing \( \sigma \) with an identity mapping simplifies it to linear attention. Adding an MLP layer after the attention layer, as \( \mathbf{X} := \mathbf{X} + \text{MLP}(\text{LN}(\mathbf{X})) \), forms a standard autoregressive Transformer block. We will explore the attention from multiple perspectives (P$\cdot$). An attention layer dynamically computes a temporal mapping weight matrix \(\mathbf{Q}\mathbf{K}^\top \in \mathbb{R}^{N \times N}\) for a sequence of length \(N\). For each input step \(t\), it generates an attention map over all \(N\) tokens as dynamic weights. In vanilla attention, these weights are softmax-normalized to sum to 1. In autoregressive attention, a lower-triangular mask \(\mathbf{M}\) ensures each step only attends to positions up to \(t\). Thus, the attention layer functions as a variable-length dynamic linear layer on the input value sequence \(\mathbf{V}\)\citep{vaswani2017attention,katharopoulos2020transformers,yang2024gated}.

\textbf{P2: Recurrent Form and Autoregression}

Autoregressive attention can be viewed as a step-by-step generative process with a recurrent formulation. For the input \(\bm{x}_t\) at step \(t\), the output \(\bm{o}_t\) is: \(
\bm{o}_t = \frac{\sum_{i=1}^{t} \sigma(\bm{q}_t, \bm{k}_i) \bm{v}_i}{\sum_{i=1}^{t} \sigma(\bm{q}_t, \bm{k}_i)},
\) where \(\bm{q}_t, \bm{k}_t, \bm{v}_t \in \mathbb{R}^{1 \times d}\) are query, key, and value vectors at step \(t\). When \(\sigma(\bm{q}_t, \bm{k}_i) = \exp(\bm{q}_t \bm{k}_i^\top)\), this represents vanilla attention, relying on all previous keys \(\bm{k}_{\{1,\dots,t\}}\) and values \(\bm{v}_{\{1,\dots,t\}}\). If \(\sigma(\bm{q}_t, \bm{k}_i)\) is derived from a kernel feature map \(k(\bm{q}_t, \bm{k}_i) = \phi(\bm{q}_t) \phi(\bm{k}_i)^\top\), the computation is linearized as: \(
\bm{o}_t = \frac{\phi(\bm{q}_t) \sum_{i=1}^{t} \phi(\bm{k}_i)^\top \bm{v}_i}{\phi(\bm{q}_t) \sum_{i=1}^{t} \phi(\bm{k}_i)^\top}.
\) This avoids the full \(N \times N\) attention map by aggregating past information into a hidden state. Ignoring the denominator, the simplified form is: \(
\bm{o}_t = \bm{q}_t \sum_{i=1}^{t} \bm{k}_i^\top \bm{v}_i
\). Here, attention acts like an RNN with a 2D hidden state \(\bm{k}_i^\top \bm{v}_i \in \mathbb{R}^{d \times d}\) and identity state updates. Studies have shown comparable performance without normalization, so we use this simplified form \citep{zhai2021attention, mao-2022-fine, qin-etal-2022-devil, sun2023retentive, yang2024gated}. In this view, attention is a dynamic autoregressive model with shared weights \(w_{t,i} = \sigma(\bm{q}_t, \bm{k}_i)\) across all \(d\) channels: \(
\bm{o}_t = \sum_{i=1}^t w_{t,i} \bm{v}_i.
\)

\textbf{P3: Fast Weight Programming} Fast weight programming (FWP) refers to the process of dynamically determining a set of linear predictor weights for each step in the input sequence, i.e., \(\mathbf{W}_{\text{FWP},t} = g(\bm{x}_1,\dots,\bm{x}_t) \in \mathbb{R}^{d \times d}\). In linear attention, this is achieved using a summation aggregator to combine past weight information \citep{schlag2021linear}:
\(
\mathbf{W}_{\text{FWP},t} = \sum_{i=1}^{t} \phi(\bm{k}_i)^\top \bm{v}_i,
\)
which serves as a dynamic linear predictor for \(\bm{q}_t\). The final output at step \(t\) is then
\(
\bm{o}_t = \bm{q}_t \mathbf{W}_{\text{FWP},t}.
\)

\subsection{Linear Attention as VAR}
Recent TSF research shows that linear attention—without softmax—often outperforms vanilla attention \citep{patro2024simba, lu2024autoregressive, behrouz2024titans}. While it can be viewed as an RNN or a form of fast weight programming (FWP), these perspectives do not directly link to the autocorrelation or generative nature of TSF data. In this section, we demonstrate that linear attention naturally forms a dynamic VAR structure, making it well-suited for modeling TSF data generation.

\textbf{P4: Vector Autoregression} 
A classic Vector Autoregressive model \(\text{VAR}(p)\) with lag \(p\) uses parameter matrices \(\mathbf{A}_j \in \mathbb{R}^{d \times d}\) to model dependencies on \(p\) previous time steps:\[
\bm{y}_t^\top = \mathbf{A}_1 \bm{y}_{t-1}^\top + \mathbf{A}_2 \bm{y}_{t-2}^\top + \cdots + \mathbf{A}_p \bm{y}_{t-p}^\top + \bm{u}_t^\top,
\] where \(\bm{y}_j \in \mathbb{R}^{1 \times d}\) represents the observations and \(\bm{u}_t \in \mathbb{R}^{1 \times d}\) is the residual. In the RNN and FWP views, the parameter matrices depend directly on \(\bm{k}_i\) and \(\bm{v}_i\) and act on the query \(\bm{q}_t\) rather than sequentially applying to previous steps as in VAR. However, we can reformulate linear attention to reveal its VAR structure. Since \(\bm{q}_t \bm{k}_i^\top\) is a scalar, rearranging terms gives:\(
\bm{o}_t = \sum_{i=1}^{t} \bm{k}_i \bm{q}_t^\top \bm{v}_i.
\)
By transposing this expression, we obtain the VAR form of a single-layer linear attention mechanism:
\begin{equation}
\label{eq:linear_attention_as_var}
\bm{o}_t^\top = \sum_{i=1}^t \mathbf{A}_{t,i} \bm{k}_i^\top, \quad \mathbf{A}_{t,i} = \bm{v}_i^\top \bm{q}_t
\end{equation}
This defines a \(\text{VAR}(t)\) structure with dynamic weights, where the observations are \(\bm{k}_i^\top\) and the rank-1 weight matrices \(\mathbf{A}_{t,i} = \bm{v}_i^\top \bm{q}_t\) are dynamically generated for each step \(t\). Unlike RNNs or FWP, where weights propagate across steps, these matrices are independently generated at each time step. Thus, autoregressive linear attention forms its own \(\text{VAR}(\cdot)\) structure at each step, as shown in Figure \ref{fig:samovar}(a).

\section{Aligning the Objective of Autoregressive Attention with Autoregressive Forecasting}

\begin{figure*}[t]
%\vskip 0.05in
\begin{center}
\centerline{\includegraphics[width=0.8\textwidth]{samovar.pdf}}
\vskip -0.15in
\caption{Visualization of Key Concepts in SAMoVAR. The subfigures highlight different structural and conceptual elements of the model.}
\label{fig:samovar}
\end{center}
\vskip -0.35in
\end{figure*}

In this section, we show that while a \textbf{single linear attention layer} naturally exhibits a dynamic VAR structure, the design of current multi-layer Transformers diverges from the VAR training objective. As a result, these models lose the beneficial VAR properties for time series forecasting (TSF). 

Time series data naturally show temporal dependence. A classic VAR model captures both autocorrelation and cross-correlation in the data generation process, with weight matrices \(\mathbf{A}_j\) decoupling how past values influence future outcomes. Although standard attention and Transformers are effective for modeling complex sequence relationships (e.g., in NLP), their architecture conflicts with VAR’s goal of explicitly representing lag-based dependencies. We outline the key sources of this misalignment below.

\textbf{VAR Loss and Position Shifting} A VAR model is designed to directly capture the relationship between past observations and future values. Suppose we rewrite Eq. \eqref{eq:linear_attention_as_var} into a strict VAR model form:
\begin{equation}
\label{eq:linear_var}
\bm{k}_{t+1}^\top = \bm{o}_t^\top + \bm{u}_t^\top 
= \sum_{i=1}^t \underbrace{\bm{v}_i^\top \bm{q}_t}_{\mathbf{A}_{t,i}} \,\bm{k}_i^\top + \bm{u}_t^\top,
\end{equation}
where \(\bm{u}_t \in \mathbb{R}^{1 \times d}\) is the residual not explained by the dynamic VAR system. To align linear attention with a VAR model, minimizing \(\bm{u}_t\) should be part of the training objective. Adding a loss term for each layer’s residual \(\bm{k}_t - \bm{o}_{t-1}\) might partially achieve this but would conflict with the overall Transformer objective.

In a VAR model, the weights are learned to perform forward shift for each input position. With \(l\) attention layers, enforcing this VAR loss would result in \(l\) shifts, whereas the Transformer’s autoregressive objective only requires a single-step shift at the output. As a result, each layer would need to perform only a fractional shift to stay aligned.

\textbf{Residual Stream} 
Next, we focus on how each attention layer operates. In decoder-only Transformers, a common pre-normalization design \citep{radford2018gpt} includes a residual shortcut from input to output, with each block learning the difference between the current and the next step. The attention layers gather information from previous steps, gradually refining this difference and adding it to the shortcut.

If all attention layers are disabled, the model reduces to a local MLP predictor that maps the current input to the next-step output. With patch tokenization, where each token covers \(L_P\) time steps, the model effectively becomes an MLP-based TSF predictor mapping \(L_P\) input steps to \(L_P\) outputs. Adding an attention layer adjusts the input pattern using past tokens to better match the next-step difference. Recent studies of in-context learning \cite{zhang2023trained, akyurek2022learning} suggest that with more layers, the model can dynamically approximate predictors like gradient descent, ridge regression, or shallow MLPs. Thus, attention layers prioritize refining local predictors through context, rather than explicitly modeling step-by-step generation considering raw observations. 

\textbf{Input and Output}
Comparing the attention output in Eq. \eqref{eq:attention_layer} with the VAR output in Eq. \eqref{eq:linear_var}, we see that attention outputs must align with the residual shortcut space, while VAR outputs correspond to the key observations $\bm{k}$. VAR represent the data generation process, so their outputs should not be treated as residuals. However, by adjusting the weight matrix at each step, we can align the dynamic VAR structure of linear attention with the residual-based objective for transitioning observations to the next step. We call this a “key shortcut,” which adds an identity matrix to the dynamic VAR weights, guided by an indicator for the output step $t$.
\begin{equation}
\label{eq:var_residual}
\begin{gathered}
\bm{k}_{t+1}^\top 
= 
\bm{k}_{t}^\top + \bm{o}_t^\top + \bm{u}_t^\top 
= 
\sum_{i=1}^t \mathbf{A}_{t,i} \,\bm{k}_i^\top + \bm{u}_t^\top,
\\
\text{where} 
\quad 
\mathbf{A}_{t,i} = \bm{v}_i^\top \,\bm{q}_t + \mathbf{I} \cdot \mathbf{1}_{[i = t]}.
\end{gathered}
\end{equation}
However, in a pre-normalization setup, the attention layer only processes the transformed input \(\text{LN}(\bm{x}_t)\) and lacks direct access to the original signal \(\bm{x}_t\) needed to model the residual. As a result, it is not possible to establish a strict VAR recurrence involving \(\bm{o}_t^\top\) and the original signal, regardless of adjustments to \(\bm{k}_t\) or the weight matrices \(\mathbf{A}_{t,i}\).

\textbf{Balanced Weights of Observations} In a VAR model, all lag positions are initially treated equally, with their influence on future steps determined by learned weights, free from positional bias. In contrast, a multi-layer Transformer requires each token to perform two roles: (1) gather information via attention to predict its next step and (2) serve as context for future tokens. As layers deepen, accumulated residual updates cause the representation \( f_{\text{rep}}(x_1, \dots, x_i) \) to drift away from the original observation semantically. This drift complicates the VAR-like stepwise shift and leads to uneven weighting of original observations in a linear attention-based VAR framework.

\section{Structural Aligned Mixture of VAR}
We show that the misalignments between linear attention and VAR-based forecasting can be resolved by reorganizing the MLP and attention layers in a linear Transformer. By redesigning the input-output flow, we can \textbf{enable multi-layer linear attention to maintain a VAR structure}, improving its ability to model the generative processes of time series data. For a single linear attention layer in Eq. \eqref{eq:linear_attention_as_var}, the Transformer naturally aligns with the VAR objective for one-step shifting, as position-wise operations are confined within the layer. The VAR weights remain balanced across past lags when viewed in the original key observation space. To maintain this structure, the output and input signals must follow the same recursive equation, and the residual shortcut should share the same normalization as the attention layer’s key inputs, avoiding typical pre-normalization shortcuts seen in standard Transformers.

\subsection{Multi-layer Linear Attention as VAR} \label{multi_var}

Using a single linear attention layer preserves a clear VAR structure but limits the Transformer's expressive power. Each outer product weight \(\bm{v}_i^\top \bm{q}_t\) forms a rank-1 matrix, and unlike RNNs or fast weight programming, this VAR formulation cannot increase rank through timestep summation. Below, we show that when multiple linear attention layers are \textbf{stacked without MLP layers in between}, and \textbf{\(\bm{o}_t\) is directly fed into the next layer}, the attention layers can still function as a \textbf{dynamic VAR model}. This model uses the first layer’s key input \(\bm{k}^{(1)}_t\) as the observation, with explicit weight matrices that remain aligned with the autoregressive forecasting objective.

Let us denote the output of the first linear attention layer at step \(t\) by \(\bm{o}_t^{(1)}\) (omitting residuals, normalization, and setting \(\mathbf{W}_o = \mathbf{I}\)). In the single-head case:
\(
\bm{o}_t^{(1)\top} 
= 
\sum_{i=1}^t 
\bm{v}_i^{(1)\top} \bm{q}_t^{(1)} \bm{k}_i^{(1)\top},
\)
where \(\bm{q}_t^{(1)} = \bm{x}_t \mathbf{W}_q^{(1)},\ 
\bm{k}_i^{(1)} = \bm{x}_i \mathbf{W}_k^{(1)},\ 
\bm{v}_i^{(1)} = \bm{x}_i \mathbf{W}_v^{(1)}.\) Denote the first layer’s key input by \(\bm{k}_t^{(1)}\equiv \bm{k}_t\), and let \(\mathbf{B}_{t,j}^{(1)}\) be the weight matrix directly acting on \(\bm{k}_j\). Then:
\[
\bm{o}_t^{(1)\top} 
= 
\sum_{j=1}^t \mathbf{B}_{t,j}^{(1)} \bm{k}_j^\top, 
\quad
\mathbf{B}_{t,j}^{(1)} 
= 
\mathbf{A}_{t,j}^{(1)}
= 
\bm{v}_j^{(1)\top} \bm{q}_t^{(1)}.
\]
Now take \(\bm{o}_t^{(1)}\) as input to the second layer. The second-layer output becomes:
\(
\bm{o}_t^{(2)\top}
=
\sum_{i=1}^t 
\underbrace{\bigl(\bm{v}_i^{(2)\top} \bm{q}_t^{(2)}\bigr)}_{\mathbf{A}_{t,i}^{(2)}}
\bm{k}_i^{(2)\top},
\)
where  
\(
\bm{q}_t^{(2)} = \bm{o}_t^{(1)}\mathbf{W}_q^{(2)}, \ 
\bm{k}_i^{(2)} = \bm{o}_i^{(1)}\mathbf{W}_k^{(2)}, \ 
\bm{v}_i^{(2)} = \bm{o}_i^{(1)}\mathbf{W}_v^{(2)}.
\)
After expansions and rearrangements, this can be rewritten in a VAR-like form in terms of the original observation \(\bm{k}_t\):
\[
\bm{o}_t^{(2)\top}
= 
\sum_{j=1}^t \mathbf{B}_{t,j}^{(2)}\,\bm{k}_j^\top, \mathbf{B}_{t,j}^{(2)}
=
\sum_{i=j}^t 
\underbrace{\bigl(\bm{v}_i^{(2)\top}\,\bm{q}_t^{(2)}\bigr)}_{\mathbf{A}_{t,i}^{(2)}}\mathbf{W}_k^{(2)\top}\,\mathbf{B}_{i,j}^{(1)}
\]
Thus, for \(l\) stacked linear attention layers, the final weight \(\mathbf{B}_{t,j}^{(l)}\) for the original key observation \(\bm{k}_j\) at step \(t\) is:
\begin{equation}
\mathbf{B}_{t,j}^{(l)}
=
\sum_{i=j}^t 
\underbrace{\bigl(\bm{v}_i^{(l)\top} \,\bm{q}_t^{(l)}\bigr)}_{\mathbf{A}_{t,i}^{(l)}}
\;\mathbf{W}_k^{(l)\top}\,\mathbf{B}_{i,j}^{(l-1)}.
\end{equation}
This shows how stacking multiple linear attention layers—without MLP layers in between—produces a expressive dynamic VAR structure for autoregressive forecasting, using the first layer’s key input as the observation.

\textbf{Temporal Influence Path} The dynamic weight matrix at step \(j\) for step \(t\), \(\mathbf{B}_{t,j}^{(l)}\), is derived by applying the layer-\(l\) weights \(\mathbf{A}_{t,i}^{(l)}\) and \(\mathbf{W}_k^{(l)\top}\) to the previous layer’s \(\mathbf{B}_{t,j}^{(l-1)}\) across all intermediate steps \(i\). Repeated multiplication of different \(\mathbf{W}_k\) matrices can lead to numerical instability. To address this, we replace the key projection with the identity matrix \(\mathbf{I}\). Under this setup, each term in the summation can be interpreted as a modification (amplification or attenuation) of the previous layer’s dynamic weights across the intermediate points after step \(j\) to $t$. The iterative factor within each path can be expressed as:
\begin{align*}
\mathbf{P}_{t,j,\{i_1, \cdots, i_{l-1}\}}^{(l)} &= \mathbf{A}_{t,i_1}^{(l)} \mathbf{A}_{i_1,i_2}^{(l-1)} \cdots \mathbf{A}_{i_{l-1},j}^{(1)} \\
&= \bm{v}_{i_1}^{(l)\top} \bm{q}_t^{(l)} \,\bm{v}_{i_2}^{(l-1)\top} \bm{q}_{i_1}^{(l-1)} \;\cdots\; \bm{v}_{j}^{(1)\top} \bm{q}_{i_{l-1}}^{(1)}
\end{align*}
where \( t \ge i_1 \ge i_2 \ge \cdots \ge i_{l-1} \ge j \). This describes a temporal influence path from step \(j\) to \(t\) involving \(l-1\) intermediate timesteps. All possible combinations of intermediate steps form the complete set of influence paths contributing to \(\mathbf{B}_{t,j}^{(l)}\). The number of such paths is given by the binomial coefficient \(
n^{\text{path}}_{t,j,l} \;=\; \binom{\,(t-j) + (l-1)\,}{\,l-1\,}.
\). Because each path representa a rank-1 matrix, the maximum rank of \(\mathbf{B}_{t,j}^{(l)}\) is \(n^{\text{path}}_{t,j,l}\). Each path’s scale is controlled by a series of dot-product scalars, e.g., \((\bm{q}_{i_1}^{(l-1)} \bm{v}_{i_3}^{(l-2)\top})\). Notably, positions farther from the output step \(t\) have more influence paths summed, reflecting the dynamic VAR structure’s capacity to capture complex long-term dependencies.

\textbf{Robust Path Pruning} To maintain numerical stability, we must prevent exploding values in each path’s multiplicative chain. Additionally, with distant timesteps, the large number of possible paths can increase weight variance, making it essential for the model to prune unimportant paths. In this setup, any path with a query-key dot product of zero is effectively pruned.

a) Controlling Exploding Values. We use root mean square layer normalization (RMSNorm) on \(\bm{q}\) and \(\bm{v}\) vectors to prevent their norms from growing excessively. The weight matrices \(\mathbf{W}_q\) and \(\mathbf{W}_v\) are initialized with low variance, ensuring paths start at small scales, especially those with many intermediate points.  b) Passive Pruning of Distant Paths. Multi-heads with a sufficiently large dimension \(d\) increases the chance that \(\bm{q}_t\) and \(\bm{v}_i\) become orthogonal, resulting in zero dot products that naturally prune unnecessary paths.

Since each layer’s output \(\bm{o}_i\) aggregates multiple influence paths, reusing \(\bm{o}_i\) directly to form the next layer’s \(\bm{q}\) and \(\bm{v}\) can further complicate control over numerical stability. In the structure above, all the \(\bm{q}\) and \(\bm{v}\) are used only to generate each layer’s dynamic VAR weights. Hence, to maintain stability, we choose to compute \(\bm{q}_t^{(l)}\) and \(\bm{v}_i^{(l)}\) directly from the original first-layer input signals. Specifically, let \(\bm{k}_i^{(1)} = \bm{x}_i^{(1)}\). For the \(l\)-th attention layer, we parametrize:
\(
\bm{q}_t^{(l)} = \mathrm{RMSNorm}^{(l)}(\bm{x}_t^{(1)}\,\mathbf{W}_q^{(l)}), 
\bm{v}_i^{(l)} = \mathrm{RMSNorm}^{(l)}(\bm{x}_i^{(1)}\,\mathbf{W}_v^{(l)}),
\bm{k}_i^{(l)} = \bm{o}_i^{(l-1)} \mathbf{I}.
\)
In this setup, the total weight at each position is a combination of multi-rank path matrices. Gradient updates amplify the ranks of important paths while suppressing less significant ones, implicitly applying a low-rank regularization.

\textbf{Key Shortcut and Mixture of VAR \footnote{Please note that even though we use the name "Mixture of VAR," its final form remains an integrated and complete dynamic VAR model. The "mixture" aspect is reflected in the traversal and combination of all possible temporal influence paths.}} 
To address instability from initializing all paths with small weights, we introduce a key shortcut by adding \(\mathbf{I}\) to \(\mathbf{B}_{t,j}^{(l)}\) when \(t = j\), as described in Eq. \eqref{eq:var_residual}. This effectively incorporates a temporal influence path for \(l = 0\) at each step. Additionally, instead of relying solely on the final layer’s output \(\bm{o}_t^{(l)}\), we aggregate outputs from all attention layers. This creates a mixture of VAR parameters, with the final output expressed as:
\[
\bm{o}_t^{(\text{final})\top} 
= 
\sum_{j=1}^t \mathbf{C}_{t,j} \bm{x}_j^{(1)\top}, 
\ 
\mathbf{C}_{t,j} 
= 
\sum_{m=1}^l \mathbf{B}_{t,j}^{(m)} + \mathbf{I} \cdot \mathbf{1}_{[i = t]}.
\]
Here, each \( \mathbf{B}_{t,j}^{(m)} \) represents the combination of all temporal influence paths from \( j \) to \( t \) using up to \( m-1 \) intermediate points. Consequently, \(\mathbf{C}_{t,j}\) contains all paths from \(j\) to \(t\) with up to \(l-1\) intermediate points, with a total of \(\sum_{m=1}^l n^{\text{path}}_{t,j,m} + 1\) paths (as illustrated in Figure \ref{fig:samovar}(b)).

\textbf{Structural VAR} A classic VAR model can be generalized into a Structural VAR (SVAR) \cite{rubio2010structural,primiceri2005time} by introducing an invertible matrix \(\mathbf{D} \in \mathbb{R}^{d \times d}\): \(
\mathbf{D}\bm{y}_t^\top = \sum_{i=1}^p \mathbf{A}_i \bm{y}_{t-i}^\top + \bm{\epsilon}_t^\top,
\) where \(\mathbf{D}\) captures instantaneous relationships among variables but requires extra constraints for proper identification. The structural shocks \(\bm{\epsilon}_t\) are derived by decomposing residuals \(\bm{u}_t\) using \(\mathbf{D}\). Since interpreting individual channel weights in the learned VAR observations \(\bm{k}_i\) can be complex, we use this SVAR form mainly to enhance representational capacity and better interpret residuals. The structural form of our multi-layer linear attention VAR is given as:
\[
\bm{x}_{t+1}^{(1)\top} = \bm{o}_t^{(\text{final})\top} + \bm{\epsilon}_t^\top = \sum_{j=1}^t \mathbf{D}^{-1} \mathbf{C}_{t,j} \bm{x}_j^{(1)\top} + \bm{\epsilon}_t^\top.
\]
where \(\mathbf{D}^{-1}\) can be viewed as a shared output projection \(\mathbf{W}_o^\top\) across \(l\) attention layers. We parameterize \(\mathbf{W}_o^\top\) via a learnable LU factorization: a lower triangular matrix \(\mathbf{L}\) (with diagonal fixed to 1) and an upper triangular matrix \(\mathbf{U}\) (diagonal activated by softplus) are multiplied to form \(\mathbf{D}\), from which we derive its inverse.

\subsection{SAMoVAR Transformer}
The aligned multi-layer linear attention module described above preserves a valid VAR structure, resolving misalignments in standard linear attention related to training objectives, data generation, input/output spaces, and autoregressive forecasting. Building on this, we introduce SAMoVAR (Structural Aligned Mixture of VAR), which reorganizes MLP and linear attention layers, serving as a drop-in replacement for standard linear Transformers in time series forecasting (TSF), as shown in Figures \ref{fig:samovar} (c) and (d).

A $l$-layer SAMoVAR Transformer includes: a) MLP Component:
Consists of \(l\) MLP layers that learn optimal representations for VAR observations. After MLP processing, layer normalization ensures VAR outputs align with the transformed input signals. b) SAMoVAR Attention:  
Comprises \(l\) linear attention layers, parameterized with the robust path pruning methods discussed earlier. A unified residual shortcut across all layers stabilizes training. The overall architecture is shown in Figures \ref{fig:samovar} (d) and (e).

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[trim={0 0 100 0},clip,width=1\columnwidth]{tokenization.pdf}}
\vskip -0.1in
\caption{Illustration of the ARX tokenization, where we use $s_{t}^j$ to represent the $t$-th patch token of series $j$, $\mathbf{S}_I^{[i:i+L_P,j]}$.}
\label{fig:arx_tokenization}
\end{center}
\vskip -0.4in
\end{figure}

\textbf{Patch-based ARX Tokenization}
Prior research highlights the importance of preserving univariate dependencies for effective multivariate time series forecasting (TSF) \citep{zeng2022dlinear,nie2023time,lu2024cats}. For multivariate inputs \(\mathbf{S}_I\), we adopt an autoregressive (ARX) tokenization strategy, which captures univariate relationships while treating other series as exogenous inputs to model multivariate dependencies. We partition a time series of length \(L_I\) into non-overlapping patches of size \(L_P\). If needed, zero-padding \(P\) is added so that \(L_I + P\) is divisible by \(L_P\), resulting in \(N = \frac{L_I + P}{L_P}\) patches \(\mathbf{S}_I^{[i:i+L_P]} \in \mathbb{R}^{L_P \times C}\). For each series \(j\), a linear projection \(\mathbf{W}_{\text{tok}} \in \mathbb{R}^{d \times L_P}\) transforms its patch \(\mathbf{S}_I^{[i:i+L_P,j]}\) into an autoregressive token \((\mathbf{W}_{\text{tok}}\,\mathbf{S}_I^{[i:i+L_P,j]})^\top \in \mathbb{R}^{1 \times d}\). This token is used to predict the next \(L_P\) steps for series \(j\), which can be viewed as a PatchTST-style tokenization \citep{nie2023time} with an autoregressive loss.

Inspired by vector autoregressive with exogenous variables (VARX), \(
\bm{y}_t = \sum_{m=1}^p \mathbf{A}_m \bm{y}_{t-m} + \sum_{n=0}^q \mathbf{B}_n \bm{e}_{t-n} + \bm{u}_t,
\)
where \(\bm{e}_{t-n}\) represents exogenous factors, we model all other series as exogenous when forecasting series \(j\). Specifically, a linear projection \(W_{\text{ex}} \in \mathbb{R}^{C \times C}\) mixes each channel independently to generate the exogenous token \(
\left(W_{\text{tok}} \,\mathbf{S}_I^{[i:i+L_P,:]} \,W_{\text{ex}}^{[:,j]}\right)^\top \in \mathbb{R}^{1 \times d}.
\) As shown in Figure \ref{fig:arx_tokenization}, we combine autoregressive and exogenous tokens along the sequence dimension to form the input tokens \(\mathcal{X}_{\text{input}} \in \mathbb{R}^{C \times 2N \times d}\). The channel dimension is treated as part of the batch for independent computation, with each exogenous token placed before its corresponding target token. Trainable position embeddings based on token positions and channel indices are added. The SAMoVAR Transformer processes the sequence, and the outputs corresponding to target tokens are projected using \(\mathbf{W}_{\text{out}} \in \mathbb{R}^{L_P \times d}\) to generate the next-step ARX predictions.

\section{Experimental Results}

\begin{figure*}[!t]
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=5.5cm]{fig/Combined_Visualization_Obs.pdf}
        \label{fig:synthetic_obs}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=5.5cm]{fig/Combined_Visualization_Attn.pdf}
        \label{fig:synthetic_attn}
    \end{minipage}
    \vskip -0.2in
    \caption{Visualization of the validation datapoint and model weights for the synthetic VAR task. See Section \ref{sec:syn} for more details.}
    \label{fig:synthetic}
    \vskip -0.1in
\end{figure*}

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{fig/train_val_loss_combined.pdf}}
\vskip -0.1in
\caption{Visualization of the loss curves for synthetic VAR tasks.}
\label{fig:synthetic_loss}
\end{center}
\vskip -0.3in
\end{figure}

\begin{figure}[!t]
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.3cm]{fig/temporal_influence_paths_variable0_cropped.pdf}
        \label{fig:synthetic_tip_0}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.3cm]{fig/temporal_influence_paths_variable1_cropped.pdf}
        \label{fig:synthetic_tip_1}
    \end{minipage}
    \vskip -0.12in
    \caption{Visualization of the 2 temporal influence paths from step 124 to step 128 for the two series in the datapoint shown in Fig. \ref{fig:synthetic}, where even-numbered steps represent endogenous tokens and odd-numbered steps represent exogenous tokens.}
    \label{fig:synthetic_tip}
    \vskip -0.2in
\end{figure}

\subsection{Synthetic Tasks} \label{sec:syn}

\textbf{Model Setup} We use the SAMoVAR architecture described in Fig. \ref{fig:samovar}(d) along with the ARX tokenization from Fig. \ref{fig:arx_tokenization} to train our TSF models. Additionally, we construct a baseline model (LinTrans) based on the classic linear Transformer structure shown in Figure \ref{fig:samovar}(c) to highlight the improvements of SAMoVAR. To demonstrate the impact of dynamic VAR weights, we replace SAMoVAR’s mixture of VAR modules with a fixed-weight VAR layer (FixedVAR).

\textbf{VAR Generalization} To test SAMoVAR’s ability to learn and generalize to the underlying data generation process, we generate training and test data using random VAR(\(p\)) models with varying lag orders. For training, \( p \in \{1, 2, 3\} \) with coefficients between -0.5 and 0.5. For validation, \( p \in \{3, 4, 5\} \) with coefficients between -0.25 and 0.25 to evaluate extrapolation. The input length is \( L_I = 64 \), output length \( L_P = 1 \), and \( C = 2 \) series, with ARX tokenization. We use 3 layers and 2 attention heads. As shown in Fig. \ref{fig:synthetic_loss}, during training, FixedVAR effectively memorizes the input structure but struggles in validation. In contrast, SAMoVAR’s dynamic VAR inference generalizes well and significantly outperforms FixedVAR. LinTrans, which does not explicitly model the TSF process, performs worse in both phases.

\textbf{Explainability} The left side of Fig. \ref{fig:synthetic} shows the original VAR weights, cumulative temporal contributions, and raw observations for a validation datapoint. The middle part compares the averaged attention maps of SAMoVAR and LinTrans. The upper right visualizes SAMoVAR’s final output contributions using \( \mathbf{W}_{\text{out}} \mathbf{D}^{-1} \mathbf{C}_{t,j} \mathbf{W}_{\text{out}}^\top \) across channels. The lower right shows LinTrans’s final-row attention map after reordering. SAMoVAR’s VAR-based attention better aligns well with the true contribution heatmap, providing more interpretable results than LinTrans.

\textbf{Temporal Influence Path} Figure \ref{fig:synthetic_tip} highlights how visualized paths reveal intermediary effects between time steps. The displayed values on the edges represent the averaged weights from the path matrix \(\mathbf{P}\). The top section shows Series 1 (S1) and the bottom, Series 2 (S2). From the original VAR weights, we know S2 has a stronger influence on S1 than vice versa. Correspondingly, in the top section, paths passing through exogenous points (marked “Ex”) show higher weights, while in the bottom section, these paths are weaker. This visualization effectively uncovers the transmission dynamics within the VAR structure, enhancing interpretability of TSF results.

\begin{table}[tb]
%\renewcommand{\arraystretch}{0.9}
\vskip -0.1in
\caption{Summary of Multivariate TSF Results. Averaged test set MSE are reported. See Table \ref{tab:full_main_results} for the original results.}
\label{mainresult}
\centering
\begin{threeparttable}
\begin{small}
\setlength{\tabcolsep}{2pt}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|ccc|cccccc}
\toprule
Model & SAMoVAR & LinTrans & FixedVAR & CATS & iTransformer & FITS & PatchTST & Dlinear & EncFormer 
\\
\midrule
Weather & \textbf{0.214} & 0.217 & 0.247 & \underline{0.216} & 0.232 & 0.222 & 0.221 & 0.233 & 0.251 \\
Solar & \textbf{0.184} & \underline{0.189} & 0.430 & 0.206 & 0.219 & 0.209 & 0.202 & 0.216 & 0.212 \\
ETTh1 & \textbf{0.401} & 0.419 & 0.564 & \underline{0.408} & 0.454 & 0.440 & 0.413 & 0.422 & 0.906 \\
ETTh2 & \underline{0.324} & 0.346 & 0.391 & \textbf{0.320} & 0.374 & 0.354 & 0.330 & 0.426 & 0.877 \\
ETTm1 & \textbf{0.339} & 0.346 & 0.519 & \underline{0.345} & 0.373 & 0.354 & 0.346 & 0.347 & 0.735 \\
ETTm2 & \textbf{0.240} & \underline{0.243} & 0.278 & \underline{0.243} & 0.265 & 0.247 & 0.247 & 0.252 & 0.576 \\
ECL & \textbf{0.151} & 0.166 & 0.345 & \textbf{0.151} & 0.170 & 0.167 & \underline{0.159} & 0.165 & 0.664 \\
Traffic & \underline{0.391} & 0.438 & 0.717 & \textbf{0.385} & 0.414 & 0.418 & \underline{0.391} & 0.431 & 0.824 \\
PEMS03 & \textbf{0.150} & \underline{0.188} & 0.375 & 0.225 & 0.212 & 0.234 & 0.230 & 0.254 & 0.443 \\
PEMS04 & \textbf{0.102} & \underline{0.136} & 0.404 & 0.184 & 0.171 & 0.256 & 0.222 & 0.246 & 0.377 \\
PEMS08 & \textbf{0.234} & \underline{0.261} & 0.674 & 0.359 & 0.271 & 0.296 & 0.290 & 0.357 & 0.681 \\
\midrule
AvgRank & \textbf{1.41} & 3.41 & 8.16 & \underline{2.86} & 5.43 & 5.20 & 4.00 & 5.82 & 8.30 \\
\#Top1 & \textbf{29} & 3 & 0 & \underline{9} & 1 & 0 & 2 & 1 & 0 \\

\bottomrule
\end{tabular}
}
\end{small}
\end{threeparttable}
\vspace{-12pt}
\vskip -0.1in
\end{table}

\begin{table}[tb]
%\renewcommand{\arraystretch}{0.9}
\caption{Summary of Ablation Study Results. Averaged test set MSE are reported. See Table \ref{tab:ablation1}, \ref{tab:ablation2}, \ref{tab:ablation3}, \ref{tab:ablation4} for the original results.}
\label{ablation}
\centering
\begin{threeparttable}
\begin{small}
\setlength{\tabcolsep}{3pt}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{Exp} & \multirow{2}{*}{SAMoVAR} & w/  & w/o  & \multicolumn{1}{c|}{w/o} & Heads=4 & Heads=8 & Heads=16 & Heads=32 \\
    &         &  $\mathbf{W}_k$ & $\mathbf{D}^{-1}$ & \multicolumn{1}{c|}{QV Norm} & dim=16 & dim=8 & dim=4 & dim=2 \\
\midrule
ETTh1 & 0.401 & 0.413 & 0.409 & \multicolumn{1}{c|}{0.421} & 0.401 & 0.406 & 0.412 & 0.413 \\
ETTm1 & 0.339 & 0.346 & 0.344 & \multicolumn{1}{c|}{0.350} & 0.339 & 0.342 & 0.343 & 0.344 \\
\midrule
Exp & $l=1$ & $l=2$ & $l=3$ & $l=4$ & $l=5$ & $l=6$ & $l=7$ & $l=8$ \\
\midrule
ETTh1 & 0.420 & 0.411 & 0.401 & 0.404 & 0.413 & 0.414 & 0.408 & 0.410 \\
ETTm1 & 0.346 & 0.342 & 0.339 & 0.345 & 0.346 & 0.348 & 0.349 & 0.351 \\

\bottomrule
\end{tabular}
}
\end{small}
\end{threeparttable}
\vspace{-12pt}
\end{table}


\subsection{Multivariate TSF}

We conducted comprehensive experiments on 12 widely-used TSF datasets, including Weather, Solar, Electricity (ECL), ETTs, Traffic, and PEMS \footnote{Note: Due to baseline models failing to train on PEMS07 when using batch size = 1 and $L_I=4096$, this dataset is excluded from the main text, as described in the fair comparison paragraph.}. See \S \ref{ap:dataset} for detailed descriptions of the datasets. Detailed hyperparameter settings and implementation details can be found in \S \ref{ap:hyperparameter_implementation}.

For SAMoVAR, LinTrans, and FixedVAR, we used $l=3$ Transformer layers, with the hidden dimension determined empirically as $d=32 \lfloor \sqrt{C} \rfloor$. The number of attention heads was set to $d/16$, ensuring that the dimension per head was 16. During testing, we predicted the next $L_P$ time steps corresponding to the last input token, following the same approach as the baselines for consistency.

\textbf{Baselines}  
In addition to SAMoVAR, LinTrans, and FixedVAR, we introduced five recent state-of-the-art baselines: CATS \citep{lu2024cats}, iTransformer \citep{liu2024itransformer}, FITS \citep{xu2024fits}, PatchTST \citep{nie2023time}, and DLinear \citep{zeng2022dlinear}. We also included an encoder-only vanilla Transformer, named Encformer, as a comparison to autoregressive Transformers. Encformer uses the same tokenization method as Autoformer \citep{wu2022autoformer} and Informer \citep{zhou2021informer}.

\textbf{Fair Comparison}  
All baseline models were trained under the same conditions with input lengths \( L_I \in \{512, 1024, 2048, 4096\} \), and the best performance was reported. This approach may yield stronger results than the original papers, ensuring a rigorous and fair comparison. For the three VAR-based models, we used consistent settings \((L_I: L_P) = (1024, 96), (2048, 192), (2048, 336), (4096, 720)\) to ensure appropriate ARX tokenization.

\textbf{Main Results}  
As shown in Table \ref{mainresult}, SAMoVAR consistently outperformed other models across all datasets, with a significantly higher average ranking and top-1 count. Notably, on datasets like Solar and PEMS, which contain many series with stable long-term patterns, SAMoVAR achieved over 30\% improvement compared to previous models. This shows the significant benefit of incorporating dynamic VAR structures and alignment when modeling complex TSF data with stable generative processes. Furthermore, although linear Transformers did not fully align with the autoregressive forecasting targets, they still outperformed most baseline models, indicating their potential in TSF.

\textbf{Ablation Studies}  
In Table \ref{ablation}, we present ablation studies to validate the effectiveness of components within SAMoVAR:  1) Reintroducing key projection weights: Based on our analysis in \S \ref{multi_var}, introducing $\mathbf{W}_k$ negatively impacts the scaling control of temporal influence paths and increases numerical instability due to the additional weight matrices. The results show a significant performance drop and training instability when $\mathbf{W}_k$ is added. 2) Removing the inverse matrix $\mathbf{D}^{-1}$: This matrix, corresponding to $\mathbf{W}_o$, plays a critical role in controlling the output space. Without it, performance degrades, as expected. 3) Removing RMSNorm from queries and values: As discussed in \S \ref{multi_var}, norm control over queries and values is essential for learning effective VAR path weights. Removing RMSNorm significantly degrades performance and causes training issues. 4) Increasing the number of attention heads: When the hidden dimension is fixed, using more attention heads reduces the dimension per head and passively disables more influence paths during initialization, leading to performance degradation. This also reduces the parameter capacity of dynamic VAR weights, further hurting performance. 5) Varying the number of Transformer layers $l$: The layer depth $l$ determines the number of intermediate points in the temporal influence paths included in the final VAR weights. The results show that $l=1$ yields the worst performance due to the lack of intermediate points. For real-world TSF datasets, $l=3$ or $l=4$ (2 or 3 intermediate points) is sufficient for good performance, while larger $l$ values lead to overfitting risks.

\textbf{Computational Costs} SAMoVAR introduces no additional computational overhead compared to the vanilla linear Transformer and even reduces the key projection step. This gives it efficiency advantages over other TSF models. Detailed comparisons are shown in Table \ref{tab:comp_cost}.

\section{Conclusion and Limitation}
This work bridges the gap between linear attention Transformers and VAR models for time series forecasting. We demonstrate that single-layer linear attention inherently captures dynamic VAR structures, while standard multi-layer architectures misalign with autoregressive objectives. By structurally aligning the input-output flow and MLP layers, we propose SAMoVAR, a multi-layer linear Transformer that integrates dynamic VAR weights through temporal influence paths. SAMoVAR achieves superior accuracy, interpretability, and efficiency compared to state-of-the-art models across synthetic and real-world benchmarks. As for limitation, we have not yet tested larger SAMoVAR models on large-scale general TSF tasks to evaluate their potential as foundation models. Additionally, we have not explored applying SAMoVAR to general sequence modeling tasks to assess whether the learned dynamic VAR weights are effective beyond TSF tasks.



% \section*{Impact Statement}

% This research introduces a novel approach to time series forecasting, advancing the accuracy and interpretability of attention-based time series models. By enabling better-informed decisions in fields such as economics and healthcare, the method demonstrates broad practical value. While its overall societal impact leans toward positive outcomes, the use of this technology in sensitive areas requires thoughtful management and ethical oversight to minimize potential risks and unintended consequences.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}

\subsection{Workflow of the SAMoVAR Attention Module}

We present the details of the attention module in the SAMoVAR Transformer, as shown in Algorithm \ref{alg:samovar}.

\begin{algorithm}[hbt]
   \caption{SAMoVAR Attention Module}
   \label{alg:samovar}
\begin{algorithmic}
   \STATE {\bfseries Input:} Sequence $X \in \mathbb{R}^{B \times L \times D}$, layers $L_{\text{attn}}$, head count $H$, head dimension $d = D / H$, query projection $\mathbf{W}_q^{(l)} \in \mathbb{R}^{D \times D}$, value projection $\mathbf{W}_v^{(l)} \in \mathbb{R}^{D \times D}$, invertible matrix $\mathbf{D} \in \mathbb{R}^{H \times d \times d}$, dropout rate $p$
   \STATE {\bfseries Output:} Processed sequence $\widetilde{X} \in \mathbb{R}^{B \times L \times D}$
   
   \STATE $B, L, D \gets \text{shape}(X)$
   \STATE $X_{\text{orig}} \gets \text{clone}(X)$ \COMMENT{Store original input for separate Q, V computation}
   \STATE Initialize $\widetilde{X} \gets X $
   \STATE Generate invertible matrices $\mathbf{D}$ using LU factorization

   \FOR{$l=1$ {\bfseries to} $L_{\text{attn}}$}
       \STATE {\bfseries Compute Query and Value Projections:}
       \STATE $Q^{(l)} \gets \text{reshape}(\mathbf{W}_q^{(l)} X_{\text{orig}}, B, L, H, d)$ \COMMENT{$Q^{(l)} \in \mathbb{R}^{B \times L \times H \times d}$}
       \STATE $V^{(l)} \gets \text{reshape}(\mathbf{W}_v^{(l)} X_{\text{orig}}, B, L, H, d)$ \COMMENT{$V^{(l)} \in \mathbb{R}^{B \times L \times H \times d}$}
       \STATE $Q^{(l)} \gets \text{RMSNorm}(Q^{(l)})$, $V^{(l)} \gets \text{RMSNorm}(V^{(l)})$
       
       \STATE {\bfseries Compute Keys from Input:}
       \STATE $K \gets \text{reshape}(X, B, L, H, d)$ \COMMENT{$K \in \mathbb{R}^{B \times L \times H \times d}$}
       
       \STATE {\bfseries Compute Cumulative Fast-Weight Updates:}
       \STATE $W \gets \text{cumsum}(K \otimes V^{(l)}, \text{dim}=1)$ \COMMENT{$W \in \mathbb{R}^{B \times L \times H \times d \times d}$}
       
       \STATE {\bfseries Compute Output using $Q^{(l)}$:}
       \STATE $Y \gets Q^{(l)} \otimes W$ \COMMENT{$Y \in \mathbb{R}^{B \times L \times H \times d}$}
       
       \STATE {\bfseries Apply Dropout and Mix Output via Structural Matrix $\mathbf{D}$:}
       \STATE $Y \gets \text{dropout}(Y, p)$
       \STATE $Y_{\text{transformed}} \gets \text{einsum}('blhd,hde->blhe', Y, \mathbf{D}^{-1})$ \COMMENT{Apply per-head invertible transformation}
       \STATE $Y_{\text{transformed}} \gets \text{reshape}(Y_{\text{transformed}}, B, L, D)$ \COMMENT{Reshape back to $\mathbb{R}^{B \times L \times D}$}
       \STATE $\widetilde{X} \gets \widetilde{X} + Y_{\text{transformed}}$
       
       \STATE {\bfseries Update Input for Next Layer:}
       \STATE $X \gets Y$
   \ENDFOR
   
   \STATE {\bfseries Return:} $\widetilde{X} \in \mathbb{R}^{B \times L \times D}$
\end{algorithmic}
\end{algorithm}


\subsection{Experimental Datasets}\label{ap:dataset}

Our multivariate time series forecasting experiments employ twelve real-world benchmark datasets. The original dataset names and their key details are summarized as follows:

\textbf{Weather Dataset}\footnote{\url{https://www.bgc-jena.mpg.de/wetter/}}\citep{wu2022autoformer}  
This dataset records 21 meteorological indicators (e.g., temperature, humidity) at 10-minute intervals throughout 2020, collected from the weather station of the Max Planck Institute for Biogeochemistry in Germany.

\textbf{Solar Dataset}\footnote{\url{http://www.nrel.gov/grid/solar-power-data.html}}\citep{lai2018modeling}  
Comprising solar power generation data from 137 photovoltaic plants, this dataset captures energy production values sampled every 10 minutes during 2006.

\textbf{Electricity Dataset}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}}\citep{wu2022autoformer}  
Containing hourly electricity consumption records of 321 clients, this dataset spans a three-year period from 2012 to 2014.

\textbf{ETT Dataset}\footnote{\url{https://github.com/zhouhaoyi/ETDataset}}\citep{zhou2021informer}  
The Electricity Transformer Temperature (ETT) dataset monitors operational parameters (including load and oil temperature) from power transformers, recorded at 15-minute (ETTm1/ETTm2) and hourly (ETTh1/ETTh2) resolutions between July 2016 and July 2018. Each subset contains seven critical operational features.

\textbf{Traffic Dataset}\footnote{\url{http://pems.dot.ca.gov/}}\citep{wu2022autoformer}  
This dataset provides hourly road occupancy rates from 862 highway sensors in the San Francisco Bay Area, collected continuously between January 2015 and December 2016.

\textbf{PEMS Dataset}\footnote{\url{http://pems.dot.ca.gov/}}\citep{li2017diffusion}  
A standard benchmark for traffic prediction, the PEMS dataset includes California freeway network statistics recorded at 5-minute intervals. Our experiments utilize four widely adopted subsets: PEMS03, PEMS04, PEMS07, and PEMS08.

\subsection{Hyper-parameter Settings and Implementation Details}
\label{ap:hyperparameter_implementation}

This section explains the hyper-parameter settings for SAMoVAR attention, linear (AR) attention, and fixed VAR models used in the experiments.

\textbf{Attention Module}: We use 3 layers, consisting of 3 MLP layers and 3 attention layers for SAMoVAR attention. For linear attention, we also use 3 Transformer blocks. For fixed VAR, we first use 3 MLP layers and replace the final mixture of VAR modules in SAMoVAR with a single-layer VAR structure with fixed weights. This VAR structure shares the same architecture as linear attention, but the query and key vectors are replaced with a set of fixed position vectors, similar to trainable positional embeddings.

\textbf{Hidden Dimension}: For all Transformers, we set the hidden dimension as $d = 32 \lfloor \sqrt{C} \rfloor$, where $C$ is the number of multivariate time series. For linear attention and fixed VAR, we use 8 attention heads. For SAMoVAR, the number of heads is determined to ensure each head dimension is 16. For example, for the ETT datasets ($C=7$), $d=64$, and the number of heads is 16. For the Weather dataset ($C=21$), $d=128$, and the number of heads is 8.

\textbf{Initialization}: All linear layers are initialized using a normal distribution with a mean of 0 and a standard deviation of 0.02. Embedding layers are zero-initialized. For projection layers in MLPs, we use GPT-2-style initialization with a scale factor of $\frac{1}{\sqrt{l}}$, where $l$ is the number of MLP/attention layers. The MLP structure follows the standard 2-layer Transformer design with an expansion ratio of 4. The dropout rate is set to 0.1 uniformly.

\textbf{Layer Normalization}: We add RMSNorm after query and key projections for SAMoVAR. For the other layer normalization modules, experiments showed no significant difference between traditional layer normalization and RMSNorm, so we opted for RMSNorm due to its lower computational cost.

\textbf{Input Preprocessing}: For input time series of shape $(C, L_P)$, where $L_P$ is the input length, we concatenate timestep embeddings along the channel dimension if available (as described in works like Autoformer \citep{wu2022autoformer} and DLinear \citep{zeng2022dlinear}). The time series is divided into $N$ non-overlapping patches of size $L_P$. Zero padding is applied if $L_P$ does not divide $L_I$ evenly. The resulting input tokens have shape $(C, N, L_P)$. We apply RevIN \citep{kim2022reversible} to normalize each token by subtracting its mean and dividing by the standard deviation of the entire input.

We then create an additional set of tokens by projecting the channel dimension using $C \times C$ linear weights. These exogenous tokens are interleaved with the univariate tokens, resulting in an ARX input of shape $(C, 2N, L_P)$. The patch size dimension $L_P$ is projected to the hidden dimension $d$, resulting in input tokens of shape $(C, 2N, d)$. We add $d$-dimensional channel and position embeddings to the input tokens.

\textbf{Transformer Module}: The input tokens are layer-normalized and passed through the Transformer module. The output has shape $(C, 2N, d)$. We select the outputs corresponding to the original univariate tokens, resulting in $(C, N, d)$. This is followed by layer normalization and projection back to dimension $L_P$, producing output tokens of shape $(C, N, L_P)$. The RevIN reverse process restores the outputs by multiplying with the previously calculated standard deviation and adding the mean. We compute the MSE loss between the output and the next timestep's univariate token values.

\textbf{Training Setup}: All experiments are conducted on a single Nvidia RTX 4090 GPU with a batch size of 32. For datasets that cause memory overflow, the batch size is reduced to 16 or 8, with 2-step or 4-step gradient accumulation to maintain an effective batch size of 32. The optimizer is AdamW with a weight decay of 0.1 and $\beta$ values of (0.9, 0.95). All baseline models are retrained under the same settings.

\textbf{Dataset Splits and Preprocessing}: Following \citet{nie2023time, liu2024itransformer}, we use a train-validation-test split ratio of 0.7, 0.1, and 0.2. Input data is standardized using the mean and standard deviation calculated from the training set.

\textbf{Training Procedure}: We apply early stopping with a patience of 12 epochs and a maximum of 100 epochs. For the first 5 epochs, we use a warm-up learning rate, gradually increasing it from 0.00006 to 0.0006, followed by linear decay until the maximum epoch is reached.

\subsection{Detailed Experimental Results}

\begin{table}[ht]
%\renewcommand{\arraystretch}{0.8}
  \caption{Full results of Multivariate TSF task. The test set MSE and MAE are reported. The best results are bolded and the second best are underlined.}\label{tab:full_main_results}
  \centering
  \begin{threeparttable}
  \begin{small}
  \renewcommand{\multirowsetup}{\centering}
  \setlength{\tabcolsep}{8pt}
  \resizebox{\textwidth}{!}{   
  \begin{tabular}{l|cccccc|cccccccccccc}
    \toprule
    
    Models & \multicolumn{2}{c}{SAMoVAR} & \multicolumn{2}{c}{LinTrans} &  \multicolumn{2}{c|}{FixedVAR} & \multicolumn{2}{c}{CATS}  & \multicolumn{2}{c}{iTransformer} & \multicolumn{2}{c}{FITS} & \multicolumn{2}{c}{PatchTST} & \multicolumn{2}{c}{DLinear} & \multicolumn{2}{c}{Encformer}  \\
    
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}\cmidrule(lr){6-7} \cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(lr){12-13}\cmidrule(lr){14-15}\cmidrule(lr){16-17}\cmidrule(lr){18-19}
    
    Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE  \\

    \midrule

Weather (96) & \textbf{0.141} & \textbf{0.193} & 0.145 & \underline{0.196} & 0.170 & 0.230 & \underline{0.143} & \underline{0.196} & 0.158 & 0.209 & 0.149 & 0.204 & 0.149 & 0.198 & 0.150 & 0.209 & 0.188 & 0.248 \\
Weather (192) & \textbf{0.186} & \textbf{0.240} & \underline{0.187} & 0.243 & 0.218 & 0.277 & 0.188 & 0.242 & 0.203 & 0.254 & 0.189 & \underline{0.241} & 0.190 & \underline{0.241} & 0.211 & 0.265 & 0.215 & 0.297 \\
Weather (336) & \textbf{0.232} & \underline{0.279} & 0.237 & 0.283 & 0.269 & 0.311 & \underline{0.235} & \textbf{0.278} & 0.250 & 0.291 & 0.237 & 0.283 & 0.240 & \underline{0.279} & 0.255 & 0.305 & 0.270 & 0.340 \\
Weather (720) & \textbf{0.295} & 0.334 & 0.299 & \underline{0.330} & 0.331 & 0.356 & \underline{0.297} & 0.331 & 0.316 & 0.341 & 0.311 & 0.332 & 0.306 & \textbf{0.327} & 0.316 & 0.350 & 0.332 & 0.382 \\
Solar (96) & \textbf{0.165} & \underline{0.230} & \underline{0.174} & 0.244 & 0.440 & 0.495 & 0.182 & 0.239 & 0.230 & 0.257 & 0.189 & 0.240 & 0.209 & 0.251 & 0.208 & 0.274 & 0.201 & \textbf{0.225} \\
Solar (192) & \underline{0.177} & 0.253 & \textbf{0.176} & \underline{0.251} & 0.405 & 0.496 & 0.214 & 0.283 & 0.204 & 0.282 & 0.206 & \textbf{0.249} & 0.192 & 0.255 & 0.208 & 0.265 & 0.209 & 0.289 \\
Solar (336) & \textbf{0.196} & 0.262 & \underline{0.198} & 0.266 & 0.406 & 0.474 & 0.216 & 0.272 & 0.222 & 0.297 & 0.219 & \underline{0.258} & 0.200 & \textbf{0.253} & 0.221 & 0.279 & 0.221 & 0.289 \\
Solar (720) & \textbf{0.199} & 0.261 & 0.207 & 0.271 & 0.467 & 0.533 & 0.213 & 0.267 & 0.218 & 0.299 & 0.221 & \textbf{0.256} & \underline{0.205} & \underline{0.258} & 0.227 & 0.291 & 0.218 & 0.274 \\
ETTh1 (96) & \textbf{0.357} & \textbf{0.394} & \underline{0.362} & \underline{0.396} & 0.493 & 0.484 & 0.365 & \underline{0.396} & 0.396 & 0.422 & 0.369 & 0.398 & 0.370 & 0.399 & 0.370 & \textbf{0.394} & 0.986 & 0.720 \\
ETTh1 (192) & \textbf{0.398} & \underline{0.419} & 0.416 & 0.437 & 0.548 & 0.531 & \underline{0.404} & 0.420 & 0.431 & 0.451 & 0.435 & 0.444 & 0.412 & 0.421 & 0.405 & \textbf{0.416} & 0.814 & 0.691 \\
ETTh1 (336) & \textbf{0.422} & 0.442 & 0.427 & 0.446 & 0.560 & 0.538 & \underline{0.423} & \underline{0.437} & 0.459 & 0.470 & 0.468 & 0.467 & \textbf{0.422} & \textbf{0.436} & 0.439 & 0.443 & 0.883 & 0.680 \\
ETTh1 (720) & \textbf{0.427} & \textbf{0.451} & 0.471 & 0.485 & 0.653 & 0.607 & \underline{0.441} & \underline{0.465} & 0.528 & 0.523 & 0.488 & 0.497 & 0.447 & 0.466 & 0.472 & 0.490 & 0.941 & 0.739 \\
ETTh2 (96) & \underline{0.266} & \underline{0.329} & 0.276 & 0.334 & 0.294 & 0.357 & \textbf{0.259} & \textbf{0.327} & 0.299 & 0.358 & 0.270 & 0.336 & 0.274 & 0.336 & 0.277 & 0.346 & 1.303 & 0.924 \\
ETTh2 (192) & \underline{0.323} & 0.386 & 0.342 & 0.382 & 0.380 & 0.423 & \textbf{0.315} & \textbf{0.368} & 0.365 & 0.399 & 0.348 & 0.400 & 0.339 & \underline{0.379} & 0.375 & 0.412 & 0.939 & 0.714 \\
ETTh2 (336) & 0.341 & 0.394 & 0.375 & 0.414 & 0.396 & 0.440 & \underline{0.339} & \underline{0.392} & 0.407 & 0.429 & 0.376 & 0.426 & \textbf{0.329} & \textbf{0.380} & 0.448 & 0.465 & 0.551 & 0.544 \\
ETTh2 (720) & \underline{0.366} & \underline{0.421} & 0.389 & 0.431 & 0.493 & 0.513 & \textbf{0.365} & \textbf{0.419} & 0.423 & 0.454 & 0.421 & 0.463 & 0.379 & 0.422 & 0.605 & 0.551 & 0.714 & 0.631 \\
ETTm1 (96) & \textbf{0.278} & \textbf{0.339} & 0.286 & 0.344 & 0.493 & 0.462 & \underline{0.282} & \textbf{0.339} & 0.325 & 0.376 & 0.305 & 0.347 & 0.290 & \underline{0.342} & 0.299 & 0.343 & 0.686 & 0.603 \\
ETTm1 (192) & \textbf{0.318} & 0.367 & \underline{0.324} & 0.371 & 0.502 & 0.471 & 0.326 & \textbf{0.363} & 0.352 & 0.388 & 0.334 & 0.371 & 0.328 & 0.369 & 0.335 & \underline{0.365} & 0.636 & 0.580 \\
ETTm1 (336) & \underline{0.359} & 0.396 & 0.363 & 0.397 & 0.496 & 0.472 & \textbf{0.358} & \textbf{0.382} & 0.382 & 0.405 & 0.363 & 0.387 & \underline{0.359} & 0.392 & \underline{0.359} & \underline{0.386} & 0.791 & 0.602 \\
ETTm1 (720) & \underline{0.401} & \underline{0.413} & 0.409 & 0.432 & 0.583 & 0.527 & 0.414 & 0.416 & 0.432 & 0.434 & 0.412 & \textbf{0.409} & 0.405 & 0.415 & \textbf{0.396} & \textbf{0.409} & 0.825 & 0.641 \\
ETTm2 (96) & \textbf{0.154} & \textbf{0.242} & \underline{0.158} & \underline{0.246} & 0.188 & 0.282 & \underline{0.158} & 0.248 & 0.187 & 0.281 & 0.164 & 0.253 & 0.165 & 0.255 & 0.184 & 0.283 & 0.481 & 0.525 \\
ETTm2 (192) & \textbf{0.208} & \underline{0.287} & 0.213 & \underline{0.287} & 0.244 & 0.318 & \underline{0.211} & \textbf{0.285} & 0.232 & 0.311 & \underline{0.211} & 0.292 & 0.214 & 0.289 & 0.218 & 0.301 & 0.434 & 0.517 \\
ETTm2 (336) & \textbf{0.257} & 0.333 & 0.263 & \underline{0.326} & 0.302 & 0.361 & 0.261 & \textbf{0.322} & 0.281 & 0.342 & \underline{0.259} & 0.334 & 0.266 & 0.328 & 0.263 & 0.333 & 0.461 & 0.531 \\
ETTm2 (720) & \underline{0.340} & \underline{0.372} & \textbf{0.339} & 0.378 & 0.377 & 0.421 & \underline{0.340} & \textbf{0.371} & 0.358 & 0.392 & 0.352 & 0.377 & 0.344 & 0.376 & 0.341 & 0.388 & 0.928 & 0.739 \\
ECL (96) & \underline{0.129} & 0.226 & 0.149 & 0.265 & 0.315 & 0.401 & \textbf{0.127} & \underline{0.223} & 0.132 & 0.226 & 0.144 & 0.246 & \underline{0.129} & \textbf{0.222} & 0.135 & 0.232 & 0.227 & 0.342 \\
ECL (192) & \textbf{0.141} & 0.243 & 0.160 & 0.268 & 0.382 & 0.457 & \underline{0.143} & \underline{0.241} & 0.166 & 0.269 & 0.149 & 0.249 & 0.147 & \textbf{0.240} & 0.151 & 0.249 & 0.658 & 0.611 \\
ECL (336) & \underline{0.156} & \underline{0.255} & 0.171 & 0.299 & 0.307 & 0.398 & \textbf{0.155} & \textbf{0.253} & 0.176 & 0.270 & 0.168 & 0.262 & 0.163 & 0.259 & 0.169 & 0.267 & 0.988 & 0.801 \\
ECL (720) & \textbf{0.176} & 0.281 & 0.183 & \underline{0.279} & 0.375 & 0.446 & \underline{0.179} & \textbf{0.273} & 0.206 & 0.283 & 0.206 & 0.303 & 0.197 & 0.290 & 0.203 & 0.301 & 0.781 & 0.739 \\
Traffic (96) & 0.371 & 0.261 & 0.423 & 0.294 & 0.764 & 0.487 & \underline{0.359} & \underline{0.253} & \textbf{0.356} & 0.259 & 0.404 & 0.286 & 0.360 & \textbf{0.249} & 0.399 & 0.286 & 0.915 & 0.460 \\
Traffic (192) & \underline{0.375} & 0.268 & 0.434 & 0.295 & 0.769 & 0.460 & \textbf{0.373} & \underline{0.258} & 0.410 & 0.278 & 0.406 & 0.274 & 0.379 & \textbf{0.256} & 0.423 & 0.287 & 0.723 & 0.485 \\
Traffic (336) & \underline{0.390} & 0.279 & 0.439 & 0.299 & 0.670 & 0.418 & \textbf{0.384} & \underline{0.274} & 0.431 & 0.281 & 0.412 & 0.279 & 0.392 & \textbf{0.264} & 0.436 & 0.296 & 0.764 & 0.451 \\
Traffic (720) & \underline{0.429} & 0.298 & 0.454 & 0.306 & 0.663 & 0.431 & \textbf{0.425} & 0.298 & 0.458 & 0.299 & 0.449 & \underline{0.291} & 0.432 & \textbf{0.286} & 0.466 & 0.315 & 0.892 & 0.483 \\
PEMS03 (96) & \textbf{0.119} & \underline{0.232} & 0.144 & 0.258 & 0.299 & 0.437 & 0.157 & 0.267 & \underline{0.135} & \textbf{0.229} & 0.193 & 0.274 & 0.198 & 0.285 & 0.198 & 0.299 & 0.162 & 0.272 \\
PEMS03 (192) & \underline{0.148} & \textbf{0.248} & \textbf{0.146} & \underline{0.264} & 0.418 & 0.523 & 0.197 & 0.300 & 0.198 & 0.299 & 0.221 & 0.299 & 0.201 & 0.288 & 0.231 & 0.328 & 0.471 & 0.513 \\
PEMS03 (336) & \textbf{0.191} & \textbf{0.279} & \underline{0.217} & 0.315 & 0.344 & 0.431 & 0.222 & 0.318 & 0.234 & 0.330 & 0.238 & 0.313 & 0.218 & \underline{0.304} & 0.254 & 0.351 & 0.542 & 0.548 \\
PEMS03 (720) & \textbf{0.142} & \textbf{0.248} & \underline{0.245} & \underline{0.306} & 0.440 & 0.509 & 0.325 & 0.406 & 0.279 & 0.344 & 0.285 & 0.353 & 0.304 & 0.392 & 0.331 & 0.413 & 0.597 & 0.610 \\
PEMS04 (96) & \textbf{0.092} & \textbf{0.193} & 0.117 & \underline{0.216} & 0.268 & 0.383 & 0.141 & 0.253 & 0.121 & 0.217 & 0.215 & 0.299 & 0.221 & 0.320 & 0.209 & 0.301 & \underline{0.116} & 0.232 \\
PEMS04 (192) & \textbf{0.100} & \textbf{0.198} & \underline{0.132} & \underline{0.250} & 0.583 & 0.604 & 0.189 & 0.310 & 0.165 & 0.279 & 0.238 & 0.331 & 0.195 & 0.294 & 0.228 & 0.323 & 0.436 & 0.482 \\
PEMS04 (336) & \textbf{0.107} & \textbf{0.207} & \underline{0.139} & \underline{0.256} & 0.341 & 0.434 & 0.196 & 0.309 & 0.175 & 0.290 & 0.265 & 0.355 & 0.226 & 0.318 & 0.252 & 0.343 & 0.432 & 0.483 \\
PEMS04 (720) & \textbf{0.109} & \textbf{0.209} & \underline{0.157} & \underline{0.271} & 0.422 & 0.468 & 0.209 & 0.327 & 0.221 & 0.331 & 0.306 & 0.391 & 0.246 & 0.344 & 0.294 & 0.377 & 0.523 & 0.538 \\
PEMS08 (96) & \textbf{0.138} & \textbf{0.213} & \underline{0.168} & 0.249 & 0.977 & 0.774 & 0.212 & 0.276 & 0.170 & \underline{0.225} & 0.337 & 0.322 & 0.183 & 0.236 & 0.318 & 0.326 & 0.253 & 0.302 \\
PEMS08 (192) & \textbf{0.197} & \textbf{0.224} & \underline{0.247} & \underline{0.284} & 0.569 & 0.503 & 0.289 & 0.313 & 0.269 & 0.295 & 0.343 & 0.361 & 0.273 & 0.305 & 0.325 & 0.344 & 0.703 & 0.619 \\
PEMS08 (336) & \textbf{0.298} & \textbf{0.313} & 0.306 & 0.328 & 0.565 & 0.471 & 0.324 & 0.338 & \underline{0.303} & \underline{0.324} & 0.352 & 0.349 & 0.335 & 0.332 & 0.374 & 0.362 & 0.831 & 0.685 \\
PEMS08 (720) & \textbf{0.304} & \textbf{0.250} & \underline{0.321} & \underline{0.340} & 0.583 & 0.515 & 0.359 & 0.369 & 0.342 & 0.351 & 0.403 & 0.387 & 0.369 & 0.376 & 0.411 & 0.415 & 0.936 & 0.716 \\
 
    \bottomrule
  \end{tabular}  }
  
  \end{small}
  \end{threeparttable}
  \vspace{-12pt}
\end{table}

\begin{table}[ht]
%\renewcommand{\arraystretch}{0.8}
  \caption{Full results of the ablation studies of the SAMoVAR module structure. The MSE and MAE of the test set are reported. The best results are bolded and the second best are underlined.}\label{tab:ablation1}
  \centering
  \begin{threeparttable}
  \begin{small}
  \renewcommand{\multirowsetup}{\centering}
  \setlength{\tabcolsep}{16pt}
  \resizebox{\textwidth}{!}{   
  \begin{tabular}{lcccccccc}
    \toprule
    
    Models & \multicolumn{2}{c}{SAMoVAR} & \multicolumn{2}{c}{w/ $\mathbf{W}_k$} &  \multicolumn{2}{c}{w/o $\mathbf{D}^{-1}$} & \multicolumn{2}{c}{w/o QV Norm}  \\
    
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}\cmidrule(lr){6-7} \cmidrule(lr){8-9}
    
    Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE  \\

    \midrule

ETTh1 (96) & \textbf{0.357} & \textbf{0.394} & 0.362 & 0.396 & \underline{0.360} & \underline{0.395} & 0.370 & 0.403 \\
ETTh1 (192) & \textbf{0.398} & \textbf{0.419} & 0.419 & 0.434 & \underline{0.404} & \underline{0.430} & 0.423 & 0.441 \\
ETTh1 (336) & \textbf{0.422} & \textbf{0.442} & \underline{0.431} & \underline{0.447} & 0.436 & 0.455 & 0.442 & 0.461 \\
ETTh1 (720) & \textbf{0.427} & \textbf{0.451} & 0.441 & 0.467 & \underline{0.435} & \underline{0.461} & 0.450 & 0.478 \\
ETTm1 (96) & \textbf{0.278} & \textbf{0.339} & 0.284 & \underline{0.341} & \underline{0.282} & 0.343 & 0.285 & 0.346 \\
ETTm1 (192) & \textbf{0.318} & \textbf{0.367} & 0.327 & \underline{0.371} & \underline{0.322} & 0.373 & 0.334 & 0.379 \\
ETTm1 (336) & \textbf{0.359} & \textbf{0.396} & 0.368 & 0.400 & \underline{0.363} & \underline{0.397} & 0.373 & 0.403 \\
ETTm1 (720) & \textbf{0.401} & \textbf{0.413} & \underline{0.405} & \underline{0.421} & 0.408 & 0.426 & 0.409 & 0.427 \\
 
    \bottomrule
  \end{tabular}  }
  
  \end{small}
  \end{threeparttable}
  \vspace{-12pt}
\end{table}

\begin{table}[ht]
%\renewcommand{\arraystretch}{0.8}
  \caption{Full results of the ablation studies of the number of heads and dimension. The MSE and MAE of the test set are reported. The best results are bolded and the second best are underlined.}\label{tab:ablation2}
  \centering
  \begin{threeparttable}
  \begin{small}
  \renewcommand{\multirowsetup}{\centering}
  \setlength{\tabcolsep}{16pt}
  \resizebox{\textwidth}{!}{   
  \begin{tabular}{lcccccccc}
    \toprule
    
    Models & \multicolumn{2}{c}{Heads=4,dim=16} & \multicolumn{2}{c}{Heads=4,dim=16} &  \multicolumn{2}{c}{Heads=4,dim=16} & \multicolumn{2}{c}{Heads=4,dim=16}  \\
    
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}\cmidrule(lr){6-7} \cmidrule(lr){8-9}
    
    Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE  \\

    \midrule

ETTh1 (96) & \textbf{0.357} & \textbf{0.394} & \textbf{0.357} & \underline{0.395} & \underline{0.363} & 0.398 & 0.365 & 0.401 \\
ETTh1 (192) & \textbf{0.398} & \textbf{0.419} & \underline{0.403} & \underline{0.424} & 0.415 & 0.426 & 0.418 & 0.431 \\
ETTh1 (336) & \textbf{0.422} & \textbf{0.442} & 0.436 & \underline{0.449} & \underline{0.435} & \underline{0.449} & 0.439 & 0.452 \\
ETTh1 (720) & \textbf{0.427} & \textbf{0.451} & \underline{0.429} & \underline{0.452} & 0.433 & 0.454 & 0.431 & 0.453 \\
ETTm1 (96) & \textbf{0.278} & \textbf{0.339} & 0.281 & 0.343 & \underline{0.280} & \underline{0.341} & 0.283 & 0.346 \\
ETTm1 (192) & \textbf{0.318} & \textbf{0.367} & \underline{0.322} & \underline{0.369} & \underline{0.322} & 0.371 & 0.323 & 0.373 \\
ETTm1 (336) & \textbf{0.359} & \textbf{0.396} & \underline{0.362} & \underline{0.399} & 0.364 & 0.401 & 0.365 & 0.401 \\
ETTm1 (720) & \textbf{0.401} & \textbf{0.413} & \underline{0.404} & \textbf{0.413} & 0.406 & \underline{0.414} & 0.406 & \underline{0.414} \\
 
    \bottomrule
  \end{tabular}  }
  
  \end{small}
  \end{threeparttable}
  \vspace{-12pt}
\end{table}

\begin{table}[ht]
%\renewcommand{\arraystretch}{0.8}
  \caption{Full results of the ablation studies of different number of layers / intermediate points in temporal influence paths. The MSE and MAE of the test set are reported. The best results are bolded and the second best are underlined.}\label{tab:ablation3}
  \centering
  \begin{threeparttable}
  \begin{small}
  \renewcommand{\multirowsetup}{\centering}
  \setlength{\tabcolsep}{5pt}
  \resizebox{\textwidth}{!}{   
  \begin{tabular}{lcccccccccccccccc}
    \toprule
    
    Models & \multicolumn{2}{c}{$l=1$} & \multicolumn{2}{c}{$l=2$} & \multicolumn{2}{c}{$l=3$} & \multicolumn{2}{c}{$l=4$} & \multicolumn{2}{c}{$l=5$} & \multicolumn{2}{c}{$l=6$} & \multicolumn{2}{c}{$l=7$} & \multicolumn{2}{c}{$l=8$}  \\
    
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}\cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17} 
    
    Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE  \\

    \midrule

ETTh1 (96) & 0.368 & 0.400 & \textbf{0.356} & \textbf{0.393} & \underline{0.357} & \underline{0.394} & 0.360 & 0.397 & 0.362 & 0.397 & 0.360 & 0.395 & \underline{0.357} & \underline{0.394} & 0.358 & \underline{0.394} \\
ETTh1 (192) & 0.409 & 0.431 & 0.402 & 0.424 & \textbf{0.398} & \textbf{0.419} & \underline{0.400} & \underline{0.421} & \underline{0.400} & 0.422 & \underline{0.400} & 0.424 & 0.406 & 0.426 & \underline{0.400} & 0.423 \\
ETTh1 (336) & 0.464 & 0.475 & 0.439 & 0.453 & \textbf{0.422} & \textbf{0.442} & 0.428 & 0.449 & 0.439 & 0.455 & 0.440 & 0.450 & 0.429 & \underline{0.444} & \underline{0.426} & 0.445 \\
ETTh1 (720) & 0.440 & 0.464 & 0.445 & 0.462 & \textbf{0.427} & \textbf{0.451} & \underline{0.428} & \underline{0.452} & 0.451 & 0.465 & 0.454 & 0.466 & 0.440 & 0.454 & 0.456 & 0.476 \\
ETTm1 (96) & 0.287 & 0.344 & \underline{0.280} & \underline{0.341} & \textbf{0.278} & \textbf{0.339} & 0.286 & 0.344 & 0.288 & 0.347 & 0.285 & 0.343 & 0.287 & 0.342 & 0.290 & 0.349 \\
ETTm1 (192) & 0.325 & 0.369 & \underline{0.322} & \underline{0.368} & \textbf{0.318} & \textbf{0.367} & 0.327 & 0.371 & 0.329 & 0.372 & 0.331 & 0.373 & 0.331 & 0.375 & 0.332 & 0.374 \\
ETTm1 (336) & 0.365 & 0.398 & \underline{0.364} & \underline{0.397} & \textbf{0.359} & \textbf{0.396} & 0.366 & 0.401 & 0.366 & 0.400 & 0.368 & 0.402 & 0.371 & 0.405 & 0.372 & 0.403 \\
ETTm1 (720) & 0.407 & 0.419 & 0.403 & 0.416 & \underline{0.401} & \textbf{0.413} & \textbf{0.400} & 0.417 & \underline{0.401} & \underline{0.415} & 0.408 & 0.424 & 0.407 & 0.426 & 0.409 & 0.430 \\
 
    \bottomrule
  \end{tabular}  }
  
  \end{small}
  \end{threeparttable}
  \vspace{-12pt}
\end{table}

\begin{table}[ht]
%\renewcommand{\arraystretch}{0.8}
  \caption{Effect of Different Random Seeds on the Results of SAMoVAR. The results show that there is almost no difference across five runs with different seeds, demonstrating the stability of SAMoVAR with respect to random initialization.}\label{tab:ablation4}
  \centering
  \begin{threeparttable}
  \begin{small}
  \renewcommand{\multirowsetup}{\centering}
  \setlength{\tabcolsep}{13pt}
  \resizebox{\textwidth}{!}{   
  \begin{tabular}{lcccccccccc}
    \toprule
    
    Models & \multicolumn{2}{c}{Seed=2023} & \multicolumn{2}{c}{Seed=2024} &  \multicolumn{2}{c}{Seed=2025} & \multicolumn{2}{c}{Seed=2026} & \multicolumn{2}{c}{Seed=2027}  \\
    
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}\cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
    
    Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE  \\

    \midrule

ETTh1 (96) & 0.356 & 0.393 & 0.357 & 0.394 & 0.357 & 0.394 & 0.358 & 0.395 & 0.357 & 0.395 \\
ETTh1 (192) & 0.401 & 0.421 & 0.397 & 0.42 & 0.398 & 0.419 & 0.404 & 0.422 & 0.401 & 0.419 \\
ETTh1 (336) & 0.424 & 0.446 & 0.421 & 0.441 & 0.422 & 0.442 & 0.423 & 0.449 & 0.425 & 0.452 \\
ETTh1 (720) & 0.429 & 0.454 & 0.431 & 0.449 & 0.427 & 0.451 & 0.425 & 0.452 & 0.428 & 0.453 \\
ETTm1 (96) & 0.279 & 0.34 & 0.279 & 0.339 & 0.278 & 0.339 & 0.278 & 0.34 & 0.277 & 0.338 \\
ETTm1 (192) & 0.317 & 0.367 & 0.318 & 0.367 & 0.318 & 0.367 & 0.317 & 0.367 & 0.319 & 0.368 \\
ETTm1 (336) & 0.359 & 0.397 & 0.358 & 0.396 & 0.359 & 0.396 & 0.359 & 0.398 & 0.361 & 0.397 \\
ETTm1 (720) & 0.401 & 0.413 & 0.402 & 0.413 & 0.401 & 0.413 & 0.403 & 0.414 & 0.4 & 0.412 \\
 
    \bottomrule
  \end{tabular}  }
  
  \end{small}
  \end{threeparttable}
  \vspace{-12pt}
\end{table}

\begin{table}[h]
\renewcommand{\arraystretch}{0.8}
  \caption{Comparison of computational costs. This comparison utilizes the data format of ETTh1 to construct model inputs. $L_I$ is set to 512 for the baselines, 1024 for VAR-based models, and the other hyper-parameters for every model are set according to their default configurations.}\label{tab:comp_cost}
  \centering
  \begin{threeparttable}
  \begin{small}
  \renewcommand{\multirowsetup}{\centering}
  \setlength{\tabcolsep}{16pt}
  \resizebox{\columnwidth}{!}{ 
\begin{tabular}{lcccccc}
\midrule
 Models & \multicolumn{2}{c}{SAMoVAR} & \multicolumn{2}{c}{LinTrans} & \multicolumn{2}{c}{FixedVAR} \\
 \cmidrule(lr){2-3} \cmidrule(lr){4-5}\cmidrule(lr){6-7}
 Metric & FLOPs & Params & FLOPs & Params & FLOPs & Params \\
\midrule
$L_P=96$ & 43.31M & 157.3K & 50.37M & 181.9K & 35.74M & 196.5K \\
$L_P=192$ & 25.24M & 175.9K & 29.08M & 200.4K & 21.11M & 215K \\
$L_P=336$ & 18.44M & 199.6K & 20.99M & 224.1K & 15.69M & 238.7K \\
$L_P=720$ & 11.38M & 272.6K & 12.63M & 297.2K & 10M & 311.8K \\
\midrule
Models & \multicolumn{2}{c}{Encformer} & \multicolumn{2}{c}{PatchTST} & \multicolumn{2}{c}{iTransformer} \\
 \cmidrule(lr){2-3} \cmidrule(lr){4-5}\cmidrule(lr){6-7}
 Metric & FLOPs & Params & FLOPs & Params & FLOPs & Params \\
\midrule
$L_P=96$ & 1.328G & 1.646M & 180.9M & 1.841M & 42.39M & 1.923M \\
$L_P=96$ & 1.442G & 1.647M & 215.5M & 3.414M & 42.66M & 1.935M \\
$L_P=96$ & 1.613G & 1.648M & 267.5M & 5.773M & 43.07M & 1.954M \\
$L_P=96$ & 2.068G & 1.65M & 405.9M & 12.07M & 44.15M & 2.003M \\
\bottomrule
\end{tabular}
  }
  \end{small}
  \end{threeparttable}
\vspace{-12pt}
\end{table}

\subsection{Additional Visualization of the Synthetic VAR Task}

\begin{figure*}[!t]
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=5.5cm]{fig/vis3/Combined_Visualization_Obs.pdf}
        \label{fig:synthetic_obs_3}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=5.5cm]{fig/vis3/Combined_Visualization_Attn.pdf}
        \label{fig:synthetic_attn_3}
    \end{minipage}
    \vskip -0.2in
    \caption{Additional Visualization of the VAR Synthetic Task with a Random Datapoint in the Validation Set.}
    \label{fig:synthetic_3}
    \vskip -0.1in
\end{figure*}

\begin{figure}[!t]
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.3cm, trim={20pt 350pt 20pt 0pt}, clip]{fig/vis3/temporal_influence_paths_variable0.pdf}
        \label{fig:synthetic_tip_0_3}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.3cm, trim={20pt 350pt 20pt 0pt}, clip]{fig/vis3/temporal_influence_paths_variable1.pdf}
        \label{fig:synthetic_tip_1_3}
    \end{minipage}
    \vskip -0.12in
    \caption{Visualization of the 2 temporal influence paths from step 124 to step 128 for the two input time series variable for the datapoint shown above, where even-numbered steps represent endogenous tokens and odd-numbered steps represent exogenous tokens.}
    \label{fig:synthetic_tip_3}
    \vskip -0.2in
\end{figure}

\begin{figure*}[!t]
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=5.5cm]{fig/vis4/Combined_Visualization_Obs.pdf}
        \label{fig:synthetic_obs_4}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=5.5cm]{fig/vis4/Combined_Visualization_Attn.pdf}
        \label{fig:synthetic_attn_4}
    \end{minipage}
    \vskip -0.2in
    \caption{Additional Visualization of the VAR Synthetic Task with a Random Datapoint in the Validation Set.}
    \label{fig:synthetic_4}
    \vskip -0.1in
\end{figure*}

\begin{figure}[!t]
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.3cm, trim={20pt 350pt 20pt 0pt}, clip]{fig/vis4/temporal_influence_paths_variable0.pdf}
        \label{fig:synthetic_tip_0_4}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.3cm, trim={20pt 350pt 20pt 0pt}, clip]{fig/vis4/temporal_influence_paths_variable1.pdf}
        \label{fig:synthetic_tip_1_4}
    \end{minipage}
    \vskip -0.12in
    \caption{Visualization of the 2 temporal influence paths from step 124 to step 128 for the two input time series variable for the datapoint shown above, where even-numbered steps represent endogenous tokens and odd-numbered steps represent exogenous tokens.}
    \label{fig:synthetic_tip_4}
    \vskip -0.2in
\end{figure}

\begin{figure*}[!t]
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=5.5cm]{fig/vis2/Combined_Visualization_Obs.pdf}
        \label{fig:synthetic_obs_2}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=5.5cm]{fig/vis2/Combined_Visualization_Attn.pdf}
        \label{fig:synthetic_attn_2}
    \end{minipage}
    \vskip -0.2in
    \caption{Additional Visualization of the VAR Synthetic Task with a Random Datapoint in the Validation Set.}
    \label{fig:synthetic_2}
    \vskip -0.1in
\end{figure*}

\begin{figure}[!t]
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.3cm, trim={20pt 350pt 20pt 0pt}, clip]{fig/vis2/temporal_influence_paths_variable0.pdf}
        \label{fig:synthetic_tip_0_2}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.3cm, trim={20pt 350pt 20pt 0pt}, clip]{fig/vis2/temporal_influence_paths_variable1.pdf}
        \label{fig:synthetic_tip_1_2}
    \end{minipage}
    \vskip -0.12in
    \caption{Visualization of the 2 temporal influence paths from step 124 to step 128 for the two input time series variable for the datapoint shown above, where even-numbered steps represent endogenous tokens and odd-numbered steps represent exogenous tokens.}
    \label{fig:synthetic_tip_2}
    \vskip -0.2in
\end{figure}

\begin{figure*}[!t]
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=5.5cm]{fig/vis5/Combined_Visualization_Obs.pdf}
        \label{fig:synthetic_obs_5}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=5.5cm]{fig/vis5/Combined_Visualization_Attn.pdf}
        \label{fig:synthetic_attn_5}
    \end{minipage}
    \vskip -0.2in
    \caption{Additional Visualization of the VAR Synthetic Task with a Random Datapoint in the Training Set.}
    \label{fig:synthetic_5}
    \vskip -0.1in
\end{figure*}

\begin{figure}[!t]
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.3cm, trim={20pt 350pt 20pt 0pt}, clip]{fig/vis5/temporal_influence_paths_variable0.pdf}
        \label{fig:synthetic_tip_0_5}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.3cm, trim={20pt 350pt 20pt 0pt}, clip]{fig/vis5/temporal_influence_paths_variable1.pdf}
        \label{fig:synthetic_tip_1_5}
    \end{minipage}
    \vskip -0.12in
    \caption{Visualization of the 2 temporal influence paths from step 124 to step 128 for the two input time series variable for the datapoint shown above, where even-numbered steps represent endogenous tokens and odd-numbered steps represent exogenous tokens.}
    \label{fig:synthetic_tip_5}
    \vskip -0.2in
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
