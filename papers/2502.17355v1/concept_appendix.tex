\section{Concept-Specific Neurons}\seclabel{concepts}
%what is happpening with concepts recap
\paragraph{Concept-Relation Overlap in the 7B Model}
%difference of 7b to 13b
Figure \ref{fig:concept_neuron_overlap} illustrates the overlap between individual relation- and concept-specific neurons in the 7b model. 
There, the overlap of concepts connected to the abstract notion of ``location'' and the relations are mostly concentrated on the \texttt{landmark\_country} relation in comparison to the 13b model, where they are spread over \texttt{company\_hq}, \texttt{landmark\_continent} and \texttt{landmark\_country}. 
This aligns with the difference between the 7B and 13B models in terms of their patterns of inter-relation results (cf. Figure \ref{fig:inter-relation}): deactivating the \texttt{landmark\_country} neurons results in a significant accuracy drop in other relations concerning ``location'' in the 13B model while not in the 7B model.
%more observations:
%some are not represented in the selected relations, but single instances might be
%Different to the main method, the concept groups include all subject instances from the different relations in the LRE, even those not included in the experiments described in \secref{method}. Because of this it is not surprising that there is few overlap for the neurons of concepts like \texttt{superhero}, \texttt{pokemon} and \texttt{food}. was included in main
%some more abstract/general concepts are distributed over more neurons, but paritally covers
Another difference between both models is that there is more distributed neuron overlap in the 7b model between the subject concept \texttt{person} and all corresponding relations.

\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{-0.1cm}
    %\setlength{\belowcaptionskip}{-0.4cm}
\includegraphics[width=0.46\textwidth]{figs/entity_neurons/neuron_overlaps_top3000_7b.pdf}
%\includegraphics[width=0.46\textwidth]{figs/entity_neurons/neuron_overlaps_top3000_llama2-13b-hf.pdf}
    \caption{Overlap between the top 3000 identified neurons for each relation and concept in the 7B model. }
    \label{fig:concept_neuron_overlap}
\end{figure}

\paragraph{Validation of Concept-Specific Neurons}
The top neurons on a concept are evaluated on a random selection of 100 prompts from the LRE dataset that include the specified concept as a subject. Examples for the concept \texttt{person} are "Tom Hanks's father is named? Answer:", "Hilary Hahn plays the instrument of? Answer:", or "Thomas Mann went to university at? Answer:".

Figure \ref{fig:concept_neuron_validation} shows the results for the validation on these validation prompts for both models with the original accuracy score, a baseline that ablates 3000 neurons randomly, and the ablation of 3000 concept-specific neurons. Note that the impact of ablating a certain amount of expert neurons varies between concepts. The observed drop in performance due to the ablation of 3000 neurons for concepts like \texttt{pokemon}, \texttt{superhero}, and \texttt{star} is very large, 
while accuracy scores of other concepts in the 13b model, such as \texttt{person} appear stable, or even improve, e.g., \texttt{presidents}. 
We assume the \textbf{neuron cumulativity} also applies to the concept-specific neurons. 
That is, the knowledge on a specific concept is distributed over a much larger population of neurons, and further accuracy drop can be observed once more concept-specific neurons are deactivated -- similar to what we observe for relation-specific neurons (cf. Figure \ref{fig:neuron_num}). 
As only partial knowledge is withheld from the deactivation of 3000 concept-specific neurons, 
this might be too little knowledge to affect the facts concerning that concept (substantial knowledge on the concept is stored in the remaining neurons), resulting in only a small accuracy drop.
Or, the 3000 concept-specific neurons store knowledge, though concerning the concept, unrelated to the prompts.
For instance, the validation prompts of the concept \texttt{presidents} all demand \textbf{historical dates} as predicted answers, which is only one kind of knowledge that might be expected in connection with presidents.
This phenomenon actually aligns with our neuron interference hypothesis: deactivating neurons that store unhelpful knowledge can less confuse the model, therefore improving the performance.


\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{-0.1cm}
    %\setlength{\belowcaptionskip}{-0.4cm}
\includegraphics[width=0.46\textwidth]{figs/entity_neurons/perf_test_3000.pdf}
%\includegraphics[width=0.46\textwidth]{figs/entity_neurons/neuron_overlaps_top3000_llama2-13b-hf.pdf}
    \caption{Accuracy results of evaluation prompts for 11 concepts in the 7b and 13b model. We report the performance of the original model (without any deactivation) e.g., \texttt{7b-original}, the model 3000 random deactivated neurons, e.g. \texttt{7b-random}, and the model with deactivating the top 3000 identified concept-specific neurons, e.g., \texttt{7b-concept}.}
    \label{fig:concept_neuron_validation}
\end{figure}
