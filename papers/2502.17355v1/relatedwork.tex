\section{Related Work}
Mechanistic interpretability (MI) is a growing subfield of interpretability that aims to understand LLMs by breaking them down into smaller components and fundamental computations. It has gained significant attention for studying how LLMs recall factual knowledge learned during pretraining \citep{meng2022locating,dai-etal-2022-knowledge,geva-etal-2023-dissecting,yu-etal-2023-characterizing,lv2024interpreting,wang-etal-2024-unveiling}. 
Following \citet{olah2020zoom, rai2024practical}, MI research can be categorized into two areas: the study of \textbf{features} and the study of \textbf{circuits}, based on the type of decomposed components. Features refer to human-interpretable properties encoded in model representations or represented by model components, such as neurons and attention heads \citep{elhage2022solu, gurnee2023finding}. Circuits are subgraphs of the model's computation graph responsible for implementing specific behaviors 
% in the language model 
\citep{wang2022interpretability, elhage2021mathematical}.

In this work, we focus on neuron-level feature-based interpretability analysis to localize relation-specific neurons, which are responsible for encoding and recalling specific types of factual knowledge. Existing studies have utilized various approaches for neuron interpretation, each offering unique advantages and limitations \cite{sajjad-etal-2022-neuron, rai2024practical}. The \textit{visualization} method \citep{olsson2022context, elhage2022solu, lieberum2023does, bills2023language,liu-etal-2024-unraveling} involves visualizing neuron activations and manually identifying the underlying concept across input text. While being straightforward, it relies heavily on human effort and risks overgeneralization. \textit{Statistics}-based methods \citep{Bau2019Identifying,neuron2022Cuadros,kojima-etal-2024-multilingual,yu-ananiadou-2024-neuron,tang-etal-2024-language,wang-etal-2024-unveiling}, on the other hand, aggregate activation statistics across data to establish connections between neurons and concepts, identifying patterns through the co-occurrence of neuron activation values and specific input features. \textit{Probing}-based methods \citep{dalvi2019one,  durrani-etal-2020-analyzing, antverg2021pitfalls, gurnee2024universal} train diagnostic classifiers on neuron activations to identify neurons associated with predefined concepts. These methods are scalable, enabling the discovery of neuron sets across large datasets, though they depend on supervised data annotations. \textit{Causation}-based methods \citep{vig2020investigating, meng2022locating, meng2022mass, kramar2024atp,song-etal-2024-large} take a different approach by directly varying the values of specific neurons or components and analyzing changes in model behavior; significant changes indicate the importance of these neurons or components to particular functionalities. 

Building on this foundation, our work adopts the statistics-based method proposed by \citet{neuron2022Cuadros} to identify relation-specific neurons -- neurons uniquely ``fired'' for queries concerning facts sharing the same relation. This approach facilitates a scalable and targeted analysis of neuron behavior in relation to factual knowledge recall.


% \shortpar{Interpreting Factual Recall in LLMs}

% There has been a growing interest recently in interpreting how LLMs recall factual knowledge learned from their pretraining stage \citep{meng2022locating,dai-etal-2022-knowledge,geva-etal-2023-dissecting,yu-etal-2023-characterizing,lv2024interpreting,wang-etal-2024-unveiling}.
% Depending on the objects to investigate, this line of studies can be roughly classified into \emph{representation-level} analysis and \emph{neuron-level} analysis \citep{sajjad-etal-2022-neuron}.
% Representation-level analysis aims to understand how knowledge is represented in the token embedding space and the dynamics of representations leading to language-based predictions \citep{li-etal-2021-implicit,meng2022locating,geva-etal-2022-transformer,Hase2023localization,merullo-etal-2024-language,lre2024Hernandez,fierro2024multilingualmodelsrememberinvestigating}.
% Neuron-level analysis aims to interpret the factual recall behavior at a more granular level by analyzing neurons. This usually involves identifying and investigating the functionality of neurons in specific elements of the model architectures, such as attention heads \citep{hao2021self,yu-etal-2023-characterizing,elhelo2024inferring} and feed-forward networks \citep{geva-etal-2021-transformer,dai-etal-2022-knowledge,wang2024sharing,wang-etal-2024-unveiling}.

% % \shortpar{Locating Knowledge Neurons.} 
% In many neuron-level analysis studies, one core question is how to locate the neurons that are relevant to specific concepts \citep{sajjad-etal-2022-neuron}. \emph{Activation-based} methods focus on the forward pass and try to solve the question by investigating the neuron activation patterns \citep{Li2023lazyneuron,voita-etal-2024-neurons,gurnee2024universal}. \emph{Gradient-based} methods look at the backward pass and measure the sensitivity of model outputs to different neurons in response to specific inputs \citep{dai-etal-2022-knowledge,Lundstrom2022Rigorous,chen2024identifying,chen2024journey}. Alternatively, some studies propose methods based on saliency scores -- some aggregated statistics that indicate neurons' importance to a group of queries that share common characteristics \citep{Bau2019Identifying,neuron2022Cuadros,kojima-etal-2024-multilingual,yu-ananiadou-2024-neuron,tang-etal-2024-language,wang-etal-2024-unveiling}. Our work adopts the method proposed by \citet{neuron2022Cuadros} and identifies relation-specific neurons -- neurons that are uniquely ``fired'' in queries concerning facts sharing the same relation.

\enote{hs}{related work sonds good -- but is very short. are
you sure this covers everything?}
\enote{yh}{no i am not very sure -- i tried to cover the papers that i found and classified them into different categories. probably we need dawar and mingyang to double check.}