\section{Introduction}


%What is the problem?

The rapid advancement of large language models (LLMs)~\cite{brown2020language}, especially in their reasoning capabilities, offers transformative potential for addressing complex challenges in atmospheric science~\cite{nguyen-etal-2024-climate, zhang2024opportunities, thulke2024climategpt, cao2024llmassisted}. However, the development of reliable and effective LLM-based applications for climate-related tasks requires \textit{a robust and comprehensive evaluation framework. Such a benchmark is essential to systematically assess the performance of LLMs across a diverse array of atmospheric science problems}, ensuring their utility, accuracy, and robustness in this critical domain.



%Why is it interesting and important? ( Why atmospheric scientific problem is different from mathematics and physics)

Constructing a comprehensive benchmark for atmospheric science is crucial to harness the recent advancements in large language models (LLMs) for diverse climate service applications~\cite{zhang2024opportunities}. Note that atmospheric science presents unique and complex challenges, ranging from micro-scale processes like cloud dynamics to global-scale climate systems. To ensure that LLMs can effectively contribute to solving these real-world problems, it is essential to establish a benchmark that evaluates their performance, especially their reasoning and interpretative abilities --- Such a well-designed benchmark will not only foster innovation but also provide a standardized framework for assessing the utility, accuracy, and robustness of LLMs in this field.


%Draft from Deng Wen
%Atmospheric science poses unique complexities that distinguish it from traditional problems in mathematics and physics, making it an important and intriguing field of study. First, a solid understanding of atmospheric science background is essential for grasping and identifying the fundamental concepts of various atmospheric systems. Unlike purely theoretical fields, atmospheric science requires interdisciplinary knowledge to contextualize problems within real-world phenomena. Furthermore, the acquisition and processing of data in atmospheric science often involve handling diverse types of data, such as latitude/longitude, temperature, wind direction, wind speed, and distance. These data are presented in various forms, including descriptive language, multiple units, numerical values, or indirectly provided empirical estimates, adding layers of complexity to their interpretation and integration. After interpreting the data and information from the problem text, it is necessary to select the most appropriate physical models and mathematical methods to solve the problem, to minimize the error in the results. This process contrasts with traditional physics and mathematics problems, which typically have more straightforward and well-defined pathways to solutions. These unique challenges underscore the importance of atmospheric science as a field that bridges theory and application.


% Why is it hard? 

Atmospheric science problems differ significantly from the mathematical and physical problems commonly found in existing LLM benchmarks~\cite{hendrycks2021measuring,wang2023scibench}. This field is inherently interdisciplinary, requiring the integration of theoretical knowledge with real-world phenomena. Atmospheric science involves analyzing and synthesizing heterogeneous data types, such as spatial coordinates, temperatures, wind patterns, and empirical estimates, which are often presented in varied formats and units. Furthermore, solving these problems necessitates the selection of appropriate physical models and mathematical methods to ensure accuracy, adding layers of complexity beyond traditional benchmarks. As such, constructing a benchmark tailored to atmospheric science is a necessary complement to existing evaluations, enabling a more comprehensive assessment of LLMs' reasoning capabilities.

%What are the key components of my approach and results? 

% components: benchmark constructions 
To address this need, we present \name, a novel benchmark to comprehensively evaluate the recent advance of LLMs for atmospheric science. Concretely, we summarize our key contributions:


\textbf{\underline{Contribution 1.}} We construct \name, a multiple-choice question benchmark designed to evaluate LLM performance across five core categories of atmospheric science: (\underline{i}) \textit{hydrology}, (\underline{ii}) \textit{atmospheric dynamics}, (\underline{iii}) \textit{atmospheric physics}, (\underline{vi}) \textit{geophysics}, and (\underline{v}) \textit{physical oceanography}. The benchmark is carefully curated from graduate-level atmospheric science problems, ensuring a high standard of relevance and complexity.
Technically, \name employs a template-based question generation framework. Using a manually implemented, rule-based mechanism, each question template can be systematically expanded into a desired number of concrete questions through effective symbolic extensions. This approach ensures both scalability and diversity in the question set, providing a robust tool for assessing LLM capabilities in reasoning and problem-solving within the domain of atmospheric science.

% components: highlight of results.

\textbf{\underline{Contribution 2.}} We conduct a comprehensive evaluation that includes a wide range of representative open-source and proprietary LLMs, which can be concretely categorized into four classes:   (\underline{i}) \textit{instruction model}s that have been fine-tuned for instruction following;  
(\underline{ii}) \textit{reasoning models} that have been aligned with advanced reasoning abilities;  
(\underline{iii}) \textit{math models} that have been augmented with more mathematical skills; and 
(\underline{vi}) \textit{domain-specific climate models} that have been continuously pre-trained with climate-relevant corpus.   


\textbf{\underline{Contribution 3.}} We carefully analyze the evaluation results and summarize the following interesting findings:

\begin{itemize}[topsep=5pt, leftmargin=1em]
\vspace{-0.5em}
\item \textit{\textbf{Finding 1.}} \textit{Reasoning models (e.g., Deepseek-R1) outperform instruction, math, and domain-specific models, demonstrating the superior significance of advanced reasoning ability in atmospheric science tasks.}

\vspace{-0.35em}
\item \textit{\textbf{Finding 2.}} \textit{The inference time scaling introduces interesting quality-efficiency tradeoffs for reasoning models---Increasing reasoning token length enhances model accuracy up to $16$K tokens, while further expansion yields diminishing returns.}

\vspace{-0.35em}
% \item \textit{\textbf{Finding 3.}} \textit{The reasoning models assessed using our benchmark exhibit a lack of robustness under symbolic perturbation, as greater degrees of variation result in substantial and often unpredictable declines in accuracy.
% The reasoning model shows the greatest robustness when handling long numerical sequences, whereas the instruction model is highly sensitive to changes in numerical precision.}

\item \textit{\textbf{Finding 3.}} \textit{While reasoning models show better robustness when handling arithmetic tasks with higher numerical precision when compared with other model categories, they still relatively struggle with symbolic perturbation.}

\vspace{-0.5em}

\end{itemize}


\section{Related Work}

\textbf{LLM advances.}
LLMs, such as \textsc{OPT}~\cite{zhang2022opt}, \textsc{LLaMA}~\cite{touvron2023llama}, \textsc{GPT}~\cite{gpt4o}, \textsc{Gemini}~\cite{reid2024gemini}, \textsc{Claude}~\cite{claude3}, and \textsc{Mixtral}\cite{jiang2024mixtral}, have demonstrated remarkable performance across a wide range of applications. While general-purpose LLMs exhibit strong adaptability, domain-specific models have also been developed to enhance performance in specialized fields.
In the context of atmospheric science, climate-focused LLMs such as \textsc{CLIMATEBERT}~\cite{webersinke2021climatebert}, \textsc{ClimateGPT}~\cite{thulke2024climategpt}, and \textsc{ClimaX~\cite{nguyen2023climax}} are designed to address the unique challenges of climate modeling and analysis, which illustrates a promising paradigm different from traditional approaches that designing a specific model for some particular task~\cite{lam2022graphcast, pathak2022fourcastnet, bi2022pangu, chen2023fengwu, chen2023fuxi}.
More recently, reasoning models, including \textsc{GPT-o1}~\cite{openai_learning_to_reason_with_llms}, \textsc{Gemini-2.0-Flash-Thinking}~\cite{deepmind_gemini_flash_thinking}, \textsc{QwQ}~\cite{qwq-32b-preview}, and \textsc{DeepSeek-R1}~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, have emerged, highlighting advancements in mathematical and scientific problem-solving. These models leverage sophisticated reasoning techniques, presenting exciting opportunities for tackling complex challenges in atmospheric science. %Their ability to integrate multi-modal data and perform domain-specific reasoning could underscore a paradigm shift in leveraging LLM for climate research.



%  task-specific climate models 
\iffalse
{
Recent advancements in task-specific climate models, such as \textsc{GraphCast}~\cite{lam2022graphcast}, \textsc{FourCastNet}~\cite{pathak2022fourcastnet}, \textsc{Pangu-Weather}~\cite{bi2022pangu}, \textsc{FengWu}~\cite{chen2023fengwu}, and \textsc{FuXi}~\cite{chen2023fuxi}, have significantly improved the accuracy and efficiency of large-scale spatio-temporal climate forecasting, rivaling traditional numerical models like European Centre for Medium-Range Weather Forecasts' Integrated Forecasting System (ECMWF-IFS).

% General-purpose model
\textsc{CLIMATEBERT}~\cite{webersinke2021climatebert} and \textsc{ClimaX}~\cite{nguyen2023climax} are models designed for climate-related applications, with the former excelling in text-based tasks like classification and sentiment analysis, and the latter extending transformer-based approaches to weather forecasting and climate projections, but both lack the versatility to address broader atmospheric science challenges.

% ClimateGPT
Models such as \textsc{ClimateGPT}~\cite{thulke2024climategpt} mark a step toward interdisciplinary applications in climate science by synthesizing research perspectives and generating conceptual responses. Despite these advancements, their ability to perform graduate-level problem solving remains limited, particularly in tasks requiring mathematical and physical reasoning.

%  Our previous case study and GPT-4o
A case study~\cite{zhang2024opportunities} on \textsc{GPT-4o}'s potential to assist atmospheric science researchers in addressing tasks, showcasing its ability to handle heterogeneous input data and execute complex tasks. However, the evaluation also reveals that \textsc{GPT-4o} falls short of solving graduate-level atmospheric science problems, indicating room for improvement in reasoning and problem-solving capabilities at advanced levels.

% Reasoning model
More recently, a new generation of reasoning-focused models, such as \textsc{GPT-o1} \textsc{Gemini-2.0-Flash-Thinking}, \textsc{QwQ} and \textsc{DeepSeek-R1}, has emerged, showcasing remarkable capabilities in mathematical and scientific problem-solving. These models represent a paradigm shift, with potential applications that extend beyond traditional tasks, leveraging advanced reasoning techniques to address challenges in complex domains like atmospheric science.
}
\fi












% In light of these advancements, our work aims to bridge the gap between task-specific and reasoning-oriented models by introducing \name, a benchmark that evaluates the reasoning, problem-solving, and domain-specific capabilities of large language models in atmospheric science.


% \TBD The Impact of Reasoning Step Length on Large Language Models: Reasoning step is important in Cot prompt



\textbf{LLM benchmarks.}
\label{sec:llm_benchmarks}
Assessing LLMs is crucial for ensuring their effectiveness in deployment across various domains~\cite{liang2022holistic}. Traditional benchmarks like \texttt{GSM8K}~\cite{cobbe2021training} and \texttt{MATH}~\cite{hendrycks2021measuring} have become less effective as state-of-the-art models achieve near-perfect scores, necessitating more challenging benchmarks to evaluate reasoning capabilities accurately.
Recent benchmarks target specialized fields, such as \texttt{GPQA-Diamond}~\cite{rein2023gpqa} for expert-level science, \texttt{AIME2024}~\cite{MAAInvitational2024} for advanced mathematics, and \texttt{SCIBENCH}~\cite{wang2023scibench} for collegiate science problems. 
However, a comprehensive LLM benchmark for atmospheric science remains underrepresented, where \texttt{CLIMAQA}~\cite{manivannan2024climaqa} only offers basic definition-based assessments, lacking depth in evaluating complex problem-solving abilities.
Designing a good LLM benchmark requires principled guidance to ensure robust, accurate, and meaningful evaluation. For example, A notable advancement is the introduction of symbolic extensions in benchmarking, as seen in \texttt{GSM-Symbolic}~\cite{mirzadeh2024gsm}, \texttt{VarBench}~\cite{qian2024varbench}, and \texttt{MM-PhyQA}. These benchmarks introduce question variants by altering numerical values or modifying problem structures, improving robustness, and mitigating contamination risks. Notably, \texttt{GSM-Symbolic} highlights that even minor perturbations can significantly impact model performance, revealing fragilities in LLM reasoning.
Additionally, numerical reasoning plays a fundamental role in evaluating LLMs, especially for scientific applications. Papers like NumberCookbook~\cite{yang2024number} and NumeroLogic~\cite{schwartz2024numerologic} uncover weaknesses in LLMs' ability to process numerical information accurately, emphasizing that tokenization strategies and internal number representation significantly affect arithmetic performance~\cite{singh2024tokenization}.
Despite advancements in benchmarking, a rigorous climate-focused evaluation framework is still missing. %We address this gap by proposing a benchmark that integrates domain-specific challenges with principled evaluation methods, ensuring LLMs are tested on reasoning, robustness, and numerical accuracy in climate science.

\iffalse
{
% Show frontier models1 do so well on MATH2 and GSM8K that these benchmarks are no longer effective at differentiating models. Using some example to show these. For example gpt o4 can got 90 something on MATH2, o1 got 9x on Math2, not effective to differentiate
% \cite{Recent frontier models1 do so well on MATH2 and GSM8K that these benchmarks are no longer effective at differentiating models. https://openai.com/index/learning-to-reason-with-llms/}
Benchmarks like the GSM8K~\cite{cobbe2021training} and MATH~\cite{hendrycks2021measuring} have long been used to evaluate instruction-tuned models. However, recent advancements in frontier models have rendered these benchmarks less effective at distinguishing model performance, as state-of-the-art models now achieve near-perfect scores on them. 

Benchmarks with higher-quality and extremely challenging questions are regarded as the primary evaluation tools for reasoning models due to their proven effectiveness. Among these, \texttt{GPQA-Diamond}~\cite{rein2023gpqa} represents the science domain, featuring PhD-level questions in chemistry, physics, and biology to assess expert-level understanding beyond general knowledge. Similarly, the American Invitational Mathematics Examination 2024 (\texttt{AIME2024})~\cite{MAAInvitational2024} stands as a representative benchmark for the mathematics domain, focusing on advanced topics and complex reasoning.
% \texttt{MMMU} and \texttt{MMMU-Pro} assess the true understanding and reasoning capabilities of multimodal models through rigorous testing, with \texttt{MMMU-Pro} introducing visual-only settings and augmented candidate options to ensure robust evaluation of vision-language integration.

% Beyond these general-purpose benchmarks, science-specific benchmarks have emerged, offering unique perspectives and challenges. \texttt{MM-PhyQA} is a multimodal physics question-answering benchmark incorporating Multi-Image Chain-of-Thought (MI-CoT) prompting. It evaluates high school-level physics problems, focusing on models’ abilities to integrate visual and textual information. \texttt{SCIBENCH} is a collegiate-level benchmark designed to evaluate scientific problem-solving abilities across mathematics, chemistry, and physics, highlighting limitations in current LLMs and showing that even top-performing non-reasoning models achieve only below 50\% accuracy.
More science benchmarks such as \texttt{SCIBENCH}~\cite{wang2023scibench} and \texttt{MM-PhyQA}~\cite{anand2024mm} have emerged focusing on collegiate-level problem-solving.

For climate science, \texttt{CLIMAQA}~\cite{manivannan2024climaqa} provides a domain-specific evaluation framework, primarily focusing on definition-based questions.
% and lacks assessments requiring the selection of appropriate physical models or advanced mathematical reasoning. Consequently, it does not adequately test models' complex reasoning capabilities.


\texttt{GSM-Symbolic}~\cite{mirzadeh2024gsm},\texttt{VarBench}~\cite{qian2024varbench}, and \texttt{MM-PhyQA} they come up with new paradigm of benchmark, for each question in dataset, they can question variants by altering the variables. This approach provides a more accurate and robust assessment of the true capabilities of language models, effectively mitigating the contamination problem. 
Notably, \texttt{GSM-Symbolic} reveals that minor changes, such as altering numerical values or adding irrelevant clauses, can significantly impact model performance. This highlights the fragility of language models in mathematical reasoning and underscores their current limitations.

% Similarly, \texttt{GSM-Symbolic} addresses the limitations of existing mathematical reasoning evaluations by introducing a benchmark based on symbolic templates that generate diverse and controllable questions. This approach reveals significant performance variability in LLMs when faced with different instantiations of similar problems and highlights the fragility of their reasoning capabilities. \texttt{GSM-Symbolic} demonstrates that even minor changes, such as altering numerical values or adding irrelevant clauses, can lead to drastic performance drops, providing critical insights into the current limitations of LLMs in mathematical reasoning.


Despite these advancements, there remains a notable gap in benchmarks designed for advanced reasoning models, particularly in the diversity and scope of scientific domains. Existing benchmarks predominantly focus on subjects like chemistry, physics, and biology, leaving significant room for expansion in diversity and lacking adequate representation of interdisciplinary fields. 
% To address this gap, we introduce \name, a novel benchmark tailored to the interdisciplinary field of atmospheric science. 
% By expanding the scope of science benchmarks and increasing their diversity. \name not only enhances the evaluation of reasoning models but also provides a rigorous framework to assess LLMs’ overall competency in scientific problem-solving.

% GSM
% Pattern matching


\textbf{Arithmatic}

The mechanisms behind arithmetic in LLMs remain unclear, and their robustness in performing arithmetic—an essential capability in scientific disciplines—continues to be a subject of investigation. In this context of reasoning ability and Numerical Understanding and Processing Ability (NUPA) are often evaluated together, Number Cookbook~\cite{yang2024number} provides a comprehensive test suite encompassing diverse numerical representations and tasks, which has uncovered surprising vulnerabilities in LLMs' handling of fundamental arithmetic operations. Similarly, NumeroLogic~\cite{schwartz2024numerologic} highlights that the challenges LLMs face in processing numerical data and performing arithmetic are partly due to the non-intuitive representation of textual numbers. Tokenization strategies significantly impact these challenges, as tokenization-dependent error patterns reveal that arithmetic computation errors often stem from tokenization choices. These findings further suggest the existence of an underlying algorithm within LLMs, challenging the notion that they rely solely on “fuzzy” matching to similar training examples~\cite{singh2024tokenization}.
}
\fi







\iffalse
\begin{itemize}
    \item The emergence of reasoning model: xxx
    \item CLIMATE Perdiction models
    \item CLIMAQA / ClimateGPT: Text definition benchmarj
    \item Reasoning model bench: AIME2024 (Math) Math Reasoning and GPQA(Science): A Graduate-Level Google-Proof Q\&A Benchmark
    \item Science + Numerical Value Variation: + Structural Variation: MM-PhyQA: Multimodal Physics Question-Answering  With Multi-Image CoT Prompting
    \item SCIBENCH: Evaluating College-Level Scientific Problem-Solving Abilities of  Large Language Models: that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills.
    \item: The Impact of Reasoning Step Length on Large Language Models: Reasoning step is important in Cot prompt
\end{itemize}
\fi


\section{Benchmark Construction}

\subsection{Benchmark Overview}

% Why MCQ
%\name is a comprehensive multiple-choice question (MCQ) benchmark specifically designed for Atmospheric Science. To enable a more effective evaluation of LLMs, we prioritize the use of MCQs over traditional metrics such as exact match, BLEU, or F1 scores. While these conventional metrics provide useful insights, they often fail to capture the nuanced reasoning capabilities required for complex scientific domains. In contrast, MCQs offer a more straightforward and effective method to evaluate understanding and reasoning by presenting discrete, comparable answer options ~\cite{balepur2024your}.

We introduce a comprehensive multiple-choice question (MCQ) benchmark, \name, specifically designed for atmospheric science to enable more effective evaluation of LLMs. Unlike traditional metrics such as exact match, BLEU, or F1 scores, which primarily assess superficial similarity, %MCQs have been shown to be able to provide a more direct and structured means of evaluating complex reasoning, where these conventional metrics often fail to capture problem-solving ability. In contrast, 
MCQs offer well-defined answer choices, reducing ambiguity and enabling a more precise assessment of model comprehension and logical inference~\cite{balepur2024your}. This structured format ensures a more robust evaluation of LLMs' capabilities in tackling atmospheric science challenges.




% data selection criteria
% We curated questions from authoritative graduate-level textbooks and exam materials, including Atmosphere, Ocean, and Climate Dynamics, Dynamic Meteorology, and Hydrosystems Engineering Exams. 
\textbf{Design principles:}
%To align with the benchmark's focus on evaluating reasoning and interpretative abilities, we meticulously filtered and selected questions to meet the following criteria:
To ensure a rigorous evaluation of LLMs in atmospheric science, we adhere to a set of well-defined principles that emphasize reasoning and interpretative abilities: 
%The selection and filtering of questions are guided by the following criteria, each essential for constructing a benchmark that effectively assesses model competence in this domain:


\begin{itemize}[topsep=5pt, leftmargin=1em]
    \vspace{-0.5em}
    \item \textit{Deep understanding of essential physical equations:} Atmospheric science is governed by fundamental physical equations, and a meaningful evaluation requires that LLMs not only recall these principles but also apply them appropriately in the corresponding contexts. Thus, the questions should be designed to assess both conceptual comprehension and the ability to use these equations in problem-solving, ensuring the benchmark measures true scientific reasoning rather than mere memorization. %Questions should require a robust comprehension of core principles and foundational equations in Atmospheric Science, ensuring that the benchmark evaluates fundamental knowledge as well as the ability to apply these principles in context.

    \vspace{-0.35em}
    \item \textit{Complex reasoning and multi-step logic:} %Problems should involve intricate reasoning processes, such as combining multiple equations, synthesizing data from diverse sources, or employing multi-step logic. This ensures that the benchmark assesses the model's ability to handle both the complexity and depth of reasoning inherent in Atmospheric Science.
    Many real-world atmospheric problems require synthesizing information from multiple sources, integrating equations, and applying multi-step logical reasoning. To reflect these challenges, benchmark questions should be crafted to go beyond simple recall, testing the model’s ability to handle intricate reasoning and dynamic problem-solving scenarios inherent to the field.

    \vspace{-0.35em}
    \item \textit{Appropriate numerical arithmetic processing:} 
    Accurate numerical computation is essential for scientific disciplines, where correct reasoning leads to fixed, verifiable answers. By incorporating numerical problems, we provide a structured and objective evaluation framework, eliminating ambiguities in assessment. This approach also enables seamless integration of reasoning tasks, extending the benchmark’s scope to evaluate mathematical intuition and computational fluency.
    %Incorporating numerical problems allows for the precise assessment of reasoning and computation skills. Correct reasoning and computation yield fixed numerical answers, reducing ambiguity and eliminating the need for subjective evaluations. This approach also facilitates the creation of structured multiple-choice questions and serves as a foundation for extending the benchmark to include symbolic reasoning tasks. The operations roughly involved include: \textit{absolute value}, \textit{trigonometric functions}, \textit{power operations}, \textit{logarithmic operations}, \textit{exponential operations}, \textit{square roots}, \textit{definite integrals}, \textit{derivatives}, and \textit{closed curve integrals}.
    %\TBD{change style}
    \vspace{-0.5em}
\end{itemize}

%Adhering to these principles, the proposed benchmark should be able to ensure that LLMs are evaluated not only on their factual recall but also on their ability to engage in structured reasoning, numerical computation, and deep scientific understanding, the key capabilities, necessary for real-world applications in atmospheric science.



% question Post processing
\subsection{Data Source and Preprocessing}

%We curated questions from authoritative graduate-level textbooks and exam materials. These resources are widely recognized for their rigor and long-standing relevance in graduate-level Atmospheric Science education, ensuring the benchmark incorporates challenging, practical, and accurate content.

%We utilized OCR tool Mathpix~\cite{mathpix} to extract both the questions and their corresponding explanations from the collected materials. For questions with multiple sub-problems or a sequence of problems where solving one is necessary to answer the next, their solution steps are interconnected and form a cohesive progression. For such cases, I have combined them into a single question to enrich the reasoning length and complexity.
%These questions are categorized into distinct but interconnected areas, each representing key domains of Atmospheric Science and its related disciplines:

To ensure the rigor and relevance of the benchmark, we curated questions from authoritative graduate-level textbooks and exam materials widely recognized in atmospheric science education. These sources provide high-quality, well-established content that aligns with the complexity and depth required for evaluating LLMs in this domain.

We leverage Mathpix OCR~\cite{mathpix} to extract both questions and their corresponding explanations from the collected materials. For multi-part problems or sequential questions where solving one step is necessary to proceed to the next, we consolidated them into single questions to enhance the complexity and depth of reasoning required. This approach preserves the logical progression of problem-solving, ensuring a comprehensive assessment of model capabilities.
The benchmark covers distinct sub-fields of atmospheric science, each representing a key subject:


\begin{itemize}[topsep=5pt, leftmargin=1em]
\vspace{-0.5em}
\item \textit{Hydrology} examines the distribution, movement, and properties of water on Earth, including the water cycle, precipitation, rivers, lakes, and groundwater dynamics. %a study of the distribution, movement, and properties of water on Earth, including the water cycle, rivers, lakes, groundwater, and precipitation. 

\vspace{-0.35em}
\item \textit{Atmospheric dynamics} focuses on the motion of the atmosphere, including large-scale weather systems, wind patterns, and governing forces of atmospheric circulation.%focus on the motion of the Earth's atmosphere, including the study of large-scale weather systems, wind patterns, and the forces that govern atmospheric circulation. The questions involve

\vspace{-0.35em}
\item \textit{Atmospheric physics} covers physical processes such as radiation, thermodynamics, cloud formation, and energy transfer within the atmosphere. %pertains to the physical processes in the atmosphere, such as radiation, thermodynamics, cloud formation, and energy transfer.

\vspace{-0.35em}
\item \textit{Geophysics} encompasses the physical processes of the Earth, including its magnetic and gravitational fields, seismic activity, and internal structure.%involve the study of the physical processes and properties of the Earth, including its magnetic and gravitational fields, seismic activity, and the Earth's interior structure.

\vspace{-0.35em}
\item \textit{Physical oceanography} investigates the physical properties and dynamics of ocean water, including currents, waves, tides, and ocean-atmosphere interactions.
 %focus on the physical properties and dynamics of ocean water, including currents, waves, tides, and interactions between the ocean and atmosphere.
\vspace{-0.5em}
\end{itemize}

% Categories



\subsection{Question Generation Framework}
\label{sec:template_based_question_generation_framework}

\iffalse
{
To evaluate the reasoning and problem-solving capabilities of LLMs in Atmospheric Science and assess their potential as reliable assistants in the field, we employ symbolic MCQs generation techniques in light of the GSM-Symbolic framework, and further enhanced by a rule-based mechanism. This approach involves designing template questions with placeholder variables instead of fixed values, allowing each template to be systematically expanded into a desired number of concrete questions through symbolic extensions. 

The rule-based mechanism ensures the validity of the generated questions and aligns them more closely with real-world phenomena. This methodology guarantees both scalability and diversity in the question set while rigorously testing whether LLMs can demonstrate genuine logical reasoning, rather than merely replicating patterns or reasoning steps observed in their training data. The whole construction pipeline is depicted in Figure\ref{fig:generation_framework}, consisting of several stages:
\vspace{-0.1em}
\begin{enumerate}[topsep=5pt, leftmargin=1em]
    \item \textbf{Question Template:} Experts in this domain identified within the questions and replaced actual numerical values with variable placeholders to create reusable question templates (Variables are highlighted with different colors in Figure\ref{fig:generation_framework}).


    \item \textbf{Rule-based Mechanism with Dedicated Granularity:} 
    
    Many variables of questions in \subject are interdependent, and randomly assigning values can lead to invalid or unrealistic scenarios. 
    % To address this, our experts have thoughtfully defined a valid \textit{range} and \textit{significant digits} for each variable to ensure the precision and validity of generated values.
    To address this, our experts have meticulously defined a a valid \textit{range} (\textit{min} and \textit{max}) and \textit{granularity} for each variable, ensuring the precision and validity of the generated values. \textit{Granularity} refers to the smallest step size or interval between possible values within the defined range. By adding control to this feature, we can easily alter significant digits~\footnote{Significant Digits (or Figures) refer to the meaningful digits in a numerical value, representing the precision of a measurement or calculation. Higher precision provides a more detailed representation of the value, reducing rounding errors, but it may also require greater computational resources.} to our variables, potentially providing alternations for the representation or tokens of numerical value, which may potentially impact arithmetic performance~\cite{schwartz2024numerologic, yang2024number, singh2024tokenization}. 
    
    % offering the flexibility to conduct more detailed on Section \ref{sec:design_of_exp} mentioned later.

    % Since in our rule-based mechanism described in Section \ref{sec:template_based_question_generation_framework}, experts defined a set of rules, including the range and granularity of variables for question generation, the precision of variables in the generated questions is higher than in the original source questions. For example, consider the sample question in Figure \ref{fig:generation_framework}. The original question specifies values such as $f0=8.0r, k=1.0$. However, our granularity settings, defined as $0.1$ and $0.01$ respectively, allow for higher-precision variables, resulting in generated values like $f0=5.6, k=1.02$.

    Furthermore, a set of \textit{rule-based} constraints was created for each question to maintain the relationships between variables, for example $t1$ need to be less than $t2$ in the example given in Figure \ref{fig:generation_framework}, ensuring the logical coherence of the generated problems and real-world alignment.





    % \item \textbf{Rule-based Mechanism:} 
    % Many variables in Atmospheric Science are interdependent, and randomly assigning values can result in invalid questions. To address this, experts defined a range (min and max) and granularity (Granularity refers to the smallest step size or interval between possible values in the range from which a random number is generated.) for each variable to ensure the precision and validity of generated values. A set of rule-based constraints was created for each question to maintain the relationships between variables, ensuring the logical coherence of the generated problems and real-world alignment.


    % \begin{table}[h!]
    %     \centering
    %     \scriptsize
    %     \caption{SNPrecision}
    %     \label{tab:sn_precision}
    %     \resizebox{0.3\textwidth}{!}{
    %     \begin{tabular}{lc}
    %         \textbf{Type} & \textbf{Digits} \\
    %         \toprule
    %         Original & $preset - (0\~3)$ \\
    %         Preset & $preset$ \\
    %         Ultra & $preset + 10$ \\
    %         \bottomrule
    %     \end{tabular}
    %     }
    % \end{table}

    % \TBD{add complext arithmatic here}
    \item \textbf{Automatic Problem Solver:} For each question, we manually implemented an Automatic Problem Solver in Python based on the step-by-step explanation provided in the source material. Given any set of valid input variables, this solver can automatically compute the correct answer.
    
    \item \textbf{Distracting Options Generation:} 
    Options that are conceptually similar to the correct answer can be misleading, especially in nuanced questions, leading to potential confusion~\cite{yue2024mmmuprorobustmultidisciplinemultimodal}. To create plausible and misleading incorrect options, we employed three methods:
    \begin{itemize}[topsep=5pt, leftmargin=1em]
        \item Diffusion: Generate a solution using the Automatic Problem Solver by randomly swapping two variables.
        \item Confusion: Generate a solution using the Automatic Problem Solver by randomly alter one variable.
        \item Randomization: Generate a solution using the Automatic Problem Solver by randomly randomly all variables under our rule-based mechanism.
        \item For questions where these methods failed to produce reasonable incorrect options (satisfy our rule-based mechanism), we used multiples ($\times2, \times3, \times4$) of the correct answer as fallback distractors.
    \end{itemize}
    \textbf{Unit and Rounding:} For every question in Automatic Problem Solvers, we manually add the units and set the significant digits for rounding the final answer, ensuring consistency, accuracy, and alignment with real-world scientific standards.

    % \item \textbf{Symbolic Extension:} In prior of question templates, variables, constants and options generated from automatic problem solver, we propose two dataset \TBD{put them in style} AtmosphericBench10 and AtmosphericBench50, that for question templates, generate 10 question instances and 50 question instances perspectively, including the original question as the first instance.
    % \item \textbf{Expert Verification:} Finally, domain experts validated all questions and their variants to ensure the correctness, relevance, and integrity of the dataset.
\end{enumerate}
}
\fi

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{images/pipeline.png}
    \caption{Construction pipeline of our template-based question generation framework. Blocks on the middle left represent the question generation process, where variables are highlighted in different colors. Blocks on the middle right depict the automatic problem solver, which derives the answer from given variables. Bottom blocks illustrate an example of a generated question and its corresponding options.}
    \label{fig:generation_framework}
\end{figure*}


To rigorously evaluate the reasoning and problem-solving capabilities of LLMs in atmospheric science, we employ symbolic MCQ generation techniques inspired by the \texttt{GSM-Symbolic} framework~\cite{mirzadeh2024gsm}, enhanced with a rule-based mechanism. This approach enables the creation of scalable and diverse question sets while ensuring logical coherence and alignment with real-world physical laws. Instead of fixed numerical values, we also design a template-based question perturbation mechanism with placeholder variables, which can be systematically instantiated through symbolic extensions. This ensures that models are tested on genuine reasoning ability rather than pattern matching from the potentially contaminated training data. Figure \ref{fig:generation_framework} illustrates the question construction pipeline as we enumerate below.



\begin{itemize}[topsep=5pt, leftmargin=1em]
\vspace{-0.5em}
\item \textit{Question template construction}: We invite domain experts in atmospheric science to systematically transform selected questions (OCR extracted) into reusable templates. The experts manually identify numerical values within each question and replace them with variable placeholders, ensuring flexibility for symbolic instantiation. These variable placeholders, highlighted in different colors in Figure \ref{fig:generation_framework}, allow for systematic variation while preserving the original scientific integrity of the problem.


\vspace{-0.35em}
\item \textit{Numerical assignment in question template}: We design a rule-based mechanism for valid numerical assignments in each question template. Note that many variables in atmospheric science problems are interdependent, meaning that the inappropriate assignment of some value(s) could lead to unrealistic or invalid physical scenarios. To fulfill this requirement, we ask the experts for each question template to define: (\underline{i}) a valid numerical range (\textit{min}, \textit{max}) for each variable to ensure scientifically plausible values; (\underline{ii}) a granularity parameter (i.e., the smallest step size between values) to control precision, allowing for variation in significant digits --- this variation affects numerical representation, potentially influencing LLM arithmetic performance~\cite{schwartz2024numerologic, yang2024number, singh2024tokenization}; and (\underline{iii}) a set of rule-based constraints that are manually implemented to enforce logical dependencies (e.g., in Figure \ref{fig:generation_framework}, ensuring $t_1 < t_2$). We believe these manual configurations ensure that all generated instances remain scientifically valid while allowing systematic variation in numerical representation.



\vspace{-0.35em}
\item \textit{Automatic problem solver to support value perturbation}: For each question, we utilize \textsc{GPT-4o} to generate an initial Python implementation based on the corresponding explanatory materials (e.g., textbook solutions). This synthesized solution is then manually reviewed, verified, and refined by experts to ensure correctness and adherence to the intended problem-solving methodology. Once validated, the solver can automatically compute the correct answer for any given set of valid input variables, ensuring consistency and scalability in question generation. Note that to ensure consistency, accuracy, and alignment with real-world scientific standards, we also manually assign appropriate units and define significant digits for rounding the final answer in each automatic problem solver. This standardization maintains numerical precision while preventing inconsistencies in representation, ensuring that generated answers adhere to established atmospheric science conventions.


\vspace{-0.35em}
\item \textit{Incorrect option generation}: To effectively assess LLM reasoning, multiple-choice questions require plausible but incorrect distracting options that challenge the model's understanding while avoiding trivial elimination strategies~\cite{yue2024mmmuprorobustmultidisciplinemultimodal}. We design the following mechanisms to generate incorrect options: (\underline{i}) producing an incorrect answer by randomly swapping two variables in the computation; (\underline{ii}) altering a single variable in the equation to generate a close but incorrect result;
(\underline{iii}) randomly assigning all variables within their predefined constraints, ensuring adherence to the rule-based mechanism; and (\underline{vi}) if above three methods fail to generate valid incorrect options (i.e., those satisfying the scientific constraints of the rule-based mechanism), we use a default strategy, where incorrect options are generated as scaled multiples of the correct answer (e.g., $\times2, \times3, \times4$).


%\vspace{-0.35em}
%\item \textit{Proofread physical units and value rounding}: Lastly, to ensure consistency, accuracy, and alignment with real-world scientific standards, we manually assign appropriate units and define significant digits for rounding the final answer in each Automatic Problem Solver. This standardization maintains numerical precision while preventing inconsistencies in representation, ensuring that generated answers adhere to established atmospheric science conventions.

\vspace{-0.5em}
\end{itemize}


















%\subsection{Categorization}
%We summarize the categorization of our benchmarks in Table\ref{category-table}.


\iffalse
\begin{table*}[t]
\caption{Number of topics for various subject types.}
\label{category-table}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{lcccc}
\toprule
Subject Type & Subcategory & Template Number & AtmosphericBench10 & AtmosphericBench50 \\
\midrule
Atmospheric Sciences & Atmospheric Dynamics & 37 & 370 & 1,850 \\
                     & Atmospheric Physics  & 14 & 140 & 700 \\
Earth, Marine and Hydrological Sciences & - & 16 & 160 & 800 \\
\midrule
Total & & 67 & 670 & 3,350 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}
\fi




\section{Evaluation Setup}
\label{sec:evaluation_setup}


We design three main experiments to assess LLM performance on our benchmark, focusing on comprehensive performance comparison among various LLMs (\textit{\underline{Q1}}), reasoning ability variations for the tasks (\textit{\underline{Q2}}), and robustness of the benchmark results for real-world deployment (\textit{\underline{Q3}}). We enumerate these concrete questions below:


\begin{itemize}[topsep=5pt, leftmargin=1em]
\vspace{-0.5em}
\item \textit{\underline{Q1.} How do various state-of-the-art LLMs (i.e., falling into different categories of instruction, math, reasoning, and domain-specific models) comprehensively perform for the proposed atmospheric science benchmark?}

\vspace{-0.35em}
\item \textit{\underline{Q2.} How do the models specialized in reasoning perform during inference time scaling, i.e., how can we improve the model's test accuracy by increasing the length of reasoning tokens?}


\vspace{-0.35em}
\item \textit{\underline{Q3.} How robust are the benchmark results, especially when we variate the scientific numerical precision and the degree of perturbation introduced by symbolic variation?}

\vspace{-0.5em}
\end{itemize}

















\subsection{Constructed Benchmark Dataset}
% By leveraging our Template-based Question Generation Framework, we generate two dataset \TBD{put them in style, no remove this} AtmosphericBench10 and AtmosphericBench50, that for question templates, generate 10 question instances and 50 question instances perspectively, including the original question as the first instance.

%In our template-based question generation framework, the question bank consists of 67 question templates. Each template can generate multiple variations of the original questions, enabling the creation of a fresh and diverse question set. We also incorporate three levels of significant digits: \textit{Low}, \textit{Standard}, and \textit{High}, introduced in \hyperlink{exp:3.1}{Experiment 3.1}.

%For \hyperlink{exp:1}{Experiment 1} and \hyperlink{exp:2}{Experiment 2}, we generate the test case \texttt{ATMSCI-BENCH10}, comprising 10 unique question sets, using the preset significant digits to evaluate all LLMs while ensuring both diversity and robustness.

%In \hyperlink{exp:3.1}{Experiment 3.1}, we generate three 10-question test cases, varying the significant digits of variables to analyze their impact on LLM performance.
%In \hyperlink{exp:3.2}{Experiment 3.2}, we produce a larger test case with 40 question sets, using the preset significant digits, to evaluate symbolic extensions.

%Across all experiments, the first question set in each test case corresponds to the original question templates and serves as a baseline for comparison.


% For end-to-end evaluation, we generate datasets \texttt{ATMSCI-BENCH10} which containing 10 instances per each question templates, ensuring the diversity and robustness. We also generate dataset \texttt{ATMSCI-BENCH10-Original-Precision} with original precision introduce in Section \ref{section:design_of_experiment}. \TBD{ADD description of other experiments}.
% In our experiments, the first instance in each dataset is always the original question dataset to serve as a baseline for comparison.
% for other two experiements, we use ...



To answer the above three questions and systematically evaluate LLMs on atmospheric science tasks, we leverage our question generation framework consisting of 67 question templates to construct a concrete benchmark dataset. As we mentioned in Section \ref{sec:template_based_question_generation_framework}, each template supports multiple variations, ensuring a diverse and scalable question set. We consider three levels of significant digits—\textit{Low}, \textit{Standard}, and \textit{High}—to analyze the impact of numerical precision on LLM performance. 
To answer \textit{\underline{Q1}} (assessing the overall performance of various LLM categories) and \textit{\underline{Q2}} (inference scaling of reasoning models), we construct \texttt{ATMOSSCI-BENCH10}, 
where 10 question test sets are generated, with each test set being constructed from all question templates while maintaining predefined significant digits (Standard). To investigate model robustness \textit{\underline{Q3}}, we construct additional test sets: 
% (\underline{i}) to investigate the influence of scientific numerical precision, we generate three \texttt{ATMOSSCI-BENCH10}, each varying the level of significant digits (\textit{Low}, \textit{Standard}, \textit{High}) to measure the effect of numerical precision;
(\underline{i}) to investigate the influence of scientific numerical precision, we generate two additional \texttt{ATMOSSCI-BENCH10}, each varying the level of significant digits (\textit{Low}, \textit{High}) along with \textit{Standard} level to measure the effect of numerical precision;
(\underline{ii}) to evaluate the robustness under symbolic variation, we generate \texttt{ATMOSSCI-BENCH30}, which consists of 30 test sets for each question template, with controlled symbolic variations to analyze sensitivity to numerical perturbations.

%For all experiments, the first question in each test set corresponds to the original question template and serves as a baseline for comparison. This structured dataset design ensures a rigorous evaluation of LLM performance, reasoning scalability, and robustness in atmospheric science tasks.


















\subsection{Benchmark Models}

To comprehensively assess LLM performance in atmospheric science, we include state-of-the-art LLMs falling into four categories: (\underline{i}) instruction models, (\underline{ii}) reasoning models, (\underline{iii}) math models, and (\underline{iv}) domain-specific models. This categorization enables a structured comparison of general-purpose, specialized, and domain-adapted models.

\textbf{Instruction models}. Instruction-tuned models serve as strong general-purpose baselines, optimized for following prompts and single-step inference tasks, where we include:

\begin{itemize}[topsep=5pt, leftmargin=1em]
\vspace{-0.5em}
\item \textsc{GPT-4o}, \textsc{GPT-4o-mini}~\cite{gpt4o}: OpenAI’s instruction-tuned models.

\vspace{-0.35em}
\item \textsc{Qwen2.5-Instruct} (3B, 7B, 32B, 72B)~\cite{qwen2.5}: Instruction-tuned Qwen models with enhanced abilities.

\vspace{-0.35em}
\item \textsc{Gemma-2-9B-it}, \textsc{Gemma-2-27B}-it~\cite{gemmateam2024gemma2improvingopen}: Google’s open-weight instruction models; along with Gemini-2.0-Flash-Exp~\cite{deepmind_gemini_flash}, the powerful Gemini model optimized for efficiency.

\vspace{-0.35em}
\item \textsc{Llama-3.3-70B-Instruct}, \textsc{Llama-3.1-405B-Instruct-Turbo}~\cite{grattafiori2024llama3herdmodels}: Meta’s widely used instruction models.

\vspace{-0.35em}
\item \textsc{DeepSeek-V3}~\cite{deepseekai2024deepseekv3technicalreport}: Deepseek's latest MoE-based instruction model for general tasks.
\vspace{-0.5em}
\end{itemize}


\textbf{Math models}. Mathematical LLMs specialize in problem-solving, computational reasoning, and theorem proving --- such ability is essential for atmospheric problems. Towards this end, we include:

\begin{itemize}[topsep=5pt, leftmargin=1em]
\vspace{-0.5em}
\item \textsc{DeepSeek-Math-7B-Instruct} and \textsc{DeepSeek-Math-7B-RL}~\cite{shao2024deepseekmath}: Deepseek's math-focused models trained for theorem proving.

\vspace{-0.35em}
\item \textsc{Qwen2.5-Math} (1.5B, 7B, 72B)~\cite{yang2024qwen2}: Qwen's recent models optimized for mathematics.
\vspace{-0.5em}
\end{itemize}


\textbf{Reasoning models}. Reasoning ability is the core technique to improve LLMs' performance over complicated tasks. We include the recent advanced reasoning models focus on deep logical reasoning and multi-step problem-solving:

\begin{itemize}[topsep=5pt, leftmargin=1em]
\vspace{-0.5em}
\item \textsc{GPT-o1}~\cite{openai_learning_to_reason_with_llms}: OpenAI’s reasoning-optimized model.


\vspace{-0.35em} 
\item \textsc{QwQ-32B-Preview}~\cite{qwq-32b-preview}: Reasoning model based on Qwen2.5-32B.


\vspace{-0.35em}
\item \textsc{Gemini-2.0-Flash-Thinking-Exp (01-21)}~\cite{deepmind_gemini_flash_thinking}: Extended Gemini-2.0-Flash-Exp for enhanced reasoning.

\vspace{-0.5em}
\item \textsc{DeepSeek-R1}~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}: Deepseek's RL-trained model for complex problem-solving.

\end{itemize}




\textbf{Domain-specific models}. We also include some models that are specially tailored for climate-related and atmospheric science tasks by supervised fine-tuning or continuous pre-training:

\begin{itemize}[topsep=5pt, leftmargin=1em]
\vspace{-0.5em}
\item \textsc{ClimateGPT-7B}, \textsc{ClimateGPT-70B}~\cite{thulke2024climategpt}: Climate models pre-trained on domain-specific data.
\vspace{-0.5em}
\end{itemize}












\iffalse
{
To ensure a comprehensive evaluation of our benchmark, we categorized the tested models into four distinct groups: \textbf{(i) Instruction Models:} This category includes advanced models that excel in instruction-following tasks, demonstrating strong performance in single-inference scenarios. These models are well-suited for assessing general-purpose instruction-following capabilities. \textbf{(ii) Reasoning Models:} Recent advances in reasoning models align closely with our objectives of evaluating genuine and complex reasoning abilities. Testing there models provides a compassion against instruction models in their ability to handle intricate reasoning tasks. \textbf{(iii) Math Models:} Model specialized in mathematical domains, which require strong problem-solving skills and foundational reasoning abilities, have show significant progress. 

% \TBD{We aim to test whether such models, equipped with mathematical reasoning capabilities, can generalize their skills to other scientific domain, such that Atmospheric science in this paper.} 

\textbf{(iv) Domain-Specific Models:} Lastly, we evaluate domain-specific LLMs in the Atmospheric science field. This allow us to assess whether models trained within the domain knowledge exhibit the reasoning abilities necessary for tasks in this specialized area. Below, we provide details description of the models in each category:
\begin{itemize}[topsep=5pt, leftmargin=1em]
    \item \textbf{Instruction Models:}
    
     \textsc{GPT-4o} and \textsc{GPT-4o-mini}~\cite{gpt4o}: State-of-the-art closed-source instruction models from OpenAI, which represent significant advancements in instruction-following tasks. 
     
     \textsc{Qwen2.5} instruction-tuned models~\cite{qwen2.5}: we evaluate \textsc{Qwen2.5} instruction-tuned models ranging from 3B to 72B parameters, which exhibit substantially enhanced knowledge and capabilities, particularly in coding and mathematical reasoning, compared to their previous series. Among these, the \textsc{Qwen2.5-32B-Instruct} model, serving as the base model for the reasoning model \textsc{QwQ-32B-Preview}.

    textsc{Gemma2}~\cite{gemmateam2024gemma2improvingopen}: We include the Gemma family of lightweight, state-of-the-art open models developed by Google, built using the same cutting-edge research and technology as the Gemini models, including \textsc{Gemma-2-9B-it} and \textsc{Gemma-2-27B-it}. 
    
    \textsc{Gemini-2.0-Flash-Exp}~\cite{deepmind_gemini_flash}: We also include Gemini 2.0 Flash Experimental model as base model of google reasoning model, \textsc{Gemini-2.0-Flash-Thinking-Exp}.
    
    \textsc{Llama3~\cite{grattafiori2024llama3herdmodels} instruction-tuned models}: For Meta's Llama model family, we evaluate the latest instruction-tuned models, including \textsc{Llama-3.3-70B-Instruct} and \textsc{Llama-3.1-405B-Instruct-Turbo}, which are specifically optimized for multilingual dialogue and general-purpose use cases.

    \textsc{DeepSeek-V3~\cite{deepseekai2024deepseekv3technicalreport}}: We assess the performance of \textsc{DeepSeek-V3}, a robust language model based on a Mixture-of-Experts (MoE) architecture. This model demonstrates strong potential in mathematical and scientific benchmarks, including its notable performance on tasks such as \texttt{AIME2024} (Math) and \texttt{GPQA-Diamond}(Science).

    \item \textbf{Math Models:}

    We conducted a comprehensive evaluation of several advanced mathematical language models to assess their capabilities in solving mathematical problems, especially the growing sophistication in leveraging CoT and TIR techniques to solve problems across diverse mathematical domains.

    \textsc{DeepSeek-Math-7B-Instruct~\cite{shao2024deepseekmath}}: builds on the\textsc{DeepSeekMath-Base-7B} with instruction-tuned optimization for enhanced mathematical reasoning, which excels in producing self-contained mathematical solutions and solving problems using formal theorem proving without external tools. 

    \textsc{DeepSeek-Math-7B-RL~\cite{shao2024deepseekmath}}: is trained on the foundation of \textsc{DeepSeek-Math-7B-Instruct} and is refined using the Group Relative Policy Optimization (GRPO) algorithm, further improving its problem-solving abilities. 

    % NuminaMath models utilize a two-stage fine-tuning process to enhance their mathematical reasoning. Stage 1 focuses on Chain-of-Thought (CoT) reasoning using a diverse dataset of natural language problems and solutions, while Stage 2 incorporates Tool-Integrated Reasoning (TIR) to improve computational accuracy and handle complex tasks such as symbolic manipulation. The NuminaMath 7B TIR model achieved significant recognition, winning the first progress prize of the AI Math Olympiad (AIMO) with a score of 29/50 on public and private test sets.

    \textsc{Qwen2.5-Math~\cite{yang2024qwen2}} series: We include 1.5B/7B/72B-Instruct represents an improvement over the earlier \textsc{Qwen2-Math} series by incorporating both CoT and TIR reasoning. 
    % These models support bilingual mathematical problem-solving in Chinese and English, offering a broader application range. 
    The \textsc{Qwen2.5-Math} series models achieve significant performance gains on mathematics benchmarks compared to their predecessors, demonstrating their enhanced reasoning and computational capabilities.

    
    \item \textbf{Reasoning Models:} 
 
    \textsc{GPT-o1}~\cite{openai_learning_to_reason_with_llms}: OpenAI's introduction of a new series of AI models, specifically designed to allocate more time to deliberation before generating responses, represents a significant advancement in the field of artificial intelligence, particularly in its capacity to address complex reasoning tasks.
    
    % The emergence of reasoning models, such as \textsc{GPT-o1}, By increasing inference time and incorporating intricate chains of reasoning, these models exhibit a process like "thinking." They reflect on their own answers, deeply analyze various possibilities, and provide well-considered final responses, achieving higher accuracy in reasoning-intensive tasks. 

    \textsc{QwQ-32B-Preview}\textsc{QwQ}~\cite{qwq-32b-preview}: an open-source experimental research model with 32 billion parameters based on \textsc{Qwen2.5-32B-Instruct}, demonstrates impressive potential in advancing AI reasoning capabilities.  
    
    \textsc{Gemini-2.0-Flash-Thinking-Exp}~\cite{deepmind_gemini_flash_thinking}, built on \textsc{Gemini-2.0-Flash-Exp} with extension of "thinking". This model showcases enhanced consistency between its reasoning processes and outputs, effectively solving complex problems, especially in science and mathematics. In our setting, version \textsc{Gemini-2.0-Flash-Thinking-Exp(01-21)} is adopted.
    
    % Recent updates, such as \textsc{Gemini-2.0-Flash-Thinking-Exp(01-21)}, exhibit further improvements over its predecessor \textsc{Gemini-2.0-Flash-Thinking-Exp(12-19)}, reflecting ongoing advancements in its reasoning abilities. 
    
    \textsc{DeepSeek-R1}~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability} model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as an initial step, delivers remarkable performance in reasoning tasks. As an open-source model, it matches the reasoning performance of proprietary models like \textsc{GPT-o1} in domains such as mathematics, programming, and general reasoning. 
    
    % All the reasoning models evaluated demonstrate substantial improvements over instruction models in benchmarks that assess complex reasoning capabilities, such as \texttt{AIME2024} (mathematics) and \texttt{GPQA-Diamond} (science). Detailed comparisons and results are provided in Table\ref{performance_on_gpqa}.

\item \textbf{Domain-Specific Models:} 
    
    \textsc{ClimateGPT}~\cite{thulke2024climategpt} Among the few existing efforts in a limited number of studies of domain-specific LLMs, \textsc{ClimateGPT} stands out as a relatively advanced model family designed to synthesize interdisciplinary research on climate change. \textsc{ClimateGPT-7B} and \textsc{ClimateGPT-70B} are developed by further pre-training Llama 2 on a specialized dataset of 4.2 billion tokens focused on climate-related content. Notably, \textsc{ClimateGPT-7B} outperforms \textsc{Llama-2-70B-Chat} on climate-specific benchmarks, highlighting its potential in tackling domain-specific challenges.

\end{itemize}
}
\fi




% \begin{table}[h!]
% \centering
% \caption{Model Performance on Benchmarks (Pass@1)}
% \label{performance_on_gpqa}
% \resizebox{0.5\textwidth}{!}{
% \begin{tabular}{lcc}
% \toprule
% \textbf{Model} & \textbf{GPQA-Diamond (Pass@1)} & \textbf{AIME (2024) (Pass@1)} \\ 
% \midrule
% GPT-o1 & 77.3  & 74.4  \\
% DeepSeek-R1 & 71.5 & 79.8 \\
% Gemini-2.0-Flash-Thinking-Exp(01-21)& 74.2 & 73.3  \\
% QwQ & 65.2 & 50.0 \\
% \bottomrule
% \end{tabular}
% }
% \end{table}





% \subsection{Evaluation Setups}
% \begin{itemize}
%     \item GPT o1 and DeepSeek R1 are currently unsupported API parameters: temperature, top\_p. For fair comparison, we adopt the default parameters of each models.
%     \item DeepSeek-R1 API does not support controlling the length of reasoning tokens.
% \end{itemize}
% Under 8K output tokens.



\iffalse{
\subsection{Design of Experiments} \label{sec:design_of_exp}

\begin{itemize}[topsep=5pt, leftmargin=1em]
\item \textbf{\hypertarget{exp:1}{Experiment 1}. Comprehensively Evaluation of LLMs}

Since advancements in frontier models have rendered some existing benchmarks less effective at distinguishing model performance, as discussed in Section \ref{sec:llm_benchmarks}, it is essential to assess whether \name is sufficiently challenging to effectively differentiate between models. To this end, we conduct a comprehensive evaluation on our 10-instances \texttt{ATMSCI-BENCH10}, across four categories of representative open-source and proprietary LLMs: instruction models, reasoning models, math models, and domain-specific climate models. 

% At present, \textsc{GPT-o1} and \textsc{DeepSeek-R1} do not support custom configurations such as temperature, top\_p, or specific reasoning tokens. We conducted a preliminary experiment and found that the reasoning token consumption for all models ranged from 1K to 20K, never exceeding 32K. Additionally, \textsc{DeepSeek-R1} has a fixed maximum reasoning tokens of 32K tokens that cannot be adjusted. Given these conditions, we standardized the maximum context length for \textsc{GPT-o1}, \textsc{QwQ-32B-Preview}, and \textsc{Gemini-2.0-Flash-Thinking-Exp-01-21} to 32K to fully utilize their reasoning capabilities and ensure a fair comparison on our benchmark.
\textsc{GPT-o1} and \textsc{DeepSeek-R1} currently do not allow custom configurations like temperature, top\_p, or specific reasoning tokens. A preliminary experiment showed that the reasoning token consumption for all models ranged from 1K to 20K, never exceeding 32K. Additionally, \textsc{DeepSeek-R1} has a fixed maximum reasoning token limit of 32K, which cannot be adjusted. Since these models do not support adjustable reasoning tokens and their token consumption never exceeds 32K, we standardized the maximum context length for \textsc{GPT-o1}, \textsc{QwQ-32B-Preview}, and \textsc{Gemini-2.0-Flash-Thinking-Exp (01-21)} to 32K. This ensures a fair comparison and fully utilizes their reasoning capabilities on our benchmark.

For instruction models, we set the maximum output tokens to 8K, which is long enough to generate high-quality answers, while keeping all other configurations at their default settings. For domain-specific models \textsc{ClimateGPT-7B} and \textsc{ClimateGPT-70B}, the context length was limited to 4K, in line with their maximum supported capacity.

By comparing model performances under these controlled settings, we can determine whether \name effectively differentiates their capabilities, thus validating its robustness as a benchmark.


\item \textbf{\hypertarget{exp:2}{Experiment 2}. Evaluation on Different Reasoning Length}

For humans, taking time to think carefully often yields better results, but excessively prolonged deliberation does not always guarantee improved performance. Inspired by this observation, we investigate whether the length of reasoning tokens influences the performance of reasoning models. In this experiment, we evaluate the QwQ-32B-Preview model with reasoning token limits ranging from 4K to 30K to examine the impact of reasoning length on performance.



\item \textbf{\hypertarget{exp:3}{Experiment 3}: Robustness in Reasoning LLMs}

To evaluate the robustness of reasoning models, we design our experiment around two aspects: the length of significant digits and symbolic extension.

\textbf{\hypertarget{exp:3.1}{Experiment 3.1} - Influence of Significant digits:} For the first aspect, we hypothesize that increasing the significant digits in numerical variables may increase the difficulty of generated questions compared to the original ones due to the fragility of arithmetic ability of LLMs~\cite{yang2024number, schwartz2024numerologic, singh2024tokenization}, examples are presented in Figure \ref{fig:exmaple_sig_digit}. To test this, we assess three reasoning models—\textsc{DeepSeek-R1}, \textsc{Gemini-2.0-Flash-Thinking-Exp (01-21)}, and \textsc{QwQ-32B-Preview}—across three configurations of significant digits:
\begin{itemize}[topsep=5pt, leftmargin=1em]
\item \textbf{Low}: Low significant digit. Matches the significant digit of the original questions.
\item \textbf{Standard}: Standard significant digit that predefined in our template-based question generation framework, which aims to introduce diversity while maintaining realistic and valid values. This digits are 0–3 digits longer than the "Low" configuration.
\item \textbf{High}: Extends the significant digits by an additional 10 digits beyond the "Standard" configuration to facilitate a more rigorous comparison.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/sig_digit.png}
    \caption{Example from datasets with varying significant digits. The \textit{High} type has 10 more significant digits than the \textit{Standard} type.}
    \label{fig:exmaple_sig_digit}
\end{figure}

\textbf{\hypertarget{exp:3.2}{Experiment 3.2} - Symbolic Extension:} Inspired by the paper GSM-Symbolic~\cite{mirzadeh2024gsm}, which demonstrated that modifying numerical variables in the GSM8K dataset caused a significant performance drop, suggesting that LLMs may rely on pattern matching rather than genuine logical reasoning. We are curious whether this phenomenon persists with advanced reasoning models.
To investigate this, we test the same three reasoning models—\textsc{DeepSeek-R1}, \textsc{Gemini-2.0-Flash-Thinking-Exp (01-21)}, and \textsc{QwQ-32B-Preview}—on our 30-instance dataset to determine whether similar performance drops occur when numerical variables are altered.

% This finding suggests that LLMs may rely on pattern matching rather than genuine logical reasoning. 



% \item \textbf{Experiment 3.1: Impact of the precision(\TBD{or we can use 'granularity' instead of 'precision'}) of numerical variables:}


% We hypothesize that increasing variable precision may inadvertently raise the difficulty of the generated questions compared to the original ones. This could lead to higher accuracy on the original questions relative to those generated with higher precision. To test this hypothesis, we modified our Template-based Question Generation Framework to produce a dataset \texttt{ATMSCI-BENCH10-Original-Precision} with variable precision matching that of the original questions (referred to as "Original Precision").

% We conducted evaluation experiments using the reasoning models \textsc{DeepSeek-R1} and \textsc{QwQ-32B-Preview}. These experiments allowed us to observe the impact of changes in precision on model performance.

% \item \textbf{Experiment 3: Genuine Reasoning Evaluation through Symbolic extension:}
% In the GSM-Symbolic paper, it was revealed that altering numerical variables in the GSM8K dataset led to a significant performance drop, suggesting that LLMs may rely on sophisticated pattern matching rather than genuine logical reasoning. With the introduction of advanced reasoning models, we aim to investigate whether this performance drop phenomenon persists.

% To explore this, we generated a dataset with larger question instances and evaluated the reasoning models \textsc{QwQ-32B-Preview}, \textsc{GPT-o1}, \textsc{DeepSeek-R1}, and \textsc{Gemini-2.0-Flash-Thinking-Exp-01-21}. Due to budget constraints, \textsc{GPT-o1} was evaluated on a 10-instance dataset, while the other three models were tested on 40-instance datasets \TBD{check instance number}.

\end{itemize}
}
\fi


\section{Evaluation Results and Discussion}

% \subsection{\name is difficult enough for assessing the reasoning capability of LLMs}

% subsection: Difficulty enough to distiguishing models / effective at differentiating models

% We present the model's performance using accuracy scores for each areas in \subject, along with an overall accuracy and symbolic standard deviation, summarized in Table \ref{tab:model_comparison}.

% % restate the goal of Effectiveness
% % The primary goal of the experiment was to evaluate the effectiveness of \name as a challenging and discriminative benchmark for assessing the performance of state-of-the-art large language models (LLMs) across four distinct categories. The analysis aims to determine whether the benchmark can effectively highlight performance differences across these categories, given the standardization of evaluation settings. 

% % Different type of LLMs for AstomSci workflow
% % 1. Reasining model is better than Instruction Models. -> our benchmark is difficult enough for assessing the reasoning capability of LLMs, to distinguish them. Low score on instruction non-reasoning model,
% The results highlight three key observations. First, reasoning models (69.55\% - 89.4\% accuracy) consistently outperform instruction models (15.08\% - 64.93\% accuracy) across the benchmark, demonstrating that \name is sufficiently challenging to assess and differentiate the reasoning capabilities of LLMs. 
% Reasoning models also generally have smaller standard deviation of symbolic reasoning performance (SymStd), indicating their more stabilize performance.
% Instruction models, which lack dedicated reasoning mechanisms, achieved notably lower accuracy and exhibited higher variance in performance. For example, \textsc{DeepSeek-R1}, a top-performing reasoning model, achieved 89.4\% accuracy, while the best instruction model, \textsc{Gemini-2.0-Flash-Exp}, reached only 64.93\%. This disparity underscores the benchmark's ability to effectively to discriminate LLMs, especially in assessing reasoning proficiency, emphasizing its utility for benchmarking advanced LLMs.

% % 2. Math model
% Secondly, the Math models show no significant performance improvement over instruction models in the Atmospheric Science field. For example, instruction models like \textsc{Qwen2.5-72B-Instruct-Turbo} achieve average accuracies of 57.61\%, while Math models such as \textsc{Qwen2.5-Math-72B-Instruct} only achieve slightly higher scores 59.85\%. Other 7B Math models perform significantly worse than \textsc{Qwen2.5-7B-Instruct}, highlighting their limitations in comparison.

% % 3. Domain-specific Models have bad performance. there leaves a potention to create a Reasining model in this domain. Our benchmark could be bedrock to create these Domain-specific Reasining model.
% Lastly, domain-specific models, such as \textsc{ClimateGPT-7B} and \textsc{ClimateGPT-70B}, performed poorly, with accuracies of 19.7\% and 27.91\%, respectively. This reflects the limitations of these models in tackling domain-specific tasks, particularly those requiring reasoning capabilities. The poor performance leaves significant potential for the development of domain-specific reasoning models. By serving as a challenging and comprehensive evaluation framework, \name could act as a bedrock for creating and refining reasoning models tailored to specific domains, such as climate science, to bridge these performance gaps and address specialized use cases effectively.



% \textbf{Key Findings}. 
% \begin{itemize}[topsep=5pt, leftmargin=1em]

% \vspace{-0.35em}
% \item Reasoning models significantly outperform all other categories, demonstrating that \name effectively differentiates LLM performance, particularly in assessing complex reasoning abilities.

% \vspace{-0.35em}
% \item Math models show no significant advantage over instruction models, suggesting that mathematical specialization alone does not improve performance in atmospheric science tasks.

% \vspace{-0.35em}
% \item Domain-specific models underperform, revealing a gap in domain-adaptive reasoning capabilities and emphasizing the need for reasoning-specialized models in atmospheric science.

% \end{itemize}


\subsection{End-to-end Evaluation Results}

\textbf{Experimental setup}. 
To comprehensively evaluate the performance of four categories of LLMs on atmospheric science tasks and assess whether \name provides a sufficiently challenging and discriminative evaluation framework, we conduct a systematic performance comparison using our \texttt{ATMOSSCI-BENCH10} benchmark across four representative LLM categories introduced in Section \ref{sec:evaluation_setup}. We standardize experimental settings for each category as:
(\underline{i}) Reasoning models use $32$K max context length, including the reasoning tokens;
(\underline{ii}) Instruction and math models use $8$K max output tokens, balancing response quality and efficiency;
(\underline{iii}) Domain-specific models are set to $4$K context length, the maximum capacity they support.
% By controlling these settings, we can determine whether \name effectively differentiates model capabilities, particularly in reasoning proficiency and domain adaptation.
By controlling these variables, we ensure that performance differences reflect genuine capability gaps rather than confounding factors, allowing us to validate whether \name effectively differentiates model performance and highlights reasoning proficiency.


\textbf{Results and analysis}. 
We present accuracy across different atmospheric science tasks, along with an overall performance comparison in Table~\ref{tab:model_comparison} with three key observations:






% \begin{table*}[h!]
% \centering
% \caption{Model Performance Comparison}
% \label{tab:model_comparison}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{llcccccc}
% \toprule
% \textbf{Category} & \textbf{Model} & \textbf{Instance Count} & \textbf{Acc Mean} & \textbf{Acc Sigma} & \textbf{No Answer Count} & \textbf{Orig. Q Acc Diff} \\
% \midrule
% \multirow{13}{*}{Instruction Models(8K)} 
% & Gemma-2-9B-it & 10 & $15.08$ & $4.07$ & $392$ & $-0.15$ \\ 
% & Gemma-2-27B-it & 10 & $36.72$ & $5.94$ & $115$ & $-8.36$ \\ 
% & Qwen2.5-3B-Instruct & 10 & $32.09$ & $7.71$ & $3$ & $-11.19$ \\ 
% & Qwen2.5-7B-Instruct & 10 & $44.78$ & $5.12$ & $5$ & $-1.5$ \\ 
% & Qwen2.5-32B-Instruct & 10 & $53.73$ & $6.05$ & $31$ & $-1.49$ \\ 
% & Qwen2.5-72B-Instruct-Turbo & 10 & $57.61$ & $4.73$ & $18$ & $8.06$ \\
% & Llama3.1-70B-Instruct & - & - & - & - & - \\
% & Llama-3.3-70B-Instruct & 10 & $52.11$ & $3.73$ & $29$ & $-5.14$ \\ 
% & Llama-3.1-405B-Instruct-Turbo & 10 & $55.52$ & $6.17$ & $0$ & $7.17$ \\ 
% & GPT-4o-mini & 10 & $45.67$ & $4.78$ & $1$ & $0.6$ \\
% & GPT-4o & 10 & $58.36$ & $5.19$ & $6$ & $2.83$ \\ 
% & DeepSeek-3V & 10 & $64.48$ & $6.28$ & $9$ & $5.67$ \\
% & Gemini-2.0-Flash-Exp & 10 & $64.93$ & $4.29$ & $3$ & $3.73$  \\
% \midrule
% \multirow{7}{*}{Reasoning Models} 
% & QwQ-32B-Preview(8K) & 10 & $63.43$ & $6.5$ & $93$ & $2.24$ \\ 
% & QwQ-32B-Preview(16K) & 10 & $69.4$ & $4.63$ & $26$ & $5.23$ \\ 
% & QwQ-32B-Preview(32K) & 10 & $69.55$ & $4.57$ & $31$ & $9.55$ \\ 
% & Gemini-2.0-Flash-Thinking-Exp-12-19\_8K(will rerun in 8K) & 10 & $72.84$ & $4.6$ & $34$ & $-4.18$ \\ 
% % & Gemini-2.0-Flash-Thinking-Exp-01-21\_8K(err) & 10 & $54.33$ & $3.16$ & $252$ & $3.88$ \\ 
% & Gemini-2.0-Flash-Thinking-Exp-01-21\_30K & 10 & $82.69$ & $3.87$ & $5$ & $3.88$ \\ 
% & GPT-o1 & 10 & $87.313$ & $3.32$ & $0$ & $6.72$ \\
% & DeepSeek-R1 & 10 & $89.4$ & $3.48$ & $0$ & $3.14$ \\ 
% \midrule
% \multirow{5}{*}{Math Models} 
% & DeepSeek-Math-7B-RL & 10 & $23.58$ & $4.44$ & $288$ & $-2.68$ \\ 
% & DeepSeek-Math-7B-Instruct & 10 & $30.9$ & $4.04$ & $42$ & $-2.54$ \\ 
% & Qwen2.5-Math-1.5B-Instruct & 10 & $30.45$ & $3.24$ & $60$ & $-0.6$ \\ 
% & Qwen2.5-Math-7B-Instruct & 10 & $35.22$ & $6.14$ & $116$ & $-6.86$ \\ 
% & Qwen2.5-Math-72B-Instruct & 10 & $59.85$ & $6.27$ & $11$ & $11.79$ \\
% % & NuminaMath-7B-CoT & - & - & - & - & - \\
% % & NuminaMath-7B-TIR & - & - & - & - & - \\
% % & NuminaMath-72B-TIR & - & - & - & - & - \\
% % & TBD & - & - & - & - & - \\
% \midrule
% \multirow{2}{*}{Domain-specific Models}
% & ClimateGPT-7B & 10 & $19.7$ & $5.25$ & $141$ & $-4.77$ \\ 
% & ClimateGPT-70B & 10 & $27.91$ & $4.45$ & $82$ & $-5.52$ \\
% % & TBD & - & - & - & - & - \\
% \bottomrule
% \end{tabular}
% }
% \end{table*}


% define color
% \definecolor{skyblue}{RGB}{135, 206, 235} 
% \definecolor{skyblue}{RGB}{102, 155, 188}
% \definecolor{skyblue}{RGB}{189, 224, 254}
% \definecolor{skyblue}{RGB}{207, 234, 241}
\definecolor{skyblue}{RGB}{207, 218, 236}

\begin{table*}[ht]
    \caption{Comparison across four LLMs categories in terms of accuracy (\%) and symbolic standard deviation for Hydrology (Hydro), Atmospheric Dynamics (AtmDyn), Atmospheric Physics (AtmosPhy), Geophysics (GeoPhy), and Physical Oceanography (PhyOcean).}
    \label{tab:model_comparison}
    \centering
    \large
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l l c c c c c | c c}
        \toprule
        \textbf{Category} & \textbf{Model} & \textbf{Hydro} & \textbf{AtmDyn} & \textbf{AtmosPhy} & \textbf{GeoPhy} & \textbf{PhyOcean} & \textbf{Overall Acc} & \textbf{SymStd.} \\
        \midrule
        \multirow{12}{*}{\textbf{Instruction Models}} 
        & Gemma-2-9B-it & 24.0 & 13.78 & 15.71 & 11.43 & 20.0 & 15.08 & 4.07 \\
        & \cellcolor{skyblue}Gemma-2-27B-it & \cellcolor{skyblue}56.0 & \cellcolor{skyblue}29.73 & \cellcolor{skyblue}45.71 & \cellcolor{skyblue}41.43 & \cellcolor{skyblue}37.5 & \cellcolor{skyblue}36.72 & \cellcolor{skyblue}5.94 \\
        & Qwen2.5-3B-Instruct & 46.0 & 29.19 & 34.28 & 30.0 & 37.5 & 32.09 & 7.71 \\
        & \cellcolor{skyblue}Qwen2.5-7B-Instruct & \cellcolor{skyblue}62.0 & \cellcolor{skyblue}38.92 & \cellcolor{skyblue}51.43 & \cellcolor{skyblue}51.43 & \cellcolor{skyblue}42.5 & \cellcolor{skyblue}44.78 & \cellcolor{skyblue}5.12 \\
        & Qwen2.5-32B-Instruct & 58.0 & 47.3 & 64.28 & 62.86 & 55.0 & 53.73 & 6.05 \\
        & \cellcolor{skyblue}Qwen2.5-72B-Instruct-Turbo & \cellcolor{skyblue}72.0 & \cellcolor{skyblue}50.0 & \cellcolor{skyblue}77.86 & \cellcolor{skyblue}44.29 & \cellcolor{skyblue}62.5 & \cellcolor{skyblue}57.61 & \cellcolor{skyblue}4.73 \\
        & Llama-3.3-70B-Instruct & 78.0 & 42.94 & 67.14 & 51.91 & 52.5 & 52.11 & 3.73 \\
        & \cellcolor{skyblue}Llama-3.1-405B-Instruct-Turbo & \cellcolor{skyblue}72.0 & \cellcolor{skyblue}48.92 & \cellcolor{skyblue}64.29 & \cellcolor{skyblue}57.14 & \cellcolor{skyblue}62.5 & \cellcolor{skyblue}55.52 & \cellcolor{skyblue}6.17 \\
        & GPT-4o-mini & 48.0 & 42.16 & 58.57 & 40.0 & 40.0 & 45.67 & 4.78 \\
        & \cellcolor{skyblue}GPT-4o & \cellcolor{skyblue}68.0 & \cellcolor{skyblue}51.08 & \cellcolor{skyblue}74.28 & \cellcolor{skyblue}60.0 & \cellcolor{skyblue}55.0 & \cellcolor{skyblue}58.36 & \cellcolor{skyblue}5.19 \\
        & Gemini-2.0-Flash-Exp & 90.0 & 58.11 & 68.57 & 77.14 & 62.5 & 64.93 & 4.29 \\
        & \cellcolor{skyblue}Deepseek-V3 & \cellcolor{skyblue}94.0 & \cellcolor{skyblue}57.3 & \cellcolor{skyblue}73.57 & \cellcolor{skyblue}64.28 & \cellcolor{skyblue}62.5 & \cellcolor{skyblue}64.48 & \cellcolor{skyblue}6.28 \\
        \midrule
        \multirow{5}{*}{\textbf{Reasoning Models}} 
        & QwQ-32B-Preview & 88.0 & 60.27 & 87.86 & 74.28 & 60.0 & 69.55 & 4.57 \\
        % & \cellcolor{skyblue}Gemini-2.0-Flash-Thinking-Exp-12-19(TBD) & \cellcolor{skyblue}88.0 & \cellcolor{skyblue}67.3 & \cellcolor{skyblue}82.86 & \cellcolor{skyblue}74.28 & \cellcolor{skyblue}67.5 & \cellcolor{skyblue}72.84 & \cellcolor{skyblue}4.6 \\
        & \cellcolor{skyblue}Gemini-2.0-Flash-Thinking-Exp (01-21) & \cellcolor{skyblue}100.0 & \cellcolor{skyblue}78.11 & \cellcolor{skyblue}85.0 & \cellcolor{skyblue}91.43 & \cellcolor{skyblue}80.0 & \cellcolor{skyblue}82.69 & \cellcolor{skyblue}3.87 \\
        & GPT-o1 & 100.0 & 82.7 & 92.14 & 92.86 & 87.5 & 87.31 & 3.32 \\
        & \cellcolor{skyblue}Deepseek-R1 & \cellcolor{skyblue}98.0 & \cellcolor{skyblue}85.68 & \cellcolor{skyblue}95.0 & \cellcolor{skyblue}95.71 & \cellcolor{skyblue}82.5 & \cellcolor{skyblue}89.4 & \cellcolor{skyblue}3.48 \\
        \midrule
        \multirow{5}{*}{\textbf{Math Models}} 
        & Deepseek-Math-7B-RL & 22.0 & 20.54 & 27.86 & 24.29 & 37.5 & 23.58 & 4.44 \\
        & \cellcolor{skyblue}Deepseek-Math-7B-Instruct & \cellcolor{skyblue}36.0 & \cellcolor{skyblue}28.38 & \cellcolor{skyblue}33.57 & \cellcolor{skyblue}30.0 & \cellcolor{skyblue}40.0 & \cellcolor{skyblue}30.9 & \cellcolor{skyblue}4.04 \\
        & Qwen2.5-Math-1.5B-Instruct & 50.0 & 30.0 & 22.86 & 34.29 & 30.0 & 30.45 & 3.24 \\
        & \cellcolor{skyblue}Qwen2.5-Math-7B-Instruct & \cellcolor{skyblue}54.0 & \cellcolor{skyblue}31.35 & \cellcolor{skyblue}39.28 & \cellcolor{skyblue}35.71 & \cellcolor{skyblue}32.5 & \cellcolor{skyblue}35.22 & \cellcolor{skyblue}6.14 \\
        & Qwen2.5-Math-72B-Instruct & 70.0 & 54.87 & 73.57 & 62.86 & 40.0 & 59.85 & 6.27 \\
        \midrule
        \multirow{2}{*}{\textbf{Domain-Specific Models}} 
        & \cellcolor{skyblue}ClimateGPT-7B & \cellcolor{skyblue}26.0 & \cellcolor{skyblue}18.65 & \cellcolor{skyblue}21.43 & \cellcolor{skyblue}11.43 & \cellcolor{skyblue}30.0 & \cellcolor{skyblue}19.7 & \cellcolor{skyblue}5.25 \\
        & ClimateGPT-70B & 24.0 & 25.4 & 30.0 & 40.0 & 27.5 & 27.91 & 4.45 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table*}


\begin{itemize}[topsep=5pt, leftmargin=1em]

\vspace{-0.5em}
% \item \textbf{\name Effectively Differentiates LLM Performance, with Reasoning Models Significantly Outperforming Others}. 
\item \textit{\name effectively differentiates LLM performance across categories, with reasoning models demonstrating the highest proficiency.}
The results confirm that our benchmark successfully distinguishes LLM performance, particularly in assessing reasoning proficiency. Reasoning models (69.55\% - 89.4\%) significantly outperform instruction models (15.08\% - 64.93\%), demonstrating superior consistency with lower symbolic reasoning standard deviation (SymStd)~\cite{mirzadeh2024gsm}. \textsc{DeepSeek-R1}, the best-performing reasoning model, achieves 89.4\% accuracy, while the top instruction model, \textsc{Gemini-2.0-Flash-Exp}, only reaches 64.93\%, a substantial 24.47\% gap. This clear performance variance underscores \name’s ability to challenge advanced LLMs, ensuring that strong reasoning skills translate into measurable performance gains.

\vspace{-0.35em}
\item \textit{Math models do not show a clear advantage over instruction models.}
Despite their specialization, math models do not significantly outperform instruction models, suggesting that mathematical optimization alone is insufficient for solving \subject~challenges. %\textsc{Qwen2.5-72B-Instruct-Turbo} (57.61\%) and \textsc{Qwen2.5-Math-72B-Instruct} (59.85\%) show only a 2.24\% difference, indicating minimal impact from math-specific training.

% Smaller math models (7B) perform even worse than instruction-trained counterparts, reinforcing that general reasoning, rather than mathematical computation, is key to excelling in atmospheric science tasks.

\vspace{-0.35em}
\item \textit{Domain-specific models underperform despite climate specialization, indicating a need for reasoning-augmented approaches.}
Domain-specific models perform poorly despite their climate specialization, with \textsc{ClimateGPT-7B} and \textsc{ClimateGPT-70B} achieving only 19.7\% and 27.91\% accuracy, respectively. Their inability suggests that specialized training alone may not compensate for weak reasoning capabilities. This highlights there is a need for a reasoning model in this \subject. \name provides a rigorous evaluation framework to guide the development of such reasoning-augmented domain models, addressing the limitations of existing approaches.
\vspace{-0.5em}
\end{itemize}

In conclusion, to answer \textit{\underline{Q1}} regarding the overall performance of various LLM categories, our evaluation reveals that \textit{reasoning models significantly outperform instruction, math, and domain-specific models in atmospheric science tasks, highlighting their superior adaptability to advanced reasoning challenges, while domain-specific models struggle despite specialized training.} 




\subsection{Inference Scaling for Reasoning Models}

% The analysis of the results presented in  Table \ref{tab:reasoning_step_comparison} or Figure \ref{fig:reasoning_step_comparison} reveals a clear trend regarding the impact of reasoning token length on the performance of the QwQ-32B-Preview model. As the reasoning token limit increases, the model's accuracy improves significantly, rising from 37.46\% at 4K tokens to 63.43\% at 8K tokens, and further to 69.4\% at 16K tokens. However, the marginal gains diminish beyond this point, with only a slight improvement to 69.55\% at 30K tokens. This suggests that extending the reasoning length beyond a certain threshold yields diminishing returns in terms of accuracy. Interestingly, the standard deviation of symbolic reasoning performance (SymStd) shows a non-linear trend, peaking at 6.5 for 8K tokens and then decreasing at higher token limits, indicating that longer reasoning lengths may stabilize performance. These findings highlight a balance between providing sufficient reasoning capacity and avoiding unnecessary computational overhead, as excessive reasoning tokens do not proportionally enhance model performance.



\textbf{Experimental setup}. 
To answer \textit{\underline{Q2}}, i.e., whether increasing the length of reasoning tokens improves the performance of reasoning models, we conduct an inference time scaling evaluation on \texttt{ATMOSSCI-BENCH10} using the \textsc{QwQ-32B-Preview} model, varying its reasoning token limits from $4$K up to $32$K. %This experiment is inspired by the human cognitive process, where extended deliberation can enhance decision-making, but excessive reasoning may yield diminishing returns. 
By systematically increasing the token limit, we aim to determine whether a longer inference process leads to higher accuracy and whether there exists an optimal threshold beyond which additional tokens provide minimal benefit.

\textbf{Results and analysis}. 
As shown in Figure \ref{fig:reasoning_step_comparison}, increasing the reasoning token limit generally improves model accuracy, but the gains diminish beyond a certain threshold. Across all evaluated metrics, including overall accuracy, performance is consistently lower at $4$K tokens, improves significantly at $8$K and $16$K tokens, and then plateaus beyond $16$K tokens, with $32$K tokens offering only marginal improvement. This trend suggests that while extending reasoning length enhances model performance up to a certain point, it further increases yield, diminishing returns without proportional accuracy gains. %The consistency of this pattern across all categories indicates that the model’s reasoning ability benefits from additional tokens but saturates beyond a critical threshold.
Thus, our answer to \textit{\underline{Q2}} is that \textit{increasing the length of reasoning tokens improves model accuracy up to 16K tokens, beyond which performance gains diminish, indicating an optimal threshold for inference time scaling.}
%These findings emphasize the need to optimize reasoning token limits rather than extending them indefinitely, ensuring a balance between reasoning capacity and computational efficiency.




\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{images/reasoning_step_bar.png}
    \caption{\textbf{Model Reasoning Step Comparison} in terms of accuracy(\%) on QwQ-32B-Preview models ranging from 4K up to 32K.}% (indicated by colors in the legend), in areas of Hydrology (Hydro), Atmospheric Dynamics (AtmDyn), Atmospheric Physics (AtmosPhy), Geophysics (GeoPhy), and Physical Oceanography (PhyOcean).}
    \label{fig:reasoning_step_comparison}
    \vspace{-1.0em}
\end{figure}


% \begin{table}[H]
% \centering
% \caption{\textbf{Model Reasoning Step Comparison} in terms of accuracy(\%) and symbolic standard deviation on QwQ-32B-Preview models ranging from 4K to 32K.}
% \label{tab:reasoning_step_comparison}
% \small
%     \begin{adjustbox}{max width=\textwidth}
%     \begin{tabular}{llccc}
%     \toprule
%     \textbf{Model} & \textbf{Accuracy} & \textbf{SymStd}\\
%     \midrule
%     QwQ-32B-Preview(4K) & $37.46$ & $3.82$ \\
%     QwQ-32B-Preview(8K) & $63.43$ & $6.5$ \\ 
%     QwQ-32B-Preview(16K) & $69.4$ & $4.63$  \\ 
%     QwQ-32B-Preview(30K) & $69.55$ & $4.57$ \\ 
%     \bottomrule
%     \end{tabular}
%     \end{adjustbox}
% \end{table}


\subsection{Robustness of \name}


To evaluate the robustness of \name (\textit{\underline{Q3}}), we conduct experiments to assess: (\underline{i}) robustness against variations in numerical precision and (\underline{ii}) robustness to different degrees of perturbation introduced by symbolic variation.




\begin{table}[ht!]
\centering
\small
\caption{Performance Comparison Among Different Significant digits in terms of accuracy (\%) and symbolic standard deviation.}
    \label{tab:sig_dig_comparison}
    \begin{adjustbox}{max width=0.5\textwidth}
    \begin{tabular}{llccc}
    \toprule
    \textbf{Model} & \textbf{Sig. Digits} & \textbf{Overall Acc} & \textbf{SymStd.} \\
    \midrule
    \multirow{3}{*}{Qwen2.5-7B-Instruct}
    & Low & $43.58$ & $3.84$ \\
    & \cellcolor{skyblue}Standard & \cellcolor{skyblue}$44.78$ & \cellcolor{skyblue}$5.12$ \\
    & High & $38.51$ & $6.56$ \\
    \midrule
    \multirow{3}{*}{Qwen2.5-Math-7B-Instruct}
    & \cellcolor{skyblue}Low & \cellcolor{skyblue}$36.12$ &\cellcolor{skyblue} $4.03$ \\
    & Standard & $35.22$ & $6.14$ \\
    & \cellcolor{skyblue}High & \cellcolor{skyblue}$35.82$ & \cellcolor{skyblue}$4.1$ \\
    \midrule
    \multirow{3}{*}{QwQ-32B-Preview}
    & Low & $68.51$ & $4.42$ \\
    & \cellcolor{skyblue}Standard & \cellcolor{skyblue}$69.55$ & \cellcolor{skyblue}$4.57$ \\
    & High & $71.34$ & $4.03$ \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}


\textbf{Experiments for numerical precision}. (\textit{\underline{Setup}}).
We hypothesize that increasing the significant digits in numerical variables may increase the difficulty of test sets or deteriorate the performance due to the fragility of the arithmetic ability of LLMs~\cite{yang2024number, schwartz2024numerologic, singh2024tokenization}.
%, examples are presented in Figure \ref{fig:exmaple_sig_digit}. 
% To test this, we assess three reasoning models—\textsc{DeepSeek-R1}, \textsc{Gemini-2.0-Flash-Thinking-Exp-01-21}, and \textsc{QwQ-32B-Preview}—across three configurations of significant digits.
To test this, we conduct our experiment across three configurations of significant digits, assessing the \textsc{Qwen}-class models from three different LLM categories, including \textsc{Qwen2.5-7B-Instruct},  \textsc{Qwen2.5-Math-7B-Instruct} and \textsc{QwQ-32B-Preview}.
We variate three degrees of scientific numerical precision: (\underline{i}) \textit{Low} significant digit, where digits are 0–3 digits shorter than the ``Standard" configuration; (\underline{ii}) \textit{Standard} significant digit that predefined in our template-based question generation framework, which aims to introduce diversity while maintaining realistic and valid values; and  (\underline{iii}) \textit{High} significant digit that extends the significant digits by an additional 10 digits beyond the ``Standard" configuration to facilitate a more rigorous comparison. 

(\textit{\underline{Results and analysis}}). 
Table \ref{tab:sig_dig_comparison} reveals distinct performance trends among the three models when varying the number of significant digits. Among the three models, the reasoning model 	\textsc{QwQ-32B-Preview} indicates performance improvement with increasing numerical precision, indicating its rigorousness in scientific tasks---where real analytics could be enabled instead of simple pattern matching. The math-specialized model, i.e., \textsc{Qwen2.5-Math-7B-Instruct}, remains stable across different levels of significant digits, with minimal variations in accuracy, potentially reflecting its specialization in numerical processing. In contrast, the standard instruction model, i.e., \textsc{Qwen2.5-7B-Instruct}, suffers from a significant drop in accuracy as numerical precision increases, suggesting its risk of reliance on pattern matching rather than desired numerical reasoning, making it more fragile with more precise scientific numerical computation.
Conclusively, in terms of robustness raised in \textit{\underline{Q3}}, we believe that the \textit{reasoning model demonstrates higher resilience to extended numerical sequences, while the instruction model exhibits significant sensitivity to variations in numerical precision.}



%Q3. How robust are the benchmark results, especially when we variate the scientific numerical precision and the degree of perturbation introduced by symbolic variation?




% Does the precision of numerical variables impact results?

% The results, depicted in Table \ref{tab:precision_comparison}, indicate a significant difference between the performance on questions with Original Precision and those with Higher Precision. These findings suggest that while precision differences may impact question difficulty, they are not the primary factor affecting model performance.



% \begin{table}[ht!]
% \centering
% \small
% \caption{Performance Comparison Among Different Significant digits in terms of accuracy (\%) and symbolic standard deviation.}
%     \label{tab:sig_dig_comparison}
%     \begin{adjustbox}{max width=0.5\textwidth}
%     \begin{tabular}{llccc}
%     \toprule
%     \textbf{Model} & \textbf{Sig. Digits} & \textbf{Overall Acc} & \textbf{SymStd.} \\
%     \midrule
%     \multirow{3}{*}{QwQ-32B-Preview(30K)}
%     & Low & $68.51$ & $4.42$ \\
%     & Standard & $69.55$ & $4.57$ \\
%     & High & $71.34$ & $4.03$ \\
%     \midrule
%     \multirow{3}{*}{DeepSeek-R1}
%     & Low & $88.21$ & $3.48$ \\
%     & Standard & $89.4$ & $3.48$ \\
%     & High & $$ & $$ \\
%     \midrule
%     \multirow{3}{*}{Gemini-2.0-Flash-Thinking-Exp-01-21}
%     & Low & $84.03$ & $4.51$ \\
%     & Standard & $82.69$ & $3.87$ \\
%     & High & $81.19$ & $3.39$ \\
%     \midrule
%     \multirow{3}{*}{Qwen2.5-7B-Instruct}
%     & Low & $43.58$ & $3.84$ \\
%     & Standard & $44.78$ & $5.12$ \\
%     & High & $38.51$ & $6.56$ \\
%     \midrule
%     \multirow{3}{*}{Qwen2.5-Math-7B-Instruct}
%     & Low & $36.12$ & $4.03$ \\
%     & Standard & $35.22$ & $6.14$ \\
%     & High & $35.82$ & $4.1$ \\
%     \bottomrule
%     \end{tabular}
%     \end{adjustbox}
% \end{table}




% \item \textbf{Genuine Reasoning Analysis through Symbolic Extension}
%\subsubsection{Genuine Reasoning Analysis through Symbolic Extension}
% \subsubsection{Does Genuine Reasoning Analysis through Symbolic Extension}

% We observe that reasoning models exhibit lower variance compared to their base instruction models, indicating that these models are more consistent and reliable in their responses, thereby demonstrating robustness. 

\textbf{Experiments for symbolic variation}. (\textit{\underline{Setup}}).
Inspired by \texttt{GSM-Symbolic}~\cite{mirzadeh2024gsm}, which demonstrates that modifying numerical variables in the \texttt{GSM8K} dataset led to significant performance drops, suggesting that LLMs may rely on pattern matching rather than genuine logical reasoning. We aim to assess the robustness of advanced reasoning models under varying degrees of symbolic perturbation.
To examine this, we evaluate three reasoning models—\textsc{DeepSeek-R1}, \textsc{Gemini-2.0-Flash-Thinking-Exp (01-21)}, and \textsc{QwQ-32B-Preview}—on \texttt{ASTMOSSCI-BENCH30}, consisting of 30 question test sets that vary in numerical variables. We systematically modify numerical variables within a scientifically reasonable range, introducing controlled variations to assess whether performance remains stable or degrades significantly with perturbation.

\vspace{-0.5em}
(\textit{\underline{Results and analysis}}). 
Figure \ref{fig:reasoning_model_distribution} illustrates the empirical performance distribution of reasoning models on \texttt{ASTMOSSCI-BENCH30}. We observe that for both \textsc{DeepSeek-R1} and \textsc{QwQ-32B-Preview}, the accuracy of the original question set (dashed line in Figure \ref{fig:reasoning_model_distribution}) is approximately one standard deviation away from the mean accuracy across perturbed instances, indicating a notable shift. In contrast, \textsc{Gemini-2.0-Flash-Thinking-Exp (01-21)} exhibits a more minor deviation, with accuracy within one standard deviation but skewed toward the right side of the distribution.
As the degree of numerical perturbation increases, we observe a consistent downward trend in model performance, reinforcing the notion that LLMs, even those specialized in reasoning, could still struggle with symbolic variation. To answer \textit{\underline{Q3}} w.r.t symbolic variation, the results indicate that \textit{the reasoning models evaluated in our benchmark could still be under the risk of insufficient robustness under symbolic perturbation, as increasing the degree of variation leads to significant and often unpredictable drops in accuracy.} This suggests that our benchmark effectively reveals weaknesses in reasoning models' ability to generalize beyond pattern-matching strategies.
Furthermore, these findings could imply that the tested reasoning models are likely trained on in-distribution data sources, such as standard textbooks in \subject. Their performance may thus be heavily influenced by pattern-matching within familiar distributions rather than true logical reasoning. The observed performance degradation under perturbation further highlights the need for robust evaluation frameworks that test models beyond their training distributions. 



\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{images/reasoning_model_distribution.png}
    % \captionsetup{aboveskip=0pt, belowskip=0pt}
    \caption{Performance distribution among reasoning LLMs (\textsc{DeepSeek-R1}, \textsc{QwQ-32B-Preview} and \textsc{gemini-2.0-flash-thinking-exp}) on \texttt{ASTMOSSCI-BENCH30}. The Y-axis represents the frequency of the symbolic test sets achieving the accuracy shown on the X-axis. The black vertical dash lines denote the accuracy of the original question set. \textsc{gemini-2.0-flash-thinking-exp} refers to the \textsc{Exp-01-21} version.}
    \label{fig:reasoning_model_distribution}
    \vspace{-1.0em}
\end{figure}





% \begin{table*}[h!]
% \centering
% \caption{Performance Comparison Among Model Families}
% \label{tab:family_comparison}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{llcccccc}
% \toprule
% \textbf{Family} & \textbf{Model} & \textbf{Instance Count} & \textbf{Acc Mean} & \textbf{Acc Sigma} & \textbf{No Answer Count} & \textbf{Orig. Q Acc Diff} \\
% \midrule
% \multirow{3}{*}{GPT Family} 
% & GPT-4o-mini & 10 & $45.67$ & $4.78$ & $1$ & $0.6$ \\
% & GPT-4o & 10 & $58.36$ & $5.19$ & $6$ & $2.83$ \\
% & GPT-o1 & 10 & $87.313$ & $3.32$ & $0$ & $6.72$ \\
% \midrule
% \multirow{2}{*}{Qwen/QwQ Family}
% & Qwen2.5-32B-Instruct & 10 & $53.73$ & $6.05$ & $31$ & $-1.49$ \\
% % & QwQ-32B-Preview(16K) & 10 & $69.4$ & $4.63$ & $26$ & $5.23$ \\
% & QwQ-32B-Preview(32K) & 10 & $69.55$ & $4.57$ & $31$ & $9.55$ \\
% \midrule
% \multirow{3}{*}{Gemini Family}
% & Gemini-2.0-Flash-Exp & 10 & $64.93$ & $4.29$ & $3$ & $3.73$  \\
% & Gemini-2.0-Flash-Thinking-Exp-12-19\_8K(will fix) & 10 & $72.84$ & $4.6$ & $34$ & $-4.18$ \\
% & Gemini-2.0-Flash-Thinking-Exp-01-21\_30K & 10 & $82.69$ & $3.87$ & $5$ & $3.88$ \\
% \midrule
% \multirow{2}{*}{DeepSeek Family}
% & DeepSeek-V3 & 10 & $64.48$ & $6.28$ & $9$ & $5.67$ \\
% & DeepSeek-R1 & 10 & $89.4$ & $3.48$ & $0$ & $3.14$ \\
% \bottomrule
% \end{tabular}
% }
% \end{table*}



\section{Conclusion}

In this paper, we introduced \name, a novel benchmark designed to systematically evaluate the reasoning and problem-solving capabilities of LLMs in atmospheric science. Our benchmark covers five core categories—hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanography—through a scalable, template-based question generation framework that ensures diversity and complexity in multiple-choice question assessments. By conducting a comprehensive evaluation across four distinct model categories—instruction-tuned models, advanced reasoning models, math-augmented models, and domain-specific climate models—we provide key insights into the strengths and limitations of LLMs in addressing atmospheric science problems. Our findings highlight that reasoning models outperform other categories, demonstrating stronger problem-solving and reasoning capabilities in the domain of atmospheric science. This also underscores the benchmark's effectiveness in differentiating models. %Additionally, we observe that performance improves with increased reasoning token length up to 16K tokens but plateaus thereafter and that models remain sensitive to symbolic perturbations, affecting their robustness.
We believe that \name (where all the implementations are fully open-sourced) can serve as an essential step toward advancing the application of LLMs in climate-related decision-making by offering a standardized and rigorous evaluation framework for future research. 



\clearpage



