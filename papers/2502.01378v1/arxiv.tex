\documentclass{article}

\pdfoutput=1

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\usepackage{algpseudocode}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{array}
\usepackage{colortbl}
\usepackage{tabularx}         % For adjustable-width columns
\usepackage{adjustbox}        % For \adjustbox
\usepackage{arydshln}
\usepackage{xspace}
\usepackage{algorithm}
\usepackage{enumitem}
\usepackage[numbers, sort&compress]{natbib}

\usepackage{arxiv}

% % Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\definecolor{skyblue}{RGB}{229,242,247}
\definecolor{blue}{RGB}{66, 109, 181}
\newcommand{\cgd}[1]{{\color{blue}#1}}

\renewcommand{\algorithmicrequire}{\textbf{Input: }}
\newcommand{\etc}{\emph{etc.}\xspace}
\newcommand{\ie}{\emph{i.e.}\xspace}
\newcommand{\eg}{\emph{e.g.}\xspace}
\newcommand{\celora}{{CE-LoRA}\xspace}
\newcommand{\textub}[1]{\underline{\textbf{#1}}}

% \newcommand\blfootnote[1]{%
%   \begingroup
%   \renewcommand\thefootnote{}\footnote{#1}%
%   \addtocounter{footnote}{-1}%
%   \endgroup
% }

\newcommand{\blfootnote}[1]{%
  \begingroup
  \renewcommand{\thefootnote}{}% 清除编号
  \footnotetext{#1}% 直接添加脚注内容（不生成上标标记）
  \addtocounter{footnote}{-1}%
  \endgroup
}


\usepackage{abstract}
\renewcommand{\abstractnamefont}{\normalfont\Large\bfseries}
\renewcommand{\abstracttextfont}{\normalfont\normalsize}

\newcommand\algname{\texttt{AC-SGD}\xspace}







\title{\celora: Computation-Efficient LoRA \\ Fine-Tuning for Language Models}

\author{%
  Guanduo~Chen$^{* \dag}$\\
  Fudan University\\
  \texttt{gdchen22@m.fudan.edu.cn} \\
  \And
  Yutong~He$^{\dag}$\\
  Peking University\\
  \texttt{yutonghe@pku.edu.cn} \\
  \And
  Yipeng~Hu\\
  Peking University\\
  \texttt{yipenghu@pku.edu.cn} \\
  \And
  Kun~Yuan$^\ddagger$\\
  Peking University\\
  \texttt{kunyuan@pku.edu.cn} \\
  \And
  Binhang Yuan$^\ddagger$\\
  HKUST \\
  \texttt{biyuan@ust.hk}\\
}


\begin{document}
\maketitle

\blfootnote{$^*$ Work done when the author was working as a research assistant under the supervision of Binhang Yuan.}
\blfootnote{$^\dag$ Both authors contributed equally to this research.}
\blfootnote{ $^\dagger$ Coressponding author.}



\begin{abstract}
Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory consumption during fine-tuning, its impact on computational cost reduction is limited. This paper identifies the computation of activation gradients as the primary bottleneck in LoRA's backward propagation and introduces the \underline{\textbf{C}}omputation-\underline{\textbf{E}}fficient \underline{\textbf{LoRA}} (\textbf{CE-LoRA}) algorithm, which enhances computational efficiency while preserving memory efficiency. \celora leverages two key techniques: Approximated Matrix Multiplication, which replaces dense multiplications of large and complete matrices with sparse multiplications involving only critical rows and columns, and the Double-LoRA technique, which  reduces error propagation in activation gradients. Theoretically, \celora converges at the same rate as LoRA, \( \mathcal{O}(1/\sqrt{T}) \), where $T$ is the number of iterations. Empirical evaluations confirm that \celora significantly reduces computational costs compared to LoRA without notable performance degradation.
\end{abstract}

\input{papers/intro}

\input{papers/preliminary}

\input{papers/CELoRA}

\input{papers/analysis}

\input{papers/experiment}
% \vspace{-5mm}
\input{papers/related_work}

\input{papers/conclusion}

% \input{papers/impact_statement}

\bibliographystyle{unsrt}
% \bibliographystyle{plainnat}
\bibliography{ref}


\clearpage
\appendix
\input{appendix/missing_proofs.tex}
% \input{appendix/delta_experiment_settings.tex}

%\section{Appendix 1}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\input{checklist}

\end{document}