\section{Experiments}

In this section, we present a comprehensive set of experiments to evaluate the convergence performance and computational efficiency of \celora, and compare it against the baseline method.


\subsection{Experimental Setup}

\textbf{Datasets.}
We follow the benchmark design outlined in \cite{hu2023llm} and evaluate \celora on two popular reasoning benchmarks:

\begin{itemize}[topsep=5pt, leftmargin=1em]
\vspace{-0.5em}
\item \textbf{Commonsense Reasoning}: This dataset includes eight tasks: BoolQ \cite{clark-etal-2019-boolq}, PIQA \cite{bisk2020piqa}, SocialQA \cite{sap2019socialiqa}, HellaSwag \cite{zellers2019hellaswag}, WinoGrande \cite{sakaguchi2021winogrande}, ARC-challenge \cite{clark2018think}, ARC-easy \cite{clark2018think}, and OpenbookQA \cite{mihaylov2018can}. In our experiments, we fine-tune all models using the Commonsense 170K dataset \cite{hu2023llm}, which is constructed by combining the training sets from these eight tasks.

\vspace{-0.5em}
\item \textbf{Arithmetic Reasoning}:
This benchmark consists of seven subsets: MultiArith \cite{roy2016solving}, GSM8K \cite{cobbe2021training}, AddSub \cite{hosseini2014learning}, AQuA \cite{ling-etal-2017-program}, SingleEq \cite{koncel-kedziorski-etal-2015-parsing}, SVAMP \cite{patel-etal-2021-nlp} and MAWPS \cite{koncel-kedziorski-etal-2016-mawps}.
We fine-tune the models on the Math 10k dataset \cite{hu2023llm}, which includes training data from GSM8K, MAWPS, and AQuA, augmented by language models with chain-of-thought reasoning steps.
\vspace{-0.5em}
\end{itemize}

\textbf{Fine-tuned models and hyper-parameters.} We fine-tune LLaMA-2-7B, LLaMA-2-13B \cite{touvron2023llama2openfoundation}, and LLaMA-3.1-8B \cite{llama3modelcard} using both \celora and LoRA.
% 
The adapter is applied to all linear layers in each transformer block, including \texttt{Q}, \texttt{K}, \texttt{V}, \texttt{O}, \texttt{Up}, \texttt{Gate}, and \texttt{Down}.
% 
Unless specified otherwise, all \celora experiments replace the frozen \texttt{V}, \texttt{O}, \texttt{Up}, and \texttt{Down} layers with \celora layers.
% 
The sparsity levels are set as follows: $p_\text{V} = p_\text{O} = 0.55$ and $p_\text{Up} = p_\text{Down} = 0.65$. 
% 
For consistency, the same set of hyperparameters is applied across both methods for each model size.
% 
All experiments are conducted using the BF16 format to optimize memory usage.
% 

\input{figures/comm_acc_table}

\subsection{Statistical Efficiency of \celora}

\begin{figure*}[!t]
  \centering
    \includegraphics[width=\linewidth]{figures/loss.pdf}
  \vspace{-5mm}
  \caption{Loss curve of commonsense reasoning fine-tune task. Each row in the figure corresponds to a different trainable parameter setting, while each column represents base models: LLaMA2-7B/13B and LLaMA3.1-8B.}
  \label{fig:loss_curve}
\end{figure*}

In this set of experiments, we evaluate the convergence performance of \celora using two critical metrics: the accuracy achieved on each benchmark and the trajectory of the fine-tuning loss across training iterations. 
% 
By monitoring these metrics, we aim to gain insights into how quickly and effectively \celora converges compared to LoRA.
%


\textbf{Accuracy}. %\label{sec:benchmark-accuracy}
We trained both \celora and LoRA under low-rank (LoRA rank of $16$, \celora rank of $14$) and high-rank (LoRA rank of $64$, \celora rank of $56$) configurations across two reasoning datasets for one epoch.
Table \ref{table:common_accuracy} and Table \ref{table:math_accuracy} summarize the results for the commonsense and arithmetic reasoning benchmarks.
% 
The experimental outcomes demonstrate that, across all LoRA rank settings in both benchmarks, \celora achieves fine-tuning accuracy that is nearly identical to that of LoRA, with an average difference in results of 1.58\%.
%
These findings suggest that our approach has a negligible impact on the original LoRA fine-tuning accuracy.
% 
The slight differences in accuracy between \celora and LoRA on the test sets can primarily be attributed to the scaling of \celora's rank, which was adjusted to ensure a fair experimental comparison.


\input{figures/math_acc_table}
\textbf{Loss curve}.
Figure \ref{fig:loss_curve} illustrates the loss curves of both \celora and LoRA under different rank settings across the three models on the commonsense reasoning fine-tuning task.
% 
In each setting, \celora's loss curves nearly overlap with those of its LoRA counterparts, indicating similar convergence behaviors.
% 
These results highlight the effectiveness of our method, empirically demonstrating that \celora can achieve nearly the same convergence capability as the original LoRA while potentially offering computational savings. 
% 
The overlapping loss curves suggest that \celora does not introduce additional convergence challenges and maintains training stability comparable to LoRA.



\subsection{Computation Efficiency}

In these experiments, we measure \celora's training efficiency by comparing the average training step latency of a single-layer \celora with a single-layer LoRA. 
% 
All experiments are conducted on a single \texttt{NVIDIA-HGX-H20-(96GB)} GPU to maintain consistent hardware conditions. 
% 
For a fair comparison, both \celora and LoRA employ the same trainable rank of $64$. 
% 
We run experiments on three different model weight sizes---$(8192, 8192)$, $(4096, 4096)$, and $(2048, 2048)$---using a fixed batch size of $16$ and a sequence length of $8192$. 
% 
To measure average training step latency, each configuration is tested over 100 runs.%, each consisting of 500 training iterations. 
% 
The first 10 iterations of each run are considered warmup and are excluded from latency measurements to mitigate initialization overhead.



Figure~\ref{fig:latency} compares the results of LoRA and \celora with various sparsity levels and shows that \celora achieves a consistent reduction in overall training time, with a maximum of $36.3\%$ speedup. 
% 
As illustrated, \celoraâ€™s forward pass latency closely matches that of LoRA's due to the unchanged forward logic of the frozen layer. 
% 
However, in the backward pass, \celora outperforms LoRA by up to $3.39\times$ with some aggressive sampling rate.
% 
The observed improvements in wall-clock speed are primarily attributed to two key factors:
% 
(\underline{i}) \celora effectively reduces the theoretical floating-point operations required during backpropagation for frozen layers. 
(\underline{ii}) We developed specialized CUDA kernels tailored for low-rank computations inherent in \celora's backpropagation process, which optimize memory access patterns, resulting in enhanced computational efficiency and reduced latency.

\begin{figure*}[!t]
  \centering
    \includegraphics[width=\linewidth]{figures/latency.pdf}
  \vspace{-5mm}
  \caption{Comparison of training latency for \celora and LoRA at various sparsity levels (i.e., $\frac{1}{2}$, $\frac{1}{4}$, $\frac{1}{8}$, $\frac{1}{16}$) across three model shapes: $(8192, 8192)$, $(4096, 4096)$, and $(2048, 2048)$. \celora provides significant speedups in the backward pass, leading to a maximum of $36.3\%$ overall reduction in end-to-end training time compared to LoRA.}
  \label{fig:latency}
\end{figure*}