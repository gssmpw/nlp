\vspace{-0.25em}
\section{Related Works}
\vspace{-0.25em}
% \yh{Language Models (Transformers), Memory-efficient training (LoRA), Computational-efficient training (baselines?)...}
    \textbf{Large Language Models. }Since the transformer structure was proposed in the famous work \cite{vaswani2017attention} in 2017, it has shown great potential in various tasks, including reasoning, planning, machine translation, \textit{etc.}, and has become a popular choice in modern LLM designs, \textit{e.g.}, GPT \citep{radford2018improving,radford2019language,brown2020language}, OPT \citep{zhang2022opt}, LLaMA \citep{touvron2023llama,touvron2023Bllama,dubey2024llama}, BLOOM \citep{le2023bloom}, BERT \citep{devlin2018bert}, Falcon \citep{penedo2023refinedweb}, \textit{etc.} In general, the basic structure of a transformer block consists of a multi-head attention (MHA) module followed by a feed-forward network (FFN), combined with normalization and residual connections. Linear layers take up most of the trainable parameters in transformers and account for the expensive training and inference costs.

\textbf{Memory-Efficient Training Algorithms. }As the scale of LLM parameters grows, the memory consumption to train these models has become a bottleneck problem. Recent studies have proposed a series of works in order to reduce training-time memory consumption, enabling LLM researchers to effectively pre-train / fine-tune larger LLMs within constrained computational resources. \cite{houlsby2019parameter,pfeiffer2020adapterhub} fine-tune LLMs parameter-efficiently by adding trainable adapter layers, LoRA \citep{hu2021lora} reparameterizes linear layers in transformers with low-rank adapters, ReLoRA \citep{lialin2023relora} extends to pre-training tasks by accumulating LoRA updates, S$^2$FT \citep{yang2024sft} applies sparse structures, SLTrain \citep{han2024sltrain} combines low-rank and sparse structures. Besides the above parameter-efficient approaches, another line of works reduce the memory consumption for optimizer states by optimizing in periodically updated subspaces, including LISA \citep{pan2024lisa}, GaLore \citep{zhao2024galore}, GoLore \citep{he2024subspace} and \textsc{Flora} \citep{hao2024flora}. In addition, BackRazor \citep{jiang2022back} and PreBackRazor \citep{yu2024sheared} improve memory-efficiency by compressing the activation memory. Furthermore, quantization methods \citep{micikevicius2017mixed,dettmers2024qlora} that are orthogonal to the above approaches have shown nice compatibilities in memory cost reduction. 

\textbf{Computation-Efficient Training Algorithms. }Though not specially designed for computational efficiency, a lot of memory-efficient training algorithms, particularly  those belong to parameter-efficient fine-tuning (PEFT), can also reduce computational costs to some extent. On the other hand, the training throughput can also be improved by utilizing a larger batch size thanks to the reduced memory consumption \citep{zhu2024apollo}. However, the computational savings of these approaches are limited by precisely retaining the complete backward propagation process. Recently, \cite{woo2024dropbp} proposes DropBP, an approach orthogonal to PEFT that saves computation by strategically skip connections in backward propagation. Since some layers are dropped during backward propagation, corresponding parameters do not have gradients for update. To the best of our knowledge, this paper provides the \textit{first} approach to accelerate LoRA by employing structured sparsity to reduce the computational bottleneck in backward propagation without sacrificing memory-efficiency or model performance.

% \paragraph{PEFT}
% S$^2$FT[nips24] \cite{yang2024sft}; 