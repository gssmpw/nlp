
\vspace{-1em}
\section{Introduction}

Large Language Models (LLMs) have garnered significant attention in recent years for their exceptional performance across a wide range of practical tasks, including machine translation, commonsense reasoning, and planning, among others~\cite{bommasani2021opportunities}.
The versatility of these models has also driven a growing demand for fine-tuning them on specific tasks or domains to unlock their full potential~\cite{zhang2023instruction,han2024parameter}.
However, fine-tuning these models remains a highly resource-intensive process, demanding substantial computational power and large amounts of GPU memory. As model parameters and training tokens scale up, the increasing training costs have made it difficult for most organizations to keep pace with advancements in LLM research due to resource constraints.

To tackle these challenges, recent advancements such as Low-Rank Adaptation (LoRA) \citep{hu2021lora} have demonstrated promising results in reducing memory consumption during LLM fine-tuning, enabling the fine-tuning of models with more parameters or larger batch sizes within constrained resources. While LoRA significantly alleviates memory requirements, its reduction of computational costs remains limited --- although the low-rank adapters can save part of the computation costs by reducing the matrix sizes, the size of original weight matrices used to calculate \textit{activation gradients} remains unchanged, which contributes to half of the total computation cost in the backpropagation of the original model. The formulation of this computation is illustrated in Section~\ref{sec:preliminaries}. Limited by this computational bottleneck, LoRA can reduce computation by at most half during the backpropagation process. This limitation raises the following open question:
% To address these challenges, recent advancements like Low-Rank Adaptation (LoRA) \citep{hu2021lora} have shown promising results in reducing the memory consumption in LLM fine-tuning, enabling fine-tuning LLMs with more parameters or larger batch-sizes within constrained computing resources. Although largely cutting down the required memory, LoRA has limited effect on reducing the computational costs. As illustrated in Sec.~\ref{sec:preliminaries}, LoRA can only save up to half of the computation in the back propagation procedure. This motivates the following open question:
% \vspace{-2mm}
\begin{quote}
    \textit{Compared with vanilla LoRA, can we develop a more computation-efficient fine-tune algorithm by the same memory budget without sacrificing the statistical efficiency (i.e., convergence)?}
\end{quote}
% \vspace{-0.75em}


To answer this question, we first conduct a computational analysis of LoRA's backward propagation procedure and identify the primary computational bottleneck as the \textit{calculation of the activation gradients}. This step accounts for the majority of the backward computation load, especially when LoRA employs a relatively small rank $r$.

% In order to answer this open question, we first present a computational analysis in LoRA's backward propagation procedure, and identify the major computational bottleneck to be the computation of the activations' gradients, which takes up most of the backward computation load, particularly when LoRA uses a relatively small rank $r$. 

% Based on this analysis, we introduce the Approximated Matrix Multiplication (AMM) technique to mitigate the computational cost associated with calculating activation gradients. Consider two matrices $P =[p_1; \cdots; p_]$


% Instead of conducting multiplications with two complete matrices, AMM samples a subset of 

% \begin{figure*}[!t]
%     \centering
%     \begin{minipage}{0.425\textwidth}
%     \centering
%         \includegraphics[width=\textwidth]{figures/CeLoRA-MMC.pdf}
%     \end{minipage}
%     \begin{minipage}{0.540\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/CeLoRA-DLR.pdf}
%     \end{minipage} 
%     \caption{\small An illustration of the Approximated Matrix Multiplication (AMM) technique (left) and the \celora framework (right).}
%     \label{fig:intro-draft}
%     \vspace{-1em}
% \end{figure*}

\begin{figure*}[!t]
  \centering
  \begin{tabular}{@{}c@{\hspace{0.05\textwidth}}c@{}}  % 控制两图间距为 5% 页面宽度
    \includegraphics[width=0.425\textwidth]{figures/CeLoRA-MMC.pdf} &
    \includegraphics[width=0.540\textwidth]{figures/CeLoRA-DLR.pdf} \\
  \end{tabular}
  \caption{\small An illustration of the Approximated Matrix Multiplication (AMM) technique (left) and the \celora framework (right).}
  \label{fig:intro-draft}
  \vspace{-1em}
\end{figure*}

Based on this analysis, we propose the Approximated Matrix Multiplication (AMM) technique to reduce the computation of activation gradients. In order to reduce the computation of a dense matrix multiplication $\mathbf{T}=\mathbf{P}\mathbf{Q}$, where $\mathbf{P}\in\mathbb{R}^{m\times n}$ and $\mathbf{Q}\in\mathbb{R}^{n\times k}$, AMM directly reduces the size of the matrices by discarding unimportant rows or columns, obtaining $\mathbf{P}[:,\mathcal{I}]$ and $\mathbf{Q}[\mathcal{I},:]$, as illustrated in Figure~\ref{fig:intro-draft} (left). Compared to the original multiplication procedure, AMM reduces the computational complexity to $|\mathcal{I}|/n$ times by instead multiplying $\mathbf{P}[:,\mathcal{I}]$ with $\mathbf{Q}[\mathcal{I},:]$, trading computational accuracy for computational efficiency. To identify the important rows or columns, we compute the importance scores $\alpha_i=\|\mathbf{P}[:,i]\mathbf{Q}[i,:]\|_F$ every $\tau$ iterations and select the top ones in the following $\tau$ times of AMM operation. 

Unfortunately, the computational error in the activations' gradients can further propagate to previous layers. Under such a long-lasting effect of inaccurate activation gradients, the  accumulated computational error can lead to poor gradient estimations, which severely harms the optimization procedure and degrades the final model performance. To resolve this issue, we develop a double-LoRA technique that significantly reduces the relative error induced to the activation gradients by AMM. Specifically, double-LoRA splits the frozen dense weight matrix into two parts, where the first part applies AMM to save computation, the second part is a frozen LoRA adapter which is computation-efficient without using AMM. In order to reduce the AMM-induced error, we expect the first part to include as little information as possible. Consequently, we initially conduct SVD of the parameter matrix and identify the first part according to the smallest singular values and corresponding vectors.
% We also consider using different AMM levels for the multi-head attention (MHA) and feed-forward network (FFN) modules in each transformer block, motivated by the observation that MHA and FFN are quite different in sensitivity to the AMM-induced computational error. 
Combining the above techniques, we propose \textub{C}omputation-\textub{E}fficient \textub{LoRA} (\celora), which is more computation-efficient and equally memory-efficient compared with LoRA.

We evaluate the proposed \celora algorithm both theoretically and empirically. In theory, we prove that \celora with momentum SGD converges at a rate of $\mathcal{O}(1/\sqrt{T})$, the same order of LoRA's convergence rate. Empirically, we validate that \celora can converge at a comparable precision as standard LoRA, with slightly reduced memory consumption and a 3.39$\times$ acceleration in computation. To our knowledge, \celora is the \textit{first} algorithm that accelerates LoRA without sacrificing memory-efficiency or leading to notable performance degradation.

The main contributions of this paper are as follows:

\begin{itemize}[topsep=5pt, leftmargin=1em]

    \vspace{-0.5em}
    \item We propose a novel algorithm \celora, which is more computation-efficient and equally memory-efficient compared with standard LoRA.

    % \vspace{-0.5em}
    \item We theoretically prove that \celora converges at a rate of $\mathcal{O}(1/\sqrt{T})$, which is the same order of standard LoRA's convergence rate.

    % \vspace{-0.5em}
    \item We experimentally validate that \celora has a 3.39$\times$ acceleration compared with LoRA without sacrificing memory-efficiency or leading to notable training performance degradation.
\end{itemize}

