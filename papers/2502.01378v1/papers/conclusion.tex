
\vspace{-1em}
\section{Conclusion}

We propose \celora that saves computational FLOPs by approximated matrix multiplication and controls the compression error by a novel double LoRA technique and layer-wise compression ratios. While enjoying a 3.39 times of acceleration compared to LoRA, \celora theoretically converges at a rate of $\mathcal{O}(1/\sqrt{T})$ and shows comparable performance in our experiments.
% \yh{We propose \celora, a computational-efficient LoRA fine-tuning algorithm that compute gradient estimates with less computational costs and achieve similar performance compared with standard LoRA. While achieving similar results, \celora can save xx times computation, largely accelerates the training procedure.
% }