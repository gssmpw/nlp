\section{\celora: Computation-Efficient LoRA}

% \yh{sampling strategy, layer-adaptive strategy, SVD strategy, algorithm}

\subsection{Approximated Matrix Multiplication (AMM)}
Consider matrix multiplication $\mathbf{T}=\mathbf{P}\mathbf{Q}$, where $\mathbf{T}\in\mathbb{R}^{m\times k}$, $\mathbf{P}\in\mathbb{R}^{m\times n}$ and $\mathbf{Q}\in\mathbb{R}^{n\times k}$.
% 
Let $\mathbf{p}_1,\mathbf{p}_2,\cdots,\mathbf{p}_n$ denote the column vectors of matrix $\mathbf{P}$, and $\mathbf{q}_1,\mathbf{q}_2,\cdots,\mathbf{q}_n$ denote the column vectors of matrix $\mathbf{Q}^\top$. 
% 
We can rewrite the matrix multiplication into:
% 
\begin{align*}
    \mathbf{T}=\sum_{i=1}^n\mathbf{p}_i\mathbf{q}_i^\top.
\end{align*}
% 
To estimate the product $\mathbf{T}$ computation-efficiently, we may assume the matrices $\mathbf{P}$ and $\mathbf{Q}$ enjoy some kinds of structured sparsity, such that a few $(\mathbf{p}_i\mathbf{q}_i^\top)'s$ contribute to most of the result $\sum_{i=1}^n\mathbf{p}_i\mathbf{q}_i^\top$, in which case we could estimate $\mathbf{T}$ by computing the most important parts only. Specifically, we identify $s$ most important indices $1\le i_1<\cdots<i_s\le n$, and the AMM estimate of $\mathbf{T}$ is given by:
% 
\begin{align*}
    \hat{\mathbf{T}}=\sum_{j=1}^s{\mathbf{p}_{i_j}\mathbf{q}_{i_j}^\top}=\hat{\mathbf{P}}\hat{\mathbf{Q}},
\end{align*}
% 
where $\hat{\mathbf{P}}$ and $\hat{\mathbf{Q}}^\top$ collect column vectors $\{\mathbf{p}_{i_j}\}_{j=1}^s$ and $\{\mathbf{q}_{i_j}\}_{j=1}^s$, respectively.


The efficiency of AMM is concerned with the number of selected indices $s$, or the structured sparsity  $p:=s/n\in(0,1]$. Replacing the dense matrix multiplication $\mathbf{T}=\mathbf{P}\mathbf{Q}$ by AMM estimate  $\hat{\mathbf{T}}=\hat{\mathbf{P}}\hat{\mathbf{Q}}$, the computational complexity is reduced from $2mnk$ to $2msk=p\cdot (2mnk)$.
% 
Hereafter, we use $\mathcal{C}_p(\mathbf{P}\cdot\mathbf{Q})$ to denote the AMM estimate of matrix multiplication $\mathbf{P}\cdot\mathbf{Q}$ with structured sparsity $p$.

An important question is how to select the indices $\mathcal{I}=\{i_1,i_2,\cdots,i_s\}$ properly. A previous research \citep{drineas2006fast} has studied a random sampling strategy, which does not work well in our experiments. Based on the above intuition, we define the importance score $\alpha_i$ of index $i$ by the Frobenius norm $\|\mathbf{p}_i\mathbf{q}_i^\top\|_F$ and attempt to select the indices with highest scores. However, as calculating $\{\alpha_i\}_{i=1}^k$ requires the same amount of computation as that of conducting the original matrix multiplication, we cannot determine $\mathcal{I}$ based on the calculation results of $\{\alpha_i\}_{i=1}^k$ in every iteration. We use historical information to mitigate this issue. Specifically, the matrices $\mathbf{P},\mathbf{Q}$ we multiply by AMM should be variables that live along the whole optimization process, and $\mathbf{P}^t,\mathbf{Q}^t$ are multiplied at every iteration $t$. The corresponding $\mathcal{I}^t$ is only re-selected according to the top-$s$ importance scores every $\tau$ iterations and is reused in intermediate ones.


To reduce the computational bottleneck in LoRA's backward propagation, we apply AMM to step \eqref{eq:A'3} and get:
\begin{align}
\mathbf{G}_{\mathbf{x},2}=&\ \mathcal{C}_p(\mathbf{W}_0^\top\cdot \mathbf{G}_\mathbf{y}).\label{eq:A''3}
\end{align}
% With AMM, the backward propagation step in (A'2) becomes: 
% \begin{align*}
%     \frac{\partial l}{\partial \mathbf{x}}=&\mathcal{C}_p(\mathbf{W}_0^\top\cdot\frac{\partial l}{\partial\mathbf{y}})+\mathbf{A}^\top\cdot\frac{\partial l}{\partial \mathbf{z}}.&\text{(A''2)}
% \end{align*}

% To implement AMM, we provide two index sampling strategies:

% \textbf{Random Sampling Strategy.}
% % 
% This method randomly selects $s$ indices from the product’s inner dimensions, ensuring that the estimate is theoretically unbiased.

% \textbf{Greedy Sampling Strategy.}
% % 
% To identify the most important indices, we periodically computes a precise stochastic gradient at intervals $\tau$.
% % 
% Given the accurate backpropagation result, we evaluate $\|p_iq_i^\top\|_F=\|p_i\|_2\|q_i\|_2$ and select the indices with top magnitudes.
% % 
% These selected indices are then used fo the next $\tau$ iterations.

% By default, \celora adopts the greedy sampling strategy. We will compare these two sampling methods in Sec. \ref{sec:exp-sample-strategy}.

% \cgd{Sort $\Vert p_iq_i^\top\Vert_F$ from largest to smallest. Greedily select from the beginning.}

% \textbf{[TODO] Reusing Sampling Patterns. }\yh{reorganize to save i/o, reduce computation, ..}



% \subsection{Layer-wise Adaptive Compression Ratio}

% \yh{cite some papers regarding difference across transformer layers}

% \yh{Add some ablation study figures here, demonstrating different properties of Q,K,V,O,U,D.}

% Consequently, it's natural to use a more aggressive sparsity for layers that are robust to computational errors, and a relatively conservative sparsity for sensitive ones. We empirically use $p=0.3$ for MHA layers, and $p=0.9$ for FFN layers, throughout our experiments.

\subsection{Double-LoRA Mechanism }
Although computation-efficient, AMM will induce errors to $g_x$, the gradient with respect to the activations. 
% 
These errors propagate backward through the network, potentially compounding as they traverse previous layers.
% 
If the magnitude of these errors is not properly controlled, the accuracy of the parameter gradients can be significantly degraded.
% 
To mitigate this issue, we propose a double-LoRA mechanism to alleviate the error induced by the AMM operation in each layer. Intuitively, we wish the objective matrix multiplication result we estimate by AMM has as little contribution to the activation gradient as possible. This drives us to further separate the frozen matrix $\mathbf{W}_0$ into two parts: a low-rank part inheriting computational efficiency without AMM, and a residual part with a relatively small magnitude.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/layerwise.pdf}
    % \centering
    % \includegraphics[width=0.75\linewidth]{figures/comm_layerwise.pdf}
    \vspace{-1em}
    \caption{\small
    Layer-wise Sensitivity Analysis of LLaMA3.2-1B. 
    }
    \label{fig:layerwise}
    \vspace{-1em}    
\end{figure}

Specifically, we initially compute the SVD of $\mathbf{W}_0$, yielding
\begin{align*}
    \mathbf{W}_0=\mathbf{U}\mathbf{\Sigma} \mathbf{V}^\top
\end{align*}
Next, We collect the principal low-rank component $\mathbf{B}_0=\mathbf{U}[:r]\mathbf{\Sigma}^{1/2}$, $\mathbf{A}_0=\mathbf{\Sigma}^{1/2}(\mathbf{V}[:r])^\top$, and the residual $\mathbf{W}_{s}=\mathbf{W}_0-\mathbf{B}_0\mathbf{A}_0$. By separating $\mathbf{W}_0$ to $\mathbf{W}_s+\mathbf{B}_0\mathbf{A}_0$, we split the matrix to two parts. The first part $\mathbf{W}_s$ is believed to have better structured sparsity and is more compatible to AMM. The second low-rank part $\mathbf{B}_0\mathbf{A}_0$ is computation-efficient just like the trainable LoRA adapter $\mathbf{B}\mathbf{A}$. Combining AMM with double-LoRA, \eqref{eq:A''3} is further replaced by
\begin{align}
\mathbf{G}_{\mathbf{x},2}=&\ \mathcal{C}_p(\mathbf{W}_s^\top\cdot \mathbf{G}_\mathbf{y})+\mathbf{A}_0^\top(\mathbf{B}_0^\top \mathbf{G}_\mathbf{y}).\label{eq:A'''3}
% +\mathbf{A}_0^\top\cdot(\mathbf{B}_0^\top g_y)
\end{align}
% \begin{align*}
%     \frac{\partial l}{\partial\mathbf{x}}=\mathcal{C}_p(\mathbf{W}_{0-}\cdot\frac{\partial l}{\partial\mathbf{y}})+\mathbf{A}_0^\top\mathbf{B}_0^\top\cdot\frac{\partial l}{\partial\mathbf{y}}+\mathbf{A}^\top\cdot\frac{\partial l}{\partial\mathbf{z}}.&\text{(A'''2)}
% \end{align*}


\subsection{Layer-wise Adaptive Sparsity}

It is natural to apply more aggressive sparsity to layers that are relatively robust to computational errors, while using more conservative sparsity for those that are more sensitive.
% 
Inspired by \cite{hu2025accelerating,ma2024first,jaiswal2024galore,zeng2024lsaq,malinovskii2024pushing,zhang2024q,liu2024training}, we adopt a layer-wise adaptive sparsity strategy for \celora.


To determine which layers are more sensitive to varying sparsity levels, we conduct experiments on two small fine-tuning datasets: the Commonsense 14K dataset and the Math 7K dataset~\cite{hu2023llm}.
% 
In these experiments, we fix LoRA's rank to 32, and set both \celora's trainable LoRA rank and its frozen Double-LoRA rank to 28. 
% 
For each \celora configuration, we vary the sparsity level of one layer type, while setting the sparsity of all remaining layer types to $p=0.3$.
As shown in Figure~\ref{fig:layerwise}, the \texttt{Gate} layers are essential for preventing error propagation. 
% 
In addition, the \texttt{Q} and \texttt{K} layers have a strong impact on arithmetic and commonsense reasoning tasks, respectively. 
% 
Based on these findings, we disable sparsity for the \texttt{Q}, \texttt{K}, and \texttt{Gate} layers. 
For the remaining MHA layers, we use $p=0.55$, and for the last two layers in the FFN, we set $p=0.65$ throughout our experiments.




\subsection{Algorithm}
\input{algorithms/celora}
Overall, \celora integrates AMM, double-LoRA, and layer-wise adaptivity, as outlined in Algorithm~\ref{alg:CeLoRA}.
% \cgd{ADD For every training loop}
During model initialization, we replace all frozen linear layers with \celora and apply the double-LoRA technique to the weight matrix $\mathbf{W}_0$, resulting in low-rank components $\mathbf{A}_0$ and $\mathbf{B}_0$ (lines 2–3). 
% 
For each training step $t$, the forward pass of a \celora linear layer behaves the same as the original frozen linear layer (lines 8).
% 
In the backward pass, \celora first computes the residual weight matrix by subtracting the low-rank components from the original weight matrix (line 11). 
% 
Next, if the current step $t$ is a multiple of $\tau$ or if the indices are empty (\eg, at the start of training), the top-K indices are updated (lines 12–15). 
% 
Finally, \celora uses AMM to compute activation gradient $\mathbf{G}_\mathbf{x_\ell}$ (line 20).

\begin{table*}[!b]
    \centering
    \caption{Computation and memory analysis for a single linear layer.}
    \label{tab:complexity}
    \vspace{0.5em}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{cccc}
    \toprule
         \textbf{Method} & Standard AdamW & LoRA & \celora\\
    \midrule
    \multirow{2}{*}{\textbf{Memory Usage}} & \multirow{2}{*}{$10mn+2bm$} & $2mn+10r(m+n)$ & $2mn+2r_0(m+n)$\\
    & &$+2b(m+r)$ & {$+10r(m+n)+2b(m+r)$}\\
    \midrule
    \textbf{Forward Computation} & $2bmn$ & $2bmn+2br(m+n)$ & $2bmn+2br(m+n)$\\
    \midrule
    \multirow{2}{*}{\textbf{Backward Computation}} & \multirow{2}{*}{$4bmn$} & \multirow{2}{*}{$2bmn+4br(m+n)$} & $(2pb+1)mn$\\
    & & & $+2(r_0 + br_0 + 2br)(m+n)$\\
    % $(2pb+1)mn+2r_0(m+n)$\\
    % & & & $+(2br_0+4br)(m+n)$\\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table*}
\subsection{Complexity Analysis}
To better illustrate the computational efficiency of \celora, we theoretically compare the computational and memory complexity of \celora with LoRA and standard AdamW fine-tuning. Consider linear layer $\mathbf{y}=\mathbf{W}\mathbf{x}$ with $\mathbf{W}\in\mathbb{R}^{m\times n}$, trained with LoRA rank $r$, double-LoRA rank $r_0$, structured sparsity $p$ and batch size $b$ using BF16 precision. As illustrated in Table~\ref{tab:complexity}, \celora can achieve a memory usage similar to LoRA by applying slightly smaller $r_0$ and $r$, while significantly reduce the backward computation by applying a relatively small $p$ when $b\gg 1$ and $r\ll\min\{m,n\}$. 
When combined with low-precision training, the influence of double-LoRA can be further reduced, as the frozen low-rank parameters do not require high-precision weight copies or gradient accumulators.
