\section{Preliminaries}\label{sec:preliminaries}

% \yh{LoRA, computational costs}

\subsection{LoRA Algorithm}
In order to fine-tune language models memory-efficiently, LoRA applies a low-rank adapter to each linear layer in the model. Specifically, let $\mathbf{y}=\mathbf{W}\mathbf{x}$ represent a linear layer with $\mathbf{y}\in\mathbb{R}^{m\times b}$, $\mathbf{W}\in\mathbb{R}^{m\times n}$ and $\mathbf{x}\in\mathbb{R}^{n\times b}$, where $m,n$ represent the output and input dimension, respectively, and $b$ represents the batch-size. The LoRA adapter is given by $\mathbf{W}=\mathbf{W}_0+\mathbf{B}\mathbf{A}$ with $\mathbf{W}_0\in\mathbb{R}^{m\times n}$ fixed as the pre-trained weights, $\mathbf{B}\in\mathbb{R}^{m\times r}$ and $\mathbf{A}\in\mathbb{R}^{r\times n}$ trainable.
% \vspace{-1mm}
\begin{align*}
    \mathbf{y}=&\ \mathbf{W}\mathbf{x},&\quad\text{(Original)}\\
    \mathbf{y}=&\ (\mathbf{W}_0+\mathbf{B}\mathbf{A})\mathbf{x}.&\quad\text{(LoRA)}
\end{align*}
% \vspace{-1mm}
When $r\ll\min\{m,n\}$, the number of trainable parameters in LoRA, $(m+n)r$, is much fewer than that in full fine-tuning, \ie, $mn$, which significantly reduces the memory consumption of the optimizer states. 

\subsection{Computational Bottleneck}
While applying LoRA can significantly reduce the memory cost for fine-tuning large language models, the computational cost for computing the gradient via back propagation is not sufficiently reduced. Specifically, let $\mathbf{G}_{\mathbf{\theta}}$ denote the stochastic gradient of $\mathbf{\theta}$ calculated by back propagation, for linear layer $\mathbf{y}=\mathbf{W}\mathbf{x}$ we compute:
% \begin{align*}
%     \frac{\partial l}{\partial\mathbf{W}}=&\frac{\partial l}{\partial\mathbf{y}}\cdot\mathbf{x}^\top,&\quad\text{(W1)}\\
%     \frac{\partial l}{\partial\mathbf{x}}=&\mathbf{W}^\top\cdot\frac{\partial l}{\partial\mathbf{y}},&\quad\text{(A1)}
% \end{align*}
\begin{align}
\mathbf{G}_\mathbf{W}=&\ \mathbf{G}_\mathbf{y}\cdot\mathbf{x}^\top,\label{eq:W1}\\
\mathbf{G}_\mathbf{x}=&\ \mathbf{W}^\top\cdot \mathbf{G}_\mathbf{y},\label{eq:A1}
\end{align}
% \begin{align*}
% g_W=&\ g_y\cdot\mathbf{x}^\top,&\quad\text{(W1)}\\
% g_x=&\ \mathbf{W}^\top\cdot g_y,&\quad\text{(A1)}
% \end{align*}
% where $l$ represents the empirical loss function. 
Both \eqref{eq:W1} and \eqref{eq:A1} require $2bmn$ FLOPs of computation. Accordingly, in LoRA we compute:
% \begin{align*}
%     g_B=&\ g_y\cdot\mathbf{z}^\top,&\quad\text{(W'1)}\\
%     g_z=&\ \mathbf{B}^\top\cdot g_y,&\quad\text{(A'1)}\\
%     g_A=&\ g_z\cdot\mathbf{x}^\top,&\quad\text{(W'2)}\\
%     g_{x,1}=&\ \mathbf{A}^\top\cdot g_z,&\quad\text{(A'2)}\\
%     g_{x,2}=&\ \mathbf{W_0}^\top\cdot g_y,&\quad\text{(A'3)}\\
%     g_x=&\ g_{x,1}+g_{x,2},&\quad\text{(A'4)}
% \end{align*}
\begin{align}
    \mathbf{G}_\mathbf{B}=&\ \mathbf{G}_\mathbf{y}\cdot\mathbf{z}^\top,\label{eq:W'1}\\
    \mathbf{G}_\mathbf{z}=&\ \mathbf{B}^\top\cdot \mathbf{G}_\mathbf{y},\label{eq:A'1}\\
    \mathbf{G}_\mathbf{A}=&\ \mathbf{G}_\mathbf{z}\cdot\mathbf{x}^\top,\label{eq:W'2}\\
\mathbf{G}_{\mathbf{x},1}=&\ \mathbf{A}^\top\cdot \mathbf{G}_\mathbf{z},\label{eq:A'2}\\
    \mathbf{G}_{\mathbf{x},2}=&\ \mathbf{W}_0^\top\cdot \mathbf{G}_\mathbf{y},\label{eq:A'3}\\
    \mathbf{G}_\mathbf{x}=&\ \mathbf{G}_{\mathbf{x},1}+\mathbf{G}_{\mathbf{x},2},\label{eq:A'4}
\end{align}
% The back propagation in LoRA is given by:
% \begin{align*}
%     \frac{\partial l}{\partial\mathbf{B}}=&\frac{\partial l}{\partial\mathbf{y}}\cdot\mathbf{z}^\top,&\quad\text{(W'1)}\\
%     \frac{\partial l}{\partial\mathbf{z}}=&\mathbf{B}^\top\cdot\frac{\partial l}{\partial\mathbf{y}},&\quad\text{(A'1)}\\
%     \frac{\partial l}{\partial\mathbf{A}}=&\frac{\partial l}{\partial\mathbf{z}}\cdot\mathbf{x}^\top,&\quad\text{(W'2)}\\
%     \frac{\partial l}{\partial\mathbf{x}}=&\mathbf{W}_0^\top\cdot\frac{\partial l}{\partial \mathbf{y}}+\mathbf{A}^\top\cdot\frac{\partial l}{\partial\mathbf{z}},&\quad\text{(A'2)}
% \end{align*}
where $\mathbf{z}=\mathbf{A}\mathbf{x}\in\mathbb{R}^{r\times b}$ represents LoRA's additional activation. \eqref{eq:W'1}\eqref{eq:A'1}\eqref{eq:W'2}\eqref{eq:A'2}\eqref{eq:A'3}\eqref{eq:A'4} require $2brm$, $2brm$, $2brn$, $2brn$, $2bmn$, and $bn$ FLOPs of computation, respectively, resulting in $4br(m+n)+2bmn+bn$ FLOPs in total. When $r\ll\min\{m,n\}$, the computational cost in LoRA's back propagation is roughly $2bmn$, half of the computation in the original approach \eqref{eq:W1}\eqref{eq:A1}. The computational bottleneck lies in the dense matrix multiplication step in \eqref{eq:A'3}, which alone requires $2bmn$ FLOPs of computation.
