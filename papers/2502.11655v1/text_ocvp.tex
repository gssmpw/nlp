
\documentclass{article}




\usepackage{hyperref}
\usepackage[accepted]{icml2025}
\usepackage[textsize=tiny]{todonotes}

\input{macros}


\icmltitlerunning{Object-Centric Image to Video Generation with Language Guidance}


\begin{document}

\twocolumn[
\icmltitle{Object-Centric Image to Video Generation with Language Guidance}


\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Angel Villar-Corrales}{equal,ais}
\icmlauthor{Gjergj Plepi}{equal,ais}
\icmlauthor{Sven Behnke}{ais}
\end{icmlauthorlist}

\icmlaffiliation{ais}{Autonomous Intelligent Systems, Computer Science Institute VI â€“ Intelligent Systems and Robotics, Center for Robotics and the Lamarr Institute for Machine Learning and Artificial Intelligence}

\icmlcorrespondingauthor{Angel Villar-Corrales}{villar@ais.uni-bonn.de}



\icmlkeywords{Object-Centric Learning, Text-Conditioned Video Prediction, Object-Centric Video Prediction, Representation Learning, Transformers}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution}



\begin{abstract}
%
Accurate and flexible world models are crucial for autonomous systems to understand their environment and predict future events.
%
Object-centric models, with structured latent spaces, have shown promise in modeling object dynamics and interactions, but often face challenges in scaling to complex datasets and incorporating external guidance, limiting their applicability in robotics.
%
To address these limitations, we propose $\Method$, an object-centric model for image-to-video generation guided by textual descriptions.
%
$\Method$ parses an observed scene into object representations, called slots, and utilizes a text-conditioned transformer predictor to forecast future object states and video frames.
Our approach jointly models object dynamics and interactions while incorporating textual guidance, thus leading to accurate and controllable predictions.
%
Our method's structured latent space offers enhanced control over the prediction process, outperforming several image-to-video generative baselines.
%
Additionally, we demonstrate that structured object-centric representations provide superior controllability and interpretability, facilitating the modeling of object dynamics and enabling more precise and understandable predictions.
%
Videos and code are available at \url{https://play-slot.github.io/TextOCVP/}.
%
\vspace{-0.4cm}
\end{abstract}



\input{./imgs/teaser_v2.tex}


\section{Introduction}

Understanding and reasoning about the real world is essential for
enabling autonomous systems to better comprehend their surroundings, predict
future events, and adapt their actions accordingly.
%
Humans achieve these capabilities by perceiving the environment as a structured composition of individual objects that interact and evolve dynamically over time~\cite{Kahneman_ReviewingOfObjectFiles_1992}.
%
Neural networks equipped with such compositional inductive biases have shown the ability to learn structured object-centric representations of the world, which enable desirable properties, such as out-of-distribution generalization~\cite{Dittadi_GeneralizationAndRobustnessImplicationsInObjectCentricLearning_2022}, compositionality~\cite{Greff_OnTheBindingProblemInNeuralNetworks_2020}, or sample efficiency~\cite{Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics}.


Recent advances in unsupervised object-centric representation learning have progressed from extracting object representations in synthetic images~\cite{Locatello_ObjectCentricLearningWithSlotAttention_2020, Lin_UnsupervisedObjectOrientedSceneRepresentationViaSpatialAttentionAndDecomposition_2020}  to modeling objects in video~\cite{Kipf_ConditionalObjectCentricLearningFromVideo_2022, Singh_STEVE_2022}
and scaling to real-world scenes~\cite{Seitzer_BridgingTheGapToRealWorldObjectCentricLearning_2023, Zadaianchuk_VideoSaur_2024}.
%
These developments have enabled object-level dynamics modeling for future prediction and planning.
%
Notably, models like SlotFormer~\cite{Wu_SlotFormer_2022} or OCVP~\cite{villar2023object} introduced object-centric world models that explicitly model spatio-temporal relationships between objects,
shifting away from image-level approaches that ignore scene compositionality.
%
Despite these advancements, current approaches struggle with complex object appearances and dynamics, and lack the ability to incorporate external guidance, thus limiting their effectiveness as world models in robotic applications.



To address these challenges, we propose \emph{\Method}, a novel object-centric model for image-to-video generation with language guidance, illustrated in \Figure{fig: teaser}.
%
Given a reference image and text instruction, \Method{} extracts object representations and predicts their evolution using a novel text-conditioned object-centric transformer.
%
This module forecasts future object representations by modeling object dynamics and interactions, while also incorporating textual information through a text-to-slot attention mechanism.
%
By jointly modeling spatio-temporal object relationships and integrating textual guidance, \Method{} generates future object states and video frames that align with the provided instructions.

We evaluate our approach through extensive experiments focusing on image-to-video generation tasks with varying levels of complexity.
Our results show that \Method{} outperforms existing text-conditioned methods by effectively leveraging object-centric representations.
%
This demonstrates the significant advantage of incorporating textual guidance into structured object-centric representations, particularly for scenes featuring multiple moving objects.


Through an in-depth model analysis, we demonstrate the importance of our proposed object-centric latent space, which allows \Method{} to generate video sequences that closely align with language instructions.
%
We further demonstrate how \Method{} adapts its generated video sequences based on different captions by routing the text information to the relevant object representations.

In summary, our contributions are as follows:
%
\begin{itemize}[itemsep=1pt, topsep=0.5pt]
	%
	\item We propose \Method, a novel text-driven image-to-video generation model, featuring a text-conditioned object-centric predictor that integrates textual guidance via a text-to-slot attention mechanism.
	\vspace{-0.1cm}
	%
	\item Through extensive evaluation, we show that \Method{} outperforms existing text-conditioned models by leveraging object-centric representations.
	\vspace{-0.1cm}
	%
	\item We demonstrate that \Method{} is controllable, seamlessly adapting to diverse textual instructions.
	\vspace{-0.1cm}
	%
\end{itemize}




\section{Related Work}


\subsection{Object-Centric Learning}
Representation learning, the ability to extract meaningful features from data, often improves model performance by enhancing its understanding of the input space~\cite{bengio2013representation}.
%
Object-centric representation learning focuses on simultaneously learning representations that  characterize individual objects within an input image or video.

Recently, object-centric models have progressed from learning object representations from synthetic images~\cite{greff2019multi, lin2020space,
villar2021unsupervised, Locatello_ObjectCentricLearningWithSlotAttention_2020} to videos~\cite{Kipf_ConditionalObjectCentricLearningFromVideo_2022,
Singh_STEVE_2022, elsayed2022savi++}, and real-world scenes~\cite{Seitzer_BridgingTheGapToRealWorldObjectCentricLearning_2023,
aydemir2023self, Zadaianchuk_VideoSaur_2024, kakogeorgiou2024spot}.
%
The learned object representations benefit downstream tasks, such as reinforcement learning~\cite{Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics} or visual-question answering~\cite{Wu_SlotFormer_2022}, among others.



\subsection{Video Prediction and Generation}

Future frame video prediction (VP) is the task of forecasting the upcoming $\NumPreds$ video frames conditioned on the preceding $\NumContext$ seed frames~\cite{oprea2020review}.
%
When the number of seed frames is $\NumContext=1$, this task is often referred to as image-to-video generation.
%
Several methods have been proposed to address this challenge, leveraging 2D convolutions~\cite{gao2022simvp, chiu2020segmenting}, 3D convolutions~\cite{vondrick2016generating, tulyakov2018mocogan},
recurrent neural networks (RNNs)~\cite{denton2018stochastic, villar2022mspred, wang2022predrnn, guen2020disentangling, franceschi2020stochastic}, Transformers~\cite{rakhimov2020latent, ye2022vptr, ye2023video}, or diffusion models~\cite{hoppe2022diffusion, ho2022video}.



\subsubsection{Object-Centric Video Prediction}

Object-centric VP presents a structured approach that explicitly models the dynamics and interactions of individual objects to forecast future video frames.
%
These methods typically involve three main steps: decomposing seed frames into object representations, forecasting future object states using a dynamics model, and rendering video frames from the predicted object representations.
%
Various approaches have been proposed for this task, using different architectural designs such as
RNNs~\cite{creswell2021unsupervised, Zoran_PARTS_2021, Nguyen_ReusableSlotwiseMechanisms_2024} or transformers~\cite{wu2021generative, Wu_SlotFormer_2022, villar2023object, Song_ObjectMotionAppearanceDynamics_2023, Daniel_DDLP_2024}.
%
Despite promising results, these models are currently limited to simple deterministic datasets or rely on action-conditioning~\cite{Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics} or inferred latent vectors~\cite{playslot}.
%
In contrast, our proposed model forecasts future video frames conditioned on past object slots and textual descriptions.



\subsubsection{Text-Conditioned Video Prediction}
%
This category of VP models leverages text descriptions to provide appearance, motion and action cues that guide the generation of future frames.
%
This task was first proposed by \citet{hu2022make}, who utilized a VQ-VAE to encode the images into visual token representations, and modeled the image dynamics with an axial transformer that jointly processes such tokens along with text descriptions.
%
TVP~\cite{song2024text} leverages RNNs to learn text representations, which condition a GAN-based framework in order to generate videos from a single frame.
%
MMVG~\cite{fu2023tell} combines a VQ-GAN~\cite{esser2021taming} with a masked transformer predictor.
%
More recently, several methods leverage diffusion models with text conditioning \cite{gu2023seer,ni2023conditional,chen2023livephoto}.

Concurrently with our work, \citet{wang2024tiv} combine object-centric learning with text-conditional image-to-video generation.
%
The authors propose an autoregressive diffusion model conditioned on object slots and text descriptions and evaluate on simple datasets.
%
In contrast, we explicitly model the object dynamics with an autoregressive transformer, and evaluate our approach on more complex robotics simulations.




\section{Method}

We propose \Method{} -- a novel object-centric model for text-conditioned image-to-video generation.
%
Given an initial reference image $\ImageT{1}$ and a text caption \Caption, \Method{} generates the subsequent $\NumPreds$~video frames $\PredImageRange{2}{\NumPreds+1}$, which maintain a similar appearance and structural composition as the reference image, and follow the motion described in the text caption.


\Method, which is illustrated in \Figure{fig: model overview}, implements an object-centric approach, in which the reference frame $\ImageT{1}$ is first decomposed with a scene parsing module (\Section{sec: scene parsing}) into a set of $\NumSlots$ $\SlotDim$-dimensional object representations called slots $\SlotsT{1} \in \R^{\NumSlots \times \SlotDim}$, where each slot represents a single object in the image.
%
The object slots are fed to a text-conditioned transformer predictor (\Section{sec: predictor}), which jointly models their spatio-temporal relations, and incorporates the textual information from the caption \Caption{} as a guidance for predicting the future object slots $\PredSlotsMany{2}{\NumPreds+1}$.
%
Finally, the predicted object slots are decoded in order to render object representations and video frames (\Section{sec: video rendering}).


We propose two different \Method{} variants, which differ in the underlying object-centric decomposition modules.
%
Specifically, $\MethodSAVi$ leverages SAVi~\cite{Kipf_ConditionalObjectCentricLearningFromVideo_2022},
whereas $\MethodDINO$ extends the  DINOSAUR~\cite{Seitzer_BridgingTheGapToRealWorldObjectCentricLearning_2023} framework for recursive object-centric video decomposition and video rendering.




\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{imgs/main_textocvp_v1.png}
	\vspace{-0.55cm}
	\caption{
		Overview of \Method.
		%
		\Method{} parses the reference frame $\ImageT{1}$ into object representations $\SlotsT{1}$.
		%
		The text-conditioned object-centric predictor models object dynamics and interactions, incorporating information from the description $\TextEmbs$ to predict future object states and frames.
	}
	\vspace{-0.2cm}
	\label{fig: model overview}
\end{figure}





\subsection{Scene Parsing}
\label{sec: scene parsing}

The scene parsing module decomposes a video sequence $\ImageRange{1}{\tau}$ into a set of permutation-invariant object representations called slots $\SlotsMany{1}{\tau} = (\SlotsT{1}, ..., \SlotsT{\tau})$, with $\SlotsT{t} = (\SingleSlotsT{t}{1}, ..., \SingleSlotsT{t}{\NumSlots})$, where each slot $\SingleSlot$ represents a single object.

For learning the object representations, we leverage the object-centric video decomposition framework proposed by~\citet{Kipf_ConditionalObjectCentricLearningFromVideo_2022}.
%
At time step $t$, the corresponding input frame $\ImageT{t}$ is encoded with a feature extractor module into a set of $\DimFeats$-dimensional feature maps $\FeatureMapsT{t} \in \R^{\NumLocs \times \DimFeats}$ representing $\NumLocs$ spatial locations.
The feature extractor is convolutional neural network in our $\MethodSAVi$~variant and a DINO-pretrained vision transformer~\cite{caron2021emerging} in $\MethodDINO$.
%
These feature maps are processed with a Slot Attention~\cite{Locatello_ObjectCentricLearningWithSlotAttention_2020} corrector, which updates the previous slots $\SlotsT{t-1}$ based on visual features from the current frame following an iterative attention mechanism.
%
Namely, Slot Attention performs cross-attention, with the attention weights
normalized over the slot dimension, thus encouraging the slots to compete to represent parts of the input.
%
It then updates the slots using a Gated Recurrent Unit~\cite{Cho_GRU_2014} (GRU). Formally, Slot Attention updates the previous slots $\SlotsT{t-1}$ by:
%
\vspace{-0.05cm}
\begin{align}
	%
	& \Attention = \softmax_{\NumSlots} \left( \frac{q(\SlotsT{t-1}) k(\FeatureMapsT{t})^T}{\sqrt{\SlotDim}} \right) \in \R^{\NumSlots \times \NumLocs}, \\
	%
	& \SlotsT{t} = \text{GRU}(W_t v(\FeatureMaps), \SlotsT{t-1}) \; \text{with} \;
		W_{i,j} = \frac{\Attention_{i,j}}{\sum_{l=1}^{L}\Attention_{i,l}},
	%
\end{align}
\vspace{-0.1cm}
%
where $k, q$ and $v$ are learned linear projections that map input features and slots into a common dimension.
%
The final output of this module is the set of slots $\SlotsT{t}$, representing the objects of the input frame.





\subsection{Text-Conditioned Object-Centric Predictor}
\label{sec: predictor}

Our proposed text-conditioned predictor module, depicted in \Figure{fig:predictor}, autoregressively forecasts future object states conditioned on the object slots from the reference frame $\SlotsT{1}$ and a textual description \Caption.


To condition the prediction process, the text description \Caption{} is encoded into text token embeddings $\TextEmbs$ employing an encoder-only transformer module.
%
We experiment with different variants of this module, including a vanilla transformer encoder~\cite{Vaswani_AttentionIsAllYouNeed_2017} or a pretrained T5~\cite{raffel2020exploring} module.
%
%For each sequence generation, the text embeddings are created only once.

At time step $t$, the predictor receives as input the corresponding text embeddings $\TextEmbs$, as well as the previous object slots $\SlotsMany{1}{t}$, which are initially mapped via an MLP to the predictor token dimensionality $\TokenDim$.
%
Additionally, these tokens are augmented with a temporal positional encoding, which applies the same sinusoidal positional embedding to all tokens from the same time-step,
thus preserving the inherent permutation-equivariance of the objects.

Each layer of our predictor module mirrors the transformer decoder architecture~\cite{Vaswani_AttentionIsAllYouNeed_2017}. First, a self-attention layer enables every slot to attend to all other object representations in the sequence, modeling the spatio-temporal relations between objects.
%
Subsequently, a text-to-slot cross-attention layer enhances the slot representations by incorporating important features from the text embeddings, such as motion or appearance information.
%
Finally, an MLP is independently applied for each token.
%
This process is repeated in every predictor layer, resulting in the predicted object slots of the subsequent time step $\PredSlotsT{t+1}$.
%
Furthermore, we apply a residual connection from $\SlotsT{t}$ to $\PredSlotsT{t+1}$, which improves the temporal consistency of the predictions.
%
This process is repeated autoregressively to obtain slot predictions for $\NumPreds$ subsequent time steps.






\subsection{Video Rendering}
\label{sec: video rendering}

The video rendering module decodes the predicted slots $\PredSlotsT{t}$ to render the corresponding video frame $\PredImageT{t}$.
%
We leverage two variants of the video rendering module, for our $\MethodSAVi$~and $\MethodDINO$~variants, respectively.

\noindent \textbf{$\MethodSAVi$ Decoder~~} This variant independently decodes each slot in $\PredSlotsT{t}$ with a CNN-based Spatial Broadcast Decoder~\cite{watters2019spatial}, rendering an object image $\SlotObject{t}{n}$ and mask $\SlotMask{t}{n}$ for each slot $\SingleSlotsT{t}{n}$.
%
The object masks are normalized across the slot dimension, and the representations are combined via a weighted sum to render video frames:
%These representations can be normalized and combined via a weighted sum to render video frames:
%
\begin{align}
	& \PredImageT{t} = \sum_{n=1}^{\NumSlots} \SlotObject{t}{n} \cdot \ProcessedMask{t}{n} \;\;\; \text{with} \;\;\; \ProcessedMask{t}{n} = \softmax_{\NumSlots}(\SlotMask{t}{n}).
\end{align}



\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{imgs/main_predictor_v0.png}
	\caption{Text-conditioned object-centric predictor.}
	\vspace{-0.2cm}
	\label{fig:predictor}
\end{figure}





\noindent \textbf{$\MethodDINO$~Decoder~~} This decoder variant decodes the object slots in two distinct stages.
%
First, following DINOSAUR~\cite{Seitzer_BridgingTheGapToRealWorldObjectCentricLearning_2023},
an MLP-based Spatial Broadcast Decoder~\cite{watters2019spatial} is used to generate object features along with their corresponding masks.
%
Similar to the $\MethodSAVi$ decoder, the object masks are normalized and combined with the object features in order to reconstruct the encoded features $\PredDinoFeatsT{t} \in \R^{\NumLocs \times \DimFeats}$.
%
In the second stage, the reconstructed features $\PredDinoFeatsT{t}$ are arranged into a grid format and processed with a CNN decoder to generate the corresponding video frame $\PredImageT{t}$.




\subsection{Training and Inference}
\label{sec: training and inference}

Our proposed \Method{} is trained in two different stages.

\noindent \textbf{Object-Centric Learning~~} We first train the scene parsing and video rendering modules for decomposing video frames into object-centric representations by minimizing a reconstruction loss.
%
In the $\MethodSAVi$~variant, these modules are trained simply by reconstructing the input images, whereas in $\MethodDINO$~they are trained by jointly minimizing an image and a feature reconstruction loss:
%
\begin{align}
	& \Loss_{\SAVi} = \frac{1}{\NumFrames} \sum_{t=1}^{\NumFrames} || \PredImageT{t} - \ImageT{t} || ,  \label{eq: savi loss} \\
	& \Loss_{\DINO} = \frac{1}{\NumFrames} \sum_{t=1}^{\NumFrames} || \PredImageT{t} - \ImageT{t} || + || \PredDinoFeatsT{t} - \DinoFeatsT{t} || . \label{eq: dino loss}
\end{align}



\noindent \textbf{Predictor Training~~} Given the pretrained scene parsing and rendering modules, we train our \Method{} predictor for text-conditioned video generation using a dataset containing paired videos and text descriptions.
%
Namely, given the object representations from a reference frame $\SlotsT{1}$ and the textual description \Caption, our \Method{} predictor forecasts the subsequent object slots $\PredSlotsT{2}$, which are decoded into a predicted video frame $\PredImageT{2}$.
%
This process is repeated autoregressively, i.e. the predicted slots are appended to the input in the next time step, in order to generate the set of slots for the subsequent $\NumPreds$  time steps.
%
This autoregressive training, in contrast to teacher forcing, enforces our predictor to operate with imperfect inputs,  leading to better modeling of long-term dynamics at inference time.

Our predictor is trained by minimizing the following combined loss:
%
\begin{align}
	&\Loss_{\Method} = \frac{1}{\NumPreds} \sum_{t=2}^{\NumPreds+1} \lambda_{\text{Img}}
	\Loss_{\text{Img}} + \lambda_{\text{Slot}} \Loss_{\text{Slot}},  \label{eq: full loss}  \\
	%
	\text{with}\;\;
	&\Loss_{\text{Img}} =  || \PredImageT{t} - \ImageT{t} ||_2^2 \label{eq: loss img}
	\;\; \text{and} \;\;
	\Loss_{\text{Slot}} =  || \PredSlotsT{t} - \SlotsT{t} ||_2^2,
	%
\end{align}
%
where $\Loss_{\text{Img}}$ measures the future frame prediction error, and $\Loss_{\text{Slot}}$ enforces the alignment of the predicted object slots with the actual inferred object-centric representations.


\noindent \textbf{Inference}
%
At inference time, \Method{} receives as input a single reference frame and a language instruction.
%
Our model parses the seed frame into object slots and autoregressively predicts future object states and video frames conditioned on the given textual description.
%
By modifying the language instruction, \Method{} can generate a new sequence continuation that performs the specified task while preserving a consistent scene composition.






\section{Experiments}

\subsection{Experimental Setup}

\subsubsection{Datasets}

We evaluate the performance of \Method{}  for text-driven image-to-video generation on two different datasets, namely CATER and CLIPort.
%
Further dataset details are provided in \Appendix{app: datasets}.
%

\noindent \textbf{CATER~~}  CATER~\cite{girdhar2019cater} is a dataset that consists of long video sequences, featuring two simultaneously moving 3D objects, with the motion described by a textual caption.
%
We used the CATER-Hard variant introduced by~\citet{hu2022make}.
%
This dataset contains 30,000 sequences featuring between three and eight objects, two of which follow a predefined action pattern.
%
In our experiments, we resize the images to $64 \times 64$.


\noindent \textbf{CLIPort~~}
%
CLIPort~\cite{shridhar2022cliport} is a robot manipulation dataset consisting of video-caption pairs.
%
We employ 21,000 $336 \times 336$ sequences of the Put-Block-In-Bowl task.
%
Each sequence features a table with multiple bowls and blocks of different color, placed at random positions on the table.
%
The corresponding caption describes the action of the robot arm picking a block of a specific color and
putting it into a specific bowl.



\subsubsection{Baselines}

We select several baselines to benchmark \Method{} against state-of-the-art image-to-video generation methods and to analyze different design choices within our architecture.
%
To assess the impact of text conditioning, we compare \Method{} with OCVP-Seq~\cite{villar2023object}, an unconditional object-centric video prediction model.
%
To evaluate the role of object-centric representations, we introduce a \Method{} variant (\emph{Non-OC TF}) that replaces the structured slot representation with a single high-dimensional embedding.
%
Finally, we benchmark our approach against three text-conditioned image-to-video generation models: MAGE~\cite{hu2022make}, MAGE$_{\text{DINO}}$, and SEER~\cite{gu2023seer}.
%
Additional details for the baselines can be found in \Appendix{app: baselines}.




\subsubsection{Implementation Details}

All our models are implemented in PyTorch and trained on a single NVIDIA A6000 (48Gb) GPU.
%
$\MethodSAVi$ closely follows~\citet{Kipf_ConditionalObjectCentricLearningFromVideo_2022} for the design of the scene parsing and video rendering modules.
%
$\MethodDINO$ leverages DINOv2~\cite{oquab2023dinov2} as image encoder, uses a four-layer MLP-based Spatial-Broadcast Decoder~\cite{watters2019spatial}, which is shared among all slots, to decode the object slots into object features and masks. Additionally, it uses a CNN decoder to map the reconstructed scene features back to images.
%
On the CATER dataset, we use the $\MethodSAVi$~variant with eight 128-dimensional object slots; whereas on CLIPort we employ the $\MethodDINO$~variant with ten 128-dimensional slots.
%
Our predictor module is an eight-layer transformer with 512-dimensional tokens, eight attention heads, and a hidden dimension of 1024.
%
%
Further implementation details are provided in \Appendix{app: implementation details}.





\subsection{Results}


\subsubsection{CATER Results}

\input{./tables/table_cater_hard.tex}

On the CATER dataset, we train the models to generate nine future frames given a single reference frame and a text caption.
%
In \Table{table: quant cater-hard} we report quantitative evaluations on CATER  using the same setting as
in training, i.e. predicting $\NumPreds = 9$ frames, as well as when predicting $\NumPreds = 19$ future frames.
%
In both settings, \Method{} outperforms all other models, demonstrating superior perceptual quality.




\input{./imgs/qual_cater_00.tex}



\Figure{fig: cater qual_00} shows a qualitative comparison between \Method{} and MAGE.
%
Our proposed method generates a sequence that is closely aligned to the ground-truth, whereas MAGE predictions feature multiple errors and artifacts, including missing objects, blurry contours, and significant changes on object shapes.
%
This highlights the effectiveness of object-centric representations to accurately represent and model object dynamics.
%
Additional qualitative results are provided in \Appendix{app: additional results}.




\subsubsection{CLIPort  Results}

\input{./tables/table_cliport.tex}

\input{./imgs/qual_cliport_comb.tex}



\Table{table: quant cliport} shows a detailed quantitative evaluation for image-to-video generation on the CLIPort dataset.
%
\Method{} outperforms all baselines when evaluated on the same setting as in training (i.e. $1\rightarrow9$).
%
When generating for longer horizons, our model shows competitive performance, closely following MAGE$_{\text{DINO}}$.


In our qualitative evaluations, we observe that \Method{} often generates the most accurate generations given the reference frame and text description.
%
\Figure{fig: cliport qual_00} shows two qualitative examples comparing \Method{} with the MAGE$_{\text{DINO}}$ baseline.
%
MAGE$_{\text{DINO}}$ fails to complete the task outlined in the textual description, as it stops
generating consistent robot motion after 30 frames (\Figure{fig: cliport qual_00}a), or misses the target block after several prediction time steps (\Figure{fig: cliport qual_00}b).
%
In contrast, \Method{} correctly generates the sequences following instructions provided in the given textual description.

However, we identify that \Method{} often generates artifacts in the background and lacks textured details in certain objects, thus degrading its quantitative performance despite generating accurate future frames.
%
Additional qualitative results are provided in \Appendix{app: additional results}.






\subsection{Model Analysis}


\subsubsection{Ablation Studies}

\input{./tables/ablation.tex}


We perform several ablation studies to support and validate the architectural choices of our model components and their impact on \Method's  image-to-video generation performance.
%
The results are presented in \Table{table: ablation_combined_tables}.

\noindent \textbf{Number of Layers (\Table{table: ablation num_layers})~~} \Method{} performs best with $\NumPredLayers = 8$ predictor layers.
%
Scaling beyond eight layers was not explored, as the performance improvements when increasing from four to eight layers were marginal.


\noindent \textbf{Residual Connection (\Table{table: ablation residual})~~} Applying a residual connection to the predictor output, $\PredSlotsT{t+1} := \PredSlotsT{t+1} + \SlotsT{t}$, enhances the model's performance by improving the temporal consistency of the predicted slots.


\noindent \textbf{Text Encoder (\Table{table: ablation text dec})~~} We evaluate the performance of different text encoders, including a vanilla transformer, a frozen T5 encoder, and a fine-tuned (FT) T5.
%
A frozen T5 module led to the best \Method{}  performance on both CATER and CLIPort datasets.
%
We argue that given the relatively small vocabulary size on CLIPort, fine-tuning T5 did not prove to be beneficial.



\noindent \textbf{Number of Slots (\Table{table: ablation num slots})~~} Using $\NumSlots = 10$ slots resulted in better performance for \Method{}  on CLIPort,
even though each scene can be described with eight slots (six objects, robot arm and background).
%
This observation suggests that additional slots can be used as registers in order to help with internal computations~\cite{darcet2024vision}.




\input{./tables/unseen_colors.tex}

\subsubsection{Model Robustness}

We evaluate the performance of \Method{}  and $\text{MAGE}_{\text{DINO}}$
on a CLIPort evaluation set featuring color variations in the text instructions that were not encountered during training.
%
This evaluation measures how well the models generalize to unseen scene features, highlighting their ability to handle novel configurations.

\Table{table: quantitative_unseen_colors} presents the results on this evaluation set.
%
We report both the video generation performance as well as the performance drop relative to the evaluation set with known colors.
%
Our model outperforms $\text{MAGE}_{\text{DINO}}$ for both prediction horizons.
%
Most notably, \Method{} demonstrates significantly higher robustness to objects with previously unseen colors compared to the baseline, showing a much smaller drop in performance for the perceptual LPIPS metric.
%
This highlights the effectiveness of leveraging object-centric representations for image-to-video generation and planning, in contrast to relying on holistic scene representations that struggle with novel scene compositions.




\subsubsection{Controllability}

\input{./imgs/control_cater.tex}

\input{./imgs/control_cliport.tex}



A key objective of text-driven image-to-video generation is to provide control over the model generations.
%
This is achieved through language instructions that describe the objects in the scene and their expected motion.
%
We qualitatively assess the controllability of our model on both the CATER and CLIPort datasets.



\noindent \textbf{CATER~~}
%
In \Figure{fig: cater control}, we showcase the control that \Method{} provides over its generations.
%
We qualitatively evaluate this ability by generating multiple videos conditioned on the same reference frame while varying the language instruction.
%
We experiment with altering both the moving and target objects and their actions in the language instruction, as well as providing instructions that include a number of tasks beyond what the model was trained on.

As shown in \Figure{fig: cater control}, \Method{} successfully identifies the objects described in the text and executes the instructions accordingly.
%
Notably, our model can distinguish between two nearly identical purple cones in the scene, despite their identical shapes and color, and correctly generates a sequence with the specified motion.

This demonstrates that key object attributes, such as size or color, are effectively captured through the text-to-slot attention mechanism.
%
Leveraging this information, the text-conditioned predictor accurately predicts the future motion of each individual object as specified in the instruction.
%
These results highlight the benefits of combining object-centric representations with text-based conditioning.





\noindent \textbf{CLIPort~~}
%
We perform a similar experiment on the CLIPort dataset, as shown in \Figure{fig: cliport control}.
%
A single frame containing multiple colored blocks and bowls is provided to \Method{}.
%
By modifying the textual instruction, we can control which block the robot arm picks up and the bowl where it is placed.
%
In both examples, \Method{} successfully selects the correct block and places it in the specified bowl.
%
Furthermore, the movement of the robot arm adapts to the motion described in the instruction, demonstrating accurate and responsive control.





\section{Conclusion}

We presented \Method, a novel object-centric model for text-driven image-to-video generation.
%
Given a single input image and a text description, \Method{} generates a sequence that matches the description by parsing the environment into object representations and modeling their dynamics conditioned on the textual instruction.
%
This is achieved with a novel text-conditioned object-centric transformer, which predicts future object states by modeling the spatio-temporal relationships between objects while incorporating the guidance from text.
%
%
Through extensive evaluations, we demonstrated that \Method{} outperforms existing text-conditioned models for the image-to-video generation, highlighting our model's ability to generate consistent long sequences and adapt its predictions based on language input.
%
Moreover, we validated our architectural choices through ablation studies, highlighting the importance of combining textual and object-centric information.
%
With its structured latent space and superior controllability, \Method{} offers potential for robotics applications, where it can serve as a controllable world model, enabling efficient planning, reasoning, and decision-making.





\newpage


\section*{Acknowledgment}

This work was funded by grant BE 2556/16-2 (Research Unit FOR 2535 Anticipating Human Behavior) of the German Research Foundation (DFG)


\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

%



\bibliography{referencesAngel}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn  % optional


\section{Limitations and Future Work}
\label{app: limitations}

While $\Method$ demonstrates promising results for text-guided object-centric video prediction, it presents some limitations that we plan to address in future work:

\noindent \textbf{Prediction Artifacts~~}  $\Method$ occasionally generates artifacts in the predicted frames, such as blurriness, inconsistent object appearances, lack of textured details in the objects, or visual artifacts in the background.
%
We believe that these limitations stem from the video rendering module, which might lack the representational power to reconstruct precise image details from the latent space representation.
%

\noindent \textbf{Poor Temporal Consistency~~}  We observe that $\Method$'s predictions often lack temporal consistency when forecasting for long  prediction horizons ($\NumPreds > 30$).
%
We attribute this limitation to the fact that $\Method$ is trained by optimizing reconstruction losses, which do not penalize temporal inconsistencies.



\noindent \textbf{Future Work~~} To address these limitations, we plan to extend our $\Method$ framework with more powerful decoder modules, such as autoregressive transformers~\cite{Singh_STEVE_2022} or diffusion models~\cite{slotdiffusion}, as well as scale our predictor module.
%
Furthermore, we plan to incorporate temporal discriminators~\cite{dvdgan} to improve the temporal consistency of the predicted video frames.
%
We believe that exploring these architectural modifications will enable us to utilize $\Method$ as a world model in complex real-world robotic environments.




\section{Implementation Details}
\label{app: implementation details}
We employ $\MethodSAVi$ for the experiments on CATER and $\MethodDINO$ for experiments on CLIPort. Below we discuss the implementation details for each of these variants.


\subsection{$\MethodDINO$}

$\MethodDINO$ variant consists of our proposed text-conditioned predictor module and an object-centric decomposition module that extends the DINOSAUR~\cite{Seitzer_BridgingTheGapToRealWorldObjectCentricLearning_2023} framework for recursive object-centric video decomposition and video rendering.


\noindent \textbf{Text-Conditioned Predictor~~}
%
The predictor is composed of $\NumPredLayers = 8$ identical layers, each containing 8-head attention mechanisms and an MLP with a single hidden layer of dimension 1024 and a ReLU activation function.
%
Furthermore, the predictor uses an embedding dimensionality of 512, context window size of ten frames, and applies a residual connection from the predictor input to its output.


\noindent \textbf{Text Encoder~~}
%
$\MethodDINO$ leverages a pretrained and frozen small version of T5 encoder~\cite{raffel2020exploring}, which consists of 6 T5 blocks.
%
This text encoder uses a vocabulary with size 32,128.


\noindent \textbf{Scene Parsing~~} 
%
The scene parsing module generates $\NumSlots=10$ slots of dimension 128. As feature extractor, we use DINOv2 ViT-Base~\cite{oquab2023dinov2}, featuring 12 layers, using a patch size of 14, and producing patch features with dimension $\DimFeats=768$.
%
The Slot Attention corrector module processes the first video frame with three iterations in order to obtain a good initial object-centric decomposition, and a single iteration for subsequent frames, which suffices to recursively update the slot representation.
%
The initial object slots $\SlotsT{0}$ are randomly sampled from a Gaussian distribution with learned mean and covariance.
%
We use a single Transformer encoder block as the transition function, which consists of a four attention heads and an MLP with hidden dimension 512.



\noindent \textbf{Video Rendering~~} 
%
The video rendering module consists of two distinct decoders.
%
First, a four-layer MLP-based Spatial Broadcast Decoder~\cite{watters2019spatial} with hidden dimension 1024 reconstructs the patch features from the slots. 
%
Then, to reconstruct the images from the features, we implement a CNN-based decoder with four convolutional layers, where each layer uses $3 \times 3$ kernels. Every layer is followed by bilinear upsampling and a ReLU activation function.
A final convolutional layer is applied to map to the RGB channels of the image.



\noindent \textbf{Training~~} 
%
We train our model for object-centric decomposition using videos sequences of length five frames for 1000 epochs.
%
We use batch size of 16, the Adam optimizer~\cite{kingma2014adam}, and a base learning rate of $4 \times 10^{-4}$, which is linearly warmed-up for the first 10000 steps, followed by cosine annealing for the remaining of the training process. Moreover, we clip the gradients to a maximum norm of 0.05.
%
The predictor module is trained given a frozen and pretrained object-centric decomposition model for 700 epochs.
%
The predictor is trained using the same hyper-parameters as for object-centric decomposition.
%
In the predictor loss function $\Loss_{\Method}$, we set $\lambda_{\text{Img}} = 1$ and $\lambda_{\text{Slot}} = 1$.





\subsection{$\MethodSAVi$}

$\MethodSAVi$ uses the same text-conditioned predictor and text-encoder architectures as $\MethodDINO$, but employs SAVi~\cite{Kipf_ConditionalObjectCentricLearningFromVideo_2022} as the object-centric decomposition module.


\noindent \textbf{Scene Parsing~~} 
%
The scene parsing module generates $\NumSlots=8$ slots of dimension 128.
%
Following~\citet{Kipf_ConditionalObjectCentricLearningFromVideo_2022}, we use as feature extractor a four-layer CNN with ReLU activation
function, where each convolutional layer features 32  $5 \times 5$ kernels, $\text{stride} = 1$, and $\text{padding} = 2$.
%
The Slot Attention corrector follows the same structure as in $\MethodDINO$.


\noindent \textbf{Video Rendering~~} 
%
Following~\citet{Kipf_ConditionalObjectCentricLearningFromVideo_2022}, we utilize a CNN-based Spatial Broadcast Decoder~\cite{watters2019spatial} with four convolutional layers with 32 kernels of size $5 \times 5$ , $\text{stride} = 1$, and $\text{padding} = 2$,
and a final convolutional layer which maps to four channels (RGB + alpha mask).


\noindent \textbf{Training~~} 
%
We train our model for object-centric decomposition using video sequences of length ten frames for 1000 epochs, using batch size of 64, and an initial learning rate of $10^{-4}$, which is warmed up for 2500 steps, followed by cosine annealing for the remaining of the training process. Moreover, we clip the gradients to a maximum norm of 0.05.
%
The predictor module is trained given a frozen and pretrained object-centric decomposition model for 1400 epochs.
%
The predictor is trained using the same hyper-parameters as for object-centric decomposition.
%
In the predictor loss function $\Loss_{\Method}$, we set $\lambda_{\text{Img}} = 1$ and $\lambda_{\text{Slot}} = 1$.




\section{Baselines}
\label{app: baselines}

We employ four different baselines to compare againts our \Method{}  model on the image-to-video generation task on CATER and CLIPort datasets.
%
To emphasize the importance of incorporating textual information, we include a comparison with OCVP-Seq~\cite{villar2023object}, a recent object-centric
video prediction model that does not utilize text conditioning.
%
Additionally, we evaluate a non-object-centric \Method{} variant (\emph{Non-OC TF})  that processes the input image into a single high-dimensional slot representation, instead of multiple object-centric slots, thus allowing us to evaluate the effect of object-centric representations.
%
Moreover, we compare \Method{}  with two popular text-conditioned image-to-video generation baselines that do not incorporate object-centricity:
MAGE~\cite{hu2022make} and SEER~\cite{gu2023seer}.
%
We train both models on CATER and CLIPort closely following the original implementation details\footnote{\url{https://github.com/Youncy-Hu/MAGE}}\footnote{\url{https://github.com/seervideodiffusion/SeerVideoLDM/tree/main}}.



\subsection{MAGE}
%
MAGE is an autoregressive text-guided image-to-video generation framework that utilizes a VQ-VAE~\cite{russakovsky2015imagenet} encoder-decoder architecture to learn efficient visual token representations.
A cross-attention module aligns textual and visual embeddings to produce a spatially-aligned motion representation termed Motion Anchor (MA), which is fused with visual tokens via
an axial transformer for video generation.
%
For experiments on CATER, we use a codebook size of $512 \times 256$ with a downsampling ratio of four, whereas on CLIPort we use a codebook size of $512 \times 1024$.
%
Moreover, on CLIPort we replace MAGE's standard CNN encoder and decoder with the DINOv2 ViT encoder and CNN decoder used in our $\MethodDINO$~approach.
%
This adjustment ensures a fair comparison and significantly enhances MAGE's performance on CLIPort.
%
We refer to this modified version as $\text{MAGE}_{\text{DINO}}$.
%
Additionally, to align with our experimental setup, we omit the speed parameter.




\subsection{SEER}
%
SEER is a diffusion-based model for language-guided video prediction. It employs an Inflated 3D U-Net derived from a pretrained text-to-image 2D latent diffusion model~\cite{rombach2022high}, extending it along the temporal axis and integrating temporal attention layers to simultaneously model spatial and temporal dynamics.
%
For the language conditioning module, SEER introduces a novel Frame Sequential Text (FSText) Decomposer, which decomposes global instructions generated by the CLIP text encoder~\cite{radford2021learning} into frame-specific sub-instructions.
%
These are aligned with frames using a transformer-based temporal network and injected into the diffusion process via cross-attention layers.
%
We initialize SEER from a checkpoint pretrained on the Something-Something V2 dataset~\cite{goyal2017something}, and further fine-tune it for a few epochs.
%16 epochs on CATER and 8 epochs on CLIPort.
We observed that incorporating a text loss enhanced SEER's performance, while other hyper parameters were kept consistent with its original implementation.





\section{Datasets}
\label{app: datasets}

\noindent \textbf{CATER~~} CATER~\cite{girdhar2019cater} is a dataset that consists of long video sequences, each described by a textual caption. The video scenes consist of multiple 3D geometric objects in a 2D table plane,
which is split into a $6 \times 6$ grid with fixed axis, allowing the exact description of object's positions using coordinates. The text instruction describes the movement of specific objects through
four atomic actions: `rotate', `pick-place', `slide', and `contain'. The caption follows a template consisting of the subject, action, and an optional object or end-point coordinate, depending on the action.
%
The movement of the objects starts at the same time step. Furthermore, the initial positions are randomly selected from the plane grid, and the camera position is fixed for every sequence.

In our work, we employ CATER-hard, which is a complete version of the CATER dataset, containing 30000 video-caption pairs. It includes 5 possible objects: cone, cube, sphere, cylinder, or snitch, which is a special small object in metallic gold color, shaped like three intertwined toruses.
%
Furthermore, every object is described by its size (small, medium, or large), material (metal or rubber), and color (red, blue, green, yellow, gray, brown, purple, cyan, or gold if the object is the snitch), and this description is included in the textual caption.
%
Every atomic action is available. The `rotate' action is afforded by cubes, cylinders and the snitch, the `contain' action is only afforded by the cones,
while the other two actions are afforded by every object.
%
Every video has between 3 and 8 objects, and two actions happen to different objects at the same time. The vocabulary size is 50.


\noindent \textbf{CLIPort~~} CLIPort~\cite{shridhar2022cliport} is a robot manipulation dataset, consisting of video-caption pairs, i.e.
long videos whose motion is described by a textual video caption. There are many variants of the CLIPort dataset, but we focus on the \emph{Put-Block-In-Bowl} variant.
We generate 21000 video-caption pairs with resolution $336 \times 336$.
%
Every video contains 6 objects on a 2D table plane, and a robot arm. Objects can be either a block or a bowl, and there is at least one of them in every sequence.
The starting position of each object is random, with the only constrain to be placed on the table.
Each video describes the action of the robot arm picking a block, and putting it in a specific bowl.
The video caption follows the template `put the [color] block in the [color] bowl'.
Each individual object in the scene has a different color. In the train and validation set, the block and the bowl that are part of the caption can have one of the following colors:
blue, green, red, brown, cyan, gray, or yellow, while in the test set they can have blue, green, red, pink, purple, white, or orange color. The other 4 objects, called distractors, can have any color.
During a video sequence, it can be possible that the robot arm goes out of frame, and comes back in later frames, thus requiring the model to leverage long range dependencies. The vocabulary size is 15.




\section{Additional Results}
\label{app: additional results}



\input{imgs/savi_vs_dinosaur/savi_dino.tex}




\subsection{SAVi vs. DINO}
%
Current object-centric approaches for video generation are limited to relatively simple synthetic datasets. We attribute this limitation primarily to the object-centric modules used for learning object representations.
%
Motivated by this observation, we extend DINOSAUR~\cite{Seitzer_BridgingTheGapToRealWorldObjectCentricLearning_2023} to handle video data and reconstruct images effectively.

To demonstrate  the significance of the object-centric module in scaling to more complex datasets, we input the same video sequence to both SAVi and our extended DINOSAUR trained on CLIPort.
%
As illustrated in \Figure{fig:savi_vs_dinosaur}, SAVi struggles to accurately model objects on the 2D plane, whereas our proposed object-centric model successfully reconstructs the scene, closely resembling the input.
%
The visual features extracted by the DINOv2~\cite{oquab2023dinov2} encoder contain high-level semantic information, and during training, the slots are specifically optimized to efficiently encode this information.
%
This design enables the model to scale and handle more complex object-centric video data effectively.





\subsection{Robustness to Number of Objects}

We evaluate the performance of \Method{} and $\text{MAGE}_{\text{DINO}}$ on a CLIPort evaluation set consisting of scenes with a larger number of objects than encountered during training, i.e. the 2D table plane contains 8 instead of 6 objects.
This evaluation demonstrates the robustness of models when generalizing to scenes with more objects.
The quantitative results are presented in \Table{table: quantitative_more_objects}. We report both the video generation performance and the performance drop relative to the original evaluation set.

We observe that our model outperforms $\text{MAGE}_{\text{DINO}}$ on every metric, for both prediction horizons. Notably, \Method{} demonstrates significantly higher robustness to scenes with more objects, 
as reflected by the smaller decline in performance for the perceptual LPIPS metric. This result is further illustrated in \Figure{fig: cliport more objects}, where we show a qualitative comparison of video generations for scenes with 8 objects.
As observed, our model correctly illustrates the motion described in the text instructions, while $\text{MAGE}_{\text{DINO}}$ fails to generate accurate sequences, missing the target block, a behaviour observed in various examples.

These results highlight once again the effectiveness of object-centric representations in image-to-video generation, as \Method{} is able to generalize to scenes with more objects by simply increasing the number of slots. %that represent these objects. 

\input{./tables/more_objects.tex}

\input{./imgs/supp_imgs/eval_more_objects/cliport_more_objects.tex}




\subsection{Text-to-Slot Attention Visualizations}

An additional advantage of using object-centric representations is the improved interpretability.
%
This can be shown in the visualizations of text-to slot attention weights, which help us understand how the textual information influences and guides the model predictions.
%
First, in \Figure{fig: tts_cater-hard}, we visualize the text-to-slot attention weights for different cross-attention heads for a single slot that represents the rotating red cube. We observe that the slot attends to relevant text tokens from the input, such as the object shape, size and the action taking place.

\input{./imgs/tts_2.tex}

In \Figure{fig: tts_cater-easy}, we additionally visualize the text-to-slot attention weights, averaged across attention heads, for different slots in a CATER sequence. 
We observe that slots that represent objects in the textual description attend to relevant text tokens, such as their target coordinate locations.
These results demonstrate that the text-to-slot attention mechanism effectively aligns the textual information with the object-centric representations, enabling the model to generate accurate video sequences based on the given text instructions, while providing superior interpretability.

\input{./imgs/tts_1.tex}




\subsection{Qualitative Evaluations}
%
\Figures{fig: sup qual cater}{fig: sup qual cater 2} show qualitative evaluations on CATER in which both MAGE and \Method{} successfully generate a sequence following the instructions from the textual description.
\Figure{fig: sup qual cater 3} illustrates an example where MAGE fails to generate a correct sequence, while \Method{} successfully completes the task described by the text.

\Figures{fig: sup cliport control 00}{fig: sup cliport control 01} show examples of \Method's control over the predictions. In both sequences, \Method{} generates a correct sequence given the text instructions, and seamlessly adapts its generations to a modified version of the textual instructions.



\input{./imgs/supp_imgs/mage_eval/mage_00.tex}
\input{./imgs/supp_imgs/mage_eval_2/mage_00.tex}
\input{./imgs/supp_imgs/mage_eval_3/mage_00.tex}


\input{./imgs/supp_imgs/control_cliport_00/control_cliport.tex}

\input{./imgs/supp_imgs/control_cliport_01/control_cliport.tex}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
