\section{Related Work}
\subsection{Object-Centric Learning}
Representation learning, the ability to extract meaningful features from data, often improves model performance by enhancing its understanding of the input space____.
%
Object-centric representation learning focuses on simultaneously learning representations that  characterize individual objects within an input image or video.

Recently, object-centric models have progressed from learning object representations from synthetic images____ to videos____, and real-world scenes____.
%
The learned object representations benefit downstream tasks, such as reinforcement learning____ or visual-question answering____, among others.



\subsection{Video Prediction and Generation}

Future frame video prediction (VP) is the task of forecasting the upcoming $\NumPreds$ video frames conditioned on the preceding $\NumContext$ seed frames____.
%
When the number of seed frames is $\NumContext=1$, this task is often referred to as image-to-video generation.
%
Several methods have been proposed to address this challenge, leveraging 2D convolutions____, 3D convolutions____,
recurrent neural networks (RNNs)____, Transformers____, or diffusion models____.



\subsubsection{Object-Centric Video Prediction}

Object-centric VP presents a structured approach that explicitly models the dynamics and interactions of individual objects to forecast future video frames.
%
These methods typically involve three main steps: decomposing seed frames into object representations, forecasting future object states using a dynamics model, and rendering video frames from the predicted object representations.
%
Various approaches have been proposed for this task, using different architectural designs such as
RNNs____ or transformers____.
%
Despite promising results, these models are currently limited to simple deterministic datasets or rely on action-conditioning____ or inferred latent vectors____.
%
In contrast, our proposed model forecasts future video frames conditioned on past object slots and textual descriptions.



\subsubsection{Text-Conditioned Video Prediction}
%
This category of VP models leverages text descriptions to provide appearance, motion and action cues that guide the generation of future frames.
%
This task was first proposed by ____, who utilized a VQ-VAE to encode the images into visual token representations, and modeled the image dynamics with an axial transformer that jointly processes such tokens along with text descriptions.
%
TVP____ leverages RNNs to learn text representations, which condition a GAN-based framework in order to generate videos from a single frame.
%
MMVG____ combines a VQ-GAN____ with a masked transformer predictor.
%
More recently, several methods leverage diffusion models with text conditioning ____.

Concurrently with our work, ____ combine object-centric learning with text-conditional image-to-video generation.
%
The authors propose an autoregressive diffusion model conditioned on object slots and text descriptions and evaluate on simple datasets.
%
In contrast, we explicitly model the object dynamics with an autoregressive transformer, and evaluate our approach on more complex robotics simulations.