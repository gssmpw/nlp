\section{Related Work}
\subsection{Object-Centric Learning}
Representation learning, the ability to extract meaningful features from data, often improves model performance by enhancing its understanding of the input space~\cite{bengio2013representation}.
%
Object-centric representation learning focuses on simultaneously learning representations that  characterize individual objects within an input image or video.

Recently, object-centric models have progressed from learning object representations from synthetic images~\cite{greff2019multi, lin2020space,
villar2021unsupervised, Locatello_ObjectCentricLearningWithSlotAttention_2020} to videos~\cite{Kipf_ConditionalObjectCentricLearningFromVideo_2022,
Singh_STEVE_2022, elsayed2022savi++}, and real-world scenes~\cite{Seitzer_BridgingTheGapToRealWorldObjectCentricLearning_2023,
aydemir2023self, Zadaianchuk_VideoSaur_2024, kakogeorgiou2024spot}.
%
The learned object representations benefit downstream tasks, such as reinforcement learning~\cite{Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics} or visual-question answering~\cite{Wu_SlotFormer_2022}, among others.



\subsection{Video Prediction and Generation}

Future frame video prediction (VP) is the task of forecasting the upcoming $\NumPreds$ video frames conditioned on the preceding $\NumContext$ seed frames~\cite{oprea2020review}.
%
When the number of seed frames is $\NumContext=1$, this task is often referred to as image-to-video generation.
%
Several methods have been proposed to address this challenge, leveraging 2D convolutions~\cite{gao2022simvp, chiu2020segmenting}, 3D convolutions~\cite{vondrick2016generating, tulyakov2018mocogan},
recurrent neural networks (RNNs)~\cite{denton2018stochastic, villar2022mspred, wang2022predrnn, guen2020disentangling, franceschi2020stochastic}, Transformers~\cite{rakhimov2020latent, ye2022vptr, ye2023video}, or diffusion models~\cite{hoppe2022diffusion, ho2022video}.



\subsubsection{Object-Centric Video Prediction}

Object-centric VP presents a structured approach that explicitly models the dynamics and interactions of individual objects to forecast future video frames.
%
These methods typically involve three main steps: decomposing seed frames into object representations, forecasting future object states using a dynamics model, and rendering video frames from the predicted object representations.
%
Various approaches have been proposed for this task, using different architectural designs such as
RNNs~\cite{creswell2021unsupervised, Zoran_PARTS_2021, Nguyen_ReusableSlotwiseMechanisms_2024} or transformers~\cite{wu2021generative, Wu_SlotFormer_2022, villar2023object, Song_ObjectMotionAppearanceDynamics_2023, Daniel_DDLP_2024}.
%
Despite promising results, these models are currently limited to simple deterministic datasets or rely on action-conditioning~\cite{Mosbach_SOLDReinforcementLearningSlotObjectCentricLatentDynamics} or inferred latent vectors~\cite{playslot}.
%
In contrast, our proposed model forecasts future video frames conditioned on past object slots and textual descriptions.



\subsubsection{Text-Conditioned Video Prediction}
%
This category of VP models leverages text descriptions to provide appearance, motion and action cues that guide the generation of future frames.
%
This task was first proposed by \citet{hu2022make}, who utilized a VQ-VAE to encode the images into visual token representations, and modeled the image dynamics with an axial transformer that jointly processes such tokens along with text descriptions.
%
TVP~\cite{song2024text} leverages RNNs to learn text representations, which condition a GAN-based framework in order to generate videos from a single frame.
%
MMVG~\cite{fu2023tell} combines a VQ-GAN~\cite{esser2021taming} with a masked transformer predictor.
%
More recently, several methods leverage diffusion models with text conditioning \cite{gu2023seer,ni2023conditional,chen2023livephoto}.

Concurrently with our work, \citet{wang2024tiv} combine object-centric learning with text-conditional image-to-video generation.
%
The authors propose an autoregressive diffusion model conditioned on object slots and text descriptions and evaluate on simple datasets.
%
In contrast, we explicitly model the object dynamics with an autoregressive transformer, and evaluate our approach on more complex robotics simulations.