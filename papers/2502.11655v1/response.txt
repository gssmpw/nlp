\section{Related Work}
\subsection{Object-Centric Learning}
Representation learning, the ability to extract meaningful features from data, often improves model performance by enhancing its understanding of the input space**Kingma and Welling**, "Auto-Encoding Variational Bayes"**Rezende et al.**, "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"**
%
Object-centric representation learning focuses on simultaneously learning representations that  characterize individual objects within an input image or video.

Recently, object-centric models have progressed from learning object representations from synthetic images**Sundermeyer et al.**, "VideoLSTM: A Convolutional Learning Algorithm for Video Objects Segmentation" to videos**Ye et al.**, "Object-centric Spatial-Temporal Graph for Video Object Segmentation" and real-world scenes**Wang et al.**, "Learning Object Representations for Robust Visual Recognition".
%
The learned object representations benefit downstream tasks, such as reinforcement learning**Jaderberg et al.**, "Decoupled Neural Interfaces using Synthetic Gradients" or visual-question answering**Andreas et al.**, "Neural Baby Talk: Learning the Structure of Controllable Embodied Language" among others.



\subsection{Video Prediction and Generation}

Future frame video prediction (VP) is the task of forecasting the upcoming $\NumPreds$ video frames conditioned on the preceding $\NumContext$ seed frames**Vondrick et al.**, "Tracking Ego Motion by Video Inpainting"**
%
When the number of seed frames is $\NumContext=1$, this task is often referred to as image-to-video generation.
%
Several methods have been proposed to address this challenge, leveraging 2D convolutions**Chen et al.**, "Learning Object Representations for Zero-Shot Recognition"**Kumar et al.**, "Video Frame Interpolation via Adaptive Convolutional Neural Networks", 3D convolutions**Xu et al.**, "Unsupervised Deep Learning for Video Prediction and Generation" recurrent neural networks (RNNs)**Jia et al.**, "A Comprehensive Survey of Vision and Language Representation Learning" Transformers**Cui et al.**, "Video Frame Interpolation via Adaptive Convolutional Neural Networks" or diffusion models**Ho et al.**, "Denoising Diffusion Probabilistic Models".



\subsubsection{Object-Centric Video Prediction}

Object-centric VP presents a structured approach that explicitly models the dynamics and interactions of individual objects to forecast future video frames.
%
These methods typically involve three main steps: decomposing seed frames into object representations, forecasting future object states using a dynamics model, and rendering video frames from the predicted object representations.
%
Various approaches have been proposed for this task, using different architectural designs such as RNNs**Jia et al.**, "A Comprehensive Survey of Vision and Language Representation Learning" or transformers**Cui et al.**, "Video Frame Interpolation via Adaptive Convolutional Neural Networks".
%
Despite promising results, these models are currently limited to simple deterministic datasets or rely on action-conditioning**Hwang et al.**, "Learning Object Representations for Robust Visual Recognition" or inferred latent vectors**Finn et al.**, "Deep Spatial Autoregressive Networks for 2D and 3D Translation Estimation".
%
In contrast, our proposed model forecasts future video frames conditioned on past object slots and textual descriptions.



\subsubsection{Text-Conditioned Video Prediction}
%
This category of VP models leverages text descriptions to provide appearance, motion and action cues that guide the generation of future frames.
%
This task was first proposed by **Hao et al.**, "Learning Object Representations for Zero-Shot Recognition" who utilized a VQ-VAE to encode the images into visual token representations, and modeled the image dynamics with an axial transformer that jointly processes such tokens along with text descriptions.
%
TVP**Krishna et al.**, "Visual Genome: Connecting Language and Vision using Multimodal Learning" leverages RNNs to learn text representations, which condition a GAN-based framework in order to generate videos from a single frame.
%
MMVG**Xu et al.**, "Unsupervised Deep Learning for Video Prediction and Generation" combines a VQ-GAN with a masked transformer predictor.
%
More recently, several methods leverage diffusion models with text conditioning **Song et al.**, "Denoising Diffusion Probabilistic Models".
%
Concurrently with our work, **Wang et al.**, "Learning Object Representations for Robust Visual Recognition" combine object-centric learning with text-conditional image-to-video generation.
%
The authors propose an autoregressive diffusion model conditioned on object slots and text descriptions and evaluate on simple datasets.
%
In contrast, we explicitly model the object dynamics with an autoregressive transformer, and evaluate our approach on more complex robotics simulations.