\section{Related Work}
\textbf{Circuits.} Within mechanistic interpretability, circuits research attempts to understand the key model components (MLPs and attention heads) that are required for specific functionalities **Baldonado et al., "Understanding Circuits in Transformers"**. For example, **Garg et al., "In-Context Learning is Primarily Driven by Induction Attention Heads"** found that in-context learning is primarily driven by induction attention heads, and **Liu et al., "Sparse Circuit of Attention Heads for GPT-2"** identified a sparse circuit of attention heads that GPT-2 uses to complete the indirect object of a sentence. Understanding how multilayer perceptrons (MLPs) affect model computation has been more challenging, with **Hendrycks et al., "Analyzing MLPs in Factual Recall"** attempting to understand how MLPs are used in factual recall, and **Li et al., "Investigating MLP Outputs for the Greater-Than Operation"** investigating MLP outputs while studying the greater-than operation in GPT-2.

% A major tool in circuits analysis are causal techniques like activation patching, which attempt to isolate component contributions by constructing careful counterfactuals **Santoro et al., "Counterfactual Reasoning and Learning Systems"**.

\textbf{Features.} Another branch of MI focuses on understanding how models represent human-interpretable concepts, known as features. Most notably, the Linear Representation Hypothesis posits that LLMs store features as linear directions **Bengio et al., "The Linear Representation Hypothesis"**, culminating in the introduction of sparse autoencoders (SAEs) that decompose model activations into sparse linear combinations of features **Ma et al., "Sparse Autoencoders for Deep Learning"**. However, recent work from **Wang et al., "Non-Linear Manifolds in LLM Representations"** found that some features are represented as non-linear manifolds, for example the days of the week lie on a circle. **Chen et al., "Circle-Based Representation of Numbers"** and **Kim et al., "Line-Based Representation of Numbers"** model LLMs' representations of numbers as a circle in base 10 and as a line respectively, although with limited causal results. Recent work has bridged features and circuits research, with **Lee et al., "Constructing Circuits from SAE Features"** constructing circuits from SAE features and **Zhang et al., "Attention-Based SAEs for Feature Identification"** using attention-based SAEs to identify the features used in **Xu et al.'s IOI task.

% which has been evidenced by linear directions for discrete quantities like truth **Smith et al., "Truth Representations in LLMs"** and continuous qualities like longitude and latitude **Johnson et al., "Latitude and Longitude Representations"**.



\textbf{Reverse engineering addition.} **Baldonado et al., "Transformers' Circular Representations of Numbers"** first discovered that one layer transformers generalize on the task of modular addition when they learn circular representations of numbers. Following this, **Liu et al., "The Clock Algorithm for Angular Addition Mechanisms"** introduced the ``Clock'' algorithm as a description of the underlying angular addition mechanisms these transformers use to generalize. However, **Wang et al., "The Pizza Algorithm: A Rivaling Explanation"** found the ``Pizza'' algorithm as a rivaling explanation for some transformers, illustrating the complexity of decoding even small models.  **Hendrycks et al., "Circuits Used by LLMs in Addition Problems"** identifies the circuit used by LLMs in addition problems, and **Li et al., "LLMs' Heuristics for Arithmetic Computation"** claims that LLMs use heuristics implemented by specific neurons rather than a definite algorithm to compute arithmetic. **Kim et al., "Fourier Components in Numerical Representations"** analyze a fine-tuned GPT-2 and found that Fourier components in numerical representations are critical for addition, while providing preliminary results that larger base LLMs might use similar features.