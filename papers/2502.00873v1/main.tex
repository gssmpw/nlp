%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{subcaption}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
% TODO: REPLACE ICML STY FILE. REMEMBER WE CHANGED
%printAffiliationsAndNotice

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Language Models Use Trigonometry to Do Addition}

\begin{document}

\twocolumn[
\icmltitle{Language Models Use Trigonometry to Do Addition}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.

\begin{icmlauthorlist}
\icmlauthor{Subhash Kantamneni}{mit}
\icmlauthor{Max Tegmark}{mit}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{mit}{Massachusetts Institute of Technology}

\icmlcorrespondingauthor{Subhash Kantamneni}{subhashk@mit.edu}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNoticeMODIFIED{}  % leave blank if no need to mention equal contribution
% TODO: COMMENT THIS OUT
%\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
Mathematical reasoning is an increasingly important indicator of large language model (LLM) capabilities, yet we lack understanding of how LLMs process even simple mathematical tasks. To address this, we reverse engineer how three mid-sized LLMs compute addition. We first discover that numbers are represented in these LLMs as a generalized helix, which is strongly causally implicated for the tasks of addition and subtraction, and is also causally relevant for integer division, multiplication, and modular arithmetic. We then propose that LLMs compute addition by manipulating this generalized helix using the “Clock” algorithm: to solve $a+b$, the helices for $a$ and $b$ are manipulated to produce the $a+b$ answer helix which is then read out to model logits. We model influential MLP outputs, attention head outputs, and even individual neuron preactivations with these helices and verify our understanding with causal interventions. By demonstrating that LLMs represent numbers on a helix and manipulate this helix to perform addition, we present the first representation-level explanation of an LLM's mathematical capability.
\end{abstract}


% FIGURE LIST
% \begin{enumerate}
%     \item Figure 1: Four dials, one for each period that shows how a+b gets computed. Perhaps a line at the bottom for linear component. One column
%     \item Figure 2: one column vertically stacked (?) fourier, pc1
%     \item Figure 3: plots for different T and projections (probably need to include some variance calc)
%     \item Figure 4: Comparing fits on a+0=
%     \item Figure 5: residual stream well modeled by helix(a+b)
%     \item Figure 6: activation patching model components. MLPs and attentions
%     \item Figure 7: two columns. MLP helix IE / total IE and DE/IE ratio graph (put line at MLP 18)
%     \item Figure 8: two columns. neuron preactivations and fits
%     \item Figure 9: Neuron fit accuracy
%     \item Figure 10: DE/IE ratio vs ab helix. shows clear break at layer 18.
% \end{enumerate}


% Fig 1 Latex: \Huge a+b=\\
% \Huge \textbf{Step 1: Represent $a,b$ as a helix with}
% \\\Huge \mathrm{helix}(t)= t+\sum_{T = 2,5,10,100} \cos({\frac{2\pi}{T}t})+\sin({\frac{2\pi}{T}t})
% \\ \Huge \textbf{Step 2: Create } \textrm{helix(}a+b)
% \\ \Huge \textbf{Step 3: Translate answer helix to tokens}


\section{Introduction}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/fig1.pdf}
    \caption{\textbf{Illustrating the Clock algorithm.} We find that LLMs represent numbers on a helix. When computing the addition problem $a+b$, LLMs rotate the $a$ and $b$ helices, as if on a clock, to create the $a+b$ helix and read out the final answer.} 
    \label{fig:fig1}
\end{figure}

Large language models (LLMs) display surprising and significant aptitude for mathematical reasoning \cite{ahn2024largelanguagemodelsmathematical, SatputeLLMMath}, which is increasingly seen as a benchmark for LLM capabilities \cite{OpenAI, glazer2024frontiermathbenchmarkevaluatingadvanced}. Despite LLMs' mathematical proficiency, we have limited understanding of how LLMs process even simple mathematical tasks like addition. Understanding mathematical reasoning is valuable for ensuring LLMs' reliability, interpretability, and alignment in high-stakes applications.

In this study, we reverse engineer how GPT-J, Pythia-6.9B, and Llama3.1-8B compute the addition problem $a+b$ for $a,b \in [0,99]$. Remarkably, we find that LLMs use a form of the ``Clock'' algorithm to compute addition, which was previously proposed by \citet{nanda2023progress} as a mechanistic explanation of how one layer transformers compute modular addition (and later named by \citet{zhong2023the}).

To compute $a+b$, all three LLMs represent $a$ and $b$ as a helix on their tokens and construct $\mathrm{helix}(a+b)$ on the last token, which we verify with causal interventions. We then focus on how GPT-J implements the Clock algorithm by investigating MLPs, attention heads, and even specific neurons. We find that these components can be understood as either constructing the $a+b$ helix by manipulating the $a$ and $b$ helices, or using the $a+b$ helix to produce the answer in the model's logits. We visualize this procedure in Fig. \ref{fig:fig1} as rotating the dial of a clock. 

Our work is in the spirit of mechanistic interpretability (MI), which attempts to reverse engineer the functionality of machine learning models. However, most LLM MI research focuses either on identifying circuits, which are the minimal set of model components required for computations, or understanding features, which are the representations of concepts in LLMs. A true mechanistic explanation requires understanding both how an LLM represents a feature and how downstream components manipulate that feature to complete a task. To our knowledge, we are the first work to present this type of description of an LLM's mathematical capability, identifying that LLMs represent numbers as helices and compute addition by manipulating these helices with the interpretable Clock algorithm.
% Understanding how LLMs process addition can provide insight into the mechanisms underlying other LLM reasoning mechanisms, especially because addition is implicit in many mathematical problems. 

% Mechanistic interpretability CITE attempts to reverse engineer the performance of machine learning models

\section{Related Work}
\textbf{Circuits.} Within mechanistic interpretability, circuits research attempts to understand the key model components (MLPs and attention heads) that are required for specific functionalities \cite{ olah2020zoom, elhage2021mathematical}. For example, \citet{olsson2022context} found that in-context learning is primarily driven by induction attention heads, and \citet{wang2023interpretability} identified a sparse circuit of attention heads that GPT-2 uses to complete the indirect object of a sentence. Understanding how multilayer perceptrons (MLPs) affect model computation has been more challenging, with \citet{nanda2023factfinding} attempting to understand how MLPs are used in factual recall, and \citet{hanna2023how} investigating MLP outputs while studying the greater-than operation in GPT-2.

% A major tool in circuits analysis are causal techniques like activation patching, which attempt to isolate component contributions by constructing careful counterfactuals \cite{meng2022locating, syed2023attributionpatchingoutperformsautomated, kramár2024atpefficientscalablemethod, heimersheim2024useinterpretactivationpatching, goldowskydill2023localizingmodelbehaviorpath, geiger2024causalabstractiontheoreticalfoundation}.

\textbf{Features.} Another branch of MI focuses on understanding how models represent human-interpretable concepts, known as features. Most notably, the Linear Representation Hypothesis posits that LLMs store features as linear directions \cite{park2023the, elhage2022superposition}, culminating in the introduction of sparse autoencoders (SAEs) that decompose model activations into sparse linear combinations of features \cite{huben2024sparse, bricken2023monosemanticity, templeton2024scaling, gao2024scalingevaluatingsparseautoencoders, rajamanoharan2024improvingdictionarylearninggated}. However, recent work from \citet{engels2024languagemodelfeatureslinear} found that some features are represented as non-linear manifolds, for example the days of the week lie on a circle. \citet{levy2024languagemodelsencodenumbers} and \citet{zhu-etal-2025-language} model LLMs' representations of numbers as a circle in base 10 and as a line respectively, although with limited causal results. Recent work has bridged features and circuits research, with \citet{marks2024sparsefeaturecircuitsdiscovering} constructing circuits from SAE features and \citet{makelov2024towards} using attention-based SAEs to identify the features used in \citet{wang2023interpretability}'s IOI task.

% which has been evidenced by linear directions for discrete quantities like truth \citep{marks2024geometrytruthemergentlinear} and continuous qualities like longitude and latitude \citep{gurnee2024language}. 



\textbf{Reverse engineering addition.} \citet{liu2022towards} first discovered that one layer transformers generalize on the task of modular addition when they learn circular representations of numbers. Following this, \citet{nanda2023progress} introduced the ``Clock'' algorithm as a description of the underlying angular addition mechanisms these transformers use to generalize. However, \citet{zhong2023the} found the ``Pizza'' algorithm as a rivaling explanation for some transformers, illustrating the complexity of decoding even small models.  \citet{Stolfo_Belinkov_Sachan_2023} identifies the circuit used by LLMs in addition problems, and \citet{Nikankin_Reusch_Mueller_Belinkov_2024} claims that LLMs use heuristics implemented by specific neurons rather than a definite algorithm to compute arithmetic. \citet{zhou2024pretrained} analyze a fine-tuned GPT-2 and found that Fourier components in numerical representations are critical for addition, while providing preliminary results that larger base LLMs might use similar features.


\section{Problem Setup}

\textbf{Models} As in \citet{Nikankin_Reusch_Mueller_Belinkov_2024}, we analyze 3 LLMs: GPT-J (6B parameters) \citep{gpt-j}, Pythia-6.9B \citep{pythia-6.9b}, and Llama3.1-8B \citep{llama3.1-8B}. All three models are autoregressive transformers which process tokens $x_0,...,x_n$ to produce probability distributions over the likely next token $x_{n+1}$ \cite{NIPSVaswani}. The $i$th token is embedded as $L$ hidden state vectors (also known as the residual stream), where $L$ is the number of layers in the transformer. Each hidden state is the sum of multilayer perceptron ($\mathrm{MLP}$) and attention ($\mathrm{attn}$) layers.

\begin{equation}
\begin{aligned}
    h^{l}_i &= h^{l-1}_i + a^{l}_i + m^{l}_i, \\
    a^{l}_i &= \mathrm{attn}^{l}\left(h^{l-1}_1, h^{l-1}_2, \dots, h^{l-1}_i\right), \\
    m^{l}_i &= \mathrm{MLP}^{l}(a^{(l)}_i + h^{l-1}_i).
\end{aligned}
\label{eq:layer_definitions}
\end{equation}


% W^{(l)}_{\mathrm{proj}} \sigma\left(W^{(l)}_{\text{fc}} \gamma\left(a^{(l)}_i + h^{(l-1)}_i\right)\right)


GPT-J and Pythia-6.9B use simple MLP implementations, namely $\mathrm{MLP}(x) = \sigma\left({xW_{\mathrm{up}}}\right)W_{\mathrm{down}}$, where $\sigma(x)$ is the sigmoid function. Llama3.1-8B uses a gated MLP, $ \mathrm{MLP}(x) = \sigma\left(x W_{\mathrm{gate}}\right) \circ \left(x W_{\mathrm{in}}\right) W_{\mathrm{out}}$, where $\circ$ represents the Hadamard product \citep{liu2021pay}. GPT-J tokenizes the numbers $[0,361]$ (with a space) as a single token, Pythia-6.9B tokenizes $[0,557]$ as a single token, and Llama3.1-8B tokenizes $[0,999]$ as a single token. We focus on the single-token regime for simplicity. 

\textbf{Data} To ensure that answers require only a single token for all models, we construct problems $a+b$ for integers $a,b \in [0,99]$. We evaluate all three models on these 10,000 addition problems, and find that all models can competently complete the task: GPT-J achieves 80.5\% accuracy, Pythia-6.9B achieves 77.2\% accuracy, and Llama3.1-8B achieves 98.0\% accuracy. For the prompts used and each model's performance heatmap by $a$ and $b$, see Appendix \ref{sec:app_model_perf}. Despite Llama3.1-8B's impressive performance, in the main paper we focus our analysis on GPT-J because its simple MLP allows for easier neuron interpretation. We report similar results for Pythia-6.9B and Llama3.1-8B in the Appendix. 



\section{LLMs Represent Numbers as a Helix}
To generate a ground up understanding of how LLMs compute $a+b$, we first aim to understand how LLMs represent numbers. To identify representational trends, we run GPT-J on the single-token integers $a \in [0,360]$. We do not use $a = 361$ because $360$ has more integer divisors, allowing for a simpler analysis of periodic structure. We conduct analysis on $h^0_{360}$, which is the residual stream following layer 0 with shape $[360, \mathrm{model\_dim}]$. We choose to use the output of layer 0 rather than directly analyzing the embeddings because prior work has shown that processing in layer 0 is influential for numerical tasks \cite{Nikankin_Reusch_Mueller_Belinkov_2024}.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/fig2.pdf}
    \caption{\textbf{Number representations are both periodic and linear.} \textit{Top} The residual stream after layer 0 in GPT-J is sparse in the Fourier domain when batching the hidden states for $a\in[0,360]$ together. We average the magnitude of the Fourier transform of the batched matrix $h_{360}^0$ across the model dimension. 
     \textit{Bottom} In addition to this periodicity, the first PCA component is roughly linear in $a$ for $a \in [0,99]$.}
    \label{fig:fig2_fourier_linear}
\end{figure}

\subsection{Investigating Numerical Structure}
\label{sec:invest-struc}


\textbf{Linear structure}. To investigate structure in numerical representations, we perform a PCA \cite{pca} on $h^0_{360}$ and find that the first principal component (PC1) for $a \in [0,360]$ has a sharp discontinuity at $a = 100$ (Fig. \ref{fig:app_pc1_360}, Appendix \ref{sec:app_struc_invest}), which implies that GPT-J uses a distinct representation for three-digit integers. Instead, in the bottom of Fig. \ref{fig:fig2_fourier_linear}, we plot PC1 for $h^0_{99}$ and find that it is well approximated by a line in $a$. Additionally, when plotting the Euclidean distance between $a$ and $a+\delta n$ for $a \in [0,9]$ (Fig. \ref{fig:app_deltan_euclid}, Appendix \ref{sec:app_struc_invest}), we see that the distance is locally linear. The existence of linear structure is unsurprising - numbers are semantically linear, and LLMs often represent concepts linearly. 

\textbf{Periodic Structure.} We center and apply a Fourier transform to $h^0_{360}$ with respect to the number $a$ being represented and the $\mathrm{model\_dim}$. In Fig. \ref{fig:fig2_fourier_linear}, we average the resulting spectra across $\mathrm{model\_dim}$ and observe a sparse Fourier domain with high-frequency components at $T=[2,5,10]$. Additionally, when we compare the residual streams of all pairs of integers $a_1$ and $a_2$, we see that there is distinct periodicity in both their Euclidean distance and cosine similarity (Fig. \ref{fig:app_cosine_euclid}, Appendix \ref{sec:app_struc_invest}). These Fourier features were also identified by \citet{zhou2024pretrained}, and although initially surprising, are sensible. The units digit of numbers in base 10 is periodic ($T = 10$), and it is reasonable that qualities like evenness ($T = 2$) are useful for tasks. 
% MT: THIS IS INCORRECT: Note that if we used $h^0_{361}$, $T = 2$ would not complete a full cycle and thus would not be identified (Appendix \ref{sec:app_struc_invest}). 




% However, it is not immediately clear how the magnitude of numbers could be stored with only periodic Fourier features. Additionally, the Euclidean distance between $a$ and $a+\delta n$ monotonically increases with $|\delta n|$  (Fig. \ref{fig:app_deltan_euclid}, Appendix \ref{sec:app_struc_invest}) without the modularity we would expect with only Fourier features.


\subsection{Parameterizing the Structure as a Helix}
To account for both the periodic and linear structure in numbers, we propose that numbers can be modeled helically. Namely, we posit that $h^l_a$, the residual stream immediately preceding layer $l$ for some number $a$, can be modeled as 
\begin{equation}
\begin{aligned}
    h^l_a &= \mathrm{helix}(a) = C B(a)^T, \\
    B(a) &= \big[a, \cos\left(\frac{2 \pi}{T_1}a\right), \sin\left(\frac{2 \pi}{T_1}a\right), \\
    &\quad \dots,  \cos\left(\frac{2 \pi}{T_k}a\right), \sin\left(\frac{2 \pi}{T_k}a\right)\big].
\end{aligned}
\label{eq:helix_param}
\end{equation}


$C$ is a matrix applied to the basis of functions $B(a)$, where $B(a)$ uses $k$ Fourier features with periods $T = [T_1, \dots T_k]$. The $k=1$ case represents a regular helix; for $k>1$, the independent Fourier features share a single linear direction. We refer to this structure as a generalized helix, or simply a helix for brevity.

We identify four major Fourier features: $T = [2,5,10,100]$. We use the periods $T = [2,5,10]$ because they have significant high frequency components in Fig. \ref{fig:fig2_fourier_linear}. We are cautious of low frequency Fourier components, and use $T = 100$ both because of its significant magnitude, and by applying the inductive bias that our number system is base 10.


\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/fig3_helix_fit_layer0_gpt-j-6B.pdf}
    \caption{\textbf{Helix subspace visualized.} For GPT-J's layer 0 output, we project each of the numbers $a \in [0,99]$ onto our fitted $T = [2,5,10,100]$ helix subspace, and visualize it. In the top row, we plot $\sin({\frac{2\pi}{T_i}a})$ vs $\cos({\frac{2\pi}{T_i}a})$ for each $T_i \in T$ and plot all $a$ congruent under $a\mod T$ in the same color and annotate their mean. The bottom row contains the linear component subplot.}
    \label{fig:fig3_helix_proj}
\end{figure*}

\subsection{Fitting a Helix}
We fit our helical form to the residual streams on top of the $a$ token for our $a+b$ dataset. In practice, we first use PCA to project the residual stream at each layer to 100 dimensions. To ensure we do not overfit with Fourier features, we consider all combinations of $k$ Fourier features, with $k \in [1,4]$. If we use $k$ Fourier features, the helical fit uses $2k+1$ basis functions (one linear component, $2k$ periodic components). We then use linear regression to find some coefficient matrix $C_\mathrm{PCA}$ of shape $100 \times 2k+1$ that best satisfies $\mathrm{PCA}(h^l_a) = C_\mathrm{PCA} B(a)^T$. Finally, we use the inverse PCA transformation to project $C_\mathrm{PCA}$ back into the model's full residual stream dimensionality to find $C$.

We visualize the quality of our fit for layer 0 when using all $k = 4$ Fourier features with $T = [2,5,10,100]$ in Fig. \ref{fig:fig3_helix_proj}. To do so, we calculate $C^\dagger h$, where $C^\dagger$ is the Moore-Penrose pseudo-inverse of $C$. Thus, $C^\dagger h$ represents the projection of the residual stream into the helical subspace. When analyzing the columns of $C$, we find that the Fourier features increase in magnitude with period and are mostly orthogonal (Appendix \ref{sec:app_helix_prop}).



\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/fig4_helix_intervention_a_gpt-j-6B.pdf}
    \caption{\textbf{Helix causal intervention results.} We use activation patching to causally determine if our fits preserve the information the model uses to compute $a+b$. We find that our helical and circular fits are strongly causally implicated, often outperforming the PCA baseline.}
    \label{fig:fig4_helix_int}
\end{figure}

\subsection{Evaluating the Quality of the Helical Fit}
\label{sec:eval-helix-fit}

We want to causally demonstrate that the model actually uses the fitted helix. To do so, we employ activation patching. Activation patching isolates the contribution of specific model components towards answer tokens \cite{meng2022locating, heimersheim2024useinterpretactivationpatching}. Specifically, to evaluate the contribution of some residual stream $h_a^l$ on the $a$ token, we first store $h_{a,\text{clean}}^l$ when the model is run on a ``clean'' prompt $a+b$. We then run the model on the corrupted prompt $a'+b$ and store the model logits for the clean answer of $a+b$. Finally, we patch in the clean $h_{a,\text{clean}}^l$ on the corrupted prompt $a'+b$ and calculate $LD_a^l = \mathrm{logit_{patched}}(a+b)-\mathrm{logit_{corrupted}}(a+b)$, where $LD_a^l$ is the logit difference for $h_a^l$. By averaging over 100 pairs of clean and corrupted prompts, we can evaluate $h_a^l$'s ability to restore model behavior to the clean answer $a+b$. To reduce noise, all patching experiments only use prompts the model can successfully complete.

To leverage this technique, we follow \citet{engels2024languagemodelfeatureslinear} and input our fit for $h_{a,\text{clean}}^l$ when patching. This allows us to causally determine if our fit preserves the information the model uses for the computation. We compare our $k$ Fourier feature helical fit with four baselines: using the actual $h_{a,clean}^l$ (layer patch), the first $2k+1$ PCA components of $h_{a,clean}^l$ (PCA), a circular fit with $k$ Fourier components (circle), and a polynomial fit with basis terms $B(a) = [a,a^2,...a^{2k+1}]$ (polynomial). For each value of $k$, we choose the combination of Fourier features that maximizes $\frac{1}{L} \sum_l LD_a^l$ as the best set of Fourier features. 

In Fig. \ref{fig:fig4_helix_int}, we see that the helical fit is most performant against baselines, closely followed by the circular fit. This implies that Fourier features are predominantly used to compute addition. Surprisingly, the $k=4$ full helical and circular fits dominate the strong PCA baseline and approach the effect of layer patching, which suggests that we have identified the correct ``variables'' of computation for addition. Additionally, we note a sharp jump between the fit for layer 0's input (the output of the embedding) and layer 1's input, aligning with evidence from \citet{Nikankin_Reusch_Mueller_Belinkov_2024} that layer 0 is necessary for numerical processing. 

In Appendix \ref{sec:app_helix_causal}, we provide evidence that Llama3.1-8B and Pythia-6.9B also use helical numerical representations. Additionally, we provide evidence that our fits are not overfitting by using a train-test split with no meaningful effect on our results. The helix functional form is not overly expressive, as a helix trained on a randomized order of $a$ is not causally relevant. We also observe continuity when values of $a$ that the helix was not trained on are projected into the helical subspace. This satisfies the definition of a nonlinear feature manifold proposed by \citet{olah2024manifold}, and provides additional evidence for the argument of \citet{engels2024languagemodelfeatureslinear} against the strongest form of the Linear Representation Hypothesis.

% Notably, when we train a simple $T = [100]$ helix on the numbers $a \in [0,99]$ for all $a$ that does not end with 3, we find that $a = 3,13,...93$ projected into this helical subspace lie continuously in the helix (Fig. \ref{fig:app_manifold} in Appendix \ref{sec:app_helix_prop}). Following \citet{olah2024manifold}, we interpret this continuity as evidence that our helices are nonlinear feature manifolds, like that of \citet{engels2024languagemodelfeatureslinear}, providing additional evidence against the strongest form of the Linear Representation Hypothesis.


\begin{table}
    \centering
    \caption{\textbf{Performance of fits across tasks.} We calculate $\max_l LD_a^l$ for each fit across a variety of numerical tasks. While the helix fit is competitive, we find that it underperforms the PCA baseline on three tasks.}
    \begin{tabular}{|l|c|c|c|c|c|}
        \textbf{Task} & \textbf{Full} & \textbf{PCA} & \textbf{Helix} & \textbf{Circle} & \textbf{Poly} \\ \specialrule{1.5pt}{0pt}{0pt}
        \textbf{$a+b=$} & \textbf{8.34} & 6.13 & \textbf{7.21} & 6.83 & 3.09 \\ \hline
        \textbf{$a-23=$} & \textbf{7.45} & 6.16 & \textbf{7.05} & 6.52 & 2.93 \\ \hline
        \textbf{$a//5=$} & \textbf{5.48} & \textbf{5.24} & 4.55 & 4.25 & 4.55 \\ \hline
        \textbf{$a*1.5=$} & \textbf{7.86} & \textbf{6.65} & 5.16 & 4.85 & 4.84 \\ \hline
        \textbf{$a \mod 2=$} & \textbf{0.98} & 1.00 & 1.11 & \textbf{1.12} & 0.88 \\ \hline
        \textbf{$x-a=0$, $x=$} & \textbf{6.70} & \textbf{4.77} & 4.56 & 4.34 & 2.98 \\ \specialrule{1.5pt}{0pt}{0pt}
    \end{tabular}
    \label{tab:other_tasks}
\end{table}

\subsection{Is the Helix the Full Picture?}
To identify if the helix sufficiently explains the structure of numbers in LLMs, we test on five additional tasks.
\begin{enumerate}
    \item $a-23$ for $a \in [23,99]$
    \item $a//5$ (integer division) for $a \in [0,99]$
    \item $a*1.5$ for even $a \in [0,98]$
    \item $a\mod 2$ for $a \in [0,99]$
    \item If $x-a = 0$, what is $x = $ for $a \in [0,99]$
\end{enumerate}

For each task, we fit full helices with $T = [2,5,10,100]$ and compare against baselines. In Table \ref{tab:other_tasks}, we describe our results on these tasks by listing $\max_l LD_a^l$, which is the maximal causal power of each fit (full plot and additional task details in Appendix \ref{sec:app_helix_causal}). Notably, while the helix is causally relevant for all tasks, we see that it underperforms the PCA baseline on tasks 2, 3, and 5. This implies that there is potentially additional structure in numerical representations that helical fits do not capture. However, we are confident that the helix is used for addition. When ablating the helix dimensions from the residual stream (i.e. ablating $C^\dagger$ from $h_a^l$), performance is affected roughly as much as ablating $h_a^l$ entirely (Fig. \ref{fig:app_ablation}, Appendix \ref{sec:app_helix_prop}).

Thus, we conclude that LLMs use a helical representation of numbers to compute addition, although it is possible that additional structure is used for other tasks.

\section{LLMs Use the Clock Algorithm to Compute Addition}

\subsection{Introducing the Clock Algorithm}

% -Helix and fourier features dominate. inspired be Neel we propose clock algorithm
% - Lay out clock algorithm
% $a$ $b$ helices moved to last token by attention heads. MLPs and some attention heads use hte $a,b$ helices to construct the $a+b$ helix. Finally, MLPs and some attention heads write the answer to logits. MAKE PRETTY BOX

Taking inspiration from \citet{nanda2023progress}, we propose that LLMs manipulate helices to compute addition using the ``Clock'' algorithm. 


\tcbset{colback=lightgray!20, colframe=white, boxrule=0pt, sharp corners}

\begin{tcolorbox}
To compute $a+b=$, GPT-J
\begin{enumerate}
    \item Embeds $a$ and $b$ as helices on their own tokens.
    \item A sparse set of attention heads, mostly in layers 9-14, move the $a$ and $b$ helices to the last token.
    \item MLPs 14-18 manipulate these helices to create the helix $a+b$. A small number of attention heads help with this operation.
    \item MLPs 19-27 and a few attention heads ``read'' from the $a+b$ helix and output to model logits.
\end{enumerate}
\end{tcolorbox}


% Evidence:for $a+b=$ $a$ token is helix(a), $b$ token is helix(b). Thus, equals token is where computation happens. We model with helix(a) +helix(b) + helix(a+b), and find that using helix(a+b) is good (better than PCA) FIGURE OF LAST TOKEN FITS. implies this is enough to do computation, SHOW TRUE FOR OTHER MODELS. 


Since we have already shown that models represent $a$ and $b$ as helices (Appendix \ref{sec:app_helix_causal}), we provide evidence for the last three steps in this section. In Fig. \ref{fig:fig5_ab_helix} we observe that last token hidden states are well modeled by $h_=^l = \mathrm{helix}(a, b, a+b)$, where $\mathrm{helix}(x,y)$ is shorthand to denote $\mathrm{helix}(x) + \mathrm{helix}(y)$. Remarkably, despite only using 9 parameters, at some layers $\mathrm{helix}(a+b)$ fits last token hidden states better than a 27 dimensional PCA. The $a+b$ helix having such causal power implies it is at the heart of the computation.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/fig5_ab_fits_logit_diff_gpt-j-6B.pdf}
    \caption{\textbf{Last token hidden states are well-modeled by $\mathrm{helix}(a+b)$.} We use activation patching to show that $h_=^l$ for GPT-J is well modeled by helices, in particular $\mathrm{helix}(a+b)$.}
    \label{fig:fig5_ab_helix}
\end{figure}


In Appendix \ref{sec:app_add_clock}, we show that other LLMs also use $\mathrm{helix}(a+b)$. Since the crux of the Clock algorithm is computing the answer helix for $a+b$, we take this as compelling evidence that all three models use the Clock algorithm. However, we would like to understand how specific LLM components implement the algorithm. To do so, we focus on GPT-J.

In Fig. \ref{fig:fig6_mlp_attn_patching}, we use activation patching to determine which last token MLP and attention layers are most influential for the final result. We also present path patching results, which isolates how much components directly contribute to logits. For example, MLP18's total effect (TE, activation patching) includes both its indirect effect (IE), or how MLP18's output is used by downstream components like MLP19, and its direct effect (DE, path patching), or how much MLP18 directly boosts the answer logit.\footnote{For more on path patching, we refer readers to \citet{goldowskydill2023localizingmodelbehaviorpath, wang2023interpretability}} In Fig. \ref{fig:fig6_mlp_attn_patching}, we see that MLPs dominate direct effect.
% Using a linear approximation of model components, the sum of each component's direct effect is the logit for the answer.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fig6_act_path_patching_mlp_attn.pdf}
    \caption{\textbf{MLPs drive computation of $a+b$.} By using activation and path patching, we find that MLPs are most implicated in constructing the final answer, along with early attention layers.}
    \label{fig:fig6_mlp_attn_patching}
\end{figure}

We now investigate specific attention heads, MLPs, and individual neurons.


% Although we believe this is compelling evidence for using Clock, we want to understand how specific components are used. We can activation patch in Fig. MLP do most of it. In fig, we also introduce path patching, which shows the effect of the model directly added to the final residual stream (not thru downstream logits), which also show MLPs dominate. To have more fine grained understanding, we investigate attention heads and MLPs and eventually neurons seperately, to figure out how they do clock. of which components do what step f the clock algorithm

\subsection{Investigating Attention Heads}
\label{sec:attn}
% Find minimum set of attention heads required for circuit. We first identify top circuit components by activation patching. One by one, we add one of them back into the model until we get 80\% of total effect, which requires $k=20$
In GPT-J, every attention layer is the sum of 16 attention heads whose outputs are concatenated. We activation and path patch each attention head on the last token and rank them by total effect (TE). To determine the minimal set of attention heads required, we activation patch $k$ attention heads at once, and find the minimum $k$ such that their combined total effect approximates patching in all attention heads. In Appendix \ref{sec:app_attn_head}, we see that patching $k=17$ heads achieves 80\% of the effect of patching in all 448 attention heads, and we choose to round up to $k = 20$ heads (83.9\% of effect). 

Since attention heads are not as influential as MLPs in Fig. \ref{fig:fig6_mlp_attn_patching}, we hypothesize that they primarily serve two roles: 1) moving the $a,b$ helices to the last token to be processed by downstream components ($a,b$ heads) and 2) outputting the $a+b$ helix directly to logits ($a+b$ heads). Some mixed heads output all three $a,b, \text{ and }a+b$ helices. We aim to categorize as few attention heads as mixed as possible. 

% Note that all head outputs are well modeled by $\mathrm{helix}(a,b,a+b)$ (Appendix \ref{sec:app_attn_head}).

To categorize attention heads, we turn to two metrics. $c_{a,b}$ is the confidence that a certain head is an $a,b$ head, which we quantify with $c_{a,b} = (1-\frac{\mathrm{DE}}{\mathrm{TE}})\frac{\mathrm{helix}(a,b)}{\mathrm{helix}(a,b,a+b)}$. The first term represents the fractional indirect effect of the attention head, and the second term represents the head's total effect recoverable by just using the $a,b$ helices instead of $\mathrm{helix}(a,b,a+b)$. Similarly, we calculate $c_{a+b}$ as the confidence the head is an $a+b$ head, using $c_{a+b} = \frac{\mathrm{DE}}{\mathrm{TE}}\frac{\mathrm{helix}(a+b)}{\mathrm{helix}(a,b,a+b)}$.

We sort the $k = 20$ heads by $c = \max({c_{a,b},c_{a+b}})$. If a head is an $a+b$ head, we model its output using the $a+b$ helix and allow it only to output to logits (no impact on downstream components). If a head is an $a,b$ head, we restrict it to outputting $\mathrm{helix}(a,b)$. For $m=[1,20]$, we allow $m$ heads with the lowest $c$ to be mixed heads, and categorize the rest as $a,b$ or $a+b$ heads. We find that categorizing $m = 4$ heads as mixed is sufficient to achieve almost 80\% of the effect of using the actual outputs of all $k=20$ heads. Thus, most important attention heads obey our categorization. We list some properties of each head type below.

\begin{itemize}
    \item $a,b$ heads (11/20): In layers 9-14 (but two heads in $l = 16,18$), attend to the $a,b$ tokens, and output $a,b$ helices which are used mostly by downstream MLPs.
    \item $a+b$ heads (5/20): In layers 24-26 (but one head in layer 19), attend to the last token, take their input from preceding MLPs, and output the $a+b$ helix to logits.
    \item Mixed heads (4/20): In layers 15-18, attend to the $a,b, \text{ and } a+b$ tokens, receive input from $a,b$ attention heads and previous MLPs, and output the $a,b, \text{ and }a+b$ helices to downstream MLPs.
\end{itemize}

For evidence of these properties refer to Appendix \ref{sec:app_attn_head}. Notably, only mixed heads are potentially involved in creating the $a+b$ helix, which is the crux of the computation, justifying our conclusion from Fig. \ref{fig:fig6_mlp_attn_patching} that MLPs drive addition. 
% Then analyze $k = 20$. We note that all attention heads can be modeled by sum of helix, Hypothesize that some are moving a,b helices (early attention heads) and some are reading from the a+b helix to logits (late attention heads even after a+b helix is made). to quantify this, we calculate the direct effect of each attention head

% \subsubsection{Explaining DE}
% We can path patch. explain path patching. If DE/TE ratio is high, most of attention heads causally relevant output is because it promotes the correct answer to logits. If low, mostly used for downstream computation

% $a,b$ head If a head just gives a,b helix, likely has low DE/TE. $a+b$ head a+b helix, has high DE/IE. Quantify this by using metrics. 

% To determine which heads are which, calc metric, then if $a,b$ head, only allow output to be helix(a)+helix(b). If $a+b$ allow it only to write to logits helix(a+b). If mixed head, allow output to be modeled by helix(a,b,a+b). Starting with $k = 20$ mixed heads, we sort heads by their confidence = max(a,b score, a+b score). 80\% of total effect of using these $k = 20$ heads is achievable with 4 mixed heads. See APPENDIX FOR ADDITIONAL GRAPHS. p

% write out which heads are which. say what properties they have. (where they pay attention, where in network, where they write/ read from. Say additional information in Appendix

\subsection{Looking at MLPs}
\label{sec:mlp_analysis}
GPT-J seems to predominantly rely on last token MLPs to compute $a+b$. To identify which MLPs are most important, we first sort MLPs by total effect, and patch in $k = [1,L = 28]$ MLPs to find the smallest $k$ such that we achieve 95\% of the effect of patching in all $L$ MLPs. We use a sharper 95\% threshold because MLPs dominate computation and because there are so few of them. Thus, we use $k = 11$ MLPs in our circuit, specifically MLPs 14-27, with the exception of MLPs 15, 24, and 25 (see Appendix \ref{sec:app_mlp} for details). 

We hypothesize that MLPs serve two functions: 1) reading from the $a,b$ helices to create the $a+b$ helix and 2) reading from the $a+b$ helix to output the answer in model logits. We make this distinction using two metrics: $\mathrm{helix}(a+b)$/TE, or the total effect of the MLP recoverable from modeling its output with $\mathrm{helix}(a+b)$, and DE/TE ratio. In Fig. \ref{fig:fig7_dete_helixab}, we see that the outputs of MLPs 14-18 are progressively better modeled using $\mathrm{helix}(a+b)$. Most of their effect is indirect and thus their output is predominantly used by downstream components. At layer 19, $\mathrm{helix}(a+b)$ becomes a worse fit and more MLP output affects answer logits directly. We interpret this as MLPs 14-18 ``building'' the $a+b$ helix, which MLPs 19-27 translate to the answer token $a+b$.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fig7_mlp_helix_ab_and_de_ie_gpt-j-6B.pdf}
    \caption{\textbf{Two stages of circuit MLPs.} MLPs 14-18's outputs are well modeled by $\mathrm{helix}(a+b)$, suggesting that they build this helical representation, while MLPs 19-27 higher DE/TE ratio implies they output the answer to model logits.}
    \label{fig:fig7_dete_helixab}
\end{figure}

However, our MLP analysis has focused solely on MLP outputs. To demonstrate the Clock algorithm conclusively, we must look at MLP inputs. Recall that GPT-J uses a simple MLP: $\mathrm{MLP}(x) = \sigma\left({xW_{\mathrm{up}}}\right)W_{\mathrm{down}}$. $x$ is a vector of size $(4096,)$ representing the residual stream, and $W_{\mathrm{up}}$ is a $(4096,16384)$ projection matrix. The input to the MLP is thus the 16384 dimensional $xW_{\mathrm{up}}$. We denote the $n$th dimension of the MLP input as the $n$th neuron preactivation, and move to analyze these preactivations.


% MLPs seem to be heart of story. First find important ones. half MLPs give 95\%, mostly late.

% helix(a+b) models well till layer 19. include veritcal line there. additionally de/IE ratio changes there. implies that something changes there. we interpret it as when MLPs stop outputting a+b helix, and start outputting final answer in logit space

\subsection{Zooming in on Neurons}
Activation patching the $27*16384$ neurons in GPT-J is prohibitively expensive, so we instead use the technique of attribution patching to approximate the total effect of each neuron using its gradient (see \citet{kramár2024atpefficientscalablemethod}). We find that using just 1\% of the neurons in GPT-J and mean ablating the rest allows for the successful completion of 80\% of prompts (see Appendix \ref{sec:app_mlp}). Thus, we focus our analysis on this sparse set of $k = 4587$ neurons.

\subsubsection{Modeling Neuron Preactivations}
For a prompt $a+b$, we denote the preactivation of the $n$th neuron in layer $l$ as $N_n^l(a,b)$. When we plot a heatmap of $N_n^l(a,b)$ for top neurons in Fig. \ref{fig:fig8_neuron_fits}, we see that their preactivations are periodic in $a,b$, and $a+b$. When we Fourier decompose the preactivations as a function of $a+b$, we find that the most common periods are $T = [2,5,10,100]$, matching those used in our helix parameterization (Appendix \ref{sec:app_mlp}). This is sensible, as the $n$th neuron in a layer applies $W_{up}^n$ of shape $(4096,)$ to the residual stream, which we have effectively modeled as a $\mathrm{helix}(a,b,a+b)$. Subsequently, we model the preactivation of each top neuron as
% \begin{multline}
%     N_n^l(a,b) = \sum_{t = a,b,a+b} c_t t + \\
%     \sum_{T = [2,5,10,100]} c_{Tt} \cos\left({\frac{2\pi}{T}(t-d_{Tt})}\right)
%     \label{eq:neuron_helix}
% \end{multline}

\begin{equation}
    N_n^l(a,b) = \sum_{t = a,b,a+b} c_t t + \sum_{T = [2,5,10,100]} c_{Tt} \cos\left(\frac{2\pi}{T}(t-d_{Tt})\right)
    \label{eq:neuron_helix}
\end{equation}


For each neuron preactivation, we fit the parameters $c$ and $d$ in Eq. \ref{eq:neuron_helix} using gradient descent (see Appendix \ref{sec:app_mlp} for details). In Fig. \ref{fig:fig8_neuron_fits}, we show the highest magnitude fit component for a selection of top neurons. 

% We emphasize that the functional form in Eq. \ref{eq:neuron_helix} is able to model 10000 values of $N_n^l(a,b)$ using only 27 parameters, and we present additional analysis of the fit quality in Appendix \ref{sec:app_mlp}.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig8_neuron_fits.pdf}
    \caption{\textbf{Neuron preactivations and fits.} We visualize the preactivations $N_n^l(a,b)$ for four top neurons. Each neuron has clear periodicity in its preactivations, which we model using a helix inspired functional form.}
    \label{fig:fig8_neuron_fits}
\end{figure*}

We evaluate our fit of the top $k$ neurons by patching them into the model, mean ablating all other neurons, and measuring the resulting accuracy of the model. In Fig. \ref{fig:fig9_neuron_fit_res}, we see that our neuron fits provide roughly 75\% of the performance of using the actual neuron preactivations. Thus, these neurons are well modeled as reading from the helix. 

% We interpret this conclusion as a generalization of \citet{Nikankin_Reusch_Mueller_Belinkov_2024}'s claim that neurons implement modular heuristics for addition.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/fig9_fit_neurons_comparison_accuracy.pdf}
    \caption{\textbf{Evaluating neuron fits.} Patching in our fitted preactivations for the top $k$ neurons is roughly as effective as patching in their actual preactivations and ablating all other neurons.} 
    \label{fig:fig9_neuron_fit_res}
\end{figure}

% We attribution patch neurons, and find that 1\% of neurons is sufficient to regain 80\% of function. Analyze 1\% of neurons, and find that htey have trig functions in preactivations. THis makes sense, because residual stream is helix (do math). Model with helix (tough but doable). Give training parameters

\subsubsection{Understanding MLP Inputs}
We use our understanding of neuron preactivations to draw conclusions about MLP inputs. To do so, we first path patch each of the top $k$ neurons to find their direct effect and calculate their DE/TE ratio. For each neuron, we calculate the fraction of their fit that $\mathrm{helix}(a+b)$ explains, which we approximate by dividing the magnitude of $c_{T,a+b}$ terms by the total magnitude of $c_{Tt}$ terms in Eq. \ref{eq:neuron_helix}. For each circuit MLP, we calculate the mean of both of these quantities across top neurons, and visualize them in Fig. \ref{fig:fig10_neuron_trends}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig10_neuron_dete_helixab.pdf}
    \caption{\textbf{Neuron trends.} Neurons in MLPs 14-18 primarily read from the $a,b$ helices, while MLPs 19-27 primarily read from the $a+b$ helix and write to logits.}
    \label{fig:fig10_neuron_trends}
\end{figure}

Once again, we see a split at layer 19, where earlier neurons' preactivation fits rely on $a,b$ terms, while later neurons use $a+b$ terms and write to logits. Since the neuron preactivations represent what each MLP is ``reading'' from, we combine this result with our evidence from Section \ref{sec:mlp_analysis} to summarize the role of MLPs in addition.
\begin{itemize}
    \setlength\itemsep{0em} % Adjust the spacing between items
    \item MLPs 14-18 primarily read from the $a,b$ helices to create the $a+b$ helix for downstream processing.
    \item MLPs 19-27 primarily read from the $a+b$ helix to write the answer to model logits.
\end{itemize}
Thus, we conclude our case that LLMs use the Clock algorithm to do addition, with a deep investigation into how GPT-J implements this algorithm. 

\subsection{Limitations of Our Understanding}
There are several aspects of LLM addition we still do not understand. Most notably, while we provide compelling evidence that key components create  $\mathrm{helix}(a+b)$ from $\mathrm{helix}(a,b)$, we do not know the exact mechanism they use to do so. We hypothesize that LLMs use trigonometric identities like $\cos(a+b) = \cos(a)\cos(b)-\sin(a)\sin(b)$ to create $\mathrm{helix}(a+b)$. However, like the originator of the Clock algorithm \citet{nanda2023progress}, we are unable to isolate this computation in the model. This is unsurprising, as there is a large solution space for how models choose to implement low-level details of algorithms. For example, \citet{pizzaintegral} finds that in \citet{zhong2023the}'s ``Pizza'' algorithm for modular addition, MLPs in one layer transformers implement numerical integration techniques to transform $\cos(\frac{k}{2}(a+b))$ to $\cos(k(a+b))$. 

% For an investigation of model errors, refer to Appendix \ref{sec:app_invest_model_errors}.
The mere existence of the Pizza algorithm demonstrates that even one layer transformers have a complex solution space. Thus, even if the Clock algorithm is used by LLMs, it could be one method of an ensemble. We see evidence of this in Appendix \ref{sec:app_add_clock}, in that $\mathrm{helix}(a+b)$ is less causally implicated for Llama3.1-8B than other models, which we hypothesize is due to its use of gated MLPs. Additionally, other models must necessarily use modified algorithms for addition because of different tokenization schemes. For example, Gemma-2-9B \cite{gemma2} tokenizes each digit of a number separately and must use additional algorithms to collate digit tokens. Additionally, at different scales LLMs potentially learn different algorithms, providing another reason to be skeptical that the Clock algorithm is the one and only explanation for LLM addition.

%. While we have found preliminary evidence for Gemma-2-9B's use of helical numerical representations, it 




% For each layer, key neurons there, we find that computing mean a+b helix component in fit vs DE/IE ratio, we see a distinct split at MLP 19, where 14-18 read little from a+b helix, although those layers write to it, while 19- read a lot from the helix and dont read from it. Implies the story we've been saying, Put it together and get clock alogirhtm



% If use clock algorithm, why get problems wrong? most problems off by -10 or 10 (graph). One theory is that algo is bad to build a+b helix. Maybe carrying is hard? but then we expect more -10 answers when units digit have > 10. We dont see that (appendix). Thus, we assume its reading out answer from logits. Neurons logit lens are periodic, often with same period as their input, but not modelable. Looking at model logits, we see that they're periodic with dominant period 10. Often, we see that periods have bas towards smaller answers, which explains why our msot common error undershoots value 60\% of time, and why we do better on smaller addition problems than bigger ones (appendix figure)
% To our knowledge, this is the first comprehensive description of an LLM's mathematical capability.
\section{Conclusion}

We find that three mid-sized LLMs represent numbers as generalized helices and manipulate them using the interpretable Clock algorithm to compute addition. While LLMs could do addition linearly, we conjecture that LLMs use the Clock algorithm to improve accuracy, analogous to humans using decimal digits (which are a generalized helix with $T = [10,100,\dots]$) for addition rather than slide rules. In Appendix \ref{sec:app-conjecture}, we present preliminary results that GPT-J would be considerably less accurate on ``linear addition'' due to noise in its linear representations. Future work could analyze if LLMs have internal error-correcting codes for addition like the grid cells presented in \citet{PhysRevE.110.054303}.
 

The use of the Clock algorithm provides striking evidence that LLMs trained on general text naturally learn to implement complex mathematical algorithms. Understanding LLM algorithms is important for safe AI and can also provide valuable insight into model errors, as shown in Appendix \ref{sec:app_invest_model_errors}. We hope that this work inspires additional investigations into LLM mathematical capabilities, especially as addition is implicit to many reasoning problems. 


\section*{Acknowledgments}
We thank Josh Engels for participating in extensive conversations throughout the project. We also thank Vedang Lad, Neel Nanda, Ziming Liu, David Baek, and Eric Michaud for their helpful suggestions. This work is supported by the Rothberg Family Fund for Cognitive Science and IAIFI through NSF grant PHY-2019786.


% \section*{Acknowledgments}
% \section*{Contribution}

% \textbf{Limitations} We still do not understand some functions of the Clock algorithm. For example, we hypothesize that the exact mechanism to create $\mathrm{helix}(a+b)$ from $a,b$ involves using the cosine identity $\cos(a+b) = \cos(a)\cos(b)-\sin(a)\sin(b)$, but are unable to directly evidence this claim, similar to \citet{nanda2023progress}. Similarly, while we believe impactful neurons use constructive interference to promote the correct answer $a+b$, which is demonstrated by periodicity in their LogitLens, we are unable to fit exact functional forms to demonstrate this. Lastly, this work would benefit from experiments done on models of larger scales and different architecture. For example, in Appendix \ref{sec:app_add_clock}, we find preliminary evidence that Llama3.1-8B could be using a different algorithm, perhaps due to its gated MLP layers.


% MLPs contribute most to final answer, so natural to see how neurons logit lens looks. Periodic in logits, but not easily modeled by cosine / sine. Instea

% \begin{itemize}
%     \item Lay outclock algorithm, reference Figure 1
%     \item Initial evidence is that last token residual stream well modeled by helix(a+b) FIGURE 5 One column graph
%     \item when activation patching last token, seems like MLPs do most of heavy lifting. FIGURE 6
%     \item SUBSECTION. To ensure this, we first look at attention heads. We can model all attention head outputs using helix(a)+helix(b)+helix(a+b). We identify sparse subset of 20 heads that when patched in create 80\% of effect of patching in all attention heads. We find that earlier in layer 14, usually output a,b helices. after layer 20, output a+b helix direct to logits (exists at that point). 4 attention heads output a,b and a+b, but they have low IE. When we path patch attention heads that output answer, usually reads from MLPs, when we path patch attention heads that output a,b helices, they usually go to MLPs. more verification that MLPs do heavy lifting
%     \item SUBSECTION Look at MLPs. Identify circuit MLPs. MLPs well modeled by helix(a+b), but start getting worse around 19. this is also when attention heads start outputting more. FIGURE 7: MLPs helix(a+b)/IE and DE/IE ratio
%     \item SUBSECTION: Zooming in On Neurons. We attribution patch all neurons. Verify that most of them are in circuit mlps, important ones for direct effect are in 19+. INCLUDE TABLE 2, PUT FIG IN APPENDIX. 
%     \item Looking at neuron preactivations, we see that many implement trigonometric looking identities. We fourier transform, find most of them have top period 2,5,10,100 (graph in appendix, include percentage). We fit with helix. FIGURE 8: PLOT PANEL OF PREACTIVATIONS 2 COLUMNS AND THEIR FITS (T=2, T=5/10, T = 100). 
%     \item Putting neurons back into model is good FIGURE 9: One column neuron accuracy graph
%     \item FIGURE 10: Show DE/IE ratio graph vs ab helix of neurons. clearly, more de/ie goes to output. associated with layers
%     \item SUBSECTION: Neuron Logit Lens outputs. Clearly periodic with similar period to what they inputted. When we look at logits, most have period ten, which is top period when we fourier transform. We find that model is often off by 10 which makes sense. We also note that model usually undershoots final answer, which makes sense bc logits have clear bias towards smaller answers. Maybe also include the thing about carrying not being the suspect. Probably no graphs?
% \end{itemize}


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\newpage

\section{Performance of all models on $a+b=$}
\label{sec:app_model_perf}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/Appendix_Figures/all_models_perf_aplusb.png}
    \caption{All models are able to competently perform the task $a+b$, with Llama3.1-8B performing best.}
    \label{fig:app-model-perf}
\end{figure*}

We test three models, GPT-J, Pythia-6.9B, and Llama3.1-8B on the task $a+b=$. At first, we attempted to prompt each model with just $a+b=$, but we achieved significantly better results by including additional instructions in the prompt. After non-exhaustive testing, we used the prompts listed in Table \ref{tab:app_model_prompts} to test each model for all 10000 addition prompts (for $a,b \in [0,99]$). We plot a heatmap of the accuracy of the model by $a$ and $b$ in Fig. \ref{fig:app-model-perf}. All three models are able to competently complete the task, with Llama3.1-8B achieving an impressive 98\% accuracy. However, in the main paper we focus on analyzing GPT-J because it employs simple MLPs that are easier to interpret. We note that all three models struggle with problems with larger values of $a$ and $b$.






\begin{table*}
\centering
\caption{The prompts used and accuracy of each model on the addition task $a+b$.}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Model Name} & \textbf{Prompt} & \textbf{Accuracy} \\ \midrule
GPT-J & Output ONLY a number. \{$a$\}+\{$b$\}= & 80.5\% \\
Pythia-6.9B & Output ONLY a number. \{$a$\}+\{$b$\}= & 77.2\% \\
Llama3.1-8B & The following is a correct addition problem.\textbackslash n\{$a$\}+\{$b$\}= & 98.0\% \\ \bottomrule
\end{tabular}
\label{tab:app_model_prompts}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional Results on the Structure of Numbers}
\label{sec:app_struc_invest}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/Appendix_Figures/fourier_comparison_gpt-j-6B.pdf}
    \caption{The $T=2$ Fourier feature is prominent when analyzing $h^0_{360}$, but not when analyzing $h^0_{361}$.}
    \label{fig:app-four-comp}
\end{figure}
Our Fourier decomposition results in Section \ref{sec:invest-struc} are sensitive to the number of $a$ values analyzed. In particular, we find that the $T = 2$ Fourier component is not identified when analyzing $h^0_{361}$, but is identified when analyzing $h^0_{360}$ (Fig. \ref{fig:app-four-comp}). While we consider this sensitivity to sample size to be a limitation of our Fourier analysis, we note that the Fourier analysis is itself preliminary. We find that the $T = 2$ Fourier feature is causally relevant when fitting the residual stream in Section \ref{sec:eval-helix-fit}. Additionally, in later sections we find that neurons often read from the helix using the $T = 2$ Fourier feature, indicating its use downstream (Fig. \ref{fig:app_neuron_preact_fourier}). Thus, we identify $T = 2$ as an important Fourier feature.



We compare the residual stream of GPT-J after layer 0 on the inputted integers $a_1,a_2 \in [0,99]$ using Euclidean distance and cosine similarity in Fig. \ref{fig:app_cosine_euclid}. We visually note periodicity in the representations, with a striking period of $10$. To analyze the similarity between representations further, we calculate the Euclidean distance between $a$ and $a+\delta n$ for all values of $\delta n$. In Fig. \ref{fig:app_deltan_euclid}, we see that representations continue to get more distant from each other for $a \in [0,99]$ as $\delta n$ grows, albeit sublinearly. This provides evidence that LLMs represent numbers with more than just periodic features. When $a$ is restricted to $a \in [0,9]$, we observe a linear relationship in $\delta n$, implying some local linearity. The first principal component of the numbers $a \in [0,360]$ (shown in Fig. \ref{fig:app_pc1_360}) is also linear with a discontinuity at $a = 100$. Thus, our focus on two digit addition is justified, as three-digit integers seem to be represented in a different space.

\begin{figure}[htbp]
    \centering
    % First figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.22\textwidth]{figures/Appendix_Figures/hs_euclidean_matrix_mina0_maxa99_skip0False_layer0_gpt-j-6B.png} % Replace with your image
    \end{subfigure}
    \hfill
    % Second figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.22\textwidth]{figures/Appendix_Figures/hs_cosine_matrix_mina0_maxa99_skip0False_layer0_gpt-j-6B.png} % Replace with your image
        \label{fig:app_cosine_sim}
    \end{subfigure}
    \caption{We see clear periodicity in GPT-J's layer 0 representations of the numbers from $0-99$.}
    \label{fig:app_cosine_euclid}
\end{figure}

\begin{figure}[htbp]
    \centering
    % First figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.22\textwidth]{figures/Appendix_Figures/diagonal_euclidean_layer0_0to9_skip0=False_gpt-j-6B.png} % Replace with your image
    \end{subfigure}
    \hfill
    % Second figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.22\textwidth]{figures/Appendix_Figures/diagonal_euclidean_layer0_0to99_skip0=False_gpt-j-6B.png} % Replace with your image
    \end{subfigure}
    \caption{For GPT-J layer 0, the Euclidean distance between $a$ and $a+\delta n$ is approximately linear for $a\in [0,9]$, and sublinear for $a \in [0,99]$. This provides additional evidence that GPT-J uses more than periodic features to represent numbers.}
    \label{fig:app_deltan_euclid}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/Appendix_Figures/pc1_layer1_range0-360_skip0=False_gpt-j-6B.png}
    \caption{The first principal component of GPT-J layer 0 for numbers $a \in [0,360]$ shows a discontinuity for three-digit $a$, implying that three-digit numbers are represented in a different space.}
    \label{fig:app_pc1_360}
\end{figure}


\section{Additional Helix Fitting Results}
\subsection{Helix Properties}
\label{sec:app_helix_prop}
For the input of layer 0 of GPT-J, we plot the magnitude of the helical fit's Fourier features. We do so by taking the magnitude of columns of $C$ in Eq. \ref{eq:helix_param}. We find that these features roughly increase in magnitude as period increases, which matches the ordering in the Fourier decomposition presented in Fig. \ref{fig:fig2_fourier_linear}.

Additionally, we visualize the cosine similarity matrix between columns of $C$, which represents the similarity between helix components. In Fig. \ref{fig:app_helix_cosine_sim}, we observe that components are mostly orthogonal, as expected. A notable exception is the similarity between the $T = 100$ $\sin$ component and the linear component. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/Appendix_Figures/helix_component_magnitudes_layer_0.png}
    \caption{We plot the magnitude of each column of $C$, the coefficient matrix for the helical basis, to calculate the importance of each Fourier feature. Feature magnitude roughly increases with period, with the notable omission of the $T = 2$ $\sin$ component. We do not plot the linear component's magnitude because its output is a different scale.}
    \label{fig:app_helix_mag}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/Appendix_Figures/helix_cosine_sim_0.png}
    \caption{We plot the cosine similarity between columns of $C$, the coefficient matrix for the helical basis. Features are roughly orthogonal, which we expect for a helix, with the exception of the $T = 100$ $\sin$ component and the linear component $a$. We ignore the $T = 2$ $\sin$ component because of its negligible magnitude.}
    \label{fig:app_helix_cosine_sim}
\end{figure}

To ensure that the helix represents a true feature manifold, we design a continuity experiment inspired by \citet{olah2024manifold}. We first fit all $a$ that do not end with 3 with a $T = [100]$ helix. Then, we project $a = 3,13,\dots,93$ into that helical space in Fig. \ref{fig:app_manifold}. We find that each point is projected roughly where we expect it to be. For example, $93$ is projected between $89$ and $95$. We take this as evidence that our helices represent a true nonlinear manifold. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/Appendix_Figures/helical_fits_superimpose_special3.png}
    \caption{We fit all $a \in [0,99]$ that do not end with 3 using a helix with $T = [100]$, and project the residual stream for $a = 3,13,\dots,93$ onto the space. We find that there is continuity in the manifold, which we take as evidence that numbers are represented as a nonlinear feature manifold.}
    \label{fig:app_manifold}
\end{figure}


\subsection{Additional Causal Experiments for Helix Fits}
\label{sec:app_helix_causal}
We first replicate our helix fitting activation patching results on Pythia-6.9B and Llama3.1-8B in Fig. \ref{fig:app_helix_pythia_llama}.


\begin{figure}[htbp]
    \centering
    % First figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.45\textwidth]{figures/Appendix_Figures/helix_intervention_pythia-6.9b.png} % Replace with your image
    \end{subfigure}
    \hfill
    % Second figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.45\textwidth]{figures/Appendix_Figures/helix_intervention_Llama-3.1-8B.png} % Replace with your image
    \end{subfigure}
    \caption{We replicate our patching results on Pythia-6.9B and Llama3.1-8B. The helical and circular fits outperform baselines at most numbers of parameters.}
    \label{fig:app_helix_pythia_llama}
\end{figure}


To ensure the helix fits are not overfitting, we use a train-test split. We train the helix with 80\% of $a$ values and patch using the other 20\% of $a$ values (left of Fig. \ref{fig:app_traintest_random_b}). We observe that the helix and circular fits still outperform the PCA baseline. We also randomize the order of $a$ and find that the randomized helix is not causally relevant (middle of Fig. \ref{fig:app_traintest_random_b}), suggesting that the helix functional form is not naturally over expressive. Finally, we demonstrate that our results hold when fitting the $b$ token on $a+b$ with $\mathrm{helix}(b)$ (right of Fig. \ref{fig:app_traintest_random_b}). Note that when activation patching fits on the $b$ token, we use clean/corrupted prompt pairs of the form ($a+b'$, $a+b$), in contrast to the ($a'+b$, $a+b$) pairs we used for the $a$ token.



\begin{figure*}
    \centering
    % First figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/Appendix_Figures/helix_interv_traintest_layer_target_a_gpt-j-6B.png} % Replace with your image
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/Appendix_Figures/random_helix_intervention.png} % Replace with your image
    \end{subfigure}
    \hfill
    % Second figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/Appendix_Figures/logit_diff_layer_target_b_gpt-j-6B.png} % Replace with your image
    \end{subfigure}
    \caption{\textit{Left} When training the helix with 80\% of $a$ values and activation patching with the other 20\% of $a$ values, we see that the helix and circular fits still outperform the PCA baseline. \textit{Middle} We randomize $a$ while fitting the helix and see that the randomized helix is not causally relevant. \textit{Right} We show that our results for fitting the $a$ token can be extended to fitting the $b$ token on the prompt $a+b$ with $\mathrm{helix}(b)$.}
    \label{fig:app_traintest_random_b}
\end{figure*}


We perform an ablation experiment by ablating the columns of $C^\dagger$ from each $h_a^l$. In Fig. \ref{fig:app_ablation}, we see that ablating the helix dimensions from the residual stream like this affects performance about as much as ablating the entire layer, providing additional causal evidence that the helix is necessary for addition. However, when we attempt to fit $a$ with $\mathrm{helix}(a)$ for other tasks in Fig. \ref{fig:app_other_tasks}, we find that while the fit is effective, it sometimes underperforms PCA baselines. This suggests that while the helix is sufficient for addition, additional structure is required to capture the entirety of numerical representations. For a description of the prompts used and accuracy of GPT-J on these other tasks, see Table \ref{tab:other_task_accuracy}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/Appendix_Figures/ablation_results_logit_diff_gpt-j-6B.png}
    \caption{We find that ablating the helix dimensions from the residual stream is roughly as destructive as ablating the entire layer, providing additional evidence that GPT-J uses a numerical helix to do addition.}
    \label{fig:app_ablation}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/Appendix_Figures/other_tasks_helix_fit_gpt-j-6B.png}
    \caption{When testing our helical fit on other numerical tasks, we find that it performs competently, but does not always outperform the PCA baseline. This implies that additional structure may be present in numerical representations.}
    \label{fig:app_other_tasks}
\end{figure*}


\begin{table*}
\centering
\caption{GPT-J's task performance with corresponding domains, prompts, and accuracies.}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Task}    & \textbf{Domain}         & \textbf{Prompt}                                    & \textbf{Accuracy} \\ \hline
$a-23$           & $a \in [23,99]$         & Output ONLY a number. {a}-23=                  & 89.61\%           \\ \hline
$a//5$           & $a \in [0,99]$          & Output ONLY a number. {a}//5=                  & 97.78\%           \\ \hline
$a*1.5$          & $a \in [0,98]$          & Output ONLY a number. {a}*1.5=                 & 80.00\%           \\ \hline
$a \mod 2$       & $a \in [0,99]$          & Output ONLY a number. {a} modulo 2=            & 95.56\%           \\ \hline
$x-a=0, x=$      & $a \in [0,99]$          & Output ONLY a number. x-a=0, x=                & 95.56\%           \\ \hline
\end{tabular}
\label{tab:other_task_accuracy}
\end{table*}


\section{Additional Clock algorithm evidence}
\label{sec:app_add_clock}
We show that $\mathrm{helix}(a+b)$ fits last token hidden states for Pythia-6.9B and Llama3.1 8B in Fig. \ref{fig:ab_helix_llama_pythia}. Notably, the results for Llama3.1-8B are less significant than those for GPT-J and Pythia-6.9B. This is surprising, since the helical fit on the $a$ token is causal for Llama3.1-8B in Fig. \ref{fig:app_helix_pythia_llama}, and is a sign that Llama3.1-8B potentially uses additional algorithms to compute $a+b$. We hypothesize that this might be due to Llama3.1-8B using gated MLPs, which could lead to the emergence of algorithms not present in GPT-J and Pythia-6.9B, which use simple MLPs. \citet{Nikankin_Reusch_Mueller_Belinkov_2024}'s analysis of Llama3-8B's top neurons in addition problems identifies neurons with activation patterns unlike those we identified in GPT-J. Due to this evidence, along with the importance of MLPs in the addition circuit, we consider it likely that Llama3.1-8B implements modified algorithms, but we do not investigate further.


\begin{figure}
    \centering
    % First figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.45\textwidth]{figures/Appendix_Addition_Figures/ab_fits_logit_diff_pythia-6.9b.png} % Replace with your image
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=0.45\textwidth]{figures/Appendix_Addition_Figures/ab_fits_logit_diff_Llama-3.1-8B.png} % Replace with your image
    \end{subfigure}
    \caption{We present helical fits on the last token for Pythia-6.9B and Llama3.1-8B. The fits are weaker for Llama3.1-8B, potentially indicating the use of non-Clock algorithms.}
    \label{fig:ab_helix_llama_pythia}
\end{figure}

\subsection{Attention Heads}
\label{sec:app_attn_head}
In Fig. \ref{fig:app_attn_act_patch}, we use activation patching to show that a sparse set of attention heads are influential for addition. In Fig. \ref{fig:app_attn_topk_circuit}, we find that patching in $k=20$ heads at once is sufficient to restore more than 80\% of the total effect of patching all $k = 448$ heads. In Fig. \ref{fig:app_attn_helix_fit}, we also find that all attention heads in GPT-J are well modeled using $\mathrm{helix}(a,b,a+b)$. We judge this by the fraction of a head's total effect recoverable when patching in a helical fit.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/Appendix_Addition_Figures/attention_head_activation_patching.png}
    \caption{A sparse set of attention heads have causal effects on the output when patched (total effect visualized).}
    \label{fig:app_attn_act_patch}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/Appendix_Addition_Figures/attn_head_topk_activation_patching.png}
    \caption{Patching $k = 20$ heads at once restores more than 80\% of model behavior of patching all $k = 448$ heads.}
    \label{fig:app_attn_topk_circuit}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/Appendix_Addition_Figures/attn_head_helix_fit_intervention_results_gpt-j-6B_act.png}
    \caption{Attention head outputs are well modeled with $\mathrm{helix}(a,b,a+b)$. Total effect shown.}
    \label{fig:app_attn_helix_fit}
\end{figure}

We categorize heads as $a,b$, $a+b$, and mixed heads using a confidence score (detailed in Section \ref{sec:attn}). To make our categorization useful, we aim to categorize as few heads as mixed as possible. We find that using $m = 4$ mixed heads is sufficient to achieve almost 80\% of the effect of patching the actual outputs of the $k = 20$ heads (Fig. \ref{fig:app_attn_topk_mixed}), although using $m = 0$ mixed heads still achieves 70\% of the effect. In Fig. \ref{fig:app_attn_category_trends}, we analyze the properties of each head type. $a+b$ heads tend to attend to the last token and occur in layers 19 onwards. $a,b$ heads primarily attend to the $a$ and $b$ tokens and occur prior to layer 18. Mixed heads attend to the $a,b$, and last tokens, and occur in layers 15-18. 

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/Appendix_Addition_Figures/attn_head_topk_categorizations.png}
    \caption{$m = 4$ mixed heads are sufficient to achieve 80\% of effect of patching in all $k=20$ heads normally. Even using $m = 0$ mixed heads achieves 70\% of the effect.}
    \label{fig:app_attn_topk_mixed}
\end{figure}

\begin{figure*}
    \centering
    % First figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/Appendix_Addition_Figures/attnhead_categorization_layer.png} % Replace with your image
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/Appendix_Addition_Figures/attnhead_categorization_last_token_attention.png} % Replace with your image
    \end{subfigure}
    \hfill
    % Second figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/Appendix_Addition_Figures/attnhead_categorization_a_b_token_attention.png} % Replace with your image
    \end{subfigure}
    \caption{\textit{Left} $a+b$ heads generally occur late in the network, while $a,b$ heads occur earlier. Mixed heads occur in middle layers. \textit{Middle} $a+b$ heads attend more to the last token. \textit{Right} $a,b$ heads attend mostly to tokens $a$ and $b$.}
    \label{fig:app_attn_category_trends}
\end{figure*}


To understand what each head type reads and writes to, we use a modification of the path patching technique we have discussed so far. Specifically, we view mixed and $a,b$ heads as ``sender'' nodes, and view the total effect of each downstream component if only the direct path between the sender node and the component is patched in (not mediated by any other attention heads or MLPs). In Fig. \ref{fig:app_attn_sender_receiver}, we find that both $a,b$ and mixed heads generally impact downstream MLPs most. Similarly, we consider mixed and $a+b$ heads as ``receiver'' nodes, and patch in the path between all upstream components and the receiver node to determine what components each head relies on to achieve its causal effect. We find that $a+b$ heads rely predominantly on upstream MLPs, while mixed heads use both $a,b$ heads and upstream MLPs. This indicates that mixed heads may have some role in creating $\mathrm{helix}(a+b)$.


\begin{figure*}
    \centering
    % First plot
    \subfigure{\includegraphics[width=0.45\textwidth]{figures/Appendix_Addition_Figures/attn_head_sender_14_13_path_patch.png}}
    % Second plot
    \subfigure{\includegraphics[width=0.45\textwidth]{figures/Appendix_Addition_Figures/attn_head_sender_18_10_path_patch.png}} \\
    % Third plot
    \subfigure{\includegraphics[width=0.45\textwidth]{figures/Appendix_Addition_Figures/attn_head_receiver_24_10_path_patch.png}}
    % Fourth plot
    \subfigure{\includegraphics[width=0.45\textwidth]{figures/Appendix_Addition_Figures/attn_head_receiver_18_10_path_patch.png}} 
    \caption{We present path patching results for top attention heads. In the top row, we view one $a,b$ head (L14H13) and one mixed head (L18H10) as senders. We plot the total effect of each downstream component when only the path between the sender head and that component is patched into the model. We find that both $a,b$ and mixed heads write primarily to MLPs. Similarly, when viewing an $a+b$ head (L24H10) and mixed head (L18H10) as receivers, we see that that the $a+b$ head is primarily dependent on the output from preceding MLPs. While that is true for the mixed head as well, we see that the mixed head also takes input from other $a,b$ heads, implying that it could have some role in creating the $a+b$ helix.}
    \label{fig:app_attn_sender_receiver}
\end{figure*}

\subsection{MLPs and Neurons}
\label{sec:app_mlp}
In Fig. \ref{fig:app_mlp_topk}, we see that patching $k = 11$ MLPs achieves 95\% of the effect of patching all MLPs. We consider these MLPs to be circuit MLPs. Zooming in at the neuron level, we find that roughly 1\% of neurons are required to achieve an 80\% success rate on prompts while mean ablating all other neurons (Fig. \ref{fig:app_neuron_acc_ldiff}). Note the use of accuracy over logit difference as a metric in this case. Fig. \ref{fig:app_neuron_acc_ldiff} shows that ablating some neurons actually \textit{helps} performance as measured by logit difference, while hurting accuracy. To account for this seemingly contradictory result, we hypothesize that ablating some neurons asymmetrically boosts the answer token across prompts, such that some prompts are boosted significantly while other prompts are not affected. We do not investigate this further as it is not a major part of our argument and instead use an accuracy threshold.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/Appendix_Addition_Figures/topk_mlp_patching_thresholds_gpt-j-6B.png}
    \caption{$k=11$ MLPs are required to achieve 95\% of the effect of patching in all MLPs.}
    \label{fig:app_mlp_topk}
\end{figure}


\begin{figure}
    \centering
    % First plot
    \subfigure
    {\includegraphics[width=0.45\textwidth]{figures/Appendix_Addition_Figures/neurons_vs_accuracy.png}}
    % Second plot
    \subfigure
    {\includegraphics[width=0.45\textwidth]{figures/Appendix_Addition_Figures/neurons_vs_logit_diff.png}} \\
    \caption{\textit{Top} We see that using around 1\% of top neurons and mean ablating the rest can restore the model to 80\% accuracy. \textit{Bottom} When measuring logit difference, we find that mean ablating some neurons on average \textit{increases} the logit for the correct answer, but does not improve accuracy. We choose not to investigate this further.}
    \label{fig:app_neuron_acc_ldiff}
\end{figure}

When plotting the distribution of top neurons across layers in Fig. \ref{fig:app_neuron_dist}, we find that almost 75\% of top neurons are located in the $k=11$ circuit MLPs we have identified. We then path patch each of these neurons to calculate their direct effect. In Fig. \ref{fig:app_neuron_path}, we see that roughly 700 neurons are required to achieve 80\% of the direct effect of patching in all $k=4587$ top neurons. 84\% of the top DE neurons occur after layer 18, which corresponds with our claim that MLPs 19-27 primarily write the correct answer to logits.


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/Appendix_Addition_Figures/neuron_circuit_distribution.png}
    \caption{When we analyze approximately 1\% of last token neurons most causally implicated in addition for GPT-J, we find that nearly 75\% of them are in circuit MLPs, which is expected.}
    \label{fig:app_neuron_dist}
\end{figure}

\begin{figure}
    \centering
    % First plot
    \subfigure
    {\includegraphics[width=0.45\textwidth]{figures/Appendix_Addition_Figures/neurons_path_patch_cumulative.png}}
    % Second plot
    \subfigure
    {\includegraphics[width=0.45\textwidth]{figures/Appendix_Addition_Figures/neurons_path_patch_distribution.png}} \\
    \caption{\textit{Top} We find that roughly 700 neurons achieve 80\% of the direct effect of patching in all $k=4587$ high impact neurons. \textit{Bottom} More than 80\% of these high direct effect neurons are in layers 19 onwards, which we have identified as being responsible for translating the $a+b$ helix to answer logits.}
    \label{fig:app_neuron_path}
\end{figure}



When we Fourier decompose the $k = 4587$ top neurons' preactivations with respect to the value of $a+b$ in Fig. \ref{fig:app_neuron_preact_fourier}, we see spikes at periods $T = [2,5,10,100]$. These are the exact periods of our helix parameterization. To leverage this intuition, we fit the neuron preactivation patterns using the helix inspired functional form detailed in Eq. \ref{eq:helix_param}. We use a stochastic gradient descent optimizer with $\mathrm{lr} = 1e-1, \mathrm{epochs} = 2500$ and a cosine annealing learning rate scheduler to minimize the mean squared error of the fit. In Fig. \ref{fig:app_neuron_fit_nrmse}, we see that more important neurons with larger total effect are fit better with this functional form, as measured by normalized root mean square error (NRMSE).

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/Appendix_Addition_Figures/neuron_fourier_hist_T_ab_mina0_maxa99.png}
    \caption{Top neurons' preactivations are periodic in $a+b$ with top periods of $T = [2,5,10,100]$. Percentages shown for Fourier periods within $5\%$ of $T$.}
    \label{fig:app_neuron_preact_fourier}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/Appendix_Addition_Figures/neuron_helix_fit_quality.png}
    \caption{When calculating the NRMSE of the helix inspired fit for neuron preactivations, we find that more impactful neurons (with higher total effect) are typically fit better.}
    \label{fig:app_neuron_fit_nrmse}
\end{figure}


\section{Why Use the Clock Algorithm at All?}
\label{sec:app-conjecture}
We conjecture that LLMs use the Clock algorithm as a form of robust, error correcting code. If LLMs used a linear representation of numbers to do addition, that representation would have to be extremely precise to be effective.

To preliminarily test this conjecture, we take the first 50 PCA dimensions of the number representations for $a \in [0,99]$ in GPT-J after layer 0 and fit a line $\ell$ to it. The resulting line has an $R^2$ of $0.997$, indicating a very good fit. We consider all problems $a_1+a_2$. We do addition on this line by taking $\ell(a_1) + \ell(a_2)$. If $\ell(a_1) + \ell(a_2)$ is closest to $\ell(a_1+a_2)$, we consider the addition problem successful.

We then take the percentage of successful addition problems where the answer $a_1+a_2$ is less than some threshold $\alpha$, and compare the accuracy as a function of $\alpha$ for GPT-J and linear addition. Surprisingly, we find that for $\alpha = 100$, linear addition has an accuracy of less than 20\%, while GPT-J has an accuracy of more than 80\% (Fig. \ref{fig:app-line-approx}).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/Appendix_Addition_Figures/line_approx.pdf}
    \caption{We find that using a line with $R^2 = 0.997$ leads to addition that performs considerably worse than GPT-J. We attribute this to the representational precision required to do addition along a line, indicating a possible reason LLMs choose to use helical representations.}
    \label{fig:app-line-approx}
\end{figure}

Thus, even with very precise linear representations, doing linear addition leads to errors. We interpret LLMs use of modular circles for addition as a built-in redundancy to avoid errors from their imperfect representations.


\section{Investigating Model Errors}
\label{sec:app_invest_model_errors}
Given that GPT-J implements an algorithm to compute addition rather than relying on memorization, why does it still make mistakes? For problems where GPT-J answers incorrectly with a number, we see that it is most often off by $-10$ (45.7\%) and $10$ (27.9\%), cumulatively making up over 70\% of incorrect numeric answers (Fig \ref{fig:app_model_error}). We offer two hypotheses for the source of these errors: 1) GPT-J is failing to ``carry'' correctly when creating the $a+b$ helix or 2) reading from the $a+b$ helix to answer logits is flawed.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/Appendix_Addition_Figures/incorrect_diff_hist.png}
    \caption{When GPT-J incorrectly answers an addition prompt ($19.5\%$ of the time), it answers with a number more than half the time. That number is usually off by $10$ or $-10$ from the correct answer.}
    \label{fig:app_model_error}
\end{figure}

We test the first hypothesis by analyzing the distribution of GPT-J errors. If carrying was the problem, we would expect that when the model is off by $-10$, the units digits of $a$ and $b$ add up to 10 or more. Using a Chi-squared test with a threshold of $\alpha = 0.05$, we see that the units digit of $a$ and $b$ summing to more than $10$ is not more likely for when the model's error is $-10$ than otherwise (Fig. \ref{fig:app_carry_dist}). This falsifies our first hypothesis. Thus, we turn to understanding how the $a+b$ helix is translated to model logits. 

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/Appendix_Addition_Figures/ones_digit_sums_by_error.png}
    \caption{If GPT-J was struggling to ``carry'' when creating the $a+b$ helix, we would expect the ones digit of $a$ and $b$ to sum up to greater than $10$ when the model is off by $-10$. However, we see that this is not significantly more likely for when the error is $-10$ than when it is not $-10$.}
    \label{fig:app_carry_dist}
\end{figure}

Since MLPs most contribute to direct effect, we begin investigating at the neuron level. We sort neurons by their direct effect, and take the $k = 693$ highest DE neurons required to achieve 80\% of the total direct effect (Fig \ref{fig:app_neuron_path}). Then, we use the technique of LogitLens to understand how each neuron's contribution boosts and suppresses certain answers (see \citet{lesswrongInterpretingGPT} for additional details). For the tokens $[0,198]$ (the answer space to $a+b$), we see that each top DE neuron typically boosts and suppresses tokens periodically (Fig. \ref{fig:app_neuron_logit_lens}). Moreover, when we Fourier decompose the LogitLens of the max activating $a+b$ example for each neuron, we find that a neuron whose preactivation fit's largest term is $c_{T_i,a+b}$ in Eq. \ref{eq:neuron_helix} often has LogitLens with dominant period of $T_i$ as well (Fig. \ref{fig:app_neuron_logit_fourier}). We interpret this as neurons boosting and suppressing tokens with a similar periodicity that they read from the residual stream helix with.


\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/Appendix_Addition_Figures/neuron_logit_lens_sampler.png}
    \caption{Using the LogitLens technique, we analyze the contributions of the top neurons presented in Fig. \ref{fig:fig8_neuron_fits} for each neuron's maximally activating $a+b$ example.}
    \label{fig:app_neuron_logit_lens}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/Appendix_Addition_Figures/neuron_logit_lens_fourier.png}
    \caption{For all neurons with top $a+b$ fit component with $T = [2,5,10,100]$, we plot the distribution of the top Fourier period in their LogitLens taken over the tokens $[0,198]$. We see that a neuron with top fitted period of $T_i$ often has a LogitLens with top Fourier period $T_i$. Surprisingly, $200$ is a common Fourier period, possibly used to differentiate numbers in $[0,99]$ from $[100,198]$.}
    \label{fig:app_neuron_logit_fourier}
\end{figure*}




Despite being periodic, the neuron LogitLens are complex and not well modeled by simple trigonometric functions. Instead, we turn to more broadly looking at the model's final logits for each problem $a+b$ over the possible answer tokens $[0,198]$. We note a similar distinct periodicity in Fig. \ref{fig:app_logit_ex}. When we Fourier decompose the logits for all problems $a+b$, we find that the most common top period is $10$ (Fig. \ref{fig:app_logit_analysis}). Thus, it is sensible that the most common error is $\pm 10$, since $a+b-10$, $a+b+10$ are also strongly promoted by the model. To explain why $-10$ is a more common error than $10$, we fit a line of best fit through the model logits for all $a+b$, and note that the best fit line almost always has negative slope (Fig. \ref{fig:app_logit_analysis}), indicating a preference for smaller answers. This bias towards smaller answers explains why GPT-J usually makes mistakes with larger $a$ and $b$ values (Fig. \ref{fig:app-model-perf}, Appendix \ref{sec:app_model_perf}).


\begin{figure*}
    \centering
    % First figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/Appendix_Addition_Figures/1+22_logits.png} % Replace with your image
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/Appendix_Addition_Figures/43+40_logits.png} % Replace with your image
    \end{subfigure}
    \hfill
    % Second figure
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/Appendix_Addition_Figures/83+76_logits.png} % Replace with your image
    \end{subfigure}
    \caption{We plot the final model logits over the token space $[0,198]$ for some randomly selected examples. We see clear periodicity with a sharp period of 10, in addition to a general downward trend indicating a preference for smaller answers.}
    \label{fig:app_logit_ex}
\end{figure*}

\begin{figure}
    \centering
    % First plot
    \subfigure{\includegraphics[width=0.45\textwidth]{figures/Appendix_Addition_Figures/logit_slopes.png}}
    % Second plot
    \subfigure{\includegraphics[width=0.45\textwidth]{figures/Appendix_Addition_Figures/logit_fourier_top1.png}} \\
    \caption{\textit{Top} When we plot the slope of the best fit line over all logits, we see that the slope is often negative, implying a bias towards smaller answers. \textit{Bottom} When applying a Fourier decomposition on the logits for all examples $a+b$ over the token space $[0,198]$, we see that $10$ is the most common period. Note that we subtract out the fitted linear component first before applying the Fourier transform. }
    \label{fig:app_logit_analysis}
\end{figure}



\section{Tooling and Compute}
We used the Python library $\mathrm{nnsight}$ to perform intervention experiments on language models \cite{fiottokaufman2024nnsightndifdemocratizingaccess}. All experiments were run on a single NVIDIA RTX A6000 GPU with 48GB of VRAM. With this configuration, all experiments can be reproduced in two days.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
