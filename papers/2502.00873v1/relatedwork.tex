\section{Related Work}
\textbf{Circuits.} Within mechanistic interpretability, circuits research attempts to understand the key model components (MLPs and attention heads) that are required for specific functionalities \cite{ olah2020zoom, elhage2021mathematical}. For example, \citet{olsson2022context} found that in-context learning is primarily driven by induction attention heads, and \citet{wang2023interpretability} identified a sparse circuit of attention heads that GPT-2 uses to complete the indirect object of a sentence. Understanding how multilayer perceptrons (MLPs) affect model computation has been more challenging, with \citet{nanda2023factfinding} attempting to understand how MLPs are used in factual recall, and \citet{hanna2023how} investigating MLP outputs while studying the greater-than operation in GPT-2.

% A major tool in circuits analysis are causal techniques like activation patching, which attempt to isolate component contributions by constructing careful counterfactuals \cite{meng2022locating, syed2023attributionpatchingoutperformsautomated, kram√°r2024atpefficientscalablemethod, heimersheim2024useinterpretactivationpatching, goldowskydill2023localizingmodelbehaviorpath, geiger2024causalabstractiontheoreticalfoundation}.

\textbf{Features.} Another branch of MI focuses on understanding how models represent human-interpretable concepts, known as features. Most notably, the Linear Representation Hypothesis posits that LLMs store features as linear directions \cite{park2023the, elhage2022superposition}, culminating in the introduction of sparse autoencoders (SAEs) that decompose model activations into sparse linear combinations of features \cite{huben2024sparse, bricken2023monosemanticity, templeton2024scaling, gao2024scalingevaluatingsparseautoencoders, rajamanoharan2024improvingdictionarylearninggated}. However, recent work from \citet{engels2024languagemodelfeatureslinear} found that some features are represented as non-linear manifolds, for example the days of the week lie on a circle. \citet{levy2024languagemodelsencodenumbers} and \citet{zhu-etal-2025-language} model LLMs' representations of numbers as a circle in base 10 and as a line respectively, although with limited causal results. Recent work has bridged features and circuits research, with \citet{marks2024sparsefeaturecircuitsdiscovering} constructing circuits from SAE features and \citet{makelov2024towards} using attention-based SAEs to identify the features used in \citet{wang2023interpretability}'s IOI task.

% which has been evidenced by linear directions for discrete quantities like truth \citep{marks2024geometrytruthemergentlinear} and continuous qualities like longitude and latitude \citep{gurnee2024language}. 



\textbf{Reverse engineering addition.} \citet{liu2022towards} first discovered that one layer transformers generalize on the task of modular addition when they learn circular representations of numbers. Following this, \citet{nanda2023progress} introduced the ``Clock'' algorithm as a description of the underlying angular addition mechanisms these transformers use to generalize. However, \citet{zhong2023the} found the ``Pizza'' algorithm as a rivaling explanation for some transformers, illustrating the complexity of decoding even small models.  \citet{Stolfo_Belinkov_Sachan_2023} identifies the circuit used by LLMs in addition problems, and \citet{Nikankin_Reusch_Mueller_Belinkov_2024} claims that LLMs use heuristics implemented by specific neurons rather than a definite algorithm to compute arithmetic. \citet{zhou2024pretrained} analyze a fine-tuned GPT-2 and found that Fourier components in numerical representations are critical for addition, while providing preliminary results that larger base LLMs might use similar features.