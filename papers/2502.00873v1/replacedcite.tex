\section{Related Work}
\textbf{Circuits.} Within mechanistic interpretability, circuits research attempts to understand the key model components (MLPs and attention heads) that are required for specific functionalities ____. For example, ____ found that in-context learning is primarily driven by induction attention heads, and ____ identified a sparse circuit of attention heads that GPT-2 uses to complete the indirect object of a sentence. Understanding how multilayer perceptrons (MLPs) affect model computation has been more challenging, with ____ attempting to understand how MLPs are used in factual recall, and ____ investigating MLP outputs while studying the greater-than operation in GPT-2.

% A major tool in circuits analysis are causal techniques like activation patching, which attempt to isolate component contributions by constructing careful counterfactuals ____.

\textbf{Features.} Another branch of MI focuses on understanding how models represent human-interpretable concepts, known as features. Most notably, the Linear Representation Hypothesis posits that LLMs store features as linear directions ____, culminating in the introduction of sparse autoencoders (SAEs) that decompose model activations into sparse linear combinations of features ____. However, recent work from ____ found that some features are represented as non-linear manifolds, for example the days of the week lie on a circle. ____ and ____ model LLMs' representations of numbers as a circle in base 10 and as a line respectively, although with limited causal results. Recent work has bridged features and circuits research, with ____ constructing circuits from SAE features and ____ using attention-based SAEs to identify the features used in ____'s IOI task.

% which has been evidenced by linear directions for discrete quantities like truth ____ and continuous qualities like longitude and latitude ____. 



\textbf{Reverse engineering addition.} ____ first discovered that one layer transformers generalize on the task of modular addition when they learn circular representations of numbers. Following this, ____ introduced the ``Clock'' algorithm as a description of the underlying angular addition mechanisms these transformers use to generalize. However, ____ found the ``Pizza'' algorithm as a rivaling explanation for some transformers, illustrating the complexity of decoding even small models.  ____ identifies the circuit used by LLMs in addition problems, and ____ claims that LLMs use heuristics implemented by specific neurons rather than a definite algorithm to compute arithmetic. ____ analyze a fine-tuned GPT-2 and found that Fourier components in numerical representations are critical for addition, while providing preliminary results that larger base LLMs might use similar features.