
@inproceedings{
liu2022towards,
title={Towards Understanding Grokking: An Effective Theory of Representation Learning},
author={Ziming Liu and Ouail Kitouni and Niklas Nolte and Eric J Michaud and Max Tegmark and Mike Williams},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=6at6rB3IZm}
}

@inproceedings{Stolfo_Belinkov_Sachan_2023, address={Singapore}, title={A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis}, url={https://aclanthology.org/2023.emnlp-main.435}, DOI={10.18653/v1/2023.emnlp-main.435}, abstractNote={Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.}, publisher={Association for Computational Linguistics}, author={Stolfo, Alessandro and Belinkov, Yonatan and Sachan, Mrinmaya}, year={2023}, pages={7035–7052}, language={en} }

@inproceedings{
Nikankin_Reusch_Mueller_Belinkov_2024,
title={Arithmetic Without Algorithms: Language Models Solve Math with a Bag of Heuristics},
author={Nikankin, Yaniv and Reusch, Anja and Mueller, Aaron and Belinkov, Yonatan},
booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=O9YTt26r2P},
note={under review}
}

@inproceedings{
nanda2023progress,
title={Progress measures for grokking via mechanistic interpretability},
author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=9XFSbDPmdW}
}



@inproceedings{
zhou2024pretrained,
title={Pre-trained Large Language Models Use Fourier Features to Compute Addition},
author={Tianyi Zhou and Deqing Fu and Vatsal Sharan and Robin Jia},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=i4MutM2TZb}
}

@misc{levy2024languagemodelsencodenumbers,
      title={Language Models Encode Numbers Using Digit Representations in Base 10}, 
      author={Amit Arnold Levy and Mor Geva},
      year={2024},
      eprint={2410.11781},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.11781}, 
}

@inproceedings{
zhong2023the,
title={The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks},
author={Ziqian Zhong and Ziming Liu and Max Tegmark and Jacob Andreas},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=S5wmbQc1We}
}

@inproceedings{Vig2020InvestigatingGB,
  title={Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
  author={Jesse Vig and Sebastian Gehrmann and Yonatan Belinkov and Sharon Qian and Daniel Nevo and Yaron Singer and Stuart M. Shieber},
  booktitle={Neural Information Processing Systems},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:227275068}
}
@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@inproceedings{
wang2023interpretability,
title={Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 Small},
author={Kevin Ro Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=NpsVSN6o4ul}
}

@inproceedings{
meng2022locating,
title={Locating and Editing Factual Associations in {GPT}},
author={Kevin Meng and David Bau and Alex J Andonian and Yonatan Belinkov},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=-h6WAS6eE4}
}

@misc{syed2023attributionpatchingoutperformsautomated,
      title={Attribution Patching Outperforms Automated Circuit Discovery}, 
      author={Aaquib Syed and Can Rager and Arthur Conmy},
      year={2023},
      eprint={2310.10348},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.10348}, 
}

@misc{kramár2024atpefficientscalablemethod,
      title={AtP*: An efficient and scalable method for localizing LLM behaviour to components}, 
      author={János Kramár and Tom Lieberum and Rohin Shah and Neel Nanda},
      year={2024},
      eprint={2403.00745},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.00745}, 
}

@misc{heimersheim2024useinterpretactivationpatching,
      title={How to use and interpret activation patching}, 
      author={Stefan Heimersheim and Neel Nanda},
      year={2024},
      eprint={2404.15255},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.15255}, 
}

@misc{goldowskydill2023localizingmodelbehaviorpath,
      title={Localizing Model Behavior with Path Patching}, 
      author={Nicholas Goldowsky-Dill and Chris MacLeod and Lucas Sato and Aryaman Arora},
      year={2023},
      eprint={2304.05969},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.05969}, 
}

@misc{geiger2024causalabstractiontheoreticalfoundation,
      title={Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability}, 
      author={Atticus Geiger and Duligur Ibeling and Amir Zur and Maheep Chaudhary and Sonakshi Chauhan and Jing Huang and Aryaman Arora and Zhengxuan Wu and Noah Goodman and Christopher Potts and Thomas Icard},
      year={2024},
      eprint={2301.04709},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2301.04709}, 
}

@misc{ 
nanda2023factfinding,  
title={Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level},  
url={https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall},  
journal={Alignment Forum},  
author={Nanda, Neel and Rajamanoharan, Senthooran and Kramar, Janos and Shah, Rohin},  
year={2023},  
month={Dec}} 

@inproceedings{
hanna2023how,
title={How does {GPT}-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
author={Michael Hanna and Ollie Liu and Alexandre Variengien},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=p4PckNQR8k}
}

@misc{marks2024geometrytruthemergentlinear,
      title={The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets}, 
      author={Samuel Marks and Max Tegmark},
      year={2024},
      eprint={2310.06824},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.06824}, 
}

@inproceedings{
gurnee2024language,
title={Language Models Represent Space and Time},
author={Wes Gurnee and Max Tegmark},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=jE8xbmvFin}
}

@inproceedings{
park2023the,
title={The Linear Representation Hypothesis and the Geometry of Large Language Models},
author={Kiho Park and Yo Joong Choe and Victor Veitch},
booktitle={Causal Representation Learning Workshop at NeurIPS 2023},
year={2023},
url={https://openreview.net/forum?id=T0PoOJg8cK}
}

@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   url={https://transformer-circuits.pub/2022/toy_model/index.html}
}

@inproceedings{
huben2024sparse,
title={Sparse Autoencoders Find Highly Interpretable Features in Language Models},
author={Robert Huben and Hoagy Cunningham and Logan Riggs Smith and Aidan Ewart and Lee Sharkey},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=F76bwRSLeK}
}

@article{bricken2023monosemanticity,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
    }

@article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
    }

@misc{gao2024scalingevaluatingsparseautoencoders,
      title={Scaling and evaluating sparse autoencoders}, 
      author={Leo Gao and Tom Dupré la Tour and Henk Tillman and Gabriel Goh and Rajan Troll and Alec Radford and Ilya Sutskever and Jan Leike and Jeffrey Wu},
      year={2024},
      eprint={2406.04093},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.04093}, 
}

@misc{rajamanoharan2024improvingdictionarylearninggated,
      title={Improving Dictionary Learning with Gated Sparse Autoencoders}, 
      author={Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Tom Lieberum and Vikrant Varma and János Kramár and Rohin Shah and Neel Nanda},
      year={2024},
      eprint={2404.16014},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.16014}, 
}

@misc{engels2024languagemodelfeatureslinear,
      title={Not All Language Model Features Are Linear}, 
      author={Joshua Engels and Eric J. Michaud and Isaac Liao and Wes Gurnee and Max Tegmark},
      year={2024},
      eprint={2405.14860},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.14860}, 
}

@misc{marks2024sparsefeaturecircuitsdiscovering,
      title={Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models}, 
      author={Samuel Marks and Can Rager and Eric J. Michaud and Yonatan Belinkov and David Bau and Aaron Mueller},
      year={2024},
      eprint={2403.19647},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.19647}, 
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@inproceedings{pythia-6.9b,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@misc{llama3.1-8B,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and others},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@inproceedings{
liu2021pay,
title={Pay Attention to {MLP}s},
author={Hanxiao Liu and Zihang Dai and David So and Quoc V Le},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=KBnXrODoBW}
}

@article{pca,
author = { Karl   Pearson   F.R.S. },
title = {LIII. On lines and planes of closest fit to systems of points in space},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
volume = {2},
number = {11},
pages = {559-572},
year  = {1901},
publisher = {Taylor & Francis},
doi = {10.1080/14786440109462720},
}


@inproceedings{SatputeLLMMath,
author = {Satpute, Ankit and Gie\ss{}ing, Noah and Greiner-Petter, Andr\'{e} and Schubotz, Moritz and Teschke, Olaf and Aizawa, Akiko and Gipp, Bela},
title = {Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657945},
doi = {10.1145/3626772.3657945},
abstract = {Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this work, we follow a two-step approach to investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperforms the current best approach on ArqMATH3 Task1, considering P@10. Our case analysis indicates that while GPT-4 can generate relevant answers, it isn't consistently accurate. This paper explores the current limitations of LLMs in navigating complex mathematical question-answering. We make our code and findings publicly available for research: https://github.com/gipplab/LLM-Investig-MathStackExchange},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2316–2320},
numpages = {5},
keywords = {math language models, math question answer, math stack exchange},
location = {Washington DC, USA},
series = {SIGIR '24}
}


@misc{ahn2024largelanguagemodelsmathematical,
      title={Large Language Models for Mathematical Reasoning: Progresses and Challenges}, 
      author={Janice Ahn and Rishu Verma and Renze Lou and Di Liu and Rui Zhang and Wenpeng Yin},
      year={2024},
      eprint={2402.00157},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.00157}, 
}

@misc{OpenAI, url={https://openai.com/index/learning-to-reason-with-llms}, journal={Learning to reason with LLMS}, author={OpenAI}} 

@misc{glazer2024frontiermathbenchmarkevaluatingadvanced,
      title={FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI}, 
      author={Elliot Glazer and Ege Erdil and Tamay Besiroglu and Diego Chicharro and Evan Chen and Alex Gunning and Caroline Falkman Olsson and Jean-Stanislas Denain and Anson Ho and Emily de Oliveira Santos and Olli Järviniemi and Matthew Barnett and Robert Sandler and Matej Vrzala and Jaime Sevilla and Qiuyu Ren and Elizabeth Pratt and Lionel Levine and Grant Barkley and Natalie Stewart and Bogdan Grechuk and Tetiana Grechuk and Shreepranav Varma Enugandla and Mark Wildon},
      year={2024},
      eprint={2411.04872},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2411.04872}, 
}

@inproceedings{NIPSVaswani,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}



@misc{olah2024manifold,
      title={What is a Linear Representation? What is a Multidimensional Feature?}, 
      author={Chris Olah and Adam Jermyn},
      year={2024},
      url={https://transformer-circuits.pub/2024/july-update/index.html#linear-representations}, 
}

@misc{lesswrongInterpretingGPT,
	author = {nostalgebraist
},
	title = {interpreting {G}{P}{T}: the logit lens — {L}ess{W}rong --- lesswrong.com},
	howpublished = {\url{https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}},
	year = {2020},
	note = {[Accessed 14-01-2025]},
}

@misc{pizzaintegral,
Author = {Chun Hei Yip and Rajashree Agrawal and Lawrence Chan and Jason Gross},
Title = {Modular addition without black-boxes: Compressing explanations of MLPs that compute numerical integration},
Year = {2024},}

@misc{gemma2,
Author = {Morgane Riviere and Shreya Pathak and Pier Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and Léonard Hussenot and Thomas Mesnard and Bobak Shahriari and others},
Title = {Gemma 2: Improving Open Language Models at a Practical Size},
Year = {2024},
Eprint = {arXiv:2408.00118},
}

@article{fiottokaufman2024nnsightndifdemocratizingaccess,
      title={NNsight and NDIF: Democratizing Access to Foundation Model Internals}, 
      author={Jaden Fiotto-Kaufman and Alexander R Loftus and Eric Todd and Jannik Brinkmann and Caden Juang and Koyena Pal and Can Rager and Aaron Mueller and Samuel Marks and Arnab Sen Sharma and Francesca Lucchetti and Michael Ripa and Adam Belfki and Nikhil Prakash and Sumeet Multani and Carla Brodley and Arjun Guha and Jonathan Bell and Byron Wallace and David Bau},
      year={2024},
      eprint={2407.14561},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.14561}, 
}

@inproceedings{
makelov2024towards,
title={Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control},
author={Aleksandar Makelov and Georg Lange and Neel Nanda},
booktitle={ICLR 2024 Workshop on Secure and Trustworthy Large Language Models},
year={2024},
url={https://openreview.net/forum?id=MHIX9H8aYF}
}

@inproceedings{zhu-etal-2025-language,
    title = "Language Models Encode the Value of Numbers Linearly",
    author = "Zhu, Fangwei  and
      Dai, Damai  and
      Sui, Zhifang",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.47/",
    pages = "693--709",
    abstract = "Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under-explored. In this paper, we study a fundamental question: how language models encode the value of numbers, a basic element in math. To study the question, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of encoded number values in LLMs on different layers, and these values can be extracted via linear probes. Further experiments show that LLMs store their calculation results in a similar manner, and we can intervene the output via simple vector additions, proving the causal connection between encoded numbers and language model outputs. Our research provides evidence that LLMs encode the value of numbers linearly, offering insights for better exploring, designing, and utilizing numeric information in LLMs."
}


@article{PhysRevE.110.054303,
  title = {Fault-tolerant neural networks from biological error correction codes},
  author = {Zlokapa, Alexander and Tan, Andrew K. and Martyn, John M. and Fiete, Ila R. and Tegmark, Max and Chuang, Isaac L.},
  journal = {Phys. Rev. E},
  volume = {110},
  issue = {5},
  pages = {054303},
  numpages = {15},
  year = {2024},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.110.054303},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.110.054303}
}
