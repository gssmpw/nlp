\section{Brain Dynamics Foundation Model by Learning Amortized Optimal Control}
\label{sec:main:method}
In this section, we introduce our proposed algorithm BDO, a novel approach to brain dynamics foundation modeling. This method integrates amortized inference for continuous-discrete SSMs with the principles of SOC.


\subsection{Stochastic Optimal Control as Amortized Inference}
\label{sec:main:method:subsection:SOC}
Rather than relying on Bayesian recursion, we employ a SOC formulation to estimate the posterior distribution~\eqref{eq:posterior distribution}. SOC~\citep{fleming2006controlled, carmona2016lectures} is a mathematical framework that combines the principles of optimization and probability theory to determine the best possible control strategy for a given dynamical system under uncertainty. We consider the \textit{control-affine} SDEs as follows:
\[\label{eq:controlled dynamics}
 d\bX^{\alpha}_t = \left[f(t, \bX^{\alpha}_t) + \sigma(t)\alpha(t, \bX^{\alpha}_t)\right]dt + \sigma(t) d\bW_t,
\]
where $\alpha(t, \cdot): \bbR^d \to \bbR^d$ represent the \textit{Markov} control we aim to optimize. The objective is to determine an \textit{optimal control policy} $\alpha^{\star}$ that steers the distribution induced by the prior dynamics in~\eqref{eq:prior dynamics} to align with the posterior distribution. The solution to this SOC optimization problem, which is also closely connected to the variational inference framework, is typically structured as follows~\citep{theodorou2015nonlinear, kappen2016adaptive, li2020scalable, park2024amortized}:
\begin{proposition}[Evidence lower bound]\label{proposition:ELBO} Let us consider a following Markov control-affine problem formulation:
\[\label{eq:cost-to-go function}
    \mathcal{J}(\alpha,\mathcal{Y}) = \bbE_{\bX^{\alpha} \sim \eqref{eq:controlled dynamics}} \left[ \int_0^T \frac{1}{2} \norm{\alpha_t}^2 dt  - \sum_{t \in \mathcal{T}} \log g(\by_{t} | \bX^{\alpha}_{t})\right], 
\]
where $\bX^{\alpha}_t$ is given by a solution of the controlled SDEs in~\eqref{eq:controlled dynamics} with initial condition $\bX^{\alpha}_0 \sim p_0$. Then, the negation of the $\mathcal{J}(\alpha, \mathcal{Y})$ coincides with evidence lower bound (ELBO):
\[\label{eq:ELBO}
    \underbrace{\log \bZ(\mathcal{Y})}_{\text{Log-likelihood}} \geq -\underbrace{\mathcal{J}(\alpha, \mathcal{Y})}_{\text{ELBO}},
\]
\end{proposition}

\textbf{Amortized Inference. } \cref{proposition:ELBO} establishes that solving the SOC optimization problem with the cost function~\eqref{eq:cost-to-go function} can be interpreted as a variational inference problem for the posterior distribution in~\eqref{eq:posterior distribution}. It aligns with continuous-time reinforcement learning with entropy regularization~\citep{todorov2006linearly}, where the integral term $\frac{1}{2}\norm{\alpha_t}^2$ enforces KL-regularization to maintain proximity to the prior process~\eqref{eq:prior dynamics} and $-\log g$ acts as reward function. Hence, once the optimal policy $\alpha^{\star}$ (the minimizer of the SOC problem) is obtained, we can sample from the posterior distribution~\eqref{eq:posterior distribution} over the given time interval by simulating the \textit{optimally controlled} SDE~\eqref{eq:controlled dynamics} and the conditional distribution in~\eqref{eq:full trajectory estimation} also can be approximated as follows:
\[\label{eq:reconstruction_with_post}
    p(\mathcal{Y} | \mathcal{Y}_{\text{obs}}) &= \int p(\mathcal{Y} | \bX_{[0, T]}) p(\bX_{[0, T]} | \mathcal{Y}_{\text{obs}}) d\bX_{[0,  T]} \\
    & = \int p(\mathcal{Y} | \bX_{[0, T]}) p(\bX^{\alpha^{\star}}_{[0, T]})d\bX^{\alpha^{\star}}_{[0,  T]}, \label{eq:reconstruction}
\]
where $p(\bX^{\alpha^{\star}}_{[0, T]})$ represents the collection of marginal distributions of the controlled SDEs in~\eqref{eq:controlled dynamics} with $\alpha^{\star}$.

In practice, we can approximate the optimal control by parameterizing the control policy $\alpha(t, \bx) := \alpha(t, \bx, \theta)$ with a neural network and optimizing the cost function~\eqref{eq:cost-to-go function} using gradient descent. However, in this case, we require caching the gradients across the entire time interval, which becomes computationally expensive and memory-intensive as the time horizon or latent dimension increases~\citep{liu2024generalized, park2024stochastic}. Additionally, the inference of latent states through SDE simulations often requires numerical solvers like Euler-Maruyama solvers~\citep{kloeden2013numerical}, thereby substantially increasing resource demands.


\textbf{Locally Linear Approximation.} To overcome these challenges, we propose an efficient approximation inspired by~\citep{becker2019recurrent, schirmer2022modeling, park2024amortized}. This method (locally) linearizes the drift function in~\eqref{eq:controlled dynamics} using an attentive mechanism to leverage observations $\mathcal{Y}$. 
It enables the derivation of a closed-form solution for the SDE, facilitating efficient sampling of latent states without relying on numerical simulation.
\begin{theorem}[Simulation-free inference]\label{theorem:simulation free inference} Let us consider a sequence of semi-positive definite (SPD) matrices $\bD_{t \in \mathcal{T}}$ where each $\bD_{t_i} \in \bbR^{d \times d}$ admits the eigen-decomposition $\bD_{t_i} = \bV \bLambda_{t_i} \bV^{\top}$ with eigen-basis $\bV \in \bbR^{d \times d}$ and eigen-values $\bLambda_{t_i} \in \text{diag}(\bbR^d)$ for all $i \in \{1, \cdots, k\}$ and time-state invariant approximation of controls $\alpha_{t \in \mathcal{T}}$, where each $\alpha_{t} \in \bbR^d$. Then, for an interval $[t_{i}, t_{i-1})$, consider the SDE:
\[\label{eq:linearized controlled SDE}
d\bX^{\alpha}_t = \left[-\bD_{t_i} \bX^{\alpha}_t + \alpha_{t_i} \right] dt + d\bW_t, 
\]
where $\bX^{\alpha}_0 \sim \mathcal{N}(\mu_0, \Sigma_0)$. Then, for any time-stamps $t_i \in \mathcal{T}$, the marginal distribution of the solution of~\eqref{eq:linearized controlled SDE} is a Gaussian distribution $\ie \bX^{\alpha}_{t_i} \sim \mathcal{N}(\mu_{t_i}, \Sigma_{t_i})$ whose the parameters are computed as
\[
    &\mu_{t_i} = \bV \Bigg( e^{-\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \hat{\mu}_{t_0} - \\ \nonumber
    & \sum_{l=0}^{i-1} e^{-\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bLambda^{-1}_{t_l} \left( \bI - e^{(t_{l+1} - t_l) \bLambda_{t_l}} \right) \hat{\alpha}_{t_{l}} \Bigg) \\
    &\Sigma_{t_i} = \bV \Bigg( e^{-2\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \hat{\Sigma}_{t_0} -  \\ \nonumber
    & \frac{1}{2}\sum_{l=0}^{i-1} e^{-2\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bLambda^{-1}_{t_l} \left( \bI - e^{2(t_{l+1} - t_l) \bLambda_{t_l}} \right) \Bigg)\bV^{\top}.
\]
\end{theorem}
\cref{theorem:simulation free inference} states that by approximating the drift function in~\eqref{eq:controlled dynamics} using a linear-affine formulation with $f(t, \bx) := -\bD_{t_i} \bx$ and $\alpha(t, \bx) \approx \alpha_{t_i}$, we achieve a \textit{simulation-free property}. Therefore, with the given matrices and controls $\{\bD_{t}, \alpha_{t}\}_{t \in \mathcal{T}}$, we can compute a closed-form solution for the latent states $\bX^{\alpha}_{t}$, which in turn allows us to infer the intermediate observations $\by_{t}$ for any time $t \in \mathcal{T}$. To ensure the latent dynamics align with observations $\mathcal{H}$, we parameterize the matrices and controls $\{\bD_{t}, \alpha_{t}\}_{t \in \mathcal{T}}$ as follow:
\[\label{eq:drift approximation}
    \bD_t = \sum_{l=1}^L w_t^l \bD^l, \quad \bw_t = w_{\theta}(\bz_t), \quad \alpha_t = \bB_{\theta}\bz_t,
\]
where the latent (auxiliary) variables $\mathcal{Z} := \bz_{t \in \mathcal{T}}$ are generated by the parameterized encoder network: 
\[\label{eq:encoder network}
     q_{\theta}(\mathcal{Z} | \mathcal{Y}) : = \prod_{t \in \mathcal{T}} q_{\theta}(\bz_t | \mathcal{Y}) = \mathcal{N}(\bz_t | \bT_{\theta}(t, \mathcal{Y}), \sigma^2_q \bI),
\]
with the transformer network $\bT_{\theta}$. This locally linear parameterization increases flexibility by integrating the given observations $\mathcal{Y}$ through an attentive structure, ensuring that $\bD_t$ and $\alpha_t$ remain constant within observed intervals $[t_i, t_{i-1})$ for all $i \in [1, \dots, k]$, allowing the dynamics to smoothly transition between adapted linear states. Furthermore, this parameterization allows integration with the parallel scan algorithm~\citep{blelloch1990prefix}, enabling parallel computation of both moments for the $k$ latent states $\{\mu_{t}, \Sigma_{t}\}_{t \in \mathcal{T}}$. It reduces the computational complexity of the posterior distribution in \eqref{eq:posterior distribution} from $\mathcal{O}(k)$ to $\mathcal{O}(\log k)$\footnote{See details on~\cref{sec:parallel_scan_algorithm}.}.


\subsection{Representation Learning with Amortized Control} \label{sec:main:method:subsection:universal features}

In the previous section, we introduced an efficient and scalable approach for approximating the posterior distribution~\eqref{eq:posterior distribution} via amortized inference, leveraging SOC theory. Unlike Bayesian recursion~\eqref{eq:bayesian updates}, which incorporates observational information into the latent dynamics through iterative updates, our method employs the optimal control $\alpha^{\star}$ to encapsulate the dynamics of the underlying time-series. This optimal control encodes key features that effectively capture the spatio-temporal representation of the observations $\mathcal{Y}$.
Therefore, we aggregate the sequence of control signals $\alpha_{t  \in \mathcal{T}}$ into a \textit{universal feature} $\bbA$, which serve as the transferable feature for downstream tasks $\ie \bbA = f(\alpha_{t  \in \mathcal{T}})$. 

\textbf{Masked Auto Encoder. }To construct a robust representation of $\bbA$ (or control signals $\alpha_{t \in \mathcal{T}}$) within our control framework outlined in~\cref{proposition:ELBO}, we focus on general reconstruction tasks. Given the complete observation set $\mathcal{Y}_{\text{obs}} = \mathcal{Y}_{\text{tar}} \cup \mathcal{Y}_{\text{ctx}}$, we generate masked targets $\mathcal{Y}_{\text{tar}}$ using contextual observations $\mathcal{Y}_{\text{ctx}}$. Building on~\eqref{eq:reconstruction}, this reconstruction problem can expressed as the estimation of the conditional distribution of $\mathcal{Y}_{\text{tar}}$ given $\mathcal{Y}_{\text{ctx}}$ as follows:
\[\label{eq: MAE_1}
    & p(\mathcal{Y}_{\text{tar}} | \mathcal{Y}_{\text{ctx}}) = \int p(\mathcal{Y}_{\text{tar}} | \bX_{[0, T]}) p(\bX^{\alpha^{\star}}_{[0, T]})d\bX^{\alpha^{\star}}_{[0,  T]}. 
\]
In this formulation, the optimal control policy $\alpha^{\star}$ is determined by solving SOC problem with objective function:
\[\label{eq: MAE_2}
    & - \log p(\mathcal{Y}_{\text{tar}} | \mathcal{Y}_{\text{ctx}}) \leq \mathcal{J}(\alpha, \mathcal{Y}_{\text{ctx}}) \\
    \label{eq: MAE_3}
    & = \bbE_{\bX^{\theta} \sim \eqref{eq:linearized controlled SDE}} \left[ \int_0^T \frac{1}{2}\norm{\alpha^{\theta}_t}^2 dt - \sum_{t \in \mathcal{T}_{\text{obs}}} \log g_{\psi}(\by_t | \bX^{\theta}_t) \right], \nonumber
\]
where we denote $\alpha^{\theta}_t := \alpha^{\text{ctx}, \theta}_t$ and $\bX^{\alpha^{\text{ctx}, \theta}}_t := \bX^{\theta}_t$ for brevity. Here, the control $\alpha^{\theta}_t$ is generated by encoding the context observations $\mathcal{Y}_{\text{ctx}}$ using a neural network $\theta$, as detailed in~(\ref{eq:drift approximation}$-$\ref{eq:encoder network}). This control problem aligns with the masked auto-encoder (MAE) framework commonly used in SSL~\citep{he2022masked}, particularly within the context of SSMs for time-series data. However, this approach may be suboptimal for highly noisy data modalities like fMRI as the na√Øve likelihood function $g_{\psi}(\by_t | \bX^{\theta}_t)$ directly fitting the latent states $\bX^{\theta}$ to the observed raw-signals $\by_t$. It can cause $\bX^{\theta}$ to overfit or fail to capture semantically meaningful features~\citep{assran2023self,dong2024brain}, thereby compromising the robustness of universal feature $\bbA$.

\textbf{Integrating Empirical Priors. }
To address the aforementioned issue, we introduce additional structure into the likelihood by modeling it as a mixture over an auxiliary variable $\bz_t$, formulated as
\[
    g_{\psi}(\by_t|\bX^{\theta}_t) = \int \gamma_{\psi}(\by_t | \bz_t) \pi(\bz_t | \bX^{\theta}_t) d\bz_t,
\]
where $\gamma_{\psi}$ is parameterized likelihood function:
\[
\gamma_{\psi}(\by_t|\bz_t) = \mathcal{N}(\by_t | \bD_{\psi}(\bz_t), \sigma^2_\gamma \bI)
\] with decoder network $\bD_{\psi} : \bbR^d \to \bbR^n$, maps the latent states $\bX^{\theta}_t$ to the output reconstruction $\by_t$ over $t \in \mathcal{T}_{\text{obs}}$.
Here, the mixing distribution $\pi(\bz_t | \bX^{\theta}_t)$ serves to predict the auxiliary variable $\bz_t$ from the latent states $\bX_t^\theta$, capturing high-level structural information in an abstract space. The emission probability $g_\psi(\by_t | \bz_t)$ then refines these predictions by encoding local variations and details.
This formulation naturally aligns with the hierarchical nature of many dynamical systems, where global structures emerge at a higher level of abstraction, while local variations manifest in finer-scale details. By structuring the generative process in this way, we ensure that the control policy $\alpha$ interacts with a well-structured latent space, facilitating more robust learning and better generalization.







The choice of the distribution $\pi$ is pivotal in ensuring the auxiliary variable $\bz_t$ remains meaningful and effectively supports the training objective function for the control $\alpha_t$. Here, we define the $\pi$ as a geometric mixture,
\[\label{eq:mixture distribution}
    \pi_{\bar{\theta}}(\bz_t | \bX^{\theta}_t) \propto p(\bz_t | \bX^{\theta}_t)^{\lambda} q_{\bar{\theta}}(\bz_t | \mathcal{Y}_{\text{tar}})^{(1-\lambda)},
\]
where $p(\bz_t | \bX^{\theta}_t) = \mathcal{N}(\bz_t | \bX^{\theta}_t, \sigma^2_p \bI)$ represents the \textit{context-driven} likelihood of $\bz_t$ given the $\bX^{\theta}_t$, which is constructed based on the information of $\mathcal{Y}_{\text{ctx}}$, delivering context-informed features to $\bz_t$ from $\bX_t^\theta$. Conversely, $q_{\bar{\theta}}(\bz_t | \mathcal{Y}_{\text{tar}}) = \mathcal{N}(\bz_t | \bT_{\bar{\theta}}(t, \mathcal{Y}), \sigma^2_q \bI)$ encapsulated a \textit{data-driven} prior knowledge derived from $\mathcal{Y}_{\text{tar}}$. We define the data-driven prior $q_{\bar{\theta}}$ using the same parameterization as encoder network $q_{\theta}$ in~\eqref{eq:encoder network}:
\[\nonumber
    q_{\bar{\theta}}(\mathcal{Z}_{\text{tar}} | \mathcal{Y}_{\text{tar}}) = \prod_{t \in \mathcal{T}_{\text{tar}}}q_{\bar{\theta}}(\bz_t | \mathcal{Y}_{\text{tar}}) = \mathcal{N}(\bz_t | \bT_{\bar{\theta}}(t, \mathcal{Y}), \sigma^2_q \bI),
\]
where $\bar{\theta}$ is a frozen copy of $\theta$ that is updated at a slower rate than $\theta$. The empirical prior encourages the auxiliary variable $\bz_t$ predicted from the current context $\mathcal{Y}_\text{ctx}$ to align with the one directly encoded from the target $\mathcal{Y}_\text{tar}$ using the slow-moving encoder $\bar{\theta}$. This design ensures that the encoder $q_\theta$ captures more abstract and invariant features, mitigating the risk of overfitting to the target signals. The balancing factor $0 < \lambda \leq 1$ allows $\bz_t$ to adjust the influence of contextual information (contained in $\bX^{\theta}_t$) and empirical priors (contained in $\mathcal{Y}_{\text{tar}}$ and encoder $\bar{\theta}$). Compared to the case where $\lambda=1$, where the model learns target information solely by reconstructing target signals $\mathcal{Y}_{\text{tar}}$, thereby implicitly embedding this information into the auxiliary state $\bz_t$ through learning, choosing $\lambda < 1$ allows the targe information to be explicitly injected into the $\bz_t$.

\textbf{Training Objective.} By incorporating the mixture distribution~\eqref{eq:mixture distribution} into the SOC problem in~\eqref{eq: MAE_2}, the ELBO is:
\[
    & - \log p(\mathcal{Y}_{\text{tar}} | \mathcal{Y}_{\text{ctx}}) \leq \bbE_{\bX^{\theta} \sim \eqref{eq:linearized controlled SDE}} \Bigg[ \int_0^T \frac{1}{2}\norm{\alpha^{\theta}_t}^2 dt -  \nonumber\\
    & \sum_{t \in \mathcal{T}_{\text{obs}}} \bbE_{p(\bz_t | \bX^{\theta}_t)} \bigg(\underbrace{\log \gamma_{\psi}(\by_t | \bz_t)}_{\text{reconstruction}} + \underbrace{\log q_{\bar{\theta}}(\bz_t | \mathcal{Y}_{\text{tar}})^{(1-\lambda)} }_{\text{regularization}}\bigg) \Bigg] \nonumber\\
    & := \mathcal{L}(\theta, \psi),\label{eq:training objective}
\]
where the reconstruction term $\log \gamma_{\psi}(\by_t | \bz_t)$ ensures accurate reconstruction of the target signals from the auxiliary variables, and the regularization term $\log q_{\bar{\theta}}(\bz_t | \mathcal{Y}_{\text{tar}})$ incorporates prior knowledge of $\mathcal{Y}_{\text{tar}}$ to prevent the context-driven auxiliary variables from overfitting to the target data. This facilitates the capture of invariant and semantically rich features, aligning with the principles of Joint Embedding Predictive Architecture (JEPA)~\citep{lecun2022path, assran2023self}, which emphasizes the integration of predictive and contextual information to develop robust and interpretable latent representations. Additionally, inspired by prior work on SSL~\citep{caron2021emerging, chen2021empirical, assran2023self}, the parameter of data-driven prior $\bar{\theta}$ updated via an exponential moving average of the encoder network parameters $\theta$. This formulation ensures smoother evolution of the target encoder, preventing abrupt changes and promoting stable and consistent representation learning. 

The parameters of encoder-decoder $\{\theta, \psi\}$ along with those governing the latent dynamics $\{w_{\theta}, \bB_{\theta}, \mu_0, \Sigma_0, \{\bD^l\}_{l=1}^L\}$ are jointly optimized by minimizing the rescaled training objective function $\mathcal{L}(\theta, \psi)$ described in~\eqref{eq:rescaled training objective}, for stable learning, in an end-to-end manner. A more detailed objective function is described in Appendix~\ref{sec:app:deriving elbo}. After training, we obtain the desired universal feature $\bbA$ by computing $\alpha_t = \bB_{\theta} \bo_t$ as described in~\eqref{eq:drift approximation}. The extracted feature $\bbA$ is then utilized for downstream tasks. The training process is outlined in the Algorithm~\ref{algorithm:pretrain} in the Appendix.



