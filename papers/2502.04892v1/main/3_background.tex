\section{Preliminaries: Continuous-Discrete SSMs}
\label{sec:main:preliminaries}
Let us consider a time series $\by_{t_1:t_k} := \{\by_{t_i}\}_{i=1}^k$  observed from a complete underlying continuous dynamics $\mathcal{Y} := \{\by_t\}_{t \in [0, T]}$ over an interval $[0, T]$, for each $\by_{t_i} \in \bbR^n$. Because we have only access to observations at \textit{discrete} context time stamps $\mathcal{T}_{\text{obs}} := \{t_i\}_{i=1}^k \subset [0, T]$, where $0 = t_0 \leq \cdots \leq t_k = T$, we focus on the discrete observations corresponding to $\mathcal{T}_{\text{obs}}$ denoted as $\mathcal{Y}_{\text{obs}} := \by_{t \in \mathcal{T}_{\text{obs}}}$.

In state-space models, these observations $\mathcal{Y}_{\text{obs}}$ are assumed to be generated from noisy measurement processes, which can be modeled as $\by_t \sim g(\cdot | \bX_t)$, where the latent state $\bX_t$ represents the underlying latent states of $\mathcal{Y}$. To build a general framework, we consider \textit{continuous} latent states $\bX_t$ defined over the interval $[0, T]$, stochastic processes governed by an It\^o stochastic differential equation (SDE):
\[\label{eq:prior dynamics}
& d\bX_t = f(t, \bX_t) dt + \sigma(t) d\bW_t,
\]
where $f(t, \cdot) : \mathbb{R}^d \to \mathbb{R}^d$ is the drift, $\sigma(t) \in \mathbb{R} \to \mathbb{R}^d$ is the diffusion coefficient and $\bW_t \in \mathbb{R}^d$ is a standard Wiener process. Within this framework, the goal is to estimate the \textit{posterior} distribution - the optimal probabilistic estimates of the latent continuous state dynamics $\bX_{[0, T]}$ given the context observations $\mathcal{Y}_{\text{obs}}$. By Bayes' rule, the posterior is written as follows:
\[\label{eq:posterior distribution}
    p(\bX_{[0:T]} | \mathcal{Y}_{\text{obs}}) &= \frac{1}{\bZ(\mathcal{Y}_{\text{obs}})} p(\mathcal{Y}_{\text{obs}} | \bX_{[0, T]}) p(\bX_{[0:T]}) \\
    & = \frac{1}{\bZ(\mathcal{Y}_{\text{obs}})} \prod_{t \in \mathcal{T}_{\text{obs}}} g(\by_{t} | \bX_{t})p(\bX_{[0:T]}),
\]
where $p(\mathcal{Y}_{\text{obs}} | \bX_{[0, T]}):=\prod_{t \in \mathcal{T}_{\text{obs}}} g(\by_{t} | \bX_{t})$, $\bZ(\mathcal{Y}_{\text{obs}}) = \int p(\mathcal{Y}|\bX_{[0:T]})p(\bX_{[0:T]})d\bX_{[0:T]}$ is a normalization constant and $p(\bX_{0:T})$ is the prior distribution 
obtained as a solution of the prior SDE in \eqref{eq:prior dynamics}.
The posterior distribution~\eqref{eq:posterior distribution} can be estimated by $k$ recursive Bayesian updates~\citep{särkkä2013bayesian}:
\begin{align}\label{eq:bayesian updates}
\lefteqn{p(\bX_{[0:t_k]} | \by_{t_1:t_{k-1}})}\nonumber\\
&\propto  \int  p(\bX_{t_k} | \bX_{t_{k-1}}) p(\bX_{[0:t_{k-1}]} |\by_{t_1:t_{k-1}}) d\bX_{[0:t_{k-1}]}, \nonumber\\
\lefteqn{p(\bX_{[0:t_k]} | \by_{t_1:t_{k}})}\nonumber\\ &\propto g(\by_{t_k} | \bX_{t_k})p(\bX_{[0:t_k]} |\by_{t_1:t_{k}}).
\end{align}
We assume that $p(\bX_{[0:t_0]} | \by_{0:t_0}) := p_0(\bX_0)$ is known and independent with the Wiener process $\bW_{[0, T]}$.  $p(\bX_{t_k} | \bX_{t_{k-1}})$ denotes a transition density describing the time-evolution of $\bX_t$ from $t_{k-1}$ to $t_k$. 
Once we infer the posterior \eqref{eq:posterior distribution}, we can use it for various inference tasks. For example, one may use it to obtain a conditional estimate of the full trajectory $\mathcal{Y}$, which is given by:
\[\label{eq:full trajectory estimation}
    p(\mathcal{Y} | \mathcal{Y}_{\text{obs}}) = \int p(\mathcal{Y} | \bX_{[0, T]}) p(\bX_{[0, T]} | \mathcal{Y}_{\text{obs}}) d\bX_{[0,  T]},
\]
where $p(\mathcal{Y} | \bX_{[0, T]}) := \prod_{t \in \mathcal{T}} g(\by_t | \bX_t)$. In other words, one can exploit the context $\mathcal{Y}_{\text{obs}}$ to estimate the entire sequence $\mathcal{Y}$ by performing the Bayesian updates in~\eqref{eq:bayesian updates} and then sampling $\by_t \sim g(\cdot | \bX_t)$.
However, this recursion incurs computational costs that scale with the length of the observations~\citep{sarkka2020temporal}.
Hence, applying this elegant paradigm directly to real-world large-scale datasets---particularly those with a large observation length---is not straightforward due to scalability issues.