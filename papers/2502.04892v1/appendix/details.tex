\section{Proofs and Derivations}
\label{sec:app:proofs}
\subsection{Proof of~\cref{proposition:ELBO}}
\label{sec:app:proof proposition}
We begin by presenting the Girsanov theorem~\citep{baldi2017stochastic}, which serves as a powerful tool for changing probability measures in stochastic processes. This theorem will be the key to relating the ELBO with the SOC cost function. 
\begin{theorem}[Girsanov Theorem]\label{theorem:girsanov} Consider the two Itô diffusion processes of form
\[
    & d\mathbf{X}_t = b(t, \mathbf{X}_t)dt + \sigma(t, \mathbf{X}_t)^\top d\mathbf{W}_t, \quad t \in [0, T]\label{eq:girsanov eq_1}, \\
    & d\mathbf{Y}_t = \tilde{b}(t, \mathbf{Y}_t)dt + \sigma(t, \mathbf{Y}_t)^\top d\mathbf{W}_t, \quad t \in [0, T] 
\]
where both drift functions $b, \tilde{b}$ and the diffusion function $\sigma$ assumed to be invertible are adapted to $\mathcal{F}_t$ and $\bW_{[0,T]}$ is $\mathbb{P}$-Wiener process. Moreover, consider $\bbO$ as the path measures induced by (\ref{eq:girsanov eq_1}). Let us define $\bH_t := \sigma^{-1}(\tilde{b} - b)$ which is assumed to be satisfying the Novikov's condition ($\ie \mathbb{E}_{\mathbb{P}}\left[\exp\left(\frac{1}{2}\int_0^T \norm{H_s}^2 ds\right)\right] < \infty$), and the $\mathbb{P}$-martingale process
\[
    \mathbf{M}_t := \exp \left(\int_0^1 \bH_s^\top d\bW_s -\frac{1}{2}\int_0^t \norm{\bH_s}^2 ds  \right)
\]
satisfies $\mathbb{E}_{\mathbb{P}}[\mathbf{M}_T] = 1$. Then for the path measure $\bbQ$ given as $d\bbQ = \mathbf{M}_T d\bbP$,
the process $\tilde{\bW}_t = \bW_t - \int_0^t \bH_s ds$ is a $\bbQ$-Wiener process and $\mathbf{Y}_t$ can be represented as
\begin{equation}
    d\mathbf{Y}_t = b(t, \mathbf{Y}_t)dt + \sigma(t, \mathbf{Y}_t)^\top d\tilde{\bW}_t, \quad t \in [0,T].
\end{equation}
Therefore $\bbQ$-law of the process $\mathbf{Y}_t$ is same as $\bbP$-law of the process $\mathbf{X}_t$. 
\end{theorem}


\begin{proof} Consider the definition of the normalization constant:
\[
 \log \bZ(\mathcal{Y}) &= \log \bbE_{\bX \sim~\eqref{eq:prior dynamics}} \left[ p(\mathcal{Y} | \bX_{[0, T]}) \right]
\]
Expanding this expectation, we have
    \[
        \log \bZ(\mathcal{Y}) &= \log \bbE_{\bX \sim~\eqref{eq:prior dynamics}} \left[ p(\mathcal{Y} | \bX_{[0, T]}) \right] \\
        & = \log \bbE_{\bX \sim~\eqref{eq:prior dynamics}} \left[ p(\mathcal{Y} | \bX^{\alpha}_{[0, T]}) \frac{p(\bX_{[0, T]}) }{p(\bX^{\alpha}_{[0, T]})}  \right] \\
        & \stackrel{(i)}{\geq} \bbE_{\bX \sim~\eqref{eq:linearized controlled SDE}} \left[\log p(\mathcal{Y} | \bX^{\alpha}_{[0, T]}) + \log \frac{p(\bX_{[0, T]})}{p(\bX^{\alpha}_{[0, T]})} \right] \\
        & = \bbE_{\bX \sim~\eqref{eq:linearized controlled SDE}} \left[\sum_{t \in \mathcal{T}} g(\by_t | \bX^{\alpha}_t) + \log \frac{p(\bX_{[0, T]})}{p(\bX^{\alpha}_{[0, T]})}\right] \\
        & \stackrel{(ii)}{=} \bbE_{\bX \sim~\eqref{eq:linearized controlled SDE}} \left[\sum_{t \in \mathcal{T}} g(\by_t | \bX^{\alpha}_t) -\frac{1}{2}\int_0^T \norm{\alpha_t}^2 dt + \int_0^1 \alpha_t d\bW_s \right]  \\
        & \stackrel{(iii)}{=} \bbE_{\bX \sim~\eqref{eq:linearized controlled SDE}} \left[\sum_{t \in \mathcal{T}} g(\by_t | \bX^{\alpha}_t) -\frac{1}{2}\int_0^T \norm{\alpha_t}^2 dt \right] \\
        & = -\mathcal{J}(\alpha, \mathcal{Y}),
    \]
where $(i)$ results from Jensen's inequality, $(ii)$ follows by applying Girsanov's theorem~\cref{theorem:girsanov}, and in the final equality, $(iii)$ holds because $\bW_t$ is a martingale process with respect to the distribution $p(\bX^{\alpha}_{[0, T]})$.

\end{proof}

\subsection{Proof of~\cref{theorem:simulation free inference}}
\label{sec:app:proof theorem}
\begin{proof} 

Since each SPD matrix \(\bD_{t}\) for \(t \in \mathcal{T}\) admits an eigen-decomposition \(\bD_{t_i} = \bV \bLambda_{t_i} \bV^{\top}\), we can transform the original process \(\bX^{\alpha}_t\), which is expressed in the canonical basis, into a new process \(\hat{\bX}^{\alpha}_t = \bV^\top \bX^{\alpha}_t\) that resides in the space spanned by the eigenbasis \(\bV\).  With this transformation, the dynamics in~\eqref{eq:linearized controlled SDE} can be rewritten, for any interval \([t_i, t_{i+1})\), as:
\[
\label{eq:linear controlled SDE appx}
d\hat{\bX}^{\alpha}_t = \left[-\bLambda_{t_i} \hat{\bX}^{\alpha}_t + \alpha_{t_i}\right] dt + d\hat{\bW}_t,
\]
where $\hat{\bX}^{\alpha}_t = \bV^\top \bX^{\alpha}_t$, $\hat{\alpha}_{t_i} = \bV^\top \alpha_{t_i}$, $\hat{\bW}_t = \bV^\top \bW_t$ and initial condition $\hat{\bX}^{\alpha}_0 \sim \mathcal{N}(\hat{\mu}_0, \hat{\Sigma}_0)$ with $\hat{\mu}_0 = \bV^\top \mu_0$ and $\hat{\Sigma}_0 = \bV^\top \Sigma_0 \bV$. Since \(\bV\) is orthonormal, \(\hat{\bW}_t\) retains the distribution \(\hat{\bW}_t \stackrel{d}{=} \bW_t\) for all \(t \in [0, T]\), allowing \(\hat{\bW}_t\) to be treated as a standard Wiener process. Now, given that \(\bLambda_{t_i}\) is diagonal, the linear SDE in equation~\eqref{eq:linear controlled SDE appx} admits a closed-form solution for any \(t \in [t_i, t_{i+1})\):
\[
\hat{\bX}^{\alpha}_t = e^{-(t - t_i)\bLambda_{t_i}} \left( \hat{\bX}^{\alpha}_{t_i} + \int_{t_i}^t e^{(s - t_i)\bLambda_{t_i}} \hat{\alpha}_{t_i} \, ds + \int_{t_i}^t e^{(s - t_i)\bLambda_{t_i}} \, d\hat{\bW}_s \right).
\]
Since the initial condition \(\hat{\bX}^{\alpha}_0\) is Gaussian and the SDE is linear with Gaussian noise, the process \(\hat{\bX}^{\alpha}_t\) remains Gaussian. Therefore, its first two moments—the mean and covariance—can be derived from the solution above. To derive the moments, we firstly evaluate the deterministic integral involving \(\hat{\alpha}_{t_i}\):
\[
\int_{t_i}^t e^{(s - t_i)\bLambda_{t_i}} \hat{\alpha}_{t_i} \, ds = -\bLambda_{t_i}^{-1} \left( \bI - e^{(t - t_i)\bLambda_{t_i}}\right) \hat{\alpha}_{t_i}.
\]
Taking the expectation of \(\hat{\bX}^{\alpha}_t\), and using the martingale property of the Wiener process \(\hat{\bW}_t\), we obtain:
\[
\label{eq:mean recur}
\hat{\mu}_t = \bbE_{\hat{\bX}^{\alpha} \sim~\eqref{eq:linear controlled SDE appx}}\left[\hat{\bX}^{\alpha}_t\right] = e^{-(t - t_i)\bLambda_{t_i}} \hat{\mu}_{t_i} - e^{-(t - t_i)\bLambda_{t_i}} \bLambda_{t_i}^{-1} \left( \bI - e^{-(t - t_i)\bLambda_{t_i}} \right) \hat{\alpha}_{t_i}.
\]
Next, compute the covariance of \(\hat{\bX}^{\alpha}_t\):
\[
    \hat{\Sigma}_{t} &= \bbE_{\hat{\bX}^{\alpha} \sim~\eqref{eq:linear controlled SDE appx}}\left[ e^{-2(t - t_i) \bLambda_{t_i}} \left( \bX_{t_i} - \mu_{t_i} + \int_{t_i}^t e^{(s - t_i) \bLambda_{t_i}} d\hat{\bW}_s  \right) \left( \bX_{t_i} - \mu_{t_i} + \int_{t_i}^t e^{(s - t_i) \bLambda_{t_i}} d\hat{\bW}_s  \right)^{\top}\right] \\
    & = e^{-2(t - t_i) \bLambda_{t_i}} \bbE_{\hat{\bX}^{\alpha} \sim~\eqref{eq:linear controlled SDE appx}}\left[ \left( \bX_{t_i} - \mu_{t_i} \right)\left( \bX_{t_i} - \mu_{t_i} \right)^{\top} +  \norm{\int_{t_i}^t e^{(s - t_i) \bLambda_{t_i}} d\hat{\bW}_s}_2^2  \right] \\
    & \stackrel{(i)}{=} e^{-2(t - t_i) \bLambda_{t_i}} \bbE_{\hat{\bX}^{\alpha} \sim~\eqref{eq:linear controlled SDE appx}}\left[ \left( \bX_{t_i} - \mu_{t_i} \right)\left( \bX_{t_i} - \mu_{t_i} \right)^{\top} +  \int_{t_i}^t e^{2(s - t_i) \bLambda_{t_i}} ds \right] \\
    & \stackrel{(ii)}{=} e^{-2(t - t_i) \bLambda_{t_i}} \hat{\Sigma}_{t_i} - \frac{1}{2} e^{-2(t - t_i) \bLambda_{t_i}} \bLambda^{-1}_{t_i} \left( \bI - e^{2(t - t_i) \bLambda_{t_i}} \right), \label{eq:cov recur}
\]
where $(i)$ follows from the martingale property of $\hat{\bW}_t$ and $(ii)$ follows from Itô isometry:
\[ 
\bbE_{\hat{\bX}^{\alpha} \sim~\eqref{eq:linear controlled SDE appx}}\left[ \norm{\int_{t_i}^t e^{(s - t_i) \bLambda_{t_i}} d\hat{\bW}_s}_2^2 \right] = \bbE_{\hat{\bX}^{\alpha} \sim~\eqref{eq:linear controlled SDE appx}}\left[ \int_{t_i}^t e^{2(s - t_i) \bLambda_{t_i}} ds \right].
\]
Using the recursive forms for the mean and covariance, we can determine these moments at each discrete time step \(t_i\). For the mean \(\hat{\mu}_{t_i}\), the recurrence relation is:
\[
    & \hat{\mu}_{t_1} = e^{-(t_1 - t_0) \bLambda_{t_0}} \hat{\mu}_{t_0} - e^{-(t_1 - t_0)\bLambda_{t_1}} \bLambda^{-1}_{t_0} \left( \bI - e^{(t_1 - t_0) \bLambda_{t_0}} \right) \hat{\alpha}_{t_0} \\
    & \hat{\mu}_{t_2} = e^{-\sum_{j=0}^1 (t_{j+1} - t_{j}) \bLambda_{t_j}}   \hat{\mu}_{t_0}  \\
    & \hspace{20mm} - e^{-\sum_{j=0}^1 (t_{j+1} - t_{j}) \bLambda_{t_j}} \bLambda^{-1}_{t_0} \left( \bI - e^{(t_1 - t_0) \bLambda_{t_0}} \right) \hat{\alpha}_{t_0} - e^{-(t_2 - t_1)\bLambda_{t_1}} \bLambda^{-1}_{t_1} \left( \bI - e^{(t_2 - t_1) \bLambda_{t_1}}\right) \hat{\alpha}_{t_1} \\
    & \vdots \\
    & \hat{\mu}_{t_i} = e^{-\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}}   \hat{\mu}_{t_0} - \sum_{l=0}^{i-1} e^{-\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bLambda^{-1}_{t_l} \left( \bI - e^{(t_{l+1} - t_l) \bLambda_{t_l}} \right) \hat{\alpha}_{t_{l}}
\]

Similarly, for the covariance \(\hat{\Sigma}_{t_i}\), the recurrence relation is:
\[
    & \hat{\Sigma}_{t_1} = e^{-2(t_1 - t_0) \bLambda_{t_0}} \hat{\Sigma}_{t_0} - \frac{1}{2}e^{-2(t_1 - t_0)\bLambda_{t_1}} \bLambda^{-1}_{t_0} \left( \bI - e^{2(t_1 - t_0) \bLambda_{t_0}} \right) \\
    & \hat{\Sigma}_{t_2} = e^{-\sum_{j=0}^1 2(t_{j+1} - t_{j}) \bLambda_{t_j}}   \hat{\Sigma}_{t_0}\\
    & \hspace{20mm} - \frac{1}{2}e^{-\sum_{j=0}^1 2(t_{j+1} - t_{j}) \bLambda_{t_j}} \bLambda^{-1}_{t_0} \left( \bI - e^{2(t_1 - t_0) \bLambda_{t_0}} \right) - \frac{1}{2}e^{-2(t_2 - t_1)\bLambda_{t_1}} \bLambda^{-1}_{t_1} \left( \bI - e^{2(t_2 - t_1) \bLambda_{t_1}}\right) \\
    & \vdots \\
    & \hat{\Sigma}_{t_i} = e^{-2\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}}   \hat{\Sigma}_{t_0} - \frac{1}{2}\sum_{l=0}^{i-1} e^{-2\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bLambda^{-1}_{t_l} \left( \bI - e^{2(t_{l+1} - t_l) \bLambda_{t_l}} \right).
\]
Now, since \(\hat{\bX}^{\alpha}_t = \bV^\top \bX^{\alpha}_t\), with \(\hat{\mu}_0 = \bV^\top \mu_0\) and \(\hat{\Sigma}_0 = \bV^\top \Sigma_0 \bV\), we can express the mean and covariance in the original canonial basis as follows. For the mean $\hat{\mu}_{t \in \mathcal{T}}$, which is given by
\[
     \bV \hat{\mu}_{t_i} &= \bV \left( e^{-\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}}   \hat{\mu}_{t_0} - \sum_{l=0}^{i-1} e^{-\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bLambda^{-1}_{t_l} \left( \bI - e^{(t_{l+1} - t_l) \bLambda_{t_l}} \right) \hat{\alpha}_{t_{l}} \right) \\
     & = \bV \left( e^{-\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bV^{\top} \mu_0 - \sum_{l=0}^{i-1} e^{-\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bLambda^{-1}_{t_l} \left( \bI - e^{(t_{l+1} - t_l) \bLambda_{t_l}} \right) \bV^{\top} \alpha_{t_{l}} \right) \\
     & = e^{-\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bD_{t_j}} \mu_0 - \bV \left( \sum_{l=0}^{i-1} e^{-\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bLambda^{-1}_{t_l} \left( \bI - e^{(t_{l+1} - t_l) \bLambda_{t_l}} \right) \bV^{\top} \alpha_{t_{l}} \right) \\
     & = e^{-\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bD_{t_j}} \mu_0 - \bV \left( \sum_{l=0}^{i-1} e^{-\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bV^{\top} \bD^{-1}_{t_l} \bV \left( \bI - e^{(t_{l+1} - t_l) \bLambda_{t_l}} \right) \bV^{\top} \alpha_{t_{l}} \right) \\
     & = e^{-\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bD_{t_j}} \mu_0 -  \sum_{l=0}^{i-1} e^{-\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bD_{t_j}} \bD^{-1}_{t_l} \left( \bI - e^{(t_{l+1} - t_l) \bD_{t_l}} \right) \alpha_{t_{l}} \\
     & = \mu_{t_i}
\]
where we used \(\bD_{t_j} = \bV \bLambda_{t_j} \bV^{\top}\) and the orthonormality of \(\bV\). Similarly, for the covariance $\hat{\Sigma}_{t \in \mathcal{T}}$, we have

\[
     \bV \hat{\Sigma}_{t_i} \bV^{\top} &= \bV \left( e^{-2\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}}   \hat{\Sigma}_{t_0} - \frac{1}{2}\sum_{l=0}^{i-1} e^{-2\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bLambda^{-1}_{t_l} \left( \bI - e^{2(t_{l+1} - t_l) \bLambda_{t_l}} \right) \right)\bV^{\top} \\
     & = \bV \left( e^{-2\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bV^{\top} \Sigma_0 \bV - \frac{1}{2}\sum_{l=0}^{i-1} e^{-2\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bLambda^{-1}_{t_l} \left( \bI - e^{2(t_{l+1} - t_l) \bLambda_{t_l}} \right)  \right)\bV^{\top} \\
     & = e^{-2\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bD_{t_j}} \Sigma_0 - \bV \left( \frac{1}{2}\sum_{l=0}^{i-1} e^{-2\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bLambda^{-1}_{t_l} \left( \bI - e^{2(t_{l+1} - t_l) \bLambda_{t_l}} \right) \right)\bV^{\top} \\
     & = e^{-2\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bD_{t_j}} \Sigma_0 - \bV \left( \sum_{l=0}^{i-1} e^{-2\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bLambda_{t_j}} \bV^{\top} \bD^{-1}_{t_l} \bV \left( \bI - e^{2(t_{l+1} - t_l) \bLambda_{t_l}} \right) \right)\bV^{\top} \\
     & = e^{-2\sum_{j=0}^{i-1} (t_{j+1} - t_{j}) \bD_{t_j}} \Sigma_0 -  \frac{1}{2}\sum_{l=0}^{i-1} e^{-2\sum_{j=l}^{i-1} (t_{j+1} - t_{j}) \bD_{t_j}} \bD^{-1}_{t_l} \left( \bI - e^{2(t_{l+1} - t_l) \bD_{t_l}} \right) \\
     & = \Sigma_{t_i}
\]
Thus, both the mean \(\mu_{t_i}\) and the covariance \(\Sigma_{t_i}\) of \(\bX^{\alpha}_t\) at each time step \(t_i\) are correctly recovered, completing the proof.
\end{proof}






\subsection{Derivation of ELBO in~\cref{eq:training objective}}
\label{sec:app:deriving elbo}
We start the derivation by integrating the mixture distribution in~\eqref{eq:mixture distribution} into the SOC problem~\eqref{eq: MAE_3} as follows:
\[
    & \log p(\mathcal{Y}_{\text{tar}} | \bX^{\theta}_{[0, T]}) = \log \int  \gamma_{\psi}(\by_t | \bz_t) \pi_{\bar{\theta}}(\bz_t | \bX^{\theta}_t) d\bz_t \\
    &  = \log \int  \gamma_{\psi}(\by_t | \bz_t) \frac{1}{\bZ(\bX^{\theta}_t)}\left[ p(\bz_t | \bX^{\theta}_{t})^{\lambda} q_{\bar{\theta}}(\bz_t| \mathcal{Y}_{\text{tar}})^{1-\lambda}  \right] d\bz_t \\
    & = \log \int \gamma_{\psi}(\by_t | \bz_t) \left[ \frac{p(\bz_t | \bX^{\theta}_{t})^{\lambda} q_{\bar{\theta}}(\bz_t | \mathcal{Y}_{\text{tar}})^{1-\lambda}}{\bZ(\bX^{\theta}_t) h(\bz_t)} \right] h(\bz_t) d\bz_t -\log \bZ(\bX^{\theta}_t) \\
    & \stackrel{(i)}{\geq}  \int \left[ \log \gamma_{\psi}(\by_t | \bz_t) + \lambda \log p(\bz_t | \bX^{\theta}_{t}) + (1-\lambda) \log q_{\bar{\theta}}(\bz_t | \mathcal{Y}_{\text{tar}}) - \log h(\bz_t) \right] h(\bz_t) d\bz_t -\log \bZ(\bX^{\theta}_t) \\
    & \stackrel{(ii)}{\geq} \int \left[ \log \gamma_{\psi}(\by_t | \bz_t) + (\lambda - 1) \log p(\bz_t | \bX^{\theta}_{t}) + (1-\lambda) \log q_{\bar{\theta}}(\bz_t | \mathcal{Y}_{\text{tar}}) \right] p(\bz_t | \bX^{\theta}_{t}) d\bz_t -\log \bZ(\bX^{\theta}_t)  \\
    & \stackrel{(iii)}{=} \int \left[ \log \gamma_{\psi}(\by_t | \bz_t) + (1-\lambda) \log q_{\bar{\theta}}(\bz_t | \mathcal{Y}_{\text{tar}}) \right] p(\bz_t | \bX^{\theta}_{t}) d\bz_t + (1 - \lambda) C  -\log \bZ(\bX^{\theta}_t) \\
    & \stackrel{(iv)}{\geq} \bbE_{\bz_t \sim p(\bz_t | \bX^{\theta}_t)} \left[\underbrace{\log \gamma_{\psi}(\by_t | \bz_t)}_{\text{MAE}} + (1-\lambda) \underbrace{\log q_{\bar{\theta}}(\bz_t | \mathcal{Y}_{\text{tar}})}_{\text{JEPA}}  \right] \\
    & = \bbE_{\bz_t \sim p(\bz_t | \bX^{\theta}_t)} \left[ \frac{1}{2\sigma^2_\gamma} \norm{\by_t - \bD_{\psi}(\bz_t)}^2 + \frac{(1-\lambda)}{2\sigma^2_q} \norm{\bz_t - \mathcal{T}_{\bar{\theta}}(t, \mathcal{Y}_{\text{tar}})}^2\right],
\]
where $(i)$ follows from Jensen's inequality, and $(ii)$ follows by setting proposal distribution $ h(\bz_t) = p(\bz_t | \bX^{\theta}_{t})$, $(iii)$ follows from the definition $p(\bz_t | \bX^{\theta}_{t}) \sim \mathcal{N}(\bX^{\theta}_{t}, \sigma_p^2 \bI)$, since the entropy of Gaussian with constant covariance:
\[
& \int (\lambda - 1) \log p(\bz_t | \bX^{\theta}_{t}) p(\bz_t| \bX^{\theta}_t) d \bz_t = (1 - \lambda)  \int -\log p(\bz_t | \bX^{\theta}_{t}) p(\bz_t| \bX^{\theta}_{t}) d\bz_t = (1 - \lambda) C \geq 0.
\]
Finally, $(iv)$ follows from $(1 - \lambda) C \geq 0$ and since the normalization constant $\bZ(\bX^{\theta}_t)$ is calculated as:
\[
    \bZ(\bX^{\theta}_{t}) &=  \int \gamma_{\psi}(\bz_t | \bX^{\theta}_{t} )^{\lambda} q_{\bar{\theta}}(\bz_t | \mathcal{Y}_{\text{tar}})^{1-\lambda}  d\bz_t = \int \bC_1 \exp \left[ - \frac{\lambda}{2 \sigma^2_p} \norm{\bz_t - \bX^{\theta}_t}^2 - \frac{(1-\lambda)}{2\sigma^2_q} \norm{\bz_t - \bT_{\bar{\theta}}(t, \mathcal{Y}_{\text{tar}})}^2 \right] \\
    & = \int \bC_1 \exp \left[ -\frac{1}{2} (\bz_t - \bm)^{\top} \bS^{-1}(\bz_t - \bm)  +  \frac{1}{2} \left( \bm^{\top} \bS^{-1} \bm - \frac{\lambda}{\sigma^2_p} \norm{\bX^{\theta}_{t}}^2 - \frac{1 - \lambda}{\sigma^2_q} \norm{\bT_{t, \bar{\theta}}(\mathcal{Y}_{\text{tar}})}^2\right)\right] \\
    & = \bC_3 \exp \left[\frac{1}{2} \left( \bm^{\top} \bS^{-1} \bm - \frac{\lambda}{\sigma^2_p} \norm{\bX^{\theta}_{t}}^2 - \frac{1 - \lambda}{\sigma^2_q} \norm{\bT_{t, \bar{\theta}}(\mathcal{Y}_{\text{tar}})}^2 \right)\right],
\]
where $\bC_1 = \frac{1}{(2\pi)^{d/2} (\sigma_1^2)^{\frac{\lambda d}{2}}(\sigma_3^2)^{\frac{(1-\lambda) d}{2}}}$, $\bC_3 = \frac{1}{\left(\frac{\lambda}{\sigma_1^2} + \frac{1-\lambda}{\sigma_3^2} \right)^{d/2} (\sigma_1^2)^{\frac{\lambda d}{2}}(\sigma_3^2)^{\frac{(1-\lambda) d}{2}}}$, 
\[
    \bm = \bS\left(\frac{\lambda}{\sigma_p^2} \bX^{\theta}_{t} + \frac{1-\lambda}{\sigma_q^2} \bT_{\bar{\theta}}(t, \mathcal{Y}_{\text{tar}}) \right), \; \text{and} \; \bS = \left(\frac{\lambda}{\sigma_p^2} + \frac{1-\lambda}{\sigma_q^2} \right)^{-1} \bI.
\]
Consequently, we get
\[
     \bZ(\bX^{\theta}_{t}) &= \bC_3 \exp \left[ \frac{1}{2} \left( \frac{\left(  \frac{\lambda}{\sigma_p^2} \bX^{\theta}_{t} + \frac{1-\lambda}{\sigma_q^2} \bT_{\bar{\theta}}(t, \mathcal{Y}_{\text{tar}})  \right)^{\top} \left( \frac{\lambda}{\sigma_p^2} \bX^{\theta}_{t} + \frac{1-\lambda}{\sigma_q^2} \bT_{\bar{\theta}}(t, \mathcal{Y}_{\text{tar}}) \right)}{\left(\frac{\lambda}{\sigma_p^2} + \frac{1-\lambda}{\sigma_q^2} \right)} \right) -  \frac{\lambda}{\sigma_p^2} \norm{\bX^{\theta}_{t}}^2 - \frac{1-\lambda}{\sigma_q^2} \norm{\bT_{\bar{\theta}}(t, \mathcal{Y}_{\text{tar}})}^2\right] \\
     & = \bC_3 \exp \left[ - \frac{\frac{\lambda(1 - \lambda)}{\sigma_1^2 \sigma^3_2}}{2 \left(\frac{\lambda}{\sigma_1^2} + \frac{1-\lambda}{\sigma_3^2} \right)} \norm{\bX^{\theta}_{t} - \bT_{\bar{\theta}}(t, \mathcal{Y}_{\text{tar}})}^2 \right].
\]
It implies that $-\log \bZ(\bX^{\theta}_t) \geq 0$. Hence we can derive the desired inequality in~\eqref{eq:training objective}:
\[
    - \log p(\mathcal{Y}_{\text{tar}} | \mathcal{Y}_{\text{ctx}}) & \leq \bbE_{\bX^{\theta} \sim \eqref{eq:linearized controlled SDE}} \left[ \int_0^T \frac{1}{2}\norm{\alpha^{\theta}_t}^2 dt - \sum_{t \in \mathcal{T}_{\text{obs}}} \bbE_{p(\bz_t | \bX^{\theta}_t)} \left(\log g_{\psi}(\by_t | \bz_t) + (1-\lambda) \log q_{\bar{\theta}}(\bz_t | \mathcal{Y}_{\text{tar}})\right) \right] \\
    & \hspace{-17mm} = \bbE_{\bX^{\theta} \sim \eqref{eq:linearized controlled SDE}} \left[ \int_0^T \frac{1}{2}\norm{\alpha^{\theta}_t}^2 dt - \sum_{t \in \mathcal{T}_{\text{obs}}} \bbE_{\bz_t \sim p(\bz_t | \bX^{\theta}_t)} \left[ \frac{1}{2\sigma^2_\gamma} \norm{\by_t - \bD_{\psi}(\bz_t)}^2 + \frac{(1-\lambda)}{2\sigma^2_q} \norm{\bz_t - \mathcal{T}_{\bar{\theta}}(t, \mathcal{Y}_{\text{tar}})}^2\right] \right] \\
    & = \mathcal{L}(\theta, \psi).
\]
For stable learning, we train our model with rescaled training objective as a factor of $2\sigma^2_{\gamma}$:
\[\label{eq:rescaled training objective}
    \hat{\mathcal{L}}(\theta, \psi) = \bbE_{\bX^{\theta} \sim \eqref{eq:linearized controlled SDE}} \left[ \int_0^T \sigma^2_q \norm{\alpha^{\theta}_t}^2 dt - \sum_{t \in \mathcal{T}_{\text{obs}}} \bbE_{\bz_t \sim p(\bz_t | \bX^{\theta}_t)} \left[ \underbrace{\norm{\by_t - \bD_{\psi}(\bz_t)}^2}_{\text{reconstruction}} + \tau \underbrace{\norm{\bz_t - \mathcal{T}_{\bar{\theta}}(t, \mathcal{Y}_{\text{tar}})}^2}_{\text{regularization}}\right] \right],
\]
Here, $\tau = \frac{(1-\lambda) \sigma^2_{\gamma}}{\sigma^2_q}$ determines the balance between reconstruction and regularization. See~\cref{sec:main:experiment:ablation study} for details on how controlling the regularization influences the performance of BDO.


\section{Parallel Scan Algorithm}\label{sec:parallel_scan_algorithm}

The computation of the first two moments—the mean $\mu_{t \in \mathcal{T}}$ and covariance $\Sigma_{t \in \mathcal{T}}$—of the controlled distributions can be efficiently parallelized using the scan (all-prefix-sums) algorithm~\citep{blelloch1990prefix}. Leveraging the associativity of the underlying operations, we reduce the computational complexity from $\mathcal{O}(k)$ to $\mathcal{O}(\log k)$ time with respect to the number of time steps $k$. We have established the linear recurrence in~\cref{theorem:simulation free inference} for the mean and covariance at each time step $t_i$:
\begin{align}
    \mathbf{m}_{t_i} &= \hat{\bA}_i \mathbf{m}_{t_{i-1}} - \hat{\bB}_i \mathbf{\alpha}_{t_i}, \label{eq:Mean_recurrence} \\
    \mathbf{\Sigma}_{t_i} &= \bar{\bA}_i \mathbf{\Sigma}_{t_{i-1}} - \bar{\bB}_i \bI, \label{eq:Covariance_recurrence}
\end{align}
where we, for brevity, we define $\Delta_{i}(t) = t - t_{i}$, $\hat{\mathbf{A}}_{i} = e^{-\Delta_{i-1}(t_{i})\bLambda_{t_i}}$, $\hat{\bB}_i = - e^{-(t_i - t_{i-1})\mathbf{\Lambda}_{t_i}} \mathbf{\Lambda}_{t_i}^{-1} \left( \mathbf{I} - e^{-(t_i - t_{i-1})\mathbf{\Lambda}_{t_i}} \right)$, $\bar{\bA}_i = e^{-2\Delta_{i-1}(t_{i})\bLambda_{t_i}}$ and $\bar{\bB}_i = \frac{1}{2} e^{-2(t_i - t_{i-1})\mathbf{\Lambda}_{t_i}} \mathbf{\Lambda}_{t_i}^{-1} \left( \mathbf{I} - e^{-2(t_i - t_{i-1})\mathbf{\Lambda}_{t_i}} \right)$. To apply the parallel scan algorithm to our recurrence, we define two separate sequences of tuples for the mean and covariance computations for all $i \in \{1, \cdots, k\}$:
\[
    \mathbf{M}_i = \left(\hat{\mathbf{A}}_{i}, \hat{\mathbf{B}}_{i} \mathbf{\alpha}_{t_i}\right),  \quad \mathbf{S}_i = \left(\bar{\mathbf{A}}_{i}, \bar{\mathbf{B}}_{i}\right)
\]
Now, we define binary associative operators $\otimes$ and  for the sequences $\{\mathbf{M}_i\}$ and $\{\mathbf{S}_i\}$:
\[
    & \mathbf{M}_i \otimes \mathbf{M}_j = \left(\hat{\mathbf{A}}_i \circ \hat{\mathbf{A}}_j, \hat{\mathbf{A}}_i \circ \hat{\mathbf{B}}_j \mathbf{\alpha}_{t_j} + \hat{\mathbf{B}}_i \mathbf{\alpha}_{t_i}\right), \label{eq:operator_M} \\
    & \mathbf{S}_i \otimes \mathbf{S}_j = \left(\bar{\mathbf{A}}_i \circ \bar{\mathbf{A}}_j, \bar{\mathbf{A}}_i \circ \bar{\mathbf{B}}_j + \bar{\mathbf{B}}_i\right), \label{eq:operator_S}
\]
where $\circ$ denotes element-wise multiplication. We can verify that $\otimes$ is an associative operator since it satisfies:
\[
    (\mathbf{M}_s \otimes \mathbf{M}_t) \otimes \mathbf{M}_u &= \left(\hat{\mathbf{A}}_t \circ \hat{\mathbf{A}}_s, \hat{\mathbf{A}}_t \circ \hat{\mathbf{B}}_s \mathbf{\alpha}_{t_s} + \hat{\mathbf{B}}_t \mathbf{\alpha}_{t_t}\right) \otimes\mathbf{M}_u \\
    &= \left(\hat{\mathbf{A}}_u \circ (\hat{\mathbf{A}}_t \circ \hat{\mathbf{A}}_s), \hat{\mathbf{A}}_u \circ (\hat{\mathbf{A}}_t \circ \hat{\mathbf{B}}_s \mathbf{\alpha}_{t_s} + \hat{\mathbf{B}}_t \mathbf{\alpha}_{t_t}) + \hat{\mathbf{B}}_u \mathbf{\alpha}_{t_u}\right) \\
    &= \left(\hat{\mathbf{A}}_u \circ \hat{\mathbf{A}}_t \circ \hat{\mathbf{A}}_s, \hat{\mathbf{A}}_u \circ \hat{\mathbf{A}}_t \circ \hat{\mathbf{B}}_s \mathbf{\alpha}_{t_s} + \hat{\mathbf{A}}_u \circ \hat{\mathbf{B}}_t \mathbf{\alpha}_{t_t} + \hat{\mathbf{B}}_u \mathbf{\alpha}_{t_u}\right) \\
    & = \left(\hat{\mathbf{A}}_u \circ \hat{\mathbf{A}}_t \circ \hat{\mathbf{A}}_s, \hat{\mathbf{A}}_u \circ (\hat{\mathbf{A}}_t \circ \hat{\mathbf{B}}_s \mathbf{\alpha}_{t_s} + \hat{\mathbf{B}}_t \mathbf{\alpha}_{t_t}) + \hat{\mathbf{B}}_u \mathbf{\alpha}_{t_u}\right) \\
    & = \left(\hat{\mathbf{A}}_u \circ \hat{\mathbf{A}}_t \circ \hat{\mathbf{A}}_s, \hat{\mathbf{A}}_u \circ \hat{\mathbf{A}}_t \circ \hat{\mathbf{B}}_s \mathbf{\alpha}_{t_s} + \hat{\mathbf{A}}_u \circ \hat{\mathbf{B}}_t \mathbf{\alpha}_{t_t} + \hat{\mathbf{B}}_u \mathbf{\alpha}_{t_u}\right) \\
    & = \mathbf{M}_s \otimes (\mathbf{M}_t \otimes \mathbf{M}_u).
\]
Thus, we get $(\mathbf{M}_s \otimes \mathbf{M}_t) \otimes \mathbf{M}_u = \mathbf{M}_s \otimes (\mathbf{M}_t \otimes \mathbf{M}_u)$, confirming associativity for $\mathbf{M}_i$. Similarly, 
\[
    (\mathbf{S}_s \otimes \mathbf{S}_t) \otimes\mathbf{S}_u &= \left(\bar{\mathbf{A}}_t \circ \bar{\mathbf{A}}_s, \bar{\mathbf{A}}_t \circ \bar{\mathbf{B}}_s + \bar{\mathbf{B}}_t\right) \otimes \mathbf{S}_u \\
    &= \left(\bar{\mathbf{A}}_u \circ (\bar{\mathbf{A}}_t \circ \bar{\mathbf{A}}_s), \bar{\mathbf{A}}_u \circ (\bar{\mathbf{A}}_t \circ \bar{\mathbf{B}}_s + \bar{\mathbf{B}}_t) + \bar{\mathbf{B}}_u\right) \\
    &= \left(\bar{\mathbf{A}}_u \circ \bar{\mathbf{A}}_t \circ \bar{\mathbf{A}}_s, \bar{\mathbf{A}}_u \circ \bar{\mathbf{A}}_t \circ \bar{\mathbf{B}}_s + \bar{\mathbf{A}}_u \circ \bar{\mathbf{B}}_t + \bar{\mathbf{B}}_u\right) \\
    & = \left(\bar{\mathbf{A}}_u \circ \bar{\mathbf{A}}_t \circ \bar{\mathbf{A}}_s, \bar{\mathbf{A}}_u \circ (\bar{\mathbf{A}}_t \circ \bar{\mathbf{B}}_s + \bar{\mathbf{B}}_t) + \bar{\mathbf{B}}_u\right) \\
    &= \left(\bar{\mathbf{A}}_u \circ \bar{\mathbf{A}}_t \circ \bar{\mathbf{A}}_s, \bar{\mathbf{A}}_u \circ \bar{\mathbf{A}}_t \circ \bar{\mathbf{B}}_s + \bar{\mathbf{A}}_u \circ \bar{\mathbf{B}}_t + \bar{\mathbf{B}}_u\right) \\
    & = \mathbf{S}_s \otimes (\mathbf{S}_t \otimes \mathbf{S}_u).
\]
Hence, $(\mathbf{S}_s \otimes \mathbf{S}_t) \otimes \mathbf{S}_u = \mathbf{S}_s \otimes (\mathbf{S}_t \otimes \mathbf{S}_u)$, confirming associativity for $\mathbf{S}_i$. Now, we can apply the parallel scan described in~\cref{alg:parallel_scan} for both $\mu_{t \in \mathcal{T}}$ and covariance $\Sigma_{t \in \mathcal{T}}$ based on the recurrence in~(\ref{eq:mean recur}, \ref{eq:cov recur}) and the defined associative operators $\otimes$. Employing the parallel scan algorithm offers significant computational benefits, especially for large-scale problems with numerous time steps $k$. The logarithmic time complexity ensures scalability, making it feasible to perform real-time computations or handle high-dimensional data efficiently.


























\begin{figure}[!t]
\begin{minipage}[t]{0.54\textwidth}
\begin{algorithm}[H]
\caption{Parallel Scan for Mean and Covariance}\label{alg:parallel_scan}
\begin{algorithmic}[1]
\STATE \textbf{Input. } Given time stamps $\mathcal{T} = \{t_1, t_2, \ldots, t_K\}$, initial mean $\mu_{t_0}$ and covariance $\Sigma_{t_0}$, control policies $\{\mathbf{\alpha}_{t_1}, \mathbf{\alpha}_{t_2}, \ldots, \mathbf{\alpha}_{t_K}\}$, matrices $\{\mathbf{\Lambda}_{t_1}, \mathbf{\Lambda}_{t_2}, \ldots, \mathbf{\Lambda}_{t_K}\}$.

    
\STATE \textbf{Initialize} sequences $\{\mathbf{M}_i\}_{i=1}^K$ and $\{\mathbf{S}_i\}_{i=1}^K$:
    \STATE \textbf{for} $i = 1$ to $K$ \textbf{do in parallel}
        \STATE \hspace{2mm} Compute $\Delta_{i}(t_i) = t_i - t_{i-1}$.
        \STATE \hspace{2mm} Compute $\hat{\mathbf{A}}_{i} = e^{-\Delta_{i}(t_i)\mathbf{\Lambda}_{t_i}}$.
        \STATE \hspace{2mm} Compute $\hat{\mathbf{B}}_{i} = - e^{-\Delta_{i}(t_i)\mathbf{\Lambda}_{t_i}} \mathbf{\Lambda}_{t_i}^{-1} \left( \mathbf{I} - e^{-\Delta_{i}(t_i)\mathbf{\Lambda}_{t_i}} \right)$.
        \STATE \hspace{2mm} Compute $\bar{\mathbf{A}}_{i} = e^{-2\Delta_{i}(t_i)\mathbf{\Lambda}_{t_i}}$.
        \STATE \hspace{2mm} Compute $\bar{\mathbf{B}}_{i} = \frac{1}{2} e^{-2\Delta_{i}(t_i)\mathbf{\Lambda}_{t_i}} \mathbf{\Lambda}_{t_i}^{-1} \left( \mathbf{I} - e^{-2\Delta_{i}(t_i)\mathbf{\Lambda}_{t_i}} \right)$.
        \STATE \hspace{2mm} Set $\mathbf{M}_i = \left(\hat{\mathbf{A}}_{i}, \hat{\mathbf{B}}_{i} \mathbf{\alpha}_{t_i}\right)$.
        \STATE \hspace{2mm} Set $\mathbf{S}_i = \left(\bar{\mathbf{A}}_{i}, \bar{\mathbf{B}}_{i}\right)$.
    \STATE \textbf{end for}

    \STATE Parallel Scan $\{\mathbf{M}'_i\}_{i=1}^K = \texttt{ParallelScan}(\{\mathbf{M}_i\}_{i=1}^K, \otimes)$

    \STATE Parallel Scan $\{\mathbf{S}'_i\}_{i=1}^K = \texttt{ParallelScan}(\{\mathbf{S}_i\}_{i=1}^K, \otimes)$
    \STATE \textbf{for} $i = 1$ to $K$ \textbf{do in parallel}
        \STATE \hspace{2mm} $\mu_{t_i} = {\mathbf{M}'}_i^{(1)} \mu_{t_0} + {\mathbf{M}'_i}^{(2)}$
        \STATE \hspace{2mm} $\Sigma_{t_i} = {\mathbf{S}'}_i^{(1)} \Sigma_{t_0} + {\mathbf{S}'_i}^{(2)}$
    \STATE \textbf{end for}
\STATE \textbf{Return} $\mu_{t \in \mathcal{T}}$, $\Sigma_{t \in \mathcal{T}}$
\end{algorithmic}
\end{algorithm}
\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
\begin{algorithm}[H]
\caption{\texttt{ParallelScan}}\label{alg:parallel_scan_procedure}
\begin{algorithmic}[1]
\STATE \textbf{Input. } Sequence of tuples $\{\mathbf{T}_1, \mathbf{T}_2, \ldots, \mathbf{T}_K\}$, associative operator $\otimes$.
    \STATE \textbf{Stage 1: Up-Sweep (Reduce)}.
    \FOR{$d = 0$ to $\lceil \log_2 K \rceil - 1$}
        \FOR{each subtree of height $d$ in parallel}
            \STATE Let $i = 2^{d+1}k + 2^{d+1} - 1$ for $k = 0, 1, \ldots$
            \IF{$i < K$}
                \STATE $\mathbf{T}_i = \mathbf{T}_{i - 2^d} \otimes \mathbf{T}_i$
            \ENDIF
        \ENDFOR
    \ENDFOR
    
    \STATE \textbf{Stage 2: Down-Sweep}.
    \STATE $\mathbf{T}_K = \mathbf{I}$, where $\mathbf{I}$ is the identity element for $\otimes$.
    \FOR{$d = \lceil \log_2 K \rceil - 1$ downto $0$}
        \FOR{each subtree of height $d$ in parallel}
            \STATE Let $i = 2^{d+1}k + 2^{d+1} - 1$ for $k = 0, 1, \ldots$
            \IF{$i < K$}
                \STATE $\mathbf{T}_{i - 2^d} = \mathbf{T}_{i - 2^d} \otimes \mathbf{T}_i$
                \STATE $\mathbf{T}_i = \mathbf{T}_{i - 2^d}$
            \ENDIF
        \ENDFOR
    \ENDFOR
    
    \STATE \textbf{Return}  Scanned sequence $\{\mathbf{T}'_1, \mathbf{T}'_2, \ldots, \mathbf{T}'_K\}$ where $\mathbf{T}'_i = \mathbf{T}_1 \otimes \mathbf{T}_2 \otimes \cdots \otimes \mathbf{T}_i$.
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure}

\section{Experimental Details}
\label{sec:app:details}

\subsection{Data Preprocessing}
\label{sec:app:data_preprocessing}


\BF{Preprocessing Pipeline. }
The preprocessing pipeline for the fMRI data involved several standard steps, including skull-stripping, slice-timing correction, motion correction, non-linear registration, and intensity normalization. All data were aligned to the Montreal Neurological Institute (MNI) standard space for consistency. A whole-brain mask was applied to exclude non-brain tissues, such as the skull, from further analysis. The fMRI data were parcellated into 450 regions of interest (ROIs), comprising 400 cortical parcels based on the Schaefer-400 atlas~\cite{10.1093/cercor/bhx179} and 50 subcortical parcels defined by Tian’s Scale III atlas~\cite{Tian2020}. The mean fMRI time-series for each ROI was extracted across all timepoints. To ensure magnetization equilibrium and minimize T1-relaxation effects, scanner instability, and initial participant adaptation, the first 10 volumes of each fMRI time-series were discarded.

\BF{Data Normalization. }
To ensure comparability across participants and reduce inter-subject variability, we applied a two-step normalization process to the fMRI data. First, participant-wise zero-mean centering was performed by subtracting the mean signal from each ROI within each subject. Second, a robust scaling procedure was applied, where the median signal was subtracted, and the resulting values were divided by the interquartile range (IQR), computed across all participants for each ROI. This normalization scheme follows the preprocessing protocols described in BrainJEPA~\citep{dong2024brain} and BrainLM~\citep{caro2024brainlm}, ensuring a fair comparison. After normalization, each fMRI sample was represented as a matrix of size $T \times N$, where $T$ corresponds to the number of timesteps and $N$ corresponds to the number of ROIs ($N = 450$). 


\label{sec:dataset_specific}

\paragraph{UK Biobank (UKB)}
\label{sec:ukb}
The UKB is a population-based prospective study comprising 500,000 participants in the United Kingdom, designed to investigate the genetic and environmental determinants of disease \cite{sudlow2015uk}. This study utilized 41,072 rs-fMRI scans from the publicly available, preprocessed UKB dataset \cite{alfaro2018image}. The preprocessing pipeline included non-linear registration to MNI space using FSL’s $\texttt{applywarp}$ function, thereby ensuring standardized spatial alignment across participants \cite{jenkinson2012fsl}.

\paragraph{Human Connectome Project in Aging (HCP-A)}
\label{sec:hcpa}
The HCP-A is a large-scale neuroimaging initiative focused on characterizing structural and functional connectivity changes associated with aging across a wide age range \cite{bookheimer2019lifespan}. This study accessed 724 rs-fMRI samples from healthy individuals between 36 and 89 years of age. Preprocessed rs-fMRI volumes provided from the HCP-A dataset were utilized for subsequent analyses.

\paragraph{Autism Brain Imaging Data Exchange (ABIDE)}
\label{sec:abide}
The ABIDE consortium aims to elucidate the neural mechanisms underlying autism spectrum disorder \cite{di2014autism}. In the present work, 1,102 rs-fMRI samples were obtained from the Neuro Bureau Preprocessing Initiative \cite{craddock2013neuro}, which employs the Configurable Pipeline for the Analysis of Connectomes (C-PAC) \cite{craddock2013towards}. The preprocessing steps included slice-timing correction, motion realignment, intensity normalization (with a 4D global mean set to 1000), and nuisance signal removal. Nuisance regression involved a 24-parameter motion model, component-based noise correction (CompCor) \cite{behzadi2007component} with five principal components derived from white matter and cerebrospinal fluid signals, and linear/quadratic trend removal. Functional-to-anatomical registration was performed via a boundary-based rigid-body approach, while anatomical-to-standard registration utilized ANTs. Band-pass filtering and global signal regression were not applied.


\paragraph{Attention Deficit Hyperactivity Disorder 200 (ADHD200)}
\label{sec:adhd200}
The ADHD200 dataset comprises 776 rs-fMRI and anatomical scans collected from individuals aged 7 to 21, including 491 typically developing individuals and 285 participants diagnosed with ADHD \cite{brown2012adhd}. A total of 669 rs-fMRI datasets were selected for this study, specifically the preprocessed versions provided by the Neuro Bureau Preprocessing Initiative (Athena Pipeline) \cite{bellec2017neuro}.

\paragraph{Human Connectome Project for Early Psychosis (HCP-EP)}
\label{sec:hcpep}
The HCP-EP is a neuroimaging initiative focused on understanding early psychosis, defined as the first five years following symptom onset, in individuals aged 16–35. The cohort includes participants with affective psychosis, non-affective psychosis, and healthy controls \cite{jacobs2024introduction, Prunier2021-ao}. For this study, 176 rs-fMRI scans were analyzed. Preprocessing was conducted using fMRIPrep \cite{esteban2019fmriprep}, followed by denoising with Nilearn \cite{Nilearn}. The denoising process employed a 24-parameter motion model (including translations, rotations, their derivatives, and quadratic terms) and CompCor-derived components extracted from white matter and cerebrospinal fluid masks. Additionally, all confound variables were demeaned to ensure consistency across participants.
\paragraph{Transdiagnostic Connectome Project (TCP)}
\label{sec:tcp}
The TCP investigates neural mechanisms underlying psychiatric conditions across traditional diagnostic boundaries \cite{chopra2024transdiagnostic}. This study included rs-fMRI data from 236 participants aged 18 to 70, consisting of 144 individuals with diverse psychiatric diagnoses and 92 healthy controls \cite{ds005237:1.0.0}. The same harmonized preprocessing and denoising pipelines, as utilized for the HCP-EP data, were applied to all TCP scans using fMRIPrep and Nilearn.



\begin{table}[ht!]
\caption{Pre-training hyper-parameters}
\vspace{-2mm}
\label{table:pretraining_hyperparameters}
\scriptsize
\centering
\begin{tabular}{c|ccccccccc}
\toprule
\textbf{BDO Variants} & \textbf{Train EP} & \textbf{Warm-up EP} & \textbf{LR} &\textbf{Initial LR} &\textbf{Minimum LR} & \textbf{Batch Size} & \textbf{$\bbR^d$} & \textbf{$\#$ of base matrices} (L)  & \textbf{EMA Momentum} \\ 
\midrule
BDO (5M) & 200 & 10 & 0.001 & 0.0001 & 0.0001 & 128 & 192 & 100 & [0.996, 1] \\
BDO (21M) & 200 & 10 & 0.001 & 0.0001 & 0.0001 & 128 & 384 & 100 & [0.996, 1] \\
BDO (85M) & 200 & 10 & 0.001 & 0.0001 & 0.0001 & 128 & 768 & 100 & [0.996, 1] \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\end{table}

\subsection{Pre-training Stage}






\BF{Pre-training Data. }
For self-supervised pre-training, we utilized the large-scale UKB dataset, which comprises resting-state fMRI recordings and medical records from 41,072 participants~\citep{alfaro2018image}. We utilized 80\% of the dataset for pre-training, while the remaining 20\% held-out data was reserved for downstream evaluation. We used a fixed random seed (42) to ensure reproducibility when partitioning the UKB dataset into pre-training and held-out subsets. All experiments, including the reproduction of foundation model baselines, were conducted using the same dataset split to maintain consistency. 

\BF{Irregular Multivariate Time-Series Sampling. }  
We introduce irregularity in the time-series data by subsampling both the observation timestamps \(\mathcal{T}_{\text{obs}}\) and the corresponding fMRI signals \(\mathcal{Y}_{\text{obs}}\). Unlike conventional approaches that assume uniformly spaced time points~\citep{caro2024brainlm, dong2024brain}, we select a uniformly sampled subset of timestamps from the full sequence, ensuring that only a fraction of the fMRI signal is observed. Specifically, from each full-length fMRI recording, we randomly sample 160 timesteps ($T = 160$), introducing variability in temporal resolution across different samples. This choice reflects the fundamental nature of brain dynamics, which evolve continuously rather than discretely, and encourages the model to infer missing states from incomplete sequences. 



\BF{Temporal Masking. }
To encourage robust representation learning and improve generalization, we employ \textit{temporal masking}, where a subset of the 160 sampled time points is randomly masked during training. We apply a masking ratio of \(\gamma = 0.75\), meaning that 75\% of the sampled timesteps are hidden while the model is trained to reconstruct them. In~\cref{fig:ablation}, we vary $\gamma$ across $[0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9]$ to examine the effect of masking ratio in learning robust representations. Actual reconstruction results are provided in the internal and external datasets as visulized in~\cref{fig:ukb_reconstruction,fig:hcpa_reconstruction}.

\BF{Pre-training Algorithm. } The pre-training of BDO follows the procedure outlined in Algorithm~\ref{algorithm:pretrain}. Given an observed fMRI time-series $\mathcal{Y}_{\text{obs}}$, we employ a masked reconstruction strategy, where a random proportion $\gamma$ of the temporal signals is masked to encourage the model to learn meaningful representations. The pre-training objective leverages amortized inference to approximate latent dynamics while enforcing spatio-temporal consistency through structured latent representations. At each iteration, a subset of observed time-series $\mathcal{Y}_{\text{ctx}}$ is used as context, while the masked portion $\mathcal{Y}_{\text{tar}}$ serves as the target for reconstruction. The encoder network $\bT_{\theta}$ maps the context data to a sequence of latent states $\bz_{t \in \mathcal{T}_{\text{ctx}}}$, which are then used to estimate drift terms and control policies, forming the basis for latent trajectory prediction. The decoder network $\bD_{\psi}$ reconstructs the missing target states, optimizing a training objective $\mathcal{L}(\theta, \psi)$ that aligns the predicted and true trajectories. 

\BF{Pre-training Details. } 
We trained BDO using a batch size of 128 and a total of 200 pre-training epochs. The learning rate was scheduled using a cosine decay scheduler~\citep{loshchilov2016sgdr} with a 10-epoch warm-up phase. During warm-up, the initial learning rate was set to $0.0001$, which increased to a peak learning rate of $0.001$ before gradually decaying to a minimum learning rate of $0.0001$. For optimization, we employed the Adam optimizer~\citep{diederik2014adam}. Across all BDO configurations, we used a fixed number of basis $l=100$ and consistently multiplied a time scale parameter of 0.1 to observation times for all datasets. To update $\bar{\theta}$, Exponential Moving Average (EMA) momentum is used and linearly increased from $0.996$ to $1.0$. It is worth noting that our models required minimal hyperparameter tuning, which demonstrates that the proposed approximation scheme operates stably and that our method functions robustly.

\textbf{Model architecture of BDO.}
To maintain the structural advantages of our SSM-based formulation, we designed our encoder network architecture in a straightforward manner. In this regard, the networks used for pre-training BDO is listed in below, where \texttt{N=450} is the number of ROIs and \texttt{d} is the dimension of latent space $\bbR^d$ as described in~\cref{table:pretraining_hyperparameters} for each models.
\vspace{-3mm}
\begin{itemize}
    \item \textbf{Encoder network} $q_{\theta}$: \\
    \texttt{Input($N$) $\to$ Linear(d) $\to$ ReLU() $\to$ LayerNorm(d) $\to$ Linear(d) $\to$ ReLU() $\to$ LayerNorm(d) $\to$ 12 $\times$ [LayerNorm(d) $\to$ Attn(d) $\to$ FFN(d)]}
    \item \textbf{FFN}: 
    \\
    \texttt{Input(d) $\to$ LayerNorm(d) $\to$ Linear(4 $\times$ d) $\to$ GeLU() $\to$ Linear(d) $\to$ Residual(Input(d))}
    \item \textbf{Attn}: 
    \\
    \texttt{Input(Q, K, V) $\to$ Normalize(Q) $\to$ Linear(Q) $\to$ Linear(K) $\to$ Linear(V) $\to$ Attention(Q, K) $\to$ Softmax(d) $\to$ Dropout() $\to$ Matmul(V) $\to$ LayerNorm(d) $\to$ Linear(d) $\to$ Residual(Q)}
    \item \textbf{Decoder network} $\bD_{\psi}$: \\
    \texttt{Input(d) $\to$ Linear(N) $\to$ ReLU() $\to$ Dropout() $\to$ Linear(d)}
\end{itemize}


\subsection{Source of Efficiency}\label{sec:source_efficiency} 
The primary source of the efficiency of BDO stems from our SSM formulation. By introducing a strong inductive bias tailored to the inherent characteristics of fMRI time-series data such as existing complex temporal relationships, we can efficiently model brain dynamics with significantly fewer parameters as demonstrated in~\cref{fig:scalability_gpu}. Compared to our method, fully data-driven approaches like BrainLM and BrainJEPA may lack an efficient mechanism to capture temporal dependencies, necessitating a larger number of parameters to learn these relationships~\citep{caro2024brainlm, dong2024brain}.

A primary distinguishing feature of our approach is the method by which we process fMRI signals within our transformer architecture. Although our model employs the same Vision Transformer backbone~\citep{alexey2020image}\footnote{\url{https://github.com/google-research/vision_transformer}, licensed under Apache 2.0.} to ensure consistency with other foundational models~\citep{caro2024brainlm, dong2024brain}, our structural design effectively mitigates the inefficiencies commonly found in previous methods. Specifically, existing methods reshape fMRI data into image-like patches, transforming its structure from $(\texttt{K}, \texttt{d})$-observation length \texttt{k} and latent dimension \texttt{d}-to $(\texttt{K} // \texttt{W} \times \texttt{d}, \texttt{W})$, where $\texttt{W}$ represents the window size. This transformation artificially inflates the effective sequence length to $(\texttt{K} // \texttt{W} \times \texttt{d})$, leading to a computational complexity of $\mathcal{O}((\texttt{K} // \texttt{W} \times \texttt{d})^2 \texttt{W})$. In contrast, our approach retains the data in its original $(\texttt{K}, \texttt{d})$ format, preserving the natural temporal structure and reducing computational complexity to $\mathcal{O}(\texttt{K}^2 \texttt{d})$. This complexity is sufficient for our model to capture temporal dynamics effectively due to the structured state-space model (SSM) formulation, which inherently models long-range dependencies without requiring excessive parameterization.

Furthermore, by efficiently modeling temporal relationships, our approach eliminates the need for additional structural transformations. Unlike other methods that rely on ROI embedding vectors and process fMRI data in a transformed format—typically $(\texttt{K} // \texttt{W} \times \texttt{d}, \texttt{W})$—our model operates directly on $(\texttt{K}, \texttt{d})$, leveraging a stack of self-attention layers efficiently. This not only simplifies the processing pipeline but also avoids the extra computational overhead introduced by artificial segmentation.

Thus, we believe that our SSM-based approach provides a more efficient and scalable framework for brain dynamics modeling, offering significant advantages in both computational cost and representational power.




\begin{figure}[!t]
\begin{minipage}[t]{0.51\textwidth}
\begin{algorithm}[H]
\caption{Pre-training BDO}
\begin{algorithmic}[1]
    \STATE \textbf{Input. } Time-series $\mathcal{Y}_{\text{obs}} = \by_{t \in \mathcal{T}_{\text{obs}}}$, masking ratio $\gamma$, encoder network $\bT_{\theta}$, decoder network $\bD_{\psi}$
    \FOR{$m=1, \cdots, M$}
        \STATE Get $\mathcal{Y}_{\text{ctx}}, \mathcal{Y}_{\text{tar}}$ by masking $\gamma \%$ of temporal signals.
        \STATE Sample $\bz_{t \in \mathcal{T}_{\text{ctx}}} \sim \prod_{t \in \mathcal{T}_{\text{ctx}}} q_{\theta}(\bz_t | \mathcal{Y}_{\text{ctx}})$ using~\eqref{eq:encoder network}
        \STATE Compute $\{\bD_t, u_t, \alpha^{\theta}_t\}_{t \in \mathcal{T}_{\text{ctx}}}$ using~\eqref{eq:drift approximation}
        \STATE Estimate $\{\mu_t, \Sigma_t\}_{t \in \mathcal{T}_{\text{tar}}}$ with parallel scan algorithm.
        \STATE Sample $\bX^{\theta}_{t \in \mathcal{T}_{\text{tar}}} \stackrel{i.i.d}{\sim} \otimes_{t \in \mathcal{T}_{\text{tar}}} \mathcal{N}(\mu_t, \Sigma_t)$.
        \STATE Sample $\hat{\bz}_{t \in \mathcal{T}_{\text{tar}}} \sim \prod_{t \in \mathcal{T}_{\text{tar}}} p(\hat{\bz}_t | \bX^{\theta}_t)$
        \STATE Compute $\hat{\mathcal{L}}(\theta, \psi)$ using~\eqref{eq:rescaled training objective}
        \STATE Update $(\theta, \psi)$ with $\nabla_{\theta, \psi} \hat{\mathcal{L}}(\theta, \psi)$
        \STATE Apply $\bar{\theta} \leftarrow \texttt{EMA}(\theta)$
    \ENDFOR
\end{algorithmic}\label{algorithm:pretrain}
\end{algorithm}
\end{minipage}
\begin{minipage}[t]{0.47\textwidth}
\begin{algorithm}[H]
\caption{Fine tuning BDO for downstream tasks}
\begin{algorithmic}[1]
    \STATE \textbf{Input. } Time-series and label $(\mathcal{Y}_{\text{obs}}, \mathcal{O}_{\text{obs}})$, pre-trained encoder network $\bT_{\theta^{\star}}$.
        \STATE Sample $\bz_{t \in \mathcal{T}_{\text{obs}}} \sim \prod_{t \in \mathcal{T}_{\text{obs}}} q_{\theta^{\star}}(\bz_t | \mathcal{Y}_{\text{obs}})$ 
        using~\eqref{eq:encoder network}
        \STATE Compute optimal control policy $\alpha_{t \in \mathcal{T}_{\text{obs}}} = \bB_{\theta^{\star}} \bz_{t \in \mathcal{T}_{\text{obs}}}$
        \STATE Compute the universal feature $\bbA = \frac{1}{|\mathcal{T}_{\text{obs}}|}\sum_{t \in \mathcal{T}_{\text{obs}}} \alpha_t$
        \STATE Predict $\hat{\mathcal{O}}_{\text{obs}} = h_{\zeta}(\bbA)$        
        \IF{\textit{Linear probing}}
            \STATE Freeze the pre-trained encoder network $\bT_{\theta^{\star}}$
            \STATE Compute $\mathcal{L}(\theta^{\star}, \zeta) = \mathcal{L}_{\text{task}}(\mathcal{O}_{\text{obs}}, \hat{\mathcal{O}}_{\text{obs}})$ using~\eqref{eq:downstream_task_loss}
            \STATE Update $\zeta$ with $\nabla_{\zeta} \mathcal{L}(\theta^{\star}, \zeta)$
        \ELSIF{\textit{Fine tuning}}
            \STATE Unfreeze the pre-trained encoder network $\bT_{\theta^{\star}}$
            \STATE Compute $\mathcal{L}(\theta^{\star}, \zeta) = \mathcal{L}_{\text{task}}(\mathcal{O}_{\text{obs}}, \hat{\mathcal{O}}_{\text{obs}})$ using~\eqref{eq:downstream_task_loss}
            \STATE Update ($\theta^{\star}, \zeta$) with $\nabla_{\theta^{\star}, \zeta} \mathcal{L}(\theta^{\star}, \zeta)$
        \ENDIF
\end{algorithmic}\label{algorithm:downstream}
\end{algorithm}
\vspace{-8mm}
\end{minipage}
\end{figure}

\subsection{Downstream Evaluation Stage}
To assess the generalization and transferability of BDO, we conducted experiments across multiple datasets and tasks, encompassing both demographic and psychiatric prediction. Datasets used in this evaluation have distinct temporal resolutions and varying numbers of timesteps, reflecting the irregularity of real-world fMRI data acquisition. Additional details are described in~\cref{table:participant_demographics}. Note that in the downstream evaluation, irregular sampling and temporal masking were disabled. The full sequence of fMRI signals, timestamps, and corresponding labels were used, denoted as $(\mathcal{Y}_{\text{obs}}, \mathcal{T}_{\text{obs}}, \mathcal{O}_{\text{obs}})$.


\begin{table}[t!]
\caption{Dataset Subject Demographics}
\vspace{-2mm}
\label{table:participant_demographics}
\scriptsize
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|cccccc}
\toprule
\textbf{Category} & \textbf{UKB} & \textbf{HCP-A} & \textbf{ABIDE} & \textbf{ADHD200} & \textbf{HCP-EP} & \textbf{TCP} \\
\midrule
\# of subjects & 41,072 & 724 & 1,102 & 669 & 176 & 236 \\
Age, mean (SD) & 54.98 (7.53) & 60.35 (15.74) & 17.05 (8.04) & 11.61 (2.97) & 23.39 (3.95) & 33.96 (13.13) \\
Female, \% (n) & 52.30 (21,480) & 56.08 (406) & 14.79 (163) & 36.17 (242) & 38.07 (67) & 56.78 (134) \\
Patient, \% (n) & - & - & 48.19 (531) & 58.15 (389) & 68.18 (120) & 61.02 (144) \\
Target Population & Healthy Population & Healthy Population & \begin{tabular}{c}ASD\\ Healthy Population\end{tabular} & \begin{tabular}{c}ADHD\\ Healthy Population\end{tabular} & \begin{tabular}{c}Psychotic Disorder\\ Healthy Population\end{tabular} & \begin{tabular}{c}Psychiatric Disorders \\ Healthy Population\end{tabular} \\
\bottomrule
\end{tabular}
} %
\vspace{-2mm}
\end{table}


\textbf{Internal Evaluation.} For \textit{internal evaluation}, we utilized a 20\% held-out subset of the UKB dataset, which was excluded from pre-training. This evaluation focused on age regression and gender classification, leveraging both LP and FT to analyze how well the model retains and transfers knowledge acquired during pre-training.

\textbf{External Evaluation.} For \textit{external evaluation}, we examined the ability of BDO to generalize to unseen datasets. Demographic and trait prediction was performed on the HCP-A dataset, where LP and FT were employed to assess model performance on age, gender, neuroticism, and flanker scores. Beyond demographic characteristics, we evaluated psychiatric diagnosis classification using 4 clinical fMRI datasets, including ABIDE, ADHD200, HCP-EP, and TCP. These evaluations relied on LP, as it provides a controlled assessment of the learned representations and their applicability to clinical classification tasks.

\BF{Random Splits. }
All the datasets are partitioned into training, validation, and test sets using a 6:2:2 ratio to ensure fair and reproducible evaluation. To maintain consistency, we perform partitioning with 3 consecutive random seeds, 0, 1, and 2.
\begin{itemize}[leftmargin=10pt]
\item For classification tasks, such as gender classification, stratified sampling is applied to preserve class distributions across the training, validation, and test sets. 
\item For regression tasks, such as age regression, binning-based stratified sampling is employed. In this approach, the continuous target variable is first discretized into bins before applying stratified sampling, ensuring a balanced distribution of the target variable and mitigating potential biases from uneven data partitioning. Additionally, to improve numerical stability and facilitate optimization, the target variable is normalized using Z-score normalization, where the mean is subtracted, and the result is divided by the standard deviation.
\item The distributions of the three random splits for age regression tasks with the UKB and HCP-A datasets, and six classification tasks with UKB gender, HCP-A gender, ABIDE diagnosis, ADHD200 diagnosis, HCP-EP diagnosis, and TCP diagnosis are described in Figure~\ref{fig:split_distribution_ukb}$-$\ref{fig:split_distribution_cls}.
\end{itemize}

\BF{Extracting the Universal Feature $\bbA$. } To extract the \textit{universal feature} $\bbA$, we define $f$ as \textit{mean-pooling} over the sequence of control signals $\alpha_{t \in \mathcal{T}}$, given by $\bbA := f(\alpha_{t \in \mathcal{T}}) = \frac{1}{|\mathcal{T}|} \sum_{t \in \mathcal{T}} \alpha_t$. This formulation ensures that $\bbA$ serves as a compact and transferable representation of the underlying spatio-temporal dynamics captured by the optimal control signals. To enhance biological interpretability, mean-pooling is chosen as it provides a \textit{global summary} of the temporal evolution of the control sequence while suppressing high-frequency fluctuations that may arise due to local variations in $\alpha_t$. Although we believe that mean-pooling provides a robust and scalable approach for summarizing temporal dynamics, we acknowledge that more sophisticated aggregation methods, such as weighted pooling or recurrent architectures, could further enhance downstream performance. These approaches may offer additional advantages for analyzing temporal dynamics, such as facilitating interpretability through attention weight analysis or capturing long-range dependencies. We leave the exploration of these advanced aggregation strategies for future work.

\BF{Downstream Evaluation Algorithm. } To evaluate the effectiveness of BDO on downstream tasks, we follow the procedure outlined in Algorithm~\ref{algorithm:downstream}. Given an observed fMRI time-series $\mathcal{Y}_{\text{obs}}$ and its corresponding labels $\mathcal{O}_{\text{obs}}$, we extract the universal feature representation $\bbA$ using the pre-trained encoder $\bT_{\theta^{\star}}$. This representation is subsequently used for classification or regression tasks through either LP or FT.
\begin{itemize}[leftmargin=10pt]
\item In LP setting, we freeze the pre-trained encoder $\bT_{\theta^{\star}}$ and train only the task-specific head $h_{\zeta} : \bbR^d \to \bbR^N$ (single linear layer). The objective function $\mathcal{L}(\theta^{\star}, \zeta)$ measures the discrepancy between the predicted $\hat{\mathcal{O}}_{\text{obs}}$ and ground-truth $\mathcal{O}_{\text{obs}}$, and is optimized with respect to $\zeta$.
\item In FT setting, the entire model, including $\bT_{\theta^{\star}}$, is optimized. Both the encoder and task-specific head $h_{\zeta}$ (single linear layer) are updated jointly to refine the feature extraction process for the target task.
\end{itemize}
\textbf{Training Objective for Downstream tasks. } The loss function for downstream tasks is defined based on the nature of the prediction problem: classification tasks use Binary Cross-Entropy (BCE) loss to measure the discrepancy between predicted and true class probabilities, while regression tasks employ Mean Squared Error (MSE) loss to minimize the squared differences between predicted and actual values.

\textbf{Model Selection. }  
To determine the optimal model for each downstream task, we performed a grid search over key hyperparameters such as learning rate and batch size. For each task, we evaluated multiple configurations using the validation set and selected the model that achieved the best performance based on the predefined evaluation metric. The complete set of hyperparameters is provided in~\cref{tab:hyperparam_grid}.



\begin{equation}
\mathcal{L}_{\text{task}}(\mathcal{O}_{\text{obs}}, \hat{\mathcal{O}}_{\text{obs}}) =
\begin{cases}
    -\frac{1}{N} \sum_{i=1}^{N} \left[ \mathcal{O}_{\text{obs}, i} \log \hat{\mathcal{O}}_{\text{obs}, i} + (1 - \mathcal{O}_{\text{obs}, i}) \log (1 - \hat{\mathcal{O}}_{\text{obs}, i}) \right], & \text{if classification} \\
    \frac{1}{N} \sum_{i=1}^{N} (\mathcal{O}_{\text{obs}, i} - \hat{\mathcal{O}}_{\text{obs}, i})^2, & \text{if regression}
\end{cases}
\label{eq:downstream_task_loss}
\end{equation}



\begin{table}[ht]
    \centering
    \caption{Search space of end-to-end fine-tuning (FT) and linear probe (LP).}\label{tab:hyperparam_grid}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Configurations} & \textbf{FT} & \textbf{LP} \\
        \midrule
        Optimizer & AdamW~\citep{loshchilov2017decoupled} & Adam~\citep{diederik2014adam} \\
        Training epochs & $50$ & $50$ \\
        Batch size & $[16, 32]$ & $[16, 32, 64]$ \\
        LR scheduler & cosine decay & cosine decay \\
        LR & $[0.001]$ & $[0.01, 0.005]$ \\
        Minimum LR & $[0, 0.0001, 0.001]$ & $[0.001, 0.005]$ \\
        Weight decay & $[0, 0.01]$ & $[0]$ \\
        Layer-wise LR decay & $[0.85, 0.90, 0.95]$ & N.A. \\
        \bottomrule
    \end{tabular}
\end{table}


\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\textwidth,]{figure/split_distribution_combined_ukb.pdf}
\caption{Age distribution across training, validation, and test splits for the UKB held-out age regression task under three different random seeds (0, 1, and 2). The dataset is partitioned using a 6:2:2 ratio, with binning-based stratified sampling applied to maintain a balanced target variable distribution. To enhance numerical stability, Z-score normalization is applied to the age variable. Each row represents a different random seed, illustrating the consistency of the sampling procedure across splits.}\label{fig:split_distribution_ukb}
\end{figure*}

\newpage
\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\textwidth,]{figure/split_distribution_combined.pdf}
\caption{Age distribution across training, validation, and test splits for the HCP-A age regression task under three different random seeds (0, 1, and 2). The dataset is partitioned using a 6:2:2 ratio, with binning-based stratified sampling applied to maintain a balanced target variable distribution. To enhance numerical stability, Z-score normalization is applied to the age variable. Each row represents a different random seed, illustrating the consistency of the sampling procedure across splits.}\label{fig:split_distribution_hcp}
\end{figure*}

\newpage
\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\textwidth,]{figure/all_tasks_distribution.pdf}

\caption{Label distributions across six classification tasks (UKB held-out gender, HCP-A gender, ABIDE autism, ADHD200 ADHD, HCP-EP psychotic disorder, and TCP patient) for training, validation, and test splits. Each row corresponds to a different task, with columns representing the proportion of samples per class across data splits. Stratified sampling ensures that label distributions remain consistent across splits, despite variations in sample composition. To illustrate this, we visualize the distributions using a single random seed (0). Gender classification tasks are divided into Female/Male categories, while disease classification tasks distinguish between Control and Patient groups (ASD vs. Control for ABIDE, ADHD vs. Control for ADHD200, Psychotic disorder vs. Control for HCP-EP, and GenPop vs. Patient for TCP).}\label{fig:split_distribution_cls}
\end{figure*}


\newpage
\begin{figure*}[ht]
\centering
\includegraphics[width=0.99\textwidth,]{figure/ukb.png}
    \caption{Reconstruction quality of BDO in the UKB held-out subset (internal dataset). Five samples are randomly drawn for visualization, with a mask ratio of $\gamma = 0.75$. Each column represents the original fMRI sample, context with masking patterns, reconstructed sample, and MAE (Mean Absolute Error) heatmaps. Although we set the mask ratio as high as $75\%$, the reconstruction quality remains robust, demonstrating that BDO efficiently captures the underlying brain dynamics and successfully reconstructs missing regions with high fidelity.}\label{fig:ukb_reconstruction}
\end{figure*}

\newpage
\begin{figure*}[ht]
\centering
\includegraphics[width=0.99\textwidth,]{figure/hcpa.png}
\caption{Reconstruction quality of BDO in HCP-A (external dataset). Five samples are randomly drawn for visualization, with a mask ratio of $\gamma = 0.75$. Each column represents the original fMRI sample, context with masking patterns, reconstructed sample, and MAE (Mean Absolute Error) heatmaps. Although we set the mask ratio as high as $75\%$, the reconstruction quality remains robust, demonstrating that BDO efficiently captures the underlying brain dynamics and successfully reconstructs missing regions with high fidelity.}\label{fig:hcpa_reconstruction}
\end{figure*}




