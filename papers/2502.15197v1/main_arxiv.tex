% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}


% Latex Packages
\input{packages}


% Loading notations
\input{notations}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\title{\includegraphics[width=0.8\baselineskip]{figures/tetris.png} \alg{}: Optimal Draft Token Selection for Batch Speculative Decoding}


\author{
 \textbf{Zhaoxuan Wu\textsuperscript{*1}},
 \textbf{Zijian Zhou\textsuperscript{*1,2}},
 \textbf{Arun Verma\textsuperscript{1}},
 \textbf{Alok Prakash\textsuperscript{1}},
\\
 \textbf{Daniela Rus\textsuperscript{1,3}},
 \textbf{Bryan Kian Hsiang Low\textsuperscript{1,2}}
\\
\\
 \textsuperscript{1}Singapore-MIT Alliance for Research and Technology, Republic of Singapore
 \\
 \textsuperscript{2}Dept. of Computer Science, National University of Singapore, Republic of Singapore \\
 \textsuperscript{3}CSAIL, Massachusetts Institute of Technology, USA
\\
 % \small{
 %   \textbf{Correspondence:} \href{mailto:lowkh@comp.nus.edu.sg}{lowkh@comp.nus.edu.sg}
 % }
}

\begin{document}
    \makeatletter
    \def\blankfootnote{\xdef\@thefnmark{}\@footnotetext}
    \makeatother
    
    \maketitle
    
    \begin{abstract}
        We propose \alg{}, a novel method that optimizes the {\em total throughput} of batch speculative decoding in multi-request settings. 
        Unlike existing methods that optimize for a single request or a group of requests as a whole, \alg{} actively selects the most promising draft tokens (for every request in a batch) to be accepted when verified in parallel, resulting in fewer rejected tokens and hence less wasted computing resources. Such an effective resource utilization to achieve fast inference in large language models (LLMs) is especially important to service providers with limited inference capacity. Compared to baseline speculative decoding, \alg{} yields a consistently higher acceptance rate and more effective utilization of the limited inference capacity. 
        We show theoretically and empirically that \alg{} outperforms baseline speculative decoding and existing methods that dynamically select draft tokens, leading to a more efficient batch inference in LLMs.
        
        \blankfootnote{\textbf{*} Equal contribution.}
    \end{abstract}

    
    \section{Introduction}
    \label{sec:intro}
    \input{latex_arxiv/introduction}

    
    \section{Related Work}
    \label{sec:related_work}
    \input{latex_arxiv/related_work}


    \section{Problem Setup}
    \label{sec:problem}
    \input{latex_arxiv/problem}


    \section{\alg{}: Optimal Draft Token Selection}
    \label{sec:draft_window}
    \input{latex_arxiv/draft_window}


    \subsection{Our Approach and Design}
    \label{sec:tetris}
    \input{latex_arxiv/tetris}


    \section{Experiments}
    \label{sec:experiment}
    \input{latex_arxiv/experiment}


    \section{Conclusion and Future Work}
    \label{sec:}
    \input{latex_arxiv/conclusion}
    
    
    \newpage
    \section{Limitations}
    \input{latex_arxiv/limitations}
    

    % References
    \bibliography{references}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \appendix
    \onecolumn
    \label{sec:appendix}
    \input{latex_arxiv/appendix}

    \hrule height 0.5mm	


\end{document}
