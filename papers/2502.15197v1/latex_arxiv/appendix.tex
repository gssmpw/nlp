%!TEX root =  main_arxiv.tex



\section{Leftover Proofs}

\subsection{Proof of \texorpdfstring{\cref{thm:tetris}}{Theorem 1}.}
\label{app:proof_of_local_tetris}

\tetris*
\begin{proof}
    We prove it by contradiction. Let the selection of \cref{alg:tetris} be $\gD^*$. Suppose the actual optimal solution is $\gD' \neq \gD^*$. Let $\tilde{\gD} = \gD' \cap \gD^*$ be the overlapping tokens selected by both \cref{alg:tetris} and the actual optimal solution. Note that the tokens in each row are selected sequentially (i.e., tokens cannot be skipped in a row). 

    \paragraph{Case 1: \alg{} selects some token $d \in \gD^* \setminus \tilde{\gD}$ before selecting $\tilde{\gD}$.} In this case, the $\mathbb{E}[\vone]$ of the token $d$ is higher than the token last selected in $\tilde{\gD}$. This suggests that the optimal selection should include $d$. However, it can be observed that $d \notin D'$ since otherwise $d \in \tilde{\gD}$. This contradicts the fact that $\gD'$ is optimal.

    \paragraph{Case 2: \alg{} selects $\tilde{\gD}$ first before selecting other tokens.} Since \cref{alg:tetris} always selects the token with the highest $\mathbb{E}[\vone]$, every element in $\gD^* \setminus \tilde{\gD}$ is larger than or equal to that in $\gD' \setminus \tilde{\gD}$. As such, we have 
    $\mathbb{E}[\sum_{p \in \gD'} \mathbf{1}_p] \leq \mathbb{E}[\sum_{p \in \gD^*} \mathbf{1}_p]$. However, this contradicts the fact that $\gD'$ is optimal as \cref{alg:tetris} has a higher number of accepted tokens. Therefore, \cref{alg:tetris} must be optimal. 
    
    Combining the two cases finishes the proof.
\end{proof}

\subsection{Running Time of \alg{}}
\label{app:lem_time_comp}

\begin{lem}
\label{lem:time_comp}
\cref{alg:tetris} achieves a time complexity of $\gO(C\log N)$.

\begin{proof}
    Note that \cref{alg:tetris} maintains a heap. The heap is initialized with $N$ items. Since only $C$ pairs are selected, there are $2C$ operations of \texttt{enqueue} and \texttt{dequeue}. Following classic results of heap operation, each \texttt{enqueue} of \texttt{dequeue} operation requires $\gO(\log C)$ time. As such, the overall time complexity of \alg{} is $\gO(C\log N)$.
\end{proof}
\end{lem}

\subsection{Proof of \texorpdfstring{\cref{thm:globalOpt}}{Theorem 2}.}
\label{app:thm_global_opt_tetris}

\globalOpt*
\begin{proof}
    The proof of global optimality is established on \cref{thm:tetris}. Since all tokens in each row have the same acceptance rate. After each step, we have the same distribution of $\vone$ no matter what tokens are accepted, where $\vone$ is the indicator variable of whether the token is accepted.
    As such, at each step, performing \alg{} is per-step optimal by \cref{thm:tetris}. Moreover, since the state at each step is identical, a per-step optimal strategy is also globally optimal.
\end{proof}


\section{Additional Related Work}

\subsection{Acceptance Rate}\label{app:acceptance-related-work}

The acceptance rate plays a vital role in the effectiveness of speculative decoding.
A higher acceptance rate should be paired with a larger draft window size $k$ to achieve optimal speedup.
In the typical rejection sampling setting of speculative decoding, the acceptance of draft tokens depends on the probability distributions of both the draft and target models.
When the probability distribution of the draft model, $p_\gS(\cdot)$, closely approximates that of the target model, $p_\gM(\cdot)$, a higher number of tokens are accepted on average.
Since the value of $k$ is chosen in the drafting process, we do not have access to $p_\gM(\cdot)$ and have to rely on $p_\gS(\cdot)$ to estimate the acceptance rate.

\citet{leviathan2023} derive that the expected acceptance rate is 1 minus the KL divergence between the token distributions of the draft and the target model.
Hence, the acceptance rates for all draft tokens are considered constant.
\citet{liu2024optimizingspeculativedecodingserving} assume uniform token acceptance behavior across diverse requests.
It proposes SmartSpec, which calculates the average acceptance rate from past generation steps.
\citet{li2024eagle2} and~\citet{wang2024opttreespeculativedecodingadaptive} utilize the draft model's confidence score (i.e., the output probability of each token) to estimate the acceptance rate.
\citet{chen2024sequoia} make the positional acceptance assumption so that the acceptance rate of tokens is determined solely by their position (i.e., number of tokens away) relative to the already accepted tokens.
\citet{agrawal2024adaedlearlydraftstopping} instead consider an approximate lower bound on the expected acceptance rate of a token that depends on the entropy of prediction probabilities of the draft model $p_\gS(\cdot)$.
Noting the acceptability of diverse tokens, especially in the real world with a high value of temperature hyperparameter, Medusa proposes to use both a hard threshold and an entropy-dependent threshold as a criterion to accept draft tokens~\citep{cai2024medusa}.
In Medusa, the first token is always accepted using greedy decoding to ensure at least one token is generated in each step.

\section{Additional Results}

\subsection{Dataset License}
ShareGPT~\citep{sharegpt-dataset}: Apache license 2.0; Arena~\citep{zheng2023arena}: CC; Domain-specific Tough Questions~\citep{yav-ai2024domain-tough}: MIT.

\subsection{Plots for End-to-end Latency}\label{app:latency-plots}

\begin{figure*}[t]
    \centering
    \setlength{\tabcolsep}{1pt} 
    \begin{tabular}{cccc}
    & \hspace{8mm}\textbf{ShareGPT} & \hspace{8mm}\textbf{Arena} & \hspace{8mm}\textbf{Tough} \\
    \rotatebox{90}{\parbox{2.5cm}{\centering \hspace{8mm}\textbf{Setting 1}}} & \includegraphics[width=0.31\linewidth]{figures/mean_e2e_latency/mean_e2el_latency_lmsys_vicuna-33b-v1.3_sharegpt_tp4_512_max64.pdf} &
    \includegraphics[width=0.31\linewidth]{figures/mean_e2e_latency/mean_e2el_latency_lmsys_vicuna-33b-v1.3_arena_tp4_512_max64.pdf} &
    \includegraphics[width=0.31\linewidth]{figures/mean_e2e_latency/mean_e2el_latency_lmsys_vicuna-33b-v1.3_domain_tough_tp4_512_max64.pdf} \\
    & (a) \hspace{5mm} $\uparrow$ 3.42\%, $\Delta$ 6.05\% & (b) \hspace{5mm} $\uparrow$ 5.30\%, $\Delta$ 6.30\% & (c) \hspace{5mm} $\uparrow$ 5.47\%, $\Delta$ 9.32\% \\
    
    \rotatebox{90}{\parbox{2.5cm}{\centering \hspace{8mm}\textbf{Setting 2}}}  & \includegraphics[width=0.31\linewidth]{figures/mean_e2e_latency/mean_e2el_latency_meta-llama_Llama-3.1-70B-Instruct_sharegpt_tp8_512_max64.pdf} & \includegraphics[width=0.31\linewidth]{figures/mean_e2e_latency/mean_e2el_latency_meta-llama_Llama-3.1-70B-Instruct_arena_tp8_512_max64.pdf} & \includegraphics[width=0.31\linewidth]{figures/mean_e2e_latency/mean_e2el_latency_meta-llama_Llama-3.1-70B-Instruct_domain_tough_tp8_512_max64.pdf}  \\
    & (d) \hspace{5mm} $\uparrow$ 2.65\%, $\Delta$ 2.70\% & (e) \hspace{5mm} $\uparrow$ 3.86\%, $\Delta$ 3.86\% & (f) \hspace{5mm} $\uparrow$ 3.65\%, $\Delta$ 3.65\% \\
    
    \rotatebox{90}{\parbox{2.5cm}{\centering \hspace{8mm}\textbf{Setting 3}}}  & \includegraphics[width=0.31\linewidth]{figures/mean_e2e_latency/mean_e2el_latency_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_sharegpt_tp8_512_max64.pdf} & 
    \includegraphics[width=0.31\linewidth]{figures/mean_e2e_latency/mean_e2el_latency_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_arena_tp8_512_max64.pdf} &
    \includegraphics[width=0.31\linewidth]{figures/mean_e2e_latency/mean_e2el_latency_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_domain_tough_tp8_512_max64.pdf} \\
    & (g) \hspace{5mm} $\uparrow$ 3.51\%, $\Delta$ 4.52\% & (h) \hspace{5mm} $\uparrow$ 6.13\%, $\Delta$ 6.13\% & (i) \hspace{5mm} $\uparrow$ 4.49\%, $\Delta$ 4.68\% \\
\end{tabular}
    \caption{Mean end-to-end latency comparison for various methods across experimental settings. $\uparrow$ indicates the improvement from best baseline method. $\Delta$ indicates the maximum gap between \alg{} and standard SD. The reported numbers reflect the mean and standard deviation over 3 independent trials.}
    \label{fig:latency-all-appendix}
\end{figure*}

We provide an extended discussion on the improvement of end-of-end latency from~\cref{sec:evaluation}.
In~\cref{fig:latency-all-appendix}, we show the plots for the end-to-end latency over all speculative decoding configurations and settings used in the paper.
\alg{} consistently outperforms the existing baselines and achieves up to 6.13\% improvement over the best baseline and up to 9.32\% maximum gap over standard SD.
Therefore, \alg{} has demonstrated to effectively reduce end-to-end request latency, which is also essential for enhancing the user experience with LLM inference service providers.


\subsection{Plots for Projected Improvement based on TER}\label{app:projected-improvement-plots}

Complementary to~\cref{tab:potential}, which contains the numerical results for the projected improvement of \alg{} in terms of the projected throughput $\hat{\gG}^{(\textit{TER})}$, we also show the plots in~\cref{fig:projected-throughput-all-appendix} to visually illustrate the effectiveness of our method. 
The dotted lines for \alg{} (drawn in blue, orange, and green) represent the projected throughput calculated based on the throughput of the standard SD and also the \alg{}'s improvement in terms of target efficiency rate (TER, as defined in~\cref{eq:TER}).
We note that these improvement numbers are theoretically computed and are not yet realizable in empirical settings due to the lack of parallelized pipeline implementations of speculative decoding in \vllm{}.


\begin{figure}[!ht]
    \centering
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{cccc}
    & \hspace{8mm}\textbf{ShareGPT} & \hspace{8mm}\textbf{Arena} & \hspace{8mm}\textbf{Tough} \\
    \rotatebox{90}{\parbox{2.5cm}{\centering \hspace{8mm}\textbf{Setting 1}}} & \includegraphics[width=0.31\linewidth]{figures/projected_improvement/projected_improvement_throughput_lmsys_vicuna-33b-v1.3_sharegpt_tp4_512_max64.pdf} &
    \includegraphics[width=0.31\linewidth]{figures/projected_improvement/projected_improvement_throughput_lmsys_vicuna-33b-v1.3_arena_tp4_512_max64.pdf} &
    \includegraphics[width=0.31\linewidth]{figures/projected_improvement/projected_improvement_throughput_lmsys_vicuna-33b-v1.3_domain_tough_tp4_512_max64.pdf} \\
    & (a) \hspace{5mm} $\hat{\gG}^{(\textit{TER})}$$\uparrow$ 9.70\% & (b) \hspace{5mm} $\hat{\gG}^{(\textit{TER})}$$\uparrow$ 7.79\% & (c) \hspace{5mm} $\hat{\gG}^{(\textit{TER})}$$\uparrow$ 8.92\% \\
    
    \rotatebox{90}{\parbox{2.5cm}{\centering \hspace{8mm}\textbf{Setting 2}}}  & \includegraphics[width=0.31\linewidth]{figures/projected_improvement/projected_improvement_throughput_meta-llama_Llama-3.1-70B-Instruct_sharegpt_tp8_512_max64.pdf} & \includegraphics[width=0.31\linewidth]{figures/projected_improvement/projected_improvement_throughput_meta-llama_Llama-3.1-70B-Instruct_arena_tp8_512_max64.pdf} & \includegraphics[width=0.31\linewidth]{figures/projected_improvement/projected_improvement_throughput_meta-llama_Llama-3.1-70B-Instruct_domain_tough_tp8_512_max64.pdf}  \\
    & (d) \hspace{5mm} $\hat{\gG}^{(\textit{TER})}$$\uparrow$ 11.70\% & (e) \hspace{5mm} $\hat{\gG}^{(\textit{TER})}$$\uparrow$ 11.17\% & (f) \hspace{5mm} $\hat{\gG}^{(\textit{TER})}$$\uparrow$ 11.91\% \\
    
    \rotatebox{90}{\parbox{2.5cm}{\centering \hspace{8mm}\textbf{Setting 3}}}  & \includegraphics[width=0.31\linewidth]{figures/projected_improvement/projected_improvement_throughput_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_sharegpt_tp8_512_max64.pdf} & 
    \includegraphics[width=0.31\linewidth]{figures/projected_improvement/projected_improvement_throughput_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_arena_tp8_512_max64.pdf} &
    \includegraphics[width=0.31\linewidth]{figures/projected_improvement/projected_improvement_throughput_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_domain_tough_tp8_512_max64.pdf} \\
    & (g) \hspace{5mm} $\hat{\gG}^{(\textit{TER})}$$\uparrow$ 11.67\% & (h) \hspace{5mm} $\hat{\gG}^{(\textit{TER})}$$\uparrow$ 10.53\% & (i) \hspace{5mm} $\hat{\gG}^{(\textit{TER})}$$\uparrow$ 12.04\% \\
\end{tabular}
    \caption{Mean projected throughput $\hat{\gG}^{(\textit{TER})}$ comparison for various methods across experimental settings. $\uparrow$ indicates the improvement from the best baseline method. The reported numbers reflect the mean over 3 independent trials.}
    \label{fig:projected-throughput-all-appendix}
\end{figure}


\subsection{Extension to Medusa}\label{app:medusa-extension}

We evaluate the top-1 proposal version (i.e., only draft the most likely token for each position) of Medusa and its integration with \alg{}.
As the Medusa model outputs multiple subsequent tokens in a single forward pass,\footnote{We use a modified implementation of Medusa in \vllm{} to ensure a fixed forward pass time.} we leverage this feature to produce extra draft tokens for \alg.
We show the results in~\cref{tab:ablation-medusa}.
We achieved a throughput improvement of 3.19\% as compared to the baseline Medusa.
The development of such multi-token prediction models, including models like EAGLE~\citep{li2024eagle} and DeepSeek-V3~\citep{deepseekai2024deepseekv3technicalreport} presents further potential for \alg{} to achieve greater speedups. Other improvements in engineering, including using tree-decoding and using a larger target model also potentially further boost the speedup.

\begin{table}[!ht]
\centering
\caption{Mean total throughput ($\pm$ standard deviation) for the ablation study of \alg{} extension to Medusa over three independent trials. The integration of \alg{} with Medusa further improves the total throughput.}
\label{tab:ablation-medusa}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{c|cccc}
\toprule
\textbf{No. Speculative Tokens} &
\multirow{1}{*}{\textbf{\alg{} (extra=1)}} & \multicolumn{1}{c}{\textbf{\alg{} (extra=2)}} & \multicolumn{1}{c}{\textbf{\alg{} (extra=3)}} & \multicolumn{1}{c}{\textbf{Baseline Medusa}}  \\ 
\midrule
1 & 591.26$\pm$0.46 & 590.83$\pm$8.30 & 586.47$\pm$3.66 &  572.97$\pm$1.79 \\
2 & 571.05$\pm$0.80 & 568.82$\pm$6.52 & 571.95$\pm$1.06 & 563.94$\pm$2.95 \\
\midrule
\textit{Best} & 591.26 & 590.83 & 586.47 & 572.97\\
\bottomrule
\end{tabular}}
\end{table}


\subsection{Improvement in Verification Success Rate}\label{app:improve-VSR}

As an ablation study, we also illustrate the improvement of \alg{} in terms of VSR (as defined in \cref{eq:VSR}), which is an important measure of the effectiveness of speculative decoding.
We show in~\cref{fig:VSR-all-appendix} that the maximum gap between \alg{} and standard SD in terms of VSR is consistently above 20\% and reaching over 30\% in some instances.
This validates the significant effect of \alg{} in selecting draft tokens that are most likely to be accepted by the target model without exceeding the system capacity of the server.
However, it is worth noting that this improvement in VSR does not translate entirely to an increment in total throughput or a reduction in end-to-end latency.
This is because the throughput in practice also depends on the running time of the draft model (especially when the speculative decoding pipeline is sequential, as discussed in~\cref{sec:parallel-implementation}), and VSR does not account for the generation of the bonus token (which takes up a portion of the generated tokens).


\begin{figure}[!ht]
    \centering
    \setlength{\tabcolsep}{1pt} 
    \begin{tabular}{cccc}
    & \hspace{8mm}\textbf{ShareGPT} & \hspace{8mm}\textbf{Arena} & \hspace{8mm}\textbf{Tough} \\
    \rotatebox{90}{\parbox{2.5cm}{\centering \hspace{8mm}\textbf{Setting 1}}} & \includegraphics[width=0.31\linewidth]{figures/VSR/good_draft_rate_lmsys_vicuna-33b-v1.3_sharegpt_tp4_512_max64.pdf} &
    \includegraphics[width=0.31\linewidth]{figures/VSR/good_draft_rate_lmsys_vicuna-33b-v1.3_arena_tp4_512_max64.pdf} &
    \includegraphics[width=0.31\linewidth]{figures/VSR/good_draft_rate_lmsys_vicuna-33b-v1.3_domain_tough_tp4_512_max64.pdf} \\
    & (a) \hspace{5mm} $\Delta$ 30.52\% & (b) \hspace{5mm} $\Delta$ 29.98\% & (c) \hspace{5mm} $\Delta$ 33.39\% \\
    
    \rotatebox{90}{\parbox{2.5cm}{\centering \hspace{8mm}\textbf{Setting 2}}}  & \includegraphics[width=0.31\linewidth]{figures/VSR/good_draft_rate_meta-llama_Llama-3.1-70B-Instruct_sharegpt_tp8_512_max64.pdf} & \includegraphics[width=0.31\linewidth]{figures/VSR/good_draft_rate_meta-llama_Llama-3.1-70B-Instruct_arena_tp8_512_max64.pdf} & \includegraphics[width=0.31\linewidth]{figures/VSR/good_draft_rate_meta-llama_Llama-3.1-70B-Instruct_domain_tough_tp8_512_max64.pdf}  \\
    & (d) \hspace{5mm} $\Delta$ 20.82\% & (e) \hspace{5mm} $\Delta$ 23.32\% & (f) \hspace{5mm} $\Delta$ 20.47\% \\
    
    \rotatebox{90}{\parbox{2.5cm}{\centering \hspace{8mm}\textbf{Setting 3}}}  & \includegraphics[width=0.31\linewidth]{figures/VSR/good_draft_rate_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_sharegpt_tp8_512_max64.pdf} & 
    \includegraphics[width=0.31\linewidth]{figures/VSR/good_draft_rate_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_arena_tp8_512_max64.pdf} &
    \includegraphics[width=0.31\linewidth]{figures/VSR/good_draft_rate_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_domain_tough_tp8_512_max64.pdf} \\
    & (g) \hspace{5mm}  $\Delta$ 21.80\% & (h) \hspace{5mm} $\Delta$ 23.66\% & (i) \hspace{5mm} $\Delta$ 21.32\% \\
\end{tabular}
    \caption{The verification success rate comparison for various methods across experimental settings. $\Delta$ indicates the maximum gap between \alg{} and standard SD. The reported numbers reflect the mean over 3 independent trials.}
    \label{fig:VSR-all-appendix}
\end{figure}


\subsection{The Effect of Batch Size on \alg{} Performance}\label{app:batch-size}

Theoretically speaking, a larger batch size creates more possible combinations for draft token selection by \alg.
Therefore, \alg{} is likely to perform better in a speculative decoding server that processes a larger batch of requests concurrently.
In~\cref{fig:batch-size-TER-appendix}, we show a visual illustration of the verification success rate (VSR) and target efficiency rate (TER) (as defined in~\cref{eq:VSR} and~\cref{eq:TER}, respectively).

In setting 2 (draft model: Llama-1B-Instruct-FP8, target model: Llama-70B-Instruct), we observe a significant increase in VSR and TER when the batch size is increased to 64.
However, batch sizes of 16 and 32 have similar VSR and TER values.

In setting 3 (draft model: Llama-1B-Instruct-FP8, target model: Llama-405B-Instruct-FP8), we do not observe a significant change in VSR and TER, suggesting that the way that the batch size affects performance is highly dependent on the specific draft-target combination, too.

Overall, we expect a more significant improvement in the performance of adopting \alg{} by LLM inference service providers with larger capacities to handle a larger number of concurrent requests.

\begin{figure}[!ht]
    \centering
    \setlength{\tabcolsep}{1pt} 
    \begin{tabular}{ccc}
    & \hspace{8mm}\textbf{Setting 2} & \hspace{8mm}\textbf{Setting 3}  \\
    \rotatebox{90}{{\centering\hspace{1.8cm}\textbf{ShareGPT}}} & \includegraphics[width=0.47\linewidth]{figures/batch_size/batch_size_good_draft_rates_meta-llama_Llama-3.1-70B-Instruct_sharegpt_tp4_512_max64.pdf} &
    \includegraphics[width=0.47\linewidth]{figures/batch_size/batch_size_good_draft_rates_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_sharegpt_tp8_512_max64.pdf} \\
    
    \rotatebox{90}{{\centering\hspace{1.8cm}\textbf{ShareGPT}}} & \includegraphics[width=0.47\linewidth]{figures/batch_size/batch_size_system_efficiencies_meta-llama_Llama-3.1-70B-Instruct_sharegpt_tp4_512_max64.pdf} &
    \includegraphics[width=0.47\linewidth]{figures/batch_size/batch_size_system_efficiencies_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_sharegpt_tp8_512_max64.pdf} \\
    
\end{tabular}
    \caption{The change in the verification success rate (VSR) and target efficiency rate (TER) when we vary the batch size (BS) from 64 to 32 and 16. The reported numbers reflect the mean over 3 independent trials.}
    \label{fig:batch-size-TER-appendix}
\end{figure}
