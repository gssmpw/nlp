%!TEX root =  main_arxiv.tex


\paragraph{Speculative Decoding (SD).}
By employing a draft-then-verify strategy for lossless accelerations of LLM inference, SD has attracted significant attention recently~\citep{ryu2024closerlook-survey,xia2024sd-survey}.
Recent advancements based on SD have focused on developing more efficient draft models by producing multiple drafts for the next few tokens~\citep{cai2024medusa,cheng2024recurrentdrafterfastspeculative,li2024eagle}.
Additionally, some methods have optimized the speculation accuracy by aligning the draft model with the target model~\citep{liu2024onlinespeculativedecoding,zhou2024distillspec} or leveraging the target model itself to draft via techniques like layer skipping~\citep{zhang2024skiplayer}.
To facilitate more efficient verification, tree attention has been proposed for speedy tree-structured candidate verification~\citep{miao2024specinfer,spector2023stagedSD}.
In contrast, our work explores a complementary approach that intervenes between the draft and target models, performing strategic draft token selection to improve throughput over batched requests.
Our method can be seamlessly integrated with the above techniques for a more efficient SD system.

\paragraph{LLM Scheduling.} With the growing popularity of LLM as a service, several works have considered improvements to the scheduling of LLM services. 
These works can be broadly categorized into client-side and server-side approaches.
Server-side approaches~\citep{fu2024efficient,kim2024accelerating,liu2024optimizingspeculativedecodingserving,wang2024opttreespeculativedecodingadaptive} have focused on increasing the throughput of LLM services, which may lead to an unfair allocation of inference resources to users, hence causing starvation. 
On the other hand, client-side approaches~\citep{liu2024andes,sheng2024fairness} have focused on improving user satisfaction by improving client-side metrics (e.g., decreasing maximal waiting time or end-to-end latency).
Our work considers the scenario where the LLM inference service provider employs SD to ensure user satisfaction with inference speed while simultaneously aiming to maximize service throughput to optimize profitability.

\paragraph{Draft Window Optimization.}
In the foundational paper on SD, the authors have proposed to generate a window of draft tokens~\citep{leviathan2023}. 
The optimal draft window is theoretically determined under an impractical assumption of identical conditional acceptance rates for all draft tokens~\citep{leviathan2023}.
Empirically, such an acceptance rate can be estimated by a moving average of past requests~\citep{liu2024optimizingspeculativedecodingserving}.
Other heuristics for finding the optimal draft window include stopping the draft generation when the draft model's confidence score falls below a predetermined threshold~\citep{kim2023speculativedecodingbiglittle,liu2024kangaroo} or when an entropy-controlled criterion is met~\citep{agrawal2024adaedlearlydraftstopping}.
\citet{cai2024medusa} have proposed taking the union of these two heuristics.
These existing works have operated at a single-request level, except that of~\citet{liu2024optimizingspeculativedecodingserving}  which adaptively determines a single draft window for all requests in a batch.
Note that considering each request independently or using a common draft window for a batch can lead to inefficiencies in allocating verification budgets (i.e., inference capacity) across multiple requests, especially when operating under the limited computing resources of an LLM inference service provider.