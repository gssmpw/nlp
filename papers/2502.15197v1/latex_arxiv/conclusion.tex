%!TEX root =  main_arxiv.tex


In this paper, we study the problem of optimizing batch speculative decoding to maximize throughput in multi-request settings, such as those faced by model service providers.
To this end, we propose \alg{}, a novel method that efficiently selects optimal draft tokens for the LLM verification in log-linear time.
We have theoretically shown that, in the absence of drafting time, \alg{} achieves optimal throughput both at each decoding step and globally under reasonable assumptions about token acceptance rates.
Our empirical results further validate that \alg{} consistently outperforms standard speculative decoding and existing dynamic draft window selection methods, even when accounting for the extra time required for drafting extra tokens. 
These results highlight the potential of \alg{} to improve inference efficiency in real-world model service deployments.
A key future direction is adapting \alg{} to tree decoding, a notable feature in recent advancements in speculative decoding~\citep{cai2024medusa,li2024eagle2,li2024eagle}.
