%!TEX root =  main_arxiv.tex


This section first introduces speculative decoding and then describes the optimal draft token selection problem and the performance metrics used.

\subsection{Speculative Decoding (SD)}
SD is an efficient inference method designed to accelerate the decoding process in LLMs and 
involves two phases: drafting followed by verification.
Initially, a lightweight draft model, denoted as $\gS$, quickly generates candidate draft tokens.
Subsequently, these tokens are verified against the generations from the target model, denoted as $\gM$, which is also often referred to as the verification model.
SD allows parallelized verifications of tokens by $\gM$, as opposed to the conventional autoregressive decoding used in language models.
Hence, SD yields significant improvement in decoding speed.

Specifically, the draft model generates $k$ draft tokens $d_1, \ldots, d_k$ in an autoregressive manner  where $k$ is the draft window size.
Given a prompt or prefix $x$, the generation process follows $d_i \sim p_\gS(\cdot|x, d_1, \ldots, d_{i-1})$.
For notational simplicity, we denote $p_\gS(d_i) = p_\gS(d_i|x, d_1, \ldots, d_{i-1})$.
The verification follows a rejection sampling procedure.
If $p_\gS(d_i) \leq p_\gM(d_i)$, the draft token $d_i$ is accepted.
Otherwise, we reject the draft token with a probability of $1 - p_\gM(d_i)/p_\gS(d_i)$ and then output a new token sampled from an adjusted distribution $p_\gM(d_i') = \mathrm{norm}(\max(0, p_\gM(d_i') - p_\gS(d_i')))$, where $\mathrm{norm}(\cdot)$ normalizes the probability distribution.
Hence, the acceptance of draft tokens depends on both $p_\gS(\cdot)$ and $p_\gM(\cdot)$ and plays a vital role in the effectiveness of SD.
A higher acceptance suggests the possibility of greater speedup gain with a larger $k$.
We defer a more detailed discussion of the acceptance rate estimation in~\cref{app:acceptance-related-work}. 
However, we highlight that the effectiveness of SD is limited by the computing resources available. 
Using a draft window exceeding the capacity for parallel inferences that the server can manage degrades the performance, which we show empirically later in~\cref{sec:experiment}. 
Consequently, it is essential to carefully \textit{select} the draft window size for each request, leading to our proposed method outlined next.

\subsection{Optimal Draft Token Selection}
\label{sec:optimal_draft_selection}

We first define a set of other notations used throughout our paper.
We consider a specific LLM inference service provider with a limited capacity $C$, which represents the maximum number of parallel inferences its computing resources can perform. 
The capacity depends on the server configurations of the service provider in practice.
At each time step, the server processes a batch of $N$ requests $r_1, r_2, \cdots, r_N$, each with a partially complete sequence $S_{i,t_i} = (d_{i,1}, \ldots, d_{i, t_i})$ where $t_i$ represents the number of tokens verified/served so far for request $r_i$. 
We allow a variable draft window size $k_i$ for each request $r_i$. 
The draft model $\gS$ drafts a set $\gD \coloneqq \{(i, t) | i \in [N], t \in [t_i+k_i]\}$ such that $|\gD| = \sum_{i=1}^{N} k_i = C$.
For each $(i,t) \in \gD$, we send $S_{i,t}$ to have its \textit{last token} verified by $\gM$. 
We aim to optimally choose the set $\gD$ at each time step to maximize the performance of the server in terms of generation throughput, which we define below.

\paragraph{Per-step Throughput.} 
For each step of SD, we are mainly concerned with maximizing the per-step throughput, i.e., the number of tokens served at each time step. 
Mathematically, let $\vone_{i,t}$ be an indicator variable representing whether the last token of $S_{i,t}$ is accepted, let $\tau_{\text{step}}$ be the time per step. The per-step throughput is then defined as
\begin{equation*}
   \textstyle \gG_{\text{step}} \coloneqq (\mathbb{E}[\sum_{(i,t) \in \gD}\vone_{i,t}] + N ) / \tau_{\text{step}} \ .
\end{equation*}

Note that at least one token is always generated by SD via the \textit{bonus token} mechanism~\citep{leviathan2023}. Thus, without considering drafting time, the throughput of SD is theoretically at least as good as that of autoregressive decoding. 


\paragraph{Total Throughput.} The total throughput is calculated as the average per-step throughput over a total of $T$ steps with a fixed $\tau_{\text{step}}$ for each step:
\begin{equation*}
    \textstyle \gG \coloneqq T^{-1}\sum_{i=1}^{T} \gG_{\text{step}}\ .
\end{equation*}
Note that it is theoretically difficult to find an optimal draft token selection strategy that maximizes $\gG$ as the relationship between previously verified tokens and the distribution of acceptance rate for the remaining tokens is extremely complex. However, under a mild assumption on token acceptance rate, the optimality of $\gG$ is equivalent to the optimality of $\gG_{\text{step}}$, as explained formally in~\cref{sec:analysis} later.