%!TEX root =  main_arxiv.tex


% %%%%% Background for Speculative Decoding %%%%%
Transformer-based large language models (LLMs) have shown remarkable abilities to solve different tasks across various domains, such as natural language~\citep{zhao2023survey}, computer vision~\citep{Yin2024mllm}, robotics~\citep{zeng2023largelanguagemodelsrobotics}, code generation~\citep{rozi√®re2024codellamaopenfoundation}, among others~\citep{maslej2024ai-report}. 
However, the autoregressive nature of LLMs (i.e., generating one token at a time) leads to an increasingly sluggish inference speed as the model size increases. 


% %%%%% Speculative Decoding and its Limitations %%%%%
To address this problem, a recent widely-used approach is speculative decoding (SD)~\citep{cai2024medusa,cheng2024recurrentdrafterfastspeculative,leviathan2023,li2024eagle2,li2024eagle}: It achieves faster inference by using a \textit{small draft model} to rapidly generate a sequence of {\em (draft) tokens} and then a {\em large target model} to verify whether to accept or reject them in parallel. 
When a token is rejected, the draft model generates a new sequence of tokens in the next step, starting from the most recently accepted token.
A key aspect of SD is to determine the optimal number of draft tokens (i.e., {\em draft window size}) to generate and verify in each step.
Generating more draft tokens allows the target model to verify a longer sequence at once (given sufficient computing resources/capacity for parallel inferences), which can potentially boost inference speed. 
However, doing so increases the risk of wasting computing resources since all tokens following the first rejected token must be discarded.
In contrast, generating fewer draft tokens reduces this risk but limits the potential benefit of SD since the computing resources are not effectively utilized.
Therefore, the optimal selection of draft tokens that would be accepted when verified by the target model in parallel is critical to improving both inference speed and resource utilization~\citep{liu2024optimizingspeculativedecodingserving}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/tetris_gif.pdf}
    \caption{
        \textcolor{red!60!black}{Standard SD} (left) uses a fixed draft window size, while \textcolor{green!50!black}{\alg{}} (right) generates extra draft tokens and dynamically optimizes draft token selection for every request in a batch, resulting in more accepted tokens. 
    }
    \label{fig:tetris_illustration}
    \vspace{-5mm}
\end{figure}


% %%%%% Motivation for Batch Speculative Decoding %%%%%
Most existing works have focused on optimizing draft token selection for individual user requests~\citep{agrawal2024adaedlearlydraftstopping,huang2024specdecboostingspeculativedecoding,liu2024parallelspeculativedecodingadaptive,mamou2024dynamicspeculation}, but may not work well for profit-driven LLM inference service providers who must manage multiple user requests under a limited inference capacity.
Moreover, LLM inference service providers typically charge users based on the number of tokens served~\citep{fireworksai,replicate}. 
Hence, they are incentivized to maximize the total number of tokens served (i.e., {\em throughput}) across all user requests while ensuring fast response time to meet service level agreement~\citep{wieder2011service}.
So, they would employ computing clusters to process large batches of user requests simultaneously and use SD to further improve the inference speed.


% %%%%% Batch Speculative Decoding %%%%%
Such batch processing of user requests entails a fundamentally different optimization objective for SD compared to handling individual requests.
For SD of a single request, supposing a fast draft model with negligible runtime, the objective is to maximize the draft window size as long as the target model can verify all draft tokens in parallel by fully utilizing the inference capacity.
It can be naively extended to batch processing by widening the draft window for all requests until the inference capacity is reached.
This is inefficient as each request may require a different optimal draft token selection due to varying difficulty in speculation (i.e., generating tokens to match the target model's output).


% %%%%% Main Contributions %%%%%
This paper presents a theoretical framework that dynamically optimizes the draft token selection for every user request from the perspective of a capacity-limited LLM inference service provider who aims to maximize resource utilization. 
Since draft token verification is the most time-consuming component of SD, we propose \textbf{\alg}, a method that greedily selects draft tokens with a high likelihood of acceptance by the target model. The name of our method is derived from the shape of its selected tokens, as shown in~\cref{fig:tetris_illustration}.
We demonstrate that \alg{} strictly outperforms standard SD by achieving higher total throughput.
Our work bridges a critical yet overlooked gap in current research, allowing service providers to improve total throughput with batch SD. 
The specific contributions of our work here are summarized below:
\squishlisttwo
    \item In \cref{sec:problem}, we introduce the problem of optimal draft token selection in multi-request settings, and in \cref{sec:tetris}, we propose \alg{}, a novel method that selects optimal draft tokens in log-linear time for the target model's verification. 

    \item In \cref{sec:analysis}, we theoretically show that \alg{} achieves optimal throughput at each decoding step and globally in the absence of drafting time (i.e., time to generate draft tokens) under reasonable token acceptance assumptions.

    \item In \cref{sec:experiment}, our empirical results show that \alg{} consistently outperforms standard SD and existing methods that use dynamic draft windows for a batch in terms of total throughput and end-to-end latency (including drafting time), highlighting the potential of \alg{} to improve inference speed in real-world model service deployments.
\squishend
