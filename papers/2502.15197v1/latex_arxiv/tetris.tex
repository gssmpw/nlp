%!TEX root =  main_arxiv.tex


We introduce inter-dependencies among requests within a batch.
We favor parallel tokens when selecting sequential tokens leads to an excessive cascading of failure rates, and \textit{vice versa}.
To achieve this, we propose to introduce a manager to actively select the best draft tokens that are most likely to be successfully verified by the target model, thus maximizing the expected number of output tokens.
The manager is integrated into the speculative decoding framework and functions as an intermediary between the draft model and the target model.
It operates on the draft tokens and auxiliary outputs (e.g., token distributions, hidden states) from the draft model and strategically selects those that will be sent for verification by the target model.


At each step, define $p_{i,j}$ the conditional acceptance rate of the token at index $(i,j)$ given its corresponding prefix.
Let $\gB_{i,j} \coloneqq (i, j, \prod_{t=1}^{j} p_{i,t})$ be the tuple containing token indices and the probability of all selected tokens in row $i$ up to $j$ being accepted. 
Instead of simply selecting a fixed window of draft tokens for verification, we \textit{greedily} look for tokens with the highest cumulative acceptance rate $\prod_{t=1}^{j} p_{i,t}$ (and not the standalone acceptance rate $p_{i,j}$).
We let the draft model  propose the \textit{extra draft tokens} beyond the server capacity and then select a set $\gD^*$ of tokens such that it maximally utilizes the compute resource by ensuring $|\gD^*| = C$. 
This process dynamically allocates longer draft windows for requests with ``easy'' tokens and shorter windows for ``hard'' ones, reducing resource wastage while sufficiently leveraging speculation, as illustrated in~\cref{fig:tetris_illustration}. 
\alg{} is outlined in \cref{alg:tetris}.
\begin{algorithm}[!ht]
    \caption{\alg{}}
    \label{alg:tetris}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} draft $\gB$, batch size $N$, capacity $C$
        \STATE Initialize $\gD^* \gets \{\}$, $\gH \gets \text{Heap}()$
        \STATE $Z \gets \text{InitArray}(size=N, value=-1)$
        \FOR{$i \in [N]$}
            \STATE $\gH.\text{insert}(\gB_{i, 0})$
        \ENDFOR
        \REPEAT
            \STATE \texttt{// Dequeue the most probable}
            \STATE $(i,j,p_{ij}) = \gH.\text{extractMax}()$
            \STATE $\gD^* = \gD^* \cup \{(i,j)\}$
            \STATE \texttt{// Record the row-wise frontier}
            \STATE $Z[i]=j$
            \STATE \texttt{// Enqueue new candidates}
            \STATE $\gH.\text{insert}(\gB_{i,j + 1})$
        \UNTIL{$|\gD^*| = C$}
        \STATE {\bfseries return} $\gD^*$
    \end{algorithmic}
\end{algorithm}


\subsection{Analysis}\label{sec:analysis}
We now present our theoretical results, which show the per-step and global optimality of \alg{}.

\begin{restatable}[Per-step Optimality of \alg{}]{thm}{tetris}
    \label{thm:tetris}
    In the absence of drafting time, given the true acceptance rate $\alpha_{i,j}$ of each draft token $(i, j)$, \cref{alg:tetris} produces the optimal per-step throughput defined in \cref{sec:problem}.
\end{restatable}
The proof is delayed to \cref{app:proof_of_local_tetris}. 
While we have established the local optimality of \alg{} in \cref{thm:tetris}, such local optimality does not trivially generalize to maximizing total throughput. 
Nevertheless, we show, in \cref{thm:globalOpt}, that \alg{} is optimal in a slightly simpler scenario that retains sufficient complexity of interest.

\begin{assu}
    \label{assu:equal_acc}
    $\forall j$, all tokens in the $j$-th sequence have an identical acceptance rate denoted as $\alpha_j$.
\end{assu}

\begin{restatable}[Global Optimality of \alg{} under Assumption]{thm}{globalOpt}
    \label{thm:globalOpt}
    Under \cref{assu:equal_acc}, in the absence of drafting time, \alg{} searches for the optimal $\gG$ under the same capacity. Morever, if $\alpha_1 = \alpha_2 = \cdots = \alpha_N$, \alg{} has the same $\gG$ as standard batched speculative decoding.
\end{restatable}

The proof is delayed to \cref{app:thm_global_opt_tetris}.
Overall, we established both per-step and global optimality of \alg{} under theoretical assumptions.
In practice, the drafting time can be hidden with appropriately designed pipeline~\citep{liu2024parallelspeculativedecodingadaptive,wang2024minions} which parallelizes the execution of the draft model and the target model.\footnote{Although, they have yet been integrated in popular battle-tested model serving frameworks such as \vllm{}~\citep{kwon2023vllm} and \sglang{}~\citep{zheng2024sglangefficientexecutionstructured} as of this writing.}
The true acceptance rates are inaccessible in practice, we thus rely on surrogate measures and show their empirical effectiveness, which we will discuss next.


\subsection{Practical Implementations}\label{sec:practical-implementations}

The acceptance rate of a draft token depends on $\max(p_\gM(d_i)/p_\gS(d_i),1)$. 
However, the \alg{} manager does not have access to $p_\gM(\cdot)$ before verification.
In practice, we use the draft model's output probability as a surrogate of the token acceptance rate~\citep{kim2023speculativedecodingbiglittle,zhang2024skiplayer}.
We show in~\cref{sec:experiment} that this surrogate empirically results in strong performance.
Additionally, while we theoretically show that~\cref{alg:tetris} achieves a time complexity of $\gO(C\log N)$ (see~\cref{app:lem_time_comp}), we can additionally leverage the parallelism of GPU to achieve empirical negligible overhead of using \alg{} ($< 0.3 \text{ms}$ compared to the average draft time per token of $> 2.5\text{ms}$) via the \texttt{scatter\_max} operation directly implemented on GPU.
Lastly, the autoregressive token drafting can also be parallelized across requests.
Hence, drafting a batch of requests with a common window size of $k$ tokens takes the same time as a single request in practice.
