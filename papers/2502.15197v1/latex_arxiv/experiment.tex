%!TEX root =  main_arxiv.tex


We evaluate the effectiveness and efficiency of \alg{} against baseline methods.
We first validate the necessity of dynamic draft token selection and improvement of token acceptance with \alg{} in~\cref{sec:variation-draft-quality,sec:effect-extra-proposal}.
Then, we show the empirical end-to-end speedup in~\cref{sec:evaluation}.
We also discuss the potential further improvement in empirical results with the future implementation of speculative decoding pipelines in~\cref{sec:parallel-implementation}.


\paragraph{Settings.}
We perform experiments on target models of various parameter sizes, including \textit{Vicuna-33B-v1.3}, \textit{Llama-3.1-70B-Instruct}, and \textit{Llama-3.1-405B-Instruct}.
We use \textit{Vicuna-68M} and \textit{Llama-3.2-1B-Instruct} as their respective draft models.
Depending on the size of the models, different server configurations and tensor parallel sizes are adopted, detailed in \cref{tab:server_config}.
\alg{} is evaluated for generation of answer completion for questions extracted from ShareGPT~\citep{sharegpt-dataset}, Chatbot Arena~\citep{zheng2023arena}, Domain Tough Questions~\citep{yav-ai2024domain-tough}, and synthetic tasks generated from Shakespeare's The Sonnet.
The standard speculative decoding (SD)~\citep{leviathan2023} and dynamic speculative decoding (DSD)~\citep{liu2024optimizingspeculativedecodingserving} are baseline methods that we compare to.
We vary the drafting window sizes, allowing up to 3 extra draft tokens for \alg{} while keeping the same number of tokens sent for verification by the target model for fair comparison.
\alg{} is implemented in \vllm{}~\citep{kwon2023vllm}.
\begin{table}[!ht]
\centering
\caption{Server and model configurations. TP indicates the tensor parallel size used for model serving.}
\label{tab:server_config}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Setting} & \textbf{Draft Model (TP)} & \textbf{Target Model (TP)} & \textbf{GPU (VRAM)} \\ 
\midrule
1 & Vicuna-68M (1) & Vicuna-33B (4) & 4$\times$L40 (180G) \\
2 & Llama-1B-FP8 (1) & Llama-70B (8) & 8$\times$L40 (360G) \\ 
3 &Llama-1B-FP8 (1) & Llama-405B-FP8 (8) & 8$\times$H100 (640G) \\ 
\bottomrule
\end{tabular}
}
\vspace{-3mm}
\end{table}

\subsection{Variations in Draft Quality}\label{sec:variation-draft-quality}

We begin by emphasizing the importance of setting an appropriate draft window size.
Using Setting 2, we collect the oracle optimal draft window size to adopt for each SD step.
Notably, the results in~\cref{fig:accepted_tokens} show flat curves with long-tail distributions for various datasets, revealing significant variations in optimal window size per step.
This diversity highlights the potential suboptimality of a fixed draft window, as it fails to adapt to the inherent characteristics of the draft-target model combination or a batch of sequences.
By tailoring the draft token selection in a batch, \alg{} is expected to achieve higher efficiency and better alignment with the model's token acceptance patterns, hence improving overall performance.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/num_accepted_tokens.pdf}
    \vspace{-3mm}
    \caption{The distribution of the number of accepted tokens per speculative decoding step.}
    \label{fig:accepted_tokens}
\end{figure}

\subsection{Effect of Extra Draft Tokens}\label{sec:effect-extra-proposal}

Having extra draft tokens provides \alg{} with greater flexibility in selecting which draft tokens to send for verification.
To empirically show this effect, we define the verification success rate (VSR), 
\begin{equation}\label{eq:VSR}
   \textstyle \textit{VSR} = \frac{\textit{Accepted tokens}}{\textit{Tokens sent for verification}} \ ,
\end{equation}
which measures the quality of the draft tokens selected by \alg{}.
We show in~\cref{fig:extra_params} that increasing the number of extra draft tokens consistently increases the VSR metric across all settings.
This finding confirms the effectiveness of \alg{}'s strategy for draft token selection utilizing extra draft tokens.
It also validates the empirical usefulness of the draft model's output probabilities as a surrogate of the selection criteria, as stated in~\cref{sec:practical-implementations}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/VSR_vs_extra_param.pdf}
    \vspace{-3mm}
    \caption{Change in VSR as the number of extra draft tokens increases. Base draft length $k=4$.}
    \label{fig:extra_params}
\end{figure}


\subsection{Evaluation of \alg}\label{sec:evaluation}

\begin{figure*}[!ht]
    \centering
    \setlength{\tabcolsep}{1pt} 
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{cccc}
        & \hspace{8mm}\textbf{ShareGPT} & \hspace{8mm}\textbf{Arena} & \hspace{8mm}\textbf{Tough} \\
        \rotatebox{90}{\parbox{2.5cm}{\centering \hspace{8mm}\textbf{Setting 1}}} & \includegraphics[width=0.31\linewidth]{figures/throughput_lmsys_vicuna-33b-v1.3_sharegpt_tp4_512_max64.pdf} &
        \includegraphics[width=0.31\linewidth]{figures/throughput_lmsys_vicuna-33b-v1.3_arena_tp4_512_max64.pdf} &
        \includegraphics[width=0.31\linewidth]{figures/throughput_lmsys_vicuna-33b-v1.3_domain_tough_tp4_512_max64.pdf} \\
        & (a) \hspace{5mm} $\uparrow$ 3.50\%, $\Delta$ 6.70\% & (b) \hspace{5mm} $\uparrow$ 5.17\%, $\Delta$ 7.47\% & (c) \hspace{5mm} $\uparrow$ 4.85\%, $\Delta$ 9.27\% \\
        
        \rotatebox{90}{\parbox{2.5cm}{\centering \hspace{8mm}\textbf{Setting 2}}} & \includegraphics[width=0.31\linewidth]{figures/throughput_meta-llama_Llama-3.1-70B-Instruct_sharegpt_tp8_512_max64.pdf} & \includegraphics[width=0.31\linewidth]{figures/throughput_meta-llama_Llama-3.1-70B-Instruct_arena_tp8_512_max64.pdf} & \includegraphics[width=0.31\linewidth]{figures/throughput_meta-llama_Llama-3.1-70B-Instruct_domain_tough_tp8_512_max64.pdf}  \\
        & (d) \hspace{5mm} $\uparrow$ 2.01\%, $\Delta$ 2.17\% & (e) \hspace{5mm} $\uparrow$ 2.71\%, $\Delta$ 2.81\% & (f) \hspace{5mm} $\uparrow$ 3.43\%, $\Delta$ 3.43\% \\
        
        \rotatebox{90}{\parbox{2.5cm}{\centering \hspace{8mm}\textbf{Setting 3}}} & \includegraphics[width=0.31\linewidth]{figures/throughput_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_sharegpt_tp8_512_max64.pdf} & 
        \includegraphics[width=0.31\linewidth]{figures/throughput_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_arena_tp8_512_max64.pdf} &
        \includegraphics[width=0.31\linewidth]{figures/throughput_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_domain_tough_tp8_512_max64.pdf} \\
        & (g) \hspace{5mm} $\uparrow$ 3.93\%, $\Delta$ 3.93\% & (h) \hspace{5mm} $\uparrow$ 5.15\%, $\Delta$ 5.15\% & (i) \hspace{5mm} $\uparrow$ 5.25\%, $\Delta$ 5.25\% \\
    \end{tabular}}
    \caption{Throughput comparison for various methods across experimental settings. $\uparrow$ indicates the improvement over the best baseline method. $\Delta$ indicates the maximum gap between \alg{} and standard SD. The reported numbers reflect the mean and standard deviation over 3 independent trials.}
    \label{fig:throughput-all}
\end{figure*}

To evaluate the effectiveness of \alg{}, we perform comprehensive experiments on various datasets and report metrics, including the total throughput and end-to-end latency.
We compare to standard SD and DSD.
Throughout the experiments, we maintain a consistent system load of 64 batched requests to ensure consistency, reproducibility, and fairness in comparisons. Note that all experiments include drafting time.

\paragraph{Total Throughput.}
We measure the performance of a speculative decoding method using the total throughput, which includes both accepted draft tokens by the target model and the bonus tokens, which make up the final completion.
As shown in \cref{fig:throughput-all}, \alg{} achieves up to approximately 5.25\% improvement in terms of total throughput compared to the best baseline, depending on the draft-target setting and the nature of the task performed.
The maximum gap between \alg{} and standard SD is up to 9.27\%.
Importantly, \alg{} consistently outperforms the standard SD and DSD across all settings of the draft window sizes.
This shows the robustness of \alg{} to different hyperparameter choices.
Additionally, it is evident that having more speculative tokens (i.e., a larger draft window size) does not always improve the performance, as having too many parallel executions of the target model exceeding the servers' parallel inference capacity degrades performance.

Empirically, we observe that \alg{} achieves optimal performance when the number of extra draft tokens is set to 1 or 2.
These results are partly attributed to the current sequential draft-target implementation for the speculative decoding pipeline, as more extra draft tokens take time to generate autoregressively.
Remarkably, this pipeline can be better designed to amplify the benefit of \alg{}, which we defer the discussion to \cref{sec:parallel-implementation}.
Moreover, while DSD is expected to outperform standard SD, we note that it is not always the case in empirical experiments.
This behavior may result from the difficulty of accurately estimating the conditional token acceptance rate in practice\footnote{Inaccurate conditional acceptance rate estimation results in inaccurate calculation of expected generation token counts.} and the quality of the fitted latency prediction model.


\paragraph{End-to-end Latency.}
We also measure the end-to-end latency of each request.
This metric measures the average latency of the speculative decoding system in finishing completions, which can affect user satisfaction.
We summarize the results in~\cref{tab:latency-all} and defer the figures to~\cref{app:latency-plots}.
Overall, \alg{} achieves up to 6.13\% improvement in latency as compared to the best baseline and up to 9.32\% improvement against standard SD.
\begin{table}[!ht]
\centering
\caption{Improvement in end-to-end latency. Refer to~\cref{fig:throughput-all} for definitions of $\uparrow$ and $\Delta$. The reported
numbers reflect the mean over 3 independent trials.}
\label{tab:latency-all}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{c|cc|cc|cc}
\toprule
\multirow{2}{*}{Setting} & \multicolumn{2}{c|}{\textbf{ShareGPT}} & \multicolumn{2}{c|}{\textbf{Arena}} & \multicolumn{2}{c}{\textbf{Tough}} \\ 
& $\uparrow$ & $\Delta$ & $\uparrow$ & $\Delta$ & $\uparrow$ & $\Delta$ \\
\midrule
\multirow{1}{*}{1} & 3.42\% & 6.05\% & 5.30\% & 6.30\% & 5.47\% & 9.32\% \\
\midrule
\multirow{1}{*}{2} & 2.65\% & 2.70\% & 3.86\% & 3.86\% & 3.65\% & 3.65\% \\
\midrule
\multirow{1}{*}{3} & 3.51\% & 4.52\% & 6.13\% & 6.13\% & 4.49\% & 4.68\% \\
\bottomrule
\end{tabular}}
\vspace{-3mm}
\end{table}

\subsection{Potentially Parallelized Pipeline}\label{sec:parallel-implementation}
We implement \alg{} to work with the \vllm{} library, one of the most efficient frameworks for LLM inference~\citep{kwon2023vllm}.
\vllm{} adopts a sequential pipeline for speculative decoding, where the target model runs sequentially after the draft model finishes generating draft tokens.
As illustrated in~\cref{fig:pipeline}, \alg{} is integrated between the draft and target models.
However, in such a sequential pipeline, \alg{} cannot fully realize its potential as the extra draft tokens incur additional computational time.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/pipeline.001.jpeg}    
    \vspace{-3mm}
    \caption{Parallelized pipeline for speculative decoding, where the draft model and \alg{} runtime can be hidden entirely through parallelization.}
    \label{fig:pipeline}
\end{figure}

Recent works such as Minions~\citep{wang2024minions} and PEARL~\citep{liu2024parallelspeculativedecodingadaptive} have started exploring the benefits of a parallelized pipeline with two processes concurrently running the draft and target models as illustrated in~\cref{fig:pipeline}.
Given that the draft model runs significantly faster than the target model, the draft time, as well as the time to run our \alg{}, can be hidden entirely in the parallelized pipeline.
Moreover, the idle time (marked in green) of Process 2 between steps can be utilized to draft more extra tokens of \alg{} or to run more complex algorithms.

Under the constraint of sequential pipelines in \vllm{}, we instead adopt an alternative performance metric that better captures the potential advantages of \alg{} in parallelized pipelines.
We use the target efficiency rate (TER) defined as follows,
\begin{equation}\label{eq:TER}
  \textstyle  \textit{TER} = \frac{\textit{Accepted tokens + Bonus tokens}}{\textit{Max possible number of tokens if all accepted}} \ .
\end{equation}
As \textit{TER} measures the efficiency of target model verifications and is unaffected by the drafting process and \alg{} runtime, it provides an accurate indication of the net benefit of \alg{}.
In~\cref{fig:TER-figure}, we demonstrate a case study for Setting 3 on Tough dataset: The improvement of \textit{TER} is first calculated from the left figure, and is then used to compute the \textit{projected throughput} $\hat{\gG}^{(\textit{TER})}_k$, following
\begin{equation*}
     \textstyle \hat{\gG}^{(\textit{TER})}_k = \gG_{\text{SD}, k} \times \frac{ (\textit{TER}_{\alg, k} - \textit{TER}_{\text{SD},k})} {\textit{TER}_{\text{SD},k}} \ ,
\end{equation*}
where $k$ is the number of speculative tokens (i.e., draft window size) and $\gG$ represents throughput.
Consequently, using \alg{} is \textit{projected} (i.e., not realized in the current implementation) to achieve 12.04\% improvement for this setting under parallelized pipeline.
The full results are shown in~\cref{tab:potential} and the figures are shown in~\cref{app:projected-improvement-plots}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/TER_throughput_neuralmagic_Meta-Llama-3.1-405B-Instruct-FP8_domain_tough_tp8_512_max64.pdf}
    \caption{Left: Baseline comparisons for \textit{TER} in different speculative configurations. Right: \textit{Projected} $\hat{\gG}^{(\textit{TER})}_k$ plot for \alg{} with baselines.}
    \label{fig:TER-figure}
\end{figure}
\begin{table}[t]
    \centering
    \caption{Projected throughput $\hat{\gG}^{(\textit{TER})}$ improvement based on \textit{TER} metric improvement, realizable under a parallelized speculative decoding pipeline.}
    \label{tab:potential}
    \resizebox{0.72\linewidth}{!}{
    \begin{tabular}{@{}clcc@{}}
        \toprule
        \textbf{Setting} & \textbf{Dataset} & \textbf{$\gG$$\uparrow$} & \textbf{$\hat{\gG}^{(\textit{TER})}$$\uparrow$} \\ 
        \midrule
        \multirow{3}{*}{1} 
        & ShareGPT & 3.50\% & 9.70\% \\
        & Arena & 5.17\% & 7.79\% \\
        & Tough & 4.85\% & 8.92\% \\
        \midrule
        \multirow{3}{*}{2} 
        & ShareGPT & 2.01\% & 11.70\% \\
        & Arena & 2.71\% & 11.17\% \\
        & Tough & 3.43\% & 11.91\% \\
        \midrule
        \multirow{3}{*}{3} 
        & ShareGPT & 3.93\% & 11.67\% \\
        & Arena & 5.15\% & 10.53\% \\
        & Tough & 5.25\% & 12.04\% \\
        \bottomrule
    \end{tabular}}
    \vspace{-3mm}
\end{table}

\subsection{Ablation Study}

\begin{table}[!ht]
    \centering
    \caption{\alg{} improvement in throughput for ablation study of robustness to variations in draft quality.}
    \label{tab:ablation-mix}
    \resizebox{0.7\linewidth}{!}{
    \begin{tabular}{cccc}
        \toprule
        \multirow{1}{*}{Setting} & \multicolumn{1}{c}{\textbf{Sonnet}} & \multicolumn{1}{c}{\textbf{Tough}} & \multicolumn{1}{c}{\textbf{Mix}}  \\ 
        \midrule
        \multirow{1}{*}{1} & 2.46\% &  4.85\% &  4.12\% \\
        \multirow{1}{*}{2} & -0.81\% & 3.43\% & 3.48\% \\
        \multirow{1}{*}{3} & 2.07\% &  5.25\% & 4.24\% \\
        \bottomrule
    \end{tabular}}
    \vspace{-3mm}
\end{table}

\paragraph{Robustness to Variations in Draft Quality.}
We artificially introduce additional variations in draft quality by mixing datasets of different difficulty levels.
We create synthetic prompts designed for models to repeat lines from a poem named Sonnet.
Since Sonnet is relatively easy for the small draft model, it achieves a high rate of successful verification by the target model.
We then construct a new dataset, Mix, by randomly mixing Sonnet and a more challenging dataset, Tough, in equal proportions.
As shown~\cref{tab:ablation-mix}, the performance improvement of \alg{} over the best baseline suffers only a marginal or no decline, indicating its robustness to substantial variations in draft quality.

\paragraph{Extension to Medusa.}
The Medusa model generates multiple subsequent draft tokens using a single forward pass (as opposed to autoregressive generation) through multiple decoding heads~\citep{cai2024medusa}.
Leveraging Medusa, it is possible to generate extra draft tokens for \alg{} at minimal marginal computational cost.
We show in~\cref{app:medusa-extension} that integrating \alg{} to Medusa achieves a 3.19\% improvement in total throughput.


\paragraph{Other Ablations.}
We also include ablations on \alg{}'s improvement in verification success rate (VFS) in~\cref{app:improve-VSR}, and the effect of batch size on the performance in~\cref{app:batch-size}.
