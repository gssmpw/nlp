%!TEX root =  main_arxiv.tex


In this paper, our empirical experiments only demonstrate results using the current sequential speculative decoding pipeline implemented on \vllm. 
That is, the target model stays idle while waiting for draft tokens from the draft model. 
Consequently, the performance improvement of \alg{} is heavily dependent on the trade-off between the additional runtime required to generate extra draft tokens and the gain in token acceptance achieved through \alg{}.
Such trade-off limits the practical effectiveness of \alg{}, especially when a slow draft model is required.
We anticipate that future implementations of a parallelized pipeline could potentially reveal greater speedups with \alg{}.
However, we have not yet integrated such features into \vllm{} for testing in empirical experiments.