\section{Experiments}
%In this section, we fully implement our DiffoRA and evaluate its performance on different downstream tasks, including natural language understanding and question answering. We first present the experimental configurations and then compare the fine-tuning accuracy of our method and baselines.


\subsection{Configurations}

\noindent\textbf{Hardware.} DiffoRA is fully implemented in Python programming language. We evaluate our method on a Desktop Core i7-12700F CPU and GeForce RTX 3090.

\noindent\textbf{Datasets and pre-trained models.} 
We utilize two types of benchmarks: i) General Natural Language Understanding (GLUE) \cite{wang-etal-2018-glue}, including MNLI, SST-2, CoLA, QQP, QNLI, RTE, MRPC, and STS-B; and ii) Question Answering, including SQuADv1.1 \cite{rajpurkar-etal-2016-squad} and SQuADv2.0 \cite{rajpurkar-etal-2018-know}. We use the DeBERTaV3-base \cite{he2023debertav} as the backbone model in the main text, and the results on GLUE using RoBERTa-base \cite{liu2019roberta} are presented in \cref{app:roberta}.

\noindent\textbf{Counterpart comparisons.} We use the following methods as baselines. 
(i) Full FT uses all parameters for fine-tuning;
(ii) BitFit \cite{zaken2022bitfit} is a sparse-fine-tuning method for pre-trained models that updates only a small subset of the bias terms;
(iii) Houlsby adapter \cite{houlsby2019parameter} adds a few trainable modules inserted between layers of a pre-trained model, allowing for task-specific tuning without altering the entire model; 
(iv) Pfeiffer adapter \cite{pfeiffer2021adapterfusion} combines multiple task-specific adapters by linearly blending their outputs;
(v) LoRA \cite{hu2022lora} reduces the number of trainable parameters by applying low-rank matrix decomposition to weight updates in pre-trained models;
(vi) AdaLoRA \cite{zhang2023adaptive} adapts LoRA by dynamically adjusting the rank of low-rank updates during training, optimizing parameter efficiency while maintaining model performance across various tasks. We select the baseline methods with comparable parameter amounts and open-sourced codes for fair comparisons. 

\noindent\textbf{Implementation details.}
We set the module retention ratio $\rho$ to 50\%, the LoRA rank to 4, and the $\alpha$ of LoRA to 16. All the results are the average values under three random seeds. See \cref{app:settings} for more detailed settings. 



\subsection{Natural language understanding}
We evaluate the performance of the fine-tuned model on the GLUE benchmark \cite{wang2018glue} using various approaches.
We use DeBERTaV3-base as the pre-trained model, which contains 183 million parameters. 
%
During this task, we set the rank of the low-rank decomposition matrices to 4 and fine-tuned 50\% of the modules in each layer of the pre-trained model.
%
We adopt all eight datasets in GLUE and the concrete fine-tuning model architectures are determined by the specific tasks. 
%
We summarize and present the results in \cref{tab: glue}.
Overall, our DiffoRA achieves the highest model accuracy among the baselines on all the datasets.
More concretely, our method achieves 0.81\% higher fine-tuning accuracy than the state-of-the-art method AdaLoRA on the CoLA dataset.
DiffoRA exhibits consistently better results than the existing methods under the same level of parameter amounts. 
Moreover, our method also outperforms the baseline methods with a larger parameter size, \ie, up to 3.84\% higher accuracy than PAdapter with twice the parameters involved. 
Compared to the full fine-tuning strategy, DiffoRA obtains a better performance, which also demonstrates the effectiveness of our approach in the GLUE benchmark. 


% \begin{table*}
%   \centering
%   \begin{tabular}{@{}l|c|ccccccccc@{}}
%     \toprule
%     \multirow{2}{*}{Method} & \multirow{2}{*}{\#Params} & MNLI & SST-2 & CoLA & QQP & QNLI & RTE & MRPC & STS-B & All \\
%      & & m/mm & Acc & Mcc & Acc/F1 & Acc & Acc & Acc & Corr & Avg.\\
%     \midrule
%     Full FT & 184M & 89.90/90.12 & 95.63 & 69.19 & 92.40/89.80 & 94.03 & 83.75 & 89.46 & 91.60 &88.09\\
%     \midrule
%     BitFit & 0.1M & 89.37/89.91 & 94.84 & 66.96 & 88.41/84.95 & 92.24 & 78.70 & 87.75 & 91.35 & 86.02 \\
%     \midrule
%     % HAdapter & 1.22M & 90.13/90.17 & 95.53& 68.64& 91.91/89.27 &94.11 &84.48& 89.95& 91.48 &88.12 \\
%     % PAdapter & 1.18M & 90.33/90.39 &95.61 &68.77& 92.04/89.40& 94.29 &85.20 &89.46& 91.54& 88.24\\
%     % LoRA$_{r=8}$ & 1.33M &90.65/90.69 &94.95& 69.82& 91.99/89.38& 93.87& 85.20& 89.95& 91.60 &88.34\\
%     % AdaLoRA& 1.27M&90.76/90.79& 96.10& 71.45 &92.23/89.74& 94.55& 88.09 &90.69 &91.84& 89.31\\
%     % DiffoRA&~1.20M & &&72.64&&&\\
%     % \midrule
%     HAdapter &0.61M& 90.12/90.23& 95.30& 67.87 &91.65/88.95& 93.76& 85.56 &89.22& 91.30& 87.93\\
%     PAdapter& 0.60M& 90.15/90.28& 95.53 &69.48 &91.62/88.86& 93.98 &84.12 &89.22& 91.52& 88.04\\
%     HAdapter &0.31M& 90.10/90.02& 95.41& 67.65& 91.54/88.81& 93.52& 83.39& 89.25& 91.31& 87.60\\
%     PAdapter &0.30M& 89.89/90.06& 94.72 &69.06 &91.40/88.62& 93.87& 84.48& 89.71 &91.38& 87.90\\
%     LoRA$_{r=2}$ &0.33M& 90.30/\underline{90.38}& 94.95 &68.71& 91.61/88.91 &94.03 &85.56& 89.71& \underline{91.68}& 88.15\\
%     SoRA$_{r=2}$& 0.25M & 90.37/90.51 & 95.64 & 70.22 & 91.88/89.12 & 93.78 & 85.18 & 89.71 & 91.63 & 88.39 \\
%     AdaLoRA& 0.32M& \textbf{90.66}/\textbf{90.70}& \underline{95.80}& \underline{70.04}& \underline{91.78}/\textbf{89.16}& \underline{94.49} &\underline{87.36} &\underline{90.44} &91.63& \underline{88.86}\\
%     AdaLoRA& 0.32M& \textbf{90.66}/\textbf{90.70}& \underline{95.80}& \underline{70.04}& \underline{91.78}/\textbf{89.16}& \underline{94.49} &\underline{87.36} &\underline{90.44} &91.63& \underline{88.86}\\
%     DiffoRA& $\sim$0.30M &\underline{-}/- &\textbf{96.09} &\textbf{70.85} & \textbf{91.79}/\underline{89.12} & \textbf{94.52}& \textbf{87.96}& \textbf{90.79}& \textbf{91.75}&  \textbf{-}\\
%     \bottomrule
%   \end{tabular}
%   \caption{The results of the fine-tuned DeBERTaV3-base model on the GLUE dataset are presented, with the best results highlighted in bold and the second-best results underlined. Our DiffoRA achieved the best results on average.}
%   \label{tab: glue}
% \end{table*}

\begin{table*}
  \centering
  \begin{tabular}{@{}l|c|ccccccccc@{}}
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{\#Params} & MNLI & SST-2 & CoLA & QQP & QNLI & RTE & MRPC & STS-B & All \\
     & & m/mm & Acc & Mcc & Acc/F1 & Acc & Acc & Acc & Corr & Avg.\\
    \midrule
    Full FT & 184M & 89.90/90.12 & 95.63 & 69.19 & 92.40/89.80 & 94.03 & 83.75 & 89.46 & 91.60 &88.09\\
    \midrule
    BitFit & 0.1M & 89.37/89.91 & 94.84 & 66.96 & 88.41/84.95 & 92.24 & 78.70 & 87.75 & 91.35 & 86.02 \\
    \midrule
    HAdapter &0.61M& 90.12/90.23& 95.30& 67.87 &91.65/88.95& 93.76& 85.56 &89.22& 91.30& 87.93\\
    PAdapter& 0.60M& 90.15/90.28& 95.53 &69.48 &91.62/88.86& 93.98 &84.12 &89.22& 91.52& 88.04\\
    HAdapter &0.31M& 90.10/90.02& 95.41& 67.65& 91.54/88.81& 93.52& 83.39& 89.25& 91.31& 87.60\\
    PAdapter &0.30M& 89.89/90.06& 94.72 &69.06 &91.40/88.62& 93.87& 84.48& 89.71 &91.38& 87.90\\
    LoRA$_{r=2}$ &0.33M& 90.30/90.38& 94.95 &68.71& 91.61/88.91 &94.03 &85.56& 89.71& \underline{91.68}& 88.15\\
    %SoRA$_{r=2}$& 0.25M & 90.37 & 95.64 & 70.22 & 91.88 & 93.78 & 85.18 & 89.71 & 91.63 & 88.39 \\
    AdaLoRA& 0.32M& 90.09/90.41& \underline{95.80}& \underline{70.04}& \underline{91.78}/\textbf{89.16}& \underline{94.49} &\underline{87.36} &\underline{90.44} &91.63& \underline{88.81}\\
    \midrule
    \rowcolor{gray!20} DiffoRA& 0.35M &\textbf{90.49}/\textbf{90.49}&\textbf{96.09} &\textbf{70.85} & \textbf{91.79}/\underline{89.12}& \textbf{94.52}& \textbf{87.96}& \textbf{90.79}& \textbf{91.75}&  \textbf{89.11}\\
    \bottomrule
  \end{tabular}
  \caption{The results of the fine-tuned DeBERTaV3-base model on the GLUE dataset are presented, with the best results highlighted in bold and the second-best results underlined. Our DiffoRA achieved the best results on average.}
  \label{tab: glue}
\end{table*}

\subsection{Question answering}
For the question-answering task, we use DeBERTaV3-base as the pre-trained model and adopt two datasets (\ie, SQuADv1.1 and SQuADv2.0) under different amounts of parameters to fine-tune the model.
% We use the same settings as in the NLU task and summarize the results in \cref{tab: qa}.
In order to keep the number of parameters close to the baseline, we choose the rank of the low-rank decomposition matrices from $\{1, 2, 5, 10\}$ in the SQuADv1.1 dataset and $\{2, 4, 8,15\}$ in the SQuADv2.0 dataset.
We use Exact Match (EM) and F1 as evaluation indicators and summarize the results in \cref{tab: qa}.
Similar to the GLUE benchmark, our DiffoRA also demonstrates consistently better results than the baseline on both SQuAD datasets. 
Specifically, on SQuADv1.1, DiffoRA obtains 0.4\% to 0.5\% higher EM and 0.1\% to 0.2\% higher F1 than the best baseline AdaLoRA. 
Compared to the full fine-tuning method results, our DiffoRA also achieves higher accuracy even with 0.08\% parameters fine-tuned. 
Furthermore, on the SQuADv2.0 dataset, our method is around 0.2\% higher than the best baseline on EM and F1. 
DiffoRA is consistently better than the fully fine-tuning strategy (\ie, 85.4\% EM and 88.4\% F1) and original LoRA, which demonstrates the effectiveness of our scheme. 

In all, the experimental results in this section demonstrate that DiffoRA works consistently better than the baseline methods on all benchmarks and datasets, which conforms to our theoretical analysis.




\begin{table*}
  \centering
  \begin{tabular}{@{}l|cccc|cccc@{}}
    \toprule
    Method & \multicolumn{4}{c|}{SQuADv1.1} & \multicolumn{4}{c}{SQuADv2.0}\\
    
    \midrule
     Full FT &  \multicolumn{4}{c|}{86.0/92.7} &  \multicolumn{4}{c}{85.4/88.4}\\
     \midrule
    \#Params &0.08\% &0.16\%& 0.32\% & 0.65\% &0.08\% &0.16\%& 0.32\% & 0.65\%\\
    \midrule
    HAdapter&84.4/91.5&85.3/92.1&86.1/92.7& 86.7/92.9& 83.4/86.6& 84.3/87.3 & \underline{84.9/87.9} & \underline{85.4/88.3}\\
    PAdapter&84.4/91.7 &85.9/92.5&86.2/92.8& 86.6/93.0 &\textbf{84.2}/\textbf{87.2}& 84.5/\underline{87.6} & \underline{84.9}/87.8& 84.5/87.5\\
    LoRA&86.4/92.8 &86.6/92.9&86.7/93.1 &86.7/93.1&83.0/86.3 &83.6/86.7&84.5/87.4 &85.0/88.0\\
    AdaLoRA&\underline{87.2/93.4}& \underline{87.5/93.6}& \underline{87.5/93.7}& \underline{87.6/93.7} & 83.0/86.3& \underline{84.6}/87.5&84.1/87.3 &84.2/87.3\\
    \midrule
    \rowcolor{gray!20} DiffoRA &\textbf{87.6}/\textbf{93.5}&\textbf{88.1}/\textbf{93.8}& \textbf{88.1}/\textbf{93.8} & \textbf{88.1}/\textbf{93.9} & \textbf{84.2}/\textbf{87.2}&\textbf{84.8}/\textbf{87.8}&\textbf{85.1}/\textbf{88.0}&\textbf{85.5}\textbf{/88.4}\\
    %r = 2&4&8&15 
    \bottomrule
  \end{tabular}
  \caption{The results of the fine-tuned DeBERTaV3-base model on the SQuAD dataset. We report EM/F1. The best results are highlighted in bold and the second-best results are underlined.}
  \label{tab: qa}
\end{table*}

% \subsection{Natural language generation}

% \begin{table*}
%   \centering
%   \begin{tabular}{@{}l|c|c|c@{}}
%     \toprule
%     \#Params& Method &XSum& CNN/DailyMail\\
%     \midrule
%     100\%& Full FT& 45.49/22.33/37.26& 44.16/21.28/40.90\\
%     \midrule
%     \multirow{3}{*}{2.20\%}& LoRA &43.95/20.72/35.68& 45.03/21.84/42.15\\
%  &AdaLoRA &44.72/21.46/36.46& 45.00/21.89/42.16 \\
%  &DiffoRA& & \\
%      \midrule
%  \multirow{3}{*}{1.10\%}& LoRA &43.40/20.20/35.20 &44.72/21.58/41.84 \\
%  &AdaLoRA &44.35/21.13/36.13 &44.96/21.77/42.09\\
%  &DiffoRA& & \\
%      \midrule
%  \multirow{3}{*}{0.26\%}& LoRA &43.18/19.89/34.92& 43.95/20.91/40.98\\
%  &AdaLoRA &43.55/20.17/35.20 &44.39/21.28/41.50\\
%  &DiffoRA& & \\
%      \midrule
%  \multirow{3}{*}{0.13\%}& LoRA& 42.81/19.68/34.73& 43.68/20.63/40.71\\
%  &AdaLoRA& 43.29/19.95/35.04& 43.94/20.83/40.96\\
%  &DiffoRA& & \\
%     \bottomrule
%   \end{tabular}
%   \caption{Results.   Ours is better.}
%   \label{tab: qa}
% \end{table*}

\vspace{-.18em}
\section{Analysis}

\noindent\textbf{DiffoRA v.s. Random select strategy.}
%
We first analyze the effectiveness of DiffoRA by comparing its performance with that of the random selection method. 
%We use the random selection method as the baseline.
Specifically, we randomly select three modules in each layer as the fine-tuning modules, and the remaining modules are processed with the weight-sharing strategy.
%
The results are shown in \cref{tab: random vs diffora}.
We choose ranks from $\{1, 2, 4\}$ for fine-tuning in the STS-B and SQuADv1.1 datasets, respectively.
It can be demonstrated that compared to random sampling, DiffoRA achieves significant performance improvements and effectively identifies important modules, e.g., DiffoRA is 0.2\% to 0.6\% higher than the random select strategy.  

\begin{table}
  \centering
  \begin{tabular}{c|c|c|c|c}
    \toprule
    Method  &Rank&\#Params&STS-B& SQuADv1.1\\
    \midrule
    Random & 1 & 0.09& 91.21&  87.23  \\
    \rowcolor{gray!20}DiffoRA & 1 & 0.08& 91.65 & 87.37 \\
    \midrule
    Random & 2 &0.19  & 91.38& 87.55  \\
    \rowcolor{gray!20}DiffoRA & 2 &0.16 & 91.52 & 87.74 \\
    \midrule
    Random &4 &0.37 & 91.11 & 87.82 \\
    \rowcolor{gray!20}DiffoRA & 4&0.34 & 91.75&  88.04 \\
    \bottomrule
  \end{tabular}
  \caption{Random Selection v.s. DiffoRA on two datasets.}
  \label{tab: random vs diffora}
\end{table}



% \subsection{The impact of scaling}
% We first analyze the impact of different scales on DiffoRA. We select the rank of the matrix in LoRA from $\{1, 2, 4, 8\}$, respectively, and train LoRA-equipped pre-trained model in STSB and SQuADv1.1 datasets to obtain the accuracy of the model.
% The results are summarized in \cref{fig: scaling}.



%\subsection{Impact of Sample Rate and Weight Sharing}

\noindent\textbf{Sample rate. } 
%We study the effects of different sample rates on DiffoRA. 
We select sample rates from \{0.2, 0.4, 0.5, 0.7, 0.9\} corresponding to the most important Top-$\{1, 2, 3, 4, 5\}$ modules, respectively. 
To explore the relationship between the performance and sample rate, we conduct experiments on three datasets: STS-B, RTE, and MRPC. The results are summarized in \cref{tab: sample rate}.
The results show that when the sampling rate is around 0.5, the performance of the fine-tuned model achieves state-of-the-art.


\begin{table}
  \centering
  \begin{tabular}{c|c|c|c|c}
    \toprule
    Sample Rate&K&STS-B& RTE & SQuADv1.1\\
    \midrule
    0.2  &1& 90.87 &85.07 & 87.09 \\
    0.4  &2& 91.66 & 85.65& 87.43 \\
    0.5  &3& 91.75 & 87.96 & 88.12\\ 
    0.7  &4& 91.41 & 87.04 &88.19\\
    0.9  &5& 91.39 &86.34 & 88.18\\
    \bottomrule
  \end{tabular}
  \caption{DiffoRA across three datasets at different sample rates.}
  \label{tab: sample rate}
\end{table}



\noindent\textbf{Weight sharing.} We further investigate the effects of the weight-sharing strategy on our DiffoRA. Specifically, we consider two scenarios: (i) weight sharing, and (ii) weight sharing combined with a selection matrix. 
Experiments are conducted on the STS-B and SQuADv1.1 datasets, and the results are summarized in \cref{tab: weight sharing}. 
The table shows that the combination of module selection and weight sharing can lead to better results. For instance, our DiffoRA with the weight sharing achieves 0.11\% to 2.9\% higher accuracy than the method without this strategy. 
% Therefore, the weight-sharing strategy is effective, as it alleviates discretization differences and enhances performance.


% We further explore the effect of the rank of the shared low-rank matrix on the method. We choose the matrix rank form $\{1, 2, 4, 8, 16\}$, and fine-tune the model on the STSB, RTE and MRPC datasets.
% The results are summarized in \cref{tab: share rank}.

% \begin{table}
%   \centering
%   \begin{tabular}{c|c|c|c|c}
%     \toprule
%     Rank&\#Params &STSB& RTE & MRPC\\
%     \midrule
%     1 & 91.38 &  & &  \\
%     2 & 91.31 &  &  &\\
%     4 &  &  &  &\\
%     8 &  &  & & \\
%     \bottomrule
%   \end{tabular}
%   \caption{Results.   Ours is better.}
%   \label{tab: share rank}
% \end{table}

\begin{table}
  \centering
  \begin{tabular}{c|c|c|c}
    \toprule
    Weight Share &STS-B& RTE & MRPC\\
    \midrule
    \Checkmark & 91.75 &87.96 &90.79  \\
    \XSolidBrush  & 91.66 & 85.06 & 89.58\\
    % Only & & & \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of DiffoRA w/o weight sharing.}
  \label{tab: weight sharing}
\end{table}

% Rank of weight sharing matrix.

% With/without weight sharing.


% \subsection{Impact of }








