% \section{}


\section{Theoretical analysis}
\subsection{Preliminaries}

\textbf{Notations.} We define $[n]=\{1,2,\ldots, n\}$. 
We denote the vectors and matrices as the lower and uppercase bold font, respectively. For instance, $\bm{x}$ is a vector with entry $x_i$, and $\bm{M}$ is a matrix with entry $[\bm{M}]_{ij}$.
%
The minimum eigenvalue of $\bm{M}$ is denoted as $\lambda_{\min}(\bm{M})$.
% $\bm{1}_{m\times n}$ represents a $m\times n$ matrix filled by ones and 
$\|\cdot\|_2$ is used to represent the $l_2$ norm of a vector. 
%
$N(\bm{0},\bm{I})$ and $U\{S\}$ represent the standard Gaussian distribution and uniform distribution over a set $S$, respectively. 
% $N(\bm{0},\bm{I})$ represent the standard Gaussian distribution. 
%
We denote by $\bm{X}=\{(\bm{x}_i, y_i)|\bm{x}_i\in\mathbb{R}^{d\times1}, y_i\in\mathbb{R},i\in[n]\}$ the training set, where $\bm{x}_i$ and $y_i$ represent the $i$-th data and label.
$\mathbb{I}\{\cdot\}$ represents the indicator function that demonstrates the event occurrence, such that for event $\mathcal{A}$, $\mathbb{I}\{\mathcal{A}\}=1$ if and only if $\mathcal{A}$ happened, otherwise it equals to 0. $P(\mathcal{A})$ represents the probability of $\mathcal{A}$ occurred event.

\noindent\textbf{Neural networks and gram matrix.} For input $\bm{x}\in \mathbb{R}^{d\times1}$, weight vector $\bm{w} \in \mathbb{R}^{d\times1}$ in the weight matrix $\bm{W} \in \mathbb{R}^{d\times m}$, and output weight $\bm{a} \in \mathbb{R}^{m\times 1}$, we denote $f(\bm{W},\bm{a},\bm{x})$ as a neural network with a single hidden layer such that
% 
\begin{equation}\label{eq: neural}
f(\bm{W},\bm{a},\bm{x})=\frac{1}{\sqrt{m}}\sum\nolimits_{r=1}^m a_r\sigma(\bm{w}_r^T \bm{x})
\end{equation}
% 
where $\sigma$ is the activation function. In this paper, we primarily consider the ReLU function, which is one of the most adopted activation functions in the literature, \ie, $\sigma(z)=z\mathbb{I}\{z>0\}$.
%
Given a training set $\bm{X}$, the optimization goal is to minimize the empirical risk loss function 
% 
\begin{equation}\label{eq:loss}
\mathcal{L}(\bm{W},\bm{a})=\sum\nolimits_{i=1}^n\frac{1}{2}(f(\bm{W},\bm{a},\bm{x}_i)-y_i)^2
\end{equation}

In this work, we aim to construct a module-wise DAM $\bm{\Gamma} \in \mathbb{R}^{L\times N}$ with entries $\gamma_{i,j} \in \{0,1\}$ to determine the necessity of each module for fine-tuning in LLM, where $L$ is the number of layers and $N$ is the module amount in each layer. To further analyze the relationship between the model performance and the model with $\bm{\Gamma}$, we expand the definitions in \cref{eq: neural} and \cref{eq:loss} such that
% \red{
% We consider pruning the matrix $\bm{\Gamma}\in\mathbb{R}^{d\times m}$ with entries $\gamma_{ij}\in \{0, 1\}$.
% The corresponding network and loss function as follows
\begin{equation}
    \begin{aligned}
        f(\bm{W},\bm{a},\bm{x};\bm{\Gamma},\bm{W}_0)=\frac{1}{\sqrt{m}}\sum_{r=1}^m a_r\sigma((\bm{w}_0+\bm{\Gamma}\bm{w}_r)^T\bm{x})\\
     \mathcal{L}(\bm{W},\bm{a};\bm{\Gamma},\bm{W}_0)=\sum\nolimits_{i=1}^n\frac{1}{2}(f(\bm{W},\bm{a},\bm{x}_i;\bm{\Gamma},\bm{W}_0)-y_i)^2
    \end{aligned}
\end{equation}
% \begin{align}
%      f(\bm{W},\bm{a},\bm{x};\bm{\Gamma},\bm{W}_0)=\frac{1}{\sqrt{m}}\sum_{r=1}^m a_r\sigma((\bm{w}_0+\bm{\Gamma}\bm{w}_r)^T\bm{x})\\ \notag
%      \mathcal{L}(\bm{W},\bm{a};\bm{\Gamma},\bm{W}_0)=\sum\nolimits_{i=1}^n\frac{1}{2}(f(\bm{W},\bm{a},\bm{x}_i;\bm{\Gamma},\bm{W}_0)-y_i)^2
% \end{align}
% }
where $\bm{w}_i$ is the $i$-th row of $\bm{W}$.
Furthermore, we follow the definitions in \cite{dugradient} and define the matrices $\mathbf{H}^\infty_{\Gamma, w_0}$ and $\mathbf{H}^\infty_{w_0}$ based on $\bm{\Gamma}$ such that

\begin{definition}[Gram Matrix]\label{def: gram}
For a neural network with a single hidden layer, the gram matrix $\mathbf{H}^\infty_{\Gamma,w_0}\in\mathbb{R}^{n\times n}$ induced by the ReLU activation function on a training set $\bm{X}:=\{(\bm{x}_i,y_i)\}_{i=1}^{n}$ with entry
% 
\begin{equation}\label{eq: gram}
\begin{aligned}\relax
[\bm{H}_{\Gamma,w_0}^\infty]_{ij}
=&\mathbb{E}_{\bm{w}\sim N(\bm{0},\bm{I})}[\bm{x}_i^T\bm{x}_j\cdot\\
&\mathbb{I}\{(\bm{w}_0+\bm{\Gamma}\bm{w})^T\bm{x}_i\geq 0,(\bm{w}_0+\bm{\Gamma}\bm{w})^T\bm{x}_j\geq 0\}]\\
=&\bm{x}_i^T\bm{x}_j[\bm{I}^{\bm{\Gamma}\bm{w}}]_{ij}
\end{aligned}
\end{equation}
% 
We further construct $\mathbf{H}_{w_0}^\infty$ with entry $[\mathbf{H}_{w_0}^\infty]_{ij}$ such that 
% 
\begin{equation}
\begin{aligned}\relax
    [\bm{H}^\infty_{w_0}]_{ij} =& \mathbb{E}_{\bm{w}\sim N(\bm{0},\bm{I})}[\bm{x}_i^T\bm{x}_j\cdot\\
    &\mathbb{I}\{(\bm{w}_0+\bm{w})^T\bm{x}_i\geq 0,(\bm{w}_0+\bm{w})^T\bm{x}_j\geq 0\}]\\
    =&\bm{x}_i^T\bm{x}_j[\bm{I}^{\bm{w}}]_{ij}
\end{aligned}
\end{equation}
where $\bm{I}^{\bm{\Gamma w}}$ and $\bm{I}^{\bm{w}}$ are the expectations of the indicator matrices corresponding to vectors $\bm{\Gamma w}$ and $\bm{w}$, respectively.
% 
We denote $\lambda_0 := \lambda_{\min}(\mathbf{H}_{w_0}^\infty)$, and $\lambda_0^\Gamma:=\lambda_{\min}(\mathbf{H}_{\Gamma,w_0}^\infty)$.
\end{definition}

\noindent\textbf{Recall LoRA.} LoRA \cite{hu2022lora} utilizes two matrices $\bm{A} \in \mathbb{R}^{r \times k}, \bm{B} \in \mathbb{R}^{d \times r}$ to substitute the parameters' increments. For $\bm{h}=\bm{W}^{(0)} \bm{x}$, the forward pass in LoRA is

\begin{equation}
    \bm{h} = \bm{W}^0 \bm{x} + \Delta \bm{x} = \bm{W}^0 \bm{x} + \bm{B}\cdot \bm{A} \bm{x} 
\end{equation}
where $d,k \ll r$, $\bm{A}$ is usually initialized by following Gaussian distribution and $\bm{B}$ is initialized with zeros. LoRA adopts this modification equally to all the modules in the model. In the following, we denote $\Delta \bm{W}$ as $\bm{B}\cdot \bm{A}$.

\subsection{Main theorems}

\noindent\textbf{Technical intuitions.} In this section, we focus on analyzing the model performance regarding two aspects, \ie, the convergence rate and the generalization capability.
The existing schemes that adaptively adopt LoRA focus on adjusting the interior rank of the matrices based on some intuitive evaluation metrics. In contrast to these methods, we argue that adopting LoRA module-wisely in each layer is sufficient. Thus, our intuition in this work is to exert a binary matrix (\ie, $\bm{\Gamma}$) on the model structure to realize a ``selective'' fine-tuning of the model.
%
%As described before, 
%
%our method is established on the premise that fine-tuning an LLM is equivalent to the training of the low-rank decomposition matrices. This insightful perspective is proposed by \citet{hu2022lora}, which allows us to train dense layers in a neural network indirectly by optimizing rank decomposition matrices while keeping the pre-trained weights frozen. 
%
%Based on the observation that applying this fine-tuning operation to all modules equally is resource-consuming and might lead to model degradation, our intuition in this work is to exert a module-wise binary matrix on the model structure to realize a ``selective'' fine-tuning of the model. 
%
In this section, we first assume there exists an appropriate algorithm to construct $\bm{\Gamma}$ and explain why adopting it can lead to better model performance.

%To better illustrate the relationship between the selective matrix-equipped model and its performance, we start with an over-parameterized Neural Network (NN). 
Our analysis is established on the intuitive observations that fine-tuning a pre-trained model can be viewed as training a well-initialized model. 
%
%a pre-trained model can be viewed as 
%an intermediate state of training a model from scratch. 
%a well-initialized over-parameterized NN.
%, and ii) \tangyu{\sout{The network performance can be promoted if a larger amount of nodes in the hidden layer is activated.}} 
We construct the theories to explain how the $\bm{\Gamma}$ of a well-initialized over-parameterized Neural Network (NN) impacts the convergence rate and generalization capability. In the following section, we will first present the theorem regarding $\lambda_0$ and the model performance then demonstrate our main theorems.
%for convergence rate and generalization capability. 

\noindent\textbf{Theoretical results.} The over-parameterized NNs are widely adopted and competitive in hierarchical feature extraction due to the large number of parameters they contain.
%
We use the architecture with wide hidden layers in this section to establish our theories of the matrix $\mathbf{\Gamma}$. This network is one of the most fundamental structures of the over-parameterized NN and is proved to be tractable in training \cite{li2018visualizing, jiang2024meco}. Based on this insight, we present the following theorem \cite{dugradient} about the convergence rate of the NN with a single hidden layer as follows.

%One of the typical architectures is the networks with wide hidden layers, which is proved to be tractable in training \cite{li2018visualizing}. 
%
%In the previous subsection, we convert a multi-channel convolutional layer to a multi-sample fully-connected layer with constraints. 
%We further argue that if the number of hidden nodes in the transformed fully-connected layer is large enough, then it can be viewed as an over-parameterized NN layer. 
%Therefore, the characteristics of the over-parameterized NN can be transferred to CNN. 
%
% To this end, we present the  

\begin{theorem} \label{theorem: du}
If gram matrix $\bm{H}^\infty\succ 0$, $\|\bm{x}_i\|_2=1$, $|y_i|<C$ for some constant $C$ and $i\in[n]$, hidden nodes $m=\Omega\left( \frac{n^6}{\lambda_{\min}(\bm{H}^\infty)^4\delta^3}\right)$, and i.i.d. initialize $\bm{w}_r\sim N(\bm{0},\bm{I})$, $a_r\sim U\{[-1, 1]\}$ for $r\in[m]$, then with probability at least $1-\delta$ over the initialization, the following inequality holds:
% 
\begin{equation}
\begin{aligned}
    &\|f(\bm{W}(t),\bm{a},\bm{X})-\bm{y}\|_2^2\\
    \leq& \exp{(-\lambda_{\min}(\bm{H}^\infty) t)}\|f(\bm{W}(0),\bm{a},\bm{X})-\bm{y}\|_2^2
\end{aligned}
\end{equation}
where $$\bm{H}^\infty:=\mathbb{E}_{\bm{w}\sim N(\bm{0},\bm{I})}[\bm{x}_i^T\bm{x}_j\mathbb{I}\{(\bm{w}^T\bm{x}_i\geq 0,\bm{w}^T\bm{x}_j\geq 0\}].$$
% 
\end{theorem}

The inequality in the above theorem demonstrates that the minimum eigenvalue of the Gram matrix positively affects the training convergence rate of the network. Thus, it is viable for us to evaluate the convergence rate of a network from the minimum eigenvalue of the Gram matrix. %\haodi{Furthermore, the convergence rate of the network is independent of the label $\bm{y}$.}

% We further point out that the convergence rate of the network is independent of the label 

Based on Theorem \ref{theorem: du}, we analyze the relationship of the minimum eigenvalue between the adaptive matrix (\ie, $\lambda^\Gamma_0$) and the original matrix (\ie, $\lambda_0$). 
%and conclude that under certain pruning conditions, the pruned model has a higher convergence rate and generalization ability.
We summarize our results in the following theorem: 

\begin{theorem}\label{theorem:convergence}
    Suppose $f$ is an NN with a single hidden layer and ReLU activation function. 
    Assume $\bm{X}\in \mathbb{R}^{d\times n}$, $\bm{w}(0)\sim N(\bm{0},\bm{I})$, hidden nodes $m=\Omega\left(\frac{n^6d^2}{(\lambda_0^\Gamma)^4\delta^3}\right)$, and $\bm{I}^{\bm{\Gamma w}}-\bm{I}^{\bm{w}}\succeq 0$, then the following formula holds with probability at least $1-\delta$ over the initialization
    %
    \begin{equation}
    \begin{aligned}
        &\|f(\bm{W}(t),\bm{a},\bm{X};\bm{\Gamma},\bm{W}_0)-\bm{y}\|_2^2\\
        \leq& \exp(-\lambda_0^\Gamma t)\|f(\bm{W}(0),\bm{a},\bm{X};\bm{\Gamma},\bm{W}_0)-\bm{y}\|_2^2\\
    \end{aligned}
    \end{equation}
    where $\lambda_0^\Gamma\geq \lambda_0$. 
\end{theorem}

\emph{Proof sketch.} The key of the proof is to find the relationship between $\lambda_0$ and $\lambda_0^\Gamma$ by
% \tangyu{\sout{Hoeffding's inequality and }}
Weyl inequalities \cite{horn2012matrix}. We provide the full proof in \cref{app:sub:th2}. 

% \tangyu{\sout{Remark that in Theorem \ref{theorem:convergence}, we regulate that the selective matrix $\mathbf{\Gamma}$ is capable of activating more nodes in the hidden layer, \ie, $P((\gamma_r w_r)^Tx\geq 0)>P(w_r^T x\geq 0)$ under ReLU activation functions. }}
%
Assume that we can establish a matrix that satisfies this requirement, then Theorem \ref{theorem:convergence} demonstrates that the minimum eigenvalue of the Gram matrix of the network with $\mathbf{\Gamma}$ is larger than the $\lambda_0$ without the selective matrix, thus leading to a higher convergence rate. 

% shows that $\lambda_0^\gamma$ can achieve a higher convergence rate.

Other than the convergence rate, we also analyze the relationship between $\lambda_0^\Gamma$ and the \textit{generalization capability} of the over-parameterized NN. We present the results in the following theorem:

\begin{theorem}\label{theorem: genera}
    For an over-parameterized neural network with the loss on the testing set as $\mathcal{L}(\bm{W}, \bm{a};\bm{\Gamma},\bm{W}_0)$. Let $\bm{y}=(y_1, ..., y_N)^T$, and $\eta = \kappa C_1\sqrt{\bm{y}^T(\bm{H}_{\Gamma,w_0}^\infty)^{-1}\bm{y}}/(m\sqrt N)$ for some small enough absolute constant $\kappa$, where $\eta$ denotes the step of SGD. Under the assumption of Theorem \ref{theorem:convergence}, for any $\delta\in(0,e^{-1}]$, there exists $m^\ast(\delta,N,\lambda_0^\Gamma)$, such that if $m\geq m^*$, then with probability at least $1-\delta$, we have
%
    \begin{equation}
    \begin{aligned}
        \mathbb{E}[L(\bm{W},\bm{a};\bm{\Gamma},\bm{W}_0)]\leq&  \mathcal{O}(C'\sqrt{\frac{\bm{y}^T \bm{y}}{\lambda_0^\Gamma N}})+ \mathcal{O}(\sqrt{\frac{\log(1/\delta)}{N}})\\
        % \leq& \mathcal{O}(C'\sqrt{\frac{\bm{y}^T \bm{y}}{\lambda_0 N}})+ \mathcal{O}(\sqrt{\frac{\log(1/\delta)}{N}})
    \end{aligned}
    \end{equation}
    where $\lambda_0^\Gamma>\lambda_0$, $C, C'$, and $\delta$ are constants. 
\end{theorem}

\emph{Proof sketch.} The proof of the above theorem derives from Corollary 3.10 of \cite{cao2019generalization} and Section D.2 of \cite{zhu2022generalization}. We present the detailed proof in \cref{app:sub:th3}. 

Similarly to Theorem \ref{theorem:convergence}, the above theorem indicates that the adoption of $\bm{\Gamma}$ can enhance the generalization capability of the model, as shown in the last inequality. 

Overall, in this section, we have proven that a selective matrix $\bm{\Gamma}$ will result in a higher convergence rate and better generalization capability of the network, thus leading to enhanced model performance. The main theorems in \cref{theorem:convergence} and \cref{theorem: genera} theoretically grounded the effectiveness of the selective matrix. The question remains is how to construct $\bm{\Gamma}$ for the network efficiently. We will provide solutions and detailed algorithms in the next section.  

% show that if proper pruning makes more units in the network activated, the network will have a higher convergence rate and generalization ability. 
% This reflects the importance and necessity of pruning in neural networks.


\section{Design of DiffoRA}

We now present the concrete method to construct the module-wise selective matrix $\mathbf{\Gamma}$. The proposed approach is called DiffoRA, which is built atop the DAM (\ie, \underline{D}ifferentiable \underline{A}daptation \underline{M}atrix).
%
% continuous relaxation and discretization methods. 
%
To capture the information of the incremental updates modeled by the low-rank decomposition matrices, DiffoRA views the elements $\gamma$ in the module-wise DAM $\bm{\Gamma}$ as trainable parameters.
%DiffoRA can efficiently optimize the hyperparameters of  in , thus liberating the fine-tuning from unnecessary and resource-consuming procedures.
More concretely, as shown in \cref{fig: difforaframe}, DiffoRA approximates the NP-hard problem by invoking the following two stages: (i) Relaxation and optimization, which aims to map $\bm{\Gamma}$ to a continuous space (\ie, $\bar{\bm{\Gamma}}$) so that it is differentiable and can be further optimized; and (ii) Discretization and fine-tuning, which binarizes $\bar{\bm{\Gamma}}$ to determine the most essential module for fine-tuning. We will first elaborate on each stage respectively, and then introduce the weight-sharing optimization.
% The above theorem and analysis indicate the following key insights: selecting a suitable alpha can make the neural network have a higher convergence rate and generalization ability.
% This suggests that we can achieve better performance by training with fewer parameters.

% You must include your signed IEEE copyright release form when you submit your finished paper.
% We MUST have this form before your paper can be published in the proceedings.

% Please direct any questions to the production editor in charge of these proceedings at the IEEE Computer Society Press:
% \url{https://www.computer.org/about/contact}.

% To this end, we propose DiffoRA, an algorithm that continuously relaxes and optimize the hyperparameters $\bm{\gamma}$. 
% Our method contains two steps:  


\subsection{Continuous relaxation}

\begin{algorithm}[tb]
\caption{DiffoRA}
\label{alg: difflora}
\begin{algorithmic}[1] 
\REQUIRE Pre-trained model with $L$ layers $\mathcal{M}^L$; Candidate fine-tuning modules $M$, where $|M|=L\times N$, $N$ is the number of candidate modules in each layers; Training dataset and valid dataset $\bm{X}_{train}$ and $\bm{X}_{valid}$; Training epochs and valid epochs, $T$ and $V$; The learning rate $\eta$; Sample rate $\rho$; LoRA rank: $r_l$; Share rank: $r_s$.
\STATE // Stage 1: Continuous Relaxation
\STATE Create the hyperparameters $\bar{\bm{\Gamma}}\in\mathbb{R}^{L\times N}$.
\FOR{$v=1;v<V;v++$}
\STATE Update hyperparameters $\bar{\bm{\Gamma}}$ as 
$
    \bar{\bm{\Gamma}}_v=\bar{\bm{\Gamma}}_v- \eta \nabla_{\bar{\bm{\Gamma}}}\mathcal{L}_{valid}(\bm{X}_{valid}; \Delta \bm{W}, \bar{\bm{\Gamma}}_{v-1})
$
\FOR{$t=1;t<T;t++$}
\STATE Update low-rank matrix $\Delta \bm{W}$ as follow:
$$
    \Delta \bm{W} = \Delta \bm{W} - \eta \nabla_{\Delta \bm{W}} \mathcal{L}_{train}(\bm{X}_{train};\Delta \bm{W}, \bar{\bm{\Gamma}}_v)
$$
\ENDFOR
\ENDFOR

\STATE // Stage 2: Discretization and Fine-Tuning
\STATE Select the Top-K modules of each layer in $\mathcal{M}^L$ according to the $\bar{\bm{\Gamma}}$, where $K=\lfloor\rho \cdot N\rfloor$.
\STATE Add a low-rank matrix $\Delta \bm{W}=\bm{B A}$ to the selected module and add the weight sharing matrix $\Delta \bm{W}_s = \bm{B}_s \bm{A}_s $ to the remaining modules, where $\bm{B}^T, \bm{A}\in\mathbb{R}^{d\times r_l}$, and $\bm{B}^T_s, \bm{A}_s\in \mathbb{R}^{d\times r_s}$.
\FOR{$t=1;t<T;t++$}
\STATE Update low-rank matrix $\Delta \bm{W}$ as follow:
$$
    \Delta \bm{W} = \Delta \bm{W} - \eta \nabla_{\Delta \bm{W}} \mathcal{L}_{train}(\bm{X}_{train};\Delta \bm{W})
$$

\ENDFOR
\RETURN Fine-tuned model $\mathcal{M}^L$.
\end{algorithmic}
\end{algorithm}

% \noindent\textbf{Relaxation and optimization}. 
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figure/overview.pdf}
    \caption{The framework of DiffoRA, contains two stages. In stage one (left part), the initialized adaptive matrix is continuously relaxed, \ie, C.R. in the figure. The DAM is then differentiable and can be updated. In stage two (right part), the obtained DAM is discretized to binary. All the modules corresponding to entry one will be fine-tuned using decomposition matrices.}
    \label{fig: difforaframe}
\end{figure*}

The first stage of DiffoRA is illustrated in the left part of \cref{fig: difforaframe}.
In contrast with the existing work, we point out that it is unnecessary to allocate the rank for every module. 
Instead, we can construct a selective matrix called DAM module-wisely that determines which module needs to be fine-tuned and which need not. 
%As described before, the goal of our method is to construct a DAM to determine which module in the LLM is more suitable and necessary for fine-tuning.
The final output DAM $\bm{\Gamma}$ should be a binary matrix, in which the ``ones'' (\textit{resp.} ``zeros'') indicate that the corresponding entries will (\textit{resp.} will not) be fine-tuned in the following procedures. 
However, directly generating a binary matrix is non-trivial and lacks foundation, \ie, it is an NP-hard problem.
To this end, in our design of DiffoRA, we first relax the range of the elements in $\bm{\Gamma}\in \{0, 1\}^{L\times N}$ to $\bm{\bar{\Gamma}}\in [0,1]^{L\times N}$ continuously and view all the row vectors in $\bar{\bm{\Gamma}}$ as the hyperparameters, which can be differentiated and updated. 
%We concretely defined the forward propagation and loss functions of the network integrated with $\bm{\bar{\Gamma}}$ to obtain the  in the continuous space. 

More specifically, 
%assume that each layer of the training model has $N$ candidate modules.
%In order to select the most suitable module for fine-tuning in the pre-trained model, we first relax the row vectors $\bm{\gamma}$ continuously and map it to the continuous space.
%Specifically, 
for the collection of the modules in the $i$-th layer, we utilize the row vector $\bar{\bm{\gamma}}^i\in [0,1]^{1\times N}$ of $\bm{\bar{\Gamma}}$ as learnable hyperparameters in continuous space.
%to relax the DAM $\mathbf{\Gamma}$. 
The forward pass of the relaxed LoRA-based fine-tuning formula can then be defined as:
% 
\begin{equation}\label{con-relax}
    h_j^i=\bm{W}_j^i \bm{x}+\bar{\gamma}_j^i \Delta \bm{W}_j^i \bm{x}=\bm{W}_j^i \bm{x}+\bar{\gamma}_j^i \bm{B}_j^i \bm{A}_j^i \bm{x},
\end{equation}
where $h_j^i$ denotes the $j$-th item of the hidden nodes in the $i$-th layer, and each $\bar{\gamma}_j^i$ in $\bar{\bm{\gamma}}^i$ satisfies
% 
\begin{equation}
    \bar{\gamma}_j^i=\frac{\exp(\gamma_j^i)}{\sum_{j\in[N]}\exp(\gamma_j^i)},
\end{equation}
$\bm{W}_j^i\in\mathbb{R}^{d\times k}$ is the pre-trained weight matrix of module $m_j^i$.
$\bm{B}_j^i$ and $\bm{A}_j^i$ are $j$-th low-rank decomposition matrices in the $i$-th layer.

Intuitively, $\bm{\bar{\Gamma}}$ can represent the necessities of fine-tuning this module using $\mathbf{A}$ and $\mathbf{B}$. %LoRA-based fine-tuned for this module. 
When this weight tends to $1$, it indicates that fine-tuning this module is necessary and might lead to better performance, and vice versa. 
By utilizing the continuous relaxation on the differential matrix, we are able to generate each $\bar{\bm{\gamma}}^i$ via a few rounds of training (\eg, five rounds, determined by the datasets).  
% , such that $\sum_{j=1}^N\gamma_j^i=1$. 
%
We denote $\mathcal{L}_{train}$ and $\mathcal{L}_{valid}$ as the training and validation loss, both of which are functions of $\Delta \bm{W}$ and $\bar{\bm{\Gamma}}$ such that
% Specifically, the loss functions have the following formulation:

\begin{equation}
    \mathcal{L}_{\cdot} := \mathcal{L}_{\cdot}(\Delta \bm{W}, \bar{\bm{\Gamma}})
\end{equation}
%where $\Delta \bm{W} = \bm{B}\cdot \bm{A}$ and $\bar{\bm{\gamma}}$ are the trainable parameters.

Our goal is to find the best hyperparameters $\bar{\bm{\Gamma}}$ to minimize the validation loss, which is equivalent to bi-level optimization problems as follows:

\begin{equation}\label{eq:opt}
    \begin{aligned}
        \min_{\bar{\bm{\Gamma}}} \quad&\mathcal{L}_{valid}( \Delta \bm{W}^\ast, \bar{\bm{\Gamma}})\\
        s.t.\quad &\Delta \bm{W} ^{\ast}=\mathop{\arg\min}_{\Delta \bm{W}}\mathcal{L}_{train}(\Delta \bm{W}, \bar{\bm{\Gamma}})
    \end{aligned}
\end{equation}

To solve the optimizations in \cref{eq:opt}, we use the gradient descent algorithm to update the parameters $\bm{A}$, $\bm{B}$ and $\bar{\bm{\Gamma}}$. 
Specifically, we randomly extract part of the data in the training set and divide it into two parts, \ie, the training data and validation data, which are used to update $\Delta \bm{W}$ and $\bar{\bm{\Gamma}}$ alternately. We present the detailed training algorithms in \cref{alg: difflora}, lines 1 to 8. Note that during this step, the remaining network parameters are kept frozen as pre-trained. 


\subsection{Discretization and fine-tuning}

Having the relaxed matrix $\bar{\bm{\Gamma}}$ in the previous stage, we perform the discretization to obtain the binary DAM $\bm{\Gamma}$. 
An intuitive representation of this procedure is shown in the right part of \cref{fig: difforaframe}.
More concretely, 
for each item in $\bar{\bm{\Gamma}}$, we update the top $K := \lfloor\rho \cdot N\rfloor$ largest entries with 1 and set the remaining values as 0, such that

\begin{equation}
    \gamma^i_{j}=
   \begin{cases}
   1 & \mbox{if $\bar{\gamma}^i_j \ge \delta^i$}\\
   0 & \mbox{otherwise}
   \end{cases}
\end{equation}
where $\delta^i$ is the value of the $K$-th largest entry in $\bar{\bm{\gamma}}^i$ and $\rho$ is the selecting ratio. In our design, $\rho$ is a hyperparameter which is set as 0.5 for fair comparison with the baselines.
The discretization is described in \cref{alg: difflora}, lines 10 and 11.
%for each layer of the pre-trained model, we retain the Top-$K$ modules that acquire the highest $\gamma_i$ during continuous relaxation, indicating that they are the most suitable modules for fine-tuning to enhance performance. 
% In the meantime, we leave the remaining modules unchanged. 
%In order to have a better comparison with the baseline methods, we choose the ratio $\rho$ to be $0.5$.
% More concretely, for the obtained $\mathbf{\Gamma}$,  
%value greater than  according to the $\bar{\bm{\gamma}}$, reset the Top-K largest values to $1$, indicating that the modules need to be fine-tuned, and set the remaining values to $0$.
The final step is to fine-tune the model equipped with the binarized DAM and low-rank decomposition matrices. 
During the fine-tuning, only the modules with weight one (\eg, the selected module) will be fine-tuned, while the others are kept unchanged in the downstream tasks. 


%The output of the discretization is a LoRA-equipped pre-trained model. We re-train the modules with the corresponding entries in $\mathbf{\Gamma}$ is 1 and keep the remaining frozen in the downstream tasks. 
% After discretization, we get a LoRA-equipped pre-train model. 
% We fine-tune the selected modules in downstream tasks under the LoRA setting.


\subsection{Optimization and weight sharing}

In the previous section, we obtain the DAM $\bm{\Gamma}$ using continuous relaxation and discretization. This approach can significantly improve the model performance after fine-tuning when the discrepancy of the weights in $\bm{\bar{\Gamma}}$ is distinct. However, when the entries in the relaxed $\bm{\bar{\Gamma}}$ display a uniform distribution, the method in the previous descriptions can be further optimized. 
More concretely, as shown in \cref{fig: module weight}, we take the continuous DAM on CoLA and MRPC \cite{wang2018glue} as examples. We visualize matrix $\bar{\bm{\Gamma}}$ on these two datasets with DeBERTaV3-base \cite{he2023debertav} as the backbone, respectively, in which each column is a layer with six trainable modules. 
It can be seen that the weight distribution on CoLA is relatively distinct, \ie, in the majority of the layers, $W_O$ and $W_Q$ obtain significantly higher weights so that they are more suitable for the following fine-tuning. 
In contrast, the entries of the $\bm{\bar{\Gamma}}$ on MRPC share a similar and uniform distribution. For instance, all the candidate modules obtain weights around 0.15 from layers 1 to 8. Under this circumstance, if we select the top-$K$ largest entries, the modules that correspond to the ``0'' entries after discretization also demonstrate the same level of importance as those selected by ones. Consequently, directly fixing the modules with zero entries in $\mathbf{\Gamma}$ might lead to performance degradation.
%For instance, \cref{fig: module weight}, we visualize the distribution of the selective matrix of the pre-trained networks in QNLI and RTE datasets.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/heatmaps-2_2.pdf}
    \caption{The module weights of the DeBERTaV3-base model in the QNLI and RTE datasets. In the figure, $W_Q$, $W_K$, $W_V$, $W_I$, $W_O$ and $W_D$ correspond to the $\mathsf{query\_proj}$, $\mathsf{key\_proj}$, $\mathsf{value\_proj}$, $\mathsf{intermediate.dense}$, $\mathsf{output.dence}$, and $\mathsf{attention.output.dense}$ modules in the pre-trained model respectively.}
    \label{fig: module weight}
\end{figure*}

%In this section, we analyze the distribution of module importance of pre-trained networks in QNLI and RTE datasets.
%In DeBERTaV3-base, we set each layer to have 6 candidate modules, namely, $\mathsf{query\_proj}$, $\mathsf{key\_proj}$, $\mathsf{value\_proj}$, $\mathsf{intermediate.dense}$, $\mathsf{output.dence}$ and $\mathsf{attention.output.dense}$ modules, which we denote as $W_Q$, $W_K$, $W_V$, $W_I$, $W_O$ and $W_D$ respectively.
%We visualize the weight of each module, as shown in the \cref{fig: module weight}.
%We found that not all network modules are suitable for fine-tuning at each layer of the network. For example, in the QNLI dataset, the $W_D$ module has high weights in layers 1-10, and the $W_V$ module has high weights in layers 11-12, and in the RTE dataset, $W_Q$ has high weights in layers 1-11.

To address this discretization discrepancy issue, we adopt the weight-sharing strategy to further optimize our method. Specifically, for the modules corresponding to the zero entries after discretization, instead of just freezing them without fine-tuning, we fine-tune them with the same weights as the modules in other layers. 
In other words, those modules with zero DAM entries share the same model weights in the fine-tuning procedure. For instance, all the modules $\bm{W}_i$ in layers 1 to 12 that are not selected will share the same weights and also participate in fine-tuning, where $\bm{W}_i \in \{\bm{W}_Q, \bm{W}_K, \bm{W}_V, \bm{W}_I, \bm{W}_O, \bm{W}_D\}$. This optimization can enhance the performance of DiffoRA without introducing a large amount of extra fine-tuning parameters. The model fine-tuning with weight-sharing is shown in \cref{alg: difflora}, lines 12 to 15.

