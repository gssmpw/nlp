\section{Related work}
\label{sec: related work}

\subsection{LLM and fine-tuning}
LLM has captured considerable public interest due to its success in multiple regions. The PEFT is essential for LLMs due to the huge amount of parameters.  
Some previous works have been proposed to fine-tune the LLMs using specifically designed modules that are added to LLMs. The fine-tuning of LLMs is thus converted to the adjustments of these small modules. For instance, multiple methods  \cite{rebuffi2018efficient, liu2022ptuning, lester2021power} insert dataset-dependent small modules or vectors between the layers to decrease the parameter amounts. 
%
Another line of work models the incremental updates of the fine-tuning procedure to make it parameter-efficient. \citet{guo2021parameter} propose to use a task-specific difference vector to extend and fine-tune the base model. There are also some methods that fine-tune parts of 
the parameters \cite{gui2023hifi}, \eg, the bias of FFN \cite{zaken2022bitfit} or the last quarter \cite{lee2019would}. 

\subsection{Low-rank adaptation and optimizations}

To further reduce the computational and storage cost, \citet{hu2022lora} proposed LoRA, in which they designed a low-rank decomposition matrix to model the incremental updates during fine-tuning. 
%Compared to the previous work, LoRA incurs significantly fewer parameters and thus is comprehensively adopted.  
%
A plethora of work has been proposed to optimize LoRA and reduce the parameter amount \cite{kopiczko2024vera, hayou2024, liu2024dora, dettmers2024qlora, renduchintala2024tied}. One of the limitations of LoRA is that it treats all the modules equally, which omits the variations of the modules in LLMs. 
To address this issue, a few works have been proposed to realize an adaptive LoRA.
Without loss of generality, these methods first evaluate the importance of the modules and then adjust the interior rank of the decomposition matrices accordingly.
For instance, AdaLoRA \cite{zhang2023adaptive} utilizes the singular value to score the importance, while some other approaches adopt the Frobenius norm \cite{mao2024doraadaptive} or norm of the outputs \cite{zhou2024loradrop} as the metrics. 
These indicators are intuitively adopted and only utilize partial information of the incremental updates. 
There are also a few works that utilize the training procedures to determine the module importance \cite{ding2023sparse, zhang2024autolora, liu2024alora}, yet they still focus on modifying the interior ranks of the decomposition matrices. 

%SoRA \cite{}, on the other hand, prunes the low-rank matrices using the gate units and proximal gradient methods.
%Similar to these works, we also manage to evaluate the importance of the modules and selectively adopt the low-rank decomposition matrices. 
