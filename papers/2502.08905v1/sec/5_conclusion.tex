\section{Conclusion}

%In this paper, w
We propose a new PEFT method called DiffoRA, which enables efficient and adaptive LLM fine-tuning based on LoRA. 
Instead of adjusting every interior rank, 
%of the decomposition matrices 
%of all modules, 
we argue that adopting LoRA module-wisely is sufficient. 
To achieve this, we construct a DAM to select the modules that are most suitable and essential to fine-tune. We theoretically analyze how the DAM impacts the convergence rate and generalization capability.
%of the pre-trained model. 
Furthermore, we adopt continuous relaxation and discretization to establish DAM.
%for each task. 
To alleviate the issue of discretization discrepancy, we utilize the weight-sharing strategy for optimization. 
%We fully implement our method and t
The experimental results demonstrate that our DiffoRA works consistently better than the baselines across all benchmarks. 