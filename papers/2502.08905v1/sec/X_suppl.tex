\clearpage
\setcounter{page}{1}
\maketitlesupplementary



\section{Detailed Proofs}
\label{app:proof}

\subsection{Proof of Theorem \ref{theorem:convergence}}
\label{app:sub:th2}

We first introduce the Wely inequality as follows.


\begin{lemma}[Weyl inequality \cite{horn2012matrix}]\label{weyl} Let $\bm{A}$, $\bm{B}\in\mathbb{R}^{n\times n}$ be Hermitian matrices, and let the eigenvalues of $\bm{A}$, $\bm{B}$, and $\bm{A} + \bm{B}$ be $\{\lambda_i(\bm{A})\}_{i=1}^n$, $\{\lambda_i(\bm{B})\}_{i=1}^n$ and $\{\lambda_i(\bm{A}+\bm{B})\}_{i=1}^n$, respectively. The eigenvalues of each matrix are arranged in ascending order. Then we have

\begin{equation}\label{weyl:1}
    \lambda_i(\bm{A}+\bm{B})\leq \lambda_{i+j}(\bm{A})+\lambda_{n-j}(\bm{B}),\quad j=\{0,1,\ldots,n-i\}
\end{equation}
%
for each $i\in [n]$, with equality for some pair $i,j$ if and only if there is a nonzero vector $\bm{x}$
such that $\bm{A}\bm{x}=\lambda_{i+j}(\bm{A})\bm{x}$, $\bm{B}\bm{x}=\lambda_{n-j}(\bm{B})\bm{x}$, and $(\bm{A}+\bm{B})\bm{x}=\lambda_{i}(\bm{A}+\bm{B})\bm{x}$. Also,

\begin{equation}\label{weyl:2}
    \lambda_{i-j+1}(\bm{A})+\lambda_j(\bm{B})\leq \lambda_{i}(\bm{A}+\bm{B}),\quad j=\{1,\ldots, i\}
\end{equation}
%
for each $i \in [n]$, with equality for some pair $i, j$ if and only if there is a nonzero vector $\bm{x}$ such that $\bm{A}\bm{x}=\lambda_{i-j+1}(\bm{A})\bm{x}$, $\bm{B}\bm{x} =\lambda_j (\bm{B})\bm{x}$, and $(\bm{A} + \bm{B})\bm{x} = \lambda_i (\bm{A} + \bm{B})\bm{x}$. 
If $\bm{A}$ and $\bm{B}$ have no common eigenvector, then inequality (\ref{weyl:1}) and (\ref{weyl:2}) are strict inequality. 
\end{lemma}
% 

We first present and proof the following lemma.

\begin{lemma}
If $\bm{x}_i\nparallel \bm{x}_j, \forall i\neq j$, we have $\bm{H}^\infty_{w_0}\succ 0$. 
\end{lemma}

\begin{proof}
By the Lemma 3.4 of \cite{dugradient}, there exists $\bm{w}\sim\mathcal{N}(\bm{0},\bm{I})$, such that when $m$ is sufficiently large, $\|\bm{w}-\bm{w}_0\|$ is sufficiently small.
Then according to the proof of Theorem 3.1 of \cite{dugradient}, we get $\lambda_{\min}(\bm{H}_{w_0}^\infty) > 0$.
    % In this paper, we regard the weights of the pre-trained network as the weights obtained by training the network in the initialized state for $t_0$ epochs.
    % Then according to Theorem 3 and Lemma 3.3 of \cite{dugradient}, for an over-parameterized NN, we have
    % \begin{equation}
    %     \lambda_{\min}(\bm{H}(t_0))\geq \frac{1}{2}\lambda_{\min}(\bm{H}^\infty)>0
    % \end{equation}
    % where 
    % \begin{equation}
    %     [\bm{H}(t_0)]_{ij}=\frac{1}{m}\bm{x}_i^T\bm{x}_j\sum_{r=1}^m\mathbb{I}\{\bm{w}_r^T(t_0)\bm{x}_i\geq 0,\bm{w}_r^T(t_0)\bm{x}_j\geq 0\}
    % \end{equation}
    % We define the empirical matrix with $t>t_0$ as follow

    % \begin{equation}
    % \begin{aligned}
    %     &[\bm{H}_{w_0}(t)]_{ij}:=\frac{1}{m}\bm{x}_i^T\bm{x}_j \cdot \\
    %     &\sum_{r=1}^m\mathbb{I}\{(\bm{w}_0+\bm{w}_r(t))^T\bm{x}_i\geq 0,(\bm{w}_0+\bm{w}_r(t))^T\bm{x}_j\geq 0\}
    % \end{aligned}
    % \end{equation}
    % where $\bm{w}_r(0)\sim \mathcal{N}(\bm{0},\bm{I})$, and $w_0=\bm{w}_r(t_0)$.

    % We define the event
    % \begin{equation}
    % \begin{aligned}
    %     \mathcal{A}_{ir}=&\{\exists \bm{w}_r(0):\|\bm{w}_r(0)-\bm{w}_0\|\leq R,\\
    %     &\mathbb{I}\{\bm{x}_i^T(\bm{w}_0+\bm{w}_r(0))\geq 0\}\neq \mathbb{I}\{\bm{x}_i^T\bm{w}_0\geq 0\}\}
    % \end{aligned}
    % \end{equation}
    % where $R:=\frac{c\delta\lambda_0}{n^2}$ for some small positive constant $c$. 
    % This event happens if and only if $|\bm{w}_r(0)^T\bm{x}_i|\leq R$ and $|\bm{w}_0^T\bm{x}_i|\leq R$, which is equivalent to $m$ is large enough (see Lemma 3.4. of \cite{dugradient}).
    % Then we have $P(\mathcal{A}_{ir})=P_{z\sim N(0,1)}(|z|<R)\leq\frac{2R}{\sqrt{2\pi}}$.
    % Thus,
    % \begin{equation}
    %     \begin{aligned}
    %         &\mathbb{E}[|[\bm{H}(t_0)]_{ij}-\bm{H}_{w_0}(0)|]\\
    %         =&\mathbb{E}[\frac{1}{m}|\bm{x}_i^T\bm{x}_j\cdot\\
    %         &\sum_{r=1}^m(\mathbb{I}\{(\bm{w}_0+\bm{w}_r(t))^T\bm{x}_i\geq 0,(\bm{w}_0+\bm{w}_r(t))^T\bm{x}_i\geq 0\}\\
    %         &-\mathbb{I}\{\bm{w}_0^T\bm{x}_i\geq 0,\bm{w}_0^T\bm{x}_j\geq 0\})]\\
    %         &\leq\frac{1}{m}\sum_{r=1}^m\mathbb{E}[\mathbb{I}\{\mathcal{A}_{ir}\cup\mathcal{A}_{jr}\}]\leq \frac{4R}{\sqrt{2\pi}}
    %     \end{aligned}
    % \end{equation}

    % Thus, we have $\mathbb{E}\left[\sum_{i,j}|[\bm{H}_{w_0}(0)]_{ij}-[\bm{H}(t_0)]_{ij}|\right]\leq\frac{4n^2R}{\sqrt{2\pi}}$. By Markov's inequality, with probability $1-\delta$, we have $\sum_{ij}|[\bm{H}_{w_0}(0)]_{ij}-[\bm{H}(t_0)]_{ij}|\leq\frac{4n^2R}{\sqrt{2\pi}\delta}$.
    % With the matrix perturbation theory, we have
    % \begin{equation}
    %     \begin{aligned}
    %         &\|\bm{H}_{w_0}(0)-\bm{H}(t_0)\|_2            \leq \|\bm{H}_{w_0}(0)-\bm{H}(t_0)\|_F \\
    %         \leq& \sum_{i,j}|[\bm{H}_{w_0}(0)]_{ij}-[\bm{H}(t_0)]_{ij}| \leq \frac{4n^2R}{\sqrt{2\pi}\delta}
    %     \end{aligned}
    % \end{equation}

    % We further have
    % \begin{equation}
    %     \lambda(\bm{H}_{w_0}(0))\geq \lambda_{\min}(\bm{H}(t_0))-\frac{4n^2R}{\sqrt{2\pi}\delta}\geq\frac{\lambda_0}{2}.
    % \end{equation}

    % By Lemma 3.2 and 3.4 of \cite{dugradient}, for $m$ large enough, we have
    % \begin{equation}
    %     \lambda_{\min}(\bm{H}_{w_0}(0))\geq\lambda_{\min}(\bm{H}(t_0))-\frac{4n^2R}{\sqrt{2\pi}\delta}\geq \frac{\lambda_{\min}(\bm{H}^\infty)}{4}
    % \end{equation}

    % By Hoeffding inequality, we have with probability $1-\delta'$,
    % \begin{equation}
    %     |[\bm{H}_{w_0}(0)]_{ij}-[\bm{H}^\infty_{w_0}]_{ij}|\leq\frac{2\sqrt{\log(1/\delta')}}{\sqrt{m}}
    % \end{equation}

    % We set $\delta'=n^2\delta$ and get
    % \begin{equation}
    %     |[\bm{H}_{w_0}(0)]_{ij}-[\bm{H}^\infty_{w_0}]_{ij}|\leq\frac{2\sqrt{\log(n/\delta)}}{\sqrt{m}}
    % \end{equation}

    % Thus, with probability at least $1-\delta$, we have
    % \begin{equation}
    %     \|\bm{H}_{w_0}(0)-\bm{H}^\infty_{w_0}\|_2^2\leq\|\bm{H}_{w_0}(0)-\bm{H}^\infty_{w_0}\|_F^2\leq\frac{16n^2\log(n/\delta)}{m}
    % \end{equation}

    % We choose $m=\Omega\left(\frac{n^2\log(n/\delta)}{\lambda_0^2}\right)$, and by matrix pertubation theory, we get
    % \begin{equation}
    %     \lambda_{\min}(\bm{H}_{w_0}^\infty)\succ 0.
    % \end{equation}
\end{proof}


Then we provide the proof of Theorem 2.

\noindent\textbf{Theorem 2.} Suppose $f$ is an NN with a single hidden layer and ReLU activation function. 
    Assume $\bm{X}\in \mathbb{R}^{d\times n}$, $\mathbf{w}(0)\sim N(\mathbf{0},\mathbf{I})$, hidden nodes $m=\Omega\left(\frac{n^6d^2}{(\lambda_0^\Gamma)^4\delta^3}\right)$, and $\bm{I}^{\bm{\Gamma w}}-\bm{I}^{\bm{w}}\succeq 0$, then the following formula holds with probability at least $1-\delta$ over the initialization
    %
    \begin{equation}
    \begin{aligned}
        &\|f(\mathbf{W}(t),\mathbf{a},\mathbf{X};\bm{\Gamma},\bm{W}_0)-\bm{y}\|_2^2\\
        \leq& \exp(-\lambda_0^\Gamma t)\|f(\mathbf{W}(0),\mathbf{a},\mathbf{X};\bm{\Gamma},\bm{W}_0)-\bm{y}\|_2^2\\
        % \leq & \exp(-\lambda_0 t)\|f(\mathbf{W}(0),\mathbf{a},\mathbf{X})-\bm{y}\|_2^2
    \end{aligned}
    \end{equation}
    where $\lambda_0^\Gamma\geq\lambda_0$. 

\begin{proof}
    We denote $\bm{I}^-:=\bm{I}^{\bm{\Gamma w}}-\bm{I}^{\bm{w}}\succeq 0$.
    Then we have the following inequalities:
    \begin{equation}
        \begin{aligned}
            \lambda_{\min}(\bm{H}^\infty_{\Gamma,w_0})&=\lambda_{\min}(\bm{H}^\infty_{w_0}+\bm{H}^\infty_{\Gamma,w_0}-\bm{H}_{w_0}^\infty)\\
            &\geq \lambda_{\min}(\bm{H}_{w_0}^\infty)+\lambda_{\min}(\bm{H}^\infty_{\Gamma,w_0}-\bm{H}_{w_0}^\infty)
        \end{aligned}
    \end{equation}
    From the definitions of $\bm{H}^\infty_{\Gamma,w_0}$ and $\bm{H}^\infty_{w_0}$ we have 

    
    \begin{equation}
        \bm{H}^\infty_{\Gamma,w_0}-\bm{H}_{w_0}^\infty = \bm{X}^T\bm{X}\odot \bm{I}^-
    \end{equation}

    Since $\bm{X}^T\bm{X}$ and $\bm{I}^-$ are both positive definite/semi-positive definite matrices, their Hadamard product is also a positive definite/semi-positive definite matrix \cite{matrixmagnus}, \ie, $\bm{H}^\infty_{\Gamma,w_0}-\bm{H}^\infty_{w_0}\succeq 0$.
    Therefore, we have 

        \begin{equation}
        \lambda_{\min}(\bm{H}_{\Gamma,w_0}^\infty)\geq \lambda_{\min}(\bm{H}_{w_0}^\infty)
    \end{equation}

    Finally, according to Theorem 1, we have 

    \begin{equation}
        \begin{aligned}
            &\|f(\mathbf{W}(t),\mathbf{a},\mathbf{X};\bm{\Gamma},\bm{W}_0)-\bm{y}\|_2^2\\
        \leq& \exp(-\lambda_0^\Gamma t)\|f(\mathbf{W}(0),\mathbf{a},\mathbf{X};\bm{\Gamma},\bm{W}_0)-\bm{y}\|_2^2\\
        \end{aligned}
    \end{equation}
    where $\lambda_0^\Gamma \geq \lambda_0$.
\end{proof}

\begin{table*}
  \centering
  \begin{tabular}{c|cccccccccc}
    \toprule
    Datasets & learning rate & batch size & epochs & share r & r & K &$\alpha$ & LoRA drop & warm-up & early stop\\
    \midrule
    MNLI & 3e-4 & 196 & 7 & 2 & 4 & 3 & 16 & 0.25 & 4000 & - \\
    RTE & 7e-4 & 32 & 50 & 2 & 4 & 3 & 16 &0.2 & 900 & -\\
    QNLI& 6e-4 & 64 & 5 & 1 & 6 & 3 & 16 & 0.35&3000&-\\
    MRPC & 1e-3 & 16 & 14 & 2 & 4 & 3 & 16 & 0 & 900 & -\\
    QQP & 8e-4 & 64 & 5 & 2 & 4 & 3 & 16 & 0 &2000&- \\
    SST-2 & 1e-4 & 32 & 5&2 & 4 & 3 & 16 & 0 & 3000&- \\
    CoLA & 3e-4 & 16 & 6 & 2 & 4 & 3 & 16 & 0.15 & 900 & 5\\
    STS-B & 7e-4 & 16 & 7 & 2 & 4 & 3 & 16 & 0 & 900 & -\\
    \bottomrule
  \end{tabular}
  \caption{Training settings for GLUE benchmarks.}
  \label{tab: training settings}
\end{table*}

\begin{table*}
  \centering
  \begin{tabular}{@{}c|cccc|cccc@{}}
    \toprule
    Settings & \multicolumn{4}{c|}{SQuADv1.1} & \multicolumn{4}{c}{SQuADv2.0}\\
     \midrule
    \#Params &0.08\% &0.16\%& 0.32\% & 0.65\% &0.08\% &0.16\%& 0.32\% & 0.65\%\\
    % \midrule
    r & 1 & 2 & 5 & 10 & 2 & 4 & 8 & 15 \\
    \midrule
    train epochs&\multicolumn{4}{c|}{3} & \multicolumn{4}{c}{3}\\
    learning rate&\multicolumn{4}{c|}{5e-4} & \multicolumn{4}{c}{7e-4}\\
    warm-up&\multicolumn{4}{c|}{2000} & \multicolumn{4}{c}{4000}\\
    share r &\multicolumn{4}{c|}{1} & \multicolumn{4}{c}{2}\\

    K & \multicolumn{4}{c|}{3} & \multicolumn{4}{c}{3}\\
    $\alpha$ & \multicolumn{4}{c|}{16} & \multicolumn{4}{c}{16} \\
    LoRA drop & \multicolumn{4}{c|}{16} & \multicolumn{4}{c}{16} \\
    %r = 2&4&8&15 
    \bottomrule
  \end{tabular}
  \caption{Training settings for SQuAD benchmarks.}
  \label{tab: training details for qa}
\end{table*}

\subsection{Proof of Theorem \ref{theorem: genera}}
\label{app:sub:th3}

\noindent\textbf{Theorem 3.}
    For an over-parameterized neural network with the loss on the testing set as $\mathcal{L}(\bm{W},\bm{a};\bm{\Gamma},\bm{W}_0)$. Let $\bm{y}=(y_1, ..., y_N)^T$, and $\eta = \kappa C_1\sqrt{\bm{y}^T(\bm{H}_{\Gamma,w_0}^\infty)^{-1}\bm{y}}/(m\sqrt N)$ for some small enough absolute constant $\kappa$, where $\eta$ denotes the step of SGD. Under the assumption of Theorem \ref{theorem:convergence}, for any $\delta\in(0,e^{-1}]$, there exists $m^\ast(\delta,N,\lambda_0^\Gamma)$, such that if $m\geq m^*$, then with probability at least $1-\delta$, we have
%
    \begin{equation}
    \begin{aligned}
        \mathbb{E}[\mathcal{L}(\bm{W},\bm{a};\bm{\Gamma},\bm{W}_0)]\leq&  \mathcal{O}(C'\sqrt{\frac{\bm{y}^T \bm{y}}{\lambda_0^\Gamma N}})+ \mathcal{O}(\sqrt{\frac{\log(1/\delta)}{N}})\\
        % \leq& \mathcal{O}(C'\sqrt{\frac{\bm{y}^T \bm{y}}{\lambda_0 N}})+ \mathcal{O}(\sqrt{\frac{\log(1/\delta)}{N}})
    \end{aligned}
    \end{equation}
    where $\lambda_0^\Gamma\geq\lambda_0$, $C, C'$, and $\delta$ are constants. 
\begin{proof}
    According to the courant minimax principle \cite{golub2013matrix}, D.2 in \cite{zhu2022generalization}, we get 
\begin{equation*}
\bm{y}^T(\mathbf{H}_{\Gamma,w_0}^\infty)^{-1}\bm{y}\leq\frac{\bm{y}^T\bm{y}}{\lambda_{\min}(\mathbf{H}_{\Gamma,w_0}^\infty)}.
\end{equation*}

Thus, we have 

\begin{align*}
    &\mathbb{E}[\mathcal{L}(\bm{W},\bm{a};\bm{\Gamma},\bm{W}_0)]\\
    &\leq  O\left(c\cdot \sqrt{\frac{\bm{y}^T(\mathbf{H}_{\Gamma,w_0}^\infty)^{-1}\bm{y}}{N}}\right)+O\left(\sqrt{\frac{\log(1/\delta)}{N}}\right)\\
    &\leq O\left(c\cdot \sqrt{\frac{\bm{y}^T\bm{y}}{N\lambda^\Gamma_0}}\right)+O\left(\sqrt{\frac{\log(1/\delta)}{N}}\right)
\end{align*}
\end{proof}

% Having the supplementary compiled together with the main paper means that:
% % 
% \begin{itemize}
% \item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
% \item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
% \item When submitted to arXiv, the supplementary will already included at the end of the paper.
% \end{itemize}
% % 
% To split the supplementary pages from the main paper, you can use \href{https://support.apple.com/en-ca/guide/preview/prvw11793/mac#:~:text=Delete%20a%20page%20from%20a,or%20choose%20Edit%20%3E%20Delete).}{Preview (on macOS)}, \href{https://www.adobe.com/acrobat/how-to/delete-pages-from-pdf.html#:~:text=Choose%20%E2%80%9CTools%E2%80%9D%20%3E%20%E2%80%9COrganize,or%20pages%20from%20the%20file.}{Adobe Acrobat} (on all OSs), as well as \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.

\begin{table*}
  \centering
  \begin{tabular}{@{}l|c|ccccccccc@{}}
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{\#Params} & MNLI & SST-2 & CoLA & QQP & QNLI & RTE & MRPC & STS-B & All \\
     & & Acc & Acc & Mcc & Acc & Acc & Acc & Acc & Corr & Avg.\\
    \midrule
    Full FT & 124.65M & 87.68 & 94.73 & 60.26 & 90.75 & 92.58 & 78.63 & 88.33 & 90.31 &85.41\\
    \midrule
    BitFit & 0.1M & 85.50 & 94.38 & 61.16 & 88.08 & 90.99 & 79.57 & 89.07 & 90.55 & 84.91 \\
    \midrule
    HAdapter &1.20M& 86.53& 93.73& 62.62 & \underline{90.83} & \underline{92.82} & 80.43 & \underline{89.90} & 90.16 & 85.88 \\
    PAdapter& 1.19M & 86.75& 93.83 & \underline{63.87} & 90.53 & 92.61 & 80.51 & 89.51& 90.65 & 86.03\\
    LoRA & 1.33M & 87.11 & 93.71 & 63.54 & 90.44 & 92.76 & 80.65 & \underline{89.90} & 90.91& 86.13\\
    %SoRA$_{r=2}$& 0.25M & 90.37 & 95.64 & 70.22 & 91.88 & 93.78 & 85.18 & 89.71 & 91.63 & 88.39 \\
    AdaLoRA & 1.27M & \textbf{87.89} & \underline{95.11}& 63.23 & 90.48 & \textbf{92.84} & 81.23 & 89.02 & \underline{91.22} & \underline{86.38} \\
    % DoRA & 1.4M & 87.14 & 94.50 & \textbf{65.35} & 90.38 & 92.75 & \underline{81.59} & 90.05 & 90.82 & -\\
    FLoRA & 1.33M & 87.43 & 94.27 & 63.31 & 90.38 & 92.75 & \underline{81.59} & \textbf{90.44} & 90.82 & 86.37\\
    DiffoRA& 1.32M &\underline{87.73} &\textbf{95.16} & \textbf{64.95} &\textbf{91.04} & \textbf{92.84} & \textbf{81.98} & 89.44& \textbf{91.35}&  \textbf{86.81}\\
    \bottomrule
  \end{tabular}
  \caption{The results of the fine-tuned RoBerta-base model on the GLUE dataset are presented, with the best results highlighted in bold and the second-best results underlined.}
  \label{tab: gluerobert}
\end{table*}

\section{Dataset Details}

\noindent\textbf{GLUE \cite{wang2018glue}} The GLUE Benchmark is a comprehensive collection of natural language understanding tasks, designed to evaluate the performance of models across various NLP applications. It includes:
\begin{itemize}
    \item MNLI: Multinomial Natural Language Inference (inference task), including 393k training data and 20k test data.
   \item SST-2: Stanford Sentiment Treebank (sentiment analysis task), including 67k
   training data and 1.8k test data.
   \item MRPC: Microsoft Research Paraphrase Corpus (paraphrase detection task), including 3.7k training data and 1.7k test data.
   \item CoLA: Corpus of Linguistic Acceptability (linguistic acceptability task), including 8.5k training data and 1k test data.
    \item QNLI: Question Natural Language Inference (inference task), including 108k training data and 5.7k test data.
    \item QQP: Quora Question Pairs (question-answering task), including 364k training data and 391k test data.
    \item RTE: Recognizing Textual Entailment (inference task), including 7k training data and 1.4k test data.
    \item STS-B: Semantic Textual Similarity Benchmark (textual similarity), including 7k training data and 1.4k test data.
\end{itemize}

\noindent\textbf{SQuAD}
SQuADv1.1 \cite{rajpurkar-etal-2016-squad} is a dataset consisting of approximately 100k question-answer pairs based on a collection of Wikipedia articles. The task is to extract exact spans of text from the articles as answers to the given questions.
SQuADv2.0 \cite{rajpurkar-etal-2018-know} builds on v1.1 by adding unanswerable questions. It includes about 150k question-answer pairs from over 500 articles, requiring models to both extract answers and identify when no answer is available.



\section{Training Details}
\label{app:settings}
We summarize the training settings of GLUE and SQuAD in \cref{tab: training settings} and \cref{tab: training details for qa}, respectively.
In order to compare with the baseline method at the same parameter level, we use different $r$ for different datasets.
We use ``share r'' to represent the rank of the shared modules.






\section{Results on RoBERTa-base}
\label{app:roberta}
Finally, to further prove the effectiveness of our proposed DiffoRA, we adopt RoBERTa-base as the backbone model which contains 125 million parameters.
We fine-tune the pre-trained model on eight tasks of the GLUE benchmark.
% We use the RoBERTa-base model to fine-tune on the GLUE benchmark, which 
We set the rank of the low-rank decomposition matrices to $4\sim12$ and fine-tuned $20\%\sim 70\%$ of the modules in each layer of the pre-trained model.
The experimental results are summarized in \cref{tab: gluerobert}. 
It can be shown that our DiffoRA achieves the best results in almost all datasets.
For instance, our approach outperforms the state-of-the-art method PAdapter on the CoLA dataset by 0.98\%. Although DiffoRA achieves the second-best result on a few tasks, we achieve the highest average accuracy among all the baselines, i.e., 86.81\% on average.  
%significant improvements on the average.
The experimental results demonstrate the effectiveness of our method.


% \begin{table*}
%   \centering
%   \begin{tabular}{@{}l|c|ccccccccc@{}}
%     \toprule
%     \multirow{2}{*}{Method} & \multirow{2}{*}{\#Params} & MNLI & SST-2 & CoLA & QQP & QNLI & RTE & MRPC & STS-B & All \\
%      & & Acc & Acc & Mcc & Acc & Acc & Acc & Acc & Corr & Avg.\\
%     \midrule
%     Full FT & 100\% & 87.62 & 94.84 & 63.58 & 91.87 & 92.80 & 78.80 & 90.20 & 91.23 &86.37\\
%     \midrule
%     BitFit & 0.08\% & 85.29 & 94.61 & 59.58 & 88.10 & 91.20 & 79.78 & 88.73 & 90.32 & 84.70 \\
%     \midrule
% HAdapter & 0.96\% & \textbf{87.74} & 94.04 & 61.53 & 89.27 & 92.57 & 80.14 & 89.46 & 90.84 & 85.70 \\ 
% PAdapter & 0.95\% & 86.90 & 94.50 & \textbf{66.52} & 90.05 & 92.46 & 80.87 & 88.24 & 90.13 & 86.21 \\ 
% LoRA & 1.06\% & 87.38 & 94.84 & 64.58 & 89.39 & 92.47 & 81.08 & 89.22 & 91.21 & 86.71 \\ 
% TriLoRA & 1.06\% & 87.44 & 94.84 & 64.98 & 89.70 & 92.80 & 84.13 & 88.73 & 90.37 & 85.99 \\ 
% AdaLoRA & 1.06\% & 87.44 & 94.61 & 65.26 & 89.88 & 92.62 & 81.95 & 89.22 & 90.53 & 85.99 \\ 
% FloRA & 1.06\% & 87.43 & 94.27 & 63.31 & 90.38 & 92.75 & 81.59 & 90.44 & 90.82 & 86.37 \\ 
% DoRA & 1.13\% & 87.14 & 94.50 & \underline{66.06} & 91.02 & 92.13 & 81.95 & 88.73 & 90.96 & 86.56 \\ 
% DiffoRA& 1.04\% &\underline{87.73} &\textbf{95.16} & 64.95 &\underline{90.05} & \textbf{92.84} & \textbf{81.98} & 89.44& \textbf{91.35}&  \textbf{-}\\
%     \bottomrule
%   \end{tabular}
%   \caption{The results of the fine-tuned RoBerta-base model on the GLUE dataset are presented, with the best results highlighted in bold and the second-best results underlined.}
%   \label{tab: gluerobert}
% \end{table*}