\section{Introduction}
Voice assistants (VAs) have become an integral part of our daily life, offering convenience and efficiency in tasks ranging from information retrieval to smart home control. However, traditional VAs primarily focus on functionality, such as in-home systems like Alexa~\cite{mclean2019hey,hoy2018alexa}, often overlooking the emotional nuances of human interaction. As voice user interface (VUI) technologies advance, the ability to recognize, understand, and appropriately respond to user emotions is becoming increasingly important.
Emotion-aware VAs, capable of detecting and responding to users' emotional states, have the potential to create more empathetic and engaging user experiences~\cite{ma2023emotion,kossack2023emotion}.

% Using advances in artificial intelligence (AI), natural language processing (NLP), and affective computing, VAs can now detect emotional cues from speech, including vocal patterns, tone, and contextual information. This capability enables them to dynamically adjust their responses, tailoring interactions to the user's emotional state and improving user satisfaction and trust~\cite{ma2022should,Parvathi2025voice}.
Recent advancements in artificial intelligence (AI), natural language processing (NLP), and affective computing have enabled VAs to detect emotional cues from speech, including vocal patterns, tone, and contextual information. This capability allows VAs to dynamically adjust their responses, tailoring interactions to the user's emotional state and improving user satisfaction and trust~\cite{ma2022should,Parvathi2025voice}.
% Despite these advancements~\cite{wani2021comprehensive,triantafyllopoulos2023overview}, effectively responding to user emotions—particularly negative ones—remains a significant challenge.
However, while advancements in speech emotion recognition and speech emotion synthesis have enabled the recognition and understanding of user emotions~\cite{wani2021comprehensive} and the generation of emotionally expressive speech~\cite{triantafyllopoulos2023overview}, effectively responding to user emotions—particularly negative ones—remains a significant challenge.
One major obstacle lies in the variability of emotional expression across individuals and cultures. Negative emotions, such as anger or sadness, can be conveyed differently depending on cultural norms, making it difficult for systems to accurately detect and respond to these emotions~\cite{mesquita1992cultural}. For instance, speech emotion recognition (SER) systems may struggle with accents, dialects, or background noise, leading to misinterpretations of emotional cues~\cite{can2023approaches}.  

Beyond detection, responding appropriately to negative emotions is equally complex. While empathetic reactions can be beneficial to users~\cite{raamkumar2022empathetic,hu2022acoustically}, as seen in Alexa's empathetic responses~\cite{carolus2021alexa}, striking the right balance between empathy and functionality is crucial. Overly intrusive or artificial responses can feel misaligned, ultimately undermining user trust and satisfaction~\cite{atta2024influence,berking2012emotion}. Moreover, emotional expression varies based on cultural, gender, and contextual differences~\cite{fischer2004gender,fischer2000relation}. As a result, purely empathetic emotional responses may not always be suitable or effective for all users. Developing adaptive, context-aware strategies for emotion-aware systems remains a critical area for future research.


% However, while advancements in speech emotion recognition and speech emotion synthesis have enabled the recognition and understanding of user emotions~\cite{wani2021comprehensive} and the generation of emotionally expressive speech~\cite{triantafyllopoulos2023overview}, effectively responding to user emotions—particularly negative ones—remains a significant challenge.
% One major difficulty lies in the variability of emotional expression across individuals and cultures. Negative emotions, such as anger or sadness, can be conveyed differently depending on cultural norms, making it challenging for systems to accurately detect and respond to these emotions~\cite{mesquita1992cultural}. For instance, speech emotion recognition (SER) systems may struggle with accents, dialects, or background noise, leading to misinterpretations of emotional cues~\cite{can2023approaches}.
% Beyond detection, responding appropriately to negative emotions is equally complex. While empathetic emotional reactions can be beneficial to users~\cite{raamkumar2022empathetic,hu2022acoustically}, as seen in Alexa's empathetic responses~\cite{carolus2021alexa}, striking the right balance between empathy and functionality is crucial. Overly intrusive or artificial responses can feel misaligned, ultimately undermining user trust and satisfaction~\cite{atta2024influence,berking2012emotion}.
% Moreover, emotional expression varies based on cultural, gender, and contextual differences~\cite{fischer2004gender,fischer2000relation}. As a result, purely empathetic emotional responses may not always be suitable or effective for all users. Developing adaptive, context-aware strategies for emotion-aware systems remains a critical area for future research.

\begin{figure}[ht]
\centering
  \includegraphics[width=0.8\columnwidth]{Figure/screen.jpg}
  \caption{The web page is designed to collect voice samples from participants. When an emoji is clicked, it turns yellow and plays an emotional context (e.g., a sad or happy scenario). Participants are then prompted to say something comforting or engage in a conversation with the emoji by clicking the "Start Recording" button. This interactive design allows users to respond naturally to the emotional context, providing valuable data for emotion-aware systems.}\label{fig:webpage}
  \vspace{-1em}
\end{figure} 

% To address this issue, an effective approach is to understand how people respond to different emotions in various contexts. By studying human emotional responding strategies, we can transfer these insights into voice assistants (VAs) to create more empathetic and contextually appropriate interactions. This approach aligns with the concept of role-swapping~\cite{ma2022should,huang2024relationship}, where human strategies for emotional responses are mimicked in AI systems.
% Our study employs a role-swapping approach, where the traditional roles of VAs and individuals are reversed. Participants are tasked with applying their own strategies to respond to various emotional contexts displayed by VAs. The primary objective of this study is to influence and regulate the emoji's emotions, guiding them toward more positive or neutral states. To achieve this, participants are encouraged to engage with the VAs using diverse emotional tones and responses, allowing for dynamic and adaptive interactions.
% To facilitate this, we designed a website (shown in Figure~\ref{fig:webpage}) where participants can join the study via a provided web link. On the website, participants can input their emotional responses, which are automatically recorded. After voice recording, the participants' voices are analyzed for emotion detection and speech features.

To address this issue, an effective approach is to understand how people respond to different emotions in various contexts. By studying human emotional responding strategies, we can transfer these insights into voice assistants (VAs) to create more empathetic and contextually appropriate interactions. This approach aligns with the concept of role-swapping~\cite{ma2022should,huang2024relationship}, where human strategies for emotional responses are mimicked in AI systems.
In this study, we employ a role-swapping approach, where the traditional roles of VAs and individuals are reversed. Participants are tasked with applying their own strategies to respond to various emotional contexts displayed by VAs. The primary objective of this study is to influence and regulate the emoji's emotions, guiding them toward more positive or neutral states. To achieve this, participants are encouraged to engage with the VAs using diverse emotional tones and responses, allowing for dynamic and adaptive interactions.
 % In this study, we employ a role-swapping approach, reversing the traditional roles of VAs and users. Participants are tasked with responding to emotional cues displayed by VAs, aiming to regulate the AI's emotions and guide them toward more positive or neutral states. 
To facilitate this, we designed a website (shown in Figure~\ref{fig:webpage}) where participants can join the study via a provided web link. On the website, participants can input their emotional responses, which are automatically recorded. After voice recording, the participants' voices are analyzed for emotion detection and speech features.
By engaging with emotionally expressive VAs, participants apply their own emotional strategies to interact in ways that aim to de-escalate or regulate emotional states. Through speech feature analysis, sentiment polarity assessments, and linguistic evaluations, our study reveals key insights into how users react to different emotional scenarios. The results show that participants favor neutral or positive emotional responses when interacting with negative emotions, indicating a natural tendency toward emotional regulation in human-AI interactions. By integrating these findings, we aim to advance the development of emotion-aware VAs, enabling more empathetic, natural, and effective interactions between humans and voice assistants.

