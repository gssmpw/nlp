\section{Discussion}

Our findings provide key insights into how users respond to different emotional scenarios when interacting with emotion-aware voice assistants (VAs). By analyzing speech signals and linguistic content, we identified patterns in emotional responses, speech features, and sentiment polarity. These findings have several implications for the design of more effective and empathetic VAs, ensuring they provide more meaningful, natural, and human-like interactions.

\subsection{Emotional Response Patterns and User Strategies}

% A key observation from our study is that users tend to adopt neutral emotional responses when interacting with negative emotional stimuli, such as anger, sadness, and fear. This suggests that individuals prefer a balanced, non-confrontational approach when faced with distressing scenarios, likely as a means of de-escalation. Such behavior aligns with established theories in interpersonal emotion regulation, where individuals employ cognitive reappraisal and expressive suppression to manage emotional interactions. Interestingly, while neutrality was the dominant response to negative emotions, participants demonstrated a greater emotional range when engaging with positive emotions, such as happiness. This variability suggests that users are more comfortable expressing emotions openly in positive contexts but may exhibit emotional restraint in negative situations. From a design perspective, VAs should recognize and mirror these behavioral tendencies, allowing for more intuitive interactions that do not feel forced or artificial. For VAs, this indicates that responses should be carefully calibrated to align with user tendencies. Instead of mirroring negative emotions or responding with excessive artificial empathy, VAs may be more effective when employing neutral yet supportive responses. Implementing context-aware, adaptive response strategies can help ensure appropriate engagement without overwhelming users.
A key observation from our study is that users tend to adopt neutral emotional responses when interacting with negative emotional stimuli, such as anger, sadness, and fear. This suggests that individuals prefer a balanced, non-confrontational approach when faced with distressing scenarios, likely as a means of de-escalation. Such behavior aligns with established theories in interpersonal emotion regulation, where individuals employ strategies like cognitive reappraisal and expressive suppression to manage emotional interactions. Interestingly, while neutrality was the dominant response to negative emotions, participants demonstrated a greater emotional range when engaging with positive emotions, such as happiness. This variability suggests that users are more comfortable expressing emotions openly in positive contexts but may exhibit emotional restraint in negative situations. From a design perspective, VAs should recognize and mirror these behavioral tendencies, allowing for more intuitive interactions that do not feel forced or artificial. For VAs, this indicates that responses should be carefully calibrated to align with user tendencies. Instead of mirroring negative emotions or responding with excessive artificial empathy, VAs may be more effective when employing neutral yet supportive responses. Implementing context-aware, adaptive response strategies can help ensure appropriate engagement without overwhelming users.

\subsection{Speech Features as Indicators of Emotion}

% Our analysis of speech signals, including root mean square (RMS), zero-crossing rate (ZCR), and speech jitter, highlighted significant variations between emotional states. Specifically, happy speech scenarios exhibited higher RMS and ZCR values compared to anger, fear, and sadness, indicating greater energy and pitch variation in positive emotional expressions. These findings underscore the potential of integrating advanced speech signal processing techniques into emotion-aware VAs. By continuously analyzing these speech features, VAs can enhance their real-time emotion recognition capabilities, allowing for more precise and adaptive responses to user emotions. Moreover, these insights could be leveraged to train VAs to detect the subtleties of human emotion, moving beyond basic categorical recognition (e.g., happy vs. sad) toward a more nuanced understanding of mixed and subtle emotional states.
Our analysis of speech signals, including root mean square (RMS), zero-crossing rate (ZCR), and speech jitter, highlighted significant variations between emotional states. Specifically, happy speech scenarios exhibited higher RMS and ZCR values compared to anger, fear, and sadness, indicating greater energy and pitch variation in positive emotional expressions. These findings underscore the potential of integrating advanced speech signal processing techniques into emotion-aware VAs. By continuously analyzing these speech features, VAs can enhance their real-time emotion recognition capabilities, allowing for more precise and adaptive responses to user emotions. Moreover, these insights could be leveraged to train VAs to detect the subtleties of human emotion, moving beyond basic categorical recognition (e.g., happy vs. sad) toward a more nuanced understanding of mixed and subtle emotional states.

\subsection{Linguistic Indicators of Emotion}

% The word cloud and NLP analysis revealed distinct linguistic patterns associated with each emotional state. Angry responses featured terms related to conflict and frustration, while fearful responses included words indicating distress and a need for safety. Happy responses, in contrast, were characterized by words denoting enthusiasm and excitement. The correlation between sentiment polarity and type-token ratio (TTR) further illustrated that positive emotions tend to exhibit higher lexical diversity, while negative emotions are often expressed with a limited vocabulary. This suggests that individuals experiencing negative emotions may struggle to articulate their thoughts, potentially due to the cognitive load associated with distressing emotions. Conversely, positive emotions may encourage more elaborate and expressive language use. Understanding these linguistic variations can inform VA design, allowing for more tailored language models that adjust to user sentiment dynamically. Furthermore, beyond basic sentiment analysis, the integration of context-aware linguistic processing can enhance VA interactions. For example, VAs could analyze shifts in user tone and word choice over time, detecting emotional trends and adapting responses accordingly. This capability would make interactions feel more personalized and engaging, reinforcing user trust and reliance on these systems.
The word cloud and NLP analysis revealed distinct linguistic patterns associated with each emotional state. Angry responses featured terms related to conflict and frustration, while fearful responses included words indicating distress and a need for safety. Happy responses, in contrast, were characterized by words denoting enthusiasm and excitement. The correlation between sentiment polarity and type-token ratio (TTR) further illustrated that positive emotions tend to exhibit higher lexical diversity, while negative emotions are often expressed with a limited vocabulary. This suggests that individuals experiencing negative emotions may struggle to articulate their thoughts, potentially due to the cognitive load associated with distressing emotions. Conversely, positive emotions may encourage more elaborate and expressive language use. Understanding these linguistic variations can inform VA design, allowing for more tailored language models that adjust to user sentiment dynamically. Furthermore, beyond basic sentiment analysis, the integration of context-aware linguistic processing can enhance VA interactions. For example, VAs could analyze shifts in user tone and word choice over time, detecting emotional trends and adapting responses accordingly. This capability would make interactions feel more personalized and engaging, reinforcing user trust and reliance on these systems.

\subsection{Implications for Emotion-Aware Voice Assistant Design}

Our findings provide strong support for the integration of emotion-aware mechanisms in VAs. However, designing such systems requires careful consideration of cultural, gender, and contextual differences in emotional expression. Future implementations should prioritize:

% \begin{itemize}
%     \item \textbf{Adaptive Response Strategies}: VAs should dynamically adjust their level of empathy based on user emotional cues, ensuring responses are neither overly detached nor excessively emotional.
%     \item \textbf{Multimodal Emotion Recognition}: Combining speech, linguistic, and contextual data can enhance emotion detection accuracy, mitigating issues related to accents, dialects, and background noise.
%     \item \textbf{User-Centric Customization}: Allowing users to set personal preferences for VA responses can improve engagement and trust, ensuring that interactions align with individual comfort levels.
%     \item \textbf{Longitudinal Emotional Analysis}: Instead of responding to individual utterances in isolation, VAs should analyze patterns in emotional expression over time to offer more contextually relevant interactions.
%     \item \textbf{Ethical and Privacy Considerations}: The implementation of emotion-aware VAs must be accompanied by strong ethical guidelines and transparent data-handling policies to ensure user trust and compliance with privacy regulations.
% \end{itemize}
\begin{itemize}
    \item \textbf{Adaptive Response Strategies}: VAs should dynamically adjust their level of empathy based on user emotional cues, ensuring responses are neither overly detached nor excessively emotional.
    \item \textbf{Multimodal Emotion Recognition}: Combining speech, linguistic, and contextual data can enhance emotion detection accuracy, mitigating issues related to accents, dialects, and background noise.
    \item \textbf{User-Centric Customization}: Allowing users to set personal preferences for VA responses can improve engagement and trust, ensuring that interactions align with individual comfort levels.
\end{itemize}


\subsection{Limitations and Future Work}

% While our study provides valuable insights, certain limitations should be acknowledged. First, the study was conducted in an online environment, which may not fully capture real-world interactions with VAs. Additionally, cultural and linguistic diversity was not explicitly controlled, which could influence the generalizability of our findings. Moreover, our study relied on single-dialogue interactions, which may not fully capture the complexity of users' emotional responses in prolonged engagements with VAs. Additionally, first impressions of VA responses may have influenced participants' reactions, limiting the scope of emotional dynamics observed. Future research should explore cross-cultural variations in emotional expression and investigate how long-term interactions with emotion-aware VAs impact user behavior and satisfaction. Furthermore, an expanded dataset incorporating more diverse user demographics and real-time conversational exchanges would improve the robustness of emotion recognition models. By addressing these challenges, future emotion-aware VAs can move closer to creating more natural, engaging, and emotionally intelligent human-AI interactions. The goal is to not only enhance technical capabilities but also to ensure that VAs contribute positively to users' emotional well-being, fostering AI systems that feel genuinely helpful, intuitive, and human-like.
While our study provides valuable insights into emotional response strategies in human-VA interactions, several limitations should be acknowledged. First, the study was conducted in an online environment, which may not fully replicate real-world interactions with VAs. The controlled nature of the experiment could limit the generalizability of the findings to more dynamic, everyday scenarios. Additionally, cultural and linguistic diversity was not explicitly controlled, which may influence the applicability of our results across different populations. Emotional expression varies significantly across cultures, and future research should explore these variations to ensure that emotion-aware VAs are culturally sensitive and inclusive.
Another limitation is the reliance on single-dialogue interactions, which may not capture the complexity of users' emotional responses in prolonged or repeated engagements with VAs. Emotional dynamics often evolve over time, and future studies should investigate how users' emotional responses and strategies change during extended interactions. Furthermore, participants' first impressions of VA responses may have influenced their reactions, potentially limiting the scope of observed emotional dynamics. To address this, future research could incorporate iterative interactions, allowing participants to engage with VAs over multiple sessions.

To build on these findings, future work should focus on several key areas. First, cross-cultural studies are needed to better understand how emotional expression and regulation vary across different cultural contexts. This would help design VAs that are adaptable to diverse user needs and expectations. Second, long-term interaction studies could provide insights into how users' emotional responses and satisfaction evolve over time, particularly in scenarios where VAs are used regularly. Third, expanding the dataset to include more diverse user demographics and real-time conversational exchanges would improve the robustness and generalizability of emotion recognition models.
By addressing these challenges, future emotion-aware VAs can move closer to creating more natural, engaging, and emotionally intelligent human-AI interactions. The ultimate goal is not only to enhance the technical capabilities of VAs but also to ensure that they contribute positively to users' emotional well-being. Emotion-aware VAs should feel genuinely helpful, intuitive, and human-like, fostering trust and reliance in human-AI interactions.
