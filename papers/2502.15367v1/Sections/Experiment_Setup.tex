\section{Experiment Setup}
% As it described in Figure~\ref{fig:webpage}, we conducted an online user study on in-car virtual natural environments (VRE), developed as part of a separate project [anonymity], to explore the potential of speech analysis for UX measurement. By analyzing post-VRE user interviews, we evaluated the restorative effects and demonstrated the feasibility of using speech analysis for UX evaluation.
As shown in Figure~\ref{fig:webpage}, we conducted an online user study to explore users' strategies for responding to different emotional contexts. By analyzing participants' voices and the content of their interactions with our one-dialog simulated VAs, we evaluated emotional responding differences, speech features, and language features using speech analysis and natural language processing (NLP) techniques. This approach allowed us to gain insights into how users naturally adapt their responses to emotional cues, providing valuable data for improving emotion-aware systems.

\subsection{Emotional Scenarios}
% In our study, based on five basic emotions~\cite{ortony1990,sortony2022all,ma2022emotion},we designed five different emotional scenarios, including neutral, happy, sad, angry, and fear~\cite{ma2022should}. When these emotion states were confirmed, we then have a brainstorming about which emotional scenarios or context can be suitable for users. In the other words, these emotional contexts may be come out in our daily life and can be identified by general users. Fianlly, we confirmed the follwing five different emotional scenarios:
In our study, we designed five distinct emotional scenarios based on the five basic emotions — neutral, happy, sad, angry, and fear~\cite{ortony1990s,ortony2022all,ma2022emotion}. These scenarios were crafted to reflect situations that users might encounter in their daily lives, ensuring they were relatable and easily identifiable by general users. After confirming the emotional states, we conducted brainstorming sessions to determine which scenarios would best represent these emotions in a way that felt authentic and meaningful. The final five emotional scenarios are as follows:

\begin{itemize}
    \item \textbf{Neutral:} I put on my shoes before leaving the house.
    \item \textbf{Happy:} I am visiting my favorite country or city.
    \item \textbf{Sad:} I see children suffering from disease, sickness, or war.
    \item \textbf{Angry:} I get betrayed by a close friend or relative.
    \item \textbf{Fear:} I am walking in the dark in the woods when I stumble upon a dead body. The blood seems fresh, and I hear a branch breaking from behind.
\end{itemize}

Based on these emotional scenarios, we recorded voice clips for each scenario and integrated them into our designed website. Users visiting the website can listen to these emotional scenario recordings, allowing them to immerse themselves in the context and respond naturally. This interactive setup enables participants to engage with the emotional content and provide authentic responses, which are then recorded and analyzed for further study.

\subsection{Apparatus, Participants and Experimental Procedure}
\subsubsection{Apparatus and Participants}
In this user study, we primarily utilized OpenVokaturi\footnote{\url{https://developers.vokaturi.com/getting-started/overview}} to analyze the emotional states of speech. Additionally, we employed two key speech analysis packages: Librosa\footnote{\url{https://librosa.org/doc/latest/feature.html}} and OpenSMILE\footnote{\url{https://audeering.github.io/opensmile-python/}}, to examine users' speech behaviors in response to emotional scenarios. For analyzing the content of users' responses, we relied on four main NLP packages: WordCloud\footnote{\url{https://amueller.github.io/word_cloud/}}, NLTK\footnote{\url{https://www.nltk.org/}}, TextStatc, and SpaCy\footnote{\url{https://spacy.io/}}. Furthermore, OpenAI Whisper\footnote{\url{https://openai.com/index/whisper/}} was used as a critical tool for converting speech into text, enabling detailed analysis of both linguistic and emotional features. Participants' voices were recorded using the built-in microphones of their own computers and uploaded to our web server when they conducted our user study on our website. All recorded voice signals were captured at a sample rate of 48 kHz, ensuring high-quality audio for subsequent analysis.
Additionally, we recruited 60 participants through Prolific\footnote{\url{https://www.prolific.com/}}, comprising 30 males and 30 females. To account for potential gender-based differences in voice perception, we conducted two separate user studies: one using a male emotional voice and the other using a female emotional voice for the five emotional scenarios. The male emotional voice had a mean age of 32.29 years (SD = 9.71), while the female emotional voice had a mean age of 27.17 years (SD = 5.02). This approach ensured a balanced and comprehensive evaluation of emotional responses across different gender contexts.
After completing the user study, each participant was compensated with £4.50 through the Prolific website. Participants were assured of complete anonymity, as clearly stated in the website's privacy policy. This policy also detailed the types of data collected during the study, which included voice recordings and responses to demographic questions, such as age and gender. All measures were taken to ensure the confidentiality and privacy of the participants' information.

\subsubsection{Experimental Procedure}
% As aforementioned, we recruited the participants through the Prolific website and participants can visit our designed website as it shown in Figure~\ref{fig:webpage} through our link. The landing page presents the study in general and guide instructions and participants can click the smiley and it becomes yellow. Meanwhile, a emotional scenarios voice will be played. Participants can respond anything to this emotional voice as they like. The microphone will be recorded their voice when they click the "Start Recording". They will also have 20 seconds to say anything to talk with the smiley or try their best to comfort smiley if they meet negative emotions, such as angry, fear and sad. In this study, they will conduct the five different emotional scenarios and male or female voice will be randomly selected during the study. In order to keep the gender and participants balance, 15 male and female parcipants will try the male five different emotional scenarios and another 30 parcipants will conduct the female different emotional scenarios. After this five different emotional scenarios, the main user study will be completed and then they will fill relevant questionnaire after this one. 
As mentioned earlier, we recruited participants through the Prolific website, and they accessed our designed website (shown in Figure\ref{fig:webpage}) via a provided link. The landing page provided an overview of the study, along with detailed instructions. Participants could click on the smiley emoji, which would turn yellow and play an emotional scenario voice. They were then free to respond in any way they liked. When participants clicked the "\textit{Start Recording}" button, their microphone would begin recording their voice. They were given 20 seconds to speak, during which they could engage in conversation with the smiley or attempt to comfort it, especially in cases of negative emotions such as anger, fear, or sadness.
In this study, participants engaged with five different emotional scenarios, with either a male or female emotional voice randomly selected for each scenario. To ensure gender balance, 15 male and 15 female participants interacted with the male emotional voice scenarios, while the remaining 30 participants interacted with the female emotional voice scenarios. After completing all five emotional scenarios, participants finished the main user study and proceeded to fill out a relevant questionnaire.



\subsection{Speech Signals Analysis}
% The collected speech data from participants were downloaded from the website server and then classified based on different emotional scenarios based on audio file naming. Then we extracted the speech features using the aforementioned  speech processing python package such as librosa and opensmile. Moreover, to analyze speech contents, we converted the audio data into text data using the OpenAI-whisper and then based on NLP technologies, we extracted the text features, such as lexical features, semantic features and syntactic features, etc. Moreover, we used Openvokatuti to obtain parcipants' speech emotion states, namely, which emotional responding they used to communicating with our VAs.
The speech data collected from participants were downloaded from the website server and systematically classified according to different emotional scenarios, based on the naming conventions of the audio files. To extract relevant speech features, we utilized advanced speech processing Python packages, such as Librosa and OpenSMILE~\cite{eyben2010opensmile}. Additionally, to analyze the content of the speech, we transcribed the audio data into text using OpenAI Whisper model. Leveraging natural language processing (NLP) techniques, we then extracted a variety of text features, including lexical, semantic, and syntactic features. Furthermore, we employed Openvokatuti to identify and analyze participants' emotional states during their interactions, specifically determining which emotional responses they exhibited while communicating with our VAs. This comprehensive approach allowed us to gain deeper insights into both the acoustic and linguistic aspects of the participants' interactions.


% The speech data collected from participants were downloaded from the website server and systematically classified into different emotional scenarios based on the naming conventions of the audio files.
% To extract relevant speech features, we utilized advanced speech processing Python libraries, including Librosa and OpenSMILE~\cite{eyben2010opensmile}. For speech content analysis, the audio recordings were transcribed into text format using OpenAI’s Whisper model. Leveraging Natural Language Processing (NLP) techniques, we extracted a range of textual features, including lexical, semantic, and syntactic attributes, to gain deeper insights into linguistic variations across emotional contexts.
% Additionally, we employed OpenVokaturi to analyze participants’ emotional states, identifying the specific emotional responses exhibited during their interactions with our voice assistants (VAs). This comprehensive multimodal approach enabled us to capture both acoustic and linguistic dimensions of participant interactions, facilitating a more nuanced understanding of human emotional expression in AI-mediated communication.

