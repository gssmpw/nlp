[
  {
    "index": 0,
    "papers": [
      {
        "key": "cheng2024enabling",
        "author": "Cheng, Ke and Hu, Wen and Wang, Zhi and Du, Peng and Li, Jianguo and Zhang, Sheng",
        "title": "Enabling Efficient Batch Serving for LMaaS via Generation Length Prediction"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "stojkovic2024dynamollm",
        "author": "Stojkovic, Jovan and Zhang, Chaojie and Goiri, {\\'I}{\\~n}igo and Torrellas, Josep and Choukse, Esha",
        "title": "Dynamollm: Designing llm inference clusters for performance and energy efficiency"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "jin2023s3",
        "author": "Jin, Yunho and Wu, Chun-Feng and Brooks, David and Wei, Gu-Yeon",
        "title": "S3: Increasing GPU Utilization during Generative Inference for Higher Throughput"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "hu2024inference",
        "author": "Hu, Cunchen and Huang, Heyang and Xu, Liangliang and Chen, Xusheng and Xu, Jiang and Chen, Shuang and Feng, Hao and Wang, Chenxi and Wang, Sa and Bao, Yungang and others",
        "title": "Inference without interference: Disaggregate llm inference for mixed downstream workloads"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "aiops2024qiu",
        "author": "Qiu, Haoran and Mao, Weichao and Patke, Archit and Cui, Shengkun and Jha, Saurabh and Wang, Chen and Franke, Hubertus and Kalbarczyk, Zbigniew T. and Ba\\c{s}ar, Tamer and Iyer, Ravishankar K.",
        "title": "Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "qiu2024power",
        "author": "Qiu, Haoran and Mao, Weichao and Patke, Archit and Cui, Shengkun and Jha, Saurabh and Wang, Chen and Franke, Hubertus and Kalbarczyk, Zbigniew and Ba{\\c{s}}ar, Tamer and Iyer, Ravishankar K",
        "title": "Power-aware Deep Learning Model Serving with $\\{$$\\mu$-Serve$\\}$"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zheng2024response",
        "author": "Zheng, Zangwei and Ren, Xiaozhe and Xue, Fuzhao and Luo, Yang and Jiang, Xin and You, Yang",
        "title": "Response length perception and sequence scheduling: An llm-empowered llm inference pipeline"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "fu2024efficient",
        "author": "Fu, Yichao and Zhu, Siqi and Su, Runlong and Qiao, Aurick and Stoica, Ion and Zhang, Hao",
        "title": "Efficient LLM Scheduling by Learning to Rank"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jin2023s3",
        "author": "Jin, Yunho and Wu, Chun-Feng and Brooks, David and Wei, Gu-Yeon",
        "title": "S3: Increasing GPU Utilization during Generative Inference for Higher Throughput"
      },
      {
        "key": "zheng2024response",
        "author": "Zheng, Zangwei and Ren, Xiaozhe and Xue, Fuzhao and Luo, Yang and Jiang, Xin and You, Yang",
        "title": "Response length perception and sequence scheduling: An llm-empowered llm inference pipeline"
      },
      {
        "key": "fu2024efficient",
        "author": "Fu, Yichao and Zhu, Siqi and Su, Runlong and Qiao, Aurick and Stoica, Ion and Zhang, Hao",
        "title": "Efficient LLM Scheduling by Learning to Rank"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "jin2023s3",
        "author": "Jin, Yunho and Wu, Chun-Feng and Brooks, David and Wei, Gu-Yeon",
        "title": "S3: Increasing GPU Utilization during Generative Inference for Higher Throughput"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zheng2024response",
        "author": "Zheng, Zangwei and Ren, Xiaozhe and Xue, Fuzhao and Luo, Yang and Jiang, Xin and You, Yang",
        "title": "Response length perception and sequence scheduling: An llm-empowered llm inference pipeline"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "hu2024inference",
        "author": "Hu, Cunchen and Huang, Heyang and Xu, Liangliang and Chen, Xusheng and Xu, Jiang and Chen, Shuang and Feng, Hao and Wang, Chenxi and Wang, Sa and Bao, Yungang and others",
        "title": "Inference without interference: Disaggregate llm inference for mixed downstream workloads"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "stojkovic2024dynamollm",
        "author": "Stojkovic, Jovan and Zhang, Chaojie and Goiri, {\\'I}{\\~n}igo and Torrellas, Josep and Choukse, Esha",
        "title": "Dynamollm: Designing llm inference clusters for performance and energy efficiency"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "cheng2024enabling",
        "author": "Cheng, Ke and Hu, Wen and Wang, Zhi and Du, Peng and Li, Jianguo and Zhang, Sheng",
        "title": "Enabling Efficient Batch Serving for LMaaS via Generation Length Prediction"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "aiops2024qiu",
        "author": "Qiu, Haoran and Mao, Weichao and Patke, Archit and Cui, Shengkun and Jha, Saurabh and Wang, Chen and Franke, Hubertus and Kalbarczyk, Zbigniew T. and Ba\\c{s}ar, Tamer and Iyer, Ravishankar K.",
        "title": "Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "qiu2024power",
        "author": "Qiu, Haoran and Mao, Weichao and Patke, Archit and Cui, Shengkun and Jha, Saurabh and Wang, Chen and Franke, Hubertus and Kalbarczyk, Zbigniew and Ba{\\c{s}}ar, Tamer and Iyer, Ravishankar K",
        "title": "Power-aware Deep Learning Model Serving with $\\{$$\\mu$-Serve$\\}$"
      }
    ]
  }
]