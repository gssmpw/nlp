%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{color}
\usepackage{xspace}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{array} 
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{tcolorbox}
\usepackage{listings}
\definecolor{mygray}{RGB}{226, 226, 226}
\definecolor{myred}{RGB}{252, 142, 142}
\definecolor{mygreen}{RGB}{147, 255, 143}
\definecolor{myblue}{RGB}{144, 155, 255}
\definecolor{myyellow}{RGB}{253, 253, 143}
\definecolor{mypurple}{RGB}{255, 142, 250}
\definecolor{softblue}{RGB}{100, 149, 237}
\definecolor{mygreen}{RGB}{62,123,39}
% \newcommand{\sys}{\mbox{\textsc{ECCOS}}\xspace}
\newcommand{\sys}{\mbox{ECCOS}\xspace}

\lstdefinestyle{json}{
    basicstyle=\ttfamily \scriptsize \bfseries,
    breaklines=true,
    frame=single,
}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
Kai Mei, Wujiang Xu, Shuhang Lin, Yongfeng Zhang \\
Department of Computer Science, Rutgers University\\
\texttt{\{kai.mei, wujiang.xu, shuhang.lin, yongfeng.zhang\}@rutgers.edu}
}


\begin{document}
% \title{\model/: A Lightweight Deep and Narrow Language Model for Generative Recommendation}
% \title{\sys: Efficient Capability and Cost Coordinated Scheduling for Multi-LLM Serving}
% \title{Smart Routing: Cost-Effectiveness Balanced Multi-LLM Serving in AIOS}
% \title{Smart Routing: Cost-Effective Multi-LLM Serving in AIOS}
\title{Smart Routing: Cost-Effective Multi-LLM Serving for Multi-Core AIOS}

\maketitle

\begin{abstract}
As large language models (LLMs) are increasingly deployed as service endpoints in systems, the surge in query volume creates significant scheduling challenges. Existing scheduling frameworks mainly target at latency optimization while neglecting the capability of LLMs to serve different level of queries, which could lead to computational resource waste. For example, those simple queries can be safely handled by small, fast and cheap LLMs, while those complex and difficult queries need to be handled by large, slow, and expensive LLMs.

This paper addresses this challenge by proposing an efficient capability-cost coordinated scheduling framework, \sys, for multi-LLM serving, which explicitly constrains response quality and workload to optimize LLM inference cost. Specifically, it introduces the two-stage scheduling by designing a multi-objective predictor and a constrained optimizer. The predictor estimates both model capabilities and computational costs through training-based and retrieval-based approaches, while the optimizer determines cost-optimal assignments under quality and workload constraints. It also introduces QAServe, a dataset for sample-wise response quality and costs collected by zero-shot prompting different LLMs on knowledge QA and mathematical reasoning. Extensive experiments demonstrate that ECCOS improves success rates by 6.30\% while reducing costs by 10.15\% compared to existing methods, consuming less than 0.5\% of LLM response time. 
The code is available at: \url{https://github.com/agiresearch/ECCOS}, and the proposed smart routing mechanism has been integrated into AIOS, the AI Agent Operating System, at \url{https://github.com/agiresearch/AIOS}.
\end{abstract}

\section{Introduction}
Large language models (LLMs) have demonstrated remarkable capabilities and empowered various applications, such as chatbots \citep{achiam2023gpt, team2023gemini, dubey2024llama, guo2025deepseek}, code assistants \citep{hui2024qwen2, wei2023magicoder, nijkamp2023codegen2, zhu2024deepseek}, and search engines\footnote{https://www.perplexity.ai/}, etc. These capabilities have naturally led to the widespread integration of LLMs into modern systems \citep{duanmuxserve, sun2024llumnix, mei2024aios, packer2023memgpt, shi2025from, zheng2023efficiently, kwon2023efficient}, with organizations increasingly deploying LLMs as serving endpoints to support diverse use cases. This LLM-served paradigm presents a fundamental challenge: how to efficiently schedule incoming queries across multiple LLM endpoints while optimizing system performance and resource utilization.

\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/intro.pdf}
    \caption{Overview of \sys, which involves multiple system constraints and provides optimization which consider both quality and costs of LLM response. }
    \label{fig:overview}
    \vspace{-15pt}
\end{figure}

While several scheduling frameworks have been proposed for LLM-served systems, current solutions exhibit significant limitations. Most existing approaches \cite{jin2023s3, zheng2024response, hu2024inference, stojkovic2024dynamollm, qiu2024power} predominantly target single-LLM serving scenarios, neglecting the complexity of managing heterogeneous model instances. The few approaches that consider multi-LLM serving \citep{duanmuxserve, sun2024llumnix} primarily optimize for overall latency without considering critical factors such as response quality or model capability matching. For example, as illustrated in \autoref{fig:overview}, some simple queries could be adequately handled by smaller models. However, without proper scheduling, it may be unnecessarily routed to more powerful and resource-consuming LLMs, which could also inhibit powerful LLMs addressing queries in terms of harder problems. As a consequence, it could lead to suboptimal resource utilization and compromised system performance in complex LLM-served systems.

Recognizing these limitations, we explore scheduling under a more general framework that can be adapted to complex multiple LLM serving scenarios with both local and global constraints. 
We formalize the scheduling for multiple LLMs as a \textbf{\textit{constrained optimization problem}} \citep{zhang2020solving, bertsekas2014constrained, homaifar1994constrained} and consider the scenario where \textbf{\textit{response quality and system workloads are constrained to minimize operational cost}}. Building upon this formalization, we propose \sys (seen from \autoref{fig:overview}), a coordinated two-stage scheduling framework for solving this problem, with its core as a multi-objective predictor and a constrained optimizer. At the first stage, this predictor is designed to predict the capability for LLMs to answer specific queries and how much do they cost to answer these queries. Specifically, we design training-based and retrieval-based approaches, respectively. Training-based methods adopt a multi-head predictor to optimize the inner product of embedded vectors of queries and LLMs and retrieve-based method predict by leveraging cosine similarity to search top-k queries existing in the vector database. At the second stage, we design a constrained optimization algorithm that attempts to approximate optimal query-to-model assignments while satisfying both quality and workload constraints. Our algorithm converts the primal problem into its differentiable problem with Lagrangian multiplier and employs a dual gradient-based approach that adjusts model selection based on quality requirements and system load. 

To evaluate our framework, we construct QAServe, a dataset for query-model wise performance by sampling questions from knowledge QA \citep{hendrycks2020measuring, rein2023gpqa} and mathematical reasoning \citep{hendrycks2021measuring, cobbe2021training} tasks and collecting the zero-shot performance of various LLMs. Through extensive experiments compared with existing scheduling baselines, we demonstrate ECCOS's effective of \textbf{\textit{achieving up to 6.30\% improvement in success rate}} while \textbf{\textit{reducing costs by 10.15\%}} compared to cost-oriented solutions. 
\sys also shows its efficiency as it \textbf{\textit{consumes less than 0.5\% of the LLM response time}} and perform even better as the batch size of incoming queries increases. 
ECCOS shows its effectiveness in workload distribution, successfully routing simpler queries to smaller models with high quality standards and demonstrates robust performance across different constraint settings. These results validate our approach to balancing response and effectiveness when serving LLMs. Our contributions are as below: 
\vspace{2pt}

\noindent $\bullet$\; We introduce QAServe, a more grounded dataset with collection of query-wise LLM response correctness and cost. It can be used for a more comprehensive evaluation of LLM serving compared with existing response quality-unaware datasets.  

\noindent $\bullet$\; We propose ECCOS, a principled scheduling framework for multi-LLM serving through two key components: a multi-objective predictor and a constrained optimizer. By formalizing LLM serving as a constrained optimization problem, ECCOS enables capability-cost coordinated scheduling.

\noindent $\bullet$\; We conduct extensive experiments comparing ECCOS with competitive baselines across various serving scenarios. The results demonstrate significant improvements in response quality and cost savings. Further analysis shows the efficiency of \sys and the effectiveness of \sys under different system constrains and requirements.


% \begin{table}[htbp]
% \centering
% \caption{Performance Comparison of Different Methods}
% \label{tab:results}
% \begin{tabularx}{\textwidth}{>{\RaggedRight}Xccc} % 关键修改处
% \toprule
% \textbf{Method} & \textbf{HR@1} & \textbf{HR@3} & \textbf{HR@5} \\
% \midrule
% MF & -- & -- & -- \\
% CausalLM & -- & -- & -- \\
% Ours (text-embedding based optimization) & 0.15 & 0.23 & 0.31 \\
% \bottomrule
% \end{tabularx}
% \end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}


% \subsection{Scheduling }
% Scheduling is critical in computer systems. First-come-first-serve (FCFS) schedules requests according to their arrival time. Shortest-job-first (SJF) and its preemptive version,
% shortest-remaining-time-first (SRTF), prioritize jobs with the shortest time to finish, which provably yield the lowest average latency, but may suffer from starvation problems. Multi-level-feedback-queue (MLFQ) maintains multiple priority queues to balance fairness and latency, but introduces substantial complexity in batch and interactive LLM workloads. 
% \vspace{-30pt}
\section{Problem Formalization} \label{sec:prb}
Given $N$ queries and $M$ models, $a_{i,j} \in [0,1]$ represents the capability score of model $j$ for answering query $i$, $c_{i,j}$ represents the cost of model $j$ processing query $i$, $L_j$ denotes the concurrent workload constraint of model $j$, $\alpha$ represents the overall response quality constraint of the LLM-served system to answer $N$ queries. The objective is to minimize the total cost while guaranteeing the overall response quality over threshold $\alpha$ and respecting each model's concurrent load limit $L_j$. 
%   \end{itemize}
% \item Decision variables:
%   \begin{itemize}
%   \item $x_{i,j} \in \{0,1\}$: whether query $i$ is assigned to model $j$
%   \end{itemize}
% \end{itemize}
This problem can be formulated as the following: 

\begin{small}
\begin{align} \label{eq:1}
\min_{x} \quad
& \sum_{i=1}^N \sum_{j=1}^M c_{i,j} x_{i,j} \nonumber \\
s.t. \quad
& \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^M a_{i,j} x_{i,j} \geq \alpha \\
& \sum_{i=1}^N x_{i,j} \leq L_j, \forall j \nonumber \\
& \sum_{j=1}^M x_{i,j} = 1, \forall i \quad x_{i,j} \in \{0,1\}, \forall i,j
\end{align}
\end{small}
% \begin{align}
% & \min_{x} \sum_{ij} x_{ij}c_{ij}, i\in [N], j \in [M] \\
% & \text{s.t.} \quad \frac{\sum_{ij} x_{ij}a_{ij}}{N} \geq \alpha, \alpha \in (0,1] \\
% & \sum_{j} x_{ij} = 1, \forall i \in [N] \\
% & \sum_{i} x_{ij} \leq L_j, \forall j \in [M] \\
% & x_{i,j} \in \{0,1\}
% \end{align}

\noindent It is a constrained optimization problem \citep{bertsekas2014constrained, homaifar1994constrained} and we will elaborate how to solve this problem in Section \ref{sec:optimizer}.   



% The Lagrangian dual function is defined as:

% \[
% g(\lambda_0, \nu, \mu) = \inf_{x \in \{0,1\}^{N \times M}} L(x, \lambda_0, \nu, \mu).
% \]

% Since \(x_{i,j} \in \{0,1\}\), for each pair \((i,j)\), if the coefficient:

% \[
% c_{i,j} - \lambda_0 a_{i,j} + \nu_i + \mu_j
% \]

% is negative, then setting \(x_{i,j} = 1\) would make the Lagrangian function arbitrarily small (\(-\infty\)), leading to an unbounded solution. Therefore, we require:

% \[
% c_{i,j} - \lambda_0 a_{i,j} + \nu_i + \mu_j \geq 0, \quad \forall i,j.
% \]

% Under this condition, setting all \(x_{i,j} = 0\) minimizes the Lagrangian function, yielding:

% \[
% g(\lambda_0, \nu, \mu) =
% \lambda_0 \alpha N - \sum_{i=1}^N \nu_i - \sum_{j=1}^M \mu_j L_j.
% \]

% Otherwise, \( g(\lambda_0, \nu, \mu) = -\infty \).

% The dual problem maximizes \( g(\lambda, \nu, \mu) \):

% \[
% \begin{aligned}
% \max_{\lambda_0, \nu, \mu} \quad
% & \lambda_0 \alpha N - \sum_{i=1}^N \nu_i - \sum_{j=1}^M \mu_j L_j, \\
% \text{subject to} \quad
% & \lambda_0 \geq 0, \quad \mu_j \geq 0, \quad \forall j, \\
% & c_{i,j} - \lambda_0 a_{i,j} + \nu_i + \mu_j \geq 0, \quad \forall i,j.
% \end{aligned}
% \]


% Given a set of large language models $\mathcal{M} = \{M_1, M_2, ..., M_n\}$, where each model $M_i$ has an associated performance function $p_i(q)$ for a query $q$, and a cost function $c_i(l)$ that depends on the output length $l$. Let $\tau$ be the minimum acceptable performance threshold.
% For any given query $q$, our objective is to find the optimal routing policy $\pi^*$ that minimizes the expected cost while maintaining performance above the threshold:
% \begin{align*}
% \pi^* = \arg\min_{\pi \in \Pi} & \mathbb{E}_{q,l}[c_{\pi(q)}(l)] \\
% s.t. & p_{\pi(q)}(q) \geq \tau, \pi(q) \in \mathcal{M}
% \end{align*}
% where $\pi(q)$ denotes the model selected by policy $\pi$ for query $q$, $c_i(l) = \alpha_i \cdot l$ represents the cost function for model $M_i$, with $\alpha_i$ being the per-token cost, $p_i(q)$ represents the performance score of model $M_i$ on query $q$, $l$ is the output length in tokens, $\Pi$ is the set of all possible routing policies. 
% The challenge lies in designing an efficient policy $\pi$ that can effectively balance the trade-off between model performance and operational costs while considering the uncertainty in output length $l$ for different queries.

\section{Methodology} \label{sec:method}
In this section, we introduce our framework \sys to solve the aforementioned problem. 
\subsection{Multi-objective Predictor}
In this problem, both the capability score ($a_{i,j}$) and the cost ($c_{i,j}$) for each model-query pair are not predetermined. 
% Here, $a_{i,j}$ represents the confidence probability of model $j$ successfully answering query $i$, while $c_{i,j}$ represents the computational cost. In modern LLM-served systems, the cost $c_{i,j}$ primarily depends on two factors: input token length and output token length. 
Since the input token length is known beforehand, the main challenge lies in estimating the output token length. To address this challenge, we design a multi-objective predictor to estimate both $a_{i,j}$ and $c_{i,j}$. Specifically, the predictor aims to accomplish two tasks: (1) predicting the confidence score of a model in answering a query, and (2) estimating the output token length required for the response. As illustrated in \autoref{fig:model}, we propose two complementary approaches: a training-based predictor and a retrieval-based predictor.

For our training-based predictor, we design a dual-head architecture that builds upon an embedding encoder (i.e., bert-base-uncased \citep{devlin2018bert}, chosen for its performance and efficiency). In this architecture, the first head focuses on capability prediction. It estimates a capability score $s \in [0,1]$ by computing the inner product between the query embedding vector $\mathbf{q} \in \mathbb{R}^d$ and the LLM embedding vector $\mathbf{e} \in \mathbb{R}^d$ such that 

\begin{small}
\begin{equation}
s = \sigma(\mathbf{W}_1(\mathbf{q} \cdot \mathbf{e}) + \mathbf{b}_1)
\end{equation}
\end{small}

\noindent where $\sigma(\cdot)$ denotes the sigmoid activation function, $\mathbf{W}_1$ and $\mathbf{b}_1$ are learnable parameters. The second head performs sequence length classification by mapping the output token length $l$ into discrete buckets $\mathcal{B} = \{B_1, B_2, ..., B_k\}$, where $k = \lceil\frac{l_{max}}{b}\rceil$, $l_{max}$ is the maximum sequence length, and $b$ is the bucket size. The probability of length $l$ belonging to bucket $B_i$ is computed as:

\begin{small}
\begin{equation}
P(B_i|l) = \text{softmax}(\mathbf{W}_2(\mathbf{q} \cdot \mathbf{e}) + \mathbf{b}_2)_i \end{equation}
\end{small}

\noindent The bucketing strategy, also mentioned in previous works \citep{jin2023s3, zheng2024response, fu2024efficient}, is adopted to relax the accurate token-level prediction to the bucket-level approximation.

For the retrieve-based predictor, we design a vector database-driven retriever that identifies the top-$k$ similar queries based on cosine similarity. 
For a given query $\mathbf{q}$, let $\mathcal{Q}_k(\mathbf{q})$ be the set of top-$k$ similar queries retrieved from the database. The predicted capability score $\hat{s}$ and output length score $\hat{l}$ are computed as:

\begin{small}
\begin{equation}
    \hat{l} = \frac{1}{k}\sum_{\mathbf{q}_i \in \mathcal{Q}_k(\mathbf{q})} l_i, \quad
    \hat{s} = \frac{1}{k}\sum_{\mathbf{q}_i \in \mathcal{Q}_k(\mathbf{q})} s_i  
\end{equation}
\end{small}

% \begin{small}
% \begin{equation}
% \end{equation}
% \end{small}

\noindent where $s_i$ and $l_i$ are the correctness score and output token length of the retrieved query $\mathbf{q}_i$, respectively.
% As is shown in \autoref{fig:model}, we design the dual-head model to deal with the performance ranking and output length ranking. 
\begin{figure}[tb!]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/model.pdf}
    \caption{Multi-objective predictor for capability prediction and output length prediction. }
    \label{fig:model}
    \vspace{-10pt}
\end{figure}

\subsection{Constrained Optimizer} \label{sec:optimizer}
Inspired by \citet{zhang2020solving, bertsekas2014constrained, wang2023multi}, we leverage the Lagrangian dual theory and introduce Lagrangian multipliers to convert the primal problem into its Lagrangian dual problem. We only provide the crucial steps in this section, and the complete induction can found in \autoref{app:proof}. 
By introducing Lagrangian multipliers $\lambda_1,\lambda_{2,j}, \mu_{i}$, we get the Lagrangian relaxation function of the original problem as follows:

\begin{small}
\begin{align}
& L(x,\lambda_1,\lambda_{2,j}, \mu_{i}) \nonumber \\
& = \sum_{i=1}^N \sum_{j=1}^M c_{i,j} x_{i,j} + \lambda_1 \left( - \frac{\sum_{i=1}^N \sum_{j=1}^M a_{i,j} x_{i,j}}{N} + \alpha \right) \nonumber \\
& \quad + \sum_{j=1}^M \lambda_{2,j} \left( \sum_{i=1}^N x_{i,j} - L_j \right) + \sum_{i=1}^N \mu_i \left( \sum_{j=1}^M x_{i,j} - 1 \right)
\end{align}
\end{small}

\noindent The dual function is formulated as Eq \ref{eq:6}:  

\begin{small}
\begin{align} \label{eq:6}
& g(\lambda_1,\lambda_2) \nonumber \\
&= \min_{x_{i,j}} \mathcal{L}(x,\lambda_1,\lambda_2) \nonumber \\ 
& = \min_{x_{i,j}} \{\sum_{i=1}^N\sum_{j=1}^M x_{i,j}(c_{i,j} - \frac{\lambda_1a_{i,j}}{N} + \lambda_{2,j}) \nonumber \\ 
& \quad+ \lambda_1\alpha - \sum_{j=1}^M \lambda_{2,j}L_j\} 
\end{align}
\end{small}

\noindent and the dual problem is formulated as: 

\begin{small}
\begin{align}
& \max_{\lambda_1,\lambda_2} g(\lambda_1,\lambda_2) & \text{s.t.} \quad \lambda_1 \geq 0, \lambda_{2,j} \geq 0, \forall j
\end{align}
\end{small}


\noindent Then we can use gradient descent to update the partial derivative of $\lambda_1$ and $\lambda_2$, respectively, which is shown in Eq \ref{eq:8} and Eq \ref{eq:9}. 
 % \begin{align}
% & \frac{\partial d}{\partial \lambda_1} = -\sum_{i=1}^N\sum_{j=1}^M x_{ij}a_{ij}/N + \alpha
% \end{align}
% \begin{align}
% & \frac{\partial d}{\partial \lambda_{2,j}} = \sum_{i=1}^N x_{ij} - L_j, \quad \forall j \in [M]
% \end{align}

\begin{small}
\begin{align} \label{eq:8}
& \lambda_1^{t+1} = \max(\lambda_1^t + \alpha_1(\frac{-\sum_{i=1}^N\sum_{j=1}^M x_{i,j}a_{i,j}}{N} + \alpha), 0) 
\end{align}
\end{small}
\vspace{-10pt}
\begin{small}
\begin{align} \label{eq:9}
& \lambda_{2,j}^{t+1} = \max(\lambda_{2,j}^t + \alpha_2(\sum_{i=1}^N x_{i,j} - L_j), 0), \forall j \in [M]
\end{align}    
\end{small}

\noindent
By updating $\lambda_1$ and $\lambda_2$, we can solve the partial sub-problem to obtain $x_{i,j}$ accordingly: 

\begin{small}
\begin{align}
& x_{i,j} = \begin{cases} 
1 & \text{if } j = j^*_i \\
0 & \text{otherwise}
\end{cases} \\
& j^*_i = \arg\min_{j \in [M]} (c_{ij} - \frac{\lambda_1a_{i,j}}{N} + \lambda_{2,j})
\end{align}
\end{small}

\noindent
An intuitive explanation of this process is: For $\lambda_1$: its partial derivative represents how much the quality constraint is violated. If the average quality is less than the threshold $\alpha$, the constraint is violated, and the gradient is positive, pushing $\lambda_1$ to increase, which in turn encourages the scheduler to select higher-quality models in the next iteration.
For $\lambda_{2,j}$, its partial derivative shows the workload violation for each model j. If a model's assigned queries exceed its capacity, the gradient is positive, increasing $\lambda_{2,j}$, which makes this overloaded model less attractive in subsequent iterations.

\section{Experiment}
% \begin{table*}[htbp]
% \centering
% \caption{Comparison between different scheduling methods. }
% \label{tab:results}
% \begin{tabular}{lcc}
% \toprule
% Scheduling Method & Accuracy(\%) & Cost \\ \midrule
%                   &              &      \\
%                   &              &      \\ \bottomrule
% \end{tabular}
% \end{table*}

\subsection{Datasets}
To assess sample-wise response quality, we construct a QA-serving dataset by sampling questions from diverse knowledge and mathematical reasoning datasets: MMLU \citep{hendrycks2020measuring} for general knowledge assessment, GPQA \citep{rein2023gpqa} for general problem-solving, MATH-500 \citep{hendrycks2021measuring} for mathematical reasoning, GSM8K \citep{cobbe2021training} for grade school mathematics, and for common sense reasoning.
We evaluated these questions using zero-shot prompting across multiple LLMs, including Llama-3.1 (8B-Instruct) \citep{dubey2024llama}, Qwen-2.5 (7B-Instruct, 14B-Instruct, 32B-Instruct) \citep{qwen2}, and Deepseek-R1 (7B, 14B) \citep{guo2025deepseek}. 
To standardize cost analysis, we exclusively focus on open-source models, where computational costs are proportional to their model parameters, to avoid the influences of provider-specific pricing strategies for close-source models. 
For each response, we measured both response correctness and output token length. To assess correctness, we employed Llama-3.1-70B-Instruct \citep{dubey2024llama} as an evaluation judge. Our dataset details and correctness assessment can be found in Appendix \ref{app:dataset}. The dataset is split into training (70\%), validation (20\%), and test (10\%) sets for our experiments. For token prices (both input and output) of the open-sourced models, we refer to the cost map provided by Litellm\footnote{https://models.litellm.ai/} library. 

\subsection{Setup}
We conduct our experiments on an Ubuntu 22.04 machine equipped with 8 RTX A5000 GPUs to serve multiple LLMs using Ollama\footnote{https://ollama.com/}. During the implementation, we set the max output length as 1024 instead of the max output length of the data for bucket classification to prevent the influences of extreme long output sequences in Deepseek-R1 models. We evaluate two distinct approaches for serving LLM requests: \textit{\textbf{streaming}} and \textit{\textbf{fixed-size batching}}.
In the streaming setting, queries are processed individually as they arrive in real-time. When a query comes in, the scheduler immediately optimizes and assigns it to an available LLM. The system has a maximum workload limit, defined as the total computational load across all active LLM services. If accepting a new query would exceed this limit, the query is placed in a queue until resources become available. 
In the fixed-size batching setting, instead of processing queries individually, the system waits to accumulate a predetermined number of queries before processing them together. Once this batch size is reached, the scheduler assigns the entire batch of queries across the available LLMs in the system. 
During the implementation, we regard the optimization under the streaming setting as the special case of the batch setting (batch size is 1), which is a common practice in existing studies \citep{crankshaw2017clipper, lee2018pretzel, hadjis2016omnivore}. 
If not specifically mentioned, we set the two constraints $\alpha = 0.75$ and the concurrent workload constraints $L = 4$, which are applied uniformly across all LLMs. A comprehensive analysis of serving performance under different values of $\alpha$ and $L$ is presented in Section \ref{sec:ablation}. To facilitate queue management, we implement a unified capacity control mechanism in both streaming and batching settings: the maximum number of concurrent jobs (represented as batch size in batching mode and working threads in streaming mode) is set to half of the total concurrent workload capacity across all LLMs.
We evaluate \sys against the following baseline scheduling methods:

\noindent \textbf{Balance-aware (BA)}: A basic scheduler only considers  their current workload capacity to maintain system balance and picks randomly if multiple LLMs have the same concurrent availability.

\noindent \textbf{S3 \citep{jin2023s3}}: A scheduler that uses an encoder-only model to predict output length, with its original goal adapted from latency-oriented to cost-oriented, i.e., making scheduling decisions based on current LLM workloads and costs, prioritizing more cost-efficient LLMs when possible.

\noindent \textbf{Perception-only (PO) \citep{zheng2024response}}: Similar to S3 but employs a decoder-only model for output length prediction. It is also adapted to consider both workload balancing and cost-efficiency.

% \noindent \textbf{Oracle: }Optimal Performance and Output Length Predictor and Constrained Optimization

\subsection{Overall Performance}

\paragraph{Performance of Predictors. } We first evaluate the prediction accuracy of \sys compared with another two prediction-based baslines, i.e., S3 and PO. The length predictions are measured with both exact match accuracy and a relaxed metric allowing predictions within one bucket of the ground truth.
\begin{table}[tb!]
\centering
\caption{Performance of capability predictors and output token length predictors, where length prediction is evaluated using both exact bucket match and relaxed (±1 bucket) metrics. }
\adjustbox{max width=1.0\linewidth}{
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Capability Acc.} & \multicolumn{2}{c}{Length Bucket Acc.} \\ \cmidrule(l){3-4} 
                        &                                  & Exact Match        & $\pm1$       \\ \midrule
S3                      &  --                                & 0.333                    & 0.656          \\
PO                      &  --                                & 0.325                    & 0.683          \\
ECCOS-R                 & 0.775                               & 0.343                    & 0.693          \\ 
ECCOS-T                 &  \textbf{0.813}                                & \textbf{0.452}                    & \textbf{0.806}          \\
\bottomrule
\end{tabular}
}
\end{table}
The results show that the training-based variant (ECCOS-T) achieves superior performance across all metrics, with 81.3\% accuracy in capability prediction and 45.2\% accuracy in exact bucket match for length prediction. The retrieval-based variant (ECCOS-R) also demonstrates competitive performance with 77.5\% capability prediction accuracy and improved length prediction compared to existing methods.
These results suggest the effectiveness of the multi-objective predictors in \sys. 

\paragraph{Serving Performance. }
The comparison of the overall serving performance under the streaming and batching settings can be observed from \autoref{tab:overall}. 
\begin{table}[htbp]
\centering
\caption{Comparison of overall serving performance under the multi-llm serving setting. } \label{tab:overall}
\adjustbox{max width=1.0\linewidth}{
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Streaming}                & \multicolumn{2}{c}{Batching}                 \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} 
                        & SR ($\uparrow$) & \$ Cost ($\downarrow$) & SR ($\uparrow$) & \$ Cost ($\downarrow$) \\ \midrule
BA                    & 69.26\%                         & 0.0927                  & \underline{70.74\%}                         & 0.1037                   \\
S3                      & \underline{69.26\%}                         & \underline{0.0517}                  & 69.63\%                         & \underline{0.0652}                  \\
PO         & 68.52\%                         & 0.0550                  & 68.89\%                         & 0.0669                  \\ \midrule
\sys-T          &  71.04\%                       & 0.0732                  & 72.22\%                          & 0.0725                  \\ 
\sys-R           &  \textbf{75.56\%}                      & \textbf{0.0464}                  & \textbf{74.81\%}                         & \textbf{0.0565}                  \\ 
$\Delta$           &  \textbf{\textcolor{mygreen}{6.30\%}}                      & \textbf{\textcolor{mygreen}{0.0053}}                  & \textbf{\textcolor{mygreen}{4.07\%}}                         & \textbf{\textcolor{mygreen}{0.0087}}                  \\ 
% \textbf{\sys-R}           &  \textbf{81\%}                       & \textbf{0.208}                  &                          &                   \\ 
\bottomrule
\end{tabular}
}
\end{table}
Our proposed methods (\sys-T and \sys-R) demonstrate substantial advantages over baseline approaches (BA, S3, and PO) in both streaming and batching scenarios. Given the theoretical performance bounds (lower bound 57.41\% and upper bound 90\%), the improvement achieved by \sys-R is particularly significant - reaching 75.56\% success rate in streaming and 74.81\% in batching mode, which represents a meaningful advancement within this constrained optimization space. Specifically, the 6.30\% and 4.07\% improvements over baselines bridge a considerable portion of the gap between existing methods and the theoretical upper bound. What makes these results even more compelling is that these substantial performance gains are achieved while simultaneously reducing costs - \sys-R maintains the lowest cost (\$0.0464 for streaming and \$0.0565 for batching) among all methods, demonstrating superior cost-effectiveness. The consistent performance improvements across both streaming and batching scenarios, while operating well within but pushing closer to the theoretical bounds, indicate that our approach has successfully captured key scheduling opportunities in multi-LLM serving environments while maintaining robust and stable performance across different serving modes.

\paragraph{Computation Time Analysis.} In this experiment, we evaluate the efficiency of our scheduling system by comparing the scheduling overhead with the actual LLM processing time across different configurations. 
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/time_analysis.pdf}
    \caption{Comparison between scheduling time and LLM response time, where \textbf{S} denotes the streaming setting, \textbf{B} denotes the batching setting. }
    \label{fig:time}
\end{figure}
\autoref{fig:time} presents the comparative results between total scheduling time and LLM response time under four different settings: ECCOS-R(S), ECCOS-R(B), EECOS-T(S), and EECOS-T(B), where B and S represent batching and streaming, respectively. The results demonstrate that our scheduling mechanism introduces minimal overhead to the overall system performance. Specifically, in ECCOS-R(S), the scheduling time is merely 2.4 seconds compared to an LLM response time of 6266.7 seconds. Similarly, ECCOS-R(B) shows an even more negligible scheduling overhead of 0.5 seconds against 3774.6 seconds of LLM processing time. For the EECOS-T variants, while the scheduling times are slightly higher (32.4 seconds for EECOS-T(S) and 9.49 seconds for EECOS-T(B)), they still represent only 0.47\% and 0.27\% of the total processing time respectively. The scheduling time becomes more negligible especially in batching setting and when batch size increases, which suggests that our approach is highly efficient and scalable for practical applications. 

\subsection{Ablation Studies} \label{sec:ablation}
In this section, we study the effects of different key factors in the scheduling framework on the serving performance of this system. We employ the fix-sized batching setting throughout the ablation studies. More results running under the streaming setting can be found at \autoref{app:add_exp}. 
% \noindent \textbf{Factors that Affect the Dual-head Model}

\paragraph{Effects of Performance Constraints.} We investigate the impact of varying performance thresholds ($\alpha$) on our scheduling system, examining both cost efficiency and actual performance across $\alpha$ values from 0.70 to 0.90. 
\begin{figure}[tb!]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/cperf_cost.pdf}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/cperf_perf.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    \vspace{-10pt}
    \caption{The effects of performance constraint $\alpha$ on the serving quality and cost efficiency under the batching (B) setting. } \label{fig:perf_constraint}
\end{figure}
As is shown in \autoref{fig:perf_constraint}, the cost trajectories reveal a non-linear relationship with performance requirements. Both ECCOS variants exhibit increasing costs up to $\alpha=0.85$, followed by a slight decline. This unexpected cost reduction at higher thresholds ($\alpha>0.85$) indicates that stricter quality requirements may encourage more sophisticated scheduling strategies. The retrieval-based approach (ECCOS-R) demonstrates superior cost efficiency, particularly in the moderate constraint range ($\alpha<0.85$), though this advantage diminishes under more stringent requirements. These observations yield important insights for system deployment. First, the existence of a performance plateau around $\alpha=0.85$ suggests diminishing returns beyond this point, making it an attractive operating threshold. Second, ECCOS-R's greater sensitivity to performance constraints makes it particularly suitable for environments requiring dynamic quality adjustments. Lastly, the inverse relationship between marginal cost and performance improvements at high $\alpha$ values indicates that optimal system efficiency might not necessarily align with the highest possible quality requirements.

\paragraph{Effects of Workload Constraints.} We also analyze the impact of concurrent workload constraints $L$ on both cost and performance metrics in the batching setting. We vary the concurrent workload constraint from 4 to 16 to evaluate how different levels of parallelism affect system behavior. 
\begin{figure}[tb!]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/cload_cost.pdf}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/cload_perf.pdf}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    \vspace{-10pt}
    \caption{The effects of concurrent workload constraint $L$ (i.e., maximum number of concurrent inferences on an LLM) on the serving quality and cost efficiency under the batching (B) setting. } \label{fig:load_constraint}
\end{figure}
From \autoref{fig:load_constraint}, we can see that ECCOS-R consistently achieves better cost efficiency across all workload constraints, with costs decreasing as the constraint increases, suggesting that it effectively leverages increased parallelism to optimize resource utilization. In contrast, ECCOS-T shows higher but relatively stable costs after an initial increase, indicating less sensitivity to workload constraints in terms of cost optimization. On the performance side, ECCOS-R maintains superior success rates throughout, though with a slight decline at higher constraints, while ECCOS-T shows significant improvement up to 8 concurrent workloads before stabilizing. This performance pattern suggests that there exists an optimal balance point for concurrent workloads - increasing beyond 8 parallel tasks yields diminishing returns and may even slightly degrade performance. The divergent behaviors of ECCOS-R and ECCOS-T highlight how different scheduling strategies respond to parallelism constraints, with ECCOS-R demonstrating better overall adaptability to varying workload conditions while maintaining a more favorable cost-performance trade-off.

\begin{table}[tb!]
\centering
\caption{Effects of number of buckets on training-based predictor and overall scheduling performance. } \label{tab:n_bucket}
\adjustbox{max width=0.9\linewidth}{
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{\# Bucket} & \multicolumn{2}{c}{Bucket Acc ($\uparrow$)} & \multirow{2}{*}{SR ($\uparrow$)} & \multirow{2}{*}{\$ Cost ($\downarrow$)} \\ \cmidrule(lr){2-3}
                           & Exact Match                & $\pm1$               &                                   &                                         \\ \midrule
10                         &  \textbf{0.452}                          & \textbf{0.806}               & 72.22\%                                 & \textbf{0.0725}                                        \\
20                         & 0.269                           & 0.526               & \textbf{73.70\%}                                & 0.0845                                        \\
50                         & 0.162                           & 0.299               & 72.22\%                                 & 0.1034                                        \\
100                        & 0.124                           & 0.189               & 70.74\%                                 & 0.0962                                        \\ \bottomrule
\end{tabular}
}

\end{table}

\paragraph{Effects of Factors in Predictors. } 
We investigate how the design choices in both predictors affect the scheduling effectiveness, focusing on the number of buckets in the training-based predictor and the K value in the retrieval-based predictor. For bucket numbers shown in \autoref{tab:n_bucket}, we observe a clear trade-off between prediction granularity and accuracy: a smaller number of buckets (10) achieves significantly better prediction accuracy (45.2\% exact match and 80.6\% ±1 accuracy) compared to larger numbers (12.4\% exact match for 100 buckets), while maintaining competitive scheduling performance (72.22\% success rate) and cost efficiency (\$0.0725). For K values in the retrieval-based predictor, which is illustrated in \autoref{tab:n_k}, moderate values (8-16) achieve optimal performance with 77.5\% prediction accuracy and maintain a stable success rate of 74.81\% with lower costs (\$0.0565). These findings reveal that: (1) coarse-grained performance discretization can be more reliable and practical than fine-grained approaches, suggesting that precise performance prediction might not be necessary for effective scheduling; (2) the retrieval-based predictor exhibits robust performance across a reasonable range of K values (8-32), indicating its stability in real-world applications, though excessive historical samples (K=64) can introduce noise and degrade performance. 

\begin{table}[tb!]
\centering
\caption{Effects of variations of K on retrieval-based predictor and overall scheduling performance. } \label{tab:n_k}
\adjustbox{max width=1.0\linewidth}{
\begin{tabular}{cccccc}
\toprule
\multirow{2}{*}{K} & \multirow{2}{*}{Perf. Acc ($\uparrow$)} & \multicolumn{2}{c}{Length Acc ($\uparrow$)} & \multirow{2}{*}{SR ($\uparrow$)} & \multirow{2}{*}{\$ Cost ($\downarrow$)} \\ \cmidrule(lr){3-4}
                   &                                         & Exact Match      & $\pm1$      &                                  &                                         \\ \midrule
4                  & 0.773                                   & 0.340            & 0.694                    &  74.07\%                                & 0.0614                                        \\
8                  & \textbf{0.775}                                   & 0.343            & 0.696                    &  74.81\%                                & 0.0565                                        \\
16                 & 0.774                                   & \textbf{0.346}            & \textbf{0.700}                    &  \textbf{74.81\%}                                & \textbf{0.0565}                                        \\
32                 & 0.765                                   & 0.340            & 0.676                    &  74.81\%                                & 0.0565                                        \\
64                 & 0.760                                   & 0.322            & 0.664                    &  73.70\%                                & 0.0684                                        \\ \bottomrule
\end{tabular}
}

\end{table}

% \noindent \textbf{Effectiveness of Cost Ranker}

% \textbf{Impact of the number of LLMs}

% \subsection{Scalability Analysis}

% \subsection{Case Study}

% \textbf{Easy Questions}

% \textbf{Hard Questions}

\noindent 

% \subsection{Case Studies}

\paragraph{Serve Multiple LLMs within Similar Scales.} In this experiment, we investigated the performance of our scheduling system (ECCOS) when managing multiple LLMs of similar model sizes, specifically Qwen-2.5-7B-Instruct, Llama-3.1-8B-Instruct, and Deepseek-R1-7B. 
From \autoref{tab:similar_size}, we can observe that the performance varied across different models. Under ECCOS-R, Qwen-2.5-7B-Instruct showed the highest correctness rate at 77\%, while under ECCOS-T, Llama-3.1-8B-Instruct performed best at 79\%. This indicates that scheduling strategy can impact individual model performance even when models are of similar scale. In terms of cost efficiency, ECCOS-T demonstrated better cost efficiency, with a total cost of \$0.0585 compared to ECCOS-R's \$0.0658. This 11\% cost reduction while maintaining slightly better accuracy suggests that ECCOS-T has more efficient resource utilization when managing similar-sized models. 

\begin{table}[tb!]
\centering
\vspace{10pt}
\caption{Scheduling performance when served LLMs are within similar scales (i.e., Qwen-2.5-7B-Instruct, Llama-3.1-8B-Instruct, Deepseek-R1-7B). } \label{tab:similar_size}
\adjustbox{max width=1.0\linewidth}{
\begin{tabular}{lllc}
\toprule
Method                   & Model                 & Correctness & \$ Cost \\ \midrule
\multirow{4}{*}{ECCOS-R} & Qwen-2.5-7B-Instruct  & 77\% (130/169)                & 0.0188                       \\
                         & Llama-3.1-8B-Instruct        & 64\% (35/55)               & 0.0070                       \\
                         & Deepseek-R1-7B & 65\% (30/46)                & 0.0400                       \\
                         & Total                 & 72\% (195/270)                & 0.0658                       \\ 
                         \midrule
\multirow{4}{*}{ECCOS-T} & Qwen-2.5-7B-Instruct  & 73\% (133/181) & 0.0197                       \\
                         & Llama-3.1-8B-Instruct        & 79\% (58/73)                & 0.0102                       \\
                         & Deepseek-R1-7B & 58\% 8/16 (50\%)                & 0.0216                       \\
                         & Total                 & 74\% (199/270)                & 0.0585                       \\ 
                         \bottomrule
\end{tabular}
}
\end{table}

\paragraph{Serve Multiple LLMs from the Same Series.} To investigate the scheduling performance within a model family, we evaluate \sys using three variants from the Qwen-2.5 series (7B, 14B, and 32B). 
% This experiment is particularly important as model providers often release a series of models with different parameter scales, and understanding how to effectively allocate queries among these variants can lead to more cost-efficient deployment strategies.
\begin{table}[tb!]
\centering
\caption{Scheduling performance when served LLMs are from the same LLM series (i.e., Qwen2.5). } \label{tab:same_series}
\adjustbox{max width=1.0\linewidth}{
\begin{tabular}{lllc}
\toprule
Method                   & Model                 & Correctness & \$ Cost \\ \midrule
\multirow{4}{*}{ECCOS-R} & Qwen-2.5-7B-Instruct  & 81\% (55/68)                & 0.0086                       \\
                         & Qwen-2.5-14B-Instruct        & 71\% (64/90)                & 0.0179                       \\
                         & Qwen-2.5-32B-Instruct & 76\% (85/112)                & 0.0585                       \\
                         & Overall                 & 76\% (204/270)                & 0.0849                       \\ 
                         \midrule
\multirow{4}{*}{ECCOS-T} & Qwen-2.5-7B-Instruct  & 62\% (15/24)                & 0.0030                       \\
                         & Qwen-2.5-14B-Instruct        & 81\% (135/167)                & 0.0341                       \\
                         & Qwen-2.5-32B-Instruct & 58\% (46/79)                & 0.0465                       \\
                         & Overall                 & 73\% (196/270)                & 0.0836                       \\ 
                         \bottomrule
\end{tabular}
}
\end{table}
As is shown in \autoref{tab:same_series}, the retrieval-based approach (ECCOS-R) achieves a slightly higher overall correctness (75.56\% vs 72.59\%) with comparable total cost (0.0849 vs 0.0836). However, the query distribution patterns differ significantly between the two methods. ECCOS-R shows a more balanced distribution across model sizes, with the 7B, 14B, and 32B models handling 68, 90, and 112 queries respectively. In contrast, ECCOS-T heavily favors the 14B model (167 queries) while assigning fewer queries to the 7B (24 queries) and 32B (79 queries) variants. From these results, we can obtain an interesting insight that although the largest model (32B) might be expected to perform best, both approaches found ways to achieve good performance while limiting its usage, demonstrating effective cost optimization, which also suggests the importance of care scheduling. 
% \subsection{Robustness to Out-of-Domain Queries}

% \begin{table}[htbp]
% \centering
% \caption{Comparison between using the vanilla \sys and \sys-R (using retriever)}
% \adjustbox{max width=0.8\textwidth}{
% \begin{tabular}{lcc}
% \toprule
% Method   & Perf Acc & Length Acc \\ \midrule
% \sys (C)  & 83.1\%          & 91.2\%                            \\
% \sys (R) & 70.4\%          & 95.6\%                            \\ \bottomrule
% \end{tabular}
% }
% \end{table}

\section{Related Work}

% \subsection{LLM Router}
% MF

% CausalLM


\subsection{LLM Generation Length Prediction}
Predicting LLM generation length is crucial for optimizing computational resources. Early attempts like Magnus \citep{cheng2024enabling} employed random forest algorithms but achieved limited accuracy. Subsequent research has explored two main directions of prediction models: encoder-only models for classification (DynamoLLM \citep{stojkovic2024dynamollm}, S3 \citep{jin2023s3}, TerriInfer \citep{hu2024inference}, SSJF \citep{aiops2024qiu}, and $\mu$3 \citep{qiu2024power}) and decoder-only models for generative prediction like Perception-only (PO) \citep{zheng2024response}. \citet{fu2024efficient} reformulated this as a ranking problem and utilized listwise ranking for predictor training. Due to the inherent difficulty in precise output length prediction, several works \citep{jin2023s3, zheng2024response, fu2024efficient} adopted a bucketing approach for approximate estimation. However, these existing studies primarily emphasize computational efficiency while overlooking a critical aspect: model capability - specifically, whether a model has the capability to answer a given query. Our research addresses this limitation by simultaneously considering both generation length and model capability, aiming to optimize both system effectiveness and efficiency.

% S3 \citep{jin2023s3}

% Perception-only \citep{zheng2024response}

% TerriInfer \citep{hu2024inference}

% DynamoLLM \citep{stojkovic2024dynamollm}

% Magnus \citep{cheng2024enabling}

% SSJF \citep{aiops2024qiu}

% $\mu$3 \citep{qiu2024power}

\subsection{General Scheduling}
Scheduling is a fundamental problem in computer systems. First-come-first-serve (FCFS) provides simplicity by processing requests in arrival order, while shortest-job-first (SJF) and its preemptive variant, shortest-remaining-time-first (SRTF), optimize for average latency by prioritizing quick tasks. Though theoretically optimal for minimizing average latency, these approaches can lead to starvation of longer jobs. Multi-level feedback queue (MLFQ) attempts to balance fairness and efficiency through multiple priority queues, but struggles with mixed workloads. In practice, modern Linux systems employ the completely fair scheduler (CFS), which uses a red-black tree to track process runtime and aims to give each process a fair share of CPU time

\section{Conclusion}
In this paper, we present ECCOS, a novel framework for scheduling multiple LLMs that explicitly considers both response quality and workload constraints while minimizing operational costs. We formalize the multi-LLM scheduling problem as a constrained optimization problem, providing a principled foundation for balancing multiple system requirements. Then, we propose a two-stage scheduling framework that combines multi-objective predictions and constrained optimization. The framework's predictor effectively assesses both model capabilities and costs, while its optimizer ensures optimal resource utilization under various constraints.
The experimental results using our introduced QAServe dataset demonstrate ECCOS's significant advantages over existing approaches. The framework not only achieves better cost efficiency but also maintains high response quality across different serving scenarios. Our ablation studies further reveal the robustness of ECCOS under varying constraint settings, confirming its practicality for real-world deployments.


\section{Limitation}
While our current work provides valuable insights into single-system scheduling, extending this framework to distributed systems remains an important direction for future research. Additionally, we have demonstrated the effectiveness of our scheduling approach primarily on question-answering (QA) tasks where ground truth answers are available or where responses can be reliably evaluated by LLM judges. Future work could explore adapting this framework to more complex tasks such as code generation and open-domain question answering, which present unique challenges in response quality assessment.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Use this command to include your bibliography file.
% \newpage
% \bibliographystyle{acl_natbib.bst}
\bibliography{main}

\clearpage
\appendix
\onecolumn

\section*{Appendix}
This appendix contains additional details for the paper. The appendix is organized as follows:
% \vspace{-4em}
Section reports \S\ref{app:proof} is about \textbf{complete induction of the constrained optimization process. }
Section reports \S\ref{app:dataset} is about \textbf{dataset details}.
Section reports \S\ref{app:add_exp} is about \textbf{additional experimental results}.

\section{Complete Induction of the Constrained Optimization Process} \label{app:proof}
The primal problem is as below: 
\begin{align}
\min_{x} \quad & \sum_{i=1}^N \sum_{j=1}^M c_{i,j} x_{i,j} \\
\text{s.t.} \quad & \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^M a_{i,j} x_{i,j} \geq \alpha & \sum_{i=1}^N x_{i,j} \leq L_j, \forall j \in [M] \\
& \sum_{j=1}^M x_{i,j} = 1, \forall i \in [N] & x_{i,j} \in \{0,1\}, \forall i,j
\end{align}
we introduce three types of Lagrange multipliers as:
\begin{itemize}
    \item $\lambda_1 \geq 0$ for the quality constraint (inequality)
    \item $\lambda_{2,j} \geq 0$ for each capacity constraint (inequality)
    \item $\mu_i$ for each assignment constraint (equality)
\end{itemize}
and write the Lagrangian function as:
\begin{align}
& L(x,\lambda_1,\lambda_{2,j}, \mu_i) \\
& = \sum_{i=1}^N \sum_{j=1}^M c_{i,j} x_{i,j} + \lambda_1 \left(-\frac{1}{N} \sum_{i=1}^N \sum_{j=1}^M a_{i,j} x_{i,j} + \alpha \right) \\
&\quad + \sum_{j=1}^M \lambda_{2,j} \left(\sum_{i=1}^N x_{i,j} - L_j\right) + \sum_{i=1}^N \mu_i \left(\sum_{j=1}^M x_{i,j} - 1\right)
\end{align}
which can rearranged to group terms with $x_{i,j}$:
\begin{align}
& L(x,\lambda_1,\lambda_{2,j}, \mu_i) = \sum_{i=1}^N \sum_{j=1}^M x_{i,j}\left(c_{i,j} - \frac{\lambda_1 a_{i,j}}{N} + \lambda_{2,j} + \mu_i\right) + \lambda_1\alpha - \sum_{j=1}^M \lambda_{2,j}L_j - \sum_{i=1}^N \mu_i
\end{align}
The KKT optimality conditions for this problem are: 
Stationarity:
\begin{align}
\frac{\partial L}{\partial x_{i,j}} = c_{i,j} - \frac{\lambda_1 a_{i,j}}{N} + \lambda_{2,j} + \mu_i = 0, \quad \forall i,j
\end{align}
Primal Feasibility:
\begin{align}
& \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^M a_{i,j} x_{i,j} \geq \alpha, \quad \sum_{i=1}^N x_{i,j} \leq L_j, \forall j, \quad \sum_{j=1}^M x_{i,j} = 1, \forall i
\end{align}
Dual Feasibility:
\begin{align}
& \lambda_1 \geq 0, \quad \lambda_{2,j} \geq 0, \forall j
\end{align}
Complementary Slackness:
\begin{align}
& \lambda_1 \left(\alpha - \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^M a_{i,j} x_{i,j}\right) = 0, \quad \lambda_{2,j} \left(L_j - \sum_{i=1}^N x_{i,j}\right) = 0, \quad \forall j
\end{align}
Note that for each $i$, we have the equality constraint $\sum_{j=1}^M x_{i,j} = 1$. This implies that for each query $i$, exactly one LLM $j$ must be selected. From the stationarity condition, for any fixed $i$, comparing two different indices $j$ and $k$:
\begin{align}
& c_{i,j} - \frac{\lambda_1 a_{i,j}}{N} + \lambda_{2,j} + \mu_i = 0 \
& c_{i,k} - \frac{\lambda_1 a_{i,k}}{N} + \lambda_{2,k} + \mu_i = 0
\end{align}
Subtracting these equations eliminates $\mu_i$:
\begin{align}
(c_{i,j} - \frac{\lambda_1 a_{i,j}}{N} + \lambda_{2,j}) = (c_{i,k} - \frac{\lambda_1 a_{i,k}}{N} + \lambda_{2,k})
\end{align}
This implies that for a given $i$, the optimal solution should choose the $j$ that minimizes $(c_{i,j} - \frac{\lambda_1 a_{i,j}}{N} + \lambda_{2,j})$.

Then the dual function becomes:
\begin{align}
& g(\lambda_1,\lambda_2) = \min_{x_{i,j}} \{\sum_{i=1}^N\sum_{j=1}^M x_{i,j}(c_{i,j} - \frac{\lambda_1a_{i,j}}{N} + \lambda_{2,j}) + \lambda_1\alpha - \sum_{j=1}^M \lambda_{2,j}L_j\}
\end{align}
Note that $\mu_i$ has disappeared from the dual function because we've analytically incorporated the equality constraints.
The dual problem can now be written as:
\begin{align}
\max_{\lambda_1,\lambda_2} \quad & g(\lambda_1,\lambda_2), & \text{s.t.} \quad \lambda_1 \geq 0, \quad \lambda_{2,j} \geq 0, \quad \forall j \in [M]
\end{align}
The partial derivatives for the remaining multipliers are:
For $\lambda_1$:
\begin{align}
\frac{\partial L}{\partial \lambda_1} = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^M x_{i,j}a_{i,j} + \alpha
\end{align}
For $\lambda_{2,j}$:
\begin{align}
\frac{\partial L}{\partial \lambda_{2,j}} = \sum_{i=1}^N x_{i,j} - L_j, \quad \forall j \in [M]
\end{align}
The gradient ascent update rules are:
\begin{align}
\lambda_1^{t+1} = & \max\left(\lambda_1^t + \alpha_1\left(-\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^M x_{i,j}a_{i,j} + \alpha\right), 0\right) \\
\lambda_{2,j}^{t+1} = & \max\left(\lambda_{2,j}^t + \alpha_2\left(\sum_{i=1}^N x_{i,j} - L_j\right), 0\right), \quad \forall j \in [M]
\end{align}
For fixed multipliers, the optimal assignment for each $i$ is:
\begin{align}
x_{i,j} = \begin{cases}
1 & \text{if } j = j^*_i \\
0 & \text{otherwise}
\end{cases}
\end{align}
where
\begin{align}
j^*_i = \arg\min_{j \in [M]} \left(c_{i,j} - \frac{\lambda_1 a_{i,j}}{N} + \lambda_{2,j}\right)
\end{align}

\begin{itemize}
    \item $\lambda_1$ acts as a penalty for violating the quality constraint. When the average quality is below $\alpha$, $\lambda_1$ increases, encouraging selection of higher-quality options.
    \item $\lambda_{2,j}$ penalizes capacity violations for each model $j$. When a model exceeds its capacity $L_j$, its corresponding multiplier increases, making it less attractive in subsequent iterations.
    \item The equality constraints ($\mu_i$) are handled analytically by direct substitution, which simplifies the dual problem.
    \item The algorithm alternates between updating multipliers and assignments until convergence. 
\end{itemize}

\section{Dataset Details} \label{app:dataset}

\subsection{Statistics of the Dataset}
The statistics of the QAServe are in \autoref{tab:sample} and an data sample example is shown as below: 
\begin{table}[tb!]
\centering
\caption{Statistics of the data distribution in our dataset. } \label{tab:sample}
\begin{tabular}{ccc}
\toprule
Data Source  & \# Samples & Ratio \\ \midrule
MMLU         & 1000       & 37.06\% \\
GPQA-Diamond & 198        & 7.33\%  \\
Math-500     & 500        & 18.53\% \\
GSM8K        & 100        & 37.06\% \\ \bottomrule
\end{tabular}
\end{table}

\begin{lstlisting}[style=json]
{
    "query": "Which of the following is encoded by the src oncogene and catalyzes the specific addition of phosphate groups to cellular proteins? Choose the correct answer from the following options: Protein phosphatase, Adenylate cyclase, G protein-linked receptor, Tyrosine kinase",
    "gt_answer": "Tyrosine kinase",
    "input_token_length": 88,
    "outputs": [
        {
            "model_name": "Qwen-2.5-7B-Instruct",
            "prediction": "The src oncogene encodes ... Therefore, the correct answer is: Tyrosine kinase.",
            "output_token_length": 61,
            "correctness": true
        },
        {
            "model_name": "Qwen-2.5-14B-Instruct",
            "prediction": "The correct answer is Tyrosine kinase. The src oncogene encodes ...",
            "output_token_length": 103,
            "correctness": true
        },
        {
            "model_name": "Qwen-2.5-32B-Instruct",
            "prediction": "The correct answer is **Tyrosine kinase**. The src oncogene encodes ...",
            "output_token_length": 80,
            "correctness": true
        },
        {
            "model_name": "Llama-3.1-8B-Instruct",
            "prediction": "## Step 1: Identify what the src oncogene ... The final answer is: $\\boxed{Tyrosine kinase}$",
            "output_token_length": 255,
            "correctness": true
        },
        {
            "model_name": "Deepseek-r1-7b",
            "prediction": "<think>\nOkay, so I have this biology question ... Answer: Tyrosine kinase",
            "output_token_length": 440,
            "correctness": true
        },
        {
            "model_name": "Deepseek-r1-14b",
            "prediction": "<think>\nOkay, so I have this question ... Answer: **Tyrosine kinase**",
            "output_token_length": 456,
            "correctness": true
        }
    ]
}
\end{lstlisting}




\subsection{Response Correctness Evaluation}
We use the Llama-3.1-70B-Instruct \citep{dubey2024llama} as the LLM judge to evaluate whether a generated response meets the ground truth answers. The prompt is shown as below. 
\begin{tcolorbox}[
    title=\texttt{Prompt for using LLM as the judge. },
    width=\columnwidth % Makes the tcolorbox span the full page width
]
\begin{flushleft}
\textbf{Prompt:} The ground truth answer is: \{gt\_answer\}. The prediction answer is: {extracted\_answer}. Judge whether the prediction answer is correct or not. You just need to output 'True' or 'False'. 

% \textbf{Ground Truth:} 383816
\end{flushleft}
\end{tcolorbox}
% \caption{The prompt used for utilizing the Llama-3.1-70B-Instruct as a judge to evaluate whether a response matches the ground truth. }
\label{fig:prompt_in_passkey_LLM}


\section{Additional Experiments} \label{app:add_exp}

\subsection{More Results under the Streaming Setting}
We conduct experiments to investigate how performance threshold ($\alpha$) and concurrent workload limit ($L$) influence our scheduling framework under the streaming setting. 

\paragraph{Effects of Performance Constraints. }\autoref{fig:ccost_s} and \autoref{fig:cperf_s} reveals ECCOS-R exhibits a U-shaped cost curve with an optimal point at $\alpha=0.75$, while consistently maintaining higher performance than ECCOS-T. This suggests that moderate performance requirements might actually lead to more efficient resource utilization, as they allow the scheduler to better balance between high-quality but expensive models and economic but less reliable ones. The divergent behaviors between ECCOS-R and ECCOS-T under increasing $\alpha$ also reveal that retrieving similar historical cases provides more adaptive quality control than learned patterns from training data.

\paragraph{Effects of Workload Constraints. }
As is illustrated in \autoref{fig:cload_perf_s} and \autoref{fig:cload__cost_s}, the workload constraint experiments ($L$ from 4 to 16) expose the fundamental differences in how these approaches handle system scaling. ECCOS-R demonstrates remarkable performance resilience under increased workload, maintaining high performance (0.75+) even at $L=16$, albeit with gradually increasing costs. This indicates that the retrieval-based method's effectiveness scales well with system capacity, possibly because larger concurrent workloads provide more opportunities for fine-grained task-model matching. In contrast, ECCOS-T shows performance saturation at higher $L$ values, suggesting that trained patterns might have limitations in capturing complex scheduling dynamics under heavy workloads. These findings not only illuminate the scalability characteristics of different scheduling strategies but also point to a deeper principle: the advantage of dynamic, instance-based decision making (ECCOS-R) over static, pattern-based approaches (ECCOS-T) becomes more pronounced as the scheduling environment grows more complex.

\begin{figure}[tb]
    \centering
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/cperf_cost_s.pdf}
        \caption{$\alpha$ to cost. }
        \label{fig:cperf_s}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/cperf_perf_s.pdf}
        \caption{$\alpha$ to performance. }
        \label{fig:ccost_s}
    \end{subfigure}
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/cload_cost_s.pdf}
        \caption{$L$ to cost. }
        \label{fig:cload__cost_s}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/cload_perf_s.pdf}
        \caption{$L$ to performance. }
        \label{fig:cload_perf_s}
    \end{subfigure}
    \hfill
    \caption{The effects of performance constraint $\alpha$ and concurrent workload constraint $L$ on the serving quality and cost efficiency under the streaming (S) setting. } \label{fig:perf_constraint}
\end{figure}

\section{Details of Related Work}
% \subsection{LLM Reasoning and Decoding Strategy}
Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including mathematical reasoning \citep{wei2022chain, yao2024tree, besta2024graph, jin-etal-2024-impact}, code generation \citep{wei2023magicoder, nijkamp2023codegen2, hui2024qwen2}, and recommender systems ~\cite{xu2024slmrec,xu2024rethinking,xu2023neural,mei2023lightlm}. 
Many studies \citep{wei2022chain, yao2024tree, besta2024graph, jin-etal-2024-impact, jin-etal-2025-exploring} have demonstrated that different prompts can have great influences on the model behavior, thus making the prediction of LLM generation length challenging. Early attempts like Magnus \citep{cheng2024enabling} employed random forest algorithms but achieved limited accuracy. Subsequent research has explored two main directions of prediction models: encoder-only models for classification (DynamoLLM \citep{stojkovic2024dynamollm}, S3 \citep{jin2023s3}, TerriInfer \citep{hu2024inference}, SSJF \citep{aiops2024qiu}, and $\mu$3 \citep{qiu2024power}) and decoder-only models for generative prediction like Perception-only (PO) \citep{zheng2024response}. \citet{fu2024efficient} reformulated this as a ranking problem and utilized listwise ranking for predictor training. Due to the inherent difficulty in precise output length prediction, several works \citep{jin2023s3, zheng2024response, fu2024efficient} adopted a bucketing approach for approximate estimation. However, these existing studies primarily emphasize computational efficiency while overlooking a critical aspect: model capability - specifically, whether a model has the capability to answer a given query. Therefore, considering both response quality and response cost to construct a general scheduling framework remains a critical challenge. 

% \noindent \textbf{Effects of Factors in Predictors. }
% \begin{table}[tb!]
% \centering
% \caption{Effects of Number of Buckets on Training-based Predictor and Overall Scheduling Performance. }
% \adjustbox{max width=0.8\linewidth}{
% \begin{tabular}{ccccc}
% \toprule
% \multirow{2}{*}{\# Bucket} & \multicolumn{2}{c}{Bucket Acc ($\uparrow$)} & \multirow{2}{*}{SR ($\uparrow$)} & \multirow{2}{*}{\$ Cost ($\downarrow$)} \\ \cmidrule(lr){2-3}
%                            & Exact Match                & $\pm1$               &                                   &                                         \\ \midrule
% 10                         &  0.452                          & 0.806               &                                  &                                         \\
% 20                         & 0.269                           & 0.526               &                                  &                                         \\
% 50                         & 0.162                           & 0.299               &                                  &                                         \\
% 100                        & 0.124                           & 0.189               &                                  &                                         \\ \bottomrule
% \end{tabular}
% }

% \end{table}


% \begin{table}[tb!]
% \centering
% \caption{Effects of Variations of K on Retrieval-based Predictor and Overall Scheduling Performance. }
% \adjustbox{max width=1.0\linewidth}{
% \begin{tabular}{cccccc}
% \toprule
% \multirow{2}{*}{K} & \multirow{2}{*}{Perf. Acc ($\uparrow$)} & \multicolumn{2}{c}{Length Acc ($\uparrow$)} & \multirow{2}{*}{SR ($\uparrow$)} & \multirow{2}{*}{\$ Cost ($\downarrow$)} \\ \cmidrule(lr){3-4}
%                    &                                         & Exact Match      & $\pm1$      &                                  &                                         \\ \midrule
% 4                  & 0.773                                   & 0.340            & 0.694                    &                                  &                                         \\
% 8                  & 0.775                                   & 0.343            & 0.696                    &                                  &                                         \\
% 16                 & 0.774                                   & 0.346            & 0.700                    &                                  &                                         \\
% 32                 & 0.765                                   & 0.340            & 0.676                    &                                  &                                         \\
% 64                 & 0.760                                   & 0.322            & 0.664                    &                                  &                                         \\ \bottomrule
% \end{tabular}
% }

% \end{table}

\end{document}
