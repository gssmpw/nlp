@inproceedings{aiops2024qiu,
  author  = {Qiu, Haoran and Mao, Weichao and Patke, Archit and Cui, Shengkun and Jha, Saurabh and Wang, Chen and Franke, Hubertus and Kalbarczyk, Zbigniew T. and Ba\c{s}ar, Tamer and Iyer, Ravishankar K.},
  title   = {Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction},
  year    = {2024},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  volume = {5},
  address = {San Diego, CA, USA},
  booktitle = {The 5th International Workshop on Cloud Intelligence / AIOps at ASPLOS 2024},
}

@article{cheng2024enabling,
  title={Enabling Efficient Batch Serving for LMaaS via Generation Length Prediction},
  author={Cheng, Ke and Hu, Wen and Wang, Zhi and Du, Peng and Li, Jianguo and Zhang, Sheng},
  journal={arXiv preprint arXiv:2406.04785},
  year={2024}
}

@article{fu2024efficient,
  title={Efficient LLM Scheduling by Learning to Rank},
  author={Fu, Yichao and Zhu, Siqi and Su, Runlong and Qiao, Aurick and Stoica, Ion and Zhang, Hao},
  journal={arXiv preprint arXiv:2408.15792},
  year={2024}
}

@article{hu2024inference,
  title={Inference without interference: Disaggregate llm inference for mixed downstream workloads},
  author={Hu, Cunchen and Huang, Heyang and Xu, Liangliang and Chen, Xusheng and Xu, Jiang and Chen, Shuang and Feng, Hao and Wang, Chenxi and Wang, Sa and Bao, Yungang and others},
  journal={arXiv preprint arXiv:2401.11181},
  year={2024}
}

@article{jin2023s3,
  title={S3: Increasing GPU Utilization during Generative Inference for Higher Throughput},
  author={Jin, Yunho and Wu, Chun-Feng and Brooks, David and Wei, Gu-Yeon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={18015--18027},
  year={2023}
}

@inproceedings{qiu2024power,
  title={Power-aware Deep Learning Model Serving with $\{$$\mu$-Serve$\}$},
  author={Qiu, Haoran and Mao, Weichao and Patke, Archit and Cui, Shengkun and Jha, Saurabh and Wang, Chen and Franke, Hubertus and Kalbarczyk, Zbigniew and Ba{\c{s}}ar, Tamer and Iyer, Ravishankar K},
  booktitle={2024 USENIX Annual Technical Conference (USENIX ATC 24)},
  pages={75--93},
  year={2024}
}

@article{stojkovic2024dynamollm,
  title={Dynamollm: Designing llm inference clusters for performance and energy efficiency},
  author={Stojkovic, Jovan and Zhang, Chaojie and Goiri, {\'I}{\~n}igo and Torrellas, Josep and Choukse, Esha},
  journal={arXiv preprint arXiv:2408.00741},
  year={2024}
}

@inproceedings{zheng2024response,
  title={Response length perception and sequence scheduling: An llm-empowered llm inference pipeline},
  author={Zheng, Zangwei and Ren, Xiaozhe and Xue, Fuzhao and Luo, Yang and Jiang, Xin and You, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

