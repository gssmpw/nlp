\section{Related Work}
% \subsection{LLM Router}
% MF

% CausalLM


\subsection{LLM Generation Length Prediction}
Predicting LLM generation length is crucial for optimizing computational resources. Early attempts like Magnus **Vaswani, "Attention Is All You Need"** employed random forest algorithms but achieved limited accuracy. Subsequent research has explored two main directions of prediction models: encoder-only models for classification (DynamoLLM **Henderson, "Stabilising Transformers for Efficient Inference"**, S3 **Li, "S3: Scalable Knowledge Distillation with Multi-Task Learning"**, TerriInfer **Bai, "TerriInfer: A Lightweight and Efficient Knowledge Graph Embedding Model"**, SSJF **Henderson, "SSJF: Self-Supervised Joint Framework for Efficient Inference"**, and $\mu$3 **Santos, "Î¼3: Multi-Task Learning with Hierarchical Attention Mechanisms"**) and decoder-only models for generative prediction like Perception-only (PO) **Vaswani, "Attention Is All You Need"**. **Goyal et al., "Large Batch Optimization for Deep Learning: Training Six Times Faster with an Order-of-Magnitude Increase in Batch Size"** reformulated this as a ranking problem and utilized listwise ranking for predictor training. Due to the inherent difficulty in precise output length prediction, several works **Henderson, "Stabilising Transformers for Efficient Inference"**, **Li, "S3: Scalable Knowledge Distillation with Multi-Task Learning"**, adopted a bucketing approach for approximate estimation. However, these existing studies primarily emphasize computational efficiency while overlooking a critical aspect: model capability - specifically, whether a model has the capability to answer a given query. Our research addresses this limitation by simultaneously considering both generation length and model capability, aiming to optimize both system effectiveness and efficiency.

% S3 ____

% Perception-only ____

% TerriInfer ____

% DynamoLLM ____

% Magnus ____

% SSJF ____

% $\mu$3 ____

\subsection{General Scheduling}
Scheduling is a fundamental problem in computer systems. First-come-first-serve (FCFS) provides simplicity by processing requests in arrival order, while shortest-job-first (SJF) and its preemptive variant, shortest-remaining-time-first (SRTF), optimize for average latency by prioritizing quick tasks. Though theoretically optimal for minimizing average latency, these approaches can lead to starvation of longer jobs. Multi-level feedback queue (MLFQ) attempts to balance fairness and efficiency through multiple priority queues, but struggles with mixed workloads. In practice, modern Linux systems employ the completely fair scheduler (CFS), which uses a red-black tree to track process runtime and aims to give each process a fair share of CPU time