%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%

\documentclass[sigconf]{acmart}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx, subfig}
\usepackage{tabularx}
\usepackage{expl3}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{threeparttable}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{tcolorbox}
\tcbuselibrary{breakable}
\usepackage{lipsum}
\usepackage{xparse}
\usepackage{array} % For defining 'p' column specifier
\usepackage{natbib}
\usepackage{listings}
\usepackage{xcolor}


%%% renjun's cmds
\usepackage{xspace}
\newcommand{\marked}[1]{{\color{blue}#1}} 
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\eat}[1]{}
\newcommand{\modelname}{{Themis}\xspace} % \texttt
\newcommand{\aligndata}{{Alignbench}\xspace} % \texttt
\newcommand{\syndata}{{SynUI}\xspace} % \texttt
\newcommand{\meanOp}[1]{\mbox{mean}\{#1\}}
\newcommand{\stitle}[1]{\textbf{#1}.}
\newcommand{\etitle}[1]{\underline{#1}.}
\newcommand{\agrpq}[2]{\mbox{Agr}(#1,#2)}

%\usepackage[backend=biber, style=acmnumeric, maxnames=8]{biblatex}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{acmlicensed}\acmConference[WWW Companion '25]{Companion Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3701716.3715265}
\acmISBN{979-8-4007-1331-6/25/04}
% 1 Authors, replace the red X's with your assigned DOI string during the rightsreview eform process.
% 2 Your DOI link will become active when the proceedings appears in the DL.
% 3 Retain the DOI string between the curly braces for uploading your presentation video.

\settopmatter{printacmref=true}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Training an LLM-as-a-Judge Model: Pipeline, Insights, and Practical Lessons}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Renjun Hu}
\authornote{Equal contribution to this work. Please refer to Renjun Hu for any correspondence.}
\email{rjhu@dase.ecnu.edu.cn}
\affiliation{%
  \institution{East China Normal University}
  \city{Shanghai}
  \country{China}
}

\author{Yi Cheng} 
\email{ceyu.cy@alibaba-inc.com}
\authornotemark[1]
\affiliation{%
  \institution{Alibaba Cloud Computing }
  \city{Hangzhou}
  \country{China}
}

\author{Libin Meng}
\email{menglibin.mlb@alibaba-inc.com}
\authornotemark[1]
\affiliation{%
  \institution{Alibaba Cloud Computing }
  \city{Shanghai}
  \country{China}
}

\author{Jiaxin Xia}
\email{xjx392321@alibaba-inc.com}
\authornotemark[1]
\affiliation{%
  \institution{Alibaba Cloud Computing }
  \city{Shanghai}
  \country{China}
}

\author{Yi Zong}
\email{yzong22@m.fudan.edu.cn}
\authornotemark[1]
\affiliation{%
  \institution{Fudan University}
  \city{Shanghai}
  \country{China}
}

\author{Xing Shi}
\email{shubao.sx@alibaba-inc.com}
\author{Wei Lin}
\email{weilin.lw@alibaba-inc.com}
\affiliation{%
  \institution{Alibaba Cloud Computing }
  \city{Hangzhou}
  \country{China}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Hu et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The rapid advancement of large language models (LLMs) has opened new possibilities for their adoption as evaluative judges. This paper introduces \modelname, a fine-tuned LLM judge that delivers sophisticated context-aware evaluations. We provide a comprehensive overview of the development pipeline for \modelname, highlighting its scenario-dependent evaluation prompts and two novel methods for controlled instruction generation. These designs enable \modelname to effectively distill evaluative skills from teacher models, while retaining flexibility for continuous development. We introduce two human-labeled benchmarks for meta-evaluation, demonstrating that \modelname can achieve high alignment with human preferences in an economical manner.
%
Additionally, we explore insights into the LLM-as-a-judge paradigm, revealing nuances in performance and the varied effects of reference answers. Notably, we observe that pure knowledge distillation from strong LLMs, though common, does not guarantee performance improvement through scaling. We propose a mitigation strategy based on instruction-following difficulty. Furthermore, we provide practical guidelines covering data balancing, prompt customization, multi-objective training, and metric aggregation. We aim for our method and findings, along with the fine-tuning data, benchmarks, and model checkpoints, to support future research and development in this area.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010179</concept_id>
       <concept_desc>Computing methodologies~Natural language processing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Natural language processing}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Large language models; LLM-as-a-judge; LLM evaluation}
% fine-tuning

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:intro}

The rapid advancement in large language models (LLMs) has endowed today's most capable artificial intelligence systems with near-human cognitive abilities, including  language understanding, mastery of world knowledge, instruction following, reasoning, and planning~\cite{qwen,openai2024gpt4,geminiteam2024gemini}. Often likened to revolutionary technologies such as electricity, LLMs are being deployed across various domains, including those with high-stakes~\cite{echterhoff2024highstakes,Schwartz2023BlackBW,Peng2023ASO}. Alongside rapid progress and widespread adoption comes an increasing concern on the large-scale potential risks~\cite{science.adn0117}.
As LLMs continue to evolve, evaluating their capacity~\cite{hendrycks2021mmlu,huang2023ceval} as well as alignment with user intentions~\cite{chiang2024chatbotarena,li2023autoj,ke2024critiquellm}, ethical standards~\cite{jiang202morality,scherrer2023evaluating}, and human values~\cite{jiang2024llm-value,hendrycks2021ethics,biedma2024humannorms}  becomes pivotal. % increasingly


In this study, we focus on assessing LLMs' alignment with user intentions in open-ended tasks, \ie the ability to accurately adhere to open-ended instructions and meet user expectations~\cite{zheng2023judging}. This represents the most natural usage of LLMs and such alignment is fundamental to ensuring their helpfulness~\cite{Bai2022TrainingAH}. 
While manual evaluation~\cite{chiang2024chatbotarena} is straightforward, it is expensive and can suffer from subjective inconsistency.
Established evaluation metrics such as perplexity~\cite{Jelinek1977PerplexityaMO}, BLUE~\cite{PapineniRWZ02Bleu}, and ROUGE~\cite{Lin2004ROUGE} often fall short in capturing the nuanced dimensions of alignment evaluation. Additionally, the assumption of unique ground-truth reference responses is frequently invalid in many open-ended scenarios, \eg advice seeking and writing assistance. 
These gaps underscore the necessity for more sophisticated and context-aware evaluation mechanisms capable of operating automatically.


The continually improving capabilities of LLMs has created a new paradigm for this problem: deploying LLMs as judges to assess other LLMs, known as LLM-as-a-judge~\cite{li2023autoj,ke2024critiquellm,zheng2023judging,lin2024RethinkingAlignment}. Recent studies have demonstrated that general-purpose or specifically fine-tuned LLMs % with intelligence on the level of GPT-4 or  / of reasonable sizes
are qualified judges in at least two aspects.
First, they could obtain a high evaluation agreement rate with human, matching the same level of human-human agreement. Second, they are able to deliver nuanced evaluations, providing more granular and explainable assessments than traditional metrics. 
Thus, the new paradigm has emerged as a robust and scalable solution to alignment evaluation. While promising, established judges are either general-purpose LLMs~\cite{lin2024wildbench,zheng2023judging} or collect real user instructions to construct their fine-tuning datasets~\cite{li2023autoj,ke2024critiquellm}, being inflexible for developing evaluation ability for various LLM applications.

To this end, we introduce  \modelname, a judge LLM retaining flexibility for continuous development. We first detail the complete development pipeline, encompassing prompt design, data construction, fine-tuning, and performance assessment. 
We adopt step-by-step scenario-dependent prompts for evaluation~\cite{li2023autoj,ke2024critiquellm} and carefully design the evaluation criteria for each scenario through human-AI collaboration. These prompts strike a balance between context-enhanced accuracy and automation, compared with unified~\cite{zheng2023judging,lin2024RethinkingAlignment} and instruction-based prompts~\cite{lin2024wildbench}.
A distinguishing feature of \modelname is the use of controlled instruction generation. Unlike previous work that relies on existing instruction sets~\cite{zheng2024lmsyschat1m}, we develop two instruction synthesis methods: reference-based questioning and role-playing quizzing, which could generate or supplement instruction data in a controlled manner. 
%
The step-by-step evaluation prompts and instruction synthesis methods together allow to comprehensively induce the evaluative skills from state-of-the-art LLMs (says, GPT-4), which are distilled into \modelname through supervised fine-tuning~\cite{mukherjee2023orca}. The combination also features flexibility for continuous development to fulfill real evaluation requirements.
%
We create two human preference benchmarks for meta-evaluation. The results reveal that \modelname achieves comparable and slightly worse performance on the in- and out-of-distribution benchmarks, respectively, using less than 1\% of parameters compared with its teacher GPT-4, and outperforms all other tested (judge) LLMs. These validate the effectiveness of the pipeline.

We next conduct a series of in-depth analyses of \modelname from a scenario-centric perspective, which yield several key insights that deepen our understanding of the paradigm. 
Our examination reveals a positive correlation between LLMs' capacity and the corresponding evaluation performance for scenarios: \modelname performs relatively well in open-ended scenarios, where LLMs' inherent capabilities are better suited, whereas it is less effective in closed-ended scenarios. 
Additionally, we analyze the impacts of reference answers on LLM-based evaluation. Our findings indicate that reference answers can improve evaluations in closed-ended scenarios, compensating the defect of direct evaluations, but have only negligible or even detrimental effects in open-ended scenarios. 
Furthermore, we investigate how fine-tuning data composition and scaling affect model performance. Surprisingly, we find that pure knowledge distillation from teacher LLMs does not guarantee performance improvement through scaling, indicating the inherent quality flaw in model-generated fine-tuning data. This poses a significant challenge for practical data engineering. In response to it, we propose a mitigation using instruction-following difficulty as a metric to guide data filtering.

We finally share practical lessons learned from \modelname and offer some advices for model optimization. These include creating more balanced fine-tuning datasets, supporting custom evaluation prompts to enhance generalization while minimizing memorization, and employing multi-objective training for further improvement. We also recommend metric aggregation to provide a single score for assessing optimization effectiveness, which is critical to maintain development efficiency for building a versatile LLM judge. 
\modelname currently offers evaluation service on Alibaba Cloud through API,\footnote{\url{https://www.alibabacloud.com/help/en/pai/user-guide/judge-model/}} compatible with Python openai SDK. 
%A demonstration is available at ModelScope.\footnote{https://modelscope.cn/studios/PAI/PAI-Themis}  
We open-source our data, benchmarks, and model checkpoints to support future research.\footnote{\url{https://github.com/aigc-apps/pai-judge-themis}}

\eat{
In summary, our contributions are three-fold:
\begin{itemize}
    \item We introduce a complete pipeline of training an LLM-as-a-Judge model. The resulting model could offer automatic and contextually informed evaluations with an accuracy close to GPT-4 while using much lower serving costs.
    \item We present both key insights and practical recommendations for the LLM-as-a-Judge paradigm.
    \item We will release our data, benchmarks, and model checkpoints to advance further research and development.
\end{itemize}
}



\input{sections/relatedwork}

\input{sections/pipeline}

\input{sections/insights}

\input{sections/lessons}

\input{sections/conclusion}



% \clearpage

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{themis}

\input{sections/appendix-long}

\end{document}
\endinput
%%
