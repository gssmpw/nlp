\section{Related Work}
\label{sec:related}

LLMs have greatly revolutionized the field of natural language processing. Classic overlapping-based methods~\cite{PapineniRWZ02Bleu,Lin2004ROUGE} are no longer suitable for evaluating today's LLMs. Human evaluation suffers from its high cost and is time-consuming~\cite{chiang2024chatbotarena}.
Currently, automatic LLM evaluation could be roughly divided into three categories.

\stitle{Evaluation on static benchmarks}
Static benchmarks have been developed to assess the performance of LLMs across various tasks, such as language understanding~\cite{hendrycks2021mmlu}, world knowledge~\cite{huang2023ceval,zhong2023agieval}, reasoning~\cite{zellers2019hellaswag}, coding~\cite{chen2021codex}, and math~\cite{hendrycks2021measuring}. 
These benchmarks usually consist of objective, \eg multi-choice, questions designed to rigorously evaluate specific abilities. Metrics like accuracy could then serve as valuable references for model comparison and advancement. 
However, evaluation on static benchmarks has its limitations. A notable shortcoming is that the tested metrics only capture LLMs' performance on predefined tasks with close-ended outputs, resulting in a gap between user perceptions of LLMs' usefulness in real-world applications. 
Additionally, static benchmarks may inadvertently incentivize models to over-fit rather than developing generalizable ability~\cite{zhou2023dontmake,bordt2024elephants}. Recent efforts have sought to expand the variety of benchmarks to include more diverse and up-to-date knowledge~\cite{mousavi2024dyknow} or reasonable perturbations~\cite{li2024perteval} for assessing genuine capacity.
%better reflect practical usage scenarios.

\stitle{Human-inspired evaluation}
Techniques in the second category treat LLMs as if they possess human-like qualities, utilizing methodologies originally developed for human assessments to evaluate these models. This types of approaches often focus on the social characteristics of LLMs, such as creativity~\cite{zhao2024creativity}, values~\cite{jiang2024llm-value,hendrycks2021ethics,biedma2024humannorms}, ethical standards~\cite{jiang202morality,scherrer2023evaluating}, trustworthiness~\cite{sun2024trustllmtrustworthinesslargelanguage}, etc.


\stitle{LLM-as-a-Judge}
The concept of utilizing LLMs as judges has emerged as a new evaluation paradigm~\cite{zheng2023judging,lin2024RethinkingAlignment}. This innovative approach leverages the intrinsic capabilities of LLMs to provide fine-grained evaluations, and it has been demonstrated that LLMs can achieve high agreement rates with human evaluators~\cite{zheng2023judging,lin2024wildbench}, effectively serving as substitutes for traditional evaluation metrics. Along the line, some studies have devoted to constructing instruction sets for evaluation, with those large-scale~\cite{zheng2024lmsyschat1m}, in the wild~\cite{lin2024wildbench}, and challenging~\cite{li2024crowdsourceddatahighqualitybenchmarks} standing out.
Others have explored fine-tuning an LLM as judge, which has also been verified effective and more economical~\cite{ke2024critiquellm,li2023autoj,vu2024foundationalautoraterstaminglarge}. 
%This paradigm includes fine-tuning specific LLMs to assess the alignment of other models with user intentions, thereby addressing the critical issue of evaluation consistency.

Our work belongs to the third category, differing from related work in two aspects. Methodologically, \modelname integrates a combination of step-by-step evaluation prompts and controlled instruction generation, featuring better flexibility to develop required evaluation ability. Empirically, we provide both scenario-centric insights and lessons for LLM-as-a-judge from an industrial perspective. Notably, the observed quality flaw in model-generated  data and our mitigation present a unique complement to the area.

%We introduced a judge model \modelname in this work. What sets it apart is the controlled instruction generation techniques for data synthesis. This allows for greater adaptability in various evaluation contexts, advancing the state of LLM-as-a-Judge methodologies. Our scenario-based insights and optimization recommendations are another complement to the literature. 









% Classic benchmarks for evaluating natural language models often rely on either overlapping-based methods, such as n-grams~\cite{PapineniRWZ02Bleu,Lin2004ROUGE}, or multiple-choice questions~\cite{hendrycks2021mmlu,huang2023ceval}, which fall short in evaluating models from a semantic perspective and many aspects related to the use cases. Recent advancements reveal that as language models grow in size and are trained on larger datasets, they exhibit emergent capabilities~\cite{emgrgent-ability}. These LLMs leverage their pre-training data and are subsequently fine-tuned on preference data to better align with human judgments~\cite{instructgpt, dpo}. This enables LLMs to serve as evaluative judges, \eg conducting natural language based evaluations to replace costly and subjective human labelers, and acting as reward models to support model training by aligning outputs with human preferences.






% Earlier version
\eat {
\section{Related Work}
\label{sec:related}
Traditional natural language benchmarks assess language models' performance using rule-based methods, such as n-grams and multiple-choice questions, but cannot evaluate language models in a semantic perspective.
% 
Fortunately, researchers have found that as the size of the model increases and the amount of training data increases, LLMs exhibit emergent abilities. These LLMs extract knowledge from pre-training data and are fine-tuned using preference data to align with human preferences~\cite{instructgpt, dpo}. Based on this, LLMs can be used as a judge to:

\noindent1. Perform offline evaluations of models, to replace human labelers which is expensive and highly subjective.

\noindent2. Serve as a Reward Model to assist in model training, aiding in aligning with human preferences, etc.

% 
\subsection{Offline Evaluation}
Grammatical and logical errors are often hidden within very few words, making it difficult to detect them using a rule-based approach. Using another LLM~\cite{pandalm, tigerscore, INSTRUCTSCORE} to evaluate the output of the target model has become a common and efficient approach to assist in hyperparameter optimization. Usually, researchers use the output of other LLMs, ChatGPT, and manual methods to construct positive and negative samples for the purpose of fine-tuning these error correction models.
% 

Beyond these dimensions, Auto-J~\cite{li2023autoj} defines many different scenarios, e.g., summarizing and coding, and establishes various evaluation dimensions for each scenario. Experiments have shown that Auto-J, trained on evaluation data, performs on par with ChatGPT in assessing LLM tasks, even while using only 13 billion parameters. This reflects the inadequacy of conventional general models in multi-scenario evaluations and indicates that a well-finetuned model can outperform larger LLM in evaluation tasks with a significantly smaller parameter size, which is highly beneficial for the development of reward models.

\subsection{Online Assistant}
RLHF~\cite{instructgpt} uses human evaluations of model outputs to improve the modelâ€™s decision-making process. First, the model generates a set of candidate outputs, which are then rated by humans based on their quality. The Reward Model is trained on this preference data, and the trained reward model is used to score the outputs produced during the training of the target model, guiding it to ultimately generate outputs that align with human preferences.
% 

However, reinforcement learning leads to unstable training and longer training times. Direct Preference Optimization (DPO) improved this process by eliminating the reward model and the sampling phase, and regards language model as a reward model, which controls the target model to generate preferred data with a higher probability, with a larger probability gap from non-preferred data.
% 

SPIN~\cite{spin} discovered that during the SFT process, the model trained to convergence still showed a significant gap between its outputs and the realistic SFT data. The authors proposed a self-play method, allowing the model to distinguish between real and generated output. The supervision objective is to maximize the probability difference between the realistic outputs and synthetic outputs, thereby achieving stronger model performance with less data.
} %% eat-close