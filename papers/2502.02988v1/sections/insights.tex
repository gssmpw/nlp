
% During the training process, we gained several data-related insights, including the interaction effects between data from different scenarios, the impact of varying data combination ratios, and the influence of different sampling methods on model performance.




\section{Insights from Scenario-centric Analysis}
\label{sec:insight}

% In this section, we present key findings from our scenario-centric analysis of \modelname{'s} performance.



%\subsection{Detailed Performance across Scenarios}
\stitle{Exp-1. Detailed performance across scenarios}
We first investigate the performance of \modelname across different scenarios and the detailed results of single answer grading $\agrpq{2}{2}$, as well as the corresponding z-value, on our benchmarks are reported in Table~\ref{tab:scenario-details}. Z-values exceeding 0.5 and falling below -0.5 are highlighted in bold and underlined, respectively. %, denoting performant and less performant scenarios. 
The results reveal that \modelname generally excels in open-ended scenarios such as role-playing, open QA, creative writing, and informational and professional writing. On the other hand, its performance diminishes in close-ended scenarios like close QA and math-related QA, which demand higher knowledge and reasoning capabilities for accurate responses and evaluations. Additionally, we observe a positive correlation between scenario-based $\agrpq{2}{2}$ and the average labeled scores (\ie avg. $y$) of our responding LLMs on these scenarios: the Pearson correlation coefficient is 0.822 and 0.426 on \aligndata and \syndata, respectively (see Fig.~\ref{fig:polyfit}). 
This suggests that the inherent capacity of LLMs significantly influences their effectiveness as judges.

\textbf{Insight 1: The evaluative performance of LLMs positively correlates with their inherent capacity.}

% \modelname performs better in open-ended scenarios where LLMs' inherent capabilities are more effectively leveraged.

\begin{figure}[t]
    \centering
    \includegraphics[width=.9\linewidth]{images/polyfit.pdf}
    \caption{The positive correlation between scenario $\agrpq{2}{2}$ and average labeled scores.}
    \label{fig:polyfit}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%



%\subsection{The Impacts of Reference Answers}
\vspace{1ex}
\stitle{Exp-2. The impacts of reference answers}
The availability of reference answers would make the evaluation tasks more manageable for humans.
Analogically, we next explore how reference answers affect alignment evaluation with LLMs.
\aligndata includes a reference answer drafted by GPT-4 and refined by human for each instruction, while \syndata uses GPT-4â€™s responses as reference answers.
The results of reference-guided grading $\agrpq{2}{2}$, as well as the resulting improvement by reference answers, are also presented in Table~\ref{tab:scenario-details}, from which we find the following.
First, reference answers improve the $\agrpq{2}{2}$ of \modelname by 0.059 on \aligndata, but have minimal overall impacts on \syndata. This difference likely arises from \aligndata{'s} higher answer quality and instruction difficulty. Moreover, we find the influence of reference answers varies between open and close-ended scenarios: they tend to improve performance in close-ended scenarios but have negligible or negative effects in open-ended ones. For instance, the average improvement on \aligndata is 0.090 for close-ended (\ie those underlined) and 0.007 for open-ended scenarios (\ie those in bold). Similar trends are noted on \syndata, where reference answers sometimes mislead \modelname (\ie leading to negative improvement), particularly in open-ended scenarios, potentially diminishing the evaluation of semantically diverse but good responses.


\textbf{Insight 2: \modelname{'s} performance in close-ended scenarios can be enhanced with high-quality reference answers.}



%%%%%%%%%%%%%%%%%%%%%%%%





\begin{figure}[t]
    \centering
    \includegraphics[width=.95\linewidth]{images/sample_single_scenario_2.png}
    \caption{Performance of fine-tuning with single scenario data. Each column denotes a model fine-tuned using data from a single scenario, with $\emptyset$ being the baseline without fine-tuning. Each row reports the performance of different models on a specific scenario.} % Scenarios are arranged following Table~\ref{tab:scenario-details}.
    \label{fig:sample_single_scenario}
\end{figure}

%\subsection{Fine-tuning with Single Scenario Data}
\vspace{1ex}
\stitle{Exp-3. Fine-tuning with single scenario data}
Previous research has validated that data composition significantly impacts model performance during pre-training and fine-tuning~\cite{ye2024datamixinglawsoptimizing,dong2024abilitieslargelanguagemodels}. 
As a basis for exploring these effects for LLM-as-a-Judge, we first examine the performance of fine-tuning with data from individual scenarios. 
We randomly sample 800 evaluation records for each of the ten scenarios, ensuring a relatively balanced distribution of grading scores and pairwise ratings. We then fine-tune ten judge models, each trained on data from a single scenario, and assess their judging performance for all scenarios on our two benchmarks. We report the combined performance metric, \ie averaged multiple metrics on both benchmarks, in Fig.~\ref{fig:sample_single_scenario}, where higher numbers indicate better performance. 


From the table we find that all fine-tuned models outperform the baseline foundation model (column $\emptyset$), highlighting the general benefit of fine-tuning for LLM-as-a-Judge.
However, the impact of data from different scenarios varies. For example, data from informative and professional writing (column IPW) enhances the evaluation performance across all scenarios; where data from close QA (column CQA) lead to  performance deterioration in several scenarios like open QA (OQA) and translation (T). This deterioration likely results from mismatches in evaluation criteria and the structured quality issues of LLM-generated fine-tuning data.
Surprisingly, fine-tuning on data from translation (T) and programming (PG) scenarios leads to decreased performance on their respective tasks, further evidencing the limitations of data quality.
These results suggest that data from different scenarios can have both positive and negative effects on evaluation performance.


\textbf{Insight 3: Fine-tuning  generally benefits  LLM-as-a-Judge, but careful data engineering at the scenario level is crucial due to the varying synergistic and inhibitory effects of different scenario data.}





%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/cluster_ratio_2.png}
    \caption{Impacts of data composition.}
    \label{fig:cluster_ratio}
\end{figure}


%\subsection{Data Composition and Scaling}
\vspace{1ex}
\stitle{Exp-4. Data composition and scaling}
In the last set of analysis, we investigate the effects of data composition and scaling on model performance. 
Following the method in~\cite{shao2024balanceddatasamplinglanguage,yang2024smalltolarges2lscalabledata}, we use K-means algorithm to group scenarios to manage the number of required tests. We employ the columns of Fig.~\ref{fig:sample_single_scenario} as the clustering features, exclude the close QA scenario due to its minimal overall improvement, and choose $K=3$, resulting in the clusters (A: MQA, GP), (B: IPW, RW, T, RP), and (C: OQA, CW, RCE). Intuitively, scenarios within same clusters exhibit similar influence on model's evaluation performance after fine-tuning, suggesting that mixing data within clusters is feasible.


To assess the impacts of data composition, we fine-tune multiple models with the same number (\ie 800) of training each, varying the proportions of records from different clusters. The overall performance of resulting models are reported in Fig.~\ref{fig:cluster_ratio}, with varying ratio of (A+B)/C on the left and varying ratio of A/B when (A+B)/C=1/2 on the right. From the results we find that data composition significantly affects evaluation performance. For example, increasing (A+B)/C from 1/2 to 1 decreases performance from 0.4 to 0.2.
Optimal data composition allows using as little as 6\% of the fine-tuning data to achieve near-equivalent performance to using the full dataset, \eg 0.3959 vs. 0.4005 across all scenarios.
However, it also turns out that the impact of varying ratios can be unpredictable, likely due to the inherent flaws in LLM-generated data. It requires numerous trials to identify an effective composition plan

Next, we investigate data scaling, a primary means for boosting LLM abilities~\cite{emgrgent-ability}. 
Previous results suggest that scaling with random data selection may not work as expected. We then explore advanced data selection strategy used in conjunction with data scaling for our task. Specifically, we choose the Instruction-Following Difficulty~\cite{li2024quantityqualityboostingllm} (IFD) metric for this purpose, which is designed to to measure how much help the instruction can provide to the generation of the corresponding response. Formally, given instruction $Q$ and its corresponding fine-tuning answer $A$, the IFD score of the instruction-answer pair is defined as:
\begin{equation} \label{eq:ifd}
    IFD_\theta(Q,A) =\frac{L_\theta(A|Q)}{L_\theta(A)}
       =\frac{- \sum_{i} \log P(\omega_i|Q, \omega_1, \dots, \omega_{i-1}; \theta)}{- \sum_{i} \log P(\omega_i|\omega_1, \dots, \omega_{i-1}; \theta)},
\end{equation}
where $\omega_i$ is the $i$-th token of $A$ and $\theta$ represents an LLM model. Higher IFD scores indicate the inability for the model to align responses to the corresponding instructions. 
We then calculate the IFD score of each fine-tuning record and observe that records associated with higher IFD scores are generally hard to evaluate, often necessitating the agile utilization of knowledge or deep understanding of instructions. The original work proposes to filter data with IFD $>1$ and then select data in descending order of IFDs. Alternatively, we suggest only filtering data with extremely high IFD, \eg with scenario-based z-score $>3$. 


Figure~\ref{fig:data_select} reports the overall performance results of data scaling under three data selection strategies including random, original IFD method, and IFD with z-score filtering. Note that we fix the best data composition ratios identified in the previous set of tests.
From the figure we find that scaling data under the random strategy does not assure increased performance. Indeed, it reaches to a peak at the beginning, \ie using 800 records, in out tests. On the other hand, the trends of the two IFD-based strategies are more predictable, keeping increasing before using 3,200 records. Among the two, the IFD + z-score method is better. Finally, we obtain the best model using 3,200 fine-tuned record, with an overall performance of 0.4095 \emph{vs.} 0.4005 fine-tuned on all data.

\textbf{Insight 4: Data composition and scaling significantly affect the performance of fine-tuned models. However, identifying the optimal combination is challenging due to the high variability in impacts. The IFD-based data selection strategy shows promise for further exploration.}



\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{images/data_select_2.png}
    \caption{Impacts of scaling \emph{w.r.t.} data selection strategies.}
    \label{fig:data_select}
\end{figure}

