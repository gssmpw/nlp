\section{Conclusion}
\label{sec:conclusion}

In this paper, we developed an LLM judge model named \modelname for user intent alignment evaluation. We first detailed the development pipeline of \modelname. Specifically, it utilized scenario-dependent evaluation prompts, incorporated two innovative methods for controlled instruction generation, and distilled evaluative skills from GPT-4. Results on our human preference benchmarks demonstrated the effectiveness of our training pipeline: \modelname could offer automatic and contextually informed evaluations with an accuracy close to GPT-4 while using much lower serving costs. We also presented key insights which could enhance the understanding of the LLM-as-a-judge paradigm. To advance further research and development, we shared our experience for performance optimization and committed to release our data, benchmarks and model checkpoints to the community. A couple of problems deserve further investigation. We are exploring multi-agent collaboration and human-in-the-loop to mitigate the data quality issues of LLM-generated SFT data. In addition, we seek to train
foundation models specific for LLM-as-a-judge to boost generalization.


% With the emergence of LLM technology, the importance of creativity, logic, and detailed assessment has increasingly become apparent. LLM-as-a-Judge offers an affordable and rapid solution compared to manual review and labeling. In this paper, we initially outlined the entire training pipeline and analyzed the correlations between data through extensive experimentation, which provides guidance for optimizing our subsequent training processes. Finally, we detailed the optimization processes and methods of Themis because we aim to demonstrate the feasibility of LLM-as-a-Judge. Furthermore, we hope that researchers can reduce their research expenditures by following our processes.