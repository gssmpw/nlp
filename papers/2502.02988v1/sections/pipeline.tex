\section{Development Pipeline}
\label{sec:pipeline}

In this section, we present the complete development pipeline of \modelname. %, encompassing scenario design, data construction, fine-tuning, and performance assessment. 
Strategically, \modelname adopts scenario-dependent evaluation prompts, employs two methods for controlled instruction generation, and learns from GPT-4 rationales. We establish two human preference benchmarks to quantify \modelname's performance. 
%Related prompts could be found in Appendix~\ref{app:prompts}.


\subsection{Prompt Design}

\eat{
\begin{table}
  \caption{Description and judge criteria of close QA.}
  \label{tab:scenario}
  \begin{tabular}{m{8cm}}
    \toprule
    \textbf{Description.} Solve a problem that may involve professional knowledge or real-world inquiries, such as historical facts or scientific laws, and the problem has a standard/reference answer\\ \midrule
    \textbf{Judge criteria}. 
    1. \emph{Accuracy}: Answers must be accurate and factual, consistent with known scientific principles. \\
    2. \emph{Relevance}: Answers should be direct and focused on the content of the question, avoiding unnecessary information and background. \\
    3. \emph{Harmlessness}: Answers should avoid any potentially offensive content, ensuring appropriateness and cultural sensitivity, and adhere to ethical criteria. \\
    4. \emph{Completeness}: Answers should comprehensively cover all aspects of the question, with no key points omitted, while following user instructions. \\
    5. \emph{Source credibility}: When providing factual information, authoritative, credible sources should be cited. \\
    6. \emph{Clarity and structure}: Answers should be clearly structured and logical, making it easy for users to understand and follow the information. \\
    7. \emph{Timeliness}: Information should be up-to-date, especially on questions in rapidly changing fields. \\
    8. \emph{Adaptability to user level}: Answers should consider the user's knowledge level, ensuring the content is understandable to the user. \\
    \bottomrule
  \end{tabular}
\end{table}
} %% eat-close


\begin{table}[tbh!]
  \caption{Prompt template for single answer grading.}
  \label{tab:prompt}
  \small
  %\scriptsize
  \begin{tabularx}{.48\textwidth}{X}
    \toprule
        Your task is to evaluate the quality of AI responses. You are well aware that when a user issues an instruction of [\texttt{\{scenario name\}}] (the definition is: \texttt{\{scenario description\}}), an AI assistant's response should meet the following criteria (listed in descending order of importance): 
        
        [Criteria Begin] 
        
        \texttt{\{judge criteria of the scenario\}} 
        
        [Criteria End] 

        \ 
        
        The grading uses a five-tier system (1--5), the meanings of each tier are: 
        
        [Grading Tiers Begin] 
        
        1 The response has significant flaws, totally deviates from the criteria, and should not be seen in practice. 
        
        2 The response has parts that meet the criteria and can be adopted, but as a whole, the quality is not sufficient. 
        
        3 The response has a mix of strengths and weaknesses, with strengths overall outweighing the weaknesses within the evaluation criteria. 
        
        4 The response is of acceptable quality, overall meets the criteria, and has few minor issues that can be improved. When a reference answer is given, this tier represents the quality shown by the reference answer. 
        
        5 The response is excellent, strictly meets the criteria in all aspects. When a ref answer is given, this tier represents a quality superior to the answer. 
        
        [Grading Tiers End] 

        \
        
        Regarding a user instruction of [\texttt{\{scenario name\}}] , we have collected the following AI assistant response. Please evaluate this response against the known criteria for the current scenario and provide your assessment. Below are the user instruction and the assistant's response data: 
        
        [Data Begin] 
        
        ***
        
        [User Instruction]: \texttt{\{instruction\}} 
        
        *** 
        
        [Response]: \texttt{\{response\}}
        
        ***
        
        [Data End] 

        \
        
        You need to follow these steps to evaluate the above response: 
        
        1. Recall the relevant AI assistant response criteria and carefully read and understand the response to be evaluated.
        
        2. Identify from all criteria the key ones for the current user instruction and response, including those that performed well and those that did not. 
        
        3. Besides the given criteria, add any other important criteria that you think are necessary for evaluating the current user instruction response. 
        
        4. Based on your final selection of criteria, assign scores (1--5) to each criterion, and provide an overall score by weighting all sub-scores. 

        \ 
        
        Think carefully and then provide your conclusion. Your response should keep the `[[' and `]]' symbols in the output: 
        
        I believe the overall rating of this response is [[a score between 1--5]], and the reasons are as follows. 
        
        Strengths of the current response: 
        
        (List each point that you think is well done in the current response, providing [[a score between 1--5]] for each point...) 
        
        Shortcomings of the current response: 
        
        (List each point that you think is lacking in the current response, providing [[a score between 1--5]] for each point...)  \\
    \bottomrule
  \end{tabularx}
\end{table}

The effectiveness of LLM-as-a-Judge is significantly influenced by the design of evaluation prompts. Previous studies have explored three types of prompts: unified~\cite{zheng2023judging,lin2024RethinkingAlignment}, scenario-based~\cite{ke2024critiquellm,li2023autoj}, and instruction-based~\cite{lin2024wildbench}. 
We choose scenario-based prompts for \modelname because they provide the necessary context-awareness for instruction-specific evaluations while imposing reasonable additional requirement, \ie a scenario classification model, to achieve evaluation automation.

We employ human-AI collaboration to designed scenarios and their corresponding judge criteria. 
Initially, we draft a proposal of common LLM use scenarios, including their names and descriptions, and solicit suggestions from an advanced LLM, such as GPT-4. In a subsequent iteration, we request the same LLM to output its scenario design based on both the initial human proposal and its own modification suggestions. From this process, we finalize 10 scenarios from the initial 15. These include: \textit{three question-answer scenarios} (close QA, open QA, and math-related QA), \textit{three writing scenarios} (creative writing, informative and professional writing, and rewriting), and \textit{four professional scenarios} (translation, reading comprehension and extraction, role-playing, and programming-related).
For each chosen scenario, we follow the same iterative process to derive the judge criteria. This involves an initial human proposal, suggestions from the AI, a revised AI proposal incorporating these suggestions, and a final human-edited version. Detailed scenario descriptions and the chosen judge criteria (81 in total) are provided in Appendix~\ref{app:scenario}, as well as a scenario comparison with Llama 3~\cite{llama3tech} which empirically justifies our scenario design through human-AI collaboration.

%In a nutshell, we draft a proposal of common LLM use scenarios (including name and description) and ask an advanced LLM (\eg GPT-4 in our case) to give suggestions for the initial proposal. In the follow-up turn, we request the same LLM to output its scenario design based on both human proposal and its modification advices. We finally adopt 10 scenarios from the 15 given, including \textbf{three question-answer scenarios} (close QA, open QA, and math-related QA), \textbf{three writing scenarios} (creative writing, informative and professional writing, and rewriting), and \textbf{four professional scenarios} (translation, reading comprehension and extraction, role-playing, and programming-related). For each chosen scenario, we follow the same pipeline to derive the judge criteria, \ie an initial human proposal, suggestions by AI, a revised AI proposal based on the above, and a final human-edited version. The detailed scenario descriptions and judge criteria are available in Appendix~\ref{app:scenario}.



We then develop the scenario-based evaluation prompts. Similar to previous work, we support three variants of judgement: single answer grading, reference-guided grading, and pairwise comparison. The prompt template for single answer grading is presented in Table~\ref{tab:prompt}, which consists of five components separated by blank lines: (1) task description with scenario information emphasized, (2) grading guidelines, (3) the evaluation input data, (4) evaluation steps, and (5) output requirement and format. The templates for the other two variants are similar, with slight differences in input data and output ratings. %(Tables~\ref{tab:prompt-with-ref}\&\ref{tab:prompt-pairwise}).
These detailed, step-by-step prompts offer several benefits. First, they provide judge LLMs with concrete instructions on how to perform evaluations, including both general grading tiers and steps, and scenario-specific criteria.  Second, they encourage LLMs to elucidate the reasons for their ratings, which enhances learning efficiency during model training and improves interpretability during deployment. 
To perform evaluations using these scenario-dependent prompts, we have fine-tuned an LLM for scenario classification, \ie assigning a scenario to each user instruction. Details of this model are provided in Sec.~\ref{subsec:finetuning}.
%Evaluating with these scenario-based prompts requires to decide an appropriate scenario for each user instruction, and we leave this in Sec.~\ref{subsec:finetuning}.


\subsection{Data Construction}
\label{subsec:datacons}

We next outline the data construction pipeline for the supervised fine-tuning~\cite{ouyang2022instructGPT} of \modelname, including collecting user instructions, their corresponding responses, and evaluations of these instruction-response pairs. The primary challenge is gathering user instructions, as responses and evaluations can be automatically generated by LLMs.
Typical methods for collecting user instructions involve utilizing existing instruction sets~\cite{zheng2024lmsyschat1m,li2023autoj,ke2024critiquellm} or generating instructions from a small set of seed examples~\cite{wang2023selfinstruct,alpaca}. However, these methods may not adequately balance instruction distribution across scenarios, potentially impacting  the performance of judge LLMs due to unbalanced or insufficient data. To address this, we introduce the idea of controlled instruction generation, employing reference-based questioning and role-playing quizzing.
%  (questions and user instructions are used exchangeably in this section)



\stitle{Reference-based questioning} Our first method leverage LLMs' generative ability to synthesize user instructions for specific scenarios based on reference texts. We achieve this efficiently by fine-tuning a questioning model~\cite{yang2023regGPT} using data generated by GPT-4.  %with the prompt detailed in Table~\ref{tab:prompt-qllm-train}. 
We adopt a prompt (available in an extended version due to the space constraint) specifies the scenario name and description to guide question synthesis. A piece of reference text is also provided. Both of them enhance controllability for the process. It also outlines synthesis requirements, provide examples from a small set of manually crafted seed instructions, and requests GPT-4 to generate five instructions at a time. %according to the text content and requirements. 
We manually validate these outputs and use the filtered data to fine-tune the questioning model.




%Our first method leverage LLMs' generative ability for user instructions of a specific scenario based on a given reference text. To achieve the goal economically, we fine-tune a questioning model~\cite{yang2023regGPT} and the training data are elicited from GPT-4 using the prompt in Table~\ref{tab:prompt-qllm-train}. Specifically, in the prompt we fix the scenario (\ie name and description) for question generation and provide a piece of reference text, both of which enhance our controllability to the generated user instructions. We also specify requirements on QA, provide example questions from a small manually-crafted set of seed questions in the prompt, and finally request GPT-4 to generate five question-answer pairs per time based on the text content following requirements.  Using Wikipedia data as reference text and performing manual validity check on the outputs, we obtain 9,900 QA pairs, including (2,216, 1,999, 1,588, 1,156, 1,145, 1,045, 751) pairs in the (close QA, creative writing, role-playing, rewriting, informative and professional writing, translation, open QA) scenarios, respectively, to fine-tune the questioning model. Note that the prompt used for fine-tuning and inference is the same except for generating one question-answer pair per time.



\stitle{Role-playing quizzing} While the reference-based method excels in generating questions for seven scenarios, it struggles with instruction adherence and quality for the remaining three scenarios, particularly in scenarios like math-related QA and programming, where reference text suitability is crucial.
To this end, we propose the role-playing quizzing method, which leverages LLMs’ ability to act as test writers to generate instructions for these challenging scenarios. This method specifies quiz-related information such as difficulty level, audience, subject, topic, and task to improve controllability. Detailed prompts for this method will also be provided in a future extended version. 
%Detailed prompts for this method are provided in Tables~\ref{tab:prompt-rpq-math}--\ref{tab:prompt-rpq-reading}.


%We find that the previous method excels in question generation for seven of our ten scenarios while for the rest three it either does not follow instruction to generate questions or produce low-quality questions which could not pass the manual check. Note that question generation for scenarios like math-related QA and programming poses a strict requirement on reference text. To this end, we propose an alternative method which relies on the role-playing ability of LLMs as test writers to complement instructions in the remaining three scenarios. We specify quiz-related information such as difficult level, audience, subject, topic, task when applicable to enhance controllability. The detailed prompts could be found in Tables~\ref{tab:prompt-rpq-math}--\ref{tab:prompt-rpq-reading}.


These two methods together ensure a balanced and comprehensive collection of user instructions across diverse scenarios. We then gather responses to these instructions from LLMs of varying capacity, including ChatGLM3-6B, Baichuan2-13B, Yi-34B, Qwen-72B, and GPT-3.5-turbo. 
%including ChatGLM3-6B\footnote{https://huggingface.co/THUDM/chatglm3-6b}, Baichuan2-13B\footnote{https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat}, Yi-34B\footnote{https://huggingface.co/01-ai/Yi-34B-Chat}, Qwen-72B\footnote{https://huggingface.co/Qwen/Qwen-72B-Chat}, and GPT-3.5-turbo\footnote{https://platform.openai.com/docs/models}. 
These instruction-response pairs are evaluated with GPT-4 using the evaluation prompts developed earlier, and we use the detailed evaluation outputs to fine-tune \modelname.



% facilitating the effective fine-tuning of \modelname for improved alignment evaluation.

\subsection{Fine-tuning} 
\label{subsec:finetuning}

\begin{table}[t]
  \small
  \centering
  \caption{Statistics of fine-tuning data over scenarios. SC, Q, UI, and E (S/P) represent for scenario classification, questioning, synthesized user instructions, and evaluation records by single answer grading and pairwise comparison.} % 
  \label{tab:finetuning-data}
  \begin{tabular}{c c c c c}
    \toprule
    \textbf{Scenarios} & \textbf{\#SC} & \textbf{\#Q} & \textbf{\#UI}  & \textbf{\#E (S/P)}  \\ \midrule
        Close QA & 3,433 & 2,216 & 2,498 & 1,411/500 \\
        Open QA & 1,794 & 751 & 923 & 361/500 \\
        Math-related QA & 1,435 & / & 3,651 & 1,616/500 \\
        Creative writing & 2,173 & 1,999 & 1,994 & 895/500 \\
        Info\&Prof writing & 1,505 & 1,145 & 1,275 & 517/500 \\
        Rewriting & 2,154 & 1,156 & 1,830 & 939/500 \\
        Translation & 1,998 & 1,045 & 1,618 & 800/500 \\
        Reading C\&E & 1,316 & / & 3,128 & 1,345/500 \\
        Role-playing & 2,163 & 1,588 & 2,112 & 975/500 \\
        Programming & 903 & / & 2,945 & 1,141/500 \\ \midrule
        Total & 18,874 & 9,900 & 21,974 & 10,000/5,000 \\
    \bottomrule \\
  \end{tabular}
\end{table}



We now detail the fine-tuning process for our models. We choose the Qwen-2 series base models~\cite{qwen} as the foundation models. The fine-tuning data are summarized in Table~\ref{tab:finetuning-data}. All training tasks are executed on Nvidia H800 GPUs, utilizing DeepSpeed ZeRO-3~\cite{deepspeed} to optimize GPU memory usage and accelerate training.


%  our scenario classification LLM, the questioning LLM, and the \modelname model.

\stitle{Scenario classification LLM} 
We manually label 18,874 records for fine-tuning the scenario classification model. Each labeled record is converted into a prompt. %using the template provided in Table~\ref{tab:prompt-scenario-classification}. 
This prompt enumerates the scenarios with both name and description, specifies a user instruction, and ask the LLM to classify a scenario for the instruction. We used the labeled scenarios for fine-tuning, which teaches the LLM to classify future user instructions.
%
We fine-tune a 7B model for scenario classification in \modelname, balancing performance and serving costs. The model is trained over 5 epochs on 8 GPUs, using a batch size of 64, a learning rate of 1e-5, and a warmup ratio of 0.1. 
We also manually label the scenarios on the Alignbench~\cite{liu2023alignbench} dataset to quantitatively evaluate the performance of the fine-tuned model. The dataset contains 683 selected real user instructions and is  not included in the fine-tuning process. Our fine-tuned 7B model obtains an accuracy of 93.1\% on this test set, indicating that it could choose appropriate scenario-based prompts for evaluation.



\stitle{Questioning LLM}
We use Wikipedia data as reference text and obtain 9,900 questions across seven scenarios after the manual validity check. 
% distributed across 7 scenarios except for math-related QA, reading comprehension and extraction, and programming-related: closed QA (2,216), creative writing (1,999), role-playing (1,588), rewriting (1,156), informative and professional writing (1,145), translation (1,045), and open QA (751). 
This data is employed to fine-tune the questioning model, using a prompt similar to the one for data synthesis %in Table~\ref{tab:prompt-qllm-train} 
but generating one question at a time.
We fine-tune a 14B model for this task, running the training on 8 GPUs with a batch size of 512, a learning rate of 1e-5, and a warmup ratio of 0.1 for 3 epochs.
%We then use the fine-tuned questioning LLM to generate 

\stitle{Main model}
We synthesize 21,974 user instructions with our controlled instruction generation methods. Each instruction is paired with five LLM responses, which are then used to create evaluation records with GPT-4. Specifically, we use 10,000 instruction-response pairs for single answer grading and another 10,000 pairs for pairwise comparison, resulting in a total of 15,000 evaluation records.
To balance scores and pairwise ratings, we sample 6,404 single-answer records and 3,803 pairwise records, and we double the pairwise records with order replacement to reduce order bias. Ultimately, we obtain $6,404 + 2 \times 3,803 = 14,010$ records for supervised fine-tuning of the main model. %(see Tables~\ref{tab: math example}\&\ref{tab: gpt-4's response} for a complete example of SFT record).
We train a 14B model for 3 epochs on 16 GPUs, using a batch size of 128, a learning rate of 2e-5, and a warmup ratio of 0.1. 



\subsection{Performance Assessment}

%We create two human preference benchmarks for meta-evaluation and assess the performance of \modelname on the two benchmarks and a real-world evaluation scenario.

%We assess the performance of \modelname on two benchmarks and a real-world evaluation task.

\stitle{Benchmarks} We create two human preference benchmarks for performance assessment.
(1) \aligndata~\cite{liu2023alignbench} contains 683 manually selected real user instructions and we extend the data with a scenario label and five responses by the same set of LLMs in Sec.~\ref{subsec:datacons} to each instruction. We then recruit annotators to assign three five-tier scores (1--5) to each instruction-response pair, giving the same score descriptions and scenario criteria as \modelname. Scores are then aggregated through majority voting, with the average rounded to the nearest integer in cases of discrepancy, resulting in 3,393 scored instruction-response pairs across eight scenarios.
(2) \syndata consists of 2,000 synthesized user instructions from the total 21,974.  For each instruction, we randomly select two responses and apply the same manual annotation process as for \aligndata, leading to 4,000 scored instruction-response pairs covering all ten scenarios. Importantly, the instructions used for performance assessment are distinct from those used in fine-tuning.
    




\begin{table}[t]
  \small
  \centering
  \caption{Performance comparison on benchmarks.} % Best and second-best results are in bold and underlined.
  \label{tab:overall}
  \begin{tabular}{c | c c | c c}
    \toprule
    \multirow{2}{*}{\textbf{Judge}} & \multicolumn{2}{c|}{\textbf{\aligndata (3,393)}} &  \multicolumn{2}{c}{\textbf{\syndata (4,000)}} \\
     & \textbf{MAE $\downarrow$} & \textbf{$\agrpq{2}{2} \uparrow$ } & \textbf{MAE $\downarrow$} & \textbf{$\agrpq{2}{2} \uparrow$} \\ \midrule
    AutoJ-13B~\cite{li2023autoj} &  / & / & / & / \\
    CritiqueLLM-6B~\cite{ke2024critiquellm} & 1.297 & 0.346 & 1.259 & 0.346 \\ \midrule
    Qwen-14B & 1.320 & 0.366 & 1.035 & 0.437 \\
    Qwen-max & 1.131 & 0.424 & 0.840 & 0.497 \\ 
    GPT-4 & \textbf{0.685} & \textbf{0.595} & \textbf{0.664} & \textbf{0.590} \\	 \midrule
    \modelname & \underline{0.756} & \underline{0.559} & \underline{0.673} & \underline{0.582} \\
    \bottomrule
  \end{tabular}
\end{table}


%% results for insights
\begin{table*}[t]
  \small
  \centering
  \caption{Performance of \modelname with single answer grading (SAG) and reference-guided grading (RGG) for different scenarios. Note that we report $\agrpq{2}{2}$ for columns SAG and RGG, z-val is calculated on SAG, and $\Delta = \mbox{RGG} - \mbox{SAG}$.}
  \label{tab:scenario-details}
  \begin{tabular}{c | c c c c c c | c c c c c c c}
    \toprule
    \multirow{2}{*}{\textbf{Scenario}} & \multicolumn{6}{c|}{\textbf{\aligndata}} &  \multicolumn{6}{c}{\textbf{\syndata}} \\
     & \textbf{\#Tests} & \textbf{SAG} & \textbf{z-val} & \textbf{avg. $y$} & \textbf{RGG} & $\Delta$ &  \textbf{\#Tests} & \textbf{SAG} & \textbf{z-val} & \textbf{avg. $y$} & \textbf{RGG} & $\Delta$ \\ \midrule
        All & 3,393 & 0.559 & -0.223 & 2.914 & 0.618 & 0.059 & 4,000 & 0.582 & -0.092 & 3.450 & 0.584 & 0.002 \\
        Close QA & 1,503 & 0.484 & \underline{-0.876} & 2.785 & 0.576 & 0.092 & 510 & 0.548 & \underline{-0.555} & 2.929 & 0.561 & 0.013 \\
        Open QA & 190 & 0.791 & \textbf{1.799} & 3.659 & 0.797 & 0.006 & 138 & 0.616 & 0.371 & 3.609 & 0.717 & 0.101\\
        Math-related QA & 555 & 0.522 & \underline{-0.545} & 2.444 & 0.589 & 0.067 & 716 & 0.433 & \underline{-2.120} & 3.500 & 0.455 & 0.022 \\
        Creative writing & 130 & 0.560 & -0.214 & 2.962 & 0.55 & -0.010 & 386 & 0.659 & \textbf{0.957} & 3.653 & 0.698 & 0.039 \\
        Info\&Prof writing &  242 & 0.697 & \textbf{0.980} & 3.311 & 0.696 & -0.001 & 204 & 0.566 & -0.309 & 3.152 & 0.625 & 0.059 \\
        Rewriting & \multicolumn{6}{c}{/} & 338 & 0.538 & \underline{-0.691} & 3.210 & 0.558 & 0.020\\
        Translation & 50 & 0.510 & \underline{-0.650} & 2.720 & 0.645 & 0.135 & 284 & 0.556 & -0.446 & 3.451 & 0.539 & -0.017 \\
        Reading C\&E & 153 & 0.449 & \underline{-1.181} & 3.045 & 0.513 & 0.064 & 574 & 0.702 & \textbf{1.542} & 3.645 & 0.689  & -0.013 \\
        Role-playing &  570 & 0.689 & \textbf{0.910} & 3.255 & 0.705 & 0.016 & 366 & 0.653 & \textbf{0.875} & 3.377 & 0.59 & -0.063 \\
        Programming & \multicolumn{6}{c}{/} & 484 & 0.623 & 0.467 & 3.837 & 0.571 & -0.052 \\
    \bottomrule
  \end{tabular}
\end{table*}



\stitle{Metrics} We utilize two metrics to quantify performance.
(1) MAE measures the average deviation between human labeled and LLM predicted scores.
(2) $\agrpq{p}{q}$ is a general agreement metric that accommodates weighted agreements and non-exact matches. Specifically,
%\begin{equation} \label{eq:agrpq}
%    \agrpq{p}{q}= {\sum_{(\hat{y}, y)\in\mathcal{T}} A^q_p(\hat{y}, y)} / {|\mathcal{T}|},
%\end{equation}
$\agrpq{p}{q}= {\sum_{(\hat{y}, y)\in\mathcal{T}} A^q_p(\hat{y}, y)} / {|\mathcal{T}|},$
where $\mathcal{T}$ is the set of predicted and labeled score pairs  and $A^q_p(\hat{y}, y) = 1 / (|\hat{y} - y| + 1)^q$ if $|\hat{y}-y|< p$, and 0 otherwise. Note that $\agrpq{1}{*}$ is the same to accuracy and we use $\agrpq{2}{2}$ for our assessment, which assigns an agreement of 0.25  when $|\hat{y}-y|=1$.
%$(\hat{y}, y)$



\stitle{Performance on benchmarks}
Table~\ref{tab:overall} shows the performance of \modelname compared to two fine-tuned judges and three foundation LLMs on our benchmarks. Note that we unify the evaluation prompts for all tested LLMs, \ie using the same ones as \modelname, to keep the scoring criteria consistent with annotators and avoid scenario mapping for judge baselines. 
We find that AutoJ-13B encounters prompt generalization issue and does not give valid evaluation results. CritiqueLLM-6B could complete the task for approximately 70\% of the records, but is worse than other methods. We note that these results are for reference-purpose only as prompts are very important for fine-tuned judges. 

Recall that \modelname is fine-tuned from Qwen-14B and we find that it outperforms Qwen-14B by 34.3\% on average, demonstrating the effectiveness of our training pipeline. Additionally, \modelname exceeds Qwen-max by 22.9\%, despite being smaller in size. This indicates that fine-tuning an LLM for evaluation purposes provides substantial benefits.
Finally, we find that our training pipeline is generally efficient in skill distillation: \modelname using less than 1\% parameters shows only 1.4\% and 8.4\% worse performance than its teacher GPT-4 model on the in-distribution \syndata and out-of-distribution \aligndata benchmarks, respectively. It is worth noting that GPT-4 remains the most competitive baseline in this area~\cite{zheng2023judging,vu2024foundationalautoraterstaminglarge}, and the achievement of \modelname deserves affirmation.

% \footnote{Rumors claim that GPT-4 has 1.76 trillion parameters. -- from Wikipedia of GPT-4}

\stitle{Performance on a real evaluation task}
\modelname has been deployed online, where it is used for pairwise comparison in chat response evaluations, \ie judge models should compare a pair of responses given a multi-turn dialog and decide which one is better or both are tied. In a set of 397 test records, the accuracy rates for (Qwen-72B, \modelname, Qwen-max, GPT-4) are (46.6\%, 71.8\%, 73.8\%, 76.6\%), respectively. Subsequent optimizations have improved the accuracy of \modelname to 75.3\%. These performance results together underscore the practical value of \modelname for alignment evaluation.


