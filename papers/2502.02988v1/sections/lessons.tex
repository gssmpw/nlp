\section{Practical Lessons}
\label{sec:lessons}

This section shares the practical lessons we learn during developing and optimizing the performance of \modelname. Note that the numbers in this section are not evaluated on the latest benchmarks, thus may be inconsistent with those in previous sections.

%\subsection{Balancing Fine-tuning Data}
\stitle{Balancing fine-tuning data}
We use full parameter fine-tuning to speedup model adaption and we find that the distribution of evaluation scores and pairwise ratings of fine-tuning data severely influences the rating bias of the obtained model. For instance, we fine-tuned an earlier version of \modelname with all single answer grading evaluation records, and the resulting MAE and \agrpq{2}{2} are 1.068 and 0.455 on the \aligndata benchmark, much worse than its teacher GPT-4 with 0.868 and 0.509. During model diagnosis we observe that the model has an extreme high trend to rate response with score 4. We check the distribution of scores in the fine-tuning data and find that evaluation records with score 4 account for approximately 56\%. We then down-sample these records to achieve a more balanced score distribution, leading to optimized MAE and \agrpq{2}{2} with 0.908 and 0.467. And the predicted scores are less biased to a specific one.




%\subsection{Supporting Custom Evaluation Prompts}
\stitle{Supporting custom evaluation prompts}
Recall Table~\ref{tab:prompt} that our scenario-based prompts use fixed criteria, steps, and a five-tier rating system for evaluation. During the deployment of \modelname, our initial users request for supporting custom prompts for criteria and rating systems. To this end, we have constructed a custom prompt generation procedure which augments required data without extra API usage for GPT-4.


% As shown in Appendix~\ref{app:scenario} and , each scenario is designed with a fixed number of criteria, and the scoring ranges from 1 to 5. To meet the personalized needs of users, such as customized criteria names and descriptions, number of criteria, and score ranges, we have constructed a custom prompt process through the following steps.

\etitle{(1) Rephrasing criteria and descriptions}
Referring to~\cite{Ovadia2023FTorRAG}, we employ an LLM to paraphrase existing criteria, requiring the paraphrased names and descriptions to have low textual similarity to the original, while maintaining semantic consistency. After manual check the results, we obtain over 2,400 name-description pairs as complement to the original ones. We then replace the original criteria with the corresponding rephrased ones.
%To ensure consistency between input and output, the criteria name in the output are replaced with the corresponding paraphase results.

\etitle{(2) Diversifying effective criteria in evaluations}
To accommodate possibly various user-defined criteria, we employ a random sampling strategy of  effective criteria in each evaluation to enhance the generalization of our model. Note that \modelname performs evaluation by firstly assigning scores for each criterion and then aggregating these scores to derive a final score. Consequently, criteria down-sampling necessitates recalculation of the final scores.
% 
To achieve this, we have developed a method wherein we extract scores from existing evaluation records. We then train a regression model to predict the final score from each criteria grade. Results show that such a simple regression model achieves a MAE of only 0.12 evaluated on a reserved validation set. %, indicating that the regression model meets the usability standards.
Subsequently, we derive extra evaluation records by down-sampling effective criteria and updating the final scores with the regressed.
%we first categorize the criteria for each sample into relevant and irrelevant. Criteria deemed irrelevant either receive no score or are marked as "Not Applicable" in our outputs. We employ a random sampling probability between 0.5 and 1 to select these two groups of criteria. We then substitute the original input and output data with the down-sampled criteria and the recalculated final scores.

\etitle{(3) Using alternative rating systems}
Except for the 5-tier rating, other systems are also popular for evaluation, such as binary (0-1 or 1-2), 3-class (1-3), and 10-class (1-10) ratings. To accommodate them, we transform the original scores into other systems with heuristic score mapping rules.
% redirected the 5 classes. For binary classification, scores of 1-3 are grouped into the first category, and 4-5 into the second category; for 3-class, scores of 1-2, 3, and 4-5 are divided into different categories; for 10-class, we multiply the scores by 2 and minus 1 to the doubled score with a 50\% probability.


\etitle{(4) Hybrid customization} 
To achieve optimal model performance, we mixed the aforementioned operations in different proportions, resulting in our final augmented fine-tuning data.


While supporting custom evaluation prompts is initially a functional requirements, we also observe improvement on performance: the MAE exhibited reductions from (0.699, 0.703) to (0.684, 0.676) and the \agrpq{2}{2} are improved from (0.577, 0.569) to (0.586, 0.581)
% and \agrpq{2}{2} are improved from (0.699, 0.577) and (0.703, 0.569) to (0.684, 0.586) and (0.676, 0.581) 
on the two benchmarks, respectively. Research on learning theory of LLMs provides a plausible explanation for the improvement. Note that fixed prompts lead to duplicated fine-tuning data, which will strengthen the memorization effect while weakening the generalization ability of LLMs~\cite{llm-acquire-know}. Custom evaluation prompts could be regarded as a de-duplication step, enhancing the generalization of the resulting models.
Similarly, we also observe the improvement by using a large batch size. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/loss_v2.pdf}
    \caption{Our multi-objective training method.}
    \label{fig:loss}
\end{figure}


%\subsection{Enabling Multi-objective Training}
\stitle{Enabling multi-objective training}
In the standard SFT process, LLMs learn from predicting the exact next tokens by minimizing the cross entropy loss.
However, we note that not all tokens in the evaluation output need to be ``perfectly" predicted. 
Take the output in Fig~\ref{fig:loss} as an example. The content in black is scores and format-related text, and  we require these words to be predicted accurately. On the other hand, the words in blue are the explanation for the specific score, for which we could tolerate more noises as long as the predicted content is semantically similar to the target.


This idea inspires a multi-objective training method illustrated in Fig~\ref{fig:loss}. Specifically, we first label each output tokens with either SFT or Sim. For SFT tokens, we still minimize the cross entropy loss between the the predicted and ground-truth logits. For Sim tokens, we minimize the difference between the embeddings of the top-1 predicted and ground-truth tokens. We find that this training method reduces the 
MAE from (0.684, 0.676) to (0.673, 0.652) and improves \agrpq{2}{2} from (0.586, 0.581) to (0.594, 0.591)
% performance by (0.684, 0.586) and (0.676, 0.581) to (0.673, 0.594) and (0.652, 0.591) 
on the two benchmarks, respectively.



%Firstly, we utilize regular expressions to separate the format-and-score words from the reasons and then employ the tokenizer to return the corresponding word-token mapping. Then a loss mask is generated to indicate which tokens should be associated with the CEL and which should be associated with the Similarity Loss. To transform the logits of shape 'vocabulary-size' into embedding shape, we directly applied matrix multiplication between the logits and the word-embedding layer to obtain the corresponding output embeddings. The similarity between the output embeddings and the input embeddings is then computed using either L2 loss or cosine similarity. At the stage of training, we compute both the CEL and Similarity Loss for all tokens, and combine these two losses to obtain the final training loss with the loss mask.

%\subsection{Unifying Performance Metrics}
\stitle{Unifying performance metrics}
We finally share a lesson for development efficiency. During our deployment of \modelname, a long-term challenge is to determine which fine-tuned checkpoint, or equivalently the corresponding optimization technique, is better. Recall that \modelname supports three variants of evaluations, which is a tradition for the LLM-as-a-Judge paradigm, as well as we create two benchmarks and use two metrics for performance assessment. Putting these together, we need to compare more than 10 numbers to come to a decision, which is not easy. Indeed, we have had a lot of controversies for which one is better within our team. Later, we decide to aggregate all performance metrics into one to close controversies. The most straightforward method is to use the average score. However, we find this is unfair due to the different effective scales for metrics. For instance, it is much harder to optimize the \agrpq{2}{2} by 0.1 than MAE. In this case, using average score will let MAE dominate the choice of optimization directions. 
To address this, we perform a linear transformation on the original metrics such that random guessing is mapped to 0 and the best performance metric is mapped to 1. Averaging the  transformed metrics gives us a fair overall performance metric which help us choose promising optimization strategies.

