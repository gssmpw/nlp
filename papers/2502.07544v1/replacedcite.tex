\section{Related Work}
\label{sec:related_works}


\subsection{Chatbots for Grammar Acquisition}
The idea of intentionally shaping language learners' input can be traced back to
____, who emphasized that input should be developmentally proximal, ie, containing linguistic forms that learners are ready to learn but not yet able to use.
This beneficial input can stem from teachers acting as role models to learners ____ or other social contexts ____. As a negative example where this input is missing for advanced structures, heritage language learners often lack late-developing forms when their input is limited to the parent-child interaction at home ____.

Driven by the advancement of LLMs, educational applications have recently focused on aligning chatbots with such linguistic demands. ____ restricted vocabulary above certain proficiency levels while decoding a chatbot model and reranked response candidates by their CEFR level. %
Similarly, ____ identified unknown vocabulary with a pre-test, increased its frequency in dialogues with a similar method, and showed that learners included the target vocabulary more often in their turns.  %
So far, only ____ have explicitly controlled \textit{grammar} in dialogue response generation by adapting models with reinforcement learning for three broad grammar categories, which did not stem from a language development framework like the CEFR.

Little work exists on the effects of adapted grammar-controlled LLMs for conversation practice. The framework of \citeposs{ellisInputSecondLanguage2009} defines hypothetical statistical relationships between input and output, which can be potentially exploited by a controllable language input generation. We, therefore, simulate the effect on learners' output in Section~\ref{sec:output}.

\begin{table*}[ht]
\centering
\small
\begin{tabular}{lp{12cm}}
\hline
\textbf{Property}        & \textbf{Example}                                    \\ \hline

Super Category           & Adjectives  \\ 
Subcategory              & Superlatives \\ 
Guideword & FORM/USE: WITH 'IN' + NOUN \\
Can-do Statement         & Can use prepositional phrases with 'in' + singular name of a place after a superlative adjective.   \\
Learner Example         & It's the biggest room in the house.  \\
CEFR Level               & A2   \\ \hline
\normalsize
\end{tabular}
\caption{An example from the 1,222 grammar skills in the English Grammar Profile, which lists the skills used in the two grammar control tasks.}
\label{tab:egp}
\end{table*}

\subsection{Pedagogical Grounding}
In contrast to previous approaches, we base our grammar control on the CEFR, specifically the EGP. The CEFR defines language competencies in broad rubrics, such as written production on six proficiency levels, ranging from A1 / A2 (beginner) to B1 / B2 (intermediate) to C1 / C2 (advanced). For example, the rubric \textit{Overall oral interaction} states about B1 proficiency: "Can communicate with some confidence on familiar routine and nonroutine matters related to their interests and professional field." Descriptions in the CEFR are \textit{function}-oriented and cater to the ultimate goal of language learning. However, language teaching needs to make learners acquire the \textit{forms} that are necessary to fulfill these functions. The EGP is a comprehensive repository of required language forms to communicate on CEFR proficiency levels, identified by an analysis of the Cambridge Learner Corpus with more than 50M words from English learners with various mother tongues. An example EGP statement on level B1 is: "Can use 'would not have' + '-ed' or 'wouldnâ€™t have' + '-ed'." (see another example in Table~\ref{tab:egp}). Here, we call these statements \textit{grammar skills}. Grounding a chatbot in this skill framework can ensure that it demonstrates the linguistic forms required to impart CEFR competencies to the learner.

\subsection{Controlling Dialogue Generation}
The field of controlled text generation (CTG) distinguishes syntactical, numerical, semantic, and prefix controls ____. In our case, the control is a set of grammar skills that imposes not only syntactical but also semantic constraints. Text generation must also be conditional on a previous dialog, restricting the possible space of responses. This combination differentiates the problem from more established tasks, such as paraphrase generation, that fixes the semantics a priori and imposes purely syntactic constraints. 

Typical approaches to CTG include prompting and adopting pre-trained language models with decoding, instruction fine-tuning, or reinforcement learning ____. %
InstructCTG ____ has shown that verbalizing control conditions in natural language and fine-tuning sequence-to-sequence models with large numbers of demonstrations of control conditions and corresponding generated text successfully increases control. They synthesized training data because the control conditions are easily determined. The approach has not been tested on our control condition, whose evaluation is nontrivial.

Decoding strategies such as \textit{Future Discriminators for Generation} (FUDGE) by ____ aim to fulfill control conditions while sampling from a model's output. FUDGE uses a lightweight predictor at each generation step on the current sequence combined with each of the most likely next tokens. The predictor returns the probability that the desired attribute will be present in the \textit{completed} sequence if the current sequence continues with the respective token. In this work, we propose a different aggregation mechanism of such prediction to align with specific characteristics of grammar skills as control conditions.