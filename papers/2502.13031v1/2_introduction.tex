
Evaluation is a long-standing critical and challenging task in NLP \cite{celikyilmaz2020evaluation, chang2024survey}.
% Traditional evaluation metrics, such as BLEU \cite{papineni-etal-2002-bleu} and ROUGE \cite{lin-2004-rouge}, usually based on n-gram overlap between generated and reference texts, have been criticized for their low correlation with human judgments \cite{sulem-etal-2018-bleu}. 
% While sophisticated model-based evaluation metrics like BERTScore \cite{bert-score} and BartScore \cite{yuan2021bartscore} yield correlation improvements, their usage is still limited by their inherent lack of interpretability and dependence on the quality of reference answers. 
Recently, with the advent of advanced large language models (LLMs) such as GPT-4, LLM-based evaluators have been widely used for various natural language generation tasks \cite{wang-etal-2023-chatgpt, liu-etal-2023-g, zheng2023judging, hu-etal-2024-llm}. 
% Recently, recent works mostly resort to LLM-based evaluation methods 
Leveraging the LLMs' strong language understanding and instruction-following capabilities, the evaluation task can be addressed via an evaluation prompt that includes a task description, evaluation rules, and the text to be assessed
%LLM evaluators demonstrates greater flexibility and effectiveness in evaluation than traditional metrics.
\cite{chen-etal-2023-exploring-use, ke-etal-2023-decompeval}. 
Various studies have reported that LLMs can provide accurate and interpretable evaluation results of text quality
%of the given text 
on the specified aspect, offering stronger generalization ability compared with previous evaluation methods \cite{papineni-etal-2002-bleu, lin-2004-rouge, bert-score, zhong-etal-2022-towards}.

\begin{figure}[t]
\scriptsize
    \centering
    \includegraphics[width=1.0\linewidth]{figures/intro.pdf}
    % \vspace{-4.5mm}
    \vspace{-4mm}
    \caption{An example of prompting strategy and its corresponding evaluation prompt for assessing the coherence of text summarization, where some factors are highlighted and others are not shown.}
    \vspace{-6mm}
    \label{fig:example}
\end{figure}

Despite the advantages mentioned above, LLM-based evaluation methods are still imperfect in aligning with human judgments \cite{wang-etal-2023-chatgpt, liu2024aligning}, which cannot fully serve as an alternative to human evaluation as the gold standard. 
An emerging line of research attempts to optimize 
% various factors of 
evaluation prompts for LLM evaluators to improve their alignment with human judgments from multiple aspects, including optimizing evaluation criteria \cite{liu-etal-2024-calibrating, liu-etal-2024-hd}, modifying output formats \cite{chiang-lee-2023-closer, chu2024better}, and providing in-context examples \cite{kim-etal-2023-better, jain-etal-2023-multi, huang2024empirical}.
%, among others.

% However, we argue that these methods are insufficient in optimizing evaluation prompts and do not fully leverage the potential of LLM evaluators. 
% On the one hand, these methods are limited to optimizing a single factor of evaluation prompts (e.g., evaluation criteria or output formats) and ignore to consider the strategy of combining multiple factors, which results in suboptimal solutions. 
% On the other hand, these methods employ a similar prompt design for various evaluation aspects, differing only in the evaluation criteria, without making adaptive adjustments to the evaluation prompts based on the characteristics of different aspects, which causes unsatisfactory evaluation performance.

However, we argue that these methods are only limited to optimizing a single factor of evaluation prompts (e.g., evaluation criteria or output formats) but neglect the prompting strategy for adjusting multiple factors \citep{kim-etal-2023-better}. 
Given that the evaluation prompts comprise multiple components \cite{gao2024llm}, each encompassing various factors that simultaneously affect the performance of LLM evaluators, it is necessary to collectively adjust these factors to fully harness its evaluation capability. 
% 加状态空间很多的描述，再说明暴力搜索不可行
Nevertheless, due to the vast state space involved in making these adjustments, extensive enumeration is required to identify well-behaved prompting strategies, highlighting the need for effective automatic optimization methods.
% Although automatic prompt optimization has been studied recently \cite{prasad-etal-2023-grips, yang2023large, hsieh-etal-2024-automatic}, most existing works only consider structure-unaware, word- or sentence-level paraphrasing of prompts, 
% making it challenging to adapt to a higher-level selection strategy optimization for multiple factors.
% Single-factor optimization leads to suboptimal solutions and fails to fully exploit the potential of LLM evaluators.
% 以上是为什么要做prompt search，借鉴结合architecture search的目标阐述
% Although some existing works on prompt optimization have explored word- or sentence-level selection strategies for task prompts. 
% To higher-level selection strategies for multiple factors in evaluation prompts, their adaptability remains to be verified.

To this end, we comprehensively integrate a series of prompting strategies for LLM evaluators from previous works, select 8 key factors, 
and propose the \textbf{\textit{H}}euristic \textbf{\textit{P}}rompting \textbf{\textit{S}}trategy \textbf{\textit{S}}earch (HPSS) algorithm to automatically optimize prompting strategies for LLM evaluators tailored to specific evaluation aspects.
Inspired by the genetic algorithm \cite{mitchell1980need}, 
HPSS gradually mutates the previous prompting strategies and selects candidate strategies based on their performance on a validation dataset. 
Leveraging the limited number of values within each factor,
% in the prompting strategy search tasks,
%HPSS utilizes the expected performance advantage of each value as a heuristic function to guide the search direction,
% 解释advantage，无论是模型层面实际是什么，还是概念上是什么
HPSS introduced a heuristic function to guide the search process, assigning higher exploration probability to more promising values, which enhances the effectiveness of mutation and leads to better performance compared to random search. 
We optimize the prompting strategy for two different LLM evaluators (i.e., GPT-4o-mini \cite{openai2023gpt4} and Qwen2.5-14B-Instruct \cite{qwen2}) across four evaluation tasks for natural language generation (NLG). 
Experiment results show that HPSS achieves substantial improvement over the baseline of human-designed evaluation prompts and existing automatic prompt optimization methods. 
Compared to the commonly-used evaluation prompt from MT-Bench \cite{zheng2023judging}, HPSS can achieve an average 29.4\% relative performance improvement with the same generation times. 
Furthermore, compared to other manually designed prompting strategies such as G-Eval \cite{liu-etal-2023-g} and CloserLook \cite{chiang-lee-2023-closer}, HPSS can still achieve significantly better performance with only 5\% of the generation times. 
Our contributions can be summarized as follows:

\begin{itemize}
    \item To the best of our knowledge, we present the first discussion of automatic prompting strategy optimization for LLM evaluators, demonstrating that appropriate prompting strategies can significantly enhance the performance of LLM evaluators.
    \item We propose HPSS, a novel prompting strategy optimization algorithm via iterative search. 
    % Furthermore, leveraging the characteristics of evaluation prompting strategies, a heuristic function is introduced to enhance the.
    Furthermore, a heuristic function is introduced to estimate the prospect of each value and enhance the effectiveness of mutation.
    \item We validate the effectiveness of HPSS across four evaluation tasks, yielding consistently better performance compared to both human-designed evaluation prompts and existing automatic prompt optimization methods.
    % HPSS can achieve an average 29.4\% performance improvement compared to manually-designed prompt templates with almost the same inference overhead.
    % \item We conduct a systematic analysis of the prompting strategies explored by HPSS, providing new principles and insights for the prompt design of LLM evaluators.
\end{itemize}
%放一个小图，说明我们究竟在做什么事情