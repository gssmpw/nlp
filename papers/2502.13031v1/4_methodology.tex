\begin{figure*}[!t]
\scriptsize
    \centering
    \includegraphics[width=1.0\textwidth]{figures/HPSS.pdf}
    \vspace{-4mm}
    \caption{Overview of our HPSS algorithm. By perturbing the value of each factor in the prompting strategy from MT-Bench, HPSS gets the initial strategy population and the advantage estimations of values. Subsequently, new strategies are iteratively searched based on the guidance of the value advantage. The performance of new strategies is used to update the top-$ k $ strategy population and the advantage estimation.}
    \vspace{-4mm}
    \label{fig:overview}
\end{figure*}

\subsection{Problem Formulation}
% \textcolor{red}{In this work, we focus on \textbf{Pointwise Grading} format for text evaluation. 
% Given a sample $ d $ to evaluate (which contains the query $ x $ and the machine-generated response $ y $), the evaluation aspect $ a $ (e.g., consistency or fluency), the objective is to obtain a rating score reflecting the quality of the response on this aspect. 
% % 重新写 query, machine-generated response, evaluation aspect, 
% For the LLM evaluator $ \mathcal{M} $, 
% % this task can be seen as an instruction-following task. 
% the above content will be fed into a prompt template $ \mathcal{T} $ to constitute an evaluation instruction, and $ \mathcal{M} $ will follow this instruction to generate the final score.}

% In this work, we mainly involve two typical evaluation
% tasks: 
% 1) \textbf{Pointwise Grading}: Given a query $x$, the machine-generated response $ y $ and the evaluation aspect $ a $ (e.g., consistency or fluency), the objective is to obtain a rating score reflecting the quality of the response on this aspect.
% 2) \textbf{Pairwise Comparison}: Given a query $x$, two machine-generated responses $y_1$ and $y_2$, and the evaluation aspect $ a $, the objective is to obtain a comparison label (i.e., win / lose) for these two responses.
% For the LLM evaluator $ \mathcal{M} $,  
% the above content will be fed into a prompt template $ \mathcal{T} $ to constitute an evaluation instruction, and $ \mathcal{M} $ will follow this instruction to generate the final judgment.


NLG evaluation tasks typically require LLM evaluators to generate the corresponding judgment given the input sample $d$ and evaluation aspect $a$. 
The input sample may include the query and model-generated text. 
Meanwhile, the judgment takes various forms, including the rating score of the generated text in pointwise grading and the preference label between two generated texts in pairwise comparison.
%For instance, they represent the rating of the input sample in pointwise grading, and the preference label between two responses in pairwise comparison.
To guide LLM evaluators to assess the quality of the input sample $d$ within the aspect $a$, a prompt template $\mathcal{T}$ is needed to provide sufficient clarifications of the task.
In practice, $d$ and $a$ will be fed into $\mathcal{T}$ to constitute an evaluation instruction, and the LLM evaluator will follow this instruction to generate the final judgment.

% (such as output format discussed in Section \ref{factors})
Since $ \mathcal{T} $ contains various factors, there are multiple prompting strategies for adjusting these factors.
An example of prompting strategy is illustrated in Figure \ref{fig:example}.
%However, there are various factors in $ \mathcal{T} $ (such as output format and the usage of auxiliary information, which will be discussed in Section \ref{factors}) and multiple selection strategies. 
% our objective is to search for the optimal prompting strategy for adjusting each factor in $ \mathcal{T} $. 
%\hnote{the word strategy is a bit confusing: prompting strategy vs., selection strategy?} 
Formally, assume that $ \mathcal{T} $ has $ n $ factors $ (F_1, F_2, \ldots, F_n ) $, each factor $ F_{i} $ has $ m_i $ possible values $ (f_{i1}, f_{i2}, \ldots, f_{im_i} ) $. 
Thus, $ \mathcal{T} $ can be determined by $ \boldsymbol{F} = (F_1, F_2, \ldots, F_n) $.
% \hnote{This is too abstract, and it's better to provide a real example for explanation.}
%or strategy $ \{F_1, F_2, \ldots, F_n\} $.
% the $i$th  factor in $ \mathcal{T} $ is $ F_i $. 
For a dataset $ D=\{d_i\}_{i=1}^{|D|} $ 
%which contains multiple samples to evaluate 
and an evaluation aspect $ a $, 
% the judgment of $ \mathcal{M} $ for each sample $ d_i $ using $ \mathcal{T} $ is $ s_{i,a} = \mathcal{M}(\mathcal{T}(d_i, a)) $ and the judgment of human experts for $d_i$ is $ s^{*}_{i,a} $. 
our objective is to search for the prompting strategy $\mathcal{T}^{o}$ to maximize the performance of LLM evaluators:
\begin{equation}
\small
\begin{aligned}
\mathcal{T}^{o} = \arg\max_{\boldsymbol{F}} r_{D,a}(\mathcal{T}_{\boldsymbol{F}})
% \\
% & = \mathop{\arg\max}\limits_{\{F_1, \ldots, F_n \}} corr( \{s^*_{i,a}\}_{i=1}^{|D|}, \{s_{i,a}\}_{i=1}^{|D|})
\end{aligned}
\end{equation}
where 
% $F_i \in \{f_{i1}, f_{i2}, \ldots, f_{im_i} \}$ , 
% $ corr(\cdot) $ is a correlation metric  and 
$ r_{D,a}(\mathcal{T}) $ represents the performance of $ \mathcal{T} $ for the aspect $ a $ on the dataset $ D $,  which is typically measured by the correlation between the evaluation results of LLM evaluators using $ \mathcal{T} $ and human judgments.
To aid the search for the optimal prompting strategy, we will use a validation dataset with human judgments to evaluate the performance of $ \mathcal{T} $.
Without loss of generality, we simplify $ r_{D,a}(\mathcal{T}) $ as $ r(\mathcal{T}) $ to denote the performance of $\mathcal{T}$ for a specific aspect on the validation dataset.

\subsection{Factors of Evaluation Prompt}

\label{factors}

Considering a series of prompting strategies from previous works, we select 8 key factors for the prompting strategy search, as described in Table \ref{tab:factors}. Detailed prompts are provided in Appendix \ref{appendix:prompt_template}.




% \textbf{Scoring Scale}: The score range for text evaluation. 
% Common scoring scales used by previous work are 1-3 \cite{gopalakrishnan2019topical} or 1-5 \cite{fabbri-etal-2021-summeval, chhun-etal-2022-human}.
% \citet{stureborg2024large} explore the impact of a larger scoring scale on the performance of LLM evaluators. 
% We consider 5 scoring scales for selection: 1-3, 1-5, 1-10, 1-50, and 1-100.

% \textbf{In-Context Example}: Previous work \cite{huang2024empirical, kim-etal-2023-better} find that providing LLM evaluators with human scoring examples can improve their performance. We consider using 0, 3, 5, or 10 human scoring examples, obtained through stratified sampling based on human scores \cite{jain-etal-2023-multi} in the training set.

% \textbf{Evaluation Criteria}: The general definition of the evaluation aspect and the scoring standards of the quality of a text.
% We consider 3 types of evaluation criteria: no criteria \cite{murugadoss2024evaluating}, manually written criteria \cite{hu-etal-2024-llm}, and self-generated criteria by LLM evaluator.

% \textbf{Reference}:  The reference answer for the task in the sample to evaluate. 
% We consider the following 3 strategies: (1) no reference, (2) prompting the LLM evaluator to independently generate a reference and then feed it into $ \mathcal{T} $ to assist in evaluation \cite{zeng2024llmbar}, (3) prompting the evaluator to generate the reference along with final rating score \cite{he-etal-2024-socreval}.

% \textbf{Chain-of-Thought}: Recent work \cite{chiang-lee-2023-closer} points out that asking the LLM evaluator to generate explanations for their ratings can improve its performance, but others \cite{liu-etal-2024-calibrating, stureborg2024large} provide opposite opinions. 
% We consider 3 output formats: (1) directly generating the rating without an explanation \cite{liu-etal-2023-g}, (2) providing an explanation first and then giving the rating \cite{chiang-lee-2023-closer} and (3) providing the rating first, followed by an explanation \cite{chiang-lee-2023-closer}.

% \textbf{AutoCoT}: G-Eval \cite{liu-etal-2023-g} requires the LLM evaluator to generate evaluation steps first (AutoCoT) and refer to these steps to finish the evaluation task. 
% We consider 2 strategies: using AutoCoT or not.

% \textbf{Metrics}: LLMBar \cite{zeng2024llmbar} and Check-Eval \cite{pereira2024check} first prompt the LLM evaluator to generate a set of sample-specific metrics or checklists that a good sample should adhere to, and then pass it to the LLM evaluator when generating the final rating. 
% We consider 2 strategies: using Metrics or not.

The first 7 factors belong to 3 main components of the evaluation prompt template $ \mathcal{T} $: 
% \hnote{maybe we can use the same example to explain this?}
(1) Task Description (\textbf{TD}), which encompasses the definition of the evaluation task and output format (including \textbf{Scoring Scale} and \textbf{Chain-of-Thought}),
(2) Evaluation Rules (\textbf{ER}), which encompass the standard and steps for evaluation (including \textbf{Evaluation Criteria} and \textbf{AutoCoT}), 
and (3) Input Content (\textbf{IC}), which encompasses the input sample and sample-specific auxiliary information (including \textbf{In-Context Example}, \textbf{Reference}, and \textbf{Metrics}). 
Previous works adopt different orders for these 3 components.
For instance, MT-Bench \cite{zheng2023judging} uses the order \textbf{TD} $\rightarrow$ \textbf{ER} $\rightarrow$ \textbf{IC}, while AutoCalibrate \cite{liu-etal-2024-calibrating} uses \textbf{TD} $\rightarrow$ \textbf{IC} $\rightarrow$ \textbf{ER}. 
We consider the order of these 3 components as the last factor and all 6 possible orders as the selection range.
The overall size of the search space is 12,960. 
We also conduct a preliminary experiment to explore the effect of these factors on Appendix \ref{appendix:preliminary_experiment}.
The results show that these factors significantly influence the performance of LLM evaluators, yet some findings diverge from previous works. 

% \input{tables/initiation}




\subsection{Heuristic Prompting Strategy Search}
% \paragraph{Overview.} 
% \begin{CJK}{UTF8}{gbsn}\textcolor{red}{方法框架图}\end{CJK}
Inspired by the Genetic Algorithms (GA) \cite{mitchell1980need}, we propose Heuristic Prompting Strategy Search (HPSS) to effectively search prompting strategies to help LLM evaluators achieve better alignment with human judgment. 
The framework is shown in Figure \ref{fig:overview}.
HPSS maintains a population of $k$ top-performing prompting strategies and mutates each strategy in this population to create new strategies. 
Their performance is measured by the human correlation between LLM evaluators and human judgment on the validation dataset.
% The top $k$ strategies are retained before proceeding to the next iteration.
Different from traditional GA that randomly selects values for mutation with equal probability, 
HPSS leverages the limited number of values within each factor 
%in the prompting strategy search task 
and calculates the expected correlation metric advantage of each value over the random selection of the corresponding factor. 
The advantage of value $ f_{ij} $ for factor $ F_i $ denotes as $ A_{ij} $. 
Then, this advantage score serves as a heuristic function to guide mutation. 
% Formally, the advantage of value $f_{ij}$ is defined as:
% % \hnote{the expectation is taken over what? testing cases?}
% \begin{equation}
% \small
% \begin{aligned}
% A_{ij} = \mathop{\mathbb{E}}_{\{F_1, F_2, \ldots, F_n \} \backslash\{F_i\}} [r(\mathcal{T}_{F_1, F_2, \ldots, f_{ij},\ldots, F_n}) \\
% -\frac{1}{m_i}\sum_{k=1}^{m_i} r(\mathcal{T}_{F_1, F_2, \ldots, f_{ik},\ldots, F_n}) ]
% \end{aligned}
% \end{equation}
This mechanism assigns higher selection probabilities to more promising values, reducing the cost caused by the blindly random search in GA. 
%thus leading to better performance. 
HPSS mainly contains two steps:

\input{tables/main_results_1}
% 要在前面，related work或者method里，讲一下遗传算法的概念
\paragraph{Initialization.} 
This step aims to get the initial strategy population and advantage estimation of values.
Starting with a commonly-used reference-free pointwise grading prompting strategy from MT-Bench, we modify the selection value for each factor separately to construct multiple strategies.
After that, we calculate the initial advantage estimation of each value based on their performance and select $k$ top-performing strategies for the subsequent iterative search step.
% This process is illustrated in Algorithm \ref{algorithm:initiation}.


\paragraph{Iterative Search.}
In each iteration, HPSS mutates each prompting strategy $\mathcal{T}^{c}$ within the population for $g$ times to generate new strategies, 
and then update the top-$k$ strategy population based on the performance of these new strategies.
% 补上整个流程
To enhance the effectiveness of mutation, HPSS takes overall advantage of all factor values as the performance estimation of one strategy and allocates more search resources to the strategies with higher overall advantages.
Specifically, we utilize two types of mutations: \textit{exploration} and \textit{exploitation}, which have the probability $\rho$ and $ 1 - \rho$ to be chosen for each mutation, respectively.

%\textit{exploitation}, which directly selects the template with the highest overall advantage, and 
\noindent (1) \textit{Exploration}: Exploration selects the strategies adjacent to $\mathcal{T}^{c}$ (i.e., the strategies set $T_{adj}$ which can be obtained by modifying a single factor of $\mathcal{T}^{c}$) and
% In each iteration, HPSS has probability $\rho$ to choose \textit{exploitation} and $ 1 - \rho$ to choose \textit{exploration}. 
calculates the disparity between the overall advantages of these strategies and $\mathcal{T}^{c}$, employing a temperature-controlled softmax function to determine the exploration probability.
Furthermore, in the preliminary experiment, we notice that inaccurate advantage estimation during \textbf{Initialization} stage may lead to insufficient exploration of well-performing values.
To address this, motivated by UCB Sampling \cite{lai1985asymptotically}, we include an additional exploration term in softmax sampling to incentivize a more comprehensive exploration of all the values. 
Formally, assume the value for $F_i$ within $\mathcal{T}^{c}$ is $f_{ic_i}$, and the strategy $\mathcal{T}$ in $T_{adj}$ modifies the value for $F_i$ from $f_{ic_i}$ to $f_{ij}$, then the exploration probability is computed as follows:
% \hnote{it is not a good idea to use $\mathcal{P}$ for both probability and template.}                                    
\begin{equation}
\small
B_{\mathcal{T}} = A_{ij} - A_{ic_{i}} + \lambda\sqrt{\ln(t) / {M_{ij}}}
\end{equation}
\begin{equation}
\small
\label{sample}
P(\mathcal{T}) = \frac{\exp(B_{\mathcal{T}} / \tau)}{\sum_{\mathcal{T}' \in T_{adj}} \exp(B_{\mathcal{T}'} / \tau)}
\end{equation}
where $t$ is the search step, and $M_{ij}$ is the appearance count of value $f_{ij}$ during search process.


After exploring a new prompting strategy $\mathcal{T}^{new}$, we evaluate its performance on the validation dataset and calculate the performance gain brought by the modified value. 
After that, a moving average with previous results is applied to update the value's advantage estimation. 
The advantages of the values that belong to the same factor as the modified value will then be normalized to have a zero mean, so as to satisfy the property that the expectation sums to zero.
% Subsequently, the advantages of this factor will be normalized to have a mean of 0.
Formally, 
% if $\mathcal{T} = \mathcal{T}_{f_{1c_{1}}, f_{ij}, \ldots, {f_{nc_{n}}}}$ is explored, 
the advantage $A$ will be updated as follows, where $N_{ij}$ is the exploration count of the value $f_{ij}$:
\begin{equation}
\label{update_1}
\small
A_{ij} \leftarrow \frac{A_{ij} \cdot N_{ij} + [r(\mathcal{T}^{new}) - (r(\mathcal{T}^{c}) - A_{ic_{i}})]}{N_{ij} + 1}
\end{equation}
\begin{equation}
\small
\label{update_2}
\begin{aligned}
& A_{ik} \leftarrow A_{ik} - avg(A_{i1}, A_{i2}, \cdots, A_{i{m_i}}), \\
& \hspace{5.em} k=1,2,\cdots,m_i
\end{aligned}
\end{equation}

\noindent (2) \textit{Exploitation}: %However, the search scope of \textit{exploration} is constrained to the vicinity of explored prompting strategies. 
%To expand the search scope and avoid getting stuck in a local optimum, HPSS also employs the mutation method \textit{exploitation} besides \textit{exploration}, which selects the unexplored strategy $\mathcal{T}^{max}$ with the highest overall advantage:
Since the search scope of \textit{exploration} is constrained to the vicinity of explored prompting strategies, HPSS also employs the mutation type \textit{exploitation} to expand the search scope and avoid getting stuck in a local optimum. 
This method selects the unexplored strategy $\mathcal{T}^{max}$ with the highest overall advantage:
\begin{equation}
\label{exploitation}
\small
\mathcal{T}^{max} = \mathop{\arg\max}\limits_{\boldsymbol{F}}\sum_{i=1}^{n} A_{F_i}
\end{equation}
% For each mutation, HPSS has the probability $\rho$ to choose \textit{exploitation} and $ 1 - \rho$ to choose \textit{exploration}. 
After reaching the maximum search step, HPSS returns the best-performing strategy on the validation dataset as the final result. 
Detailed implementation of these two steps is presented in Appendix \ref{appendix:algorithm}.