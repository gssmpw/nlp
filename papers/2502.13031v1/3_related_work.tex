% \subsection{Automatic Text Evaluation}
% While the gold standard for text evaluation remains human judgment, it can be time-consuming and labor-intensive \cite{ouyang2022training, zheng2023judging}, calling for cost-effective automatic evaluation methods as alternatives.
% Early evaluation methods are usually based on n-gram \cite{papineni-etal-2002-bleu, lin-2004-rouge} or embedding \cite{bert-score, zhao-etal-2019-moverscore} levels similarity calculation between generated and reference texts, which are limited by their 
% % dependence on reference answers and 
% low correlation with humans. 
% More recent researches focus on using human curated \cite{rei-etal-2020-comet, goyal-durrett-2021-annotating} or synthetic \cite{zhong-etal-2022-towards} data to specialize pre-trained language model evaluators and achieve significant progress. 
% However, these approaches still face challenges such as limited evaluation dimensions \cite{huang-etal-2020-grade} and insufficient generalization \cite{huang2024empirical}.
% With the emergence of LLMs, using LLMs as evaluators to assign scores for given texts has gradually become prevalent \cite{wang-etal-2023-chatgpt, chen-etal-2023-exploring-use, ke-etal-2023-decompeval} due to its advantages in flexibility, interpretability, and generalization.


\subsection{Prompt Design for LLM Evaluators}
% While the gold standard for text evaluation remains human judgment, it can be time-consuming and labor-intensive \cite{ouyang2022training, zheng2023judging}, calling for cost-effective automatic evaluation methods as alternatives. 
With the emergence of LLMs, utilizing LLMs as evaluators to assess the quality of given texts has gradually become prevalent \cite{wang-etal-2023-chatgpt, chen-etal-2023-exploring-use, ke-etal-2023-decompeval} due to its advantages in flexibility, interpretability, and generalization.
% To improve the alignment between LLM evaluations and humans, 
To enhance their performance, 
recent researches attempt to optimize the evaluation prompts for LLM evaluators from multiple aspects
\cite{stureborg2024large, kim-etal-2023-better, jain-etal-2023-multi, murugadoss2024evaluating, he-etal-2024-socreval, pereira2024check}. 
Specifically, G-Eval \cite{liu-etal-2023-g} requires LLMs to generate evaluation steps first and refer to these steps to finish the evaluation task. 
LLMBar \cite{zeng2024llmbar} explores the effects of self-generated reference answers and sample-specific metrics on text evaluation. 
% CloserLook \cite{chiang-lee-2023-closer} investigates the impact of output formats and finds that generating explanations can enhance the performance of LLM evaluators.
HD-Eval \cite{liu-etal-2024-hd} attempts to align LLM evaluators with humans through hierarchical evaluation criteria decomposition.
However, these works are constrained to optimize an individual factor for evaluation prompts, leading to insufficient optimization. In contrast, our work focuses on adjusting multiple factors and searching for effective prompting strategies to fully stimulate the potential of LLM evaluators.


\subsection{Prompt Optimization}

\input{tables/factors}

Although LLMs have demonstrated impressive performance across various NLP tasks \cite{zhao2023survey}, researchers point out that their abilities are highly dependent on sophisticated prompt design \cite{pryzant-etal-2023-automatic, leidinger-etal-2023-language, raina-etal-2024-llm}. Considering that manual prompt engineering is time-consuming, many works attempt to optimize prompt automatically. GPS \cite{xu-etal-2022-gps} and GRIPS \cite{prasad-etal-2023-grips} employ a genetic algorithm, using rule or generative language models to mutate the initial prompt. PromptBreeder \cite{fernando2023promptbreeder} and \textsc{EvoPrompt} \cite{guo2023connecting} utilize LLMs for mutation and crossover operations in evolutionary searches. OPRO \cite{yang2023large}  demonstrates the potential of LLMs to perform iteratively prompt optimization based on search history.
% and corresponding scores.
%\textcolor{red}{
Furthermore, \citet{hsieh-etal-2024-automatic} extends prompt optimization to long prompts containing multiple sentences. They utilized Lin-UCB to guide the selection of sentences and employed in-context learning for LLM-based mutation.
% For comparison, our work shifts the focus from word or sentence-level prompt optimization to a higher-level selection strategy for multiple components of prompts. 
For comparison, our work shifts the focus from word or sentence-level prompt optimization to a higher-level strategy that adjusts multiple factors of evaluation prompts. 
Additionally, we introduce a heuristic function to enhance the effectiveness of mutation based on the characteristics of evaluation prompting strategies.



% However, those works focus on word or sentence-level prompt optimization, making them naturally unsuitable for multiple higher-level factors optimization.
%再想一想怎么改更强力
%}
% \vspace{-1mm}
% \footnotetext[1]{Constructing high-quality human-written references requires extensive manual annotation. We do not consider this usage in our study to ensure a fair comparison.}