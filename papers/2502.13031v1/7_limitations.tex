The limitations of our work are summarized as follows:

% \paragraph{To reduce the search overheads.} Although HPSS can find a well-behaved prompting strategy with only about 70 evaluations on the validation dataset, the overheads are still not negligible, especially when optimizing a closed-source LLM evaluator with cost or rate limits. 
% Introducing early stopping or some pruning techniques could potentially reduce the search overheads of HPSS.

% \paragraph{To cover more design factors of LLM-as-a-Judge.} 
%  Due to budget limitations, 
%  HPSS focuses on the factors of the input prompts for LLM-as-a-Judge using a single decode strategy (i.e., greedy search).
%  Our search framework has the potential to encompass more design factors of LLM-as-a-Judge, such as the decoding strategies and the interaction methods among multiple LLM evaluators.
%  Although our experiments demonstrate that incorporating decoding strategies such as self-consistency can further enhance the performance of HPSS,
%  integrating a broader range of factors into our search framework could potentially yield even greater improvements in evaluation performance.
%  We consider this an important future work.

1) HPSS requires iterative search on a validation dataset with human annotations, introducing additional annotation and inference overheads. However, we believe that it is not a severe problem for the following three reasons:
\textbf{Firstly}, the overall cost is affordable in the vast majority of scenarios. Under the default setting, where 50\% of the dataset is selected as the validation dataset, the search cost for a specific evaluation aspect of one dataset is approximately \$8 for GPT-4o-mini, and approximately 40 minutes using 4 H100 GPUs for Qwen2.5-14B-Instruct (See Appendix \ref{appendix:hpss}).
\textbf{Secondly}, HPSS remains effective in low-resource scenarios. As shown in Figure \ref{fig:size}, even when annotated data is scarce 
(reducing the validation dataset size to 10\% of the entire dataset, $ \sim $ 100 samples per dataset)
, HPSS can still significantly outperform manually designed prompt templates.
\textbf{Finally}, HPSS is efficient during inference on the test dataset, requiring only a single greedy decoding pass for each test sample while still achieving better performance than much larger manually designed LLM evaluators that require 20 generations and self-consistency decoding (See Table \ref{tab:main_results_1}, Table \ref{tab:main_results_2}, and Appendix \ref{appendix:stronger}).
Nonetheless, introducing well-designed early stopping strategies
%or pruning techniques 
could potentially reduce the search overheads of HPSS, which is considered as important future work.

2) HPSS mainly focuses on the prompting strategy optimization of the input prompts for LLM evaluators. 
Despite the importance of input prompts, other modules such as decoding and interaction strategies may also benefit the performance of LLM evaluators.
%Integrating appropriate decoding strategies and interaction strategies among multiple LLM evaluators to HPSS can further improve the performance of LLM-as-a-Judge. 
Regarding decoding strategies, our experiments demonstrate that incorporating self-consistency decoding can enhance the performance of HPSS (See Appendix \ref{appendix:sc}).
Regarding multiple LLM evaluators' interaction, in our preliminary experiments, we follow the settings of ChatEval \cite{chan2024chateval} to explore the impact of interaction strategies with multiple Qwen2.5-14B-Instruct evaluators, using the prompting strategies found by HPSS. 
We find that different interaction strategies have negligible impacts on the final evaluation performance, which is similar to the self-consistency method that directly averages the evaluation results of multiple individual LLM evaluators. 
Given that the search space increases significantly in multi-agent interaction scenarios and that it is difficult for LLM evaluators with different output formats to interact directly, we leave the optimization of interaction strategies among multiple LLM evaluators as important future work.