\section{List of Evaluation Prompt Templates}
\label{appendix:prompt_template}
This section lists all prompt templates applied throughout this study, including the prompt templates utilized to generate the final rating (Table \ref{tab:evaluation_prompt_summeval}, \ref{tab:evaluation_prompt_topical_chat}, \ref{tab:evaluation_prompt_sfhot}, \ref{tab:evaluation_prompt_hanna}) and the templates used to generate Reference, AutoCoT, and Metrics (Table \ref{tab:generation_prompt_summeval}, \ref{tab:generation_prompt_topical_chat}, \ref{tab:generation_prompt_sfhot}, \ref{tab:generation_prompt_hanna}). 
For these prompt templates, 
we generally refer to the reference-free single answer pointwise grading prompt from MT-Bench \cite{zheng2023judging}. 
However, due to the contents of some components of this template 
being mixed, we make minor adjustments to the order of some sentences to ensure that the search for factor Order can be conducted.
We use the prompt template of LLMBar \cite{zeng2024llmbar} to generate Metrics.


\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/init.pdf}
  \vspace{-2mm}
  \caption{Average dataset-level Spearman human correlation on Topical-Chat for GPT-4o-mini evaluator using different prompting strategies, which are modified based on the baseline strategy from MT-Bench for each factor.}
  \vspace{-4mm}
  \label{fig:init}
\end{figure*}

\section{Preliminary Experiment on Factor Effects}
\label{appendix:preliminary_experiment}
% As shown in Table \ref{tab:factors}, although previous works reach relatively consistent conclusions regarding the effects of some factors, there remains controversy regarding some other factors. 
To further explore the effect of each factor on the performance of LLM evaluators, we conducted a preliminary experiment with GPT-4o-mini on a commonly-used dialogue evaluation dataset Topical-Chat \cite{gopalakrishnan2019topical}. 
Using the prompting strategy from MT-Bench \cite{zheng2023judging} with the default scoring scale 1-3 of Topical-Chat as the baseline, we adjust the selection value of each factor separately, employ greedy search decoding  
% \hnote{it means greedy decoding?} 
to generate rating scores, and report the Spearman correlations with human judgments.
The results, as shown in Figure \ref{fig:init}, indicate that these factors significantly influence the performance of LLM evaluators, yet some findings diverge from previous works. 
For instance, \citet{pereira2024check} claims that Metrics can improve the performance of LLM evaluators. 
However, we observed a performance decrease on Topical-Chat. 
\citet{chiang-lee-2023-closer} finds that CoT plays an important role in the evaluation prompt. 
However, when it comes to the GPT-4o-mini evaluator, adding CoT results in negligible differences.
% generating explanations can enhance the performance of LLM evaluators.
% \citet{stureborg2024large} find that a moderate scoring score 1-10 achieves the best performance. However, 1-50 performs better in our experiment.
These results highlight the importance of optimizing the prompting strategy for adjusting these factors. 
% (2) The effect of the same factor across different aspects shows considerable variability, confirming the necessity of adaptive adjustments to the prompt template for different aspects.

\section{Details of In-Context Example Selection}
\label{appendix:in_context_example}
We perform stratified sampling based on human ratings within the validation dataset to obtain in-context examples, aiming to ensure an even distribution of examples across different human ratings. 
When evaluating the performance of some prompting strategies on the validation dataset, we remove the corresponding example in the in-context examples set if this example is to be evaluated, aiming to prevent data leakage.


\section{Details of Benchmarks}
\label{appendix:benchmarks}
A brief introduction of the meta-evaluation benchmarks involved is as follows:
\begin{itemize}
\item  \textbf{Summeval} \cite{fabbri-etal-2021-summeval} is a meta-evaluation benchmark for summarization. 
It contains human evaluation annotations for 16 summarization systems on 100 articles from the CNN / DailyMail corpus, resulting in a total of 1600 summary-level annotations. 
Each summary is evaluated on four aspects: \textit{Coherence}, \textit{Consistency}, \textit{Fluency}, and \textit{Relevance}. 
The authors recruit annotators on Amazon Mechanical Turk (AMT) to rate each summary on a scale from 1 to 5. 
Cross-validation by other annotators and experts is conducted to correct errors and enhance annotation quality.
\item \textbf{Topical-Chat} \cite{gopalakrishnan2019topical} is a meta-evaluation benchmark for knowledge-grounded dialogue generation. 
It contains 360 samples, each including dialogue context, relevant knowledge, a response, and human ratings of the response across five aspects: \textit{Coherence}, \textit{Engagingness}, \textit{Groundedness}, \textit{Naturalness}, and \textit{Understandability}, ranging from 1 to 3. 
The annotators recruited from AMT provide the ratings. 
Following HD-Eval \cite{liu-etal-2024-hd}, the first four aspects are used to measure the performance of HPSS.
\item \textbf{SFHOT/ SFRES} \cite{wen-etal-2015-semantically} are meta-evaluation benchmarks for data-to-text generation. 
They contain 875 / 1181 samples respectively, which provide information about restaurants and hotels in San Francisco and aim to let the model generate corresponding utterances. 
The authors recruit annotators from AMT to rate the \textit{Informativeness} and \textit{Naturalness} of the generated utterances for each sample on a scale from 1 to 6.
\item \textbf{HANNA} \cite{chhun-etal-2022-human} serves as a meta-evaluation benchmark for story generation. 
It contains 1,056 stories produced by 10 different automatic story generation systems. 
Each story is rated by 3 annotators recruited from Amazon Mechanical Turk on 6 aspects: \textit{Coherence}, \textit{Relevance}, \textit{Empathy}, \textit{Surprise}, \textit{Engagement}, and \textit{Complexity}. 
Ratings range from 1 to 5. The final score for each aspect is the average of the three annotators' ratings.
\item \textbf{MT-Bench} \cite{zheng2023judging} comprises 3.3k expert-level pairwise human evaluation of responses,  generated by six LLMs on 80 carefully designed questions. 
These questions cover 7 categories: \textit{Writing}, \textit{Roleplay},
\textit{Reasoning Math}, 
\textit{Coding}, \textit{Extraction}, \textit{STEM} and \textit{Humanities}.
We select the first round of dialogues from this dataset and filter out the tied cases, leaving a final evaluation dataset of 1020 instances.

\item \textbf{AUTO-J (Eval-P)} \cite{li2024generative} provides 1,392 pairwise comparison data, each of which contains a query, two LLM-generated responses, and a human-annotated preference label. 
This dataset involves 58 real-world scenarios and the responses are generated from 6 LLM families. 
We filter out the tied cases and leave a final evaluation dataset of 1019 instances.

\item  \textbf{LLMBar} \cite{zeng2024llmbar} is a meta-evaluation benchmark for instruction-following, which consists of two components:
(1) The Natural set, which is gathered from existing human-preference datasets. 
(2) The Adversarial set, where the authors intentionally create misleading outputs that appear plausible but deviate from the instructions to challenge the evaluators.
This dataset contains a total of 419 pairwise comparison instances.

\end{itemize}

\section{Metric Calculation}
\label{appendix:correlation}
\subsection{Pointwise Grading}
Following previous work \cite{liu-etal-2024-hd, liu-etal-2024-calibrating, zhong-etal-2022-towards}, we adopt dataset-level (sample-level for Summeval as an exception) Spearman ($\rho$) correlation coefficient between human judgments and LLM evaluations to measure the performance of LLM evaluators. 
Given a dataset $\mathcal{D}$, evaluation aspect $a$ and evaluation metric $f(\cdot)$, we could calculate the human correlation of this evaluation metric at either dataset or sample level:
\begin{itemize}
\item \textbf{Dataset Level}: For dataset-level human correlation, we evaluate the correlations on all samples in the dataset, as follows:
\begin{equation}
\small
\begin{aligned}
corr_{dataset}(\{s^*_{i,a}\}_{i=1}^{|D|}, \{s_{i,a}\}_{i=1}^{|D|}) = & \\
\rho([{s}^*_{i,a}, ..., {s}^*_{|D|,a}], [s_{i,a}, ..., s_{|D|,a}])
\end{aligned}
\end{equation}
where $\{s^*_{i,a}\}_{i=1}^{|D|}$ and $\{s_{i,a}\}_{i=1}^{|D|}$ denote the 
evaluation results (for free-text evaluations, scores are extracted via rules as final evaluation results) for the aspect $a$ of dataset $\mathcal{D}$ from human annotations and evaluation metric $f(\cdot)$, respectively. 
\item \textbf{Sample Level}: Assume that the dataset $\mathcal{D}$ consists of $J$ queries where each query has target responses from $M$ diverse systems (with a total of $|D| = M \times J$ samples),
and for sample-level human correlation, we first compute correlations on multiple responses to an individual query (e.g., the summaries from 16 summarization systems on one article for Summeval), then average them across all queries:
\begin{equation}
\small
\begin{aligned}
corr_{sample}(\{s^*_{i,a}\}_{i=1}^{|D|}, \{s_{i,a}\}_{i=1}^{|D|}) = & \\
\frac{1}{J} \sum_{i=1}^{J}(\rho([{s}^*_{i1,a}, ..., {s}^*_{iM,a}], [s_{i1,a}, ..., s_{iM,a}]))
\end{aligned}
\end{equation}
where $s^*_{ij,a}$ and $s_{ij,a}$ denote the evaluation results for the $j$-th response to $i$-th query 
for the aspect $a$ of dataset $\mathcal{D}$, from human annotations and evaluation metric $f(\cdot)$, respectively.
\end{itemize}

The evaluation metric $f(\cdot)$ is the LLM evaluator using a specific prompt template in our implementation, and the calculation for the Spearman correlation coefficient $\rho$ between two vectors of length $n$ is as follows:
\begin{equation}
\small
\begin{aligned}
\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}
\end{aligned}
\end{equation}
where $d_i$ represents the difference in the rank of the $i$-th element between two vectors, where the ranks are determined by sorting the elements within their respective vectors in ascending order.

\subsection{Pairwise Comparison}
Following previous work \cite{zeng2024llmbar, lambert2024rewardbench}, we directly adopt accuracy to measure the performance of LLM evaluators in pairwise comparison. Given a dataset $\mathcal{D}$, evaluation aspect $a$ and evaluation metric $f(\cdot)$, the accuracy is calculated as follows:
\begin{equation}
\small
\begin{aligned}
acc(\{s^*_{i,a}\}_{i=1}^{|D|}, \{s_{i,a}\}_{i=1}^{|D|}) = 
\frac{1}{|D|}\mathbb{I}(s^*_{i,a}=s_{i,a}) 
\end{aligned}
\end{equation}


\section{Implementation Details of Baselines}
\label{appendix:baselines}
As for APE, we use the LLM evaluator to resample new prompts. 
The queue size is set to 5. 
In each iteration, two new prompt candidates are resampled based on each prompt in the queue. 
As for OPRO, we use the LLM evaluator to generate new prompting strategies.
We provide the LLM with the selection range for each factor, as well as the 20 previously top-performing strategies and their corresponding correlation metrics, which serve as the search history.  
The LLM is asked to generate a list containing the new selection strategy for each factor. 
The explored prompting strategies in \textbf{Initiation} will serve as the initial search history. 
As for Greedy, we perturb the current prompting strategy 5 times in each iteration by randomly replacing the value of one factor to generate new strategies. 
The strategy that performs best on the validation dataset is retained for the next iteration.
Finally, as for Stepwise-Greedy, we follow the order shown in Table \ref{tab:factors} to optimize each factor sequentially. 
In each step, we select the value for the current factor that performs best on the validation dataset while holding all other factors fixed.
This selected choice is then established as the final optimization result for the current factor.



\section{Implementation Details of HPSS}
\label{appendix:hpss}
We determine the hyperparameters for HPSS via grid search on the validation dataset of Topical-Chat using the Qwen2.5-14B-Instruct evaluator. Specifically, the population size $k$ is set to 5.
The mutation time for each template $g = 2$.  The exploitation probability $\rho = 0.2$.
The temperature $\tau$ for the softmax function used to calculate the exploration probability of each template is set to 5, and the weight $\lambda$ for the additional exploration term is set to 4. 
We provide the performance of Qwen2.5-14B-Instruct after modifying each hyperparameter choice on the validation dataset of Topical-Chat in Figure \ref{fig:hyper}.
% For this model, We utilize vllm \cite{10.1145/3600006.3613165} framework for inference on 4 H800 GPUs.
% The total runtime for HPSS across all datasets is approximately 12 hours.

Under the computational budget described in Section \ref{sec:experiments} (i.e., 71), the search cost of HPSS (i.e., the inference times of the LLM evaluator) for a specific evaluation aspect of one dataset is approximately 70 times the size of the validation dataset. 
When the size of the validation dataset is 50\% of the entire dataset, for GPT-4o-mini, the cost of HPSS for a specific evaluation aspect of one dataset is approximately \$8. 
In total, the overall cost of HPSS across all the datasets and evaluation aspects is approximately \$140. 
For Qwen2.5-14B-Instruct, the runtime of HPSS for a specific evaluation aspect of one dataset is approximately 40 minutes using 4 H100 GPUs and the vllm \cite{10.1145/3600006.3613165} inference framework, while the overall runtime of HPSS across all the datasets and aspects is approximately 12 hours. 
The overall costs for other automatic prompt optimization methods for LLM evaluators are the same as HPSS, which is affordable in the vast majority of scenarios.
We also validate that even with 1/5 of the above search cost (reducing the validation dataset size to 10\% of the entire dataset), HPSS can still significantly outperform human-designed LLM evaluators in Figure \ref{fig:size}.



\section{Details of Algorithm Implementation}
 \label{appendix:algorithm}
We provide the detailed implementation of the two steps of HPSS in Algorithm \ref{algorithm:initiation} and \ref{algorithm:search} respectively.



\section{Experiments on Pairwise Comparison}
\label{appendix:pairwise}
\subsection{Experimental Setup}
Apart from pointwise grading tasks, we also validate our method on three pairwise comparison benchmarks, i.e., MT-Bench \cite{zheng2023judging}, AUTO-J \cite{li2024generative}, and LLMBar \cite{zeng2024llmbar}, which primarily focus on instruction-following tasks.
Qwen2.5-14B-Instruct is employed as the evaluation model, with all hyperparameters remaining the same as pointwise grading experiments. 
Regarding the search space, we remove the factor Scoring Scale, and the value \textit{Self-Generated Criteria} of the factor Evaluation Criteria, which only exist in the pointwise grading setting. 
We compare our method with the prompting strategies from MT-Bench, the best human-designed prompting strategies \textit{Metrics+Reference$^*$} found by LLMBar, and all automatic prompt optimization methods examined in pointwise grading experiments.
Results of prompt search are averaged over 3 random seeds and the standard deviation is provided.
\subsection{Main Results}
The main results are provided in Table \ref{tab:pairwise_results}.
HPSS substantially improves the performance of LLM evaluators compared to human-designed prompting strategies and achieves the best average performance across all automatic prompt optimization methods, which validates its effectiveness in pairwise comparison prompt optimization.
On the AUTO-J (Eval-P), multiple prompt optimization methods simultaneously achieve the best results.
Upon data examination, we find that %the reason is that 
the best prompting strategy is close to the starting point, with only one factor having a different value, resulting in lower search difficulty. 
Overall, we observe that human-designed prompting strategies for pairwise comparison tasks have already been well-optimized, and the improvements brought by automatic prompt optimization are relatively modest compared to those in pointwise grading tasks.

\input{tables/pairwise_results}
\input{tables/stronger_model}
\input{tables/sc}
\begin{figure*}[!h]
\scriptsize
    \centering
    \includegraphics[width=1.0\textwidth]{figures/hyper.png}
    \vspace{-4mm}
    \caption{Performance of Qwen2.5-14B-Instruct evaluator under different hyperparameters settings. 
    We provide the average results over 3 seeds on the validation dataset of Topical-Chat.}
    \label{fig:hyper}
\end{figure*}
\section{Comparison with Stronger LLM Evaluator}
\label{appendix:stronger}
As shown in Table \ref{tab:stronger_model}, we compare the performance of the Qwen2.5-14B-Instruct evaluator using the prompting strategies obtained by HPSS with the Qwen2.5-72B-Instruct evaluator using human-designed prompting strategies. 
Notably, Qwen2.5-14B-Instruct with HPSS achieve
significantly better evaluation performance than the human-designed Qwen2.5-72B-Instruct evaluator, even with only 5\% of the generation times.
These results demonstrate the efficiency of HPSS.


\section{Incorporating HPSS with Inference-Time Methods}
\label{appendix:sc}
As illustrated in Table \ref{tab:main_results_2}, for GPT-4o-mini evaluator, 
the performance of CloserLook + ICL surpasses HPSS on HANNA. 
Considering the different inference overheads of these two methods, 
we attempt to incorporate self-consistency \cite{wang2022self} decoding strategy into HPSS to ensure a fair comparison. 
Specifically, we conduct 20 generations and compute the average evaluation score,
% as the final score
which is consistent with the setting of CloserLook + ICL. 
As shown in Table \ref{tab:sc}, self-consistency further enhances the performance of HPSS, consistently outperforming CloserLook + ICL with the same generation times. 
This result indicates that integrating HPSS with inference-time methods can further improve the performance of LLM evaluators.


% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=1.0\linewidth]{figures/sc.png}
%   \caption{Performance of GPT-4o-mini using HPSS and baseline prompt (CloserLook + ICL) under different decoding strategies on HANNA: greedy and self-consistency (SC). }
%   \label{fig:sc}
% \end{figure}


% \section{Case Study}
% \label{appendix:f}
% In Table \ref{tab:case_study}, we present some cases of prompting strategies explored by HPSS for specific tasks. 
% We can observe that there is a significant performance gap between the best-performing and worst-performing prompting strategies, 
% emphasizing the sensitivity of LLM evaluators to prompting strategy. 
% Some of the best-performing prompting strategies include values that are rarely considered in human-designed evaluation prompts. 
% For instance, for the aspect of Complexity in HANNA, the best-performing prompting strategy places the sample to evaluate (\textbf{IC}) first and the task description (\textbf{TD}) last.
% For the aspect of Coherence in Topical-Chat, evaluation criteria are not used. 
% These results demonstrate the limitations of manual prompt design and underscores the importance of automatic prompting strategy optimization.


% We investigate the detailed selection of all factors of the prompts that have been searched for during HPSS in certain tasks. The result (Table \ref{tab:case_study}) demonstrates that although the best prompts tend to encompass selections with relatively high advantages, there still exist cases in which the best prompts include certain selection that possesses very low advantages or contradicts the common sense of humans, for instance, the selection of \textbf{IET}, \textbf{Dialectic} and \textbf{Metrics}. This further provides a solid reason why we need to leverage HPSS to improve the performance of LLM evaluators.
% \input{tables/case_study}

\begin{algorithm*}[h]
\caption{Initialization}
\label{algorithm:initiation}
\begin{algorithmic}[1]
\small
\Require $ n $ factors $ F_1, F_2, \ldots, F_n $, baseline prompting strategy $ \mathcal{T}^{base} = \mathcal{T}_{f_{1b_{1}}, f_{2b_{2}}, \ldots, {f_{nb_{n}}}} $, performance metrics $r$
\State $T \gets \{\mathcal{T}^{base}\} $
\State $s_{base} \gets c(\mathcal{T}^{base}) $
\For{$i = 1, \cdots, n$} 
\For{$j = 1, \cdots, m_i \ \text{and} \ j \neq b_i$}
\State $T \gets T \cup \{\mathcal{T}_{f_{1b_{1}}, f_{2b_{2}}, \ldots, f_{ij}, \ldots, {f_{nb_{n}}}}\} $
\State $s_{ij} \gets r(\mathcal{T}_{f_{1b_{1}}, f_{2b_{2}}, \ldots, f_{ij}, \ldots, {f_{nb_{n}}}}) $
\EndFor
\For{$j = 1, \cdots, m_i$}
\State $A_{ij} \gets s_{ij} - avg(s_{i1}, s_{i2}, \cdots, s_{i{m_i}}) $
\EndFor
\EndFor
\State \Return the best $k$ prompt templates based on $s$,
as $best$
\end{algorithmic}
\end{algorithm*}

\begin{algorithm*}[h]
\caption{Iterative Search}
\label{algorithm:search}
\begin{algorithmic}[1]
\small
\Require Explored prompting strategies set $T$, initial prompting strategies population $best$, performance metrics $r$, beam size $k$, budget $m$, hyper-parameters $\lambda, g, \tau, \rho$
\State $cost \gets 0 $
\While{$cost < m $}
\State $new\_best \gets best$
\For {each $\mathcal{T}^{c} $ in $best$}
\For {$l = 1, \cdots, g$}

\State Sample new candidate prompting strategy $\mathcal{T}^{new}$ in $T_{adj}$ based on Equation \ref{sample}
\If {$\mathcal{T}^{new} \in T$} 
\State \textbf{continue} \Comment{Skip explored strategies}
\EndIf
\State Sample $\alpha \sim \text{Bernoulli}(\rho)$
\If {$\alpha$}
\State Select $\mathcal{T}^{max}$ with the highest overall advantage and $\mathcal{T}^{max} \notin T$ based on Equation \ref{exploitation}
\State $\mathcal{T}^{new} \gets \mathcal{T}^{max} $
\EndIf
\State $T \gets T \cup \{\mathcal{T}^{new}\} $
\State $s^{new} \gets r(\mathcal{T}^{new})$
\State $new\_best \gets new\_best \cup \{\mathcal{T}_{new}\}$
\State $cost \gets cost + 1$
\If {not $\alpha$}
\State Update advantage $A$ based on Equation \ref{update_1} and \ref{update_2}
\EndIf
\EndFor
\EndFor
\State Select the best $k$ prompting strategies in $new\_best$, as $best$
\EndWhile
\State \Return $ best[0] $
\end{algorithmic}
\end{algorithm*}





\input{tables/base_prompt_summeval}
\input{tables/self_generation_summeval}
\input{tables/base_prompt_topical_chat}
\input{tables/self_generation_topical_chat}
\input{tables/base_prompt_sfhot_sfres}
\input{tables/self_generation_sfhot_sfres}
\input{tables/base_prompt_hanna}
\input{tables/self_generation_hanna}

\input{tables/case_study}
\section{Case Study}
\label{app:case}
In Table \ref{tab:case_study}, we present some cases of prompting strategies explored by HPSS for specific tasks. 
We find that there is a significant performance gap between the best-performing and worst-performing prompting strategies, 
emphasizing the sensitivity of LLM evaluators to prompting strategy. 
Figure \ref{fig:case_1} and \ref{fig:case_2} illustrate the original evaluation prompts from MT-Bench alongside the optimized one found by HPSS, specifically focusing on the aspect \textit{Complexity} in HANNA and the aspect \textit{Coherence} in Topical-Chat.
The prompting strategies of these prompts are shown in the "origin" and "best" rows in Table \ref{tab:case_study} for their respective datasets and aspects.
We observe that some of the prompting strategies found by HPSS include values that are rarely considered in human-designed evaluation prompts. 
For instance, for the aspect \textit{Complexity} in HANNA, the prompting strategy found by HPSS places the input content (\textbf{IC}) first and the task description (\textbf{TD}) last.
For the aspect \textit{Coherence} in Topical-Chat, evaluation criteria are not used in HPSS. 
These results demonstrate the limitations of manual prompt design and underscores the importance of automatic prompting strategy optimization.

Furthermore, to intuitively show the reason why HPSS achieves better evaluation performance than human-designed LLM evaluators, we
provide two judgment generation cases for different LLM evaluators in Table \ref{tab:judgement_case_1} and \ref{tab:judgement_case_2}. 
The corresponding evaluation prompts are shown in Figure \ref{fig:case_3} and \ref{fig:case_4}.
In the first case from the text summarization task presented in Table \ref{tab:judgement_case_1}, HPSS provides a balanced assessment by analyzing both strengths and weaknesses of the summary, with emphasis on overall fluency. 
In contrast, MT-Bench and CloserLook + ICL overemphasize minor issues like typos while overlooking its advantages in overall fluency, and exhibit some hallucinations in judgment.
In the second case from the story generation task presented in Table \ref{tab:judgement_case_2},
HPSS conducts a systematic analysis of the story and correctly identifies both key plots that effectively convey emotions and overly idealized plots that weaken emotional delivery. 
In contrast, MT-Bench and CloserLook + ICL each overlook one of these two important points, resulting in less accurate evaluations. 
These observations demonstrate that HPSS improves the ability of LLM evaluators to conduct comprehensive evaluations
and achieve a balanced assessment of strengths and weaknesses within the input sample.

\begin{figure*}[!t]
\scriptsize
    \centering
    \includegraphics[width=1.0\textwidth]{figures/case_1.pdf}
    \vspace{-4mm}
    \caption{The original evaluation prompt for the aspect \textit{Complexity} in HANNA from MT-Bench and the corresponding evaluation prompt found by HPSS for GPT-4o-mini evaluator.
    Factors with values other than "None" in the evaluation prompts are highlighted. 
    The three main components of the evaluation prompt are annotated on the right side, e.g., Task Description (\textbf{TD}), Evaluation Rule (\textbf{ER}), and Input Content (\textbf{IC}). 
    The placement order of these three components is also considered a factor in our optimization.}
    \label{fig:case_1}
\end{figure*}

\begin{figure*}[!t]
\scriptsize
    \centering
    \includegraphics[width=1.0\textwidth]{figures/case_2.pdf}
    \vspace{-4mm}
    \caption{The original evaluation prompt for the aspect \textit{Coherence} in Topical-Chat from MT-Bench and the corresponding evaluation prompt found by HPSS for Qwen2.5-14B-Instruct evaluator.}
    \label{fig:case_2}
\end{figure*}

\begin{figure*}[!t]
\scriptsize
    \centering
    \includegraphics[width=1.0\textwidth]{figures/case_3.pdf}
    \vspace{-4mm}
    \caption{The original evaluation prompt for the aspect \textit{Fluency} in Summeval from MT-Bench and the corresponding evaluation prompt found by HPSS for Qwen2.5-14B-Instruct evaluator.}
    \label{fig:case_3}
\end{figure*}

\begin{figure*}[!t]
\scriptsize
    \centering
    \includegraphics[width=1.0\textwidth]{figures/case_4.pdf}
    \vspace{-4mm}
    \caption{The original evaluation prompt for the aspect \textit{Empathy} in HANNA from MT-Bench and the corresponding evaluation prompt found by HPSS for Qwen2.5-14B-Instruct evaluator.}
    \label{fig:case_4}
\end{figure*}

\input{tables/judgement_case_1}
\input{tables/judgement_case_2}
