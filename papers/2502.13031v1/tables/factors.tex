\begin{table*} [t]
\small
\centering
\resizebox{0.95\linewidth}{!} {
\begin{tabular}{p{0.09\linewidth}p{0.24\linewidth}p{0.44\linewidth}p{0.23\linewidth}}
\toprule
\textbf{Factor} & \textbf{Definition} & \textbf{Common Usage}  & \textbf{Selection Range} \\
\midrule
Scoring \newline Scale & The scoring range for text evaluation. &
Various scoring scales are used in previous work, such as 1-3 \cite{gopalakrishnan2019topical, lin2023llm}, 1-5 \cite{fabbri-etal-2021-summeval, chhun-etal-2022-human}, 1-10 \cite{liu-etal-2024-alignbench} and 1-100 \cite{stureborg2024large}. & 
\begin{minipage}[t]{\linewidth}\vspace{-7pt}\begin{itemize}[noitemsep, topsep=0pt, left=0pt]
    \item \textbf{1-3}
    \item \textbf{1-5}
    \item \textbf{1-10}
    \item \textbf{1-50}
    \item \textbf{1-100}
\end{itemize}\vspace{1pt}\end{minipage} \\
\midrule
In-Context \newline Example & Human evaluation examples & 
Previous works try to construct in-context examples through random or stratified sampling based on human ratings to enhance the performance of LLM evaluators \cite{kim-etal-2023-better, huang2024empirical, jain-etal-2023-multi}. The number of examples usually ranges from 1 to 4. & \begin{minipage}[t]{\linewidth}\vspace{-7pt}\begin{itemize}[noitemsep, topsep=0pt, left=0pt]
    \item \textbf{0 examples}
    \item \textbf{3 examples}
    \item \textbf{5 examples}
    \item \textbf{10 examples}
\end{itemize}\vspace{-7pt}\end{minipage} \\
\midrule
Evaluation \newline Criteria & The general definition of the evaluation aspect and the scoring standards of the quality of a text. & 
The majority of previous works use human-written criteria. Some recent works also prompt LLM evaluators to generate criteria by themselves \cite{kotonya-etal-2023-little} or even use no criteria \cite{murugadoss2024evaluating}. 
&  \begin{minipage}[t]{\linewidth}\vspace{-7pt}\begin{itemize}[itemsep=0.5pt, topsep=0pt, left=0pt]
    \item \textbf{No Criteria}
    \item \textbf{Human-Written Criteria}
    \item \textbf{Self-Generated Criteria}
\end{itemize}\vspace{-7pt}\end{minipage} \\
\midrule
Reference & The reference answer for the task in the sample to evaluate. & 
In addition to using no reference \cite{liu-etal-2023-g}, previous work also utilizes the following types of references: 
% (1) \textbf{Human-Written Reference}\footnotemark[1] \cite{zheng2023judging, doddapaneni2024finding}, 
(1) \textbf{Self-Generated Reference}: prompting the evaluator to generate a reference independently and then feed it into $ \mathcal{T} $ \cite{zeng2024llmbar}, 
(2) \textbf{Dialectic}: prompting the evaluator to generate a reference along with the final rating \cite{he-etal-2024-socreval}
 & \begin{minipage}[t]{\linewidth}\vspace{-7pt}\begin{itemize}[itemsep=0.5pt, topsep=0pt, left=0pt]
     \item \textbf{No Reference}
     \item \textbf{Self-Generated Reference}
     \item \textbf{Dialectic}
\end{itemize}\vspace{0pt}\end{minipage} 
\\
\midrule
 Chain-of-Thought\newline(CoT) & Request for the LLM evaluator to generate evaluation explanations & Previous work mainly utilizes following CoT formats: (1) \textbf{No CoT}: generate the rating without an explanation \cite{liu-etal-2023-g}, (2) \textbf{Prefix CoT}: provide an explanation first and then give the rating \cite{zheng2023judging, chiang-lee-2023-closer}, and (3) \textbf{Suffix CoT}: provide the rating first, followed by an explanation \cite{chiang-lee-2023-closer} & \begin{minipage}[t]{\linewidth}\vspace{-7pt}\begin{itemize}[itemsep=0.5pt, topsep=0pt, left=0pt]
     \item \textbf{No CoT}
     \item \textbf{Prefix CoT}
     \item \textbf{Suffix CoT}
 \end{itemize}\vspace{-7pt}\end{minipage} \\
 \midrule
 AutoCoT & Self-generated evaluation steps for specific evaluation aspect & G-Eval \cite{liu-etal-2023-g}, OP-I-Prompt \cite{siledar-etal-2024-one} & \begin{minipage}[t]{\linewidth}\vspace{-7pt}\begin{itemize}[noitemsep, topsep=0pt, left=0pt]
     \item \textbf{No AutoCoT}
     \item \textbf{AutoCoT}
 \end{itemize}\vspace{-7pt}\end{minipage} \\
 \midrule
 Metrics & Self-generated sample-specific metrics for good answer & LLMBar \cite{zeng2024llmbar}, Check-Eval \cite{pereira2024check}  & \begin{minipage}[t]{\linewidth}\vspace{-7pt}\begin{itemize}[noitemsep, topsep=0pt, left=0pt]
     \item \textbf{No Metrics}
     \item \textbf{Metrics}
 \end{itemize}\vspace{-7pt}\end{minipage} \\
 \midrule
 Order & The placement order of each component in $ \mathcal{T} $ & For three main components of $ \mathcal{T} $, i.e., Task Description (\textbf{TD}), Evaluation Rule (\textbf{ER}), and Input Content (\textbf{IC}), there are two commonly-used placement orders: \textbf{TD $\rightarrow$ ER $\rightarrow$ IC} \cite{zheng2023judging, liu-etal-2023-g} and \textbf{TD $\rightarrow$ IC $\rightarrow$ ER} \cite{liu-etal-2024-calibrating}.
 & \begin{minipage}[t]{\linewidth}\vspace{-7pt}\begin{itemize}[noitemsep, topsep=0pt, left=0pt]
     \item \textbf{TD $\rightarrow$ ER $\rightarrow$ IC}
     \item \textbf{TD $\rightarrow$ IC $\rightarrow$ ER}
     \item \textbf{ER $\rightarrow$ TD $\rightarrow$ IC}
     \item \textbf{ER $\rightarrow$ IC $\rightarrow$ TD}
     \item \textbf{IC $\rightarrow$ TD $\rightarrow$ ER}
     \item \textbf{IC $\rightarrow$ ER $\rightarrow$ TD}
\end{itemize}\vspace{0pt}\end{minipage} \\
\bottomrule
\end{tabular}}
\vspace{-2mm}
\caption{Factors of evaluation prompts, including their definition, common usage in previous works, and selection range considered in our work. The definitions of each component in the last row are described in Section \ref{factors}. } 
\vspace{-4.5mm}
\label{tab:factors}
\end{table*}
% \footnotetext[1]{Constructing high-quality references requires extensive manual annotation. We do not consider this usage in our study to ensure a fair comparison.}