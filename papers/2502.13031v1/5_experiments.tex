
\subsection{Experimental Setup}
\paragraph{Tasks and Datasets}
We evaluate HPSS on four pointwise grading evaluation tasks: Summeval \cite{fabbri-etal-2021-summeval} for text summarization, Topical-Chat \cite{gopalakrishnan2019topical} for dialogue generation, SFHOT / SFRES \cite{wen-etal-2015-semantically} for data-to-text generation, and HANNA \cite{chhun-etal-2022-human} for story generation. 
The Spearman ($\rho$) correlation coefficient between human judgments and LLM evaluations is adopted as the performance metric. 
We also validate HPSS on pairwise comparison benchmarks MT-Bench \cite{zheng2023judging}, AUTO-J \cite{li2024generative}, and LLMBar \cite{zeng2024llmbar}. 
The results are provided in Appendix \ref{appendix:pairwise}.
More details about benchmarks and metrics can be found in Appendix \ref{appendix:benchmarks} and \ref{appendix:correlation}.
Following HD-Eval \cite{liu-etal-2024-hd}, for each dataset, a 50\% proportion is held out for testing, while the rest is applied for validation.



\paragraph{Baselines}
We compare HPSS with three types of baselines: 
(1) \textbf{Non-LLM Evaluators}: This category includes BLEU-4 \cite{papineni-etal-2002-bleu}, \textsc{BertScore} \cite{bert-score}, 
% which are based on similarity calculations with the reference text, 
% and the pre-trained language model evaluator 
and \textsc{UniEval} \cite{zhong-etal-2022-towards}.
%under supervised fine-tuning. 
(2) \textbf{Human-Designed LLM Evaluators}: 
The prompting strategy from MT-Bench \cite{zheng2023judging} 
% propose a prompt template for pointwise grading, which 
stands as the starting point of searching.
G-Eval \cite{liu-etal-2023-g} integrates AutoCoT to enhance the performance of LLM evaluators. 
\citet{chiang-lee-2023-closer} explores various evaluation schemes. We use its best setting \textit{analyze-rate}, denoted as CloserLook, and attempt to further improve its performance by providing human evaluation examples (CloserLook + ICL). 
(3) \textbf{Automatic Prompt Optimization for LLM Evaluators}: 
We implement 4 strong prompt optimization baselines, including APE \cite{zhou2023large}, 
% a sentence-level prompt optimization approach with LLM-powered prompt paraphrasing, 
% with iterative Monte Carlo search, 
OPRO \cite{yang2023large}, 
%an evolutionary search method that employs LLM for proposing new strategies based on search history and corresponding scores, 
Greedy \cite{prasad-etal-2023-grips, zhou-etal-2023-survival} and Stepwise-Greedy. 
Greedy is a widely used iterative search algorithm, where random perturbations are applied to the current strategy to generate multiple candidates at each iteration, and the best-performing one is retained for the next iteration. 
Stepwise-Greedy sequentially optimizes each factor, choosing the best-performing value in each step.
More details can be found in Appendix \ref{appendix:baselines}.

\input{tables/main_results_2}
\paragraph{Models and Configurations}
We choose the representative closed-source model GPT-4o-mini \cite{openai2023gpt4} and open-source model Qwen2.5-14B-Instruct \cite{qwen2} as evaluation models. 
For G-Eval and CloserLook, we use their default settings (with 20 generation times
%per sample 
and the decoding temperature as 1.0). 
For other LLM evaluators, greedy search is used for reproducibility. 
We limit the computational budget to 71 evaluations on the validation dataset (including 21 evaluations during initiation in HPSS) for all prompt search methods (21 for Stepwise-Greedy as an exception, as their evaluation count is constant). 
Results of prompt search on Qwen2.5-14B-Instruct are averaged over 3 seeds and the standard deviation is provided.
While for GPT-4o-mini, we report the results of one seed due to the API cost.
More details including the cost of HPSS are in Appendix \ref{appendix:hpss}.



\subsection{Experimental Results}

\paragraph{Human Alignment}
The main results are presented in Table \ref{tab:main_results_1} and \ref{tab:main_results_2}. 
\textbf{Firstly}, with the same generation times, 
HPSS substantially improves the performance of LLM evaluators compared to the starting point (i.e., the prompting strategy from MT-Bench), resulting in an average 29.4\% relative improvement in Spearman correlation across various tasks and outperforming other Non-LLM and human-designed LLM evaluators by a large margin. 
Even with only 5\% of the generation times, HPSS remains superior to G-Eval and CloserLook.
Additional experiments in Appendix \ref{appendix:stronger} also demonstrate that the Qwen2.5-14B-Instruct evaluator with HPSS substantially surpasses the human-designed Qwen2.5-72B-Instruct evaluator.
%These results demonstrate the importance of our research problem. 
\textbf{Secondly}, HPSS achieves a significant performance gain over other automatic prompt optimization methods, showing its better adaptability to NLG evaluation tasks.
%for prompting strategy search tasks. 
\textbf{Finally}, HPSS brings consistent performance improvements to different evaluation models, highlighting its cross-model generalization capability. 
We also validate that integrating HPSS with inference-time methods such as self-consistency \cite{wang2022self} can further improve the performance of LLM evaluators in Appendix \ref{appendix:sc}. 

\begin{figure}[t]
\scriptsize
    \centering
    \includegraphics[width=1.0\linewidth]{figures/size.png}
    \vspace{-4mm}
    \caption{Average performance of Qwen2.5-14B-Instruct evaluator under different validation dataset sizes on Summeval and Topical-Chat.} 
    \vspace{-4mm}
    \label{fig:size}
\end{figure}

\paragraph{Ablation Study} In Table \ref{tab:main_results_1} and \ref{tab:main_results_2}, we provide an ablation study on key components of HPSS for the Qwen-2.5-14B-Instruct evaluator, 
including the mutation type \textit{exploitation}, 
the additional exploration term, 
and the entire heuristic-function-guided mutation mechanism. 
The results validate that all components contribute to the final performance of HPSS. 
Notably, removing the entire mechanism leads to the most significant performance degradation, highlighting its effectiveness.
Moreover, the performance gains from the heuristic search mechanism vary across different datasets, probably due to different search difficulties.
% Larger gains are observed in the Topical-chat, SFHOT, and HANNA, whereas smaller gains are found in Summeval and SFRES.


\paragraph{The impact of validation dataset size and search budget.} 
Figure \ref{fig:size} shows the performance of HPSS under different validation dataset sizes.
When validating with only 10\% of the human expert annotations, 
the performance of HPSS remains marginally off and superior to human-designed LLM evaluators, 
demonstrating its effectiveness in low-resource scenarios.
Figure \ref{fig:step} shows the performance of Qwen2.5-14B-Instruct evaluator under different search steps on Topical-Chat. 
While HPSS does not exhibit an advantage over baseline methods in the early stages of the search, it avoids getting trapped in local optima prematurely (which often appears in baseline methods) and ultimately converges into better solutions. 
We speculate that this can be attributed to two reasons: 
First, the mutation type \textit{exploitation} expands the search scope of HPSS; 
Second, the advantage estimations for each value become more accurate as the search progresses, enabling an effective search even after good solutions have been obtained. 


\begin{figure*}[t]
\scriptsize
    \centering
    \includegraphics[width=1.0\textwidth]{figures/step.png}
    \vspace{-5mm}
    \caption{Performance of Qwen2.5-14B-Instruct evaluator under different search steps on Topical-Chat. The first 21 steps for HPSS are in \textbf{Initialization}, which also serves as the initial search history for OPRO, causing the two methods to produce identical results during these steps. Post-convergence results are omitted.} 
    \label{fig:step}
    \vspace{-4mm}
\end{figure*}


\subsection{Analysis}
\paragraph{The generalizability of prompting strategies across datasets.}
Using Qwen2.5-14B-Instruct as the evaluation model, we directly apply the prompting strategies found by HPSS in Table \ref{tab:main_results_1} and \ref{tab:main_results_2} to other datasets. 
% The Spearman corrections with human judgments are shown in Table \ref{tab:across_datasets}.
As shown in Table \ref{tab:across_datasets}, these prompting strategies demonstrate strong generalizability across different datasets on the same evaluation aspects, 
as they still achieve significantly better evaluation performances than human-designed prompting strategies in most scenarios. 
We also observe that these prompting strategies show cross-aspect generalization of different datasets. 
However, the improvements in cross-aspect generalization are generally smaller than those in same-aspect generalization, compared to human-designed prompting strategies.
The cross-dataset generalizability of prompting strategies may further reduce the overhead and improve the practicality of HPSS.
% These results further demonstrate the practicality of HPSS.

\input{tables/across_datasets}

\paragraph{The advantages of different values.}
% \begin{CJK}{UTF8}{gbsn}\textcolor{red}{最优因素选取的特点分析 / Case Study}\end{CJK}
We calculate the mean and standard deviations of the advantages for each factor at the end of HPSS across different datasets and evaluation aspects. 
% As presented in 
Table \ref{tab:advantages} shows that the advantages exhibit considerable variance across tasks and evaluators, indicating the necessity of finding appropriate prompting strategies tailored to specific tasks and evaluators.
However, there are some common characteristics: 
%For instance, 
(1) A moderate scoring scale of 1-10 generally enhances evaluator performance, whereas a too coarse-grained scoring scale of 1-3 is less effective.
(2) Under greedy decoding, directly generating scores and employing human-written evaluation criteria without AutoCoT, Metrics, and Reference generally improves evaluation performance.
(3) Regarding the placement order of different components, positioning the task description at the beginning seems to yield better results, possibly because this arrangement is more logically coherent and helps evaluators concentrate more on sample information and evaluation criteria.
These findings provide insights into future evaluation prompt design.
We also provide a case study of HPSS in Appendix \ref{app:case}.
\input{tables/advantages}

