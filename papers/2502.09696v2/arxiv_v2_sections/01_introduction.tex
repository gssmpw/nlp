\begin{figure}[th!]
        \centering
        \includegraphics[width=\linewidth]{figures/plots/sota_comparison2.pdf}
        \vspace{-0.8cm} %
        \caption{\textbf{State of the art performance on public visual benchmarks.} Frontier LMMs score highly on many popular benchmarks, leaving little headroom. By comparison, our \benchmarkName proves impossible for current models, leaving maximum headroom. %
        }
        \label{fig:sota_comparison}
\end{figure}


\section{Introduction}
\label{sec:introduction}



Numerous works have demonstrated the broad capabilities of foundation models in language-based tasks, such as knowledge-based question answering \cite{hendrycks2020measuring}, information retrieval \cite{kamradt2023LLMTest}, and %
conducting end-to-end machine learning research with minimal guidance \cite{schmidgall2025agent}. However, when it comes to image-text multimodal tasks, the results for LMMs are more nuanced. Although possessing impressive multimodal abilities in some cases \cite{yang2023dawn}, a growing body of work indicates significant flaws in the visual interpretation and reasoning of LMMs. In particular, LMMs have been shown to struggle with low-level reasoning tasks such as counting and identifying points of intersection \cite{rahmanzadehgervi2024vision} or localising points on maps \cite{roberts2024charting}; they also appear to have inferior spatial cognition skills to animals \cite{ramakrishnan2024does}. 




A series of benchmarks have been proposed to measure and track the visual capabilities of LMMs in various domains \cite{ramakrishnan2024does, rahmanzadehgervi2024vision, padlewski2024vibe}. Despite falling short on visual interpretation, rapid progress has been made on these benchmarks, with new State of the Art (SotA) scores steadily eroding the benchmark headroom---the difference between the maximum possible and SotA scores (Fig \ref{fig:sota_comparison}). Fig. \ref{fig:benchmark_timelines} illustrates this observation, with increasing performance on several visual benchmarks within months. As the headroom on benchmarks is reduced, they typically become less informative and able to meaningfully differentiate  capabilities. Moreover, the longevity of a benchmark -- the timespan it is useful -- is correlated with difficulty\footnote[2]{Note, observing SotA scores on arbitrary benchmarks \textbf{A} and \textbf{B} of 35\% and 3\%, respectively, does not ensure \textbf{B} is harder than \textbf{A} (in the long run); just that it is more challenging for the \textit{current generation} of frontier models.}. Consequently, there is a pressing need for \textit{hard evals}---benchmarks and evaluations that are sufficiently challenging for contemporary models. 

\begin{figure}[t]
        \centering
        \includegraphics[width=\linewidth]{figures/plots/sota_progress2.pdf}
        \vspace{-1cm} %
        \caption{\textbf{Rapid progress was made on visual benchmarks last year.} Compiled from \cite{2023opencompass}.}
        \label{fig:benchmark_timelines}
\end{figure}

Among recently released models, there is a notable paradigm shift toward models that spend more time ``thinking,'' resulting in better-reasoned generation \cite{o1_reasoning}. Concretely, these models are encouraged to leverage \textit{test-time compute scaling} \citep{jones2021scaling,snell2024scaling} and spend more time to consider and refine their ``thoughts'' rather than naively sampling a single stream of tokens. Earlier approaches to test-time scaling focused on eliciting longer, more reasoned responses by conditioning generation on step-by-step reasoned examples \cite{wei2022chain}. Later methods search among multiple generated candidate answers or steps, selecting trajectories according to a specified strategy \cite{hf_ttcompute}. Scaling up the number of generations %
can be extremely costly, reducing the feasibility of evaluating models on large-scale benchmarks and incentivising the creation of lightweight benchmarks. 

Motivated by the \textit{desiderata} outlined above, we introduce \benchmarkName, a visual reasoning benchmark for LMMs. \benchmarkName is both (1) \textbf{lightweight} -- comprised of just \nquestions questions, and (2) \textbf{challenging} -- frontier models achieve a SotA score of 0.0\%. To our knowledge, this is the first benchmark that, at release, is completely impossible for current frontier models---positioning it to be well-suited to evaluate future models. Furthermore, being small-scale, \benchmarkName is more conducive to approaches leveraging test-time scaling. Correctly answering questions in \benchmarkName requires complex reasoning over one or more images, and covers numerous domains and reasoning categories, as well as, both natural and synthetic images. Therefore, \benchmarkName is also (3) \textbf{diverse} -- questions are bespoke and manually made by a pool of creators through a process that encouraged creativity. Focusing on a small set of well-reviewed questions also ensures \benchmarkName is (4) \textbf{high-quality and noise-free} -- label errors in popular benchmarks \cite{northcutt2021pervasive} limit headroom and reduce evaluation reliability.



We evaluate \nmodelsevaluated frontier LMM baselines on \benchmarkName, finding it to be a considerable challenge. Results are reported using both pass@1 and pass@k evaluation. Following a similar approach to \cite{xu2024theagentcompanybenchmarkingllmagents}, to differentiate model performance we decompose the questions into steps that are useful and related to the final answer and evaluate performance on these steps. We carry out rigorous error analysis, including qualitative examples of model answers, and compare the prevalence of visual errors versus reasoning errors. To summarise, our core contributions are 3-fold:
\begin{itemize}
    \item We introduce \benchmarkName, a lightweight, challenging visual reasoning benchmark for LMMs consisting of \nquestions hand-crafted questions and \nsubquestions subquestions.%
    \item We evaluate \nmodelsevaluated models on \benchmarkName, all of which%
    score 0.0\% across the board on the main questions.
        \item Through fine-grained error analysis, we identify recurring failure modes across various models. Our findings reveal common patterns of mistakes, primarily related to the visual interpretation of inputs. These errors prevent models from arriving at the correct final answer, regardless of their reasoning capabilities.
\end{itemize}









