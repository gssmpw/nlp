\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Models}
\label{sec:baselines}

We benchmark \nmodelsevaluated proprietary and open-weight LMM baselines on \benchmarkName. Given the challenging nature of our benchmark and multi-step structure of our questions, we focus our evaluation on leading models, including \textit{reasoning} models that leverage test-time compute scaling, such as o1 and o1 pro \cite{openai2024o1}, Gemini 2 Flash Thinking \cite{google2024gemini2flash} and QVQ \cite{qvq-72b-preview}. The other proprietary models we evaluate include Gemini 2, 1.5 Pro and 1.5 Flash \cite{reid2024gemini}, and 1 Pro Vision \cite{team2023gemini}, Claude 3 Opus, Sonnet and Haiku \cite{anthropic2024claude}, as well as, Claude 3.5 Sonnet and Claude 3.5 Sonnet v2 \cite{sonnet35}, Reka Edge \cite{team2024reka}, GPT-4o and GPT-4o mini \cite{gpt4o}.
We also evaluate the following strong open-weights baselines: Llama 3.2 90B~\cite{dubey2024llama}, Pixtral-Large~\cite{agrawal2024pixtral}, \QwenLong~\cite{wang2024qwen2},  and \NVLMLong \cite{dai2024nvlm}.



\subsection{Prompting and Hyperparameters}

For all baselines, we use a simple conversational prompt containing the question text, a zero-shot CoT \cite{kojima2022large} phrase and instructions to enclose the final answer in `$\{\}$'. The exact structure of the prompt was iteratively refined during initial experimentation to encourage chains of thought while ensuring consistent final answer structure. Following established practices (e.g., \cite{zhang2024humaneval}), we consider two settings for generating responses. %
For reproducibility, we use \textbf{greedy decoding} to sample a single (near) deterministic response for each question; to make the output as deterministic as possible, we set the temperature to 0 and specify random seeds. To leverage the creativity of the models, we also sampled \textit{k} responses for each question using \textbf{stochastic decoding}, in which we approximate the models' default hyperparameters by setting the temperature and top p hyperparameters to 0.7 and 0.95, respectively. We set the maximum output tokens to the upper limit for each model. More details can be found in the \nameref{sec:appendix}.









\subsection{Evaluation}

As previous work has shown \cite{roberts2024grab}, the instruction-following capabilities of chat and instruction-tuned models are sufficiently reliable for answers to be parsed without requiring an LLM. Inspection of model responses on our benchmark questions corroborated this. Therefore, we took the following approach to answer evaluation: (1) we automatically parse the answer text within the curly braces, and (2) evaluate it against the ground truth via exact matching. Exceptions to this included o1 pro and QVQ, which failed to consistently follow output instructions. To evaluate responses from these models, we used Gemini 2 Flash to initially parse the final answer and then compare to the ground truth. To ensure a realistic and consistent evaluation reasoning chain errors were treated strictly---if the reasoning chain exceeded the maximum number of output tokens -- and hence did not reach a final answer -- the response was evaluated as incorrect. Moreover, some API services return no completion tokens if the output is too long; these cases were also evaluated as incorrect. 







\paragraph{Main questions.} We use accuracy as our metric to evaluate the \nquestions \benchmarkName main questions. As each question is evaluated with a binary outcome (correct or incorrect), we calculate the accuracy as: $a = n / N,$ where $n$ questions are answered correctly from a total of $N$ questions. As most questions in \benchmarkName require a numeric answer, we considered assigning partial credit based on proximity to the ground truth or error with respect to a baseline model answer \cite{kazemi2024remi}. However, given the diversity of the answer space, finding an appropriate distance-based metric is not possible.

\paragraph{Subquestions.} To differentiate model performance, we additionally evaluate the `easier' subquestions derived from each main question. There are on average 3.3 subquestions for each main question; all are also evaluated with a binary outcome. Specifically, we compute the mean accuracy on the subquestions as: \begin{math}a_{sq} = \frac{1}{N}\sum_{i=1}^{N} a_i = \frac{1}{N}\sum_{i=1}^{N} \frac{n_i}{N_i},\end{math} where $n_i$ subquestions are answered correctly out of $N_i$ for the $i$th main question. As suggested by \citet{miller2024adding}, we also report an estimate of standard error using the central limit theorem (C.L.T): \begin{equation*}
SE_{CLT} = \sqrt{\left(\frac{1}{N-1} \sum_{i=1}^{N}{(a_i - a_{sq})^2}\right)/N}.
\end{equation*}

\paragraph{pass@k and k/k reliability.} Rather than only considering single responses, we also evaluate using the pass@k metric. Specifically, we generate $n$ responses, sample $k$ (where $n\geq k$), and evaluate a given question as correct if \textit{any} of the sampled responses are correct. To additionally gauge model consistency and reliability, we use the k/k reliability metric \cite{OpenAI2024o1pro}, in which we generate $n$ responses and sample $k$ (where $n\geq k$); a given question is evaluated as correct if \textit{all} of the sampled responses are correct.

\subsection{Inference}

We access a number of the models via public APIs. Specifically, we evaluate the Claude, Gemini and Llama models via the VertexAI API \cite{vertexaiapi}; Reka Edge via the Reka API \cite{rekaapi}; o1, GPT-4o and GPT-4o mini via the OpenAI API \cite{openaiapi}; and Pixtral via the Mistral API \cite{mistralapi}. We host three models---QVQ, \QwenShort and \NVLMShort---using weights available via the Hugging Face Transformers library \cite{wolf-etal-2020-transformers}. For inference, we use vLLM~\cite{kwon2023efficient}, utilising four A100 GPUs with tensor parallelization, without any quantization. We access o1 pro using the ChatGPT interface and leverage a semi-automated script to process requests and save responses. This model has a daily limit, making it infeasible to process the entire benchmark in a single day, necessitating distributing the requests across different days. When evaluating multi-image questions, our default approach was to input images separately in a single prompt. However, for models evaluated via vLLM and Llama 3.2 we collate the question images into a single collage.













