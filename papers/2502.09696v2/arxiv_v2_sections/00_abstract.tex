\begin{abstract}
Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model progress. To address this, there is a pressing need for difficult benchmarks that remain relevant for longer. We take this idea to its limit by introducing \benchmarkName---a lightweight visual reasoning benchmark that is entirely impossible for contemporary frontier LMMs. Our benchmark consists of \nquestions manually curated questions and \nsubquestions less difficult subquestions. We evaluate \nmodelsevaluated LMMs on \benchmarkName, all of which score 0.0\%, and rigorously analyse the errors. To encourage progress in visual understanding, we publicly release \benchmarkName at \url{https://zerobench.github.io/}.%
\end{abstract}
