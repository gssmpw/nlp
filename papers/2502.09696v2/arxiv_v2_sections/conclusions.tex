\section{Conclusions}
\label{sec:conclusion}

We introduce \benchmarkName, a set of \nquestions challenging visual reasoning questions that are impossible for current frontier LMMs. The high-quality questions in our benchmark are manually-curated and designed to exceed the capabilities of contemporary models. We calibrate the difficulty of our questions by adversarially filtering from a pool of candidates, only selecting those that current models cannot answer correctly. This results in a lightweight benchmark that can be evaluated efficiently and inexpensively -- an increasingly important consideration with the rise of reasoning models that leverage test-time scaling. To differentiate model performance, we construct a larger set of \nsubquestions subquestions by decomposing the main questions into smaller, easier questions. We comprehensively evaluate \nmodelsevaluated  LMMs on both question sets in pass@1, pass@5 and 5/5 reliability settings. A key motivation for this work was to create an evaluation challenging enough to counter the rate of model development that has been rapidly eroding the headroom on popular visual benchmarks -- we hope \benchmarkName will serve this purpose, at least for the time being.

