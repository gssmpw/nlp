

\section{Experimental Results}
\label{sec:experiments}

\begin{table*}[t]
\centering
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{4}{c}{\textbf{Main questions} (\nquestions)} 
 & \multicolumn{2}{c}{\textbf{Subquestions} (\nsubquestions)} \\
 & k/k [\%] \textcolor{gray}{(n)} & \multicolumn{2}{c}{pass@k [\%] \textcolor{gray}{(n)}} &  & pass@k [\%] \textcolor{gray}{($SE_{CLT}$)} & Num. correct \\
\textbf{Models} & k=5 & k=1 & k=5 &  & k=1 & k=1 \\
\midrule

\multicolumn{7}{c}{\textcolor{gray}{\textbf{Reasoning LMMs}}} \\
o1 pro$^{\diamond, \mathsection}$ & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & - &  &22.40 (\textcolor{gray}{2.48})  &  75  \\
o1$^\diamond$ & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} &  & 20.21 (\textcolor{gray}{2.33}) & 68 \\
Gemini 2 Flash Thinking & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & \textbf{5.0 \textcolor{gray}{(5)}} &  & 20.51 (\textcolor{gray}{2.60}) & 69 \\
QVQ & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 4.0 \textcolor{gray}{(4)} &  & 20.47 (\textcolor{gray}{2.38}) & 70  \\
\midrule

\multicolumn{7}{c}{\textcolor{gray}{\textbf{Proprietary LMMs}}} \\
GPT-4o & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} &  & 19.60 (\textcolor{gray}{2.37}) & 67 \\
GPT-4o mini & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 2.0 \textcolor{gray}{(2)} &  & 16.58 (\textcolor{gray}{2.41}) & 54 \\
Gemini 2 Flash & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 3.0 \textcolor{gray}{(3)} &  & 23.24 (\textcolor{gray}{2.85}) & 75 \\
Gemini 1.5 Pro & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 2.0 \textcolor{gray}{(2)} &  & 20.88 (\textcolor{gray}{2.47}) & 74 \\
Gemini 1.5 Flash & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 2.0 \textcolor{gray}{(2)} &  & 17.87 (\textcolor{gray}{2.41}) & 63 \\
Gemini 1 Pro Vision & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 2.0 \textcolor{gray}{(2)} &  & 12.36 (\textcolor{gray}{2.08}) & 46 \\
Claude 3.5 Sonnet v2 & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 2.0 \textcolor{gray}{(2)} &  & \textbf{25.50 (\textcolor{gray}{2.67})} & \textbf{82} \\
Claude 3.5 Sonnet & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 1.0 \textcolor{gray}{(1)} &  & 20.71 (\textcolor{gray}{2.48}) & 72 \\
Claude 3 Opus & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} &  & 15.10 (\textcolor{gray}{2.16}) & 45 \\
Claude 3 Sonnet & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 1.0 \textcolor{gray}{(1)} &  & 16.08 (\textcolor{gray}{2.25}) & 49 \\
Claude 3 Haiku & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} &  & 12.27 (\textcolor{gray}{2.05}) & 41 \\
Reka Edge & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} &  & 3.74 (\textcolor{gray}{0.96}) & 13 \\
\midrule

\multicolumn{7}{c}{\textcolor{gray}{\textbf{Open-weight LMMs}}} \\
Llama 3.2 90B & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} &  & 13.26 (\textcolor{gray}{1.92}) & 48 \\
\QwenLong & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 2.0 \textcolor{gray}{(2)} &  & 13.00 (\textcolor{gray}{2.32}) & 43  \\
\NVLMLong & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 1.0 \textcolor{gray}{(1)} &  & 14.91 \textcolor{gray}{(2.36)} &  51 \\
\PixtralLong & 0.0 \textcolor{gray}{(0)} & 0.0 \textcolor{gray}{(0)} & 3.0 \textcolor{gray}{(3)} &  & 18.68 \textcolor{gray}{(2.26)}  & 62  \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\caption{\textbf{Overall results on \benchmarkName (\texttt{v2})}. We report pass@1 using greedy decoding and k/k reliability and pass@5 using stochastic decoding. For a set of k sampled responses, pass@k is evaluated as correct if at least one response is correct; k/k reliability is evaluated as correct if all responses are correct. $^\diamond$all responses are sampled using default model settings. $^\mathsection$as there is no publicly available API, o1-pro was only evaluated on \benchmarkName \texttt{v1}.}
\label{tab:main_results}
\end{table*}

\subsection{Main Results}

We evaluate \nmodelsevaluated LMMs on the \benchmarkName \texttt{v2} main questions and subquestions and present the results in Tab. \ref{tab:main_results}. We report results from a single response generated with greedy decoding (pass@1) and sampling from 5 responses generated non-deterministically (pass@5 and 5/5 reliability).

\paragraph{\benchmarkName is impossible for contemporary LMMs.} In a reproducible setting, we find all models score 0\% pass@1 on our benchmark, failing to answer a single main question correctly. Given our curation approach, this outcome is expected and proves the questions in \benchmarkName are beyond the reach of current frontier models.

\paragraph{Some questions are within reach.} Considering the pass@5 setting, we find non-zero performance, with most models answering 1-3 questions correctly on at least one sampling. The highest-performing model is Gemini 2 Flash Thinking, which attains a pass@5 score of 5\% (5 questions correct); QVQ, Gemini 2 Flash and Pixtral-Large also perform relatively well, correctly answering 3 or 4 questions. %
Although these pass@5 scores are very low, they show that some questions are slightly easier, and are just within the limits of the capabilities of some models. However, when considering the 5/5 reliability metric for the same responses the results are all 0\%. This indicates low consistency---despite returning a correct answer in at least one sampling, no questions were consistently answered correctly across all 5 samplings, by any model.


\paragraph{Subquestions differentiate model performance.} As intended, the subquestions prove less challenging for the models, which all attain non-zero scores. Moreover, there is sufficient signal in the subquestion scores to clearly differentiate model performance and form a hierarchy. Claude Sonnet 3.5 v2 is the best-performing model after scoring 25.50\% pass@1 and answering 82 out of \nsubquestions subquestions correctly. 
The performance lead is relatively small however, with the scores of o1 pro and Gemini 2 Flash also clustering near the top of the hierarchy within two percentage points. Although significantly less challenging than the main questions, on the whole, the models still struggle to correctly answer these subquestions, with the vast majority being too difficult. Reasoning models often generate an extended chain of thought during inference, allowing them to explore multiple pathways before arriving at a final solution.
However, on \benchmarkName, there appears to be no clear advantage of such models over traditional models. In cases where the same number of subquestions are correctly answered, the pass@k scores may differ as the main questions have different numbers of subquestions resulting in minor differences in weightings.




\paragraph{Proprietary vs. Open-weight.} On the main questions, performance remains poor across both open- and closed-source models, with no clear distinction. However, a comparison of the subquestion scores reveals a substantial gulf in performance %
with the leading open-source model (QVQ 20.47\%) trailing the SotA (Claude 3.5 Sonnet v2 25.50\%) by 5 percentage points.

\subsection{Completion Tokens}
To compare the efficiency of the evaluated baselines during inference, we record 
the number of output or completion tokens used for each request. In Tab. \ref{tab:token_usage}, we report the number of tokens used by each model, averaged across all questions. A key observation is the significantly higher number of completion tokens used by the reasoning models, with o1 and QVQ averaging  7.5k and 3.1k tokens, respectively, to answer each question compared to the few hundred tokens used by the other LMMs. Yet, as Tab. \ref{tab:main_results} shows, these additional tokens do not lead to improvements in performance. Another observation is that most models use approximately half the number of completion tokens when answering the subquestions. We consider this a result of the easier nature of the subquestions, which require shorter chains of reasoning to reach the correct answer. The estimated completion token costs per \benchmarkName question shows that the vast majority of models can be evaluated relatively inexpensively on our benchmark. However, at nearly \textbf{\$0.5 per question}, the cost of evaluating frontier reasoning models, such as o1, becomes prohibitive---this clearly demonstrates the benefit and necessity of ensuring new benchmarks are \textit{lightweight}, especially if multiple responses are sampled.







\begin{table}[t]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lrrrr}
\toprule
& \multicolumn{4}{c}{\textbf{Avg. per question}} \\
& \multicolumn{2}{c}{Main questions} & \multicolumn{2}{c}{Subquestions} \\
\textbf{Model} & \#tkns & Cost (\$)$^*$ & \#tkns & Cost (\$)$^*$ \\
\midrule
\multicolumn{5}{c}{\textcolor{gray}{\textbf{Reasoning LMMs}}} \\
o1 pro$^\diamond$ & - & - & - \\
o1$^\diamond$ & 7496 & 0.472 & 3891 & 0.245\\
Gemini 2 Flash Thinking$^{**}$  & 548  & - & 237 & - \\
QVQ & 3147 & 0.002 & 1822 & 0.001 \\
\midrule

\multicolumn{5}{c}{\textcolor{gray}{\textbf{Proprietary LMMs}}} \\
GPT-4o  & 452 & 0.005 & 229 & 0.002 \\
GPT-4o mini & 897 & $<$0.001 & 214 & $<$0.001\\
Gemini 2 Flash & 1041 & 0.011 & 490 & 0.005 \\
Gemini 1.5 Pro & 272 & 0.002 & 115 & $<$0.001 \\
Gemini 1.5 Flash & 279 & $<$0.001 & 123 & $<$0.001\\
Gemini 1 Pro Vision  & 228 & $<$0.001 & 104 & $<$0.001\\
Claude 3.5 Sonnet v2 & 257 & 0.004 & 164 & 0.003 \\
Claude 3.5 Sonnet & 296 & 0.005 & 217 & 0.003 \\
Claude 3 Opus & 267 & 0.021 & 169 & 0.013 \\
Claude 3 Sonnet & 279 & 0.004 & 176 & 0.003 \\
Claude 3 Haiku & 321 & $<$0.001 & 135 & $<$0.001\\
Reka Edge$^{**}$ & 478 & - & 196 & - \\
\midrule

\multicolumn{5}{c}{\textcolor{gray}{\textbf{Open-weight LMMs}}} \\
Llama 3.2 90B & 665 & $<$0.001 & 260 & $<$0.001\\
\QwenLong &  472 & $<$0.001 & 481 &  $<$0.001 \\
NVLM-D-72B$^{**}$ & 814 & - & 155  & - \\
Pixtral-Large$^{**}$   & 579 & - & 384 & - \\
\bottomrule
\end{tabular}}
\vspace{-1mm}
\caption{\textbf{Average per question cost and number of \textit{completion} tokens generated during greedy decoding.} $^*$calculated based on AI/ML API pricing \cite{aiml_api};  $^{**}$cost data unavailable; $^\diamond$responses sampled using default model settings.}
\label{tab:token_usage}
\end{table}








\subsection{Error Analysis}

We undertake fine-grained inspection of the model outputs on both the main and subquestions to build intuition of model strengths and weaknesses. We derive several insights from this qualitative analysis. One clear takeaway can be found in the distribution of error types, which is heavily skewed towards visual interpretation errors rather than logical reasoning or knowledge-based errors. This supports our claim that \benchmarkName is a \textit{visual} reasoning benchmark, and broadly follows the findings of other studies \cite{rahmanzadehgervi2024vision, ramakrishnan2024does}. However, a comparable study \cite{kazemi2024remi} carried out on the ReMI benchmark found the opposite to be true---that reasoning errors were the more frequent type. It is worth noting that the study evaluated the \textit{previous} generation of LMMs (with Gemini 1.5 Pro as SotA) and the differences in findings broadly indicate that subsequent model development has focused on reasoning capabilities, which have outpaced the development of visual capabilities. While this is likely the key factor, it is worth also considering that the \benchmarkName questions might have harder visual elements.

In Fig. \ref{fig:failure_examples} we illustrate recurrent visual interpretation errors on \benchmarkName subquestions, such as incorrectly counting objects, an inability to `see' fine-grained detail or accurately extract information, and difficulties understanding spatial relations. More examples can be found in the \nameref{sec:appendix_failure_modes}.















    

    
\begin{figure*}[t]
\centering   
\newcommand{\tablespace}{
    \vspace{1cm}
    \centering
}

\begin{minipage}[t][8cm][t]{0.33\textwidth}
\begin{questionboxmaintext}[title=\footnotesize{Resolving fine-grained detail}]
\footnotesize{\textbf{\textcolor{darkblue}{Question:}} How many cells in the first column %
contain circles that are completely disjoint?}\\
\vspace{2mm}
\centering
\includegraphics[width=\textwidth,trim=0cm 0cm 0cm 0cm,clip]{figures/qualitatives/fine-grained/circle-grid.png}
\par
\vspace{4mm}
\begin{tabular}{c|c|c|c}
\gptlogo & \claudelogo & \geminilogo  & \qwenlogo \\
\hline
5 & 7 & 0 & 5 \\
\hline
\textcolor{ForestGreen}{\cmark} & \textcolor{red}{\xmark}  & \textcolor{red}{\xmark} & \textcolor{ForestGreen}{\cmark}\\
\end{tabular}
\end{questionboxmaintext}
\end{minipage}
\hfill
\begin{minipage}[t][8cm][t]{0.33\textwidth}
\begin{questionboxmaintext}[title=\footnotesize{Counting}]
\footnotesize{\textbf{\textcolor{darkblue}{Question:}} How many books are there?}\\
\vspace{2mm}
\centering
\includegraphics[width=0.85\textwidth,trim=0cm 0cm 0cm 0cm,clip]{figures/qualitatives/counting/books2.jpeg} 
\par
\vspace{1mm}
\begin{tabular}{c|c|c|c}
\gptlogo & \claudelogo & \geminilogo  & \qwenlogo \\
\hline
17 & 13 & 15 & 17 \\
\hline
\textcolor{ForestGreen}{\cmark} & \textcolor{red}{\xmark}  & \textcolor{red}{\xmark} & \textcolor{ForestGreen}{\cmark}\\
\end{tabular}
\end{questionboxmaintext}
\end{minipage}
\hfill
\begin{minipage}[t][8cm][t]{0.33\textwidth}
\begin{questionboxmaintext}[title=\footnotesize{Solving visual puzzle}]
\footnotesize{\textbf{\textcolor{darkblue}{Question:}} Once the puzzle pieces are assembled, what overall shape do they form ?}\\ 
\vspace{5mm}
\centering
\includegraphics[width=\textwidth,trim=0cm 0cm 0cm 0cm,clip]{figures/qualitatives/heart.jpg}
\par
\vspace{1mm}
\begin{tabular}{c|c|c|c}
\gptlogo & \claudelogo & \geminilogo  & \qwenlogo \\
\hline
\smallcircle &  \smallcircle &  \smallrect & \smallcircle \\
\hline
 \textcolor{red}{\xmark}  & \textcolor{red}{\xmark}  & \textcolor{red}{\xmark}  & \textcolor{red}{\xmark}  \\
\end{tabular}
\end{questionboxmaintext}
\end{minipage}
\vspace{-4mm}
\caption{\textbf{Sample recurring visual interpretation errors on \benchmarkName subquestions.} These examples demonstrate that despite attaining respectable scores on the subquestions  -- indicating a high level of competency -- models such as Claude 3.5 Sonnet v2 and Gemini 2.0 Flash Thinking struggle on these relatively simple resolving and counting tasks. Full-resolution images were used for evaluation. \gptlogo o1 pro --- \claudelogo Claude 3.5 Sonnet v2 --- \qwenlogo QVQ --- \geminilogo Gemini 2.0 Flash Thinking.}
\label{fig:failure_examples}
\end{figure*}










\section{Future Outlook}

\textbf{Evaluating new models.} 
Our initial experiments showed that when presented with a candidate set of questions near the limit of current capabilities, most models could correctly answers a handful. We then adversarially pruned these questions from \benchmarkName, resulting in 0\% scores in a deterministic pass@1 setting. Hence, there is a reasonable likelihood of low non-zero scores when evaluating new models as they were not involved in the adversarial selection, even if their capabilities are not superior.

\textbf{Timeline to saturation.} 
An interesting research question to consider in light of the results is the timeline for which we expect progress to be made on \benchmarkName %
and what form it will take: will model performance increase incrementally or will the capabilities to answer specific types of questions emerge, resulting in larger steps? Many questions in \benchmarkName require fine-grained visual reasoning. Due to computational constraints, contemporary LMMs necessarily downsample input images to lower resolutions, rendering a non-trivial portion of questions impossible currently. Therefore, a breakthrough that allows for significantly higher resolution input could be a key factor towards realising significant gains on \benchmarkName.

\textbf{Creating difficult questions.} 
A perennial problem when creating benchmarks alongside rapid model development progress is ensuring questions are difficult enough for the latest models; this challenge is amplified when trying to create an `impossible' benchmark, such as \benchmarkName. Despite intentionally designing questions to be beyond the reach of current models, many had to be iteratively refined and made harder. Creating suitably difficult questions for future benchmarks is becoming non-trivial. Moreover, verifying the quality (correctness and answerability) becomes increasingly difficult when human verification requires a non-trivial amount of time per question. We suggest our \textit{community red teaming} (\S\ref{sec:red_team}) approach as a useful option for improving the quality of future benchmarking.



