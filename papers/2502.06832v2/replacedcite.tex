\section{Related Work}
MoE has long been explored in the machine learning community as a method for tackling complex tasks by combining specialized expert networks ____. Each expert focuses on certain aspects of the data, and their outputs are combined through a weighted sum determined by a gating mechanism or router ____. A notable advancement is the sparsely gated MoE ____, which activates only a subset of experts based on a routing mechanism, allowing conditional computation and enabling models to scale parameters independently of computational cost ____. This approach has been successfully applied in Natural Language Processing ____ and Computer Vision ____. 

Despite their widespread application, there has been limited research on the robustness of MoE, especially in adversarial settings.Small, carefully crafted perturbations, known as adversarial examples, can cause deep neural networks to make incorrect predictions____. Defenses against such attacks often rely on adversarial training methods based on min-max optimization ____. In the context of MoEs, only a few initial studies have begun exploring adversarial robustness with obvious limitations. The first work focused on Vision Transformers (ViTs) with MoE structures, examining the relationship between model capacity and robustness, only considering traditional adversarial training to robustify the model ____. Another study investigated adversarial robustness of MoEs with a method only working on convolutional neural networks ____. Moreover, these methods sacrifice standard accuracy for robustness, limiting their practical applications. Another line of related works is ensemble methods, which combine predictions from multiple models. Ensembles can improve robustness by aggregating predictions from multiple models, reducing the impact of individual vulnerabilities ____. While MoE models share conceptual similarities with ensembles by leveraging multiple sub-models, they differ fundamentally due to their dynamic routing mechanism, where a router assigns inputs to specific experts rather than combining outputs from all experts. This distinction necessitates tailored approaches for enhancing MoE robustness. Our work introduces a comprehensive framework to robustify MoEs while optimizing the balance between robustness and accuracy.