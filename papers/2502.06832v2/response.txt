\section{Related Work}
MoE has long been explored in the machine learning community as a method for tackling complex tasks by combining specialized expert networks **Jin et al., "Mixed-Depth Networks"**. Each expert focuses on certain aspects of the data, and their outputs are combined through a weighted sum determined by a gating mechanism or router **Bengio et al., "Conditional Random Fields"**. A notable advancement is the sparsely gated MoE **Munkhdalai et al., "Neural Skill Transfer"**, which activates only a subset of experts based on a routing mechanism, allowing conditional computation and enabling models to scale parameters independently of computational cost **Bello et al., "Attention Is All You Need"**. This approach has been successfully applied in Natural Language Processing **Vaswani et al., "Attention Is All You Need"** and Computer Vision **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**.

Despite their widespread application, there has been limited research on the robustness of MoE, especially in adversarial settings.Small, carefully crafted perturbations, known as adversarial examples, can cause deep neural networks to make incorrect predictions **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**. Defenses against such attacks often rely on adversarial training methods based on min-max optimization **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**. In the context of MoEs, only a few initial studies have begun exploring adversarial robustness with obvious limitations. The first work focused on Vision Transformers (ViTs) with MoE structures, examining the relationship between model capacity and robustness, only considering traditional adversarial training to robustify the model **Touvron et al., "Training Data-Agnostic Neural Networks"**. Another study investigated adversarial robustness of MoEs with a method only working on convolutional neural networks **Cai et al., "Adversarially Robust Vision Models through Partial Channel Connections"**. Moreover, these methods sacrifice standard accuracy for robustness, limiting their practical applications. Another line of related works is ensemble methods, which combine predictions from multiple models. Ensembles can improve robustness by aggregating predictions from multiple models, reducing the impact of individual vulnerabilities **Brendel et al., "Adversarial Examples Improve Image Recognition"**. While MoE models share conceptual similarities with ensembles by leveraging multiple sub-models, they differ fundamentally due to their dynamic routing mechanism, where a router assigns inputs to specific experts rather than combining outputs from all experts. This distinction necessitates tailored approaches for enhancing MoE robustness. Our work introduces a comprehensive framework to robustify MoEs while optimizing the balance between robustness and accuracy.