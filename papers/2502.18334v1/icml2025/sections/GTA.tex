\section{Test-Time Structural Alignment  (\proj)}


Motivated by our theoretical analysis, we propose \proj to address GTTA in a principled way. % mitigate variations in neighborhood information for GTTA principally.
To address neighborhood shift, \proj first conducts neighborhood alignment via weighted aggregation and then properly balances self and neighboring node information combination based on the SNRs of these representations. Lastly, as a generic approach for  neighborhood shift, \proj can be combined with non-graph TTA methods to refine the decision boundary to address the remaining feature and label shift. 

% \hans{neighborhood shift: difference compared to pairalign
% 1.compute inverse (1st order)
% 2.trainable $\rightarrow$ TTA is not trainable $\rightarrow$ adjust classifier, SNR (2nd order)
% 3.stable usage $\rightarrow$ why gamma not accurate $\rightarrow$ entropy
% }


\subsection{Neighborhood Alignment}
\label{subsec:1stalign}


Neighborhood shift alters the node label ratios in the aggregated neighborhood information, causing the shift in the GNN-encoded representations. 
In training-time GDA, PairAlign \cite{liu2024pairwise} leverages such a technique by assigning edge weights to align the distribution of the source neighborhood with the target domain.

%\pan{the original version of the next paragraph}

Inspired by this idea, for GTTA, we aim to achieve a similar goal but in a different direction. Based on Theorem \ref{theory:decomposeerror}, we align the target neighborhood distribution with the source domain, leveraging the fact that the pre-trained model is optimized for the source distribution. Specifically, the neighborhood distribution determines the ratio of messages passed from a neighboring class $j$ to center class $i$. To adjust for distributional differences, this ratio can be rescaled by assigning weights to edges from class $j$ to class $i$, effectively acting as an up/down-sampling mechanism during message aggregation. To ensure that message aggregation in the target domain aligns with the expected behavior in the source domain, \proj incorporates the following adjustment:










% This ratio can be rescaled by assigning weights on $j$ to $i$ edges, which can be interpreted as up/down-sampling of messages during message aggregation.  

% Hence, \proj incorporates defined as follows to make the target domain message aggregation match the source domain in expectation:


\begin{definition}
Let $\probt(Y_v=j|Y_u=i,v\in \neighbor_u)>0, \forall i,j \in \gY$, we have $\mgamma \in \R^{|\gY|\times|\gY|}$ as:
% \label{def:gamma}
\begin{equation} 
    [\mgamma]_{i,j} = \frac{\probs(Y_v=j|Y_u=i,v\in \neighbor_u)}{\probt(Y_v=j|Y_u=i,v\in \neighbor_u)}, \: \forall i,j \in \gY
\label{eq:gamma_ratio}
\end{equation}
\end{definition}
\vspace{-2mm}

% Though it may seem trivial that applying the same alignment strategy with the ratio is simply the inverse of the ratio used in GDA.
To estimate $\mgamma$, we assume that the source summary statistics $\probs(Y_v=j|Y_u=i,v\in \neighbor_u)$ are recorded and available at test time; otherwise, alignment would not be feasible. Storing  $\probs(Y_v=j|Y_u=i,v\in \neighbor_u)$ incurs minimal cost, as it is merely an $|\gY|\times|\gY|$ matrix. Beyond this, no additional information from the source domain is required. For $\probt(Y_v=j|Y_u=i,v\in \neighbor_u)$, we estimate it based on target pseudo labels. Note that PairAlign~\cite{liu2024pairwise} enhances estimation accuracy that relies pseudo labels~\cite{liu2023structural} by leveraging a least-squares constrained optimization. However, in GTTA, the absence of source graphs and the potential need for real-time adaptation render this approach impractical.




While it seems direct to apply a similar alignment strategy by using the inverse of the ratio employed in PairAlign, assigning $\gamma$ requires knowledge of node labels, which is missing in the targe graph, making it challenging in GTTA.


\paragraph{Reliable assignment of $\mgamma$.} 
The ratio $\gamma$ should be assigned to edge weights based on the label pairs of the central and neighboring nodes. 
In training-time GDA, this assignment is straightforward as it relies on the ground-truth labels of the source graph. However, in GTTA, this information is unavailable. A naive approach is to use target pseudo labels, but this often results in significant mismatches. \proj addresses this by quantifying the uncertainty of target pseudo labels~\cite{zhang2019bayesian,stadler2021graph,hsu2022graph}. In particular, TSA assigns $[\mgamma]_{i,j}$ only to node pairs $v\rightarrow u$ where both of their soft target pseudo labels $\hat{y}$ have low entropy $H(\hat{y})=-\sum_{i\in \gY}[\hat{y}]_i\ln([\hat{y}]_i)\leq\rho_1 \cdot \ln ( |\gY|)$. Here, $\rho_1$ is a hyperparameter and $ \ln( |\gY|)$ is the maximum entropy in $|\gY|$ class prediction. In Fig.\ref{fig:pa_vis} (a), nodes with low-entropy soft predictions are more reliable, resulting in higher accuracy after the assignment of $\mgamma$. 

% In GTTA, the assignment is challenging as the label is missing when assigning $[\mgamma]_{i,j}$ to the target graph.
% Furthermore,  $\probt(Y_v=j|Y_u=i,v\in \neighbor_u)$ can not be directly computed without node labels.
% The bootstrapping ratio should be assigned to edge weight based on the center and neighboring node label pairs. 
%Fig. \ref{fig:pa_vis} (a) demonstrates the challenge of correctly assigning $\gamma$ to edge weight in GTTA.
%The (top) represents the assignment under neighbor shift the (bottom) indicates an structure shift.
% An additional label shift also lowers its assigned accuracy.

% The challenge arises when estimating $\probt(Y_v=j|Y_u=i,v\in \neighbor_u)$, where no ground-truth labels are available. We may adopt target pseudo labels to compute this distribution. However, we found that the obtained $\mgamma$ might be inaccurate, as illustrated in Fig.\ref{fig:pa_vis}(a) \pan{(reference incorrect here)}. This observation aligns with prior findings in training-time GDA~\cite{liu2023structural}. PairAlign~\cite{liu2024pairwise} improves estimation accuracy by leveraging a least-squares constrained optimization to align \emph{soft} source and target pseudo-label predictions. However, in GTTA, the absence of source graphs and the requirement for real-time adaptation, even if source graphs were available, make this strategy infeasible.

%In GDA, we assume access to ground truth source data, and in GTTA, we assume the availability of source summary statistics $\probs(Y_v=j|Y_u=i,v\in \neighbor_u)$.

%However, target labels are absent in both scenarios, causing $\mgamma$ estimation to be unreliable due to unstable target pseudo-labels, as noted in initial GDA studies~\cite{liu2023structural}. Later, PairAlign enhanced the robustness of $\mgamma$ estimation by employing a least square constrained optimization aimed at aligning soft source and target pseudo-label predictions. However, In GTTA, the absence of source data hinders the use of such robust estimation strategies. Relying solely on target hard pseudo-label predictions for $\mgamma$ estimation is proven to be unsatisfactory when compared to oracle results, as shown in Fig.~\ref{fig:pa_vis}(a). Thus, developing a robust estimation method for $\mgamma$ without source data access remains a significant challenge.


% In GDA, obtaining $\mgamma$ is easier when we have the access to the source graph ground truth information, and is relatively stable through solving a constrained least square optimization. However, in GTTA, we need to assign $[\mgamma]_{i,j}$ based on only summary statistics of source graph neighborhood distribution and the unstable target pseudo-label predictions. 

%\proj addresses this by quantifying the uncertainty of target pseudo labels~\cite{zhang2019bayesian,stadler2021graph,hsu2022graph}. In particular, TSA assigns $[\mgamma]_{i,j}$ only to node pairs $v\rightarrow u$ where both of their soft target pseudo labels have low entropy $H(\hat{y}')=-\sum_{i\in \gY}[\hat{y}']_i\ln([\hat{y}']_i)\leq\rho_1 \cdot \ln ( |\gY|)$. Here, $\rho_1$ is a hyperparameter and $ \ln( |\gY|)$ is the maximum entropy in $|\gY|$ class prediction. In Fig.\ref{fig:pa_vis}(a), nodes with low-entropy soft predictions are more reliable, resulting in higher accuracy after the assignment of $\mgamma$. 


%^by assigning  $[\mgamma]_{i,j}$ based on the uncertainty of the node pair prediction \cite{zhang2019bayesian,stadler2021graph,hsu2022graph}.
%In particular, we estimate $\probt(Y_v=j|Y_u=i,v\in \neighbor_u)$ using refined soft prediction $\hat{y}'_u$ from the classifier adapted TTA.
%The assignment relies on the accuracy of pseudo-label $y'_u=\argmax \hat{y}'_u$, hence TSA filter out unconfident node label pairs using entropy to estimate uncertainty.
%In particular, TSA assigns $[\mgamma]_{i,j}$ to node pairs where both predictions have entropy $H(\hat{y}_u')=-\sum_{i\in \gY}[\hat{y}_u']_i\ln([\hat{y}_u']_i)\leq\rho_1 \cdot \ln ( |\gY|)$. Here, $\rho_1$ is a hyperparameter and $ \ln( |\gY|)$ is the maximum entropy in $|\gY|$ class prediction.

% \paragraph{Neighborhood Information SNR and Classifier Boundary Bias from Source Training.}
% GTTA does not train the model from scratch; the combination of self and neighborhood features in the GNN as well as the classifier's decision boundary are predetermined based on the source domain. 
% On one hand, in the target domain, the optimal combination from the source domain may be suboptimal, as the variance of neighborhood information changes (see Fig. \ref{fig:pa_vis} (e)).
% Furthermore, even within the same domain, the SNR of neighborhood information varies across different GNN layers and node degrees. On the other hand, a decision boundary obtained from label-imbalanced source training often lies in high density regions, which may further hinder test-time accuracy under label shift. 

% We explain how these two issues are addressed in Sec. \ref{subsec:2ndSNR} and \ref{subsec:boundary_refine}.



\begin{algorithm}[t]
   \caption{Test-Time Structural Alignment (TSA)}
   \label{alg:example}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} A GNN $\phi$ and a classifier $g$ pretrained on source graph $\gG^{\gS}$; Test-time target graph $\gG^{\gT}$; Source statistics $\probs(Y_v|Y_u,v\in \neighbor_u)\in \R_+^{|\gY|\times|\gY|}$
   \STATE Initialize $b^{(k)}\leftarrow 1$ and $\text{MLP}^{(k)}\text{ parameters} \leftarrow0$ for the $k$-th layer.
    \STATE Perform boundary refinement based on embeddings from $g\circ \phi(\gG^{\gT})$ and get soft pseudo-labels $\hat{y}$
    \STATE {\bfseries Get} $\gG^{\gT}_{new}$ {\bfseries by assigning edge weights:}
    \INDSTATE Compute $\gamma$ using  $\hat{y}$ via Eq. \ref{eq:gamma_ratio}
    \INDSTATE  Assign $\gamma$ only if node pairs $H(\hat{y})\leq \rho_1 \cdot \ln(|\mathcal{Y}|)$
        \INDSTATE Assign the parameterized $\alpha$ via Eq. \ref{eq:alpha_ratio}
    \STATE Update $\alpha$'s parameters $b^{(k)}$ and $\text{MLP}^{(k)}$ via Eq. \ref{eq:loss}
    %\STATE Perform boundary refinement again %using $\hat{y}'$ where $H(\hat{y}')\leq \rho_2 \cdot \ln(|\mathcal{Y}|)$ as supervision 
   % \ENDFOR
   \STATE {\bfseries return} $\hat{y}_{\mathrm{final}}$ after another boundary refinement
   % $\hat{Y}_{final}\leftarrow\mathit{ClsTTA}(g(\phi(\gG^{\gT}_{new})))$
   % \REPEAT
   % \STATE Initialize $noChange = true$.
   % \FOR{$i=1$ {\bfseries to} $m-1$}
   % \IF{$x_i > x_{i+1}$}
   % \STATE Swap $x_i$ and $x_{i+1}$
   % \STATE $noChange = false$
   % \ENDIF
   % \ENDFOR
   % \UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}



% On top of the neighborhood alignment, we introduce additional techniques that help better adaptation under the GTTA setting, taking into account the constraints of starting with a pretrained source model and lacking the opportunity for substantial retraining.
\subsection{SNR-Inspired Adjustment}
\label{subsec:snr}
Building on neighborhood alignment, we further optimize the signal-to-noise ratio (SNR) of node representations to enhance performance. Specifically, SNR is characterized by the ratio of inter-class representation distances to intra-class representation distances. %, as illustrated in Fig.~\ref{}. 
A higher SNR indicates more informative and well-separated representations, which benefit classification.

Optimizing SNR complements the neighborhood alignment approach. Even if neighborhood label distributions are perfectly aligned, variations in neighbor set sizes between the source and target graphs can impact the SNR of aggregated neighboring node representations. Consequently, the combination of these aggregated representations with self-node representations in a typical GNN pipeline (Eq.~\ref{eq:gnn}) should be adjusted accordingly across source and target domains, particularly when the source and target graphs exhibit very different degree distributions. Furthermore, the SNR of node self-representations may vary across GNN layers, as deeper layers generally reduce variance. As a result, node self-representations in deeper layers tend to have higher SNR and should be assigned greater emphasis.


To implement the SNR-inspired adjustment, we introduce a parameter to perform the weighted combination of self-node representations and neighborhood-aggregated representations at each layer, adapting to node degrees as follows:

% \hans{is this appropriate to use definition as it is not something derived from principle?}
\begin{definition}
Let $\tilde{d_u}=\left(\frac{\ln(d_u +1)}{\ln(\max_{v\in\node} d_v+1)}\right)$ denote log-normalized degree of node $u$ and let $\text{MLP}^{(k)}$ and $b^{(k)}$ to be learnable parameters for adjusting $k$-th layer combination, define the weights for combination $\alpha \in \R^{L}$ as:
\begin{equation}
    [\alpha]_{k} =
\sigma(\text{MLP}^{(k)}(\tilde{d_u}))-0.5+ b^{(k)}, \: \forall k \in [L]
\label{eq:alpha_ratio}
\end{equation}
\end{definition}

where $\sigma$ is the sigmoid function. 
%The term $b^{(k)}$ accounts for the global tendency, while $\text{MLP}^{(k)}$ gives a local interpretation with respect to node degrees, both at the $k$-th layer. 
During initialization $[\alpha]_{k}$ is set to $1$.
Degree values are taken in the logarithmic domain to handle their often long-tailed distribution. 

Combined with $\mgamma$, $[\alpha]_{k}\cdot[\mgamma]_{i,j}$ is used to reweight the GNN message for non-self-loop node pairs, adjusting the influence from the node with a highly certain pseudo label $j$ to the node with a highly certain pseudo label $i$.
%Since $[\alpha]_{k}$ is constant with respect to the GNN aggregation function, it can be considered as a bootstrapping ratio for non-self-loop pairs.

% Moreover, the SNR of node self representations may also vary across different GNN layers, since in general deep layers help with reducing the variance. 

\textbf{Remark.} Neighborhood alignment alone does not address potential shifts in SNR. This is because the alignment approach, inspired by Thm.~\ref{theory:decomposeerror}, focuses solely on expectation-based (first-order statistical) performance, whereas SNR also incorporates variance (second-order statistics). Thus, these two aspects complement each other. 

% In the context of GNN, the formation of final node representations combines the neighborhood representations after aggregation and self-features. SNR of final node representations will be affected by the SNR of each component. Firstly, when aggregating the neighborhood messages, the density of graph that affects the neighborhood representation SNR. Given the same neighborhood label distribution, the variance will change depending on different number of neighbors. Namely, we have higher SNR if we are aggregating over graph with higher degrees since the aggregated representations have lower variance. Secondly, self-node representation SNR can vary across different layers of GNN. GNN layers have smoothing effect that reduces variances when we aggregate more neighbors. Therefore, node representations in later layers have higher SNR. 

% % In GNN aggregation, self-features should be properly combined with the neighboring aggregated features. 
% Consequently, the key is to identify the optimal combination to form final node representations based on the SNR of both self and neighborhood representations. Under GTTA, the predetermined combination provided by the source model may be suboptimal in terms of SNR in the target graph. Consequently, we introduce SNR-inspired adjustments to enhance the quality of the node representation space. Additionally, note that considering SNR implies the second-order statistics adjustment in terms of the variance which compensates our theoretical analysis that primarily focuses on demonstrating the impact of neighborhood shift on first-order expectation under mean pooling following~\cite{liu2024pairwise}. 

% However, it is important to note that node feature distribution and neighborhood distribution naturally incorporates second-order information, like variance ($\sigma^2$). This can result in different Signal-to-Noise Ratios (SNR), expressed as $\frac{\mu^2}{\sigma^2}$ even when the expectation ($\mu$) remains aligned after addressing neighborhood and feature shifts. In practice, SNR of the node representations can vary depending on node degrees and different GNN layers \pan{why?}. Specifically, below we show the empirical impact from different SNR of the aggregated representation space caused by different graph sparsity in the source and target graph. 

% \paragraph{Signal-to-noise Ratio (SNR) Shift}
% In Fig. \ref{fig:pa_vis} top (a) and top (e), the SNR of aggregated node representation decreases in the target domain as node connectivity decreases while with the same expectations.
% In this case, target node representations, especially minority class, become noisier and scatter among other classes.
% This causes the node representations in Fig. \ref{fig:pa_vis} bottom (e) to spread into regions of other classes, leading to misclassification. Therefore, the observation highlights the necessity of designing a method that optimizes the formation of final node representations by considering the SNR of both the neighborhood representations and self-node features \pan{why should balance these two parts according to SNR}. \pan{the better way to show snr is via different layer mu/sigma; and different density mu/sigma;} \pan{having a proposition and the best balance}


% As discussed in Sec.~\ref{subsec:empirical_invest}, achieving a higher SNR ratio for the final node representations can lead to empirical performance gains. PairAlign does not account for this adjustment during training-time adaptation because it has the flexibility to train the model on a transformed source graph that mimics the target graph, allowing it to learn a more generalizable model for the target graph. 
% % \hans{I think it should be PairAlign access the target graph during training, so the model can adapt to the SNR in target graph through the estimated pseudo-label distribution.}





% Specifically, we adopt a scaling parameter to adjust the optimal combination at each layer as follows:

% % \hans{is this appropriate to use definition as it is not something derived from principle?}
% \begin{definition}
% Let $\tilde{d_u}=\left(\frac{\ln(d_u +1)}{\ln(\max_{v\in\node} d_v+1)}\right),  \forall d_u, d_v \in \gD$ and let $\text{MLP}^{(k)}$ and $b^{(k)}$ to be learnable parameters for adjusting $k$-th layer combination, we have $\alpha \in \R^{L}$ as:
% \begin{equation}
%     [\alpha]_{k} =
% \sigma(\text{MLP}^{(k)}(\tilde{d_u}))-0.5+ b^{(k)}, \: \forall k \in [L]
% \label{eq:alpha_ratio}
% \end{equation}
% \end{definition}

% where $\sigma$ is the sigmoid function. 
% The term $b^{(k)}$ accounts for the global tendency, while $\text{MLP}^{(k)}$ gives a local interpretation with respect to node degrees, both at the $k$-th layer. 
% During initialization $[\alpha]_{k}$ is set to $1$.
% Degree values are taken in the logarithmic domain to handle their often long-tailed distribution.
% Since $[\alpha]_{k}$ is constant with respect to the GNN aggregation function, it can be considered as a bootstrapping ratio for non-self-loop pairs.

\subsection{Decision Boundary Refinement}
\label{subsec:boundary}


% Although Fig. \ref{fig:pa_vis} top (d) shows an aligned latent node representation space, label shift results in a mismatch of the decision boundary between the source and target domains, 
In GTTA, label shift can result in a mismatch of the decision boundary, even after addressing neighborhood shift and obtaining high-SNR node representations. This is illustrated in  Fig.~\ref{fig:pa_vis} (b) and (c). 
% The primary issue arises due to data imbalance during source training. 
%As shown in Fig.~\ref{fig:pa_vis}(b), when the source model is trained on label-imbalanced data with class proportions $[0.1, 0.3, 0.6]$, the decision boundary for the minority class (class $0$) tends to be located in high-density regions. 
%Consequently, even when the neighborhood distribution is perfectly aligned with Oracle assignment, the predefined boundary from the source model can still be affected by label shift. % where the target class ratio is $[0.3, 0.3, 0.3]$ (Fig.~\ref{fig:pa_vis}(c)).


% \label{subsec:boundary_refine}
% Section \ref{Sec:Test Error Analysis} theoretically and empirically shows that simply addressing neighborhood shift in GNNs leads to suboptimal target risk $\errort(g\circ\phi)$. The predefined decision boundary in the source model can easily suffer from the shift in label distribution during test time. Recall the example that label imbalance in the source domain causes the boundary to lie in high density regions for small portion classes, which cannot be solved by simply aligning the neighborhood distribution.
% Under label shift, accuracy may be further hindered as the portion of small-portion classes increases, leading to a decrease in overall accuracy (see Fig. \ref{fig:pa_vis} (c)).
A straightforward approach to refining the decision boundary at test time is to adjust the classifier's batch normalization parameters (TENT~\cite{wang2020tent}) or directly modify its output (T3A~\cite{iwasawa2021test} and LAME~ \cite{boudiaf2022parameter}). 
We integrate these techniques into our framework for two folds: (1) their refined pseudo-labels provide a more reliable assignment of $\mgamma$ and can supervise the update of SNR adjustment. (2) Reciprocally, better alignment of neighborhood information can further refine the decision boundary.
% The final adaptation from the the classifier-adapting TTA further enhances prediction accuracy by reducing CSS discrepancy.

% We integrate these techniques into our framework. \pan{after classifier, y , y}

% leverage non-graph TTA 

% without changing the GNN encoder. 
% Non-graph TTA has widely explored this by adjusting the classifier's batch norm parameters \cite{wang2020tent} or its output directly \cite{iwasawa2021test,boudiaf2022parameter}.
% By integrating with non-graph TTA, TSA effectively handles both label shift and feature shift.

% \shikun{we may want to include more insights in how and why previous works can adjust decision boundary. how they are trained, provide some brief summarization, list two works in 3-4 sentences. }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{TSA Overview.}
\label{Sec:overview}
 %Since $\bar{\gamma}_{u,v}$ relies on the quality of the predictions, TSA utilizes more accurate prediction from a classifier-adapting TTA to get a more reliable estimation on $\probt(Y_v,Y_u | v\in \neighbor_u)$ and pseudo-label assignments.
Note that $\mgamma$ is obtained from the initial pseudo labels.
Only the parameters in $\text{MLP}^{(k)}$ and $b^{(k)}$ for $\alpha$ estimation in Eq.~\ref{eq:alpha_ratio} need to be optimized in the test time according to Eq.~\ref{eq:loss}.

% The learnable bootstrapping $\alpha$ and parameters in the MLP are optimized by the cross-entropy loss:

% The parameters in $\text{MLP}^{(k)}$ and $b^{(k)}$ will be optimized in the test time via Eq.~\ref{eq:loss} supervised by hard pseudo labels.


\vspace{-5mm}
\begin{equation}
    \gL_{CE} = \frac{1}{|\gV^{\gT}|}\sum_{u\in\gV^{\gT}}\text{cross-entropy}(y_u',\hat{y}_u) 
    \label{eq:loss}
\end{equation}
\vspace{-2mm}

where $y_u'$ is the hard pseudo label refined by procedure in Sec.~\ref{subsec:boundary} and $\hat{y}_u$ is the soft prediction from the original model.
After updating $\alpha$, TSA makes predictions on the newly weighted graphs and then further adapts the boundaries as described in Sec. \ref{subsec:boundary}. %This procedure will iterate. 
Our proposed algorithm is summarized in Alg. \ref{alg:example}. % to adapt predictions on the newly weighted graph.


\textbf{Comparison to AdaRC \cite{bao2024adarc}}
AdaRC is a recent work on GTTA that also integrates non-graph TTA methods with an approach for addressing structure shifts. However, it considers only degree shift and homophily shift, where homophily shift is merely a special case of neighborhood shift in our context. Consequently, AdaRC does not introduce the parameter $\mgamma$ and, therefore, lacks the ability to properly align neighborhood aggregated representations when the neighboring label distribution shifts. 
%Their idea is that these two shifts will yield the neighborhood information degraded and hence learn to adapt the k-hop aggregation parameters in order to restore better node representation from the combination of self and neighboring feature.  
%However, AdaRC 
%and thus cannot properly align \emph{neighborhood representations} when neighboring node label shifts. %that have already been mismatched due to neighborhood shift.
%Also, AdaRC does not distinguish between degree shift and homophily shift when addressing them. 
%On the other hand, TSA learns a function that takes node degree as input, providing better interpretability on the emphasis between center and neighbor information with respect to node degree.
% Our work is also the first work to systematically explore the idea of domain invariant learning in GTTA \cite{kurmi2021domain, liang2021source, mirza2023actmad, abdul2023align}.\shikun{what does it mean by this sentence?}















% \begin{restatable}{proposition}{scaledbootstrap}
% \label{prop:scaledbootstrap}If $f^{(k)}_{u,v}(\cdot)$ is a constant with respect to the aggregation function and is a scalar used to scale the aggregated neighboring nodes’ representations:  $h_u^{(k+1)} = \text{UPT}(h_u^{(k)}, f^{(k)}_{u,v}(\cdot)\text{AGG}(\{h_{v}^{(k)}, v \in \neighbor_u\}))$. Then, the scaled aggregation is equivalent to bootstrapping the neighboring nodes by a ratio of $f^{(k)}_{u,v}(\cdot)$. \pan{do we really want to add this proposition? }
% \end{restatable}

% Proposition \ref{prop:scaledbootstrap} demonstrates a design that enables bootstrapping to balance  the information from the center node and its neighborhood.
% Note that this can be combined with the neighborhood shift mitigation strategy by assigning edge weights as $\bar{\gamma}_{u,v}\cdot f^{(k)}_{u,v}(\cdot)$, together TSA addresses the CSS.