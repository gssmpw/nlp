
% \subsection{CSS Decomposition}
% \decomposeCSS*

% \begin{proof} 
 
% \begin{align*}
%     \prob(\rmA|\rmY)
%     &=\prod_{u,v}\prob(A_{u,v} | Y_u, Y_v)  \\
%     %  \\
%     % &=
%     %  \prod_{u,v}\sum_{\gD}\prob(D_{u,v} | Y_u, Y_v) \prob\prob(e | Y_u, Y_v, D_{u,v})
%     %  \\
%     %  &=
%     %  \\
%     &=\prod_{u\in\gV}\prob(E_{u\leftarrow \neighbor_u} | Y_u) \tag{a}
%     \\
%     &=\prod_{u\in\gV}\sum_{\gD}\prob(D_u | Y_u)\prob(E_{u\leftarrow\neighbor_u} | Y_u, D_u)
%     \\    
%     &=\prod_{u\in\gV}\sum_{\gD}\prob(D_u | Y_u)\prod_{v\in \neighbor_u}\prob(E_{u\leftarrow v} | Y_u, D_u) \tag{b}
%     \\    
%     &=\prod_{u\in\gV}\sum_{\gD}\prob(D_u | Y_u)\prod_{v \in \neighbor_u}(\sum_{\gY}\prob(E_{u\leftarrow v} | Y_u, Y_v, D_u)\prob(Y_v|Y_u, v\in \neighbor_u))
%     \tag{c}
%     \\
% \end{align*}
% (a) We define $E_{u\leftarrow \neighbor_u}$ as a random variable indicates node $u$ receiving connections.
% Given our edges are independently sampled from labels, the conditional structure probability can be rewritten as the joint probability of $\prob(E_{u\leftarrow \neighbor_u} | Y_u)$ with respect to nodes.
% (b) The probability $\prob(E_{u\leftarrow v} | Y_u, D_u)$ represents the probability that the center node $u$ receive an edge from a node $v$ conditioned on node label and degree.
% (c) Based on our assumption, $\prob(E_{u\leftarrow v} | Y_u, Y_v, D_u)$ should be a constant among all data generation processes. However $\prob(D_u | Y_u)$ and $\prob(Y_v|Y_u, v\in \neighbor_u)$ may change in different doamin.
% \end{proof}


\subsection{Proof of Theorem \ref{theory:decomposeerror}}

\begin{lemma}
\label{error decomposition lemma}
    Assume $\probm \coloneq 
    \alpha_j 
    % \omegas{u_k|u_j}
    \probs(\predy_u=i| \rvY_u=j)
    + 
    \beta_j 
    \probt(\predy_u=i| \rvY_u=j)$ to be a mixture conditional probability of source and target domain. The mixture weight is given by $\alpha_j$ and $\beta_j$ where $\forall \alpha_j,\beta_j \geq 0$ and $\alpha_j+\beta+j=1$, the following upper bound holds:
    \begin{align*}
    &\abs{\gammas{u_j}\probs(\predy_u=i| \rvY_u=j) - \gammat{u_j}\probt(\predy_u=i| \rvY_u=j} \\
    &\leq\abs{\gammas{u_j} - \gammat{u_j}} \cdot 
    \probm
    + (\gammas{u_j}\beta_j+\gammat{u_j}\alpha_j)\abs{ \probs(\predy_u=i| \rvY_u=j) -  \probt(\predy_u=i| \rvY_u=j)}
    \end{align*}


\begin{proof}
    To simplify the derivation we first abuse the notation by letting $\probs$ denote $\probs(\predy_u=i| \rvY_u=j, \rvW_u=k)$ and $\probt$ denote $\probt(\predy_u=i| \rvY_u=j, \rvW_u=k)$.
    \begin{align*}
    &\abs{\gammas{u_j}\probs - \gammat{u_j}\probt} 
    - \abs{\gammas{u_j}- \gammat{u_j}} \cdot \probm \\
    &\leq \abs{\gammas{u_j}\probs - \gammat{u_j}\probt - 
    \rpar{\gammas{u_j} - \gammat{u_j}}\probm}\\
    &=\abs{\gammas{u_j}\rpar{\probs-\probm} - \gammat{u_j}\rpar{\probt-\probm}} \\
    &\leq \gammas{u_j}\abs{\probs-\probm} + \gammat{u_j} \abs{\probt-\probm}
    \end{align*}
    In order to simplify the first term we substitute the definition of $\probm \coloneq \alpha_j \probs
    + \beta_j \probt$:
    \begin{align*}
        \abs{\probs-\probm} = \abs{\probs - \alpha_j \probs -\beta_j\probt} 
        =  \abs{ \beta_j\probs -\beta_j \probt}
    \end{align*}
    Similarly for the second term:

    \begin{align*}
        \abs{\probt-\probm} = \abs{\probt - \alpha_j \probs -\beta_j \probt} = \abs{\alpha_j \probt -\alpha_j \probs}
    \end{align*}
    Using the two identities, we can proceed with the derivation:
    \begin{align*}
         &\abs{\gammas{u_j}\probs - \gammat{u_j}\probt} - \abs{\gammas{u_j}
         - \gammat{u_j}} \cdot \probm \\
    &\leq \gammas{u_j}\beta_j\abs{ \probs - \probt} + \gammat{u_j}\alpha_j\abs{\probt-\probs} \\
    &= (\gammas{u_j}\beta_j+\gammat{u_j}\alpha_j)\abs{ \probs - \probt}
    \end{align*}
\end{proof}
\end{lemma}

\decomposeerror*


\begin{proof}
     
    We start by converting the average error rate into individual error probabilities. For sufficiently large $|\node|$ we have $|\node|\approx |\node|-1$. Given $|\nodes|\rightarrow\infty$ and $|\nodet|\rightarrow\infty$, we can have $|\node|\approx |\nodes| \approx |\nodet|$. By applying the triangle inequality and assuming that the graphs are sufficiently large, we obtain the following inequality:
    \begin{align*}
        &\abs{\errors(h \circ g) - \errort(h \circ g)} \\
       = &\abs{\probs(g(\phi(X_u, \rmA))\neq Y_u) - \probt(g(\phi(X_u, \rmA))\neq Y_u)}\\
       =&\abs{ \probs(\predy_u \neq \rvY_u) -  \probt(\predy_u \neq \rvY_u)} \\
    \end{align*}
To simplify the notation, define $\gammau{u_j}\coloneq \probu(\rvY_u=j)$, $\piu{v_k|u_j} \coloneq \probu(\rvY_{v}=k | \rvY_u=j, v\in \neighbor_u )$, and $\omegau{u_i|u_j,v_k}\coloneq \probu(\predy_u=i| \rvY_u=j, \rvY_{v}=k, v\in \neighbor_u)$ for $\mathcal{U}) \in \cbrace*{\mathcal{S}, \mathcal{T}}$. Using the the law of total probability and assuming that the label prediction depends on the distribution of neighborhood labels, we can derive the following identity:

\begin{align*}
    \probu(\predy_u \neq \rvY_u) &= \sum_{i\neq j}\probu(\predy_u=i, \rvY_u=j)= \sum_{i\neq j}\probu(\rvY_u=j) \probu(\predy_u=i| \rvY_u=j)\\
    % &=\sum_{i\neq j}\probu(\rvY_u=j) \sum_{d \in \mathcal{D}} \probu(|\neighbor_u|=d | \rvY_u=j)
    % \probu(\predy_u=i| \rvY_u=j, |\neighbor_u|=d)
    % \\
    % &\begin{aligned}
    %     =\sum_{i\neq j}\probu(\rvY_u=j) \sum_{d \in \mathcal{D}} \probu(|\neighbor_u|=d | \rvY_u=j) \bigg(\prod^{d}_{t=1} &\sum_{k \in \mathcal{Y}}  \probu(\rvY_{v_t}=k | \rvY_u=j, v_t\in\neighbor_u)  \\
    %     &\probu(\predy_u=i| \rvY_u=j, |\neighbor_u|=d, \set{\rvY_{v}}=k^d) \bigg)
    % \end{aligned}
    \\
      &=\sum_{i\neq j}\probu(\rvY_u=j)  \bigg(\sum_{k\in\mathcal{Y}}  \probu(\rvY_{v}=k | \rvY_u=j, v\in \neighbor_u )
        \probu(\predy_u=i| \rvY_u=j, \rvY_{v}=k, v\in \neighbor_u) \bigg)\\
    &=\sum_{i\neq j}\gammau{u_j}\sum_{k\in\mathcal{Y}}\piu{v_k|u_j}\omegau{u_i|u_j,v_k}
\end{align*}

% Plugging the identities into the above, we can show that
% \begin{align*}
%      &|\errors(h \circ g) - \errort(h \circ g)| \\
%      &\leq \frac{1}{|\node|}\sum_{u \in \node} |\sum_{i\neq j} \gammas{u_j} \sum_{k\in \mathcal{W}} \omegas{u_k|u_j}\probs(\predy_u=i| \rvY_u=j, \rvW_u=k)
%      - \sum_{i\neq j}\gammat{u_j} \sum_{k\in \mathcal{W}} \omegat{u_k|u_j}\probt(\predy_u=i| \rvY_u=j, \rvW_u=k)| \\
%      &\leq \frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j}\sum_{k\in \mathcal{W}}\abs{\gammas{u_j}\omegas{u_k|u_j}\probs(\predy_u=i| \rvY_u=j, \rvW_u=k) - \gammat{u_j}\omegat{u_k|u_j}\probt(\predy_u=i| \rvY_u=j, \rvW_u=k)}
% \end{align*}

The function of mean pooling naturally marginalizes the effect of degree magnitude, meaning that only the expectation of the neighboring nodes' label distribution and the label distribution of the central node itself influence the prediction.
We use Lemma \ref{error decomposition lemma} to bound the terms above.
Since $\forall j \in \mathcal{Y},
\gammas{u_j} \text{ and } \gammat{u_j} \in [0,1]$ and we have $\alpha_j + \beta_j = 1$, we obtain:
\begin{align*}
    &|\errors(h \circ g) - \errort(h \circ g)| \\
    &\leq 
    % \frac{1}{|\node|}
    % \sum_{u \in \node}\abs{
    \sum_{i\neq j}\gammas{u_j}\probs(\predy_u=i| \rvY_u=j) - \sum_{i\neq j}\gammat{u_j}\probt(\predy_u=i| \rvY_u=j)
    % } 
    \\
    &\leq \sum_{i\neq j}
    \abs{\gammas{u_j}\probs(\predy_u=i| \rvY_u=j) - \gammat{u_j}\probt(\predy_u=i| \rvY_u=j)} 
    \tag{a}
    \\
    &\leq \sum_{i\neq j}
    \rpar*{\abs{\gammas{u_j} - \gammat{u_j}} \cdot 
    \probm
    +(\gammas{u_j}\beta_j + \gammat{u_j}\alpha_j )
    \abs{ \probs - \probt}  
    }
    \tag{b}
    \\
    &= \sum_{i\neq j}
    \rpar*{\abs{\gammas{u_j} - \gammat{u_j}} \cdot\probs(\predy_u=i| \rvY_u=j) + \gammat{u_j}
    \abs{\probs(\predy_u=i| \rvY_u=j) - \probt(\predy_u=i| \rvY_u=j)}
    }
    \tag{c}
    \\
    &= \sum_{i\neq j}\abs{\gammas{u_j} - \gammat{u_j}} \cdot\probs(\predy_u=i| \rvY_u=j) + \sum_{i\neq j}\gammat{u_j}
    \abs{\probs(\predy_u=i| \rvY_u=j) - \probt(\predy_u=i| \rvY_u=j)}
    \\
    &\leq 
    \rpar*{\sum_{j\in\gY}\abs{\gammas{u_j} - \gammat{u_j}} }
    \cdot\balerrorrate{\predy_u}{\rvY_u} 
    + 
    \sum_{i\neq j}\gammat{u_j}
        \abs{\probs(\predy_u=i| \rvY_u=j) - \probt(\predy_u=i| \rvY_u=j)}
    \tag{d}
    \\
    &=2TV\rpar*{\probs(\rvY_u=j), \probt(\rvY_u=j)}\cdot\balerrorrate{\predy_u}{\rvY_u}+ 
    \sum_{i\neq j}\gammat{u_j}
        \abs{\probs(\predy_u=i| \rvY_u=j) - \probt(\predy_u=i| \rvY_u=j)}
    \tag{e} 
\end{align*}
By applying the triangle inequality, (a) is obtained.
Following Lemma \ref{error decomposition lemma} we have (b).
(c) is obtained by choosing $\alpha_j=1$ and $\beta_j=0, \forall j \in \mathcal{Y}$.
(d) is derived by applying Holder's inequality.
(e) is based on the definition of total variation distance where 
    $TV(\probs(\rvY_u=j), \probt(\rvY_u=j))=\frac{1}{2}\sum_{j\in\gY}\abs{\probs(\rvY_u=j) - \probt(\rvY_u=j)}$ is the total variation distance between the label distribution of source and target. 

% The second term can be further decomposed by using Lemma  \ref{error decomposition lemma} while setting $\probm_d \coloneq 
%     \alpha_j 
%      \probs(\predy_u=i| \rvY_u=j, |\neighbor_u|=d)
%     + 
%     \beta_j 
%      \probt(\predy_u=i| \rvY_u=j, |\neighbor_u|=d)$, $\probs_d\coloneq \probs(\predy_u=i| \rvY_u=j, |\neighbor_u|=d)$, and $\probt_d\coloneq\probt(\predy_u=i| \rvY_u=j, |\neighbor_u|=d)$. 
% The decomposition is instead with respect to $\omegau{u_d|u_j}\coloneq \probu(D_u=d | \rvY_u=j)$.

% \begin{align*}
%     &\frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j}\gammat{u_j}
%         \abs{\probs(\predy_u=i| \rvY_u=j) - \probt(\predy_u=i| \rvY_u=j)}
%     \\
%      &=\frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j} \gammat{u_j}
%         \abs{\sum_{d \in \mathcal{D}} \omegas{u_d|u_j}\probs_d - \sum_{d \in \mathcal{D}} \omegat{u_d|u_j}\probt_d}
%     \\
%      &\leq\frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j} \gammat{u_j}\sum_{d \in \mathcal{D}}
%         \abs{ \omegas{u_d|u_j}\probs_d -  \omegat{u_d|u_j}\probt_d}
%     \\
%     &\leq\frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j} \gammat{u_j}\sum_{d \in \mathcal{D}}
%     \rpar*{\abs{\omegas{u_d|u_j} -\omegat{u_d|u_j} }\cdot \probm_d 
%     + 
%     (\omegas{u_d|u_j}  \beta_j+\omegat{u_d|u_j}  \alpha_j)
%     \abs{\probs_d
%     - 
%     \probt_d}}
%     \\
%     &=\frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j} \gammat{u_j}\sum_{d \in \mathcal{D}}
%     \rpar*{\abs{\omegas{u_d|u_j} -\omegat{u_d|u_j} }\cdot \probs_d 
%         + 
%         \omegat{u_d|u_j}
%         \abs{\probs_d
%         - 
%         \probt_d}
%     }
%     \\
%     &=\frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j} \gammat{u_j}\sum_{d \in \mathcal{D}}\abs{\omegas{u_d|u_j} -\omegat{u_d|u_j} }\cdot \probs_d 
%         +
%     \frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j} \gammat{u_j}\sum_{d \in \mathcal{D}} 
%         \omegat{u_d|u_j}
%         \abs{\probs_d
%         - 
%         \probt_d}
%     \\
%     &=\frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j} \gammat{u_j}\sum_{d \in \mathcal{D}}\abs{1 -\frac{\omegat{u_d|u_j}}{\omegas{u_d|u_j}} }\cdot\omegas{u_d|u_j} \probs_d 
%         +
%     \frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j} \gammat{u_j}\sum_{d \in \mathcal{D}} 
%         \omegat{u_d|u_j}
%         \abs{\probs_d
%         - 
%         \probt_d}
%     \\
%     &\leq\frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j} \gammat{u_j}
%     \max_{d\in\gD}\abs{1 -\frac{\omegat{u_d|u_j}}{\omegas{u_d|u_j}} }
%     \probs(\predy_u=i| \rvY_u=j)
%         +
%     \frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j} \gammat{u_j}\sum_{d \in \mathcal{D}} 
%         \omegat{u_d|u_j}
%         \abs{\probs_d
%         - 
%         \probt_d}
%     \tag{a}
%     \\
%     &\leq\frac{1}{|\node|}\sum_{u \in \node}\E\spar*{
%     \max_{d\in\gD}\abs{1 -\frac{\omegat{u_d|u_j}}{\omegas{u_d|u_j}} }}
%    \balerrorrate{\predy_u}{\rvY_u} 
%         +
%     \frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j} \gammat{u_j}\sum_{d \in \mathcal{D}} 
%         \omegat{u_d|u_j}
%         \abs{\probs_d
%         - 
%         \probt_d}
%     \\
% \end{align*}

% (a) is obtained by applying Holder's inequality and marginalized over $d$: $\probs(\predy_u=i| \rvY_u=j)=\sum_{d \in \mathcal{D}}\probs(|\neighbor_u|=d | \rvY_u=j)\probs(\predy_u=i| \rvY_u=j, |\neighbor_u|=d)$.

Similarly, the second term can be decomposed by Lemma  \ref{error decomposition lemma} while setting $\omegam{u_i|u_j,v_k} \coloneq 
    \alpha_j 
     \probs(\predy_u=i| \rvY_u=j, \rvY_{v}=k, v\in \neighbor_u)
    + 
    \beta_j 
     \probt(\predy_u=i| \rvY_u=j, \rvY_{v}=k, v\in \neighbor_u)$, $\omegas{u_i|u_j,v_k}\coloneq \probs(\predy_u=i| \rvY_u=j, \rvY_{v}=k, v\in \neighbor_u)$, and $\omegat{u_i|u_j,v_k}\coloneq \probt(\predy_u=i| \rvY_u=j, \rvY_{v}=k, v\in \neighbor_u)$. 
The decomposition is instead with respect to $\piu{v_k|u_j} \coloneq \probu(\rvY_{v}=k | \rvY_u=j, v\in \neighbor_u )$.

\begin{align*}
    &\sum_{i\neq j}\gammat{u_j}
        \abs{\probs(\predy_u=i| \rvY_u=j) - \probt(\predy_u=i| \rvY_u=j)}
    \\
    &=\sum_{i\neq j}\gammat{u_j} 
    \begin{aligned}[t]
        &\abs{ \sum_{k\in\mathcal{Y}}  
        \probs(\rvY_{v}=k | \rvY_u=j, v\in \neighbor_u )
        \probs(\predy_u=i| \rvY_u=j, \set{\rvY_{v}:v\in \neighbor_u}) \\
        &\quad - \sum_{k\in\mathcal{Y}}  
        \probt(\rvY_{v}=k | \rvY_u=j, v\in \neighbor_u )
        \probt(\predy_u=i| \rvY_u=j, \set{\rvY_{v}:v\in \neighbor_u})
        }
    \end{aligned}
    \\
    % & =\sum_{i\neq j}\probu(\rvY_u=j)  \bigg(\sum_{k\in\mathcal{Y}}  \probu(\set{\rvY_{v}:v\in \neighbor_u} | \rvY_u=j)
    %     \probu(\predy_u=i| \rvY_u=j, \set{\rvY_{v}:v\in \neighbor_u}) \bigg)
    %     \abs*{\prod^{d}_{t=1} \sum_{k \in \mathcal{Y}}  \probs(\rvY_{v}=k | \rvY_u=j, v\in\neighbor_u)
    %     \probt(\predy_u=i| \rvY_u=j, \set{\rvY_{v}}=k^d)
    %     -
    %     \prod^{d}_{t=1} \sum_{k \in \mathcal{Y}} \probt(\rvY_{v}=k | \rvY_u=j, v\in\neighbor_u) \probt(\predy_u=i| \rvY_u=j, \set{\rvY_{v}}=k^d)
    %     }
    &\leq\sum_{i\neq j} \gammat{u_j}\sum_{k\in\mathcal{Y}}
        \abs{  \pis{v_k|u_j}
        \omegas{u_i|u_j,v_k} 
        -
        \pit{v_k|u_j} \omegat{u_i|u_j,v_k}
        }
    \\
    &\leq\sum_{i\neq j} \gammat{u_j}\sum_{k\in\mathcal{Y}}\rpar*{
    \abs{\pis{v_k|u_j} -\pit{v_k|u_j} }\cdot \omegam{u_i|u_j,v_k} 
    + 
    (\pis{v_k|u_j}  \beta_j+\pit{v_k|u_j}  \alpha_j)
    \abs{\omegas{u_i|u_j,v_k}
    - 
    \omegat{u_i|u_j,v_k}}}
    \tag{a}
    \\
     &=\sum_{i\neq j} \gammat{u_j} \sum_{k\in\mathcal{Y}}
    \abs{\pis{v_k|u_j} -\pit{v_k|u_j} }\cdot \omegas{u_i|u_j,v_k} 
    + 
    \sum_{i\neq j} \gammat{u_j}\sum_{k\in\mathcal{Y}}
   \pit{v_k|u_j}\
    \abs{\omegas{u_i|u_j,v_k}
    - 
    \omegat{u_i|u_j,v_k}}
    \tag{b}
    \\
     &=\sum_{i\neq j} \gammat{u_j}  \sum_{k\in\mathcal{Y}}
    \pis{v_k|u_j}\abs{1 -\frac{\pit{v_k|u_j}}{\pis{v_k|u_j}} }\cdot \omegas{u_i|u_j,v_k} 
    + 
    \sum_{i\neq j} \gammat{u_j}\sum_{k\in\mathcal{Y}}
   \pit{v_k|u_j}\
    \abs{\omegas{u_i|u_j,v_k}
    - 
    \omegat{u_i|u_j,v_k}}
    \\    
    &\leq \sum_{i\neq j} \gammat{u_j}  \rpar*{\max \abs{1-\frac{\probt(Y_v=k|Y_u=j,v\in \neighbor_u)}{\probs(Y_v=k|Y_u=j,v\in \neighbor_u)}}} \cdot \probs(\predy_u=i| \rvY_u=j) 
    + 
    \sum_{i\neq j} \gammat{u_j}\sum_{k\in\mathcal{Y}}
   \pit{v_k|u_j}\
    \abs{\omegas{u_i|u_j,v_k}
    - 
    \omegat{u_i|u_j,v_k}}    
    \tag{c}\\
    &\leq \E^\gT\spar*{\rpar*{\max \abs{1-\frac{\probt(Y_v=k|Y_u=j,v\in \neighbor_u)}{\probs(Y_v=k|Y_u=j,v\in \neighbor_u)}}}}  \balerrorrate{\predy_u}{\rvY_u}
    + 
    \sum_{i\neq j} \gammat{u_j}\sum_{k\in\mathcal{Y}}
   \pit{v_k|u_j}\
    \abs{\omegas{u_i|u_j,v_k}
    - 
    \omegat{u_i|u_j,v_k}}
    \\
    &\leq \E_{Y_u}^\gT\spar*{\rpar*{\max \abs{1-\frac{\probt(Y_v=k|Y_u=j,v\in \neighbor_u)}{\probs(Y_v=k|Y_u=j,v\in \neighbor_u)}}}}  \balerrorrate{\predy_u}{\rvY_u}
    + 
    \max_{i\
    \neq j}\E^\gT\spar*{|\omegas{u_i|u_j,v_k} - \omegat{u_i|u_j,v_k} |}
    \\
    &\begin{aligned}[t]
    \leq \E_{Y_u}^\gT&\spar*{\rpar*{\max \abs{1-\frac{\probt(Y_v=k|Y_u=j,v\in \neighbor_u)}{\probs(Y_v=k|Y_u=j,v\in \neighbor_u)}}}}  \balerrorrate{\predy_u}{\rvY_u}\\
    &+ \max_{i\neq j}\E^\gT_{\rvY_{v}}\spar{|\probs(\predy_u=i| \rvY_u=j, \rvY_{v}=k, v\in \neighbor_u)
    - \probt(\predy_u=i| \rvY_u=j, \rvY_{v}=k, v\in \neighbor_u) |}
    \end{aligned}
    \\
    &\leq \E_{Y_u}^\gT\spar*{\rpar*{\max \abs{1-\frac{\probt(Y_v=k|Y_u=j,v\in \neighbor_u)}{\probs(Y_v=k|Y_u=j,v\in \neighbor_u)}}}}  \balerrorrate{\predy_u}{\rvY_u} + \Delta_{CE} \tag{d}
\end{align*}


% \hans{add explanation (b)}
(a) comes from Lemma \ref{error decomposition lemma}. (b) comes from setting $\alpha_j=1$ and $\beta_j=0, \forall j \in \mathcal{Y}$
(c) The inequality comes from Holder's inequality, where we take the maximum-norm for $k$ and take the $L_1$ norm to marginalize $Y_v$: $\probs(\predy_u=i| \rvY_u=j) =\sum_{k\in\mathcal{Y}}  \probu(\rvY_{v}=k | \rvY_u=j, v\in \neighbor_u )\probu(\predy_u=i| \rvY_u=j, \rvY_{v}=k, v\in \neighbor_u)$
(d) The last term in the previous line corresponds to the conditional error gap in \cite{tachet2020domain} but in a GNN mean pooling fashion.

% The last term $\probu(\predy_u=i| \rvY_u=j, \rvY_{v}=k, v\in \neighbor_u)$ we introduce a random variable of the one-hop structure $\rmA \sim \probu_j(N)$ that samples from the histogram of neighborhood distribution N given the center node being $j$.
The term $\probu(\predy_u=i| \rvY_u=j, \rvY_{v}=k, v\in \neighbor_u)$ can be interpreted as the probability of error prediction using a mean pooling GNN encoder.
By definition, $Y_v$ represents a random variable of the neighborhood label ratio and $Y_u$ denotes a random variable of self class label, their realization corresponds to the mean pooled and center value.
As a reult, $\Delta_{CE}$ only exists if and only if feature shift exists. Putting everything together, we have

% Knowing that $Y_v$ represents a random variable of the neighborhood label ratio and $Y_u$, we can conclude to know the random vector $N$. 

% Hence we can rewrite the term to $\probu(\predy_u=i| \rvY_u=j, \rmA)$.

% % \begin{align*}
% %  \Delta_{CE}\coloneq\max_{i\neq j}\E^\gT_{\rvY_{v}}\spar{|\probs(\predy_u=i| \rvY_u=j, \rmA)
% %     - \probt(\predy_u=i| \rvY_u=j, \rmA) |}
% % \end{align*}

% As $\probu(\predy_u=i| \rvY_u=j, \rmA)$  conditioned on its self label and the graph structure, $\Delta_{CE}$ only exists if and only if feature shift exists. Putting everything together, we have

\vspace{-6mm}
\begin{align*}
&\abs{\errors(g \circ \phi) - \errort(g \circ \phi)} 
\\
&\leq  
\balerrorrate{\predy_u}{\rvY_u}\cdot
    \cbrace{\underbrace{
    \rpar{2TV(\probs(\rvY_u), \probt(\rvY_u)}
    }_{\text{Label Shift}}
    +\underbrace{\E_{Y_u}^\gT\spar{\max_{k\in \gY} \abs{1-\frac{\probt(Y_v=k|Y_u,v\in \neighbor_u)}{\probs(Y_v=k|Y_u,v\in \neighbor_u)}}}  }_{\textbf{Neighborhood shift}}}
    + \underbrace{\Delta_{CE}}_{\text{Feature shift}}
    \vspace{-2mm}
\end{align*}









% By choosing $\alpha_j=1$ and $\beta_j=0, \forall j \in \mathcal{Y}$, we have
% \begin{align*}
%     &|\errors(h \circ g) - \errort(h \circ g)|\\
%     &\leq\frac{1}{|\node|}\sum_{u \in \node}\sum_{i\neq j}\sum_{k\in \mathcal{W}}\abs{\gammas{u_j}\omegas{u_k|u_j} - \gammat{u_j}\omegat{u_k|u_j}} 
%     \cdot
%     \probs(\predy_u=i| \rvY_u=j, \rvW_u=k)
%     +\frac{1}{|\node|}\sum_{u \in \node}2(c-1)\conerrorgap{\predy_u}\\
%     &=\frac{1}{|\node|}\sum_{u \in \node}\sum_{ j\in\mathcal{Y}}\sum_{k\in \mathcal{W}}\abs{\gammas{u_j}\omegas{u_k|u_j} - \gammat{u_j}\omegat{u_k|u_j}}
%     \cdot 
%     \rpar*{\sum_{i=1,i\neq j}\probs(\predy_u=i| \rvY_u=j, \rvW_u=k)}
%     +\frac{1}{|\node|}\sum_{u \in \node}2(c-1)\conerrorgap{\predy_u}\\
%     &=\frac{1}{|\node|}\sum_{u \in \node}\sum_{ j\in\mathcal{Y}}\sum_{k\in \mathcal{W}}\abs{\gammas{u_j}\omegas{u_k|u_j} - \gammat{u_j}\omegat{u_k|u_j}}
%     \cdot 
%     \probs(\predy_u\neq\rvY_u| \rvY_u=j, \rvW_u=k)
%     +\frac{1}{|\node|}\sum_{u \in \node}2(c-1)\conerrorgap{\predy_u}\\
%     &\leq \frac{1}{|\node|}\sum_{u \in \node}
%     \rpar*{
%     \sum_{ j\in\mathcal{Y}}\sum_{k\in \mathcal{W}}\abs{\gammas{u_j}\omegas{u_k|u_j} - \gammat{u_j}\omegat{u_k|u_j}}
%     } 
%     \cdot
%     \balerrorrate{\predy_u}{\rvY_u}
%     +\frac{1}{|\node|}\sum_{u \in \node}2(c-1)\conerrorgap{\predy_u}\\
%     &= \frac{1}{|\node|}\sum_{u \in \node}
%     \norm{\probs_{Y_u}\probs_{W_u|Y_u}
%     -
%     \probt_{Y_u}\probt_{W_u|Y_u}
%     }_1 
%     \cdot
%     \balerrorrate{\predy_u}{\rvY_u}
%     +\frac{1}{|\node|}\sum_{u \in \node}2(c-1)\conerrorgap{\predy_u}
% \end{align*}
% The last inequality holds by Holder's inequality and the definition of structured balanced error rate. 
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
