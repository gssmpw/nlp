\section{Experiments}  
\input{icml2025/sections/result_analysis}
We evaluate \proj %by comparing with non-graph TTA methods and existing GTTA methdos the combination with the three non-graph TTA methods and compared to basedlines from TTA
on synthetic datasets and 5 real-world datasets. More discussions and results in experiments can be found in Appendix~\ref{app:exp}.

\subsection{Datasets and Baselines}

\textbf{Synthetic Data.} 
We use the CSBM model~\cite{deshpande2018contextual} to generate three-class datasets, focusing on structure shifts while keeping feature distributions unchanged. Specifically, we evaluate performance under three conditions: (1) neighborhood shift, (2) neighborhood shift plus SNR (induced by degree changes) shift, and (3) neighborhood shift combined with both SNR shift and label shift. This experimental setup is motivated by Thm. \ref{theory:decomposeerror} and the observations in Sec.\ref{subsec:snr}. For each shift scenario, we examine two levels of severity, with the left column in later Table~\ref{table:syn} corresponding to the smaller shift.

\textbf{MAG~\cite{liu2024pairwise}} is a citation network extracted by OGB-MAG \cite{hu2020open}.
Distribution shifts arise from partitioning papers into distinct graphs based on their countries of publication. The task is to classify the publication venue of each paper. Our model is pretrained on graphs from the US and China and subsequently adapted to graphs from other countries.

\textbf{Pileup Mitigation~\cite{liu2023structural}} is a dataset curated for the data-denoising step in high energy physics~\cite{bertolini2014pileup}.
Particles are generated by proton-proton collisions in LHC experiments. The task is to classify leading-collision (LC) neutral particles from other-collision (OC) particles. Particles are connected via kNN graphs if they are close in the $\eta-\phi$ space shown in Fig. \ref{fig:pileup_example}.
Distribution shifts arise from pile-up (PU) conditions (primarily structure shift), where PU level indicates the number of OC in the beam, and from the particle generation processes gg$\rightarrow$qq and qq$\rightarrow$gg (primarily feature shift).


\textbf{Arxiv~\cite{hu2020open}} is a citation network between all Computer Science (CS) arXiv papers. 
Distribution shifts originate from different time.
Our model is pretrained on the earlier time span 1950 to 2007/ 2009/ 2011 and test on later 2014 to 2016 and 2016 to 2018.


\textbf{DBLP and ACM~\cite{tang2008arnetminer, wu2020unsupervised}} are two citation networks. 
The model is trained on one network and adapted to the other to predict the research topic of a paper (node).
% \hans{I was thinking to remove DMLP ACM because the variance it too large same in PairAlign and SOGA did pretty well on A to D}

\textbf{Baselines} We compare TSA with six baselines. For non-graph TTA methods, we include TENT \cite{wang2020tent}, LAME \cite{boudiaf2022parameter}, and T3A \cite{iwasawa2021test}. Note that TENT is only applied to the classifier, as GNNs typically do not include batch normalization layers. GTrans \cite{jin2022empowering}, SOGA \cite{mao2024source}, and AdaRC \cite{bao2024adarc} are direct comparisons in GTTA.
AdaRC is limited to GPRGNN due to its design. We present the results for GraphSAGE \cite{hamilton2017inductive} in the main paper, while the results for GPRGNN \cite{chien2020adaptive} (including AdaRC) are provided in the Appendix.
% TSA results are presented for one epoch of optimization for Eq. \ref{eq:loss}.

% \vspace{-3mm}
\subsection{Result Analysis}
From the MAG dataset results in Table~\ref{table:mag}, TSA combined with boundary refinement techniques consistently achieves top performance among all baselines. Compared to non-graph TTA baselines, such as TENT vs. TSA-TENT, TSA also provides considerable improvements over the original adaptation results. Notably, the benefits of TSA after neighborhood alignment are more pronounced in scenarios with larger neighborhood shifts, such as adaptation from the US to FR, as reflected in the dataset shift metrics (Table~\ref{table:magstats}). 

Among all baselines, T3A consistently outperforms as a representative non-gradient-based method that refines the decision boundary, while TENT struggles in most scenarios. This is primarily due to the relatively imbalanced class distribution in the MAG datasets, where evaluation is conducted on the top 19 classes. Since TENT learns feature-wise transformations through entropy optimization on the target graph, it tends to be biased toward the dominant class. Other GTTA methods, such as GTrans, achieve performance comparable to ERM, whereas SOGA sometimes underperforms due to its strong reliance on the graph homophily assumption, which is often weak in sparse graphs with many classes, such as MAG and Arxiv.

The results for the synthetic datasets are presented in Table~\ref{table:syn}.
The first six columns correspond to experiments conducted under imbalanced source training.
Compared to real datasets, we observe more significant advantages from TSA. TENT outperforms LAME and T3A, as it directly maximizes the most probable class through entropy minimization. In contrast, LAME incorporates regularization, while T3A relies on distance to prototypes, making it less sensitive to majority-class dominance \cite{zhang2022divide}. When combined with \proj, we observe similar results across all three variants, indicating that the primary adaptation ability stems from \proj rather than non-graph TTA methods. While GTTA baselines perform better than ERM, they still fall short of our TSA-based methods. \proj also demonstrates strong performance under additional shifts in SNR caused by degree shifts. 
Additionally, we consider training a pretrained model on a balanced source domain under the same edge connection probability as the previous two columns.
The last two columns demonstrate better performance than the previous two under balanced training, even though they encounter the same shift in label connection.
This showcases that dataset imbalance is a severe issue in source training, which aligns with the worst-case error described in Theorem \ref{theory:decomposeerror}. 

The pileup results in Table~\ref{table:pileup} demonstrate the effectiveness of TSA-based methods under both neighborhood shift and label shift, with each module contributing in different ways. The neighborhood alignment module provides significant improvements when generalizing from a low PU level to a high PU level, as this scenario is primarily dominated by neighborhood shift. Conversely, when adapting from a high PU level to a low PU level, label shift becomes the dominant factor. In this case, incorporating non-graph TTA techniques is crucial, while neighborhood alignment offers an additional but smaller benefit. Moreover, TENT can be vulnerable when generalizing from an imbalanced source graph to a balanced target graph. 
This is because source training on high PU graphs suffers from class imbalance, making entropy minimization non-robust due to a mismatched decision boundary when adapting to low PU graphs. The last two columns, which represent the same PU level but different physical signals, show minimal benefits from TSA. This is expected, as these scenarios primarily exhibit feature shifts rather than structure or label shifts.

The results from Arxiv and DBLP/ACM in Table~\ref{table:arxiv} confirm that TSA can be effectively integrated with non-graph TTA methods to mitigate feature shift while also facilitating reasonable adjustments for structure shifts. These citation networks exhibit relatively mild structure shifts, leading to closely comparable performance across all baselines. TENT and TSA-TENT stand out by maximizing the majority classes through entropy minimization. Nonetheless, we still observe meaningful improvements with the addition of TSA when compared to the corresponding baselines, demonstrating its effectiveness in enhancing adaptation.




% Neighborhood alignment are essential in handling neighborhood shift, SNR-based representation adjustment can alleviate SNR shift and decision boundary refinement should be added on top of the other techniques to compensate additional label and feature shift.



\subsection{Analysis of $\alpha$ for SNR-induced refinement}
\label{subsec:alpha}
As defined in Eq.\ref{eq:alpha_ratio}, $\alpha$ is learned, depending on node degrees and GNN layer depth. The trend of $\alpha$ in Fig.~\ref{fig:pa_vis} (d) aligns with our expectations: earlier GNN layers require less attention to self-node representations (small $\alpha$), while deeper layers increasingly rely on them. Additionally, the plot reveals a correlation with node degrees, where nodes with higher degrees (and thus higher SNR) place greater weight on neighborhood-aggregated messages. However, the overall effect of different GNN layers appears to be greater than that of varying node degrees. % as we observed that $\alpha$ for does not goes up even when the degree becomes larger. 

\subsection{Ablation Studies}
In Table~\ref{table:ablation} (Appendix), we examine the effects of neighborhood alignment and SNR-based representation adjustments. The results consistently show that incorporating both modules yields the best performance. In some cases, removing SNR adjustments leads to a greater performance drop compared to removing neighborhood alignment. However, this does not necessarily indicate that neighborhood alignment is less important. Instead, we think this effect arises because the additional backpropagation training for SNR adjustments amplifies their contributions. 

% \begin{figure}[t]
%     \centering
%         \centering
%         \includegraphics[width=0.45\columnwidth]{icml2025/figures/uscn_alpha.pdf}
%         \centering
%         \includegraphics[width=0.45\columnwidth]{icml2025/figures/cnus_alpha.pdf}
%     \caption{Analysis of SNR adjustment $\alpha$ with respect to different layers and node degrees on MAG $US\rightarrow CN$ and $CN \rightarrow US$.}
%     \label{fig:side_by_side}
% \end{figure}





