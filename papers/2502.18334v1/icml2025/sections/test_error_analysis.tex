\section{Test Error Analysis}
\label{Sec:Test Error Analysis}

% In this section, we first formally define the distribution shift on graphs, categorizing it into feature shift, label shift, and conditional structure shift (CSS).
% Addressing CSS can be reduced to addressing neighborhood shift in terms of first-order alignment \pan{too technical, in the expectation sense?, or not saying this. Later, say the missing part} under GNN mean pooling.
% Next, we examine how these shifts impact the generalization gap in GTTA given a pretrained source model.
% Lastly, we conduct an empirical investigation to corroborate and provide deeper insights into our theoretical findings, discussing additional second-order adjustment \pan{variance} strategies that are necessary but not covered in the theoretical analysis.
In this section, we characterize the generalization error between source and target graphs and explicitly attribute it to three different kinds of shifts: label shift, feature shift, as well as neighborhood shift. Motivated by our theoretical analysis on the generalization error, we then propose \proj algorithm to minimize the across-graph generalization error. 
% how different shifts between the source and target graph can impact the generalization gap. Our error decomposition echoes the findings in previous GDA works, but in a more formal bound \han{Bound is always formal. Can we claim our bound to be tighter? More general?}, which further motivates the \proj algorithm design. 

\begin{figure*}[t]
\centering
\begin{tabular}{c|cc|c}
(a) Reliability of $\gamma$ 
% &\multicolumn{2}{|c|}{ (b) Decision Boundary Refinement}
&(b) Source &(c) Nbr. Align 
& (d) SNR $\alpha$  \\
\includegraphics[width=0.23\textwidth]{icml2025/figures/uncertainty_nbr.pdf} &
\includegraphics[width=0.22\textwidth]{icml2025/figures/aggre_src.pdf} &
% \includegraphics[width=0.18\textwidth]{icml2025/figures/aggre_ss.pdf} &
\includegraphics[width=0.22\textwidth]{icml2025/figures/aggre_no_nbr.pdf} &
\includegraphics[width=0.23\textwidth]{icml2025/figures/uscn_alpha.pdf} \\

\includegraphics[width=0.23\textwidth]{icml2025/figures/uncertainty_str.pdf} &
\includegraphics[width=0.22\textwidth]{icml2025/figures/output_src.pdf} &
% \includegraphics[width=0.18\textwidth]{icml2025/figures/output_ss.pdf} &
\includegraphics[width=0.22\textwidth]{icml2025/figures/output_no_nbr.pdf} &
\includegraphics[width=0.23\textwidth]{icml2025/figures/cnus_alpha.pdf} 
\end{tabular}
\vspace{-4mm}
\caption{(a) Comparison of neighborhood alignment with $\mgamma$ from model prediction and Oracle on the CSBM graphs~\cite{deshpande2018contextual}. The top (or bottom) subfigures represents the assignment under neighbor shift (or neighbor shift plus label shift, respectively). Nodes are grouped by the entropy of their soft pseudo labels and the y axis shows the accuracy after assigning $\mgamma$. Ideally, a correct assignment (red) would lead to near-perfect accuracy.
However, the assignment based on pseudo labels is far from optimal (blue).
%(b) and (c) top \pan{} represent the t-SNE visualization of the node representations. (b) and (c) bottom visualize the node representations before passing through the classifier with its decision boundary on CSBM. 
From (b) to (c), the figure with the t-SNE visualization of node representations indicates a model trained from the source domain (CSBM with a label distribution $[0.1, 0.3, 0.6]$) to the target domain (CSBM with a label distribution $[0.3, 0.3, 0.3]$). The color of the nodes represents the ground-truth labels. The top subfigures of (b) and (c) show the output given by the GNN encoder while the bottom subfigures show the classifier decision boundaries.  
%Detail discussed in Sec. \ref{subsec:boundary}.
(d) Analysis of SNR adjustment $\alpha$ with respect to different layers and node degrees on MAG.
Detail discussed in Sec. \ref{subsec:alpha}.
}
\label{fig:pa_vis}
\vspace{-2mm}
\end{figure*}

\subsection{Distribution Shifts on Graphs} 
\label{sec:shift}
Distribution shifts on graphs were formally studied in previous GDA works \cite{wu2020unsupervised, liao2021information, zhu2021shift, wu2022handling, you2023graph, zhu2023explaining}.  
Following their definition, we categorize shifts in graphs into two types: feature shift and structure shift. For simplicity, our analysis is based on a data generation process: $\rmX \leftarrow \rmY \rightarrow \rmA$,  
where graph structure and node features are both conditioned on node labels.

\begin{definition}
\label{def:attrshift}
(Feature Shift). Assume node features $x_u, u \in \gV$ are i.i.d sampled given labels $y_u$, then we have $\prob(\rmX|\rmY)=\prod_{u\in\gV}\prob(X_u|Y_u)$. We then define the feature shift as $\probs(X_u|Y_u)\neq \probt(X_u|Y_u)$.
\end{definition}


\begin{definition}
\label{def:strucshift}
(Structure Shift). As graph structure involves the connecting pattern between labels, we consider the joint distribution of the adjacency matrix and labels $\prob(\rmA,\rmY)$, where \emph{Structure shift}, denoted by $\probs(\rmA,\rmY) \neq \probt(\rmA,\rmY)$,  can be decomposed into as \emph{conditional structure shift (CSS)} $\probs(\rmA|\rmY) \neq \probt(\rmA|\rmY)$ and \emph{label shift (LS)} $\probs(\rmY) \neq \probt(\rmY)$. 

\end{definition}

% While feature shift and label shift have been widely studied  in non-graph literature, CSS is specific to graph data as it manifest the connection discrepancy given the same label. 
% Notably, in real-world datasets, these shifts often coexist \cite{li2022out, zhang2024survey}.

% According to Theorem 3.3 in \cite{liu2024pairwise}, the sufficient condition for mitigating the influence of CSS under GNNs lies in aligning \emph{degree shift} $\probs(D_u | Y_u)\neq\probt(D_u | Y_u)$, where the node degrees within the
% same label differs, and \emph{neighborhood shift} $\probs(Y_v|Y_u, v \in\neighbor_u)\neq\probt(Y_v|Y_u, v \in\neighbor_u)$, where the neighboring node label connections
% within the same class differs.

\subsection{Theoretical Analysis}

% Among the defined shifts, \textit{feature shift} and \textit{label shift} have been studied for TTA in non-graph settings.  CSS has been studied for training-time GDA~\cite{liu2024pairwise}. % showing the impacts from both \textit{neighborhood shift:} $\probs(Y_v|Y_u, v \in\neighbor_u)\neq\probt(Y_v|Y_u, v \in\neighbor_u)$ and \textit{degree shift:} $\probs(|\neighbor_u||Y_u) =\probt(|\neighbor_u||Y_u)$. 
% % However, there is no previous works investigating the overall generalization gap in GTTA. 
% In this works, we aim to extend the scope to GTTA by characterizing how above shifts impact the target domain error. The analysis  
%  further inspires our later algorithm framework. %respectively given the pretrained source model in GTTA. 

% In practice, mean pooling is commonly preferred in GNNs for node
% classification tasks, as it smoothens the graph signal by
% reducing variance within the pooling, which shows superior performance.
% Since mean pooling directly addresses the variation of magnitude in neighborhood information, first-order expectation of degree distribution is naturally aligned. \shikun{naturally aligned?}
% As a result, we first focus on addressing neighborhood shift $\probs(Y_v|Y_u, v \in\neighbor_u)\neq\probt(Y_v|Y_u, v \in\neighbor_u)$ to mitigate CSS in our analysis.



% \shikun{For the bound, we need to revise the proof later on to provide more clear indication of mean pooling assumption, what we mean by consider the first-order information and how that reduce the term. The original degree term expansion is not needed, we can just start from the very beginning to expand over $\pi$ and $\omega$ and be clear about why $\omega$ is not considered. Need some revision on the bound and the corresponding discussion}
% \shikun{also, the notation here is not consistent, the subscript $v$ or $v_t$. Check the consistency again when modifying the proof and the bound}

Let $\errors(g \circ \phi)$ denote the error of a pretrained GNN on the source domain and $\errort(g \circ \phi)$ the error of the model when applied to a test graph in the target domain.
Inspired by \citet{tachet2020domain}, we provide an upper bound on the error gap between source and target domains, showing how a pretrained GNN (e.g., ERM) can be influenced. 
% \vspace{-1mm}
%\begin{definition} 
We denote the \emph{balanced error rate} of a pretrained node predictor $\predy_u$ on the source domain as
%\label{sber definition}
    %\begin{align*}
        $\balerrorrate{\predy_u}{\rvY_u} \coloneq \max_{j \in \mathcal{Y}} \probs(\predy_u\neq\rvY_u | \rvY_u=j).$
    %\end{align*}
%\end{definition}

%\vspace{-1mm}
\begin{restatable}[Error Decomposition Theorem]{theorem}{decomposeerror}
\label{theory:decomposeerror}
Suppose $\gG^{\gS}$ and $\gG^{\gT}$, we can decouple both graphs into independent ego-networks (center nodes and 1-hop neighbors).
For any classifier $g$ with a mean pooling GNN encoder $\phi$ in node classification tasks, we have the following upper bound for on the error gap between source and target under feature shift and structure shift: 
%{\small
\begin{align*}
&\abs{\errors(g \circ \phi) - \errort(g \circ \phi)} \\ 
&\leq   
\balerrorrate{\predy_u}{\rvY_u}\cdot
    \cbrace{\underbrace{
    \rpar{2\text{TV}(\probs(\rvY_u), \probt(\rvY_u)}
    }_{\text{Label Shift}}
   \\
   % &+
   % \underbrace{
   % \E_{\rvY}\spar{
   %  \rpar{
   %  \max_{k \in \mathcal{Y}}
   %  \abs{1 -\frac{\probt(\rvY_{v_t}=k | \rvY_u=j, v_t\in\neighbor_u)}{\probs(\rvY_{v_t}=k | \rvY_u=j, v_t\in\neighbor_u)} }
   %  }^{d_{m,y}}
   %  }
   %  }_{\text{Neighborhood Shift}}
   %  }\\
   %  &+\underbrace{\max_{i\
   %  \neq j}\E_{\{Y_v \}}\spar*{|\probs_k - \probt_k |}}_{\text{Feature shift}}
    &+\underbrace{\E_{Y_u}^\gT\spar{\max_{k\in \gY} \abs{1-\frac{\probt(Y_v=k|Y_u,v\in \neighbor_u)}{\probs(Y_v=k|Y_u,v\in \neighbor_u)}}}  }_{\textbf{Neighborhood shift}}}
    + \underbrace{\Delta_{CE}}_{\text{Feature shift}}
\end{align*}
%}
where 
    $TV(\probs(\rvY_u), \probt(\rvY_u))$ % \coloneq \frac{1}{2}\sum_{j\in\gY}\abs{\probs(\rvY_u=j) - \probt(\rvY_u=j)}$ 
    is the total variation distance between the source and target label distributions. % of source and target
    and 
    $\Delta_{CE}$ is the error gap that exists if and only if feature shift exists.
\end{restatable}

% \pan{first term should be in total variation; it is not good to mix sum and expectation; why there is a node association between source and target domain; the degree is a distribution, does max over d make sense? what is the neighbor shift here?; where is i,j in the feature shift?; Does the above bound assume mean pooling or anything?}

Our bound aligns with the findings of \citet{liu2024pairwise}, which highlight the impact of neighborhood shift, label shift, and feature shift on generalization to the target graph. We extend this understanding by deriving an explicit error bound. Notably, neighborhood shift is reduced from conditional structure shift given the assumptions  in Thm.\ref{theory:decomposeerror}.

% The error decomposition gives a way to analyze how node predictions in graphs are influenced by feature shift, label shift, degree shift and neighborhood shift \pan{no degree shift above} \pan{here define neighborhood shift and explain it, and cite Liu et al. here. And point out to fig5 c}, where the last three manifest as structure shift. The bound provide several insights: 1) Source model worst-class accuracy determines the test error upper bound.
% 2) In neighborhood shift, low density of the neighboring 
%  node label in the source domain contributes to the error.
% 3) Degree shift \pan{no label shift} and neighborhood shift of the majority classes in target domain contribute primarily to the error.
% 4) If the source neighbor label density is two times less than the target neighbor label density $2<\max\frac{\pit{t_k|u_j}}{\pis{t_k|u_j}}$, $d_{m,y}$ should be the node that contains the highest degree given the class label.
% Otherwise, $0<\max\frac{\pit{t_k|u_j}}{\pis{t_k|u_j}}<2$,  low-degree nodes contribute primarily to the error and $d_{m,y}$ should be the node that contain the lowest degree given the class label. \pan{the degree shift notation here looks weird}

% Following the arguments in \cite{liu2024pairwise} regarding the handling of CSS, it is important to note that in practical applications, mean pooling is commonly preferred in GNNs for node classification tasks. This is because it smoothens the graph signal by reducing noise in node representations, % within the pooling, 
% leading to superior performance. Aligning the first-order expectations of \pan{do we need to emphasize first order here?} the source and target neighborhood distributions should, therefore, result in satisfactory performance \cite{xu2018powerful}. Our proposed error gap primarily focuses on demonstrating the impact of \textit{neighborhood shift} \pan{why not CSS? neighorhood shift was not defined} on first-order statistics \pan{ here?}. Furthermore, we consider the impact of \textit{degree shift} as a change in the signal-to-noise ratio (SNR) of the aggregated neighborhood message, which will be discussed in Sec.~\ref{subsec:empirical_invest}.

 

% \subsection{Empirical Investigation}

% \label{subsec:empirical_invest}

% % To provide empirical validation to support our theoretical findings, we conduct a fine-grained study on synthetic datasets using contextual stochastic block model (CSBM) \cite{deshpande2018contextual}.
% % We use a one-layer GraphSAGE \cite{hamilton2017inductive} as the GNN encoder $\phi$ and a one-layer MLP with batch normalization \cite{batchnorm} as the classifier $g$. 
% % The model is trained on an imbalanced source graph with a label ratio of $[0.1, 0.4, 0.5]$ and analyzed for adaptation on two domains:
% % 1) Under CSS and label shift, where label ratio becomes $[1/3, 1/3, 1/3]$.
% % 2) SNR decreases from degree shift, where no neighborhood or label shift occurs.
% % See Appendix for more experimental details.

% % Below we summarize the main insights of our results:


% % To provide empirical validation to support our theoretical findings, we conduct a fine-grained study on synthetic datasets using contextual stochastic block model (CSBM) \cite{deshpande2018contextual} to mimic different types of shifts in our bound. We use a one-layer GraphSAGE \cite{hamilton2017inductive} as the GNN encoder $\phi$ and a one-layer MLP with batch normalization \cite{batchnorm} as the classifier $g$. 
% % The model is trained on a source graph as in Fig.\ref{fig:pa_vis} top (a). 
% \pan{if just visualization, no need to be a new section} \pan{also too long}
% \pan{better thing is to show why naively extending pairalign cannot solve the issue.}
% In this section, we (aim to demonstrate empirically) \pan{visualize} how the shifts identified in Thm.~\ref{theory:decomposeerror}, which contribute to target test time error, negatively impact model adaptation. We conduct a detailed study using synthetic datasets generated by the contextual stochastic block model (CSBM) \cite{deshpande2018contextual} to simulate various types of shifts outlined in our bound. For this analysis, we employ a one-layer GraphSAGE \cite{hamilton2017inductive} as the GNN encoder $\phi$ and a one-layer MLP with batch normalization \cite{batchnorm} as the classifier $g$. 

% The model is initially trained on a fixed source graph, and then adapted to target graphs that exhibit distinct types of shifts. We present the final node representation space after mean pooling aggregation and the corresponding classifier decision boundary for both the source and target models in Fig~\ref{fig:pa_vis}, illustrating the reasons for performance degradation.

% Based on the error bound, we construct three target graphs, each constructed by one of the following shifts: \textit{feature shift}, \textit{neighborhood shift}, and \textit{label shift}. Detailed experimental settings are provided in Appendix~\ref{}.

% \paragraph{Feature Shift} \pan{use \textbf{Feature shift} not paragraph} As illustrated in Fig~\ref{fig:pa_vis} top (b), feature shift in graph-structured data also leads to misalignment in the latent representation space, a phenomenon well-explored in the non-graph TTA literature \cite{wang2020tent, boudiaf2022parameter, iwasawa2021test}. This issue can be addressed by adapting existing techniques. Therefore, this paper focuses on addressing the remaining structure shifts, specifically neighborhood shift and label shift, in the context of graph test-time adaptation (GTTA). \pan{can be substantially trimmed}

% \paragraph{Neighborhood Shift} From Fig. \ref{fig:pa_vis}, top (a) to (c), we observe a misalignment of aggregated node representations caused by discrepancies in neighborhood distribution, even in the absence of feature shift. Notably, neighborhood shift results in aggregated node representations that are less distinguishable, in contrast to the misaligned yet distinguishable representation space caused by feature shift. Consequently, merely aligning the feature distribution is insufficient to address the neighborhood shift. Alignment over the neighborhood distribution is the key to mitigate the impact of neighborhood shift as indicated in the bound. 
% From Fig. \ref{fig:pa_vis} top (a) to (b), we can observe that the neighborhood information degrades as structure shift occurs. 
% Based on the conclusion from Theorem \ref{theory:decomposeerror}, we employ Oracle values to bootstrap neighboring nodes for first-order alignment (detailed methodology in Sec. \ref{subsec:1stalign}). 
% Fig. \ref{fig:pa_vis} top (c) shows the neighborhood hidden representations after alignment, which match those in the source domain.
% As a result, we achieve domain-invariant representations across the source and target graphs, i.e., $\probs(H_u|Y_u) = \probt(H_u|Y_u)$.






% Note that SNR refers to the second moment, providing an additional perspective on aligning neighborhood information. 
% In contrast, our theory focuses on the first moment of the generalization error.

% \hans{not final node representations. did not contain self representation only aggregated neighborhood representation}




