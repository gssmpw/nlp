\section{Preliminaries and Related Works}

\subsection{Notations and Problem Setup}

We use upper-case letters, such as $Y$ to represent random variables, lower-case letters, such as $y$ to represent their realization.
The calligraphic letters, such as $\gY$ denote the domain of random variables.
We use bold capital such as $\rmY$ to represent the vectorized corresponds, i.e., a collections of random variables.
The probability distribution of a random variable $Y$ for a realization is expressed as $\prob(Y=y)$.

% CheatSheet
% \rx random variable x
% \rvx random vector x
% \ervx element of random vector x
% \rmX random matrix X
% \ermX element of random matrix X
% \vx vector x
% \evx elements of vector x
% \mX matrix X


% \paragraph{Test-Time Adaptation (TTA).}
% Assume the model consists of a feature encoder $\phi:\gX\rightarrow \gH$ and a a classifier $g: \gH \rightarrow \gY$ and each domain $\gU \in \{\gS,\gT\}$ consists of a joint feature and label distribution $\probu(X,Y)$.
% In TTA, the model is first trained on the source domain $\probs(X,Y)$. The objective is to adapt the model to minimize the target test error $\errort(g\circ\phi)=\probt(g(\phi(X))\neq Y)$ \emph{without} accessing the source domain.

\paragraph{Graph Neural Networks (GNNs).} 
We let $\gG = (\gV,\gE)$ denote an undirected and unweighted graph with the symmetric adjacency matrix $\mA\in\R^{N\times N}$ and the node feature matrix $\mX=[x_1,\dots,x_N]^T$.
GNNs utilize neighborhood information by encoding $\mA$ and $\mX$ into node representations $\{h_{v}^{(k)}, v \in \neighbor_u\}$.
With $h_u^{(1)}=x_u$, the message passing in standard GNNs for node $v$ and each layer $k \in [L] \coloneqq \{1,\dots,L\}$ can be written as
\begin{equation}
    h_u^{(k+1)} = \text{UPT}(h_u^{(k)}, \text{AGG}(\{h_{v}^{(k)}, v \in \neighbor_u\})) \label{eq:gnn}
\end{equation}
where $\neighbor_u$ denotes the set of neighbors of node $u$, which $|\neighbor_u|$ represent the node degree $d_u$.
The AGG function aggregates messages
from the neighbors, and the UPT function updates the node
representations. 

\paragraph{Graph Test-Time Adaptation (GTTA).} 

% Assume the model consists of a feature encoder $\phi:\gX\rightarrow \gH$ and a a classifier $g: \gH \rightarrow \gY$ and each domain $\gU \in \{\gS,\gT\}$ consists of a joint feature and label distribution $\probu(X,Y)$.
% In TTA, the model is first trained on the source domain $\probs(X,Y)$. The objective is to adapt the model to minimize the target test error $\errort(g\circ\phi)=\probt(g(\phi(X))\neq Y)$ \emph{without} accessing the source domain.

% We focus on the node classification tasks in GTTA. 
% Similar to TTA, the model is trained on source graph $\gG^{\gS}=(\gV^{\gS}, \gE^{\gS})$ and the goal is to enhance model performance on test graph $\gG^{\gT}=(\gV^{\gT}, \gE^{\gT})$.
% The encoder $\phi$ is a switched to graph based method such as a GNN.
% For node classification tasks, we aim to minimize the test error $\errort(g\circ\phi)=\probt(g(\phi(X_u, \rmA))\neq Y_u)$ \emph{without} accessing $\gG^{\gS}$.

Assume the model consists of a GNN feature encoder $\phi:\gX\rightarrow \gH$ and a classifier $g: \gH \rightarrow \gY$. The model is trained on source graph $\gG^{\gS}=(\gV^{\gS}, \gE^{\gS})$ with node labels $\mathbf{y}^\gS$ and the goal is to enhance model performance on test graph $\gG^{\gT}=(\gV^{\gT}, \gE^{\gT})$ with distribution shifts that will be defined in Sec.~\ref{sec:shift}. For node classification tasks, we aim to minimize the test error $\errort(g\circ\phi)=\probt(g(\phi(X_u, \rmA))\neq Y_u)$ \emph{without} accessing $\gG^{\gS}$.
%We use the prime $(')$ symbol to denote prediction from TTA and the hat $(\, \hat{}\,)$ symbol to denote the soft prediction.
%For example, $\hat{y}'$ represents the soft prediction from TTA.


\subsection{Related Works}
%TTA aims to adapt a pre-trained model from the source domain to unlabeled target domain without accessing the source domain during adaptation \cite{liang2024comprehensive}.
\textbf{Test-time Adaptation}
\emph{Test-time training} \cite{sun2020test, liu2021ttt, bartler2022mt3} adapts the source model to the target domain but requires to first add a customized self-supervised losses in model pretraining.
In contrast, our setup falls into the category of \emph{fully test-time adaptation} \cite{wang2020tent,liang2024comprehensive}, where we do not alter the model training pipeline.  %agnois arbitrary pretrained model. 
 Tent \cite{wang2020tent} adapts the batch normalization parameters by minimizing the entropy, which motivated following-up studies on normalization layer adaptation
 %Other works were motivated by this and perform adaptation in normalizing layers 
 ~\cite{gong2022note, zhao2023delta, lim2023ttn, niu2023towards}. 
%In addition to normalization layer adaptation, 
Some TTA works directly modify the classifier's prediction, such as
 LAME \cite{boudiaf2022parameter} that directly refines the model's soft prediction by applying a regularization %that enforces  
 %label consistency for neighboring points 
 in the feature space, 
T3A \cite{iwasawa2021test} that %updates the classifier's last layer using prototypes from pseudo-labels and 
classifies test data based on the distance to the pseudo-prototypes derived from pseudo-labels, 
TAST \cite{jang2022test} and PROGRAM \cite{sunprogram} that extend T3A through constructing more reliable %pseudolabel through  nearest-neighbor and 
prototype graphs.
% self-training a new classifier with nearest neighbor information and construct pseudo label through ensemble.
However, the above methods are designed for image-based applications and cannot handle the shifts in neighborhood information of graph data. %do not explicitly mitigate discrepancies in aggregated node labels induced by shifts in neighborhood information.



% \paragraph{Test-Time Adaptation on Label Shifts.}
% Label shift studies cases where the marginal label distributions differ $\probs(Y)\neq\probt(Y)$ but the conditional distributions given class remain fixed $\probs(X|Y)=\probt(X|Y)$. Based on the assumption, the prediction of the model in target domain can be adapted as $\hat{Y}=\argmax_{y\in\gY}g(\phi(X)) + \log (\frac{\probt(Y)}{\probs(Y)})$. Exiting works that are originally developed for domain adaptation are applicable to the TTA framework by having the model saved with source label distribution or the confusion matrix. 
% However the assumption of fixed class-conditional distributions do not hold on graphs, as structure shifts can can affect the aggregated node labels, thereby causing attribution shifts on node representations. \pan{this should be put in the intro to argue against the non-graph test-time works}

% \paragraph{Test-Time Adaptation on Graphs} 
\textbf{Graph Test-time Adaptation} Studies on GTTA are in two categories - node and graph classification.
% Existing work on graph classification 
Graph classification problems can treat each graph as an i.i.d.\ input, allowing more direct extension of image-based TTA techniques to graphs \cite{chen2022graphtta, wang2022test}.
Our work focuses on node classification. %, where the connection patterns introduce unique challenges.
GTrans \cite{jin2022empowering} proposes to augment the target graph at the test time by optimizing a contrastive loss by generating positive views from DropEdge \cite{rong2019dropedge} and negative samples from the features of the shuffling nodes \cite{velivckovic2018deep}. 
GraphPatcher \cite{ju2024graphpatcher} learns to generate virtual neighbors to improve low-degree nodes classification. 
SOGA \cite{mao2024source} designs a self-supervised loss that relies on  mutual information maximization and homophily assumption.
These works are mostly built upon heuristics and may not address  structure shifts in principle.

% Recently, AdaRC \cite{bao2024adarc} partially tackles degree and homophily shifts by adapting the hop-aggregation parameters to restore representation. However, AdaRC still does not fully handle structure shifts.
% We provide a detailed comparison with AdaRC in Sec. \ref{subsec:TSA_CSS}. 
% Notably, the aforementioned studies overlook the aspect of label shift on graphs which entails structure shift.
% To the best of our knowledge, this work is the first to study structure shift, including label shift, in a principled framework in GTTA. \pan{the last sentence may not be needed. Still the importance of label shift is overemphasized unless we have algorithms dedicated to it.}