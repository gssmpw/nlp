\newpage
\section{Detailed Experimental Setup}
\label{app:exp}

\subsection{Datasets Setup}
For Arxiv, MAG, and DBLP/ACM we follow the setting of \cite{liu2024pairwise}. Hence we discuss the setting of synthetic dataset and pileup in the following:

\paragraph{Synthetic Dataset.}
The synthetic dataset is generated by the contextual stochastic block model (CSBM) \cite{deshpande2018contextual}. 
The generated graph contains 6000 nodes and 3 classes.
We alter the edge connection probability matrix $\mathbf{B}$ and the label ratio $\prob_Y$
Specifically, the edge connection probability matrix $\mathbf{B}$ is a symmetric matrix given by
\[
\mathbf{B} = 
\begin{bmatrix}
p & q & q \\
q & p & q \\
q & q & p
\end{bmatrix}
\]
where $p$ and $q$ indicate the intra- and the inter-class edge probability respectively.
We assume the node features are sampled from a Gaussian distribution $X_u \sim \gN(\bm{\mu}_u, \sigma\mI)$ where we set $\sigma=0.3$ as well as $\bm{\mu}_u=[1,0,0]$ for class 0, $\bm{\mu}_u=[0,1,0]$ for class 1, and $\bm{\mu}_u=[0,0,1]$ for class 2.

\begin{itemize}
    \item The source graph for setting 1-6 has $\prob_Y = \left[ 0.1,0.3,0.6 \right]$, with $p = 0.01$ and $q = 0.0025$.
    
    \item For neighborhood shift we fix the same class ratio but change the connection probability:
    \begin{itemize}
        \item Condition 1: $p = 0.005$, $q = 0.00375$.
        \item Condition 2: $p = 0.005$, $q = 0.005$.
    \end{itemize}

    \item For condition 3 and 4 we  introduce degree shift for SNR discrepancy.
    \begin{itemize}
        \item Condition 3: $p = \frac{0.005}{2}$, $q = \frac{0.00375}{2}$.
        \item Condition 4: $p = \frac{0.005}{2}$, $q = \frac{0.005}{2}$.
    \end{itemize}

    \item For condition 5 and 6 we investigate structure shift under training from give training is from the imbalanced source label ratio.
    \begin{itemize}
        \item Condition 5: $\mathbb{P}_Y =  \left[ 1/3, 1/3, 1/3\right]$ $p = \frac{0.005}{2}$, $q = \frac{0.00375}{2}$.
        \item Condition 6: $\mathbb{P}_Y =  \left[ 1/3, 1/3, 1/3\right]$ $p = \frac{0.005}{2}$, $q = \frac{0.005}{2}$.
    \end{itemize}

    \item The source graph for Condition 7-8 has $\prob_Y = \left[ 1/3, 1/3, 1/3\right]$, with $p = 0.01$ and $q = 0.0025$.

    \item For condition 7 and 8 we investigate structure shift under training from balanced source ratio. The $p$ and $q$ in Condition 7 and 8 is the same as in Condition 5 and 6, respectively.
    \begin{itemize}
        \item Condition 7: $\mathbb{P}_Y = [0.1, 0.3, 0.6]$, $p = \frac{0.005}{2}$, $q = \frac{0.00375}{2}$.
        \item Condition 8: $\mathbb{P}_Y = [0.1, 0.3, 0.6]$, $p = \frac{0.005}{2}$, $q = \frac{0.005}{2}$.
    \end{itemize}
\end{itemize}

\paragraph{Pileup.} Under the study of different PU levels we use the data from signal $gg$. Compared to the Pileup datasets used in the \cite{liu2024pairwise}, we explicitly label all types of particles to better align the neighborhood distribution. Note that the charged particle can achieve all most perfect recall as their label information is encoded in their feature, hence the four-class classification still reduces into a binary classification between LC neutral particles and OC neutral particles.
For the study of different data generation process, we use PU10 from signal $gg\rightarrow qq$ and $qq \rightarrow gg$.

\subsection{Pretraining Setup}
\paragraph{Model architecture and Pretraining} 
We use GraphSAGE as the model backbone, with a 3-layer mean pooling GNN as the encoder and a 2-layer MLP with a batch normalization layer as the classifier.
The hidden dimension of the encoder is set to 300 for Arxiv and MAG, 50 for Pileup, 128 for DBLP/ACM, and 20 for the synthetic datasets.
The hidden dimension of the classifier is set to 300 for Arxiv and MAG, 50 for Pileup, 40 for DBLP/ACM, and 20 for the synthetic datasets.
For the GPRGNN backbone, we follow the same configuration but increase the number of encoder layers to 5.
All models are trained for 400 epochs with the learning rate set to 0.003. A learning rate scheduler is applied with a decay rate of 0.9.
All experiments are repeated 5 times under different initializations and data splits to ensure consistency.

% \paragraph{Hardware}
% We conduct all experiments on NVIDIA RTX A600 with 48G memory.

\subsection{Evaluation and Metric}
The source graph is splitted into train/val/test 60/20/20 during the pretraining stage. The target graph is splitted into labeled/unlabeled 3/97 during test time. 
We use the the labeled nodes in the target graph to do hyperparameter tuning and select the model with the optimal validation score.
We follow the metrics in~\cite{liu2024pairwise} for evaluation. MAG, Arxiv, DBLP/ACM, and synthetic datasets are evaluated with accuracy.
For the MAG datasets, only the top 19 classes are evaluated.
Pileup is evaluated with f1 score.

\subsection{Hyperparameter Tuning}
Below we introduce the search space of the hyperparameters. LAME is a parameter-free approach so we do not tune it.

\begin{itemize}
\item TSA introduces three hyperparameters (1) the learning rate $lr$ for optimizing $\alpha$, (2) the ratio $\rho_1$ for reliable assignment of $\gamma$ based on entropy $H(\hat{y})\leq \rho_1 \cdot \ln ( |\gY|)$, and (3) the ratio $\rho_2$ for filtering out unreliable hard pseudo-labels in Eq.~\ref{eq:loss} based on entropy $H(\hat{y})\leq \rho_2 \cdot \ln ( |\gY|)$.
We select $lr$ from $\{0.001, 0.01, 0.05, 0.1 \}$, $\rho_1$ from $\{0, 0.01, 0.1, 0.5\}$, and $\rho_2$ from $\{0.1, 1.0\}$.
We present the results updated by one epoch.
We observe that given the same domain shifts, the optimal hyperparameters of $\rho_1$ may differ due to different performance of the boundary refinement approaches (TENT, T3A, and LAME). 

\item AdaRC introduces two hyperparameters learning rate $lr$ and the number of epochs $T$. Based on their hyperparameter study, We select $lr$ from $\{1, 5, 10 \}$ and $T$ from $\{1, 10, 100\}$.

\item GTrans introduces four hyperparameters learning rate of feature adaptation $lr_f$, learning rate of structure adaptation $lr_A$, the number of epochs $T$, and the budget to update. We select $lr_f$ from $\{0.001, 0.01 \}$, $lr_A=\{0.01,0.1,0.5\}$ from $\{0.01, 0.05 \}$, $T$ from $\{5, 10\}$, and budget from $\{0.01, 0.05, 0.1 \}$.

\item SOGA introduces one hyperparameters $lr$ for one epoch update.  We select $lr$ from $\{0.001, 0.01 \}$.

\item TENT introduces one hyperparameters $lr$ for one epoch update.  We select $lr$ from $\{0.001, 0.01, 0.05\}$.

\item T3A introduces one hyperparameters $M$ for deciding the number of supports to restore. We select $M$ from $\{5, 20, 50, 100\}$
\end{itemize}
