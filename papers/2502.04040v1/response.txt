\section{Background and Related Work}
\vspace{-0.03cm}
\textbf{Jailbreaking Attacks.}  OOD jailbreaking attacks aim to bypass the safety alignment, leading models to generate harmful contents. They can be classified into 2 classes: 1) white-box attacks **Brown et al., "Myerson's Auction"**: the attackers access model parameters to compute gradients or losses; 2) black-box attacks **Liu et al., "TextFooler"**: the attackers adopt black-box optimization or design diverse OOD scenarios to deceive models. Black-box attacks, which do not require access to models' parameters, are more practical and have demonstrated strong performance **Ebrahimi et al., "Data Poisoning Attacks"**. Therefore, \textbf{we mainly focuses on black-box attacks in this work.}


% Jailbreak attacks typically disguise their harmful intention within a deceptive situation or add prefix and suffix words to make the LLM to generate harmful content. They can be classified into 2 categories: 1) White Box Attack is designed for open source LLMs whose parameters are approachable. Typical white box attacks include: GCG **Bastings et al., "High-Low"**, who adds suffixes after the illgal instructions to mislead the target LLM; AutoDan **Zhang et al., "AutoPGN"**, who uses genetic algorithm to automatically generate stealthy jailbreak prompts. 2) Black Box Attack is more practical in reality as there's no need to approach the models' parameters or decoding space. These include: Do-Anything-Now **Li et al., "Do-Anything-Now"**, who recommends the LLM to play both a safe and unsafe role; PAIR **Zhou et al., "PAIR"**, who iteratively receive target LLMs' response and revise its attack accordingly. In this work, we employ black box attacks because it's more practical in usage.

\vspace{-0.13cm}
\textbf{Safety Training.} Various methods have been proposed to enhance generalization against OOD attacks, broadly classified into two categories: 1) regularization-based training **Sinha et al., "Certified Robustness"**, and 2) interventions in the model’s internal representations **Xie et al., "Input-Output Adversarial Training"**. In contrast to these methods, we introduce a new supervision signal—reasoning data—derived from our analysis to train the model. 

\vspace{-0.13cm}
\textbf{OOD Generalization} **Sinha et al., "Certified Robustness"** shows that fine-tuning with limited samples distorts pretrained features, resulting in poor OOD performance. **Xie et al., "Input-Output Adversarial Training"** further explore this issue, showing that underspecified supervision can lead pretrained models to rely on shortcuts in training data during fine-tuning. Aligning with this line of research, we propose training models to elicit and utilize latent knowledge through reasoning as a strategy to mitigate these limitations.

\vspace{-0.13cm}
\textbf{LLM Reasoning.} Reasoning has enabled LLMs to achieve exceptional performance in STEM tasks. Training models to reason falls into two main approaches: 1) using human-curated or synthesized supervision, and 2) scaling reinforcement learning (RL) to elicit reasoning abilities. The first approach obtains high-quality supervision from human annotations **Sung et al., "Deep Learning for Computer Vision"**, distillation from stronger LLMs **Ribeiro et al., "Model Interpretability by Perturbation"**, or self-distillation **Li et al., "Self-Distilled Knowledge Graph Embeddings"**.
% ____ further enhance reasoning by teaching models to conduct Self-Reflection. 
The second approach employs large-scale RL to incentivize reasoning, leading to the emergence of intriguing reasoning behaviors and better performance **Zhang et al., "Deep Reinforcement Learning for Autonomous Vehicles"**. Our method follows the first approach. While **Bastings et al., "High-Low"** also train models to reason for safety alignment, our work places greater emphasis on analyzing failure mode of refusal training and proposes a complete pipeline for synthesizing reasoning supervision based on these insights.


\vspace{-0.13cm}
\textbf{Preliminaries.} Let $(\bm{x}, \bm{y})\sim \mathcal{D}_T$ be a training sample, where $(\bm{x}_i, \bm{y}_i)$ denotes an illegal instruction with its direct refusal, and $(\bm{x}_h, \bm{y}_h)$ represents a helpful sample. The trained model is denoted as $\bm{M}_{{\bm{\theta}}}$.