\section{Background and Related Work}
\vspace{-0.03cm}
\textbf{Jailbreaking Attacks.}  OOD jailbreaking attacks aim to bypass the safety alignment, leading models to generate harmful contents. They can be classified into 2 classes: 1) white-box attacks \cite{gcg,liu2023autodan,geisler2024attacking}: the attackers access model parameters to compute gradients or losses; 2) black-box attacks \cite{PAIR, wei2023jailbroken, shen2024anything, gptsmart, zeng2024johnny}: the attackers adopt black-box optimization or design diverse OOD scenarios to deceive models. Black-box attacks, which do not require access to models' parameters, are more practical and have demonstrated strong performance \cite{mazeika2024harmbench}. Therefore, \textbf{we mainly focuses on black-box attacks in this work.}


% Jailbreak attacks typically disguise their harmful intention within a deceptive situation or add prefix and suffix words to make the LLM to generate harmful content. They can be classified into 2 categories: 1) White Box Attack is designed for open source LLMs whose parameters are approachable. Typical white box attacks include: GCG~\citep{gcg}, who adds suffixes after the illgal instructions to mislead the target LLM; AutoDan~\citep{liu2023autodan}, who uses genetic algorithm to automatically generate stealthy jailbreak prompts. 2) Black Box Attack is more practical in reality as there's no need to approach the models' parameters or decoding space. These include: Do-Anything-Now~\citep{DANismy}, who recommends the LLM to play both a safe and unsafe role; PAIR~\cite{PAIR}, who iteratively receive target LLMs' response and revise its attack accordingly. In this work, we employ black box attacks because it's more practical in usage.

\vspace{-0.13cm}
\textbf{Safety Training.} Various methods have been proposed to enhance generalization against OOD attacks, broadly classified into two categories: 1) regularization-based training \cite{youliang,qi2024safety}, and 2) interventions in the model’s internal representations \cite{zou2024improving,sheshadri2024latent}. In contrast to these methods, we introduce a new supervision signal—reasoning data—derived from our analysis to train the model. 

\vspace{-0.13cm}
\textbf{OOD Generalization} \citet{kumar2022fine} shows that fine-tuning with limited samples distorts pretrained features, resulting in poor OOD performance. \citet{izmailov2022feature,lee2022diversify} further explore this issue, showing that underspecified supervision can lead pretrained models to rely on shortcuts in training data during fine-tuning. Aligning with this line of research, we propose training models to elicit and utilize latent knowledge through reasoning as a strategy to mitigate these limitations.

\vspace{-0.13cm}
\textbf{LLM Reasoning.} Reasoning has enabled LLMs to achieve exceptional performance in STEM tasks. Training models to reason falls into two main approaches: 1) using human-curated or synthesized supervision, and 2) scaling reinforcement learning (RL) to elicit reasoning abilities. The first approach obtains high-quality supervision from human annotations \cite{uesato2022solving,lightman2023let}, distillation from stronger LLMs \cite{yu2023metamath,mitra2024orca}, or self-distillation \cite{zelikman2022star,yuan2023scaling,wang2024math,guan2025rstar}.
% \citet{qu2024recursive,kumar2024training} further enhance reasoning by teaching models to conduct Self-Reflection. 
The second approach employs large-scale RL to incentivize reasoning, leading to the emergence of intriguing reasoning behaviors and better performance \cite{o1,guo2025deepseek}. Our method follows the first approach. While \citet{guan2024deliberative} also train models to reason for safety alignment, our work places greater emphasis on analyzing failure mode of refusal training and proposes a complete pipeline for synthesizing reasoning supervision based on these insights.


\vspace{-0.13cm}
\textbf{Preliminaries.} Let $(\bm{x}, \bm{y})\sim \mathcal{D}_T$ be a training sample, where $(\bm{x}_i, \bm{y}_i)$ denotes an illegal instruction with its direct refusal, and $(\bm{x}_h, \bm{y}_h)$ represents a helpful sample. The trained model is denoted as $\bm{M}_{{\bm{\theta}}}$.


\vspace{-0.2cm}