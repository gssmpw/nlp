\section{RELATED WORK}
\subsection{Localization Using Prior Maps}
An important method of localization in GNSS-denied environments is to match the pre-built maps to the sensor inputs. 
Several previous studies have employed feature matching techniques based on 3D maps to address this challenge~\cite{irschara2009structure, sattler2016efficient, toft2018semantic, sarlin2019coarse}. However, the creation and storage of 3D maps is expensive and their use in large-scale environments is difficult.
Consequently, recent research has focused on localization using more lightweight 2D maps.

Panphattarasap~\textit{et al.}~\cite{panphattarasap2018automated} present an image-based localization approach in urban environments by leveraging binary semantic descriptors (BSD) to match semantic features from images with the 2D map.
Subsequently, Yan~\textit{et al.}~\cite{yan2019global} advance this concept by incorporating the BSD descriptor into 3D LiDAR-based localization systems for mobile vehicles.
Samano~\textit{et al.}~\cite{samano2020you} design a siamese network to learn descriptors for both panoramic images and 2D map tiles in a low-dimensional embedded space.
Cho~\textit{et al.}~\cite{cho2022openstreetmap} propose a LiDAR-based vehicle localization method using OSM data instead of prior LiDAR maps. The localization is achieved by comparing the OSM descriptors and the LiDAR descriptors.
Based on the advantages of OSM being free and available globally~\cite{osmpaper}, Sarlin~\textit{et al.}~\cite{sarlin2023orienternet} propose OrienterNet, the first end-to-end image-to-OSM localization approach with high precision.




\subsection{BEV Representation for Autonomous Driving}
Learning powerful representations in BEV for perception tasks has been well studied in recent years.
LSS~\cite{philion2020lift} first introduces a well-known 2D-to-3D transformation framework based on depth. LSS predicts grid-wise depth distribution on 2D features, and then lifts the 2D features into 3D space via depth estimation.
Many subsequent works have adopted this approach~\cite{huang2021bevdet, li2023bevdepth, liu2023bevfusion}.
However, LSS-based methods are sensitive to the accuracy of depth estimation, some methods encode 2D features into 3D space via 3D-to-2D geometric projection, eliminating the need for explicit depth estimation~\cite{li2022bevformer, chen2022persformer}.
More recently, some studies further remove the 3D-to-2D geometric projection and instead implicitly learn the correlation between 2D features and BEV representations through the multi-layer perceptron (MLP) or transformer architecture~\cite{vaswani2017attention}.
For instance, HDMapNet~\cite{li2022hdmapnet} models the relationship between pixels in the perspective view and the camera coordinate system by an MLP.
CVT~\cite{zhou2022cross} uses a cross-view transformer to implicitly learn the geometric transformation. 
PETR~\cite{liu2022petr} introduces an innovative approach by integrating the 3D position embedding into image features, allowing the object queries to interact directly with 3D position-aware image features through vanilla cross attention, achieving a simpler and more elegant framework.

\subsection{Combining BEV Perception and Prior Maps}
Recently, there have been a series of works that combine BEV perception and prior maps to estimate vehicle's ego pose directly from sensor inputs and prior maps.
BEV-Locator~\cite{zhang2025bev} designs an end-to-end visual semantic localization network using multi-view camera images and first formulates the visual localization problem as an end-to-end learning scheme.
Building on a similar concept, OrienterNet~\cite{sarlin2023orienternet} achieves localization exclusively through monocular images and OSM, making it suitable for consumer-grade Augmented Reality (AR) applications.
% SNAP~\cite{sarlin2023snap} further improves the localization accuracy and robustness by fusing ground-level and overhead imagery into a single 2D neural map.
Building upon OrienterNet, OSMLoc~\cite{liao2024osmloc} enhances the generalization ability by integrating the pre-trained foundation model~\cite{oquab2023dinov2} into the framework.
U-BEV~\cite{camiletto2024u} by Camiletto~\textit{et al.} employs a more powerful BEV representation based on the U-Net architecture~\cite{ronneberger2015u}, enabling localization in lower detailed maps.
He~\textit{et al.}~\cite{he2024egovm} propose a multimodal approach (EgoVM) that takes multi-view images, 3D LiDAR points, and offline vectorized maps to estimate the pose offset relative to the initial pose in an end-to-end manner.
MapLocNet~\cite{wu2024maplocnet} introduced by Wu~\textit{et al.} presents a coarse-to-fine registration framework for aligning BEV and neural map features, and investigates the impact of segmentation task on localization performance.

\begin{figure*}[ht]
%\vspace{0.2cm}
  \centering
  \includegraphics[width=0.95\linewidth]{pics/overall.pdf}
  \caption{The overall architecture of~\M. We use a camera BEV encoder and a LiDAR BEV encoder to extract BEV features in a low-dimensional embedded space from the respective sensor inputs. Subsequently, the multimodal BEV features are fused with a lightweight convolutional neural network. The fused BEV features are then upsampled to the target resolution by the segmentation decoder to produce the BEV segmentation. Meanwhile, for each observation, a tile of local map is queried from the prior maps based on the given initial position. The vehicle's ego pose is estimated by exhaustively matching the BEV segmentation with the local prior map.}
  \label{fig:overall}
  % \vspace{-0.3cm}
\end{figure*}

Inspired by the aforementioned works, we propose a novel multimodal end-to-end localization network that achieves high-accuracy localization performance with generalization by training a BEV segmentation network.
This design not only maximize the sharing of limited computational resources on the vehicle with other perception tasks, but also simplifies the integration of our method into an end-to-end autonomous driving framework~\cite{hu2022st, hu2023planning}.
Furthermore, by unifying the map representations of HD and SD maps, our method is capable of adapting to different types of maps with minimal changes.