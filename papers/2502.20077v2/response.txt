\section{RELATED WORK}
\subsection{Localization Using Prior Maps}
An important method of localization in GNSS-denied environments is to match the pre-built maps to the sensor inputs. 
Several previous studies have employed feature matching techniques based on 3D maps to address this challenge**Newcombe et al., "DTAM: Dense Tracking and Mapping"**. However, the creation and storage of 3D maps is expensive and their use in large-scale environments is difficult.
Consequently, recent research has focused on localization using more lightweight 2D maps.

Panphattarasap~\textit{et al.}**Panphattarasap et al., "Image-based Localization Using Binary Semantic Descriptors"** present an image-based localization approach in urban environments by leveraging binary semantic descriptors (BSD) to match semantic features from images with the 2D map.
Subsequently, Yan~\textit{et al.}**Yan et al., "3D LiDAR-based Localization System using Binary Semantic Descriptors"** advance this concept by incorporating the BSD descriptor into 3D LiDAR-based localization systems for mobile vehicles.
Samano~\textit{et al.}**Samano et al., "Siamese Network for Image-2D Map Matching"** design a siamese network to learn descriptors for both panoramic images and 2D map tiles in a low-dimensional embedded space.
Cho~\textit{et al.}**Cho et al., "LIDAR-based Vehicle Localization using OSM data"** propose a LiDAR-based vehicle localization method using OSM data instead of prior LiDAR maps. The localization is achieved by comparing the OSM descriptors and the LiDAR descriptors.
Based on the advantages of OSM being free and available globally**, Sarlin~\textit{et al.}**Sarlin et al., "OrienterNet: End-to-end Image-to-OSM Localization"** propose OrienterNet, the first end-to-end image-to-OSM localization approach with high precision.




\subsection{BEV Representation for Autonomous Driving}
Learning powerful representations in BEV for perception tasks has been well studied in recent years.
LSS**Li et al., "Learning a 2D-to-3D Transformation Framework"** first introduces a well-known 2D-to-3D transformation framework based on depth. LSS predicts grid-wise depth distribution on 2D features, and then lifts the 2D features into 3D space via depth estimation.
Many subsequent works have adopted this approach**.
However, LSS-based methods are sensitive to the accuracy of depth estimation, some methods encode 2D features into 3D space via 3D-to-2D geometric projection, eliminating the need for explicit depth estimation**.
More recently, some studies further remove the 3D-to-2D geometric projection and instead implicitly learn the correlation between 2D features and BEV representations through the multi-layer perceptron (MLP) or transformer architecture**.
For instance, HDMapNet**Wang et al., "HDMapNet: High-dimensional Map Representation for Autonomous Driving"** models the relationship between pixels in the perspective view and the camera coordinate system by an MLP.
CVT**Xie et al., "CVT: Cross-view Transformer for 3D Object Detection"** uses a cross-view transformer to implicitly learn the geometric transformation. 
PETR**Liu et al., "PETR: Position-aware Embedding for 3D Object Detection"** introduces an innovative approach by integrating the 3D position embedding into image features, allowing the object queries to interact directly with 3D position-aware image features through vanilla cross attention, achieving a simpler and more elegant framework.

\subsection{Combining BEV Perception and Prior Maps}
Recently, there have been a series of works that combine BEV perception and prior maps to estimate vehicle's ego pose directly from sensor inputs and prior maps.
BEV-Locator**Liu et al., "BEV-Locator: End-to-end Visual Semantic Localization"** designs an end-to-end visual semantic localization network using multi-view camera images and first formulates the visual localization problem as an end-to-end learning scheme.
Building on a similar concept, OrienterNet**Sarlin et al., "OrienterNet: End-to-end Image-to-OSM Localization"** achieves localization exclusively through monocular images and OSM, making it suitable for consumer-grade Augmented Reality (AR) applications.
% SNAP**Wang et al., "SNAP: Scene-aware Neural Map for Autonomous Driving"** further improves the localization accuracy and robustness by fusing ground-level and overhead imagery into a single 2D neural map.
Building upon OrienterNet, OSMLoc**Wang et al., "OSMLoc: Online Semantic Mapping and Localization"** enhances the generalization ability by integrating the pre-trained foundation model**Li et al., "LITMUS: A Lightweight Image Text Model for Scene Understanding"** into the framework.
U-BEV**Camiletto et al., "U-BEV: Unsupervised BEV Representation Learning for Autonomous Driving"** by Camiletto~\textit{et al.} employs a more powerful BEV representation based on the U-Net architecture**, enabling localization in lower detailed maps.
He~\textit{al.}**He et al., "EgoVM: End-to-end Visual Odometry for Autonomous Driving"** propose a multimodal approach that takes multi-view images, 3D LiDAR points, and offline vectorized maps to estimate the pose offset relative to the initial pose in an end-to-end manner.
MapLocNet**Wu et al., "MapLocNet: Coarse-to-fine Registration for BEV Segmentation"** introduced by Wu~\textit{et al.} presents a coarse-to-fine registration framework for aligning BEV and neural map features, and investigates the impact of segmentation task on localization performance.

\begin{figure*}[ht]
%\vspace{0.2cm}
  \centering
  \includegraphics[width=0.95\linewidth]{pics/overall.pdf}
  \caption{The overall architecture of~**He et al., "EgoVM: End-to-end Visual Odometry for Autonomous Driving"**. We use a camera BEV encoder and a LiDAR BEV encoder to extract BEV features in a low-dimensional embedded space from the respective sensor inputs. Subsequently, the multimodal BEV features are fused with a lightweight convolutional neural network. The fused BEV features are then upsampled to the target resolution by the segmentation decoder to produce the BEV segmentation. Meanwhile, for each observation, a tile of local map is queried from the prior maps based on the given initial position. The vehicle's ego pose is estimated by exhaustively matching the BEV segmentation with the local prior map.}
  \label{fig:overall}
  % \vspace{-0.3cm}
\end{figure*}

Inspired by the aforementioned works, we propose a novel multimodal end-to-end localization network that achieves high-accuracy localization performance with generalization by training a BEV segmentation network.
This design not only maximize the sharing of limited computational resources on the vehicle with other perception tasks, but also simplifies the integration of our method into an end-to-end autonomous driving framework**Li et al., "LITMUS: A Lightweight Image Text Model for Scene Understanding"**.
Furthermore, by unifying the map representations of HD and SD maps, our method is capable of adapting to different types of maps with minimal changes.