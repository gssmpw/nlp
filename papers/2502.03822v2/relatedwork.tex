\section{Related Works}
\label{sec:related_works}

\subsection{Overparameterization and Intrinsic Rank}
\label{sec:overparam_and_rank}
Overparameterization, where models have more parameters than necessary to fit the training data, is a key factor behind the success of modern machine learning \cite{krizhevsky2012imagenet, kaplan2020scaling}. Large models such as diffusion models \cite{ho2020denoising} and transformers \cite{vaswani2017attention} excel in tasks like image synthesis \cite{rombach2022high}, robotic manipulation \cite{chi_dp}, and language generation \cite{zhao2023survey}. Although overparameterized models offer impressive performance, their size also poses significant challenges for training and fine-tuning due to high computational and memory requirements.

To tackle these challenges, researchers have observed that overparameterized models often reside in a low-dimensional subspace \cite{aghajanyan_intrinsic_dim, li_intrinsic_dim}. This insight has led to techniques like Low-Rank Adaptation (LoRA) \cite{hu_lora, dettmers_qlora}, which fine-tune a small low-rank adapter while keeping the main model frozen. Although LoRA effectively reduces computational costs, it is primarily suited for fine-tuning pre-trained models and is less practical for training models from scratch due to its fixed low-rank structure \cite{liu2024dora}.

For LoRA, the need to merge and re-inject adapters during rank adjustments and the increased parameters during forward pass can destabilize training and increase computational overhead. In contrast, the DRIFT framework dynamically adjusts trainable ranks without adding new parameters or introducing instability. By using SVD to partition weight matrices into trainable and frozen components, DRIFT maintains stability and efficiency, making it well-suited for training overparameterized diffusion policies from scratch.

\subsection{Imitation Learning and Diffusion Policy}
\label{sec:il_dp}

Imitation Learning (IL) is a widely studied policy learning paradigm applied to various robotic scenarios. IL involves collecting demonstration data from an expert and training a neural network using supervised learning techniques \cite{zare2024survey}. Before the emergence of diffusion policies, IL research focused on improving sample efficiency and mitigating compounding errors through strategic sample collection and labeling \cite{spencer2021feedback}. \citet{ross2011reduction} first address these challenges with an iterative, interactive IL method. This approach collects additional demonstration rollouts using a suboptimal policy and refines the trajectories with corrections provided by an expert. Building on this work, expert-gated \cite{kelly2019hg, sun_mega_dagger} and learner-gated \cite{hoque2021thriftydagger, hoque2021lazydagger} methods allow experts or learners to dynamically take or hand over control during online rollout collection, which further improves data efficiency.

These methods primarily rely on interactive demonstration strategies and typically utilize simple neural network architectures, such as Multi-Layer Perceptrons (MLPs) \cite{jin2020geometric, loquercio2021learning, zhou2024developing} or Long Short-Term Memory (LSTM) networks \cite{cai2019vision, huang2020real, wu2024deep}. Historically, these interactive IL methods often employ shallow and small MLPs or LSTM, which are constrained by their relatively small number of parameters, limiting the performance of the learned policies.

Diffusion policies \cite{chi_dp} shift the focus of IL research to leveraging the representational power of overparameterized models. Inspired by generative models \cite{ho2020denoising, song2020denoising}, diffusion policies use large networks to achieve strong performance in various tasks. However, the computational demands of these models create challenges for both training and inference. Few existing works attempt to integrate interactive IL with diffusion models \cite{lee2024diff, zhang2024diffusion}. For example, \citet{lee2024diff} leverage diffusion loss to better handle multimodality; however, this work focuses on a robot-gated interactive approach rather than an expert-gated one and does not contain real-world experiments. Similarly, while \citet{zhang2024diffusion} employ diffusion as a policy representation, the primary innovation in this work lies in using diffusion for data augmentation rather than improving the interactive learning process. Notably, neither approach addresses the critical issue of reducing batch training time, which is essential for making online interactive learning with large models more practical and usable. Recent efforts to accelerate diffusion policies focus on inference through techniques like distillation \cite{prasad2024consistency, wang2024one}, but no existing work focuses on improving training efficiency. As a result, diffusion policy research remains largely confined to offline training scenarios \cite{sridhar2024nomad, sun2024comparative, ze20243d}.