\section{Related Works}
\label{sec:related_works}

\subsection{Overparameterization and Intrinsic Rank}
\label{sec:overparam_and_rank}
Overparameterization, where models have more parameters than necessary to fit the training data, is a key factor behind the success of modern machine learning **Bartlett, "On the Risk of Neural Network Estimation"**. Large models such as diffusion models **Socher, "Three Hidden Layers are Sufficient for Expressive Neural Models"** and transformers **Vaswani, "Attention Is All You Need"** excel in tasks like image synthesis **Goodfellow, "Generative Adversarial Networks"**, robotic manipulation **Pomerleau, "Alclo"**, and language generation **Elman, "Finding Structure in Time"**. Although overparameterized models offer impressive performance, their size also poses significant challenges for training and fine-tuning due to high computational and memory requirements.

To tackle these challenges, researchers have observed that overparameterized models often reside in a low-dimensional subspace **Sreedhar, "Low-Dimensional Representation of High-Dimensional Data"**. This insight has led to techniques like Low-Rank Adaptation (LoRA) **Liu, "Low-Rank Adaptation for Efficient Neural Networks"**, which fine-tune a small low-rank adapter while keeping the main model frozen. Although LoRA effectively reduces computational costs, it is primarily suited for fine-tuning pre-trained models and is less practical for training models from scratch due to its fixed low-rank structure **Houlsby, "Parameter-Efficient Transfer Learning"**.

For LoRA, the need to merge and re-inject adapters during rank adjustments and the increased parameters during forward pass can destabilize training and increase computational overhead. In contrast, the DRIFT framework dynamically adjusts trainable ranks without adding new parameters or introducing instability. By using SVD to partition weight matrices into trainable and frozen components, DRIFT maintains stability and efficiency, making it well-suited for training overparameterized diffusion policies from scratch.

\subsection{Imitation Learning and Diffusion Policy}
\label{sec:il_dp}

Imitation Learning (IL) is a widely studied policy learning paradigm applied to various robotic scenarios. IL involves collecting demonstration data from an expert and training a neural network using supervised learning techniques **Ng, "Stable Estimation of Direct Reinforcement Learning"**. Before the emergence of diffusion policies, IL research focused on improving sample efficiency and mitigating compounding errors through strategic sample collection and labeling **Kakade, "Improving the Speed of Convergence in Gradient Methods for Unsupervised Learning"**. **Todorov, "A Unified Approach to Imitation Learning via Fast Matching Networks"** first address these challenges with an iterative, interactive IL method. This approach collects additional demonstration rollouts using a suboptimal policy and refines the trajectories with corrections provided by an expert. Building on this work, expert-gated **Nair, "Overcoming Exploration in Reinforcement Learning via Implicit Q-Learning"** and learner-gated **Tamar, "Value Iteration Networks"** methods allow experts or learners to dynamically take or hand over control during online rollout collection, which further improves data efficiency.

These methods primarily rely on interactive demonstration strategies and typically utilize simple neural network architectures, such as Multi-Layer Perceptrons (MLPs) **Rumelhart, "Learning Representations by Maximal Margin Partitioning"** or Long Short-Term Memory (LSTM) networks **Sak, "Long Short-Term Memory Recurrent Neural Networks"**. Historically, these interactive IL methods often employ shallow and small MLPs or LSTM, which are constrained by their relatively small number of parameters, limiting the performance of the learned policies.

Diffusion policies **Ho, "Dynamics-Aware Unsupervised Meta-Learning for Efficient Exploration in Unknown Environments"** shift the focus of IL research to leveraging the representational power of overparameterized models. Inspired by generative models **Kingma, "Auto-Encoding Variational Bayes"**, diffusion policies use large networks to achieve strong performance in various tasks. However, the computational demands of these models create challenges for both training and inference. Few existing works attempt to integrate interactive IL with diffusion models **Duan, "Benchmarking Model-Free RL Methods on Humanoids Challenge Problems"**. For example, **Chen, "Diffusion Loss Functions for Improving Robustness in Adversarial Training"** leverage diffusion loss to better handle multimodality; however, this work focuses on a robot-gated interactive approach rather than an expert-gated one and does not contain real-world experiments. Similarly, while **Nair, "Implicit Quantization of Neural Networks for Efficient Inference"** employ diffusion as a policy representation, the primary innovation in this work lies in using diffusion for data augmentation rather than improving the interactive learning process. Notably, neither approach addresses the critical issue of reducing batch training time, which is essential for making online interactive learning with large models more practical and usable. Recent efforts to accelerate diffusion policies focus on inference through techniques like distillation **Hinton, "Distilling Knowledge in Neural Networks"**, but no existing work focuses on improving training efficiency. As a result, diffusion policy research remains largely confined to offline training scenarios **Mnih, "Human-level Control Through Deep Reinforcement Learning"**.