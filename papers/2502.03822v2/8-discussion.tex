\section{Discussion}
\label{sec:discussion}

This work introduces DRIFT, a framework designed to leverage the intrinsic low-rank properties of large diffusion policy models for efficiency while preserving the benefits of overparameterization. To achieve this, we propose rank modulation and rank scheduler, which dynamically adjust trainable ranks using SVD and a decay function. We instantiate DRIFT within an interactive IL algorithm, DRIFT-DAgger, and show this efficacy of this method through extensive experiments and ablation studies in both simulation and real-world settings. Our results demonstrate that DRIFT-DAgger reduces training time and improves sample efficiency while maintaining performance on par with full-rank policies trained from scratch.

\subsection{Limitations}
This work evaluates and demonstrates the DRIFT framework using DRIFT-DAgger as an instantiation within the IL paradigm. However, prior to the adoption of large models, online reinforcement learning (RL) approaches \cite{schulman2017proximal, haarnoja2018soft} were also a popular area of research. This work does not explore the application of the DRIFT framework in the online RL paradigm. Investigating the potential of DRIFT within online RL could serve as an valuable direction for future research.

Additionally, while we have tested and evaluated various decay functions for the rank scheduler, the current implementation of dynamic rank adjustment in the DRIFT framework follows a monotonic schedule. Although we have conducted ablation studies on decay functions and terminal ranks, the impact of these design choices is likely task-dependent. Furthermore, the rank adjustment of different convolutional blocks in this work is applied uniformly throughout the U-Net backbone, even though different blocks may have varying highest possible ranks. Future research could explore more adaptive and intelligent strategies for adjusting trainable ranks to enhance training efficiency and performance, as well as identify suitable decay functions and terminal ranks for scenarios beyond those covered in this work.

\subsection{Implications}
As discussed in \S\ref{sec:related_works}, prior to the era of large models, innovations in robot learning primarily focused on learning processes with interaction. However, the increasing size of models has resulted in significantly longer training times, making many previous innovations in online interactive learning less practical due to the time required for policy updates. While the machine learning community has made progress in leveraging the intrinsic rank of large models to improve training efficiency, most of these methods are tailored for fine-tuning rather than training from scratch. This distinction arises from the availability of foundation models in general machine learning, whereas robotics often requires training policies from scratch to address scenario-specific tasks.

Despite years of research in the machine learning community, the concepts of overparameterization and intrinsic ranks remain relatively underexplored in robotics. This work introduces reduced-rank training as a means to address the challenges of training efficiency, thereby making online interactive learning methods more feasible in the era of large models for robot learning. By bridging these gaps, we aim to raise awareness within the robotics community about leveraging overparameterization and intrinsic ranks to design more efficient learning methods while preserving the powerful representations afforded by overparameterized models.

%To speculate on the potential impact of our work on large-scale training scenarios, we extrapolate the results from \S\ref{sec:abl_rm} to a large dataset such as Open-X Embodiment \cite{o2024open}, which contains over one million trajectories. Assuming the same training configuration of 100 offline epochs and 50 online rollouts, a batch size equal to the average trajectory length, and a terminal rank of $r_{min}=256$ remaining sufficient for high quality training, the proposed DRIFT-DAgger with rank modulation and rank scheduler could possibly save about 1250 hours (52 days) of training time compared to full-rank training methods.  While testing on such a large dataset was beyond the scope of our current work, we are eager to explore these possibilities ourselves and enable such investigations in the community going forward.     
