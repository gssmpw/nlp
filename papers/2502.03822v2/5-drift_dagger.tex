
\section{DRIFT-DAgger}
\label{sec:drift_dagger}

DRIFT-DAgger combines the sample efficiency of interactive IL with the computational efficiency of low-rank training methods, making it well-suited for training large policies interactively.

Algorithm \ref{alg:cap} outlines the DRIFT-DAgger procedure. Similar to previous interactive IL methods, DRIFT-DAgger consists of an offline bootstrapping stage followed by an online adaptation stage. The process begins with an initial policy $\pi_{N_0}$, parameterized by a neural network that can adjust the number of trainable ranks. Although DRIFT-DAgger is proposed as an instantiation of the DRIFT framework, the adjustment of trainable ranks can be achieved through any kind of low-rank adapters other than the rank modulation proposed in \S\ref{sec:rank_modulation}, such as LoRA.

In the offline bootstrapping stage, DRIFT-DAgger trains the policy $\pi_{N_i}$ on an offline dataset $\mathcal{D}_B$ over several epochs $i$ using BC, similar to prior interactive IL methods. However, unlike these methods, DRIFT-DAgger optionally employs a rank scheduler that gradually reduces the number of trainable ranks during training. The rank scheduler uses a decay function based on the epoch index $i$, along with the highest possible ranks ($r_{\text{max}}$) and terminal trainable ranks ($r_{\text{min}}$) for the policy network. This approach reduces computational costs while maintaining performance. Details of the rank scheduler are presented in \S\ref{sec:rank_scheduler}.

If the rank scheduler is not used, the number of trainable ranks is set fixed at $r_{min}$ after offline bootstrapping and before transitioning to the online adaptation stage. At the end of offline bootstrapping, the offline dataset $\mathcal{D}_B$ is merged into a global dataset $\mathcal{D}$ for further use in online adaptation.

During the online adaptation stage, the learner policy interacts with the environment through rollouts. At each iteration $j$, the learner executes a rollout in the environment. If the expert policy $\pi_{exp}$ detects that the learner has deviated from the desired trajectory, the expert intervenes, taking control to provide corrective demonstrations. The expert can be a human teleoperator, an algorithm, or another neural network. These demonstrations are recorded in a dataset specific to the current rollout $\mathcal{D}_j$. After each rollout, $\mathcal{D}_j$ is merged into the global dataset $\mathcal{D}$, and the learner policy $\pi_{N_{\mathcal{I}+j}}$ is updated using the expanded dataset $\mathcal{D}$.

The full procedure of DRIFT-DAgger leverages low-rank training and online interaction to achieve better sample and training efficiency.

\begin{algorithm}
\caption{DRIFT-DAgger}\label{alg:cap}
%\KwIn{Expert policy $\pi_{exp}$, terminal ranks $r_t$}
%\KwOut{Trained policy $\pi$}
\SetKwProg{Procedure}{procedure}{}{}
\Procedure{\textnormal{DRIFT-DAgger}$(\pi_{exp}$, $\pi_{N_0}$, $\mathcal{D}_B)$}{}
\For{\textnormal{offline epoch} $i = 1, 2, \cdots ,\mathcal{I}$}{
    train $\pi_{N_i}$ on offline dataset $\mathcal{D}_B$\\
    \If{\textnormal{use rank scheduler}}{
       $r_{i} = \texttt{Decay Function}(i, r_{\text{min}}, r_{\text{max}})$\\
       $\pi_{N_i} = \texttt{Rank Reduction}(r_{i}, \pi_{N_i})$\\
    }
}

\If{\textnormal{not use rank scheduler}}{
    $\pi_{N_{\mathcal{I}}} = \texttt{Rank Reduction}(r_{\text{min}}, \pi_{N_{\mathcal{I}}})$
}

$\mathcal{D} \gets \mathcal{D}_B$\\

\For{\textnormal{online iteration} $j = 1, 2, \cdots ,\mathcal{J}$}{
        \For{\textnormal{timestep} $t \in T$ \textnormal{of online rollout} $j$}{
            \If{$\pi_{exp}$ \textnormal{takes control}}{
                $observation \gets \texttt{rollout}_{j}^t$\\
                $action \gets \pi_{exp}(observation)$\\
                $\mathcal{D}_j \gets (observation, action)$\\
            }
        }
        $\mathcal{D} \gets \mathcal{D} \cup \mathcal{D}_j$
        
        Train $\pi_{N_{\mathcal{I}+j}}$ on $\mathcal{D}$
    }
\Return $\pi_{N_{{\mathcal{I}}+\mathcal{J}}}$
\end{algorithm}

