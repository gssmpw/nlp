\section{Background}

\subsection{Diffusion Policy Primer}

A denoising diffusion probabilistic model (DDPM) \cite{ho2020denoising, song2020denoising} consists of a forward process and a reverse process. In the forward process, Gaussian noise is gradually added to the training data, $x_0 \sim p(x_0)$, over $T$ discrete time steps. This process is governed by a predefined noise schedule, $\beta_t$, which controls how much noise is added at each step. Mathematically, the forward process is defined as:
\begin{align*}
&q(x_{1:T} \mid x_0) := \prod_{t=1}^T q(x_t \mid x_{t-1}), \\
&q(x_t \mid x_{t-1}) := \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I),
\end{align*}
where $q(x_t \mid x_{t-1})$ is a Gaussian distribution with a mean of $\sqrt{1 - \beta_t} x_{t-1}$ and variance $\beta_t$. Intuitively, this step progressively adds noise to $x_0$, such that by the final step $x_T$, the data is almost entirely noise.

The reverse process aims to undo this noise, step by step, to recover the original data $x_0$. This is parameterized by a neural network, $\pi_\theta$, which predicts the noise added to $x_t$ at each step $t$. Using this prediction, the reverse process reconstructs the data from the noisy input $x_t$:
\begin{equation*}
x_{t-1} \sim p_\theta(x_{t-1} \mid x_t) := \mathcal{N}(x_{t-1}; \mu_k(x_t, \pi_{\theta}(x_t, t)), \sigma_t^2 I),
\end{equation*}
where $\mu_k(\cdot)$ computes the mean for the denoised data at step $t-1$, and $\sigma_t^2$ is a fixed variance term.

In the context of robotics, a diffusion policy \cite{chi_dp} adapts the DDPM framework for visuomotor control by treating robot actions as $x$ and conditioning the denoising process on robot observations, such as camera images or sensor data. Specifically, the noise prediction network $\pi_\theta$ takes the current noisy action representation $x_t$ and the observations as inputs and predicts the noise to remove. Architectures like U-Nets \cite{ronneberger2015u} or transformers \cite{vaswani2017attention} are commonly used for $\pi_\theta$. Diffusion policies are typically trained offline using BC, where the model learns to mimic expert demonstrations.

\subsection{Ranks in Diffusion Models}
\label{sec:ranks_in_diffusion_models}
The rank of a matrix, defined as the maximum number of linearly independent rows or columns \cite{strang2022introduction}, is closely tied to the expressiveness and representational power of a model. For example, in linear models, the rank of the weight matrix determines the dimensionality of the feature space that the model can effectively capture. Weight matrices with low ranks often correspond to models with limited capacity but faster training, while those with high ranks indicate overparameterization, which can improve optimization but at the cost of slower training \cite{du2018power}.

In the context of a diffusion policy that employs a U-Net with one-dimensional convolutional blocks as its network backbone, a weight matrix for each convolutional block \( W \in \mathbb{R}^{n \times m} \) can be created by reshaping a corresponding weight tensor $W_{\text{conv}} \in \mathbb{R}^{C_{\text{out}} \times C_{\text{in}} \times k}$ via
$$
W = \texttt{reshape}(W_{\text{conv}}, (n, m)),
$$ 
where \( C_{\text{out}} \) is the number of output channels, \( C_{\text{in}} \) is the number of input channels, and \( k \) is the kernel size. The reshaping can be performed by setting $n=C_{\text{out}} * k$ and $m=C_{\text{in}}$ or other equivalent view transformations.

The highest possible rank \( r_{\text{max}} \) of this weight matrix is bounded by:
$$
r_{\text{max}} \leq \min(n, m).
$$

\subsection{Problem Statement}
\label{sec:problem_statement}
% As highlighted in several machine learning studies, while overparameterization aids the optimization process, the final learned weights often reside in a lower-dimensional subspace \cite{aghajanyan_intrinsic_dim, li_intrinsic_dim}. This observation motivates our goal of balancing the trade-off between training efficiency and the final performance of an overparameterized diffusion policy. 

In this work, we investigate diffusion policies that use a U-Net backbone composed of one-dimensional convolutional blocks, as introduced above.  For each convolutional block with weight \( W \) and highest possible rank \( r_{\text{max}} \) in a diffusion policy \( \pi_{\theta} \), we aim to enable dynamic adjustment of the rank \( r \) of a trainable segment of the weight matrix, \( W_{\text{train}} \), for any $r$ integer satisfying \( 1 \leq r \leq r_{\text{max}} \).  We assume all weight matrices $W$ throughout the network $\pi_\theta$ will vary uniformly based on $r$.  Importantly, $r$ should remain adjustable throughout the learning process without introducing instability or computational overhead.  

% while ensuring that the performance of the trained policy remains comparable to that of a policy trained with full rank \( r_{\text{max}} \), as measured by an evaluation metric \( V \). This problem can be formally stated as:

% $$
% V(\pi, r_{max}) \approx V(\pi, r) \quad \text{given} \quad r \leq r_{\text{max}},
%$$
%where $V$ is a performance metric. $V(\pi, r_{max})$ and $V(\pi, r)$ are values of that performance metric when evaluating policies trained with full ranks or reduced ranks respectively.



