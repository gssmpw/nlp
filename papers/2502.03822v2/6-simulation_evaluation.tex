


\section{Simulation Evaluation}
\label{sec:simulation_evaluation}

We evaluate the proposed DRIFT framework, instantiated in the DRIFT-DAgger algorithm, through extensive simulation experiments and ablation studies. All experiments are conducted using the PyTorch framework \cite{pytorch}, with the UNet-based diffusion policies that enable RGB perception following the specifications from \citet{chi_dp}. The batch size and learning rate is set to 256 and $10^{-4}$ for all experiments. We use Adam \cite{kingma2014adam} as the optimizer. Training is performed on a desktop PC with an AMD PRO 5975WX CPU, 4090 GPU, and 128GB RAM. To ensure a fair comparison with interactive methods like HG-DAgger~\cite{kelly_hg_dagger} and DRIFT-DAgger, we implement BC with an incremental dataset during the online phase, similar to the interactive loop of HG-DAgger and DRIFT-DAgger.


\subsection{Decay Functions}
\label{sec:abl_rs}

\begin{figure}[htbp]
\centering
\begin{minipage}{\columnwidth}
\includegraphics[width=\columnwidth]{figures/decay_functions.pdf}
\caption{Experimental results of DRIFT-DAgger with different decay functions for the rank scheduler. We use HG-DAgger (HG) as a baseline for comparison.}
\label{fig:decay_functions}
\end{minipage}

\vspace{1em} 

\begin{minipage}{\columnwidth}
    \centering
    \renewcommand\arraystretch{1.2}
    \captionof{table}{Summary of experimental results on mean batch training time (MBT) and success rate with different rank decay functions for DRIFT-DAgger. \label{tab:training_scheduler}}
    \resizebox{\columnwidth}{!}{ 
    \begin{tabular}{l|c|c|c|c}
    \hline 
    Function & \makecell{Success \\ Rate} & \makecell{MBT \\ (Offline)} & \makecell{MBT \\ (Online)} & \makecell{MBT \\ (All Stages)} \\ 
    \hline
    HG                  & $\mathbf{1.0}$ & 0.27 & 0.27 & 0.27 \\
    Linear              & $0.96$ & $0.26$ & $0.22$ & $0.25$ \\
    Cosine              & $\mathbf{1.0}$ & $0.26$ & $0.22$ & $0.25$ \\
    Exp. 0.1            & $0.88$ & $0.23$ & $0.23$ & $0.23$ \\
    Exp. 0.5            & $0.72$ & $\mathbf{0.22}$ & $\mathbf{0.22}$ & $\mathbf{0.22}$ \\
    Sig. 0.1            & $0.98$ & $0.25$ & $0.23$ & $0.24$ \\
    Sig. 0.5            & $\mathbf{1.0}$ & $0.26$ & $0.22$ & $0.24$ \\
    \hline
    \end{tabular}
    }
\end{minipage}
\end{figure}
To identify the scheduling strategy that best exploits the benefits of reduced-rank training while balancing the trade-off between training time and policy performance, we evaluate six variants of four decay functions for the rank scheduler. These functions dynamically adjust the number of trainable ranks as training progresses. The decay functions considered include linear, cosine, exponential, and sigmoid, as covered in \S\ref{sec:rank_scheduler}, with steepness parameters $\tau$ set to 0.1 and 0.5 for exponential and sigmoid. 

% As mentioned in \S\ref{sec:rank_scheduler}

We use DRIFT-DAgger with rank modulation and rank scheduler for this ablation study. We set $r_{\text{min}}$ to 256 for all DRIFT-DAgger variants. Since both BC and HG-DAgger employ full-rank training, they should exhibit similar batch training times. Given that HG-DAgger outperforms BC in terms of sample efficiency\cite{kelly2019hg}, we use HG-DAgger as the baseline for full-rank training methods. 

To assess performance and training efficiency when using different decay functions, we employ the success rate of a policy to evaluate the overall performance, where a high success rate indicates better completion of a given task. We also monitor the step loss during training as an indicator of convergence. Additionally, we measure the mean batch training time for each epoch, which includes the offline stage, online stage, and both stages combined, to reflect training efficiency when considered alongside the success rate and step loss. 

The ablation study is conducted on a pick-and-place scenario from the Manipulation with Viewpoint Selection (MVS) tasks \cite{sun2024comparative}. The pick-and-place scenario, as illustrated in Fig. \ref{fig:simulation_environments}, requires the agent to pick up a green cube and place it in a red region.  All MVS tasks involve two UFactory xArm7 robots\footnote{\href{https://www.ufactory.us/product/ufactory-xarm-7}{https://www.ufactory.us/product/ufactory-xarm-7}} mounted on linear motors, where one arm has a gripper and the other is equipped with a camera. Mounted on one end effector, the camera enables active perception, working in synergy with the gripper on the other end effector to cooperatively execute the manipulation task. The state-action space for all MVS tasks is a reduced end-effector space for the dual-arm system, with automatically computed camera orientation using an additional Inverse Kinematics (IK) objective.

The learning process uses 100 offline demonstration rollouts, 100 offline bootstrapping epochs, and 50 online iterations. We plot the experimental results by combining the number of offline epochs and online iterations, resulting in a total of 150 iterations.

The results of this experiment are presented in Fig. \ref{fig:decay_functions} and summarized in Table \ref{tab:training_scheduler}. While all decay functions reduce batch training time compared to full-rank training represented by HG-DAgger, some decay functions lead to decreased policy performance, as indicated by the success rates in Fig. \ref{fig:decay_functions} and Table \ref{tab:training_scheduler}. Notably, the exponential decay functions, due to their aggressive reduction of trainable ranks, underperform relative to the other variants, despite yielding the lowest mean batch training time.

The linear decay function, while offering near-perfect policy performance, results in the highest training time among all variants, suggesting that it is less effective than the sigmoid decay functions in balancing training time with performance. We observe that the sigmoid functions, particularly with a steep decay parameter $\tau = 0.5$, strike the best balance between training time and policy performance. These functions maintain a high number of trainable ranks during the early training phase, allowing overparameterization to effectively minimize approximation error, as shown in the loss plot in Fig. \ref{fig:decay_functions}. This facilitates more efficient learning of the predominant behavior during the early stage of training, while still preserving the flexibility required for online adaptation.

\subsection{Terminal Rank}
\label{sec:abl_tr}


% Compare with other baselines

\begin{figure}[tbp]
\centering
\begin{minipage}{\columnwidth}
\includegraphics[width=\columnwidth]{figures/terminal_ranks.pdf}
\caption{Experimental results of DRIFT-DAgger with different terminal ranks $r_{\text{min}}$.}
\label{fig:terminal_ranks}
\end{minipage}

\vspace{1em} 

\begin{minipage}{\columnwidth}
    \centering
    \renewcommand\arraystretch{1.2}
    \caption{Summary of experimental results on mean batch training time (MBT) and success rate with different values for terminal ranks for DRIFT-DAgger. \label{tab:terminal_ranks}}
    \resizebox{\columnwidth}{!}{ 
    \begin{tabular}{l|c|c|c|c}
    \hline 
    \makecell{$r_{\text{min}}$} & \makecell{Success \\ Rate} & \makecell{MBT \\ (Offline)} & \makecell{MBT \\ (Online)} & \makecell{MBT \\ (All Stages)} \\ 
    \hline
    64     & 0.78 & $\mathbf{0.24}$ & $\mathbf{0.19}$ & $\mathbf{0.22}$ \\
    128    & 0.98 & 0.25 & 0.21 & 0.23 \\
    256    & $\mathbf{1.0}$ & 0.26 & 0.22 & 0.24 \\
    512    & $\mathbf{1.0}$ & 0.27 & 0.24 & 0.26\\
    \hline
    \end{tabular}
    }
\end{minipage}
\end{figure}

To explore the effect of different terminal ranks for the DRIFT framework, we conduct an ablation study by varying the terminal rank $r_{\text{min}}$ in DRIFT-DAgger with rank modulation and rank scheduler. We use the same experimental task, setup, and evaluation metrics as \S\ref{sec:abl_rs}, and use sigmoid decay function with $\tau$ set to 0.5 for rank scheduler. The terminal rank $r_{min}$ is set to 64, 128, 256, and 512 for comparison.

As shown in Fig. \ref{fig:terminal_ranks} and summarized in Table \ref{tab:terminal_ranks}, decreasing the terminal ranks reduces training time but also impacts policy performance, as reflected in the success rate. This performance degradation occurs due to the diminished representational power of the model when the number of trainable ranks is reduced. Compared to reduced-rank methods for fine-tuning, such as \cite{hu_lora}, where fine-tuning with an adapter requires only 4 trainable ranks, DRIFT-DAgger with reduced-rank training from scratch necessitates a significantly higher number of trainable ranks, given that there is a noticeable performance drop when $r_{\text{min}}$ is set to 64. This is to fully leverage the benefits of both better representation power from overparameterization and improved computational efficiency from intrinsic low ranks, as fine-tuning with extremely small adapters does not capture the full representational potential of the model.

Conversely, setting $r_{\text{min}}$ too high, such as 512, offers no clear benefit in terms of policy performance, as the model already achieves a perfect success rate. The approximation error, as indicated by the loss curve in Fig. \ref{fig:terminal_ranks}, also shows negligible differences between $r_{\text{min}}$ values of 256 and 512.

\begin{table}[ht!]
\centering
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{8pt}
\caption{The configurations of DRIFT-DAgger for all simulation and real-world tasks. For simulation, we use other neural network policies trained with BC as experts, and compare the cosine similarity of the expert action and learner action with the threshold to determine whether the expert take over or not.}
\normalsize
\label{tab:exp_config}
\begin{comment}
\begin{tabular}{l|c|c|c|c}
    \hline
    Task & \makecell{Offline \\ Rollouts} & \makecell{Boot. \\ Epochs} & \makecell{Online \\ Iterations} & \makecell{Cos. Sim. \\ Threshold} \\ 
    \hline
    Robo-L      & 300  & 100  & 100 & 0.94  \\
    Robo-C      & 275  & 100  & 100 & 0.95  \\
    MVS-M     & 75   & 35   & 100 & 0.99  \\
    MVS-PnP   & 100  & 100  & 100 & 0.99  \\
    \hline 
    Real-BS   & 150  & 150  & 50  & - \\
    Real-DA   & 200  & 200  & 50  & - \\
    Real-DI   & 100  & 150  & 50  & - \\
    \hline
\end{tabular}    
\end{comment}
\begin{tabular}{l|c|c|c|c|c}
    \hline
    & Task & \makecell{\scriptsize Offline \\ \scriptsize Rollouts} & \makecell{\scriptsize Boot. \\ \scriptsize Epochs} & \makecell{\scriptsize Online \\ \scriptsize Iterations} & \makecell{\scriptsize Cos. Sim. \\ \scriptsize Threshold} \\ 
    \hline
    \multirow{4}{*}{\rotatebox{90}{\tiny Simulation Tasks}} 
    & Robo-L      & 300  & 100  & 100 & 0.94  \\
    & Robo-C      & 275  & 100  & 100 & 0.95  \\
    & MVS-M       & 75   & 35   & 100 & 0.99  \\
    & MVS-PnP     & 100  & 100  & 100 & 0.99  \\
    \hline 
    \multirow{3}{*}{\rotatebox{90}{\tiny Real-World Tasks}} 
    & Real-BS     & 150  & 150  & 50  & - \\
    & Real-DA     & 200  & 200  & 50  & - \\
    & Real-DI     & 100  & 150  & 50  & - \\
    \hline
\end{tabular}
\end{table}


\subsection{Benchmark Comparison}
\label{sec:sim_exp}


\begin{figure*}[htb]
\centering
\begin{minipage}{\textwidth}
    \centering    
    \renewcommand{\arraystretch}{0.8} 
    \setlength{\tabcolsep}{0pt} 
    
    \begin{tabular}{cccc}
        % Sim images
        \includegraphics[width=0.25\textwidth]{figures/sim/rs_lift_sim.pdf} &
        \includegraphics[width=0.25\textwidth]{figures/sim/rs_can_sim.pdf} &
        \includegraphics[width=0.25\textwidth]{figures/sim/mvs_microwave_sim.pdf} &
        \includegraphics[width=0.25\textwidth]{figures/sim/mvs_pnp_sim.pdf} \\
        
        % Plots
        \begin{minipage}{0.25\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/sim/rs_lift_plot.pdf}
            \subcaption{Robosuite - Lift}
        \end{minipage} &
        \begin{minipage}{0.25\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/sim/rs_can_plot.pdf}
            \subcaption{Robosuite - Can}
        \end{minipage} &
        \begin{minipage}{0.25\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/sim/mvs_microwave_plot.pdf}
            \subcaption{MVS - Microwave}
        \end{minipage} &
        \begin{minipage}{0.25\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/sim/mvs_pnp_plot.pdf}
            \subcaption{MVS - Pick and Place}
        \end{minipage} \\
    \end{tabular}
    \caption{The upper row shows the simulation scenarios from robosuite and Manipulation with Viewpoint Selection (MVS) tasks. The lower row shows the plots of success rate with respect to the number of expert labels. HG, D(L), D(LR), and D(RR) represent HG-DAgger, DRIFT-DAgger with LoRA adapters that are only instantiated with $r_{\text{min}}$ when switching to online mode, DRIFT-DAgger with LoRA and rank scheduler, and DRIFT-DAgger with rank modulation and rank scheduler.}
    \label{fig:simulation_environments}
\end{minipage}

\vspace{1em}

\begin{minipage}{\textwidth}
    \centering
    \renewcommand\arraystretch{1.2}
    \setlength{\tabcolsep}{1.1pt}
    \captionof{table}[Simulation Scenarios]{Summary of experimental results from simulation scenarios. The metrics include success rate (SR), mean and standard deviation of task duration (MSD), number of expert labels (NEL), and cumulative training time (CT). CT is measured in hours, MSD is measured in steps and at the scale of $\times 10^{2}$, and NEL is at the scale of $\times 10^{4}$}
    \label{tab:sim_scenarios}
    
    \begin{tabular}{l|cccc|cccc|cccc|cccc}
        \hline
        & \multicolumn{4}{c|}{Robosuite - Lift} & \multicolumn{4}{c|}{Robosuite - Can} &  \multicolumn{4}{c|}{MVS - Microwave} & \multicolumn{4}{c}{MVS - Pick and Place} \\
        \cline{2-17}
        & SR & MSD & NEL & CT & SR & MSD & NEL & CT & SR & MSD & NEL & CT & SR & MSD & NEL & CT\\
        \hline          %43.78 \pm 3.85
        Expert & $1.00$ & $0.43 \pm 0.03$ & - & - 
                        %117.00 \pm 56.50
               & $0.98$ & $1.17 \pm 0.56$ & - & -  
                        %300.19 \pm 29.58
               & $1.00$ & $3.00 \pm 0.29$ & - & - 
                        %303.81 \pm 69.38
               & $0.92$ & $3.03 \pm 0.69$ & - & - \\

                    %58.62 \pm 61.39 17174 
        BC & $\mathbf{1.00}$ & $0.58 \pm 0.61$ & $1.71$ & $1.66$
                    %152.68 \pm 119.11 40689
            & $0.90$ & $1.52 \pm 1.19$ & $4.06$ & $3.56$
                    %317.94 \pm 75.65 48534
            & $\mathbf{1.00}$ & $3.17 \pm 0.75$ & $4.85$ & $2.66$
                    %261.76 \pm 23.12 48682
            & $\mathbf{1.00}$ & $2.61 \pm 0.23$ & $4.86$ & $3.76$\\  
            
                    %168.12 \pm 140.24 15826 
        HG & $0.88$ & $1.68 \pm 1.40$ & $1.58$ & $1.62$
                    %120.60 \pm 80.17 31505
           & $\mathbf{0.96}$ & $\mathbf{1.20 \pm 0.80}$ & $\mathbf{3.15}$ & $3.30$  
                    %343.26 \pm 93.35 40129
           & $\mathbf{1.00}$ & $3.43 \pm 0.93$ & $4.01$ & $2.41$  
                    %254.42 \pm 23.54 32678
           & $\mathbf{1.00}$ & $\mathbf{2.54 \pm 0.23}$ & $3.26$ & $3.30$\\  

                        %73.14 \pm 61.64 15077
        D(L) & $\mathbf{1.00}$ & $0.73 \pm 0.61$ & $1.50$ & $1.48$
                        %133.14 \pm 108.83 32524
                & $0.92$ & $1.33 \pm 1.08$ & $3.25$ & $3.07$ 
                        %337.88 \pm 78.13 36046
                & $0.92$ & $3.37 \pm 0.78$ & $3.60$ & $2.03$  
                        %260.08 \pm 40.73 31498
                & $0.98$ & $2.60 \pm 0.40$ & $\mathbf{3.14}$ & $3.01$\\  

                        %276.6 \pm 217.27 14737
        D(LR) & $0.54$ & $2.76 \pm 2.17$ & $1.47$ & $1.47$
                        %361.23 \pm 158.81 38060
                    & $0.44$ & $3.61 \pm 1.58$ & $3.80$ & $3.20$ 
                            %473.90 \pm 42.96 48549
                    & $0.34$ & $4.73 \pm 0.42$ & $4.85$ & $2.34$ 
                            %354.94 \pm 84.05 34989 
                    & $0.58$ & $3.54 \pm 0.84$ & $3.49$ & $3.10$\\ 

                        %50.64 \pm 21.72 14649
        D(RR) & $\mathbf{1.00}$ & $\mathbf{0.50 \pm 0.21}$ & $\mathbf{1.46}$ & $\mathbf{1.41}$ 
                        %142.94 \pm 109.58 33440 
                    & $0.92$ & $1.42 \pm 1.09$ & $3.34$ & $\mathbf{2.97}$
                        %316.24 \pm 60.35 31727
                    & $\mathbf{1.00}$ & $\mathbf{3.16 \pm 0.60}$ & $\mathbf{3.17}$ & $\mathbf{1.84}$  
                            %273.24 \pm 50.96 32116
                    & $\mathbf{1.00}$ & $2.73 \pm 0.50$ & $3.21$ & $\mathbf{2.91}$ \\  
        \hline
    \end{tabular}
\end{minipage}

\end{figure*}

We perform a benchmark comparison in four environments: two from the robosuite~\cite{zhu2020robosuite} and two from the Manipulation with Viewpoint Selection (MVS) tasks \cite{sun2024comparative}. Fig. \ref{fig:simulation_environments} provides illustrations of the four environments.
The robosuite environments, Lift and 
Can, involve a Panda arm performing manipulation tasks, such as lifting a red cube and placing a can into a category. The state-action spaces for the robosuite tasks are the end-effector space with quaternions for rotation. The MVS tasks include opening a microwave, and the same pick-and-place scenario we used for previous ablation studies.  

The methods we evaluate in this benchmark comparison include Behavior Cloning (BC), HG-DAgger, and three variants of DRIFT-DAgger: one that uses LoRA, one that uses LoRA with rank scheduler, and one that uses rank modulation and rank scheduler. 
For methods utilizing rank scheduler, we apply the sigmoid decay function with steepness $\tau$ set to 0.5. We set $r_{\text{max}}$ and $r_{\text{min}}$ to 2048 and 256, respectively, based on the maximum rank of the diffusion policy and prior ablation study on the terminal rank. For all methods that use LoRA, we set the scaling factor $\alpha$ to 1.0. 
Experimental parameters related to the interactive mechanism, including the number of offline rollouts in $\mathcal{D}_B$, bootstrapping epochs $\mathcal{I}$, and online iterations $\mathcal{J}$, are provided in Table \ref{tab:exp_config}. 

During the online iterations, we save checkpoints every 20 iterations. Each checkpoint undergoes evaluation with 50 rollouts, and the success rate, mean and standard deviation of task duration, the number of expert labels, and cumulative training time are recorded as metrics for comparison. 
The success rate is used as the primary performance metric. The mean and standard deviation of task duration reflect how consistent and certain a trained policy is in completing a given task. A lower mean and standard deviation of task duration suggest that the policy is well-trained and converges better to the desired behavior. The number of expert labels, when considered alongside the other metrics, provides insight into the sample efficiency of a specific training method. For example, at the same level of success rate, a lower number of expert labels indicates better sample efficiency. The cumulative training time is for reflecting the training efficiency.

To fairly and efficiently evaluate different methods, for each scenario, we first train an expert policy using human-collected data from \citet{robomimic2021} and \citet{sun2024comparative} for robosuite and MVS tasks, respectively. The expert policy performs interventions when the cosine similarity between the learner actions and expert actions falls below a threshold, as detailed in Table \ref{tab:exp_config}. The thresholds are computed based on the mean cosine similarity between consecutive steps in the expert training datasets.

As shown in Fig. \ref{fig:simulation_environments} and Table \ref{tab:sim_scenarios}, DRIFT-DAgger variants demonstrate a pronounced reduction in cumulative training time compared to BC and HG-DAgger.  Additionally, all interactive IL methods exhibit superior expert sample efficiency compared to BC, as evidenced by higher success rates with respect to the number of expert labels. An exception is observed in the DRIFT-DAgger variant with LoRA and rank scheduler, where the merging and re-injection of LoRA adapters destabilize training due to the initialization of new trainable parameters. In contrast, the DRIFT-DAgger variant that instantiates LoRA adapters only once during the transition to the online phase, or the variant that combines rank modulation and rank scheduler, achieve performance and sample efficiency comparable to HG-DAgger, which consistently uses full ranks instead of reduced ranks. The benefits of interactive IL are more pronounced in tasks with longer durations. 
Furthermore, the policies learned with DRIFT-DAgger using the combination of rank modulation and rank scheduler, exhibit good stability and better convergence to the desired behavior of the task, as indicated by the relatively lower standard deviation in task duration.


\subsection{Batch Training Time}
\label{sec:abl_rm}
% Compare with other baselines
\begin{figure}[htbp]
\centering
\begin{minipage}{\columnwidth}
\centering
\includegraphics[width=\columnwidth]{figures/d_comparison.pdf}
\caption{Experimental results of three DRIFT-DAgger variants with HG-DAgger for demonstrating the reduced batch training time of DRIFT framework compared to full-rank training, while still maintaining equivalent performance.}
\label{fig:batch_training_time}
\end{minipage}

\vspace{1em} 

\begin{minipage}{\columnwidth}
\centering
\renewcommand\arraystretch{1.2}
\captionof{table}[Ablation on Training Strategies]{Summary of experimental results on mean batch training time (MBT) and success rate with full-rank and reduced-rank training methods.}
\label{tab:training_strategies}
\resizebox{\columnwidth}{!}{ 
    \begin{tabular}{l|c|c|c|c}
    \hline 
    Method & \makecell{Success \\ Rate} & \makecell{MBT \\ (Offline)} & \makecell{MBT \\ (Online)} & \makecell{MBT \\ (All Stages)} \\ 
    \hline
    HG             & $\mathbf{1.0}$ & 0.27 & 0.27 & 0.27 \\
    D(L)        & $\mathbf{1.0}$ & 0.27 & 0.23 & 0.26 \\
    D(LR)      & 0.56 & 0.27 & 0.23 & 0.26 \\
    D(RR)        & $\mathbf{1.0}$ & $\mathbf{0.26}$ & $\mathbf{0.22}$ & $\mathbf{0.24}$ \\
    \hline
    \end{tabular}
} 
\end{minipage}
\end{figure}
To better understand and demonstrate the improved training efficiency of DRIFT-DAgger, we conduct an ablation study on the mean batch training time across all stages, comparing different methods and variants. The methods evaluated in this ablation study include HG-DAgger and three DRIFT-DAgger variants: one using LoRA, one combining LoRA with a rank scheduler, and one employing rank modulation alongside rank scheduler. We use HG-DAgger as the baseline for full-rank training.

This ablation study is performed using the MVS pick-and-place scenario, same as \S\ref{sec:abl_rs} and \S\ref{sec:abl_tr}. We use 100 offline demonstration rollouts, 100 offline bootstrapping epochs, and 50 online iterations for training. We set terminal ranks $r_{min}$ to 256 and use the sigmoid decay function with $\tau$ set to 0.5 for DRIFT-DAgger variants applied.

The results are presented in Fig. \ref{fig:batch_training_time} and Table \ref{tab:training_strategies}. We observe that the DRIFT-DAgger variant with fixed-rank LoRA and the variant with rank modulation and rank scheduler both achieve success rates comparable to the full-rank baseline, HG-DAgger. When considering batch training time, the variant with rank modulation and rank scheduler reduces the mean batch training time across all stages by 11\%, demonstrating improved training efficiency without sacrificing performance. Specifically, during the online training stage, this variant achieves an 18\% reduction in batch training time compared to the full-rank baseline. 

In contrast, the DRIFT-DAgger variant with LoRA and rank scheduler also shows reduced training time. However, the success rate significantly drops compared to HG-DAgger. This decline is attributed to the instability caused by merging and re-injecting LoRA adapters, which is also reflected in the higher step loss observed for this variant. Additionally, despite the use of a rank scheduler, the batch training time for this variant is slightly higher than that of the variant with rank modulation, likely due to the additional computational overhead introduced by LoRA within the DRIFT framework.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/real_world_system.pdf}
\caption{The 17-DOF robotic system for real-world experiments aligns with the MVS simulation environments. It includes two xArm7 mounted on linear motors, with camera and gripper attached to each end effectors.}
\label{fig:real_world_system}
\end{figure}

% \subsection{Real-World Implications}

% To illustrate the potential impact of these efficiency gains in large-scale training scenarios, we extrapolate the results from \S\ref{sec:abl_rm} to a large dataset such as Open-X Embodiment \cite{o2024open}, which contains over 1 million trajectories. Assuming the same training configuration of 100 offline epochs and 50 online rollouts, a batch size equal to the average trajectory length, and that a terminal rank of $r_min=256$ would still be sufficient for high quality training, the proposed DRIFT-DAgger with rank modulation and rank scheduler could possibly save about 1250 hours (52 days) of training time compared to full-rank training methods.  While testing these results on such a large dataset were beyond the scope of our current work, we are eager to investigate such possibilities in the community going forward.     
