\section{Introduction}
\label{sec:introduction}

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/teaser.pdf}
\caption{This paper explores balancing overparameterization and training efficiency in diffusion policies by dynamically adjusting the frozen and trainable portions of weight matrices. In the top section of the figure, the learner, trained offline via behavior cloning with full-rank training, attempts to insert the upper drawer box into the container but fails due to collisions with both the container and the lower drawer box. In the bottom section, after efficient online adaptation with reduced trainable ranks, the learner efficiently improves its performance, successfully completing the task.}
% enabling more efficient online interactive imitation learning
\label{fig:teaser_img}
\vspace{-0.3cm}
\end{figure}

Diffusion policies have recently emerged as a powerful paradigm for robotic motion generation, demonstrating impressive performance across various manipulation tasks \cite{chi_dp}. Their strong performance is attributed to overparameterization, which has been shown to enhance both the generalization and representational capacity of neural networks \cite{dar_overparam}. However, this advantage comes with a significant drawback, which is the inefficiency of training overparameterized diffusion models \cite{zhang_overparam_dm}. While the machine learning community has made strides in addressing this inefficiency by leveraging the intrinsic low-rank structure of neural networks \cite{li_low_rank, li_low_rank2}, these approaches primarily focus on fine-tuning pre-trained models \cite{hu_lora, dettmers_qlora}. In robotics, however, policies are often trained from scratch, making these fine-tuning methods unsuitable.

To exploit the intrinsic low-rank structure, training from scratch in robotics necessitates a dynamic approach to balancing the trade-off between overparameterization, which provides strong representational power, and the efficiency gained from reduced-rank training. Ideally, at the beginning of training, maintaining high trainable ranks allows the policy to capture the general patterns of desired behaviors. As training progresses, the number of trainable ranks can be reduced to improve training efficiency, as the policy shifts to incremental refinement. This capability is particularly valuable in scenarios like interactive imitation learning (IL) with diffusion policies.



Before the adoption of diffusion policies, interactive IL methods typically used simple and compact network architectures as policy backbones. These methods were developed to address the sample inefficiency of behavior cloning (BC) \cite{sun_il, zare_il_survey}, and often involve an offline bootstrapping stage for initial training, followed by an online adaptation stage where experts provide corrective interventions to refine the policy. However, directly extending these methods to diffusion policies is impractical due to their significantly larger number of trainable parameters, which is often an order of magnitude greater than those of compact networks and results in substantially increased training times. This challenge undermines the feasibility of online interactive IL with diffusion policies.


Typically, diffusion policies are trained offline via BC, where a large dataset of demonstrations is collected, and training occurs in isolation. However, when these policies underperform, the expert must collect additional demonstrations targeting the challenging trajectories, provide corrective demonstrations, and retrain the policy iteratively in an offline manner. This process is both inefficient and unintuitive, as the expert often has limited insight into the trajectories where the policy struggles and may find it difficult to reproduce such challenging scenarios.

To address these limitations, we propose \textbf{D}ynamic \textbf{R}ank-adjustable D\textbf{IF}fusion Policy \textbf{T}raining (DRIFT), a framework designed to enable dynamic adjustments of the number of trainable parameters in diffusion policies through reduced-rank training. The framework introduces two key components, which are \emph{rank modulation} that leverages Singular Value Decomposition (SVD) \cite{strang2022introduction} to adjust the proportion of trainable and frozen ranks while maintaining the total rank constant, and \emph{rank scheduler} that dynamically modulates the number of trainable ranks during training using a decay function.

% Dynamic Diffusion Policies with Rank Adjustments for Training (DRIFT)

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/workflow.pdf}
\caption{DRIFT-DAgger combines offline policy bootstrapping with online adaptation. The gating function, following the nomenclature of HG-DAgger \cite{kelly_hg_dagger}, refers to expert intervention and demonstration when the learner reaches undesirable states during online adaptation. Compared to BC, DRIFT-DAgger reduces the need for expert labeling while maintaining high performance. The trainable rank reduction accelerates batch training, improving the usability and practicality of online adaptation for large models without sacrificing performance.}
\label{fig:workflow}
\end{figure}

To demonstrate and evaluate the effectiveness of DRIFT, we instantiate and implement it into DRIFT-DAgger, an expert-gated interactive IL method that incorporates reduced-rank training. As shown in Fig. \ref{fig:workflow}, DRIFT-DAgger uses low-rank component to speed up training of diffusion policies. By freezing a significant portion of the ranks during online adaptation, DRIFT-DAgger reduces training time, making online interactive IL with diffusion policies more practical.

Despite being inspired by existing parameter-efficient fine-tuning methods, DRIFT-DAgger with rank modulation and rank scheduler is specifically designed for training diffusion policies from scratch and dynamically adjusting the trainable ranks, avoiding the need to reinitialize and inject low-rank components during training. This design enhances stability and reduces the time for forward passes during each training batch (\S\ref{sec:abl_rm}). Additionally, we perform extensive ablation studies on different variants of rank schedulers (\S\ref{sec:abl_rs}), and minimum trainable ranks (\S\ref{sec:abl_tr}). By combining diffusion policies with online interactive IL, DRIFT-DAgger improves sample efficiency compared to training diffusion policies with BC (\S\ref{sec:sim_exp}). We also validate our methods in real-world scenarios (\S\ref{sec:real_exp}). Finally, we discuss the limitations and implications of our work (\S\ref{sec:discussion}). Our contributions are as follows:
\begin{itemize}
    \item We propose DRIFT, a framework for diffusion policies that includes rank modulation and rank scheduler as novel components that exploit the intrinsic low-rank structure of overparameterized models, balancing training efficiency and model performance.
    \item We instantiate DRIFT into DRIFT-DAgger, an interactive IL algorithm that combines offline bootstrapping with efficient online adaptation, enabling effective integration of expert feedback during the novice policy training.
    \item We perform extensive experiments to demonstrate that DRIFT-DAgger improves sample efficiency and reduces training time while achieving comparable performance to diffusion policies trained with full rank.
    \item We provide open-source implementations in Pytorch for the DRIFT framework and DRIFT-DAgger algorithm.\footnote{We are in the progress of preparing the code for release.} 
\end{itemize}


