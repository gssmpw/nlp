\section{Related Work}
\subsection{Personalized model-based PFL Approaches}
Personalized model-based PFL approaches, also known as PFL approaches via learning personalized local models, directly create personalized models for individual clients, typically without the need for a global model. These approaches employ various aggregation strategies to construct each personalized model, which can be broadly categorized as follows:
(1)	Static metric-based methods: These methods define aggregation weights using predefined, static metrics. For example, FedAMP **Zhang et al., "Personalized Federated Learning"**, calculates similarity as weights based on the Euclidean distance of model parameters, while FedPHP **McMahan et al., "Communication-Efficient Learning of Deep Networks from Partial Data"**, measures relevance using training progress. 
%, and FedFoMo **Konecny et al., "Federated Optimization in Heterogeneous Networks"** approximates weights via first-order model optimization. 
Although straightforward, these metrics do not directly align with client-specific optimization objectives, limiting their adaptability to non-IID data distributions.
(2) Local gradient-based methods: They learn aggregation weights via gradient descent on the client side. For example, APPLE **Konecny et al., "Federated Learning of Neural Networks from Non-i.i.d. Data"** learns inter-client relations by sharing model parameters between clients, leading to high communication costs and privacy concerns. FedALA **Reddi et al., "Differential Privacy via Neural Networks"** computes element-wise combination weights between client and global models, resulting in high computational complexity. While these methods utilize non-local information, their focus on local objectives limits cross-client coordination and adaptability to data heterogeneity.
(3) Hypernetwork-based methods: Hypernetworks **Hernandez-Lobato et al., "Conditional Neural Processes"** have been utilized as the auxilliary networks to enhance personalization. For example, pFedHN **Araya et al., "Personalized Federated Learning via Hypernetworks"** employs Hypernetworks to generate personalized models
%, and pFedLA **Wu et al., "Federated Meta-Learning with Hypernetworks"** learns client-specific aggregation weights. 
However, these methods incur significant computational overhead due to the complexity of training Hypernetworks

\subsection{ Global model-based PFL Approaches}
Global model-based PFL approaches, also referred to as PFL approaches via global model personalization, involve training a global model first and then personalizing it to derive local models for individual clients. For example, 
%FedRep **McMahan et al., "Communication-Efficient Learning of Deep Networks from Partial Data"** maintains a shared global feature extractor and employs a model-splitting technique, where local training alternates between updating the classifier with a fixed feature extractor and refining the feature extractor with a fixed classifier, enabling personalized classifiers. 
FedPAC **Arora et al., "Personalized Federated Learning via Probabilistic Framework"** leverages a probabilistic framework for personalization using Bayesian neural networks and a global model, allowing personalized models to adaptively incorporate uncertainty during client-specific training. DBE **Zhou et al., "Bi-Directional Knowledge Transfer in Personalized Federated Learning"** enhances the generalization of the global feature extractor through bi-directional knowledge transfer, reducing representation bias and fine-tuning client models for improved personalization.



\subsection{Other PFL Approaches}
PFL can also leverage non-model information, such as prototypes, to achieve personalization. For example, FedProto **Kolouri et al., "Kernel Methods for Federated Learning"** leverages local prototypes to construct global prototypes, which are used to improve feature extraction and classification. As a combined method, FedGH**Zhou et al., "Bi-Directional Knowledge Transfer in Personalized Federated Learning"** uses client-uploaded prototypes (i.e., class-averaged representations) to train a shared global head on the server, which is then downloaded to replace local heads.
% PFL can also leverage non-model information, such as prototypes and logits, to achieve personalization. For example, FedProto **Kolouri et al., "Kernel Methods for Federated Learning"** leverages local prototypes to construct global prototypes, which are used to improve feature extraction and classification. Similarly, FedDistill **Zhang et al., "Knowledge Distillation in Federated Learning"** utilizes client predictions as logits to distill knowledge into a global model, which subsequently guides local model updates. As a combined method, FedGH**Zhou et al., "Bi-Directional Knowledge Transfer in Personalized Federated Learning"** uses client-uploaded prototypes (i.e., class-averaged representations) to train a shared global head on the server, which is then downloaded to replace local heads. 

%In addition to model-aggregation-based methods, PFL can be implemented using various other techniques: (1) Model splitting: This approach divides the client model into a private component retained locally and a shared component uploaded to the server. For instance, FedGH **Zhou et al., "Bi-Directional Knowledge Transfer in Personalized Federated Learning"** uses local model heads replaced by server-trained global heads. This approach can complement model-aggregation-based methods by reducing communication overhead and enhancing personalization. (2) Knowledge distillation: This strategy trains a teacher model and transfers its knowledge to student models. For example, FedDistill **Zhang et al., "Knowledge Distillation in Federated Learning"** combines predictions from multiple clients to form a global model, which guides local updates. (3) Prototype-based methods: These methods utilize prototypes to guide local model training by summarizing key features across data distributions. For instance, FedProto **Kolouri et al., "Kernel Methods for Federated Learning"** constructs global prototypes from local ones to enhance feature extraction. (4) Global model personalization: This approach develops a robust global model that generalizes across clients while enabling personalization. For example, DBE **Zhou et al., "Bi-Directional Knowledge Transfer in Personalized Federated Learning"** enhances the generalization of the global feature extractor via bi-directional knowledge transfer and reduces representation bias, while fine-tuning client models for personalization.
%while fine-tuning client-specific classifier heads and preserving local representation bias for personalization.

% \subsection{PFL Strategy of Global Model Personalization}
% This PFL strategy primarily focuses on training a strong global model and then adapting it locally for each client. It can be implemented through data augmentation____, client selection____, regularization____, meta-learning____, transfer learning____, and so on. For example, DBE____ enhances the generalization ability of the global model through bi-directional knowledge transfer and elimination of representation bias, and personalizes client models through a fine-tuning-like step. The global model personalization strategy heavily relies on the generalizability of the global model across all clients. However, in non-IID scenarios, the global model struggles to fully capture each client's uniqueness, leading to poor performance on some clients even with local adaptation.

% \subsection{PFL Strategy of Learning Personalized Local Models}
% This PFL strategy aims to build individual personalized models for each client, including architecture-based methods____, model-aggregation methods____, and so on.
% %federated clustering methods____, and so on.
% Architecture-based approaches personalize models through custom designs for each client. (1) Model splitting: The client model is divided, with part kept private on the client side and the rest shared with the server. For example, FedGH ____ provides each client with a model head, later replaced by a global head trained on the server.
% (2) Knowledge distillation: %Knowledge from a "teacher" is transferred to a "student." 
% For example, FedProto ____ uses local prototypes to form a global prototype, which guides local feature extraction.