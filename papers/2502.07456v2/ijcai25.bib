
IEEEexample.bib 
V1.12 (2007/01/11)
Copyright (c) 2002-2007 by Michael Shell
See: http://www.michaelshell.org/
for current contact information.

This is an example BibTeX database for the official IEEEtran.bst
BibTeX style file.

Some entries call strings that are defined in the IEEEabrv.bib file.
Therefore, IEEEabrv.bib should be loaded prior to this file. 
Usage: 

\bibliographystyle{./IEEEtran}
\bibliography{./IEEEabrv,./IEEEexample}


Support sites:
http://www.michaelshell.org/tex/ieeetran/
http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
and/or
http://www.ieee.org/

*************************************************************************
Legal Notice:
This code is offered as-is without any warranty either expressed or
implied; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE! 
User assumes all risk.
In no event shall IEEE or any contributor to this code be liable for
any damages or losses, including, but not limited to, incidental,
consequential, or any other damages, resulting from the use or misuse
of any information contained here.

All comments are the opinions of their respective authors and are not
necessarily endorsed by the IEEE.

This work is distributed under the LaTeX Project Public License (LPPL)
( http://www.latex-project.org/ ) version 1.3, and may be freely used,
distributed and modified. A copy of the LPPL, version 1.3, is included
in the base LaTeX documentation of all distributions of LaTeX released
2003/12/01 or later.
Retain all contribution notices and credits.
** Modified files should be clearly indicated as such, including  **
** renaming them and changing author support contact information. **

File list of work: IEEEabrv.bib, IEEEfull.bib, IEEEexample.bib,
                   IEEEtran.bst, IEEEtranS.bst, IEEEtranSA.bst,
                   IEEEtranN.bst, IEEEtranSN.bst, IEEEtran_bst_HOWTO.pdf
*************************************************************************


Note that, because the example references were taken from actual IEEE
publications, these examples do not always contain the full amount
of information that may be desirable (for use with other BibTeX styles).
In particular, full names (not abbreviated with initials) should be
entered whenever possible as some (non-IEEE) bibliography styles use
full names. IEEEtran.bst will automatically abbreviate when it encounters
full names.

@article{10.1145/3298981,
author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
title = {Federated Machine Learning: Concept and Applications},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
doi = {10.1145/3298981},
abstract = {Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jan},
articleno = {12},
numpages = {19},
keywords = {Federated learning, GDPR, transfer learning}
}

@article{Huang_Chu_Zhou_Wang_Liu_Pei_Zhang_2021, 
title={Personalized Cross-Silo Federated Learning on Non-IID Data}, volume={35}, DOI={10.1609/aaai.v35i9.16960}, abstractNote={Non-IID data present a tough challenge for federated learning. In this paper, we explore a novel idea of facilitating pairwise collaborations between clients with similar data. We propose FedAMP, a new method employing federated attentive message passing to facilitate similar clients to collaborate more. We establish the convergence of FedAMP for both convex and non-convex models, and propose a heuristic method to further improve the performance of FedAMP when clients adopt deep neural networks as personalized models. Our extensive experiments on benchmark data sets demonstrate the superior performance of the proposed methods.}, number={9}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Huang, Yutao and Chu, Lingyang and Zhou, Zirui and Wang, Lanjun and Liu, Jiangchuan and Pei, Jian and Zhang, Yong}, year={2021}, month={May}, pages={7865-7873} } 

@InProceedings{10.1007/978-3-030-86486-6_36,
author="Li, Xin-Chun
and Zhan, De-Chuan
and Shao, Yunfeng
and Li, Bingshuai
and Song, Shaoming",
editor="Oliver, Nuria
and P{\'e}rez-Cruz, Fernando
and Kramer, Stefan
and Read, Jesse
and Lozano, Jose A.",
title="FedPHP: Federated Personalization with Inherited Private Models",
booktitle="Proceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Research Track",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="587--602",
abstract="Federated Learning (FL) generates a single global model via collaborating distributed clients without leaking data privacy. However, the statistical heterogeneity of non-iid data across clients poses a fundamental challenge to the model personalization process of each client. Our significant observation is that the newly downloaded global model from the server may perform poorly on local clients, while it could become better after adequate personalization steps. Inspired by this, we advocate that the hard-won personalized model in each communication round should be rationally exploited, while standard FL methods directly overwrite the previous personalized models. Specifically, we propose a novel concept named ``inHerited Private Model'' (HPM) for each local client as a temporal ensembling of its historical personalized models and exploit it to supervise the personalization process in the next global round. We explore various types of knowledge transfer to facilitate the personalization process. We provide both theoretical analysis and abundant experimental studies to verify the superiorities of our algorithm.",
isbn="978-3-030-86486-6"
}

@article{DBLP:journals/corr/abs-2012-08565,
  author       = {Michael Zhang and
                  Karan Sapra and
                  Sanja Fidler and
                  Serena Yeung and
                  Jos{\'{e}} M. {\'{A}}lvarez},
  title        = {Personalized Federated Learning with First Order Model Optimization},
  journal      = {CoRR},
  volume       = {abs/2012.08565},
  year         = {2020},
  eprinttype    = {arXiv},
  eprint       = {2012.08565},
  timestamp    = {Sat, 01 Jul 2023 10:38:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2012-08565.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Li_2021_CVPR,
    author    = {Li, Qinbin and He, Bingsheng and Song, Dawn},
    title     = {Model-Contrastive Federated Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {10713-10722}
}

@InProceedings{pmlr-v97-yurochkin19a,
  title = 	 {{B}ayesian Nonparametric Federated Learning of Neural Networks},
  author =       {Yurochkin, Mikhail and Agarwal, Mayank and Ghosh, Soumya and Greenewald, Kristjan and Hoang, Nghia and Khazaeni, Yasaman},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7252--7261},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/yurochkin19a/yurochkin19a.pdf},
  abstract = 	 {In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited. We develop a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to provide local neural network weights, which are modeled through our framework. We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision, data pooling and with as few as a single communication round. We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.}
}




@inproceedings{ijcai2022p301,
  title     = {Adapt to Adaptation: Learning Personalization for Cross-Silo Federated Learning},
  author    = {Luo, Jun and Wu, Shandong},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {2166--2173},
  year      = {2022},
  month     = {7},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2022/301}
}

@article{Zhang_Hua_Wang_Song_Xue_Ma_Guan_2023, title={FedALA: Adaptive Local Aggregation for Personalized Federated Learning}, volume={37}, DOI={10.1609/aaai.v37i9.26330}, abstractNote={A key challenge in federated learning (FL) is the statistical heterogeneity that impairs the generalization of the global model on each client. To address this, we propose a method Federated learning with Adaptive Local Aggregation (FedALA) by capturing the desired information in the global model for client models in personalized FL. The key component of FedALA is an Adaptive Local Aggregation (ALA) module, which can adaptively aggregate the downloaded global model and local model towards the local objective on each client to initialize the local model before training in each iteration. To evaluate the effectiveness of FedALA, we conduct extensive experiments with five benchmark datasets in computer vision and natural language processing domains. FedALA outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy. Furthermore, we also apply ALA module to other federated learning methods and achieve up to 24.19% improvement in test accuracy. Code is available at https://github.com/TsingZ0/FedALA.}, number={9}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zhang, Jianqing and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing}, year={2023}, month={Jun.}, pages={11237-11244} }

@article{DBLP:journals/corr/HaDL16,
  author       = {David Ha and
                  Andrew M. Dai and
                  Quoc V. Le},
  title        = {HyperNetworks},
  journal      = {CoRR},
  volume       = {abs/1609.09106},
  year         = {2016},
  eprinttype    = {arXiv},
  eprint       = {1609.09106},
  timestamp    = {Mon, 13 Aug 2018 16:48:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/HaDL16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v139-shamsian21a,
  title = 	 {Personalized Federated Learning using Hypernetworks},
  author =       {Shamsian, Aviv and Navon, Aviv and Fetaya, Ethan and Chechik, Gal},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9489--9502},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/shamsian21a/shamsian21a.pdf},
  abstract = 	 {Personalized federated learning is tasked with training machine learning models for multiple clients, each with its own data distribution. The goal is to train personalized models collaboratively while accounting for data disparities across clients and reducing communication costs. We propose a novel approach to this problem using hypernetworks, termed pFedHN for personalized Federated HyperNetworks. In this approach, a central hypernetwork model is trained to generate a set of models, one model for each client. This architecture provides effective parameter sharing across clients while maintaining the capacity to generate unique and diverse personal models. Furthermore, since hypernetwork parameters are never transmitted, this approach decouples the communication cost from the trainable model size. We test pFedHN empirically in several personalized federated learning challenges and find that it outperforms previous methods. Finally, since hypernetworks share information across clients, we show that pFedHN can generalize better to new clients whose distributions differ from any client observed during training.}
}

@ARTICLE{9743558,
  author={Tan, Alysa Ziying and Yu, Han and Cui, Lizhen and Yang, Qiang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Towards Personalized Federated Learning}, 
  year={2023},
  volume={34},
  number={12},
  pages={9587-9603},
  keywords={Data models;Training;Adaptation models;Collaborative work;Data privacy;Servers;Edge computing;Federated learning;Edge computing;federated learning (FL);non-IID data;personalized FL (PFL);privacy preservation;statistical heterogeneity},
  doi={10.1109/TNNLS.2022.3160699}}

@InProceedings{Ma_2022_CVPR,
    author    = {Ma, Xiaosong and Zhang, Jie and Guo, Song and Xu, Wenchao},
    title     = {Layer-Wised Model Aggregation for Personalized Federated Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10092-10101}
}



@INPROCEEDINGS{9252066,
  author={Gao, Yansong and Kim, Minki and Abuadbba, Sharif and Kim, Yeonjae and Thapa, Chandra and Kim, Kyuyeon and Camtep, Seyit A. and Kim, Hyoungshick and Nepal, Surya},
  booktitle={2020 International Symposium on Reliable Distributed Systems (SRDS)}, 
  title={End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things}, 
  year={2020},
  volume={},
  number={},
  pages={91-100},
  keywords={Performance evaluation;Training;Power demand;Distributed databases;Machine learning;Internet of Things;Convergence;split learning;federated learning;distributed machine learning;IoT},
  doi={10.1109/SRDS51746.2020.00017}}

@article{Thapa_Mahawaga_Arachchige_Camtepe_Sun_2022, title={SplitFed: When Federated Learning Meets Split Learning}, volume={36}, DOI={10.1609/aaai.v36i8.20825}, abstractNote={Federated learning (FL) and split learning (SL) are two popular distributed machine learning approaches. Both follow a model-to-data scenario; clients train and test machine learning models without sharing raw data. SL provides better model privacy than FL due to the machine learning model architecture split between clients and the server. Moreover, the split model makes SL a better option for resource-constrained environments. However, SL performs slower than FL due to the relay-based training across multiple clients. In this regard, this paper presents a novel approach, named splitfed learning (SFL), that amalgamates the two approaches eliminating their inherent drawbacks, along with a refined architectural configuration incorporating differential privacy and PixelDP to enhance data privacy and model robustness. Our analysis and empirical results demonstrate that (pure) SFL provides similar test accuracy and communication efficiency as SL while significantly decreasing its computation time per global epoch than in SL for multiple clients. Furthermore, as in SL, its communication efficiency over FL improves with the number of clients. Besides, the performance of SFL with privacy and robustness measures is further evaluated under extended experimental settings.}, number={8}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Thapa, Chandra and Mahawaga Arachchige, Pathum Chamikara and Camtepe, Seyit and Sun, Lichao}, year={2022}, month={Jun.}, pages={8485-8493} }


@InProceedings{pmlr-v139-zhu21b,
  title = 	 {Data-Free Knowledge Distillation for Heterogeneous Federated Learning},
  author =       {Zhu, Zhuangdi and Hong, Junyuan and Zhou, Jiayu},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12878--12889},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zhu21b/zhu21b.pdf},
  abstract = 	 {Federated Learning (FL) is a decentralized machine-learning paradigm, in which a global server iteratively averages the model parameters of local users without accessing their data. User heterogeneity has imposed significant challenges to FL, which can incur drifted global models that are slow to converge. Knowledge Distillation has recently emerged to tackle this issue, by refining the server model using aggregated knowledge from heterogeneous users, other than directly averaging their model parameters. This approach, however, depends on a proxy dataset, making it impractical unless such a prerequisite is satisfied. Moreover, the ensemble knowledge is not fully utilized to guide local model learning, which may in turn affect the quality of the aggregated model. Inspired by the prior art, we propose a data-free knowledge distillation approach to address heterogeneous FL, where the server learns a lightweight generator to ensemble user information in a data-free manner, which is then broadcasted to users, regulating local training using the learned knowledge as an inductive bias. Empirical studies powered by theoretical implications show that our approach facilitates FL with better generalization performance using fewer communication rounds, compared with the state-of-the-art.}
}

@inproceedings{NEURIPS2020_18df51b9,
 author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U and Jaggi, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {2351--2363},
 publisher = {Curran Associates, Inc.},
 title = {Ensemble Distillation for Robust Model Fusion in Federated Learning},
 volume = {33},
 year = {2020}
}

@ARTICLE{9090366,
  author={Wu, Qiong and He, Kaiwen and Chen, Xu},
  journal={IEEE Open Journal of the Computer Society}, 
  title={Personalized Federated Learning for Intelligent IoT Applications: A Cloud-Edge Based Framework}, 
  year={2020},
  volume={1},
  number={},
  pages={35-44},
  keywords={Internet of Things;Computational modeling;Data models;Adaptation models;Servers;Performance evaluation;Learning systems;Edge computing;federated learning;internet of things;personalization},
  doi={10.1109/OJCS.2020.2993259}}


@InProceedings{pmlr-v139-li21h,
  title = 	 {Ditto: Fair and Robust Federated Learning Through Personalization},
  author =       {Li, Tian and Hu, Shengyuan and Beirami, Ahmad and Smith, Virginia},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6357--6368},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/li21h/li21h.pdf},
  abstract = 	 {Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.}
}

@ARTICLE{10525198,
  author={Zhu, Zheqi and Shi, Yuchen and Fan, Pingyi and Peng, Chenghui and Letaief, Khaled B.},
  journal={IEEE Internet of Things Journal}, 
  title={ISFL: Federated Learning for Non-i.i.d. Data With Local Importance Sampling}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  keywords={Training;Monte Carlo methods;Data models;Convergence;Federated learning;Distributed databases;Deep learning;federated learning;importance sampling;non i.i.d data;water-filling optimization},
  doi={10.1109/JIOT.2024.3398398}}


@inproceedings{NEURIPS2020_f4f1f13c,
 author = {T. Dinh, Canh and Tran, Nguyen and Nguyen, Josh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21394--21405},
 publisher = {Curran Associates, Inc.},
 title = {Personalized Federated Learning with Moreau Envelopes},
 volume = {33},
 year = {2020}
}



@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}



@Book{MAL-083,
  editor={Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan },
  title={Advances and Open Problems in Federated Learning},
  year = {2021},
  volume = {14},
  publisher = {Foundations and Trends® in Machine Learning},
  booktitle = {Advances and Open Problems in Federated Learning},
  doi = {10.1561/2200000083},
  issn = {1935-8237},
  number = {1–2},
  pages = {1-210},
}




@InProceedings{pmlr-v54-mcmahan17a,  title =  {{Communication-Efficient Learning of Deep Networks from Decentralized Data}},  author =  {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Aguera y},  booktitle =  {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},  pages =  {1273--1282},  year =  {2017},  editor =  {Singh, Aarti and Zhu, Jerry},  volume =  {54},  series =  {Proceedings of Machine Learning Research},  month =  {20--22 Apr},  publisher =    {PMLR},  pdf =  {http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf},  abstract =  {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches.  We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent. }}

@ARTICLE{9141436,
  author={Duan, Moming and Liu, Duo and Chen, Xianzhang and Liu, Renping and Tan, Yujuan and Liang, Liang},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={Self-Balancing Federated Learning With Global Imbalanced Data in Mobile Systems}, 
  year={2021},
  volume={32},
  number={1},
  pages={59-71},
  keywords={Distributed databases;Training;Machine learning;Mobile handsets;Data models;Servers;Neural networks;Federated learning;distributed machine learning;neural networks},
  doi={10.1109/TPDS.2020.3009406}}

@INPROCEEDINGS{9533876,
  author={Li, Li and Duan, Moming and Liu, Duo and Zhang, Yu and Ren, Ao and Chen, Xianzhang and Tan, Yujuan and Wang, Chengliang},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={FedSAE: A Novel Self-Adaptive Federated Learning Framework in Heterogeneous Systems}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  keywords={Training;Performance evaluation;Neural networks;Machine learning;Collaborative work;Servers;Reliability},
  doi={10.1109/IJCNN52387.2021.9533876}}

@INPROCEEDINGS{10077950,
  author={Jin, Cheng and Chen, Xuandong and Gu, Yi and Li, Qun},
  booktitle={2022 IEEE 28th International Conference on Parallel and Distributed Systems (ICPADS)}, 
  title={FedDyn: A dynamic and efficient federated distillation approach on Recommender System}, 
  year={2023},
  volume={},
  number={},
  pages={786-793},
  keywords={Differential privacy;Federated learning;Perturbation methods;Multimedia Web sites;Distributed databases;Data models;Reliability;Distributed Machine Learning;Federated Learning;Recommender System;Knowledge Distillation},
  doi={10.1109/ICPADS56603.2022.00107}}

@inproceedings{NEURIPS2020_24389bfe,
 author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {3557--3568},
 publisher = {Curran Associates, Inc.},
 title = {Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach},
 volume = {33},
 year = {2020}
}

@ARTICLE{9076082,
  author={Chen, Yiqiang and Qin, Xin and Wang, Jindong and Yu, Chaohui and Gao, Wen},
  journal={IEEE Intelligent Systems}, 
  title={FedHealth: A Federated Transfer Learning Framework for Wearable Healthcare}, 
  year={2020},
  volume={35},
  number={4},
  pages={83-93},
  keywords={Data models;Medical services;Biomedical monitoring;Servers;Intelligent systems;Data privacy;Training;Wearable computing;Collaborative work;Federated learning;Transfer learning;Wearable healthcare},
  doi={10.1109/MIS.2020.2988604}}

@inproceedings{NEURIPS2023_2e0d3c6a,
 author = {Zhang, Jianqing and Hua, Yang and Cao, Jian and Wang, Hao and Song, Tao and XUE, Zhengui and Ma, Ruhui and Guan, Haibing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {14204--14227},
 publisher = {Curran Associates, Inc.},
 title = {Eliminating Domain Bias for Federated Learning in Representation Space},
 volume = {36},
 year = {2024},
note={[Online]. Available: \url{https://dl.acm.org/doi/10.5555/3666122.3666747}}
}

@inproceedings{10.1145/3581783.3611781,
author = {Yi, Liping and Wang, Gang and Liu, Xiaoguang and Shi, Zhuan and Yu, Han},
title = {FedGH: Heterogeneous Federated Learning with Generalized Global Header},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3581783.3611781},
abstract = {Federated learning (FL) is an emerging machine learning paradigm that allows multiple parties to train a shared model collaboratively in a privacy-preserving manner. Existing horizontal FL methods generally assume that the FL server and clients hold the same model structure. However, due to system heterogeneity and the need for personalization, enabling clients to hold models with diverse structures has become an important direction. Existing model-heterogeneous FL approaches often require publicly available datasets and incur high communication and/or computational costs, which limit their performances. To address these limitations, we propose a simple but effective Federated Global prediction Header (FedGH) approach. It is a communication and computation-efficient model-heterogeneous FL framework which trains a shared generalized global prediction header with representations extracted by heterogeneous extractors for clients' models at the FL server. The trained generalized global prediction header learns from different clients. The acquired global knowledge is then transferred to clients to substitute each client's local prediction header. We derive the non-convex convergence rate of FedGH. Extensive experiments on two real-world datasets demonstrate that FedGH achieves significantly more advantageous performance in both model-homogeneous and -heterogeneous FL scenarios compared to seven state-of-the-art personalized FL models, beating the best-performing baseline by up to 8.87\% (for model-homogeneous FL) and 1.83\% (for model-heterogeneous FL) in terms of average test accuracy, while saving up to 85.53\% of communication overhead.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {8686–8696},
numpages = {11},
keywords = {model heterogeneity, federated learning},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{Tan_Long_LIU_Zhou_Lu_Jiang_Zhang_2022, title={FedProto: Federated Prototype Learning across Heterogeneous Clients}, volume={36}, DOI={10.1609/aaai.v36i8.20819}, abstractNote={Heterogeneity across clients in federated learning (FL) usually hinders the optimization convergence and generalization performance when the aggregation of clients’ knowledge occurs in the gradient space. For example, clients may differ in terms of data distribution, network latency, input/output space, and/or model architecture, which can easily lead to the misalignment of their local gradients. To improve the tolerance to heterogeneity, we propose a novel federated prototype learning (FedProto) framework in which the clients and server communicate the abstract class prototypes instead of the gradients. FedProto aggregates the local prototypes collected from different clients, and then sends the global prototypes back to all clients to regularize the training of local models. The training on each client aims to minimize the classification error on the local data while keeping the resulting local prototypes sufficiently close to the corresponding global ones. Moreover, we provide a theoretical analysis to the convergence rate of FedProto under non-convex objectives. In experiments, we propose a benchmark setting tailored for heterogeneous FL, with FedProto outperforming several recent FL approaches on multiple datasets.}, number={8}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Tan, Yue and Long, Guodong and LIU, LU and Zhou, Tianyi and Lu, Qinghua and Jiang, Jing and Zhang, Chengqi}, year={2022}, month={Jun.}, pages={8432-8440} }

@ARTICLE{9832954,
  author={Ghosh, Avishek and Chung, Jichan and Yin, Dong and Ramchandran, Kannan},
  journal={IEEE Transactions on Information Theory}, 
  title={An Efficient Framework for Clustered Federated Learning}, 
  year={2022},
  volume={68},
  number={12},
  pages={8076-8091},
  keywords={Clustering algorithms;Collaborative work;Convergence;Data models;Partitioning algorithms;Distributed databases;Task analysis;Federated learning;clustering;alternating minimization},
  doi={10.1109/TIT.2022.3192506}}

@ARTICLE{9174890,
  author={Sattler, Felix and Müller, Klaus-Robert and Samek, Wojciech},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Clustered Federated Learning: Model-Agnostic Distributed Multitask Optimization Under Privacy Constraints}, 
  year={2021},
  volume={32},
  number={8},
  pages={3710-3722},
  keywords={Data models;Sociology;Statistics;Servers;Optimization;Privacy;Training;Clustering;distributed learning;federated learning;multi-task learning},
  doi={10.1109/TNNLS.2020.3015958}}

@inproceedings{zhang2024fedtgp,
  title={{FedTGP}: Trainable global prototypes with adaptive-margin-enhanced contrastive learning for data and model heterogeneity in federated learning},
  author={Zhang, Jianqing and Liu, Yang and Hua, Yang and Cao, Jian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={15},
  pages={16768--16776},
  year={2024}
}

@article{jeong2018communication,
  title={Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data},
  author={Jeong, Eunjeong and Oh, Seungeun and Kim, Hyesung and Park, Jihong and Bennis, Mehdi and Kim, Seong-Lyun},
  journal={arXiv preprint arXiv:1811.11479v2},
  year={2023}
}

@inproceedings{collins2021exploiting,
  title={Exploiting shared representations for personalized federated learning},
  author={Collins, Liam and Hassani, Hamed and Mokhtari, Aryan and Shakkottai, Sanjay},
  booktitle={International conference on machine learning},
  pages={2089--2099},
  year={2021},
  organization={PMLR}
}
@article{xu2023personalized,
  title={Personalized federated learning with feature alignment and classifier collaboration},
  author={Xu, Jian and Tong, Xinyi and Huang, Shao-Lun},
  journal={arXiv preprint arXiv:2306.11867},
  year={2023}
}



An example of a IEEEtran control entry which can change some IEEEtran.bst
settings. An entry like this must be cited via \bstctlcite{} command
before the first real \cite{}. The same entry key cannot be called twice
(just like multiple \cite{} of the same entry key place only one entry
in the bibliography.)
The available control fields are:

CTLuse_article_number
"no" turns off the display of the number for articles.
"yes" enables

CTLuse_paper
"no" turns off the display of the paper and type fields in inproceedings.
"yes" enables

CTLuse_forced_etal 
"no" turns off the forced use of "et al."
"yes" enables

CTLmax_names_forced_etal
The maximum number of names that can be present beyond which an "et al."
usage is forced. Be sure that CTLnames_show_etal (below)
is not greater than this value!

CTLnames_show_etal
The number of names that will be shown with a forced "et al.".
Must be less than or equal to CTLmax_names_forced_etal

CTLuse_alt_spacing 
"no" turns off the alternate interword spacing for entries with URLs.
"yes" enables

CTLalt_stretch_factor
If alternate interword spacing for entries with URLs is enabled, this is
the interword spacing stretch factor that will be used. For example, the
default "4" here means that the interword spacing in entries with URLs can
stretch to four times normal. Does not have to be an integer.

CTLdash_repeated_names
"no" turns off the "dashification" of repeated (i.e., identical to those
of the previous entry) names. IEEE normally does this.
"yes" enables

CTLname_format_string
The name format control string as explained in the BibTeX style hacking
guide.
IEEE style "{f.~}{vv~}{ll}{, jj}" is the default,

CTLname_latex_cmd
A LaTeX command that each name will be fed to (e.g., "\textsc").
Leave empty if no special font is desired for the names.
The default is empty.

CTLname_url_prefix
The prefix text used before URLs.
The default is "[Online]. Available:" A space will be inserted after this
text. If this space is not wanted, just use \relax at the end of the
prefix text.


Those fields that are not to be changed can be left out.
@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
  CTLuse_article_number     = "yes",
  CTLuse_paper              = "yes",
  CTLuse_forced_etal        = "yes",
  CTLmax_names_forced_etal  = "6",
  CTLnames_show_etal        = "6",
  CTLuse_alt_spacing        = "yes",
  CTLalt_stretch_factor     = "4",
  CTLdash_repeated_names    = "yes",
  CTLname_format_string     = "{f.~}{vv~}{ll}{, jj}",
  CTLname_latex_cmd         = "",
  CTLname_url_prefix        = "[Online]. Available:"
}

