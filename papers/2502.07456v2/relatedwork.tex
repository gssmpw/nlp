\section{Related Work}
\subsection{Personalized model-based PFL Approaches}
Personalized model-based PFL approaches, also known as PFL approaches via learning personalized local models, directly create personalized models for individual clients, typically without the need for a global model. These approaches employ various aggregation strategies to construct each personalized model, which can be broadly categorized as follows:
(1)	Static metric-based methods: These methods define aggregation weights using predefined, static metrics. For example, FedAMP \cite{Huang_Chu_Zhou_Wang_Liu_Pei_Zhang_2021} calculates similarity as weights based on the Euclidean distance of model parameters, while FedPHP \cite{10.1007/978-3-030-86486-6_36} measures relevance using training progress. 
%, and FedFoMo \cite{DBLP:journals/corr/abs-2012-08565} approximates weights via first-order model optimization. 
Although straightforward, these metrics do not directly align with client-specific optimization objectives, limiting their adaptability to non-IID data distributions.
(2) Local gradient-based methods: They learn aggregation weights via gradient descent on the client side. For example, APPLE \cite{ijcai2022p301} learns inter-client relations by sharing model parameters between clients, leading to high communication costs and privacy concerns. FedALA \cite{Zhang_Hua_Wang_Song_Xue_Ma_Guan_2023} computes element-wise combination weights between client and global models, resulting in high computational complexity. While these methods utilize non-local information, their focus on local objectives limits cross-client coordination and adaptability to data heterogeneity.
(3) Hypernetwork-based methods: Hypernetworks \cite{DBLP:journals/corr/HaDL16} have been utilized as the auxilliary networks to enhance personalization. For example, pFedHN \cite{pmlr-v139-shamsian21a} employs Hypernetworks to generate personalized models
%, and pFedLA \cite{Ma_2022_CVPR} learns client-specific aggregation weights. 
However, these methods incur significant computational overhead due to the complexity of training Hypernetworks

\subsection{ Global model-based PFL Approaches}
Global model-based PFL approaches, also referred to as PFL approaches via global model personalization, involve training a global model first and then personalizing it to derive local models for individual clients. For example, 
%FedRep \cite{collins2021exploiting} maintains a shared global feature extractor and employs a model-splitting technique, where local training alternates between updating the classifier with a fixed feature extractor and refining the feature extractor with a fixed classifier, enabling personalized classifiers. 
FedPAC \cite{xu2023personalized} leverages a probabilistic framework for personalization using Bayesian neural networks and a global model, allowing personalized models to adaptively incorporate uncertainty during client-specific training. DBE \cite{NEURIPS2023_2e0d3c6a} enhances the generalization of the global feature extractor through bi-directional knowledge transfer, reducing representation bias and fine-tuning client models for improved personalization.



\subsection{Other PFL Approaches}
PFL can also leverage non-model information, such as prototypes, to achieve personalization. For example, FedProto \cite{Tan_Long_LIU_Zhou_Lu_Jiang_Zhang_2022} leverages local prototypes to construct global prototypes, which are used to improve feature extraction and classification. As a combined method, FedGH\cite{10.1145/3581783.3611781} uses client-uploaded prototypes (i.e., class-averaged representations) to train a shared global head on the server, which is then downloaded to replace local heads.
% PFL can also leverage non-model information, such as prototypes and logits, to achieve personalization. For example, FedProto \cite{Tan_Long_LIU_Zhou_Lu_Jiang_Zhang_2022} leverages local prototypes to construct global prototypes, which are used to improve feature extraction and classification. Similarly, FedDistill \cite{jeong2018communication} utilizes client predictions as logits to distill knowledge into a global model, which subsequently guides local model updates. As a combined method, FedGH\cite{10.1145/3581783.3611781} uses client-uploaded prototypes (i.e., class-averaged representations) to train a shared global head on the server, which is then downloaded to replace local heads. 

%In addition to model-aggregation-based methods, PFL can be implemented using various other techniques: (1) Model splitting: This approach divides the client model into a private component retained locally and a shared component uploaded to the server. For instance, FedGH \cite{10.1145/3581783.3611781} uses local model heads replaced by server-trained global heads. This approach can complement model-aggregation-based methods by reducing communication overhead and enhancing personalization. (2) Knowledge distillation: This strategy trains a teacher model and transfers its knowledge to student models. For example, FedDistill \cite{jeong2023federated} combines predictions from multiple clients to form a global model, which guides local updates. (3) Prototype-based methods: These methods utilize prototypes to guide local model training by summarizing key features across data distributions. For instance, FedProto \cite{Tan_Long_LIU_Zhou_Lu_Jiang_Zhang_2022} constructs global prototypes from local ones to enhance feature extraction. (4) Global model personalization: This approach develops a robust global model that generalizes across clients while enabling personalization. For example, DBE \cite{NEURIPS2023_2e0d3c6a} enhances the generalization of the global feature extractor via bi-directional knowledge transfer and reduces representation bias, while fine-tuning client models for personalization.
%while fine-tuning client-specific classifier heads and preserving local representation bias for personalization.

% \subsection{PFL Strategy of Global Model Personalization}
% This PFL strategy primarily focuses on training a strong global model and then adapting it locally for each client. It can be implemented through data augmentation\cite{9141436}, client selection\cite{9533876}, regularization\cite{10077950}, meta-learning\cite{NEURIPS2020_24389bfe}, transfer learning\cite{9076082}, and so on. For example, DBE\cite{NEURIPS2023_2e0d3c6a} enhances the generalization ability of the global model through bi-directional knowledge transfer and elimination of representation bias, and personalizes client models through a fine-tuning-like step. The global model personalization strategy heavily relies on the generalizability of the global model across all clients. However, in non-IID scenarios, the global model struggles to fully capture each client's uniqueness, leading to poor performance on some clients even with local adaptation.

% \subsection{PFL Strategy of Learning Personalized Local Models}
% This PFL strategy aims to build individual personalized models for each client, including architecture-based methods\cite{10.1145/3581783.3611781,Tan_Long_LIU_Zhou_Lu_Jiang_Zhang_2022}, model-aggregation methods\cite{Huang_Chu_Zhou_Wang_Liu_Pei_Zhang_2021,10.1007/978-3-030-86486-6_36,ijcai2022p301,Zhang_Hua_Wang_Song_Xue_Ma_Guan_2023,pmlr-v139-shamsian21a,Ma_2022_CVPR,DBLP:journals/corr/abs-2012-08565}, and so on.
% %federated clustering methods\cite{9832954,9174890}, and so on.
% Architecture-based approaches personalize models through custom designs for each client. (1) Model splitting: The client model is divided, with part kept private on the client side and the rest shared with the server. For example, FedGH \cite{10.1145/3581783.3611781} provides each client with a model head, later replaced by a global head trained on the server.
% (2) Knowledge distillation: %Knowledge from a "teacher" is transferred to a "student." 
% For example, FedProto \cite{Tan_Long_LIU_Zhou_Lu_Jiang_Zhang_2022} uses local prototypes to form a global prototype, which guides local feature extraction.