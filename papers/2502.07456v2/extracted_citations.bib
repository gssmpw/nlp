@InProceedings{10.1007/978-3-030-86486-6_36,
author="Li, Xin-Chun
and Zhan, De-Chuan
and Shao, Yunfeng
and Li, Bingshuai
and Song, Shaoming",
editor="Oliver, Nuria
and P{\'e}rez-Cruz, Fernando
and Kramer, Stefan
and Read, Jesse
and Lozano, Jose A.",
title="FedPHP: Federated Personalization with Inherited Private Models",
booktitle="Proceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Research Track",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="587--602",
abstract="Federated Learning (FL) generates a single global model via collaborating distributed clients without leaking data privacy. However, the statistical heterogeneity of non-iid data across clients poses a fundamental challenge to the model personalization process of each client. Our significant observation is that the newly downloaded global model from the server may perform poorly on local clients, while it could become better after adequate personalization steps. Inspired by this, we advocate that the hard-won personalized model in each communication round should be rationally exploited, while standard FL methods directly overwrite the previous personalized models. Specifically, we propose a novel concept named ``inHerited Private Model'' (HPM) for each local client as a temporal ensembling of its historical personalized models and exploit it to supervise the personalization process in the next global round. We explore various types of knowledge transfer to facilitate the personalization process. We provide both theoretical analysis and abundant experimental studies to verify the superiorities of our algorithm.",
isbn="978-3-030-86486-6"
}

@inproceedings{10.1145/3581783.3611781,
author = {Yi, Liping and Wang, Gang and Liu, Xiaoguang and Shi, Zhuan and Yu, Han},
title = {FedGH: Heterogeneous Federated Learning with Generalized Global Header},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3581783.3611781},
abstract = {Federated learning (FL) is an emerging machine learning paradigm that allows multiple parties to train a shared model collaboratively in a privacy-preserving manner. Existing horizontal FL methods generally assume that the FL server and clients hold the same model structure. However, due to system heterogeneity and the need for personalization, enabling clients to hold models with diverse structures has become an important direction. Existing model-heterogeneous FL approaches often require publicly available datasets and incur high communication and/or computational costs, which limit their performances. To address these limitations, we propose a simple but effective Federated Global prediction Header (FedGH) approach. It is a communication and computation-efficient model-heterogeneous FL framework which trains a shared generalized global prediction header with representations extracted by heterogeneous extractors for clients' models at the FL server. The trained generalized global prediction header learns from different clients. The acquired global knowledge is then transferred to clients to substitute each client's local prediction header. We derive the non-convex convergence rate of FedGH. Extensive experiments on two real-world datasets demonstrate that FedGH achieves significantly more advantageous performance in both model-homogeneous and -heterogeneous FL scenarios compared to seven state-of-the-art personalized FL models, beating the best-performing baseline by up to 8.87\% (for model-homogeneous FL) and 1.83\% (for model-heterogeneous FL) in terms of average test accuracy, while saving up to 85.53\% of communication overhead.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {8686–8696},
numpages = {11},
keywords = {model heterogeneity, federated learning},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@INPROCEEDINGS{10077950,
  author={Jin, Cheng and Chen, Xuandong and Gu, Yi and Li, Qun},
  booktitle={2022 IEEE 28th International Conference on Parallel and Distributed Systems (ICPADS)}, 
  title={FedDyn: A dynamic and efficient federated distillation approach on Recommender System}, 
  year={2023},
  volume={},
  number={},
  pages={786-793},
  keywords={Differential privacy;Federated learning;Perturbation methods;Multimedia Web sites;Distributed databases;Data models;Reliability;Distributed Machine Learning;Federated Learning;Recommender System;Knowledge Distillation},
  doi={10.1109/ICPADS56603.2022.00107}}

@ARTICLE{9076082,
  author={Chen, Yiqiang and Qin, Xin and Wang, Jindong and Yu, Chaohui and Gao, Wen},
  journal={IEEE Intelligent Systems}, 
  title={FedHealth: A Federated Transfer Learning Framework for Wearable Healthcare}, 
  year={2020},
  volume={35},
  number={4},
  pages={83-93},
  keywords={Data models;Medical services;Biomedical monitoring;Servers;Intelligent systems;Data privacy;Training;Wearable computing;Collaborative work;Federated learning;Transfer learning;Wearable healthcare},
  doi={10.1109/MIS.2020.2988604}}

@ARTICLE{9141436,
  author={Duan, Moming and Liu, Duo and Chen, Xianzhang and Liu, Renping and Tan, Yujuan and Liang, Liang},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={Self-Balancing Federated Learning With Global Imbalanced Data in Mobile Systems}, 
  year={2021},
  volume={32},
  number={1},
  pages={59-71},
  keywords={Distributed databases;Training;Machine learning;Mobile handsets;Data models;Servers;Neural networks;Federated learning;distributed machine learning;neural networks},
  doi={10.1109/TPDS.2020.3009406}}

@ARTICLE{9174890,
  author={Sattler, Felix and Müller, Klaus-Robert and Samek, Wojciech},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Clustered Federated Learning: Model-Agnostic Distributed Multitask Optimization Under Privacy Constraints}, 
  year={2021},
  volume={32},
  number={8},
  pages={3710-3722},
  keywords={Data models;Sociology;Statistics;Servers;Optimization;Privacy;Training;Clustering;distributed learning;federated learning;multi-task learning},
  doi={10.1109/TNNLS.2020.3015958}}

@INPROCEEDINGS{9533876,
  author={Li, Li and Duan, Moming and Liu, Duo and Zhang, Yu and Ren, Ao and Chen, Xianzhang and Tan, Yujuan and Wang, Chengliang},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={FedSAE: A Novel Self-Adaptive Federated Learning Framework in Heterogeneous Systems}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  keywords={Training;Performance evaluation;Neural networks;Machine learning;Collaborative work;Servers;Reliability},
  doi={10.1109/IJCNN52387.2021.9533876}}

@ARTICLE{9832954,
  author={Ghosh, Avishek and Chung, Jichan and Yin, Dong and Ramchandran, Kannan},
  journal={IEEE Transactions on Information Theory}, 
  title={An Efficient Framework for Clustered Federated Learning}, 
  year={2022},
  volume={68},
  number={12},
  pages={8076-8091},
  keywords={Clustering algorithms;Collaborative work;Convergence;Data models;Partitioning algorithms;Distributed databases;Task analysis;Federated learning;clustering;alternating minimization},
  doi={10.1109/TIT.2022.3192506}}

@article{DBLP:journals/corr/HaDL16,
  author       = {David Ha and
                  Andrew M. Dai and
                  Quoc V. Le},
  title        = {HyperNetworks},
  journal      = {CoRR},
  volume       = {abs/1609.09106},
  year         = {2016},
  eprinttype    = {arXiv},
  eprint       = {1609.09106},
  timestamp    = {Mon, 13 Aug 2018 16:48:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/HaDL16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2012-08565,
  author       = {Michael Zhang and
                  Karan Sapra and
                  Sanja Fidler and
                  Serena Yeung and
                  Jos{\'{e}} M. {\'{A}}lvarez},
  title        = {Personalized Federated Learning with First Order Model Optimization},
  journal      = {CoRR},
  volume       = {abs/2012.08565},
  year         = {2020},
  eprinttype    = {arXiv},
  eprint       = {2012.08565},
  timestamp    = {Sat, 01 Jul 2023 10:38:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2012-08565.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Huang_Chu_Zhou_Wang_Liu_Pei_Zhang_2021, 
title={Personalized Cross-Silo Federated Learning on Non-IID Data}, volume={35}, DOI={10.1609/aaai.v35i9.16960}, abstractNote={Non-IID data present a tough challenge for federated learning. In this paper, we explore a novel idea of facilitating pairwise collaborations between clients with similar data. We propose FedAMP, a new method employing federated attentive message passing to facilitate similar clients to collaborate more. We establish the convergence of FedAMP for both convex and non-convex models, and propose a heuristic method to further improve the performance of FedAMP when clients adopt deep neural networks as personalized models. Our extensive experiments on benchmark data sets demonstrate the superior performance of the proposed methods.}, number={9}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Huang, Yutao and Chu, Lingyang and Zhou, Zirui and Wang, Lanjun and Liu, Jiangchuan and Pei, Jian and Zhang, Yong}, year={2021}, month={May}, pages={7865-7873} }

@InProceedings{Ma_2022_CVPR,
    author    = {Ma, Xiaosong and Zhang, Jie and Guo, Song and Xu, Wenchao},
    title     = {Layer-Wised Model Aggregation for Personalized Federated Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10092-10101}
}

@inproceedings{NEURIPS2020_24389bfe,
 author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {3557--3568},
 publisher = {Curran Associates, Inc.},
 title = {Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach},
 volume = {33},
 year = {2020}
}

@inproceedings{NEURIPS2023_2e0d3c6a,
 author = {Zhang, Jianqing and Hua, Yang and Cao, Jian and Wang, Hao and Song, Tao and XUE, Zhengui and Ma, Ruhui and Guan, Haibing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {14204--14227},
 publisher = {Curran Associates, Inc.},
 title = {Eliminating Domain Bias for Federated Learning in Representation Space},
 volume = {36},
 year = {2024},
note={[Online]. Available: \url{https://dl.acm.org/doi/10.5555/3666122.3666747}}
}

@article{Tan_Long_LIU_Zhou_Lu_Jiang_Zhang_2022, title={FedProto: Federated Prototype Learning across Heterogeneous Clients}, volume={36}, DOI={10.1609/aaai.v36i8.20819}, abstractNote={Heterogeneity across clients in federated learning (FL) usually hinders the optimization convergence and generalization performance when the aggregation of clients’ knowledge occurs in the gradient space. For example, clients may differ in terms of data distribution, network latency, input/output space, and/or model architecture, which can easily lead to the misalignment of their local gradients. To improve the tolerance to heterogeneity, we propose a novel federated prototype learning (FedProto) framework in which the clients and server communicate the abstract class prototypes instead of the gradients. FedProto aggregates the local prototypes collected from different clients, and then sends the global prototypes back to all clients to regularize the training of local models. The training on each client aims to minimize the classification error on the local data while keeping the resulting local prototypes sufficiently close to the corresponding global ones. Moreover, we provide a theoretical analysis to the convergence rate of FedProto under non-convex objectives. In experiments, we propose a benchmark setting tailored for heterogeneous FL, with FedProto outperforming several recent FL approaches on multiple datasets.}, number={8}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Tan, Yue and Long, Guodong and LIU, LU and Zhou, Tianyi and Lu, Qinghua and Jiang, Jing and Zhang, Chengqi}, year={2022}, month={Jun.}, pages={8432-8440} }

@article{Zhang_Hua_Wang_Song_Xue_Ma_Guan_2023, title={FedALA: Adaptive Local Aggregation for Personalized Federated Learning}, volume={37}, DOI={10.1609/aaai.v37i9.26330}, abstractNote={A key challenge in federated learning (FL) is the statistical heterogeneity that impairs the generalization of the global model on each client. To address this, we propose a method Federated learning with Adaptive Local Aggregation (FedALA) by capturing the desired information in the global model for client models in personalized FL. The key component of FedALA is an Adaptive Local Aggregation (ALA) module, which can adaptively aggregate the downloaded global model and local model towards the local objective on each client to initialize the local model before training in each iteration. To evaluate the effectiveness of FedALA, we conduct extensive experiments with five benchmark datasets in computer vision and natural language processing domains. FedALA outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy. Furthermore, we also apply ALA module to other federated learning methods and achieve up to 24.19% improvement in test accuracy. Code is available at https://github.com/TsingZ0/FedALA.}, number={9}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zhang, Jianqing and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing}, year={2023}, month={Jun.}, pages={11237-11244} }

@inproceedings{collins2021exploiting,
  title={Exploiting shared representations for personalized federated learning},
  author={Collins, Liam and Hassani, Hamed and Mokhtari, Aryan and Shakkottai, Sanjay},
  booktitle={International conference on machine learning},
  pages={2089--2099},
  year={2021},
  organization={PMLR}
}

@inproceedings{ijcai2022p301,
  title     = {Adapt to Adaptation: Learning Personalization for Cross-Silo Federated Learning},
  author    = {Luo, Jun and Wu, Shandong},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {2166--2173},
  year      = {2022},
  month     = {7},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2022/301}
}

@article{jeong2018communication,
  title={Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data},
  author={Jeong, Eunjeong and Oh, Seungeun and Kim, Hyesung and Park, Jihong and Bennis, Mehdi and Kim, Seong-Lyun},
  journal={arXiv preprint arXiv:1811.11479v2},
  year={2023}
}

@InProceedings{pmlr-v139-shamsian21a,
  title = 	 {Personalized Federated Learning using Hypernetworks},
  author =       {Shamsian, Aviv and Navon, Aviv and Fetaya, Ethan and Chechik, Gal},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9489--9502},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/shamsian21a/shamsian21a.pdf},
  abstract = 	 {Personalized federated learning is tasked with training machine learning models for multiple clients, each with its own data distribution. The goal is to train personalized models collaboratively while accounting for data disparities across clients and reducing communication costs. We propose a novel approach to this problem using hypernetworks, termed pFedHN for personalized Federated HyperNetworks. In this approach, a central hypernetwork model is trained to generate a set of models, one model for each client. This architecture provides effective parameter sharing across clients while maintaining the capacity to generate unique and diverse personal models. Furthermore, since hypernetwork parameters are never transmitted, this approach decouples the communication cost from the trainable model size. We test pFedHN empirically in several personalized federated learning challenges and find that it outperforms previous methods. Finally, since hypernetworks share information across clients, we show that pFedHN can generalize better to new clients whose distributions differ from any client observed during training.}
}

@article{xu2023personalized,
  title={Personalized federated learning with feature alignment and classifier collaboration},
  author={Xu, Jian and Tong, Xinyi and Huang, Shao-Lun},
  journal={arXiv preprint arXiv:2306.11867},
  year={2023}
}



An example of a IEEEtran control entry which can change some IEEEtran.bst
settings. An entry like this must be cited via \bstctlcite{} command
before the first real \cite{}. The same entry key cannot be called twice
(just like multiple \cite{} of the same entry key place only one entry
in the bibliography.)
The available control fields are:

CTLuse_article_number
"no" turns off the display of the number for articles.
"yes" enables

CTLuse_paper
"no" turns off the display of the paper and type fields in inproceedings.
"yes" enables

CTLuse_forced_etal 
"no" turns off the forced use of "et al."
"yes" enables

CTLmax_names_forced_etal
The maximum number of names that can be present beyond which an "et al."
usage is forced. Be sure that CTLnames_show_etal (below)
is not greater than this value!

CTLnames_show_etal
The number of names that will be shown with a forced "et al.".
Must be less than or equal to CTLmax_names_forced_etal

CTLuse_alt_spacing 
"no" turns off the alternate interword spacing for entries with URLs.
"yes" enables

CTLalt_stretch_factor
If alternate interword spacing for entries with URLs is enabled, this is
the interword spacing stretch factor that will be used. For example, the
default "4" here means that the interword spacing in entries with URLs can
stretch to four times normal. Does not have to be an integer.

CTLdash_repeated_names
"no" turns off the "dashification" of repeated (i.e., identical to those
of the previous entry) names. IEEE normally does this.
"yes" enables

CTLname_format_string
The name format control string as explained in the BibTeX style hacking
guide.
IEEE style "{f.~}{vv~}{ll}{, jj}

