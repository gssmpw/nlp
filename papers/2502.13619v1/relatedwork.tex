\section{Related Work}
\label{sec:related_work}

Previous work in ontology matching has explored the use of embeddings and language models. The review in \cite{DBLP:conf/semweb/SousaLT22} categorizes ontology matching approaches according to how information is incorporated into embeddings. More recently, LLMs have also been exploited \cite{10.1145/3587259.3627571,DBLP:conf/adbis/PeetersB23,DBLP:journals/pvldb/ZeakisPSK23,DBLP:journals/vldb/LiLSDT23}, where LLMs extract matching candidates through prompt engineering techniques. However, very few approaches have still addressed the issue of complex matching in ontologies. Matcha-DL \cite{DBLP:conf/om2/FariaSCFBP23} is an ontology matching approach that addresses both simple and complex ontology matching. It employs multiple metrics to identify alignments between ontologies, including lexical and structural metrics, background knowledge, and pattern-based complex matching. It also incorporates an LLM module to generate embeddings for entity labels, enhancing similarity computation, along with a translation module based on deep learning models. In this approach, a neural network is further employed to combine all similarity metrics, producing the final similarity score, which can be refined through training. Besides its potential, the matcher is still not evaluated in the complex matching task, and is difficult to compare its performance with other systems. 

In \cite{DBLP:conf/i-semantics/DhouibFT19,DBLP:journals/jodsn/DhouibFT21}, a complex matching approach is proposed, which applies word vector embeddings to encode entity labels that are subsequently refined with a radius measure. The initial step in this approach involves generating the embedding representation of entities. This representation is created using fastText \cite{DBLP:journals/tacl/BojanowskiGJM17} to embed each word within the textual information of the entity. Given that the textual information of an entity can comprise multiple words, an aggregation step is performed to merge the word embeddings into a single embedding for each entity. Following this step, a contextualized representation is generated for each entity by treating each entity as a cluster root. Then, the embeddings of the entities within its hierarchy are aggregated to generate the cluster embedding. Finally, a measure of the cluster's radius is employed to refine the relationship between the entities. For instance, if an entity is close to the cluster center, it indicates a broader and more general relation exists between them, as the radius encapsulates the concepts of all entities within the cluster. But in this work, the embedding models used are static and as shown in the results, they have worse performance compared to LLMs and transformer-based models. 

In \cite{DBLP:conf/dis/AkremiAZ22}, a fuzzy ontology embedding is applied to produce embeddings for complex matching. It employs a fuzzification strategy to generate a fuzzy representation of the ontology, which is subsequently encoded using a similar strategy to RDF2Vec. A series of random walks generate a document of sentences that are then input into Word2Vec to generate the final word embeddings. Then, embeddings are used to calculate the similarity between entities. Later, in \cite{DBLP:conf/inista/AkremiAZ23}, the above approach is improved to project the embeddings using a hypothesis graph \cite{DBLP:journals/biomedsem/AgibetovJOSBGCO18} that is subsequently encoded using traversal walking methods such as in RDF2Vec \cite{DBLP:conf/semweb/RistoskiP16}, and node2vec \cite{DBLP:conf/kdd/GroverL16} to generate sentences encoded with Word2Vec and fastText \cite{DBLP:journals/tacl/BojanowskiGJM17}. Besides the proposition of a complex matcher, the models based on random graph traversal are known to predict the relatedness instead of similarity \cite{DBLP:conf/esws/PortischCSKHP22}, as needed in the matching task reducing its performance.

In few works \cite{DBLP:conf/i-semantics/DhouibFT19,DBLP:journals/jodsn/DhouibFT21,DBLP:conf/dis/AkremiAZ22} embeddings are used in complex matching. However, the embeddings applied are still word embeddings like Glove that have less performance than the Transformer model embeddings, as observed in the experiments with these models in this work. Also, these works do not apply recent models like LLM which produces better representations of natural language. Also, in Matcha-DL \cite{DBLP:conf/om2/FariaSCFBP23} LLMs are applied in the matcher pipeline and the approach can be trained using data in a supervised manner. However, for the task, still few datasets with reference alignments for training are available and this type of data can be difficult to find and produce. For that reason, pre-trained models and unsupervised techniques are still preferred for this task as applied in this work. More recently, \cite{DBLP:journals/corr/abs-2404-10329} has proposed to use LLM to produce complex alignments where ChatGPT-4 is used to produce alignments between the ontologies GMO and GBO. The ontologies are modularized and the LLM is prompted to find the common modules between the two ontologies. However, this approach is effective while the complex entity modules are present and the majority of ontologies aren't modularized. The alignment response of the LLM contains natural language text that difficult automatic evaluation.