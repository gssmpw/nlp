\section{Discussion \& Future Development}
In this study, we presented \textbf{Gener}\textit{ator}, a generative genomic foundation model based on the transformer decoder architecture, meticulously trained on an extensive dataset of eukaryotic DNA sequences. Our model demonstrates state-of-the-art performance across various genomic tasks. This work marks a significant advancement in merging generative capabilities with genomic data analysis, delivering a model that not only comprehends the semantics of DNA but also excels in sequence design and optimization.

A pivotal feature of the \textbf{Gener}\textit{ator} is its adherence to the central dogma of molecular biology, generating protein-coding DNA sequences that can produce proteins analogous to those in established families, such as Histones and Cytochrome P450. This capability is validated by the structural congruence of translated proteins with existing structures, as verified by AlphaFold and Foldseek analyses. Additionally, in the sequence design task, the \textbf{Gener}\textit{ator} demonstrates its capability in optimizing enhancer sequences, achieving specific activity profiles with precision and computational efficiency. This underscores the model’s potential to advance the field of synthetic biology, facilitating precise sequence design and optimization and enhancing our understanding and manipulation of complex biological systems.

The remarkable performance of the \textbf{Gener}\textit{ator} can be primarily attributed to its training on a diverse and extensive dataset, which endows it with enhanced robustness and versatility in genomic data analysis. Through comprehensive investigation and careful selection of suitable tokenization and data filtering strategies, the \textbf{Gener}\textit{ator} efficiently captures the semantic richness of functional DNA regions, consequently achieving outstanding results in downstream tasks.

Nevertheless, this work also reveals certain challenges and limitations. A notable limitation of the \textbf{Gener}\textit{ator} is its exclusive focus on eukaryotic DNA, without considering prokaryotic and viral data. In this context, the \textbf{Gener}\textit{ator} serves as a complementary model to Evo, which is trained on prokaryotic and viral genomes. Consequently, we lack appropriate tasks to directly compare the \textbf{Gener}\textit{ator} with Evo~\cite{Evo}. This delineation reflects the inherent differences in cellular mechanisms between eukaryotes and prokaryotes, despite both utilizing DNA as genetic material. Variations in gene architecture and organelle types justify this specialized focus. Considering the computational costs of training LLMs, partitioning datasets into eukaryotic and prokaryotic-plus-viral categories emerges as a practical and effective strategy.

In future work, we plan to train a prokaryotic-plus-viral version of \textbf{Gener}\textit{ator}, facilitating a fair and comprehensive comparison with Evo. Additionally, the remarkable performance of the \textbf{Gener}\textit{ator} in the gene classification task suggests its potential for gene annotation tasks, which involve identifying gene regions within entire DNA sequences. We are currently developing a specialized model, \textbf{Gener}\textit{anno}, dedicated to this endeavor. This model is in the training phase and will be detailed in our subsequent work.

In summary, the \textbf{Gener}\textit{ator} exemplifies the potential of generative models in genomics, with its primary contribution lying in fostering innovation that enhances current biological understanding. Serving as a foundational tool in genomic research, the model holds promise for significant contributions to scientific and technological advances in genomics. The prospective integration of the \textbf{Gener}\textit{ator} into practical applications, such as clinical genomics, warrants further exploration. By aligning sequence generation capabilities with specific therapeutic objectives, the \textbf{Gener}\textit{ator} could play a pivotal role in advancing precision medicine and biotechnology interventions.

We are committed to promoting transparency and collaboration in research. To this end, all necessary materials to replicate this work—including data, code, and model weights, will be made fully open-source on the GenerTeam GitHub \href{https://github.com/GenerTeam}{\textcolor{blue}{page}}. We hope that this contribution will help to advance the development of the DNA language model community.


