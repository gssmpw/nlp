\section{Method}

\subsection{Data Preparation}
\label{sec:data_preparation}
For training the \textbf{Gener}\textit{ator} model, we sourced raw DNA sequences from all eukaryotic organisms in the RefSeq database. We explored two data processing strategies:
\begin{enumerate}
    \item \textbf{Gene Sequence Training}: Utilizing the rich annotation data available in RefSeq, we isolated gene regions from genomic sequences. These regions encompass a wide array of functionalities, including transcription into various RNA molecules, translation into complex proteins, and regulatory functions such as promoters and enhancers that control gene expression. Defined broadly as gene regions, these biologically functional DNA segments formed our training samples, totaling 386B nucleotides.
    \item \textbf{Whole Sequence Training}: In this approach, we fed a mixture of gene and non-gene DNA sequences from all eukaryotic organisms in RefSeq directly into the language model for training. This dataset includes approximately 2T nucleotides. This strategy aligns with the conventional pre-training paradigm for general LLMs and forms the foundational training approach seen in existing DNA language models.
\end{enumerate} 
Generally, while Scheme 2 displays a lower pre-training loss (\textit{Fig.} \ref{fig:pretrain_loss}), Scheme 1 consistently outperforms Scheme 2 across a range of downstream tasks (even for tasks where non-gene context dominates, as demonstrated in \textit{Sec.} \ref{sec:experiments}). One possible explanation for this seemingly counterintuitive result is the inherent difference between DNA and human language. DNA is not a `concise language' but rather filled with `randomness' and `redundancy'. If we hypothesize that, at the origin of life, DNA was nearly entirely random, then over billions of years of evolution, processes such as mutation, recombination, and natural selection have randomly given rise to a limited number of biologically functional segments \cite{MLMSC, hemiplasy}. These specific nucleotide sequences, which have biological functions, represent the `semantics' of DNA~\cite{RefSeq}. In contrast, the majority of non-gene regions, while potentially holding undiscovered genes, largely consist of segments that have not yet acquired function (or semantic meaning). These regions often include highly repetitive and simple segments, such as long stretches of \texttt{AAAAAA} or \texttt{GCGCGC}~\cite{dna-msa}, contributing to a lower pre-training loss.

Supporting this hypothesis, observed mutation rates in gene regions are significantly lower than in non-gene regions~\cite{mutation-rates}. This difference arises because mutations in gene regions tend to have pronounced effects on biological functions, leading to negative selection pressures, whereas non-gene regions can accommodate more variability. Many genetic editing techniques~\cite{CRISPR} exploit this property by replacing non-functional or redundant non-gene regions with meaningful gene sequences to endow new functions. Based on this assumption, incorporating non-gene regions does not merely function as `increasing data volume' but could be seen as `contaminating high-quality data'. This insight offers an explanation for the superior performance of Scheme 1, where training is concentrated on semantically rich regions, thereby enhancing model efficacy on downstream tasks.

\subsection{Tokenization}
\label{sec:tokenization}
In this section, we examine the process of tokenizing DNA sequences into suitable tokens for input into language models. The widely used tokenizers in this field include:
\begin{enumerate}
    \item \textbf{Single Nucleotide Tokenizer}: This tokenizer uses individual nucleotides (\texttt{A}, \texttt{T}, \texttt{C}, \texttt{G}) as tokens, providing the finest granularity and serving as the most foundational tokenizer. It is employed by models like Evo, Caduceus, HyenaDNA, and megaDNA.
    \item \textbf{K-mer Tokenizer}: This widely used tokenizer for DNA sequences treats all possible combinations of K consecutive nucleotides as tokens. Models like DNABERT and NT utilize this approach.
    \item \textbf{Byte Pair Encoding (BPE) Tokenizer}~\cite{SentencePiece}: Commonly used in NLP, this tokenizer learns a vocabulary by aggregating frequent subwords. Models such as DNABERT-2 and GROVER adopt this method.
\end{enumerate}
Overall, the selection of an appropriate tokenizer involves a trade-off between sequence resolution and contextual coverage. The single nucleotide tokenizer provides the highest resolution, capturing details at the finest level. However, for a fixed number of tokens, this increased granularity results in a reduced context window compared to K-mer or BPE tokenizers. Additionally, the computational cost associated with attention calculation escalates quadratically as sequence length extends. To mitigate this challenge, models like HyenaDNA, Evo, and Caduceus adopt more streamlined state space models~\cite{ssm}, such as Hyena~\cite{Hyena}, StripedHyena~\cite{StripedHyena}, and BiMamba~\cite{Caduceus}, to achieve longer context lengths. However, as our experiments demonstrate, they still fall short in achieving equivalent long-text modeling capabilities. These constraints underscore the difficulty in training DNA language models. Unlike protein sequences, which typically consist of hundreds to a few thousand amino acids and can be effectively handled by transformer architectures in protein language models~\cite{esm2}, gene segments often extend to tens of thousands of nucleotides, as illustrated in \textit{Fig.} \ref{fig:model_overview}A. This emphasizes the importance of selecting an appropriate tokenizer that strikes a balance between granularity and context window size.

In separate investigations, DNABERT-2~\cite{DNABERT-2} and GROVER~\cite{GROVER} assessed how tokenizer choice impacts masked language models, both concluding that BPE is the optimal choice, albeit with varying vocabulary sizes tailored to their datasets. Prior to our research, the impact of DNA tokenizer selection for causal language models had not been thoroughly explored. Although we initially hypothesized that BPE would be most effective in this context, our empirical findings indicate that the K-mer tokenizer significantly outperforms others in the next token prediction (NTP) pre-training. A detailed comparison of tokenizers is provided in \textit{Sec.} \ref{sec:kmer_predition}, where the 6-mer model emerges as the most effective among our experiments. This may seem counterintuitive, as it goes against common experiences in NLP~\cite{SentencePiece}. However, upon closer analysis of their pre-training paradigms, it becomes evident that NTP pre-training is not optimally aligned with BPE tokenization for genomic sequences. This misalignment stems from the hierarchical nature of the BPE vocabulary, where words frequently have nested relationships. For instance, with a target next token like \texttt{GCCT}, predictions such as \texttt{G}, \texttt{GC}, or \texttt{GCC} would be deemed incorrect, inadvertently complicating the training process.

In contrast, the masked language modeling (MLM) task does not encounter this issue. When a token in the sequence is masked, MLM's learning objective is to fill in the blank with precisely one token from the vocabulary, eliminating the possibility of multiple partially correct answers. This observation further underscores the intrinsic differences between DNA sequences and human language. Unlike human language, which has clear lexical boundaries, DNA sequences lack such explicit delimiters, suggesting that BPE's strong performance in human language may not necessarily translate to DNA sequences. This recognition is one of the critical objectives of our work. While the fundamental approach of LLMs can be adapted for DNA, our aim is to develop a foundation model specifically tailored to the unique characteristics of DNA data.

\subsection{Pre-training}
In this section, we describe the pre-training phase of the \textbf{Gener}\textit{ator}, designed to effectively and efficiently handle DNA sequence data using the gene sequence training strategy alongside a 6-mer tokenizer.

In terms of model architecture, we broadly follow the structure of Llama~\cite{llama,tiny-llama}, with the detailed configuration shown in Table \ref{tab:model_arch}. The pre-training process employs a batch size of 2 million tokens and utilizes the AdamW~\cite{adamw} optimizer, coupled with a cosine learning rate scheduler with a warm-up phase. The entire pre-training spans 6 epochs, processing a total of 386B tokens. To mitigate discrepancies arising from variable tokenization starting points, our K-mer tokenizer introduces a randomized starting position between 0 and 5 for each sample. This method enhances the model's robustness against variations in sequence alignment. To improve the efficiency of long-context pre-training, we utilize Flash Attention~\cite{flashattention} and Zero Redundancy Optimizer~\cite{deepspeed}. More details regarding the pre-training process are provided in the appendix. Overall, our configuration effectively harnesses the potential of transformer architectures while being specifically tailored to the nuances of DNA sequence data, thereby advancing the model's capabilities in genomic modeling.

\input{tables/model_arch}

\subsection{Downstream Tasks}
In this section, we provide an overview of the evaluation datasets and describe relevant downstream tasks. We draw on two established benchmarks: Genomic Benchmarks~\cite{genomic-benchmarks} and NT tasks~\cite{nucleotide-transformer}, and introduce new tasks and datasets.

\subsubsection{Sequence Classification}
Both Genomic Benchmarks and NT tasks focus on sequence classification. Genomic Benchmarks emphasize human data but also include organisms like mouse, roundworm, and fruit fly. Its datasets are categorized into human regulatory elements (e.g., promoters, enhancers), demo datasets for species or transcript type classification, and dummy datasets for rapid prototyping. In contrast, NT tasks cover a broader species range and involve tasks such as promoter classification, enhancer classification, yeast-based epigenetic marks prediction, and splice site identification across over 100 organisms. Despite the advantages of these benchmarks, they share a significant limitation: the sequences they utilize are predominantly short, typically measuring in the hundreds of nucleotides. This limitation significantly reduces their relevance to practical applications, where longer sequences are often encountered. To address this issue, we introduce two new sequence classification tasks: gene classification and taxonomic classification.

The gene classification task assesses the model's ability to understand short to medium-length sequences, ranging from 100 to 5000 bp. It includes six different gene types and control samples drawn from non-gene regions, with balanced sampling from six distinct eukaryotic taxonomic groups in RefSeq. The classification goal is to predict the gene type. The taxonomic classification task is designed to assess the model's comprehension of longer sequences, which include both gene and predominantly non-gene regions, ranging in length from 10,000 to 100,000 bp. Samples are similarly balanced and sourced from RefSeq across the same six taxonomic groups, with the objective being to predict the taxonomic group of each sample.

\subsubsection{Next K-mer Prediction}
In addition to sequence classification tasks, we have designed a task to evaluate the generative capabilities of models: next K-mer prediction. Defining metrics to assess generative ability is notably challenging, as the generated sequences often lack a direct reference point, making quantitative evaluation difficult. In NLP, perplexity (PPL) is commonly used to measure generative capability. However, for DNA sequences, there is significant variation in PPL across different tokenizers, precluding direct comparison. Therefore, we resort to next K-mer prediction.

The concept of next K-mer prediction was first introduced by GROVER~\cite{GROVER}. It involves inputting a sequence segment into the model and having it predict the next K base pairs. The predicted sequence is then compared to the actual sequence to assess accuracy. In GROVER, this evaluation occurs after fine-tuning the pre-trained model. In this paper, we present next K-mer prediction as a zero-shot task to universally evaluate the generative capabilities of various pre-trained models. For masked language models, we design a specific autoregressive generation strategy: a \texttt{<mask>} token is appended to the end of the input sequence at each step, and the prediction for this \texttt{<mask>} token is added to the input sequence for the subsequent step.

Importantly, the evaluation dataset is meticulously constructed to ensure that the region subjected to generation is confined to gene regions, as the primary objective of sequence generation is to produce functional and meaningful DNA sequences. Conversely, we also ensure that the input segment contains a substantial (or even predominant) amount of non-gene content, enabling models utilizing the whole sequence training to fully exploit the input context.

\subsubsection{Central Dogma}
In addition to the aforementioned tasks designed for evaluating and comparing model performance, we have developed two downstream tasks that align more closely with practical applications: central dogma and sequence design. A crucial criterion for assessing the practicality of generative DNA LLMs is their ability to create functional and meaningful DNA sequences. The central dogma of molecular biology describes the process by which DNA is transcribed and translated into proteins, enabling life functions~\cite{central-dogma}. We specifically design experiments to test whether the fine-tuned \textbf{Gener}\textit{ator} model can generate protein-coding DNA sequences that translate into proteins structurally similar to those in a target protein family.

We sourced protein-coding DNA sequences from specific protein families and fine-tuned the \textbf{Gener}\textit{ator} on these sequences, directing it to generate similar protein-coding DNA sequences. The newly generated DNA sequences are translated into protein sequences using a codon table and compared against the target protein families. To further assess the generated proteins, we examine their conformity to the natural distribution using the perplexity metric from Progen2~\cite{progen2}, and verify their foldability with AlphaFold3~\cite{AlphaFold3}.

\subsubsection{Sequence Design}
\label{sec:sequence_design}
As a central and promising field in biology, sequence design plays a crucial role in advancing our understanding and manipulation of biological systems~\cite{sequence-design,protein-design}. A notable example is Evo~\cite{Evo}, which undertakes the design of CRISPR-Cas molecular complexes using a methodology akin to our `central dogma' approach. By fine-tuning on target sequences, Evo generates millions of candidate sequences that mirror the distribution of these targets. Suitable samples are then selected through external evaluation.

In contrast, we employ a more efficient and flexible approach, refining supervised fine-tuning to accommodate prompt-responsive sequence generation. In our sequence design task, we utilize enhancer sequences and activity data from the DeepSTARR~\cite{DeepSTARR} dataset. We select enhancer sequences from the top and bottom quartiles of activity values. These samples are labeled with prompts \texttt{<high>} for high-activity and \texttt{<low>} for low-activity enhancers. This refined fine-tuning approach enables the design of enhancer sequences with desired activity profiles, illustrating the model's capability in biologically directed sequence optimization.
