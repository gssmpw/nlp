\section{Related Work}

\subsection{Large Language Models in Biosciences}
Large language models (LLMs) have emerged as powerful tools for natural language comprehension and generation~\cite{llms-survey}. Beyond their application in traditional natural language tasks, there is a growing interest in leveraging LLMs to accelerate scientific research. Early studies revealed that general-purpose LLMs, owing to their rich pre-training data, exhibit promise across various research domains~\cite{ai4science}. Subsequent efforts have focused on directly training LLMs using domain-specific data, aiming to extend the transfer learning paradigm from natural language processing (NLP) to biosciences. This body of work primarily falls into three categories: molecular LLMs, protein LLMs, and genomic LLMs.

For molecular modeling, extensive work has been conducted on training with various molecular string representations, such as SMILES~\cite{Smiles-bert,space-of-chemical,large-scale-chemical}, SELFIES~\cite{SELFIES,chemberta,chemberta2}, and InChI~\cite{inchi}. Additionally, several studies address the modeling of molecular 2D~\cite{mol-2d} and 3D structures~\cite{uni-mol} to capture more detailed molecular characteristics. In the realm of protein LLMs, related work~\cite{msa-transformer,esm2,Prottrans} mainly concentrates on modeling the primary structure of proteins (amino acid sequences), providing a solid foundation for protein structure prediction~\cite{AlphaFold2,AlphaFold3}. For genomic sequences, numerous studies have attempted to leverage the power of LLMs for improved genomic analysis and understanding. These efforts predominantly involve training models on DNA~\cite{BPNet,DNABERT,enformer,nucleotide-transformer,DNABERT-2,GROVER,gena-lm,Caduceus,dnagpt,megaDNA,HyenaDNA,Evo} and RNA~\cite{RNAErnie,uni-rna,Rinalmo} sequences. In the following section, we delve deeper into genomic LLMs specifically designed for DNA sequence modeling.

\subsection{DNA Language Models}
In the early stages, \citeauthor{BPNet} introduced the BPNet convolutional architecture to learn transcription factor binding patterns and their syntax in a supervised manner. Prior to the emergence of large-scale pre-training, BPNet was widely used in genomics for supervised learning on relatively small datasets. With the advent of BERT~\cite{BERT}, DNABERT~\cite{DNABERT} pioneered the application of pre-training on the human genome using K-mer tokenizers. To effectively capture long-range interactions, Enformer~\cite{enformer} advanced human genome modeling by incorporating convolutional downsampling into transformer architectures.

Following these foundational works, numerous models based on the transformer encoder architecture have emerged. A notable example is the Nucleotide Transformer (NT)~\cite{nucleotide-transformer}, which scales model parameters from 100 million to 2.5 billion and includes a diverse set of multispecies genomes. Recent studies, DNABERT-2~\cite{DNABERT-2} and GROVER~\cite{GROVER}, have investigated optimal tokenizer settings for masked language modeling, concluding that Byte Pair Encoding (BPE) is better suited for masked DNA LLMs. The majority of these models face the limitation of insufficient context length, primarily due to the high computational cost associated with extending the context length in the transformer architecture. To address this limitation, GENA-LM~\cite{gena-lm} employs sparse attention, and Caduceus~\cite{Caduceus} uses the more lightweight BiMamba architecture~\cite{Mamba}, both trained on the human genome.

Although these masked DNA LLMs effectively understand and predict DNA sequences, they lack generative capabilities, and generative DNA LLMs remain in the early stages of development. An early preprint~\cite{dnagpt} introduced DNAGPT, which learns mammalian genomic structures through three pre-training tasks, including next token prediction. Recent works, such as HyenaDNA~\cite{HyenaDNA} and megaDNA~\cite{megaDNA}, achieve longer context lengths by employing the Hyena~\cite{Hyena} and multiscale transformer architectures respectively, though they are significantly limited by their data and model scales. A more recent influential study, Evo~\cite{Evo}, trained on an extensive dataset of prokaryotic and viral genomes, has garnered widespread attention for its success in designing CRISPR-Cas molecular complexes, thus demonstrating the practical utility of generative DNA LLMs in the genomic field.
