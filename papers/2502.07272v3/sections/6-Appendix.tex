\section{Supplementary Experiment Results}

\subsection{Gener Tasks}
The confusion matrices for the gene classification task are shown in \textit{Fig.} \ref{fig:gene_classification}, whereas \textit{Fig.} \ref{fig:species_classification} illustrates those for the taxonomic classification task. For both tasks, the \textbf{Gener}\textit{ator} not only excels in average performance but also maintains top performance across different categories of genes and taxonomic groups.

\subsection{Next K-mer Prediction}
The evaluations of next K-mer prediction are provided in \textit{Fig.} \ref{fig:kmer_tokenizer} and \textit{Fig.} \ref{fig:kmer_model}. Specifically, \textit{Fig.} \ref{fig:kmer_tokenizer} presents the evaluations for tokenizer comparison, where the 6-mer tokenizer outperforms others across different taxonomic groups. \textit{Fig.} \ref{fig:kmer_model} presents the evaluations for model comparison, where the \textbf{Gener}\textit{ator} consistently outperforms others across various taxonomic groups.

\section{Details of Pre-training}
In the pre-training phase, we employed the AdamW~\cite{adamw} optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.95$, and $\text{weight decay} = 0.1$. The learning rate schedule incorporated a linear warm-up followed by cosine decay: the learning rate increased linearly from 0 to its peak value over the first 2,000 steps, then followed a cosine decay, decreasing until it reached 10\% of the peak value by the end of the training process. The peak learning rate was established at $4e^{-4}$, and gradient clipping was applied with a norm threshold of $1.0$. We adhered to standard practices in pre-training LLMs, using a batch size that encompasses 2 million tokens. Given the maximum sequence length of 16,384 tokens, this configuration resulted in each batch comprising 128 samples, where each sample was initialized with a random starting point between 0 and 5 at each epoch. The complete pre-training of the \textbf{Gener}\textit{ator} spanned 6 epochs, amounting to approximately 185,000 steps. To enhance computational efficiency, we employed optimization techniques like Flash Attention~\cite{flashattention,flashattention2} and the Zero Redundancy Optimizer~\cite{deepspeed,fsdp}. The pre-training process, utilizing 32 NVIDIA A100 GPUs, was completed in 368 hours. This configuration achieved a model FLOPs utilization (MFU) of approximately 46\%.

The summary statistics of the pre-training data in terms of the number of nucleotides and number of genes are provided in Table \ref{tab:pretrain_data_statistics_species} and Table \ref{tab:pretrain_data_statistics_gene}. The pre-training losses for the \textbf{Gener}\textit{ator} and \textbf{Gener}\textit{ator}-All are presented in \textit{Fig.} \ref{fig:pretrain_loss}. Although the \textbf{Gener}\textit{ator}-All exhibits a lower pre-training loss compared to the \textbf{Gener}\textit{ator}, it consistently underperforms in almost all benchmark tests, as discussed in previous sections. This discrepancy is likely due to the inclusion of non-gene regions, which often contain highly repetitive and simple segments. Additionally, the pre-training losses for different tokenizers and the Mamba model are shown in \textit{Fig.} \ref{fig:pretrain_loss_tokenizer}. The significant variation in vocabulary sizes among the tokenizers results in considerable differences in pre-training losses, making direct comparisons challenging. This issue inspires the proposition of next K-mer prediction for a more effective model comparison.

\section{Details of Benchmark Experiments}

\subsection{Benchmarks}
In this paper, we conducted benchmark evaluations on two widely used benchmarks, Nucleotide Transformer tasks~\cite{nucleotide-transformer} and Genomic Benchmarks~\cite{genomic-benchmarks}, as well as on the Gener tasks we propose. For Nucleotide Transformer tasks, there are two different versions available: the original version can be accessed \href{https://huggingface.co/datasets/InstaDeepAI/nucleotide_transformer_downstream_tasks}{\textcolor{blue}{here}}, and the revised version is available \href{https://huggingface.co/datasets/InstaDeepAI/nucleotide_transformer_downstream_tasks_revised}{\textcolor{blue}{here}}. For Genomic Benchmarks, the dataset can be downloaded from \href{https://huggingface.co/katarinagresova}{\textcolor{blue}{here}}.

\subsection{Metrics}
Regarding evaluation metrics, we follow the original settings of the benchmarks. For Nucleotide Transformer tasks, we employ the Matthews correlation coefficient (MCC):
\begin{equation}
\text{MCC} = \frac{\text{TP} \times \text{TN} - \text{FP} \times \text{FN}}{\sqrt{(\text{TP} + \text{FP})(\text{TP} + \text{FN})(\text{TN} + \text{FP})(\text{TN} + \text{FN})}}, \nonumber
\end{equation} 
where TP, TN, FP, FN represent true positives, true negatives, false positives, and false negatives, respectively. For Genomic Benchmarks, accuracy is used as the evaluation metric. For Gener tasks, we utilize the weighted F1 score for multi-classification, calculated as follows:
\begin{equation}
\text{Precision}_i = \frac{\text{TP}_i}{\text{TP}_i + \text{FP}_i},
~~~
\text{Recall}_i = \frac{\text{TP}_i}{\text{TP}_i + \text{FN}_i}, \nonumber
\end{equation}

\begin{equation}
\text{F1}_i = 2 \times \frac{\text{Precision}_i \times \text{Recall}_i}{\text{Precision}_i + \text{Recall}_i},
~~~
\text{F1}_{\text{weighted}} = \sum_{i=1}^{n} w_i \times \text{F1}_i, \nonumber
\end{equation}
where $w_i = {n_i}/{N}$ denotes the weight for class $i$, $n_i$ is the number of samples in class $i$, and $N$ is the total number of samples.

\subsection{Baselines}
To comprehensively assess the capability of the \textbf{Gener}\textit{ator} in sequence comprehension, we include several state-of-the-art models as baselines. These models differ in terms of training data, model size, model architecture, and other aspects. Table~\ref{tab:baseline_introductuon} provides a summary of these baselines. Where applicable, evaluation results for the original and revised NT tasks are sourced directly from the NT paper~\cite{nucleotide-transformer}. For the remaining baseline models and datasets, we conduct consistent and fair evaluation tasks ourselves to obtain the results. These self-evaluated baseline models can be accessed through the following Huggingface\footnote{\url{https://huggingface.co}} repositories:
\begin{itemize}
    \item DNABERT-2: \texttt{zhihan1996/DNABERT-2-117M}
    \item HyenaDNA: \texttt{LongSafari/hyenadna-large-1m-seqlen}
    % \item NT-multi: \texttt{InstaDeepAI/nucleotide-transformer-2.5b-multi-species}
    \item NT-v2: \texttt{InstaDeepAI/nucleotide-transformer-v2-500m-multi-species}
    \item Caduceus-Ph: \texttt{kuleshov-group/caduceus-ph\_seqlen-131k\_d\_model-256\_n\_layer-16}
    \item Caduceus-PS: \texttt{kuleshov-group/caduceus-ps\_seqlen-131k\_d\_model-256\_n\_layer-16}
    \item GROVER: \texttt{PoetschLab/GROVER}
\end{itemize}

\subsection{Experimental Setups}
In our benchmark experiments, we retained the optimizer configuration from the pre-training phase, with $\beta_1=0.9$, $\beta_2=0.95$ and $\text{weight decay} = 0.1$. We adopted the `reduce on plateau' strategy for the learning rate scheduler and implemented early stopping based on validation metrics, with a patience of 5. Optimal learning rates and batch sizes for various models and datasets were determined through hyperparameter search, as detailed in Tables~\ref{tab:benchmark_hyperparam1} to \ref{tab:benchmark_hyperparam4}. For Nucleotide Transformer tasks and Genomic Benchmarks, all models had sufficient context length, allowing them to utilize the entire input sequences. In contrast, for Gener tasks, input sequences exceeding the available context length were truncated to fit within the model constraints. For all causal language models, we performed prediction through an additional linear layer using the embedding of the \texttt{<EOS>} token, while for masked language models, we used the \texttt{<BOS>} token or \texttt{<CLS>} token instead. Notably, for Caduceus, we followed its original configuration using mean pooling and applied the recommended reverse complement data augmentation for Caduceus-Ph. All models obtained embeddings from their final layer and underwent full fine-tuning. All evaluation metrics were obtained through 10-fold cross-validation.

\section{Details of Central Dogma}
\label{sec:central_dogma_supp}
We sourced the protein sequences and the corresponding protein-coding DNA fragments for the Histone and Cytochrome P450 families from the UniProt~\cite{UniProt} database. Each family had approximately 400,000 entries. The \textbf{Gener}\textit{ator} model was then fine-tuned on these datasets using the protein-coding sequences, with a learning rate of $5e^{-5}$ and a batch size of 1,024. For the fine-tuned models, we generated 100,000 sequences for each combination of temperature $T \in \{0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$ and nucleus sampling $P \in \{0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$. Following translation into protein sequences, duplicates were removed by comparing the generated sequences with the training set, resulting in a unique selection of 10,000 samples per family. The hyperparameters were set to $T=0.6$, $P=0.6$ for the Cytochrome P450 family (\textit{Fig.}~\ref{fig:cytochrome_generation}), and $T=1.0$, $P=0.8$ for the Histone family (\textit{Fig.}~\ref{fig:histone_generation}). 

\section{Details of Enhancer Design}
For the enhancer activity prediction, we adhered to the data partitioning of DeepSTARR~\cite{DeepSTARR}. We conducted the same hyperparameter search strategy as in our benchmark experiments and selected a learning rate of $2e^{-5}$ and a batch size of 64 for training the activity predictor. For enhancer generation, we selected enhancer sequences from the top and bottom quartiles of activity values within the DeepSTARR training set. These sequences were used to perform supervised fine-tuning (SFT) on the \textbf{Gener}\textit{ator}, labeled with prompts \texttt{<high>} and \texttt{<low>} for high and low activity sequences, respectively. The hyperparameters for SFT were set to a learning rate of $1e^{-5}$ and a batch size of 2,048. During the generation process, we applied the same hyperparameter search strategy detailed in \textit{Sec.}~\ref{sec:central_dogma_supp}. The hyperparameters used for visualization in \textit{Fig.}~\ref{fig:enhancer_design} were set to $T=0.5$, $P=0.7$ for the developmental activity, and $T=0.8$, $P=0.6$ for the housekeeping activity.

\section{Computational Resources}
In this study, the primary computational resources utilized for model training and inference included NVIDIA V100 and A100 GPUs. Prior to pre-training, we trained 14 models for one epoch each to evaluate the DNA sequence modeling capabilities of various tokenizers. Our pre-training phase involved two models that varied based on the training data used. In the downstream tasks, we executed over 10,000 runs, which entailed an extensive hyperparameter search across 36 combinations and utilized 10-fold cross-validation for each benchmark task per model. Detailed statistics on the usage of main computational resources are presented in Table~\ref{tab:computational_resources}.

\section{Supplementary Figures \& Tables}

\input{figures/tex/gene_classification}
\input{figures/tex/species_classification}
\input{figures/tex/kmer_tokenizer_full}
\input{figures/tex/kmer_model_full}
% \input{figures/tex/histone_generation}
\input{figures/tex/pretrain_loss}
\input{figures/tex/pretrain_loss_tokenizer}

% \input{tables/nucleotide_transformer_tasks}
% \input{tables/genomic_benchmarks}
\input{tables/pretrain_data_statistics}
\input{tables/baseline_introductuon}
\input{tables/benchmark_hyperparam}
\input{tables/computational_resources}
