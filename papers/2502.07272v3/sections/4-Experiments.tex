\section{Experiments}
\label{sec:experiments}

\subsection{Next K-mer Prediction}
\label{sec:kmer_predition}
\input{figures/tex/kmer_prediction_main}

As mentioned in \textit{Sec.} \ref{sec:tokenization}, we conducted extensive experiments to explore the most suitable tokenizer for training causal DNA language models. This was achieved by training multiple models on identical datasets, each employing a different tokenizer. All models share the same architecture as the \textbf{Gener}\textit{ator} and are uniformly compared at 32,000 training steps. We employed the accuracy of the next K-mer prediction task as our evaluation metric. This zero-shot task facilitates a direct assessment of the pre-trained model quality, ensuring equitable comparisons across various tokenizers. As depicted in \textit{Fig.} \ref{fig:kmer_main}A, the tested tokenizers include BPE tokenizers with vocabulary sizes ranging from 512 to 8192, and K-mer tokenizers with K values from 1 to 8 (noting that the single nucleotide tokenizer corresponds to a K-mer tokenizer with K=1). Overall, K-mer tokenizers demonstrate superior performance compared to BPE tokenizers. Among the K-mer tokenizers, the 6-mer tokenizer is selected for its robust performance with limited input tokens and its ability to maintain top-tier performance as the number of input tokens increases.

Moreover, we evaluated the performance of Mamba \cite{Mamba,Mamba-2}, recognized for its capacity in handling long-context pre-training. To adequately assess its capabilities, we configured a Mamba model utilizing the single nucleotide tokenizer with 1.2B parameters and a context length of 98k bp. The Mamba model is compared to the 1-mer and 6-mer models under varied configurations. The comparison with the 1-mer model is straightforward; the Mamba model (denoted as Mamba\texttimes1 in \textit{Fig.} \ref{fig:kmer_main}A) exhibits slightly better performance with fewer input tokens but underperforms as the token count increases. Despite Mamba's context length being six times that of the 1-mer model, this feature does not translate into improved performance. This might suggest that Mamba's renowned ability to handle long-context pre-training primarily refers to cost-effective training rather than enhanced model performance \cite{Empirical, DeciMamba}. To compare against the 6-mer model, we adjust the input token count for the Mamba model by a factor of six (denoted as Mamba\texttimes6) to compare the models on the same base-pair basis. In this context, Mamba\texttimes6 shows slightly better performance with fewer input tokens; however, it rapidly lags as the token count increases. These findings collectively indicate that a transformer decoder architecture paired with a 6-mer tokenizer provides the most effective approach for training causal DNA language models, aligning with the configuration of the \textbf{Gener}\textit{ator}.

We further compared the \textbf{Gener}\textit{ator} model with other baseline models to evaluate their generative capabilities. As illustrated in \textit{Fig.} \ref{fig:kmer_main}B, we assess model performance using a dataset composed exclusively of mammalian DNA, given that HyenaDNA and GROVER are trained solely on human genomes. The \textbf{Gener}\textit{ator} significantly outperforms other baseline models, including its variant, \textbf{Gener}\textit{ator}-All, which incorporates pre-training on non-gene regions. This suggests that the gene sequence training strategy, which emphasizes semantically rich regions, provides a more effective training scheme compared to the conventional whole sequence training. This effectiveness is likely due to the sparsity of gene segments in the whole genome (less than 10\%) and the disproportionate importance of these segments. Among the other baseline models, NT-multi demonstrates the best performance, likely attributable to its extensive model scale (2.5B parameters), yet it still lags significantly behind the \textbf{Gener}\textit{ator}. This result aligns with expectations, as the MLM training paradigm is recognized for its limitations in generative capabilities. Meanwhile, HyenaDNA, despite utilizing the NTP training paradigm, does not show improved performance compared to other masked language models, likely due to its overly small model size (55M parameters), insufficient for exhibiting robust generative abilities. This comparison underscores the critical role of the \textbf{Gener}\textit{ator} in bridging the gap for large-scale generative DNA language models within the eukaryotic domain.

Due to space constraints, we have chosen only to demonstrate specific examples with mammalian DNA data and a fixed K-mer prediction length of 16 bp in \textit{Fig.} \ref{fig:kmer_main}. A more comprehensive analysis across various taxonomic groups and K-mer lengths is provided in the appendix.

\subsection{Benchmark Evaluations}
In this section, we compare the \textbf{Gener}\textit{ator} with state-of-the-art genomic foundation models: Enformer~\cite{enformer}, DNABERT-2, HyenaDNA, Nucleotide Transformer, Caduceus, and GROVER, across various benchmark tasks. To ensure a fair comparison, we uniformly fine-tune each model and perform a 10-fold cross-validation on all datasets. For each model on each dataset, we conduct a hyperparameter search, exhaustively tuning learning rates in $\{1e^{-5}, 2e^{-5}, 5e^{-5}, \ldots, 1e^{-3}, 2e^{-3}, 5e^{-3}\}$ and batch sizes in $\{64, 128, 256, 512\}$. Detailed hyperparameter settings and implementation specifics are provided in the appendix.

\paragraph{Nucleotide Transformer Tasks}
Since the NT task dataset was revised recently~\cite{nucleotide-transformer}, we conducted experiments on both the original and revised datasets. The results for the revised NT tasks are provided in Table~\ref{tab:nucleotide_transformer_tasks_revised}, and the results for the original NT tasks are provided in Table~\ref{tab:nucleotide_transformer_tasks}. Overall, the \textbf{Gener}\textit{ator} outperforms other baseline models. However, the \textbf{Gener}\textit{ator}-All variant shows some performance decline. Notably, despite its earlier release, Enformer continues to deliver competitive results in chromatin profile and regulatory element tasks. This performance could be attributed to its original training in a supervised manner specifically for chromatin and gene expression tasks. The latest release of Nucleotide Transformer, NT-v2, although smaller in size (500M), demonstrates enhanced performance compared to NT-multi (2.5B). In contrast, DNABERT-2 and GROVER, which utilize BPE tokenizers, along with HyenaDNA and Caduceus, which employ the finer-grained single nucleotide tokenizer, do not show distinct performance advantages, likely due to the limited model scope and data scale.

\paragraph{Genomic Benchmarks}
We also conducted a comparative analysis on the Genomic Benchmarks~\cite{genomic-benchmarks}, which primarily focus on the human genome. The evaluation results are provided in Table~\ref{tab:genomic_benchmarks}. Overall, the \textbf{Gener}\textit{ator} still outperforms other models. However, it is worth noting that the Caduceus models also exhibit comparable performance while being significantly smaller (8M). This is likely due to the fact that Caduceus models are trained exclusively on the human genome, making them efficient and compact. Nevertheless, this exclusivity may limit their generalizability to other genomic contexts.

\paragraph{Gener Tasks} 
Lastly, we evaluated the newly proposed Gener tasks, which focus on assessing genomic context comprehension across various sequence lengths and organisms. As shown in Table~\ref{tab:gener_tasks}, the \textbf{Gener}\textit{ator} achieves the best performance on both gene and taxonomic classification tasks, with NT-v2 also demonstrating similar results. Further details on the evaluation of Gener tasks, including visualizations of confusion matrices, are provided in the appendix. The superior performance of the \textbf{Gener}\textit{ator} and NT-v2 is likely due to their pre-training on multispecies datasets. In contrast, despite also being trained on multispecies data, DNABERT-2 exhibits noticeable performance degradation. This may be attributed to its limited model size (117M for DNABERT-2, 500M for NT-v2, and 1.2B for \textbf{Gener}\textit{ator}) and shorter context length (3k for DNABERT-2, 12k for NT-v2, and 98k for \textbf{Gener}\textit{ator}). Other models, such as HyenaDNA and Caduceus, although trained exclusively on the human genome, still exhibit relevant generalizability on both tasks after fine-tuning, attributable to their long-context capacity (\textgreater 100k). GROVER, on the other hand, significantly lags behind in taxonomic classification due to its limited context length (3k).

\input{tables/nucleotide_transformer_tasks_revised}
\input{tables/nucleotide_transformer_tasks}
\input{tables/genomic_benchmarks}
\input{tables/gener_tasks}

\subsection{Central Dogma}

In our experimental setup, we selected two target protein families from the UniProt~\cite{UniProt} database: the Histone and Cytochrome P450 families. By cross-referencing gene IDs and protein IDs, we extracted the corresponding protein-coding DNA sequences from RefSeq~\cite{RefSeq}. These sequences served as training data for fine-tuning the \textbf{Gener}\textit{ator} model, directing it to generate analogous protein-coding DNA sequences.

To assess the quality of generation, we compared several summary statistics. The results for the Histone family are provided in \textit{Fig.} \ref{fig:histone_generation}, while the evaluation results for the Cytochrome P450 family are provided in \textit{Fig.} \ref{fig:cytochrome_generation}.  After deduplication, the lengths of the generated DNA sequences and their translated protein sequences, using a codon table, closely resemble the distributions observed in the target families. This preliminary validation suggests that our generated DNA sequences maintain stable codon structures that are translatable into proteins. We conducted a more in-depth structural and functional analysis of these translated protein sequences. First, we assessed whether protein language models `recognize' these generated protein sequences by calculating their perplexity (PPL) using Progen2~\cite{progen2}. The results show that the PPL distribution of generated sequences closely matches that of the natural families and significantly differs from the shuffled sequences.

Furthermore, we used AlphaFold3 to predict the folded structures of the generated protein sequences and employed Foldseek~\cite{Foldseek} to find analogous proteins in the Protein Data Bank (PDB)~\cite{RCSBPDB}. Remarkably, we identified numerous instances where the conformations of the generated sequences exhibited high similarity to established structures in the PDB ($\text{TM-score}>0.8$). This structural congruence is observed despite substantial divergence in sequence composition, as indicated by sequence identities less than $0.3$. This low sequence identity positively suggests that the model is not merely replicating existing protein sequences but has learned the underlying principles to design new molecules with similar structures. This highlights the capability of the \textbf{Gener}\textit{ator} in generating biologically relevant sequences. 

\subsection{Enhancer Design}
We employed the enhancer activity data from DeepSTARR~\cite{DeepSTARR}, following the dataset split initially proposed by DeepSTARR and later adopted by NT. Using this data, we developed an enhancer activity predictor by fine-tuning the \textbf{Gener}\textit{ator}. This predictor surpasses the accuracy of DeepSTARR and NT-multi (Table \ref{tab:enhancer_benchmark}), establishing itself as the current state-of-the-art predictor. By applying our refined SFT approach as outlined in \textit{Sec.} \ref{sec:sequence_design}, we generated a collection of candidate enhancer sequences with specific activity profiles. As illustrated in \textit{Fig.} \ref{fig:enhancer_design}, the predicted activities of these candidates exhibit significant differentiation between the generated high/low-activity enhancers and natural samples.

To our knowledge, this represents one of the first attempts to use LLMs for prompt-guided design of DNA sequences, highlighting the capability of the \textbf{Gener}\textit{ator} in this domain. These generated sequences, and more broadly, this sequence design paradigm using the \textbf{Gener}\textit{ator}, merit further exploration. Our approach underscores the potential of the \textbf{Gener}\textit{ator} model to transform DNA sequence design methodologies, providing a novel pathway for the conditional design of DNA sequences using LLMs. In our subsequent research, we plan to extend our evaluations through further validations in wet lab conditions to explore the real-world applicability of these designed sequences.

\input{figures/tex/histone_generation}
\input{figures/tex/cytochrome_generation}

\input{tables/enhancer_benchmark}
\input{figures/tex/enhancer_design}
