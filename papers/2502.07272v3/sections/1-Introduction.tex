\section{Introduction}
Genomic sequences encapsulate vast biological information that drives complex processes, from gene regulation to protein synthesis, influencing phenotypic traits and disease states~\cite{bucleotide-sequence,bioinformatics-and-functional-genomics}. Rapid advancements in DNA sequencing technologies~\cite{next-generation-dna-sequencing} have significantly enhanced our ability to decode genomic sequences of various organisms on an unprecedented scale. However, the prediction and interpretation of genomic sequences remain a formidable challenge due to the intricate nature of genetic material and the scarcity of high-quality, task-specific datasets.

Recent progress in machine learning, particularly with large language models (LLMs)~\cite{llms-survey}, has opened new avenues for understanding biological sequences. In natural language processing (NLP), LLMs such as BERT~\cite{BERT} and GPT~\cite{GPT4} have shown remarkable success by leveraging large-scale pre-training data to generalize across diverse downstream tasks. This success extends to the realm of biological sequences, as demonstrated by models like AlphaFold~\cite{AlphaFold2,AlphaFold3} and ESM~\cite{esm2,esm3}, which adeptly unravel the complexities of protein structures and functions. Within genomics, early efforts have primarily focused on masked language models such as DNABERT~\cite{DNABERT,DNABERT-2}, GROVER~\cite{GROVER}, Caduceus~\cite{Caduceus}, and Nucleotide Transformer (NT)~\cite{nucleotide-transformer}, which excel at understanding DNA semantics but lack generative capabilities and are generally restricted to relatively short sequence lengths.

Generative models offer the potential to overcome these limitations by combining semantic understanding with the capability to perform sequence design tasks. However, attempts to train large-scale generative DNA language models have been relatively underexplored. Models like megaDNA~\cite{megaDNA} focus solely on bacteriophage genomes, and HyenaDNA~\cite{HyenaDNA} is confined to human genomesâ€”both are limited in scope regarding model parameter size and dataset scale. A notable exception is Evo~\cite{Evo}, trained on a comprehensive dataset of bacterial and viral genomes, which demonstrates proficiency in assembling functional CRISPR-Cas molecular complexes. However, there remains a gap for a generative model in the broader eukaryotic domain, known for its biodiversity and complex gene structures.

\input{figures/tex/model_overview}

In this study, we introduce \textbf{Gener}\textit{ator}, a generative genomic foundation model utilizing the transformer decoder architecture, trained on an expansive dataset comprising 386 billion base pairs (bp) of eukaryotic DNA derived from the RefSeq database~\cite{RefSeq}. The extensive and diverse pre-training data endow the \textbf{Gener}\textit{ator} with enhanced understanding and generation capabilities across various organisms. Our evaluations demonstrate that the \textbf{Gener}\textit{ator} consistently achieves state-of-the-art performance across a wide spectrum of benchmarks, including Genomic Benchmarks~\cite{genomic-benchmarks}, NT tasks~\cite{nucleotide-transformer}, and our newly proposed Gener tasks. An overview of the \textbf{Gener}\textit{ator} is provided in \textit{Fig.} \ref{fig:model_overview}.

Beyond benchmark performance, the \textbf{Gener}\textit{ator} adheres to the central dogma of molecular biology~\cite{central-dogma}, accurately generating protein-coding DNA sequences that produce proteins structurally analogous to known families. Moreover, the \textbf{Gener}\textit{ator} showcases significant promise in sequence optimization, particularly in the design of enhancer sequences that regulate gene expression during various biological stages, highlighting its potential for a series of biologically significant tasks. Our findings position the \textbf{Gener}\textit{ator} as a vital resource for genomic research and biotechnological advancement. By enhancing our capability to interpret and predict genomic sequences, the \textbf{Gener}\textit{ator} paves the way for profound improvements in our understanding of complex biological systems and the development of precise genomic interventions.

The primary contributions of this paper are as follows:
\begin{enumerate}
\item Introduction of \textbf{Gener}\textit{ator}, a generative genomic foundation model trained on 386B bp of eukaryotic DNA, featuring a context length of 98k bp and 1.2B parameters, consistently achieving state-of-the-art performance across various genomic benchmarks.

\item Validation of the model's alignment with the central dogma of molecular biology through its ability to generate proteins structurally analogous to known families.

\item Demonstration of the model's capability in enhancer design, highlighting its potential to advance biotechnological applications and genomic research.

\end{enumerate}
