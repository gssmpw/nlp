\section{Implementation details}
\label{sec:opti}
In this section, we describe the key aspects of the implementation that impact the algorithm's running time and its motif discovery capabilities. Namely, the number of hash evaluations and comparisons and the challenge of not knowing the dimensionality of the motifs to discover.
\subsection{Index building}
\subsubsection*{Setting the quantization width r}
\label{sec:r_auto}
Recall from Equation~\ref{eq:drp} the formulation of the hash function we use.
Even though the choice of the parameter $r$ does not compromise the correctness of the algorithm, it is fundamental since it influences the performance.
A value that is too high will group many subsequences together at long indices, mitigating the filtering effect of LSH, a value that is too low will separate even the smallest perturbation between subsequences, forcing the algorithm to visit shorter prefixes.
To deal with this parameter automatically, the algorithm adopts an estimation-based heuristic.

First we sample a number of random vectors from $\sim \mathcal{N}(0,1)$, computing their dot product with a random sample of subsequences from the time series.
This produces an estimate of the distribution of the values that are discretized by Equation~\ref{eq:drp}.

Then, we discretize this empirical distribution into $256$ equal-width buckets.
The width of these buckets will be used as the parameter $r$ in the hash function.
The rationale is that by doing so we will be able to represent each hash value with a single byte.


Note that this heuristic can be implemented efficiently by leveraging on the \emph{cyclical convolution theorem}, the same method used by MASS for the \textit{Distance Profile} \cite{zhong2024mass}.
We can obtain, with one convolution between one of the random vectors and the sample of the time series, all the dot products for that vector.


\subsubsection*{Tensoring}
Tensoring is a technique to reduce the number of evaluations for the hash functions \cite{christianitensoring}.
Let $\mathcal{H}$ be a LSH family and $K,L\geq 1$ integers. 
For $m=\sqrt{L}$,
we define $\mathcal{H}_l$ as a set of $m$ hash functions sampled from $\mathcal{H}^{\frac{K}{2}}$ and similarly for $\mathcal{H}_r$. Then $\left(h_a,h_b\right) \in \mathcal{H}_l\times \mathcal{H}_r, 1\leq a,b\leq m$ provides $m^2$ repetitions with $Km$ evaluations.
Furthermore, let us define:
\begin{equation}
h_{K,j}=\left(h_{\frac{K}{2},l},h_{\frac{K}{2},r}\right) \in \mathcal{H}_{l}\times \mathcal{H}_{r} \text{ where } 
\begin{cases}
    l = j  \div \sqrt{L}\\
    r = j \text{ }\% \text{ } \sqrt{L}
\end{cases}
\end{equation}
for $1 \leq j\leq L$.
\newline
The resulting hash is obtained from interleaving values from the selected left and right hash.
Reducing the total evaluations from $K\cdot L$ to $K\cdot \sqrt{L}$ at the cost of interdependent repetitions.
\begin{corollary}[lemma:line3]
    With the tensoring approach the index construction takes time proportional to $O(D\cdot K\cdot \sqrt{L} \cdot n\text{ log }n)$.
\end{corollary}

\subsubsection*{Trie implementation.}
A key operation the index needs to provide is the retrieval of subsequences with hash values sharing the same prefix, which in the previous section we realized as a trie.
Given that we need to iterate over all the subsequence with the same prefix, in practice
we store hash values for each repetition in an array, sorted in lexicographic order.
By doing so, hash values with the same prefix appear in contiguous ranges of the array,
providing higher locality of reference compared to a pointer-based data structure such as a trie.

\subsection{Index traversal}
\subsubsection*{Comparisons on the fly}
In \Cref{alg:emitaggr}, maintaining the structure of line~\ref{ln:prefix-cycle} at runtime is very expensive, 
since it would require quadratic space to store the weights for, potentially, each pair of subsequences.
To deal with this problem, while maintaining the same theoretical approach, we just scan the index over each dimension and every time a unidimensional collision is seen we immediately compute $W(a,b)$, if greater or equal than the searched motif dimensionality $d$, we perform the insertion of the pair in the priority queue. This approach does not require any additional space at the cost of a slightly higher number of hash comparisons, since we may evaluate a colliding pair up to $D$ times.

\subsubsection*{Duplicate collisions within the same repetition}
Our approach dynamically chooses the length of the composite hashes by progressively iterating on the prefixes of the full ones, this results in looking into prefixes of decreasing size. Consequently, collisions at level $i$ are a \textit{superset} of the collisions at level $i+1$, in order to avoid unnecessary distance computations, the algorithm will check if the colliding hashes appear at the level $i+1$, skipping the pair in the positive case.
\subsection{Finding motifs of multiple dimensionalities}
\label{sec:multisub}
One of the critical points up to now is the fact that at input we require the number of dimensions $d$ that span the motif. In many real scenarios, it is possible to know only approximately the expected dimensionality of the pattern in a certain domain.

Our method can be easily extended to discover motifs spanning different dimensions,
allowing the user to specify a range of dimensionalities $d_{low}, d_{high}$ of the motifs they want to discover.
The data structure at line~\ref{ln:priority-queue-init} of \Cref{alg:emitaggr} becomes a set of $d_{high}-d_{low}+1$ priority queues, which are independently updated during the discovery process.
This allows the algorithm to reuse information from a single distance computation across the different requested dimensionalities. Since evaluating $\distd{\subTa,\subTb}$ for a pair of subsequences requires computing $\dist{\subTaf,\subTbf} \quad\forall f\in[D]$, we can efficiently derive $\distd{\subTa,\subTb} \quad \forall d\in[1,D]$.
The condition of line~\ref{ln:weigth-constr} is initially based on $d_{low}$ and gradually increases every time the lowest dimensionality has its motif confirmed.
This approach allows to return solutions for each motif dimensionality as soon as their error probabilities satisfy the required quality, inheriting all the properties that are derived in \Cref{sec:complexity}.
\subsection{Constrained Search}
When discovering multidimensional motifs there exist some constraints used for domain specific tasks: 
dimensional \textit{exclusion}, were some dimensions are not to be considered,
and \textit{inclusion}, were some dimensions \emph{must} be part of the motifs~\cite{keoghMP}.
The former can be achieve by removing the dimensions before running the algorithm.
To implement the inclusion constraint we change the weight constraint at line~\ref{ln:weigth-constr} of \Cref{alg:emitaggr}:
let $(a,b)$ be a pair at iteration $i,j$ and $F$ be the set of required dimensions, then $(a,b)$ is a candidate pair if we have that 
\begin{equation*}  
W(a,b) \ge d  \quad\wedge\quad h_{i,j}^f(a)=h_{i,j}^f(b)\quad\forall f \in F.
\end{equation*}

\subsection{Anytime properties}
The iterative process of line~\ref{ln:prefix-cycle} to \ref{ln:second-end}
of Algorithm~\ref{alg:emitaggr}
inherently exhibits the characteristics of an anytime algorithm,
as the priority queues only allow for insertion of better solutions. As we well see later, meaningful motifs of different dimensionalities are found with short time intervals between one another. Early stopping can be employed by the user, to retrieve at any time the confirmed motifs and the candidate motifs with their respective error probabilities.

\begin{table}[t]
\caption{\textbf{Information about the evaluation datasets.}}
\centering
\begin{tabular}{@{}lrrrrr@{}}
\toprule
dataset          & n      & D & window & $d$ &$c_{d,1|n}$ \\ \midrule
potentials       & 2 500   & 8 &   50     &  8 & 6.11\\
evaporator       & 7 000   & 5 &   75    &   2 & 2.60\\
RUTH             & 14 859   & 32 &   500    &   4 & 3.24  \\
weather          & 100 057 & 8 &   5000     &  2 &   2.43  \\
whales           & 450 001 & 32 & 300 & 6 & 1.22*\\
quake            & 6 440 998 & 32 & 100 & 4 & 1.65* \\
electrical\_load & 6 960 008 & 10  &  1000 & 5 & 1.91*   \\ 
ltmm             & 25 132 289 & 6 & 200 & 3 & 5.31* \\
\bottomrule
\multicolumn{6}{r}{{\small * obtained through random sampling.}}
\end{tabular}
\label{tab:dataset}
\end{table}





