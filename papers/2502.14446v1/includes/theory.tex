\begin{figure*}[t]
    \centering
    \includeinkscape[width=\linewidth]{svg-inkscape/mts_svg-tex.pdf_tex}
    \caption{
        Multidimensional time series from an industrial evaporator \cite{DaISyEVAP}.
        The top-3 two dimensional motifs are highlighted.}
    \label{fig:multits}
    \Description[A time series with 3 dimensions, 3 motifs are highlighted in the first and third dimensions.]{The first and third dimension represent steam pressure and temperature, the second is an indicator that acts like noise in this context. The motifs represent some normal behaviors where one dimension increases while the other decreases.}
\end{figure*}
\section{Preliminaries}
\label{sec:ts}
\begin{table}[t]
    \centering
    \caption{Table of symbols.}
    \begin{tabular}{p{.2\columnwidth}p{.7\columnwidth}}
        \specialrule{1pt}{0pt}{0pt}
        $\Tmulti$ & Multidimensional time series \\
        $\mathbf{T}^w$ & Set of subsequences from $\mathbf{T}$ of length $w$\\
        $\subTa$ & Subsequence at index $a$\\
        $\subTaf$ & Dimension $f$ of subsequence $\subTa$\\
        $D$ & Dimensionality of the time series \\
        $d$ & Dimensionality of the motifs to discover \\
        $\dist{\cdot, \cdot}$ & Distance function \\
        $\distd{\cdot, \cdot}$ & $d$-dimensional distance function \\
        $\dims{\cdot, \cdot}$ & dimensions involved in the computation of the $d$-dimensional distance function \\
        $k$ & Number of motifs to find \\
        \hline
        $h_{i,j}\left(\subTaf \right)$ & hash value of length $i$ at repetition $j$
                                         for dimension $f$ of subsequence $\subTa$ \\
        $L$ & Number of repetitions for LSH \\
        $K$ & Number of concatenations for LSH \\
        \specialrule{1pt}{0pt}{0pt}
    \end{tabular}
    \label{tab:notation}
\end{table}
\subsection{Time Series and Motifs}
We will start our introduction from the special case of a single dimension time series.
\begin{definition}
        \textup{A} \textit{time series} $T\in \mathbb{R} ^n$ \textup{is an ordered sequence of real valued numbers} $T = [t_1,...\text{ }t_n]: t_u\in \mathbb{R} \text{, }\forall i\in[1,n]$ \textup{where $n$ is the length of the time series}.
\end{definition}
\begin{definition}
    \textup{A \textit{time series subsequence} $T_{u,w}\in \mathbb{R}^w$ is a subset of adjacent elements from T starting at position $u$ and length $w$, $T_{u,w}=[t_u,...\text{ }t_{u+w-1}]$.
    }
\end{definition}
\begin{definition}
    The \textit{z-normalized Euclidean Distance} between two time series subsequences $T_{u,w}$ and $T_{v,w}$ is defined as:
    \begin{equation}
    \label{eq:zned}
    \dist{T_{u,w},T_{v,w}} = 
    \sqrt{\sum_{i\in[w]}
        \left( \frac{T_{u}(i)-\mu(T_{u,w})}{\sigma(T_{u,w})}
        -
        \frac{T_{v}(i)-\mu(T_{v,w})}{\sigma(T_{v,w})} \right)^2
    }
    \end{equation}
    where $\mu$ and $\sigma$ are the means and the standard deviations of the subsequences, respectively, and $T_{u}(i)$ is the $i$-th value of subsequence $T_{u,w}$.
\end{definition}
\begin{definition}
    \textup{A \textit{time series motif} is the pair of subsequences $T_{u,w}$ and $T_{v,w}$: \begin{equation*}
        \dist{T_{u,w},T_{v,w}}\leq \dist{T_{a,w},T_{b,w}}
    \end{equation*} for all $a,b \in [1,...\text{ }n-w+1]$.
    }
\end{definition}
It comes that overlapping sequences are more likely to satisfy the condition above, these pairs are excluded in the problem formulation by ensuring no trivial matches.
\begin{definition}
    A pair of subsequences $T_{u,w}$ and $T_{v,w}$ is a \textit{trivial match} when: $v=u$ or 
        if, given an exclusion zone $\epsilon>0$, then $|u-v|\le\epsilon$. 
\end{definition}
Common values for the exclusion zone $\epsilon$ are fractions of $w$ to exclude matches in the neighboring area of a subsequence \cite{mp1,mp2}. We employ $\epsilon=w/2$.

Expanding upon the single dimensional case, the notions for the \textit{multidimensional} case are derived.
\begin{definition}
    \textup{A \textit{multidimensional time series} $\textbf{T}\in \mathbb{R}^{n\times D}$ is a tuple of $D$ time series $T^{(j)}\in \mathbb{R}^n : \textbf{T}=\left(T^{(1)},...\text{ }T^{(D)}\right)$ where $D$ is the dimensionality of $\textbf{T}$ and $n$ is its length.   
    }
\end{definition}
\begin{definition}
    \textup{A \textit{multidimensional subsequence} $\textbf{T}_{u,w}\in \mathbb{R}^{w\times D}$ is a tuple of unidimensional subsequences from $\textbf{T}$ starting at position $u$ and length $w$, $\textbf{T}_{u,w}=\left(T^{(1)}_{u,w},...\text{ }T^{(D)}_{u,w}\right)$.
    }
\end{definition}
When clear from context we will omit the subscript $w$ from the subsequence notation.


Given two multidimensional subsequences, we are interested in computing their distance.
However, as we mentioned oftentimes considering all dimensions in this computation is uninteresting at best, misleading at worst.
Following \cite{keoghMP} we therefore consider only a subset of $d$ dimensions
when computing the distance.


\begin{definition}
\label{def:distdims}
The $d$-dimensional distance between two subsequences $\subTa$ and $\subTb$
is
\[
\distd{\subTa, \subTb}
=
\min_{F \subseteq 2^{[D]}, |F| = d} \sum_{f \in F} \dist{\subTaf, \subTbf}
\]
\end{definition}
In other words, we select a subset of the dimensions of size $d$ such that the sum of the distances between the individual dimensions is minimized.
Similarly, with
\[
\dims{\subTa, \subTb}
=
\argmin_{F \subseteq 2^{[D]}, |F| = d} \sum_{f \in F} \dist{\subTaf, \subTbf}
\]
we denote the $d$ dimensions that define the distance between two subsequences.
Note that this set of dimensions is potentially different for different pairs of subsequences.
Furthermore, for a given pair of subsequences we name the distance of the dimensions maximally far apart among the ones belonging to $\dims{\subTa, \subTb}$:
\begin{equation}\label{eq:distdmax}
\distdmax{\subTa, \subTb} =
\max_{f \in \dims{\subTa, \subTb}}
\dist{
    \subTaf, \subTbf
}
\end{equation}


\begin{definition}
    \label{def:dist}
    A $d$-dimensional motif is the pair of subsequences $\subTa$, $\subTb$
    such that
    \[
        \distd{\subTa, \subTb}
        \le
        \distd{\subTmulti{u}, \subTmulti{v}}
        \quad
        \forall u, v \in [n-w+1]
    \]
\end{definition}

We are interested in finding the most similar subsequences in an unknown subspace.
\begin{definition}
\textup{Given a $D$-dimensional time series $\mathbf{T}$, motif length $w$, motif dimensionality $d$, and a distance function $dist$, the \textit{top-k multidimensional motifs} are the $k$ subsequences and their subspaces $F$ that minimize the $d$-dimensional distance with respect to all other subsequences of length $w$ in $\mathbf{T}$, ensuring no trivial matches between all possible pair of indices that are part of the motif pairs (i.e., no motif overlaps with another).}
\end{definition}

\begin{example}
    \Cref{fig:multits} reports an example of multidimensional motif discovery:  readings of different sensors from an industrial evaporator \cite{DaISyEVAP}. We highlight the top-3 motifs of length $w = 75$ and dimensionality $d=2$.
    Notice how the middle signal does not participate in the motifs.
\end{example}

\subsection{Locality Sensitive Hashing}

A powerful technique for approximate similarity search in high-dimensional space is Locality Sensitive Hashing (LSH for short), which we briefly introduce here.
Given that the subsequences of length $w$ of a time series can be seen as vectors in $\mathbb{R}^w$, this will prove a useful tool in this setting as well.
For an in-depth discussion of LSH, refer to \cite{lsh, wang2014hashingsimilaritysearchsurvey}.
We provide, as online supplemental material, a short interactive LSH primer\footnote{\url{https://www.dei.unipd.it/~ceccarello/LEIT-motifs-supplemental/}}.

Intuitively, LSH partitions a set of vectors randomly in such a way that close vectors are more likely to end in the same part than far away vectors.
To formalize this intuition, the definition below introduces a distance threshold $R$:
vectors that are closer to each other than $R$ are considered close, and vectors farther than
$cR$ are considered far away.

\begin{definition}[Locality Sensitive Hashing~\cite{lsh}]
\label{def:lsh}
Let $(\mathcal{X}, \operatorname{dist})$ be a metric space
and $\mathcal{H}$ be a family of functions $h: \mathcal{X} \to U$ for some set $U$. 
For a distance threshold $R$ and a constant $c>1$ the family $\mathcal{H}$ is called
$(R,cR,p_1,p_2)$-\textit{locality sensitive}
if $\forall x,y \in \mathcal{X}$ and
for $h$ sampled from $\mathcal{H}$:
\begin{equation}
\label{eq:lsh}
\begin{split}
    \text{if }
        \operatorname{dist}(x, y)\leq R 
        ~~\text{ then } ~~
        \Pr_{h\sim \mathcal{H}}\left[h\left(q\right)=h\left(p\right)\right]\geq p_1 \\
    \text{if }
        \operatorname{dist}(x, y)\geq cR 
        ~~\text{ then }~~
        \Pr_{h\sim H}\left[h\left(q\right)=h\left(p\right)\right]\leq p_2
\end{split}
\quad.
\end{equation}
\end{definition}
The event of two vectors having the same hash value is collide a \emph{collision}.
A key quantity to assess the performance of LSH families is
\[
\rho=\frac{\log 1/p_1}{\log 1/p_2}
\]
A small $\rho$ value entails that the LSH family is effective at discerning close vectors from far-away ones.


For the common case of the Euclidean distance considered in this paper a widely used LSH family is that of \emph{Discretized Random Projections}~\cite{datar2004}.
For a vector $x\in \mathbb{R}^w$ and quantization parameter $r\in \mathbb{R}^+$ the hash function is
\begin{equation}
    h(x) = \left\lfloor \frac{a\cdot x+b}{r} \right\rfloor
    \label{eq:drp}
\end{equation}
where $a\in\mathbb{R}^w$ is a vector with random components following the $\mathcal{N}(0,1)$ Gaussian distribution, and $b\in\mathbb{R}$ is chosen uniformly at random in the interval $[0,r]$.

The probability that two vectors $x,y$ at Euclidean distance $R$ collide is:
\begin{equation}\label{eq:eucl-collision-probability}
    \Pr_{h\sim\mathcal{H}}\left[h(x) = h(y)\right] = 1-2\cdot norm\left(-\frac{r}{R}\right)-\frac{2}{\sqrt{2\pi}~r/R}\left(1-e^{-\left(\frac{r^2}{2R^2}\right)}\right)
\end{equation}
where $norm$ is the cumulative distribution function of a \textit{Standard normal distribution} \cite{datar2004}.
For this family of LSH functions we have $\rho = 1/c$ \cite{rhoboundlsh},
a fact that we will use in the analysis of the complexity of our algorithm.

As a notational shorthand, for two vectors $x$ and $y$ at distance $\dist{x, y}$ we define
\[
P(\dist{x, y}) = \Pr_{h\in \mathcal{H}}[h(x) = h(y)] ~.
\]

In order to \emph{amplify} the gap between the collision probability of close vectors
(at distance $\le R$) and far vectors (at distance $\ge cR$) a common strategy
is to create a \emph{composite} hash function by sampling $K$ hash functions and concatenating their outputs in a tuple of length $K$:
\[
    h'(x) = \langle
        h_1(x), h_2(x), \dots, h_K(x)
    \rangle
\]
The resulting LSH family is $(R, cR, p_1^K, p_2^K)$-locality sensitive.
Using larger values of $K$ lowers the probability for both close and far points to collide, with a more marked effect on the latter.
Repeating this process with $L$ independent composite hash functions implies that
points at distance smaller than $R$ collide in at least one of the repetitions with probability at least $1-\left(1- p_1^K\right)^L$.

Setting the parameters $K$ and $L$ requires the knowledge of the distance threshold
$R$, which in our setting is the distance of the top-$k$ motif.
Of course we do not know this distance beforehand, therefore in the following we describe
an algorithm that automatically tunes them based on the input.


