\section{Related Work}
\label{sec:relwork}
A relatively large body of literature exists on time series and motif discovery, however the multidimensional case, despite aggregating a large interest, has seen only a fraction of the various works developed for the unidimensional case.

Comparisons between subsequences in time series are usually carried out by comparing their shapes rather than the raw values, in order to show invariance to noise and scale.
Several similarity measures have been used in time series processing, they can be split in two major subgroups: \textit{elastic} measures, that create a non linear one-to-many mapping between points of sequences, and \textit{lock-step} measures, where the mapping is one-to-one \cite{distances}. Between the most commonly used distances is \textit{Dynamic Time Warping} (DWT), an \textit{elastic} measure that allows the comparison of temporally misaligned sequences due to compression or stretching of shapes (i.e., warp in time).
\textit{Edit Distances}, are a family of distances that measure the number of \textit{edits} (e.g., substitutions, deletions, insertions) needed to obtain equal subsequences \cite{xiao2019edit}. Another common measure is the \textit{z-normalized Euclidean Distance}, a lock-step measure that z-normalizes the data before computing the Euclidean Distance, allowing variations in amplitude and mean values, so that the measured similarity is between the \textit{shapes} \cite{de2019implications}. 
This distance measure is, up to a constant factor, equivalent to the Pearson correlation coefficient \cite{berthold2016pearseucl}.

Many motif discovery techniques rely on symbolic abstraction of the raw data to facilitate matching of common patterns, besides smoothing out noise in the data. \textit{SAX} \cite{sax} found great success for its efficiency, requiring only a mean for the \textit{Piecewise Aggregate Approximation} and a table look-up for the symbol association.
It is less computationally complex than symbolization methods like \textit{ACA} \cite{sant2011symbolization}. Moreover, it is more general compared to methods like \textit{Persist} \cite{persist2005}, which require time series with a recognizable underlying structure \cite{sant2011symbolization}. We refer to \cite{symbolizationreview} for a complete overview on symbolization techniques.

Approximate algorithms for multidimensional motif discovery can be categorized in two major families: those that reduce in some way the time series into a unidimensional one and those that effectively work on multidimensional data.

Algorithms in the first category use techniques like \textit{Principal Component Analysis} (PCA) to generate a \textit{meta-}unidimensional time series that can be processed with the standard approaches developed for motif discovery in the univariate case. The work of \cite{tanaka2005discovery} employs \textit{Minimum Description Length} (MDL), to find the motifs. This approach is based on the strong assumption that all dimensions are relevant, as even a small number of noise dimensions leads to a meta-time series with little to no information, the  work of \cite{tanaka2005discovery} asserts how this algorithm \textit{can extract
a motif that can be recognized intuitively by human}, underlining how more work is needed when the structure of the time series is unknown. Moreover, the authors highlight the challenge of dynamically tuning the input parameters, since suboptimal sets can lead to poor outcomes in discovery.

The approaches that fall into the second category can be divided into two subfamilies: those who find motifs that span simultaneously in all dimensions \cite{berlin2012detecting} and those who find \textit{subdimensional motifs}. The second category is the one that allows the extraction of the most amount of information, since finding motif in all dimensions falls into a similar assumption of the algorithms that synthesize an univariate meta-time series, considering also irrelevant dimensions in the process.

For the task of subdimensional motif discovery, the \textit{Axis-Aligned Projection} algorithm of \cite{4470297}, applies \textit{SAX} to independently symbolize each dimension of the time series, then a matrix of collisions between the subsequences is populated by iteratively random selecting the a set of dimensions, creating words by concatenating the selected symbols and finding the matches. The algorithm has a linear running time in expectation, but it is greatly affected by the input threshold on the distance, a data dependent variable that is difficult to have an idea of without knowing in great details the data and the kind of motif searched.

Common denominators for approximation algorithms are input parameters whose choice influences both the quality of the results and the efficiency of the algorithms, other than a low resistance to irrelevant dimensions.

The \textit{matrix profile} \cite{keoghMP} is commonly used to solve this problem exactly. It is a data structure that stores the distance between a subsequence and its nearest neighbor. The first motif can be identified by searching the minimal entry in the matrix, the second is the next minimum not overlapping with the first one, and so on. When discovery is limited to motifs with a certain dimensionality, the search can be restricted to only the $d$-th row of the matrix. To find the set of dimensions that span the motif, a variety of techniques can be used (e.g., finding the subset with minimal distance) with MDL being the one used by the state-of-the-art implementation \cite{Law2019}.


Locality Sensitive Hashing (LSH) is a technique often employed in similarity search~\cite{lsh, 2020mining} which we will review in the next section.
Relevant for the scope of this paper is the family of hash functions for the Euclidean distance \cite{datar2004}.

LSH has already been used in the context of time series to discover motifs, the work of \cite{LSHearthquake} employed LSH to derive fingerprints for earthquake waveforms.
In the one dimensional case the work of \cite{attimo} provided an algorithm with guarantees on recall by employing the properties of LSH. Subsequences are matched using their \textit{fingerprints}, the distances are verified only on the collisions allowing the algorithm to compute just a fraction of all the distance computations.

Using LSH requires setting up a number of parameters. The framework developed by PUFFIN \cite{puffinn} is capable of automatically tuning these parameters for $k$-nearest neighbours queries. The work of \cite{confsampling} develops a technique to automatically find the best number of concatenations and repetitions to successfully answer a nearest neighbour query with a certain probability even when the probability of collision between pairs of points is unknown. We follow the implementation described in \cite{puffinn}, expanding on the case where the pairs are actually ordered sets.
