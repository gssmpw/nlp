\section{Experiments}
\label{sec:exp}
This section aims at answering the following questions:
\begin{itemize}
    \item How does our algorithm compare with the state-of-the-art?
    \item How do parameters $K$, $L$, and $r$ influence the performance?
    \item How does the algorithm scale with respect to the input size?
    \item Is the algorithm able to find motifs in high dimensional noisy time series?
\end{itemize}
\subsubsection*{Baselines}
Our algorithm is compared to \textsc{Mstump} \cite{Law2019}, the state of the art implementation for the multidimensional matrix profile. Alongside, we consider the Extended Motif Discovery (\textsc{EMD}) algorithm \cite{tanaka2005discovery}, reimplemented in \cite{emdsilva}, for comparisons with approximate approaches.
All algorithms were subject to a global timeout of 4 hours per execution for datasets under the million points, and 24 hours for larger datasets.
\subsubsection*{Experimental Setup}
The evaluation was carried out on a machine with a 8 core Intel Xeon W-2245 @ 3.90 GHz equipped with 128 GB of memory.
\subsubsection*{Software Implementation}
Our implementation of the algorithm with all the optimizations discussed above is available at \url{\vldbavailabilityurl}. We refer to our algorithm as \textsc{LEIT-motifs}, which stands for scaLablE multIdimensional Time series Motifs.
\subsubsection*{Datasets}
All the experiments are run on the following real datasets from different domains, whose details can be found in \Cref{tab:dataset}.
\begin{itemize}
    \item \textsc{Potentials} is a record of skin potentials of a pregnant woman \cite{DaISyPREG}.
    \item \textsc{Evaporator} is data from a four-stage evaporator to reduce the water content from products \cite{DaISyEVAP}.
    \item \textsc{Ruth}, the Mel-spectrogram of the song \textit{Running Up That Hill} by Kate Bush, extracted with the following parameters: 32-Mel scale filters, 46 milliseconds short time Fourier transform window and 23 milliseconds hop.
    \item \textsc{Weather} is representing the hourly climate data near Monash University, Clayton, Victoria, Australia, for about $10$ years \cite{godahewa_2021_5184708}.
    \item \textsc{Whales} is obtained from data of an underwater passive acoustic network and is a 10 minute recording of humpback whales vocalizations \cite{NOAA_PIPAN_2021}.
    \item \textsc{Quake} is a waveform from the Observatories and Research Facilities for European Seismology (ORFEUS) during the 2014 Aegean Sea earthquake \cite{quake}.
    \item \textsc{Electrical\_load} includes cleaned electrical consumption data in Watts for 20 households at aggregate and appliance level \cite{PMID:28055033}.
    \item \textsc{LTMM} contains 3-day 3D accelerometer recordings of elder community residents, used to study gait, stability, and fall risk \cite{falldataset, fall}.
\end{itemize}

The choice of the reference motif dimensionality $d$ for each dataset is guided by the additional information available. Specifically, for \textsc{Potentials}, \textsc{Evaporator} and \textsc{Weather} we use metadata information for the expected dimensionality of the pattern. For \textsc{Ruth} and \textsc{Whales}, we follow previous work on audio data to find, respectively, the drum pattern of the song and whale harmonization \cite{keoghMP}. Finally, \textsc{Electrical\_load}, \textsc{Quake} and \textsc{Ltmm} were chosen based on the domain information.

Furthermore, we characterize each dataset by the \emph{contrast} $c_{d,1|n}$ of its top $d$-dimensional motif, as per Definition~\ref{sec:contrast}.
Datasets with a small contrast are expected to be more difficult, i.e. to require more time to find the top motif.
For instance \textsc{Electrical\_load} is expected to be more difficult compared to \textsc{Weather}, since the latter has a pair of subsequences that repeat the same shape at a distance that is significantly smaller than the $n$-th clostest pair.

\subsubsection*{Default parameter values}
\textsc{LEIT-motifs} will have its parameters set to default unless otherwise indicated.
The failure probability is set at $\delta=0.01$, the maximum hash length is set to $K = 8$, while the maximum number of repetitions is set to $L = 200$. $r$ is automatically estimated using the heuristic introduced in \Cref{sec:r_auto}.


\subsection{Finding the top motif}
\begin{table}[t]
\caption{Time required to find the $d$-dimensional top motif, for fixed $d$, averaged over over 9 runs. Values in parentheses are estimates.
}
\centering
\begin{tabular}{lrrrrr}
\toprule
dataset    &\multicolumn{2}{c}{LEIT-motifs} & MSTUMP    & EMD     \\ 
\cmidrule{2-3}
 & Index build & Total  &&\\
\midrule
potentials & 0.11 & \cellcolor[HTML]{CDF2D9} 0.51 &   3.65     &   4.80 \\
evaporator & 0.16 & \cellcolor[HTML]{CDF2D9} 0.55 &  4.45 &    12.95\\
RUTH &  2.91 & \cellcolor[HTML]{CDF2D9} 8.10 &   84.04 & 1.5h\\
weather   & 15.04 & \cellcolor[HTML]{CDF2D9} 33.37 &   1035.73    &          -\\ 
whales   & 60.67 & \cellcolor[HTML]{CDF2D9} 2.2 h   &    (2.7 days) & -\\ 
quake    & 175.3 & \cellcolor[HTML]{CDF2D9} 3.6 h &  (7.2 days) & -\\ 
electrical\_load    & 180.2 & \cellcolor[HTML]{CDF2D9} 2.8 h &   (8.4 days) & -\\ 
LTMM    & 240.6 & \cellcolor[HTML]{CDF2D9} 15.6 h &   (11.8 days)    &  -\\
\bottomrule
\end{tabular}
\label{tab:time-fixed-d}
\end{table}

In this first experiment, the task is to find the top motif at a given dimensionality $d$ (as per Table~\ref{tab:dataset}).
For each dataset we report the total running time of the two baselines \textsc{Mstump} and EMD in the last two columns, whereas for our approach we report both the total time and the time required to set up the index.
All times are in seconds, unless otherwise noted. Furthermore, for timed out runs of \textsc{Mstump} we report, in parentheses, an estimate of the running time, which is made possible by the very regular behavior of \textsc{Mstump} with respect to the input size. For EMD the running time is more unpredictable, therefore we refrain from providing estimates for timed out runs.

As can be seen, our algorithm is faster on all settings, completing the execution orders of magnitude faster than the baselines on the larger datasets.
To substantiate this observation, Table~\ref{tab:num_dist} reports the number of distance computations carried out by each approach:
\textsc{LEIT-motifs} usage of LSH allows to effectively prune most distance computations, whereas \textsc{Mstump} computes a quadratic number of distances.

\begin{table}[t]
\caption{{Number of cumulative distance computations to find the top $d$-dimensional motif.}}
\begin{tabular}{lrrr}
\hline
dataset    & LEIT-motifs & MSTUMP & EMD \\ \hline
potentials & $64$&  \addstackgap[1.5pt]{ $2.4 \cdot 10^7$ }& $1.0\cdot 10^2$\\
evaporator & $4.7\cdot 10^2$&   $1.4\cdot 10^8$ & 5\\
RUTH       & $1.2 \cdot 10^2$  &  $3.3 \cdot 10^9$ & $5.4\cdot 10^3$\\
weather    & $9.8 \cdot 10^4$   &   $3.9 \cdot 10^{10}$ & -\\
whales    & $3.1 \cdot 10^8$   &   $3.2 \cdot 10^{12}$ & -\\
quake    & $1.1 \cdot 10^6$   &   $6.6\cdot10^{14}$ & -\\
electrical\_load & $4.4 \cdot 10^2$ & $2.4\cdot 10^{14}$ & - \\
LTMM & $4.4 \cdot 10^2$ & $1.9\cdot 10^{15}$ & - \\\hline
\end{tabular}
\label{tab:num_dist}
\end{table}

The \textsc{EMD} algorithm appears to be the slowest in all cases, its major drawback being the set up of the parameters for a successful discovery.

We stress that the Matrix Profile computed by \textsc{Mstump} allows the discovery of motifs of any dimensionality, while in this test \textsc{LEIT-motifs} only finds the top motif for a fixed dimensionality. In the next section
we will investigate how \textsc{LEIT-motifs} behaves when finding motifs for all dimensionalities at the same time.

\begin{table}[t]
\caption{\textbf{Memory required to find the top $d$-dimensional motif.}}
\centering
\begin{tabular}{lrrrr}
\hline
& \multicolumn{3}{c}{Space (Gb)} & \\ \cline{2-4} 
dataset    & LEIT-motifs       & MSTUMP    & EMD \\ \hline
potentials &    \cellcolor[HTML]{CDF2D9}     0.016&        0.028&           0.79\\
evaporator &    \cellcolor[HTML]{CDF2D9}     0.020&        0.030&           0.81\\
RUTH & \cellcolor[HTML]{CDF2D9} 0.025&        0.084&           1.19\\
weather    &  \cellcolor[HTML]{CDF2D9}  0.027&       0.106&           -\\ 
whales    &  \cellcolor[HTML]{CDF2D9}  0.24&       3.2&           -\\ 
quake    &  \cellcolor[HTML]{CDF2D9}  2.13&       23.0&           -\\ 
electrical\_load    &  \cellcolor[HTML]{CDF2D9} 1.50      &    7.4     &     -      \\ 
LTMM    &  \cellcolor[HTML]{CDF2D9}  3.00&       15.7&           -\\
\hline
\end{tabular}
\label{tab:space}
\end{table}

Finally, Table~\ref{tab:space} reports the memory usage, in gigabytes, of the different algorithms.
We observe that \textsc{LEIT-motifs} requires the least memory to execute.


\subsection{Finding the top motifs with different dimensionalities}

\begin{table}[t]
\caption{Time required for the top motifs for all dimensionalities. Mean over 9 runs. Values in parentheses are estimates.
}
\centering
\begin{tabular}{lrr}
\toprule
dataset    & LEIT-motifs  & MSTUMP      \\
\midrule
potentials &  0.75  & 3.65   \\
evaporator &  0.89  & 4.45 \\
RUTH &  12.46  & 84.04 \\
weather   &  47.85   & 1035.73 \\ 
whales   &  2.5 h  &  (2.7 days) \\ 
quake    & 5.8 h  &(7.2 days)\\ 
electrical\_load    &  3.1 h   & (8.4 days)\\ 
LTMM    &  16 h & (11.8 days) \\
\bottomrule
\end{tabular}
\label{tab:time-multi-d}
\end{table}

\begin{figure}
    \centering
    \includeinkscape[width=\linewidth]{svg-inkscape/multisub_svg-tex.pdf_tex}
    \caption{Solid lines mark the time required by \textsc{LEIT-motifs} to find the top motif of each dimensionality for all datasets; dotted lines mark the time required by the \textsc{Mstump} baseline for the same task.}
    \label{fig:multisub}
    \Description[Time required for multidimensional motif discovery in each dataset. The times are plotted with respect to the distance. Greater distances require more time.]{}
\end{figure}

In this second experiment the aim is to find the motifs for all dimensionalities $2\le d \le D$ in the same execution.
MSTUMP does this natively, whereas our algorithm \textsc{LEIT-motifs} can be adapted to do so as described in Section~\ref{sec:multisub}. EMD can only achieve this through multiple executions and is thus excluded from this experiment.

In Table~\ref{tab:time-multi-d} we report the results for this experiment.
Note that the running times for \textsc{Mstump} are identical to the ones reported in Table~\ref{tab:time-fixed-d}, given that \textsc{Mstump} finds motifs for all dimensionalities natively.
As for \textsc{LEIT-motifs} the indexing time is the same in this setting as the one reported in Table~\ref{tab:time-fixed-d}, therefore we omit it for clarity.

We observe that even in this more challenging scenario our algorithm is able to discover the motifs for all dimensionalities faster than the \textsc{Mstump} baseline.
Indeed, the running time of \textsc{LEIT-motifs} is always within a factor $\approx 1.6$ of its running time on the one dimensional case.

To further investigate this behavior, in Figure~\ref{fig:multisub} we report the relation between the discovery time of a motif ($x$ axis) and its distance ($y$ axis). Each color identifies a dataset, and each point represents a motif for some dimensionality: for instance the \textsc{Weather} dataset (orange) has 7 points in this figure because it has 8 dimensions, and we set the algorithms to find the 7 motifs spanning between 2 and 8 dimensions.
The dotted horizontal lines mark the runtime performance of \textsc{Mstump}, which reports all the motifs at the same time at the end of its execution.
For instance, the orange dotted horizontal line reports that \textsc{Mstump} takes $\approx 10^3$ seconds on the \textsc{Weather} dataset.
The times marked by dotted lines and by the rightmost points of each solid line are the same as in Table~\ref{tab:time-multi-d}.

As can be observed in \Cref{fig:multisub}, by virtue of how the algorithm runs, motifs at a shorter distance are found earlier.
For instance, for \textsc{Weather} the motif of dimensionality $4$ has a $\operatorname{dist}_4\simeq49$ while the motif of dimensionality $5$ has $\operatorname{dist}_5\simeq 77$, which requires more iterations (and thus more time) to meet the stopping condition of the algorithm.
As we discussed earlier, this fact can be used in an \emph{anytime} fashion:
the execution can be stopped at any point after the first motifs have been returned, knowing that motifs yet to be found are at larger distances and thus might be uninteresting.

\subsection{Scalability}
\begin{figure}
    \centering
    \includeinkscape[width=\linewidth]{svg-inkscape/scalability_svg-tex.pdf_tex}
    \caption{Scalability vs. input size (log scale).}
    \label{fig:scalability}
    \Description[Subquadratic behaviour of \textsc{LEIT-motifs} with respect to \textsc{Mstump} on different input sizes.]{}
\end{figure}

We now test the scalability of \textsc{LEIT-motifs} compared to \textsc{Mstump}. We omit EMD from this comparison, as previous sections show that it does not scale beyond moderately sized time series.
While the behavior of \textsc{Mstump} is data-independent, i.e. its running time depends only on the size of the data,
for \textsc{LEIT-motifs} the running time is influenced by both the size of the time series and the contrast of the motifs.
Therefore, to test the scalability in a robust way we employ synthetic datasets.
In particular, we consider random walks with $D=5$ of length between 10000 and 100 million, planting a sinusoidal motif of length $w=300$ across $d=2$ dimensions.
For each length size we create two datasets, a \emph{hard} instance and a \emph{easy} one.
To this end the sinusoids are perturbed with Gaussian noise so that the motif distance is such that the contrast
$c_{2,1|n}$ is 30 for the \emph{easy} dataset and 1.1 for the \emph{hard} dataset.

The results are reported in \Cref{fig:scalability}, where both axes use a logarithmic scale.
We observe that our algorithm scales better with the input size, with a sub-quadratic running time.

\subsection{Influence of parameters on the running time}
We now study the impact of the parameters $K$, $L$, and $r$ on the running time, that in the previous experiments were either fixed ($K=8$ and $L=200$) or estimated from the data (the quantization parameter $r$).
Remember that our algorithm finds the correct answer with probability $1-\delta$ for any setting of the parameters, that influence only the performance.

Each experiment in the following has been repeated 10 times: the plots report the average as a line and the corresponding confidence interval as a band around said line.
To be able to test several combinations of parameters while repeating the experiments 10 times, we focus on the 4 smallest datasets in our testbed.

\paragraph{Impact of concatenations $K$}
\begin{figure}[t]
    \centering
    \includeinkscape[width=\linewidth]{svg-inkscape/K_plot_svg-tex.pdf_tex}
    \caption{Time required for motif discovery at different maximum allowed values of $K$.}
    \label{fig:K_graphs}
    \Description[All datasets show a minimum around the value 8.]{}
\end{figure}
We test $K\in\{4,8,12,16\}$, reporting the results in \Cref{fig:K_graphs}.
We observe that a higher number of concatenations is related to longer computing times,
both because more repetition of the outer loop of Algorithm~\ref{alg:emitaggr} have to be executed and because the index construction takes longer.
Conversely, using short hash values with $K=4$ incurs high execution times as well, mainly because fewer distance computation are pruned.
In all tested cases the best tradeoff is achieved with $K=8$, which is the recommended value and the one we used in all previous experiments.
We stress that for any value of $K$ considered in this section \textsc{LEIT-motifs} is faster than the baseline \textsc{Mstump}.


\begin{figure}[t]
    \centering
    \includeinkscape[width=\linewidth]{svg-inkscape/L_plot_svg-tex.pdf_tex}
    \caption{Time required for motif discovery at different maximum allowed values of L.}
    \label{fig:l_graphs}
    \Description[Both a small and a large value of L requires more search time than the middle values]{}
\end{figure}
    
\begin{figure}[t]
    \centering
    \includeinkscape[width=\linewidth]{svg-inkscape/r_plot_svg-tex.pdf_tex}
    \caption{\textbf{Number of comparisons for different values of $r$.} The red dot is the value found by the heuristic in \Cref{sec:r_auto}.}
    \label{fig:r_graphs}
    \Description[Each dataset shows a different behavior with respect to the r values, the one found by our heuristic is always close to the empirical minima.]{}
\end{figure}

\paragraph{Impact of repetitions L}
We test $L$ in the range from 10 to 400:
a large $L$ will allow the stopping condition to be satisfied in earlier iteration of the outer loop of Algorithm~\ref{alg:emitaggr}, thus requiring fewer distance computations, at the expense of a higher index build time.
\Cref{fig:l_graphs} explores this trade-off, with values around $L=100$ mark capable of minimizing the search time.
Datasets that benefit more from an increased number of repetitions are those whose relative contrast is higher (e.g. \textsc{weather}), because motifs are more likely to share long hash prefix.

\paragraph{Impact of the quantization parameter $r$}
In this experiment, we manually fix $r\in\{4,8,12,16\}$ and compare it with the value automatically chosen by the algorithm using the procedure described in \Cref{sec:r_auto} in terms of the resulting number of distance computations, and hence running time.
Figure~\ref{fig:r_graphs} shows that the choice of $r$ has a dramatic impact on the number of distance computations. 
Remarkably, the value automatically picked by our algorithm in a data-dependent way (and used in the rest of the experiments we presented) attains the same performance as the best fixed parameter considered in this experiment.


\subsection{Resistance to noise dimensions}
In this last experiment we replicate the experimental design of~\cite{keoghMP},
where the task is to reliably finding motifs even when datasets are cluttered by noisy dimensions.
This setting has proven to be challenging for other approximation algorithms employing dimensionality reduction or that consider all dimensions~\cite{keoghMP}.


To evaluate the robustness of our algorithm,
we added from 4 to 256 additional dimensions, each being a random walk, to datasets
\textsc{Potentials}, \textsc{Evaporator}, \textsc{Ruth}, \textsc{Weather}.
Then we ran \textsc{LEIT-motifs} on each dataset looking for the $d$-dimensional motif, with $d$ as per Table~\ref{tab:dataset}, repeating each experiment 12 times.
In all settings, irrespective of the number of additional noisy dimensions, our algorithm attained recall values of 1, showing its robustness to the number of irrelevant dimensions.



