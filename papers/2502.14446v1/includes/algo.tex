\begin{algorithm}[t]
    \SetKwInOut{Input}{\raggedright{Input}}\SetKwInOut{Output}{Output}
    \Input{$D$-dimensional time series $\mathbf{T}$, subsequence length $w$, number of motifs $k$, $K$ max allowed number of concatenations, $L$ max number of repetitions, dimensionality of the motifs to find $d$, failure probability $\delta$}
    \Output{\{Set of the top-$k$ motifs\}, with probability $1-\delta$}
    \tcp{Initialization}
    \For{$\mathbf{T}_a \in \mathbf{T}^w, f\in [D], j\in [L]$}{
            \label{ln:hash-computation}
        compute $h_{K,j}(\subTaf)$ and insert it into the trie data structure\; 
    }
    \BlankLine
    
    \texttt{TOP = PriorityQueue()}\label{ln:priority-queue-init}\;
    \For{$i\leftarrow K \text{ to } 1$}{ \label{ln:prefix-cycle}
        \For{$j\leftarrow 1 \text{ to } L$}{
            \Let{$E$}{$\emptyset$}\label{ln:define-edges}\;
            \For{$f \in [D]$}{
                \For{$(\subTa, \subTb) \in \Tmulti^w \times \Tmulti^w : h^f_{i,j}(\subTaf) = h^f_{i,j}(\subTbf)$}{
                    \label{ln:prefix-eq}
                    \uIf{$(\subTa, \subTb) \notin E$}{
                        \Let{$E$}{$E \cup \{(a,b)\}$}\;
                        \Let{$W(a,b)$}{1}\;
                    }
                    \Else {
                        \Let{$W(a,b)$}{$W(a,b)+1$}\label{ln:increment-weight}\;
                    }
                } 
            }
            \For{$(a, b) \in E$}{
                \label{ln:second-start}
                \If{$W(a, b) \ge d$}{
                    \label{ln:weigth-constr}
                    \texttt{TOP.insert}$\left((\subTa,\subTb)\right)$\;
                    \If{$|$\texttt{TOP}$|$>k}{
                        \texttt{TOP.pop()} \label{ln:second-end}
                    }
                }
            }
            \If{$|$\texttt{TOP}$|=k$ $\wedge$ \texttt{STOP}$\left(\text{\texttt{TOP.max()}},i,j,\delta\right)$}
            {\Return \texttt{TOP}}
        }
    }
    \Return true top-$k$ by computing all pairs\;
\caption{LEIT-motifs}
\label{alg:emitaggr}
\end{algorithm}

\begin{algorithm}[t]
    \caption{Stopping condition\label{alg:stopping-condition}}
    \SetKwProg{Fn}{Function}{ is}{end}
    \Fn{\texttt{STOP}$((\subTa, \subTb), i, j, \delta)$}{
        \Let{$p$}{
            $P\left(\distdmax{\subTa, \subTb}\right)^{d}$
        }\label{ln:collision-probability}\;
        \uIf{$i=K$}{
            \Return 
                $\left(1-p^i\right)^j \le \delta$ \label{ln:case-k}
        }\Else {
            \Return 
                $\left(1-p^i\right)^j \cdot \left(1-p^{i+1}\right)^{L-j} \le \delta$
                \label{ln:case-shorter-than-k}
        }
    }
\end{algorithm}


\section{Algorithm}
\label{sec:algo}

We now describe our algorithm, named \textsc{LEIT-motifs}, to find the top-$k$ motifs in a multidimensional time series. Our algorithm has a user-defined error probability $\delta$.
At a high level, our algorithm is comprised of two main phases.
First, it builds a LSH-based index of the time series subsequences, where the dimensions of all subsequences are hashed independently.
Then the index is traversed to discover candidate motif pairs, until a data-dependent stopping condition is met.
The pseudocode of our algorithm is presented in Algorithm~\ref{alg:emitaggr}.

\paragraph{Index construction}
As we have seen in the previous section, two critical parameters in an LSH setup are the number of concatenations $K$ and the number of repetitions $L$.
Furthermore, the hash function of Equation~\eqref{eq:drp} requires a quantization parameter $r$ to be set as well. We shall see how to automatically set $r$ later in Section~\ref{sec:opti}.

For a time series $\Tmulti$, setting $K$ and $L$ so to minimize the number of distance computations requires the knowledge of the motif distance \emph{before} we construct the index.
To work around the fact that we, of course, do not know this distance beforehand, our index instead sets the \emph{maximum} $K$ and $L$ values to be used in the second phase.

To construct the index of the subsequence of length $w$ of a multivariate time series $\Tmulti$
we first sample multiple independent composite hash functions of length $K$: one for each dimension $f\in[D]$ and repetition $j\in[L]$. As described in the previous section, this is achieved by simply sampling a random vector $a$ and a random value $b$ for each function.
We denote the composite hash function at repetition $j\in[L]$ for dimension $f\in[D]$ with
$h_{K,j}^f$.

Then, for each subsequence $\subTa$, we compute multiple independent hash values:
for each dimension $f\in[D]$ and for each repetition $j\in[L]$, we compute
the \emph{composite} hash value of length $K$ of the vector $\subTaf$ using the corresponding hash function, that is we compute $h_{K,j}^f(\subTaf)$.

Note that a composite hash value can be seen as a string of $K$ integer values.
A fundamental operation in the next phase will be retrieving, for a given dimension $f\in[D]$
and repetition $j\in[L]$, all the subsequences whose hash share the prefix of a given length.
To efficiently support this operation, we construct an index on hash values consisting of a family of tries. Specifically, for each dimension $f\in[D]$ and each repetition $j\in[L]$, each trie indexes the string of $K$ hash values of all subsequences.
In the following, for $0 <i \le K$ we denote with $h_{i, j}^f(\subTaf)$ the prefix
of length $i$ of the hash value for the subsequence dimension $\subTaf$ in repetition $j$.


\paragraph{Index traversal}

A key property of the index introduced in the previous paragraph is that very similar subsequences are likely to share long prefixes of their hashes.
At the same time, due to the probabilistic nature of LSH it is not certain that similar
subsequences share a long prefix in the first repetition.

We first give the intuition behind the algorithm, and then give all the details.
Starting from the longest possible hash prefix, $K$, the $L$ repetitions are considered,
focusing in each one on all pairs of subsequences
sharing the same hash prefix: for each such pair we can compute the distance, which is used to rank candidate motif pairs in a priority queue.
Furthermore, from the distance we can derive the collision probability by means of Equation~\eqref{eq:eucl-collision-probability}.
We use this probability to define a stopping condition that allows to rule out the event that the true motif has \emph{not} been seen in the repetitions considered so far.
If the probability of this negative event is less than a user defined threshold $\delta$ and the priority queue contains at least $k$ elements then
the algorithm stops, returning the top-$k$ pairs in the priority queue.
Otherwise the next iteration is considered, with a caveat: if the last of the $L$ repetitions is reached with the stopping condition not satisfied, then it means that hashes of length $K$ are too selective for the dataset at hand. Therefore, the process is restarted considering prefixes of length $K-1$. This procedure continues, potentially considering shorter and shorter prefixes, until the stopping condition is met.

The above high level intuition is complicated by the fact that we have to deal with multidimensional subsequences whose dimensions are hashed independently. In the following we thus detail the algorithm with reference to the pseudocode in Algorithm~\ref{alg:emitaggr}.

The algorithm proceeds in rounds from $K$ to $1$: in round $i$ hash prefixes of
length $i$ are considered.
In each round $i$ then all repetitions $1\le j \le L$ are considered,
and each such iteration is comprised of three steps:
counting the number of collisions, computing distances, and checking the stopping condition.

The first step (line~\ref{ln:define-edges} to~\ref{ln:increment-weight}) counts for each pair of subsequences the number of dimensions in which they share a prefix of length $i$.
Note that retrieving the pairs of subsequences colliding in each dimension (line~\ref{ln:prefix-eq}) is done efficiently by leveraging the fact that hash values are stored in tries.
By the end of the first step the algorithm has built a set $E$ of pairs of subsequences
that collided in at least one dimension, and for each pair $(a,b) \in E$ the function
$W(a,b)$ reports the number of dimensions on which $\subTa$ and $\subTb$ collided.

In the second step the focus (lines from~\ref{ln:second-start} to~\ref{ln:second-end})
is on the pairs that share prefixes in at least $d$ dimensions out of $D$, where $d$ is the target number of dimensions spanned by the motifs.
The intuition is that a motif pair $(\subTa, \subTb)$, being similar in at least $d$ dimensions, will have $W(a, b) \ge d$.
For each such pair, the algorithm computes the distance and updates the priority queue of candidates, keeping only the top-$k$ in memory.

Finally, if the priority queue contains $k$ candidates then the algorithm checks the stopping condition (Algorithm~\ref{alg:stopping-condition}), which considers the pair at maximum
distance in the priority queue. In particular, for this pair $(\subTa, \subTb)$ the stopping condition focuses on $\distdmax{\subTa, \subTb}$ as defined in Equation~\eqref{eq:distdmax}.
This distance is used to compute an upper bound $p$ to the probability of the two subsequences colliding in $d$ dimensions at the same time independently (line~\ref{ln:collision-probability}).
This probability $p$ is then used to compute the probability that a pair of subsequences with a smaller pairwise distance was missed by the algorithm in all the previous iterations.
If this probability is smaller than the user-defined $\delta$, then the algorithm can successfully terminate.
In particular, line~\ref{ln:case-k} is evaluated when the full hashes are being considered, and only the first $j$ repetition have been executed.
Line~\ref{ln:case-shorter-than-k} is executed when prefixes shorter than $K$ are under consideration, and takes into account the fact that the algorithm executed $L-j$ iterations with prefixes of length $i+1$.












 




