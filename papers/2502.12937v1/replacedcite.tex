\section{Related Work}
\paragraph{Graph Based Semi-supervised Learning.} Semi-supervised Learning is a popular machine learning paradigm with significant theoretical interest____. Classical algorithms focus on label-propagation based techniques, such as ____, ____, and many more. 
In recent years, graph neural networks (GNNs) have become increasingly popular in a wide range of application domains
____.
A large number of different architectures have been proposed, including graph convolution networks, graph attention networks, message passing, and so on ____. 
Both label propagation-based algorithms and neural network-based algorithms are useful in real life and perform  well with appropriate hyperparameter tuning. For example, although GNN-based algorithms are more predominant in certain applications, ____ show that modifications to label propagation-based algorithms can outperform GNN. For node classification in GNNs, many prior works study generalization guarantees for tuning network weights in GNNs ____. In contrast, we study the tuning of the \textit{hyperparameters} related to the GNN architecture. 

\paragraph{Hyperparameter Selection.} Hyper-parameters %, such as the weight for self-loops,
play an important role in the performance of both classical semi-supervised learning methods and GNNs. 
In general, hyperparameter tuning is performed on a validation dataset, and follows the same procedure: determine which hyperparameters to tune and then search within their domain for the combination of parameter values with best performance ____. Many methods have been proposed to efficiently search within the parameter space, such as grid search, random search ____, and Bayesian optimization (____; ____; ____). A few existing works investigate the theoretical aspects of these methods, such as through generalization guarantees and complexities of the algorithms. In particular, a recent line of work____ studies the tuning regularization hyperparameter in linear regression in statistical settings and provides generalization guarantees. For unsupervised learning, prior work ____ has proposed several different parameterized algorithm families of clustering algorithms and studied the sample and computational complexity of learning the parameters. For semi-supervised learning, a recent line of work____ considers the problem of learning the best graph hyperparameter from a set of problem instances drawn from a data distribution. Another recent work ____ investigates the kernel hyperparameters in GNN architectures, and derives the generalization guarantees through pseudo-dimension. However, no existing work theoretically studies the tuning of the graph partitioning (labeling) \textit{algorithm hyperparameter} in semi-supervised learning, or investigates data-dependent bounds on hyperparameter selection in deep semi-supervised learning algorithms through Rademacher Complexity. We note that in this work we focus on the statistical learning setting (i.e.\ the problem instances are drawn from a fixed, unknown distribution), but it would be an interesting direction to study online tuning of the hyperparameters using tools from prior work____.