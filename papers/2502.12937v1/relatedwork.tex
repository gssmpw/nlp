\section{Related Work}
\paragraph{Graph Based Semi-supervised Learning.} Semi-supervised Learning is a popular machine learning paradigm with significant theoretical interest~\citep{zhou2003learning,delalleau2005efficient,garg2020generaliz}. Classical algorithms focus on label-propagation based techniques, such as \citet{zhou2003learning}, \citet{zhu2003semi}, and many more. 
In recent years, graph neural networks (GNNs) have become increasingly popular in a wide range of application domains
~\citep{kipf2016semi,velivckovic2017graph,iscen2019label}.
A large number of different architectures have been proposed, including graph convolution networks, graph attention networks, message passing, and so on \citep{dwivedi2023benchmarking}. 
Both label propagation-based algorithms and neural network-based algorithms are useful in real life and perform  well with appropriate hyperparameter tuning. For example, although GNN-based algorithms are more predominant in certain applications, \cite{huang2020combininglabelpropagationsimple} show that modifications to label propagation-based algorithms can outperform GNN. For node classification in GNNs, many prior works study generalization guarantees for tuning network weights in GNNs ~\citep{oono2021optimizationgeneralizationanalysistransduction, esser2021learningtheorysometimesexplain, tang2023understandinggeneralizationgraphneural}. In contrast, we study the tuning of the \textit{hyperparameters} related to the GNN architecture. 

\paragraph{Hyperparameter Selection.} Hyper-parameters %, such as the weight for self-loops,
play an important role in the performance of both classical semi-supervised learning methods and GNNs. 
In general, hyperparameter tuning is performed on a validation dataset, and follows the same procedure: determine which hyperparameters to tune and then search within their domain for the combination of parameter values with best performance \citep{yu2020hyperparameteroptimizationreviewalgorithms}. Many methods have been proposed to efficiently search within the parameter space, such as grid search, random search \citep{JMLR:v13:bergstra12a}, and Bayesian optimization (\cite{Mockus1974bayesian}; \cite{Mockus1978application}; \cite{jones1998efficient}). A few existing works investigate the theoretical aspects of these methods, such as through generalization guarantees and complexities of the algorithms. In particular, a recent line of work~\citep{balcan2024provablytuningelasticnetinstances,balcan2023new} studies the tuning regularization hyperparameter in linear regression in statistical settings and provides generalization guarantees. For unsupervised learning, prior work \citep{balcan2019datadrivenclusteringparameterizedlloyds,balcan2020learning,balcan2024accelerating} has proposed several different parameterized algorithm families of clustering algorithms and studied the sample and computational complexity of learning the parameters. For semi-supervised learning, a recent line of work~\citep{balcan2021data,sharma2023efficiently} considers the problem of learning the best graph hyperparameter from a set of problem instances drawn from a data distribution. Another recent work \citep{balcan2025samplecomplexitydatadriventuning} investigates the kernel hyperparameters in GNN architectures, and derives the generalization guarantees through pseudo-dimension. However, no existing work theoretically studies the tuning of the graph partitioning (labeling) \textit{algorithm hyperparameter} in semi-supervised learning, or investigates data-dependent bounds on hyperparameter selection in deep semi-supervised learning algorithms through Rademacher Complexity. We note that in this work we focus on the statistical learning setting (i.e.\ the problem instances are drawn from a fixed, unknown distribution), but it would be an interesting direction to study online tuning of the hyperparameters using tools from prior work~\citep{balcan2018dispersion,sharma2020learning,sharma2025offline}.