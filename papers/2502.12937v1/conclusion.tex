\section{Conclusion}
We study the problem of hyperparameter tuning in graph-based semi-supervised learning for both classical label-propagation based techniques as well as modern deep learning algorithms. For the former, we obtain tight learning guarantees by bounding the pseudo-dimension of the relevant loss function classes. For the latter, we study a novel interpolation of convolutional and attention based graph neural network architectures and provide data-dependent bounds on the complexity of tuning the  hyperparameter the interpolates the two architectures. We obtain a sharper generalization error bound for tuning the hyperparameter in the simplified graph convolutional networks proposed in prior work. Our experiments indicate that we can achieve consistently good empirical performance across datasets by tuning the interpolation parameter.