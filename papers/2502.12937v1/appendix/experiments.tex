\section{Experiments}\label{appendix:experiments}
We  present here additional details related to our empirical studies.
\subsection{Label Propagation-based Method: Normalized Adjacency Matrix-Based Algorithmic Family}
We empirically validate our findings in \Cref{sec:label_prop}. For each of the eight datasets, the number of nodes per problem instance, $n$, is fixed at $30$.  We set the target generalization error to $\eps=0.1$,  and calculate the required number of problem instances as $m = O(\log n/\eps^2) \approx 300$. To evaluate performance, we randomly sample 300 graphs with 30 nodes each, tune the hyperparameter values to maximize accuracy on these graphs, and then test the selected hyperparameter on a separate set of 300 randomly sampled graphs. The results of evaluating the Normalized Adjacency Matrix-Based Algorithmic Family is presented in \Cref{tab:label_prob}, confirming that the observed generalization error is well within the scale of the target value $0.1$ (our bounds are somewhat conservative).

\begin{table*}[ht]
    \centering
    % \resizebox{\textwidth}{!}{%
    % \small
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
         & CIFAR10 & WikiCS & CORA & Citeseer & PubMed & AmazonPhotos & Actor\\  
     \hline
         Train Acc. & 0.9445 & 0.7522 & 0.7927 &0.7845  &   0.9993 & 0.9983 &0.9185\\
    \hline
         Test Acc. & 0.9397 & 0.7485 & 0.8010 & 0.7714  &     0.9993 & 0.9989 & 0.9239\\
    \hline
        Abs. Diff. & 0.0048&  0.0037  &  0.0083 & 0.0131 &   0.    & 0.0006 & 0.0054\\
    \hline
    \end{tabular}
    % }
    \caption{The Training Accuracy and Testing Accuracy of learning the hyperparameter $\delta$ in Normalized Adjacency Matrix Based Family ($\mathcal{F}_\delta$). The absolute difference between the accuracies (i.e. generalization error) is well within the scale of our target value $0.1$. }
    \label{tab:label_prob}
\end{table*}


\subsection{GCAN Experiments}

In this section, we empirically evaluate our proposed GCAN interpolation methods on nine standard benchmark datasets. Our goal is to see whether tuning $\eta$ gives better results than both GCN and GAT. 
The setup details of our experiment are described in \Cref{appendix:datasets}.
% In \Cref{appendix:datasets}, we also empirically study the number of training problem instances needed to learn a good parameter value over unseen test instances and the dependence of algorithmic performance on the hyperparameters in the algorithms introduced in \Cref{sec:label_prop}.
%
% Table: gcan with different eta

\begin{table*}[ht]
\small
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|*{13}
{>{\centering\arraybackslash}p{1.3cm}|}}
\hline
\textbf{Dataset} & $0.0$ & $0.1$ & $0.2$ & $0.3$ & $0.4$ & $0.5$ & $0.6$ & $0.7$ & $0.8$ & $0.9$ & $1.0$ &  {Rel. GCN} &  {Rel. GAT}\\
\hline
CIFAR10 & $0.7888 \pm 0.0010$ 
&$0.7908 \pm 0.0008$ 
&$0.7908 \pm 0.0015$ 
&$0.7907 \pm 0.0012$ 
&$0.7943 \pm 0.0022$ 
&$0.7918 \pm 0.0018$ 
&$0.7975 \pm 0.0017$ 
&$0.7971 \pm 0.0023$ 
&$0.7921 \pm 0.0023$ 
&$0.7986 \pm 0.0028$ 
&$\textbf{0.7984}  \boldsymbol{\pm} \textbf{0.0023}$ 
&  {$4.54\%$}
&  {$0\%$}\\
\hline
WikiCS &$0.9525 \pm 0.0007$ 
&$0.9516 \pm 0.0006$ 
&$0.9532 \pm 0.0011$ 
&$0.9545 \pm 0.0008$ 
&$0.9551 \pm 0.0015$ 
&$0.9545 \pm 0.0012$ 
&$0.9539 \pm 0.0012$ 
&$\textbf{0.9553} \boldsymbol{\pm} \textbf{0.0012}$ 
&$0.9530 \pm 0.0007$ 
&$0.9536 \pm 0.0009$ 
&$0.9539 \pm 0.0009$
&  {$5.89\%$}
&  {$3.04\%$}
\\
\hline
Cora & $0.6132 \pm 0.0218$ 
&$0.8703 \pm 0.0251$ 
&$0.8879 \pm 0.0206$ 
&$0.8396 \pm 0.0307$ 
&$0.8022 \pm 0.0385$ 
&$0.8615 \pm 0.0402$ 
&$ \textbf{0.9011} \boldsymbol{\pm} \textbf{0.0421}$ 
&$0.8088 \pm 0.0362$ 
&$0.8505 \pm 0.0240$ 
&$0.8549 \pm 0.0389$ 
&$0.8725 \pm 0.0334$ 
&  {74.43\%}
&  {22.43\%}
\\
\hline
Citeseer & $\textbf{0.7632} \boldsymbol{\pm} \textbf{0.0052}$ 
&$0.6944 \pm 0.0454$ 
&$0.7602 \pm 0.0566$ 
&$0.7500 \pm 0.0461$ 
&$0.7339 \pm 0.0520$ 
&$0.7427 \pm 0.0462$ 
&$0.7588 \pm 0.0504$ 
&$0.7193 \pm 0.0567$ 
&$0.7661 \pm 0.0482$ 
&$0.7266 \pm 0.0412$ 
&$0.7471 \pm 0.0444$ 
&  {0\%}
&  {6.37\%
}
\\
\hline
PubMed & $0.9350 \pm 0.0009$ 
&$0.9306 \pm 0.0006$ 
&$0.9356 \pm 0.0009$ 
&$0.9281 \pm 0.0007$ 
&$\textbf{0.9356} \boldsymbol{\pm} \textbf{0.0007}$ 
&$0.9319 \pm 0.0009$ 
&$0.9313 \pm 0.0007$ 
&$0.9288 \pm 0.0009$ 
&$0.9313 \pm 0.0006$ 
&$0.9338 \pm 0.0010$ 
&$0.9356 \pm 0.0009$ 
&  {0.92\%}
&  {0\%}
\\ \hline
CoauthorCS & $0.9733 \pm 0.0007$ 
&$0.9733 \pm 0.0008$ 
&$\textbf{0.9765} \boldsymbol{\pm} \textbf{0.0005}$ 
&$0.9744 \pm 0.0005$ 
&$0.9733 \pm 0.0009$ 
&$0.9690 \pm 0.0007$ 
&$0.9712 \pm 0.0009$ 
&$0.9722 \pm 0.0005$ 
&$0.9722 \pm 0.0011$ 
&$0.9722 \pm 0.0007$ 
&$0.9744 \pm 0.0007$ 
&  {$11.99\%$}
&  {$08.20\%$}
\\ \hline
AmazonPhotos & $0.9605 \pm 0.0022$ 
&$0.9617 \pm 0.0007$ 
&$0.9629 \pm 0.0015$ 
&$0.9599 \pm 0.0013$ 
&$0.9641 \pm 0.0017$ 
&$0.9574 \pm 0.0018$ 
&$0.9641 \pm 0.0019$ 
&$0.9592 \pm 0.0133$ 
&$\textbf{0.9653} \boldsymbol{\pm} \textbf{0.0027}$ 
&$0.9635 \pm 0.0031$ 
&$0.9562 \pm 0.0019$ 
&  {$12.15\%$}
&  {$20.77\%$}
\\ \hline
Actor & $0.5982 \pm 0.0016$ 
&$0.5919 \pm 0.0022$ 
&$\textbf{0.6005} \boldsymbol{\pm} \textbf{0.0039}$ 
&$0.5959 \pm 0.0039$ 
&$0.5965 \pm 0.0038$ 
&$0.5970 \pm 0.0027$ 
&$0.5976 \pm 0.0037$ 
&$0.5993 \pm 0.0043$ 
&$0.5930 \pm 0.0041$ 
&$0.5970 \pm 0.0037$ 
&$0.5953 \pm 0.0031$ 
& {0.57\%}
& {1.28\%}
\\ \hline
Cornell & $0.7341 \pm 0.0097$ 
&$0.7364 \pm 0.0165$ 
&$0.7364 \pm 0.0073$ 
&$0.7205 \pm 0.0154$ 
&$0.7523 \pm 0.0109$ 
&$0.7795 \pm 0.0120$ 
&$0.7568 \pm 0.0188$ 
&$0.7500 \pm 0.0140$ 
&$0.7477 \pm 0.0138$ 
&$0.7909 \pm 0.0136$ 
&$\textbf{0.8000} \boldsymbol{\pm} \textbf{0.0423}$ 
& {24.78\%}
& {0\%}
\\ \hline
Wisconsin & $0.8688 \pm 0.0077$ 
&$\textbf{0.8922} \boldsymbol{\pm} \textbf{0.0035}$ 
&$0.8688 \pm 0.0080$ 
&$0.8906 \pm 0.0049$ 
&$0.8797 \pm 0.0044$ 
&$0.8578 \pm 0.0120$ 
&$0.8875 \pm 0.0037$ 
&$0.8781 \pm 0.0082$ 
&$0.8563 \pm 0.0128$ 
&$0.8750 \pm 0.0121$ 
&$0.8719 \pm 0.0076$
&  {17.84\%}
&  {15.84\%}
\\ \hline
\end{tabular}
}
\caption{Results on the proposed GCAN interpolation. Each column corresponds to one $\eta$ value. Each row corresponds to one dataset. Each entry shows the accuracy and the interval. The accuracy with optimal $\eta$ value outperforms both pure GCN and pure GAT. The right two columns show the percentage of prediction error reduction relative to GCN and GAT.} 
\label{tab:gat_gcn_table}
\end{table*}

In Table \ref{tab:gat_gcn_table}, we show the mean accuracy across $30$ runs of each $\eta$ value and the $90\%$ confidence interval associated with each experiment. It is interesting to note that for various datasets we see varying optimal $\eta$ values for best performance. More often than not, the best model is interpolated between GCN and GAT, showing that we can achieve an improvement on both baselines simply by interpolating between the two. For example, GCN achieves the best accuracy among all interpolations in Citeseer, but in other datasets such as CIFAR 10 or Wisconsin, we see higher final accuracies when the $\eta$ parameter is closer to $1.0$ (more like GAT). The interpolation between the two points also does not increase or decrease monotonically for many of the datasets. The optimal $\eta$ value for each dataset can be any value between $0.0$ and $1.0$. This suggests that one should be able to learn the best $\eta$ parameter for each specific dataset. By learning the optimal $\eta$ value, we can outperform both GAT and GCN. 

% \section{Additional Experiment Details}\label{appendix:datasets}

\subsection{Experiment Setup for GCAN}\label{appendix:datasets}
We apply dropout with a probability of $0.4$ for all learnable parameters, apply 1 head of the specialized attention layer (with new update rule), and then an out attention layer. The activation we choose is eLU activation (following prior work \citep{velivckovic2017graph}), with 8 hidden units, and 3 attention heads. 
% We start training with an initial learning rate of $7 \times 10^{-5}$ and a weight decay of $5 \times 10^{-4}$.

These GCAN interpolation experiments are all run with only $20\%$ of the dataset being labeled datapoints, and the remaining $80\%$ representing the unlabeled datapoints that we test our classification accuracy on. \Cref{tab:gat_gcn_hyperparameter} notes the exact setup of each dataset, and the overall training time of each experiment.

\begin{table}[ht!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
Dataset     & Num of train nodes & learn rate & Epoch & Num of exp & Train time(sec) & Dim of hid. layers & Num of Attention Heads \\
\midrule
CiFAR10                        & 400                   & 7e-3          & 1000   & 30         & 13.5354               & 1                   & 3                 \\
WikiCS                        & 192                   & 7e-3          & 1000   & 30         & 6.4742               & 1                   & 3                 \\
Cora                        & 170                   & 7e-3          & 1000   & 30         & 7.4527               & 1                   & 3                 \\
Citeseer                        & 400                   & 7e-3          & 1000   & 30         & 6.4957               & 1                   & 3                 \\
Pubmed                        & 400                   & 7e-3          & 1000   & 30         & 13.1791               & 1                   & 3                 \\
CoAuthor CS       & 400                   & 0.01         & 1000   & 30         & 6.8015               & 1                   & 3                 \\

Amazon Photos       & 411                   & 0.01          & 400   & 30         & 11.0201              & 1                   & 3                \\
Actor       & 438                   & 0.01          & 1000   & 30         & 14.7753              & 1                   & 3                 \\
Cornell       & 10                   & 0.01          & 1000   & 30         & 6.9423              & 1                   & 3                 \\
Wisconsin       & 16                   & 0.01          & 1000   & 30         & 6.9271              & 1                   & 3                 \\
\end{tabular}%
}
\caption{Details of the datasets and experimental setup.}
\label{tab:gat_gcn_hyperparameter}
\end{table}

For datasets that are not inherently graph-structured (e.g., CIFAR-10), we first compute the Euclidean distance between the feature vectors of each pair of nodes. An edge is then added between two nodes if their distance is below a predefined threshold.