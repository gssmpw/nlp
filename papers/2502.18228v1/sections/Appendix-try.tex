
\section{All LLMs in our Experiments} \label{app:models}

We comprehensively evaluate nine LLMs, encompassing both API-based models and open-source models. The API-based models include the GPT series (GPT-4o, GPT-4o-mini, o1-mini) \citep{GPT-4, openai20254o,openai2025o1}, Claude-3.5 \citep{anthropic2025}, MiniMax (abab6.5s-chat) \citep{minimaxi2025}, Sensechat \citep{sensetime2025}, DeepSeek series (DeepSeek-R1 and DeepSeek-V3) \citep{deepseekai2025deepseekr1incentivizingreasoningcapability,deepseekai2024deepseekv3technicalreport} and Doubao \citep{doubao2025}. The open-source models include the Llama series (LlaMA-2-13B-Chat, LlaMA-3-8B-Instruct, LlaMA-3-70B-Instruct) \citep{LLaMA} and the Qwen-2.5 series (Qwen-2.5-7B, Qwen-2.5-14B and Qwen-2.5-72B) \citep{qwen2025qwen25technicalreport}. These models are run using vLLM~\citep{kwon2023efficient} on eight Nvidia A100 GPUs with the same random seed. For each model, the entire test set was processed in approximately one hour using parallel methods. All temperatures are set to 0 (Due to API-provider's closed-source non-deterministic implementation, small changes may still occur in the reproduction process). Specific model hyperparameters and version details can be found in Table~\ref{tab:model-hyperparams}. All models and tools (vLLM and LLaMa-Factory~\citep{zheng2024llamafactory}) used in this study, including closed-source API-based models, open-source models, were used in compliance with their respective licenses. What's more, the use of these generative models for dialogue tasks is well-established in the field and follows standard practices.

\begin{table*}[h!]
\centering
\caption{\textcolor{black}{Hyperparameters of Each Model.}}
\label{tab:model-hyperparams}
\textcolor{black}{
\resizebox{1\textwidth}{!}{%
\begin{tabular}{lll}
\hline
\textcolor{black}{\textbf{Model Name}} & \textcolor{black}{\textbf{Parameters}} & \textcolor{black}{\textbf{Comments}} \\ 
\hline
\textcolor{black}{Qwen-2.5-7B} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "qwen-2.5-7b-instruct"} \\
\textcolor{black}{Qwen-2.5-14B} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "qwen-2.5-14b-instruct"} \\
\textcolor{black}{Qwen-2.5-72B} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "qwen-2.5-72b-instruct"} \\
\textcolor{black}{GPT-4o} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "gpt-4o-2024-11-20"} \\ 
\textcolor{black}{GPT-4o Mini} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "gpt-4o-mini"} \\ 
\textcolor{black}{o1-Mini} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "o1-mini"} \\ 
\textcolor{black}{LLaMa-3-8B} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "llama-3-8b-instruct"} \\ 
\textcolor{black}{LLaMa-3-70B} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "llama-3-70b-instruct"} \\ 
\textcolor{black}{Doubao} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "Doubao-pro-4k"} \\ 
\textcolor{black}{Claude-3.5} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "claude-3-5-sonnet-20241022"} \\ 
\textcolor{black}{DeepSeek-V3} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "deepseek-chat"} \\ 
\textcolor{black}{DeepSeek-R1} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "deepseek-reasoner"} \\ 
\textcolor{black}{MiniMax} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "abab6.5s-chat"} \\ 
\textcolor{black}{SenseChat} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "SenseChat"} \\ 
\hline
\end{tabular}
}}
\end{table*}

\section{Prompts} 

\subsection{Basic Prompts for Role-playing Debtor and Creditor.} \label{app:prompts}

Figures~\ref{img:deb_prompt} and~\ref{img:cre_prompt} illustrate the prompts given to the large model to act as the debtor and the creditor, respectively. Originally in Chinese, these prompts have been appropriately simplified and automatically translated into English for display purposes (the full Chinese prompts is available to be disclosed later). Additionally, the instructions provided to human annotators were consistent with the prompts given to the model.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{latex/images/deb_prompt.pdf}  
  \caption{Prompt of Debtor.}
\vspace{-0.0in}
\label{img:deb_prompt}
\end{figure*}

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{latex/images/cre_prompt.pdf}  
  \caption{Prompt of Creditor (Debt Collector).}
\vspace{-0.0in}
\label{img:cre_prompt}
\end{figure*}




\subsection{Prompts for Planning Agent and Judging Agent.} \label{app:agent_promopt}

Figures~\ref{img:plan_prompt} and~\ref{img:judge_prompt} display the prompts for the planning agent and judging agent in the MADaN framework. Similarly, these prompts have been simplified and translated for ease of presentation. The prompt for the communicating agent remains unchanged, as previously shown.


\begin{figure*}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{latex/images/plan_prompt.pdf}  
  \caption{Prompt of Planning Agent.}
\vspace{-0.0in}
\label{img:plan_prompt}
\end{figure*}

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{latex/images/judge_prompt.pdf}  
  \caption{Prompt of Judging Agent.}
\vspace{-0.0in}
\label{img:judge_prompt}
\end{figure*}


\subsection{Defective prompt} \label{app:deprompts}

There are three main methods for generating Defective Prompts, as shown in Table~\ref{deprompt}. In practice, we first generate a list of prompts and then randomly select one from the list to generate the negative samples.

\begin{table*}[ht]
\centering
\caption{\label{deprompt}Defective Prompt Modifications for Debt Collection Negotiation.}
    \setlength{\tabcolsep}{3.5mm}{
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lll}
        \toprule
        \textbf{Modification Type} & \textbf{Description} & \textbf{Example} \\
        \midrule
        Deletion & Remove specific instructions & Removing "Offer a 10\% discount when the debtor shows clear financial difficulty." \\
        Replacement & Reverse guidance & Changing "Be cautious when the debtor makes a request" to "Approve requests without further consideration." \\
        Addition & Add negative guidance & Adding "If installment terms are discussed, set them to 24 months without negotiation." \\
        \bottomrule
        \end{tabular}
    }}
\end{table*}

% \textcolor{black}{GPT-4-turbo} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "GPT-4-turbo"} \\ 
% \textcolor{black}{GPT-3.5-turbo} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "gpt-3.5-turbo-0125"} \\ 
% \textcolor{black}{Qwen1.5-110B} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "qwen1.5-110b-chat"} \\ 
% \textcolor{black}{QwenMax} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "qwen-max"} \\ 
% \textcolor{black}{Claude-3-Opus} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{version = "claude-3-opus-20240229"} \\ 
% \textcolor{black}{LLaMA2-13B-Chat} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{model = "Llama-2-13b-chat"} \\ 
% \textcolor{black}{LLaMA3-70B-Instruct} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{model = "Llama-3-70B-Instruct"} \\ 
% \textcolor{black}{LLaMA3-8B-Instruct} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{model = "Llama-3-8B-Instruct"} \\ 
% \textcolor{black}{Qwen2-7B-Instruct} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024} & \textcolor{black}{model = "Qwen2-7B-Instruct"} \\ 
% \hline
% \textcolor{black}{LLaMA3-8B-Base-FT} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024, train\_batch\_size: 4,"finetuning\_type": lora, } & \textcolor{black}{model = "Llama-3-8B"} \\ 
% & "learning\_rate": 1.0e-4, "num\_train\_epochs": 10.0, "bf16": true & \\
% \textcolor{black}{LLaMA3-8B-Instruct-FT} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024,"train\_batch\_size": 4,"finetuning\_type": lora,} & \textcolor{black}{model = "Llama-3-8B-Instruct"} \\ 
% &  "learning\_rate": 1.0e-4, "num\_train\_epochs": 10.0, "bf16": true & \\
% \textcolor{black}{Qwen2-7B-Instruct-FT} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024,"train\_batch\_size": 4,"finetuning\_type": lora, } & \textcolor{black}{model = "Qwen2-7B-Instruct"} \\ 
% &  "learning\_rate": 1.0e-4, "num\_train\_epochs": 10.0, "bf16": true &\\
% \textcolor{black}{Qwen2-7B-Base-FT} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024,"train\_batch\_size": 4,"finetuning\_type": lora, } & \textcolor{black}{model = "Qwen2-7B"} \\ 
% &  "learning\_rate": 1.0e-4, "num\_train\_epochs": 10.0, "bf16": true & \\
% \hline


\section{The performance of different models as the debtor}\label{sec:model_deb}


In Section~\ref{sec:res}, we evaluate the debt collection outcomes when different models act as the creditor. We alse examine the performance of different models as debtors, using the Qwen-2.5-72B model exclusively as the creditor. We observed significant differences in the results when using different models for the debtor as shown in Table~\ref{img:reverseresult}. The SenseChat and Llama-3-70b models exhibited some inconsistencies, yielding excessively high DHI scores. During the examination of the dialogue process, we found that these models tended to neglect \textit{repeated statements} within the dialogue, leading to the inclusion of some irrelevant or ineffective content. Additionally, some models were more sensitive to the debtor’s prompt, likely due to the more complex nature of the debtor agent’s objectives. In contrast, the Qwen-2.5-72 model showed relatively balanced performance, suggesting that our choice was appropriate.

Since our focus is on studying the model’s performance as a debt collector, we did not design specific metrics for debtor models. Our primary aim is to use models capable of understanding the debtor’s objectives and engaging in dialogue for simulations prior to further manual testing.

\begin{table*}[ht]
\vspace{-0.1in}
    \centering
    \caption{\label{img:reverseresult}The performances of some models as Debtors.}
    \vspace{-0.1in}
    \setlength{\tabcolsep}{3.5mm}{
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccccccc}
        \toprule
        Model  & SR & RR & QRD & HRD & CD & L1D & L2D & ATV & CRI & DHI & CCI \\
        \midrule
        Qwen-2.5-72B  & 0.98 & 0.88 & 36.98 & 185.18 & 404.98 & 3.76 & 78.84 & 0.83 & 0.76 & 0.76 & 0.76\\
        llama-3-8b  & 1.00 & 0.94 & 29.13 & 134.13 & 296.25 & 3.25 & 80.31 & 0.91 & 0.83 & 0.71 & 0.81 \\
        llama-3-70b  & 1.00 & 0.92 & 10.33 & 150.33 & 369.33 & 0.33 & 51.33 & 0.85 & 0.83 & 0.97 & 0.85 \\
        gpt-4o-2024-11-20  & 1.00 & 0.94 & 35.26 & 146.86 & 312.66 & 3.50 & 85.72 & 0.86 & 0.82 & 0.73 & 0.80 \\
        o1-mini  & 0.98 & 0.93 & 26.76 & 111.96 & 240.56 & 4.92 & 92.34 & 0.93 & 0.85 & 0.58 & 0.78 \\
        deepseek-chat & 0.97 & 0.93 & 32.42 & 125.32 & 269.48 & 3.74 & 93.00 & 0.90 & 0.83 & 0.66 & 0.79 \\
        Doubao-pro-4k & 1.00 & 0.83 & 75.28 & 190.48 & 324.72 & 2.16 & 80.34 & 0.84 & 0.73 & 0.82 & 0.74 \\
        abab6.5s-chat & 0.90 & 0.92 & 58.53 & 204.53 & 484.53 & 8.63 & 76.50 & 0.89 & 0.70 & 0.52 & 0.66 \\
        SenseChat & 1.00 & 0.96 & 135.0 & 345.00 & 734.00 & 0.70 & 51.00 & 0.88 & 0.54 & 0.96 & 0.60 \\
        \bottomrule
    \end{tabular}%
    }}
 \label{tab:mainresults}
     \vspace{-10pt}
\end{table*}

% \begin{table*}[ht]
%     \centering
%     \caption{\label{img:mainresult_detial}The performances of some models}
%     \vspace{-0.1in}
%     \setlength{\tabcolsep}{3.5mm}{
%     \resizebox{\textwidth}{!}{%
%     \begin{tabular}{lccccccccc}
%         \toprule
%          & \multicolumn{2}{c}{\textbf{Debt Recovery}} & \multicolumn{3}{c}{\textbf{Collection Efficiency}} & \multicolumn{3}{c}{\textbf{Debtor’s Financial Health}} \\
%         \cmidrule(lr){2-3} \cmidrule(lr){4-6} \cmidrule(lr){7-9}
%         \textbf{Model} & \textbf{SR(\%)} & \textbf{RR} & \textbf{QRD} & \textbf{HRD} & \textbf{CD} & \textbf{L1D}& \textbf{L2D}& \textbf{ATV} \\
%         \midrule
%         Vanilla & 90 & 84.00 & 34.35 & 178.35 & 397.85 & \textbf{4.10} & 73.50 & \textbf{0.83}\\
%         + Planning& 80 & 94.38 & 24.35 & 142.85 & 398.35 & 11.80 & \textbf{72.75} & 0.96\\
%         + Judging & 95 & \textbf{97.62} & 19.60 & 124.60 & 327.60 & 6.15 & 76.75 & 0.86\\
%         Ours  & \textbf{95} & 97.40 & \textbf{18.00} & \textbf{124.50} & \textbf{300.5} & 4.45 & 79.85 & 0.86\\
%         \bottomrule
%     \end{tabular}%
%  }}
%  \label{tab:mainresults}
%      % \vspace{-10pt}
% \end{table*}


\section{Settings of Post-training}

All post-training experiments were conducted on an 8-GPU A100 server using the LLaMa-Factory framework~\citep{zheng2024llamafactory}. The training time per session was around five minutes. The specific parameter settings for each group are provided in Table~\ref{tab:model-hyperparams-post}. The four sets of training data will be made publicly available at a later stage.



\begin{table*}[h!]
\centering
\caption{\textcolor{black}{Hyperparameters of Each Post-trained Model.}}
\label{tab:model-hyperparams-post}
\textcolor{black}{
\resizebox{1\textwidth}{!}{%
\begin{tabular}{lll}
\hline
\textcolor{black}{\textbf{Model Name}} & \textcolor{black}{\textbf{Parameters}} & \textcolor{black}{\textbf{Comments}} \\ 
\hline
\textcolor{black}{SFT-DG} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024, train\_batch\_size: 4,"finetuning\_type": lora, } & \textcolor{black}{model = "qwen-2.5-7b-instruct"} \\ 
& "learning\_rate": 5.0e-6, "num\_train\_epochs": 5.0, "bf16": true & \\
\textcolor{black}{SFT-MAG} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024,"train\_batch\_size": 4,"finetuning\_type": lora,} & \textcolor{black}{model = "qwen-2.5-7b-instruct"} \\ 
&  "learning\_rate": 5.0e-6, "num\_train\_epochs": 5.0, "bf16": true & \\
\textcolor{black}{DPO-DG} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024,"train\_batch\_size": 4,"finetuning\_type": lora, } & \textcolor{black}{model = "qwen-2.5-7b-instruct"} \\ 
&  "learning\_rate": 5.0e-6, "num\_train\_epochs": 5.0, "bf16": true &\\
\textcolor{black}{DPO-MAG} & \textcolor{black}{"temperature": 0, "max\_tokens": 1024,"train\_batch\_size": 4,"finetuning\_type": lora, } & \textcolor{black}{model = "qwen-2.5-7b-instruct"} \\ 
&  "learning\_rate": 5.0e-6, "num\_train\_epochs": 5.0, "bf16": true & \\
\hline
\end{tabular}
}}
\end{table*}


\section{Supplementary Information}
This paper utilized AI tools including Google Translate for assisted translation when presenting prompts and examples, and employed the use of a Cursor for coding to enhance efficiency. No potential risks were involved in the course of this study.