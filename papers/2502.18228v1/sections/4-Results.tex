\section{Experiments and Results} \label{sec:res}


\begin{table*}[ht]
\vspace{-0.1in}
    \centering
    \caption{\label{img:mainresult}The performances of some models as debt collectors (~\textsuperscript{*} denotes the second-best performance).}
    \vspace{-0.1in}
    \setlength{\tabcolsep}{3.5mm}{
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|cc|cc|ccc|ccc|cccc}
        \toprule
         & \multicolumn{2}{c}{\textbf{Conversation}} & \multicolumn{2}{c}{\textbf{Debt Recovery}} & \multicolumn{3}{c}{\textbf{Collection Efficiency}} & \multicolumn{3}{c}{\textbf{Debtor’s Financial Health}} & \multicolumn{3}{c}{\textbf{Average Metrics}}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} 
        \textbf{Model} & \textbf{DC}$ \uparrow $ & \textbf{DS}$ \uparrow $ & \textbf{SR}$ \uparrow $ & \textbf{RR(\%)}$ \uparrow $ & \textbf{QRD}$ \downarrow $ & \textbf{HRD}$ \downarrow $ & \textbf{CD}$ \downarrow $ & \textbf{L1D}$ \downarrow $ & \textbf{L2D}$ \downarrow $& \textbf{ATV}$ \downarrow $ & \textbf{CRI}$ \uparrow $ & \textbf{DHI}$ \uparrow $ & \textbf{CCI}$ \uparrow $\\
        \midrule
        Qwen-2.5-7B  & 0.94 & 4.57 & 0.98 & 87.15 & 46.04 & 214.04 & 436.84 & \textbf{2.82} & 79.46 & \textbf{0.84\textsuperscript{*}} & 0.732 & \textbf{0.793} & 0.743\\
        Qwen-2.5-14B & 0.94 & 4.60 & 0.96 & 89.62 & 28.60 & 154.60 & 358.80 & 6.30 & 79.82 & 0.88 & 0.793 & 0.613 & 0.749\\
        Qwen-2.5-72B & 0.96 & 4.75 & 0.98 & 88.50 & 36.98 & 185.18 & 404.98 & 3.76 & \textbf{78.84\textsuperscript{*}} & \textbf{0.83} & 0.764 & \textbf{0.767\textsuperscript{*}} & 0.764\\
        LLaMa-3-8B  & 0.91 & 3.64 & \textbf{1.00} & 89.01 & 51.36 & 184.02 & 399.02 & 3.38 & 88.02 & 0.87 & 0.756 & 0.713 & 0.747\\
        LLaMa-3-70B & 0.87 & 3.94 & 0.98 & 92.24 & 36.72 & 157.32 & 371.12 & 4.50 & 79.56 & 0.87 & 0.792 & 0.695 & 0.771\\
        GPT-4o & \textbf{1.00} & 4.65 & \textbf{1.00} & 95.76 & 27.00 & \textbf{128.40\textsuperscript{*}} & \textbf{297.20\textsuperscript{*}} & 6.18 & 85.18 & 0.90 & 0.844 & 0.580 & 0.774\\
        GPT-4o-mini & 0.99 & 4.61 & 0.96 & \textbf{96.32\textsuperscript{*}} & 31.60 & 131.20 & 312.00 & 6.30 & 84.08 & 0.89 & 0.836 & 0.589 & 0.771\\
        o1-mini & \textbf{1.00} & 4.68 & 0.94 & 94.61 & 29.52 & 140.52 & 352.52 & 5.58 & 83.80 & 0.89 & 0.807 & 0.619 & 0.760\\
        Doubao-pro & 0.98 & \textbf{4.91\textsuperscript{*}} & 0.96 & 93.11 & \textbf{21.22\textsuperscript{*}} & 143.02 & 365.22 & 5.98 & 83.68 & 0.89 & 0.814 & 0.603 & 0.760\\
        Claude-3.5 & \textbf{1.00} & 4.59 & 0.98 & 93.30 & 34.92 & 140.52 & 312.32 & \textbf{3.32\textsuperscript{*}} & 87.30 & 0.89 & 0.816 & 0.698 & \textbf{0.789\textsuperscript{*}}\\
        MiniMax & \textbf{1.00} & 4.75 & 0.96 & 92.77 & 38.66 & 167.66 & 401.26 & 7.12 & \textbf{76.44} & 0.88 & 0.776 & 0.591 & 0.730\\
        SenseChat & \textbf{1.00} & 4.70 & 0.98 & 89.28 & 34.56 & 155.76 & 354.56 & 5.24 & 81.14 & 0.87 & 0.791 & 0.661 & 0.761\\
        Deepseek-V3 & \textbf{1.00} & 4.85 & 0.99 & 91.65 & 28.40 & 141.20 & 313.60 & 5.42 & 83.82 & 0.89 & \textbf{0.818\textsuperscript{*}} & 0.625 & 0.771\\
        Deepseek-R1 & 0.98 & 4.81 & 0.98 & 93.10 & 37.72 & 146.32 & 348.12 & 5.68 & 83.94 & 0.88 & 0.802 & 0.624 & 0.759\\
        \midrule
        Human & \textbf{1.00} & \textbf{4.93} & \textbf{1.00} & \textbf{98.50} & \textbf{16.73} & \textbf{119.49} & \textbf{260.90} & 3.81 & 78.49 & 0.86 & \textbf{0.870} & 0.736 & \textbf{0.840}\\
        \bottomrule
    \end{tabular}%
 }}
 \label{tab:mainresults}
     \vspace{-10pt}
\end{table*}


In this section, we report the implementation details and the benchmark performances of several well-known LLMs in the DCN task on our dataset. 

\subsection{Implementation Details}

 % These models are run using vLLM on eight Nvidia A100 GPUs with the same random seed. All temperatures are set to 0. Specific model hyperparameters and version details can be found in Table~\ref{tab:model-hyperparams}.

For \textit{open-source models}, such as Qwen series, we use their respective \textbf{chat} versions. For \textit{api-based models}, we aim to select the latest and most advanced versions available, the \textit{inference models} such as o1-mini \citep{openai2025o1} and DeepSeek-R1 \citep{deepseekai2025deepseekr1incentivizingreasoningcapability} are also included. The list of LLMs used is provided in Appendix~\ref{app:models}. The human baseline was derived from the average results of benchmark tasks completed by two finance professionals with relevant backgrounds.

Since our task focuses on Chinese, we chose one of the best open-source models currently available in the Chinese language domain: Qwen2.5-70B model to represent the \textbf{debtor}, while using different models for the \textbf{creditor} in order to compare their performance. The results of different models as the debtor are also presented in Appendix~\ref{sec:model_deb}. 
% Additionally, as an open-source model, it allows us to deploy it directly on our servers, thus reducing costs.

% We used the model itself as the agent without incorporating any additional modules, such as memory or backtracking. 
For both sides, we employed the Chain of Thought (CoT) approach \citep{wei2023chainofthoughtpromptingelicitsreasoning}, providing the model with instructions for the DCN task and a specified format for dialogue generation, which consisted of \textit{“Thought”}, \textit{“Dialogue”} and \textit{“Action” }in each interaction. The prompts are detailed in Appendix~\ref{app:prompts}.

\subsection{Benchmark Results}



% \begin{table*}[htbp]
% \centering
% \caption{\label{img:mainresult}The performances of some models}
% \begin{tabular}{@{}lccccccc@{}}
% \toprule
% models           & Succ  & Pred   & L1    & L2    & Var.   & 25\% Rec & 50\% Rec \\ \midrule
% Qwen2.5-7B-Instruct         & 0.98  & 14.76  & 2.82  & 79.46 & 0.84   & 46.04    & 214.04   \\
% Qwen2.5-72B-Instruct       & 0.96  & 11.70  & 6.30  & 79.82 & 0.89   & 28.60    & 154.60   \\
% Qwen2.5-14B-Instruct        & 0.96  & 13.26  & 9.82  & 77.58 & 0.93   & 33.56    & 187.06   \\
% bailing-80b-16k     & 0.98  & 20.10  & 3.38  & 71.70 & 0.82   & 87.16    & 269.56   \\ 
% \bottomrule
% \end{tabular}
% \end{table*}


The comparison of performance across different models is clearly illustrated in Table~\ref{img:mainresult}.

\textbf{LLMs perform well in terms of basic interaction format and overall dialogue capabilities.} From the perspectives of dialogue completeness (DC) and soundness (DS), we find that the models effectively cover all negotiation objectives. The dialogue content is generally reasonable, aligns with the set objectives, and shows little difference from the human baseline. Specifically, the Chinese-based model outperforms the English-based model in terms of dialogue soundness for our task.

\textbf{However, from the perspective of the negotiation outcomes, the performance of the LLMs was subpar and did not align well with requirements.} Observing the Comprehensive Collection Index (CCI), we found that the model’s overall evaluation result deviates from human-level performance by more than 0.05. This discrepancy might stem from the fact that the negotiation outcomes are numerical, making it challenging to align numerical-related requirements through prompt-based methods. 

\textbf{Most models tend to offer more generous concessions to debtors, both in repayment ratios and deadlines.} These concessions are crucial because they directly affect the financial company’s asset losses, a point emphasized in the prompt. However, as shown in the table, all models except for the GPT series have repayment ratios below 95\%, meaning they did not fully follow the prompt’s guidelines. In addition, the large models show lower collection efficiency compared to human benchmarks. For example, the time taken to recover 25\% of the debt is 2-3 times longer than the human baseline. This suggests the models give debtors more time to repay, rather than encouraging earlier repayment. Some models, like GPT-4o, come close to human-level efficiency, but this is at the cost of worsening the debtor’s financial situation. The average minimum repayment days for these models are twice as long as the human level, showing that they \textit{struggle to adapt} to the debtor’s real circumstances. This could be due to the models \textit{misjudging the debtor’s financial situation or choosing easier solutions to reach an agreement}.

\textbf{The collection results achieved by the model do not hold the debtor’s financial health, despite providing considerable room in terms of recovery and efficiency. }We found that, with the exception of the Qwen-2.5 model, the Debt Health Index (DHI) for all other models was below the human-level threshold. Considering the concessions offered to the debtor during the collection process, these results suggest that the model did not provide \textit{targeted debt resolution solutions} during the negotiation process.

\textbf{Non-inference models may be more suitable for this task compared to inference models.} By comparing the performance of the inference models o1-mini and Deepseek-R1 with their non-inference counterparts, gpt-4o-mini and Deepseek-V3, we observed a notable decline in the performance of the inference models across multiple metrics, particularly in collection efficiency.