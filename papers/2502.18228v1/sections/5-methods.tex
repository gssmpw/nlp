\begin{figure*}[t]
\vspace{-0.35in}
  \centering
  \includegraphics[width=0.87\textwidth]{latex/images/framework.pdf}  
\vspace{-0.1in}
  \caption{MADeN Framework overview.}
\vspace{-0.1in}
\label{img:framwork}
\end{figure*}

\section{Method} \label{sec:method}


\subsection{A Multi-agent framework for DCN}\label{sec:framework}

To balance the model’s attention between debt recovery quality and the debtor’s financial health, and to avoid decisions that may harm the creditor’s interests in order to reach an agreement, we propose a method to enhance the decision alignment for DCN. Inspired by recent advancements in LLM-as-a-judge frameworks \citep{NEURIPS2023_91f18a12} and LLM planning methodologies \citep{Kannan2023SMARTLLMSM}, we designed the framework illustrated in Figure~\ref{img:framwork}. The subsequent sections provide a detailed explanation of each module (agent) within this framework.

% Due to the inherent tendency of large language models (LLMs) to aim for agreement, these agents often accommodate a debtor’s demands regardless of their reasonableness, leading to decisions that deviate from the factual context. 

\textbf{Planning Agent.} The planning agent is activated after the debtor shares their financial difficulties, following the initial stage of the conversation. This agent is responsible for classification and strategy formulation. We categorize debtors into four distinct groups, each corresponding to different negotiation strategies and outcome spaces. This approach ensures that the model follows a consistent framework throughout the negotiation, avoiding deviations from the core objective. 

\textbf{Judging Agent.} The judging agent evaluates the debtor’s decision after each round, following the initial stage. After the communicating agent provides content, the judging agent performs an internal evaluation, and then the communicating agent adjusts and delivers the revised content to the debtor. It is set to be completely neutral and does not need to align with both sides.

By combining these two agents with the original communicating agent, we obtain a debtor \textbf{M}ulti-\textbf{A}gent \textbf{De}bt \textbf{N}egotiation system \textbf{(MADeN)} capable of self-planning and self-adjustment. Prompts for the agents can be found in Appendix~\ref{app:agent_promopt}.
% Through the seamless collaboration of the communicating, planning, and judging agents, we have developed a comprehensive framework.

\subsection{Experiment Results of MADeN}

We use Qwen2.5-70B as the baseline model (Vanilla) to test the effectiveness of MADeN. We also conducted ablation experiments to separately evaluate the effectiveness of the two modules. 

As the results shown in Figure~\ref{img:ablaresult}, our multi-agent framework performs well. Compared to the vanilla group, it significantly improves debt recovery and efficiency while ensuring the debtor’s economic health (The CRI has increased by more than \textbf{0.1}, while the DHI remains above \textbf{0.7}). Meanwhile, using only one of the modules does not achieve similar results, indicating the effectiveness of our two-agent design.
% As the experiment results shown in Table, 



\begin{table}[ht]
\centering
\vspace{-0.1in}
\caption{\label{img:ablaresult}The performances of our framework}

\setlength{\tabcolsep}{3.5mm}{
\resizebox{0.36\textwidth}{!}{%
\begin{tabular}{lccc}
    \toprule
    \textbf{Model} & \textbf{CRI} & \textbf{DHI} & \textbf{CCI} \\
    \midrule
    Vanilla & 0.740 & \textbf{0.771} & 0.746 \\
    + Planning & 0.766 & 0.335 & 0.610 \\
    + Judging & 0.840 & 0.648 & 0.793 \\
    MADeN & \textbf{0.847} & 0.706 & \textbf{0.814} \\
    \bottomrule
\end{tabular}%
}}
\vspace{-0.1in}
\end{table}

\subsection{Post-training with Rejection Sampling} \label{sec:post}

To enhance the model’s direct performance through post-training, we explored two approaches: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO)~\citep{rafailov2024directpreferenceoptimizationlanguage}. We use the Qwen-2.5-70B model to generate two types of data for later sampling: (1) Directly Generated Data \textbf{(DG Data)}: Data directly generated by the model. (2) Multi-agent Generated Data \textbf{(MAG Data)}: Data produced through the Multi-agent framework, with content from the planning and judging agents discarded during processing.

% Initial data was generated using the framework from Section~\ref{sec:framework}, followed by rejection sampling to obtain high-quality data for SFT training. This data was then used as positive samples, combined with negative samples generated by deleting parts of the prompt and using the Qwen-2.5-7B model. These samples formed the training data for DPO.
 
\begin{figure*}[htbp]
\vspace{-0.1in}
  \centering
  \includegraphics[width=1\textwidth]{latex/images/golddatagen.pdf}  
  
  \caption{Reject Sampling Process. \textbf{D.Agent} and \textbf{C.Agent} represent the debtor agent and creditor agent. Creditor agent can be designed in two forms, depending on the use of the MADeN framework (DG and MAG).}
\vspace{-0.1in}
\label{img:sampling}
\end{figure*}

\begin{table*}[ht]
% \vspace{-0.1in}
    \centering
    \caption{\label{img:ftresult}Model performances on the test set under different handling methods}
    % \vspace{-0.1in}
    \setlength{\tabcolsep}{3.5mm}{
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|cc|ccc|ccc|cccc}
        \toprule
         & \multicolumn{2}{c}{\textbf{Debt Recovery}} & \multicolumn{3}{c}{\textbf{Collection Efficiency}} & \multicolumn{3}{c}{\textbf{Debtor’s Financial Health}} & \multicolumn{3}{c}{\textbf{Average Metrics}}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-12} 
        \textbf{Model} & \textbf{SR}$ \uparrow $ & \textbf{RR(\%)}$ \uparrow $ & \textbf{QRD}$ \downarrow $ & \textbf{HRD}$ \downarrow $ & \textbf{CD}$ \downarrow $ & \textbf{L1D}$ \downarrow $ & \textbf{L2D}$ \downarrow $ & \textbf{ATV}$ \downarrow $ & \textbf{CRI}$ \uparrow $ & \textbf{DHI}$ \uparrow $ & \textbf{CCI}$ \uparrow $\\
        \midrule
        Vanilla & 0.98 & 87.15 & 46.04 & 214.04 & 436.84 & \textbf{2.82} & 79.46 & \textbf{0.84} & 0.731 & \textbf{0.793} & 0.737 \\
        MADeN & 0.96 & \textbf{95.21} & \textbf{22.26} & \textbf{136.26} & \textbf{329.06} & 6.72 & 86.24 & 0.93 & \textbf{0.828} & 0.525 & \textbf{0.783} \\
        \midrule
        SFT-DG & 0.96 & 88.25 & 33.56 & 187.06 & 396.76 & 9.82 & \textbf{77.58} & 0.93 & 0.763 & 0.429 & 0.708 \\
        SFT-MAG & 0.94 & 88.02 & 29.32 & 157.72 & 370.92 & 6.10 & 81.08 & 0.91 & 0.779 & 0.587 & 0.755 \\
        DPO-DG & 0.98 & 88.90 & 29.84 & 159.44 & 386.44 & 6.03 & 79.04 & 0.88 & 0.787 & 0.623 & 0.766 \\
        DPO-MAG & \textbf{1.00} & 89.05 & 24.38 & 170.78 & 421.78 & 5.62 & 79.23 & 0.89 & 0.787 & 0.632 & 0.768 \\
        \bottomrule
    \end{tabular}%
 }}
 \label{tab:mainresults}
     \vspace{-10pt}
\end{table*}


\textbf{Reject Sampling Process. }The sampling processes are shown in Figure ~\ref{img:sampling}. For each debtor’s data in \textit{training set}, we need to generate multiple candidate dialogues by employing different prompting styles (e.g., strict, gentle). These dialogues were subsequently transformed into multiple question-answer pairs. After filtering and screening the data, a ranking was constructed based on predefined metrics. After filtering out data with incomplete negotiation content and poor performance on certain metrics (Filter 1), we rank the remaining data based on CCI and select the best set for the candidate pool. Then, we sort the CCI of the candidate pool and choose the top 60\% as our final dataset (Filter 2). 

\textbf{Negative outputs generation for DPO. }For the negative samples of DPO, we used the input data obtained earlier, while replacing the prompt with a defective prompt. Its prompt construction method is detailed in Appendix~\ref{app:deprompts}, and the samples were generated using Qwen-2.5-7B to increase the gap with the positive output. Finally, each of the four datasets consists of 437 pairs.

\textbf{Training Setup.} Due to memory constraints, we use Qwen-2.5-7B for the experiments. We obtain four models using different methods and data types. 
% Specific parameters are provided in Appendix 2.

\subsection{Performance Comparison: Post-training vs. Multi-agent Method}

We use the four trained models to conduct DCN and compared its performance with the previous multi-agent framework based on the same original model. The results of the metrics related to negotiation decision outcomes are shown in Table~\ref{img:ftresult}.

\textbf{MAG data is more effective than DG data. }The two training sets yielded significantly different results, with a 5\% improvement in CCI during SFT, indicating that the multi-agent approach generates data with higher quality.

\textbf{DPO outperforms the SFT method in DCN task.} Using two different types of data, DPO always outperforms SFT in most metrics. Although fine-tuning with DG data results in slightly worse performance than the original model, DPO shows a significant improvement. It suggests that even with low-quality positive samples, the constructed negative samples can still help align the model towards the target, significantly enhancing performance.

\textbf{Both the post-training method and the multi-agent approach significantly improve model performance}, with the latter showing a slight edge. The multi-agent method shows better generalization, even the best-performing model (DPO-MAG), its CCI is still slightly lower than the Multi-agent method, but they perform similarly across many metrics, with it outperforming in the Debtor Health Index (DHI exceeds over \textbf{0.1}). This suggests that a well-designed post-training method can achieve results similar to the multi-agent approach.
