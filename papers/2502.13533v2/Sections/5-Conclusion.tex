\section{Conclusion}
% \textcolor{custommagenta}{
We propose \method, a memory-efficient LoRA training scheme for large language models. \method significantly reduces the count of parameters of the original model by 16.95$\times$, while maintaining the performance of large-scale LLM fine-tuning.
We identify several open questions for \method, including the potential for reduced inference costs through context-aware computational graph recovery and its applicability to models like vision transformers~\citep{DosovitskiyB0WZ21} and diffusion models~\citep{HoJA20}. We hope our work inspires further research on memory-efficient LoRA training from a sparsity perspective and believe \method will serve as a valuable tool for the community, enabling LoRA training of large-scale models on consumer-grade hardware.
% }