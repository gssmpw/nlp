\section{Experiments}
%%TODO
% 1.LLaMA-3.1 70b损失正常和下游都正常了，我下午会把实验部分重新整理一遍
% RQ
% We demonstrate that \method can significantly reduce parameter count the training of large language models while acquiring performance gain.
% Specifically, we show that:
% \begin{itemize}[leftmargin=20pt]
%     \item[(1)] 

%     \item[(2)]

%     \item[(3)] 
% \end{itemize}
\subsection{Setup}
\paragraph{Pre-train Corpus.}
%breif version
To align the inconsistent knowledge between the pruned model during training and the original model during inference, we apply \method to continual pre-training 
LLMs
% in a teacher-forcing manner~\citep{bachmann2024teacherforce}
on a mixed corpus of FineWeb~\citep{penedo2024fineweb} and OpenWebMath~\citep{paster2023openwebmath}.
Notably, this alignment process is a one-time, offline operation that can be executed by model publishers.

%full version
% To align the inconsistent knowledge between the pruned model during training and the original model during inference, we apply \method to continual pre-training 
% LLMs in a teacher-forcing manner~\citep{bachmann2024teacherforce}
% on a mixed corpus of FineWeb~\citep{penedo2024fineweb} and OpenWebMath~\citep{paster2023openwebmath}.
% FineWeb, containing over 15TB of cleaned and deduplicated English web data from Common Crawl. 
% OpenWebMath, extracted from over 200 billion HTML files on Common Crawl, provides high-quality mathematical text. Mixing these datasets enhances the pruned model's capabilities in both general and mathematical domains.

% Unless specified otherwise, we randomly sample 102,400 instances from both FineWeb and OpenWebMath to construct a mixed dataset with a sequence length of 512, yielding approximately 105 million tokens. The default training batch size is 128, allowing up to 1,600 update steps. We train without data repetition over a sufficiently large corpus to simulate a realistic pre-training scenario. 
% Notably, this alignment process is a one-time, offline operation that model publishers can execute.

\paragraph{Fine-tuning Data.}
%breif version
Following the fine-tuning setup of LoRA~\citep{Edw:2022lora}, we primarily conduct supervised fine-tuning (SFT) on the OpenHermes-2.5~\citep{OpenHermes} (referred to as OpenHermes) and OpenOrca~\citep{OpenOrca} datasets. To effectively assess the overall fine-tuning performance, we evaluate test perplexity not only on in-domain test sets constructed from the instruction fine-tuning data but also on out-of-domain test sets built from Alpaca~\citep{alpaca}.

%full version
% Following the fine-tuning scenario of LoRA~\citep{Edw:2022lora}, we primarily conduct supervised fine-tuning (SFT) on the OpenHermes-2.5~\citep{OpenHermes} (referred to as OpenHermes). OpenHermes is a large-scale dataset constructed from synthetically generated instructions and chat samples, encompassing diverse sources such as Airoboros 2.2~\citep{wang2023selfinstructaligning}, CamelAI Domain Expert Dataset~\citep{li2023camel}, ChatBot Arena (GPT-4 Only)~\citep{zheng2023lmsyschat1m}, and more.
% To further demonstrate the general effectiveness of the \method alignment process, we also evaluate \method on the OpenOrca~\citep{OpenOrca} dataset. OpenOrca is a widely used instruction fine-tuning dataset where each data instance represents entries from the FLAN collection~\citep{longpre2023flan}, augmented by submitting the listed questions to either GPT-4 or GPT-3.5.

% By default, we train SFT on the instruction dataset with a batch size of 128 and a sequence length of 512 for 400 steps, totaling approximately 26.2 million tokens. 
% To effectively evaluate the overall fine-tuning performance, we assess the perplexity of the fine-tuned model on an out-of-domain test set. This out-of-domain test set is constructed by randomly sampling 2,000 instances from the Alpaca~\citep{alpaca} test set, truncated to a sequence length of 512.

\paragraph{Downstream Task.}
%breif version
We focus on the performance of \method in various downstream tasks, including MathQA~\citep{amini-etal-2019-mathqa} and GSM8K~\citep{cobbe2021gsm8k} in mathematical reasoning, six tasks—Arc Challenge \& Easy~\citep{clark2018arc}, HellaSwag~\citep{zellers-etal-2019-hellaswag}, OpenBookQA~\citep{OpenBookQA2018}, PIQA~\citep{Bisk2020}, and WinoGrande~\citep{WinoGrande2021} in common sense reasoning, and HumanEval~\citep{chen2021evaluating} in code generation.

%full version
% We focus on the performance of \method in various downstream tasks, including mathematical reasoning, common sense reasoning, and code generation.

% For mathematical reasoning, we benchmark the accuracy of baseline models using greedy decoding on MathQA~\citep{amini-etal-2019-mathqa} with a 1-shot setting and GSM8K (Grade School Math 8K)~\citep{cobbe2021gsm8k} with 8-shots, Chain of Thought (CoT) prompting and strict match\footnote{
% % The evaluation script 
% Follow~\url{https://github.com/EleutherAI/lm-evaluation-harness} (MIT License).}.
% MathQA is a large-scale dataset comprising 37k English multiple-choice math word problems, covering diverse math domains. It extends the AQuA-RAT dataset~\citep{ling2017program} by annotating problems with fully specified operational programs using a new representation language, building on the questions, options, rationale, and correct answers provided by AQuA-RAT.
% The GSM8K is a dataset of 8.5K high-quality, linguistically diverse grade school math word problems, designed to evaluate multi-step reasoning in basic arithmetic operations (+-×÷). We conduct evaluations on its 1.3K test set to assess logical and mathematical reasoning in language models.

% For commonsense reasoning (CSR), we report the average accuracy across six tasks—Arc Challenge \& Easy~\citep{clark2018arc}, HellaSwag~\citep{zellers-etal-2019-hellaswag}, OpenBookQA~\citep{OpenBookQA2018}, PIQA~\citep{Bisk2020}, and WinoGrande~\citep{WinoGrande2021}—under 1-shot and greedy decoding settings. These benchmarks comprehensively assess the model’s ability to apply ``commonsense" or world knowledge for reasoning, rather than relying on pattern recognition.

% For code generation, we compare two pass rates, \textsc{Pass@1} and \textsc{Pass@10}~\citep{kulal2019spoc}, on HumanEval~\citep{chen2021evaluating} of each baseline in a zero-shot setting with sampling parameters of  $\textsc{temperature}=0.6$, and $\textsc{top}_\textsc{p} =0.95$\footnote{
% % The evaluation script 
% Follow~\url{https://github.com/abacaj/code-eval} (MIT License).}.
% The HumanEval dataset released by OpenAI consists of 164 handwritten Python programming problems, each with a function signature, docstring, body, and unit tests. Serving as a benchmark, HumanEval assesses models on a range of Python coding skills, from basic syntax to complex problem-solving, offering insights into their programming capabilities alongside language-focused tasks.


\paragraph{Sparsification \& Quantization.}
%breif version
% \method incorporates two model compression techniques: sparsification, which generates a pruned model for low-rank matrix updates, and quantization, which forms \Qmethod further to reduce the memory footprint of the pruned model.
For sparsification $\mathtt{P}(\cdot)$, we first establish a variant \methodrand randomly structured pruning and adapt \method to another three variants based on leading approaches: \methodstru with the structured pruning LLM-Pruner\footnote{\url{https://github.com/horseee/LLM-Pruner} (Apache-2.0 license)}~\citep{ma2023llmpruner} and \methodsemi and \methodunst with the non-structured (semi-structured \& unstructured) pruning SparseGPT\footnote{\url{https://github.com/IST-DASLab/sparsegpt} (Apache-2.0 license)}~\citep{FrantarA23spasegpt}.
For quantization $\mathtt{Q}(\cdot)$, we achieve \Qmethod by combining \method with the LoRA-tailored quantization algorithm QLoRA~\citep{Tim:2023qlora}.
% \textcolor{custommagenta}{
The storage cost of the original model primarily drives the memory consumption during LoRA weights training. Thus, we define the \textit{parameter reduction ratio} as the count of parameters before and after pruning, to evaluate the memory efficiency of baselines.
The details of our experiment setups and hyperparameters are provided in~\cref{apd:detail_setup}.
% }


%full version
% \method incorporates two model compression techniques: sparsification, which generates a pruned model for low-rank matrix updates, and quantization, which forms \Qmethod further to reduce the memory footprint of the pruned model.
% For sparsification, 
% to validate the general effectiveness of \method, we benchmark its performance across various pruning strategies $\mathtt{P}(\cdot)$. 
% Specifically, we first establish a variant using randomly structured pruning and adapt \method to another three variants based on leading approaches: the structured pruning LLM-Pruner\footnote{\url{https://github.com/horseee/LLM-Pruner} (Apache-2.0 license)}~\citep{ma2023llmpruner} and the non-structured (semi-structured \& unstructured) pruning SparseGPT\footnote{\url{https://github.com/IST-DASLab/sparsegpt} (Apache-2.0 license)}~\citep{FrantarA23spasegpt}. 
% These baselines are summarized as follows:
% \begin{itemize}[leftmargin=20pt]
%     \item \textbf{\methodrand}: 
%     We adhere to the pruning settings of \methodstru, modifying only by randomly removing weights instead of the original gradient-based pruning criterion.
%     \item \textbf{\methodstru}: 
%     We follow LLM-Pruner and employ a block-wise strategy for local structured pruning. Attention and MLP layers are treated as separate blocks, with non-critical coupling weights pruned based on gradient information at a uniform ratio. We retain the first four and last two layers of both blocks, focusing pruning on the intermediate layers.
%     \item \textbf{\methodsemi}: 
%     We utilize SparseGPT with a 4:8 semi-structured sparsity pattern to prune pre-trained weights across all model layers.
%     \item \textbf{\methodunst}:  
%     We prune individual weights uniformly across layers using a predefined pruning ratio based on an unstructured version of SparseGPT.
% \end{itemize}

%For quantization $\mathtt{Q}(\cdot)$, to further reduce memory usage during training, especially when dealing with models exceeding 70 billion parameters, we achieve \Qmethod by combining \method with the LoRA-tailored quantization algorithm QLoRA~\citep{Tim:2023qlora}. While \method is compatible with the quantization of other customized LoRA methods~\citep{Xu:2023QALoRA,li2024loftq,guo2024lqlora,OPTQ2023,chai2023int21}, this falls outside the scope of this article.

% \paragraph{Architecture \& Hyperparameters.}
% %TODO, 这部分所有实验结果定了再改
% We adopt a LLaMA architecture with RMSNorm~\citep{ZhangS19a} and SwiGLU
% activations~\citep{Noglu,ZhaoSA22}. 
% % We use the same hyperparameters across methods for each model size, except the learning rate. 
% We run all experiments with BF16 format to reduce memory usage.
% % , and we tune the learning rate for each method under the same amount of computational budget and report the best performance.
% All experiments run on NVIDIA A100-80GB GPUs.
% We set low-rank matrices $\mathbf{B}$ and $\mathbf{A}$ of rank $r=8$ for $\mathbf{W}_\text{q}$, $\mathbf{W}_\text{k}$, $\mathbf{W}_\text{v}$, and $\mathbf{W}_\text{o}$ in the attention layer, $\mathbf{W}_\text{up}$, $\mathbf{W}_\text{gate}$, and $\mathbf{W}_\text{down}$ in the MLP layer, and the head embedding matrix $\mathbf{W}_\text{lm\_head}$.
% The details of our experiment setups and hyperparameters are provided in~\cref{apd:detail_setup}.
% Notably, following LLM-Pruner for model with Multi Query Attention, like LLaMA-2-7B and LLaMA-13B, we prune the same head index by $\mathbf{W}_\text{q}$; for model group query attn, like LLaMA-2-70B and LLaMA-3.1 heards, we prune the head index by $\mathbf{W}_\text{k}$ and the $\mathbf{W}_\text{q}$ and $\mathbf{W}_\text{v}$ prune consistent 8 heads.
% For LLaMA-3.1-405B, INT8

%  \subsection{Core Competitive Scenarios} Why naive pruning fail
%  \label{sec:core_scena}
% First, we use LLaMA-2-13B as the base model to evaluate the effects of different pruning ratios during the pruned full-rank weight generation phase of \methodstru.
% Specifically, we analyze how pruning impacts the parameter reduction ratio (defined as the ratio of parameter count before and after pruning) and the perplexity of the Alpaca test set. 
% The results (shown in~\cref{exp:core_comp}) are then used to introduce a \textit{core competitive scenario}, outlining the conditions under which \methodstru can serve as a cost-effective training alternative to standard LoRA.

% \begin{wrapfigure}[15]{r} 
% {0.60\textwidth}
%     \vspace{-10pt} 
%     \begin{center}
%     \includegraphics[width=0.60\textwidth]{fig/2-0_evaluation_alpaca_sft_7b-13b_lr1e-3_prune_ratio.pdf}
%     \vspace{-20pt}
%         \caption{Parameter reduction ratio and perplexity on the Alpaca across various pruning ratios for LLaMA-2-13B.}
%     \label{exp:core_comp}
%     \end{center}
% \end{wrapfigure}

% As illustrated in \cref{exp:core_comp},
% increasing the pruning ratio sharply raises the parameter reduction ratio (blue dashed line of $\textsc{13B w/ }\mathtt{P}(\cdot)$), but also degrades performance, as reflected in the perplexity increase (yellow line). 
% At a 75\% pruning ratio, the model reduces its parameters by 2.63$\times$ but suffers a 21.2$\times$ increase in perplexity (281.64).
% Given the LLaMA-2 herds, the user's available memory may only support inference on the 13B model, while training is feasible on the 7B model via LoRA.
% Thus, a \textit{core competitive scenario} arises for \methodstru if it can achieve a higher parameter reduction while maintaining better performance than both the LoRA-trained 7B and the original 13B models.
% At a 46\% pruning ratio (7B model is roughly equivalent to pruning the 13B model by 46\%) shown in~\cref{exp:core_comp},  
% \methodstru must achieve a parameter reduction ratio greater than 1.93$\times$ (blue triangle), i.e., a pruning ratio $>$ 60\%.
% Additionally, while a modest 25\% pruning on the 13B model leads to worse perplexity than both the original 7B and 13B models, 
% \methodstru must still outperform the LoRA-trained 7B and original 13B models in terms of perplexity.

% Next, our experiments comprehensively confirm that \method dominates this core competitive scenario across various model scales and instruction datasets.
% Based on the introduced scenarios, the default pruning ratios are as follows: for the 13B model, structured \method variants (\methodrand \& \methodstru) are set at 65\%, \methodunst at 55\%, and \methodsemi adheres to a fixed 4:8 sparsity pattern. For the 70B model, structured \method variants have a pruning ratio of 75\%, while non-structured variants (\methodsemi \& \methodunst) are not evaluated due to their limited effectiveness in reducing memory usage.
% (11.18)(11.18) (10.42)


\subsection{Fine-tuning Convergence}
\label{ssec:exp_convergence}
% Here we explore the performance of \method on the test set under different scale models and different instruction fine-tuning data, measured by perplexity. 

%观察1：域外测试集优于域内测试集，但域内测试集确与下游任务表现无关。
%说图：
%原因1：我们认为这是过拟合造成的，因此用域外测试集去衡量训练后模型性能是更可靠的！
%观察2：更大模型，基于神经元重要性的结构化剪枝带来的增益是非常明显的
%说图：
%原因：70B模型神经元之间的差异更大，因此针对性的选取神经元能有效增加模型训练性能
%观察：半结构化和非结构化剪枝在域内测试集上表现更好
%说图：
%原因：

% 这里我们探讨LoRAM在不同规模模型（LLaMA-2-13B和LLaMA-2-70B）和不同指令微调数据（OpenHermes和OpenOrca）下的收敛趋势。
% 为了评估训练的性能，针对不同的微调数据，我们会分别衡量模型在不同训练迭代轮次在域外测试集（Alpaca）和域内测试集（OpenHermes或OpenOrca）的困惑度，并展示在图4和图5中。

% 这里我们探讨LoRAM在不同规模模型（LLaMA-2-13B和LLaMA-2-70B）和不同指令微调数据（OpenHermes和OpenOrca）下的收敛趋势。
% 为了评估训练的性能，针对不同的微调数据，我们会分别衡量模型在不同训练迭代轮次在域外测试集（Alpaca）和域内测试集（OpenHermes或OpenOrca）的困惑度，并展示在图4和图5中。

% 域外收敛趋势. 使用LoRAM在不同规模和不同数据上的训练 ，在域外测试集上的表现趋势都是一致的，他们的困惑度都统一的展示了夹在未训练的原始模型和使用全量LoRA训练的小模型之间。比如，针对LLaMA-2-13B在OpenHermes上训练的，图4（a）展示了，不同剪枝算法下的LoRAM变体其困惑度高于LLaMA-2-7B LoRA，但低于LLaMA-2-13B LoRA。而这些LoRAM将参数减少了2.17$\times$相比于LLaMA-2-13B LoRA, 对于70B模型，参数减少比例更是扩大至12.84$\times$。

% 域内收敛趋势. 我们发现LoRAM的收益较小，但我们认为原始模型通过LoRA训练倾向过拟合域内数据，这能够被小节4.3中的下游性能评估证明，因为在域内测试集表现好的方法在下游任务上并非相对优越，比如LLaMA-2-7B LoRA在域内优于LLaMA-2-13B \methodrand 和 \methodstru, 但在多个下游任务上表现都差于他们。

% 半/非结构LoRAM对域内友好。我们在不同数据和模型规模上发现，非结构化LoRAM(\methodsemi和\methodunst)在域内测试集上明显优于结构化LoRAM(\methodrand和\methodstru)。如图4(a)和（b）对比，以及如图5（a）和（b）对比，非结构化LoRAM的困惑度在域内明显优于结构化数据，而在域外测试的困惑度他们之前的差别被缩小。我们认为这是因为非结构化会更精巧的选择剪枝的神经元，因此其捕获信息的方式和原始模型更接近，从而在域内表现更好。

% 非随机LoRAM对参数量友好。我们发现当模型的参数量增加，非随机LoRAM的性能增益被显著扩大。比如，如图4(c)和（d）与（a）和（b）比较，\methodstru在70B上显著优于LoRAM.Rand，但在13B上他们的差别几乎可以忽略。我们认为这是更大参数的模型，为有选择的剪枝带来更多潜在的可能，因为神经元间的性能差异更大，因此有选择剪枝收益会更高。

We investigate the convergence trends of \method across varying model scales (LLaMA-2-13B \& LLaMA-2-70B) and different instruction-tuning datasets (OpenHermes \& OpenOrca).
To assess training performance, we track perplexity over training iterations on both out-of-domain (Alpaca) and in-domain (OpenHermes or OpenOrca) test sets, as shown in~\cref{fig:llama2_convergency_openhermes} and~\cref{fig:llama2_convergency_openorca}.

\paragraph{Out-of-Domain Performance.}
\method consistently achieves out-of-domain performance with similar trends, positioned between the LoRA fine-tuned models of the same scale and smaller models, across different models and datasets.
As shown in~\cref{fig:llama2_convergency_openhermes,fig:llama2_convergency_openorca} (a), for the 13B model, the perplexity of \method variants pruned by different algorithms is lower than that of the LoRA-trained 7B model but higher than the LoRA-trained 13B model, with \methodrand and \methodstru achieving a 2.17$\times$ parameter reduction. 
Similarly, as shown~\cref{fig:llama2_convergency_openhermes,fig:llama2_convergency_openorca} (c), for the 70B model, this reduction extends to 12.84$\times$ under similar convergence trends.

\paragraph{In-Domain Performance.}
\method shows limited improvement in in-domain performance, likely due to overfitting when the base models are fine-tuned with LoRA, resulting in relatively lower perplexity. This is further supported by downstream evaluations, where models that excel in in-domain perplexity often underperform in downstream tasks. As shown in~\cref{fig:llama2_convergency_openhermes,fig:llama2_convergency_openorca} (b), while the LoRA-trained 7B model outperforms 13B \methodrand and \methodstru on in-domain tests, it underperforms on several downstream tasks as shown in~\cref{ssec:exp_downstream}.


\begin{figure*}[!t]
% \vspace{-10ex}
% \setcounter{subfigure}{0}
\begin{center}
\includegraphics[width=\textwidth]{fig/3-2_llama-2_convergency_openhermes.pdf}
    \caption{The test perplexity of training LLaMA-2-13B \& LLaMA-2-70B on OpenHermes.}
    \label{fig:llama2_convergency_openhermes}
\end{center}
\begin{center}
% \vspace{-1ex}
\includegraphics[width=\textwidth]{fig/3-2_llama-2_convergency_openorca.pdf}

    \caption{The test perplexity of training LLaMA-2-13B \& LLaMA-2-70B on OpenOrca.}
    \label{fig:llama2_convergency_openorca}
\end{center}
    \vspace{-4ex}
\end{figure*}


% \begin{table*}[!ht]
% \caption{Performance of the MathQA (1-shot) \& GSM8K (8-shots) in the mathematical domain. 
% \ding{115} indicates that the theoretical parameters of non-structured pruning are reduced. However, these parameters are filled with zeros in actual training, so the memory footprint is not reduced.}
% \label{exp:math}
% % \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \renewcommand{\arraystretch}{0.6}
% \setlength{\tabcolsep}{1.2mm}
% \begin{tabular}{@{}lp{4cm}ccccr@{}}
% \toprule
% \multirow{2}{*}{Model} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{OpenHermes} & \multicolumn{2}{c}{OpenOrca} & \multirow{2}{*}{\makecell{Parameter \\ Redu. Ratio}} \\
% \cmidrule(r){3-4} \cmidrule(r){5-6}
% & &  MathQA & GSM8K  &  MathQA & GSM8K &\\
% \midrule
% \multirow{8}{*}{LLaMA-2}
% & \gray{13B w/o FT}   
% & \gray{0.3260} 
% & \gray{0.2426} 
% & \gray{0.3293} 
% & \gray{0.2335} 
% & \gray{1.00$\times$} \\
% % & \gray{13B LoRA}    
% % & \gray{0.3203}
% % & \gray{0.3669}
% % & \gray{0.3323} 
% % & \gray{0.2798}
% % & \gray{0.998$\times$} \\
% & \gray{7B LoRA}     
% & \gray{0.2961} 
% & \gray{0.2282} 
% & \gray{0.3095} 
% & \gray{0.1387} 
% & \gray{1.93$\times$} \\
% & 13B \methodrand 
% & \bluethree{0.3377} & \bluetwo{0.2722} 
% & 0.3283
% & \bluetwo{0.2593} & 2.17$\times$\\
% & 13B \methodstru 
% & \bluefour{0.3380} & \blueone{0.2464} 
% & \bluefour{0.3307} & \blueone{0.2449} & 2.17$\times$\\
% & 13B \methodsemi  
% & 0.3176 & \bluefour{0.3692} 
% & \bluefour{0.3307} 
% & \bluefour{0.2729} 
% & \ding{115} 1.95$\times$  \\
% & 13B \methodunst  
% & 0.3012 & \bluethree{0.3192} 
% & 0.3270 & \bluethree{0.2661} & \ding{115} 2.16$\times$  \\
% % & 13B \methodunst (0.65)  
% % & 0.2972 & 0.2525 
% % & 0.2972 & 0.2525 & \ding{115} 2.73$\times$  \\
% \midrule
% \multirow{5}{*}{LLaMA-2}
% & \gray{70B w/o FT}  
% & \gray{0.3953} 
% & \gray{0.5201} 
% & \gray{0.3953} 
% & \gray{0.5201}
% & \gray{1.00$\times$} \\
% % & \gray{70B LoRA}    
% % & \gray{0.3920} 
% % & \gray{0.6020} 
% % & \gray{0.4017} 
% % & \gray{0.4890}
% % & \gray{0.998$\times$} \\
% & \gray{13B LoRA}    
% & \gray{0.3203} 
% & \gray{0.3669} 
% & \gray{0.3353}  
% & \gray{0.2350}
% & \gray{5.30$\times$} \\
% % &70B \textsc{\Qmethodrand} (0.75 QO) 
% % & \textbf{0.3977} 
% % & \textbf{0.5489} 
% % & \textbf{0.5974} 
% % & \textbf{0.6224}
% % & 6.01$\times$\\
% &70B \textsc{\Qmethodrand}
% & \bluethree{0.3966} 
% & \bluefour{0.5762} 
% & 0.3886 
% &  \bluefour{0.5572}
% & 12.84$\times$\\
% &70B \Qmethodstru 
% & \bluefour{0.3977} 
% & \bluethree{0.5716} 
% & \bluefour{0.3973}
% & \bluethree{0.5444}
% & 12.84$\times$\\
% % \midrule
% % \multirow{2}{*}{Qwen2}
% % & Qwen2-Small & 0.2001 & 0.4002 & 0.2001 & 0.4002 & 1.5$\times$ \\
% % & Qwen2-Large & 0.3003 & 0.6004 & 0.3003 & 0.6004 & 2.0$\times$ \\
% \midrule
% \multirow{4}{*}{LLaMA-3.1}
% % & 405B (FP8) w/o FT  
% % & \underline{0.6023} 
% % & \underline{0.8726} 
% % & \underline{0.8704} 
% % & \underline{0.8704} 
% % & 1.00$\times$ \\
% & \gray{405B w/o FT}  % (INT8)
% & \gray{0.5822}
% & \gray{0.8719}
% & \gray{0.5822}
% & \gray{0.8719}
% & \gray{1.00$\times$} \\
% & \gray{70B w/o FT}
% & \gray{0.5464} 
% & \gray{0.7468} 
% & \gray{0.5464} 
% & \gray{0.7468} 
% & \gray{5.70$\times$}\\
% % & \gray{70B LoRA}
% % & \gray{} 
% % & \gray{} 
% % & \gray{} 
% % & \gray{} 
% % & \gray{5.70$\times$}\\
% % & 70B QLoRA 
% % & \textbf{0.3990} 
% % & \textbf{0.5800} 
% % & \textbf{0.6224} 
% % & \textbf{0.6224} 
% % & 1.00$\times$\\
% & 405B \textsc{\Qmethodrand} %(INT8)
% & \bluefour{0.5846} 
% & \bluefour{0.8719} 
% & {} 
% & {} 
% & 16.00$\times$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}

\paragraph{Non-Structured \method Excels in In-Domain.}
The non-structured variants (\methodsemi \& \methodunst) consistently outperform their structured counterparts (\methodrand \& \methodstru) on in-domain test sets.
As shown in ~\cref{fig:llama2_convergency_openhermes} (a) vs.~\cref{fig:llama2_convergency_openhermes} (b) and~\cref{fig:llama2_convergency_openorca} (a) vs.~\cref{fig:llama2_convergency_openorca} (b), 
the in-domain perplexity of \methodsemi and \methodunst is notably lower, while their out-of-domain performance shows less pronounced differences. This advantage likely arises from the more selective weight pruning in the non-structured variants, which preserves information capture capabilities similar to the original model, thus enhancing in-domain performance.

\paragraph{Non-Random \method Benefits from Scaling.}
The performance gains of the non-random \method become more evident as the model size grows. 
As shown in~(a,b) of \cref{fig:llama2_convergency_openhermes,fig:llama2_convergency_openorca} vs.~(c,d) of \cref{fig:llama2_convergency_openhermes,fig:llama2_convergency_openorca}, \methodstru outperforms \methodrand considerably on the 70B model, while the difference is marginal on the 13B model. 
This indicates that larger models exhibit greater differences in the redundancy of individual weights, making selective pruning more effective\footnote{The trained low-rank matrices are visualized in~\cref{apd:vis_matrix}, and the update patterns they exhibit somewhat align with these insights.}.

\subsection{Downstream Task Performance}
\label{ssec:exp_downstream}
% \paragraph{Code Generation.}
% \begin{table*}[ht]
% \caption{Performance of the HumanEval in the code generation domain under zero-shot setting (temperature 0.6, $top_p$ 0.95). Metric of HumanEval: Pass@1 \& Pass@10. Offline Model FineWeb \& MathWeb CKPT1600; Online LoRA OpenHermes CKPT400. \ding{115} indicates that the theoretical parameters of unstructured pruning are reduced. However, these parameters are filled with zeros in actual training, so memory footprint is not reduced.}
% \label{exp:code}
% % \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \setlength{\tabcolsep}{1mm}
% \begin{tabular}{@{}lccr@{}}
% \toprule
% Method (LLaMA-2)               & Pass@1           &Pass@10   &\#Para. $\downarrow$ \\
% \midrule
% \rowcolor{gray!10}
% 13B w/o FT   & 0.1451 & 0.3354 & 1.00$\times$ \\
% % 13B LoRA      & \textbf{0.1567} & 0.3415 & 1.00$\times$ \\
% % 7B w/o FT  & 0.1104 & 0.2500 & 1.93$\times$ \\
% \rowcolor{gray!10}
% 7B LoRA     & 0.1262  & 0.2561 & 1.93$\times$ \\
% % 13B \method \textcolor{red}{Sparse4:8 Pruned} &0.0646  &0.1646   & \ding{115} 1.95$\times$ \\
% % 13B \method Sparse4:8 \textcolor{red}{Recovered} & 0.1372 & 0.2927  & \ding{115} 1.95$\times$ \\
% 13B \methodsemi  
% &\underline{0.1567} &\underline{0.3598}
% &\ding{115} 1.95$\times$  \\
% 13B \methodunst (0.55)  
% &0.1506 &0.3171
% &\ding{115} 2.16$\times$  \\
% 13B \methodunst (0.65)  
% &0.1189 &0.2805
% &\ding{115} 2.73$\times$  \\
% % 13B \method \textcolor{red}{Taylor-0.65 FineWeb} & 0.1348  & 0.2927 & 2.17$\times$\\
% 13B \methodstru (0.65)  & 0.1366 & 0.3109 & 2.17$\times$\\
% 13B \methodrand (0.65) & 0.1463 & 0.3597 & 2.17$\times$\\
% \midrule
% \rowcolor{gray!10} 
% 70B w/o FT  & \underline{0.2591} & \underline{0.5793}  & 1.00$\times$ \\
% % 13B w/o FT   & 0.1451 & 0.3354 & 5.30$\times$ \\
% \rowcolor{gray!10} 
% 13B LoRA      & 0.1567 & 0.3415 & 5.30$\times$ \\
% 70B \Qmethodrand (0.75 QO) & \textbf{0.2646} & \textbf{0.5915} & 6.01$\times$\\
% 70B \Qmethodrand (0.75 KV) & \textbf{0.2646} & \textbf{0.5915} & 12.84$\times$\\
% 70B \Qmethodstru (0.75 KV) & \textbf{0.2646} & \textbf{0.5915} & 12.84$\times$\\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}

We evaluate the performance of various models trained with \method on different instruction data across three downstream tasks: mathematical reasoning, common sense reasoning (CSR), and code generation. 
Results are summarized in~\cref{exp:math,exp:csr,exp:code}. 
We highlight the \textit{core competition scenario} with a gray background, which includes the untrained original model and a smaller sibling model trained with LoRA.
For instance, for \method-trained LLaMA-2-13B, we report the scores of the 13B without fine-tuning and the LoRA-trained 7B model. 
Blue backgrounds of varying intensity indicate the degree of improvement for each \method variant relative to the \textit{core competition scenario}: darker shades indicate greater improvements, while lighter shades signify smaller gains.


% To better visualize performance improvements, we highlight the \textit{core competitive scenario} introduced in~\cref{sec:core_scena} with a gray background.

\begin{table*}[!t]
\caption{Accuracy (\%) of the MathQA (1-shot) \& GSM8K (8-shots) in the mathematical domain under LLaMA-2. 
\textcolor{uclablue}{\ding{115}} indicates the theoretical parameters reduction of non-structured pruning. However, these parameters are filled with zeros in actual training, so the memory footprint is not reduced.}
\label{exp:math}
\begin{center}
\begin{footnotesize}
\begin{sc}
\renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{0.4mm}
\begin{tabular}{@{}p{3cm}ccccr@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{OpenHermes} & \multicolumn{2}{c}{OpenOrca} & \multirow{2}{*}{\makecell{Parameter \\ Redu. Ratio}} \\
\cmidrule(r){2-3} \cmidrule(r){4-5}
&  MathQA & GSM8K  &  MathQA & GSM8K &\\
\midrule
\gray{13B w/o FT}   
& \gray{32.60} 
& \gray{24.26} 
& \gray{32.93} 
& \gray{23.35} 
& \gray{1.00$\times$} \\
\gray{7B LoRA}     
& \gray{29.61} 
& \gray{22.82} 
& \gray{30.95} 
& \gray{13.87} 
& \gray{1.93$\times$} \\
13B \methodrand 
& \bluethree{33.77} & \bluetwo{27.22} 
& 32.83
& \bluetwo{25.93} & 2.17$\times$\\
13B \methodstru 
& \bluefour{33.80} & \blueone{24.64} 
& \bluefour{33.07} & \blueone{24.49} & 2.17$\times$\\
13B \methodsemi  
& 31.76 & \bluefour{36.92} 
& \bluefour{33.07} 
& \bluefour{27.29} 
& \textcolor{uclablue}{\ding{115}} 1.95$\times$  \\
13B \methodunst  
& 30.12 & \bluethree{31.92} 
& 32.70 & \bluethree{26.61} & \textcolor{uclablue}{\ding{115}} 2.16$\times$  \\
\midrule
\gray{70B w/o FT}  
& \gray{39.53} 
& \gray{52.01} 
& \gray{39.53} 
& \gray{52.01}
& \gray{1.00$\times$} \\
\gray{13B LoRA}    
& \gray{32.03} 
& \gray{36.69} 
& \gray{33.63}  
& \gray{25.70}
& \gray{5.30$\times$} \\
70B \textsc{\Qmethodrand}
& \bluethree{39.66} 
& \bluefour{57.62} 
& 39.40
&  \bluefour{55.72}
& 12.84$\times$\\
70B \Qmethodstru 
& \bluefour{39.77} 
& \bluethree{57.16} 
& \bluefour{39.73}
& \bluethree{54.44}
& 12.84$\times$\\
\bottomrule
\end{tabular}
\end{sc}
\end{footnotesize}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[t]
\caption{Average accuracy (\%) of the CSR in the common sense reasoning domain (1-shot) under the LLaMA-2. Baseline results for each subtask of CSR are detailed in~\cref{apd:detail_csr}.
}
\label{exp:csr}
\begin{center}
\begin{footnotesize}
\begin{sc}
\renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{0.8mm}
\begin{tabular}{@{}p{3cm}
>{\centering\arraybackslash}p{2.4cm}
>{\centering\arraybackslash}p{2.4cm}r@{}}
\toprule
 \multirow{2}{*}{Method} & OpenHermes & OpenOrca & \multirow{2}{*}{\makecell{Parameter \\ Redu. Ratio}} \\
\cmidrule(r){2-3}
&  Mean $\pm$ Std 
&  Mean $\pm$ Std  
&  \\
\midrule
\gray{13B w/o FT}   
& \gray{64.28$\pm$1.30} 
& \gray{64.28$\pm$1.30}
& \gray{1.00$\times$} \\
\gray{7B LoRA}     
& \gray{61.51$\pm$1.29} 
& \gray{61.42$\pm$1.30} 
& \gray{1.93$\times$} \\
13B \methodrand
&\bluefour{64.64$\pm$1.29}
&\bluetwo{64.49$\pm$1.30}
& 2.17$\times$\\
13B \methodstru
&\bluethree{64.42$\pm$1.29}
&\blueone{64.32$\pm$1.29}
&2.17$\times$\\
13B \methodsemi 
&\bluetwo{64.38$\pm$1.29}
&\bluefour{64.73$\pm$1.30}
&\textcolor{uclablue}{\ding{115}} 1.95$\times$  \\
13B \methodunst
&64.12$\pm$1.29
&\bluethree{64.68$\pm$1.29}
&\textcolor{uclablue}{\ding{115}} 2.16$\times$  \\
\midrule
\gray{70B w/o FT}  
& \gray{68.69$\pm$1.27}
& \gray{68.69$\pm$1.27}
& \gray{1.00$\times$} \\
 \gray{13B LoRA} 
& \gray{65.05$\pm$1.29}
& \gray{65.40$\pm$1.29}
& \gray{5.30$\times$} \\
70B \Qmethodrand
&\bluethree{68.99$\pm$1.27}
&68.46$\pm$1.27
&12.84$\times$\\
70B \Qmethodstru 
&\bluefour{69.10$\pm$1.27}
&  \bluefour{68.94$\pm$1.27}
& 12.84$\times$\\
\bottomrule
\end{tabular}
\end{sc}
\end{footnotesize}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[!t]
\caption{\textsc{Pass@1}(\%) and \textsc{Pass@10}(\%) of HumanEval in the code generation domain under LLaMA-2. The best results for all baselines are reported, selected from \textsc{temperature} settings in \{0.0, 0.2, 0.4, 0.6, 0.8\} with $\textsc{top}_\textsc{p}$ fixed at 0.95.}
\label{exp:code}
% \vskip 0.15in
\begin{center}
\begin{footnotesize}
\begin{sc}
\renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{0.3mm}
\begin{tabular}{@{}p{3cm}ccccr@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{OpenHermes} & \multicolumn{2}{c}{OpenOrca} & \multirow{2}{*}{\makecell{Parameter \\ Redu. Ratio}} \\
\cmidrule(r){2-3} \cmidrule(r){4-5}
 & Pass@1 & Pass@10 & Pass@1 & Pass@10 & \\
\midrule
 \gray{13B w/o FT}
 & \gray{17.68} 
 & \gray{35.37} 
 & \gray{17.68}
 & \gray{35.37} 
 & \gray{1.00$\times$} \\
\gray{7B LoRA}
 & \gray{15.24}
 & \gray{28.04} 
 & \gray{15.85} 
 & \gray{26.21} 
 & \gray{1.93$\times$} \\
13B \methodrand 
& \bluetwo{19.51} & 33.54
& \bluefour{19.51} & 32.32  & 2.17$\times$ \\
13B \methodstru 
& \blueone{17.68} & \bluefour{35.37} 
& 17.07 & 31.71 & 2.17$\times$ \\
13B \methodsemi 
& \bluethree{20.12} & \bluefour{35.37}  
& \bluefour{18.29} & \bluefour{39.63} & \textcolor{uclablue}{\ding{115}} 1.95$\times$ \\
13B \methodunst 
& \bluefour{22.56} & 34.15
& \bluefour{18.29} & \bluethree{37.20} & \textcolor{uclablue}{\ding{115}} 2.16$\times$ \\
\midrule
\gray{70B w/o FT}
& \gray{31.71} 
& \gray{58.54}
& \gray{31.71} 
& \gray{58.54}
& \gray{1.00$\times$} \\
\gray{13B LoRA} 
& \gray{18.29} 
& \gray{35.98} 
& \gray{18.29} 
& \gray{39.02} 
& \gray{5.30$\times$} \\
70B \textsc{\Qmethodrand} 
& 29.27 & 57.32 
& \bluefour{31.71} & 56.71 
& 12.84$\times$ \\
70B \Qmethodstru
& \bluefour{32.32} & \bluefour{58.54} 
& \bluefour{32.32} & \bluefour{59.15} & 12.84$\times$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{footnotesize}
\end{center}
\vskip -0.1in
\end{table*}


% \begin{figure} %[14]{r} 
% % {0.60\textwidth}
%     % \vspace{-10pt} 
%     \begin{center}
%     \includegraphics[width=0.60\textwidth]{fig/Performance_405b_openherms.pdf}
%     % \vspace{-20pt}
%         \caption{Performance on LLaMA-3.1-405B.}
%     \label{exp:scale_405b}
%     \end{center}
%     % \vspace{-20pt}
% \end{figure}

Overall, we observe that most \method variants outperform the core competitive baseline across all downstream tasks, particularly in mathematical and common sense reasoning. This improvement is further amplified by increasing the model scale.
Specifically, as shown in~\cref{exp:math}, the 70B \methodrand and \methodstru models achieve a 12.84$\times$ reduction in parameters compared to the original 70B model (70B w/o FT), exceeding the 5.30$\times$ reduction of the LoRA-trained 13B model. 
In terms of performance, \method improves the original 70B model's score on GSM8K from 52\% to 57\%, significantly outperforming the LoRA-trained 13B model, which only achieved 37\%.
These results demonstrate that updating low-rank matrices on pruned models effectively reduces memory requirements during training. Merging the recovered low-rank matrices into the original model yields substantial performance gains during inference.


%%TO
%%%LLaMA-3
%%8B CSR:65.32$pm$1.28 MathQA 41.14 GSM8K 55.80  Pass@1 34.15 Pass@10 71.34 

% Openorca                CSR   (FE)  (SM)  MathQA Pass@1 Pass@10 
% 13B w/o                 64.28 28.20 24.26 32.60 17.68 35.37
% 7B LoRA (1e-3)          62.08 15.39 13.50 30.05 17.07 26.83 
% 7B LoRA (1e-5)          61.42 15.62 13.87 30.95 15.85 26.22  
% 13B Random(1e-4)        64.44 29.57 23.28 32.76 17.68 35.37
% 13B Structure(1e-4)     64.59 30.40 23.12 33.30 17.68 31.71
% 13B Random(5e-5)        64.38 30.71 24.11 33.40 17.07 32.32
% 13B Structure(5e-5)     64.59 30.40 23.12 33.30 17.68 31.71
% 13B Random(1e-5)        64.49 29.57 25.93 33.40 19.51 32.32
% 13B Structure(1e-5)     64.32 29.11 24.49 33.07 17.07 31.71
% 13B Semi(1e-5)          64.73 30.71 27.29 33.07 18.29 39.63
% 13B Unst(1e-5)          64.68 30.25 26.61 32.70 18.29 37.20
% 13B LoRA(1e-4)          65.40 31.24 25.70 33.63 18.29 39.02
% 13B LoRA(3e-5)          65.05 30.40 24.26 33.60 18.90 34.76
% 70B w/o FT              68.69 57.32 52.01 39.53 31.71 58.54 
% 70B Stru(3e-5)          68.96 59.21 55.27 39.36 31.71 60.98
% 70B Rand(3e-5)          68.67 58.68 56.25 39.43 33.54 56.10

% \paragraph{Math Reasoning.}

% \begin{table*}[ht]
% \caption{Performance of the MathQA \& GSM8K in the mathematical domain under the CoT (default 8-shots) setting. Metric of MathQA: Acc (Accuracy). Filter-Metric of GSM8K: SM-EM (Strict Match-Exact Match); FE-EM (Flexible Extract-Exact Match). Offline Model FineWeb \& MathWeb CKPT1600; Online LoRA OpenHermes CKPT400. \ding{115} indicates that the theoretical parameters of unstructured pruning are reduced. However, in actual training, these parameters are filled with zeros, so the memory footprint is not reduced.}
% \label{exp:math}
% % \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \setlength{\tabcolsep}{1.2mm}
% \begin{tabular}{@{}lcccr@{}}
% \toprule
% Method (LLaMA-2)               & MathQA (Acc)           &GSM8K (SM)  &GSM8K (FE)   &  \#Para. $\downarrow$ \\
% \midrule
% \rowcolor{gray!10}
% 13B w/o FT   & 0.3293$\pm$0.0086 & 0.2335$\pm$0.0117 & 0.2729$\pm$0.0123 & 1.00$\times$ \\
% % 7B w/o FT & 0.2995$\pm$0.0084 & 0.1357$\pm$0.0094 & 0.1448$\pm$0.0097 & 1.93$\times$ \\
% \rowcolor{gray!10}
% 7B LoRA     & 0.2995$\pm$0.0084 & 0.2206$\pm$0.0114 & 0.2411$\pm$0.0118 & 1.93$\times$ \\
% % 13B LoRA    & 0.3266$\pm$0.0086  & \textbf{0.3654$\pm$0.0133} & \textbf{0.4071$\pm$0.0135} & 1.00$\times$ \\
% % 70B w/o FT  & 0.3943$\pm$0.0089 & 0.5133$\pm$0.0138 & 0.5686$\pm$0.0136 & 0.19$\times$ \\
% % 13B \method \textcolor{red}{Sparse4:8 Pruned} & 0.2824$\pm$0.0082 & 0.2525$\pm$0.0120 & 0.2570$\pm$0.0120 & \ding{115} 1.95$\times$ \\
% % 13B \method Sparse4:8 \textcolor{red}{Recovered}  & 0.3146$\pm$0.0085 & \underline{0.3252$\pm$0.0129} & \underline{0.3321$\pm$0.0130} & \ding{115} 1.95$\times$  \\
% % 13B \method \textcolor{red}{Taylor-0.65 FineWeb} & 0.3347$\pm$0.0086 & 0.2320$\pm$0.0116 & 0.3017$\pm$0.0126 & 2.17$\times$\\
% % 13B \method Taylor-0.75 FineWeb & 0.3347$\pm$0.0086 & 0.2365$\pm$0.0117 & 0.3033$\pm$0.0127 & 2.67$\times$\\
% 13B \methodstru (0.65) & 0.3374$\pm$0.0087 & 0.2381$\pm$0.0117 & 0.3230$\pm$0.0129 & 2.17$\times$\\
% 13B \methodrand (0.65) & \textbf{0.3397$\pm$0.0087} & 0.2714$\pm$0.0122 & 0.3313$\pm$0.0130 & 2.17$\times$\\
% 13B \methodsemi  
% &0.3169$\pm$0.0085 & \textbf{0.3768$\pm$0.0133} 
% &\textbf{0.3980$\pm$0.0135} & \ding{115} 1.95$\times$  \\
% 13B \methodunst (0.55)  
% &0.3059$\pm$0.0084 & \underline{0.3222$\pm$0.0129} 
% &\underline{0.3495$\pm$0.0131} & \ding{115} 2.16$\times$  \\
% 13B \methodunst (0.65)  
% &0.2972$\pm$0.0084 & 0.2525$\pm$0.0120 
% &0.2805$\pm$0.0124 & \ding{115} 2.73$\times$  \\
% \midrule
% \rowcolor{gray!10}
% 70B w/o FT  
% & \underline{0.3943$\pm$0.0089} 
% & \underline{0.5133$\pm$0.0138} 
% & \underline{0.5686$\pm$0.0136} & 1.00$\times$ \\
% \rowcolor{gray!10}
% 13B LoRA    
% & 0.3266$\pm$0.0086 
% & 0.3654$\pm$0.0133 
% & 0.4071$\pm$0.0135 & 5.30$\times$ \\
% 70B \textsc{\Qmethodrand} (0.75 QO) 
% & \textbf{0.3977$\pm$0.0090} 
% & \textbf{0.5489$\pm$0.0137} 
% & \textbf{0.5974$\pm$0.0135} 
% & 6.01$\times$\\
% 70B \textsc{\Qmethodrand} (0.75 KV) 
% & \textbf{0.3973$\pm$0.0090} 
% & \textbf{0.5777$\pm$0.0136} 
% & \textbf{0.6065$\pm$0.0135} 
% & 12.84$\times$\\
% 70B \Qmethodstru (0.75 KV) 
% & \textbf{0.3990$\pm$0.0090} 
% & \textbf{0.5800$\pm$0.0136} 
% & \textbf{0.6224$\pm$0.0134} 
% & 12.84$\times$\\
% \midrule
% \rowcolor{gray!10}
% 405B (FP8) w/o FT  
% & \underline{0.6023$\pm$0.0090} 
% & \underline{0.8726$\pm$0.0092} 
% & \underline{0.8704$\pm$0.0093} & 1.00$\times$ \\
% 405B (INT8) w/o FT  
% & \underline{0.5876$\pm$0.0090} 
% & \underline{0.8673$\pm$0.0093} 
% & \underline{0.8704$\pm$0.0093} & 1.00$\times$ \\
% 70B QLoRA 
% & \textbf{0.3990$\pm$0.0090} 
% & \textbf{0.5800$\pm$0.0136} 
% & \textbf{0.6224$\pm$0.0134} 
% & 1.00$\times$\\
% 405B (INT8) \textsc{\Qmethodrand} 
% & \underline{0.5843$\pm$0.0090} 
% & \underline{0.8650$\pm$0.0094} 
% & \underline{0.8688$\pm$0.0093} & 50.00$\times$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}




% \paragraph{Common Sense Reasoning.}
% 
% \begin{table*}[t]
% \caption{Performance of the CSR in the common sense reasoning domain under the 1-shot setting. Baseline results for each subtask of CSR are detailed in~\cref{apd:detail_csr}.
% }
% \label{exp:csr}
% % \vskip 0.15in
% \begin{center}
% \begin{footnotesize}
% \begin{sc}
% \renewcommand{\arraystretch}{0.6}
% \setlength{\tabcolsep}{1.2mm}
% \begin{tabular}{@{}lp{4cm}
% >{\centering\arraybackslash}p{2.7cm}
% >{\centering\arraybackslash}p{2.7cm}r@{}}
% \toprule
% \multirow{2}{*}{Model} & \multirow{2}{*}{Method} & OpenHermes & OpenOrca & \multirow{2}{*}{\makecell{Parameter \\ Redu. Ratio}} \\
% \cmidrule(r){3-4}
% &
% &  Mean $\pm$ Std 
% &  Mean $\pm$ Std  
% &  \\
% \midrule
% \multirow{8}{*}{LLaMA-2}
% & \gray{13B w/o FT}   
% & \gray{0.6428$\pm$0.0130} 
% & \gray{0.6428$\pm$0.0130}
% & \gray{1.00$\times$} \\
% % & \gray{13B LoRA}   
% % & \gray{0.6505$\pm$0.0129} 
% % & \gray{0.6566$\pm$0.0129}
% % & \gray{0.998$\times$} \\
% & \gray{7B LoRA}     
% & \gray{0.6151$\pm$0.0129} 
% & \gray{0.6142$\pm$0.0130} 
% & \gray{1.93$\times$} \\
% &13B \methodrand
% &\bluefour{0.6464$\pm$0.0129}
% &\bluetwo{0.6449$\pm$0.0130}
% & 2.17$\times$\\
% &13B \methodstru
% &\bluethree{0.6442$\pm$0.0129}
% &\blueone{0.6432$\pm$0.0129}
% &2.17$\times$\\
% &13B \methodsemi. 
% &\bluetwo{0.6438$\pm$0.0129}
% &\bluefour{0.6473$\pm$0.0130}
% & \ding{115} 1.95$\times$  \\
% % &13B \methodunst (0.55)  
% % &0.6416$\pm$0.0129
% % &0.6416$\pm$0.0129
% % &\ding{115} 2.16$\times$  \\
% &13B \methodunst
% &0.6412$\pm$0.0129
% &\bluethree{0.6468$\pm$0.0129}
% &\ding{115} 2.16$\times$  \\
% \midrule
% \multirow{5}{*}{LLaMA-2}
% & \gray{70B w/o FT}  
% & \gray{0.6869$\pm$0.0127}
% & \gray{0.6869$\pm$0.0127}
% & \gray{1.00$\times$} \\
% % & \gray{70B LoRA} 
% % & \gray{0.6956$\pm$0.0127}
% % & \gray{0.7031$\pm$0.0127}
% % & \gray{0.998$\times$} \\
% & \gray{13B LoRA} 
% & \gray{0.6505$\pm$0.0129}
% & \gray{0.6447$\pm$0.0129}
% & \gray{5.30$\times$} \\
% % &70B \textsc{\Qmethodrand} (0.75 QO) 
% % &  \underline{0.6874$\pm$0.0127}
% % &  \underline{0.6874$\pm$0.0127}
% % & 6.01$\times$\\
% &70B \Qmethodrand
% &\bluethree{0.6899$\pm$0.0127}
% &0.6846$\pm$0.0127
% &12.84$\times$\\
% &70B \Qmethodstru 
% &\bluefour{0.6910$\pm$0.0127}
% &  \bluefour{0.6894$\pm$0.0127}
% & 12.84$\times$\\
% % \midrule
% % \multirow{2}{*}{Qwen2}
% % & Qwen2-Small & 0.2001  & 0.4002 & 1.5$\times$ \\
% % & Qwen2-Large & 0.3003 & 0.6004 & 2.0$\times$ \\
% \midrule
% \multirow{4}{*}{LLaMA-3.1}
% % & 405B (FP8) w/o FT  
% % &  \underline{0.6874$\pm$0.0127}
% % &  \underline{0.6874$\pm$0.0127}
% % & 1.00$\times$ \\
% & \gray{405B w/o FT}  
% &  \gray{0.7220$\pm$0.0124}
% &  \gray{0.7220$\pm$0.0124}
% & \gray{1.00$\times$} \\
% % & \gray{70B LoRA} 
% % & \gray{}
% % & \gray{} 
% % & \gray{5.70$\times$} \\
% & \gray{70B w/o FT} 
% & \gray{0.7036$\pm$0.0125}
% & \gray{0.7036$\pm$0.0125} 
% & \gray{5.70$\times$} \\
% % & 70B QLoRA 
% % &  \underline{0.6874$\pm$0.0127}
% % &  \underline{0.6874$\pm$0.0127}
% % & 1.00$\times$\\
% & 405B \textsc{\Qmethodrand} 
% &  0.7206$\pm$0.0124
% &  {}
% & 16.00$\times$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{footnotesize}
% \end{center}
% \vskip -0.1in
% \end{table*}

% \begin{table*}[t]
% \caption{Performance on HumanEval for code generation tasks. The best \textsc{Pass@1} and \textsc{Pass@10} results for all baselines are reported, selected from \textsc{temperature} settings in [0, 0.2, 0.4, 0.6, 0.8] with $\textsc{top}_\textsc{p}$ fixed at 0.95.}
% \label{exp:code}
% % \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \renewcommand{\arraystretch}{0.6}
% \setlength{\tabcolsep}{0.8mm}
% \begin{tabular}{@{}lp{4cm}ccccr@{}}
% \toprule
% \multirow{2}{*}{Model} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{OpenHermes} & \multicolumn{2}{c}{OpenOrca} & \multirow{2}{*}{\makecell{Parameter \\ Redu. Ratio}} \\
% \cmidrule(r){3-4} \cmidrule(r){5-6}
% & & Pass@1 & Pass@10 & Pass@1 & Pass@10 & \\
% \midrule
% \multirow{8}{*}{LLaMA-2} 
%  & \gray{13B w/o FT}
%  & \gray{0.1768} 
%  & \gray{0.3537} 
%  & \gray{0.1768}
%  & \gray{0.3537} 
%  & \gray{1.00$\times$} \\
%  % & \gray{13B LoRA} 
%  % & \gray{0.1726}
%  % & \gray{0.3963}
%  % & \gray{0.1287}
%  % & \gray{0.3049}
%  % & \gray{0.998$\times$} \\
% &  \gray{7B LoRA}
%  & \gray{0.1524}
%  & \gray{0.2804} 
%  & \gray{0.1585} 
%  & \gray{0.2621} 
%  & \gray{1.93$\times$} \\
% & 13B \methodrand 
% & \bluetwo{0.1951} & 0.3354
% & \bluefour{0.1951} & 0.3232  & 2.17$\times$ \\
% & 13B \methodstru 
% & \blueone{0.1768} & \bluefour{0.3537} 
% & 0.1707 & 0.3171 & 2.17$\times$ \\
% & 13B \methodsemi 
% & \bluethree{0.2012} & \bluefour{0.3537}  
% & \bluefour{0.1829} & \bluefour{0.3963} & \ding{115} 1.95$\times$ \\
% & 13B \methodunst 
% & \bluefour{0.2256} & 0.3415
% & \bluefour{0.1829} & \bluethree{0.3720} & \ding{115} 2.16$\times$ \\
% % & 13B \methodunst (0.65) & 0.1189 & 0.2805 & 0.1189 & 0.2805 & \ding{115} 2.73$\times$ \\
% \midrule
% \multirow{5}{*}{LLaMA-2}
% & \gray{70B w/o FT}
% & \gray{0.3171} 
% & \gray{0.5854}
% & \gray{0.3171} 
% & \gray{0.5854}
% & \gray{1.00$\times$} \\
% % & \gray{70B LoRA} 
% % & \gray{0.3293} 
% % & \gray{0.5488} 
% % & \gray{0.2829} 
% % & \gray{0.5305} 
% % & \gray{0.998$\times$} \\
% % & \cellcolor{gray!10} 70B QLoRA 
% % & \cellcolor{gray!10}  
% % & \cellcolor{gray!10}  
% % & \cellcolor{gray!10}  
% % & \cellcolor{gray!10}  
% % & \cellcolor{gray!10} $\approx$ $\times$ \\
% & \gray{13B LoRA} 
% & \gray{0.1829} 
% & \gray{0.3598} 
% & \gray{0.1829} 
% & \gray{0.3354} 
% & \gray{5.30$\times$} \\
% % & 70B \Qmethodrand (0.75 QO) & \textbf{0.2646} & \textbf{0.5915} & \textbf{0.2646} & \textbf{0.5915} & 6.01$\times$ \\
% & 70B \textsc{\Qmethodrand} 
% & 0.2927 & 0.5732 
% & \bluefour{0.3171} & 0.5671 
% & 12.84$\times$ \\
% & 70B \Qmethodstru
% & \bluefour{0.3232} & \bluefour{0.5854} 
% & \bluefour{0.3232} & \bluefour{0.5915} & 12.84$\times$ \\
% \midrule
% % \multirow{5}{*}{LLaMA-3.1}
% & \gray{405B w/o FT} 
% & \gray{0.4793} 
% & \gray{0.8293} 
% & \gray{0.4793} 
% & \gray{0.8293} 
% & \gray{1.00$\times$} \\
% % & 405B QLoRA & 0.1701 & 0.3702 & 0.1701 & 0.3702 & 1.7$\times$ \\
% LLaMA-3.1 
% % & \gray{70B LoRA}
% % & \gray{} 
% % & \gray{} 
% % & \gray{} 
% % & \gray{} 
% % & \gray{5.70$\times$}  \\
% & \gray{70B w/o FT}
% & \gray{0.4463} 
% & \gray{0.8170} 
% & \gray{0.4463}
% & \gray{0.8170}
% & \gray{5.70$\times$}  \\
% % & 70B QLoRA & 0.2702 & 0.5703 & 0.2702 & 0.5703 & 2.3$\times$  \\
% & 405B \textsc{\Qmethodrand} & 0.4695 & 0.8293 & {} & {} & 16.00$\times$  \\
% % \midrule
% % \multirow{2}{*}{Qwen2}
% % & Qwen2-Small & 0.2001 & 0.4002 & 0.2001 & 0.4002 & 1.5$\times$ \\
% % & Qwen2-Large & 0.3003 & 0.6004 & 0.3003 & 0.6004 & 2.0$\times$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}

\subsection{Adaption to LLaMA-3.1}
% \textcolor{custommagenta}{
Here, we extend LoRAM-Stru to LLaMA-3.1 herds and investigate two key questions: (1) How does LoRAM perform in terms of perplexity and downstream tasks within this model series? (2) What is the effect of continued pre-training iteration steps (proportional to corpus size) on performance?
% }

% \textcolor{custommagenta}{
As shown in~\cref{fig:llama3.1_downstream_openhermes} (a,b), \Qmethodstru for the LLaMA-3.1-70B model exhibits consistent trends across both out-of-domain and in-domain test sets. It achieves a 15.81$\times$ parameters reduction while its perplexity falls between that of the smaller LoRA-trained 8B and LoRA-trained 70B. 
\begin{wrapfigure}[16]{r} 
{0.7\textwidth}
    \vspace{-10pt} 
    \begin{center}
    \includegraphics[width=0.7\textwidth]{fig/performance_3.1.pdf}
    \vspace{-15pt}
        \caption{The test perplexity \& downstream performance of training LLaMA-3.1-70B on OpenHermes.}
    \label{fig:llama3.1_downstream_openhermes}
    \end{center}
    % \vspace{-5pt}
\end{wrapfigure}
In downstream tasks (\cref{fig:llama3.1_downstream_openhermes} (c)), \Qmethodstru significantly exceeds the 8B w/o FT, 8B LoRA, and 70B w/o FT, even surpassing the LoRA-trained 70B on MathQA and HumanEval (Pass@10).
% }
% \textcolor{custommagenta}{
Moreover, we observe that a minimal pre-training corpus can yield substantial performance gains. For instance, \Qmethodstru \textsc{200}, with just 200 updates (about 13 million tokens), achieves a 15.81$\times$ parameter reduction alongside performance improvements. This one-time, low-cost alignment allows 
% model 
publishers to offer aligned pruned models for low-resource users to customize tasks.
% }




% \subsection{Why does naive pruning fail?}

\subsection{Necessity of Recovery \& Alignment}
\label{ssec:exp_abal}
We conduct an ablation study on two critical phases of \method: recovery and alignment. 
To assess their necessity, we analyze the convergence trends of various pruning variants on the Alpaca test set using LLaMA-2-13B.
\paragraph{Impact of Recovery.}
we compare the standard approach with an alternative setup where the pruned low-rank matrices are directly combined with the pruned full-rank model weights (w/o Recovery) and track perplexity changes over iterations. 
As shown in~\cref{fig:pruning-methods}, for all four pruning strategies, models without the recovery phase (solid lines, w/o Recovery \& *) consistently exhibit higher perplexity compared to those with recovery (dashed lines, w/ Recovery \& *), particularly in structured \method(see in~\cref{fig:pruning-methods} (a) and (b)). 
This highlights that the recovery phase leverages relatively redundant neurons during training to enhance inference performance significantly.

\paragraph{Impact of Alignment.}
We also introduce a variant of \method without continual pre-training for alignment (w/o Alignment). 
As shown in~\cref{fig:pruning-methods}, aligned pruned models (yellow lines, * \& w/ Alignment) consistently achieve lower perplexity than unaligned counterparts (blue lines, * \& w/o Alignment), irrespective of the pruning strategy or recovery phase. This highlights that even low-cost continual pre-training on a small general corpus effectively narrows the knowledge gap between pruned and original models, enhancing the overall performance of \method.



\begin{figure*}[!t]
% \vspace{-10ex}
% \setcounter{subfigure}{0}
% \setlength{\tabcolsep}{1.2mm}
\begin{center}
\includegraphics[width=\textwidth]{fig/4_1_llama-2_abla.pdf}
    \caption{Necessity of Recovery \& Alignment across different pruning strategies on LLaMA-2-13B.}
    \label{fig:pruning-methods}
\end{center}
    \vspace{-3ex}
\end{figure*}
% \subsection{Effect of Pruning Ratio}
\subsection{Scaling Laws for Parameter Reduction on \method}
We explore the impact of scaling the parameter reduction ratios in~\cref{exp:scale_prune_ratio}. The LoRA-trained LLaMA-2-13B (triangles) achieves a 5.30$\times$ parameter reduction, while \Qmethodstru maintains superior perplexity on the Alpaca and further reduces parameters across both instruction datasets.
% }
% \textcolor{custommagenta}{
In contrast, naive pruning leads to a significant increase in perplexity with minimal pruning. When the parameter reduction ratio reaches 28.56$\times$, \Qmethodstru sustains an effective perplexity of approximately 2.5, whereas naive pruning escalates to 621.98.
% }
% \textcolor{custommagenta}{
These highlight \method's ability to drastically reduce memory of the base model by updating LoRA weights in the pruned model, 
while seamlessly integrating with the full model to preserve inference performance.
% }

%1,4.82, 12.84, 16.95, 28.56
% \textcolor{custommagenta}{
We then evaluate the performance of models trained with \method on OpenHermes across various
downstream tasks under different pruning ratios. 
As shown in~\cref{fig:ratio_vs_downstream},  overall performance improves as the parameter reduction ratio increases from 9.82$\times$ to 16.95$\times$, before declining.
\begin{wrapfigure}[15]{r} 
{0.7\textwidth}
    \vspace{-10pt} 
    \begin{center}
    \includegraphics[width=0.7\textwidth]{fig/scaling_laws_for_loram.pdf}
    \vspace{-15pt}
        \caption{Effect of scaling parameter reduction ratio.}
    \label{exp:scale_prune_ratio}
    \end{center}
    % \vspace{-20pt}
\end{wrapfigure}
Notably, tasks achieve optimal performance between parameter reduction ratios of 12.84$\times$ and 16.95$\times$, consistently outperforming 13B LoRA and 70B w/o FT. 
However, at a parameter reduction ratio of 9.82$\times$, despite the larger memory capacity available for \method, downstream performance does not always exceed that of higher parameter reduction ratios. 
We attribute this to the fact that lower parameter reduction ratios fine-tune more parameters, potentially degrading the pre-trained model's performance on certain tasks  (e.g.,~\cref{fig:ratio_vs_downstream} (a,c,e)). 
This effect is also reflected in MathQA, where a fully fine-tuned LoRA model underperforms the pre-trained model without fine-tuning (see~\cref{fig:ratio_vs_downstream} (b)).
Moreover, excessive pruning at a ratio of 28.56$\times$ leaves too few neurons to capture the rich information needed for downstream improvements, particularly in tasks like code generation (see~\cref{fig:ratio_vs_downstream} (d,e)).
% }
\begin{figure*}[!hb]
\begin{center}
\includegraphics[width=\textwidth]{fig/effect_of_pruning_reduction_downstream.pdf}
    \caption{Performance of downstream tasks across different parameter reduction ratios.} %on LLaMA-2-70B
    \label{fig:ratio_vs_downstream}
\end{center}
    \vspace{-2ex}
\end{figure*}

% 接着，我们评估不同剪枝率下利用LoRAM在OpenHerms训练下的模型在下游任务上的表现。如图8，总的来说，除了MathQA外，从剪枝率从0.65到0.95，下游性能呈现先上升后下降的趋势，但当剪枝率在0.75和0.85时，所有任务上的表现都优于核心竞争场景。其中，我们发现0.65时，即使用户有更大的显存资源去执行LoRAM，但在下游的表现不一定好于更高的剪枝率。我们认为这是因为过低的剪枝率，将会微调更多参数，从而破坏原来预训练模型的性能，导致在某些下游任务上表现稍差（如图8（a,c,e）），这也可以被反应在用LoRA全量微调的模型在MathQA上差于未微调的模型(如图8 （b）)。此外过度剪枝，即剪枝率未0.95时，此时模型的知识被大部分破坏，而少量的可训练的神经元难以捕获丰富的知识去改善下游性能，如图8（d,e）这在代码生成任务上尤其明显。




% \begin{table*}[ht]
% \caption{Performance of the HumanEval in the code generation domain under zero-shot setting (temperature 0.6, $top_p$ 0.95). Metric of HumanEval: Pass@1 \& Pass@10. Offline Model FineWeb \& MathWeb CKPT1600; Online LoRA OpenHermes CKPT400. \ding{115} indicates that the theoretical parameters of unstructured pruning are reduced. However, these parameters are filled with zeros in actual training, so the memory footprint is not reduced.}
% \label{exp:code}
% % \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \setlength{\tabcolsep}{1mm}
% \begin{tabular}{@{}lccr@{}}
% \toprule
% Method (LLaMA-2)               & Pass@1           &Pass@10   &\#Para. $\downarrow$ \\
% \midrule
% \rowcolor{gray!10}
% 13B w/o FT   & 0.1451 & 0.3354 & 1.00$\times$ \\
% % 13B LoRA      & \textbf{0.1567} & 0.3415 & 1.00$\times$ \\
% % 7B w/o FT  & 0.1104 & 0.2500 & 1.93$\times$ \\
% \rowcolor{gray!10}
% 7B LoRA     & 0.1262  & 0.2561 & 1.93$\times$ \\
% 13B \method-Oracle (0.65 L2 Norm) & \textbf{0.1598} & \textbf{0.3720} & 2.17$\times$\\
% 13B \method-Oracle (0.65 Q Up) & 0.1390 & \underline{0.3598} & 2.17$\times$\\
% 13B \method-Oracle (0.65 Q Gate) & 0.1537 & 0.3476 & 2.17$\times$\\
% 13B \method-Oracle (0.65 Q Down) & 0.1378 & 0.3415 & 2.17$\times$\\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}

% \begin{table*}[ht]
% \caption{Performance of the MathQA \& GSM8K in the mathematical domain under the CoT (default 8-shots) setting. Metric of MathQA: Acc (Accuracy). Filter-Metric of GSM8K: SM-EM (Strict Match-Exact Match); FE-EM (Flexible Extract-Exact Match). Offline Model FineWeb \& MathWeb CKPT1600; Online LoRA OpenHermes CKPT400. \ding{115} indicates that the theoretical parameters of unstructured pruning are reduced. However, in actual training, these parameters are filled with zeros, so the memory footprint is not reduced.}
% \label{exp:math}
% % \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \setlength{\tabcolsep}{1.2mm}
% \begin{tabular}{@{}lcccr@{}}
% \toprule
% Method (LLaMA-2)               & MathQA (Acc)           &GSM8K (SM)  &GSM8K (FE)   &  \#Para. $\downarrow$ \\
% \midrule
% \rowcolor{gray!10}
% 13B w/o FT   & 0.3293$\pm$0.0086 & 0.2335$\pm$0.0117 & 0.2729$\pm$0.0123 & 1.00$\times$ \\
% % 7B w/o FT & 0.2995$\pm$0.0084 & 0.1357$\pm$0.0094 & 0.1448$\pm$0.0097 & 1.93$\times$ \\
% \rowcolor{gray!10}
% 7B LoRA     & 0.2995$\pm$0.0084 & 0.2206$\pm$0.0114 & 0.2411$\pm$0.0118 & 1.93$\times$ \\
% 13B \method-Oracle (0.65 L2 Norm) 
% & \underline{0.3390$\pm$0.0087} 
% & 0.2896$\pm$0.0125 
% & 0.3252$\pm$0.0129 & 2.17$\times$\\
% 13B \method-Oracle (0.65 Q Up) 
% & 0.3290$\pm$0.0086
% & 0.2600$\pm$0.0121
% & 0.3222$\pm$0.0129 & 2.17$\times$\\
% 13B \method-Oracle (0.65 Q Gate) 
% & 0.3323$\pm$0.0086
% & \underline{0.2972$\pm$0.0126} 
% & 0.3374$\pm$0.0130 & 2.17$\times$\\
% 13B \method-Oracle (0.65 Q Down) 
% & 0.3410$\pm$0.0087
% & 0.2532$\pm$0.0120
% & 0.3169$\pm$0.0128 & 2.17$\times$\\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}

% \begin{table*}[ht]
% \caption{Performance of the CSR in the natural language understanding domain under the 1-shot setting. Metric of CSR: Accuracy. Offline Model FineWeb \& MathWeb CKPT1600; Online LoRA OpenHermes CKPT400. \ding{115} indicates that the theoretical parameters of unstructured pruning are reduced. However, these parameters are filled with zeros in actual training, so memory footprint is not reduced.}
% \label{exp:csr}
% % \vskip 0.15in
% \begin{center}
% \begin{tiny}
% \begin{sc}
% \setlength{\tabcolsep}{0.3mm}
% \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{@{}lcccccccr@{}}
% \toprule
% Method (LLaMA-2)               & Arc Challenge  & Arc Easy  & HellaSwag  & OpenBookQA & PIQA &WinoGrande  & Mean &  \#Para. $\downarrow$ \\
% \midrule
% \rowcolor{gray!10}
% 13B w/o FT 
% &0.5239$\pm$0.0146 
% &0.8131$\pm$0.0080 
% &0.6029$\pm$0.0049
% &0.3720$\pm$0.0216
% &0.8014$\pm$0.0093 
% &0.7388$\pm$0.0123
% &0.6420$\pm$0.0129
% & 1.00$\times$ \\
% % 7B w/o FT & 0.2995$\pm$0.0084 & 0.1357$\pm$0.0094 & 0.1448$\pm$0.0097 & 1.93$\times$ \\
% \rowcolor{gray!10}
% 7B LoRA     
% &0.4915$\pm$0.0146 &0.7866$\pm$0.0084 &0.5743$\pm$0.0049 &0.3320$\pm$0.0211 &0.7943$\pm$0.0094 &0.7135$\pm$0.0127 &0.6154$\pm$0.0129 & 1.93$\times$ \\
% 13B \method-Oracle (0.65 L2 Norm) 
% &0.4957$\pm$0.0146 &0.8013$\pm$0.0082 &0.6060$\pm$0.0049 &0.3640$\pm$0.0215 &0.8009$\pm$0.0093 &0.7372$\pm$0.0124
% &0.6342$\pm$0.0130
% & 2.17$\times$ \\
% 13B \method-Oracle (0.65 Q Up)
% &0.5307$\pm$0.0146
% &\textbf{0.8224$\pm$0.0078} 
% &0.6072$\pm$0.0049 
% &\textbf{0.3840$\pm$0.0218} 
% &0.8020$\pm$0.0093 
% &0.7364$\pm$0.0124
% &\textbf{0.6471$\pm$0.0130}
% & 2.17$\times$ \\
% 13B \method-Oracle (0.65 Q Gate) 
% &\underline{0.5307$\pm$0.0146}
% &0.8093$\pm$0.0081 
% &0.6058$\pm$0.0049 
% &\underline{0.3760$\pm$0.0217} 
% &0.7976$\pm$0.0094 
% &0.7348$\pm$0.0124
% &0.6423$\pm$0.0130
% & 2.17$\times$ \\
% 13B \method-Oracle (0.65 Q Down)
% &0.5265$\pm$0.0146
% &0.8161$\pm$0.0079 
% &0.6055$\pm$0.0049 
% &0.3720$\pm$0.0216 
% &0.8025$\pm$0.0093 
% &0.7364$\pm$0.0124
% &0.6432$\pm$0.0130
% & 2.17$\times$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{tiny}
% \end{center}
% \vskip -0.1in
% \end{table*}

