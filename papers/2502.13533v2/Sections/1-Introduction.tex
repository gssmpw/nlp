
\section{Introduction}
Large language models (LLMs), such as GPT-4~\citep{openai:2023gpt4}, LLaMA~\citep{Hugo:2023llama,Hugo:2023llama2,meta2024llama3}, and PaLM~\citep{Aak:2023palm}, have recently revolutionized natural language applications. 
These models excel in task generalization, driven by their exponential increase in scale, with some exceeding 400 billion parameters~\citep{meta2024llama3}.
Fine-tuning pre-trained LLMs is critical for task-specific customization, enhancing desired behaviors while mitigating undesired ones~\citep{qi2024finetuning}.
However, this process is constrained by substantial memory requirements; 
%zj
% \textcolor{blue}{
for instance, fine-tuning a 70B LLaMA in 16-bit precision demands over 1178GB\footnote{
% \textcolor{blue}{
Training is performed on one sample with a length of 4K using \texttt{BF16} mixed precision with the Adam optimizer, incorporating gradient checkpointing.}
% } 
of memory, necessitating an expensive setup of 15 GPUs (A100 or H100, each with 80GB HBM).
% }
%model 1
%grad 1 （FP） BF(2)
%copy 2
%adam 2+2 mix-pretrain FSDP 不开offloading

%32 actiavtion 2 *17 4k 开了checkpoint，
% hs=8192 * 4096(input len) * 2Byte(BF/FP) * 80 (n_layer) = 5G 一条数据
% %BUAK 没开*17(不确定)=85G 激活值重计算
% Deepspeed BF 梯度 FP32
% FP16 梯度 16
% megtro
To mitigate the high cost of fine-tuning LLMs, parameter-efficient fine-tuning~\citep{Prefix2021,Brian:2021PT,P-Tuning2021,Qiu:OFT2023,liu2024boft,Liu:2022IA3}, particularly Low-Rank Adaption (LoRA)~\citep{Edw:2022lora} and its variants~\citep{liu2024dora,ding2023sora,zi2023deltalora,zhang2023adalora,kala2023rslora}, freezes the original LLM weights and only updates the injected low-rank matrices to adapt to new tasks under limited resources.
% Moreover, some LoRA variants~\citep{zhou:2024loradrop,zhang2023lorafa,kopiczko2024vera,azizi2024lamda,wang2024prolora} have attempted to reduce the increased memory overhead of these learnable matrices as model sizes grow.
However, during training, they still struggle with the significant memory footprint of the parameters of the base LLM, even with quantization~\citep{Tim:2023qlora,Xu:2023QALoRA,li2024loftq,guo2024lqlora,OPTQ2023,chai2023int21}.
Typically, they reduce the precision to 4 bits at most due to quality considerations.
This memory dilemma raises an interesting and necessary question:

% \textcolor{blue}{
\textit{
% Can we lift the Damocles sword of the original model's memory cost that hangs over LoRA training, paving the way for a more memory-efficient and performance-effective scheme?
% Can the original model's memory cost, which hangs like a Damocles sword over LoRA training, be lifted to enable a memory-efficient yet performance-effective scheme?
Can we further reduce the memory overhead of the base model during LoRA training while still maintaining the inference accuracy?
}
% jue: this needs more work. 
% }
% Can we design a more memory-efficient LoRA training scheme for cost-effective performance gains?

Our answer is a resounding \textit{Yes}!
In this paper, we propose \textbf{M}emory-efficient \textbf{LoRA} training, coined \method, 
a novel scheme to reduce the memory overhead of LoRA fine-tuning for LLMs.
We revisit the training and inference process of the LoRA paradigm, building on it with a unique twist:
Unlike typical LoRA, which uses the same original model for training and inference, \method employs different models at each stage, i.e.,~it trains a \textit{pruned (small)} model by updating the pruned LoRA weights and then performs inference on the \textit{original (large)} model with the 
% dimensionally 
% \textcolor{blue}{
recovered low-rank matrices.
% }
%zj To be considered
% \textcolor{blue}{
This recovery process reshapes the pruned matrices to ensure be merged into the original model, allowing for the updating of unpruned weights while utilizing the pruned weights during inference.
% }
The key insight driving our approach comes from reconciling two seemingly contradictory concepts.
% The scaling laws~\citep{2022scalinglaw1,2020scalinglaw2,2021scalinglaw3} indicate that all parameters of a pre-trained LLM (e.g.,~LLaMA-2 and LLaMA-3 herds of models~\citep{meta2024llama3}) guided by them are essential for maximizing model generalization. 
The scaling laws~\citep{2022scalinglaw1,2020scalinglaw2,2021scalinglaw3} suggest that a large number of parameters of LLMs is essential for effective model generalization.
\begin{wrapfigure}[15]{r} 
{0.30\textwidth}
% \vskip 0.2in
    \vspace{-15pt} % Adjust vertical space as needed
    \begin{center}
\includegraphics[width=0.30\textwidth]{fig/insight.pdf}
\vspace{-20pt}
        \caption{Idea of \method}
        \label{fig:idea}
    \end{center}
    % \vskip 0.2in
    \vspace{-10pt} % Adjust vertical space as needed
    
\end{wrapfigure}
Conversely, sparsity in LLMs ~\citep{zhang2024dynamic,ma2023llmpruner,sun2024a,FrantarA23spasegpt,xia2024sheared} show that these pre-trained models can be compressed by removing redundant weights. 
The goal is to minimize the difference in the model's output before and after pruning.
%zj To be considered
% \textcolor{blue}{However, such methods tend to falter at higher pruning rates, as the redundancy in inference is relatively limited, and aggressively pruned models lose critical inference capacity.}
% \textcolor{blue}{
However, such methods tend to falter at higher pruning ratios and aggressively pruned models lose critical reasoning capabilities, e.g., only pruning 10\%$\sim$20\%~\citep{ma2023llmpruner,FrantarA23spasegpt}.
% }
% Our intuition builds on this: fine-tuning pruned parameters by updating low-rank matrices is less cost-effective, yet these parameters remain crucial for improving performance during inference---they are relatively redundant for training but still essential for inference. 
Our intuition builds on this: 
%To be refined
% \textcolor{blue}{
some critical parameters contribute significantly to fine-tuning, while other parameters, though essential for inference, usually remain unchanged during fine-tuning.
% }
% fine-tuning pruned parameters with low-rank matrix updates provides limited information injection, leading to diminishing returns compared to full-parameter tuning. However,  these parameters, though seemingly redundant during LoRA training, remain essential for improving performance in inference.
Therefore, \method leverages this insight by updating the weights retained through pruning from LoRA training (yellow blocks in~\cref{fig:idea}) to significantly reduce memory usage and training time, while employing the pruned weights (blue blocks in~\cref{fig:idea}) during inference to enhance generation performance
(see~\cref{ssec:exp_abal}).
% Figure 1 shows the perplexity performance of pruned LoRA and recovered LoRA on Alpaca, demonstrating the feasibility and effectiveness of our approach.


Despite the significant 
% performance leap 
% \textcolor{blue}{reduction in memory cost}
reduction in memory cost
achieved by the pruning-recovery process of \method, maintaining gains at more aggressive pruning rates (e.g., 65\% or higher) remains challenging. We attribute this to the knowledge inconsistency between the pruned model used for training and the original model used for inference. To address this, we propose an effective alignment strategy: low-cost continual pre-training of the pruned model on a small dataset.
% \textcolor{blue}{dataset.}
% amount of general corpus.
% via teacher-forcing~\citep{bachmann2024teacherforce}. 
This alignment is performed once offline, allowing the model's publisher to execute it. For instance, Meta AI could release a set of aligned pruned models for LLaMA-3, enabling low-resource users to fine-tune large models for customized tasks using \method.
Notably, as a bonus, \method seamlessly integrates with existing quantization schemes designed for LoRA, such as QLoRA, forming \Qmethod, which further reduces memory overhead.

The contributions of this work are summarized as follows: 

\begin{itemize}[leftmargin=20pt]
\item[(1)]
\textit{Novel Training Scheme}: We propose \method, a memory-efficient LoRA training scheme. \method trains a pruned model by updating the pruned low-rank matrices and then uses dimensionally recovered low-rank matrices to integrate with the original model for inference. The process significantly 
% saves memory during training by reducing the model parameters 
% \textcolor{blue}{
reduces the memory consumption incurred by the model parameters during training,
% }
and synergistically boosts performance by leveraging the full original parameters during inference. Thus, \method efficiently enhances performance under limited device memory resources.

\item[(2)]\textit{Effective Alignment Strategy}: We identify that the knowledge inconsistency between the pruned model used for training and the original model used for inference limits the 
% robust 
performance gain of \method under aggressive pruning rates. To tackle this, we train the pruned model on a small amount of general corpus 
% via teacher-forcing 
to achieve alignment, which is a one-shot offline process and can be easily performed by the model publisher.

\item[(3)]\textit{Extensive Experimental Evaluation}: We conduct comprehensive experiments to validate the effectiveness of \method across various pruning algorithms, models of different sizes, and tasks in different domains. Notably, \Qmethod which combines \method with 
% random 
structured pruning and 4-bit quantization reduces the memory cost of LLaMA-2-70B parameters by 16.95$\times$ while 
% efficiently 
effectively
achieving performance gains superior to both the original LLaMA-2-70B and LLaMA-2-13B fine-tuned with LoRA.
\end{itemize}

