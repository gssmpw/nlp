\section{Related Work}
\paragraph{Low-Rank Adaptation.} 
LoRA (Low-Rank Adaptation)~\citep{Edw:2022lora} has emerged as a prominent technique for parameter-efficient fine-tuning (PEFT)~\citep{Prefix2021,Brian:2021PT,P-Tuning2021,Qiu:OFT2023,liu2024boft,Liu:2022IA3}. 
By injecting lightweight, trainable low-rank decomposition matrices into frozen pre-trained weights, LoRA enables efficient task customization, especially in resource-constrained settings.
Some LoRA variants~\citep{liu2024dora,ding2023sora,zi2023deltalora,zhang2023adalora,kala2023rslora} have been developed to enhance its generalization and robustness,
while others~\citep{zhou:2024loradrop,zhang2023lorafa,kopiczko2024vera,azizi2024lamda,wang2024prolora} address the increased memory overhead associated with scaling up model sizes.
However, during training, these efficient LoRA variants still struggle with the substantial memory footprint of the original LLM parameters.

\paragraph{LoRA-related Compression.}
Model compression techniques like quantization~\citep{han2015deep, jacob2018quantization,nagel2019data,zhao2019improving,yao2022zeroquant,park2022nuqmm,dettmers2022llm,xiao2022smoothquant,frantar2022gptq}, sparsification~\citep{molchanov2016pruning,liu2018rethinking,he2019filter,hoefler2021sparsity,frantar2023massive,liu2023deja,bansal2022rethinking}, and distillation~\citep{hinton2015distilling,cho2019efficacy,tang2019distilling,touvron2021training,Hsieh:2023distill,gu2024minillm} have proven effective in reducing the memory footprint of LLM during training and inference.
Naturally, the concept of compression has been adapted to LoRA to alleviate the substantial memory consumption dominated by pre-trained model parameters.
In particular, LoRA-related quantization schemes~\citep{Tim:2023qlora,Xu:2023QALoRA,li2024loftq,guo2024lqlora,OPTQ2023,chai2023int21} have been widely explored, but they still face the limitations of 1-bit precision, typically quantize weights to 4-bit to balance training efficiency with performance.  
Our work aims to push the boundaries of memory-efficient LoRA training by leveraging sparsification to achieve cost-effective performance improvements. 
Notably, existing LoRA-related sparsification works~\citep{chen2023lorashear,zhang2024loraprune} focus on designing pruning algorithms to slim down models and use LoRA to recover the knowledge of pruned models, thereby producing compact but high-quality models. In contrast, \method enables effective general pruning under high base-model sparsity, whereas~\citep{gu-etal-2024-light} focuses on task-specific LoRA sparsification with limited impact on base model memory reduction.

