\section{Experimental Details}
\label{apd:detail_setup}
\paragraph{Pre-train Corpus.}
To align the inconsistent knowledge between the pruned model during training and the original model during inference, we apply \method to continual pre-training 
LLMs in a teacher-forcing manner~\citep{bachmann2024teacherforce}
on a mixed corpus of FineWeb~\citep{penedo2024fineweb} and OpenWebMath~\citep{paster2023openwebmath}.
FineWeb, containing over 15TB of cleaned and deduplicated English web data from Common Crawl. 
OpenWebMath, extracted from over 200 billion HTML files on Common Crawl, provides high-quality mathematical text. Mixing these datasets enhances the pruned model's capabilities in both general and mathematical domains.

Unless specified otherwise, we randomly sample 102,400 instances from both FineWeb and OpenWebMath to construct a mixed dataset with a sequence length of 512, yielding approximately 105 million tokens. The default training batch size is 128, allowing up to 1,600 update steps. We train without data repetition over a sufficiently large corpus to simulate a realistic pre-training scenario. 
Notably, this alignment process is a one-time, offline operation that model publishers can execute.

\paragraph{Fine-tuning Data.}
Following the fine-tuning scenario of LoRA~\citep{Edw:2022lora}, we primarily conduct supervised fine-tuning (SFT) on the OpenHermes-2.5~\citep{OpenHermes} (referred to as OpenHermes). OpenHermes is a large-scale dataset constructed from synthetically generated instructions and chat samples, encompassing diverse sources such as Airoboros 2.2~\citep{wang2023selfinstructaligning}, CamelAI Domain Expert Dataset~\citep{li2023camel}, ChatBot Arena (GPT-4 Only)~\citep{zheng2023lmsyschat1m}, and more.
To further demonstrate the general effectiveness of the \method alignment process, we also evaluate \method on the OpenOrca~\citep{OpenOrca} dataset. OpenOrca is a widely used instruction fine-tuning dataset where each data instance represents entries from the FLAN collection~\citep{longpre2023flan}, augmented by submitting the listed questions to either GPT-4 or GPT-3.5.

By default, we train SFT on the instruction dataset with a batch size of 128 and a sequence length of 512 for 400 steps, totaling approximately 26.2 million tokens. 
To effectively evaluate the overall fine-tuning performance, we assess the perplexity of the fine-tuned model on an out-of-domain test set. This out-of-domain test set is constructed by randomly sampling 2,000 instances from the Alpaca~\citep{alpaca} test set, truncated to a sequence length of 512.

\paragraph{Downstream Task.}
We focus on the performance of \method in various downstream tasks, including mathematical reasoning, common sense reasoning, and code generation. All our downstream task evaluations are performed on lm-evaluation-harness\footnote{~\url{https://github.com/EleutherAI/lm-evaluation-harness} (MIT License).} and code-eval~\footnote{~\url{https://github.com/abacaj/code-eval} (MIT License).} with VLLM~\footnote{~\url{https://github.com/vllm-project/vllm} (Apache-2.0 license).}.

For mathematical reasoning, we benchmark the accuracy of baseline models using greedy decoding on MathQA~\citep{amini-etal-2019-mathqa} with a 1-shot setting and GSM8K (Grade School Math 8K)~\citep{cobbe2021gsm8k} with 8-shots, Chain of Thought (CoT) prompting and strict match
% \footnote{
% % The evaluation script 
% Follow~\url{https://github.com/EleutherAI/lm-evaluation-harness} (MIT License).}.
MathQA is a large-scale dataset comprising 37k English multiple-choice math word problems, covering diverse math domains. It extends the AQuA-RAT dataset~\citep{ling2017program} by annotating problems with fully specified operational programs using a new representation language, building on the questions, options, rationale, and correct answers provided by AQuA-RAT.
The GSM8K is a dataset of 8.5K high-quality, linguistically diverse grade school math word problems, designed to evaluate multi-step reasoning in basic arithmetic operations (+-×÷). We conduct evaluations on its 1.3K test set with \textit{strict-match} to assess logical and mathematical reasoning in language models.

For commonsense reasoning (CSR), we report the average accuracy across six tasks—Arc Challenge \& Easy~\citep{clark2018arc}, HellaSwag~\citep{zellers-etal-2019-hellaswag}, OpenBookQA~\citep{OpenBookQA2018}, PIQA~\citep{Bisk2020}, and WinoGrande~\citep{WinoGrande2021}—under 1-shot and greedy decoding settings. These benchmarks comprehensively assess the model’s ability to apply ``commonsense" or world knowledge for reasoning, rather than relying on pattern recognition.

For code generation, we compare two pass rates, \textsc{Pass@1} and \textsc{Pass@10}~\citep{kulal2019spoc}, on HumanEval~\citep{chen2021evaluating} of each baseline in a zero-shot setting with sampling parameters of  $\textsc{temperature}=\{0.0,0.2,0.4,0.6,0.8\}$, and $\textsc{top}_\textsc{p} =0.95$.
The HumanEval dataset released by OpenAI consists of 164 handwritten Python programming problems, each with a function signature, docstring, body, and unit tests. Serving as a benchmark, HumanEval assesses models on a range of Python coding skills, from basic syntax to complex problem-solving, offering insights into their programming capabilities alongside language-focused tasks.

\paragraph{Sparsification \& Quantization.}
\method incorporates two model compression techniques: sparsification, which generates a pruned model for low-rank matrix updates, and quantization, which forms \Qmethod further to reduce the memory footprint of the pruned model.
For sparsification, 
to validate the general effectiveness of \method, we benchmark its performance across various pruning strategies $\mathtt{P}(\cdot)$. 
Specifically, we first establish a variant using randomly structured pruning and adapt \method to another three variants based on leading approaches: the structured pruning LLM-Pruner\footnote{\url{https://github.com/horseee/LLM-Pruner} (Apache-2.0 license)}~\citep{ma2023llmpruner} and the non-structured (semi-structured \& unstructured) pruning SparseGPT\footnote{\url{https://github.com/IST-DASLab/sparsegpt} (Apache-2.0 license)}~\citep{FrantarA23spasegpt}. 
These baselines are summarized below, with the corresponding configurations presented in~\cref{tab:llama2_13b,tab:llama2_70b,tab:llama2_70b_q}.
\begin{itemize}[leftmargin=20pt]
    \item \textbf{\methodrand}: 
    We adhere to the pruning settings of \methodstru, modifying only by randomly removing weights instead of the original gradient-based pruning criterion.
    \item \textbf{\methodstru}: 
    We follow LLM-Pruner and employ a block-wise strategy for local structured pruning. Attention and MLP layers are treated as separate blocks, with non-critical coupling weights pruned based on gradient information at a uniform ratio. We retain the first four and last two layers of both blocks, focusing pruning on the intermediate layers.
    \item \textbf{\methodsemi}: 
    We utilize SparseGPT with a 4:8 semi-structured sparsity pattern to prune pre-trained weights across all model layers.
    \item \textbf{\methodunst}:  
    We prune individual weights uniformly across layers using a predefined pruning ratio based on an unstructured version of SparseGPT.
\end{itemize}

For quantization $\mathtt{Q}(\cdot)$, to further reduce memory usage during training, especially when dealing with models exceeding 70 billion parameters, we achieve \Qmethod by combining \method with the LoRA-tailored quantization algorithm QLoRA~\citep{Tim:2023qlora}. While \method is compatible with the quantization of other customized LoRA methods~\citep{Xu:2023QALoRA,li2024loftq,guo2024lqlora,OPTQ2023,chai2023int21}, this falls outside the scope of this article.

\paragraph{Architecture \& Hyperparameters.}
We adopt a LLaMA architecture with RMSNorm~\citep{ZhangS19a} and SwiGLU
activations~\citep{Noglu,ZhaoSA22}. 
We run all experiments with BF16 format to reduce memory usage.
For all configurations, we default to a learning rate of 1e-3. However, the downstream performance of models fine-tuned on OpenOrca is relatively sensitive to the learning rate. Therefore, in this evaluation, we tune the learning rates for each baseline within the range of [1e-5, 1e-3] and report their respective optimal downstream scores. Specifically, we use 1e-5 for the 7B LoRA and 13B \& 70B LoRAM models, and 1e-4 for the 13B LoRA model.
All experiments run on NVIDIA A100-80GB GPUs with environments of CUDA 12.2, PyTorch 2.4.0, and Transformer 4.45.1.
For LLaMA-2 herds, we set low-rank matrices $\mathbf{B}$ and $\mathbf{A}$ of rank $r=8$ for $\mathbf{W}_\text{q}$, $\mathbf{W}_\text{k}$, $\mathbf{W}_\text{v}$, and $\mathbf{W}_\text{o}$ in the attention layer, $\mathbf{W}_\text{up}$, $\mathbf{W}_\text{gate}$, and $\mathbf{W}_\text{down}$ in the MLP layer, and the head embedding matrix $\mathbf{W}_\text{lm\_head}$;
for LLaMA-3 herds, we exclude the injection of the low-rank matrix of $\mathbf{W}_\text{lm\_head}$.
\begin{table*}[h]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{3pt}

    \caption{LoRAM configures on LLaMA-2-13B. Comparison of different pruning methods in terms of parameter reduction ratio (Reduction) and HBM footprint (GB) of pruned parameters (HBM), ignoring low-rank matrix overhead.}
    \label{tab:llama2_13b}
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        Method & \#Orig. Params & Pruning Ratio & \#Pruned Params & Reduction & HBM  \\ 
        \midrule
        LoRAM-Semi & 13015864320 & 0.50 & 6738415616 & 1.93$\times$ & 12.55 \\
        LoRAM-Unst & 13015864320 & 0.55 & 6037628912 & 2.16$\times$ & 11.25 \\
        LoRAM-Rand \& Stru & 13015864320 & 0.65 & 6005662720 & 2.17$\times$ & 11.19 \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[h]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{3pt}

    \caption{LoRAM configures on LLaMA-2-70B and LLaMA-3.1-70B with different pruning ratios.}
    \label{tab:llama2_70b}
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        Method & \#Orig. Params & Pruning Ratio & \#Pruned Params & Reduction & HBM \\ 
        \midrule
        LoRAM-Rand \& Stru & 68976648192 & 0.65 & 28099436544 & 2.45$\times$ & 52.34 \\
        LoRAM-Rand \& Stru & 68976648192 & 0.75 & 21488738304 & 3.21$\times$ & 40.03 \\
        LoRAM-Rand \& Stru & 68976648192 & 0.85 & 16272924672 & 4.24$\times$ & 30.31 \\
        LoRAM-Rand \& Stru & 68976648192 & 0.95 & 9662226432 & 7.14$\times$ & 18.00 \\
        LoRAM-Rand \& Stru & 70553706496 & 0.85 & 17849982976 & 3.95$\times$ & 33.25 \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[h]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{3pt}

    \caption{QLoRAM configures on LLaMA-2-70B and LLaMA-3.1-70B with , demonstrating more aggressive parameter compression.}
    \label{tab:llama2_70b_q}
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        Method & \#Orig. Params & Pruning Ratio & \#Pruned Params & Reduction & HBM \\ 
        \midrule
        QLoRAM-Rand \& Stru & 68976648192 & 0.65 & 7024859136 & 9.82$\times$ & 13.08 \\
        QLoRAM-Rand \& Stru & 68976648192 & 0.75 & 5372184576 & 12.84$\times$ & 10.01 \\
        QLoRAM-Rand \& Stru & 68976648192 & 0.85 & 4068231168 & 16.95$\times$ & 7.58 \\
        QLoRAM-Rand \& Stru & 68976648192 & 0.95 & 2415556608 & 28.56$\times$ & 4.50 \\
        QLoRAM-Rand \& Stru & 70553706496 & 0.85 &  4462495744 & 15.81$\times$ & 8.31 \\
        \bottomrule
    \end{tabular}
\end{table*}










\clearpage
\section{Visualization of Dimension Evolution}
\label{sec:dimension_vis}
To clearly illustrate the evolution of weight matrix dimensions across the multiple stages in the proposed scheme, we take LLM-Pruner~\citep{ma2023llmpruner} as an example in (e.g.,~\methodstru) in~\cref{fig:dimension_vis}, visualizing the transformation from $\mathbf{W}_{0} \Rightarrow \mathbf{W}_{0}^\mathtt{P}$, $\mathbf{W}_{\Delta} \Rightarrow \mathbf{W}_{\Delta}^\mathtt{P}$, and $\mathbf{W}_{\Delta}^{\mathtt{P}^{\star}}\Rightarrow \mathbf{W}_{\Delta}^{\mathtt{R}^{\star}}$ under \method with structured pruning. 
For \method variants employing non-structured pruning, the parameter dimensionality remains unchanged during training due to the use of a mask matrix. Therefore, these visualizations are omitted.
\begin{figure*}[ht]
\begin{center}
\includegraphics[width=\textwidth]{fig/dimension_evaluation_final.pdf}
\caption{
Dimensional evolution of the weight matrices: $\mathbf{W}_{0} \Rightarrow \mathbf{W}_{0}^\mathtt{P}$ (a), $\mathbf{W}_{\Delta} \Rightarrow \mathbf{W}_{\Delta}^\mathtt{P}$ (b), and $\mathbf{W}_{\Delta}^{\mathtt{P}^{\star}} \Rightarrow \mathbf{W}_{\Delta}^{\mathtt{R}^{\star}}$ (c) during \methodstru training. This includes updates for $\mathbf{W}_\text{q}$, $\mathbf{W}_\text{k}$, $\mathbf{W}_\text{v}$, and $\mathbf{W}_\text{o}$ in the attention layer, as well as $\mathbf{W}_\text{up}$, $\mathbf{W}_\text{gate}$, and $\mathbf{W}_\text{down}$ in the MLP layer.
}
\label{fig:dimension_vis}
\end{center}
\end{figure*}


\clearpage
\section{Visualization of Low-rank Matrices}
\label{apd:vis_matrix}
In this section, we utilize the \(L_{2}\)-norm to evaluate variations in low-rank matrices trained with different \method variants. This metric facilitates the visualization of captured features and allows for an analysis of \method's effectiveness. Specifically, we examine the updated low-rank matrices in the self-attention and MLP layers of LLaMA-2-13B and LLaMA-2-70B, trained with \method variants on OpenHermes.

\subsection{Head-wise Norm of Attention}
For the low-rank matrices in the attention layer, denoted as \(\mathbf{W}_{\Delta^{*}}\) where \({*} \in \{\text{q}, \text{k}, \text{v}, \text{o}\}\), we compute the \(L_{2}\) norms for each attention head. Let \(\text{H}^{*}\) represent the number of heads. The \(L_{2}\) norms for each head \(\text{h}\) (where \(\text{h} = 0, 1, \ldots, \text{H}^{*} - 1\)) are defined as follows:

\begin{equation}
\| \mathbf{W}_{\Delta^{*}}^{(h)} \|_2 = 
\begin{cases}
\left\| \mathbf{W}_{\Delta^{*}}[h, :] \right\|_2 & \text{if } {*} \in \{\text{q}, \text{k}, \text{v}\} \\
\left\| \mathbf{W}_{\Delta^{*}}[:, h] \right\|_2 & \text{if } {*} = \text{o}
\end{cases}.
\end{equation}

The results are visualized through heatmaps in ~\cref{fig:attn_lora_vis_13B,fig:attn_lora_vis_70B}, effectively illustrating the distribution of features captured by different attention heads.

\subsection{Layer-wise Norm of MLP}
For the low-rank matrices in the MLP layers, denoted as \(\mathbf{W}_{\Delta^{*}}\) where \(\Delta^{*} \in \{\text{up}, \text{gate}, \text{down}\}\), we denote the number of layers as \(\text{L}\). The average \(L_{2}\) norm for a specific layer \(l\) (where \(l = 0, 1, \ldots, \text{L} - 1\)) is computed as follows, excluding elements equal to zero using a mask, ensuring that only active parameters contribute to the average:

\begin{equation}
\| \mathbf{W}_{\Delta^{*}}^{(l)} \|_2 = 
\begin{cases}
\frac{1}{m} \sum_{i=0}^{m-1} \left\| \mathbf{W}_{\Delta^{*}}^{(l)}[i, :] \right\|_2 \cdot \mathbb{I}(\mathbf{W}_{\Delta^{*}}^{(l)}[i, :] \neq 0) & \text{if } \Delta^{*} \in \{\text{up}, \text{gate}\} \\
\frac{1}{n} \sum_{j=0}^{n-1} \left\| \mathbf{W}_{\Delta^{*}}^{(l)}[:, j] \right\|_2 \cdot \mathbb{I}(\mathbf{W}_{\Delta^{*}}^{(l)}[:, j] \neq 0) & \text{if } \Delta^{*} = \text{down}
\end{cases}.
\end{equation}

Here, \(\mathbb{I}(\cdot)\) denotes the indicator function, which returns 1 only when the corresponding element is non-zero, effectively excluding zero elements from the average calculation. The average norms for the MLP layers are visualized in ~\cref{fig:mlp_lora_vis_13B,fig:mlp_lora_vis_70B}, clearly depicting the trends in updating amplitudes across the various projections.

\subsection{Attention Update Patterns}

\paragraph{Layer Update Patterns in \method and LoRA.} \cref{fig:attn_lora_vis_13B,fig:attn_lora_vis_70B} reveal that both LoRA and \method display similar layer update behaviors. In any low-rank matrix \(\mathbf{W}_{\Delta^{*}}\) where \({*} \in \{\text{q}, \text{k}, \text{v}, \text{o}\}\), deeper colors predominantly concentrate in either shallow or deep layers, while middle layers receive relatively few updates. This suggests that training primarily focuses on optimizing the shallow layers to capture semantic information, with deeper layers refining this knowledge, rendering middle layers somewhat redundant.

\paragraph{More Uniform Projection Updates in \method.} \cref{fig:attn_lora_vis_13B,fig:attn_lora_vis_70B} further indicates that updates in the LoRA-trained low-rank matrices, particularly for \(\mathbf{W}_{\Delta^{\text{v}}}\), are relatively uniform, exhibiting substantial deep colors across multiple heads. In contrast, other matrices emphasize specific rows and heads. For instance, in the 70B model's \(\mathbf{W}_{\Delta^{\text{k}}}\), only the heads in the uppermost layers experience significant updates, while lower layers show minimal changes. This suggests that the unpruned model retains rich knowledge, requiring only minor adjustments to a few heads in certain layers for task adaptation. Conversely, \method demonstrates a more uniform distribution of deep colors across each low-rank matrix, indicating that the pruned model must effectively utilize every limited neuron to capture knowledge, thereby enhancing downstream performance.

\begin{figure*}[!ht]
\begin{center}
\includegraphics[width=\textwidth]{fig/l2_norms_attn_13B.pdf}
    \caption{Visualization of low-rank matrices in the attention layers of LLaMA-2-13B.}
    \label{fig:attn_lora_vis_13B}
\includegraphics[width=\textwidth]{fig/l2_norms_mlp_13B.pdf}
    \caption{Average \(L_{2}\) norms of low-rank matrices in the MLP layers of LLaMA-2-70B.}
    \label{fig:mlp_lora_vis_13B}
\end{center}
\end{figure*}

\begin{figure*}[!t]
\begin{center}
\includegraphics[width=\textwidth]{fig/l2_norms_attn_70B.pdf}
    \caption{Visualization of low-rank matrices in the attention layers of LLaMA-2-70B.}
    \label{fig:attn_lora_vis_70B}
\includegraphics[width=\textwidth]{fig/l2_norms_mlp_70b.pdf}
    \caption{Average \(L_{2}\) norms of low-rank matrices in the MLP layers of LLaMA-2-70B.}
    \label{fig:mlp_lora_vis_70B}
\end{center}
\end{figure*}

\subsection{MLP Update Patterns}

\paragraph{\method Exhibits Greater Update Amplitude than LoRA.} For both the 13B and 70B models, \method consistently exhibits a greater update amplitude across each layer compared to LoRA, as shown in~\cref{fig:mlp_lora_vis_13B,fig:mlp_lora_vis_70B}. This increased amplitude indicates that \method is more effective in adjusting the weights in all layers, thus enhancing the adaptability and overall performance.
% \clearpage
\paragraph{Distinct Update Trends in Layer Amplitudes.} The amplitude changes reveal a distinct pattern in~\cref{fig:mlp_lora_vis_13B,fig:mlp_lora_vis_70B}: first decreasing, then increasing, and finally decreasing again. Shallow layers (0-3) and deeper layers (25-35 for the 13B model and 50–75 for the 70B model) undergo intensive updates. This behavior indicates that model prioritizes foundational feature extraction in shallow layers and the refinement of complex representations in deeper layers. Such a strategic update distribution optimizes the learning process, ensuring effective capture of basic and advanced features.

\subsection{Analysis of Unchanged Weights}
Here, we try to analyze the unchanged weights to support the motivation of LoRAM.
\paragraph{Fine-Grained Visualizations.} 
As the above visualization, we conducted detailed visualizations comparing the updated magnitudes of pruned and unpruned weights across layers. The results demonstrate that unpruned weights in both attention and MLP layers exhibit consistently smaller updates during fine-tuning as shown in \cref{fig:attn_lora_vis_70B}, indicating their critical role in preserving the model's capacity for inference.{\paragraph{Theoretical Perspective.} The phenomenon can be explained by the gradient-based importance of these weights, which prioritize parameters with minimal updates but high sensitivity during recovery. These weights stabilize inference outputs, making them indispensable despite their limited fine-tuning updates.}

{\paragraph{Quantitative Evidence} Our analysis reveals a strong correlation between weight update magnitudes and downstream performance. Pruning weights with smaller updates significantly degrades performance, highlighting their importance for inference and validating our intuition.}
{\paragraph{Impact on Large Models} The selective pruning strategy shows notable benefits in larger models such as LLaMA-2-70B, where it outperforms random pruning by a substantial margin. Retaining critical parameters ensures effective task adaptation and generalization across diverse domains.}

% \section{Pruning with Oracle}

% \section{Alignment with Distillation}
\clearpage
\section{Performance of Sub-Tasks in CSR}
\label{apd:detail_csr}
We report the performance of six sub-tasks in CSR, with~\cref{fig:csr_comparison_13B,fig:csr_comparison_70B} showcasing the results for \method-trained LLaMA-2-13B and LLaMA-2-70B, respectively. Our findings indicate that various \method variants outperform core competitive benchmarks: for the 13B model, \method surpasses both the untrained 13B and the LoRA-trained 7B, while for the 70B model, it exceeds the untrained 70B and the LoRA-trained 13B. This demonstrates that \method consistently achieves performance gains across models of different scales while effectively reducing memory usage. Furthermore, selective weight contributions in the 70B model significantly enhance performance, as evidenced by \methodstru's marked improvement, particularly in the challenging Arc Challenge multi-choice question-answering task. This suggests that \methodstru effectively identifies and leverages weight differences, focusing on the most trainable weights compared to \methodrand.
\begin{figure}[ht]
    \centering
    \begin{minipage}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/CSR_LLaMA-2-13B_OpenHermes.pdf}
\label{fig:csr_detail_13B_hermes}
    \end{minipage}%
    \begin{minipage}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/CSR_LLaMA-2-13B_OpenOrca.pdf}     \label{fig:csr_detail_13B_orca}
    \end{minipage}
    \caption{Performance of six CSR sub-tasks on the trained LLaMA-2-13B using \method.}
    \label{fig:csr_comparison_13B}
\begin{minipage}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/CSR_LLaMA-2-70B_OpenHermes.pdf}
\label{fig:csr_detail_70B_hermes}
    \end{minipage}%
    \begin{minipage}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/CSR_LLaMA-2-70B_OpenOrca.pdf}     \label{fig:csr_detail_70B_orca}
    \end{minipage}
    \caption{Performance of six CSR sub-tasks on the trained LLaMA-2-70B using \method.}
    \label{fig:csr_comparison_70B}
\end{figure}

\clearpage
\section{Algorithm of \method}
\label{alg:loram}
Here, we present the complete algorithm of \method in \cref{algo:loram}.

\input{Appendix/B-Algorithm}



\clearpage

\section{{Tuning of Learning Rate}}
\label{apd:detail_lr}
{We provide additional details on the learning rate tuning process for full LoRA applied to LLaMA-2-7B and LLaMA-2-13B models, trained on the OpenHermes dataset. These experiments in~\cref{fig:LR-Tuning} demonstrate that a learning rate of 1e-3 consistently achieves the best perplexity across both in-domain and out-of-domain datasets, further validating the reliability of our comparison.}

\begin{figure*}[ph]
\begin{center}
\includegraphics[width=\textwidth]{fig/LR-Tuning.pdf}
\caption{
{Learning rate tuning for LLaMA-2-7B and LLaMA-2-13B on OpenHermes using LoRA.}
}
\label{fig:LR-Tuning}
\end{center}
\end{figure*}

\section{{Performance of Domain-Specific Task}}

{To assess the effectiveness of LoRAM in domain-specific tasks, we conducted experiments on GSM8K (using the training set for tuning and the test set for evaluation), a mathematical reasoning benchmark known for its sensitivity to sparsification. Specifically, we trained LLaMA-3.1-70B using QLoRAM under various configurations.}

{The results, summarized in~\cref{tab:gsm8k_results}, highlight that LoRAM achieves excellent performance in this domain-specific setting. Notably, LoRAM-based models maintain high accuracy with substantial parameter reduction ratios, showcasing their robustness and efficiency in domain-specific tasks. These findings emphasize LoRAM's broad applicability beyond general-purpose instruction fine-tuning.}

\begin{table}[ht]
    \centering
    \caption{{Evaluation of LoRAM on the GSM8K dataset for domain-specific fine-tuning. Results show accuracy (\%) and parameter reduction ratios for different configurations.}}
    \label{tab:gsm8k_results}
    {
    \begin{tabular}{@{}lcc@{}}
        \toprule
        \textbf{LLaMA-3.1} & \textbf{GSM8K} & \textbf{Parameter Reduction Ratio} \\
        \midrule
        8B w/o Fine-Tuning            & 55.27  & 8.79× \\
        8B LoRA (OpenHermes 400)      & 55.80  & 8.79× \\
        70B w/o Fine-Tuning           & 75.28  & 1.00× \\
        70B QLoRAM-Stru 400 (OpenHermes 400) & 80.36 & \textbf{15.81×} \\
        70B QLoRAM-Stru 400 (GSM8K 100) & 77.18 & \textbf{15.81×} \\
        70B QLoRAM-Stru 400 (GSM8K 200) & 79.15 & \textbf{15.81×} \\
        70B LoRA (OpenHermes 400)     & \textbf{80.74} & 1.00× \\
        \bottomrule
    \end{tabular}}
\end{table}

\clearpage
\section{{Analysis of LoRAM Cost}}
\label{apd:diverse_metrics}

{Identifying the costs of LoRAM is indeed important, which is why we report both the number of training tokens used during the alignment phase and the parameter reduction ratios in the low-rank training phase. Below, we clarify the two stages of LoRAM:}
{\paragraph{Offline Knowledge Alignment Phase.} 
The offline phase is task-agnostic and can be conducted by the model publisher prior to deployment, making its cost negligible for end users. To quantify the offline cost, we measured the number of training tokens (as in \citet{xia2024sheared}) rather than end-to-end latency, which can vary based on hardware configurations. As shown in Figure~5, LoRAM achieves significant performance gains using only 13 million tokens, demonstrating the efficiency of the alignment phase.}
{\paragraph{Online Low-Rank Matrix Training Phase.} 
For the online phase, the memory and latency costs are primarily determined by the size of the base model parameters, which dominate resource consumption during training. To avoid redundancy in reporting, we focused on parameter reduction ratios instead of absolute time or memory usage.}
{\paragraph{Comparative Metrics for Online Training.}
Here, we provide additional metrics, including memory and latency comparisons for the online training phase. We conducted experiments using a workload of 1024 samples (batch size 128, micro-batch size 4, sequence length 512) randomly selected from OpenHermes. The results in \cref{tab:lora_comparison} demonstrate that LoRAM with a structured pruning ratio of $2.17\times$ (13B $\rightarrow$ 6B) achieves comparable peak memory, latency, and throughput to 7B LoRA, with only minor trade-offs. These differences arise due to the larger layer count in 13B LoRAM, introducing more non-GEMM operations, slightly affecting latency and throughput.}

{These results underscore the advantages of LoRAM's design in achieving substantial resource efficiency without significant trade-offs in memory or latency.}

\begin{table*}[ht]
\centering
\caption{{Comparison of peak memory (MiB), latency (s), and throughput (samples/s) during the online training phase for LoRAM and LoRA models. Results are based on a workload of 1024 samples (batch size 128, micro-batch size 4, sequence length 512).}}
{
\label{tab:lora_comparison}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{LLaMA-2}            & \textbf{\#Model Params} & \textbf{Reduction Ratio} & \textbf{Memory} & \textbf{Latency} & \textbf{Throughput} \\
\midrule
7B LoRA           & 6.73B                & 1.93$\times$             & 30,517                     & \textbf{134.27}      & \textbf{7.626}                 \\
13B LoRA          & 13.02B               & 1.00$\times$             & 51,661                     & 206.07               & 4.969                          \\
13B LoRAM-Stru    & \textbf{6.01B}       & \textbf{2.17$\times$}    & \textbf{29,799}            & 147.86               & 6.925                          \\
\bottomrule
\end{tabular}}
\end{table*}

\clearpage
\section{{Analysis of Changes in Performance Trends}}
{We analyze performance at two stages: after fine-tuning but before recovery, and after both fine-tuning and recovery.}
{\paragraph{After Fine-Tuning but Before Recovery.}
At this stage, the results of LoRAM align with prior work (e.g., SparseGPT, Wanda, and LLM-Pruner). Unstructured and semi-structured pruning consistently outperform structured pruning (see \cref{fig:pruning-methods}, solid lines). This trend holds true across both aligned and unaligned settings, with the performance order as follows: \methodsemi $<$ \methodunst $<$ \methodstru $<$ \methodrand
The slight advantage of \methodsemi over \methodunst can be attributed to its smaller pruning ratio, which retains more parameters and mitigates performance degradation.}
{\paragraph{After Fine-Tuning and Recovery.}
Post-recovery results show that structured pruning outperforms unstructured pruning. This can be explained by two factors:}

\begin{itemize}
    \item {\textbf{Preserved Structure for Recovery:} Structured pruning maintains the organization of the pruned weights into coherent structures (e.g., rows and columns in MLP layers, attention heads in attention layers), ensuring that activations after recovery are aligned with those of the original model. This alignment improves the recovery process.}
    \item {\textbf{Pruned Weight Quality:} The quality of pruned weights influences the recovery effectiveness. Structured pruning tends to remove less critical weights, leaving more recoverable parameters. In contrast, unstructured pruning can remove weights that are more difficult to recover, which negatively impacts performance post-recovery.}
\end{itemize}

{These results highlight the interplay between pruning and recovery, suggesting that structured pruning, despite initial performance disadvantages, facilitates more effective recovery.}




% \section{Exploration of Diverse Tasks}

% \section{Effect of Alignment Data Size}

% \section{Exploration of Learning Rate}

% \subsection{Effect of Model Scale}


