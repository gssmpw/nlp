
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% \iclrfinalcopy

\usepackage{hyperref}
\usepackage{url}

% for figures
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{wrapfig}

% for equation
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{bm}

\usepackage{bbm}
% for table
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
\usepackage{caption}
\usepackage{multirow}
\usepackage{array}
\usepackage{caption}
\usepackage{color}
\usepackage{colortbl}
\usepackage{tablefootnote}
\usepackage{adjustbox}

% for algorithm
% \usepackage{caption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand{\mycolor}[1]{\textcolor[RGB]{64,101,149}{#1}}
\newcommand{\mydarkcolor}[1]{\textcolor[RGB]{64,101,149}{#1}}
\algnewcommand{\LineComment}[1]{\Statex ~~~~~~\textsc{//}~\textit{#1}}

%for itemize
\usepackage{enumitem}
\setenumerate[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setdescription{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}

% for highlight
\usepackage{soul}

% for notation
% \usepackage[mathscr]{euscript}
% \newcommand{\M}{\mathscr{M}}
% \newcommand{\M}{\mathcal{M}}

%for itemize
\usepackage{amssymb}  
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\newcommand{\greenyes}{\textcolor{green}{\ding{51}}}
\newcommand{\redno}{\textcolor{red}{\ding{55}}}
\usepackage[capitalize]{cleveref}
% for table color box
\definecolor{c0}{cmyk}{1,0.3968,0,0.2588} 
\definecolor{LightCyan}{rgb}{0.88,1,1}
\newcommand{\gray}{\cellcolor{gray!10}} % LightCyanc0!5
\usepackage{scalerel} 

%% === commands for comments ===
% \usepackage{ulem}
\usepackage[normalem]{ulem}
\usepackage{makecell} %zj
\usepackage{colortbl} %zj

\usepackage{scalerel} % for logo

%citation color
\definecolor{uclablue}{rgb}{0.15, 0.45, 0.68}
\definecolor{custommagenta}{rgb}{0.1, 0.90, 1}
% {0.85, 0.1, 0.85}
\newcommand{\blueone}{\cellcolor{uclablue!10}} 
\newcommand{\bluetwo}{\cellcolor{uclablue!20}}
\newcommand{\bluethree}{\cellcolor{uclablue!30}}
\newcommand{\bluefour}{\cellcolor{uclablue!40}}

\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{tikz}

\newcommand{\rainbowurl}{%
    \textbf{\texttt{\textcolor{red!50}{h}\textcolor{orange!50}{t}\textcolor{yellow!50}{t}\textcolor{green!50}{p}\textcolor{blue!50}{s}\textcolor{purple!50}{:}//%
    \textcolor{red!50}{g}\textcolor{orange!50}{i}\textcolor{yellow!50}{t}\textcolor{green!50}{h}\textcolor{blue!50}{u}\textcolor{purple!50}{b}%
    \textcolor{red!50}{.}\textcolor{orange!50}{c}\textcolor{yellow!50}{o}\textcolor{green!50}{m}/%
    \textcolor{blue!50}{j}\textcolor{purple!50}{u}\textcolor{red!50}{n}\textcolor{orange!50}{z}\textcolor{yellow!50}{h}\textcolor{green!50}{a}\textcolor{blue!50}{n}\textcolor{purple!50}{g}%
    \textcolor{red!50}{-}\textcolor{orange!50}{z}\textcolor{yellow!50}{j}/%
    \textcolor{green!50}{L}\textcolor{blue!50}{o}\textcolor{purple!50}{R}\textcolor{red!50}{A}\textcolor{orange!50}{M}}%
}}

\hypersetup{
    breaklinks,
    citecolor=uclablue,
    colorlinks=true,
    linkcolor=uclablue
}


\title{
\scalerel*{\includegraphics{fig/loram_logo.png}}{{\rule{1.6ex}{1.6ex}}}
Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models
}


\author{Jun Zhang$^{1,3} \thanks{\ Work done during an internship at OPPO AI Center.}$, Jue Wang$^{1,3}$, Huan Li$^{1,2}\thanks{\ Huan Li and Lidan Shou are the corresponding authors.}$ , Lidan Shou$^{1,2 {\dagger}}$, Ke Chen$^{1,2}$, \\ \textbf{Yang You$^{4}$, Guiming Xie$^{5}$, Xuejian Gong$^{5}$, and Kunlong Zhou$^{5}$} \\
  $^1$The State Key Laboratory of Blockchain and Data Security, Zhejiang University \\
  $^2$Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security \\
  $^3$College of Computer Science and Technology, Zhejiang University \\
  $^4$Department of Computer Science, National University of Singapore \\
  $^5$AI Center, Guangdong OPPO Mobile Telecommunications Corp., Ltd. \\
  \texttt{\{zj.cs,zjuwangjue,lihuan.cs,should,chenk\}@zju.edu.cn},\\ \texttt{youy@comp.nus.edu.sg}, \texttt{\{xieguiming,gongxuejian,zhoukunlong\}@oppo.com}
}
% \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% The State Key Laboratory of Blockchain and Data Security\\
% Zhejiang University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{zj.cs,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\usepackage{xspace}
\newcommand{\method}{\textsc{LoRAM}\xspace}
\newcommand{\methodrand}{\textsc{LoRAM-Rand}\xspace}
\newcommand{\methodstru}{\textsc{LoRAM-Stru}\xspace}
\newcommand{\methodsemi}{\textsc{LoRAM-Semi}\xspace}
\newcommand{\methodunst}{\textsc{LoRAM-Unst}\xspace}
\newcommand{\Qmethod}{\textsc{QLoRAM}\xspace}
\newcommand{\Qmethodrand}{\textsc{QLoRAM-Rand}\xspace}
\newcommand{\Qmethodstru}{\textsc{QLoRAM-Stru}\xspace}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Large Language Models (LLMs) have significantly advanced natural language processing with exceptional task generalization capabilities. 
% Parameter-Efficient Fine-Tuning (PEFT), exemplified by LoRA, 
Low-Rank Adaption (LoRA) offers a cost-effective fine-tuning solution, freezing the original model parameters and training only lightweight, low-rank adapter matrices.
However, the memory footprint of LoRA is largely dominated by the original model parameters.
% \textcolor{custommagenta}{
% To further reduce the fine-tuning memory footprint, 
To mitigate this, 
we propose \method, a memory-efficient LoRA training scheme
founded on the intuition that many neurons in over-parameterized LLMs have low training utility but are essential for inference. 
\method presents a unique twist: it trains on a pruned (small) model to obtain pruned low-rank matrices, 
which are then
% dimensionally 
recovered and utilized with the original (large) model for inference.
Additionally, minimal-cost continual pre-training,
% }
performed by the model publishers in advance, aligns the knowledge discrepancy between pruned and original models.
Our extensive experiments demonstrate the efficacy of \method across various pruning strategies and downstream tasks. For a model with 70 billion parameters, \method enables training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA training and 15 GPUs for full fine-tuning. Specifically, \Qmethod implemented by structured pruning combined with 4-bit quantization, for LLaMA-3.1-70B (LLaMA-2-70B), reduces the parameter storage cost that dominates the memory usage in low-rank matrix training by 15.81×
% \footnote{The reported QLoRAM parameter reduction is based on Hugging Face’s implementation, which underestimates the impact of \texttt{NF4} quantization, implying a higher actual reduction.}
(16.95×), while achieving dominant performance gains over both the original LLaMA-3.1-70B (LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).
Code is available at \href{https://github.com/junzhang-zj/LoRAM}{\rainbowurl}.



\end{abstract}

\input{Sections/1-Introduction}
% \input{Sections/2-RelatedWork}
\input{Sections/3-Methodology}
\input{Sections/4-Experiments}
% \input{Sections/7-Analysis}
\input{Sections/5-Conclusion}
% Reproducibility Statement
\subsubsection*{Acknowledgments}
This work is supported by the Pioneer R\&D Program of Zhejiang (No.~2024C01021), ``Leading Talent of Technological Innovation Program'' of Zhejiang Province (No.~2023R5214), OPPO Research Fund,
the Major Research Program of Zhejiang Provincial Natural Science Foundation (No.~LD24F020015), and NSFC Grant No.~62402420.

\clearpage
\bibliography{loram,custom}
\bibliographystyle{iclr2025_conference}
\clearpage
\appendix
\section*{Appendix}
\input{Sections/2-RelatedWork}
\input{Appendix/A-Details}

\end{document}
