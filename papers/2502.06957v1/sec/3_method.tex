\section{Method}
\label{sec:method}
% why generalizable nerf? not gaussian splats

% sherf is only designed for one image. but other generalizable nerf methods like NHP can integrate sparse view observations. show an experiment result for this and also mention it in the method section
% single image -> generalizable Animatable Nerf (**highlight** SMPL driven animation)
% Render (+ SMPL normal map) -> SVD (How to add the condition; fuse)
% switcher -> disentangle modalities
% Training and Inference

\input{fig/pipeline}

%Our method generates multi-view and temporally coherent avatar renderings from a single image by leveraging the dense appearance information of a generalizable NeRF combined with the generative capabilities of a video diffusion model.
Our method generates view and time-wise coherent avatar renderings from a single image by leveraging the dense appearance information of a generalizable human reconstruction model combined with the generative capabilities of a video diffusion model.

\subsection{Notation}
%
%\mypara{Notation.} 
Functions (e.g., neural network mapping) are denoted with uppercase calligraphic letters (e.g., $\mathcal{U}$). Vectors are denoted with bold lowercase letters (e.g., $\bm{x}$). Matrices are denoted with uppercase letters (e.g., $C$). Sets are denoted with bold uppercase letters (e.g., $\bm{I}_{nerf}$).


\subsection{Single-view Generalizable Human Synthesis}
Given a single reference image $I_{\text{ref}}$, the camera parameter $P_\text{ref}$, and the parameters of a human template, \textit{i.e.}, the SMPL~\cite{loper2015smpl} model, we adopt single-view generalizable human NeRF \cite{hu2023sherf} to synthesize an image corresponding to the target camera parameters $P_{tar}$ and target template parameters consists of pose $\bm{\theta}_{tar}$ and shape $\bm{\beta}_{tar}$. %(pose $\theta_\text{t}$ and shape $\beta_\text{t}$). 

To render the target image, a point $\bm{x}$ is sampled along the cast rays in the target space. Then it is transformed to point $\bm{x_c}$ in the the SMPL canonical space via inverse LBS, where the features associated with $\bm{x_c}$ are queried from the observation space. Specifically, pixel-aligned features and human template-conditioned features are obtained and fused together, denoted as $\bm{p}$. The density $\bm{\sigma}$ and color $\bm{c}$ are obtained by a multi-layer perception
(MLP) network $\mathcal{F}$:
\begin{equation}
    \bm{\sigma}(\bm{x}), \bm{c}(\bm{x}) = \mathcal{F}(\bm{x}, \bm{p}, \gamma_d(\bm{d})),
\end{equation}
where $\gamma_d$ is the positional encoding of viewing direction $\bm{d}$.

The target image $I_{tar}$ in target view and pose is rendered using the volume rendering~\cite{mildenhall2021nerf}.

We emphasize that this human NeRF model is designed to generalize, enabling the synthesis of human appearances across a range of novel views and poses. 
%
%However, due to its tendency toward a mean-seeking mode, producing sharp renderings—particularly in occluded or unseen regions—remains challenging. 
However, due to its mean-seeking tendency, producing sharp renderings—particularly in occluded or unseen regions—remains challenging. 
This limitation motivates our introduction of generative priors to learn a distribution over the human views and poses, as discussed in the next section.

%\subsection{Synthesis as Conditional Video Generation}
\subsection{Synthesis as Video Generation Condition}
In this section, we elaborate how we leverage video diffusion prior to reformulate the novel view and pose synthesis as a video generation task conditioned on the human radiance fields and human geometric templates. We would like to note that prior approaches often treat novel view and novel pose synthesis as separate tasks. In contrast, our approach unifies both tasks by conditioning on NeRF-rendered images, allowing for more consistent renderings. Using human templates alone as a condition provides only sparse geometric guidance, which often leads to inconsistencies in appearance across novel views. By integrating generalizable radiance field as a unified conditioning mechanism, we address this issue and achieve better appearance consistency across varied poses and views.


\noindent \textbf{Novel View Synthesis.} Given a reference image $I_{ref}$ and a camera trajectory $\{P_1, P_2, \cdots, P_T\}$, we render the corresponding images $\bm{I}_{nerf} = \{I_1, I_2, \cdots, I_T\}$ from the NeRF, which servers as an input to our video diffusion model, \textit{i.e.}, Stable Video Diffusion \cite{blattmann2023stable} that adopts spatio-temporal attention module and 3D residual convolution in the diffusion UNet. For the single input image $I_\text{ref}$, we extract its feature with CLIP \cite{radford2021learning} and repeat it for $T$ times to match the number of frames, denoted as $\bm{h}_\text{clip}$, which is then added to the video diffusion model through the cross attention. Meanwhile, we use the VAE encoder to encode our input image $I_\text{ref}$ and obtain its latent feature $C_\text{vae}$.


To introduce NeRF renderings $\bm{I}_{nerf}$ to our video diffusion model, we feed $\bm{I}_{nerf}$ to the VAE encoder, after which it is further encoded by a small convolutional neural network. We denote the output latent feature as $C_{\text{nerf}}$.


In practice, we observe that solely relying on NeRF renderings in our video diffusion model is insufficient, as these renderings may sometimes exhibit artifacts, particularly due to inaccurate SMPL fittings or occlusions. Such artifacts can corrupt the guidance and hinder the diffusion model from learning meaningful conditional distributions. To address this limitation, we further integrate a geometric cue, \textit{i.e.}, the SMPL model, to provide additional structural guidance. The SMPL model captures essential geometry information, which leads to the enhanced spatial consistency as well as the robust human shape recovery. 

In order to integrate this information with the NeRF rendered features in the 2D pixel space, similar to \cite{zhu2024champ}, we render the human template mesh into 2D normal maps. Specifically, we render the SMPL normal maps $\bm{M}=\{M_1, M_2, \cdots, M_T\}$ under the camera trajectory $\{P_1, P_2, \cdots, P_T\}$. Then a set of 2D convolution layers are utilized to extract the features, denoted by $C_{\text{smpl}}$.

To effectively fuse \( C_{\text{nerf}} \) and \( C_{\text{smpl}} \), we combine them through element-wise addition. The resulting fused feature is then added to the output of the first convolutional layer of the UNet in the video diffusion model.



\noindent \textbf{Novel Pose Synthesis.} Given a sequence of SMPL poses $\{\bm{\theta}_1, \bm{\theta}_2, \cdots, \bm{\theta}_T\}$ and a fixed camera parameter, we render corresponding images from the NeRF and SMPL normal maps. They can be used as conditions to the video diffusion model in the same manner as illustrated above.

Now we can formulate the learning objective of our diffusion model. The diffusion UNet \( \mathcal{U}_\Theta \) predicts the noise \( \epsilon \) for each diffusion step \( t \), and our training objective is
%
\begin{equation}
\mathcal{L}_{\mathcal{U}_{\theta}} = \mathbb{E} \left[ \| \epsilon - \mathcal{U}_{\theta} (Z_t, t, \bm{h}_{\text{clip}}, C_{\text{vae}}, C_{\text{nerf}}, C_{\text{smpl}}) \| \right] 
\label{eq:prev_objective}
\end{equation}


where \( Z_t = \alpha_t Z + \sigma_t \epsilon \). Here, \( Z \) is the ground-truth latent, \( \epsilon \sim \mathcal{N}(0, I) \), and \( \alpha_t \) and \( \sigma_t \) define the noise at timestep \( t \). $\Theta$ is the  learnable parameters of the UNet $\mathcal{U}$.



%\subsection{Switcher for Disentangled Novel View and Pose Synthesis}
\subsection{Switcher for Disentangled Synthesis}
Joint learning of human view synthesis and pose synthesis presents inherent challenges for feed-forward methods. Our proposed framework addresses these challenges by unifying both tasks into a single video generation task, leveraging the capability of video diffusion model to effectively model each task under the same representation. Under this framework, a straightforward approach would be to train the video diffusion model on both multi-view and dynamic video data simultaneously. However, our empirical findings reveal that dynamic motions embedded within view synthesis videos can disrupt view consistency (see Figure \ref{fig:ablation_swicher}). This issue arises due to the inherent modality differences between static view synthesis videos and dynamic animation videos. To mitigate this problem, we introduce a switcher mechanism within our video diffusion model that explicitly controls and disentangles these two modes, ensuring task-specific consistency and performance.

In particular, we introduce the switcher \( \bm{s} \), which is a one-hot vector that labels each of the two modalities, as the additional condition to the video diffusion model. This allows us to extend the formulation in Equation \ref{eq:prev_objective} as follows:
%
\begin{equation}
    \mathcal{L}_{\mathcal{U}_{\theta}} = \mathbb{E} \left[ \| \epsilon - \mathcal{U}_{\theta} (Z_t, t, \bm{h}_{\text{clip}}, {C}_{\text{vae}}, {C}_{\text{nerf}}, {C}_{\text{smpl}}, \bm{s}) \| \right] 
    \label{eq:train_objective}
\end{equation}


To incorporate the domain switcher \( \bm{s} \), we first apply positional encoding and then concatenate it with the time embedding. This combined encoding is subsequently fed into the UNet within the video diffusion model.



\subsection{Training and Inference}
% two stage training: first finetune generaliable nerf
% second large scale training diffusion model
% inference: cfg
\noindent \textbf{Training.} The entire training process of our pipeline consists of two stages. During the first stage, we train the generalizable human NeRF $\mathcal{F}$ model on the multi-view datasets. Following \cite{hu2023sherf}, we randomly sample observation and target image pairs from each subject. The prediction of the target view is supervised by minimizing the loss objective $\mathcal{L} = \mathcal{L}_2 + \lambda_{\text{ssim}} \cdot \mathcal{L}_{\text{ssim}} + \lambda_{\text{lpips}} \cdot \mathcal{L}_{\text{lpips}} + \lambda_{\text{mask}} \cdot \mathcal{L}_{\text{mask}}
$
where $\mathcal{L}_2, \mathcal{L}_{\text{ssim}}, \mathcal{L}_{\text{lpips}}$ are photometric \(L_2\) loss, SSIM loss \cite{wang2004image} and LPIPS \cite{zhang2018unreasonable} loss between the predicted image and the ground truth. $\mathcal{L}_{\text{mask}}$ is the \(L_2\) difference between the accumulated volume density and the ground truth human binary mask. $\lambda_{\text{ssim}}, \lambda_{\text{lpips}}, \lambda_{\text{mask}}$ are weights of each loss to balance their contributions to the final loss function. 

In the second stage, we freeze the generalizable NeRF model and train our video diffusion model. We train the full spatio-temporal UNet and feature encoders for NeRF and SMPL normal renderings following our training objective in Equation \ref{eq:train_objective}. The second stage is trained on our complete dataset to ensure the generalization.

\noindent \textbf{Inference.} We apply classifier-free guidance (CFG) \cite{ho2022classifier} to inference from the video diffusion model. The two tasks are done with different CFG schedules according to the task properties. For the novel view synthesis task, we utilize a triangular CFG scaling \cite{voleti2024sv3d}, where we linearly increase CFG from 1 at the front view to 2 at the back view, then linearly decrease it back to 1 at the front view. For the novel pose synthesis task, we fix the CFG scale to be 2.

