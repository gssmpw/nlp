\maketitlesupplementary
\section*{A Limitations and future works}
Although our method achieves state-of-the-art performance in both novel view and pose synthesis, it is not free from limitations. (1) Due to the constraints of current off-the-shelf single-image human mesh recovery methods, we use SMPL \cite{loper2015smpl} to establish geometry and appearance conditions for scalable training. However, SMPL lacks expressiveness in regions such as the face and hands, resulting in artifacts in these areas. Future works could explore scalable solutions inspired by recent efforts, such as regional supervision \cite{xu2024high, zhang2024mimicmotion}. (2) While we utilize Stable Video Diffusion (SVD) \cite{blattmann2023stable} to model view and pose synthesis, achieving strong consistency across both spatial and temporal dimensions, some challenges remain. Specifically, we occasionally observe degraded frame quality and difficulties in accurately generating complex clothing textures, particularly during significant clothing deformations. Addressing the former may involve exploring advanced video generative models, such as Diffusion Transformers \cite{peebles2023scalable, yang2024cogvideox}. For the latter, leveraging synthetic human datasets with intricate textures presents an exciting avenue for future research.

\section*{B Ethical considerations}
\noindent\textbf{Human subjects data.} We adhere to strict ethical guidelines in the collection and use of data involving human subjects. Below, we provide details on how we obtained each dataset utilized in our work:

\begin{itemize}
    \item THuman2.1 \cite{tao2021function4d} is a publicly available dataset. We signed the necessary agreement with the dataset authors to obtain access via an official download link.
    \item 2K2K \cite{han2023Recon2K} and MVHumanNet \cite{xiong2024mvhumannet} follow the same procedure as THuman2.1, involving an agreement with the authors to obtain official download links.
    \item TikTok Dataset \cite{jafarian2021learning} is publicly available and was downloaded in its preprocessed form from MagicPose \cite{chang2023magicpose}.
    \item Additional Real-World Data was manually selected and filtered from the publicly released portions of Champ \cite{zhu2024champ} training data.
\end{itemize}

\noindent\textbf{Broader Impact.} Our proposed method enables affordable and accessible solutions for a wide range of applications. By leveraging generative AI, we transform content generation, making it possible to generate novel views and poses from just a casually-captured single image. It has the potential to revolutionize fields such as virtual reality, gaming, and digital content creation by significantly lowering the barriers to high-quality multi-view synthesis and animation.

However, along with these benefits come potential risks that warrant careful consideration. The misuse of generative AI in creating synthetic content raises ethical concerns, such as the possibility of producing misleading or harmful media. Additionally, privacy concerns may arise when real-world data is used for training, especially when the data involves human subjects. Ensuring robust safeguards, transparent practices, and compliance with ethical standards is crucial to mitigate these risks while maximizing the positive impact of our method.

\section*{C Implementation details}
\subsection*{C.1 Model architecture}
\input{fig/model_arch}
We detail the processing of each conditioning input and their integration into the video diffusion model, as illustrated in Figure \ref{fig:model_arch}. Starting with a single reference image \( I_{ref} \in \mathbb{R}^{H\times W\times C} \), we extract its VAE latent representation, repeat it \( T \) times, and concatenate it with the input noise latent along the channel dimension. This combined representation is passed through a convolutional layer to generate \( C_\text{vae} \in \mathbb{R}^{T\times H_1\times W_1\times C_1} \).

For the geometry cue, represented as a sequence of SMPL \cite{loper2015smpl} normal maps, we use a 2D ConvNet \( \boldsymbol{\epsilon}_{\text{geo}} \) to extract features \( C_{\text{smpl}} \in \mathbb{R}^{T\times H_1\times W_1\times C_1} \). Similarly, for the appearance cue, which consists of corresponding NeRF renderings, we pass the sequence through a VAE and subsequently a 2D ConvNet \( \boldsymbol{\epsilon}_{\text{appr}} \) to obtain features \( C_{\text{nerf}} \in \mathbb{R}^{T\times H_1\times W_1\times C_1} \).

The feature representations \( C_\text{vae} \), \( C_{\text{smpl}} \), and \( C_{\text{nerf}} \) are element-wise added and fed into the diffusion UNet \( \mathcal{U}_\Theta \) to predict noise. Additionally, the CLIP embedding of the reference image, \( \bm{h}_\text{clip} \in \mathbb{R}^d \), is repeated \( T \) times to match the frame sequence and injected into \( \mathcal{U}_\Theta \) via cross-attention.

Finally, the switcher, represented as a one-hot vector, is embedded and element-wise added to the time embedding. This is also repeated \( T \) times to align with the frame sequence and injected into \( \mathcal{U}_\Theta \) within the ResNet layers.

\subsection*{C.2 Training details}
We leverage a combination of 3D human scans, multi-view videos and monocular videos to train our diffusion UNet \( \mathcal{U}_\Theta \), geometry cue encoder \( \boldsymbol{\epsilon}_{\text{geo}} \) and appearance cue encoder \( \boldsymbol{\epsilon}_{\text{appr}} \).

For 3D scans, we utilize 20 renderings for training the novel view synthesis task. A view is randomly selected as the reference, and the model is tasked with predicting all 20 consecutive novel views. Note that the reference view and the predicted starting view do not need to be the same. Furthermore, the order of the predicted views is randomly determined, \textit{i.e.}, either clockwise or counterclockwise. 

For multi-view videos, we train the model for the novel pose synthesis task. A frame is randomly selected as the reference image, and the model is tasked with predicting a 20-frame video clip. For a single pose, we also experiment with the novel view synthesis task. However, due to the fluctuating camera trajectory and sparse camera setup, we observe suboptimal outcomes.

For monocular videos, each video is split into a sequence of images at 30 frames per second. We sample one image every four consecutive frames for training. Similarly, a frame is randomly chosen as the reference, and the model is tasked with predicting 20 consecutive frames.


\section*{D Additional results}
\subsection*{D.1 Video results}
We provide additional video results, including in-the-wild avatar synthesis, comparison with baselines, ablations and additional results. Details explained below:

\begin{itemize}
    \item In-the-wild avatar synthesis: We demonstrate novel view synthesis, pose animation and interactive 4D synthesis on in-the-wild avatars. 
    \item Comparison with baselines: We compare our results with Animate Anyone \cite{hu2023animateanyone} and Champ \cite{zhu2024champ} on THuman \cite{tao2021function4d} for novel view synthesis and TikTok \cite{jafarian2021learning} for pose animation. The video results demonstrate our method outperforms the two baseline methods in terms of consistency and quality.
    \item Ablations: We highlight the importance of the dense appearance cue and switcher through the ablation videos.
    \item Additional results: We show novel view synthesis results on MVHumanNet \cite{xiong2024mvhumannet}. We also show free-view animation results, where the human subject and the camera are both moving.
\end{itemize}



\subsection*{D.2 Runtime at inference}
Given a single in-the-wild image, we report the runtime of our proposed method, in terms of both novel view and pose synthesis. The detailed runtime breakdown is as follows. (1) geometry cue rendering; (2) appearance cue rendering; (3) video diffusion inference. The runtime is reported on a single NVIDIA A800 GPU and measured in seconds. 

\noindent\textbf{Novel view synthesis.} We report the runtime for generating 20 novel views given a single input image, as shown in Table \ref{tab:runtime_nvs}. 

\begin{table}[ht]
\centering
\begin{tabular}{lcccc}
\hline
 & 20 frames & Per frame \\
\hline
Geo. cue rendering & 5.81 & 0.29 \\
Appr. cue rendering & 28.6   & 1.43 \\
Diffusion inference & 15.88  & 0.79 \\ 
\hline
Total runtime & 50.29 & 2.51 \\
\hline
\end{tabular}
\caption{\textbf{Runtime at inference for generating 20 novel views of a single human image.}}
\vspace{-1.0em} 
\label{tab:runtime_nvs}
\end{table}

\noindent\textbf{Novel pose synthesis.} We report the runtime for generating 100 consecutive novel poses from a single input image, as shown in Table \ref{tab:runtime_np}. The additional video diffusion inference time arises due to the 6-frame overlap between consecutive video segment windows \cite{zhang2024mimicmotion, shao2024human4dit}.

\begin{table}[ht]
\centering
\begin{tabular}{lcccc}
\hline
 & 100 frames & Per frame \\
\hline
Geo. cue rendering & 29 & 0.29 \\
Appr. cue rendering & 143 & 1.43  \\
Diffusion inference & 106.49    & 1.06  \\ 
\hline
Total runtime & 278.49 & 2.78\\
\hline
\end{tabular}
\caption{\textbf{Runtime at inference for generating 50 consucutive novel poses from a single human image.}}
\vspace{-1.0em} 
\label{tab:runtime_np}
\end{table}

\noindent\textbf{Efficiency comparison.} We provide a comparison of runtime and VRAM usage with the strongest baseline Champ \cite{zhu2024champ}, as shown in Table \ref{tab:efficiency-comparison}.

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\hline
 & fps & VRAM (GB) \\
\hline
Champ & 0.57  & 9.88 \\
Ours & 0.40 & 5.32  \\
\hline
\end{tabular}
\caption{\textbf{Efficiency comparison with baseline method.}}
\vspace{-1.0em} 
\label{tab:efficiency-comparison}
\end{table}



\subsection*{D.3 Ablation on geometry \& appearance cues}
We provide a complete quantitative ablation study on the geometry and appearance cue, as shown in Table \ref{tab:geo_appear_ablation}. We use THuman \cite{tao2021function4d} to evaluate the multi-view synthesis and MVHumanNet \cite{xiong2024mvhumannet} to evaluate the novel view animation performance.

\subsection*{D.4 Ablation on merging novel view \& pose tasks}
We provide ablation analysis on merging both novel view and novel pose tasks into one model. From the static novel view synthesis perspective, the unified framework allow us to train on the abundant internet videos - which leads to the improved generalizability, as shown in Figure \ref{fig:ablation_generalization}. It does not hurt the in-domain dataset performance according to Table \ref{tab:merge_thuman_2k2k}.


% \begin{figure}[h]
%     \centering
%     %\vspace{-1.5em}
%     \includegraphics[width=0.45\textwidth]{fig/generalization_comparison.pdf}
%     \vspace{-1em} 
%     \caption{\textbf{Ablation on merging novel view and novel pose tasks.} w.o. merging means we only train the model on our 3D scan datasets for novel view synthesis, while w. merging means we train the model on our full dataset for both novel view and novel pose synthesis.}
%     \label{fig:ablation_generalization}
%     \vspace{-1.5em} 
% \end{figure}

From the pose animation perspective, training on additional 3D datasets can improve the quality when we animate the avatar from a novel view, as shown in Table \ref{tab:ablation_quantitative_merging}.

\begin{table}[h]
\centering
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{lcccc}
\hline
Training data & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & FVD $\downarrow$ \\
\hline
Monocular videos & 28.67  & \textbf{0.946} & 0.041  & 208.3 \\
Full & \textbf{28.74} & 0.945 & \textbf{0.040}  & \textbf{188.5} \\

\hline
\end{tabular}
}
\caption{\textbf{Quantitative ablation on merging view synthesis and pose animation into one model.} Results reported for novel view animation task on MVHumanNet dataset.}
\label{tab:ablation_quantitative_merging}

\end{table}



\subsection*{D.5 Ablation on switcher}
In addition to the qualitative ablation for the switcher presented in the main paper and video results, we also conduct quantitative ablation analysis on the switcher. With the switcher, our method effectively supports both novel view and pose synthesis, while also providing comparable or even better quantitative results, as shown in Table \ref{tab:ablation_switcher}.

\begin{table}[h]
\centering
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{lcccc}
\hline
 & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & FVD $\downarrow$ \\
\hline
w.o. switcher & 26.58 & \textbf{0.944} & 0.042 & 198.9 \\
w. switcher & \textbf{26.77} & 0.943 & \textbf{0.041}  & \textbf{194.8} \\

\hline
\end{tabular}
}
% \vspace{-0.5em} 
\caption{\textbf{Ablation on switcher for novel view synthesis task on THuman.}}
\label{tab:ablation_switcher}
% \vspace{-1em} 
\end{table}

\subsection*{D.6 Novel pose synthesis on scan datasets}
We show the novel pose animation results for subjects in THuman and 2K2K in Figure \ref{fig:thuman_2k2k_animation}, where the reference images are animated through pose sequences derived from disparate videos.

\begin{figure}[h]
    \centering
    %\vspace{-1.5em}
    \includegraphics[width=0.45\textwidth]{fig/fig_thuman_2K2K_animation.pdf}
    % \vspace{-0.5em} 
    \caption{\textbf{Novel pose results on THuman and 2K2K.} The reference images are animated by pose sequences derived from MVHumanNet dataset.}
    \label{fig:thuman_2k2k_animation}
    % \vspace{-1em} 
\end{figure}

\begin{table*}[t]
\centering
\resizebox{0.7\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
Training data & \multicolumn{2}{c}{PSNR $\uparrow$} & \multicolumn{2}{c}{SSIM $\uparrow$} & \multicolumn{2}{c}{LPIPS $\downarrow$} & \multicolumn{2}{c}{FVD $\downarrow$} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
& THuman & 2K2K & THuman & 2K2K & THuman & 2K2K & THuman & 2K2K \\
\midrule
3D scans only & \textbf{26.82}  & 28.76 & 0.936  & 0.953  & \textbf{0.040}  & 0.040  & \textbf{189.5}   & \textbf{187.9} \\
% \midrule
3D scans+dynamic videos & 26.77 & \textbf{28.82}  & \textbf{0.943}  & \textbf{0.954} & 0.041 & \textbf{0.039}   & 194.8 & 191.3 \\
\bottomrule
\end{tabular}
}
% \vspace{-0.5em} 
\caption{\textbf{Quantitative ablation on merging view synthesis and pose animation into one model.} Results reported for novel view synthesis task on in-domain THuman and 2K2K testset. 
% w.o. merging means that the model is trained on THuman and 2K2K datasets. w. merging means that the model is trained on additional dynamic video datasets.
}
\label{tab:merge_thuman_2k2k}
% \vspace{-0.5em} 
\end{table*}


\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
Method & \multicolumn{2}{c}{PSNR $\uparrow$} & \multicolumn{2}{c}{SSIM $\uparrow$} & \multicolumn{2}{c}{LPIPS $\downarrow$} & \multicolumn{2}{c}{FVD $\downarrow$} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
& THuman & MVHumanNet & THuman & MVHumanNet & THuman & MVHumanNet & THuman & MVHumanNet \\
\midrule
w.o. geo. cue & 26.07 & 28.33  & 0.938  & 0.943  & \textbf{0.045}  & 0.044 & 227.1  & 210.6 \\
w.o. appear. cue & 26.38  & 27.66  & 0.938  & 0.941  & 0.041  & 0.043  & 207.5  & 234.6\\
\midrule
Ours & \textbf{26.77} & \textbf{28.74}   & \textbf{0.943}  & \textbf{0.945} & 0.041 & \textbf{0.040}   & \textbf{194.8} & \textbf{188.5} \\
\bottomrule
\end{tabular}
}
% \vspace{-0.5em} 
\caption{\textbf{Quantitative ablation on geometry and appearance cues.}}
\label{tab:geo_appear_ablation}
% \vspace{-0.5em} 
\end{table*}

\subsection*{D.7 Robustness to input view angles}
We train our model using an arbitrary view as input.
Thus, we are interested in the novel view synthesis robustness from different input view angles. To this end, we present our quantitative results in Table \ref{tab:nvs_robustness}. We find that our pipeline demonstrates robustness with different views of input. Visualized results are shown in Figure \ref{fig:view_robustness}, where our proposed method can maintain
the faithful appearance near the reference view and generate reasonable appearances in unseen regions.


\begin{table}[h]
\centering
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{lcccc}
\hline
Angle & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & FVD $\downarrow$ \\
\hline
Front & 28.46   & 0.952  & 0.04  & 178.3\\
Back & 28.34   & 0.952   & 0.04  & 211.5 \\
Left side & 29.18  & 0.956  & 0.039  & 199.9  \\
Right side & 29.29   & 0.955   & 0.038  & 175.2  \\
\hline
% \rowcolor[gray]{0.9}
Mean & 28.82 & 0.954 & 0.039 & 191.3 \\
% \rowcolor[gray]{0.9}
Std & 0.487 & 0.002 & 0.001 & 17.42 \\
\hline
\end{tabular}
}
\caption{\textbf{Quantitative results on 2K2K dataset for novel view synthesis from different input view angles.}}
\vspace{-1.0em} 
\label{tab:nvs_robustness}
\end{table}



\section*{E Verification of design choices}
We found that generative human novel view synthesis remains relatively under-explored, primarily due to the challenge of synthesizing consistent appearances across multiple novel views, especially in unseen regions. We have shown the importance of generalizable appearance cue and geometry cue in the main paper. Here, we aim to additionally validate the choice of leveraging video diffusion models and training with smooth camera orbits.

\subsection*{E.1 Image v.s. video diffusion model}
Image diffusion model is believed to be good at image-to-image translation tasks. Specifically, given a single reference image and the target appearance cue, we can train a image diffusion model to synthesize the target image. The model architecture can be a variant of Animate Anyone \cite{hu2023animateanyone} and Champ \cite{zhu2024champ}, where we adopt two ReferenceNets to inject the rich information from the reference image and appearance cue respectively. The results are shown in Figure \ref{fig:img-2-img}. To enable multi-view consistent synthesis for a single subject, we have tried adding an 1D temporal-axis attention layers \cite{guo2023animatediff} and only fine-tune these new added layers. We have also attempted to leverage human geometric prior to construct 3D correspondence across different views \cite{pan2024humansplat, huang2024epidiff}. 

\begin{figure}[h]
    \centering
\includegraphics[width=0.45\textwidth]{fig/fig_img-img-translation.pdf}
    \caption{\textbf{Image-to-image novel view synthesis on MVHumanNet \cite{xiong2024mvhumannet} dataset by using image diffusion model and appearance cue.}}
    \label{fig:img-2-img}
    %\vspace{-1.5em} 
\end{figure}

\noindent\textbf{Consistency comparisons.} Compared to directly using a pretrained video diffusion model, these image-based diffusion methods exhibit significantly inferior performance. Their results are shown in Figure \ref{fig:mv_diffusion_model}.

\begin{figure}[h]
    \centering
\includegraphics[width=0.45\textwidth]{fig/fig_mv_diffusion_result.pdf}
    \caption{\textbf{Novel view synthesis on THuman2.1 by a multi-view image-based diffusion model.} Inconsistent clothing wrinkles appear between two adjacent novel view generations.}
    \label{fig:mv_diffusion_model}
    %\vspace{-1.5em} 
\end{figure}

\noindent\textbf{Free-view interpolation.} Due to GPU memory limitations, we are restricted to training with approximately 20 novel views per subject per batch. During inference, we also test and compare the ability of both models to generate a dense trajectory of novel views (\textit{e.g.,} 100 views). However, neither approach achieves satisfactory results: image-based diffusion models show significant inconsistencies, while video diffusion models produce blurry frames.

\input{fig/view_robustness}

\subsection*{E.2 Novel view camera trajectories}
On our 3D scan dataset, we render a smooth camera trajectory with the same elevation and evenly distributed azimuth. We also explore the possibility of leveraging the fluctuant novel views in MVHumanNet dataset \cite{xiong2024mvhumannet} to learn the multi-view consistency. However, we empirically find that the diffusion models are not able to capture the consistency even with the aid of appearance cues, causing the textures to 'flow' across views. The reason might be two folds: 1) number of cameras is too sparse in our adopted subset of MVHumanNet dataset; 2) diffusion models cannot establish accurate correspondence for arbitrary two views with human geometric prior. 