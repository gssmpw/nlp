% \clearpage
% \setcounter{page}{1}
% \maketitlesupplementary
\section{Experiments}
\label{sec:experiment}
\subsection{Experimental Setup}
\subsubsection{Datasets}
%We have curated diverse datasets, including 3D scans, multi-view videos, and monocular videos, to train and evaluate our method.

\noindent\textbf{3D Scans.} We use THuman2.1 \cite{tao2021function4d} and 2K2K dataset \cite{han2023Recon2K} with around 4500 3D scans in total. THuman2.1 comprises 2445 high-quality 3D scans and texture maps. We randomly sample 2345 subjects for training and the remaining 100 subjects for testing. 2K2K dataset consists of 2000 train and 50 test subjects, totally 2050 scans. For both datasets, RGB images are rendered from 20 uniformly distributed views around the scan, at the resolution of $1024\times1024$. SMPL parameters are estimated using off-the-shelf method for multi-view SMPL fitting \cite{easymocap}.

\noindent\textbf{Multi-view Videos.} MVHumanNet \cite{xiong2024mvhumannet} is a multi-view video dataset featuring a large number of diverse identities and everyday clothing. We use a subset of 944 human captures, each consisting of synchronized 16-view videos per subject. We reserve 48 subjects for evaluation and use the remaining subjects for training. The dataset also includes SMPL parameters, optimized from multi-view images.

\noindent\textbf{Monocular Videos.} For in-the-wild datasets, we use the TikTok dataset \cite{jafarian2021learning} and an additional collection of internet videos. The TikTok dataset includes 350 dance videos, from which we processed and filtered 289 valid video sequences for training. Following the protocol in \cite{wang2024disco, chang2023magicpose}, we use subjects from 335 to 340 for testing. To further diversify the data, we selected 122 video sequences from Champ \cite{zhu2024champ} training dataset, originally sourced from reputable online platforms such as TikTok and YouTube. For both datasets, we obtain the foreground human masks using Grounded-SAM \cite{ren2024grounded} and the SMPL parameters using 4DHumans \cite{goel2023humans}. %which is developed for monocular human mesh recovery.

\subsubsection{Implementation Details}
We trained the generalizable human NeRF model \cite{hu2023sherf} on MVHumanNet dataset. 
To accelerate the video diffusion training, instead of creating and rendering the human NeRF on-the-fly, we choose to store the NeRF renderings for all datasets offline. 
For the video diffusion model, we initialize it with the pre-trained Stable Video Diffusion 1.1 image-to-video model \footnote{https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1} \cite{blattmann2023stable}. We resize all images to a resolution of 512$\times$512. Each batch consists of 20 frames. We train the model for 150k iterations with an effective batch size of 8 and a learning rate of $10^{-5}$. We utilize 8 A100 GPUs and the total training time is 3 days. 

\subsubsection{Baselines and Metrics}
\noindent\textbf{Baselines.} We benchmark our method against state-of-the-art generative human rendering methods including Champ \cite{zhu2024champ} and Animate Anyone \cite{hu2023animateanyone}. As for Animate Anyone \cite{hu2023animateanyone}, we use the implementations from Moore Threads \footnote{https://github.com/MooreThreads/Moore-AnimateAnyone}. %Additionally, we include SV3D \cite{voleti2024sv3d} for comparisons on the novel view synthesis task. 

\noindent\textbf{Metrics.} 
We evaluate the fidelity and consistency of our results using both image-level and video-level metrics. For image-level comparisons, we report peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM) \cite{wang2004image}, and learned perceptual image patch similarity (LPIPS) \cite{zhang2018unreasonable}. For video-level evaluation, we use the Fr√©chet Video Distance (FVD) \cite{unterthiner2018towards} metric.

\begin{table*}[ht]
\centering
\resizebox{0.8\textwidth}{!}{ % Resizing the table
\begin{tabular}{lcccccccc}
\toprule
Method & \multicolumn{2}{c}{PSNR $\uparrow$} & \multicolumn{2}{c}{SSIM $\uparrow$} & \multicolumn{2}{c}{LPIPS $\downarrow$} & \multicolumn{2}{c}{FVD $\downarrow$} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
& THuman & 2K2K & THuman & 2K2K & THuman & 2K2K & THuman & 2K2K \\
\midrule
% SV3D & & & & & & & & \\
Animate Anyone & 22.48 & 18.48 & 0.927 & 0.557 & 0.061 & 0.263 & 460.3 & 1422.1 \\
Champ & 20.96 & 22.14 & 0.909 & 0.910 & 0.074 &  0.075 & 470.3 & 480.3 \\
\midrule
Animate Anyone* & 25.20 & 26.22 & 0.938 & 0.936 & 0.046 & 0.050 & 302.7 & 286.4 \\
Champ* & 23.89 & 25.66 & 0.928 & 0.935 & 0.054 &  0.052 & 296.1 & 279.3 \\
\midrule
Ours & \textbf{26.77} & \textbf{28.82}  & \textbf{0.943}  & \textbf{0.954} & \textbf{0.041} & \textbf{0.039}  & \textbf{194.8} & \textbf{191.3} \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Quantitative comparison for novel view synthesis on THuman and 2K2K dataset.} For all the methods, we report the average score on 20 views using four orthogonal input views (front, back, and side views). * indicates methods fine-tuned on our 3D scan dataset.}
\label{tab:mv_comparison}
\end{table*}

\input{fig/nvs_comparison_fig}
\input{fig/animation_comparison}

\subsection{Comparisons on Novel View Synthesis}
We show the comparisons with Animate Anyone \cite{hu2023animateanyone} and Champ \cite{zhu2024champ}, which are two competitors focusing on generative human synthesis. For a fair comparison, we fine-tuned both methods on our full 3D scan dataset for 10k iterations. %so that they can better adapt to the novel view synthesis task. %As they cannot generate reasonable novel view results, to ensure a fair and strong comparison, we fine-tuned both methods with our full 3D scan dataset for 10k iterations so that they can better adapt to novel view synthesis task. 

\noindent\textbf{Quantitative results.} We adopt a comprehensive evaluation protocol for human novel view synthesis from a single image. Specifically, for each subject, we sample four orthogonal input views and compute their mean score across all generated novel views. It is worth noting that Animate Anyone \cite{hu2023animateanyone} often produces a noisy background. For a fair comparison that focuses solely on the human subjects, we apply the ground truth mask to remove backgrounds in the THuman dataset, while in the 2K2K dataset, we retain the generated images as they are. 

Quantitative results are presented in Table \ref{tab:mv_comparison}. Results show that our method achieves state-of-the-art performance across all evaluation metrics, highlighting the advantages of our generalizable human radiance field in preserving intricate details across viewpoint changes.

% By contrast, baseline methods, which condition only on sparse geometry signals, often suffer from detail loss or inconsistent synthesis. Additionally, we compare baseline methods fine-tuned on our full 3D scan dataset against our model, which is trained on a mixture of high-quality 3D scan data and extensive dynamic human videos. Remarkably, our method achieves a significant improvement in FVD metrics, indicating superior spatio-temporal consistency. This performance underscores the effectiveness of our proposed switcher in learning disentangled control over static views and dynamic motions.

\noindent\textbf{Qualitative results.} Figure \ref{fig:nvs_comparison} presents our qualitative comparisons with baseline methods, focusing on challenging reference views and novel views that have less overlapping. 

% For example, the hair regions marked by the red circles are inconsistent between frontal and back views.
%Baseline methods tend to generate inconsistent details across views. For example, the hair regions marked by the red circles are inconsistent between frontal and back views.

% Although our approach does not include modules specifically designed for refining face and hand regions, we consistently observe superior synthesis in these areas compared to baseline methods, which often exhibit distortions. 
%and artifacts. 
% This observation aligns with related findings \cite{shao2024human4dit}, which suggest that recent diffusion-based human generative models, despite being pre-trained on large-scale datasets, tend to struggle with smaller, intricate regions like faces and hands.





\subsection{Comparisons on Novel Pose Synthesis}
We compare our method on novel pose synthesis task with strong baseline methods Animate Anyone \cite{hu2023animateanyone} and Champ \cite{zhu2024champ}, which are designed for single-image animation. For a fair comparison, we also further fine-tuned the baseline methods on our complete video dataset for 10k iterations.
% Following previous works, we report our metrics on the TikTok dataset for monocular animation. Additionally, we  evaluate the methods on MVHumanNet dataset, where the reference view differs from the animated target view.

\noindent\textbf{Quantitative results.}
To evaluate on the TikTok dataset, we randomly sample a frame as the reference and generate the subsequent 100 frames.  Quantitative results are presented in Table \ref{tab:novepose_comparison}. Our method consistently outperforms the baseline methods, even after additional fine-tuning. 

% This highlights the capability of video diffusion models to leverage rich geometric and appearance cues for capturing high-fidelity visual details and temporal coherence.


\noindent\textbf{Qualitative results.}
Qualitative results are shown in Figure \ref{fig:animation_comparison}, where we compare our method with Champ \cite{zhu2024champ} on the novel view animation task for the MVHumanNet dataset. Competing methods display flickering and fluctuations, especially in unseen regions. In contrast, ours synthesizes temporally consistent animations, even from novel views.
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\hline
Method & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & FVD $\downarrow$ \\
\hline
Animate Anyone & 17.21  & 0.762  & 0.225 & 1274.1\\
Champ & 18.48  & 0.806  & 0.182 & 585.0 \\
\midrule
Animate Anyone* & 17.83  & 0.791  & 0.204 & 840.5  \\
Champ* & 18.57  & 0.797  & 0.187 & 893.7  \\
\midrule
Ours & \textbf{19.11} & \textbf{0.833} & \textbf{0.176} & \textbf{362.0} \\
\hline
\end{tabular}
}
\caption{\textbf{Quantitative comparisons for novel pose synthesis on TikTok dataset.} * Indicates methods fine-tuned on our multi-view video and monocular video dataset.}
\vspace{-1.0em} 
\label{tab:novepose_comparison}
\end{table}


\subsection{Ablation Studies and Analyses}
We conduct ablation studies evaluating variants of our proposed method. Please refer to the supplement for more
ablations with quantitative and qualitative comparisons.

\noindent\textbf{Generalizable geometry and appearance cues.}
To study the effect of both geometry (\textit{i.e.}, SMPL normal map) and appearance cues (\textit{i.e.}, human radiance field), we train a variant with the geometry cue completely removed and a variant with appearance cue removed. 
%
Our hypothesis for the generalizable human radiance field is that it provides rich and consistent appearance cues across different views and times. To validate this, we present quantitative comparisons for novel view synthesis on the THuman dataset and pose animation from a novel view on the MVHumanNet dataset. 
%we show both quantitative comparisons for novel view synthesis on THuman dataset and novel view animation task on MVHumanNet dataset, where we choose the frontal view as the input view and animate the human avatar on the side view. 
%
As shown in Table \ref{tab:ablation_appear_cue}, our approach consistently improved performance across both tasks and datasets. %we obtain consistent improvements on both tasks across two different dataset. 

\begin{table}[h]
\centering
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{lcccc}
\hline
Metric: FVD $\downarrow$ & THuman & MVHumanNet \\
\hline
w.o. appear. cue & 207.5 & 234.6\\
w. appear. cue & \textbf{194.8} & \textbf{188.5} \\

\hline
\end{tabular}
}
\caption{\textbf{Quantitative ablation study on our appearance cue.} Please refer to Table \ref{tab:geo_appear_ablation} in the supplement for a complete quantitative evaluation on geometry and appearance cues.}
\vspace{-1em} 
\label{tab:ablation_appear_cue}
\end{table}

\input{fig/ablation_no_appear_cue}
\input{fig/ablation_wo_geo_cue}
\input{fig/ablation_mixed_training}

\input{fig/ablation_internet_video}
\input{fig/ablation_switcher}

To demonstrate the importance of the geometry cue, we present a qualitative ablation study on internet videos (\textit{e.g.}, TikTok dataset). Occlusion is a common challenge in in-the-wild videos, often causing artifacts in generalizable human radiance fields due to pixel-aligned feature extraction and warping. As shown in Figure \ref{fig:ablation_wo_geo_cue}, without a clean geometry guidance, the radiance field can misguide the diffusion generation, leading to visible distortions.



\noindent\textbf{Mixed training with 3D/multi-view captures and internet videos.} 
We analyze the impact of our training strategy, which leverages diverse data sources, aiming to improve generalizability. As shown in Figure \ref{fig:ablation_mixed_training}, our model demonstrates strong generalization capability for novel view synthesis in real-world scenarios. Figure \ref{fig:ablation_generalization} presents an ablation on involving internet videos for training.



\noindent\textbf{Effect of switcher for disentangled synthesis.}
In our setup, where we jointly learn static view synthesis and dynamic motion modeling, we introduce a switcher module. To illustrate the effectiveness of this switcher, we train a model variant without the switcher and present qualitative ablation studies. Without the switcher, dynamic motion often appears in the generated consecutive novel views, as shown in Figure \ref{fig:ablation_swicher}. 

