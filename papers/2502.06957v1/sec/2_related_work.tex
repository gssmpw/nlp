\section{Related Work}
\label{sec:related_work}

\subsection{Generalizable Human Radiance Fields}

Radiance fields methods, like NeRF \cite{mildenhall2021nerf}, have shown strong capabilities in generating high-fidelity renderings across views. To extend these methods to generalize to new scenes and handle sparse-view inputs, several approaches \cite{yu2021pixelnerf, saito2019pifu, wang2022attention} have incorporated pixel-aligned features to aid novel view synthesis. However, applying these approaches directly to human modeling presents challenges due to the complex geometry of the human body, including self-occlusions, which often lead to overly smoothed outputs. To address these challenges, some methods~\cite{kwon2021neural, kwon2023neural, Zhao_2022_CVPR, peng2021animatable, peng2021neural, dong2022totalselfscan,dong2023ivs} use 3D human templates, such as the SMPL model~\cite{loper2015smpl}, to anchor features accurately on the human form. Although these techniques achieve good results even with sparse or single-view inputs \cite{hu2023sherf}, they are often hindered by slow rendering speeds. Recently, 3D Gaussian Splatting \cite{kerbl20233d}, optimized with GPU-based rasterization, has emerged as a promising radiance field representation, enabling faster rendering. Building on this, new methods \cite{zheng2024gps,kwon2024generalizable, zhuang2024idolinstant} have achieved photorealistic human rendering from sparse observations. Nevertheless, these methods still struggle with producing clear details in unseen regions due to mean-seeking behavior in one-to-many mapping scenarios \cite{liu2021neural, kwon2024deliffas}. In this work, we propose learning a distribution conditioned on generalizable human radiance fields, allowing us to synthesize clean, consistent view and pose sequences by sampling from this distribution.



\subsection{Generative Human Animation}
Generative human animation aims to employ generative models to produce coherent videos from static human images, utilizing guidance such as text and motion sequences. This body of work focuses on leveraging generative priors to sample complex dynamic motions, including pose-dependent clothing deformations. Early approaches harnessed the generative capabilities of Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} for synthesizing novel human poses \cite{wang2021one, chan2019dance, liu2019neural}. In recent years, latent diffusion models \cite{rombach2022high} have gained traction in the realm of human animation due to their robust controllability and superior generation quality. Various methods \cite{wang2024disco, chang2023magicpose, hu2023animateanyone, zhu2024champ, li2024dispose, sun2024drive, chang2025x, li2024synthesizing} implement distinct motion guidance and conditioning techniques. Notably, Animate Anyone \cite{hu2023animateanyone} introduces a UNet-based ReferenceNet to extract features from reference images, utilizing DWPose \cite{yang2023effective} for pose guidance. Subsequent works \cite{zhu2024champ} also incorporate guidance from 3D human parametric models, such as SMPL \cite{loper2015smpl, zhao2024metric,zhao2024synergistic}, leveraging the advantages of multiple forms of guidance. Following this trajectory, recent studies \cite{he2024magicman, shao2024human4dit, qiu2024anigs, liu2024human} explore human view controllability within the diffusion frameworks. Human4DiT \cite{shao2024human4dit} develops a hierarchical 4D diffusion transformer that disentangles the learning of 2D images, viewpoints, and time. 
%
%However, this approach is challenged by the need for complex training strategies (i.e., separate training for each block).
%
However, these methods struggle to synthesize view-consistent and temporally coherent results due to the gap between the sparse driving signal and the actual subject.
In this paper, we propose a unified approach to human view and pose synthesis by conditioning on generalizable geometry and appearance features and mapping them to a cohesive representation, resulting in a more straightforward and effective process.

\subsection{Diffusion Models for Video Generation}
Diffusion models have demonstrated exceptional performance in image synthesis over the past few years. The research community has since started to extend these models to the more challenging task of video generation \cite{zeng2024dawn}. Significant efforts have been made to adapt UNet-based image diffusion models for video generation by incorporating additional temporal modules. Several studies \cite{blattmann2023align, guo2023animatediff} have proposed freezing pre-trained image diffusion models and training only the newly added temporal module on video data. Stable Video Diffusion \cite{blattmann2023stable} adds temporal layers after each spatial convolution and attention layer, fine-tuning the entire model on large-scale, curated video datasets. It has proven to be a strong foundational model, facilitating further advancements in video generation as well as 3D/4D synthesis \cite{voleti2024sv3d, xie2024sv4d}. Beyond UNet-based diffusion models, diffusion transformers \cite{peebles2023scalable, yang2024cogvideox, videoworldsimulators2024} have emerged as a robust architecture for video generation, applying full attention to space-time patches of video and image latent codes. In this work, we opt to leverage the strong foundational capabilities of Stable Video Diffusion to model both multi-view correspondences and temporally coherent realistic deformations of human subjects.