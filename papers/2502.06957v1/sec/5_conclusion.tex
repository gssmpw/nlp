\section{Conclusion}
\label{sec:conclusion}
We present a generalizable and unified framework for avatar synthesis from a single image, addressing key challenges in appearance consistency and generalization to in-the-wild avatars. Our approach integrates a regression-based human radiance field with a video diffusion model, effectively utilizing dense conditioning to minimize discrepancies between driving signals and target representations. Additionally, our framework enables large-scale training on realistic human data from different sources, while maintaining a disentangled modeling of static novel views and dynamic motion. This synergy leads to high-fidelity view synthesis and pose animation with lifelike deformations, demonstrating strong generalization across diverse real-world scenarios.
% \textbf{Limitations.}