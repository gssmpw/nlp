\section{Introduction}
\label{sec:intro}


\input{fig/teaser}

Human avatar generation is a longstanding research area in computer vision and graphics, with applications spanning gaming, film, sports, fashion, and telepresence. Despite its potential to revolutionize daily life, current avatar generation technologies require costly capturing setups and highly complicated procedures, making them hard to access by broader audiences.

Recent efforts have sought to make avatar generation more affordable by harnessing the power of neural rendering. In particular, some works \cite{kwon2021neural, kwon2023neural, hu2023sherf, kwon2024generalizable} focus on generalizable 3D human reconstruction that synthesize novel views and poses of arbitrary human subjects from very sparse or even single input images. These methods utilize 3D human priors to enable robust synthesis of complex human geometry, and interpolate well between available observations. However, due to their regression-based nature, they often struggle with extrapolation, leading to blurry results. In addition, they are often limited to rigid deformations. %resulting in blurred, averaged appearances. In addition, they are often limited to rigid deformations or require complex post-processing for realistic animation.

Generative models like GANs and diffusion models have recently shown remarkable capability in producing photorealistic detailed images and videos with realistic dynamics.  Building on the success of these generative techniques, several approaches use diffusion models conditioned on human priors (e.g., 2D keypoints, depth maps, or normal maps) to create high-quality avatars with realistic animation from a single image \cite{hu2023animateanyone, zhu2024champ, chang2023magicpose, shao2024human4dit}. However, due to the sparsity in these conditioning signals, the generated avatars often suffer from inconsistencies such as flickering across different views and over time.

To tackle these challenges, we propose a method for single-image avatar synthesis that achieves both view and temporal consistency. 
%
Instead of relying solely on sparse signals, we first generate intermediate novel views or poses using a regression-based 3D human reconstruction model. We then use them as conditioning inputs for a video diffusion model. By incorporating dense information from the 3D reconstruction, our approach preserves both structural accuracy and rich visual details, ensuring high-quality, consistent results across views and time.

Beyond selecting the appropriate driving signal, another key challenge is improving generalization to real-world data, because ultimately we want a system that works not just on controlled datasets but also on casually captured in-the-wild human images. Most human digitization models are trained on multi-view datasets captured in controlled studios \cite{kwon2021neural,kwon2023neural,hu2023sherf}, which lack the diversity of real-world scenarios, such as varied lighting, clothing, and motion. 
%
To address this, we propose to incorporate in-the-wild internet videos, which provide an abundant source of diverse real-world distribution. While leveraging such videos significantly improves generalization, training a model on them presents challenges—particularly for novel view synthesis, which typically requires multi-view supervision, rarely available in internet videos.

To overcome this, we propose a unified framework that jointly learns novel view and pose synthesis, sharing parameters across tasks to enable cross-task generalization. While we still use studio multi-view datasets for novel view synthesis, we incorporate both multi-view and in-the-wild videos for novel pose synthesis. Since the same model parameters are used for both tasks, the generalization gained from pose synthesis naturally transfers to novel view synthesis as well. This approach significantly enhances the model’s ability to handle real-world data and improves overall synthesis quality as can be seen in Figure \ref{fig:teaser}.
Additionally, in order to employ this unified framework effectively, we introduce a mode switcher that differentiates between novel view and pose synthesis. This switcher disentangles the tasks, allowing the network to focus on view consistency when synthesizing novel views and realistic deformations when generating novel poses. 


In summary, our main contributions are:
% \yixing{*Modified contributions not sure about the 3rd point **}
\begin{itemize}
%\setlength{\itemsep}{6pt}
\item A unified framework for novel view and pose synthesis of avatars, which enables shared model parameters across both tasks with real human data (e.g., internet videos) at scale for training, leading to broad generalizability.
\item A dense appearance cue as synthesis guidance, ensuring consistent appearance preservation.
\item A switcher integrated into the video diffusion model to enable disentangled modeling of static novel views and dynamic pose animation.
\end{itemize} 

