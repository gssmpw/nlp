
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\noindent {\bf Datasets.}
We explore two distinct types of open-vocabulary action recognition evaluation in this work: \textit{in-context} and \textit{out-of-context} settings.
For in-context scenarios, we conduct experiments following the common practice in the literature~\cite{rasheed2023fine,weng2023open,huang2024froster,rasheed2023fine} on the Kinetics-400 (K400)~\cite{k400}, UCF-101 (UCF)~\cite{UCF101}, HMDB-51 (HMDB)~\cite{HMDB51}, Something-Something V2 (SSv2)~\cite{ssv2} and Kinectics-600 (K600)~\cite{k600} datasets under widely-used evaluation protocols: \textit{cross-dataset} and \textit{base-to-novel} evaluation. For more challenging out-of-context scenarios, we newly conduct general cross-dataset evaluations using K400 dataset as the training set and testing on the synthetic UCF-SCUBA~\cite{li2023mitigating} and UCF-HAT~\cite{chung2022enabling,bae2023devias} benchmarks. 

\noindent {\bf Implementation details.}
Generally, we use the official CLIP ViT-B/16 backbone for all experiments, and our video learner is the adaptation of the CLIP model follows~\cite{weng2023open}, unless stated otherwise.
During our meta-optimization process, we construct a batch of 4 tasks, each task contains 8 support and query samples from the training set. The learning rates of inner and outer loops for support batches \ie, $\alpha$, and $\beta$, are synchronized with the initial value of $3.33\times 10^{-6}$ and decay to $3.33\time 10^{-8}$ utilizing the AdamW~\cite{loshchilov2017decoupled} optimizer following a cosine decay scheduler, while the hyperparameter $\delta$ for query batches is set to $1.67\times 10^{-3}$. 
For cross-dataset evaluation, we warm up the training on the K400 dataset for the first 2 epochs and further fine-tune the video learner for 20 epochs. For base-to-novel evaluation, we train the learner for 12 epochs with the first two warm-up epochs on training data. 
% Each training video clip is sampled with 8 frames uniformly and is processed with basic augmentations like color jittering, random flipping and random cropping.
During inference, we use 3 temporal and 1 spatial views per video and linearly aggregate the recognition results. 
% We also adopt decision aggregation with pre-trained CLIP with the video learner for in-context evaluations.
% The experiments are conducted on four NVIDIA RTX 24G 4090 GPUs.
See Appendix for more experimental details about evaluation protocols and our implementations.

\subsection{Comparison with state-of-the-art methods}
We compare our framework with the state-of-the-art open-vocabulary action recognition methods on the following commonly used \textit{in-context} and newly proposed \textit{out-of-context} evaluation protocols.

\noindent {\bf In-context base-to-novel generalization.} 
In~\cref{tab:B2N}, we compare the proposed framework with other CLIP-based methods under the popular in-context base-to-novel setting. 
All methods are initially learned on the frequently occurring base classes and evaluated on both base and novel classes, where the novel classes represent a realm of previously uncounted scenarios.
From the results, we can summarize the observations: (1) Most of the methods show reasonable improvements from the frozen CLIP~\cite{radford2021learning}, except for ActionCLIP~\cite{wang2021actionclip}, X-CLIP~\cite{ni2022expanding} and VPT~\cite{ju2022prompting} suffering inferior performances especially on the novel sets of K400, HMDB and UCF, indicating the strong generalization of CLIP and the potential overfitting of these adapted video learners toward the training samples.
(2) Our framework experiences noticeable gains in novel class performance and consistent achievements on all four datasets, spanning spatially dense and temporally focused scenarios, which validates the effectiveness of enhancing generalization and static debiasing for both known and open classes.
% (3) Despite our method with the highest accuracy among all compared approaches, the limited improvements and modest performance on the SSv2 dataset remain challenging for open-vocabulary methods.

\input{Tabs/2-cross-dataset} 
\noindent {\bf In-context cross-dataset generalization.}
In~\cref{tab:cross-dataset}, we present the compared results under in-context cross-dataset zero-shot evaluations, where all learners undergo further fine-tuning on K400 training set and are tested directly on downstream cross-datasets \ie, UCF, HMDB and K600.
Similar findings can be noticed from the results as base-to-novel evaluations that frozen CLIP outperforms several adapted learners, especially on the most generalizability demanding benchmark, \ie, K600, further demonstrating the generalization degradation of overfitting within these methods.
Remarkably, our framework based on meta-learning consistently surpasses state-of-the-art approaches on all three benchmarks, demonstrating its superior effectiveness and enhanced generalizability.

\input{Tabs/3-ooc-cross-dataset}
\noindent {\bf Out-of-context cross-dataset generalization.}
In~\cref{tab:ooc}, we further compare our method with the previous state of the arts under more challenging out-of-context cross-dataset evaluations on SCUBA and HAT benchmarks of the UCF dataset.
It can be noticed that: 
(1) Integrating with CLIP regularization, both Open-VCLIP~\cite{weng2023open} and FROSTER~\cite{huang2024froster} achieve promising improvements compared with X-CLIP under original UCF in-context scenarios.
(2) However, the compared methods suffer from severely limited generalization when encountering out-of-context scenarios due to the static bias within these video learners.
(3) Our method significantly outperforms partially fine-tuned X-CLIP and CLIP regularization methods on various out-of-context scenarios. We outperform the second-best competitor by $4.6\%$ on UCF-SCUBA and $4.9\%$ on UCF-HAT, with the highest HM striking an impressive balancing on cross-dataset generalization for in-context and out-of-context scenarios. We attribute the superiority of our video learner to the natural know-to-open generalizing and image-to-video debiasing via the newly proposed meta-optimization and self-ensemble independent from CLIP's persistent interference of static biases for robust and generic generalizability.


\subsection{Ablation Studies}
\input{Tabs/4-applicability}
\noindent {\bf Applicability with different video learners.}
In~\cref{tab:applicability}, we adopt other video learners (with the frozen text encoder) from adapter-based ST-Adapter~\cite{pan2022st}, prompt-based Vita-CLIP~\cite{wasim2023vita}, partially fine-tuned X-CLIP~\cite{ni2022expanding} and fully fine-tuned VCLIP~\cite{weng2023open} to validate the effectiveness of our model-agnostic framework.
We find that: (1) All CLIP-adapted video learners integrating with our method achieve consistent improvements on in-context cross-dataset evaluations, highlighting its broad and flexible applicability.
(2) Our approach generally exhibits more improvements for partially and fully fine-tuned methods than PEFT learners, suggesting the importance of sufficient fitting capacity (\ie, learnable parameters) for video learners to attain video-specific generalizability.

\input{Figs/K400}
\input{Tabs/5-optim}
\noindent {\bf Effect of cross-batch meta-optimization.}
In~\cref{tab:optim}, we conduct experiments to verify the effect of our cross-batch meta-optimization scheme. The compared strategies and analyses are as follows: (a) Consider VCLIP with standard fine-tuning objectives as a baseline of the plain learner. (b) When adopting RFD to VCLIP, the K400 closed-set performance experiences a slight decline for both IC and OC scenarios, while cross-dataset in-context generalization improves, with gains of $+4.5\%$ on UCF-IC, whereas it severely impairs generalization for UCF-OC ($-3.1\%$). (c) Similar results are observed when integrating IWR regularization with VCLIP. (d) For the previous meta unseen optimization method for zero-shot learning, all three accuracies under UCF cross-dataset evaluation increase, where K400 evaluations challenge its closed-set generalizations, indicating the potential overfitting to meta unseen tasks.
(e) Notably, our cross-batch meta-optimization scheme ((a)$\rightarrow$(e)) enhances all closed-set and zero-shot performance on harmonic mean with gains of $+3.9\%$ and $+6.3\%$, respectively. This showcases the superiority of our scheme for enhancing know-to-open generalizing and image-to-video debiasing, which establishes a promising balance for robust generalization capabilities.


\noindent {\bf Effect of weight self-ensemble.}
In~\cref{fig:k400-ucf}, we investigate the trend of generalization performance during K400 training and the efficacy of weight self-ensemble stabilization using various strategies. 
In particular, the curves illustrate the performance within the video learner's optimization trajectory at different epochs, where the $x$-axis and $y$-axis display the different stages of training epochs and various generalization evaluation protocols, respectively.
It is noticeable that the overall performance has experienced trends of significant enhancement on both closed-set and zero-shot generalization while quickly leading to drops in zero-shot performance at the tail of the fine-tuning phase, suggesting the plasticity degradation that highly features supervised task-specific distributions on the downstream dataset.
The results show that weight ensembling methods improve both specialty and generalizability, with our Gaussian self-ensemble excelling significantly, strongly suggesting it as a better choice for robust generalization.

\input{Figs/visual}
\subsection{Visualizations} 
\cref{fig:visual} compares the t-SNE visualizations of Open-VCLIP and our framework for in-context and out-of-context UCF predictions. Note that our predictions for videos within the same category are more concentrated, with reduced confusion between different categories, compared to Open-VCLIP. This suggests that the proposed framework effectively learns temporal information, mitigating known and static biases while demonstrating robust generalizability. However, there remains considerable room for improvement in out-of-context scenarios for video-adapted learners.

% More experimental results and visualizations are provided in Appendix.