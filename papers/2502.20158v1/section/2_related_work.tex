\section{Related Work}
\label{sec:related}
\subsection{Adapting CLIP to Action Recognition}
% 【(+)说CLIP】
A seminal work of I-VL,  CLIP~\cite{radford2021learning} has demonstrated remarkable static generalization, achieving promising performance in image-based zero-shot inference. 
% 【(+)说fine-tuning】
% Rather than learning from scratch, a recent line of works adopts full fine-tuning~\cite{rasheed2023fine} or parameter-efficient fine-tuning~\cite{houlsby2019parameter} to adapt CLIP to action recognition with further training on video datasets like Kinetics-400~\cite{k400}.
% 【(+)说相关工作】
Despite extensive works~\cite{rasheed2023fine,wang2021actionclip,wu2023revisiting} fully fine-tuning the video learner, a collection of studies focuses on adopting lightweight adapters~\cite{yang2023aim,pan2022st,cao2024task} or incorporating learnable prompts~\cite{wasim2023vita,ju2022prompting}  for easy video adaptation.
% 【(+)说标准微调和时序建模】
% They adhere to the standard fine-tuning paradigm and share a resembled goal of introducing lightweight temporal modeling modules for efficient video understanding, which has emerged as a data-friendly trend.
% 【(—)说过拟合】
However, these video learners adhere to the standard fine-tuning paradigm, which tends to overfit in the closed-set setting, thereby limiting expertise in open-vocabulary settings.
% 【(+)说Open-VCLIP】
To this end, Open-VCLIP~\cite{weng2023open} regularizes the fine-tuning process of the video learner, preventing deviation from CLIP's generalization, by interpolating frozen CLIP weights with the current learner on the fly.
% 【(+)说FROSTER】
FROSTER~\cite{huang2024froster} and STDD~\cite{yu2024building} enforce the regularization from the perspective of knowledge distillation~\cite{chen2022improved,romero2014fitnets,castro2022fitclip,deng2021comprehensive}, aligning features of the video learner and frozen CLIP via a tailored residual module.
% 【(—)说训练代价和static bias】
Despite demonstrating superiority in open-vocabulary evaluations,  the increased computational overhead and excessive reliance on static cues introduced by CLIP regularization hinder efficient adaptation and robust generalization.
% 【(+)说解决】
In contrast, we approach the problem of adapting CLIP-based video learners to OVAR from a fresh view of “learning to generalize without bias”. During training, the learner is explicitly forced to quickly generalize to forthcoming data by sorely resorting to the knowledge learned by itself rather than by the virtue of CLIP’s static generalization.

\subsection{Meta-learning}
% 【(+)说meta-learning】
Rather than directly learning from experiences, with the goal of learning to learn, meta-learning can quickly generalize to new tasks by leveraging prior learning abilities~\cite{hospedales2021meta}.
% 【(+)说MAML】
As the representative works in meta-learning, MAML~\cite{finn2017model} boasts simplicity and has actively driven the development of the gradient-based methods in few-shot learning. 
% 【(+)说Meta-ZS】
Recently, meta-learning techniques have also been explored in zero-shot learning~\cite{verma2020meta,park2024prompt,liu2021task,huang2019generative}, which typically perform episode-wise training by dividing the training set into support and query sets with different classes distributions.
% 【(—)说缺点】
Targeting long-tailed issues within closed-set video scene generation, MVSGG~\cite{xu2022meta} employs meta-learning across various types of tasks \wrt certain conditional biases through meticulous structuring of training data.
However, these approaches are often prone to meta-overfitting due to insufficient meta tasks and limited application scopes of generalization.
% 【(+)】
Differently, our proposed meta-optimization scheme naturally mimics diverse known-to-open tasks incurring no additional computational overhead. This tackles ubiquitous challenges in video understanding beyond closed-set and in-context settings, \ie, mitigating static bias of video learners for open-vocabulary generalization. 