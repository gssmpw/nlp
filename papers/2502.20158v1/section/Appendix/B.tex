\section{Additional Experimental Results}
\label{appen:ad-results}
\subsection{Additional Evaluations and Ablation Studies}
\label{appen:ad-ablation}
\input{Tabs/3-ooc-cross-dataset-re}
\noindent {\bf Out-of-context cross-dataset evaluation on HMDB dataset.}
Regarding results shown in~\cref{tab:ooc(re)}, our method achieves the highest accuracy of $32.5\%$ on HMDB-SCUBA, and builds up an impregnable lead of $+5.7\%$ of HM results over the nearest competitor, enabling a superior balance for open-vocabulary generalization.

\input{Tabs/X-delta}
\noindent {\bf Effect of the learning rate $\delta$.}
As shown in~\cref{tab:delta}, we conduct experiments by setting the learning rate $\delta$ to different magnitudes.
It can be observed that as $\delta$ decreases, the general performance remains stable, which validates the robustness of our cross-batch meta-optimization. However, a further reduction to $1.67 \times 10^{-4}$ slightly decreases performance across most datasets, suggesting that the optimal value for $\delta$ lies at $1.67 \times 10^{-3}$, which is chosen as the default setting. This value achieves a balanced performance with the highest or nearly highest scores in each dataset, particularly noticeable on UCF-SCUBA benchmark.

\input{Tabs/X-meta}
\noindent {\bf Effect of cross-batch meta-optimization.}
To investigate the efficacy of our cross-batch meta-optimization complementing the main paper, we further evaluate the performance using the scheme of gradient accumulation. To ensure a fair comparison of the total gradient steps with cross-batch meta-optimization, we accumulate the gradients over two steps before performing a single parameter update. As shown in~\cref{tab:meta}, the gradient accumulation technique demonstrates modest improvements over the plain method for both in-context and out-of-context benchmarks. This indicates that the strength of our meta-optimization approach lies in its ability to enhance known-to-open generalization, rather than doubling the batch size for a single parameter update.

\input{Tabs/X-shuffle}
\noindent {\bf Effect of randomness of the batch sampler for cross-batch meta-optimization.}
To verify the efficacy of constructing tasks across batches with different inherent label distributions, we further conduct several additional studies about the sampling randomness during cross-batch meta-optimization. As shown in~\cref{tab:shuffle}, the randomness of the batch sampler is indeed an important factor to bring out the best of our cross-batch meta leaner, which improves the overall generalization greatly ($+5.0\%$ of harmonic mean) especially for out-of-context performance ($+4.6\%$ on UCF-SCUBA).
However, Plain learner shows insensibility to the sampling randomness, experiencing negligible growth of generalization performance.
Without shuffling the batch sampler, our method still outperforms the non-shuffle plain learner by $+1.4\%$ of HM results.

\input{Tabs/X-batch}
\noindent {\bf Effect of the batch size of tasks and samples.}
In~\cref{tab:batch}, we evaluate the performance with different batch sizes of the task and data for cross-batch meta-optimization. Each task consists of two data batches, one for the support set and one for the query set. From the results, we observe that increasing the batch size leads to slight improvements in performance, especially for K600. While larger batch sizes provide marginal improvements, they may not justify the increased computational cost. Thus, the default setting provides an effective balance between performance and computational efficiency.

\input{Tabs/X-ensemble}
\noindent {\bf Effect of the CLIP ensemble.}
In~\cref{tab:ensemble}, we evaluate the effectiveness of the CLIP ensemble in the weight space and decision space, with the ensemble ratios all set to $0.5$.
The results demonstrate that both types of CLIP ensemble improve performance in in-context evaluations, with the prediction-based ensemble yielding the most consistent gains across all methods. This suggests that integrating CLIP predictions effectively leverages the strengths of CLIP, leading to significant performance enhancements, particularly over the naive approach. However, there is a noticeable drop on UCF-SCUBA for the out-of-context generalization, indicating that the static generalization derived from the CLIP ensemble can adversely affect the model's robustness and overall generalizability.

\input{Tabs/X-debias}
\noindent {\bf Effect of static debiasing strategies.}
In~\cref{tab:debias}, we compare Open-MeDe with several baselines especially designed for mitigating static bias in action recognition, including three scene-debiasing methods (BE~\cite{wang2021removing}, FAME~\cite{ding2022motion} and StillMix~\cite{li2023mitigating}) and a state-of-the-art action-scene disentanglement method (DEVIAS~\cite{bae2023devias}). Note that DEVIAS leverages additional scene labels for disentangled video representation. As can be seen from the results, while FAME and DEVIAS perform well in the K400 closed-set out-of-context evaluation against static bias, they fall short in in-context performance and lack zero-shot inference capability. In contrast, our Open-MeDe, despite not employing explicit debiasing or disentangled action modeling, achieves favorable out-of-context generalization with a balanced harmonic mean. This highlights its robust generalizability across both in-context and out-of-context scenarios, particularly excelling in zero-shot generalization.

\input{Tabs/X-train}
\subsection{Training cost analysis}
In~\cref{tab:train}, we show the training cost analysis of our approach and compare it with other methods under identical training conditions. All approaches utilize the same video learner, ensuring equal GFLOPs. Our Open-MeDe achieves the lowest CUDA memory usage at $16.59$ GB and a significantly reduced epoch time of $74.33$ minutes, compared to other methods. This demonstrates its efficiency in terms of training time and memory consumption, providing a cost-effective solution without compromising on computational complexity.


\subsection{Visualization Results}
As shown in~\cref{fig:benchpress,fig:playingpiano,fig:horseriding,fig:diving}, we present additional visualization comparisons of Open-VCLIP and the proposed framework under in-context and out-of-context scenarios. Overall, our approach effectively attends to more motion-relevant regions, achieving higher confidence scores and correct predictions in most cases. This demonstrates its greater reliability, and robust generalizability in open-vocabulary action recognition tasks.