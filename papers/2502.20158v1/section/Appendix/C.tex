\section{Discussions}
\label{appen:discussion}
In this part, we further elucidate the core distinction between the proposed method and similar paradigms through comparative analysis.

\noindent {\bf Meta learner \vs Plain learner.}
As discussed in the main paper, Open-MeDe formulates the video learner into a meta learner by employing the cross-batch meta-optimization scheme that mimics sequences of known-to-open generalization tasks, enhancing adaptability to unseen data through iterative virtual evaluations during training.
Plain learners, such as those employing standard fine-tuning paradigms on CLIP-based video learners, are typically straightforward and focus on in-distribution class-specific knowledge.
Following a traditional gradient descent over a single objective function can lead to a narrower optimization landscape prone to overfitting.
Therefore, plain learners can gain reasonable in-context performance but struggle to generalize to novel and out-of-context scenarios due to the tendency to overfit in training data and short-cutting static cues. 

In contrast, our meta learner is designed to derive the training towards learning more generalizable features by optimizing not just for class-specific knowledge but for adaptability across diverse known-to-open tasks. 
It explicitly counteracts inherent known and static biases by leveraging feedback from virtual evaluations, ensuring the video learner does not over-rely on vulnerable static cues.
By alternating between \textit{meta training} (\wrt support data) and \textit{meta testing} (\wrt query data), the meta learner ensures smoother optimization trajectories and enhanced robustness in a cost-effective manner.
The episodic training of the meta learner fosters adaptability across varying class distributions, making it highly effective for open-vocabulary tasks.

% And meta-learning based training within our proposed framework is the key component to boost robust generalization.

\noindent {\bf Meta-optimization \vs Train-validation.}
% meta: train-evaluate feedback learning more generalizable features
% train-validation: training and evaluating for hyperparameter selection
In our meta-optimization framework, training involves two key stages: \textit{meta training} (on support data) and \textit{meta testing} (on query data). The query data evaluation provides generalization feedback via loss gradients, enabling the learner to adjust the learning trajectory to prioritize generalizable features. This iterative approach inherently targets learning to generalize and mitigates overfitting by encouraging robust learning across diverse distribution shifts.
Conversely, the train-validation paradigm typically partitions data into training and validation subsets, optimizing model parameters by minimizing errors on the training data while evaluating performance on a held-out validation set for hyper-parameter tuning or early stopping.
This paradigm monitors the generalization performance indirectly by balancing the performance between training and validation data without explicitly improving the open-vocabulary generalization capability toward novel data.

Both paradigms leverage the feedback to refine model training, where the feedback of meta-optimization comes from query evaluations, while in train-validation, it arises from validation performance.  
Additionally, the feedback of train-validation is aggregated at coarser intervals, limited to hyper-parameter adjustment on constant training-validation splits.
It is worth noting that the meta-optimization provides granular, iterative feedback during training, manifesting as loss gradients to refine generalizable representation learning by dynamically constructing tasks with support-query splits.
Therefore, the proposed meta-optimization framework provides a more robust and explicit mechanism for adapting to novel data, setting a new baseline for open-vocabulary action recognition.

% inherently adapts to unseen tasks through its iterative learning-to-learn paradigm.
% provides a more robust and explicit mechanism for adapting to unseen tasks, effectively addressing the challenges of static bias
% establishes a more principled balance between specialization and flexibility, setting a new standard for open-vocabulary action recognition.


\noindent {\bf Cross-batch meta-optimization \vs Gradient accumulation.}
As introduced in the Open-MeDe framework, the proposed cross-batch meta-optimization takes inspiration from meta-learning with minimal modification to the standard training setup, which leverages adjacent mini-batches in training, treating one as the support batch (\textit{meta training}) and the subsequent as the query batch (\textit{meta testing}).
It aims to explicitly promote generalization by evaluating how well the model can adapt its learned parameters to open or dynamically different data distributions, thereby mitigating inherent and static biases in the video learner.
When it comes to the gradient accumulation technique, by simulating large batch training, it aggregates gradients over multiple mini-batches and applies the update after a predefined number of steps, emphasizing the efficiency of stabilizing training and improving convergence on hardware-constrained scenarios. However, it primarily improves training stability without inherently targeting adaptability and enhanced generalization.
Therefore, cross-batch meta-optimization differs fundamentally from gradient accumulation in its goal and methodology, which achieves a superior balance between specialization and generalization.

\noindent {\bf Meta-debiasing with MVSGG~\cite{xu2022meta}.}
1) Objective \wrt mitigating biases.
% 【conditional bias from data vs. static bias from model】
MVSGG addresses certain conditional biases within video scene generation tasks, targeting long-tailed data issues.
Here, we tackle a ubiquitous challenge for video understanding, \ie mitigating static bias present in video learners.
2) Methodology \wrt meta-optimization.
% 【against explicit biases and meta-op per epoch vs. meta-op per iter】
MVSGG emphasizes on constructing various types of conditional biases within data at each training epoch, with its meta-optimization employed per epoch against specific biases.
We perform meta-optimization densely in iterations with a diverse distribution of tasks. The evaluation on a subsequent batch explicitly simulates known-to-open generalization and mitigates static bias implicitly.
3) Application scope \wrt generalization.
MVSGG enhances model's generalization under closed-set settings against conditional biases within training data.
Notably, we achieve more robust open-vocabulary generalization beyond training data, where MVSGG is insufficient to our requirements. 
4) Computational cost \wrt task construction.
MVSGG requires careful organization of training data, significantly increasing computational cost. Remarkably, our method incurs no additional computational overhead compared to standard training by effortlessly utilizing cross-batch data.

\noindent {\bf Gaussian self-ensemble with PromptSRC~\cite{khattak2023self}.}
Our GWA is related to PromptSRC with two key differences:
1) Objective \wrt implementation.
We aim to achieve a generic optimal solution for video learners by assigning different weights to learner's parameters during optimization, while PromptSRC focuses on regularizing prompt learning to reduce overfitting with frozen backbones.
2) Patching strategy \wrt start point.
Our GWA starts after fine-tuning the pre-trained weights of the learner (\eg, CLIP weights), which exhibits substantial static-related knowledge. With the purpose of mitigating static bias, the initial patching weights are sampled from low Gaussian probabilities.
However, the start point of PromptSRC is randomly initialized, given the prompt learning framework, where lower weight assignments guarantee the task-specific knowledge. 
