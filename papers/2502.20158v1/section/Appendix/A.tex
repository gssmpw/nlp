\section{Additional Experimental Details}
\label{appen:experimental details}
\subsection{Datasets}
\label{appen:datasets}
In this work, we categorize the datasets into \textit{in-context} and \textit{out-of-context} datasets. The videos from \textit{in-context} datasets consist of actions with frequent static context, \eg swimming in the swimming pool, while the videos from \textit{out-of-context} datasets contain actions occurring with an unusual static context, \eg dancing in the mall~\cite{choi2019can}.
We conduct the experiments on five \textit{in-context} benchmarks: Kinectics-400~\cite{k400} (K400), Kinectis-600~\cite{k600} (K600), UCF101~\cite{UCF101} (UCF), HMDB51~\cite{HMDB51} (HMDB), and Something-Something V2~\cite{ssv2} (SSv2). Additionally, we evaluate our approach on two \textit{out-of-context} benchmarks: SCUBA~\cite{li2023mitigating} and HAT~\cite{chung2022enabling}.

\noindent {\bf K400 and K600} are both comprehensive video datasets for human action recognition. K400 contains 400 action categories of approximately 240k training and 20k validation videos collected from YouTube, which covers a wide range of human actions, including sports activities, daily life actions, and various interactions, serving as a widely-used action recognition dataset for pre-training. The duration of video clips in K400 varies, with most clips being around 10 seconds long. This diversity in video duration helps models learn temporal dynamics and context for action recognition. K600 extends K400 by incorporating 220 additional new categories, thus enabling the evaluation of zero-shot learning capabilities on these novel categories. 

\noindent {\bf UCF} is a human action recognition dataset collected from YouTube, and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories encompass a wide range of realistic actions including body motion, human-human interactions, human-object interactions, playing musical instruments and sports. Officially, there are three splits allocating 9,537 videos for training and 3,783 videos for testing.

\noindent {\bf HMDB} is a relatively small video dataset comprising a diverse range of sources, including movies, public databases, and YouTube videos, and is composed of 6,766 videos across 51 action categories (such as ``jump'', ``kiss'' and ``laugh''), ensuring at least 101 clips within each category. The original evaluation scheme employs three distinct training/testing splits, allocating 70 clips for training and 30 clips for testing of each category in each split.

\noindent {\bf SSv2} is a temporally focused video dataset across 174 fine-grained action categories, consisting of 168,913 training videos and 24,777 testing videos showing the objects and the actions performed on them. These action categories are presented using object-agnostic templates, such as ``Dropping [something] into [something]'' containing slots (``[something]'') that serve as placeholders for objects. This dataset focuses on basic, physical concepts rather than higher-level human activities, which challenges the temporal modeling capabilities.

\noindent {\bf SCUBA} is an out-of-distribution (OOD) video benchmark designed to quantitatively evaluate static bias in the background. It comprises synthetic out-of-context videos derived from the first test split of HMDB and UCF, as well as the validation set of K400. These videos are created by superimposing action regions from one video onto diverse scenes, including those from Place365~\cite{zhou2017places} and VQGAN-CLIP~\cite{crowson2022vqgan} generated scenes. 
Due to the differences in test sets and background sources, the domain gaps of SCUBA benchmarks vary. A domain gap is defined as the ratio of accuracies between the original test sets and synthetic datasets obtained by a 2D reference network, where a higher ratio indicates a greater domain gap with respect to static features.
The UCF-SCUBA and K400-SCUBA used in our experiments consist of 4,550 and 10,190 videos with domain gaps of $20.49$ and $6.09$, respectively, whose backgrounds are replaced by the test set of Place365.

\noindent {\bf HAT} is a more ``realistic-looking'' mixed-up benchmark for quantitative evaluation of the background bias by automatically generating synthetic counterfactual validation videos with different visual cues. It provides four Action-Swap sets with distinct characteristics: \textit{Random} and \textit{Same} refer to the swap of actions and backgrounds from different and same classes, respectively, while \textit{Close} and \textit{Far} denote the swap of videos from a class with similar and very different backgrounds, respectively. The UCF-HAT benchmark used in our experiments consists of Action-Swap videos in \textit{Close} and \textit{Far} sets from 5 closest and 30 farthest action categories, respectively, following the literature~\cite{chung2022enabling}. Note that we only consider videos from the first test split of UCF where all frames have human masks taking up 5\% to 50\% of the pixels to ensure that sufficient human and background cues are present in each generated Action-Swap video.

\subsection{Evaluation Protocols}
\label{appen:EP}
For the experimental settings, we follow the previous works~\cite{weng2023open,rasheed2023fine,ni2022expanding} for in-context generalization evaluations and perform the newly proposed out-of-context generalization evaluations described below.

\noindent {\bf In-context base-to-novel generalization.}
Under this setting, we divide the entire set of action categories into two equal halves: base and novel, with the most frequently occurring classes designated as the base classes. We conduct generalization evaluations on four in-context datasets, \ie K400, HMDB, UCF and SSv2, where the models are initially trained on the base classes within the training splits of the dataset, and evaluated on both base and novel classes within the validation splits. Every training split consists of 16 video clips of each base class. During inference within HMDB and UCF datasets, only the novel class samples in the first validation splits are used for evaluation. For K400 and SSv2 datasets, the full validation split of each is used for evaluation here. We report the results of the average top-1 accuracies for both base and novel classes as well as the harmonic mean.

\noindent {\bf In-context cross-dataset generalization.}
Under this setting, the models are fine-tuned on the training set of K400, and evaluated on three in-context cross-datasets, \ie UCF, HMDB and K600. We report top-1 average accuracies with performance variances on the three validation splits in case of UCF and HMDB. For K600, the models are evaluated on non-overlapping 220 categories with K400, and we report top-1 average accuracies over three randomly sampled splits of 160 categories.

\noindent {\bf Out-of-context cross-dataset generalization.}
Under the more challenging out-of-context cross-dataset setting, the models are also trained on K400, and then evaluated on two out-of-context datasets based on UCF, \ie UCF-SCUBA and UCF-HAT. We report the top-1 and top-5 average accuracies over the synthetic counterfactual validation splits from UCF's first validation split. We further conduct the closed-set out-of-context evaluation based on the K400-SCUBA benchmark and report the harmonic mean of the accuracies under in-context and out-of-context settings to comprehensively analyze the generalization of the models.

\subsection{Implementation Details}
\label{appen:implementation}
Each training video clip is sampled with $8$ frames uniformly, and each sampled frame is spatially scaled in the shorter side to $256$ pixels and is processed with basic augmentations like color jittering, random flipping and random cropping of $224\times 224$.
We leverage multi-view inference with $3$ temporal and $1$ spatial views per video and linearly aggregate the recognition results. 
For our Gaussian Weight Average scheme, we use $\mu=7$ and $\sigma^2=10$ for in-context base-to-novel generalization and $\mu=15$ and $\sigma^2=10$ for in-context and out-of-context cross-dataset generalization.
We also adopt decision aggregation with pre-trained CLIP with the video learner for in-context evaluations.
The experiments are conducted on two computing clusters with four NVIDIA RTX 24G 4090 GPUs.
