\section{Broader Impacts and Limitations}
\label{appen:limitation}

\noindent {\bf Broader Impacts.}
The proposed Open-MeDe framework for open-vocabulary action recognition introduces substantial advancements in several key aspects, underscoring its broader impact on both research and real-world applications: 
1) By addressing the overfitting to static cues inherent in pre-trained models like CLIP, Open-MeDe introduces innovative solutions for robust generalization. Its combination of meta-optimization and Gaussian self-ensemble stabilization enables robust performance in challenging out-of-context scenarios, providing a pathway for video learners to bridge the gap between image and video modalities effectively. 
2) Unlike previous approaches reliant on CLIP regularization, Open-MeDe reduces computational overhead and efficiently balances class-specific learning with generalization capabilities, leveraging a cross-batch meta-optimization approach. 
3) Open-MeDe demonstrates remarkable adaptability across diverse scenarios, including base-to-novel, cross-dataset, and out-of-context evaluations. Its model-agnostic design enables seamless integration with various CLIP-based video learners, enhancing performance across parameter-efficient fine-tuned, partially-tuned, and fully-tuned video learners. This flexibility significantly broadens its utility, making it a versatile tool for tasks requiring robust generalization without extensive domain-specific tailoring.
4) Extensive experiments demonstrate the state-of-the-art results achieved by our Open-MeDe, highlighting its role in advancing general video understanding. Our framework can empower many downstream applications, such as video-based surveillance and security, autonomous vehicles, human-computer interaction, \etc.

\noindent {\bf Limitations.}
Despite achieving promising open-vocabulary generalization with our framework, the out-of-context scenarios remain challenging and constrained by the reliance on temporal and static feature alignment. 
Specifically, scenarios with extreme domain shifts (\eg, SCUBA and HAT benchmarks) show significant performance gaps. 
However, the residual influence of static visual cues persists, particularly in complex video backgrounds and more compact foregrounds.
Incorporating stronger, explicitly targeted debiasing strategies, such as adversarial learning or counterfactual data augmentation, may further enhance robustness, which will be explored in our future work.

