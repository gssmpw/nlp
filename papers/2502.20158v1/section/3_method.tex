\section{Method}
\label{sec:method}
\input{Figs/fig2}
\subsection{Preliminaries}
\noindent {\bf Action recognition with CLIP-based video learner.}
% 
Pre-training on large-scale image-text data based on contrastive learning, CLIP learns separate uni-modal encoders for image and text, embedding them into a joint feature space, respectively.
% 
Consider a CLIP-based video learner with a ViT architecture~\cite{dosovitskiy2020image}, that incorporates temporal modeling for video understanding~\cite{wu2023revisiting,wang2021actionclip,weng2023open,zhu2023orthogonal,yang2023aim,wasim2023vita}.
% 
Next, we present the standard vision-only fine-tuning paradigm that applies such a video learner $f_{\theta_v}$ with a frozen text encoder $f_{\theta_t}$ to action recognition. 
% 
Specifically, given a video clip $V_i$, and a candidate action label $T_j\in \mathcal{Z}_{tr}$ described in predefined textual templates (\eg, ``\textit{a video of \{action\}}'') from the training set $\mathcal{D}_{tr}$, the similarity is calculated as:
\begin{equation}
    s_{i,j}=\frac{\left \langle v_i,t_j \right \rangle }{\left \| v_i \right \| \left \| t_j \right \| },v_i=f_{\theta_v}(V_i),t_j=f_{\theta_t}(T_j),
\end{equation}
where the training objective is to maximize it of the matched $V_i$ and $T_j$, or to minimize it otherwise. The loss function is implemented by the cross-entropy loss in~\cite{wu2023revisiting,chen2021empirical,radford2021learning} as:
\begin{equation}
\label{eq:ce}
    \mathcal{L}_{CE} = -\frac{1}{B}\sum_{i}^{B}\sum_{k}^{K}y_{i,k}\log \left ( \frac{\exp(s_{i,k})}{\sum_{j}^{K}\exp (s_{i,j})}  \right ),  
\end{equation}
where $B$ and $K$ denote the minibatch size and the number of all known classes, respectively. If the $i$-th video belongs to the $k$-th class, $y_{i,k}$ equals $1$; otherwise, $y_{i,k}$ equals $0$.
% 
In OVAR, the trained video learner should achieve good generalization on test data with the class label $T_i\in \mathcal{Z}_{te}$, where $\mathcal{Z}_{te} \cap \mathcal{Z}_{tr}=\emptyset $.

\noindent {\bf Model-agnostic meta-learning (MAML).}
MAML~\cite{finn2017model} is a gradient-based meta-optimization framework designed for few-shot learning, which aims to learn good initialization such that a few gradient steps will lead to fast learning on new tasks.
% 
Formally, consider a model $f_\theta$ with parameters $\theta$, MAML learns a set of initial weight values, which will serve as a good starting point for fast adaptation to a new task $\mathcal{T}_i$, sampled from a task distribution $p(\mathcal{T})$.
% 
When adapting to the task $\mathcal{T}_i$, the fast weights $\theta_i'$ are computed \textit{w.r.t.} examples from $\mathcal{T}_i$ though single inner-loop update as:
\begin{equation}
    \theta_i'=\theta-\alpha \nabla_\theta \mathcal{L}_{\mathcal{T}_i}(f_\theta),
\end{equation}
where $\alpha$ denotes the step size for inner loops. Then, the model with fast weights $f_{\theta_i'}$ is evaluated on new samples from the same task $\mathcal{T}_i$, to act as the feedback (\ie, loss gradients) to adapt to current task $\mathcal{T}_i$ to optimize the initialization $\theta$ for generalization as:
\begin{equation}
\label{eq:maml}
    \theta \leftarrow \theta-\beta \nabla_\theta \sum_{\mathcal{T}_i}\mathcal{L}_{\mathcal{T}_i}(f_{\theta_i'}).
\end{equation}
where $\beta$ is the step size for outer loops. 
Computationally, due to the additional backward propagation burden of the gradient by gradient update, MAML presents a first-order approximation, FOMAML, by dropping the backward pass. 
% Additionally, Reptile~\cite{nichol2018first} algorithm defines the update as:
% \begin{equation}
% \label{eq:reptile}
%     \theta \leftarrow \theta+ \epsilon(\theta_i'-\theta),
% \end{equation}
% where $(\theta-\theta_i')/\alpha$ can serve as the gradient term with scaling factor $\epsilon$ for updating $\theta$ for adaptation to task $\mathcal{T}_i$.

\subsection{Open-MeDe}
% 【(—)说现有】
As discussed above, the standard fine-tuning paradigm can cause the video learner to overfit to the known classes during training, leading to poor zero-shot capabilities. Also, CLIP regularization-based approaches face challenges in achieving robust generalization due to the excessive reliance on superficial static cues in videos.
% 【(+)说解决】
To tackle these issues, we draw upon the philosophy and methodology from meta-learning, and propose Open-MeDe framework, which is illustrated in~\cref{fig:overview}, to enhance both know-to-open generalizing and image-to-video debiasing simultaneously.

\subsubsection{Cross-batch meta-optimization}
% 【(+)说方法】
Our Open-MeDe framework primarily adopts a cross-batch meta-optimization scheme (in~\cref{fig:overview}(a)) to enhance the video learner via \textit{meta training and testing}, enabling it to acquire generalizable, video-specific knowledge instead of overly exploiting static biases. 
% 【说划分】
Note that we neither sample from a distribution of $N$-way $K$-shot tasks as done in few-shot MAML nor deliberately split the training set into support and query sets as Meta-ZSL~\cite{liu2021task,verma2020meta} suggested. Instead, our support and query examples are constructed effortlessly and arbitrarily by the default training data sampler.
% 【说cross-batch好处】
In effect, we consider this arbitrariness a blessing for building the natural ``\textit{known-to-open generalization task}'', since the known biases in \textit{meta training} data do not hold in \textit{meta testing} data due to different inherent label distributions across batches.
% 【说meta-training】
A known-to-open task can be created by extending the original gradient step into two consecutive mini-batches in one pass, with the current batch acting as support data and the subsequent batch as query data.
Specifically, in line with the episode-wise training akin to MAML, we first train the learner within an inner loop (\ie, \textit{meta training}), where the fast weights are obtained through a single gradient step for each support batch.
% 【说meta-testing】
Following this adaptation, in the outer loop, query videos are sampled to evaluate the generalization performance of the adapted learner with fast weights (\ie, \textit{meta testing}).
% 【说优化】
In this work, our framework further updates the fast weights of the learner based on the evaluation performance during \textit{meta testing}, which then provides feedback for the task to derive more generalizable optimization for the learner.

\noindent {\bf Meta training.}
At each training iteration, we first utilize each support batch $\mathcal{S}=\{V_i,T_i\}^B$ from the task $\mathcal{T}_i$ to train the video learner $f_{\theta}$ (with parameters $\theta$), via one standard gradient step. The inner loop update is governed by the loss on the support batch as:
\begin{equation}
\label{eq:spt}
     \mathcal{L}_{\mathcal{T}_i}^\mathcal{S}(\theta) =\mathcal{L}(f_{\theta }(\mathcal{S})),
\end{equation}
where $\mathcal{L}(\cdot)$ refers to the loss function (\eg, the cross-entropy loss $\mathcal{L}_{CE}$ \wrt Eq.~\eqref{eq:ce}). Then, we make a temporary copy for the original parameters $\theta$ and update the intermediate parameters for fast weights as follows:
\begin{equation}
\label{eq:spt_update}
    \theta'_i = \theta-\alpha \nabla_{\theta} \mathcal{L}_{\mathcal{T}_i}^{\mathcal{S}}(\theta),
\end{equation}
where $\alpha$ denotes the learning rate for \textit{meta training}. Intuitively, this step simulates a direct update to train the learner to obtain class-specific knowledge of the support data.

\noindent {\bf Meta testing.}
After meta training on the support batch, we then scheme a virtual testing process, leveraging the query batch $\mathcal{Q}=\{V_i, T_i\}^B$, where $\mathcal{S}\cap \mathcal{Q}=\emptyset$,  to evaluate the generalization performance of the base learner $f_{\theta_i'}$. Formally, we measure the known-to-open performance on $\mathcal{T}_i$ by calculating the class-specific loss concerning the query data as:
\begin{equation}
\label{eq:qry}
    \mathcal{L}_{\mathcal{T}_i}^\mathcal{Q}(\theta'_i) =\mathcal{L}(f_{\theta_i' }(\mathcal{Q})).
\end{equation}
Here, the formulation closely relates to the standard fine-tuning process, which aims to obtain decent class-specific performance for all training batches. Differently, this step merely evaluates the intermediary base learner for its known-to-open generalizability on each task, due to the original parameters $\theta$ remaining immune to the task-specific updates.
% 
Hence, it can be used to provide feedback on \textit{what video-specific knowledge should be learned in the sense that the learner can derive the robust generalization across different class distributions against inherent known and static biases} in the following meta-optimization.

\noindent {\bf Meta-optimization.}
As mentioned above, the intuition behind our approach is that the virtual evaluation during meta testing can provide useful feedback to encourage the learning of more robust representations for fast known-to-open generalization after \textit{meta training} on the support data (\ie, $\theta'_i \leftarrow \theta$).
Note that original MAML approaches focus on optimizing parameters for a strong initialization, enabling quick adaptation to new tasks with minimal gradient updates. Conversely, open-vocabulary recognition requires zero-shot capabilities, where no further adaptation can be applied for new tasks. Therefore, class-specific knowledge should be strengthened in terms of global optimization.
To this end, within the outer loop, the parameters of the learner are optimized to minimize the class-specific errors for the support data and the adaptation cost for the query data simultaneously. 
The combination of both Eq.~\eqref{eq:spt} and Eq.~\eqref{eq:qry} is used to carry out the outer loop update, thus the objective for meta-optimization can be defined as:
\begin{equation}
\label{eq:maml*}
\begin{aligned}
\min _{\theta}\mathcal{L}_{\mathcal{T}_i}(\theta )=& \min _{\theta}\left( \mathcal{L}_{\mathcal{T}_i}^\mathcal{S} (\theta) +\mathcal{L}_{\mathcal{T}_i}^\mathcal{Q}(\theta_i') \right)  \\
= & \min _{\theta} \left(\mathcal{L}_{\mathcal{T}_i}^\mathcal{S} (\theta)+\mathcal{L}_{\mathcal{T}_i}^\mathcal{Q}(\theta-\alpha \nabla_{\theta} \mathcal{L}_{\mathcal{T}_i}^{\mathcal{S}}(\theta))\right).
\end{aligned}
\end{equation}
Here, the first term refers to the class-specific knowledge learned on the support batch, while the second term provides the known-to-open generalization feedback based on $\theta_i'$ towards robust representation learning \wrt the task $\mathcal{T}_i$.
The optimizing process of the parameter $\theta$ can be given by:
\begin{equation}
\label{eq:maml**}
    \theta \leftarrow \theta - \beta \nabla_{\theta} \sum _{i = 1}^{N}\left( \mathcal{L}_{\mathcal{T} _i}^\mathcal{S} (\theta) + \mathcal{L}_{\mathcal{T} _i}^\mathcal{Q} \left ( \theta - \alpha \nabla_{\theta} \mathcal{L}_{\mathcal{T} _i}^\mathcal{S} (\theta) \right) \right),
\end{equation}
where $N$ is the batch size of the task for meta-optimization.
Since the MAML meta-gradient update needs to differentiate through the optimization process (\ie, a gradient by a gradient), it’s not an ideal solution where we need to optimize a large number of tasks during the training phase. 
Therefore, we opt for the one-step update approximation by dropping the backward pass of $\theta \gets \theta'_i$, where Eq.~\eqref{eq:maml**} can be rewritten as:
\begin{equation}
\label{eq:FOMAML}
    \theta \leftarrow \theta - \beta  \sum _{i=1}^{N}\left (\nabla_{\theta}\mathcal{L}_{\mathcal{T} _i}^\mathcal{S} (\theta) +\delta \nabla_{\theta'_i} \mathcal{L}_{\mathcal{T}_i}^{\mathcal{Q}}(\theta'_i)\right),
\end{equation}
where $\beta$ and $\delta$ are the learning rates for meta-optimization.
With the genuine update of the learner in Eq.~\eqref{eq:FOMAML} without CLIP regularization, we can optimize a parallel or batch version that evaluates on $N$ known-to-open tasks of different class distributions (\ie, class-specific knowledge), which encourages to learn more generalizable features against known and static biases.

\input{Tabs/1-base-to-novel}
\subsubsection{Gaussian self-ensemble stabilization}
% 【(—)说传统】
Typically, training the video learner for longer iterations to gain specialization on the supervised tasks comes with the risk of diminished plasticity and generalizability. 
Model patching~\cite{ilharco2022patching,wortsman2022robust,shu2023clipood,weng2023open} of weight ensembling has been shown to improve both the performance and generalization.
% 【(*)说谨慎利用】
Given that the fine-tuning videos are limited in class-specific knowledge, while the open-vocabulary tasks are unconstrained, the static generalizable flexibility derived from large-scale I-VL pre-training should be scrupulously exploited to enhance the adaptation of the video learner while minimizing the impact of static bias.
% 【(+)说GWA】
Therefore, we further incorporate self-ensemble stabilization tailored to the video learner over its optimization trajectory, which utilizes the knowledge from previous training iterations for a generalizable solution.
% 
In a fine-tuning procedure of $R$ epochs with $l$ step length for each, the learner's optimization trajectory is represented by $\{\theta_t\}_{t=0}^R$, where $\theta_0$ is the pre-trained weights. The self-ensemble averages the weights of the learner as:
\begin{equation}
    \theta_{\texttt{WA}}= \sum_{t=0}^{R}\frac{w_t}{\sum_{i=0}^{R}w_i}\cdot \theta_t,
\end{equation}
where $w_t$ specifies the weight contributed by the parameters at $t$-th epoch.
% 【(—)说早期】
Intuitively, during the early fine-tuning epochs (\ie, at a smaller epoch $t$), the video learner lacks the maturity to effectively capture video-specific knowledge while still retaining substantial static-related orientation from large-scale pre-training, which introduces vulnerable information for temporal understanding. 
% 【(—)说晚期】
Conversely, the parameters at the last few epochs (\ie, at a larger epoch $t$) have integrated more video-specific knowledge, highly featuring the supervised downstream task distribution, whereas the plasticity of the unconstrained zero-shot capability is not guaranteed.
% 【(—)说动机】
As both sides degrade the final open-vocabulary generalizability, we aim to weaken the contribution of the parameters near the initial and terminal epochs by employing a distribution prior, resulting in a generic optima for robust generalization.
\input{Tabs/training}
% 【(+)说GWA】
Driven by~\cite{khattak2023self} in prompt learning, we perform Gaussian Weight Average (GWA) based on model patching, as shown in~\cref{fig:overview}(b), which assigns the parameters with lower weights at initial epochs, higher weights at middle epochs, and relatively lower weights at final epochs.
% 【(+)】
Given a  Gaussian distribution $w_t\sim \mathcal{N}(\mu,\sigma^2)$ defined over the epochs, we sample the weight values for the parameters $\theta_t$ as its corresponding probability in the distribution as:
\begin{equation}
    w_t=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(t-\mu)^{2}}{2 \sigma^{2}}}, t=1,\dots,R.
\end{equation}
Here, we exclude the integration of CLIP weights $\theta_0$ for the purpose of static debiasing. $\mu$ and $\sigma^2$ are hyper-parameters for the distribution, and in practice, we determine the value of $\mu$ according to the epoch number. Then, we perform normalization towards the weights of total epochs to achieve $\sum_{t=1}^{R}w_t=1$. 
We also formulate GWA as a moving average to avoid increasing the storage cost of saving multiple snapshots of the parameters by updating the average of current learner $\theta_t$ on the fly (\ie, at epoch $t$) as:
\begin{equation}
\label{eq:GWA}
    \theta_{\texttt{GWA}}\gets \frac{\sum_{i=1}^{t-1} w_{i}}{\sum_{i=1}^{t} w_{i}} \cdot \theta_{\texttt{GWA}}+\frac{w_{t}}{\sum_{i=1}^{t} w_{i}} \cdot \theta_{t} .
\end{equation}


\subsection{Algorithm overview}
We present the overall training procedure of the proposed model-agnostic Open-MeDe in~\cref{alg:training}. The video learner is fine-tuned on training videos based on our cross-batch meta-optimization scheme cost-effectively. And the Gaussian self-ensemble stabilization is performed on the video learner via our GWA for robust generalization under open-vocabulary settings.

