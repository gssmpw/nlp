\section{Introduction}
\label{sec:intro}
\input{Figs/fig1}
% 【(—)说挑战】
Open-vocabulary Action Recognition (OVAR) aims to identify test videos whose classes are not previously encountered during the training phase, which challenges the generalization and zero-shot capabilities of the video learners~\cite{zhu2023orthogonal,brattoli2020rethinking,wu2023revisiting}. 
% 【(+)说I-VL的静态泛化】
Recently, the emergence of image-based visual-language (I-VL) pre-training, such as CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling}, has shown promising zero-shot inference in image-based tasks. 
% 【(#)说传统范式】
Inspired by this success, recent attempts~\cite{brattoli2020rethinking,wasim2023vita,chen2024ost,ni2022expanding,pan2022st} have been made to adapt CLIP for general action recognition via additional temporal modeling following the ``\textit{pre-train, prompt and fine-tune}'' paradigm~\cite{wang2021actionclip}. 
% 【(#)说标准微调】
Broadly, these video learners optimize the learnable parameters from the start point of CLIP, pursuing decent performance on the training videos, known as standard fine-tuning objectives.
% 【(—)说过拟合】
However, adapting CLIP to the video domain, especially for OVAR, is extremely challenging, as the video learners with standard fine-tuning objectives often lead to overfitting, which achieves improved specialization at the cost of generalization degradation.

% 【(+)说正则化】
To build an improved zero-shot video learner, Open-VCLIP~\cite{wu2024building} and FROSTER~\cite{huang2024froster} propose to regularize the fine-tuning process curbing deviation from CLIP's generalization from the perspective of model patching~\cite{ilharco2022patching} and knowledge distillation~\cite{castro2022fitclip,dai2022enabling,pei2023clipping}, respectively. 
% 【(+)说inc性能好/说in-context】
In~\cref{fig:teaser}, these methods have achieved satisfying performance compared to frozen CLIP and X-CLIP~\cite{ni2022expanding} on UCF101~\cite{UCF101} dataset under in-context open-vocabulary evaluation, where the action categories have strong correlations with the context in videos.
% 【(—)说ooc性能差】
However, when it comes to the out-of-context evaluation in SCUBA~\cite{li2023mitigating}, where the video background is replaced by other images, the performance degrades severely.
% 【(—)说静态偏见】
As these video learners are intimately tied to the learning of shortcut static features, which manifest as static bias, they interfere with the learning of motion cues, resulting in poor out-of-context generalization~\cite{duan2022mitigating}.
% 【(#)说动机】
Based on these observations, we argue that the static generalization of CLIP can (1) effectively adapt to in-context scenarios for OVAR by regularizing video learners; yet (2) it undesirably hinders the sensitivity of such video learners to motion cues, exerting a notable detrimental impact on generalization under out-of-context, open-vocabulary setting.

% 【(#)引设问】
\textit{How can we encourage the emergence of such robust open-vocabulary generalization for both in-context and out-of-context scenarios?}
% 【(+)说解决】
We explore an explicit approach to this problem: as the video learner is trained with a sampled batch of videos at each gradient step, our objective is to optimize the learner from a meta-learning standpoint so that it can quickly adapt to arbitrary subsequent data, thereby minimizing inherent biases toward known data and static cues.

% 【(+)说框架】
Based on this insight, we propose \textbf{Open-MeDe}, a novel \underline{Me}ta-learning based framework with static \underline{De}biasing for in-context and out-of-context \underline{Open}-vocabulary action recognition.
% 【(+)说meta learning】
Meta-learning, also known as ``\textit{learning to learn}'', incorporates virtual evaluation during the training process for better generalization~\cite{finn2017model,nichol2018first,antoniou2018train}.
% 【(+)说task】
In our meta-learning scheme, the ``\textit{learning to generalize}'' process is enhanced by naturally treating sequences of adjacent batches sampled from the training set as a distribution of tasks. 
% 【(+)说meta过程】
More concretely, our procedure optimizes the video learner to obtain fast weights by gradient descent updates on the current batch (\ie, \textit{meta training}), while evaluating the subsequent batch (\ie, \textit{meta testing}) based on fast weights of the learner, which mimics a known-to-open task.
% 【(+)说优化】
Based on the evaluation performance in \textit{meta testing}, our procedure can further optimize the learner to obtain more generalizable video-specific knowledge against inherent known and static biases.
In effect, this cross-batch meta-optimization formulates a meta-learner free of CLIP regularization, thereby facilitating smoother optimization and robust video representation learning for fast known-to-open generalizing, thus enhancing image-to-video debiasing.
% 【(+)说gwa】
Tailored to the optimization trajectory of the video learner, we further employ self-ensemble stabilization, \ie, Gaussian Weight Average (GWA), to derive generic optima for robust generalization at open-vocabulary test time.
% 【(+)说性能】
Overall, while integrating the same video learner, our model-agnostic Open-MeDe outperforms existing regularization-based methods, which strikes a promising balance on in-context and out-of-context generalization settings (\cref{fig:teaser}).

% 【(+)说贡献】
The contribution of our work can be summarized as:
\begin{itemize}
    \item We introduce a novel meta-learning based framework, Open-MeDe, which provides new insights for more generalized open-vocabulary action recognition.

    \item We propose cross-batch meta-optimization and self-ensemble stabilization, which effectively power known-to-open generalizing and image-to-video debiasing of the video learner for robust generalizability.

    \item We conduct extensive evaluations on various scenarios including base-to-novel, cross-dataset, and out-of-context open-vocabulary action recognition. Experimental results show that Open-MeDe consistently improves performance across all the benchmarks.

\end{itemize}