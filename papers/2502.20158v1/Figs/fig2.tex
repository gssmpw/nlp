%%%%%%%% 图2：  %%%%%%%%%%%%%%%%
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figs/02.pdf}
    \caption{Illustration of our framework. (a) The cross-batch meta-optimization scheme aims to mimic the known-to-open generalization task $\mathcal{T}_i$ by performing the gradient descent update (\ie, \textit{meta training}) on the support batch $\mathcal{S}$ and virtual evaluation (\ie, \textit{meta testing}) on the query batch $\mathcal{Q}$. Then, the video learner is optimized by both class-specific losses from $\mathcal{S}$ and task feedback from $\mathcal{Q}$ for more generalizable knowledge against inherent known and static biases. (b) Overview of the Open-MeDe framework with self-ensemble stabilization. During the episodic training process, we exploit the optimization trajectory of the video learner to perform Gaussian Weight Average (GWA) to derive generic optima for robust generalization.}
    \label{fig:overview}
\end{figure*}