\section{RELATED WORK}
\subsection{Referring Image Segmentation for Natural Images}
Referring Image Segmentation (RIS), an extension of visual grounding ____, aims to segment specific regions in an image based on textual descriptions. This task requires not only precise alignment of text and image features but also the ability to accurately delineate object boundaries, handle fine-grained details, and manage object appearance variations across different scales and contexts.

In the early stages, RIS models primarily used convolutional and recurrent neural networks to extract features, which were then combined for joint prediction. Hu et al. first introduced the RIS task to address challenges in existing semantic segmentation tasks when dealing with complex referential texts____. Since then, Li \textit{et al}. and Nagaraja \textit{et al}. employed Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) for processing bimodal information and performing joint mask prediction____. By incorporating a modal interaction structure, Liu \textit{et al}. improved cross-modal alignment____. Moreover, Margffoy \textit{et al}. introduced a dynamic multimodal network to integrate recursive visual and linguistic features____.

Follow-up studies emphasize the importance of effective cross-modal interactions, with self-attention mechanisms serving as a cornerstone for facilitating such interactions. For example, Ye \textit{et al}. proposed a cross-modal self-attention module to capture long-range dependencies, paired with a gated multi-level fusion module for feature integration____. Similarly, Hu \textit{et al}. introduced a bidirectional cross-modal attention module to enhance alignment between linguistic and visual features____. Moreover, Shi \textit{et al}. developed a keyword-aware network that leverages keywords to refine region relationships____.

With the continuous evolution of deep learning architectures, Transformers have emerged as the leading approach for RIS, offering superior global modeling and cross-modal alignment. Transformer-based methods, such as CGAN____, LAVT____, RESTR____, M3Att____, MagNet____, have significantly advanced the field. Kim \textit{et al}. introduced the first convolution-free architecture, leveraging Transformers for long-range interactions____. Liu \textit{et al}. proposed multi-modal mutual attention and a mutual decoder for improved feature integration____. Luo \textit{et al}. designed a cascaded group attention network for iterative reasoning____, while Chng \textit{et al}. introduced a text reconstruction task for fine-grained cross-modal alignment____.

In conclusion, while RIS research has progressed from traditional to Transformer-based architectures, achieving notable improvements in multimodal interaction and alignment, it continues to face challenges such as fine-grained object differentiation, managing occlusions, and accurately aligning spatially distributed objects with their corresponding textual descriptions. Additionally, context-dependent understanding and the inherent ambiguity of language remain significant hurdles.

\subsection{Referring Remote Sensing Image Segmentation}
%They established a new dataset named RefSegRS, which collects and annotates 4,420 triplets of remote sensing images-language-label for the RRSIS task. Designed a large-scale benchmark dataset called RRSIS-D, which includes 17,402 images- language-label triples for the RRSIS task. Additionally, they %
Identifying objects in remote sensing images typically requires specialized expertise. Referring Remote Sensing Image Segmentation (RRSIS) has gained attention as it helps non-expert users extract precise information based on textual descriptions. This task is closely related to visual grounding in remote sensing ____, where textual cues are aligned with specific image regions. However, research in this area is still in its early stages____ with limited studies available. 

%They established a new dataset named RefSegRS, which collects and annotates 4,420 triplets of remote sensing images-language-label for the RRSIS task. Designed a large-scale benchmark dataset called RRSIS-D, which includes 17,402 images- language-label triples for the RRSIS task. Additionally, they %
Liu \textit{et al}. introduced the RRSIS task, specifically designed to segment targets in remote sensing images based on natural language expressions____. This work references the LAVT model proposed by____ for the natural image referring segmentation task. To address the common occurrence of small and dispersed objects in remote sensing images, ____ designed the cross-scale enhancement module that effectively utilizes language guidance to integrate deep and shallow visual features, enhancing the model's ability to discern small targets. Liu \textit{et al}. proposed a model named RMSIN that deal with the complex spatial scale and directional issues in remote sensing images through rotated convolutions____.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{relate.png}
    \vspace{-5mm}
    \caption{A comparison of two scenarios. (a) shows clear targets with simple descriptions, demonstrating the superior performance of existing methods. (b) illustrates complex descriptions with ambiguous backgrounds, highlighting the limitations of current approaches. }
    \label{fig:relate}
    
\end{figure}

In general, as shown in Figure \ref{fig:relate}, existing methods for referring remote sensing image segmentation achieve good performance when coping with visually clear objects and brief descriptions. However, their effectiveness diminishes when faced with complex textual relationships, as their capabilities in textual feature extraction and visual-textual modality alignment are limited, leading to inferior results. Additionally, these models cannot effectively address potential confusion between targets and backgrounds in remote sensing images.
%Fig. 2. 展示了我们的xxx模型的全部结构，主要包括三部分组件：bidirectional feature extractor，Target-Background twinStream decoder， and the Rephrase module。双向特征提取器对于一组输入的图像文本对 i-e，会在分析特征的过程中进行层级化的多尺度两模态交互，经过Bidirectional Spatial Correlation Module处理后的多模态特征 V'和L0会以视觉分支和文本分支输入注意力模块提取双向的上下文信息以强化特征的对象和环境感知信息，以此得到具备前景提示的文本嵌入。同时，xxx会先对文本进行实体对象的mask得到Lm，并将其的描述属性部分经过文本编码器做为用于背景提示文本嵌入的先验信息，经过可学习层融合后，与前景对象文本嵌入一同参与视觉特征的查询和解码，从而得到预测目标对象及其周边环境背景的掩码。此外，我们的rephrase模块会利用视觉主导的多模态特征引导“被擦除”文本Lm的重建，促进模型的语义理解学习。接下来我们将针对每个部分的具体构建情况进行阐述。
\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{methodgai.png}
    \vspace{-6mm}
    \caption{The overall framework of the proposed \ours Network includes the following components: \textbf{(a)} \textbf{Bidirectional Feature Extractor}, in which the visual and text encoders extract features from image and text inputs, respectively, with the Bidirectional Spatial Correlation Module enabling bidirectional information exchange at the feature level. \textbf{(b)} \textbf{Target-Background TwinStream}, implementing a text-aware dual-stream inference strategy for entity targets and category-agnostic areas during the mask prediction stage. \textbf{(c)} \textbf{Dual-Modal Object Learning}, designed to apply a reconstruction loss named \textit{Lre} after erasing text entity words, in which the reconstruction module leverages multimodal visual features to guide text reconstruction.}
    \vspace{-3mm}
    \label{fig:method}
\end{figure*}