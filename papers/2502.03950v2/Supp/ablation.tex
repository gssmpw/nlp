
\section{Implementation Details}

\textbf{Dataset Weights:}
In Table~\ref{tab:dataset_weight_value}, we have shown the dataset-specific weight values used to compute weighted aggregated robustness for low-resolution. All models were trained on 2 48GB GPUs. 
% DATASET\_WEIGHTS = 'Imagenet': 0.15556157429688613, 'ImageNet-A': 0.970498446080589, 'ImageNet-V2': 0.2854574367981364, 'ImageNet-R': 0.01, 'ImageNet-Sketch': 0.021456095637452655, 'Caltech101 (300 x 200)': 0.01, 'DTD split-1 (300x300 - 640x640)': 0.505922498560715, 'Food101 (512*512)': 0.01, 'SUN397': 0.407563119725743, 'Stanford Cars (360Ã—240)': 0.13583821249199218, 'FGVC Aircraft': 0.8229545014750042, 'Oxford Pets': 0.08995285864599148, 'Flowers102': 0.08972060770047119, 'EuroSAT': 1.0, 'UCF101': 0.01
% Shown using pie chart in the main paper 

% \begin{table}[!h]
% \centering
% \begin{tabular}{|l|c|}
% \hline
% \textbf{Dataset} & \textbf{Weight} \\ \hline
% Imagenet & 0.15556157429688613 \\ \hline
% ImageNet-A & 0.970498446080589 \\ \hline
% ImageNet-V2 & 0.2854574367981364 \\ \hline
% ImageNet-R & 0.01 \\ \hline
% ImageNet-Sketch & 0.021456095637452655 \\ \hline
% Caltech101 (300x200) & 0.01 \\ \hline
% DTD split-1 (300x300 - 640x640) & 0.505922498560715 \\ \hline
% Food101 (512x512) & 0.01 \\ \hline
% SUN397 & 0.407563119725743 \\ \hline
% Stanford Cars (360x240) & 0.13583821249199218 \\ \hline
% FGVC Aircraft & 0.8229545014750042 \\ \hline
% Oxford Pets & 0.08995285864599148 \\ \hline
% Flowers102 & 0.08972060770047119 \\ \hline
% EuroSAT & 1.0 \\ \hline
% UCF101 & 0.01 \\ \hline
% \end{tabular}
% \caption{Dataset Weights}
% \label{}
% \end{table}


\begin{table}[!h]
  % \caption{\textbf{Robustness analysis of CNN vs ViT-based backbones of CLIP model across 15 datasets for different severity labels using $\hat{\gamma}_{R,Agg.} (\uparrow)$.}}
  \caption{Optimized dataset weight values for WAR-16, shown using pie chart in \Cref{fig:datasets_weights} (right) in the main paper.}
  \label{tab:dataset_weight_value}
  \centering
  \setlength\tabcolsep{2pt}
  \begin{tabular}{l|c|c|c}
    \toprule
    Dataset & Weight & SAR-16 Correlation & WAR-16 Correlation\\ 
    \midrule
    Imagenet & 0.15556157429688613 & 0.99269 & 0.93295 \\ 
    \rowcolor{mygray} 
    ImageNet-A & 0.970498446080589 & 0.55646 & \textbf{0.68070} \\
    ImageNet-V2 & 0.2854574367981364 & 0.99165 & 0.93733 \\ 
    ImageNet-R & 0.01 & 0.98201 & 0.90682 \\
    ImageNet-Sketch & 0.021456095637452655 & 0.95086 & 0.87241 \\
    Caltech101 & 0.01 & 0.97695 & 0.90853 \\
    DTD split-1 & 0.505922498560715 & 0.87676 & 0.82507 \\
    Food101 & 0.01 & 0.97771 & 0.91575 \\
    SUN397 & 0.407563119725743 & 0.98760 & 0.94531 \\
    Stanford Cars & 0.13583821249199218 & 0.96639 & 0.91721 \\
    FGVC Aircraft & 0.8229545014750042 & 0.89746 & 0.89016 \\
    Oxford Pets & 0.08995285864599148 & 0.97224 & 0.90114 \\
    Flowers102 & 0.08972060770047119 & 0.97073 & 0.91809 \\
    \rowcolor{mygray} 
    EuroSAT & 1.0 & 0.25753 & \textbf{0.49229} \\
    UCF101 & 0.01 & 0.97324 & 0.93516 \\

    \hline
  \bottomrule
  \end{tabular}
\end{table}


\noindent \textbf{Super Resolution Method Preprocessing:} 
Here, we present preprocessing steps for two pipelines \emph{i.e.} (i) \textbf{Vanilla Pipeline:} raw image $\rightarrow$ create a low-resolution image using \texttt{transforms.Resize}\:($\cdot$)\:$\rightarrow$ upscale it to the model resolution using \texttt{transforms.Resize}\:($\cdot$)\:$\rightarrow$ input to the model; and (ii) \textbf{Super Resolution Pipeline:} raw image $\rightarrow$ create a low-resolution image using \texttt{transform\_test}\:($\cdot$)$\rightarrow$ pass through Super Resolution models $\rightarrow$ get the model resolution using \texttt{sr\_transform\_test}\:($\cdot$) $\rightarrow$ input to the model. The detailed implementation of these two pipelines is illustrated in the code below:

% how given a low resolution image as input who these two the preprocessing applying to (i) the vanilla model and (ii) for EVA model, but the 
\begin{lstlisting}[language=Python, caption=SR data preprocessing, label=code:transformation, basicstyle=\ttfamily\footnotesize, keywordstyle=\color{blue}, commentstyle=\color{green!60!black}, stringstyle=\color{red}]

# org_res is the original model resolution
# low_res is the low resolution 
# normalize (mean, std) is the normalization specific to the model

# Pipeline-1: Vanilla
transform_test = transforms.Compose([
    transforms.Resize(low_res,interpolation=InterpolationMode.BICUBIC),
    transforms.Resize(org_res,interpolation=InterpolationMode.BICUBIC),
    transforms.CenterCrop(size=(org_resolution, org_resolution)),
    _convert_image_to_rgb, # converts img to RBG using PIL
    transforms.ToTensor(),
    normalize,
])

EVA_INPUT = transform_test(RGB_IMG)
...
# Pipeline-2: SUPER RESOLUTION
# RAW Image --> SR model 
transform_test = transforms.Compose([
    transforms.Resize(low_res,interpolation=InterpolationMode.BICUBIC),
    transforms.CenterCrop(size=(low_res, low_res)),
    _convert_image_to_rgb,
    transforms.ToTensor(),
    normalize,
])

# SR Image --> EVA model 
mean = (0.48145466, 0.4578275, 0.40821073)
std = (0.26862954, 0.26130258, 0.27577711)
sr_transform_test = transforms.Compose([
    transforms.Resize(org_res,interpolation=InterpolationMode.BICUBIC),
    transforms.CenterCrop(size=(org_res, org_res)),
    _convert_image_to_rgb,
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
])

SR_INPUT = transform_test(RGB_IMG)
SR = SR_MODEL (SR_INPUT)
SR = transforms.functional.to_pil_image(normalize(SR), mode=None)
EVA_INPUT = sr_transform_test(SR)
....

\end{lstlisting}


\noindent \textbf{RobustSAM implementation for Classification:}
We use the official code\footnote{URL: \url{https://robustsam.github.io/}} to replace the mask token with the vision class token. 
Robust SAM is a segmentation model. We remove all its segmentation mask components and mask prediction step. The vision transformer encoder's last block is used instead of the decoder, and all the mask component is stripped away. 
Vanilla Transformer is treated as a teacher. In the student model, the class token is replaced with a \textit{learnable token}. 
This new learnable token is passed through each transformer block. After the first block, we treat this as ``early\_feature" as mentioned in the official github. 
Using RobustSAM denoising trainable modules, we generate `complementary\_features' of these early features. 
After the final block, we use the new learnable token to generate 
`final\_image\_embeddings' using the `self.fourier\_last\_layer\_features\:(image\_embeddings, clear=CLEAR)'.

`robust\_features = complementary\_features + final\_image\_embeddings'.

MSE makes noisy and clear class token and robust features similar. 


\noindent \textbf{VPT Implementation:}
VPT is the same as ours, instead of adding on top of spatial tokens, trainable 50 tokens are concatenated to frozen spatial tokens before the first block. 
The decline in the performance at higher resolution indicates the need for introducing tokens at every layer instead of just once at the start. 

Both methods follow the same training environment as our LR-TK0 (multi-training paradigm and diffusion-based images 7k * 30).







\section{More Results}


% Robustness >100 if acc > 224 (the case for 128 x 128)

\subsection{Dataset wise resolution vs. accuracy}
% In Figure~\ref{fig:dataset_wise_performance}, we showcase superior zero-shot low-resolution robustness of our proposed \textit{LR-Tk0} with vanilla EVA-B/16 model while keeping the same backbone across 15 datasets and different resolution \emph{i.e.} $128\times128$, $64\times64$, and $32\times32$. In the main paper, we already shown for $16\times16$ resolution in Figure 12. Since EVA performs far superior to random prediction we have considered to show dataset and resolution wise breakdown of $\Gamma^{D}_{16}\!\approx\!\gamma^{D}_{16}\!$, $\gamma^{D}_{16}$ which is being plotted in the figure.

In \textbf{\Cref{fig:dataset_wise_performance}}, we highlight the superior zero-shot low-resolution performance (\emph{i.e.} accuracy) of our proposed method, \textbf{LR-TK0}, compared to the vanilla \texttt{EVA-02-CLIP-B/16} model, while utilizing the same backbone across 15 datasets at varying resolutions: $32\times32$, $64\times64$, and $128\times128$. The main paper already demonstrates the results for the $16\times16$ resolution in \Cref{fig:performance_dataset}.

% Given EVA's significant outperformance over random predictions, we provide a detailed dataset-specific breakdown of the gamma robustness \emph{i.e.} $\Gamma^{D}_{n}\approx\gamma^{D}_{n}$, and $\gamma^{D}_{n}$, where $n$ is the resolution \emph{i.e.} 16, 32, 64, and 128 are being plotted in the Figure~\ref{fig:gamma_variation}.

Since EVA performs far superior to random prediction, we present a detailed dataset-specific breakdown of gamma robustness, denoted as $\Gamma^{D}_{n} \approx \gamma^{D}_{n}$ for our proposed method compared with the vanilla \texttt{EVA-02-CLIP-B/16} across resolutions $n = 16, 32, 64,$ and $128$. These results are detailed in \textbf{Figure~\ref{fig:gamma_variation}}. It should be noted that robustness is the absolute value and in Figure~\ref{fig:gamma_variation}, robustness exceeds 100 only when the model's accuracy at lower resolutions surpasses its accuracy at the original 224 resolution.

% \subsection{Numbers of Images Per Caption}


\begin{figure}[!t]
\centering
\subfloat[\centering Top-1 Accuracy $32\x32$]
{
\includegraphics[height=3cm]{Images/acc_eva_32.pdf}
}
\hfill
\subfloat[\centering Top-1 Accuracy $64\x64$]
{
\includegraphics[height=3cm]{Images/acc_eva_64.pdf}
}
\hfill
\subfloat[\centering Top-1 Accuracy $128\x128$]
{
\includegraphics[height=3cm]{Images/acc_eva_128.pdf}
}
\caption{\textbf{Vanilla vs LR-TK0 (Our)}: 
  Top-1 accuracy for \texttt{EVA-02-CLIP-B/16} model for different resolutions.
}    
\label{fig:dataset_wise_performance}
\end{figure}





\begin{figure}[!t]
\centering
\subfloat[\centering $\gamma^{D}_{16} (\approx\Gamma^{D}_{16}$) Robustness on $16\x16$]
{
\includegraphics[height=3cm]{Images/gamma_eva_16.pdf}
}
\hfill
\subfloat[\centering $\gamma^{D}_{32} (\approx\Gamma^{D}_{32}$) Robustness on $32\x32$]
{
\includegraphics[height=3cm]{Images/gamma_eva_32.pdf}
}
\hfill
\subfloat[\centering $\gamma^{D}_{64} (\approx\Gamma^{D}_{64}$) Robustness on $64\x64$]
{
\includegraphics[height=3cm]{Images/gamma_eva_64.pdf}
}
\hfill
\subfloat[\centering $\gamma^{D}_{128} (\approx\Gamma^{D}_{128}$) Robustness on $128\x128$]
{
\includegraphics[height=3cm]{Images/gamma_eva_128.pdf}
}
\caption{\textbf{Vanilla vs LR-TK0 (Our)}: 
  Gamma Robustness for \texttt{EVA-02-CLIP-B/16} model for different resolutions on each dataset.
}    
\label{fig:gamma_variation}
\end{figure}


\begin{table}[!t]
\centering
\caption{\textbf{Comparison with SR}: 
EVA-B/16 results, with different data preprocessing (for SR).
}
\label{tab:comparison_with_SR_supp}
\setlength\tabcolsep{2.2pt}
\renewcommand{\arraystretch}{1.2}
\scalebox{0.98}{
\centering
\begin{tabular}{
l|
c|c|c|    c|c|c|    c|c|c|   c|c|c
}
\specialrule{1pt}{0pt}{0pt}
\rowcolor{mygray} 
& \multicolumn{3}{c|}{$16\x16$}
& \multicolumn{3}{c|}{$32\x32$}
& \multicolumn{3}{c|}{$64\x64$}
& \multicolumn{3}{c}{$128\x128$}
\\[-.4pt] \hhline{~------------}
% \cline{5-16}
% \hhline{-}
\rowcolor{mygray} 
\multirow{-2}{*}{Method}
& SAR & WAR & Acc
& SAR & WAR & Acc
& SAR & WAR & Acc
& SAR & WAR & Acc
\\
\hline\hline
Baseline 
& 34.1 & 26.8 & 25.0 
& 71.8 & 59.0 & 51.2 
& \textbf{91.6} & 83.8 & \textbf{63.8 }
&\textbf{ 98.2} & \textbf{95.4} & \textbf{67.6} 
\\ 
\hline
BSRGAN~\SPCITE{zhang2021designing} 
& 12.4 & 12.2 & 8.8    
& 37.3 & 28.7 & 26.9 
& 70.1 & 58.0 & 49.4
& 88.9 & 77.2 & 61.9 
\\
ESRGAN~\SPCITE{wang2018esrgan}
& 14.2 & 15.1 & 10.0 
& 40.3 & 32.6 & 28.9 
& 74.4 & 61.8 & 52.4
& 90.8 & 79.7 & 63.2 
\\ 
Swinir~\SPCITE{liang2021swinir}
& 17.9 & 17.6 & 12.7 
& 47.7 & 38.3 & 34.3
& 79.2 & 68.9 & 55.6 
& 92.7 & 84.6 & 64.2
\\ 
AddSR~\SPCITE{xie2024addsr} 
& 20.5 & 16.8 & 15.0 
& 48.3  & 36.0 &  35.2 
& 73.5 & 57.5 & 52.3 
& 83.6 & 69.4 & 58.7 
\\ 
\hline
\rowcolor{mygray} 
Our 
& \textbf{38.9} & \textbf{29.5} & \textbf{28.4} 
& \textbf{73.1} & \textbf{62.0} & \textbf{52.0}
& 91.4  &  \textbf{85.5 }& 63.6 
&  97.6 & 95.2 & 67.3  
\\
% IDM~\SPCITE{gao2023implicit} \\ 
% Inf-DiT~\SPCITE{yang2024inf} \\ 
\specialrule{1pt}{0pt}{0pt}
\end{tabular}
}
\end{table}


















\subsection{ALL SR Results}
% In Table~\ref{tab:comparison_with_SR_supp}, we have compared our proposed \textbf{LR.Tk0} method with baseline method and existing state-of-the-art super resolution methods including BSRGAN~\cite{}, ESRGAN~\cite{}, Swinir~\cite{}, and AddSR~\cite{}. Notably, we have adopted super-resolution methods in a zero-shot manner to make a fair comparison. Our methods outperformed existing super-resolution methods with a huge margin across all the resolutions and outperformed baseline with a significant margin at resolution $16\time16$, $32\time32$, and have equivalent robustness for resolution $64\times64$ and $128\times128$.

In \Cref{tab:comparison_with_SR_supp}, we present a comparison of our proposed \textbf{LR-TK0} method against the baseline and several state-of-the-art super-resolution methods, including BSRGAN~\cite{zhang2021designing}, ESRGAN~\cite{wang2018esrgan}, SwinIR~\cite{liang2021swinir}, and AddSR~\cite{xie2024addsr}. All super-resolution methods were employed in a zero-shot setting to ensure a fair comparison. Our method significantly outperformed these super-resolution techniques across all resolutions and demonstrated a substantial improvement over the baseline at resolutions of $16\times16$ and $32\times32$. Furthermore, it exhibited comparable robustness at resolutions of $64\times64$ and $128\times128$ with the baseline method.





\subsection{SR results for IDM and Inf-DiT}
\textbf{\Cref{tab:idm_inf_dit}}: 
IDM generalized Zero shot weights do not match their GitHub implementation. Hence we use their weight for cat datasets. 
We evaluate IDM on the pets dataset which is the closest to its pretrained weights. 
For uniformity, we compare Inf-DiT on the pets dataset as well. Both diffusion-based models take around ~4-5 mins per batch of 10 images, making large-scale dataset evaluation impossible. 

\subsection{Grad CAM results}
\textbf{\Cref{fig:gram_cam_results_supp}}, an extension of  \Cref{fig:gram_cam_results} in the main paper, presents the Grad CAM visualization of the vanilla model and proposed method, showcasing the effect of proposed LR tokens.



\begin{table}[!t]
\centering
\renewcommand{\arraystretch}{1.2}
\setlength\tabcolsep{6pt}
\centering
\captionof{table}{
IDM \& Inf-DiT performance on Pets dataset. 
}
\label{tab:idm_inf_dit}  
\scalebox{1}{
\centering
\begin{tabular}{c|c| c| c| c }
\specialrule{1pt}{0pt}{0pt}
\rowcolor{mygray} 
Method &  Top -1 $16\x16$ & Top -5 $16\x16$ & Top-1 $32\x32$ & Top-5 $32\x32$\\
\hline\hline
Eva-B/16 & 51.840	& 84.710 & 82.530 & 98.530 \\ 
Eva-B/16 + LR-Tk0 & 57.92	 & 88.66 & 83.07 & 98.36\\
IDM + Eva-B/16 & 7.2 & 29.03 & 7.88 & 30.25\\ 
Inf-DiT + Eva-B/16 & 29 & 60.94 & 73.43 & 94.36\\ 
\specialrule{1pt}{0pt}{0pt}
\end{tabular}
}
\end{table}







\section{Ablations}

\noindent \textbf{Number of Images Per Caption:} In the main paper, \Cref{fig:img_per_caption} presents the number of generated (by diffusion model) images (captions) with SAR-16 metric to emphasize how it helps to improve the model robustness. Here, in \textbf{\Cref{fig:ldm_gsize_per}}, we extend this by including ACC-16 and WAR-16 evaluation metrics, while varying the number of generated images.

\begin{figure}[!t]
\centering
\subfloat{
\includegraphics[height=4.5cm]{Images/imgs_captions_ACC.pdf}
}
\hfill
% \subfloat
% {
% \includegraphics[height=3.4cm]{Images/imgs_captions_SAR.pdf}
% }
\subfloat
{
\includegraphics[height=4.5cm]{Images/imgs_captions_WAR.pdf}
}
\caption{ 
\textbf{Images/ Caption }: For ACC, and WAR, evaluation metrics on $16\x16$. SAR in the main paper.
}
\label{fig:ldm_gsize_per}
\end{figure}










\noindent \textbf{Hyperparameter $\mathbf{\alpha}$} signifies the rate of robustness declines as accuracy approaches random prediction. In \textbf{\Cref{fig:alpha_hyperparam_supp}}, we varied the $\alpha$ value with robustness and considered $\alpha=200$ for our experiments as shown in \Cref{fig:hyperparam_war} (left) of the main paper.

\begin{figure}[!t]
\centering
\subfloat{
\includegraphics[height=2.8cm]{Images/supp_alpha_10.pdf}}
\subfloat{
\includegraphics[height=2.8cm]{Images/supp_alpha_100.pdf}}
\subfloat{
\includegraphics[height=2.8cm]{Images/supp_alpha_500.pdf}}
\subfloat{
\includegraphics[height=2.8cm]{Images/supp_alpha_1000.pdf}}
\caption{ 
\textbf{Rate of robustness declines as accuracy approaches random prediction.
}
}
\label{fig:alpha_hyperparam_supp}
\end{figure}



\noindent \textbf{LR token position:} In the main paper, \Cref{fig:pos_lr_tokens} shows the performance (in terms of SAR-16, 16 is for resolution) with respect to the position of LR tokens being introduced in the form of a line chart. Here, in \textbf{\Cref{tab:pos_lr_tokens}}, we detailed the corresponding numerical values of \Cref{fig:pos_lr_tokens} for better clarity.
Furthermore, we present a side-by-side comparison between the LR token introduction for WAR-16 and SAR-16 metrics in \textbf{\Cref{fig:pos_lrtk_supp}}.
% Main paper shows a line chart for this table





\begin{table}[!t]
\centering
\renewcommand{\arraystretch}{1.2}
\setlength\tabcolsep{2pt}
\centering
\captionof{table}{\textbf{LR token Position (Pos):} 
$[i]$ means LR tokens after $i^{th}$ block (and no token after patchification). 
}
\label{tab:pos_lr_tokens}  
\scalebox{1.0}{
\centering
\begin{tabular}{c|c| c| c| c }
\specialrule{1pt}{0pt}{0pt}
\rowcolor{mygray} 
Pos. &  SAR-16 & WAR-16 & SAR-32 & WAR-32 \\
\hline\hline
$[0]$ & \textbf{42.4} & \textbf{35.4} & 75.3 & 66.4 \\ 
$[5]$ & 41.4 & 35.3 & \textbf{75.4} & \textbf{67.0} \\
$[8]$ & 39.6 & 33.3 & 74.8 & 65.5 \\ 
$[11]$ & 38.4 & 31.3 & 74.5 &  64.6 \\ 
\specialrule{1pt}{0pt}{0pt}
\end{tabular}
}
\end{table}
\begin{figure}[!t]
\centering
\subfloat{
\includegraphics[height=4.5cm]{Images/position_lr_token-SAR.pdf}
}
\hfill
\subfloat
{
\includegraphics[height=4.5cm]{Images/position_lr_token-WAR.pdf}
}
\caption{ 
\textbf{Position of LR tokens introduction}: No tokens were added after the position embedding stage. [i]-th indicates the block from which LR tokens were introduced. Performance metrics variants of \Cref{fig:pos_lr_tokens}.
}
\label{fig:pos_lrtk_supp}
\end{figure}





\begin{figure}[!t]
\centering
\subfloat{
\includegraphics[height=4.5cm]{Images/grad_cam1.pdf}
}
\hfill
\subfloat
{
\includegraphics[height=4.5cm]{Images/grad_cam2.pdf}
}
\caption{ 
\textbf{Effect of LR token}: `@' is input resolution. Vanilla model attention is scattered at $16\x16$ (compared to $224\x224$), while our LR tokens focus on the object, capturing fine-grained details.
}
\label{fig:gram_cam_results_supp}
 \vspace{-8pt}
\end{figure}


\noindent \textbf{Spearman correlation for other resolutions:}
In \textbf{\Cref{fig:Sr_images_supp_16}}, weights derived for $16\x16$ are used for other models. Weights for $16\x16$ hold for $32\x32$ but degrade for $64\x64$ and $128\x128$ becoming identical to SAR.
\textbf{\Cref{fig:diff_optimization_configs}} shows different configurations for obtaining dataset weights. 


\begin{figure}[!t]
\centering
\subfloat[\centering Spearman correlation for $32\x32$]
{
\includegraphics[height=5.5cm]{Images/sar_war_32.pdf}
}
\hfill
\subfloat[\centering Spearman correlation for $64\x64$]
{
\includegraphics[height=5.5cm]{Images/sar_war_64.pdf}
}
\hfill
\subfloat[\centering Spearman correlation for $128\x128$]
{
\includegraphics[height=5.5cm]{Images/sar_war_128.pdf}
}
\caption{\textbf{Spearman Correlation} for weights derived for $16\x16$ for higher resolutions.
}    
\label{fig:Sr_images_supp_16}
\end{figure}


\begin{figure}[!t]
\centering
\subfloat[\centering All dataset weights as 1]
{
\includegraphics[height=5.5cm]{Images/Spearman-SAR-WAR-Radar-Impro-16_6.png}
}
\subfloat[\centering Imagenet: 1, ImageNet-A:1, EuroSAT:1]
{\includegraphics[height=5.5cm]{Images/Spearman-SAR-WAR-Radar-Impro-16_2.png}
}\\
\subfloat[\centering Imagenet: 1, EuroSAT:1]
{\includegraphics[height=5.5cm]{Images/Spearman-SAR-WAR-Radar-Impro-16_7.png}
}
\subfloat[\centering Imagenet: 0.95, EuroSAT:1]
{\includegraphics[height=5.5cm]{Images/Spearman-SAR-WAR-Radar-Impro-16_3.png}
}
\caption{\textbf{Spearman Correlation} for different optimization function.
The optimization objective is to maximize the mentioned dataset Spearman correlation (SC). 
For Example `Imagenet: 0.95, EuroSAT:1' means: SC (Imagenet) $\x$ 0.95 + SC (EuroSAT) $\x$ 1. 
}    
\label{fig:diff_optimization_configs}
\end{figure}





\noindent \textbf{Samples of Diffusion Generated Images:}
In \textbf{\Cref{fig:ldm_images_1}} and \textbf{\Cref{fig:ldm_images_2}}, we showcase a few sample images generated using PIXART-$\alpha$. These plots are an extension of \Cref{fig:Sr_images} presented in the main paper.

\begin{figure}[!t]
\centering
\subfloat{
\includegraphics[height=4.5cm]{Images/gen_image1.pdf}
}
\hfill
\subfloat
{
\includegraphics[height=4.5cm]{Images/gen_image2.pdf}
}
\caption{ 
\textbf{Synthetic Images}: Images generated using PIXART-$\alpha$~\citep{chen2023pixartalpha} using the captions randomly sampled from Conceptual Captions~\citep{sharma2018conceptual}.
\textit{Left}: Sample Images, while \textit{right} shows multiple images per caption generated via different seeds.
More examples of ~\Cref{fig:Sr_images} (in main paper).
}
\label{fig:ldm_images_1}
\vspace{-2pt}
\end{figure}


\section{More Observations}

\noindent \textbf{Semantically correct mispredictions:}
As described in \Cref{fig:img_ex} of the main paper, misclassified low-resolution images are still assigned reasonable semantic predictions. Here in \textbf{\Cref{fig:semantically_resonable_prediction}}, we showcase more such examples where the above phenomenon holds.

\noindent \textbf{Real World low-resolution images:} We have taken a few real-world low-resolution sample images from Google as shown in \textbf{\Cref{fig:semantically_resonable_prediction2}} to see the considered model's performance. Here, we have considered the top-5 predictions of the model and see which indicates (i) correct predictions, (ii) semantically reasonable predictions, and (iii) wrong predictions. The ground labels (or templates) for considered images are chosen from Imagenet.


\begin{figure}[!t]
\centering
\subfloat{
\includegraphics[height=5.5cm]{Images/gen_images6.pdf}
}
\hfill
\subfloat
{
\includegraphics[height=5.5cm]{Images/gen_images7.pdf}
}
\caption{ 
\textbf{Synthetic Images2}: Mutliple Images / Caption. More examples of ~\Cref{fig:Sr_images} (in the main paper)
}
\label{fig:ldm_images_2}
\vspace{-2pt}
\end{figure}









\section{Limitation}
\iffalse
\begin{enumerate}
    \item Doesn't work for CLIP A deeper investigation
    \item Like SR, resolution-dependent 
    \item Investigate where to insert tokens future todo patchification may be hiring 
    \item very naive solution proposed.... modified todo in future 
    \item Investigate what differentiates datasets-based performance. 
\end{enumerate}
\fi

% \YSR{we can say, analysing pretrained datasets can provide more insights into models performance... due dataset scale and also unavailablity, it will be challenging, but with available resources and source datasets, it might be a good thing to do...}

A key limitation of our study is the lack of detailed analysis of the pre-training datasets, which could provide deeper insights into model performance, particularly regarding how dataset quality impacts robustness. However, due to the scale and unavailability of certain datasets, conducting such an analysis is challenging, though it remains a promising direction for future work with available resources and accessible datasets.













\begin{figure}[!t]
\centering
\subfloat{
\includegraphics[width=0.95\linewidth]{Images/correct_misprediction.pdf}
}
\caption{ 
\textbf{Semantically Correct Predictions}: More examples of \Cref{fig:img_ex} in the main paper. Visually different examples were chosen to show the usefulness of the pre-trained weights even in low resolution. 
}
\label{fig:semantically_resonable_prediction}
\end{figure}


\begin{figure}[!t]
\centering
\subfloat{
\includegraphics[width=0.95\linewidth]{Images/real_wod.pdf}
}
\caption{ 
\textbf{Real World low-resolution images}: Top-5 predictions for images taken from Google (true label shown, below image). \textbf{Blue} indicates Semantically reasonable prediction, \textbf{Green} indicates correct prediction, and \textbf{Red} means wrong prediction. \texttt{EVA-02-CLIP-B/16} model predictions for unknown resolution (real-world footage). Labels/templates are chosen from the \textbf{ImageNet dataset}.
}
\label{fig:semantically_resonable_prediction2}
\end{figure}
