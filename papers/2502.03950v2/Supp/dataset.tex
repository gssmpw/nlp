\begin{center}
    \Large{\textbf{LR0.FM: Low-Resolution Zero-Shot Classification\\ benchmark for Foundation Models\\
(Appendix)}}
\end{center}

\section{Dataset Description}
This paper presents a comprehensive benchmarking of zero-shot image classification on low-resolution images utilizing 15 diverse datasets, each representing prominent computer vision challenges as depicted in \Cref{tab:dataset}. Among them, ImageNet~\cite{deng2009imagenet} stands out as a significant repository, containing 50,000 (in test-set) labeled images and serving as a standard for evaluating image classification models. 
Caltech101~\cite{griffin2007caltech}, with its 6,085 test-set images spanning 101 object categories, is widely used for object recognition tasks.
The Describable Textures Dataset (DTD)~\cite{cimpoi2014describing}, comprising over 1,880 texture images in the test-set, facilitates texture analysis.
Food101 provides 25,250 test-set images across 101 food categories, supporting food recognition tasks. SUN397â€™s~\cite{zhou2014learning} 19,850 annotated test-set images aid scene recognition in understanding diverse environments. Stanford Cars~\cite{kramberger2020lsun} and FGVC Aircraft~\cite{maji2013fine} datasets focus on fine-grained classification tasks for vehicles and aircraft, respectively. Oxford Pets~\cite{parkhi2012cats} offers a dataset for pet breed classification, while Flower102~\cite{liu2016flower} is dedicated to flower species recognition. Eurosat~\cite{helber2019eurosat} specializes in land use and cover classification using satellite imagery. UCF101~\cite{soomro2012ucf101}, containing over 1,794 video clips (in test-set), is pivotal for action recognition research, offering a diverse range of action sequences.  Moreover, we explore four ImageNet variants for natural distribution shifts, previously considered as out-of-distribution (OOD) data for ImageNet~\cite{radford2021learning, shu2022test}. ImageNet-V2~\cite{recht2019imagenet} provides an independent test set with 10,000 natural images collected from different sources across 1,000 ImageNet categories, while ImageNet-A~\cite{hendrycks2021nae} contains 7,500 challenging ``natural adversarial examples'' from 200 ImageNet categories misclassified by a standard ResNet-50~\cite{he2016deep}. Lastly, ImageNet-R~\cite{hendrycks2021many} adds further diversity by offering 30,000 artistic renditions across 200 ImageNet categories, and ImageNet-Sketch~\cite{wang2019learning} includes 50,000 black-and-white sketches covering 1,000 categories, collected independently from the original ImageNet validation set. The test dataset size, the number of classes, and dataset focus are further elaborated in \Cref{tab:dataset}.

\begin{table}[!h]
\begin{center}
  \caption{\textbf{Statistics of benchmark datasets for zero-shot image recognition.}}
  \label{tab:dataset}
  % \setlength\tabcolsep{2pt}
  \begin{tabular}{@{}p{4.2cm}@{}|
@{}P{1.4cm}@{}|P{2cm}@{}P{2cm}@{}P{4cm}}
    \toprule
    Dataset & Year & Test Size & \# classes & Focus  \\  
    \midrule
    ImageNet-A (\citeyear{hendrycks2021nae}) & 2021 & 7500 & 200 & Generic\\ 
     ImageNet-V2 (\citeyear{recht2019imagenet}) & 2019 & 10,000 & 1000 & Generic  \\ 
     ImageNet (\citeyear{deng2009imagenet}) & 2009 & 50,000 & 1000  & Generic \\
     Caltech101 (\citeyear{griffin2007caltech}) & 2004 & 6,085 & 101 & Generic \\
     \hline 
     ImageNet-Sketch (\citeyear{wang2019learning}) & 2019 & 50,000 & 1000 & Edges \\ 
     ImageNet-R (\citeyear{hendrycks2021many}) & 2021 & 30,000 & 200 & Texture  \\ 
    EuroSAT (\citeyear{helber2019eurosat}) & 2019 & 5,000 & 10 & Texture \\ 
     DTD (\citeyear{cimpoi2014describing}) & 2014 & 1,880 & 47 & Edges, Texture \\
     \hline 
     Food101 (\citeyear{bossard2014food}) & 2014 & 25,250 & 101 & Fine-grained \\  
     Stanford Cars (\citeyear{kramberger2020lsun}) & 2013 & 8,041 & 196 & Fine-grained\\
     FGVC-Aircraft (\citeyear{maji2013fine}) & 2013 & 3,333 & 100 & Fine-grained \\ 
     Oxford Pets (\citeyear{parkhi2012cats}) & 2012 & 3,669 & 37 & Fine-grained \\ 
     Oxford Flowers102 (\citeyear{liu2016flower}) & 2008 & 6149 & 102 & Fine-grained  \\ 
    \hline 
     SUN397 (\citeyear{zhou2014learning}) & 2010 & 19,850 & 397 & Scene understanding\\
     UCF101 (\citeyear{soomro2012ucf101}) & 2012 & 1,794 & 101 & Scene understanding\\
  \bottomrule
  \end{tabular}
  \end{center}
\end{table}


% template
\textbf{\Cref{tab:dataset_template}: Dataset templates:} As the main paper outlines, we adopt CLIP~\cite{radford2021learning} evaluation protocol for all models to ensure a fair comparison of low-resolution robustness. To generate the text embedding for a given image, we utilize dataset-specific templates, such as ``a photo of a [label]'', ``a low-resolution photo of a [label]'', \emph{etc} as detailed in \Cref{tab:dataset_template}. For each class label, we generate multiple text embeddings by inserting the label into $n$ prompt templates and then average these $n$ embeddings. For instance, consider an image of a cat from the Imagenet dataset. With 1000 class labels and 80 prompt templates, we insert the label ``cat'' into the templates, generate 80 corresponding text embeddings, and compute their average to represent the cat class in text space. This process yields 1000 text embeddings, one for each class. The dot product between the image embedding and these 1000 text embeddings produces class logits, where the highest logit score determines the predicted class. In \Cref{tab:dataset_template}, we present data-specific prompt template samples along with the total number of such prompts.


\begin{table}[!t]
\begin{center}
  \caption{\textbf{Benchmark Datasets Templates} Zero-shot image classification. Here \textbf{[L]} is the class name (labels). These templates are taken from CLIP~\citep{radford2021learning} and OPENCLIP~\citep{ilharco_gabriel_2021_5143773}
  }
  \label{tab:dataset_template}
  \begin{tabular}{@{}l@{ }|@{ }l@{ }|@{}P{1.2cm}@{}}
    \toprule
    Dataset & Sample prompt template & \# 
 Prompts \\  
    \midrule
     ImageNet & a low resolution photo of a \textbf{[L]}, a photo of a small \textbf{[L]}, art of a \textbf{[L]}, \etc & 80\\
     ImageNet-SK & a sketch of the \textbf{[L]}, a rendering of a \textbf{[L]}, a drawing of a \textbf{[L]}, \etc & 80\\ 
     ImageNet-A & a sculpture of a \textbf{[L]}, a close-up photo of the \textbf{[L]}, the cartoon \textbf{[L]} \etc  & 80\\ 
     ImageNet-V2 & a black and white photo of a \textbf{[L]}, a \textbf{[L]} in a video game, a toy \textbf{[L]}, \etc 
 & 80\\ 
     ImageNet-R  & a cropped photo of the \textbf{[L]}, a blurry photo of the \textbf{[L]}, graffiti of a \textbf{[L]}, \etc  & 80\\ 
     
     Caltech101  & a photo of a \textbf{[L]}, a painting of a \textbf{[L]}, the origami \textbf{[L]}, the toy \textbf{[L]}, \etc & 34\\
     DTD & a photo of a \textbf{[L]} texture, a photo of a \textbf{[L]} pattern, \etc & 8\\
     Food101 & a photo of \textbf{[L]}, a type of food & 1\\  
     SUN397 & a photo of a \textbf{[L]}, a photo of the \textbf{[L]} & 2\\
     Cars & a photo of a \textbf{[L]}, a photo of my new \textbf{[L]}, a photo of my dirty \textbf{[L]}, \etc & 8\\
     Aircraft & a photo of a \textbf{[L]}, a type of aircraft \& a photo of the \textbf{[L]}, a type of aircraft & 2\\ 
     Pets & a photo of a \textbf{[L]}, a type of pet & 1\\ 
     Flowers102 & a photo of a \textbf{[L]}, a type of flower & 1\\ 
     EuroSAT & a centered satellite photo of the \textbf{[L]}, a centered satellite photo of a \textbf{[L]}, \etc & 3\\ 
     UCF101 & a video of a person doing \textbf{[L]}, a example of a person practicing \textbf{[L]}, \etc & 48\\
  \bottomrule
  \end{tabular}
  \end{center}
\end{table}





