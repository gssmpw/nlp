

\begin{figure}[!t]
\centering
\subfloat{\includegraphics[height=3.5cm]{Images/SR1.pdf}}
\hfill
\subfloat{\includegraphics[height=3.5cm]{Images/SR2.pdf}}
\caption{
\textbf{Super resolution at {\boldsymbol{$16\x16$}}}: Image from Pets (\textit{left}) and Food102 (\textit{right}). Models include AddSR~\SPCITE{xie2024addsr}, BSRGAN~\SPCITE{zhang2021designing}, ESRGAN~\SPCITE{wang2018esrgan}, IDM~\SPCITE{gao2023implicit}, Inf-DiT~\SPCITE{yang2024inf}, and Swinir~\SPCITE{liang2021swinir}.
}
\label{fig:sr_images}
\vspace{-6pt}
\end{figure}

\section{Proposed Method: LR-TK0}
\vspace{-4pt}
\Cref{fig:img_ex} reveals two key insights: i) LR lacks fine-grained details ii) FM(s) make semantically reasonable predictions even at $16\x16$, 
highlighting the importance of preserving semantic capabilities (pre-training).
While super-resolution (SR) methods could restore lost details without affecting models, zero-shot SR for very low resolutions $(\le64\x64)$, doesn't work well in practice, as shown in \cref{fig:sr_images}, where SR models fail to reconstruct out-of-domain images at $16\x16$.
To enhance model robustness against low resolution, our solution \textbf{LR-TK0} adds 
trainable LR tokens on top of frozen transformers (preserving the pre-trained weights).
These LR tokens learn to bridge the gap between the high-resolution (HR) and low-resolution (LR) domains, via self-supervised distillation (\cref{sec:model_training}).
We train these tokens on synthetically generated diffusion-based images (\Cref{sec:training_data}) in a task-agnostic setting, ensuring the model is not exposed to any of the 15 target datasets.
\vspace{-5pt}






\subsection{LR tokens}
\vspace{-6pt}
\label{sec:model_training}
To preserve the zero-shot capabilities of the model; \textit{pre-trained weights of the model are frozen}.
Instead, additional trainable tokens, referred to as ``LR Tokens", are added on top of the spatial tokens after RGB to patch tokens conversion (patchification) and before each transformer block. As shown in \cref{fig:model} (left) \# LR tokens = \# Spatial tokens $\x$ (N+1) blocks. 
These tokens aim to compensate for the loss of details in low resolution, thereby enhancing the model’s interpretability of LR images.
Contrary to prompt learning~\citep{jia2022visual}, where task-specific tokens are concatenated to the spatial tokens, ours are added/merged.  
\Cref{fig:feat_heatmap} (right)  indicates LR feature at the initial layer deviates more than the later ones, thus LR tokens are added at every block.  
% as supported by \cref{fig:feat_heatmap} (right), where the model’s $16\x16$ features deviate from the $224\x224$ features more in the initial layers than in the final layer.



\begin{figure}[!t]
\centering
\subfloat
{\includegraphics[height=3.8cm]{Images/model4.pdf}}
\hfill
\subfloat
{
\includegraphics[height=3.8cm]{Images/pseudo_distillation2.pdf}
}
\caption{
Fire (\& ice) icons represent trainable  (\& frozen) parameters.
\textit{Left:} \textbf{LR tokens} are \underline{added} to the frozen spatial patches (white) after patch generation, before each frozen transformer block, and class token as a final feature. 
\textit{Right:} \textbf{LR-TK0:}
Multi-scale training (only 1 shown for simplicity). 
Teacher (w/o LR tokens) generates $f^T_{HR}$  (HR), Student (w/ LR tokens) generates both $f^S_{HR},f^S_{LR}$.
}
\label{fig:pseudo_distillation}
\label{fig:model}
 % \vspace{-3pt}
\end{figure}

\noindent \textbf{LR-TK0 Technique}:
We adopt the multi-scale paradigm~\citep{chen2019learning} \ie training multiple low resolutions per HR image, given its success in the LR domain.
Model without LR tokens (frozen pre-trained weights) acts as a teacher generating feature representations for HR images, as true embedding $f^\text{\textbf{T}}_{HR}$. 
In contrast, LR tokens (\& pre-trained model) act as student, generating embeddings for both HR ($f^\text{\textbf{S}}_{HR}$) and LR image(s) ($f^\text{\textbf{S}}_{LR}$) as shown in  \cref{fig:pseudo_distillation} (right).
$f^\text{\textbf{S}}_{HR},f^\text{\textbf{S}}_{LR}(s)$ are matched with $f^\text{\textbf{T}}_{HR}$ using a contrastive loss~\citep{radford2021learning}, similar to text and image alignment. 
Anchoring HR-LR features around frozen teacher avoids direct matching of HR-LR embeddings, preventing pulling the HR features towards LR ones (converging into one)~\citep{9137263}. 
This also ensures features w/ and w/o spatial tokens remain similar (regularization). 
Feature matching doesn't require any labels for these synthetic images, aka \textbf{unsupervised}.
It also \textbf{task agnostic}, \ie doesn't involve any model task-related characteristics (classification in this case). 
\vspace{-15pt}
  

\begin{figure}[!t]
\centering
\subfloat{
\includegraphics[height=2cm]{Images/gen_image4.pdf}
\vspace{-3pt}
}
\hfill
\subfloat
{
\includegraphics[height=2cm]{Images/gen_image5.pdf}
\vspace{-3pt}
}
\caption{ 
\textbf{Synthetic Images}: (\textit{Left}) Images generated using PIXART-$\alpha$~\SPCITE{chen2023pixartalpha} using randomly sampled captions from Conceptual Captions ~\SPCITE{sharma2018conceptual}.
(\textit{Right}) Multiple images per caption.
}
\label{fig:Sr_images}
\vspace{-5pt}
\end{figure}


\subsection{Synthetic HR Dataset}
% \vspace{-8pt}
\label{sec:training_data}
We use the diffusion model PIXART-$\alpha$~\citep{chen2023pixartalpha} to generate synthetic HR images, via 7,000 randomly sampled captions from Conceptual Captions~\citep{sharma2018conceptual}. 
We expand our training set by creating multiple images (variations, human observation) per caption as shown in \cref{fig:Sr_images}. 
Conceptual Captions are commonly used in pretraining many zero-shot models (\cref{tab:model_desc}), and using synthetic diffusion-based images helps LR tokens capture a wide range of domains, ensuring generalized training.
\textbf{Random captions avoid targeting any specific dataset}. 
To our knowledge, our work is the first to train a model on synthetic diffusion images for zero-shot evaluation, contrary to training on a subset of target datasets~\citep{chen2024robustsam}.
% We believe these synthetic images should generalize well to other low-resolution tasks as well. 
Following the multi-scale paradigm, we downsample HR images to a randomly sampled spatial resolution (height = width) from three LR resolution buckets {[$16$,$32$]}, {[$32$, $64$]}, {[$64$, $128$]}, forming  HR-LR image pairs. \vspace{4pt} \\ 
\noindent \textbf{Zero-Shot:}
If \underline{7,000 (or fewer)} concepts/captions can consistently enhance model performance across \underline{15 datasets}, it suggests that the model is likely learning the relationship between HR and LR features rather than exploiting shortcuts. 
This is supported by greater improvements at LR ($16\x16$) compared to HR ($128\x128$). 
If the model somehow cheats the zero-shot evaluation using diffusion-generated images, we would expect similar or better performance improvements at HRs.
\vspace{-5pt}




\section{Proposed Method: Experimentation \& Ablation}
\vspace{-4pt}
\noindent
\textbf{Implementation Details}
\label{sec:implementation}
Models are trained with 7K captions (\& 30 images/captions) in a multi-scale paradigm. 
EVA is trained for 200 epochs, while MetaCLIP and OpenCLIP are for 10 epochs. 
% for choosing the best weights.
% via three low-resolution images, resolutions randomly sampled from [16,32], [32,64], and [64,128] respectively. 
Evaluation metrics (\cref{sec:eval_metrics}): \underline{SAR} (simple averaging of $\gamma^{D}_{n}$), \underline{WAR} (weighted averaging of $\Gamma^{D}_{n}$), and \underline{Acc} (average top-1). Higher number means better performance. 
Vanilla model's HR accuracy computes the accuracy gap $\mathcal{E}_D$, and dataset weights derived for $16\x16$ used for all resolutions (more in \Supp). 
\textbf{`EVA-02-CLIP-B/16' (EVA-B/16)}, is used for all our model-level analysis. 
\vspace{-8pt}


\subsection{Results}
\label{sec:results}
\textbf{\Cref{tab:proposed_main_results}} shows our LR tokens consistently enhance robustness at low resolutions ($16\x16$ \& $32\x32$), particularly for MetaCLIP. 
While the low resolution is often seen as a domain shift problem~\citep{9098036},
leading to potential declines in HR performance, our multi-scale training and HR teacher distillation minimize accuracy drops at higher resolutions (1-2\% accuracy drop).
% with LR-TK0 at $64\x64$ and $128\x128$.
Also, LR tokens have a minimal parameter gain ($+3\%$).
\textbf{\Cref{fig:performance_dataset}} shows Top-1 accuracy for EVA-B/16 with and without our LR-TK0, at $16\x16$, with max improvement on Flower-102 (6.2\%).
% Improvement $\ge\!4\%\!$ is seen on ImageNet, Pets, ImageNet-V2, Sun397, EuroSAT, and Flower102. 
\textbf{\Cref{tab:comparison_with_SR}} compares EVA-B/16 with super-resolution (SR) methods, with SR methods performing poorly in zero-shot settings for very low resolutions (\cref{fig:sr_images}).
In contrast, our approach is better suited for zero-shot scenarios. 
Diffusion-based SR method IDM is too computationally expensive to evaluate on large datasets like ImageNet (results in \Supp).
\textbf{\Cref{tab:vpt_robust_sam}} applies our LR-TK0 technique to visual prompt tuning which concatenates tokens (instead of adding) only before the first block.
RobustSAM (segmentation models) modified for image classification (\Supp).
\vspace{-2pt}
% both dropping the performance after $16\x16$.


\begin{table}[!t]
\caption{\textbf{LR-TK0 improvement on Foundation models:} `Meta-B/16': MetaCLIP-ViT-B/16 (2.5B), 
 `OC-B/16': OpenCLIP-ViT-B/16. 
 Higher number $\propto$ better performance. 
}
\vspace{-3pt}
\label{tab:proposed_main_results}
\renewcommand{\arraystretch}{1.2}
\setlength\tabcolsep{2.5pt}
\scalebox{0.83}{
\centering
\begin{tabular}{
c|c|
c|c|c||    c|c|c||    c|c|c||   c|c|c|| c|c|c
}
\specialrule{1pt}{0pt}{0pt}
\rowcolor{mygray} 
 &  \# 
& \multicolumn{3}{c||}{$16\x16$}
& \multicolumn{3}{c||}{$32\x32$}
& \multicolumn{3}{c||}{$64\x64$}
& \multicolumn{3}{c||}{$128\x128$}
& \multicolumn{3}{c}{$224\x224$}
\\
[-.4pt]\hhline{~~ --- --- --- --- ---}
\rowcolor{mygray} 
\multirow{-2}{*}{Model} & Param 
& SAR & WAR & Acc
& SAR & WAR & Acc
& SAR & WAR & Acc
& SAR & WAR & Acc
& SAR & WAR & Acc
\\
\hline\hline
EVA-B/16 & 149.7M & 
38.0 & 30.7 & 28.1 
& 74.4 & 64.8 & 53.5 
& 92.4 & 85.8 & 65.2 
& 98.4 & 96.1 & 68.8
& 100 & 100 & 69.6 \\  
\rowcolor{mygray} 
\textbf{+LR-TK0} & 155.2M  & 
42.4 & 35.4 & 31.3 
& 75.3 & 66.4 & 54.1 
& 91.8 & 85.9 & 64.8 
& 97.8& 95.5 & 68.3 
& 99.1 & 98.7 & 69.0 
\\ 
\hline \hline
Meta-B/16  & 149.6M  & 
32.1 & 27.2 & 23.4 & 
65.3 & 54.4 & 47.0  
& 89.5 & 83.6 & 62.9 
& 98.5 & 96.7 & 68.5 
& 100 & 100.0 & 69.4 
\\
\rowcolor{mygray} 
\textbf{+LR-TK0} & 151.6M
& 41.9 & 38.9 &  30.2 
& 71.7 & 66.0 & 51.0
& 89.3 & 85.4 & 62.6 
& 96.7 & 95.4 & 67.3
& 97.6 & 97.4 & 67.9
\\
\hline \hline 
OC-B/16 & 149.6M
& 33.4 & 26.5 & 24.8 
& 68.6 & 59.5 & 49.8  
& 89.2 & 84.1 & 63.6 
& 96.8 & 94.8 & 68.3 
& 100 & 100 & 70.4
\\
\rowcolor{mygray} 
\textbf{+LR-TK0} & 151.6M &   
37.4 & 34.4 & 27.4 
& 69.0 & 63.0 & 49.9
& 88.8  & 84.2 & 63.4
& 96.8  &  95.1 & 68.4      
& 99.0 &  99.0 & 69.8 
\\ 
\specialrule{1pt}{0pt}{0pt}
\end{tabular}
}
\vspace{-3pt}
\end{table}


\begin{figure}[!tb]
  \centering
  \includegraphics[width=1\linewidth]{Images/proposed_base.pdf} 
  \vspace{-11pt}
  \caption{\textbf{Baseline vs LR-TK0}: 
  Top-1 accuracy for EVA-B/16 on $16\x16$. (more in \Supp)
  }  
\label{fig:performance_dataset}
\vspace{-5pt}
\end{figure}



\begin{table}[!t]
\begin{minipage}{.47\linewidth}
\caption{\textbf{Comparison with SR methods}: 
EVA-B/16 results, SR-specific pre-processing.
}
\label{tab:comparison_with_other_zeroshot}
\label{tab:comparison_with_SR}
\setlength\tabcolsep{2.2pt}
\renewcommand{\arraystretch}{1.2}
\scalebox{0.98}{
\centering
\begin{tabular}{
l|
c|c|c|    c|c|c 
}
\specialrule{1pt}{0pt}{0pt}
\rowcolor{mygray} 
& \multicolumn{3}{c|}{$16\x16$}
& \multicolumn{3}{c}{$32\x32$}
\\[-.4pt] \hhline{~--- ---}
\rowcolor{mygray} 
\multirow{-2}{*}{Method}
& SAR & WAR & Acc
& SAR & WAR & Acc
\\
\hline\hline
Baseline 
& 34.1 & 26.8 & 25.0 
& 71.8 & 59.0 & 51.2 
\\ 
\hline
BSRGAN
& 12.4 & 12.2 & 8.8    
& 37.3 & 28.7 & 26.9 
\\
ESRGAN
& 14.2 & 15.1 & 10.0 
& 40.3 & 32.6 & 28.9 
\\ 
Swinir
& 17.9 & 17.6 & 12.7 
& 47.7 & 38.3 & 34.3
\\ 
AddSR
& 20.5 & 16.8 & 15.0 
& 48.3  & 36.0 &  35.2 \\
Inf-DiT
& 29.0 & 25.3 & 20.9
& 67.7  & 58.6 &  48.0
\\ 
\hline
\rowcolor{mygray} 
Our 
& \textbf{38.9} & \textbf{29.5} & \textbf{28.4} 
& \textbf{73.1} & \textbf{62.0} & \textbf{52.0}
\\
\specialrule{1pt}{0pt}{0pt}
\end{tabular}
}
\end{minipage}
% \quad
\hfill
\begin{minipage}{.49\linewidth}
\setlength\tabcolsep{1.4pt}
\caption{\textbf{Generalization of LR-TK0 with other Zero-Shot Techniques}: 
Visual prompt Tuning (VPT)~\SPCITE{jia2022visual} concatenates 50 learnable tokens to spatial tokens.
RobustSAM~\SPCITE{chen2024robustsam} is an image segmentation model modified for classification.
}
\label{tab:vpt_robust_sam}
\setlength\tabcolsep{2.5pt}
\scalebox{0.95}{
\begin{tabular}{
l| c|c| c|c | c || c 
}
\specialrule{1pt}{0pt}{0pt}
\rowcolor{mygray} 
& \multicolumn{5}{c||}{WAR}
& SAR
\\[-.4pt] \hhline{~--- ---}
\rowcolor{mygray} 
\multirow{-2}{*}{LR-TK0}
& 16 &  32 & 64 & 128 & 224 
& 16 
\\
\hline\hline
Baseline 
& 30.7 & 64.8 & 85.8 & 96.1 & 100 & 38.0 \\
+VPT
& 35.5 & 64.1 & 84.6 & 94.5 & 97.8 & 42.6 \\
+RobustSAM
& 32.2 & 61.5 & 82.7 & 92.4 & 93.0 & 37.8 \\
+LR Tokens 
& 35.4 & 66.4 & 85.9 & 95.5 & 98.7 & 42.4 \\
\specialrule{1pt}{0pt}{0pt}
\end{tabular}
}
\end{minipage}
\vspace{-10pt}
\end{table}












