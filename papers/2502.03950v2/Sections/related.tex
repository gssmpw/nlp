\section{Related Works}
\label{sec:related_works}
\noindent \textbf{Foundation Models (FM):} 
Large-scale models~\citep{kirillov2023segment, girdhar2023imagebind}, pre-trained on massive datasets, demonstrate generalization across numerous downstream tasks. 
% A notable example is 
For example, CLIP~\citep{radford2021learning} embeds $\!\sim\!400$ million image-text pairs in a shared feature space for zero-shot image classification and image-text retrieval. It is also effective in other domains like video-text retrieval~\citep{luo2022clip4clip}, and video and audio understanding~\citep{lin2022frozen, guzhov2022audioclip}.
Joint vision-text learning has also succeeded in tasks such as self-supervision~\citep{ miech2020end}, few-shot~\citep{alayrac2022flamingo}, multi-modal retrieval~\citep{yu2022coca} \etc
% their robustness against real-world challenges needs more work... listing what has been done...} 
However, the robustness of these models against \textit{real-world challenges} \eg harmful images~\citep{qu2024unsafebench}, image quality~\citep{wu2023q}, text quality~\citep{Xu2024}, \etc requires further exploration. \vspace{3pt}\\
\noindent \textbf{Zero Shot:} Zero Shot/Open-set/In-the-wild image classification predicts an unseen class by matching the image with labels~\citep{sun2023eva}.
% , without additional training. 
% tasks like detection~\citep{ zhong2022regionclip}, recognition~\citep{ilharco_gabriel_2021_5143773}, segmentation~\citep{kirillov2023segment} \etc
% evaluates models on unfamiliar modalities (image, video, audio, \textit{etc.}) or concepts/classes . 
% Zero-sho . 
In the past, traditional models have been tested for their zero-shot capabilities~\citep{chao2016empirical, xian2017zero}, however, FMs are better suited for this task. 
Benchmarking their zero-shot capabilities is a relatively newer area of research~\citep{schiappa2023probing, Schulter_2023_ICCV}.
To assess the performance comprehensively, 
we have expanded the pool of models from traditional 10-11 FM backbones \eg 4 backbones \citep{NEURIPS2022_3c4688b6}, 9 backbones \citep{liu2024few}, 6 backbones (\citep{zhang2024progressive}) \etc to \textit{66 backbones}. \vspace{3pt}\\
% \begin{table}[!tb]
%   \caption{\textbf{Foundation Models (FM) Zero-Shot Image Classification Benchmarks}:
%   Few relevant works address real-world challenges, but none study the impact of low resolution on FMs.
%   }
% \label{tab:benchmarks}
% \centering
% \setlength\tabcolsep{2.2pt}
% \renewcommand{\arraystretch}{1.05}
% \begin{tabular}{l|
% c|c|c|c|c
% }
% \specialrule{1pt}{0pt}{0pt}
% \rowcolor{mygray} 
% Benchmark & Year & \#FMs & \#Backbones & \#Datasets & Real World Challenges Simulated \\  
% \hline \hline
% % https://arxiv.org/pdf/2204.08790
% ELEVATER (\citeauthor{NEURIPS2022_3c4688b6}) &  2022 & 4 & 4 & 20 & No \\
% % https://arxiv.org/pdf/2205.03860
% % Zero and R2D2~\citep{xie2022zero} & 2023 & 1 & 2 & 5 & - \\ 
% % BEAR~\cite{Deng_2023_ICCV}*\textit{(video)} & 2023 & 1 & 1 & 18 & - \\
% % https://arxiv.org/pdf/2304.00685
% Survey (\citeauthor{10445007}) & 2024 & 27 & 27 & 11 & No  \\ 
% % https://arxiv.org/pdf/2401.01736
% \citeauthor{liu2024few} & 2024 & 9 & 9 & 14 & No \\ 
% % https://arxiv.org/pdf/2306.16048
% \citeauthor{Xu2024} & 2024 & 7 & 13 & 1 &  
% Specificity (Vagueness) of captions \\ 
%  % https://arxiv.org/pdf/2404.11249
% \citeauthor{zhang2024progressive} & 2024 & 6 & 6 & 8 & No  \\ 
% \hline 
% LR0.FM \textit{(Ours)} & 2024 & 10 & 66 & 15 & Low resolution / Pixelation \\
% \specialrule{1pt}{0pt}{0pt}
% \end{tabular}
% \vspace{-8pt}
% \end{table}
\noindent \textbf{Low Resolution (LR):}
LR images are captured in various practical scenarios and are sometimes used intentionally for computational cost reduction (RECLIP~\citep{li2023reclip}). LR benchmarks mostly focus on face recognition~\citep{9432821, li2018face}, with some work in zero-shot/unconstrained recognition~\citep{8600370, cheng2019low}. 
Super Resolution~\citep{ohtani2024rethinking, gao2023implicit} are often domain-specific or restores only $\ge64\x64$. 
However, there is a lack of study on the robustness of FM(s) against real-world challenges~\citep{Xu2024}, with no previous work on very LR. 
We benchmark FM(s) against LR images and propose a lightweight solution for improving robustness, without training on any of the target datasets~\citep{chen2024robustsam}.
% \sv{Are all works cited in first sentence benchmarks? With the first sentence it reads like that. Is this para ' low resolution bench-marking' or 'low resolution classification'? or just 'low resolution image analysis'}
\vspace{-7pt}