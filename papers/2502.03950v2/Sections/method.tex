\begin{table}[!t]
\centering
\caption{\textbf{Benchmark Models (66 Backbones):}
Pre-training is image-text pairs from datasets like DataComp-1B (DC-1B) \citep{datacomp}, Conceptual Captions (CC) \citep{sharma2018conceptual}, Conceptual 12M (C-12M) \citep{changpinyo2021conceptual}.
% Models with `*' notion are taken from OpenCLIP~\citep{ilharco_gabriel_2021_5143773} implementation. 
Text Encoders are mostly modified vanilla transformers (Tran.)\citep{NIPS2017_3f5ee243}.
Vision backbones use (modified) ViTs~\citep{dosovitskiy2020image}.
\vspace{-4pt}
}
\renewcommand{\arraystretch}{1.2}
\label{tab:model_desc}  
\scalebox{0.9}{
\centering
\setlength\tabcolsep{2pt}
\begin{tabular}{p{2.7cm}|
P{2cm}| P{5.8cm}| P{1.7cm}| P{2.3cm}
}
\specialrule{1pt}{0pt}{0pt}
\rowcolor{mygray} 
Models & \#Backbones & 
\multicolumn{2}{P{7.64cm}|}{Pre-training (Dataset / Size Billion:B \& Million:M)}  
& Text Encoder \\
\hline\hline
% https://assets.amazon.science/cb/e3/e85cc0ca4eb2a81cb223e973ae6e/benchmarking-zero-shot-recognition-with-vision-language-models-challenges-on-granularity-and-specificity.pdf
CLIP \SPCITE{radford2021learning}
& 4 ViTs \& 5 ResNets & WIT-400M 
\SPCITE{radford2021learning} & 400M &  Tran. \SPCITE{radford2019language} \\ 
\hline 
OpenCLIP \SPCITE{ilharco_gabriel_2021_5143773}
& 8 ViTs & \STEXT{DC-1B}, LAION-2B\SPCITE{schuhmann2022laion}, DFN-5B\SPCITE{fang2023data} & 1B-5B & Tran.\SPCITE{ ilharco_gabriel_2021_5143773} \\ 
\hline 
MetaCLIP \SPCITE{xu2023metaclip} & 8 ViTs & \textit{Self} & 400M-2.5B & OpenCLIP \\ 
\hline 
CLIPA \hspace{-0.6cm} (v1\&v2)  
\SPCITE{li2023clipa, li2023clipav2} & 7 ViTs & DC-1B, LAION-2B \SPCITE{schuhmann2022laion} & 1B-2B & Autoregressive Tran. \SPCITE{NIPS2017_3f5ee243} \\
\hline 
SigLIP
\SPCITE{Zhai_2023_ICCV} & 8 ViTs & WebLI \SPCITE{chen2022pali} & 10B & Tran.\\
\hline 
CoCa
\SPCITE{yu2022coca} & 3 ViTs & LAION-2B \SPCITE{schuhmann2022laion}, 
COCO \SPCITE{lin2014microsoft} & 2B & Tran. Decoder \\ 
\hline 
M\textsuperscript{2}-Encoder\SPCITE{guo2024m2encoder}& 
3M\textsuperscript{2}-Encoder & BM-6B \SPCITE{guo2024m2encoder} & 6B & Magneto \SPCITE{Magneto}\\
\hline 
ALBEF \SPCITE{ALBEF} & 4 ALBEF (ViT) & COCO \SPCITE{lin2014microsoft}, Visual Genome \SPCITE{krishna2017visual}, CC, SBU Captions \SPCITE{ordonez2011im2text}, C-12M & 4M-14M & BERT \SPCITE{Devlin2019BERTPO} \\
\hline 
BLIP \SPCITE{li2022blip} & 8 ViTs & ALBEF \SPCITE{ALBEF}, LAION-400M \SPCITE{schuhmann2021laion} & 14M-129M & BERT \SPCITE{Devlin2019BERTPO}\\
\hline
% \makecell[l]{EVA-CLIP (\citeyear{sun2023eva}) \& EVA-CLIP-18B (\citeyear{EVA-CLIP-18B})}
EVA-CLIP(\&18B) \SPCITE{sun2023eva, EVA-CLIP-18B}
& 8 EVA(s) (ViT(s)) & 
LAION-400M\SPCITE{schuhmann2021laion}, LAION-2B\SPCITE{schuhmann2022laion}, Merged-2B \SPCITE{sun2023eva} & 400M-2B & OpenCLIP\\
\specialrule{1pt}{0pt}{0pt}
\end{tabular}
}
\vspace{-6pt}
\end{table}

\vspace{-3pt}
\section{Benchmarking Setup}
\vspace{-2pt}
\noindent \textbf{Model:}
\Cref{tab:model_desc} lists all 10 Foundation models used in our benchmarking\footnote{ 
EVA-CLIP~\citep{sun2023eva} \& EVA-CLIP-18B~\citep{EVA-CLIP-18B} merged into one.}.
CLIP, OpenCLIP, MetaCLIP, CLIPA, and SigLIP use the same ViT model with different pre-training datasets and slight architectural modifications (\eg layer norm position, token masking \textit{etc.}).
M\textsuperscript{2}-Encoder (built on top of CoCa), ALBEF, and BLIP use modified cross attention between text and vision transformers.  
EVA-CLIP is a family of models equipped with recent advancements \eg architectural modifications, token dropping, training via distillation \etc surpassing all existing works. 
Backbones are referred to using their publicly available pre-trained weights, \eg CLIP-ViT L (400M), which means: CLIP model ViT-L architecture, pre-trained on 400 million datasets. `B' would indicate a billion. \vspace{4pt}\\  
% Pre-training Sizes are indicated like 
\noindent \textbf{Dataset:}
\begin{figure}[!tb]
\centering
\subfloat
{
\includegraphics[height=2.9cm]{Images/dataset.png}
}
\hfill 
\subfloat
{
\includegraphics[height=2.9cm]{Images/eval_pipeline.pdf}
}
\vspace{-2pt}
\caption{ 
\emph{Left}: \textbf{Dataset}: Size $\propto\log$ \# test images, and color gradient $\propto$ \# of test classes 
{\color{orange}\textbf{orange}} is 10 \& {\color{black}\textbf{black}} is 1000 classes). 
% \sv{can it be monochrome? lot of blue, single color will help? we can say light and dark?}
\emph{Right}: \textbf{Zero Shot Evaluation}: 
Food-101 image ($32\x32$) generates image embeddings $f_{Img}$, while class labels are filled in templates (1 shown) generating text embeddings (averaged across templates).
The dot product of $f_{Img}$ with text features gives classification logits.
}
\vspace{-7pt}
\label{fig:eval_pipeline}
\label{fig:dataset}
\label{fig:dataset_eval_pipeline}
\end{figure}
\Cref{fig:eval_pipeline} (left) highlights benchmarking datasets size and the number of classes for: 
% of the test set (font size)  (gradient of color) \YSR{this info should be in caption, not here... someone looking at figure will not be able to find it here...}. 
Imagenet \SPCITE{deng2009imagenet}, 
ImageNet-A \SPCITE{hendrycks2021nae}, 
ImageNet-V2 \SPCITE{recht2019imagenet}, 
ImageNet-R \SPCITE{hendrycks2021many}, 
ImageNet-Sketch (ImageNet-SK) \SPCITE{wang2019learning}, 
Caltech101 \SPCITE{griffin2007caltech}, 
DTD split-1 (DTD) \SPCITE{cimpoi2014describing}, 
Food101 \SPCITE{bossard2014food}, 
SUN397 \SPCITE{zhou2014learning}
Stanford Cars (Cars) \SPCITE{kramberger2020lsun}, 
FGVC Aircraft (Aircraft) \SPCITE{maji2013fine},
Oxford Pets (Pets) \SPCITE{parkhi2012cats}, 
Oxford Flowers102 (Flowers102) \SPCITE{liu2016flower}, 
EuroSAT \SPCITE{helber2019eurosat}, 
UCF101 \SPCITE{soomro2012ucf101}. 
Details in \Supp. \vspace{4pt}\\         
\noindent\textbf{Zero-Shot Image Classification}
We adopt CLIP~\citep{radford2021learning} evaluation protocol for all the models as shown in \cref{fig:eval_pipeline} (right).
Image encoder generates embeddings for images, while test labels are used with dataset-specific templates (multiple templates, \Supp) \eg ``a photo of a [label]". Model’s Text encoder generates final text embeddings (averaged across all templates) for the class label.
The dot product of visual and text embeddings produces class logits, with the highest logit score determining the predicted class. 
Accuracy is computed using Top-1 match. \vspace{4pt} \\ 
\noindent \textbf{Low Resolution:}
% As resolution increases, model performance improves~\citep{dosovitskiy2020image}. 
Models are evaluated on their pre-trained resolution, namely $224\x224$ $256\x256$, $378\x378$ \etc 
Low resolution is simulated by downsampling HR images to $16\x16$, $32\x32$, $64\x64$, and $128\x128$ using bicubic interpolation, followed by model specific preprocessing similar to their HR counterparts, \eg resizing to $224\x224$, center crop, \etc 
Performance degradation starts below $64\x64$ (\cref{fig:acc_drop}), so we focus mainly on $16\x16$ and $32\x32$.
This downsampling mimics pixelation as seen in low-resolution cameras (\eg self-driving cars) and distant images (\eg CCTV), \etc \vspace{4pt}\\
\noindent \textbf{Evaluation Metrics}:
\label{sec:eval_metrics}
We represent top-1 accuracy on the dataset `$D$' with a resolution \underline{$n\x n$} as $A^{D}_{n}\in [0,1]$, \eg HR accuracy $A_{HR}^{D}$ $\ge A^{D}_{n}$ (LR accuracy), where HR is model specific $\in$ \{224, 256, 372, 384, 512\}. 
Top-1 scores averaged across datasets is \textbf{ACC-n}. 
Robustness against artifacts~\citep{schiappa2024robustness} is measured by relative robustness 
($\gamma_{n}^{D}=1-(A^{D}_{HR}- A^{D}_{n})/A^{D}_{HR}$).
% and absolute robustness ($\gamma_A^n=1-(A_{224}-A_{n})/100$).
$\gamma_{n}^{D}$ is dataset-specific, and it is common to average scores across datasets for model comparison, denoted by \textbf{Simple Aggregated Robustness (SAR-n)}.
\textit{Higher number indicates more robustness}.
However, there are two significant issues with $\gamma_{n}^{D}$ and SAR-n:





\textbf{\textit{Problem A) Misleading high robustness:}} If the model performs poorly on a challenging dataset \ie performance close to random predictions, then downsampling will likely maintain this random prediction with minimal drop in accuracy, giving abnormally high robustness score. \textit{Ex.} `ALBEF (4M)' for Aircraft dataset, ($A_{\text{rand}}^{\text{aircraft}}\!=\!1\%$), $A_{HR}^{\text{aircraft}}\!=\!2.7\%, A_{16}^{\text{aircraft}}\!=\!1\%$,
\underline{$\gamma_{16}^{\text{aircraft}}\!=\!37\%$}, \ie random predictions yields $\!\sim\!40\%$ robustness ($40\%$ robustness is among the highest, more in \Supp).  


\begin{figure}[!t]
\centering 
\subfloat
{
\includegraphics[height=2.8cm]{Images/new_alpha.pdf}
}
\hfill
\subfloat
{
\includegraphics[height=2.8cm]{Images/selective_spearman3.pdf}
}
\hfill
\subfloat
{
\includegraphics[height=2.8cm]{Images/Pie_dt_weight.pdf}
}
\vspace*{-3pt}
\caption{ 
\textit{Left}: {\textbf{Improved} 
 \boldmath{$\Gamma^{D}_{n}$}} \textbf{vs traditional} {\boldmath{$\gamma^{D}_{n}$}}:
$\!\Gamma^{D}_{n}\!\approx\!\gamma^{D}_{n}\!$ except near random predictions ($\mathcal{E}_D\!\rightarrow\!0$). 
\textit{Mid}: \textbf{Correlation} 
between the ordering of models after averaging of robustness (SAR) across datasets ($\gamma^{D}_{16}$ \& $\Gamma^{D}_{16}$) with dataset's true ordering. 
SAR final ranking ignores datasets like EuroSAT (0.26). 
\textit{Right}: \textbf{Optimized dataset weights} for WAR-16. \textit{Supplementary} contains numeric value. 
}
\label{fig:datasets_weights}
\label{fig:corr_both_gamma}
\label{fig:new_gamma_old_gamma}
\label{fig:hyperparam_war}
\vspace{-3pt}
\end{figure}

\textbf{\textit{Solution: Improved Relative Robustness $\Gamma^{D}_{n}$}:} 
A naive solution is to calculate relative robustness only for correct predictions at the HR resolution. 
However, tracking predictions for each model across all datasets might not be scalable, especially if the dataset contains millions of images. We propose \textit{zero-ing out robustness near random predictions}. We first define \textit{accuracy gap} for the model on a dataset with `C’ classes as \underline{$\boldmath{\mathcal{E}_D\!=\!A^{D}_{HR}\!-\!A^{D}_{rand}}$}, with $\mathcal{E}_D\!\in\![0,1]$, and 
$A^{D}_{rand}\!=\!1/C$ represents random prediction accuracy\footnote{Random guessing one of the `C' class yields $1/C$ accuracy, referred to as $A^{D}_{rand}$ in this work.}.
If $A^{D}_{HR}\!>>\!A^{D}_{rand}$, $\mathcal{E}_D$ will be high. Conversely, if $A^{D}_{HR} \simeq$ random prediction, $\mathcal{E}_D\!\rightarrow\!0$.
Using $\mathcal{E}_D$, we compute \textbf{improved relative robustness} $\Gamma^{D}_{n}$ as 
\vspace{-2pt}
\begin{align}
    \Gamma^{D}_{n} &= \gamma^{D}_{n} \times (1 -  e^{- \alpha (\mathcal{E}_D)^2}) \hspace{0.5cm} \mid \hspace{0.5cm} \alpha >> 1 \hspace{0.5cm} \& \hspace{0.5cm} 0 \le \mathcal{E}_D \le 1 \label{eq:improved_robsutness}
\end{align}
when $\mathcal{E}_D\!\sim\!0$ \ie near random predictions, $\Gamma^{D}_{n}\!\sim\!0$, otherwise $\Gamma^{D}_{n} \approx \gamma^{D}_{n}$, as shown in \cref{fig:new_gamma_old_gamma} (left). 
Hyperparameter
$\alpha$  
is the rate at which 
$\Gamma^{D}_{n}$  declines as accuracy approaches random prediction.
We chose $\alpha=200$ as a middle between 100 (the drop at $\mathcal{E}_D\sim0.2$) and 500 (the drop at $\mathcal{E}_D\sim 0$). 



\textbf{\textit{Problem B) SAR overlooks datasets:}} 
When comparing models, their robustness scores are averaged across datasets (SAR). 
Ideally, model ranking, after averaging robustness across datasets, 
should stay consistent with
their rankings on individual datasets.
However, \cref{fig:corr_both_gamma} (mid) shows the rankings of 66 models after averaging correlate (Spearman Rank) highly with ImageNet (0.99) and DTD (0.88), but only moderately with ImageNet-A (0.56) and weakly / not with EuroSAT (0.26). 
Most datasets follow the ImageNet trend, influencing the final model rankings and minimizing the impact of datasets like ImageNet-A and EuroSAT (behave differently) as if these datasets aren't present.


\textbf{\textit{Solution: Weighted Aggregated Robustness:}}
Averaging the robustness scores gives each dataset score of 1. 
We propose adjusting the dataset weights so that the \textit{model rankings after aggregation reflect each dataset fairly} (\cref{fig:datasets_weights} (right)). 
Weights are optimized such that the correlation (Spearman) between the model rankings after the weighted average and individual dataset rankings are maximized. 
The weighted sum of robustness is:
\textbf{WAR-n}$=\sum_d^{\text{Datasets}} | \Gamma^{d}_{n} \x w^d_n |  / \sum_d^{\text{Datasets}} | w^d_n |$, where $w^d_n$ is dataset weight, and $\Gamma^{d}_{n}$ is dataset-specific improved robustness score for the resolution $n\x n$.

We use Ax tool~\citep{bakshy2018} for optimizing the weights of the dataset $w^d_{16}\!\in\![0.1,1]$ such that the Spearman correlation (SC) between the final model ranking obtained after the weighted averaging and individual dataset ranking is maximized on  
empirically found (more in \Supp):
\vspace{-0.4cm}
\begin{align}
0.95\x
\big(\MTEXT{SC(Imagenet)}\!+\!\MTEXT{SC(ImageNet-V2)}\!+\! \MTEXT{SC(DTD)}\big) + \MTEXT{SC(ImageNet-A)}\!+\!\MTEXT{SC(EuroSAT)}
\end{align} \vspace{-0.65cm}

Optimizing $w^d_{16}$ may give minimal weights to some datasets, thus WAR-n may not reflect the true robustness and is more apt for model comparisons, representing all the datasets.  
Hence we use both Weighted Aggregated Robustness (WAR) using improved relative robustness $\Gamma^{D}_{n}$ (\cref{eq:improved_robsutness}) and simple averaging (SAR) using traditional robustness $\gamma^{D}_{n}$ for evaluating models. 
\textit{Note}, \textbf{$\gamma^{D}_{n}$ and $\Gamma^{D}_{n}$ measure dataset robustness while SAR and WAR measure averaged robustness across the datasets}. 

 












 
 


   



