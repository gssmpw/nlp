\begin{figure}[!t]
\centering
\subfloat
% [\textbf{SAR vs WAR Correlation for $16\x16$}\label{fig:robustness_metrics} ]
{
\includegraphics[height=4.25cm]{Images/sar_war_16.pdf} 
}
\hfill
\subfloat
% [\textbf{Model \& Pretraining dataset Size}. ]
{
\includegraphics[height=4.15cm]{Images/robustness_scatter.pdf}
}
\vspace{-3pt}
\caption{ 
Evaluations at $\!16\!\x\!16\!$. \textit{Left:} 
\textbf{SAR vs WAR:}
WAR improves the correlation (between the ordering of models after aggregation with individual datasets) for EuroSAT (0.26 $\rightarrow$ 0.49 and ImageNet-A ($0.56\rightarrow 0.68$), both computed via $\Gamma^{D}_{16}$.
% (\cref{eq:improved_robsutness})
\textit{Right}:
\textbf{i) Model Size \& ii) Pre-training dataset size} positively impacts robustness.
(i) Dot size $\propto$ GFLOPs, no impact on robustness (ii) Dot size $\propto$ Model Size, positively impact robustness. 
ResNets ($\star$), and transformers ($\bigcirc$).
}
\label{fig:robustness_metrics}
\label{fig:model_arch}
\label{fig:model_arch2}
\vspace{-7pt}
\end{figure}













\begin{figure}[!t]
\centering 
\subfloat
{\includegraphics[height=2.8cm]{Images/deeper_analysis-1.pdf}}
\hfill
\subfloat
{\includegraphics[height=2.8cm]{Images/model_specific.pdf}}
\hfill
\subfloat
{\includegraphics[height=2.8cm]{Images/FT.pdf}}
\vspace{-4pt}
\caption{ 
\textit{Left:} \textbf{DataComp-1B vs LAION-2B}: Smaller DataComp-1B pre-training helps robustness. Models are ordered via size.  
\textit{Mid:} \textbf{Model comparison w/o Size}: Models binned into size buckets ($\pm30$M).
\textit{Right:} \textbf{Fine-tuning degrades} robustness.  (\textit{left} \& \textit{mid}): bigger models are more robust.
}
\label{fig:model_size_wise_comparison} 
\label{fig:DC1B_L2B}
\label{fig:deepanalysis}
\vspace{-10pt}
\end{figure}
\begin{figure}[!t]
\centering 
\subfloat
{\includegraphics[height=3.2cm]{Images/inp_res.pdf}}
\hfill
\subfloat
{\includegraphics[height=3.2cm]{Images/dataset_clustering.pdf}}
\hfill
\subfloat{\includegraphics[height=3.6cm]{Images/heatmap.pdf}}
\vspace{-3pt}
\caption{ 
\textit{Left}: \textbf{High Input Resolution Model} are less robust. 
\textit{Mid}: \textbf{t-SNE of Dataset robustness} Dataset represented via 66 models robustness ($\Gamma^{D}_{16}$), indicates 3 clusters. 
\textit{Right}: \textbf{Layers-wise features L2 similarity}: 
$n\x n$ model layers similarity w/ $224\x224$ ones, for EVA02-B-16. For a given heatmap (\eg $16\x16$), the lower right indicates the similarity of deeper layers (brighter means more similar), while the upper left represents non-similar shallow layers (dull means less similar).  
}
\label{fig:feat_heatmap}
\label{fig:deepanalysis2}
\vspace{-11pt}
\end{figure}

\section{Benchmarking Analysis}

\noindent\textbf{Proposed WAR Metrics}: 
Spearman correlation between the rankings of 66 models, calculated using SAR and WAR averaging of relative robustness $\Gamma^{D}_{16}$ (across all datasets), and the individual dataset rankings is shown in \underline{\cref{fig:robustness_metrics}(left)}.
WAR shows a slight decrease in avg. correlation (SAR-16 \textit{0.89} vs WAR-16 \textit{0.87}), but it also improves the representation of EuroSAT \& ImageNet-A.
The correlation score for EuroSAT increased from a weak/no correlation of 0.26 to a moderate 0.49. \vspace{5pt} \\
\noindent \textbf{Model Architecture / Pretraining:}
\underline{\Cref{fig:model_arch} (right, (i))} shows, on average, 
\textbf{larger model} (x-axis) are \textbf{more robust}. 
Among the models, 
CLIP-ResNets (stars) are the least robust (compared to transformers (dots)) while EVA, MetaCLIP, CLIPA, and OpenCLIP exhibit the highest robustness against the LR.
Higher GFLOP (size of dots) weakly impacts robustness with too many exceptions. \vspace{5pt}\\
\noindent \textbf{Pretraining `Quality over Quantity'}: 
\underline{\Cref{fig:model_arch} (right (ii))} shows pre-training dataset size weakly correlates with robustness, with exceptions like SigLIP (10B), and M2-encoder (6B) performing worse. 
Models pre-trained on DataComp-1B generally outperform those pre-trained on LAION-2B, despite having over 500M fewer image-text pairs (\underline{\cref{fig:DC1B_L2B} (left)}). 
This suggests that the \textbf{model} and \textbf{quality of pre-training} have a greater impact on robustness \textbf{than the quantity of pre-training}. \vspace{5pt}\\
\noindent \textbf{Model Specific}:
We remove architectural size advantages by categorizing top-performing models into parameter buckets as shown in \underline{\cref{fig:model_size_wise_comparison} (mid)}. 
For smaller models (150M and 430M parameters), OpenCLIP matches EVA and outperforms MetaCLIP and CLIPA, despite these two being built on top of OpenCLIP. 
However, for larger models, this trend reverses, with EVA-CLIP remaining superior for comparable sizes.
Two factors contribute to performance discrepancies within models of the same parameter size:
\textbf{(1) Fine-tuning:} 
ALBEF and BLIP fine-tuned variants 
are less robust on EuroSAT and Aircraft, reducing their overall robustness (\underline{\cref{fig:deepanalysis} (right)})
\textbf{(2) Higher input resolution:} 
Models with higher input resolutions (\eg $336\x336$) are generally less robust than their $224\x224$ counterparts, likely due to increased interpolation from $16\x16$ to higher resolutions (\underline{\cref{fig:deepanalysis2} (left)}). \vspace{5pt} \\  
\noindent \textbf{Dataset Specific}:
Relative robustness of 66 models on each dataset forms its robustness vector representations. 
Representing these vectors using t-SNE (\underline{\cref{fig:deepanalysis2} (mid)}), reveal three major dataset clusters:
high-robustness (long bars) (\eg Caltech101), 
weakly robust (medium bars) (\eg ImageNet), and 
least robust (smallest bar) (\eg, ImageNet-A).
This indicates that \textbf{low-resolution performance varies by dataset}, which warrants a deeper dive into dataset-specific robustness, \textit{left as future work}.  


\begin{figure}[!t]
  \centering
  \includegraphics[width=0.98\linewidth]{Images/tsne.pdf}
  \caption{\textbf{Feats t-SNE}: EVA-02-CLIP-B/16 test features for Food-101, colored using class labels. With low resolutions ($16\x16$, and $32\x32$), features become indistinguishable, thereby overlapping.
  }
  \label{fig:tsne_feats}
  \vspace{-3pt}
\end{figure}

\noindent \textbf{Inside Model}:
\underline{\Cref{fig:acc_drop}} shows the accuracy of all models first drops at $64\x64$, with a more significant decline after $32\x32$.
EVA-B/16 features t-SNE (\underline{\cref{fig:tsne_feats}}) shows \textbf{features become indistinguishable as resolution decreases}.
Inside the model, \underline{\Cref{fig:feat_heatmap} (right)} shows the pairwise similarity (L2 distance)~\citep{kornblith2019similarity}
between layers of models trained at different resolutions with the $224\x224$. Diagonal elements ($i^{th}$ layer of $n\x n$ model similarity with $i^{th}$ layer of a model trained at $224\x224$), is \textbf{more similar towards the deeper end} (lower right, the similarity is brighter), \textbf{than the initial layers} (upper left, the similarity is dull).  
Additionally, 
model similarity increases with resolution, while layers remain differentiable at all resolutions (dull non-diagonal values).
\vspace{-5pt}