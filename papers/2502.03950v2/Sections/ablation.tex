
\subsection{Ablation Study}
\noindent \textbf{Design Choices:} \textbf{\Cref{tab:design_choices}} shows 
not freezing the pre-trained weights (\ie fine-tuning the last 4 blocks at 1/100 of the default learning rate) with and without LR tokens (first two rows) degrades the performance, indicating the necessity of preserving pre-trained weights. 
Our design choice is task agnostic \ie model's classification plays no role in learning the HR-LR relationship but classifying LR images into captions (as class labels, \textit{task-oriented}) has more or less the same performance.
\textbf{\Cref{tab:design_choices2}} shows benefit of multi-scale training
% a random sampling of resolution from multi-scale buckets of size  size 3 [$16,32$], [$32,64$], [$64,128$] (\& 4 +[$64,128$]) 
% (\eg [$16,32$] means random sampling of H=W from [$16,32$] dimension) 
% both perform well. 
(3 buckets, faster to train). 

\begin{table}[!tb]
\renewcommand{\arraystretch}{1.2}
\setlength\tabcolsep{1.5pt}
\begin{minipage}{.55\linewidth}
\caption{\textbf{Ablation:} EVA-B/16 trained with 7K captions \\and 50 images/caption. `CL': use of classifier. \textit{Not} frozen means fine-tuning end-to-end. 
}
\vspace{-2pt}
\label{tab:design_choices}  
\centering
\scalebox{0.95}{
\begin{tabular}{
c|c|c|c|
c|c| c
}  
\specialrule{1pt}{0pt}{0pt}
\rowcolor{mygray} 
Frozen & LR Tk. & CL 
& SAR-16 & WAR-16 & SAR-32 & WAR-32 \\
\hline\hline 
\multicolumn{3}{c|}{Baseline (frozen)} 
& 38.0 & 30.7  &  74.4  &  64.8 \\
\hline 
  &  &    
& 31.1 & 24.5 &  67.2  &  56.6 \\
  & \checkmark &   
& 32.8 & 27.8 &  68.1  & 58.3 \\
\rowcolor{mygray} 
\checkmark & \checkmark &    
& 42.3 & 35.2 &  75.3& 66.4 \\
\checkmark & \checkmark &  \checkmark   
&  42.0 & 34.7 &  75.2 & 65.9 \\ 
\specialrule{1pt}{0pt}{0pt}
\end{tabular}
} 
\end{minipage}%
\quad
\begin{minipage}{.41\linewidth}
\setlength\tabcolsep{5pt}
\centering
\caption{\textbf{Multi-Scale (MS) Buckets}: `+' indicates Cumulative addition. E.g. [64,128] has [16,32] and [32,64] buckets.
}
\vspace{-2pt}
\label{tab:design_choices2}  
\scalebox{0.9}{
\begin{tabular}{@{}l@{ }|
c|c
}
\rowcolor{mygray} 
\specialrule{1pt}{0pt}{0pt}
MS Buckets & WAR-16 & WAR-32 \\
\hline\hline
Baseline & 30.74 & 64.81 \\ 
\hline
{[$16,32$]} 
& 34.01 & 64.77 \\
+ {[$32,64$]} 
& 35.28 &  66.10 \\
\rowcolor{mygray} 
+ {[$64,128$]} 
&  35.45 &  \textbf{66.40} \\ 
+ {[$128,224$]} 
& \textbf{35.73} &  65.91 \\
 \specialrule{1pt}{0pt}{0pt}
\end{tabular}
}
\end{minipage}
\vspace{-6pt}
\end{table}


\noindent \textbf{\# Images/Caption:}
\textbf{\Cref{fig:img_per_caption}} shows 
multiple images per caption \& even 2000 captions consistently improve performance across 15 datasets, hinting at bridging the gap between HR-LR  domains. \vspace{4pt} \\
\noindent \textbf{EVA backbones:} 
\textbf{\Cref{fig:eva_backbones}}  shows 
LR tokens enhance various EVA backbones, namely, Base (B/16), Large (L/14 \& L@336), and G (G/14 \& G/14+).
Larger backbones, B$<$L$<$G, benefit from 
more tokens (via more layers).
% L/14 with 336 input resolution) 
Model with 
$336\x336$ input underperforms (validation, \cref{fig:deepanalysis2} (left)). \vspace{4pt} \\
\noindent \textbf{Position of LR Tokens:}
 \textbf{\Cref{fig:pos_lr_tokens}} shows introducing tokens in the earlier layer (starting from $[i]$-th block, and subsequent layers) is more helpful than later. This helps validate the observation in~\cref{fig:feat_heatmap} (right), \ie 
 initial layers suffer more at low resolution than deeper ones, validating the choice of fixing (introducing tokens) at initial layers than just at final features. \vspace{4pt} \\
\noindent \textbf{Grad-CAM results:} 
On low resolutions of $16\x16$, vanilla model attention is dispersed and not as concentrated as $224\x224$ \textbf{(\cref{fig:gram_cam_results})}. However, our method (w/ LR tokens) shows focus on the object which helps to learn better representations at low resolution.
\vspace{-4pt}

\begin{figure*}[!tb]
\centering
\begin{minipage}{0.3\textwidth}
\centering
\includegraphics[height=3.25cm]{Images/imgs_captions_SAR.pdf}
\vspace{-2pt}
\caption{\textbf{\#Images/Caption:} Robustness vs. Size of diffusion generated dataset.
}
\label{fig:img_per_caption}
\end{minipage}
\hfill
\begin{minipage}{0.31\textwidth}
\centering
\includegraphics[height=3cm]{Images/eva_backbones.pdf}
\vspace{-2pt}
\caption{\textbf{LR-TK0 improves all EVA backbones}: L@336 is L/14 with 336 input}
\label{fig:eva_backbones}
\end{minipage}
\hfill
\begin{minipage}{0.31\textwidth}
\centering
\includegraphics[height=3.1cm]{Images/position_lr_token-SAR.pdf}
\vspace{-2pt}
\caption{ 
\boldsymbol{$[i]$} LR tokens introduced starting from $i^{th}$ block (\& none after patchification). 
}
\label{fig:pos_lr_tokens}  
\end{minipage}
 \vspace{-6pt}
\end{figure*}



\begin{figure}[!t]
\centering
\subfloat{
\includegraphics[width=0.48\linewidth]{Images/grad_cam3.pdf}
}
\hfill
\subfloat
{
\includegraphics[width=0.48\linewidth]{Images/grad_cam4.pdf}
}
\caption{ 
\textbf{LR token Grad-CAM}: Baseline (EVA-B/16) attention is scattered at $16\x16$ (compared to $224\x224$).  LR-TK0 focuses on the object, likely capturing fine-grained details. @: input resolution. 
}
\label{fig:gram_cam_results}
 \vspace{-8pt}
\end{figure}
