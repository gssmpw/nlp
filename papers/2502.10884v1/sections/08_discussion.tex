\section{Discussion}
\label{discuss}
\begin{comment}
Unremarkable AI:
by augmenting the users’ routines technology/AI can have signifcant importance for the users yet remain unobtrusive
https://dl.acm.org/doi/pdf/10.1145/3290605.3300468
\end{comment}

This paper has explored how AI coding assistants currently contribute to UI code that is accessible to people with disabilities. While these tools offer a new opportunity for achieving accessibility, we have revealed the remaining challenges and showed how they could be addressed with changes to the way the coding assistants operate.

\ipstart{Which comes first: Adoption or Awareness?}
Adoption is a perennial challenge faced by most accessibility technologies, even when the technology could lead to substantial improvements in user experience.
One reason adoption is low is because awareness is low -- people who could benefit from access technology do not know about it. For example, a survey found that only 10\% of older adults knew what the term ``accessibility'' meant and therefore did not enable any useful settings~\cite{wu2021can,peng2019personaltouch}.
Similarly, developers benefit in many ways from tools that improve the accessibility of their code (\textit{e.g.,} linters, scanners), resulting in better-designed applications and reaching more users. Unfortunately, many developers are unaware of these tools or unwilling to adopt new practices that require changing their original workflow.
For example, while AI assistants like Copilot are capable of generating accessible code, our formative study found that developers were unaware or unwilling to explicitly prompt it to do so.

One goal of our work was to investigate whether developers could increase the adoption of accessibility technology and development practices independently or in tandem with awareness.
For example, prior work~\cite{bigham2014making} found that by ``opportunistically'' zooming into web pages and configuring settings, users could automatically benefit from improved accessibility.
Our motivation is similar: we aimed to improve code accessibility while introducing minimal changes to existing AI-assistant developer workflows \textit{i.e.,} GitHub Copilot.
According to the Visual Studio Marketplace, GitHub Copilot has been installed over 20 million times (as of the time of writing), suggesting that many developers are already familiar with the plugin's interactions, tooling, and interface. 
Our results show that CodeA11y significantly improved code accessibility while maintaining a similar (slightly improved) ease of use to Copilot.
This suggests that if GitHub Copilot included our set of features or were willing to use a similar plugin without requiring substantial deviation from existing workflows, millions of developers could start writing more accessible code immediately.

% While it would have been nice to seen a difference in accessibility awareness among the participants as they interacted with CodeA11y, it is clear that its usage leads to more accessible code with minimal intrusion in the developers' workflows. 
% Our plugin facilitated more accessible development without encroaching on their existing workflows, similar to previous work that sought to automatically improve the accessibility without user knowledge or effort.
% if VScode or copilot adopted this by default, millions of users would likely produce more accessible code using their existing workflows. Although more targeted, accessibility-specific interactions are possible to implement, they risk introducing additional friction for short-term adoption

% While other here is likely a tradeoff between

% The anecdotal observations on reminder type (floating popup vs.~modal) are preliminary and inconclusive, but it is clear that reminders as a whole had a significant, positive influence on completion of accessibility tasks.
% \ipstart{Accessibility Awareness Remains Critical}

\ipstart{AI-Assisted but Developer-Completed}
Even when developers adopt accessibility tools, additional expertise is required to maximize their utility.
This is especially true for AI assistants, which are incapable of generating entirely correct or accessible code. Our work offers some insight into the manual effort needed to write accessible code.
Both our formative and evaluation studies underscore the necessity for developers to manually intervene with AI-generated code to effectively implement accessibility features. A recurrent challenge in AI-assisted coding is the generation of incomplete or boilerplate code, which often requires developers to take additional steps for completion and validation. Our findings reveal that novice developers tend to critically evaluate AI outputs in areas they prioritize, such as visual enhancements, while overlooking aspects they are less familiar with, like accessibility.

One of the tensions of this work is that while we aim to increase the adoption of AI-driven accessibility tools among users with little expertise or awareness, some degree of understanding is required to use these tools effectively. CodeA11y and other tools can employ strategies to scaffold this interaction, \textit{e.g.,} asking a user ``can you describe what's in this image?'' instead of asking them directly for ``alternative text.'' However, developers ultimately need to be willing to expend additional effort and manually implement the more challenging aspects of this work. Thus, while our work suggests that it is possible to ``silently'' improve the accessibility of developer-written code, it is ultimately not a replacement for better accessibility awareness, development practices, and education. Nevertheless, CodeA11y could help gradually improve awareness by slowly introducing and explaining accessibility concepts to users after they have found benefits from using the tool.

\ipstart{Limitations \& Future Work}
We describe several limitations in the current scope of the study and identify avenues for future work to build upon our findings:

First, the utility of the CodeA11y plugin was limited by the constraints placed by our target development environment (Visual Studio Code). 
Because CodeA11y was implemented as a Copilot plugin, we could only access a few APIs available to standard VSCode IDE plugins. The Copilot plugin infrastructure was limiting because it restricted the source code that could be passed to the model (\textit{i.e.,} context window length). Our implementation contained some mechanisms for heuristically determining the most relevant files but ultimately serves as a proof of concept of what would be possible in the future, well-integrated version (\textit{e.g.,} built into Copilot). These factors affected the code generation of our system.

\begin{highlight}
Second, although our user study provides statistical evidence that CodeA11y helps developers write more accessible website code, we acknowledge certain limitations of AI coding assistants that could affect their overall effectiveness and reliability. One well-documented issue, particularly with proactive AI assistants~\cite{chen2024need}, is their potential to provide untimely or irrelevant guidance. For instance, the models may occasionally suggest fixes for problems that do not exist (i.e., false positives), such as recommending changes to code that is already fully compliant with accessibility standards.
While our study did not surface such occurrences---likely because CodeA11y’s suggestions were tied directly to verified issues from an accessibility checker, rather than the tool identifying issues on its own---this risk becomes more salient as AI assistants evolve to more proactively identify and address accessibility problems. Still, prior research suggests that developers often tolerate false positives more readily than false negatives~\cite{kocielnik2019will}, reasoning that overly cautious guidance from an assistant is less harmful than failing to flag genuine accessibility issues. Indeed, even standalone accessibility checkers, which are also known for producing false positives~\cite{huq2023a11ydev}, have been widely adopted due to their overall beneficial effect on UI quality.
Moreover, the occasional presence of false positives does not necessarily negate the value of employing such tools. By raising awareness and prompting developers to consider accessibility from the outset, AI assistants can help cultivate a proactive mindset toward inclusive design. In this sense, the technology does not need to achieve perfect accuracy to have a net positive effect. As the underlying models and APIs improve, and assistants become better integrated with real-world workflows, their precision and utility in improving accessibility are likely to increase.

% Although our user study provides statistical evidence that CodeA11y helps developers write more accessible website code, \begin{highlight} we acknowledge limitations of AI coding assistants that could affect their effectiveness and reliability. One prevalent issue, especially in proactive AI assistants~\cite{chen2024need} like CodeA11y, is the potential for providing irrelevant or untimely feedback, such as suggesting fixes for false alarms (the code leads to accessible UI but the models still suggest fixing). Although this was not observed in our study (likely because CodeA11y was used to resurface verified errors from an accessibility checker rather than directly identifying errors), it remains a concern for broader adoption. However, prior research suggests that users are often more accepting of false positives than false negatives in such systems~\cite{kocielnik2019will}. Furthermore, as with accessibility checkers, which are similarly criticized for false positives~\cite{huq2023a11ydev} but still adopted for their overall improvement of UI accessibility, AI assistants do not need to be perfect to enhance the likelihood of good code quality. \end{highlight}As both LLMs and APIs evolve, we expect CodeA11y's performance to improve.
% Future work could further validate the effectiveness of our approach on a larger scale to examine a broader range of real-world coding tasks and development workflows. Furthermore, the brief duration of our studies may not fully capture the long-term impact of AI coding assistants in real-world scenarios. Extended study periods could provide deeper insights, especially as developers may potentially develop greater awareness of accessibility practices over time through continued use of CodeA11y.


Third, while our study demonstrates the potential of CodeA11y to encourage developers to adopt accessibility practices, we acknowledge that the broader impact of AI coding assistants on long-term learning and behavior changes (e.g., for accessibility awareness) remains underexplored in the current scope of the study. Research on how real-time AI tools can help developers internalize new practices, such as accessibility, or foster long-term behavioral changes could have strengthened the case for CodeA11y’s instructional components. For instance, prior work has shown that AI coding tools can enhance immediate task performance but may not consistently lead to deeper learning or sustained skill retention~\cite{kazemitabaar2023studying}. Similarly, studies on meta-cognitive demands in AI-assisted workflows emphasize the importance of tools promoting reflective learning and adaptive strategies, particularly as developers integrate them into their daily practices~\cite{tankelevitch2024metacognitive}. Although our findings suggest that CodeA11y has the potential to raise awareness of accessibility issues through direct integration with verified accessibility checks, further research is needed to understand whether such tools can foster a lasting developer's commitment to accessibility or similar best practices. Additionally, exploring how these tools impact broader developer workflows, collaboration habits, and the ability to generalize learned behaviors across contexts would provide a more comprehensive view of their instructional value. By examining these dimensions, future studies could better elucidate the role of AI coding assistants in shaping not just productivity, but also the culture of inclusive and responsible software development. While CodeA11y focuses on improving accessibility, its approach could extend to other non-functional requirements, such as privacy and security. Investigating how specialized copilots could be seamlessly invoked within mainstream coding assistants for high-stakes scenarios---such as leveraging CodeA11y for front-end development tasks---represents a promising direction.
\end{highlight}

Finally, we see opportunities to iterate on and refine CodeA11y’s design. Our conservative approach adhered closely to Copilot’s existing interface to minimize friction during adoption. Future work could explore how developers respond to new features and interactions, identifying areas where innovation could enhance usability and functionality without compromising adoption.
\begin{highlight}
By addressing current limitations and exploring broader applications, tools like CodeA11y can refine how developers approach accessibility and other critical non-functional requirements. Beyond technical improvements, such advancements hold the potential to redefine AI’s role in shaping more inclusive, secure, and efficient coding practices. 
\end{highlight}

% \begin{highlight}
% While CodeA11y focuses on improving UI code accessibility, its broader implications include prioritizing non-functional requirements such as privacy and security in software development. Future work should investigate automatic invocation of specialized copilots within mainstream AI coding assistants for high-stakes scenarios, such as leveraging CodeA11y for front-end development tasks.
% \end{highlight}

% Further, we see opportunities to iterate and refine CodeA11y's design.
% We were intentionally conservative in adhering to Copilot's existing interface and set of interactions to reduce developer friction when adopting our tool. However, future work could explore which new features developers might be open to or even eager to adopt.