\section{Related Work}
\label{lit_review}

\begin{highlight}
{

Our research builds upon {\em (i)} Assessing Web Accessibility, {\em (ii)} End-User Accessibility Repair, and {\em (iii)} Developer Tools for Accessibility.

\subsection{Assessing Web Accessibility}
From the earliest attempts to set standards and guidelines, web accessibility has been shaped by a complex interplay of technical challenges, legal imperatives, and educational campaigns. Over the past 25 years, stakeholders have sought to improve digital inclusion by establishing foundational standards~\cite{chisholm2001web, caldwell2008web}, enforcing legal obligations~\cite{sierkowski2002achieving, yesilada2012understanding}, and promoting a broader culture of accessibility awareness among developers~\cite{sloan2006contextual, martin2022landscape, pandey2023blending}. 
Despite these longstanding efforts, systemic accessibility issues persist. According to the 2024 WebAIM Million report~\cite{webaim2024}, 95.9\% of the top one million home pages contained detectable WCAG violations, averaging nearly 57 errors per page. 
These errors take many forms: low color contrast makes the interface difficult for individuals with color deficiency or low vision to read text; missing alternative text leaves users relying on screen readers without crucial visual context; and unlabeled form inputs or empty links and buttons hinder people who navigate with assistive technologies from completing basic tasks. 
Together, these accessibility issues not only limit user access to critical online resources such as healthcare, education, and employment but also result in significant legal risks and lost opportunities for businesses to engage diverse audiences. Addressing these pervasive issues requires systematic methods to identify, measure, and prioritize accessibility barriers, which is the first step toward achieving meaningful improvements.

Prior research has introduced methods blending automation and human evaluation to assess web accessibility. Hybrid approaches like SAMBA combine automated tools with expert reviews to measure the severity and impact of barriers, enhancing evaluation reliability~\cite{brajnik2007samba}. Quantitative metrics, such as Failure Rate and Unified Web Evaluation Methodology, support large-scale monitoring and comparative analysis, enabling cost-effective insights~\cite{vigo2007quantitative, martins2024large}. However, automated tools alone often detect less than half of WCAG violations and generate false positives, emphasizing the need for human interpretation~\cite{freire2008evaluation, vigo2013benchmarking}. Recent progress with large pretrained models like Large Language Models (LLMs)~\cite{dubey2024llama,bai2023qwen} and Large Multimodal Models (LMMs)~\cite{liu2024visual, bai2023qwenvl} offers a promising step forward, automating complex checks like non-text content evaluation and link purposes, achieving higher detection rates than traditional tools~\cite{lopez2024turning, delnevo2024interaction}. Yet, these large models face challenges, including dependence on training data, limited contextual judgment, and the inability to simulate real user experiences. These limitations underscore the necessity of combining models with human oversight for reliable, user-centered evaluations~\cite{brajnik2007samba, vigo2013benchmarking, delnevo2024interaction}. 

Our work builds on these prior efforts and recent advancements by leveraging the capabilities of large pretrained models while addressing their limitations through a developer-centric approach. CodeA11y integrates LLM-powered accessibility assessments, tailored accessibility-aware system prompts, and a dedicated accessibility checker directly into GitHub Copilot---one of the most widely used coding assistants. Unlike standalone evaluation tools, CodeA11y actively supports developers throughout the coding process by reinforcing accessibility best practices, prompting critical manual validations, and embedding accessibility considerations into existing workflows.
% This pervasive shortfall reflects the difficulty of scaling traditional approaches---such as manual audits and automated tools---that either demand immense human effort or lack the nuanced understanding needed to capture real-world user experiences. 
%
% In response, a new wave of AI-driven methods, many powered by large language models (LLMs), is emerging to bridge these accessibility detection and assessment gaps. Early explorations, such as those by Morillo et al.~\cite{morillo2020system}, introduced AI-assisted recommendations capable of automatic corrections, illustrating how computational intelligence can tackle the repetitive, common errors that plague large swaths of the web. Building on this foundation, Huang et al.~\cite{huang2024access} proposed ACCESS, a prompt-engineering framework that streamlines the identification and remediation of accessibility violations, while López-Gil et al.~\cite{lopez2024turning} demonstrated how LLMs can help apply WCAG success criteria more consistently---reducing the reliance on manual effort. Beyond these direct interventions, recent work has also begun integrating user experiences more seamlessly into the evaluation process. For example, Huq et al.~\cite{huq2024automated} translate user transcripts and corresponding issues into actionable test reports, ensuring that accessibility improvements align more closely with authentic user needs.
% However, as these AI-driven solutions evolve, researchers caution against uncritical adoption. Othman et al.~\cite{othman2023fostering} highlight that while LLMs can accelerate remediation, they may also introduce biases or encourage over-reliance on automated processes. Similarly, Delnevo et al.~\cite{delnevo2024interaction} emphasize the importance of contextual understanding and adaptability, pointing to the current limitations of LLM-based systems in serving the full spectrum of user needs. 
% In contrast to this backdrop, our work introduces and evaluates CodeA11y, an LLM-augmented extension for GitHub Copilot that not only mitigates these challenges by providing more consistent guidance and manual validation prompts, but also aligns AI-driven assistance with developers’ workflows, ultimately contributing toward more sustainable propulsion for building accessible web.

% Broader implications of inaccessibility—legal compliance, ethical concerns, and user experience
% A Historical Review of Web Accessibility Using WAVE
% "I tend to view ads almost like a pestilence": On the Accessibility Implications of Mobile Ads for Blind Users

% In the research domain, several methods have been developed to assess and enhance web accessibility. These include incorporating feedback into developer tools~\cite{adesigner, takagi2003accessibility, bigham2010accessibility} and automating the creation of accessibility tests and reports for user interfaces~\cite{swearngin2024towards, taeb2024axnav}. 

% Prior work has also studied accessibility scanners as another avenue of AI to improve web development practices~\cite{}.
% However, a persistent challenge is that developers need to be aware of these tools to utilize them effectively. With recent advancements in LLMs, developers might now build accessible websites with less effort using AI assistants. However, the impact of these assistants on the accessibility of their generated code remains unclear. This study aims to investigate these effects.

\subsection{End-user Accessibility Repair}
In addition to detecting accessibility errors and measuring web accessibility, significant research has focused on fixing these problems.
Since end-users are often the first to notice accessibility problems and have a strong incentive to address them, systems have been developed to help them report or fix these problems.

Collaborative, or social accessibility~\cite{takagi2009collaborative,sato2010social}, enabled these end-user contributions to be scaled through crowd-sourcing.
AccessMonkey~\cite{bigham2007accessmonkey} and Accessibility Commons~\cite{kawanaka2008accessibility} were two examples of repositories that store accessibility-related scripts and metadata, respectively.
Other work has developed browser extensions that leverage crowd-sourced databases to automatically correct reading order, alt-text, color contrast, and interaction-related issues~\cite{sato2009s,huang2015can}.

One drawback of collaborative accessibility approaches is that they cannot fix problems for an ``unseen'' web page on-demand, so many projects aim to automatically detect and improve interfaces without the need for an external source of fixes.
A large body of research has focused on making specific web media (e.g., images~\cite{gleason2019making,guinness2018caption, twitterally, gleason2020making, lee2021image}, design~\cite{potluri2019ai,li2019editing, peng2022diffscriber, peng2023slide}, and videos~\cite{pavel2020rescribe,peng2021say,peng2021slidecho,huh2023avscript}) accessible through a combination of machine learning (ML) and user-provided fixes.
Other work has focused on applying more general fixes across all websites.

Opportunity accessibility addressed a common accessibility problem of most websites: by default, content is often hard to see for people with visual impairments, and many users, especially older adults, do not know how to adjust or enable content zooming~\cite{bigham2014making}.
To this end, a browser script (\texttt{oppaccess.js}) was developed that automatically adjusted the browser's content zoom to maximally enlarge content without introducing adverse side-effects (\textit{e.g.,} content overlap).
While \texttt{oppaccess.js} primarily targeted zoom-related accessibility, recent work aimed to enable larger types of changes, by using LLMs to modify the source code of web pages based on user questions or directives~\cite{li2023using}.

Several efforts have been focused on improving access to desktop and mobile applications, which present additional challenges due to the unavailability of app source code (\textit{e.g.,} HTML).
Prefab is an approach that allows graphical UIs to be modified at runtime by detecting existing UI widgets, then replacing them~\cite{dixon2010prefab}.
Interaction Proxies used these runtime modification strategies to ``repair'' Android apps by replacing inaccessible widgets with improved alternatives~\cite{zhang2017interaction, zhang2018robust}.
The widget detection strategies used by these systems previously relied on a combination of heuristics and system metadata (\textit{e.g.,} the view hierarchy), which are incomplete or missing in the accessible apps.
To this end, ML has been employed to better localize~\cite{chen2020object} and repair UI elements~\cite{chen2020unblind,zhang2021screen,wu2023webui,peng2025dreamstruct}.

In general, end-user solutions to repairing application accessibility are limited due to the lack of underlying code and knowledge of the semantics of the intended content.

\subsection{Developer Tools for Accessibility}
Ultimately, the best solution for ensuring an accessible experience lies with front-end developers. Many efforts have focused on building adequate tooling and support to help developers with ensuring that their UI code complies with accessibility standards.

Numerous automated accessibility testing tools have been created to help developers identify accessibility issues in their code: i) static analysis tools, such as IBM Equal Access Accessibility Checker~\cite{ibm2024toolkit} or Microsoft Accessibility Insights~\cite{accessibilityinsights2024}, scan the UI code's compliance with predefined rules derived from accessibility guidelines; and ii) dynamic or runtime accessibility scanners, such as Chrome Devtools~\cite{chromedevtools2024} or axe-Core Accessibility Engine~\cite{deque2024axe}, perform real-time testing on user interfaces to detect interaction issues not identifiable from the code structure. While these tools greatly reduce the manual effort required for accessibility testing, they are often criticized for their limited coverage. Thus, experts often recommend manually testing with assistive technologies to uncover more complex interaction issues. Prior studies have created accessibility crawlers that either assist in developer testing~\cite{swearngin2024towards,taeb2024axnav} or simulate how assistive technologies interact with UIs~\cite{10.1145/3411764.3445455, 10.1145/3551349.3556905, 10.1145/3544548.3580679}.

Similar to end-user accessibility repair, research has focused on generating fixes to remediate accessibility issues in the UI source code. Initial attempts developed heuristic-based algorithms for fixing specific issues, for instance, by replacing text or background color attributes~\cite{10.1145/3611643.3616329}. More recent work has suggested that the code-understanding capabilities of LLMs allow them to suggest more targeted fixes.
For example, a study demonstrated that prompting ChatGPT to fix identified WCAG compliance issues in source code could automatically resolve a significant number of them~\cite{othman2023fostering}. Researchers have sought to leverage this capability by employing a multi-agent LLM architecture to automatically identify and localize issues in source code and suggest potential code fixes~\cite{mehralian2024automated}.

While the approaches mentioned above focus on assessing UI accessibility of already-authored code (\textit{i.e.,} fixing existing code), there is potential for more proactive approaches.
For example, LLMs are often used by developers to generate UI source code from natural language descriptions or tab completions~\cite{chen2021evaluating,GitHubCopilot,lozhkov2024starcoder,hui2024qwen2,roziere2023code,zheng2023codegeex}, but LLMs frequently produce inaccessible code by default~\cite{10.1145/3677846.3677854,mowar2024tab}, leading to inaccessible output when used by developers without sufficient awareness of accessibility knowledge.
The primary focus of this paper is to design a more accessibility-aware coding assistant that both produces more accessible code without manual intervention (\textit{e.g.,} specific user prompting) and gradually enables developers to implement and improve accessibility of automatically-generated code through IDE UI modifications (\textit{e.g.}, reminder notifications).

}
\end{highlight}



% Work related to this paper includes {\em (i)} Web Accessibility and {\em (ii)} Developer Practices in AI-Assisted Programming.

% \ipstart{Web Accessibility: Practice, Evaluation, and Improvements} Substantial efforts have been made to set accessibility standards~\cite{chisholm2001web, caldwell2008web}, establish legal requirements~\cite{sierkowski2002achieving, yesilada2012understanding}, and promote education and advocacy among developers~\cite{sloan2006contextual, martin2022landscape, pandey2023blending}. In the research domain, several methods have been developed to assess and enhance web accessibility. These include incorporating feedback into developer tools~\cite{adesigner, takagi2003accessibility, bigham2010accessibility} and automating the creation of accessibility tests and reports for user interfaces~\cite{swearngin2024towards, taeb2024axnav}. 
% % Prior work has also studied accessibility scanners as another avenue of AI to improve web development practices~\cite{}.
% However, a persistent challenge is that developers need to be aware of these tools to utilize them effectively. With recent advancements in LLMs, developers might now build accessible websites with less effort using AI assistants. However, the impact of these assistants on the accessibility of their generated code remains unclear. This study aims to investigate these effects.

% \ipstart{Developer Practices in AI-Assisted Programming}
% Recent usability research on AI-assisted development has examined the interaction strategies of developers while using AI coding assistants~\cite{barke2023grounded}.
% They observed developers interacted with these assistants in two modes -- 1) \textit{acceleration mode}: associated with shorter completions and 2) \textit{exploration mode}: associated with long completions.
% Liang {\em et al.} \cite{liang2024large} found that developers are driven to use AI assistants to reduce their keystrokes, finish tasks faster, and recall the syntax of programming languages. On the other hand, developers' reason for rejecting autocomplete suggestions was the need for more consideration of appropriate software requirements. This is because primary research on code generation models has mainly focused on functional correctness while often sidelining non-functional requirements such as latency, maintainability, and security~\cite{singhal2024nofuneval}. Consequently, there have been increasing concerns about the security implications of AI-generated code~\cite{sandoval2023lost}. Similarly, this study focuses on the effectiveness and uptake of code suggestions among developers in mitigating accessibility-related vulnerabilities. 


% ============================= additional rw ============================================
% - Paulina Morillo, Diego Chicaiza-Herrera, and Diego Vallejo-Huanga. 2020. System of Recommendation and Automatic Correction of Web Accessibility Using Artificial Intelligence. In Advances in Usability and User Experience, Tareq Ahram and Christianne Falcão (Eds.). Springer International Publishing, Cham, 479–489
% - Juan-Miguel López-Gil and Juanan Pereira. 2024. Turning manual web accessibility success criteria into automatic: an LLM-based approach. Universal Access in the Information Society (2024). https://doi.org/10.1007/s10209-024-01108-z
% - s
% - Calista Huang, Alyssa Ma, Suchir Vyasamudri, Eugenie Puype, Sayem Kamal, Juan Belza Garcia, Salar Cheema, and Michael Lutz. 2024. ACCESS: Prompt Engineering for Automated Web Accessibility Violation Corrections. arXiv:2401.16450 [cs.HC] https://arxiv.org/abs/2401.16450
% - Syed Fatiul Huq, Mahan Tafreshipour, Kate Kalcevich, and Sam Malek. 2025. Automated Generation of Accessibility Test Reports from Recorded User Transcripts. In Proceedings of the 47th International Conference on Software Engineering (ICSE) (Ottawa, Ontario, Canada). IEEE. https://ics.uci.edu/~seal/publications/2025_ICSE_reca11.pdf To appear in IEEE Xplore
% - Achraf Othman, Amira Dhouib, and Aljazi Nasser Al Jabor. 2023. Fostering websites accessibility: A case study on the use of the Large Language Models ChatGPT for automatic remediation. In Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments (Corfu, Greece) (PETRA ’23). Association for Computing Machinery, New York, NY, USA, 707–713. https://doi.org/10.1145/3594806.3596542
% - Zsuzsanna B. Palmer and Sushil K. Oswal. 0. Constructing Websites with Generative AI Tools: The Accessibility of Their Workflows and Products for Users With Disabilities. Journal of Business and Technical Communication 0, 0 (0), 10506519241280644. https://doi.org/10.1177/10506519241280644
% ============================= additional rw ============================================