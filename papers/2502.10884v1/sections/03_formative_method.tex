\section{Formative Study Methods}
\label{form_methods}
We conducted a formative study to assess the implications of AI coding assistants on web accessibility. We recruited novice developers and tasked them with editing real-world websites using GitHub Copilot. Our goal was to better understand how the use of Copilot affected the accessibility of the user interface code they produced.


%Tasks were chosen from among real issues. We chose issues for which accessibility needed to be considered to complete them correctly, but accessibility was not explicitly mentioned as a requirement in either the task description given to participants or the issue description on the real website.


\ipstart{Tasks}
The participants completed tasks in the codebases for two open-source websites, Kubernetes~\cite{kubernetes} and BBC News~\cite{bbcnews}. Both websites received over 2 million monthly visits worldwide~\cite{similarweb2024} and belong to different categories in the IAB Content Taxonomy~\cite{webshrinker2024}. These websites were developed using different web development frameworks (Hugo and React, respectively). To choose the four specific tasks used in this formative study (Table~\ref{tab:tasks}), we sampled actual issues from each website's repository. We chose issues for which accessibility needed to be considered to complete them correctly, but accessibility was not explicitly mentioned as a requirement in either the task description given to participants or on the issue description on the website's code repository, as illustrated in Figure~\ref{fig:tasks}. Correctly performing the tasks required the consideration of several common web accessibility issues: color contrast, alternative text, link labels, and form labeling~\cite{webaim2024}. The goal was to mirror the kinds of specifications that developers often receive that do not explicitly mention accessibility.

% Based on observations from pilot studies, including success rates and completion times, we classified these tasks from easy to difficult.


\begin{highlight}
\begin{figure*}
    % \tcbox[colframe=purple, colback=white, boxrule=0.35mm, arc=0mm, left=0mm, right=0mm, top=0mm, bottom=0mm]{
    %     \includegraphics[width=\textwidth, trim=15 177 12 140, clip]{assets/tasks.png}
    % }
    \includegraphics[width=\textwidth, trim=15 280 12 100, clip]{assets/tasks.png}
    \caption{\begin{highlight}Examples of task descriptions and visual references given to our participants: (a) Task 2 was to implement a new contact form for subscribing to a mailing list, and (b) Task 3 was to add a `Top Stories' section with linked articles. Successfully completing them required proper labeling of the form elements and links, but this was not explicitly stated in the instructions.\end{highlight}}
    \Description{The figure depicts the two tasks: i) Task T2's instruction is about developing a subscription feature for a weekly mailing list on the Kubernetes Website. It's visual reference includes a subscription box labeled "email address" with a "Subscribe" button, inviting users to sign up for "KubeWeekly" updates. ii) Task T3's instruction is to develop the frontend for adding the Top Stories section on the BBC Website. t's visual reference includes a list of headlines with corresponding dates styled as clickable elements.}
    \label{fig:tasks}
\end{figure*}
\end{highlight}

\begin{table*}
  \caption{Our formative study included four tasks. Each task was not primarily about accessibility but included an accessibility issue that was required to complete the task successfully. \begin{highlight} We adopt the scales of Unacceptable, Average and Good from prior work~\cite{pillai2022website}. Uninformative attributes are those that merely reflect the field, such as `alt' as alt-text or `click here' as link description, without providing more meaningful or descriptive content~\cite{ross2018examining}. Tasks are ranked from easy to difficult based on the time taken and success rates observed in our pilot studies.\end{highlight}}
  \label{tab:tasks}
  \small
  \begin{tabular}{|p{0.185\textwidth}|p{0.075\textwidth}|p{0.13\textwidth}|p{0.525\textwidth}|}
    \toprule
    \textbf{Task} & \textbf{Difficulty} & \textbf{Accessibility Issue} & \textbf{Evaluation Criteria}\\
    \midrule
    (T1) Button Visibility & Easy & Color Contrast & \textit{Unacceptable}: contrast ratio of $<$ 4.5:1 for normal text and $<$ 3:1 for large text\\
    & & & \textit{Average}: WCAG level AA in default state (contrast ratio of $>=$ 4.5:1 for normal text) \\
    & & & \textit{Good}: WCAG level AA in all states (default, hover, active, focus, etc.)\\
    \midrule
    (T2) Form Element & Moderate & Form Labeling & \textit{Unacceptable}: Missing form labels and keyboard navigation \\
    & & & \textit{Average}: One of form labels and keyboard navigation\\
    & & & \textit{Good}: Both form labeling and keyboard navigation \\
    \midrule
    (T3) Add Section & Moderate & Link Labeling & \textit{Unacceptable}: Missing link descriptions \\
    & & & \textit{Average}: Uninformative link descriptions \\
    & & & \textit{Good}: Descriptive link descriptions\\
    \midrule
    (T4) Enhance Image for SEO & Difficult & Adding alt-text & \textit{Unacceptable}: Missing or uninformative alt-text\\
     & & & \textit{Average}: Added alt-text with $<$ 3 required descriptors~\cite{10.1145/3441852.3471207}\\
    & & & \textit{Good}: Added alt-text with $>=$ 3 out of 4 required descriptors\\
    \bottomrule
  \end{tabular}
\end{table*}

% Based on our observations from pilot studies, we classified the tasks as easy to difficult and added time bounds of 30 minutes to each task. 
\begin{comment}
    \begin{table}
  \caption{Study Task Descriptions}
  \label{tab:tasks}
  \small
  \begin{tabular}{m{0.3\textwidth}|p{0.65\textwidth}}
    \toprule
    \textbf{Task} & \textbf{Description}\\
    \midrule
    \makecell[l]{(Task 1) \\ Improving a button's visibility} & An easy task to modify an existing button's styling to improve its visibility.\\
    \midrule
     \makecell[l]{(Task 2) \\ Adding a form element} & A moderate task to implement a new contact form for subscribing to a mailing list, focusing on functionality and user experience.\\
    \midrule
     \makecell[l]{(Task 3) \\ Adding "Top Stories" section} & A moderate task to add links to the top stories in the secondary column on the BBC news article page using data from a provided JSON file.\\
    \midrule
     \makecell[l]{(Task 4) \\ Enhancing image for SEO optimisation} & A difficult task to improve the discoverability of a BBC article image by adding alt-text in the article JSON file and rendering it using an image component.\\
    \bottomrule
  \end{tabular}
\end{table}
\end{comment}


\begin{comment}
\begin{table}
  \caption{Manual Evaluation Criteria for Web Accessibility}
  \label{tab:a11y}
  \small
  \begin{tabular}{p{0.175\textwidth}|p{0.775\textwidth}}
    \toprule
    \textbf{Task Category} & \textbf{Evaluation Criteria}\\
    \midrule
    & \textit{Unacceptable}: contrast ratio of < 4.5:1 for normal text and < 3:1 for large text\\ 
    Button colour contrast  & \textit{Average}: WCAG level AA in default state: 
    minimum contrast ratio of 4.5:1 for normal text\\ 
    & \textit{Good}: WCAG level AA in all states (default, hover, active, focus, etc.)\\ 
    \midrule
    & \textit{Unacceptable}: Missing or uninformative form labels\\ 
    Form labeling & \textit{Average}: Somewhat descriptive form labels~\cite{pillai2022website}\\
    & \textit{Good}: Descriptive form labels and keyboard navigation\\
    \midrule
    & \textit{Unacceptable}: Missing or uninformative ~\cite{ross2018examining} link descriptions\\  
    Link labeling & \textit{Average}: Somewhat descriptive links~\cite{pillai2022website}\\
    & \textit{Good}: Descriptive link labels\\
    & \textit{Unacceptable}: Missing or uninformative~\cite{pillai2022website} alt-text\\
    Adding alt-text & \textit{Average}: Added alt-text with < 3 required descriptors~\cite{10.1145/3441852.3471207}\\
    & \textit{Good}: Added alt-text with >= 3 out of 4 required descriptors\\
    \bottomrule
  \end{tabular}
\end{table}
\end{comment}

\begin{table}[h!]
\caption{\begin{highlight}
Participant User Groups: Each group is assigned specific order of tasks and testing conditions. Participants are evenly and randomly distributed among these groups.
\end{highlight}}
\centering
\small
\label{tab:usergroup}
\begin{highlight}
\begin{tabular}{|p{0.01\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|}
\toprule
\textbf{\#} & \textbf{Order 1, Testing Condition} & \textbf{Order 2, Testing Condition} \\ \midrule
1 & Kubernetes, With AI Assistance & BBC News, No AI Assistance \\ \midrule
2 & Kubernetes, No AI Assistance & BBC News, With AI Assistance \\ \midrule
3 & BBC News, With AI Assistance & Kubernetes, No AI Assistance \\ \midrule
4 & BBC News, No AI Assistance & Kubernetes, With AI Assistance \\ \midrule
\end{tabular}
\end{highlight}
\end{table}

\ipstart{Protocol}
Our within-subjects user study had two conditions: (1) a control condition where participants received no AI assistance, and (2) a test condition where participants used GitHub Copilot. Each participant was assigned to edit two distinct websites, each with two tasks. To counterbalance order effects, participants were evenly and randomly assigned to one of four user groups (Table~\ref{tab:usergroup}), balanced by website order and control/test conditions. Further, to simulate real-world scenarios, we concealed the true purpose of the study from participants. Participants were informed that the study was about the usability of AI pair programmers in web development tasks but were not explicitly instructed to make their web components accessible. This allowed us to observe how developers naturally handle accessibility when it is not explicitly emphasized, reflecting typical developer behavior. The research protocol was reviewed and approved by the Institutional Review Board (IRB) at our university.

\ipstart{Participants}
We employed convenience sampling and snowball sampling methods to recruit our participants. Our study was advertised on university bulletin boards, social media, and shared communication channels (Twitter, Slack, and mailing groups). Our recruitment criteria stipulated that participants must be over 18 years of age, live in the United States, and have self-assessed familiarity with web development. Further, we required the participants to be physically present on our university campus for the duration of the study. To avoid priming during participant recruitment, we did not stipulate awareness of web accessibility as an eligibility criterion. We chose university-specific avenues for recruiting CS students, that reflect a typical novice developer cohort.

Our study enlisted 16 participants (7 female and 9 male; ages ranged from 22 to 29). Almost all of our participants were students and had multiple years of coding experience. Most (n=10) had multi-year \textit{industrial} programming experience (e.g., full-time or intern experience in the company). Nearly all participants (except one) had previously used AI coding assistants. GitHub Copilot and OpenAI ChatGPT were the most popular (n=10). Others preferred Tabnine (n=6) and AWS CodeWhisperer (n=2). 12 participants had self-described substantial experience with HTML and CSS. 10 were proficient in JavaScript and 7 were proficient in React.js. Despite this expertise, the majority (14 participants) were unfamiliar with the Web Content Accessibility Guidelines (WCAG). Only 2 participants knew about these guidelines, but they had not actively engaged in creating accessible web user interfaces or received formal training on the subject (details are provided in Table~\ref{tab:awareness}).

\begin{table*}
  \caption{The distribution of participants' opinions on AI-powered programming tools and their awareness of web accessibility. The percentages in the distribution column indicate the proportion of participants who either disagree (including both `strongly disagree' and `disagree') or agree (including both `strongly agree' and `agree') with the provided statements.}
  \label{tab:awareness}
  \begin{tabular}{>{\raggedright\arraybackslash}p{0.6\textwidth}|>{\raggedright\arraybackslash}p{0.35\textwidth}}
    \toprule
    \textbf{Statement} & \textbf{Distribution}\\
    \midrule
    ``I trust the accuracy of AI programming tools.''& \likertpct{0}{0}{2}{10}{4}{0}{0}\\
    ``I am proficient in web accessibility.''& \likertpct{9}{0}{3}{1}{2}{0}{1}\\
    ``I am familiar with the web accessibility standards, such as WCAG 2.0.''& \likertpct{11}{0}{3}{0}{0}{0}{2}\\
    ``I am familiar with ARIA roles, states, and properties.''& \likertpct{10}{0}{1}{1}{3}{0}{1}\\
  \midrule
\end{tabular}
% \vspace{1em}
\begin{tabular}{@{}>{\centering\arraybackslash}p{\textwidth}@{}}
        \textcolor{customorange}{\rule{7pt}{7pt}} Strongly Disagree \quad
        \textcolor{custompeach}{\rule{7pt}{7pt}} Disagree \quad
        \textcolor{customgray}{\rule{7pt}{7pt}} Neutral \quad
        \textcolor{customlightblue}{\rule{7pt}{7pt}} Agree \quad
        \textcolor{customblue}{\rule{7pt}{7pt}} Strongly Agree \\
    \bottomrule
    \end{tabular}
\end{table*}


\ipstart{Procedure}
The study was conducted in person at our lab, where participants performed programming tasks on a MacBook Pro laptop equipped with IntelliJ IDEA with the GitHub Copilot plugin preinstalled. Before starting the study, we explained the study procedure to the participants and took their informed consent. The participants then watched a 5-minute instructional video explaining Copilot's features, such as code autocompletion and the Copilot chat\footnote{\url{https://www.youtube.com/watch?v=jXp5D5ZnxGM}}. Participants were assigned tasks related to two selected websites, with a total of four tasks to complete in 90 minutes. They were required to work on one website with and the other without GitHub Copilot. Further, they were allowed to access the web for task exploration or code documentation through traditional search engines like Google Search, but with generative results turned off. During the coding session, a researcher observed silently, offering help with tasks, tool usage, or debugging only if participants were stuck, and asked them to move on after 30 minutes, without giving any accessibility-related hints. Based on our observations from pilot studies, we set time limits ranging from 15 to 30 minutes per task. Finally, after completing the coding tasks, they were asked to participate in a 10-15 minute survey on their experience in AI-assisted programming and web accessibility, development expertise, and open-ended feedback. In the end, the participants were compensated with a gift voucher worth 30 USD.



\ipstart{Data Collection and Analysis}
We collected both quantitative and qualitative data for a mixed-method analysis.
For quantitative data, we used an IntelliJ IDEA plugin~\cite{dkandalov_activity_tracker} that tracked user actions --- such as keyboard input (typing, backspace), IDE commands (copy, paste, undo), and interactions with GitHub Copilot (accepting suggestions, opening the Copilot Chat window) --- and recorded their timestamps. 
Additionally, we employed the axe-Core Accessibility Engine 2 to gather accessibility violation metrics, including the type and count of WCAG failures, for each code submission, a method proven reliable in previous studies~\cite{p2023towards}. We also collected AI usage, programming languages and framework preferences, and expertise in web accessibility via a post-task survey.

On the qualitative side, we captured the entire study sessions through screen recordings, resulting in a total of 18.73 hours of video data. We complemented this with observational notes taken during the sessions, documenting verbal comments made by participants. The participants' interactions with Copilot Chat were also recorded for further analysis between prompts and the final code.
The analysis of this data was carried out using open coding and thematic analysis~\cite{clarke2017thematic}. \begin{highlight} Some themes that emerged were: `visual enhancement', `recalling syntax', `feature request', and `code understanding'.\end{highlight} For accessibility evaluation, we manually inspected the websites created during the study and evaluated their accessibility on a qualitative scale of ``Unacceptable,'' ``Average,'' and ``Good'' adopted from prior work~\cite{pillai2022website}. The criteria for these evaluations were developed per best practices identified in prior research published in CHI and ASSETS, detailed further in Table~\ref{tab:tasks}.

% \ipstart{Limitations}
% Firstly, we recruited participants from a single university, suggesting that these individuals may have had access to specific resources and training not universally available, which could limit the generalizability of our findings to a wider population. Additionally, the brief duration of our study may not accurately represent long-term real-world interactions with AI coding assistants; extended study periods could potentially unveil more comprehensive insights into users' ongoing engagement and challenges. Lastly, the laboratory environment of our study, coupled with constant researcher observation, might have introduced an environmental bias, influencing participant behavior. These factors collectively imply that while our study provides important insights into the accessibility and awareness of AI coding assistants, caution should be exercised when extending these findings to the broader developer community.