\section{Linear Parameter Varying Systems} \label{sec:lpv}

Linear Parameter Varying (LPV) systems are a class of nonlinear systems with input-to-output linearity. However, the model coefficients depend on a different set of inputs, referred to as parameters.

Different forms of expressing an LPV model exist, mainly state-space and input-to-output \cite{lpvs}.
%
For this work, we consider an LPV system of the following state-space form:
             %%%%%
    \begin{align} \label{eqn:lpvs}
    \mathbf{x}[k+1] = \mathbf{A}(\Theta[k])\mathbf{x}[k] + \mathbf{B}(\Theta[k])\mathbf{u}[k]
    \end{align}
where $\mathbf{A}(\Theta[k]), \mathbf{B}(\Theta[k])$ are both matrix functions of the parameter vector $\Theta[k]$, $\mathbf{x}[k]$ are the system states, and $\mathbf{u}[k]$ are the inputs.
%
All variables above are functions of discrete time $k \in \{0,1,2,\hdots,\infty\}$.
%
When $\Theta[k]$ depends on either the states or the inputs, the LPV system is referred to as a quasi-LPV model \cite{cisneros2018}.
%
Although this form describes the dynamic portion of an LPV state-space model, the states are treated as direct outputs, which makes the LPV model a first-order input-to-output model.

LPV systems are intrinsically related to fully nonlinear systems, represented as follows:
    %%%
\begin{equation}
    \mathbf{x}[k+1] = \mathbf{f}(\mathbf{x}[k],\mathbf{u}[k])
\end{equation}

We approximate the fully nonlinear system into an LPV representation \cite{lpvs_incremental} by simply employing Taylor's linearization:
    %%%
\begin{equation}
\left \{
\begin{aligned} \label{eqn:incremental_model}
    \Delta \mathbf{x}[k+1] &= \partial_{\mathbf{x}[k]}\mathbf{f}(\mathbf{x}[k],\mathbf{u}[k])\Delta\mathbf{x}[k] + \partial_{\mathbf{u}[k]}\mathbf{f}(\mathbf{x}[k],\mathbf{u}[k])\Delta \mathbf{u}[k]\\
    \mathbf{x}[k+1] &= \mathbf{x}[k] + \Delta \mathbf{x}[k+1]\\
    \mathbf{u}[k+1] &= \mathbf{u}[k] + \Delta \mathbf{u}[k]
    \end{aligned}
    \right .
\end{equation}

If the system is sufficiently well behaved, the LPV counterpart provides a good approximation to the original system dynamics, hence why employing a quasi-LPV model can be a potential representative for nonlinear systems alike, considering $\Theta [k] = (\mathbf{x}[k]^T,\mathbf{u}[k]^T)^T$.
%
This means that the full scheduling function of the quasi-LPV system is the Jacobian.
%
In a black-box scenario, the identification problem consists of identifying the nonlinear system Jacobian and employing it to predict the system.

\mbox{}

\noindent\textit{\textbf{Example}:} Representing a system in LPV form is a matter of context.
%
For instance, take a simple mass-spring system governed by the following differential equations:
    %%%%%
\begin{equation}
\begin{aligned}
    \dot{x} &= v_x\\
    \dot{v}_x &= \frac{F - kx - bv_x}{m}
\end{aligned}
\end{equation}
where $x$ is the load position, $v_x$ is the load velocity, $m$ is the load mass, $k$ is the spring constant, $F$ is any exogenous force exerted in the system, and $b$ is the viscous friction constant.

The model is linear by itself, but if some considerations about the problem are made, it can be regarded as an LPV system.
%
For instance, suppose the viscous friction constant varies over time and is assumed to be not known with exactitude.
%
Then, if one considers $\Theta = b$, and $m = k = 1$ for this system:
\begin{equation}
\begin{aligned}
    \mathbf{A}(\Theta) &= 
    \begin{bmatrix}
      0 & 1\\
      -1 & -\Theta
    \end{bmatrix}\\
    \mathbf{B}(\Theta) &=
    \begin{bmatrix}
        0\\
        1
    \end{bmatrix}
\end{aligned}
\end{equation}

Another convenient example of a (quasi)-LPV system is the Van der Pol oscillator:
    %%%%
\begin{equation}
\begin{aligned} \label{eq:sys:VandePol}
  \dot{x}_1 &= x_2\\
  \dot{x}_2 &= \mu(1-x_1^2)x_2 - x_1 + u
\end{aligned}
\end{equation}

The Van der Pol oscillator is a fully nonlinear system; however, if we consider $\mu = 1$ and $\Theta = x_1^2$, then one can represent it as an LPV system with:
\begin{equation}
\begin{aligned}
    \mathbf{A}(\Theta) &= 
    \begin{bmatrix}
      0 & 1\\
      -1 & 1 -\Theta
    \end{bmatrix}\\
    \mathbf{B}(\Theta) &=
    \begin{bmatrix}
        0\\
        1
    \end{bmatrix}
\end{aligned}
\end{equation}

Since a function of the state was placed as a parameter, the system in question is referred to as  quasi-LPV. However, many of the analysis tools remain the same for both LPV and quasi-LPV.


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LPV System Identification} \label{sec:lpv_ident}

Consider an arbitrary LPV system in the form \eqref{eqn:lpvs}.
%
Also, assume that both matrix functions $\mathbf{A}(\Theta)$ and $\mathbf{B}(\Theta)$ have the following form for the purpose of black-box system identification:
   %%%%
\begin{align}\label{eqn:scheduling_function}
\left \{ 
\begin{aligned}
    \mathbf{A}(\boldsymbol{\Theta}[k]) &= \mathbf{A}_0 + \sum_{i=1}^{N_f}\mathbf{A}_i\phi_i(\Theta[k])\\
    \mathbf{B}(\boldsymbol{\Theta}[k]) &= \mathbf{B}_0 + \sum_{i=1}^{N_f}\mathbf{B}_i\psi_i(\Theta[k])
    \end{aligned} \right .
\end{align}
where $\phi_i$ and $\psi_i$ (with $i \in \{1,2,\hdots,N_f\}$ are scalar functions, the so-called basis functions,  with $N_f$ being the number of different basis functions to compute the scheduling function of the LPV system, through linear combination. This means that the scheduling functions $\mathbf{A}(\boldsymbol{\Theta})$ and $\mathbf{B}(\boldsymbol{\Theta})$ are linear in the parameters, whereas the weights $\mathbf{A}_i$ and $\mathbf{B}_i$ are constants for $i \in \{0,1,\hdots,N_f\}$.
%


Considering the scheduling function candidates in Eqn. \eqref{eqn:scheduling_function}, the LPV system \eqref{eqn:lpvs} becomes:
\begin{equation} \label{eqn:full_lpv}
        \mathbf{x}[k+1] = \left(\mathbf{A}_0 + \sum_{i=1}^{N_f}\mathbf{A}_i\phi_i(\Theta[k])\right)\mathbf{x}[k] + \left(\mathbf{B}_0 + \sum_{i=1}^{N_f}\mathbf{B}_i\psi_i(\Theta[k])\right)\mathbf{u}[k] 
\end{equation}

An alternative way to present the LPV system above is as follows:
   %%%
\begin{equation} \label{eq:LPV:Kronecker}
 \mathbf{x}[k+1] = \mathbf{W_A}\Bigl (\underbrace{\boldsymbol{\phi}(\Theta[k]) \otimes \mathbf{x}[k]}_{\text{features}}\Bigr)    
   + \mathbf{W_B}\Bigl (\underbrace{\boldsymbol{\psi}(\Theta[k]) \otimes \mathbf{u}[k]}_{\text{features}}\Bigr )
 \end{equation}
 where:
\begin{equation} \label{eq:LPV:Kronecker:matrices}
\left \{
\begin{aligned}
 \mathbf{W_A} &=
 \begin{pmatrix}
     \mathbf{A}_0 & \mathbf{A}_1 & \cdots & \mathbf{A}_{N_f}
 \end{pmatrix}\\
     %%%
 \mathbf{W_B} &= \begin{pmatrix}
     \mathbf{B}_0 & \mathbf{B}_1 & \cdots & \mathbf{B}_{N_f}
 \end{pmatrix} \\
    %%%%
 \boldsymbol{\phi} &=
 \begin{pmatrix}
     1 & \phi_1(\Theta[k]) & \phi_2(\Theta[k]) & \cdots & \phi_{N_f}(\Theta[k])
 \end{pmatrix}^T \\
      %%%
 \boldsymbol{\psi} &=
 \begin{pmatrix}
     1 & \psi_1(\Theta[k]) & \psi_2(\Theta[k]) & \cdots & \psi_{N_f}(\Theta[k])
 \end{pmatrix}^T
    \end{aligned}
        \right .
\end{equation}
in which $\otimes$ is the Kronecker product operator.
%
The Kronecker product of two column vectors represents the second operand being replicated $N_f$ times, with each instance being multiplied by the corresponding $\phi_i$ or $\psi_i$.
%
The column of the resulting \textit{feature} vector has dimension $N_f \times n_s$ for the states and $N_f \times n_{in}$ for the inputs, respectively, with $n_s$ being the number of states and $n_{in}$ being the number of inputs. 

Regarding the Van der Pol system \eqref{eq:sys:VandePol} with $\mu=1$, it can be written in the following alternative form using the Kronecker product, referencing the definition in \eqref{eq:LPV:Kronecker}:
   %%%
\begin{equation}
   \left \{ \begin{aligned}
        \boldsymbol{\phi}(\Theta) &= 
        \begin{bmatrix}
         1 \\ \Theta   
        \end{bmatrix}\\
        \mathbf{A_0} &=
        \begin{bmatrix}
            0 & 1\\
            -1 & 1
        \end{bmatrix}, &\quad &
        \mathbf{A_1} &=
        \begin{bmatrix}
            0 & 0\\
            0 & -1
        \end{bmatrix}\\
        \mathbf{B_0} &=
        \begin{bmatrix}
        0\\
        1
    \end{bmatrix}
    \end{aligned} \right .
\end{equation}


Since we do not know the LPV model parameters a priori, the problem of identifying an LPV reduces to identifying the parameters $\mathbf{W_A}$ and $\mathbf{W_B}$ of the scheduling function.
%
There are two main strategies for identifying an LPV system:
    %%%
\begin{itemize}
    \item \textbf{Global Identification}: The system is identified holistically from the whole dataset by performing identification considering the parameters, current states, and control inputs as features and the following states as output.
         %
    This strategy is the same as identifying any model linear on the parameters, performing least squares on weights that multiply a set of features, leading to the curse of dimensionality, which results from an exponential number of features. Mathematically, the features result from the Kronecker products in Eq. \eqref{eq:LPV:Kronecker}, which is the product between the states (or inputs) and the parameters. In contrast, the weights are the matrices $\mathbf{W_A}$  and $\mathbf{W_B}$.
         %
    If global LPV were performed in a system with a large number of states (e.g., any PDE-derived system), a combinatorial explosion would naturally ensue.
    
    \item \textbf{Local Identification}: Provided the parameters can be made constant for an arbitrarily long period of time, one can identify an LPV system through a collection of LTI systems.
    %
    Whenever a parameter is constant, the LPV system is an LTI system.
    %
    Therefore, the local identification can be performed in two steps:
    \begin{enumerate}
        \item Obtain a family of LTI systems, each associated with a constant parameter instance $\Theta_i$ for the $i^{\text{th}}$ LTI system identified.
        \item Identify the scheduling function \eqref{eqn:scheduling_function} with the LTI parameters as output and the corresponding $\Theta_i$ as input, with a dataset the size of the number of LTI systems identified.
    \end{enumerate}
    One can only perform local identification in slow systems, where a constant parameter can be assumed.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proper Orthogonal Decomposition for LPV Systems}  \label{sec:pod_lpv}

Proper Orthogonal Decomposition (POD) is a model order reduction method \cite{Jordanou2023} performed in the context of Dynamic Mode Decomposition (DMD) \cite{DMDc}, as Section \ref{sec:dmdc} shows.
%
POD consists of obtaining a linear transformation that optimally maps the full states of a system into a reduced state \cite{Jordanou2023}.
%
The mapping naturally occurs through the Singular Value Decomposition (SVD) of a matrix $\mathbf{X}$ containing snapshots of the state at given time instants:
     %%%%
\begin{equation}
    \mathbf{W}\mathbf{\Sigma}\mathbf{V}^{T} = \mathbf{X}
\end{equation}
where each column of $\mathbf{X}$ is the system state at a given time.

The singular value decomposition orders the left singular values by how much the state in a resulting transformation contributes to the system output.
%
Therefore, the transformation $\mathbf{T}$ is only the first few columns of the left-singular vectors matrix $\mathbf{W}=[\mathbf{T}~\widetilde{\mathbf{T}}]$, according to the dimension of the reduced state.
%
Thus, obtaining the mapping:
\begin{equation}
    \mathbf{x}[k] = \mathbf{T}\mathbf{z}[k]
\end{equation}
where $\mathbf{x}$ are the states in the full dimension and $\mathbf{z}$ are the states in the reduced order space.

Since the singular vectors matrix has the property $\mathbf{T}^T\mathbf{T} = \mathbf{I}$, obtaining a reduced linear system from $\mathbf{x}[k+1] = \mathbf{A}\mathbf{x}[k] + \mathbf{B}\mathbf{u}[k]$ is trivial:
\begin{equation}
   \mathbf{z}[k+1] = \mathbf{T}^T\mathbf{AT}\mathbf{z}[k] + \mathbf{T}^T\mathbf{B}\mathbf{u}[k]
\end{equation}
notice that matrices such as $\mathbf{T}^T\mathbf{AT}$ can be calculated offline (outside of runtime).

The matter with nonlinear systems is not as straightforward:
\begin{equation}
    \mathbf{z}[k+1] = \mathbf{T}^T\mathbf{f}(\mathbf{Tz}[k],\mathbf{u[k]})
\end{equation}
since, besides the reduction being performed, the computation remains of the scale of the full order model because of the $\mathbf{T}^T\mathbf{f}(\cdot)$ term.
%
Countermeasures to reduce computation, such as Discrete Empirical Interpolation \cite{Chatu2010}, are not discussed in this work.

For LPV systems such as \eqref{eq:LPV:Kronecker}, the reduction has the following form:
     %%%%
\begin{equation}
 \mathbf{z}[k+1] = \mathbf{T}^T\mathbf{W_A}\Bigl(\boldsymbol{\phi}(\Theta[k]) \otimes \mathbf{T}\mathbf{z}[k]\Bigr) 
     + \mathbf{T}^T\mathbf{W_B}\Bigl(\boldsymbol{\psi}(\Theta[k]) \otimes \mathbf{u}[k]\Bigr)   
\end{equation}

The Kronecker product obeys the following property given four arbitrary matrices with matching dimensions:
     %%%
\begin{equation}
    \mathbf{AB} \otimes \mathbf{CD} = (\mathbf{A} \otimes \mathbf{C})(\mathbf{B} \otimes \mathbf{D}),
\end{equation}
which is known as the \textit{Kronecker mixed-product property} \cite{Zhang2013}.

By using this property, we can rewrite the LPV system \eqref{eq:LPV:Kronecker} as follows:
   %%%%
\begin{equation}
\begin{aligned}
 \mathbf{z}[k+1] &= \mathbf{T}^T\mathbf{W_A}(\mathbf{I} \otimes \mathbf{T})\bigl(\boldsymbol{\phi}(\Theta[k]) \otimes \mathbf{z}[k]\bigr)    + \mathbf{T}^T\mathbf{W_B}\bigl (\boldsymbol{\psi}(\Theta[k]) \otimes \mathbf{u}[k]\bigr)   \\
        %%%
    &= \widetilde{\mathbf{W}}_{\mathbf{A}}\bigl(\boldsymbol{\phi}(\Theta[k]) \otimes \mathbf{z}[k]\bigr)   +\widetilde{\mathbf{W}}_{\mathbf{B}}\bigl(\boldsymbol{\psi}(\Theta[k]) \otimes \mathbf{u}[k]\bigr) 
 \end{aligned} \label{z:reduced:dynamics}
\end{equation}
This way, the matrix  $\widetilde{\mathbf{W}}_{\mathbf{A}}=\mathbf{T}^T\mathbf{W_A}(\mathbf{I} \otimes \mathbf{T})$ can be calculated offline, which means that LPV and quasi-LPV systems benefit fully from the POD method. Notice that the reduced-order system dynamics in \eqref{z:reduced:dynamics} is analogous to the full-state system dynamics in \eqref{eq:LPV:Kronecker}.

\begin{comment}
% There are some cases where the number of parameters is at least as high as the number of states, specially if an LPV is performing an identification to obtain the incremental form \eqref{eqn:incremental_model} of a nonlinear system model, as the number of parameters will be a function of the number of inputs and states.
% %
% In this case, we can easily perform POD to reduce the number of features to a suitable dimension:
% \begin{equation}
%   \boldsymbol{\phi}(\Theta[k]) = \mathbf{T_p}\mathbf{p}[k]  
% \end{equation}
\end{comment}


%%%%%%%%%%%%%%%
\section{Dynamic Mode Decomposition with Control (DMDc)}\label{sec:dmdc}

Consider the following linear state-space model with state $\mathbf{x}$ and control input $\mathbf{u}$:
\begin{equation}
    \mathbf{x}[k+1] = \mathbf{A}\mathbf{x}[k] + \mathbf{B}\mathbf{u}[k]
        \label{eq:DMCc:full-state:dyn}
\end{equation}
where the number of states $n_s$ of $\mathbf{x}[k]$ is large enough so that the matrix $\mathbf{A}$ is difficult to directly compute, and $n_u$ is the number of control inputs.

Instead of directly solving the least squares problem, the Dynamic Mode Decomposition addresses the rank-limited Procrustes problem \cite{DMDc}:
\begin{subequations} \label{eqn:procustes}
\begin{align}
    \min_{\mathbf{A},\mathbf{B}} ~&\| \mathbf{Y} - (\mathbf{A}\mathbf{X} + \mathbf{B}\mathbf{U})\|\\
    \text{s.t.} ~~&  \text{rank}(\left[\mathbf{A},\mathbf{B}\right]) = r
\end{align}
\end{subequations}
%
where:
\begin{equation}
\left \{ \begin{aligned}
    \mathbf{X} &=
    \begin{pmatrix}
    \mathbf{x}[0] & \mathbf{x}[1] & \cdots & \mathbf{x}[N-1] 
    \end{pmatrix}\\
        %%%%%
    \mathbf{U} &=
    \begin{pmatrix}
    \mathbf{u}[0] & \mathbf{u}[1] & \cdots & \mathbf{u}[N-1] 
    \end{pmatrix}\\
        %%%
    \mathbf{Y} &=
    \begin{pmatrix}
    \mathbf{x}[1] & \mathbf{x}[2] & \cdots & \mathbf{x}[N] 
    \end{pmatrix}
\end{aligned} \right . \label{eq:def:X-U-Y}
\end{equation}

DMDc solves the least squares problem, limiting the solution's rank to a given value $r$.
%
One obtains this solution by performing SVD on the concatenation of $\mathbf{X}$ and $\mathbf{U}$:
     %%%
\begin{equation}
    \mathbf{W}\mathbf{\Sigma}\mathbf{V}^T =
    \begin{bmatrix}
        \mathbf{X}\\
        \mathbf{U}
    \end{bmatrix}
\end{equation}
where $\mathbf{W}$, the left singular vectors matrix, has dimension $n \times N$, $\mathbf{\Sigma}$, the singular values matrix, has dimension $N \times N$, $\mathbf{V}$ has dimension $N \times N$; further, $n=n_s+n_u$ is the number of states and control inputs and $N$ is the number of training examples/snapshots contained inside the matrix.

The solution to the standard least squares problem is expressed as \cite{DMDc}:
    %%%
\begin{equation}
    \begin{bmatrix}
        \mathbf{A} & \mathbf{B} 
    \end{bmatrix} = \mathbf{Y}\mathbf{V}\mathbf{\Sigma}^{-1}\mathbf{W}^T
\end{equation}

The singular value matrix $\mathbf{\Sigma}$ is a good indicator of whether or not the least squares problem is well posed.
%
As a diagonal matrix, the inversion is merely the reciprocal of each element.
%
If a singular value is too small, the inversion $\mathbf{\Sigma}^{-1}$ can result in values close to infinity.
%
Regularization is a popular solution to bad training data conditioning and, in the SVD framework, it can be included in the problem as follows:
     %%%%%%
\begin{equation}
    \begin{bmatrix}
        \mathbf{A} & \mathbf{B} 
    \end{bmatrix} = \mathbf{Y}\mathbf{V}\mathbf{\Sigma^r}\mathbf{W}^T
\end{equation}
where $\mathbf{A}$ corresponds to the first $n_s$ columns of the right-hand side, and $\mathbf{B}$ is composed of the rest $n_u$ columns, and each diagonal element of $\mathbf{\Sigma^r}$ has the following definition:
     %%%%
\begin{equation}
   \mathbf{\Sigma}^{\mathbf{r}}_i = \frac{\sigma_i}{\sigma_i^2 + \lambda^2}
\end{equation}
where $\lambda$ is the regularization parameter and $\sigma_i$ is the $i^{th}$ largest singular value.
%
This equation shows how the regularization term avoids undesirably large values by having the regularization parameter dominate over the small singular value.

To constrain the rank $r$ of the solution, we consider only the first $r$ columns of $\mathbf{W}$ and $\mathbf{V}$, and keep only the largest $r$ singular values of $\mathbf{\Sigma}^{\mathbf{r}}$, obtaining $\mathbf{W}_r$, $\mathbf{V}_r$, and $\mathbf{\Sigma}_r^{\mathbf{r}}$.

With DMDc, we perform model-order reduction without computing the full matrices $\mathbf{A}$ and $\mathbf{B}$.
%
We define a new SVD from $\mathbf{Y}$, performing the same rank-$r$ truncation as the previously described SVD:
    %%%
\begin{equation} \label{eqn:pod_in_dmd}
\mathbf{W}_{\mathbf{y}r}\mathbf{\Sigma}_{\mathbf{y}r}\mathbf{V}_{\mathbf{y}r}^T \approx \mathbf{Y}
\end{equation}

Then, we perform POD for the definitions of $\mathbf{A}$ and $\mathbf{B}$ according to DMDc \cite{DMDc} with transformation matrix $\mathbf{W}_{\mathbf{y}r}$:
    %%%
\begin{equation} \label{eqn:dmd2}
\left \{
\begin{aligned}
    \widetilde{\mathbf{A}} &= \mathbf{W}_{\mathbf{y}r}^T\mathbf{Y}\mathbf{V}_{r}\mathbf{\Sigma}_{r}^{\mathbf{r}}
                \mathbf{W}_{r,1}^T\mathbf{W}_{\mathbf{y}r}\\
       %%%
    \widetilde{\mathbf{B}} &= \mathbf{W}_{\mathbf{y}r}^T\mathbf{Y}\mathbf{V}_{r}
         \mathbf{\Sigma}_r^\mathbf{r}\mathbf{W}_{r,2}^T
\end{aligned}
         \right .
\end{equation}
%
obtaining the reduced-order matrices $\widetilde{\mathbf{A}}$ and $\widetilde{\mathbf{B}}$, with $\mathbf{W}_{r,1}$ corresponding to the rows of $\mathbf{W}_{r,1}$ related to $\mathbf{A}$, and $\mathbf{W}_{r,2}$ corresponding to the rows of $\mathbf{W}_{r}$ related to $\mathbf{B}$, namely
\begin{equation*}
   \mathbf{W}_r = 
       \begin{bmatrix}
            \mathbf{W}_{r,1} \\
            \mathbf{W}_{r,2}
        \end{bmatrix}
\end{equation*}


The reduced-order model can now be expressed as:
   %%%
\begin{equation}
\mathbf{z}[k+1] = \widetilde{\mathbf{A}}\mathbf{z}[k] 
         + \widetilde{\mathbf{B}}\mathbf{u}[k] 
\end{equation}


The non-zero eigenvalues of $\widetilde{\mathbf{A}}$ are also eigenvalues of $\mathbf{A}$ \cite{Htu2014}, and an eigenvector $\boldsymbol{\phi}$ of $\mathbf{A}$ can be obtained given an eigenvalue-eigenvector pair $(\lambda,\boldsymbol{\omega})$ of $\widetilde{\mathbf{A}}$ as follows:
    %%%%
\begin{equation}
    \boldsymbol{\phi} = \frac{1}{\lambda}\mathbf{Y}\mathbf{V}\mathbf{\Sigma}^{-1}\boldsymbol{\omega}
\end{equation}

In summary, DMDc is a faster way to compute eigenvalues and eigenvectors of a large-scale system without having to compute the matrix itself \cite{Htu2014}.
%
The algorithm was designed for situations where $n \gg N$ ($r = N$), where $n$ is so large that gathering $n$ samples is impeditive (\textit{e.g.}, grid-based EDP simulations, as an oil reservoir).
  
 
%%%%%%%%%%%%%%%%%%%
\section{Identification of Large-Scale LPV Systems} \label{sec:dmd_lpv}

The idea developed in this work is based on \cite{Gosea2021}, which shows how to apply DMD to systems that are nonlinear but linear in the parameters.
%
The method is the same as DMDc, with the nonlinear features added to the Procrustes problem's input \eqref{eqn:procustes}.
%
We reformulate the same idea in the context of LPV system identification, both locally and globally.


%%%%%%%%%%%%%%%%%%%%%%
\subsection{Global LPV System Identification} \label{subsec:Glb-LPV}

The least squares cost function for LPV cases is:
    %%%%
\begin{equation} \label{eqn:procustes}
\begin{aligned}
    \min_{\mathbf{W_A},\mathbf{W_B}} ~& \| \mathbf{Y} - (\mathbf{W_A}\mathbf{X_P} + \mathbf{W_B}\mathbf{U_P})\|\\
        %%%%
    \text{s.t.} ~~& \text{rank}(\left[\mathbf{A},\mathbf{B}\right]) = r
\end{aligned}
\end{equation}
where $\mathbf{X_P}$ and $\mathbf{U_P}$ are matrices that have their columns defined by the Kronecker product $\otimes$ between each corresponding column of the parameter data matrix $\mathbf{P_x}$ and state data matrix $\mathbf{X}$ for $\mathbf{X_P}$; and $\mathbf{P_u}$ with $\mathbf{U}$ for $\mathbf{U_P}$.
%
The parameter data matrix $\begin{bmatrix}\mathbf{P_x} & \mathbf{P_u}\end{bmatrix}$ contains the features resulting from the nonlinear mapping of the parameters, with each column corresponding to a given instant in time (note that $\boldsymbol{\phi}[k] \coloneqq \boldsymbol{\phi}(\Theta [k])$), with the analog being valid for $\psi$:
\begin{equation} \label{eq:def:Px-Pu}
 \left \{ 
\begin{aligned}
  \mathbf{P_x} &=
    \begin{pmatrix}
        \boldsymbol{\phi}[0] & \boldsymbol{\phi}[1] & \cdots & \boldsymbol{\phi}[N-1]
    \end{pmatrix}\\
     \mathbf{P_u} &=
    \begin{pmatrix}
        \boldsymbol{\psi}[0] & \boldsymbol{\psi}[1] & \cdots & \boldsymbol{\psi}[N-1]
    \end{pmatrix}
\end{aligned} \right .
\end{equation}
and, therefore, $\mathbf{X_p}$ and $\mathbf{U_p}$ have the following form:
\begin{equation} \label{eq:def:Xp-Up}
 \left \{ 
\begin{aligned}
    \mathbf{X_P} &=
    \begin{pmatrix}
        \boldsymbol{\phi}[0] \otimes \mathbf{x}[0] & \boldsymbol{\phi}[1] \otimes \mathbf{x}[1] & \cdots & \boldsymbol{\phi}[N-1] \otimes \mathbf{x}[N-1]
    \end{pmatrix}\\
                %%%%%
     \mathbf{U_P} &=
    \begin{pmatrix}
        \boldsymbol{\psi}[0] \otimes \mathbf{u}[0] & \boldsymbol{\psi}[1] \otimes \mathbf{u}[1] & \cdots & \boldsymbol{\psi}[N-1]\otimes \mathbf{u}[N-1]
    \end{pmatrix}
\end{aligned} \right .
\end{equation}


For the global method, the application of DMD is straightforward, as the least squares solution for such LPV identification problems is:
\begin{equation}
    \begin{bmatrix}
        \mathbf{W_A} & \mathbf{W_B} 
    \end{bmatrix} = \mathbf{Y}\mathbf{V}\mathbf{\Sigma^r}\mathbf{W}^T
\end{equation}
with $\mathbf{Y}$ defined as in \eqref{eq:def:X-U-Y}, and 
considering the following SVD operation and the definitions of Section \ref{sec:dmdc}:
    %%%%
\begin{equation}
    \mathbf{W}\mathbf{\Sigma}\mathbf{V}^T =
    \begin{bmatrix}
        \mathbf{X_P}\\
        \mathbf{U_P}
    \end{bmatrix}
\end{equation}

The reduction follows \eqref{eqn:pod_in_dmd} exactly as in DMDc.
%
However, the next step must be performed differently.
%
By performing Model Order Reduction with $\eqref{eqn:pod_in_dmd}$ in an LPV system of the same type as \eqref{eqn:full_lpv}, one obtains the following reduced system:
     %%%%%%%%%%%%%%%
\begin{multline} \label{eqn:mor_lpv}
        \mathbf{z}[k+1] = \left(\mathbf{W}_{\mathbf{y}r}^T\mathbf{A}_0\mathbf{W}_{\mathbf{y}r} + \sum_{i=1}^{N_f}\mathbf{W}_{\mathbf{y}r}^T\mathbf{A}_i\mathbf{W}_{\mathbf{y}r}\phi_i(\Theta[k])\right)\mathbf{z}[k] \\ 
             %%
          + \mathbf{W}_{\mathbf{y}r}^T\left(\mathbf{B}_0 + \sum_{i=1}^{N_f}\mathbf{B}_i\psi_i(\Theta[k])\right)\mathbf{u}[k] 
\end{multline}

One can see that the reduced matrix for $\mathbf{W_{B}}$ is $\widetilde{\mathbf{W}}_{\mathbf{B}} = \mathbf{W}_{\mathbf{y}r}^T\mathbf{W_B}$, making the definition of the control weights similar to the DMDc case for linear systems. On the other hand, the reduced matrix $\widetilde{\mathbf{W}}_{\mathbf{A}}$ for $\mathbf{W_A}$  is obtained differently.

The compact form of the LPV places the reduced system in the following form:
   %%%%
\begin{multline}
\mathbf{z}[k+1] = \mathbf{W}_{\mathbf{y}r}^T\mathbf{W_A}\Bigl(\boldsymbol{\phi}(\Theta[k]) \otimes \mathbf{W}_{\mathbf{y}r}\mathbf{z}[k]\Bigr)   \\  
         %%%%%%%%%%%%
 + \mathbf{W}_{\mathbf{y}r}^T\mathbf{W_B}\Bigl(\boldsymbol{\psi}(\Theta[k]) \otimes \mathbf{u}[k]\Bigr)    
\end{multline}

By considering the mixed-product property of the Kronecker product, we obtain the equivalent representation:
    %%%%
\begin{multline}
\mathbf{z}[k+1] = \mathbf{W}_{\mathbf{y}r}^T\mathbf{W_A}\Bigl(\mathbf{I}_p \otimes \mathbf{W}_{\mathbf{y}r}\Bigr)\Bigl(\boldsymbol{\phi}(\Theta[k]) \otimes \mathbf{z}[k]\Bigr) 
           %%%%%%%%%%%%%
  \\ + \mathbf{W}_{\mathbf{y}r}^T\mathbf{W_B}\Bigl(\boldsymbol{\psi}(\Theta[k]) \otimes \mathbf{u}[k]\Bigr)    
\end{multline}
where $\mathbf{I}_p$ is an identity matrix of the same dimension as the number of  parameter functions.


The newly obtained Kronecker product $(\mathbf{I}_p \otimes \mathbf{W}_{\mathbf{y}r})$ represents directly the post multiplication of each block element $\mathbf{A}_i$ by $\mathbf{W}_{\mathbf{y}r}$.

The result of this discussion is the synthesis of the reduced form of the weights of the DMD over the LPV system: 
    %%%
\begin{equation}
\left \{
\begin{aligned} \label{eqn:dmdc_lpv}
    \widetilde{\mathbf{W}}_{\mathbf{A}} &= \mathbf{W}_{\mathbf{y}r}^T\mathbf{Y}\mathbf{V}_{r}\mathbf{\Sigma}_r^{\mathbf{r}}\mathbf{W}_{r,1}^T\left(\mathbf{I}_p \otimes \mathbf{W}_{\mathbf{y}r}\right)\\
             %%%%
    \widetilde{\mathbf{W}}_{\mathbf{B}} &= \mathbf{W}_{\mathbf{y}r}^T\mathbf{Y}\mathbf{V}_r\mathbf{\Sigma}_{r}^{\mathbf{r}}\mathbf{W}_{r,2}^T
\end{aligned}
  \right .
\end{equation}

The methods for obtaining the dynamic modes and approximate original eigenvectors are precisely as in DMDc, with the detail that one can calculate the eigenvalues and eigenvectors related to the contribution of a specific block $\mathbf{A}_i$ related to feature map $\phi_i(\Theta [k])$ of $\mathbf{W_A}$.


%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Local LPV Identification}

Local system identification entails applying the system identification procedure multiple times.
%
 To perform local identification in the DMD-LPV context, we first obtain the POD transformation matrix $\mathbf{W}_{\mathbf{y}r}$ with the entire dataset of state snapshots.
%
Then, the reduced weights are identified directly by performing POD on the full matrices $\mathbf{A}(i)$ and $\mathbf{B}(i)$, which are obtained beforehand.
%
The equation that defines the identification of $\widetilde{\mathbf{W}}_{\mathbf{A}}$ and $\widetilde{\mathbf{W}}_{\mathbf{B}}$ in this case is:
   %%%%
   \begin{equation}
   \left \{
\begin{aligned} \label{eqn:local_dmdlpv}
     \mathbf{W}_{\mathbf{y}r}^T\mathbf{A}(i)\mathbf{W}_{\mathbf{y}r} &=\widetilde{\mathbf{W}}_{\mathbf{A}}(\phi(\boldsymbol{\Theta}_i) \otimes \mathbf{I_s})\\
          %%%
     \mathbf{W}_{\mathbf{y}r}^T\mathbf{B}(i) &=\widetilde{\mathbf{W}}_{\mathbf{B}}(\psi(\boldsymbol{\Theta}_i) \otimes \mathbf{I_{in}})
\end{aligned}
 \right .
\end{equation}
where $\mathbf{I_s}$ is the identity matrix on the same dimension as the reduced state, and $\mathbf{I_{in}}$ is the identity matrix on the same dimension as the number of system inputs.
%
Although the system outputs are matrices, least squares can be performed normally for both reduced weights.
%
Recovering the original system involves the same steps as the global LPV identification and DMDc cases.

Note that there are two possible approaches for this identification:
     %%%
\begin{itemize}
    \item Identify $\mathbf{W_{yr}}$ and the LTI systems separately, and then reduce the matrices dimension during the identification of the LPV parameters.
    \item Use $\mathbf{W_{yr}}$ to identify each LTI system by mapping the states to the reduced dimension, so no identification is performed in the full space.
\end{itemize}

In both cases, the reduced weights can be identified by solving the Procrustes problem, which makes this method more efficient than global identi\-fi\-cation---the global case uses a Kronecker product between the parameters and the full state inputs as input matrix to the SVD.
%
The proposal to identify each LTI system in the latent space obeys the DMD logic of noninvasive MOR identification since every model training is not performed in the full state space.
%
In fact, every linear system is identified in the context of the full LPV model latent space, and the POD transformation $\mathbf{W_{yr}}$ serves to map the reduced order states into the full order states in the context of the LPV model.



\begin{comment}
% \subsection{Reducing the Number of Features: Principal Component Analysis}

% The DMD-LPV, up until now, was formulated without considering that the number of parameters (and parameter features) can also be large.
% %
% If both the number of parameters and features is large, then the dimension of matrices such as $\mathbf{X_P}$, defined in Eq. \eqref{eq:def:Xp-Up}, becomes ever more computationally intractable.
% %

% To reduce the number of features in a model, POD has a counterpart theory in machine learning literature called Principal Component Analysis (PCA). The principle is essentially the same, performing SVD on a matrix full of data points, with the difference being that the model is not assumed to be dynamic.
% %
% PCA theory considers that a transformation with the $k$ largest singular vectors carries the most variance information of the data, and the rest can be discarded.

% Consider the following linear transformations:
% \begin{equation}
%  \left \{ \begin{aligned}
%     \boldsymbol{\phi}(\Theta[k]) = \mathbf{T}_{px}\mathbf{p_x}[k]\\
%               %%%%%
%     \boldsymbol{\psi}(\Theta[k]) = \mathbf{T}_{pu}\mathbf{p_u}[k]
% \end{aligned} \right .
% \end{equation}
% where $\mathbf{T}_{pca}$ is a truncation of the left singular vectors of $\mathbf{P_x}$ corresponding to the $k$ largest singular values.
%     Then, the system would be expressed as (in full order):
% %(\mathbf{I} \otimes \mathbf{T})
% \begin{equation}
%  \mathbf{x}[k+1] = \mathbf{W_A}\left(\mathbf{T}_{px}\mathbf{p_x}[k] \otimes \mathbf{x}[k]\right)    +\mathbf{W_B}\left(\mathbf{T}_{pu}\mathbf{p_u}[k] \otimes \mathbf{u}[k]\right)   
% \end{equation}


% By employing the mixed product property of the Kronecker Product, the system assumes the following form:
%      %%%
% \begin{equation}
%  \mathbf{x}[k+1] = \mathbf{\overline{W}_A}\left(\mathbf{p_x}[k] \otimes \mathbf{x}[k]\right) + \mathbf{\overline{W}_B}\left(\mathbf{p_u}[k] \otimes \mathbf{u}[k]\right)   
% \end{equation}
% where:
% \begin{equation}
%  \left \{ 
%  \begin{aligned}
%   \mathbf{\overline{W}_A} &=\mathbf{W_A}\left(\mathbf{T}_{px} \otimes \mathbf{I}\right) \\
%     %%%
%   \mathbf{\overline{W}_B} &=\mathbf{W_B}\left(\mathbf{T}_{pu} \otimes \mathbf{I}\right) 
% \end{aligned}  \right .
% \end{equation}

% Notice that the number of elements in $\mathbf{\overline{W}_A}$ and $\mathbf{\overline{W}_B}$ is quite smaller than their full-order counterparts.
% %
% Hence it is more interesting to solve the LPV-DMD algorithm for  $\mathbf{p_x}$ and $\mathbf{p_u}$ instead.
% %
% Although the model would consider these two variables as the parameter, it is pretty easy to compute them in terms of prediction since both linear transformations are obtained from SVD:
%    %%%
% \begin{equation}
%  \left \{ 
%  \begin{aligned}
%     \mathbf{p_x}[k] &= \mathbf{T}_{px}^T\boldsymbol{\phi}(\Theta[k])\\
%          %%%%
%     \mathbf{p_u}[k] &= \mathbf{T}_{pu}^T\boldsymbol{\psi}(\Theta[k])
%   \mathbf{\overline{W}_B} &=\mathbf{W_B}\left(\mathbf{T}_{pu} \otimes \mathbf{I}\right) 
% \end{aligned}  \right .
% \end{equation}

\end{comment}

\subsection{Local and Global Identification Procedures}

To summarize the discussion in this section, the global identification of a DMD-LPV model for a given system is summarized in Algorithm \ref{alg:global:LPV}.



\begin{algorithm}
  \caption{Global DMD-LPV Identification\label{alg:global:LPV}}
  \begin{algorithmic}[1] 
    \Inputs{State data $\mathbf{X}$, Input data $\mathbf{U}$, Parameter data $\mathbf{P}$, Output data $\mathbf{Y}$ (state forwarded 1t time step), Procrustes rank $r_{pr}$, POD rank $r_{pod}$, Regularization ($\lambda$)}
    \Initialize{Obtain $\mathbf{P_x}$ and $\mathbf{P_u}$ from $\mathbf{P}$ according to Eqn. \eqref{eq:def:Px-Pu}}, 
    \For{k = 1 to N}
    \State $\mathbf{X_p}[:,k] \gets \mathbf{P_x}[:,k] \otimes \mathbf{X}[:,k]$
    \State $\mathbf{U_p}[:,k] \gets \mathbf{P_u}[:,k] \otimes \mathbf{U}[:,k]$   
    \EndFor
    \State $\mathbf{INPUT_{svd}} \gets $ Vertical concatenation of $\mathbf{X_p}$ and $\mathbf{U_p}$.
    
    \State $\mathbf{W_r},\mathbf{s_r},\mathbf{V_r} \gets svd(\mathbf{INPUT_{svd}})$. SVD is truncated to rank $r_{pr}$.
    
    \State $\Sigma_r^{\mathbf{r}} \gets$ perform the operation: $\mathbf{s_r} \oslash( \mathbf{s_r}\odot \mathbf{s_r} + \lambda^2\mathbf{1})$. ($\odot$: element-wise product, $\oslash$: element-wise division.)
    
    \State $\mathbf{W_{yr}} \gets$ left-singular vectors of $svd(\mathbf{Y})$ (truncated to rank $r_{pod}$).
    \State $\mathbf{W_{r,1}} \gets $ First rows of $\mathbf{W_r}$ corresponding to $n_s \times n_f$. (Number of states times number of parameter features in state computation).
    \State $\mathbf{W_{r,2}} \gets $ Rows of $\mathbf{W_r}$ that are not in $\mathbf{W_{r,1}}$
    \State Compute weights with Eqn. \eqref{eqn:dmdc_lpv}.
  \end{algorithmic}
\end{algorithm}


The solution of the least squares problem would require finding the pseudo-inverse of a matrix with $n_f \times n_s$ features to compute the weights of the LPV model for $n_s$ outputs, with a global identification method.
%
The assumption for a large-scale LPV system is that $n_s$ is a huge number, and multiplying it by $n_f$ might make the LPV problem very difficult to solve.
%
This is pertinent for systems with an $n_s$ dimension as a function of $n_f$.
%
A quick assessment of the algorithm reveals that
the rank truncation reduces the problem to compute a $r_{pr}-$rank approximation for the pseudo-inverse, which is considerably cheaper than the alternative.
%
Also, the POD reduction leads to the calculation of fewer weights.


In turn, the local identification of a DMD-LPV system follows Algorithm 2.


\begin{algorithm}
  \caption{Local DMD-LPV Identification \label{alg:local:LPV}}
  \begin{algorithmic}[1] 
    \Inputs{An arbitrary number of values for $\mathbf{p}$. For each value $\mathbf{p}_i$, a corresponding state matrix $\mathbf{X}_i$, input matrix $\mathbf{U}_i$ and output-state matrix $\mathbf{Y}_i$, with each matrix obtained by exciting the LPV frozen in value $\mathbf{p}_i$ with $\mathbf{U}_i$. Rank $r$.}
    \Initialize{$n_{vp} \gets $ number of values configuration used for $\mathbf{p}$.\\ $\mathbf{Y_{tot}} \gets$ Concatenation of all output matrices $\mathbf{Y}_i$.}
    \State $\mathbf{W_{yr}} \gets$ left-singular vector of $svd(\mathbf{Y_{tot}})$.
    \For{$i = 1$ to $n_{vp}$}
    \State Identify discrete LTI system $\mathbf{x}[k+1] = \mathbf{A}(i)\mathbf{x}[k] + \mathbf{B}(i)\mathbf{u}[k]$ with fixed $\mathbf{p}$ and corresponding $\mathbf{X}_i,\mathbf{U}_i$ and $\mathbf{Y}_i$ (Least Squares or Procrustes problem).
    \State Store matrices $\mathbf{A}(i)$ and $\mathbf{B}(i)$ of LTI system.
    \EndFor
    \State $\mathbf{\widetilde{W}_A},\mathbf{\widetilde{W}_B} \gets$ Find reduced weights by solving the $r$-rank Procrustes problem for the system $\eqref{eqn:local_dmdlpv}$, with every parameter as input and every corresponding LTI weight as output (reduced by the POD transformation).
  \end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:local:LPV} has the issue of identifying the LTI systems on the original dimension, which defeats the purpose of the DMD non-intrusive MOR.
%
As an alternative, we propose using the POD identification from $\mathbf{Y}_{tot}$ to obtain a latent space and identify the LTI systems in it, which is formalized in Algorithm \ref{alg:local:LPV:B}.


\begin{algorithm}
  \caption{Local Non-Intrusive DMD-LPV Identification \label{alg:local:LPV:B}}
  \begin{algorithmic}[1] 
    \Inputs{An arbitrary number of values for $\mathbf{p}$. For each value $\mathbf{p}_i$, a corresponding state matrix $\mathbf{X}_i$, input matrix $\mathbf{U}_i$ and output-state matrix $\mathbf{Y}_i$, with each matrix obtained by exciting the LPV frozen in value $\mathbf{p}_i$ with $\mathbf{U}_i$. Rank $r$.}
    \Initialize{$n_{vp} \gets $ number of values configuration used for $\mathbf{p}$.\\ $\mathbf{Y_{tot}} \gets$ Concatenation of all output matrices $\mathbf{Y}_i$.}
    \State $\mathbf{W_{yr}} \gets$ left-singular vectors of $svd(\mathbf{Y_{tot}})$ ($r$-rank truncation).
    \For{$i = 1$ to $n_{vp}$}
    \State Identify discrete LTI system $\mathbf{x}[k+1]=\mathbf{\widetilde{A}}(i)\mathbf{x}[k] + \mathbf{\widetilde{B}}(i)\mathbf{u}[k]$ with fixed $\mathbf{p}$ and corresponding $\mathbf{W_{yr}}^T\mathbf{X}_i,\mathbf{U}_i$ and $\mathbf{W_{yr}}^T\mathbf{Y}_i$ (Least Squares or Procrustes problem).
    \State Get matrices $\mathbf{\widetilde{A}}(i)$ and $\mathbf{\widetilde{B}}(i)$ of LTI system.
    \EndFor
    \State $\mathbf{\widetilde{W}_A},\mathbf{\widetilde{W}_B} \gets$ Find reduced weights by solving the $r$-rank Procrustes problem for the system:
    \begin{equation}
   \left \{
\begin{aligned} 
     \mathbf{\widetilde{A}}(i) &=\widetilde{\mathbf{W}}_{\mathbf{B}}(\phi(\boldsymbol{\Theta}_i) \otimes \mathbf{I_s})\\
          %%%
     \mathbf{\widetilde{B}}(i) &=\widetilde{\mathbf{W}}_{\mathbf{B}}(\psi(\boldsymbol{\Theta}_i) \otimes \mathbf{I_{in}})
\end{aligned},
 \right .
\end{equation}
    with every parameter as input and every corresponding LTI weight as output (reduced by the POD transformation).
  \end{algorithmic}
\end{algorithm}



With this procedure, all training is performed in the reduced space, reducing overhead from training.
%
Both algorithms will be experimented to show that both have very close performance in terms of one-step-prediction, showing that we can perform the local identification in the reduced order space.
%
Since we assumed $n_s$ to be a large number, performing multiple least-squares identification of LTI systems in the case of a full-model LPV identification might be costly.
%
If ones uses Algorithm \ref{alg:local:LPV:B}, then it is possible to obtain a model by training multiple $r-$sized LTI systems.
%
Both Algorithm \ref{alg:local:LPV} and \ref{alg:local:LPV:B} solve the $r-$rank Procrustes problem over an $r \times r$ and a $r \times n_u$ output matrices (same as solving for $r^2$ and $rn_u$ outputs), and $2rn_p$ + $2rn_u$ inputs.
%
The problem itself is already presented in the latent space, and by performing a $r-$rank Procrustes Approximation, one just has to solve an $r-$truncated SVD instead of the full SVD to find the pseudo-inverse, and thus the model.


\begin{comment}
% \subsection{Solving the Kronecker Product Overload: Feature Selection}

% Notice that, if the system has a large number of parameters and states, the model would have a number of features equal to the product between the number of states and the number of parameter functions.
% %
% The resulting number tends to be quite large and above the dimension of the original problem.
% %
% Fortunately such problem is not without ways to circumvent it.

% We assume that not all features resulting from the Kronecker product have influence on the output.
% %
% To evaluate how much influence a given feature has on the outputs, we employ the Pearson correlation coefficient:
% \begin{equation}
%     r = \frac{\sum_{k=0}^N(x[k] - \overline{x})((y[k] - \overline{y}))}{\sum_{k=0}^N(x[k] - \overline{x})^2\sum_{k=0}^N(y[k] - \overline{y})^2}
% \end{equation}
% for a simulation of $N$ time steps, feature $x$, which is always a product between a given parameter and a given state, and output $y$, which is always a state at the next time step.
% %
% Such procedure is referred to as feature selection in statistcs and machine learning literature \cite{Kuzudisli2023}.

% For the context of this work, it is interesting to find features that have no impact at all on the output, so that they can removed from the DMD-LPV algorithm computation, reducing the computational overhead.
% %
% Therefore, every feature that has a low correlation coefficient value for every output is removed from the formulation.
% %
% This mapping between the full features vector and the reduced feature space can be represented by permutation matrix $\mathbf{P}$, which is the identity matrix, but with the rows corresponding to the non-influential features removed.

% Thus, the identification model in full order becomes: 

% \begin{equation} \label{eq:LPV:Kronecker}
%  \mathbf{x}[k+1] = \mathbf{W_A}\mathbf{P_x}\Bigl (\boldsymbol{\phi}(\Theta[k]) \otimes \mathbf{x}[k]\Bigr)    + \mathbf{W_B}\mathbf{P_u}\Bigl (\boldsymbol{\psi}(\Theta[k]) \otimes \mathbf{u}[k]\Bigr )
%  \end{equation}

%  Instead of solving the Procrustes problem considering $\Bigl (\boldsymbol{\phi}(\Theta[k]) \otimes \mathbf{x}[k]\Bigr)$ and $\Bigl (\boldsymbol{\psi}(\Theta[k]) \otimes \mathbf{u}[k]\Bigr )$ as the input to perform SVD on, we perform it on $\mathbf{P_x}\Bigl (\boldsymbol{\phi}(\Theta[k]) \otimes \mathbf{x}[k]\Bigr)$ and $\mathbf{P_u}\Bigl (\boldsymbol{\psi}(\Theta[k]) \otimes \mathbf{u}[k]\Bigr )$

\end{comment}


\begin{comment}

% \section{Modeling of Oil and Water Reservoirs} \label{sec:reservoir}

% To discuss physical aspects of a petroleum reservoir, we consider a two-phase reservoir model, since a one-phase reservoir is linear \cite{Jansen2013}, and a two-phase reservoir, while still simple, has enough nonlinearities to be of relevance to the discussion at hand.

% The partial differential equations that govern a two-phase reservoir is \cite{Jansen2013}:
% \begin{align}
%  \alpha \left[ \phi(\mathbf{x}) S_w(c_w + c_r)\partial_tp + \phi(\mathbf{x}) \partial_tS_w \right] &= \alpha q + \frac{\alpha}{\mu_w}\nabla \cdot \left(kk_{rw}\nabla p\right)\\
%  \alpha \left[ \phi(\mathbf{x})(1- S_w)(c_o + c_r)\partial_tp - \phi(\mathbf{x}) \partial_tS_w \right] &= \alpha q + \frac{\alpha}{\mu_o}\nabla \cdot \left(kk_{ro}\nabla p\right)    
% \end{align}
% where $\phi(\mathbf{x})$ is the porosity reservoir (and $\mathbf{x}$ is the vector space corresponding each physical dimension of the reservoir), $k(\mathbf{x})$ is the absolute permeability, $k_{rw}(S_w),k_{rw}(1-S_w)$ are the permeabilities of the water and oil phases, respectively, and are functions of the water saturation $S_w(\mathbf{x},t)$, which is a system state alongside the pressure $p(\mathbf{x},t)$.
% %
% The $q(\mathbf{x},t)$ function represents the external flow going into our out the reservoir system (namely oil produciton and water injection wells).
% %
% Each phase compression coefficient $c_o,c_w$ and $c_r$ is considered constant for the purposes of this work, as they are fluid-dependant and has a weak dependence on pressure \cite{Jansen2013}.
% %
% The variable $\alpha$ is a constant which property depends on the number of dimensions.
% %
% If one-dimensional, $\alpha$ is the reservoir area.
% %
% If two-dimensional, $\alpha$ is the reservoir height.
% %
% If three-dimensional, $\alpha = 1$.

% The relative permeabilities are calculated through the Corey model, which has a quadratic shape:
% \begin{align}
%     k_{rw} &= c_1(S_W^2 - 2c_2S_W + c_3)\\
%     k_{ro} &= c_4(S_W^2 + 2c_5S_W + c_6)
% \end{align}
% with $c_i$, $I = \{1,2,3,4,5,6\}$ as generic constants.

% When applying finite differences to the PDE associated with a two-phase reservoir, the result, in terms of state equations, is shaped as follows \cite{Jansen2013}:

% \begin{equation}
%     \begin{pmatrix}
%         \mathbf{V_{wp}}(\mathbf{s}) & \mathbf{V_{ws}}\\
%         \mathbf{V_{op}}(\mathbf{s}) & \mathbf{V_{os}}
%     \end{pmatrix}
%     \begin{pmatrix}
%         \mathbf{\dot{p}}\\
%         \mathbf{\dot{s}}
%     \end{pmatrix} =
%     \begin{pmatrix}
%         \mathbf{T}_w(\mathbf{s})\\
%         \mathbf{T}_o(\mathbf{s})
%     \end{pmatrix}\mathbf{p} +
%     \begin{pmatrix}
%         \mathbf{F_w}(\mathbf{s})\\
%         \mathbf{F_o}(\mathbf{s})
%     \end{pmatrix}\mathbf{q_l}
% \end{equation}

% In this discretization, each element of a matrix represents a corresponding grid-block assigned to it.
% %
% For instance, the pressure vector $\mathbf{p}$ is organized as follows in a two-dimensional reservoir:
% \begin{equation}
% \mathbf{p} = 
% \begin{pmatrix}
% \vdots\\
%     p_{i,j-1}\\
%     p_{i-1,j}\\
%     p_{i,j}\\
%     p_{i+1,j}\\
%     p_{i,j+1}\\
%     \vdots
% \end{pmatrix}
% \end{equation}
% %
% where,
% The matrices $\mathbf{V_{ws}}$ and $\mathbf{V_{os}}$ are diagonal matrices containing the value for $\phi_{i,j}$, which is the porosity of a given grid block associated with the saturation $\mathbf{s}_{i,j}$.
% %
% The matrices $\mathbf{V_{wp}}$ and $\mathbf{V_{op}}$ are diagonal matrices with each element having the following form:
% \begin{align}
%     \mathbf{V_{wp}}_{i,j} = V(c_w + c_r)\phi_{i,j}\mathbf{s}_{i,j}\\
%     \mathbf{V_{op}}_{i,j} = V(c_o + c_r)\phi_{i,j}(1- \mathbf{s}_{i,j})
% \end{align}
% notice that in this case, $i$ and $j$ correspond to the grid block assigned to the element of $\mathbf{s}$, not the matrix position.
% %
% The non-diagonal elements of the matrices in this context mean a connection between two grid blocks, and the $\mathbf{V}$ matrices represent properties of a single grid block.
% %
% A more compact representation would be:
% \begin{equation}
%     \mathbf{V}(\mathbf{s})\begin{pmatrix}
%         \mathbf{\dot{p}}\\
%         \mathbf{\dot{s}}
%     \end{pmatrix}
%     = \mathbf{T}(\mathbf{s})\mathbf{p} + \mathbf{F}(\mathbf{s})\mathbf{q}
% \end{equation}

% Matrices such as $\mathbf{T_w}(\mathbf{s})$ and $\mathbf{T_o}(\mathbf{s})$ are referred to as transmissibility matrices, and they are either tri-diagonal (one dimensional) penta-diagonal (two-dimensional) or hepta-diagonal (tridimensional), since in a system derived from PDEs such as the oil reservoir, each grid block is only connected to its adjancent counterparts.
% %
% The effect of a grid block pressure on an adjancent one is called the transmissibility term, and is governed by the following equations:
% \begin{align}
%     (T_w)_{i-1:i,j} = r\frac{h}{\mu_w}(k_{i,j}k_{rw}(\mathbf{s}_{i,j},\mathbf{s}_{i-1,j})_{i-1:i,j}\\
%     (T_o)_{i-1:i,j} = r\frac{h}{\mu_o}(k_{i,j}k_{ro}(\mathbf{s}_{i,j},\mathbf{s}_{i-1,j})_{i-1:i,j}
% \end{align}
% where $:$ has the same meaning as the MATLAB slicing operator.
% %
% We are assuming a cartesian two-dimensional grid in this case, that connects row $i-1$ to row $i$ in collumn $j$.
% %
% The analogue would be a connection between two grid blocks in a collumn, $(T_W)_{i-1:i,j}$.
% %
% Since the relative permeability considered is between two grid-blocks, $k_{kw}$ and $k_{ko}$ depend on the two adjancent water saturation for each grid block.

% Matrix $\mathbf{T_k}$, $k \in \{w,o\}$ is actually composed by rows with non-zero elements as:

% \begin{equation}
% \begin{pmatrix}
%     {T_k,}_{i,j-1:j} & {T_k,}_{i-1:1,j} &-({T_k,}_{i,j-1:j} + {T_k,}_{i-1:1,j} + {T_k,}_{i:i+1,j} + {T_k,}_{i,j:j+1}) & {T_k,}_{i:i+1,j} & {T_k,}_{i,j:j+1}
% \end{pmatrix}
% \begin{pmatrix}
%     p_{i,j-1}\\
%     p_{i-1,j}\\
%     p_{i,j}\\
%     p_{i+1,j}\\
%     p_{i,j+1}
% \end{pmatrix}    
% \end{equation}
% this internal product correponds to a component of the multiplication $\mathbf{T_i}\mathbf{p}$ related to grid block $i,j$. 
% %
% When placed together, it is easy to see that $\mathbf{T_i}$ is penta-diagonal, as each grid block only affects a set of four others in a two dimensional space.
% %
% The rows related to grid blocks on the corner of the grid must be defined differently, according to the given boundary conditions of the simulation.

% The vector $\mathbf{q}$ corresponds to the flow of each well connected to the reservoir, therevor $\mathbf{F}(\mathbf{s})$ is a sparse matrix, with nonzero elements only in grid blocks connected to a well.
% %
% Each element of $\mathbf{F}$ is defined as follows:

% \begin{align}
%     f_w = \frac{k k_{rw}/\mu_w}{k k_{rw}/\mu_w + k k_{ro}/\mu_o}\\
%     f_o = \frac{k k_{ro}/\mu_o}{k k_{rw}/\mu_w + k k_{ro}/\mu_o}
% \end{align}
% which is a rational quadratic function on the water saturation, if considering the Corey model.

% If we assume the fluids to be incompressible, it is not possible to solve this nonlinear state equation explicitly.
% %
% In case the fluid is compressible, the matrix multiplying the time derivative terms is composed of four diagonal blocks, which are trivial to invert, since the inversion of a diagonal matrix is merely the inversion of each element.
% %
% Also, the explicit formulation for this system is: 

% \begin{equation} \label{eqn:fin_diff_explicit}
%     \begin{pmatrix}
%         \mathbf{\dot{p}}\\
%         \mathbf{\dot{s}}
%     \end{pmatrix}
%     = \mathbf{V}(\mathbf{s})^{-1}\mathbf{T}(\mathbf{s})\mathbf{p} + \mathbf{V}(\mathbf{s})^{-1}\mathbf{F}(\mathbf{s})\mathbf{q}
% \end{equation}

% The inversion of a square $2 \times 2$ block matrix such as $\mathbf{V}(\mathbf{s})$ can be calculated as:

% \begin{align}
%     \mathbf{V}(\mathbf{s})^{-1} &= \begin{pmatrix}
%         \mathbf{A} & \mathbf{B}\\
%         \mathbf{C} & \mathbf{D}
%     \end{pmatrix}\\
%     \mathbf{A} &= \mathbf{V_{wp}}^{-1} + \mathbf{V_{wp}}^{-1}\mathbf{V_{ws}}(\mathbf{V_{os}} - \mathbf{V_{op}}\mathbf{V_{wp}}\mathbf{V_{ws}})^{-1} \mathbf{V_{op}}\mathbf{V_{wp}}^{-1}\\ 
%     \mathbf{B} &= - \mathbf{V_{wp}}^{-1}\mathbf{V_{ws}}(\mathbf{V_{os}} - \mathbf{V_{op}}\mathbf{V_{wp}}\mathbf{V_{ws}})^{-1}\\
%     \mathbf{C} &= - (\mathbf{V_{os}} - \mathbf{V_{op}}\mathbf{V_{wp}}\mathbf{V_{ws}})^{-1}\mathbf{V_{op}}\mathbf{V_{wp}}^{-1}\\
%     \mathbf{D} &= (\mathbf{V_{os}} - \mathbf{V_{op}}\mathbf{V_{wp}}^{-1}\mathbf{V_{ws}})^{-1}
% \end{align}

% do notice that matrices $\mathbf{A.B.C}$ and $\mathbf{D}$ are also diagonal matrices.

% The diagonal element of each submatrix of $\mathbf{V}^{-1}$ corresponding to grid-block $i,j$ is:

% \begin{align}
%     A_{i,j} &= \frac{1}{c\phi_{ij}(2s_{ij} - 1)}\\
%     B_{i,j} &= -\frac{1}{c\phi_{ij}(2s_{ij} - 1)}\\
%     C_{i,j} &= -\frac{1 - s_{ij}}{c\phi_{ij}(2s_{ij} - 1)}\\
%     D_{i,j} &= \frac{s_{ij}}{c\phi_{ij}(2s_{ij} - 1)}
% \end{align}
% which are all one-degree rational function.

% Another form of expressing Eq. \eqref{eqn:fin_diff_explicit} is:

% \begin{equation}
% \begin{pmatrix}
%         \mathbf{\dot{p}}\\
%         \mathbf{\dot{s}}
%     \end{pmatrix}
%     = \begin{pmatrix}
%         \mathbf{A}\mathbf{T_w} + \mathbf{B}\mathbf{T_o}\\
%         \mathbf{C}\mathbf{T_w} + \mathbf{D}\mathbf{T_o}
%     \end{pmatrix}\mathbf{p} + \begin{pmatrix}
%         \mathbf{A}\mathbf{F_w} + \mathbf{B}\mathbf{F_o}\\
%         \mathbf{C}\mathbf{F_w} + \mathbf{D}\mathbf{F_o}
%     \end{pmatrix}\mathbf{q}
% \end{equation}

% We know that pre-multiplying a diagonal matrix means multipying each row of the second matrix by the corresponding element of the first.
% %
% This whole discussion means that the nonlinearities that descibe a two-phase reservoir are rational functions of the adjancent saturations in a grid block, while the model is linear both for the pressure states and the control.


% Since the reservoir model has a distributed structure, the model of a single grid block has the following form:

% \begin{equation}
% \begin{pmatrix}
% c\phi_{i,j}\mathbf{s}_{i,j} & c\phi_{i,j}\\
% c\phi_{i,j}(1 - \mathbf{s}_{i,j}) & c\phi_{i,j}
% \end{pmatrix}
% \begin{pmatrix}
%     \mathbf{\dot{p}}_{i,j}\\
%     \mathbf{\dot{s}}_{i,j}
% \end{pmatrix}
% =
% a(\mathbf{s}_{i,j},\mathbf{s}_{near})\mathbf{p_{i,j}} + \mathbf{B}(\mathbf{s}_{i,j},\mathbf{s}_{near})\mathbf{p_{near}} + f(\mathbf{s}_{i,j})q
% \end{equation}

% \color{red}
% \textbf{Remarks:}
% \begin{itemize}
% \item I suggest in Section 6, but will elaborate here. What about introducing an algorithm, may be two, or for global and another for local LPV identification? 

% \item It would useful and help consolidate the developments into an algorithm, or step-by-step procedure declaring the inputs, steps performed and output (product) of identification procedure.

% \item Such an algorithm would be applied in the example(s) in Section 6.
% \item Started a subsection in the previous section doing just that.
% \end{itemize}

\end{comment}
