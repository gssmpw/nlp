\subsection{Global Identification of a Quasi-LPV System: Nonlinear Diffusion}

We now study a case where we identify a quasi-LPV system.
%
Consider a nonlinear PDE similar to the previous diffusion case:
    %%%%
\begin{equation} \label{eqn:diffusion_pde}
  \frac{\partial T}{\partial t} = \frac{\partial }{\partial x}\left(k(T) \frac{\partial T}{\partial x}\right) - w\frac{\partial T}{\partial x} 
\end{equation}
where $T(x,t)$ is the unknown problem solution and the advection
velocity is $w = 0.1$.
%
The PDE \eqref{eqn:diffusion_pde} is referred to as the nonlinear diffusion equation, and just like in the previous case study system, $T(x,t)$ represents the temperature of a system at a given point in space and time.
%
The nonlinearity $k(T)$ is defined as:
\begin{equation}
    k(T) = a + T^b
\end{equation}

We consider the same spatial domain and boundaries as the other diffusion equation shown in this work.
  
{%\color{blue}
Theis nonlinear model is quite similar to the one used as case study in \cite{Huhn2023}, being the same PDE type with a few minor differences:
\begin{itemize}
    \item The domain is 1D.
    %
    Since our work focuses on obtaining an LPV model that can represent the system using DMDc as a model order reduction tool, the use of an 1D domain makes the discretized form \textcolor{blue}{simpler}.
    
    \item Parameters $a$ and $b$ are fixed at $(0.01,3)$. Instead, we treat the diffusion equation as a quasi-LPV system with parameters being the terms drawn from $k(T)$. This is done to showcase the properties of this identification approach in comparison to works such as \cite{Huhn2023}, since this means that the number of \textcolor{blue}{nonlinearities considered as} parameters is equal the number of states.
    %
    \item The Dirichlet boundary condition at $x = 0$ is variable and treated as an input to the system.
\end{itemize}
}
\color{blue}
Through the derivative product rule:

\begin{equation}
  \frac{\partial }{\partial x}\left(k(T) \frac{\partial T}{\partial x}\right)   = \left(\frac{\partial k(T)}{\partial T}\frac{\partial T}{\partial x}\right)\frac{\partial T}{\partial x} + k(T)\frac{\partial^2 T}{\partial x^2}
\end{equation}

\color{black}
equation \eqref{eqn:diffusion_pde} has an alternate representation by applying the product rule on the first right-hand term:
\begin{equation} \label{eqn:diffusion_pde2}
  \frac{\partial T}{\partial t} = \frac{d k(T) }{d T}\left( \frac{\partial T}{\partial x}\right)^2 + k(T)\frac{\partial^{2} T}{\partial x^2}  - w\frac{\partial T}{\partial x} 
\end{equation}
where
\begin{equation}
    \frac{dk(T)}{dT} = bT^{b-1}
\end{equation}

As in previous case, we must apply finite differences to the PDE, with the same $h-$spaced grid.
    %%%
This time, the nonlinear diffusion equation has the following form for any point of $x$ inside $\Omega$:
  %CENTRAL
\begin{equation} \label{eqn:diffusion_fdm1}
  \dot{T}_i = \frac{d k(T_i) }{d T_i}\frac{(T_{i+1} - T_{i-1})^2}{(2h)^2} + k(T_i)\frac{T_{i+1} - 2T_i + T_{i-1}}{h^2} - w\frac{T_{i+1} - T_{i-1}}{2h} 
\end{equation}


%FORWARD
% \begin{equation} \label{eqn:diffusion_fdm1}
%   \dot{T}_i = \frac{d k(T_i) }{d T_i}\frac{T_{i+1}^2 -2T_iT_{i+1} + T_i^2}{h^2} + k(T_i)\frac{T_{i+1} + T_{i-1} - 2T_i}{h^2} - w\frac{T_{i+1} - T_i}{h} 
% \end{equation}

Or, by vectorizing \ref{eqn:diffusion_fdm1}:
   %%%
\begin{multline} \label{eqn:diffusion_fdm2}
  \dot{T}_i =
  \left(
  \begin{bmatrix}
     1\\
     0\\
     -1
  \end{bmatrix}
  (T_{i+1} - T_{i-1})\frac{d k(T_i) }{d T_i}/(4h^2) +
  \begin{bmatrix}
     1\\
     0\\
     -1
  \end{bmatrix}(- w/(2h)) + \right . \\
      %%%%%
      \left . +
  \begin{bmatrix}
     1\\
     -2\\
     1
  \end{bmatrix}k(T_i)/h^2\right)^T
  \begin{bmatrix}
      T_{i+1}\\
      T_i\\
      T_{i-1}
  \end{bmatrix}
\end{multline}



%FORWARD VECTORIZED
% \begin{equation} \label{eqn:diffusion_fdm2}
%   \dot{T}_i =
%   \begin{bmatrix}
%      \frac{d k(T_i) }{d T_i}(T_{i+1} - T_i)/h^2 + k(T_i)/h^2 -w/h\\
%      \frac{d k(T_i) }{d T_i}(T_i - T_{i+1})/h^2 -2k(T_i)/h^2 + w/h\\
%      k(T_i)/h^2
%   \end{bmatrix}^T
%   \begin{bmatrix}
%       T_{i+1}\\
%       T_i\\
%       T_{i-1}
%   \end{bmatrix}
% \end{equation}


% By defining:
% \begin{align}
%     f(T_i,T_{i+1}) &= \frac{d k(T_i) }{d T_i}(T_{i+1} - T_i)%\\
%     %g(T_i) &= k(T_i) - k_0
% \end{align}
% %with $k_0$ being $k(T_i = 0)$, 
% we can represent the equation as:

% \begin{equation} \label{eqn:diffusion_fdm2}
%   \dot{T}_i =
%   \left(
%   \begin{bmatrix}
%      1\\
%      -1\\
%      0
%   \end{bmatrix}\frac{f(T_i,T_{i+1}) - wh}{h^2} +
%   \begin{bmatrix}
%      1\\
%      -2\\
%      1
%   \end{bmatrix}k(T_i)/h^2\right)^T
%   \begin{bmatrix}
%       T_{i+1}\\
%       T_i\\
%       T_{i-1}
%   \end{bmatrix}
% \end{equation}


%Central vectorized
% \begin{equation} \label{eqn:diffusion_fdm2}
%   \dot{T}_i =
%   \begin{bmatrix}
%      \frac{d k(T_i) }{d T_i}(T_{i+1} - T_{i-1})/4h^2 + k(T_i)/h^2-w/2h\\
%      -2k(T_i)/h^2 \\
%      \frac{d k(T_i) }{d T_i}(T_{i-1} - T_{i+1})/4h^2 + k(T_i)/h^2+w/2h
%   \end{bmatrix}^T
%   \begin{bmatrix}
%       T_{i+1}\\
%       T_i\\
%       T_{i-1}
%   \end{bmatrix}
% \end{equation}


% By defining:
% \begin{align}
%     f(T_i,T_{i+1},T_{i-1}) &= \frac{d k(T_i) }{d T_i}(T_{i+1} - T_{i-1})%\\
%     %g(T_i) &= k(T_i) - k_0
% \end{align}
% %with $k_0$ being $k(T_i = 0)$, 
% we can represent the equation as:

% \begin{equation} \label{eqn:diffusion_fdm2}
%   \dot{T}_i =
%   \left(
%   \begin{bmatrix}
%      1\\
%      0\\
%      -1
%   \end{bmatrix}\frac{f(T_i,T_{i+1},T_{i-1}) - 2hw}{4h^2} +
%   \begin{bmatrix}
%      1\\
%      -2\\
%      1
%   \end{bmatrix}k(T_i)/h^2\right)^T
%   \begin{bmatrix}
%       T_{i+1}\\
%       T_i\\
%       T_{i-1}
%   \end{bmatrix}
% \end{equation}


Since there is no information on $T_{-1}$, the boundary condition on $T_0$ must be obtained through forward difference.
%
This implies the Neumann boundary condition at $T(0,t)$, imposed by holding the following relation on the heat source $u(t)$:
\begin{equation}
    T_0 = u(t)
\end{equation}
Therefore:

\begin{multline} \label{eqn:fdm_boundary1}
  \dot{T}_1 =
  \left(
  \begin{bmatrix}
     1\\
     0
  \end{bmatrix}
  T_2\frac{d k(T_1) }{d T_1}/(4h^2) +
  \begin{bmatrix}
     1\\
     0
  \end{bmatrix}(- w/(2h)) +
  \begin{bmatrix}
     1\\
     -2
  \end{bmatrix}k(T_1)/h^2\right)^T
  \begin{bmatrix}
      T_2\\
      T_1
  \end{bmatrix}\\
  + \left (k(T_1)/h^2 + w/2h + ( u(t) - 2T_2)\frac{d k(T_1) }{d T_1}/(4h^2)\right )u(t)
\end{multline}



%FORWARD

% \begin{equation} \label{eqn:diffusion_fdm2}
%   \dot{T}_1 = 
%   \begin{bmatrix}
%      1\\
%      -1
%   \end{bmatrix}^T\left(f(T_1,T_2)/h^2 + k(T_1)/h^2 - w/h\right)
%   \begin{bmatrix}
%       T_2\\
%       T_1
%   \end{bmatrix}
%   - k(T_1)u(t)
% \end{equation}


%CENTRAL
% \begin{multline}
%     \label{eqn:fdm_boundary1}
%   \dot{T}_1 =
%   \begin{bmatrix}
%      \frac{d k(T_1) }{d T_1}(T_2 - T_1 + hu(t))/4h^2 + k(T_1)/h^2-w\\
%      \frac{d k(T_1) }{d T_1}(T_1 - hu(t) - T_2)/4h^2 - k(T_1)/h^2+w
%   \end{bmatrix}^T
%   \begin{bmatrix}
%       T_2\\
%       T_1
%   \end{bmatrix}\\
%   +(\frac{d k(T_1) }{d T_1}(T_2 - T_1)/4h^2 + u(t)/4h - k(T_1)/h^2 -w)hu(t)
% \end{multline} 
%CENTRAL2
% \begin{multline} \label{eqn:fdm_boundary1}
%   \dot{T}_1 =
%   \left(
%   \begin{bmatrix}
%      1\\
%      -1
%   \end{bmatrix}\frac{f_{fwd}(T_1,T_2) - 2hw}{4h^2} +
%   \begin{bmatrix}
%      1\\
%      -1
%   \end{bmatrix}k(T_i)/h^2\right)^T
%   \begin{bmatrix}
%       T_2\\
%       T_1
%   \end{bmatrix}\\
% + (\frac{f_{fwd}(T_1,T_2) - 2hw + \frac{d k(T_1) }{d T_1}}{4h} - k(T_i)/h)u(t)
% \end{multline}

% \begin{multline} \label{eqn:fdm_boundary1}
%   \dot{T}_1 =
%   \left(
%   \begin{bmatrix}
%      1\\
%      -1
%   \end{bmatrix}
%   (T_2 - T_1)\frac{\frac{d k(T_1) }{d T_1}}{4h^2} +
%   \begin{bmatrix}
%      1\\
%      -1
%   \end{bmatrix}\frac{- w}{2h} +
%   \begin{bmatrix}
%      1\\
%      -1
%   \end{bmatrix}\frac{k(T_1)}{h^2}\right)^T
%   \begin{bmatrix}
%       T_2\\
%       T_1
%   \end{bmatrix}\\
% + (\frac{- 2hw + \frac{d k(T_1) }{d T_1}(2(T_2 - T_1) + hu(t))}{4h} - \frac{k(T_1)}{h})u(t)
% \end{multline}

% where
% \begin{align}
%     f_{fwd}(T_1,T_2) &= \frac{d k(T_i) }{d T_i}(T_2 - T_1)%\\
%     %g(T_i) &= k(T_i) - k_0
% \end{align}

{\color{blue}The second boundary condition implies that $T_{N} = T_{N+1}$, where $T_{N+1}$ corresponds to the point where $x = 1$ and $T_{N}$ is its neighbor inside the grid.}
%
The state $T_N$ is described as:
  %CENTRAL
\begin{equation} \label{eqn:fdm_boundary2}
  \dot{T}_N =
  \left(\begin{bmatrix}
     1\\
     -1
  \end{bmatrix}\left((T_N - T_{N-1})\frac{d k(T_N) }{d T_N}
     / (4h^2) - k(T_N)/h^2-w/(2h)\right)
     \right)^T
  \begin{bmatrix}
      T_N\\
      T_{N-1}
  \end{bmatrix}
\end{equation}


%FORWARD
% \begin{equation} \label{eqn:fdm_boundary2}
%   \dot{T}_N =
%   \begin{bmatrix}
%      -1\\
%      1
%   \end{bmatrix}^T(k(T_N))/h^2
%   \begin{bmatrix}
%       T_N\\
%       T_{N-1}
%   \end{bmatrix}
% \end{equation}


% where 

% \begin{align}
%     f_{bd}(T_N,T_{N-1}) &= \frac{d k(T_N) }{d T_N}(T_N - T_{N-1})%\\
%     %g(T_i) &= k(T_i) - k_0
% \end{align}

The complete system is then represented by the following quasi-LPV system:
     %%%
\begin{equation}
    \mathbf{\dot{T}} = \bigl (\mathbf{A_0} +\mathbf{A}(\mathbf{T})\bigr)\mathbf{T} + \bigl(\mathbf{B_0}+ \mathbf{B}(T_1,T_2,u(t))\bigr)u(t)
\end{equation}
  %%%
where $\mathbf{T}$ is the vector of states along the $1D$ domain, indexed according to equation \eqref{eqn:state_indexing}, and:
   %%%
\begin{footnotesize}
\begin{align}
 \mathbf{A_0} &=
 -(w/2h)\mathbf{D_1}\\
 \mathbf{A}(\mathbf{T}) &= (1/4h^2)
 \Bigl (\text{diag}\left((\mathbf{D_1}\mathbf{T})\odot \mathbf{dk}(\mathbf{T})\right)\mathbf{D_1} +4\text{diag}(\mathbf{k}(\mathbf{T}))\mathbf{D_2}\Bigr)\\
 \mathbf{B_0} &=(w/2h)
 \begin{bmatrix}
     1\\
     0\\
     0\\
     \vdots\\
     0
 \end{bmatrix} \\
 \mathbf{B}(T_1,T_2,u(t)) &=
 \begin{bmatrix}
     1\\
     0\\
     0\\
     \vdots\\
     0
 \end{bmatrix}\left (k(T_1)/h^2 + ( u(t) - 2T_2)\frac{d k(T_1) }{d T_1}/(4h^2)\right)\\
 \mathbf{D_1} &=
 \begin{bmatrix}
     0 & 1 & 0 & 0 & \hdots & 0\\
     -1 & 0 & 1 & 0 & \hdots & 0\\
     0 & -1 & 0 & 1 & \hdots & 0\\
     \vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
     0 & 0 & 0 & -1 & \hdots & 1
 \end{bmatrix}\\
 \mathbf{D_2} &=
 \begin{bmatrix}
     -2 & 1 & 0 & 0 & \hdots & 0\\
     1 & - 2 & 1 & 0 & \hdots & 0\\
     0 & 1 & - 2 & 1 & \hdots & 0\\
     \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
     0 & 0 & 0 & 0 & 1 & -1
 \end{bmatrix}
\end{align}
\end{footnotesize} 

Notice that matrix $\mathbf{D_1}$ comes directly from the discretization of the first-order derivative.
%
The portion of $\mathbf{A}(\mathbf{T})$ that depends on the diagonal of $\mathbf{D_1}\mathbf{T}$ \textcolor{blue}{is derived form the discretization of the squared gradient term.}

For initial condition, we consider $\mathbf{T} = 0$.

% \subsubsection{N-dimensional Cartesian Case}

% If the domain of the diffusion equation is n-dimensional, then the equation is expressed as follows:


% \begin{equation}
%     \frac{\partial T}{\partial t} = \frac{dk(T)}{dT}\|\nabla T\|^2 + k(T)\nabla \cdot \nabla T - \mathbf{w}\cdot \nabla T
% \end{equation}


% Applying finite differences on an internal element of the solution set, will lead to the following equation:


%PLACE THIS IN A FUTURE PAPER. THIS IS INTERESTING
% \subsubsection{Considerations}

% An important characteristic of a state space system discretized by PDE is that they tend to have a localized structure (in this case, matrices $\mathbf{D_1}$ and $\mathbf{D_2}$ are tridiagonal because the space domain is one dimensional, however these matrices could be penta-diagonal if the domain was 2-d or hepta-diagonal if the domain was 3-d. An 1-d domain was selected for simplicity.

% %
% As for the consequence of this locality, if one is to see the nonlinear diffusion as a quasi-LPV system, then each nonlinearity will only take place when placed together with its correspondent state.
% %
% In other words, instead of employing \eqref{eq:LPV:Kronecker} as a model, we could employ the following model:

% \begin{equation} \label{eq:diffusion_lpv}
%  \mathbf{x}[k+1] = \sum_{i=1}^{N_{nlf}}\mathbf{W_{A}}_{,i}\Bigl (\boldsymbol{\phi}_i(\mathbf{x}[k]) \odot \mathbf{x}[k]\Bigr)    + \sum_{i=1}^{N_{nlf,u}}\mathbf{W_B}_{,i}\Bigl (\boldsymbol{\psi}(\mathbf{x}[k]) \odot \mathbf{u}[k]\Bigr )
%  \end{equation}
%  where $N_{nlf}$ is he number of nonlinear functions and $N_{nlf}$ is the number of nonlinear functions related to the input.
%  %
%  In the case, of the nonlinear diffusion equation, the exact translation to this form would be:
% \begin{align}
%     \boldsymbol{\phi}_1(\mathbf{x}[k]) = \mathbf{1}\\
%     \boldsymbol{\phi}_2(\mathbf{x}[k]) = k(\mathbf{T}[k])\\
%     \boldsymbol{\phi}_2(\mathbf{x}[k])
% \end{align}

\subsubsection{Implementation, Simulation and Data Gathering}

To simulate the system, we set $h = 0.02$ so that there are $49$ states in the discretized system.
%
The discretization is implemented using Python and simulated in time using Runge Kutta (4th order) with $\delta t = 10^{-5}$ so that the dynamics of the system are thoroughly captured.
%
The sampling time for obtaining a data point was $T_s =  5 \times 10^{-5}$

To gather data for the nonlinear diffusion equation system, we consider that $\forall t, u(t) \in [0,4]$.
%
Then, we perform the APRBS excitation signal with a minimum step size of $10000$ time steps.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/diffusion_equation_dataset2.eps}
    \caption{Data-set employed for the diffusion equation system. The top plot shows the dynamics of the whole $49$ points measured for $T$, whereas the bottom plot depicts the excitation signal for $u(t)$. Note that, besides the plot being in seconds, this figure corresponds to $120000$ data points.}
    \label{fig:diff_equation_dataset}
\end{figure}

Figure \ref{fig:diff_equation_dataset} shows the plot of the data set obtained with the excitation signal.
%
This simulation shall be used as a training set, with wich we test different aspects of the LPV-DMD reduction.

\subsubsection{Structure Selection}

The focus of this work is to assess the properties of DMD-LPV in reducing the order of the full system.
%
For the structure of our identification model, we consider $2$ cases:

\begin{itemize}
    \item \textbf{Exact Structure}: The model and the identified system have the same structure. This means that the features utilized are: $\boldsymbol{\phi}_1(\mathbf{T}) = k(\mathbf{T})$ and $\boldsymbol{\phi}_2 = {\tt diag}(D_1\mathbf{T}) \odot dk(\mathbf{T})$, which totals $98$ features.
    %
    We also decide that only one $\psi(T_1,T_2,u) = k(T_1)/h^2 + (u - 2T_2)dk(T_2)$ is used in this case.
    %
    This makes the input space fully one-dimensional.
    \item \textbf{Parametric Difference}: We also consider the case where $p = 3.2$ instead of $3$, so that a parametric uncertainty is present and the model is not fully exact.
\end{itemize}

When the model has the same structure as the system, it is expected that, with data sufficiently well collected, the model will behave exactly as the system.
%
With the parameter being different, a bias error about the original system takes place.
%
These experiments aim to assess the performance of the reduced order identification, evaluating the decay of performance in solving the Procrustes problem and the POD reduction instead of the least squares.

Since the LPV model considered uses the Kronecker product, it has a feature space of $49\times 2\times 49 + 1 = 4802$, which is quite a large number when compared to the original $49$ states.
%
The workstations involved in this research could not perform the full order identification, but solving the Procrustes problem was feasible, as seen in Section \ref{subsec:procrustes}.
%

% The inclusion of parametric uncertainty transform the problem at hand into a more traditional system identification problem, treating the LPV-DMD as a possible learning model.

\color{blue}

\subsection{Parameter Features Reduction}

Before identifying the LPV system through DMD, we perform PCA on the parameter features related to the states.
%
This means applying SVD on a $98$ parameter functions data matrix, as there are $49$ states and two functions that map $n_s \mapsto n_s$ in the quasi-LPV system.
   We carry out such operation considering a fixed set of reduced dimensions and run the full least-squares algorithm on it.




%[0.5399376057883077, 0.4266604975718727, 0.005234525259975347, 2.89365632418811e-06, 8.186876429110066e-07, 8.185213764507288e-07, 8.17373305646361e-07]


%[0.5741992206161991, 0.4650572498157927, 0.005433716193298836, 7.795079914847267e-06, 8.61040858969931e-07, 8.669743078702819e-07, 8.68010431866666e-07]
\begin{table}[]
    \caption{Result of the parameter feature reduction experiment, measured by the mean squared error over the training data for different ranks for the parameter data matrix. The full rank least-squares problem was solved for all examples.}
    \label{tab:parred_result}
    \centering
    \begin{tabular}{|c|c|c|}
    \cline{2-3}
      \multicolumn{1}{c}{}  & \multicolumn{2}{|c|}{MSE (training)} \\\hline
      Parameter rank & $b = 3$ (model) & $b = 3.2$ (model) \\\hline
        $2$ & $0.540$ & $0.575$\\
        $5$ & $0.427$ & $0.465$\\
        $10$ & $0.00524$ & $0.00544$\\
        $15$ & $2.90 \times 10^{-6}$ & $7.79\times 10^{-6}$\\
        $20$ & $8.19\times 10^{-7}$ & $8.62\times 10^{-7}$\\
        $25$ & $8.19\times 10^{-7}$ & $8.67\times 10^{-7}$\\
        $30$ & $8.18\times 10^{-7}$ & $8.69\times 10^{-7}$ \\
        \hline
        %$600$ & $8.19\times 10^{-7}$ & \\
    \end{tabular}
    \end{table}
    
Table \ref{tab:parred_result} depicts the experiment's result on reducing the number of features.
%
We stopped at $30$ because the training error started showing diminishing returns at rank $20$.
%
Also, notice that the error for the parametric difference case increases when the rank is larger than $20$.
%
This might be a result of committing less error with the PCA approximation, compensating for guessing the parameter $b$ wrong.

Hence, the rest of the experiments will consider the reduction to $20$ features.
%
This means that the dimension of the Kronecker product will be $50\times 20 + 1 = 1001$, which is around $5$ times smaller than the original $4998$.

\color{black}

\subsubsection{Procrustes Problem} \label{subsec:procrustes}

The LPV-DMD identification problem involves a number of linear parameters $n_f$ (number of features) times larger than the number of states, which entails the SVD truncation for the least squares solution and for the POD reduction being at different ranks.
%
We now solve the Procrustes problem at different ranks without performing POD.
%
The Procrustes problem is solved for ranks $r \in \{50,100,150,200,250,300,350, 400, 450, 500, 550\}$, where further values for $r$ exceed the computational capacity available for this work.
%
This experiment is done to evaluate how much can we safely reduce the rank of the matrix without compromising the least squares solution for this problem.
%
Since the exact structure can potentially present a perfect copy of the real system, the metric for this evaluation is the mean squared error over the training set, so as to see how limiting the rank underfits an exact model for this case.

% Data for b = 3
% [0.4815208265111338, 0.022672396025059584, 0.0033285078876834767, 0.0009720131541647944, 0.00019736076667017049, 1.1599635330093489e-05, 1.9377161295508213e-06, 9.487832370588555e-07, 8.413439324692565e-07, 8.223283098522572e-07, 8.189236529879774e-07]


%Data for b = 3.2
% [0.11626899453028124, 0.09882161189463637, 0.09284131462021235, 0.08442850566906795, 0.06228769113719705, 0.046855300686213214, 0.04347247118281876, 0.03782991478101217, 0.033062998174721504, 0.028859314515275968, 0.025220601451338604]


%par_rank 20
%[0.5092791431127558, 0.44945512058151077, 0.07233174780703326, 0.0078088441744012275, 0.0016867501894932966, 0.0002418767648135462, 6.362205920013058e-05, 1.7296697068802147e-05, 6.053421745223061e-06, 2.6852028891244214e-06, 1.4201984268979695e-06]
%b = 3.2:
%[0.5505642526767065, 0.4878806879493821, 0.07339002620117237, 0.01050788182957436, 0.0030293461049014873, 0.00039672192548098037, 8.172820780973574e-05, 2.2499948031294248e-05, 5.760112545337715e-06, 2.0183689078730817e-06, 1.1920634934495615e-06]



\begin{table}[]
    \caption{Result of the Procrustes problem experiment, measured by the mean squared error over the training data for different ranks for the weight matrices. The percentage in parenthesis comes from the ratio between the Procrustes problem rank and the output.}
    \label{tab:procrustes_result}
    \centering
    \begin{tabular}{|c|c|c|}
      \cline{2-3}
    \multicolumn{1}{c}{}    & \multicolumn{2}{|c|}{MSE (training)} \\\hline
      Rank & $b = 3$ (model) & $b = 3.2$ (model) \\\hline
        $50 (1.04\%)$ & $4.82 \times 10^{-1}$ & $0.117$\\
        $100(2.08\%)$ & $2.27 \times 10^{-2}$ &$0.0989$\\
        $150(3.12\%)$ & $3.33 \times 10^{-3}$ &$0.0929$\\
        $200(4.16\%)$ & $9.73 \times 10^{-4}$ & $0.0845$\\
        $250(5.20\%)$ & $1.98 \times 10^{-4}$ & $0.0623$\\
        $\mathbf{300(6.24\%)}$ & $\mathbf{1.16\times 10^{-5}}$ & $\mathbf{0.0469}$\\
        $\mathbf{350(7.28\%)}$ & $\mathbf{1.94\times 10^{-6}}$ & $\mathbf{0.0435}$\\
        $\mathbf{400(8.32\%)}$ & $\mathbf{9.49\times 10^{-7}}$ & $\mathbf{0.0379}$\\
        $450(9.36\%)$ & $8.414e\times 10^{-7}$ & $0.0331$\\
        $500(10.4\%)$ & $8.23\times 10^{-7}$ & $0.0289$\\
        $550(11.44\%)$ & $8.19\times 10^{-7}$ & $0.0253$ \\
        \hline
        %$600$ & $8.19\times 10^{-7}$ & \\
    \end{tabular}

\end{table}

%TESLA CRASHA A PARTIR DE r = 650. Performance horrivel do POD em 600.

Table \ref{tab:procrustes_result} presents the results of the experiments in terms of mean quadratic training error for each model utilized.
%
Of course the exact ($b = 3$) model was expected to perform well, with the training errors being magnitudes lower than in the parametric error case ($b = 3.2$).
%
One can notice that the error has logarithmic leaps in the first case. 
%

The bold values are the Procrustes ranks considered for the next experiment.
%
The reason for the choice is that the system achieves satisfactory performance in terms of training error when the rank of the solution is $300$.
%
When the rank is $400$, the system starts to see diminishing returns regarding the exact case simulation.
%
A bias error is present in the parametric error case, as the model is not fully equal to the system. 
%
The rank $200$ and $300$ show a large decay of training error in comparison to the other cases.
%
The rank $300$ also shows the start of a diminishing-return tendency in terms of the parametric difference case.
%
The next point is to know how well a POD reduction performs in terms of identified models with the presence of errors.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{POD Reduction}

Contrary to the computation of the input SVD, the full SVD computation is feasible.
%
Therefore, the first step for this experiment is to analyze the singular values closely.
%
Suppose $s_i$ is the $i$'th singular value of the output data.
%
The energy contribution of a given reduced dimension $N_r$ and the full dimension $N = 99$ is
     %%%%%
\begin{equation}
    EC(N_r) = \frac{\sum_{i=1}^{N}s_i}{\sum_{j=1}^{N}s_j}
\end{equation}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/ec_plot_diffusion2.eps}
    \caption{Plot of the energy contribution in function of the POD reduced order of the nonlinear diffusion equation system.}
    \label{fig:ec_diffusion}
\end{figure}


Figure \ref{fig:ec_diffusion} showcases a plot of the energy contribution obtained by performing POD to reduce the system to a given rank, from $N_r = 5$ up to $N_r = 40$.
%
This plot shows that we can obtain more than $95\%$ of the information by reducing the rank to $N_r = 5$, while an $N_r = 18$ gives us more than $99\%$ of the approximation precision.
%
There are diminishing returns by selecting $N_r > 18$. With $N_r = 27$, we are supposed to obtain an approximation with $99.5\%$ exactness. However, reducing the order to half the original dimension is not that significant of a gain.
%
Therefore, we chose to test for $N_r \in \{5,18,27\}$ in our experiments because of the milestone energy contribution value they provide.


% 300: [100762.21733660765, 6.734804881527875, 0.45969575435614446]
% 350   [108134.86913965839, 39.471881721706225, 1.052986015092897]

% 600 [40488.7019733169, 29.394622944923167, 0.4629259323645497, 8.184978833145843e-07]

%350

\textcolor{red}{(Jean: Experiment seems to have bad performance.)}

\subsubsection{Result Showcase}






% \subsection{Proof of Concept: Bidimensional Two-phase Reservoir}\label{sec:reservoir}

% To discuss physical aspects of a petroleum reservoir, we consider a two-phase reservoir model, since a one-phase reservoir is linear \cite{Jansen2013}, and a two-phase reservoir, while still simple, has enough nonlinearities to be of relevance to the discussion at hand.

% The partial differential equations that govern a two-phase reservoir is \cite{Jansen2013}:
% \begin{align}
%  \alpha \left[ \phi(\mathbf{x}) S_w(c_w + c_r)\partial_tp + \phi(\mathbf{x}) \partial_tS_w \right] &= \alpha q + \frac{\alpha}{\mu_w}\nabla \cdot \left(kk_{rw}\nabla p\right)\\
%  \alpha \left[ \phi(\mathbf{x})(1- S_w)(c_o + c_r)\partial_tp - \phi(\mathbf{x}) \partial_tS_w \right] &= \alpha q + \frac{\alpha}{\mu_o}\nabla \cdot \left(kk_{ro}\nabla p\right)    
% \end{align}
% where $\phi(\mathbf{x})$ is the porosity reservoir (and $\mathbf{x}$ is the vector space corresponding each physical dimension of the reservoir), $k(\mathbf{x})$ is the absolute permeability, $k_{rw}(S_w),k_{rw}(1-S_w)$ are the permeabilities of the water and oil phases, respectively, and are functions of the water saturation $S_w(\mathbf{x},t)$, which is a system state alongside the pressure $p(\mathbf{x},t)$.
% %
% The $q(\mathbf{x},t)$ function represents the external flow going into our out the reservoir system (namely oil produciton and water injection wells).
% %
% Each phase compression coefficient $c_o,c_w$ and $c_r$ is considered constant for the purposes of this work, as they are fluid-dependant and has a weak dependence on pressure \cite{Jansen2013}.
% %
% The variable $\alpha$ is a constant which property depends on the number of dimensions.
% %
% If one-dimensional, $\alpha$ is the reservoir area.
% %
% If two-dimensional, $\alpha$ is the reservoir height.
% %
% If three-dimensional, $\alpha = 1$.

% The relative permeatbilities are calculated through the Corey model, which has a quadratic shape:
% \begin{align}
%     k_{rw} &= c_1(S_W^2 - 2c_2S_W + c_3)\\
%     k_{ro} &= c_4(S_W^2 + 2c_5S_W + c_6)
% \end{align}
% with $c_i$, $I = \{1,2,3,4,5,6\}$ as generic constants.

% When applying finite differences to the PDE associated with a two-phase reservoir, the result, in terms of state equations, is shaped as follows \cite{Jansen2013}:

% \begin{equation}
%     \begin{pmatrix}
%         \mathbf{V_{wp}}(\mathbf{s}) & \mathbf{V_{ws}}\\
%         \mathbf{V_{op}}(\mathbf{s}) & \mathbf{V_{os}}
%     \end{pmatrix}
%     \begin{pmatrix}
%         \mathbf{\dot{p}}\\
%         \mathbf{\dot{s}}
%     \end{pmatrix} =
%     \begin{pmatrix}
%         \mathbf{T}_w(\mathbf{s})\\
%         \mathbf{T}_o(\mathbf{s})
%     \end{pmatrix}\mathbf{p} +
%     \begin{pmatrix}
%         \mathbf{F_w}(\mathbf{s})\\
%         \mathbf{F_o}(\mathbf{s})
%     \end{pmatrix}\mathbf{q_l}
% \end{equation}

% In this discretization, each element of a matrix represents a corresponding grid-block assigned to it.
% %
% For instance, the pressure vector $\mathbf{p}$ is organized as follows in a two-dimensional reservoir:
% \begin{equation}
% \mathbf{p} = 
% \begin{pmatrix}
% \vdots\\
%     p_{i,j-1}\\
%     p_{i-1,j}\\
%     p_{i,j}\\
%     p_{i+1,j}\\
%     p_{i,j+1}\\
%     \vdots
% \end{pmatrix}
% \end{equation}
% %
% where,
% The matrices $\mathbf{V_{ws}}$ and $\mathbf{V_{os}}$ are diagonal matrices containing the value for $\phi_{i,j}$, which is the porosity of a given grid block associated with the saturation $\mathbf{s}_{i,j}$.
% %
% The matrices $\mathbf{V_{wp}}$ and $\mathbf{V_{op}}$ are diagonal matrices with each element having the following form:
% \begin{align}
%     \mathbf{V_{wp}}_{i,j} = V(c_w + c_r)\phi_{i,j}\mathbf{s}_{i,j}\\
%     \mathbf{V_{op}}_{i,j} = V(c_o + c_r)\phi_{i,j}(1- \mathbf{s}_{i,j})
% \end{align}
% notice that in this case, $i$ and $j$ correspond to the grid block assigned to the element of $\mathbf{s}$, not the matrix position.
% %
% The non-diagonal elements of the matrices in this context mean a connection between two grid blocks, and the $\mathbf{V}$ matrices represent properties of a single grid block.
% %
% A more compact representation would be:
% \begin{equation}
%     \mathbf{V}(\mathbf{s})\begin{pmatrix}
%         \mathbf{\dot{p}}\\
%         \mathbf{\dot{s}}
%     \end{pmatrix}
%     = \mathbf{T}(\mathbf{s})\mathbf{p} + \mathbf{F}(\mathbf{s})\mathbf{q}
% \end{equation}

% Matrices such as $\mathbf{T_w}(\mathbf{s})$ and $\mathbf{T_o}(\mathbf{s})$ are referred to as transmissibility matrices, and they are either tri-diagonal (one dimensional) penta-diagonal (two-dimensional) or hepta-diagonal (tridimensional), since in a system derived from PDEs such as the oil reservoir, each grid block is only connected to its adjancent counterparts.
% %
% The effect of a grid block pressure on an adjancent one is called the transmissibility term, and is governed by the following equations:
% \begin{align}
%     (T_w)_{i-1:i,j} = r\frac{h}{\mu_w}(k_{i,j}k_{rw}(\mathbf{s}_{i,j},\mathbf{s}_{i-1,j})_{i-1:i,j}\\
%     (T_o)_{i-1:i,j} = r\frac{h}{\mu_o}(k_{i,j}k_{ro}(\mathbf{s}_{i,j},\mathbf{s}_{i-1,j})_{i-1:i,j}
% \end{align}
% where $:$ has the same meaning as the MATLAB slicing operator.
% %
% We are assuming a cartesian two-dimensional grid in this case, that connects row $i-1$ to row $i$ in collumn $j$.
% %
% The analogue would be a connection between two grid blocks in a collumn, $(T_W)_{i-1:i,j}$.
% %
% Since the relative permeability considered is between two grid-blocks, $k_{kw}$ and $k_{ko}$ depend on the two adjancent water saturation for each grid block.

% Matrix $\mathbf{T_k}$, $k \in \{w,o\}$ is actually composed by rows with non-zero elements as:

% \begin{equation}
% \begin{pmatrix}
%     {T_k,}_{i,j-1:j} & {T_k,}_{i-1:1,j} &-({T_k,}_{i,j-1:j} + {T_k,}_{i-1:1,j} + {T_k,}_{i:i+1,j} + {T_k,}_{i,j:j+1}) & {T_k,}_{i:i+1,j} & {T_k,}_{i,j:j+1}
% \end{pmatrix}
% \begin{pmatrix}
%     p_{i,j-1}\\
%     p_{i-1,j}\\
%     p_{i,j}\\
%     p_{i+1,j}\\
%     p_{i,j+1}
% \end{pmatrix}    
% \end{equation}
% this internal product correponds to a component of the multiplication $\mathbf{T_i}\mathbf{p}$ related to grid block $i,j$. 
% %
% When placed together, it is easy to see that $\mathbf{T_i}$ is penta-diagonal, as each grid block only affects a set of four others in a two dimensional space.
% %
% The rows related to grid blocks on the corner of the grid must be defined differently, according to the given boundary conditions of the simulation.

% The vector $\mathbf{q}$ corresponds to the flow of each well connected to the reservoir, therevor $\mathbf{F}(\mathbf{s})$ is a sparse matrix, with nonzero elements only in grid blocks connected to a well.
% %
% Each element of $\mathbf{F}$ is defined as follows:

% \begin{align}
%     f_w = \frac{k k_{rw}/\mu_w}{k k_{rw}/\mu_w + k k_{ro}/\mu_o}\\
%     f_o = \frac{k k_{ro}/\mu_o}{k k_{rw}/\mu_w + k k_{ro}/\mu_o}
% \end{align}
% which is a rational quadratic function on the water saturation, if considering the Corey model.

% If we assume the fluids to be incompressible, it is not possible to solve this nonlinear state equation explicitly.
% %
% In case the fluid is compressible, the matrix multiplying the time derivative terms is composed of four diagonal blocks, which are trivial to invert, since the inversion of a diagonal matrix is merely the inversion of each element.
% %
% Also, the explicit formulation for this system is: 

% \begin{equation} \label{eqn:fin_diff_explicit}
%     \begin{pmatrix}
%         \mathbf{\dot{p}}\\
%         \mathbf{\dot{s}}
%     \end{pmatrix}
%     = \mathbf{V}(\mathbf{s})^{-1}\mathbf{T}(\mathbf{s})\mathbf{p} + \mathbf{V}(\mathbf{s})^{-1}\mathbf{F}(\mathbf{s})\mathbf{q}
% \end{equation}

% The inversion of a square $2 \times 2$ block matrix such as $\mathbf{V}(\mathbf{s})$ can be calculated as:

% \begin{align}
%     \mathbf{V}(\mathbf{s})^{-1} &= \begin{pmatrix}
%         \mathbf{A} & \mathbf{B}\\
%         \mathbf{C} & \mathbf{D}
%     \end{pmatrix}\\
%     \mathbf{A} &= \mathbf{V_{wp}}^{-1} + \mathbf{V_{wp}}^{-1}\mathbf{V_{ws}}(\mathbf{V_{os}} - \mathbf{V_{op}}\mathbf{V_{wp}}\mathbf{V_{ws}})^{-1} \mathbf{V_{op}}\mathbf{V_{wp}}^{-1}\\ 
%     \mathbf{B} &= - \mathbf{V_{wp}}^{-1}\mathbf{V_{ws}}(\mathbf{V_{os}} - \mathbf{V_{op}}\mathbf{V_{wp}}\mathbf{V_{ws}})^{-1}\\
%     \mathbf{C} &= - (\mathbf{V_{os}} - \mathbf{V_{op}}\mathbf{V_{wp}}\mathbf{V_{ws}})^{-1}\mathbf{V_{op}}\mathbf{V_{wp}}^{-1}\\
%     \mathbf{D} &= (\mathbf{V_{os}} - \mathbf{V_{op}}\mathbf{V_{wp}}^{-1}\mathbf{V_{ws}})^{-1}
% \end{align}

% do notice that matrices $\mathbf{A.B.C}$ and $\mathbf{D}$ are also diagonal matrices.

% The diagonal element of each submatrix of $\mathbf{V}^{-1}$ corresponding to grid-block $i,j$ is:

% \begin{align}
%     A_{i,j} &= \frac{1}{c\phi_{ij}(2s_{ij} - 1)}\\
%     B_{i,j} &= -\frac{1}{c\phi_{ij}(2s_{ij} - 1)}\\
%     C_{i,j} &= -\frac{1 - s_{ij}}{c\phi_{ij}(2s_{ij} - 1)}\\
%     D_{i,j} &= \frac{s_{ij}}{c\phi_{ij}(2s_{ij} - 1)}
% \end{align}
% which are all one-degree rational function.

% Another form of expressing Eq. \eqref{eqn:fin_diff_explicit} is:

% \begin{equation}
% \begin{pmatrix}
%         \mathbf{\dot{p}}\\
%         \mathbf{\dot{s}}
%     \end{pmatrix}
%     = \begin{pmatrix}
%         \mathbf{A}\mathbf{T_w} + \mathbf{B}\mathbf{T_o}\\
%         \mathbf{C}\mathbf{T_w} + \mathbf{D}\mathbf{T_o}
%     \end{pmatrix}\mathbf{p} + \begin{pmatrix}
%         \mathbf{A}\mathbf{F_w} + \mathbf{B}\mathbf{F_o}\\
%         \mathbf{C}\mathbf{F_w} + \mathbf{D}\mathbf{F_o}
%     \end{pmatrix}\mathbf{q}
% \end{equation}

% We know that pre-multiplying a diagonal matrix means multipying each row of the second matrix by the corresponding element of the first.
% %
% This whole discussion means that the nonlinearities that descibe a two-phase reservoir are rational functions of the adjancent saturations in a grid block, while the model is linear both for the pressure states and the control.


% Since the reservoir model has a distributed structure, the model of a single grid block has the following form:

% \begin{equation}
% \begin{pmatrix}
% c\phi_{i,j}\mathbf{s}_{i,j} & c\phi_{i,j}\\
% c\phi_{i,j}(1 - \mathbf{s}_{i,j}) & c\phi_{i,j}
% \end{pmatrix}
% \begin{pmatrix}
%     \mathbf{\dot{p}}_{i,j}\\
%     \mathbf{\dot{s}}_{i,j}
% \end{pmatrix}
% =
% a(\mathbf{s}_{i,j},\mathbf{s}_{near})\mathbf{p_{i,j}} + \mathbf{B}(\mathbf{s}_{i,j},\mathbf{s}_{near})\mathbf{p_{near}} + f(\mathbf{s}_{i,j})q
% \end{equation}

% \subsubsection{Experiments}

% \subsubsection{Discussion}

% \subsection{Real data or more complex reservoir}

% \subsubsection{Experiments}

% \subsubsection{Discussion}
