%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications} \label{sec:application}


%\subsection{Identification of an LPV System: Parametric Linear Diffusion}

This section reports results from applying global and local LPV-system identification to the discretized numerical scheme for solving the partial differential equation (PDE) regarding the diffusion system in a 1-d line.
%
Such application serves as a proof of concept for the proposed DMD-LPV identification method and its model order reduction properties.

\subsection{Parametric Linear Diffusion}


Consider the following partial differential equation (PDE):
     %%%%
\begin{equation} \label{eqn:lineardiffusion_pde}
  \frac{\partial T}{\partial t} = k(p) \frac{\partial^2 T}{\partial x^2} - w\frac{\partial T}{\partial x} 
\end{equation}
where $T(x,t)$ is the unknown solution, and $w=0.1$ is the so-called advection velocity, a constant of the system.
%
The PDE \eqref{eqn:lineardiffusion_pde} is known as the diffusion equation and is widely used in thermodynamics.
%
In that sense, $T(x,t)$ represents the temperature of a system at a given point in space and time.
%

In this application, we consider the functions that define the diffusion gain $k(p)$ to be a polynomial on the parameter $p$:
\begin{align}
    k(p) &= 0.1 + 0.05p + 0.01p^2 + 0.03p^3
\end{align}

Further, the spatial domain of the PDE is a straight-line segment. 
  Therefore, the domain is $x \in \Omega$, $\Omega = (0,1)$.
%
The boundary conditions are as follows:
\begin{equation}
\left \{ \begin{aligned}
    T(0,t) &= u(t)\\
    \partial_x T(1,t) &= 0
\end{aligned}
   \right .
\end{equation}
The boundary $T(0,t)$ is seen as a point where the temperature of the system is controlled to stay at a fixed level $u(t)$, modeled as the sole input to the system.
%
  On the other hand, the boundary condition $\partial_x T(1,t)$ means that there is no heat leaving the system  at $x = 1$, therefore it finds itself in equilibrium at that given point in space.



PDE systems can be seen as dynamic systems with an infinite number of states.
%
From the point of view of identification and control, we tend to approximate them into an ODE system \cite{Thomee20011} by discretizing the space domain.
%
ODEs have a wide array of tools to perform identification and control compared to PDEs.
%%
However, to approximate a PDE into an ODE through space discretization, a large number of points in the discretized grid of the space domain are needed.
%
Hence, any PDE-described system with a sufficiently fine grid can be labeled as a large-scale dynamic system because of the number of states.
%


To simplify the discussion, we take the domain defined for the nonlinear one-dimensional diffusion equation and distribute it into several equally spaced points with distance $h$. 
%
The finite difference method, one such PDE discretization method, states that, through center approximation:
    %%%%%
\begin{equation}
\left \{ \begin{aligned}
 \frac{\partial T(x_i,t)}{\partial x_i} &= \frac{(T(x_i + h) - T(x_i-h,t))}{2h}\\
     %%%%
  \frac{\partial^{2} T(x_i,t)}{\partial x^2} &= \frac{(T(x_i + h,t) - 2T(x_i,t) + T(x_i - h,t))}{h^2}
\end{aligned}
\right .
\end{equation}
for an arbitrary value $x_i$ inside the domain and the interval $h$.
%
Also, we consider the following, omitting the time-dependence in the notation, as in state-space notation:
\begin{equation} \label{eqn:state_indexing}
\left \{ \begin{aligned}
  x_0 &= 0\\
   x_{i+1} &= x_i + h\\
  x_{N+1} &= 1\\
  T(x_i,t) &= T_i
\end{aligned} \right . 
\end{equation}
where $x_0$ and $x_N$ are defined by the edge of the original, continuous domain.
%
The value of $T$ at those points is defined by the boundary conditions, which is why they do not correspond to a state for the ODE.
%
This means that the number of states on the discretized system is $N = \left \lceil{1/h}\right \rceil - 1$ for $h < 1$ (the actual length of the line segment considered as domain $\Omega$).

Regarding the finite-difference numerical scheme, the diffusion equation becomes as follows, for any point of $x_i$ inside $\Omega$:
    %
\begin{equation} \label{eqn:lineardiffusion_fdm1}
  \dot{T}_i =  k(p)\frac{T_{i+1} - 2T_i + T_{i-1}}{h^2} - w\frac{T_{i+1} - T_{i-1}}{2h} 
\end{equation}



Or, in a vectorized form:
   %%%
\begin{equation} \label{eqn:lineardiffusion_fdm2}
  \dot{T}_i =
  \left(
   \begin{bmatrix}
     1\\
     -2\\
     1
  \end{bmatrix}k(p)/h^2
      +
 \begin{bmatrix}
     1\\
     0\\
     -1
  \end{bmatrix}(- w/(2h))
  \right)^T
  \begin{bmatrix}
      T_{i+1}\\
      T_i\\
      T_{i-1}
  \end{bmatrix}
\end{equation}


The first boundary condition defines the value that $T_0$ assumes, which is expressed as follows:
\begin{equation}
    T_0 = u(t)
\end{equation}
Therefore:
    %%%
\begin{equation} \label{eqn:linearfdm_boundary1}
  \dot{T}_1 =
  \left(
  \begin{bmatrix}
     1\\
     0
  \end{bmatrix}(- w/(2h)) +
  \begin{bmatrix}
     1\\
     -2
  \end{bmatrix}k(p)/h^2\right)^T
  \begin{bmatrix}
      T_2\\
      T_1
  \end{bmatrix}\\
  + \left (k(p)/h^2 + w/2h\right )u(t)
\end{equation}



The second boundary condition, a Neumann boundary condition \cite{Thomee20011} in $T_{N+1}$, through the backward discretization method, basically states in this case that $T_{N} = T_{N+1}$ where $T_{N+1}$ corresponds to the point where $x = 1$ and $T_{N}$ is the closest point to it.
%
The state $T_N$ is described as:
    %
\begin{equation} \label{eqn:linearfdm_boundary2}
  \dot{T}_N =
  \left(\begin{bmatrix}
     1\\
     -1
  \end{bmatrix}\left(- k(p)/h^2-w/(2h)\right)\right)^T
  \begin{bmatrix}
      T_N\\
      T_{N-1}
  \end{bmatrix}
\end{equation}



The following LPV state space then represents the complete system:
    %%%%
\begin{equation}
    \mathbf{\dot{T}} = \left(\mathbf{A_0} +\mathbf{A}(p)\right)\mathbf{T} + \left (\mathbf{B_0} + \mathbf{B}(p)\right)u(t)
\end{equation}
   %%%%
where $\mathbf{T}$ is the vector of states along the $1D$ domain, indexed according to Equation \eqref{eqn:state_indexing}, and:
    %%%
\begin{footnotesize}
\begin{align}
 \mathbf{A_0} &=
 -(w/2h)\mathbf{D_1}\\
 \mathbf{A}(p) &= (k(p)/h^2)
 \mathbf{D_2}\\
 \mathbf{B_0} &=(w/2h)
 \begin{bmatrix}
     1\\
     0\\
     0\\
     \vdots\\
     0
 \end{bmatrix} \\
 \mathbf{B}(p) &=(1/h^2)
 \begin{bmatrix}
     1\\
     0\\
     0\\
     \vdots\\
     0
 \end{bmatrix}k(p)\\
 \mathbf{D_1} &=
 \begin{bmatrix}
     0 & 1 & 0 & 0 & \hdots & 0\\
     -1 & 0 & 1 & 0 & \hdots & 0\\
     0 & -1 & 0 & 1 & \hdots & 0\\
     \vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
     0 & 0 & 0 & -1 & \hdots & 1
 \end{bmatrix}\\
 \mathbf{D_2} &=
 \begin{bmatrix}
     -2 & 1 & 0 & 0 & \hdots & 0\\
     1 & - 2 & 1 & 0 & \hdots & 0\\
     0 & 1 & - 2 & 1 & \hdots & 0\\
     \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
     0 & 0 & 0 & 0 & 1 & -1
 \end{bmatrix}
\end{align}
\end{footnotesize} 



\subsection{Implementation, Simulation and Data Gathering}

To simulate the system, we set $h = 0.02$ so that there are $n_s = 49$ states in the discretized system, and the number of inputs is $n_u = 1$.
%
The discretization is implemented using Python and simulated in time using Runge Kutta (4th order) at $\delta t = 10^{-3}$ so that the dynamics of the system is thoroughly captured.
%
The sampling time for obtaining a data point was $T_s =  10^{-3}$.

To gather data for the nonlinear diffusion equation system, we consider that $\forall t, u(t) \in [0,4]$ and $p \in [0,1]$.
%
Then, we excitate the system with an APRBS (Amplitude-modulated pseudo-random binary signal) signal, considering a minimum step size for the control action and parameter $p$ equivalent to $10000$ time steps.
%
In other words, the system is excited for identification through a stair signal varying randomly between the aforementioned value sets for both the parameters and the control signal.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/lineardiffusion_equation_dataset.eps}
    \caption{Data-set employed for the linear parametric diffusion equation system, where the top plot is the dynamics of the whole $n_s = 49$ points measured for $T$ (with the closest point to $x=1$ being depicted as yellow), the middle plot is the excitation signal for the sole system input $u(t)$, and the bottom plot shows the values selected for $p$ along the simulation. Note that, besides the plot being in seconds, this figure corresponds to $N = 90000$ data points.}
    \label{fig:lineardiff_equation_dataset}
\end{figure}

Figure \ref{fig:lineardiff_equation_dataset} depicts the resulting simulation, which is used as a training set for all the experiments.



This data-set will be used in the experiment to provide a metric to compare each instance of the DMD-LPV parameters, with different Procrustes and POD ranks.
%
To simplify the analysis, regularization shall not be considered for this work.
%
The metric is the mean-squared error (MSE) associated with the obtained model and this data-set, being is defined as follows:
   %%%%
\begin{equation}
    MSE = \frac{1}{Nn_s}\sum_{k=1}^N\sum_{i=1}^{n_s}\left(y_{i,data}[k] - y_{i,model}[k]\right)^2
\end{equation}




%%%%%%%%%%%%%%%%%
\subsection{Structure Selection}

Since the main objective of this work is to evaluate aspects of model order reduction tools on LPV and quasi-LPV systems, we chose two structures for the LPV model to be identified:
    %%%%
\begin{itemize}
    \item \textbf{Exact Structure}: The model and the identified system have the same structure. This means that the LPV model considered for identification assumes the following features:
    \begin{equation}
        \boldsymbol{\phi(p)} = \boldsymbol{\psi(p)} = 
        \begin{bmatrix}
            1\\
            p\\
            p^2\\
            p^3
        \end{bmatrix}
    \end{equation}
    %
    In this case, $N_f = 4$.
    \item \textbf{Structural Underestimation}: For this case, we assume that, in selecting the model structure, we miss the cubic parameter-feature relation, therefore:
    \begin{equation}
        \boldsymbol{\phi(p)} = \boldsymbol{\psi(p)} = 
        \begin{bmatrix}
            1\\
            p\\
            p^2
        \end{bmatrix}
    \end{equation}
    For this parameter selection, $N_f = 3$.
    \item \textbf{Structural Overestimation}: Instead of missing one feature, this time, the model design experiments with adding $p^4$ to the parameter feature poll without knowing that it does not exist in the real system. Then, 
    \begin{equation}
        \boldsymbol{\phi(p)} = \boldsymbol{\psi(p)} = 
        \begin{bmatrix}
            1\\
            p\\
            p^2\\
            p^3\\
            p^4
        \end{bmatrix}
    \end{equation}
    For this polynomial, $N_f = 5$.
\end{itemize}

Since the number of features generated by the single parameter is small (a single digit), applying PCA to the parameter scheduling function is unnecessary.

\subsection{Procrustes Problem}


We now perform an experiment to assess how close the Procrustes problem \eqref{eqn:procustes} approaches\footnote{Refer to Section \ref{subsec:Glb-LPV} for details.} the least squares when applied for identification of the linear parametric diffusion equation.
   We evaluate different rank values $r \in \{10,20,30,40,50,60,80,100,120\}$ for the singular values and vector truncation, applying Algorithm \ref{alg:global:LPV} without performing the POD operation. 
Then, we calculate the training MSE error for each solution, while also computing the full least-squares for the system (full-rank Procrustes problem). 
%
Note that the maximum rank for the Procrustes problem is actually $n_s \times N_f$ ($4 \times 49 = 196$), since it is the number of model features.
%
Solving a Procrustes problem with such rank is the same as solving the Least Squares Problem.
%
This number is $N_f$ times larger than the number of states, which is why it is expected that in global LPV identification, the Procrustes rank and the POD rank are not the same.



\begin{table}[]
    \caption{Results of the Procrustes Problem experiment for the linear diffusion equation, measured by the mean squared error over the training data for different ranks for the weight matrices. The percentage in parenthesis comes from the ratio between the Procrustes problem rank and the output.}
    \label{tab:procrustes_result_linear}
    \centering
    \begin{tabular}{|c|c|c|c|} 
      \cline{2-4}
     \multicolumn{1}{c}{}   & \multicolumn{3}{|c|}{MSE (training)} \\  \hline
      Rank & Exact & Underestimated & Overestimated \\\hline
      $10$ & 1.47e-05 & 6.37e-06 & 3.41e-05  \\ \hline
      $20$ & 6.53e-08 & 1.35e-08 & 1.46e-07 \\ \hline 
      $30$ & 3.30e-10 & 2.94e-11 & 1.74e-09 \\ \hline 
      $40$ & 3.96e-12 & 1.21e-12 & 2.82e-11 \\ \hline 
      $50$ & 5.56e-14 & 9.72e-13& 4.37e-13 \\ \hline
      $60$ & 1.43e-14 &9.06e-13 & 1.76e-14 \\ \hline
      $80$ & 1.32e-14 &7.30e-13 & 2.11e-16 \\ \hline
      $100$ & 1.25e-14 &6.88e-13 & 1.89e-16 \\ \hline
      $120$ & 1.23e-14 & 6.933e-13 & 1.7996e-16 \\ \hline
      Full ($196$)  & 1.23e-14 & 6.943e-13 & 1.7974e-16 \\
      \hline
    \end{tabular}
\end{table}


Table \ref{tab:procrustes_result_linear} depicts the results from the experiment that performs only the Procrustes reduction without performing POD.
%
The full rank differs for each case: $147$ for the underestimated case, $196$ for the exact case, and $245$ for the overestimated case.
% 
The main idea of solving the Procrustes problem in the context of DMD-LPV is to perform an efficient computation of the weight matrix through rank limitation, which would cost less than solving the full Least Squares problem.
%
The underestimated and exact cases show diminishing returns (being in the same decade) at rank $50$, while the overestimated case should show such diminishing returns at rank $80$.
%
Since the very low error associated with the overestimated case is likely a consequence of overfitting, solving the rank $50$ Procrustes problem is as efficient as solving the full least squares problem, even accounting for a lack of precision.

Procrustes rank $40$, $50$, and $60$ make interesting test cases for the next experiment, as $40$ corresponds to before the diminishing return starts, $50$ is where the exact and underestimated cases enter the same decade  in the log scale as the full least squares error, and $60$ has a small MSE improvement over rank $50$.



\subsection{POD Reduction}

To evaluate the POD reduction, we applied Algorithm \ref{alg:global:LPV} with the POD operation for ranks $r \in\{1,5,10,15,20,25,30,35,40,45,49\}$ which correspond to the dimension of reduced state variable $\mathbf{z}$.
%
Since our interest is to find the smallest rank that does not degenerate the system behavior, we tested values for where the diminishing returns on training MSE started to appear.
%
This is why we selected ranks $\{40,50,60,80\}$ of the Procrustes reduction.
%
Procrustes Rank of $40$ was right before the MSE diminishing returns started, and $40$, $50$, and $60$ are the earliest stages.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/lineadiffusion_pod_plot.eps}
    \caption{Plot of the training data MSE in log scale as a function of the POD reduction rank for the exact case  ($N_f=4$). Each curve represents a different value for the Procrustes Problem rank.}
    \label{fig:pod_error_plot}
\end{figure}


Figure \ref{fig:pod_error_plot} showcases the result of the experiment in log scale.
%
It shows that for the Procrustes ranks depicted, there is diminishing returns on POD rank $15$ and $20$, which represent around $31\%$ and $41\%$ of the original number of states considering the exact case ($N_f=4$).

Figure \ref{fig:pod_error_plot} corroborates the fact that the identification error is degenerated by a factor of $100$ between Procrustes ranks $40$ and $50$ (two decades in log scale). Meanwhile, a Procrustes rank of $60$ shows very similar error behavior to a Procrustes rank of $80$ for any rank, even behaving better at a POD rank of $15$.
%
This is why only Procrustes of rank $50$ and $60$ are considered for experiments on the underestimated and overestimated models.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/under_and_over_pod_plot.eps}
    \caption{Plot of the training data MSE for Procrustes ranks 50 and 60 in log scale as a function of the POD reduction rank, now further depicting the results for the underestimated and the overestimated case. Each curve represents a different value for the Procrustes Problem rank. The first plot depicts the underestimated, exact, and overestimated model for the rank $50$ case, and the second plot is analog, but for the rank $60$ case. Notice that for the $r = 60$ case, the performance of the overestimated model is close to the exact model.}
    \label{fig:pod_error_underandover_plot}
\end{figure}

Figure \ref{fig:pod_error_underandover_plot} depicts the POD error for Procrustes rank $50$ and $60$, considering the exact, underestimated, and overestimated model.
%
One factor that must be considered when analyzing errors for the overestimated case is that they come from overfitting the model.
%
The $p^4$ monomial does not exist in the real system, yet the training error is lower, which does not hold for untrained data.
%
This is due to the larger sensitivity of the overestimated model.
%
In fact, the rank $ 50 $ performs some regularization (note that no model regularization is employed), with the error of the overfitted case coherently being larger than the exact case.
%
The error of the overestimated case is close to the underestimated case for higher POD ranks.
%
Also, the error incurred in the underestimated case is lower in the rank $50$ case than in the rank $60$ case for lower POD ranks.
%
The discussion of these results is further elaborated later on with a separate simulation.


\subsection{Local Identification}


Local identification is trivial since the parametric linear diffusion model is a true LPV and not a quasi-LPV.
%
%
To identify the linear parametric diffusion system, we provide a different cluster of data sets where the system is excited with a fixed $p$ and an APRBS signal for $u$. 
%
The values of $p$ for this experiment are drawn from the set $\{0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1\}$.
%
Here, we apply Algorithms \ref{alg:local:LPV} and \ref{alg:local:LPV:B}  to obtain a reduced-order model locally, aiming to showcase that both algorithms achieve a nearly equivalent performance, thus validating Algorithm \ref{alg:local:LPV:B} as a local model order reduction method.
   However, Algorithm \ref{alg:local:LPV:B} is computationally cheaper because the identification of LTI systems is performed in reduced-order space, whereas Algorithm \ref{alg:local:LPV} carries out the LTI identification in the full state space.  


    We use the global training data as a metric despite training each LTI model in a $12000$ timestep simulation.
%
This means that we are intrinsically performing a one-step prediction test. Therefore, the MSE value is supposed to be higher than an error measure over training data.
%
In fact, the full-order local identification with the exact polynomial performs with an MSE of $1.39 \times 10^{-7}$.


\begin{table}[htb]
    \centering
        \caption{MSE Result from the global case training data-set for the local identification, identifying full order LTI systems. The rank in the first column is both the POD rank and the Procrustes rank to perform the final weight identification, which are the same. The $\Delta$ means that the value depicted is the difference between the MSE in question and the Exact MSE for that rank value. A negative value is a lower error, and a positive value is a larger error.
        %
        This choice of depiction is because the difference in errors is quite small.}     \label{tab:local_result_linear1}
    \begin{tabular}{|c|c|c|c|}
       \cline{2-4}
     \multicolumn{1}{c}{}   & \multicolumn{3}{|c|}{MSE (one-step prediction of global training data-set)} \\\hline
      Rank & Exact & Underestimated ($\Delta$) & Overestimated ($\Delta$) \\\hline
      $1$ & $2.91\times 10^{-4} $& $2\times 10^{-9}$ & $2\times 10^{-9}$ \\ \hline
      $5$ & $1.26\times 10^{-6}$ & $4\times 10^{-10}$ & $4\times 10^{-10}$ \\ \hline 
      $10$ & $6.82\times 10^{-7}$ & $1.7\times 10^{-10} $ & $1.7\times 10^{-10}$ \\ \hline 
      $15$ & $4.55\times 10^{-7}$ & $1.7\times 10^{-10} $ & $1.7\times 10^{-10}$\\ \hline 
      $20$ & $3.41\times 10^{-7}$ &$1.3\times 10^{-10}$& $1.3\times 10^{-10}$\\ \hline
      $25$ & $2.73\times 10^{-7}$ &$1.1\times 10^{-10}$& $1.1\times 10^{-10}$ \\ \hline
      $30$ & $2.28\times 10^{-7}$ &$9\times 10^{-11}$ & $9\times 10^{-11}$\\ \hline
      $35$ & $1.95\times 10^{-7}$ &$7\times 10^{-11}$ & $7\times 10^{-11}$\\ \hline
      $40$ & $1.71\times 10^{-7}$ & $7\times 10^{-11}$ &$7\times 10^{-11}$\\ \hline
      $45$  & $1.52\times 10^{-7} $& $6\times 10^{-11}$& $6\times 10^{-11}$ \\ 
      \hline
    \end{tabular}
\end{table}


Table \ref{tab:local_result_linear1} shows the results for the local LPV identification experiment with the full-order LTI training.
%
The MSE was measured for the global training set, considering both the exact, underestimated, and overestimated models for the different rank values depicted.
%
The local identification allows the Procrustes and POD reduction at the same rank since the maximum rank for the Procrustes problem is $N_f \times r_{pod}$.


This method looks robust regarding polynomial structural uncertainty for one-step prediction, since the MSE error difference for the three cases is relatively small.
%
The reduced-order model reaches the same MSE decade as the full order model at a rank as small as $10$, which means that performing the reduced-order identification with such rank would provide a close representation of the whole system.
%
We also considered ranks $5$ and $15$ for the results showcased in Section \ref{sec:results_showcase} to assess the model performance in a full simulation problem before and after the diminishing returns. 


\begin{table}[htb]
    \caption{MSE result from the global case training data-set for the local identification, identifying LTI systems in the latent space defined by the POD transformation. The rank in the first column is both the POD rank and the Procrustes rank to perform the final weight identification, which are the same. The $\Delta$ means that the value depicted is the difference between the MSE in question and the Exact MSE for that rank value. A negative value is a lower error, and a positive value is a larger error.
        %
        This choice of depiction is because the difference in errors is quite small.}
    \label{tab:local_result_linear2}

    \centering
    \begin{tabular}{|c|c|c|c|}
     \cline{2-4}
     \multicolumn{1}{c}{}   & \multicolumn{3}{|c|}{MSE (one-step prediction of global training data-set)} \\\hline
      Rank & Exact & Underestimated ($\Delta$) & Overestimated ($\Delta$)\\\hline
      $1$ & $4.99\times 10^{-6}$& $3\times 10^{-18}$ & $3\times 10^{-18}$ \\ \hline
      $5$ & $1.67\times 10^{-6}$ & $1.2\times 10^{-19} $& $1.3\times 10^{-19} $ \\ \hline 
      $10$ & $6.82\times 10^{-7}$ & $-1\times 10^{-19}$  & $-2\times 10^{-19}$\\ \hline 
      $15$ &$ 4.55\times 10^{-7}$ & $-2\times 10^{-19}$ & $-2\times 10^{-19}$ \\ \hline 
      $20$ & $3.41\times 10^{-7}$ & $3\times 10^{-17}$ & $3\times 10^{-17}$ \\ \hline
      $25$ & $2.73\times 10^{-7}$ &$-1\times 10^{-14}$& $-1\times 10^{-14}$ \\ \hline
      $30$ & $2.28 \times 10^{-7}$ &$-1\times 10^{-13}$ & $-1\times 10^{-13}$\\ \hline
      $35$ & $1.95\times 10^{-7} $&$3\times 10^{-11}$ & $ 6\times 10^{-11} $\\ \hline
      $40$ & $1.71\times 10^{-7}$ & -$1\times 10^{-11}$ & -$1\times 10^{-11}$\\ \hline
      $45$  & $1.52\times 10^{-7}$ & -$1\times 10^{-11}$ & -$1\times 10^{-11}$ \\
      \hline
    \end{tabular}
\end{table}

Table \ref{tab:local_result_linear2} showcases the same experiment as the one depicted in Table \ref{tab:local_result_linear1}, however, with the alternate proposed local LPV identification method.
%
The purpose of this experiment was to evaluate whether using the POD transformation corresponding to the full LPV model would serve as a valid linear transformation to identify the local LTI systems in a reduced latent space, and then employ such a transformation to map the reduced LPV model states back into full order.
%
It turned out that both methods showed similar efficacy in terms of the one-step prediction problem, which validates the use of the POD transformation in local LPV identification.
%
Notice that the error difference ($\Delta$) between the model mismatch cases and the exact case might even miss machine precision for the lower rank cases, which are the ones of interest in this application.


Overall, these experiments show that a model order reduction in a local linear environment might be more computationally efficient than a global one, while using the same decade criterion as the full-order error criterion.


%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussion} \label{sec:results_showcase}

The previous experiments tested different rank (Procrustes and POD) values using the MSE metric for a given data set to train the DMD-LPV model with a global training procedure.
%
While the local trained DMD-LPV demanded a different kind of data set by design, the global data set was used to test performance in one-step prediction type of problems. 
%
The previous experiments aimed to find rank values for which the MSE presented diminishing returns considering: the global Procrustes reduction rank, global POD rank, and local POD/Procrustes rank. 
%
In this section, we perform a simulation experiment considering a completely separate data set. The LPV model only has access to the initial condition. Thus, the runtime is susceptible to cumulative error in contrast to the one-step prediction, where the previous step is given by the data.
%
The experiment in question considers the following models:
\begin{itemize}
    \item Globally identified DMD-LPV model with (Cartesian product):
    \begin{itemize}
        \item Procrustes Rank: $40$, $50$, and $60$.
        \item POD Rank: $5$, $10$, and $15$.
    \end{itemize}
    \item Locally identified DMD-LPV model rank: $5$, $10$, and $15$.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/test_excitation_signal.eps}
    \caption{Plot of the excitation signal for both $p[k]$ and $u[k]$ utilized for the simulation experiments.}
    \label{fig:test_excitation}
\end{figure}

Figure \ref{fig:test_excitation} depicts the experiment's excitation signal concerning the parameter $p$ and input $u$.
%
Although every model performs well in a one-step prediction test similar to the one performed with the local identification set, there are identification models prone to numerical instability for simulation---the model only receives the initial condition and does not use further information from data.
%

To showcase the simulation result, we consider the signal for $T$ where $x = 0.98$, as it is the furthest point away from the region the input affects $x = 0$, hence that highest order input-to-output dynamic.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/local_sim_test.eps}
    \caption{Simulation results for the local, exact polynomial case at the $T$ for the point closest to $x = 1$. The results show that the LPV systems identified locally behaved similarly, showing that a reduction to a dimension of $5$ suffices for model simulation of this application.}
    \label{fig:local_test}
\end{figure}

Figure \ref{fig:local_test} showcases the simulation of the obtained local models in comparison to the test output.
%
The local models seem less exact than the global models; however, they do not bring issues regarding numerical instability and are also theoretically easier to identify (all models were identified using the reduced LTI procedure).
%
The three cases tested behaved similarly, which means that a reduction to a dimension of $5$ is perfectly safe to obtain a model simulation of this application.


\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/global_sim_test.eps}
    \caption{Simulation results for the global, exact polynomial case. Each plot corresponds to a different Procrustes rank. These results show that the LPV system identified globally achieved nearly perfect simulation, considering a Procrustes rank of $40$, $50$, and $60$ in combination with a POD rank of $5$, $10$, and $15$.}
    \label{fig:global_test}
\end{figure}

Figure \ref{fig:global_test} showcases the simulation of the globally obtained models compared to the test output.
%
Procrustes rank $40$ and POD rank $15$ are not shown because the simulation was halted in the test set because it was numerically unstable.
%
Testing with other ranks showed that a higher POD rank and a lower Procrustes rank make the identified model prone to numerical instability.
%
For instance, we did not manage to find a good run for Procrustes rank of $20$.


Overall, global identification might seem superior in performance to local identification, but the proposed local identification procedure is more efficient for finding a reduced-order model that is numerically stable and easier to compute than the full model. 
%
This might indicate a pattern in which non-intrusive lower-order identification has properties akin to Tikhonov regularization in relation to numerical instability protection.


\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/under_and_over_simulation.eps}
    \caption{Simulation results for the lowest rank global and locally identified models in terms of underestimated and overestimated polynomial estimation.}
    \label{fig:underover_sim}
\end{figure}


Figure \ref{fig:underover_sim} depicts a simulation involving the DMD-LPV models identified with a structural error, and all simulation plots in this figure refer to the lowest (both Procrustes and POD) ranks for the evaluated models.
%
The local models seem insensitive to the structural errors committed, corroborated by the one-step prediction experiments.
%
Meanwhile, the global identification seems more prone to overfitting, as the overestimated model shows steady-state error.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/large_lineardiffusion_simulation.eps}
    \caption{Simulation of identified DMD-LPV models for a version of the linear parametric diffusion equation with $h = 0.005\, (N = 199)$. The rank parameters were selected so that they are the minimum possible to perform a coherent identification.}
    \label{fig:200statediffusion}
\end{figure}

Finally, to better state the state compression power of the DMD-LPV identification, we perform the same described identification procedure in a counterpart of the diffusion equation system with an $h$ of $0.005$ instead of $0.02$.
%
This entails that the system has a total of $199$ states instead of the $49$ states utilized in the other experiments.
%
Only a local training procedure for rank $5$ and a global training considering the Procrustes rank $20$ and the POD rank $5$ were performed.
%
The result, in Figure \ref{fig:200statediffusion}, shows that the simulation of a $200$ state LPV system can be compressed to a $5$ state reduced-order model.
%
Note that, by the same logic as the rest of this section, the last point in the bar is evaluated; therefore, it presents a $199$ order dynamic in relation to the input, which is the temperature located at $x = 0$, propagating up until $x = 0.995$.



%%%%%%%%%%%%%%%%%%%%%%