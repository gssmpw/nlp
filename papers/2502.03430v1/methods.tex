\section{Methods}
\label{sec:met}
This section outlines the annotation process and characteristics of the frame-level annotations released in this work for the REAL-Colon dataset (Section \ref{subsec:realcolon}). Then, it offers a comprehensive explanation of TCN and MS-TCN models utilized for the temporal segmentation of colonoscopy videos (Section \ref{subsec:tcn}) and of proposed TCN architecture, ColonTCN, and provides a description of the adopted loss functions for its optimization (Section \ref{subsec:loss}). Finally, Section \ref{subsec:dataset} introduced the training and evaluation protocol that we propose to benchmark models on the REAL-Colon dataset for the temporal video segmentation task.

\subsection{Annotation of the REAL-Colon dataset}
\label{subsec:realcolon}
The REAL-Colon dataset \cite{biffi2024real} is an open-access dataset comprising 2.7 million high-resolution frames extracted from 60 full-procedure colonoscopy videos, capturing a diverse patient population across four cohorts from six medical centers spanning three continents. "The first two dataset cohorts, Cohort 1 and Cohort 2, include 15 videos each from clinical studies NCT03954548 and NCT04884581 on ClinicalTrials.gov, acquired across three U.S. centers and one Italian center, respectively. Cohorts 3 and 4 also consist of 15 videos each, sourced from separate acquisition campaigns at centers in Austria and Japan \cite{biffi2024real}. The video recordings document each procedure from its initiation near the entrance of the colon to its conclusion as the endoscope is withdrawn. For comprehensive details on the REAL-Colon data acquisition, dataset construction, patient statistics, and privacy considerations, we refer readers to \cite{biffi2024real}.

\input{fig_text/fig1a}
\input{fig_text/fig1b}

For this study, all 60 videos in the REAL-Colon dataset underwent additional annotation. Eight medical image annotation specialists, supervised by an expert gastroenterologist from a pool of three experts, followed a standardized process using a specialized in-house annotation tool capable of timestamp placement. The process included multiple feedback rounds and can be summarized as follows. First, three colonoscopy phase labels — outside, insertion, and withdrawal — were annotated, each with a start and stop frames. Instruction was to make the withdrawal phase start when the endoscope reached the appendiceal orifice or ileocecal valve, and the endoscopist initiated cecum exploration following cleaning. The transition between the colon interior and exterior was marked at the frame where the anus was entered or exited. Then, the withdrawal phase was annotated into seven colon segment sub-classes: ceacum, ileum, ascending colon, transverse colon, descending colon, sigmoid colon, and rectum. The ascending, transverse, and descending colon were delineated by identifying the first frame in which the hepatic and splenic flexure was visible. Similarly, the cecum was defined by identifying the ileocecal valve and appendiceal orifice, while the rectum was defined by identifying the rectosigmoid junction. In cases where the endoscope entered the ileocecal valve and explored the terminal ileum, distinguished by its specific mucosal appearance, the corresponding label was assigned to those frames.

As a result, at the end of the annotation process, only a small percentage of frames (0.2\%) were classified as uncertain, all coming from only two videos. These frames correspond to instances of brief recording interruptions or cases where an expert gastroenterologist could not confidently assign a frame label during frame class transitions. Figure \ref{fig:fig1a} illustrates the sequence of segments and the proportional distribution of each label class within the dataset resulting from the annotation process. Annotations have been released here: \url{https://doi.org/10.6084/m9.figshare.26472913}. 

Figure \ref{fig:fig1a} reveals the pronounced class imbalance typical of colonoscopy procedures, with the outside and ileum class frames being significantly less frequent in the dataset (in 34 out of 60 videos the ileum was not reached). Figure \ref{fig:fig1b} complements this by presenting a box and whisker plot depicting the percentage of video length occupied by each frame label in the REAL-Colon dataset, highlighting the substantial intra-class and inter-class variability inherent to the problem.

\subsection{ColonTCN}
\label{subsec:tcn}
Each colonoscopy video \( \mathbf{V} \in \mathbb{R}^{T \times H \times W \times 3} \) consists of \( T \) frames, each with dimensions \( H \times W \) and 3 color channels. TCN approaches typically involve two key steps:

\begin{enumerate}
    \item A visual frame feature extraction step, where video frames are embedded into a low-dimensional representation \( \mathbf{X} \in \mathbb{R}^{T \times D} \). In line with several previous works on temporal video segmentation and surgical workflow analysis, in this work we utilize ResNet-50 pretrained on ImageNet as the feature extractor \cite{he2016deep}, yielding frame representations of size \( D=2048 \).
    \item A TCN model that performs spatio-temporal reasoning across the entire set of video frames \( \mathbf{X} \), outputting predictions \( \mathbf{Y} \in \mathbb{R}^{T \times C} \), where \( C \) represents the number of distinct frame labels to be predicted. Each prediction for the \( i \)-th frame is influenced by both preceding and succeeding frames in the context of  colonoscopy video temporal segmentation since the analysis is conducted using acasual temporal convolutions at the procedure end.
\end{enumerate}

In this work, we propose ColonTCN for this second step. Similarly to TCN-based approaches adopted in similar fields \cite{czempiel2020tecno}, ColonTCN commences with an initial feature reduction (FR) layer comprising a \(1 \times 1\) convolutional layer paired with a ReLU activation function (Figure \ref{fig:fig2a}). This layer is designed to align the dimensions of the input features \( \mathbf{X} \) obtained with the ResNet-50 model with the feature map $\mathbf{H} \in \mathbb{R}^{T \times F}$ input of the first temporal block (TB) of ColonTCN, where $F \ll D$ enables the construction of shallower models as illustrated in Figure \ref{fig:fig2b}. Following this, the network includes a series of \(l\) TBs applied to the features maps \( \mathbf{H}_{l-1} \) of the previous layer. Each TB of the ColonTCN architecture, specifically designed for this task, contains two weight-normalized, dilated, acausal 1D convolutional layers with kernels \(\mathbf{W}_{1,l}\) and \(\mathbf{W}_{2,l}\), biases \(\mathbf{b}_{1,l}\) and \(\mathbf{b}_{2,l}\) and dilation factor \(2^l\). Acasual convolutions are utilized to ensure that the prediction for the \( i \)-th frame is informed by both preceding and succeeding frames. After each convolutional layer, a Rectified Linear Unit (ReLU) activation function and dropout regularization are applied, as described in the following equations:

\begin{equation}
\mathbf{C}_{1,l} = \text{DropOut}[\text{ReLU}(\mathbf{W}_{1,l} * \mathbf{H}_{l-1} + \mathbf{b}_{1,l})]
\end{equation}

\begin{equation}
\mathbf{C}_{2,l} = \text{DropOut}[\text{ReLU}(\mathbf{W}_{2,l} * \mathbf{C}_{1,l} + \mathbf{b}_{2,l})]
\end{equation}

Additionally, each block integrates a residual connection to facilitate gradient flow and enable training of deeper networks:

\begin{equation}
\mathbf{H}_l = \text{ReLU}(\mathbf{H}_{l-1} + \mathbf{C}_{2,l})
\end{equation}

The addition of each TB allows the models' receptive field to expand exponentially. The use of double dilated convolutions in these blocks further broadens the models receptive field, enhancing the capture of global temporal dependencies. The use of dropout, weight regularization and residual connections enables better gradient flow even when stacking several of these TBs. In contrast, approaches such as TeCNO \cite{czempiel2020tecno} instead only stack dilated convolutions, which we have found to lead to sub-optimal learning when trying to reaching large receptive fields and lead to the proposal of this TB architecture. Finally, the output class probabilities are computed by applying a \(1 \times 1\) convolution to the last dilated convolution layer's output, followed by a softmax activation function: $ \mathbf{Y} = \text{Softmax}(\text{Conv}_{1 \times 1}(\mathbf{H}_l)) $. This results in the output prediction matrix \( \mathbf{Y} \in \mathbb{R}^{T \times C} \) for the video \( \mathbf{V}\).

In a MS-TCN model, such as the ones proposed in TeCNO \cite{czempiel2020tecno}, the output \( \mathbf{Y}^1 \) from the initial TCN stage is refined through \( M \) additional TCN stages. In this work, we apply the same approach to ColonTCN, yielding MS-ColonTCN. Specifically, in a multi-stage model each stage processes the output from its preceding stage and, while all stages adhere to the architecture of the first stage, they possess distinct weight sets. The only difference in subsequent stages is the input feature size set to \( C \), eliminating the need for the FR layer. Instead, tensors are directly input to the first TB of each stage. These stages sequentially output a series of tensors \( \mathbf{Y}^S \) for \( S=1 \) to \( M \), representing the class probabilities for each stage.

\input{fig_text/fig2a}
\input{fig_text/fig2b}

\subsection{Loss Function}
\label{subsec:loss}
In the training of TCN models for surgical workflow analysis, a classification loss is typically computed at each frame $t$ using weighted cross-entropy loss:

\begin{align}
L_{\text{cls}} = \frac{1}{T} \sum_{t} - w_c \; y_{t,c} \; \log(\hat{y}_{t,c})
\end{align}

where class weights \( w_c \) are determined using median frequency balancing to address class imbalances \cite{czempiel2020tecno}, and \( \hat{y}_{t,c} \) represents the predicted probability for the ground truth label \( c \) at time \( t \).

Multi-stage architectures typically also employ truncated mean squared error over the frame-wise log-probabilities to reduce over-segmentation errors \cite{li2020ms}

\begin{align}
L_{\text{T-MSE}} = \frac{1}{TC} \sum_{t,c} \tilde{\Delta}_{t,c}^2
\end{align}
\begin{equation}
\tilde{\Delta}_{t,c} = 
\begin{cases} 
\Delta_{t,c} & \text{if } |\log y_{t,c} - \log y_{t-1,c}| \leq \tau, \\
\tau & \text{otherwise},
\end{cases}
\end{equation}


The overall loss, computed over the  \( M \) stages, thus becomes

\begin{align}
L &= \frac{1}{M} \sum_{M} L_{\text{cls}} + \lambda L_{\text{T-MSE}} 
\end{align}

where \( \lambda \) is a balancing weight. In this work, this same loss function was adopted for the training of ColonTCN and competitive approaches using $\tau=4$ and $\lambda=0.15$.

\subsection{Model Evaluation Framework}
\label{subsec:dataset}

We propose both a 5-fold and a 4-fold cross-validation (CV) method to train and evaluate models on the 60 full-procedure videos of the REAL-Colon dataset we have annotated. For the 5-fold CV approach, we randomly distributed the dataset into subsets comprising 44 training videos, 4 validation videos, and 12 testing videos. This division was achieved by randomly selecting videos from the four clinical cohorts within the REAL-Colon dataset, ensuring that each cohort contributed 11, 1, and 3 videos to the training, validation, and testing subsets, respectively. This sampling approach ensured the uniqueness of the videos in each of the five testing sets, allowing for a thorough evaluation of the models across the entirety of the REAL-Colon dataset. Random selection was repeated multiple times to make certain that each test fold included at least one video depicting the ileum, sourced from a minimum of three out of the four cohorts. The specifics of the selected video distribution across the 5-fold splits are detailed in the Supplementary Materials. The 4-fold CV method was instead designed to include two studies from the REAL-Colon dataset in the training set, one in the validation set, and one in the testing set. This CV experiment aimed to test the models' ability to generalize to unseen centers and in a more challenging scenario (less training data too). Indeed, each model in this setting is trained on only 30 videos, as opposed to the 44 used in the 5-fold scenario.

\input{tables/folds}

Consistent with established practices in the field, we standardized the frame rate of all videos to 5 frames per second (fps) for both training and inference \cite{czempiel2020tecno,demir2023deep}. The length of these videos showed considerable variation, with an average of 8,471 frames (28 minutes), a standard deviation of 4,947 frames (16 minutes), and a range from a minimum of 3,485 (11 minutes) to a maximum of 24,772 frames (82 minutes) per video. Table \ref{tbl:folds} provides a comprehensive breakdown of the frame class distributions in the test sets from both the 5-fold and 4-fold CV evaluations of the REAL-Colon dataset, with videos adjusted to 5 fps. It is noteworthy that Cohort 3 includes longer videos than the other studies and none of the videos in Cohort 2 reached the ileum, presenting an additional real-world challenge for the 4-fold CV evaluation.

In both cross-validation (CV) settings, performance evaluation for a given model is computed for all $C=9$ classes by computing the average F1 score per-frame, as commonly done in surgical workflow analysis research \cite{demir2023deep}. Using the set of frames labeled as Ground Truth (GT) for a class \(c\) and the set of frame Predictions (P) predicted for that class, the F1 score represents the harmonic mean of precision and recall over all the frames labeled to that class:
\[
\text{Precision (PR)} = \frac{|GT \cap P|}{|P|}, \quad \text{Recall (RE)} = \frac{|GT \cap P|}{|GT|}, \quad \text{F1} = \frac{2 \cdot PR \cdot RE}{PR + RE}.
\]

Moreover, for each model, three additional metrics are computed, split by split, over all videos of the REAL-Colon dataset:

\begin{itemize} 
    \item The class-weighted average F1 score (wF1) accounts for the inherent class imbalances within the dataset, offering a more nuanced understanding of the models' performance by assigning greater importance to less frequent classes. The weighted F1 score is defined as:
    \[
    \text{wF1} = \sum_{c=1}^{C} w_c \cdot \text{F1}_c,
    \]
    where \(w_c\) is the weight for class \(c\) and \(\text{F1}_c\) is the F1 score for class \(c\).
    
    \item The weighted Jaccard index (wJacc). The Jaccard index is defined as:
    \[
    \text{Jaccard Index (J)} = \frac{|GT \cap P|}{|GT \cup P|}.
    \]
    and measures the models' temporal segmentation accuracy by evaluating the overlap between predicted and actual segments. Its weighted version is:
    \[
    \text{wJacc} = \sum_{i=1}^{C} w_c \cdot \text{J}_c,
    \]
    
    \item The Mean Absolute Percentage Error (MAPE) metric for the withdrawal time (WMAPE). The withdrawal time is defined as the time spent (derived by counting the frames) neither outside the colon nor during the insertion phase. Accurate withdrawal time estimation relies on the precise segmentation of short sequences (e.g., frames outside the colon) and a comprehensive understanding of the long temporal sequences to identify the start of the withdrawal phase. The WMAPE measures the deviation between the predicted and actual number of withdrawal frames, employing the absolute mean percentage error to account for the varying lengths of withdrawal times present in the dataset:
    \[
    \text{WMAPE} = \frac{1}{N} \sum_{i=1}^{N} \left| \frac{A_i - P_i}{A_i} \right| \times 100,
    \]
    where \(A_i\) is the actual number of withdrawal frames, \(P_i\) is the predicted number of withdrawal frames and N is the number of videos.
\end{itemize}

Finally, we profile each model to estimate its computational efficiency by reporting the number of parameters and Giga Floating-Point Operations (GFLOPs), which quantify the floating-point operations required for a forward pass through the model.