Conducting domain-specific training from scratch, using sufficient number of annotated samples, is time-consuming and resource-intensive. Zero-shot or few-shot setups play an important role in such cases, particularly in low-resource scenario.
We investigated the performance of our model in zero-shot or few-shot setups. Zero-shot experiments were carried out on three biomedical datasets -- DDI, DTI and PPI, to exploit the similarity of the three domains. In the zero-shot experiments, learning and evaluation were carried out on different datasets. For the zero-shot set up, initial node feature dimensions of the test-domain nodes are changed by changing the number of supernodes or changing the dimension of the random vector to turn the test-domain node features with dimensions equal to that of the train-domain node features. If the dimension is to be increased, then the number of supernodes is increased, and if the dimension is to be decreased, then the random vector dimension is reduced. The results of the zero-shot experiments are reported in Table \ref{tab:biomedical_zeroshot} in terms of AUPRC and edge-accuracy. In order to compare the results of the zero-shot experiments with the in-domain experiments, we also include the results of the in-domain experiments (cf. Table \ref{tab:DDI_DTI_compare}) in Table \ref{tab:biomedical_zeroshot} (shaded rows). Table \ref{tab:biomedical_zeroshot} shows that in most of the cases, Zero-Shot learning in cross-domain produces better results than the original (in-domain) experiments. We observe that, if in one domain, when the graph is bipartite (e.g., DTI dataset) and in the other domain, the graph is single or unpaired (e.g., DDI or PPI dataset) the cross-domain experiments typically does not produce better results in terms of AURPC score. However, it is interesting to note that for the model trained on DTI and tested on DDI or PPI, the accuracy improves even though the AUPRC decreases. The italicized scores in Table \ref{tab:biomedical_zeroshot} are same or better than the in-domain experiment scores. 



Few-Shot learning was performed on the YAGO dataset. Maximum number of samples per relation type was increased step by step to observe how much data is required for the model to perform better than the
%existing baselines 
SOTA model and the results are presented in Table \ref{tab:YAGO_fewshot}. Table \ref{tab:YAGO_fewshot} shows that, with our method only 156,184 edges out of 1,079,040 edges in the training set with maximum training samples of 8,000 per relation type is adequate to beat the existing SOTA results. 
\input{Latex/Tabledef/zeroshot_fewshot_res_tab}