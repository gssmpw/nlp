Another important and challenging problem in graph learning is link prediction on Social Network Analysis datasets. It is essentially an edge-level binary classification task, where existence of edge between two nodes (along with its direction) is to be predicted.

We carried out (directed) link prediction on two widely used Social Network datasets -- Epinions and Slashdot.
In each dataset, number of negative edges sampled were same as the number of positive edges, in both training and test set. Due to limited RAM-size, loss of entire positive or negative edge set could not be back propagated in the training epochs. Therefore, in each epoch, 14,250 random samples were selected from each of the positive and negative edges for back propagation of loss. This sample-size may vary with available RAM-size. 
\subsubsection{Epinions}
\label{sec:epns}
For the Epinions  dataset,
%(cf. Section \ref{dataset} Para \ref{data:epi}), 
out of total 355,217 positive edges, 255,217 positive edges were used as the training set and the rest (100,000 positive edges) were used for testing. Here negative edges denote neither trust nor distrust between two nodes (users). %Considering negative edges along with the positive edges
Using balanced negative sampling, training was conducted with a learning rate of 0.005 on total 510,434 edges and evaluation was carried out on a total of 200,000 edges. The required node features are initialized using the user ratings of the products across 27 different categories, as well as the helpfulness scores associated with those ratings. Let us consider the rating vector for $user_n$ denoted by $r^n=[r_0^n, r_1^n, \dots , r_{26}^n]$, wherein $r_i^n$ represents the total rating assigned by $user_n$ to the products falling under $category_i$. Similarly, let us denote the helpfulness vector for $user_n$ by $h^n=[h_0^n, h_1^n, \dots , h_{26}^n]$, where $h_i^n$ represents the total helpfulness of the rating $r_i^n$. The initial 54 dimensional node feature of the user can be expressed as $f^n=[\frac{r_0^n}{c_0^n}, \frac{r_1^n}{c_1^n}, \dots , \frac{r_{26}^n}{c_{26}^n}, \frac{h_0^n}{c_0^n}, \frac{h_1^n}{c_1^n}, \dots , \frac{h_{26}^n}{c_{26}^n}]$, where $c_i^n$ is the total number of ratings given by $user_n$ to the products belonging to $category_i$. It is to be noted that if no rating is assigned to $category_i$ by $user_n$, then we set $c_i^n=\epsilon$ to avoid division by zero.
%Table~\ref{tab:Epinions_PRF} presents the performance of our proposed model on the directed edge prediction task on the Epinions dataset. 
Table~\ref{tab:SocNet_compare}, which presents a comparison (in terms of accuracy) against other baselines on the Social Network datasets, shows that our model produces SOTA performance on the Epinions dataset for directed link prediction task. 
% Let the rating vector for $user_n$ be $r^n=[r_0^n, r_1^n, \dots , r_{26}^n]$ where $r_i^n$ is the total rating given to the products under $category_i$ by $user_n$ \sudip{(users?)} and the helpfulness vector for $user_n$ be $h^n=[h_0^n, h_1^n, \dots , h_{26}^n]$ where $h_i^n$ is the total helpfulness of the rating $r_i^n$. The initial 54 dimensional node feature of the user is $f^n=[\frac{r_0^n}{c_0^n}, \frac{r_1^n}{c_1^n}, \dots , \frac{r_{26}^n}{c_{26}^n}, \frac{h_0^n}{c_0^n}, \frac{h_1^n}{c_1^n}, \dots , \frac{h_{26}^n}{c_{26}^n}]$, where $c_i^n$ is the total number of ratings given to the products under $category_i$ by $user_n$. If no rating is given to $category_i$ by $user_n$, then $c_i^n=\epsilon$ to avoid division by zero. 




 
 % In this dataset, the training set had 255,217 positive edges out of total 355,217 positive edges, and 100,000 positive edges were used for testing and the required negative samples or Here negative edges denote neither trust nor distrust between two nodes (users). 
 % \hl{Considering negative edges along with the positive edges} \sudip{(Instead, mention negative sampling)}, training was conducted with a learning rate of 0.005 on total 510,434 edges and testing was carried out on a total of 200,000 edges. 
  
\subsubsection{Slashdot}
\label{sec:slashdot}
Out of total 870,161 positive edges in this Slashdot dataset, %(cf. Section \ref{dataset} Para \ref{data:slash}), 
we considered  670,161 positive edges for the training set, and the rest (200,000) for testing. Here negative edges denote neither friendship nor enmity between two nodes (users). Considering a balanced negative sampling, training was conducted with a learning rate of 0.005 on 1,340,322 edges and testing was performed on 400,000 edges. Due to the unavailability of the required node feature representation for this dataset, we utilize the supernode feature using the following strategy to generate the node feature representation.


% In this dataset, training set had 670,161 positive edges out of total 870,161 positive edges, and 200,000 positive edges were used for testing. Here, negative edges denote neither friendship nor enmity between two nodes (users). \hl{Considering negative edges along with the positive edges}\sudip{(Mention negative sampling)}, training was conducted with a learning rate of 0.005 on total 1,340,322 edges and testing was done on total 400,000 edges. \sudip{(The last sentence looks almost similar to the sentence for the Epinions dataset, may be just copied. Rephrase a bit.)} Since no node feature is available in the dataset, initial node features are created using the following steps.

%\vspace*{-\baselineskip}
\begin{enumerate}
\item \textbf{Supernode Creation}: As mentioned in Section~\ref{super}, 83 supernodes are constructed, the first 82 each consisting of 1000 nodes and the last one containing 168 nodes.
%, using Eqn~\ref{eqn:supnode}.

% First 82 supernodes are consructed each consisting of 1,000 nodes (the last one contains 168 nodes).\hl{$1^{st}$ super-node is SET($node_0$, $node_1$, \dots , $node_{999}$). $2^{nd}$ super-node is SET($node_{1000}$, $node_{1001}$, \dots , $node_{1999}$). Following the pattern, $82^{nd}$ super-node is SET($node_{81000}$, $node_{81001}$, \dots , $node_{81999}$) and $83^{rd}$ super-node is SET($node_{82000}$, $node_{82001}$, \dots , $node_{82167}$).}\sudip{(@MG, rewrite this using a better representation. Use set notation.)}

\item \textbf{Calculatation of Edge-Count Vectors}: Let, the incoming edge-count vector for $node_i$ be $[ic_0^i, ic_1^i, \dots , ic_{82}^i]$ where $ic_k^i$ is the total number of edges incoming from $super$-$node_k$ to $node_i$. Similarly, the outgoing edge-count vector for $node_i$ be $[og_0^i, og_1^i, \dots , og_{82}^i]$ where $og_k^i$ is the total number of edges outgoing from $node_i$ to $super$-$node_k$. Finally, the edge-count vector for $node_i$, $ec_i$, is formed by concatenating the outgoing edge-count vector with the incoming edge-count vector and normalizing the counts by the total number of nodes in the super-nodes, i.e., $ec_i=\left[\frac{ic^i_0}{1000}, \frac{ic^i_1}{1000}, \dots , \frac{ic^i_{82}}{168}, \frac{og^i_0}{1000}, \frac{og^i_1}{1000}, \dots , \frac{og_{82}^i}{168}\right]$.
\item \textbf{Feature Vectors Creation}: Final feature-vector of $node_i$, $f_i$, is obtained by augmenting the edge-count vector $ec_i$ with a 10 dimensional random-vector $r^i$ such that

$f_i=\left[\frac{ic^i_0}{1000}, \dots , \frac{ic^i_{82}}{168}, \frac{og^i_0}{1000}, \dots , \frac{og^i_{82}}{168}, r^i_0, \dots , r^i_9\right]$,$r^i_j\in[0,1] \forall j\in[0,9]$. The random-vector is introduced to ensure different feature vectors for different nodes in case the edge-count vectors of two or more nodes are the same. Thus, the initial feature-vectors of the nodes are 176 dimensional.
\end{enumerate}

%Table \ref{tab:Slashdot_PRF} showcases the efficacy of our proposed framework, as demonstrated by its precision, recall, f1-score, and accuracy, in directed edge prediction task on the Slashdot dataset. 
Table \ref{tab:SocNet_compare} presents the results on the 2 Social Network datasets and it shows that our proposed framework achieves SOTA accuracy on both the datasets.

\input{Latex/Tabledef/socnet_res_cmp_tab}

% \madhu{Table \ref{tab:Slashdot_PRF} presents the performance of our proposed model (in terms of precision, recall, f1-score and accuracy) on the directed edge prediction task on the Slashdot dataset. Table \ref{tab:SocNet_compare} compares our model's performance (in terms of accuracy) on the Social Network datasets to other well known methods. Table \ref{tab:SocNet_compare} shows that, our model produces SOTA performance on the Slashdot dataset for directed edge prediction task.}

% \sudip{(Similarly, use $og_k^i$ or $og_{i_k}$)}

% \sudip{(Use $ic_k^i$ or $ic_{i_k}$ or simply $ic_{ik}$; $i$ should be there somewhere in every component to represent that it is part of $i^{th}$ node)}