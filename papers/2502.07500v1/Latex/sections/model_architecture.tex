 

%\subsection{Data Representation and Feature Construction}
% \madhu{}
 


\subsection{Encoder-Decoder Model}
\label{EandD}
% The proposed model consists of an encoder - decoder based architecture. An encoder module transforms the input data or parts of it, into a dense vector representation. This dense vector representation captures important patterns or features from the input data. This compressed representation \textcolor{red}{typically has lower dimensions} than the original input, which helps in reducing storage requirements as well as the computational costs, while extracting meaningful representations for downstream tasks.
% A decoder uses the dense representation generated by the encoder to produce an output from it. The decoder reverses the process of the encoder, transforming the compressed representation back into a format that can be interpreted as the output of the desired task. The output can be in the form of images, audio signals, textual information, or any other relevant format depending on the application. The overall architecture of proposed framework has been depicted in Figure~\ref{fig:arch}.
The proposed model uses an encoder-decoder architecture. The encoder transforms the input into a dense vector that captures key features, typically with lower dimensions to reduce storage and computational costs. The decoder then uses this dense representation to generate the desired output in the appropriate format.The overview of our proposed framework namely UGN has been depicted in Figure~\ref{fig:arch}.
\input{Latex/FigDef/architecture}

\para{Encoder}
\label{para:enc}
The encoder module uses a sequence of Graph Convolutional Network (GCN) \cite{4700287} layers to encode meaningful representations of nodes. It takes the input feature matrix and the adjacency matrix as input and generates the latent space representation of the nodes as output. The input graph $G$ has $N$ nodes and $C$ number of features for every node. Therefore, the input feature matrix $F$, which stores feature vectors of every node as its row, is of size $N \times C$.
\begin{comment}
\begin{gather}
F = \begin{bmatrix}
F_{0,0} & F_{0,1} & F_{0,2} & \dots & F_{0,C-1}\\
F_{1,0} & F_{1,1} & F_{1,2} & \dots & F_{1,C-1}\\
\vdots&\ddots&&&\vdots\\
F_{N-1,0} & F_{N-1,1} & F_{N-1,2} & \dots & F_{N-1,C-1}
\end{bmatrix}
\end{gather}
\end{comment}
Adjacency matrix of $G$ is $A$, with dimensions $N \times N$. Let, $I$ be an identity matrix of size $N \times N$. 
\begin{comment}
\begin{equation}
\widetilde{A} = A + I
\end{equation}
\end{comment}

 New adjacency matrix $\widetilde{A}$ introduces self-loop to all the nodes in the graph, i.e., $\widetilde{A} = A + I$, to avoid self-information loss in the message passing operation.
Suppose, $D$ is the degree matrix with all non-diagonal elements being zero and diagonal elements representing degree of the nodes i.e., $D_{i,i}$ measures the degree of $i^{th}$ node including self-connections.
\begin{comment}
\begin{equation}
\begin{split}
\widehat{A} & = D^{-\frac{1}{2}}\times \widetilde{A} \times D^{-\frac{1}{2}}\\
& = \begin{bmatrix}
\frac{1}{\sqrt{D_{0,0}}} & 0 & 0 & \dots & 0\\
0 & \frac{1}{\sqrt{D_{1,1}}} & 0 & \dots & 0\\
\vdots && \ddots && \vdots\\
0 & 0 & 0 & \dots & \frac{1}{\sqrt{D_{N-1,N-1}}}
\end{bmatrix} \times 
\begin{bmatrix}
1 & A_{0,1} & A_{0,2} & \dots & A_{0,N-1}\\
A_{1,0} & 1 & A_{1,2} & \dots & A_{1,N-1}\\
\vdots && \ddots && \vdots \\
A_{N-1,0} & A_{N-1,1} & A_{N-1,2} & \dots & 1
\end{bmatrix} \times
D^{-\frac{1}{2}}
\end{split}
\end{equation}
\end{comment}

\vspace*{-\baselineskip}
\begin{equation}
\widehat{A} = D^{-\frac{1}{2}}\times \widetilde{A} \times D^{-\frac{1}{2}}
\end{equation}
%\vspace*{-\baselineskip}


\begin{comment}
\textcolor{violet}{The scale factor of $\widetilde{A}$ could be only $D^{-1}$ making $\widehat{A} = D^{-1}\times \widetilde{A}$. From this perspective, each row $i$ of $\widetilde{A}$ will be scaled by $D_{i,i}$. If we scale each row $i$ by $D_{i,i}$, intuitively we can understand that we should do the same for its corresponding column too. Mathematically we are scaling $\widetilde{A}_{i,j}$ only by $D_{i,i}$. We are ignoring the $j$ index. So, we should scale $\widetilde{A}_{i,j}$ by both $D_{i,i}$ and $D_{j,j}$ making $\widehat{A} = D^{-1}\times \widetilde{A}\times D^{-1}$. Here we actually normalize twice, one time for the row, and another time for the column. It would make sense if we rebalance by modifying $\left(D_{i,i}\times D_{j,j}\right)$ to $\sqrt{D_{i,i}\times D_{j,j}}$. In other words, instead of using $D^{-1}$ we use $D^{-\frac{1}{2}}$. So, we further alter the formula to $\widehat{A} = D^{-\frac{1}{2}}\times \widetilde{A} \times D^{-\frac{1}{2}}$.}
\end{comment}

If the output feature matrix $F_{h_1}$ is of size $N \times H_1$ and the trainable weight matrix of the GCN layer, $W^{(0)}$, is of size $C \times H_1$, then $F_{h_1} = ReLU(\widehat{A} \times F \times W^{(0)})$.
%\begin{equation}
%F_{h_1} = ReLU(\widehat{A} %\times F \times W^{(0)})
%\end{equation}
Similarly, for a subsequent GCN layer, which takes the the edge index and the output feature matrix $F_{h_1}$ from the previous layer as input and generates output feature matrix $F_{h_2}$ of size $N \times H_2$ using trainable weight matrix $W^{(1)}$ of size $H_1 \times H_2$, we can state,
\begin{equation}
\begin{split}
F_{h_2} & = ReLU(\widehat{A} \times F_{h_1} \times W^{(1)})\\
& = ReLU(\widehat{A} \times ReLU(\widehat{A} \times F \times W^{(0)}) \times W^{(1)})
\end{split}
\end{equation}
There are $k$ GCN layers in the encoder. Consequently, the final latent space representation of nodes is given by $F_{h_k}$ of size $N \times H_k$, where $H_k$, represented as $L$ in the rest of the paper, is the latent dimension. These latent representations are passed to the decoder block for solving node and edge classification tasks.


\para{Decoder}
\label{par:decoder}
The decoder consists of two dimensional convolutional layers\cite{fukushima1980neocognitron}, max-pooling layers and linear layers. Graph problems may require the decoder to predict whether an edge $e$ incident on nodes $u$ and $v$ exists (link prediction), or predicting the class label attached to it (edge classification). For solving such problems, let the latent vectors for the nodes $u$ and $v$ be $l_u$ and $l_v$, respectively.
\begin{equation}
\begin{split}
l_u & = \left[ F_{h_k(u,0)},  F_{h_k(u,1)}, \dots ,  F_{h_k(u,L-1)}\right]
= \left[ u_0, u_1, \dots , u_{L-1}\right]\\
l_v & = \left[ F_{h_k(v,0)},  F_{h_k(v,1)}, \dots ,  F_{h_k(v,L-1)}\right]
= \left[ v_0, v_1, \dots , v_{L-1}\right]
\end{split}
\end{equation}
An intermediate matrix $M$ of size $L \times L$ is formed 
%as $M=l_v^T \times l_u$,
following equation (\ref{eqn:intm}) 
using the products of all pairs of elements, where $M_{ij}$ = $v_j \times u_i$, $i$, $j$ are integers in [0, $L$-1]. The pairwise multiplication allows all possible latent feature pairs from the source node and target node to be considered as input, while concatenation only allows to consider them separately. In terms of our model, pairwise multiplication helps revealing inter-feature dependency or relationship between the source nodes and target nodes, whereas concatenation reveals inter-class dependency. 
Let the model parameters be $\Theta$ and output be $u$ and $v$. While training the model, if $u$ and $v$ are computed separately, then their individual losses can be back-propagated as $\frac{\partial \left(u+v\right)}{\partial \Theta}=\frac{\partial \left(u\right)}{\partial \Theta} + \frac{\partial \left(v\right)}{\partial \Theta}$. Instead, if $u\cdot v$ is computed, then each loss component is supported by the counterpart while back-propagating as $\frac{\partial \left(u\cdot v\right)}{\partial \Theta}=u\cdot\frac{\partial \left(v\right)}{\partial \Theta} + v\cdot\frac{\partial \left(u\right)}{\partial \Theta}$ .

\begin{comment}
\begin{equation}
\begin{split}
\frac{\partial \left(u+v\right)}{\partial \Theta}&=\frac{\partial \left(u\right)}{\partial \Theta} + \frac{\partial \left(v\right)}{\partial \Theta}\\
\frac{\partial \left(u\cdot v\right)}{\partial \Theta}&=u\cdot\frac{\partial \left(v\right)}{\partial \Theta} + v\cdot\frac{\partial \left(u\right)}{\partial \Theta}
\end{split}
\label{eqn:partial}
\end{equation}
\end{comment}

%\sudip{(Just double checking, does M really look like this in your implementation, i.e. column for the source node and row for the target node, or the other way round?)}

\begin{comment}

\begin{gather}
M = \begin{bmatrix}
u_0*v_0 & u_1*v_0 & u_2*v_0 & \dots & u_{L-1}*v_0\\
u_0*v_1 & u_1*v_1 & u_2*v_1 & \dots & u_{L-1}*v_1\\
\vdots && \ddots && \vdots \\
u_0*v_{L-1} & u_1*v_{L-1} & u_2*v_{L-1} & \dots & u_{L-1}*v_{L-1}
\end{bmatrix}
\label{eqn:intm}
\end{gather} 

\end{comment}

\vspace*{-\baselineskip}
\begin{equation}
M=l_v^T \times l_u
\label{eqn:intm}
\end{equation}


The internal feature representation, denoted by $M\in \mathbb{R}^{L \times L}$, is considered as a one-channel $L \times L$ image to enhance the performance of the downstream classification tasks. The image-like representation of $M$ allows for the utilization of convolution and max-pooling operations to generate significant abstract representations. To achieve this, the internal input representation ($M$) is passed into the required convolution layer, followed by the max-pooling operation, as we treat $M$ as an image. To obtain the most crucial abstract feature representation, we repeat the convolutional and max-pooling operations for $s$ times. The abstract output representation of the max-pooling layer is subsequently passed into the $t$ linear layers followed by the necessary activation function to predict the required edges and their corresponding types.
%This matrix $M$ is considered as a one-channel $L \times L$ image and the downstream classification tasks are performed on this image using $s$ two dimensional convolutional layers, max-pooling and $t$ linear layers.
\begin{comment}
\begin{align}
\begin{aligned}
J_1 & = ReLU(MaxPool(Conv2D_1(M)))\\
J_2 & = ReLU(MaxPool(Conv2D_2(J_1)))\\
\vdots \\
J_s & = ReLU(MaxPool(Conv2D_s(J_{s-1})))
\end{aligned}
&&
\begin{aligned}
Y_0 & = Flatten(J_s)\\
Y_1 & = ReLU(Linear_1(Y_0))\\
\vdots \\
Y_t & = Softmax/Sigmoid(Linear_t(Y_{t-1}))
\end{aligned}
\end{align}
\end{comment}
The values of $k$, $s$ and $t$ range from three to five depending on the size of the graph and input feature dimension of the nodes. While working with node classification problem, the intermediate matrix $M$ is formulated by replacing $l_v$ with $l_u$ in the equation (\ref{eqn:intm}) for classifying node $u$. Finally, we train our model in an end-to-end fashion, using categorical cross-entropy loss function as shown in equation (\ref{eqn:sup_loss}), where $\mathcal{L}_s$ is the supervised loss, $V$ is the node set, $c$ is the number of classes, $V=V_k \cup V_u$, where $V_k$ is the set of nodes with known node-labels and $V_u$ is the set of nodes with unknown node-labels (generally, $V_u=\emptyset$), $N$, $N_k$ and $N_u$ are the number of nodes in the node sets $V$, $V_k$ and $V_u$, respectively. Furthermore, $T_{N\times c}$ is the one-hot encoded ground-truth labels of all the nodes, $S_{N\times c}$ is the logits of all the nodes, and $O_{N\times c}$ is the transformer output for all the nodes. Similarly, for edge classification, $V$ is replaced by $E$ in the equation (\ref{eqn:sup_loss}), where $E$ is the edge set.
\vspace{-.2cm}
\begin{equation}
\begin{split}
\mathcal{L}_s &= -\sum_{v\in V_k}\sum_{i=0}^{c-1}T[v][i]\times\log (S[v][i])\\
&= -\sum_{v\in V_k}\sum_{i=0}^{c-1}T[v][i]\times\log\left(\frac{e^{O[v][i]}}{\sum_{j=0}^{c-1}e^{O[v][j]}}\right)
\end{split}
\label{eqn:sup_loss}
\end{equation}



%\sout{The loss function is taken as binary cross entropy or categorical cross entropy depending on the problem type i.e., binary classification (e.g., predicting presence of edge between two nodes) or multi class classification (e.g., edge type prediction) respectively.} 

\subsection{Special Cases of our Proposed Approach}
\label{sec:SpecialCases}
In this section, we investigate whether integration of diverse components into our proposed model can facilitate its applicability to a wide range of graph related problems.
\subsubsection{Supernode Feature}
\label{super}
% \textcolor{magenta}{Some datasets do not provide node features. Most common practice in such scenarios is to initialize node features is to take one hot encoding of the nodes. But when the graph size is large, using one hot encoding as initial node features is not suitable because the RAM size is not enough to accommodate the entire computational graph. So, a smart technique called supernode is proposed to initialize node features. Let, there be $N$ nodes in the graph. From the $N$ nodes, $S$ supernodes are formed.
% \begin{equation}
% \begin{split}
% supernode_0 &= \{ node_0, node_1, \dots, node_{k-1}\}\\
% \vdots&\\
% supernode_i &= \{node_{ik}, node_{ik+1}, \dots, node_{ik+m-1}\}\\
% \vdots&\\
% supernode_{S-1} &= \{node_{N-m}, node_{N-m+1}, \dots, node_{N-1}\}
% \end{split}
% \end{equation}
% If $N$ is divisible by $S$, then $i\in[0,S-1]$ and $k=m=\frac{N}{S}$ i.e. all supernodes are of same size. Otherwise, $0<i<S-1$ and $N=k\times i+m\times (S-i)$ i.e. $i$ supernodes are of size $k$ and $S-i$ supernodes are of size $m$. For each node $node_i$, a connection vector $v^i$ is formed. $v^i=[v^i_0, v^i_1, \dots, v^i_{S-1}]$, where $v^i_j=\frac{\text{number of connections between } node_i \text{ and } supernode_j}{\text{number of nodes in } supernode_j}$. If the graph is bidirectional, then this computation is enough to encode connections to supernodes with $node_i$. If the graph is directed, then incoming and outgoing connection vectors are formed separately and concatenated to encode connections to supernodes with $node_i$.}


Many existing datasets (e.g., Slashdot, BioSNAP-DDI, BioSNAP-DTI, etc.) used for solving graph problems generally suffer from the absence of node features. The prevalent approaches \cite{wang2020edge2vec,kishan2021predicting}, initialize such features with one-hot representation of the nodes. However, when the size of the graph is substantially large, using one-hot encoding as initial node feature becomes unsuitable due to high memory requirement. To mitigate this problem, we introduce a novel component, namely \textit{supernode} feature, to accommodate large graphs into neural network frameworks in low-resource environment. Supernodes are mutually exclusive and exhaustive subsets of the node set $V$. Suppose, a graph $G$ contains $N$ number of nodes and $s$ number of supernodes.
\begin{comment}
\begin{equation}
\begin{split}
supernode_0 &= \{ node_0, node_1, \dots, node_{k-1}\}\\
\vdots&\\
supernode_i &= \{node_{ik}, node_{ik+1}, \dots, node_{ik+m-1}\}\\
\vdots&\\
supernode_{S-1} &= \{node_{N-m}, node_{N-m+1}, \dots, node_{N-1}\}
  \end{split}
\label{eqn:supnode}
\end{equation}
\end{comment}
%where $k$ and $m$ are the sizes of the supernodes. 
If $N$ is divisible by $s$, then each supernode consists of exactly $k=\frac{N}{s}$ number of nodes. Otherwise, some supernodes (say, $i$ many) have $k$ nodes while the rest have $m$ nodes, such that $N=k\times i+m\times (s-i)$. Let, $S_j$ be a supernode set, where $j$ is an integer in [0, s-1]. For each node $v \in V$, we compute a connection vector $d^i=[d^v_0,d^v_1,â€¦,d^v_{s-1}]$, where $d^v_j$ is the normalized number of connections between $v$ and $S_j$, i.e.,\\ $d^v_j$ = $\frac{\text{\# of connections between $v$ and $S_j$}}{ \text{\# of nodes in $S_j$}}$. Given the graph is undirected, this vector captures the connection strength of a node to each supernode. 

\begin{comment}
If the graph is directed, we concatenate two vectors - the incoming connection vector {$\overleftarrow{ v^i}$} and the outgoing connection vector $\overrightarrow{ v^i}$, i.e., $v^i=[\overleftarrow {v^i},\overrightarrow{v^i}]$. 
\end{comment}
%Fig. \ref{supernodeConstruction} illustrates the supernode feature.

Although many graph coarsening methods are known in classical graph problems, such techniques have not been popularly used in neural network frameworks. In our model, we apply coarsening of large graphs through the supernode feature to alleviate memory requirement and empirically show that it contributes to additional performance gain.

%\input{Latex/Tabledef/supernode_form_fig_tab}

% \textcolor{magenta}{Using this technique, there is a possibility of duplicate connection vector for two or more nodes. To avoid the duplicate node features, a $R$ dimensional random vector $r^i=[r^i_0, r^i_1, \dots, r^i_{R-1}], r^i_j\in[0,1] \text{ or } r^i_j\in[-1,0] \forall j\in[0,R-1]$, is concatenated with the connection vector of each node $node_i$.}



\subsubsection{Unsupervised Loss for Semi-supervised Task}

\label{unsupervised}
A popular challenge in neural graph analysis is posed by the semi-supervised task of community detection, where only a few nodes are labeled with community labels \cite{zachary1977information}. In this scenario, a supervised classification loss, when employed in conjunction with encoder-decoder framework, fails to capture adequate structural information of the entire graph. To leverage the efficacy of the unlabeled nodes, we introduce an unsupervised loss function ($\mathcal{L}_s$) that supplements the supervised loss function ($\mathcal{L}_s$, as in equation (\ref{eqn:sup_loss})).
The unsupervised component of the loss function, $\mathcal{L}_u$, is defined as in equation (\ref{eqn:unsup_loss}), where $c$ is the number of classes, $O_{N\times c}$ is the model output of all the nodes, and $E$ is the edge set of the graph. 

\vspace*{-\baselineskip}
\begin{equation}
\mathcal{L}_u = \sum_{(u,v) \in E}\sum_{i=0}^{c-1}\frac{1}{c}\times(O[u][i]-O[v][i])^2
\label{eqn:unsup_loss}
\end{equation} 
\vspace{-.3cm}

The loss function has been designed based on the assumption that densely connected nodes tend to belong to the same community. 
\begin{comment}
Therefore, community-wise membership distribution should be similar for two connected nodes.
\end{comment}
This assumption leads to the expectation that MSE-loss between the transformer-outputs for two connected nodes should converge to zero. The final loss function $\mathcal{L}$ is defined as $\mathcal{L} = \mathcal{L}_s + \mathcal{L}_u$. 
%\hl{Experimental results show that utilization of $\mathcal{L}_u$ along with $\mathcal{L}_s$ helps our proposed neural framework significantly to produce state-of-the-art (SOTA) results in the semi-supervised graph analysis task of community detection.}

\begin{comment}
\begin{equation}
\begin{split}
\mathcal{L} &= \mathcal{L}_s + \mathcal{L}_u\\
&= \mathcal{L}_s + \sum_{(u,v) \in E}\sum_{c=0}^{C-1}\frac{1}{C}\times(O[u][c]-O[v][c])^2
\end{split}
\label{eqn:unsup_loss}
\end{equation} 
\end{comment}

The problem of community detection can be compared to an environment where nodes are balls in a multidimensional space, connected via springs in place of edges. In this setup, the nodes with community labels have fixed positions. Here, the unsupervised loss $\mathcal{L}_u$ is the potential energy stored in the springs. The model tries to minimize this energy to reach equilibrium. 




% There are various tasks where the number of samples, for which sample-labels are known, is very less. One such example is the task of community detection where the belongingness of only one representative from each community is revealed. For such cases, only the supervised component of the loss function (e.g. categorical cross-entropy for multi-class classification) is not good enough for the model to learn structural information of the entire graph. Therefore, to utilize the known structural information, i.e. the edges of the graph, an unsupervised component is introduced in the loss function. Let, $N$ be the set of all nodes, $E$ be the set of all edges and $N_k$ be the set of nodes for which community-label is known for the community detection task.


\subsubsection{Mean Target Connectivity Matrix (MTCM) Rrepresentation for Complete Graphs}
\label{mtcm}
In the graph translation task, it is generally observed that when both the source and target graphs are complete, the corresponding adjacency matrices are populated by a correlation-based connectedness score. This score ranges from -1 to 1 and is assigned to all possible pairs of nodes in the graph. The adjacency matrix for the source graph is referred to as the source connectivity matrix ($S$) representation while that of the target graph is known as the target connectivity matrix ($T$). Being complete graphs, the number of edges escalates significantly to the maximum possible connections between the nodes, which imparts a direct effect on the training duration of the model. To address this issue and leverage the similarity present in the target connectivity matrices, we propose a mean target connectivity matrix (MTCM) representation component to reduce the computational resources. The MTCM is constructed using equation (\ref{eqn:mtm}) where $\overline{T}$ represents the required MTCM, $N$ denotes the order of the graphs in the dataset, and $|D|$ corresponds to the total number of source-target connectivity pairs within the dataset.

% \textcolor{red}{While using mean, other graph patterns leave impact on one's prediction.}

\vspace*{-\baselineskip}
\begin{equation}
\overline{T}[i][j]=\frac{1}{\mid D\mid}\sum_{T\in Dataset}T[i][j],\forall i,j\in[0,N-1]
\label{eqn:mtm}
\end{equation}

Subsequently, for each target connectivity matrix, a difference matrix $Dif$ is created by subtracting MTCM from the corresponding target connectivity matrix as in equation (\ref{eqn:diff_matrix}), where $Dif_n$ denotes the difference matrix for the target connectivity matrix $T_n$. The model is trained in an end-to-end fashion to predict $Dif_n$ by taking source connectivity matrix (i.e., $S_n$) as the required input representation.

\vspace*{-\baselineskip}
\begin{equation}
Dif_n[i][j]=T_n[i][j]-\overline{T}[i][j], \forall i,j\in[0,N-1], \forall n\in[0,\mid D\mid -1]
\label{eqn:diff_matrix}
\end{equation}

The predicted connectivity matrix is obtained by adding the MTCM to the predicted difference matrix from a source connectivity matrix. Using this method, the model can exploit the similarity among the target connectivity matrices in its prediction and helps improve the evaluation scores.









% In the graph translation task, it is generally observed that when both the source and target graphs are complete, the corresponding adjacency matrices are populated by a correlation-based connectedness score. This score ranges from -1 to 1 and is assigned to all possible pairs of nodes in the graph. The adjacency matrix for the source graph is referred to as the source connectivity matrix ($S$) representation while that of the target graph is known as the target connectivity matrix ($T$). Being complete graphs, the number of edges escalates significantly to the maximum possible connections between the nodes, which imparts a direct effect on the training duration of the model. To address this issue and leverage the similarity present in the target connectivity matrices, we propose a mean target connectivity matrix  (MTCM) representation component to reduce the computational burden. \textcolor{red}{While using mean, other graph patterns leave impact on one's prediction.} The MTCM is constructed using equation (\ref{eqn:mtm}) where $\overline{T}$ represents the required MTCM, $N$ denotes the order of the graphs in the dataset, and $|D|$ corresponds to the total number of source-target connectivity pairs within the dataset.

% \vspace*{-\baselineskip}
% \begin{equation}
% \overline{T}[i][j]=\frac{1}{\mid D\mid}\sum_{T\in Dataset}T[i][j],\forall i,j\in[0,N-1]
% \label{eqn:mtm}
% \end{equation}

% Subsequently, for each target connectivity matrix, a difference matrix $Dif$ is created by subtracting MTCM from the corresponding target connectivity matrix as in equation (\ref{eqn:diff_matrix}), where $Dif_n$ denotes the difference matrix for the target connectivity matrix $T_n$. The model is trained in an end-to-end fashion to predict $Dif_n$ by taking source connectivity matrix (i.e., $S_n$) as the required input representation.

% \vspace*{-\baselineskip}
% \begin{equation}
% Dif_n[i][j]=T_n[i][j]-\overline{T}[i][j], \forall i,j\in[0,N-1], \forall n\in[0,\mid D\mid -1]
% \label{eqn:diff_matrix}
% \end{equation}


% The predicted connectivity matrix is obtained by adding the MTCM to the predicted difference matrix from a source connectivity matrix. Using this method, the model can exploit the similarity among the target connectivity matrices in its prediction and helps improving the evaluation scores.
