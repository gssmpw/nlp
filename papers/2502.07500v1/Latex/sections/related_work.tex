\citet{guo2019deep} proposed a generic, end-to-end framework for joint node and edge attributes prediction, which is a need for realworld applications such as malware confinement in IoT networks and structural-to-functional network translation. Their novel edge translation path is generic, which is proven to be a generalization of the existing topology translation models. Authors also proposed a spectral graph regularization based strategy on their non-parametric graph Laplacian in order to learn and maintain the consistency of the predicted nodes and edges.

\citet{bruna2014spectral} first introduced the spectral graph convolutional neural networks, and then it was extended by \citet{defferrard2016convolutional} using fast localized convolutions, which is further approximated by \citet{DBLP:conf/iclr/KipfW17} for an efficient architecture for a semi-supervised setting. \citet{li2018diffusion} proposed a Diffusion Convolution Recurrent Neural Network (DCRNN) for traffic forecasting which incorporates both spatial and temporal dependency in the traffic flow. \citet{Yu2018SpatioTemporalGC} formulated the node attributes prediction problem of graphs based on the complete convolution structures. \citet{9737289} tackled graph topology translation problem by proposing a generative model consisting of a graph translator with graph convolution and deconvolution layers and a new conditional graph discriminator. \citet{sun2019graph} proposed a graphRNN based model which generates a graph’s topology based on another graph.

\citet{wang2020edge2vec} proposed edge-based graph embedding (edge2vec) to map the edges in social networks directly to low-dimensional vectors considering an important property of social networks, i.e., the networka are sparse, and hence the average degree of nodes is bounded. Edge2vec takes both the local and the global structure information of edges into consideration to preserve structure information of embedded edges as much as possible. To achieve this goal, edge2vec ingeniously combines the deep autoencoder and Skip-gram model through a well-designed deep neural network.

\citet{perozzi2014deepwalk} proposed DeepWalk to employ the truncated random walks and obtain the context information of nodes. \citet{grover2016node2vec} proposed Node2vec to improve DeepWalk through replacing the truncated random walks with a new search strategy which combines the property of Breadth First Search (BFS) and Depth First Search (DFS). \citet{tang2015line} proposed LINE to consider the neighborhood of a node as its context. \citet{cao2015grarep} proposed GraRep to extend LINE by considering the indirect neighbors, but it suffers from the length of embedding vectors. Discriminative Deep Random Walk (DDRW) and max-margin DeepWalk (MMDW) proposed by \citet{li2016discriminative} and \citet{tu2016max} respectively try to jointly optimize the classifier and the embedding model. RSDNE proposed by \citet{wang2018rsdne} approximately guarantees intra-class similarity and inter-class dissimilarity, obtaining favorable performance in both the general and zero-shot graph embedding problems.

\citet{waskiewicz2012friend} divided the static community detection methods into four broad categories as spectral methods, methods based on statistical inference, methods based on optimization and methods based on dynamics. Spectral methods use spectral properties like eigen value spectrum of the matrix representations (e.g., adjacency matrix, Laplacian matrix, modularity matrix etc.) of networks to detect communities.  The works of \citet{newman2006finding}\cite{newman2013spectral} can be given as examples of spectral methods. Methods based on statistical inference generally adopts an ordinary approach to fit data to a generative network model. Most popular generative model with communities for networks is Stochastic Block Model (SBM). The works of \citet{come2015model}, and \citet{newman2016estimating} can be given as examples of statistical inference-based community detection methods. Methods based on optimization try to find a maximum or minimum of a quality function that shows the quality of community structure. Most popular quality function is modularity proposed by \citet{newman2004finding}. Modularity is a concept depending on the maximization of difference between actual network and another form of actual network that have randomly destroyed community structure. The methods proposed by \citet{clauset2005finding}, and \citet{lancichinetti2011limits} can be given as examples of methods based on optimization. Methods based on dynamics uses running dynamics of the networks like diffusion, random walk and spin dynamics to detect community structure in a network. The works of \citet{reichardt2006statistical}, and \citet{rosvall2008maps} can be given as examples for methods based on dynamics.

\begin{comment}
\textcolor{magenta}{The matrix completion problem has been studied using GNNs. \citet{monti2017geometric} developed a multi-graph CNN (MGCNN) model to extract user and item latent features from their respective nearest-neighbor networks. \citet{berg2017graph} proposed graph convolutional matrix completion (GC-MC) which directly applies a GNN to the user-item bipartite graph to extract user and item latent features using a GNN. Although using GNNs for matrix completion, all these models are still transductive – MGCNN requires graph Laplacians which do not generalize to new graphs, while GC-MC uses one-hot encoding of node IDs as initial node features, thus cannot generalize to unseen users/items. A recent inductive graph-based recommender system, PinSage (\citet{ying2018graph}), uses node content as initial node features (instead of the one-hot encoding in GC-MC), and is successfully used in recommending related pins in Pinterest. \citet{zhanginductive} proposed an Inductive Graph-based Matrix Completion (IGMC) model which trains a graph neural network (GNN) based purely on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps those subgraphs to their corresponding ratings.}
\end{comment}

\textcolor{magenta}{Knowledge graph embedding is an active research topic with many different methods. They can be categorized based on the interaction mechanisms and the computation of the matching score. Tensor-decomposition-based models adapt tensor representation formats such as CP format, Tucker format, and block term format to represent the knowledge graph data tensor (\citet{kolda2009tensor}). They make up most of recent SOTA models such as ComplEx (\citet{trouillon2016complex}), SimplE (\citet{kazemi2018simple}), TuckER (\citet{balavzevic2019tucker}), and MEIM (\citet{DBLP:conf/ijcai/TranT22}). They often achieve good trade-off between efficiency and expressiveness. Neural-network-based models use a neural network to compute the matching score, such as ConvE (\citet{dettmers2018convolutional}) using convolutional neural networks, CompGCN (\citet{vashishthcomposition}) using graph convolutional networks. These models are generally more expensive but not always get better results than tensor-decomposition-based models. Translation-based models use geometrical distance to compute the matching score, with relation embeddings act as the translation vectors, such as TransE (\citet{bordes2013translating}). These models are efficient and intuitive, but they have limitations in expressive power (\citet{kazemi2018simple}). There are several ways to use orthogonality in knowledge graph embedding, such as RotatE (\citet{sunrotate}) using complex product, Quaternion (\citet{tran2019analyzing}) and QuatE (\citet{zhang2019quaternion}) using quaternion product, GCOTE (\citet{tang2020orthogonal}) using the Gram Schmidt process, and RotH (\citet{chami2020low}) using the Givens matrix.}

Our model is unique in terms of scalability and genericness.
\textcolor{magenta}{
%\begin{enumerate}
%\item 
(i) We experimented with smaller graphs containing 20-60 nodes as well as large graphs containing 20K-80K nodes using the same model architecture and obtained SOTA or near-SOTA results in both cases.\\
%\item 
(ii) Our model can smoothly transit from supervised learning domain to semi-supervised learning domain only by adding an extra component in the loss function, keeping the model architecture same. We applied our model for supervised tasks where number of known labels of nodes or edges in the training set is much larger or at least similar to the test set, and semi-supervised tasks where number of known labels in the training set is much less than the test set, producing SOTA or near-SOTA results for both the tasks.
%\end{enumerate}
}