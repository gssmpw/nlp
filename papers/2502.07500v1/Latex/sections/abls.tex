% \input{Latex/Tabledef/abls_iot60_slashdot_hcp_karate_tab}

\input{Latex/FigDef/ablation}
To examine the impact of the intermediate matrix representation and the convolution layer-based decoder on the overall performance of the model, we conduct an ablation study using the IoT-60 dataset.
We follow the same pathway of existing encoder-decoder based graph neural network literature, where the output representation from the encoder module is passed to the decoder module without forming an additional intermediate matrix representation for the decoder module. More specifically, latent vector representation of the nodes from the encoder block are passed as input to the decoder for the node classification task. For the edge classification, initially an edge vector \(e_{i,j}\) is formed by element-wise multiplication of the latent vectors (\(l_i, l_j\)) of the two nodes for each edge, i.e., 
%\[ 
$e_{i,j} = [l_{i_0} \cdot l_{j_0}, l_{i_1} \cdot l_{j_1}, \dots, l_{i_{L-1}} \cdot l_{j_{L-1}}]$,  
%\]
and is passed to the decoder as input. Additionally, to investigate the contribution of different blocks in comparison to the convolutional layers, we employ different existing neural network blocks such as multi-layer-perceptron (MLP), long-short-term-memory (LSTM) and bidirectional-LSTM (BiLSTM). The results of these experiments are reported in Figure~\ref{fig:ablation}a. 
From Figure~\ref{fig:ablation}a, we can observe that  UGN outperforms other models by significant margins in terms of accuracy on the IoT-60 dataset.

%We also conducted an experiment to investigate the feasibility of utilizing supernode based features for our downstream task to process the large volume of graph. 
To assess the impact of the supernode based features, we conducted an experiment on the Slashdot dataset with randomly initialized node features having values between 0 and 1. All other necessary hyper-parameters were kept the same to measure the contribution of the supernode based features. From Figure~\ref{fig:ablation}b, 
we can observe an edge accuracy of 0.80 using this setup, i.e., without utilizing the supernode features, which is the same as obtained with edge2vec~\cite{wang2020edge2vec} (cf. Table \ref{tab:SocNet_compare}).
Thus our baseline model without the supernode features performs at par with edge2vec~\cite{wang2020edge2vec} on the directed link prediction task on the Slashdot dataset and the supernode based features bring in the additional improvements.
%examine the significance of the mean target connectivity matrix (MTCM) for fully connected graphs, the MTCM component was removed from the training process.

To examine the significance of MTCM for complete graphs, we conducted another experiment in which the MTCM component was removed from the training process. 
We modified the training procedure of the model on the HCP dataset, where the model learned to predict the FC matrix directly from the SC matrix of a subject. 
In this experimental setup, we achieved an average pearson correlation of 0.36 for the motoring task on the HCP dataset, as mentioned in Figure~\ref{fig:ablation}c, keeping all other parameters and hyper-parameters the same. 
Thus, our baseline architecture without the MTCM component performs at par with the previous SOTA model, GT-GAN \cite{9737289}, on the HCP dataset. Introduction of the MTCM module into our framework helps in significant performance gain and producing SOTA performance.  

%To conclude more concretely, we additionally conduct additional experiments to investigate the feasibility of utilizing the mean target connectivity matrix (MTCM) for fully connected graphs, the MTCM component was removed from the training process for the HCP dataset. 
%In contrast to that by introducing MTCM module into our framework helps significantly to acheive state-of-the-art (SOTA) performance in terms of Pearson Correlation Coefficient score. 

% whereas for the motoring task on the test set keeping all other parameters and hyper-parameters as same. Thus, our baseline framework without previous MTCM module acheive  SOTA model -GT-GAN~\cite{9737289} on the HCP dataset, and the huge performance gain on the HCP dataset is attributed by the incorporation of MTCM.
%\input{Latex/Tabledef/abls_hcp_karate_tab}

Finally, we conduct another ablation experiment to study the effect of the unsupervised component ($\mathcal{L}_u$) of the loss function (cf. equation \ref{eqn:unsup_loss}) for semi-supervised learning tasks. To carry out the experiment, we removed $\mathcal{L}_u$ from the final loss function ($\mathcal{L}$). Keeping everything else unchanged, we conducted the ablation experiment on the Zachary's karate club dataset. Our empirical findings from Figure~\ref{fig:ablation}d demonstrate that the unsupervised component ($\mathcal{L}_u$) of the final loss function ($\mathcal{L}$) helps significantly in achieving state-of-the-art (SOTA) performance on Zachary's karate club dataset for the semi-supervised learning task.
% The unsupervised component $\mathcal{L}_u$ was removed from the final loss function $\mathcal{L}$ in Equation \ref{eqn:unsup_loss}. Keeping everything else the same, we carried out the ablation experiments on the Zachary's karate club dataset and the results are reported in Table \ref{tab:abls_karate}. Results of these experiments suggest that the unsupervised component ($\mathcal{L}_u$) introduced in the loss function contributes significantly in the performance gain for semi-supervised learning tasks.


% \sudip{(Summarize the results.)}

 % We modified the training procedure of the model on the HCP dataset, where the model learned to predict the functional connectivity matrix directly from the structural connectivity matrix of a subject.