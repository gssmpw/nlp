

% \begin{itemize}
%     \item  The development and evaluation of educational interventions
%     \item Designing a guide to support human learning of AI-generated artifacts 
%     \item Design Objectives
%     \begin{itemize}
%     \item Coming up with a hierarchy or ontology
%     \item More comprehensive than anything currently out there
%     \item Concrete and identifiable categories
%     \end{itemize}
%     \item Design Process
%          \begin{itemize}
%      \item How we dealt with different pieces
%      \item How we created the ontology
%      \item Website as the primary source of info and feedback
%      \end{itemize}

%     \item  Systematic approach to investigate the effectiveness of these supports


    % Matt's comments: Almost all CHI and CSCW papers these days frame the paper around 2-4 research questions listed like

    % RQ1: What visual features influence untrained people's ability to distinguish between diffusion model generated images and real photographs? 
    
    % RQ2: How much does human curation of images generated by a diffusion model influence the apparent photorealism of diffusion models?
    
    % RQ3: How should an AI literacy guide categorize artifacts and implausibilities that emerge in near photorealistic diffusion model generated images to help people learn to identify these visual cues?
    
    % RQ4: How should an AI literacy guide categorize visual elements in real images that rarely appear in photorealistic diffusion model generated images?


% % 4. Development of the How-To Guide

   
    
    
%    % 4.1 Initial Content Creation
%    %    - Key visual cues and artifacts
%    %    - Common AI generation quirks
%    %    - Metadata analysis techniques
%    % 4.2 User Testing and Feedback
%    %    - Think-aloud protocols
%    %    - Task completion rates
%    %    - User satisfaction surveys
%    % 4.3 Refinement and Iteration
%    %    - Incorporating user feedback
%    %    - Simplifying complex concepts
   

% % 5. Guide Structure and Components
% %    - Visual examples and comparisons
% %    - Step-by-step instructions
% %    - Interactive exercises
   

% % 6. Evaluation
% %    6.1 Usability Testing
% %       - Task completion rates
% %       - Time-on-task metrics
% %       - System Usability Scale (SUS) scores
% %    6.2 Effectiveness Measurement
% %       - Pre- and post-guide knowledge assessments
% %       - Real-world application tests




% This outline aims to fit the CHI community's style and culture by:

% 1. Emphasizing user-centered design and iterative development
% 2. Including rigorous user testing and evaluation methods
% 3. Addressing broader societal implications
% 4. Focusing on accessibility and practical application
% 5. Incorporating established HCI evaluation metrics (e.g., SUS)

\subsubsection{References: \href{https://www.synthesia.io/post/the-future-of-synthetic-media}{The future of synthetic media - Synthesia}}


% Problem statement
% Media literacy, Importance of distinguishing AI-generated images from real photographs + implication on HCI
% Current challenges in image forensics \cite{farid2010image}
% Designing a guide to support human learning of AI-generated artifacts 
% Algorithmic - Human - Why Human in spite of 99\% Algorithm accuracy?
% Motivation for the taxonomy
% User-centered design approach
% Research Questions and Objectives
% Aim of the paper: Creating an accessible how-to guide


%problem statement and importance

METHDOLOGY

Our research process began with initial observations of AI-generated images and reviewing guide articles and websites, such as the NYT hosting the "Can You Detect?" section. We identified gaps in their images, which mainly featured portraits. To address this, we expanded our scope to include dynamic group photos.

Simultaneously, we explored literature for examples and discovered that the laws of physics provide classic cues in digital forensics. We incorporated sample images from diffusion models and noted the limitations of online tests. Recognizing further gaps in existing guides, we curated a diverse set of images based on our findings and developed a website for user testing.

This website allowed us to evaluate the accuracy of identifying AI-generated images and refine our guide accordingly. We iterated through multiple cycles of expansion and refinement, incorporating additional literature review and analysis. This iterative process ultimately culminated in the development of a comprehensive taxonomy



% old intro

% Rapid advancements in Generative AI have recently fueled the proliferation of AI-generated images in media. We come across these AI-generated images in a wide range of contexts.  They appear in light-hearted social media content, advertising campaigns \cite{simchon2024persuasive}, and sometimes as malicious attempts to manipulate people's political opinions such as a viral image of presidential candidate Kamala Harris posing with convicted sex offender Jeffrey Epstein \cite{Reichenbach2024}. Commercial tools like DALL-E and Midjourney allow anyone to create detailed and highly realistic images that can be indistinguishable from real photographs. As AI-generated media continues to advance and become widespread, there is growing concern for the consequences it has on promoting disinformation \cite{monteith2024artificial} as well as violating privacy and intellectual property \cite{golda2024privacy, sag2023copyright}. These concerns point to the critical need to empower humans with robust methods for distinguishing between AI-generated and real media. This need is further emphasized by recent regulatory initiatives, such as the U.S. Executive Order on Safe and Trustworthy AI \cite{ExecutiveOrder2023}, which explicitly calls for advancements in establishing authenticity and provenance.




% current situation
% using AI models to detect deepfakes


% Results 

% While median accuracy was similar for both AI-generated and real images (78.4\% (95\% CI: [77,80]) for AI-generated images vs 78\% (95\% CI: [76,80]) for real images), AI-generated images exhibited greater variability in accuracy scores (SD: 14.7\% vs 11.5\%) suggesting greater variability in participants' ability to accurately identify AI-generated images.  
% \jessica{I don't think you need this paragraph actually, it seems pretty redundant with the previous paragraph}
% \jessica{would not give both IQR and SD. Once you give SD you've given IQR so just do SD}
% Despite the wider range and greater variability in accuracy scores for AI-generated images, their first quartile ($Q_1 = 66.0\%$) was lower than that of real images ($Q_1 = 70.3\%$). This indicates that a considerable portion of AI-generated images were more challenging to classify correctly compared to real photographs. On the other hand, the third quartile for AI-generated images ($Q_3 = 86.7\%$) exceeded that of real images ($Q_3 = 82.9\%$), indicating that some AI-generated images were more consistently identifiable than even the most recognizable real photographs. 


% \jessica{why do we need to do this, and what is the scientific motivation?} 
% This resulted in a narrowing of the accuracy range for AI-generated images from 31.7\%-98.8\% to 31.7\%-90.0\%, and real photographs from 27.5\%-91.7\% to 27.5\%-89.8\%. The median accuracy for AI-generated images decreased to 75.8\% (95\% CI: [74.1, 77.5]), while for real images it remained at 77.7\% (95\% CI: [75.5, 79.8]). 
% Importantly, this exclusion reversed the trend observed in the full dataset, with real images now showing a slightly higher median accuracy. 
% \jessica{This is weird, why would I believe the new trend more given that you just told me you did a posthoc exclusion?}\negar{we mitigated the ceiling effect?} The interquartile range (IQR) for AI-generated images decreased from 20.7\% to 19.0\%, while for real images it increased slightly from 12.6\% to 12.9\% indicating that AI-generated content still exhibits greater variability in detection accuracy. \jessica{this is weird and needs to be rethought, as its not clear why we are doing this. i would keep all the images in if they were the best we could make that covered the categories. otherwise we need to make clear why it was a mistake according to our own goals to have included these images}\negar{originally we didn't have know how accurate people are. After measuring the accuracy, we decide to exclude highly accurate images. maybe for this general analysis we just need to report median, SD, etc and later on when reporting the effect of different factors, we exclude the images}

% Additionally, the first quartile ($Q_1$) for AI-generated images decreased to 64.1\% from 66.0\% in the unfiltered set, while for real images it decreased only slightly to 69.8\% from 70.3\%. Standard deviations ($\sigma = 13.7\%$ for AI-generated, $\sigma = 11.1\%$ for real) indicate that AI-generated content still exhibits greater variability in detection accuracy. 


% \begin{figure}[H]
% \centering
% \captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}
% \begin{subfigure}[t]{0.48\textwidth}
%     \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/accuracy_histograms_25plus.png}}}
%     % \caption*{\footnotesize Accuracy Distribution for \≥25 Images}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.48\textwidth}
%     \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/accuracy_histograms_10to25.png}}}
%     % \caption*{\footnotesize Accuracy Distribution for 10-25 Images}
% \end{subfigure}
% \caption{\mybold{Distribution of Individual Accuracy Levels.} (A) Users who viewed more than 25 images (n=4899), showing mean accuracy (dashed line) and 95\% CI (solid lines). (B) Users who viewed between 10-25 images (n=10384), also showing mean accuracy and 95\% CI. \jessica{Would prefer a scatterplot so I can see everything. This figure is not sufficiently labeled, you have too many bins (it shouldn't look so spiky), text is hard to read... lots of work needs to be done on these figures for this to be submittable to CHI}}
% \label{fig:individual-acc}
% \Description{Two histograms showing the distribution of individual accuracy levels. A. Accuracy distribution for users who viewed more than 25 images, with mean accuracy and 95\% confidence intervals indicated. B. Accuracy distribution for users who viewed between 10 and 25 images, with mean accuracy and 95\% confidence intervals indicated.}
% \end{figure}



% For participants who viewed more than 25 images, the mean accuracy for detecting AI-generated images was 78.5\% (95\% CI: [78.1, 78.8]), while for real images it was 77.1\% (95\% CI: [76.7, 77.5]. Interquartile range for real and fake images were similar at 13.2\% and 13.9\%. \jessica{why are we giving CIs and IQR when its redundant}

% For participants who viewed between 10 and 25 images, the mean accuracy for detecting AI-generated images was 77.5\% (95\% CI: [77.2, 77.9]), while for real images it was 75.9\% (95\% CI: [75.5, 76.3]). The interquartile range (IQR) was 17.9\% for AI-generated images and 17.7\% for real images.

%Overall, participants generally performed well in detecting both AI-generated and real images. Most participants achieved accuracy rates above 75\%, indicating a strong ability to distinguish between real and AI-generated content. 


% \subsubsection{Individual Variability Based on Image Difficulty}\label{indiv-acc-2}
% To assess how individual performance varied according to the difficulty of AI-generated images, we categorized the images into three difficulty levels: "Easy," "Medium," and "Hard." \jessica{this is circular! we are assessing performance by difficulty where we define difficulty by performance. Omit} The categorization was based on the accuracy with which participants identified each image. Specifically, images with accuracy in the bottom 33\% were labeled as "Hard," those in the middle 33\% as "Medium," and those in the top 33\% as "Easy." With this categorization, we have 200 "Easy" images, 205 "Medium" images and 199 "Hard" images. For this analysis, we focused on participants who had viewed between 10 to 25 images. \jessica{just an fyi, the science reform lit is full of arguments of how these kind of arbitrary thresholds imposed on data are usually associated with bad science. This whole paragraph could be omitted}
 

% \begin{figure}[H]
% \centering
% \captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}
% \begin{subfigure}[t]{0.3\textwidth}
%     \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/accuracy_histograms_easy.png}}}

% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.3\textwidth}
%     \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/accuracy_histograms_medium.png}}}
%     % \caption*{\footnotesize b) Left: Medium AI-generated images; Right: Medium real images}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.3\textwidth}
%     \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/accuracy_histograms_hard.png}}}
%     % \caption*{\footnotesize c) Left: Hard AI-generated images; Right: Hard real images}
% \end{subfigure}
% \caption{\mybold{Distribution of individual accuracy levels for various image difficulties for users who viewed between 10 to 25 images.} Each column represents a different difficulty level: Easy (A), Medium (B), and Hard (C), with AI-generated images on the top and real images on the bottom.}
% \label{fig:individual-acc-difficulty}
% \Description{Three histograms showing the distribution of individual accuracy levels for different image difficulties: A. Easy, B. Medium, and C. Hard, for users who viewed between 10 to 25 images. AI-generated images are shown on top and real images on the bottom in each histogram. \jessica{I'm not seeing the value of this analysis}}
% \end{figure}


% To categorize performance levels and identify patterns across image types and participant groups, we applied a grading scale to their accuracy scores: "A" (90-100\%) and "F" (below 65\%). \ref{fig:individual-acc-difficulty} shows the distribution of accuracy. For "Easy" images, participants showed strong performance across both AI-generated and real images. For fake images, 89.9\% of responses received an "A," while 10.1\% received an "F." For real images, the distribution was slightly lower, with 87.3\% earning an "A" and 12.7\% an "F." The mean accuracy for fake images was 89.9\% (95\% CI: 89.6\% - 90.2\%), slightly higher than the 87.3\% (95\% CI: 86.9\% - 87.7\%) observed for real images. For "Medium" difficulty images, the performance declined as expected, but still majority of participants received "A" in both real and fake images. For fake images, 77.1\% of participants achieved an "A," whereas 22.9\% were graded "F." For real images, 78.8\% received an "A," and 21.2\% received an "F." The mean accuracy was 77.1\% (95\% CI: 76.6\% - 77.7\%) for fake images and 78.8\% (95\% CI: 78.4\% - 79.2\%) for real images, showing a slight improvement for real images over fake ones in this difficulty category. When it came to "Hard" images, For fake images, 59.0\% of participants were graded "A," and 41.0\% were graded "F." Real images exhibited the same pattern, with 59.0\% earning an "A" and 41.0\% receiving an "F." The mean accuracy was 59.0\% (95\% CI: 58.5\% - 59.5\%) for both fake and real images. Overall, these results indicate a clear decline in accuracy as image difficulty increased. \jessica{This is not a result! You defined the categories based on accuracy, and now you are finding that accuracy is correlated with image difficulty? I'm not sure what to say, this is entirely circular reasoning} Participants exhibited slightly higher accuracy for easy and medium real images compared to fake ones, while performance was identical for the hardest images. 


% \jessica{Why are we only mentioning bootstrapped CIs now? We've already given CIs. You need to clarify your reporting strategy at the beginning of results}added to the begining of results

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.7\linewidth]{sections/images/individual_user_progression_smooth.png}
%     \caption{Moving average accuracy (window size = 1) for 10 randomly selected users who viewed 50+ images. Each line represents an individual user's performance as they progress through the task.}
%     \label{fig:individual-performance-time}
% \end{figure}

% Figure \ref{fig:individual-performance-time} displays accuracy of 10 random participants who saw 50+ images. 

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.7\linewidth]{sections/images/real_vs_fake_accuracy_smooth.png}
%     \caption{Mean moving average accuracy trend with 95\% confidence intervals for all users who viewed 20+ images (n=1107). The solid line represents the mean accuracy, while the shaded area indicates the bootstrapped 95\% confidence interval, filtered for 'unlimited' time assignment}
%     \label{fig:mean-trend}
% \end{figure}

% In the next sections we explore the factors that contribute to the performance of individuals in distiguishing real and AI-generated images and informed our taxonomy development. 
% These findings inform our taxonomy development by highlighting the need to address both the features that make AI-generated images detectable and the characteristics of real images that may lead to false positives. Furthermore, the substantial variability in individual performance emphasizes the importance of creating a flexible, multi-level taxonomy that can accommodate different levels of detection skill.


% We find that the mean accuracy  and the upper bound of 95\% confidence interval for portrait images are lower than the 95\% confidence interval lower bounds for all the other pose complexities in both AI-generated and real images, except for a slight overlap in confidence intervals of Portrait and candid real photos.
% \jessica{Why are we saying this. If you're going to talk about  significance use the correct terms and report the test. But then you should be using it throughout or else it seems weird that it only pops up here.}
% \begin{table}
% \centering
% \begin{tabular}{|l|c|c|}
% \hline
% \textbf{Image Type} & \textbf{AI-generated} & \textbf{Real} \\
%  & \textbf{Accuracy [95\% CI]} & \textbf{Accuracy [95\% CI]} \\
% \hline
% Portraits & 73\% [71\%, 75\%] & 71\% [68\%, 74\%] \\
% \hline
% Full Body & 78\% [76\%, 80\%] & 78\% [75\%, 80\%] \\
% \hline
% Posed Groups & 80\% [78\%, 83\%] & 79\% [77\%, 81\%] \\
% \hline
% Candid Groups & 78\% [76\%, 80\%] & 77\% [74\%, 80\%] \\
% \hline
% \end{tabular}
% \caption{Accuracy in identifying AI-generated and real images by image type}
% \end{table}
% To assess the statistical significance of the differences in scene complexity categories, we performed pairwise comparisons and adjusted the p-values using the Benjamini-Hochberg procedure to control for multiple comparisons. Using a significance threshold of 0.05, we found statistically significant differences for both AI-generated and real images. For AI-generated images, portraits were significantly different from the other categories: full body (p = 0.002), candid group (p = 0.002), and posed group (p < 0.001). There were no significant differences among the other categories. For real images, portraits were also significantly different from all other categories: full body (p = 0.008), candid group (p = 0.026), and posed group (p = 0.001). As with AI-generated images, there were no significant differences among the other categories. To ensure our analysis had sufficient statistical power, we conducted an a priori power analysis. We aimed to detect a medium effect size (Cohen's f = 0.25) with 80\% power at a significance level of 0.01 for a one-way ANOVA with four groups (corresponding to our four scene complexity categories). The analysis indicated that a total sample size of 254 images, or approximately 64 images per category, would be required to achieve these statistical parameters. For AI-generated images, we analyzed 92 full body, 98 candid group, 185 portrait, and 81 posed group images \negar{should we say the number for middle 90\%?}. For real images, our results should be viewed as preliminary and suggestive rather than definitive. Future research with larger samples of real images would be valuable to confirm and extend these findings.
% These results suggest that scene complexity does contribute to the accuracy of identifying AI-generation and hence the photorealism of the image. However, scene complexity is best represented by whether the image is a portrait or not as opposed to the more nuanced four categories we analyzed. 

% \subsubsection{Artifact Types Across Time Conditions}
% For \textbf{Anatomical artifacts}, the mean accuracy for the 5-second condition is 65.9\% (95\% CI: [62.3\%, 69.2\%]), for the 20-second condition, it is 67.9\% (95\% CI: [64.1\%, 71.2\%]), and for the unlimited time condition, it is 69.3\% (95\% CI: [66.6\%, 71.9\%]). For \textbf{Functional artifacts}, the mean accuracy for the 5-second condition is 61.8\% (95\% CI: [57.7\%, 65.5\%]), for the 20-second condition, it is 63.1\% (95\% CI: [59.9\%, 66.5\%]), and for the unlimited time condition, it is 62.8\% (95\% CI: [59.9\%, 65.8\%]). For \textbf{Stylistic artifacts}, the mean accuracy for the 5-second condition is 63.8\% (95\% CI: [60.2\%, 67.5\%]), for the 20-second condition, it is 67.4\% (95\% CI: [64.0\%, 70.9\%]), and for the unlimited time condition, it is 66.2\% (95\% CI: [62.8\%, 69.7\%]).

% Examining the confidence intervals, we observe no non-overlapping CIs between time conditions within each artifact type, suggesting that the differences in accuracy across time conditions for each artifact type are not statistically significant. However, some potential differences exist between artifact types. In the unlimited time condition, the CI for Anatomical artifacts [66.6\%, 71.9\%] does not overlap with the CI for Functional artifacts [59.9\%, 65.8\%], suggesting a potentially significant difference in accuracy between these two artifact types. In the 20-second condition, the CI for Anatomical artifacts [64.1\%, 71.2\%] barely overlaps with the CI for Functional artifacts [59.9\%, 66.5\%], indicating a possible difference that may be worth further investigation.

% \subsubsection{Single Artifact vs. At Least One Artifact}

% When comparing accuracy between images containing a single artifact type versus those with at least one artifact present under the \textbf{unlimited time assignment} condition, we observe the following results for each artifact type: For \textbf{Anatomical artifacts}, the mean accuracy is 69.3\% (95\% CI: [66.6\%, 71.9\%]) for images with a single artifact, compared to 68.2\% (95\% CI: [66.4\%, 69.9\%]) for images with at least one artifact. For \textbf{Functional artifacts}, the accuracy is 62.8\% (95\% CI: [59.9\%, 65.8\%]) for single artifacts and 65.8\% (95\% CI: [64.3\%, 67.4\%]) for at least one artifact. For \textbf{Stylistic artifacts}, the accuracy is 66.2\% (95\% CI: [62.8\%, 69.7\%]) for single artifacts and 67.4\% (95\% CI: [65.4\%, 69.3\%]) for at least one artifact. All confidence intervals overlap between the single and at least one artifact conditions for each artifact type under the unlimited time assignment, suggesting no statistically significant differences in accuracy between these conditions.


% We analyzed the 38635 comments provided by participants in our online experiment, by leveraged [GPT model] to first extract ten main themes that summarized the features and artifacts mentioned by participants in identifying images:

% \begin{enumerate}
%     \item \textbf{Image Quality (Stylistic Artifacts):} Comments on overall appearance, smoothness, and perfection of various elements in the images.
    
%     \item \textbf{Facial and Anatomical Inconsistencies (Anatomical Implausibilities):} Observations about eyes, mouths, noses, skin texture, expressions, and general human anatomy.
    
%     \item \textbf{Physical Anomalies (Anatomical and Functional implausibilities):} Mentions of deformities, misplaced body parts, and irregularities in objects or environments.
    
%     \item \textbf{Lighting and Environmental Inconsistencies ( Laws of Physics artifacts):} Critiques on unnatural lighting, inconsistent shadows, reflections, and mismatched elements in the scene.
    
%     \item \textbf{Digital Manipulation Indicators (Stylistic Artifacts):} Suspicions and observations suggesting the images are AI-generated or digitally altered.
    
%     \item \textbf{Biometric Discrepancies (Anatomical Artifacts):} Focus on particular details like hands, fingers, and other body parts that seem unnatural or imperfect.
    
%     \item \textbf{Uncanny Valley Perceptions (Anatomical Implausibilities and Stylistic Artifacts):} Descriptions of images appearing almost human but with subtle unnatural features causing discomfort.
    
%     \item \textbf{Contextual Incongruities (Social and Cultural Implausibilities):} Observations about unrealistic scenarios, mismatched social elements, and overall composition that seems implausible.
    
%     \item \textbf{Physical Anomalies (Functional Implausibilities):} Comments on elements in the image that appear to work in ways that defy logic.
    
%     \item \textbf{Holistic Authenticity Assessment (Multiple Artifact Types):} Overall evaluation of whether images are genuine or artificial based on a combination of visual cues and inconsistencies across multiple categories.
% \end{enumerate}

% Following this theme extraction, we selected a random sample of 10,000 comments for further analysis. We utilized [GPT model] to map each comment in our sample to one or more of the ten identified themes. This automated classification process allowed us to efficiently categorize a large number of comments while maintaining consistency in the application of our thematic framework. 


% \begin{figure}[H]
% \centering
% \captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}

% \begin{minipage}[t]{0.32\textwidth}
%     \centering
%     \subcaption{}
%     \includegraphics[width=\linewidth]{sections/images/Anatomical_accuracy_over_time.png}
% \end{minipage}%
% \hfill%
% \begin{minipage}[t]{0.32\textwidth}
%     \centering
%     \subcaption{}
%     \includegraphics[width=\linewidth]{sections/images/Functional_accuracy_over_time.png}
% \end{minipage}%
% \hfill%
% \begin{minipage}[t]{0.32\textwidth}
%     \centering
%     \subcaption{}
%     \includegraphics[width=\linewidth]{sections/images/Stylistic_accuracy_over_time.png}
% \end{minipage}

% \caption{\textbf{Mean Accuracy Over Time for Different Artifact Types} A) Mean accuracy over time for images with Anatomical artifacts. B) Mean accuracy over time for images with Functional artifacts. C) Mean accuracy over time for images with Stylistic artifacts. Each subfigure shows the change in mean accuracy across different time assignments (1 second, 5 seconds, 10 seconds, 20 seconds, and unlimited) with 95\% confidence intervals. \jessica{why are you using different axes? Why is this not one plot? Put all three series along a single axis, we don't need a separate axes and ticks for each one}}
% \label{fig:artifact_accuracy_over_time}
% \Description{Three line plots showing mean accuracy over time for different artifact types. A) Mean accuracy for images with Anatomical artifacts over various time intervals. B) Mean accuracy for images with Functional artifacts over time. C) Mean accuracy for images with Stylistic artifacts over time. Each plot includes 95\% confidence intervals across time assignments (1 second, 5 seconds, 10 seconds, 20 seconds, and unlimited).}
% \end{figure}


% \begin{table}[h]
% \centering
% \begin{tabular}{|l|c|}
% \hline
% \textbf{Category} & \textbf{Count} \\
% \hline
% No Artifacts & 2 \\
% At Least One Artifact & 227 \\
% \hline
% \multicolumn{2}{|l|}{\textbf{Images with Only One Artifact}} \\
% \hline
% Only Stylistic & 27 \\
% Only Functional & 35 \\
% Only Anatomical & 33 \\
% \hline
% \end{tabular}
% \caption{Distribution of Fake Images by Artifact Category}
% \label{tab:image_distribution}
% \end{table}
%Our analysis revealed relationship between presence of artifacts and accuracy .... 

%How categories influence difficulty  ...

%Varying visibilities of artifacts across categories...



\begin{figure}[htb]
\centering
\captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}

\begin{minipage}[t]{0.32\textwidth}
    \centering
    \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/mj_ng3_022.png}
\end{minipage}%
\hfill%
\begin{minipage}[t]{0.32\textwidth}
    \centering
    \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/sd_portrait3_049.png}
\end{minipage}%
\hfill%
\begin{minipage}[t]{0.32\textwidth}
    \centering
    \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/mj_ng3_012.png}
\end{minipage}
\caption{\textbf{Instances where the presence of multiple types of artifacts did not necessarily aid detection.} A. Image with a Stylistic Artifact in the cinematization of the scene and Functional Implausibilities in the woman's handbag at the bottom of the image, the unusual textures of the man's jacket, and combination of elements part of a train station (signs, clock) and inside of a train car (handrail) (36\% accuracy) B. Image with a Stylistic Artifact in the soft and wispy textures of the woman's hair and a Functional Implausibilities in her shirt collar (47\% accuracy) C. Image with a Violation of Physics in the brightness of the reflection on the window and the boy and father on the other side of the window. An additional Stylistic Artifact in the smooth texture of the overall image (43\% accuracy).}
\label{fig:artifact_examples}
\Description{Three images where the presence of multiple types of artifacts did not necessarily facilitate detection.(A) Image with Anatomical and Functional artifacts, showing a woman holding a handbag with unusual textures on a man's jacket.(B) Image with Stylistic and Functional artifacts, featuring soft and wispy hair with an atypical jacket collar design.(C) Image with Laws of Physics and Stylistic artifacts, including an unusual shadow and smooth, washed-out textures. Each image highlights specific artifacts contributing to visual implausibility.}
\end{figure}



%     \item Seeing is not always believing: Benchmarking Human and Model Perception of AI-Generated Images \cite{lu2024seeing}
% The perception of faces by humans has been studied extensively in the fields of neuroscience and perceptual psychology, showing that humans visual systems are naturally adept at reading and interpreting faces \cite{sinha2006face}. For example, there is a region of the brain dedicated to processing faces [N. Kanwisher] and infants respond to faces before being exposed to them [C. C. Goren, V. M. Reid]. It has also been shown that humans are faster at locating faces than other objects [R. T. Keys,].

%see groh et al 2022 (which is coming out in nature communications later this month~\cite{groh2022human}


%groh: see PNAS paper where we write: "performance in detecting deepfakes.
Based on research demonstrating humans’ expert visual processing of faces, we may expect humans to perform quite well at
identifying the synthetic face manipulations in deepfake videos.
Research in visual neuroscience and perceptual psychology has
shown that the human visual system is equipped with mechanisms
dedicated for face perception (56). For example, there is a region
of the brain specialized for processing faces (57). Human infants
show sensitivity to faces even before being exposed to them (58,
59), and adults are less accurate at recognizing faces when images
are inverted or contain misaligned parts (60–62). The human
visual system is faster and more efficient at locating human faces
than other objects, including objects with illusory faces (63).
Whether human visual recognition of faces is an innate ability
or a learned expertise through experience, visual processing of
faces appears to proceed holistically for the vast majority of
people (64, 65). In order to examine specialized processing of
faces as a potential mechanism explaining deepfake detection
performance, we include a randomized experiment where we
obstruct specialized face processing by inverting, misaligning,
and occluding videos"

Aug 27, 2024 4:42 PM


groh: this is from the paper: Related research demonstrates how fake images can be persuasive and difficult to distinguish from real images. People
rarely question the authenticity of images even when primed37. Images can increase the credibility of disinformation38. Images
of synthetic faces produced by StyleGAN210 are indistinguishable to research participants from the original photos on which
the StyleGAN2 algorithm was trained39. Moreover, research shows that non-probative and uninformative photos can lead
people to believe false claims40, lead people to believe they know more than they actually know41, and promote “truthiness” by
creating illusory truth effects42, 43, which can lead people to believe falsehoods they previously knew to be falsehoods44, 45
.
When it comes to ostensibly probative videos of political speeches, the question of whether people are more likely to believe an
event occurred because they saw it as opposed to only read about it remains open.
In fact, today’s algorithmically generated deepfakes are not yet consistently indistinguishable from real videos. On a sample
of 166 videos from the largest publicly available dataset of deepfake videos to date46, people are significantly better than chance
but far from perfect at discerning whether an unknown actor’s face has been visually manipulated by a deepfake algorithm47
.
This finding is significant because it demonstrates that people can identify deepfake videos from real videos based solely on
visual cues. However, some videos are more difficult than others to distinguish because of their blurry, dark, or grainy visual
features. On a subset of 11 of the 166 videos, Kobis et al 2021 do not find that people can detect deepfakes better than chance48
.
In another experiment with 25 deepfake videos and 4 real videos but only 94 participants, researchers found that the
overall discernment accuracy is 51% and a media literacy training increases discernment accuracy by 24 percentage points for
participants assigned to the training relative to the control group





\iffalse
\subsection{Limits of machine learning to detect AI-generated images}
\jessica{why are we including this section? If you're going to have it, you need to make clear to reader from the get-go why they need to know all this to interpret your work}


Machine learning models to detect synthetic media generated by AI suffer from a persistent cat-and-mouse game. Once models learn a pattern to detect synthetic media, adversaries can retrain generative AI models or retouch generative AI outputs to evade detection~\cite{schaul_2024}. The space of possible pixel arrangements that humans would describe as photorealistic is extremely large, \jessica{why are we saying this?} and even models trained on large datasets such as the Deepfake Detection Contest dataset are prone to large errors on out-of-distribution media~\cite{deepfakedetectionbyhumancrowds}. 

%This section should highlight innovative approaches to detect synthetic media (especially diffusion model generated images) and then detail the limitations of each model. 

Initially, approaches analyzing frequency components of an image through Fourier and Discrete Cosine Transform \cite{marra2018gansleaveartificialfingerprints, yu2019attributingfakeimagesgans, 9035107, durall2020watchupconvolutioncnnbased, bi2023detectinggeneratedimagesreal, pmlr-v119-frank20a}, head poses, and facial landmark positions  \cite{yang2018exposingdeepfakesusing, yang2019exposinggansynthesizedfacesusing} showed success in detecting GAN-generated faces . However these methods can be vulnerable to simple post-processing like blurring or resizing due to overfitting on specific low-level artifacts that do not generalize \cite{9879575}. Another limitation of many machine learning approaches is adaptability across different image-generation models. A ResNet-50 based classifier trained on \ ProGAN images was originally reported to achieve an accuracy of over 90\% for unseen GAN architectures \cite{wang2020cnn}. This was later shown to perform with a true positive rate of 46.8\% and 9.6\% for StyleGAN2 and StyleGAN3 respectively \cite{Mundra_2023_CVPR}. Another classifier trained on low dimensional embeddings of StyleGAN images correctly classifed 99.5\% of synthetic images in their dataset  \cite{Mundra_2023_CVPR}. However the success of the model is contingent on future generative models containing the same facial regularities observed in the training data of StyleGANs \cite{Mundra_2023_CVPR}. 

%Analysis of gradients of GAN-generated images \cite{10203908} and a patch-based classifier \cite{chai2020makes} are additional attempts at more generalizable models, but the authors maintain that images that were not included in the training of these models may continue to evade detection.

When tested on diffusion-generated images, detectors trained on GAN-generated image perform poorly due to differences in frequency based and spatial artifacts that exist in GAN and diffusion-generated images \cite{corvi2023intriguingpropertiessyntheticimages}. One study reported that a GAN-trained detector scored a 26.34\% accuracy in identifying diffusion-generated images \cite{ricker2024detectiondiffusionmodeldeepfakes}. The same study re-trained GAN detectors with diffusion images and achieved over 90\% accuracy for both diffusion and GAN images, leading to the hypothesis that diffusion models produce fewer detectable artifacts than GANs \cite{ricker2024detectiondiffusionmodeldeepfakes}. Alternatively, studies have also identified artifacts specific to diffusion images in the frequency domain \cite{xi2023aigeneratedimagedetectionusing, 10334046} as well as diffusion fingerprints like  Diffusion Reconstruction Error (DIRE) \cite{wang2023dirediffusiongeneratedimagedetection}, SeDID \cite{ma2023exposingfakeeffectivediffusiongenerated}, and Diffusion Noise Features\cite{zhang2024diffusionnoisefeatureaccurate}. Other approaches for diffusion-generated image detection include classifiers trained on multiple diffusion architectures \cite{epstein2023onlinedetectionaigeneratedimages}, on over 100,000 GAN and diffusion-generated images \cite{porcile2024findingaigeneratedfaceswild}, with hierarchical approaches that narrow in on a model \cite{10.1145/3652027}, and on inter-pixel correlations between patches of an image \cite{zhong2023rich, zhong2024patchcraftexploringtexturepatch}. While  many detection models have been developed to adapt to diffusion-generated images , machine learning approaches to identifying synthetic media do not have robustness guarantees on out-of-distribution data and are vulnerable to context shift~\cite{groh2022identifying}. \jessica{why are repeating things} Vision transformers like Contrastive Language-Image
Pre-training (CLIP) \cite{radford2021learning} have also been employed with greater success against manipulations like down-sampling and adding low-level traces \cite{ojha2024universalfakeimagedetectors, cozzolino2024raisingbaraigeneratedimage}. Still, the authors maintain that intentional adversarial attacks \cite{carlini2020evadingdeepfakeimagedetectorswhite} and large architecture changes in generative models may continue to be challenges. A large survey of several video deepfake detection models echoed this problem concluding that transferability, interpretability, and robustness has not yet been solved \cite{wang2023deepfakedetectioncomprehensivestudy}. Moreover, detection as a problem of true or false forecloses the many dimensions of visual information and discussions on ambiguity that go beyond the binaries of real or fake \cite{jacobsen2024deepfakes}.
\jessica{I am not sure why we are including all this detail. Seems like way too much given that we are not doing ML-based detection}
\fi




%We develop our taxonomy to cover visual features from a wide range of existing research and discussions on identifying artifacts and implausibilities in AI-generated images. These resources have often focused on mismatches between what is generated and expectations of human anatomy and the laws of physics. 

% The goal of our framework is to facilitate a more sophisticated discussion of the perception of authenticity in images, by developing a language to access these nuances of an image that both contribute to photorealism or expose artificiality. \jessica{previous sentence could be omitted} 

%``identify and understand hidden patterns in AI-generated images'' via ``compact distilled representations''~\cite{huang2024asap}

%\subsubsection{Practitioner perspectives on identifying AI-generated images} \leavevmode

%In addition to academic literature discussed in Section \ref{sec:relwork}, we surveyed existing AI literacy resources on how to identify AI-generated content in media, online discussions of AI-generated images in response to viral deepfakes, and the most popular posts discussing photorealism on online forums for AI image creators (Reddit channels r/Midjourney and r/StableDiffusion) to initialize the taxonomy. These sources mentioned visual cues related to (1) anatomical implausibilities such as pupil dilation and misaligned eyes \cite{norton2024deepfakes, techtarget2024deepfakes, nytimes2024deepfake}, teeth \cite{realitydefender2024deepfake}, hair  \cite{norton2024deepfakes, realitydefender2024deepfake}, fingers, and alignment of body parts  \cite{norton2024deepfakes, realitydefender2024deepfake, nytimes2024deepfake}; (2) irregular reflections and shadows \cite{realitydefender2024deepfake, eweek2024deepfake, techtarget2024deepfakes}; (3) unnatural color balances  \cite{norton2024deepfakes, realitydefender2024deepfake}; (4) a mismatch in textures and styles within an image  \cite{norton2024deepfakes, realitydefender2024deepfake, eweek2024deepfake}; (5) garbled or nonsensical details \cite{nytimes2024deepfake}; (6) photoshoot-like perfection and cinematized scenarios \cite{reddit2023}.









%In response to the image of presidential candidate Kamala Harris' rally which was falsely claimed as AI-generated by opposing candidate Donald Trump, many shared their opinions on what visual features supported their views on the image on a LinkedIn post by Hany Farid, a professor in the field. People who believed the image was AI-generated pointed to the plane lacking a shadow, distant people seeming similarly as sharp as those closer to the camera, the lack of the reflection of the crowd on the engine, odd hands when zooming in, and that the specific plane (Airforce 2) typically has a tail number that is not seen in the image. These suspicions were explained, in order, by the unusual stage lighting from above resulting in a lack of a shadow, the plausibility of a long depth of focus with enough light, a simulation of the plane and the scene which showed only a small reflection of the crowd in the engine of the plane, a higher quality image that did not show artifacts in the hands, and images of a more recently produced AirForce 2 that have no tail number.


%The most popular post on the Reddit channel r/Midjourney for the search 'photorealism' is titled 'Boring America Realism' with an AI-generated image of an office party that is not particularly glamorous or dramatic and gives the impression that it was shot on a low quality camera \href{https://www.reddit.com/r/midjourney/comments/157hsdd/boring_america_photorealism/}{[cite]}. \jessica{too much detail for something not directly relevant to what we did} Top comments mention the image looking 'average', 'mundane', and 'closer to what normal phone photos look like.' Posts on r/StableDiffusion also mention similar views of the addition of imperfections as techniques for realism. These include low quality image textures and imperfections on people like freckles, wrinkles, and faces that are not perfect like a model. These discussions indicate that previous examples of AI-generated images have led photoshoot-like perfection and cinematized scenarios to be a heuristic for artificiality. 
%\jessica{Again, main impression I have from all this is that we should be listing the things prior sources have pointed to much more concisely, maybe in the method section. }

%Our work builds on previously identified features of a wide range of artificially generated media to provide a more comprehensive taxonomy of artifacts commonly found in AI-generated media.

%cite
%\href{https://www.reddit.com/r/StableDiffusion/comments/15m4j3m/is_it_possible_to_get_anymore_realistic_with_sd/}{r/StableDiffusion: Is it possible to get anymore realistic with SD?}
%\href{https://www.reddit.com/r/StableDiffusion/comments/17nsujr/photo_realistic_approach_using_realism_engine/}{r/StableDiffusion: Photo Realistic approach using Realism Engine SDXL and Depth Controlnet. (ignore the hands for now)}


%Existing AI literacy resources on visible artifacts to identify AI-generated images are predominantly public-facing articles. These lack rigor and comprehensiveness... We build on previously identified features to produce more comprehensive taxonomy  




%  Commented out stuff, but maybe useful for later

% Our taxonomy is grounded in the digital forensics literature builds upon Borji's taxonomy of qualitative failures of diffusion models~\cite{borji2023qualitative}. we expand the taxonomy to include two new categories, functional and sociocultural implausibilities, keep two categories, anatomical implausibilities and violations of physics, and  

% Researchers and practitioners have been concerned about realistic-looking images from diffusion models is one of the many concerns arising in the space of AI-generated media, which include related concerns of amplified political and health disinformation \cite{goldstein2024persuasive, monteith2024artificial, farid2022creating} and threats to privacy and intellectual property \cite{golda2024privacy, sag2023copyright, epstein2023art}. Regulatory initiatives such as the 2023 U.S. Executive Order on Safe and Trustworthy AI explicitly call for methods to verify the authenticity of media and demonstrate the attention governments are devoting to the concerns arising with generative AI and trust in the media\cite{ExecutiveOrder2023}.

% While there are areas of research dedicated towards computational methods to autonomously detect these AI-generated images, they continue to face numerous challenges such as failing to generalize across models, being misled by simple image manipulations, and constantly being outpaced by rapid advancements in generative models [cite]. In light of the capabilities and limitations of diffusion models and their sociopolitical consequences, we sought to design a framework for categorizing visible implausibilities and artifacts in diffusion model generated images to create a shared language and a methodology to help practitioners and researchers alike navigating the genuine and the artificial. 


%     \item %Meta's initiative on watermaking and adding digital signatures to AI-generated media
%     \item \cite{chesney2019deep} discusses causes and consequences of deepfakes on society
% \cite{simchon2024persuasive}
% \cite{de2020you}


    
% AI image generation technologies have evolved through several different architectures from Variational Autoencoders (VAE) [] to Generative Adversarial Networks (GAN) \cite{goodfellow2020generative} to the modern state of the art, Diffusion models []. 

% The photorealism of AI-generated face images was achieved with the development of the StyleGAN architecture which offers control parameters for the style of the generated image \cite{karras2019style}. While initially producing artifacts perceptible to the human eye, these were solved in the subsequent evolutions of StyleGAN2 \cite{karras2020analyzingimprovingimagequality} and StyleGAN3 \cite{karras2021aliasfreegenerativeadversarialnetworks}. These images were popularized through the website This Person Does Not Exist. Other variants of the GAN architecture that also showed impressive photorealism include DCGAN \cite{radford2015unsupervised} which improves image quality by combining Convolutional Neural Networks (CNNs) [] and StarGAN which was trained with parameters for facial attributes such as hair color, gender and age, as well as facial expressions such as “happy.” \cite{choi2018starganunifiedgenerativeadversarial}

% With DALL-E 2 \cite{ramesh2021zero, ramesh2022hierarchicaltextconditionalimagegeneration}, diffusion models emerged as the new state of the art in generating photorealistic images. The model uses Contrastive Language-Image Pre-training (CLIP) [] to encode a text input that is used to condition the generated image. Diffusion-generated images feature photorealism in a wider range of scenarios. More advanced diffusion models have continued to emerge such as OpenAI DALL·E 3 [], Stable Diffusion V1.5, SDXL, V3, [], Google Imagen [], Midjourney [], and Adobe Firefly []. As these models allow anyone to generate a photorealistic image through a text prompt, they significantly expanded access to photorealistic AI-generated images.

% Diffusion models have also given rise to powerful image manipulation tools. This includes generative inpainting which allow one to erase and re-generate parts of an image based on the rest of the image. The latest Google Pixel 9 features a ‘Reimagine’ tool that uses generative inpainting to allow easy realistic manipulation of images taken on your phone []. Face swapping is another AI-powered photo manipulation technique that modifies the face exclusively using trained embedding models for facial features []. These manipulation tools produce partially AI-generated images... We focus on images generated entirely generated by diffusion models.



% We analyzed accuracy across two image characteristics:  \textit{scene complexity} described in Section \ref{initializing-taxonomy} and the \textit{taxonomy categories} present. The taxonomy categorizes artifacts and implausibilities into 1. Anatomical Implausibilities, 2. Stylistic Artifacts 3. Functional Implausibilities, 4. Violations of Physics, and 5. Sociocultural Implausibilities. A detailed description of each of the \textit{taxonomy categories} is provided in \autoref{sec:taxonomy}. Four co-authors independently labeled artifacts present in images and provided explanations of the annotations. Each annotation was then reviewed by two co-authors separate from the initial annotator. To ensure consistency, a fifth co-author then reviewed all annotations. Using data on the display time conditions, we also investigated the potential interplay between artifact type and display time.

%We investigated the impact of types of artifacts as defined by our taxonomy categories on the percieved photorealism and detection accuracy by having four co-authors independently annotate images for five artifact categories which included 1. Anatomical Implausibilities, 2. Stylistic Artifacts 3. Functional Implausibilities, 4. Violations of Physics, and 5. Sociocultural Implausibilities. Each annotation was reviewed by two co-authors separate from the initial annotator. All annotators provided explanations for the artifacts they noticed in each image. To ensure consistency, the fifth co-author then reviewed all annotations, verifying the accuracy of labels and harmonizing the explanations provided for each identified artifact. 
% Notably, sociocultural Implausibilities and Violations of Physics rarely ocurred independently; they typically co-occurred with other artifact types. In order to analyze the impact of individual artifacts, we narrowed our analysis to the three more prevalent categories, (Anatomical, Stylistic, and Functional) which had sufficient occurrences of appearing independently.
% We also analyzed distributions of accuracy across different display times. 
% \jessica{what does this have to do with artifacts?} 

% Additionally, we assessed the variation in perceived photorealism across Midjourney, Firefly, and Stable Diffusion. We further analyzed variations across different generations (seeds) of the same prompts and the impact of curation on perceived photorealism. Even with refined prompts that we created to generate highly photorealistic images, diffusion models generate different images with a new seed. Images were selected to be in the dataset after experimenting with a number of generations. We evaluated the impact of this curation on the perceived photorealism of AI-generated images. We selected a subset of 37 images from our dataset with a diverse range in accuracy and generated at least 12 more images using the same original prompts (and pipeline in Stable Diffusion) to create a set of re-generated variations of the prompts. These new images were included on our website as part of the ongoing experiment in order to compare accuracy across the variations.



%These images had been generated using carefully refined prompts, iterated upon until we determined that further improvements in photorealism were unlikely. However, even with a refined prompt, diffusion models generate multiple images at a time, and we selected the most realistic one from each set. To assess the potential impact of this final selection step on perceived photorealism, we regenerated 12 images using the same final prompts and included these uncurated variations in our ongoing experiment website. This allowed us to compare the detection accuracy between the curated and uncurated images generated from the same prompt. 


%In response to an AI-generated candid image (Figure~\ref{fig:comments-all} (a)) a participant has commented, "\textit{Cosmetic style of women out of character with vintage setting}". This comment was categorized under the "Contextual Incongruities (Social and Cultural Implausibilities)" theme, exemplifies how participants used contextual reasoning to assess image authenticity. Another participant observed, "\textit{Skin looks too smooth, depth of field is unreasonably shallow}" for (Figure~\ref{fig:comments-all}(b)). This comment was mapped to themes "Image Quality (Stylistic Artifacts)" and "Lighting and Environmental Inconsistencies ( Laws of Physics Artifacts)", highlighting both Stylistic issues and violations of expected photographic properties. For Figure~\ref{fig:comments-all} (c), a participant has commented:"\textit{If this is not AI then it is a staged photograph like a movie set because of the lighting and he is an actor}". This comment was mapped to themes (Lighting and Environmental Inconsistencies (Laws of Physics Artifacts) and "Contextual Incongruities (Social and Cultural Implausibilities)". Here participant has chosen this image to be AI-generated. The comment is interesting in the sense that it shows the thought process of the participants when exposed to a cinemtaic-style image. In another instance, a participant noted, "\textit{4 fingers and the blanket in the back looks like it would have already fallen down with the way it lays and so the can below it}" (Figure~\ref{fig:comments-all} (d)). This observation was mapped to themes "Physical Anomalies" and "Physical Anomalies - Functional Implausibilities", showing attention to both Anatomical errors and violations of expected physical behavior. This example demonstrates how elements present in real photographs might lead to misclassification, especially when viewed alongside AI-generated images. In another instance, a participant commented, "\textit{The group looks back-lit and pasted onto a background of buildings}" (Figure~\ref{fig:comments-all} (e)). This observation was categorized under theme "Digital Manipulation Indicators", reflecting attention to digital alteration.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.3\textwidth]{sections/images/mj_ng3_007.png}
%     \caption{AI-generated portrait of a  female astronaut}
%     \label{fig:comments-1}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.3\textwidth]{sections/images/mj_portrait3_001.png}
%     \caption{AI-generated image with unnaturally smooth skin and shallow depth of field}
%     \label{fig:comments-2}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.3\textwidth]{sections/images/ff_fullbody2_003.jpeg}
%     \caption{AI-generated image with Anatomical anomalies}
%     \label{fig:comments-3}
% \end{figure}



% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.3\textwidth]{sections/images/fullbody3_023.jpeg}
%     \caption{Real image with perceived Anatomical and Functional artifacts}
%     \label{fig:comments-4}
% \end{figure}


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\textwidth]{sections/images/artifact_category_distribution.png}
%     \caption{Artifact category distribution mentioned in participants comments}
%     \label{fig:comments-dist}
% \end{figure}



%In developing our Functional artifact taxonomy, we prioritized using language that aligns with common observations and is easily understandable to the general public. \jesssica{omit, soundsd silly:} We believe that familiarity with this terminology will empower individuals to detect Functional implausibilities more effectively. This approach is supported by our findings, as images containing only Functional implausibilities were the most challenging to identify (\ref{fig:artifacts} (A)).



% The overlapping confidence intervals for \textit{Midjourney} and \textit{Stable Diffusion} indicate their performance difference may not be statistically significant, while \textit{Firefly}'s non-overlapping interval suggests a significant difference in its performance. 

% \begin{figure}[H]
% \captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}

% % Subfigure A: 5-second condition
% \begin{subfigure}[t]{.3\textwidth}
%     \centering
%     \subcaption{}
%     \includegraphics[width=\linewidth]{sections/images/accuracy_by_model_bootstrap_5_seconds.png}
% \end{subfigure}%
% \hfill
% % Subfigure B: 20-second condition
% \begin{subfigure}[t]{.3\textwidth}
%     \centering
%     \subcaption{}
%     \includegraphics[width=\linewidth]{sections/images/accuracy_by_model_bootstrap_20_seconds.png}
% \end{subfigure}%
% \hfill
% % Subfigure C: Unlimited time condition
% \begin{subfigure}[t]{.3\textwidth}
%     \centering
%     \subcaption{}
%     \includegraphics[width=\linewidth]{sections/images/accuracy_by_model_bootstrap_unlimited.png}
% \end{subfigure}

% \caption{\mybold{Accuracy by Model Across Different Time Conditions}(A) 5-Second Condition.(B) 20-Second Condition.(C) Unlimited Time Condition. These plots show the distribution of accuracy for the three AI models (Stable Diffusion, Midjourney, and Firefly) across different time conditions.}
% \label{fig:accuracy_by_model_time}
% \end{figure}

% When looking at different viewing times in Figure~\ref{fig:accuracy_by_model_time}, across all time conditions, \textit{Midjourney} consistently demonstrated the highest mean accuracy, followed closely by \textit{Stable Diffusion}, with \textit{Firefly} showing the lowest accuracy. For the 5-second condition, \textit{Midjourney} achieved 74.3\% accuracy (95\% CI: [71.1\%, 77.5\%]), \textit{Stable Diffusion} 72.4\% (95\% CI: [69.8\%, 75.0\%]), and \textit{Firefly} 62.1\% (95\% CI: [57.2\%, 67.1\%]). The 20-second condition had higher accuracy: \textit{Midjourney} at 76.8\% (95\% CI: [73.4\%, 80.0\%]), \textit{Stable Diffusion} at 75.0\% (95\% CI: [72.5\%, 77.5\%]), and \textit{Firefly} at 67.5\% (95\% CI: [62.9\%, 71.9\%]). In the unlimited time condition, the results were: \textit{Midjourney} 74.1\% (95\% CI: [70.9\%, 77.4\%]), \textit{Stable Diffusion} 73.0\% (95\% CI: [70.1\%, 75.7\%]), and \textit{Firefly} 67.0\% (95\% CI: [63.0\%, 71.4\%]). While all models showed some improvement from the 5-second to the 20-second condition, the unlimited time condition did not yield further substantial improvements.

% Our analysis of accuracy across viewing times for each AI model revealed no significant changes, regardless of the model or time spent viewing the image. Midjourney images were consistently identified with the highest accuracy, followed closely by Stable Diffusion, while Firefly images proved the most challenging to detect. 




% Artifact Types Impact Accuracy: Anatomical inconsistencies are easier to spot, while functional implausibilities are more subtle. This shows the importance of educating individuals on the more subtle functional implausibilities, as these can be easily overlooked.
% Human Curation Makes a Difference: Curation increases the perceived realism of AI-generated images

% \textbf{\textit{Human Performance is Variable: People are generally better at identifying AI-generated images than real ones, but individual accuracy varies widely.}}\matt{Why compare people at identifying AI-generated images and real ones? I don't see how this is relevant to anything...}
% Participants in our experiment were more accurate at identifying AI-generated images than real ones, suggesting that participants may have misclassified authentic photos as AI-generated likely due to the incorrect heuristics for detecting fakes. While the median accuracy for both AI-generated and real images was similar (78.4\% vs. 78\%, respectively), AI-generated images exhibited greater variability in accuracy scores, with a wider interquartile range and higher standard deviation. This indicates that participants' ability to accurately detect AI-generated content was less consistent. Some AI-generated images were identified with nearly 100\% accuracy, likely due to the presence of obvious features, whereas real photographs often lacked clear indicators, making them harder to classify. Although we observed a slight improvement in participants’ detection accuracy over time, the overall learning effect was minimal. Additionally, our analysis showed no significant changes in detection accuracy based on the time participants spent viewing the images.\matt{Can we say something more specific... performance is always variable... I don't think this above paragraph adds anything to what we describe in our abstract/intro as the main points and contributions, so it's probably fine to just delete this}

% When an image is easily identifiable as AI-generated, such as the examples shown in Figure~\ref{fig:three-fake-images}, most participants perform well, limiting our ability to detect differences between conditions as there is not much room for improvement. As explained in Section \ref{data-preliminaries}, for analyses in Sections \ref{acc-scene-complexity} and \ref{acc-model} we exclude images with detection accuracy above 90\%, removing 72 images from our dataset.