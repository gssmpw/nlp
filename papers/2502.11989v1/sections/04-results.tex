\section{Accuracy in Distinguishing AI-generated Images from Real Photographs}\label{sec:results}

In the main phase of the experiment, we collected 539,749 responses on 599 images from 37,568 participants from February 5, 2024 to June 22, 2024. Sections~\ref{sec:acc-general} through \ref{sec:acc-model} focus on data from the main phase of the experiment. The second phase of the experiment started on June 22 and ended on August 30, with 83,577 responses on 482 images from 3,787 participants. Sections~\ref{sec:imagestimuli} and~\ref{sec:human-curation} describe the influence of human curation of the stimuli on how accurately participants identify the stimuli as AI-generated or real. 

The design of our experiment involves several important design choices. First, we selected the three models
of Midjourney, Firefly, and Stable Diffusion as the diffusion models. Second, we crafted prompts to produce realistic
outputs across various pose categories and content types. Third, we curated 450 images from over 3000 images generated
to use as image stimuli in the experiment. These images were selected to maximize realism while also representing
different visual artifacts and implausibilities. Inevitably, these design choices on models, prompts, and stimuli introduce some selection bias  into the experiment.

Additionally, we implemented two exclusion criteria that should be considered when interpreting our results. First, for all the analyses in Section ~\ref{sec:results}, we excluded observations where participants checked the box on the website ``I have seen this before''. These observations, which account for 2\% of the total observations, were excluded because of the strong possibility that participants who had previously seen the images were already aware of whether they were fake or real.
%the experiment aimed to evaluate whether participants could identify if an unfamiliar image was AI-generated.. 
For these observations marked as having been seen before, 38\% of these observations were on AI-generated stimuli and 62\% were on real images. The image most frequently reported as 'seen before' is a real portrait of Martin Luther King Jr, which was one of the few real images of a well-known celebrity included in the experiment. 

Second, in line with our goals of studying detection ability on images for which there was some ambiguity, we excluded all images where participants' accuracy suggested very little ambiguity. We operationalized this as accuracy above 90\%. 

These exclusion criteria remove all observations on 68 fake images and 4 real images, which represent 14\% of observations from the entire experiment. 

In the human-coded analysis of artifacts discussed in Section~\ref{sec:acc-presence-artifacts}, we apply an additional exclusion criterion to make the coding tractable. Specifically, we exclude all images accurately identified in more than 80\% of observations. This exclusion criterion focuses the analysis on the most challenging images by excluding the most egregious distortions that lead to low photorealism (i.e., high participant accuracy).
%and offers a lower bound on the full extent of the differences between image categories because .

\subsection{Overall Accuracy} \label{sec:acc-general}

In the main study, participants correctly identified AI-generated images and authentic photographs in 76\% and 74\% of observations, respectively. Accuracy varied substantially across images. Prior to implementing our accuracy-based exclusion described above, we found that for AI-generated images, accuracy ranged from 32\% to 99\%. Similarly, accuracy on real photographs ranged from 28\% to 92\%. Figure~\ref{fig:accuracy_real_fake} shows the distribution of accuracy in both AI-generated and real images with example images selected from the top, bottom, and middle deciles of each distribution. At the image level, the mean accuracy for identifying AI-generated and real images was 76\% (95\% CI:[74,77]) and 74\% (95\% CI:[72,76]), respectively. 

Despite our efforts to minimize obvious artifacts, some images - particularly non-portraits - were challenging to generate without noticeable artifacts. As a result, participants achieved nearly 100\% accuracy on a few AI-generated images with obvious features. We present examples of these images in Figure~\ref{fig:three-fake-images}.  %that further motivate the exclusion criteria that we apply to the rest of the results section. 
In contrast to AI-generated images, real photographs rarely contain definitive artifacts and visual cues often seen in AI-generated images, which limits participants from achieving near-perfect accuracy on real photographs.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{sections/images/general_accuracy.pdf}
    \caption{Distribution of accuracy scores for real and AI-generated images with example images representing different accuracy levels.}
    \label{fig:accuracy_real_fake}
    \Description{Histograms showing the distribution of accuracy scores for real and AI-generated images, accompanied by example images representing various accuracy levels.}
\end{figure}
\begin{figure}[H]
\centering
\captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}
\begin{subfigure}[t]{0.3\linewidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/sd_portrait3_040.jpg}}}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.3\linewidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/ff_pg3_010.jpeg}}}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.3\linewidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/ff_fullbody3_007.jpeg}}}
\end{subfigure}
\caption{\mybold{Examples of obviously AI-generated images and their corresponding accuracy.} \normalfont{\textbf{A.} AI-generated portrait with 92\% accuracy. \textbf{B.} AI-generated posed group image with 95\% accuracy. \textbf{C.} AI-generated full-body image with 99\% accuracy.}}
\label{fig:three-fake-images}
\Description{Three examples of obviously AI-generated images with corresponding accuracy scores: A. Portrait with 92\% accuracy, B. Posed group image with 95\% accuracy, C. Full-body image with 99\% accuracy. }
\end{figure}

\subsection{Participant Level Accuracy}\label{sec:indiv-acc}

\begin{figure*}[h]
    \centering
    \captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}

    % Top Row - A and B
    \begin{subfigure}[t]{0.4\textwidth}  
        \centering
        \subcaption[]{}  
        \vspace{-3pt}  
        \includegraphics[width=\linewidth]{sections/images/scatter_plot_fake_images.jpg} 
        % \subcaption{}
    \end{subfigure}
    \hspace{0.05\textwidth} 
    \begin{subfigure}[t]{0.38\textwidth}  
        \centering
        \subcaption[]{}  
        \vspace{-3pt}
        \includegraphics[width=\linewidth]{sections/images/scatter_plot_real_images.jpg}
        % \subcaption{}
    \end{subfigure}

    % Bottom Row - C and D
    \begin{subfigure}[t]{0.36\textwidth}  
        \centering
        \subcaption[]{}  
        \vspace{-3pt}
        \includegraphics[width=\linewidth]{sections/images/accuracy_distribution.jpg}
        % \subcaption{}
    \end{subfigure}
    \hspace{0.05\textwidth}  
    \begin{subfigure}[t]{0.36\textwidth}  
        \centering
        \subcaption[]{}  
        \vspace{-3pt}
        \includegraphics[width=\linewidth]{sections/images/learning_curve_with_fake_real_bias.png}
        % \subcaption{}
    \end{subfigure}

    \caption{\textbf{Participant-level accuracy and learning trends.}  
    \normalfont{ \textbf{A.} Scatterplot of participant-level accuracy for AI-generated images. \textbf{B.} Scatterplot of participant-level accuracy for real images. \textbf{C.} Histogram showing the distribution of accuracy across the first ten images seen by participants who viewed at least 10 images. \textbf{D.} Learning curve illustrating accuracy trends and classification biases when detecting AI-generated and real images.}}
    
    \label{fig:combined-participant-accuracy}

    \Description{A composite figure showing participant accuracy trends:  
    A. Scatterplot displaying accuracy levels for detecting AI-generated images, with points representing individual participants.  
    B. Scatterplot for real images, structured similarly to A.  
    C. Histogram showing the distribution of accuracy for participants' first 10 images.  
    D. Learning curve tracking accuracy trends over time, highlighting biases in AI-generated and real image classification.}
\end{figure*}

Given the organic nature of participants' engagement with this experiment, we did not impose restrictions on the number of images a participant saw. Most participants in this study provided responses to at least seven images, but some participants only provided a single response, and one participant provided 502 responses. 

The vast majority of participants (75\%) saw 16 or fewer images. Figure~\ref{fig:combined-participant-accuracy}A and B present the distribution of participant--level accuracy by number of viewed images. 

In order to compare participant performance and avoid issues that arise with differential attrition, we focus on the first ten images seen by participants who saw at least 10 images, which includes 152,050 observations from 15,205 participants. First, we note that 34\% of these participants achieved 90\% accuracy or higher on the first ten images seen. If the AI-generated images were perfectly photorealistic such that the human ability to distinguish is no higher than random guessing, then we would have expected only 1\% of participants to achieve this threshold of accuracy (assuming random guessing at 50\% accuracy, with participants evaluating 10 images each, achieving at least 9 out of 10 correct responses would occur with a probability of approximately 1.07\%, based on the binomial probability distribution). Figure~\ref{fig:combined-participant-accuracy}C shows the distribution of accuracy across the first ten images seen by participants who saw at least 10 images.

In Figure~\ref{fig:combined-participant-accuracy}D, we present accuracy rates by the number of images seen. We find that on average, participants begin the experiment by disproportionally identifying images as fake in 63\% of observations. Notably, this bias is reduced after only a few images.


\subsection{Accuracy by Scene Complexity} \label{sec:acc-scene-complexity}

We find that on average, participants' accuracy increases as scene complexity increases. For example, we find that 16\% of portraits appear in the bottom decile of accuracy scores (representing the highest level of photorealism), whereas only 3\% of AI-generated posed group images appear in the bottom decile. Figure~\ref{fig:pose-complexity} presents the distribution of accuracy for each category, separately for real and AI-generated images. For AI-generated images, the mean accuracy was 72.7\% (95\% CI: [72.4, 72.9]) for portraits, 77.2\% (95\% CI: [76.8, 78.6]) for full body, 76.2\% (95\% CI: [75.8, 76.7]) for posed groups, and 73.4\% (95\% CI: [73, 73.8]) for candid groups. For real images, the accuracy was 71.1\% (95\% CI: [70, 71.4]) for portraits, 75.5\% (95\% CI: [75.1, 75.8]) for full body, 76.7\% (95\% CI: [76.3, 77]) for posed groups, and 74.8\% (95\% CI: [74.4, 75.1]) for candid groups. 

As exemplified in Figure~\ref{fig:pose-complexity}C, we note that portraits, relative to the other levels of scene complexity, typically have less detail, simpler and more standardized poses, more blurred backgrounds, and fewer available cues than full-body or group images. 
\begin{figure}[h]
    \captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}
    \centering

    % First subfigure - Reduce space
    \begin{subfigure}[t]{\linewidth}
        \subcaption{}
        \vspace{-12pt}  % Reduce vertical space
        \includegraphics[width=\linewidth]{sections/images/scene_complexity_filtered90_Real.png}
    \end{subfigure}
    
    % Second subfigure - Reduce space
    \begin{subfigure}[t]{\linewidth}
        \subcaption{}
        \vspace{-12pt}  % Reduce vertical space
        \includegraphics[width=\linewidth]{sections/images/scene_complexity_filtered90_AI-generated.png}
    \end{subfigure}
    
    % Third subfigure - Reduce space
    \begin{subfigure}[t]{\linewidth}
    \centering
        \subcaption{}
        \vspace{-12pt}  % Reduce vertical space
        \includegraphics[width=0.9\linewidth]{sections/images/scene_complexity_B.pdf}
    \end{subfigure}
    
    \caption{\textbf{Scene complexity}: Accuracy of real and AI-generated images by scene complexity levels. 
    \normalfont{Beeswarm plots of image-level accuracy for each dimension of scene complexity with bootstrapped 95\% confidence intervals. We exclude images identified with above 90\% accuracy in this analysis. \textbf{A.} Real images \textbf{B.} AI-generated images \textbf{C.} AI-generated images across scene complexities.}}
    
    \label{fig:pose-complexity}
    
    \Description{Beeswarm plots showing the accuracy of real and AI-generated images by pose complexity. Each dot represents an individual image, with error bars indicating the bootstrapped 95\% confidence interval around the mean.}
\end{figure}


\subsection{Accuracy by Presence of Artifacts}\label{sec:acc-presence-artifacts}
\begin{figure*}[h]
\centering
\captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}

% Full PDF Image
\includegraphics[width=\linewidth]{sections/images/combined_artifact_trends.pdf}

\caption{\textbf{Accuracy by artifact types and display times} \normalfont{\textbf{A. Mean accuracy for different artifact types.} Distribution of accuracy scores by artifact type
for images with at least one artifact. \textbf{B. Mean accuracy over display time.} Change in mean accuracy across different display time assignments (1 second, 5 seconds, 10 seconds, 20 seconds, and unlimited) with 95\% confidence intervals and bee swarm plots of image accuracy for AI-generated and real images. \textbf{C. Mean accuracy over time for different artifact types.} Change in mean accuracy across different time assignments (1 second, 5 seconds, 10 seconds, 20 seconds, and unlimited) with 95\% confidence intervals and bee swarm plots of image accuracy for
images with anatomical (pink), functional (green), and stylistic
(blue) artifacts. The x–axis shows the display time intervals, and the
y–axis shows accuracy.}}

\label{fig:combined-artifact-trends}

\Description{A composite figure showing accuracy-related analyses:  
(A) A beeswarm plot displaying the distribution of accuracy scores for images containing at least one artifact, categorized by artifact type.  
(B) A line plot illustrating mean accuracy across different display time conditions (1 second, 5 seconds, 10 seconds, 20 seconds, and unlimited), with 95\% confidence intervals. Overlaid bee swarm plots represent individual accuracy scores for AI-generated and real images.  
(C) A line plot showing mean accuracy over time for different artifact types (Anatomical, Functional, and Stylistic). Each artifact type is color-coded (pink for Anatomical, green for Functional, and blue for Stylistic). Bee swarm plots depict individual accuracy scores for images within each artifact category. The x-axis represents display time intervals, and the y-axis represents accuracy.}

\end{figure*}


In order to analyze accuracy by artifact type, we annotated images with diffusion model artifact categories from the taxonomy based on a three-step process. First, four co-authors independently annotated all 218 images with accuracy below 80\%, identifying artifacts and providing detailed explanations for their annotations. Second, each of these annotations was reviewed and edited by two additional co-authors. Third, a fifth co-author reviewed all annotations for consistency. Figures~\ref{fig:combined-varying-artifacts-visibility}A--C and \ref{fig:combined-varying-artifacts-visibility}D--F provide examples of how we annotated images, displaying the identified artifact categories, the reasoning behind their identification, and the associated detection accuracy for each image. During this process, we observed that the three main artifact types---anatomical implausibilities, stylistic artifacts, and functional artifacts---each appeared in nearly a third of the images we annotated. In contrast, violations of physics and sociocultural implausibilities were less common, appearing in only 20 and 12 images, respectively. In light of this distribution of artifacts, Figure~\ref{fig:combined-artifact-trends}A presents the distribution of accuracy scores across images containing at least the three listed artifact types.

Based on our annotations of artifacts in images, we find participants are less accurate on images with functional implausibilities than images with anatomical implausibilities or stylistic artifacts. The mean accuracy on images with at least one functional implausibility, one anatomical implausibility, and one stylistic artifact is 64.1\% (95\% CI: [63.8, 64.5]), 65\% (95\% CI: [64.6, 65.4]), and 64.9\% (95\% CI: [64.5, 65.3]), respectively. While the accuracy on images with functional implausibilities is lower than on images with other implausibilities and artifacts, the mean accuracy scores are similar. However, this similarity in means masks the differences in the distribution of accuracy scores, as shown in Figure~\ref{fig:combined-artifact-trends}A. We find that images with participant accuracy scores in the 40--60\% range (which represent images approaching indistinguishability between real and AI-generated) make up 32.8\% of images annotated with functional implausibilities compared to 21.4\% and 22.4\% of images annotated with anatomical implausibilities and stylistic artifacts, respectively. 


We find that images that we annotated as containing multiple artifacts can still appear photorealistic enough to make detection difficult for most people. Artifacts vary in levels of visibility, as shown in Figure~\ref{fig:combined-varying-artifacts-visibility}A--C. While Figure~\ref{fig:combined-varying-artifacts-visibility}A and C contain stylistic artifacts, they are far more apparent in Figure~\ref{fig:combined-varying-artifacts-visibility}B, which is reflected in its higher detection accuracy. Despite Figure~\ref{fig:combined-varying-artifacts-visibility}A and C containing multiple artifact categories, they had low detection accuracy, suggesting that the presence of multiple artifacts does not necessarily make images easier to identify and that artifact visibility is also a contributing factor. 


The visibility of artifacts is highly variable, and Figure~\ref{fig:combined-varying-artifacts-visibility}D--F present examples highlighting this variability. The anatomical implausibility in the fingers in image Figure~\ref{fig:combined-varying-artifacts-visibility}D is very noticeable, whereas the functional implausibilities in the tennis racket and shirt design of Figure~\ref{fig:combined-varying-artifacts-visibility}F are more subtle. The corresponding accuracy scores for these images--- 62\% for Figure~\ref{fig:combined-varying-artifacts-visibility}E and 54\% for Figure~\ref{fig:combined-varying-artifacts-visibility}F —reinforce the observation that anatomical artifacts tend to be more easily detected, while functional implausibilities often require closer attention and familiarity with depicted objects. The stylistic artifacts in the cinematization of Figure~\ref{fig:combined-varying-artifacts-visibility}E and plastic-like skin texture fall in between, further showing the spectrum of detectability across different artifact categories and visibility. 


\begin{figure*}[h!]
\centering
\captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}

% First Row of Images
\begin{subfigure}[t]{0.27\linewidth}
\centering
    \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/8059c316907c586bdf33ad3cb9ca3f95.jpeg}
\end{subfigure}
\hspace{1cm}
\begin{subfigure}[t]{0.27\linewidth}
\centering
    \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/616ba73f50088eb13244a807076248f7.jpeg}
\end{subfigure}
\hspace{1cm}
\begin{subfigure}[t]{0.27\linewidth}
\centering
    \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/2c4c0b171577884f5c0991cacb5c5ebc.jpeg}
\end{subfigure}

\vskip 5mm % Adds vertical spacing between rows

% Second Row of Images
\begin{subfigure}[t]{0.27\linewidth}
\centering
    \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/ff_pg3_009.jpeg}
\end{subfigure}
\hspace{1cm}
\begin{subfigure}[t]{0.27\linewidth}
\centering
    \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/mj_portrait3_010.jpeg}
\end{subfigure}
\hspace{1cm}
\begin{subfigure}[t]{0.27\linewidth}
\centering
    \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/ff_portrait3_004.jpeg}
\end{subfigure}

\caption{\textbf{Examples of images with varying artifact visibility.}  
\normalfont{\textbf{Top row (A--C):} Example images showcasing stylistic and functional artifacts with varying visibility.  
\textbf{A.} A subtle stylistic artifact in the soft and wispy textures of the woman's hair and a minor functional implausibility in the atypical design of her shirt collar (Accuracy: 47\%). \textbf{B.} An obvious stylistic artifact due to the overall cinematization of the image (Accuracy: 73\%). \textbf{C.} A combination of multiple artifacts, including anatomical implausibilities in the woman's hand, functional implausibilities in the table shape and wall panels, and a stylistic artifact in the soft texture of the woman's face (Accuracy: 38\%). \textbf{Bottom row (D--F):} Images with anatomical, stylistic, and functional artifacts of varying visibility. \textbf{D.} Anatomical implausibilities in the fingers of the three students (Accuracy: 84\%). \textbf{E.} A stylistic artifact in the cinematized look and plastic-like texture of the woman's skin (Accuracy: 62\%). \textbf{F.} No obvious anatomical or stylistic artifacts, but closer inspection reveals functional implausibilities: the tennis racket is asymmetrical, its strings are not taut, and the shirt has irregularly shaped designs with glitch-like inconsistencies (Accuracy: 54\%).}}

\label{fig:combined-varying-artifacts-visibility}

\Description{A composite figure showing six images with varying visibility of AI-generated artifacts.  
(A--C) The first row highlights stylistic and functional artifacts, including wispy hair, cinematized lighting, and a distorted table.  
(D--F) The second row focuses on anatomical, stylistic, and functional artifacts, including distorted fingers, plastic-like textures, and inconsistencies in objects like a tennis racket.}
\end{figure*}

\subsection{Accuracy by Randomized Display Time}\label{sec:acc-time}
\begin{figure*}[h]
\centering
\captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}
\begin{subfigure}[t]{0.27\linewidth}
\centering
    \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/bbbfb2a12cd66783ce7e4015ec0084b9.jpg}
\end{subfigure}
\hspace{1cm}
\begin{subfigure}[t]{0.27\linewidth}
\centering
  \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/5204545de13342cbefdc0e9022d821d2.jpg}
\end{subfigure}
\hspace{1cm}
\begin{subfigure}[t]{0.27\linewidth}
\centering
  \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/ccc04b661d52c055a44fc01718c6a2bc.jpg}
\end{subfigure}
\caption{\textbf{Exemplar AI-generated images for which a closer look improves accuracy.} \normalfont{\textbf{A.} Accuracy: 38\% at 1 second display time to 65\% at 20 second display time. \textbf{B.} Accuracy: 44\% at 1 second display time to 82\% at 20 second display time. \textbf{C.} Accuracy: 27\% at 1 second display time to 70\% at 20 second display time.}}
\label{fig:displaytime}
\Description{Three images showing AI-generated images for which a closer look improves accuracy. (A) Image of woman generated by Stable Diffusion: Accuracy is 38\% at 1 second display time and it improves to 65\% at 20 second display time with  (B) Image of people generated by Stable Diffusion: Accuracy is 44\% at 1 second display time and it improves to 82\% at 20 second display time with (C) Image of people generated by Stable Diffusion: Accuracy is 27\% at 1 second display time and it improves to 70\% at 20 second display time with.}
\end{figure*}

By randomizing the display time of images in this experiment, our results support evaluating how viewing duration influences participants' accuracy. We find that longer viewing times improve performance. With just 1 second of display time, participants are  72\% accurate (95\% CI=[71.6, 72.5], 95\% CI=[71.3, 72.2]) on AI-generated and real images, respectively. With 5 seconds of display time, accuracy increases to 77\% (95\% CI=[77.0, 77.8], 95\% CI=[76.6, 77.4]) for both AI-generated and real images, respectively. While accuracy on real images appears to plateau by 5 seconds of display time, accuracy on AI-generated images increases up to 80\% (95\% CI=[79.6, 80.4]) at 10 seconds and 82\% (95\% CI=[81.2, 81.9]) at 20 seconds. Figure~\ref{fig:combined-artifact-trends}B presents the distribution of accuracy scores across display time conditions. Across the observations where display time was randomized, we find that the proportion of AI-generated images that are identified below random chance decreases from 43\% when participants only have 1 second to view the image to 30\%, 25\%, 17\%, and 17\% when participants have 5, 10, 20 seconds, and unlimited time to view the image.

In some images, AI artifacts can be noticed with a quick glance, but for others, careful attention to detail is necessary to spot the artifact. Figure~\ref{fig:displaytime} presents three images that require careful attention, as evidenced by the fact that most participants mark as real when they are limited to seeing the image for a second but
fake once they take into account the details of the scene.

Accuracy across all artifact types improved with increased display time. As shown in Figure~\ref{fig:combined-artifact-trends}C, participants showed higher accuracy when images were displayed for longer time (anatomical artifacts: 63\% at 5 seconds vs. 59\% at 1 second; stylistic artifacts: 63\% at 5 seconds vs. 60\% at 1 second; functional artifacts: 60\% at 5 seconds vs. 55\% at 1 second). For all artifacts, there is a significant improvement in detection accuracy when increasing display time from 5 seconds to unlimited. 

In Figure~\ref{fig:combined-artifact-trends}C, we observe that participants improved the most in identifying functional artifacts, with an 18\% improvement from 1 second to unlimited viewing time. In comparison, anatomical and stylistic artifacts showed smaller improvements of 11\% each over the same time interval. Unlike anatomical and stylistic implausibilities that can be identified at first glance, functional artifacts often require a closer look and familiarity with the elements in the image as they often appear in parts of the image that are not the main subject. 


\subsection{Qualitative Analysis of Participant Comments}\label{sec:qualitative-analysis}

We collected 34,675 comments from participants who filled out the optional text input box asking participants: ``If you think this is AI-generated, please explain why.'' In order to identify themes from these 34,675 comments, we prompted GPT-3.5 Turbo to identify 10 main themes across these comments. GPT-3.5 Turbo responded with the following ten themes, which we manually reviewed and refined to mitigate the ambiguities and generalization typical of large language models \cite{stephan2024rlvflearningverbalfeedback}: (1) Image quality focusing on the overall appearance, smoothness, and sometimes unrealistic perfection of image elements; (2) Facial and anatomical inconsistencies where participants pointed to irregularities in eyes, mouths, noses, skin texture, expressions, and general human anatomy; (3) Anatomical and functional anomalies such as deformities, misplaced body parts, and irregularities in objects or environments; (4) Lighting and environmental inconsistencies including unnatural lighting, inconsistent shadows, and reflections; (5) Digital manipulation indicators suggesting suspicions of AI-generation or digital alteration; (6) Biometric discrepancies particularly unnatural or imperfect body parts like hands and fingers; (7) Uncanny valley perceptions where images almost looked human but had subtle unnatural features that caused discomfort; (8) Contextual incongruities such as unrealistic scenarios and mismatched social elements; (9) Physical anomalies highlighting illogical physical interactions within the images; and (10) holistic authenticity assessment making overall judgments based on a combination of multiple cues and inconsistencies. Based on these ten main themes, we prompted GPT-3.5 to label each comment with one of the ten themes. Figure~\ref{fig:comments-all} illustrates examples of participant comments for four images and how they were categorized into themes. Figure~\ref{fig:themes} displays the distribution of themes across the comments and the related concept from our taxonomy in parentheses.

Based on GPT-3.5 Turbo, we find that 61\% of participants' comments mentioned relying on anatomical implausibilities. The next most common concept referred to is stylistic artifacts, which is mentioned in 30\% of comments. Participants mentioned functional implausibilities in 21\% of comments, violations of physics in 15\% of comments, and sociocultural implausibilities in only 4\% of comments. 

Based on the authors' annotations of artifacts, we find functional implausibilities to be the most prevalent, appearing in 58.7\% of images, followed by anatomical implausibilities in 51.4\% and stylistic artifacts in 39.0\% of images. We identify violations of physics and sociocultural implausibilities in only 9.17\% and 5.50\% of images, respectively. 
\begin{figure}[H]
\centering
\captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt}
\begin{subfigure}[t]{0.9\linewidth}
    % \subcaption{}
    \includegraphics[width=\linewidth]{sections/images/theme_distribution_single_column.jpg}
\end{subfigure}
\caption{\mybold{Distribution of themes identified in participant comments.}}
\label{fig:themes}
\Description{A horizontal bar chart showing the distribution of themes identified in participant comments explaining their reasoning for AI image detection. The themes include Image Quality, Facial and Anatomical Inconsistencies, Anatomical and Functional Anomalies, Lighting and Environmental Inconsistencies, Digital Manipulation Indicators, Biometric Discrepancies, Uncanny Valley Perceptions, Contextual Incongruities, Physical Anomalies, and Holistic Authenticity Assessment.}
\end{figure}
While functional artifacts were the most prevalent in human researcher annotated images, they were less frequently mentioned in participant comments annotated by GPT--3.5. Conversely, anatomical artifacts were emphasized more in participant comments than in their prevalence in annotated images. 

\begin{figure}[htb]
\centering
\captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}
\begin{subfigure}[t]{0.22\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/mj_ng3_007.jpg}}}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.22\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/mj_portrait3_001.jpg}}}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.22\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/mj_fullbody3_003.jpg}}}
\end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.19\textwidth}\subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/fullbody3_023.jpeg}}}
% \end{subfigure}
\hfill
\begin{subfigure}[t]{0.22\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/mj_pg3_002.jpg}}}
\end{subfigure}
\caption{\mybold{Examples of participant comments mapped to themes.} \normalfont{\textbf{A.} ``Cosmetic style out of character with vintage setting": Contextual Incongruities. \textbf{B.} ``Skin too smooth, depth of field shallow.": Image Quality, Lighting Inconsistencies. \textbf{C.} ``If this is not AI then it is a staged photograph like a movie set because of the lighting and he is an actor.": Lighting inconsistencies, Contextual Incongruities.
\textbf{D.} ``Group looks pasted onto background.": Digital Manipulation Indicators.}}
\label{fig:comments-all}
\Description{Four images with participant comments mapped to themes.(A) AI-generated candid image with a comment on cosmetic style being out of character with a vintage setting.(B) AI image with smooth skin, with a comment on skin being too smooth and shallow depth of field.(C) AI-generated full body shot of a man, with a comment suggesting it resembles a staged photograph due to lighting.
(D) AI image of a group, with a comment on the group looking pasted onto the background.}
\end{figure}

\subsection{Accuracy by Models} \label{sec:acc-model}

In the process of generating the images for this experiment's stimuli set, we noticed that Midjourney, Firefly, and Stable Diffusion have different capabilities and limitations. For example, we noticed that Midjourney often produced images with persistent stylistic artifacts that were challenging to eliminate. Firefly, on the other hand, frequently exhibited a tendency toward synthetic emotional expressions, with subjects often appearing unnaturally and overly cheerful, necessitating multiple iterations to produce more realistic results. Stable Diffusion struggled significantly with generating group images, often introducing artifacts such as anatomical inconsistencies. In light of the limitations to generate non-portrait images with Stable Diffusion, 75\% of the Stable Diffusion-generated stimuli in this experiment were portraits. On the other hand, 30\% of Midjourney and Firefly-generated images in this experiment depict portraits. In order to compare the three models fairly, we focus our comparison on portrait images. Figure~\ref{fig:models} presents accuracy shown on portraits by each of the three models and reveals that participants' mean accuracy on Midjourney, Stable Diffusion, and Firefly were  76\% (95\% CI: [75.2, 75.8]), 74\% (95\% CI: [73.9, 74.8]), and 73\% (95\% CI: [72.7, 73.3]), respectively. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{sections/images/model_accuracy_combined.jpg} 
    \caption{\textbf{Accuracy across generative AI models} \normalfont{Each point represents an image. The black dots and error bars show the mean accuracy and 95\% bootstrapped confidence intervals for each model}}
    \label{fig:models}
    \Description{Bee swarm chart showing accuracy across different generative AI models. The chart compares the accuracy rates for identifying AI-generated content among various models along with bootstrapped 95\% confidence intervals}
\end{figure}


\subsection{Accuracy on Human Curated Images vs. Uncurated Images} \label{sec:human-curation}
\begin{figure*}[h]
\centering
\captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, font=small}
\begin{subfigure}[t]{0.24\linewidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/sd_portrait3_003.jpg}}}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.24\linewidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/0bce6c35c24ec5ce8ae8ad5bb4f67d59_r4.jpg}}}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.24\linewidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/0bce6c35c24ec5ce8ae8ad5bb4f67d59_r11.jpg}}}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.24\linewidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/0bce6c35c24ec5ce8ae8ad5bb4f67d59_r2.jpg}}}
\end{subfigure}
\caption{\mybold{Re-generated images from the same prompt.} \normalfont{ \textbf{A.} Stage 1 image generated by Stable Diffusion and curated by our team (37\% accuracy) \textbf{B.} Most photorealistic of 12 prompt-matched image generations by Stable Diffusion (42\% accuracy) \textbf{C.} Median photorealistic of 12 prompt-matched image generations by Stable Diffusion (59\% accuracy) \textbf{D.} Least photorealistic of 12 prompt-matched image generations by Stable Diffusion(83\% accuracy)}}
\label{fig:regeneration}
\Description{Four images showing re-generated outputs from the same prompt.\textbf{A.} Stage 1 image generated by Stable Diffusion and curated by our team (37\% accuracy) \textbf{B.} Most photorealistic of 12 prompt-matched image generations by Stable Diffusion (42\% accuracy) \textbf{C.} Median photorealistic of 12 prompt-matched image generations by Stable Diffusion (59\% accuracy) \textbf{D.} Least photorealistic of 12 prompt-matched image generations by Stable Diffusion(83\% accuracy)}
\end{figure*}

\begin{figure*}[h]
\centering
\captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt}
\begin{subfigure}[t]{0.48\textwidth}  
\subcaption{}
\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/curation_value_add_min.jpg}}}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
\subcaption{}
\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/curation_value_add_mean.jpg}}}
\end{subfigure}
\caption{\mybold{Comparing accuracy scores on curated images and uncurated prompt-matched images.} \normalfont{\textbf{A.} Scatterplot showing human detection accuracy of the original curated image compared to human detection accuracy of its most photorealistic regeneration out of 11 to 24 prompt-matched images labeled as re-generations. \textbf{B.} Scatterplot showing human detection accuracy of the original curated image compared to human detection accuracy of its mean photorealistic regeneration out of 11 to 24 re-generations.}}
\label{fig:curation-value}
\Description{Two scatterplots showing curation value analysis.(A) Minimum curation value added.(B) Mean curation value added. Each chart illustrates the impact of curation on the overall value added to the dataset.}
\end{figure*}

Generating photorealistic AI-generated images involves three key ingredients: the diffusion model, the prompt, and human curation. In this section, we examine how human curation of diffusion model-generated images influences the aggregate accuracy scores of human participants. In order to show this influence, we compare diffusion model images from the main experiment, which were curated by our research team, with multiple diffusion model images generated from the same prompt as the curated images. This comparison reveals the increase in photorealism (as measured by the decrease in participants' accuracy) on the curated images relative to the prompt-matched images.

In this second phase of the experiment, we randomly sampled 39 AI-generated images from the main stimuli set, where the sample was stratified on 10 percentage point wide bins on human detection accuracy. For each of these 39 images, we generated at least 11 prompt-matched images using Midjourney, Firefly, and the same pipeline in Stable Diffusion. Figure~\ref{fig:regeneration} displays a Stable Diffusion-generated image from our original stimuli set and three of the twelve generations using the same prompt. We generated 482 total additional images, with at least 11 per prompt. These 482 images were included alongside the 149 real images on the experiment website.

In Figure~\ref{fig:curation-value}, we present scatterplots comparing human detection accuracy on the initial curated images and the best prompt-matched images in panel A, and mean prompt-matched images in panel B. We find the human-curated images have lower human detection accuracy than the best regenerated image in 18 of 39 instances and the mean re-generated image in 35 of 39 instances. In total, the human-curated images were perceived to be more photorealistic than 408 of the 482 (84\%) uncurated prompt-matched images. Specifically, we find the marginal value added by human curation for images that were initially detected in the range of 30\% to 50\% is 31 percentage points, 50 to 60\% is 23 percentage points, 60-70\% is 11 percentage points, 70-80\% is 8 percentage points, and 80+\% is 4 percentage points. Across the stimuli selected from Midjourney, Firefly, and Stable Diffusion, the marginal value of human curation is 7.8, 19.0, and 16.9 percentage points, respectively. 


The two panels in Figure~\ref{fig:curation-value} illustrate the positive correlation between accuracy on the human-curated image and accuracy on the regeneration. This reveals how the prompt influences photorealism. The Pearson Correlation Coefficient between accuracy on curated images and their best, mean, and worst re-generations are .58, .53, and .32, respectively. This positive correlation suggests the choice of a prompt plays a significant role in the photorealism of an image. Figure~\ref{fig:goodandbadprompt} displays two original curated images where A is generated by a prompt in which re-generations achieved low human detection accuracy (a `good' prompt), and B is generated by a prompt in which re-generations achieved a high human detection accuracy (a `bad' prompt). Prompts that consistently generate easily detectable images often have elements that are difficult to generate and result in artifacts. The prompt ``Persian woman astronaut in astronaut clothes, family photo with husband and two toddlers, high resolution, realistic" for Figure~\ref{fig:goodandbadprompt}B  generates a posed group image that tends to be easy to detect. On the other hand, the prompt ``American woman faculty portrait, not a close-up, blond" for Figure~\ref{fig:goodandbadprompt}A generates a portrait image that tends to be perceived as more photorealistic.