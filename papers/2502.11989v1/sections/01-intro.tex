\section{Introduction}
The capabilities of diffusion models to generate photorealistic images of people are beginning to contribute to disinformation and erode trust in the media \cite{epstein2023art}. For example, in March 2023, realistic AI-generated images of world leaders went viral on social media, showing Pope Francis wearing what appeared to be a designer puffer jacket, Donald Trump getting arrested, and Vladimir Putin standing behind prison bars \cite{apnews2024}. These exemplar images may appear both provocative and realistic at first glance, but they are far from perfectly photorealistic; they contain distortions of hands and faces, implausible grasping of objects, and shadows that do not match the objects that appear to cast them. These distortions are not unique to these particular images but are pervasive in diffusion model-generated images produced by text--to--image tools such as Midjourney (the source of these fake images of world leaders), Stable Diffusion by Stability AI, and Firefly by Adobe~\cite{kamali2024distinguish}. While it is possible to generate images that seem indistinguishable from photographs, many diffusion model-generated images still leave behind human-identifiable artifacts. This raises an open research question for human--computer interaction: What drives human perception of photorealism in images generated by diffusion models?

We approach this question by conducting a large--scale, online experiment where we collect data on human participants' accuracy in identifying whether images are AI-generated or real. We measure photorealism following a psychophysics approach ~\cite{zhou2019hype} that defines photorealism as human discrimination performance. Accuracy scores are inversely associated with photorealism: a high accuracy score indicates low photorealism, whereas a low accuracy score indicates high photorealism. By defining photorealism based on discrimination performance, we avoid the speculation and subjectivity of asking participants questions like ``Is the image photorealistic?"~\cite{liang2024rich} and ``Could these images be taken with a camera?"~\cite{yan2024sanitycheckaigeneratedimage}. 

By comparing human detection accuracy across a diverse set of images, we can evaluate the contexts that influence the continuum of photorealism. Past research has demonstrated that GAN(Generative Adversarial Networks)-generated human portraits can be indistinguishable from real portrait images~\cite{nightingale2022ai}. However, open questions on context remain: How often do portrait images appear indistinguishable from real portraits? How does scene complexity across styles of photographic portraiture (e.g., single-subject close-up, single-subject full body, posed group, and candid group) influence aggregate measures of photorealism? How accurately can people identify real images across scene complexities? What kind of artifacts arise in diffusion model-generated images and how are the presence of those artifacts related to photorealism? How does display time of an image influence measures of photorealism? How does human curation of AI-generated image stimuli influence measures of photorealism?

We approach each of these questions in turn and then consider an interventional question: How should an AI literacy guide categorize artifacts and implausibilities that emerge in photorealistic AI-generated images to promote attention to and communicate these visual cues? We also consider a flipped version of that question: How do we help people avoid falsely identifying a real image as AI-generated? This question differs from the first because it focuses on how people can identify when to trust what they see. This is important because there has already been a case where a politician has incorrectly, publicly claimed that an authentic photograph of his opponent was AI-generated~\cite{apnews2024b, wired2024kamala}. Enhancing human skill at distinguish real from AI-generated remains important because technical platform--level solutions (e.g. watermarking and machine learning classification) lack robustness and are susceptible to error when images are slightly modified via cropping, compression, and other edits. AI literacy guides have the potential to help humans stay abreast of the capabilities and limitations of AI to better navigate assessing the authenticity of images.

Our contributions toward answering these research questions are fourfold: First, we contribute a taxonomy of artifacts and implausibilities in diffusion model-generated images of humans along five dimensions: (1) anatomical implausibilities: representations of human anatomy that deviate from realistic or common forms; (2) stylistic artifacts: visual elements that often appear in images generated by diffusion models like shiny or plastic textures; (3) functional implausibilities: design or structural flaws that would make an object or system unlikely to function as intended in the real world; (4) violations of physics: instances where an object or scenario defies the laws of physics; and (5) sociocultural implausibilities: representations of people that are unlikely to reflect the norms of a culture or society. Second, we conduct a large--scale digital experiment and present an empirical evaluation of photorealism – as measured by human detection accuracy – across images from three state-of-the-art diffusion models, varying photographic contexts, types of artifacts in an image, and randomized display time of an image. This evaluation offers insight into the capabilities and limitations of diffusion models to produce photorealistic images. Third, we provide empirical evidence revealing the influence of human curation on the level of photorealism in images generated by diffusion models. This finding reveals the importance of human curation and the limits of state-of-the-art diffusion models to producing photorealistic images, and create consistent and believable narratives via an automated deluge of fake images that are indistinguishable from real photographs. Fourth, we release a public dataset of 749,828 responses from 50,444 participants on 599 images to enable the replications of our results and further research on photorealism in diffusion models; replication data, code, and stimuli can be found at the following link: https://github.com/negarkamali/Replication-for-Characterizing-Photorealism-2025/.