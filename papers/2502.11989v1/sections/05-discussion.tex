\section{Discussion}\label{sec:disc}

While diffusion models can generate highly realistic images, most of the images they produce still contain visible artifacts. In particular, we find that only 17\% of diffusion model-generated images are misclassified as real at rates consistent with random guessing. Notably, this misclassification rate increases to 43\% when the viewing duration is restricted to 1 second. By curating a dataset of 599 images and conducting a large scale digital experiment, we can begin to answer fundamental questions about what drives the appearance of photorealism in diffusion model-generated images. 

First, we find that images with greater scene complexity tend to introduce more opportunities for artifacts to appear, making it easier for participants to detect AI-generated images. Our results reveal that participants were less accurate at identifying AI-generated portraits compared to more complex scenes, such as those involving multiple people in candid settings.  Based on qualitative analysis of the images, we identify three main reasons for this difference. First, portraits often feature a single person against a blurred background, which can obscure details and provide fewer cues compared to full-body or group images. Second, portraits typically involve fewer and simpler poses, focusing only on the face and torso, leaving fewer opportunities for errors or inconsistencies to be apparent. Third, the prevalence of edited and retouched portraits in real-world photography complicates the distinction between real and AI-generated portraits, addressing the question of how subject type and context (e.g., unknown people vs. public figures) influence the perceived authenticity of an image. In contrast, more complex images, like full-body or group shots, involve a greater number of elements, increasing the likelihood of noticeable errors or inconsistencies. Similar to our results on AI-generated images, we find that real images with lower scene complexity are also harder to identify as real. 

Second, we identify five high-level categories of artifacts and implausibilities and find that the easiest images to identify as diffusion model generated are the ones with anatomical implausibilities, such as unrealistic body proportions and stylistic artifacts like overly glossy or waxy features.

Third, by randomizing display time, we identify the relationship between how long an individual looks at an image and their accuracy at distinguishing between real and AI-generated images. Specifically, we find that participants' accuracy at identifying an AI-generated image upon a quick glance of 1 second is 72\% and increases by 5 percentage points with just an additional 4 seconds of viewing time and 10 percentage points when unconstrained by time. Given the nature of rapid scrolling on social media and how much time people have to see advertisements as they pass by billboards on a highway, these results reveal the importance of attentive viewing of images before making judgments about an image's veracity. 


Fourth, we find that human curation had a notable negative impact on participants' accuracy compared to uncurated images generated by the same prompts as the human-curated AI-generated images. In particular, the images curated by our research team were harder to identify as AI-generated than 84\% of the uncurated images generated using the same prompts as the curated images. This finding reveals the limitation of state-of-the-art diffusion models in producing images of consistent quality. It also suggests that human curation is a bottleneck to generating fake images at scale. The process of generating high-quality AI images is inherently iterative---users refine prompts and select outputs until they achieve their desired result. This fundamental aspect of AI image generation is evident across all applications, from advertising and marketing to education and beyond. While concerns exist about fake images being used to mislead or impersonate, many use cases exist for business and educational applications~\cite{vartiainen2023using, hartmann2023power, gvirtz2023text}. The critical role of human curation in this iterative process further emphasizes how the photorealism of images produced by diffusion models depends not only on the capabilities of the diffusion model but also on the quality of human curation, choice of prompts, and context of the scene. Given the importance of these factors beyond the generative AI model, these results reveal the importance of considering these factors in research examining human perception of AI-generated images. Without considering these elements, it is possible to produce biased findings showing AI-generated images are more or less realistic than they really appear in real-world settings. 

The taxonomy offers a practical framework on which to build AI literacy tools for the general public. We synthesized information from diverse sources such as social media posts, scientific literature, and our online behavioral study with 50,444 participants to systematically categorize artifacts in AI-generated images. Through this process, we identify five key categories: anatomical implausibilities, which involve unlikely artifacts in individual body parts or inconsistent proportions, particularly in images with multiple people;  stylistic artifacts, referring to overly glossy, waxy, or picturesque qualities of specific elements of an image; functional implausibilities, arising from a lack of understanding of real-world mechanics and leading to objects or details that appear impossible or nonsensical; violations of physics, which include inconsistencies in shadows, reflections, and perspective that defy physical logic; and sociocultural implausibilities, focusing on scenarios that violate social norms, cultural context, or historical accuracy. Our taxonomy builds upon the Borji 2023 taxonomy \cite{borji2023qualitative} and focuses on images that appear more realistic at first glance, which is useful for comparing and contrasting real photographs with diffusion model generated images for revealing the nuances of the artifacts and implausibilities~\cite{kamali2024distinguish}. Moreover, this taxonomy offers a shared language by which practitioners and researchers can communicate about artifacts commonly seen in AI-generated images and exposes the persistent challenges that can help people identify AI-generated images. 

\subsection{Future Work and Limitations}

In addition to aiding in identifying AI-generated content, the taxonomy offers insights into the open problems for producing realistic AI-generated images. Future work may explore integrating such taxonomies into model evaluation frameworks to provide iterative feedback during the development of generative models. As models advance to address the weaknesses presented in this taxonomy, new and more subtle artifacts may emerge, requiring future updates to this taxonomy. This dynamic interplay between detection and generation capabilities demonstrates why we need to maintain robust human detection abilities even as models evolve. We acknowledge the potential dual use of these insights to create more deceptive synthetic media, and we believe that transparent documentation of artifacts does more good than harm by offering detection strategies and an opportunity to develop general awareness in the public.

Large-scale digital experiments with participants who participate based on their own interests come with certain limitations. First, we did not collect demographic data from participants. Participants were not recruited for this experiment; instead, participants found the experiment organically and participated. Given the organic nature of the participation, we prioritized maximizing engagement, which involves questions unrelated to distinguishing AI-generated and real images like demographic questions. While this approach enabled substantial data collection, it limits analysis by excluding factors like age, gender, and cultural background that may influence detection. 

Second, we provided feedback on the correct answer after each participant made an observation, which has the potential to introduce learning effects. Future research could address these open questions by collecting demographic data to design more inclusive AI literacy tools and evaluating how performance changes with and without feedback. 

This research focused on images generated by state-of-the-art generative models available in 2024, and the findings are inherently tied to the state of diffusion models and generative AI technologies as of 2024. In the future, models are likely to change, and the somewhat visible errors that emerge will also likely change. Past state-of-the-art GAN models such as StyleGAN2~\cite{karras2020analyzingimprovingimagequality} and BigGAN~\cite{brock2018biggan}, often produced more noticeable artifacts in facial features, color balance, and overall photorealism, making their outputs more easily distinguishable. Nonetheless, the current taxonomy on diffusion models points out elements like anatomical implausibilities and stylistic artifacts that can be mapped to the facial feature and color balance cues. These recurring issues offer evidence of the taxonomyâ€™s robustness to differences across model generations, but future studies should explore how the taxonomy may need to adapt to these changes, which may involve adding or removing categories or may involve further identifying nuances within these categories. As an example of how this taxonomy may be applied to AI-generated video, Figure~\ref{fig:sora} presents an example of an anatomical implausibility that we never saw in diffusion model-generated images because it involves a temporal inconsistency. Future research on the realism of AI-generated audio and video may also consider following the three-step process involved in building this taxonomy for images generated by diffusion models. Based on first surveying AI literacy resources, academic literature, and social media, second generating media with state-of-the-art models, and third collecting empirical data on the human ability to distinguish AI-generated media from authentically recorded media, researchers can build empirical insights towards characterizing realism and categorizing the artifacts in AI-generated media.  
 
The empirical insights on the photorealism of AI-generated images and the resulting taxonomy designed to help people better navigate real and synthetic images online lead to a practical research question: How can AI literacy interventions improve people's ability to distinguish real photographs and AI-generated images? Future research may address this question via randomized experiments comparing a control group with no intervention to a treatment group that receives training based on the taxonomy presented in this paper. Likewise, future research may explore this with just-in-time interventions to direct people's attention to the cues identified in the taxonomy.