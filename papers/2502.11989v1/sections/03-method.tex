\section{Methods}\label{sec:method}
\begin{figure*}[!htb]
    \centering
    \includegraphics[width=\linewidth]{sections/images/overview.pdf}
    \caption{\mybold{Overview of the taxonomy development process.} \normalfont{In the background research stage, we reviewed existing literature on visible features of AI-generated images from a wide range of sources. This included academic literature, practitioner perspectives in AI literacy articles, and discussions on the photorealism of AI-generated images online. From these features, we developed an initial taxonomy of artifacts. In the Generation and Curation stage, we used our taxonomy of artifacts to create a dataset of 599 images. Of these images, 149 were real photographs curated from the internet, and 450 were generated in Midjourney, Firefly, and Stable Diffusion through extensive iteration with photorealistic image generation techniques. We used the dataset of images for an online crowdsourced experiment where we evaluated participant accuracy in identifying AI-generated images. We iteratively refined the taxonomy based on results from the experiment and continued monitoring new literature on AI-generated images as generative models evolved.}}
    \label{fig:overview}
    \Description{Overview of the Taxonomy Development Process. In the background research stage, we reviewed existing literature on visible features of AI-generated images from a wide range of sources. This included academic literature, practitioner perspectives in AI literacy articles, and discussions on the photorealism of AI-generated images online. From these features, we developed an initial taxonomy of artifacts. In the Generation and Curation stage, we used our taxonomy of artifacts to create a dataset of 599 images. Of these images, 149 were real photographs curated from the internet, and 450 were generated in Midjourney, Firefly, and Stable Diffusion through extensive iteration with photorealistic image generation techniques. We used the dataset of images for an online crowdsourced experiment where we evaluated participant accuracy in identifying AI-generated images. We iteratively refined the taxonomy based on results from the experiment and continued monitoring new literature on AI-generated images as generative models evolved.}
\end{figure*}

We develop a detailed taxonomy of visual features, qualities, and artifacts that offer cues that an image is AI-generated or not following a three-step process based on the taxonomy development method proposed by Nickerson et al.~\cite{nickerson2013method}. We began by drafting an initial version of the taxonomy based on a review of visual features previously identified in AI literacy resources, academic literature, and online discussions (\autoref{sec:initializing-taxonomy}). We then employed two parallel processes to develop the taxonomy: iteratively generating and curating a dataset of 599 images to showcase the taxonomy artifacts (\autoref{sec:curation}) and conducting an online, crowdsourced experiment using these curated images to assess human detection ability (\autoref{sec:onlexp}). Third, we integrated participant feedback---both accuracy metrics and thematic comments---back into the taxonomy, allowing real-world human detection behaviors to inform the final categorization.

While the taxonomy development was guided by data, we acknowledge that subjectivity is inherent in the categorization process. To mitigate this subjectivity and ensure methodological rigor, multiple team members independently identified recurring patterns and artifacts during image generation and curation, and we reconciled any differences through structured discussion until stable, consistently observed phenomena emerged. In Figure~\ref{fig:overview}, we show an overview of the taxonomy development process.


\subsection{Initializing the Taxonomy} \label{sec:initializing-taxonomy}
In addition to reviewing academic literature discussed in Section \ref{sec:relwork}, we surveyed traditional and social media discussions about distinguishing AI-generated images. These included AI literacy resources on how to identify AI-generated content in media (see~\autoref{fig:nytimes}), online discussions of AI-generated images in response to viral deepfakes, and popular posts discussing photorealism on online forums for AI image creators (Reddit channels such as r/Midjourney and r/StableDiffusion) to initialize the taxonomy. These sources highlighted several visual cues, including (1) anatomical implausibilities such as pupil dilation and misaligned eyes \cite{norton2024deepfakes, techtarget2024deepfakes, nytimes2024deepfake}, teeth \cite{realitydefender2024deepfake}, hair  \cite{norton2024deepfakes, realitydefender2024deepfake}, fingers, and alignment of body parts  \cite{norton2024deepfakes, realitydefender2024deepfake, nytimes2024deepfake}; (2) irregular reflections and shadows \cite{realitydefender2024deepfake, eweek2024deepfake, techtarget2024deepfakes}; (3) unnatural color balances  \cite{norton2024deepfakes, realitydefender2024deepfake}; (4) a mismatch in textures and styles within an image  \cite{norton2024deepfakes, realitydefender2024deepfake, eweek2024deepfake}; (5) garbled or nonsensical text \cite{nytimes2024deepfake}; 
(6) photoshoot-like perfection and overly cinematic scenarios \cite{reddit2023}.

While some prior research has suggested that AI-generated face images can be indistinguishable from real ones \cite{hulzebosch2020detectingcnngeneratedfacialimages, nightingale2022ai}, more complex scenes, such as group photos have not been thoroughly explored.  We address this by introducing a detailed categorization of \textit{scene complexity} across all images. We identified four distinct scene types that capture varying levels of detail within an image:
 \begin{itemize}[leftmargin=*, label=\tiny{$\bullet$}]
    \item \textbf{Portraits (Single-Subject Close-Up)}: An image featuring a single individual, typically focusing on the face and torso. The individual is the primary focus, often set against a blurred or minimal background. Portraits have relatively low scene complexity.
    
     \item \textbf{Full-Figure (Single-Subject Full Body)}: An image featuring a single individual whose entire body is visible along with the surrounding environment. These images exhibit moderate scene complexity, as they include more details than portraits, such as the person's posture and interaction with their setting.
         
     \item \textbf{Posed Group}: An image featuring multiple people posing for the camera in a structured manner. These images involve higher scene complexity due to the presence of multiple subjects, their interactions, and the added challenge of capturing each person accurately. 
    
     \item \textbf{Candid Group}: An image of multiple people captured in candid moments. These images often feature intricate interactions between people and their environments, representing the highest level of scene complexity.
 \end{itemize}

\subsection{Stimuli Generation and Curation} \label{sec:curation}
We created a dataset of 599 images. This image set included 149 real photographs curated from the internet, from which we derived the scenarios for 450 images that we generated using AI. Of the AI-generated images, 207 were generated in Midjourney, 133 in Firefly, and 110 in Stable Diffusion. 

Drawing on techniques shared on online forums and articles, we developed strategies to generate photorealistic images. We first curated real photographs and then experimented extensively with the three AI-generation tools to create over 3000 images that depict similar scenarios as the real images. The final dataset represents a selection of images from this larger set that we judged to be not immediately identifiable as AI-generated at first glance. This selection enabled us to focus on more challenging cases, better assess participants' ability to detect subtle artifacts, and enhance the relevance of our taxonomy in real-world scenarios. 

\subsubsection{Curating Real Photographs.}  
We sourced real photographs from online platforms, selecting them to represent diverse scenarios (e.g., diverse cultural settings with celebrity and non-celebrity figures engaging in common and uncommon activities) across the four dimensions of scene complexity \autoref{sec:initializing-taxonomy}. We established these categories to curate a diverse range of real photographs and ensure our dataset accurately captures how the features in our taxonomy may manifest and be perceived in both real and AI-generated images. 
We verified that these images were real photographs by confirming details like the creation date, photographer, and publisher. We include a complete list of image sources and verification details in the following link: https://github.com/negarkamali/Replication-for-Characterizing-Photorealism-2025/. We used these real images to inform the prompts to generate images using AI tools.
% example set as a basis to create prompts for generating AI-generated images with similar content. 


\begin{figure*}[htbp]
\centering
\captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt, aboveskip=0pt, belowskip=2pt}

% First Row
\begin{subfigure}[t]{0.24\linewidth}
    \subcaption{}\includegraphics[width=\linewidth]{sections/images/Obama_mj_1.jpg}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.24\linewidth}
    \subcaption{}\includegraphics[width=\linewidth]{sections/images/Obama_mj_2.jpg}
\end{subfigure}
\hfill
% Second Row
% \vskip 0.5em  % Reduce vertical space between rows
\begin{subfigure}[t]{0.24\linewidth}
    \subcaption{}\includegraphics[width=\linewidth]{sections/images/Obama_mj_3.jpg}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.24\linewidth}
    \subcaption{}\includegraphics[width=\linewidth]{sections/images/Obama_mj_4.jpg}
\end{subfigure}

\caption{\mybold{Images of Barack Obama generated in Midjourney V5.} \normalfont{Images were created by progressively adding details to the prompt shown below each image: \textbf{A.} ``Portrait of Barack Obama." \textbf{B.} ``Portrait of Barack Obama, hyperrealistic, megapixel." \textbf{C.} ``Portrait of Barack Obama, sitting in his Oval Office, smiling, hyperrealistic, megapixel." \textbf{D.} ``A portrait of Barack Obama sitting in the Oval Office, smiling, wearing a suit and tie, shot on Kodak, hyperrealistic, grainy, official portrait."}}


\label{fig:obamaprompt}
\Description{Images of Barack Obama generated in Midjourney V5 by progressively adding more details to the prompt as such: \textbf{A.} "Portrait of Barack Obama". \textbf{B.} "Portrait of Barack Obama, Hyper realistic, Megapixel." \textbf{C.} "Portrait of Barack Obama, sitting in his oval office, smiling, hyperrealistic, megapixel". \textbf{D.} "A portrait of Barack Obama sitting in the Oval Office, smiling, wearing a suit and tie, shot on kodak, hyperrealistic, grainy, Official Portrait".} 
\end{figure*}

\subsubsection{Generating Images using AI tools.} Based on our curated set of real images, we generated images in Midjourney V5 and V6, Adobe Firefly Image 2, and Stable Diffusion to depict similar scenarios. In Midjourney and Firefly, we started the image generation process by creating a simple prompt describing the scenario. We then progressively refined the prompts by adding details about the quality of the image using keywords known to enhance image quality and resolution from the sources mentioned in~\autoref{sec:initializing-taxonomy}. Our prompts followed the basic structure of: ``[Subject description] [action or pose], [context or setting description], [clothing or appearance details], [image quality and style attributes], [camera or film type if applicable]." If the images were insufficiently realistic, then further details were added to the end of the above prompt such as: [specific details unique to the scenario], [`high resolution', `hyper-realistic', `megapixel', etc.]. The sequence of images in Figure \ref{fig:obamaprompt} shows the progression of a prompt and the resulting image qualities as more details and keywords are added in Midjourney V6. 

We also generated images inspired by real scenes of human interactions found in publicly available news sources and online media, ensuring they maintained a similar context and zoom level to real photographs. For example, we used a real reference image of a Ukrainian soldier getting married from New York Magazine~\cite{nymag2024ukraine}.

In Stable Diffusion, we developed custom pipelines in order to generate images that were more realistic than the outputs of the original models. Using SD1.5 \cite{rombach2021highresolution} and SDXL \cite{podell2024sdxl} as the base models, we used techniques such as merging fine-tuned portrait models and combining outputs of different models to reduce obvious artifacts and generate highly photorealistic images, particularly for portraits.  We also experimented with generating the same poses in various styles in order to isolate the impact of certain categories of artifacts, as shown in Figure \ref{fig:stylespipe}. We used ControlNets \cite{zhang2023adding} to maintain consistent poses while altering other elements such as models, seed, and prompt scheduling. Additionally, we used Low-Rank Adaptation (LoRAs) \cite{hu2021loralowrankadaptationlarge} to introduce realistic imperfections like wrinkles and shorten the depth of field to produce iPhone-style images. We further refined images by implementing pipelines that regenerate artifacts in the hands and faces. A refining pipeline is shown in Appendix \ref{fig:refiningpipe}. 
\begin{figure*}[htb]
\centering
\captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt}
\begin{subfigure}[t]{0.45\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/sd15variations.jpg}}}
\end{subfigure}
\begin{subfigure}[t]{0.5\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/pizzaboystyles.jpg}}}
\end{subfigure}
\caption{\mybold{Stable Diffusion pipeline and outputs of varied styles from the same pose and prompt.} \normalfont{\textbf{A.} Four pipelines for generating four variations of the prompt ``photo of a 25 year old man eating a slice of pizza, outside on the grass in a park, sunny, plain clothes." \textbf{B.} A sample of the variations that we labeled as having the style of a ``3D Render", ``Photoshoot", or ``iPhone photo." }}
\label{fig:stylespipe}
\Description{Two sets of images generated by Stable Diffusion showing style variations. A. Four variations of a man eating pizza in a park, generated with different pipelines. B. Samples labeled as "3D Render", "Photoshoot", or "iPhone photo" styles.}
\end{figure*}

\subsection{Crowdsourced Experiment} \label{sec:onlexp}

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{sections/images/website1.jpg}  
    \caption{A screenshot of the experiment website interface. }
    \label{fig:website}
    \Description{A screenshot of the experiment website interface}
\end{figure}


\subsubsection{Image Stimuli}
\label{sec:imagestimuli}

Across the entire experiment timeline, we collected 749,828 responses to whether 1083 images are AI-generated or real from 50,444 participants. Across the experiment timeline, we added and removed stimuli for two reasons. First, we included higher quality and more diverse images over the course of the experiment as new tools for controlling diffusion models became available (e.g., ControlNets and LORAs), and we identified prompt engineering techniques for producing more photorealistic images. Second, we split the experiment into two phases based on how we selected the diffusion model-generated images. In the first and main phase of the experiment, the stimuli were 149 real photographs and 450 most photorealistic images that our research team could generate with diffusion models. By comparison, the 482 stimuli in the second phase were based on generating 11 or more images for each of the 39 text prompts without curation. This second phase enables us to identify the effect of human curation (the selection bias involved in our research team selecting the most photorealistic AI-generated images) relative to no human curation on how accurately participants can distinguish AI-generated images.

\subsubsection{Experimental Design}\label{exp-design}

To ensure the quality of results, we implemented two measures. First, participants were shown an attention check image that was clearly AI-generated. Those who failed to identify this image correctly were excluded from the analysis. Second, we included an optional checkbox allowing participants to indicate if they recognized an image from outside the experiment, allowing us to filter out responses influenced by prior familiarity with the image. 

In the initial version of the experiment from February to May 2024, we prioritized presenting unseen images to participants rather than maintaining a balanced ratio of real and AI images. In the next version of the experiment, from May to June 2024, we used stratified random sampling to select stimuli, ensuring participants saw real images 50\% of the time and AI-generated images 50\% of the time. We repeated the analysis both with and without the data from the initial version of the experiment and did not find significant changes in the accuracy distributions. Details of this comparison are provided in Appendix \ref{tab:accuracy-comparison-dataset} and \ref{fig:accuracy-comparison-dstaset}. To ensure that newly added images to the stimuli set as described in \autoref{sec:imagestimuli} were adequately represented in participant responses, we implemented an up-sampling strategy that prioritized showing images that were labeled fewer than 100 times. 

After participants responded to five images, we randomized the display time of each subsequent image to one of the following conditions: unlimited time, 20 seconds, 10 seconds, 5 seconds, and 1 second. Participants were informed of the time limit at the start of each time-restricted trial by an on-screen message (e.g., "You have 20 seconds to view this image") and were instructed to click a button to reveal the image and begin the countdown.

\subsubsection{Participants}

We collected data through a public website (detectfakes.kellogg.northwestern.edu) where people could test their ability to detect AI-generated images. The website remained accessible throughout our taxonomy development, allowing us to gather responses as we updated image stimuli to reflect improvements in generation models. In total, 50,444 unique participants contributed 749,828 observations. According to Google Analytics, participants who visited our website came from 165 countries; the five countries with the most participants were United States, South Korea, United Kingdom, India, and Germany. We did not collect additional demographic data or other data on participants. 


\subsubsection{Image-level and Participant Level Analyses}\label{data-analysis}
We define accuracy as a binary measure of whether a participant selected the correct label (Real/AI-generated) for an image. We aggregated accuracy at two levels: image-level accuracy and participant-level accuracy. For image-level accuracy measurements described in Sections~\ref{sec:acc-general},~\ref{sec:acc-scene-complexity},~\ref{sec:acc-time},~\ref{sec:acc-presence-artifacts} and~\ref{sec:human-curation}, we aggregated and averaged the binary responses (0 for "real" and 1 for "AI-generated") provided by participants for each image. Image-level accuracy was calculated as the mean of correct identifications across various factors contributing to photorealism, which are described in each section. For participant-level accuracy measurements described in Section \ref{sec:indiv-acc}, we calculated each participant's accuracy by averaging their correct identifications across all viewed images. 

We present descriptive statistics to summarize our findings, focusing on mean accuracies and their associated 95\% confidence intervals (CIs) obtained through non-parametric bootstrapping~\cite{TransparentStatsJun2019}. We use these measures to describe trends and patterns in the data without using statistical significance to dichotomize effects.
However, readers can apply that interpretation to the CIs if they desire.

For a qualitative analysis of the 34,675 optional comments provided on our website, we utilized GPT-3.5-turbo to extract ten recurring themes and map the comments to our taxonomy categories based on the types of artifacts and patterns reported by participants. 

\subsubsection{Ethics} \leavevmode
This research complied with all relevant ethical regulations. The Northwestern University Institutional Review Board (IRB) determined that it met the criteria for exemption from further review. The study's IRB identification number is STU00220627.

\section{Taxonomy of Artifacts in AI-generated Images}
\label{sec:taxonomy}
 \begin{figure*}[h!]
\centering
\includegraphics[width=0.9\linewidth]{sections/images/artifacts.pdf}
\caption{\mybold{Categories of artifacts in AI-generated images.}  
\normalfont{This figure presents representative examples of common artifacts found in AI-generated images across five categories: 
\textbf{1. Anatomical Implausibilities:}  
\textbf{A.} Stable Diffusion image of a group of people where one woman has an abnormally long neck.  
\textbf{B.} Stable Diffusion image of a man eating pizza where his left fingers appear anatomically implausible. 
\textbf{2. Stylistic Artifacts:}  
\textbf{A.} Firefly image of a woman with a waxy texture.  
\textbf{B.} Stable Diffusion image with a cinematized style.  
\textbf{C.} Midjourney image of a woman with glossy skin. 
\textbf{3. Functional Implausibilities:}  
\textbf{A.} Stable Diffusion image where the guitar strings are not taut.  
\textbf{B.} AI-generated image of a woman holding a sandwich in an unlikely way.  
\textbf{C.} Firefly image where the strap on the red backpack merges into the denim jacket.  
\textbf{D.} AI-generated image of a woman wearing a shirt with incomprehensible text. 
\textbf{4. Violations of Physics:}  
\textbf{A.} Stable Diffusion image where the shadows fall in inconsistent directions.  
\textbf{B.} Stable Diffusion image of a woman standing in front of a mirror in which her reflection is inconsistent with the direction of her face. 
\textbf{5. Sociocultural Implausibilities:}  
\textbf{A.} Midjourney image of Donald Trump being restrained \cite{higgins2023tweet}.  
\textbf{B.} Firefly image depicting Ukrainian servicemen dressed in white shirts and hats that are not commonly part of the uniform. The flags on their shirts are different, and on the right serviceman, the flag is positioned awkwardly on their back and not their arm.  
\textbf{C.} Stable Diffusion image of an unlikely scenario of two Japanese businessmen hugging in a professional setting.}}

\label{fig:artifacts}

\Description{A composite figure illustrating five categories of AI-generated image artifacts:

1. Anatomical Implausibilities:  
   A. AI-generated image of a group of people, where one woman has an unnaturally long neck.  
   B. AI-generated image of a man eating pizza, where his left fingers appear anatomically incorrect, with extra or missing segments.  

2. Stylistic Artifacts:  
   A. AI-generated image of a woman with an unnaturally smooth and waxy skin texture.  
   B. AI-generated image with an exaggerated cinematic style, creating an overly dramatic effect.  
   C. AI-generated image of a woman with a glossy, plastic-like skin appearance.  

3. Functional Implausibilities:  
   A. AI-generated image of a guitar where the strings are not properly aligned or taut.  
   B. AI-generated image of a woman holding a sandwich in an awkward and impractical way.  
   C. AI-generated image where a red backpack strap appears to merge into the denim jacket, lacking clear separation.  
   D. AI-generated image of a woman wearing a shirt with distorted, incomprehensible text.  

4. Violations of Physics:  
   A. AI-generated image where shadows are inconsistent, falling in conflicting directions.  
   B. AI-generated image of a woman standing in front of a mirror, where her reflection does not match her actual pose.  

5. Sociocultural Implausibilities:  
   A. AI-generated image of Donald Trump being restrained in an unlikely or fabricated scenario.  
   B. AI-generated image depicting Ukrainian servicemen wearing incorrect uniforms with misplaced flags.  
   C. AI-generated image of two Japanese businessmen engaging in an uncharacteristically intimate hug in a formal setting.}
\end{figure*}

Our taxonomy organizes artifacts and implausibilities that may appear in AI-generated images into five high-level categories that are described in further detail in a how-to guide for identifying diffusion model-generated images\cite{kamali2024distinguish}. 

\textbf{1. Anatomical Implausibilities:} This category refers to artifacts that appear in the depiction of people within an image. These include unlikely artifacts in individual body parts, like hands with extra or missing fingers as shown in Figure \ref{fig:artifacts}--1B, or the disproportionately long woman's neck in Figure \ref{fig:artifacts}--1A. They also include artifacts in facial features such as an unnaturally empty gaze, overly shiny eyes, overlapping of the teeth and mouth, and unlikely proportions or configurations of limbs. In images of multiple people, this includes merged body parts and inconsistent proportions of body parts across different people. Anatomical implausibilities also include biometric artifacts such as size, shape, contours, and proportions of specific facial features if the person in the image is known. These biometric features include eyes, nose structure, mouth edges, interpupillary distance, ear shape and positioning, as well as distinctive markers like moles, dimples, and scars~\cite{Lakshmiprabha2011}


\textbf{2. Stylistic Artifacts}: This category refers to qualities of entire images or inconsistencies of those qualities within an image. This includes images of people that are waxy (Figure \ref{fig:artifacts}--2A), glossy (Figure \ref{fig:artifacts}-- 2C), shiny, and appear perfect like a model doing a photoshoot. These characteristics often appear in plastic--like skin and excessively soft hair. Additionally, this category includes noticeably cinematic, picturesque, and dramatic images that often appear in artistic photographs like Figure \ref{fig:artifacts}--2B. Stylistic artifacts also include inconsistencies between different subjects or parts of an image. This may appear as smudge-like distortions at the edges of different components or differences in resolution that make these parts look like they are cut out from different scenes.


\textbf{3. Functional Implausibilities}: Functional implausibilities result from a lack of understanding of the fundamental logic of real--world mechanical principles. This includes implausibilities in the objects themselves, their placement within the environment, and how the people in the image may be holding or using these objects, such as the woman holding a sandwich sideways in Figure \ref{fig:artifacts}--3B. Objects may also appear unable to function, like the loose strings of the guitar in Figure \ref{fig:artifacts}--3A, or placed in a way that they cannot function. Functional implausibilities also include distortion in fine details of the image. The image may present atypical designs in details like the print, buttons, and buckles on pieces of clothing, as seen in a backpack strap merging into a denim jacket in Figure \ref{fig:artifacts}--3C. Functional implausibilities also include errors in text, such as distorted or unconventional glyphs and odd spelling errors as seen in Figure \ref{fig:artifacts}--3D.

\textbf{4. Violations of Physics}: This category addresses inconsistencies in the image content that violate the expected logic of physical reality. Examples include shadows pointing in diverging directions, as shown in Figure \ref{fig:artifacts}--4A, or shadows that do not correspond to their light sources. Additionally, reflections on surfaces like water, mirrors, or shiny objects may appear misaligned with their surroundings, as illustrated in Figure \ref{fig:artifacts}--4B. Violations of physics also include depth and perspective issues, like warping and trajectories that do not align with the rest of the image. These distortions can also occur in real photographs, for example as seen with fish-eye lens distortions.


\textbf{5. Sociocultural Implausibilities}: This category includes scenarios that are socially inappropriate and unlikely to be seen in the real world, such as people wearing bathing suits at a funeral and a selfie with a bear. Violations of social and cultural norms could also be more subtle, present in details specific to certain cultures like Figure \ref{fig:artifacts}-- 5B and 5C attempting to depict Ukrainian and Japanese cultures, respectively. Historical inaccuracies and fake images of public figures in unlikely settings like 5A of Figure\ref{fig:artifacts} are also examples of sociocultural implausibilities.

