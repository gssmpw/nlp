\section{Conclusion}\label{sec:con}

Our work contributes empirical insights on the photorealism of AI-generated images and a taxonomy of artifacts commonly found in AI-generated images, organized into five categories: anatomical implausibilities, stylistic artifacts, functional implausibilities, violations of physics, and sociocultural implausibilities. We find that the photorealism of AI-generated images depends on the scene complexity of the image, the kind of artifacts and implausibilities, if any, detectable in an image, the duration of visual attention to an image, and the quality of human effort to select appropriate prompts and curate images. A question such as ``How photorealistic are state-of-the-art diffusion models'' may sound simple, but the answer is more complex and depends on many details, including what images are generated and selected, how photorealism is measured, what real images are included in the experiment, and how much time, skill, and effort a human participant has and willing to offer. This paper offers an initial exploration into how we can address this question and develops a practical taxonomy that offers scaffolding for building AI--literacy interventions to help people navigate the capabilities and limitations of diffusion models and whether an image is AI-generated or not. 

\begin{acks}

This material is based upon work supported by Robert Pozen, and in part with funding from the Department of Defense (DoD). Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the DoD or any agency or entity of the United States Government. We thank Will Thompson from Kellogg Research Support for performing a replication check.
\end{acks}