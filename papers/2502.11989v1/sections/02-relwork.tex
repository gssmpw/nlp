\section{Background}\label{sec:relwork}

\subsection{Limitations of machine learning approaches to detect AI-generated images}

Machine learning models for detecting AI-generated images are brittle and lack robustness to simple data transformations. Corvi et al.~\cite{corvi2023intriguingpropertiessyntheticimages} compare four different machine learning approaches to deepfake detection and demonstrate that recropping and compression – simple modifications common on social media – lead to drops in accuracy such that the classifiers are nearly just as good as random guessing. Dong et al.~\cite{9879575}reveal the ease with which spectral artifacts used in the identification of GAN-generated images can be mitigated via blurring and resizing, demonstrating a noticeable decrease in accuracy under basic modifications. Cozzolino et al.~\cite{cozzolino2024raisingbaraigeneratedimage} demonstrate that post--processing images by random--cropping, resizing, and compression lead to a drop in AI-generated image detection from 90\% accuracy to 85\% accuracy. The fundamental problem is that machine learning models for deepfake detection lack robustness to context shift, out--of--distribution data and adversarial perturbations ~\cite{wang2023deepfakedetectioncomprehensivestudy, ha2024, groh2022identifying, hulzebosch2020detectingcnngeneratedfacialimages}. 

How an image is generated influences the ability of deepfake detection classifiers to accurately identify it as AI-generated. Classifiers trained to detect GAN-generated images tend to fail to detect diffusion model-generated images. For example, the approach to detecting GAN-generated images based on frequency spectra~\cite{marra2018gansleaveartificialfingerprints, yu2019attributingfakeimagesgans, 9035107, durall2020watchupconvolutioncnnbased, bi2023detectinggeneratedimagesreal, pmlr-v119-frank20a} and inconsistencies in head poses and facial landmark positions~\cite{yang2018exposingdeepfakesusing, yang2019exposinggansynthesizedfacesusing, Mundra_2023_CVPR}, do not generalize to detecting images generated by diffusion models~\cite{ojha2024universalfakeimagedetectors}. GAN-trained detection models miss these patterns because they have learned patterns for identifying GAN-generated images~\cite{wang2020cnn, ricker2024detectiondiffusionmodeldeepfakes}. Likewise, it is possible to learn the statistical regularities in diffusion model-generated images but these regularities are not invariant to image post-processing.~\cite{xi2023aigeneratedimagedetectionusing, 10334046, wang2023dirediffusiongeneratedimagedetection, ma2023exposingfakeeffectivediffusiongenerated, yang2023diffusion}. 

Moreover, machine learning models' lack of robustness for detection is exacerbated by the changing architectures of generative AI models~\cite{lin2024, Mirsky2021}. Vision transformers~\cite{radford2021learning, ojha2024universalfakeimagedetectors} and multi--architecture training~\cite{epstein2023onlinedetectionaigeneratedimages, porcile2024findingaigeneratedfaceswild, jia2024can} show promise for enhancing the detection of AI-generated images, but adversarial attacks and large architectural changes in generative models continue to affect robustness of detection.  

Figure~\ref{fig:AI-faces} highlights the increasing complexity of AI-generated images over the past decade. The changing architectures and increasing photorealism pose a challenge for both humans and machines to distinguish real from AI-generated images. However, humans and machines are fundamentally different. For example, humans can critically reason about an image's elements and its context~\cite{wang2023context}. On the other hand, machine learning classifiers for detecting AI-generated images often oversimplify image authenticity as a question of real versus fake and ignore the critical reasoning about component parts and sub--questions that an ordinary person or digital forensics expert may consider when evaluating an image's authenticity~\cite{jacobsen2024deepfakes}.


\begin{figure*}[h]
\centering
\captionsetup{justification=raggedright, singlelinecheck=false, skip=2pt}
\begin{subfigure}[t]{0.13\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/ganfacesgoodfellow.jpg}}}
    \caption*{\footnotesize GAN '14}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.13\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/dcgan.jpg}}}
    \caption*{\footnotesize DCGAN '15}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.13\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/PGGAN.jpg}}}
    \caption*{\footnotesize PGGAN '18}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.13\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/stylegan.jpg}}}
    \caption*{\footnotesize StyleGAN '19}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.13\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/stylegan2.jpg}}}
    \caption*{\footnotesize StyleGAN2 '20}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.13\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/sdxl.jpg}}}
    \caption*{\footnotesize SDXL '23}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.13\textwidth}
    \subcaption{}\vtop{\vskip0pt\hbox{\includegraphics[width=\linewidth]{sections/images/ff_portrait3_021.jpeg}}}
    \caption*{\footnotesize Firefly '24}
\end{subfigure}
\caption{\mybold{Exemplar images of photorealism across a range of generative models.} \normalfont{Examples of AI-generated images from 2014 to 2024~\cite{goodfellow2014generativeadversarialnetworks,radford2015unsupervised, faceimageforgery,karras2018progressivegrowinggansimproved,karras2019style,karras2020analyzingimprovingimagequality, podell2024sdxl,adobe_firefly}.}}
\label{fig:AI-faces}
\Description{Portrait images from various image generation models that improve in quality and complexity over the years.}
\end{figure*}


\subsection{Human perception and evaluation of AI-generated media}

In response to the increasing realism of AI-generated media, researchers have been examining the degree to which humans can distinguish between authentic and AI-generated media. For example, researchers found that GAN-generated images of faces are indistinguishable from real face portraits~\cite{nightingale2022ai, Lago_2022}. However, for video deepfakes, humans are much better than random guessing~\cite{deepfakedetectionbyhumancrowds}, which may in part be due to humans' specialized ability to process the temporal elements of faces~\cite{deepfakedetectionbyhumancrowds, sinha2006face}. Researchers found that text--to--speech voices were rated as lower in quality and clarity than human voices in 2020~\cite{cambre2020choice} but have reached the point where research participants cannot tell the difference between short 20-second recordings of AI-generated voices and authentically recorded voices~\cite{barrington2024people}.

Recent research has identified specific cues and heuristics that people use to evaluate AI-generated media. For example, cues such as recording settings in the detection of text-to-speech audio~\cite{han2024uncovering} and speaking patterns in political deepfake videos~\cite{groh2024human}. However, two studies found that participants rarely attributed their judgments to specific visual features~\cite{hameleers2024they, wohler2021towards}, and in one of these deepfake studies, researchers found that participants are noticing the artifacts but rarely linking these to manipulation~\cite{wohler2021towards}. With respect to AI-generated text, research has highlighted that people tend to use flawed heuristics when attempting to distinguish AI-generated text from human--written text, like associating grammatical errors with AI-generation~\cite{Jakesch2022HumanHF}. 

Social context also plays a significant role in both what diffusion models generate~\cite{luccioni2024stable} and how people form beliefs about AI-generated images and their content. For example, researchers have found detection ability is influenced by shared identity between the viewer and subject of the content~\cite{mink2024s}. Furthermore, researchers have found that white AI-generated faces were disproportionately judged as human more frequently than their real counterparts~\cite{miller2023ai}. GAN-generated faces in portrait images were often perceived as more trustworthy than real faces~\cite{nightingale2022ai}, and as a result, people were less likely to question their authenticity~\cite{Lago_2022}. In instances where AI-generated images are linked to misinformation, researchers find that labeling AI-generated images and the associated content as ``potentially misleading" instead of simply ``AI-generated" had a stronger influence on curtailing participants' self--reported intentions to share misinformation~\cite{epstein2023label, wittenberg2024labeling}.

Researchers have approached a number of methods for measuring photorealism perceived by humans. For example, prior research has examined photorealism with carefully worded questions such as ``Is the image photorealistic?''~\cite{liang2024rich}, ``Does the image look like a real photo or an AI-generated image?''~\cite{lee2024holistic, otani2023toward} and ``Whether this image could be taken with a camera?''\cite{yan2024sanitycheckaigeneratedimage}. These questions are useful for assessing participants' subjective opinions but do not capture the human ability to distinguish real images from fake images and can potentially suffer from demand characteristic bias. Another approach has been to characterize photorealism by examining the features that can influence realism, such as aesthetics and semantically meaningful content of an image~\cite{peng2024crafting}. A third approach involves simply defining images as photorealistic if they are rendered with computer graphics software~\cite{lyu2005realistic}. In this paper, we approach photorealism from the psychophysics perspective, examining participants' objective performance at distinguishing real images from fake images~\cite{zhou2019hype}. 

\subsection{Categorizing artifacts and implausibilities in diffusion model-generated images} \label{sec:artifimpl}

Previous research on earlier versions of diffusion models categorized the kinds of qualitative failures of diffusion model-generated images as distorted body parts, impossible geometry, physics violations, illogical relationships in a scene, and noise~\cite{borji2023qualitative}. In addition to obvious issues with hands, feet, eyes, and teeth, research at the intersection of digital foresnics and AI-generated images shows details such as corneal reflections~\cite{hu2021exposing} and irregular pupil shapes~\cite{guo2022eyes} can also be artifacts.  Likewise, violations of physics like implausible shadows, lighting, and perspective errors~\cite{farid2022perspectiveinconsistencypainttext, farid2022lighting, sarkar2024shadows} often occur in diffusion model generated images that otherwise appear photorealistic. 