\section{Related Work}
\label{sec:related_work}
 \subsection{Video Anomaly Analysis} 
   Analyzing a video's anomaly includes many aspects.
 For the detection task, BN-WVAD\cite{BN-WVAD} embeds videos with I3D\cite{I3D} and enhances the features with a Transformer. A convolutional classifier then outputs an anomaly score.
 For the captioning task, PDVC\cite{PDVC} uses C3D\cite{C3D} for feature extraction, a Transformer-based encoder-decoder for multi-task prediction, and an LSTM for caption generation.
 For the temporal grounding task, MMN\cite{MMN} employs C3D and DistillBERT\cite{DistilBERT} to embed both video and text queries into a shared space for mutual matching.
These top-performing methods highlight the integration of traditional CNNs with Transformers; however, this approach undermines the inherent generalization capabilities of Transformer architectures.
 
 Meanwhile, the different metrics and annotations across benchmarks for different tasks make it challenging to evaluate general models.
 UCC\cite{UCFCrime} includes only abnormal categories and evaluates classification performance using Area Under Curve (AUC). Furthermore, UCA\cite{UCA} segments the UCC videos and annotates each part with a sentence, allowing evaluation of temporal grounding using Intersection over Union (IoU) and captioning using BLEU\cite{BLEU}.




 \begin{table*}[ht]
  \centering
  \setlength\tabcolsep{4pt}
  \renewcommand{\arraystretch}{1.2}
  {\Large
  \resizebox{1.0\textwidth}{!}{
      \begin{tabular}{c|c|l|p{0.1\textwidth}}
      \Xhline{1.0pt}
      \textbf{Task} & \textbf{Capability} & \textbf{QA Example} & \textbf{Truth} \\
      \Xhline{1.0pt}
      \textbf{Anomaly} & Anomaly   & \textit{\myblue{Is there any abnormal event that might be related to violence, crime or danger in this video?}} &  \multirow{2}{*}{\centering Yes} \\
      \textbf{Detection}  & Perception &  \textit{\myblue{You should only answer Yes or No.} }                  & ~ \\
      \Xhline{1.0pt}
      \textbf{Anomaly}         & Coarse   & \textit{\myblue{We believe that a dangerous event occurred in this video. Identify its category.}} &  \multirow{2}{*}{Assault} \\
      \textbf{Classification}  & Analysis &  \textit{\myblue{Your Answer should only consist of the categories from \texttt{[13 crime types]}.}}                 & ~ \\
      \Xhline{1.0pt}
      \textbf{}              & Causal   & \textit{Who was the main perpetrator in the video? } (A)The police officer. (B)The man in a white T-shirt. &  \multirow{2}{*}{\centering B} \\
      \textbf{}             & Reasoning &  { (C)The woman in white clothes.  (D)The individual pointing at the suspect.}    & ~ \\
      \cline{2-4}
      \textbf{}         & Object Detail   & \textit{How was the suspect transported away?} (A) Stretcher. (B) Car. (C) Electric scooter. (D) Bike. &  A \\
      \cline{2-4}
      \textbf{Multi-Choice} & & \textit{{What action did the male police officer in a brown uniform take towards the man in the white T-shirt?}} &  \multirow{2}{*}{\centering C}\\
      \textbf{Question}                & Action & {(A) Shot him. (B) Walked away. (C) Subdued and forced him to the ground. (D) Ignored him.} &  ~     \\
      \cline{3-4}
      \textbf{(MCQ)}                & Recognition  & \textit{{What was the response of the man in the white T-shirt when additional officers tried to handcuff him?}} &  \multirow{2}{*}{\centering D}\\
      \textbf{} & & {(A) He cooperated. (B) He ran away. (C) He surrendered. (D) He showed resistance.} &  ~     \\
      \cline{2-4}
       ~         & \multirow{2}{*}{Count}  & \textit{{How many black police officers joined the arrest operation later in the video?}} &  \multirow{2}{*}{\centering B}\\
        ~ & ~ & (A) One. (B) Two. (C) Three. (D) None. & ~\\
      \Xhline{1.0pt}
       \textbf{}      & Anomaly & \textit{\myblue{The video lasts for} 105 \myblue{seconds, and} 32 \myblue{frames are uniformly sampled from it. These frames are}}  & \{"start\_ \\
       \textbf{Time} & Perception  & \textit{\myblue{located at} \texttt{[32 timestamps]}\myblue{. Detect an abnormal event of Arrest in it and locate the start time}}  &time":21,  \\ 
       \cline{2-2}
        \textbf{Grounding}  &Causal  &\textit{\myblue{and the end time of this event.}} & "end\_time"\\
         &Reasoning &\textit{\myblue{Your answer should ONLY be \{"start\_time": start\_time, "end\_time": end\_time\}.}}   &  :109\}   \\
      \Xhline{1.0pt}
      \textbf{}                 &Anomaly      & \multicolumn{2}{l}{(For AD) \textit{\myblue{We believe an event of} Arrest \myblue{occurs in this video.}} (For both) \textit{\myblue{Detect whether this video contains  }}}\\
      \textbf{Event}            &Perception   & \multicolumn{2}{l}{\textit{\myblue{abnormal events or only normal events, and then give a description of the detected events with details, }}}\\
      \cline{2-2}
      \textbf{Description}      &Video        & \multicolumn{2}{l}{\textit{\myblue{especially environment, human looking and action.}}}\\
      \textbf{}                 &Synthesis    & \multicolumn{2}{l}{~~Truth: An \darkGreen{arrest} operation occurred in a public area. \myred{A male police officer} in a brown uniform confronted }\\
      \cline{1-2}
      \textbf{}                 &       & \multicolumn{2}{l}{\darkGreen{a man in a white T-shirt}, who was holding a bag, while \myred{a woman in white clothes} observed. The officer \darkGreen{} }\\
      \textbf{Anomaly}          &\multirow{2}{*}{Video}       & \multicolumn{2}{l}{\darkGreen{subdued the man, forcing him to the ground}. Another individual intervened, \myred{pointing at the suspect}. Later, }\\
      \textbf{Description}      &\multirow{2}{*}{Synthesis}       & \multicolumn{2}{l}{two black police officers joined,  with one putting on gloves to assist in lifting  the man and pushing him     }\\
      \textbf{}                 &      & \multicolumn{2}{l}{against a wall. \darkGreen{The man showed resistance}, but additional officers helped to handcuff him. Eventually, the  }\\
      \textbf{}                 &      & \multicolumn{2}{l}{ handcuffed man was placed on  a \darkGreen{stretcher} and transported by the police.}\\
      \Xhline{1.0pt}

    \end{tabular}
  }}
  \caption{An elaboration on UCVL's QA design. \myblue{Blue} marks the frozen question sentences across all videos, 
            while the question sentences for MCQs are automatically generated by the LLM. The truth for description questions are the summary of UCA‘s annotations.
            The \myred{red} phrases refer to the distractor options of the MCQs, while the \darkGreen{green} phrases refer to the correct option.
            }
    \label{table:QADesk}
\end{table*}


 
 
 \subsection{Multimodal Large Language Models}
 The capabilities of MLLMs have been extended to handle multi-image sequences and videos without altering the structure used for single-image inputs\cite{llava-onevision,internvl}.
While MLLMs generally adopt a universal architecture that includes a ViT encoder, an LLM decoder, and a connector that maps visual tokens to text embeddings, they differ in terms of training procedures and data.
 LLaVA-OneVision\cite{llava-onevision} and InternVL2\cite{internvl}  obtain video capability through finetuning on broad range of open-source academic datasets, including single-image, multi-image, and video of various tasks. %, while they pre-train on only single-image datasets.
 % LLaVA-OneVision first trains vision-language alignment using single-image-text pairs, and then finetunes on diverse open-source academic datasets, including single-image, multi-image, and video data of various tasks.
In contrast, Qwen2-VL\cite{qwen2VL} employs a three-stage training approach with distinct alignment processes, leveraging a proprietary data configuration.
 % InternVL2 takes advantage of its own InternViT, which has the largest parameters and resolution, and unfreezes all parts across 2 training stages on a mixture of publicly accessible datasets.







 \subsection{MLLM Benchmark}
 %介绍一下多模态大模型榜单的基本内容（问题设置、问题范围、是否有NSFW数据），重点介绍他们在开放问答方面的情况。
 %我们并不清楚这些榜单里是否含有异常检测数据，但是在它们的能力图里，并没有涉及异常概念的理解。
Quantitative evaluation of MLLMs relies on benchmarks designed to assess the model’s capabilities across diverse topics and aspects. Each benchmark is typically assigned to a specific category, such as General QA, Document/Chart, Math, Code, OCR, Grounding, and others, with certain methods for scoring open-ended responses.  We compare 4 video benchmarks in Table \ref{table video large model benchs}. To assess models' responses, image-text benchmark VQAv2\cite{VQAv2} simplifies scoring by using standard and concise answers such as yes/no, 1/5 and kite/surfboard. 
  Meanwhile, video-MME\cite{video-mme} and many other benchmarks predominantly rely on multiple-choice questions (MCQs) for evaluation. 
 Additionally, MM-Vet\cite{mm-vet} leverages GPT for long-text comparison with few-shot instructions, which is also integrated in MMBench-Video\cite{mmbench-video}.

 Despite the abundance of image-text benchmarks, video benchmarks for MLLMs remain limited in number and have yet to include topics like anomaly analysis or the assessment of open-ended responses.