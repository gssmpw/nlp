\section{Related Work}
Unsupervised domain adaptation is a wildly used setting of transfer learning methods that aims to minimize the discrepancy between the source and target domains. To solve cross-domain classification tasks, these methods are based on deep feature representation**Ben-David et al., "A Theory of Learning from Different Domains"**, which maps different domains into a common feature space. Some recent studies have overcome the imbalance of domains and the label distribution shift of classes to transfer model well**Mansour et al., "Domain Adaptation with Multiple Sources"**. Some novel settings in domain adaption have also gotten a lot of attention, like source free domain adaption(SFDA)**Wilson et al., "Deep Unsupervised Learning for Domain Adaptation"**, test time domain adaption(TTDA)**Shankar et al., "Learning Hierarchical Compositions of Neural Networks"**.
As for graph-structured data, several studies have been proposed for cross-graph knowledge transfer via GDA setting methods**He et al., "Graph Attention Network"**. ACDNE**Zhang et al., "Attribute-Invariant Cross-Domain Embedding Learning"** adopt k-hop PPMI matrix to capture high-order proximity as global consistency for source information on graphs. CDNE**Li et al., "Cross-Modal Domain Adaptation via Dual Transfer Networks"** learning cross-network embedding from source and target data to minimize the maximum mean discrepancy (MMD) directly. GraphAE**Zhang et al., "Graph Attention Network with Adversarial Training"** analyzes node degree distribution shift in domain discrepancy and solves it by aligning message-passing routers. DM-GNN**Wang et al., "Domain Adaptation for Node Classification with Graph Convolutional Networks"** proposes a method to propagate node label information by combining its own and neighbors’ edge structure. 
UDAGCN**Zhang et al., "Dual Graph Convolutional Network for Domain Adaptation"** develops a dual graph convolutional network by jointly capturing knowledge from local and global levels to adapt it by adversarial training. ASN**Guo et al., "Adversarial Network Learning for Cross-Domain Classification"** separates domain-specific and domain-invariant variables by designing a private en-coder and uses the domain-specific features in the network to extract the domain-invariant shared features across networks.  SOGA**Zhang et al., "Structural Consistency for Source-Free Domain Adaptation"** first time uses discriminability by encouraging the structural consistencies between target nodes in the same class for the SFDA in the graph. GraphAE**Zhang et al., "Graph Attention Network with Adversarial Training"** focuses on how shifts in node degree distribution affect node embeddings by minimizing the discrepancy between router embedding to eliminate structural shifts.
SpecReg**Mansour et al., "Spectral Regularization for Unsupervised Domain Adaptation"** used the optimal transport-based GDA bound for graph data and discovered that revising the GNNs’ Lipschitz constant can be achieved by spectral smoothness and maximum frequency response.  JHGDA**Li et al., "Joint Hierarchical Graph Domain Adaptation"** studies the shifts in hierarchical graph structures, which are inherent properties of graphs by aggregating domain discrepancy from all hierarchy levels to derive a comprehensive discrepancy measurement. ALEX**Zhang et al., "Adversarial Label Shift Enhanced Graph Augmentation"** first creates a label shift enhanced augmented graph view using a low-rank adjacency matrix obtained through singular value decomposition by driving contrasting loss. SGDA**Wang et al., "Spectral Graph Domain Adaptation"** enhances original source graphs by integrating trainable perturbations (adaptive shift parameters) into embeddings by conducting adversarial learning to simultaneously train both the graph encoder and perturbations, to minimize marginal shifts.