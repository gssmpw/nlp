
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{ulem}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands}
\input{my_macro}
\usepackage{setspace} % In preamble or before your content 
\usepackage{graphicx}  % 用于图像处理
\usepackage{wrapfig}   % 用于包裹文本的图形
% \usepackage{subcaption} % 用于子图
\usepackage{subfigure}
\usepackage{multirow}
% \usepackage{subfigure}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{mathtools}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{makecell}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{minitoc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{adjustbox}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{caption}
\usepackage{soul}
\usepackage{enumitem}
\theoremstyle{plain}
\usepackage{array}
\theoremstyle{definition}
\usepackage{ulem}


\usepackage{algorithm2e}
% \theoremstyle{remark}
% \usepackage[textsize=tiny]{todonotes}
% \usepackage[toc,page,header]{appendix}
% \newtheorem{proposition}{Proposition} 
% \newtheorem{theorem}{Theorem}
% \newtheorem{prop}{Proposition}
% \newtheorem{assumption}{Assumption}
% \newtheorem{lemma}{Lemma}
% \newtheorem{cor}{Corollary}
% \newtheorem{definition}{Definition}
% \newtheorem{remark}{Remark}
\newcommand{\ppa}{{\small\textsc{ogbl-ppa}}}
\newcommand{\ddi}{{\small\textsc{ogbl-ddi}}}
\newcommand{\collab}{{\small\textsc{ogbl-collab}}}
\newcommand{\cora}{{\small\textsc{cora}}}
\newcommand{\citepseer}{{\small\textsc{citeseer}}}
\newcommand{\pubmed}{{\small\textsc{pubmed}}}

\newcommand{\power}{{\small\textsc{power}}}
\newcommand{\photo}{{\small\textsc{photo}}}


\renewcommand \thepart{}
\renewcommand \partname{}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\definecolor{firstcolor}{HTML}{009E73}
\definecolor{secondcolor}{HTML}{0072B2}
\definecolor{thirdcolor}{HTML}{D55E00}

\newcommand\colorfirst[1]{\textcolor{firstcolor}{\textbf{#1}}}
\newcommand\colorsecond[1]{\textcolor{secondcolor}{\textbf{#1}}}\newcommand\colorthird[1]{\textcolor{thirdcolor}{\textbf{#1}}}


\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}  %也可以使用{color}宏包，但是颜色库里面颜色较少，使用xcolor会更好
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{myblue}{HTML}{3b75c3}
\definecolor{myred}{HTML}{E33222}
\definecolor{mygreen}{HTML}{438773}
\definecolor{mymaroon}{RGB}{142,27,19}
\definecolor{maroon}{HTML}{800000}
\definecolor{mycite}{cmyk}{0.55,1,0,0.15}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\hypersetup{
    colorlinks=true,
    citecolor=mymaroon,
    linkcolor=codeblue,
    urlcolor=maroon
          }



\title{On the Benefits of Attribute-Driven Graph Domain adaptation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Ruiyi Fang$^{1,*}$\quad  Bingheng Li$^{2,}$\thanks{Equal contribution}\quad Zhao Kang$^{4}$\quad  Qiuhao Zeng$^{1}$\quad  Nima Hosseini Dashtbayaz$^{1}$\\
\textbf{Ruizhi Pu}$^{1}$\thanks{Corresponding author} \quad
\textbf{Boyu Wang}$^{1,3}$\quad 
\textbf{Charles Ling}$^{1,3}$\quad \\
$^{1}$  Western University \quad\quad 
$^{2}$ Michigan State University\quad\quad 
$^{3}$ Vector Institute\\
$^{4}$ University of Electronic Science and Technology of China\\
\texttt{\{rfang32,rpu2,qzeng53,charles.ling\}@uwo.ca} \\
\texttt{libinghe@msu.edu} \quad \texttt{zkang@uestc.edu.cn}\quad \texttt{ bwang@csd.uwo.ca}\\
}

% \author{Li Yi$^{*}$\quad  Gezheng Xu\thanks{Equal contribution}\quad  Pengcheng Xu\quad  Jiaqi Li\quad  Ruizhi Pu\\
% \textbf{Charles Ling}\quad 
% \textbf{A. Ian McLeod}\quad 
% \textbf{Boyu Wang}\thanks{Corresponding author}\quad \\
% University of Western Ontario\\
% \texttt{\{lyi7,gxu86,pxu67,jli3779,rpu2,charles.ling,aimcleod\}@uwo.ca} \\
% \texttt{ bwang@csd.uwo.ca} \\
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\renewcommand{\fix}{\marginpar{FIX}}
\renewcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}

Graph Domain Adaptation (GDA) addresses a pressing challenge in cross-network learning, particularly pertinent due to the absence of labeled data in real-world graph datasets. Recent studies attempted to learn domain invariant representations by eliminating structural shifts between graphs. In this work, we show that existing methodologies have overlooked the significance of the graph node attribute, a pivotal factor for graph domain alignment. 
Specifically, we first reveal the impact of node attributes for GDA by theoretically proving that in addition to the graph structural divergence between the domains, the node attribute discrepancy also plays a critical role in GDA. Moreover, we also empirically show that the attribute shift is more substantial than the topology shift, which further underscore the importance of node attribute alignment in GDA. Inspired by this finding, a novel cross-channel module is developed to fuse and align both views between the source and target graphs for GDA. Experimental results on a variety of benchmark verify the effectiveness of our method.
\end{abstract}

\section{Introduction}

In the area of widespread internet data collection, graph vertices are frequently associated with content information, referred to as node attributes within basic graph data. 
Such graph data can be widely used in prevalent real-world applications, with data suffering from label scarcity problems in annotating complex structured data is both expensive and difficult~\citep{xu2022graph}. To solve such a challenge, transferring abundant labeling knowledge from task-related graphs is a method considered~\citep{chen2019graph}. Giving labeled graphs as a source to solve unlabeled graph targets has been proposed as graph domain adaptation (GDA) as a paradigm to effectively transfer knowledge across graphs by addressing distribution shifts~\citep{shi2024graph}.
\begin{wrapfigure}{R}{0.45\textwidth}
    \vspace{-1.5em}
    \centering
    \includegraphics[width=0.45\textwidth]{Fig1.pdf}
    \vskip -1em
    \caption{\footnotesize This represents feature value in two groups of datasets. This shows the feature value distribution gap in the {attribute} is larger than in the {topology}.\label{figdis1}}
    \vskip -1em
\end{wrapfigure}

%Basically, pioneering works create tasks on node classification for transferring knowledge on graph.
%Early works directly use strategy form domain adaptation (DA) in computer version field. 
Early works on GDA apply deep domain adaptation (DA) techniques directly, thereby{ ~\citep{shen2020network, shui2023towards, wu2020unsupervised, shen2020adversarial, dai2022graph}} without considering the topological structures of graphs for domain alignment. To address this issue, several recent works have been proposed to leverage the inherent properties of graph topology (e.g., adjacency matrix). While these methods~\citep{yan2020graphae,shi2023improving,shen2023domain,wu2023non} have achieved substantial improvements by alleviating the topological discrepancy between domains, they overlook the importance of node attributes, a fundamental aspect of GDA. To verify our argument, we investigate the projected feature values\footnote{Details on the construction of project features are presented in Section\ref{Definition of average feature value} of Appendix.} of graph topology and attribute on two GDA benchmarks, as shown in Figure\ref{figdis1}. It can be observed that feature value discrepancy exists in all GDA benchmark inside datasets, with feature value discrepancy for attributes significantly larger than topology feature value discrepancy.
Based on this observation, we can conclude that (1) graph distribution shift exists in both attribute and topology; (2) attribute divergence between the source and target graphs is more significant than the topology divergence.

%Due to that influence, most early works~\citep{shen2020network, wu2020unsupervised, shen2020adversarial, dai2022graph} in GDA have mainly relied on source data. However, they completely rely on the information of the source domain and ignore the target domain. Recently, some methods~\citep{zhang2021adversarial,qiao2023semi} based on eliminating complex graph domain shift information have gained attention by using precisely designed graph feature extraction frameworks. While the above strategy may align overall embedding distributions, it fails to capture the nuances of differences in complex graph structures and task-specific components, leading to subpar outcomes. To address the above issue, researchers have proposed methods focusing on eliminating graph structural divergence~\citep{yan2020graphae,shi2023improving,shen2023domain,wu2023non}, which usually directly analyze the inherent properties of graph topology (e.g adjacency matrix). While these methodologies consider the graph's structural characteristics, they overlook a fundamental aspect of node classification tasks: the impact of node attributes. 


%To address our concerns, we investigated the distribution shift of graph attributes. It aims to show divergence of graph topology and attribute feature value respectively in Fig.\ref{figdis1}. Specifically, we obtain a value roughly representing the overall characteristics of a graph by calculating the average value of L1-nrom of the $AX$ and $\hat{A}X$ to represent the topology feature value and the attribute feature value, respectively. As shown in Fig.\ref{figdis1}, we observe that the feature value gap of the attribute graph is larger than that of the topology graph. This illustrates that the attribute divergence between the source and target graphs is more significant than the topology divergence.
 




\begin{figure}[!htbp]
    \centering
    \subfigure[Framework Overview]{
    \includegraphics[width=85mm]{Fig1-2.pdf}
    }
    \hspace{+5mm}
    \subfigure[Attribute-Driven Example]{
    \includegraphics[width=40mm]{Fig1-1.pdf}
    }
    \vspace{-2.mm}
     \caption{(a) An overview of our method. GAA gives attribute and topology graph representation, where minimizing source and target distribution shift through two views. (b)(i) Distribution shifts exist in both topology and attribute views before alignment. (ii) Existing GDA algorithms can only address graph topology shifts but not attribute shifts. (iii) GAA can address GDA attribute shifts.}
  \label{fig:overview}
\end{figure}





Motivated by this observation, we theoretically investigate the domain discrepancy between two graphs, revealing the role of node attribute for GDA. Specifically, by leveraging the PAC-Bayes framework, we derive a generalization bound of GDA, which unveils how graph structure and node attributes jointly affect the expected risk of the target graph. Moreover, we also show that the discrepancy between the source and target graphs can be upper bounded in terms of both node attributes and topological structure. In other words, our theoretical analysis reveals that both attribute and topology views should be considered for GDA, with the former having a more significant impact on domain alignment, as shown in Figure .\ref{figdis1}.


Our theoretical insights highlight the significance of characterizing the cross-network domain shifts in both node attributes and topology. To this end, we propose a novel cross-channel graph attribute-driven alignment (GAA) algorithm for cross-network node classification, as shown in Figure.\ref{fig:overview} (a). Unlike existing methods that rely solely on topology, GAA also constructs an attribute graph (feature graph) to mitigate domain discrepancies. Furthermore, GAA also introduces a cross-view similarity matrix, which acts as a filter to enhance and integrate feature information within each domain, facilitating synergistic refinement of both attribute and topology views for GDA. Figure\ref{fig:overview} (b) illustrates the benefits of GAA for GDA, which alleviates both attribute and topology shifts. 

%Inspired by the aforementioned theoretical result, the crucial idea is to characterize the cross-network knowledge transferability from both node attribute and structural information. 
%As shown in  Fig.\ref{fig:overview} (b), our attribute-driven alignment method eliminates both attribute divergence and topology divergence while other existing methods ignore attribute divergence. 
%We propose a novel cross-channel GDA attribute-driven alignment (GAA) framework to solve a significant task in cross-network learning: cross-network node classification as shown in Fig.\ref{fig:overview} (a). 
%GAA utilizes an attribute graph (feature graph) to provide node attribute information, which carries meaningful node attribute information while remaining adaptive to downstream tasks. 
%Then, GAA introduces a cross-view similarity matrix to improve the reliability of attributes, and then obtains node attributes and topology embedding through a cross-view similarity matrix filter during adaptation training.
%After training, adaptation results are obtained by aligning both the attribute and topology embedding, which eventually reduces the conditional shift of the source and target graphs. The efficiency of the proposed GAA framework is confirmed on various cross-network node classification mining data sets.


%As shown in Fig.\ref{fig:overview}, we obtain graph information in both the topology view and attribute view, and then we minimize the distribution shift in both views. As a result, attribute-driven alignment can directly reduce the generalization error between the target and source graph. Ultimately, we enhance GDA by minimizing the domain shift across the graph's node attribute and topology perspectives.

Our main contributions are summarized as follows:
\begin{itemize}
    \item We reveal the importance of node attributes in GDA from both empirical and theoretical aspects.
    \item Motivated by our theoretical analysis, we proposed GAA, a novel GDA algorithm that minimizes both attribute and topology distribution shifts based on intrinsic graph property.
    \item Comprehensive experiments on benchmarks show the superior performance of our method compared to other state-of-the-art methods for real-world datasets of the cross-network node classification tasks.
\end{itemize}

\section{Related Work}

Unsupervised domain adaptation is a wildly used setting of transfer learning methods that aims to minimize the discrepancy between the source and target domains. To solve cross-domain classification tasks, these methods are based on deep feature representation~\citep{zhu2022weakly,zengtowards}, which maps different domains into a common feature space. Some recent studies have overcome the imbalance of domains and the label distribution shift of classes to transfer model well~\citep{jing2021towards,xu2023class,pu2024leveraging,zeng2024latent}. Some novel settings in domain adaption have also gotten a lot of attention, like source free domain adaption(SFDA)~\citep{yang2021generalized, xu2025unraveling}, test time domain adaption(TTDA)~\citep{wang2022continual}.
As for graph-structured data, several studies have been proposed for cross-graph knowledge transfer via GDA setting methods~\citep{shen2019network,dai2022graph,shi2024graph}. ACDNE~\citep{shen2020adversarial} adopt k-hop PPMI matrix to capture high-order proximity as global consistency for source information on graphs. CDNE~\citep{shen2020network} learning cross-network embedding from source and target data to minimize the maximum mean discrepancy (MMD) directly. GraphAE~\citep{yan2020graphae} analyzes node degree distribution shift in domain discrepancy and solves it by aligning message-passing routers. DM-GNN~\citep{shen2023domain} proposes a method to propagate node label information by combining its own and neighbors’ edge structure. 
UDAGCN~\citep{wu2020unsupervised} develops a dual graph convolutional network by jointly capturing knowledge from local and global levels to adapt it by adversarial training. ASN~\citep{zhang2021adversarial} separates domain-specific and domain-invariant variables by designing a private en-coder and uses the domain-specific features in the network to extract the domain-invariant shared features across networks.  SOGA~\citep{mao2021source} first time uses discriminability by encouraging the structural consistencies between target nodes in the same class for the SFDA in the graph. GraphAE~\citep{guo2022learning} focuses on how shifts in node degree distribution affect node embeddings by minimizing the discrepancy between router embedding to eliminate structural shifts.
SpecReg~\citep{you2022graph} used the optimal transport-based GDA bound for graph data and discovered that revising the GNNs’ Lipschitz constant can be achieved by spectral smoothness and maximum frequency response.  JHGDA~\citep{shi2023improving} studies the shifts in hierarchical graph structures, which are inherent properties of graphs by aggregating domain discrepancy from all hierarchy levels to derive a comprehensive discrepancy measurement. ALEX~\citep{yuan2023alex} first creates a label shift enhanced augmented graph view using a low-rank adjacency matrix obtained through singular value decomposition by driving contrasting loss. SGDA~\citep{qiao2023semi} enhances original source graphs by integrating trainable perturbations (adaptive shift parameters) into embeddings by conducting adversarial learning to simultaneously train both the graph encoder and perturbations, to minimize marginal shifts.


\section{Theoretical Analysis}
In this subsection, we provide a discussion on the PAC-Bayesian analysis with the graph domain adaptation.

\textbf{Notations.} 
% An attributed graph can be represented as $G=\left\{\mathcal{V}, \mathcal{E}, A, X, Y\right\}$, with the set of nodes $\mathcal{V}$ and the set of edges $\mathcal{E}$.
An undirected graph $ G = \left\{\mathcal{V}, \mathcal{E}, A, X, Y\right\} $ consists of a set of nodes $ \mathcal{V} $ and edges $ \mathcal{E} $, along with an adjacency matrix $ A $, a feature matrix $ X $, and a label matrix $ Y $. The adjacency matrix $ A \in \mathbb{R}^{N \times N} $ encodes the connections between $ N $ nodes, where $ A_{ij} = 1 $ indicates an edge between nodes $ i $ and $ j $, and $ A_{ij} = 0 $ means the nodes are not connected. The feature matrix $ X \in \mathbb{R}^{N \times d} $ represents the node features, with each node described by a $ d $-dimensional feature vector. Finally, $ Y \in \mathbb{R}^{N \times C} $ contains the labels for the $ N $ nodes, where each node is classified into one of $ C $ classes.

In this work, we explore the task of node classification in a unsupervised setting, where both the node feature matrix $X$ and the graph structure $A$ are given before learning. We assume that all key aspects of our analysis are conditioned on the fixed graph structure $A$ and feature matrix $X$, while the uncertainty arises from the node labels $Y$. 
Specifically, we assume that the label $ y_i $ for each node $ i \in \mathcal{V} $ is drawn from a latent conditional distribution $ \Pr(y_i \mid Z_i) $, where $ Z = f(X, G) $, with $ f $ being an aggregation function that combines features from the local neighborhood of each node within the graph. Additionally, we assume that the labels for different nodes are independent of each other, given their respective aggregated feature representations $ Z_i $.
With a partially labeled node set $ V_0 \subseteq \mathcal{V} $, our objective in the node classification problem is to learn a model $ h : \mathbb{R}^{N \times d} \times G_N \to \mathbb{R}^{N \times C} $ from a family of classifiers $ \mathcal{H} $ that can predict the labels for the remaining unlabeled nodes. For a given classifier $ h $, the predicted label $ \hat{Y}_i $ for node $ i $ is determined by:$\:
\hat{Y}_i = \arg\max_{k \in \{1, \ldots, C\}} h_i(X, G)[k],
$
where $ h_i(X, G) $ is the output corresponding to node $ i $ and $ h_i(X, G)[k] $ represents the score for the $ k $-th class for node $ i $.



% In the context of unsupervised GDA node classification, we are given one source graph $G^S=\left\{\mathcal{V}^S, \mathcal{E}^S, A^S, X^S, Y^S\right\}$ and one unlabeled target graph $G^T=\left\{\mathcal{V}^T, \mathcal{E}^T, A^T, X^T\right\}$ during training. 

% The relevance between $G^S$ and $G^T$ is defined from two aspects: 1) $X^S$ and $X^T$ are sampled from the same feature space but have different distributions, and 2) the classification task remains unchanged across domains. $G^S$ is associated with the label matrix $Y^S$, while $G^T$ is unlabeled. 


\textbf{Margin loss on each domain.} Now we can define the empirical and expected margin loss of a classifier $h\in \gH$ on source graph $G^S=\left\{\mathcal{V}^S, \mathcal{E}^S, A^S, X^S, Y^S\right\}$ and target graph $G^T=\left\{\mathcal{V}^T, \mathcal{E}^T, A^T, X^T\right\}$. Given $Y^S$, the empirical margin loss of $h$ on $G^S$ for a margin $\gamma \ge 0$ is defined as
\begin{equation}
	\label{eq:emp-margin-loss}
	\hLg_S(h) := \frac{1}{N_S} \sum_{i\in \mathcal{V}^S}\ind{h_i(X^S, G^S)[Y_i] \le \gamma + \max_{c\neq Y_i}h_i(X^S, G^S)[c]}
\end{equation}
where $\ind{\cdot}$ is the indicator function, $c$ represents node labeling . The expected margin loss is then defined as
\begin{equation}
	\label{eq:margin-loss}
	\Lg_S(h) := \E_{Y_i\sim \Pr(Y\mid Z_i), i\in \mathcal{V}^S} \hLg_S(h)
\end{equation}

% \begin{definition} [Distance To Source graph and Target graph]
% 	For $G^S$, $G^T$, define the distance from the source node set $\mathcal{V}^T$ to the target node set $\mathcal{V}^S$ as 
% 	$\epsilon_T := \max_{j\in \mathcal{V}^T}\min_{i\in \mathcal{V}^S} \|g_i(X, G^S) - g_j(X, G^T)\|_2.$
% \end{definition}
\vspace{0.5em}
\begin{definition} [Expected Loss Discrepancy]
Given a distribution $P$ over  a function family $\gH$, for any $\lambda > 0$ and $\gamma \ge 0$, for any $G^S$ and $G^T$, define the expected loss discrepancy between $\mathcal{V}^S$ and $\mathcal{V}^T$  as
 $
D^{\gamma}_{S, T}(P;\lambda) := \ln \E_{h\sim P} e^{\lambda \sbra \Lgh_T(h) - \Lg_{S}(h)\sket},
$
where $\Lgh_T(h)$ and $\Lg_{S}(h)$ follow the definition of Eq.~(\ref{eq:margin-loss}).
\end{definition}

 Intuitively, $D^{\gamma}_{S, T}(P;\lambda)$ captures the difference of the expected loss between $\mathcal{V}^S$ and $\mathcal{V}^T$ in an average sense (over $P$).
\vspace{0.5em}
\begin{theorem}[Domain Adaptation Bound for Deterministic Classifiers] 
	\label{thm:PAC-Bayes-deterministic}
    Let $ \mathcal{H} $ be a family of classification functions. For any classifier $ h $ in $ \mathcal{H} $, and for any parameters $ \lambda > 0 $ and $ \gamma \geq 0 $, consider any prior distribution $ P $ over $ \mathcal{H} $ that is independent of the training data $ \mathcal{V}^S $. With a probability of at least $ 1 - \delta $ over the sample $ Y^S $, for any distribution $ Q $ on $ \mathcal{H} $ such that 
$\Pr_{\tilde{h} \sim Q} \left[ \max_{i \in \mathcal{V}^S \cup \mathcal{V}^T} \| h_i(X, G) - \tilde{h}_i(X, G) \|_{\infty} < \frac{\gamma}{8} \right] > \frac{1}{2}$, the following inequality holds:

\begin{equation}
	\begin{aligned}
		\label{eq:general-deterministic}
		\risk_T(\tilde{h}) \leq \hLg_S(\tilde{h}) + \frac{1}{\lambda} \left[ 2(\KL(Q \| P) + 1) + \ln \frac{1}{\delta} + \frac{\lambda^2}{4N_S} + D^{\gamma/2}_{S, T}(P;\lambda) \right].
	\end{aligned}	
\end{equation}

\end{theorem}

We follow the characterization from \citep{ma2021subgroup}.   In the generalization bound, the KL-divergence $D_{\mathrm{KL}}(Q \| P)$ is usually considered as a measurement of the model complexity.
The terms  $\ln(1/\delta)$ and $\frac{\lambda^2}{4N_S}$ are commonly seen in PAC-Bayesian analysis for IID supervised settings. The expected loss discrepancy $D^{\gamma/2}_{S, T}(P;\lambda)$ between the source nodes $\mathcal{V}^S$ and the targeted nodes $\mathcal{V}^T$ is essential to our analysis.
% \textbf{Upper-bounding $D^{\gamma}_{S, T}(P;\lambda)$.}
To derive the generalization guarantee, we need to upper-bound the expected loss discrepancy $D^{\gamma}_{S, T}(P;\lambda)$.

% \begin{proposition}[Bound for $D^{\gamma}_{S, T}(P;\lambda)$]
%      For any $\gamma \ge 0$, assume the prior $P$ on $\gH$, we have 
%     \begin{equation}
%         \begin{aligned}
%             D^{\gamma/2}_{S, T}(P;\lambda) \le &O\left(\sum_{i\in V^S}\sum_{j \in V^T} ||(A^SX)_i - (A^TX)_j||_{2}^{2} + \sum_{i\in V^S}\sum_{j \in V^T} ||X_i - X_j||_{2}^{2}\right)
%         \end{aligned}
%     \end{equation}
%     \label{equ: Proposition}
% \end{proposition}
\vspace{0.5em}
\begin{proposition}[Bound for $D^{\gamma}_{S, T}(P;\lambda)$]
    For any $\gamma \ge 0$, and under the assumption that the prior distribution $P$ over the classification function family $\mathcal{H}$ is defined, we establish a bound for the domain discrepancy measure $ D^{\gamma/2}_{S, T}(P;\lambda) $.
    Specifically, we have the following inequality:
    \begin{equation}
        \begin{aligned}
            D^{\gamma/2}_{S, T}(P;\lambda) \le & O\left( \sum_{i \in V^S} \sum_{j \in V^T} ||(A^S X^S)_i - (A^T X^T)_j||_{2}^{2} + \sum_{i \in V^S} \sum_{j \in V^T} ||X^S_i - X^T_j||_{2}^{2} \right).
        \end{aligned}
    \end{equation}
    \label{equ: Proposition}
\end{proposition}

From Proposition \ref{equ: Proposition},  the topological divergence $||(A^SX^S)_i - (A^TX^T)_j||_{2}^{2}$ and the attribute divergence $||X^S_i -X^T_j||_{2}^{2}$ constitute the upper bound of $D^{\gamma}_{S, T}(P;\lambda)$ and further bound graph domain discrepancy. It also reveals both attribute and topology divergence of intrinsic graph property influence on GDA generalization upper-bound. We introduce attribute and topology alignment loss by minimizing attribute divergence by utilizing the attribute graph and minimizing topology divergence by utilizing the original graph. Graph $G^S$ and $G^T$ with topology information $A^S$ and $A^T$ through the feature extraction module can represent $(A^S X^S)_i$ and $(A^S X^S)_j$.

\section{The proposed methodology}
In this section, we propose a novel GDA method with attribute-driven alignment (GAA), which first minimizes graph attribute divergence.
The overall framework of GAA is shown in Figure\ref{fig:overview}. The main
components of the proposed method include the specific attribute convolution module and the attribute-driven alignment module. We will detail the proposed GAA in the following subsections.

\subsection{Specific Attribute Convolution Module}
Inspired by Proposition 1, we design an attribute-driven GDA model by using topology graph and feature graph. Our model mainly contains attribute-driven alignment that directly minimize discrimination in attribute and topology between source and target graph. 

\textbf{Feature Graph}
Merely using node attribute information through $X$ is unstable ~\citep{fang2022structure,mao2023revisiting,li2024pc,xie2024provable}.
A natural idea would be to utilize graph node attribute by fully making use of the information through feature space propagation~\citep {wang2020gcn,xie2024one,kang2024cdc,li2025simplified}.
Therefore we introduce feature graph into our work.

To represent the structure of nodes in the feature space, we build a 
$k$NN graph \textbf{$\hat{G}$} based on the feature matrix $X$. To be precise, 
a node similarity matrix \textbf{$SM$} is computed using the cosine similarity formula:
\begin{equation}
    SM_{ij} = \frac{X_{i}\cdot X_{j}}{|X_{i}| \cdot |X_{j}|}
\label{equ: cos}
\end{equation}
where $SM_{ij}$ is the similarity between node feature $X_i$ and node feature $X_j$.
We derivate feature graph $\hat{G}=\left\{ \mathcal{V}, \mathcal{\hat{E}}, \hat{A}, X, Y\right\}$,
which shares the same $X$ with $G$, but has a different adjacency matrix.
Therefore, topology graph and feature graph refer to 
$G$ and $\hat{G}$ respectively.
Then for each node we choose the top k nearest neighbors and
establish edges. In this way, we construct a feature graph in attribute view for the source graph $\hat{G}^S=\left\{\mathcal{V}^S, \mathcal{\hat{E}}^S, \hat{A}^S, X^S, Y^S\right\}$ and target graph $\hat{G}^T=\left\{\mathcal{V}^T, \mathcal{\hat{E}}^T, \hat{A}^T, X^T\right\}$. 


\textbf{Feature Extraction Module}
%\subsubsection*{\textbf{Feature Extraction Module}}
%\ 
%\newline
To extract meaningful features from graphs, we adopt GCN that is 
comprised of multiple graph convolutional layers. With the input graph $G$, the
$(l+1)$-th layer's output $H^{(l+1)}$ can be represented as:
\begin{equation}
    H^{(l+1)} = ReLU(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}H^{(l)}W^{(l)})
\end{equation}
\begin{spacing}{1.1} 
where $ReLU$ is the Relu activation function ($ReLU(\cdot) = max(0, \cdot) $), 
$D$ is the degree matrix of $A$, $W^{(l)}$ is a layer-specific trainable weight matrix, 
$H^{(l)}$ is the activation matrix in the $l$-th layer and $H^{(0)} = X$.
In our study we use two GCNs to exploit the information in topology and feature space. 
For source graph, output node embedding is donated by $Z^S$ generated from $G^S$ and $Z^S_f$ generated from $\hat{G}^S$. Similarly, for the target graph, the output node embedding is donated by $Z^T$ generated from $G^T$ and $Z^T_f$ generated from $\hat{G}^T$. 
\end{spacing}
%For source graph, output is donated by $Z^S=\left\{ z^s_{1}, z^s_{2}, \cdots, z^s_{N}\right\}$ generated from $G^S$
%and $Z^S_f=\left\{ z^s_{f1}, z^s_{f2}, \cdots, z^s_{fN}\right\}$ generated from $\hat{G}^S$, where $N = \left| V^S\right|$ is the %number of nodes in $G^S$. Respectively, target graph is donated by $Z^T=\left\{ z^t_{1}, z^t_{2}, \cdots, z^t_{M}\right\}$ 
%and $Z^T_f=\left\{ z^t_{f1}, z^t_{f2}, \cdots, z^t_{fM}\right\}$, where $M = \left| V^T\right|$ is the number of nodes in $G^T$.



\subsection{Source Classifier Loss}
\ 
\newline
The source classifier loss $\mathcal{L}_S\left(f_S\left(Z^S\right), Y^S\right)$ is to minimize the cross-entropy for the labeled data node in the source domain:
\begin{equation}
\centering
\begin{aligned}
\mathcal{L}_S\left(f_S\left(Z^S\right), Y^S\right)=-\frac{1}{N_S} \sum_{i=1}^{N_S} y^S_i \log \left(\hat{y}^S_i\right)
\label{equ: cl_loss}
\end{aligned}
\end{equation} 
where $y^S_i$ denotes the label of the $i$-th node in the source domain and $\hat{y}^S_i$ are the classification prediction for the $i$-th source graph labeled node $v^S_i \in \mathcal{V}^S$.


\subsection{Attribute-Driven Alignment}
\ 
\newline
To make the attribute view fully learnable, we design the attention attribute module to dynamically utilize the important attribute. Specifically, we design learnable domain adaptive models for alignment embeddings in topology and attribute views.

\textbf{Attention-based Attribute} To guide the network to take more attention to the important node attributes and make attributes learnable, we design attention-based embedding models. Specifically, we map the node attributes into three different latent spaces. By given an example in source graph attribute embedding: $Q={W}_q {Z^S_f}^{\top}$, $K={W}_k {Z^S_f}^{\top}$, $M={W}_v {Z^S_f}^{\top}$, where ${W}_q \in \mathbb{R}^{d \times d}, {W}_k \in \mathbb{R}^{d \times d}, {W}_v \in \mathbb{R}^{d \times d}$ are the learnable parameter matrices. And ${Q} \in \mathbb{R}^{d \times N}, {K} \in \mathbb{R}^{d \times N}$ and ${M} \in \mathbb{R}^{d \times N}$ denotes the query matrix, key matrix and value matrix, respectively.

The attention-based attribute matrix ${att^S_f}$ can be calculated by:
\begin{equation}
\centering
\begin{aligned}
{att^S_f}={softmax}\left(\frac{{K}^{\top} {Q}}{\sqrt{d}}\right) {M}^{\top}
\label{equ: att}
\end{aligned}
\end{equation} 
Likewise, we can obtain a similar objective of each learnable graph embedding ${att^S}$, ${att^T_f}$ and ${att^T}$.

\textbf{Cross-view Similarity Matrix Refinement} 
\begin{spacing}{1.1} 
Subsequently, the cross-view similarity matrix $S^S$ represents the similarity between the source attribute and topology graph. $S^T$ represents the similarity between the target attribute and topology graph. $S^S$ as formulated:
\end{spacing}
\begin{equation}
\centering
\begin{aligned}
{S^S}=\frac{{Z^S_f} \cdot\left({Z^S}\right)^{\top}}{||{Z^S_f}||_2 \cdot||{Z^S}||_2}
\label{equ: S_S}
\end{aligned}
\end{equation} 
Likewise, we can obtain a similarity matrix of the target graph by:
\begin{equation}
\begin{aligned}
S^T = \frac{Z^T_f \cdot \left(Z^T\right)^{\top}}{\|Z^T_f\|_2 \cdot \|Z^T\|_2}
\end{aligned}
\label{equ:S_T}
\end{equation}

where ${S}^S$ and ${S}^T$ is the cross-view similarity matrix, and $\langle\cdot\rangle$ is the function to calculate similarity. Here, we adopt cosine similarity~\cite{qian2024upper}. The proposed similarity matrix ${S}^S$ and ${S}^T$ measures the similarity between samples by comprehensively considering attribute and structure information. The connected relationships between different nodes could be reflected by ${S}^S$ and ${S}^T$. Therefore, we utilize ${S}^T$ and ${S}^S$ to refine the structure in augmented view with Hadamard product, $att^S_f$ can be formulated as:
\begin{equation}
\centering
\begin{aligned}
{att^S} = {att^S} \odot  {S^S}
\label{equ: X}
\end{aligned}
\end{equation}
\begin{spacing}{1.1} 
Similarly we can get ${att^S_f} $ by $ {att^S_f} \odot  {S^S}$, ${att^T_f}$ by $ {att^T_f} \odot  {S^T}$, ${att^T}$ by $ {att^T} \odot  {S^T}$, which respectively represent source graph and target graph in both topology view and attribute view embedding.
\end{spacing}
\textbf{Attribute-Driven Domain Adaptive} 
\begin{spacing}{1.1} 
The proposed framework follows the transfer learning paradigm, where the model minimizes the divergence of the two views. In detail, GAA jointly optimizes two views of GDA alignment. To be specific, $\mathcal{L_A}$ is the Mean Squared Error (MSE) loss between the source graph $att^S$ and $att^S_f$ and the target graph $att^T$ and $att^T_f$, which can be formulated as:
\end{spacing}
\vspace{0.5em}
\begin{equation}
\centering
\begin{aligned}
\mathcal{L}_A=-\left(||{att^S}-{att^T}||_2^2+||{att^S_f}-{att^T_f}||_2^2\right)
\label{equ: LEMMA}
\end{aligned}
\end{equation}

\begin{spacing}{1.1} 
We adapt the domain in two views, domain classifier loss in the topology view is $||{att^S_f}-{att^T_f}||_2^2$
enforces that the attribute graph node representation after the node feature extraction and similarity matrix refinement from source and target graph $G^S_f$ and $G^T_f$. Similarly, we get $||{att^S}-{att^T}||_2^2$ from $G^S$ and $G^T$. And $||{att^S}-{att^T}||_2^2$ corresponds to the first item of Proposition \ref{equ: Proposition}, which is $ ||(A^SX^S)_i - (A^TX^T)_j||_2^2$ means minimizing structural distribution shift. In attribute view is $||{att^S_f}-{att^T_f}||_2^2$ trying to discriminate corresponds to the second term $ ||X^S_i -X^T_j||_2^2$  of Proposition \ref{equ: Proposition}, which means minimizing attribute distribution shift.
\end{spacing}


\subsection{Target Node Classification}
\ 
\newline
We use Gradient Reversal Layer (GRL) ~\citep{ganin2016domain} for adversarial training. Mathematically, we define the GRL as $Q_\lambda(x) = x$ with a reversal gradient\textbf{$\frac{\partial Q_\lambda(x)}{\partial x}=-\lambda I$}. Learning a GRL is adversarial in such a way that: on the one side, the reversal gradient enforces $f_S (Z^S)$ to be maximized; on the other side, $\theta_D$ is optimized by minimizing the cross-entropy domain classifier loss:
\begin{equation}
\centering
\begin{aligned}
\mathcal{L}_{D}=-\frac{1}{N_S+N_T} \sum_{i=1}^{N_S+N_T} m_i \log \left(\hat{m}_i\right)+\left(1-m_i\right) \log \left(1-\hat{m}_i\right)
\label{equ: DA_loss}
\end{aligned}
\end{equation} 
where $m_i \in\{0,1\}$ denotes the groundtruth, and $\hat{m}_i$ denotes the domain prediction for the $i$-th node in the source domain and target domain, respectively.
To utilize the data in the target domain, we use entropy loss for the target classifier $f_T$ :
\begin{equation}
\centering
\begin{aligned}
\mathcal{L}_T\left(f_T\left(Z^T\right)\right)=-\frac{1}{N_T} \sum_{i=1}^{N_T} \hat{y}^T_i \log \left(\hat{y}^T_i\right)
\label{equ: T_loss}
\end{aligned}
\end{equation} 
where $\hat{y}^T_i$ are the classification prediction for the $i$-th node in the target graph $v^T_i$. Finally, by combining $\mathcal{L}_{A}$, $\mathcal{L}_{S}$, $\mathcal{L}_{D}$ and $\mathcal{L}_{T}$, the overall loss function of our model can be represented as:
\begin{equation}
\centering
\begin{aligned}
    \mathcal L = \mathcal L_{A} + \alpha \mathcal {L}_{S} + \beta \mathcal L_{D} + \tau \mathcal L_{T}
\label{equ: all_loss}
\end{aligned}
\end{equation}
where $\alpha$, $\beta$ and $\tau$ are trade-off hyper-parameters. The parameters of the whole framework are updated via backpropagation. 







\section{EXPERIMENT}




\begin{wraptable}{r}{0.45\textwidth}
\centering
\small
\scalebox{0.9}{\begin{tabular}{c|c|c|c|c}
\toprule[0.8pt]
Types                      & Datasets        & \#Node  & \#Edge               & \#Label              \\ 
\midrule[0.8pt]
\multirow{3}{*}{Airport} & USA      & 1,190 & 13,599   & \multirow{3}{*}{4} \\
                          & Brazil & 131 & 1,038                        &                    \\
                          & Europe     & 399 & 5,995                          &                    \\ 
\midrule[0.8pt]
\multirow{3}{*}{Citation} & ACMv9      & 9,360 & 15,556   & \multirow{3}{*}{5} \\
                          & Citationv1 & 8,935 & 15,098                         &                    \\
                          & DBLPv7     & 5,484 & 8,117                          &                    \\ 
\midrule[0.8pt]
\multirow{2}{*}{Social}   & Blog1    & 2,300 & 33,471  & \multirow{2}{*}{6} \\
                          & Blog2   & 2,896 & 53,836                         &                    \\ 
\midrule[0.8pt]
\multirow{2}{*}{Social}   & Germany    & 9,498 & 153,138  & \multirow{2}{*}{2} \\
                          & England    & 7,126 & 35,324                         &                    \\ 

\midrule[0.8pt]

{\multirow{6}{*}{MAG}}      & 
{US}      &
{ 132,558} & 
{697,450}   & 
{\multirow{6}{*}{20}} \\
                          & 
{CN}      &  {101,952} &  {285,561}                       &                    \\
                          & 
{DE}      & {43,032} & {126,683}                         &                    \\ 
                          & {JP}      & {37,498} & {90,944}                         &                    \\
                          & 
{RU}      & {32,833} & { 67,994  }                        &                    \\ 
                          &
{ FR}      & {29,262} & {78,222   }                      &                    \\

\midrule[0.8pt]


\end{tabular}}
\caption{Dataset Statistics.}
\label{tab:datasets}
\end{wraptable}




\subsection{Datasets}
To prove the superiority of our work on domain adaptation node classification tasks, we evaluate it on four types of datasets, including Airport dataset~\citep{ribeiro2017struc2vec}, Citation dataset~\citep{wu2020unsupervised}, Social dataset~\citep{liu2024rethinking} and Blog dataset~\citep{li2015unsupervised}.
The airport dataset involves three countries' airport traffic networks: USA (U), Brazil (B), and Europe (E), in which the node indicates the airport and the edge indicates the routes between two airports. The citation dataset includes three different citation networks: DBLPv8 (D) , ACMv9 (A), and Citationv2 (C), in which the node indicates the article and the edge indicates the citation relation between two articles. As for social networks, we choose Twitch gamer networks and Blog Network, which are collected from Germany(DE) and England(EN). Two disjoint Blog social networks, Blog1 (B1) and Blog2 (B2), which are extracted from the BlogCatalog dataset.
extracted from the BlogCatalog dataset. Because these four groups of dataset ingredients are generated from different data sources, their distributions are naturally diverse. 
For a comprehensive overview of these datasets, please refer to Tab \ref{tab:datasets}.
%Thus, we give four groups of datasets: for  Citation dataset(source$\rightarrow$target): A$\rightarrow$D, D$\rightarrow$A, A$\rightarrow$C, C$\rightarrow$A, D$\rightarrow$C, C$\rightarrow$D, for Airport dataset: USA$\rightarrow$Brazil, Brazil$\rightarrow$USA, USA$\rightarrow$ Europe, Europe$\rightarrow$USA, Brazil$\rightarrow$Europe, Europe$\rightarrow$Brazil, for Social dataset: DE$\rightarrow$ EN, EN$\rightarrow$DE and for Blog dataset:  B1$\rightarrow$B2, B2$\rightarrow$B1.


\begin{table*}[!t]
\centering
\small
% \setlength\tabcolsep{3pt}
\scalebox{0.9}{\begin{tabular}{|l|cccccc|cc|}
\toprule
Methods & U $\rightarrow$ B & U $\rightarrow$ E & B $\rightarrow$ U & B $\rightarrow$ E & E $\rightarrow$ U & E $\rightarrow$ B & DE $\rightarrow$ EN & EN $\rightarrow$ DE \\\midrule
GCN & 0.366 &	0.371 &	0.491 &	0.452 &	0.439 &	0.298 &	0.673 &	0.634  \\
$k$NN-GCN & 0.436 &	0.437 &	0.461 &	0.478 &	0.459 &	0.464  &	0.661 &	0.623 \\ \midrule
DANN & 0.501 &	0.386 &	0.402 &	0.350 &	0.436 &	0.538  &0.512 &	0.528\\\midrule
DANE& 0.531 &	0.472 &	0.491 &	0.489 &	0.461 &	0.520 &	0.642 &	0.644 \\ 
UDAGCN & 0.607 &	0.488 &	0.497 &	0.510 &	0.434 &	0.477 &	0.724 &	0.660  \\
ASN & 0.519 &	0.469 &	0.498 &	0.494 &	0.466 &	0.595 &0.550 &	0.679\\
EGI & 0.523 &0.451 &	0.417 &	0.454 &	0.452 &	0.588 &0.681 &	0.589 \\ 
GRADE-N & 0.550 &	0.457 &	0.497 &	0.506 &	0.463 &	0.588 &	0.749 &	0.661 \\
JHGDA  & \underline{0.695} &0.519 &	0.511 &	\underline{0.569} &	0.522 &	\bf0.740 &	\underline{0.766} &	\underline{0.737}\\
SpecReg & 0.481 &0.487 &0.513 &	0.546 &	0.436 &	0.527 &0.756 &	0.678  \\
{GIFI}  & {0.636}    &	{0.521}    &	{0.493}    &	{0.535}    &	{0.501}    &	{0.623}    &	{0.719} &	{0.705} \\
{PA}  & {0.679} &	{\underline{0.557}} &	{\underline{0.528}} &	{0.562}    &	{\underline{0.547}}    &	{0.529 }   & {0.677}     &	{0.760} \\ \midrule
{\bf GAA}  & \bf0.704 &	\bf0.563 &	\bf0.542 &	\bf0.573 &	\bf0.546 &\underline{0.691} &\bf0.779 &\bf0.751\\
\bottomrule
\end{tabular}}
\caption{Cross-network node classification on the Airport network.}
\label{tab:airport_classification}
\end{table*}

\begin{table*}[!t]
\centering
\small
% \setlength\tabcolsep{3pt}
\scalebox{0.9}{\begin{tabular}{|l|cccccc|cc|}
\toprule
Methods & A $\rightarrow$ D & D $\rightarrow$ A & A $\rightarrow$ C & C $\rightarrow$ A & C $\rightarrow$ D & D $\rightarrow$ C &  B1 $\rightarrow$ B2 & B2 $\rightarrow$ B1  \\\midrule
GCN & 0.632 &	0.578 &	0.675 &	0.635 &	0.666 &	0.654 & 0.408 &	0.451   \\
$k$NN-GCN  & 0.636 &	0.587 &	0.672 &	0.648 &	0.668 &	0.426 & 0.531 &	0.579 \\ \midrule
DANN & 0.488 &0.436 &0.520 &	0.518 &	0.518 &	0.465 & 0.409 &0.419   \\\midrule
DANE& 0.664 &	0.619 &	0.642 &	0.653 &	0.661 &	0.709 & 0.464 &	0.423 4  \\ 
UDAGCN  & 0.684 &	0.623 &	0.728 &	0.663 &	0.712 &	0.645 & 0.471 &	0.468   \\
ASN & 0.729 &0.723 &0.752 &	0.678 &	0.752 &	0.754 & 0.732 &0.524   \\
EGI & 0.647 & 0.557 &0.676 &	0.598 &	0.662 &	0.652 & 0.494 & 0.516    \\ 
GRADE-N & 0.701 &	0.660 &	0.736 &	0.687 &	0.722 &	0.687 & 0.567 &	0.541  \\
JHGDA & 0.755 &0.737 &	\underline{0.814} &	\underline{0.756} &	0.762 &	\underline{0.794} & 0.619 &0.643 \\
SpecReg & \underline{0.762} &0.654 &0.753 &	0.680 &	\underline{0.768} &	0.727  &0.661 &0.631 \\
{GIFI}  & {0.751} &	{0.737} &	{0.793} &	{0.755} &	{0.739} &	{0.751} & {0.653} &	{0.642} \\ 
{PA} & {0.752} &	{\underline{0.751}} &	{0.804} &	{0.768} &	{0.755} &	{0.780} & {\underline{0.662}} &	{\underline{0.654}} \\ \midrule
{\bf GAA} & \bf0.789 &	\bf0.754 &\bf0.824 &\bf0.782 &	\bf0.771 &	\bf0.798 & \bf0.681 &	\bf0.679   \\
\bottomrule
\end{tabular}}
\caption{Cross-network node classification on the Citation, Blog and Social network.}
\label{tab:citation_classification}
\end{table*}

\begin{table*}[!t]
\centering
\small
% \setlength\tabcolsep{3pt}
\scalebox{0.73}{\begin{tabular}{|l|cccccccccc|}
\toprule
Methods & US $\rightarrow$ CN & US $\rightarrow$ DE & US $\rightarrow$ JP & US $\rightarrow$ RU & US $\rightarrow$ FR & CN $\rightarrow$ US & CN $\rightarrow$ DE &  CN $\rightarrow$ JP & CN $\rightarrow$ RU & CN $\rightarrow$ FR \\\midrule
GCN &0.042 &0.168 &0.219 &	0.147 &	0.182 &	0.193 & 0.064  &0.160 &0.069 &0.067   \\
$k$NN-GCN  &0.092 &0.189 &0.269 &	0.186 &	0.213 &	0.210 & 0.133  &0.201 &0.105 &0.102 \\ \midrule
DANN &0.242 &0.263 &0.379 &	0.218 &	0.207 &	0.302 & 0.134 &0.214 &0.119 &0.107 \\\midrule
DANE &0.272 &0.250 &0.280 &	0.210 &	0.186 &	0.279 & 0.108 &0.228 &0.170 &0.184  \\ 
UDAGCN  & OOM &	OOM  &	OOM  &	OOM  &	OOM  &	OOM  & OOM  &	OOM &	OOM &	OOM  \\
ASN &0.290 &0.272 &0.291 &	0.222 &	0.199 &	0.268 & 0.121 &0.207 &0.189 &0.190   \\
EGI & OOM &	OOM  &	OOM  &	OOM  &	OOM  &	OOM  & OOM  &	OOM &	OOM  &	OOM   \\ 
GRADE-N &0.304 &0.299 &0.306 &	0.240 &	0.217 &	0.258 & 0.137 &0.210 &0.178 &0.199  \\
JHGDA & OOM &	OOM  &	OOM  &	OOM  &	OOM  &	OOM  & OOM  &	OOM &	OOM &	OOM\\
SpecReg &0.237 &0.267 &0.377 &	0.228 &	0.218 &	0.317 & 0.134 &0.199 &0.109 &116 \\
PA  & \underline{0.400} &	\underline{0.389} &	\underline{0.474} &	\underline{0.371} &	\underline{0.252} &	\underline{0.452} & \underline{0.262} &	\underline{0.383}  &	\underline{0.333} &	\underline{0.242}\\ \midrule
{\bf GAA} & \bf0.410 &	\bf0.401 &\bf0.492 &\bf0.372 &	\bf0.2881 &	\bf0.453 & \bf0.302 &	\bf0.400   &	\bf0.351 &	\bf0.293   \\
\bottomrule
\end{tabular}}
\caption{Cross-network node classification on MAG datasets.}
\label{tab:citation_classification}
\end{table*}




\subsection{Baselines}
We choose some representative methods to compare.
\textbf{GCN}~ ~\citep{kipf2016semi} further solves the efficiency problem by introducing first-order approximation of ChebNet. 
\textbf{$k$NN-GCN}~ ~\citep{wang2020gcn} use the sparse $k$-nearest neighbor graph 
calculated from feature matrix as the input graph of GCN and name it $k$NN-GCN.
\textbf{DANN}~ ~\citep{ganin2016domain} use a 2-layer perceptron to provide features and a gradient reverse layer (GRL) to learn node embeddings for domain classification
\textbf{DANE}~ ~\citep{zhang2019dane} shared distributions embedded space on different networks and further aligned them through adversarial learning regularization.
\textbf{UDAGCN}~ ~\citep{wu2020unsupervised} is a dual graph
convolutional network component learning framework for unsupervised GDA, which captures knowledge from local and global levels to adapt it by adversarial training.
\textbf{ASN}~ ~\citep{zhang2021adversarial} use the domain-specific features in the network to extract the domain-invariant shared features across networks.
\textbf{EGI}~ ~\citep{zhu2021transfer} through Ego-Graph Information maximization to analyze structure-relevant transferability regarding the difference between source-target graph.
\textbf{GRADE-N}~ ~\citep{wu2023non} propose a graph subtree discrepancy to measure the graph distribution shift between source and target graphs.
\textbf{JHGDA}~ ~\citep{shi2023improving} explore information from different levels of network hierarchy by hierarchical pooling model.
\textbf{SpecReg}~ ~\citep{you2022graph} achieve improving performance regularization inspired by cross-pollinating between the optimal transport DA and graph filter theories .
{\textbf{GIFI}~ ~\citep{qiao2024information} uses a parameterized graph reduction module and variational information bottleneck to filter out irrelevant information.}
{\textbf{PA}~ ~\citep{liu2024pairwise} mitigates distribution shifts in graph data by recalibrating edge influences to handle structure shifts and adjusting classification losses to tackle label shifts.}


\subsection{Experimental Setup}
The experiments are implemented in the PyTorch platform using an Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz, and GeForce RTX A5000 24G GPU.
Technically, two layers GCN is built and we train our model by utilizing the Adam~\citep{adam} optimizer with learning rate ranging from 0.0001 to 0.0005. In order to prevent over-fitting, we set the dropout rate to 0.5.
In addition, we set weight decay $\in \left\{1e-4, \cdots, 5e-3 \right\}$ and $k$ $\in \left\{1, \cdots, 10 \right\}$ for $k$NN graph. For fairness, we use the same parameter settings for all the cross-domain node classification methods in our experiment, except for some special cases. For GCN, UDA-GCN, and JHGDA the GCNs of both the source and target networks contain two hidden layers $(L = 2)$ with structure as $128-16$. The dropout rate for each GCN layer is set to $0.3$.
We repeatedly train and test our model for five times with the same partition of dataset and then report the average of ACC.

 


\subsection{Cross-network Node Classification Results}
The results of experiments are summarized in Table \ref{tab:airport_classification} and \ref{tab:citation_classification}, where the best performance is highlighted in boldface.
Some results are directly taken from~\citep{shi2023improving,pang2023sa}. We have the  following findings:
It can be seen that our proposed method boosts the performance of SOTA methods across most evaluation metrics on four group datasets with 16 tasks, which proves its effectiveness. Particularly, compared with other optimal performances in all datasets, GAA achieves a maximum average improvement of $1.80\%$ for ACC. This illustrates that our proposed model can effectively utilize node attribute information. Our GAA achieves much better performances than SpecReg and JHGDA on all of the metrics in a dataset of Airport and most of the metrics in a dataset of Citation. This can be explained by our method's use of attribute and topology structure.
In most cases, GAA produces better performance than GRADE-
N~\citep{wu2023non} and JHGDA~\citep{shi2023improving}, which were published in 2023. This verifies the advantage of our approach.
On most occasions, the feature graph produces a better result than the original graph. For example, in airport data, $ k$NN-GCN performance averages better than $5.30\%$ to GCN, and in citation datasets, performance averages better than $0.60\%$ to GCN. Our findings affirm that the observed discrepancy in node attributes surpasses that of the topological misalignment, thus suggesting that the alignment of node attributes holds potential for yielding more substantial enhancements.



\subsection{Ablation Study}
To validate the effectiveness of different components in our model, we compare GAA with its three variants on  Citation and Airport datasets.
\begin{itemize}
\item \textbf{GAA$_1$}: GAA without cross-view similarity matrix Refinement to show the importance of comprehensive attribute and structure information.
\item \textbf{GAA$_2$}: GAA without $\mathcal{L}_{A}$ to show the impact of attribute benefit alignment.
\item \textbf{GAA$_3$}: GAA without $\mathcal{L}_{A}$ and remove channel feature graph and only utilize $\mathcal{L}_{D}$ to show the effect of attribute(feature) graph impact.
\end{itemize}


\begin{wrapfigure}{R}{0.50\textwidth}
    \vspace{-2em}
    \subfigure[Citation]{
    \centering
    \begin{minipage}[b]{0.24\textwidth}
    \includegraphics[width=1.0\textwidth]{c_ablation.pdf}
    \end{minipage}
    }\hspace{-1em}
    \subfigure[Airport]{
    \centering
    \begin{minipage}[b]{0.24\textwidth}
    \includegraphics[width=1.0\textwidth]{a_ablation.pdf}
    \end{minipage}
    }
    \vskip -1.3em
    \caption{\label{fig:ablation} The classification accuracy of GAA and its variants on citation datasets and airport dataset.}
    \vskip -1.5em
\end{wrapfigure}

%\begin{figure}[!htbp]
%\centering
%  \includegraphics[width=0.9\linewidth]{k-parament.png}
%  \caption{Accuracy of parameter $k$ influence.}
  %\Description{Time per epoch}
%  \label{fig: k_}
%\end{figure}


According to Figure\ref{fig:ablation}, we can draw the following conclusions: (1) The results of GAA are consistently better than all variants, indicating the rationality of our model. (2) Both topology and feature information are crucial to domain adaptation. (3) The cross-view similarity matrix can improve performance by enhancing and integrating feature information, benefiting the synergistic refinement of both attribute and topology. %Other dataset experimental evidence can be found in Fig 1 in Appendix.


\subsection{Parameter Analysis}
\begin{figure*}[!t]
\centering
\includegraphics[width=1.0\linewidth]{hyper_p.pdf}
\caption{The influence of parameters $\alpha$, $\beta$, $\tau$ and $k$ on Citation and Airport dataset.}
\label{fig:alpha_beta_gamma}
\end{figure*}

In this section, we analyze the sensitivity of the parameters of our method on the Airport dataset and  Citation dataset. As shown in Figure.\ref{fig:alpha_beta_gamma} in Subfigure (4), the accuracy usually peaks at $2-3$ with $k$. This is reasonable since increasing $k$ means more high-order proximity information is incorporated. On the other hand, extremely large $k$ could also introduce noise that will deteriorate the performance. 
From Figure.\ref{fig:alpha_beta_gamma} Subfigure (1) (2) (3), we can see GAA has competitive performance on a large range of values, which suggests the stability of our method. %Other dataset experimental evidence can be found in Fig 2 in Appendix.









\section{Conclusion}
In this paper, we propose GAA framework to solve the GDA problem in cross-network node classification tasks. {We reveal the importance of both attributes and topology in GDA through both empirical and theoretical analysis, which minimizes attribute and topology distribution shifts based on intrinsic graph properties.} Comprehensive experiments verify the superiority of our approach. In the future, we may strive to design new frameworks for other cross-network learning tasks, including link-level and graph-level. We will also deep into graph domain adaptation theory for developing more powerful models.



\section{ACKNOWLEDGMENTS}
This work is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grants program. This work is also supported by the Vector Scholarship in Artificial Intelligence, provided through the Vector Institute.



\normalem
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
% \section{Appendix}

\section{Proof of Proposition \ref{equ: Proposition}}

To facilitate the analysis, we adopt the following data assumption:

\setcounter{definition}{0}
\begin{definition}\label{def:1}
    The generated nodes consist of two disjoint sets, denoted as $c_0$ and $c_1$. Each node feature $x$ is sampled from $N(\mu_i, \sigma_i)$ for $i \in \{0, 1\}$.

    Each set $c_i$ corresponds to the source graph and target graph compositions, respectively: $c_i^{(S)}$ and $c_i^{(T)}$. The class distribution is balanced, such that $\mathbb{P}(\mathbb{Y} = c_0) = \mathbb{P}(\mathbb{Y} = c_1)$.
\end{definition}

\begin{theorem} \label{lemma:csbm_diff_detail} 
    For nodes $s \in V_S$ and $t \in V_T$ with aggregated features $\rvf = \text{GNN}(x)$, the following inequality holds:
\begin{equation}
    ||\mathbb{P}(y_u = c_0 | \rvf_u) - \mathbb{P}(y_v = c_0 | \rvf_v)|| \le O(||\rvf_u - \rvf_v|| + ||\rvf_v - \vmu^{(S)}_{1}|| + ||\rvf_v - \vmu^{(T)}_{1}||).
\end{equation}
\end{theorem}
\begin{proof}
The conditional probability of class $c_0$ given the aggregated feature $f$ can be expressed using Bayes' theorem:

\begin{equation}     
    \mathbb{P}(\rvy_s = c_0 | \rvf_s) = \frac{\mathbb{P}(\rvf_s | \rvy_s = c_0) \mathbb{P}(\rvy_s = c_0)}{\mathbb{P}(\rvf_s | \rvy_s = c_0) \mathbb{P}(\rvy_s = c_0) + \mathbb{P}(\rvf_s | \rvy_s = c_1) \mathbb{P}(\rvy_s = c_1)}.
\end{equation}

Under the assumption $\mathbb{P}(\rvy = c_0) = \mathbb{P}(\rvy = c_1)$, we simplify this to:

\begin{equation}     
    \mathbb{P}(\rvy_s = c_0 | \rvf_s) = \frac{\mathbb{P}(\rvf_s | \rvy_s = c_0)}{\mathbb{P}(\rvf_s | \rvy_s = c_0) + \mathbb{P}(\rvf_s | \rvy_s = c_1)}.
\end{equation}


Substituting in the expressions for the Gaussian distributions:

\begin{equation}
    \mathbb{P}(\rvy_s = c_0 | \rvf_s) = \frac{\operatorname{exp}\left(-\frac{(\rvf_u - \vmu_{0}^{(S)})^2}{\sigma^2}\right)}{\operatorname{exp}\left(-\frac{(\rvf_u - \vmu_{0}^{(S)})^2}{\sigma^2}\right) + \operatorname{exp}\left(-\frac{(\rvf_u - \vmu_{1}^{(S)})^2}{\sigma^2}\right)}.
\end{equation}


Thus, we have:

\begin{equation}
    \begin{aligned}
        & ||\mathbb{P}(\rvy_u = c_0 | \rvf_u) - \mathbb{P}(\rvy_v = c_0 | \rvf_v)|| \\
        &= ||\frac{\mathbb{P}(\rvf_s | \rvy_s = c_0)}{\mathbb{P}(\rvf_s | \rvy_s = c_0) + \mathbb{P}(\rvf_s | \rvy_s = c_1)} - \frac{\mathbb{P}(\rvf_v | \rvy_v = c_0)}{\mathbb{P}(\rvf_v | \rvy_v = c_0) + \mathbb{P}(\rvf_v | \rvy_v = c_1)}|| \\
        &= \frac{||\mathbb{P}(\rvf_s | \rvy_s = c_0) \mathbb{P}(\rvf_v | \rvy_v = c_1) - \mathbb{P}(\rvf_v | \rvy_v = c_0) \mathbb{P}(\rvf_s | \rvy_s = c_1)||}{\left[\mathbb{P}(\rvf_s | \rvy_s = c_0) + \mathbb{P}(\rvf_s | \rvy_s = c_1)\right] \left[\mathbb{P}(\rvf_v | \rvy_v = c_0) + \mathbb{P}(\rvf_v | \rvy_v = c_1)\right]}.
    \end{aligned}
\end{equation}


Noting that the denominator is bounded, we substitute the probabilities of the Gaussian distributions into the expression:

\begin{equation}
    \begin{aligned}
        & ||\mathbb{P}(\rvy_u = c_0 | \rvf_u) - \mathbb{P}(\rvy_v = c_0 | \rvf_v)|| \\
        &= \frac{||\operatorname{exp}\left(-\frac{(\rvf_u - \vmu^{(S)}_{0})^2}{\sigma^2}\right)\operatorname{exp}\left(-\frac{(\rvf_v - \vmu^{(T)}_{1})^2}{\sigma^2}\right) - \operatorname{exp}\left(-\frac{(\rvf_u - \vmu^{(S)}_{1})^2}{\sigma^2}\right)\operatorname{exp}\left(-\frac{(\rvf_v - \vmu^{(T)}_{0})^2}{\sigma^2}\right)||}{\operatorname{exp}(-A)}.
    \end{aligned}
\end{equation}


This leads us to:

\begin{equation}
    ||\mathbb{P}(\rvy_u = c_0 | \rvf_u) - \mathbb{P}(\rvy_v = c_0 | \rvf_v)|| \le \frac{1}{\sigma^2} ||(\vmu^{(T)}_{0} - \vmu^{(S)}_{0})(2\rvf_u - \vmu^{(S)}_{0} - \vmu^{(T)}_{0}) - (\vmu^{(T)}_{1} - \vmu^{(S)}_{1})(2\rvf_v - \vmu^{(S)}_{1} - \vmu^{(T)}_{1})||.
\end{equation}


This simplifies to:

\begin{equation}
    ||\mathbb{P}(\rvy_u = c_0 | \rvf_u) - \mathbb{P}(\rvy_v = c_0 | \rvf_v)|| \le O(||\rvf_u - \rvf_v|| + ||2\rvf_v - \vmu^{(S)}_{1} - \vmu^{(T)}_{1}||).
\end{equation}
\end{proof}

\text{(a)} We note that $\delta^{\vmu}_{(\cdot)} = ||\vmu^{(\cdot)}_{1} - \vmu^{(\cdot)}_{0}||$ and $\Delta^{\vmu}_{i} = ||\vmu^{(T)}_{i} - \vmu^{(S)}_{i}||$.



\begin{proposition}[Bound for $D^{\gamma}_{S, T}(P;\lambda)$]
    For any $\gamma \ge 0$, and under the assumption that the prior distribution $P$ over the classification function family $\mathcal{H}$ is defined, we establish a bound for the domain discrepancy measure $ D^{\gamma/2}_{S, T}(P;\lambda) $.
    Specifically, we have the following inequality:
    \begin{equation}
        \begin{aligned}
            D^{\gamma/2}_{S, T}(P;\lambda) \le & O\left( \sum_{i \in V^S} \sum_{j \in V^T} ||(A^S X^S)_i - (A^T X^T)_j||_{2}^{2} + \sum_{i \in V^S} \sum_{j \in V^T} ||X^S_i - X^T_j||_{2}^{2} \right).
        \end{aligned}
    \end{equation}
    % \label{equ: Proposition1}
\end{proposition}

\begin{proof}
    For notational simplicity, let $ h_i \equiv h_i(X, G) $for any $ i \in V_S \cup V_T $. Define $ \eta_k(i) = \Pr(y_i = k \mid g_i(X, G))$ \text{ for } $k \in \{0, 1\}$, \text{ and let }
$
    \gL^{\gamma}(h_i, y_i) = \ind{h_i[y_i] \leq \gamma + \max_{k \neq y_i} h_i[k]}.
$

We can express the difference in the loss functions as follows:

\begin{align}
    \Lgh_T(h) - \Lg_S(h) &= \E_{y^T}\left[\frac{1}{N_T} \sum_{j \in V_T} \gL^{\gamma/2}(h_j, y_j)\right] - \E_{y^S}\left[\frac{1}{N_S} \sum_{i \in V_S} \gL^{\gamma}(h_i, y_i)\right] \\
    &\leq \frac{1}{\max(N_S, N_T)} \E_{y^S, y^T} \sum_{i \in V_S} \left( \frac{1}{N_T} \sum_{j \in V_T} \gL^{\gamma/2}(h_j, y_j) - \gL^{\gamma}(h_i, y_i) \right).
\end{align}

Using Definition \ref{def:1}, we derive:


    \begin{align}
        \Lgh_T(h) - \Lg_S(h) &= \frac{1}{\max(N_S, N_T)} \sum_{i \in V_S} \frac{1}{N_T} \left( \sum_{j \in V_T} \E_{y_j} \gL^{\gamma/2}(h_j, y_j) - \E_{y_i} \gL^{\gamma}(h_i, y_i) \right) \nonumber \\
        &= \frac{1}{\max(N_S, N_T)} \sum_{i \in V_S} \frac{1}{N_T} \sum_{j \in V_T} \sum_{k} \left( \eta_k(j) \gL^{\gamma/2}(h_j, k) - \Pr(y_i = k) \gL^{\gamma}(h_i, k) \right) \nonumber \\
        &= \frac{1}{\max(N_S, N_T)} \sum_{i \in V_S} \frac{1}{N_T} \sum_{j \in V_T} \sum_{k} \left( \eta_k(j) \gL^{\gamma/2}(h_j, k) - \eta_k(i) \gL^{\gamma}(h_i, k) \right) \nonumber \\
        &= \frac{1}{\max(N_S, N_T)} \sum_{i \in V_S} \frac{1}{N_T} \sum_{j \in V_T} \sum_{k} \left( \eta_k(j) \left( \gL^{\gamma/2}(h_j, k) - \gL^{\gamma}(h_i, k) \right) + \left( \eta_k(j) - \eta_k(i) \right) \gL^{\gamma}(h_i, k) \right) \label{eq:same-eta} \\
        &\leq \frac{1}{\max(N_S, N_T)} \sum_{i \in V_S} \frac{1}{N_T} \sum_{j \in V_T} \sum_{k} \left( \gL^{\gamma/2}(h_j, k) - \gL^{\gamma}(h_i, k) + \|\eta_k(j) - \eta_k(i)\|_2^2 \right). \label{eq:sample-wise-loss-diff}
    \end{align}

    The last inequality holds since both $\eta_k(j)$ and $\gL^{\gamma}(h_i, k)$ are upper-bounded by $1$, and we assume $\gL^{\gamma/2}(h_j, k) \leq \gL^{\gamma}(h_i, k)$.

    By applying Theorem \ref{lemma:csbm_diff_detail}, we obtain:
    \begin{align*}
        \sum_{k} \|\eta_k(j) - \eta_k(i)\|_2^2 &\leq O\left(\|\rvf_u - \rvf_v\|_2^2 + \|\rvf_v - \vmu^{(S)}_{1}\|_2^2 + \|\rvf_v - \vmu^{(T)}_{1}\|_2^2 + \|\rvf_v - \vmu^{(S)}_{0}\|_2^2 + \|\rvf_v - \vmu^{(T)}_{0}\|_2^2\right).
    \end{align*}

    Thus, we have:
    \begin{align*}
        \Lgh_T(h) - \Lg_S(h) &\leq \frac{1}{\max(N_S, N_T)} \sum_{i \in V_S} \frac{1}{N_T} \sum_{j \in V_T} \sum_{k} \|\eta_k(j) - \eta_k(i)\|_2^2 \\
        &\leq O\left(\sum_{i \in V_S} \sum_{j \in V_T} \|\rvf_u - \rvf_v\|_2^2 + \|\rvf_v - \vmu^{(S)}_{1}\|_2^2 + \|\rvf_v - \vmu^{(T)}_{1}\|_2^2 + \|\rvf_v - \vmu^{(S)}_{0}\|_2^2 + \|\rvf_v - \vmu^{(T)}_{0}\|_2^2\right) \\
        &\leq O\left(\sum_{i \in V_S} \sum_{j \in V_T} \|(A^S X^S)_i - (A^T X^T)_j\|_2^2 + \sum_{i \in V_S} \sum_{j \in V_T} \|X^S i - X^T_j\|_2^2\right).
    \end{align*}
\end{proof}



\section{Definition of average feature value}
%\subsubsection*{\textbf{Definition of average feature value}}
\ 
\newline
We hope to quantitatively compare the differences in feature values between topology view and attribute view. Similarly, for the purpose of convenient comparison, we decided to calculate the average of their feature values. Specifically, we first obtain a topology view matrix through topology filtering, multiplying $A$ and $X$ to $\mathcal{F}$. Similarly, we perform attribute filtering by multiplying $\hat{A}$ and $X$ to $\mathcal{F}_f$ to obtain a matrix of attribute view. So our topology average value is ${Fetuare_t}=\sum_{j=1}^{d}\sum_{i=1}^{N}\left| \mathcal{F} \right|/(d*N)$ and attribute feature value is ${Feature_f}=\sum_{j=1}^{d}\sum_{i=1}^{N}\left| \mathcal{F}_f \right|/(d*N)$.
\label{Definition of average feature value}

\section{Description of Algorithm GAA}
\begin{algorithm}
    \caption{The proposed algorithm GAA}
    \label{algo: algo}
  %  \small
    \SetKwFunction{isOddNumber}{isOddNumber}
    \SetKwInput{Input}{Input}
    \SetKwInput{Output}{Output}
    % \SetKwInOut{KwIn}{Input}
    % \SetKwInOut{KwOut}{Output}

    \KwIn{Source node feature matrix $X^S$; source original graph adjacency matrix $A^S$; Target node feature matrix $X^T$; Target original graph adjacency matrix $A^T$
    source node label matrix $Y^S$; maximum number of iterations $\eta$}
    
    % \tcc{Construct the topology graph $G$ and the feature graph $\hat{G}$ }
    Compute the feature graph topological structure $\hat{A^S}$ and $\hat{A^T}$ according to $X^S$ and $X^T$ by running $k$NN algorithm.
    
    \For{$it=1$ \KwTo $\eta$}{
    %\tcc{Consensus representation learning}
    
    ${Z^S}$ = $GCN$($A^S,X^S$) 
    
    ${Z^S_f}$ = $GCN$($\hat{A}^S,X^S$)\tcp{embedding of source graph}

    ${Z^T}$ = $GCN$($A^T,X^T$) 
    
    ${Z^T_f}$ = $GCN$($\hat{A}^T,X^T$)\tcp{embedding of target graph}

    $Z^S$ and $Z^S_f$ through cross-view similarity matrix refinement to get $S^S$.
    
    $Z^T$ and $Z^T_f$ through cross-view similarity matrix refinementto get $S^T$.  
    
    Attribute-Driven domain adaptive between $S^S$ and $S^T$\tcp{adaptive in two views}
    
    Domain Adaptive Learning between $Z^S$ and $Z^T$
    
   
    $\hat{y}^S_i$constrained by$y^S_i$ and $\hat{y}^T_i$constrained by$\hat{y}^T_i$

    
    Calculate the overall loss with Eq.(\ref{equ: all_loss})
    
    Update all parameters of the framework according to
the overall loss
    }
    
    Predict the labels of target graph nodes based on the trained framework.
    
    \KwOut{Classification result $\hat{Y}^T$}
\end{algorithm}

\section{Hyperparameter tuning detial}

\subsection{Parameter Analysis}

{ $\alpha$, $\beta$, and $\tau$ are chosen from the set $\{0.005, 0.01, 0.1, 0.5, 1, 5\}$. These values provide flexibility for adjusting the relative importance of different loss terms. $k$ (the number of neighbors for $k$-NN graph construction) is typically $k$ $\in \left\{1, \cdots, 10 \right\}$ . The optimal value for $k$ depends on the density and connectivity of the graph. Due to extremely large $k$ could also introduce noisy that will deteriorate the performance. Usually our largest $k$ will be $5$.}

{Airport Dataset: Often contains transportation networks with fewer nodes but complex edge relationships.
Given the sparsity of this dataset, $\alpha$, $\beta$ and $\tau$ should be set relatively higher to emphasize topology alignment and capture key structural relationships. $\alpha$, $\beta$ and $\tau$ is selected from $\{ 0.1, 0.5 \}$. A smaller $k$ could be more effective due to the sparser nature of these networks. We select $k$ from $\{ 3, 4 \}$.}

{Citation Dataset: This dataset often has a higher node count and diverse structural characteristics.
In such datasets, balance the impact of node attributes and topology. $\alpha$, $\beta$ and $\tau$ is selected from $\{ 0.1, 0.5 \}$. A moderate value of $k$ to capture relevant local structures could work well for this dataset. We select $k$ from $\{ 4, 5 \}$.}

{Social Network Dataset (Blog and Twitch): Social networks often contain a large number of nodes with rich attribute information but high variance in structural patterns.
Emphasize attribute alignment since social networks tend to have highly distinctive attributes. Thus, attribute shifts are more sensitive to the values of $\alpha$, $\beta$ and $\tau$, which are selected from the set $\{0.01, 0.1, 0.5\}$. Small $k$ is recommended due to the dense connections in social networks. We select $k$ from $\{ 3, 4 \}$.}

{MAG Dataset: The MAG dataset is large and diverse, containing losts of classes with various relationships and rich metadata. Structural and attribute alignment are key factors. In this context, attribute shifts are both important to the values of $\alpha$,$\beta$ and $\tau$ which are selected from the set $\{ 0.1, 0.5\}$.
The parameter $k$ works well in this context, enabling the model to capture high-level local and global structural information within the graph. We select $k$ from $\{ 4, 5 \}$.}
\begin{figure*}[!t]
\centering
\includegraphics[width=1.0\linewidth]{hyper_social.pdf}
\caption{The influence of parameters $\alpha$, $\beta$, $\tau$ and $k$ on two social datasets.}
\label{fig:alpha_beta_gamma}
\end{figure*}

\begin{table*}[!t]
\centering
\small
\scalebox{0.9}{\begin{tabular}{|c|c|c|c|c|c|}
\toprule[0.8pt]
Types                      & Datasets        &$\alpha$  & $\beta$               & $\tau$    & $k$          \\ 
\midrule[0.8pt]
\multirow{6}{*}{Airport}  & U$\rightarrow$B      & 0.5 & 0.5   & 0.01 & 4 \\
                          & U$\rightarrow$E     & 0.1 & 0.1   & 0.01 & 2 \\
                          & B$\rightarrow$U      & 0.1 & 0.1   & 0.01 & 4 \\
                          & B$\rightarrow$E      & 0.5 & 0.1   & 0.1 & 3 \\
                          & E$\rightarrow$U      & 0.5 & 0.5   & 0.1 & 4 \\
                          & E$\rightarrow$B      & 0.5 & 0.5   & 0.1 & 4 \\
\midrule[0.8pt]
\multirow{6}{*}{Citation} & A$\rightarrow$D      & 0.1 & 0.1   & 0.1 & 3 \\
                          & D$\rightarrow$A     & 0.1 & 0.1   & 0.01 & 4 \\
                          & A$\rightarrow$C      & 0.5 & 0.5   & 0.01 & 4 \\
                          & C$\rightarrow$A      & 0.1 & 0.1   & 0.1 & 3 \\
                          & C$\rightarrow$D      & 0.1 & 0.1   & 0.1 & 4 \\
                          & D$\rightarrow$C      & 0.1 & 0.1   & 0.1 & 4 \\



\midrule[0.8pt]
\multirow{2}{*}{Blog}     & B1$\rightarrow$B2      & 0.1 & 0.1   & 0.1 & 2 \\
                          & B2$\rightarrow$B1     & 0.1 & 0.1   & 0.1 & 3 \\

\midrule[0.8pt]
\multirow{2}{*}{Twitch}   & DE$\rightarrow$EN      & 0.1 & 0.1   & 0.01 & 2 \\
                          & EN$\rightarrow$DE     & 0.1 & 0.5   & 0.5 & 2 \\


\midrule[0.8pt]

\multirow{6}{*}{MAG}      & US$\rightarrow$CN      & 0.5 & 0.1   & 0.1 & 5 \\
                         & US$\rightarrow$DE      & 0.1 & 0.1   & 0.1 & 5 \\
                          & US$\rightarrow$JP      & 0.1 & 0.5   & 0.01 & 6 \\
                          & US$\rightarrow$RU      & 0.1 & 0.1   & 0.5 & 5 \\
                          & US$\rightarrow$FR      & 0.1 & 0.1   & 0.1 & 6 \\
                          & CN$\rightarrow$US      & 0.1 & 0.1   & 0.01 & 6 \\
                          & CN$\rightarrow$DE      & 0.1 & 0.1   & 0.5 & 6 \\
                         & CN$\rightarrow$JP      & 0.1 & 0.1   & 0.01 & 5 \\
                          & CN$\rightarrow$RU      & 0.5 & 0.1   & 0.1 & 5 \\
                          & CN$\rightarrow$FR      & 0.1 & 0.01   & 0.1 & 6 \\


\midrule[0.8pt]


\end{tabular}}
\caption{{Experiment hyperparameter setting Value.}}
\label{tab:datasets}
\end{table*}




\section{T-SNE sample}

\begin{figure*}[!htbp]
\centering
\subfigure[UDAGCN]{\includegraphics[width=0.18\textwidth]{UDAGCN.png}}
\hfill
\subfigure[JHGDA]{\includegraphics[width=0.18\textwidth]{JHGDA.png}}
\hfill
\subfigure[GRADE]{\includegraphics[width=0.18\textwidth]{GRADE.png}}
\hfill
\subfigure[SpecReg]{\includegraphics[width=0.18\textwidth]{SpecReg.png}}
\hfill
\subfigure[GAA]{\includegraphics[width=0.18\textwidth]{our_method.png}}
\caption{Visualization of learnt representations of different methods on D-A task of dataset.}
\label{fig:tsne graph with different methods}
\end{figure*}



\section{Tightness of bounds}
To evaluate the tightness of our bounds, we conduct additional experiments to verify the effects of node attribute divergence and topology divergence independently. The following experimental detail settings are designed to verify these divergences.
\subsection{attribute divergence}
To evaluate the impact of graph attribute discrepancy on GDA, we designed an experiment for this purpose. In this experiment, we provide node classification tasks across different graphs under different attribute discrepancy with same topology structure.
In this procedure, we aim to generate a collection of graph datasets, where each graph is characterized by a fixed adjacency matrix $A$, consisting of $100$ nodes with an average degree of $0.3$, and node attribute matrices $X$ randomly simulated from Gaussian-distributed samples. The specific steps are as follows: Each graph $ G_i = (A, X_i) $ shares the same fixed adjacency matrix $ A $, representing the same graph topology. $ A $ is predetermined and defines the connectivity between nodes, remaining consistent across all generated graphs. Node attributes $ X_i $ for each graph $ G_i $ are generated using 'make-blobs' function from scikit-learn. Parameters for 'make-blobs': Number of nodes: $ n_\text{samples} = 100 $, representing the total number of nodes in the graph.  
Number of clusters: $ \text{centers} = 2 $, corresponding to two distinct classes. $ n_{\text{features}} = 10 $, meaning each node is described by a 10-dimensional feature vector. $ \text{cluster\_std} $ is a variable parameter uniformly sampled from the range $ [0, 2] $, determining the dispersion of node features within each cluster. We construct a dataset of 1000 graphs, $ \{G_i = (A, X_i)\}_{i=1}^{1000} $, where: $ A $: the adjacency matrix, remains fixed across all graphs, representing the structural relationships between nodes. $ X_i $: the feature matrix, varies between graphs. The variance of the node features is determined by $ \text{cluster\_std} $, which is uniformly sampled for each graph to introduce diversity in the node attributes. GAA is trained for 100 epochs on a fixed source graph and target graphs with different attribute variances. After training, we reports three key metrics for each dataset: the bound value, $ \mathcal{L}_A $ (loss value), and the target graph accuracy. To ensure that the bound value and loss value are on the same scale, we normalize the bound value by dividing it by the number of nodes, i.e., $ 100 $. As illustrated in Figure 6(a), both the bound value and the loss value of the model increase as the attribute discrepancy grows. Conversely, the classification performance declines with increasing attribute discrepancy, highlighting that the bound attribute component is closely related to the GDA performance.

\subsection{topology divergence}
This procedure involves generating 1000 graphs by Stochastic Block Model (SBM), a probabilistic model for community-structured graphs. Each graph consists of different adjacency matrix $ A $ with uniformly distributed edge weights and a node attribute matrix $ X $ with fixed-dimensional feature vectors. The generation process is detailed below:
The graph $ G_i = (A_i, X_i) $ for each instance is generated using the SBM. SBM parameters are as follows: community contain $100$ nodes( num\_nodes = 100): $ \text{sizes} = \left[\frac{\text{num\_nodes}}{2}, \text{num\_nodes} - \frac{\text{num\_nodes}}{2}\right], $
where the graph is divided into two communities of approximately equal size, which can be seen as 2 classes.
Inter- and Intra-community connection probabilities: $\text{probs} = \begin{bmatrix} p & \frac{p}{10} \\ \frac{p}{10} & p \end{bmatrix}$, where $ p = 0.8 $ denotes the probability of edges forming within a community and $ \frac{p}{10} = 0.08 $ denotes the probability of edges forming different communities.
To incorporate variability in edge strengths, the weights of edges in the adjacency matrix $ A_i $ are drawn from a uniform distribution. Nonexistent edges are assigned a weight of $ 0 $, thereby preserving the sparsity structure dictated by the Stochastic Block Model (SBM). Each adjacency matrix $ A_i $, representing graph $ G_i $, is a symmetric $ n \times n $ matrix. Additionally, each graph $ G_i $ is associated with a node attribute matrix $ X_i $, where $ X_i $ contains $ n $ rows, corresponding to the attributes of the $ n $ nodes. Each nodes is a fixed 10-dimensional vector, ensuring consistent node attribute dimensionality across all graphs. 
All elements of $ X_i $ are set to a constant value of $ 1 $, ensuring uniform node attributes across the dataset. The dataset comprises 1000 graphs, each containing 100 nodes, represented as $ \{G_i = (A_i, X_i)\}_{i=1}^{1000} $. In this representation: $ A_i $ varies between graphs, following the Stochastic Block Model (SBM) with uniform edge weights, while $ X_i $ is a fixed matrix where each of its 10-dimensional elements is set to $ 1 $.
GAA is trained for 100 epochs on a fixed source graph and target graphs with different topology variances. After training, we reports three key metrics for each dataset: the bound value, $ \mathcal{L}_A $ (loss value), and the target graph accuracy.
Similarly, we normalize the bound value by dividing it by the number of nodes, i.e., $ 100 $. As illustrated in Figure 6(b), both the bound value and the loss value of the model increase as the attribute discrepancy grows. Conversely, the classification performance declines with increasing topology discrepancy, emphasizing that the bound's topology component is also closely linked to the GDA performance.





\begin{figure}[!htbp]
\centering
\subfigure[Attribute component]{\includegraphics[width=0.6\textwidth]{Fig-appendix1.png}}
\hfill
\subfigure[Topology component]{\includegraphics[width=0.6\textwidth]{Fig-appendix2.png}}
\caption{{Visualization of bound value and $\mathcal{L}_A$ value.}}
\label{fig:tsne graph with different methods}
\end{figure}


\section{Model Efficient Experiment}
{To further investigate the efficiency of GAAo, Table\ref{time datasets} reports the running time comparison across various algorithms. We also compared the training time and GPU memory usage of common baselines UDAGCN and a recent SOTA method, JHGDA, which aligns graph domain discrepancy hierarchical levels. As shown in Table, the evaluation results on airport dataset further demonstrate that our method achieves superior performance with tolerable computational and storage overhead.}

\begin{table*}[!t]
\centering
\small
\scalebox{0.8}{\begin{tabular}{|c|c|c|c|c|c|}
\toprule[0.8pt]
Dataset   &Method  &Training Time (Normalized w.r.t. UDAGCN)   &Memory Usage (Normalized w.r.t. UDAGCN)    & Accuracy($\%$)         \\ 
\midrule[0.8pt]
\multirow{6}{*}{U$\rightarrow$B}  & UDAGCNB      & 1 & 1   & 0.607 \\
                          & JHGDA     & 1.314 & 1.414    & 0.695 \\
                          & PA      & \bf0.498 & \underline{0.517}   & 0.679 \\
                          &$GAA_o$ & \underline{0.504} & \bf0.514    & \underline{0.697} \\
                          & $GAA$ & 1.063   & 1.113 & \bf0.704 \\
\midrule[0.8pt]

\multirow{6}{*}{U$\rightarrow$E}  & UDAGCNB      & 1 & 1   & 0.488 \\
                          & JHGDA     & 1.423 & 1.513    & 0.519 \\
                          & PA      & \underline{0.511} & \bf0.509   & \underline{0.557} \\
                          &$GAA_o$ & \bf0.507 & \underline{0.513}   & 0.556 \\
                          & $GAA$ & 1.109   & 1.098 & \bf0.563 \\
\midrule[0.8pt]
\multirow{6}{*}{B$\rightarrow$E}  & UDAGCNB      & 1 & 1   & 0.510 \\
                          & JHGDA     & 1.311 & 1.501    & 0.569 \\
                          & PA      & \bf0.502 & \bf0.497   & 0.562 \\
                          &$GAA_o$ & \underline{0.507} & \underline{0.503}   & \underline{0.566} \\
                          & $GAA$ & 1.048   & 1.107 & \bf0.573 \\
\midrule[0.8pt]



\end{tabular}}
\caption{{Comparison of Training Time, Memory Usage, and Accuracy on Airport datset.}}
\label{tab:time datasets}
\end{table*}




\end{document}
