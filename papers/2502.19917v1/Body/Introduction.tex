\section{Introduction}

Recent advancements in multimodal large language models (MLLMs) have led to significant progress in general visual understanding~\citep{DBLP:journals/corr/abs-2303-08774,DBLP:journals/corr/abs-2305-06500,DBLP:conf/nips/LiuLWL23a}. By integrating state-of-the-art large vision models (LVMs) with large language models (LLMs) and training on vast amounts of visual data, MLLMs have demonstrated the ability to comprehend complex images, achieving remarkable performance across a variety of visual tasks, including object recognition~\citep{DBLP:conf/cvpr/ZhangHMLLXQLLLG22}, visual question answering~\citep{DBLP:conf/icml/YuYLWL0WW24}, and image captioning~\citep{DBLP:conf/naacl/LevinboimTSS21}.
The training of MLLMs generally involves two main steps: first, pretraining on image-caption pairs to align the large vision model with the language model; second, instruction tuning on downstream visual tasks to enable the model to better understand complex images and perform sophisticated visual tasks.
To improve the multimodal understanding and task-solving abilities of MLLMs, existing approaches typically collect large-scale visual task data from a variety of sources for constructing instruction tuning datasets~\citep{DBLP:journals/corr/abs-2408-03326,DBLP:conf/nips/TongBWWIAYYMWPF24}. 


\input{Figures/motivation}


Specifically, existing works have primarily focused on collecting open-source training datasets from existing visual tasks~\citep{DBLP:journals/corr/abs-2408-03326,DBLP:conf/cvpr/LiuLLL24,DBLP:conf/nips/TongBWWIAYYMWPF24}, or synthetically generating various instructions using GPT-4 based on the collected images and description~\citep{DBLP:conf/nips/LiuLWL23a,DBLP:journals/corr/abs-2406-19736}. Such approaches enable the collection of large amounts of instructional data at a relatively low cost, without the need for extensive manual annotation. 
However, they often introduce significant noise into the training datasets. For example, images with low complexity or irrelevant instruction-image pairs can be problematic.  
As illustrated in Figure~\ref{fig:motivation}, the left subfigure shows an image of a presentation slide, which contains only a simple icon and basic presentation information. Such images provide little useful visual content or knowledge, making it difficult for models trained on such images to generalize to more complex real-world applications, where informative visuals are essential for learning robust visual representations. Another common issue is when the instruction is weakly aligned with the image, as shown in the right subfigure, where the instruction is a common-sense question that can be answered without referencing the image. This type of data fails to effectively link the visual model's perception with the textual knowledge stored in the language model, reducing the efficiency of multimodal learning.


To address this issue, we present a novel approach called \textbf{Vi}sual-Centric \textbf{S}election via \textbf{A}gents Collaboration (ViSA). This method leverages collaboration among multiple visual agents to assess the quality of visual data, selecting high-quality images and instruction pairs to enhance the training efficiency of MLLMs. The approach consists of two key components: 1) \textbf{Visual Information Quantification}, where agents collaborate to quantify visual elements and assess the richness of diverse image perspectives, selecting informative images; and 2) \textbf{Image-Centric Instruction Quality Quantification}, which focuses on selecting instructions tightly coupled with the images. This is achieved by leveraging multiple agents to calculate metrics such as prior token perplexity and image-text mutual information, enabling the evaluation of response quality and its relevance to the visual content. Additionally, to improve the collaboration across multiple visual agents, we introduce a Shapley value based on the Pearson correlation coefficient to weigh the reliability of each agent’s assessment.


To thoroughly assess the effectiveness of our method, we filter out 80K high-quality instruction data from the LLaVA-OneVision dataset~\cite{DBLP:journals/corr/abs-2408-03326}. We then conduct extensive experiments to evaluate the training effectiveness of the filtered data.
The results on seven visual benchmarks show that a model trained with less than 5\% of the data used by baselines achieves performance comparable to, or even surpassing, state-of-the-art models, highlighting the importance of high-quality data for understanding intricate images.
Furthermore, we observe significant improvements when continuing the training of a 72B model with a small set of high-information-density data, demonstrating that larger models can still benefit from informative images and well-aligned instructions. 
Overall, our agent collaboration-based data selection method effectively reduces noise in training datasets, leading to a significant increase in MLLM training efficiency.


Our main contributions can be summarized as follows:
\begin{itemize}[leftmargin=*]

\item We propose a novel multi-agent framework for quantifying and selecting high-quality visual instruction data. This framework evaluates both image informativeness and instruction relevance, leveraging a diverse set of visual agents to assess general visual elements as well as diverse image-specific features. To the best of our knowledge, this is the first work to explicitly introduce image informativeness evaluation in the context of visual data selection.

\item We introduce a new instruction quality quantification method based on prior token perplexity and image-text mutual information, which measure the response quality for selecting instructions tightly coupled with the images, respectively. We propose the Shapley value approach based on the Pearson correlation coefficient to effectively combine the evaluations from multiple agents.

\item We conduct extensive experiments and ablation studies, demonstrating that our approach significantly improves the training efficiency of MLLMs. Using only 80K selected samples, our method enables both 2B-scale and 7B-scale vision models to achieve performance on par with state-of-the-art models. Moreover, with a larger 72B-scale model, our high-quality selected data consistently enhances the model’s performance in visual understanding and reasoning tasks.

\end{itemize}
