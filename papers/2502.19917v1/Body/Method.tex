\section{Method}
\label{sec:method}

\input{Figures/framework}

In this section, we introduce our visual-centric selection via
agents collaboration. First, to identify informative images, we comprehensively consider various aspects of image information density assessment, including evaluation based on general visual elements~\S\ref{sec: Visual Elements Quantification} and assessment from other diversity visual information perspectives~\S\ref{sec: Diversity Perspectives Quantification}. Subsequently, we refine multimodal data selection on the textual level, incorporating text quality evaluation based on prior token perplexity and text relevance assessment based on mutual information~\S\ref{sec: Text Quality Quantification}. Finally, using our proposed method, we curated a high-quality instruction fine-tuning dataset~\S\ref{sec:Data Selection}.

\subsection{Visual Elements Quantification}
\label{sec: Visual Elements Quantification}

Evaluating image complexity from the perspective of visual elements is crucial for understanding image informativeness. However, existing methods primarily focus on object-level recognition or coarse-grained overall image assessment, lacking a fine-grain quantification of visual complexity. To address this, we assess image informativeness based on general visual elements (e.g., visual objects, graphical elements, characters, etc.), which are inherently linked to image complexity.
Specifically, we introduce two evaluation metrics: \textbf{Segmentation Complexity Score (SC Score)} and \textbf{Object Alignment Score (OA Score)}. The SC Score is designed to assess the richness of graphical elements and characters in an image, leveraging a visual segmentation method. In contrast, the OA Score evaluates the richness of objects in an image using the object detection approach based on a predefined category set.

Specifically, for the SC Score, we predefine 512 anchor points and employ the SAM2 model~\citep{DBLP:journals/corr/abs-2408-00714}, which segments target regions based on the provided points, to generate a set of segmentation boxes denoted as $M$. The SC Score is then computed using the following formula:
\begin{equation}
     \text{Score}(SC) = \text{count}(\text{IoU}(M, 0.75)).
\end{equation}

To evaluate the richness of objects, motivated by prior grounding work~\cite{DBLP:journals/corr/abs-2401-14159}, we first predefine a set of over 1800 common image object categories. Using the DINO model~\cite{DBLP:conf/eccv/LiuZRLZYJLYSZZ24}, which identifies the presence and location of predefined tags within images, we detect the visual objects corresponding to each category.
The OA Score is then computed by calculating the frequency of each category and normalizing it using the term frequency-inverse document frequency weight algorithm (TF-IDF), as shown in the following:
\begin{equation}
    \text{Score}(OA) = \sum_{i} \text{TF-IDF}(C_i),
\end{equation}
where $C_i$ is the predicted frequency of the $i$-th category, which emphasizes the importance of rare objects within the image set.

\subsection{Diversity Perspectives Quantification}
\label{sec: Diversity Perspectives Quantification}

We found that while the aforementioned visual element metrics provide a estimate of image informativeness with expectations, they are insufficient for fully capturing the richness of other types of image information. To address this limitation, we propose a multi-agent-based  \textbf{Diversity Perspective Score (DP Score)}, designed to evaluate images from multiple perspectives and dimensions, including events, emotions, culture, composition, and dynamics. These diverse perspectives offer a more comprehensive view of image informativeness, accounting for the varied origins and functional purposes of image creation. Specifically, we design a detailed evaluation guideline, complete with multiple examples, prompting various state-of-the-art visual agents to assess the image from each perspective and produce a final integrated score. 
For the specific evaluation guideline used, please refer to Appendix~\ref{sec:prompt appendix}.


To integrate the final score of each agent, we use a Shapley value approach~\citep{DBLP:conf/nips/LundbergL17} based on the Pearson correlation coefficient to compute the weight of each agent, motivated by the idea of determining model weights based on score consistency. Specifically, the weight $W_j$ of each agent is calculated as follows:
\begin{equation}
\begin{split}
    W_j = \sum_{S \subseteq N \setminus \{j\}} \omega(S) \Delta \rho(S, j), \\
    \omega(S) = \frac{|S|! (|N| - |S| -1)!}{|N|!}, \\
    \Delta \rho(S, j) = \rho(S \cup \{j\}) - \rho(S)
\end{split}
\end{equation}
where $N$ represents the set of all agents, and $S$ is any subset of agents excluding the $j$-th agent. $\text{Pearsonr}(S)$ is the Pearson correlation coefficient of the score distribution over the agents set $S$, measuring the linear relationship between the scores. The overall diversity perspective score for each image is then computed as the weighted average of the diversity perspective scores $E_j$ from multiple agents.
\begin{equation}
    \text{Score}(DP) = \sum^{j}_{|N|} W_j E_j.
\end{equation}


\subsection{Text Quality Quantification}
\label{sec: Text Quality Quantification}

To fully capture the visual and textual aspects of image informativeness, the quality of the associated text is also important. In this regard, we propose a multi-agent-based  text quality quantification method, which includes two key metrics: the  \textbf{Prior Token Perplexity Score (PT Score)}, which measures the quality of the textual content, and the \textbf{Image-Text Mutual Information Score (IM Score)}, which assesses the degree of association between the text and the image.
Similar to the approach in Section~\ref{sec: Visual Elements Quantification}, we utilize three different agents to compute the PT Score and the IM Score separately, and then aggregate these scores using the Shapley value approach to obtain their comprehensive quality score.

Specifically, the PT Score is calculated by evaluating the perplexity of the response, which is defined as follows. Due to the significant variability in the length distribution of the data instructions, we adopt a more length robust approach to prior token perplexity~\cite{DBLP:conf/emnlp/LiuLHZCHZ24}:
\begin{equation}
    P_j = \exp \left( -\frac{1}{N} \sum_{i=1}^{K} \log P(w_i | w_{<i}) \right),
\end{equation}
\begin{equation*}
   \text{Score}(TP) = \sum^j_{N} \sum_{S \subseteq N \setminus \{j\}} \omega(S) \Delta \rho(S, j) P_j,
\end{equation*}
where $K$ denotes the number of prior tokens. This score reflects how predictable or coherent the sequence of tokens is based on preceding words.

Next, we compute the IM Score, which measures the mutual dependence between the image and the associated text, formulated as following:
\begin{equation}
    I_j = H(\text{Text}) - H(\text{Text} | \text{Image}),
\end{equation}
\begin{equation*}
   \text{Score}(IM) = \sum^j_{N} \sum_{S \subseteq N \setminus \{j\}} \omega(S) \Delta \rho(S, j) I_j ,
\end{equation*}
where $H(\text{Text})$ is the entropy of the text without the image input, and $H(\text{Text} | \text{Image})$ is the conditional entropy of the text given the image. A higher mutual information value indicates a stronger association between the text and the image. 


\subsection{Data Selection}
\label{sec:Data Selection}

To evaluate the effectiveness of our data selection approach, random sample 175K instruction data points from the open-source LLaVA-OneVision dataset~\citep{DBLP:journals/corr/abs-2408-03326}. We then applied the SC score, OA score, and DP score to filter out 15\%, 20\%, and 13\% of the data, respectively, resulting in 100K high-quality image samples. Subsequently, we further refined the dataset by applying the TP score and IM score to select 10\% of the data, yielding a final set of 80K high-quality instruction fine-tuning samples.