\section{Experiment}

\subsection{Evaluation}

To evaluate the performance of our method, we introduce three well-established image understanding benchmarks, including VQAv2~\citep{DBLP:conf/iccv/AntolALMBZP15}, OKVQA~\citep{DBLP:conf/cvpr/MarinoRFM19}, TextVQA~\citep{DBLP:conf/cvpr/SinghNSJCBPR19}, along with four cutting-edge datasets designed for complex visual comprehension and reasoning, including MMBench~\citep{DBLP:conf/eccv/LiuDZLZZYWHLCL24}, MME-RealWorld~\citep{DBLP:journals/corr/abs-2408-13257}, SEED-Bench~\citep{DBLP:journals/corr/abs-2307-16125}, MMMU~\citep{DBLP:conf/cvpr/YueNZ0LZSJRSWYY24}. We provide more detail in the Appendix~\ref{sec:evaluation_appendix}.

\subsection{Baselines}

We compare our approach against SOTA MLLMs, including Qwen2-VL~\citep{DBLP:journals/corr/abs-2409-12191}, Llava-OneVision~\citep{DBLP:journals/corr/abs-2408-03326}, Llama-3.2-Vision~\citep{DBLP:journals/corr/abs-2407-21783}, as well as InternVL2 and InternVL2.5~\citep{DBLP:journals/corr/abs-2312-14238}. all of which have been trained on massive-scale datasets with extensive computational resources.
To comprehensively evaluate the effectiveness of our method across different model scales, we conduct experiments using models of three representative parameter sizes: small-scale (2B parameters), medium-scale (7B–11B), and large-scale (70B–90B). 
% Specifically, we include 2B versions of Qwen2-VL, InternVL2, and InternVL2.5. For medium-scale models, we utilize Qwen2-VL-7B, InternVL2-8B and InternVL2.5-8B, Llava-OneVision-Qwen2-7B, and Llama-3.2-Vision-11B. Finally, for large-scale models, we experiment with Qwen2-VL-72B, InternVL2-76B, InternVL2.5-78B, Llava-OneVision-Qwen2-72B and Llama-3.2-Vision-90B.
Due to the computational constraints, we employ the INT4 quantized versions of all large-scale models. 

\input{Table/MianResult}

\input{Table/Ablation}

\subsection{Implementation Detail}

We conduct full fine-tuning on Qwen2-VL-2B and Qwen2-VL-7B as our base models. Due to computational constraints, we adopt Qwen2-VL-72B-instruction as the base model for large-scale experiments and apply LoRA for efficient continued fine-tuning.
All models are optimized using AdamW~\cite{DBLP:conf/iclr/LoshchilovH19} with a learning rate of 1e-5 for full fine-tuning and 1e-4 for LoRA-based fine-tuning, along with a weight decay of 0.01. We employ a linear warmup strategy with a 0.1 warmup ratio and set the LoRA rank to 16. The maximum sequence length is 2048 tokens, and all models are trained and evaluated using the BFloat16 floating-point format.
For training efficiency, we use a batch size of 32 with four-step gradient accumulation on a single GPU. Additionally, we follow the original chat template of Qwen2-VL for model interactions. We used beam search with the beam size set to 4 for generation. We run all the experiments on the NVIDIA H800 GPU. 

\subsection{Main Result}



In this section, we explore the two following research questions: (1) Q1: How does our data compare with current mainstream training datasets in terms of efficiency? and (2) Q2: Can a small amount of high-quality data continuously improve a well-trained, large-scale model?

\nosection{Q1: A small amount of high-quality data significantly efficiency improves the instruction-following ability of pretrained MLLMs.}
As shown in Table~\ref{tab:main result}, with less than 5\% of the baseline training data, our method achieves comparable or superior performance across all datasets. Specifically, our Qwen2-VL-2B-AC model achieves an average score of 61.0\% on 7 benchmarks, outperforming Qwen2-VL-2B (57.3\%), InternVL2.5-2B (59.4\%), and InternVL2-2B (57.5\%). For middle-scale models, our Qwen2-VL-7B-AC model reaches an average score of 68.2\%, surpassing Llama3.2-vision-11B (53.9\%), InternVL2-8B (63.8\%), and matching the performance of Llava-Onevision-7B (68.1\%), Intern2.5-8B (68.3\%), and Qwen2-VL-7B (68.7\%).
These results suggest that the key factor for training pretrained MLLMs is not solely the amount of data, but rather the quality of the data. A small amount of high-quality data can significantly boost the multimodal instruction-following capability of pretrained MLLMs, enabling the model to activate the visual knowledge learned during pretraining and perform a variety of visual tasks.
Moreover, we observe that on the MME-RealWorld dataset, a complex real-world visual benchmarks, our model consistently outperforms the baseline, highlighting the crucial role of complex visual data in real-world applications.
However, we also see that compared to the small-scale model, the advantages of high-quality data are less pronounced in the middle-scale model. This may be because larger models require more high-quality data to fully fine-tune the deeper parameters, a finding also shown in the work of training scaling law~\citep{DBLP:journals/corr/abs-2001-08361}.


\nosection{Q2: A small amount of high-quality data can continuously boost large-scale MLLMs.} 
As shown in Table~\ref{tab:main result}, continued training on the Qwen2-VL-72B model leads to noticeable improvements in its image understanding capabilities. The Qwen2-VL-72B-AC model achieves 73.9\% on 7 datasets, surpassing other baseline models. Specifically, compared to the original Qwen2-VL-72B, our fine-tuned model shows significant gains on the two datasets that focus on complex image perception and understanding: OKVQA and MME-RealWorld (4.1\% and 6.6\% improvement, separately). These results suggest that even large models trained on vast amounts of data can still be hungry for high-quality data, and a small amount of high-quality data can rapidly boost their performance. Moreover, this also highlights the importance of high-quality data for effective continued instruction tuning in very large models.



\section{Ablation Study}


In this section, we conduct ablation experiments to assess the effectiveness of the proposed filtering methods, as presented in Table~\ref{tab:ablation result}. Our findings reveal that even when the total amount of data is increased by removing the specific filter, the performance quickly degrades. This suggests that fine-tuning with a small amount of high-quality data is significantly more efficient than using large amounts of noisy data. Our method effectively filters out noisy data, thereby improving the fine-tuning efficiency of the model.
Notably, the performance shows the most significant decline when the 
instruction quality filter is removed, with average performance drops of 3.1\% and 2.5\% for the small and middle-scale models, respectively. This confirms our observation that weakly aligned instructions in the current visual instruction data have a detrimental effect. Poorly aligned instruction data negatively impacts the perception and knowledge alignment between the visual encoder and the large language model, ultimately reducing the overall performance.
Moreover, the removal of the visual element filter also leads to a noticeable decrease in performance (1.6\% and 2.1\% for the small and middle-scale models, respectively). This emphasizes the critical role of image quality in data selection; low-quality or noisy images hinder the efficiency of visual representation learning by the vision encoder.

\input{Figures/Distribution}
\input{Figures/perception_eval}


\section{Complex Visual Understanding}

To fully explore the role of high-quality data in complex image perception and understanding, we selected six sub-tasks of image  comprehension from the practical related benchmark, including sentiment classification, quality assessment, scene perception, and style classification from MMBench, as well as scene understanding from SEED Bench and visual perception from MME-RealWorld. We report the results of our method on a 7B model setup, as shown in Figure~\ref{fig:complex understanding}. 

Our findings reveal that, unlike the similar overall performance with baselines in the main results, our Qwen2-VL-7B-AC model consistently outperforms the baseline across all complex image understanding tasks. This demonstrates that informative image data is crucial for improving a model's ability to perceive and understand complex images. While recent works~\citep{DBLP:conf/cvpr/LiYLMZYSLB24,DBLP:conf/eccv/GuoXYCNGCLH24} in visual model architectures often focus on increasing image resolution to enhance complex image understanding, higher resolution images do not necessarily equate to greater complexity. For example, compared to a high-resolution document scan with more content, a lower-resolution yet content-rich street scene image proves to be far more beneficial for a model’s visual representation learning. Images containing dense visual elements effectively activate the model’s ability to recognize and learn the relationships between these elements, thus bridging the gap between the visual model's low-level visual perception and the world knowledge stored in the pretrained language model.



\section{Quantification Distribution Analysis}


In this section, we present a visualization and detailed analysis of the proposed image informativeness metrics, focusing on three metrics, with the additional metrics provided in Appendix~\ref{sec:distrubution appendix}.

\paragraph{Segmentation complexity score distribution}
As shown in Figure~\ref{fig:score_distributions}, we observed that the distribution of the segmentation complexity score varies across different datasets. For example, the score of the ``llava\_gpt4\_20k'' dataset is relatively low, as it typically contains text-image pairs with limited visual content, resulting in fewer visual elements within the dataset. In contrast, the ``sharegpt4v'' dataset primarily includes scene images from real-life scenarios, which are generally richer in visual elements, leading to higher SC scores. This difference indicates that the ``sharegpt4v'' dataset contains more complex images.
This distribution demonstrates that our proposed segmentation complexity score effectively reflects the number of visual elements in the data, indirectly capturing the complexity of the images. 


\paragraph{Diversity perspectives score distribution}
Unlike the distribution of other scores, the diversity perspectives score exhibits a notably extreme trend, with the visual agent tending to assign either the highest or the lowest ratings. Our analysis suggests that this is primarily due to the multifaceted nature of the image evaluation process, where images are assessed with multiple dimensions. The diversity in the types and sources of images further complicates the task of providing a comprehensive score. Additionally, we intentionally designed the system to prevent the agent from assigning low scores based on a few weak dimensions of an image. For example, we cannot devalue the quality of a close-up photograph of still life simply due to the lacking of visual events. As a result, the agent tends to classify images as either extremely poor or exceedingly rich in terms of visual content.
This underscores the substantial challenge in evaluating images across multiple dimensions of quality. The diverse characteristics of images make it difficult to achieve a balanced and nuanced assessment, which remains an open issue in multimodal evaluation tasks.

\paragraph{Prior token perplexity score distribution}
For analytical convenience, we apply the negative transformation to the prior token perplexity scores when visualization. Upon examining the distribution of the prior token perplexity scores, we observe significant variability across datasets. This can likely be attributed to the greater disparity in the response text across different tasks. While higher scores (shifted towards the right) indicate that the token probability distribution is more certain. However, excessively high scores may also reflect overly simplistic responses. Therefore, when setting the threshold for filtering prior token perplexity scores, we exclude a small number of outlines values to enhance training efficiency.
 

