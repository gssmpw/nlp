\section{Related Work}

\paragraph{Vision Large Models}: The development of Vision Large Models (VLMs) has led to significant advances in visual comprehension and reasoning~\citep{DBLP:conf/icml/0001LXH22,DBLP:conf/icml/0008LSH23,DBLP:conf/iclr/Zhu0SLE24,DBLP:conf/nips/LiuLWL23a,DBLP:conf/cvpr/LiuLLL24,DBLP:journals/corr/abs-2405-11273}. The training of multimodal large models (MLLMs) typically consists of two key stages. First, vision-language alignment is achieved by mapping the visual representations obtained from a pretrained Large Visual Model (LVMs)~\citep{DBLP:journals/corr/abs-2303-15389,DBLP:conf/icml/RadfordKHRGASAM21} into the LLM representation space through a learnable projection network~\citep{DBLP:conf/iclr/MerulloCEP23,DBLP:conf/icml/0008LSH23}. Subsequently, the aligned models are fine-tuned using instruction data to enhance their ability to process complex visual instructions~\citep{DBLP:journals/corr/abs-2312-14238,DBLP:journals/corr/abs-2308-12966,DBLP:journals/tmm/LiHCMXZ24,DBLP:conf/nips/Dai0LTZW0FH23}. Recently, there has been a growing trend in high-resolution MLLMs~\citep{DBLP:conf/eccv/GuoXYCNGCLH24,DBLP:journals/corr/abs-2407-07895,DBLP:journals/corr/abs-2308-12966,DBLP:journals/corr/abs-2409-12191}. Consequently, the ability to benefit from complex images has become increasingly important as informative images that can maximize these models' potential. Hence, our work aims to develop a quantitative assessment of image informativeness to select high-quality data.



\paragraph{Vision Instruction Datasets}: Instruction tuning aims to enhance a model's ability to understand and reason about images through diverse vision-language tasks, such as visual question answering~\citep{DBLP:conf/iccv/AntolALMBZP15}, image captioning~\citep{DBLP:conf/icml/RadfordKHRGASAM21}, and object detection~\citep{DBLP:journals/corr/abs-2408-00714}. MultiInstruct~\citep{DBLP:conf/acl/XuSH23} introduced the first human-annotated multimodal instruction tuning dataset, designed to enhance the zero-shot capabilities of pretrained VLMs. Building upon this foundation, subsequent studies have increasingly focused on curating diverse image datasets and harnessing the power of large multimodal models, such as GPT-4V, to automatically generate large-scale visual instruction data~\citep{DBLP:journals/corr/abs-2408-03326, DBLP:conf/acl/XuFSASJCWH24}. Several recent studies have explored different strategies to improve visual instruction tuning. Some works~\citep{DBLP:journals/corr/abs-2306-17107,DBLP:conf/aaai/HuXLLCT24} employ OCR tools and GPT-4 to generate instruction-following tasks that help VLMs better understand text within images. Meanwhile, recent works~\citep{DBLP:journals/corr/abs-2311-07574, DBLP:conf/eccv/ChenLDZHWZL24} directly prompt GPT-4V with images as inputs to generate visual instruction tuning data.
Unlike previous works that focus on instruction generation strategies, our approach emphasizes high-quality data selection by systematically assessing image informativeness and image-instruction relevance. 

