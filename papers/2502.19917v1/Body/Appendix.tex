\label{sec:appendix}

\section{Diversity Perspectives Prompt}
\label{sec:prompt appendix}

We present the default version prompt for diversity perspectives quantification used in our experiment in the~\autoref{tab:full_prompt}. 

\section{Quantification Distribution Analysis}
\label{sec:distrubution appendix}

\input{Figures/Distribution_appendix}

\paragraph{Object alignment score distribution}
As shown in Figure~\ref{fig:score_distributions_appendix}, our object alignment score  exhibit similar trends with previous segmentation complexity score. However, while the SC score tends to reflect small visual elements such as shapes and color blocks, the OA score is more aligned with visual entities. As a result, the distribution of the OA score is more concentrated. Higher OA scores indicate the presence of more rare and distinctive visual entities, highlighting the uniqueness of the image. In contrast, images with lower OA scores contain more common and less distinctive entities, resulting in lower informational richness. The presence of too many common entities may introduce bias into the visual model’s representation learning, thereby affecting its efficiency.

\paragraph{Image-text mutual information score distribution}
 The distribution of the image-text mutual information score reveals a concerning trend: certain open-source datasets primarily focus on evaluating instruction-following ability, often overlooking the critical relationship between the instruction and the corresponding image. For example, we observed that the llava\_gpt4\_20k dataset frequently contains textual explanations or discussions that are largely unrelated to the image content. Such data not only fails to effectively guide the visual model in learning meaningful visual representations but also introduces significant noise into the visual instruction learning process. This highlights the need for careful curation of datasets to mitigate such mismatches and enhance the quality of visual instruction learning. These findings emphasize the importance of aligning image-instruction pairs in multimodal datasets, ensuring that both components are relevant and complementary to the task at hand. Such principles should guide future efforts in constructing more effective and coherent multimodal datasets.
 

\section{Case Study}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/case_study_cut.pdf}
    \caption{Case study of our proposed image informative metrics. Images on the right exhibit higher scores and richer content, while images on the left correspond to lower scores and simpler content.}  
    \vspace{-4mm}
    \label{fig:case study}
\end{figure*}


To provide a more intuitive demonstration of the proposed image information richness metrics, we present a case study, shown in Figure~\ref{fig:case study}. In this figure, images on the right correspond to higher scores, while images on the left are associated with lower scores. Upon observation, we find that all three proposed metrics effectively filter for images with richer content. The segmentation complexity score favours  simple visual elements, such as shapes and color blocks. For instance, in the SC score examples, the image on the far left—featuring only a single object, a pair of scissors—receives a low SC score, whereas the volleyball match image on the far right, with richer content, achieves a higher score. However, we also present an example of misjudgment: the second image on the right, a fabric with repeated textures, has a relatively high SC score despite its overall simplicity. This highlights the limitation of using basic image segmentation as a sole measure of image complexity.

In contrast, the object alignment score does not suffer from this issue. For example, in the third image on the right, depicting a crowd, the presence of numerous people does not result in an inflated OA score, as the frequent repetition of entities in the image reduces their individual weight. Thus, the image is correctly assigned a higher OA score without any misjudgment.

Regarding the diversity perspectives score, we focus on the multidimensional evaluation of images, which allows us to prioritize "interesting" images. For instance, the third image on the right, a comic, and the second image, a meme, receive high scores for emphasizing specific cultural elements. The photo on the far right earns the highest score due to its aesthetic value, while the screenshot on the far left, primarily composed of text, is assigned a low score due to its lack of visual information.

\section{Evaluation Detail}
\label{sec:evaluation_appendix}

In this section, we provide additional information and details regarding the evaluation benchmarks.
VQAv2 is a widely adopted open-domain visual question answering dataset, consisting of over one million question-answer pairs. For our evaluation, we sample a subset of 10,000 questions. OKVQA is designed to assess models' ability to answer visual questions by leveraging external knowledge sources. Likewise, TextVQA includes over 45,000 text-centric questions based on more than 28,000 images extracted from selected categories of the OpenImages dataset.
To evaluate advanced multimodal reasoning capabilities, MMBench (V1.1) provides a diverse set of 4876 multiple-choice questions compiled from various sources, spanning 20 distinct capability dimensions. MME-RealWorld (MME-RW) serves as a benchmark tailored to real-world applications, focusing on practical scenarios; we utilize its lite version, which consists of 1919 of the most challenging questions. SEED-Bench (SEED) comprises 14232 carefully annotated multiple-choice questions, covering 12 key evaluation dimensions. Lastly, MMMU consists of 1050 challenging questions across a wide range of interdisciplinary tasks, requiring college-level subject knowledge and advanced reasoning skills. For each dataset, we follow the answer-matching strategy recommended in its original paper and compute accuracy with a widely used open-source evaluation toolkit~\citep{DBLP:conf/mm/DuanYQFCLDZZWL024}.


\input{Figures/prompt}