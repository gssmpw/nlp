\begin{table*}[ht!]
\centering
\small
{
\setlength{\tabcolsep}{4pt}
	\begin{tabular}{l|c|ccccccc}
		\toprule
        \textbf{Models}                 &\textbf{Data Scale}      &\multicolumn{1}{c}{OKVQA}    &\multicolumn{1}{c}{VQAv2} &\multicolumn{1}{c}{TextVQA} &\multicolumn{1}{c}{MMBench} &\multicolumn{1}{c}{MME-RW} &\multicolumn{1}{c}{SEED} &\multicolumn{1}{c}{MMMU} \\ 
        \cline{1-9}
        InternVL2-2B                    &>5M        &41.2	          &75.9	          &72.0           &70.9	          &37.8	          &71.8	          &33.0     \\
        InternVL2.5-2B                  &>8M        &39.2	          &\textbf{77.8}  &71.9           &\textbf{79.0}  &34.6	          &\textbf{72.8}  &40.3       \\
        \cdashline{1-9}
        Qwen2-VL-2B                     &>5M        &44.3	          &71.5	          &77.6           &74.6	          &31.2	          &68.2	          &33.6         \\
        Qwen2-VL-2B-AC (ours)           &80K        &\textbf{48.4}	  &73.3	          &\textbf{81.4}  &73.0	          &\textbf{39.1}  &69.4	          &\textbf{42.2}         \\
        \hline\hline 
        Llama3.2-vision-11B             &>3M        &23.6	          &72.2	          &56.5	          &65.8           &38.5           &72.7           &48.1       \\
        Llava-Onevision-7B              &>1.6M      &\textbf{61.6}	  &82.5           &80.3	          &80.9           &48.6           &75.4         &47.9       \\
        InternVL2-8B                    &>5M        &46.4	          &79.2	          &77.9	          &79.4           &37.4          &75.4           &51.2       \\
        InternVL2.5-8B                  &>8M        &54.4	          &80.7	          &80.9           &\textbf{82.5}  &46.4          &\textbf{76.8}   &\textbf{56.2}       \\
        \cdashline{1-9}
        Qwen2-VL-7B                     &>5M        &58.0	          &83.0	          &\textbf{85.4}  &81.0           &48.3           &75.0   &50.6       \\
        Qwen2-VL-7B-AC (ours)           &80K        &54.2	          &\textbf{83.6}  &84.3	          &80.7           &\textbf{49.4}  &75.2   &49.9         \\
        \hline\hline   
        Llama3.2-vision-90B             &>3M        &46.7	          &73.1	          &71.0           &84.3           &45.4	          &75.8	      &38.2           \\
        Llava-Onevision-72B             &>1.6M      &\textbf{66.3}	  &84.8	         &82.6	         &86.4	          &48.8	          &77.8	      &50.6       \\
        InternVL2-76B                   &>5M        &60.9	          &84.6	          &87.1	          &87.3	          &\textbf{49.1}  &76.5	      &52.2       \\
        InternVL2.5-78B                 &>8M        &57.4	          &85.2	          &86.3	          &\textbf{91.9}  &46.4	          &77.0       &\textbf{62.5}         \\
        \cdashline{1-9}
        Qwen2-VL-72B                    &>5M        &52.9	          &83.2	          &90.8	          &90.6	          &40.8	          &78.3	          &59.1       \\
        Qwen2-VL-72B + AC-lora (ours)   &80K        &58.0             &\textbf{90.5}   &\textbf{91.3}   &89.8	      &47.4	          &\textbf{79.8}  &60.6       \\
        \hline
	\end{tabular}
 }
    \caption{Main result of visual understanding task. The "Data Scale" refers to the instruction data scale reported in each respective paper. Models with more than 72B parameters use InT4 quantification. The best results in each task are highlighted in \textbf{bold}.}
    \label{tab:main result}
    \vspace{-0.1cm}
\end{table*}

