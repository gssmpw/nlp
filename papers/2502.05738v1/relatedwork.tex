\section{Related Work}
% -------------------------------------------------------------------------------
\subsection{Visual Question Answering}
Visual Question Answering (VQA) is an interdisciplinary task that combines computer vision and natural language processing. It requires models to answer questions based on visual content. Various approaches have been proposed to improve VQA performance through advanced attention mechanisms and neural architectures.

\begin{itemize}
    \item \textbf{Spatial Memory Network}: Employs a two-hop attention mechanism. The first hop aligns question words with image regions, capturing detailed local evidence. The second hop refines this evidence by considering the entire question embedding, enhancing prediction accuracy \cite{chen2018}.
    \item \textbf{BIDAF Model}: Utilizes a bi-directional attention mechanism to create query-aware context representations, capturing interactions between context and query \cite{seo2017}.
    \item \textbf{CNN for Text Representation}: Replaces RNNs with CNNs for text representation in VQA, demonstrating superior capability in capturing textual features \cite{noh2016}.
    \item \textbf{Structured Attentions}: Models visual attention as a multivariate distribution over a conditional random field (CRF) to better encode relationships between multiple image regions \cite{zhang2018}.
    \item \textbf{Inverse VQA (iVQA)}: Introduces the inverse VQA task, using question-ranking-based evaluation to diagnose model strengths and weaknesses \cite{gordon2018}.
\end{itemize}
% -------------------------------------------------------------------------------
\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{figure1.png} % 请确保图片文件名和路径正确
    \caption{The VQA model architecture consisting of (a) question feature extraction, (b) image feature extraction, (c) attention mechanism, and (d) feature fusion and classification modules.}
    \label{fig:framework}
\end{figure*}
% -------------------------------------------------------------------------------
\subsection{Image Captioning}
Image captioning generates descriptive textual information for images, requiring models to understand and describe visual content accurately.

\begin{itemize}
    \item \textbf{Show, Attend and Tell}: Integrates CNNs with LSTMs and uses attention mechanisms to focus on relevant image regions for accurate captions \cite{xu2015}.
    \item \textbf{Self-Critical Sequence Training (SCST)}: Uses the REINFORCE algorithm for reinforcement learning, optimizing the CIDEr metric to reduce exposure bias \cite{rennie2017}.
    \item \textbf{Meshed-Memory Transformer (M2)}: Employs multi-level encoding and memory-augmented attention for improved caption generation \cite{cornia2020}.
    \item \textbf{X-Linear Attention Networks (X-LAN)}: Captures second-order interactions using bilinear pooling, enhancing feature representation \cite{pan2020}.
\end{itemize}
% -------------------------------------------------------------------------------
\subsection{Multi-Modal}
Multimodal research focuses on models that integrate information from multiple modalities, such as text, images, and audio, to perform complex tasks.

\begin{itemize}
    \item \textbf{LayoutLMv2}: Integrates text, layout, and image information through a multi-modal Transformer architecture, enhancing document understanding \cite{xu2020}.
    \item \textbf{Cross-Modal Context for Image Captioning}: Combines textual and visual contextual information using CLIP and Visual Genome datasets \cite{mokady2021}.
    \item \textbf{Transformer-Based Multi-Modal Proposal and Re-Rank}: Uses CLIP and XLM-RoBERTa for image-caption matching through a multi-modal approach \cite{li2021}.
\end{itemize}

% ===============================================================================