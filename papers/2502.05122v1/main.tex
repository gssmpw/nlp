

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{multirow}
\usepackage{enumitem}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Causal Velocity Models}

\input{defs}

\begin{document}

\twocolumn[
\icmltitle{Distinguishing Cause from Effect with Causal Velocity Models}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Johnny Xi}{ubc}
\icmlauthor{Hugh Dance}{gatsby}
\icmlauthor{Peter Orbanz}{gatsby}
\icmlauthor{Benjamin Bloem-Reddy}{ubc}
\end{icmlauthorlist}

\icmlaffiliation{ubc}{Department of Statistics, University of British Columbia}
\icmlaffiliation{gatsby}{Gatsby Unit, University College London}

\icmlcorrespondingauthor{Johnny Xi}{johnny.xi@stat.ubc.ca}

\icmlkeywords{}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution} %

\begin{abstract}

Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a \emph{causal velocity} by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated nonparametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location-scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model nonâ€“identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation.











\end{abstract}

\section{Introduction}
\label{sec:intro}

Distinguishing cause from effect from purely observational data is a challenging task. It is generally impossible to distinguish a causal direction $X \to Y$ from an anti-causal direction $Y\to X$ without intervention, as the corresponding causal graphs are Markov equivalent. To make causal discovery possible, one common approach is to make functional assumptions, for example, non-linear additive noise \citep{hoyer2008nonlinear, zhang2009identifiability} or location-scale noise \citep{xu2022heterscedastic,strobl2023identifying, immer2023identifiability} on the underlying causal process, and decide on the causal direction based on model fit and/or complexity in both candidate directions. 

Likelihood-based approaches seem natural here, as they can be used both for model estimation and evaluation. However, they require the full specification of both the mechanism and the noise distribution. This is undesirable due to the risks of model misspecification, particularly with respect to the noise distribution, which is not directly involved in causality, but can still lead to incorrect causal inference \citep{schultheiss2023pitfalls}. To avoid that risk, the most prominent example of a functional method is to fit an additive noise model (ANM) using nonparametric regression, which avoids needing to model the noise distribution \citep{mooij2009regression, peters2014causal}. Instead of a likelihood-based measure, an independence score \citep[e.g., HSIC,][]{gretton2005measuring} between the estimated residuals and cause variable can be used to determine goodness-of-fit \citep{mooij2009regression, peters2014causal}. This approach has been extended to location-scale models (LSNM) by \citet{strobl2023identifying, immer2023identifiability}. 
Separately, \citet{rolland2022score, montagna2023scalable} derived other goodness-of-fit criteria for ANMs that avoid structural model estimation altogether. Instead, those methods extract the signal directly from estimates of the score function $\nabla \log p(x, y)$ under the assumption of Gaussian noise. Subsequent work used score estimation in conjunction with explicit model estimation in the case of non-Gaussian noise \citep{montagna2023causal}. However, we are unaware of any work on functional causal discovery methods beyond the ANM and LSNM classes, which may still be misspecified in many settings. 

In this work, we develop a new framework that treats bivariate causal models as dynamical systems, and we use the framework to devise a new estimation procedure based on the score function, applying it to causal discovery. Specifically, we parametrize a causal model via its \emph{causal velocity}, which describes infinitesimal counterfactuals and can be directly recovered from the score in a simulation-free way analogous to the recent literature on flow-based generative modeling \citep{lipman2023flow, albergo2023building}. Assuming that the marginal and joint score functions can be estimated accurately, our proposed method combines the advantages of various state-of-the-art methods in that it is agnostic to the noise distribution and avoids explicit estimation of the functional model, even for non-Gaussian and non-additive cases. Beyond this, the velocity parametrization allows us to specify model classes that extend beyond ANM and LSNM in terms of dependence on the noise, but are still restrictive enough to be useful for causal discovery. 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{conceptual_fig.pdf}
    \vspace{-20pt}
    \caption{Given a bijective SCM $X \to Y$ and an observation $(x,y)$, counterfactual prediction is made by identifying the noise $\epsilon$ that generated the observation and moving along the \textbf{causal curve}. In this paper, we show that the \textbf{causal velocity} $v(y,x)$, which measures the counterfactual effect of an infinitesimal intervention, fully characterizes the SCM mechanism and can be estimated implicitly via estimating the score function.}
    \label{fig:conceptual_fig}
    \vspace{-10pt}
\end{figure}



\textbf{Contributions\ \ } We present a new perspective on invertible SCMs as dynamical systems through their implied velocity functions. In particular, counterfactual prediction becomes the solution to an initial value problem in which the factual observation is the initial condition. This allows us to specify novel causal models. It also allows us to use established ideas from measure transport to connect the causal velocity to the score functions of the data distribution. We use this to devise a novel simulation-free estimation procedure and goodness-of-fit criterion for bivariate causal discovery. 

To support the methodological contributions, we also examine the causal direction identifiability problem from the dynamical perspective, where we obtain a general result that specializes to those existing in the literature. We also establish statistical consistency of our estimation procedure, and show that the rate of convergence is dictated by the rate at which the score estimators converge. 
We show through synthetic and benchmark experiments that the added model flexibility is beneficial in situations where ANM and LSNM models can fail, but simple parametric velocity families recover the correct causal direction.\footnote{Code supporting our experiments can be found on Github at \href{https://github.com/xijohnny/causal-velocity}{https://github.com/xijohnny/causal-velocity}.} 




The appealing properties of our method depend on an accurate estimate of the score function at the data. The nonparametric score estimators that we use have known consistency properties and convergence rates \citep{zhou2020nonparametric}, which justifies the method asymptotically. However, some distributions, including those seen in common benchmarks, may not have favourable finite-sample behaviour even in the low-dimensional regime. We describe and probe failure modes of our method by way of poor score estimation. %

\textbf{Outline\ \ } In \cref{sec:background} we cover the necessary background on bivariate causal discovery and on dynamical systems. \cref{sec:dynamical} describes our framework of viewing SCMs as dynamical systems and the causal velocity parametrization. In \cref{sec:gof}, we use the continuity equation to derive an identity that can be used to recover the causal velocity from the score function. \cref{sec:velocity:discovery} applies this to the causal discovery problem and discusses identifiability. In \cref{sec:asymptotics}, we discuss consistency of the score estimator and apply these results to our setting. Finally, \cref{sec:related} and \ref{sec:experiments} describe related work and experimental evaluations.











\section{Background}
\label{sec:background}

\begin{table*}[t]
\centering
\vspace{-6pt}
\caption{Summary of existing and novel SCMs and their velocities. The velocity perspective allows for specification of novel model classes and can also be used as an alternative means of estimating existing model classes.}\label{tab:examples}
\resizebox{\textwidth}{!}{%
    \begin{tabular}{c|cccc}
    \toprule
     \shortstack{\textbf{Model} \vspace{1.0em}} &  \shortstack{\vspace{-0.4em}\textbf{SCM} \\ \\ $ f(X, \epsilon_y)$} & \shortstack{\vspace{-0.4em}\textbf{Counterfactual} \\ \\ $f_{x'}(f_x^{-1}(y))$} & \shortstack{\vspace{-0.6em}\textbf{Velocity} \\ \\ $(d/d{x'})f_{x'}(f_x^{-1}(y)) |_{x' = x} $} & \shortstack{\textbf{Parameters} \vspace{1.0em}} \\ \midrule
    ANM & $m(X) + \epsilon_y$& $y + m(x') - m(x)$ & $\dot m(x)$ & $\dot m: \bbR \to \bbR$  \\ 
    PNL & $g(m(X) + \epsilon_y)$ & $g(g^{-1}(y + m(x') - m(x))$ & $\dot g(g^{-1}(y))\dot m(x)$ & $\dot m, \dot g: \bbR \to \bbR$\\ 
    LSNM & $m(X) + e^{h(X)} \epsilon_y$ & $m(x') + e^{h(x')-h(x)}(y - m(x))$ & $\dot m (x) + \dot h(x)(y - m(x))$ & $\dot m, \dot h: \bbR \to \bbR$  \\ 
    Basis & $\epsilon_y + \smallint_{x_0}^{x} a^\top \Phi(y(u), u) du$ & $y + \smallint_{x}^{x'} a^\top \Phi(y(u),u)du$ & $a^\top \Phi(y,x)$ & $a \in \bbR^{K}$ \\ 
    Black box & $\epsilon_y$ + $\smallint_{x_0}^{x} v(y(u), u) du$ & $y +\smallint_{x}^{x'} v(y(u), u) du$ & $v(y,x)$ & $v: \bbR^2 \to \bbR$ \\ 
    \bottomrule
    \end{tabular}
    }
    \vspace{-10pt}
\end{table*}

\subsection{Structural Causal Models}

A structural causal model (SCM) for $X$ causing $Y$ is typically defined as the tuple $(f, P_{X}, P_{\epsilon_y})$,
\begin{align} \label{eq:SCM:def}
    Y = f(X, \epsilon_y), \quad X \condind \epsilon_y  \;, 
\end{align}
with $X \sim P_{X}$ and $\epsilon_y\sim P_{\epsilon_y}$. Throughout, we will assume that $f_x(\argdot) := f(x, \argdot)$ is bijective for each $x$, and denote $f^{-1}_x(\argdot)$ as its inverse. The pair $(f_{x}, P_{\epsilon_y})$ represents the conditional distribution of $Y | X = x$, and $P_X$ completes the specification of the joint distribution. The representation of a conditional distribution as a function of the conditioning variable and independent noise is always possible \citep[][Lem.\ 4.22]{Kallenberg_2021}, and it can be chosen to be bijective in the noise if, for each $x$, $Y|X=x$ is atomless.



Bijectivity enables identification of counterfactuals. In particular, let $(x, y(x)) = (x, f_x(\epsilon_y))$ denote a realization of the SCM. Assuming no hidden variables, the counterfactual realization ``had $x$ been $x'$'' is computed by the abduction-action-prediction procedure \citep{pearl2009causality} as $(x', f_{x'}\circ f^{-1}_x(y(x))) = (x', f_{x'}(x', \epsilon_y)):= (x', y(x'))$. Without bijectivity, $\epsilon_y$ must be inferred by, for example, estimating the posterior distribution of $\epsilon_y | x, y(x)$. A bijective SCM allows us to write the counterfactual directly as a deterministic transformation:
\begin{align}
    y(x) \mapsto y(x') = f_{x'} \circ f_{x}^{-1} (y(x)).\label{eq:counterfactual_transformation}
\end{align}
\citet{nasr2023counterfactual} established the identifiability in various settings of general bijective SCMs, with $y(x')$ given by \eqref{eq:counterfactual_transformation}. Applying this transformation for all possible $x'$ yields a curve of counterfactual outcomes, see \cref{fig:conceptual_fig} for a visual description, and \cref{tab:examples} for examples common in causal discovery. %





\subsection{Functional Bivariate Causal Discovery} 
\label{sec:bg:discovery}

Causal discovery aims to determine, from pairs of observed data $(X,Y)$, whether $X$ or $Y$ is the cause. This problem is underdetermined when using purely observational data, due to the universality of the SCM representation of conditional distributions. 
One common approach, based on the argument that the data-generating process in the causal direction is simpler \citep{mooij2016distinguishing}, is to restrict the model class. It is hoped that in doing so, the model will fit the causal direction but exclude the anti-causal direction \citep{goudet2019learning}. 
Because the distribution of the noise variables is not related to causality \emph{per se}, most attention is paid to modeling the mechanism. 
ANMs are appealing because they allow the noise distribution to be unspecified, with methods that involve quantifying model goodness-of-fit based on a model estimated with nonparametric regression; see \citet{mooij2016distinguishing} and references therein. 
For LSNMs, two-step nonparametric regression is also possible \citep{strobl2023identifying}. PNL models post-process ANM model estimates with non-linear ICA \citep{zhang2009identifiability} or rank-based approaches \citep{keropyan2023rank}. A key takeaway from this literature is that we should aim for methods that allow for flexible---though not infinitely so---model classes that allow the noise distribution to be unspecified. 



\subsection{Flows and Differential Equations}
\label{sec:bg:flows}

The subsequent sections will expose connections between SCMs and dynamical systems generated by differential equations, so we briefly review some relevant results about the latter here. Our exposition largely follows \citet[][Appendix B]{Arnold1998} and \citet[Ch.\ 4.1.2]{Santambrogio2015}. 
Consider the ordinary differential equation (ODE) in $\bbR^d$, 
\begin{align} \label{eq:basic:ode}
    \frac{d y(t)}{d t} = v(y(t),t) \;, \quad t \in \bbR \;.
\end{align}
Under regularity conditions, ODEs are known to be in correspondence with \emph{two-parameter flows}, defined as follows.
\begin{definition}%
    \label{def:two:p:flow}
    A two-parameter flow (or flow) $\varphi_{s,t}$ on $\bbR^d$ is a continuous mapping 
    \begin{align*}
        (s,t,y) \mapsto \varphi_{s,t}(y), \quad s,t, y \in \bbR \times \bbR \times \bbR^d,
    \end{align*}
    that satisfies the flow properties
    \begin{enumerate}[itemsep=0pt,topsep=0pt]
        \item $\varphi_{x,x}(y) = \text{id}(y) = y$, for all $x, y$.
        \item $\varphi_{s,t} \circ \varphi_{t,u} = \varphi_{s,u}$ for all $s,t,u \in \bbR$.
    \end{enumerate}
\end{definition}

We note that $\varphi_{s,t}^{-1} = \varphi_{t,s}$. 
The ODE \eqref{eq:basic:ode} is said to \emph{generate} the flow $\varphi_{s,t}$ if
\begin{align} \label{eq:ode:flow:def}
    \varphi_{s,t}(y) = y + \int_{s}^t v(\varphi_{s,u}(y), u) \ du \;, \quad t \in \bbR \;.
\end{align}
If $\varphi_{s,t}$ is differentiable in $t$ and satisfies
\begin{align} \label{eq:classical:sol}
    \frac{d}{dt} \varphi_{s,t}(y) = v(\varphi_{s,t}(y),t) \quad \text{and} \quad \varphi_{s,s}(y) = y\;,
\end{align}
then $\varphi_{s,t}$ is said to be a (classical) \emph{solution} of \eqref{eq:ode:flow:def}. Conversely, a flow can be used to define an ODE. If $t\mapsto \varphi_{s,t}(y)$ is differentiable in $t$ at $t = s$ for all $y$, then
\begin{align} \label{eq:ode:from:flow}
    v(y,s) := \frac{d}{dt}\varphi_{s,t}(y)\big|_{t=s}
\end{align}
yields an ODE to which $\varphi_{s,t}$ is a solution, and $v$ is known as the \emph{velocity}. 
Under regularity conditions (\cref{appx:flows}), velocities and flows are in one-to-one correspondence. 



Now suppose that an initial condition for the ODE \eqref{eq:basic:ode} is chosen randomly as $Y(s) \sim p_s$, where $p_s$ is the density of a probability distribution on $\bbR^d$. Letting $Y(s)$ evolve according to the ODE yields $Y(t) = \varphi_{s,t}(Y(s))$. Viewing the flow as a measure transport, the density of $Y(t)$ is $p_t = (\varphi_{s,t})_* p_s$. It can be shown that the family of densities $(p_t)_{t \in \bbR}$ solves the PDE (known as the \emph{continuity equation})
\begin{align} \label{eq:continuity:eqn}
    \partial_t p_t + \nabla_y \cdot (p_t v_t) = 0 \;,
\end{align}
with $v_t := v(\argdot,t)$ and $\partial_t = \partial/\partial t$. 
It turns out that the solution is unique up to (Lebesgue) almost everywhere equivalence. (See \cref{appx:flows} for a formal statement.) 
Note that if $p_t$ is strictly positive then \eqref{eq:continuity:eqn} is equivalent to
\begin{align} \label{eq:log:continuity}
    \partial_t \log p_t = - \nabla \cdot v_t - v_t \cdot \nabla_y \log p_t \;.
\end{align}





 \section{SCMs as Dynamical Systems} \label{sec:dynamical}

  
Generally, flows describe the time evolution of dynamical systems: the usual statement of \cref{def:two:p:flow} yields the evolution of $y$ between time points, and causality is associated with the forward flow of time. Here, we take a different perspective, viewing counterfactual transformation as a flow in which the role of time is played by a cause variable $x$. Whilst a similar connection has been made in previous work \citep{dance2024causal}, by specializing to the bivariate real valued case and viewing the cause explicitly as time, we are able to make a direct connection to ODEs. This enables us to derive a novel velocity-based parametrization of SCMs and simulation-free learning procedure. For the remainder of the paper, we work in the setting of $X \in \bbR$ and $Y \in \bbR$. 


\begin{definition}
    Let $X$ cause $Y$, with bijective SCM \eqref{eq:SCM:def}. 
    The flow generated by the SCM, or \emph{SCM flow}, is defined as
    \begin{align}
    (x, x', y) \mapsto f_{x'}(f_x^{-1}(y)) =: \varphi_{x,x'}(y) \;. \label{eq:scm_flow}
    \end{align}
\end{definition}

It is easy to see that \cref{eq:scm_flow} satisfies the axioms of a flow \cref{def:two:p:flow}. In particular, for an observation $(x, y)$, the \emph{causal curve} is the function of $x'$ defined by
\begin{align}
    x' \mapsto y(x') = \varphi_{x,x'}(y) \;.
\end{align}
Causal curves represent the counterfactual outcomes under the SCM from a factual observation $(x, y)$. Generally, each observation specifies a different causal curve. 
Based on the dynamical systems perspective reviewed in \cref{sec:bg:flows}, $(x,y)$ can be viewed as initial conditions.

In analogy to \eqref{eq:ode:from:flow}, if $(x,x',y)\mapsto \varphi_{x,x'}(y)$ is continuous and $x \mapsto f_x(\epsilon_y)$ is differentiable for all $\epsilon_y$ then the  \emph{causal velocity} of the SCM flow is 
\begin{align} \label{eqn:generator}
    v(y,x) = \frac{d}{dx'} f_{x'}(f_{x}^{-1}(y)) \big|_{x' = x} \;.
\end{align}
This holds for all $(x,y)$ in the support of $P_{X,Y}$. 
The resulting $v$ describes the local behaviour of the flow at $x$: $v(y,x)$ is the effect of an infinitesimal intervention $x' = x + \delta$ as $\delta \to 0$, which, based on the algebraic properties of the flow, generates the entire counterfactual curve. 
Thus, we see that the mechanism of a bijective SCM uniquely determines a flow and its associated velocity. %

Conversely, the causal mechanism of a bijective SCM can be parametrized by a velocity $v$ via its corresponding flow from an arbitrary $x_0 \in \bbR$ acting as the noise state, 
\begin{align} \label{eq:flow:from:v}
    \varphi_{x_0,x}(y) = y + \int_{x_0}^{x} v(\varphi_{x_0,u}(y), u) \ du \;. 
\end{align}
To complete the specification of the SCM, the density of the observational noise is required, here $p(y|x_0)$, so that
\begin{align*}
    f(X,\epsilon_y) = \varphi_{x_0,X}(\epsilon_y) \;, \quad \epsilon_y \sim p(y|x_0) \;.
\end{align*}
Under regularity conditions, the SCM is unique and holds over all of $\bbR^2$. Technically, an SCM in the sense of \cref{sec:background} also requires $P_X$ to fully specify the joint distribution but it plays no role in the causal mechanism or counterfactuals. The following theorem formalizes the relationship between bijective SCMs and dynamical systems. See \cref{appx:flows} for technical details on the regularity conditions.

\begin{theorem} \label{thm:scm:velocity}
    Let $X$ cause $Y$, with $X,Y \in \bbR$. 
    Under regularity conditions, a bijective SCM uniquely determines a velocity-density pair $(v, p(y|x_0))$. Conversely, $(v, p(y|x_0))$ determines a SCM uniquely up to changes in $P_X$.  
\end{theorem}

The above equivalence shows that we can view bijective SCMs in terms of their underlying velocity functions without loss of generality. See \cref{tab:examples} for the velocity functions associated with familiar classes of SCMs. 

\begin{figure*}
    \centering
\includegraphics[width=\linewidth]{parametric_fig.pdf}
    \vspace{-20pt}
    \caption{Velocities and example realizations of counterfactual curves for various models. 
    The velocity of an ANM is constant over $y$ and thus result in parallel curves. LSNM curves can spread out, but must do so equally at each point in $x$. No matter how flexible $m$ and $h$ are, ANM and LSNMs cannot represent the mechanism generated by a quadratic velocity function, which can be parametrized with a single parameter. Finally, existing model classes such as LSNM can be extended by combining velocities.}
    \label{fig:parametrization}
    \vspace{-10pt}
\end{figure*}


\subsection{Velocity Parametrization of SCMs}

The dynamical perspective allows a causal model to be specified and interpreted by the velocity and its implied counterfactuals. By examining the velocity and causal curves (\cref{fig:parametrization}), we see that ANMs imply that all trajectories remain parallel, indicating that the effect of an intervention affects individuals with different factual conditions equally. LSNMs relax this, but still implies that for any intervention, the difference between individual trajectories are proportional (with a factor of $e^{h(x') - h(x)}$). On the other hand, a quadratic velocity extends these classes, which models a transition depending on whether the factual is $y > 0$ or $y < 0$ (\cref{fig:parametrization}). In practice, the velocity class can be specified in an interpretable way via domain knowledge of the underlying process, or with basis functions or neural networks for more flexible models. The velocity parametrization automatically specify bijective SCMs, and as such their counterfactuals are identifiable \citep{nasr2023counterfactual}. 


























\section{Scores and SCM Flows}
\label{sec:gof}


In the previous sections, we established an equivalence between SCMs and an associated pair $(v,p(y|x_0))$. Here, we show that when a joint distribution is generated by an SCM, the velocity leaves an explicit signature on its score function. We leverage this observation to derive a goodness-of-fit criterion based on the score function, which can then be used for model fitting and checking based directly on the velocity. Remarkably, this allows us to check whether data can possibly be generated from an SCM with velocity $v$ without ever having to evaluate the mechanism, nor making any modeling assumptions about the underlying distributions. 

Let $v$ be the velocity of a SCM that generates the conditional distribution $P_{Y|X}$, and assume that the joint distribution has full support, with differentiable marginal and joint log-densities. Let $s_x(y|x) := \partial_x \log p(y|x)$ and $s_y(y|x) := \partial_y \log p(y|x)$ denote the partial derivatives of the log conditional density (i.e., scores), and similarly $s_x(x,y)$ and $s_x(x)$ the partial derivatives of the log joint and marginal densities, respectively. 
Treating $x$ as time, the $\log$ version of the continuity equation \eqref{eq:log:continuity} yields 
\begin{align} \label{eq:cond:score:continuity}
    s_x(y|x) = - \partial_y v(y, x) - v(y,x) s_y(y|x) \;.
\end{align}
By known results (see \cref{appx:flows}), every solution $p(y|x)$ arises from some initial condition density $p(y|x_0)$ transported by the SCM flow, for arbitrary $x_0 \in \bbR$. Moreover, that solution is unique. Therefore, \eqref{eq:cond:score:continuity} can be used to characterize when a conditional density $p(y|x)$ could possibly have been generated by a SCM with velocity $v$. For practical purposes, \eqref{eq:cond:score:continuity} can also be stated in terms of marginal and joint scores, which can be estimated from data.

\begin{theorem} \label{thm:id}
    Let $X$ cause $Y$, with $X,Y \in \bbR$. Assume that $P_{X,Y}$ has full support, with differentiable joint, conditional, and marginal log densities. Then
    \begin{align} \label{eq:joint:id}
        s_x(x,y) = -\partial_y v(y,x) - v(y,x) s_y(x,y) + s_x(x)
    \end{align}
    for all $(x,y) \in \bbR^2$ if and only if $P_{X,Y}$ can be represented by a SCM with velocity $v$ and marginal density $p(x)$. 
\end{theorem}

Observe that \eqref{eq:joint:id} is entirely in terms of observable scores, and thus agnostic to the unobserved noise distribution $P_{\epsilon_y}$. Indeed, SCMs with the same mechanism but different noise distributions will still satisfy \eqref{eq:joint:id} with their respective scores. This makes it an especially suitable objective for functional causal discovery, which asks whether distributions can be generated by a restricted mechanism class, while leaving the noise distributions unspecified. 
















\section{Velocity-Based Causal Discovery}
\label{sec:velocity:discovery}

For methods that resolve the direction of causality by fitting a model to the data in both candidate directions, some restriction to model complexity must be made. 
As discussed in \cref{sec:bg:discovery}, most existing methods assume an ANM in order to avoid making specific distributional assumptions about the unobserved noise. 
Our velocity-based method, described in this section, makes no assumptions on the noise distribution, capturing its influence via score estimation, and allows all modeling and complexity control to be imposed via the parametrization of the velocity. As established in previous sections, this allows the model to be substantially more flexible than ANMs or LSNMs. 

The ability of \eqref{eq:joint:id} to decouple noise from mechanism motivates a two-step approach to velocity-based causal modeling. First, we estimate the joint and marginal scores using nonparametric estimators \citep{zhou2020nonparametric}; second, we impute the velocity from the estimated score by minimizing 
\begin{align}
    \label{eqn:loss}
    \bfL(v) := \bbE[
    \left(s_x(X) - \partial_y v(Y, X) - s_{v}(X,Y)\right)^2
    ],
\end{align}
which is derived from \eqref{eq:joint:id} and uses the notation
\begin{align}
    \label{eqn:directionaldx}
    s_{v}(x,y) := s_{x}(x, y) + v(y, x) s_{y}(x, y) \;.
\end{align}
This notation reflects the fact that $s_v$ is  the directional derivative of $\log p(x,y)$ along the causal curve, i.e., in the direction $\partial_{x}(x, y(x)) = (1, v(y(x), x))$ (see  \cref{sec:density:curve}). 
The objective \eqref{eqn:loss} is zero if and only if \eqref{eq:joint:id} holds $P_{Y,X}$-almost everywhere. 
For the $Y \to X$ direction,
\begin{align}
    \label{eqn:loss:rev}
    \tilde{\bfL}(\tv) := \bbE[
    \left(s_y(Y) - \partial_x \tv(X, Y) - s_{\tv}(X, Y)\right)^2
    ] \;. 
\end{align}
In principle, once the velocity is estimated it could be integrated to a base point in order to estimate the noise variables, i.e., $\hat{\epsilon}_{y} = \hat{\varphi}_{X,x_0}(Y)$, which could then be used as residuals in independence tests. However, a more direct approach is to use the objective itself as a goodness-of-fit criterion. 
The resulting causal discovery method is as follows: 
\begin{enumerate}[itemsep=0pt,topsep=0pt]
    \item Estimate joint and marginal scores from data. 
    \item Impute causal velocity in both  directions by minimizing \eqref{eqn:loss}, \eqref{eqn:loss:rev}, with expectation taken over the data.
    \item Choose as causal whichever direction has a lower value of objectives \eqref{eqn:loss}, \eqref{eqn:loss:rev} evaluated on the data. 
\end{enumerate}

As might be expected, and as we demonstrate empirically in \cref{sec:experiments}, this method requires reasonably accurate estimates of the scores. 
Since \eqref{eq:joint:id} expresses a fixed and deterministic relationship between the scores and the velocity, an inaccurate score estimate will produce an inaccurate velocity that nonetheless yields a small value of \eqref{eqn:loss}, thereby potentially introducing error into subsequent tasks such as causal direction estimation. 
We show in \cref{sec:asymptotics} that the rate of convergence of the empirical estimator of \eqref{eqn:loss} is controlled by the rate at which the score estimator converges to the true score, thus we expect the method to perform well on problems in which the score can be estimated well. 


























\subsection{Identifiability}

\label{sec:id}






In the limit of infinite data, the causal direction of an ANM is known to be identifiable under certain conditions, formulated as the non-existence of a solution to a differential equation for $\log p(x)$ \citep{hoyer2008nonlinear,zhang2009identifiability}. For fixed ANM parameters $m, p_{\epsilon}$, the set of marginal densities that satisfy the differential equation (and hence are non-identifiable) is a three-dimensional space contained in an infinite-dimensional space, and therefore it is believed that ANMs can identify causal direction in ``most cases'' \citep{peters2014causal}. 
Identifiability in ANMs is the strongest known result, as the characterization does not depend simultaneously on the parameters of both model directions, and therefore holds uniformly over the model space. 

As argued by \citet{tagasovska20aBQCD,xu2022heterscedastic,strobl2023identifying,immer2023identifiability} and others, additive noise can be an overly restrictive assumption: even basic heteroscedastic noise (i.e., a LSNM) can lead ANM-based causal discovery procedures to mis-identify the correct causal direction. %
While LSNMs extend ANMs, even that straightforward generalization significantly complicates the identifiability analysis. In analogy to ANM identifiability, \citet{strobl2023identifying,immer2023identifiability} derived a partial differential equation (PDE) whose solutions characterize non-identifiability. In contrast to ANMs, the LSNM PDE depends simultaneously on the parameters of both model directions. Therefore, it is useful  on a case-by-case basis, but does not yield a characterization of non-identifiability uniformly over the model class. 
As we show in \cref{appx:id}, a similar characterization for general velocity models can be obtained by analyzing the continuity equation \eqref{eq:joint:id} in both model directions. 

\begin{proposition} \label{prop:velocity:id}
   Assume that the joint distribution of the observed data can be expressed as a causal velocity model in both directions,
    \begin{align*}
        Y & \equdist \varphi_{x_0, X}(\epsilon_y) \;, \quad X \condind \epsilon_y  \;, \\
        X & \equdist \tilde{\varphi}_{y_0, Y}(\epsilon_x) \;, \quad Y \condind \epsilon_x \;,
    \end{align*}
    with corresponding velocities $v$ and $\tv$, respectively. 
    Then, as long as $\pi(x,y) = \log p(x,y)$ and the required derivatives exist, the  velocities satisfy on the support of $p(x,y)$, 
    \begin{align*}
        \partial^2_y v + \partial_y(v\cdot \pi) = \partial^2_x \tv + \partial_x(\tv \cdot \pi) \;.
    \end{align*}
\end{proposition}

Expressions for the relevant partial derivatives of $\pi$ in terms of the forward model parameters $\varphi,v$ are given in \cref{appx:id}. 
We show in \cref{appx:anm:id} how the known ANM non-identifiability differential equation follows naturally from this characterization. In \cref{appx:lsnm:id}, we use it to derive a criterion for non-identifiability that holds uniformly over the class of LSNMs, albeit at a loss of interpretability. Thus, we expect that a general analysis of identifiability---if one is possible---will require new techniques. 




\section{Consistency of Loss Estimation}
\label{sec:asymptotics}

In this section we propose a nonparametric estimator for the loss \eqref{eqn:loss} and prove its consistency properties. Our two-step approach is to estimate first the marginal and joint score functions using nonparametric score-matching, and then estimate the objectives \eqref{eqn:loss}, \eqref{eqn:loss:rev} empirically using the available data samples $(x_i,y_i)_{i=1}^n$, yielding the objective
\begin{align}
    \hat {\bfL}_n(v) = \frac{1}{n} \sum_{i=1}^n (
        \left(\hat s_x(x_i) - \partial_y v(y_i, x_i) - \hat s_{v}(x_i,y_i)\right)^2 \label{eqn:loss_est}
\end{align}
Here, $\hat s_v(x_i,y_i) = v(y_i,x_i)\hat s_y(x_i,y_i) + \hat s_x(x_i,y_i)$ and $\hat s_x, \hat s_y$ are the score estimators, for which we use either the Stein estimator \cite{li2018gradient} or one based on a kernel density estimator \citep{wibisono24a_score}.
These estimators can both be viewed as doing regularized, vector-valued reproducing kernel Hilbert space (RKHS) regression on the true scores \cite{zhou2020nonparametric}. Based on this, the following result shows that our estimator for the loss function converges at a rate no worse than convergence rate of the score function estimators. In the following result we denote $s_{x,y}^{(x)}(x,y) := s_{x}(x,y)$, $s_{x,y}^{(y)}(x,y) := s_{y}(x,y)$ for clarity, and similarly for the estimators. %

\begin{theorem}\label{thm:consistency}
    Let $s_x \in \calH_{\calX}$ and  $s_{x,y}^{(x)},  s_{x,y}^{(y)} \in {\calH}_{\calX, \calY}$ where $\calH_{\calX}, \calH_{\calX, \calY}$ are RKHSs induced by bounded kernels $k_x < B, k_{x,y} = k_x \otimes k_y < B$. Additionally, let  $v \in C_b^1(\bbR^2,\bbR )$. Then, if the score estimators have bounded RKHS norm
    that converges, such that each of $\left \| \hat s_x - s_{x} \right \|_{\calH_{\calX}}$, $\left \| \hat s_{x,y}^{(x)} - s_{x,y}^{(x)}\right \|_{\calH_{\calX,\calY}}$, $\left \| \hat s_{x,y}^{(y)} - s_{x,y}^{(y)}\right \|_{\calH_{\calX,\calY}}$ is $\calO_p(n^{-\frac{1}{\alpha}})$ 
    for some $\alpha > 0$, then $ \hat\bfL_n(v) - \bfL(v) = \calO_p(n^{-\frac{1}{\beta}})$, for $\beta   = \max \{\alpha, 2\}$. 
\end{theorem}

\citet{zhou2020nonparametric} showed that under smoothness assumptions on $\log p(x,y), k_x, k_y$, $\alpha \in [3,4]$ can be achieved. 








\section{Related Work}
\label{sec:related}

\textbf{Score Matching ANMs\ \ } A recent line of work also use signatures in the score function for causal discovery in ANMs. \citep{rolland2022score, montagna2023scalable} use properties of Gaussian noise to derive conditions on the score that are satisfied by non-cause variables. \citep{montagna2023causal} show that even under arbitrary distributions the score is a deterministic function of the noise variables, and fit the model using nonparametric regression to evaluate this condition. These methods are restricted to the ANM case, but are focused on sink node (non-cause) identification in multivariate SCMs, while our focus is on more general model classes in the bivariate setting. Note the continuity equation \eqref{eq:cond:score:continuity} simplifies for ANMs, we show how to recover the identities used in this line of work in \cref{sec:appendix:scorebasedanm}.

\textbf{Cocycles in Causal Modeling\ \ }
Connections between causal models and flows of dynamical systems have been previously established by \citet{dance2024causal} in a more general setting, using a class of maps called cocycles. A two-parameter flow is an example of a cocycle. \citet{dance2024causal} focused on distribution-robust inference in a known causal graph with multiple cause variables, rather than bivariate discovery. Furthermore, they do not analyze the cocycle as a dynamical system nor make any connection to the score function.  

\textbf{Continuous Normalizing Flows\ \ } A popular class of velocity-based probabilistic model are  continuous normalizing flows (CNF) \citep{chen2018neural}. There, instead of conditioning, time is an auxiliary variable, and the generative model is for a marginal density $\varphi_{0,1}(y_0)$, where $y_0$ is drawn from an arbitrary base distribution. Our learning objective \eqref{eqn:loss_est}, which targets conditional distributions instead, is similar to recently proposed simulation-free objectives, which attempt to target the velocity directly in CNFs, e.g., Flow Matching \citep{lipman2023flow}. Normalizing flows have been used for causal inference \citep{khemakhem21a_carfl,javaloy2024causal} with a known causal graph. The causal auto-regressive flow model \citep{khemakhem21a_carfl} when restricted to the bivariate case represents a flexible LSNM, and was used for likelihood-based causal discovery.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{score_estim_fig.pdf}
    \vspace{-20pt}
    \caption{We test our velocity parametrization on a well-specified synthetic data generated from an LSNM with Gaussian noise. Top left: the ground truth velocity and causal curves. Top right: the estimated velocity when given the analytically computed ground truth scores. Bottom: estimated scores. Note that the curves from estimated scores tend to ignore variation in the tails (in the $y$ direction). On the other hand, estimating the velocity under the true score is very effective at recovering the underlying SCM.}
    \label{fig:score}
    \vspace{-5pt}
\end{figure}


\section{Experiments}
\label{sec:experiments}

\begin{table*}[bt]
    \centering
    \vspace{-8pt}
    \caption{Accuracy (and AUDRC) of velocity models on determining the correct causal direction, evaluated on synthetic data generated using different mechanisms (see main text). The model column indicates the velocity parametrization. The results shown here represent the best performance over the choice of kernel. For full tables of results, see \cref{sec:additional}.}
    \label{tab:synthetic}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|ccc|ccc|ccc}
    \toprule
    \multirow{2}{*}{\textbf{Model}} &  \multicolumn{3}{c}{Periodic} &  \multicolumn{3}{c}{Sigmoid} &  \multicolumn{3}{c}{Velocity} \\
    & \texttt{KDE} & \texttt{STEIN}  & \texttt{HYBRID}  & \texttt{KDE} & \texttt{STEIN}  & \texttt{HYBRID} & \texttt{KDE} & \texttt{STEIN}  & \texttt{HYBRID} \\
    \midrule
     \texttt{MLP}& 70 (84) & 98 (99) & \textbf{100 (100)}  & 48 (51) & 62 (68)& 70 (80) & 93 (99) & 90 (98) & 91 (99) \\
     \texttt{B-LIN}& 70 (90) &  98 (100) & \textbf{100 (100)} & 25 (35) & 71 (82) & 71 (85) & 91 (99) & \textbf{95 (100)} & 92 (99)\\
     \texttt{B-QUAD}& 91 (99) &  99 (100) & \textbf{100 (100)}  & \textbf{79 (93)} & 72 (87) & 72 (82) & 91 (99) & 95 (99) & 92 (99) \\
     \texttt{ANM}& 55 (60) & 97 (99) & 99 (100) & 15 (21) & 62 (68) & 67 (77) & 92 (99) & 81 (95) & 90 (98)\\
     \texttt{LSNM}& 60 (69) & 93 (99) & \textbf{100 (100)} & 42 (34) & 64 (68) & 67 (75)& 88 (98) & 82 (96)& 91 (99) \\
     \midrule 
     LOCI & \multicolumn{3}{c|}{86 (95)} & \multicolumn{3}{c|}{50 (72)} & \multicolumn{3}{c}{61 (87)} \\
     \bottomrule
\end{tabular}}
\vspace{-10pt}
\end{table*}


We evaluate our method empirically on new synthetic datasets as well as existing benchmarks from the literature \citep{mooij2016distinguishing, tagasovska20aBQCD,immer2023identifiability}. We study the relative performance of different model classes, including velocity-based parametrizations of known classes such as ANM. We also examine how different nonparametric score estimators \citep{li2018gradient,wibisono24a_score} affect performance. 

We compare our method to LOCI, an LSNM-based model which represents the state-of-the-art based on the results reported by \citet{immer2023identifiability}. For our new synthetic settings we use the original author implementation, while for benchmarks we simply report results from the original paper. In our synthetic settings, we show that velocity-based estimation is able to recover causal direction consistently, even under mild misspecification, where the existing LSNM-based methods can fail (\cref{tab:synthetic}). On both synthetic and benchmark data \citep{mooij2016distinguishing, tagasovska20aBQCD}, we find that our novel parametrizations, despite requiring less than 10 parameters, consistently perform better than more complex counterparts and even achieve state-of-the-art performance on the SIM-G benchmark (\cref{tab:benchmark}). 




\begin{table*}[bt]
    \centering
     \caption{Accuracy (and AUDRC) of velocity models on benchmark data. Results shown represent the best performance over the choice of score estimation and kernel as in \cref{tab:synthetic}. For full tables of results, see \cref{sec:additional}. We report the best performing version of LOCI here, more specific performance of other methods on these benchmarks is well-documented in \citet{mooij2016distinguishing} and \citet{immer2023identifiability}.}
    \label{tab:benchmark}
    \resizebox{\textwidth}{!}{
\begin{tabular}{c|cccccccc}
    \toprule
    {\textbf{Model}} & AN & AN-s & LS & LS-s & MNU & SIM & SIM-G & Tue\\
    \midrule
     \texttt{MLP}& \textbf{98 (100)} & 93 (99) & \textbf{97 (100)}  & 93 (99) & 81 (94) & 54 (61) & 78 (92) & 58 (68)  \\
     \texttt{B-LIN}& 70 (82) & 98 (100)  & 67 (88) & 96 (100)  & \textbf{77 (88)} & 58 (58) & 74 (78) & \textbf{69 (70)} \\
     \texttt{B-QUAD}& 92 (98) & \textbf{100 (100)}  & 95 (99)  & \textbf{97 (100)} & \textbf{100 (100)} & 63 (67)  & \textbf{96 (99)}  & 59 (68) \\
     \texttt{ANM}& 87 (98) & 89 (97) & 95 (99) & 86 (95) & 68 (75) & \textbf{67 (75)} & 82 (93) &  64 (68)\\
     \texttt{LSNM}& \textbf{98 (100)} & 94 (100) & 94 (100) & \textbf{97 (100)} & 80 (93) & 66 (70) & 84 (95)  & 59 (68) \\
     \midrule 
     LOCI  & 100 (100) & 100 (100) & 100 (100) & 100 (100) & 100 (100) & 79 (89) & 81 (91) &60 (56) \\ 
     \bottomrule
\end{tabular}}
\vspace{-8pt}
\end{table*}


\textbf{Velocity Estimation\ \ }
We first consider novel parameterizations of the velocity: a black-box 2-layer MLP (\texttt{MLP}), and linear/quadratic basis expansions (\texttt{B-LIN}/\texttt{B-QUAD}). In addition, we also experiment by fitting ANM and LSNMs via their velocity parameterizations directly. See \cref{tab:examples} and \cref{appx:experiment} for details. 
Given an estimate of the score, the velocity is estimated by minimizing \eqref{eqn:loss_est} with the Adam optimizer. The term $\partial_y v(y, x)$ is handled by automatic differentiation.  The datasets we consider are relatively small ($n < 10^5$), and thus we can take full batch gradient steps. To mitigate potential non-identifiability and regularize estimation, we penalize the complexity of the model via higher order terms $d^ky(x)/dx^k$ \citep{kelly2020learning} in the case where both directions are specified to encourage selecting the simpler model. In practice we use $k = 2$ so the penalty is $\lambda_c\ell(v)$, where we use $\lambda_c = 10^{-5}$, and 
\begin{align}
    \ell(v) = (\partial_x v(y, x) + v(y, x)\partial_y v(y, x))^2\;.
\end{align}


\textbf{Score Estimation\ \ } For nonparametric estimation of the score, we use the Stein gradient estimator of \citet{li2018gradient} (\texttt{STEIN}) as well as the score estimate from a standard kernel density estimator with empirical Bayes smoothing \citep{wibisono24a_score} (\texttt{KDE}). We also consider a hybrid approach where the marginal 1-d scores are estimated with KDE while the joint 2-d score is estimated with the Stein method (\texttt{HYBRID}). We use both Gaussian and exponential (Laplace) kernels with standard choices of bandwidth. 

\textbf{Evaluation\ \ } A causation score $C_{X \to Y}$ for the causal direction $X \to Y$ is obtained by comparing the empirical loss \eqref{eqn:loss_est} so that $C_{X \to Y} > 0$ indicates a better fit in the $X \to Y$ direction. Either the squared or absolute values can be used; here we use the absolute value which results in slightly better performance. We follow the literature and report the accuracy of inferring the causal direction, as well as the associated area under the decision rate curve (AUDRC). 


\subsection{Datasets}

\paragraph{Synthetic}
We design three synthetic datasets for which existing model classes are misspecified. Two are variations on LSNMs, where mechanisms are small neural networks. The first is a periodic noise SCM with mechanism $f(x, \epsilon_y) = a(x) + e^{b(x)} \sin (e^{c(x)} \epsilon_y)$. The second is similar to a post-nonlinear LSNM to which another affine transformation is further applied (see \cref{appx:experiment}). The last synthetic dataset is generated via numerical integration by specifying a velocity model with linear and exponential terms. We generate 100 datasets where the mechanism parameters are randomly sampled, each of $n = 2000$ observations. We specify all three with standard Gaussian noise, and where we scale the observational noise by a factor of $3$.



\textbf{Benchmark Data\ \ }
Following previous work \citep{tagasovska20aBQCD, immer2023identifiability}, we also evaluate our method on the \texttt{SIM}-series of simulated benchmarks and the T\"{u}bingen Cause-Effect pairs of \citet{mooij2016distinguishing}. We found that the SIM-ln and SIM-c variants have similar performance to SIM and thus we omit these results. We also evaluate on the five synthetic datasets from \citet{tagasovska20aBQCD} that correspond to various cases of LSNM. Note that LOCI is well-specified in these settings, and their performance here reflects that. 

\subsection{Probing Score Estimation}

The validity of our method requires accurate estimation of the score. Despite the consistency result in \cref{sec:asymptotics}, the finite sample implications on model estimation and causal discovery are unclear. As a toy example, we plug in the ground truth scores to \eqref{eqn:loss_est} which results in impressive performance (\cref{fig:score}). In contrast, the estimated scores recover high-density areas of the data but less accurate near the tails, which is unsurprising for KDE-type methods.



\section*{Acknowledgments}

This research was supported in part through computational resources and services provided by Advanced Research
Computing at the University of British Columbia. 
JX is supported by an Natural Sciences and Engineering Research Council of Canada
(NSERC) Canada Graduate Scholarship.
BBR acknowledges the support of NSERC: RGPIN2020-04995, RGPAS-2020-00095. HD and PO are supported by the Gatsby Charitable Foundation.



\bibliography{references}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn










\section{Additional Details}

\subsection{Background on Flows}
\label{appx:flows}

The key results we use from dynamical systems are as the following two theorems. We refer the reader to \citet{Arnold1998,Santambrogio2015}, and references therein, for more details. 

\begin{theorem}[{\citealp[Thm.\ B.3.1, B.3.5]{Arnold1998}}]
    \label{thm:ode:flow}
    Suppose that $(t,y) \mapsto v(y,t)$ is locally Lipschitz continuous in $y$, and satisfies the local linear growth condition, 
    \begin{align*}
        |v(y,t)| \leq a(t) |y| + b(t) \;,
    \end{align*}
    where $a$ and $b$ are non-negative locally integrable functions. 
    Then the maximal solution to \eqref{eq:basic:ode} generates a unique solution flow $\varphi_{s,t}$ as in \cref{eq:ode:flow:def,eq:classical:sol}. If $v(\argdot,t)$ $k$-times continuously differentiable in $y$ for each $t$, then so is $\varphi_{s,t}(\argdot)$. Conversely, if a flow $\varphi_{s,t}(y)$ is differentiable in $t$ at $t = s$ for every $y$, then the ODE defined by \eqref{eq:ode:from:flow} both generates and is solved by $\varphi_{s,t}$. 
\end{theorem}

Flows generated by velocities also characterize the possible solutions to the continuity equation. 


\begin{theorem}[{\citealp[Thm.\ 4.4]{Santambrogio2015}}]
    \label{thm:ce:solutions}
    Fix $v$ and assume the conditions of \cref{thm:ode:flow}. Then for any $p_s$, the family of densities $p_t = (\varphi_{s,t})_* p_s$ uniquely solves the continuity equation \eqref{eq:continuity:eqn} with initial condition density $p_s$. Moreover, every solution $(p_t)_{t\in \bbR}$ to \eqref{eq:continuity:eqn} for fixed $v$ is obtained from the corresponding flow of some initial condition density $p_s$.  %
\end{theorem}

\subsection{Density Along a Causal Curve} 
\label{sec:density:curve}

The continuity equation in \eqref{eq:joint:id} can be viewed as an Eulerian perspective on the problem, tracking velocity and density at points $(x,y)$. Alternatively, we may consider the Lagrangian perspective, viewing the problem along trajectories, which in this case are causal curves. The result is the same, but we include the analysis here to shed additional light on the problem. 

Let $y(x)$ be a causal curve with velocity $v$. 
We evaluate the log joint density along the curve $(x, y(x))$:
\begin{align}
    \label{eqn:logjointdecomposition}
     \log p(x, & y(x)) = \log p(x) \\ & + \log p(y(x_0) \mid x_0) + \log \biggl| \frac{\partial \varphi_{x,x_0}(y(x))}{\partial y(x)} \biggr| \;,   \nonumber
\end{align}
where $x_0$ is an arbitrary origin point. This follows from the usual change-of-variables formula that results from transporting along the causal curve from $(x,y(x))$ to $(x_0, y(x_0))$. 
Compared to the usual SCM construction, $\log p(y(x_0) \mid x_0)$ is the log-density of the noise, and $\varphi_{x,x_0}$ represents the residual map. Notice this ``noise'' term does not depend on $x$. This is because the noise realization is the same no matter where the counterfactual prediction is made. 

Now, we take a (total) derivative in $x$, and use the instantaneous change-of-variables formula commonly used in the neural ODEs literature \citep{chen2018neural, hodgkinson2021stochastic} to obtain
\begin{align}
    \label{eqn:totaldx}
    \frac{d \log p(x, y(x))}{dx} & = \frac{d \log p(x)}{dx} - \frac{\partial v(y(x), x)}{\partial y(x)} \nonumber \\
    & =s_x(x) - \frac{\partial v(y(x), x)}{\partial y(x)}.
\end{align}
Notice the first term is precisely the marginal score $s_x(x)$. We can take the same total derivative using the chain rule as
\begin{align}
    \label{eqn:chain_totaldx}
    & \frac{d \log p(x, y(x))}{dx} \nonumber \\ =&  \frac{\partial \log p(x, y(x))}{\partial x} + \frac{d y(x)}{dx} \frac{\partial \log p(x, y(x))}{\partial y(x)} \nonumber \\
     =&  s_{x}(x, y(x)) + v(y(x), x) s_{y}(x, y(x))
\end{align}
In fact, this is a directional derivative of the joint log-density along the causal curve, i.e., in the direction $\partial_{x}(x, y(x)) = (1, v(y(x), x))$. Hence, denoting
\begin{align}
    \label{eqn:directionaldx2}
    s_{v}(x,y(x)) := s_{x}(x, y(x)) + v(y(x), x) s_{y}(x, y(x)) \;,
\end{align}
and equating (\ref{eqn:totaldx}) and (\ref{eqn:chain_totaldx}), we obtain an expression equivalent to \eqref{eq:joint:id},
\begin{align}
    \label{eqn:gof_raw}
    & s_x(x) - \partial_{y(x)} v(y(x), x) = s_{v}(x,y(x)).
\end{align}
This is a relation between the marginal and joint log-density functions that is satisfied when the conditional distribution arises from an SCM with velocity $v$. 
It is intuitive that this is in terms of $v$ and the score, which both characterize local change. 



\subsection{Score-based ANM Methods}
\label{sec:appendix:scorebasedanm}
\citet{rolland2022score, montagna2023causal, montagna2023scalable} also use the score function for functional causal discovery. There, the focus is on finding leaf nodes in a multivariate causal graph, that is, non-cause nodes. This is equivalent to finding the effect variable in the bivariate setting. Here, we show how the conditions derived there can be interpreted via the continuity equation. Throughout, let $y$ be the effect variable so that $X \to Y$ is the causal graph.

The continuity equation in our setting states \eqref{eq:continuity:eqn}
\begin{align}
    s_x(y|x) = - \dot m(x) s_y(y|x) \;,
\end{align}
where note for an ANM, $v(y,x) = \dot m(x)$ and hence $\partial_y v(y,x) = 0$. Recall that for an ANM we also have 
\begin{align}
    p(y\mid x) = p_{\epsilon}(y - m(x)),
\end{align}
where $p_\epsilon$ is the density of the noise variable $\epsilon_y$. Let $s_\epsilon = \partial_\epsilon \log p_{\epsilon}(\epsilon)$, then the continuity equation becomes
\begin{align}
    -\dot m(x) s_\epsilon(\epsilon) = -\dot m(x) s_y(y\mid x),
\end{align}
where we wrote $\epsilon = y - m(x)$ on the LHS. Now, noting that $s_y(y \mid x) = s_{y}(x,y)$, the $y$ component of the joint score, we have
\begin{align}
    s_{\epsilon}(\epsilon) = s_y(x,y),
\end{align}
which holds for all $(x,y)$ where $\dot m(x) \neq 0$ (thus, for all $x$ if $m$ is injective, corresponding to Condition (2d) in \cref{sec:appendix:regularity}). The above expression is precisely the general one used by \citet{montagna2023causal} for general non-Gaussian noise. There, they use score estimation to estimate $s_y(x,y)$, then fit a non-parametric regression model to estimate $m$ and obtain the residuals $\hat{\epsilon}$. Then, the equation above says that for the effect variable, it is possible to perfectly predict $s_y(x,y)$ from $\hat{\epsilon}$. 

Under the Gaussian noise assumption $\epsilon_y \sim \mathcal{N}(0, \sigma^2)$, \citet{rolland2022score, montagna2023scalable} derive a more specific equation that is easily derived independently of the continuity equation. Consider the $y$ component of the joint score, 
\begin{align}
    s_y(x,y) = s_y(y \mid x) = s_{\epsilon}(y - m(x)).
\end{align}
Under the Gaussian noise assumption, $s_{\epsilon}(\epsilon) = -\epsilon/\sigma^2$. Thus, we have 
\begin{align}
    s_y(x,y) = \frac{m(x) - y}{\sigma^2}. 
\end{align}
This indicates that $\partial_y s_y(x,y) = -\sigma^{-2}$, which is constant over $(x,y)$. \citet{rolland2022score} hence devise an algorithm to estimate the Hessian of the log-likelihood (i.e., Jacobian of the score) and select the node with minimum empirical variance as the effect. 


\section{Proofs}

\subsection{Regularity Conditions of SCM-Velocity Correspondence}
\label{sec:appendix:regularity}

The correspondence in \cref{thm:scm:velocity} between SCMs and velocity-initial condition density pairs $(v,p(y|x_0))$ essentially follows from the known results on dynamical systems reviewed in \cref{appx:flows}. The only subtleties are under what conditions the relationships hold over all of $\bbR^2$, rather than on a subset. We discuss them here before proving \cref{thm:scm:velocity}.


\begin{enumerate}[itemsep=0pt,topsep=0pt]
    \item $P_{X,Y}$ has full support on $\bbR^2$ and is absolutely continuous with respect to Lebesgue measure. 

    \item For any SCM mechanism $f$:
    \begin{enumerate}[itemsep=0pt,topsep=0pt]
        \item For each $x \in \bbR$, $f(x,\argdot)$ is a bijection $\bbR \to \bbR$.
        
        \item The mapping $(x,x',y) \mapsto f_{x'}(f_{x}^{-1}(y))$ is continuous for each $x,x',y \in \bbR^3$.

        \item The mapping $x' \mapsto f_{x'}(f_{x}^{-1}(y))$ is differentiable at $x' = x$ for all $x,y \in \bbR^2$. 

        \item There is a unique $x_0 \in \bbR$ such that for all $\epsilon_y \in \bbR$, $f(x_0,\epsilon_y) = \epsilon_y$.
    \end{enumerate}

    \item For any velocity field $v$:
    \begin{enumerate}[itemsep=0pt,topsep=0pt]
        \item For each $x \in \bbR$, $v(\argdot,x)$ is locally Lipschitz continuous or $k$-times continuously differentiable.

        \item Satisfies 
        \begin{align*}
            |v(y,x)| \leq a(x) |y| + b(x) \;,
        \end{align*}
        where $a$ and $b$ are non-negative locally integrable (integrable on every compact subset of $\bbR$) functions. 
    \end{enumerate}
    
\end{enumerate}

Within the confines of standard practice, these assumptions are not restrictive. Assumption 2(d) may require some care, but is easy to achieve in practice. For example, with LSNMs, it becomes
\begin{align*}
    m(x_0) + e^{h(x_0)} \epsilon_y = \epsilon_y \;,
\end{align*}
which requires each of $m,h$ to have a unique zero at $x_0$. Assuming that each of $m,h$ have at least one zero, if they are injective then the zero is unique. 

In general, assuming that at least one such $x_0$ exists, a sufficient condition for uniqueness is that $x\neq x'$ implies that $f(x,\epsilon_y) \neq f(x',\epsilon_y)$ for \emph{some} $\epsilon_y \in \bbR$. 

Assumption 3(b) on the velocity ensures that a local solution to the ODE $dy/dx = v(y,x)$ extends to a unique global solution
\begin{align*}
    \varphi_{x_0,x}(y) = y + \int_{x_0}^x v(\varphi_{x_0,u}(y), u)\ du \;,
\end{align*}
whereas assumption 3(a) ensures that
\begin{align*}
    \frac{d}{dx} \varphi_{x_0,x}(y) = v(\varphi_{x_0,x}(y),x) \;, \quad \varphi_{x_0,x_0}(y) = y \;,
\end{align*}
holds for all $x$ in the solution domain. See, for example, \citet[][Appendix B]{Arnold1998} for details. 

\begin{proof}[Proof of \cref{thm:scm:velocity}]
    First, assume that $P_{X,Y}$ has full support on $\bbR^2$ and is specified by a bijective SCM 
    \begin{align*}
        Y = f(X, \epsilon_y) \;, \quad X \condind \epsilon_y
    \end{align*}
    with noise density $\epsilon_y \sim p_0$.  Then
    \begin{align*}
        \varphi_{x,x'}(y) = f_{x'}(f_{x
        }^{-1}(y))
    \end{align*}
    defines a continuous flow. 
    Along with the assumed differentiability in 2(c), by \citep[][Thm.\ B.3.5]{Arnold1998}, 
    \begin{align*}
        v(y,x) := \frac{d}{dx'} f_{x'}(f_{x
        }^{-1}(y)) \bigg|_{x'=x} 
    \end{align*}
    defines the velocity that generates the flow. As long as the mechanism is bijective on all of $\bbR$ for each $x$ then this relationship holds over all of $\bbR^2$. 
    By the uniqueness of $x_0$, $p(y|x_0) = p_0(y)$ is uniquely specified by the SCM.  

    Conversely, fix a pair $(v, p(y|x_0))$ such that $p(y|x_0)$ has full support on $y\in\bbR$. As long as $v$ is sufficiently regular so as to yield a flow $(x,y) \mapsto \varphi_{x_0,x}(y)$ over all of $\bbR^2$, then the flow is unique and therefore
    \begin{align*}
        f(X,\epsilon_y) := \varphi_{x_0,X}(\epsilon_y) \;, 
    \end{align*}
    with $\epsilon_y \sim p(y|x_0)$ uniquely specifies a bijective SCM, except for the marginal distribution $P_X$. Assumption 3(b) above guarantees the global uniqueness of the flow-based mechanism. 
\end{proof}

\subsection{Proof of \cref{thm:id}}

\begin{proof}[Proof of \cref{thm:id}]
    By \cref{thm:ce:solutions}, the continuity equation is uniquely solved by densities generated by the flow associated with $v$ and some initial condition density $p(y|x_0)$. Hence, if $P_{X,Y}$ can be represented by a SCM with velocity $v$ then its conditional $p(y|x)$ will satisfy the continuity equation 
    \begin{align*}
        \partial_x  p(y|x) = -\partial_y (p(y|x) \cdot v(y,x)) \;.
    \end{align*}
    Since $P_{X,Y}$ (and hence $P_{Y|X}$) is assumed to have full support, its density will be strictly positive and the continuity equation can be written
    \begin{align*}
        \frac{\partial_x  p(y|x) }{p(y|x)} = -\partial_y v(y,x) - v(y,x) \frac{\partial_y p(y|x)}{p(y|x)} \\
        \partial_x \log p(y|x) = -\partial_y v(y,x) - v(y,x) \partial_y \log p(y|x) \;.
    \end{align*}
    Noting that $\partial_y p(y|x) = \partial_y p(x,y)$ and adding $\partial_x \log p(x)$ to both sides, we have
    \begin{align*}
        \partial_x \log p(x,y) = -\partial_y v(y,x) - v(y,x) \partial_y \log p(x,y) + \partial_x \log p(x) \;,
    \end{align*}
    which is \eqref{eq:joint:id}.
    
    Conversely, if $P_{X,Y}$ satisfies \eqref{eq:joint:id} then $p(y|x)$ must satisfy the continuity equation and therefore the associated velocity can be used to represent $p(y|x)$ with the SCM constructed as in \cref{thm:scm:velocity}. 
\end{proof}

\subsection{Proofs for \cref{sec:asymptotics}}

\begin{theorem}
    Let $s_x \in \calH_{\calX}$ and  $s_{x,y}^{(x)},  s_{x,y}^{(y)} \in {\calH}_{\calX, \calY}$ where $\calH_{\calX}, \calH_{\calX, \calY}$ are RKHSs induced by bounded kernels $k_x < B, k_{x,y} = k_x \otimes k_y < B$. Additionally, let  $v \in C_b^1(\bbR^2,\bbR )$. Then, if the score estimators have bounded norm
    and converge in RKHS norm, such that each of $\left \| \hat s_x - s_{x} \right \|_{\calH_{\calX}}$, $\left \| \hat s_{x,y}^{(x)} - s_{x,y}^{(x)}\right \|_{\calH_{\calX,\calY}}$, $\left \| \hat s_{x,y}^{(y)} - s_{x,y}^{(y)}\right \|_{\calH_{\calX,\calY}}$ is $\calO_p(n^{-\frac{1}{\alpha}})$ 
    for some $\alpha > 0$, then $ \hat\bfL_n(v) - \bfL(v) = \calO_p(n^{-\frac{1}{\beta}})$, for $\beta   = \max \{\alpha, 2\}$. 
\end{theorem}

\begin{proof}
    For convenience in this proof, we will use the notation $s_1 := s_x, s_2 := -s_{x,y}^{(x)}, s_3:= -s_{x,y}^{(y)}$, $h_1 := -\frac{\partial v}{\partial y}$, $h_2 = v$, and $Z = (X,Y) \sim p_{x,y}$. Note that by assumption, we have that $s = (s_1,s_2,s_3) \in \calH$, where $\calH = \calH_1 \otimes \calH_2 \otimes \calH_3$ is a tensor product reproducing kernel Hilbert space with components $\calH_1 := \cal H_{\cal X}$, $\calH_2 = \calH_3 := \calH_{\calX,\calY}$.
    Using this notation, we can express \cref{eqn:loss} and its estimator \cref{eqn:loss_est} as
        \begin{align}
     {\bfL}(v) & = \bbE f(s, Z) \\
    \hat {\bfL}_n(v) & = \frac{1}{n} \sum_{i=1}^{n} f(\hat s, Z_i)
    \end{align}
    where $f( s, Z) =  \left(s_1(Z) +h_1(Z) + s_2(Z) + h_2(Z) s_3(Z)\right)^2$ and $\calD_n : = \{Z_i\}_{i=1}^{n} = \{X_i,Y_i\}_{i=1}^{n} \overset{iid}{\sim}p_{X,Y}$ is an i.i.d. dataset, and $\hat s = (\hat s_1, \hat s_2, \hat s_3)$ are the estimated scores using $\calD_n$. We can expand the deviation between the estimated and true loss into two bounding terms
    \begin{align}
   |\hat \bfL_n(v) -   \bfL(v)| & \leq \left|\frac{1}{n} \sum_{i=1}^n f( \hat s, Z_i) - \bbE_Z f( \hat s, Z)\right| + \left|\bbE_Z f( \hat s, Z) - \bbE f( s, Z)\right| \\
    & \leq \sup_{s \in \calB(\calH,M)}\left|\frac{1}{n} \sum_{i=1}^n f( s, Z_i) - \bbE f(s, Z)\right| + \left|\bbE_Z f( \hat s, Z) - \bbE f( s, Z)\right| \label{eq:loss_split}
    \end{align}
    where for clarity $\calB(\calH,M) =  \calB(\calH_1,M) \cup \calB(\calH_2,M)\cup \calB(\calH_3,M) \subset \calH$, $f( s, z)  =    \left(s_1(z) +h_1(z) + s_2(z) + h_2(z) s_3(z)\right)^2$, and $Z$ is an iid copy.
    

    To analyze the LHS term, we first show that the function class  $\{f(s,\cdot) : s \in \calB(\calH,M)$ \} is a \emph{separable and complete Carath\'{e}odory family}. This requires that (i) $\calB(\calH,M)$ is a separable, complete metric space, and (ii) $s \mapsto f(s,z)$ is continuous for every $z \in \calZ$ \cite{steinwart2008support}. (i) Follows immediately from the properties of RKHS's and the fact that $\calB(\calH,M)$ is a closed ball in this space. (ii) Follows from the fact that  $s_1, s_2, s_3, h_1, h_2$ are all bounded and continuous (note the boundedness of $s_1,s_2,s_3$ follows from the boundedness of the kernels $k_x,k_y$). This property also means that $\calF := \{f(s,\cdot) : s \in \calB(\calH,M)\} \in L_\infty(\calZ)$ and $\sup_{s \in \calB(\calH,M)} \lVert f(s,\cdot)\rVert_{\infty}$. As a result, by Proposition 7.10 in \citet{steinwart2008support} we have
    \begin{align}
        \bbE \sup_{s \in \calB(\calH,M)} \left| \bbE f(s,Z) - \frac{1}{n}\sum_{i=1}^n f(s,Z_i)\right| \leq 2 \bbE \Rad_{\calD_n}(\calF, n) \label{eq:rad}
    \end{align}

    Where $\Rad_{\calD_n}(\calF, n) = \bbE_{ \sigma} \sup_{s \in \calB(\calH,M)} \left| \frac{1}{n}\sum_{i=1}^n \sigma_i f(s,Z_i)\right|$ is the empirical Rademacher average (i.e., $\{\sigma_i\}_{i=1}^n \overset{iid}{\sim} \Rad(1/2)$). Now, note that we can expand $f$ as 
    \begin{align}
       f(s,Z) & =  (s_1(Z)^2 + s_2(Z)^2 + h_2(Z)^2 s_3(Z)^2 + 2s_1(Z)s_2(Z) + 2s_1(Z)s_3(Z)h_2(Z)+2s_2(Z)s_3(Z)h_2(Z) \nonumber \\
           & \qquad + 2(s_1(Z) + s_2(Z)+ s_3(Z)h_2(Z))h_1(Z) + h_1(Z)^2)
    \end{align}
    If we substitute in this definition of $f$ into \eqref{eq:rad}, we get the inequality.
    \begin{align}
      \Rad_{\calD_n}(\calF, n) & \leq   \Rad_{\calD_n}(\calB_{\calH_1^2}, n) +  \Rad_{\calD_n}(\calB_{\calH_2^2},n) +  \Rad_{\calD_n}(\{h_2\}\otimes \calB_{\calH_3^2},n)  + 2 \Rad_{\calD_n}(\calB_{\calH_1}\otimes \calB_{\calH_2}, n)  \nonumber \\
    & \qquad + 2 \Rad_{\calD_n}(\calB_{\calH_1}\otimes \calB_{\calH_3} \otimes \{h_2\}, n) + 2 \Rad_{\calD_n}(\calB_{\calH_2}\otimes \calB_{\calH_3} \otimes \{h_2\}, n) + 2 \Rad_{\calD_n}(\calB_{\calH_1} \otimes \{h_1\}, n) \nonumber \\
    & \qquad + 2 \Rad_{\calD_n}(\calB_{\calH_2} \otimes \{h_1\}, n) + 2 \Rad_{\calD_n}(\calB_{\calH_3} \otimes \{h_1\}\otimes \{h_2\}, n) + \Rad_{\calD_n}(\{h_1\} \otimes \{h_1\},n) \label{eq:rad_decomp}
    \end{align}
    Where in the above we use the shorthand $\calB_{\cal H} = \calB(\calH,M)$. The tensor product spaces with singletons satisfy the property that if $\phi \in \calG \otimes \{h\}$ where $\calG$ is a space of functions and $\{h\}$ is a singleton, then there is some $g \in \calG$ such that $\phi(z) = g(z)h(z),\ \forall z \in \calZ$. Note that the above inequality follows from the fact that, for any two function classes $\calG_1, \calG_2$, we have by the triangle inequality, 
    \begin{align}
        \sup_{(g_1,g_2) \in \calG_1 \times \calG_2} \left| \frac{1}{n}\sum_{i=1}^n \sigma_i (g_1(Z_i)+g_2(Z_i))\right| \leq  \sup_{g_1 \in \calG_1} \left| \frac{1}{n}\sum_{i=1}^n \sigma_i g_1(Z_i)\right| +  \sup_{g_2 \in \calG_2} \left| \frac{1}{n}\sum_{i=1}^n \sigma_i g_2(Z_i)\right|
    \end{align}

   Now, note that every term in \eqref{eq:rad_decomp} is the Rademacher average of a closed ball in an RKHS of bounded functions. For the terms involving tensor-product spaces without singletons like $\{h_1\}$ this is by true by definition (since we work with tensor products of bounded RKHS's). To briefly show that this holds for the terms involving tensor product spaces with singletons (e.g. $\calH_1 \otimes \{h_1\}$), note in general that if $\calH_k$ is an RKHS with bounded kernel $k$, then so is $\calH_k \otimes \{h_1\}$, if $h_1 \in C^0(\calZ,\bbR)$, because the evaluation functional $\delta_z : h \otimes h_1 \mapsto h(z)h_1(z)$ remains continuous: $\delta_z(hh_1)  = \langle h,k(z,\cdot)\rangle_{\calH_k}h_1(z) \leq \lVert h_1 \rVert_{\infty}k(z,z) \lVert h \rVert_{\calH_k} \leq \tilde B\lVert h \rVert_{\calH_k}, \tilde B > 0$. Similarly, if $\lVert h \rVert_{\calH_{k}} \leq M$,then $\lVert hh_1\rVert_{\calH_k} \leq M$ also. This is because one can simply choose the norm of $\calH_k \otimes \{h_1\}$ as the norm of $\calH_k$, due to the boundedness of $h_1$.
   
   Now, by Lemma 22 in \citet{bartlett2002rademacher}, we have $\Rad_{\calD_n}(\calB(\calH_k,M),n) < \frac{C}{\sqrt{n}}$ for some $C>0$, where $\calB(\calH_k,M) = \{h \in \calH_k : \|h\|_{\calH_k} \leq M\}$ and $\calH_k$ is an RKHS with bounded kernel $k$. This means that 
    \begin{align}
        \bbE \sup_{s \in \calB(\calH,M)} \left| \bbE f(s,Z) - \frac{1}{n}\sum_{i=1}^n f(s,Z_i)\right| \leq 2 \bbE \Rad_{\calD_n}(\calF, n) \leq 2 D/\sqrt{n} \label{eq:rad_final}
    \end{align}
for an appropriate constant $D>0$, which gives us the desired result for the first term of \cref{eq:loss_split}.

Now for the second term. For this section, all expectations are taken over $Z$ only (i.e. not $\hat s$). To start, we can expand out $f$.
\begin{align}
    \bbE f(s, Z) & =     \bbE
   [s_1(Z)^2 + s_2(Z)^2 + h_2(Z)^2 s_3(Z)^2 + 2s_1(Z)s_2(Z) + 2s_1(Z)s_3(Z)h_2(Z)+2s_2(Z)s_3(Z)h_2(Z) \nonumber \\
    & \qquad + 2(s_1(Z) + s_2(Z)+ s_3(Z)h_2(Z))h_1(Z) + h_1(Z)^2]
\end{align}
Taking differences with the same quantity at $\hat s$ gives
\begin{align}
    \bbE f(s, Z) -  \bbE f(\hat s, Z) & = \bbE
   (s_1(Z)^2 - \hat s_1(Z)^2 + \bbE (s_2(Z)^2 - \hat s_2(Z)^2) +  \bbE h_2(Z)^2 (s_3(Z)^2 - \hat s_3(Z)^2) \\
     +2 \bbE[s_1(Z)s_2(Z) - \hat s_1(Z)\hat s_2(Z)] & + 2\bbE [(s_1(Z)s_3(Z)-\hat s_1(Z)\hat s_3(Z))h_2(Z)]+2\bbE [(s_2(Z)s_3(Z)-\hat s_2(Z)\hat s_3(Z))h_2(Z)] \nonumber \\
     \qquad + 2\bbE [(s_1(Z) - \hat s_1(Z))h_1(Z)] & + 2\bbE [(s_2(Z) - \hat s_2(Z))h_1(Z)] + 2\bbE [(s_3(Z) - \hat s_3(Z))h_1(Z)]
\end{align}

Since  $v \in C^1(\bbR^2, \bbR)$, we know $h_1 < A_1, h_2 < A_2$ are bounded, where $A_1, A_2 > 0$ are constants. Using Jensen's inequality we therefore have
\begin{align}
    |\bbE f(s, Z) -  \bbE f(\hat s, Z)| & \leq \bbE
   |s_1(Z)^2 - \hat s_1(Z)^2| + \bbE |s_2(Z)^2 - \hat s_2(Z)^2| +  A_1\bbE|s_3(Z)^2 - \hat s_3(Z)^2| \nonumber \\
     +2 \bbE[s_1(Z)s_2(Z) - \hat s_1(Z)\hat s_2(Z)] & + 2A_2\bbE [|s_1(Z)s_3(Z)-\hat s_1(Z)\hat s_3(Z)|]+2A_2\bbE [(s_2(Z)s_3(Z)-\hat s_2(Z)\hat s_3(Z))] \nonumber \\
     \qquad + 2A_1\bbE [(s_1(Z) - \hat s_1(Z)) & + 2A_1\bbE [|s_2(Z) - \hat s_2(Z)] + 2A_1\bbE [(|s_3(Z) - \hat s_3(Z)|]
    \end{align}
    
Which can be simplified using a change of notation to
\begin{align}
  |\bbE f(s, Z) -  \bbE f(\hat s, Z)|   & \leq \lVert s_1^2 - \hat s_1^2 \rVert_{L_1} + \lVert s_2^2 - \hat s_2^2 \rVert_{L_1} +  \lVert s_3^2 - \hat s_3^2 \rVert_{L_1} \\
     &+2 \lVert s_1s_2 - \hat s_1\hat s_2\rVert _{L_1} + 2A_2\lVert s_1s_3-\hat s_1\hat s_3\rVert_{L_1}+2A_2 \lVert s_2s_3-\hat s_2\hat s_3\rVert_{L_1} \nonumber \\
     & \qquad + 2A_1\lVert s_1 - \hat s_1 \rVert_{L_1} + 2A_1\lVert s_2- \hat s_2 \rVert_{L_1} + 2A_1\lVert s_3 - \hat s_2 \rVert_{L_1}
\end{align}
where we define $s_i^2: z \mapsto s_i(z)^2$ and $(s_is_j): z \mapsto s_i(z)s_j(z)$. There are two kinds of summands above: (i) $\lVert s_i - \hat s_i \rVert_{L_1}$ and (ii) $\lVert s_is_j - \hat s_i\hat s_j \rVert_{L_1}$ for $i,j \in \{1,2,3\}$. All that remains is to bound each of these terms by sums of terms like $\lVert s_i - \hat s_i \rVert_{L_2}$. This is immediate for (i) by the properties of $L_p$ norms. To show (ii) we can simply use the triangle inequality and Cauchy Schwartz:
\begin{align}
    \lVert s_is_j - \hat s_i \hat s_j \rVert_{L_1} &  = \lVert s_i(s_j - \hat s_j) + s_j(\hat s_i - s_i)\rVert_{L_1} \\
    & \leq  \bbE|s_i(Z)|| s_j(Z) - \hat s_j(Z)| + \bbE|s_j(Z)||(\hat s_i(Z) - s_i(Z))| \\
    & \leq \lVert s_i \rVert_{L_2} \lVert s_j - \hat s_j \rVert_{L_2} + \lVert s_j \rVert_{L_2} \lVert s_i - \hat s_i \rVert_{L_2} \\
    & \leq A_S(\lVert s_j - \hat s_j \rVert_{L_2} + \lVert s_i - \hat s_i \rVert_{L_2})
\end{align}

Now, note that for any bounded positive-definite kernel $k: \calZ^2 \to \bbR$ with associated RKHS $\calH_k$, if $k < B$ we have $\|f\|_{L_2} \leq \|f\|_{\calH_k}$, because $|f(z)|^2 = \langle k(z,\cdot), f \rangle _{\calH_k}^2 \leq |k(z,z)| \|f\|_{\calH_k}^2 \leq B$ . This means that
\begin{align}
    |\bbE f(s, Z) -  \bbE f(\hat s, Z)| = \calO \left(\sum_{i=1}^3 \lVert s_i - \hat s_i \rVert _{\calH_i}\right) = \calO_p(n^{-\frac{1}{\alpha}}).
\end{align}
Where the last equality follows from the convergence assumption of the score estimators in the theorem. Combining this result with the convergence result in \cref{eq:rad_final} for the first term of \cref{eq:loss_split} completes the proof.

\end{proof}
 

\section{Identifiability of Causal Direction with Bivariate Velocity Models}
\label{appx:id}

In order for the causal direction to be identifiable, it cannot be the case that $p(x,y)$ can be expressed in terms of a SCM from the same class in both causal directions. For the model in the $Y \to X$ direction, let $\tv(x,y)$ denote the velocity. \cref{thm:id} can be used to determine conditions under which the causal direction cannot be identified. 


For convenience, let $\pi$ denote $\log p$, for example $\pi(x,y) = \log p(x,y)$, $\pi(y|x) = \log p(y|x)$, and so on. 
Starting with \eqref{eq:joint:id}, taking a partial derivative in $y$ yields
\begin{align} \label{eq:id:forward}
    \partial_y \partial_x \pi(x,y)  = -\partial^2_y v(y,x) - \partial_y v(y,x) \partial_y \pi(x,y) - v(y,x)\partial^2_y \pi(x,y) \;.
\end{align}
Similarly, in other model direction, \eqref{eq:joint:id} becomes $\partial_y \pi(x,y) = -\partial_x \tv(x,y) - \tv(x,y) \partial_x \pi(x,y) + \partial_y \pi(y)$, so taking a derivative with respect to $x$, we have
\begin{align} \label{eq:id:backward}
    \partial_x \partial_y \pi(x,y)  = -\partial^2_x \tv(x,y) - \partial_x \tv(x,y) \partial_x \pi(x,y) - \tv(x,y)\partial^2_x \pi(x,y) \;.
\end{align}
Equating them, we find that the direction is not identifiable if and only if
\begin{align} \label{eq:id:main}
    \partial^2_y v(y,x) + \partial_y v(y,x) \partial_y \pi(x,y) + v(y,x)\partial^2_y \pi(x,y) 
    =
    \partial^2_x \tv(x,y) + \partial_x \tv(x,y) \partial_x \pi(x,y) + \tv(x,y)\partial^2_x \pi(x,y) \;.
\end{align}
This proves \cref{prop:velocity:id}. 

To use this, we write the various partial derivatives of $\pi(x,y)$ in terms of the forward model. 
For a SCM with velocity $v$ and flow $\varphi$, so that the SCM is $y = \varphi_{x_0,x}(\epsilon_y)$, the log joint density can be written 
\begin{align*}
    \pi(x,y) = \pi(x) + \pi_0(\varphi^{-1}_{x_0,x}(y)) + \log | \partial_y \varphi^{-1}_{x_0,x}(y) | = \pi(x) + \pi_0(\varphi_{x,x_0}(y)) + \log | \partial_y \varphi_{x,x_0}(y) | \;,
\end{align*}
and therefore,
\begin{align}
    \partial_y \pi(x,y) & = \dot{\pi}_0(\varphi_{x,x_0}(y)) \partial_y \varphi_{x,x_0}(y) + \partial_y \log | \partial_y \varphi_{x,x_0}(y) | \\
    \partial_x \pi(x,y) & = \dot{\pi}(x) + \dot{\pi}_0(\varphi_{x,x_0}(y)) \partial_x \varphi_{x,x_0}(y) + \partial_x \log | \partial_y \varphi_{x,x_0}(y) | \\
    & =
    \dot{\pi}(x) -  \dot{\pi}_0(\varphi_{x,x_0}(y)) v(y,x) \partial_y \varphi_{x,x_0}(y) - \partial_y v(y,x) \\
    \partial^2_y \pi(x,y) & = \ddot{\pi}_0(\varphi_{x,x_0}(y))(\partial_y \varphi_{x,x_0}(y))^2 + \dot{\pi}_0(\varphi_{x,x_0}(y)) \partial^2_y \varphi_{x,x_0}(y) + \partial^2_y \log | \partial_y \varphi_{x,x_0}(y) |
    \\
    \partial^2_x \pi(x,y) & = \ddot{\pi}(x) + \ddot{\pi}_0(\varphi_{x,x_0}(y))(\partial_x  \varphi_{x,x_0}(y))^2 + \dot{\pi}_0(\varphi_{x,x_0}(y)) \partial^2_x \varphi_{x,x_0}(y) - \partial_x\partial_y v(y,x)
    \\
    \partial_x \partial_y \pi(x,y) & = \ddot{\pi}_0(\varphi_{x,x_0}(y)) (\partial_y \varphi_{x,x_0}(y)) (\partial_x \varphi_{x,x_0}(y)) + \dot{\pi}_0(\varphi_{x,x_0}(y)) (\partial_x \partial_y \varphi_{x,x_0}(y)) + \partial_y \partial_x \log | \partial_y \varphi_{x,x_0}(y) | \\
    & = - \ddot{\pi}_0(\varphi_{x,x_0}(y)) v(y,x) (\partial_y \varphi_{x,x_0}(y))^2 - \dot{\pi}_0(\varphi_{x,x_0}(y)) \partial_y ( v(y,x) \partial_y\varphi_{x,x_0}(y)) - \partial^2_y v(y,x)
\end{align}
We have used the identities $\partial_x \varphi_{x,x_0}(y) = -v(y,x) \partial_y \varphi_{x,x_0}(y)$ and $\partial_x \log | \partial_y \varphi_{x,x_0}(y) | = -\partial_y v(y,x)$ to simplify some of the expressions. 
Substituting these into \eqref{eq:id:main}, if we view $\pi_0, v, \varphi$, and $\pi(x)$ as given (specified by nature), the result is a PDE for the reverse model velocity, $\tv$.  
Alternatively, as is common in the literature \citep{peters2014identifiability}, if we allow $\pi(x)$ to vary then we might manipulate some combination of \cref{eq:joint:id,eq:id:forward,eq:id:backward,eq:id:main} to obtain a differential equation for $\pi(x)$ in terms of only the fixed forward model. 

A more thorough general analysis of identifiability is beyond the scope of this work, though we analyze below the special cases of ANMs and LSNMs. In doing so, we obtain a new characterization of non-identifiability that holds uniformly over the model class. However, even in that somewhat simple extension of ANMs, the characterizing equation is much more complicated and does not yield an easy interpretation. 





\subsection{Additive noise models}
\label{appx:anm:id}

For ANMs, write the models as $Y = m(X) + \epsilon_y$ with $\epsilon_y \sim p_0$, and $X = \tm(Y) + \epsilon_x$ with $\epsilon_x \sim \tp_0$. As shown in the main text, $v(y,x) = \dot{m}(x)$. Putting this into \eqref{eq:id:main} yields
\begin{align*}
    \dot{m}(x) \partial^2_y \pi(x,y) = \dot{\tm}(y) \partial^2_x \pi(x,y) \;.
\end{align*}
Hence, the structural functions are mutually constrained, with $\dot{\tm}(y)$ satisfying
\begin{align} \label{eq:anm:constraint}
    \dot{\tm}(y) = \dot{m}(x) \frac{\partial^2_y \pi(x,y)}{\partial^2_x \pi(x,y)} \;.
\end{align}
We observe that in the special case of linear ANMs, $\dot{m(x)} = a$ and $\dtm(y) = b$, so that \eqref{eq:anm:constraint} implies that each of $\partial^2_y \pi(x,y) = \partial^2_y \pi(y|x)$ and $\partial^2_x \pi(x,y) = \partial^2_x \pi(x|y)$ must be constant and therefore Gaussian, i.e., $b/a = \sigma_x^2/\sigma_y^2$. We also see that if $\pi(x,y)$ is jointly Gaussian then the ANM is not identifiable if and only if $\dm(x)/\dtm(y)$ is a constant, i.e., each of the model directions is linear. 

Continuing from \eqref{eq:anm:constraint} by taking another partial derivative in $x$, we find that
\begin{align*}
    \partial_x \frac{\dot{m}(x)\partial^2_y \pi(x,y)}{\partial^2_x \pi(x,y)} = 0 \;.
\end{align*}
Moreover, 
\begin{align*}
    \pi(x,y) = \pi(x) + \pi_0(y - m(x)) \;,
\end{align*}
so that
\begin{gather*}
    \partial_y \pi(x,y) = \dot{\pi}_0(y-m(x)) \\
    \partial^2_y \pi(x,y) = \ddot{\pi}_0(y-m(x)) \\
    \partial_x \pi(x,y) = \dot{\pi}(x) - \dot{\pi}_0(y-m(x))\dot{m}(x) \\
    \partial^2_x \pi(x,y) = \ddot{\pi}(x) + \ddot{\pi}_0(y-m(x)) (\dot{m}(x))^2 - \dot{\pi}_0(y-m(x))\ddot{m}(x) 
\end{gather*}
Carrying out the algebra, we find
\begin{align} \label{eq:anm:id:de}
    \dddot{\pi}(x) =\ddot{\pi}(x) G(x,y) + H(x,y) \;,
\end{align}
where
\begin{align*}
    G(x,y) & = \frac{\ddot{m}(x)}{\dot{m}(x)} - \frac{\dot{m}(x) \dddot{\pi}_0(y-m(x))}{\ddot{\pi}_0(y-m(x))}\\
    H(x,y) & = -2\ddot{\pi}_0(y - m(x)) \ddot{m}(x) \dot{m}(x) + \dot{\pi}_0(y-m(x)) \dddot{m}(x) \\ 
    & \quad + \frac{\dot{\pi}_0(y-m(x)) \dddot{\pi}_0(y-m(x)) \ddot{m}(x) \dot{m}(x)}{\ddot{\pi}_0(y-m(x))} - \frac{\dot{\pi}_0(y-m(x)) (\ddot{m}(x))^2}{\dot{m}(x)} \;.
\end{align*}
This is the same differential equation obtained by \citet{hoyer2008nonlinear} in their analysis of identifiability using ANM models. 

Alternatively, Eq.\ (6) from \citep{hoyer2008nonlinear}, which also leads to the identifying differential equation \eqref{eq:anm:id:de}, can be obtained directly from the continuity equation \eqref{eq:joint:id} for the backward model $Y \to X$. In that case, \eqref{eq:id:backward} yields
\begin{align*}
    \partial_x \partial_y \pi(x,y) = -\dot{\tm}(y) \partial^2_x \pi(x,y).
\end{align*}
Hence, solving for $1/\dot{\tm}(y)$ and differentiating with respect to $x$,
\begin{align*}
    \partial_x \left( \frac{\partial_x \partial_y \pi(x,y)}{\partial^2_x \pi(x,y)} \right) = 0 \;,
\end{align*}
which is Eq.\ (6) in \citep{hoyer2008nonlinear}, from which the rest of their identifiability results on ANMs follow. 

\subsection{Location-Scale Noise Models}
\label{appx:lsnm:id}

For LSNMs with $y = m(x) + e^{h(x)} \epsilon_y$ and $x = \tm(y) + e^{\th(y)}\epsilon_x$, we have that $v(y,x) = \dm(x) + \dh(x)(y - m(x))$, and similarly $\tv(x,y) = \dtm(y) + \dth(y)(x - \tm(y))$. Therefore, \eqref{eq:id:backward} yields
\begin{align*}
    - \partial_x\partial_y \pi(x,y) & = \dth(y)\partial_x \pi(x,y) + (\dtm(y) + \dth(y)(x - \tm(y)))\partial^2_x \pi(x,y) \;. %
\end{align*}
Dividing by $\partial^2_x \pi(x,y)$ and differentiating with respect to $x$,
\begin{align*}
    - \partial_x \left(\frac{\partial_x\partial_y \pi(x,y)}{\partial^2_x \pi(x,y)}\right) = \dth(y)\left(1 + \partial_x \left( \frac{\partial_x \pi(x,y)}{\partial^2_x \pi(x,y)} \right) \right) \;.
\end{align*}
Therefore,
\begin{align*}
    \partial_x \left[ \partial_x \left(\frac{\partial_x\partial_y \pi(x,y)}{\partial^2_x \pi(x,y)}\right) 
    \bigg/ 
    \left(1 + \partial_x \left( \frac{\partial_x \pi(x,y)}{\partial^2_x \pi(x,y)} \right) \right) \right]
    = 0\;.
\end{align*}
Carrying out the differentiation and simplifying yields (suppressing the $(x,y)$ arguments of $\pi$),
\begin{align*}
    [3(\partial^3_x \pi)^2 - 2(\partial^2_x \pi)(\partial^4_x \pi)]
    (\partial_x \partial_y \pi)
    +
    [(\partial_x \pi)(\partial^4_x \pi) - 3(\partial^2_x \pi)(\partial^3_x \pi)]
    (\partial^2_x \partial_y \pi) 
    +
    [2 (\partial^2_x \pi)  -
    (\partial_x \pi)(\partial^3_x \pi)]
    (\partial^3_x \partial_y \pi)
    = 0 \;.
\end{align*}
Since by using the forward model we have
\begin{align*}
    \pi(x,y) = \pi(x) + \pi_0\left( e^{-h(x)}(y - m(x)) \right) - \dot{h}(x) \;,
\end{align*}
in principle this can be solved to characterize, for fixed $\pi_0, h, m$ the set of $\pi(x)$ for which the causal direction is not identifiable. 

Alternatively, substituting
\begin{align*}
    \partial_x \partial_y \pi(x,y) = \dh(x) \partial_y \pi(x,y) + (\dm(x) + \dh(x)(y - m(x)))\partial^2_y \pi(x,y) 
\end{align*}
yields a different but equivalent equation. 

\section{Experiment and Simulation Details}
\label{appx:experiment}

We describe extra details of the experiment and simulation here. All experiments are conducted using the JAX library \citep{jax2018github}.

\subsection{Synthetic Data Generation}

All three synthetic data settings can be written as an SCM $Y = f(X, \epsilon_y)$. In all cases, $X \sim \calN(0,1)$, and $\epsilon_y \sim \calN(0,1)$ or $\calN(0,3)$. 

\paragraph{Periodic Data} We generate data from 100 random SCMs of the following form:
\begin{align}
    Y = f(X, \epsilon_y) = a(X) + e^{b(X)} \sin(e^{c(X) \epsilon_y}).  
\end{align}
Note this is similar to an LSNM but where an additional factor of $c(X)$ is applied before a non-linear transformation on the noise (here, the transformation is non-injective, further complicating matters). To sample $a, b, c$, we randomly draw the weights of a 2-layer, 64 hidden unit MLP from $\calN(0, 0.2)$.

\paragraph{Sigmoid Data} We generate data from 100 random SCMs of the following form:
\begin{align}
    Y = f(X, \epsilon_y) = c(X)+ e^{d(X)} \Phi^{-1}(\text{sigmoid}(a(X) + e^{b(X)} \epsilon_y)),
\end{align}
where $\Phi^{-1}$ is the standard Gaussian inverse CDF. Again, $a, b, c, d$ are randomly drawn MLPs as in the periodic dataset. 

\paragraph{Velocity Data} We sample 100 instances of the following velocity:
\begin{align}
    v(y,x) = c_1 + c_2x + c_3y + c_4 e^{-x^2} + c_5 e^{-y^2} + c_6 e^{-(x - y)^2}. 
\end{align}

We use numerical integration with the Diffrax package \citep{kidger2021on}, setting the starting time to $x_0 = 0$. To be specific, the SCM is of the following form:

\begin{align}
    Y = f(X, \epsilon_y) = \epsilon_y + \int_{0}^{X} v(y(u), u) du. 
\end{align}



\subsection{Velocity Parametrization and Training}

We distinguish two types of models: those where the velocity function is a linear combination of basis functions, and those where the velocity function are neural network based. 

For the basis models, we use the Adam optimizer with a learning rate of $1/2\log(\text{\# of parameters})$ to optimize the objective, while for neural network-based models we use a learning rate of $1/\log(\text{\# of parameters})$. 

The basis functions we use are as follows:
\begin{align}
    \Phi_{\text{lin}}(y,x) = \begin{bmatrix}
        1 & x & y
    \end{bmatrix}, \quad \Phi_{\text{quad}}(y,x) = \begin{bmatrix}
        1 & x & y & x^2 & y^2 & xy
    \end{bmatrix}.
\end{align}
For each of these, our experiments also include appending the following exponential terms:
\begin{align}
    \Phi_{\text{exp}}(y,x) = \begin{bmatrix}
        e^{-x^2} & e^{-y^2} & e^{-(x^2 + y^2)}
    \end{bmatrix}. 
\end{align}
The linear basis and quadratic basis models are hence parametrized by $K = 3, 6$ real-valued parameters, respectively. When the exponential terms are added, the parameter count is increased further by $3$. 

All neural networks involved are 2 layer fully connected MLPs with a hidden size of $64$, and $\tanh$ activation functions. For the ANM, we directly parametrize the velocity (i.e., $\dot m$) as an MLP, while for the LSNM, we parametrize the functions $m$ and $h$, and evaluate their derivatives using automatic differentiation to obtain the LSNM velocity (see \cref{tab:examples} for the specific form). 

\section{Additional Figures and Results}

\label{sec:additional}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{anm_estim_fig.pdf}
    \caption{Effect of score estimation on velocity. Synthetic data generated from an ANM. Top left: ground-truth velocity. Top right: velocity estimated from the true score. Bottom left: velocity estimated after estimating the score with the KDE-type estimator. Bottom right: after estimating the score with the Stein estimator.}
    \label{fig:anm_score}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{velocity_estim_fig.pdf}
    \caption{Comparing the estimated velocities from \cref{fig:anm_score} (black arrows) with the ground-truth velocity (gray arrows).}
    \label{fig:anm_curves}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{parametric_tue.pdf}
    \caption{Examples of parametric (\texttt{B-QUAD} with exponential terms) velocity models fit to real data, here, \texttt{pair0001} from the Tuebingen dataset \citep{mooij2016distinguishing}. The true causal direction is $X$ (altitude) $\to$ $Y$ (temperature).}
    \label{fig:anm_velocity}
\end{figure}

\begin{table*}[bt]
    \centering
    \caption{Synthetic results using only the Gaussian kernel.}
    \label{tab:synthetic}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|ccc|ccc|ccc} \toprule 
\multirow{2}{*}{Model} & \multicolumn{3}{c}{Periodic} & \multicolumn{3}{c}{Sigmoid} & \multicolumn{3}{c}{Velocity} \\ 
& \texttt{KDE} & \texttt{STEIN} & \texttt{HYBRID} & \texttt{KDE} & \texttt{STEIN} & \texttt{HYBRID} & \texttt{KDE} & \texttt{STEIN} & \texttt{HYBRID} \\ \midrule 
\texttt{MLP}& 70 (84) & 98 (100)& 88 (96) & 25 (35) & 59 (75) & 29 (44) & 93 (99) & 90 (98) & 91 (99)\\
\texttt{B-LIN} & 70 (90) & 98 (100) & 97 (100) & 25 (35) & 71 (85) & 33 (45) & 91 (99) & 95 (100) & 92 (99)\\ 
\texttt{B-QUAD}& 88 (98) & 99 (100) & 98 (100) &  35 (49) & 72 (87) & 35 (51)& 91 (99) & 95 (99) & 92 (99)\\ 
\texttt{ANM}& 55 (60) & 86 (95) & 79 (84) & 15 (21) & 45 (42) & 14 (22) & 92 (99) & 54 (77) &90 (99)\\ 
\texttt{LSNM}& 60 (69) & 87 (94) & 68 (73) & 31 (38) & 63 (64) & 31 (42) & 88 (98) & 75 (91) & 91 (99) \\
\bottomrule
\end{tabular}}
\end{table*}

\begin{table*}[bt]
    \centering
    \caption{Synthetic results using only the Exponential kernel.}
    \label{tab:synthetic}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|ccc|ccc|ccc} \toprule 
\multirow{2}{*}{Model} & \multicolumn{3}{c}{Periodic} & \multicolumn{3}{c}{Sigmoid} & \multicolumn{3}{c}{Velocity} \\ 
& \texttt{KDE} & \texttt{STEIN} & \texttt{HYBRID} & \texttt{KDE} & \texttt{STEIN} & \texttt{HYBRID} & \texttt{KDE} & \texttt{STEIN} & \texttt{HYBRID} \\ \midrule 
\texttt{MLP}& 66 (76) & 93 (99)& 100 (100) & 48 (51) & 62 (68) & 70 (80) & 77 (95) & 83 (96) & 90 (99)\\
\texttt{B-LIN} & 15 (7) & 97 (100) & 100 (100) & 21 (32) & 64 (68) & 71 (82) & 88 (98) & 82 (96) & 90 (99)\\ 
\texttt{B-QUAD}& 91 (99) & 95 (100) & 100 (100) &  79 (93) & 62 (68) & 72 (82)& 83 (96) & 82 (96) & 90 (99)\\ 
\texttt{ANM}& 4 (1) & 97 (99) & 99 (100) & 12 (18) & 62 (68) & 67 (77) & 81 (95) & 81 (95) &90 (99)\\ 
\texttt{LSNM}& 48 (53) & 93 (99) & 100 (100) & 42 (34) & 64 (69) & 67 (75) & 72 (85) & 82 (96) & 91 (99) \\
\bottomrule
\end{tabular}}
\end{table*}

\begin{table*}[bt]
    \centering
     \caption{Results on benchmark datasets using all score estimation methods.}
    \label{tab:benchmark}
\begin{tabular}{c|cccccccc}
    \toprule
    {\textbf{Model}} & AN & AN-s & LS & LS-s & MNU & SIM & SIM-G & Tue\\
    \midrule
    & \multicolumn{8}{|c}{\texttt{KDE-GAUSS}} \\ \midrule
     \texttt{MLP}& 92 (99) & 93 (99) & 87 (94) &  93 (99) & 81 (94) & 41 (38) & 78 (92) & 51 (48) \\
     \texttt{B-LIN}& 61 (69) & 91 (99) & 45 (56) & 94 (99) & 77 (88) & 45 (39) & 52 (72) &  61(61) \\
     \texttt{B-QUAD}& 78 (85) & 95 (100) & 73 (86) & 97 (100) & 96 (100)  & 40 (39) & 85 (96) & 46 (53) \\
     \texttt{ANM}& 87 (98) & 83 (97) & 95 (99) &  64 (79) & 68 (75) & 47 (41) & 60 (76) & 60 (65) \\
     \texttt{LSNM}& 87 (97) & 92 (99) & 92 (98) & 87 (96) & 70 (85) & 39 (37) & 84 (95) & 44 (49) \\
     \midrule
    & \multicolumn{8}{|c}{\texttt{KDE-EXP}} \\
    \midrule     
    \texttt{MLP}& 98(100) & 92 (99) & 97 (100) & 85 (95) & 52 (48) & 39 (35) & 69 (88) & 43 (47) \\
     \texttt{B-LIN}& 70 (82) & 98 (100) & 61 (75) & 96 (100) & 8 (2)  & 40 (39) & 53 (73) & 69 (70)  \\
     \texttt{B-QUAD}& 92 (98) & 100 (100) & 95 (99) & 97 (100)& 100 (100) & 40 (35) & 96 (99) & 42 (47)  \\
     \texttt{ANM}& 83 (97) & 72 (90) & 89 (98) & 63 (73) & 49 (45) & 38 (38) & 56 (73) & 64 (68) \\
     \texttt{LSNM}& 98 (100) & 94 (100) & 94 (100) &  97 (100)& 80 (93) & 43 (44) & 80 (94) & 38 (46) \\
     \midrule
    & \multicolumn{8}{|c}{\texttt{STEIN-GAUSS}}  \\ \midrule
     \texttt{MLP}& 80 (92) & 30 (20) & 26 (18) & 77 (90) & 50 (66) & 54 (61) & 73 (86) & 54 (60) \\
     \texttt{B-LIN}& 15 (7) & 7 (3) & 20 (21)  & 59 (71) & 42 (32) & 58 (58) & 49 (58) & 41 (54) \\
     \texttt{B-QUAD}& 25 (10) & 9 (1)  & 18 (23) & 62 (76) & 35 (25) & 63 (67) & 78 (87) & 51 (55) \\
     \texttt{ANM}& 30 (13) & 66 (80) & 72 (85) & 52 (79) & 9 (1) & 67 (75) & 78 (94) &  52 (61)\\
     \texttt{LSNM}& 57 (67) & 56 (66) & 37 (43) & 74 (91) & 34 (24) & 66 (70) & 78 (93) & 46 (52) \\
     \midrule  & \multicolumn{8}{|c}{\texttt{STEIN-EXP}} \\\midrule
    \texttt{MLP}& 69 (78) & 88 (97) & 68 (85) & 86 (95) & 65 (71) & 44 (37) & 67 (82) & 58 (68)\\
     \texttt{B-LIN}& 61 (77) & 86 (97) & 67 (88) & 86 (95) & 61 (66) & 44 (37) & 65 (82) & 59 (68)  \\
     \texttt{B-QUAD}&  63 (78)& 86 (97) & 69 (88) & 86 (96) & 62 (66) & 43 (37)& 66 (83) & 59 (68) \\
     \texttt{ANM}& 69 (83) & 89 (97) & 73 (90) & 86 (95)& 60 (68) & 42 (37) & 65 (83) & 58 (68) \\
     \texttt{LSNM}& 64 (80)& 87 (97)  & 69 (89) & 86 (95) & 66 (68) & 42 (37) & 64 (83) & 59 (68) \\
     \midrule
    & \multicolumn{8}{|c}{\texttt{HYBRID-GAUSS}} \\\midrule
    \texttt{MLP}& 50 (47) & 70 (87) & 38 (34) & 69 (88) & 77 (91) & 38 (40) & 63 (83) & 51 (49)\\
     \texttt{B-LIN}& 26 (19) & 52 (60) & 26 (29) & 59 (79) & 71 (85) & 46 (40) & 49 (62) & 51 (48) \\
     \texttt{B-QUAD}& 33 (32) & 57 (71) & 29 (33) & 66 (85) & 67 (80) & 53 (43)  & 72 (88) & 52 (48) \\
     \texttt{ANM}& 86 (96) & 82 (93) & 77 (88) &  54 (80)&  10 (2) & 45 (44) & 67 (86) &  54 (53)\\
     \texttt{LSNM}& 67 (80) & 83 (93) & 49 (56) & 70 (89)  &  64 (75) & 44 (43) & 74 (89) & 52 (50) \\
    \midrule
    & \multicolumn{8}{|c}{\texttt{HYBRID-EXP}} \\\midrule 
     \texttt{MLP}& 11 (2) & 53 (61) &  28 (39)& 71 (93) & 40 (29) & 34 (38) & 75 (85) & 43 (52)\\
     \texttt{B-LIN}& 6 (1) & 43 (45) &  32 (42) & 69 (92) & 20 (5) & 40 (36) & 74 (78)  &  45 (51) \\
     \texttt{B-QUAD}& 8 (1) & 44 (48) & 29 (44) & 69 (93) & 20 (6)& 39 (38) &  76 (87)& 45 (51) \\
     \texttt{ANM}& 35 (35) & 51 (63) & 28 (39) &  70 (93)& 22 (6) & 41 (40) & 82 (93) & 45 (52) \\
     \texttt{LSNM}& 13 (3) & 49 (56) & 40 (62) & 70 (93) & 22 (8)  & 37 (38) & 78 (89) & 45 (52)\\
     \bottomrule
\end{tabular}
\end{table*}





\end{document}
