\section{Related Work}
\subsection{Personalized T2I Generation}
With the advance of diffusion models, current text-to-image (T2I) generation~\cite{nichol2021glide, rombach2022high, wang2024stablegarment, cui2024localize} has shown remarkable generalization ability. 
As these methods ignore the concepts that do not appear in the training set, some works study personalized text-to-image generation which aims to adapt text-to-image models to specific concepts (attributions, styles, or objects) given several reference images.
For example, Textual Inversion~\cite{gal2022image} adjusts text embeddings of a new pseudoword to describe the concept. DreamBooth~\cite{ruiz2023dreambooth} fine-tunes denoising networks to connect the novel concept and a less commonly used word token. 
Based on that, several recent works~\cite{kumari2023multi,chen2023disenbooth, shi2024instantbooth} have been proposed to enhance controllability and flexibility when processing image visual concepts. 
These advancements enhance the capabilities of text-to-image models, making them more accessible to a wider range of users.


\subsection{Adversarial Examples}
Adversarial examples are crafted by adding imperceptible perturbations to mislead models, primarily applied in anti-classification, anti-deepfakes, and anti-facial recognition. Existing methods fall into two categories:  
\textbf{Image-specific adversarial examples} generate tailored perturbations per image. \citet{szegedy2013intriguing} pioneered this concept with LBFGS optimization, while \citet{goodfellow2014explaining} proposed the efficient FGSM. Subsequent works~\cite{xiao2018generating, xiong2023black, chen2023advdiffuser} improved naturalness via generative models. These are extended to disrupt deepfakes~\cite{ruiz2020disrupting, wang2022anti, wang2022deepfake, li2023unganable} and protect facial privacy~\cite{shan2020fawkes, yang2021towards, cherepanova2021lowkey, deb2020advfaces} from unauthorized face recognition systems.  
\textbf{Universal adversarial perturbations (UAPs)} apply a single perturbation to all images. \citet{moosavi2017universal} first revealed UAPs' existence, with \citet{liu2023enhancing} addressing gradient vanishing via aggregation and \citet{poursaeed2018generative} synthesizing UAPs via generative models. 
For privacy, \citet{zhong2022opom} proposed gradient-based OPOM for identity-specific protection, while \citet{liu2025advcloak} trained generators for natural adversarial cloaks. Our work aligns with UAPs but focuses on protecting all images of a target identity from unauthorized personalized generation.

 

\subsection{Anti-Personalization}
The remarkable generative capability of personalized T2I generation comes with safety concerns~\cite{carlini2023extracting, vyas2023provable}, particularly regarding the unauthorized exploitation of personal images. 
To mitigate these risks, recent studies have proposed the use of adversarial examples to counteract such safety issues. 
AdvDM \cite{liang2023adversarial} pioneered a theoretical framework for crafting adversarial examples against diffusion models. 
Anti-DreamBooth \cite{van2023anti} tackled anti-personalization with a bi-level protection objective and ASPL optimization, later refined by \citet{wang2024simac} via time-step selection. 
MetaCloak \cite{liu2024metacloak} enhanced cloak robustness against image transformations using ensemble learning and EoT, while \citet{xue2023toward} reduced computational costs via SDS loss. 
\citet{li2024pid} and \citet{wan2024prompt} addressed prompt discrepancies between protectors and attackers with encoder-based protection and prompt distribution modeling, respectively.
Despite these advancements, existing methods predominantly generate image-specific cloaks, which are impractical for widespread user adoption. 
In contrast, our work introduces a universal cloak tailored to individual users, enabling it to be applied across all their images, significantly enhancing usability and reducing privacy risks.