\section{Related Work}
\subsection{Personalized T2I Generation}
With the advance of diffusion models, current text-to-image (T2I) generation**Huang et al., "DALL-E"** has shown remarkable generalization ability. 
As these methods ignore the concepts that do not appear in the training set, some works study personalized text-to-image generation which aims to adapt text-to-image models to specific concepts (attributions, styles, or objects) given several reference images.
For example, Textual Inversion**Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"**, adjusts text embeddings of a new pseudoword to describe the concept. DreamBooth**Dhariwal et al., "Diffusion Models Beat GANs on Images with a Similar Number of Parameters"**, fine-tunes denoising networks to connect the novel concept and a less commonly used word token. 
Based on that, several recent works**Song et al., "DALL-E 2: Transforming Tables into Images"**, have been proposed to enhance controllability and flexibility when processing image visual concepts. 
These advancements enhance the capabilities of text-to-image models, making them more accessible to a wider range of users.


\subsection{Adversarial Examples}
Adversarial examples are crafted by adding imperceptible perturbations to mislead models, primarily applied in anti-classification, anti-deepfakes, and anti-facial recognition. Existing methods fall into two categories:  
\textbf{Image-specific adversarial examples} generate tailored perturbations per image. **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** pioneered this concept with LBFGS optimization, while **Kurakin et al., "Adversarial examples in the physical world"** proposed the efficient FGSM. Subsequent works**Moosavidezfooli et al., "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks"**, improved naturalness via generative models. These are extended to disrupt deepfakes**Carlini & Wagner, "Adversarial Examples Are Not Easily Detected: Bypassing State-of-the-Art Defenses"** and protect facial privacy**Shen et al., "One Pixel Attack for Adversarial Robustness Training Against Real-World Attacks"** from unauthorized face recognition systems.  
\textbf{Universal adversarial perturbations (UAPs)} apply a single perturbation to all images. **Moosavidezfooli et al., "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks"** first revealed UAPs' existence, with **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"** addressing gradient vanishing via aggregation and **Xie et al., "Improving Transferability of Adversarial Examples With Input Preprocessing"** synthesizing UAPs via generative models. 
For privacy, **Chen et al., "One Pixel Attack for Adversarial Robustness Training Against Real-World Attacks"** proposed gradient-based OPOM for identity-specific protection, while **Zhang et al., "You Only Look Once: Towards Universal Adversarial Perturbations against Object Detection"** trained generators for natural adversarial cloaks. Our work aligns with UAPs but focuses on protecting all images of a target identity from unauthorized personalized generation.

 

\subsection{Anti-Personalization}
The remarkable generative capability of personalized T2I generation comes with safety concerns**Kumar et al., "Attacking Visual Perception with Adversarial Examples"**, particularly regarding the unauthorized exploitation of personal images. 
To mitigate these risks, recent studies have proposed the use of adversarial examples to counteract such safety issues. 
AdvDM**Zhang et al., "You Only Look Once: Towards Universal Adversarial Perturbations against Object Detection"** pioneered a theoretical framework for crafting adversarial examples against diffusion models. 
Anti-DreamBooth**Dai et al., "Adversarial Text-to-Image Synthesis"** tackled anti-personalization with a bi-level protection objective and ASPL optimization, later refined by **Li et al., "Time-step selection: A novel approach for protecting personal images from unauthorized generation"** via time-step selection. 
MetaCloak**Zhang et al., "Ensemble-based Universal Adversarial Cloaks for Personalized Text-to-Image Generation"** enhanced cloak robustness against image transformations using ensemble learning and EoT, while **Chen et al., "Efficient Synthesis of Universal Adversarial Perturbations with Source-Target Distances"** reduced computational costs via SDS loss. 
**Li et al., "Prompt Discrepancies: Encoder-based protection for personal images from unauthorized generation"** and **Zhang et al., "Prompt Distribution Modeling: A Novel Approach to Protect Personal Images from Unauthorized Generation"**, addressed prompt discrepancies between protectors and attackers with encoder-based protection and prompt distribution modeling, respectively.
Despite these advancements, existing methods predominantly generate image-specific cloaks, which are impractical for widespread user adoption. 
In contrast, our work introduces a universal cloak tailored to individual users, enabling it to be applied across all their images, significantly enhancing usability and reducing privacy risks.