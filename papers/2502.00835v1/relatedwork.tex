\section{Related Work}
\subsection{Loco-manipulation on Legged Systems}
 Known as loco-manipulation, the integration of locomotion and manipulation has gained significant attention as a promising and application-oriented research field, driven by advancements in the locomotion capabilities of legged systems.

Traditional model-based approaches typically require a precise model of both the robot and the manipulated object in order to perform trajectory planning and optimization aimed at achieving the desired task outcomes~\cite{zimmermann2021go, murooka2015whole, rigo2023contact, polverini2020multi}.
While optimization-based methods have demonstrated impressive results in applications such as box carrying~\cite{bellicoso2019alma} and door opening~\cite{sleiman2021unified}, they often necessitate meticulous coordination of multiple limb end-effectors, including attached manipulators, to ensure stable loco-manipulation~\cite{li2023multi, lin2024locoman, schakkal2024dynamic}.
Moreover, these model-based methods generally do not scale efficiently to an arbitrary number of contact points or varying positions.
For non-prehensile tasks involving large and heavy objects, such planning approaches are prone to the combinatorial explosion of contact modes, significantly complicating the solution process~\cite{cheng2023enhancing, smith2012dual}.

Reinforcement learning (RL) has emerged as a compelling alternative for achieving desired manipulation behaviors without the need for explicit robot-object interaction modeling or predefined contact point constraints.
End-to-end RL-based loco-manipulation has demonstrated success in applications such as soccer dribbling~\cite{ji2023dribblebot, hu2024dexdribbler}, button pushing~\cite{cheng2023legs, he2024learning}, and door opening~\cite{arm2024pedipulate, schwarke2023curiosity, zhang2024learning}.
However, due to the sparse interactions between the robot and manipulated objects, end-to-end learning often requires carefully crafted reward functions or sophisticated curriculum to effectively guide the robot in acquiring the desired manipulation skills~\cite{shi2021circus, fu2023deep}.

To alleviate the exploration challenges in learning-based loco-manipulation, researchers have proposed incorporating prior knowledge from model-based planning or demonstrations~\cite{sleiman2024guided, liu2024opt2skill} to enhance exploration during end-to-end training. 
In addition, task-agnostic exploration rewards have been utilized to facilitate efficient exploration for loco-manipulation~\cite{schwarke2023curiosity, zhang2024wococo}.

Furthermore, hierarchical control frameworks have been introduced to decompose tasks into manageable sub-problems. 
In this paradigm, a high-level planner selects sub-goals that are subsequently executed by a low-level policy.
The outputs of the high-level policy can take the form of target positions for the end-effector, base velocity commands, or other kinematic objectives~\cite{rigo2024hierarchical, kumar2023cascaded, wang2024hypermotion}.
Although the hierarchical framework has proven effective in applications such as precise soccer shooting~\cite{ji2022hierarchical}, whole-body pushing~\cite{jeon2023learning}, and collaborative manipulation~\cite{nachum2020multi}, its success remains contingent on the development of an effective high-level planner.

In our work, we build upon the hierarchical control framework for whole-body loco-manipulation, and decouple the high-level planning and low-level locomotion similarly to~\citet{jeon2023learning}. 
Our primary focus is on the sample-efficient training of high-level policies under sparse task rewards, particularly in complex scenarios where achieving effective manipulation trajectories poses significant challenges.

\subsection{Intrinsically Motivated Reinforcement Learning}
%MUTUAL INFORMATION STATE INTRINSIC CO
Intrinsic motivation (IM) \citep{rm2000intrinsic} plays a vital role in training RL agents, particularly in scenarios where extrinsic rewards are sparse or difficult to design.
Key mechanisms include curiosity~\cite{schmidhuber1991possibility}, 
%information gain~\cite{lindley1956measure}, 
learning progress~\cite{schmidhuber2010formal}, and empowerment~\cite{klyubin2005empowerment}, each promoting exploration and skill acquisition. 

Curiosity~\cite{schmidhuber1991possibility} incentivizes exploration by rewarding agents for encountering novel or surprising states. 
Usually modeled as the prediction error of the agent's world model~\cite{pathak2017curiosity, pathak2019self, burda2019exploration}, 
curiosity has been integrated to learning loco-manipulation skills for quadrupeds and humanoids~\cite{schwarke2023curiosity, zhang2024wococo}.

Learning progress~\cite{schmidhuber2010formal} measures the rate at which an agent improves its ability to predict or control the environment.
It encourages agents to focus on regions in the state space where learning is rapid, thus facilitating efficient exploration~\cite{blaes2019control}. 
Furthermore, learning progress can serve as a metric of task difficulty, guiding agents in independently developing curriculum for resourcefully learning more complex tasks~\cite{colas2019curious}.

Our work is most closely related to the third mechanism for IM, the idea of empowerment. 
Empowerment \citep{klyubin2005empowerment} is an information-theoretic quantity defined as the channel capacity between the agent's actions and its sensory observations. 
By rewarding the agent for seeking states where it has the greatest influence, empowerment helps the agent efficiently learn to solve tasks that require interactions with various entities in the environment ~\cite{mohamed2015variational,zhao2021mutual,jung2011empowerment}.
The intersection of RL and causality has recently been explored to improve interpretability, enhance sample efficiency, and facilitate the learning of better representations~\citep{buesing2018woulda, bareinboim2015bandits, lu2018deconfounding, rezende2020causally}. 
\citet{sontakke2021causal} investigate how agents can uncover the causal relationships between their actions and the environment.
Additionally, \citet{zhao2021mutual} employ mutual information (MI) to achieve similar objectives. 

Our work is inspired by the use of causal action influence~\cite{seitzer2021causal} as an intrinsic motivation signal, which provides a state-conditioned measure of an agent’s ability to influence its environment and can be conceptually defined as a lower bound of empowerment. 
Closely related to our work, \citet{pitis2020, urpicausal} also utilize influence detection, but for generating counterfactuals.

Although CAI’s effectiveness has been demonstrated in simplified simulated robotic environments, this work is to the best of our knowledge the first successful application within high-dimensional hierarchical control frameworks for real-world hardware environments.