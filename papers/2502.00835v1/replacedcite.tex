\section{Related Work}
\subsection{Loco-manipulation on Legged Systems}
 Known as loco-manipulation, the integration of locomotion and manipulation has gained significant attention as a promising and application-oriented research field, driven by advancements in the locomotion capabilities of legged systems.

Traditional model-based approaches typically require a precise model of both the robot and the manipulated object in order to perform trajectory planning and optimization aimed at achieving the desired task outcomes____.
While optimization-based methods have demonstrated impressive results in applications such as box carrying____ and door opening____, they often necessitate meticulous coordination of multiple limb end-effectors, including attached manipulators, to ensure stable loco-manipulation____.
Moreover, these model-based methods generally do not scale efficiently to an arbitrary number of contact points or varying positions.
For non-prehensile tasks involving large and heavy objects, such planning approaches are prone to the combinatorial explosion of contact modes, significantly complicating the solution process____.

Reinforcement learning (RL) has emerged as a compelling alternative for achieving desired manipulation behaviors without the need for explicit robot-object interaction modeling or predefined contact point constraints.
End-to-end RL-based loco-manipulation has demonstrated success in applications such as soccer dribbling____, button pushing____, and door opening____.
However, due to the sparse interactions between the robot and manipulated objects, end-to-end learning often requires carefully crafted reward functions or sophisticated curriculum to effectively guide the robot in acquiring the desired manipulation skills____.

To alleviate the exploration challenges in learning-based loco-manipulation, researchers have proposed incorporating prior knowledge from model-based planning or demonstrations____ to enhance exploration during end-to-end training. 
In addition, task-agnostic exploration rewards have been utilized to facilitate efficient exploration for loco-manipulation____.

Furthermore, hierarchical control frameworks have been introduced to decompose tasks into manageable sub-problems. 
In this paradigm, a high-level planner selects sub-goals that are subsequently executed by a low-level policy.
The outputs of the high-level policy can take the form of target positions for the end-effector, base velocity commands, or other kinematic objectives____.
Although the hierarchical framework has proven effective in applications such as precise soccer shooting____, whole-body pushing____, and collaborative manipulation____, its success remains contingent on the development of an effective high-level planner.

In our work, we build upon the hierarchical control framework for whole-body loco-manipulation, and decouple the high-level planning and low-level locomotion similarly to____. 
Our primary focus is on the sample-efficient training of high-level policies under sparse task rewards, particularly in complex scenarios where achieving effective manipulation trajectories poses significant challenges.

\subsection{Intrinsically Motivated Reinforcement Learning}
%MUTUAL INFORMATION STATE INTRINSIC CO
Intrinsic motivation (IM) ____ plays a vital role in training RL agents, particularly in scenarios where extrinsic rewards are sparse or difficult to design.
Key mechanisms include curiosity____, 
%information gain____, 
learning progress____, and empowerment____, each promoting exploration and skill acquisition. 

Curiosity____ incentivizes exploration by rewarding agents for encountering novel or surprising states. 
Usually modeled as the prediction error of the agent's world model____, 
curiosity has been integrated to learning loco-manipulation skills for quadrupeds and humanoids____.

Learning progress____ measures the rate at which an agent improves its ability to predict or control the environment.
It encourages agents to focus on regions in the state space where learning is rapid, thus facilitating efficient exploration____. 
Furthermore, learning progress can serve as a metric of task difficulty, guiding agents in independently developing curriculum for resourcefully learning more complex tasks____.

Our work is most closely related to the third mechanism for IM, the idea of empowerment. 
Empowerment ____ is an information-theoretic quantity defined as the channel capacity between the agent's actions and its sensory observations. 
By rewarding the agent for seeking states where it has the greatest influence, empowerment helps the agent efficiently learn to solve tasks that require interactions with various entities in the environment ____.
The intersection of RL and causality has recently been explored to improve interpretability, enhance sample efficiency, and facilitate the learning of better representations____. 
____ investigate how agents can uncover the causal relationships between their actions and the environment.
Additionally, ____ employ mutual information (MI) to achieve similar objectives. 

Our work is inspired by the use of causal action influence____ as an intrinsic motivation signal, which provides a state-conditioned measure of an agent’s ability to influence its environment and can be conceptually defined as a lower bound of empowerment. 
Closely related to our work, ____ also utilize influence detection, but for generating counterfactuals.

Although CAI’s effectiveness has been demonstrated in simplified simulated robotic environments, this work is to the best of our knowledge the first successful application within high-dimensional hierarchical control frameworks for real-world hardware environments.