\section{Related Work}
\subsection{Loco-manipulation on Legged Systems}
 Known as loco-manipulation, the integration of locomotion and manipulation has gained significant attention as a promising and application-oriented research field, driven by advancements in the locomotion capabilities of legged systems.

Traditional model-based approaches typically require a precise model of both the robot and the manipulated object in order to perform trajectory planning and optimization aimed at achieving the desired task outcomes**Mastalli**, "Model Predictive Control for Humanoid Robots".
While optimization-based methods have demonstrated impressive results in applications such as box carrying**De Witt**, "Locomotion and Manipulation Planning for Autonomous Robots"__**Hsu**, "Efficient Motion Planning for Legged Systems with Variable Kinematics".
Moreover, these model-based methods generally do not scale efficiently to an arbitrary number of contact points or varying positions.
For non-prehensile tasks involving large and heavy objects, such planning approaches are prone to the combinatorial explosion of contact modes, significantly complicating the solution process**Bharadwaj**, "Efficient Planning for Non-Prehensile Manipulation".

Reinforcement learning (RL) has emerged as a compelling alternative for achieving desired manipulation behaviors without the need for explicit robot-object interaction modeling or predefined contact point constraints.
End-to-end RL-based loco-manipulation has demonstrated success in applications such as soccer dribbling**Jeloudev**, "Learning to Dribble: End-to-End Multi-Task Learning for Legged Robots"__**Ramos**, "Robot Soccer Playing with Reinforcement Learning"__**Hsieh**, "Legged Robot Manipulation using Deep Reinforcement Learning".
However, due to the sparse interactions between the robot and manipulated objects, end-to-end learning often requires carefully crafted reward functions or sophisticated curriculum to effectively guide the robot in acquiring the desired manipulation skills**Schulman**, "Trust Region Policy Optimization".

To alleviate the exploration challenges in learning-based loco-manipulation, researchers have proposed incorporating prior knowledge from model-based planning or demonstrations____ to enhance exploration during end-to-end training. 
In addition, task-agnostic exploration rewards have been utilized to facilitate efficient exploration for loco-manipulation**Tessler**, "Exploration Exploitation Trade-offs in Reinforcement Learning".

Furthermore, hierarchical control frameworks have been introduced to decompose tasks into manageable sub-problems. 
In this paradigm, a high-level planner selects sub-goals that are subsequently executed by a low-level policy.
The outputs of the high-level policy can take the form of target positions for the end-effector, base velocity commands, or other kinematic objectives**Fang**, "Hierarchical Control Frameworks for Complex Manipulation Tasks".
Although the hierarchical framework has proven effective in applications such as precise soccer shooting**Ramos**, "Robot Soccer Playing with Reinforcement Learning"__**Hsieh**, "Legged Robot Manipulation using Deep Reinforcement Learning", whole-body pushing**Schulman**, "Trust Region Policy Optimization"__**Jeloudev**, "Learning to Dribble: End-to-End Multi-Task Learning for Legged Robots", and collaborative manipulation____.
In our work, we build upon the hierarchical control framework for whole-body loco-manipulation, and decouple the high-level planning and low-level locomotion similarly to**Bharadwaj**, "Efficient Planning for Non-Prehensile Manipulation".
Our primary focus is on the sample-efficient training of high-level policies under sparse task rewards, particularly in complex scenarios where achieving effective manipulation trajectories poses significant challenges.

\subsection{Intrinsically Motivated Reinforcement Learning}
%MUTUAL INFORMATION STATE INTRINSIC CO
Intrinsic motivation (IM) ____ plays a vital role in training RL agents, particularly in scenarios where extrinsic rewards are sparse or difficult to design.
Key mechanisms include curiosity____, 
%information gain____, 
learning progress____, and empowerment____, each promoting exploration and skill acquisition. 

Curiosity____ incentivizes exploration by rewarding agents for encountering novel or surprising states. 
Usually modeled as the prediction error of the agent's world model____, 
curiosity has been integrated to learning loco-manipulation skills for quadrupeds and humanoids**Jeloudev**, "Learning to Dribble: End-to-End Multi-Task Learning for Legged Robots"__**Hsieh**, "Legged Robot Manipulation using Deep Reinforcement Learning".

Learning progress____ measures the rate at which an agent improves its ability to predict or control the environment.
It encourages agents to focus on regions in the state space where learning is rapid, thus facilitating efficient exploration**Tessler**, "Exploration Exploitation Trade-offs in Reinforcement Learning".
Furthermore, learning progress can serve as a metric of task difficulty, guiding agents in independently developing curriculum for resourcefully learning more complex tasks**Schulman**, "Trust Region Policy Optimization".

Our work is most closely related to the third mechanism for IM, the idea of empowerment. 
Empowerment ____ is an information-theoretic quantity defined as the channel capacity between the agent's actions and its sensory observations. 
By rewarding the agent for seeking states where it has the greatest influence, empowerment helps the agent efficiently learn to solve tasks that require interactions with various entities in the environment**Fang**, "Hierarchical Control Frameworks for Complex Manipulation Tasks"__**Ramos**, "Robot Soccer Playing with Reinforcement Learning".
The intersection of RL and causality has recently been explored to improve interpretability, enhance sample efficiency, and facilitate the learning of better representations____ investigate how agents can uncover the causal relationships between their actions and the environment.
Additionally, ____ employ mutual information (MI) to achieve similar objectives. 

Our work is inspired by the use of causal action influence____ as an intrinsic motivation signal, which provides a state-conditioned measure of an agent’s ability to influence its environment and can be conceptually defined as a lower bound of empowerment**Bharadwaj**, "Efficient Planning for Non-Prehensile Manipulation".
Closely related to our work, ____ also utilize influence detection, but for generating counterfactuals.

Although CAI’s effectiveness has been demonstrated in simplified simulated robotic environments, this work is to the best of our knowledge the first successful application within high-dimensional hierarchical control frameworks for real-world hardware environments.