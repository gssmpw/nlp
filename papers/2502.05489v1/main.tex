% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
%\usepackage[review]{acl}
% \usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{rotating}
\usepackage{xcolor}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert} 

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Mechanistic Interpretability of Emotion Inference in \\Large Language Models
}


\author{
 \textbf{Ala N. Tak\textsuperscript{1,3,*}},
 \textbf{Amin Banayeeanzade\textsuperscript{2,3,*}},
 \textbf{Anahita Bolourani\textsuperscript{4}},
\\
 \textbf{Mina Kian\textsuperscript{3}},
 \textbf{Robin Jia\textsuperscript{3}},
 \textbf{Jonathan Gratch\textsuperscript{1,3}},
\\
 \textsuperscript{1}Institute for Creative Technologies, University of Southern California (USC),
\\
\textsuperscript{2}
Information Sciences Institute, University of Southern California (USC),
\\
 \textsuperscript{3}Department of Computer Science, University of Southern California (USC),
\\
 \textsuperscript{4}Department of Statistics and Data Science, University of California, Los Angeles (UCLA)
\\
 \small{
   \textbf{Correspondence:} \href{mailto:antak@ict.usc.edu}{antak@ict.usc.edu}, \textsuperscript{*}Equal contribution
 }
% \\
%  \small{.}
}

\begin{document}
\maketitle
\begin{abstract}

% We used Mechanistic Interpretability to reveal localized emotion representations in LLMs, validate their psychological alignment, and demonstrate causal control over emotion inference for better interpretability.

Large language models (LLMs) show promising capabilities in predicting human emotions from text. However, the mechanisms through which these models process emotional stimuli remain largely unexplored. Our study addresses this gap by investigating how autoregressive LLMs infer emotions, showing that emotion representations are functionally localized to specific regions in the model. Our evaluation includes diverse model families and sizes and is supported by robustness checks. We then show that the identified representations are psychologically plausible by drawing on cognitive appraisal theory—a well-established psychological framework positing that emotions emerge from evaluations (appraisals) of environmental stimuli. By causally intervening on construed appraisal concepts, we steer the generation and show that the outputs align with theoretical and intuitive expectations. This work highlights a novel way to causally intervene and precisely shape emotional text generation, potentially benefiting safety and alignment in sensitive affective domains.

% keywords: Mechanistic Interpretability, emotion detection and analysis, cognitive modeling

\end{abstract}

\section{Introduction}

\begin{figure}[!t]
\centering
  \includegraphics[width=0.9\columnwidth]{figs/intro_figs.pdf}
  \caption{
  Emotion inference through latent appraisal-like mechanisms in LLMs. Given the description of a situation, the model leverages internal \textit{appraisal} structures to recognize the emotion inferred from the context. For example, different perceptions of \textit{self-agency} can distinguish between \textit{guilt} and \textit{sadness}.
  }
  \label{fig:appraisal}
\end{figure} 


Large Language Models (LLMs) demonstrate remarkable capabilities in emotion recognition and reasoning tasks, occasionally surpassing human performance \cite{elyoseph2023chatgpt, tak2024gpt4emulatesaveragehumanemotional}. Prior research primarily engages with LLMs as black boxes, utilizing zero-shot inference or in-context learning to gauge their performance on tasks such as emotion classification \cite{yongsatianchot2023investigating, broekens2023fine}, emotional decision-making and situational appraisal \cite{10388119}, emotional intelligence \cite{wang2023emotional}, emotional dialogue understanding \citep{zhao2023chatgpt}, and generation of emotional text \cite{gagne2023inner}.
However, there remains a limited understanding of \emph{how} LLMs internally represent and process emotional information. Given LLMs' increasingly significant societal impact—spanning domains such as mental health \cite{2023_cognitive} and legal decision-making \cite{lai2024large}—investigating these internal mechanisms is crucial.

Cognitive neuroscience uses functional localization approaches to identify specific brain regions responsible for particular functions and manipulate them by up/down-regulating neural activations in those regions. Akin to the shift from behaviorism to cognitive neuroscience in psychology—i.e. from treating the mind as a black box to studying brain-based cognitive processes—Mechanistic Interpretability (MI) allows for moving from black-box techniques \cite{casper2024black}, to a focus on the internal mechanics of LLMs \cite{bereska2024mechanisticinterpretabilityaisafety}. MI can offer a fundamental understanding of how information processing is represented in LLMs, yielding fundamental insights into their inner-workings and offering new ways to control their reasoning \cite{li2021implicit, rai2024practical, feng2024monitoringlatentworldstates}. Building on this line of research and by drawing inspiration from emotion theory in psychology, we elucidate the inner workings of emotion processing in LLMs. 

In this work, we start by training linear classifiers on top of hidden representations to probe for regions where the strongest emotion-related activations occur. We provide evidence for functional localization of emotion processing and show that emotion-relevant operations are concentrated in specific layers, a consistent behavior across various model families and scales. We complement these findings by applying causal interventions, namely patching activations in the computation graphs, to identify essential components in neural representations \cite{conmy2023towards,patchscopes}. 
%specifically, we inspect the role of architectural components such as Multi-Head Self-Attention (MHSA) and Feed-Forward Network (FFN) \cite{geva2022transformer, 2024circuitreuse}. 
 As a result, we show that Multi-Head Self-Attention (MHSA) units in the mid-layers are responsible for shaping the LLM decision. To further corroborate this finding, we visualize attention patterns to show that these units consistently attend to tokens with high emotional importance. Our findings are robust and not influenced by variations in prompt wording or formatting.

Additionally, we use the \textit{appraisal theory} from psychology to shed light on the structure of LLMs' internal processing. According to appraisal theory \cite{frijda1989relations, scherer1984nature, smith1985patterns}, people reason about emotional situations by forming \textit{appraisal} judgments (see Figure~\ref{fig:appraisal}). We analyze the structure of emotion representations in LLMs by conducting inference-time probing on appraisal concepts to show that representations are psychologically plausible. Moreover, by modulating latent appraisal concepts to promote/demote a particular appraisal dimension (e.g., promoting self-agency), we show that the resulting changes in the output emotion align with theoretical expectations (from sadness to guilt) \cite{laguna2024beyond, wu2024replymakelovetal, li2024inferencetimeinterventionelicitingtruthful}. 

Overall, our work extends existing MI methodologies by applying them to more ecologically valid, unstructured examples, moving beyond the common practice of analyzing simplified sentence structures. Furthermore, this work serves as an early step to bridge MI techniques with applications in psychological and cognitive domains, offering insights into the inner workings of LLMs in complex, socially relevant contexts.

\section{Related Work}

\textbf{Appraisal Theory.}
Appraisal theory is a model that explains how peoples' emotions are a result of their evaluations of a situation \cite{lazarus1991emotion}. It provides a comprehensive framework for understanding the precursors of emotions \cite{smith2011role}. Neuroscience studies build on this framework by manipulating cognitive appraisals and examining associated brain activity, linking specific brain regions to appraisal processes \cite{leitao2020computational, kragel2024can, brosch2013comment}. These methods can be extended to evaluate how LLMs understand emotions, and identify the mechanisms responsible for those evaluations. 

% \subsection{LLMs' Emotion Capabilities}
% In their review, \citet{Yongsatianchot} investigate the emotion capabilities of LLMs. They identify two ongoing areas of research exploring the use of LLMs: emotion recognition and emotion generation. Within the domain of emotion recognition, many studies have approached the evaluation of LLMs through the perspective of appraisal \cite{yongsatianchot2023investigating,zhan2023evaluating,yongsatianchot2023s}, emotional intelligence \cite{wang2023emotional,paech2023eq,croissant2024appraisal,zhao2024both}, or emotional awareness \cite{elyoseph2023chatgpt}  theories. LLMs have also been evaluated in emotion generation tasks \cite{broekens2023fine,coda2023inducing,croissant2024appraisal,li2023large, vu2024psychadapter}. In addition to these ongoing areas of evaluation, \citet{Yongsatianchot} recommend that future research investigate \textit{how} these models are able to approach emotion-related tasks.

% % Croissant et al. (2023) chain of emotion prompting 


\textbf{Mechanistic Interpretability.}
 % Mechanistic interpretability (MI) aims to analyze the internal structures of models and understand how those structures are leveraged to perform various tasks \cite{rai2024practical}. %With the advent of increasingly more powerful LLMs, an emerging field in NLP focuses on understanding how these models perform various tasks.
 %As a result, various MI techniques are applied to assess transformer-based language models \cite{rai2024practical}.
 %A variety of MI methods are proposed to investigate the representations in LLMs including probing \cite{prob2019, belinkov2022probing},
 %circuit discovery \cite{nanda2023progress, wang2022interpretability, conmy2023towards, meng2022locating, prakash2024finetuningenhancesexistingmechanisms}
 %patching \cite{heimersheim2024use}, and generation steering \cite{templeton2024scaling}. 
Probing is an MI technique that uses a simple model, called a “probe”, to assess the internal representations across various layers in a model. As explained by \citet{belinkov2018internal}, the groundwork for what we now refer to as probing relates back to earlier work evaluating trained classifiers on static work embeddings to predict linguistic features \cite{kohn2015s,gupta2015distributional}, and classified hidden states of neural models \cite{ettinger2016probing,kadar2017representation,shi2016does,adi2016fine,hupkes2018visualisation,belinkov2022probing, giulianelli2018under}. Probing is used across a variety of tasks \cite{hewitt2019designing, tenney2019bert,tenney2019you,peters2018dissecting,clark2019does,belinkov2018internal,conneau2018you}. 

% The method of probing employed in our experiments most closely aligns with that described by \cite{hewitt2019designing}. 


%While probes investigate what a model’s internal representation is at each layer, Logit Lens explores what the model believes at each layer (nostalgebraist 2020). Logit Lens has been used to interpret transformer weight matrices (Halawi et al., 2023; Dar et al., 2023; Geva et al., 2022), and can be applied to a variety of tasks, such as assessing a models’ bias (Prakash and Lee 2023). Dar et al., 2023 attribute the popularization of this method to Elhage et al., (2022). While it is a popular method, we did not employ Logit Lens in our assessments for a few reasons. First, our data has emotion and appraisal labels, allowing us to train probes at various points in the model, Logit Lens is best employed when the task does not have labels or straightforward interpretations. The appraisal concepts used in our assessment are diverse and cannot be captured with a single token (ex: self-agency), making the vocabulary space unclear.

%%%% circuits
%Circuit identification is another technique in MI which is used to identify the regions in a network that are responsible for a computation that will support the specified downstream task. By granulating the interpretability task and building up from smaller subsections of the network, the complex problem of interoperability becomes somewhat more accessible \cite{cammarata2021curve}. The Circuits approach was initially applied to vision models 
%\cite{cammarata2021curve,olah2020zoom}, however, it was later leveraged as an approach with LLMs by \citet{elhage2021mathematical}.

%\textbf{Causal Intervention and Activation Modulation}
%Ablation, or Knockout as it is often referred to, is another MI technique that focuses on identifying the causal relationship between weights at a specific point in the model to the model's ability to perform a downstream task. An ablation test can be performed by replacing the output of a model component with a new vector \citet{rai2024practical} identify three methods for this test, 1) replacing it with a zero vector \cite{olsson2022context}, mean value of the randomly sampled inputs \cite{wang2022interpretability} and 3) resampling with a completely random input \cite{chen2023sudden}.
%A method similar to Ablation but more directed is activation patching 
Activation patching \cite{heimersheim2024use}, is a causal intervention used to identify if certain activations are important to the downstream task \cite{vig2020investigating}. By using patching, \citet{meng2022locating} were able to localize where models store factual information.
%The process of activation patching works as follows: first, develop two similar prompts, a source prompt and a destination prompt, then run the model with the source prompt. Next, run the model again with the destination prompt, but overwrite the internal activations with ones saved from the original run with the source prompt at key points in the model. Finally, observe the changes to the model output. This process is repeated for all activations of interest. 
Patchscope, a method that extends on activation patching, is used to translate LLM representations into natural language \cite{patchscopes}. 
%Testing with Concept Activation Vectors (TCAV) leverages high-level interpretations of a models internal state in terms of human-friendly concept (Kim et al. 2024). While originally developed for image processing, it has also been used as an approach for evaluations LLMs as well ()

% and show that different internal “circuits” are activated under zero-shot and few-shot conditions.

Yet another MI technique is generation steering. This method entails manipulating a model's activations to control the outputs \cite{rai2024practical, todd2024function}. \citet{geva2022transformer} investigated the model's prediction process, identifying the contributions of the FFN's output. To steer generation, they applied sub-updates promoting safety and were able to reduce the model's toxicity. 
%%\citet{templeton2024scaling} find that feature steering can be used to control the models' output. By "clamping" on the features for the Golden Gate Bridge, the model began to self-identify as the popular landmark in it's generated responses. They were also able to steer the model's stated goals, biases, and  
\citet{templeton2024scaling} find that "clamping" on features can be used to control the models' output, steering the model's stated goals and biases, for both desirable and undesirable outputs. 
\citet{nanda2023emergent} demonstrate that sequence models can have linear internal representations and that these representations can be used to manipulate the model's behavior. This method closely resembles those of \citet{turner2308activation} and \citet{lieberum2023does}.



%inference time intervention 

\begin{figure*}[ht!]
  \includegraphics[width=\linewidth]{figs/emotion_probe_heatmap.pdf}
  \caption {Layer-wise accuracies of emotion probe experiments across different models (each row) with varying depths at (\textbf{Left}) MHSA, (\textbf{Mid}) FFN, and (\textbf{Right}) hidden states. The results suggest an increasing signal with clear consolidation in the mid layers across various model families and sizes, which indicates that models predominantly make emotion-related decisions by the mid layers with minimal improvement in higher layers. %FFN outputs closely follow the hidden states, while attention accuracies exhibit some fluctuations even in the upper layers.
}
  \label{fig:e_probe_heat}
\end{figure*}
\section{Experimental Setup}

\textbf{Dataset and Prompt Design.}
We employ the crowd-enVENT dataset developed by \citet{envent}, which comprises 6,800 emotional vignettes annotated with self-reported emotions among a list of 13 options and 23 self-rated appraisal variables, reflecting nuanced stimuli evaluations, including: \textit{pleasantness/unpleasantness}, \textit{self-agency/other-agency}, \textit{predictability/suddenness}. Appendix~\ref{app:dataset} presents more details on the dataset, including a detailed list of appraisal variables, along with the scales used for measurement. 

To evaluate the model's ability to infer emotions from textual contexts, we design prompts that guide the model to predict the appropriate emotion as the next immediate output token, framing the task as a causal language modeling problem. Subsequently, we consider a classification problem and evaluate the model by inspecting the logits confined to the set of targeted emotion labels. The primary prompt template used in this study is shown in Figure~\ref{fig:aggregate_attention_viz}. 

Emotion attribution is inherently subjective, making it challenging to define a single ground truth label for each input, particularly given our fine-grained list of emotions. Thus, we focus on the correctly classified examples when inspecting each language model. In other words, we only analyze the data points for which the LLM and the human annotator agreed on the same label, totaling at least 2,700 samples among different language models (see Appendix~\ref{app:task} for more details). This ensures that we investigate tasks where the model performs reliably to understand the underlying mechanisms. %However, we acknowledge that some emotion classes have a limited number of examples after filtering, which may present constraints in our experiments. 

% , including, \textit{anger, boredom, disgust, fear, guilt, joy, neutral, pride, relief, sadness, shame, surprise, trust}. 
% . To identify the most significant factors for analysis, we employ Factor Analysis, reducing these variables to primary appraisal dimensions 

\textbf{Model Architecture.} To account for the impact of model scale and architectural variations, we evaluate a diverse set of model families and sizes, including Llama~3.2~1B Instruct and Llama~3.1~8B~Instruct \cite{llama3}, Gemma~2~2B~Instruct and Gemma~2~9B~Instruct \cite{gemma2}, OLMo~2~7B~Instruct and OLMo~2~13B~Instruct \cite{olmo2}, Phi~3.5~mini~Instruct and Phi~3~medium-Instruct \cite{abdin2024phi}, and Ministral~8B~Instruct and Mistral~12B~Nemo~Instruct \cite{mistral2024nemo} (see Appendix~\ref{app:arch} for more details). Some detailed analyses, robustness tests, and appraisal concept interventions are exclusively conducted on Llama~3.2~1B to manage computational resources effectively. 


\section{Notations and Preliminaries} \label{sec:notations}

% To locate the most salient emotion-related information in the language model, we specify several potential activation extraction locations within each transformer layer. 
Prior research suggests that both MHSA and Feed-Forward Network (FFN) units drive the generation in specific downstream tasks such as indirect object identification or concept promotion \cite{2024circuitreuse, geva2022transformer}. By examining activations immediately after these units, we aim to evaluate their respective contributions to emotion processing within each transformer layer. More formally, let \( \mathbf{h}^{(l)}_t \in \mathbb{R}^d \) denote the hidden state vector at layer \( l \) and token index \( t \in \{1, \cdots, T\} \), where \( d \) is the dimensionality of the model's hidden representations and $T$ is the input sequence length. Then,

\[
\mathbf{a}^{(l)}_t = \mathbf{MHSA}(\mathbf{h}^{(l-1)}_{1:t}),
\] \vspace{-2mm}
\[
\mathbf{m}^{(l)}_t = \mathbf{FFN}(\mathbf{h}^{(l)}_t + \mathbf{a}^{(l)}_t),
\]\vspace{-2mm}
\[
\mathbf{h}^{(l)}_t = \mathbf{h}^{(l-1)}_t + \mathbf{a}^{(l)}_t + \mathbf{m}^{(l)}_t,
\]


where \( \mathbf{a}^{(l)}_t \in \mathbb{R}^d \) and \( \mathbf{m}^{(l)}_t \in \mathbb{R}^d \) are MHSA and FFN outputs at layer \( l \) for token \( t \). \( \mathbf{h}^{(l-1)}_{1:t} \) are the previous layer's hidden states for tokens 1 to \( t \). 

Throughout this paper, we focus on activations $\mathbf{x}_t^{(l)}$ selected from one of the three candidates in  $\{ \mathbf{a}_t^{(l)}, \mathbf{m}_t^{(l)}, \mathbf{h}_t^{(l)} \}$ and study their properties at different layers and token positions. While activations can be extracted from any layer and token, we anticipate the strongest emotion signals to be present at the last token, as it directly influences the model's next-word prediction in a causal language modeling setup. Therefore, when clear from context, we omit the subscript $T$ while studying the last token. Also, we drop the superscript $(l)$ when generally discussing any activation across different layers.


\begin{figure*}[t!]
  \includegraphics[width=\linewidth]{figs/emotion_patch_heatmap.pdf}
  \caption {Results of activation patching experiments where we measure the success rate of transferring the output emotion by patching an activation from a source sample to a target sample. The patching is performed at (\textbf{Left}) MHSA, (\textbf{Mid}) FFN, and (\textbf{Right}) hidden states, respectively. The MHSA and FFN heatmaps demonstrate a clear localization with successful transfer peaks happening in the mid-layers consistently across various model families and scales. This observation aligns with the consolidation points observed in the probe heatmap (Figure \ref{fig:e_probe_heat}) and indicates how activation patching identifies critical regions for emotion prediction.
}
  \label{fig:e_patch_heat}
  \vspace{-3mm}
\end{figure*}
\section{Probing for Emotion Signals} \label{sec:emotion_probe}

Building on the linear representation hypothesis \cite{mikolov-etal-2013-linguistic, elhage2022toymodelssuperposition, parklinearhype}, we perform probing experiments to assess the presence and strength of emotion-related signals at different activations within the model. Specifically, we train linear classifiers \cite{hewitt2019designing} to predict the corresponding emotions. We formalize the linear classifiers as follows:

    \[
    \hat{\mathbf{y}} = \mathbf{W}^\top \mathbf{x} + \mathbf{b},
    \]

where \( \mathbf{x} \in \mathbb{R}^d \) denotes the activation vectors at one of the locations specified in the previous section. \( \mathbf{W} \in \mathbb{R}^{d \times C} \) is the weight matrix for emotion classification, \( \mathbf{b} \in \mathbb{R}^C \) is the bias vector, \( C \) represents the number of emotion classes, and \( \hat{\mathbf{y}} \in \mathbb{R}^C \) denotes the predicted logits for each emotion class.


We perform probing separately over different activation locations and layers across the model for the last token index. The probing results in Figure~\ref{fig:e_probe_heat}, measured as the accuracy on a held-out test set, indicate that the models begin consolidating emotional information in the hidden states $\mathbf{h}^{(l)}$ neither too early nor too late, but predominantly around the mid-layers across all models. For example, in the first row corresponding to Llama~3.2~1B in Figure~\ref{fig:e_probe_heat}, the emotional signal peaks by layer \( {l = 10} \) out of a total of $16$ layers. Beyond layer $10$, there is no significant increase in probe accuracy, suggesting that the model effectively captures emotional content by this stage. 

There is no clear distinction in probing performance between \( \mathbf{m}^{(l)} \) and \( \mathbf{h}^{(l)} \). Measurements from FFN closely track the hidden state dynamics, showing a steady increase in emotional conceptualization, peaking around the mid-layers. However, the heatmap corresponding to \( \mathbf{a}^{(l)} \) shows a more dispersed pattern while following the same consistent increasing trend observed in other locations. 
%The fluctuations observed even in the later layers suggest a specialization of attention units in each layer, with an increased potential for functional localization.

Our experiments reveal that emotion-processing mechanisms in LLMs are most pronounced in the middle layers across model families and sizes. Our observation aligns with the understanding that higher transformer layers capture more abstract and task-specific features. \textit{These findings suggest that the model has largely determined the output emotion by the mid-layers, with subsequent layers adding little additional processing. }

Lastly, to evaluate the hypothesis regarding the importance of the last token in causal modeling, we repeat the analysis on the last five tokens for Llama~3.2~1B in Appendix~\ref{app:token}. We observe a consistent increase in signal strength from earlier to later tokens, reinforcing the focus on the last token as the primary contributor to output generation.

\begin{figure*}[t!] 
\centering
 \begin{subfigure}[b]{.8\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/Prompt.pdf}
\end{subfigure}
\hfill
\par\bigskip\bigskip
\centering
 \begin{subfigure}[b]{1.0\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/attention_words.pdf}
\end{subfigure}
\caption {(\textbf{Top}) Primary prompt used in this study. Different segments of the prompt are colored differently. (\textbf{Bottom}) Most attended tokens at each layer from the perspective of the last token in Llama~3.2~1B. Layer 9 is the first layer in which the model attends to the tokens in the query with high emotional importance.}\label{fig:aggregate_attention_viz}
 \vspace{-3mm}
\end{figure*}

\section{Emotion Transfer by Activation Patching} \label{sec:patching}

Given evidence suggesting that the model's internal representation of emotional content stabilizes around the mid-layers, we explore causal intervention in these regions to test their functional importance. Specifically, we assess whether the output emotion of a \textit{source} example can be transferred to a \textit{target} example, with a different emotion, by selectively patching activations from the source computation graph into the inference pass of the target at corresponding locations, a method referred to as activation patching \cite{patchscopes}. 

Formally, let \( \mathbf{x}^{(l)}_t \) be the activation vector from the target example, and \( \hat{\mathbf{x}}^{(l)}_t \) be the activations from a different example, i.e. the source sentence, which has a mismatched label from the target. The patching operation involves replacing the activation at layer \( l \) and token \( t \) by substituting \(\mathbf{x}^{(l)}_t \leftarrow \hat{\mathbf{x}}^{(l)}_t\) and letting the model continue the processing flow in the following layers and tokens.

The goal is to determine whether substituting specific activations with those from another example can manipulate the model's final prediction to reflect the intended label. We conduct experiments by substituting activations at the last token and within a window spanning five layers, consistently across all model families and sizes\footnote{Smaller models are more sensitive to fewer transferred layers, while larger models resist emotion transfer and require a larger span. For consistency, we use the same window size across all models.}. %For example, to test activation patching at the last token of layer 10, we replace the activations of the last token in layers \({l  \in [8-12]} \) from the source into the target activations and evaluate the new emotion labels.

The results shown in Figure~\ref{fig:e_patch_heat} demonstrate consistent behavior across model sizes and families. When intervening on hidden state activations, we observe a clear increase in intervention success (Figure~\ref{fig:e_patch_heat} right). Take Llama~3.2~1B as an example. Patching hidden states in the early layers is totally non-effective. There is a critical point, e.g. layer 10 in this model, after which copying the hidden state transfers the emotion label with a high success rate. A high success rate at the final layers is naturally an expected behavior, as the residual stream of hidden states aggregates the total information as it gets closer to the final layers. However, remarkably, the chance of success peaks and stabilizes around the mid-layers in all models, an observation that aligns with our findings in the previous section.

To dig deeper, we look at the patching effect of MHSA and FFN units (see Figure~\ref{fig:e_patch_heat} left and middle, respectively), which provides clear evidence for functional localization of emotion processing. More precisely, interventions targeting \( \mathbf{a}^{(l)} \) and \( \mathbf{m}^{(l)} \) show clear evidence of success localized to specific mid-layers, e.g., MHSA units of layers \({l  \in [9-11]} \) in Llama~3.2~1B. In other words, successful patching of both MHSA and FFN units occurs only in a subset of layers and predominantly in the middle rather than the final layers, with FFN successful patching happening only slightly later than the MHSA patching. \textit{We hypothesize that there are a few consecutive layers in each language model whose MHSA units are responsible for gathering emotional information from the rest of the tokens and integrating it into the hidden state of the last token. This mechanism is immediately followed by a processing in the subsequent FFN units.}

To complement these findings, we perform an additional experiment, where a set of activations is knocked out in the forward pass to assess their impact. The results in Appendix~\ref{app:zero} consistently align with those of our activation patching and probing, reinforcing the evidence of functional localization in emotion processing. These observations hold across different models and experimental methods, providing evidence for the \textit{universality} of the identified units \cite{rai2024practical}.

Our results in all previous sections are not prompt-dependent. Specifically, changes in the format, wording, structure, or the number of demonstrations in the prompt do not affect the findings (See Appendix~\ref{app:prompt}). Additionally, we provide a control experiment in Appendix~\ref{app:control} to show that the identified units are not critical when performing a different but syntactically similar task. In fact, the final layers of the model are most critical for this syntactic task, which differs significantly from the units we found for emotion processing.

%Additionally, we explore whether the observed outcomes could be attributed solely to syntactic features and task structure by conducting an isomorphic experiment. We modified the task to focus purely on syntax—predicting the first word in the sequence—and repeated the activation patching procedure. As detailed in Appendix~\ref{app:control}, the final layers of the model are most critical for this syntactic task, which contrasts significantly with the emotion patching findings.

%\subsection{Attention Visualization}

To further explain our findings, we analyze the attention patterns in Llama~3.2~1B. Specifically, we record the top 3 tokens attended to by all attention heads in the last token of each layer. We conduct this analysis for all samples in the dataset, providing insight into which tokens are most frequently attended to. %For clarity, we categorize the prompt into multiple segments: initial instructions, demonstration samples, the query, and 
%lastly the pre- and post-syntax words, e.g., \textit{"\_Context"}, \textit{"\_Answer"}, and \textit{":"}.
%(See Figure~\ref{fig:aggregate_attention_viz} top).
Figure~\ref{fig:aggregate_attention_viz} presents the aggregate attention patterns. The results indicate that early layers primarily focus on syntax. Around the mid-layers, the model shifts its attention to emotionally relevant tokens in the context and maintains this focus until the final layers. In the last few layers, attention predominantly focuses on the last token in the sequence, suggesting that it primarily carries forward the last token's hidden state from earlier layers. These patterns provide evidence that the identified middle units are meaningfully related to emotion processing.

% However, when no examples are provided (zero-shot), the model's behavior deviates significantly. These findings suggest that the analyses and results in this study are primarily applicable to scenarios involving few-shot prompts. This raises the question of whether the identified regions are more fundamental than emotion processing itself, potentially serving as high-level enablers for emotion signals or the causal mechanisms behind emotion prediction (as demonstrated in our experiments).

\begin{figure}[t]
    \centering
  \includegraphics[width=0.75\linewidth]{figs/appraisal_emo_mappings.pdf} 
  \caption {Appraisal emotion associations extracted from the dataset. For example, when participants report \textit{anger}, they typically perceive a high degree of \textit{other-agency} and a low level of \textit{pleasantness} in the situation.}
  \label{fig:app_emo_cor}
  \vspace{-3mm}
\end{figure}

\section{Investigating Appraisal Concepts}
\label{sec:appraisal_modulation}

We draw inspiration from cognitive appraisal theory to show an existing emotion structure in LLMs' latent representations. Appraisals are known to have significant associations with emotions and, in some accounts, are considered causal factors in their emergence \cite{rosenman2001appraisal}.

\begin{figure}[t]
    % \centering
\includegraphics[width=0.85\columnwidth]{figs/emotion_to_appraisal_projection_7.pdf} 
  \caption {Cosine similarity of emotion vectors with \textit{pleasantness} and \textit{other-agency} appraisal vectors extracted from the hidden state of different Llama~3.2~1B layers. Layer number is written inside each marker.}
  \label{fig:emo_app_projection}
  \vspace{-3mm}
\end{figure}


Figure~\ref{fig:app_emo_cor} illustrates three primary appraisal dimensions and their associations with a set of basic emotions. For instance, \textit{guilt} and \textit{pride} are both materialized in situations with high self-agency, with the former happening in pleasant situations while the latter comes with an unpleasant experience. These mappings, which align closely with prior findings in appraisal theory \cite{Ellsworth2015}, are extracted from our dataset by taking the average appraisal score for each emotion label. 


We begin by training linear probes for appraisal signals within the model representations. In contrast to the linear classification probes used in Section~\ref{sec:emotion_probe}, here we solve multiple independent regression tasks for each appraisal. More formally, consider a set of $n$ appraisals and let $\mathbf{v}_a \in \mathbb{R}^d$ represent the weight vector corresponding to the appraisal $a \in \{1,\cdots,n\}$. Let $\mathbf{x}$ be an activation vector, as introduced in Section~\ref{sec:notations}. We train the regression weight $\mathbf{v}_a$ and the bias $b_a \in \mathbb{R}$ such that,

\[
    \hat{r}_a = \mathbf{v}_a^\top \mathbf{x} + b_a,
\]

where $\hat{r}_a \in \mathbb{R}$ is the estimate for the appraisal score $a$, given the input $\mathbf{x}$.
We train a separate appraisal probe for each layer per each appraisal at each hidden state of each layer. The weight matrices obtained in this way serve as representations of the corresponding appraisal, encoding features of the appraisal concept at the activations of the specified locations. The success of appraisal probing highly depends on whether the target concept is linearly detectable in the targeted activation. We provide the appraisal probing results in Appendix~\ref{app:appraisal_probing}, showing that the appraisal signals are not linearly detectable at earlier layers but are strongly present as we approach the hidden state of the final layers.

\section{Emotion-Appraisal Mappings}
\label{sec:app_emo_projection}
In this section, we analyze the representations of emotions and appraisals to reveal a structure within the latent LLM representations. Remember the weight matrix $\mathbf{W} \in \mathbb{R}^{d\times C}$ introduced in Section~\ref{sec:emotion_probe}. Let $\mathbf{w}_e \in \mathbb{R}^d$ represent the column $e$ of $\mathbf{W}$ corresponding to the emotion index $e \in \{1, \cdots, C\}$. We define the cosine similarity of appraisal $a$ with emotion $e$ as $\textit{sim($a$,$e$)}=\frac{\mathbf{v}^\top_a \mathbf{w}_e}{\norm{\mathbf{v}_a}_2 \norm{\mathbf{w}_e}_2}$.

Figure~\ref{fig:emo_app_projection} shows the similarity score of emotion vectors with two appraisal vectors, i.e. the pleasantness and other-agency, throughout the layers of Llama~3.2~1B. Notably, we observe psychologically plausible appraisal-emotion mappings across all layers. However, the projection strength peaks in the early layers and fades to near zero in the final layer, suggesting orthogonality in the final layers. %This observation is counter-intuitive to our earlier findings that emotion and appraisal signals are mostly linearly identifiable in the later layers. 

We hypothesize that in the earlier layers, there exists a meaningful structure on appraisal and emotion concepts, which aligns with our expectations from the appraisal theory. However, these concepts gradually decouple as the processing progresses through the network, and by the final layers, they become orthogonal and fully decoupled, reflecting the specialization of the network toward higher-level tasks. We finish this section by drawing the connection to our findings in previous sections. Notice that the decoupling starts around the critical layer, e.g. layer 10 in Llama~3.2~1B, which we identified in previous sections. Therefore, we conclude that \textit{the appraisals build a foundation to understand emotion representations in LLM hidden states, but the structure vanishes as we progress through the network.}

\section{Intervention on Appraisal Concepts} \label{sec:appraisal_intervention}

After finding the appraisal vectors, we investigate the possibility of indirectly modifying the emotion of an input example by modulating its appraisals within the model representations. For this purpose, we need to isolate the role of each appraisal \( a \), by considering its associated latent vector \( \mathbf{v}_a \) and distinguishing it from other appraisal vectors. More precisely, we define \(
    \mathbf{V}_{-a} := \left[ \mathbf{v}_1, \dots, \mathbf{v}_{a-1}, \mathbf{v}_{a+1}, \dots, \mathbf{v}_n \right]
\) by contacting all appraisal vectors except \( \mathbf{v}_a \). Next, we introduce the \textit{unique effect vector} of appraisal $a$ 
as $\mathbf{z}_a := (\mathbf{I} - \mathbf{P}_{-a}) \mathbf{v}_a$, where \( \mathbf{P}_{-a} = \mathbf{V}_{-a} (\mathbf{V}^\top_{-a}\mathbf{V}_{-a})^{-1} \mathbf{V}^\top_{-a}\) is the projection matrix onto the column space of \( \mathbf{V}_{-a} \) and $\mathbf{I}\in \mathbb{R}^{d \times d}$ is the identity matrix. We perform \textit{appraisal modulation} by injecting \( \mathbf{z}_a \) into the model's latent representation. More formally, the intervention is expressed as

\vspace{-3mm}
\[
    \mathbf{x} \leftarrow \mathbf{x} + \beta \frac{\mathbf{z}_a}{\norm{\mathbf{z}_a}_2},
\]

where \( \mathbf{x} \) on the RHS is the original latent representation, e.g., a hidden state vector from a specific layer and \( \beta \) is a scaling factor controlling the strength of the concept modulation. Notice that a positive $\beta$ corresponds to an appraisal promotion while a negative $\beta$ has the opposite effect of appraisal demotion. To measure the success of interventions, we evaluate the new emotion label derived by this modification and repeat this procedure across all examples. This modification is applicable to each layer of the model.

Figure~\ref{fig:app_modulation2} illustrates concept modulation results with different magnitudes of $\beta$ on layer $9$ of Llama~3.2~1B. We observe a remarkable alignment with theoretical and intuitive expectations. For instance, we observe that increasing the \textit{pleasantness} appraisal promotes both \textit{joy} and \textit{pride}, aligning with the fact that both of these emotions have high associations with the appraisal. 

In contrast to these results, the appraisal modulation when applied to earlier layers, does not generate psychologically valid results and is totally ineffective when applied to later layers. This observation matches the intuitions on the mechanism we provided earlier; Intervening on the early layers is not valid since modifications to latent representations are overwritten by emotion-specialized mid-layers. On the other hand, intervention on final layers is not effective because of the orthogonality of concepts as demonstrated in Section \ref{sec:app_emo_projection}. Appendix~\ref{app:appraisal_modulation} presents the full results, including intervention across all layers of Llama~3.2~1B.

\begin{figure*}[ht!]
  % \includegraphics[width=.9\linewidth]
  % {figs/app_modulation.png} \vfill
  \centering 
  \includegraphics[width=1.0\linewidth]
  {figs/app_surgery.pdf}
  \caption {
Results of appraisal concept modulation by intervening at layer~9 hidden states of Llama~3.2~1B for increasing scaling factors (\(\beta\)). \(|\beta| = 0\) represents the original distribution of emotions in the dataset. For example, promoting ($\uparrow$) or demoting ($\downarrow$) \textit{other-agency} significantly increases the share of \textit{anger} and \textit{guilt}, respectively. Similarly, promoting or demoting \textit{pleasantness} increases the share of \textit{joy}/\textit{pride} and \textit{sadness}/\textit{guilt}/\textit{anger} outputs, respectively. Additionally, promoting \textit{pleasant other-agency} significantly increases the share of \textit{joy} outputs, while the promotion of \textit{unpleasant other-agency} significantly increases the share of \textit{anger}.
  }
  \label{fig:app_modulation2}
  \vspace{-3mm}
\end{figure*}

Appraisal theory is also predictive of the situations in which two appraisals are promoted simultaneously. To test this capability in LLMs, we perform an intervention on the superposition of two appraisal dimensions: \textit{other-agency} and \textit{pleasantness}, with mathematical details provided in Appendix~\ref{app:appraisal_modulation}. The results, depicted in Figure~\ref{fig:app_modulation2}, show a successful promotion of emotion \textit{pride} with no further occurrences of \textit{joy}. These findings provide strong evidence that layer 9 in Llama~3.2~1B directly contributes to cognitive processes related to emotions (see Appendix~\ref{app:appraisal_modulation} for experiments with other appraisal concepts). Additionally, we provide complementary experiments such as direct emotion promotion using emotion vectors in Appendix~\ref{app:emotion_promotion}.

\section{Discussion}
\label{sec:discussion}
We employed mechanistic interpretability techniques to investigate the inner workings of emotion inference in LLMs. Our results reveal that mid-layer MHSA units within these models are responsible for processing emotional content. By applying linear algebraic manipulations to modulate the antecedents of emotions, i.e. the appraisal concepts, we steered the model outputs in controlled and predictable ways. This is particularly important for ensuring the \textit{reliability and steerability} of LLMs in high-stakes affective domains such as legal decision-making and clinical therapy. 

A key distinction of our work is that we grounded it on psychological theory and applied MI analysis on \textit{in-the-wild examples}, rather than relying on \textit{synthetically generated simplistic structures}, as seen in prior studies \cite{ 2024circuitreuse}. For example, \citet{wang2022interpretability} study the Indirect Object Identification task, by considering a fixed input structure, such as \textit{``person1 and person2 had fun at school. person2 gave a ring to”} where the model is expected to predict \textit{``person1”}. \citet{hanna2023doesgpt2} study the ``Greater-than" task using a dataset of examples like \textit{``The war lasted from 1517 to 15”}, where the model is expected to predict any two-digit number larger than 17. While it is possible to identify specialized circuits for such tasks, interpreting them effectively requires structured inputs, making it challenging to generalize findings to more naturalistic settings

Beyond these contributions, our work also highlights new opportunities for future research. Despite significant advancements in understanding human emotions, debates persist regarding the definition of emotion, the role of cognition in emotion, and the mechanisms underlying emotion inference \cite{ortony2022cognitive, ellsworth2003appraisal, moors2013causal, barrett2017theory}. In parallel, cognitive neuroscience has explored the neural basis of emotion in support of differing theoretical perspectives \cite{kragel2024can}. The study of LLMs, combined with insights from emotion theory and neuroscience, opens a unique intersection for advancing our understanding of emotions \cite{thornton_neuroscience}. 

Furthermore, our steering approach opens promising possibilities for conditioning LLMs to exhibit specific personality traits or moods, which could benefit applications requiring tailored affective responses \cite{jiang2023personality, jiang268032940personallm, petrov2024limited, suh2024rediscovering, li2024big5, suh2024rediscovering}. However, to ensure these interventions do not introduce unintended disruptions to other critical language-processing functions, it is essential to rigorously evaluate models on standard NLP benchmarks after inducing traits or moods.

Given LLMs’ increasing societal impact —spanning areas such as mental health, legal decision-making, and human-AI interaction—it is imperative to deepen our understanding of their internal mechanisms. Our study breaks new ground in the interpretability of emotion inference in LLMs, offering a novel way to causally intervene in emotional text generation. These findings hold promise for improving safety and alignment in sensitive affective domains. Moving beyond black-box approaches to rigorously test and refine LLM emotion processing will not only advance the field of LLM interpretability but also unlock new pathways for more responsible AI systems.

\section{Limitations}
\label{sec:Limitations}

In this study, we build upon the linear representation hypothesis \cite{mikolov2013exploiting,mikolov-etal-2013-linguistic, levy2014linguistic, elhage2022toymodelssuperposition}—the idea that high-level concepts are encoded linearly within model representations \cite{parklinearhype}. This hypothesis is particularly appealing because, if true, it could enable simple and effective methods for interpreting and controlling LLMs—an approach we leveraged to localize and manipulate latent emotion representations. However, despite recent notable efforts to formalize the notions of linearity \cite{parklinearhype} and orthogonality \cite{jiang2024uncovering} in model representations, further research is needed to enhance clarity and robustness in this area.

Furthermore, we demonstrated the ability to manipulate affective outputs by modifying appraisal concepts. Nevertheless, the precise nature of this relationship remains unclear—it is possible that appraisals are merely correlated with emotions rather than exerting a direct causal influence or that the relationship follows an inverse causal pattern. Establishing causality requires further investigation in future studies to disentangle directional dependencies. A deeper understanding of the interplay between LLM emotional inference, emotion theory, and neuroscience will be crucial for both theoretical insights and practical applications. Addressing these challenges will refine our understanding of LLMs and enhance their reliability in affective computing.

\section{Ethical Impact Statement}
\label{sec:ethic}
This study re-analyzes previously collected, de-identified data that had already undergone ethical review. The dataset is used for investigating the inner mechanisms by which auto-regressive LLMs process emotion. However, caution must be exercised when generalizing these results to models not examined in this work, to superficially similar tasks, or to different languages. Our analysis highlights potential concerns for those deploying LLMs in high-stakes affective domains or for generating emotionally charged content. Given the risks associated with emotional manipulation by LLMs, it is crucial to develop a deeper understanding of how these models process emotions. To this end, we advocate for further research in this domain to ensure that LLMs align with ethical standards and human-centered AI principles.


% Although our results are broadly consistent across models, we observe notable variations, particularly as model scale increases. Larger models exhibit less pronounced functional localization, likely due to significant redundancy and parallel processing pathways. Future research should investigate whether functional localization can be explicitly encouraged, potentially by modifying training objectives or architectural designs to balance interpretability with performance. 

%However, a limitation of our study is that the concept vectors we employ are inherently noisy. Future work should explore techniques to refine these representations, such as constructing synthetic concept datasets that generalize to \textit{out-of-distribution} examples and diverse real-world applications.

% Variations among models of similar size can be attributed to architectural design choices, such as the extensive use of dropout (e.g., in model X) or differing normalization strategies (e.g., both pre- and post-normalization in model Y). 
% A detailed overview of these architectural elements is provided in Appendix~\ref{sec:zeroactive}, enabling readers to evaluate their potential impact further. 

% Lastly, our study raises intriguing questions about task-specific localization. For instance, if many tasks exhibit similar functional localization patterns in their decision-making processes, does this undermine the uniqueness of our findings on emotion processing? Future work should systematically analyze whether mid-to-upper layer localization is common across tasks and assess its implications for understanding functional localization.  

% Our results showed divergence in model behavior depending on whether prompt templates included examples (few-shot) or did not (zero-shot). Investigating why such divergences occur could shed light on how models internalize and utilize contextual information during inference.

% \section*{Acknowledgments}


\bibliography{main}

\clearpage 

\appendix
\input{appendix}

 

\end{document}