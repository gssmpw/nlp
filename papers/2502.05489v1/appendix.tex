\section{Experimental Details}


\subsection{Dataset Details} \label{app:dataset}
For this project, we employed the crowd-enVENT dataset. \citet{envent} developed the dataset by asking crowdsource writers to share an event that made them feel a particular emotion. The participants were then asked to evaluate their subjective experiences during that event, including their perceived appraisals. Both were rated on a 5-point Likert scale with 5 representing the highest agreement. 
%Finally, their written account of the event was shown to annotators who were tasked with identifying the emotion of the writer and their appraisals of the event.

The dataset is comprised of examples from the following list of emotions: \textit{Joy, Pride, Surprise, Trust, Relief, Neutral, Boredom, Sadness, Fear, Guilt, Shame, Disgust, and Anger}. The dataset has 500 examples for each emotion, except for guilt and shame, which have 250 samples. The events were appraised along the following dimensions:  
\textit{pleasantness, other-agency, predictability, suddenness, familiarity, unpleasantness, goal-relatedness, 
own responsibility,  situational responsibility, goal support,
consequence anticipation, urgency of response, own control, others’ control, situational control, accepted control, internal standards, external norms, attention, not considered, and effort}. We selected a subset of dimensions for our analyses, previously shown to have a high association with emotions \cite{Ellsworth2015, tak2024gpt4emulatesaveragehumanemotional}.


To give a few examples from the dataset, the emotion label for the sentence ``I baked a delicious strawberry cobbler'' is \textit{pride}, with appraisals $\text{\textit{pleasantness}}=5$, $\text{\textit{other-agency}}=1$, while ``A housemate came at me with a knife'' is an example of \textit{fear} with $\text{\textit{pleasantness}}=1$ and $\text{\textit{other-agency}}=5$.

\begin{table*}[ht]
\centering
\tiny
\caption{Architectural details of the language models used in this study.}
\begin{tabular}{ cccccc }
\hline
 & Llama~3.2~1B Instruct& Llama~3.1~8B Instruct& Gemma~2~2b-it& Gemma2~9b-it &Ministral~8B Instruct
 \\
\hline
Parameters & 1B & 8B & 2B & 9B  &8B\\  
hidden size $d$ & 2048& 4096& 2304& 3584& 4096\\  
Layers & 16& 32& 26& 42&36\\   
layer norm type & RMSNorm & RMSNorm & RMSNorm & RMSNorm & RMSNorm\\  
Non-linearity & SiLU& SiLU& GeLU& GeLU&SiLU\\  
Feedforward dim & 8192& 14336& 9216& 14336&12288\\  
Head type & GQA  & GQA  & GQA  & GQA &GQA \\  
Num heads & 32& 32& 8& 16&32\\  
Num KV heads & 8& 8& 4& 8 & 8\\    
Context Window& 131072& 131072& 8192& 8192&32768\\  
Vocab size & 128256& 128256& 256000& 256000&131072\\
Tied embedding & True& False& True& True&False\\
\hline 
\end{tabular}
\newline
\newline
\newline
\newline
\newline
\centering
\begin{tabular}{ cccccc }
\hline
 & Mistral~12B Nemo Instruct& Phi~3.5 mini Instruct& Phi~3~medium Instruct& OLMo~2~7B Instruct & OLMo~2~13B Instruct\\
\hline
Parameters & 12.2B& 3B & 14B& 7B& 13B\\  
hidden size $d$ & 5120& 3072 & 5120& 4096& 5120\\  
Layers & 40& 32 & 40& 32& 40\\   
layer norm type & RMSNorm & RMSNorm & RMSNorm & RMSNorm& RMSNorm\\  
Non-linearity & SiLU& SiLU& SiLU& SiLU& SiLU\\  
Feedforward dim & 14336& 8192&  17920& 11008& 13824\\  
Head type & GQA & Multi-Head & GQA & Multi-Head & Multi-Head \\  
Num heads & 32& 32 & 40& 32 & 40\\  
Num KV heads & 8& 32& 10& 32 & 40\\   
Context Window & 131072& 131072& 131072& 4096& 4096\\  
Vocab size & 131072& 32064& 32064& 100352& 100352\\
Tied embedding & False& False& False& False& False\\
\hline 
\end{tabular}
\label{tab:model_arch}
\end{table*}


\subsection{Architecture and Model Details} \label{app:arch}

In this paper we experimented with ten LLMs: meta-llama/Llama-3.2-1B-Instruct and meta-llama/Llama-3.1-8B-Instruct \cite{llama3}, google/gemma-2b-it and google/gemma-2-9b-it \cite{gemma2}, allenai/OLMo-7B-Instruct and allenai/OLMo-2-1124-13B-Instruct \cite{olmo2}, microsoft/Phi-3.5-mini-instruct and microsoft/Phi-3-medium-128k-instruct \cite{abdin2024phi}, and mistralai/Ministral-8B-Instruct-2410 and nvidia/Mistral-NeMo-12B-Instruct \cite{mistral2024nemo}.  The architectural details for these language models are provided in Table~\ref{tab:model_arch}. Unless stated otherwise, the default model used in this paper is Llama~3.2~1B since it is the lightest model, allowing for efficient analysis.

All models were implemented using the \href{https://huggingface.co/models}{Hugging Face framework}\footnote{\url{https://huggingface.co/models}}, leveraging the respective model weights, with additional integration of libraries such as TransformerLens \cite{nandatransformerlens2022} to enable the hooking and intervention on hidden states and activations.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs_appendix/open_vocab_result.pdf}
  \caption{Llama~3.2~1B open vocab generation of emotions, comparing the true label to the predicted label through open vocab prediction}
  \label{fig:Llama1B_open_vocab}
\end{figure} 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\columnwidth]{figs_appendix/confusion_matrix.pdf}
  \caption{Confusion matrix comparing the true labels to Llama~3.2~1B predicted labels}
  \label{fig:Llama1B_confusion_matrix}
\end{figure} 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figs_appendix/emotion_distribution_models.pdf}
  \caption{The distribution of next word emotion label predictions. The self-reported human labels are shown on the top row. The subsequent rows demonstrate the distribution of the predicted emotion labels from each model.}
  \label{fig:models_pred_distribution}
\end{figure*} 

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs_appendix/emotion_accuracy_models.pdf}
  \caption{Comparison of models' accuracy on the emotion classification task. This experiment is performed across all samples in the dataset.}
  \label{fig:models_accuracy}
\end{figure} 

\subsection{Task Details} \label{app:task}

Emotion attribution is a challenging and subjective task, one that even humans often find difficult. The accuracy of third-person human annotations is approximately 50\% in this dataset, as calculated using 1000 bootstrap resampling with 95\% confidence intervals (the chance baseline being around 7\%) \cite{envent}. Thus, it is not a surprise that defining a ground truth emotion label for each sample is challenging. Consequently, it is reasonable to expect that this task would also be challenging for LLMs. Figure \ref{fig:Llama1B_open_vocab} demonstrates the emotion label prediction on an open vocabulary, for Llama~3.2~1B. As the figure suggests, the LLM vocab choice in the output rarely matches the human-reported emotion label.

Therefore, we avoid doing an open-vocab generation, but instead, we confine the logits of LLM output to the set of emotion labels in the dataset. With this modification, we achieve an accuracy of approximately 40\% or higher using self-reported ground truth emotions across all architectures and scales. Figure \ref{fig:Llama1B_confusion_matrix} demonstrates the closed vocabulary results through a confusion matrix for Llama~3.2~1B, comparing the true and predicted labels. Furthermore, Figure~\ref{fig:models_pred_distribution} illustrates the emotion label predictions across the LLMs tested, and Figure~\ref{fig:models_accuracy} demonstrates the accuracy results from these experiments.

For the rest of our analysis, we only focus on the correctly classified examples, which ensures at least 2,700 data points or more across different model architectures. We apply this filtering method to ensure that the samples selected for experimentation are ones where the LLM understood the emotion labeling task, allowing us to properly investigate the underlying mechanisms that led to the models' selected emotion label.  
However, we acknowledge that some emotion classes have a limited number of examples after filtering, which may present constraints in our experiments. 

\section{Knockout Experiment} \label{app:zero}
In this section, we elaborate on a causal approach in MI commonly referred to as \textit{ablation}, \textit{knockout}, or \textit{zero/random activation intervention} \cite{rai2024practical, olsson2022context, wang2022interpretability,chen2023sudden}. This is a complementary approach to the activation patching experiment to provide further evidence for the localization of emotion processing in LLMs. Precisely, we zero out the activations at different points in the model or replace them with random activations and assess the impact on the generated output label. In other words, we intervene in the activations in the forward pass of the model at MHSA, FFN, and the hidden states at the last token across different layers.
Formally, let \( \mathbf{x}\) be the activation vector from the target example at a specific layer and location. The zero-activation operation involves replacing the activation by substituting \(\mathbf{x} \leftarrow \mathbf{0}\) and letting the model continue the processing flow in the following layers. Similarly, for random intervention, let \( \mathbf{x} \) be the target activation to interrupt. The random activation intervention replaces \( \mathbf{x} \) by substituting
\[
\mathbf{x} \leftarrow \frac{\norm{\mathbf{x}}}{\norm{\mathbf{r}}}\mathbf{r},
\]

where \( \mathbf{r} \in \mathbb{R}^d \) is sampled from a standard Gaussian $\mathcal{N}(\mathbf{0}, \mathbf{I})$ distribution and the normalization factor $\frac{\norm{\mathbf{x}}}{\norm{\mathbf{r}}}$ ensures that the new activation has the same norm as the original one.

The modified activations propagate forward, affecting the model's outputs. Then, we compare the prediction coming out of the modified logits with the clean forward pass to measure the model's accuracy after the intervention. The lower this accuracy is, the more significant that activation is affecting emotion label generation.


In Figure~\ref{fig:zero_all}, knockout-intervention across all models, we find remarkably consistent behavior with our probing and patching results provided in Sections~\ref{sec:emotion_probe} and ~\ref{sec:patching}—that after a certain point, even removing all MHSA units and, to some extent, the FFN units do not impact the final emotion classification accuracy. This indicates that the model's internal representation of the emotional content is established before that point. Additionally, we observe that knocking out activations with both zero and random interventions at \( \mathbf{a}^{(10)} \) has a significantly greater impact than \(\mathbf{m}^{(10)} \), which suggests that the MSHA unit in mid layers plays a more crucial role in collecting the emotion label from previous tokens.

For example, given the first row in the plot, we observe that zeroing out \( \mathbf{a}^{(9-11)} \) and \( \mathbf{m}^{(9-11)}\) LLama~3.2~1B has the greatest impact on the emotion label (corresponding to each example). As expected, activations at \( \mathbf{h}^{(l)} \) have a significant impact throughout all layers since they constitute the mainstream of the forward path reaching the model's output and are fundamentally different from \( \mathbf{a}^{(l)} \) and \( \mathbf{m}^{(l)} \), which contribute additional processing to the residual stream. 

\clearpage
\begin{sidewaysfigure*}[bht]
    \centering
    \includegraphics[width=0.99\textwidth]{figs_appendix/emotion_all_heatmap.pdf}
    \caption{
    Comparison of probing, zero-activation and random-activation interventions, and activation patching on MSHA, FFN, and hidden state units across all layers of Llama~3.2~1B.  
    The probe heatmap shows accuracy on the holdout set, zero/random activation interventions measure the model accuracy after disrupting causal pathways, and patching heatmaps indicate how effectively outputs transfer from source to target examples. The span sizes of $3$ and $5$ are used for the presented knockout interventions and patching experiments. This means that we intervene simultaneously on three/five consecutive layers, with the center being the indicated layer.
    }
    \label{fig:zero_all}
\end{sidewaysfigure*}
\clearpage



\clearpage
\section{Case Studies on Llama~3.2~1B}
In this section, we focus on the Llama~3.2~1B language model and investigate the validity of our findings from multiple aspects. In Section~\ref{app:detailed_figures}, we provide detailed results on emotion probing, activation patching and knockout interventions. In Section~\ref{app:token} we extend our studies to include token dimension. Finally, we show that our results are robust to prompt design by conducting experiments using several hand-designed prompts in Section~\ref{app:prompt}. 


\begin{figure}[bht]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs_appendix/emotion_probe_last_token.pdf}

    \includegraphics[width=1.0\columnwidth]{figs_appendix/non_linear_emotion_probe_last_token.pdf}
    
  \caption{Probing test accuracy on last token of llama~3.2~1B for all layers. \textbf{Top} linear probe, \textbf{bottom} non-linear probe. There is a noticeable increase in probe performance in early layers when using a simple non-linear probe.}
  \label{fig:Llama1B_probe}
\end{figure} 


\begin{figure}[bht]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs_appendix/zero_intervention_last_token_span_1.pdf}
  \caption{Zero-Intervention accuracy with span $1$ on the last token index of Llama~3.2~1B across all layers. There is a clear drop in accuracy when MHSA activations in layer 10 are knocked out.}
  \label{fig:Llama1B_zero}
\end{figure} 

\begin{figure}[bht]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs_appendix/random_intervention_last_token_span_1.pdf}
  \caption{Random-Intervention accuracy with span $1$ on the last token index of Llama~3.2~1B across all layers. A noticeable drop in accuracy is observed when MHSA activations in layer 10 are knocked out.}
  \label{fig:Llama1B_random}
\end{figure} 


\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figs_appendix/activation_patching_3.pdf}
    \caption{Activation patching results for Llama~3.2~1B across different layers (FFN, MHSA, and hidden state units) with Span = 3, evaluated over 200 source-target pairs. Blue indicates unsuccessful patching where the original label remained unchanged, red represents successful patching, and green denotes cases where the label changed but did not match the exact expected target.} 
    \label{fig:Llama1B_patching}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figs_appendix/activation_patching_all_spans_MHSA.pdf}
    \caption{Effect of span size on Llama~3.2~1B activation patching at MHSA across different layers, evaluated over 200 source-target pairs.} 
    \label{fig:Llama1B_span_size}
\end{figure*}

\subsection{Details on Probing and Causal-Intervention} \label{app:detailed_figures}
Here, we report the results of probing and causal intervention experiments on the Llama 3.2 1B model, focusing on the last token index across all layers, along with a simplified illustration of the findings. Figure~\ref{fig:Llama1B_probe} top demonstrates linear probing results.

It is noteworthy that we use linear probes to detect and extract emotion vectors. Low probe accuracy on earlier layers does not mean that there is no emotion signal at early layers but rather suggests that the signal is not \textit{linearly} identifiable. In fact, Figure~\ref{fig:Llama1B_probe} bottom shows that when using a non-linear probe, i.e. a simple neural network with one hidden layer, the probe on earlier layers boosts considerably. However, both linear and non-linear probes peak around layer 10. After this layer, the model has finalized its output label decision and no major change happens. 

Intervention experiments further support this observation. Figures~\ref{fig:Llama1B_zero} and \ref{fig:Llama1B_random} show the effects of zero and random interventions with a span of 1, revealing a clear drop in accuracy when knocking out activations at layer 10. Finally, Figure~\ref{fig:Llama1B_patching} provides a detailed visualization of the patching experiment. The left-most plot highlights that the most successful emotion transfers occur at layer 10, which also exhibits the lowest number of unchanged labels. Notably, while some labels shifted to semantically similar emotions, they did not exactly match the target label and were, therefore, not counted as successful patches.



\subsection{Effect of Span Size on Activation Patching} \label{app:Span}
To examine the effect of span size on patching success, we repeated the experiment with three span sizes—1, 3, and 5 layers—on Llama 3.2 1B. Figure~\ref{fig:Llama1B_span_size} presents the results, showing a clear increase in patching effectiveness as the span size increases. Notably, layer 10 continues to exhibit peak patching performance, reinforcing the idea of emotion-related functional localization in that layer.

% Moreover, our findings suggest the existence of an optimal span size: small spans show negligible success, while excessively large spans risk disrupting unrelated functions in the model’s causal process. Identifying this balance is crucial for effective interventions, and future research should further investigate the trade-offs involved in span selection.



\subsection{Investigating Token Dimension} \label{app:token}

\begin{figure*}[bht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/emotion_probe_heatmap.pdf}
  \caption{Probing test accuracy on different tokens of Llama~3.2~1B across all layers. We observe a consistent increase in signal strength from earlier to later tokens and from lower to higher layers.}
  \label{fig:Llama1B_probe_all_tokens}
\end{figure*} 

\begin{figure*}[bht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/zero_intervention_span_1.pdf}
  \caption{Zero intervention accuracy on different token indices of llama~3.2~1B for all layers with span = $1$. The vertical dimension shows tokens and \textit{"all"} means to knock out all activations of the specific layers at the same time. MHSA units beyond the critical layer (\( l = 10 \)) have minimal impact on the causal path, even when an entire layer is knocked out.}
  \label{fig:Llama1B_zero_all_tokens}
  % \vspace{-2mm}
\end{figure*} 

\begin{figure*}[bht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/random_intervention_span_1.pdf}
  \caption{Random intervention accuracy on different token indices of Llama~3.2~1B across all layers with span = $1$. The vertical axis represents token positions, where \textit{"all"} denotes the simultaneous knockout of all activations in the specified layers. Notably, MHSA units beyond the critical layer (\( l = 10 \)) contribute minimally to the causal path, even when an entire layer is deactivated.}
  \label{fig:Llama1B_random_all_tokens}
\end{figure*} 

\begin{figure*}[bht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/activation_patching_span_3.pdf}
  \caption{Results of activation patching success on different tokens of Llama~3.2~1B across all layers with span = 3. Clear functional localization is observed in layers 9–11.}
  \label{fig:Llama1B_patching_all_tokens}
\end{figure*} 

Throughout the paper, we extracted and analyzed the last token index activations $\mathbf{x}^{(l)}$---selected from one of the $\{ \mathbf{a}^{(l)}, \mathbf{m}^{(l)}, \mathbf{h}^{(l)} \}$---across different layers of all tested models. We hypothesized that the strongest emotion signals appear at the last token, as it directly influences the model's next-word prediction in a causal language modeling setup. Earlier work demonstrates the presence of strong causal states immediately before the prediction, as well as their emergence at the final token of a noun phrase \cite{meng2022locating}. Therefore, we suspect whether the last token of the query part in the prompt contains information more significant than the final token of the whole prompt.

To evaluate this hypothesis, we repeated the experiment on the last five tokens for Llama~3.2~1B extending the analysis to also include the final tokens in the query context (colored purple in Figure~\ref{fig:aggregate_attention_viz}-Top). In Figure~\ref{fig:Llama1B_probe_all_tokens}, we observe a consistent increase in signal strength from earlier to later tokens, reinforcing the focus on the last token as the primary contributor to output generation, but no clear importance on the last token of the query part. 

Similarly, we conduct zero-activation intervention, random-activation intervention, and activation patching on Llama~3.2~1B’s last five tokens across all layers. Figures~\ref{fig:Llama1B_zero_all_tokens}, \ref{fig:Llama1B_random_all_tokens}, and \ref{fig:Llama1B_patching_all_tokens} confirm our hypothesis, suggesting that the last token's MSHA units in mid-layers, particularly \({l = 10}\), are critical for processing emotional content, while other token positions exhibit no localization.

Furthermore, the first row of Figures~\ref{fig:Llama1B_zero_all_tokens} and \ref{fig:Llama1B_random_all_tokens} illustrates the effect of zero or random activation interventions applied to all token positions in a layer (i.e., when all units in a layer are knocked out). Notably, the results indicate that MSHA units beyond the critical layer \({l = 10}\) contribute minimally to the causal path, even when an entire layer is knocked out.

\subsection{Control Experiment on an Isomorphic Task} \label{app:control}

\begin{figure}[bht]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs_appendix/control_probe_last_token.pdf}
  \caption{Probing test accuracy of the control isomorphic experiment on the last token of Llama~3.2~1B across all layers.}
  \label{fig:Llama1B_control_probe}
  % \vspace{-3mm}
\end{figure} 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figs_appendix/control_activation_patching_3.pdf}
    \caption{Control isomorphic experiment results for Llama~3.2~1B activation patching across different layers at MHSA, FFN, and hidden states with span = 3, evaluated over 200 source-target pairs. Localization is less evident, with the highest patching performance observed around the final layers.} 
    \label{fig:Llama1B_control_patching}
\end{figure*}


Here, we investigate whether the observed outcomes could be attributed solely to syntactic features and task structure rather than the target task of emotion processing. To assess this, we conduct an isomorphic experiment in which we modify the task to focus purely on syntax—predicting the first word in the sequence—and repeat the activation patching procedure as described in Section~\ref{sec:patching}. The altered prompt is shown below:

\begin{quote}
\textbf{What is the first word in the following contexts?}  
Context: My dog died last week. Answer: \textbf{My};   
Context: I saw moldy food. Answer: \textbf{I};   
Context: I could see my friend after a long time. Answer:
\end{quote}

As shown in Figures~\ref{fig:Llama1B_control_probe} and \ref{fig:Llama1B_control_patching}, MHSA units in the final layers of the model are most critical for this syntactic task, which contrasts significantly with the emotion patching findings. Additionally, we observe weaker evidence of functional localization based on the patching results from the isomorphic control experiment.


\subsection{Robustness to Prompt Design} \label{app:prompt}

To evaluate the model's robustness to various prompts, we designed a variety of prompt templates, as illustrated in Table \ref{table:prompt_templates}.
Figure \ref{fig:prompt_distribution} demonstrates the distribution of the next word emotion label predictions for the different prompt templates and different numbers of few-shot examples. Figure \ref{fig:prompt_accs} demonstrates the final accuracy of these tests. Noteworthy that again in the following interpretability experiments, we confine our focus only on the samples that the model could predict correctly using each specific prompt.

Figures \ref{fig:prompt_probe} and \ref{fig:prompt_patch} validate that the probing and patching results presented earlier in the paper are robust to various prompt templates. For probing, we see that the model has determined the predicted label by the mid layers of the model, a result that is consistent across various prompt templates and the number of few shot examples provided. For activation patching, we also get consistent results across various prompt templates and with varied numbers of few-shot examples. 


%mention figures \ref{fig:prompt_accs}, \ref{fig:prompt_distribution}, \ref{fig:prompt_probe}, \ref{fig:prompt_patch}.
%Also, please add these prompt templates into a table and mention we used them:


\begin{table}[ht]
% \centering
\tiny
\begin{tabular}{|p{0.02\columnwidth}|p{0.85\columnwidth}|}
\hline
\# & Prompt Template                                                                                                                                                                                                                                                                                                                                                              \\ \hline
1      & \begin{tabular}[c]{@{}l@{}}What are the inferred emotions in the following contexts?\\ Context: My first child was born.\\ Answer: joy\\ Context: My dog died last week.\\ Answer: sadness\\ Context: {[}Input{]}  \\ Answer:\end{tabular}                                                                                                                                       \\ \hline
2      & \begin{tabular}[c]{@{}l@{}}Consider this list of emotions: anger, boredom, disgust, fear, guilt, joy,\\ pride, relief, sadness, shame, surprise, trust, neutral.\\ What are the inferred emotions in the following contexts?\\ Context: My first child was born.\\ Answer: joy\\ Context: My dog died last week.\\ Answer: sadness\\ Context: {[}Input{]}  \\ Answer:\end{tabular} \\ \hline
3      & \begin{tabular}[c]{@{}l@{}}Context: My first child was born.\\ Answer: joy\\ Context: My dog died last week.\\ Answer: sadness\\ Context: {[}Input{]} \\ Answer:\end{tabular}                                                                                                                                                                                                   \\ \hline
4      & \begin{tabular}[c]{@{}l@{}}Guess the emotion.\\ Context: My first child was born.\\ Answer: joy\\ Context: My dog died last week.\\ Answer: sadness \\ Context: {[}Input{]}  \\ Answer:\end{tabular}                                                                                                                                                                             \\ \hline
\end{tabular}
\caption{The prompt templates used for experimenting with the language models. The [Input] would be replaced with the sample sentence from the dataset that we are trying to label.}
\label{table:prompt_templates}
% \vspace{-3mm}
\end{table}



% {\color{red}
% Template 1: What are the inferred emotions in the following contexts? Context: My first child was born. Answer: joy Context: My dog died last week. Answer: sadness Context: \{\_\_PLACEHOLDER\_\_\} Answer:



% Template 2: Consider this list of emotions: anger, boredom, disgust, fear, guilt, joy, pride, relief, sadness, shame, surprise, trust, neutral. What are the inferred emotions in the following contexts? Context: My first child was born. Answer: joy Context: My dog died last week. Answer: sadness Context: \{\_\_PLACEHOLDER\_\_\} Answer:



% Template 3:  Context: My first child was born. Answer: joy Context: My dog died last week. Answer: sadness Context: \{\_\_PLACEHOLDER\_\_\} Answer:



% Template 4: Guess the emotion. Context: My first child was born. Answer: joy Context: My dog died last week. Answer: sadness Context: \{\_\_PLACEHOLDER\_\_\} Answer:
% }


\begin{figure*}[bht]
    \centering
    \includegraphics[width=0.7\textwidth]{figs_appendix/prompt_distributions.pdf}
  \caption{The distribution of next word emotion label predictions for different prompt templates and varied numbers of few shot examples.}
  \label{fig:prompt_distribution}
  % \vspace{-3mm}
\end{figure*} 

\begin{figure}[bht]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs_appendix/prompt_accs.pdf}
  \caption{Accuracy of different prompts on the emotion classification task. This experiment varies both the prompt template and the number of provided few-shot examples. Experiments are conducted on Llama 3.2 1B.}
  \label{fig:prompt_accs}
  % \vspace{-3mm}
\end{figure} 

\begin{figure*}[bht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/prompt_emotion_probes.pdf}
  \caption{Probing accuracy with different prompts measured at last token in Llama~3.2~1B for all layers. This experiment varies both the prompt template and the number of provided few-shot examples.}
  \label{fig:prompt_probe}
\end{figure*} 

\begin{figure*}[bht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/prompt_patching_3.pdf}
  \caption{Success of activation patching with different prompts, measured at the last token index in Llama~3.2~1B across all layers with span = 3. This experiment varies both the prompt template and the number of provided few-shot examples.}
  \label{fig:prompt_patch}
\end{figure*} 


\section{Direct Emotion Promotion} \label{app:emotion_promotion}

In Section~\ref{sec:appraisal_intervention}, we showed that it is possible to change the model's output by manipulating appraisal concepts and directing it toward emotions with certain specifications of appraisals. In this section, we show that one can also directly inject a desired specific emotion label output by linearly adding the corresponding emotion vector to the hidden state of the model. More formally, recall the weight vector $\mathbf{w}_e$ for emotion $e$ as introduced in Section~\ref{sec:app_emo_projection}. We define the emotion promotion modification as 

\[
    \mathbf{x} \leftarrow \mathbf{x} + \beta \frac{\mathbf{w}_e}{\norm{\mathbf{w}_e}_2},
\]

where $\mathbf{x}$ on the RHS is the activation from the original model at any desired layer or location, and $\beta$ is the scaling factor that controls the strength of emotion promotion.

Figure~\ref{fig:emotion_promotion} shows the results of direct emotion promotion when performed on the hidden states across different layers of Llama~3.2~1B for different target emotion labels. As the figure suggests, direct emotion promotion is not effective when applied to the early layers, which completely aligns with our prior results. However, after layer 9, the success chance greatly improves, especially for large enough values of $\beta$. Again, this is a validation of our previous findings which shows that emotion concepts are linearly accessible and modifiable after the mid-layers. But before these layers, even a direct modification may fail since it will be overwritten later by the subsequent layers.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/emotion_promotion_heatmap.pdf}
  \caption{The heatmap showing the success of direct emotion promotion when applied at different layers of Llama~3.2~1B. Success score 1 means that the intervention successfully changed all output labels to the target emotion label. Scores 0 means that the intervention made no changes to the output and -1 means that the intervention resulted to complete opposite results, even damaging the samples with the correct original label. All the interventions in this figure used an intervention of layer span size 3.}
  \label{fig:emotion_promotion}
\end{figure*} 

\section{Appraisal Probing} 
\label{app:appraisal_probing}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{1.0\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figs_appendix/appraisal_probe_last_token_part_1.pdf}
    \end{subfigure}%
    \newline
    \newline
    \newline
    \begin{subfigure}[t]{1.0\textwidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figs_appendix/appraisal_probe_last_token_part_2.pdf}
    \end{subfigure}
    \caption{Probing results for Llama~3.2~1B, conducted separately for each appraisal dimension across different activation locations and layers for the last token. Results are measured as the regression $R^2$ score on a held-out test set.} 
    \label{fig:Llama1B_app_probing}
\end{figure*}

As detailed in Section~\ref{sec:appraisal_modulation} and similar to emotion probing experiments provided in Section~\ref{sec:emotion_probe}, we perform probing experiments to assess the presence and strength of appraisal signals at different activations within the Llama~3.2~1B model. We perform probing separately for each appraisal dimension over different activation locations and layers across the model for the last token. The probing results in Figure~\ref{fig:Llama1B_app_probing} are measured as the regression $R^2$ score on a held-out test set.

Following the behavior observed in probing emotion signals, models begin consolidating appraisal-related information in the hidden states $\mathbf{h}^{(l)}$ around the mid-layers. We observe that beyond layer $10$, there is no significant increase in probe accuracy in any appraisal dimension. As also observed in the emotion probing experiment, there is no clear distinction in probing performance between \( \mathbf{m}^{(l)} \) and \( \mathbf{h}^{(l)} \). 

As discussed in length in the main test, the success of linear probing highly depends on whether the target concept is linearly detectable given an activation. The results here also enforce the notion that the appraisal signals are not linearly detectable at earlier layers but are strongly present as we approach the hidden state of the final layers.

% \clearpage
\section{Further Details on Appraisal Modulation} \label{app:appraisal_modulation}

\begin{figure*}[bht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/app_surgery_pleasantness.pdf}
  \caption{Effect of promoting and demoting \textit{pleasantness} at different layers of Llama~3.2~1B with three levels of scaling factor $\beta$. $\beta = 0$ represents the original distribution without appraisal modulation. A consistent increase in distribution shift is observed as $\beta$ increases across all intervention experiments. However, when intervening on earlier layers, particularly at higher $\beta$ values, the shift in the distribution of represented emotions does not always align with theoretical expectations.}
  \label{fig:appraisal_layers_pleasantness}
\end{figure*} 

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/app_surgery_other_responsblt.pdf}
  \caption{Effect of promoting and demoting \textit{other-agency} at different layers of Llama~3.2~1B using three levels of scaling factor $\beta$. $\beta = 0$ represents the original distribution without appraisal modulation. Mid-layer appraisal modulation exhibits a theoretically plausible shift in emotion distribution.}
  \label{fig:appraisal_layers_otheragency}
\end{figure*} 

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/app_surgery_predict_event.pdf}
  \caption{Effect of promoting and demoting \textit{predictability} at different layers of Llama~3.2~1B using three levels of scaling factor $\beta$. $\beta = 0$ represents the original distribution without appraisal modulation. Mid-layer appraisal modulation exhibits a theoretically plausible shift in emotion distribution.}
  \label{fig:appraisal_layers_predictability}
\end{figure*} 





As discussed in Section~\ref{sec:appraisal_intervention}, we show the possibility of indirectly modifying the emotion of an input example by modulating its appraisals within the model representations. In this section, we provide more details and further experiments on appraisal modulation.

First, we redefine our appraisal modulation method to generalize to cases where we want to modify multiple concepts simultaneously. Let's assume that we have a set of $r$ appraisal vectors $\mathcal{A} := \{\mathbf{v}_{i_1}, \mathbf{v}_{i_2}, \cdots, \mathbf{v}_{i_r}\}$ which we want to modify, and consider a second set $\mathcal{B} := \{\mathbf{v}_{j_1}, \mathbf{v}_{j_2}, \cdots, \mathbf{v}_{j_k} \}$ to contain the $k$ appraisal vectors which we want to maintain the corresponding appraisal concept fixed during the update. We define the project matrix $\mathbf{P}_{\mathcal{B}}$ as the projection matrix which projects into the span of $\mathcal{B}$. More formally, form the matrix $\mathbf{V}_{\mathcal{B}} = [\mathbf{v}_{j_1}; \mathbf{v}_{j_2}; \cdots; \mathbf{v}_{j_k}] \in \mathbb{R}^{d \times k}$ by concatenating the vectors in $\mathcal{B}$ as the columns of $\mathbf{V}_{\mathcal{B}}$. With this in mind, the projection matrix, $P_{\mathcal{B}}$ as

\[
P_{\mathcal{B}} = \mathbf{V}_{\mathcal{B}}(\mathbf{V}_{\mathcal{B}}^\top \mathbf{V}_{\mathcal{B}})^{-1} \mathbf{V}_{\mathcal{B}}^\top,
\]

and define the net effect vector as 

\[
    \mathbf{z}_{\mathcal{A}} := (I - \mathbf{P}_{\mathcal{B}})\sum_{a=i_1}^{i_r} \gamma_a \mathbf{v}_a,
\]

where each $\gamma_a$ is a variable from $\{-1, +1\}$ to indicate if the modulation promotes concept $a$ or demote it. Finally, the modulation is performed as

$$\mathbf{x} \leftarrow \mathbf{x} + \beta \frac{\mathbf{z}_{\mathcal{A}}}{\norm{\mathbf{z}_{\mathcal{A}}}_2}$$

where $\beta \in \mathbb{R}_+$ is a positive scaling factor.After performing the intervention, we measure the intervention's success by evaluating the new emotion label obtained by this modification across all examples in the dataset.

Section~\ref{sec:appraisal_intervention} reported the inference-time intervention results targeting the hidden state at layer 9 in Llama~3.2~1B, as we showed it is a critical point in emotion processing in Llama~3.2~1B. Here, we report the intervention results across all layers for a varied set of appraisal dimensions and their superposition to create more complex but specific concepts. 

As shown in Figures~\ref{fig:appraisal_layers_pleasantness}, \ref{fig:appraisal_layers_otheragency}, and \ref{fig:appraisal_layers_predictability}, the intervention on appraisal concepts changes the distribution of the output labels and this distribution shift is intensified with higher values of
\( \beta \) in all intervention experiments. However, the shift in the distribution of represented emotions does not necessarily conform with theoretical and intuitive expectations when intervening on earlier layers, particularly noticeable when \( \beta \) is sufficiently large. For example, in Figure~\ref{fig:appraisal_layers_pleasantness}, we note an unexpected decrease in the distribution of \textit{joy} and \textit{pride} in early layers, whereas psychologically plausible manipulations—such as an increase in high-valence emotions like \textit{joy}, \textit{pride}, and \textit{surprise}, only emerge in mid-layers, peaking at layer 9. This observation supports the notion that intervening on the first layers is not effective because the linear structure in representations is not formed well yet. 

Observing the intervention effect on later layers, we see significantly less pronounced distribution shifts. This also supports our earlier finding that the intervention on final layers is not effective because of the orthogonality of concepts that we showed in Section \ref{sec:app_emo_projection}. 

On the other hand, We observe a remarkable alignment with theoretical and intuitive expectations in the distribution shifts associated with interventions on middle layers (specifically layers 9-11). For instance, we observe that increasing the \textit{pleasantness} appraisal promotes both \textit{joy} and \textit{pride}, aligning with the fact that both of these emotions have high associations with the appraisal. Also see \ref{fig:appraisal_sankey_pleasantness} for a different visualization.

To better evaluate intervention success, we also provide intervention results on the superposition of two appraisal dimensions (e.g., \textit{other-agency} and \textit{pleasantness}) across all layers in Llama~3.2~1B. The results, demonstrated in Figure~\ref{fig:appraisal_layers_pleasantness_otheragency}, show a successful promotion of emotion \textit{pride} with no further occurrences of \textit{joy} in layers 9-11 when demoting \textit{other-agency} and the promotion of \textit{guilt} and \textit{fear} with no occurrence of anger in mid-layers when demoting \textit{other-agency}. Similarly, in Figure~\ref{fig:appraisal_layers_pleasantness_predictability}, we see the transition from \textit{pride} to \textit{surprise} and a larger distribution of \textit{fear} as compared with \textit{anger} when we promote \textit{unpredictability}.

These findings provide strong evidence that the mid-layers in Llama~3.2~1B directly contribute to cognitive processes related to emotions. 

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/app_surgery_pleasantness_other_responsblt.pdf}
  \caption{Superposition of \textit{pleasantness} and \textit{other-agency} appraisal modulation at different layers of Llama~3.2~1B. Results show successful promotion of \textit{pride} with no further occurrences of \textit{joy} in layers 9–11 when demoting \textit{other-agency}, and the promotion of \textit{guilt} and \textit{fear} with no occurrences of \textit{anger} in mid-layers when demoting \textit{other-agency}.}
  \label{fig:appraisal_layers_pleasantness_otheragency}
\end{figure*} 


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/app_surgery_pleasantness_predict_event.pdf}
  \caption{Superposition of \textit{pleasantness} and \textit{predictability} appraisal modulation at different layers of Llama~3.2~1B. Results show a successful transition from \textit{pride} to surprise and a greater distribution of \textit{fear} compared to anger in mid-layers when promoting unpredictability.}
  \label{fig:appraisal_layers_pleasantness_predictability}
\end{figure*} 

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figs_appendix/app_surgery_sankey_pleasantness.pdf}
  \caption{Sankey plot for \textit{pleasantness} appraisal modulation when we perform it at layer 9 of Llama~3.2~1B model.}
  \label{fig:appraisal_sankey_pleasantness}
\end{figure*} 

% \clearpage
\section{Code and Compute Resources}
Our experiments are conducted using GPU-accelerated compute resources, with hardware such as NVIDIA A100 GPUs. For larger models, our studies are feasible on GPUs with at least 40GB of VRAM, with the full experiment running in approximately 24 hours. For smaller models, GPUs with 12GB of VRAM are sufficient to carry out our analyses efficiently. Our implementation and experiment code are publicly available at:~\href{https://github.com/aminbana/emo-llm.git}{Emo-LLM Github Repository}.

Generative AI tools are utilized to improve the tone and style of writing, as well as for code completion during implementation.

