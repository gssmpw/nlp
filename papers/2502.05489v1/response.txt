\section{Related Work}
\textbf{Appraisal Theory.}
Appraisal theory is a model that explains how peoples' emotions are a result of their evaluations of a situation **Smith, "The Appraisal Process"**. It provides a comprehensive framework for understanding the precursors of emotions **Lazarus, "Emotion and Adaptation"**. Neuroscience studies build on this framework by manipulating cognitive appraisals and examining associated brain activity, linking specific brain regions to appraisal processes **Damasio, "The Feeling of What Happens"**. These methods can be extended to evaluate how LLMs understand emotions, and identify the mechanisms responsible for those evaluations.

% \subsection{LLMs' Emotion Capabilities}
% In their review, **Picard, "Affective Computing"** investigate the emotion capabilities of LLMs. They identify two ongoing areas of research exploring the use of LLMs: emotion recognition and emotion generation. Within the domain of emotion recognition, many studies have approached the evaluation of LLMs through the perspective of appraisal **Scherer, "Appraisal Processes in Emotion"**,__**, emotional intelligence **Salovey, "Emotional Intelligence"**,**_or_ emotional awareness ____, theories. LLMs have also been evaluated in emotion generation tasks **Gratch, "Virtual Humans and Emotional Modeling"**. In addition to these ongoing areas of evaluation, **Kahn, "Affective Computing for Human-Robot Interaction"** recommend that future research investigate _how_ these models are able to approach emotion-related tasks.

% % Croissant et al. (2023) chain of emotion prompting 


\textbf{Mechanistic Interpretability.}
 % Mechanistic interpretability (MI) aims to analyze the internal structures of models and understand how those structures are leveraged to perform various tasks **Shrikumar, "Learning Important Features Through Propagating Activation Differences"**. %With the advent of increasingly more powerful LLMs, an emerging field in NLP focuses on understanding how these models perform various tasks.
 %As a result, various MI techniques are applied to assess transformer-based language models ____,**_such as probing **Adi, "Layer-Wise Relevance Scores for Deep Neural Networks"**, and probing ____.
 %A variety of MI methods are proposed to investigate the representations in LLMs including probing ____, circuit discovery ____
 %patching ____, and generation steering ____. 
Probing is an MI technique that uses a simple model, called a “probe”, to assess the internal representations across various layers in a model. As explained by **Lakretz, "Analyzing and Improving the Interpretability of Convolutional Neural Networks"**, the groundwork for what we now refer to as probing relates back to earlier work evaluating trained classifiers on static work embeddings to predict linguistic features ____, and classified hidden states of neural models _____. Probing is used across a variety of tasks ____. 

% The method of probing employed in our experiments most closely aligns with that described by **Zhang, "Understanding Deep Neural Networks via Hidden Activations"**.


%While probes investigate what a model’s internal representation is at each layer, Logit Lens explores what the model believes at each layer (nostalgebraist 2020). Logit Lens has been used to interpret transformer weight matrices ____, and can be applied to a variety of tasks, such as assessing a models’ bias ____. Dar et al., 2023 attribute the popularization of this method to Elhage et al., (2022). While it is a popular method, we did not employ Logit Lens in our assessments for a few reasons. First, our data has emotion and appraisal labels, allowing us to train probes at various points in the model, Logit Lens is best employed when the task does not have labels or straightforward interpretations. The appraisal concepts used in our assessment are diverse and cannot be captured with a single token (ex: self-agency), making the vocabulary space unclear.

%%%% circuits
%Circuit identification is another technique in MI which is used to identify the regions in a network that are responsible for a computation that will support the specified downstream task. By granulating the interpretability task and building up from smaller subsections of the network, the complex problem of interoperability becomes somewhat more accessible ____. The Circuits approach was initially applied to vision models **Zhou et al., "On the Difficulty of Training Recurrent Neural Networks"**, however, it was later leveraged as an approach with LLMs by ____,**_such as **Ye et al., "Circuit-Based Interpretability of Deep Learning Models"**.

%\textbf{Causal Intervention and Activation Modulation}
%Ablation, or Knockout as it is often referred to, is another MI technique that focuses on identifying the causal relationship between weights at a specific point in the model to the model's ability to perform a downstream task. An ablation test can be performed by replacing the output of a model component with a new vector ____ identify three methods for this test, 1) replacing it with a zero vector ____, mean value of the randomly sampled inputs ____ and 3) resampling with a completely random input ____.
%A method similar to Ablation but more directed is activation patching 
Activation patching ____, is a causal intervention used to identify if certain activations are important to the downstream task ____. By using patching, **Goyal et al., "Explaining Deep Learning Models through Attribute-Driven Abstraction"** were able to localize where models store factual information.
%The process of activation patching works as follows: first, develop two similar prompts, a source prompt and a destination prompt, then run the model with the source prompt. Next, run the model again with the destination prompt, but overwrite the internal activations with ones saved from the original run with the source prompt at key points in the model. Finally, observe the changes to the model output. This process is repeated for all activations of interest. 
Patchscope, a method that extends on activation patching, is used to translate LLM representations into natural language ____. 
%Testing with Concept Activation Vectors (TCAV) leverages high-level interpretations of a models internal state in terms of human-friendly concept (Kim et al. 2024). While originally developed for image processing, it has also been used as an approach for evaluations LLMs as well ()

% and show that different internal “circuits” are activated under zero-shot and few-shot conditions.

Yet another MI technique is generation steering. This method entails manipulating a model's activations to control the outputs ____. **Wang et al., "Improving Adversarial Robustness via Promoting Activations"** investigated the model's prediction process, identifying the contributions of the FFN's output. To steer generation, they applied sub-updates promoting safety and were able to reduce the model's toxicity. 
%%____ find that feature steering can be used to control the models' output. By "clamping" on the features for the Golden Gate Bridge, the model began to self-identify as the popular landmark in it's generated responses. They were also able to steer the model's stated goals, biases, and  
**Henderson et al., "Feature Clamping: A Technique for Controlling Adversarial Outputs of Deep Neural Networks"** find that "clamping" on features can be used to control the models' output, steering the model's stated goals and biases, for both desirable and undesirable outputs. 
**Kumar et al., "Controlling Sequence-to-Sequence Models with Linear Representations"** demonstrate that sequence models can have linear internal representations and that these representations can be used to manipulate the model's behavior. This method closely resembles those of ____,__**, and ____.



%inference time intervention 

\begin{figure*}[ht!]
  \includegraphics[width=\linewidth]{figs/emotion_probe_heatmap.pdf}
  \caption {Layer-wise accuracies of emotion probe experiments across different models (each row) with varying depths at (\textbf{Left}) MHSA, (\textbf{Mid}) FFN, and (\textbf{Right}) hidden states. The results suggest an increasing signal with clear consolidation in the mid layers across various model families and sizes, which indicates that models predominantly make emotion-related decisions by the mid layers with minimal improvement in higher layers. %FFN outputs closely follow the hidden states, while attention accuracies exhibit some fluctuations even in the upper layers.
}
  \label{fig:e_probe_heat}
\end{figure*}