\section{Related Work}
\textbf{Appraisal Theory.}
Appraisal theory is a model that explains how peoples' emotions are a result of their evaluations of a situation \cite{lazarus1991emotion}. It provides a comprehensive framework for understanding the precursors of emotions \cite{smith2011role}. Neuroscience studies build on this framework by manipulating cognitive appraisals and examining associated brain activity, linking specific brain regions to appraisal processes \cite{leitao2020computational, kragel2024can, brosch2013comment}. These methods can be extended to evaluate how LLMs understand emotions, and identify the mechanisms responsible for those evaluations. 

% \subsection{LLMs' Emotion Capabilities}
% In their review, \citet{Yongsatianchot} investigate the emotion capabilities of LLMs. They identify two ongoing areas of research exploring the use of LLMs: emotion recognition and emotion generation. Within the domain of emotion recognition, many studies have approached the evaluation of LLMs through the perspective of appraisal \cite{yongsatianchot2023investigating,zhan2023evaluating,yongsatianchot2023s}, emotional intelligence \cite{wang2023emotional,paech2023eq,croissant2024appraisal,zhao2024both}, or emotional awareness \cite{elyoseph2023chatgpt}  theories. LLMs have also been evaluated in emotion generation tasks \cite{broekens2023fine,coda2023inducing,croissant2024appraisal,li2023large, vu2024psychadapter}. In addition to these ongoing areas of evaluation, \citet{Yongsatianchot} recommend that future research investigate \textit{how} these models are able to approach emotion-related tasks.

% % Croissant et al. (2023) chain of emotion prompting 


\textbf{Mechanistic Interpretability.}
 % Mechanistic interpretability (MI) aims to analyze the internal structures of models and understand how those structures are leveraged to perform various tasks \cite{rai2024practical}. %With the advent of increasingly more powerful LLMs, an emerging field in NLP focuses on understanding how these models perform various tasks.
 %As a result, various MI techniques are applied to assess transformer-based language models \cite{rai2024practical}.
 %A variety of MI methods are proposed to investigate the representations in LLMs including probing \cite{prob2019, belinkov2022probing},
 %circuit discovery \cite{nanda2023progress, wang2022interpretability, conmy2023towards, meng2022locating, prakash2024finetuningenhancesexistingmechanisms}
 %patching \cite{heimersheim2024use}, and generation steering \cite{templeton2024scaling}. 
Probing is an MI technique that uses a simple model, called a “probe”, to assess the internal representations across various layers in a model. As explained by \citet{belinkov2018internal}, the groundwork for what we now refer to as probing relates back to earlier work evaluating trained classifiers on static work embeddings to predict linguistic features \cite{kohn2015s,gupta2015distributional}, and classified hidden states of neural models \cite{ettinger2016probing,kadar2017representation,shi2016does,adi2016fine,hupkes2018visualisation,belinkov2022probing, giulianelli2018under}. Probing is used across a variety of tasks \cite{hewitt2019designing, tenney2019bert,tenney2019you,peters2018dissecting,clark2019does,belinkov2018internal,conneau2018you}. 

% The method of probing employed in our experiments most closely aligns with that described by \cite{hewitt2019designing}. 


%While probes investigate what a model’s internal representation is at each layer, Logit Lens explores what the model believes at each layer (nostalgebraist 2020). Logit Lens has been used to interpret transformer weight matrices (Halawi et al., 2023; Dar et al., 2023; Geva et al., 2022), and can be applied to a variety of tasks, such as assessing a models’ bias (Prakash and Lee 2023). Dar et al., 2023 attribute the popularization of this method to Elhage et al., (2022). While it is a popular method, we did not employ Logit Lens in our assessments for a few reasons. First, our data has emotion and appraisal labels, allowing us to train probes at various points in the model, Logit Lens is best employed when the task does not have labels or straightforward interpretations. The appraisal concepts used in our assessment are diverse and cannot be captured with a single token (ex: self-agency), making the vocabulary space unclear.

%%%% circuits
%Circuit identification is another technique in MI which is used to identify the regions in a network that are responsible for a computation that will support the specified downstream task. By granulating the interpretability task and building up from smaller subsections of the network, the complex problem of interoperability becomes somewhat more accessible \cite{cammarata2021curve}. The Circuits approach was initially applied to vision models 
%\cite{cammarata2021curve,olah2020zoom}, however, it was later leveraged as an approach with LLMs by \citet{elhage2021mathematical}.

%\textbf{Causal Intervention and Activation Modulation}
%Ablation, or Knockout as it is often referred to, is another MI technique that focuses on identifying the causal relationship between weights at a specific point in the model to the model's ability to perform a downstream task. An ablation test can be performed by replacing the output of a model component with a new vector \citet{rai2024practical} identify three methods for this test, 1) replacing it with a zero vector \cite{olsson2022context}, mean value of the randomly sampled inputs \cite{wang2022interpretability} and 3) resampling with a completely random input \cite{chen2023sudden}.
%A method similar to Ablation but more directed is activation patching 
Activation patching \cite{heimersheim2024use}, is a causal intervention used to identify if certain activations are important to the downstream task \cite{vig2020investigating}. By using patching, \citet{meng2022locating} were able to localize where models store factual information.
%The process of activation patching works as follows: first, develop two similar prompts, a source prompt and a destination prompt, then run the model with the source prompt. Next, run the model again with the destination prompt, but overwrite the internal activations with ones saved from the original run with the source prompt at key points in the model. Finally, observe the changes to the model output. This process is repeated for all activations of interest. 
Patchscope, a method that extends on activation patching, is used to translate LLM representations into natural language \cite{patchscopes}. 
%Testing with Concept Activation Vectors (TCAV) leverages high-level interpretations of a models internal state in terms of human-friendly concept (Kim et al. 2024). While originally developed for image processing, it has also been used as an approach for evaluations LLMs as well ()

% and show that different internal “circuits” are activated under zero-shot and few-shot conditions.

Yet another MI technique is generation steering. This method entails manipulating a model's activations to control the outputs \cite{rai2024practical, todd2024function}. \citet{geva2022transformer} investigated the model's prediction process, identifying the contributions of the FFN's output. To steer generation, they applied sub-updates promoting safety and were able to reduce the model's toxicity. 
%%\citet{templeton2024scaling} find that feature steering can be used to control the models' output. By "clamping" on the features for the Golden Gate Bridge, the model began to self-identify as the popular landmark in it's generated responses. They were also able to steer the model's stated goals, biases, and  
\citet{templeton2024scaling} find that "clamping" on features can be used to control the models' output, steering the model's stated goals and biases, for both desirable and undesirable outputs. 
\citet{nanda2023emergent} demonstrate that sequence models can have linear internal representations and that these representations can be used to manipulate the model's behavior. This method closely resembles those of \citet{turner2308activation} and \citet{lieberum2023does}.



%inference time intervention 

\begin{figure*}[ht!]
  \includegraphics[width=\linewidth]{figs/emotion_probe_heatmap.pdf}
  \caption {Layer-wise accuracies of emotion probe experiments across different models (each row) with varying depths at (\textbf{Left}) MHSA, (\textbf{Mid}) FFN, and (\textbf{Right}) hidden states. The results suggest an increasing signal with clear consolidation in the mid layers across various model families and sizes, which indicates that models predominantly make emotion-related decisions by the mid layers with minimal improvement in higher layers. %FFN outputs closely follow the hidden states, while attention accuracies exhibit some fluctuations even in the upper layers.
}
  \label{fig:e_probe_heat}
\end{figure*}