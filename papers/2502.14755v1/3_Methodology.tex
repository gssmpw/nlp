\section{Solving \textsc{mo-cbo} Problems}\label{sec:mo_cbo_solve}

In this section, we propose our methodology for solving \textsc{mo-cbo} problems, which is depicted in \cref{fig:methodology_overview}. In summary, we reduce the search space to a subset $\mathcal{S} \subseteq \mathbb{P}(\mathbf{X})$, solve the corresponding local \textsc{mo-cbo} problems to find their Pareto fronts, and extract only Pareto-optimal intervention-set value pairs to construct the causal Pareto front.

\subsection{Reducing the Search Space}\label{subsec:search_space_reduction}
The complexity of solving the local \textsc{mo-cbo} problems with $\mathcal{S} = \mathbb{P}(\mathbf{X})$ rises exponentially with the number of treatment variables, making this strategy impracticable for most tasks. This section proposes a superior method of exploiting the graph topology to identify a minimal subset $\mathcal{S} \subseteq \mathbb{P}(\mathbf{X})$ with $\mathcal{P}_f^{\textsf{c}}(\mathbb{P}(\mathbf{X})) = \mathcal{P}_f^{\textsf{c}}(\mathcal{S})$.
To this end, we generalize the results from \citet{NEURIPS2018_c0a271bc} to the multi-objective setting. All proofs and derivations are given \cref{appendix:search_space_reduction}. For now, we assume that there are no non-manipulative variables, i.e., $\mathbf{C} = \varnothing$.

First, we reduce the search space by disregarding intervention sets where some variables do not affect the targets:

\begin{definition}[Minimal intervention set] \label{def:causal_bandits.mis}
    A set $\mathbf{X}_s \in \mathbb{P}(\mathbf{X})$ is called a \textit{minimal intervention set} if, for every \textsc{scm} conforming to $\mathcal{G}$, there exists no subset $\mathbf{X}_s' \subset \mathbf{X}_s$ such that for all $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ it holds $\mu(\mathbf{X_s},\mathbf{x}_s) = \mu(\mathbf{X}_s',\mathbf{x}_s[\mathbf{X}_s'])$, for all $1 \leq i \leq m$.
\end{definition}

We denote the set of minimal intervention sets with $\mathbb{M}_{\mathcal{G},\mathbf{Y}}$. The following proposition characterizes such sets in a given causal graph $\mathcal{G}$. It is proven in \cref{subsec:causal_bandits_mis}.

\begin{proposition}\label{prop:causal_bandits.mis}
    $\mathbf{X}_s \in \mathbb{P}(\mathbf{X})$ is a minimal intervention set if and only if it holds $\mathbf{X}_s \subseteq \textnormal{an}(\mathbf{Y})_{\mathcal{G}_{\overline{\mathbf{X}}_s}}$.
\end{proposition}

Next, we adapt the notion of so called \textit{possibly-optimal minimal intervention sets} \cite{NEURIPS2018_c0a271bc} for Pareto-optimality. Intuitively said, a minimal intervention set is called possibly Pareto-optimal if it includes a Pareto-optimal intervention set-value pair whose outcome is unattainable with any other intervention set, for at least one \textsc{scm} conforming to $\mathcal{G}$.

\begin{definition}[Possibly Pareto-optimal minimal intervention set]\label{def:causal_bandits.pomis}
    A set $\mathbf{X}_s \in \mathbb{M}_{\mathcal{G},\mathbf{Y}}$ is called \textit{possibly Pareto-optimal} if, for at least one \textsc{scm} conforming to $\mathcal{G}$, there exists $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ such that $(\mathbf{X}_s,\mathbf{x}_s)$ is Pareto-optimal for $\mathbb{P}(\mathbf{X})$, and for no $\mathbf{X}_s' \in \mathbb{M}_{\mathcal{G},\mathbf{Y}} \backslash \mathbf{X}_s, \mathbf{x}_s' \in \mathcal{D}(\mathbf{X}_s')$ it holds $\mu(\mathbf{X}_s',\mathbf{x}_s') \leq \mu(\mathbf{X}_s,\mathbf{x}_s)$, for all $1\leq i \leq m$.
\end{definition}

We denote the set of possibly Pareto-optimal minimal intervention sets with $\mathbb{O}_{\mathcal{G},\mathbf{Y}}$. 
Next, we establish graph-theoretical criteria to identify such sets in a given causal graph. First, the proposition below, which we proof in \cref{subsec:causal_bandits_pomis}, considers a special case:

\begin{proposition}\label{prop:causal_bandits.pomis_no_confounders}
    If no $Y_i$ is confounded with $\textnormal{an}(Y_i)_{\mathcal{G}}$ via unobserved confounders, then $\textnormal{pa}(\mathbf{Y})_{\mathcal{G}}$ is the only possibly Pareto-optimal minimal intervention set.
\end{proposition}

To characterize possibly Pareto-optimal minimal intervention sets in arbitrary graphs, we extend the following two definitions from \citet{NEURIPS2018_c0a271bc} to the multi-objective setting. They aim to identify a region, starting from $\mathbf{Y}$, that is governed by unobserved confounders, along with its outside border that determines the realization of variables within the region.

\begin{definition}[Minimal unobserved confounders' territory]
    \label{def:mo_cbo.uc_territory}
    Let $\mathcal{H}=\mathcal{G}[\text{An}(\mathbf{Y})_{\mathcal{G}}]$. A set of variables $\mathbf{T}$ in $\mathcal{H}$, with $\mathbf{Y} \subseteq \mathbf{T}$, is called a \textit{UC-territory} for $\mathcal{G}$ w.r.t. $\mathbf{Y}$ if $\text{De}(\mathbf{T})_{\mathcal{H}}=\mathbf{T}$ and $\text{CC}(\mathbf{T})_{\mathcal{H}} = \mathbf{T}$. The UC-territory $\mathbf{T}$ is said to be \textit{minimal}, denoted $\mathbf{T} = \text{MUCT}(\mathcal{G},\mathbf{Y})$, if no $\mathbf{T}' \subset \mathbf{T}$ is a UC-territory.
\end{definition}

\begin{definition}[Interventional border]
    \label{def:mo_cbo.int_border}
    Let $\mathbf{T}=\text{MUCT}(\mathcal{G},\mathbf{Y})$. Then, $\mathbf{B} = \text{pa}(\mathbf{T})_{\mathcal{G}} \backslash \mathbf{T}$ is called the \textit{interventional border} for $\mathcal{G}$ w.r.t. $\mathbf{Y}$, denoted as $\text{IB}(\mathcal{G},\mathbf{Y})$.
\end{definition}

\begin{figure}[t]
    \centering
    \includegraphics{SA_S1_S2.pdf}
    \caption{Two causal graphs with $\mathbf{X}=\{ X_1,X_2,X_3,X_4\}$, $\mathbf{Y}=\{Y_1,Y_2\}$. (a) No unobserved confounders. (b) An unobserved confounder between $X_4$ and $Y_1$ depicted with the dashed bi-directed edge.}
    \label{fig:synthtic_dags}
\end{figure}

\paragraph{Example} We illustrate these two concepts with the causal graphs from \cref{fig:synthtic_dags}. In \cref{fig:synthtic_dags} (a), there are no unobserved confounders and thus, it holds $\text{CC}(\mathbf{Y})_{\mathcal{G}} = \mathbf{Y}$ and $\text{De}(\mathbf{Y})_{\mathcal{G}} = \mathbf{Y}$. 
It follows $\text{MUCT}(\mathcal{G},\mathbf{Y}) = \{ Y_1,Y_2\}$ and $\text{IB}(\mathcal{G},\mathbf{Y})=\{X_1,X_2\}$. In \cref{fig:synthtic_dags} (b), we construct the minimal UC-territory, starting from $\mathbf{T}=\mathbf{Y}$,  as follows: 
Since $Y_1$ has an unobserved confounder with $X_4$, we update $\mathbf{T} = \text{CC}(\mathbf{Y})_{\mathcal{G}} = \{ Y_1,Y_2,X_4\}$, and thereafter add all the descendants of $X_4$, obtaining $\mathbf{T} = \{Y_1,Y_2,X_4,X_1\}$. 
Since there are no more unobserved confounders between $\mathbf{T}$ and $\text{An}(\mathbf{Y})_{\mathcal{G}} \backslash \mathbf{T}$, the minimal UC-territory has been found and is given by $\text{MUCT}(\mathcal{G},\mathbf{Y}) = \{ Y_1,Y_2, X_1, X_4\}$ along with $\text{IB}(\mathcal{G},\mathbf{Y})=\{X_2,X_3\}$. 

Interventional borders can fully characterize possibly Pareto-optimal minimal intervention sets, as seen in the following two results, both of which we prove in \cref{subsec:causal_bandits_pomis}.

\begin{proposition}
\label{prop:causal_bandits.intervenention_IB_pomis}
   $\textnormal{IB}(\mathcal{G}_{\overline{\mathbf{X}}_s}, \mathbf{Y})$ is a possibly Pareto-optimal minimal intervention set for any $\mathbf{X}_s \in \mathbb{P}(\mathbf{X})$.
\end{proposition}

\begin{theorem}\label{thm:causal_bandits}
    A set $\mathbf{X}_s \in \mathbb{P}(\mathbf{X})$ is a possibly Pareto-optimal minimal intervention set if and only if it holds $\textnormal{IB}(\mathcal{G}_{\overline{\mathbf{X}}_s}, \mathbf{Y}) = \mathbf{X}_s$.
\end{theorem}

The following corollary states that intervening on the interventional border of a set $\mathbf{X}_s \in \mathbb{P}(\mathbf{X})$ is at least as optimal as intervening on the set $\mathbf{X}_s$ itself.

\begin{corollary}\label{cor:causal_bandits}
    Let $\mathbf{X}_s \in \mathbb{P}(\mathbf{X})$ and $\mathbf{X}_s'=\textnormal{IB}(\mathcal{G}_{\overline{\mathbf{X}}_s}, \mathbf{Y})$. For any $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ there exist $\mathbf{x}_s' \in \mathcal{D}(\mathbf{X}_s')$ such that it holds $\mu(\mathbf{X}_s',\mathbf{x}_s') \leq \mu(\mathbf{X}_s,\mathbf{x}_s)$, for all $1\leq i \leq m$.
\end{corollary}

In the setting of \cref{cor:causal_bandits}, it is possible to construct an \textsc{scm} conforming to $\mathcal{G}$ such that strict inequality holds in at least one component. For such constructions, we refer to \cref{subsec:causal_bandits_pomis}. 

Finally, we show that it suffices to only consider possibly Pareto-optimal minimal intervention sets to solve \textsc{mo-cbo} problems. Due to the significance of this result, we also present its proof here.

\begin{proposition}\label{prop:pomis_suffice}
    It holds $\mathcal{P}_f^{\textsf{c}}(\mathbb{P}(\mathbf{X})) = \mathcal{P}_f^{\textsf{c}}(\mathbb{O}_{\mathcal{G},\mathbf{Y}})$.
\end{proposition}

\begin{proof}
    $\subseteq:$ Assume $\mathcal{P}_f^{\textsf{c}}(\mathbb{P}(\mathbf{X})) \not\subseteq \mathcal{P}_f^{\textsf{c}}(\mathbb{O}_{\mathcal{G},\mathbf{Y}})$. 
    Then, there exists $\mathbf{z} \in \mathbb{R}^m$, with $\mathbf{z}= \boldsymbol{\mu}(\mathbf{X}_s,\mathbf{x}_s)$ for some intervention set-value pair $(\mathbf{X}_s,\mathbf{x}_s)$, such that $\mathbf{z} \in \mathcal{P}_f^{\textsf{c}}(\mathbb{P}(\mathbf{X}))$ and $\mathbf{z} \not\in \mathcal{P}_f^{\textsf{c}}(\mathbb{O}_{\mathcal{G},\mathbf{Y}})$. 
    If $\mathbf{X}_s \in \mathbb{O}_{\mathcal{G},\mathbf{Y}}$, it follows that $(\mathbf{X}_s,\mathbf{x}_s)$ is not Pareto-optimal for $\mathbb{O}_{\mathcal{G},\mathbf{Y}}$, which is a contradiction since it is Pareto-optimal for $\mathbb{P}(\mathbf{X})$. 
    Conversely, if $\mathbf{X}_s \in \mathbb{P}(\mathbf{X}) \backslash  \mathbb{O}_{\mathcal{G},\mathbf{Y}}$, we set $\mathbf{X}_s' = \text{IB}(\mathcal{G}_{\overline{\mathbf{X}}_s},\mathbf{Y})$ and from \cref{cor:causal_bandits}, we infer that, for some \textsc{scm} conforming to $\mathcal{G},$ there exists $\mathbf{x}_s' \in \mathcal{D}(\mathbf{X}_s')$ with $\mu(\mathbf{X}_s',\mathbf{x}_s') \leq \mu(\mathbf{X}_s,\mathbf{x}_s)$, for all $1\leq i \leq m$, and $\mu(\mathbf{X}_s',\mathbf{x}_s') < \mu(\mathbf{X}_s,\mathbf{x}_s)$, for at least one $1\leq i \leq m$. This results in $\mathbf{z} \not\in \mathcal{P}_f^{\textsf{c}}(\mathbb{P}(\mathbf{X}))$, which is a contradiction. 

    $\supseteq:$ Assume $\mathcal{P}_f^{\textsf{c}}(\mathbb{O}_{\mathcal{G},\mathbf{Y}}) \not\subseteq  \mathcal{P}_f^{\textsf{c}}(\mathbb{P}(\mathbf{X})) $. Then, there exists $\mathbf{z} \in \mathbb{R}^m$, with $\mathbf{z}= \boldsymbol{\mu}(\mathbf{X}_s,\mathbf{x}_s)$, such that $\mathbf{z} \in \mathcal{P}_f^{\textsf{c}}(\mathbb{O}_{\mathcal{G},\mathbf{Y}})$ and $\mathbf{z} \not\in \mathcal{P}_f^{\textsf{c}}(\mathbb{P}(\mathbf{X}))$. There exists some $\mathbf{X}_s' \in \mathbb{P}(\mathbf{X}) \backslash  \mathbb{O}_{\mathcal{G},\mathbf{Y}}$, $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s')$ such that $(\mathbf{X}_s',\mathbf{x}_s')$ is Pareto optimal and for which it holds $\mu(\mathbf{X}_s',\mathbf{x}_s') \leq \mu(\mathbf{X}_s,\mathbf{x}_s)$, for all $1\leq i \leq m$, and $\mu(\mathbf{X}_s',\mathbf{x}_s') < \mu(\mathbf{X}_s,\mathbf{x}_s)$, for at least one $1\leq i \leq m$. Since $\mathbf{X}_s'$ is not possibly Pareto-optimal, we infer from \cref{cor:causal_bandits} that for $\mathbf{X}_s'' = \text{IB}(\mathcal{G},\mathbf{Y})$ there exists $\mathbf{x}_s'' \in \mathcal{D}(\mathbf{X}_s'')$ such that $\mu(\mathbf{X}_s'',\mathbf{x}_s'') \leq \mu(\mathbf{X}_s',\mathbf{x}_s')$, for all $1\leq i \leq m$. Hence, it holds $\boldsymbol{\mu}(\mathbf{X}_s'',\mathbf{x}_s'') \in \mathcal{P}_f^{\textsf{c}}(\mathbb{P}(\mathbf{X}))$, which is a contradiction to $\mathbf{z} \in \mathcal{P}_f^{\textsf{c}}(\mathbb{O}_{\mathcal{G},\mathbf{Y}})$ since $\mathbf{X}_s'' \in \mathbb{O}_{\mathcal{G},\mathbf{Y}}$.
\end{proof}

Using \cref{prop:pomis_suffice}, we reduce the search space of \textsc{mo-cbo} problems to $\mathcal{S} = \mathbb{O}_{\mathcal{G},\mathbf{Y}}$.

\paragraph{Example} We illustrate the search space reduction with the causal graphs from \cref{fig:synthtic_dags}. Note that in both cases it holds $\mathbb{P}(\mathbf{X}) = 2^{|\mathbf{X}|} = 16$. In \cref{fig:synthtic_dags} (a), there are no unobserved confounders and it follows $\mathbb{O}_{\mathcal{G},\mathbf{Y}} = \text{pa}(\mathbf{Y})_{\mathcal{G}} = \{ X_1,X_2\}$. In \cref{fig:synthtic_dags} (b), the intervention sets $\{ X_1,X_2,X_3\}$ and  $\{X_2,X_3\}$ satisfy the condition from \cref{thm:causal_bandits}, and thus, $\mathbb{O}_{\mathcal{G},\mathbf{Y}} = \{ \{ X_2,X_3\}, \{ X_1,X_2,X_3\} \}$.

We now consider the more general case with $\mathbf{C} \neq \varnothing$, where non-manipulative variables can be present. The definitions for minimal intervention set and possibly Pareto-optimal minimal intervention set are a straightforward extension. %(formulated in \cref{subsec:causal_bandits_pomis}).
\citet{lee2019structural} propose a projection $\mathcal{G} \rightarrow \mathcal{G}[\mathbf{V} \backslash \mathbf{C}]$ which preserves the distribution of the underlying \textsc{scm}. Given such a projection, we can identify the possibly Pareto-optimal minimal intervention sets in $\langle \mathcal{G},\mathbf{Y},\mathbf{X}, \mathbf{C} \rangle$ by applying \cref{thm:causal_bandits} to $\langle \mathcal{G}[\mathbf{V} \backslash \mathbf{C}],\mathbf{Y},\mathbf{X}\rangle$.


\subsection{Algorithm}\label{subsec:mo_cbo_algorithm}
We introduce the algorithm \textsc{Causal ParetoSelect}\footnote{The full implementation of our algorithm is available at \href{https://github.com/ShriyaBhatija/MO-CBO}{\texttt{https://github.com/ShriyaBhatija/MO-CBO}}.}, summarized in \cref{alg:cps}. It assumes a known causal graph $\langle \mathcal{G},\mathbf{Y}, \mathbf{X}, \mathbf{C} \rangle$, prior interventional data $\mathcal{D}^{I}$, and a set $\mathcal{S} \subseteq \mathbb{P}(\mathbf{X})$ that specifies which local problem to consider. The idea is to alternately solve the local \textsc{mo-cbo} problems using the batch multi-objective Bayesian optimization algorithm \textsc{dgemo} \cite{dgemo}.

\begin{algorithm}[tb]
   \caption{\textsc{Causal ParetoSelect}}
   \label{alg:cps}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\langle \mathcal{G}, \mathbf{Y}, \mathbf{X}, \mathbf{C} \rangle$, $\mathcal{S} \subseteq \mathbb{P}(\mathbf{X})$, $\mathcal{D}^{I}$, batch size $B$, number of iterations $N$
   \STATE {\bfseries Output:} $\mathcal{P}_s^{\textsf{c}}(\mathcal{S})$, $\mathcal{P}_f^{\textsf{c}}(\mathcal{S})$
   \STATE Initialise the dataset $\mathcal{D}_0^{I}=\mathcal{D}^{I}$
   \FOR{$s=1$ {\bfseries to} $|\mathcal{S}|$}
   \STATE Fit surrogates $\tilde{\mu}_i(\mathbf{X_s}, \ \cdot \ )$ with $\mathcal{D}_0^{I}$, $i=1,\dots,m$
   \STATE Approximate $\mathcal{P}_s^{\textsf{l}}(\mathbf{X}_s)$ and $\mathcal{P}_f^{\textsf{l}}(\mathbf{X}_s)$ using $\tilde{\mu}_1,\dots,\tilde{\mu}_m$
   \ENDFOR
   \FOR{$n=1$ {\bfseries to} $N$}
   \FOR{$s=1$ {\bfseries to} $|\mathcal{S}|$}
   \STATE Select batch $\mathbf{B}_s = \{\mathbf{x}_{s}^{b}\}_{b=1}^B$ via \cref{eq:mo_cbo.dgemo_batch_selection}
   \ENDFOR
   \STATE Select batch $\mathbf{B}_{\hat{s}}$ from $\{ \mathbf{B}_1, \dots, \mathbf{B}_{|\mathcal{S}|} \}$ via \cref{eq:mo_cbo.batch_selection}
   \STATE Intervene on $\mathbf{X}_{\hat{s}}$ with $\mathbf{B}_{\hat{s}}$
   \STATE Augment $\mathcal{D}_n^{I} = \mathcal{D}_{n-1}^{I} \cup \{(\mathbf{X}_{\hat{s}},\mathbf{x}_{\hat{s}}^b),\boldsymbol{\mu}(\mathbf{X}_{\hat{s}}, \mathbf{x}_{\hat{s}}^b))\}_{b=1}^B$
   \STATE Update surrogates $\tilde{\mu}_i(\mathbf{X}_{\hat{s}}, \ \cdot \ )$ with $\mathcal{D}_{n}^{I}$, $i=1,\dots,m$
   \STATE Approximate $\mathcal{P}_s^{\textsf{l}}(\mathbf{X}_{\hat{s}})$ and $\mathcal{P}_f^{\textsf{l}}(\mathbf{X}_{\hat{s}})$ using $\tilde{\mu}_1,\dots,\tilde{\mu}_m$
   \ENDFOR
   \STATE Compute $\mathcal{P}_s^{\textsf{l}}(\mathbf{X}_s), \mathcal{P}_f^{\textsf{l}}(\mathbf{X}_s)$ from $\mathcal{D}_N^{I}$, $s=1,\dots, |\mathcal{S}|$
   \STATE Compute $\mathcal{P}^{\textsf{c}}_s(\mathcal{S})$ and $\mathcal{P}^{\textsf{c}}_f(\mathcal{S})$ 
\end{algorithmic}
\end{algorithm}

More specifically, \textsc{Causal ParetoSelect} operates as follows: For each local \textsc{mo-cbo} problem w.r.t. $\mathbf{X}_s \in \mathcal{S}$, we first fit the surrogate model to the objectives $\mu_i(\mathbf{X}_s, \ \cdot \ )$, $1\leq i\leq m$, via independent Gaussian processes as proposed by \textsc{dgemo}. Based on the means of the Gaussian process posteriors, we compute approximations of $\mathcal{P}_s^{\textsf{l}}(\mathbf{X}_s)$ and $\mathcal{P}_f^{\textsf{l}}(\mathbf{X}_s)$ utilizing the Pareto discovery approach from \textsc{dgemo}. After this initial step, at each iteration, the most promising intervention set is selected for batch evaluation. The dataset is augmented with the newly evaluated batch of samples. 
For the corresponding intervention set, we again update the surrogate model, as well as Pareto set and front approximations of the associated local problem. 
After completing all iterations, \textsc{Causal ParetoSelect} identifies the Pareto sets and fronts for each local \textsc{mo-cbo} problem using the collected objective function evaluations $\mathcal{D}_N^{I}$. 
Among those, $\mathcal{P}_s^{\textsf{c}}(\mathcal{S})$ and $\mathcal{P}_f^{\textsf{c}}(\mathcal{S})$ are determined by computing the Pareto-optimal points, which is justified by Proposition \ref{prop:mo_cbo.decomposition}.

\paragraph{Acquisition Function} 
We specify the batch selection strategy of our method in more detail. For a local \textsc{mo-cbo} problem w.r.t. $\mathbf{X}_s \in \mathcal{S}$, let $\mathcal{R}_1(\mathbf{X}_s),\dots,\mathcal{R}_K(\mathbf{X}_s) \subseteq \mathcal{D}(\mathbf{X}_s)$ denote the identified diversity regions from \textsc{dgemo}, discussed in \cref{subsec:prelim_mobo}. \textsc{Causal ParetoSelect} seeks to balance the exploration of the Pareto fronts associated to several different local \textsc{mo-cbo} problems. Evaluating all $\mathbf{B}_s$, $s=1,\dots,|\mathcal{S}|$, during a single iteration, is an inefficient strategy. Rather, at each iteration, we select the batch with the most promising hypervolume improvement. 
To this end, we introduce the term  \textit{relative hypervolume improvement}, which we define as
\begin{equation}
    \text{RHVI}(\boldsymbol{\mu}(\mathbf{X}_s,\mathbf{B}_s), \mathcal{P}_f^{\textsf{l}}(\mathbf{X_s})) = \frac{\text{HVI}(\boldsymbol{\mu}(\mathbf{X}_s,\mathbf{B}_s), \mathcal{P}^{{\textsf{l}}}_f(\mathbf{X_s}))}{\mathcal{H}(\mathcal{P}^{\textsf{l}}_f(\mathbf{X}_s))}.
\end{equation}
As the name suggests, relative hypervolume improvement is a normalized measure of improvement and therefore enables the assessment of batch evaluation across different intervention sets.
Given $\mathbf{B}_1,\dots,\mathbf{B}_{|\mathcal{S}|}$, we propose the following batch selection strategy for \textsc{Causal ParetoSelect}:
\begin{equation}\label{eq:mo_cbo.batch_selection}
    \mathbf{B}_{\hat{s}}= \underset{ \mathbf{B}_s \in \{\mathbf{B}_1,\dots,\mathbf{B}_{{|\mathcal{S}|}}\}}{\text{arg max }} \text{RHVI} (\boldsymbol{\mu}(\mathbf{X}_s,\mathbf{B}_s), \mathcal{P}_f^{\textsf{l}}(\mathbf{X_s})).
\end{equation}
Overall, the proposed batch selection is designed to alternately advance the Pareto fronts $\mathcal{P}_f^{\textsf{l}}(\mathbf{X}_1),\dots,\mathcal{P}_f^{\textsf{l}}(\mathbf{X}_{|\mathcal{S}|})$.
