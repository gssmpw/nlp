\section{Reducing the Search Space}\label{appendix:search_space_reduction}
\citet{NEURIPS2018_c0a271bc} leverage the graph topology of an \textsc{scm} to identify intervention sets that are redundant to consider in any optimisation scheme. Their formalism exploits the rules of do-calculus to identify invariances and partial-orders among intervention sets, in order to obtain those sets that could potentially yield optimal outcomes for a given graph. To take advantage of their ideas for this paper, the relevant concepts and their theoretical properties must be extended to accommodate multi-target settings, which will be the focus of this section.

Let $\langle \mathbf{V}, \mathbf{U}, \mathbf{F}, P(\textbf{U}) \rangle$ denote an \textsc{scm} and $\mathcal{G}$ its associated acyclic graph that encodes the underlying causal mechanisms. Recall that we assume $\mathbf{C} = \varnothing$, i.e., there are no non-manipulative variables. In this section, we require the notation $\mathbb{E}_{P(\mathbf{W} | \text{do}(\mathbf{X}_s = \mathbf{x}_s))}[\mathbf{W}] := \mathbb{E}[\mathbf{W} | \text{do}(\mathbf{X}_s = \mathbf{x}_s)]$ for sets $\mathbf{X}_s \subseteq \mathbf{X}$, $\mathbf{W} \subseteq \mathbf{V}$.

\subsection{Equivalence of Intervention Sets}\label{subsec:causal_bandits_mis}

As a first step, we establish invariances within $\mathbb{P}(\mathbf{X})$ in regards to the effects of intervention sets on the target variables. Recall the following definition from the main part of the paper.

\textbf{Definition \ref{def:causal_bandits.mis} \textnormal{(Minimal intervention set)}.} A set $\mathbf{X}_s \in \mathbb{P}(\mathbf{X})$ is called a \textit{minimal intervention set} if, for every \textsc{scm} conforming to $\mathcal{G}$, there exists no subset $\mathbf{X}_s' \subset \mathbf{X}_s$ such that for all $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ it holds $\mu(\mathbf{X_s},\mathbf{x}_s) = \mu(\mathbf{X}_s',\mathbf{x}_s[\mathbf{X}_s'])$, for all $1 \leq i \leq m$.

We denote the set of minimal intervention sets with $\mathbb{M}_{\mathcal{G},\mathbf{Y}}$. In other words, no subset of a minimal intervention set can achieve the same expected outcome on $\mathbf{Y}$. Intervention sets, that are not \textit{minimal} in the sense of Definition \ref{def:causal_bandits.mis}, are redundant to consider in any optimization task. 

\textbf{Proposition \ref{prop:causal_bandits.mis}.} 
\textit{$\mathbf{X}_s \in \mathbb{P}(\mathbf{X})$ is a minimal intervention set if and only if it holds $\mathbf{X}_s \subseteq \textnormal{an}(\mathbf{Y})_{\mathcal{G}_{\overline{\mathbf{X}}_s}}$.}

\begin{proof}
    (If) Let $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ be any intervention value. Assume that there is a subset $\mathbf{X}_s' \subset \mathbf{X}_s$ such that $\mathbb{E}[Y_i | \text{do}(\mathbf{X}_s = \mathbf{x}_s)] = \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s' = \mathbf{x}_s[\mathbf{X}_s'])]$ for all $1 \leq i \leq m$. Consider an \textsc{scm} with real-valued variables where each $V \in \mathbf{V}$ is associated with its own binary exogenous variable $U_V$ with $P(U_V=1)=0.5$. Let the function of an endogenous variable be the sum of values of its parents. For the sake of contradiction, assume $\mathbf{X}_s \subseteq \text{an}(\mathbf{Y})_{\mathcal{G}_{\overline{\mathbf{X}}_s}}$. Then, there exists a directed path from $\mathbf{X}_s \backslash \mathbf{X}_s'$ to some $Y_i$ without passing $\mathbf{X}_s'$. Hence, setting $\mathbf{W} = \mathbf{X}_s \backslash \mathbf{X}_s'$ to the values \mbox{$\mathbf{w} = \mathbb{E}[\mathbf{W} | \text{do}(\mathbf{X}_s' = \mathbf{x}_s[\mathbf{X}_s'])] + 1$} yields \mbox{$\mathbb{E}[Y_i | \text{do}(\mathbf{W} = \mathbf{w}, \mathbf{X}_s' = \mathbf{x}_s[\mathbf{X}_s'])] >$} \mbox{$\mathbb{E}[Y_i | \text{do}(\mathbf{X}_s' = \mathbf{x}_s[\mathbf{X}_s'])]$}, contradicting the assumption.
    
    (Only if) Assume that $\mathbf{X}_s \not\subseteq \text{an}(\mathbf{Y})_{\mathcal{G}_{\overline{\mathbf{X}}_s}}$. Then, for $\mathbf{X}_s' = \mathbf{X}_s \cap \text{an}(\mathbf{Y})_{\mathcal{G}_{\overline{\mathbf{X}}_s}}$ it holds $\mathbf{X}_s' \subset \mathbf{X}_s$ and by the third rule of do-calculus, for every $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ it holds $\mathbb{E}[Y_i | \text{do}(\mathbf{X}_s = \mathbf{x}_s)] = \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s' = \mathbf{x}_s[\mathbf{X}_s'])]$, $1 \leq i \leq m$. This is a contradiction because $\mathbf{X}_s$ was assumed to be a minimal intervention set.
\end{proof}

\subsection{Partial-Orders among Intervention Sets}\label{subsec:causal_bandits_pomis}

Recall the definition of possibly Pareto-optimal minimal intervention sets.

\textbf{Definition \ref{def:causal_bandits.pomis} \textnormal{(Possibly Pareto-optimal minimal intervention set)}.}
    A set $\mathbf{X}_s \in \mathbb{M}_{\mathcal{G},\mathbf{Y}}$ is called \textit{possibly Pareto-optimal} if, for at least one \textsc{scm} conforming to $\mathcal{G}$, there exists $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ such that $(\mathbf{X}_s,\mathbf{x}_s)$ is Pareto-optimal for $\mathbb{P}(\mathbf{X})$, and for no $\mathbf{X}_s' \in \mathbb{M}_{\mathcal{G},\mathbf{Y}} \backslash \mathbf{X}_s, \mathbf{x}_s' \in \mathcal{D}(\mathbf{X}_s')$ it holds $\mu(\mathbf{X}_s',\mathbf{x}_s') \leq \mu(\mathbf{X}_s,\mathbf{x}_s)$, for all $1\leq i \leq m$.

Characterizing such sets is the aim of this section. For simplicity, we first consider the special case in which $\mathcal{G}$ exhibits no unobserved confounders between $Y_i$ and any of its ancestors. 

\textbf{Proposition \ref{prop:causal_bandits.pomis_no_confounders}.} \textit{If no $Y_i$ is confounded with $\textnormal{an}(Y_i)_{\mathcal{G}}$ via unobserved confounders, then $\textnormal{pa}(\mathbf{Y})_{\mathcal{G}}$ is the only possibly Pareto-optimal minimal intervention set.}


\begin{proof}\label{proof:causal_bandits.pomis_no_confounders}
    Let $\mathbf{X}_s = \text{pa}(\mathbf{Y})_{\mathcal{G}}$, and let $\text{pa}(\mathbf{Y})_{\mathcal{G}} \neq \mathbf{X}_s'$ be another minimal intervention set with $\mathbf{x}_s' \in \mathcal{D}(\mathbf{X}_s')$. Define $\mathbf{Z}=\mathbf{X}_s \backslash (\mathbf{X}_s' \cap \mathbf{X}_s)$ and $\mathbf{W}=\mathbf{X}_s' \backslash (\mathbf{X}_s' \cap \mathbf{X}_s)$. Moreover, we choose an intervention value $\mathbf{x}_s^* \in \mathcal{D}(\mathbf{X}_s)$ such that it dominates $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ which is given by $\mathbf{x}_s[\mathbf{X}_s']=\mathbf{x}_s'[\mathbf{X}_s]$ and $\mathbf{x}_s[\mathbf{Z}] = \mathbb{E}[\mathbf{Z} | \text{do}(\mathbf{X}_s' = \mathbf{x}_s')]$. If $\mathbf{x}_s$ is non-dominated, define $\mathbf{x}_s^*=\mathbf{x}_s$. Then, for all $i=1,\dots,m$ it holds
    \begin{align}
        \mathbb{E} [Y_i | \text{do}(\mathbf{X}_s = \mathbf{x}_s^*)] & = \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s \cap \mathbf{X}_s' = \mathbf{x}_s^*[\mathbf{X}_s'], \mathbf{Z} = \mathbf{x}_s^*[\mathbf{Z}])] \\
        &\leq \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s \cap \mathbf{X}_s' = \mathbf{x}_s[\mathbf{X}_s'], \mathbf{Z} = \mathbf{x}_s[\mathbf{Z}])] \\
        & = \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s \cap \mathbf{X}_s' = \mathbf{x}_s[\mathbf{X}_s'], \mathbf{Z} = \mathbf{x}_s[\mathbf{Z}], \mathbf{W}=\mathbf{x}_s'[\mathbf{W}])] \\
        & = \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s \cap \mathbf{X}_s' = \mathbf{x}_s[\mathbf{X}_s'], \mathbf{W}=\mathbf{x}_s'[\mathbf{W}]), \mathbf{Z} = \mathbf{x}_s[\mathbf{Z}]] \\
        &= \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s \cap \mathbf{X}_s' = \mathbf{x}_s[\mathbf{X}_s'], \mathbf{W}=\mathbf{x}_s'[\mathbf{W}])] \displaybreak[1] \\
        &= \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s' = \mathbf{x}_s')],
    \end{align}
    where the inequality holds because $\mathbf{x}_s$ (weakly) dominates $\mathbf{x}_s^*$. Note that the second and third equalities are derived through the third and second rules of do-calculus, respectively. The second rule of do-calculus assumes that $Y_i$ is not confounded with $\text{an}(Y_i)_{\mathcal{G}}$ via unobserved confounders. For $\mathbf{X}_s = \text{pa}(\mathbf{Y})_{\mathcal{G}}$, it is possible to construct an \textsc{scm}, conforming to $\mathcal{G}$, such that strict inequality holds for some $Y_i$, see the proof of \autoref{prop:causal_bandits_IB}. This shows that $\text{pa}(\mathbf{Y})_{\mathcal{G}}$ is the only possibly Pareto-optimal minimal intervention set.
\end{proof}

We continue and study the more general case where unobserved confounders can be present between $Y_i$ and any of its ancestors. For this intent, we extend two existing concepts, called \textit{minimal unobserved-confoundersâ€™ territory} and \textit{interventional border} \citep{NEURIPS2018_c0a271bc}, to the multi-objective setting. Using these notions, we derive results which can fully characterize possibly Pareto-optimal minimal intervention sets in the aforementioned scenario.

\textbf{Definition \ref{def:mo_cbo.uc_territory} \textnormal{(Minimal unobserved confounders' territory)}.}
Let $\mathcal{H}=\mathcal{G}[\text{An}(\mathbf{Y})_{\mathcal{G}}]$. A set of variables $\mathbf{T}$ in $\mathcal{H}$, with $\mathbf{Y} \subseteq \mathbf{T}$, is called a \textit{UC-territory} for $\mathcal{G}$ w.r.t. $\mathbf{Y}$ if $\text{De}(\mathbf{T})_{\mathcal{H}}=\mathbf{T}$ and $\text{CC}(\mathbf{T})_{\mathcal{H}} = \mathbf{T}$. The UC-territory $\mathbf{T}$ is said to be \textit{minimal}, denoted $\mathbf{T} = \text{MUCT}(\mathcal{G},\mathbf{Y})$, if no $\mathbf{T}' \subset \mathbf{T}$ is a UC-territory.

A minimal UC-territory for $\mathcal{G}$ w.r.t. $\mathbf{Y}$ can be constructed by extending a set of variables, starting from $\mathbf{Y}$, and iteratively updating the set with the c-component and descendants of the set. More intuitively, it is the minimal subset of $\text{An}(\mathbf{Y})_{\mathcal{G}}$ that is governed by unobserved confounders, where at least one target $Y_i$ is adjacent to an unobserved confounder.

\textbf{Definition \ref{def:mo_cbo.int_border} \textnormal{(Interventional border)}.}
Let $\mathbf{T}=\text{MUCT}(\mathcal{G},\mathbf{Y})$. Then, $\mathbf{B} = \text{pa}(\mathbf{T})_{\mathcal{G}} \backslash \mathbf{T}$ is called the \textit{interventional border} for $\mathcal{G}$ w.r.t. $\mathbf{Y}$, denoted as $\text{IB}(\mathcal{G},\mathbf{Y})$.

We have already described these concepts in the main part. Before connecting the notion of minimal UC-territory and interventional border to possibly Pareto-optimal minimal intervention sets, we require the following proposition:

\begin{proposition}
    \label{prop:causal_bandits.subsumption}
    Let $\mathbf{T}$ be a minimal UC-territory and $\mathbf{B}$ an interventional border for $\mathcal{G}$ w.r.t. $\mathbf{Y}$. Let $\mathbf{X}_s \subseteq \mathbf{X}$ be an intervention set and $\mathbf{S} = (\mathbf{T} \cap \mathbf{X}_s) \cup \mathbf{B}$. Then, for any $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ there exists $\mathbf{s} \in \mathcal{D}(\mathbf{S})$ such that $\mathbb{E}[Y_i | \textnormal{do}(\mathbf{S}=\mathbf{s})] \leq \mathbb{E}[Y_i | \textnormal{do}(\mathbf{X}_s=\mathbf{x}_s)]$, for all $i=1,\dots,m$.
\end{proposition}

\begin{proof}
    (Case $\mathbf{B} \subseteq \mathbf{X}_s$) Let $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ be an intervention value. Then, by the third rule of do-calculus, it holds $\mathbb{E}[Y_i | \text{do}(\mathbf{X}_s=\mathbf{x}_s)]=\mathbb{E}[Y_i | \text{do}(\mathbf{X}_s \cap (\mathbf{T} \cup \mathbf{B}) =\mathbf{x}_s[\mathbf{T} \cup \mathbf{B}])]$, $1 \leq i \leq m$. Since $\mathbf{X}_s \cap (\mathbf{T} \cup \mathbf{B}) = \mathbf{S}$, by setting $\mathbf{s} = \mathbf{x}_s[\mathbf{T} \cup \mathbf{B}]$, it follows $\mathbb{E}[Y_i | \text{do}(\mathbf{X}_s=\mathbf{x}_s)] = \mathbb{E}[Y_i | \text{do}(\mathbf{S}=\mathbf{s})]$.
    \vspace{0.2cm}\\
    (Case $\mathbf{B} \not\subseteq \mathbf{X}_s$) Let $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ be an intervention value. We define $\mathbf{B}' = \mathbf{S} \backslash (\mathbf{X}_s \cap \mathbf{S}) = \mathbf{S} \backslash (\mathbf{X}_s \cap (\mathbf{T} \cup \mathbf{B})) = \mathbf{B} \backslash (\mathbf{X}_s \cap \mathbf{B})$ and $\mathbf{W} = \mathbf{X}_s \backslash (\mathbf{X}_s \cap \mathbf{S}) = \mathbf{X}_s \backslash (\mathbf{X}_s \cap (\mathbf{T} \cup \mathbf{B}))$. Moreover, let $\mathbf{s}^* \in \mathcal{D}(\mathbf{S})$ such that it dominates $\mathbf{s} \in \mathcal{D}(\mathbf{S})$, which is given by $\mathbf{s}[\mathbf{B}'] = \mathbb{E}[\mathbf{B}' | \text{do}(\mathbf{X}_s=\mathbf{x}_s)]$ and $\mathbf{s}[\mathbf{X}_s] = \mathbf{x}_s[\mathbf{T} \cup \mathbf{B}]$. If $\mathbf{s}$ is non-dominated, we set $\mathbf{s}^* = \mathbf{s}$. Then, for all $i=1,\dots,m$ it holds
    \begin{align}
        \mathbb{E}[Y_i | \text{do}(\mathbf{S}=\mathbf{s}^*)] &= \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s \cap (\mathbf{T} \cup \mathbf{B}) =\mathbf{s}^*[\mathbf{X}_s], \mathbf{B}'=\mathbf{s}^*[\mathbf{B}'])]\\
        & \leq \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s \cap (\mathbf{T} \cup \mathbf{B}) =\mathbf{s}[\mathbf{X}_s], \mathbf{B}'=\mathbf{s}[\mathbf{B}'])] \\
        &= \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s \cap (\mathbf{T} \cup \mathbf{B}) =\mathbf{s}[\mathbf{X}_s], \mathbf{B}'=\mathbf{s}[\mathbf{B}'], \mathbf{W}=\mathbf{x}_s[\mathbf{W}])]\\
        &=\mathbb{E}[Y_i | \text{do}(\mathbf{X}_s \cap (\mathbf{T} \cup \mathbf{B}) =\mathbf{s}[\mathbf{X}_s], \mathbf{W}=\mathbf{x}_s[\mathbf{W}]), \mathbf{B}'=\mathbf{s}[\mathbf{B}']]\\
        &= \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s \cap (\mathbf{T} \cup \mathbf{B}) =\mathbf{s}[\mathbf{X}_s], \mathbf{W}=\mathbf{x}_s[\mathbf{W}])]\\
        &= \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s=\mathbf{x}_s)],
    \end{align}
    where the inequality holds because $\mathbf{s}$ is (weakly) dominated by $\mathbf{s}^*$. Furthermore, the second and third equalities are derived through the third and second rules of do-calculus, respectively.
\end{proof}

The following proposition is a building block for characterizing possibly Pareto-optimal minimal intervention sets via interventional borders. The proof is similar to the one given by \citet{NEURIPS2018_c0a271bc} 

\begin{proposition}
    \label{prop:causal_bandits_IB}
    $\textnormal{IB}(\mathcal{G},\mathbf{Y})$ is a possibly Pareto-optimal minimal intervention set.
\end{proposition}

\begin{proof}
    The intuition of this proof is to construct an \textsc{scm}, conforming to $\mathcal{G}$, for which the single best strategy involves intervening on $\text{IB}(\mathcal{G},\mathbf{Y})$. Let $\mathbf{T}$ and $\mathbf{B}$ denote $\text{MUCT}(\mathcal{G},\mathbf{Y})$ and $\text{IB}(\mathcal{G},\mathbf{Y})$, respectively. Every exogenous variable in $\mathbf{U}$ shall be a binary variable with its domain being $\{ 0, 1\}$. Let $\oplus$ denote the exclusive-or function and $\bigvee$ the logical OR operator.
    \vspace{0.2cm}\\
    (Case $\mathbf{T} = \mathbf{Y}$) 
    In this case, $\mathbf{B}$ corresponds to the parents of $\mathbf{Y}$. Therefore, no target variable $Y_i$ is confounded with $\text{an}(Y_i)_{\mathcal{G}}$ via unobserved confounders. Define an \textsc{scm} such that
    \begin{itemize}
        \item Each endogenous variable $V \in \mathbf{V}$ is influenced by an exogenous variable $U_V \in \mathbf{V}$;
        \item $f_{Y_i} = \bigvee \mathbf{u}^{Y_i} \oplus \bigvee \mathbf{pa}_{Y_i}$ with $P(\mathbf{U}^{Y_i}=0) \approx 1$, for all $i=1,\dots,m$;
        \item $f_{X} = (\bigoplus \mathbf{u}^X) \oplus (\bigoplus \mathbf{pa}_X)$ for $X \in \mathbf{X}$ and $P(U = 0) = 0.5$ for every $U \in \mathbf{U} \backslash (\bigcup_{i=1}^m \mathbf{U}^{Y_i})$. 
    \end{itemize}
    By the third rule of do-calculus and by taking conditional expectations, it holds 
    \begin{align}
        \mathbb{E}[Y_i | \text{do}(\mathbf{B}=0)] &= \mathbb{E}[Y_i | \text{do}(\text{pa}({Y_i})_{\mathcal{G}}=0)] \\
        &= \mathbb{E}[Y_i | \text{do}(\text{pa}({Y_i})_{\mathcal{G}}=0), \mathbf{U}^{Y_i}\neq0] P(\mathbf{U}^{Y_i}\neq0) + \mathbb{E}[Y_i | \text{do}(\text{pa}({Y_i})_{\mathcal{G}}=0), \mathbf{U}^{Y_i}=0] P(\mathbf{U}^{Y_i}=0)\\
        &\approx 0
    \end{align}
    for every $1 \leq i \leq m$. Meanwhile, all other interventions yield expectations greater than or equal to 0.5 in at least one component. Therefore, $\mathbf{B}$ is a possibly Pareto-optimal minimal intervention set.
 
    (Case $\mathbf{T} \subset \mathbf{Y}$) In this case, at least one target variable $Y_i$ has an unobserved confounder with its ancestors. As a first step, it will be shown that there exists an \textsc{scm}, conforming to $\mathcal{H} = \mathcal{G}[\mathbf{T} \cup \mathbf{B}]$, where the intervention $\text{do}(\mathbf{B} = 0)$ is the single best strategy. To achieve this, we first define individual \textsc{scm}s for each unobserved confounder in $\mathcal{H}[\mathbf{T}]$, and merge them into a single \textsc{scm} where $\text{do}(\mathbf{B} = 0)$ is indeed the best strategy. Let $\mathbf{U}' = \{ U_j\}_{j=1}^k$ be the set of unobserved confounders in $\mathcal{H}[\mathbf{T}]$.

    Given $U_j \in \mathbf{U}'$, let $B^{(j)}$ and $R^{(j)}$ denote its two children. We define an \textsc{scm} $\mathcal{M}_j$, where the graph structure is given by
 
    \begin{equation}
        \mathcal{H}_j = \mathcal{H} \left[ \text{De}\left( \left\{ B^{(j)}, R^{(j)}\right\}\right)_{\mathcal{H}} \cup \left( \mathbf{B} \cap \text{pa}\left(\text{De} \left( \left\{ B^{(j)}, R^{(j)}\right\} \right)_{\mathcal{H}} \right) \right)\right],
    \end{equation}
    and all bidirected edges, except for $U_j$, are removed. In order to set the structural equations for variables in $\mathcal{H}_j$, the vertices will be labelled via colour coding: Let vertices in $\text{De}\left(B^{(j)}\right)_{\mathcal{H}} \backslash \text{De}\left(R^{(j)}\right)_{\mathcal{H}}$ be labelled as \textcolor{blue}{blue}, $\text{De}\left(R^{(j)}\right)_{\mathcal{H}} \backslash \text{De}\left(B^{(j)}\right)_{\mathcal{H}}$ as \textcolor{red}{red}, and $\text{De}\left(B^{(j)}\right)_{\mathcal{H}} \cap \text{De}\left(R^{(j)}\right)_{\mathcal{H}}$ as \textcolor{purple}{purple}. All target variables are coloured as \textcolor{purple}{purple} as well. Moreover, $B^{(j)}$ and $R^{(j)}$ shall perceive $U_j$ as a parent coloured as \textcolor{blue}{blue} with value $U_j$ and \textcolor{red}{red} with value $1-U_j$, respectively. The \textcolor{blue}{blue}-, \textcolor{red}{red}- and \textcolor{purple}{purple}-coloured variables are set to 3 if any of their parents in $\mathbf{B}$ is not 0. Otherwise, their values are determined as follows. For every \textcolor{blue}{blue} and \textcolor{red}{red} vertex, the associated structural equation returns the common value of its parents of the same colour and returns 3 if coloured parents' values are not homogeneous. For every \textcolor{purple}{purple} vertex, its corresponding equation returns 2 if every \textcolor{blue}{blue}, \textcolor{red}{red} and \textcolor{purple}{purple} parent is 0,1, and 2, respectively, and returns 1 if 1,0,1, respectively.

    Next, the \textsc{scm}s $\mathcal{M}_1,\dots,\mathcal{M}_k$ will be merged into one single \textsc{scm}, that conforms to $\mathcal{H}$, and for which $\text{do}(\mathbf{B}=0)$ is the single best intervention. Note that in $\mathcal{M}_j$ all variables can be represented with just two bits. To construct a unified \textsc{scm}, variables in $\mathbf{T}$ are represented with $2k$ bits, where $\mathcal{M}_j$ takes the $2j-1^{\text{th}}$ and $2j^{\text{th}}$ bits. Every target variable $Y_i$ is represented as a sequence of bits and binarised as follows. $Y_i$ is set to $0$ if its $2j-1^{\text{th}}$ and $2j^{\text{th}}$ bits are 00, 01 or 10 for every $1 \leq j \leq k$, and $1$ otherwise. Let $P(U_j=1) = 0.5$ for $U_j \in \mathbf{U}'$. Therefore, it holds $Y_i = 0$ if $\text{do}(\mathbf{B}=0)$ and $Y_i=1$ if $\text{do}(\mathbf{B}\neq 0)$.
    If any variable in $\mathbf{T}$ is intervened, then at least one \textsc{scm} $\mathcal{M}_j$ will be disrupted, resulting in an expectation larger than or equal to 0.5 for at least one target variable.
    In the multi-target setting, it may happen that some target variables do not occur in any of the $\mathcal{M}_j$'s. This happens if a target $Y_i$ has no parents in  $\mathbf{T}$, but only in $\mathbf{B}$. For all such $Y_i$'s, we set $f_{Y_i} = \mathbf{u}^{Y_i} \oplus \bigvee \mathbf{pa}_{Y_i}$ with $P(\mathbf{U}^{Y_i} = 0) \approx 1$. As such, the newly constructed \textsc{scm} enforces $\mathbb{E}[Y_i | \text{do}(\mathbf{B}=0)] \approx 0$. Meanwhile, all other interventions yield expectations greater than or equal to 0.5
    \vspace{0.2cm}\\
    As a last step, the previously defined \textsc{scm} for $\mathcal{H} = \mathcal{G}[\mathbf{T} \cup \mathbf{B}]$, will be extended to an \textsc{scm} for $\mathcal{G}$. However, we can ignore joint probability distributions for any exogenous variables only affecting endogenous variables outside of $\mathcal{H}$. Setting structural equations for endogenous variables outside of $\mathcal{H}$ is redundant as well. For $V \in \text{An}(\mathbf{Y})_{\mathcal{G}} \backslash \mathbf{T}$, we define the structural equations as $f_V = (\bigoplus \mathbf{u}^V) \oplus (\bigoplus \mathbf{pa}_V)$. For $U \in \mathbf{U} \backslash \mathbf{U}'$, we set $P(U=0)=0.5$ if $U$'s child(ren) is disjoint to $\mathbf{T}$, and $P(U=0)\approx 1$ otherwise. Note that $\text{do}(\mathbf{B}=0)$ is still the single optimal intervention. Therefore, $\mathbf{B}$ is a possibly Pareto-optimal minimal intervention set. 
\end{proof}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{SA_IB_constr_graphs.png}
    \caption{Original causal graph $\mathcal{G}$ and its color-coded subgraphs for each unobserved confounder.}
    \label{fig:causal_bandits.IB}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{SA_IB_constr_table.png}
    \caption{Values for $\mathcal{M}_1$, $\mathcal{M}_2$ and $\mathcal{M}$ given $X_4= X_5=0$. The target variables are shown as bit sequences, $Y_1'$ and $Y_2'$, as well as binary values, $Y_1$ and $Y_2$.}
    \label{tab:causal_bandits_IB}
\end{figure} 

In order to illustrate the construction of an \textsc{scm} where $\text{do}(\text{IB}(\mathcal{G}, \mathbf{Y}) = 0)$ is the single best strategy, consider \cref{fig:causal_bandits.IB}, showing an exemplary graph and its colour-coded subgraphs, $\mathcal{H}_1$ and $\mathcal{H}_2$, for each unobserved confounder. \cref{tab:causal_bandits_IB} presents the associated values for $\mathcal{M}_1$ and $\mathcal{M}_2$, as well as values for the target variables in the final \textsc{scm} $\mathcal{M}$. The next proposition generalizes the previous one.


\textbf{Proposition \ref{prop:causal_bandits.intervenention_IB_pomis}.}
\textit{$\textnormal{IB}(\mathcal{G}_{\overline{\mathbf{X}}_s}, \mathbf{Y})$ is a possibly Pareto-optimal minimal intervention set for any $\mathbf{X}_s \in \mathbb{P}(\mathbf{X})$.}


\begin{proof}
  Let $\mathbf{X}_s$ be an intervention set. Let us denote $\mathbf{T} = \text{MUCT}(\mathcal{G}_{\overline{\mathbf{X}}_s},\mathbf{Y})$, $\mathbf{B} = \text{IB}(\mathcal{G}_{\overline{\mathbf{X}}_s},\mathbf{Y})$ and $\mathbf{T}_0 = \text{MUCT}(\mathcal{G},\mathbf{Y})$. Using the strategy from \autoref{prop:causal_bandits.intervenention_IB_pomis}, we construct an \textsc{scm} for $\mathcal{G}[\mathbf{T} \cup \mathbf{B}]$ while ignoring unobserved confounders between $\mathbf{T}$ and $\mathbf{T}_0 \backslash \mathbf{T}$. Let $\mathbf{U}'$ be the set of such unobserved confounders. Now, the \textsc{scm} needs to be modified to ensure that $\text{do}(\mathbf{B}=0)$ is the single best intervention. Every $U \in \mathbf{U}'$ shall flip (i.e., $0 \xleftrightarrow{} 1$) the value of its endogenous child in $\mathbf{T}$ whenever $U=1$. Let $P(U=0) \approx 1$, so that it holds $\mathbb{E}[Y_i | \text{do}(\mathbf{B}=0)] \approx 0$. Intervening on $\mathbf{B} \neq 0$ or on any variable in $\mathbf{T}$ results in expectations around 0.5 or above.
\end{proof}


Notably, Proposition \ref{prop:causal_bandits.intervenention_IB_pomis} extends Proposition \ref{prop:causal_bandits_IB} when $\mathbf{X}_s \neq \varnothing$. Note that, by iterating over all intervention sets $\mathbf{X}_s \in \mathbb{P}(\mathbf{X})$, we can discover possibly Pareto-optimal minimal intervention sets in a given graph. The following theorem is an extension of the main result by \citet{NEURIPS2018_c0a271bc} to the scenario where multiple target variables are present. It shows that the aforementioned strategy suffices to find not some, but all, such sets.

\textbf{Theorem \ref{thm:causal_bandits}.}
    \textit{A set $\mathbf{X}_s$ is a possibly Pareto-optimal minimal intervention set if and only if it holds $\textnormal{IB}(\mathcal{G}_{\overline{\mathbf{X}}_s}, \mathbf{Y}) = \mathbf{X}_s$.}

\begin{proof}
    (If) This is a special case of Proposition \ref{prop:causal_bandits.intervenention_IB_pomis}.
    \vspace{0.2cm}\\
    (Only if) Let $\mathbf{X}_s$ be a minimal intervention set and $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ an intervention value. Denote $\mathbf{T}=\text{MUCT}(\mathcal{G}_{\overline{\mathbf{X}}_s},\mathbf{Y})$, $\mathbf{B}=\text{IB}(\mathcal{G}_{\overline{\mathbf{X}}_s},\mathbf{Y})$, $\mathbf{T}_0=\text{MUCT}(\mathcal{G},\mathbf{Y})$ and $\mathbf{B}_0=\text{IB}(\mathcal{G},\mathbf{Y})$. From \autoref{prop:causal_bandits.subsumption}, we know that no \textsc{pomis} intersects with $\text{An}(\mathbf{B}_0)_{\mathcal{G}} \backslash \mathbf{B}_0$ and thus, it is possible to conclude $\mathbf{X}_s \subseteq \mathbf{T}_0 \cup \mathbf{B}_0 \backslash \mathbf{Y}$. Note that it holds $\mathbf{X}_s \subseteq \text{An}(\mathbf{B})_{\mathcal{G}}$ since otherwise it would follow $\mathbf{X}_s \cap \mathbf{T} \neq \emptyset$, which contradicts that $\mathbf{X}_s$ is neither a descendant of some variable nor confounded in $\mathcal{G}_{\overline{\mathbf{X}}_s}$. Let $\mathbf{B}' = \mathbf{B} \backslash (\mathbf{X}_s \cap \mathbf{B})$ and $\mathbf{W} = \mathbf{X}_s \backslash (\mathbf{X}_s \cap \mathbf{B})$. Moreover, we define an intervention value $\mathbf{b}^* \in \mathcal{D}(\mathbf{B})$ such that it dominates $\mathbf{b} \in \mathcal{D}(\mathbf{B})$, which is given by $\mathbf{b}[\mathbf{B}'] = \mathbb{E}[\mathbf{B}' | \text{do}(\mathbf{X}_s= \mathbf{x}_s)]$ and $\mathbf{b}[\mathbf{X}_s] = \textbf{x}_s[\mathbf{B}]$. If $\mathbf{b}$ is non-dominated, we set $\mathbf{b}^*=\mathbf{b}$. Then, for all $i=1,\dots,m$, it holds
    \begin{align}
        \mathbb{E}[Y_i | \text{do}(\mathbf{B}=\mathbf{b}^*)] &=  \mathbb{E}[Y_i | \text{do}(\mathbf{B} \cap \mathbf{X}_s = \mathbf{b}^*[\mathbf{X}_s], \mathbf{B}' = \mathbf{b}^*[\mathbf{B}'])]  \\
        &\geq  \mathbb{E}[Y_i | \text{do}(\mathbf{B} \cap \mathbf{X}_s = \mathbf{b}[\mathbf{X}_s], \mathbf{B}' = \mathbf{b}[\mathbf{B}'])] \\
        &=  \mathbb{E}[Y_i | \text{do}(\mathbf{B} \cap \mathbf{X}_s = \mathbf{b}[\mathbf{X}_s], \mathbf{B}' = \mathbf{b}[\mathbf{B}'], \mathbf{W}=\mathbf{x}_s[\mathbf{W}])] \\
        &=  \mathbb{E}[Y_i | \text{do}(\mathbf{B} \cap \mathbf{X}_s = \mathbf{b}[\mathbf{X}_s], \mathbf{W}=\mathbf{x}_s[\mathbf{W}]), \mathbf{B}' = \mathbf{b}[\mathbf{B}']]\\
        &=  \mathbb{E}[Y_i | \text{do}(\mathbf{B} \cap \mathbf{X}_s = \mathbf{b}[\mathbf{X}_s], \mathbf{W}=\mathbf{x}_s[\mathbf{W}])]\\
        &= \mathbb{E}[Y_i | \text{do}(\mathbf{X}_s = \mathbf{x}_s)],
    \end{align}
    where the inequality holds because $\mathbf{b}$ is (weakly) dominated by $\mathbf{b}^*$. Furthermore, the second and third equalities are derived through the third and second rules of do-calculus, repectively.
\end{proof}

\autoref{thm:causal_bandits} provides a necessary and sufficient condition for a set of variables to be a possibly Pareto-optimal minimal intervention set. The proof of the theorem gives the following corollary:

\textbf{Corollary \ref{cor:causal_bandits}.}
    \textit{Let $\mathbf{X}_s \in \mathbb{P}(\mathbf{X})$ and $\mathbf{X}_s'=\textnormal{IB}(\mathcal{G}_{\overline{\mathbf{X}}_s}, \mathbf{Y})$. For any $\mathbf{x}_s \in \mathcal{D}(\mathbf{X}_s)$ there exist $\mathbf{x}_s' \in \mathcal{D}(\mathbf{X}_s')$ such that it holds $\mu(\mathbf{X}_s',\mathbf{x}_s') \leq \mu(\mathbf{X}_s,\mathbf{x}_s)$, for all $1\leq i \leq m$.}
