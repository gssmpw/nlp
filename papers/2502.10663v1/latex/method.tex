\section{Method}
% \heng{report accuracy of VQA component used}
We study image realism in three dimensions: correctness of fine-grained visual attributes, plausibility of unusual visual relationships, and realistic visual styles. \name evaluates each dimension with a separate module.

\subsection{Evaluation of Visual Attributes}
\name's first dimension of evaluation is the correctness of visual attributes. T2I models, when tasked with producing specific objects such as a particular animal species, are prone to generating outputs with missing or inaccurate attributes \cite{huang2024t2i, parihar2024precisecontrol}, which degrade output quality from a realism perspective. Inspired by the Davidsonian scene graph~\cite{chodavidsonian}, we design a graphical schema using a series of atomic and unique questions to verify the presence and accuracy of each visual attribute.

The process begins with an existence check to determine whether the target object is present in the image. The prompt \texttt{"Is there a realistic animal or plant in the image?"}, is sent to a VQA model for verifying. If the response is negative, the evaluation concludes with a score of zero for the image. Otherwise, the framework evaluates specific attributes based on a pre-defined schema $\mathcal{S}$, where each attribute $a \in \mathcal{S}$ consists of a pair $(p, d)$, representing the attribute's part name and description, respectively. The schema can be automatically generated using a LLM with access to a knowledge base, and we provide more details on the different ways we use to generate schema for each dataset in \S\ref{dataset}.

For each attribute $a_i = (p_i, d_i)$, the framework performs two checks:
1. A visibility check, where the model determines whether the part $p_i$ is discernible in the image using a prompt like \texttt{"Can you see the $p_i$?"}. Let $V_i \in \{0, 1\}$ denote the result, where $V_i = 1$ if $p_i$ is visible.
2. A description match check, performed only if $V_i = 1$, where the model verifies whether the part's appearance matches its description $d_i$ using a prompt like \texttt{"Is the $p_i$ $d_i$?"}. Let $M_i \in \{0, 1\}$ denote the result, and $M_i = 1$ if the description matches. The framework computes two key metrics: the confidence score $C$ defined as the total number of visible attributes, and the realism score $R$ defined as the total number of visible attributes that are correctly depicted. The notations are as follows, where $N$ denotes the total number of attributes in $\mathcal{S}$. \\
\[
C = \sum_{i=1}^N V_i, \quad R = \sum_{i=1}^N V_i \cdot M_i.
\]
Finally, the normalized attribute score is computed as the ratio between realism and confidence scores:
\[
S_{att} = \begin{cases} 
\frac{R}{C}, & \text{if } C > 0, \\
0, & \text{if } C = 0.
\end{cases}
\]

\subsection{Evaluation of Visual Relations}
The second dimension of evaluation focuses on the realism for different objects within an image, and the visual relations between them. T2I models often fail with generating unusual visual relationships \cite{gokhale2022benchmarking}, and even if the relationship is correctly depicted, the objects may look unreal as depicted in Figures~\ref{fig:teaser} and \ref{fig:method}. Therefore, we evaluate whether the objects and  relationships between them are both present and realistic.

Given a query that specifies relationships among objects, the framework evaluates each entity $e$ and the relationship $r$, where the relationship between entities $e_i$ and $e_j$ is expressed as $r_{ij}$. For each entity, two prompts are issued: one for visibility: \texttt{"Can you see a $e$?"}, and another for realism: \texttt{"Is the $e$ realistic and natural?"}. If any object is found to be missing, the evaluation concludes with a score of zero for the image. If all objects are present, each relationship is evaluated using a separate prompt, \texttt{"Can you see the $e_i$ $r_{ij}$ $e_j$?"}. The final relationship score is computed as:
\[
S_{rel} = \sum_{i=1}^N (V_i + M_i) + \sum_{i,j=1, i\neq j}^N{R_{ij}},
\]
where $V$, $M$, and $R$ refer to visibility, realism, and relationship checks, respectively, and $N$ is the total number of objects in the query.

\subsection{Evaluation of Visual Styles}
The last dimension of evaluation is the realism of visual styles. Even if the prompt asks for a photorealistic image, T2I models often produce outputs with inconsistent styles, especially for uncommon objects or unusual relationships. The outputs typically have an illustrative or cartoonish appearance, deviating sharply from the realism requirement, as shown in Figure~\ref{fig:teaser} and \ref{fig:method}

To assess the style realism of an image, we fine-tune the CLIP model~\cite{clip} on two classes of images: "photo" and "illustration." We create a fine-tuning dataset of 9,400 images, where the realistic ones are randomly sampled from the iNaturalist, Birds, and UnRel datasets, and the illustrative ones are evenly generated by the four T2I models using the prompt \texttt{"an illustration of \{CONTENT\}"}. The fine-tuning process is configured with a constrative learning objective, a learning rate of $5 \times 10^{-5}$, a batch size of 8, and five training epochs. During evaluation, an input image is processed through the fine-tuned model, which outputs a probability score indicating the likelihood of the image belonging to the "photo" class, which we use as the realism score.

By combining scores for attribute and relationship with style, \name~provides a comprehensive framework for evaluating the realism of T2I-generated images.
