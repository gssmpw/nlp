\section{Experiments}
% \heng{It would be good to conduct ablation studies to measure the impact of each dimension}
% \heng{the experiments section is very tedious. try to provide insights instead of repeating numbers from result tables. provide some qualitative analysis and examples}
We demonstrate \name's alignment with human judgment and applicability to three machine learning applications, and arrive at a realism benchmark for current T2I models. While different VQA model can be used, we choose GPT-4o for its state-of-the-art vision capability, scoring 69.1 on MMLU \cite{openai2024gpt4o}. A comparison for different VQA models is in \S\ref{sec:model}. Copyright information on scientific artifacts used are in Appendix \ref{sec:copyright}. 

\subsection{Datasets}
\label{dataset}
To evaluate \name's capability in scoring image realism, we experiment with three datasets:
\begin{itemize}
    \item iNaturalist \cite{van2018inaturalist} is a challenging dataset of natural objects. It contains 10,000 fine-grained classes of animals, plants, and fungi, and we randomly sample 200 classes for attribute and style evaluation. The dataset does not come with an attribute schema, but we create the schema for each class by crawling its "Description" column from Wikipedia, then using a GPT-4 model to extract and summarize the major parts and corresponding descriptions.
    \item Birds \cite{WahCUB_200_2011} is another fine-grained dataset containing 200 classes of bird species. Every image is annotated with 312 binary visual annotations covering features such as color and shape. We group images of the same class, and gather common annotations as attribute schema for the class. We use all 200 classes for attribute and style evaluation.
    \item UnRel \cite{peyre2017weakly} is a scene graph dataset with unusual visual relationships such as "a person carrying a bed." The number of unique object-relationship-object triplets is 76. The low resource nature of the images brings challenges for T2I models to generate high quality augmented data. We use it for visual relation and style evaluation.
\end{itemize}

\begin{table*}[ht!]
\centering
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Eval Method} & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{Birds} & \multicolumn{2}{c}{UnRel} \\ 
  & Spearman's $\rho$ & Kendall's $\tau$ & Spearman's $\rho$ & Kendall's $\tau$ & Spearman's $\rho$ & Kendall's $\tau$ \\ \midrule
SPICE & 0.0846 & 0.0596 & 0.2011 & 0.1541 & 0.2239 & 0.1758 \\
CLIP Score & 0.2176 & 0.1590 & 0.1698 & 0.1167 & 0.1670 & 0.1255 \\
GPT Score & 0.2716 & 0.2175 & 0.1106 & 0.0816 & 0.2092 & 0.1817 \\
\name & \textbf{0.5223} & \textbf{0.4281} & \textbf{0.6162} & \textbf{0.4880} & \textbf{0.5672} & \textbf{0.5034} \\ \bottomrule
\end{tabular}
\caption{Alignment with human judgment for \name and the baseline metrics on iNaturalist, Birds, and UnRel datasets. \name exhibits highest correlation, with a Spearman's $\rho$ metric of up to 0.62.}
\label{tab:correlation}
% \vspace{-1em}
\end{table*}


\subsection{Text-to-Image models}
To generate diverse and high-quality synthetic data, we utilize four T2I models from three different model families. Stable Diffusion 1.1 \cite{sd1} is an early open-source latent diffusion model from Stability AI, trained on the LAION dataset. Stable Diffusion 3.5 \cite{sd3} Turbo is a more advanced iteration featuring enhanced image quality and efficiency through a diffusion transformer. DALL-E 3 \cite{dalle3}, a proprietary model by OpenAI, leverages a diffusion-based architecture and private datasets for improved caption alignment. Kandinsky 3 \cite{kandinsky3} is an open-source latent diffusion U-Net model trained on private datasets with a focus on Russian cultural elements.

% To generate diversely-distributed and high-quality synthetic data, we use four T2I models, where the later three are updated versions from different model families: 
% \begin{itemize}
%     \item Stable Diffusion 1.1 \cite{sd1} is an early version of the open-source T2I model developed by Stability AI. It is based on a latent diffusion model and was trained on the LAION dataset \cite{schuhmann2021laion}.
%     \item Stable Diffusion 3.5 Turbo \cite{sd3} is the most recent iteration from Stability AI, featuring improved image quality and reduced inference steps. It employs a diffusion transformer and was trained using multiple datasets.
%     \item DALL-E 3 \cite{dalle3} is a proprietary T2I model developed by OpenAI. It is a transformer-based model trained on private datasets with improved, descriptive image captions.
%     \item Kandinsky 3 \cite{kandinsky3}  is an open-source T2I model based on a latent diffusion U-Net. It was trained on private datasets with an emphasis on Russian culture.
% \end{itemize}

\subsection{Alignment with Ground Truth}
To demonstrate the effectiveness of \name's evaluation schema, we first evaluate its alignment with ground-truth human judgment. We randomly select 100 images from each dataset, forming a subset of 300 images, and leverage the Amazon Mechanical Turk (MTurk) platform to gather human annotations. Details on worker demographics and evaluation format can be found in Appendix \ref{sec:mturk}.

We conduct the MTurk evaluation as follows: for each image, we manually summarize related attributes or relations, and craft questions on their realism. Then, each question is presented to three human workers for "yes" or "no" labels. The majority vote for each question is taken as the ground truth label. Finally, we compute a score for each image as the ratio between the number of positive labels and all questions. We calculate Spearman’s $\rho$ and Kendall’s $\tau$ scores to measure the correlation between ground truth score and the scores generated by \name end-to-end, and compare performance against three baseline methods:

\begin{itemize}
    \item SPICE: Caption-based evaluation proposed by \citet{hong2018inferring}. For each image, a caption is generated using the BLIP-Image-Captioning-Base model, and SPICE \cite{anderson2016spice}, a popular metric for caption evaluation, calculates the correlation between generated and ground truth caption, which is: "A realistic image of a \{NAME\}."
    \item CLIPScore \cite{hessel2021clipscore}: This method uses the CLIP-ViT-Base model to compute embedding for both the image and its caption, then returns their cosine similarity. The caption is: "A realistic image of a \{NAME\}."
    \item GPT Score: This method directly prompts GPT-4o, the VQA model used in \name, with the identical fine-grained knowledge base but without adhering to \name's evaluation schema. The prompt is: "This is an image of a \{NAME\}. Assess the realism of the image based on the description: \{DESC\}. Each correctly depicted and clearly visible attribute earns 1 point. Output the total score."
\end{itemize}

As shown in Table~\ref{tab:correlation}, \name exhibits better alignment with human judgment than the baselines, demonstrating superior realism reflection. The T2I alignment metrics, SPICE and CLIPScore, does not adapt well to realism evaluation, as reflected in their relatively low alignment scores. Despite leveraging the same attribute information and the GPT-4o model, \name improves Spearman’s $\rho$ by 37.1\% and Kendall’s $\tau$ by 31.3\% over the GPT Score, highlighting the effectiveness of \name’s question schema in guiding the VQA model.

\subsection{Applicability for Machine Learning Tasks}
To demonstrate the practical applicability of \name, we apply it to score and filter augmented data for three machine learning tasks: image classification, image captioning, and visual relationship detection. To ensure deterministic results, all model weights are initialized with a fixed seed, and we set sampling temperature for the VQA model to 0.

\subsubsection{Image Classification}
We evaluate image classification using the ViT-Base-Patch16-224 \cite{vit} model on the iNaturalist and Birds datasets. For each dataset, we construct four training sets: none, low-quality, random, and high-quality augmentation. Each dataset consists of 200 classes, with five randomly sampled non-synthetic images per class forming both the testing and "no augmentation" training sets. From the remaining images (synthetic and non-synthetic), we compute attribute ($S_{att}$) and style ($S_{sty}$) scores using \name, combining them via the heuristic $S_{att} \times S_{sty}$. The contribution of the style score is analyzed in an ablation study in \S\ref{ablation1}. Images are ranked by their combined score, with the top five per class forming the "high-quality augmentation" set, the lowest five forming the "low-quality augmentation" set, and five randomly selected images forming the "random augmentation" set. 

We train the classification model on each set separately for 10 epochs (20 epochs for the "no augmentation" set, as it has half the number of images per class), using a batch size of \texttt{8}, an initial learning rate of \texttt{5e-5} and a weight decay of \texttt{0.01} on a single GPU with 24GB memory.

Models trained with high-scoring images outperform the low-quality and random augmented ones by 10.6\% and 4.28\% in F1 score on average, as shown in Table~\ref{tab:classification}. In addition, we plot test performance against training steps in Figure~\ref{fig:inaturalist_f1}, providing a temporal view of the results. Importantly, we notice that models with low-scoring images may perform even worse than the ones without. These images either exhibit incorrect visual traits that mislead the classifier, or shows unrealistic styles, which may be used as an unreliable feature for real-world image classification. Figure~\ref{fig:example} shows a pair of examples, with a high-quality synthetic image on the left and a low-quality one on the right, whose visual attributes (color, petal number, leaf size) and style both deviate from the realistic requirement. \name's scoring effectively separates these.

% Models trained with high-quality augmentation, containing the highest-scoring images, outperform those trained with low-quality and random augmentation by 10.6\% and 4.28\% in F1 score on average, respectively, while the low-quality dataset contributes the least to model performance, even deteriorating the accuracy and F1 scores by 5.00\% and 4.95\% on iNaturalist dataset. This highlights the importance of realistic data augmentation to image classification, and \name's effectiveness for this application.

\begin{table}[h!]
\centering
\begin{tabular}{llcc}
\toprule
Dataset & Aug Method & Accuracy & F1 \\ \midrule
% \multirow{2.5}{*}{Dataset} & \multirow{2.5}{*}{Aug Method} & \multicolumn{2}{c}{Metrics} \\ \cmidrule(l){3-4} 
 % &  & Accuracy & F1 \\ \midrule
\multirow{4}{*}{iNaturalist} & none & 0.6950 & 0.6937 \\
 & low quality & 0.6450 & 0.6442 \\
 & random & 0.7200 & 0.7271 \\
 & high quality & \textbf{0.8100} & \textbf{0.8070} \\ \midrule
\multirow{4}{*}{Birds} & none & 0.6700 & 0.6649 \\
 & low quality & 0.6750 & 0.6685 \\
 & random & 0.7100 & 0.7112 \\
 & high quality & \textbf{0.7200} & \textbf{0.7170} \\ \bottomrule
\end{tabular}
\caption{Image classification training results on high-quality, random, low-quality, and no augmentation training sets according to \name scores. The model trained on high-quality sets exhibits the highest accuracy.}
\label{tab:classification}
% \vspace{-1em}
\end{table}

\begin{figure}
    \begin{minipage}{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/good.jpg}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/bad.jpg}
    \end{minipage}
    \caption{Examples of high and low quality images.}
    % \vspace{-1em}
    \label{fig:example}
\end{figure}

\subsubsection{Image Captioning}
We evaluate image captioning using the BLIP-Image-Captioning-Base \cite{blip} model on the iNaturalist and Birds datasets. The four training sets are constructed similarly to classification, except each image is assigned a caption in the format \texttt{"a photo of a \{CLASS\_NAME\}"}. The model is trained for 10 epochs with a batch size of \texttt{8}, an initial learning rate of \texttt{1e-5}, and a weight decay of \texttt{0.01} on a single GPU with 24GB memory.

We show training results in Table~\ref{tab:captioning} measured by ROUGE 1 \cite{lin2004rouge} and BLEU \cite{papineni2002bleu}. ROUGE 1 measure the unigram overlap between generated and reference captions, and BLEU computes n-gram precision with a brevity penalty that penalizes candidates that are too short relative to the reference. The high-quality augmented model outperforms the random and low-quality ones with an average of 3.88\% and 5.63\% in BLEU respectively. Despite the high-quality model did not converge the fastest, as shown in the temporal plot in Figure~\ref{fig:birds_bleu}, it achieves the highest accuracy. This is because \name-selected high-quality images provide rich real-world features that takes time to learn, but eventually resulting in the model's robustness to unseen data.

% The model trained on the high-quality augmentation dataset outperforms the random and low-quality ones with an average of 3.88\% and 5.63\% in BLEU respectively, and the low-quality dataset yields to a 1.86\% lower BLEU score than baseline on the iNaturalist dataset, again showing that \name scores are effective in differentiating augmented images and benefit image captioning training.

\begin{figure*}[ht!]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/inaturalist_f1.pdf}
        \caption{image classification on iNaturalist dataset (F1 score)}
        \label{fig:inaturalist_f1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/inaturalist_bleu.pdf}
        \caption{image captioning on iNaturalist dataset (BLEU score)}
        \label{fig:birds_bleu}
    \end{minipage}
    \hfill
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/mR50.pdf}
        \caption{visual relationship detection on UnRel dataset (mR@50)}
        \label{fig:mR50}
    \end{minipage}
\end{figure*}

\begin{table}[h!]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
Dataset & Aug Method & ROUGE 1 & BLEU \\ \midrule
% \multirow{2.5}{*}{Dataset} & \multirow{2.5}{*}{Aug Method} & \multicolumn{2}{c}{Metrics} \\ \cmidrule(l){3-4} 
 % &  & ROUGE 1 & BLEU \\ \midrule
\multirow{4}{*}{iNaturalist} & none & 0.8890 & 0.7194 \\
 & low quality & 0.8859 & 0.7008 \\
 & random & 0.8942 & 0.7345 \\
 & high quality & \textbf{0.9084} & \textbf{0.7612} \\ \midrule
\multirow{4}{*}{Birds} & none & 0.8753 & 0.6454 \\
 & low quality & 0.8851 & 0.6624 \\
 & random & 0.8866 & 0.6635 \\
 & high quality & \textbf{0.9042} & \textbf{0.7143} \\ \bottomrule
\end{tabular}
\caption{Image captioning training results on high-quality, low-quality, and no augmentation training sets according to \name scores. The model trained on high-quality sets exhibits the highest accuracy.}
\label{tab:captioning}
% \vspace{-1em}
\end{table}

\subsubsection{Visual Relationship Detection}
We evaluate visual relationship detection using the Relation Transformer for Scene Graph Generation (RelTR) model \cite{reltr}. RelTR combines an object detection model for entity localization with a relational transformer that generates a sparse scene graph in a single step, predicting relationships in an object–relationship–object triplet. 

We use \name's relationship and style scores to construct four training sets from the UnRel dataset, then fine-tune and compare the RelTR model, pre-trained on the Visual Genome dataset by the original authors, across these sets. As the UnRel dataset lacks object bounding boxes, we generate them using the YOLO 11 model \cite{yolo11}. Following the authors' setup, we train RelTR for 150 epochs with a batch size of \texttt{2} and an initial learning rate of \texttt{1e-4} on four GPUs with 24GB memory each.

We show training results in Table~\ref{tab:vrd} and plot test performance in Figure~\ref{fig:mR50}. We measure relationship detection performance by mean recall at 20 and 50, which calculates the proportion of relevant relationships among top 20 and 50 recommendations, respectively. The model trained on high-quality training set achieves 7.42\% higher mR@20 and 5.32\% higher mR@50 than the low-quality one, which again highlights \name's applicability to a wide range of machine learning tasks.

% We show that the model trained on high-quality training set achieves 7.42\% higher mR@20 and 5.32\% higher mR@50 than the low-quality one, and 4.06\% higher mR@20 and 4.76\% higher mR@50 than the random one, which again highlights \name's applicability to a wide range of machine learning tasks.

\begin{table}[h!]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
Dataset & Aug Method & mR@20 & mR@50 \\ \midrule
% \multirow{2.5}{*}{Dataset} & \multirow{2.5}{*}{Aug Method} & \multicolumn{2}{c}{Metrics} \\ \cmidrule(l){3-4} 
 % &  & mR@20 & mR@50 \\ \midrule
\multirow{3}{*}{UnRel} & none & 0.1036 & 0.1821 \\
 & low quality & 0.2199 & 0.2787 \\
 & Random & 0.3123 & 0.3137 \\
 & high quality & \textbf{0.3529} & \textbf{0.3613} \\ \bottomrule
\end{tabular}
\caption{Visual relationship detection training results on high-quality, low-quality, and no augmentation training sets according to \name scores. The model trained on high-quality sets exhibits the highest accuracy.}
\label{tab:vrd}
% \vspace{-1em}
\end{table}

\subsection{Realism Benchmark for T2I Models}
As \name is effective in rating augmented data, we propose using its output score to benchmark current T2I models, and show results in Table~\ref{tab:benchmark}. The benchmark composes of four scores: attribute, relationship, style, and average. While DALL-E 3 has the highest relationship score among the four T2I models, it is prone to generating illustration-style images and has the lowest style score. Stable Diffusion v3.5 exhibits higher attribute and relationship scores than its predecessor, yet its style score is lower than it. Kandinsky 3 generates images with the highest degree of style realism but its knowledge of fine-grained object attributes is relatively lacking. On average, however, \name rates Kandinsky 3 to be the best-performing among the four models in terms of output realism. We believe that the benchmark provides novel yet overlooked aspects of evaluation for current T2I models, and the results would benefit the whole community.

\begin{table*}[ht!]
\centering
\begin{tabular}{lcccc}
\toprule
T2I Model & Attribute Score & Relationship Score & Style Score & Average Score \\ \midrule
DALL-E 3 & 0.5475 & \textbf{0.7827} & 0.2430 & 0.5244 \\
SD v1.1 & 0.5717 & 0.3739 & 0.6356 & 0.5271 \\
SD v3.5 & \textbf{0.5791} & 0.7315 & 0.5380 & 0.6162 \\
Kandinsky 3 & 0.4925 & 0.7301 & \textbf{0.6745} & \textbf{0.6324} \\ \bottomrule
\end{tabular}
\caption{Realism benchmark for four popular T2I models in the three dimensions. Kandinsky 3 achieves the highest average score while DALL-E 3 has the lowest due to unrealistic output styles.}
\label{tab:benchmark}
% \vspace{-1em}
\end{table*}
