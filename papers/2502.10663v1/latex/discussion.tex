\section{Discussion}

\subsection{Performance of Different VQA Models}
\label{sec:model}
We conduct an ablation study to study the effect of using different VQA models for \name evaluation. While GPT-4o is our default VQA model, we compare the performance of four other models: BLIP2-Flan-T5-XL \cite{li2023blip}, PaliGemma-3B \cite{steiner2024paligemma}, mPLUG-Owl3-7B \cite{ye2024mplug}, and Gemini-1.5-Flash-002 \cite{team2024gemini}. The sampling temperature is always set to 0. We evaluate the models by alignment with human judgment on iNaturalist dataset, and show results in Table~\ref{tab:ablation_vqa}. We observe that the performance of commercial models (Gemini and GPT) are stronger than the open-source ones, and GPT-4o has the highest correlation with human labels. mPLUG is the recommended open-source model as it offers a correlation value close to Gemini, with a difference of 1.27\% in Kendall's $\tau$. BLIP2 exhibits poor performance mainly because it is prone to false positives, namely answering "yes" when the ground truth answer is "no". Specifically, its average score for all test images is 0.63, whereas that provided by GPT is 0.57. With the rapid improvements of T2I models, we expect \name's accuracy to further improve in the future.

\begin{table}[ht!]
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
VQA Model & Spearman's $\rho$ & Kendall's $\tau$ \\ \midrule
BLIP2 & 0.0255 & 0.0132 \\
PaLI & 0.3336 & 0.2845 \\
mPLUG & 0.4733 & 0.3969 \\
Gemini & 0.4950 & 0.4096 \\
GPT-4o & \textbf{0.5223} & \textbf{0.4281} \\ \bottomrule
\end{tabular}
}
\caption{Comparison of \name performance using different VQA models, measured by alignment with human judgment on the iNaturalist dataset.}
\label{tab:ablation_vqa}
% \vspace{-1em}
\end{table}

\subsection{Contribution of Style Score in Augmented Data Ranking}
\label{ablation1}
To demonstrate the contribution of incorporating the style score in addition to attribute and relationship scores for ranking augmented images, we conduct an ablation study on the iNaturalist dataset. The results in Table~\ref{tab:ablation_style} show that when training the image classification model, the high-quality training set achieves a 3.70\% higher F1 score when ranked using the combined attribute and style scores, compared to using the attribute score alone. This indicates that the combined score effectively filters images with realistic styles, leading to improved model performance.

\begin{table}[ht!]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
Aug Setting & Accuracy & F1  \\ \midrule
Attribute Only & 0.7700 & 0.7700 \\
Attribute + Style & \textbf{0.8100} & \textbf{0.8070} \\ \bottomrule
\end{tabular}
\caption{Ablation study on \name's use of style score in addition to attribute score for image classification.}
\label{tab:ablation_style}
% \vspace{-1em}
\end{table}

\subsection{Effect of Fine-Tuning in Visual Style Evaluation}
In order to show that our fine-tuned CLIP models better evaluates style realism than the out-of-the-box one, we conduct an ablation study using 100 real and illustrated-styled images on iNaturalist dataset, and show results in Table~\ref{tab:ablation_style}. After fine-tuning, the Spearman's $\rho$ correlation with ground truth labels increases by 4.92\% and the Kendall's $\tau$ metric increases by 4.05\%, demonstrating the effectiveness of our style dataset.

\begin{table}[ht!]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}lccc@{}}
\toprule
Setting & Spearman's $\rho$ & Kendall's $\tau$ \\ \midrule
W/o Fine-Tuning & 0.7775 & 0.6349 \\
w/ Fine-Tuning & \textbf{0.8267} & \textbf{0.6754} \\ \bottomrule
\end{tabular}
}
\caption{Comparison of style evaluation results with and without fine-tuning the CLIP model, measured by alignment with human judgment on the iNaturalist dadtaset.}
\label{tab:ablation_ft}
% \vspace{-1em}
\end{table}
