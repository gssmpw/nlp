\section{Related Work}

% \heng{add citation to xiaomeng's paper on data augmentation}
\subsection{Text-to-Image Generation Models}
The first generation of T2I models are based on Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}. \citet{reed2016generative} first applied GAN to T2I generation, demonstrating its capability in producing plausible images. The original DALL-E \cite{dalle1}, introduced by OpenAI in 2021, leveeraged an autoregressive model trained on vast amount of data to generate diverse images. Most recently, however, diffusion models \cite{ho2020denoising} became the mainstream method for image generation, and powered state-of-the-art T2I models like Stable Diffusion 3 \cite{sd3}, DALL-E 3 \cite{dalle3}, and Kandinsky 3 \cite{kandinsky3}. Despite these advancements and the ability to generate photorealistic images, challenges remain in producing faithful and realistic outputs, particularly for objects and scenes with limited training data \cite{zhang2024text}.

\subsection{Evaluation of Text-to-Image Models}  
Various evaluation metrics for T2I models in different aspects have been proposed over the years. DALL-EVAL \cite{cho2023dall} measures the reasoning capabilities and social biases of T2I models. CommonSense-T2I \cite{fu2024commonsense} evaluates the commonsense knowledge of these models. A notable family of metrics are on output faithfulness, namely the alignment between text prompt and generated image. CLIP-Based metrics like CLIPScore \cite{hessel2021clipscore} and CLIP-R \cite{park2021benchmark} leverage the CLIP model \cite{clip} to compute embeddings for both the text and the image, then returns the cosine similarity between the embeddings. More recently, VQA-based methods such as TIFA \cite{hu2023tifa} employ visual question-answering (VQA) models to answer structured questions generated from the text prompt, arriving at a faithfulness score. On top of it, Davidsonian scene graph \cite{chodavidsonian} formalizes the properties of these structured questions to ensure consistency and reliability in evaluations. Despite much progress in faithfulness metrics, reference-free evaluation that focus on image quality, especially image realism, remains largely unexplored. Inception Score \cite{salimans2016improved} and Fr√©chet inception distance \cite{heusel2017gans} are two widely-used image quality metrics, but they require ground truth images, and rely on a relatively small pre-trained classifier that is not suitable for complex datasets \cite{frolov2021adversarial}. This gap highlights the need for visual realism assessments.

% Since the introduction of Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, various T2I models have been proposed. \citet{reed2016generative} first applied GAN to T2I generation, demonstrating its capability in producing plausible images. Subsequent advancements include StackGAN \cite{zhang2017stackgan}, AttnGAN \cite{xu2018attngan}, and GigaGAN \cite{kang2023scaling}. Another family of T2I models is based on the transformer architecture \cite{vaswani2017attention}. The original DALL-E \cite{dalle1}, introduced by OpenAI in 2021, employed the autoregressive model trained on vast amount of data to generate diverse images. CogView \cite{ding2021cogview} and Parti \cite{yu2022scaling} followed a similar architecture. More recently, however, diffusion models \cite{ho2020denoising} became the mainstream method for image generation, and powered state-of-the-art T2I models like Stable Diffusion 3 \cite{sd3}, DALL-E 3 \cite{dalle3}, and Kandinsky 3 \cite{kandinsky3}. Despite these advancements and the ability to generate photorealistic images, challenges remain in producing faithful and realistic outputs, particularly for objects and scenes with limited training data \cite{zhang2024text}.

% Most notably are the evaluations for text-to-image alignment, and such metrics can be broadly categorized into caption-based, CLIP-based, and VQA-based approaches. Caption-Based methods \cite{hong2018inferring, hinz2020semantic} first generate a caption from the image using an image captioning model, then assess alignment by comparing the generated caption to the original text prompt using caption evaluation metrics like BLEU \cite{papineni2002bleu}, ROUGE \cite{lin2004rouge}, and SPICE \cite{anderson2016spice}. CLIP-Based metrics like CLIPScore \cite{hessel2021clipscore} and CLIP-R \cite{park2021benchmark} leverage the CLIP model \cite{clip} to compute embeddings for both the text and the image, then returns the cosine similarity between the embeddings. These methods generally achieve higher accuracy, but their interpretability is limited, and their performance depends heavily on the underlying embedding model. For instance, the CLIP model struggles with counting objects \cite{clip} and compositional reasoning \cite{ma2023crepe}, leading to potential evaluation errors. To address the limitations of prior methods, VQA-based methods such as TIFA \cite{hu2023tifa} employs a visual question-answering (VQA) model to answer structured questions generated from the text prompt, resulting in a faithfulness score. Additionally, the Davidsonian Scene Graph \cite{chodavidsonian} formalizes the properties of these structured questions to ensure consistency and reliability in evaluations.