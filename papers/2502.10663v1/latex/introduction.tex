\section{Introduction}
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/teaser.pdf}
    \caption{Overview of the three dimensions of realism we study. On the left, a image generated by Stable Diffusion v3.5 exhibits incorrect visual attributes for the species, resembling a cat despite having the distinctive tail. In the middle, Kandinsky 3 struggles with the unusual visual relationships, generating a person that's overlapping with the bed instead of carrying it. On the right, DALL-E 3 produces a stylized illustration instead of a photorealistic image as prompted. \name score correctly identifies all real images, whereas CLIPScore does not.}
    \label{fig:teaser}
    % \vspace{-1em}
\end{figure*}

Recent years have witnessed significant development in T2I generation models \cite{reed2016generative, xu2018attngan, sd1, dalle3, kandinsky3, sd3}. Nevertheless, challenges persist in generating images that accurately reflect demanding textual descriptions.
% \heng{point to example in the figure} 
As shown in Figure~\ref{fig:teaser} and \ref{fig:method}, this is especially true for prompts that involve objects with fine-grained details and intricate relationships between them. Several metrics have been developed to evaluate the faithfulness of generated images to their textual prompts. One of the most popular metric was CLIPScore \cite{hessel2021clipscore}, but the CLIP model it leverages is unreliable for more complex tasks such as visual reasoning \cite{clip}. To overcome this weakness, \citet{hu2023tifa} proposed TIFA, which structurally prompts a visual question answering (VQA) model for more detailed evaluation. Furthermore, Davisonian scene graph \cite{chodavidsonian} defines a detailed standard for schemed evaluation, offering insights in how the evaluation prompts should be generated to ensure reliable results.

While the above-mentioned evaluation frameworks are effective in assessing an output image's faithfulness to its textual prompt, they usually ignore to assess the realism of the generated images. It has been shown that T2I models can be applied to generate augmented data for various machine learning tasks \cite{shivashankar2023semantic, jin2024schema, jin2024armada}. For this purpose, not only faithfulness but also the realism of these images is crucial for training effective downstream machine learning models. High-quality data is essential for model performance, as poor data quality can lead to biased or inaccurate models \cite{polyzotis2019data, jain2020overview}. Despite the importance of image realism, there is a notable lack of automatic evaluation frameworks specifically designed to assess the realism of synthetic images used in data augmentation.

To bridge this gap, we propose \name, a framework for evaluating output realism of T2I models. Distinct from previous works, we measure three unique aspects of realism: correctness of fine-grained visual attributes, plausibility of unusual visual relationships, and realistic visual styles, through prompting a VQA model based on sophisticated schemas.
% \heng{need to elaborate how you measure these, add "through xxx"} 
\name demonstrates strong alignment with human judgment with a Spearman's $\rho$ score of up to 0.62, and we furthermore leverage its output scores to rank and filter augmented data for several downstream computer vision tasks, including image captioning, classification, and visual relationship detection. Empirical results indicate that images assigned high scores by our framework lead to improved training outcomes. For example, augmenting the training set with images labeled with high realism score by \name could enhance F1 score for image classification by up to 11.3\%, and using the ones with low realism scores could deteriorate it by up to 4.95\%. Finally, we benchmark four major T2I models on the three dimensions, providing insights in each model's strength and weakness that guide future researchers to improve output realism of T2I models.

Our contributions are as follows:
\begin{itemize}
    \item Unique from previous works that evaluate T2I alignment, we evaluate realism.
    \item We apply the \name framework to data augmentation in machine learning, showing that images with high realism score contribute to model training, and vice versa.
    \item We apply the \name framework to benchmark state-of-the-art T2I models, providing insights in their strengths and weaknesses.
\end{itemize}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/method.pdf}
    \caption{Overview of the three components of the \name framework. For visual attributes and relationships, \name performs schematic evaluation on the presense and realism of each component. For visual styles, \name leverages a fine-tuned CLIP model for rating.}
    \label{fig:method}
    % \vspace{-1em}
\end{figure*}
