This work introduces \bench, the first large-scale benchmark for retrieval-augmented insight generation over multiple tables, alongside \eval, a novel automated evaluation framework designed to address the limitations of conventional metrics in assessing multi-table insights. Extensive experiments reveal that even frontier LLMs and SOTA TableQA systems struggle to meet the challenges posed by \bench.