Recent advancements in table-based reasoning have expanded beyond factoid-level QA to address insight-level tasks, where systems should synthesize implicit knowledge in the table to provide explainable analyses. Although effective, existing studies remain confined to scenarios where a single gold table is given alongside the user query, failing to address cases where users seek comprehensive insights from multiple unknown tables. To bridge these gaps, we propose \textbf{\textsc{MT-RAIG Bench}}, design to evaluate systems on \textbf{R}etrieval-\textbf{A}ugmented \textbf{I}nsight \textbf{G}eneration over \textbf{M}ulitple-\textbf{T}ables.
Additionally, to tackle the suboptimality of existing automatic evaluation methods in the table domain, we further introduce a fine-grained evaluation framework \textbf{\eval}, which achieves better alignment with human quality judgments on the generated insights. 
We conduct extensive experiments and 
reveal that even frontier LLMs still struggle with complex multi-table reasoning, establishing our \bench as a challenging testbed for future research\footnote{\url{https://github.com/KWONDU/mt-raig}}.