\subsection{Data Annotation}
\label{apx:annotate_detail}
We show prompts used in data annotation process from Table \ref{pmt:step2} to \ref{pmt:step3_3}.
\paragraph{Multi-table Set Collection}
To extend the existing single table corpus into a collection of multi-table sets, we classify scenarios where multiple tables are combined into two categories: 
\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]
\item\textit{Joinable tables}:
We first classify the SQL queries in the existing dataset SPIDER based on the join operator to identify tables that can be joined and then link the tables as the multi-table set. 

\item\textit{Topic-related tables}:
To cluster topically related tables, we semantically group the tables with the table meta data. Within each grouped table set, all tables are linked with related topics (at least two matching subtitles among page, section, or caption titles) and similar headers (differing by at most one column name). To consider the spatio-temporal relationships between tables, we use exact matching while excluding numeric values.
\end{itemize}

\paragraph{Question Types}
We provide the following definitions for each type of question to human reviewers who guide the agent with initial seed questions and validate the generated samples iteratively:
\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]
    \item \textit{Analysis \& Summary}:
Synthesize multiple data sources into a coherent narrative, focusing on interpretation rather than raw figures. Identifies key metrics and contextualizes quantitative outcomes to uncover biases influencing decision-making.

    \item \textit{Performance \& Outcome}: 
Evaluate measurable achievements by linking performance metrics to qualitative success factors. Assesses growth and long-term accomplishments to provide a broader context for understanding progress.
    
    \item \textit{Comparison \& Relationship}:
Analyze relationships between data points, explaining connections between attributes. Examines structural components and grouping logic to reveal organizational hierarchies and patterns, enhancing understanding of data organization.
    \item \textit{Trend \& Pattern}:
Examine temporal or categorical changes to identify recurring behaviors and systemic shifts. Goes beyond documenting events to interpret transformations, providing insights into structural changes or cyclical phenomena.
\end{itemize}

\subsection{Quality Control}
\label{apx:verify_detail}
\paragraph{Agent Self-Verification}
We first utilize an agent annotator as a self-verifier, ensuring that each multi-table set, question, and insight triple satisfies strict criteria:

\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]
    \item \textbf{\textit{Relevance}}: Does the question appropriately capture the relationships between the tables in the multi-table set, ensuring that it pertains to all tables and can be answered solely using the provided information?
    
    \item \textbf{\textit{Faithfulness}}: Does the insight accurately reflect the information within the multi-table set, ensuring it remains grounded in the given data while providing a clear and unambiguous response?
    
    \item \textbf{\textit{Completeness}}: Does the insight fully and logically address the question, covering all necessary aspects while maintaining clarity and coherence?
\end{itemize}

Any triple (multi table set, question, and insight) that fails at least one of these standards is discarded.
Representative examples of discarded data are in Table \ref{case:filter_i} and \ref{case:filter_ii}.
We report the statistics of self-verification process in Table \ref{tab:apx_ratio}.
Prompts used in agent verification process for three criteria are shown in Table \ref{pmt:step4_1}.
\input{texTABLE/016discardedDataRatio}


\paragraph{Human Validation}
Alongside the agent verification, we incorporate human validation as critical review points for the annotated data, balancing the efficiency of automated annotation with the reliability of human annotation to ensure the quality of MT-RAIG BENCH.
We provide the human validation interface in Figure \ref{fig:step4_2}.
Specifically, we adopt the following criteria to check the quality of table,question,and insight triples:
\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]
\item \textbf{\textit{Inter-Table Relevance}}: Measures how effectively data from different tables can be connected or combined to provide comprehensive insights.
    
 \item \textbf{\textit{Table-Question Relevance}}:Assesses the extent to which the contents of a table directly support and address the given question.
  \item \textbf{\textit{Quesiton Complexity }}:Evaluates the level of difficulty and the number of factors or layers involved in understanding or answering the question.
   \item \textbf{\textit{Question Meaningfulness}}:Determines whether the question is clearly defined, significant, and natural.
\item \textbf{\textit{Question-Insight Completeness}}: Checks if the insight provides all the necessary key information to address the question.
     \item \textbf{\textit{Table-Insight Faithfulness}}:Ensures that the insights drawn accurately and reliably reflect the source tables.
\end{itemize}