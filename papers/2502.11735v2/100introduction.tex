\input{texFIGURE/001motivation}

Tables are ubiquitous in the real-world data and constitute a significant portion of the information available on the web and databases. 
While their structured nature efficiently encapsulates diverse information, it also poses challenges for developing robust table understanding systems~\citep{pasupat-liang-2015-compositional,chen-etal-2020-hybridqa,tang2024strucbenchlargelanguagemodels,Seo2024UnveilingIT}.
This inherent complexity of table data has led to a persistent demand for the systems capable of faithfully interpreting table content and presenting it to users in a human-readable format.

In response to these needs, existing table-based question answering (TQA) works~\citep{pasupat-liang-2015-compositional,Nan2021FeTaQAFT,Wang2024ChainofTableET} have predominantly focused on extracting explicit facts from a given table by developing systems that can follow the detailed instructions outlined in user queries.
However, these \textit{factoid-level} queries often constrain the systems’ operation to functioning as an extractive executor, reducing its role to retrieving only a fraction of values explicitly presented in the table. 
This narrow focus prevents the systems from comprehensively analyzing the table’s full context, which is essential for uncovering  implicit information embedded within the table.


Recently, some studies~\citep{moosavi2021scigen,Zhao2023QTSummAN,Seo2024UnveilingIT} have begun to explore more practical user information needs by moving beyond this traditional \textit{factoid-level} scenario. 
These studies address \textit{insight-level} table reasoning, wherein the user requires not just simple fact retrieval but also deeper insight mining from the table. 
This shift has led to the development of advanced agents designed to deliver explainable analyses and meaningful data insights, accommodating scenarios where users seek comprehensive interpretations and synthesized knowledge.

\input{texTABLE/001comparison}
Despite these advancements in table-based reasoning, two critical limitations remain unaddressed. 
\textbf{(1)} Existing approaches typically operate in a \textbf{closed-domain setting}, where a pre-defined gold table is provided alongside the query at test time. 
While it simplifies the testbed for evaluating the system's reasoning ability, it also introduces a significant drawback. 
In particular, requiring users to manually craft table inputs for every query is both costly and unrealistic, as users often lack prior knowledge about which specific tables are relevant to their needs.
\textbf{(2)} Most prior works focus on \textbf{single-table tasks}, assuming all source information for the reasoning is contained within a single table.
However, considering that users' information needs may require comprehensive insights spanning multiple aspects across tables, this scenario falls short in robustly handling diverse user needs.

To bridge these gaps, we propose \textbf{\textsc{MT-RAIG Bench}}, aiming to measure the system’s ability on \textbf{R}etrieval-\textbf{A}ugmented \textbf{I}nsight \textbf{G}eneration over \textbf{M}ulitple-\textbf{T}ables.
To enable more practical applications of the table-based system, our \textsc{MT-RAIG Bench} requires the system to retrieve multiple evidence tables based on the input query and integrate information across them to generate insightful response.
Compared to existing benchmarks, this task introduces new challenges by requiring the systems not only to retrieve query-relevant tables but also to faithfully extract evidences scattered among irrelevant tables and comprehensively aggregate these evidences to derive meaningful insight. 

Although \textsc{MT-RAIG Bench} can serve as a promising testbed for insight-level table reasoning, reliably evaluating long-form outputs in table-based tasks remains a longstanding challenge in the field~\citep{zhao-etal-2024-tapera}.
While recent studies~\citep{zhao-etal-2023-investigating,wang-etal-2024-revisiting} employ table-specific metrics that assess the quality of generated outputs beyond the surface-level matching, they still struggle to evaluate the quality of the insight on the MT-RAIG task.
Such limitation arises from their reliance on coarse-grained analyses, which fall short in detecting the finer distinctions needed to check both the output’s groundness on multi-tables and completeness on a multi-hop query. 
\input{texFIGURE/002benchmark}

In light of these challenges, we propose a novel decomposition-based evaluation framework \textbf{\textsc{MT-RAIG Eval}}.
To reliably assess the finer quality of long-form insights in \textsc{MT-RAIG Bench}, \textsc{MT-RAIG Eval} performs: (1) \textit{table-aware insight decomposition} to verify the explicit grounding between the fact-entangled insight and retrieved tables, (2) \textit{question-aware insight decomposition} to check whether the key steps required to address the question are completely followed in the insight. 
Our meta-evaluation validates that \textsc{MT-RAIG Eval} outperforms conventional metrics in aligning with human judgments, reliably assessing both faithfulness and completeness of generated insights.
We summarize our contributions as follows:
\begin{itemize}[leftmargin=*,topsep=2pt,itemsep=2pt,parsep=0pt]
    \item We propose \bench, the first large-scale benchmark for retrieval-augmented insight generation over multiple tables.
    \item We introduce \textsc{MT-RAIG Eval}, a novel automated evaluation framework that assesses the fine-grained quality of the generated insights.
    \item We evaluate various LLMs and SOTA methods, revealing that existing models still struggle with multi-table reasoning, establishing our benchmark as a challenging testbed for future research.
\end{itemize}
