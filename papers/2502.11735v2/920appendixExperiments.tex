\subsection{Meta Evaluation}
\label{apx:setup}
Considering that all automatic metrics are designed with distinct objectives and functionalities, direct numerical comparisons between their scores are inherently limited. 
Instead, it is intuitive that a metric’s reliability should be judged by its capacity to align with relative human preferences. 
To operationalize this principle, we adopt the meta-evaluation protocol proposed by \citet{ru2024ragchecker}, constructing a meta evaluation dataset of 250 response pairs sampled from 11 different baseline generators.
Human evaluators are then tasked with annotating each pair across two dimensions—faithfulness and completeness—by selecting one of three options: win, tie, or loss.
The annotation interface is illustrated in Figure \ref{fig:meta_ui}.
We implement \eval on the top of \texttt{gpt-4o-mini-2024-07-18}. 
To check the reproducibility of \eval, we additionally conduct an ablation study in Table~\ref{tab:apx_autoeval}, examining the impact of different backbone LLMs on \eval.
 We consider the following automatic metrics as baselines:

\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]

    \item\textbf{SacreBLEU}~\citep{Post2018ACF} standardizes BLEU ~\cite{Papineni2002BleuAM} score calculations by ensuring consistent and reproducible results.
    It measures the geometric mean of n-gram precision over the output text.
    
    \item\textbf{ROUGE-L}~\citep{Lin2003AutomaticEO} evaluates text similarity based on the longest common subsequence.
    Specifically, we reported F1 score.
    
    \item\textbf{METEOR}~\citep{Banerjee2005METEORAA} evaluates text similarity by using unigram matching between machine-generated outputs and human reference texts.
    
    \item\textbf{BERTScore}~\citep{Zhang2019BERTScoreET} computes the similarity between generated and reference texts using contextual word embeddings.
    
    \item\textbf{A3CU}~\citep{liu-etal-2023-towards-interpretable} evaluates summarization quality by directly comparing texts without extracting atomic content units, providing a human-aligned assessment of content similarity.
    
    \item\textbf{TAPAS-Acc}~\citep{Liu2022PLOGTP} is a reference-free metric that uses a TAPAS~\citep{Herzig2020TaPasWS} model fine-tuned on the TabFact~\cite{Chen2019TabFactAL} dataset to assess the faithfulness of generated text by verifying factual consistency.
    
    \item\textbf{G-Eval}~\citep{Liu2023GEvalNE} assesses the quality of generated text based on specific evaluation criteria using LLMs.
    We adopt G-Eval to separately evaluate \textit{faithfulness} and \textit{completeness} on a 5-point Likert scale.
    The evaluation prompts are provided in Table \ref{pmt:apx_geval}.

\end{itemize}

% \paragraph{Insight Generation}
% To assess the effectiveness of insight generation for a given set of multiple tables, we evaluate three categories of text generation models, including proprietary LLMs, open-source LLMs, and SOTA table-based QA (TQA) methods.
% Proprietary LLMs consist of commercial models known for their high-quality text generation and reasoning abilities.
% Open-source LLMs, which offer transparency and adaptability, leverage large-scale trainind data and advanced architectures to achieve competitive performance comparable to proprietary models.
% SOTA TQA methods focus on table-aware reasoning, explicitly designed for question-answering over table data.
% In this study, we utilize \texttt{gpt-4o-} \texttt{mini-2024-07-18} checkpoint as backbone LLM.
% Detailed explanation about generator baselines are provided at \ref{apx:gen_base}.



\subsection{Retrieval Baselines}
\label{apx:rtr_base}
To assess the effectiveness of multi-table retrieval, we evaluate both general-purpose retrievers and table-specific retrievers.
General-purpose retrievers serve as strong baselines, given their widespread use in plain retrieval tasks.
Table-specific retrievers are designed explicitly for tables, leveraging table-specific representations for improved retrieval performance.
Specifically, we consider the following retrievers as the baselines:
\paragraph{General-purpose retrievers}
\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]

    \item\textbf{BM25}~\citep{Robertson2009ThePR} is a sparse retriever that relies on a traditional bag-of-words representation to score relevant documents.
    In this study, we rank linearized tables as documents based on term frequency and inverse document frequency, leveraging lexical overlaps between questions and tables.

    \item\textbf{DPR}~\citep{Karpukhin2020DensePR} employs BERT-based encoders to independently map questions and documents into a shared embedding space.
    It learns dense vector representations that enable semantic similarity matching.
    In this study, we implement DPR utilizing \texttt{facebook/dpr-} \texttt{encoder-multiset-base}.

    \item\textbf{Contriever}~\citep{Izacard2021UnsupervisedDI} encodes questions and documents into a shared embedding space, optimizing for relevance through contrastive learning.
    In this study, we implement Contriever utilizing \texttt{facebook/} \texttt{contriever-msmacro}.

\end{itemize}

\paragraph{Table-specific retrievers}
\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]

    \item\textbf{DTR}~\citep{Herzig2021OpenDQ} is a table-specific dense retriever built on a TAPAS~\citep{Herzig2020TaPasWS} backbone, designed to effectively encode tabular structures and relationships.
    In this study, we fine-tune \texttt{google/tapas-base}.

    \item\textbf{TableLlama}~\citep{Zhang2023TableLlamaTO} is Llama 2-7B~\citep{Touvron2023Llama2O} based open-source LLM-based generalist model that is designed for a variety of table-based tasks.
    We utilize TableLlama to generate question and table embeddings for table retrieval task.
    In this study, we fine-tune \texttt{osunlp/TableLlama}.

\end{itemize}

\subsection{Generator Baselines}
\label{apx:gen_base}
To comprehensively compare the performance of insight generation across multiple tables, we consider three types of baselines: proprietary LLMs, open-source LLMs, and SOTA table question-answering methods. 
Proprietary LLMs encompass commercial models renowned for their advanced reasoning and high-quality text generation capabilities. Open-source LLMs, serving as transparent and adaptable alternatives, leverage large-scale training data and advanced architectures to deliver performance that is competitive with proprietary counterparts. SOTA TQA methods are specialized approaches explicitly engineered for complex, table-aware reasoning tasks, prioritizing accuracy in tabular question-answering. 
We consider the following models as the baseline generators: 

\paragraph{Proprietary LLMs}
\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]

    \item\textbf{o3-mini}~\citep{O3mini} is a smaller yet advanced LLM developed by OpenAI, which is designed to efficiently solve complex problems by breaking them into constituent parts.
    In this study, we leverage \texttt{o3-mini-2025-01-31} with reasoning effort parameter as medium.

    \item\textbf{GPT-4o}~\citep{GPT4o} is an advanced OpenAI's proprietary LLM known for its enhanced reasoning capabilities and performance across various disciplines.
    In this study, we leverage \texttt{gpt-4o-2024-08-06} checkpoint.

    \item\textbf{Claude 3.5 Sonnet}~\citep{Claude3S} is developed by Anthropic.
    It features improvements in coding proficiency and multimodal capabilities.
    In this study, we leverage \texttt{claude-3-5-sonnet} \texttt{-20241022} checkpoint.

\end{itemize}

\paragraph{Open-source LLMs}
\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]

    \item\textbf{DeepSeek-R1-8B}~\citep{DeepSeekAI2025DeepSeekR1IR} is distilled version of DeepSeek-R1, which is a open-source LLM released by DeepSeek AI.
    In this study, we utilize \texttt{unsloth/DeepSeek-R1-Distill-Llama} \texttt{-8B}, which is based on Llama-3.1-8B.

    \item\textbf{Qwen2-7B}~\citep{Yang2024Qwen2TR} is an open-source LLM developed by Alibaba Cloud, with the largest model containing 72 billion parameters.
    In this study, we utilize \texttt{Qwen/Qwen2-7B-} \texttt{Instruct}.

    \item\textbf{Gemma-7B}~\citep{Mesnard2024GemmaOM} is an open-source LLM developed by Google DeepMind, which is known for its multilingual capabilities and creative outputs.
    In this study, we utilize \texttt{google/gemma-7b-it}.

    \item\textbf{Llama 3.1-8B}~\citep{llama-3} is an open-source LLM released by Meta AI, which is available in multiple sizes up to 405 billion parameters.
    In this study, we utilize \texttt{meta-llama/Llama-} \texttt{3.1-8B-Instruct}.

    \item\textbf{Mistral-7B}~\citep{Jiang2023Mistral7} is an open-source LLM developed by Mistral AI.
    In this study, we utilize \texttt{mistralai/Mistral-7B-} \texttt{Instruct-v0.3}.

\end{itemize}

\paragraph{SOTA TQA-methods}
\begin{itemize}[leftmargin=*,topsep=4pt,itemsep=4pt,parsep=0pt]

    \item\textbf{Chain-of-Table}~\citep{Wang2024ChainofTableET} is LLM-based method designed to enhance table-based reasoning.
    It employs iterative reconstruction of input table through dynamic tabular operations.

    \item\textbf{TaPERA}~\citep{zhao-etal-2024-tapera} a modular framework designed to enhance faithfulness and interpretability in long-form table question answering by combining a QA-based content planner and execution-based reasoning..

    \item\textbf{Dater}~\citep{Ye2023LargeLM} focuses on selecting relevant information from input tables and providing contextual information to support the statement verification process.

\end{itemize}

\subsection{Implementation Details}
% The training set consists of questions and table candidates filtered based on relevance criteria during the self-verification process.
% Ensure that the temperature of all generator baseline LLMs was set to 0.

\paragraph{Table Input Serialization}
Following recent studies that utilize language models for table-related tasks~\citep{chen2023largelanguagemodelsfew1shot,Seo2024UnveilingIT}, we serialize the table input into a flattened sequence to effectively represent table data for language model processing.
The table title is enclosed within \texttt{[TITLE]} tags, followed by the table headers marked with \texttt{[HEADER]}, where individual column names are separated by a vertical bar (\texttt{|}).
Each row is prefixed with a \texttt{[ROW]} tag and an index, while cell values are separated by a vertical bar.
This approach ensures that the table format is preserved while making the input compatible with language models.
For example, the input table is formatted as follows:

\begingroup
\spaceskip=3pt
\texttt{[TITLE] title [HEADER] col 1 | col2 | ... [ROW 1] cell 1,1 | cell 1,2 | ... [ROW 2]  cell 2,1 | ...}
\endgroup

\paragraph{Training and Inference}
For the table-specific retriever, we fine-tune each model on a single-table QA dataset using the AdamW optimizer with a learning rate of for 5 epochs. To efficiently finetune the model, we adopt LoRA and set the parameters as $r$ = 8, $\alpha$ = 32.  We use a constant learning rate schedule set at 2e-5, and train with the batch size of 1 on 4 NVIDIA A100 GPU. The inference of open-source LLMs are conducted using vLLM framework~\citep{kwon2023efficientmemorymanagementlarge}.  We set 
temperature to 0.0 for efficient and robust output generation.
