\subsection{Task Formulation}
In line with the recent advancements of retrieval-augmented generation (RAG) in diverse domains~\citep{ram-etal-2023-context,shi-etal-2024-replug}, \textsc{MT-RAIG} task consists of two main steps: (1) \textit{table retrieval}, (2) \textit{insight generation}. 
Formally, given a natural language question $q$ and an external datasource with tables $T$, the task is to first retrieve a set of evidence tables $\hat{T_{q}} \subset T$ to approximate the gold table set $T_{q}$ relevant to $q$. Subsequently, an insight $i$ is generated by grounding it on the retrieved $\hat{T_{q}}$. These two steps can be formulated as follows:
\begin{equation}
% \small
    \hat{T_q} = Ret(q, T), \quad i = Gen(q, \hat{T_q})
\end{equation}

\subsection{Benchmark Construction}
% In this section, 
We provide detailed information on the construction of the \bench.
Specifically, we describe the process of creating a machine-annotated dataset, building on efforts from previous works~\citep{tang2024multihoprag,wei2024longform,ni2024mixeval,kim2024evaluatinglanguagemodelssynthetic,yao2024taubenchbenchmarktoolagentuserinteraction} while leveraging several notable advantages over fully human-annotated benchmark. 
These benefits include: \textbf{(1) \textit{scalability}}, where large-scale expansion is possible with significantly reduced human labor, 
% \textbf{(2) \textit{cost-effectiveness}}, which requires significantly lower costs compared to human annotation, 
and \textbf{(2) \textit{consistency}} in labeling standards, a crucial aspect that can be more challenging to maintain when multiple human annotators are involved.
Additionally, we incorporate human quality checks as critical review points for the automatically generated data, balancing the efficiency of automated annotation with the reliability of human annotation to ensure the high-quality of \bench.
We provide all the detailed prompts in Appendix~\ref{apx:annotate_detail} and benchmark examples in Appendix~\ref{apx:case_study}.


\subsubsection{Multi-Table Set Collection}
The goal of this step is to extend the existing single-table corpus into a collection of multi-table sets. 
To group individual tables in the data source into coherent sets that are relevant to real user queries, we classify scenarios where multiple tables are combined into two categories: \textit{joinable} and \textit{topic-related}.
Joinable tables can be directly linked through common key columns, following similar concept to standard text-to-SQL tasks. 
To construct such sets, we use SPIDER~\cite{Yu2018SpiderAL} as a source dataset, leveraging foreign key connections to identify joinable tables from single-table corpus. 

While joinable tables offer a straightforward approach to combining individual tables, real-world table sets often present more complex relationships, where tables cannot be directly joined via a common key column. Instead, these tables are loosely connected by shared topics or contextual relevance.
To effectively group these tables, we leverage table titles and headers as semantic indicators to cluster topically related tables.  
We source these tables from \wikidataset~\cite{Kweon2023OpenWikiTableDF}, a collection of tables extracted from Wikipedia, where rich metadata provides the necessary cues for collecting topically coherent multi-table sets.

\subsubsection{Question Annotation} 
Unlike existing factoid-level questions, our approach aims to annotate  questions that seek comprehensive insights across tables. 
Following previous work~\citep{kim2024evaluatinglanguagemodelssynthetic}, we employ GPT-4o mini as an agent annotator, and generate 10 distinct questions for each table set, designed to capture diverse relational aspects among the tables. 
To strike a robust middle ground between the reliability of human annotation and the versatility of LLMs, we adopt a \textit{human-in-the-loop} process that faithfully guides the question annotation while preserving the flexibility of LLMs. 
Specifically, the question annotation process incorporates three key methods:

\textbf{(1) Decontextualization}:
The goal of decontextualization~\citep{ Choi2021DecontextualizationMS,Kweon2023OpenWikiTableDF} is to enhance the clarity of how each question links back to the relevant tables. 
By explicitly including keywords derived from the table titles into the questions, this step ensures that the semantic alignment between the question and the tables become apparent. 
To achieve this, the agent annotator first extracts key terms from table titles and incorporate them directly into the phrasing of the questions. 

\textbf{(2) Relation-focused context augmentation:}
This step aims to highlight relationships between tables by enriching the questions with contextual information shared across the tables.
We find that naively prompting the agent annotator to generate questions for multiple sources often leads to mere concatenations of information separately extracted from each table, resulting in questions that fail to reflect the connections between the tables.
To address this issue, we first manually identify common attributes and shared data points across multiple tables and let the agent annotator incorporate these overlapping values into the question generation to ensure that critical relational cues between the tables are preserved within the generated questions.

\textbf{(3) Human-guided  demonstration refinement:}
To guide the agent in generating questions that are both diverse and aligned with our annotation objectives, we adopt an iterative workflow that combines agent-based automation with human feedback. 
We begin by categorizing questions into distinct types as shown in Table~\ref{tab:statistics}, and annotate initial human-written questions for each type.
These seed questions serve as initial demonstrations for agent annotator to generate new questions. Then, if human reviews confirm a novel pattern from generated questions, it is incorporated back into the seed set.
Through this iterative human-guided refinement, we build a final demonstration set to help the agent generate well-grounded questions for each type while covering diverse multi-hop relationships.

\input{texTABLE/002statistics}

\subsubsection{Insight Annotation}
\paragraph{Programmatic multi-table facts expansion}
To enhance the agent annotators' understanding of structured data, we augment multi-table set with natural language (NL) facts that provide  additional context for each table set.
These facts are initialized from human-annotated sources in existing datasets (SPIDER, Open-WikiTable), which are accurate but focus narrowly on specific table portions, lacking comprehensive coverage of the full table context. 
To address this, we employ a programmatic fact expansion process to cover a broader range of information within the tables. 
Specifically, given the source tables and initial NL facts, the agent annotator generates a Python function, \texttt{expand\_facts}. 
This function is then executed on the tables to systematically extract enriched facts while ensuring faithfulness to the source table.

\paragraph{Question-relevant knowledge extraction}
Based on the enriched facts obtained in the previous step, the next step involves annotating insights that serve as a comprehensive answer to the given question.  
However, even when providing the agent annotator with enriched NL contexts derived from fact expansion, such contexts might introduce noise and hinder the insight annotation, as not all information helps address each question.
Therefore, we filter out irrelevant content for each question and selectively extract only the relevant facts to provide the agent annotator with a condensed form of knowledge.
This knowledge is then fed to the agent annotators to generate insights for each table set.

\subsubsection{Dual-Stage Quality Control}
For the last step of our benchmark construction, we adopt dual-stage quality assurance process that combines human and agent verification. 
First, following ~\citet{tang2024multihoprag}, we utilize an agent annotator as a self-verifier, ensuring that each multi-table set, question, and insight triple satisfies strict criteria for \textit{relevance}, \textit{faithfulness}, and \textit{completeness}â€”discarding any triple that fails at least one of these standards.
Second, we conduct a human validation, wherein the machine-verified samples are manually reviewed according to the criteria in Table~\ref{tab:validation}, thereby confirming the accuracy and coherence of the agent-based annotations. 
Table~\ref{tab:validation} summarizes this result, showing high degree of agreement among human evaluators, demonstrating the high-quality of our benchmark.
We provide the case study, statistics of self-verification, and detailed process of quality control in Appendix~\ref{apx:verify_detail}.
% \url{https://kwondu.github.io/mt-raig/validate/}

\input{texTABLE/003validation}

