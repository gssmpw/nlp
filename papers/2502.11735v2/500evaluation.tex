Automatic evaluation on the quality of long-form output in table-based tasks remains a long-standing challenge that often does not align well with human evaluations~\citep{zhao-etal-2024-tapera,wang-etal-2024-revisiting,Seo2024UnveilingIT}.
This issue is even more pronounced in the MT-RAIG task, as it is challenging to check (1) the explicit grounding between fact-entangled insight and multiple tables, (2) whether the key steps required to address the multi-hop questions are completely followed in insight.
We posit that these challenges arise as existing automatic evaluation methods (\textit{e.g.,} BLEU~\citep{Papineni2002BleuAM}, TAPAS-Acc~\citep{Liu2022PLOGTP}, and G-Eval~\citep{Liu2023GEvalNE}) often analyze the output in a coarse-grained manner. 
To tackle this challenge, we propose a novel \textbf{decomposition-based evaluation framework} \textbf{\textsc{MT-RAIG Eval}} that enables finer distinctions in assessing the quality of the long-form outputs. 
We focus on evaluating the following two key dimensions:

\input{texFIGURE/003evaluation}
\paragraph{Faithfulness Score} 
Faithfulness score evaluates whether an insight is fully grounded in the provided source tables.
Ideally, a perfect evaluator should assess whether all atomic facts entangled within an insight are correctly grounded in the retrieved tables.
To systematically verify these atomic facts, we introduce \textbf{\textit{table-aware insight decomposition}}, which breaks down an insight into verifiable claims. 
Specifically, we leverage an LLM-based decomposer, enhanced with structural guidance from table schemas, to generate a set of granular claims explicitly linked to their originating tables.
Each claim is then validated by an LLM verifier against the retrieved tables.
This table-aware decomposition enables a fine-grained evaluation of the predicted insights by ensuring traceability and reducing ambiguity of the verifiable claims in multi-table contexts.
Formally, given a set of retrieved tables $\hat{T_q}$, a predicted insight $i$,  a decomposer $\mathcal{D}$, and a verifier $\mathcal{V}$, the final score $S_{Faith.}$ is computed as follows:
\begin{equation}
% \small
    C = \mathcal{D}(\hat{T_q}, i), \ S_{Faith.}= \frac{1}{|C|} \sum_{k=1}^{|C|} \mathcal{V}(c_k, \hat{T_q})
\end{equation}
where $C = \{c_k\}_{k=1}^{n}$ is a set of decomposed claims, and $\mathcal{V}(c_k, \hat{T_q}) \in \{0, 1\}$ verifies each claim against the retrieved tables $\hat{T_q}$.

\paragraph{Completeness Score}
Completeness refers to the idea that a generated insight should fully address all the requirements outlined in the given question.
Independent of faithfulness, this dimension penalizes outputs that omit key analytical steps or introduce redundant content that deviates from the questionâ€™s intent.
To verify that these key steps are properly addressed, we propose a \textbf{\textit{question-aware insight decomposition}}, which deconstructs both the ground truth insight $i$ and the predicted insight $\hat{i}$ into atomic topics representing key steps necessary to resolve the question (\textit{e.g.,} identifying causal relationships or synthesizing cross-table comparisons). 
Specifically, we first employ an LLM-based decomposer $\mathcal{D}$, conditioned on the input question $q$, to generate two sets of atomic topics $A = \mathcal{D}(q, i)$ and $\hat{A} = \mathcal{D}(q, \hat{i})$. 
Subsequently, we perform a semantic matching by using an LLM $\mathcal{M}$, between the atomic topics in $A$ and $\hat{A}$ to compute the precision $P$ and recall $R$ based on the degree of overlap between these sets.   
The final completeness score $S_{Comp.}$ is the F1 score, formulated as follows:
\begin{equation}
\small 
    P = \frac{|\mathcal{M}(A, \hat{A})|}{|\hat{A}|}, \   
    R = \frac{|\mathcal{M}(A, \hat{A})|}{|A|}, \   
    F1 = \frac{2 \cdot P \cdot R}{P + R} 
\end{equation}