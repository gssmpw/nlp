\subsection{Baselines}
% \textbf{Baselines}\quad
We benchmark the performance of diverse baselines on \textsc{MT-RAIG Bench}.
For multi-table retrieval, we consider both \textit{general-purpose} retriever ~\citep{Robertson2009ThePR,Karpukhin2020DensePR,Izacard2021UnsupervisedDI}, and \textit{table-specific} retriever~\citep{Herzig2021OpenDQ,Zhang2023TableLlamaTO} as the baselines. 
For insight generation, we employ 11 LLM-based baselines including \textit{proprietary}~\citep{O3mini,GPT4o,Claude3S}, \textit{open-source}~\citep{DeepSeekAI2025DeepSeekR1IR,Yang2024Qwen2TR,Mesnard2024GemmaOM,llama-3,Jiang2023Mistral7}, and \textit{SOTA-TQA} methods~\citep{Ye2023LargeLM,Wang2024ChainofTableET,zhao-etal-2024-tapera} for comprehensive evaluations.
The results are shown in Table~\ref{tab:retriever} and Table~\ref{tab:generator}, respectively.
Please refer to Appendix~\ref{apx:experiments} for the detailed information on the experimental setup and all the baselines.

\subsection{Results}
\paragraph{Multi-table Retrieval Evaluation} To understand the challenges of multi-table retrieval in \bench, we compare the performance of five widely-used retrievers. 
From the results in Table~\ref{tab:retriever}, we observe that there remains a significant gap between high and low top‑$k$ performance,  which is critical for the generation performance given that LLMs often hold only a limited number of tables in their context window.
We also find that table-specific embedding models do not improve retrieval performance on \bench. Instead, general text-based embedding model achieves better performance, similar to the findings of \citet{wang-etal-2022-table}. 
We attribute this to the nature of the MT-RAIG task, which prioritizes identifying insight-level semantic connection of tables for multi-hop queries rather than understanding the structural characteristics of tables.
Based on these findings, we use DPR’s top-10 retrieved tables (considering the LLM’s context window limitation) for experiments in Table~\ref{tab:meta_main} and Table~\ref{tab:generator}.

\input{texTABLE/004retriever}


\paragraph{Meta Evaluation}
We conduct a meta evaluation to assess the reliability of \textsc{MT-RAIG Eval} against existing automatic metrics.
Specifically, we construct a meta evaluation dataset comprising 250 pairs of responses sampled from baseline generators, where each pair is labeled by two human evaluators based on their relative preferences on response faithfulness and completeness.
For scoring, we follow the setting of \citet{ru2024ragchecker} to normalize each auto-evaluation score difference to the human preference scale of [-1,0,1] and then measure the Pearson correlation with human preference ratings. 
Additionally, we report the correlation between the human evaluators as the upper bound. 
From the results in Table~\ref{tab:meta_main}, we can observe that \eval achieves the highest correlation with human preference ratings across both dimensions, demonstrating its reliability over baseline methods for evaluating \textsc{MT-RAIG Bench}. 
We provide more detailed results in Appendix~\ref{apx:results}. 
\input{texTABLE/005meta}

\input{texTABLE/006generator}
\input{texFIGURE/004baselinesComparison}
\paragraph{Insight Generation Evaluation}
Leveraging the \eval, we evaluate the insight generation performance of various baselines on \bench. 
From the results in Table~\ref{tab:generator} and Figure~\ref{fig:baselines_fig}, we derive the following key conclusions:
\textbf{(1) \bench poses a significant challenge for insight-level table reasoning, even for frontier LLMs}. Both open-source and proprietary models struggle to generate insights for questions in \bench, achieving only around 40\% in faithfulness and 60\% in completeness even when provided with gold tables $T_q$ as input.
\textbf{(2) Deep thinking with scaling test-time compute also shows promise in insight-level table reasoning tasks.}
Although proprietary models generally outperform open-source counterparts, DeepSeek-R1-8b achieves performance on par with proprietary models and even surpasses Claude and o3-mini in completeness score, despite having a smaller number of parameters.
Given that a similar reasoning model, o3-mini, also demonstrates strong performance, these results supports  \citet{testtimecompute, DeepSeekAI2025DeepSeekR1IR}'s recent finding
that inference-time scaling substantially enhances models' capacity for complex reasoning, suggesting its applicability can also extend to insight-level table reasoning tasks. 
\noindent \textbf{(3) SOTA TQA-methods underperform compared to general-purpose LLMs.}
We attribute this to the focus of existing TQA methods on single-table factoid extraction and their
specialization in closed-domain scenarios, which limits their robustness to noise from irrelevant tables. 
\input{texFIGURE/005experimentsPlot}

\subsection{Further Analysis}
\textbf{Trade-offs in retrieving more tables} \quad
We first analyze the relationship between retrieval and generation performance by varying the number of retrieved tables (\textit{k}).
While increasing the number of \textit{k} is an intuitive way to improve generation performance in RAG systems~\citep{kim2024is}, we investigate whether this also holds for the table domain, where retrieved information is structurally more complex than general text.
From the results in Figure~\ref{fig:plot_exp} (Upper), we first observe that the generation performance broadly aligns with conventional intuition, exhibiting an overall improvement across all models as \textit{k} increases.  
However, we also find that a continued increase in \textit{k} does not guarantee sustained performance gains. Beyond a certain threshold, performance plateaus or even declines, suggesting that retrieving an excessive number of tables introduces noise into the generation process.

\paragraph{Factuality of the generated insights are more sensitive to the noisy tables}
Guided by the observations in Figure~\ref{fig:plot_exp} (Upper), we further investigate how sensitively the generator responds to noisy table information. To isolate the impact of noise from retrieval errors, we analyze samples where the entire gold table set is successfully retrieved.
Within these samples, we analyze the performance by varying the ratio of irrelevant to gold tables while keeping the gold tables fixed as part of the model input.
The results in Figure~\ref{fig:plot_exp} (Middle) reveal that, as the proportion of irrelevant tables increases, faithfulness shows a marked decrease while completeness remains relatively stable. 
This suggests that although including the correct tables ensures the generator can somewhat address the high-level key steps (i.e., maintain completeness), the added noise from irrelevant tables hinders the extraction of precise factors for the final insights.

\paragraph{Even without the noisy tables, the model still struggles as the number of tables to reference increases}
While our findings indicate that generation performance degrades with noisy tables, one might wonder how challenging it becomes for the model as the number of source tables it should handle increases, even in noise-free conditions.
To explore this, we conduct a closed-domain experiment in which only the gold tables are provided as input, and investigate how generation performance varies depending on the number of tables in the gold table set that the model should reference.
From the results in Figure~\ref{fig:plot_exp} (Lower), we observe a significant drop in both faithfulness and completeness as the number of gold tables to reference grows, showing that current models struggle to conduct complex reasoning across multiple sources of tables.