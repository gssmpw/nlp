\subsection{Reproducibility of \eval Backbone}
To assess whether \eval produces consistent results across different backbone LLMs, we analyze two open-source models (DeepSeek-R1-8B and Llama-3.1-8B) against the GPT-4o-mini, which is the original backbone of \eval. Specifically, we measure how closely the evaluation scores from each open-source model align with those from GPT-4o-mini by calculating pairwise correlation.
From the results in Table~\ref{tab:apx_evalbaseline}, we can observe that even when replacing the backbone with open-source models, the high correlation persists, demonstrating that our evaluation method is both reproducible and robust.

\input{texTABLE/041ablationEvalBackbone}

\subsection{Effect of Parameter Scaling on \bench Performance}
From the results in Table~\ref{tab:generator}, we observe that proprietary models with larger model sizes generally show higher performance compared to open-source models that have relatively fewer parameters. 
To further investigate this, we conduct additional experiments by differ the parameter sizes of open-source LLM to understand the effect of model size in the table reasoning performance.
Specifically, we evaluate DeepSeek-R1, which is most powerful open-source LLM among our baselines ranging from 8B to 70B parameters.
From the results in Table~\ref{tab:apx_deepseek}, we confirm a general trend where increasing model size correlates with improved performance, with a particularly notable improvement in the faithfulness score.
This discrepancy suggests that the modelâ€™s capacity to accurately interpret and reason about table structures (which is a prerequisite for faithfulness) is more directly enhanced by parameter scaling, compared to the broader task coverage implied by completeness.
\input{texTABLE/042ablationParameterSize}