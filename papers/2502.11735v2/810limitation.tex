Although we believe that our \bench and \eval could serve as valuable resources for the research community, several limitations remain, suggesting areas for future improvement.
First, while the synthetic generation of \bench enables scalable and consistent dataset construction~\citep{tan2024largelanguagemodelsdata, hao2024syntheticdataaichallenges,han-etal-2024-rag} as we discussed in Section~\ref{sec:benchmark}, this approach carries inherent risks~\citep{10.1145/3442188.3445922}, such as reduced linguistic diversity in questions or potential overalignment with model generated responses that are unnatural or unfaithful description. 
Despite this downside, we minimize these risks by adopting a human-in-the-loop annotation process, where human experts iteratively refine machine-generated questions. Further, we conduct a machine-human dual quality control, in which automated filters first strictly discard low-quality instances, followed by human validation to ensure the quality of the final benchmark.

Second, although \bench covers both relational DB tables and Wikipedia tables that constitute the two main portion of table domain, it could be beneficial to add more data sources from diverse domains (e.g., financial, scientific, or medical tables).
Future works could examine how existing models perform in these specialized domains to assess their robustness to domain-specific terminology or extremely large table sets.

Lastly, a notable limitation lies in our \textsc{MT-RAIG Eval}’s reliance on LLM for assessment.
While recent works increasingly adopt LLM-based evaluators in diverse domains due to their scalability and flexibility~\citep{ye2024flask,kim2024stopplayingguessinggame, ru2024ragchecker, han-etal-2024-rag, wang-etal-2024-revisiting}, it is important to note that their judgments may still be affected by biases or inconsistencies inherent in the backbone model’s training data or architectural design.
% However, we believe that this issue has been largely mitigated in our work by conducting a meta evaluation in Section~\ref{sec:evaluation} to demonstrate the reliability of our evaluation method in aligning with human judgements, and additional experiments on varying the LLM-backbone of \eval to open-source models (Table~\ref{tab:apx_meta}), demonstrating the reproducibility of our evaluation framework.
However, we believe this issue has been largely minimized in our study. Through a meta-evaluation in Section~\ref{sec:evaluation}, we demonstrate our \textsc{MT-RAIG Eval}’s reliability in aligning with human judgments. Furthermore, additional experiments that replacing the \textsc{MT-RAIG Eval}'s LLM backbone with open-source models (Table~\ref{tab:apx_meta}) confirm the reproducibility of our evaluation framework.