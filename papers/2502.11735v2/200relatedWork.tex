\paragraph{Benchmarks for Table-based Tasks}
Benchmarks for table-based reasoning has been studied across several major tasks, including text-to-SQL~\citep{zhong2018seqsql}, table-to-text~\citep{Parikh2020ToTToAC}, and tableQA~\citep{pasupat-liang-2015-compositional}.
Early works~\citep{Yu2018SpiderAL,Nan2021FeTaQAFT} mainly focus on measuring the performance of neural models as executors for extracting factoid-level information, where models generate SQL queries or direct answers from tables.
More recently, some works~\citep{Zhao2023QTSummAN,Seo2024UnveilingIT} target insight-level table reasoning tasks, moving beyond extracting explicit information from tables.
% In this paper, we propose \bench, a large-scale benchmark for evaluating the insight-level table reasoning ability of the model.
Unlike prior works, our \bench significantly differs by exploring more realistic scenario where the system retrieves multiple evidence tables to generate insightful responses.
\paragraph{Automated Evaluation of Long-form Outputs in Table-based Tasks}
Existing methods have primarily compute the lexical overlap ~\citep{Post2018ACF}, or  semantic similarity with the reference~\citep{Zhang2019BERTScoreET}, while some~\citep{Liu2022PLOGTP} focus on factuality assessment using a trained verifier to check the grounding against the source table.
Some recent studies~\citep{zhao-etal-2024-tapera,wang-etal-2024-revisiting} find that LLM-based evaluation metrics (\textit{i.e.,} G-Eval), show stronger alignment with human evaluation.
Despite these advancements, these metrics still struggle to capture the fine-grained quality of long-form outputs. To address this gap, we propose a novel decomposition-based evaluation framework, \eval, which outperforms existing metrics in aligning with human judgments.
