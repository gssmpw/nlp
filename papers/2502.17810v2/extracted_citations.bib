@article{ao2024sd,
  title={{SD-Eval}: A benchmark dataset for spoken dialogue understanding beyond words},
  author={Ao, Junyi and Wang, Yuancheng and Tian, Xiaohai and Chen, Dekun and Zhang, Jun and Lu, Lu and Wang, Yuxuan and Li, Haizhou and Wu, Zhizheng},
  journal={arXiv preprint arXiv:2406.13340},
  year={2024}
}

@article{chen2024slam,
  title={{SLAM-Omni}: Timbre-Controllable Voice Interaction System with Single-Stage Training},
  author={Chen, Wenxi and Ma, Ziyang and Yan, Ruiqi and Liang, Yuzhe and Li, Xiquan and Xu, Ruiyang and Niu, Zhikang and Zhu, Yanqiao and Yang, Yifan and Liu, Zhanxun and others},
  journal={arXiv preprint arXiv:2412.15649},
  year={2024}
}

@article{chen2024voicebench,
  title={{VoiceBench: Benchmarking LLM-based voice assistants}},
  author={Chen, Yiming and Yue, Xianghu and Zhang, Chen and Gao, Xiaoxue and Tan, Robby T and Li, Haizhou},
  journal={arXiv preprint arXiv:2410.17196},
  year={2024}
}

@article{chu2024qwen2-audio,
  title={{Qwen2-audio technical report}},
  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2407.10759},
  year={2024}
}

@article{fang2024llama,
  title={{Llama-Omni: Seamless speech interaction with large language models}},
  author={Fang, Qingkai and Guo, Shoutao and Zhou, Yan and Ma, Zhengrui and Zhang, Shaolei and Feng, Yang},
  journal={arXiv preprint arXiv:2409.06666},
  year={2024}
}

@article{gao2024adu,
  title={Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models},
  author={Gao, Kuofeng and Xia, Shu-Tao and Xu, Ke and Torr, Philip and Gu, Jindong},
  journal={arXiv preprint arXiv:2412.05167},
  year={2024}
}

@article{hu2024wavllm,
  title={{WavLLM: Towards robust and adaptive speech large language model}},
  author={Hu, Shujie and Zhou, Long and Liu, Shujie and Chen, Sanyuan and Meng, Lingwei and Hao, Hongkun and Pan, Jing and Liu, Xunying and Li, Jinyu and Sivasankaran, Sunit and others},
  journal={arXiv preprint arXiv:2404.00656},
  year={2024}
}

@article{tang2023salmonn,
  title={{SALMONN: Towards generic hearing abilities for large language models}},
  author={Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao},
  journal={arXiv preprint arXiv:2310.13289},
  year={2023}
}

@article{wang2024freeze,
  title={{Freeze-Omni:} A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen {LLM}},
  author={Wang, Xiong and Li, Yangze and Fu, Chaoyou and Xie, Lei and Li, Ke and Sun, Xing and Ma, Long},
  journal={arXiv preprint arXiv:2411.00774},
  year={2024}
}

@article{xie2024mini,
  title={{Mini-Omni: Language models can hear, talk while thinking in streaming}},
  author={Xie, Zhifei and Wu, Changqiao},
  journal={arXiv preprint arXiv:2408.16725},
  year={2024}
}

@article{xie2024mini2,
  title={{Mini-Omni2: Towards open-source GPT-4o with vision, speech and duplex capabilities}},
  author={Xie, Zhifei and Wu, Changqiao},
  journal={arXiv preprint arXiv:2410.11190},
  year={2024}
}

@article{yang2024air,
  title={{AIR-Bench: Benchmarking large audio-language models via generative comprehension}},
  author={Yang, Qian and Xu, Jin and Liu, Wenrui and Chu, Yunfei and Jiang, Ziyue and Zhou, Xiaohuan and Leng, Yichong and Lv, Yuanjun and Zhao, Zhou and Zhou, Chang and others},
  journal={arXiv preprint arXiv:2402.07729},
  year={2024}
}

@article{zeng2024glm,
  title={{GLM-4-Voice: Towards intelligent and human-like end-to-end spoken chatbot}},
  author={Zeng, Aohan and Du, Zhengxiao and Liu, Mingdao and Wang, Kedong and Jiang, Shengmin and Zhao, Lei and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2412.02612},
  year={2024}
}

