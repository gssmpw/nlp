[
  {
    "index": 0,
    "papers": [
      {
        "key": "chu2024qwen2-audio",
        "author": "Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others",
        "title": "{Qwen2-audio technical report}"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "tang2023salmonn",
        "author": "Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao",
        "title": "{SALMONN: Towards generic hearing abilities for large language models}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hu2024wavllm",
        "author": "Hu, Shujie and Zhou, Long and Liu, Shujie and Chen, Sanyuan and Meng, Lingwei and Hao, Hongkun and Pan, Jing and Liu, Xunying and Li, Jinyu and Sivasankaran, Sunit and others",
        "title": "{WavLLM: Towards robust and adaptive speech large language model}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "xie2024mini",
        "author": "Xie, Zhifei and Wu, Changqiao",
        "title": "{Mini-Omni: Language models can hear, talk while thinking in streaming}"
      },
      {
        "key": "xie2024mini2",
        "author": "Xie, Zhifei and Wu, Changqiao",
        "title": "{Mini-Omni2: Towards open-source GPT-4o with vision, speech and duplex capabilities}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "fang2024llama",
        "author": "Fang, Qingkai and Guo, Shoutao and Zhou, Yan and Ma, Zhengrui and Zhang, Shaolei and Feng, Yang",
        "title": "{Llama-Omni: Seamless speech interaction with large language models}"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "chen2024slam",
        "author": "Chen, Wenxi and Ma, Ziyang and Yan, Ruiqi and Liang, Yuzhe and Li, Xiquan and Xu, Ruiyang and Niu, Zhikang and Zhu, Yanqiao and Yang, Yifan and Liu, Zhanxun and others",
        "title": "{SLAM-Omni}: Timbre-Controllable Voice Interaction System with Single-Stage Training"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wang2024freeze",
        "author": "Wang, Xiong and Li, Yangze and Fu, Chaoyou and Xie, Lei and Li, Ke and Sun, Xing and Ma, Long",
        "title": "{Freeze-Omni:} A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen {LLM}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zeng2024glm",
        "author": "Zeng, Aohan and Du, Zhengxiao and Liu, Mingdao and Wang, Kedong and Jiang, Shengmin and Zhao, Lei and Dong, Yuxiao and Tang, Jie",
        "title": "{GLM-4-Voice: Towards intelligent and human-like end-to-end spoken chatbot}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "yang2024air",
        "author": "Yang, Qian and Xu, Jin and Liu, Wenrui and Chu, Yunfei and Jiang, Ziyue and Zhou, Xiaohuan and Leng, Yichong and Lv, Yuanjun and Zhao, Zhou and Zhou, Chang and others",
        "title": "{AIR-Bench: Benchmarking large audio-language models via generative comprehension}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ao2024sd",
        "author": "Ao, Junyi and Wang, Yuancheng and Tian, Xiaohai and Chen, Dekun and Zhang, Jun and Lu, Lu and Wang, Yuxuan and Li, Haizhou and Wu, Zhizheng",
        "title": "{SD-Eval}: A benchmark dataset for spoken dialogue understanding beyond words"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "chen2024voicebench",
        "author": "Chen, Yiming and Yue, Xianghu and Zhang, Chen and Gao, Xiaoxue and Tan, Robby T and Li, Haizhou",
        "title": "{VoiceBench: Benchmarking LLM-based voice assistants}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "gao2024adu",
        "author": "Gao, Kuofeng and Xia, Shu-Tao and Xu, Ke and Torr, Philip and Gu, Jindong",
        "title": "Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models"
      }
    ]
  }
]