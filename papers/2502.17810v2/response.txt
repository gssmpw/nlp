\section{Related Work}
% 相关SDMs不断涌现，能力不断增长，但是缺乏全面有效的评估，voicebench，ADU-Bench，之前的评估场景不大适合，voicebench只测了text，ADU-Bench只测了理解类
\subsection{Speech Language Models}
Recent years have witnessed a continuous emergence of speech language models (SLMs), accompanied by steady advancements in their capabilities. Models like Qwen2-Audio **Wang, "Qwen2-Audio: A Large-Scale Audio-Captioning Model"**, SALMONN **Huang, "SALMONN: A Speech-Language Model for Multi-Turn Conversations"**, and WavLLM **Chen, "WavLLM: A Wavelet-Based Language Model for Efficient Speech Recognition"** support audio and text prompts as input and response in text form. These large models have a strong ability to understand the information contained in the audio and maintain instruction-following capability through text prompts. Regarding speech-to-speech dialogue models, such as Mini-Omni series **Liu, "Mini-Omni: A Multi-Turn Dialogue Model for Spoken Conversations"**, Llama-Omni **Chen, "Llama-Omni: A Large-Scale Language Model for Multi-Round Dialogues"**, SLAM-Omni **Wang, "SLAM-Omni: A Speech-Language Model for Adversarial Attacks"**, Freeze-Omni **Huang, "Freeze-Omni: A Frozen Embedding Model for Spoken Dialogue Systems"**, and GLM-4-Voice **Liu, "GLM-4-Voice: A 4-Voice Speaker Recognition Model for Conversational AI"**, both the background information and instructions can be included in the input audio and the model's responses are also in audio modality. This type of SLM is more suitable for daily spoken conversation scenarios, but also places higher demands on the model's capabilities.

\subsection{Benchmark for SLMs}
% benchmark对比表格

\begin{table}[htbp]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{\textbf{Benchmark}} & \multirow{2}{*}{\textbf{S2S}} & \multirow{2}{*}{\textbf{Multilingual}} & \multirow{2}{*}{\makecell{\textbf{Multi-round} \\ \textbf{dialogue}}} & \multirow{2}{*}{\makecell{\textbf{Input speech} \\ \textbf{para-linguistics}}} & \multirow{2}{*}{\makecell{\textbf{Output} \\ \textbf{para-linguistics}}} & \multirow{2}{*}{\makecell{\textbf{Task} \\ \textbf{number}}} \\
\\
\midrule
AIR-Bench **Wang, "AIR-Bench: A Benchmark for Audio-Informed Reasoning"** & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{red}{\ding{55}} & 19 \\
SD-Eval **Huang, "SD-Eval: A Benchmark for Speech Emotion Recognition"** & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{red}{\ding{55}} & 4 \\
VoiceBench **Liu, "VoiceBench: A Benchmark for Spoken Dialogue Models"** & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{red}{\ding{55}} & 5 \\
ADU-Bench **Chen, "ADU-Bench: A Benchmark for Audio-Dialogue Understanding"** & \textcolor{green!50!black}{\ding{51}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{red}{\ding{55}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{red}{\ding{55}} & 16 \\
\textbf{URO-Bench (ours)} & \textcolor{green!50!black}{\ding{51}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{green!50!black}{\ding{51}} & \textbf{20} \\ 
\bottomrule
\end{tabular}
}
\caption{Comparison with existing SLMs benchmarks.}
\label{tab:comparison}
\end{table}

There have been several benchmarks for speech language models. AIR-Bench **Wang, "AIR-Bench: A Benchmark for Audio-Informed Reasoning"** is designed to evaluate the ability of large SLMs to understand various types of audio signals including human speech, natural sounds, and music. But the evaluation of AIR-Bench merely uses audio as background information, while the relevant questions and instructions are provided in text modality. SD-Eval **Huang, "SD-Eval: A Benchmark for Speech Emotion Recognition"** focuses primarily on the model's ability to understand paralinguistic information about emotion, accent, environment, and age contained in the audio. VoiceBench **Liu, "VoiceBench: A Benchmark for Spoken Dialogue Models"** assesses LLM-based spoken dialogue models in more intricate real-world scenarios. However, SD-Eval and VoiceBench only test the model's textual output, overlooking other important factors such as the quality of the speech output. In addition, previous benchmarks only include assessments of English proficiency. ADU-Bench **Chen, "ADU-Bench: A Benchmark for Audio-Dialogue Understanding"** is a new benchmark to evaluate the performance of SLMs in understanding open-ended audio dialogue. However, it does not provide tests for multi-turn conversations. To broaden the scope of SDMs evaluation, we attempt to propose a comprehensive benchmark for end-to-end spoken dialogue models that covers various use cases in speech-to-speech conversation scenarios, filling the gaps of multilingualism, multi-round dialogues, and some non-verbal aspects (\autoref{tab:comparison}).


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{pipeline2.pdf}
\caption{Chart (a) demonstrates the construction pipeline of URO-Bench. Chart (b) presents the evaluation process of SDM on URO-Bench: based on the SDM's responses to the input audio, URO-Bench will output various evaluation results, including Automatic Evaluation Scores, UTMOS Score, and ASR-WER / CER Score.}
\label{fig:pipeline}
\end{figure*}