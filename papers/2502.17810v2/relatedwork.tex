\section{Related Work}
% 相关SDMs不断涌现，能力不断增长，但是缺乏全面有效的评估，voicebench，ADU-Bench，之前的评估场景不大适合，voicebench只测了text，ADU-Bench只测了理解类
\subsection{Speech Language Models}
Recent years have witnessed a continuous emergence of speech language models (SLMs), accompanied by steady advancements in their capabilities. Models like Qwen2-Audio \citep{chu2024qwen2-audio}, SALMONN \citep{tang2023salmonn}, and WavLLM \citep{hu2024wavllm} support audio and text prompts as input and response in text form. These large models have a strong ability to understand the information contained in the audio and maintain instruction-following capability through text prompts. Regarding speech-to-speech dialogue models, such as Mini-Omni series \citep{xie2024mini, xie2024mini2}, Llama-Omni \citep{fang2024llama}, SLAM-Omni \citep{chen2024slam}, Freeze-Omni \citep{wang2024freeze}, and GLM-4-Voice \citep{zeng2024glm}, both the background information and instructions can be included in the input audio and the model's responses are also in audio modality. This type of SLM is more suitable for daily spoken conversation scenarios, but also places higher demands on the model's capabilities.

\subsection{Benchmark for SLMs}
% benchmark对比表格

\begin{table}[htbp]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{\textbf{Benchmark}} & \multirow{2}{*}{\textbf{S2S}} & \multirow{2}{*}{\textbf{Multilingual}} & \multirow{2}{*}{\makecell{\textbf{Multi-round} \\ \textbf{dialogue}}} & \multirow{2}{*}{\makecell{\textbf{Input speech} \\ \textbf{para-linguistics}}} & \multirow{2}{*}{\makecell{\textbf{Output} \\ \textbf{para-linguistics}}} & \multirow{2}{*}{\makecell{\textbf{Task} \\ \textbf{number}}} \\
\\
\midrule
AIR-Bench & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{red}{\ding{55}} & 19 \\
SD-Eval & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{red}{\ding{55}} & 4 \\
VoiceBench & \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{red}{\ding{55}} & 5 \\
ADU-Bench & \textcolor{green!50!black}{\ding{51}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{red}{\ding{55}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{red}{\ding{55}} & 16 \\
\textbf{URO-Bench (ours)} & \textcolor{green!50!black}{\ding{51}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{green!50!black}{\ding{51}} & \textcolor{green!50!black}{\ding{51}} & \textbf{20} \\ 
\bottomrule
\end{tabular}
}
\caption{Comparison with existing SLMs benchmarks.}
\label{tab:comparison}
\end{table}

There have been several benchmarks for speech language models. AIR-Bench \citep{yang2024air} is designed to evaluate the ability of large SLMs to understand various types of audio signals including human speech, natural sounds, and music. But the evaluation of AIR-Bench merely uses audio as background information, while the relevant questions and instructions are provided in text modality. SD-Eval \citep{ao2024sd} focuses primarily on the model's ability to understand paralinguistic information about emotion, accent, environment, and age contained in the audio. VoiceBench \citep{chen2024voicebench} assesses LLM-based spoken dialogue models in more intricate real-world scenarios. However, SD-Eval and VoiceBench only test the model's textual output, overlooking other important factors such as the quality of the speech output. In addition, previous benchmarks only include assessments of English proficiency. ADU-Bench \citep{gao2024adu} is a new benchmark to evaluate the performance of SLMs in understanding open-ended audio dialogue. However, it does not provide tests for multi-turn conversations. To broaden the scope of SDMs evaluation, we attempt to propose a comprehensive benchmark for end-to-end spoken dialogue models that covers various use cases in speech-to-speech conversation scenarios, filling the gaps of multilingualism, multi-round dialogues, and some non-verbal aspects (\autoref{tab:comparison}).


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{pipeline2.pdf}
\caption{Chart (a) demonstrates the construction pipeline of URO-Bench. Chart (b) presents the evaluation process of SDM on URO-Bench: based on the SDM's responses to the input audio, URO-Bench will output various evaluation results, including Automatic Evaluation Scores, UTMOS Score, and ASR-WER / CER Score.}
\label{fig:pipeline}
\end{figure*}