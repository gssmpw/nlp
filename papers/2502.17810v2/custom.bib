@article{chen2024voicebench,
  title={{VoiceBench: Benchmarking LLM-based voice assistants}},
  author={Chen, Yiming and Yue, Xianghu and Zhang, Chen and Gao, Xiaoxue and Tan, Robby T and Li, Haizhou},
  journal={arXiv preprint arXiv:2410.17196},
  year={2024}
}

@article{saeki2022utmos,
  title={{UTMOS: Utokyo-Sarulab system for voiceMOS challenge 2022}},
  author={Saeki, Takaaki and Xin, Detai and Nakata, Wataru and Koriyama, Tomoki and Takamichi, Shinnosuke and Saruwatari, Hiroshi},
  journal={arXiv preprint arXiv:2204.02152},
  year={2022}
}

@inproceedings{radford2023robust,
  title={{Robust speech recognition via large-scale weak supervision}},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}

@article{guan2022corpus,
  title={{A corpus for understanding and generating moral stories}},
  author={Guan, Jian and Liu, Ziqi and Huang, Minlie},
  journal={arXiv preprint arXiv:2204.09438},
  year={2022}
}

@article{lin2021truthfulqa,
  title={{TruthfulQA: Measuring how models mimic human falsehoods}},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@article{zhao2024wildchat,
  title={{WildChat: 1M chatGPT interaction logs in the wild}},
  author={Zhao, Wenting and Ren, Xiang and Hessel, Jack and Cardie, Claire and Choi, Yejin and Deng, Yuntian},
  journal={arXiv preprint arXiv:2405.01470},
  year={2024}
}

@article{xie2024mini,
  title={{Mini-Omni: Language models can hear, talk while thinking in streaming}},
  author={Xie, Zhifei and Wu, Changqiao},
  journal={arXiv preprint arXiv:2408.16725},
  year={2024}
}

@article{xie2024mini2,
  title={{Mini-Omni2: Towards open-source GPT-4o with vision, speech and duplex capabilities}},
  author={Xie, Zhifei and Wu, Changqiao},
  journal={arXiv preprint arXiv:2410.11190},
  year={2024}
}

@article{fang2024llama,
  title={{Llama-Omni: Seamless speech interaction with large language models}},
  author={Fang, Qingkai and Guo, Shoutao and Zhou, Yan and Ma, Zhengrui and Zhang, Shaolei and Feng, Yang},
  journal={arXiv preprint arXiv:2409.06666},
  year={2024}
}

@article{zeng2024glm,
  title={{GLM-4-Voice: Towards intelligent and human-like end-to-end spoken chatbot}},
  author={Zeng, Aohan and Du, Zhengxiao and Liu, Mingdao and Wang, Kedong and Jiang, Shengmin and Zhao, Lei and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2412.02612},
  year={2024}
}

@article{sakshi2024mmau,
  title={{Mmau: A massive multi-task audio understanding and reasoning benchmark}},
  author={Sakshi, S and Tyagi, Utkarsh and Kumar, Sonal and Seth, Ashish and Selvakumar, Ramaneswaran and Nieto, Oriol and Duraiswami, Ramani and Ghosh, Sreyan and Manocha, Dinesh},
  journal={arXiv preprint arXiv:2410.19168},
  year={2024}
}

@article{hu2015lcsts,
  title={{LCSTS: A large scale Chinese short text summarization dataset}},
  author={Hu, Baotian and Chen, Qingcai and Zhu, Fangze},
  journal={arXiv preprint arXiv:1506.05865},
  year={2015}
}

@article{mihaylov2018can,
  title={{Can a suit of armor conduct electricity? a new dataset for open book question answering}},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@article{bai2024mt,
  title={{MT-Bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues}},
  author={Bai, Ge and Liu, Jie and Bu, Xingyuan and He, Yancheng and Liu, Jiaheng and Zhou, Zhanhui and Lin, Zhuoran and Su, Wenbo and Ge, Tiezheng and Zheng, Bo and others},
  journal={arXiv preprint arXiv:2402.14762},
  year={2024}
}

@article{gao2022paraformer,
  title={{Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition}},
  author={Gao, Zhifu and Zhang, Shiliang and McLoughlin, Ian and Yan, Zhijie},
  journal={arXiv preprint arXiv:2206.08317},
  year={2022}
}

@misc{li2023alpacaeval,
  title={{AlpacaEval: An automatic evaluator of instruction-following models}},
  author={Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}

@article{wang2024freeze,
  title={{Freeze-Omni:} A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen {LLM}},
  author={Wang, Xiong and Li, Yangze and Fu, Chaoyou and Xie, Lei and Li, Ke and Sun, Xing and Ma, Long},
  journal={arXiv preprint arXiv:2411.00774},
  year={2024}
}

@article{yang2024qwen2,
  title={{Qwen2 technical report}},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{ardila2019common,
  title={{Common voice: A massively-multilingual speech corpus}},
  author={Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M and Weber, Gregor},
  journal={arXiv preprint arXiv:1912.06670},
  year={2019}
}

@article{chen2024slam,
  title={{SLAM-Omni}: Timbre-Controllable Voice Interaction System with Single-Stage Training},
  author={Chen, Wenxi and Ma, Ziyang and Yan, Ruiqi and Liang, Yuzhe and Li, Xiquan and Xu, Ruiyang and Niu, Zhikang and Zhu, Yanqiao and Yang, Yifan and Liu, Zhanxun and others},
  journal={arXiv preprint arXiv:2412.15649},
  year={2024}
}

@misc{openai2024gpt4omini,
  title={{GPT-4o mini: advancing cost-efficient intelligence}},
  author={{OpenAI}},
  year={2024},
  url={https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}
}

@misc{openai2024gpt4o,
  title={{Hello GPT-4o}},
  author={{OpenAI}},
  year={2024},
  url={https://openai.com/index/hello-gpt-4o/}
}

@misc{zijie2025doubao,
  title={{Realtime Voice - Doubao Team}},
  author={{Doubao Team}},
  year={2025},
  url={https://team.doubao.com/zh/special/realtime_voice}
}

@article{chu2024qwen2-audio,
  title={{Qwen2-audio technical report}},
  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2407.10759},
  year={2024}
}

@article{tang2023salmonn,
  title={{SALMONN: Towards generic hearing abilities for large language models}},
  author={Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao},
  journal={arXiv preprint arXiv:2310.13289},
  year={2023}
}

@article{hu2024wavllm,
  title={{WavLLM: Towards robust and adaptive speech large language model}},
  author={Hu, Shujie and Zhou, Long and Liu, Shujie and Chen, Sanyuan and Meng, Lingwei and Hao, Hongkun and Pan, Jing and Liu, Xunying and Li, Jinyu and Sivasankaran, Sunit and others},
  journal={arXiv preprint arXiv:2404.00656},
  year={2024}
}

@article{ao2024sd,
  title={{SD-Eval}: A benchmark dataset for spoken dialogue understanding beyond words},
  author={Ao, Junyi and Wang, Yuancheng and Tian, Xiaohai and Chen, Dekun and Zhang, Jun and Lu, Lu and Wang, Yuxuan and Li, Haizhou and Wu, Zhizheng},
  journal={arXiv preprint arXiv:2406.13340},
  year={2024}
}

@article{gao2024adu,
  title={Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models},
  author={Gao, Kuofeng and Xia, Shu-Tao and Xu, Ke and Torr, Philip and Gu, Jindong},
  journal={arXiv preprint arXiv:2412.05167},
  year={2024}
}

@article{yang2024air,
  title={{AIR-Bench: Benchmarking large audio-language models via generative comprehension}},
  author={Yang, Qian and Xu, Jin and Liu, Wenrui and Chu, Yunfei and Jiang, Ziyue and Zhou, Xiaohuan and Leng, Yichong and Lv, Yuanjun and Zhao, Zhou and Zhou, Chang and others},
  journal={arXiv preprint arXiv:2402.07729},
  year={2024}
}

@article{glm2024chatglm,
  title={{ChatGLM: A family of large language models from GLM-130b to GLM-4 all tools}},
  author={GLM, Team and Zeng, Aohan and Xu, Bin and Wang, Bowen and Zhang, Chenhui and Yin, Da and Zhang, Dan and Rojas, Diego and Feng, Guanyu and Zhao, Hanlin and others},
  journal={arXiv preprint arXiv:2406.12793},
  year={2024}
}

@article{dubey2024llama3,
  title={{The Llama 3 herd of models}},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{chen2024f5,
  title={{F5-TTS: A fairytaler that fakes fluent and faithful speech with flow matching}},
  author={Chen, Yushen and Niu, Zhikang and Ma, Ziyang and Deng, Keqi and Wang, Chunhui and Zhao, Jian and Yu, Kai and Chen, Xie},
  journal={arXiv preprint arXiv:2410.06885},
  year={2024}
}

@article{du2024cosyvoice,
  title={{CosyVoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens}},
  author={Du, Zhihao and Chen, Qian and Zhang, Shiliang and Hu, Kai and Lu, Heng and Yang, Yexin and Hu, Hangrui and Zheng, Siqi and Gu, Yue and Ma, Ziyang and others},
  journal={arXiv preprint arXiv:2407.05407},
  year={2024}
}

@inproceedings{lipping2022clotho,
  title={{Clotho-aqa: A crowdsourced dataset for audio question answering}},
  author={Lipping, Samuel and Sudarsanam, Parthasaarathy and Drossos, Konstantinos and Virtanen, Tuomas},
  booktitle={2022 30th European Signal Processing Conference (EUSIPCO)},
  pages={1140--1144},
  year={2022},
  organization={IEEE}
}

@article{weck2024muchomusic,
  title={{MuChoMusic: Evaluating music understanding in multimodal audio-language models}},
  author={Weck, Benno and Manco, Ilaria and Benetos, Emmanouil and Quinton, Elio and Fazekas, George and Bogdanov, Dmitry},
  journal={arXiv preprint arXiv:2408.01337},
  year={2024}
}

@article{ma2023emotion2vec,
  title={emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation},
  author={Ma, Ziyang and Zheng, Zhisheng and Ye, Jiaxin and Li, Jinchao and Gao, Zhifu and Zhang, Shiliang and Chen, Xie},
  journal={Proc. ACL 2024 Findings},
  year={2024}
}

@article{livingstone2018ryerson,
  title={{The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in north American english}},
  author={Livingstone, Steven R and Russo, Frank A},
  journal={PloS one},
  volume={13},
  number={5},
  pages={e0196391},
  year={2018},
  publisher={Public Library of Science San Francisco, CA USA}
}