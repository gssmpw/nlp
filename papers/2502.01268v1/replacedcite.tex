\section{Related Work}
Many works in the literature have recently adopted RL and deep RL in the wireless domain, specifically in UAV networks. Among these, the authors in____ optimize the UAV optimal path to maximize the data collected from IoT devices. In____, the authors propose a DQN algorithm that can jointly minimize AoI and transmission power of limited-power devices. The authors in____ compare centralized and decentralized deep RL techniques using soft actor-critic (SAC) for trajectory planning in integrated sensing and communications UAV network, where the work in____ formulates a multi-objective deep RL algorithm for trajectory planning and beamforming design.

Apart from RL and deep RL, meta-learning has been fundamental in ensuring scalability in recent wireless applications. Many works have leveraged meta-learning techniques in the wireless domain. For example, the work in____ was among the first to investigate meta-learning approaches for wireless communication. It proposes a MAML algorithm for fast training an autoencoder designed for transmitting and receiving data over fading channels. The authors in____ design fast downlink beamformers using transfer learning and meta-learning. The authors in____ exploit meta-learning techniques with graph neural networks (GNNs) for fast wireless indoor localization. In____, deep RL is combined with meta-learning to enhance the fast adaptability of the optimizing the allocation policy a dynamic V2X network, where____ proposes a multi-agent meta-RL algorithm for trajectories design of multiple UAVs. The authors in____ jointly minimize IoT devices' AoI and transmission power using a meta-RL algorithm that adapts quickly to environments with adaptive objectives.


Although most existing RL-related works rely on online RL, offline RL has begun to get further attention in the wireless domain. The work in____ was the first to introduce offline to the wireless domain. The authors formulate a distributional and offline multi-agent RL algorithm for planning the trajectories of multiple UAVs. The authors in____ evaluate various offline RL techniques with a mixture of datasets collected from different behavioral policies for the RRM problem. In contrast, the authors in____ combine distributional RL with offline RL for the RRM problem, where the work in____ solves the RRM problem using multi-agent offline RL to minimize the combination of sum and tail rates jointly.