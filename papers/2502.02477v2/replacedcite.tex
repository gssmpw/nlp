\section{Related Work.}
\label{sec:related_works}
The problem of representing a graph in a compressed form has been extensively explored,
with techniques typically falling into two categories: \emph{lossless} and \emph{lossy compression}____. The lossless compression retains the entirety of graph information, while the lossy compression sacrifices some details during the compression process. As one of the key requirements of our approach is to preserve the path information, it falls into the lossless compression techniques category. The majority of the proposed lossless graph compression techniques focus on specific domains such as web graphs, social networks, and chemistry networks____. Those techniques primarily focus on reducing the storage space____ and/or efficiently addressing specific graph queries, such as graph pattern matching, community query, etc.____. Buehrer and Chellapilla____ achieved high compression ratios and small depths by using bipartite cliques with frequent itemset mining for compressing web graphs. However, while these techniques achieve high compression ratios, the resulting compressed graphs may not be directly usable as input to algorithms like all-pairs shortest paths, often requiring significant modifications or reconstruction overhead. Similarly, Fan et al.____ achieved significant compression of large graphs by converting stars, cliques, and paths into super-nodes, achieving speedups in query tasks like shortest distance computations. While this technique is compatible with algorithms like matching, obtaining the path information for all nodes may introduce high overhead, increasing the total running time. While compressed graphs tailored for specific query tasks demonstrate efficiency, seamlessly integrating them with certain graph algorithms, such as matching, remains a challenge due to the critical need for comprehensive path information. 

% Despite of achieving higher compression ratio the obtained compressed graph cannot be used directly on algorithms such as matching and may requires large modification or high reconstruction overhead to meet the requirements of the down stream algorithms. While Fan et al. obtains compression/contraction up to 71.2\%  of large graphs  by converting starts, cliques and path into super-nodes. This approach achieves a speedup of up to  2.14 times in addressing queries such as shortest distance from a node. This compression technique of using super-nodes can be used in algorithms such as matching but may require  a large over head to get the path information of all nodes and thus increase the total execution time to obtain the results____. 


To the best of our knowledge the closest work to ours is that of Feder and Motwani____ who proposed a graph compression technique that preserves the path information of the original graph while retaining the algorithmic properties of the graph. Using the compressed graph obtained by their technique as input to other graph algorithms, such as bipartite matching____, edge connectivity____, and vertex connectivity____, leads to a significant reduction in their running time.
The technique of Feder and Motwani____ is based on partitioning the graph into bipartite cliques (complete bipartite subgraphs),
that is, a set of bipartite cliques  that partition the edge set of the graph.
They provided an algorithm for compressing bipartite graphs based on partitioning the graph into bipartite cliques that runs in $O(mn^{\delta} \log^2 n)$ time, where $n$ is the number of vertices, $m$ is the number of edges, and $\delta$ is a constant such that $0\leq \delta \leq 1$. 
%We discuss in the next section the main differences between our algorithm and that provided by Feder and Motwani____.\looseness -1
In the following section, we present Feder and Motwani's work as the background for clique partitioning-based graph compression and provide the motivation for our approach. \looseness=-1