%%%%%%%%%%%%%%%%%%%%%%%%%%  ltexpprt_twocolumn.tex  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is ltexpprt_twocolumn.tex, an example file for use with the SIAM LaTeX2E
% Preprint Series macros. It is designed to provide two-column output.
% Please take the time to read the following comments, as they document
% how to use these macros. This file can be composed and printed out for
% use as sample output.

% Any comments or questions regarding these macros should be directed to:
%
%                 Rachel Ginder
%                 SIAM
%                 3600 University City Science Center
%                 Philadelphia, PA 19104-2688
%                 USA
%                 Telephone: (215) 382-9800
%                 Fax: (215) 386-7999
%                 e-mail: rginder@siam.org


% This file is to be used as an example for style only. It should not be read
% for content.

%%%%%%%%%%%%%%% PLEASE NOTE THE FOLLOWING STYLE RESTRICTIONS %%%%%%%%%%%%%%%

%%  1. There are no new tags.  Existing LaTeX tags have been formatted to match
%%     the Preprint series style.
%%
%%  2. Do not change the margins or page size!  Do not change from the default
%%     text font!
%%
%%  3. You must use \cite in the text to mark your reference citations and
%%     \bibitem in the listing of references at the end of your chapter. See
%%     the examples in the following file. If you are using BibTeX, please
%%     supply the bst file with the manuscript file.
%%
%%  4. This macro is set up for two levels of headings (\section and
%%     \subsection). The macro will automatically number the headings for you.
%%
%%  5. No running heads are to be used for this volume.
%%
%%  6. Theorems, Lemmas, Definitions, Equations, etc. are to be double numbered,
%%     indicating the section and the occurrence of that element
%%     within that section. (For example, the first theorem in the second
%%     section would be numbered 2.1. The macro will
%%     automatically do the numbering for you.
%%
%%  7. Figures and Tables must be single-numbered.
%%     Use existing LaTeX tags for these elements.
%%     Numbering will be done automatically.
%%
%%  8. Page numbering is no longer included in this macro.
%%     Pagination will be set by the program committee.
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\documentclass[twoside,leqno,twocolumn]{article}

% Comment out the line below if using A4 paper size
\usepackage[letterpaper]{geometry}
\usepackage{booktabs} % For formal tables
\usepackage{ltexpprt}
\usepackage{hyperref}
\usepackage{caption}
% \usepackage{subfig}
% \usepackage{subfigure}
% \captionsetup[subfigure]{justification=centering}


\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage{fullpage}
\usepackage{algorithm}
\usepackage{algorithmicx}
% \usepackage{amsmath}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{physics}
\usepackage[noend]{algpseudocode}
\usepackage{color}
\usepackage{array}
\usepackage{pgfplots}
% \usepackage{subcaption}
% \renewcommand\thesubfigure{(\alph{subfigure})}

\usepackage{color,soul}
% \usepackage{caption}
 \usepackage{subcaption}
 \usepackage{mathrsfs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage[figuresright]{rotating}
\usepackage{floatpag}
\rotfloatpagestyle{empty}
% \usepackage{amsmath}% if you are using this package,
%                       % it must be loaded before amsthm.sty

% \usepackage{amsthm}
% % \usepackage{amssymb}
\usepackage{epigraph}
% \usepackage{subfloat}
% % \usepackage[margin=1in,headheight=13.6pt]{geometry}
% \usepackage[labelformat=simple]
\usepackage{multirow}
\usepackage{booktabs}
% \usepackage{caption}
\usepackage{array}
\algblockdefx{FORALLP}{ENDFAP}[1]%
  {\textbf{parallel for}#1 \textbf{do}}%
  {\textbf{end parallel for}}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\usetikzlibrary{positioning}
\usetikzlibrary{patterns}






%%%%%%%%%%%%%%%%algorithm %%%%%%%%%%%%%%%%%%%%%%%%%
\def\disp{\displaystyle}
\def\e{\epsilon}
\def\infinity{\rotatebox{90}{8}}
\def\ar{\ensuremath\leftarrow }
\def\lar{\ensuremath\leftarrow }

\def\rar{\ensuremath\rightarrow }

\algnewcommand{\algorithmicand}{\textbf{ and }}
\algnewcommand{\algorithmicor}{\textbf{ or }}
\algnewcommand{\OR}{\algorithmicor}
\algnewcommand{\AND}{\algorithmicand}
\algnewcommand{\var}{\texttt}


\algblockdefx{FORALLP}{ENDFAP}[1]%
  {\textbf{parallel for}#1 \textbf{do}}%
  {\textbf{end parallel for}}

% \DeclareMathOperator*{\argmin}{\arg\!\min}
% \DeclareMathOperator*{\argmax}{\arg\!\max}

\usetikzlibrary{positioning}
\usetikzlibrary{patterns}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcommand{\altura}{.45cm}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\bibliographystyle{plain}
\pgfplotsset{compat=1.18} % Set the compatibility version

\begin{document}

%
\newcommand\relatedversion{}
\renewcommand\relatedversion{\thanks{The full version of the paper can be accessed at \protect\url{https://arxiv.org/abs/1902.09310}}} % Replace URL with link to full paper or comment out this line


%\setcounter{chapter}{2} % If you are doing your chapter as chapter one,
%\setcounter{section}{3} % comment these two lines out.

\title{\Large A Clique Partitioning-Based Algorithm for Graph Compression}
\author{Akshar Chavan\thanks{Akshar Chavan is  with the Energy Aware Systems Laboratory (EAS), at The Ohio State University, OH 43210, USA
        {\tt\small chavan.43@osu.edu}} \and
         Sanaz Rabinia\thanks{Sanaz Rabinia is with the  Parallel and Distributed Computing Lab (PDCL), at Wayne State University, MI 48202, USA
        {\tt\small srabin@wayne.edu}
        } \and 
         Daniel Grosu\thanks{Daniel Grosu is with the  Parallel and Distributed Computing Lab (PDCL), at Wayne State University, MI 48202, USA
        {\tt\small dgrosu@wayne.edu}
        } \and 
         Marco Brocanelli\thanks{Marco Brocanelli is  with the Energy Aware Systems Laboratory (EAS), at The Ohio State University, OH 43210, USA
        {\tt\small brocanelli.1@osu.edu}} }%

\date{}

\maketitle

% Copyright Statement
% When submitting your final paper to a SIAM proceedings, it is requested that you include
% the appropriate copyright in the footer of the paper.  The copyright added should be
% consistent with the copyright selected on the copyright form submitted with the paper.
% Please note that "20XX" should be changed to the year of the meeting.

% Default Copyright Statement
% \fancyfoot[R]{\scriptsize{Copyright \textcopyright\ 20XX by SIAM\\
% Unauthorized reproduction of this article is prohibited}}

% Depending on which copyright you agree to when you sign the copyright form, the copyright
% can be changed to one of the following after commenting out the default copyright statement
% above.

%\fancyfoot[R]{\scriptsize{Copyright \textcopyright\ 20XX\\
%Copyright for this paper is retained by authors}}

%\fancyfoot[R]{\scriptsize{Copyright \textcopyright\ 20XX\\
%Copyright retained by principal author's organization}}

%\pagenumbering{arabic}
%\setcounter{page}{1}%Leave this line commented out.

\begin{abstract}
Reducing the running time of graph algorithms is vital for tackling real-world problems such as shortest paths and matching in large-scale graphs, where path information plays a crucial role. 
This paper addresses this critical challenge of reducing the running time of graph algorithms by proposing a new graph compression algorithm that partitions the graph into bipartite cliques and uses the partition to obtain a compressed graph having a smaller number of edges while preserving the path information.
This compressed graph can then be used as input to other graph algorithms for which path information is essential, leading to a significant reduction of their running time, especially for large, dense graphs. 
The running time of the proposed algorithm is~$O(mn^\delta)$, where $0 \leq \delta \leq 1$, which is better than $O(mn^\delta \log^2 n)$, the running time of the best existing clique partitioning-based graph compression algorithm (the Feder-Motwani (\textsf{FM}) algorithm).   
Our extensive experimental analysis show that our algorithm achieves a compression ratio of up to~$26\%$ greater and executes up to~105.18 times faster than the \textsf{FM} algorithm. In addition, on large graphs with up to 1.05 billion edges, it achieves a compression ratio of up to~3.9, reducing the number of edges up to~$74.36\%$.
Finally, our tests with a matching algorithm on sufficiently large, dense graphs, demonstrate a reduction in the running time of up to 72.83\% 
when the input is the compressed graph obtained by our algorithm, compared to the case where the input is the original uncompressed graph. 


\end{abstract}


\section{Introduction}

Graphs are versatile and powerful when used to model and analyze complex real-world problems in various domains, such as social, biological, and communication networks. As the volume of data generated and stored continues to grow, graphs become increasingly large and complex, presenting significant challenges for processing and  analyzing data, including prohibitively high processing times and huge storage requirements. To address these challenges, \emph{graph compression} techniques such as edge compression are employed to decrease the size  of a graph while maintaining its essential properties. \looseness -1
Edge compression involves shrinking the edge set of a graph, resulting in a compressed graph with fewer edges than the original. During edge compression it is important to preserve the \emph{path information} that plays a crucial role in graph's connectivity, i.e., the ability to reach one vertex from another using a sequence of edges. If the path information is preserved during compression, the compressed graphs can be used as input to several other graph algorithms for which the path information is essential such as matching and all-pairs shortest path algorithms, leading to a significant reduction of their running time, especially for large, dense graphs.
%For instance, a compressed graph with a reduced number of edges which maintains the path information of the original graph, enables matching algorithms to efficiently pair resources with tasks, optimizing resource allocation with a lower execution time. This reduction in execution time is especially noticeable in large, dense graphs, where using matching algorithm on uncompressed graphs may experience significant computation time. \looseness -1

However, to benefit from using the compressed graph as input to a graph algorithm, it is critical that the compression algorithm and the obtained compressed graph meet the following requirements: %\textcolor{red}{1 and 3 need revison}
1) the compression algorithm must have a low running time so that when used as a preprocessing step for other graph algorithms, like matching, leads to a lower overall execution time than in the case of executing the graph algorithms on the original graph; and
%benefit from a combined lower running time with compressed graphs; 
2) the compressed graphs are directly usable as input to other graph algorithms or require minimal modifications by these algorithms.
%3) requires minimal or no reconstruction overhead on the compressed graph to meet the requirements of the subsequent algorithms. 
These conditions make the design of graph compression algorithms challenging.
In this paper, we address this challenge by focusing on bipartite graphs and propose a new Clique Partitioning-based Graph Compression (\textsf{CPGC}) algorithm that compresses bipartite graphs while preserving their path information. A bipartite graph is a graph where the vertices can be divided into two disjoint sets, such that no two vertices within the same set are adjacent. We theoretically and experimentally show that our algorithm improves the running time and the compression rate over the algorithm proposed by Feder and Motwani~\cite{federMotwani}, i.e., the best existing clique partitioning-based graph compression algorithm used to speed-up the execution of other graph algorithms. \looseness -1

% Furthermore, we discuss the extension of our approach to non-bipartite graphs. 
\subsection{Related Work.}
\label{sec:related_works}
The problem of representing a graph in a compressed form has been extensively explored,
with techniques typically falling into two categories: \emph{lossless} and \emph{lossy compression}~\cite{besta2019survey}. The lossless compression retains the entirety of graph information, while the lossy compression sacrifices some details during the compression process. As one of the key requirements of our approach is to preserve the path information, it falls into the lossless compression techniques category. The majority of the proposed lossless graph compression techniques focus on specific domains such as web graphs, social networks, and chemistry networks~\cite{besta2019survey}. Those techniques primarily focus on reducing the storage space~\cite{danisch2023compressing} and/or efficiently addressing specific graph queries, such as graph pattern matching, community query, etc.~\cite{DAG_compression, scalable_compression, compressing_bisection, lossless_contraction, gZip}. Buehrer and Chellapilla~\cite{scalable_compression} achieved high compression ratios and small depths by using bipartite cliques with frequent itemset mining for compressing web graphs. However, while these techniques achieve high compression ratios, the resulting compressed graphs may not be directly usable as input to algorithms like all-pairs shortest paths, often requiring significant modifications or reconstruction overhead. Similarly, Fan et al.~\cite{lossless_contraction} achieved significant compression of large graphs by converting stars, cliques, and paths into super-nodes, achieving speedups in query tasks like shortest distance computations. While this technique is compatible with algorithms like matching, obtaining the path information for all nodes may introduce high overhead, increasing the total running time. While compressed graphs tailored for specific query tasks demonstrate efficiency, seamlessly integrating them with certain graph algorithms, such as matching, remains a challenge due to the critical need for comprehensive path information. 

% Despite of achieving higher compression ratio the obtained compressed graph cannot be used directly on algorithms such as matching and may requires large modification or high reconstruction overhead to meet the requirements of the down stream algorithms. While Fan et al. obtains compression/contraction up to 71.2\%  of large graphs  by converting starts, cliques and path into super-nodes. This approach achieves a speedup of up to  2.14 times in addressing queries such as shortest distance from a node. This compression technique of using super-nodes can be used in algorithms such as matching but may require  a large over head to get the path information of all nodes and thus increase the total execution time to obtain the results~\cite{lossless_contraction}. 


To the best of our knowledge the closest work to ours is that of Feder and Motwani~\cite{federMotwani} who proposed a graph compression technique that preserves the path information of the original graph while retaining the algorithmic properties of the graph. Using the compressed graph obtained by their technique as input to other graph algorithms, such as bipartite matching~\cite{Hopcroft:sjc73}, edge connectivity~\cite{Matula:focs87}, and vertex connectivity~\cite{Even:siam-jc75}, leads to a significant reduction in their running time.
The technique of Feder and Motwani~\cite{federMotwani} is based on partitioning the graph into bipartite cliques (complete bipartite subgraphs),
that is, a set of bipartite cliques  that partition the edge set of the graph.
They provided an algorithm for compressing bipartite graphs based on partitioning the graph into bipartite cliques that runs in $O(mn^{\delta} \log^2 n)$ time, where $n$ is the number of vertices, $m$ is the number of edges, and $\delta$ is a constant such that $0\leq \delta \leq 1$. 
%We discuss in the next section the main differences between our algorithm and that provided by Feder and Motwani~\cite{federMotwani}.\looseness -1
In the following section, we present Feder and Motwani's work as the background for clique partitioning-based graph compression and provide the motivation for our approach. \looseness=-1


\section{Background and Motivation}
\label{sec:b_and_m}
\subsection{Background.}
\label{sec:background}


Feder and Motwani~\cite{federMotwani} proposed a graph compression algorithm (called \textsf{FM} in the rest of the paper) that involves finding $\delta$-cliques and replacing each of them with a tripartite graph having two of the partitions the same as those of the clique it replaces, and the third partition composed of a single newly added vertex. 
A $\delta$-\emph{clique} in a bipartite graph graph~$G(U,W,E)$ with $|U|=|W|=n$ and $|E|=m$, is a complete bipartite subgraph with the left partition~$U'$ of size~$\lceil n^{1-\delta} \rceil$ and the right partition~$W'$ of 
size~${k}(n, m,\delta) = \Big\lfloor{\frac{\delta \log n}{\log(2n^2/{m})} }\Big\rfloor$,
where~$\delta$ is a constant such that $0 \leq \delta \leq 1$.
%A $\delta$-\emph{clique} is a complete bipartite graph in~$G(U,W,E)$, with size of right partition  is  ${k}(n, m,\delta) = \Big\lfloor{\frac{\delta \log n}{\log(2n^2/{m})} }\Big\rfloor$, where, $n = |U| =|W|$ the size of the left partition $U$ and right partition $W$ of $G$, $m = |E|$ the number of edges in $G$, and $\delta$ is a constant such that $0 < \delta < 1$. 
A \emph{tripartite graph} $G^* = (U, W, Z, E^*)$ consists of three disjoint sets of vertices~$U$, $W$, and~$Z$, $(U \cap W = W \cap Z =  U\cap Z = \emptyset)$, and a set of edges~$E^*$, where each edge connects either a vertex in~$U$ to a vertex in~$W$, or a vertex in~$W$ to a vertex in~$Z$, or a vertex in~$U$ to a vertex in~$Z$. Figure~\ref{fig:given_graph(fm)} and~\ref{fig:ch-matching-bip:bm2(fm)} show an example of a $\delta$-clique with left partition $\{u_1,u_2,u_3, u_4, u_5, u_7, u_8\}$ and right partition $\{w_4, w_5\}$ in a bipartite graph~$G$, and the corresponding tripartite graph that replaces it in the compressed graph~$G^*$, respectively. The number of edges in the $\delta$-clique is $14 = 7 \times 2$ while the number of edges in the corresponding tripartite graph is $9=7+2$, thus the number of edges in the compressed graph is reduced by~5. \looseness=-1


\tikzstyle{vertex}=[circle,draw,minimum size=14pt,inner sep=0pt]
\tikzstyle{edge} = [draw,thick,-]
\tikzstyle{weight} = [font=\small]
\usetikzlibrary{decorations.markings}

%\usetikzlibrary{arrows}
\usetikzlibrary{shapes,decorations,arrows,calc,arrows.meta,fit,positioning, shapes.geometric}
\tikzset{edge/.style = {->,> = latex'}}

\begin{figure*}[t!]
\vspace*{-0.2cm}
\centerline{
\subfloat[\centering]{

        \begin{tikzpicture}[scale=0.8, auto, swap]
	\tikzset{edge/.style = {->,> = latex'}}
    		\foreach \pos/\name in {{(0,7)/u_1}, {(0,6)/u_2}, {(0,5)/u_3}, {(0,4)/u_4}, {(0,3)/u_5}, {(0,2)/u_6}, {(0,1)/u_7}, {(0,0)/u_8},
      {(2.25,7)/w_1}, {(2.25,6)/w_2}, {(2.25,5)/w_3}, {(2.25,4)/w_4}, {(2.25,3)/w_5}, {(2.25,2)/w_6}, {(2.25,1)/w_7}, {(2.25,0)/w_8}}
        	\node[vertex] (\name) at \pos {$\name$};
        \draw (u_1)--(w_1);
        \draw (u_1)--(w_2);
        \draw (u_1)--(w_3);
        \draw [blue,line width=0.3mm](u_1)--(w_4);
        \draw [blue,line width=0.3mm](u_1)--(w_5);
        \draw (u_1)--(w_6);

        \draw (u_2)--(w_2);
        \draw (u_2)--(w_3);
        \draw [blue,line width=0.3mm](u_2)--(w_4);
        \draw [blue,line width=0.3mm](u_2)--(w_5);
        \draw (u_2)--(w_6);
        \draw (u_2)--(w_7);

        \draw (u_3)--(w_2);
        \draw (u_3)--(w_3);
        \draw [blue,line width=0.3mm](u_3)--(w_4);
        \draw [blue,line width=0.3mm](u_3)--(w_5);
        \draw (u_3)--(w_6);
        \draw (u_3)--(w_7);
        \draw (u_3)--(w_8);

        \draw (u_4)--(w_1);
        \draw (u_4)--(w_2);
        \draw (u_4)--(w_3);
        \draw [blue,line width=0.3mm](u_4)--(w_4);
        \draw [blue,line width=0.3mm](u_4)--(w_5);
        \draw (u_4)--(w_6);
        \draw (u_4)--(w_7);
        \draw (u_4)--(w_8);

        \draw (u_5)--(w_8);
        \draw (u_5)--(w_1);
        \draw (u_5)--(w_2);
        \draw (u_5)--(w_3);
        \draw [blue,line width=0.3mm](u_5)--(w_4);
        \draw [blue,line width=0.3mm](u_5)--(w_5);
        \draw (u_5)--(w_6);

        \draw (u_6)--(w_4);
        \draw (u_6)--(w_7);
        \draw (u_6)--(w_8);
        \draw (u_6)--(w_1);
        \draw (u_6)--(w_2);

        \draw (u_7)--(w_3);
        \draw [blue,line width=0.3mm](u_7)--(w_4);
        \draw [blue,line width=0.3mm](u_7)--(w_5);
        \draw (u_7)--(w_6);
        \draw (u_7)--(w_7);
        \draw (u_7)--(w_8);
        \draw (u_7)--(w_1);

        \draw (u_8)--(w_1);
        \draw (u_8)--(w_2);
        \draw (u_8)--(w_3);
        \draw [blue,line width=0.3mm](u_8)--(w_4);
        \draw [blue,line width=0.3mm](u_8)--(w_5);
        \draw (u_8)--(w_6);
        \draw (u_8)--(w_7);
        \draw (u_8)--(w_8);            		
 	\end{tikzpicture}
 	\label{fig:given_graph(fm)}
}

\subfloat[\centering]{
    \begin{tikzpicture}[ball/.style={ellipse,  draw}, >=LaTeX]
    \tikzstyle{vertex}=[circle,draw,minimum size=8pt,inner sep=0pt]
    \tikzstyle{vertex_set} = [ball/.style={ellipse, minimum width=0.5cm, minimum height=0.5cm, draw}, >=LaTeX]
        \tikzset{edge/.style = {->,> = latex'}}
        \foreach \pos/\name in {
          {(0,1)/w_1}, {(1,1)/w_2}, {(2,1)/w_3}, {(3,1)/w_4},  {(5,1)/w_6}, {(6,1)/w_7}, {(7,1)/w_8}, 
          {(3.5,4)/W}}
        \node[vertex] (\name) at \pos {$\name$};
        \foreach \pos/\name in {{(4,1)/w_5}}
            \node[vertex,blue] (\name) at \pos {$\name$}; % {(4,1)/w_5},

    \tikzstyle{vertex}=[circle,draw,minimum size=9pt,inner sep=1pt]

    \node[ball, at={(0.5,2)}, font=\fontsize{6}{10}\selectfont] (3) {$\{w_1, w_2\}$};
    \node[ball, at={(2.5,2)}, font=\fontsize{6}{10}\selectfont] (4) {$\{w_3,w_4\}$};
    \node[ball, at={(4.5,2)}, font=\fontsize{6}{10}\selectfont, blue] (5) {$\{w_5,w_6\}$};
    \node[ball, at={(6.5,2)}, font=\fontsize{6}{10}\selectfont] (6) {$\{w_7,w_8\}$};
    \node[ball, at={(1.5,3)}, font=\fontsize{6}{10}\selectfont] (1) {$\{w_1,w_2,w_3,w_4\}$};
    \node[ball, at={(5.5,3)}, font=\fontsize{6}{10}\selectfont, blue] (2) {$\{w_5,w_6,w_7,w_8\}$};

    
    \draw (W)--(1) node[at={(1.5,3.5)}, font=\fontsize{6}{10}\selectfont] {$(0,3)$};
    \draw (W)--(2)[ blue]  node[at={(5.5,3.5)}, font=\fontsize{6}{10}\selectfont] {$(1,3)$};
    \draw (1)--(3)  node[at={(0.4,2.5)}, font=\fontsize{6}{10}\selectfont] {$(00,1)$};
    \draw (1)--(4)  node[at={(2.55,2.5)}, font=\fontsize{6}{10}\selectfont] {$(01,2)$};
    \draw (2)--(5)[ blue]  node[at={(4.4,2.5)}, font=\fontsize{6}{10}\selectfont] {$(10,2)$};
    \draw (2)--(6)  node[at={(6.55,2.5)}, font=\fontsize{6}{10}\selectfont] {$(11,1)$};


    \draw (3)--(w_1)[dashed]  node[at={(0,0.5)}, font=\fontsize{6}{10}\selectfont] {$(000,0)$};
    \draw (3)--(w_2)  node[at={(1,0.5)}, font=\fontsize{6}{10}\selectfont] {$(001,1)$};
    \draw (4)--(w_3)  node[at={(2,0.5)}, font=\fontsize{6}{10}\selectfont] {$(010,1)$};
    \draw (4)--(w_4)  node[at={(3,0.5)}, font=\fontsize{6}{10}\selectfont] {$(011,1)$};
    \draw (5)--(w_5)[ blue]  node[at={(4,0.5)}, font=\fontsize{6}{10}\selectfont] {$(100,1)$};
    \draw (5)--(w_6)  node[at={(5,0.5)}, font=\fontsize{6}{10}\selectfont] {$(101,1)$};
    \draw (6)--(w_7)  node[at={(6,0.5)}, font=\fontsize{6}{10}\selectfont] {$(110,1)$};
    \draw (6)--(w_8)[dashed]  node[at={(7,0.5)}, font=\fontsize{6}{10}\selectfont] {$(111,0)$};
    \draw node[at={(3.5,4.5)}, font=\fontsize{7}{10}\selectfont] {$u_2: \quad (\epsilon,6)$};
    \end{tikzpicture}



    \label{fig:neighborhoodTree}
    }

 \subfloat[\centering]{
	\begin{tikzpicture}[scale=0.8, auto, swap]
	\tikzset{edge/.style = {->,> = latex'}}
    		% draw the vertices
    		\foreach \pos/\name in {{(0,7)/u_1}, {(0,6)/u_2}, {(0,5)/u_3}, {(0,4)/u_4}, {(0,3)/u_5}, {(0,2)/u_6}, {(0,1)/u_7}, {(0,0)/u_8}, 
      % {(2,5)/c_1}, {(2,4)/c_2}, {(2,3)/c_3},
      {(3,7)/w_1}, {(3,6)/w_2}, {(3,5)/w_3}, {(3,4)/w_4}, {(3,3)/w_5}, {(3,2)/w_6}, {(3,1)/w_7}, {(3,0)/w_8}}
        	\node[vertex] (\name) at \pos {$\name$};
        	
        \draw (u_1)--(w_1);
        \draw (u_1)--(w_2);
        \draw (u_1)--(w_3);
        % \draw (u_1)--(w_4);
        % \draw (u_1)--(w_5);
        \draw (u_1)--(w_6);

        \draw (u_2)--(w_2);
        \draw (u_2)--(w_3);
        % \draw (u_2)--(w_4);
        % \draw (u_2)--(w_5);
        \draw (u_2)--(w_6);
        \draw (u_2)--(w_7);

        \draw (u_3)--(w_2);
        \draw (u_3)--(w_3);
        % \draw (u_3)--(w_4);
        % \draw (u_3)--(w_5);
        \draw (u_3)--(w_6);
        \draw (u_3)--(w_7);
        \draw (u_3)--(w_8);

        \draw (u_4)--(w_1);
        \draw (u_4)--(w_2);
        \draw (u_4)--(w_3);
        % \draw (u_4)--(w_4);
        % \draw (u_4)--(w_5);
        \draw (u_4)--(w_6);
        \draw (u_4)--(w_7);
        \draw (u_4)--(w_8);

        \draw (u_5)--(w_8);
        \draw (u_5)--(w_1);
        \draw (u_5)--(w_2);
        \draw (u_5)--(w_3);
        % \draw (u_5)--(w_4);
        % \draw (u_5)--(w_5);
        \draw (u_5)--(w_6);

        \draw (u_6)--(w_4);
        \draw (u_6)--(w_7);
        \draw (u_6)--(w_8);
        \draw (u_6)--(w_1);
        \draw (u_6)--(w_2);

        \draw (u_7)--(w_3);
        % \draw (u_7)--(w_4);
        % \draw (u_7)--(w_5);
        \draw (u_7)--(w_6);
        \draw (u_7)--(w_7);
        \draw (u_7)--(w_8);
        \draw (u_7)--(w_1);

        \draw (u_8)--(w_1);
        \draw (u_8)--(w_2);
        \draw (u_8)--(w_3);
        % \draw (u_8)--(w_4);
        % \draw (u_8)--(w_5);
        \draw (u_8)--(w_6);
        \draw (u_8)--(w_7);
        \draw (u_8)--(w_8);            		
        \foreach \pos/\name in {{(1.5,3.5)/z_1}}
            \node[vertex,fill=gray] (\name) at \pos {$\name$};
        \draw [blue,line width=0.5mm] (u_1)--(z_1);
        \draw [blue,line width=0.5mm] (u_2)--(z_1);
        \draw [blue,line width=0.5mm](u_3)--(z_1);
        \draw [blue,line width=0.5mm](u_4)--(z_1);
        \draw [blue,line width=0.5mm](u_5)--(z_1);
        \draw [blue,line width=0.5mm](u_7)--(z_1);
        \draw [blue,line width=0.5mm](u_8)--(z_1);
        \draw [blue,line width=0.5mm](z_1)--(w_5);
        \draw [blue,line width=0.5mm](z_1)--(w_4);
 	\end{tikzpicture}
 	\label{fig:ch-matching-bip:bm2(fm)}
  }
}
\vspace*{-0.2cm}
    \caption{(a) Given bipartite graph $G(U,W,E)$; (b) Neighborhood tree of vertex $u_2 \in U$ that shows the path $\omega$ taken from root to the vertex $w_j \in W$ at the leaf and number of edges $d_{u_2,\omega}$ for each node using the tuple $(\omega, d_{u_2,\omega})$; and (c) the tripartite graph that replaces the $\delta$-clique with left partition $\{u_1,u_2,u_3, u_4, u_5, u_7, u_8\}$ and right partition $\{w_4, w_5\}$ in the compressed graph $G^{*}(U,W,Z, E^{*})$.}
\vspace*{-0.2cm}
\end{figure*}


The \textsf{FM} algorithm iteratively extracts $\delta$-cliques from the input graph~$G(U,W,E)$ by selecting $\hat{k} = k(n,m,\delta)$ vertices based on the neighborhood trees associated with each of the vertices~$u_i \in U$.
A \emph{neighborhood tree} for a vertex $u_i \in U$ is a binary tree whose nodes at level~$i$ correspond to a partition of~$W$ into sets of size~$|W|/2^i$. Thus, the root node corresponds to the set of all the vertices in~$W$, while each leaf node corresponds to a vertex in~$W$. Figure~\ref{fig:neighborhoodTree} shows the neighborhood tree for vertex~$u_2 \in U$ which has six neighbour vertices in~$W$. The neighborhood trees are described in more detail in 
Appendix~\ref{sec:appendix:FM}.
Once the cliques are extracted, \textsf{FM} compresses the graph by adding a new vertex set~$Z$ to the bipartite graph, thus converting it into a tripartite graph~$G^*$ (Figure~\ref{fig:ch-matching-bip:bm2(fm)}), where each vertex in~$Z$ corresponds to one $\delta$-clique extracted by the algorithm. Each vertex of the left and right partitions of the $\delta$-clique~$C_q$ is then connected via an edge to a new vertex~$z_q \in Z$.  This decreases the number of edges from $|U_q| \times |K_q|$ to $|U_q| + |K_q|$, thus compressing the graph. The obtained compressed graph~$G^*$ preserves the path information of the original graph, i.e., the connectivity between the vertices $u_i \in U$ and $w_j \in W$ of graph~$G$. The running time of the \textsf{FM} algorithm is~$O(mn^{\delta} \log^2 n)$. Feder and Motwani~\cite{federMotwani} also showed that their \textsf{FM} algorithm can be \emph{extended to the case of non-bipartite graphs} to obtain compression. In Appendix~\ref{sec:appendix:FM} we provide a more detailed description of the \textsf{FM} algorithm. 



\subsection{Motivation.}
\label{sec:motivation}
While the \textsf{FM} algorithm provides a solid foundation for lossless graph compression, there are several aspects that can be improved to achieve better compression and efficiency. In this section, we motivate our approach by delineating three critical aspects of a $\delta$-clique-based graph compression technique aimed at improving both compression ratio and execution efficiency.
\looseness=-1

%In this section, we investigate the \textsf{FM} algorithm in more details and state the motivation behind the design of our new algorithm for graph compression. 
% Primarily, we focus on improving in the following three main areas:

\vspace*{0.07cm}
\noindent\textbf{Vertex selection.} For each $\delta$-clique~$C_q$, the \textsf{FM} algorithm iteratively chooses~$\hat{k}$ vertices from~$W$ to form the right partition~$W_q$ of~$C_q$ using the neighborhood trees associated to each $u_i \in U$. The vertices $w_j \in W$ are located at the leaf of the neighborhood trees and are selected by computing a path from the root node to the leaf node of the neighborhood trees. The computation of the path depends on the cumulative sum of degrees, $d_{u_i, \omega}$, $\forall u_i \in U$ of each level of the neighborhood trees. Figure~\ref{fig:neighborhoodTree} shows one such path for selecting $w_5 \in W$ for the right partition~$K_q$ of $\delta$-clique~$C_q, q = 1$. 
% Therefore, to avoid any repetition of the selected vertices in the right partition of the $\delta$-cliques, the   \textsf{FM} algorithm updates the neighborhood trees. This update involves subtracting 1 from $d_{u_i, \omega}$  along the path of the selected vertex in the neighborhood trees corresponding to $u_i \in U$ which  has an edge with the selected vertex, making this approach highly sequential. 
It is important to note that the selection of a vertex is based on the cumulative degrees $d_{u_i, \omega}$ at every level of the neighborhood trees, i.e., $\sum_{u_i \in U} d_{u_i, \omega}$, which could overlook a vertex $w_j \in W$ with high degree, i.e., with high number of neighbors, and thus, lowering the number of common neighbours found for the right partition of the $\delta$-clique. 
Additionally, to avoid the selection of the same vertex in the following iteration, the neighborhood trees need to be updated, which makes vertex selection a sequential process. Thus,  to select vertices that lead to a larger set of common neighbors and to remove the dependency for the vertex selection, our algorithm selects vertices for the right partition of the $\delta$-cliques based on the non-increasing order of their degrees. We now describe the update required for the neighborhood trees to avoid repetitive selection of a vertex in $K_q$.
% (see Lemma~\ref{common_neighbour_limit}).
% This method of vertex selection also gains on updating the degrees of vertices which we describe next.  

\vspace*{0.07cm}
\noindent\textbf{Updating neighborhood trees.} With each selected vertex $w_j \in W$, the \textsf{FM} algorithm updates each neighborhood tree corresponding to all vertices $u_i \in U$ that have an edge with the selected vertex $w_j \in W$, i.e., neighborhood trees associated with $U_{K_q} = \{u_i \in U \ | \ w_j \in \Gamma(u_i)\}$. This update is made by subtracting~1 from $d_{u_i, \omega}$ at each node on the path from the root to the selected vertex $w_j$ (a leaf) regardless of whether the vertices $u_i \in U_{K_q}$ have an edge with $\forall w_j \in K_q$. 
Thus, we can say that with every clique the \textsf{FM} algorithm extracts, it eliminates the selection of at least one vertex $w_j \in W$ in future iterations of finding $\delta$-cliques. Thus restricting the extraction of cliques in future iterations even if there exists potential edges that can form non-trivial $\delta$-cliques (i.e., $\hat{k}> 1$) improving compression. As we will explain in Section~\ref{sec: experimental results}, this limitation is particularly relevant in less dense graphs, where there may not be enough common neighbors for the right partition of the $\delta$-clique to form additional cliques using the remaining edges. Therefore, in our approach we choose to update on the degree of each vertex~$w_j$ in the right partition~$K_q$ of the $\delta$-clique by subtracting the number of common neighbors found for~$K_q$. This makes sure that only edges that will be removed by the  $\delta$-clique are eliminated for the future iterations to find the $\delta$-clique, thereby giving the vertex $w_j \in W$ a chance to be selected for future $\delta$-cliques. We will now discuss the extraction of the $\delta$-cliques. \looseness=-1



% The \textsf{FM} algorithm updates each neighborhood tree corresponding to all vertices $u_i \in U$ that have an edge with the selected vertex $w_j \in W$, i.e., neighborhood trees associated with $U_{K_q} = \{u_i \in U \ | \ w_j \in \Gamma(u_i)\}$. This update is made by subtracting~1 from $d_{u_i, \omega}$ at each node on the path from the root to the selected vertex $w_j$ (a leaf) regardless of whether the vertices $u_i \in U_{K_q}$ have an edge with $\forall w_j \in K_q$. This process eliminates selecting the same vertex $w_j$ in future iterations of finding $\delta$-cliques. Thus, we can say that when the sum of~$\hat{k}$ of the cliques equals the number of vertices in the right partition, the $d_{i, \epsilon}$ of the root of all neighborhood trees becomes zero and the algorithm cannot extract any cliques, even if there are still edges that could potentially form non-trivial $\delta$-cliques (i.e., $\hat{k}> 1$). As we will explain in Section~\ref{sec: experimental results}, this limitation is particularly relevant in less dense graphs, where there may not be enough common neighbors for the right partition of the $\delta$-clique to form additional cliques using the remaining edges. Therefore, in our approach we choose to update on the degree of each vertex~$w_j$ in the right partition~$K_q$ of the $\delta$-clique by subtracting the number of common neighbors found for~$K_q$. This makes sure that only edges that will be removed by the  $\delta$-clique are eliminated for the future iterations to find the $\delta$-clique, thereby giving the vertex $w_j \in W$ a chance to be selected for future $\delta$-cliques. We will now discuss about the extraction of the $\delta$-cliques.



\begin{figure*}[t]
\vspace*{-0.4cm}
\centering
  \begin{subfigure}{.4\textwidth}
    \vspace*{-0.2cm}
    \includegraphics[width=\linewidth]{figures/fm_k_m_hat.pdf}  
    \vspace*{-0.5cm}
    \caption{\textsf{FM}}
    \label{fig:k_hatAndM_hat(fm)}
    \vspace*{-0.3cm}
  \end{subfigure}%
  \hfill % maximize the horizontal separation
  \begin{subfigure}{.4\textwidth}
  \vspace*{-0.2cm}
    \includegraphics[width=\linewidth]{figures/cpgc_k_m_hat.pdf}
    \vspace*{-0.5cm}
  \caption{\textsf{CPGC}}
 \label{fig:k_m_cliques(5)}
 \vspace*{-0.3cm}
  \end{subfigure}%
  \caption{Progression of $\hat{m}$, $\hat{k}$, and number of cliques extracted for a graph with 128 vertices in each bi-partition, density 0.98, and $\delta = 1$ by \textsf{FM} and \textsf{CPGC}.}
  
  \label{fig:example}
  \vspace*{-0.5cm}
\end{figure*}

\vspace*{0.07cm}
\noindent\textbf{Extracting  $\delta$-cliques.} A $\delta$-clique is formed by selecting $\hat{k} = k(n,\hat{m},\delta)$ vertices from~$W$ in~$G$. $\hat{k}$ depends on $\hat{m}$, the number of remaining edges, and on~$n$, the number of vertices in~$G$. It decreases as the number of iterations of the \textsf{FM} algorithm increases, as shown in Figure~\ref{fig:k_hatAndM_hat(fm)}. 
% , shows the changes in $\hat{m}$ and $\hat{k}$ throughout the progression of the \textsf{FM} algorithm. 
With each iteration of the algorithm, $|U_q| \times |W_q|$ edges are removed from~$G$ which results in decreasing~$\hat{m}$, and thus~$\hat{k}$. As more $\delta$-cliques are extracted, both~$\hat{m}$ and~$\hat{k}$ monotonically decrease. Furthermore, as we observe in Figure~\ref{fig:k_hatAndM_hat(fm)}, $\hat{k}$ remains constant for several iterations. This is due to the fact that~$\hat{k}$ is computed by using a function involving~$\hat{m}$ and~$n$, and the algorithm can remove a maximum of~$n \times \hat{k}$ edges in each iteration. As a result, $\hat{k}$ is decreased by at most one, leading to a relatively slow decrease. Consequently, $\hat{k}$ remains constant for an increasing number of iterations as the rate of edges being removed decreases with decreasing~$\hat{k}$, which motivates the selection of multiple $\delta$-cliques for the same~$\hat{k}$ in each iterations of the algorithm. 
Figure \ref{fig:k_m_cliques(5)}, shows the progression of~$\hat{m}$ and~$\hat{k}$ and the number of $\delta$-cliques extracted by our algorithm, \textsf{CPGC}. We observe that for the same $\hat{k}$, unlike \textsf{FM}, \textsf{CPGC} extracts more $\delta$-cliques in one iteration leading to the formation of larger $\delta$-cliques compared to \textsf{FM}. For example, for $\hat{k}=4$, \textsf{FM} extracts one clique in each iteration i.e, from iteration~7 to~11, while \textsf{CPGC} extracts~8 cliques in one iteration. Extracting larger $\delta$-cliques results in removing more edges in each iteration of \textsf{CPGC} and thus accelerates the rate at which~$\hat{m}$ decreases. Because of this we also observe that for the same graph \textsf{CPGC} extracted fewer $\delta$-cliques compared to \textsf{FM}. Despite of removing a smaller number of $\delta$-cliques, our experimental results (Section~\ref{sec: experimental results}) show that \textsf{CPGC} obtains a better compression ratio than \textsf{FM}.  \looseness=-1





\vspace*{0.1cm}
\noindent\textbf{Our contributions.} The above limitations of the \textsf{FM} algorithm (the best existing algorithm for clique partitioning-based graph compression) motivated us to design a novel algorithm called  Clique Partitioning-based Graph Compression (\textsf{CPGC}). Our \emph{contributions} are:
\begin{enumerate}

    \item \textsf{CPGC} enables the removal of multiple $\delta$-cliques in each iteration, thereby obtaining a running time of $O(mn^{\delta})$ which is better than $O(mn^{\delta} \log^2 n)$, the running time of \textsf{FM}. \looseness=-1
    %\item Extract multiple $\delta$-cliques in each iteration of the algorithm, allowing for further accelerating the process.
    
    \item \textsf{CPGC} %ensure uniformity in updating the degrees of the vertices in the extracted cliques, 
    enables the selection of vertices for forming cliques more than once. This can increase the number of cliques that are extracted during the compression, particularly in the case of large and high density graphs.

    \item \textsf{CPGC} obtains an average compression ratio \emph{at least} as good as that of the \textsf{FM} algorithm.
    The \emph{compression ratio} is defined as $\frac{m}{m^{*}}$, where~$m$ is the number of edges in the original graph~$G$ and~$m^{*}$ is the number of edges in the compressed graph~$G^{*}$.

     \item A comprehensive experimental analysis showing that on large dense graphs, \textsf{CPGC} achieves a compression ratio of up to~26\% greater than that obtained by the \textsf{FM} algorithm and executes up to~105.18 times faster than the \textsf{FM} algorithm. 
\end{enumerate}
%By addressing these objectives, \textsf{CPA} seeks to enhance the performance of the algorithm by enabling extraction of multiple $\delta$-cliques, and maintaining compression effectiveness.





\section{The Proposed Algorithm}
\label{sec:proposed_algorithms}
\subsection{Clique Partitioning-based Graph Compression \textsf{(CPGC)} Algorithm.}

In this section, we present the design of our Clique Partitioning-based Graph Compression (\textsf{CPGC}) algorithm. 
For a given input graph~$G$, \textsf{CPGC} builds the compressed graph~$G^{*}$ of~$G$ by partitioning~$G$ into bipartite cliques.
\textsf{CPGC} iteratively finds bipartite cliques in the given graph~$G(U, W, E)$ and compresses it until finding new bipartite cliques does not contribute to the compression of~$G$. The size of the right partition of the cliques is determined by~$\hat{k}$, which guarantees a $\delta$-clique, and the selection of vertices~$w_j \in W$ depends on the degree of vertices~$d_{w_j} = |N(w_j)|$. The partition of~$G$ into bipartite cliques guarantees that each edge in~$G$ is in exactly one clique, i.e., %\textcolor{blue}{$E(C_{i}) \cap E(C_{i+1}) \cap \ldots \cap E(C_q) = \emptyset$} 
$E(C_i) \cap E(C_j) = \emptyset$,  $\forall i, j$ and $ i \neq j$, where $E(C_i)$ is the set of edges of $\delta$-clique~$C_i$. \textsf{CPGC} is given in Algorithm~\ref{alg:CPGC}. The input of \textsf{CPGC} consists of the adjacency matrix~$A$ of $G$, and a constant~$\delta$. \looseness=-1

\vspace*{0.1cm}
\noindent {\bf Initialization} (Line~1). For the given bipartite graph $G(U,W, E)$, \textsf{CPGC} initializes~$q$, the index of the bipartite cliques extracted from~$G$, the number of vertices, $n$ where $n = |U| = |W|$, the number of edges~$\hat{m}$, and the degree of the vertices ${d_{w_j}}$ $\forall j=1, \ldots,  |W|$, to~$[0]_{n}$. \textsf{CPGC} also initializes the set of extracted bipartite cliques~$\mathcal{C}$, to the empty set. 


\label{subsec:CPGC}
\begin{algorithm}[!t]
\caption{{\small  \textsf{CPGC}: Clique Partitioning-based Graph Compression Algorithm}}
{
\small
%\footnotesize
%\scriptsize  
%\tiny
\begin{algorithmic}[1]
\INPUT {$A$: Adjacency matrix;  \hspace*{0.2cm} $\delta$: a constant such that $0\leq \delta \leq 1$.} 
%\Statex	\hspace*{0.64cm}{$\delta$: a constant such that $0\leq \delta \leq 1$.}

\State{$q \leftarrow 0; \quad n \leftarrow|W|; \quad \hat{m} \leftarrow|E|; \quad {d_w} \gets [0]_n; \quad \mathcal{C} \gets \emptyset$}
% \State{$A \gets $ adjacency matrix}
% \State{$\mathcal{C} \gets \emptyset $}
% \State{$d_u \gets [0]_n$}
% \State{${d_w} \gets [0]_n$}
\For{{${i = 1,\ldots, n}$}}
\For{{${j = 1,\ldots, n}$}}
% \State{${d_{u_i}} \gets {d_{u_i}} +  E_{{u_i}, w_j}$}
\State{ ${d_{w_j}} \gets  {d_{w_j}} +  E_{{u_i}, w_j}$}
% \State{ $ {d_{w_j}} \gets \sum_{u \in \mathcal{U}} E_{{u_i}, w_j} \gets  |\ (v_i)|$}


\EndFor
% \State{Broadcast $d_{u_i}$ to all $i$}
\EndFor
\State{$\hat{k} \gets \Big\lfloor{\frac{\delta \log n}{\log(2n^2/\hat{m})} }\Big\rfloor$}
% \While{$\hat{m} \geq n^{2-\delta}$}
\While{$\hat{k} > 1$ } % \textbf{and} $\delta$-clique exists)
    % \State{$ q \leftarrow q+1$}

    \State{$\mathscr{C}, \hat{A} , \hat{d_w}\gets \textsf{CSA(}q, \hat{k}, A, n, d_w\textsf{)}$ }
    \State{$\mathcal{C} \gets \mathcal{C} \cup  \mathscr{C} $}
    \State{$d_w \gets \hat{d_w}$}
    \State{$ A \leftarrow \hat{A}$}
    %\State{$\hat{m} \gets |E|$}
    \State{$\hat{m} \gets \sum_{j=1}^{|W|} d_{w_j}$}
    \State{$ q \leftarrow q+ |\mathscr{C}|$}
    \State{$\hat{k} \gets \Big\lfloor{\frac{\delta \log n}{\log(2n^2/\hat{m})} }\Big\rfloor$}
\EndWhile
\State{For each clique in $\mathcal{C} = \{C_1, \ldots, C_q\}$  add edges from all the vertices in the left partition $U_q$ to an additional vertex~$z_q$, and from~$z_q$ to all the vertices in the right partition $K_q$. This would compress the graph by replacing $|U_q| \times |K_q|$ edges with $|U_q| + |K_q|$ edges.}
\State{The remaining edges in~$G$ are trivial cliques and are added in the compressed graph~$G^*$.}
\State{\textbf{Output:} $G^*$, the compressed graph of~$G$.}
\end{algorithmic}
\label{alg:CPGC}
}
\end{algorithm}



\vspace*{0.1cm}
\noindent {\bf Partition size} (Lines 2-5).
\textsf{CPGC} computes the degree of each vertex $w_j \in W$, $d_{w_j} = |N(w_j)|$, where $E_{{u_i}, w_j}$ is~1, if there is an edge between~${u_i}$ and~$w_j$, and 0, otherwise (Lines 2-4). It determines the size of the right partition of the bipartite clique, $\hat{k}$ (Line 5), which guarantees the existence of a $\delta$-clique with $k(n,\hat{m}, \delta)$~\cite{federMotwani}. It is important to say that $\hat{m}=|E|=\sum_{j=1}^{|W|} d_{w_j}$. \looseness=-1

\vspace*{0.1cm}
\noindent {\bf Clique extraction} (Lines 6-13). \textsf{CPGC} proceeds with extracting $\delta$-cliques until extracting new cliques does not contribute to the graph compression. This happens when $\hat{k} = 1$, which results in obtaining trivial bipartite cliques. Thus, the while loop in Lines~6 to~13 is executed until trivial cliques are produced.
% and, when for the current selection of~$\hat{k}$ vertices, the Clique Stripping Algorithm (\textsf{CSA}) given in Algorithm~\ref{alg:CSA} could not find any common neighbors, which leads to no change in the remaining number of edges,~$\hat{m}$. 
% or \textsf{CSA} could not find any common neighbours. 
\emph{Clique Stripping Algorithm} (\textsf{CSA}), presented in Algorithm~\ref{alg:CSA}, finds $\delta$-cliques in the bipartite graph~$G$. \textsf{CSA} takes the updated index of the bipartite clique, $q$, the size of the right partition of the bipartite clique,~$\hat{k}$, the adjacency matrix, $A$, the size of the left partition of~$G$, $n$, and the degree of vertices $d_{w_j} = |N(w_j)|, \ \forall w_j \in W$ as input.
\textsf{CSA}, which is described in Subsection~\ref{subsec:S-CPGC}, returns the set of bipartite cliques~$\mathscr{C}$ that it determined in the current execution, the updated adjacency matrix, $\hat{A}$, and the updated degree of vertices~$\hat{d_w}$. In Line~8, \textsf{CPGC} adds the new bipartite cliques to the set of all bipartite cliques~$\mathcal{C}$, updates the degree of vertices~$d_w$ (Line~9), and adjacency matrix~$A$ (Line~10). In Line 11, \textsf{CPGC} updates the number of edges, $\hat{m}$ in the given graph $G(U,W,E)$ after extracting the $\delta$-cliques, and the clique index~$q$ (Line~12). Finally, in Line~13, after the $\delta$-cliques are removed, \textsf{CPGC} updates~$\hat{k}$ for the next iterations. \looseness=-1

\vspace*{0.1cm}
\noindent {\bf Graph compression} (Lines 14-15).
When the while loop (Lines 6-13) terminates, \textsf{CPGC} compresses the graph by adding a new vertex set~$Z$ to the bipartite graph.
% , thus converting it into a tripartite graph.
% (Figure~\ref{fig:ch-matching-bip:bm2(e)}), where $\mathcal{C}$ is the set of $\delta$-cliques.  
Each vertex of the left and right partitions of clique $C_q$ are connected to a vertex $z_q$ via an edge, thus forming a tripartite graph. This decreases the number of edges from $|U_q| \times |K_q|$ to $|U_q| + |K_q|$, thus compressing the graph. The remaining edges in the given bipartite graph $G(U,W, E)$ connecting the set~$U$ and~$W$, which are not part of any $\delta$-clique are connected directly in the tripartite graph. Therefore, the compressed graph obtained by the \textsf{CPGC} preserves the path information of the original graph. \looseness=-1

In Appendix~\ref{sec:appendix:example} we provide an example showing the execution of \textsf{CPGC} on a given bipartite graph.

 %%%%%%%%%%%%%%%%%%%%%% Sequential Clique Partitioning %%%%%%%%%%%%%%%%%%%%%%%
\subsection{Clique Stripping  Algorithm \textsf{(CSA).}}
\label{subsec:S-CPGC}

The Clique Stripping Algorithm \textsf{(CSA).}, given in Algorithm~\ref{alg:CSA}, extracts $\delta$-cliques from the given bipartite graph~$G(U,W, E)$. \textsf{CSA} initially selects the vertices for the right partition~$K_c$ of the clique~$C_c$ and then the common neighbours for the set~$K_c$. The input of \textsf{CSA} consists of the updated index of the clique, $q$, the size of the right partition $\hat{k}$ which guarantees a $\delta$-clique in~$G$, the adjacency matrix~$A$, the size of left/right partition~$n$, and the degree of vertices $d_{w_j} =  |N(w_j)|$. \looseness=-1

 \begin{algorithm}[!t]
\caption{{\small \textsf{CSA}: Clique Stripping Algorithm }}
{
\small
%\footnotesize
%\scriptsize  1
%\tiny
\begin{algorithmic}[1]
\INPUT {$q$: Index of bipartite clique; \hspace*{0.1cm} $\hat{k}$: Size of the right bipartition in bipartite clique;}
   \Statex	\hspace*{0.6cm}{$A$: Adjacency matrix; \hspace*{0.9cm} $n$: Number of vertices in left/right bipartition of $G$;}
   \Statex	\hspace*{0.6cm}{$d_{w}$: Degree of vertices in bipartition $W$.}
   %\Statex	\hspace*{0.6cm}{$\hat{k}$: Size of the right bipartition in bipartite clique;}
   %\Statex	\hspace*{0.6cm}{$A$: Adjacency matrix;}
   %\Statex	\hspace*{0.6cm}{$n$: Number of vertices in left/right bipartition of $G$;}
   %\Statex	\hspace*{0.6cm}{$d_{w}$: Degree of vertices in bipartition $W$.}
\State{$ \mathcal{K} \gets \emptyset$}
\State{Sort $d_w$ in non-increasing order.}
\Statex{Let $ d_{w_{\pi(1)}}, d_{w_{\pi(2)}},\ldots,d_{w_{\pi(n)}} $ be the order.}
\State{$j \gets 1$}
% \While{$ s < \hat{k}$}
\While{$d_{w_{\pi(j)}} \geq d_{w_{\pi(\hat{k})}}$}
    % \State{$y_s \gets w_j$}
    
    \State{$ \mathcal{K} \gets \mathcal{K} \cup \{w_{\pi(j)}\}$}
    \State{$j \gets j + 1$}

\EndWhile
\State{$\gamma \gets   \big\lfloor{\frac{|\mathcal{K}|}{\hat{k}}} \big\rfloor$}
\For{ $ c = q + 1,\ldots,q + \gamma $}
    \State{$K_c \gets \{ w_{\pi_{(j)}} \in \mathcal{K} \ | \ (c - (q + 1) )\cdot \hat{k} < j \leq (c -q ) \cdot \hat{k} \} $}
\State{$U_{K_c} \gets \{ u_i \in U | K_c \subseteq N(u_i)\}$ }
\State{$C_c \gets \{ (U_{K_c}, K_c) \}$}
\State{Update $A$, by removing edges $( U_{K_c} \times  K_c) \in C_c$ }
\State{Update $d_w$ by subtracting $|U_{K_c}|$ from each $d_{w_{\pi(j)}}$, where $w_{\pi(j)} \in K_c$}
\EndFor
\State{$\mathscr{C} \gets \{C_{q+1},\ldots,C_{q + \gamma} \} $}

\State \textbf{Output} Set of cliques $\mathscr{C}, \hat{A}, \hat{d_w}$
\end{algorithmic}
\label{alg:CSA}
}
\end{algorithm}


\vspace*{0.07cm}
\noindent {\bf Initialization} (Lines 1-3). \textsf{CSA} initializes the set of selected vertices for clique extraction, $\mathcal{K}$ to the empty set. %where $\mathcal{K} = \{ w_{\pi(j)} \ | \ w_{\pi(j)} \in W\}$ (Line 1).
In Line 2, it sorts~$d_w$ in non-increasing order of the degrees of vertices $w_j \in W$, i.e., $d_{w_{\pi(1)}} \geq d_{w_{\pi(2)}} \geq \ldots \geq d_{w_{\pi(n)}}$. Then, in Line 3 it initializes the index~$j$ in the list of sorted vertices to~1. \looseness=-1


\vspace*{0.07cm}
\noindent {\bf Vertex selection} (Lines 4-7).
\textsf{CSA} selects the vertices of a clique to be extracted, based on their degrees. Selecting a vertex with higher degree results in a larger set of neighbours that can be part of the selected clique. Therefore, \textsf{CSA} selects the vertices whose degrees are greater than or equal to the $\hat{k}$-th largest degree in the sorted~$d_w$, i.e., $\mathcal{K} = \{ w_{\pi(j)} \ | \ d_{w_{\pi(j)}} \geq d_{w_{\pi(\hat{k})}} \}$. 
%In Line 4, \textsf{CSA} uses this termination condition for the while loop, since the selection of the vertices in such a ways may result in extracting more vertices than~$\hat{k}$, as we explain this in details in property \textbf{II}. 
\textsf{CSA} adds the selected vertices to $\mathcal{K}$ and increments the iterator~$j$ (Lines~5-6). Thus, the while loop in Lines~4-~6 is executed until all vertices with degrees greater than~$d_{w_{\pi(\hat{k})}}$ are chosen. In Line~7, \textsf{CSA} calculates~$\gamma$ to partition the set~$\mathcal{K}$ such that the size of a partition is less than or equal to~$\hat{k}$. Therefore, every time \textsf{CSA} is executed it finds~$\gamma$ cliques.

\vspace*{0.07cm}
\noindent {\bf Clique extraction} (Lines 8-13).
The for loop (Lines 8-9) first partitions~$\mathcal{K}$ into subsets~$K_c$, such that
%\begin{equation}
$\bigcup_{c=q+1, \ldots, q+\gamma}K_c = \mathcal{K}$ %\label{eq:set_k_eq1}
%\end{equation}
%\begin{equation}
%\quad \mathrm{and} \quad
and $K_{q+i}  \bigcap K_{q+j} = \emptyset, \ i = 1, \ldots, \gamma, j = 1, \ldots, \gamma, i \neq j.$
\label{eq:set_k_eq2}
%\end{equation}
Then it constructs $U_{K_c}$ such that each vertex in $U_{K_c}$ is part of the set of common neighbors of~$K_c$ (Line~10). In Line~11, \textsf{CSA} forms clique $C_c$ with partitions~$U_{K_c}$ and~$K_c$, where $|U_{K_c}| \geq \lceil{n^{1-\delta}\rceil} $ and $|K_c| = \hat{k}$. After forming the clique~$C_c$, in Line~12, the algorithm updates the adjacency matrix by removing the edges in the clique~$C_c$ and in Line~13, updates the degrees of the vertices that are part of the clique by removing the size of left partition from the degree of each vertex~$w_{\pi(j)} \in K_c$, i.e., $d_{w_{\pi(j)}} = d_{w_{\pi(j)}} - |U_{K_c}|, \  \forall \  w_{\pi(j)} \in K_c , \ c = q+1, \ldots q+\gamma$. Finally, in Line~14, \textsf{CSA} forms~$\mathscr{C}$, the set of all the cliques extracted in the current execution, and in Line~15, it returns the set of cliques~$\mathscr{C}$, the updated adjacency matrix~$\hat{A}$, and the updated degrees of vertices~$\hat{d}_w$ to~\textsf{CPGC}. \looseness=-1

\vspace*{0.07cm}
\noindent \textbf{Time complexity of \textsf{CSA}.} \textsf{CSA} takes $O(n \log n)$ to sort the degrees of vertices~$d_w$, in Line~2. The while loop in Lines 4-6, takes at most $O(n)$ to select vertices for the $\delta$-cliques. The for loop (Lines 8-13) executes~$O(\gamma)$ times, where~$1\leq \gamma \leq \frac{n}{\hat{k}}$. In the for loop, Line 9 and~10 takes $O(\hat{k})$ and $O(n\hat{k})$ to find the right and left partition of the $c$-th $\delta$-clique, respectively.  \textsf{CSA} takes $O(n\hat{k})$ to remove the edges in the $c$-th $\delta$-clique, in Line~12. It takes $O(\hat{k})$ to update the degrees $d_{w}\in K_{c}$, in Line~13. Therefore, the total running time of \textsf{CSA} is dominated by Lines 10 and 12 and thus  \textsf{CSA} takes $O(n \gamma \hat{k})$ time. \looseness=-1 


\vspace*{0.07cm}
\noindent {\textbf{Time complexity of} \textbf{\textsf{CPGC}}}.  \textsf{CPGC} takes $O(n^2)$ to calculate the degrees~$d_w$ of vertices in~$W$ in Lines \mbox{2-4}. The running time of the while loop (Lines 6-13) is dominated by the running time of the function \textsf{CSA} in Line~7 which is given by Algorithm~\ref{alg:CSA}. \textsf{CSA} takes $O(n\gamma \hat{k})$ to remove $\gamma \hat{k} n^{1-\delta}$ edges. Therefore, on average it takes $O(n^{\delta})$  time to remove one edge from the given graph. Thus, the while loop in \textsf{CPGC} takes $O(m n^{\delta})$ time to extract $\delta$-cliques. In the end, to compress the graph, \textsf{CPGC} in Lines 14-15, takes linear time. Therefore, the total running time of \textsf{CPGC} is $O(mn^{\delta})$ which is dominated by the while loop in Lines 6-13.   \looseness=-1


\section{Properties of \textsf{CPGC}}
\label{properties}
%In this section, we show the properties of \textsf{CPGC}.


% \cite{federMotwani} \textsf{} in 
    % For our proof we use , which is based on fixed ordered set $K \subset W$ of size $\hat{k} = k(n,m,\delta)$. As \textsf{CPGC} selects $\hat{k} = k(n,m,\delta)$ vertices to form $K_q \in W$, $K_q$ is one of the ordered set of size $\hat{k}$. Therefore, 



\begin{theorem}
\label{theorem1}
%\textsf{CPGC} preserves the graph's path information. %while extracting multiple $\delta$-cliques.
The compressed graph~$G^*(U,W,Z,E^*)$ obtained by {\rm \textsf{CPGC}} preserves the path information of the original graph~$G(U,W,E)$. 
\end{theorem}
\begin{proof}
Provided in Appendix~\ref{sec:appendix:proofs}.
\end{proof}


In the following, we determine a bound on the number of edges in the compressed graph obtained by \textsf{CPGC}. This bound tells us how good the compression achieved by \textsf{CPGC} is. First, we state a theorem from~\cite{federMotwani} that guarantees the existence of a $\delta$-clique in a bipartite graph. This theorem will be used in the proofs of \textsf{CPGC}'s compression properties. 
\begin{theorem}
    ${\mathrm{\cite{federMotwani}(Theorem \ 2.2)}}$ Every bipartite graph $G(U,W,E)$ contains a $\delta$-clique.
\end{theorem}

Next, we provide a bound on the minimum number of edges~$\hat{m}$ of~$G$ required to obtain a compression of~$G$ into $G^*$ by extracting $\delta$-cliques. 

\begin{lemma}
\label{min_m_hat}
Given a graph $G(U,W,E)$, where $n = |U| = |W|$, $m = |E|$, and a constant $\delta$, $0 < \delta < 1$, if $m < 2n^{2-\frac{\delta}{2}}$ then extracting $\delta$-cliques and replacing them with tripartite graphs as done in \textsf{CPGC} does not lead to a compression of~$G$.
\end{lemma}
\begin{proof}
Provided in Appendix~\ref{sec:appendix:proofs}.
\end{proof}



\begin{theorem}
\label{comp_ratio}
    Let $G(U,W,E)$ be any bipartite graph with $|U| = |W| = n$ and $|E| = m > 2n^{2-\frac{\delta}{2}}$, where~$\delta$ is a constant such that $0 < \delta < 1$. Then, the number of edges in the compressed graph $G^*(U,W,Z,E^*)$ obtained by \textsf{CPGC} is $|E^*| = O\left(\frac{m}{k(n,m,\delta)}\right)$.
\end{theorem}
\begin{proof}
    We follow the basic idea of the proof from Theorem 2.4 in \cite{federMotwani} and extend it to apply to the \textsf{CPGC} algorithm. We assume that initially the compressed graph $G^*$ has 0 edges, i.e.,~$|E^*| = 0$, and that edges are added to~$E^*$ as the algorithm progress. To estimate the number of edges in~$E^*$, we divide the iterations of \textsf{CPGC} into stages, where each stage~$i$ consists of extracting one or more~$\delta$-cliques with a fixed~$\hat{k}_i$. Therefore, the~$i^{th}$ stage includes all $\delta$-cliques extracted after the number of edges in~$G$ becomes less than~$m/2^{i-1}$ for the first time, and before the number of edges in~$G$ becomes less than~$m/2^{i}$ for the first time. For stage~$i$,  $\hat{k}$ is always going to be at least
    %\begin{align}
     $   k_i =  \frac{\delta \log n}{2 \log(2^i \cdot 2n^2/m)}$.
    %\end{align}
    
    Assume that a clique~$C_q$ with right partition~$K_q \subseteq W$ and left partition~$U_{K_q} \subseteq U$ is extracted in stage~$i$. Extracting~$C_q$ removes $|K_q \times U_{K_q}|$ edges from $G$ and adds $|K_q| + |U_{K_q}|$ edges in~$E^*$. Therefore, the average number of edges added in~$E^*$ for each edge removed from~$G$ by extracting $C_q$ is $\rho = \frac{|K_q|+|U_{K_q}|}{|K_q \times U_{K_q}|} = \frac{1}{|K_q|} + \frac{1}{|U_{K_q}|}$. From the definition of a $\delta$-clique, during stage~$i$ we have $|K_q| = \hat{k} \geq k_i$ and $|U_{K_q}| \geq n^{1-\delta}$, and therefore, $\rho \leq \frac{1}{k_i} + \frac{1}{n^{1-\delta}}$.
    %\begin{align}
    %    \frac{|K_q|+|U_{K_q}|}{|K_q \times U_{K_q}|} = \left( \frac{1}{|K_q|} + \frac{1}{|U_{K_q}|}\right) \leq \left( \frac{1}{k_i} + \frac{1}{n^{1-\delta}} \right)
    %\end{align}
    %where $|K_q| = \hat{k} \geq k_i$ in stage $i$ and $|U_{K_q}| \geq n^{1-\delta}$ by the definition of the $\delta$-clique. 
    The total number of edges removed from~$G$ in stage~$i$ cannot be greater than $2m/2^i$, therefore, the number of edges added to~$E^*$ during stage~$i$ is less than or equal to $\rho \frac{2m}{2^i} = \left(\frac{1}{k_i} + \frac{1}{n^{1-\delta}}\right)\frac{2m}{2^i}$.  
    %\begin{align}
    %    \left(\frac{1}{k_i} + \frac{1}{n^{1-\delta}}\right)\frac{2m}{2^i}
    %\end{align}

    \textsf{CPGC} terminates extracting cliques when the number of remaining edges in~$G$ is less than~$2n^{2 - \frac{\delta}{2}}$. This is to eliminate the extraction of trivial cliques, as shown in Lemma~\ref{min_m_hat}.
    Next, we determine an upper bound on the total number of edges~$m^*$ added by \textsf{CPGC} to~$G^*$ before it terminates extracting $\delta$-cliques (i.e., when $m < 2n^{2 - \frac{\delta}{2}}$). 
% $m^*$ be the total number of edges in $G^*$, now following the Theorem 2.4 in \cite{federMotwani} we get,
   % \begin{gather}
   %       M \leq \sum_{i=1}^{\big\lceil\log(\frac{m}{n^{2 - \delta}})\big\rceil} \bigg(\frac{1}{k_i} + \frac{1}{n^{1-\delta}}\bigg)\frac{2m}{2^i}
   %  \end{gather}
    Therefore, 
    % \begin{gather}
    %      m^* \leq \sum_{i=1}^{\big\lceil\log(\frac{m}{2n^{2 - \delta/2}})\big\rceil} \bigg(\frac{1}{k_i} + \frac{1}{n^{1-\delta}}\bigg)\frac{2m}{2^i}  \\
    %      \leq \sum_{i=1}^{\infty} \bigg(\frac{1}{k_i} + \frac{1}{n^{1-\delta}}\bigg)\frac{2m}{2^i}. 
    % \end{gather}
    \begin{gather}
    \begin{split}
        m^* \leq \sum_{i=1}^{\big\lceil\log\left(\frac{m}{2n^{2 - \delta/2}}\right)\big\rceil} \bigg(\frac{1}{k_i} + \frac{1}{n^{1-\delta}}\bigg)\frac{2m}{2^i} \\
        \leq \sum_{i=1}^{\infty} \bigg(\frac{1}{k_i} + \frac{1}{n^{1-\delta}}\bigg)\frac{2m}{2^i}.
    \end{split}
\end{gather}


    Since $1/(2k_i) \leq 1/k + i/(\delta\log n)$, we obtain,
    \begin{gather}
    \begin{split}
         m^* \leq \frac{4m}{k} \sum_{i=1}^{\infty} \frac{1}{2^i} + \frac{4m}{\delta \log n} \sum_{i=1}^{\infty} \frac{i}{2^i} + \frac{2m}{n^{1-\delta}} \sum_{i=1}^{\infty} \frac{1}{2^i} \\
         \leq 2m \left(\frac{2}{k} + \frac{4}{\delta \log n} + \frac{1}{n^{1-\delta}}\right).
    \end{split}
\end{gather}
    Thus, $m^* = O\left(\frac{m}{k}\right)$. After \textsf{CPGC} finishes extracting cliques, there are $2n^{2-\delta/2}$ edges remaining in~$G$. Those remaining edges are trivial cliques (i.e., single edges) that are added to~$G^*$. The number of remaining edges $2n^{2-\delta/2}$ is in $O\left(\frac{m}{k}\right)$. Therefore, the number of edges in the compressed graph $G^*(U,W,Z,E^*)$ obtained by \textsf{CPGC} is $|E^*| = O\left(\frac{m}{k}\right)$.
    %Similarly, we can find that the remaining trivial edges $\hat{m} < 2n^{2-\delta/2}$ in $G$ that are added to $G^*$ is also bounded by $O(m/k)$. Thus, the number of edges $|E^*| = m^*$ in the compressed graph $G^*$ is $O(m/k)$.
\end{proof}

\vspace*{0.02cm}
\noindent {\textbf{Extension to non-bipartite graphs.}} In Appendix~\ref{sec:appendix:non-bip} we describe how \textsf{CPGC} can be extended to compress non-bipartite graphs. 


\section{Experimental Results}
\label{sec: experimental results}
% \subsection{Execution Time and Compression Ratio.}
% \label{subsec: Execution time and compression ratio}

We investigated the performance of \textsf{CPGC} in terms of both running time and compression ratio, and compared it with the \textsf{FM} algorithm proposed by Feder and Motwani~\cite{federMotwani}. We could not locate any existing implementation of the \textsf{FM} algorithm. Thus, we independently implemented it in C to the best of our ability. However, we encountered a limitation with the \textsf{FM} algorithm regarding large graphs, where the calculations for selecting vertices may exceed the machine representation limits. Consequently, we could only obtain results for bipartite graphs with up to $n = 128$ vertices in each bipartition and 16 thousand edges (i.e., small graphs).  
To ensure a fair comparison between algorithms, we excluded the time spent on extracting trivial cliques ($\hat{k} = 1$) when measuring the runtime of the \textsf{FM} algorithm. \textsf{CPGC} addresses this by excluding trivial cliques considering $\hat{k} > 1$, as indicated in Line 6 of Algorithm~\ref{alg:CPGC}.
Furthermore, we investigated the performance of our algorithm, \textsf{CPGC}, on large bipartite graphs with up to 32 thousand vertices in each bipartition and approximately 1.05 billion edges. Both \textsf{CPGC} and \textsf{FM} algorithms were implemented in C and experiments were conducted on a Linux system with an AMD EPYC-74F3 processor, 1 CPU core, 3.2GHz, and 128 GB of memory.
% \footnote{Codes implementing the algorithms are available at \textcolor{red}{\href{https://anonymous.4open.science/r/CPGC-Paper-DCFF}{https://anonymous.4open.science/r/CPGC-Paper-DCFF}}}
We used the GCC compiler (version 8.5.0) for compiling and executing the C code.
 \looseness=-1

\begin{figure*}[t]
  % \centering{
  \centerline{
  \includegraphics[width=0.9\textwidth]{figures/combined_plots.pdf}
  % }  
  }
  \caption{\textsf{CPGC}: Average compression ratio (top row: (a), (b), and (c)) and running time (bottom row: (d), (e), and (f)) for graphs with 3.36~million to 1.05~billion edges.} 
  %different~$\delta$~(0.5, 0.6, 0.7, 0.8, 0.9, and 1) and different densities (0.80, 0.90, and 0.98).}
 \label{fig:combined_cpgc_results}
 % \vspace{-6mm}
\end{figure*}


For our experiments, we generated bipartite graphs with various densities using the $G(n,p)$ random graph model adapted to bipartite graphs. In this model, an edge is included in the bipartite graph with probability $p$, independently from every other edge. We implemented a Python program to generate instances of bipartite graphs with the number of vertices $|U| = |W| = n$, where $n$ ranges from 32 to 32,768, and densities of 0.8, 0.85, 0.90, 0.95, and 0.98. This generated graphs with up to 1.05 billion edges, considering $p$ as a measure of the density of the generated bipartite graphs.
For each bipartite graph with a given density, we created 10 different instances. We then ran each instance for six different values of delta ($\delta$: 0.5, 0.6, 0.7, 0.8, 0.9, 1), presenting the average values and standard deviations of the running time and compression ratio for these 10 instances. 


\subsection{Results for Large Graphs.}
We present the results obtained from testing \textsf{CPGC} on large bipartite graphs, where each bipartition consists of~$n=2^i$ vertices, with~$i$ ranging from~11 to~15, and a number of edges~$m$ ranging from approximately 3.36~million to 1.05~billion. Figures~\ref{fig:combined_cpgc_results}a-\ref{fig:combined_cpgc_results}c show the average compression ratio, while Figures~\ref{fig:combined_cpgc_results}d-\ref{fig:combined_cpgc_results}f show the average running time for bipartite graphs. \looseness=-1


\vspace*{0.1cm}
\noindent {\bf Average compression ratio.} The compression ratio for \textsf{CPGC}  is calculated as $\frac{m}{m^*}$, where $m$ and $m^*$ represent the number of edges in the given and compressed graphs, respectively.
We observe that, for the same~$n$ and~$\delta$, the compression ratio increases with density. For instance, with $n = 2^{15}$ and $\delta = 0.5$, at a density of $0.80$ with $m = 214.75 $ million, the compression ratio is $1.95$. In contrast, at a density of $0.98$, with $m = 1.05 $ billion, the compression ratio increases to $2.44$. This is because the probability of finding common neighbours is correlated with the density,  
that is, increasing the density increases the probability of finding a large set of common neighbours of the right partition of the $\delta$-clique. Finding more common neighbours results in \textsf{CPGC} extracting more edges, and thus increasing the compression ratio.
However, when the density is kept constant, the compression ratio is not monotonic and depends on $\delta$ and $n$. For instance in Figure~\ref{fig:combined_cpgc_results}a, when $n = 2^{13}$ and $m = 53.69$ million for density~$0.80$, the compression ratio initially increases from~1.85, for $\delta = 0.5$, to~2, for~$\delta = 0.6$, achieving the maximum compression. Yet, further increasing~$\delta$ to~1 leads to a reduction in the compression ratio to~1.36. This is because increasing~$\delta$ increases the number of vertices in the right partition~$K_q$ of a~$\delta$-clique, as $|K_q| = \hat{k}$ and $\hat{k} \propto \delta$. Consequently, this decreases the probability of finding the common neighbors, and %which is \textbf{comparable to density$^{\hat{k}}$}, 
therefore for lower density, increasing~$\delta$ results in a decrease in the compression ratio. Conversely, as the density increases, the probability of discovering common neighbors also increases. Hence, at a density of~0.98, an increase in~$\delta$ correlates with an increase in the compression ratio for all~$n$ and~$m$ values (Figure~\ref{fig:combined_cpgc_results}c). 
It is important to note that the compression ratio depends on~$n$, $m$, and~$\delta$. Certain combinations of~$n$, $m$, and~$\delta$ lead to the highest compression ratio. We leave the investigation of finding the optimal combination of these parameters for future work.\looseness=-1




\begin{figure*}
% \vspace{-0.2cm}
     \centerline{
    \includegraphics[width=0.9\linewidth]{figures/relative_combined_plots.pdf}
    % \caption{Relative compression ratio on small graphs \\with $\delta =1$ and different densities.}
    \label{fig:relative_comparision}}
  %\end{subfigure}%
  % \hfill % maximize the horizontal separation
  % \begin{subfigure}{.47\textwidth}
  %   \includegraphics[width=\linewidth]{figures/relative_comparison_delta.pdf}
  %   % \caption{Relative speedup on small graphs with \\$\delta =1$ and different densities.}
  %   \label{fig:relative_speedup}
  % \end{subfigure}%
  % \hfill
  
  % \vspace{-0.2cm}
  \caption{\textsf{CPGC} vs.\ \textsf{FM}: Compression ratio relative to \textsf{FM} (top row: (a), (b) and (c)) and speedup relative to \textsf{FM} (bottom row: (d), (e) and (f)) for graphs with 819 to 16~thousand edges.} %different~$\delta$~(0.5, 0.6, 0.7, 0.8, 0.9, and 1) and different densities (0.80, 0.90, and 0.98).}
  \label{fig:relative_CR_speedup}
    % \vspace{-0.5cm}
\end{figure*}


\vspace*{0.1cm}
\noindent {\bf Average running time.} 
We observe that for the same~$\delta$ and density, the running time increases as the number of vertices increases which is expected (\textsf{CPGC} running time is $O(mn^{\delta})$). We also observe that for the same~$n$ and density, the running time decreases as~$\delta$ increases. For instance, with~$m = 214.75$ million for  density = 0.80 and $\delta = 0.5$, the running time is~176.27 seconds, whereas for~$\delta = 1$ it reduces to~117.8 seconds. This is because of the relation between~$\delta$ and~$\hat{k}$ described earlier, \textsf{CPGC} cannot find a large set of common neighbours which results in a lower compression ratio for lower density. Also, when \textsf{CPGC} does not find an edge with~$u_i \in U$ for any vertex $w_j \in K_q$, then it skips to the next vertex in~$U$ to check for an edge with~$w_j \in K_q$ thus, decreasing the running time. Therefore, we do not observe an increase in the running time as~$\delta$ increases. %$\delta$ as stated in the execution time of the algorithm. 
However, for density = 0.98, the running time decreases as~$\delta$ increases, even when the compression ratio increases. For example, with  $m = 1.05$ billion for density = 0.80 and $\delta = 0.5$, the running time is~164.83 seconds, whereas for~$\delta = 1$ the running time is~150.97 seconds. This is because for higher density and large~$\delta$, large $\delta$-cliques are extracted which decreases the total number of iterations performed by  \textsf{CPGC} and subsequently reduces the running time. \looseness=-1

\begin{figure*}[t!]
  % \centering{
  \vspace*{-0.3cm}
  \centerline{
  \includegraphics[width=0.9\linewidth]{figures/dinics_execution_time_with_density.pdf}
  % }  
  \vspace{-.3cm}
  }
  \caption{\textsf{Dinitz}($G$) vs.\ \textsf{Dinitz}($G^*$): Average running time of Dinitz's algorithm on the original bipartite graph $G$ (labeled Dinitz($G$)) and on the compressed graph $G^*$ (labeled Dinitz($G^*$)), for a large graph with approximately 32,000 vertices in each bipartition and 214.75 million to 1.05 billion edges corresponding to densities = $0.80, 0.90,$ and $0.98$ and different~$\delta$.}
 % \label{fig:k_m_cliques(5)}
 \label{fig:avg_execution_time}
 \vspace*{-0.5cm}
\end{figure*}



\subsection{Results for Small Graphs.}
In this section, we compare the performance of \textsf{CPGC} against \textsf{FM} on small graphs, particularly, on graphs with~$n = 2^i$ vertices in each bipartition, where~$i$ ranges from~5 to~7 and the number of edges ranging from 819 to 16,056. However, due to the limitations of \textsf{FM} as previously mentioned, our comparison is constrained to $n = 2^7$ and $m = 16,056$. We compare the performance of the two algorithms using two metrics, relative compression ratio, and  relative speedup. \looseness=-1






\vspace*{0.1cm}
\noindent {\bf Compression ratio relative to \textsf{FM}.} 
%The compression ratio is defined as $\frac{m}{m^{*}}$ where~$m$ is the number of edges in the original graph~$G$ and~$m^{*}$ is the number of edges in the compressed graph~$G^{*}$. 
We define the \emph{compression ratio relative to} \textsf{FM}, as the ratio of  compression ratio achieved by \textsf{CPGC} and \textsf{FM}. 
%quotient of the fraction of the compression ratio of \textsf{CPGC} over \textsf{FM}. 
Figures~\ref{fig:relative_CR_speedup}a-\ref{fig:relative_CR_speedup}c show the relative compression ratio relative to \textsf{FM} achieved by \textsf{CPGC} in the case of small graphs, for $\delta = 0.5, 0.6, 0.7, 0.8, 0.9, 1$, and densities = 0.80, 0.90, 0.98. In all cases, the relative compression ratio ranges from~$1$~to~$1.27$, highlighting that the compression ratio achieved by \textsf{CPGC} is at least as that of \textsf{FM}. \textsf{CPGC} achieves a  greater compression ratio than \textsf{FM} because it extracts multiple $\delta$-cliques before~$\hat{k}$ decreases as shown in Figure~\ref{fig:k_m_cliques(5)}, resulting in removing more edges at higher values of~$\hat{k}$ (Section~\ref{subsec:S-CPGC}). 
We observe that \textsf{CPGC} achieves a compression ratio equal to that of \textsf{FM} for a small graph with $n = 32$, $m = 819$, density = 0.80, and  $\delta = 0.5$. This is because a small graph does not require many iterations for extracting $\delta$-cliques, thereby not giving \textsf{CPGC} the opportunity to extract more cliques over the \textsf{FM} algorithm. However, for the same $n$ and $\delta$ when the graph density  increases the compression ratio achieved by \textsf{CPGC} also increases. For example, with $n = 32$,  $m = 921$, and $1,003$, the relative compression ratio is 1.03 and 1.25, respectively. %\looseness=-1
As explained before, certain combinations of $n$, $m$, and $\delta$ lead to the highest compression ratio. For instance, when $n = 128$, $m = 16,056$ for a density of 0.98, we observe that the relative compression ratio initially increases from 1.1 to 1.14 with increasing $\delta$ from 0.5 to 0.7. However, further increasing $\delta$ to 1 results in a decrease in the relative compression ratio to 1.1.  \looseness=-1

\vspace*{0.1cm}
\noindent {\bf Speedup relative to \textsf{FM}.} 
We define the \emph{speedup relative} to \textsf{FM} as the ratio of the runing time of \textsf{FM} over \textsf{CPGC}. Figures~\ref{fig:relative_CR_speedup}d-f illustrate the speedup achieved by \textsf{CPGC} for bipartite graphs with $n$ = 32, 64, and 128 vertices in each partition, $m$ = 819 to 16 thousand for densities of 0.80, 0.90, and 0.98, and $\delta$ values ranging from 0.5 to 1. The running time of \textsf{CPGC} is consistently lower than that of \textsf{FM}, resulting in speedup by \textsf{CPGC} across all cases. This is primarily due to the selection of vertices in the while loop (Lines 4-6) in \textsf{CSA}, facilitating faster compression by extracting more than one i.e., $\gamma$, $\delta$-cliques in a single iteration, as shown in Figure~\ref{fig:k_m_cliques(5)}. The speedup increases notably with higher $\delta$ and density. For example, with $n = 32$ and $m = 819$ for density 0.8, \textsf{CPGC} achieves an average speedup of~3.66 for $\delta = 0.5$, increasing to~16.28 for $\delta = 1$. Similarly, with $n = 32$ and $\delta = 1$, \textsf{CPGC} achieves an average speedup ranging from 16.28 for $m = 819$ with density 0.8 to 20.97 for $m = 16$ thousand with density 0.98. It is worth noting that for small graphs, certain combinations of $n$, $m$, and $\delta$ yield higher speedup values. For instance, with $n = 128$ and $m = 819$ for density 0.8, the speedup increases from 52.81 to 81.97 for $\delta = 0.8$, and then slightly decreases to 60.68 for $\delta = 1$.
\looseness=-1

\vspace{-2mm}
\subsection{Speeding-up Matching Algorithms.}
\label{subsec: Dinitz's algorithm}

Since the compressed representation of the original graph maintains all the path information, it can be used as input to other graph algorithms such as maximum cardinality matching, to speedup their execution. 
Consider Dinitz's algorithm~\cite{Dinitz:dans70} for finding the maximum cardinality matching in a bipartite graph, whose running time is in~$O(\sqrt{n} m)$.  According to Theorem~\ref{comp_ratio} the compression ratio of \textsf{CPGC} is %at least that of \textsf{FM}, which in the case of dense enough graphs is 
$k=\Omega\Big(\frac{\delta \log n}{\log(2n^2/{m})}\Big)$, where $0<\delta<1$. This is the same as the compression ratio achieved by \textsf{FM}~\cite{federMotwani}.
Because the number of edges in the compressed graph is reduced by a factor of $k$, using \textsf{CPGC} as preprocessing for Dinitz's algorithm leads to a total time of $O(\sqrt{n} m /k + mn^{\delta})$. For $\delta < 1/2$, the total running time becomes $O(\sqrt{n}m \log_n{(\frac{n^2}{m}}))$. To the best of our knowledge, this matches the best known asymptotic bound for maximum cardinality bipartite matching algorithms (also achieved by employing \textsf{FM} instead of \textsf{CPGC} and by the bipartite matching algorithm based on push-relabel method~\cite{Goldberg:siam-jdm97}).
Similar reasoning applies if instead of Dinitz's algorithm we consider the Hopcroft-Karp algorithm~\cite{Hopcroft:sjc73}. 
% To adapt the Dinitz algorithm for the compressed tripartite graph, we made modifications to the Dinitz's algorithm. In Appendix~\ref{sec:appendix:Dinitz_extension} we describe how \textsf{CPGC} can be extended to compress non-bipartite graphs. 
% Specifically, we introduced a condition for the additional vertices in the graph. This condition ensures that the flow from each additional vertex is less than or equal to the number of vertices connected to their right (i.e., $\hat{k}$ the size of right partition of the corresponding $\delta$-clique).
\looseness=-1




Figure~\ref{fig:avg_execution_time} shows the average running time of Dinitz's algorithm on the original bipartite graph~$G$ with~$n=32$ thousand vertices in each bipartition, and $m$ ranging from 214.75 million to 1.05 billion edges for densities = $0.80, 0.90,$ and $0.98$ and different $\delta$. It also shows %the sum of the average execution time of \textsf{CPGC} as a preprocessing algorithm, and 
the run time of Dinitz's algorithm executed on the compressed tripartite graph~$G^*$, for different values of~$\delta$, and densities $=0.80, 0.90,$ and $0.98$. We observe that in all cases the Dinitz algorithm applied to the compressed graph~$G^*$ outperforms its execution on the original graph~$G$, achieving a reduction in the running time %\textcolor{blue}{(Should this one be reduction in running time and not speedup!!)} 
of up to 72.83\%. Even when considering the preprocessing time required to compress the graph $G$, the running time of the Dinitz algorithm for large, dense graphs is still reduced by up to 25.09\% with the compressed graph $G^*$.
%\textcolor{blue}{(I am not sure if we also need to give one example for speedup including the preprocessing time. You can add one example.)} 
This reduction in the running time is directly proportional to the level of compression achieved, where larger compression ratios result in reduced running time.
For instance, as discussed in the context of large graphs (Figure \ref{fig:combined_cpgc_results}(a-c)), for a density of $0.98$, \textsf{CPGC} demonstrates an increasing compression ratio as the edge count $m$ reaches 1.05 billion with increasing values of $\delta$. Consequently, the run time of Dinitz's algorithm on the compressed graph $G^*$ also decreases. \looseness=-1

\vspace{-2mm}
\section{Conclusion and Future Work}
\label{conclusion}

We proposed a new algorithm for graph compression based on partitioning the graph into
bipartite cliques. The proposed algorithm runs in time $O(mn^\delta)$ which improves over the $O(mn^\delta \log^2 n)$
time of the Feder-Motwani (\textsf{FM}) algorithm (the best existing clique partitioning-based graph compression algorithm).
Our algorithm achieves an average compression ratio at least as good as that of \textsf{FM}. 
%We performed a comprehensive experimental analysis of our algorithm and compared it with the \textsf{FM} algorithm.
Experimental results showed that our algorithm achieved a compression ratio of up to 26\% greater than that
obtained by \textsf{FM} and executes up to~105.18 times faster than \textsf{FM}. 
We also investigated the reduction in running time of a cardinality matching algorithm (Dinitz's algorithm) when
using the compressed graph instead of the original graph.
%speedup obtained by using \textsf{CPGC} as a preprocessing step for a cardinality matching algorithm (Dinitz's algorithm). 
%The results showed that for sufficient large dense graphs, using \textsf{CPGC} as a preprocessing step leads to a speedup \textcolor{red}{speedup??} of up to~368.12\% over the matching algorithm without using \textsf{CPGC}.
The results showed that for sufficient large dense graphs, using the compressed graph obtained by \textsf{CPGC} as an input to the matching algorithm leads to a reduction in the running time of up to~72.83\% over the matching algorithm using the original graph.
In future work, we plan to continue investigating the graph compression problem and propose algorithms that improve over the compression ratio obtained by our proposed algorithm. \looseness=-1

\newpage
\bibliography{reference}
\input{appendix.tex}

% \begin{thebibliography}{99}

% %\bibitem{GUIDE}
% %R.~E. Bank, {\em PLTMG  users' guide, edition 5.0}, tech. report,
% %  Department of Mathematics, University of California, San Diego, CA, 1988.

% %\bibitem{HBMG}
% %R.~E. Bank, T.~F. Dupont, and H.~Yserentant, {\em The hierarchical basis
% %  multigrid method}, Numer. Math., 52 (1988), pp.~427--458.

% \bibitem{BANKSMITH}
% R.~E. Bank and R.~K. Smith, {\em General sparse elimination requires no
%   permanent integer storage}, SIAM J. Sci. Stat. Comput., 8 (1987),
%   pp.~574--584.

% \bibitem{EISENSTAT}
% S.~C. Eisenstat, M.~C. Gursky, M.~Schultz, and A.~Sherman, {\em
%   Algorithms and data structures for sparse symmetric gaussian elimination},
%   SIAM J. Sci. Stat. Comput., 2 (1982), pp.~225--237.

% \bibitem{GEORGELIU}
% A.~George and J.~Liu, {\em Computer Solution of Large Sparse Positive
%   Definite Systems}, Prentice Hall, Englewood Cliffs, NJ, 1981.

% \bibitem{LAW}
% K.~H. Law and S.~J. Fenves, {\em A node addition model for symbolic
%   factorization}, ACM TOMS, 12 (1986), pp.~37--50.

% \bibitem{LIU}
% J.~W.~H. Liu, {\em A compact row storage scheme for cholesky factors
%   using elimination trees}, ACM TOMS, 12 (1986), pp.~127--148.

% \bibitem{LIU2}
% \sameauthor , {\em The role of
%   elimination trees in sparse factorization}, Tech. Report CS-87-12,Department
%   of Computer Science, York University, Ontario, Canada, 1987.

% \bibitem{ROSE72}
% D.~J. Rose, {\em A graph theoretic study of the numeric solution of
%   sparse positive definite systems}, in Graph Theory and Computing, Academic  Press, New
% York, 1972.

% \bibitem{ROSE76}
% D.~J. Rose, R.~E. Tarjan, and G.~S. Lueker, {\em Algorithmic aspects of
%   vertex elimination on graphs}, SIAM J. Comput., 5 (1976), pp.~226--283.

% \bibitem{ROSEWHITTEN}
% D.~J. Rose and G.~F. Whitten, {\em A recursive analysis of disection
%   strategies}, in Sparse Matrix Computations, Academic Press, New York, 1976.

% \bibitem{SCHREIBER}
% R.~Schrieber, {\em A new implementation of sparse gaussian elimination},
%   ACM TOMS, 8 (1982), pp.~256--276.

% \end{thebibliography}
\end{document}

% End of ltexpprt.tex 