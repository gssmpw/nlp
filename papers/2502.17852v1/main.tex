%% 
%% Copyright 2019-2024 Elsevier Ltd
%% 
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3c of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3c or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for cas-sc documentclass for 
%% double column output.

% \documentclass[a4paper,fleqn]{cas-sc}
\documentclass[times, review, 10pt]{elsarticle}

\usepackage{booktabs}
\usepackage{tabularx} 

% If the frontmatter runs over more than one page
% use the longmktitle option.

%\documentclass[a4paper,fleqn,longmktitle]{cas-sc}

%\usepackage[numbers]{natbib}
%\usepackage[authoryear]{natbib}
\usepackage[numbers]{natbib}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{cleveref}
\hypersetup{pdfauthor=author}  % [------litingw]: use this to fix warning 'Package hyperref Warning: Token not allowed in a PDF string'

\crefname{figure}{Fig.}{Figs.}
\usepackage{subcaption}


%%%Author macros
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
\tsc{EP}
\tsc{PMS}
\tsc{BEC}
\tsc{DE}
%%%


% Uncomment and use as if needed
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newdefinition{rmk}{Remark}
%\newproof{pf}{Proof}
%\newproof{pot}{Proof of Theorem \ref{thm}}

\begin{document}

\begin{frontmatter}

\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}
% \shorttitle{Sketch-1-to-3}
% \shortauthors{Liting Wen et~al.}

% Main title of the paper
\title{Sketch-1-to-3: One Single Sketch to 3D Detailed Face Reconstruction}


% First author
%
% Options: Use if required
% eg: \author[1,3]{Author Name}[type=editor,
%       style=chinese,
%       auid=000,
%       bioid=1,
%       prefix=Sir,
%       orcid=0000-0000-0000-0000,
%       facebook=<facebook id>,
%       twitter=<twitter id>,
%       linkedin=<linkedin id>,
%       gplus=<gplus id>]

% Corresponding author indication
%\cormark[1]
% Footnote of the first author
%\fnmark[1]

\author[1]{Liting Wen}
% \ead{litingw@andrew.cmu.edu}

%\address[1]{, Street 129, 1043 NX Amsterdam, The Netherlands}
\affiliation[1]{organization={School of Computer Science, Carnegie Mellon University},
                city={Pittsburgh},
%               citysep={}, % Uncomment if no comma needed between city and postcode
                state={PA},
                postcode={15213}, 
                country={USA}}
% eg: \credit{Conceptualization of this study, Methodology, Software}
% \credit{Conceptualization, Methodology, Writing – original draft}

\author[2]{Zimo Yang}
% \ead{zimo002@e.ntu.edu.sg}

\affiliation[2]{organization={College of Computing and Data Science, Nanyang Technological University},
                postcode={639798}, 
                country={Singapore}}
% \credit{Dataset Preparation, Software, Writing – original draft}

\author[3]{Xianlin Zhang\corref{cor1}}
\ead{zxlin@bupt.edu.cn}
% \credit{Writing – review \& editing, Funding acquisition, Supervision}

\author[3]{Chi Ding}
% \ead{charmingchi@bupt.edu.cn}
% \credit{Visualization, Validation}

\author[3]{Yue Zhang}
% \ead{zhangyuereal@163.com}
% \credit{Writing – review \& editing, Investigation}

\affiliation[3]{organization={School of Digital Media and Design Arts, Beijing University of Posts and Telecommunications},
                city={Beijing},
                postcode={102206}, 
                country={China}}

\author[4]{Mingdao Wang}
% \ead{wmingdao@tsinghua.edu.cn}
\affiliation[4]{organization={Tsinghua University},
                city={Beijing},
                postcode={100084}, 
                country={China}}
% \credit{Validation, Investigation}

\author[3]{Xueming Li}
% \ead{lixm@bupt.edu.cn}
% \credit{Writing – review \& editing, Investigation}

% Corresponding author text
\cortext[cor1]{Corresponding author}

% For a title note without a number/mark
%\nonumnote{}
% Here goes the abstract
\begin{abstract}
3D face reconstruction from a single sketch is a critical yet underexplored task with significant practical applications. The primary challenges stem from the substantial modality gap between 2D sketches and 3D facial structures, including: (1) accurately extracting facial keypoints from 2D sketches; (2) preserving diverse facial expressions and fine-grained texture details; and (3) training a high-performing model with limited data. In this paper, we propose Sketch-1-to-3, a novel framework for realistic 3D face reconstruction from a single sketch, to address these challenges. Specifically, we first introduce the \textbf{G}eometric \textbf{C}ontour and \textbf{T}exture \textbf{D}etail (GCTD) module, which enhances the extraction of geometric contours and texture details from facial sketches. Additionally, we design a deep learning architecture with a domain adaptation module and a tailored loss function to align sketches with the 3D facial space, enabling high-fidelity expression and texture reconstruction. To facilitate evaluation and further research, we construct SketchFaces, a real hand-drawn facial sketch dataset, and Syn-SketchFaces, a synthetic facial sketch dataset. Extensive experiments demonstrate that Sketch-1-to-3 achieves state-of-the-art performance in sketch-based 3D face reconstruction.
\end{abstract}

% Use if graphical abstract is present
%\begin{graphicalabstract}
%\includegraphics{}
%\end{graphicalabstract}

% Research highlights
%\begin{highlights}
%\item Feature enhancement improves the capture of facial contours and details.
%\item Geometric priors and two-stage loss enable reconstruction without 3D supervision.
%\item Domain adaptation aligns sketches with 3D space for realistic outputs.
%\item New datasets SketchFaces and Syn-SketchFaces fill the gap in face sketch resources.
%\end{highlights}


% Keywords
% Each keyword is seperated by \sep
\begin{keyword}
Sketch-based modeling \sep 3D face reconstruction \sep High-fidelity \sep Single sketch \sep Face sketch dataset
\end{keyword}

\end{frontmatter}

% \maketitle

% Main text

% Numbered list
% Use the style of numbering in square brackets.
% If nothing is used, default style will be taken.
%\begin{enumerate}[a)]
%\item 
%\item 
%\item 
%\end{enumerate}  

% Unnumbered list
%\begin{itemize}
%\item 
%\item 
%\item 
%\end{itemize}  

% Description list
%\begin{description}
%\item[]
%\item[] 
%\item[] 
%\end{description}  

%\clearpage %%Remove this from your manuscript

% Uncomment and use as the case may be
%\begin{theorem} 
%\end{theorem}

% Uncomment and use as the case may be
%\begin{lemma} 
%\end{lemma}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

\section{Introduction}

3D face reconstruction has attracted considerable attention due to its wide-ranging applications, including expression analysis ~\cite{mao2024poster++}, animation creation ~\cite{wu2023audio}, and face recognition ~\cite{wang2025implicit}. While most existing research has focused on image-based 3D face reconstruction, sketch-based 3D face reconstruction poses a unique set of challenges, particularly concerning the accuracy of information conveyed by the artist. In this context, "accuracy" refers to a balance between completeness and restraint—faithfully representing the intended features without introducing misleading embellishments or omissions. Achieving such accuracy necessitates not only an accurate depiction of geometric contours but also a precise capture of fine details in the reconstructed 3D model, despite the inherently lower information content of sketches compared to real images.


Simply combining the sketch-to-photo and photo-to-3D approaches to achieve sketch-to-3D purpose introduces a significant amount of misleading information that is not inherently associated with the original sketch, thereby impeding a faithful reconstruction of the artist's original intent, as illustrated in \hyperref[fig:sketch-2D-3D]{Fig~\ref{fig:sketch-2D-3D}}. Thus, a dedicated sketch-to-3D face reconstruction framework is necessary, one that reduces misleading information while preserving essential features to achieve an authentic representation.



Several previous works have made attempts in the field of sketch-based 3D face reconstruction. ~\cite{han2017deepsketch2face} and ~\cite{luo2023sketchmetaface} primarily address the effects of art and design on facial features and reveal shortcomings in detail and precision with the output of 3D facial caricatures and exaggerated cartoon characters. ~\cite{lin2023sketchfacenerf} deriving from generative techniques, tends to generate an excess of information that lacks relevance to the input sketch. The facial detail reconstruction performance of ~\cite{Alpher22} is unsatisfactory, and the model training encounters limitations due to the requirement of both sketches and the corresponding photos as training data.


The task of reconstructing 3D faces from 2D sketches is challenging mainly due to: 1) unlike the rich information of 2D face images, the sketch has inherent uncertainty, and it’s difficult to obtain keypoints from a single sparse 2D hand-drawn sketch; 2) 3D faces with high fidelity vivid facial expressions and geometric details are difficult to generate as the abstract and concise inherent attributes of sketches; 3) the facial sketch dataset required for model training is scarce. 
To address the issues mentioned above, we propose a novel paradigm of Sketch-1-to-3 deep framework for reconstructing 3D faces from a single sketch. Sketch-1-to-3 can accurately extract facial contours and geometric details from hand-drawn sketches, and then reconstruct the high-fidelity 3D face. 
We also present a real hand-drawn sketch dataset SketchFaces and a synthetic dataset Syn-SketchFaces. SketchFaces comprises facial sketches of various drawing styles, and the creation methodology of Syn-SketchFaces has undergone careful comparison and selection. 
%----------------Fig-------------------
\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{Figure_1-sketch-2D-3D.png}
	\caption{Challenge demonstration. We employ \cite{lin2023sketchfacenerf} to transform the input sketch into the photo and then use \cite{feng2021learning} to generate the 3D face from the obtained photo, as elucidated by the yellow line.  The green line signifies our method. It is apparent that employing the circuitous sketch-photo-3D (yellow line) method may give rise to reconstruction bias.}
	\label{fig:sketch-2D-3D}
\end{figure*}
%-------------------------------------------------------------------------

In summary, our main contributions are:
\begin{quote}
$\bullet$ We propose an end-to-end framework, Sketch-1-to-3, for high-fidelity 3D face reconstruction from a single sketch. Our method is the first to explicitly address the precise transmission of information within a single sketch, enabling the reconstruction of a 3D face model with both accurate contours and fine-grained details.

$\bullet$ We introduce a novel enhancement \textbf{G}eometric \textbf{C}ontour and \textbf{T}exture \textbf{D}etail (GCTD) module, which effectively extracts accurate geometric structures and fine details from sketches. We further design a domain adaption module and a task-specific loss function to seamlessly align 2D sketches with 3D space, ensuring expressive and detail-preserving 3D face reconstruction.

$\bullet$ To address the critical data scarcity challenge in this domain, we introduce two comprehensive datasets: SketchFaces, containing real hand-drawn facial sketches, and Syn-SketchFaces, a large-scale synthetic sketch face dataset. These datasets can significantly advance research in sketch-based 3D face learning.

$\bullet$ Extensive experiments demonstrate that Sketch-1-to-3 achieves state-of-the-art result in sketch-based 3D face reconstruction compared with previous methods.

\end{quote}


\section{Related work}
%-------------------------------------------------------------------------
\subsection{Monocular 3D Face Reconstruction} 
Reconstructing 3D faces from a single 2D image is an inherently ill-posed problem, requiring supplementary prior knowledge to constrain the reconstruction process. A widely adopted approach is to estimate the parameters of a 3D Morphable Model (3DMM). Existing methods can be broadly categorized into optimization-based and learning-based techniques. Optimization-based methods, such as the work of Jo et al. \cite{jo2015single}, propose an reconstruction approach that integrates a simplified 3D Morphable Model with Structure-from-Motion to improve robustness against pose variations. By leveraging bilateral symmetry to recover occluded facial feature points, their method enhances reconstruction accuracy while addressing the limitations of model-based and SfM-based approaches under large pose variations. For learning-based methods, Dou et al. \cite{dou2018monocular} designed a coupled-dictionary learning framework, which estimates a sparse 3D shape from 2D landmarks and refines it via super-resolution. Recent advancements by Feng et al. \cite{feng2021learning} further enhance reconstruction quality by introducing a novel approach that robustly generates detailed 3D face models, disentangling person-specific facial details from expression-dependent variables. Dinesh et al. \cite{dinesh2022fully} proposed a method for partial face reconstruction, leveraging a Fully Convolutional Network for spatial feature extraction and Deep Stacked Denoising Sparse Autoencoders for refinement. Morales et al. propose BabyNet \cite{morales2023babynet}, combining a 3D graph convolutional autoencoder with a 2D encoder via transfer learning. Zhu et al. \cite{zhu2025driving} integrates inpainting and reconstruction in a mutually reinforcing manner, enabling high-fidelity 3D face reconstruction from largely masked images.

A significant challenge for learning-based methods is the scarcity of ground truth 2D-3D paired data. To address this, researchers have explored the use of synthetic data for training models. In the work by Sela et al. \cite{sela2017unrestricted}, the authors propose an unrestricted approach to facial geometry reconstruction using an Image-to-Image translation network. The network is trained exclusively on synthetic data, which includes a wide range of facial identities, poses, expressions, lighting conditions, and material parameters. Despite the synthetic nature of the training data, the network demonstrates the ability to generalize to real-world facial images, producing detailed and accurate reconstructions. Inspired by these advancements, we propose a novel method for reconstructing detailed 3D face models from single free-hand sketches. Unlike prior works, our approach avoids the need for 3D face data during training, relying solely on sketch inputs. This enables direct extraction of facial information and precise reconstruction of 3D faces with intricate details, offering a practical solution for sketch-based 3D modeling.
%-------------------------------------------------------------------------
\subsection{Sketch-based 2D Facial Image Generation} 
Converting free-hand sketches to 2D faces is currently under extensive exploration. Jo et al. ~\cite{Alpher10} proposed SC-FEGAN, which generated images when users provided free-form masks, sketches, and colors as input. Fang et al. ~\cite{Alpher11}proposed an identity-aware CycleGAN~\cite{zhu2017unpaired} model that applied a new perception loss to supervise an image generation network.
By focusing more on the synthesis of key facial regions crucial for identity recognition, CycleGAN's photo sketch synthesis has been improved. There have been also attempts to utilize StyleGAN~\cite{karras2019style} which significantly improved performance by introducing novel technologies such as progressive generation and style-based architecture~\cite{Alpher14}. 

Different from traditional GANs, Chen et al. ~\cite{Alpher15} implicitly modeled the shape space of plausible face images and synthesized a face image in this space to approximate an input sketch. Li et al. ~\cite{Alpher16} proposed DeepFacePencil, which is able to generate photo-realistic face images from hand-drawn sketches, based on a novel dual generator image translation network during training. Yang et al. ~\cite{Alpher18} proposed a novel controllable sketch-to-image translation framework, which can allow users to flexibly define how the final output through refinement level control parameters. The above research focuses on the generation of 2D images from 2D sketches, and there is no research that directly considers the generation of 3D high-fidelity faces from abstract sketches.

\subsection{Sketch-based 3D Face Reconstruction} 
Currently, there are only few methods for reconstructing 3D faces directly from hand-drawn sketches. 
Han et al. proposed DeepSketch2Face~\cite{han2017deepsketch2face}, a fast, interactive, deep learning based 3D face and comic modeling sketch system. By simply sketching facial images, corresponding 3D facial models can be quickly generated, and facial contours and expressions can be simultaneously fitted. However, due to its low-cost interactive 3D facial modeling method, the generated model is so cartoonish, exaggerated, and lacks details that it cannot complete the task of generating detailed 3D faces from free-hand sketches.

Lin et al. proposed SketchFaceNeRF ~\cite{lin2023sketchfacenerf}, a 3D facial reconstruction method based on NeRF. However, the NeRF method consumed relatively high computational resources and training time. What’s more, it was not possible to obtain a complete 3D face model based solely on one sketch, but can only obtain a face model within a limited perspective range. Yang et al. proposed a method of learning 3D facial reconstruction from a single sketch ~\cite{Alpher22}. The method can be divided into two stages. In the first stage, information from photos and sketches was used to obtain rough reconstruction results, while in the second stage, the depicted lines were further used to refine the results. However, the facial model reconstructed by this method was not perfect and had defects in details. Therefore, we propose our reconstruction strategy to handle different facial regions and detail types.
%-------------------------------------------------------------------------



%-------------------------------------------------------------------------
%-------------------------------------------------------------------------

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{Figure_2-result.png}
    \caption{Reconstruction results of Sketch-1-to-3 based on our sketch dataset. Sketch-1-to-3 reconstructs 3D face from a single sketch. The first row represents the input sketch, the second row illustrates the detailed 3D face reconstruction.}
   \label{fig:first}
\end{figure*}
%-------------------------------------------------------------------------

\section{Method}
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
Our goal is to reconstruct the 3D face shape from a sketch vividly and precisely. We find that face sketches drawn by individuals typically exhibit two characteristics: firstly, the facial composition tends to be unrestrained, not necessarily conforming to the symmetry found in real faces; secondly, each stroke in the sketch is often crucial for conveying a lifelike representation of the face, as depicted aspects usually represent the features that individuals wish to emphasize during the sketching process. Based on these findings, we propose a sketch-to-3D detailed face reconstruction method with a novel feature enhancement module, aiming to facilitate the recognition of sketches while accentuating finer details embedded in sketches. Notably, our method exclusively relies on 2D training data, thus circumventing the limitations associated with the lack of 3D training data. As shown in \hyperref[fig:first]{Fig~\ref{fig:first}}, leveraging the sketch dataset, our model achieves highly accurate and realistic sketch-to-3D face reconstruction results.
%--------------------------------------
\par
\subsection{Proposed Architecture}

The architecture of our model, illustrated in \hyperref[fig:framework]{Fig~\ref{fig:framework}}, is designed to achieve precise and expressive 3D reconstruction from sketches through a two-stage training strategy that balances coarse structural understanding and fine detail refinement. The model pipeline begins with a novel feature enhancement module of GCTD (\textbf{G}eometric \textbf{C}ontour and \textbf{T}exture \textbf{D}etail), which is applied across both training stages to aid in the effective extraction of contours and facial details. In the first training stage, we introduce an independent shape encoder, denoted \(E_{s}\), which we find crucial for extracting the overall facial representation in sketch-to-3D reconstruction. Alongside \(E_{s}\), we also use a coarse encoder \(E_{c}\) \cite{feng2021learning}, which is fixed during the training process. The coarse encoder \(E_{c}\) extracts features from the input sketch and then regresses parameters \(\bm{\theta}\), \(\bm{\psi}\), \(\bm{l}\), \(\bm{c}\), \(\bm{\alpha}\). Concurrently, the shape encoder \(E_{s}\) regresses parameters \(\bm{\beta}\), and the rough 3D face shape is reconstructed. 
Then, in the second training stage, the model further refines the 3D reconstruction by employing a detail encoder, \(E_{d}\), which generates a latent code \(\bm{\delta}\) that correlates with the displacement map, adding finer structural details. All three encoders share a ResNet-50 backbone architecture\cite{he2016deep} to facilitate robust feature extraction across stages.
\par
In summary, our two-stage architecture integrates diverse components to achieve a stable and expressive 3D reconstruction process. By leveraging separate encoders tailored to coarse shape and fine detail, our model effectively addresses the challenges associated with sketch-to-3D translation, producing reconstructions that are both structurally accurate and rich in detail.
%--------------------------------------
\par
\subsection{Preliminaries}
Due to the fact that reconstructing 3D face from single sketch is ill-posed, we combine the prior knowledge from FLAME \cite{li2017learning}, a 3D head model that separates the representation of identity shape $\bm{\beta}  \in \mathbb{R} ^{\left | \bm{\beta} \right | } $, pose parameters $\bm{\theta}  \in \mathbb{R} ^{3k+3} $, and expression components $\bm{\psi}  \in \mathbb{R} ^{\left | \bm{\psi} \right | } $. It can be defined as:
\begin{eqnarray}
M\left ( \bm{\beta},\bm{\theta} ,\bm{\psi}  \right ) :\mathbb{R} ^{\left | \bm{\beta} \right |\times \left | \bm{\theta} \right | \times \left | \bm{\psi} \right |} \to \mathbb{R}^{3N},
\end{eqnarray}
where N denotes 5023 vertices. We estimate 3.1 FLAME parameters $\bm{\beta}  \in \mathbb{R} ^{100} $, $\bm{\theta}  \in \mathbb{R} ^{6} $, $\bm{\psi}  \in \mathbb{R} ^{50} $, albedo parameters $\bm{\alpha}  \in \mathbb{R} ^{50} $, Spherical Harmonics (SH) lighting $\bm{l}  \in \mathbb{R} ^{27} $, and camera model $\bm{c}  \in \mathbb{R} ^{3} $ for each input sketch. 

Note that $\bm{\alpha}$ helps gain FLAME’s texture map $A\left ( \bm{\alpha} \right ) \in \mathbb{R} ^{d\times d\times 3}$ using an appearance model, which is converted from Basel Face Model’s linear albedo space to the FLAME UV layout. Camera information $\bm{c}$ helps project 3D vertices to the 2D space, consisting of isotropic scale $s  \in \mathbb{R} $ and 2D translation $t  \in \mathbb{R}^{2} $.

Using the differentiable renderer \(R\) \cite{genova2018unsupervised}, we can obtain the rendering result as
\begin{equation}
R\left ( M,\bm{\alpha} ,\bm{l},\bm{c} \right ) \to I_{g},
\end{equation}
where \(I_{g} \) denotes the 2D sketch generated from the 3D shape.


%----------------Fig-Framework------------------
\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{Figure_3-frame.png}
	%\includegraphics[width=0.8\linewidth]{egfigure.eps}
	%\includegraphics[width=\linewidth]{your_image.png}
	\caption{An overview of our work. Given an input of a single sketch, it undergoes initial processing through our novel feature enhancement module in order to reinforce the existing facial features within the sketch, effectively ensuring the robustness of subsequent reconstruction outcomes. In the coarse training stage, we employ \(E_{c}\) and \(E_{s}\) to regress coarse shape coefficients. Subsequently, we integrate FLAME to generate a 3D coarse shape, followed by utilizing renderer to obtain \(I_{g}\). We thus compute the loss \(L_{S}\) using \(I_{g}\) and the input. The workflow of the detail training stage is similar.}
	\label{fig:framework}
\end{figure*}
%-------------------------------------------------------------------------
\subsection{Enhancement Module of GCTD}
The feature enhancement module of GCTD (\textbf{G}eometric \textbf{C}ontour and \textbf{T}exture \textbf{D}etail) is primarily employed to facilitate better detection of the facial contours in the sketch and contribute to a more refined representation of facial details. This non-learning based module mainly relies on digital image processing technology to efficiently solve the problem of unclear facial features in sketches.

Firstly, the input image is subjected to noise reduction processing, mainly through filtering methods to mitigate the noise of the image. The process is defined as
\begin{equation}
	g\left ( x,y \right ) = \sum\limits_{a,b}^{} f\left ( x+a,y+b \right )h\left ( a,b \right ),
\end{equation}
where $f\left ( x,y \right )$ represents the processed pixels in the image, $\left ( a,b \right )$ represents the size of the filter, and the processed pixel $g\left ( x,y \right )$ is obtained after filtering. Subsequently, the sketch is further enhanced by balancing the image to make it clearer. The process is defined as
\begin{equation}
	s = T\left ( r \right ) = \int_{0}^{r} p\left ( r \right )dr,
\end{equation}
where r represents the original image grayscale, s represents the transformed image grayscale, and $p\left ( r \right )$ is the probability of the occurrence of r. The sketches have more distinct features after being processed by the GCTD module, which is designed with the explicit intention of not producing a realistic image as an intermediate state. The sketches with enhanced features are used for the later facial contour detection, providing a relatively more economical alternative in terms of computational cost.
%-------------------------------------------------------------------------
\subsection{Domain Adaptation}
Despite our efforts to enrich the training dataset with real hand-drawn sketches—such as the CUFSF dataset \cite{zhang2011coupled} and our self-collected facial sketch dataset, SketchFaces—the limited availability of large-scale, high-quality facial sketch datasets necessitates the extensive use of synthetic data. To this end, we introduce Syn-SketchFaces, a synthetic dataset designed to augment the diversity and quantity of training samples. However, the inherent differences between synthetic sketches and real hand-drawn sketches can lead to a domain shift, where the model trained on synthetic data may underperform when applied to real-world inputs. To mitigate this issue, we incorporate the domain adaptation operation into our approach. With MixStyle \cite{zhou2023mixstyle}, we amalgamate feature statistics from both the synthetic dataset Syn-SketchFaces and the sketch dataset SketchFaces based on the inherent connection between image style and feature statistics \cite{huang2017arbitrary}. This amalgamation effectively bridges the gap between synthetic and real sketches, leading to a substantial improvement in our model's performance on sketch inputs.

%------------------------------------------------------------------------
\subsection{Loss Function}
\noindent{\bf Weighted Landmark Loss: }This Landmark loss calculates the correspondence of landmarks by compelling the projection of the 3D landmarks to align with the ground-truth landmarks of the sketch.
\begin{equation}
	L_{lmk}=\sum_{i=1}^{68}\omega_{i} \left \| k_{i}-s\Pi \left ( M_{i} \right ) +t \right \|_{1},
\end{equation}
where \(\Pi\in\mathbb{R}^{2\times3}\) refers to the projection matrix in the process of projecting 3D mesh into the 2D space, and \(M_{i}\in\mathbb{R}^{3}\) is the corresponding vertex of FLAME model M. Note that landmarks in distinct facial regions have been assigned diverse weights \(\omega_{i}\) in our method, enabling a more accurate reconstruction of the 3D face. Specifically, the utmost importance was given to the jawline and the inner mouth, followed by relatively lower weights for the corners of the nose and mouth. The remaining facial areas were allocated the minimal weights in our arrangement.
%--------------------------------------
\par
\noindent{\bf Mutual Distance Loss: }Compared to \(L_{lmk}\), the mutual distance loss is less susceptible to misalignment during the reconstruction process and can especially facilitate a more effective capture of expressions and shapes. Simply put, the mutual distance between keypoint pair \(P\) can be defined as
\begin{equation}
	L_{p}=\sum_{\left ( i,j \right ) \in P}^{}\left \| k_{i}-k_{j}-s\Pi \left ( M_{i}-M_{j} \right )  \right \|_{1},
\end{equation}
where \(P\) is a set of keypoint pairs, \(k_{i}\in\mathbb{R}^{2}\) and \(k_{j}\in\mathbb{R}^{2}\) refer to corresponding ground-truth 2D landmarks. Based on this definition, we opt for three categories of keypoint pairs to calculate our mutual distance loss, including keypoint pairs of eyes, inner mouth, and facial contours. To sum up, the loss computes as
\begin{equation}
	L_{md}=\omega_{eyeP}L_{eyeP}+\omega_{mouP}L_{mouP}+\omega_{conP}L_{conP},
\end{equation}
where \(\omega_{x}\) denotes the respective weight.
%--------------------------------------
\par
\noindent{\bf Photometric Loss: }We use a photometric loss measuring differences between the ground-truth sketch \(I\) and the generated image \(I_{g}\).
\begin{equation}
	L_{pho}=\left \|V_{I}\odot \left ( I-I_{g}\right ) \right \|_{1,1},
\end{equation}
where \(V_{1}\) is a face mask, and the operator \(\odot\) refers to the Hadamard product. Note that this kind of loss is calculated in both the coarse and detail stages to ensure photometric consistency. We designate them as \(L_{phoS}\) and \(L_{phoD}\), respectively, to make a distinction.

%--------------------------------------
\par
\noindent{\bf Regularization: }
For the coefficients of the coarse and the detail stages, we employ their L2 regularization terms as regularization losses, denoted as \(L_{regS}\) and \(L_{regD}\), respectively.
%--------------------------------------
\par
\noindent{\bf Overall Loss Functions: }
Overall, in coarse training stage, our optimization goal is
\begin{equation}
	L_{S}=L_{lmk}+L_{md}+\omega_{pho}L_{phoS}+\omega_{reg}L_{regS}.
\end{equation}
In detail training stage, our optimization goal is
\begin{equation}
	L_{D}=L_{md}+\omega_{pho}L_{phoD}+\omega_{reg}L_{regD}.
\end{equation}
The weights of \(L_{pho}\) and \(L_{reg}\) are denoted as \(\omega_{pho}\) and \(\omega_{reg}\), respectively.

\section{Experiment}
%-------------------------------------------------------------------------

\subsection{Dataset}
There are currently very few facial sketch datasets available, and existing relevant datasets, such as \cite{zhang2011coupled}, can not suffice for training robust sketch-to-3D face reconstruction models because of their limited scales. To optimize the performance of our model, we have collected a real hand-drawn sketch dataset, identified as SketchFaces. Moreover, considering there is a significant cost associated with gathering real hand-drawn sketches, we propose a novel approach to generating the synthetic sketch dataset from facial photos. Given the relative abundance of facial photo datasets, we thus obtained a synthetic dataset named Syn-SketchFaces that played a crucial role in the training phase. Additionally, we use the methods proposed by \cite{bulat2017far, zou2025towards} to obtain facial landmarks and \cite{nirkin2018face} for face segmentation, thereby labeling our sketches.
\par
\vspace{\baselineskip}
\noindent{\bf SketchFaces: }
We gathered real hand-drawn facial sketches from individuals with varying levels of artistic proficiency and diverse sources on the internet, thereby ensuring a diversity of sketch styles and drawing mediums. See \hyperref[fig:dataset]{Fig~\ref{fig:dataset}}(b). We created our dataset SketchFaces with these sketches, which is used as part of the training and testing datasets to verify the performance of our model.
\par
\vspace{\baselineskip}
\noindent{\bf Syn-SketchFaces: }After selection and comparison, we choose PiDiNet \cite{su2021pixel} to capture gradient information from facial images in SCUT-FBP5500 \cite{liang2018scut} and CelebAMask-HQ \cite{lee2020maskgan} and simulate a sketch-like visual result. See \hyperref[fig:dataset]{Fig~\ref{fig:dataset}} (a). First, the original facial images undergo edge detection using PiDiNet to obtain the images in the second row. Then, through color inversion, the images required for the facial sketch dataset can be obtained. PiDiNet adopts a novel pixel difference convolution (PDC) that integrates traditional edge detection operators into popular convolution operations in modern CNN to enhance task performance. The facial sketch dataset generated provides more accurate facial features by using PiDiNet for contour extraction. The generated facial sketch dataset retains both the distinct facial outline and crucial facial details, facilitating their utilization in our training process. 


%----------------Fig-Dataset------------------
\begin{figure*}[ht]
  \centering
  \begin{minipage}[b]{0.5\linewidth}
    \includegraphics[width=\linewidth]{Figure_4a-dataset.png}
    \subcaption{}
  \end{minipage}
  \hspace{10pt} 
  \begin{minipage}[b]{0.35\linewidth} 
    \includegraphics[width=\linewidth]{Figure_4b-dataset2.png}
    \subcaption{}
  \end{minipage}
  \caption{In (a), we utilize PiDiNet and facial dataset to create the required facial sketch dataset Syn-SketchFaces for training. The first row is photos of the original human face dataset, the second row is images obtained after contour extraction using PiDiNet, and the third row are the images required for creating a facial sketch dataset by inverting the color of the contour images in the second row. In (b), we create SketchFaces with real hand-drawn facial sketches.}
  \label{fig:dataset}
\end{figure*}

%------------------------------------------------------------------------
\subsection{Implementation Details }
During the first training stage, we conduct training for 10 epochs with a batch size of 16. Subsequently, in the second stage, we set training for 15 epochs with a reduced batch size of 6.
We employed Adam \cite{kingma2014adam} as the optimizer with a learning rate of 1e-4. 
We set \(\omega_{eyeP}=10.0\), \(\omega_{mouP}=5.0\), \(\omega_{conP}=10.0\), \(\omega_{pho}=0.2\), \(\omega_{reg}=1e-05\). When calculating weighted landmark loss, we assigned weights to the inner mouth and the jawline as 3, the weight for the corners of the nose and mouth is set to 1.5, and the remaining parts are assigned a weight of 1. 
%----------------Fig-Dataset------------------
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{Figure_5a-comparison.png}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   %\includegraphics[width=\linewidth]{your_image.png}
   \caption{Qualitative Comparisons on real hand-drawn sketches. From left to right: input sketches, DeepSketch2Face \cite{han2017deepsketch2face}, SketchFaceNeRF \cite{lin2023sketchfacenerf}, the combination of \cite{lin2023sketchfacenerf} and \cite{feng2021learning}, and our Sketch-1-to-3 method.}
   \label{fig:comparison-a}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{Figure_5b-comparison.png}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   %\includegraphics[width=\linewidth]{your_image.png}
   \caption{Qualitative Comparisons on synthetic sketches. From left to right: input sketches, DeepSketch2Face \cite{han2017deepsketch2face}, SketchFaceNeRF \cite{lin2023sketchfacenerf}, the combination of \cite{lin2023sketchfacenerf} and \cite{feng2021learning}, and our Sketch-1-to-3 method.}
   \label{fig:comparison-b}
\end{figure*}
%-------------------------------------------------------------------------
\subsection{Quantitative Comparisons}
In this section, we use SSIM and GMSD to quantify our results. SSIM focuses on the structural similarity of images. GMSD incorporates gradient information from images, demonstrating particular sensitivity to variations in structural and edge features. We integrated SSIM and GMSD for a comprehensive assessment of the reconstruction efficacy. In order to circumvent the constraints arising from the lack of sketch-3D paired facial sketch data, we render the predicted 3D model into 2D image, and then, compare it with the input sketch. All methods are evaluated using the SketchFaces dataset.
As delineated in Table \ref{tab:Quantitative Comparisons}, our Sketch-1-to-3 achieves the most superior reconstruction performance among these methods.
%------------------------------------------------------------------------
\begin{table*}[tb]
  \caption{Quantitative Comparisons. We employ two metrics, SSIM and GMSD, to evaluate the reconstruction performance of several models. The notation ($\downarrow$) signifies a preference for lower values, while ($\uparrow$) signifies a preference for higher values.}
  \label{tab:Quantitative Comparisons}
  \centering
  \footnotesize
  \begin{tabularx}{\textwidth}{X X X X X}
    \toprule
    Method & SketchMetaFace\cite{luo2023sketchmetaface} & SketchFaceNeRF\cite{lin2023sketchfacenerf} & \cite{lin2023sketchfacenerf}-DECA~\cite{feng2021learning} & Ours\\
    \midrule
    SSIM($\uparrow$) & 0.5399 & 0.5209 & 0.9616 & \textbf{0.9689}\\
    GMSD($\downarrow$) & 0.5569 & 0.5345 & 0.4584 & \textbf{0.4072}\\
    \bottomrule
  \end{tabularx}
\end{table*}
%------------------------------------------------------------------------
\subsection{Qualitative Comparisons}
We conduct qualitative comparisons between our proposed Sketch-1-to-3 method and several state-of-the-art 3D face reconstruction approaches to demonstrate its effectiveness. Our evaluation covers both real hand-drawn sketches and synthetic sketches, highlighting the robustness and versatility of our method. \hyperref[fig:comparison-a]{Fig~\ref{fig:comparison-a}} showcases the reconstruction results on real hand-drawn sketches, while \hyperref[fig:comparison-b]{Fig~\ref{fig:comparison-b}} presents results on synthetic sketches. Each figure is organized as follows: the first column displays the input sketches, followed by the reconstruction results of DeepSketch2Face \cite{han2017deepsketch2face}, SketchFaceNeRF \cite{lin2023sketchfacenerf}, a hybrid approach combining SketchFaceNeRF \cite{lin2023sketchfacenerf} and Feng et al. \cite{feng2021learning}, and finally, our Sketch-1-to-3 method. Our method consistently produces more accurate and detailed 3D faces, demonstrating its effectiveness across diverse sketch inputs.

 
% We conduct qualitative comparisons between our proposed method Sketch-1-to-3 and the current state-of-the-art 3D face reconstruction methods. The results demonstrate the superior performance of our method on both synthetic sketches and real hand-drawn sketches. See \hyperref[fig:comparison]{Fig~\ref{fig:comparison}}. The left five columns depict the 3D facial reconstruction results on synthetic sketches, while the right five columns illustrate the 3D facial reconstruction results on real hand-drawn sketches. Sequentially, the columns represent the input sketches, the reconstruction results of DeepSketch2Face \cite{han2017deepsketch2face}, SketchFaceNeRF \cite{lin2023sketchfacenerf}, the combination of \cite{lin2023sketchfacenerf} and \cite{feng2021learning}, and our Sketch-1-to-3 method, from left to right.

Through a comparative analysis with DeepSketch2Face \cite{han2017deepsketch2face}, it is apparent that the facial models generated by DeepSketch2Face \cite{han2017deepsketch2face} tend to be exaggerated, abstract, and cartoonish, and the processing of facial details is also very rough. In contrast, our Sketch-1-to-3 proves to be more precise in capturing the facial construction of input sketches, manifesting superiority in the alignment of facial contours and key points such as the positioning of facial features. Moreover, compared with SketchFaceNeRF \cite{lin2023sketchfacenerf}, it is evident that SketchFaceNeRF \cite{lin2023sketchfacenerf} tends to stochastically generate superfluous information in the process of constructing 3D facial representations. Despite the apparent augmentation in the intricacies of the generated 3D facial structures, it is important to elucidate that these intricacies do not authentically originate from the input sketches. In opposition, our method not only achieves the detailed reconstruction of a 3D face but also guarantees that the encompassed details originate exclusively from the input sketch itself. We contend that this augmentation enhances the pragmatic applicability of the reconstruction methodology. We posit that such an outcome contributes to a more pragmatic application space for the 3D face reconstruction method. What's more, SketchFaceNeRF \cite{lin2023sketchfacenerf} has poor adaptability to occlusion. The reconstruction outcomes of SketchFaceNeRF \cite{lin2023sketchfacenerf} exhibit notable deviations in scenarios featuring impediments like hats within the input sketch. Conversely, our method demonstrates comparatively minimal susceptibility to occlusion effects.

In addition, we employ a combined approach of sketch-to-2D and 2D-to-3D methodologies to validate the outstanding efficacy of our end-to-end reconstruction of the 3D face. In this context, we use \cite{lin2023sketchfacenerf} to generate 2D images from sketches and use \cite{feng2021learning} to transform these 2D images into 3D facial representations.
It is evident from the results that the combination of methods \cite{lin2023sketchfacenerf} and \cite{feng2021learning}, both distinguished within their specific domains, falls short of achieving a satisfactory outcome in the realm of sketch-to-3D face reconstruction.
We attribute this phenomenon to the generation of extraneous information when producing 2D images from sketches (refer to \hyperref[fig:sketch-2D-3D]{Fig~\ref{fig:sketch-2D-3D}}).
Therefore, it becomes challenging for 3D reconstruction models to discern essential and significant information, consequently leading to reconstruction biases.
Our approach effectively mitigates this phenomenon, demonstrating robust and excellent performance across diverse sketches.


%-------------
\subsection{Ablation Studies}
\hyperref[fig:ablation_a]{Fig~\ref{fig:ablation_a}} and \hyperref[fig:ablation_b]{Fig~\ref{fig:ablation_b}} respectively illustrate the efficacy of our GCTD module and loss functions through the reconstructed results. Table \ref{tab:ablation} presents the results of objective indicators of SSIM and GMSD, which demonstrate the effectiveness of the proposed loss functions and the enhancement module GCTD.

% %---------- double column version
% \par
% \begin{figure}[ht]
%   \centering
%   \begin{minipage}[b]{\linewidth}
%     \includegraphics[width=\linewidth]{ablation-loss.png}
%     \subcaption{}
%   \end{minipage}
%   \hspace{20pt} 
%   \begin{minipage}[b]{\linewidth}
%     \includegraphics[width=\linewidth]{ablation-GCTD.png}
%     \subcaption{}
%   \end{minipage}
%   \caption{Ablation studies. In (a), the first row sequentially presents: the input sketch, Sketch-1-to-3 without \(L_{md}\), and Sketch-1-to-3; the second row sequentially presents: the input sketch, Sketch-1-to-3 without \(L_{conP}\), and Sketch-1-to-3. In (b), the two rows depict, in sequence, the following columns: the input sketch, Sketch-1-to-3 without GCTD, and Sketch-1-to-3.}
%   \label{fig:entire_figure}
% \end{figure}


% %---------- single column version
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.55\linewidth]{Figure_6a-ablation-loss.png}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   %\includegraphics[width=\linewidth]{your_image.png}
   \caption{Ablation studies on loss functions. The figure is organized into two rows, each sequentially presenting: (a) the input sketch, (b) Sketch-1-to-3 without \(L_{md}\) implementation, and (c) Sketch-1-to-3 results with \(L_{md}\) implementation.}
   \label{fig:ablation_a}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.55\linewidth]{Figure_6b-ablation-GCTD.png}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   %\includegraphics[width=\linewidth]{your_image.png}
   \caption{Ablation studies on GCTD. The figure is organized into two rows, each sequentially presenting: (a) the input sketch, (b) Sketch-1-to-3 results without GCTD implementation, and (c) Sketch-1-to-3 results with GCTD.}
   \label{fig:ablation_b}
\end{figure*}


% %---------- single column version
% \begin{figure*}[ht]
%   \centering
%   \begin{minipage}[b]{0.55\linewidth}
%     \includegraphics[width=\linewidth]{Figure_6a-ablation-loss.png}
%     \subcaption{}
%   \end{minipage}
%   \vspace{10pt} 
%   \begin{minipage}[b]{0.55\linewidth}
%     \includegraphics[width=\linewidth]{Figure_6b-ablation-GCTD.png}
%     \subcaption{}
%   \end{minipage}
%   \caption{Ablation studies. In (a), the first row sequentially presents: the input sketch, Sketch-1-to-3 without \(L_{md}\), and Sketch-1-to-3; the second row sequentially presents: the input sketch, Sketch-1-to-3 without \(L_{conP}\), and Sketch-1-to-3. In (b), the two rows depict, in sequence, the following columns: the input sketch, Sketch-1-to-3 without GCTD, and Sketch-1-to-3}
%   \label{fig:entire_figure}
% \end{figure*}

\noindent{\bf Ablation Studies on Loss Functions: }
We conduct ablation studies on the proposed loss functions to evaluate their impact on our model's reconstruction performance. In the following studies, keeping all other conditions constant, we change the specific loss during the training stage and observe the experimental results. The loss functions proposed by us are crucial for the accurate identification and reconstruction of sketched faces. When calculations are performed without the mutual distance loss \(L_{md}\), as in the first row, second column of \hyperref[fig:ablation_a]{Fig~\ref{fig:ablation_a}}, the model's ability to capture the facial shapes of individuals is diminished, particularly in areas such as the chin, where it fails to align well with the original sketch. Furthermore, we present ablation study specifically targeting one component of \(L_{md}\), namely \(L_{conP}\). As indicated in the second row of \hyperref[fig:ablation_a]{Fig~\ref{fig:ablation_a}}, \(L_{conP}\) exerts a particularly significant influence on the jawline of individuals. The absence of \(L_{conP}\) results in a reconstructed jawline that appears highly unnatural.
\par
\noindent{\bf Ablation Studies on GCTD: }
We also conduct ablation studies on our innovative feature enhancement module GCTD to assess its role in the process of sketch-to-3D face reconstruction. In \hyperref[fig:ablation_b]{Fig~\ref{fig:ablation_b}}, the second column illustrates the reconstruction results of the model without the feature enhancement module, while the third column depicts the reconstruction outcomes when our feature enhancement module is applied. It is evident that our module exerts a formidable impact on several facets of sketch-to-3D reconstruction. The first column highlights our module's proficiency in aiding the model to more accurately identify facial contours in a single sketch. Turning to the second column, it is apparent that our module contributes to a quite precise representation of facial details from the input sketch onto the reconstructed 3D face. Moreover, GCTD also contributes to alleviating some misalignments in recognition errors, serving a corrective role to a certain extent. In summary, the GCTD module is a straightforward yet highly potent component that significantly elevates the effectiveness of the model.
%------------------------------------------------------------------------
\begin{table}[tb]
  \caption{Ablation studies on the GCTD module and loss functions. We employ two metrics, SSIM and GMSD to evaluate the reconstruction performance of the model. The notation ($\downarrow$) signifies a preference for lower values, while ($\uparrow$) signifies a preference for higher values.}
  \label{tab:ablation}
  \centering
  \footnotesize
  \begin{tabularx}{\textwidth}{X X X}
    \toprule
    Model & SSIM($\uparrow$)  & GMSD($\downarrow$)\\
    \midrule
    Sketch-1-to-3 w/o GCTD  & 0.9611 & 0.4469\\
    Sketch-1-to-3 w/o \(L_{conP}\) & 0.9676 & 0.4304 \\
    Sketch-1-to-3 w/o \(L_{md}\)  & 0.9682 & 0.4265 \\
    Sketch-1-to-3 w/o \(L_{lmk}\) & 0.9685 & 0.4259 \\
    Sketch-1-to-3 & \textbf{0.9689} & \textbf{0.4072}\\
    \bottomrule
  \end{tabularx}
\end{table}
%------------------------------------------------------------------------
\begin{table*}[tb]
  \caption{User Studies. Modeling Support evaluates the level of assistance provided to users during 3D modeling. Convenience evaluates how easy and direct it is for users to obtain results. Accuracy measures the reliability of the reconstructed results. General Satisfaction evaluates the overall user experience.}
  \label{tab:userStudies}
  \centering
  \footnotesize
  \begin{tabularx}{\textwidth}{p{20pt} X X X X}
    \toprule
    Group & Modeling Support & Convenience & Accuracy & General Satisfaction\\
    \midrule
    A  & 4.000 & 4.267 & 3.467 & 3.933\\
    B & 4.333  & 4.400 &  4.067 &  4.533\\
    \bottomrule
  \end{tabularx}
\end{table*}

\subsection{User Studies}
In order to assess the advantages our approach could provide in real-world scenarios, we invited 30 users to evaluate and score the model's performance. These participants are between 19 and 26 years old and consist of 17 males and 13 females. They are divided into two groups based on their knowledge level: Group A consists of individuals who have received training in 3D modeling in the past and possess practical experience, while Group B consists of those with no experience in 3D modeling or limited experience as amateurs. Both groups had an equal number of participants.
\par
During the evaluation process, we intentionally avoid setting any specific stylistic requirements for the sketches created by participants, allowing them to work freely and expressively. We also advise participants not to refer to any images to ensure that each sketch reflects their unique interpretation and drawing ability. All participants are instructed to independently draw free-hand sketches that cover aspects such as facial details, diverse head orientation, and occlusion, to gain a thorough understanding of our model's capabilities. As shown in Table \ref{tab:userStudies}, our model produces satisfactory results across both user groups. It provides convenience for users with different levels of expertise and also shows that it can support the modeling process, especially for amateurs.

% \subsection{Discussion and Future Work}
% Although our method achieves promising results in general situations, there are still areas that can be explored in the future. The sketches inherently incorporate a pronounced level of ambiguity, posing a considerable challenge for the current model to accurately comprehend the intended information in sketches. As depicted in \hyperref[fig:limitation]{Fig~\ref{fig:limitation}}, crucial features such as eyebrows and the mouth of the figure in the sketch can be succinctly represented by a single ink dot or a straight line, which we identify as a key factor contributing to the challenges in sketch-based 3D reconstruction.


% Furthermore, as our work focuses on accurate facial geometry and detail reconstruction, we anticipate future efforts to deliver a complete 3D face creation pipeline. This would enable the reconstruction of colorized, texture-mapped 3D faces that faithfully reflect the artist's intent. We think potential solutions to this challenge could come from colorization~\cite{chen2024spcolor}, generation~\cite{zhan2023multimodal}, and more.

%% %--------------for double column version
% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=0.9\linewidth]{futureWork01.png}
%   \caption{Evaluation of the model performance in some specific circumstances.}
%   \label{fig:limitation}
% \end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{Figure_7-futureWork.png}
  \caption{The model faces challenges in interpreting some sketch features, such as a single dot or line representing complex details like a mouth or eyebrows.}
  \label{fig:limitation}
\end{figure}


\section{Conclusion}
\label{sec:Conclusion}
\par
In this paper, we present Sketch-1-to-3, a novel reconstruction method that generates a 3D face with accurate contours and diverse details from a single sketch. Distinguished from prior works, our approach introduces a GCTD enhancement module to precisely extract facial contours and vividly reconstruct intricate details embedded in the sketch. By integrating a novel deep architecture and a diverse set of loss functions, our method enhances the realism of the reconstructed 3D face, making it appear more human-like. This integration leverages 3D face priors, improving controllability while simplifying the overall process. Extensive experiments demonstrate that our method outperforms existing approaches, delivering a remarkably detailed and accurate reconstruction result based on a single image. Additionally, to address the challenge of dataset scarcity in sketch-to-3D facial reconstruction, we propose an innovative strategy and contribute a new sketch dataset to the community, filling a critical gap and serving as a benchmark for future studies. Another key contribution of our work is its end-to-end solution from a single sketch to a 3D model, seamlessly integrated into 3D creation software. User studies demonstrate its effectiveness for creators of all skill levels, bridging the gap between sketch-based design and 3D modeling workflows.


While our method achieves promising results, it faces challenges due to the inherent ambiguity of sketches. As shown in \hyperref[fig:limitation]{Fig~\ref{fig:limitation}}, key features like eyebrows or mouths may be represented by a single dot or line, posing difficulties for accurate 3D reconstruction. Additionally, while our work focuses on precise geometry and details, future efforts could expand the pipeline to include colorization and texturing, enabling fully realistic 3D faces that align with the artist's intent. Potential solutions may leverage advancements in colorization~\cite{chen2024spcolor}, generation~\cite{zhan2023multimodal}, and related fields.





% To print the credit authorship contribution details
% \printcredits

% \credit{Conceptualization, Methodology, Writing – original draft}
% \credit{Dataset Preparation, Software, Writing – original draft}
%  \credit{Writing – review \& editing, Funding acquisition, Supervision}
%  \credit{Visualization, Validation}
%   \credit{Writing – review \& editing, Investigation}
%  \credit{Validation, Investigation}
% \credit{Writing – review \& editing, Investigation}

\section*{CRediT authorship contribution statement}
\textbf{Liting Wen: }Conceptualization, Methodology, Writing – original draft. \textbf{Zimo Yang: }Dataset Preparation, Software, Writing – original draft. \textbf{Xianlin Zhang: }Writing – review \& editing, Funding acquisition, Supervision. \textbf{Chi Ding: }Visualization, Validation. \textbf{Yue Zhang: }Writing – review \& editing, Investigation. \textbf{Mingdao Wang: }Validation, Investigation. \textbf{Xueming Li: }Writing – review \& editing, Resources, Investigation.

\section*{Declaration of competing interest}
The authors declare that they have no known competing financial
interests or personal relationships that could have appeared
to influence the work reported in this paper.
\section*{Data availability}
Data will be made available on request.
\section*{Acknowledgements}
This work was supported by National Key R\&D Program "Cultural Technology and Modern Service Industry" Key Special Project (\#2021YFF0901700).
\par
\par

%% Loading bibliography style file
%\bibliographystyle{model1-num-names}
\bibliographystyle{elsarticle-num}

% Loading bibliography database
\bibliography{main}

% Biography
%\bio{}
% Here goes the biography details.
%\endbio

%\bio{pic1}
% Here goes the biography details.
%\endbio


\end{document}

