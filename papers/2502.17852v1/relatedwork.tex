\section{Related work}
%-------------------------------------------------------------------------
\subsection{Monocular 3D Face Reconstruction} 
Reconstructing 3D faces from a single 2D image is an inherently ill-posed problem, requiring supplementary prior knowledge to constrain the reconstruction process. A widely adopted approach is to estimate the parameters of a 3D Morphable Model (3DMM). Existing methods can be broadly categorized into optimization-based and learning-based techniques. Optimization-based methods, such as the work of Jo et al. \cite{jo2015single}, propose an reconstruction approach that integrates a simplified 3D Morphable Model with Structure-from-Motion to improve robustness against pose variations. By leveraging bilateral symmetry to recover occluded facial feature points, their method enhances reconstruction accuracy while addressing the limitations of model-based and SfM-based approaches under large pose variations. For learning-based methods, Dou et al. \cite{dou2018monocular} designed a coupled-dictionary learning framework, which estimates a sparse 3D shape from 2D landmarks and refines it via super-resolution. Recent advancements by Feng et al. \cite{feng2021learning} further enhance reconstruction quality by introducing a novel approach that robustly generates detailed 3D face models, disentangling person-specific facial details from expression-dependent variables. Dinesh et al. \cite{dinesh2022fully} proposed a method for partial face reconstruction, leveraging a Fully Convolutional Network for spatial feature extraction and Deep Stacked Denoising Sparse Autoencoders for refinement. Morales et al. propose BabyNet \cite{morales2023babynet}, combining a 3D graph convolutional autoencoder with a 2D encoder via transfer learning. Zhu et al. \cite{zhu2025driving} integrates inpainting and reconstruction in a mutually reinforcing manner, enabling high-fidelity 3D face reconstruction from largely masked images.

A significant challenge for learning-based methods is the scarcity of ground truth 2D-3D paired data. To address this, researchers have explored the use of synthetic data for training models. In the work by Sela et al. \cite{sela2017unrestricted}, the authors propose an unrestricted approach to facial geometry reconstruction using an Image-to-Image translation network. The network is trained exclusively on synthetic data, which includes a wide range of facial identities, poses, expressions, lighting conditions, and material parameters. Despite the synthetic nature of the training data, the network demonstrates the ability to generalize to real-world facial images, producing detailed and accurate reconstructions. Inspired by these advancements, we propose a novel method for reconstructing detailed 3D face models from single free-hand sketches. Unlike prior works, our approach avoids the need for 3D face data during training, relying solely on sketch inputs. This enables direct extraction of facial information and precise reconstruction of 3D faces with intricate details, offering a practical solution for sketch-based 3D modeling.
%-------------------------------------------------------------------------
\subsection{Sketch-based 2D Facial Image Generation} 
Converting free-hand sketches to 2D faces is currently under extensive exploration. Jo et al. ~\cite{Alpher10} proposed SC-FEGAN, which generated images when users provided free-form masks, sketches, and colors as input. Fang et al. ~\cite{Alpher11}proposed an identity-aware CycleGAN~\cite{zhu2017unpaired} model that applied a new perception loss to supervise an image generation network.
By focusing more on the synthesis of key facial regions crucial for identity recognition, CycleGAN's photo sketch synthesis has been improved. There have been also attempts to utilize StyleGAN~\cite{karras2019style} which significantly improved performance by introducing novel technologies such as progressive generation and style-based architecture~\cite{Alpher14}. 

Different from traditional GANs, Chen et al. ~\cite{Alpher15} implicitly modeled the shape space of plausible face images and synthesized a face image in this space to approximate an input sketch. Li et al. ~\cite{Alpher16} proposed DeepFacePencil, which is able to generate photo-realistic face images from hand-drawn sketches, based on a novel dual generator image translation network during training. Yang et al. ~\cite{Alpher18} proposed a novel controllable sketch-to-image translation framework, which can allow users to flexibly define how the final output through refinement level control parameters. The above research focuses on the generation of 2D images from 2D sketches, and there is no research that directly considers the generation of 3D high-fidelity faces from abstract sketches.

\subsection{Sketch-based 3D Face Reconstruction} 
Currently, there are only few methods for reconstructing 3D faces directly from hand-drawn sketches. 
Han et al. proposed DeepSketch2Face~\cite{han2017deepsketch2face}, a fast, interactive, deep learning based 3D face and comic modeling sketch system. By simply sketching facial images, corresponding 3D facial models can be quickly generated, and facial contours and expressions can be simultaneously fitted. However, due to its low-cost interactive 3D facial modeling method, the generated model is so cartoonish, exaggerated, and lacks details that it cannot complete the task of generating detailed 3D faces from free-hand sketches.

Lin et al. proposed SketchFaceNeRF ~\cite{lin2023sketchfacenerf}, a 3D facial reconstruction method based on NeRF. However, the NeRF method consumed relatively high computational resources and training time. Whatâ€™s more, it was not possible to obtain a complete 3D face model based solely on one sketch, but can only obtain a face model within a limited perspective range. Yang et al. proposed a method of learning 3D facial reconstruction from a single sketch ~\cite{Alpher22}. The method can be divided into two stages. In the first stage, information from photos and sketches was used to obtain rough reconstruction results, while in the second stage, the depicted lines were further used to refine the results. However, the facial model reconstructed by this method was not perfect and had defects in details. Therefore, we propose our reconstruction strategy to handle different facial regions and detail types.
%-------------------------------------------------------------------------



%-------------------------------------------------------------------------
%-------------------------------------------------------------------------

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{Figure_2-result.png}
    \caption{Reconstruction results of Sketch-1-to-3 based on our sketch dataset. Sketch-1-to-3 reconstructs 3D face from a single sketch. The first row represents the input sketch, the second row illustrates the detailed 3D face reconstruction.}
   \label{fig:first}
\end{figure*}
%-------------------------------------------------------------------------