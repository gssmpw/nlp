\section{Related Work}
\label{sec:related:work}


% \paragraph{Approximation}
Extensive research has explored the approximation capabilities of neural networks across various architectures. Early works established the universal approximation theorem for single-hidden-layer networks____, proving their ability to approximate specific functions arbitrarily well, though without quantifying error in relation to network size. Subsequent studies____ analyzed approximation errors, relating them to network width, depth, or parameter count, and addressed the spectral bias in neural network approximations. 
Here, we specifically highlight two papers ____ that are closely related to our theoretical results.
 The results in ____ imply that \ReLU/\sine-activated fully connected neural networks (FCNNs) with width $O(d)$ and depth $O(L)$ can approximate a 1-Lipschitz function $f:[0,1]^d\to\mathbb{R}$ within an error of $O(2^{-\sqrt{L}})$. Compared to this, our work achieves several key improvements: 
1) Our results incorporate width $N$, extending beyond fixed-width networks.  
2) We improve the approximation error rate to $O(N^{-L})$, much better than $O(2^{-\sqrt{L}})$ when $N$ fixed.  
3) We introduce \SinTU{}, a novel hybrid activation function, where a single \SinTU{} function can replace two activation functions (\ReLU,\,\sine) in the approximation results.  
4) Our results specifically apply to MMNNs, which introduce an additional dimension called rank beyond width and depth.  
In ____, the author proposes a simple activation function, \EUAF, and demonstrates that a fixed-size \EUAF-activated FCNN can approximate any continuous function $f\in C([0,1]^d)$ to an arbitrarily small error by adjusting only finitely many parameters. 
\EUAF{} emphasizes theoretical approximation but tends to perform less effectively in practice, often appearing somewhat artificial. 
In contrast, our FMMNN is naturally structured. Although its theoretical approximation power is comparatively weaker, an exponential approximation rate is typically sufficient in practical applications. Our extensive experiments further confirm its effectiveness, demonstrating surprisingly strong empirical performance.




% However, the above works primarily focus on theoretical constructions, with limited attention to computational parameter determination, numerical errors, and finite-precision effects in simulations. This paper first establishes strong approximation results, then employs an intuitive landscape analysis of the cost function to assess learning potential, and finally validates our findings through extensive numerical experiments.
% \paragraph{\Sine{} activation function}


Some previous works have explored the use of \sine{} as an activation function in multi-layer networks ____. The authors in ____ present numerical examples demonstrating the advantages of sine-activated networks, primarily in FCNNs. Additionally, ____ introduces a multiplicative filter network that incorporates sinusoidal filters and illustrates its benefits through various examples.
To the best of our knowledge, existing works do not provide strong mathematical or numerical justification for the benefits of \texttt{sine}, which is a key reason why it is rarely used in practice.
This paper first establishes rigorous approximation results, followed by an intuitive landscape analysis of the cost function to assess learning potential, and finally validates these findings through extensive numerical experiments. Our experiments show that using the \texttt{sine} activation in FCNNs does not always lead to good performance. While it works well in some cases, it performs quite poorly in others. We believe this inconsistency is the main reason why \texttt{sine} is not widely adopted in practice.
Surprisingly, we found that our MMNN network structure and \texttt{sine} (or \SinTU{s}) form a highly effective combination. Compared to FCNNs, MMNNs exhibit a simpler optimization landscape. Our experiments confirm this observation, as MMNNs achieve consistently strong performance across all test cases.



% This gap motivated our current investigation, which considers practical training processes and numerical errors. Specifically, the balanced structure of MMNN, the choice of training parameters, and the associated learning strategy discussed here are intended to facilitate a smooth decomposition of the function, thereby promoting an efficient training process.

% \paragraph{Low-rank methods }
% Low-rank structures in $\bmW$ of a fully connected neural network have been investigated by a variety of groups.
% % which is equivalent to pursuing a linear dimension reduction between layers.
% \zhou{For example, the methods proposed in ____ focus on accelerating training and reducing memory requirements while maintaining final performance. The low-rank idea is generalized to tensor train decomposition in____.}
% %However this may not be effective in a nonlinear representation using a deep neural network. For example, the low-rank methods proposed in ____ focus on accelerating training and reducing memory requirements while maintaining final performance. 
% \zhou{This is different from the MMNN proposed here in two aspects. First there are two matrices in each layer, $\bmA$ outside and $\bmW$ inside the activation functions. Each row of $\bmA$ is the weights to a linear combination of a set of random basis functions which forms a component in each layer. The number of rows of $\bmA$, which is the number of components, are selected depending on the complexity of the function and is usually much smaller than the number of columns, which corresponds to the number of basis functions. Each row of $(\bmW, \bmb)$ corresponds to a random parametrization of a basis function. The number of rows of $\bmW$ corresponds to the number of basis functions which is usually much larger than the number of columns of $\bmW$ which is the input dimension. Secondly, in our MMNN, only $\bmA$ is trained while $\bmW$ is fixed with randomly initialized values. The theoretical study and numerical experiments demonstrate that the architecture of MMNN, combined with the learning strategy, is effective in approximating complicate functions. 
% %Our key contribution is the design of a balanced multi-component and multi-layer structure to decompose the complexity of the target function that is conducive to approximation and learning. We theoretically and numerically demonstrate that our architecture, combined with our learning strategy, is effective in approximating complex functions. 
% }

% \paragraph{Low-rank methods}
% Low-rank structures in the weight matrix $\bmW$ of a fully connected neural network have been investigated by various groups. For example, the methods proposed in ____ focus on accelerating training and reducing memory requirements while maintaining final performance. The concept of low-rank structures is further extended to tensor train decomposition in____.
% The MMNN proposed here differs in two key aspects. First, each layer contains two matrices: $\bmA$ outside and $\bmW$ inside the activation functions. Each row of $\bmA$ represents the weights for a linear combination of a set of random basis functions, forming a component in each layer. The number of rows in $\bmA$, which equals the number of components, is selected based on the complexity of the function and is typically much smaller than the number of columns, corresponding to the number of basis functions. Each row of $(\bmW, \bmb)$ represents a random parameterization of a basis function, with the number of rows in $\bmW$ corresponding to the number of basis functions, usually much larger than the number of columns in $\bmW$, which is the input dimension.
% Secondly, in our MMNN, only $\bmA$ is trained while $\bmW$ remains fixed with randomly initialized values. Theoretical studies and numerical experiments demonstrate that the architecture of MMNN, combined with the learning strategy, is effective in approximating complex functions. 
% Fixing $(\bmW, \bmb)$ of each layer and use of random basis functions in the MMNNs is inspired by a previous approach known as random features ____. In typical random feature methods, only the linear combination parameters at the output layer are trained which also leads to the issue of ill-conditioning of the representation. While in MMNNS matrix $\bmA$ and vector $\bmc$ of each layer are trained. 
% % Although this approach increases the computational cost compared to random feature methods, it is still less expensive than training a network where both $(\bmA,\bmc)$ and $(\bmW, \bmb)$ are learnable parameters. Additionally,
% Our MMNN employs a composition architecture and learning mechanism that enhances the approximation capabilities compared to random feature methods while achieving a more effective training process than a standard fully connected network of equivalent size. Extensive experiments demonstrate that our approach can strike a satisfactory balance between approximation accuracy and training cost.


% \paragraph{Komogolrov-Arnold (KA) representation:} \zhou{KA representation states a multivariate continuous function on a bounded domain can be written as a finite composition of continuous functions of a single variable and the binary operation of addition. Despite of the possibility of encountering non-smooth or even fractal univariate functions in the KA representation, it provides an appealing framework to build neural network structures to approximate functions especially in higher dimensions. This idea has been explored in a number of studies ____ (please add citations). For example, a recent proposed network called KA network (KAN) uses spline functions to approximate the univariate functions in the KA representation. Aimed at approximating highly oscillatory functions, the proposed MMNN is based on the idea of ``divide and conquer'', which uses different network architectures, activation functions, and training cost than those KA representation based methods.}
%Recently, a network structure inspired by the Kolmogorov-Arnold (KA) representation, called KAN, is proposed. KA representation states a multivariate continuous function on a bounded domain can be written as a finite composition of continuous functions of a single variable and the binary operation of addition. The key idea of KAN is to use spline functions to approximate those univariate functions. However, those univariate functions may be highly non-smooth or even fractal which makes splines difficult to approximate well. Hence a network that is wider and deeper than the compact form of KA representation is needed in practice.  Moreover, the coordinate system may be artificial and simple additions of different univariate functions may not be effective in approximating a generic multi-variate function. Also the training process seems relative expensive. 

% \paragraph{Komogolrov-Arnold (KA) representation} 
% The KA representation theorem ____ states that any multivariate continuous function on a hypercube can be expressed as a finite composition of continuous univariate functions and the binary operation of addition. However, this elegant mathematical representation may result in non-smooth or even fractal univariate functions in general due to this very specific form of representation, a computational challenge one has to address in practice. 
% %Although this representation may result in non-smooth or even fractal univariate functions, it offers a compelling framework for constructing neural network structures to approximate functions, particularly in higher dimensions. 
% KA representation has been explored in several studies ____. A recently proposed network known as the KA network (KAN) utilizes spline functions to approximate the univariate functions in the KA representation. The proposed MMNN is motivated by a multi-component and multi-layer smooth decomposition, or a ``divide and conquer" approach, employing distinct network architectures, activation functions, and training strategies.


% 	To compute the integral
% 	\[
% 	\int \sin(ax + b) \sin(cx + d) \, dx,
% 	\]
% 	we use the product-to-sum identity:
% 	\[
% 	\sin(A) \sin(B) = \frac{1}{2} \left[\cos(A - B) - \cos(A + B)\right].
% 	\]
% 	Applying this identity with \( A = ax + b \) and \( B = cx + d \), we get
% 	\[
% 	\sin(ax + b) \sin(cx + d) = \frac{1}{2} \left[\cos((a - c)x + (b - d)) - \cos((a + c)x + (b + d))\right].
% 	\]
% 	Thus, the integral becomes
% 	\[
% 	\int \sin(ax + b) \sin(cx + d) \, dx = \frac{1}{2} \int \left[\cos((a - c)x + (b - d)) - \cos((a + c)x + (b + d))\right] dx.
% 	\]
% 	Now, we can integrate each term separately:
% 	\[
% 	= \frac{1}{2} \left( \int \cos((a - c)x + (b - d)) \, dx - \int \cos((a + c)x + (b + d)) \, dx \right).
% 	\]
% 	For each cosine term, the integral is straightforward:
% 	\[
% 	\int \cos(kx + \phi) \, dx = \frac{1}{k} \sin(kx + \phi),
% 	\]
% 	where \( k \) is a constant. Therefore,
% 	\[
% 	\int \sin(ax + b) \sin(cx + d) \, dx = \frac{1}{2} \left( \frac{\sin((a - c)x + (b - d))}{a - c} - \frac{\sin((a + c)x + (b + d))}{a + c} \right) + C,
% 	\]
% 	where \( C \) is the constant of integration.
    


% $u,v$ fixed, $w,b$ learnable
% \begin{equation*}
%     \int_{-m}^{m} \big(\sin  (wx+b) - \sin(ux+v)\big)^2 \, dx
% \end{equation*}

% $u,v$ fixed, $w,b$ learnable
% \begin{equation*}
%     \int_{-m}^{m} \big(w\sin  (x)+b - u \sin(x)-v\big)^2 \, dx
% \end{equation*}

% Formally, we are minimizing the squared loss \((\sin(\nu x) - \sin(wx))^2\). For a fixed choice of \(\nu\) and \(m\),
% the loss landscape \(L(\nu, w, m)\) with respect to \(w\) has the form
% \[
% L(\nu, w, m) = \int_{-m}^{m} (\sin(\nu x) - \sin(wx))^2 \, dx
% = -\frac{2 \sin \left( m (w - \nu) \right)}{w - \nu} + \frac{2 \sin \left( m (w + \nu) \right)}{w + \nu} - \frac{\sin (2mw)}{2w} + c(\nu, m)
% \]
% where \(c(\nu, m)\) is a constant term. As illustrated in Fig. 1, for a fixed choice of \(\nu\) and \(m\), the three main terms in \(L(\nu, w, m)\) are three cardinal \sine{} s (or sincs): the first is negative and centered at \(w = \nu\), which is the only global minimum and where the loss is 0; the second term is positive and centered at \(w = -\nu\), and is the only global maximum; the third sinc is negative and centered in \(w = 0\). The latter creates a local minimum for small values of \(w\) and large values of \(m\) and \(\nu\), where the function expressed by the network is a constant \(\sin(0) = 0\).





% \clearpage