 % \documentclass[11pt,a4paper]{article}
% \documentclass[twoside,10pt]{article}
\documentclass[11pt,a4paper]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsthm}
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{,}{,} 
%% modify plainnat in \bibliographystyle{plainnat}

% \usepackage[numbers,sort]{natbib} % load natbib package for references
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \usepackage[preprint,hyperref]{jmlr2eEdited}
% \usepackage{xcolor}
\usepackage[bookmarks,colorlinks]{hyperref}
\usepackage{xcolor}
\definecolor{darkblue}{rgb}{0,0.22,0.66}
\definecolor{darkcyan}{RGB}{0,139,139}
\definecolor{darkgray}{HTML}{666666}
\hypersetup{colorlinks=true,
citecolor=darkcyan,
linkcolor=darkblue,
urlcolor=darkgray
% urlcolor=magenta
}
% \usepackage{jmlr2e}
% %%%%%%%%%%%%%%%%%%%%%%%%%%
\input{preamble}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\let\ldots\cdots
\let\dots\cdots


\newcommand{\sz}[1]{\quad\textcolor{blue}{\textsf{[SZ: #1]}}\quad}

\newcommand{\zhou}[1]{\quad\textcolor{brown}{\textsf{[Zhou: #1]}}\quad}



%\title{On the Theoretical Efficiency and Practical Challenges of ReLU-Sine Mixed Neural Networks}

% \title{Fourier Multi-Component and Multi-Layer Neural Network: its Landscape and Beyond}

\title{Fourier Multi-Component and Multi-Layer Neural Networks: Unlocking High-Frequency Potential}


% \author{Shijun Zhang\thanks{Department of Mathematics, Duke University, Durham, NC 27708; \href{mailto:shijun.zhang@duke.edu}{shijun.zhang@duke.edu}}
% \author{Shijun Zhang\thanks{Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong;\\ \href{mailto:shijun.zhang@polyu.edu.hk}{shijun.zhang@polyu.edu.hk}}
% \And Hongkai Zhao\thanks{Department of Mathematics, Duke University, Durham, NC 27708; \href{mailto:zhao@math.duke.edu}{zhao@math.duke.edu}}
% \AND Yimin Zhong\thanks{Department of Mathematics and Statistics, Auburn University, Auburn, AL 36830; \href{mailto:yimin.zhong@auburn.edu}{yimin.zhong@auburn.edu}}
% \And Haomin Zhou\thanks{ School of Mathematics,
%  Georgia Institute of Technology, Atlanta, GA 30332;
% 	\href{mailto:hmzhou@math.gatech.edu}{hmzhou@math.gatech.edu} }}

% \author{
% Shijun Zhang\\
% Department of Applied Mathematics\\
% The Hong Kong Polytechnic University\\ \href{mailto:shijun.zhang@polyu.edu.hk}{shijun.zhang@polyu.edu.hk}
% \And Hongkai Zhao\\
% Department of Mathematics\\
% Duke University\\ \href{mailto:zhao@math.duke.edu}{zhao@math.duke.edu}
% \AND Yimin Zhong\\ 
% Department of Mathematics and Statistics\\ Auburn University\\ \href{mailto:yimin.zhong@auburn.edu}{yimin.zhong@auburn.edu}
% \And Haomin Zhou\\
% School of Mathematics\\
%  Georgia Institute of Technology\\
% 	\href{mailto:hmzhou@math.gatech.edu}{hmzhou@math.gatech.edu} 
%     }
%  % \date{}


\author{\name   Shijun Zhang
\email  \href{mailto:shijun.zhang@polyu.edu.hk}{shijun.zhang@polyu.edu.hk}\\
\addr  Department of Applied Mathematics\\
% The 
Hong Kong Polytechnic University
\AND
\name  Hongkai Zhao
\email 
\href{mailto:zhao@math.duke.edu}{zhao@math.duke.edu}\\
\addr  Department of Mathematics\\
Duke University
\AND  \name  Yimin Zhong
\email \href{mailto:yimin.zhong@auburn.edu}{yimin.zhong@auburn.edu}
\\ 
\addr  Department of Mathematics and Statistics\\ Auburn University
\AND  \name  Haomin Zhou
\email  	\href{mailto:hmzhou@math.gatech.edu}{hmzhou@math.gatech.edu} 
\\
\addr School of Mathematics\\
 Georgia Institute of Technology
    }
% \author{\name  \href{https://shijunzhang.top/}{\color{black}Shijun Zhang}\thanks{\hspace{-2pt}Corresponding author.}
% 	\email \mailto[shijun.math@outlook.com]{shijun.zhang@duke.edu}\\ 
% 	\addr   Department of Mathematics\\  
% 	Duke University
% 	\AND  \name Jianfeng Lu
% 		\email \mailto{jianfeng@math.duke.edu} \\
% 		\addr   Department of Mathematics\\
% 		  Duke University
% 	\AND \name Hongkai Zhao 
% 	\email \mailto{zhao@math.duke.edu}  \\
% 	\addr   Department of Mathematics\\  
% 	Duke University
% 		}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\begin{document}
\maketitle



\begin{abstract}

The two most critical ingredients of a neural network are its structure and the activation function employed, and more importantly, the proper alignment of these two that is conducive to the effective representation and learning in practice.
In this work, we introduce a surprisingly effective synergy, termed the Fourier Multi-Component and Multi-Layer Neural Network (FMMNN), and demonstrate its surprising adaptability and efficiency in capturing high-frequency components. 
First, we theoretically establish that FMMNNs have exponential expressive power in terms of approximation capacity.  Next, we analyze the optimization landscape of FMMNNs and show that it is significantly more favorable compared to fully connected neural networks. Finally, systematic and extensive numerical experiments validate our findings, demonstrating that FMMNNs consistently achieve superior accuracy and efficiency across various tasks, particularly impressive when high-frequency components are present.  Our code and implementation details are available
\href{https://github.com/ShijunZhangMath/FMMNN}{here}.




% The two most critical ingredients of a neural network are its structure and the activation function employed, and more importantly, the proper alignment of these two that is conducive to the effective representation and learning in practice. In this work, we will investigate both ingredients for the multi-component and multi-layer neural networks (MMNNs),
% % \cite{ZZZZ-24-MMNN}, 
% the key structure of which is based on a structured and balanced decomposition (using multi-components) and composition (using multi-layers). 
% We show that, surprisingly, using \sine{} or \SinTU{s} (\Sine{} Truncated Units) as activation functions within MMNNs, named FMMNNs, forms an effective synergy. In particular, we prove the \red{exponential} mathematical capacity for approximation and demonstrate the \red{simplified} optimization landscape. 
% Systematic and extensive numerical experiments are conducted to validate our analysis, showing that FMMNNs consistently outperform in both accuracy and efficiency across various tasks, especially those involving high-frequency components. In most cases, the \sine{} activation function is sufficient, as the MMNN structure demonstrates strong adaptability.
% Systematic and extensive numerical experiments are conducted to corroborate our investigation and demonstrate that FMMNNs achieve superior performance in terms of both accuracy and efficiency across various tasks, particularly those involving high-frequency components. 


% We first introduce a novel activation function called the \sine{}  Truncated Unit (\SinTU) and then demonstrate that the \sine{}  (or \SinTU{}) activation function combined with multi-component and multi-layer neural networks (MMNNs) forms an effective synergy.
% First, we show that employing \texttt{sine} or \SinTU{s} as the activation function within the MMNN architecture offers significant mathematical advantages in terms of approximation capability. 
% Next, we demonstrate that the optimization landscape of the cost function for MMNNs is considerably simpler compared to that of fully connected neural networks (FCNNs).
% Furthermore, through extensive numerical experiments, we show that Fourier MMNNs (FMMNNs), which utilize \texttt{sine} or \SinTU{s} as activation functions within MMNNs, achieve superior performance in terms of both accuracy and efficiency across various tasks.}



% The two most important ingredients of a neural network are its structure and the activation function used. In this work, we will investigate both of these ingredients for the multi-component and multi-layer neural networks (MMNNs), a structured and balanced network structure proposed in our earlier work \cite{ZZZZ-24-MMNN}. 
% The key structure of MMNNs is based on a balanced decomposition (using multi-components) and composition (using multi-layers) which allows MMNNs to achieve much improved accuracy and training efficiency with a much smaller network size compared to standard fully connected neural networks (FCNNs a.k.a. MLPs). The key idea of MMNNs is that, instead of treating each hidden neuron individually, each component, which is a linear combination of a family of randomly parametrized bases (or neurons), is viewed as a whole. The key observation is that each component, which is a one hidden layer network, only needs to approximate a smooth function and can be trained effectively using MMNN structure. In this study, we study two important properties for MMNN: 1) the landscape of the loss function in terms of the parameters; and 2) the effect of using different activation functions. In particular, we show that MMNN has a much simpler landscape than (FCNN). We also propose using \texttt{sine} (or \texttt{cosine}) as the activation function, FMMNN, and show that it is more effective than other commonly used activation functions in general. The possible use of hybrid activation functions is also explored. 



%In particular we show that using a combination of \texttt{sine} and \texttt{ReLU} activation functions can lead to less Gibbs phenomena than FMMNN.

%In particular, each component is a linear combination, the weights of which are learned through optimization, of a family of random basis defined by the activation function with randomly initialized and fixed parameters. In order for the training to be efficient, the key observation is that each component, which is a two layer (one hidden layer) network, should be smooth. Hence the key question, studied in this work, is: how does activation function parametrized randomly affect a learning process based on gradient descent based algorithms. We demonstrate both mathematically and numerically (with extensive experiments) that using \sine{}  (or cosine) as the activation function, i.e., random Fourier basis, is very effective in most cases. In the case where the target function is nearly discontinuous, the use of hybrid activation functions is investigated and we show that an appropriate combination of \sine{}  (or cosine) and ReLu activation functions can lead to better results with less Gibbs phenomenon.


\end{abstract}



\begin{keywords}
        % rectified linear unit, 
        high-frequency approximation,
        deep neural networks,
        Fourier analysis,
        sine activation function,
        function compositions
        % ,
        % nonlinear approximation
        % Fourier series
\end{keywords}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% pre defined cmds
% Use A and alpha as examples
%%%%%%%%%%%%%%%%%%%%
% \calA  means \mathcal{A}
% \calalpha  means \mathcal{\alpha}

% \cal can be replaced by \bm, \scr, \tilde, \hat, \bmcal, \bmscr, \tildebm, \hatbm, and so on


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}


% The two most important ingredients of a neural network are its structure and the activation function used. Together they determine the network effectiveness. Most importantly, network representation and its training efficiency are highly related and need to be considered concurrently to achieve a good performance in practice. For example, although a shallow neural network, e.g., two layer (or one hidden layer), has been proved to have a universal approximation property as the network width goes to infinity as long as a nonlinear non-polynomial activation function is used, it is only useful to approximate smooth functions when global activation functions are used. This is due to the strong correlations among the activation functions (parametrized by weights and biases), which cause ill-conditioning with a bias against high frequencies in both the representation and the training process as analyzed in \cite{ZZZZ-23}. Moreover, it was shown that the smoother the activation function is, the worse conditioned and more biased against high frequencies the network is. Hence the use of smooth activation functions, such as \texttt{tanh}, \texttt{sigmoid}, and \texttt{sine}, have very poor performance when approximating functions which contain relative high frequency components, such as functions with fast transitions and/or oscillations. The best achievable numerical accuracy is limited by truncation frequency, which is determined by the ill-conditioning (depends on the activation function used) and the machine error or noise level in the data, and frequency components in the target function. Moreover, the limit can not be improved even with unlimited computing resources, e.g., as the network width and/or training time is unlimited.

% Although multi-layer networks are more powerful and flexible in representation using compositions of shallow networks, however, the network structure is of crucial importance in determining the effectiveness of the network and in particular, the training efficiency. Moreover, most training processes are gradient descent based (first order) methods which are very local and sensitive to ill-conditioning of the cost function (Jacobian) in terms of the typically humongous number of parameters. In our earlier work \cite{ZZZZ-24-MMNN}, we proposed structured and balanced multi-component and multi-layer neural networks (MMNN) based on our understanding of a one-hidden-layer network. The key idea of MMNNs is to view a linear combination of a family neurons (randomly parameterized activation functions), called a component, as a whole,  which can be trained to approximate a smooth function rather easily. Each layer has multiple components all sharing a set of randomly parametrized neurons, the number of which is the layer's width, with different linear combinations. The number of components, called rank, is typically much smaller than the layer’s width and both of which can be increased to enhance the capability of decomposition when dealing with more complex functions. These components are combined and composed (through layers) in a structured and balanced way in terms of network width, rank, and depth to approximate a target function effectively. Another important feature we used in practice is that weights and biases inside activation functions (or neurons) are randomly assigned and fixed during the training while the linear combination weights of neurons in each component are trained. MMNN structure leads to more efficient training processes motivated by our findings: 1) a mildly sized one-hidden-layer neural network can be trained effectively to approximate a smooth function well using random basis functions \cite{ZZZZ-23}, and 2) a more complex function can be decomposed and composed through MMNN sturcture \cite{ZZZZ-24-MMNN}. While an easy modification to fully connected neural networks (FCNNs) a.k.a. multi-layer perceptrons (MLPs) through the introduction of balanced multi-component structures in the network, MMNNs achieve a significant reduction of training parameters, a much more efficient training process, and a much improved accuracy compared to FCNNs \cite{ZZZZ-24-MMNN}. 

% The two key components of a neural network are its architecture and the choice of activation function, both of which jointly determine its effectiveness.
% In our previous work \cite{ZZZZ-23}, we demonstrated that shallow networks (i.e., those with a single hidden layer) using various activation functions struggle to capture high-frequency components. 
% To overcome this, later in \cite{ZZZZ-24-MMNN}, we introduced structured and balanced multi-component and multi-layer neural networks (MMNNs), leveraging insights from one-hidden-layer networks. 
% This structure leads to significantly more efficient training, supported by two main findings: (1) a moderately sized one-hidden-layer network can effectively approximate a smooth function using random basis functions \cite{ZZZZ-23}, and (2) complex functions can be decomposed and composed within the MMNN framework \cite{ZZZZ-24-MMNN}.
% MMNNs provide a simple yet impactful modification of fully connected neural networks (FCNNs), also known as multi-layer perceptrons (MLPs), by incorporating balanced multi-component structures. This results in fewer trainable parameters, more efficient training, and significantly improved accuracy compared to conventional FCNNs \cite{ZZZZ-24-MMNN}.


% The key idea is to treat a linear combination of a family of neurons (randomly parameterized activation functions), called a component, as a single unit that can be efficiently trained to approximate a smooth function. Each layer consists of multiple components that share a common set of randomly parameterized neurons, whose count defines the layer’s width, while different components use distinct linear combinations. The number of components, known as the  rank, is typically much smaller than the layer width, and both can be adjusted to improve function decomposition and representation. These components are systematically combined and composed across layers in a structured and balanced manner to approximate target functions effectively.


% A key feature of MMNNs is that weights and biases inside activation functions remain fixed during training, while only the linear combination weights of neurons within each component are updated. This structure leads to significantly more efficient training, supported by two main findings: (1) a moderately sized one-hidden-layer network can effectively approximate a smooth function using random basis functions \cite{ZZZZ-23}, and (2) complex functions can be decomposed and composed within the MMNN framework \cite{ZZZZ-24-MMNN}.
% MMNNs provide a simple yet impactful modification of fully connected neural networks (FCNNs), also known as multi-layer perceptrons (MLPs), by incorporating balanced multi-component structures. This results in fewer trainable parameters, more efficient training, and significantly improved accuracy compared to conventional FCNNs \cite{ZZZZ-24-MMNN}.

% In this work, we further explore the behavior and potential of MMNNs and discover, quite surprisingly, that the \sine{}  function serves as an exceptionally effective activation function for MMNNs since a linear combination of \sine{} can approximate each smooth component well and using multi-layer composition can effectively generate high frequencies. On the other hand, as shown in our study \cite{ZZZZ-23} for shallow networks, singularity in an activation function, e.g., in \texttt{ReLU}, can help in representation capacity. In particular, using \sine{}  alone may not perform well for non-smooth target functions (e.g., the Gibbs phenomenon in Fourier series for discontinuous functions). To mitigate this issue, we introduce singularity into the \sine{}  function. A simple and natural approach is to truncate the \sine{}  function.
% Specifically, we propose a novel activation function called the  {Sine Truncated Unit (\SinTU)}, derived by truncating the \sine{}  function. Each \SinTU{} has a form of 
% $\sin\circ \calT_s$ where
% \[
% \calT_s(x) \coloneqq 
% \begin{cases} 
% x & \text{if } x \ge s, \\ 
% s & \text{if } x < s.
% \end{cases}
% \]
% \red{
% We note that the value of \( s \) (generally $\le 0$) is used to control the likelihood of singularities. As \( s \) decreases, \SinTU{} increasingly resembles the \sine{} function, thereby reducing the likelihood of singularities. In particular, \( s \) can be treated as a learnable parameter—either individually for each neuron or shared across all neurons. We refer to this variant of \SinTU{s} with a learnable \( s \) as \(\PSinTU\). A detailed exploration of \(\PSinTU\) is beyond the scope of this paper and is left for future work.
% }

% \red{In this study, we theoretically demonstrate that \texttt{sine} and \SinTU{s} significantly improve both representation and training efficiency in MMNNs for two key reasons. First, each component in MMNNs is smooth. Second, the Fourier basis effectively approximates smooth functions.}

The two key components of a neural network are its architecture and the choice of activation function, both of which jointly determine its effectiveness. In our previous work \cite{ZZZZ-23}, we showed that shallow networks (i.e., those with a single hidden layer) employing various activation functions struggle to capture high-frequency components. This limitation arises from the strong correlations among activation functions (parameterized by weights and biases), leading to ill-conditioning and a bias against high frequencies in both representation and training.
While multi-layer networks enhance representational power through compositions of shallow networks, their architecture is crucial for training efficiency. Most training methods rely on first-order gradient descent techniques, which are inherently local and sensitive to the ill-conditioning of the cost function (in terms of the Hessian) with respect to a typical large number of parameters.
To address this limitation, we later introduced structured and balanced multi-component and multi-layer neural networks (MMNNs) in \cite{ZZZZ-24-MMNN}, building on insights from one-hidden-layer networks. MMNNs enhance training efficiency through a ``divide-and-conquer" approach, where complex functions are decomposed (through components) and composed (through layers) within the MMNN framework. Each component in MMNNs is designed to be a linear combination of randomized hidden neurons that is easy to train.
MMNNs offer a straightforward yet impactful modification of fully connected neural networks (FCNNs), also known as multi-layer perceptrons (MLPs), by integrating balanced multi-component structures. This design reduces the number of trainable parameters, improves training efficiency, and achieves substantially higher accuracy compared to conventional FCNNs \cite{ZZZZ-24-MMNN}.
The structure of MMNNs is described in detail in Section~\ref{sec:MMNN:structure}.

In this work, we further investigate the behavior and potential of MMNNs and demonstrate a surprising discovery that the \sine{} function serves as an exceptionally effective activation function for MMNNs. 
% We believe this finding represents a significant step forward in understanding MMNNs, particularly from a Fourier analysis perspective.
% A linear combination of parameterized \sine{} activation functions can accurately approximate each smooth component, while multi-layer compositions effectively generate high-frequency components. 
Each component in an MMNN is fundamentally a linear combination of parameterized \sine{} activation functions and can therefore be viewed as a Fourier series, albeit with relatively small frequency parameters. As a result, each component in an MMNN facilitate the efficient and accurate approximation of a smooth function, while multi-layer compositions can effectively produce high-frequency components, enabling the network to capture more complex function structures with efficient training. Moreover, FMMNN can effectively approximate not only the function but also its derivatives, which can be very important in practice.
In the case of approximating a non-smooth function, Fourier approximation can be less effective or result in Gibbs phenomenon. At the same time, as demonstrated in our previous study \cite{ZZZZ-23} on shallow networks, activation functions with singularities, such as \texttt{ReLU}, can enhance representational capacity. 
To address this issue, we introduce a \texttt{ReLU} type of singularity by truncating \sine{}, leading to a novel hybrid activation function called the Sine Truncated Unit (\SinTU). Each \SinTU{} has a form of 
$\SinTU_s\coloneqq \sin\circ \calT_s$, where
% \[
% \calT_s(x) \coloneqq 
% \begin{cases} 
% x & \text{if } x \ge s, \\ 
% s & \text{if } x < s.
% \end{cases}
% \]
% \(\calT_s(x) \coloneqq 
% \begin{cases} 
% x & \text{if } x \ge s, \\ 
% s & \text{if } x < s.
% \end{cases}\)
% \(\calT_s(x) \coloneqq 
% \left\{\begin{smallmatrix*}[l] x &\; \text{if } x \ge s, \\ s & \; \text{if } x < s. \end{smallmatrix*}\right.\)
\(\calT_s(x) \coloneqq \max\{x,\, s\}.\)
The parameter \( s \) (typically \( \leq 0 \)) controls the occurrence of singularities and the balance of the hybridization. As \( s \) decreases, $\SinTU_s$ increasingly resembles the \sine{} function, reducing singularity effects. Moreover, \( s \) can be treated as a learnable parameter, either individually for each neuron or shared across all neurons. This variant, denoted as \(\PSinTU\), is an avenue for future exploration, as its detailed analysis falls beyond the scope of this paper.


% We illustrate that the structure of MMNNs and \sine{}  (or \SinTU{s}) forms a very effective synergy. 
% To demonstrate this, we:
% \begin{itemize}
% \item prove the powerful approximation capability of MMNNs activated by \sine{}  or sinTus;
% \item show that the learning landscape of MMNNs activated by \sine{}  or sinTus is significantly simpler than that of FCNNs with the same activations;
% \item validate the effectiveness of MMNNs activated by \sine{}  or sinTus through extensive experiments.
% \end{itemize}
% We illustrate that the structure of MMNNs and \sine{}  (or \SinTU{s}) forms a very effective synergy, particular efficiently capture high-freqnercy  components.
% \begin{itemize}
%     \item First, we show that the use of \texttt{sine} or \SinTU{s} as the activation function in the MMNN structure has great mathematical potential in terms of approximation capability (see Theorems~\ref{thm:main1} and \ref{thm:main2}).  
%     \item Next,
% we show the landscape of the cost function in terms of the network parameters, an indication of the training complexity in practice, for various network structures and using different activation functions. In particular, the MMNN structure leads to a much more benign landscape in terms of the parameters than FCNNs (see Section~\ref{sec:landscape}).  
% \item  Moreover,  together with extensive numerical experiments, we show that FMMNNs (Fourier MMNNs), using \texttt{sine} or \SinTU{s} as the activation function in MMNNs, produces the best performance in terms of accuracy and efficiency in general (see Section~\ref{sec:experiments}). 
% \end{itemize}

% We demonstrate that the combination of the MMNN structure with \sine{} (or \SinTU{s}) creates a highly effective synergy, particularly in efficiently capturing high-frequency components.
% We call MMNNs acticated by \sine{} (or \SinTU{s}) as 
% Fourier MMNNs (FMMNNs).

We demonstrate that integrating the MMNN structure with \sine{} (or \SinTU{s}) creates a surprising effective synergy, particularly for efficiently capturing high-frequency components.
% We refer to MMNNs activated by \sine{} (or \SinTU{s}) as Fourier MMNNs (FMMNNs).

\begin{itemize}
\item First, we establish that using \texttt{sine} or \SinTU{s} as activation functions within the MMNNs framework offers significant mathematical potential in terms of approximation capability.
In particular, given a $1$-Lipschitz function $f:[0,1]^d\to \mathbb{R}$ and a \SinTU{} function $\varrho$, for any $p \in [1,\infty)$, there exists   $\phi$ realized by an $\varrho$-activated MMNN of width $2d(4N-1)$, rank $3d$, and depth $L+2$, such that
\[
\| \phi - f\|_{L^p([0,1]^d)} \leq  2\sqrt{d} \cdot N^{-L}.
\]
For the generalized version (applicable to generic continuous functions) and the \sine-related version, see Theorems~\ref{thm:main1} and \ref{thm:main2}.








\item Next, we analyze the landscape of the cost function with respect to network parameters, which provides insight into the training complexity across different network architectures and activation functions. Notably, the MMNN structure results in a significantly more favorable optimization landscape compared to FCNNs, as illustrated in Figure~\ref{fig:landscape:intro:v} (see Section~\ref{sec:landscape} for further details).

\begin{figure}[ht]%[htbp!]  
            \centering
            \begin{subfigure}[b]{0.242046\textwidth}
                    \centering            
                    \includegraphics[width=0.9997\textwidth]{figures/LandScapeFCNN1Act1.pdf}
                    \subcaption{\Sine{} FCNN.}
                \end{subfigure}
                \hfill
            \begin{subfigure}[b]{0.242046\textwidth}
                    \centering            \includegraphics[width=0.9997\textwidth]{figures/LandScapeMMNN1Act1.pdf}
                    \subcaption{\Sine{} MMNN.}
                \end{subfigure}
\hfill
                \begin{subfigure}[b]{0.24207300245\textwidth}
                    \centering            
                    \includegraphics[width=0.999\textwidth]{figures/LandScapeFCNN1Act2.pdf}
                    \subcaption{$\SinTU_0$ FCNN.}
                \end{subfigure}
                \hfill
            \begin{subfigure}[b]{0.242\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/LandScapeMMNN1Act2.pdf}
                    \subcaption{$\SinTU_0$ MMNN.}
                \end{subfigure}             
                \caption{Comparison of the cost function landscapes in terms of two parameters.
                % for FCNNs and MMNNs. 
                }
\label{fig:landscape:intro:v}
\end{figure}

\item 
Finally, extensive numerical experiments demonstrate that Fourier MMNNs (FMMNNs), which use \texttt{sine} or \SinTU{s} as activation functions, consistently outperform other models in both accuracy and efficiency, as shown in Table~\ref{tab:error:comparison:MMNNs:vs:FCNNs:intro}. For \( f_1 \), \sine{}-activated MMNNs achieve the best result, aligning with expectations since \( f_1 \) is \( C^\infty(\mathbb{R}) \) (see Figure~\ref{fig:f1:f2:intro}). The accurate approximation of derivatives is particularly noteworthy, given the complexity of \( f_1 \) and the fact that
the training process relies
solely on function values, without incorporating derivative information.
% the cost function is based solely on function values, without incorporating \( f_1^\prime \) or \( f_1^\dprime \). 
For \( f_2 \in C^0(\mathbb{R}) \backslash C^1(\mathbb{R}) \) (see Figure~\ref{fig:f1:f2:intro}), which contains numerous singularities, \( \SinTU_{-\pi} \) achieves the best accuracy, demonstrating its effectiveness in capturing these singular features.
Notably, even in this inherently challenging case, \sine{}-activated MMNNs still achieve results comparable to the best. 
%This suggests that when the properties of the target function are uncertain, choosing \sine{} activation in MMNNs as an initial strategy is reasonable.
For \( f_3 \in C^0(\mathbb{R}) \backslash C^1(\mathbb{R}) \) (see Figure~\ref{fig:f1:f2:intro}), FCNNs are highly sensitive to training hyperparameters and often fail with small mini-batches. In contrast, FMMNNs remain stable and perform well across different settings. Even with large mini-batches, training FCNNs is time-consuming, yet they still underperform compared to \sine-activated MMNNs.
For more details on the experiments, including additional tests in two and three dimensions, refer to Section~\ref{sec:experiments}.

% It would

% Finally, extensive numerical experiments demonstrate that Fourier MMNNs (FMMNNs), which use \texttt{sine} or \SinTU{s} as activation functions within MMNNs, outperform other models in both accuracy and efficiency, as shown in Table~\ref{tab:error:comparison:MMNNs:vs:FCNNs}.  
% For \( f_1 \), sine-activated MMNNs achieve the best results, which aligns with expectations since \( f_1 \) is \( C^\infty(\mathbb{R}) \) (see Figrue~\ref{fig:f1:f2:intro}). The precise approximation of derivatives is particularly noteworthy, given the complexity of \( f_1 \) and the fact that the cost function is based solely on function values, without incorporating \( f_1^\prime \) or \( f_1^\dprime \).  
% For \( f_2 \) (see Figrue~\ref{fig:f1:f2:intro}), \SinTU{s} yield the highest accuracy, as expected, due to their ability to effectively capture singularities in \( f_2 \). Remarkably, even in this unfavorable scenario, \sine{} MMNNs still perform well. Thus, when the properties of the target function are uncertain, using sine activation as an initial choice is a practical strategy.  
% For further details on the experiments, refer to Section~\ref{sec:experiments}.

% Finally, through extensive numerical experiments, we demonstrate that Fourier MMNNs (FMMNNs), which utilize \texttt{sine} or \SinTU{s} as activation functions within MMNNs, achieve superior performance in terms of both accuracy and efficiency, as shown in Table~\ref{tab:error:comparison:MMNNs:vs:FCNNs}.
% For \( f_1 \), sine-activated MMNNs perform best, aligning with expectations since \( f_1 \) is \( C^\infty(\mathbb{R}) \) (see \eqref{fig:f1:f2:intro}). The accurate approximation of derivatives is remarkable, given the complexity of \( f_1 \) and the fact that the cost function relies solely on function values, without incorporating \( f_1^\prime \) or \( f_1^\dprime \).  
% For \( f_2 \) (see \eqref{fig:f1:f2:intro}), \SinTU{s} achieve the best performance, as expected, since they effectively handle singularities in \( f_2 \). Notably, even in this unfavorable setting, \sine{} MMNNs still perform well. Thus, when the target function's properties are uncertain, trying sine activation first is a reasonable strategy.  
% For more details on the experiments, see Section~\ref{sec:experiments}.

\end{itemize}

\begin{figure}[ht]%[htbp!]  
            \centering
  \begin{subfigure}[b]{0.95\textwidth}
                    \centering            
                    \includegraphics[width=0.999\textwidth]{figures/MMNNvsFCNN_f1Intro.pdf}
                    % \subcaption{$f$.}
                \end{subfigure}
            \begin{subfigure}[b]{0.95\textwidth}
                    \centering            
                    \includegraphics[width=0.999\textwidth]{figures/MMNNvsFCNN_f2funcIntro.pdf}
                    % \subcaption{$f$.}
                \end{subfigure}
            \begin{subfigure}[b]{0.95\textwidth}
                    \centering            
                    \includegraphics[width=0.999\textwidth]{figures/MMNNvsFCNN_f3funcIntro.pdf}
                    % \subcaption{$f$.}
                \end{subfigure}
\caption{Illustrations of $f_1\in C^\infty(\R)$ and $f_2,f_3\in C^0(\R)\backslash 
 C^1(\R)$, defined in \eqref{eq:def:f1:Cinfty:MMNN:vs:FCNN}, \eqref{eq:def:f2:C0:MMNN:vs:FCNN}, and \eqref{eq:def:f3:C0:MMNN:vs:FCNN}.}
    \label{fig:f1:f2:intro}   
\end{figure}

\begin{table}[ht]%[htbp!]
	\centering  
 \setlength{\tabcolsep}{0.868em} 
 \renewcommand{\arraystretch}{1.15}
\caption{Comparison of test errors. Training is conducted in double precision.
% measured in the mean squared error (MSE) and $L^\infty$-norm (MAX). 
% The errors for derivatives are reported as relative values. The training process relies solely on function values, without incorporating derivative information.
}
\label{tab:error:comparison:MMNNs:vs:FCNNs:intro}
	\resizebox{0.880\textwidth}{!}{ 
		\begin{tabular}{ccccccc@{\hspace{10pt}}c} 
			\toprule% \toprule[1.2pt]  
            & 
            &\multicolumn{4}{c}{
            % networks with 
            $\bm{6}$ hidden layers and $\bm{7.3\times 10^{4}}$ trainable parameters } & & \\
            % \cmidrule(lr){5-8}
            \cmidrule(lr){3-6}
        & 
        &\multicolumn{2}{c}{MMNN} 
        &\multicolumn{2}{c}{FCNN} & 
        \multicolumn{2}{c}{\#training-samples}
        \\
            \cmidrule(lr){3-4}
            \cmidrule(lr){5-6}
            \cmidrule(lr){7-8}
      target & 
      {activation}
      &
      MSE 
    &  MAX 
    &    MSE 
    &  MAX   &
      % mini-batch size & \#samples (training)
      mini-batch &  all
    \\
			\midrule
$f_1$ & $\mathtt{ReLU}$ &  $ 2.24 \times 10^{-5} $  &  $ 4.06 \times 10^{-2} $  &  $ 2.31 \times 10^{-4} $  &  $ 1.93 \times 10^{-1} $  &  3000  & 3000  
 \\ 

\rowcolor{mygray}$f_1$ & $\mathtt{tanh}$ &  $ 4.91 \times 10^{-6} $  &  $ 1.24 \times 10^{-2} $  &  $ 2.67 \times 10^{-3} $  &  $ 3.69 \times 10^{-1} $  &  3000  & 3000  
 \\ 

 $f_1$ & $\mathtt{sine}$ &  $ \bm{3.43 \times 10^{-8}} $  &  $ \bm{8.37 \times 10^{-4}} $  &  $ 2.62 \times 10^{-5} $  &  $ 2.35 \times 10^{-2} $  &  3000  & 3000  
 \\ 

\rowcolor{mygray}$f_1$ & $\mathtt{SinTU}_{-\pi}$ &  $ 3.65 \times 10^{-7} $  &  $ 4.90 \times 10^{-3} $  &  $ 4.14 \times 10^{-4} $  &  $ 1.44 \times 10^{-1} $  &  3000  & 3000  
 \\ 
 
 \midrule

$f^\prime_1$ & $\mathtt{tanh}$ &  $ 1.24 \times 10^{-4} $  &  $ 5.67 \times 10^{-2} $  &  $ 1.10 \times 10^{-2} $  &  $ 6.31 \times 10^{-1} $  &    &   
 \\ 

\rowcolor{mygray}$f^\prime_1$ & $\mathtt{sine}$ &  $ 1.27 \times 10^{-6} $  &  $ 5.12 \times 10^{-3} $  &  $ 5.83 \times 10^{-4} $  &  $ 1.06 \times 10^{-1} $  &    &   
 \\ 
\midrule
 $f^\dprime_1$ & $\mathtt{tanh}$ &  $ 9.92 \times 10^{-4} $  &  $ 1.59 \times 10^{-1} $  &  $ 3.02 \times 10^{-2} $  &  $ 8.62 \times 10^{-1} $  &    &   
 \\ 

\rowcolor{mygray}$f^\dprime_1$ & $\mathtt{sine}$ &  $ 7.82 \times 10^{-6} $  &  $ 1.37 \times 10^{-2} $  &  $ 4.45 \times 10^{-3} $  &  $ 2.71 \times 10^{-1} $  &    &   
 \\ 
 
 \midrule
 
$f_2$ & $\mathtt{ReLU}$ &  $ 1.53 \times 10^{-3} $  &  $ 4.37 \times 10^{-1} $  &  $ 2.13 \times 10^{-2} $  &  $ 6.35 \times 10^{-1} $  &  3000 & 60000  
 \\ 

\rowcolor{mygray}$f_2$ & $\mathtt{tanh}$ &  $ 1.42 \times 10^{-4} $  &  $ 1.51 \times 10^{-1} $  &  $ 1.21 \times 10^{-2} $  &  $ 5.56 \times 10^{-1} $  &  3000 & 60000  
 \\ 

 $f_2$ & $\mathtt{sine}$ &  $ 4.84 \times 10^{-6} $  &  $ 2.88 \times 10^{-2} $  &  $ 9.07 \times 10^{-5} $  &  $ 9.22 \times 10^{-2} $  &  3000 & 60000  
 \\ 

\rowcolor{mygray}$f_2$ & $\mathtt{SinTU}_{-\pi}$ &  $ \bm{1.28 \times 10^{-6}} $  &  $ \bm{2.31 \times 10^{-2}} $  &  $ 6.14 \times 10^{-3} $  &  $ 5.31 \times 10^{-1} $  &  3000 & 60000  
 \\ 


 \midrule  

 $f_3$ & $\mathtt{sine}$ &  $ 7.68 \times 10^{-8} $  &  $ 6.06 \times 10^{-3} $  &  $ 3.04 \times 10^{-2} $  &  $ 6.99 \times 10^{-1} $  & 500 & 18000  
 \\ 

\rowcolor{mygray} $f_3$ & $\mathtt{sine}$ &  $ 1.27 \times 10^{-7} $  &  $ 6.58 \times 10^{-3} $  &  $ 6.69 \times 10^{-2} $  &  $ 6.68 \times 10^{-1} $  & 1000 & 18000  
 \\ 

  $f_3$ & $\mathtt{sine}$ &  $ 8.75 \times 10^{-8} $  &  $ 6.28 \times 10^{-3} $  &  $ 2.43 \times 10^{-4} $  &  $ 1.52 \times 10^{-1} $  & 1500 & 18000  
 \\ 

\rowcolor{mygray} $f_3$ & $\mathtt{sine}$ &  $ \bm{6.41 \times 10^{-8}} $  &  $ \bm{5.33 \times 10^{-3}} $  &  $ 5.86 \times 10^{-6} $  &  $ 3.38 \times 10^{-2} $  & 2000 & 18000  
 \\ 
 \bottomrule% \bottomrule[1.2pt] 
		\end{tabular} 
	}%%% \resizebox
% 	\vskip -0.1in
\end{table} 

% \begin{table}[ht]%[htbp!]
% 	\centering  
%  \setlength{\tabcolsep}{0.868em} 
%  \renewcommand{\arraystretch}{1.15}
% \caption{Comparison of test errors.
% % measured in the mean squared error (MSE) and $L^\infty$-norm (MAX). 
% % The errors for derivatives are reported as relative values. The training process relies solely on function values, without incorporating derivative information.
% }
% \label{tab:error:comparison:MMNNs:vs:FCNNs:intro}
% 	\resizebox{0.7580\textwidth}{!}{ 
% 		\begin{tabular}{ccccccccc} 
% 			\toprule% \toprule[1.2pt]  
%             &
%             &\multicolumn{4}{c}{
%             % networks with 
%             $\bm{6}$ hidden layers and $\bm{7.3\times 10^{4}}$ trainable parameters } \\
%             \cmidrule(lr){3-6}
%         &
%         &\multicolumn{2}{c}{MMNN} 
%         &\multicolumn{2}{c}{FCNN}
%         \\
%             \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%       target &   {activation} &          MSE 
%     &  MAX 
%     &    MSE 
%     &  MAX 
%     \\
% 			\midrule
%  $f_1$ & $\mathtt{ReLU}$ &  $ 2.24 \times 10^{-5} $  &  $ 4.06 \times 10^{-2} $  &  $ 2.31 \times 10^{-4} $  &  $ 1.93 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f_1$ & $\mathtt{tanh}$ &  $ 4.91 \times 10^{-6} $  &  $ 1.24 \times 10^{-2} $  &  $ 2.67 \times 10^{-3} $  &  $ 3.69 \times 10^{-1} $ 
%  \\ 

%  $f_1$ & $\mathtt{sine}$ &  $ \bm{3.43 \times 10^{-8}} $  &  $ \bm{8.37 \times 10^{-4} }$  &  $ 2.62 \times 10^{-5} $  &  $ 2.35 \times 10^{-2} $ 
%  \\ 

% \rowcolor{mygray}$f_1$ & $\mathtt{SinTU}_{-\pi}$ &  $ 3.65 \times 10^{-7} $  &  $ 4.90 \times 10^{-3} $  &  $ 4.14 \times 10^{-4} $  &  $ 1.44 \times 10^{-1} $ 
%  \\ 
 
%  \midrule

% $f^\prime_1$ & $\mathtt{tanh}$ &  $ 1.24 \times 10^{-4} $  &  $ 5.67 \times 10^{-2} $  &  $ 1.10 \times 10^{-2} $  &  $ 6.31 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f^\prime_1$ & $\mathtt{sine}$ &  $ \bm{1.27 \times 10^{-6} }$  &  $\bm{ 5.12 \times 10^{-3}} $  &  $ 5.83 \times 10^{-4} $  &  $ 1.06 \times 10^{-1} $ 
%  \\ 
% \midrule
%  $f^\dprime_1$ & $\mathtt{tanh}$ &  $ 9.92 \times 10^{-4} $  &  $ 1.59 \times 10^{-1} $  &  $ 3.02 \times 10^{-2} $  &  $ 8.62 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f^\dprime_1$ & $\mathtt{sine}$ &  $ \bm{7.82 \times 10^{-6} }$  &  $\bm{ 1.37 \times 10^{-2} }$  &  $ 4.45 \times 10^{-3} $  &  $ 2.71 \times 10^{-1} $ 
%  \\ 
 
%  \midrule
 
% $f_2$ & $\mathtt{ReLU}$ &  $ 1.53 \times 10^{-3} $  &  $ 4.37 \times 10^{-1} $  &  $ 2.13 \times 10^{-2} $  &  $ 6.35 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f_2$ & $\mathtt{tanh}$ &  $ 1.42 \times 10^{-4} $  &  $ 1.51 \times 10^{-1} $  &  $ 1.21 \times 10^{-2} $  &  $ 5.56 \times 10^{-1} $ 
%  \\ 

%  $f_2$ & $\mathtt{sine}$ &  $ 4.84 \times 10^{-6} $  &  $ 2.88 \times 10^{-2} $  &  $ 9.07 \times 10^{-5} $  &  $ 9.22 \times 10^{-2} $ 
%  \\ 

% \rowcolor{mygray}$f_2$ & $\mathtt{SinTU}_{-\pi}$ &  $ \bm{1.28 \times 10^{-6} }$  &  $ \bm{2.31 \times 10^{-2}} $  &  $ 6.14 \times 10^{-3} $  &  $ 5.31 \times 10^{-1} $ 
%  \\ 
%  \bottomrule% \bottomrule[1.2pt] 
% 		\end{tabular} 
% 	}%%% \resizebox
% % 	\vskip -0.1in
% \end{table} 

% \begin{table}[ht]%[htbp!]
% 	\centering  
%  \setlength{\tabcolsep}{0.68em} 
%  \renewcommand{\arraystretch}{1.15}
% \caption{Comparison of test errors. Errors for derivatives are relative errors, as absolute errors for derivatives can be misleading (the \( L^\infty \)-norm of \( f_1^\dprime \) exceeds 70000). The cost function is based only on function values, without incorporating derivatives in the optimization.  
% }


% \label{tab:error:comparison:MMNNs:vs:FCNNs:intro}
% 	\resizebox{0.95\textwidth}{!}{ 
% 		\begin{tabular}{ccccccccc} 
% 			\toprule% \toprule[1.2pt]  
%             % \multicolumn{2}{c}
%             % {
%             % target function
%             % }
%             % & \multicolumn{2}{c}{$f_1:[-1,1]\to \R$}
%             % & \multicolumn{2}{c}{$f_2:[-1,1]^2\to \R$}
%             % & \multicolumn{2}{c}{$f_3:[-1,1]^2\to \R$}
%             % % & \multicolumn{2}{c}{$f_4:[-1,1]^3\to \R$}
%             % \\    
%             \multicolumn{2}{c}{\#parameters (trained / all)} &\multicolumn{2}{c}{35235 / \textbf{72993}} &\multicolumn{2}{c}{\textbf{72981} / 151281} &\multicolumn{2}{c}{\textbf{72961} / \textbf{72961}} \\
%             % \cmidrule(lr){1-2} 
            
%             \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%             % \cmidrule(lr){7-8}
%             % \cmidrule(lr){2-3}
%             % \cmidrule(lr){4-5}
%             % \cmidrule(lr){6-7}
%             % \cmidrule(lr){8-9}
%         \rowcolor{mygray}   
%         % \multicolumn{2}{c}{network} 
%         &
%         &\multicolumn{2}{c}{MMNN of size (434,16,6)} &\multicolumn{2}{c}{MMNN of size (900,16,6)} &\multicolumn{2}{c}{FCNN of size (120,--,6)}
%         % &\multicolumn{2}{c}{ResMMNN of size (1024,40,16)}
%         \\
%            %             \cmidrule(lr){3-4}
%            %  \cmidrule(lr){5-6}
%            %  \cmidrule(lr){7-8}
%            % & &\multicolumn{2}{c}{MMNN1}
%            %  &\multicolumn{2}{c}{MMNN2}
%            %  &\multicolumn{2}{c}{FCNN1}
%            %  \\
%             \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%             \cmidrule(lr){7-8}
%             % \cmidrule(lr){2-3}
%             % \cmidrule(lr){4-5}
%             % \cmidrule(lr){6-7}
%             % \cmidrule(lr){8-9}
% 			 %    target  &  activation &        TE (MSE) 
%     % & TE (MAX) &        TE (MSE) 
%     % & TE (MAX)
%     % &        TE (MSE) 
%     % & TE (MAX)\\
%     % \multicolumn{2}{c}
%       target function &   {activation} &          MSE 
%     &  MAX &    MSE 
%     &  MAX &    MSE 
%     &  MAX
%     % &   
%     % MSE 
%     % &  MAX
%     \\
    
%     % {activation} &        test error (MSE) 
%     % & test error (MAX) &        test error (MSE) 
%     % & test error (MAX)
%     % &        test error (MSE) 
%     % & test error (MAX)\\
% 			 % \cmidrule{3-5}
% 			 % & & MSE & MAE & MAX\\
% 			\midrule


% $f_1$ & $\mathtt{ReLU}$ &  $ 2.14 \times 10^{-5} $  &  $ 3.88 \times 10^{-2} $  &  $ 2.73 \times 10^{-5} $  &  $ 3.96 \times 10^{-2} $  &  $ 7.19 \times 10^{-5} $  &  $ 9.62 \times 10^{-2} $ 
%  \\ 

% \rowcolor{mygray}$f_1$ & $\mathtt{GELU}$ &  $ 8.20 \times 10^{-5} $  &  $ 8.22 \times 10^{-2} $  &  $ 6.78 \times 10^{-5} $  &  $ 5.06 \times 10^{-2} $  &  $ 3.25 \times 10^{-3} $  &  $ 5.45 \times 10^{-1} $ 
%  \\ 

%  $f_1$ & $\mathtt{tanh}$ &  $ 5.80 \times 10^{-5} $  &  $ 3.46 \times 10^{-2} $  &  $ 3.36 \times 10^{-6} $  &  $ 1.01 \times 10^{-2} $  &  $ 4.08 \times 10^{-3} $  &  $ 4.35 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f_1$ & $\mathtt{sin}$ &  $ 6.67 \times 10^{-7} $  &  $ 3.67 \times 10^{-3} $  &  $ \bm{3.79 \times 10^{-8} } $  &  $ \bm{9.38 \times 10^{-4}} $  &  $ 1.73 \times 10^{-5} $  &  $ 2.21 \times 10^{-2} $ 
%  \\ 

%  % $f_1$ & $\mathtt{cos}$ &  $ 1.30 \times 10^{-5} $  &  $ 1.56 \times 10^{-2} $  &  $ 1.08 \times 10^{-7} $  &  $ 2.14 \times 10^{-3} $  &  $ 1.88 \times 10^{-6} $  &  $ 6.26 \times 10^{-3} $ 
%  % \\ 

% % \rowcolor{mygray}$f_1$ & $\mathtt{SinTU}_{0}$ &  $ 1.14 \times 10^{-4} $  &  $ 9.74 \times 10^{-2} $  &  $ 5.15 \times 10^{-6} $  &  $ 1.90 \times 10^{-2} $  &  $ 3.68 \times 10^{-4} $  &  $ 2.19 \times 10^{-1} $ 
% %  \\ 

%  $f_1$ & $\mathtt{SinTU}_{-\pi}$ &  $ 1.16 \times 10^{-5} $  &  $ 2.79 \times 10^{-2} $  &  $ 2.71 \times 10^{-7} $  &  $ 4.92 \times 10^{-3} $  &  $ 1.91 \times 10^{-4} $  &  $ 1.09 \times 10^{-1} $ 
%  \\ 

% % \rowcolor{mygray}$f_1$ & $\mathtt{SinTU}_{-2\pi}$ &  $ 4.11 \times 10^{-6} $  &  $ 1.44 \times 10^{-2} $  &  $ 1.45 \times 10^{-7} $  &  $ 3.25 \times 10^{-3} $  &  $ 1.87 \times 10^{-5} $  &  $ 3.18 \times 10^{-2} $ 
% %  \\ 
%  \midrule




%  $f^\prime_1$ & $\mathtt{GELU}$ &  $ 1.78 \times 10^{-3} $  &  $ 3.46 \times 10^{-1} $  &  $ 1.72 \times 10^{-3} $  &  $ 2.19 \times 10^{-1} $  &  $ 1.51 \times 10^{-2} $  &  $ 8.54 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f^\prime_1$ & $\mathtt{tanh}$ &  $ 1.01 \times 10^{-3} $  &  $ 1.29 \times 10^{-1} $  &  $ 9.02 \times 10^{-5} $  &  $ 5.05 \times 10^{-2} $  &  $ 1.40 \times 10^{-2} $  &  $ 6.77 \times 10^{-1} $ 
%  \\ 

%  $f^\prime_1$ & $\mathtt{sin}$ &  $ 2.48 \times 10^{-5} $  &  $ 2.14 \times 10^{-2} $  &  $ \bm{1.43 \times 10^{-6} }$  &  $ \bm{5.72 \times 10^{-3}} $  &  $ 4.04 \times 10^{-4} $  &  $ 9.62 \times 10^{-2} $ 
%  \\ 

% % \rowcolor{mygray}$f^\prime_1$ & $\mathtt{cos}$ &  $ 3.25 \times 10^{-4} $  &  $ 7.42 \times 10^{-2} $  &  $ 4.19 \times 10^{-6} $  &  $ 1.28 \times 10^{-2} $  &  $ 6.30 \times 10^{-5} $  &  $ 3.39 \times 10^{-2} $ 
% %  \\ 

% \midrule
%  $f^\dprime_1$ & $\mathtt{GELU}$ &  $ 1.52 \times 10^{-2} $  &  $ 1.95 \times 10^{0} $  &  $ 1.31 \times 10^{-2} $  &  $ 1.19 \times 10^{0} $  &  $ 5.88 \times 10^{-2} $  &  $ 2.28 \times 10^{0} $ 
%  \\ 


% \rowcolor{mygray}$f^\dprime_1$ & $\mathtt{tanh}$ &  $ 7.10 \times 10^{-3} $  &  $ 3.19 \times 10^{-1} $  &  $ 7.54 \times 10^{-4} $  &  $ 1.40 \times 10^{-1} $  &  $ 3.28 \times 10^{-2} $  &  $ 7.70 \times 10^{-1} $ 
%  \\ 

%  $f^\dprime_1$ & $\mathtt{sin}$ &  $ 2.11 \times 10^{-4} $  &  $ 7.47 \times 10^{-2} $  &  $ \bm{8.77 \times 10^{-6}} $  &  $ \bm{1.54 \times 10^{-2}} $  &  $ 3.21 \times 10^{-3} $  &  $ 2.53 \times 10^{-1} $ 
%  \\ 

% % \rowcolor{mygray}$f^\dprime_1$ & $\mathtt{cos}$ &  $ 2.71 \times 10^{-3} $  &  $ 1.99 \times 10^{-1} $  &  $ 3.15 \times 10^{-5} $  &  $ 5.07 \times 10^{-2} $  &  $ 5.65 \times 10^{-4} $  &  $ 1.02 \times 10^{-1} $ 
% %  \\ 
%  \midrule



%  $f_2$ & $\mathtt{ReLU}$ &  $ 1.38 \times 10^{-2} $  &  $ 3.64 \times 10^{-1} $  &  $ 5.54 \times 10^{-3} $  &  $ 3.04 \times 10^{-1} $  &  $ 3.20 \times 10^{-2} $  &  $ 5.43 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f_2$ & $\mathtt{GELU}$ &  $ 2.30 \times 10^{-2} $  &  $ 5.00 \times 10^{-1} $  &  $ 2.64 \times 10^{-2} $  &  $ 5.00 \times 10^{-1} $  &  $ 3.49 \times 10^{-2} $  &  $ 4.99 \times 10^{-1} $ 
%  \\ 

%  $f_2$ & $\mathtt{tanh}$ &  $ 2.13 \times 10^{-3} $  &  $ 2.88 \times 10^{-1} $  &  $ 3.84 \times 10^{-3} $  &  $ 3.18 \times 10^{-1} $  &  $ 4.64 \times 10^{-2} $  &  $ 5.00 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f_2$ & $\mathtt{sin}$ &  $ 2.37 \times 10^{-5} $  &  $ 3.81 \times 10^{-2} $  &  $ 1.99 \times 10^{-5} $  &  $ 2.69 \times 10^{-2} $  &  $ 4.64 \times 10^{-2} $  &  $ 5.08 \times 10^{-1} $ 
%  \\ 

% %  $f_2$ & $\mathtt{cos}$ &  $ 5.10 \times 10^{-4} $  &  $ 2.02 \times 10^{-1} $  &  $ 4.27 \times 10^{-5} $  &  $ 4.30 \times 10^{-2} $  &  $ 1.77 \times 10^{-4} $  &  $ 8.92 \times 10^{-2} $ 
% %  \\ 

% % \rowcolor{mygray}$f_2$ & $\mathtt{SinTU}_{0}$ &  $ 1.21 \times 10^{-3} $  &  $ 2.77 \times 10^{-1} $  &  $ 4.43 \times 10^{-6} $  &  $ 1.87 \times 10^{-2} $  &  $ 2.71 \times 10^{-2} $  &  $ 5.18 \times 10^{-1} $ 
% %  \\ 

%  $f_2$ & $\mathtt{SinTU}_{-\pi}$ &  $ 2.95 \times 10^{-5} $  &  $ 4.65 \times 10^{-2} $  &  $ \bm{1.58 \times 10^{-6} } $  &  $ \bm{1.57 \times 10^{-2}} $  &  $ 4.91 \times 10^{-2} $  &  $ 5.67 \times 10^{-1} $ 
%  \\ 

% % \rowcolor{mygray}$f_2$ & $\mathtt{SinTU}_{-2\pi}$ &  $ 4.81 \times 10^{-5} $  &  $ 4.77 \times 10^{-2} $  &  $ 2.71 \times 10^{-6} $  &  $ \bm{1.54 \times 10^{-2}} $  &  $ 4.64 \times 10^{-2} $  &  $ 5.01 \times 10^{-1} $ 
% %  \\ 

%  \bottomrule% \bottomrule[1.2pt] 
% 		\end{tabular} 
% 	}%%% \resizebox
% % 	\vskip -0.1in
% \end{table} 


% Additionally, using \texttt{sine} as the activation function enables efficient approximation of derivatives by minimizing a cost function based on function values (Section~\ref{sec:derivative:approx}), a crucial feature in practice. This is unexpected, as \texttt{sine} in a one-hidden-layer network is known to be highly ill-conditioned for representation and training due to its smoothness (Section~\ref{sec:shallow:nets}). While some prior works have proposed \sine{} as an activation function in multi-layer networks \cite{2025arXiv250200869M} (\red{
% \cite{novello2024tamingfrequencyfactorysinusoidal} use some numerical examle to show benefit of sinusoidal FCNNs;
% \cite{fathony2021multiplicative} introduces multiplicative filter network using  sinusoidal filter}), they lack strong mathematical or numerical justification for its benefits, making \texttt{sine} rarely used in practice.  
% In this study, we demonstrate that \texttt{sine} and \SinTU{s} significantly improve both representation and training efficiency in MMNNs for two key reasons. First, each component in MMNNs is smooth. Second, the Fourier basis effectively approximates smooth functions.





% In addition, using \texttt{sine} or \SinTU{s} (Section~\ref{sec:derivative:approx}) as the activation function allows one to approximate the derivatives of the network representation easily, an important feature in practice. This is surprising since we know that the use of \texttt{sine} or \SinTU{s} (Section~\ref{sec:shallow:nets}) as the activation function in two layer network is very ill-conditioned in terms of representation and training due to its smoothness. Although some previous works have proposed the use of \sine{}  as the activation function in multi-layer networks \cite{2025arXiv250200869M}, there are no convincing arguments or evidence for its advantages, either mathematically or numerically. Hence \texttt{sine} and  \SinTU{s} are rarely used as the activation function in practice. In this study we demonstrate the advantage of using \texttt{sine} or \SinTU{s} as the activation function in MMNNs in terms of both representation and training due to the following two reasons: 1) each component in MMNNs is smooth, 2) Fourier basis approximates smooth functions effectively. 
%We use extensive numerical experiments to verify our study.   compare compared to the use of other popular activation functions such as ReLU, sigmod, tanh, tanh’, and tanh”. 

% In the case where the target function is (almost) discontinuous, the use of \texttt{sine} as the activation function may lead to Gibbs phenomena. We propose a hybrid approach, using a mixture of \texttt{sine} and \texttt{ReLU} as activation functions, and show that it can reduce Gibbs phenomena. Further investigation will be carried out in the future.

% Here we summarize our main contributions in this work.
% \begin{itemize}
% \item 
% Propose FMMNN, i.e., using \texttt{sine} as the activation function in MMNN.
% \item 
% Show the theoretical approximation potential of using \texttt{sine} as activation function.
% \item 
% Study the landscape of the lost function in terms of the network parameters.
% \item 
% Conduct extensive computational experiments to show the superior performance of FMMNN for (rather complicated) function approximation. 
% \end{itemize}


%The paper is organized as follows. In Section 2, we show the mathematical potential of using \texttt{sine} as the activation function in terms of approximation. In Section 3, we demonstrate the landscape of the loss function in terms of the network parameters for various network structures and using different activation functions. In particular, it shows why \texttt{sine} as activation function combines well with MMNN structure. We also show why fixing the weights inside the activation function in MMNN is a good strategy. In Section 4, we discuss a hybrid approach for MMNN. In Section 5 we use extensive numerical experiments to verify our study.

% The paper is structured as follows: Section~\ref{sec:sine:potential} study the mathematical potential of using \texttt{sine} and a hybrid use of \texttt{ReLU} and \texttt{sine} as activation functions respectively, in terms of approximation capabiity. Section~\ref{sec:landscape} examines the loss function landscape across various network structures and activation functions, highlighting the compatibility of the \texttt{sine} function with the MMNN structure and the advantages of fixing weights within the activation function in MMNN. Section~\ref{sec:experiments} presents extensive numerical experiments to validate our findings. Specifically, Section~\ref{sec:sine_vs_others} compares the performance of the \sine{}  activation function with others, while Section~\ref{sec:hybrid_activation} tests  a hybrid approach for MMNN. 
% Section~\ref{sec:proof:thm:main} offers a detailed proof of the theorem introduced in Section~\ref{sec:sine:potential}. 
% Finally, Section~\ref{sec:conclusion} concludes the paper with a brief discussion.


% The structure of the paper is as follows. In Section~\ref{sec:sine:potential}, we first demonstrate that MMNNs activated by \texttt{sine} or \SinTU{s} exhibit strong approximation capabilities. We then explore the practical aspects by analyzing the cost function landscapes, emphasizing the synergy between the \texttt{sine} function and the MMNN architecture, as well as the benefits of fixing weights within the activation functions in MMNNs. 
% Section~\ref{sec:experiments} presents extensive numerical experiments to support our theoretical findings. Section~\ref{sec:proof:thm:main} provides detailed proofs of the theorems introduced in Section~\ref{sec:sine:potential}, based on several propositions, whose proofs are given in Section~\ref{sec:proof:props}. 
% Finally, Section~\ref{sec:conclusion} concludes the paper with a brief discussion.



The paper is structured as follows. Section~\ref{sec:sine:potential} provides a detailed analysis of the MMNN architecture, demonstrating its strong approximation capabilities when using \texttt{sine} and \SinTU{s} as activation functions. This section also explores key practical aspects, including the cost function landscape and the interaction between \texttt{sine} and MMNNs, highlighting the advantages of keeping weights fixed within activation functions.
% We then further discuss FMMMNs from the perspective of composing shallow networks. 
The section concludes with a discussion of related work.
Section~\ref{sec:experiments} presents extensive numerical experiments that support our theoretical findings. Section~\ref{sec:proof:thm:main} provides rigorous proofs for the theorems introduced in Section~\ref{sec:sine:potential}, supplemented by several propositions, whose proofs are detailed in Section~\ref{sec:proof:props}.
Finally, Section~\ref{sec:conclusion} concludes the paper with a brief discussion.


% The paper is organized as follows. Section~\ref{sec:sine:potential} provides an in-depth analysis of the MMNN architecture, showcasing its strong approximation capabilities with \texttt{sine} and \SinTU{s} as activation functions. We then examine key practical aspects, including the cost function landscape and the interaction between \texttt{sine} and MMNNs, emphasizing the benefits of keeping weights within activation functions fixed. The section concludes with a discussion of related work.
% Section~\ref{sec:experiments} presents extensive numerical experiments that validate our theoretical findings. Section~\ref{sec:proof:thm:main} offers rigorous proofs of the theorems introduced in Section~\ref{sec:sine:potential}, supported by several propositions, whose proofs are detailed in Section~\ref{sec:proof:props}.
% Finally, Section~\ref{sec:conclusion} wraps up the paper with a brief discussion.


% \red{
% \begin{itemize}
%     \item 
%     ReLU, tanh, tanh’, and tanh”;  sin trun

% \item FCNN vs MMNN
% \end{itemize}
% }





% \newpage
% \begin{itemize}
%     \item Spectrum decay: ReLU and Sin (almost -4) 
%     \item Approximation: ReLU and Sin  better than ReLU
% \end{itemize}

% \begin{itemize}
%     \item FCNN: sin local minia; ReLU better \href{https://arxiv.org/abs/2407.21121}{https://arxiv.org/abs/2407.21121}
%     \item MMNN (large size): sin better
%     \item MMNN (small size): ReLU better
% \end{itemize}

% Try EUAF

% Papers:  overcome the challenge of high-frequency

% 1.  Phase shift DNN:  
% \url{https://arxiv.org/abs/1909.11759}

% 2. Adaptive activation functions: \url{https://arxiv.org/abs/1906.01170}

% 3. Multi-scale DNN: \url{https://arxiv.org/abs/2007.11207}

% 4. Fourier feature network: \url{https://arxiv.org/abs/2006.10739}

% 5. Multi-stage neural network: \url{https://arxiv.org/abs/2307.08934}

% 6. Taming the Frequency Factory of Sinusoidal Networks:
% \url{https://arxiv.org/pdf/2407.21121}

% 7. Implicit Neural Representations with Periodic
% Activation Functions \url{https://proceedings.neurips.cc/paper/2020/file/53c04118df112c13a8c34b38343b9c10-Paper.pdf}

% 8. \sine{} KAN: KOLMOGOROV-ARNOLD NETWORKS USING
% SINUSOIDAL ACTIVATION FUNCTIONS \url{https://arxiv.org/pdf/2407.04149}

% \red{
% Oct 11

% Numerically compare norms of gradient w.r.t $w_{i,j}$ vs $a_{i,j}$

% one hidden layer sin network vs ReLU; several bumps; behavior of $b_i$; $x-b_i$;  or  $w_ix-b_i$, see $b_i/w_i$;

% target $\sin(5\pi x)$
% }

% \red{Oct 18; richness of activation functions; trainable cubic Spline as activation function }

% \red{Oct 31;  
% try ReLU (first several layers) and sin (last several layers)

% Use
% \begin{equation*}
%     g(x)=u\sin\big(v\sin(wx)\big)
% \end{equation*}
% as activation function, where 
% $u,v,w$ are learnable.

% try two hidden layers and three.

% Nov 7: why efficiently learn piecewise constant (Only ReLU, ReLU and sin; only sin); KAN


% Nov 14: piecewise constant [R, S, S, S] well, how to explain;
% optimization landscape Section~\ref{sec:landscape}
% }

% \begin{theorem}
% Given a continuous function $f \in C([0,1]^d)$, for any $n \in \mathbb{N}^+$ and $p \in [1,\infty]$, there exists a function $\phi$ implemented by a ReLU/Sin-activated FCNN (or MMNN) with $n$ paramters such that
% \[
% \| f - \phi \|_{L^p([0,1]^d)} \leq  \omega_f\big(e^{-C\sqrt{n}}\big).
% \]
% \end{theorem}


% \begin{theorem}
% \label{thm:main}
% Given a continuous function $f \in C([0,1]^d)$, for any $N,L \in \mathbb{N}^+$ and $p \in [1,\infty]$, there exists a function $\phi$ implemented by a $(\Sin;0.5;\ReLU)$-activated FCNN (or MMNN) of width $O(N)$ and depth $O(L)$ such that
% \[
% \| f - \phi \|_{L^p([0,1]^d)} \leq  \omega_f\big(N^{-L}\big).
% \]
% \end{theorem}


% $N^{-L}$;  constant given

% \newpage

% \tableofcontents



\section{The Potential of the \Sine{} Activation Function}
\label{sec:sine:potential}

In this section, we first provide a detailed analysis of the MMNN architecture in Section~\ref{sec:MMNN:structure}, followed by an exploration of its mathematical approximation potential when using \texttt{sine} and \SinTU{s} as activation functions in Section~\ref{sec:approx:power}. In Section~\ref{sec:landscape}, we examine key practical considerations, including the cost function landscape and the interaction between \texttt{sine} and MMNNs, highlighting the benefits of keeping weights within activation functions fixed. 
% We then further discuss FMMMNs from the perspective of composing shallow networks. 
The section concludes with a discussion of related work in Section~\ref{sec:related:work}.
% we first demonstrate that MMNNs activated by \texttt{sine} or \SinTU{s} possess powerful approximation capabilities in Section~\ref{sec:approx:power}, followed by a discussion of their practical aspects in Section~\ref{sec:landscape}.


% \red{Introduce Architecture of MMNNs again in this paper?}

% \blue{
% Try 
% \begin{equation*}
%     f(x)=\sin(10\pi x)+ \frac{\sin(100\pi x)}{100}
% \end{equation*}
% and 
% \begin{equation*}
%     f(x)=\sin(\pi x)+ \frac{\sin(100\pi x)}{100}
% \end{equation*}
% }




% We introduce a novel activation function called \sine{}  Truncated Unit (\SinTU), derived by truncating the \sine{}  function. Specifically, an activation function is identified as a \SinTU\ if it is defined as follows

% $$
% \varrho_s(x) = 
% \begin{cases} 
% \sin(x) & \text{if } x \ge s, \\ 
% \sin(s) & \text{if } x < s.
% \end{cases}
% $$
% In other words, the set of \SinTU{}s
% is given by 
% \begin{equation*}
%     \calS\coloneqq \{\varrho_s: s\in\R\}.
% \end{equation*}

% \subsection{Introduction of MMNN Structure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Structure of MMNNs}
% \label{sec:MMNN:structure}

% We will demonstrate that MMNNs activated by \texttt{sine} or \SinTU{s} possess exponential approximation capabilities. 
% Before presenting the main results, let us first introduce the architecture of MMNNs. 
% An MMNN is defined as a multi-layer composition of functions $\bmh_i$, formally represented as $\bmh: \R^{d_0}\to \R^{d_m}$ with
% \begin{equation}
%     \label{eq:h:MMNN}
%     \bmh = \bmh_m \circ \bmh_{m-1} \circ \cdots \circ \bmh_1,
% \end{equation}
% where each layer $\bmh_i: \R^{d_{i-1}} \to \R^{d_i}$ corresponds to a multi-component shallow network with width $n_i$ and $d_i$ components, given by
% \begin{equation*}
%     \bmh_i(\bmx) = \bmA_i \, \sigma(\bmW_i \bmx + \bmb_i) + \bmc_i,
% \end{equation*}
% where $\bmW_i \in \R^{n_i \times d_{i-1}}$, $\bmb_i\in \R^{n_i}$, $\bmA_i \in \R^{d_i \times n_i}$, and $\bmc_i\in\R^{d_i}$. Here $\sigma(\bmW_i{[j,:]} \cdot \bmx + \bmb_i[j]) : j = 1, 2, \dots, n_i$, is a family of $n_i$ hidden neurons used as a set of randomly parametrized basis functions in $\R^{d_{i-1}}$. Each $\bmA_i[k,:]\sigma(\bmW_i\bmx+\bmb_i) +\bmc_i[k]$ for $ k=1,2, \ldots, d_i$ is a linear combination of these basis, called a component. In each layer, we let the number of components $d_{i-1}$, called rank, be much smaller than the number of hidden neurons $n_i$, called the layer width. This setup of using a family of random basis functions that are diverse enough, due to $n_i\gg d_{i-1}$, and well conditioned, due to random parametrization, result in easy training of $\bmA_i, \bmc_i$ for each component to approximate a smooth function in $\R^{d_{i-1}}$. Through combination of multiple components in each layer and then composition of multiple layers, this balance of rank and width as well as the flexible component structure using random basis is the key to the effectiveness of MMNNs in both representation and learning.
% The \textbf{width} of the MMNN is defined as $\max\{n_i : i = 1, 2, \dots, m-1\}$, the \textbf{rank} as $\max\{d_i : i = 1, 2, \dots, m-1\}$, and the \textbf{depth} as $m$.
% For brevity, we denote a network of width $N$, rank $R$, and depth $L$ using the compact notation $(N, R, L)$. In most of our experiments, we use equal layer width and rank, i.e., $n_i=N$ and $r_i=R$.

% In summary, MMNNs view each component, a linear combination of a family of randomly parametrized neurons (basis) as a unit instead of viewing each neuron as a unit in fully connected neural networks (FCNNs),
% These components are combined (in each layer) and composed (through layers) to approximate a target function effectively. The MMNN structure is further balanced by the introduction of the additional dimension, rank, in addition to network width and depth, providing greater flexibility in network architecture. Moreover, the training strategy for MMNNs differs significantly from that of FCNNs.
% In each layer of an MMNN, represented by $\bmA \, \sigma(\bmW \bmx + \bmb) + \bmc$, we interpret the set
% \[
% \big\{ \sigma(\bmW{[j,:]} \cdot \bmx + \bmb[j]) : j = 1, 2, \dots, n \big\}
% \]
% as a common random basis for each component. As a result, during training, only the parameters $\bmA$ and $\bmc$ are updated, while $\bmW$ and $\bmb$ are randomly initialized and remain fixed.

% To address the gradient vanishing issue in very deep MMNNs, one can adopt techniques inspired by ResNets \cite{7780459}, which enhance training efficiency. Building on this idea, we introduce the ResMMNN, which modifies the structure of \eqref{eq:h:MMNN} as follows:
% \begin{equation*}
%     \bmh = \bmh_m \circ (\bmI + \bmh_{m-1}) \circ \cdots \circ (\bmI + \bmh_3) \circ (\bmI + \bmh_2) \circ \bmh_1,
% \end{equation*}
% where $\bmI$ denotes the identity mapping.
% It is worth noting that the definition of ResMMNN can be generalized by applying identity mappings to only specific layers. We continue to refer to such variations as ResMMNN.
% See 
% Figure~\ref{fig:MMNN:eg}
% for an illustration of ResMMNN of size $(6,2,3)$.

\subsection{Structure of MMNNs}
\label{sec:MMNN:structure}

Before presenting the main results, we first introduce the architecture of MMNNs. 
An MMNN is a multi-layer composition of functions $\bmh_i$, formally defined as $\bmh: \R^{d_0}\to \R^{d_m}$ with
\begin{equation}
    \label{eq:h:MMNN}
    \bmh = \bmh_m \circ \bmh_{m-1} \circ \cdots \circ \bmh_1,
\end{equation}
where each layer $\bmh_i: \R^{d_{i-1}} \to \R^{d_i}$ represents a multi-component shallow network with width $n_i$ and $d_i$ components, given by
\begin{equation*}
    \bmh_i(\bmx) = \bmA_i \, \sigma(\bmW_i \bmx + \bmb_i) + \bmc_i,
\end{equation*}
where $\bmW_i \in \R^{n_i \times d_{i-1}}$, $\bmb_i\in \R^{n_i}$, $\bmA_i \in \R^{d_i \times n_i}$, and $\bmc_i\in\R^{d_i}$. Here, $\sigma(\bmW_i{[j,:]} \cdot \bmx + \bmb_i[j])$ for $ j = 1, 2, \dots, n_i$ act as randomly parameterized basis functions in $\R^{d_{i-1}}$. Each component $\bmA_i[k,:]\sigma(\bmW_i\bmx+\bmb_i) +\bmc_i[k]$, for $ k=1,2, \ldots, d_i$, is a linear combination of these basis functions. 

In each layer, the number of components $d_{i-1}$, referred to as rank, is significantly smaller than the number of hidden neurons $n_i$, known as the layer width. The utilization of a diverse set of random basis functions, enabled by $n_i\gg d_{i-1}$, along with their well-conditioned nature due to random parametrization, facilitates easy training of $\bmA_i$ and $\bmc_i$ to approximate smooth functions in $\R^{d_{i-1}}$. By integrating multiple components per layer and composing multiple layers, this balance between rank and width, combined with the flexible component structure employing random bases, enhances the effectiveness of MMNNs in both representation and learning.
The \textbf{width} of an MMNN is defined as $\max\{n_i : i = 1, 2, \dots, m-1\}$, the \textbf{rank} as $\max\{d_i : i = 1, 2, \dots, m-1\}$, and the \textbf{depth} as $m$. For convenience, we use the compact notation $(N, R, L)$ to denote a network of width $N$, rank $R$, and depth $L$. In most of our experiments, we assume equal layer width and rank, i.e., $n_i=N$ and $d_i=R$.

In summary, MMNNs consider each component as a fundamental unit, where it consists of a linear combination of randomly parameterized neurons (basis functions). This contrasts with FCNNs, which treat individual neurons as the primary units. Components within each layer are combined and further composed across layers to effectively approximate target functions. The MMNN structure is enriched by introducing rank as an additional dimension alongside width and depth, offering greater flexibility in network architecture.
Furthermore, the training paradigm for MMNNs diverges significantly from that of FCNNs. 
Within each MMNN layer, represented by $\bmA \, \sigma(\bmW \bmx + \bmb) + \bmc$, the set
\[
\big\{ \sigma(\bmW{[j,:]} \cdot \bmx + \bmb[j]) : j = 1, 2, \dots, n \big\}
\]
is interpreted as a shared random basis for all components. Consequently, during training, only the parameters $\bmA$ and $\bmc$ are updated, while $\bmW$ and $\bmb$ remain fixed after random initialization.

To mitigate the vanishing gradient issue in deep MMNNs, techniques inspired by ResNets \cite{7780459} can be employed to improve training efficiency. Building on this concept, we introduce the ResMMNN, which modifies the structure of \eqref{eq:h:MMNN} as 
% follows:
\begin{equation*}
    \bmh = \bmh_m \circ (\bmI + \bmh_{m-1}) \circ \cdots \circ (\bmI + \bmh_3) \circ (\bmI + \bmh_2) \circ \bmh_1,
\end{equation*}
where $\bmI$ denotes the identity mapping. This definition of ResMMNN can be further generalized by applying identity mappings selectively to specific layers. Such variations are still referred to as ResMMNNs. See Figure~\ref{fig:MMNN:eg} for an illustration of a ResMMNN with size $(6,2,3)$. 
Furthermore, additional layer operations, such as Batch Normalization \cite{pmlr-v37-ioffe15} and Dropout \cite{JMLR:v15:srivastava14a}, can also be applied to specific layers of MMNNs to enhance training stability, accelerate convergence, and improve generalization, among other benefits.




\begin{figure}[ht]%[htbp!] 
	\centering
 % \vskip 3.1pt
	\includegraphics[width=0.95\linewidth]{MMNNeg.pdf}
\caption{An illustration of a ResMMNN of size $(6,2,3)$. During training, only the parameters $\bmA_i$'s and $\bmc_i$'s are updated, while $\bmW_i$'s and $\bmb_i$'s are randomly initialized and remain fixed.}
	\label{fig:MMNN:eg}
\end{figure}

\subsection{Approximation Capability}
\label{sec:approx:power}

We first introduce some notations before presenting our main results on the exponential approximation capabilities of MMNNs using \texttt{sine} or \SinTU{s} as activation functions.
We denote $\mn{\varrho}{N}{R}{L}{\R^d}{\R^n}$ as the set of vector-valued functions $\bmphi:\mathbb{R}^d\to\mathbb{R}^n$ that can be represented
by $\varrho$-activated 
% fully connected neural networks (FCNNs) 
MMNNs
of width $\le N\in \N^+$, rank $\le R\in\N^+$, and depth $\le L\in\N^+$. 
Additionally, in this notation, if $\varrho$ is replaced by $(\varrho_1,\dots,\varrho_k)$, it indicates that each neuron can be activated by any of $\varrho_i$'s.
Let
$\omega_f(\cdot)$ be the modulus of continuity of $f\in C([0,1]^d)$ defined via
\begin{equation*}
    \omega_f(t)\coloneqq \sup \big\{|f(\bmx)-f(\bmy)|: \|\bmx-\bmy\|_2\le t,\,\;\bmx,\bmy\in [0,1]^d\big\}\quad \tn{for any $t\ge 0$.}
\end{equation*}
 Let $\calS$ denote  the set of \SinTU{s}, i.e., 
\begin{equation*}
    \calS\coloneqq \{\SinTU_s:s\in\R\}\quad \tn{where}\quad \SinTU_s\coloneqq \sin\circ \calT_s\quad \tn{and}\quad \calT_s(x) \coloneqq \max\{x,\, s\}=
\begin{cases} 
x & \text{if } x \ge s, \\ 
s & \text{if } x < s.
\end{cases}
\end{equation*}
With the above notations, we present the following theorem, which demonstrates that MMNNs activated by \SinTU{s} possess exponential approximation power.

\begin{theorem}
\label{thm:main1}
Given $f \in C([0,1]^d)$ and $\varrho\in\calS$, for any $N,L \in \mathbb{N}^+$ and $p \in [1,\infty)$, there exists  
% a function $\phi$ implemented by a $\varrho$-activated MMNN of width $d(4N-1)$, rank $3d$, and depth $L$ 
$\phi\in \mn[\big]{\varrho}{2d(4N-1)}{3d}{L+2}{\mathbb{R}^d}{\mathbb{R}}$
such that
\[
\| \phi - f\|_{L^p([0,1]^d)} \leq  2\sqrt{d}\cdot\omega_f\big(N^{-L}\big).
\]
\end{theorem}

% The above theorem describes the approximation power of MMNNs activated by \SinTU{s}, which are a truncated variations of the \sine{}  function. Next, we consider the case of using the pure \sine{}  function. However, it is difficult to localize in space using only \sine{} due to its lack of singularity, which leads to the difficulty in using mathematical construction of spatial decomposition based on continuity. To address this mathematical difficulty, we introduce \ReLU{} as an additional activation function, leading to the following theorem.

The preceding theorem establishes the approximation capabilities of MMNNs activated by \SinTU{s}, which are truncated variations of the \sine{} function. Next, we explore the case where the pure \sine{} function is used as the activation function. However, due to its lack of singularity, the \sine{} function poses challenges in spatial localization, making it difficult to construct mathematical frameworks for spatial decomposition based on continuity.
To overcome this mathematical challenge, we introduce \ReLU{} as an additional activation function. This modification enables a more effective spatial decomposition, leading to the following theorem.

\begin{theorem}
\label{thm:main2}
Given $f \in C([0,1]^d)$, for any $N,L \in \mathbb{N}^+$ and $p \in [1,\infty)$, there exist  
% a function $\phi$ implemented by 
% a $(\sin,\ReLU)$-activated MMNN of width $d(4N-1)$, rank $3d$, and depth $L$ 
$\phi\in \mn[\big]{(\sine,\,\ReLU)}{d(4N-1)}{3d}{L+2}{\mathbb{R}^d}{\mathbb{R}}$
such that
\[
\| \phi - f\|_{L^p([0,1]^d)} \leq  2\sqrt{d}\cdot\omega_f\big(N^{-L}\big).
\]
\end{theorem}
\begin{remark}
    The theorem does not impose a specific arrangement of $\sine$ and \ReLU{}. However, our proof demonstrates that applying \ReLU{} to all but the last two hidden layers, where $\sine$ is used, is sufficient. This result is theoretical; in practice, additional $\sine$ activation functions may be required, as discussed later.
\end{remark}

% \red{
% \begin{remark}
%     The theorem does not specify the arrangement of $\sine$ and \ReLU{}. However, our proof shows that using \ReLU{} for all but the last two hidden layers, where $\sine$ is applied, suffices. This is a theoretical result; in practice, more $\sine$ activation functions may be needed, as discussed later.
% \end{remark}
% }



% \red{We denote $\nn{\varrho}{N}{L}{\R^d}{\R^n}$ as the set of vector-valued functions $\bmphi:\mathbb{R}^d\to\mathbb{R}^n$ that can be represented
% by $\varrho$-activated fully connected neural networks (FCNNs) of width $\le N\in \N^+$ and depth $\le L\in\N^+$.

% We denote $\mn{\varrho}{N}{R}{L}{\R^d}{\R^n}$ as the set of vector-valued functions $\bmphi:\mathbb{R}^d\to\mathbb{R}^n$ that can be represented
% by $\varrho$-activated 
% % fully connected neural networks (FCNNs) 
% MMNNs
% of width $\le N\in \N^+$, rank $\le R\in\N^+$, and depth $\le L\in\N^+$. }
% In our context, the width of a network refers to the maximum number of neurons in a hidden layer and the depth corresponds to the number of hidden layers. 
% For instance, suppose $\bmphi:\R^d\to\R^n$ is a vector-valued function realized by a $\varrho$-activated network, where $\varrho$ is the activation function that can be applied elementwise to a vector input. Then $\bmphi$ can be expressed as
% \begin{equation*}
%     \bmphi =\calbmL_L\circ\varrho\circ
%     		\calbmL_{L-1}\circ %\varrho\circ
%     \ \cdots \  \circ 
%     		\varrho\circ
%     \calbmL_1\circ\varrho\circ\calbmL_0,
% \end{equation*}
% where $\calbmL_\ell$ is an affine linear map given by $\calbmL_\ell(\bmy)\coloneqq \bmW_\ell \cdot \bmy +\bmb_\ell$ for $\ell=0,1,\cdots,L$. Here, $\bmW_\ell\in \R^{N_{\ell+1}\times N_{\ell}}$ and $\bm{b}_\ell\in \R^{N_{\ell+1}}$ are the weight matrix and the bias vector, respectively, with
%  $N_0=d$, $N_1,N_2,\cdots,N_L\in\N^+$, and 
% $N_{L+1}=n$.  Clearly, $\bmphi\in \nn{\varrho}{N}{L}{\R^d}{\R^n}$, where $N=\max\{N_1,N_2,\cdots,N_L\}$.


% \begin{theorem}
% \label{thm:main}
% Given a continuous function $f \in C([0,1]^d)$, for any $N,L \in \mathbb{N}^+$ and $p \in [1,\infty)$, there exist $u,v\in \R$ and 
% % a function $\phi$ implemented by a \ReLU\  FCNN (or MMNN) of width $d(4N-1)$ and depth $L$ 
% $\phi\in \nn[\big]{\ReLU}{d(4N-1)}{L}{\mathbb{R}^d}{\mathbb{R}}$
% such that
% \[
% \| \psi\circ \phi - f\|_{L^p([0,1]^d)} \leq  2\sqrt{d}\cdot\omega_f\big(N^{-L}\big)\quad \tn{where}\quad 
% \psi(x)\coloneqq u\cdot\sin\big(v\cdot \sin(x)\big)\quad\tn{for any}\  x\in\R.
% \]
% \end{theorem}
% where
% \begin{equation*}
%     \psi(\cdot)\coloneqq u\sin\big(v\sin(\cdot)\big)
% \end{equation*}

% Similar to \(\mn{\varrho}{N}{R}{L}{\R^d}{\R^n}\) for MMNNs, we adopt analogous notation for FCNNs. Specifically, we denote \(\nn{\varrho}{N}{L}{\R^d}{\R^n}\) as the set of vector-valued functions \(\bmphi: \mathbb{R}^d \to \mathbb{R}^n\) that can be represented by \(\varrho\)-activated FCNNs with width \(\le N \in \mathbb{N}^+\) and depth \(\le L \in \mathbb{N}^+\). Based on Theorems~\ref{thm:main1} and \ref{thm:main2}, we obtain the following two immediate corollaries for FCNNs.

We adopt a notation for FCNNs analogous to \(\mn{\varrho}{N}{R}{L}{\R^d}{\R^n}\) used for MMNNs. Specifically, let \(\nn{\varrho}{N}{L}{\R^d}{\R^n}\) denote the set of vector-valued functions \(\bmphi: \mathbb{R}^d \to \mathbb{R}^n\) that can be realized by \(\varrho\)-activated FCNNs with width at most \(N \in \mathbb{N}^+\) and depth at most \(L \in \mathbb{N}^+\).
Similarly, if \(\varrho\) is replaced by \((\varrho_1,\dots,\varrho_k)\), it indicates that each neuron can be activated by any of the \(\varrho_i\)'s.
As a direct consequence of Theorems~\ref{thm:main1} and \ref{thm:main2}, we establish the following two corollaries for FCNNs.



\begin{corollary}
\label{coro:main1}
Given $f \in C([0,1]^d)$ and $\varrho\in\calS$, for any $N,L \in \mathbb{N}^+$ and $p \in [1,\infty)$, there exists  
% a function $\phi$ implemented by a $\varrho$-activated MMNN of width $d(4N-1)$, rank $3d$, and depth $L$ 
$\phi\in \nn[\big]{\varrho}{2d(4N-1)}{L+2}{\mathbb{R}^d}{\mathbb{R}}$
such that
\[
\| \phi - f\|_{L^p([0,1]^d)} \leq  2\sqrt{d}\cdot\omega_f\big(N^{-L}\big).
\]
\end{corollary}

\begin{corollary}
\label{coro:main2}
Given $f \in C([0,1]^d)$, for any $N,L \in \mathbb{N}^+$ and $p \in [1,\infty)$, there exist  
% a function $\phi$ implemented by 
% a $(\sin,\ReLU)$-activated MMNN of width $d(4N-1)$, rank $3d$, and depth $L$ 
$\phi\in \nn[\big]{(\sin,\,\ReLU)}{d(4N-1)}{L+2}{\mathbb{R}^d}{\mathbb{R}}$
such that
\[
\| \phi - f\|_{L^p([0,1]^d)} \leq  2\sqrt{d}\cdot\omega_f\big(N^{-L}\big).
\]
\end{corollary}

% \begin{remark}
% Note the significant difference in the total number of parameters in a MMNN with network width $N$, rank $R$, and depth $L$, which is $O(NRL)$, vs. a FCNN: with network width $N$ and depth $L$, which is $O(N^2L)$, where the rank $R$ (number of components in each layer) is much less than network width $N$ (number of random hidden neurons in each layer) in a MMNN. Moreover, only half of the parameters in a MMNN are trained as explained 
% previously.
% % in \ref{sec:MMNN}.
% \end{remark} 

\begin{remark}
    It is worth highlighting the substantial difference in the total number of parameters between an MMNN and an FCNN. For an MMNN of width \( N \), rank \( R \), and depth \( L \), the parameter count is \( O(NRL) \), whereas for an FCNN of width \( N \) and depth \( L \), it is \( O(N^2L) \). Notably, in an MMNN, the rank \( R \) (the number of components in each layer) is significantly smaller than the network width \( N \) (the number of random hidden neurons per layer), which guarantees that the set of $N$ random basis is diverse enough to approximate smooth functions in the input space of dimension $R$ from the previous layer. Additionally, as previously discussed, only half of the parameters in an MMNN are trained.
\end{remark}

\begin{remark}
%It is worth noting that the approximation error in Theorem~\ref{thm:main} is measured in the \( L^p \)-norm for any \( p \in [1, \infty) \). However, 
By applying techniques from \cite{shijun:smooth:functions} (specifically Theorem~2.1), the above results could be extended to the \( L^\infty \)-norm, although the constants involved would be considerably larger. The extension involves more technical complexities and is of little importance to the main themes of this paper, so we do not pursue it here.
\end{remark}

% The proofs of Theorems~\ref{thm:main1} and \ref{thm:main2} (see Section~\ref{sec:proof:thm:main}) rely on two key components. The first is the construction of a subnetwork that partitions a $d$-dimensional unit hypercube into uniform subcubes of small size, except for a small discrepancy due to the continuity of the activation function. Within each subcube, the function is approximated by a constant function.
% The second component is the existence of a subnetwork that maps the index of each subcube to the function value at a representative point within the subcube (e.g., its center). In constructing this second subnetwork, we only need to ensure accuracy at finitely many points rather than over an entire interval. This demonstrates the power of composition, simplifying the network design.
% As we shall see later, the periodicity and irrationality of the \texttt{sine} function play a crucial role in efficiently handling the second component. Specifically, for any $\epsilon > 0$ and any $M \in \mathbb{N}^+$, given $f_n \in (-1,1)$, we can find appropriate values $v$ and $w$ such that
% \begin{equation}
%     \left| \sin \left[ v \sin (n w) \right] - f_n \right| < \epsilon, \quad \text{for } n = 1, 2, \dots, M.
% \end{equation}

The proofs of Theorems~\ref{thm:main1} and \ref{thm:main2} (see Section~\ref{sec:proof:thm:main}) rely on two key components. 
The first component involves constructing a subnetwork that partitions a $d$-dimensional unit hypercube into uniform subcubes of small size, with only a minor discrepancy due to the continuity of the activation function. Within each subcube, the function is approximated by a constant function.
The second component is the existence of a subnetwork that maps the index of each subcube to the function value at a representative point within the subcube (e.g., its center). In designing this subnetwork, it suffices to ensure accuracy at a finite set of points rather than over an entire interval. This highlights the power of composition, which simplifies the construction.
As we shall see later, the periodicity and irrationality of the \texttt{sine} function play a crucial role in efficiently addressing the second component. Specifically, for any $\epsilon > 0$ and any $M \in \mathbb{N}^+$, given $f_n \in [-1,1]$, there exist suitable values of $v$ and $w$ such that
\begin{equation}
\label{eq:sin:sin::-:fn}
    \left| \sin \big( v\cdot \sin (n w) \big) - f_n \right| < \epsilon  \quad \text{for } n = 1, 2, \dots, M.
\end{equation}

Although many mathematical approximation results show theoretical representation power, however, in practice, a more important issue is whether one has an efficient training strategy to achieve a good computational performance. For most mathematical neural network approximation results (constructive or non-constructive), the network parameters depends on target function nonlinearly and globally. On the other hand, most training processes are gradient descent based (first order) methods which are very local and sensitive to ill-conditioning of the cost function in terms of a very large number of parameters. This typically leads to a gap between the theoretical results and practical performance. In our case, although two \sine{} functions (or \SinTU{s}) are theoretically sufficient for value fitting (e.g., Equation~\eqref{eq:sin:sin::-:fn}), finding the two appropriate numbers, $v$ and $w$, is non-practical in general. Consequently, a larger network with multiple components and layers is essential for effective optimization.

Mathematical and numerical investigations in later sections demonstrate that using \texttt{sine} or \SinTU{s} as activation functions in MMNNs with well balanced structures, significantly improves the network’s capability and learning efficiency. This is consistent with the key feature of MMNNs that each component, which is a one hidden layer network, only needs to approximate a smooth function and can be trained effectively while Fourier series can approximate smooth functions efficiently.



%This advantage arises from the natural alignment of sinusoidal functions with oscillatory patterns, making them particularly effective in representing complex variations in data. Moreover, careful architectural and training choices are essential to ensure stability and efficient convergence, highlighting the interplay between theoretical insights and practical implementation.

% Numerical experiments
% confirm that FMMNN consistently achieves top performance, aligning with MMNN’s key advantage: each component, a single hidden-layer network, efficiently approximates smooth functions, leveraging Fourier series for accuracy.




% The proofs of Theorems~\ref{thm:main1} and \ref{thm:main2} (see Section~\ref{sec:proof:thm:main}) have two main ingredients. The first one is to construct a \texttt{ReLU} network in $\nn[\big]{\ReLU}{d(4N-1)}{L}{\mathbb{R}^d}{\mathbb{R}}$ that can divide a unit cube in $\mathbb{R}^d$ into uniform subcubes of size $N^{-L}$ (except for a small part due to the continuity of \texttt{ReLU}). Then each the function in each subcube can be approximated by a constant function. The second ingredient is an existence proof using the following two important properties of \texttt{sine}: 1) its periodicity, and 2) its irrationality. So there is a $w\in \mathbb{R}$ such that $a_n=\sine (nw), n=1, \ldots, N^{Ld}$ are irrational numbers and then $x_n(u)=\sine (ua_n), u\in \mathbb{R}$ is a space-filling curve on the torus in $\mathbb{R}^M, M=N^{Ld}$. 

% Although this theorem like many other mathematical approximation results illustrates that the use of ReLU-sine mixed activation function can be theoretically effective in terms of representation, however, in practice, a more important issue is whether one has an efficient training strategy to achieve a good computational result. In most mathematical neural network approximation results (constructive or non-constructive), the network parameters depends on target function nonlinearly and globally. On the other hand, most training processes are gradient descent based (first order) methods which are very local and sensitive to ill-conditioning of the cost function (Jacobian) in terms of the typically humongous number of parameters. This typically leads to a huge gap between the theoretical results and practical performance. 

% In the following we use mathematical and numerical investigations to show that the use of \texttt{sine} as activation function in a specific network architecture (MMNN) and a customized learning strategy is quite effective. Furthermore, while a small number of \sine{}  functions is theoretically sufficient, it is not insufficient in practice. This is because our theoretical existence result, i.e. the very explicit problem of finding the appropriate two numbers $v$ and $w$ such that $|\sin [v\sine (nw)]-f_n|$ are uniformly small for given $f_n\in (-1,-1), n=1,2, M$ is a very challenging computational task in actual training. In practice, a much larger network using many components and layers with \texttt{sine} as the activation function is needed for the optimizer to perform effectively. 

% As can be seen in our extensive numerical experiments, FMMNN always gives the best or close to the best performances. This is consistent with the key feature of MMNN that each component, which is a one hidden layer network, only needs to approximate a smooth function and can be trained effectively while Fourier series can approximate smooth functions efficiently.

% When the target function, \sine{}  functions alone deliver the best performance, consistent with the efficiency of Fourier series in approximating smooth functions.

% In our experiments, the best overall performance was achieved using a combination of predominantly \sine{}  functions with a small number of ReLU functions.
% When the target function is smooth, \sine{}  functions alone deliver the best performance, consistent with the efficiency of Fourier series in approximating smooth functions.

% \red{
% This theorem demonstrates that the ReLU-sine mixed activation function is theoretically efficient. However, in practice, the landscape becomes overly complex. To address this, we incorporated a specific network architecture (MMNN) and a tailored learning strategy in subsequent experiments. Moreover, while a small number of \sine{}  functions is sufficient theoretically, it proves inadequate in practice. After all, our construction is too idealized; in actual training, more \sine{}  functions are needed for the optimizer to achieve good optimization results.

% In our experiments, a combination of mostly \sine{}  functions with a small number of ReLU functions performed best overall. When the target function is smooth, \sine{}  functions alone achieve the best performance, which is reasonable given the efficiency of Fourier series in approximating smooth functions. 
% }



% The following is an overview of the basic notations used in this paper.
% \begin{itemize}
% 	\item The set difference of two sets $A$ and $B$ is denoted as $A\backslash B\coloneqq\{x:x\in A,\ x\notin B\}$. 
	
% 	\item 
% The symbols $\N$, $\Z$, $\Q$, and $\R$ are used to denote the sets of natural numbers (including $0$), integers, rational numbers, and real numbers, respectively. The set of positive natural numbers is denoted as $\N^+=\N\backslash\{0\}=\{1,2,3,\cdots\}$.

	
% 	\item 
%  The floor and ceiling functions of a real number $x$ can be represented as
% 	$\lfloor x\rfloor=\max \{n: n\le x,\ n\in \Z\}$ and $\lceil x\rceil=\min \{n: n\ge x,\ n\in \Z\}$.
	
% 	\item Given any $p\in [1,\infty]$, the $p$-norm (also known as $\ell^p$-norm) of a vector $\bmx=(x_1,\cdots,x_d)\in\R^d$ is defined via
% 	\begin{equation*}
% 		\|\bmx\|_p=\|\bmx\|_{\ell^p}\coloneqq \big(|x_1|^p+\cdots+|x_d|^p\big)^{1/p}\quad \tn{if $p\in [1,\infty)$}
% 	\end{equation*}
% 	and
% 	\begin{equation*}		\|\bmx\|_{\infty}=\|\bmx\|_{\ell^\infty}\coloneqq \max\big\{|x_i|: i=1,2,\cdots,d\big\}.
% 	\end{equation*}
	

%  \item A network is labeled as ``a network of width $N$ and depth $L$'' when it satisfies the following  two conditions.
% \begin{itemize}
% \item The count of neurons in each hidden layer of the network does not exceed $N$.
% \item The total number of hidden layers in the network is at most $L$.
% \end{itemize}

% \end{itemize}




% \section{Further Discussion}
% \label{sec:discussion}




% \subsection{Learning Difficulty
% % for \sine{}  activation
% }
\subsection{Optimization Landscapes}
\label{sec:landscape}


In Section~\ref{sec:approx:power}, we demonstrate that MMNNs activated by \texttt{sine} or \SinTU{s} possess strong approximation capabilities. However, having good approximation power only reflects theoretical potential and does not necessarily guarantee effective learning in practice. 
Next, we discuss the practical learning difficulty of MMNNs activated by \texttt{sine} or \SinTU{s}. We focus on the most intuitive aspect: the landscape of the cost function with respect to the network parameters, which serves as an indicator of the training complexity in practice. This analysis is conducted across various network architectures and activation functions.
 

% \begin{equation*}
%     \bmphi 
%     = \bmW_3\bmA_2\sigma\Big(
%     \red{\bmW_2 \bmA_1}
%     \sigma(\bmW_1\bmx+\bmb_1)+\bmb_2\Big)+\bmb_3
% \end{equation*}

% A full matrix $\bm{W}_2 \cdot \bm{A}_1$:
% \begin{itemize}
%     \item When $\bm{W}$ is tailored, training $\bm{A}$ becomes challenging and less stable.
%     \item When $\bm{W}$ is random, training $\bm{A}$ is more stable.
% \end{itemize}


% \red{For deep networks, the decay speed of the Gram matrix spectrum is not particularly critical.}

% \red{Characterize the landscape of the parameter space with a large number of parameters.}

% \medskip

% We analyze the use of a sinusoidal network for approximating a target function.

% To simplify, we consider a shallow network. Assuming the target function has a Fourier expansion, we focus primarily on a single term to examine the loss landscape. Specifically, we suppose the target function is of the form \( a^* \sin(w^* x + b^*) + c^* \) with the corresponding loss function given by
% \begin{equation*}
%     \sfL_0(a,w,b,c)=\int_{-\pi}^{\pi} \Big(\big(a\sin  (wx+b) +c\big)- \big(a^*\sin(w^* x+b^*)+c^*\big)\Big)^2 \, dx
% \end{equation*}

% Landscape in $a$, $w$, $b$, or $c$.

% Landscape in $w_1$ and $w_2$.
% \begin{equation*}
%     \sfL_1(w_1,w_2)=\int_{-\pi}^{\pi} \Big(\sum_{i=1}^2 \sin (w_ix )- \sum_{i=1}^2 \sin (w_i^*x )\Big)^2 \, dx
% \end{equation*}


% \begin{equation*}
%     \sfL_2(w_1,w_2)=\int_{-\pi}^{\pi} \Big(\sin\big(w_2 \sin (w_1x )\big)- \sin\big(w_2^* \sin (w_1^*x )\big)\Big)^2 \, dx
% \end{equation*}

We first consider three basic cases where the target function \( f \) takes the following forms: 
\[
\sin(w^* x + b^*), \quad \sum_{i=1}^2 \sin(w_i^* x), \quad \text{and} \quad \sin\big(w_2^* \sin(w_1^* x)\big),
\]
respectively. The corresponding cost functions are given by
\begin{equation*}
    \mathcal{L}_1(w_1, w_2) = \int_{-\pi}^{\pi} \Big(\sin(w_1x + w_2) - \sin(w_1^* x + w_2^*)\Big)^2 \, dx,
\end{equation*}
\begin{equation*}
    \mathcal{L}_2(w_1, w_2) = \int_{-\pi}^{\pi} \Bigg(\sum_{i=1}^2 \sin(w_i x) - \sum_{i=1}^2 \sin(w_i^* x)\Bigg)^2 \, dx,
\end{equation*}
and
\begin{equation*}
    \mathcal{L}_3(w_1, w_2) = \int_{-\pi}^{\pi} \Big(\sin\big(w_2 \sin(w_1 x)\big) - \sin\big(w_2^* \sin(w_1^* x)\big)\Big)^2 \, dx.
\end{equation*}
\begin{figure}[ht]%[htbp!]  
            \centering

            \,\hfill
            \begin{subfigure}[b]{0.302245\textwidth}
                    \centering            
                    \includegraphics[width=0.999\textwidth]{figures/LandScape1.pdf}
                    \subcaption{$\calL_1(w_1,w_2)$.}
                \end{subfigure}
                \hfill
            \begin{subfigure}[b]{0.302245\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/LandScape3.pdf}
                    \subcaption{$\calL_2(w_1,w_2)$.}
                \end{subfigure}
            \hfill
            \begin{subfigure}[b]{0.302245\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/LandScape4.pdf}
                    \subcaption{$\calL_3(w_1,w_2)$.}
                \end{subfigure}\hfill
                \,                
    \caption{
    Landscape visualizations of $\mathcal{L}_i$ for $i = 1,2,3$, where $x$, $y$, and $z$ represent $w_1$, $w_2$, and $\mathcal{L}_i(w_1,w_2)$, respectively.
    % (a) \(x\), \(y\), and \(z\) correspond to \(w\), \(b\), and \(\mathcal{L}_0(w, b)\), respectively, where $(w^*,b^*)=(2\pi, 3)$.
    % (c) \(x\), \(y\), and \(z\) correspond to \(w_1\), \(w_2\), and \(\mathcal{L}_1(w_1,w_2)\), respectively.
    % (d) \(x\), \(y\), and \(z\) correspond to \(w_1\), \(w_2\), and \(\mathcal{L}_2(w_1,w_2)\), respectively.
}
\label{fig:landscape:three:base:cases}
\end{figure}

\noindent
Here, we use the integration range \((-\pi, \pi)\) instead of \((-1, 1)\) because the \sine{} function has a period of \(2\pi\), which simplifies the calculations and makes the test more straightforward.
The landscapes of these three cost functions are illustrated in Figure~\ref{fig:landscape:three:base:cases}. As observed, the landscapes are quite complex, featuring numerous local minima. This indicates that using the \sine{} function as an activation function poses significant challenges for effective learning in practice.
We will later see that this issue is particularly severe for FCNNs. However, the structure of MMNNs simplifies the landscape, making them more conducive to effective learning.


% \begin{equation*}
%     \sfL_0(w,b)=\int_{-\pi}^{\pi} \Big(\sin  (wx+b) - \sin(w^* x+b^*)\Big)^2 \, dx,
% \end{equation*}
% \begin{equation*}
%     \sfL_1(w_1,w_2)=\int_{-\pi}^{\pi} \Big(\sum_{i=1}^2 \sin (w_ix )- \sum_{i=1}^2 \sin (w_i^*x )\Big)^2 \, dx,
% \end{equation*}
% and
% \begin{equation*}
%     \sfL_2(w_1,w_2)=\int_{-\pi}^{\pi} \Big(\sin\big(w_2 \sin (w_1x )\big)- \sin\big(w_2^* \sin (w_1^*x )\big)\Big)^2 \, dx.
% \end{equation*}



% \begin{figure}[ht]%[htbp!]  
%             \centering
%             \begin{subfigure}[b]{0.2245\textwidth}
%                     \centering            
%                     \includegraphics[width=0.999\textwidth]{figures/LandScape1.pdf}
%                     \subcaption{}
%                 \end{subfigure}
%                 \hfill
%             \begin{subfigure}[b]{0.2245\textwidth}
%                     \centering            \includegraphics[width=0.999\textwidth]{figures/LandScape2.pdf}
%                     \subcaption{}
%                 \end{subfigure}
%             \hfill
%                             \begin{subfigure}[b]{0.2245\textwidth}
%                     \centering            
%                     \includegraphics[width=0.999\textwidth]{figures/LandScape3.pdf}
%                     \subcaption{}
%                 \end{subfigure}\hfill
%             \begin{subfigure}[b]{0.2245\textwidth}
%                     \centering            \includegraphics[width=0.999\textwidth]{figures/LandScape4.pdf}
%                     \subcaption{}
%                 \end{subfigure}
%     \caption{Landscape. (a) \(x\), \(y\), and \(z\) correspond to \(w\), \(b\), and \(\mathsf{L}_0(a^*, w, b, c^*)\), respectively, where $(a^*,w^*,b^*,c^*)=(1/\pi, 2\pi, 3, 4)$.
%     (b) \(x\), \(y\), and \(z\) correspond to \(a\), \(c\), and \(\mathsf{L}_0(a, w^*, b^*, c)\), respectively, where $(a^*,w^*,b^*,c^*)=(1/\pi, 2\pi, 3,4)$.
%     (c) \(x\), \(y\), and \(z\) correspond to \(w_1\), \(w_2\), and \(\mathsf{L}_1(w_1,w_2)\), respectively.
%     (d) \(x\), \(y\), and \(z\) correspond to \(w_1\), \(w_2\), and \(\mathsf{L}_2(w_1,w_2)\), respectively.
% }
%     \label{fig:landscape}
    
% \end{figure}

% \cleardoublepage


Next, we examine general network architectures, specifically FMMNNs and FCNNs.
To ensure a fair comparison, the FCNN is designed to have a comparable number of learnable parameters to the MMNN.
The FCNN is defined as
\begin{equation}\label{eq:FCNN:eg}
    \phi(x) = \mathcal{A}_2 \circ \varrho \circ {\mathcal{A}}_1 \circ \varrho \circ {\mathcal{A}}_0(x),
\end{equation}
where \({\mathcal{A}}_0: \mathbb{R} \to \mathbb{R}^{64}\), \({\mathcal{A}}_1: \mathbb{R}^{64} \to \mathbb{R}^{64}\),   \(\mathcal{A}_2: \mathbb{R}^{64} \to \mathbb{R}\) are affine linear maps, and $\varrho$ is either the \sine{} or $\SinTU_0$ activation function.
The MMNN is defined as
\begin{equation}\label{eq:MMNN:eg}
    \phi(x) = \tilde{\mathcal{A}}_3 \circ \varrho \circ {\tilde{\mathcal{A}}}_2 \circ {\tilde{\mathcal{A}}}_1 \circ \varrho \circ {\tilde{\mathcal{A}}}_0(x),
\end{equation}
where \({\tilde{\mathcal{A}}}_0: \mathbb{R} \to \mathbb{R}^{128}\), \({\tilde{\mathcal{A}}}_1: \mathbb{R}^{128} \to \mathbb{R}^{32}\), \({\tilde{\mathcal{A}}}_2: \mathbb{R}^{32} \to \mathbb{R}^{128}\),  \(\tilde{\mathcal{A}}_3: \mathbb{R}^{128} \to \mathbb{R}\) are affine linear maps.
% where \({\tilde{\mathcal{A}}}_0: \mathbb{R} \to \mathbb{R}^{256}\), \({\tilde{\mathcal{A}}}_1: \mathbb{R}^{256} \to \mathbb{R}^{16}\), \({\tilde{\mathcal{A}}}_2: \mathbb{R}^{16} \to \mathbb{R}^{256}\),  \(\tilde{\mathcal{A}}_3: \mathbb{R}^{256} \to \mathbb{R}\) are affine linear maps.
The cost function is given by
\begin{equation*}
    \calL(w_1,w_2)=\int_{-\pi}^{\pi} \Big(\phi(x)- f(x)\Big)^2 \, dx,\quad \tn{where}\quad 
    f(x)=\frac{1}{1+100x^2}.
\end{equation*}

As shown in (a), (b), (d), and (e) of Figure~\ref{fig:landscape:FCNN:vs:MMNN}, the learning landscape of MMNNs is considerably simpler than that of FCNNs. Likewise, (b), (c), (e), and (f) of Figure~\ref{fig:landscape:FCNN:vs:MMNN} clearly illustrate that our learning strategy, which involves fixing parameters in $\tildecalA_2$ while optimizing those in $\tildecalA_1$ rather than the reverse, is well-justified and reasonable. We would like to point out that
different initializations can produce varying results; however, the overall landscape complexity remains largely consistent. The figures shown in Figure~\ref{fig:landscape:FCNN:vs:MMNN} represent relatively complex cases among several initializations with identical settings.
 We have experimented with deeper networks and other target functions, and the results are generally similar to the two-hidden-layer cases presented.
 For deeper MMNNs, if two parameters are selected from the trainable parameters (i.e., \( \bmA_i \)'s and \( \bmc_i \)'s, see Section~\ref{sec:MMNN:structure}), the landscape always remains simple, reflecting the effectiveness and rationality of our training strategy.

% As observed in (a) and (b) of Figure~\ref{fig:landscape:FCNN:vs:MMNN}, the learning landscape of MMNNs is significantly simpler compared to that of FCNNs.
% Similarly, from (b) and (c) of Figure~\ref{fig:landscape:FCNN:vs:MMNN}, it is evident that our learning strategy—fixing parameters in $\tildecalA_2$ while learning parameters in $\tildecalA_1$ (rather than the reverse)—is well-justified and reasonable.


% \begin{remark}
% Different initializations can produce varying results; however, the overall landscape complexity remains largely consistent. The figures shown in Figure~\ref{fig:landscape:FCNN:vs:MMNN} represent relatively complex cases among several initializations with identical settings.
%  We have experimented with deeper networks and other target functions, and the results are generally similar to the two-hidden-layer cases presented.
%  For deeper MMNNs, if two parameters are selected from the trainable parameters (i.e., \( \bmA_i \)'s and \( \bmc_i \)'s, see Section~\ref{sec:MMNN:structure}), the landscape always remains simple, reflecting the effectiveness and rationality of our training strategy.
% \end{remark}


\begin{figure}[ht]%[htbp!]  
            \centering
            % \begin{subfigure}[b]{0.27300245\textwidth}
            %         \centering            
            %         \includegraphics[width=0.999\textwidth]{figures/LandScapeFCNNWidx1.pdf}
            %         \subcaption{FCNN ($\calA_1$ in \eqref{eq:FCNN:eg}).}
            %     \end{subfigure}
            %     \hfill
            % \begin{subfigure}[b]{0.27300245\textwidth}
            %         \centering            \includegraphics[width=0.999\textwidth]{figures/LandScapeMMNNWidx1.pdf}
            %         \subcaption{MMNN ($\tildecalA_1$ in \eqref{eq:MMNN:eg}).}
            %     \end{subfigure}
            %     \hfill
            % \begin{subfigure}[b]{0.27300245\textwidth}
            %         \centering            \includegraphics[width=0.999\textwidth]{figures/LandScapeMMNNWidx2.pdf}
            %         \subcaption{MMNN ($\tildecalA_2$ in \eqref{eq:MMNN:eg}).}
            %     \end{subfigure}\\
            %                 
            \,\hfill\begin{subfigure}[b]{0.27300245\textwidth}
                    \centering            
                    \includegraphics[width=0.999\textwidth]{figures/LandScapeFCNN1Act1.pdf}
                    \subcaption{FCNN ($\calA_1$ in \eqref{eq:FCNN:eg}).}
                \end{subfigure}
                \hfill
            \begin{subfigure}[b]{0.27300245\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/LandScapeMMNN1Act1.pdf}
                    \subcaption{MMNN ($\tildecalA_1$ in \eqref{eq:MMNN:eg}).}
                \end{subfigure}
                \hfill
            \begin{subfigure}[b]{0.27300245\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/LandScapeMMNN2Act1.pdf}
                    \subcaption{MMNN ($\tildecalA_2$ in \eqref{eq:MMNN:eg}).}
                \end{subfigure}\hfill
                \,
                \\
                \,\hfill\begin{subfigure}[b]{0.27300245\textwidth}
                    \centering            
                    \includegraphics[width=0.999\textwidth]{figures/LandScapeFCNN1Act2.pdf}
                    \subcaption{FCNN ($\calA_1$ in \eqref{eq:FCNN:eg}).}
                \end{subfigure}
                \hfill
            \begin{subfigure}[b]{0.27300245\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/LandScapeMMNN1Act2.pdf}
                    \subcaption{MMNN ($\tildecalA_1$ in \eqref{eq:MMNN:eg}).}
                \end{subfigure}
                \hfill
            \begin{subfigure}[b]{0.27300245\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/LandscapeMMNN2Act2.pdf}
                    \subcaption{MMNN ($\tildecalA_2$ in \eqref{eq:MMNN:eg}).}
                \end{subfigure}\hfill
                \,
                
                \caption{Comparison of the cost function landscapes for FCNNs and MMNNs. All parameters are initialized using PyTorch's default linear initialization. Here, $z$ represents the cost function, while $x$ and $y$ denote two parameters  from the weights of
\({\mathcal{A}}_1\), \({\tilde{\mathcal{A}}}_1\), and \({\tilde{\mathcal{A}}}_2\) in 
 \eqref{eq:FCNN:eg} and \eqref{eq:MMNN:eg}.
% (b,e) \({\tilde{\mathcal{A}}}_1\) in \eqref{eq:MMNN:eg}, and 
% (c,f) \({\tilde{\mathcal{A}}}_2\) in \eqref{eq:MMNN:eg}.
The top and bottom rows correspond to the \sine{} and $\SinTU_0$ activation functions, respectively.
% target function $f(x)=\sin (2x )$; Bottom row: target function 
% $f(x)=\frac{1}{1+100x^2}$.
}


%     \caption{Landscape comparison for FCNNs and MMNNs.  all paramters are init from pytroch linear default. $z$ is loss function, $x,y$ are two paramters (first and last ones) from  weights of (a) $\bmcalL_1$ in \eqref{eq:FCNN:eg}.
% (b)  $\bmtildecalL_1$ in \eqref{eq:MMNN:eg}.
% (c)  $\bmtildecalL_2$ in \eqref{eq:MMNN:eg}.
% }
    \label{fig:landscape:FCNN:vs:MMNN}
    
\end{figure}


% \subsection{Further Discussion}
% \label{sec:further:discussion}

% % The two key components of a neural network are its architecture and the choice of activation function, both of which jointly determine its effectiveness. Notably, n

% Network representation and training efficiency are closely interconnected and must be considered together to achieve strong practical performance. For example, although shallow neural networks (i.e., those with a single hidden layer) are theoretically capable of universal approximation in the infinite-width limit if they use a nonlinear, non-polynomial activation function, their ability to approximate non-smooth functions remains limited when global activation functions are used. This limitation arises from the strong correlations among activation functions (parameterized by weights and biases), leading to ill-conditioning and a bias against high frequencies in both representation and training, as analyzed in \cite{ZZZZ-23}. Furthermore, it has been shown that the smoother the activation function, the more severe the ill-conditioning and the stronger the bias against high frequencies. Consequently, smooth activation functions such as \texttt{tanh}, \texttt{sigmoid}, and \texttt{sine} perform poorly when approximating functions containing relatively high-frequency components, such as those with rapid transitions or oscillations. The best achievable numerical accuracy is fundamentally constrained by the truncation frequency, which depends on the activation function’s conditioning properties, the machine error or noise level in the data, and the frequency components of the target function. Importantly, this limitation persists regardless of the available computational resources, meaning that increasing network width or extending training time cannot overcome it.

% We compare the approximation performance of shallow neural networks with different activation functions. A shallow network, typically with a single hidden layer, is expressed as  
% \begin{equation}
% \label{eq:def:shallow:net}
%     h(\bmx) = \sum_{i=1}^n a_i \sigma(\bmw_i \cdot \bmx - b_i) + c, \quad \text{for any } \bmx \in \mathbb{R}^d,
% \end{equation}
% where $n \in \mathbb{N}^+$ is the network width, and $\bmw_i \in \mathbb{R}^d$, $a_i, b_i, c \in \mathbb{R}$ are parameters for each $i \in \{1,2,\dots,n\}$. The activation function is denoted by $\sigma$.  
% A key tool for analyzing shallow networks is the Gram matrix (e.g., see \cite{ZZZZ-23}), which provides insights into their ability to distinguish inputs and represent complex functions. In learning dynamics, the Gram matrix influences convergence and generalization, with its eigenvalues affecting learning speed. A well-conditioned Gram matrix, with balanced eigenvalues, generally leads to stable and efficient learning.  
% Focusing on the 1D case, we define the Gram matrix as  
% \begin{equation*}
%    \bmG_{i,j} \coloneqq  \int_{D} \sigma(w_i x - b_i) \sigma(w_j x - b_j) dx,
% \end{equation*}
% where $D = [-1,1]$. The spectra of Gram matrices for various activation functions are illustrated in Figure~\ref{fig:spectrum:gram:m}.
 

%  \begin{figure}[!htp]
% 		\centering
% 		\begin{subfigure}[b]{0.324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/SpecturmAll0.pdf}
% 			\subcaption{$\bmw\sim \calU(-0.2,0.2)$.}
% 		\end{subfigure}
% \hfill
% 		\begin{subfigure}[b]{0.324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/SpecturmAll1.pdf}
% 			\subcaption{$\bmw\sim \calU(-1,1)$.}
% 		\end{subfigure}\hfill
% 		\begin{subfigure}[b]{0.324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/SpecturmAll2.pdf}
% 			\subcaption{$\bmw\sim \calU(-5,5)$.}
% 		\end{subfigure}
% 	\caption{Spectra of Gram matrices corresponding to various activation functions, where $\bmb\sim \calU(-1,1)$.}
% 	\label{fig:spectrum:gram:m}
% \end{figure}


% In this test, we use shallow networks with widths of 256, 512, or 1024 to approximate the target functions $f_i$ ($i=1,2,3$), whose expressions are provided in Table~\ref{tab:error:comparison:shallow}. A total of 1000 uniformly sampled points in $[-1,1]$ are used, with a learning rate set as $10^{-3} \times 0.9^{\lfloor k/300 \rfloor}$, where $k = 1,2,\dots,15000$ denotes the epoch number. The mini-batch size is set to 100, meaning all samples are trained simultaneously. This test is conducted using double precision.  

% As shown in Figure~\ref{fig:shllow:nets:approx:f2} and Table~\ref{tab:error:comparison:shallow}, among shallow networks, $\SinTU_0$ achieves the best performance, followed closely by \ReLU. The performance of activation functions generally aligns with the decay rate of their corresponding Gram matrix spectra—the slower the decay, the better the shallow network performs. This observation supports our analysis in \cite{ZZZZ-23}.

%  \begin{figure}[!htp]
% 		\centering
% 		\begin{subfigure}[b]{0.192324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/ShallowNets1D1.pdf}
% 			\subcaption{\ReLU.}
% 		\end{subfigure}
% \hfill
% 		\begin{subfigure}[b]{0.192324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/ShallowNets1D2.pdf}
% 			\subcaption{\GELU.}
% 		\end{subfigure}\hfill
% 		\begin{subfigure}[b]{0.192324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/ShallowNets1D3.pdf}
% 			\subcaption{\texttt{tanh}.}
% 		\end{subfigure}
%         \hfill
% 		\begin{subfigure}[b]{0.192324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/ShallowNets1D4.pdf}
% 			\subcaption{\sine.}
% 		\end{subfigure}
%         \hfill
% 		\begin{subfigure}[b]{0.192324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/ShallowNets1D7.pdf}
% 			\subcaption{$\SinTU_0$.}
% 		\end{subfigure}
% 	\caption{Illutrations of the true function $f_2(x)=\frac{1}{1+600(x-0.2)^2}$ and learned shallow networks of width $512$ with various activation functions.}
% 	\label{fig:shllow:nets:approx:f2}
% \end{figure}

% \begin{table}[ht]%[htbp!]
% 	\centering  
%  \setlength{\tabcolsep}{0.68em} 
%  \renewcommand{\arraystretch}{1.15}
% \caption{Comparison of test errors. float64}
% \label{tab:error:comparison:shallow}
% 	\resizebox{0.95\textwidth}{!}{ 
% 		\begin{tabular}{ccccccccc} 
% 			\toprule% \toprule[1.2pt]  
%             % \multicolumn{2}{c}
%             % {
%             % target function
%             % }
%             % & \multicolumn{2}{c}{$f_1:[-1,1]\to \R$}
%             % & \multicolumn{2}{c}{$f_2:[-1,1]^2\to \R$}
%             % & \multicolumn{2}{c}{$f_3:[-1,1]^2\to \R$}
%             % % & \multicolumn{2}{c}{$f_4:[-1,1]^3\to \R$}
%             % \\        
%             % {target function} &\multicolumn{2}{c}{$f_1(x)=\frac{1}{1+100x^2}$} &\multicolumn{2}{c}{$f_2(x)=\frac{1}{1+600x^2}$} &\multicolumn{2}{c}{$f_3(x)=\frac{1}{1+3600x^2}$} \\
%         {target function} &\multicolumn{2}{c}{$f_1(x)=\frac{1}{1+100(x-0.2)^2}$} &\multicolumn{2}{c}{$f_2(x)=\frac{1}{1+600(x-0.2)^2}$} &\multicolumn{2}{c}{$f_3(x)=\frac{1}{1+3600(x-0.2)^2}$} \\
%             % \cmidrule(lr){1-2} 
%             % \cmidrule(lr){3-4}
%             % \cmidrule(lr){5-6}
%             % \cmidrule(lr){7-8}
%             \cmidrule(lr){2-3}
%             \cmidrule(lr){4-5}
%             \cmidrule(lr){6-7}
%             % \cmidrule(lr){8-9}
%         \rowcolor{mygray}   
%         % \multicolumn{2}{c}{network} 
%         % &
%         &\multicolumn{2}{c}{Shallow Network of width 256} &\multicolumn{2}{c}{Shallow Network of width 512} &\multicolumn{2}{c}{Shallow Network of width 1024}
%         % &\multicolumn{2}{c}{ResMMNN of size (1024,40,16)}
%         \\
%            %             \cmidrule(lr){3-4}
%            %  \cmidrule(lr){5-6}
%            %  \cmidrule(lr){7-8}
%            % & &\multicolumn{2}{c}{MMNN1}
%            %  &\multicolumn{2}{c}{MMNN2}
%            %  &\multicolumn{2}{c}{FCNN1}
%            %  \\
%             % \cmidrule(lr){3-4}
%             % \cmidrule(lr){5-6}
%             % \cmidrule(lr){7-8}
%             \cmidrule(lr){2-3}
%             \cmidrule(lr){4-5}
%             \cmidrule(lr){6-7}
%             \cmidrule(lr){8-9}
% 			 %    target  &  activation &        TE (MSE) 
%     % & TE (MAX) &        TE (MSE) 
%     % & TE (MAX)
%     % &        TE (MSE) 
%     % & TE (MAX)\\
%     % \multicolumn{2}{c}
%         {activation} &         MSE 
%     &  MAX &    MSE 
%     &  MAX &    MSE 
%     &  MAX
%     % &   
%     % MSE 
%     % &  MAX
%     \\
    
%     % {activation} &        test error (MSE) 
%     % & test error (MAX) &        test error (MSE) 
%     % & test error (MAX)
%     % &        test error (MSE) 
%     % & test error (MAX)\\
% 			 % \cmidrule{3-5}
% 			 % & & MSE & MAE & MAX\\
% 			\midrule

% $\mathtt{ReLU}$ &  $ 2.50 \times 10^{-7} $  &  $ 3.65 \times 10^{-3} $  &  $ 9.94 \times 10^{-7} $  &  $ 8.40 \times 10^{-3} $  &  $ 2.46 \times 10^{-4} $  &  $ 1.43 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{GELU}$ &  $ 2.82 \times 10^{-3} $  &  $ 2.00 \times 10^{-1} $  &  $ 1.06 \times 10^{-2} $  &  $ 5.86 \times 10^{-1} $  &  $ 8.85 \times 10^{-3} $  &  $ 8.41 \times 10^{-1} $ 
%  \\ 

%  $\mathtt{tanh}$ &  $ 9.24 \times 10^{-5} $  &  $ 3.86 \times 10^{-2} $  &  $ 3.10 \times 10^{-3} $  &  $ 3.27 \times 10^{-1} $  &  $ 5.84 \times 10^{-3} $  &  $ 7.22 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{sin}$ &  $ 5.93 \times 10^{-3} $  &  $ 2.71 \times 10^{-1} $  &  $ 1.15 \times 10^{-2} $  &  $ 6.09 \times 10^{-1} $  &  $ 9.72 \times 10^{-3} $  &  $ 8.78 \times 10^{-1} $ 
%  \\ 

%  $\mathtt{cos}$ &  $ 6.36 \times 10^{-3} $  &  $ 2.76 \times 10^{-1} $  &  $ 1.38 \times 10^{-2} $  &  $ 6.41 \times 10^{-1} $  &  $ 1.08 \times 10^{-2} $  &  $ 9.08 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{cos(\cdot -\pi/4)}$ &  $ 6.04 \times 10^{-3} $  &  $ 2.75 \times 10^{-1} $  &  $ 1.25 \times 10^{-2} $  &  $ 6.23 \times 10^{-1} $  &  $ 9.60 \times 10^{-3} $  &  $ 8.91 \times 10^{-1} $ 
%  \\ 

%  $\mathtt{SinTU}_{0}$ &  $ \bm{5.95 \times 10^{-8}} $  &  $ 2.47 \times 10^{-3} $  &  $ \bm{6.75 \times 10^{-7}} $  &  $ 6.35 \times 10^{-3} $  &  $ \bm{2.19 \times 10^{-4}} $  &  $ 1.28 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{SinTU}_{-\pi}$ &  $ 2.53 \times 10^{-3} $  &  $ 1.82 \times 10^{-1} $  &  $ 1.03 \times 10^{-2} $  &  $ 5.83 \times 10^{-1} $  &  $ 1.30 \times 10^{-2} $  &  $ 8.92 \times 10^{-1} $ 
%  \\ 
%  \bottomrule% \bottomrule[1.2pt] 
% 		\end{tabular} 
% 	}%%% \resizebox
% % 	\vskip -0.1in
% \end{table} 

% While multi-layer networks enhance representational power through compositions of shallow networks, their architecture is crucial for both effectiveness and training efficiency. Most training methods rely on first-order gradient descent techniques, which are inherently local and sensitive to the ill-conditioning of the cost function (Jacobian) due to the large number of parameters.
% In our earlier work \cite{ZZZZ-24-MMNN}, we introduced structured and balanced multi-component and multi-layer neural networks (MMNNs), leveraging insights from one-hidden-layer networks. The key idea is to treat a linear combination of a family of neurons (randomly parameterized activation functions), called a component, as a single unit that can be efficiently trained to approximate a smooth function. Each layer consists of multiple components that share a common set of randomly parameterized neurons, whose count defines the layer’s width, while different components use distinct linear combinations. The number of components, known as the  rank, is typically much smaller than the layer width, and both can be adjusted to improve function decomposition and representation. These components are systematically combined and composed across layers in a structured and balanced manner to approximate target functions effectively.
% The structure of MMNNs is described in detail in Section~\ref{sec:MMNN:structure}.

% A key feature of MMNNs is that weights and biases inside activation functions remain fixed during training, while only the linear combination weights of neurons within each component are updated. This structure leads to significantly more efficient training, supported by two main findings: (1) a moderately sized one-hidden-layer network can effectively approximate a smooth function using random basis functions \cite{ZZZZ-23}, and (2) complex functions can be decomposed and composed within the MMNN framework \cite{ZZZZ-24-MMNN}.
% MMNNs provide a simple yet impactful modification of fully connected neural networks (FCNNs), also known as multi-layer perceptrons (MLPs), by incorporating balanced multi-component structures. This results in fewer trainable parameters, more efficient training, and significantly improved accuracy compared to conventional FCNNs \cite{ZZZZ-24-MMNN}.




\subsection{Related Work}
\label{sec:related:work}


% \paragraph{Approximation}
Extensive research has explored the approximation capabilities of neural networks across various architectures. Early works established the universal approximation theorem for single-hidden-layer networks~\cite{Cybenko1989ApproximationBS,HORNIK1991251,HORNIK1989359}, proving their ability to approximate specific functions arbitrarily well, though without quantifying error in relation to network size. Subsequent studies~\cite{yarotsky18a,yarotsky2017,doi:10.1137/18M118709X,ZHOU2019,10.3389/fams.2018.00014,2019arXiv190501208G,2019arXiv190207896G,MO,shijun:NonlineArpprox,shijun:Characterized:by:Numer:Neurons,shijun:smooth:functions,shijun:arbitrary:error:with:fixed:size,shijun:thesis,shijun:intrinsic:parameters,shijun:2023:beyond:ReLU:to:diverse:actfun,yarotsky:2019:06, fang2024addressing,shijun:2023:beyond:ReLU:to:diverse:actfun} analyzed approximation errors, relating them to network width, depth, or parameter count, and addressed the spectral bias in neural network approximations. 
Here, we specifically highlight two papers \cite{shijun:arbitrary:error:with:fixed:size, yarotsky:2019:06} that are closely related to our theoretical results.
 The results in \cite{yarotsky:2019:06} imply that \ReLU/\sine-activated fully connected neural networks (FCNNs) with width $O(d)$ and depth $O(L)$ can approximate a 1-Lipschitz function $f:[0,1]^d\to\mathbb{R}$ within an error of $O(2^{-\sqrt{L}})$. Compared to this, our work achieves several key improvements: 
1) Our results incorporate width $N$, extending beyond fixed-width networks.  
2) We improve the approximation error rate to $O(N^{-L})$, much better than $O(2^{-\sqrt{L}})$ when $N$ fixed.  
3) We introduce \SinTU{}, a novel hybrid activation function, where a single \SinTU{} function can replace two activation functions (\ReLU,\,\sine) in the approximation results.  
4) Our results specifically apply to MMNNs, which introduce an additional dimension called rank beyond width and depth.  
In \cite{shijun:arbitrary:error:with:fixed:size}, the author proposes a simple activation function, \EUAF, and demonstrates that a fixed-size \EUAF-activated FCNN can approximate any continuous function $f\in C([0,1]^d)$ to an arbitrarily small error by adjusting only finitely many parameters. 
\EUAF{} emphasizes theoretical approximation but tends to perform less effectively in practice, often appearing somewhat artificial. 
In contrast, our FMMNN is naturally structured. Although its theoretical approximation power is comparatively weaker, an exponential approximation rate is typically sufficient in practical applications. Our extensive experiments further confirm its effectiveness, demonstrating surprisingly strong empirical performance.




% However, the above works primarily focus on theoretical constructions, with limited attention to computational parameter determination, numerical errors, and finite-precision effects in simulations. This paper first establishes strong approximation results, then employs an intuitive landscape analysis of the cost function to assess learning potential, and finally validates our findings through extensive numerical experiments.
% \paragraph{\Sine{} activation function}


Some previous works have explored the use of \sine{} as an activation function in multi-layer networks \cite{doi:10.1137/19M1310050,novello2024tamingfrequencyfactorysinusoidal,2025arXiv250200869M,fathony2021multiplicative,doi:10.1137/21M144431X,NEURIPS2020_53c04118}. The authors in \cite{novello2024tamingfrequencyfactorysinusoidal,2025arXiv250200869M} present numerical examples demonstrating the advantages of sine-activated networks, primarily in FCNNs. Additionally, \cite{fathony2021multiplicative} introduces a multiplicative filter network that incorporates sinusoidal filters and illustrates its benefits through various examples.
To the best of our knowledge, existing works do not provide strong mathematical or numerical justification for the benefits of \texttt{sine}, which is a key reason why it is rarely used in practice.
This paper first establishes rigorous approximation results, followed by an intuitive landscape analysis of the cost function to assess learning potential, and finally validates these findings through extensive numerical experiments. Our experiments show that using the \texttt{sine} activation in FCNNs does not always lead to good performance. While it works well in some cases, it performs quite poorly in others. We believe this inconsistency is the main reason why \texttt{sine} is not widely adopted in practice.
Surprisingly, we found that our MMNN network structure and \texttt{sine} (or \SinTU{s}) form a highly effective combination. Compared to FCNNs, MMNNs exhibit a simpler optimization landscape. Our experiments confirm this observation, as MMNNs achieve consistently strong performance across all test cases.



% This gap motivated our current investigation, which considers practical training processes and numerical errors. Specifically, the balanced structure of MMNN, the choice of training parameters, and the associated learning strategy discussed here are intended to facilitate a smooth decomposition of the function, thereby promoting an efficient training process.

% \paragraph{Low-rank methods }
% Low-rank structures in $\bmW$ of a fully connected neural network have been investigated by a variety of groups.
% % which is equivalent to pursuing a linear dimension reduction between layers.
% \zhou{For example, the methods proposed in \cite{2022arXiv220913569R,9157223,6638949} focus on accelerating training and reducing memory requirements while maintaining final performance. The low-rank idea is generalized to tensor train decomposition in~\cite{novikov2015tensorizing}.}
% %However this may not be effective in a nonlinear representation using a deep neural network. For example, the low-rank methods proposed in \cite{2022arXiv220913569R,9157223,6638949} focus on accelerating training and reducing memory requirements while maintaining final performance. 
% \zhou{This is different from the MMNN proposed here in two aspects. First there are two matrices in each layer, $\bmA$ outside and $\bmW$ inside the activation functions. Each row of $\bmA$ is the weights to a linear combination of a set of random basis functions which forms a component in each layer. The number of rows of $\bmA$, which is the number of components, are selected depending on the complexity of the function and is usually much smaller than the number of columns, which corresponds to the number of basis functions. Each row of $(\bmW, \bmb)$ corresponds to a random parametrization of a basis function. The number of rows of $\bmW$ corresponds to the number of basis functions which is usually much larger than the number of columns of $\bmW$ which is the input dimension. Secondly, in our MMNN, only $\bmA$ is trained while $\bmW$ is fixed with randomly initialized values. The theoretical study and numerical experiments demonstrate that the architecture of MMNN, combined with the learning strategy, is effective in approximating complicate functions. 
% %Our key contribution is the design of a balanced multi-component and multi-layer structure to decompose the complexity of the target function that is conducive to approximation and learning. We theoretically and numerically demonstrate that our architecture, combined with our learning strategy, is effective in approximating complex functions. 
% }

% \paragraph{Low-rank methods}
% Low-rank structures in the weight matrix $\bmW$ of a fully connected neural network have been investigated by various groups. For example, the methods proposed in \cite{2022arXiv220913569R,9157223,6638949} focus on accelerating training and reducing memory requirements while maintaining final performance. The concept of low-rank structures is further extended to tensor train decomposition in~\cite{novikov2015tensorizing}.
% The MMNN proposed here differs in two key aspects. First, each layer contains two matrices: $\bmA$ outside and $\bmW$ inside the activation functions. Each row of $\bmA$ represents the weights for a linear combination of a set of random basis functions, forming a component in each layer. The number of rows in $\bmA$, which equals the number of components, is selected based on the complexity of the function and is typically much smaller than the number of columns, corresponding to the number of basis functions. Each row of $(\bmW, \bmb)$ represents a random parameterization of a basis function, with the number of rows in $\bmW$ corresponding to the number of basis functions, usually much larger than the number of columns in $\bmW$, which is the input dimension.
% Secondly, in our MMNN, only $\bmA$ is trained while $\bmW$ remains fixed with randomly initialized values. Theoretical studies and numerical experiments demonstrate that the architecture of MMNN, combined with the learning strategy, is effective in approximating complex functions. 
% Fixing $(\bmW, \bmb)$ of each layer and use of random basis functions in the MMNNs is inspired by a previous approach known as random features \cite{NIPS2007_013a006f,NIPS2016_e7061188,9495136,NIPS2017_61b1fb3f,peng2021random}. In typical random feature methods, only the linear combination parameters at the output layer are trained which also leads to the issue of ill-conditioning of the representation. While in MMNNS matrix $\bmA$ and vector $\bmc$ of each layer are trained. 
% % Although this approach increases the computational cost compared to random feature methods, it is still less expensive than training a network where both $(\bmA,\bmc)$ and $(\bmW, \bmb)$ are learnable parameters. Additionally,
% Our MMNN employs a composition architecture and learning mechanism that enhances the approximation capabilities compared to random feature methods while achieving a more effective training process than a standard fully connected network of equivalent size. Extensive experiments demonstrate that our approach can strike a satisfactory balance between approximation accuracy and training cost.


% \paragraph{Komogolrov-Arnold (KA) representation:} \zhou{KA representation states a multivariate continuous function on a bounded domain can be written as a finite composition of continuous functions of a single variable and the binary operation of addition. Despite of the possibility of encountering non-smooth or even fractal univariate functions in the KA representation, it provides an appealing framework to build neural network structures to approximate functions especially in higher dimensions. This idea has been explored in a number of studies \cite{2024arXiv240419756L} (please add citations). For example, a recent proposed network called KA network (KAN) uses spline functions to approximate the univariate functions in the KA representation. Aimed at approximating highly oscillatory functions, the proposed MMNN is based on the idea of ``divide and conquer'', which uses different network architectures, activation functions, and training cost than those KA representation based methods.}
%Recently, a network structure inspired by the Kolmogorov-Arnold (KA) representation, called KAN, is proposed. KA representation states a multivariate continuous function on a bounded domain can be written as a finite composition of continuous functions of a single variable and the binary operation of addition. The key idea of KAN is to use spline functions to approximate those univariate functions. However, those univariate functions may be highly non-smooth or even fractal which makes splines difficult to approximate well. Hence a network that is wider and deeper than the compact form of KA representation is needed in practice.  Moreover, the coordinate system may be artificial and simple additions of different univariate functions may not be effective in approximating a generic multi-variate function. Also the training process seems relative expensive. 

% \paragraph{Komogolrov-Arnold (KA) representation} 
% The KA representation theorem \cite{kolmogorov1957} states that any multivariate continuous function on a hypercube can be expressed as a finite composition of continuous univariate functions and the binary operation of addition. However, this elegant mathematical representation may result in non-smooth or even fractal univariate functions in general due to this very specific form of representation, a computational challenge one has to address in practice. 
% %Although this representation may result in non-smooth or even fractal univariate functions, it offers a compelling framework for constructing neural network structures to approximate functions, particularly in higher dimensions. 
% KA representation has been explored in several studies \cite{2024arXiv240419756L,MAIOROV199981,shijun:arbitrary:error:with:fixed:size,ISMAYILOVA2024106333}. A recently proposed network known as the KA network (KAN) utilizes spline functions to approximate the univariate functions in the KA representation. The proposed MMNN is motivated by a multi-component and multi-layer smooth decomposition, or a ``divide and conquer" approach, employing distinct network architectures, activation functions, and training strategies.


% 	To compute the integral
% 	\[
% 	\int \sin(ax + b) \sin(cx + d) \, dx,
% 	\]
% 	we use the product-to-sum identity:
% 	\[
% 	\sin(A) \sin(B) = \frac{1}{2} \left[\cos(A - B) - \cos(A + B)\right].
% 	\]
% 	Applying this identity with \( A = ax + b \) and \( B = cx + d \), we get
% 	\[
% 	\sin(ax + b) \sin(cx + d) = \frac{1}{2} \left[\cos((a - c)x + (b - d)) - \cos((a + c)x + (b + d))\right].
% 	\]
% 	Thus, the integral becomes
% 	\[
% 	\int \sin(ax + b) \sin(cx + d) \, dx = \frac{1}{2} \int \left[\cos((a - c)x + (b - d)) - \cos((a + c)x + (b + d))\right] dx.
% 	\]
% 	Now, we can integrate each term separately:
% 	\[
% 	= \frac{1}{2} \left( \int \cos((a - c)x + (b - d)) \, dx - \int \cos((a + c)x + (b + d)) \, dx \right).
% 	\]
% 	For each cosine term, the integral is straightforward:
% 	\[
% 	\int \cos(kx + \phi) \, dx = \frac{1}{k} \sin(kx + \phi),
% 	\]
% 	where \( k \) is a constant. Therefore,
% 	\[
% 	\int \sin(ax + b) \sin(cx + d) \, dx = \frac{1}{2} \left( \frac{\sin((a - c)x + (b - d))}{a - c} - \frac{\sin((a + c)x + (b + d))}{a + c} \right) + C,
% 	\]
% 	where \( C \) is the constant of integration.
    


% $u,v$ fixed, $w,b$ learnable
% \begin{equation*}
%     \int_{-m}^{m} \big(\sin  (wx+b) - \sin(ux+v)\big)^2 \, dx
% \end{equation*}

% $u,v$ fixed, $w,b$ learnable
% \begin{equation*}
%     \int_{-m}^{m} \big(w\sin  (x)+b - u \sin(x)-v\big)^2 \, dx
% \end{equation*}

% Formally, we are minimizing the squared loss \((\sin(\nu x) - \sin(wx))^2\). For a fixed choice of \(\nu\) and \(m\),
% the loss landscape \(L(\nu, w, m)\) with respect to \(w\) has the form
% \[
% L(\nu, w, m) = \int_{-m}^{m} (\sin(\nu x) - \sin(wx))^2 \, dx
% = -\frac{2 \sin \left( m (w - \nu) \right)}{w - \nu} + \frac{2 \sin \left( m (w + \nu) \right)}{w + \nu} - \frac{\sin (2mw)}{2w} + c(\nu, m)
% \]
% where \(c(\nu, m)\) is a constant term. As illustrated in Fig. 1, for a fixed choice of \(\nu\) and \(m\), the three main terms in \(L(\nu, w, m)\) are three cardinal \sine{} s (or sincs): the first is negative and centered at \(w = \nu\), which is the only global minimum and where the loss is 0; the second term is positive and centered at \(w = -\nu\), and is the only global maximum; the third sinc is negative and centered in \(w = 0\). The latter creates a local minimum for small values of \(w\) and large values of \(m\) and \(\nu\), where the function expressed by the network is a constant \(\sin(0) = 0\).





% \clearpage
\section{Numerical Experiments}
\label{sec:experiments}

In this section, we conduct extensive experiments to validate our analysis and demonstrate the effectiveness of MMNNs. Across all tests, we ensure: (1) sufficient data sampling to capture fine details of the target function, (2) the use of the Adam optimizer \cite{DBLP:journals/corr/KingmaB14} for training, (3) mean squared error (MSE) as the training loss function, with both MSE and MAX ($L^\infty$-norm) used for evaluation, (4) parameter initialization following PyTorch's default settings,
% unless stated otherwise, 
(5) fixed $\bmW$ and $\bmb$ (parameters inside activation functions), while only $\bmA$ and $\bmc$ (parameters outside activation functions) are trained (see Section~\ref{sec:MMNN:structure} for details).
% , and (6) single precision is used unless otherwise specified.  

Section~\ref{sec:FMMNN_vs_FCNN} compares MMNNs and FCNNs across various activation functions, including \ReLU, \GELU, \texttt{tanh}, \texttt{sine}, \cosine, and \SinTU{s}. Section~\ref{sec:sine_vs_others} focuses on MMNNs, analyzing the performance of \texttt{sine} relative to other activation functions. 
% In Section~\ref{sec:derivative:approx}, we examine derivative approximation using only function values in the cost function. Section~\ref{sec:shallow:nets} evaluates the effectiveness of different activation functions in shallow networks.

% 6)
% computations are conducted on an \textit{NVIDIA RTX 3500 Ada Generation Laptop GPU (power cap 130W)}, with most experiments concluding within a range from a few dozen to several thousand seconds.
% All our MMNN setups are specified by three parameters $(w,r,l)$ which depends on the function complexity. Another tuning parameter is the learning rate the choice of which is guided by the following criteria: 1) not too large initially due to stability, 2) a decreasing rate with iterations such that the learning rate becomes small near the equilibrium to achieve a good accuracy while not decreasing too fast (especially during a long training process for more difficult target functions) so that the training is stalled. 

% In Section~\ref{sec:FMMNN_vs_FCNN}, we examine FMMNNs versus FCNNs using the relu, \sine{}, and SinTu activation function.
% In Section~\ref{sec:sine_vs_others}, we focus on MMNNs and compare the \sine{}  activation function with other activation functions. 

% \subsection{FCNN vs MMNN: diff activations}
% \label{sec:FMMNN_vs_FCNN}
% % \subsection{Comparison of FMMNNs and others}

% \subsection{Sine Versus Other Activation Functions for MMNNs}
% \label{sec:sine_vs_others}

% \red{One hidden layer, Least square vs Adam; spectrum decay and  experiments; diff act}



\subsection{MMNNs Versus FCNNs}
\label{sec:FMMNN_vs_FCNN}


% \subsubsection{Infinitely Differentiable Example}

% We will investigate the potential of FMMNNs for approximating derivatives by minimizing a cost function that depends solely on the function values of the target function at sampled points. 


We will thoroughly compare the performance of MMNNs and FCNNs using various activation functions.
The overall message from these tests is that: 1) MMNNs perform better than FCNNs when the same activation function is used, and 2) FMMNNs always produce the best results. 
In our tests, we select rather complex target functions -- highly oscillatory with or without non-smoothness, as both network types perform well on simple functions, making their differences less apparent. Additionally, our target functions should not be generated using \sine, \cosine, or their compositions and combinations, since we use \sine{} as the activation function.
Based on these considerations, we first choose an oscillatory target function $f_1\in C^\infty(\R)$ defined as
\begin{equation}\label{eq:def:f1:Cinfty:MMNN:vs:FCNN}
    f_1(x) = \frac{1}{1 + 2x^2}\sum_{i=-n}^{n}   (-1)^{(i\bmod 3)}\cdot\frac{|i| + n}{ n}  \cdot g\left( (2n+1)\left(x-\frac{i}{n+1} \right)\right),\\
\end{equation}
where \( n = 36 \), and \( (i \bmod 3)\in \{0,1,2\} \) represents the remainder when an integer \( i\in\Z \) is divided by 3.
Here, $g$ serves as a basis function, defined as
\begin{equation*}
   g(x) = \frac{g_0(x+1) g_0(1-x)}{g_0^2(1)},\quad \tn{where}\quad g_0(x) =
\begin{cases}
 \exp\left(-\frac{1}{x^2}\right) &\tn{if}\   x > 0, \\
 0 & \tn{if}\  x \leq 0.
\end{cases}
\end{equation*}
It is easy to verify that \( g_0 \in C^\infty(\mathbb{R}) \), and hence \( f \in C^\infty(\mathbb{R}) \). See Figure~\ref{fig:fig:f1:g:f2:and:zoomin} for illustrations of \( f_1 \) and \( g \).
% \begin{figure}[ht]%[htbp!]  
%             \centering
%             \begin{subfigure}[b]{0.71\textwidth}
%                     \centering        
%                     \includegraphics[width=0.999\textwidth]{figures/MMNNvsFCNN_f1.pdf}
%                     % \subcaption{$f$.}
%                 \end{subfigure}
%                 \hfill
%             \begin{subfigure}[b]{0.265\textwidth}
%                     \centering         \includegraphics[width=0.999\textwidth]{figures/MMNNvsFCNN_g.pdf}
%                     % \subcaption{$f^\prime$}
%                 \end{subfigure}     
% \caption{Illustrations of $f_1$ (left) and $g$ (right).}
%     \label{fig:f1:and:g}   
% \end{figure}
Next, we choose two non-smooth oscillatory functions $f_2, f_3 \in C^0(\R)\backslash C^1(\R)$ given by 
%To compare the performance of MMNNs and FCNNs with various activation functions, we select a sufficiently complex target function \( f \), defined as
\begin{equation}
\label{eq:def:f2:C0:MMNN:vs:FCNN}
    f_2(x) = 
\frac{1 + 6x^8}{1 + 8x^6} \cdot 
\left( 120x^2 - 2 \left\lfloor \frac{120x^2 + 1}{2} \right\rfloor \right)^2
\end{equation}
and 
\begin{equation}
\label{eq:def:f3:C0:MMNN:vs:FCNN}
    f_3(x) = 
\frac{1 + 6x^8}{1 + 8x^6} \cdot 
\left( 32x  - 2 \left\lfloor \frac{32 x  + 1}{2} \right\rfloor \right)^2,
\end{equation}
where $\lfloor \cdot \rfloor$ denotes the floor function.
% An illustration of \( f \) is provided in Figure~\ref{fig:f2:and:zoomin}. 
% See Figure~\ref{fig:fig:f1:g:f2:and:zoomin} for an illustration 
See Figure~\ref{fig:fig:f1:g:f2:and:zoomin} for visual depictions of \( f_2 \) and \( f_3 \).
% and its zoom-in.
For \( f_3 \), we use different mini-batch size to demonstrate that MMNNs are less sensitive to the hyper-parameters of training and therefore more stable compared to FCNNs. 


\begin{figure}[ht]%[htbp!]  
            \centering
  % \begin{subfigure}[b]{0.91\textwidth}
  %                   \centering            
  %                   \includegraphics[width=0.999\textwidth]{figures/MMNNvsFCNN_f1.pdf}
  %                   % \subcaption{$f$.}
  %               \end{subfigure}
  
            \begin{subfigure}[b]{0.72\textwidth}
                    \centering            
                    \includegraphics[width=0.999\textwidth]{figures/MMNNvsFCNN_f1.pdf}
                    % \subcaption{$f$.}
                \end{subfigure}
            \begin{subfigure}[b]{0.27\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/MMNNvsFCNN_g.pdf}
                    % \subcaption{$f^\prime$}
                \end{subfigure}
      \begin{subfigure}[b]{0.999\textwidth}
                    \centering            
                    \includegraphics[width=0.999\textwidth]{figures/MMNNvsFCNN_f2func.pdf}
                    % \subcaption{$f$.}
                \end{subfigure}
                      \begin{subfigure}[b]{0.999\textwidth}
                    \centering            
                    \includegraphics[width=0.999\textwidth]{figures/MMNNvsFCNN_f3func.pdf}
                    % \subcaption{$f$.}
                \end{subfigure}
                % \hfill
                % \hspace{10pt}
            % \begin{subfigure}[b]{0.32265\textwidth}
            %         \centering            \includegraphics[width=0.999\textwidth]{figures/MMNNvsFCNN_f2zoomin.pdf}
            %         % \subcaption{$f^\prime$}
            %     \end{subfigure}
        
\caption{Illustrations of $f_1$, $g$, $f_2$, and $f_3$.}
    \label{fig:fig:f1:g:f2:and:zoomin}   
\end{figure}


% We employ various network architectures to approximate these functions and assess their performance.
% For the test corresponding to \( f_1 \), we use a total of 3,000 uniformly sampled points in \( [-1,1] \). The learning rate is defined as \( 10^{-3} \times 0.9^{\lfloor k/3000 \rfloor} \), where \( k = 1,2,\dots,300000 \) represents the epoch number. The mini-batch size is set to 3000, meaning all samples are trained simultaneously.
% while for the test, 3000 samples under unifrom distribution $\calU(-1,1)$.  
We employ various network architectures to approximate these functions and evaluate their performance. Notably, these tests are conducted using double precision rather than the default single precision to ensure precise comparisons.
For the test corresponding to \( f_1 \), we use a total of 3000 uniformly sampled points from \( [-1,1] \) for training. The mini-batch size is set to 3000, meaning all samples are trained simultaneously. The learning rate is defined as \( 10^{-3} \times 0.9^{\lfloor k/3000 \rfloor} \), where \( k = 1,2,\dots,300000 \) denotes the epoch number. 
For the test error evaluation, we use another set of 3,000 samples drawn from the uniform distribution \( \mathcal{U}(-1,1) \).
We emphasize that the mini-batch method is not used for \( f_1 \) because our tests indicate that while mini-batching preserves the approximation of the original function, it leads to poor derivative approximation by only including function values in the cost function. As we can see from Table~\ref{tab:error:comparison:MMNNs:vs:FCNNs}, \sine-activated MMNNs perform best as the target function $f_1$ is smooth. We point out that the errors for derivatives in Table~\ref{tab:error:comparison:MMNNs:vs:FCNNs}  are relative errors, as absolute errors for derivatives can be misleading (the \( L^\infty \)-norm of \( f_1^\dprime \) exceeds 70,000). 
The accurate approximation of derivatives is surprising, given the complexity of the target function and the fact that the cost function is formulated solely based on the function values, meaning that no information about \( f_1^\prime\) or \( f_1^\dprime\) is incorporated into the optimization process.

% \begin{figure}[ht]%[htbp!]  
%             \centering
%             \,\hfill
%             \begin{subfigure}[b]{0.27300245\textwidth}
%                     \centering            
%                     \includegraphics[width=0.999\textwidth]{figures/DerivativeApprox1D1order0true.pdf}
%                     \subcaption{$f$.}
%                 \end{subfigure}
%                 \hfill
%             \begin{subfigure}[b]{0.27300245\textwidth}
%                     \centering            \includegraphics[width=0.999\textwidth]{figures/DerivativeApprox1D1order1true.pdf}
%                     \subcaption{$f^\prime$}
%                 \end{subfigure}
%                 \hfill
%             \begin{subfigure}[b]{0.27300245\textwidth}
%                     \centering            \includegraphics[width=0.999\textwidth]{figures/DerivativeApprox1D1order2true.pdf}
%                     \subcaption{$f^\dprime$}
%                 \end{subfigure}
% \hfill\,\\

%             \,\hfill
%             \begin{subfigure}[b]{0.27300245\textwidth}
%                     \centering            
%                     \includegraphics[width=0.999\textwidth]{figures/DerivativeApprox1D1order0diff.pdf}
%                     \subcaption{$(\phi-f)/\|f\|_{L^\infty([-1,1])}$.}
%                 \end{subfigure}
%                 \hfill
%             \begin{subfigure}[b]{0.27300245\textwidth}
%                     \centering            \includegraphics[width=0.999\textwidth]{figures/DerivativeApprox1D1order1diff.pdf}
%                     \subcaption{$(\phi^\prime-f^\prime)/\|f^\prime\|_{L^\infty([-1,1])}$.}
%                 \end{subfigure}
%                 \hfill
%             \begin{subfigure}[b]{0.27300245\textwidth}
%                     \centering            \includegraphics[width=0.999\textwidth]{figures/DerivativeApprox1D1order2diff.pdf}
%                     \subcaption{$(\phi^\dprime-f^\dprime)/\|f^\dprime\|_{L^\infty([-1,1])}$.}
%                 \end{subfigure}
% \hfill\,

% \caption{Illustrations of the target function $f$ (see \eqref{eq:true:func:sum}), its derivatives, and their normalized differences, where $\phi$ is the learned network.}
%     \label{fig:derivative:approx}   
% \end{figure}

% \subsubsection{Continuous but Not Differentiable Examples}


% \begin{table}[ht]%[htbp!]
% 	\centering  
%  \setlength{\tabcolsep}{0.68em} 
%  \renewcommand{\arraystretch}{1.15}
% \caption{Comparison of test errors. Errors for derivatives are relative errors, as absolute errors for derivatives can be misleading (the \( L^\infty \)-norm of \( f_1^\dprime \) exceeds 70000). The cost function is based only on function values, without incorporating derivatives in the optimization.  
% }
% \label{tab:error:comparison:MMNNs:vs:FCNNs}
% 	\resizebox{0.95\textwidth}{!}{ 
% 		\begin{tabular}{ccccccccc} 
% 			\toprule% \toprule[1.2pt]  
%             % \multicolumn{2}{c}
%             % {
%             % target function
%             % }
%             % & \multicolumn{2}{c}{$f_1:[-1,1]\to \R$}
%             % & \multicolumn{2}{c}{$f_2:[-1,1]^2\to \R$}
%             % & \multicolumn{2}{c}{$f_3:[-1,1]^2\to \R$}
%             % % & \multicolumn{2}{c}{$f_4:[-1,1]^3\to \R$}
%             % \\        
%             \multicolumn{2}{c}{\#parameters (trained / all)} &\multicolumn{2}{c}{35235 / \textbf{72993}} &\multicolumn{2}{c}{\textbf{72981} / 151281} &\multicolumn{2}{c}{\textbf{72961} / \textbf{72961}} \\
%             % \cmidrule(lr){1-2} 
%             \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%             \cmidrule(lr){7-8}
%             % \cmidrule(lr){2-3}
%             % \cmidrule(lr){4-5}
%             % \cmidrule(lr){6-7}
%             % \cmidrule(lr){8-9}
%         \rowcolor{mygray}   
%         % \multicolumn{2}{c}{network} 
%         &
%         &\multicolumn{2}{c}{MMNN of size (434,16,6)} &\multicolumn{2}{c}{MMNN of size (900,16,6)} &\multicolumn{2}{c}{FCNN of size (120,--,6)}
%         % &\multicolumn{2}{c}{ResMMNN of size (1024,40,16)}
%         \\
%            %             \cmidrule(lr){3-4}
%            %  \cmidrule(lr){5-6}
%            %  \cmidrule(lr){7-8}
%            % & &\multicolumn{2}{c}{MMNN1}
%            %  &\multicolumn{2}{c}{MMNN2}
%            %  &\multicolumn{2}{c}{FCNN1}
%            %  \\
%             \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%             \cmidrule(lr){7-8}
%             % \cmidrule(lr){2-3}
%             % \cmidrule(lr){4-5}
%             % \cmidrule(lr){6-7}
%             % \cmidrule(lr){8-9}
% 			 %    target  &  activation &        TE (MSE) 
%     % & TE (MAX) &        TE (MSE) 
%     % & TE (MAX)
%     % &        TE (MSE) 
%     % & TE (MAX)\\
%     % \multicolumn{2}{c}
%       target function &   {activation} &          MSE 
%     &  MAX &    MSE 
%     &  MAX &    MSE 
%     &  MAX
%     % &   
%     % MSE 
%     % &  MAX
%     \\
    
%     % {activation} &        test error (MSE) 
%     % & test error (MAX) &        test error (MSE) 
%     % & test error (MAX)
%     % &        test error (MSE) 
%     % & test error (MAX)\\
% 			 % \cmidrule{3-5}
% 			 % & & MSE & MAE & MAX\\
% 			\midrule



% $f_1$ & $\mathtt{ReLU}$ &  $ 4.47 \times 10^{-5} $  &  $ 6.34 \times 10^{-2} $  &  $ 2.24 \times 10^{-5} $  &  $ 4.06 \times 10^{-2} $  &  $ 2.31 \times 10^{-4} $  &  $ 1.93 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f_1$ & $\mathtt{GELU}$ &  $ 5.54 \times 10^{-5} $  &  $ 5.94 \times 10^{-2} $  &  $ 1.45 \times 10^{-4} $  &  $ 1.00 \times 10^{-1} $  &  $ 1.63 \times 10^{-3} $  &  $ 2.91 \times 10^{-1} $ 
%  \\ 

%  $f_1$ & $\mathtt{tanh}$ &  $ 4.12 \times 10^{-5} $  &  $ 2.90 \times 10^{-2} $  &  $ 4.91 \times 10^{-6} $  &  $ 1.24 \times 10^{-2} $  &  $ 2.67 \times 10^{-3} $  &  $ 3.69 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f_1$ & $\mathtt{sine}$ &  $ 7.30 \times 10^{-7} $  &  $ 3.69 \times 10^{-3} $  &  $ \bm{3.43 \times 10^{-8}} $  &  $ \bm{8.37 \times 10^{-4}} $  &  $ 2.62 \times 10^{-5} $  &  $ 2.35 \times 10^{-2} $ 
%  \\ 

%  $f_1$ & $\mathtt{cosine}$ &  $ 6.89 \times 10^{-6} $  &  $ 9.54 \times 10^{-3} $  &  $ 1.35 \times 10^{-7} $  &  $ 1.85 \times 10^{-3} $  &  $ 2.80 \times 10^{-6} $  &  $ 7.76 \times 10^{-3} $ 
%  \\ 

% \rowcolor{mygray}$f_1$ & $\mathtt{SinTU}_{0}$ &  $ 8.11 \times 10^{-5} $  &  $ 8.65 \times 10^{-2} $  &  $ 4.40 \times 10^{-6} $  &  $ 1.81 \times 10^{-2} $  &  $ 2.15 \times 10^{-4} $  &  $ 1.52 \times 10^{-1} $ 
%  \\ 

%  $f_1$ & $\mathtt{SinTU}_{-\pi}$ &  $ 1.25 \times 10^{-5} $  &  $ 2.76 \times 10^{-2} $  &  $ 3.65 \times 10^{-7} $  &  $ 4.90 \times 10^{-3} $  &  $ 4.14 \times 10^{-4} $  &  $ 1.44 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f_1$ & $\mathtt{SinTU}_{-2\pi}$ &  $ 4.19 \times 10^{-6} $  &  $ 1.30 \times 10^{-2} $  &  $ 3.97 \times 10^{-7} $  &  $ 4.93 \times 10^{-3} $  &  $ 1.67 \times 10^{-5} $  &  $ 2.63 \times 10^{-2} $ 
%  \\ 


% \midrule

%  $f^\prime_1$ & $\mathtt{GELU}$ &  $ 1.24 \times 10^{-3} $  &  $ 2.61 \times 10^{-1} $  &  $ 2.34 \times 10^{-3} $  &  $ 2.97 \times 10^{-1} $  &  $ 7.74 \times 10^{-3} $  &  $ 5.16 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f^\prime_1$ & $\mathtt{tanh}$ &  $ 7.50 \times 10^{-4} $  &  $ 1.11 \times 10^{-1} $  &  $ 1.24 \times 10^{-4} $  &  $ 5.67 \times 10^{-2} $  &  $ 1.10 \times 10^{-2} $  &  $ 6.31 \times 10^{-1} $ 
%  \\ 

%  $f^\prime_1$ & $\mathtt{sine}$ &  $ 2.64 \times 10^{-5} $  &  $ 2.12 \times 10^{-2} $  &  $\bm{ 1.27 \times 10^{-6}} $  &  $\bm{ 5.12 \times 10^{-3}} $  &  $ 5.83 \times 10^{-4} $  &  $ 1.06 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f^\prime_1$ & $\mathtt{cosine}$ &  $ 1.77 \times 10^{-4} $  &  $ 4.76 \times 10^{-2} $  &  $ 5.35 \times 10^{-6} $  &  $ 1.15 \times 10^{-2} $  &  $ 8.91 \times 10^{-5} $  &  $ 3.84 \times 10^{-2} $ 
%  \\ 
% \midrule

%  $f^\dprime_1$ & $\mathtt{GELU}$ &  $ 1.03 \times 10^{-2} $  &  $ 1.22 \times 10^{0} $  &  $ 1.55 \times 10^{-2} $  &  $ 9.39 \times 10^{-1} $  &  $ 3.10 \times 10^{-2} $  &  $ 1.39 \times 10^{0} $ 
%  \\ 

% \rowcolor{mygray}$f^\dprime_1$ & $\mathtt{tanh}$ &  $ 5.40 \times 10^{-3} $  &  $ 2.79 \times 10^{-1} $  &  $ 9.92 \times 10^{-4} $  &  $ 1.59 \times 10^{-1} $  &  $ 3.02 \times 10^{-2} $  &  $ 8.62 \times 10^{-1} $ 
%  \\ 

%  $f^\dprime_1$ & $\mathtt{sine}$ &  $ 2.32 \times 10^{-4} $  &  ${ 7.40 \times 10^{-2} }$  &  $
%  \bm{7.82 \times 10^{-6} }$  &  $\bm{ 1.37 \times 10^{-2}} $  &  $ 4.45 \times 10^{-3} $  &  $ 2.71 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f^\dprime_1$ & $\mathtt{cosine}$ &  $ 1.48 \times 10^{-3} $  &  $ 1.30 \times 10^{-1} $  &  $ 4.12 \times 10^{-5} $  &  $ 5.98 \times 10^{-2} $  &  $ 7.87 \times 10^{-4} $  &  $ 1.15 \times 10^{-1} $ 
%  \\ 

%  \midrule

% $f_2$ & $\mathtt{ReLU}$ &  $ 4.15 \times 10^{-3} $  &  $ 4.87 \times 10^{-1} $  &  $ 1.53 \times 10^{-3} $  &  $ 4.37 \times 10^{-1} $  &  $ 2.13 \times 10^{-2} $  &  $ 6.35 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f_2$ & $\mathtt{GELU}$ &  $ 4.15 \times 10^{-3} $  &  $ 4.72 \times 10^{-1} $  &  $ 1.24 \times 10^{-3} $  &  $ 4.15 \times 10^{-1} $  &  $ 1.33 \times 10^{-2} $  &  $ 5.20 \times 10^{-1} $ 
%  \\ 

%  $f_2$ & $\mathtt{tanh}$ &  $ 4.19 \times 10^{-3} $  &  $ 5.16 \times 10^{-1} $  &  $ 1.42 \times 10^{-4} $  &  $ 1.51 \times 10^{-1} $  &  $ 1.21 \times 10^{-2} $  &  $ 5.56 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f_2$ & $\mathtt{sine}$ &  $ 4.30 \times 10^{-5} $  &  $ 7.36 \times 10^{-2} $  &  $ 4.84 \times 10^{-6} $  &  $ 2.88 \times 10^{-2} $  &  $ 9.07 \times 10^{-5} $  &  $ 9.22 \times 10^{-2} $ 
%  \\ 

%  $f_2$ & $\mathtt{cosine}$ &  $ 3.17 \times 10^{-4} $  &  $ 1.28 \times 10^{-1} $  &  $ 5.65 \times 10^{-6} $  &  $ 3.15 \times 10^{-2} $  &  $ 5.68 \times 10^{-5} $  &  $ 7.86 \times 10^{-2} $ 
%  \\ 

% \rowcolor{mygray}$f_2$ & $\mathtt{SinTU}_{0}$ &  $ 2.10 \times 10^{-3} $  &  $ 4.61 \times 10^{-1} $  &  $ 2.51 \times 10^{-6} $  &  $ 2.61 \times 10^{-2} $  &  $ 2.00 \times 10^{-2} $  &  $ 6.19 \times 10^{-1} $ 
%  \\ 

%  $f_2$ & $\mathtt{SinTU}_{-\pi}$ &  $ 3.61 \times 10^{-5} $  &  $ 7.42 \times 10^{-2} $  &  $ \bm{1.28 \times 10^{-6} }$  &  $\bm{ 2.31 \times 10^{-2} }$  &  $ 6.14 \times 10^{-3} $  &  $ 5.31 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$f_2$ & $\mathtt{SinTU}_{-2\pi}$ &  $ 3.46 \times 10^{-5} $  &  $ 7.03 \times 10^{-2} $  &  $ 5.04 \times 10^{-6} $  &  $ 3.05 \times 10^{-2} $  &  $ 3.05 \times 10^{-3} $  &  $ 3.47 \times 10^{-1} $ 
%  \\

% \midrule
% HP 1 & $f_2$ & $\mathtt{ReLU}$ &  $ 8.87 \times 10^{-7} $  &  $ 1.43 \times 10^{-2} $  &  $ 2.10 \times 10^{-6} $  &  $ 3.36 \times 10^{-2} $  &  $ 9.07 \times 10^{-6} $  &  $ 3.38 \times 10^{-2} $ 
%  \\ 

% \rowcolor{mygray}HP 1 & $f_2$ & $\mathtt{sine}$ &  $ 1.04 \times 10^{-6} $  &  $ 1.26 \times 10^{-2} $  &  $ 4.50 \times 10^{-7} $  &  $ 9.44 \times 10^{-3} $  &  $ 6.53 \times 10^{-2} $  &  $ 6.65 \times 10^{-1} $ 
%  \\ 

%  HP 2 & $f_2$ & $\mathtt{ReLU}$ &  $ 1.01 \times 10^{-6} $  &  $ 1.33 \times 10^{-2} $  &  $ 6.48 \times 10^{-7} $  &  $ 1.29 \times 10^{-2} $  &  $ 4.87 \times 10^{-5} $  &  $ 4.23 \times 10^{-2} $ 
%  \\ 

% \rowcolor{mygray}HP 2 & $f_2$ & $\mathtt{sine}$ &  $ 8.11 \times 10^{-7} $  &  $ 1.20 \times 10^{-2} $  &  $ 4.22 \times 10^{-7} $  &  $ 1.01 \times 10^{-2} $  &  $ 6.66 \times 10^{-2} $  &  $ 6.65 \times 10^{-1} $ 
%  \\ 

%  HP 3 & $f_2$ & $\mathtt{ReLU}$ &  $ 7.79 \times 10^{-7} $  &  $ 1.42 \times 10^{-2} $  &  $ 7.22 \times 10^{-7} $  &  $ 1.47 \times 10^{-2} $  &  $ 4.11 \times 10^{-6} $  &  $ 1.56 \times 10^{-2} $ 
%  \\ 

% \rowcolor{mygray}HP 3 & $f_2$ & $\mathtt{sine}$ &  $ 9.18 \times 10^{-7} $  &  $ 1.31 \times 10^{-2} $  &  $ 5.93 \times 10^{-7} $  &  $ 1.05 \times 10^{-2} $  &  $ 7.16 \times 10^{-5} $  &  $ 6.52 \times 10^{-2} $ 
%  \\ 

%  HP 4 & $f_2$ & $\mathtt{ReLU}$ &  $ 8.00 \times 10^{-7} $  &  $ 1.37 \times 10^{-2} $  &  $ 2.31 \times 10^{-6} $  &  $ 1.66 \times 10^{-2} $  &  $ 1.71 \times 10^{-5} $  &  $ 3.73 \times 10^{-2} $ 
%  \\ 

% \rowcolor{mygray}HP 4 & $f_2$ & $\mathtt{sine}$ &  $ 7.53 \times 10^{-7} $  &  $ 1.20 \times 10^{-2} $  &  $ 4.67 \times 10^{-7} $  &  $ 1.14 \times 10^{-2} $  &  $ 1.23 \times 10^{-4} $  &  $ 8.02 \times 10^{-2} $ 
%  \\ 
 
 
%  \bottomrule% \bottomrule[1.2pt] 
% 		\end{tabular} 
% 	}%%% \resizebox
% % 	\vskip -0.1in
% \end{table} 


\begin{table}[ht]%[htbp!]
	\centering  
 \setlength{\tabcolsep}{0.68em} 
 \renewcommand{\arraystretch}{1.15}
 \caption{Comparison of test errors. Training is conducted in double precision, with the cost function relying only on function values and excluding derivatives. Relative errors are reported for derivatives, as the \( L^\infty \)-norm of \( f_1^\dprime \) exceeds 70000, making absolute errors misleading.
}
% \caption{Comparison of test errors. Errors for derivatives are relative errors, as absolute errors for derivatives can be misleading (the \( L^\infty \)-norm of \( f_1^\dprime \) exceeds 70000). The cost function is based only on function values, without incorporating derivatives in the optimization.   Training is performed in double precision.
% }
% \caption{Comparison of test errors. Relative errors are used for derivatives, as absolute errors can be misleading (the \( L^\infty \)-norm of \( f_1^\dprime \) exceeds 70,000). The cost function is formulated solely based on function values, without including derivatives in the optimization. Training is conducted in double precision.}
\label{tab:error:comparison:MMNNs:vs:FCNNs}
	\resizebox{0.985\textwidth}{!}{ 
		\begin{tabular}{ccccccccc@{\hspace{10pt}}c} 
			\toprule% \toprule[1.2pt]  
            % \multicolumn{2}{c}
            % {
            % target function
            % }
            % & \multicolumn{2}{c}{$f_1:[-1,1]\to \R$}
            % & \multicolumn{2}{c}{$f_2:[-1,1]^2\to \R$}
            % & \multicolumn{2}{c}{$f_3:[-1,1]^2\to \R$}
            % % & \multicolumn{2}{c}{$f_4:[-1,1]^3\to \R$}
            % \\  
             
            \multicolumn{2}{c}
            {\#parameters (trained / all)} &\multicolumn{2}{c}{35235 / \textbf{72993}} &\multicolumn{2}{c}{\textbf{72981} / 151281} &\multicolumn{2}{c}{\textbf{72961} / \textbf{72961}} && \\
            % \cmidrule(lr){1-2} 
            \cmidrule(lr){3-4}
            \cmidrule(lr){5-6}
            \cmidrule(lr){7-8}
            % \cmidrule(lr){9-10}
            % \cmidrule(lr){2-3}
            % \cmidrule(lr){4-5}
            % \cmidrule(lr){6-7}
            % \cmidrule(lr){8-9}
        \rowcolor{mygray}   
        % \multicolumn{2}{c}{network} 
        & 
        &\multicolumn{2}{c}{MMNN of size (434,16,6)} &\multicolumn{2}{c}{MMNN of size (900,16,6)} &\multicolumn{2}{c}{FCNN of size (120,--,6)} 
        &\multicolumn{2}{c}{\#training-samples}
        % &\multicolumn{2}{c}{ResMMNN of size (1024,40,16)}
        \\
           %             \cmidrule(lr){3-4}
           %  \cmidrule(lr){5-6}
           %  \cmidrule(lr){7-8}
           % & &\multicolumn{2}{c}{MMNN1}
           %  &\multicolumn{2}{c}{MMNN2}
           %  &\multicolumn{2}{c}{FCNN1}
           %  \\
            \cmidrule(lr){3-4}
            \cmidrule(lr){5-6}
            \cmidrule(lr){7-8}
            \cmidrule(lr){9-10}
            % \cmidrule(lr){2-3}
            % \cmidrule(lr){4-5}
            % \cmidrule(lr){6-7}
            % \cmidrule(lr){8-9}
			 %    target  &  activation &        TE (MSE) 
    % & TE (MAX) &        TE (MSE) 
    % & TE (MAX)
    % &        TE (MSE) 
    % & TE (MAX)\\
    % \multicolumn{2}{c}
      target function &   {activation}
      &          MSE 
    &  MAX &    MSE 
    &  MAX &    MSE 
    &  MAX &
      mini-batch & all
      % size & \#samples (training)
    % &   
    % MSE 
    % &  MAX
    \\
    
    % {activation} &        test error (MSE) 
    % & test error (MAX) &        test error (MSE) 
    % & test error (MAX)
    % &        test error (MSE) 
    % & test error (MAX)\\
			 % \cmidrule{3-5}
			 % & & MSE & MAE & MAX\\
			\midrule



$f_1$ & $\mathtt{ReLU}$ &  $ 4.47 \times 10^{-5} $  &  $ 6.34 \times 10^{-2} $  &  $ 2.24 \times 10^{-5} $  &  $ 4.06 \times 10^{-2} $  &  $ 2.31 \times 10^{-4} $  &  $ 1.93 \times 10^{-1} $  &  3000 & 3000  
 \\ 

\rowcolor{mygray}$f_1$ & $\mathtt{GELU}$ &  $ 5.54 \times 10^{-5} $  &  $ 5.94 \times 10^{-2} $  &  $ 1.45 \times 10^{-4} $  &  $ 1.00 \times 10^{-1} $  &  $ 1.63 \times 10^{-3} $  &  $ 2.91 \times 10^{-1} $  &  3000 & 3000  
 \\ 

 $f_1$ & $\mathtt{tanh}$ &  $ 4.12 \times 10^{-5} $  &  $ 2.90 \times 10^{-2} $  &  $ 4.91 \times 10^{-6} $  &  $ 1.24 \times 10^{-2} $  &  $ 2.67 \times 10^{-3} $  &  $ 3.69 \times 10^{-1} $  &  3000 & 3000  
 \\ 

\rowcolor{mygray}$f_1$ & $\mathtt{sine}$ &  $ 7.30 \times 10^{-7} $  &  $ 3.69 \times 10^{-3} $  &  $ \bm{3.43 \times 10^{-8}} $  &  $ \bm{8.37 \times 10^{-4}} $  &  $ 2.62 \times 10^{-5} $  &  $ 2.35 \times 10^{-2} $  &  3000 & 3000  
 \\ 

 $f_1$ & $\mathtt{cosine}$ &  $ 6.89 \times 10^{-6} $  &  $ 9.54 \times 10^{-3} $  &  $ 1.35 \times 10^{-7} $  &  $ 1.85 \times 10^{-3} $  &  $ 2.80 \times 10^{-6} $  &  $ 7.76 \times 10^{-3} $  &  3000 & 3000  
 \\ 

\rowcolor{mygray}$f_1$ & $\mathtt{SinTU}_{0}$ &  $ 8.11 \times 10^{-5} $  &  $ 8.65 \times 10^{-2} $  &  $ 4.40 \times 10^{-6} $  &  $ 1.81 \times 10^{-2} $  &  $ 2.15 \times 10^{-4} $  &  $ 1.52 \times 10^{-1} $  &  3000 & 3000  
 \\ 

 $f_1$ & $\mathtt{SinTU}_{-\pi}$ &  $ 1.25 \times 10^{-5} $  &  $ 2.76 \times 10^{-2} $  &  $ 3.65 \times 10^{-7} $  &  $ 4.90 \times 10^{-3} $  &  $ 4.14 \times 10^{-4} $  &  $ 1.44 \times 10^{-1} $  &  3000 & 3000  
 \\ 

\rowcolor{mygray}$f_1$ & $\mathtt{SinTU}_{-2\pi}$ &  $ 4.19 \times 10^{-6} $  &  $ 1.30 \times 10^{-2} $  &  $ 3.97 \times 10^{-7} $  &  $ 4.93 \times 10^{-3} $  &  $ 1.67 \times 10^{-5} $  &  $ 2.63 \times 10^{-2} $  &  3000 & 3000  
 \\ 





\midrule



  $f^\prime_1$ & $\mathtt{GELU}$ &  $ 1.24 \times 10^{-3} $  &  $ 2.61 \times 10^{-1} $  &  $ 2.34 \times 10^{-3} $  &  $ 2.97 \times 10^{-1} $  &  $ 7.74 \times 10^{-3} $  &  $ 5.16 \times 10^{-1} $  &    &   
 \\ 

\rowcolor{mygray}$f^\prime_1$ & $\mathtt{tanh}$ &  $ 7.50 \times 10^{-4} $  &  $ 1.11 \times 10^{-1} $  &  $ 1.24 \times 10^{-4} $  &  $ 5.67 \times 10^{-2} $  &  $ 1.10 \times 10^{-2} $  &  $ 6.31 \times 10^{-1} $  &    &   
 \\ 

 $f^\prime_1$ & $\mathtt{sine}$ &  $ 2.64 \times 10^{-5} $  &  $ 2.12 \times 10^{-2} $  &  $\bm{ 1.27 \times 10^{-6} }$  &  $ \bm{5.12 \times 10^{-3}} $  &  $ 5.83 \times 10^{-4} $  &  $ 1.06 \times 10^{-1} $  &    &   
 \\ 

\rowcolor{mygray}$f^\prime_1$ & $\mathtt{cosine}$ &  $ 1.77 \times 10^{-4} $  &  $ 4.76 \times 10^{-2} $  &  $ 5.35 \times 10^{-6} $  &  $ 1.15 \times 10^{-2} $  &  $ 8.91 \times 10^{-5} $  &  $ 3.84 \times 10^{-2} $  &    &   
 \\ 
\midrule

 $f^\dprime_1$ & $\mathtt{GELU}$ &  $ 1.03 \times 10^{-2} $  &  $ 1.22 \times 10^{0} $  &  $ 1.55 \times 10^{-2} $  &  $ 9.39 \times 10^{-1} $  &  $ 3.10 \times 10^{-2} $  &  $ 1.39 \times 10^{0} $  &    &   
 \\ 

\rowcolor{mygray}$f^\dprime_1$ & $\mathtt{tanh}$ &  $ 5.40 \times 10^{-3} $  &  $ 2.79 \times 10^{-1} $  &  $ 9.92 \times 10^{-4} $  &  $ 1.59 \times 10^{-1} $  &  $ 3.02 \times 10^{-2} $  &  $ 8.62 \times 10^{-1} $  &    &   
 \\ 

 $f^\dprime_1$ & $\mathtt{sine}$ &  $ 2.32 \times 10^{-4} $  &  $ 7.40 \times 10^{-2} $  &  $ \bm{7.82 \times 10^{-6} }$  &  $ \bm{1.37 \times 10^{-2} }$  &  $ 4.45 \times 10^{-3} $  &  $ 2.71 \times 10^{-1} $  &    &   
 \\ 

\rowcolor{mygray}$f^\dprime_1$ & $\mathtt{cosine}$ &  $ 1.48 \times 10^{-3} $  &  $ 1.30 \times 10^{-1} $  &  $ 4.12 \times 10^{-5} $  &  $ 5.98 \times 10^{-2} $  &  $ 7.87 \times 10^{-4} $  &  $ 1.15 \times 10^{-1} $  &    &   
 \\ 

 

 \midrule




$f_2$ & $\mathtt{ReLU}$ &  $ 4.15 \times 10^{-3} $  &  $ 4.87 \times 10^{-1} $  &  $ 1.53 \times 10^{-3} $  &  $ 4.37 \times 10^{-1} $  &  $ 2.13 \times 10^{-2} $  &  $ 6.35 \times 10^{-1} $  &  3000 & 60000  
 \\ 

\rowcolor{mygray}$f_2$ & $\mathtt{GELU}$ &  $ 4.15 \times 10^{-3} $  &  $ 4.72 \times 10^{-1} $  &  $ 1.24 \times 10^{-3} $  &  $ 4.15 \times 10^{-1} $  &  $ 1.33 \times 10^{-2} $  &  $ 5.20 \times 10^{-1} $  &  3000 & 60000  
 \\ 

 $f_2$ & $\mathtt{tanh}$ &  $ 4.19 \times 10^{-3} $  &  $ 5.16 \times 10^{-1} $  &  $ 1.42 \times 10^{-4} $  &  $ 1.51 \times 10^{-1} $  &  $ 1.21 \times 10^{-2} $  &  $ 5.56 \times 10^{-1} $  &  3000 & 60000  
 \\ 

\rowcolor{mygray}$f_2$ & $\mathtt{sine}$ &  $ 4.30 \times 10^{-5} $  &  $ 7.36 \times 10^{-2} $  &  $ 4.84 \times 10^{-6} $  &  $ 2.88 \times 10^{-2} $  &  $ 9.07 \times 10^{-5} $  &  $ 9.22 \times 10^{-2} $  &  3000 & 60000  
 \\ 

 $f_2$ & $\mathtt{cosine}$ &  $ 3.17 \times 10^{-4} $  &  $ 1.28 \times 10^{-1} $  &  $ 5.65 \times 10^{-6} $  &  $ 3.15 \times 10^{-2} $  &  $ 5.68 \times 10^{-5} $  &  $ 7.86 \times 10^{-2} $  &  3000 & 60000  
 \\ 

\rowcolor{mygray}$f_2$ & $\mathtt{SinTU}_{0}$ &  $ 2.10 \times 10^{-3} $  &  $ 4.61 \times 10^{-1} $  &  $ 2.51 \times 10^{-6} $  &  $ 2.61 \times 10^{-2} $  &  $ 2.00 \times 10^{-2} $  &  $ 6.19 \times 10^{-1} $  &  3000 & 60000  
 \\ 

 $f_2$ & $\mathtt{SinTU}_{-\pi}$ &  $ 3.61 \times 10^{-5} $  &  $ 7.42 \times 10^{-2} $  &  $ \bm{1.28 \times 10^{-6} }$  &  $\bm{ 2.31 \times 10^{-2} }$  &  $ 6.14 \times 10^{-3} $  &  $ 5.31 \times 10^{-1} $  &  3000 & 60000  
 \\ 

\rowcolor{mygray}$f_2$ & $\mathtt{SinTU}_{-2\pi}$ &  $ 3.46 \times 10^{-5} $  &  $ 7.03 \times 10^{-2} $  &  $ 5.04 \times 10^{-6} $  &  $ 3.05 \times 10^{-2} $  &  $ 3.05 \times 10^{-3} $  &  $ 3.47 \times 10^{-1} $  &  3000 & 60000  
 \\ 



\midrule

$f_3$ & $\mathtt{sine}$ &  $ 1.52 \times 10^{-7} $  &  $ 7.86 \times 10^{-3} $  &  $ 7.68 \times 10^{-8} $  &  $ 6.06 \times 10^{-3} $  &  $ 3.04 \times 10^{-2} $  &  $ 6.99 \times 10^{-1} $  & 500 & 18000  
 \\ 

\rowcolor{mygray} $f_3$ & $\mathtt{sine}$ &  $ 1.11 \times 10^{-6} $  &  $ 2.23 \times 10^{-2} $  &  $ 1.27 \times 10^{-7} $  &  $ 6.58 \times 10^{-3} $  &  $ 6.69 \times 10^{-2} $  &  $ 6.68 \times 10^{-1} $  & 1000 & 18000  
 \\ 

  $f_3$ & $\mathtt{sine}$ &  $ 3.52 \times 10^{-7} $  &  $ 1.07 \times 10^{-2} $  &  $ 8.75 \times 10^{-8} $  &  $ 6.28 \times 10^{-3} $  &  $ 2.43 \times 10^{-4} $  &  $ 1.52 \times 10^{-1} $  & 1500 & 18000  
 \\ 

\rowcolor{mygray} $f_3$ & $\mathtt{sine}$ &  $ 5.11 \times 10^{-7} $  &  $ 1.10 \times 10^{-2} $  &  $ \bm{6.41 \times 10^{-8}} $  &  $\bm{ 5.33 \times 10^{-3} }$  &  $ 5.86 \times 10^{-6} $  &  $ 3.38 \times 10^{-2} $  & 2000 & 18000  
 \\ 
 \bottomrule% \bottomrule[1.2pt] 
		\end{tabular} 
	}%%% \resizebox
% 	\vskip -0.1in
\end{table} 



\begin{figure}[ht]%[htbp!]  
            \centering
            \begin{subfigure}[b]{0.49\textwidth}
                    \centering            
                    \includegraphics[width=0.999\textwidth]{figures/FCNNvsMMNN1Dact1model1.pdf}
                    \subcaption{\ReLU{} MMNN of size $(434,16,6)$. 
                    % Maximum ($L^\infty$-norm) test error: $6.36\times 10^{-2}$.
                    }
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering   \includegraphics[width=0.999\textwidth]{figures/FCNNvsMMNN1Dact4model1.pdf}
                    \subcaption{\Sine{} MMNN of size $(434,16,6)$. 
                    % Maximum ($L^\infty$-norm) test error: $6.36\times 10^{-2}$.
                    }
                \end{subfigure}
                \\
            \begin{subfigure}[b]{0.490\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/FCNNvsMMNN1Dact1model2.pdf}
                    \subcaption{\ReLU{} MMNN of size $(900,16,6)$.
                    % Maximum ($L^\infty$-norm) test error: $1.64\times 10^{-2}$.
                    }
                \end{subfigure}
                \hfill
                            \begin{subfigure}[b]{0.490\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/FCNNvsMMNN1Dact4model2.pdf}
                    \subcaption{\Sine{} MMNN of size $(900,16,6)$.
                    % Maximum ($L^\infty$-norm) test error: $1.64\times 10^{-2}$.
                    }
                \end{subfigure}
                \\
            \begin{subfigure}[b]{0.490\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/FCNNvsMMNN1Dact1model3.pdf}
                    \subcaption{\ReLU{} FCNN of size $(120,-,6)$. 
                    % Maximum ($L^\infty$-norm) test error: $1.36\times 10^{-1}$.
                    }
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.490\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/FCNNvsMMNN1Dact4model3.pdf}
                    \subcaption{\Sine{} FCNN of size $(120,-,6)$. 
                    % Maximum ($L^\infty$-norm) test error: $1.36\times 10^{-1}$.
                    }
                \end{subfigure}
                \caption{Illustrations of the true function $f_2$ and learned networks.
                % $\sine$-activated networks.
                }
    \label{fig:MMNNvsFCNN:123}   
\end{figure}


% For the test corresponding to \( f_2 \), a total of 60000 uniformly sampled points in \( [-1,1] \) are used with a mini-batch size of 3000 and a learning rate of \( 10^{-3} \times 0.9^{\lfloor k/500 \rfloor} \), where \( k = 1,2,\dots,50000 \) denotes the epoch number. To ensure safe computation of the test error, we select 60,000 test samples from the uniform distribution \( \mathcal{U}(-1,1) \). This test is conducted in double precision.
For the test corresponding to \( f_2 \), we use a total of 60000 uniformly sampled points from \( [-1,1] \) for training. The mini-batch size is set to 3000, and the learning rate is defined as \( 10^{-3} \times 0.9^{\lfloor k/500 \rfloor} \), where \( k = 1,2,\dots,50000 \) denotes the epoch number. 
To ensure accurate computation of the test error, we select another set of 60000 test samples from the uniform distribution \( \mathcal{U}(-1,1) \). 
As illustrated in Figure~\ref{fig:MMNNvsFCNN:123}, the MMNN architecture exhibits superior efficiency relative to the FCNN. Additionally, the \sine{} activation function proves more effective than ReLU for approximating the complex target function \( f_2 \). It is worth noting that our training process involved a sufficiently large number of iterations, effectively eliminating the possibility of inadequate training as a contributing factor. Moreover, expanding the size of the MMNN would further substantially improve its performance.
As shown in Table~\ref{tab:error:comparison:MMNNs:vs:FCNNs}, MMNNs generally outperform FCNNs, regardless of the activation function. This advantage may stem from the simpler optimization landscape of MMNNs, which enables more efficient training.
Furthermore, Table~\ref{tab:error:comparison:MMNNs:vs:FCNNs} highlights that \SinTU{s} achieve the best performance, which is expected since \( f_2 \) contains many singularities that \SinTU{s} are well-suited to handle. Notably, even in this inherently unfavorable setting for \sine-based models, \sine{}-activated MMNNs still perform well. Thus, when the properties of the target function are uncertain in practical applications, trying the \sine{} activation function first is a reasonable strategy.
When the mini-batch size is relatively large, the number of training epochs tends to be high, making the process time-consuming. More importantly, even when \sine-activated FCNNs are given sufficiently large mini-batches and a sufficient number of training epochs, their final performance still falls short of \sine-activated MMNNs.




For the test corresponding to \( f_3 \), we use a total of 18000 uniformly sampled points from \( [-1,1] \) for training. The mini-batch size is set to \( n_{\tn{mbs}} \in \{500, 1000, 1500, 2000\} \), and the learning rate is defined as \( 10^{-3} \times 0.9^{\lfloor 2k/n_{\tn{mbs}} \rfloor} \), where \( k = 1,2,\dots,50n_{\tn{mbs}} \) denotes the epoch number. 
To ensure accurate computation of the test error, we select another set of 18000 test samples from the uniform distribution \( \mathcal{U}(-1,1) \).
As shown in Table~\ref{tab:error:comparison:MMNNs:vs:FCNNs}, FCNNs are relatively sensitive to the hyper-parameters of training. If the mini-batch size is too small, training may fail. However, MMNNs are more stable and succeed under various settings.




% Among activation functions, \texttt{sine} outperforms \ReLU, while \SinTU{s} generally surpass \texttt{sine}. The superior performance of \texttt{sine} is due to its stronger expressive power, whereas \SinTU{s} excel because the target functions frequently exhibit singularities at multiple points.





% As shown in Figure~\ref{fig:MMNNvsFCNN:123}, the FMMNN significantly outperforms the sine-activated FCNN. Additionally, as demonstrated in Table~\ref{tab:error:comparison:MMNNs:vs:FCNNs},
% \begin{itemize}
%     \item MMNNs consistently outperform FCNNs, regardless of the activation function used. This is primarily because MMNNs have a simpler optimization landscape, making them easier to train effectively.
%     \item The \texttt{sine} activation function performs better than ReLU, and \SinTU{s} generally outperforms \texttt{sine}. The superior performance of \texttt{sine} can be attributed to its stronger expressive power, while \SinTU{s} excel because the target functions often exhibit singularities at many points.
% \end{itemize}







% \begin{table}[ht]%[htbp!]
% 	\centering  
%  \setlength{\tabcolsep}{0.68em} 
%  \renewcommand{\arraystretch}{1.15}
% \caption{Comparison of test errors for second derivative. first three row for $f_1$, second three row for $f_2$, final three row for $f_3$; float64}
% \label{tab:error:comparison:derivative}
% 	\resizebox{0.95\textwidth}{!}{ 
% 		\begin{tabular}{ccccccccc} 
% 			\toprule% \toprule[1.2pt]  
%             % \multicolumn{2}{c}
%             % {
%             % target function
%             % }
%             % & \multicolumn{2}{c}{$f_1:[-1,1]\to \R$}
%             % & \multicolumn{2}{c}{$f_2:[-1,1]^2\to \R$}
%             % & \multicolumn{2}{c}{$f_3:[-1,1]^2\to \R$}
%             % % & \multicolumn{2}{c}{$f_4:[-1,1]^3\to \R$}
%             % \\        
%             % {\#parameters (trained / all)}
%            &  &\multicolumn{2}{c}{original function} &\multicolumn{2}{c}{first derivative} &\multicolumn{2}{c}{second derivative} \\
%             % \cmidrule(lr){1-2} 
%             % \cmidrule(lr){3-4}
%             % \cmidrule(lr){5-6}
%             % \cmidrule(lr){7-8}
%         %     \cmidrule(lr){2-3}
%         %     \cmidrule(lr){4-5}
%         %     \cmidrule(lr){6-7}
%         %     % \cmidrule(lr){8-9}
%         % \rowcolor{mygray}   
%         % % \multicolumn{2}{c}{network} 
%         % % &
%         % &\multicolumn{2}{c}{MMNN of size (434,16,6)} &\multicolumn{2}{c}{MMNN of size (900,16,6)} &\multicolumn{2}{c}{FCNN of size (120,--,6)}
%         % % &\multicolumn{2}{c}{ResMMNN of size (1024,40,16)}
%         % \\
%            %             \cmidrule(lr){3-4}
%            %  \cmidrule(lr){5-6}
%            %  \cmidrule(lr){7-8}
%            % & &\multicolumn{2}{c}{MMNN1}
%            %  &\multicolumn{2}{c}{MMNN2}
%            %  &\multicolumn{2}{c}{FCNN1}
%            %  \\
%             \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%             \cmidrule(lr){7-8}
%             % \cmidrule(lr){2-3}
%             % \cmidrule(lr){4-5}
%             % \cmidrule(lr){6-7}
%             % \cmidrule(lr){8-9}
% 			 %    target  &  activation &        TE (MSE) 
%     % & TE (MAX) &        TE (MSE) 
%     % & TE (MAX)
%     % &        TE (MSE) 
%     % & TE (MAX)\\
%     % \multicolumn{2}{c}
%        target function &   {activation} &         MSE 
%     &  MAX &    MSE 
%     &  MAX &    MSE 
%     &  MAX
%     % &   
%     % MSE 
%     % &  MAX
%     \\
    
%     % {activation} &        test error (MSE) 
%     % & test error (MAX) &        test error (MSE) 
%     % & test error (MAX)
%     % &        test error (MSE) 
%     % & test error (MAX)\\
% 			 % \cmidrule{3-5}
% 			 % & & MSE & MAE & MAX\\
% 			\midrule

% $\mathtt{GELU}$ &  $ 5.43 \times 10^{-5} $  &  $ 4.53 \times 10^{-2} $  &  $ 1.72 \times 10^{-3} $  &  $ 2.19 \times 10^{-1} $  &  $ 1.31 \times 10^{-2} $  &  $ 1.19 \times 10^{0} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{tanh}$ &  $ 2.69 \times 10^{-6} $  &  $ 9.05 \times 10^{-3} $  &  $ 9.02 \times 10^{-5} $  &  $ 5.05 \times 10^{-2} $  &  $ 7.54 \times 10^{-4} $  &  $ 1.40 \times 10^{-1} $ 
%  \\ 

%  $\mathtt{sin}$ &  $ 3.04 \times 10^{-8} $  &  $ 8.39 \times 10^{-4} $  &  $ 1.43 \times 10^{-6} $  &  $ 5.72 \times 10^{-3} $  &  $ 8.77 \times 10^{-6} $  &  $ 1.54 \times 10^{-2} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{cos}$ &  $ 8.66 \times 10^{-8} $  &  $ 1.92 \times 10^{-3} $  &  $ 4.19 \times 10^{-6} $  &  $ 1.28 \times 10^{-2} $  &  $ 3.15 \times 10^{-5} $  &  $ 5.07 \times 10^{-2} $ 
%  \\ 
%  \bottomrule% \bottomrule[1.2pt] 
% 		\end{tabular} 
% 	}%%% \resizebox
% % 	\vskip -0.1in
% \end{table} 




% \subsection{Performance of \texttt{sine} Versus Other Activation Functions in MMNNs}
% \label{sec:sine_vs_others}


% To simplify notation, we define the truncation function as
% \begin{equation*}
%     \calT_K(x)=\begin{cases}
%         x & \tn{if}\ |x|\le K,\\
%         -K & \tn{if}\ x< -K,\\
%         K & \tn{if}\ x> K.\\
%     \end{cases}
% \end{equation*}

% We examine a two-dimensional target function defined as follows:

% $$
% g_s(x_1, x_2) = \sum_{i=1}^2 \sum_{j=1}^2 a_{ij} \sin(s b_i x_i + s c_{ij} x_i x_j) \cos(s b_j x_j + s d_{ij} x_i^2),
% $$

% where 

% $$
% (a_{ij}) = \begin{bmatrix*} 0.3 & 0.2 \\ 0.2 & 0.3 \end{bmatrix*}, \quad
% (b_i) = \begin{bmatrix*} 2\pi \\ 4\pi \end{bmatrix*}, \quad
% (c_{ij}) = \begin{bmatrix*} 2\pi & 4\pi \\ 8\pi & 4\pi \end{bmatrix*}, \quad \text{and} \quad
% (d_{ij}) = \begin{bmatrix*} 4\pi & 6\pi \\ 8\pi & 6\pi \end{bmatrix*}.
% $$

% Additionally, we define 
% $ f_1 = g_{s=2} $
% and $ f_2 = 2|g_{s=1}| $.

% Due to the complexity of these functions, we utilize MMNNs with configurations of $(1220, 21, 6)$ and $(684, 18, 6)$, as well as an FCNN with a width of 160 and a depth of 6. To demonstrate the advantages of FMMNN, we evaluate various activation functions, as shown in Table~\ref{tab:error:comparison}.

% For this evaluation, a total of $500^2$ data points are sampled on a uniform grid within $[-1, 1]^2$, using a mini-batch size of 2000 and a learning rate of $10^{-3} \times 0.9^{\lfloor k/40 \rfloor}$, where $k = 1, 2, \ldots, 4000$ represents the epoch number.

% ReLU, \sine{} , cos, tanh, tanh', tanh", \sine{}  $[-\pi/2, \pi/2]$,
% \sine{}  $[-\pi, \pi]$
% \sine{}  $[-2\pi, 2\pi]$,

% $\sin\circ \calT_{\pi/2}$, 
% $\sin\circ \calT_{\pi}$, 
% $\sin\circ \calT_{2\pi}$

% $cos(x+\pi/4)$,
% $\tanh^\prime(x+1)-0.5$ 

% FCNNs and MMNNs






% \begin{table}[ht]%[htbp!]
% 	\centering  
%  \setlength{\tabcolsep}{0.68em} 
%  \renewcommand{\arraystretch}{1.15}
% \caption{Comparison of test errors.}
% 	\label{tab:error:comparison}
% 	\resizebox{0.9985\textwidth}{!}{ 
% 		\begin{tabular}{ccccccccc} 
% 			\toprule% \toprule[1.2pt]  
%             \multicolumn{2}{c}{(width, rank, depth)}
%             & \multicolumn{2}{c}{(1220, 21, 6)}
%             & \multicolumn{2}{c}{(684, 18, 6)}
%             & \multicolumn{2}{c}{(160, $-$, 6)}\\        
%             % \cmidrule(lr){1-2} 
%             \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%             \cmidrule(lr){7-8}
%         \rowcolor{mygray}   \multicolumn{2}{c}{\#parameters (trained / all)} &\multicolumn{2}{c}{\textbf{129426} / 267286} &\multicolumn{2}{c}{62335 / \textbf{129367}} &\multicolumn{2}{c}{\textbf{129441} / \textbf{129441}} \\
%                        \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%             \cmidrule(lr){7-8}
%            & &\multicolumn{2}{c}{MMNN1}
%             &\multicolumn{2}{c}{MMNN2}
%             &\multicolumn{2}{c}{FCNN1}
%             \\
%             \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%             \cmidrule(lr){7-8}
% 			 %    target  &  activation &        TE (MSE) 
%     % & TE (MAX) &        TE (MSE) 
%     % & TE (MAX)
%     % &        TE (MSE) 
%     % & TE (MAX)\\
%     target  &  activation &        test error (MSE) 
%     & test error (MAX) &        test error (MSE) 
%     & test error (MAX)
%     &        test error (MSE) 
%     & test error (MAX)\\
% 			 % \cmidrule{3-5}
% 			 % & & MSE & MAE & MAX\\
% 			\midrule
 
% 			\bottomrule% \bottomrule[1.2pt] 
% 		\end{tabular} 
% 	}%%% \resizebox
% % 	\vskip -0.1in
% \end{table} 


% \begin{table}[ht]%[htbp!]
% 	\centering  
%  \setlength{\tabcolsep}{0.68em} 
%  \renewcommand{\arraystretch}{1.15}
% \caption{Comparison of test errors. $f_3=g_{s=3}$}
% 	\label{tab:error:comparison}
% 	\resizebox{0.9985\textwidth}{!}{ 
% 		\begin{tabular}{ccccccccc} 
% 			\toprule% \toprule[1.2pt]  
%             \multicolumn{2}{c}{(width, rank, depth)}
%             & \multicolumn{2}{c}{(1056, 38, 10)}
%             & \multicolumn{2}{c}{(615, 32, 10)}
%             & \multicolumn{2}{c}{(200, $-$, 10)}\\        
%             % \cmidrule(lr){1-2} 
%             \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%             \cmidrule(lr){7-8}
%         \rowcolor{mygray}   \multicolumn{2}{c}{\#parameters (trained / all)} &\multicolumn{2}{c}{\textbf{362551} / 736375} &\multicolumn{2}{c}{178024 of \textbf{362524}} &\multicolumn{2}{c}{\textbf{362601} / \textbf{362601}} \\
%                        \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%             \cmidrule(lr){7-8}
%            & &\multicolumn{2}{c}{ResMMNN1}
%             &\multicolumn{2}{c}{ResMMNN2}
%             &\multicolumn{2}{c}{ResFCNN1}
%             \\
%             \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%             \cmidrule(lr){7-8}
% 			 %    target  &  activation &        TE (MSE) 
%     % & TE (MAX) &        TE (MSE) 
%     % & TE (MAX)
%     % &        TE (MSE) 
%     % & TE (MAX)\\
%     target  &  activation &        test error (MSE) 
%     & test error (MAX) &        test error (MSE) 
%     & test error (MAX)
%     &        test error (MSE) 
%     & test error (MAX)\\
% 			 % \cmidrule{3-5}
% 			 % & & MSE & MAE & MAX\\
% 			\midrule
 
% 			\bottomrule% \bottomrule[1.2pt] 
% 		\end{tabular} 
% 	}%%% \resizebox
% % 	\vskip -0.1in
% \end{table} 

% \newpage

% $K>0$, $s\in\R$
% \begin{equation*}
%     \calT_K(x)=\begin{cases}
%         x & \tn{if}\ |x|\le K,\\
%         -K & \tn{if}\ x< -K,\\
%         K & \tn{if}\ x> K.\\
%     \end{cases}
%    \quad \tn{and}\quad
%     \caltildeT_K(x)=\begin{cases}
%         x & \tn{if}\   x\ge s,\\
%         s & \tn{if}\ x<  s,\\
%     \end{cases}
% \end{equation*}


% \clearpage

\subsection{MMNNs: \texttt{Sine} Versus Other Activation Functions}
\label{sec:sine_vs_others}

In this section, we compare the performance of MMNNs using different activation functions to demonstrate that FMMNNs consistently produce the best results.  The three target functions used in the tests are $f_1:[-1,1]\to\R$, $f_2:[-1,1]^2\to\R$, and $f_3:[-1,1]^3\to\R$, which are given by
\[
f_1(x) =
% \frac{1+8x^4}
% {1 + 10x^2}
0.6\sin(200\pi x)+0.8\cos(160\pi x^2)
+ \frac{1 + 8x^8}{1 + 10x^4} \cdot \left| 180x - 2 \left\lfloor \frac{180x + 1}{2} \right\rfloor \right|,
\]
\[f_2(x_1, x_2) = \sum_{i=1}^2 \sum_{j=1}^2 a_{ij} \sin(b_i x_i +  c_{ij} x_i x_j)\cdot  \big|\cos( b_j x_j +  d_{ij} x_i^2)\big|,\]
% \[f_3(x_1, x_2) =  \sum_{i=1}^2 \sum_{j=1}^2 a_{ij} \sin(4 b_i x_i + 4 c_{ij} x_i x_j) \cos(4 b_j x_j + 4 d_{ij} x_i^2),\]
% $$
% g_s(x_1, x_2) = \sum_{i=1}^2 \sum_{j=1}^2 a_{ij} \sin(s b_i x_i + s c_{ij} x_i x_j) \cos(s b_j x_j + s d_{ij} x_i^2),
% $$
and
$$
f_3(x_1, x_2,x_3) = \sum_{i=1}^3 \sum_{j=1}^3 \tildea_{ij} \sin(\tildeb_i x_i + \tildec_{ij} x_i x_j) \cdot\big|\cos(\tildeb_j x_j + \tilded_{ij} x_i^2)\big|,
$$
where  
$$
(a_{ij}) = \begin{bmatrix*} 0.3 & 0.2 \\ 0.2 & 0.3 \end{bmatrix*}, 
% \quad
\
(b_i) = \begin{bmatrix*} 12\pi \\  8\pi \end{bmatrix*}, 
% \quad
\
(c_{ij}) = \begin{bmatrix*} 4\pi & 18\pi \\ 16\pi & 10\pi \end{bmatrix*}, 
% \quad  \quad
\
(d_{ij}) = \begin{bmatrix*} 14\pi & 12\pi \\ 18\pi & 10\pi \end{bmatrix*},
$$
$$
(\tildea_{ij}) = \begin{bmatrix*} 
0.3 & 0.1 & 0.4 \\
0.2 & 0.3 & 0.1 \\
0.2 & 0.1 & 0.3
\end{bmatrix*}, 
% \quad
\
(\tildeb_i) = \begin{bmatrix*} 
\pi \\ 4\pi \\ 3\pi
\end{bmatrix*}, 
% \quad
\
(\tildec_{ij}) = \begin{bmatrix*} 
2\pi & \pi & 3\pi \\
2\pi & 3\pi & 2\pi \\
3\pi & \pi & \pi
\end{bmatrix*}, 
% \quad \text{and} \quad
\ \tn{and} \
(\tilded_{ij}) = \begin{bmatrix*} 
2\pi & 3\pi &  \pi \\
 \pi & 3\pi & 2\pi \\
 \pi & 2\pi & 3\pi
\end{bmatrix*}.
$$
Note that all three functions are only continuous but not differentiable. Illustrations of these three functions are shown in Figure~\ref{fig:f:123}. 


\begin{figure}[ht]%[htbp!]  
            \centering
            \,\hfill
            \begin{subfigure}[b]{0.3227300245\textwidth}
                    \centering            
                    \includegraphics[width=0.8999\textwidth]{figures/f1.pdf}
                    \subcaption{$f_1$ limited on $[-0.2,0.2]$.}
                \end{subfigure}
                \hfill
            \begin{subfigure}[b]{0.3227300245\textwidth}
                    \centering            \includegraphics[width=0.8999\textwidth]{figures/f2.pdf}
                    \subcaption{$f_2$.}
                \end{subfigure}
                \hfill
            \begin{subfigure}[b]{0.3227300245\textwidth}
                    \centering            \includegraphics[width=0.8999\textwidth]{figures/f3.pdf}
                    \subcaption{$f_3(x,y,z=0)$.}
                \end{subfigure}
\hfill\, \\[5pt]
       \,\hfill
            \begin{subfigure}[b]{0.3227300245\textwidth}
                    \centering            
                    \includegraphics[width=0.8999\textwidth]{figures/f1plot1d.pdf}
                    \subcaption{$f_1$.}
                \end{subfigure}
                \hfill
            \begin{subfigure}[b]{0.3227300245\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/f2plot3d.pdf}
                    \subcaption{$z=f_2(x,y)$.}
                \end{subfigure}
                \hfill
            \begin{subfigure}[b]{0.3227300245\textwidth}
                    \centering            \includegraphics[width=0.999\textwidth]{figures/f3plot3d.pdf}
                    \subcaption{$w=f_3(x,y,z=0)$.}
                \end{subfigure}
\hfill\,
\caption{Illustrations of $f_i$ for $i=1,2,3$.}

%     \caption{Landscape comparison for FCNNs and MMNNs.  all paramters are init from pytroch linear default. $z$ is loss function, $x,y$ are two paramters (first and last ones) from  weights of (a) $\bmcalL_1$ in \eqref{eq:FCNN:eg}.
% (b)  $\bmtildecalL_1$ in \eqref{eq:MMNN:eg}.
% (c)  $\bmtildecalL_2$ in \eqref{eq:MMNN:eg}.
% }
    \label{fig:f:123}
    
\end{figure}

% We employ different network architectures to approximate the target functions and evaluate their performance. For the  {1D case}, we use 60,000 uniformly sampled points from \([-1, 1]\) with a mini-batch size of 600, and a learning rate of \(10^{-3} \times 0.9^{\lfloor k/100 \rfloor}\), where \(k = 1, 2, \ldots, 10000\) represents the epoch number. To ensure accurate computation of the test error, we choose $60000$ sample under uniform distribution in $[-1,1]$.
% In the  {2D case}, \(600^2\) uniformly sampled points from \([-1,1]^2\) are used with a mini-batch size of 1,200, and the learning rate is set to \(10^{-3} \times 0.9^{\lfloor k/30 \rfloor}\), where \(k = 1, 2, \ldots, 3000\). To ensure accurate computation of the test error, we choose $300^2$ sample under uniform distribution in $[-1,1]^2$. For the  {3D case}, \(150^3\) points from \([-1,1]^3\) are sampled with a mini-batch size of 1,500, and the learning rate is defined as \(10^{-3} \times 0.9^{\lfloor k/4 \rfloor}\), where \(k = 1, 2, \ldots, 400\). To ensure accurate computation of the test error, we choose $100^3$ sample under uniform distribution in $[-1,1]^3$.

We employ MMNN structures with different activation functions to approximate the target functions and evaluate their performance.
For the one-dimensional case, we use 60000 uniformly sampled points from \([-1, 1]\) for training, with a mini-batch size of 600 and a learning rate defined as \( 10^{-3} \times 0.9^{\lfloor k/100 \rfloor} \), where \( k = 1, 2, \ldots, 10000 \) represents the epoch number. To ensure accurate computation of the test error, we select 60000 test samples from the uniform distribution in \( [-1,1] \).
In the two-dimensional case, \( 600^2 \) uniformly sampled points from \( [-1,1]^2 \) are used for training, with a mini-batch size of 1200 and a learning rate set to \( 10^{-3} \times 0.9^{\lfloor k/30 \rfloor} \), where \( k = 1, 2, \ldots, 3000 \). For test error evaluation, we select \( 300^2 \) samples from the uniform distribution in \( [-1,1]^2 \).
For the three-dimensional case, \( 150^3 \) points from \( [-1,1]^3 \) are uniformly sampled for training, with a mini-batch size of 1500 and a learning rate defined as \( 10^{-3} \times 0.9^{\lfloor k/4 \rfloor} \), where \( k = 1, 2, \ldots, 400 \). To ensure accurate computation of the test error, we select \( 100^3 \) samples from the uniform distribution in \( [-1,1]^3 \).



% $f_2(r, \theta)$ defined in polar coordinates $(r, \theta)$ as
% \begin{equation*}
%     f_2(r,\theta)=\begin{cases}
%         0 & \tn{if  } 0.5+10\rho-10r\le 0,\\
%         1 & \tn{if  } 0.5+10\rho-10r\ge 1,\\
%         0.5+10\rho-10r & \tn{otherwise},\\
%     \end{cases}
%     \quad \tn{where}\quad \rho=0.5+0.2\cos(\pi^2 \theta^2).
% \end{equation*}

% $$
% f_3(x_1, x_2) = \sum_{i=1}^2 \sum_{j=1}^2 a_{ij} \sin(s b_i x_i + s c_{ij} x_i x_j) \cos(s b_j x_j + s d_{ij} x_i^2),
% $$

% where  \red{$s=3$}

% $$
% (a_{ij}) = \begin{bmatrix*} 0.3 & 0.2 \\ 0.2 & 0.3 \end{bmatrix*}, \quad
% (b_i) = \begin{bmatrix*} 2\pi \\ 4\pi \end{bmatrix*}, \quad
% (c_{ij}) = \begin{bmatrix*} 2\pi & 4\pi \\ 8\pi & 4\pi \end{bmatrix*}, \quad \text{and} \quad
% (d_{ij}) = \begin{bmatrix*} 4\pi & 6\pi \\ 8\pi & 6\pi \end{bmatrix*}.
% $$





\begin{table}[ht]%[htbp!]
	\centering  
 \setlength{\tabcolsep}{0.68em} 
 \renewcommand{\arraystretch}{1.15}
\caption{Comparison of test errors. Training is performed in single precision.}
	\label{tab:error:comparison:MMNNs}
	\resizebox{0.92\textwidth}{!}{ 
		\begin{tabular}{ccccccccc} 
			\toprule% \toprule[1.2pt]  
            % \multicolumn{2}{c}
            {
            target function
            }
            & \multicolumn{2}{c}{$f_1:[-1,1]\to \R$}
            & \multicolumn{2}{c}{$f_2:[-1,1]^2\to \R$}
            % & \multicolumn{2}{c}{$f_3:[-1,1]^2\to \R$}
            & \multicolumn{2}{c}{$f_3:[-1,1]^3\to \R$}\\        
            % \cmidrule(lr){1-2} 
            % \cmidrule(lr){3-4}
            % \cmidrule(lr){5-6}
            % \cmidrule(lr){7-8}
            \cmidrule(lr){2-3}
            \cmidrule(lr){4-5}
            \cmidrule(lr){6-7}
            % \cmidrule(lr){8-9}
        \rowcolor{mygray}   
        % \multicolumn{2}{c}{network} 
        % &
        &\multicolumn{2}{c}{MMNN of size (1024,16,6)} &\multicolumn{2}{c}{MMNN of size (1024,36,8)} &\multicolumn{2}{c}{ResMMNN of size (1024,36,10)}
        
        % &\multicolumn{2}{c}{ResMMNN of size (1024,32,10)}
        \\
           %             \cmidrule(lr){3-4}
           %  \cmidrule(lr){5-6}
           %  \cmidrule(lr){7-8}
           % & &\multicolumn{2}{c}{MMNN1}
           %  &\multicolumn{2}{c}{MMNN2}
           %  &\multicolumn{2}{c}{FCNN1}
           %  \\
            % \cmidrule(lr){3-4}
            % \cmidrule(lr){5-6}
            % \cmidrule(lr){7-8}
            \cmidrule(lr){2-3}
            \cmidrule(lr){4-5}
            \cmidrule(lr){6-7}
            % \cmidrule(lr){8-9}
			 %    target  &  activation &        TE (MSE) 
    % & TE (MAX) &        TE (MSE) 
    % & TE (MAX)
    % &        TE (MSE) 
    % & TE (MAX)\\
    % \multicolumn{2}{c}
        {activation} &         MSE 
    &  MAX &    MSE 
    &  MAX &    MSE 
    &  MAX 
    % &    MSE 
    % &  MAX
    \\
    
    % {activation} &        test error (MSE) 
    % & test error (MAX) &        test error (MSE) 
    % & test error (MAX)
    % &        test error (MSE) 
    % & test error (MAX)\\
			 % \cmidrule{3-5}
			 % & & MSE & MAE & MAX\\
			\midrule

$\mathtt{ReLU}$ &  $ 3.52 \times 10^{-2} $  &  $ 1.57 \times 10^{0} $  &  $ 6.50 \times 10^{-5} $  &  $ 6.99 \times 10^{-2} $  &  $ 8.18 \times 10^{-5} $  &  $ 7.86 \times 10^{-2} $ 
 \\ 

\rowcolor{mygray}$\mathtt{ELU}$ &  $ 1.62 \times 10^{-1} $  &  $ 1.78 \times 10^{0} $  &  $ 2.43 \times 10^{-3} $  &  $ 2.68 \times 10^{-1} $  &  $ 6.70 \times 10^{-5} $  &  $ 7.03 \times 10^{-2} $ 
 \\ 

 $\mathtt{GELU}$ &  $ 1.51 \times 10^{-1} $  &  $ 1.61 \times 10^{0} $  &  $ 6.19 \times 10^{-5} $  &  $ 6.67 \times 10^{-2} $  &  $ 6.66 \times 10^{-5} $  &  $ 6.14 \times 10^{-2} $ 
 \\ 

\rowcolor{mygray}$\mathtt{sigmoid}$ &  $ 5.68 \times 10^{-1} $  &  $ 1.95 \times 10^{0} $  &  $ 4.97 \times 10^{-2} $  &  $ 8.00 \times 10^{-1} $  &  $ 1.04 \times 10^{-3} $  &  $ 1.91 \times 10^{-1} $ 
 \\ 

 $\mathtt{tanh}$ &  $ 1.84 \times 10^{-1} $  &  $ 1.77 \times 10^{0} $  &  $ 9.76 \times 10^{-4} $  &  $ 2.33 \times 10^{-1} $  &  $ 1.06 \times 10^{-4} $  &  $ 1.02 \times 10^{-1} $ 
 \\ 

\rowcolor{mygray}$\mathtt{sine}$ &  $ 1.16 \times 10^{-5} $  &  $ 3.18 \times 10^{-2} $  &  $ 2.26 \times 10^{-5} $  &  $ 5.18 \times 10^{-2} $  &  $ \bm{2.67 \times 10^{-5}} $  &  $ 5.22 \times 10^{-2} $ 
 \\ 

 $\mathtt{cosine}$ &  $ 1.55 \times 10^{-5} $  &  $ 3.50 \times 10^{-2} $  &  $ 2.46 \times 10^{-5} $  &  $ 4.26 \times 10^{-2} $  &  $ 3.06 \times 10^{-5} $  &  $ 5.74 \times 10^{-2} $ 
 \\ 

% \rowcolor{mygray}$\mathtt{cosine(\cdot -\pi/4)}$ &  $ 1.20 \times 10^{-5} $  &  $ 3.16 \times 10^{-2} $  &  $ 1.78 \times 10^{-5} $  &  $ 4.06 \times 10^{-2} $  &  $ 2.71 \times 10^{-5} $  &  $ 5.70 \times 10^{-2} $ 
%  \\ 

 $\mathtt{SinTU}_{0}$ &  $ 2.14 \times 10^{-6} $  &  $ 3.04 \times 10^{-2} $  &  $ 3.18 \times 10^{-5} $  &  $ 5.18 \times 10^{-2} $  &  $ 4.21 \times 10^{-5} $  &  $ 6.17 \times 10^{-2} $ 
 \\ 

\rowcolor{mygray}$\mathtt{SinTU}_{-\pi}$ &  $ \bm{1.72 \times 10^{-6} } $  &  $ \bm{2.80 \times 10^{-2}} $  &  $ 2.27 \times 10^{-5} $  &  $ 5.38 \times 10^{-2} $  &  $ 3.27 \times 10^{-5} $  &  $ 5.56 \times 10^{-2} $ 
 \\ 

 $\mathtt{SinTU}_{-2\pi}$ &  $ 5.87 \times 10^{-6} $  &  $ 3.17 \times 10^{-2} $  &  $ 1.62 \times 10^{-5} $  &  $ \bm{3.90 \times 10^{-2}} $  &  $ 2.85 \times 10^{-5} $  &  $ 5.27 \times 10^{-2} $ 
 \\ 

\rowcolor{mygray}$\mathtt{SinTU}_{-4\pi}$ &  $ 1.21 \times 10^{-5} $  &  $ 3.17 \times 10^{-2} $  &  $ \bm{ 1.52 \times 10^{-5}} $  &  $ 4.73 \times 10^{-2} $  &  $ 2.68 \times 10^{-5} $  &  $ 5.43 \times 10^{-2} $ 
 \\ 

 $\mathtt{SinTU}_{-8\pi}$ &  $ 9.47 \times 10^{-6} $  &  $ 3.03 \times 10^{-2} $  &  $ 1.88 \times 10^{-5} $  &  $ 4.53 \times 10^{-2} $  &  $ 2.74 \times 10^{-5} $  &  $ \bm{5.16 \times 10^{-2}} $ 
 \\ 

% \rowcolor{mygray}$\mathtt{SinTU}_{-16\pi}$ &  $ 1.20 \times 10^{-5} $  &  $ 3.26 \times 10^{-2} $  &  $ 2.09 \times 10^{-5} $  &  $ 4.89 \times 10^{-2} $  &  $ 2.70 \times 10^{-5} $  &  $ 5.23 \times 10^{-2} $ 
%  \\ 

%  $\mathtt{SinTU}_{-32\pi}$ &  $ 1.19 \times 10^{-5} $  &  $ 3.12 \times 10^{-2} $  &  $ 1.68 \times 10^{-5} $  &  $ 4.50 \times 10^{-2} $  &  $ 2.70 \times 10^{-5} $  &  $ 5.27 \times 10^{-2} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{SinTU}_{-64\pi}$ &  $ 3.89 \times 10^{-6} $  &  $ 3.11 \times 10^{-2} $  &  $ 2.00 \times 10^{-5} $  &  $ 4.79 \times 10^{-2} $  &  $ 2.79 \times 10^{-5} $  &  $ 5.54 \times 10^{-2} $ 
%  \\ 
 \bottomrule% \bottomrule[1.2pt] 
		\end{tabular} 
	}%%% \resizebox
% 	\vskip -0.1in
\end{table} 

% Summary:
% \begin{itemize}
% \item The Sin-activated MMNN performs sufficiently well, and replacing a small portion of sin activations with ReLU often results in slight improvements.
% \item The periodicity of sin does have an effect, as long as the target function exhibits a certain level of complexity.
% \end{itemize}

As shown in Table~\ref{tab:error:comparison:MMNNs}, \texttt{sine} and \SinTU{s} are the most effective activation functions for MMNNs. Our results further confirm that the combination of sinusoidal activations and MMNN structures is particularly well-suited for function approximation.  
%For relatively smooth or simple target functions, \texttt{sine} achieves strong performance. However, when the target function contains multiple singularities and exhibits higher complexity, \SinTU{s} provide a more suitable and robust choice.







% % f2 f3

% \subsection{More Activation Functions}

% % \subsection{FMMNNs Compared to \sine{} -Activated FCNNs}
% % \label{sec:FMMNN_vs_FCNN}

% \subsection{Hybrid Activation Functions}
% \label{sec:hybrid_activation}

% \[
% f(x)=\cos(30\pi x) - 0.8\cos(22\pi x) + 0.5\cos(15\pi x)
% \]




% \red{Our preliminary experiments suggest that a small proportion of \texttt{ReLU} activations combined with a majority of \sine{}  activation functions is highly effective. For instance, within the same layer, using 90\% \sine{}  and 10\% \texttt{ReLU} activations, or across blocks employing 10\% \texttt{ReLU} and 90\% identity (pass-through), yields promising results.}


% % mixing ReLU and \sine{}  activations within the same layer is not an ideal choice; alternating ReLU and \sine{}  in different layers yields the best results.

% % % ReLu layer + Sin layer + ReLU layer + Sin layer +...

% % \subsection{components}

% \[
% f(x)=\cos(30\pi x) - 0.8\cos(22\pi x) + 0.5\cos(15\pi x)
% \]






% \[
% f(x)=\cos(30\pi x) - 0.8\cos(22\pi x) + 0.5\cos(15\pi x)
% \]

% learned NN $\phi$

% Fourier transform of $\phi$

% compare cofficients

% Plot $\phi-f$

% try different activation function:
% \ReLU; 
% second derivative of \Arctan


% % \subsection{\red{float64}}

% % We use a ReLU-activated, $\tanh^\dprime$-activated, or sin-activated  MMNN of size $(666,36,6)$ to approximate

% deep sin networks; (depth 12)
% batch size 100; local minimum;

% We consider a more complicated target function, \[
% f(x) =
% \frac{1}
% {1 + x^2}\cdot \left| kx - 2 \left\lfloor \frac{kx + 1}{2} \right\rfloor \right|.
% \]
% We sin-activated  ResMMNN of size $(666,36,15)$ to approximate $f$.

%  For this test, there are a total of 10000 uniformly sampled points in $[-1,1]$ with a mini-batch size of 1000 and a learning rate of $0.001\times 0.9^{\lfloor k/500 \rfloor}$, where $\lfloor \cdot \rfloor$ denotes floor operation and $k=1,2,\cdots,40000$ is the epoch number. 
%  Use \red{float64}


% \begin{figure}[H]%[htbp!] 
% 	\centering
%  % \vskip 3.1pt
% 	\includegraphics[width=0.8\linewidth]{figures/errors1D.pdf}
% \caption{Error vs. epoch.
% \red{MSE: 8.37e-09; MAX: 1.77e-03. }
% }
% 	\label{fig:error1D}
% \end{figure}

% % \clearpage




% \section{Proof of Theorem~\ref{thm:main}}


% before giving the detailed proof of Theorem~\ref{thm:main},Let us first outline the main ideas behind it. 
% During the proof, our main approach involves constructing a piecewise constant function to approximate the desired continuous function.
% However, the continuity of ReLU and \sine{}  poses a challenge in uniformly approximating piecewise constant functions.
% To bridge this gap, we  design  networks
% to realize piecewise constant functions outside a sufficiently small region to approximate the target function well. The error inside this small region is controlled since its measure can be arbitrary small. 


% Based on the aforementioned ideas, let us delve into the specific details.
% We divide $[0,1]^d$  into a union of ``important'' cubes $\{Q_k\}_{k\in \{1,2,\cdots,M^d\}}$ and a small region $\Omega$, where $M=N^L$. Each $Q_k$ is associated with a representative $\bm{x}_k\in Q_k$. 
% See Figure~\ref{fig:idea:main} for an illustration of $\bmx_k$, $\Omega$, and $Q_k$.
% Then, the construction of the desired network approximating the target function can be divided into three steps as follows.
% \begin{enumerate}
% 	\item First, we design a sub-network to realize a vector-valued function $\phi_1$ mapping the whole cube $Q_k$ to its index $k$ for each $k$. That is, $\phi_1(\bmx)=k$ for any $\bmx\in  Q_k$ and $k\in \{1,2,\cdots,M^d\}$.
 
% 	\item Next, we design a sub-network to realize a function $\phi_2$ mapping $k$ approximately to $f(\bmx_k)$ for each $k$. Then $\phi_2\circ\phi_1\approx f$ outside $\Omega$, specificly, $\phi_2\circ\phi_1(\bmx)=\phi_2(k)\approx f(\bmx_k)\approx f(\bmx)$ for any $\bmx\in Q_k$ and each $k\in \{1,2,\cdots,M^d\}$.
% \end{enumerate}


% \subsection{More Experiments}



% \begin{itemize}
%     \item {function approximation: error of derivatives}
%     \item  CNN, fc part replaced by MMNN
% \end{itemize}

% \clearpage
% \subsection{Derivative Approximation}
% \label{sec:derivative:approx}

% \begin{table}[ht]%[htbp!]
% 	\centering  
%  \setlength{\tabcolsep}{0.68em} 
%  \renewcommand{\arraystretch}{1.15}
% \caption{Comparison of test errors for second derivative. first three row for $f_1$, second three row for $f_2$, final three row for $f_3$; float64}
% \label{tab:error:comparison:derivative}
% 	\resizebox{0.95\textwidth}{!}{ 
% 		\begin{tabular}{ccccccccc} 
% 			\toprule% \toprule[1.2pt]  
%             % \multicolumn{2}{c}
%             % {
%             % target function
%             % }
%             % & \multicolumn{2}{c}{$f_1:[-1,1]\to \R$}
%             % & \multicolumn{2}{c}{$f_2:[-1,1]^2\to \R$}
%             % & \multicolumn{2}{c}{$f_3:[-1,1]^2\to \R$}
%             % % & \multicolumn{2}{c}{$f_4:[-1,1]^3\to \R$}
%             % \\        
%             % {\#parameters (trained / all)}
%            &  &\multicolumn{2}{c}{original function} &\multicolumn{2}{c}{first derivative} &\multicolumn{2}{c}{second derivative} \\
%             % \cmidrule(lr){1-2} 
%             % \cmidrule(lr){3-4}
%             % \cmidrule(lr){5-6}
%             % \cmidrule(lr){7-8}
%         %     \cmidrule(lr){2-3}
%         %     \cmidrule(lr){4-5}
%         %     \cmidrule(lr){6-7}
%         %     % \cmidrule(lr){8-9}
%         % \rowcolor{mygray}   
%         % % \multicolumn{2}{c}{network} 
%         % % &
%         % &\multicolumn{2}{c}{MMNN of size (434,16,6)} &\multicolumn{2}{c}{MMNN of size (900,16,6)} &\multicolumn{2}{c}{FCNN of size (120,--,6)}
%         % % &\multicolumn{2}{c}{ResMMNN of size (1024,40,16)}
%         % \\
%            %             \cmidrule(lr){3-4}
%            %  \cmidrule(lr){5-6}
%            %  \cmidrule(lr){7-8}
%            % & &\multicolumn{2}{c}{MMNN1}
%            %  &\multicolumn{2}{c}{MMNN2}
%            %  &\multicolumn{2}{c}{FCNN1}
%            %  \\
%             \cmidrule(lr){3-4}
%             \cmidrule(lr){5-6}
%             \cmidrule(lr){7-8}
%             % \cmidrule(lr){2-3}
%             % \cmidrule(lr){4-5}
%             % \cmidrule(lr){6-7}
%             % \cmidrule(lr){8-9}
% 			 %    target  &  activation &        TE (MSE) 
%     % & TE (MAX) &        TE (MSE) 
%     % & TE (MAX)
%     % &        TE (MSE) 
%     % & TE (MAX)\\
%     % \multicolumn{2}{c}
%         {activation} & target function &         MSE 
%     &  MAX &    MSE 
%     &  MAX &    MSE 
%     &  MAX
%     % &   
%     % MSE 
%     % &  MAX
%     \\
    
%     % {activation} &        test error (MSE) 
%     % & test error (MAX) &        test error (MSE) 
%     % & test error (MAX)
%     % &        test error (MSE) 
%     % & test error (MAX)\\
% 			 % \cmidrule{3-5}
% 			 % & & MSE & MAE & MAX\\
% 			\midrule

% $\mathtt{GELU}$& $f_1$ &  $ 2.09 \times 10^{-9} $  &  $ 1.10 \times 10^{-4} $  &  $ 2.15 \times 10^{-7} $  &  $ 1.43 \times 10^{-3} $  &  $ 1.41 \times 10^{-6} $  &  $ 4.16 \times 10^{-3} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{tanh}$& $f_1$ &  $ 5.21 \times 10^{-9} $  &  $ 2.24 \times 10^{-4} $  &  $ 8.37 \times 10^{-7} $  &  $ 3.72 \times 10^{-3} $  &  $ 8.67 \times 10^{-6} $  &  $ 1.38 \times 10^{-2} $ 
%  \\ 

%  $\mathtt{sin}$& $f_1$ &  $ 1.18 \times 10^{-9} $  &  $ 1.41 \times 10^{-4} $  &  $ 1.35 \times 10^{-7} $  &  $ 2.53 \times 10^{-3} $  &  $ 8.11 \times 10^{-7} $  &  $ 3.86 \times 10^{-3} $ 
%  \\ 

% \midrule
% \rowcolor{mygray}$\mathtt{GELU}$& $f_2$ &  $ 2.79 \times 10^{-8} $  &  $ 7.45 \times 10^{-4} $  &  $ 4.59 \times 10^{-6} $  &  $ 1.34 \times 10^{-2} $  &  $ 1.74 \times 10^{-5} $  &  $ 3.09 \times 10^{-2} $ 
%  \\ 

%  $\mathtt{tanh}$& $f_2$ &  $ 1.33 \times 10^{-7} $  &  $ 1.27 \times 10^{-3} $  &  $ 1.19 \times 10^{-5} $  &  $ 1.85 \times 10^{-2} $  &  $ 2.77 \times 10^{-5} $  &  $ 3.78 \times 10^{-2} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{sin}$& $f_2$ &  $ 1.35 \times 10^{-7} $  &  $ 1.31 \times 10^{-3} $  &  $ 1.76 \times 10^{-5} $  &  $ 2.13 \times 10^{-2} $  &  $ 5.23 \times 10^{-5} $  &  $ 4.83 \times 10^{-2} $ 
%  \\ 

% \midrule
%  $\mathtt{GELU}$& $f_3$ &  $ 4.55 \times 10^{-7} $  &  $ 3.01 \times 10^{-3} $  &  $ 1.08 \times 10^{-4} $  &  $ 5.06 \times 10^{-2} $  &  $ 3.95 \times 10^{-4} $  &  $ 1.17 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{tanh}$& $f_3$ &  $ 2.08 \times 10^{-4} $  &  $ 6.00 \times 10^{-2} $  &  $ 1.11 \times 10^{-2} $  &  $ 5.56 \times 10^{-1} $  &  $ 1.09 \times 10^{-2} $  &  $ 6.11 \times 10^{-1} $ 
%  \\ 

%  $\mathtt{sin}$& $f_3$ &  $ 1.71 \times 10^{-6} $  &  $ 5.67 \times 10^{-3} $  &  $ 2.49 \times 10^{-4} $  &  $ 8.03 \times 10^{-2} $  &  $ 5.76 \times 10^{-4} $  &  $ 1.33 \times 10^{-1} $ 
%  \\ 

% \midrule
% \rowcolor{mygray}$\mathtt{GELU}$& $f_4$ &  $ 2.94 \times 10^{-7} $  &  $ 1.70 \times 10^{-3} $  &  $ 3.59 \times 10^{-5} $  &  $ 2.23 \times 10^{-2} $  &  $ 1.13 \times 10^{-4} $  &  $ 4.74 \times 10^{-2} $ 
%  \\ 

%  $\mathtt{tanh}$& $f_4$ &  $ 8.40 \times 10^{-6} $  &  $ 6.93 \times 10^{-3} $  &  $ 2.85 \times 10^{-4} $  &  $ 8.09 \times 10^{-2} $  &  $ 6.24 \times 10^{-4} $  &  $ 1.49 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{sin}$& $f_4=\frac{dd}{dddd}$ &  $ 1.58 \times 10^{-6} $  &  $ 4.76 \times 10^{-3} $  &  $ 1.71 \times 10^{-4} $  &  $ 6.42 \times 10^{-2} $  &  $ 3.56 \times 10^{-4} $  &  $ 1.13 \times 10^{-1} $ 
%  \\ 

% \midrule
%  $\mathtt{GELU}$& $f_5$ &  $ 1.39 \times 10^{-6} $  &  $ 3.65 \times 10^{-3} $  &  $ 1.00 \times 10^{-4} $  &  $ 2.96 \times 10^{-2} $  &  $ 2.38 \times 10^{-4} $  &  $ 6.76 \times 10^{-2} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{tanh}$& $f_5$ &  $ 1.84 \times 10^{-3} $  &  $ 1.60 \times 10^{-1} $  &  $ 6.41 \times 10^{-2} $  &  $ 1.26 \times 10^{0} $  &  $ 4.36 \times 10^{-2} $  &  $ 1.18 \times 10^{0} $ 
%  \\ 

%  $\mathtt{sin}$& $f_5$ &  $ 3.14 \times 10^{-5} $  &  $ 1.79 \times 10^{-2} $  &  $ 2.63 \times 10^{-3} $  &  $ 1.99 \times 10^{-1} $  &  $ 4.73 \times 10^{-3} $  &  $ 2.62 \times 10^{-1} $ 
%  \\ 
%  \bottomrule% \bottomrule[1.2pt] 
% 		\end{tabular} 
% 	}%%% \resizebox
% % 	\vskip -0.1in
% \end{table} 
% \subsection{Derivative Approximation}
% \label{sec:derivative:approx}


% We will investigate the potential of FMMNNs for approximating derivatives by minimizing a cost function that depends solely on the function values of the target function at sampled points. To this end, we select a complex function, as derivative approximation is not particularly challenging for smooth or simple functions. Moreover, our target function should not be generated using sine, cosine, or their compositions or combinations, as we employ \sine{} as the activation function. Based on these considerations, we choose
% \begin{equation}
% \label{eq:true:func:sum}
%     f(x) = \frac{1}{1 + 6x^2}\sum\limits_{i=-n}^{n} \frac{(-1)^i (|i|+1)}{ 1 + 666 \big( x^2 - \tfrac{i}{n+6} \big)^2 }\quad \tn{with $n=16$.}
% \end{equation}
% For this test, we adopt an FMMNN of size $(256, 8, 6)$. A total of 1000 uniformly sampled points in $[-1,1]$ are utilized, and the learning rate is set to $10^{-3} \times 0.9^{\lfloor k/3000 \rfloor}$, where $k = 1,2,\dots,300000$ denotes the epoch number. The mini-batch size is set to 1000, meaning all samples are trained simultaneously. It is worth noting that this test is conducted using double precision.


% In Figure~\ref{fig:derivative:approx}, we plot the relative difference between the target function and the learned network (or their derivatives) because the differences are not easily distinguishable to the naked eye when plotted on the same graph. 

% As shown in Figure~\ref{fig:derivative:approx}, the learned network approximates the target function well, with a relatively uniform error of approximately $0.004$. For the first derivative approximation, the relative uniform error is around $0.015$, while for the second derivative approximation, it is about $0.04$. The accurate approximation of derivatives is surprising, given the complexity of the target function and the fact that the cost function relies only on the function values of the target function.
% \begin{figure}[ht]%[htbp!]  
%             \centering
%             \,\hfill
%             \begin{subfigure}[b]{0.27300245\textwidth}
%                     \centering            
%                     \includegraphics[width=0.999\textwidth]{figures/DerivativeApprox1D1order0true.pdf}
%                     \subcaption{$f$.}
%                 \end{subfigure}
%                 \hfill
%             \begin{subfigure}[b]{0.27300245\textwidth}
%                     \centering            \includegraphics[width=0.999\textwidth]{figures/DerivativeApprox1D1order1true.pdf}
%                     \subcaption{$f^\prime$}
%                 \end{subfigure}
%                 \hfill
%             \begin{subfigure}[b]{0.27300245\textwidth}
%                     \centering            \includegraphics[width=0.999\textwidth]{figures/DerivativeApprox1D1order2true.pdf}
%                     \subcaption{$f^\dprime$}
%                 \end{subfigure}
% \hfill\,\\

%             \,\hfill
%             \begin{subfigure}[b]{0.27300245\textwidth}
%                     \centering            
%                     \includegraphics[width=0.999\textwidth]{figures/DerivativeApprox1D1order0diff.pdf}
%                     \subcaption{$(\phi-f)/\|f\|_{L^\infty([-1,1])}$.}
%                 \end{subfigure}
%                 \hfill
%             \begin{subfigure}[b]{0.27300245\textwidth}
%                     \centering            \includegraphics[width=0.999\textwidth]{figures/DerivativeApprox1D1order1diff.pdf}
%                     \subcaption{$(\phi^\prime-f^\prime)/\|f^\prime\|_{L^\infty([-1,1])}$.}
%                 \end{subfigure}
%                 \hfill
%             \begin{subfigure}[b]{0.27300245\textwidth}
%                     \centering            \includegraphics[width=0.999\textwidth]{figures/DerivativeApprox1D1order2diff.pdf}
%                     \subcaption{$(\phi^\dprime-f^\dprime)/\|f^\dprime\|_{L^\infty([-1,1])}$.}
%                 \end{subfigure}
% \hfill\,

% \caption{Illustrations of the target function $f$ (see \eqref{eq:true:func:sum}), its derivatives, and their normalized differences, where $\phi$ is the learned network.}
%     \label{fig:derivative:approx}   
% \end{figure}



% \subsection{Shallow Network Approximation}
% \label{sec:shallow:nets}


% In this subsection, we compare the approximation performance of shallow neural networks with different activation functions. A shallow network, typically with a single hidden layer, is expressed as  
% \begin{equation}
% \label{eq:def:shallow:net}
%     h(\bmx) = \sum_{i=1}^n a_i \sigma(\bmw_i \cdot \bmx - b_i) + c, \quad \text{for any } \bmx \in \mathbb{R}^d,
% \end{equation}
% where $n \in \mathbb{N}^+$ is the network width, and $\bmw_i \in \mathbb{R}^d$, $a_i, b_i, c \in \mathbb{R}$ are parameters for each $i \in \{1,2,\dots,n\}$. The activation function is denoted by $\sigma$.  

% A key tool for analyzing shallow networks is the Gram matrix (e.g., see \cite{ZZZZ-23}), which provides insights into their ability to distinguish inputs and represent complex functions. In learning dynamics, the Gram matrix influences convergence and generalization, with its eigenvalues affecting learning speed. A well-conditioned Gram matrix, with balanced eigenvalues, generally leads to stable and efficient learning.  
% Focusing on the 1D case, we define the Gram matrix as  
% \begin{equation*}
%    \bmG_{i,j} \coloneqq  \int_{D} \sigma(w_i x - b_i) \sigma(w_j x - b_j) dx,
% \end{equation*}
% where $D = [-1,1]$. The spectra of Gram matrices for various activation functions are illustrated in Figure~\ref{fig:spectrum:gram:m}.
 

%  \begin{figure}[!htp]
% 		\centering
% 		\begin{subfigure}[b]{0.324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/SpecturmAll0.pdf}
% 			\subcaption{$\bmw\sim \calU(-0.2,0.2)$.}
% 		\end{subfigure}
% \hfill
% 		\begin{subfigure}[b]{0.324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/SpecturmAll1.pdf}
% 			\subcaption{$\bmw\sim \calU(-1,1)$.}
% 		\end{subfigure}\hfill
% 		\begin{subfigure}[b]{0.324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/SpecturmAll2.pdf}
% 			\subcaption{$\bmw\sim \calU(-5,5)$.}
% 		\end{subfigure}
% 	\caption{Spectra of Gram matrices corresponding to various activation functions, where $\bmb\sim \calU(-1,1)$.}
% 	\label{fig:spectrum:gram:m}
% \end{figure}


% In this test, we use shallow networks with widths of 256, 526, or 1024 to approximate the target functions $f_i$ ($i=1,2,3$), whose expressions are provided in Table~\ref{tab:error:comparison:shallow}. A total of 1000 uniformly sampled points in $[-1,1]$ are used, with a learning rate set as $10^{-3} \times 0.9^{\lfloor k/300 \rfloor}$, where $k = 1,2,\dots,15000$ denotes the epoch number. The mini-batch size is set to 100, meaning all samples are trained simultaneously. This test is conducted using double precision.  

% As shown in Figure~\ref{fig:shllow:nets:approx:f2} and Table~\ref{tab:error:comparison:shallow}, among shallow networks, $\SinTU_0$ achieves the best performance, followed closely by \ReLU. The performance of activation functions generally aligns with the decay rate of their corresponding Gram matrix spectra—the slower the decay, the better the shallow network performs. This observation supports our analysis in \cite{ZZZZ-23}.

%  \begin{figure}[!htp]
% 		\centering
% 		\begin{subfigure}[b]{0.192324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/ShallowNets1D1.pdf}
% 			\subcaption{\ReLU.}
% 		\end{subfigure}
% \hfill
% 		\begin{subfigure}[b]{0.192324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/ShallowNets1D2.pdf}
% 			\subcaption{\GELU.}
% 		\end{subfigure}\hfill
% 		\begin{subfigure}[b]{0.192324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/ShallowNets1D3.pdf}
% 			\subcaption{\texttt{tanh}.}
% 		\end{subfigure}
%         \hfill
% 		\begin{subfigure}[b]{0.192324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/ShallowNets1D4.pdf}
% 			\subcaption{\sine.}
% 		\end{subfigure}
%         \hfill
% 		\begin{subfigure}[b]{0.192324\textwidth}
% 			\centering
% 			\includegraphics[width=0.999\textwidth]{figures/ShallowNets1D7.pdf}
% 			\subcaption{$\SinTU_0$.}
% 		\end{subfigure}
% 	\caption{Illutrations of the true function $f_2(x)=\frac{1}{1+600(x-0.2)^2}$ and learned shallow networks of width $512$ with various activation functions.}
% 	\label{fig:shllow:nets:approx:f2}
% \end{figure}

% \begin{table}[ht]%[htbp!]
% 	\centering  
%  \setlength{\tabcolsep}{0.68em} 
%  \renewcommand{\arraystretch}{1.15}
% \caption{Comparison of test errors. float64}
% \label{tab:error:comparison:shallow}
% 	\resizebox{0.95\textwidth}{!}{ 
% 		\begin{tabular}{ccccccccc} 
% 			\toprule% \toprule[1.2pt]  
%             % \multicolumn{2}{c}
%             % {
%             % target function
%             % }
%             % & \multicolumn{2}{c}{$f_1:[-1,1]\to \R$}
%             % & \multicolumn{2}{c}{$f_2:[-1,1]^2\to \R$}
%             % & \multicolumn{2}{c}{$f_3:[-1,1]^2\to \R$}
%             % % & \multicolumn{2}{c}{$f_4:[-1,1]^3\to \R$}
%             % \\        
%             % {target function} &\multicolumn{2}{c}{$f_1(x)=\frac{1}{1+100x^2}$} &\multicolumn{2}{c}{$f_2(x)=\frac{1}{1+600x^2}$} &\multicolumn{2}{c}{$f_3(x)=\frac{1}{1+3600x^2}$} \\
%         {target function} &\multicolumn{2}{c}{$f_1(x)=\frac{1}{1+100(x-0.2)^2}$} &\multicolumn{2}{c}{$f_2(x)=\frac{1}{1+600(x-0.2)^2}$} &\multicolumn{2}{c}{$f_3(x)=\frac{1}{1+3600(x-0.2)^2}$} \\
%             % \cmidrule(lr){1-2} 
%             % \cmidrule(lr){3-4}
%             % \cmidrule(lr){5-6}
%             % \cmidrule(lr){7-8}
%             \cmidrule(lr){2-3}
%             \cmidrule(lr){4-5}
%             \cmidrule(lr){6-7}
%             % \cmidrule(lr){8-9}
%         \rowcolor{mygray}   
%         % \multicolumn{2}{c}{network} 
%         % &
%         &\multicolumn{2}{c}{Shallow Network of width 256} &\multicolumn{2}{c}{Shallow Network of width 512} &\multicolumn{2}{c}{Shallow Network of width 1024}
%         % &\multicolumn{2}{c}{ResMMNN of size (1024,40,16)}
%         \\
%            %             \cmidrule(lr){3-4}
%            %  \cmidrule(lr){5-6}
%            %  \cmidrule(lr){7-8}
%            % & &\multicolumn{2}{c}{MMNN1}
%            %  &\multicolumn{2}{c}{MMNN2}
%            %  &\multicolumn{2}{c}{FCNN1}
%            %  \\
%             % \cmidrule(lr){3-4}
%             % \cmidrule(lr){5-6}
%             % \cmidrule(lr){7-8}
%             \cmidrule(lr){2-3}
%             \cmidrule(lr){4-5}
%             \cmidrule(lr){6-7}
%             \cmidrule(lr){8-9}
% 			 %    target  &  activation &        TE (MSE) 
%     % & TE (MAX) &        TE (MSE) 
%     % & TE (MAX)
%     % &        TE (MSE) 
%     % & TE (MAX)\\
%     % \multicolumn{2}{c}
%         {activation} &         MSE 
%     &  MAX &    MSE 
%     &  MAX &    MSE 
%     &  MAX
%     % &   
%     % MSE 
%     % &  MAX
%     \\
    
%     % {activation} &        test error (MSE) 
%     % & test error (MAX) &        test error (MSE) 
%     % & test error (MAX)
%     % &        test error (MSE) 
%     % & test error (MAX)\\
% 			 % \cmidrule{3-5}
% 			 % & & MSE & MAE & MAX\\
% 			\midrule

% $\mathtt{ReLU}$ &  $ 2.50 \times 10^{-7} $  &  $ 3.65 \times 10^{-3} $  &  $ 9.94 \times 10^{-7} $  &  $ 8.40 \times 10^{-3} $  &  $ 2.46 \times 10^{-4} $  &  $ 1.43 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{GELU}$ &  $ 2.82 \times 10^{-3} $  &  $ 2.00 \times 10^{-1} $  &  $ 1.06 \times 10^{-2} $  &  $ 5.86 \times 10^{-1} $  &  $ 8.85 \times 10^{-3} $  &  $ 8.41 \times 10^{-1} $ 
%  \\ 

%  $\mathtt{tanh}$ &  $ 9.24 \times 10^{-5} $  &  $ 3.86 \times 10^{-2} $  &  $ 3.10 \times 10^{-3} $  &  $ 3.27 \times 10^{-1} $  &  $ 5.84 \times 10^{-3} $  &  $ 7.22 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{sin}$ &  $ 5.93 \times 10^{-3} $  &  $ 2.71 \times 10^{-1} $  &  $ 1.15 \times 10^{-2} $  &  $ 6.09 \times 10^{-1} $  &  $ 9.72 \times 10^{-3} $  &  $ 8.78 \times 10^{-1} $ 
%  \\ 

%  $\mathtt{cos}$ &  $ 6.36 \times 10^{-3} $  &  $ 2.76 \times 10^{-1} $  &  $ 1.38 \times 10^{-2} $  &  $ 6.41 \times 10^{-1} $  &  $ 1.08 \times 10^{-2} $  &  $ 9.08 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{cos(\cdot -\pi/4)}$ &  $ 6.04 \times 10^{-3} $  &  $ 2.75 \times 10^{-1} $  &  $ 1.25 \times 10^{-2} $  &  $ 6.23 \times 10^{-1} $  &  $ 9.60 \times 10^{-3} $  &  $ 8.91 \times 10^{-1} $ 
%  \\ 

%  $\mathtt{SinTU}_{0}$ &  $ \bm{5.95 \times 10^{-8}} $  &  $ 2.47 \times 10^{-3} $  &  $ \bm{6.75 \times 10^{-7}} $  &  $ 6.35 \times 10^{-3} $  &  $ \bm{2.19 \times 10^{-4}} $  &  $ 1.28 \times 10^{-1} $ 
%  \\ 

% \rowcolor{mygray}$\mathtt{SinTU}_{-\pi}$ &  $ 2.53 \times 10^{-3} $  &  $ 1.82 \times 10^{-1} $  &  $ 1.03 \times 10^{-2} $  &  $ 5.83 \times 10^{-1} $  &  $ 1.30 \times 10^{-2} $  &  $ 8.92 \times 10^{-1} $ 
%  \\ 
%  \bottomrule% \bottomrule[1.2pt] 
% 		\end{tabular} 
% 	}%%% \resizebox
% % 	\vskip -0.1in
% \end{table} 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proofs of Theorems~\ref{thm:main1} and \ref{thm:main2}}
\label{sec:proof:thm:main}

In this section, we establish the proofs of Theorems~\ref{thm:main1} and \ref{thm:main2}. To facilitate understanding, Section~\ref{sec:notation} provides a concise overview of the notations used throughout the paper. In Section~\ref{sec:proof:ideas:thm:main}, we outline the main ideas behind the proofs of Theorems~\ref{thm:main1} and \ref{thm:main2}. Additionally, for simplification, we introduce two propositions whose proofs are deferred to later sections. Assuming the validity of these propositions, we present the full detailed proofs of Theorems~\ref{thm:main1} and \ref{thm:main2} in Section~\ref{sec:proof:thm:main}.


% In this section, we will prove Theorem~\ref{thm:main}
% To enhance clarity, Section~\ref{sec:notation} offers a concise overview of the notations employed throughout this paper.
% Next in Section~\ref{sec:proof:ideas:thm:main}, we present the ideas for proving Theorem~\ref{thm:main}.
% Moreover, to simplify the proofs, we establish several propositions, which will be proved in later sections.
% By assuming the validity of these propositions,
% we provide the detailed proof of Theorem~\ref{thm:main}
% in Section~\ref{sec:proof:thm:main}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Notations}
\label{sec:notation}


Below is a summary of the fundamental notations used throughout this paper.

\begin{itemize}
    \item The difference between two sets \( A \) and \( B \) is denoted by \( A \backslash B \coloneqq \{ x : x \in A, \ x \notin B \} \).
    
    \item The symbols \( \mathbb{N} \), \( \mathbb{Z} \), \( \mathbb{Q} \), and \( \mathbb{R} \) represent the sets of natural numbers (including 0), integers, rational numbers, and real numbers, respectively. We denote the set of positive natural numbers by \( \mathbb{N}^+ = \mathbb{N} \backslash \{0\} = \{1, 2, 3, \dots \} \).
    
    \item The floor and ceiling functions of a real number \( x \) are given by
    \( \lfloor x \rfloor = \max \{ n : n \le x, \ n \in \mathbb{Z} \} \) and \( \lceil x \rceil = \min \{ n : n \ge x, \ n \in \mathbb{Z} \} \).
    
    \item For any \( p \in [1, \infty] \), the \( p \)-norm (or \( \ell^p \)-norm) of a vector \( \bm{x} = (x_1, \dots, x_d) \in \mathbb{R}^d \) is defined as
    \begin{equation*}
        \|\bm{x}\|_p = \|\bm{x}\|_{\ell^p} \coloneqq \big( |x_1|^p + \dots + |x_d|^p \big)^{1/p}\quad \text{for } p \in [1, \infty),
    \end{equation*}
    and
    \begin{equation*}
        \|\bm{x}\|_{\infty} = \|\bm{x}\|_{\ell^\infty} \coloneqq \max\big\{ |x_i| : i = 1, 2, \dots, d \big\}.
    \end{equation*}


    \item Let $\cpl(n)$ denote the space of all continuous piecewise linear functions on $\R$ with at most $n \in \N$ breakpoints.
    
    \item The supremum norm of a bounded vector-valued function $\bmf: \Omega\subseteq \R^d \to \R^n$ is defined as
    \begin{equation*}
        \|\bmf\|_{\sup(\Omega)}\coloneqq \sup\big\{|f_i(\bmx)|: \bmx\in \Omega,\   i\in\{1,2,\dots,n\}\big\},
    \end{equation*}
    where $f_i$ represents the $i$-th component of $\bmf$ for $i = 1,2,\dots,n$.

    \item The symbol ``$\rightrightarrows$'' denotes uniform convergence. Specifically, if $\bmf:\R^d\to\R^n$ is a vector-valued function and $\bmf_\delta(\bmx) \rightrightarrows \bmf(\bmx)$ as $\delta\to 0$ for all $\bmx\in \Omega\subseteq \R^d$, then for any $\eps>0$, there exists $\delta_\eps\in (0,1)$ such that
    \begin{equation*}		
        \|\bmf_\delta-\bmf\|_{\sup(\Omega)}< \eps\quad \text{for all } \delta\in (0,\delta_\eps).
    \end{equation*}

    \item We adopt slicing notation for vectors and matrices. Given a vector $\bmx = (x_1, \dots, x_d) \in \mathbb{R}^d$, the notation $\bmx[n:m]$ refers to the slice from the $n$-th to the $m$-th entry for any $n, m \in \{1,2,\dots,d\}$ with $n \leq m$, while $\bmx[n]$ represents the $n$-th entry. For example, if $\bmx = (x_1, x_2, x_3) \in \mathbb{R}^3$, then 
    \(    (6\bmx)[2:3] = (6x_2, 6x_3)\) and \( (8\bmx+1)[3] = 8x_3 + 1.\)
    Similarly, for a matrix $\bmA$, the notation $\bmA[:,i]$ denotes its $i$-th column, while $\bmA[i,:]$ represents its $i$-th row. Moreover, $\bmA[i,n:m]$ is equivalent to $(\bmA[i,:])[n:m]$, extracting the $n$-th to $m$-th entries from the $i$-th row.


%     \item Let $\cpl(n)$ denote the space that consists of all continuous piecewise linear functions with at most $n\in\N$ breakpoints on $\R$.
    
%     \item Define the supremum norm of a bounded vector-valued function $\bmf: \Omega\subseteq \R^d \to \R^n$  via
% \begin{equation*}
% 	\|\bmf\|_{\sup(\Omega)}\coloneqq \sup\big\{|f_i(\bmx)|: \bmx\in \Omega,\   i\in\{1,2,\cdots,n\}\big\},
% \end{equation*}
% where $f_i$ is the $i$-th component of $\bmf$ for $i=1,2,\cdots,n$. 



%     		\item Let ``$\rightrightarrows$" denote the uniform convergence. For example, if $\bmf:\R^d\to\R^n$ is a vector-valued function and $\bmf_\delta(\bmx)\rightrightarrows \bmf(\bmx)$ as $\delta\to 0$ for any $\bmx\in \Omega\subseteq \R^d$, then
% 	for any $\eps>0$, there exists $\delta_\eps\in (0,1)$ such that
%  \begin{equation*}		
%  \|\bmf_\delta-\bmf\|_{\sup(\Omega)}< \eps\quad \tn{for any $\delta\in (0,\delta_\eps)$.}
% 	\end{equation*}

%     \item 
%  We use slicing notation for a vector $\bmx = (x_1, \dots, x_d) \in \mathbb{R}^d$, where $\bmx[n:m]$ represents the slice from the $n$-th to the $m$-th entry for any $n, m \in \{1,2,\dots,d\}$ with $n \leq m$, and $\bmx[n]$ refers to the $n$-th entry of $\bmx$. For example, if $\bmx = (x_1, x_2, x_3) \in \mathbb{R}^3$, then $(5\bmx)[2:3] = (5x_2, 5x_3)$ and $(6\bmx+1)[3] = 6x_3 + 1$.  
% A similar notation applies to matrices. Specifically, $\bmA[:,i]$ denotes the $i$-th column of $\bmA$, while $\bmA[i,:]$ refers to its $i$-th row. Moreover, $\bmA[i,n:m]$ is equivalent to $(\bmA[i,:])[n:m]$, meaning it extracts the $n$-th to $m$-th entries from the $i$-th row.

    
    % \item A network is referred to as ``a network of width \( N \) and depth \( L \)'' if it satisfies the following two conditions:
    % \begin{itemize}
    %     \item The maximum number of neurons in each hidden layer does not exceed \( N \).
    %     \item The network contains no more than \( L \) hidden layers in total.
    % \end{itemize}
\end{itemize}

\subsection{Ideas and Propositions for Proving Theorems~\ref{thm:main1} and \ref{thm:main2}}
\label{sec:proof:ideas:thm:main}

Before presenting the detailed proofs of Theorems~\ref{thm:main1} and \ref{thm:main2}, let us first outline the key ideas underlying our approach. The main strategy in the proof involves constructing a piecewise constant function that approximates the desired continuous target function. However, achieving a uniform approximation with piecewise constants is challenging due to the continuity of ReLU and \sine{}  functions. To address this, we design networks that approximate piecewise constant behavior over most of the domain, specifically outside a small region, ensuring that the approximation error remains well-controlled. Within this small region, the error is manageable, as its measure can be made arbitrarily small.

With this foundation, we now proceed to the details. We divide the domain $[0,1]^d$ into a collection of ``important" cubes, denoted $\{Q_k\}_{k \in \{1,2,\cdots,M^d\}}$, along with a ``negligible'' region $\Omega$, where $M = N^L$. Each cube $Q_k$ is associated with a representative point $\bm{x}_k \in Q_k$. An illustration of $\bm{x}_k$, $\Omega$, and $Q_k$ can be seen in Figure~\ref{fig:idea:main}. The construction of the desired network to approximate the target function is organized into two main steps below.

\begin{enumerate}
    \item First, we construct a sub-network that realizes a function $\phi_1$ which maps each cube $Q_k$ to its respective index $k$. Specifically, we have $\phi_1(\bm{x}) = k$ for any $\bm{x} \in Q_k$ and $k \in \{1,2,\cdots,M^d\}$.
    
    \item Next, we design a sub-network to implement a function $\phi_2$ that maps each index $k$ approximately to $f(\bm{x}_k)$. Consequently, we obtain $\phi_2 \circ \phi_1(\bm{x}) = \phi_2(k) \approx f(\bm{x}_k) \approx f(\bm{x})$ for any $\bm{x} \in Q_k$ and $k \in \{1,2,\cdots,M^d\}$, implying that $\phi_2 \circ \phi_1 \approx f$ outside of $\Omega$.
\end{enumerate}



\begin{figure}[ht]%[htbp!] 
	\centering
 % \vskip 3.1pt
	\includegraphics[width=0.825\linewidth]{ideaPicMain.pdf}
\caption{An illustration of the approach for constructing a network to approximate \( f \). 
Observe that \(\phi_2 \circ \phi_1 \approx f\) outside of \(\Omega\), as \(\phi_2 \circ \phi_1(\bm{x}) = \phi_2(k) \approx f(\bm{x}_k) \approx f(\bm{x})\) for any \(\bm{x} \in Q_k\) and \(k \in \{1, 2, \dots, M^d\}\). 
% We note that, for clarity and logical coherence, the detailed proofs of Theorem~\ref{thm:main1} and \ref{thm:main2} in Section~\ref{sec:detailed:proof:thm:main} does not directly construct \(\phi_1\) and \(\phi_2\), though the underlying concepts remain the same.
}
 % \vskip -0.1pt
	% \caption{An illustration of the ideas for constructing the desired function $\phi=\phi_2\circ\phi_1$. 
	% 	Note that $\phi\approx f$ outside $\Omega$
	% 	since $\phi(\bmx)=\phi_2\circ\phi_1(\bmx)=\phi_2(k)\approx f(\bmx_k)\approx f(\bmx)$ for any $\bmx\in Q_k$ and $k\in\{1,2,\cdots,M^d\}$.}
	\label{fig:idea:main}
\end{figure}

% we remakr that in the detailed proof presented later, 

The floor function is quite effective for handling the first step. To simplify the final proof, we introduce Proposition~\ref{prop:floor:approx} below, which demonstrates how to construct a network that efficiently approximates the floor function. The proof of Proposition~\ref{prop:floor:approx} is provided in Section~\ref{sec:proof:prop:floor:approx}.



\begin{proposition}
	\label{prop:floor:approx}
	Given any $\delta\in (0,1)$ and $N,L\in \N^+$,
	there exists 
 % a function $\phi$ realized by a ReLU FCNN of width $4N-1$ and depth $L$  
 \[\phi\in \mn[\big]{\ReLU}{4N-1}{3}{L}{\R}{\R}\]
 such that 
	\begin{equation*}
		% \label{eq:floor:approx:final}
		\phi(x)=\lfloor x\rfloor\quad \tn{for any $x\in \bigcup_{k=0}^{N^L-1}\big[k,\, k+1-\delta\big].$}
	\end{equation*}
	% if $x\in \big[k,\,k+1-\delta\cdot\one_{\{k\le n-2\}}\big]$ for $k=0,1,\cdots,n-1$.
\end{proposition}

% \red{ReLU networks can ``exponentially'' approximate the floor function.}




% The purpose of $\phi_2$ is to approximate $f(\bm{x}_k)$ by mapping each $k \in \{1,2,\cdots,M^d\}$ to a value close to $f(\bm{x}_k)$. Notably, in constructing $\phi_2$, it suffices to focus only on its values at a finite set of points, $\{1,2,\cdots,M^d\}$, rather than over the entire domain. This focus significantly simplifies the design of a network that realizes $\phi_2$. 

% However, even with this simplification, the ReLU activation function is not well-suited for efficiently handling such point-fitting tasks. In Proposition~\ref{prop:k:to:yk}, we demonstrate that the \sine{}  function is highly effective for this type of problem. The proof of Proposition~\ref{prop:k:to:yk} is provided in Section~\ref{sec:proof:prop:k:to:yk}.

The purpose of $\phi_2$ is to map each $k$ approximately to $f(\bmx_k)$ for $k \in \{1, 2, \cdots, M^d\}$. Notably, in constructing $\phi_2$, we only need to ensure correct values at a finite set of points $\{1, 2, \cdots, M^d\}$, rather than over an entire continuous domain. This key insight significantly simplifies the design of a network that realizes $\phi_2$.
However, even with this simplification, the \ReLU\ activation function is not particularly effective for this type of point-matching problem. In Proposition~\ref{prop:k:to:yk} below, we demonstrate that the \sine{}  function is exceptionally efficient for this task. The proof of Proposition~\ref{prop:k:to:yk} is provided in Section~\ref{sec:proof:prop:k:to:yk}.


% \begin{proposition}
%     \label{prop:k:to:yk}
%     Given any  $\eps>0$ and $y_k\in \R$ for $k=1,2,\cdots,K$, there exist $u ,v ,w \in\R$ such that
%     \begin{equation*}
%         \big|u \cdot \sin\big(v  \cdot \sin(kw )\big)-y_k\big|<\eps \quad \tn{for $k=1,2,\cdots,K$.}
%     \end{equation*}
% \end{proposition}


\begin{proposition}
    \label{prop:k:to:yk}
    Given any  $\eps>0$ and $y_k\in \R$ for $k=1,2,\cdots,K$, there exist $u ,v ,w \in\R$ such that
    \begin{equation*}
        \big|u \cdot \sin\big(v  \cdot \sin(kw )\big)-y_k\big|<\eps \quad \tn{for $k=1,2,\cdots,K$.}
    \end{equation*}
\end{proposition}

We remark that Proposition~\ref{prop:k:to:yk} can also be understood through the concept of density. Specifically, for any \( K \in \mathbb{N}^+ \), the set
\begin{equation*}
    \Bigg\{\bigg(u \cdot \sin\big(v \cdot \sin(w)\big),\; u \cdot \sin\big(v \cdot \sin(2w)\big),\; \cdots,\; u \cdot \sin\big(v \cdot \sin(Kw)\big)\bigg) : u, v, w \in \mathbb{R} \Bigg\}
\end{equation*}
is dense in \( \mathbb{R}^K \). Additionally, we note that in Proposition~\ref{prop:k:to:yk}, we can set \( u = \max\{|y_k| : k = 1, 2, \cdots, K\} \).





When analyzing the approximation power of MMNNs activated by \SinTU{s}, we need to leverage the singularity of  \SinTU{s}  for spatial partitioning. To simplify this process, we use \(\ReLU\) for spatial partitioning and employ sub-MMNNs activated by  \SinTU{s}  to reproduce/approximate  \ReLU. Proposition~\ref{prop:approx:ReLU} below is specifically introduced to streamline the proof. The detailed proof of Proposition~\ref{prop:approx:ReLU} can be found in Section~\ref{sec:proof:prop:approx:ReLU}.


\begin{proposition}
	\label{prop:approx:ReLU}
	Given any $B>0$, $k\in \N$, and $\varrho\in \calS$, there exists
	$\phi_\eta\in \nn{\varrho}{2}{1}{\R}{\R}$ for each $\eta\in (0,1)$
%	\begin{equation*}
%		\phi_\eta\in \nn{\varrho}{k+2}{1}{\R}{\R}\quad \tn{for each $\eta\in (0,1)$}
%	\end{equation*}  
	such that
	\begin{equation*}
		\phi_\eta(x)\rightrightarrows \ReLU(x)\quad \tn{as}\   \eta\to 0^+ \quad \tn{for any $x\in [-B,B]$.}
	\end{equation*}
\end{proposition}

The above proposition demonstrates that two active \(\varrho\)-activated neurons are sufficient to approximate \(\ReLU\) arbitrarily well.





% \begin{proposition}
% \label{prop:composition:approx}
% Given $A>0$ and   two vector-valued functions $\bmf:\R^d\to\R^n$ and $\bmg:\R^n\to\R^m$. Suppose 
%  $\bmg$ is continuous,
% 	\begin{equation*}
% 		\bmf_\eta(\bmx)\rightrightarrows \bmf(\bmx)\quad \tn{as}\  \eta\to 0\quad \tn{for any $\bmx\in [-A,A]^d$,}
% 	\end{equation*}
%     and 
%     	\begin{equation*}
% 		\bmg_\eta(\bmy)\rightrightarrows \bmg(\bmy)\quad \tn{as}\  \eta\to 0\quad \tn{for any $\bmy\in [-\tildeA,\tildeA]^n$,}
% 	\end{equation*}
%     where $\tildeA=1+\|\bmf\|_{\sup([A,A])}$. 
%     Then 
% \begin{equation*}
% 		\bmg \circ \bmf_\eta(\bmx)\rightrightarrows \bmg\circ \bmf(\bmx)\quad \tn{as}\  \eta\to 0\quad \tn{for any $\bmx\in [-A,A]^d$.}
% \end{equation*}
%     and 
%     	\begin{equation*}
% 		\bmg_\eta\circ \bmf_\eta(\bmx)\rightrightarrows \bmg\circ \bmf(\bmx)\quad \tn{as}\  \eta\to 0\quad \tn{for any $\bmx\in [-A,A]^d$.}
% 	\end{equation*}
% \end{proposition}



% \subsection{Proof of Proposition~\ref{prop:composition:approx}}


\subsection{Detailed Proofs of Theorems~\ref{thm:main1} and \ref{thm:main2} Based on Propositions}
\label{sec:detailed:proof:thm:main}

We are now prepared to present the detailed proofs of Theorems~\ref{thm:main1} and \ref{thm:main2}, assuming the validity of Propositions~\ref{prop:floor:approx} and \ref{prop:k:to:yk}, which will be proven in Sections~\ref{sec:proof:prop:floor:approx} and \ref{sec:proof:prop:k:to:yk}, respectively.



% The key of proof is to construct $\phi$.
% The proof can be divided into four steps as follows:
% \begin{enumerate}
% 	\item Divide $[0,1]^d$ into a union of sub-cubes $\{Q_{\bm{\beta}}\}_{\bm{\beta}\in \{0,1,\cdots,K-1\}^d}$ and the trifling region $\Omega([0,1]^d,K,\delta)$, and denote $\bmx_\bmbeta$ as the vertex of $Q_\bmbeta$ with minimum $\|\cdot\|_1$ norm;
	
% 	\item Construct a sub-network to implement a vector function $\bmPhi_1$ projecting the whole cube $ Q_\bmbeta$ to the $d$-dimensional index $\bmbeta$ for each $\bmbeta$, i.e., $\bmPhi_1(\bmx)=\bmbeta$ for all $\bmx\in Q_\bmbeta$;
	
% 	\item Construct a sub-network to implement a  function $\phi_2$  mapping the index $\bmbeta$ approximately to $\tildef(\bmx_\bmbeta)$. Construct the final target network to implement the desired function $\phi$ such that $\phi(\bmx)= \phi_2 \circ \bmPhi_1(\bmx) +f(\bmzero)-\omega_f(\sqrt{d})\approx \tildef(\bmx_\bmbeta) +f(\bmzero)-\omega_f(\sqrt{d}) = f(\bmx_\bmbeta)$ for $\bmx\in Q_\bmbeta$.
% \end{enumerate}    

% The details of these steps can be found below.
Let $M=N^L$ and $\delta\in (0,1)$ be a small number determined later.
% \mystep{1}{Divide $[0,1]^d$ into a set of sub-cubes $\{Q_{k}\}_{k\in \{1,2,\cdots,M^d\}}$ and a small region $\Omega$.}
We first divide $[0,1]^d$ into a set of sub-cubes  and a small region. To this end, we
define  $\tildebmx_\bmbeta \coloneqq \bmbeta/M$ and 
\[
\tildeQ_{\bm{\beta}}\coloneqq\Big\{{\bm{x}}= (x_1,\cdots,x_d) \in [0,1]^d:x_i\in[\tfrac{\beta_i}{M},\tfrac{\beta_i+1-\delta}{M}], \ i=1,\cdots,d\Big\}
\]
for each $d$-dimensional index  ${\bm{\beta}}= (\beta_1,\cdots,\beta_d) \in \{0,1,\cdots,M-1\}^d$. Then the ``negligible'' region $\Omega$, given by
\[\Omega=[0,1]^d\big\backslash \big(\cup_{\bm{\beta}\in \{0,1,\cdots,M-1\}^d}\tildeQ_{\bm{\beta}}\big),\]
has a sufficiently small measure for small \(\delta\).


To simplify notation, we reindex the \(d\)-dimensional indices as one-dimensional indices. For this purpose, we establish a one-to-one mapping between \(\{0, 1, \cdots, M-1\}^d\) and \(\{1, 2, \cdots, M^d\}\), defined by\footnote{Note that the definition of \(g\) is inspired by concepts from representations of integers in various bases.}
\[
g(\bm{\beta}) = 1 + \sum_{i=1}^{d} \beta_i \cdot M^{i-1}\quad \tn{for any $\bmbeta=(\beta_1,\cdots,\beta_d)\in \{0, 1, \cdots, M-1\}^d$}.
\]
Thus, for each \(k \in \{1, 2, \cdots, M^d\}\), there exists a unique \(\bm{\beta} \in \{0, 1, \cdots, M-1\}^d\) such that \(g(\bm{\beta}) = k\). Accordingly, we reindex \(\tilde{\bm{x}}_{\bm{\beta}}\) and \(\tilde{Q}_{\bm{\beta}}\) as \(\bm{x}_k\) and \(Q_k\), respectively. That is,
\begin{equation*}
    \bm{x}_{k}= \tilde{\bm{x}}_{\bm{\beta}}
 \quad \tn{and}\quad  Q_{k}=  \tilde{Q}_{\bm{\beta}}\quad \tn{with $k=g(\bmbeta)$}\quad \tn{for any $\bmbeta=(\beta_1,\cdots,\beta_d)\in \{0, 1, \cdots, M-1\}^d$}.
\end{equation*}
See Figure~\ref{fig:Omega:Q:x} for illustrations of  $\Omega$,  $\tildeQ_\bmbeta$,  $\tildebmx_\bmbeta$, $Q_k$, and $\bmx_k$ for $\bmbeta\in \{0,1,\cdots,M-1\}^d$ and $k\in \{1,2,\cdots,M^d\}$
 when $M=4$ and $d=2$.

 \begin{figure}[!htp]
	\centering
	\begin{minipage}{0.905\textwidth}
		\centering
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=0.7999\textwidth]{cubes2Dbeta.pdf}
			% \subcaption{}
		\end{subfigure}
			\begin{minipage}{0.1\textwidth}
				\
			\end{minipage}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=0.7999\textwidth]{cubes2Dk.pdf}
			% \subcaption{}
		\end{subfigure}
	\end{minipage}
	\caption{Illustrations of  $\Omega$,  $\tildeQ_\bmbeta$,  $\tildebmx_\bmbeta$, $Q_k$, and $\bmx_k$ for $\bmbeta\in \{0,1,\cdots,M-1\}^d$ and $k\in \{1,2,\cdots,M^d\}$
 when $M=4$ and $d=2$. }
	\label{fig:Omega:Q:x}
\end{figure}

Next, we construct a network-realized function that maps \(\bm{x}  \in Q_k\) to \(k\) for any \(k \in \{1, 2, \cdots, M^d\}\). Fixing \(\bm{x}= (x_1, \cdots, x_d) \in Q_k\) for some \(k \in \{1, 2, \cdots, M^d\}\), there exists a unique \(\bm{\beta} = (\beta_1, \cdots, \beta_d)\) such that \(g(\bm{\beta}) = k\). 
Then, \(\bm{x} \in Q_k = \tilde{Q}_{\bm{\beta}}\) implies
\begin{equation*}
    x_i \in \Big[\frac{\beta_i}{M},\; \frac{\beta_i + 1 - \delta}{M}\Big]\quad \tn{for } i = 1, \cdots, d,
\end{equation*}
from which we deduce
\begin{equation}\label{eq:Mxi:in:beta:i:i+1}
    Mx_i \in [\beta_i,\; \beta_i + 1 - \delta]  \quad \tn{with } \beta_i \in \{0, 1, \cdots, M-1\} \quad  \tn{ for } i = 1, \cdots, d.
\end{equation}
By Proposition~\ref{prop:floor:approx}, there exists a function \( {\psi}  \in \nn[\big]{\ReLU}{4N-1}{L}{\mathbb{R}}{\mathbb{R}}\) such that
\begin{equation*}
     {\psi} (x) = \lfloor x \rfloor \quad \text{for any } x \in \bigcup_{m=0}^{N^L - 1} \big[m, m + 1 - \delta\big] = \bigcup_{m=0}^{M - 1} \big[m, m + 1 - \delta\big].
\end{equation*}
Then, by Equation~\eqref{eq:Mxi:in:beta:i:i+1}, we have
\begin{equation*}
    {\psi}(x_i) = \beta_i \quad \text{for } i = 1,   \cdots, d.
\end{equation*}
By defining
\begin{equation*}
\bmPsi(\bmy)\coloneqq \Big(\psi(y_1),\psi(y_2),\cdots,\psi(y_d)\Big) \quad \tn{for any } \bmx=(y_1,\cdots,y_d) \in \R^d,
\end{equation*}
we have $\bmPsi(\bmx)=\bmbeta$, implying
\begin{equation*}
    g\circ \bmPsi(\bmx)=g(\bmbeta)=k.
\end{equation*}
Since \(\bm{x} \in Q_k\) and \(k \in \{1, 2, \cdots, M^d\}\) are arbitrary, we have
\begin{equation}\label{eq:g:tildePsi:x:to:k}
    g \circ \bmPsi(\bm{x}) = k \quad \tn{for any } \bm{x} \in Q_k \text{ and } k \in \{1, 2, \cdots, M^d\}.
\end{equation}

To construct a function that maps \( k \in \{1, 2, \cdots, M^d\} \) approximately to \( f(\bm{x}_k) \), we apply Proposition~\ref{prop:k:to:yk} with \( K = M^d \) and \( y_k = f(\bm{x}_k) \). This yields the existence of constants \( u, v, w \in \mathbb{R} \) such that
\begin{equation}
\label{eq:sinsin:approx:f(xk):eps}
    \left| u \cdot \sin\big(v \cdot \sin(k w)\big) - f(\bmx_k) \right| < \epsilon \quad \text{for } k = 1, 2, \cdots, M^d,
\end{equation}
where \( \epsilon > 0 \) is a small number to be determined later.

We define 
% $\phi\coloneqq \phi_2\circ \phi_1$ where
\begin{equation*}
    \phi_1(\bmy)\coloneqq  g\circ \bmPsi(\bmy)\quad \tn{for any $\bmy\in \R^d$}
\end{equation*}
and
\begin{equation*}
    \phi_2(z) = u \cdot \sin\big(v \cdot \sin(wz)\big)
    \quad \tn{for any $z\in \R$.}
\end{equation*}
It is clear that 
\(\psi \in \mn[\big]{\ReLU}{4N-1}{3}{L}{\mathbb{R}}{\mathbb{R}}\)
implies
\[{\bm{\Psi}} \in \mn[\big]{\ReLU}{d(4N-1)}{3d}{L}{\mathbb{R}^d}{\mathbb{R}^d}.\]
Recall that \( g:\R^d\to\R\) is an affine linear map. 
% By combining \(g\) and the final affine linear map (correspoding to $\bmPsi$) into a new one, we have 
By combining \(g\) with the final affine linear map (correspoding to \(\bm{\Psi}\)) into a new mapping, we obtain
\[\phi_1 =  g \circ {\bm{\Psi}} \in \mn[\big]{\ReLU}{d(4N-1)}{3d}{L}{\mathbb{R}^d}{\mathbb{R}^d}.\]
Since $\phi_2(z) = u \cdot \sin\big(v \cdot \sin(wz)\big)$, we have
\[ \phi_2\circ\phi_1  \in \mn[\big]{(\sin,\ReLU)}{d(4N-1)}{3d}{L+2}{\mathbb{R}}{\mathbb{R}^d}.\]

To complete the proof of Theorem~\ref{thm:main2}, where the corresponding $\phi$ is defined as $\phi \coloneqq \phi_2 \circ \phi_1$, it remains to bound the approximation error.
For any \(\bm{x} \in Q_k\) and \(k \in \{1, 2, \cdots, M^d\}\), by Equations~\eqref{eq:g:tildePsi:x:to:k} and \eqref{eq:sinsin:approx:f(xk):eps}, we have
\begin{equation*}
    \begin{split}
        % |\phi(\bmx)-f(\bmx_k)|
        % =
        |\phi_2\circ \phi_1(\bmx)-f(\bmx_k)|  
        =\Big|\phi_2\Big(
        \underbrace{g\circ\bmPsi(\bmx)}_{\tn{$=k$ by \eqref{eq:g:tildePsi:x:to:k}}}
        \Big)-f(\bmx_k)\Big|        
        &= |\phi_2 ( k )-f(\bmx_k) |
        \\ &
        % =|\phi_2 ( w )-f(\bmx_k) |
         =\left| u \cdot \sin\big(v \cdot \sin(k w)\big) - f(\bmx_k) \right| < \epsilon
    \end{split}
\end{equation*}
and $\|\bmx_k-\bmx\|_2\le \tfrac{\sqrt{d(1-\delta)}}{M}\le \frac{\sqrt{d}}{M}$,
from which we deduce
\begin{equation*}
    \begin{split}
        |\phi_2\circ \phi_1(\bmx)-f(\bmx)|
         \le 
        |\phi_2\circ \phi_1(\bmx)-f(\bmx_k)|
        +|f(\bmx_k)-f(\bmx)|
        \le \eps + \omega_f(\tfrac{\sqrt{d}}{M}).
    \end{split}
\end{equation*}
Recall that 
\begin{equation*}
    \bigcup_{k\in \{0,1,\cdots,M^d-1\}} Q_{k} = [0,1]^d \setminus \Omega\quad \tn{ and}  \quad
  % \|\phi\|_{L^\infty([0,1]^d)}=
  \|\phi_2 \circ \phi_1\|_{L^\infty([0,1]^d)} \le \|\phi_2\|_{L^\infty(\mathbb{R})}  \le |u|,
\end{equation*}
where \(u\) is a constant determined by \(f\) and is independent of \(\delta\).
Therefore
	\begin{equation*}
	\begin{split}
	\| \phi_2\circ \phi_1-f\|_{L^p([0,1]^d)}^p
	&=\int_{\Omega}| \phi_2\circ \phi_1(\bmx)-f(\bmx)|^p\tn{d}\bmx
    +
    \int_{[0,1]^d\backslash\Omega}|\phi_2\circ \phi_1(\bmx)-f(\bmx)|^p\tn{d}\bmx
 \\
 	&\le 
 \mu(\Omega)\cdot \| \phi_2\circ \phi_1-f\|_{L^\infty(\Omega)}^p
 + \sum_{k=1}^{M^d}\int_{Q_k}| \phi_2\circ \phi_1(\bmx)-f(\bmx)|^p\tn{d}\bmx
	\\&\le 
 \mu(\Omega)\cdot \Big(|u|+\|f\|_{L^\infty([0,1]^d)}\Big)^p
 + \sum_{k=1}^{M^d}
 \mu(Q_k)\cdot\Big(\eps + \omega_f(\tfrac{\sqrt{d}}{M})\Big)^p
 \\&\le 
 \mu(\Omega)\cdot \Big(|u|+\|f\|_{L^\infty([0,1]^d)}\Big)^p
 + \Big(\eps + \omega_f(\tfrac{\sqrt{d}}{M})\Big)^p
 \le \Big(\tfrac{11}{10}\omega_f(\tfrac{\sqrt{d}}{M})\Big)^p
	\end{split}
	\end{equation*}
where the last inequality is achieved by setting
\begin{equation*}
    \eps = \tfrac{1}{11}\omega_f(\tfrac{\sqrt{d}}{M})>0
\end{equation*}
and choosing a sufficiently small \(\delta \in (0,1)\) to make \(\mu(\Omega)\) small enough, ensuring that
\begin{equation*}
    \mu(\Omega)\cdot \Big(|u|+\|f\|_{L^\infty([0,1]^d)}\Big)^p
    \le  \Big(\tfrac{11}{10}\omega_f(\tfrac{\sqrt{d}}{M})\Big)^p - \Big(\tfrac{12}{11}\omega_f(\tfrac{\sqrt{d}}{M})\Big)^p
    % =\Big( \big(\tfrac{11}{10})^p-\big(\tfrac{12}{11}\big)^p\Big)\cdot \Big(\omega_f(\tfrac{\sqrt{d}}{M})\Big)^p
    >0.
\end{equation*}
We note that the condition \(\omega_f\big(\frac{\sqrt{d}}{M}\big) > 0\) can be satisfied; otherwise, \(f\) would be a constant function, which is a trivial case.
That is, we obtain
\begin{equation}
\label{eq:phi2:phi1:-f}
\begin{split}
\| \phi_2\circ \phi_1-f\|_{L^p([0,1]^d)}
\le \tfrac{11}{10}\omega_f(\tfrac{\sqrt{d}}{M}).
\end{split}
\end{equation}
Recall that  $\omega_f(n\cdot  t)\le n\cdot \omega_f(t)$ for any $n\in\N^+$ and $t\in [0,\infty)$. Thus, we have
	\begin{equation*}
	\begin{split}
	\| \phi-f\|_{L^p([0,1]^d)}
 \le  \tfrac{11}{10}\omega_f(\tfrac{\sqrt{d}}{M}) \le \tfrac{11}{10}\omega_f(\tfrac{\lceil\sqrt{d}\rceil}{M}) 
 \le   \tfrac{11}{10}\big\lceil\sqrt{d}\big\rceil\cdot\omega_f(\tfrac{1}{M}) 
 \le 2\sqrt{d}\cdot\omega_f(N^{-L}),
	\end{split}
	\end{equation*}
where the last inequality follows from \(M = N^L\) and the fact that \(\frac{11}{10} \lceil \sqrt{n} \, \rceil \le 2 \sqrt{n}\) for any \(n \in \mathbb{N}^+\). This completes the proof of Theorem~\ref{thm:main2}.


Recall that \(\varrho \in \mathcal{S}\) is a \SinTU{} activation function. 
To complete the proof of Theorem~\ref{thm:main1}, it is necessary to implement/approximate \(\phi_1\) and \(\phi_2\) using $\varrho$-activated MMNNs, rather than \ReLU{} or \(\sine\) as was done in the proof of Theorem~\ref{thm:main2}.

First, we will construct 
$\phi_{1,\eta}\in \mn{\varrho}{2d(4N-1)}{3d}{L}{\R^d}{\R}$ for any $\eta\in(0,1)$ such that
\begin{equation*}
		\phi_{1,\eta}(\bmx)\rightrightarrows \phi_{1 }(\bmx)\quad \tn{as}\  \eta\to 0^+\quad \tn{for any $\bmx\in [0,1]^d$.}
	\end{equation*}
To this end, we first construct a $\varrho$-activation MMNN to approximate the \ReLU\ function, since $\phi_{1 }\in \mn{\ReLU}{2d(4N-1)}{3d}{L}{\R^d}{\R}$. 
Without loss of generality, we assume that $\phi_1$ can be expressed as a composition of functions
	\begin{equation*}
		\phi_1(\bmx) =\calA_L\circ\ReLU\circ
        \calhatA_{L-1}\circ\caltildeA_{L-1}\circ  \ \cdots \  \circ \ReLU\circ \calhatA_{ 1}\circ\caltildeA_{ 1}\circ \ReLU\circ\calA_0(\bmx)\quad \tn{for any $\bmx\in\R^d$},
	\end{equation*}
	where $\calA_0:\R^d\to \R^{3d(4N-1)}$, $\calbmtildeA_\ell:\R^{3d(4N-1)}\to\R^{3d}$, $\calbmhatA_\ell:\R^{3d}\to \R^{3d(4N-1)}$, and $\calA_L:\R^{3d(4N-1)}\to\R $ are affine linear maps for any $\ell\in \{1,2,\cdots,L-1\}$.
    By setting $\calA_\ell=\calbmhatA_\ell\circ\calbmtildeA_\ell$ for any $\ell\in \{1,2,\cdots,L-1\}$, we have
	\begin{equation*}
		\phi_1(\bmx) =\calA_L\circ\ReLU\circ\calA_{L-1}\circ  \ \cdots \  \circ \ReLU\circ\calA_1\circ\ReLU\circ\calA_0(\bmx)\quad \tn{for any $\bmx\in\R^d$},
	\end{equation*}
	% where $N_0=d$, $N_1,N_2,\cdots,N_L\in\N^+$ with $\max\{N_1,N_2,\cdots,N_L\}\le N$,  $N_{L+1}=n$, $\bmW_\ell\in \R^{N_{\ell+1}\times N_{\ell}}$ and $\bm{b}_\ell\in \R^{N_{\ell+1}}$ are the weight matrix and the bias vector in the $\ell$-th affine linear transform $\calA_\ell:\bmy \mapsto \bmW_\ell\cdot\bmy+\bmb_\ell$ for each $\ell\in \{0,1,\cdots,L\}$.

Since $\varrho\in\calS$, by Proposition~\ref{prop:approx:ReLU}, there exists 
$\sigma_\eta\in\nn{\varrho}{2}{1}{\R}{\R}$
for each $\eta\in(0,1)$
such that
\begin{equation*}
		\sigma_{ \eta}( x)\rightrightarrows \ReLU(x)\quad \tn{as}\  \eta\to 0^+\quad \tn{for any $ x\in [-B,B]$,}
	\end{equation*}
    where $B>0$ is a large number determined later.

	For each $\eta\in (0,1)$, we define 
		\begin{equation*}
		\phi_{1,\eta}(\bmx) =\calA_L\circ\sigma_{\eta}\circ\calA_{L-1}\circ  \ \cdots \  \circ \sigma_{\eta}\circ\calA_1\circ\sigma_{\eta}\circ\calA_0(\bmx)\quad \tn{for any $\bmx\in\R^d$}.
	\end{equation*}
    In other words,
    	\begin{equation*}
		\phi_{1,\eta}(\bmx) =\calA_L\circ\sigma_{\eta}\circ
        \calhatA_{L-1}\circ\caltildeA_{L-1}\circ  \ \cdots \  \circ \sigma_{\eta}\circ \calhatA_{ 1}\circ\caltildeA_{ 1}\circ \sigma_{\eta}\circ\calA_0(\bmx)\quad \tn{for any $\bmx\in\R^d$}.
	\end{equation*}
	%	Recall that $\ReLU_\eta$ can be realized by a $\varrho$-activated network with width $\tildeN$ and depth $\tildeL$. Thus, $\bmphi_{\ReLU_\eta}$ can be realized by a $\varrho$-activated network with width $N\cdot\tildeN$ and depth $L\cdot\tildeL$.
    Recall that 
$\sigma_\eta\in\nn{\varrho}{2}{1}{\R}{\R}$. To replace the ReLU activation function with $\sigma_\eta$ in a network, we substitute each ReLU   with two $\varrho$-activated neurons. Consequently,
$\phi_{1}\in \mn{\varrho}{d(4N-1)}{3d}{L}{\R^d}{\R}$ implies
	\begin{equation*}
		\phi_{1,\eta}\in \mn{\varrho}{2d(4N-1)}{3d}{L}{\R^d}{\R}.
	\end{equation*}
	
    Next, we will prove 
	\begin{equation*}
		\phi_{1,\eta}( \bmx) 
		\rightrightarrows
		\phi_{1}( \bmx)
		\quad \tn{as}\   \eta\to 0^+
		\quad \tn{for any $\bmx\in[0,1]^d$}.
	\end{equation*}	
     For each $\eta \in (0,1)$ and $\ell = 0, 1, \cdots, L$, let $\bmh_\ell$ and $\bmh_{\ell,\eta}$ denote the functions represented by the first $\ell$ hidden layers of the MMNNs corresponding to $\phi_1$ and $\phi_{1,\eta}$, respectively, i.e.,
	\begin{equation*}
		\bmh_\ell(\bmx)
		\coloneqq \calA_{\ell }\circ\ReLU\circ\calA_{\ell-1}\circ  \ \cdots \  \circ \ReLU\circ\calA_1\circ\ReLU\circ\calA_0(\bmx)\quad \tn{for any $\bmx\in\R^d$}
	\end{equation*}
	and 
	\begin{equation*}
		\bmh_{\ell,\eta}(\bmx)
		\coloneqq \calA_{\ell}\circ\sigma_\eta\circ\calA_{\ell-1}\circ  \ \cdots \  \circ \sigma_\eta\circ\calA_1\circ\sigma_\eta\circ\calA_0(\bmx)\quad \tn{for any $\bmx\in\R^d$}.
	\end{equation*}
	For  $\ell = 0, 1, \cdots, L$, we will prove by induction that	\begin{equation}\label{eq:induction:h:ell}
		\bmh_{\ell,\eta}(\bmx)\rightrightarrows \bmh_{\ell}(\bmx) \quad \tn{as}\    \eta\to 0^+ \quad \tn{for any $\bmx\in [0,1]^d$.}
	\end{equation}
	
	
	First, we consider the base case $\ell=0$. Clearly,
	\begin{equation*}
		\bmh_{0,\eta}(\bmx)=\calA_0(\bmx)= \bmh_{0}(\bmx)\rightrightarrows \bmh_{0}(\bmx) \quad \tn{as}\  \eta\to 0^+\quad \tn{for any $\bmx\in[0,1]^d$.}
	\end{equation*}
	This means Equation~\eqref{eq:induction:h:ell} holds for $\ell=0$.
	
	Next, supposing Equation~\eqref{eq:induction:h:ell} holds for $\ell=i\in \{0,1,\cdots,L-1\}$, our goal is to prove that it also holds for $\ell=i+1$. Determine $B>0$ via
	\begin{equation*}
		B=  \max \Big\{\|\bmh_j\|_{\sup([0,1]^d)}+1:
		 j=0,1,\cdots,L \Big\},
	\end{equation*}
	where the continuity of $\ReLU$ guarantees the above supremum is finite, i.e., $M\in [1,\infty)$.
	By the induction hypothesis, we have
	\begin{equation*}
		\bmh_{i,\eta}(\bmx)\rightrightarrows \bmh_{i}(\bmx) \quad \tn{as}\    \eta\to 0^+\quad \tn{for any $\bmx\in [0,1]^d$.}
	\end{equation*}
	Clearly, for any $\bmx\in [0,1]^d$, we have $\|\bmh_{i}(\bmx)\|_{\ell^\infty}\le B$ and  \begin{equation*}
	    \|\bmh_{i,\eta}(\bmx)\|_{\ell^\infty} \le \|\bmh_{i}(\bmx)\|_{\ell^\infty}+1\le  B\quad \tn{ for small $\eta>0$.}
	\end{equation*}
	
	Recall that $\sigma_\eta(t)\rightrightarrows \ReLU(t)$ as $\eta\to 0^+ $ for any $t\in [-B,B]$. Then, we have 
	\begin{equation*}
		\sigma_\eta\circ \bmh_{i,\eta}(\bmx)
		-\ReLU\circ \bmh_{i,\eta}(\bmx)
		\rightrightarrows \bmzero\quad \tn{as}\   \eta\to 0^+ \quad \tn{for any $\bmx\in [0,1]^d$.}
	\end{equation*}
	The continuity of $\ReLU$ implies
	the uniform continuity of $\ReLU$ on $[-B,B]$, from which we deduce
	\begin{equation*}
		\ReLU\circ \bmh_{i,\eta}(\bmx)
		-
		\ReLU\circ\bmh_{i}(\bmx) 
		\rightrightarrows \bmzero
		\quad \tn{as}\    \eta\to 0 ^+\quad \tn{for any $\bmx\in [0,1]^d$.}
	\end{equation*}
	
	
	
	Therefore, for any $\bmx\in [0,1]^d$, as $\eta\to 0^+ $, we have
	\begin{equation*}
		\sigma_\eta\circ \bmh_{i,\eta}(\bmx)
		-
		\ReLU\circ\bmh_{i}(\bmx) 
		=
		\underbrace{
			\sigma_\eta\circ \bmh_{i,\eta}(\bmx)
			-\ReLU\circ \bmh_{i,\eta}(\bmx)
		}_{\rightrightarrows \bmzero}
		+
		\underbrace{\ReLU\circ \bmh_{i,\eta}(\bmx)
			-
			\ReLU\circ\bmh_{i}(\bmx) 
		}_{\rightrightarrows \bmzero}
		\rightrightarrows \bmzero,
	\end{equation*}
	from which we deduce
	\begin{equation*}
		\bmh_{i+1,\eta}(\bmx)=\calA_{i+1}\circ\sigma_\eta\circ \bmh_{i,\eta}(\bmx)
		\rightrightarrows
		\calA_{i+1}\circ\ReLU\circ \bmh_{i}(\bmx)=\bmh_{i+1}(\bmx).
	\end{equation*}
	This means Equation~\eqref{eq:induction:h:ell} holds for $\ell=i+1$. So we complete the inductive step.
	
	
	By the principle of induction,  we have
	\begin{equation*}
		\phi_{1,{\eta}}(\bmx) =\bmh_{L ,\eta}(\bmx)
		\rightrightarrows
		\bmh_{L }(\bmx)=
		\phi_{1}(\bmx)
		\quad \tn{as}\   \eta\to 0^+ 
		\quad \tn{for any $\bmx\in[0,1]^d$}.
	\end{equation*}

Next, we consider  replacing \(\sine\) in $\phi_2$ with \(\varrho\). 
Since $\varrho\in\calS$, there exists \( s\in\R \) such that \(\varrho(x) = \sin(x)\) for all \( x \ge s \).
Since $\phi_1$ is bounded on $[0,1]^d$, there exists a sufficiently large integer \( m \in \mathbb{Z} \) such that
\[
    2m\pi  + w\cdot \phi_1(\bmx) \ge s 
    \quad \text{and} \quad
    2m\pi  + v \cdot \sin\big(w\cdot \phi_1(\bmx)\big)  \ge s\quad \tn{for any $\bmx\in[0,1]^d$},
\]
from which we deduce
\begin{equation*}
% \label{eq:sinsin:approx:f(xk):eps}
\begin{split}
    &\phantom{=\;\;}\sin\Big(v \cdot \sin\big(w\cdot \phi_1(\bmx)\big)\Big)
    = \sin\Big(\underbrace{2m \pi + v \cdot \sin\big(w\cdot \phi_1(\bmx)\big)}_{\ge\; s}\Big)
    = \varrho\Big(2m \pi + v \cdot \sin\big(w\cdot \phi_1(\bmx)\big) \Big)
    \\ & = \varrho\Big(2m \pi + v \cdot \sin\big(\underbrace{2m \pi +  w\cdot \phi_1(\bmx)}_{\ge \; s}\big) \Big)
    = \varrho\Big(2m \pi + v \cdot \varrho\big( 2m \pi +   w\cdot \phi_1(\bmx) \big) \Big).
\end{split}
\end{equation*}
Then by defining 
\begin{equation*}
   \tildephi_2(y)
   \coloneqq 
   u\cdot \varrho\Big(2m \pi + v \cdot \varrho\big( 2m \pi +   w y \big) \Big)\quad \tn{for any $y\in\R $,}
\end{equation*}
for any $\bmx\in[0,1]^d$, we have
\begin{equation}
\label{eq:tildephi2:to:phi2}
\begin{split}
\tildephi_2\circ \phi_1(\bmx)
&= u\cdot
\varrho\Big(2m \pi + v \cdot \varrho\big( 2m \pi +   w\cdot \phi_1(\bmx) \big) \Big)
\\&=u\cdot \sin\Big(v \cdot \sin\big(w\cdot \phi_1(\bmx)\big)\Big)
    = \phi_2\cdot\phi_1(\bmx).
\end{split}
\end{equation}

Recall that
\[|\phi_{1,{\eta}}(\bmx) |\le |\phi_1(\bmx)|+1\le B\quad \tn{ for small $\eta>0$ and any $\bmx\in [0,1]^d$}.\]
The continuity of $\tildephi_2$ implies
	the uniform continuity of $\tildephi_2$ on $[-B,B]$, from which we deduce
	\begin{equation*}
		\tildephi_2\circ \phi_{1,{\eta}}(\bmx) 
		\rightrightarrows		
		\tildephi_2\circ\phi_{1}(\bmx)
		\quad \tn{as}\   \eta\to 0 
		\quad \tn{for any $\bmx\in[0,1]^d$}.
	\end{equation*}
    Then we can choose sufficiently small $\eta_0>0^+$ such that
    \begin{equation}
    \label{eq:tildephi2:phi1:error}
        \|\tildephi_2\circ \phi_{1,{\eta_0}}-		
		\tildephi_2\circ\phi_{1}\|_{L^p([0,1]^d)}\le \tfrac{1}{10}\omega_f(\tfrac{\sqrt{d}}{M}).
    \end{equation}
Now we can define the desired $\phi$ for Theorem~\ref{thm:main1} via $\phi\coloneqq \tildephi_2\circ \phi_{1,{\eta_0}}$.

 By Equations~\eqref{eq:phi2:phi1:-f}, \eqref{eq:tildephi2:to:phi2}, and \eqref{eq:tildephi2:phi1:error},
 we have
\begin{equation*}
\begin{split}
% \| \phi -f\|_{L^p([0,1]^d)}
% =
\| \tildephi_2\circ \phi_{1,{\eta_0}} -f\|_{L^p([0,1]^d)}
&\le \| \tildephi_2\circ \phi_{1,{\eta_0}} -\tildephi_2\circ \phi_{1 }\|_{L^p([0,1]^d)}+\| \tildephi_2\circ \phi_{1 } -f\|_{L^p([0,1]^d)}
\\&
\le \tfrac{1}{10}\omega_f(\tfrac{\sqrt{d}}{M})+\| \phi_2\circ \phi_1-f\|_{L^p([0,1]^d)}
\\& \le \tfrac{1}{10}\omega_f(\tfrac{\sqrt{d}}{M}) + \tfrac{11}{10}\omega_f(\tfrac{\sqrt{d}}{M})=\tfrac{6}{5}\omega_f(\tfrac{\sqrt{d}}{M}).
\end{split}
\end{equation*}
Recall that  $\omega_f(n\cdot  t)\le n\cdot \omega_f(t)$ for any $n\in\N^+$ and $t\in [0,\infty)$. Thus, we have
	\begin{equation*}
	\begin{split}
	\|  \tildephi_2\circ \phi_{1,{\eta_0}} -f\|_{L^p([0,1]^d)}
 \le  \tfrac{6}{5}\omega_f(\tfrac{\sqrt{d}}{M}) \le \tfrac{6}{5}\omega_f(\tfrac{\lceil\sqrt{d}\rceil}{M}) 
 \le   \tfrac{6}{5}\big\lceil\sqrt{d}\big\rceil\cdot\omega_f(\tfrac{1}{M}) 
 \le 2\sqrt{d}\cdot\omega_f(N^{-L}),
	\end{split}
	\end{equation*}
where the last inequality follows from \(M = N^L\) and the fact that \(\frac{6}{5} \lceil \sqrt{n} \, \rceil \le 2 \sqrt{n}\) for any \(n \in \mathbb{N}^+\).
Recall that $\tildephi_2(y)
   \coloneqq 
   u\cdot \varrho\Big(2m \pi + v \cdot \varrho\big( 2m \pi +   w y \big) \Big)$
   for any $y\in\R$
   and 
	\begin{equation*}
		\phi_{1,\eta_0}\in \mn{\varrho}{2d(4N-1)}{3d}{L}{\R^d}{\R}.
	\end{equation*}
    We conclude that
 	\begin{equation*}
		\tildephi_2\circ \phi_{1,\eta_0}\in \mn{\varrho}{2d(4N-1)}{3d}{L+2}{\R^d}{\R},
	\end{equation*}   
which completes the proof of Theorem~\ref{thm:main1}.

% \begin{proof}[Proof of Theorem~\ref{thm:main2}]
% Finally, we define $\phi\coloneqq \phi_2\circ \phi_1$ where
% \begin{equation*}
%     \phi_1(\bmy)\coloneqq w \cdot g\circ \bmPsi(\bmy)\quad \tn{for any $\bmy\in \R^d$}
% \end{equation*}
% and
% \begin{equation*}
%     \phi_2(z) = u \cdot \sin\big(v \cdot \sin(z)\big)
%     \quad \tn{for any $z\in \R$.}
% \end{equation*}
% It is clear that 
% \(\psi \in \mn[\big]{\ReLU}{4N-1}{3}{L}{\mathbb{R}}{\mathbb{R}}\)
% implies
% \({\bm{\Psi}} \in \mn[\big]{\ReLU}{d(4N-1)}{3d}{L}{\mathbb{R}^d}{\mathbb{R}^d}\).
% Recall that \(w\cdot  g\) is an affine linear map. By combining \(w\cdot  g\) and the final affine linear map into a new one, we have 
% \[\phi_1 = w \cdot  g \circ {\bm{\Psi}} \in \mn[\big]{\ReLU}{d(4N-1)}{3d}{L}{\mathbb{R}^d}{\mathbb{R}^d}.\]
% Therefore, 
% \[\phi=\phi_2\circ\phi_1 = w \cdot  g \circ {\bm{\Psi}} \in \mn[\big]{(\sin,\ReLU)}{d(4N-1)}{3d}{L+2}{\mathbb{R}}{\mathbb{R}^d}.\]

% It remains to estimate the approximation error. For any \(\bm{x} \in Q_k\) and \(k \in \{1, 2, \cdots, M^d\}\), by Equations~\eqref{eq:g:tildePsi:x:to:k} and \eqref{eq:sinsin:approx:f(xk):eps}, we have
% \begin{equation*}
%     \begin{split}
%         |\phi(\bmx)-f(\bmx_k)|
%         =|\phi_2\circ \phi_1(\bmx)-f(\bmx_k)|  
%         &=\Big|\phi_2\Big(w
%         \cdot\underbrace{g\circ\bmtildePhi(\bmx)}_{\tn{$=k$ by \eqref{eq:g:tildePsi:x:to:k}}}
%         \Big)-f(\bmx_k)\Big|        
%         = |\phi_2 (w \cdot  k )-f(\bmx_k) |
%         \\ &
%         =|\phi_2 (kw )-f(\bmx_k) |
%          =\left| u \cdot \sin\big(v \cdot \sin(k w)\big) - f(\bmx_k) \right| < \epsilon
%     \end{split}
% \end{equation*}
% and $\|\bmx_k-\bmx\|_2\le \tfrac{\sqrt{d(1-\delta)}}{M}\le \frac{\sqrt{d}}{M}$,
% from which we deduce
% \begin{equation*}
%     \begin{split}
%         |\phi(\bmx)-f(\bmx)|
%          \le 
%         |\phi(\bmx)-f(\bmx_k)|
%         +|f(\bmx_k)-f(\bmx)|
%         \le \eps + \omega_f(\tfrac{\sqrt{d}}{M}).
%     \end{split}
% \end{equation*}
% Recall that 
% \begin{equation*}
%     \bigcup_{k\in \{0,1,\cdots,M^d-1\}} Q_{k} = [0,1]^d \setminus \Omega\quad \tn{ and}  \quad
%   \|\phi\|_{L^\infty([0,1]^d)}=\|\phi_2 \circ \phi_1\|_{L^\infty([0,1]^d)} \le \|\phi_2\|_{L^\infty(\mathbb{R})}  \le |u|,
% \end{equation*}
% where \(u\) is a constant determined by \(f\) and is independent of \(\delta\).
% Therefore
% 	\begin{equation*}
% 	\begin{split}
% 	\| \phi-f\|_{L^p([0,1]^d)}^p
% 	&=\int_{\Omega}| \phi(\bmx)-f(\bmx)|^p\tn{d}\bmx
%     +
%     \int_{[0,1]^d\backslash\Omega}|\phi(\bmx)-f(\bmx)|^p\tn{d}\bmx
%  \\
%  	&\le 
%  \mu(\Omega)\cdot \| \phi-f\|_{L^\infty(\Omega)}^p
%  + \sum_{k=1}^{M^d}\int_{Q_k}| \phi(\bmx)-f(\bmx)|^p\tn{d}\bmx
% 	\\&\le 
%  \mu(\Omega)\cdot \Big(|u|+\|f\|_{L^\infty([0,1]^d)}\Big)^p
%  + \sum_{k=1}^{M^d}
%  \mu(Q_k)\cdot\Big(\eps + \omega_f(\tfrac{\sqrt{d}}{M})\Big)^p
%  \\&\le 
%  \mu(\Omega)\cdot \Big(|u|+\|f\|_{L^\infty([0,1]^d)}\Big)^p
%  + \Big(\eps + \omega_f(\tfrac{\sqrt{d}}{M})\Big)^p
%  \le \Big(\tfrac{11}{10}\omega_f(\tfrac{\sqrt{d}}{M})\Big)^p
% 	\end{split}
% 	\end{equation*}
% where the last inequality is achieved by setting
% \begin{equation*}
%     \eps = \tfrac{1}{11}\omega_f(\tfrac{\sqrt{d}}{M})>0
% \end{equation*}
% and choosing a sufficiently small \(\delta \in (0,1)\) to make \(\mu(\Omega)\) small enough, ensuring that
% \begin{equation*}
%     \mu(\Omega)\cdot \Big(|u|+\|f\|_{L^\infty([0,1]^d)}\Big)^p
%     \le  \Big(\tfrac{11}{10}\omega_f(\tfrac{\sqrt{d}}{M})\Big)^p - \Big(\tfrac{12}{11}\omega_f(\tfrac{\sqrt{d}}{M})\Big)^p
%     % =\Big( \big(\tfrac{11}{10})^p-\big(\tfrac{12}{11}\big)^p\Big)\cdot \Big(\omega_f(\tfrac{\sqrt{d}}{M})\Big)^p
%     >0.
% \end{equation*}
% We note that the condition \(\omega_f\big(\frac{\sqrt{d}}{M}\big) > 0\) can be satisfied; otherwise, \(f\) would be a constant function, which is a trivial case.

% Recall that  $\omega_f(n\cdot  t)\le n\cdot \omega_f(t)$ for any $n\in\N^+$ and $t\in [0,\infty)$. Thus, we have
% 	\begin{equation*}
% 	\begin{split}
% 	\| \phi-f\|_{L^p([0,1]^d)}
%  \le  \tfrac{11}{10}\omega_f(\tfrac{\sqrt{d}}{M}) \le \tfrac{11}{10}\omega_f(\tfrac{\lceil\sqrt{d}\rceil}{M}) 
%  \le   \tfrac{11}{10}\big\lceil\sqrt{d}\big\rceil\cdot\omega_f(\tfrac{1}{M}) 
%  \le 2\sqrt{d}\cdot\omega_f(N^{-L}),
% 	\end{split}
% 	\end{equation*}
% where the last inequality follows from \(M = N^L\) and the fact that \(\frac{11}{10} \lceil \sqrt{n} \, \rceil \le 2 \sqrt{n}\) for any \(n \in \mathbb{N}^+\). This completes the proof of Theorem~\ref{thm:main}.
% \end{proof}


\section{Proofs of Propositions in Section~\ref{sec:proof:ideas:thm:main}}
\label{sec:proof:props}

% \section{Proofs of Propositions}

In this section, we present the detailed proofs of all propositions stated in Section~\ref{sec:proof:ideas:thm:main}. Specifically, the proofs of Propositions~\ref{prop:floor:approx}, \ref{prop:k:to:yk}, and \ref{prop:approx:ReLU} are provided in Sections~\ref{sec:proof:prop:floor:approx}, \ref{sec:proof:prop:k:to:yk}, and \ref{sec:proof:prop:approx:ReLU}, respectively.


\subsection{Proof of Proposition~\ref{prop:floor:approx}}
\label{sec:proof:prop:floor:approx}

We will prove Proposition~\ref{prop:floor:approx}, which demonstrates the efficiency of \ReLU\ FCNNs in approximating the floor function. The core idea is to use compositions of continuous piecewise functions to approximate the floor function effectively. To simplify the proof, we introduce a lemma below, which shows that continuous piecewise functions can be exactly represented by one-hidden-layer \ReLU\ FCNNs.


\begin{lemma}\label{lem:cpwl(n):in:nn}
	For any $n\in \N^+$, it holds that
	\begin{equation}\label{eq:cpl:subset:nn}
		\cpl\big(n\big)\subseteq  \nn[\big]{\ReLU}{n+1}{1}{\R}{\R}.
	\end{equation}
\end{lemma}
\begin{proof}%[Proof of Lemma~\ref{lem:cpwl(n):in:nn}]
We proceed by mathematical induction to prove Equation~\eqref{eq:cpl:subset:nn}. We begin with the base case \( n = 1 \).
For any \( f \in \cpl\big(1\big) \), there exist \( a_1, a_2, x_0 \in \mathbb{R} \) such that
\begin{equation*}
    f(x) = 
    \begin{cases}
                a_1(x - x_0) + f(x_0)  & \text{if } x \ge x_0,\\
        a_2(x_0 - x) + f(x_0)  & \text{if } x < x_0.
    \end{cases}
\end{equation*}
Thus, we can express \( f(x) \) as \( f(x) = a_1 \sigma(x - x_0) + a_2 \sigma(x_0 - x) + f(x_0) \) for any \( x \in \mathbb{R} \), which implies
\(f \in \nn[\big]{\ReLU}{2}{1}{\mathbb{R}}{\mathbb{R}}.\)
Therefore, Equation~\eqref{eq:cpl:subset:nn} holds for \( n = 1 \).

Now, suppose Equation~\eqref{eq:cpl:subset:nn} holds for \( n = k \in \mathbb{N}^+ \). We aim to show that it also holds for \( n = k + 1 \).
For any \( f \in \cpl\big(k+1\big) \), we assume without loss of generality that \( f \) has a largest breakpoint at \( x_0 \) (the case where \( f \) has no breakpoints is trivial). 
% Let \( a_1 \) and \( a_2 \) denote the slopes of the linear segments immediately to the left and right of \( x_0 \), respectively. Define
% \[
% \tilde{f}(x) \coloneqq f(x) - (a_2 - a_1) \sigma(x - x_0)\quad \text{for any } x \in \mathbb{R}.
% \]
% % That is ,  $\tildef$ is attained  by removing the breakpoint $x_0$ of $f$.
% % Then \( \tilde{f} \) has at most \( k \) breakpoints. 
% Then, for $\tildef$, the slopes of the linear segments immediately to the left and right of \( x_0 \) both $a_1$.
% In other words, \( \tilde{f} \) is obtained by removing the breakpoint \( x_0 \) of \( f \). Consequently, \( \tilde{f} \) has at most \( k \) breakpoints.
Let \( a_1 \) and \( a_2 \) represent the slopes of the linear segments directly to the left and right of \( x_0 \), respectively. Define
\[
\tilde{f}(x) \coloneqq f(x) - (a_2 - a_1) \sigma(x - x_0) \quad \text{for any } x \in \mathbb{R}.
\]
With this construction, \( \tilde{f} \) has slope \( a_1 \) on both sides of \( x_0 \), effectively smoothing out the breakpoint at \( x_0 \) in \( f \). Thus, \( \tilde{f} \) is obtained by eliminating this breakpoint, leaving it with at most \( k \) breakpoints.
By the induction hypothesis, we know that
\[
\tilde{f} \in \cpl\big(k\big) \subseteq \nn[\big]{\ReLU}{k+1}{1}{\mathbb{R}}{\mathbb{R}}.
\]
Thus, there exist constants \( u_j,v_j,w_j,c \) for \( j = 1, 2, \dots, k+1 \) such that
\[
\tilde{f}(x) = \sum_{j=1}^{k+1} u_{j} \sigma(v_{j} x + w_{j}) + c \quad \text{for any } x \in \mathbb{R}.
\]
Therefore, for any \( x \in \mathbb{R} \), we can write
\begin{equation*}
    f(x) = (a_2 - a_1) \sigma(x - x_0) + \tilde{f}(x) = (a_2 - a_1) \sigma(x - x_0) + \sum_{j=1}^{k+1} u_{j} \sigma(v_{j} x + w_{j}) + c,
\end{equation*}
implying that \( f \in \nn[\big]{\ReLU}{k+2}{1}{\mathbb{R}}{\mathbb{R}} \).
Thus, Equation~\eqref{eq:cpl:subset:nn} holds for \( k + 1 \), completing the induction process and, hence, the proof of Lemma~\ref{lem:cpwl(n):in:nn}.
\end{proof}

% \begin{proof}%[Proof of Lemma~\ref{lem:cpwl(n):in:nn}]
% We will use mathematical induction to prove Equation~\eqref{eq:cpl:subset:nn}. First, we examine the case \( n = 1 \).
% Given any $f \in \cpl\big(1\big)$, there exist $a_1,a_2,x_0\in\R$ such that
% \begin{equation*}
% 	f(x)=\left\{
% 	\begin{array}{ll}
% 		a_1(x-x_0)+f(x_0), &\tn{if \  }  x\ge x_0,\\
% 		a_2(x_0-x)+f(x_0), &\tn{if \  }  x< x_0.\\
% 	\end{array}
% \right.
% \end{equation*}
% Thus, $f(x)=a_1\sigma(x-x_0)+a_2\sigma(x_0-x)+f(x_0)$ for any $x\in \R$, implying 
% \[f \in \nn[\big]{\ReLU}{2}{1}{\R}{\R}.\]
% Thus, Equation~\eqref{eq:cpl:subset:nn} holds for $n=1$.

% Now assume Equation~\eqref{eq:cpl:subset:nn} holds for $n=k\in \N^+$, we would like to show it is also  true for $n=k+1$.
% Given any $f \in \cpl\big(k+1\big)$, we may assume the biggest breakpoint of $f$ is $x_0$ since it is trivial for the case that $f$ has no breakpoint. Denote the slopes of the linear pieces left and right next to $x_0$ by $a_1$ and $a_2$, respectively.
% Define 
% \[\tildef(x)\coloneqq f(x)- (a_2-a_1)\sigma(x-x_0),\quad \tn{ for any $x\in\R$.}\]
%  Then $\tildef$ 
%  has at most $k$ breakpoints.
% By the induction hypothesis, we have
% \begin{equation*}
% 	\tildef \in \cpl\big(k\big)\subseteq \nn[\big]{\ReLU}{k+1}{1}{\R}{\R}.
% \end{equation*}
% Thus, there exist   $w_{0,j},b_{0,j},w_{1,j},b_1$ for $j=1,2,\cdots,k+1$  such that
% \begin{equation*}
% 	\tildef(x)=\sum_{j=1}^{k+1} w_{1,j}\sigma(w_{0,j}x+b_{0,j})+b_1,\quad \tn{for any $x\in\R$.}
% \end{equation*}
%  Therefore, for any $x\in\R$, we have
% \begin{equation*}
% 		f(x)=(a_2-a_1)\sigma(x-x_0)+\tildef(x)
% 		= 	(a_2-a_1)\sigma(x-x_0)
% 				+\sum_{j=1}^{k+1} w_{1,j}\sigma(w_{0,j}x+b_{0,j})+b_1,
% \end{equation*}
% implying $f\in \nn[\big]{\ReLU}{k+1}{1}{\R}{\R}$.
% Thus, Equation~\eqref{eq:cpl:subset:nn} holds for $k+1$, which means we finish the induction process. So we complete the proof of Lemma~\ref{lem:cpwl(n):in:nn}.
% \end{proof}

With Lemma~\ref{lem:cpwl(n):in:nn} established, we are now ready to prove Proposition~\ref{prop:floor:approx}.

\begin{proof}[Proof of Proposition~\ref{prop:floor:approx}]
Given any $x\in \bigcup_{k=0}^{N^L-1}\big[k,\, k+1-\delta\big]$, our goal is to construct $\phi$, realized by a network with desired size, mapping $x$ to $\lfloor x\rfloor$.
Clearly  $\lfloor x\rfloor\in \{0,1,\cdots,N^L-1\}$ and hence there exists unique $(n_1,n_2,\cdots,n_L)\in \{0,1,\cdots,N-1\}^L$ such that
    \begin{equation}\label{eq:floor:x:decomposition}
        \lfloor x\rfloor= \sum_{i=1}^L n_i\cdot N^{L-i}.
    \end{equation}
In other words, the above equation forms a one-to-one map between $\{0,1,\cdots,N^L-1\}$ and $\{0,1,\cdots,N-1\}^L$.

        For $\ell=0,1,\cdots,L$, we define
    \begin{equation*}
    m_\ell = \sum_{i=\ell+1}^{L}
    n_i\cdot N^{L-i}\quad \tn{and}\quad
        z_\ell = x- \sum_{i=1}^{\ell}
    n_i\cdot N^{L-i}.
\end{equation*}
Clearly, $z_0=x$,
\begin{equation*}
\begin{split}
    z_\ell
    =
    x - \lfloor x\rfloor + \lfloor  x\rfloor - \sum_{i=\ell+1}^{L}
    n_i\cdot N^{L-i} 
   & =
   x - \lfloor x\rfloor + \sum_{i=1}^{L}
    n_i\cdot N^{L-i} 
    - \sum_{i=\ell+1}^{L}
    n_i\cdot N^{L-i} 
   \\& =x - \lfloor x\rfloor + \sum_{i=1}^{\ell}
    n_i\cdot N^{L-i} 
   =x - \lfloor x\rfloor + m_\ell,
\end{split}
\end{equation*}
and
\begin{equation*}
\begin{split}
        \lfloor z_\ell\rfloor
    =
    \Big\lfloor 
    \underbrace{x - \lfloor x\rfloor}_{\in\, [0,1)} + \underbrace{m_\ell}_{\in\, \bbZ}
    \Big\rfloor
   & =
    m_\ell
\end{split}
\end{equation*}
for $\ell=0,1,\cdots,L$.
% \begin{equation*}
% \begin{split}
%         \lfloor z_\ell\rfloor
%     =
%     \bigg\lfloor x- \underbrace{\sum_{i=\ell+1}^{L}
%     n_i\cdot N^{L-i}}_\tn{integer}\bigg\rfloor
%    & =
%     \left\lfloor x\right\rfloor
%     - \sum_{i=\ell+1}^{L}
%     n_i\cdot N^{L-i} 
%     \\ &= \sum_{i=1}^{L}
%     n_i\cdot N^{L-i} 
%     - \sum_{i=\ell+1}^{L}
%     n_i\cdot N^{L-i} =m_\ell,
% \end{split}
% \end{equation*}
% for $\ell=0,1,\cdots,L$.
We claim 
\begin{equation}\label{eq:z:ell:lower:upper:bounds}
    n_{\ell+1} \le \frac{z_\ell}{N^{L-\ell-1}}\le n_{\ell+1}+1 -\frac{\delta}{N^{L-\ell-1}}\quad \tn{for $\ell=0,1,\cdots,L-1$}.
\end{equation}
To demonstrate this, we first establish the lower bound. Clearly,
\begin{equation*}
    \frac{z_\ell}{N^{L-\ell}}
    =\frac{x - \lfloor x\rfloor + m_\ell}{N^{L-\ell-1}}
    \ge \frac{ m_\ell}{N^{L-\ell-1}}
    = \frac{ \sum_{i=\ell+1}^{L}
    n_i\cdot N^{L-i}}{N^{L-\ell-1}}
    \ge \frac{ 
    n_{\ell+1}\cdot N^{L-(\ell+1)}}{N^{L-\ell-1}}
    =  n_{\ell+1}.
\end{equation*}
Next, we proceed to verify the upper bound. Clearly, 
\begin{equation*}
\begin{split}
        \frac{z_\ell}{N^{L-\ell-1}}
    = \frac{x - \lfloor x\rfloor + m_\ell}{N^{L-\ell-1}}
    &= \frac{x - \lfloor x\rfloor +\delta + m_\ell}{N^{L-\ell-1}}
   - \frac{\delta}{N^{L-\ell-1}}
   \\& \le \frac{1 + m_\ell}{N^{L-\ell-1}}
   - \frac{\delta}{N^{L-\ell-1}}
    \le  n_\ell+1-\frac{\delta}{N^{L-\ell}},
\end{split}
\end{equation*}
where the last inequality come from
\begin{equation*}
    \begin{split}
        \frac{1 + m_\ell}{N^{L-\ell-1}}
    =\frac{1 + \sum_{i=\ell+1}^{L}
    n_i\cdot N^{L-i}}{N^{L-\ell-1}}
    & =\frac{1 + \sum_{i=\ell+2}^{L}
    n_i\cdot N^{L-i}+ n_{\ell+1}\cdot N^{L-(\ell+1)}}{N^{L-\ell-1}}
   \\&  \le 
   \frac{1 + \sum_{i=\ell+2}^{L}
    (N-1)\cdot N^{L-i}+ n_{\ell+1}\cdot N^{L-(\ell+1)}}{N^{L-\ell-1}}
    \\&  =
   \frac{N^{L-\ell-1} + n_{\ell+1}\cdot N^{L-(\ell+1)}}{N^{L-\ell-1}}
    =  n_\ell+1.
    \end{split}
\end{equation*}
Thus, we complete the proof of Equation~\eqref{eq:z:ell:lower:upper:bounds}.

Let $h$ be a continuous piecewise linear function with $h\in \cpwl(2N-2)$ and
\begin{equation*}
    h(k)=h\Big(k+1-\frac{\delta}{N^{L-1}}\Big)=k\quad \tn{for $k=0,1,\cdots,N-1$}.
\end{equation*}

\begin{figure}[ht]
    \centering   \includegraphics[width=0.55\textwidth]{figures/h.pdf}
\caption{An illustration of $h$ when $N=5$.}
    \label{fig:h}
\end{figure}

See Figure~\ref{fig:h} for an illustration of $h$ when $N=5$.
Obviously, 
\begin{equation*}
    h(t)=k\quad \tn{if $t\in \Big[k,\; k+1-\frac{\delta}{N^{L-1}}\Big]$}\quad \tn{for $k=0,1,\cdots,N-1$.}
\end{equation*}
% $h(t)=k$ if $t\in [k, k+1-\delta/N^{L-1}]$ for $k=0,1,\cdots,N-1$. Moreover,
For $\ell=0,1,\cdots,L-1$, we have $n_{\ell+1}\in \{0,1,\cdots,N-1\}$ and Equation~\eqref{eq:z:ell:lower:upper:bounds} implies 
\begin{equation*}
     \frac{z_\ell}{N^{L-\ell-1}}\in
     \Big[n_{\ell+1},\; n_{\ell+1}+1 -\frac{\delta}{N^{L-\ell-1}}\Big]
     \subseteq 
     \Big[n_{\ell+1},\; n_{\ell+1}+1 -\frac{\delta}{N^{L-1}}\Big].
\end{equation*}
It follows that 
\begin{equation*}
    h\Big(\frac{z_\ell}{N^{L-\ell-1}}\Big)= n_{\ell+1}\quad \tn{for $\ell=0,1,\cdots,L-1$,}
\end{equation*}
from which we deduce
\begin{equation*}
    \begin{split}
        z_{\ell+1} = x- \sum_{i=1}^{\ell+1}
    n_i\cdot N^{L-i}
    &= x- \sum_{i=1}^{\ell}
    n_i\cdot N^{L-i}-n_{\ell+1} \cdot N^{L-(\ell+1)}
    \\& = z_\ell - n_{\ell+1} \cdot N^{L-(\ell+1)}
    = z_\ell - h\Big(\frac{z_\ell}{N^{L-\ell-1}}\Big) \cdot N^{L-\ell-1}.   
    \end{split}
\end{equation*}
Therefore, by defining 
\begin{equation*}
    h_\ell(z) =  h\Big(\frac{z}{N^{L-\ell-1}}\Big)\quad \tn{and}\quad 
    \tildeh_\ell(z) =  z- h\Big(\frac{z}{N^{L-\ell-1}}\Big)\cdot N^{L-\ell-1}\quad \tn{for any $z\in\R$,}
\end{equation*}
we have 
\begin{equation}
\label{eq:z:ell:to:n:z:ell+1}
    h_\ell(z_\ell) = n_{\ell+1}\quad \tn{and}\quad 
    \tildeh_\ell(z_\ell) =  z_{\ell+1}\quad \tn{for $\ell=0,1,\cdots,L-1$.}
\end{equation}
Moreover, $h\in \cpwl(2N-2)$ implies
$h_\ell,\tildeh_\ell \in \cpwl(2N-2)$
for $\ell=0,1,\cdots,L-1$. Then by Lemma~\ref{lem:cpwl(n):in:nn}, 
\begin{equation}\label{eq:h:ell:tildeh:ell:in:NN}
    h_\ell,\tildeh_\ell \in \cpwl(2N-2)\subseteq \nn[\big]{\ReLU}{2N-1}{1}{\R}{\R}.
\end{equation}
This means that \( h_\ell \) and \( \tilde{h}_\ell \) can be implemented by one-hidden-layer \ReLU\ FCNNs of width \( 2N-1 \).

Consequently, the desired function \( \phi \) can be realized by a \ReLU\ MMNN of width \( 1 + (2N - 1) + (2N - 1) = 4N - 1 \), rank $3$, and depth \( L \), as illustrated in Figure~\ref{fig:floorApprox}. That is,
\[\phi \in \mn[\big]{\ReLU}{4N-1}{3}{L}{\R}{\R},\]
which completes the proof of Proposition~\ref{prop:floor:approx}.
% Consequently, the desired function \( \phi \) can be realized by a \ReLU\ FCNN with width \( 1 + (2N - 1) + (2N - 1) = 4N - 1 \) and depth \( L \), as illustrated in Figure~\ref{fig:floorApprox}. This completes the proof of Proposition~\ref{prop:floor:approx}.
\end{proof}
\begin{figure}[ht]
    \centering   \includegraphics[width=0.975\textwidth]{figures/floorApprox.pdf}
\caption{The target network implementing $\phi$ based on Equations~\eqref{eq:floor:x:decomposition}, \eqref{eq:z:ell:to:n:z:ell+1}, and \eqref{eq:h:ell:tildeh:ell:in:NN}. In this architecture, redundant (fake) layers can be removed by merging the preceding and succeeding affine linear transformations into a single one.}
    \label{fig:floorApprox}
\end{figure}
%     \begin{equation*}
%         h_{L-1}(x)\coloneqq  x-N^{L-1}\cdot  g(x/N^{L-1})
%     \end{equation*}
% %     \begin{equation*}
% %     z_\ell = x- \sum_{i=\ell+1}^{L}
% %     n_i\cdot N^{L-i}
% % \end{equation*}



% \begin{equation*}
%     h_\ell(z)=h(z/N^{L-\ell})
% \end{equation*}
% \begin{equation*}
%     \tildeh_\ell(z)=z- N^{L-\ell}\cdot  h(z/N^{L-\ell})
% \end{equation*}
%



\subsection{Proof of Proposition~\ref{prop:k:to:yk}}
\label{sec:proof:prop:k:to:yk}


We will establish the proof of Proposition~\ref{prop:k:to:yk}. To facilitate this, we introduce two key lemmas that serve as intermediate steps in proving Proposition~\ref{prop:k:to:yk}. 
The first lemma demonstrates how to use the \sine{}  function with a single parameter to generate rationally independent numbers. The second lemma shows the density of point sets generated by the \sine{}  function combined with rational numbers within a high-dimensional hypercube.



\begin{lemma}
    \label{lem:rationally:independent}
	Given $K\in\bbN^+$, there exists $w_0\in \big(-\tfrac{1}{K}, \tfrac{1}{K}\big)$ such that $\sin (k w_0  )$, for $k=1,2,\cdots,K$, are rationally independent.
\end{lemma}

\begin{lemma}\label{lem:dense:sin}
	Given any rationally independent numbers $a_1,a_2,\cdots,a_K$, for any $K\in\N^+$, the following set 
	\begin{equation*}
		\Big\{\Big(\sin(wa_1),\, \sin(wa_2),\, \cdots,\,\sin(wa_K)\Big) : w\in\R
		\Big\}
	\end{equation*}
is dense in $[-1,1]^K$.
\end{lemma}


We are now prepared to prove Proposition~\ref{prop:k:to:yk}, assuming the validity of Lemmas~\ref{lem:rationally:independent} and \ref{lem:cpwl(n):in:nn}, which will be proven in Sections~\ref{sec:proof:lem:rationally:independent} and \ref{sec:proof:lem:dense:sin}, respectively.



\begin{proof}[Proof of Proposition~\ref{prop:k:to:yk}]
% Given any  $\eps>0$ and $y_k\in \R$ for $k=1,2,\cdots,K$, we define
% \begin{equation*}
%     u\coloneqq \max\big\{|y_k|: k=1,2,\cdots,K\big\}.
% \end{equation*}
% We may assume $u>0$ as the case $u=0$ is traivial. we set $z_k=y_k/u \le  1$, then
% by Lemma~\ref{lem:rationally:independent}, there exists $w_0$ such that
% $a_k=\sin (k w_0  )$, for $k=1,2,\cdots,K$, are rationally independent. 
Given any \(\epsilon > 0\) and \(y_k \in \mathbb{R}\) for \(k = 1, 2, \cdots, K\), we define
\begin{equation*}
    u \coloneqq \max\big\{|y_k| : k = 1, 2, \cdots, K\big\}.
\end{equation*}
Assuming \(u > 0\) (the case \(u = 0\) is trivial), we set \(z_k = y_k / u \leq 1\). Then, by Lemma~\ref{lem:rationally:independent}, there exists \(w\) such that 
\(a_k = \sin(k w)\) for \(k = 1, 2, \cdots, K\) are rationally independent.
Moreover, by Lemma~\ref{lem:dense:sin}, 
the following set 
\begin{equation*}
    \Big\{\big[\sin(va_1),\, \sin(va_2),\, \cdots,\,\sin(va_K)\big]^\ts : v\in\R
    \Big\}
\end{equation*}
is dense in $[-1,1]^K$.
That is, there exists $v\in\R$ such that
\begin{equation*}
    \big|\sin(v a_k)-z_k\big|\le \eps/u \quad \tn{for $k=1,2,\cdots,K$.}
\end{equation*}
Therefore, for $k=1,2,\cdots,K$, we have
\begin{equation*}
\begin{split}
    \Big|u\cdot \sin\big(v \cdot \sin(kw)\big)-y_k\Big|
    &= u\Big| \sin\big(v \cdot \sin(kw)\big)-y_k/u\Big|\\
    &= u\big| \sin\big(v \cdot a_k\big)-z_k\big|
    <u\cdot \eps/u = \eps.
\end{split}
\end{equation*}
So we finish the proof of Proposition~\ref{prop:k:to:yk}.
\end{proof}



\subsubsection{Proof of Lemma~\ref{lem:rationally:independent}}
\label{sec:proof:lem:rationally:independent}
% {\color{red} We prove by construction. Let $p > 2\pi K > 2K$ be a sufficiently large prime number and let $w = 2\pi/p < \frac{1}{K}$, then $\{ e^{ik w} \}_{k=0}^{p-1}$ is the set of primitive roots of unity which forms a basis over $\mathbb{Q}$ of the $p$th cyclotomic field $\mathbb{Q}(e^{iw})$. Then if there are coefficients $a_k\in\mathbb{Q}$ that 
% \begin{equation*}
%     \sum_{k=1}^K a_k \sin(kw) = 0,
% \end{equation*}
% it implies 
% \begin{equation*}
% \sum_{k=1}^K a_k \left( e^{ikw} - e^{-ikw} \right) = \sum_{k=1}^K a_k e^{ikw} + \sum_{k=1}^{K} a_k e^{i(p - k) w} = 0.
% \end{equation*}
% Due to the linear independence over $\mathbb{Q}$, we must have $a_k = 0$. 
% }

% \begin{proof}[Proof of Lemma~\ref{lem:rationally:independent}]
We prove this lemma by contradiction. If it does not hold, then $\sin( k w )$, for $k=1,2,\cdots,K$, 
are rationally dependent for any $w\in \big(-\tfrac{1}{K}, \tfrac{1}{K}\big)=\calI$. That means, for all $w\in\calI$, there exists $\bmlambda=(\lambda_1,\cdots,\lambda_K)\in \bbQ^K\backslash\{\bmzero\}$ such that $\sum_{k=1}^K \lambda_k \sin(k w )=0$.
For each $\bmlambda\in \Q^K$, we define
\begin{equation*}
    \calI_\bmlambda \coloneqq 
    \left\{w \in \calI: \sum_{k=1}^K \lambda_k \sin(k w )=0\right\}.
\end{equation*}
It follows that
\begin{equation*}
    \calI=\bigcup_{\bmlambda\in\Q^K} \left\{w \in \calI: \sum_{k=1}^K \lambda_k \sin(k w )=0\right\}=\bigcup_{\bmlambda\in\Q^K} \calI_\bmlambda
\end{equation*}
% Recall that a countable union of countable sets is countable. 
% We observe that $\calI=\cup_{\bmlambda\in\Q^K} \calI_\bmlambda$ is uncountable and $\bbQ^K\backslash\{\bmzero\}$ is countable.
Recall that a countable union of countable sets remains countable. However, we observe that while
\(\mathbb{Q}^K \backslash \{\mathbf{0}\}\)
is countable, the union
\(\calI = \bigcup_{\bmlambda \in \mathbb{Q}^K} \calI_\bmlambda\)
is uncountable.
Then there exists $\bmlambda=(\lambda_1,\cdots,\lambda_K)\in \bbQ^K\backslash\{\bmzero\}$ such that
$\calI_\bmlambda$ is uncountable, i.e.,
% $\sum_{k=1}^K \lambda_k \sin(k w )=0$ for all $w$ in an uncountable subset of $\calI$.
\begin{equation*}
    \sum_{k=1}^K \lambda_k \sin(k w )=0\quad \tn{ for all $w\in \calI_\bmlambda$.}
\end{equation*} 
Define the function 
\[
g(w) \coloneqq \sum_{k=1}^K \lambda_k \sin\left(k w\right) \quad \text{for all } w \in \mathcal{I}.
\]
The real analyticity of \(\sine\) ensures that \(g\) is also real analytic. 
By the property that each zero of a real analytic function is isolated, a non-zero real analytic function has only countably many zeros. It follows that \(g(w) = 0\) for all \(w \in \mathcal{I}\).
Thus, we have
\begin{equation*}
    0= g^{(m)}(w)=\sum_{k=1}^K \lambda_k k^m \sin^{(m)}\left(kw\right)\quad \tn{for all $w\in \calI$ and $m\in \N$,}
\end{equation*}
from which we deduce

% By expanding \(g(w)\) into its Taylor series around \(w = 0\), we get
% \[
% 0 = g(w) = \sum_{k=1}^K \lambda_k \sin\left( k w\right) = \sum_{m=0}^\infty {\color{red}\frac{(-1)^m}{m!}}\left(\sum_{k=1}^K \lambda_k k^m \sin^{(m)}\left(0\right)\right) w^m \quad \text{for all } w \in \mathcal{I}.
% \]
% Thus, we obtain
\[
\sum_{k=1}^K \lambda_k k^m \sin^{(m)}\left(0\right)= 0 \quad \text{for all } m\in\N.
\]
It is easy to verify that 
$|\sin^{(m)}(0)|=1$ for odd $m\in\N$. It follows that
\begin{equation}\label{eq:lambdak:km:sum0:infinite:m}
    \sum_{k=1}^K \lambda_k k^m = 0\quad \tn{for odd $m\in\N$.}
\end{equation}

We assert that Equation~\eqref{eq:lambdak:km:sum0:infinite:m} leads to \(\bmlambda = (\lambda_1, \cdots, \lambda_K) = \bmzero\), which contradicts the assumption that \(\bmlambda \in \mathbb{Q}^K \setminus \{\bmzero\}\). To complete the proof, it suffices to establish this assertion. We assume \(K \ge 2\), as the case \(K = 1\) is straightforward.
By Equation~\eqref{eq:lambdak:km:sum0:infinite:m},
for any odd $m\in \N$, we have
    \begin{equation*}
    0=\frac{1}{(K-1)^m}\sum_{k=1}^K \lambda_k k^m
    =\sum_{k=1}^K \lambda_k \Big(\frac{k}{K-1}\Big)^m
    =\sum_{k=1}^{K-1} \lambda_k \Big(\frac{k}{K-1}\Big)^m +  \lambda_K\Big(\frac{K}{K-1}\Big)^m,
\end{equation*}
implying
    \begin{equation*}
    |\lambda_K|\cdot\Big(\frac{K}{K-1}\Big)^m=
    \left| -\sum_{k=1}^{K-1} \lambda_k \Big(\frac{k}{K-1}\Big)^m\right|\le \sum_{k=1}^{K-1} |\lambda_k|.
\end{equation*}
If \(\lambda_K \neq 0\), then in the above equation, the left-hand side becomes unbounded as \(m\) grows large, while the right-hand side remains bounded.  Thus, we must have \(\lambda_K = 0\). Using a similar argument, we can show that \(\lambda_k = 0\) for \(k = K-1, K-2, \ldots, 1\).
So we finish the proof of Lemma~\ref{lem:rationally:independent}.
% \end{proof}




\subsubsection{Proof of Lemma~\ref{lem:dense:sin}}
\label{sec:proof:lem:dense:sin}

The proof of Lemma~\ref{lem:dense:sin} primarily relies on the fact that an irrational winding is dense on the torus, which is a fascinating phenomenon in transcendental number theory and Diophantine approximations. For completeness, we establish the following lemma.

\begin{lemma}
\label{lem:dense:decimal}
Given any \( K \in \mathbb{N}^+ \) and rationally independent numbers \( a_1, a_2, \dots, a_K \), the set
\begin{equation*}
    \Big\{ \Big(\tau(wa_1), \, \tau(wa_2), \, \cdots, \, \tau(wa_K)\Big) : w \in \mathbb{R} \Big\} \subseteq [0,1)^K
\end{equation*}
is dense in \( [0,1]^K \), where \( \tau(x) \coloneqq x - \lfloor x \rfloor \) for any \( x \in \mathbb{R} \).
\end{lemma}

Lemma~\ref{lem:dense:decimal} is equivalent to Lemma~22 in \cite{shijun:arbitrary:error:with:fixed:size} and Lemma~2 in \cite{yarotsky:2021:02}, where proofs can be found. Now, assuming Lemma~\ref{lem:dense:decimal} holds, let us proceed with the proof of Lemma~\ref{lem:dense:sin}.
 
\begin{proof}[Proof of Lemma~\ref{lem:dense:sin}]
	Define $g(x)\coloneqq \sin(2\pi x)$ for any $x\in \R$. Clearly, $g$ is  periodic with period $1$ and  uniformly continuous on $[-\tfrac{1}{4},\tfrac{1}{4}]$. 
	For any $\varepsilon>0$, there exists $\delta\in (0,\tfrac{1}{2})$ such that
	\begin{equation}\label{eq:g:uniformly:cont}
		|g(u)-g(v)|<\varepsilon\quad\tn{for any $u,v\in [-\tfrac{1}{4},\tfrac{1}{4}]$ with $|u-v|<\delta$.}
	\end{equation}
	
Given any $\bmxi=[\xi_1,\xi_2,\cdots,\xi_K]\in [-1,1]^K=[g(-\tfrac{1}{4}),\, g(\tfrac{1}{4})]^K$,  there exists
\[y_1,y_2,\cdots,y_K\in [-\tfrac{1}{4},\tfrac{1}{4}]\] 
such that 
\begin{equation}\label{eq:g:zk:xik}
    g(y_k)=\xi_k\quad \tn{ for any $k=1,2,\cdots,K$.}
\end{equation}
For $k=1,2,\cdots,K$, 
by setting \begin{equation*}
	\tildey_k=y_k +\tfrac{\delta}{2}\cdot\one_{\{y_k\le -\tfrac{1}{4}+\tfrac{\delta}{2}\}}-\tfrac{\delta}{2}\cdot\one_{\{y_k\ge \tfrac{1}{4}-\tfrac{\delta}{2}\}},
\end{equation*}
 we have 
\begin{equation*}
	\tildey_k=y_k +\tfrac{\delta}{2}\cdot\one_{\{y_k\le -\tfrac{1}{4}+\tfrac{\delta}{2}\}}-\tfrac{\delta}{2}\cdot\one_{\{y_k\ge \tfrac{1}{4}-\tfrac{\delta}{2}\}}\in \big[-\tfrac{1}{4}+\tfrac{\delta}{2},\,\tfrac{1}{4}-\tfrac{\delta}{2}\big]
\end{equation*}
and 
\begin{equation*}
	|\tildey_k-y_k|\le \Big|\tfrac{\delta}{2}\cdot\one_{\{y_k\le -\tfrac{1}{4}+\tfrac{\delta}{2}\}}-\tfrac{\delta}{2}\cdot\one_{\{y_k\ge \tfrac{1}{4}-\tfrac{\delta}{2}\}}\Big|\le \delta/2.
\end{equation*}

Define $\tau(x)\coloneqq  x-\lfloor x\rfloor$ for any $x\in\R$. Clearly, $[\tau(\tildey_1),\tau(\tildey_2),\cdots,\tau(\tildey_K)]^\ts \in [0,1]^K$. Then, by Lemma~\ref{lem:dense:decimal}, there exists $w_0\in\R$ such that
\begin{equation*}
	|\tau(w_0a_k)-\tau(\tildey_k)|<\delta/2\quad \tn{for $k=1,2,\cdots,K$,}
\end{equation*}
from which we deduce
\begin{equation*}
	\Big|\tau(w_0a_k)+\lfloor \tildey_k\rfloor- \tildey_k\Big|=\Big|\tau(w_0a_k)- \big(\tildey_k-\lfloor \tildey_k\rfloor\big)\Big|=\big|\tau(w_0a_k)-\tau(\tildey_k)\big|<\delta/2.
\end{equation*}
It follows from 
 $\tildey_k\in [-\tfrac{1}{4}+\tfrac{\delta}{2},\tfrac{1}{4}-\tfrac{\delta}{2}]$ that
\begin{equation*}
    \tau(w_0a_k)+\lfloor \tildey_k\rfloor\in [-\tfrac{1}{4},\tfrac{1}{4}]\quad \tn{ for $k=1,2,\cdots,K$.}
\end{equation*}
Moreover,
\begin{equation*}
	\Big|\tau(w_0a_k)+\lfloor \tildey_k\rfloor-y_k\Big|\le \Big|\tau(w_0a_k)+\lfloor \tildey_k\rfloor-\tildey_k\Big|+ \big|\tildey_k-y_k\big|<\delta/2+\delta/2=\delta
\end{equation*}
for $k=1,2,\cdots,K$. Then, by Equation~\eqref{eq:g:uniformly:cont}, we have
\begin{equation*}
	\Big| g\big(\tau(w_0a_k)+\lfloor \tildey_k\rfloor\big)-g(y_k) \Big|<\varepsilon\quad \tn{for $k=1,2,\cdots,K$.}
\end{equation*}

Recall that \( g \) is periodic with a period of \( 1 \). Thus, we have
\begin{equation*}
	g\big(\tau(w_0a_k)+\lfloor \tildey_k\rfloor\big)=g\big(w_0a_k-\lfloor w_0 a_k\rfloor+\lfloor \tildey_k\rfloor\big)=g(w_0a_k)=\sin(2\pi  w_0a_k)
\end{equation*}
for $k=1,2,\cdots,K$, implying
\begin{equation*}
	\begin{split}
		\big|\sin(2\pi    w_0a_k)- \xi_k\big|
  =\big|\sin(2\pi    w_0a_k)- g(y_k) \big|
  =\Big| g\big(\tau(w_0a_k)+\lfloor \tildey_k\rfloor\big)-g(y_k) \Big|<\varepsilon.
	\end{split}
\end{equation*}
Therefore, by setting $w_1=2\pi w_0\in\R$, we get 
\begin{equation*}
	\Big\| \Big(g(w_1a_1),\, g(w_1a_2),\, \cdots,\, g(w_1a_K)\Big)  -\bmxi    \Big\|_{\ell^\infty}<\varepsilon,
\end{equation*}
Since $\varepsilon>0$ and $\bmxi\in [-1,1]^K$ are arbitrary, the set
\begin{equation*}
	\Big\{\big(g(wa_1),\, g(wa_2),\, \cdots,\,g(wa_K)\Big) : w\in\R
	\Big\}
\end{equation*}
is dense in $[-1,1]^K$. So we finish the proof of Lemma~\ref{lem:dense:sin}.
\end{proof}


% We remark that the target parameter $w_0=\tfrac{\lfloor s_0\rfloor \hatn+\tildexi_J}{a_J}$ designed in the above proof may not be bounded uniformly for any approximation error $\varepsilon$ since $\hatn$ can be arbitrarily large as $\varepsilon$ goes to $0$. Therefore, the network in Theorem~\ref{thm:main} may require sufficiently large parameters to achieve an arbitrarily small error $\varepsilon$.


\subsection{Proof of Proposition~\ref{prop:approx:ReLU}}
\label{sec:proof:prop:approx:ReLU}


Given any $\eps\in (0,1)$, our goal is to construct $\phi_\eps\in \nn{\varrho}{2}{1}{\R}{\R}$ with $\varrho\in \calS$ to approximate \ReLU\  well on $[-B,B]$.
Since $\varrho\in \calS$, there exists  $x_0\in\R$ such that
\begin{equation*}
    \varrho(x) = 
\begin{cases} 
\sin(x) & \text{if } x \ge x_0, \\ 
\sin(x_0) & \text{if } x < x_0.
\end{cases}
\end{equation*}
Clearly, we have
	\begin{equation*}
		\lim_{t\to 0^-}\frac{\varrho (x_0+t)-\varrho (x_0)}{t}=\lim_{t\to 0^-}\frac{\sin(x_0)-\sin(x_0)}{t}=0
	\end{equation*}
    and 
    \begin{equation*}
L\coloneqq \lim_{t\to 0^+}\frac{\varrho (x_0+t)-\varrho (x_0)}{t}
=\lim_{t\to 0^+}\frac{\sin (x_0+t)-\sin (x_0)}{t}=\sin^\prime(x_0)=\cos(x_0).
\end{equation*}
We split the remainder of the proof into two cases: \( L \neq 0 \) and \( L = 0 \).


\mycase{1}{$L\neq 0$.}
First, we consider the case $L\neq 0$. Since
\begin{equation*}
		\lim_{t\to 0^-}\frac{\varrho (x_0+t)-\varrho (x_0)}{t}=0
	\neq
L= \lim_{t\to 0^+}\frac{\varrho (x_0+t)-\varrho (x_0)}{t}.
\end{equation*}
There exists a small $\delta_\eps\in (0,1)$ such that
\begin{equation*}%\label{eq:t:lessthan:xidelta:scrA3}
	\Big|\frac{\varrho (x_0+t  )-\varrho (x_0)}{t}-0\Big|<\frac{|L|\eps}{B}\quad \tn{for any $t\in (-\delta_\eps,0)$.}
\end{equation*}
and
\begin{equation*}%\label{eq:t:lessthan:xidelta:scrA3}
	\Big|\frac{\varrho (x_0+t  )-\varrho (x_0)}{t }-L\Big|<\frac{|L|\eps}{B}\quad \tn{for any $t\in (0,\delta_\eps)$.}
\end{equation*}
That is,
\begin{equation*}%\label{eq:t:lessthan:xidelta:scrA3}
	\Big|\frac{\varrho (x_0+t  )-\varrho (x_0)}{t}- L \cdot\one_{\{t>0\}} \Big|
%	=\Big|\tfrac{\varrho (x_0+t  )-\varrho (x_0)}{t}-g(t)\Big|
	<\frac{|L|\eps}{B}
    \quad \tn{for any $t\in (-\delta_\eps,0)\cup(0,\delta_\eps)$.}
\end{equation*}
Recall that \( \ReLU(x) = x \cdot \one_{\{x > 0\}} \).  
Therefore, the expression \( \frac{\varrho (x_0 + t) - \varrho (x_0)}{t} \cdot \frac{x}{L} \) should provide a good approximation of \( \ReLU \). Based on this, we define  
\begin{equation*}
	\phi_{\eps}(x)\coloneqq\frac{\varrho (x_0+\eps x)-\varrho (x_0)}{L\eps}\quad \tn{for any $x\in \R$.}
\end{equation*}
Clearly, $\phi_\eps(0)=0$ and 
\[\phi_\eps\in \nn{\varrho}{1}{1}{\R}{\R}\subseteq \nn{\varrho}{2}{1}{\R}{\R}.\]
Moreover, for any $x\in [-B,B]\backslash \{0\}$ and each $\eps\in \big(0,\tfrac{\delta_\eps}{B}\big)$, we have $\eps x\in (-\delta_\eps,0)\cup(0,\delta_\eps)$,	
from which we deduce
\begin{equation*}
	\begin{split}
    \big|\phi_\eps(x)-\ReLU(x)\big|
   & =\frac{|x|}{|L|}\cdot\Big|\frac{L}{x}\cdot \phi_\eps(x)-\frac{L}{x}\cdot\ReLU(x)\Big|
\\& =\frac{|x|}{|L|}\cdot\Big|\frac{L}{x}\cdot \frac{\varrho (x_0+\eps x)-\varrho (x_0)}{L\eps}-\frac{L}{x}\cdot  x\cdot \one_{\{x>0\}}\Big|
\\& =\frac{|x|}{|L|}\cdot\Big|  \frac{\varrho (x_0+\eps x)-\varrho (x_0)}{ \eps  x}-{L}\cdot \one_{\{x>0\}}\Big|
\le 
\frac{|x|}{|L|}\cdot \frac{|L|\eps}{B}\le \eps.
	\end{split}
\end{equation*}
Therefore, we can conclude that
	\begin{equation*}
	\phi_\eps(x)\rightrightarrows \ReLU(x)\quad \tn{as}\   \eps\to 0^+\quad \tn{for any $x\in [-M,M]$.}
\end{equation*}
That means we finish the proof for the case of $L\neq 0$.

\mycase{2}{$L = 0$.}
Next, we consider the case where \( 0 = L = \cos(x_0) \). This implies that  
\( x_0 = \frac{(2k + 1)\pi}{2} \) for some \( k \in \mathbb{Z} \).  
It is straightforward to verify that \( \varrho \in C^1(\mathbb{R}) \backslash C^2(\mathbb{R}) \).  
Specifically, we have  
\begin{equation*}
    \lim_{t \to 0^-} \frac{\varrho^\prime(x_0 + t) - \varrho^\prime(x_0)}{t}
    =\lim_{t \to 0^-} \frac{0-0}{t}= 0.
\end{equation*}
and
\begin{equation*}
\begin{split}
        \tildeL &\coloneqq\lim_{t \to 0^+} \frac{\varrho^\prime(x_0 + t) - \varrho^\prime(x_0)}{t}
    =\lim_{t \to 0^+} \frac{\cos(x_0 + t) - 0}{t}
    =\lim_{t \to 0^+} \frac{\cos(x_0 + t) - \cos\big(\tfrac{(2k + 1)\pi}{2}\big)}{t}
   \\& =\lim_{t \to 0^+} \frac{\cos(x_0 + t) - \cos(x_0)}{t}=-\sin(x_0)=-\sin\big(\tfrac{(2k + 1)\pi}{2}\big)\in \{-1,1\}.
\end{split}
\end{equation*}
Then there exists a small $\delta_\eps\in (0,1)$ such that
\begin{equation*}%\label{eq:t:lessthan:xidelta:scrA3}
	\Big|\frac{\varrho^\prime  (x_0+t  )-\varrho^\prime  (x_0)}{t}- \tildeL \cdot\one_{\{t>0\}} \Big|
%	=\Big|\tfrac{\varrho^\prime  (x_0+t  )-\varrho^\prime  (x_0)}{t}-g(t)\Big|
	<\frac{|\tildeL|\eps}{ 2B }
    \quad \tn{for any $t\in (-\delta_\eps,0)\cup(0,\delta_\eps)$.}
\end{equation*}
We define  
\begin{equation*}
	\tildephi_{\eps}(x)\coloneqq\frac{\varrho^\prime  (x_0+\eps x)-\varrho^\prime  (x_0)}{\tildeL\eps}\quad \tn{for any $x\in \R$.}
\end{equation*}
Clearly, $\tildephi_\eps(0)=0$. Moreover, for any $x\in [-B,B]\backslash \{0\}$ and each $\eps\in \big(0,\tfrac{\delta_\eps}{B}\big)$, we have $\eps x\in (-\delta_\eps,0)\cup(0,\delta_\eps)$,	
from which we deduce
\begin{equation*}
	\begin{split}
    \big|\tildephi_\eps(x)-\ReLU(x)\big|
   & =\frac{|x|}{|\tildeL|}\cdot\Big|\frac{\tildeL}{x}\cdot \tildephi_\eps(x)-\frac{\tildeL}{x}\cdot\ReLU(x)\Big|
\\& =\frac{|x|}{|\tildeL|}\cdot\Big|\frac{\tildeL}{x}\cdot \frac{\varrho^\prime  (x_0+\eps x)-\varrho^\prime  (x_0)}{\tildeL\eps}-\frac{\tildeL}{x}\cdot  x\cdot \one_{\{x>0\}}\Big|
\\& =\frac{|x|}{|\tildeL|}\cdot\Big|  \frac{\varrho^\prime  (x_0+\eps x)-\varrho^\prime  (x_0)}{ \eps  x}-{\tildeL}\cdot \one_{\{x>0\}}\Big|
\le 
\frac{|x|}{|\tildeL|}\cdot \frac{|\tildeL|\eps}{2B}\le \frac{\eps}{2}.
	\end{split}
\end{equation*}
That is, for each $\eps\in (0,\frac{\delta_\eps}{B})$, we have 
\begin{equation}
\label{eq:tildephieps:minus:ReLU}
	\begin{split}
    \big|\tildephi_\eps(x)-\ReLU(x)\big|
   \le \frac{\eps}{2}\quad \tn{for any $x\in [-B, B]$.}
	\end{split}
\end{equation}


For each $\eta\in (0,1)$, we define
\begin{equation*}
	h_\eta(z)\coloneqq\frac{\varrho (z+\eta)-\varrho (z) }{\eta}\quad \tn{for any $z\in\R$.}
\end{equation*}
Recall that  $\varrho\in C^1(\R)\backslash C^{2}(\R)$.
By Lagrange's mean value theorem, for any $z\in\R$, there exists
$\xi\in (z,z+\eta)$  such that 
\begin{equation*}
	h_\eta(z)= \frac{\varrho (z+\eta)-\varrho (z) }{\eta}=\varrho^\prime(\xi),
\end{equation*}
from which we deduce
\begin{equation*}
	h_\eta(z)= \frac{\varrho (z+\eta)-\varrho (z) }{\eta}=\varrho^\prime(\xi)
	\rightrightarrows \varrho^{\prime}(z)\quad\tn{as}\  \eta\to0\quad \tn{for any $z\in [x_0-1,x_0+1]$.}
\end{equation*}
Then there exists $\eta_\eps>0$ such that
\begin{equation*}
%	\bigg|	\frac{\sum_{i=0}^{k}(-1)^i\binom{k}{i} \varrho(z+i\eta_\eps) }{(-\eta_\eps)^k} - \varrho^{\prime}(z)\bigg|
%	=
	\big|	h_{\eta_\eps}(z) - \varrho^{\prime}(z)\big|<\frac{|\tildeL|\eps^2}{2}\quad \tn{for any $z\in [x_0-1,x_0+1]$.}
\end{equation*}				
Next, we can define the desired $\phi_\eps$ via
\begin{equation*}
	\begin{split}
		\phi_{\eps}(x)\coloneqq
		 \frac{h_{\eta_\eps}(x_0+\eps x)-\varrho^{\prime}(x_0)}{\tildeL\eps}
         &=\frac{ \frac{\varrho(x_0+\eps x+ \eta_\eps) -\varrho(x_0+\eps x)}{\eta_\eps} -\varrho^{\prime}(x_0)}{\tildeL\eps}
		\\& =\frac{  \varrho(x_0+\eps x+ \eta_\eps) -\varrho(x_0+\eps x) -{\eta_\eps}\varrho^{\prime}(x_0)}{{\eta_\eps}\tildeL\eps}
	\end{split}
\end{equation*}	
for any $x\in \R$. Clearly, $\phi_\eps\in \nn{\varrho}{2}{1}{\R}{\R}$. Moreover, for each $\eps\in \big(0,\tfrac{\delta_\eps}{B}\big)\subseteq \big(0,\tfrac{1}{B}\big)$ and any $x\in [-B,B]$, we have $x_0+\eps  x \in [x_0-1,x_0+1]$, implying
\begin{equation*}
	\begin{split}
		\big|\phi_{\eps}(x)-\tildephi_{\eps}(x)\big|
        &=  \bigg|\frac{h_{\eta_\eps}(x_0+\eps x)-\varrho^{\prime}(x_0)}{\eps}
         -\frac{\varrho^{\prime}(x_0+\eps x)-\varrho^{\prime}(x_0)}{\eps} \bigg|\\	
		&\le  \frac{1}{\eps}\cdot\Big|	h_{\eta_\eps}(x_0+\eps x)-\varrho^{\prime}(x_0+\eps x)\Big|
		\le \frac{1}{|\tildeL|\eps}\cdot\frac{|\tildeL|\eps^2}{2}=\frac{\eps}{2}.
	\end{split}
\end{equation*}
Combining this with Equation~\eqref{eq:tildephieps:minus:ReLU}, we can conclude that
\begin{equation*}
	\big|\phi_\eps(x)-\ReLU(x)\big|\le \big|\phi_\eps(x)-\tildephi_\eps(x)\big|+\big|\tildephi_\eps(x)-\ReLU(x)\big|\le \frac{\eps}{2}+ \frac{\eps}{2}=\eps,
\end{equation*}
for each $\eps\in \big(0,\tfrac{\delta_\eps}{B}\big)$ and any $x\in [-B,B]$. That means
	\begin{equation*}
	\phi_\eps(x)\rightrightarrows \ReLU(x)\quad \tn{as}\   \eps\to 0^+\quad \tn{for any $x\in [-B,B]$.}
\end{equation*}
Thus, we have completed the proof for the case \( L = 0 \), thereby concluding the proof of Proposition~\ref{prop:approx:ReLU}.





\section{Conclusion}
\label{sec:conclusion}

In this work, we investigate the crucial interplay between neural network architectures and activation functions, emphasizing how their proper alignment significantly influences practical performance. Specifically, we propose the use of \texttt{sine} and a new class of activation functions, \SinTU{s}, and examine their effectiveness within Multi-Component and Multi-Layer Neural Networks (MMNNs) structures, termed FMMNNs. Our findings demonstrate that the combination of \texttt{sine} or \SinTU{s} with MMNNs establishes a highly synergistic framework, offering both theoretical and empirical advantages especially for capturing high frequency components in the target functions.

First, we establish that MMNNs equipped with \texttt{sine} or \SinTU{s} exhibit strong approximation capabilities, surpassing traditional architectures in mathematical expressiveness. We further analyze the optimization landscape of MMNNs, revealing that their training dynamics are considerably more favorable than those of standard FCNNs. This insight suggests that MMNNs benefit from reduced training complexity and improved convergence properties.

To validate our theoretical analysis, we conduct extensive numerical experiments focused on function approximation. The results consistently show that FMMNNs outperform conventional models in both accuracy and computational efficiency. These findings highlight the potential of MMNNs with \texttt{sine}-based activation functions as a robust and efficient paradigm for deep learning applications.

While our current experiments primarily focus on function approximation, applying FMMNNs to broader practical tasks remains an important direction for future research. Additionally, from a theoretical standpoint, we have only explored the expressiveness of FMMNNs. A deeper understanding of their optimization dynamics is equally crucial but remains beyond the scope of this paper. These aspects are left for future investigation.


% \clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}
S. Zhang was partially supported by
start-up fund  P0053092  from 
% The 
Hong Kong Polytechnic University.
H. Zhao was partially supported by NSF grants DMS-2309551, and DMS-2012860. Y. Zhong was partially supported by NSF grant DMS-2309530, H. Zhou was partially supported by NSF grant DMS-2307465.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% {\small
% \let\oldhref\href
% \def\href#1#2{\oldhref{#1}{\nolinkurl{#2}}}

% \let\olddoi\doi
% \def\doi#1{\nolinkurl{\olddoi{#1}}}

% \let\olddoi\doi
% \renewcommand{\doi}[1]{\nolinkurl{#1}}

\hypersetup{colorlinks=true,citecolor=black,linkcolor=black,urlcolor=black}
\renewcommand{\doi}[1]{\textnormal{\doitext}~\texttt{\href{https://doi.org/#1}{#1}}}

%%%\clearpage
% \bibliographystyle{siam}
\bibliographystyle{plainnat}
% \bibpunct{(}{)}{;}{a}{,}{,}
% \bibliographystyle{plainurl}   
  
\bibliography{references}
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}