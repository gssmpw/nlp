@article{cisthpan, title={CI-STHPAN: Pre-trained Attention Network for Stock Selection with Channel-Independent Spatio-Temporal Hypergraph}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/28770}, DOI={10.1609/aaai.v38i8.28770}, abstractNote={Quantitative stock selection is one of the most challenging FinTech tasks due to the non-stationary dynamics and complex market dependencies. Existing studies rely on channel mixing methods, exacerbating the issue of distribution shift in financial time series. Additionally, complex model structures they build make it difficult to handle very long sequences. Furthermore, most of them are based on predefined stock relationships thus making it difficult to capture the dynamic and highly volatile stock markets. To address the above issues, in this paper, we propose Channel-Independent based Spatio-Temporal Hypergraph Pre-trained Attention Networks (CI-STHPAN), a two-stage framework for stock selection, involving Transformer and HGAT based stock time series self-supervised pre-training and stock-ranking based downstream task fine-tuning. We calculate the similarity of stock time series of different channel in dynamic intervals based on Dynamic Time Warping (DTW), and further construct channel-independent stock dynamic hypergraph based on the similarity. Experiments with NASDAQ and NYSE markets data over five years show that our framework outperforms SOTA approaches in terms of investment return ratio (IRR) and Sharpe ratio (SR). Additionally, we find that even without introducing graph information, self-supervised learning based on the vanilla Transformer Encoder also surpasses SOTA results. Notable improvements are gained on the NYSE market. It is mainly attributed to the improvement of fine-tuning approach on Information Coefficient (IC) and Information Ratio based IC (ICIR), indicating that the fine-tuning method enhances the accuracy and stability of the model prediction.}, number={8}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Xia, Hongjie and Ao, Huijie and Li, Long and Liu, Yu and Liu, Sen and Ye, Guangnan and Chai, Hongfeng}, year={2024}, month={Mar.}, pages={9187-9195} }

@inproceedings{mambair,
    title={MambaIR: A Simple Baseline for Image Restoration with State-Space Model},
    author={Guo, Hang and Li, Jinmin and Dai, Tao and Ouyang, Zhihao and Ren, Xudong and Xia, Shu-Tao},
    booktitle={ECCV},
    year={2024}
}

@misc{mambastock,
  title={MambaStock: Selective state space model for stock prediction},
  author={Zhuangwei Shi},
  journal={arXiv preprint arXiv:2402.18959},
  year={2024},
}

@misc{mambavision,
      title={MambaVision: A Hybrid Mamba-Transformer Vision Backbone}, 
      author={Ali Hatamizadeh and Jan Kautz},
      year={2024},
      journal={arXiv preprint arXiv:2407.08083},
}

@article{master, 
title={MASTER: Market-Guided Stock Transformer for Stock Price Forecasting}, volume={38}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Li, Tong and Liu, Zhaoyang and Shen, Yanyan and Wang, Xue and Chen, Haokun and Huang, Sen}, 
year={2024}
}

@misc{samba,
      title={Mamba Meets Financial Markets: A Graph-Mamba Approach for Stock Price Prediction}, 
      author={Ali Mehrabian and Ehsan Hoseinzade and Mahdi Mazloum and Xiaohong Chen},
      year={2024},
      journal={arXiv preprint arXiv:2410.03707},
}

@inproceedings{thgnn,
author = {Xiang, Sheng and Cheng, Dawei and Shang, Chencheng and Zhang, Ying and Liang, Yuqi},
title = {Temporal and Heterogeneous Graph Neural Network for Financial Time Series Prediction},
year = {2022},
booktitle = {Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
}

@misc{timemachine,
  title     = {TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting},
  author    = {Ahamed, Md Atik and Cheng, Qiang},
  journal   = {arXiv preprint arXiv:2403.09898},
  year      = {2024},
}

