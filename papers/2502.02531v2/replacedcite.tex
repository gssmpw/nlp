\section{Related Works}
\label{sec:related_works}
\vspace{-5pt}
\paragraph{Statics and Bayesian Networks} Theory of the statics\footnote{ This could be interpreted as sampling a neural network from an equilibrium distribution, which can be contrasted with out-of-equilibrium dynamics which are our focus in this work. } of deep Bayesian linear neural networks in the regime where dataset $P$ and width $N$ are proportionally large were first characterized in ____, resulting in a theory of a renormalized kernel after training. ____ and ____ theoretically and experimentally argue that the scale-renormalization of the NNGP kernel continues to capture the effect of feature learning in \textit{nonlinear} Bayesian networks. Their results were recently extended to convolutional networks ____. Learning curves which average over a randomly sampled training set were obtained by ____ with the replica method, allowing contrast between Bayesian inference over all layers with training random feature models, who extended the theory to structured covariates ____. ____ showed that these learning curves also describe learning curves for \textit{nonlinear} multilayer Perceptrons (MLPs) under Bayes-optimal teacher-student learning setup in the proportional limit. ____ used similar techniques to analyze weakly nonlinear Bayesian networks, where the nonlinearity is depth dependent, at large width $N$, depth $L$ and dataset size $P$ with $\frac{L P}{N}$ fixed.

\vspace{-10pt}
\paragraph{Dynamics from Random Initialization} In the lazy learning regime, the infinite limit of a randomly initialized network is characterized by the Neural Tangent Kernel (NTK) ____. However, if one adopts a scaling that enables feature learning at infinite width (mean field / $\mu$P scaling) then the dynamics of deep network training from random initialization are more complicated. The limiting $N \to \infty$ gradient descent dynamics in MLPs for a fixed (finite) stream of data in the rich regime have been characterized with partial differential equations in one-hidden layer networks ____  as well as Tensor programs ____ and dynamical mean field theory ____ techniques for deeper networks. These methods describe the typical case behavior of training nonlinear networks over random realizations of the initial weights. However, the resulting theories are generally intractable as they scale poorly with the size of the training dataset $P$ and the number of steps of training $T$ outside of the lazy training regime. A key challenge in this deep setting is to also average over the random data to obtain a reduced description where effects of order $\frac{ P }{ N }$ are preserved. Various perturbative approximations $\frac{1}{N}$ have been studied, though these do not capture a full joint limit for data and width and also suffer from intractability at large dataset sizes ____.

\vspace{-8pt}
\paragraph{Residual Networks and Large Depth Limits} Theory to characterize infinite width and depth training dynamics $N \to \infty$ and $L \to \infty$ have been recently obtained for networks starting at random initial conditions ____.  More recently, the work of 
____ studied depth $L = 2$ finite width linear networks trained with a projected gradient descent using mean field techniques. In this present work, we characterize exact gradient descent for arbitrary depth $L$ networks. We also study the role of $\mu$P scaling to achieve learning rate transfer across width and scaled residual branches ____ for transfer across depths. 

\vspace{-8pt}
\paragraph{Linear Network Gradient Flow ODEs} The dynamics of gradient flow training in deep linear networks in the very rich regime (or equivalently for small initialization) were computed by ____ as a simple set of ordinary differential equations.  ____ and ____ recently derived similar equations for the dynamics of networks at different laziness/richness scale and unbalanced initializations. However, these methods require a number of assumptions including whitened input data, a balance assumption on products of weights in adjacent layers, and lack of bottleneck or overparameterized layers. Our work goes beyond these assumptions by studying generic (random and non-negligible) initialization of the network weights, which introduce interesting dependence of the dynamics on network width and dataset size. In addition, our theory allows for sampling random training data, which will generally not give a whitened empirical covariance\footnote{Even if the data's population distribution is isotropic, the empirical eigenvalues will follow a Marchenko-Pastur law.}. 

\vspace{-10pt}
\paragraph{Linear Network Models of Neural Scaling Laws.} ____ and ____ recently analyzed a linear random feature model trained with gradient descent, finding that training dynamics on data that satisfies source and capacity conditions recovers Chinchilla-like neural scaling laws ____. ____ recently extended this theory to a case where the random projection matrix is also updated with projected gradient descent, resulting in a faster power law convergence rate for hard tasks. Their model is mathematically equivalent to a one hidden layer linear network trained with a projected version gradient descent. We find this same acceleration behavior in \textit{deeper} linear networks trained on power law data in this work.  

\vspace{-10pt}
\paragraph{DMFT Approaches to Machine Learning Theory} 
Dynamical mean field theory methods, originally developed in the physics of spin glasses, have been used to describe the dynamics of high dimensional optimization (including stochastic gradient methods) in terms of single variable stochastic integro-differential equations ____. These prior works have shown the validity of DMFT to characterizing general linear models or shallow neural networks trained with parameters and data going to infinity proportionally. In our work the disorder comes from \textit{both the data matrix and the random initial weights} of the network and we are interested in the behavior of arbitrary depth networks. 

\vspace{-10pt}