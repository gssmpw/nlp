@article{imagenet15russakovsky,
    Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
    Title = { {ImageNet Large Scale Visual Recognition Challenge} },
    Year = {2015},
    journal   = {International Journal of Computer Vision (IJCV)},
    doi = {10.1007/s11263-015-0816-y},
    volume={115},
    number={3},
    pages={211-252}
}

@article{zhao2024deconstructing,
  title={Deconstructing What Makes a Good Optimizer for Language Models},
  author={Zhao, Rosie and Morwani, Depen and Brandfonbrener, David and Vyas, Nikhil and Kakade, Sham},
  journal={arXiv preprint arXiv:2407.07972},
  year={2024}
}

@article{dandi2024benefits,
  title={The benefits of reusing batches for gradient descent in two-layer networks: Breaking the curse of information and leap exponents},
  author={Dandi, Yatin and Troiani, Emanuele and Arnaboldi, Luca and Pesce, Luca and Zdeborov{\'a}, Lenka and Krzakala, Florent},
  journal={arXiv preprint arXiv:2402.03220},
  year={2024}
}

@article{agoritsas2018out,
  title={Out-of-equilibrium dynamical mean-field equations for the perceptron model},
  author={Agoritsas, Elisabeth and Biroli, Giulio and Urbani, Pierfrancesco and Zamponi, Francesco},
  journal={Journal of Physics A: Mathematical and Theoretical},
  volume={51},
  number={8},
  pages={085002},
  year={2018},
  publisher={IOP Publishing}
}

@article{noci2024learning,
  title={Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning},
  author={Noci, Lorenzo and Meterez, Alexandru and Hofmann, Thomas and Orvieto, Antonio},
  journal={arXiv preprint arXiv:2402.17457},
  year={2024}
}

@inproceedings{
bordelon2024depthwise,
title={Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit},
author={Blake Bordelon and Lorenzo Noci and Mufan Bill Li and Boris Hanin and Cengiz Pehlevan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=KZJehvRKGD}
}

@inproceedings{cui2023bayes,
  title={Bayes-optimal learning of deep random networks of extensive-width},
  author={Cui, Hugo and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle={International Conference on Machine Learning},
  pages={6468--6521},
  year={2023},
  organization={PMLR}
}

@article{hanin2024bayesian,
  title={Bayesian Inference with Deep Weakly Nonlinear Networks},
  author={Hanin, Boris and Zlokapa, Alexander},
  journal={arXiv preprint arXiv:2405.16630},
  year={2024}
}
@inproceedings{vankadarafeature,
  title={On Feature Learning in Structured State Space Models},
  author={Vankadara, Leena Chennuru and Xu, Jin and Haas, Moritz and Cevher, Volkan},
year = {2024},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems}
}

@article{hayou2024commutative,
  title={Commutative Scaling of Width and Depth in Deep Neural Networks},
  author={Hayou, Soufiane},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={299},
  pages={1--41},
  year={2024}
}

@article{domine2024lazy,
  title={From Lazy to Rich: Exact Learning Dynamics in Deep Linear Networks},
  author={Domin{\'e}, Cl{\'e}mentine CJ and Anguita, Nicolas and Proca, Alexandra M and Braun, Lukas and Kunin, Daniel and Mediano, Pedro AM and Saxe, Andrew M},
  journal={arXiv preprint arXiv:2409.14623},
  year={2024}
}


@misc{bordelon2024featurelearningimproveneural,
      title={How Feature Learning Can Improve Neural Scaling Laws}, 
      author={Blake Bordelon and Alexander Atanasov and Cengiz Pehlevan},
      year={2024},
      eprint={2409.17858},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2409.17858}
}

@article{li2021statistical,
  title={Statistical mechanics of deep linear neural networks: The backpropagating kernel renormalization},
  author={Li, Qianyi and Sompolinsky, Haim},
  journal={Physical Review X},
  volume={11},
  number={3},
  pages={031059},
  year={2021},
  publisher={APS}
}

@article{de1978dynamics,
  title={Dynamics as a substitute for replicas in systems with quenched random impurities},
  author={De Dominicis, C},
  journal={Physical Review B},
  volume={18},
  number={9},
  pages={4913},
  year={1978},
  publisher={APS}
}


@article{sompolinsky1981dynamic,
  title={Dynamic theory of the spin-glass phase},
  author={Sompolinsky, Haim and Zippelius, Annette},
  journal={Physical Review Letters},
  volume={47},
  number={5},
  pages={359},
  year={1981},
  publisher={APS}
}

@article{cui2021generalization,
  title={Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime},
  author={Cui, Hugo and Loureiro, Bruno and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10131--10143},
  year={2021}
}

@article{advani2020high,
  title={High-dimensional dynamics of generalization error in neural networks},
  author={Advani, Madhu S and Saxe, Andrew M and Sompolinsky, Haim},
  journal={Neural Networks},
  volume={132},
  pages={428--446},
  year={2020},
  publisher={Elsevier}
}

@article{varre2021last,
  title={Last iterate convergence of SGD for Least-Squares in the Interpolation regime.},
  author={Varre, Aditya Vardhan and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21581--21591},
  year={2021}
}

@article{mignacco2022effective,
  title={The effective noise of stochastic gradient descent},
  author={Mignacco, Francesca and Urbani, Pierfrancesco},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2022},
  number={8},
  pages={083405},
  year={2022},
  publisher={IOP Publishing}
}

@inproceedings{mannelli2019passed,
  title={Passed \& spurious: Descent algorithms and local minima in spiked matrix-tensor models},
  author={Mannelli, Stefano Sarao and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborova, Lenka},
  booktitle={international conference on machine learning},
  pages={4333--4342},
  year={2019},
  organization={PMLR}
}

@article{michaud2023quantization,
  title={The quantization model of neural scaling},
  author={Michaud, Eric J and Liu, Ziming and Girit, Uzay and Tegmark, Max},
  journal={arXiv preprint arXiv:2303.13506},
  year={2023}
}

@article{arora2023theory,
  title={A theory for emergence of complex skills in language models},
  author={Arora, Sanjeev and Goyal, Anirudh},
  journal={arXiv preprint arXiv:2307.15936},
  year={2023}
}

@article{caballero2022broken,
  title={Broken neural scaling laws},
  author={Caballero, Ethan and Gupta, Kshitij and Rish, Irina and Krueger, David},
  journal={arXiv preprint arXiv:2210.14891},
  year={2022}
}

@article{long2021properties,
  title={Properties of the after kernel},
  author={Long, Philip M},
  journal={arXiv preprint arXiv:2105.10585},
  year={2021}
}

@article{cortes2012algorithms,
  title={Algorithms for learning kernels based on centered alignment},
  author={Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={795--828},
  year={2012},
  publisher={JMLR. org}
}


@article{caponnetto2005fast,
  title={Fast rates for regularized least-squares algorithm},
  author={Caponnetto, Andrea and Vito, Ernesto De},
  year={2005}
}

@article{cheng2022dimension,
  title={Dimension free ridge regression},
  author={Cheng, Chen and Montanari, Andrea},
  journal={arXiv preprint arXiv:2210.08571},
  year={2022}
}

@inproceedings{wei2022more,
  title={More than a toy: Random matrix models predict how real-world neural representations generalize},
  author={Wei, Alexander and Hu, Wei and Steinhardt, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={23549--23588},
  year={2022},
  organization={PMLR}
}

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}


@inproceedings{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={3635--3673},
  year={2020},
  organization={PMLR}
}

@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}


@book{bach2024learning,
  title={Learning theory from first principles},
  author={Bach, Francis},
  year={2024},
  publisher={MIT press}
}

@inproceedings{
bordelon2022learning,
title={Learning Curves for {SGD} on Structured Features},
author={Blake Bordelon and Cengiz Pehlevan},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=WPI2vbkAl3Q}
}

@article{du2018algorithmic,
  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
  author={Du, Simon S and Hu, Wei and Lee, Jason D},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{atanasov2024scaling,
  title={Scaling and renormalization in high-dimensional regression},
  author={Atanasov, Alexander B and Zavatone-Veth, Jacob A and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2405.00592},
  year={2024}
}

@article{pillaud2018statistical,
  title={Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes},
  author={Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}


@inproceedings{refinetti2023neural,
  title={Neural networks trained with SGD learn distributions of increasing complexity},
  author={Refinetti, Maria and Ingrosso, Alessandro and Goldt, Sebastian},
  booktitle={International Conference on Machine Learning},
  pages={28843--28863},
  year={2023},
  organization={PMLR}
}

@misc{pearce2022conditional,
  author = {Tim Pearce},
  title = {Conditional Diffusion MNIST},
  year = {2022},
  howpublished = {\url{https://github.com/TeaPearce/Conditional_Diffusion_MNIST}},
  note = {Accessed: 2024-05-14}
}

@article{ghorbani2020neural,
  title={When do neural networks outperform kernel methods?},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14820--14830},
  year={2020}
}

@article{ba2022high,
  title={High-dimensional asymptotics of feature learning: How one gradient step improves the representation},
  author={Ba, Jimmy and Erdogdu, Murat A and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37932--37946},
  year={2022}
}

@inproceedings{abbe2023sgd,
  title={Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics},
  author={Abbe, Emmanuel and Adsera, Enric Boix and Misiakiewicz, Theodor},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={2552--2623},
  year={2023},
  organization={PMLR}
}

@article{abbe2021staircase,
  title={The staircase property: How hierarchical structure can guide deep learning},
  author={Abbe, Emmanuel and Boix-Adsera, Enric and Brennan, Matthew S and Bresler, Guy and Nagaraj, Dheeraj},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={26989--27002},
  year={2021}
}

@inproceedings{
dandi2023how,
title={How Two-Layer Neural Networks Learn, One (Giant) Step at a Time},
author={Yatin Dandi and Florent Krzakala and Bruno Loureiro and Luca Pesce and Ludovic Stephan},
booktitle={NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning},
year={2023},
url={https://openreview.net/forum?id=iBDcaBLhz2}
}

@article{bardone2024sliding,
  title={Sliding down the stairs: how correlated latent variables accelerate learning with neural networks},
  author={Bardone, Lorenzo and Goldt, Sebastian},
  journal={arXiv preprint arXiv:2404.08602},
  year={2024}
}

@article{martin1973statistical,
  title={Statistical dynamics of classical systems},
  author={Martin, Paul Cecil and Siggia, ED and Rose, HA},
  journal={Physical Review A},
  volume={8},
  number={1},
  pages={423},
  year={1973},
  publisher={APS}
}

@article{park2020towards,
  title={Towards nngp-guided neural architecture search},
  author={Park, Daniel S and Lee, Jaehoon and Peng, Daiyi and Cao, Yuan and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:2011.06006},
  year={2020}
}


@article{vyas2023beyond,
  title={Beyond implicit bias: The insignificance of sgd noise in online learning},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Kaplun, Gal and Kakade, Sham and Barak, Boaz},
  journal={arXiv preprint arXiv:2306.08590},
  year={2023}
}

@article{paccolat2021geometric,
  title={Geometric compression of invariant manifolds in neural networks},
  author={Paccolat, Jonas and Petrini, Leonardo and Geiger, Mario and Tyloo, Kevin and Wyart, Matthieu},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={4},
  pages={044001},
  year={2021},
  publisher={IOP Publishing}
}

@article{bordelon2021learning,
  title={Learning curves for sgd on structured features},
  author={Bordelon, Blake and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2106.02713},
  year={2021}
}

@article{mignacco2020dynamical,
  title={Dynamical mean-field theory for stochastic gradient descent in Gaussian mixture classification},
  author={Mignacco, Francesca and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9540--9550},
  year={2020}
}

@inproceedings{paquette2021sgd,
  title={SGD in the large: Average-case analysis, asymptotics, and stepsize criticality},
  author={Paquette, Courtney and Lee, Kiwon and Pedregosa, Fabian and Paquette, Elliot},
  booktitle={Conference on Learning Theory},
  pages={3548--3626},
  year={2021},
  organization={PMLR}
}

@article{ruben2023learning,
  title={Learning Curves for Noisy Heterogeneous Feature-Subsampled Ridge Ensembles},
  author={Ruben, Benjamin S and Pehlevan, Cengiz},
  journal={ArXiv},
  year={2023},
  publisher={arXiv}
}

@article{zavatone2022contrasting,
  title={Contrasting random and learned features in deep Bayesian linear regression},
  author={Zavatone-Veth, Jacob A and Tong, William L and Pehlevan, Cengiz},
  journal={Physical Review E},
  volume={105},
  number={6},
  pages={064118},
  year={2022},
  publisher={APS}
}

@misc{zavatoneveth2023learning,
      title={Learning curves for deep structured Gaussian feature models}, 
      author={Jacob A. Zavatone-Veth and Cengiz Pehlevan},
      year={2023},
      eprint={2303.00564},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{
ghosh2022the,
title={The Three Stages of Learning Dynamics in High-dimensional Kernel Methods},
author={Nikhil Ghosh and Song Mei and Bin Yu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=EQmAP4F859}
}

@inproceedings{
nakkiran2021the,
title={The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers},
author={Preetum Nakkiran and Behnam Neyshabur and Hanie Sedghi},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=guetrIHLFGI}
}

@article{catapult,
  author    = {Aitor Lewkowycz and
               Yasaman Bahri and
               Ethan Dyer and
               Jascha Sohl{-}Dickstein and
               Guy Gur{-}Ari},
  title     = {The large learning rate phase of deep learning: the catapult mechanism},
  journal   = {CoRR},
  volume    = {abs/2003.02218},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.02218},
  eprinttype = {arXiv},
  eprint    = {2003.02218},
  timestamp = {Tue, 10 Mar 2020 13:33:48 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-02218.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NakkiranKBYBS20,
  author       = {Preetum Nakkiran and
                  Gal Kaplun and
                  Yamini Bansal and
                  Tristan Yang and
                  Boaz Barak and
                  Ilya Sutskever},
  title        = {Deep Double Descent: Where Bigger Models and More Data Hurt},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=B1g5sA4twr},
  timestamp    = {Thu, 07 May 2020 17:11:47 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/NakkiranKBYBS20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{mosaic-llm,
  author = {Mosaic ML},
  title = {{Mosaic ML - LLMs}},
  howpublished = "\url{https://web.archive.org/web/2/https://github.com/mosaicml/examples/tree/main/examples/llm}",
  year = {2023}, 
  note = "[Online; accessed 19-May-2023]"
}
@article{bordelon2024infinite,
  title={Infinite Limits of Multi-head Transformer Dynamics},
  author={Bordelon, Blake and Chaudhry, Hamza Tahir and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2405.15712},
  year={2024}
}
@article{yang2023tensor,
  title={Tensor programs vi: Feature learning in infinite-depth neural networks},
  author={Yang, Greg and Yu, Dingli and Zhu, Chen and Hayou, Soufiane},
  journal={arXiv preprint arXiv:2310.02244},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{mosaic-125m,
  author = {Mosaic ML},
  title = {{Mosaic ML -125m.yaml}},
  howpublished = "\url{https://web.archive.org/web/20230519183813/https://github.com/mosaicml/examples/blob/main/examples/llm/yamls/mosaic_gpt/125m.yaml}",
  year = {2023}, 
  note = "[Online; accessed 19-May-2023]"
}



@article{rotskoff2018parameters,
  title={Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks},
  author={Rotskoff, Grant and Vanden-Eijnden, Eric},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{alabdulmohsin2023getting,
  title={Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design},
  author={Alabdulmohsin, Ibrahim and Zhai, Xiaohua and Kolesnikov, Alexander and Beyer, Lucas},
  journal={arXiv preprint arXiv:2305.13035},
  year={2023}
}

@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@inproceedings{nakkiran2021deep,
  title={The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers},
  author={Nakkiran, Preetum and Neyshabur, Behnam and Sedghi, Hanie},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{d2020double,
  title={Double trouble in double descent: Bias and variance (s) in the lazy regime},
  author={d’Ascoli, St{\'e}phane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
  booktitle={International Conference on Machine Learning},
  pages={2280--2290},
  year={2020},
  organization={PMLR}
}

@inproceedings{fang2021modeling,
  title={Modeling from features: a mean-field framework for over-parameterized deep neural networks},
  author={Fang, Cong and Lee, Jason and Yang, Pengkun and Zhang, Tong},
  booktitle={Conference on learning theory},
  pages={1887--1936},
  year={2021},
  organization={PMLR}
}

@article{bachmann2024scaling,
  title={Scaling mlps: A tale of inductive bias},
  author={Bachmann, Gregor and Anagnostidis, Sotiris and Hofmann, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{shan2021theory,
  title={A theory of neural tangent kernel alignment and its influence on training},
  author={Shan, Haozhe and Bordelon, Blake},
  journal={arXiv preprint arXiv:2105.14301},
  year={2021}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{vyas2022limitations,
  title={Limitations of the NTK for understanding generalization in deep learning},
  author={Vyas, Nikhil and Bansal, Yamini and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2206.10012},
  year={2022}
}

@inproceedings{kornblith2019similarity,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={3519--3529},
  year={2019},
  organization={PMLR}
}

@article{seroussi2023separation,
  title={Separation of scales and a thermodynamic description of feature learning in some CNNs},
  author={Seroussi, Inbar and Naveh, Gadi and Ringel, Zohar},
  journal={Nature Communications},
  volume={14},
  number={1},
  pages={908},
  year={2023},
  publisher={Nature Publishing Group UK London}
}


@article{park2023trak,
  title={TRAK: Attributing Model Behavior at Scale},
  author={Park, Sung Min and Georgiev, Kristian and Ilyas, Andrew and Leclerc, Guillaume and Madry, Aleksander},
  journal={arXiv preprint arXiv:2303.14186},
  year={2023}
}

@inproceedings{
atanasov2022neural,
title={Neural Networks as Kernel Learners: The Silent Alignment Effect},
author={Alexander Atanasov and Blake Bordelon and Cengiz Pehlevan},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=1NvflqAdoom}
}


@article{adlam2020understanding,
  title={Understanding double descent requires a fine-grained bias-variance decomposition},
  author={Adlam, Ben and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11022--11032},
  year={2020}
}

@inproceedings{
yang2021tuning,
title={Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
author={Greg Yang and Edward J Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki and Weizhu Chen and Jianfeng Gao},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=Bx6qKuBM2AD}
}

@article{ortiz2021can,
  title={What can linearized neural networks actually say about generalization?},
  author={Ortiz-Jim{\'e}nez, Guillermo and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8998--9010},
  year={2021}
}

@misc{mup,
  author = "Yang, Greg and Hu, Edward",
  title = "{Maximal Update Parametrization ($\mu$P) and Hyperparameter Transfer ($\mu$Transfer)}",
  howpublished = {\url{https://github.com/microsoft/mup}},
  year = {2022},
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2103.00065},
  year={2021}
}

@article{yang2021theory,
  title={A theory of representation learning in deep neural networks gives a deep generalisation of kernel methods},
  author={Yang, Adam X and Robeyns, Maxime and Milsom, Edward and Schoots, Nandi and Aitchison, Laurence},
  journal={arXiv preprint arXiv:2108.13097},
  year={2021}
}

@article{canatar2021spectral,
  title={Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks},
  author={Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={2914},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{luo2020positional,
  title={Positional artefacts propagate through masked language model embeddings},
  author={Luo, Ziyang and Kulmizev, Artur and Mao, Xiaoxi},
  journal={arXiv preprint arXiv:2011.04393},
  year={2020}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{araujo2019mean,
  title={A mean-field limit for certain deep neural networks},
  author={Ara{\'u}jo, Dyego and Oliveira, Roberto I and Yukimura, Daniel},
  journal={arXiv preprint arXiv:1906.00193},
  year={2019}
}

@article{nguyen2020rigorous,
  title={A rigorous framework for the mean field limit of multilayer neural networks},
  author={Nguyen, Phan-Minh and Pham, Huy Tuan},
  journal={arXiv preprint arXiv:2001.11443},
  year={2020}
}

@article{sirignano2020mean,
  title={Mean field analysis of neural networks: A law of large numbers},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={SIAM Journal on Applied Mathematics},
  volume={80},
  number={2},
  pages={725--752},
  year={2020},
  publisher={SIAM}
}

@article{nguyen2019mean,
  title={Mean field limit of the learning dynamics of multilayer neural networks},
  author={Nguyen, Phan-Minh},
  journal={arXiv preprint arXiv:1902.02880},
  year={2019}
}


@book{roberts2022principles,
  title={The principles of deep learning theory},
  author={Roberts, Daniel A and Yaida, Sho and Hanin, Boris},
  year={2022},
  publisher={Cambridge University Press Cambridge, MA, USA}
}

@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{geiger2020scaling,
  title={Scaling description of generalization with number of parameters in deep learning},
  author={Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and d’Ascoli, St{\'e}phane and Biroli, Giulio and Hongler, Cl{\'e}ment and Wyart, Matthieu},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2020},
  number={2},
  pages={023401},
  year={2020},
  publisher={IOP Publishing}
}

@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  pages={2388--2464},
  year={2019},
  organization={PMLR}
}


@article{rotskoff2022trainability,
  title={Trainability and accuracy of artificial neural networks: An interacting particle system approach},
  author={Rotskoff, Grant and Vanden-Eijnden, Eric},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={9},
  pages={1889--1935},
  year={2022},
  publisher={Wiley Online Library}
}

@article{lee2020finite,
  title={Finite versus infinite neural networks: an empirical study},
  author={Lee, Jaehoon and Schoenholz, Samuel and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15156--15172},
  year={2020}
}

@article{yang2022tensor,
  title={Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2203.03466},
  year={2022}
}

@article{geiger2020disentangling,
  title={Disentangling feature and lazy training in deep neural networks},
  author={Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2020},
  number={11},
  pages={113301},
  year={2020},
  publisher={IOP Publishing}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}


@article{bahri2021explaining,
  title={Explaining neural scaling laws},
  author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
  journal={arXiv preprint arXiv:2102.06701},
  year={2021}
}

@article{gilboa2019wider,
  title={Wider networks learn better features},
  author={Gilboa, Dar and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:1909.11572},
  year={2019}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{dyerasymptotics,
  title={Asymptotics of Wide Networks from Feynman Diagrams},
  author={Dyer, Ethan and Gur-Ari, Guy},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{chen2020dynamical,
  title={A dynamical central limit theorem for shallow neural networks},
  author={Chen, Zhengdao and Rotskoff, Grant and Bruna, Joan and Vanden-Eijnden, Eric},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22217--22230},
  year={2020}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{yang2021tensor,
  title={Tensor programs iv: Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J},
  booktitle={International Conference on Machine Learning},
  pages={11727--11737},
  year={2021},
  organization={PMLR}
}

@article{bordelon2022self,
  title={Self-consistent dynamical field theory of kernel evolution in wide neural networks},
  author={Bordelon, Blake and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2205.09653},
  year={2022}
}

@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@article{bordelon2023dynamics,
  title={Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks},
  author={Bordelon, Blake and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2304.03408},
  year={2023}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@inproceedings{bordelon2020spectrum,
  title={Spectrum dependent learning curves in kernel regression and wide neural networks},
  author={Bordelon, Blake and Canatar, Abdulkadir and Pehlevan, Cengiz},
  booktitle={International Conference on Machine Learning},
  pages={1024--1034},
  year={2020},
  organization={PMLR}
}

@inproceedings{loureiro2022fluctuations,
  title={Fluctuations, bias, variance \& ensemble of learners: Exact asymptotics for convex losses in high-dimension},
  author={Loureiro, Bruno and Gerbelot, C{\'e}dric and Refinetti, Maria and Sicuro, Gabriele and Krzakala, Florent},
  booktitle={International Conference on Machine Learning},
  pages={14283--14314},
  year={2022},
  organization={PMLR}
}

@article{bodin2021model,
  title={Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model},
  author={Bodin, Antoine and Macris, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21605--21617},
  year={2021}
}

@article{kunin2024get,
  title={Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning},
  author={Kunin, Daniel and Ravent{\'o}s, Allan and Domin{\'e}, Cl{\'e}mentine and Chen, Feng and Klindt, David and Saxe, Andrew and Ganguli, Surya},
  journal={arXiv preprint arXiv:2406.06158},
  year={2024}
}

@inproceedings{
tu2024mixed,
title={Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes},
author={Zhenfeng Tu and Santiago Aranguri and Arthur Jacot},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=9zQl27mqWE}
}

@inproceedings{canatar2022kernel,
  title={A kernel analysis of feature learning in deep neural networks},
  author={Canatar, Abdulkadir and Pehlevan, Cengiz},
  booktitle={2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={1--8},
  year={2022},
  organization={IEEE}
}

@inproceedings{baratin2021implicit,
  title={Implicit regularization via neural feature alignment},
  author={Baratin, Aristide and George, Thomas and Laurent, C{\'e}sar and Hjelm, R Devon and Lajoie, Guillaume and Vincent, Pascal and Lacoste-Julien, Simon},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2269--2277},
  year={2021},
  organization={PMLR}
}

@inproceedings{kornblith2019similarity,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={3519--3529},
  year={2019},
  organization={PMLR}
}


@article{chizat2024infinite,
  title={Infinite-width limit of deep linear neural networks},
  author={Chizat, L{\'e}na{\"\i}c and Colombo, Maria and Fern{\'a}ndez-Real, Xavier and Figalli, Alessio},
  journal={Communications on Pure and Applied Mathematics},
  volume={77},
  number={10},
  pages={3958--4007},
  year={2024},
  publisher={Wiley Online Library}
}

@article{gerbelot2022rigorous,
  title={Rigorous dynamical mean field theory for stochastic gradient descent methods},
  author={Gerbelot, Cedric and Troiani, Emanuele and Mignacco, Francesca and Krzakala, Florent and Zdeborova, Lenka},
  journal={arXiv preprint arXiv:2210.06591},
  year={2022}
}

@article{loureiro2021learning,
  title={Learning curves of generic features maps for realistic datasets with a teacher-student model},
  author={Loureiro, Bruno and Gerbelot, Cedric and Cui, Hugo and Goldt, Sebastian and Krzakala, Florent and Mezard, Marc and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18137--18151},
  year={2021}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{mei2022generalization,
  title={The generalization error of random features regression: Precise asymptotics and the double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={4},
  pages={667--766},
  year={2022},
  publisher={Wiley Online Library}
}

@article{maloney2022solvable,
  title={A solvable model of neural scaling laws},
  author={Maloney, Alexander and Roberts, Daniel A and Sully, James},
  journal={arXiv preprint arXiv:2210.16859},
  year={2022}
}

@article{spigler2020asymptotic,
  title={Asymptotic learning curves of kernel methods: empirical data versus teacher--student paradigm},
  author={Spigler, Stefano and Geiger, Mario and Wyart, Matthieu},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2020},
  number={12},
  pages={124001},
  year={2020},
  publisher={IOP Publishing}
}

@inproceedings{adlam2020neural,
  title={The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization},
  author={Adlam, Ben and Pennington, Jeffrey},
  booktitle={International Conference on Machine Learning},
  pages={74--84},
  year={2020},
  organization={PMLR}
}

@article{pacelli2023statistical,
  title={A statistical mechanics framework for Bayesian deep neural networks beyond the infinite-width limit},
  author={Pacelli, R and Ariosto, S and Pastore, Mauro and Ginelli, F and Gherardi, Marco and Rotondo, Pietro},
  journal={Nature Machine Intelligence},
  volume={5},
  number={12},
  pages={1497--1507},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{baglioni2024predictive,
  title={Predictive power of a bayesian effective action for fully connected one hidden layer neural networks in the proportional limit},
  author={Baglioni, P and Pacelli, R and Aiudi, R and Di Renzo, F and Vezzani, A and Burioni, R and Rotondo, P},
  journal={Physical Review Letters},
  volume={133},
  number={2},
  pages={027301},
  year={2024},
  publisher={APS}
}

@article{aiudi2025local,
  title={Local kernel renormalization as a mechanism for feature learning in overparametrized convolutional neural networks},
  author={Aiudi, R and Pacelli, R and Baglioni, P and Vezzani, A and Burioni, R and Rotondo, P},
  journal={Nature Communications},
  volume={16},
  number={1},
  pages={568},
  year={2025},
  publisher={Nature Publishing Group UK London}
}

@article{bassetti2024feature,
  title={Feature learning in finite-width Bayesian deep linear networks with multiple outputs and convolutional layers},
  author={Bassetti, Federico and Gherardi, Marco and Ingrosso, Alessandro and Pastore, Mauro and Rotondo, Pietro},
  journal={arXiv preprint arXiv:2406.03260},
  year={2024}
}


@article{hu2022universality,
  title={Universality laws for high-dimensional learning with random features},
  author={Hu, Hong and Lu, Yue M},
  journal={IEEE Transactions on Information Theory},
  volume={69},
  number={3},
  pages={1932--1964},
  year={2022},
  publisher={IEEE}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@article{simon2021eigenlearning,
  title={The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks},
  author={Simon, James B and Dickens, Madeline and Karkada, Dhruva and DeWeese, Michael R},
  journal={arXiv preprint arXiv:2110.03922},
  year={2021}
}

@article{simon2023more,
  title={More is Better in Modern Machine Learning: when Infinite Overparameterization is Optimal and Overfitting is Obligatory},
  author={Simon, James B and Karkada, Dhruva and Ghosh, Nikhil and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2311.14646},
  year={2023}
}

@article{hastie2022surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={Annals of statistics},
  volume={50},
  number={2},
  pages={949},
  year={2022},
  publisher={NIH Public Access}
}

@inproceedings{
atanasov2023onset,
title={The Onset of Variance-Limited Behavior for Networks in the Lazy and Rich Regimes},
author={Alexander Atanasov and Blake Bordelon and Sabarish Sainathan and Cengiz Pehlevan},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=JLINxPOVTh7}
}

@article{hanin2019finite,
  title={Finite depth and width corrections to the neural tangent kernel},
  author={Hanin, Boris and Nica, Mihai},
  journal={arXiv preprint arXiv:1909.05989},
  year={2019}
}

@article{sorscher2022beyond,
  title={Beyond neural scaling laws: beating power law scaling via data pruning},
  author={Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={19523--19536},
  year={2022}
}

@article{vyas2023feature,
  title={Feature-Learning Networks Are Consistent Across Widths At Realistic Scales},
  author={Vyas, Nikhil and Atanasov, Alexander and Bordelon, Blake and Morwani, Depen and Sainathan, Sabarish and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2305.18411},
  year={2023}
}

@article{muennighoff2023scaling,
  title={Scaling Data-Constrained Language Models},
  author={Muennighoff, Niklas and Rush, Alexander M and Barak, Boaz and Scao, Teven Le and Piktus, Aleksandra and Tazi, Nouamane and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin},
  journal={arXiv preprint arXiv:2305.16264},
  year={2023}
}

@inproceedings{SmithDBD21,
  author    = {Samuel L. Smith and
               Benoit Dherin and
               David G. T. Barrett and
               Soham De},
  title     = {On the Origin of Implicit Regularization in Stochastic Gradient Descent},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=rq\_Qr0c1Hyo},
  timestamp = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/SmithDBD21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{BarrettD21,
  author    = {David G. T. Barrett and
               Benoit Dherin},
  title     = {Implicit Gradient Regularization},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=3q5IqUrkcF},
  timestamp = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/BarrettD21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{helias2020statistical,
  title={Statistical field theory for neural networks},
  author={Helias, Moritz and Dahmen, David},
  volume={970},
  year={2020},
  publisher={Springer}
}


@inproceedings{FortDPK0G20,
  author    = {Stanislav Fort and
               Gintare Karolina Dziugaite and
               Mansheej Paul and
               Sepideh Kharaghani and
               Daniel M. Roy and
               Surya Ganguli},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {Deep learning versus kernel learning: an empirical study of loss landscape
               geometry and the time evolution of the Neural Tangent Kernel},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  timestamp = {Tue, 19 Jan 2021 15:57:37 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/FortDPK0G20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{shan2021theory,
  title={A theory of neural tangent kernel alignment and its influence on training},
  author={Shan, Haozhe and Bordelon, Blake},
  journal={arXiv preprint arXiv:2105.14301},
  year={2021}
}

@article{cui2023error,
  title={Error scaling laws for kernel classification under source and capacity conditions},
  author={Cui, Hugo and Loureiro, Bruno and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={Machine Learning: Science and Technology},
  volume={4},
  number={3},
  pages={035033},
  year={2023},
  publisher={IOP Publishing}
}

@article{crisanti2018path,
  title={Path integral approach to random neural networks},
  author={Crisanti, A and Sompolinsky, H},
  journal={Physical Review E},
  volume={98},
  number={6},
  pages={062120},
  year={2018},
  publisher={APS}
}

@article{cammarata2020thread,
  author = {Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris and Petrov, Michael and Schubert, Ludwig and Voss, Chelsea and Egan, Ben and Lim, Swee Kiat},
  title = {Thread: Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits},
  doi = {10.23915/distill.00024}
}

@misc{liu2022convnet,
      title={A ConvNet for the 2020s}, 
      author={Zhuang Liu and Hanzi Mao and Chao-Yuan Wu and Christoph Feichtenhofer and Trevor Darrell and Saining Xie},
      year={2022},
      eprint={2201.03545},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{bordelon2024dynamical,
  title={A dynamical model of neural scaling laws},
  author={Bordelon, Blake and Atanasov, Alexander and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2402.01092},
  year={2024}
}
@article{paquette20244+,
  title={4+ 3 Phases of Compute-Optimal Neural Scaling Laws},
  author={Paquette, Elliot and Paquette, Courtney and Xiao, Lechao and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:2405.15074},
  year={2024}
}

@article{lin2024scaling,
  title={Scaling Laws in Linear Regression: Compute, Parameters, and Data},
  author={Lin, Licong and Wu, Jingfeng and Kakade, Sham M and Bartlett, Peter L and Lee, Jason D},
  journal={arXiv preprint arXiv:2406.08466},
  year={2024}
}

@article{everett2024scaling,
  title={Scaling Exponents Across Parameterizations and Optimizers},
  author={Everett, Katie and Xiao, Lechao and Wortsman, Mitchell and Alemi, Alexander A and Novak, Roman and Liu, Peter J and Gur, Izzeddin and Sohl-Dickstein, Jascha and Kaelbling, Leslie Pack and Lee, Jaehoon and others},
  journal={arXiv preprint arXiv:2407.05872},
  year={2024}
}