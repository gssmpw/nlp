%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{bm}

\def\h{\bm h}
\def\W{\bm W}
\def\w{\bm w}
\def\x{\bm x}
\def\g{\bm g}
\def\Q{\bm Q}
\def\M{\bm M}
\def\D{\bm D}
\def\I{\bm I}
\def\v{\bm v}
\def\a{\bm a}
\def\A{\bm A}
\def\mR{\mathcal R}
\def\mH{\mathcal H}
\def\mG{\mathcal G}
\def\mC{\mathcal C}


\newcommand{\cp}[1]{\textcolor{red}{[Cengiz: #1]}}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Deep Linear Network Training Dynamics from Random Initialization: Data, Width, Depth, and Hyperparameter Transfer}

\begin{document}

\twocolumn[
\icmltitle{Deep Linear Network Training Dynamics from Random Initialization: Data, Width, Depth, and Hyperparameter Transfer }

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Blake Bordelon}{seas,cbs,kempner}
\icmlauthor{Cengiz Pehlevan}{seas,cbs,kempner}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{seas}{John Paulson School of Engineering and Applied Sciences, Harvard University}
\icmlaffiliation{cbs}{Center for Brain Sciences}
\icmlaffiliation{kempner}{Kempner Institute}

\icmlcorrespondingauthor{Blake Bordelon}{blake\_bordelon@g.harvard.edu}
\icmlcorrespondingauthor{Cengiz Pehlevan}{cpehlevan@seas.harvard.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We theoretically characterize gradient descent dynamics in deep linear networks trained at large width from random initialization and on large quantities of random data. Our theory captures the ``wider is better" effect of mean-field/maximum-update parameterized networks as well as hyperparameter transfer effects, which can be contrasted with the neural-tangent parameterization where optimal learning rates shift with model width. We provide asymptotic descriptions of both non-residual and residual neural networks, the latter of which enables an infinite depth limit when branches are scaled as $1/\sqrt{\text{depth}}$. We also compare training with one-pass stochastic gradient descent to the dynamics when training data are repeated at each iteration. Lastly, we show that this model recovers the accelerated power law training dynamics for power law structured data in the rich regime observed in recent works. 
\end{abstract}
\vspace{-15pt}
\section{Introduction}
\label{submission}
Understanding the training dynamics and scaling properties of neural networks is critical to deriving parameterizations and optimizers that achieve stable dynamics across model sizes \cite{yang2021tuning,  bordelon2024depthwise,  yang2023tensor,vankadarafeature, everett2024scaling}, which in turn lead to significant computational savings in hyperparameter search via optimal hyperparameter transfer from smaller to larger sized models \cite{achiam2023gpt}. However, characterizing the training dynamics of neural networks is generally complicated due to nonlinear dynamics that depend on the parameterization of the network, the weight initialization strategy, optimizer, and data structure. 

Despite the complexity of the dynamics induced by gradient descent from random initial conditions, significant theoretical insight into deep network training can be gained by studying tractable limits (large width or large depth networks) or specialized architectures (such as deep linear networks) or the behavior of training under specific data distributions (such as random isotropic data). While the literature on various special cases of training contains numerous works which we review in Section \ref{sec:related_works} below, a theoretical model that can capture simultaneously the effect of varying width, depth, richness, data quantity on the learning dynamics in randomly initialized networks is missing. Of specific interest is that no existing theory can theoretically characterize the two effects necessary for successful hyperparameter transfer \cite{yang2021tuning}: (1) monotonic improvement in performance with model width or model depth and (2) approximately consistent optimal learning rates when appropriately parameterizing the model and optimizer.



\textbf{\textit{Is there a simple theoretical model of deep networks that captures the influence of random initialization, network parameterization, width, and depth in order to characterize the hyperparameter transfer effect?}}

In this work, we provide such a theory, though for a limited model class of deep linear networks, by characterizing the training dynamics from random initialization on random data which captures both the effect of network width, depth, dataset size, and feature learning strength (richness). Concretely our contributions are 
\vspace{-8pt}
\begin{itemize}
    \item We develop a novel dynamical mean field theory (DMFT) which is specific for deep linear neural networks that enables a prediction of typical case loss and representation trajectories over both initial weights and random data, or stochastic gradient descent (SGD) samples, as the disorder. This theory completely characterizes the train and test loss dynamics in terms of a closed set of equations for correlation and response functions at each hidden layer of the network. 
    \item We show that our theory provides a minimal model that captures the impact of parameterization on training dynamics. In particular, our results capture both the ``wider is better" effect of deep networks throughout training as well as the hyperparameter transfer effect for networks in $\mu$P. Conversely, our theory captures that wider networks can train more slowly and hyperparameters can fail to transfer in NTK parameterization. 
    \item We extend our theory to provide a solution to the training dynamics of deep \textit{residual networks}, enabling the study of the large depth limit, though in a tractable setting. In this setting, we reproduce that hyperparameter transfer across depths by scaling the residual branch by $1/\sqrt{\text{depth}}$, consistent with prior works. 
    \item We also show that our theory can capture the behavior of linear networks trained on structured data, such as power-law covariance eigenvalues. This results in power law scaling of the loss with training iterations, though with improved exponents compared to lazy learning, consistent with recent findings in the shallow case \cite{bordelon2024featurelearningimproveneural}. 
\end{itemize}

\vspace{-12pt}
\subsection{Related Works}\label{sec:related_works}
\vspace{-5pt}
\paragraph{Statics and Bayesian Networks} Theory of the statics\footnote{ This could be interpreted as sampling a neural network from an equilibrium distribution, which can be contrasted with out-of-equilibrium dynamics which are our focus in this work. } of deep Bayesian linear neural networks in the regime where dataset $P$ and width $N$ are proportionally large were first characterized in \citet{li2021statistical}, resulting in a theory of a renormalized kernel after training. \citet{pacelli2023statistical} and \citet{baglioni2024predictive} theoretically and experimentally argue that the scale-renormalization of the NNGP kernel continues to capture the effect of feature learning in \textit{nonlinear} Bayesian networks. Their results were recently extended to convolutional networks \cite{aiudi2025local, bassetti2024feature}. Learning curves which average over a randomly sampled training set were obtained by \citet{zavatone2022contrasting} with the replica method, allowing contrast between Bayesian inference over all layers with training random feature models, who extended the theory to structured covariates \cite{zavatoneveth2023learning}. \citet{cui2023bayes} showed that these learning curves also describe learning curves for \textit{nonlinear} multilayer Perceptrons (MLPs) under Bayes-optimal teacher-student learning setup in the proportional limit. \citet{hanin2024bayesian} used similar techniques to analyze weakly nonlinear Bayesian networks, where the nonlinearity is depth dependent, at large width $N$, depth $L$ and dataset size $P$ with $\frac{L P}{N}$ fixed.

\vspace{-10pt}
\paragraph{Dynamics from Random Initialization} In the lazy learning regime, the infinite limit of a randomly initialized network is characterized by the Neural Tangent Kernel (NTK) \cite{jacot2018neural, lee2019wide}. However, if one adopts a scaling that enables feature learning at infinite width (mean field / $\mu$P scaling) then the dynamics of deep network training from random initialization are more complicated. The limiting $N \to \infty$ gradient descent dynamics in MLPs for a fixed (finite) stream of data in the rich regime have been characterized with partial differential equations in one-hidden layer networks \cite{mei2019mean, rotskoff2022trainability}  as well as Tensor programs \cite{yang2021tensor} and dynamical mean field theory \cite{bordelon2022self} techniques for deeper networks. These methods describe the typical case behavior of training nonlinear networks over random realizations of the initial weights. However, the resulting theories are generally intractable as they scale poorly with the size of the training dataset $P$ and the number of steps of training $T$ outside of the lazy training regime. A key challenge in this deep setting is to also average over the random data to obtain a reduced description where effects of order $\frac{ P }{ N }$ are preserved. Various perturbative approximations $\frac{1}{N}$ have been studied, though these do not capture a full joint limit for data and width and also suffer from intractability at large dataset sizes \cite{dyerasymptotics, roberts2022principles, hanin2019finite, bordelon2023dynamics}.

\vspace{-8pt}
\paragraph{Residual Networks and Large Depth Limits} Theory to characterize infinite width and depth training dynamics $N \to \infty$ and $L \to \infty$ have been recently obtained for networks starting at random initial conditions \citet{bordelon2024depthwise, yang2023tensor}.  More recently, the work of 
\citet{bordelon2024featurelearningimproveneural} studied depth $L = 2$ finite width linear networks trained with a projected gradient descent using mean field techniques. In this present work, we characterize exact gradient descent for arbitrary depth $L$ networks. We also study the role of $\mu$P scaling to achieve learning rate transfer across width and scaled residual branches \cite{bordelon2024dynamical, yang2023tensor} for transfer across depths. 

\vspace{-8pt}
\paragraph{Linear Network Gradient Flow ODEs} The dynamics of gradient flow training in deep linear networks in the very rich regime (or equivalently for small initialization) were computed by \citet{saxe2013exact, atanasov2022neural} as a simple set of ordinary differential equations.  \citet{kunin2024get} and \citet{domine2024lazy} recently derived similar equations for the dynamics of networks at different laziness/richness scale and unbalanced initializations. However, these methods require a number of assumptions including whitened input data, a balance assumption on products of weights in adjacent layers, and lack of bottleneck or overparameterized layers. Our work goes beyond these assumptions by studying generic (random and non-negligible) initialization of the network weights, which introduce interesting dependence of the dynamics on network width and dataset size. In addition, our theory allows for sampling random training data, which will generally not give a whitened empirical covariance\footnote{Even if the data's population distribution is isotropic, the empirical eigenvalues will follow a Marchenko-Pastur law.}. 

\vspace{-10pt}
\paragraph{Linear Network Models of Neural Scaling Laws.} \citet{bordelon2024dynamical} and \citet{paquette20244+} recently analyzed a linear random feature model trained with gradient descent, finding that training dynamics on data that satisfies source and capacity conditions recovers Chinchilla-like neural scaling laws \cite{hoffmann2022training}. \citet{bordelon2024featurelearningimproveneural} recently extended this theory to a case where the random projection matrix is also updated with projected gradient descent, resulting in a faster power law convergence rate for hard tasks. Their model is mathematically equivalent to a one hidden layer linear network trained with a projected version gradient descent. We find this same acceleration behavior in \textit{deeper} linear networks trained on power law data in this work.  

\vspace{-10pt}
\paragraph{DMFT Approaches to Machine Learning Theory} 
Dynamical mean field theory methods, originally developed in the physics of spin glasses, have been used to describe the dynamics of high dimensional optimization (including stochastic gradient methods) in terms of single variable stochastic integro-differential equations \cite{agoritsas2018out, mannelli2019passed, mignacco2020dynamical, mignacco2022effective, gerbelot2022rigorous, dandi2023how, bordelon2024dynamical, bordelon2024featurelearningimproveneural}. These prior works have shown the validity of DMFT to characterizing general linear models or shallow neural networks trained with parameters and data going to infinity proportionally. In our work the disorder comes from \textit{both the data matrix and the random initial weights} of the network and we are interested in the behavior of arbitrary depth networks. 

\vspace{-10pt}
\section{Deep Linear Network with Random Data}

\vspace{-5pt}

The first model that we study is a vanilla feedforward linear MLP which has $L$ hidden layers of neurons (and $L+1$ trainable weights) which maps input $\x \in \mathbb{R}^D$ to outputs $f(\x)$ defined as
\begin{align}
    f(\x) = \frac{1}{N \gamma_0} \left( \w^{L } \right)^\top \prod_{\ell=1}^{L-1} \left( \frac{1}{\sqrt N} \bm W^\ell \right)  \left( \frac{1}{\sqrt D} \bm W^0 \right) \x .
\end{align}
The first matrix has dimensions $\bm W^0 \in \mathbb{R}^{N \times D}$, while all intermediate weight matrices are all of size $\W^{\ell} \in \mathbb{R}^{N \times N}$. Lastly, the output $\w^L \in \mathbb{R}^N$ maps to a single output. We can provide extensions to unmatched hidden widths, structured data, and multiple (finite) output classes in Appendix \ref{app:diff_rel_widths, app:structured_data, app:multiple_outputs}, but we begin our analysis with this simplest possible case. We start by characterizing the dynamics for a random training dataset $\mathcal D = \{ \x_\mu, y_\mu \}_{\mu=1}^P$ of size $P$, and random initial weights
\begin{align}
    \x_{\mu } \sim \mathcal{N}(0,\bm I) \ , \ w^L_i , W^\ell_{ij}, W^0_{ij} \sim \mathcal{N}(0,1)  .
\end{align}
We consider a linear (potentially noisy) target function of the form
\begin{align}
    y(\x) = \frac{1}{\sqrt D} \bm w_\star \cdot \bm x + \sigma \epsilon(\x) 
\end{align}
where $\left< \epsilon(\x) \right> =0$ and $\left< \epsilon(\x)^2 \right> = 1$. All parameters follow gradient descent dynamics on the square loss with the $\mu$P scaling of the update from gradient descent which takes the form
\begin{align}
    \W^{\ell}(t+1) = \W^{\ell}(t) + \frac{\eta \gamma_0}{\sqrt N} \ \g^{\ell+1}(t) \h^\ell(t)^\top ,
\end{align}
where we introduced the following vectors
\begin{align}
    &\h^{\ell+1}(t) = \frac{1}{\sqrt N} \W^{\ell}(t) \h^\ell(t) \ , \ \h^0(t) = \frac{\sqrt D}{P} \sum_{\mu=1}^P \Delta_\mu(t) \x_\mu \nonumber
    \\
    &\g^{\ell}(t) = \frac{1}{\sqrt N} \W^{\ell}(t)^\top \g^{\ell+1}(t) \ , \ \bm g^L(t)= \w^L(t).
\end{align}
and the instantaneous training errors $\Delta_\mu(t) = y(\x_\mu) - f(\x_\mu,t)$. We note that the vectors $\h^\ell(t)$ defined above differ from the preactivations used in the DMFT of \citet{bordelon2022self} since ours are \textit{training-set averaged} quantities which are computed as a correlation between the errors $\Delta_\mu$ and the usual per-sample preactivation vectors. The fact that sums over training points and sums over neurons can be reordered in linear networks enables easy analysis of a proportional limit, where width, data, and input dimension are all comparable, rather than only computing the $N \to \infty$ limit at fixed $P, D$. In addition to capturing better finite width and finite data effects, this theory leads to significantly simpler and easier-to-compute description of training dynamics as we illustrate in Appendix Table \ref{tab:comp_costs}. 

\vspace{-10pt}
\subsection{Dynamics of (S)GD in a Joint Asymptotic Limit}
\vspace{-5pt}

\begin{figure*}[t]
    \centering
    \subfigure[Varying Width in NTK-P]{\includegraphics[width=0.325\linewidth]{figures/ntk_param_two_layer_vary_nu.pdf}}
    \subfigure[Varying Width in MF/$\mu$P]{\includegraphics[width=0.325\linewidth]{figures/MF_param_two_layer_vary_nu.pdf}}
    \subfigure[Varying MLP Depth]{\includegraphics[width=0.325\linewidth]{figures/MF_param_deep_vary_L.pdf}}
    \caption{Our theory captures the role of width, depth and dataset size on the test loss $\mathcal L(t)$ and train loss $\hat{\mathcal L}(t)$ dynamics. The behavior of different models across widths depends on how $\gamma_0$ scales with $N$. (a) Test loss of a $L=4$ network in NTK parameterization where $\gamma_0 \sim \Theta(\frac{1}{\sqrt \nu})$, which results in a kernel method (linear model) in the $\nu \to \infty$ limit. (b) In MFP/$\mu$P, the parameter $\gamma_0 = \Theta(1)$ and the convergence rate improves as $\nu$ increases.  (c) Our theory allows us to track dynamics in deeper networks of arbitrary depth $L$ as well.}
    \label{fig:ntk_vs_MF}
\end{figure*}

In this work we consider two distinct setting that allow us to identify different effects from subsampling data. First, we examine full batch gradient descent with $P$ data points that are reused at each step. In this setting, we study a large system size limit with $P, D, N \to \infty$ limit where the ratios are constants 
\begin{align}
\text{(Full Batch GD)} \quad  P/D \equiv \alpha \quad , \quad  N/D \equiv \nu 
\end{align}
Analyzing fixed points of GD in the above limit is a popular setting for two layer random feature models \cite{mei2022generalization, adlam2020neural,hu2022universality} and deep Bayesian networks at equilibrium \cite{li2021statistical, zavatone2022contrasting, cui2023bayes} but the training dynamics from a random initial condition in such a limit has not been explored\footnote{The $\nu \to \infty$ limit would recover to the commonly studied ``infinite width limits" where width is taken to infinity first at fixed $P,D$ that disregards effects of order $N/D$ or $N/P$ \cite{jacot2018neural, lee2019wide, bordelon2022self}.}. In addition to full batch gradient descent training, we also look at online stochastic gradient descent (SGD) training, where at each step a fresh batch of $B$ data points are sampled and used to estimate the gradient with the following proportional limit
\begin{align}
\text{(Online SGD)} \quad  B/D \equiv \alpha_B \quad , \quad  N/D \equiv \nu . 
\end{align}
These two settings (full batch GD and online SGD) both converge to the dynamics of gradient descent on the population loss as $\alpha \to \infty$ or $\alpha_B \to \infty$ respectively. In either of these settings, large system size limits are taken with the number of iterations $t$ fixed, which is appropriate since all learning timescales are $\mathcal{O}(1)$.

The key quantities which determine the generalization dynamics in both of our settings are correlation functions
\begin{align}
    &C_h^\ell(t,t') =  \frac{1}{N} \h^\ell(t) \cdot \h^\ell(t') \ , \ C_g^\ell(t,t') = \frac{1}{N} \g^\ell(t) \cdot \g^\ell(t') \nonumber
    \\
    &C_\Delta(t,t') = \frac{1}{P} \bm \Delta(t) \cdot \bm \Delta(t') \ , \ C_v(t,t') = \frac{1}{D} \v(t) \cdot \v(t') ,
\end{align}
where the error vector $\v(t)$ is defined as 
\begin{align}
\v(t) = \w^\star - \frac{\sqrt D}{N \gamma_0 } \W^1(t)^\top \g^1(t) .
\end{align}
From the correlation functions, we can directly obtain the test loss $\mathcal L(t)$ and train loss $\hat{\mathcal L}(t)$ as
\begin{align}
    \mathcal L(t) = C_v(t,t) + \sigma^2 \ , \ \mathcal{\hat L}(t) = C_\Delta(t,t) .
\end{align}
In addition to the correlation functions, the theory also requires computing a collection of response functions $\{ R_{vu}^0, R_{vr}^0, R_\Delta, R_{hr}^0, ...  R_{hr}^{L-1}, R_{gu}^1, ..., R_{gu}^L  \}$ which capture the effect of limited data and limited width from finite $\alpha$ and finite $\nu$ effects respectively as well as interactions between the forward/backward $\h^\ell, \g^\ell$ variables (see Appendix \ref{app:dmft_derivation}). In the joint limit, each entry of the vectors $\{\v(t), \bm\Delta(t) , \h^\ell(t) , \g^\ell(t) \}$ become statistically independent and identically distributed following a stochastic process known as the single-site process. As an example, the intermediate layers of the network $1 < \ell < L$ satisfy the following single site (scalar) processes
\begin{align}
    h^\ell(t) = u^\ell(t) + \sum_{t'<t} \left[ R^{\ell-1}_{hr}(t,t') + \eta\gamma_0 C^{\ell-1}_h(t,t')  \right] g^\ell(t') \nonumber
    \\
    g^\ell(t) = r^\ell(t) + \sum_{t'<t} \left[ R^{\ell+1}_{gu}(t,t') + \eta\gamma_0 C^{\ell+1}_g(t,t')  \right] h^\ell(t')
\end{align}
where $u^\ell(t) \sim \mathcal{N}(0,C^{\ell-1}_h)$ and $r^\ell(t) \sim \mathcal{N}(0, G^{\ell+1})$ are colored noise sources. The response functions are computed as derivatives of these single site variables with respect to their noise source
\begin{align}
    R^\ell_{hr}(t,t') = \left< \frac{\delta h(t)}{\delta r^\ell(t')} \right> \ , \ R^\ell_{gu}(t,t') = \left< \frac{\delta g^\ell(t)}{\delta u^\ell(t')} \right>.
\end{align}
Intuitively, these functions measure the response of the variables at time $t$ to a ``kick" in the noise source at a time $t'$ (due to causality, these only contribute for $t' < t$). We provide the complete set of DMFT equations in Appendix \ref{app:final_DMFT_eqns}, which form a closed set of equations for the correlation and response functions. Once these are solved for, the the test error takes the form
%
\begin{align}
   \mathcal{L}(t) &=  \underbrace{\sum_{s<t} \sum_{s'<t} R_{vr}^0(t,s) R^0_{vr}(t,s')}_{\text{Bias}}  +  \underbrace{\sigma^2}_{\text{Label Noise}} \nonumber
   \\
   &+ \underbrace{\frac{1}{\nu \gamma_0^2} \sum_{s<t} \sum_{s'<t} R_{vr}^0(t,s) R^0_{vr}(t,s') C_g^1(s,s') }_{\text{Limited Width Variance}} \nonumber
   \\
   &+ \underbrace{\frac{1}{\alpha} \sum_{s<t} \sum_{s'<t} R_{vu}^0(t,s) R_{vu}^0(t,s') C_\Delta(s,s')}_{\text{Limited Data Variance}}
\end{align}
\vspace{-5pt}
The last two lines capture the fluctuations in the entries of the error vector $\v(t)$ which are due to limited width (finite $\nu$) and limited data (finite $\alpha$) respectively while the first line would be the error dynamics of an ensembled and bagged model (the error of the average function learned over different initializations and different draws of training data) \cite{adlam2020understanding, bordelon2024dynamical}. The response function $R^0_{vr}$ also depends on both $\nu$ and $\alpha$ and captures the effect of finite $\nu$ and $\alpha$ on the bias dynamics, which persist even for an ensembled and bagged predictor.

\vspace{-10pt}
\paragraph{Varying Width and Parameterization} Our theory captures the effect of varying width through the constant $\nu \equiv N/D$. Depending on how the model is parameterized with respect to model width $N$, different behaviors are possible as $\nu$ varies in the proportional limit. In Figure \ref{fig:ntk_vs_MF} (a)-(b) we compare how models behave across different $\nu$ in NTK parameterization (where $\gamma_0 \sim \Theta\left( \nu^{-1/2} \right)$) compared to mean field parameterization where $\gamma_0$ is constant with respect to $\nu$. In NTK parameterization, the network approaches a linear model (in parameter space) as $\nu \to \infty$, whereas the dynamics remain non-linear in the $\nu \to \infty$ limit. In either case, the key equation which captures the effect of network width is the single-site equation for the firs layer $h^1(t)$ variables 
%
\begin{align}
    h^1(t) = &\underbrace{u^1(t)}_{\text{Gaussian Process}} + \underbrace{\frac{1}{\nu \gamma_0 } \sum_{t' < t} R_{hr}^0(t,t') g^1(t') }_{\text{Limited Width Effects}} \nonumber
    \\
    &+ \underbrace{\eta \gamma_0 \sum_{t' < t} C^0_h(t,t') g^1(t') }_{\text{Feature Learning Updates}}.
\end{align}

These limited width effects can propagate through the other layers in the forward and backward pass and ultimately bias the dynamics of the network predictions, leading to slower training. For mean field parameterization, the feature learning updates scale as $\sim \eta \gamma_0$ which is constant across model widths, while for NTK parameterization, the feature learning update scales as $\sim \eta \gamma / \sqrt{\nu}$ which decrease with model width. 
\vspace{-10pt}
\paragraph{Learning Rate Transfer}
Experiments from \citet{yang2021tuning} illustrate that the optimal learning rate is approximately preserved across model scales in the $\mu$P scaling, while \citet{noci2024learning} show experiments where optimal learning rates shift to the right as width increases in NTK parameterization. Our theory can capture both of these effects as we show in Figure \ref{fig:hp_transfer_width}. The key effect is that, at fixed learning rate $\eta$, the rate of change of the kernel decays with network width $\nu$ in NTK parameterization, while it is approximately constant in $\mu$P. 
\begin{figure}[t]
    \centering
    \subfigure[Learning Rate Sweep in NTK-P]{\includegraphics[width=0.8\linewidth]{figures/LR_transfer_NTK_two_layer_DMFT_gamma_1.2_T_50.pdf}}
    \subfigure[Learning Rate Sweep in $\mu$P]{\includegraphics[width=0.8\linewidth]{figures/LR_transfer_two_layer_DMFT_gamma_2.5_T_50.pdf}}
    \caption{The theory can capture the failure of hyperparameter transfer in NTK parameterization and the success of hyperparameter transfer in $\mu$P networks. (a) The optimal learning rate increases with model width $\nu = N/D$ in NTK parameterization. This is due to the fact that the effective feature learning rate $\gamma_0$ is decreasing as $1/\sqrt{\nu}$. (b) The optimal learning rate is approximately preserved across $\nu = N/D$ in $\mu$P networks where $\gamma_0$ is constant, enabling learning rate transfer.   }
    \label{fig:hp_transfer_width}
\end{figure}

\vspace{-10pt}
\paragraph{Effect of Limited Data or Batch Size} The effect of limited data on the dynamics generates a memory term for the training error $\Delta(t)$ response function 
\begin{align}
    R_{\Delta}(t,t') = \delta(t-t') + \underbrace{\frac{1}{\alpha} \sum_{t''<t} R_{vu}^0(t,t'') R_{\Delta}(t'',t') }_{\text{Limited Data Bias}} , 
\end{align}
which results in a slow buildup in the gap between train and test loss dynamics as we illustrate in in Figure \ref{fig:vary_data} (a). When data is repeated for many iterations, the loss can exhibit overfitting peaks \cite{d2020double, mei2022generalization, advani2020high, zavatone2022contrasting} as we show in Figure \ref{fig:vary_data}. However, for online SGD training the loss is monotonically improving with model width $\nu$ and batchsize $B = \alpha_B D$ as we illustrate in Figure \ref{fig:vary_data} (c)-(d).  


\begin{figure*}[t]
    \centering
    \subfigure[Test Loss Varying Data $\gamma_0 = 2$]{\includegraphics[width=0.42\linewidth]{figures/MF_param_test_deep_vary_data.pdf}}
    %\subfigure[Train Loss Varying Data]{\includegraphics[width=0.32\linewidth]{figures/MF_param_train_deep_vary_data.pdf}}
    \subfigure[Emergence of Overfitting from Data Repetition ]{\includegraphics[width=0.42\linewidth]{figures/loss_vs_alpha_vary_t.pdf}}
    \subfigure[Online SGD Dynamics $\nu = 0.25$]{\includegraphics[width=0.42\linewidth]{figures/MF_param_SGD_deep_vary_batch.pdf}}
    \subfigure[SGD varying $\nu$ at Small $\alpha_B = 0.05$]{\includegraphics[width=0.42\linewidth]{figures/MF_param_SGD_deep_vary_nu.pdf}}
    \caption{Train and test losses of a depth $L= 4$ linear network versus dataset size $P = \alpha D$ compared to online SGD with batch size $B = \alpha_B D$. (a) Networks can fit the training data faster for smaller datasets $\alpha$. (b) The test error can be non-monotonic as a function of data $\alpha$ if training is performed too long. (c) The dynamics of online SGD with $\nu = 0.5$ across batchsizes $B = \alpha_B D$. Smaller batchsizes slow down training through accumulated SGD variance, but there is not a bottleneck error as $t \to \infty$ provided the dynamics are stable. (d) Increasing the width $\nu$ eliminates the initial overfitting transient.  }
    \label{fig:vary_data}
\end{figure*}


\vspace{-5pt}
\section{Residual Networks and Depth Limits}
\vspace{-5pt}

While we studied the large width limit at any fixed finite value of $L$, to obtain a well defined large depth $L \to \infty$ limit, we next turn to residual networks which improve faithful forward and backward signal propagation
\begin{align}
    f(\x) = \frac{1}{N \gamma_0} \left(\w^L \right)^\top \prod_{\ell=1}^{L-1} \left( \bm I + \frac{\beta }{\sqrt{N}} \bm W^\ell \right) \left(\frac{1}{\sqrt D} \bm W^0 \right) \x
\end{align}
We first provide the DMFT for the proportional $P,D,N$ limit for any finite $\beta$ and $L$. We show that unlike standard feedforward networks, residual networks require characterizing cross-layer correlation and response functions since the residual stream variables are statistically dependent. Once we have derived the proportional limit, we can then consider scaling $\beta = \beta_0 / \sqrt{L}$ for constant $\beta_0$, which recovers the parameterization recovers the depth-$\mu$P of \cite{bordelon2024depthwise, yang2023tensor}\footnote{Alternatively, if one commits to $\beta = \beta_0 / \sqrt{L}$, then one can take the $L \to \infty$ limit first and then take the proportional $N,P,D \to \infty$ limit, which gives the same result. }. 

We show our theory compared to experiments in Figure \ref{fig:res_nets} where we illustrate the impact of richness $\gamma_0$, branch multiplier $\beta_0$ and depth $L$ on the dynamics when we are adopting the $\beta = \frac{\beta_0}{\sqrt L}$ scaling that admits a large depth limit. Examples illustrating hyperparameter transfer are provided in Figure \ref{fig:depth_dynamics}, where keeping $\beta = \Theta(1)$ constant leads to diverging activity at fixed learning rate as $L \to \infty$ whereas scaling $\beta = \Theta( L^{-1/2} )$ admits a well defined limit. Thus hyperparameter transfer in the rich regime is maintained in the $\beta = \beta_0 / \sqrt{L}$ scaling.


\begin{figure*}[h]
    \centering
    \subfigure[Increasing Richness $L=16$]{\includegraphics[width=0.32\linewidth]{figures/vary_gamma_resnet_L_4.pdf}}
     \subfigure[Increasing Branch Scale $L=16$]{\includegraphics[width=0.32\linewidth]{figures/vary_beta_resnet_L_4.pdf}}
    \subfigure[Increasing Depth]{\includegraphics[width=0.32\linewidth]{figures/vary_L_resnet_converge_beta_1.2.pdf}}
    \caption{The training dynamics of large depth residual networks with random initialization. (a) Increasing $\gamma_0$ leads to an acceleration of training, but the initial kernel governing the updates are not changed so that $\frac{d}{dt} \mathcal L(t) |_{t=0}$ is preserved. (b) Increasing $\beta_0$ alters both the initial kernel and the feature learning dynamics. (c) Increasing depth $L$ with $\beta_0 = 1.2$ eventually converges to a limiting dynamics.  }
    \label{fig:res_nets}
\end{figure*}



\begin{figure}
    \centering
    %\subfigure[Non-Residual $\beta = 1$]{\includegraphics[width=0.32\linewidth]{figures/LR_transfer_depth_nonres_DMFT_gamma_2.0_T_50.pdf}}
    \subfigure[No Branch Scaling $\beta = 1$]{\includegraphics[width=0.85\linewidth]{figures/LR_depth_transfer_no_scale_DMFT_gamma_2.0_T_15.pdf}}
    \subfigure[Depth $\mu$P Scaling $\beta = \beta_0 / \sqrt{L}$]{\includegraphics[width=0.85\linewidth]{figures/LR_depth_transfer_scale_res_DMFT_gamma_1.5_T_15.pdf}}
    \caption{ Learning rate transfer fails for vanilla ResNets but succeeds for scaled resnets, consistent with theory (dashed lines). (a) ResNets of varying depth $L$ with constant raw branch scale $\beta = 1$. The optimal learning rate changes by several orders of magnitude. (b) ResNets with scaled branch $\beta =\frac{\beta_0 }{\sqrt L}$ with $\beta_0 = 1.25$. The optimal learning rate does not shift significantly as depth increases. }
    \label{fig:depth_dynamics}
\end{figure}

\vspace{-20pt}
\paragraph{Infinite Depth Limit} When scaling the branch multiplier as $\beta = \frac{\beta_0}{\sqrt L}$, the dynamics admit a $L \to \infty$ limit. This $L \to \infty$ limit is a simplified version of the result obtained in \citet{bordelon2024depthwise} where both the response functions and correlation functions survive in the $L \to \infty$ limit (see Appendix \ref{app:large_L_limit_resnet}). In this case, the residual stream can be described as a stochastic process over layer time $\tau = \ell / L$ driven by a Brownian motion term $du(t,\tau)$ and a feature learning correction that depends on correlation and response dynamics (to be integrated from $\tau = 0$ up to $\tau = 1$), giving single site dynamics 
\begin{align}
    &dh(\tau,t) = \beta_0 du(\tau,t) \nonumber
    \\
    &+ d\tau \beta_0^2 \sum_{t'<t} \left[ R_{hr}(\tau,t,t') + \eta \gamma_0 C_h(\tau,t,t')  \right] g(\tau,t') .
\end{align}
where the Brownian motion has covariance structure $\left< du(\tau,t) du(\tau',t') \right> = \delta(\tau-\tau') C_h(\tau,t,t') d\tau d\tau'$ which is local in layer time $\tau$ but non-local across training steps (see Appendix \ref{app:large_L_limit_resnet} for more detail). 

\vspace{-10pt}
\section{Structured Data \& Power Law Scalings}
\vspace{-5pt}

\begin{figure*}
    \centering
    \subfigure[SGD Noise $N = 1024$]{\includegraphics[width=0.32\linewidth]{figures/sgd_vary_batchsize_depth_L_5.pdf}}
    \subfigure[Easy Task $B = 16$]{\includegraphics[width=0.32\linewidth]{figures/feature_learning_easy_task_vary_N.pdf}}
    \subfigure[Hard Task $B = 16$]{\includegraphics[width=0.32\linewidth]{figures/feature_learning_hard_task_vary_N.pdf}}   
    \caption{A dimension free mean field description of the dynamics captures SGD effects and finite width $N$ effects for power law data covariates. (a) SGD with small batchsize $B$ is approximately captured by the mean field theory for $L = 4$. (b) Deep linear networks ($L=4$) trained on ``easy" power law tasks with $\beta > 1$ exhibit the same scaling law as in the lazy learning regime. Finite width networks eventually deviate from the infinite width limit with a slower convergence rate. (c) Deep linear networks trained on ``hard" powerlaw tasks in the sense of \cite{bordelon2024featurelearningimproveneural} exhibit a speedup of their scaling exponent relative to their convergence rate in the lazy limit. }
    \label{fig:dimension_free_powerlaws}
\end{figure*}

While the focus in the paper so far has been on data drawn from an isotropic distribution and on the proportional limit, we stress that our methods can also characterize the dynamics of training on structured data. For example, suppose that our covariates $\x$ have the following covariance structure
\begin{align}
    \left< x_k x_\ell \right> = \lambda_k \delta_{k\ell}  \ , \ y(\x) = \sum_k w^\star_k x_k
\end{align}
where the target weights $w^\star_k$ are arbitrary. Of particular interest are trace class covariances with $\sum_{k=1}^\infty \lambda_k < \infty$ which require developing a dimension-free version of the theory \cite{cheng2022dimension, bordelon2024dynamical}. We develop an approximate mean field theory using the same techniques. The error along the $k$-th eigenspace of the data $v_k(t) = w^\star_k - \frac{1}{N\gamma_0} \W^0(t)^\top \g^1(t)$ follows the dynamics
\begin{align}
    &v_k(t) = \sum_{t'<t} \mathcal H_k(t,t') \left[ w^\star_k - r_k^0(t') -  \tilde{u}_k^0(t')  \right],
\end{align}
where the Gaussian noise variables $r_k^0(t)$ and $\tilde{u}^0_k(t)$ are due to finite width effects and finite dataset (or finite batch) respectively. We introduced the transfer function $\mathcal{H}_k$ for each eigenmode $k$ that describes the (biased) trajectory of $v_k$'s dynamics in the absence of these noise terms (see Appendix \ref{app:structured_data}). This transfer function $\mathcal H_k(t)$ contains finite width $N$ and finite data $P$ effects (but not finite batch effects for SGD). The test loss can be computed as $\mathcal{L}(t) = \sum_{k} \lambda_k \left< v_k(t)^2 \right>$. Of particular interest is the behavior of this model under powerlaw covariates 
\begin{align}
    \lambda_k \sim k^{-\alpha} \ , \  (w^\star_k)^2 \lambda_k \sim k^{-\alpha\beta - 1}  ,
\end{align}
known as source/capacity conditions \cite{cui2023error, bordelon2024dynamical}\footnote{Not to be confused with the $\alpha,\beta$ of previous sections. We use this to be consistent with the notation of \cite{bordelon2024featurelearningimproveneural}. }. Tasks with larger (smaller) $\beta$ are easier (harder) to estimate for the model. We illustrate the behavior of our theory for deep linear networks trained with SGD on these power law tasks compared to simulations in Figure \ref{fig:dimension_free_powerlaws}. Despite this mean field description being only approximate for small $N,B$, the theory begins to become accurate around $N, B \sim 16$ for power law eigenspectra. While a linear model would always follow a power law scaling with training iterations $\mathcal L \sim t^{-\beta}$, we note that large width networks in the feature learning regime can obey the following power law scaling
\begin{align}
   \lim_{N \to \infty} \mathcal{L}(t,N) \sim  \begin{cases}
       t^{-\frac{2\beta}{1+\beta}} & \beta < 1  \quad \text{(Hard Tasks)}
       \\
       t^{-\beta} & \beta \geq 1  \quad \text{(Easy Tasks)}
   \end{cases}
\end{align}
which agrees with the exponents derived by \citet{bordelon2024featurelearningimproveneural} in the one-hidden layer $L=1$ case. However, our theory shows that this improved exponent for the hard task regime due to non-lazy dynamics is in fact the same for deeper networks as well. This indicates that the structure of the task could induce some universal behavior in linear networks of different architectures. 


\section*{Discussion}
\vspace{-5pt}
In this work, we developed an asymptotic mean field description of the training dynamics of randomly initialized deep linear networks trained on random data. The theory was able to capture both vanilla MLP architectures and deep residual neural networks and characterize the impact of parameterization, learning rate, richness parameter, width, depth, dataset size, and batchsize for online SGD. This is the first work to also numerically verify the asymptotic description of training dynamics in infinite width residual networks. We demonstrated that our theory can capture the hyperparameter transfer effect for properly parameterized networks across both widths and depths which have been observed empirically prior works. For structured and trace-class covariates, we modify our theory to approximately capture the impact of finite batchsize $B$ and finite width $N$. 



\vspace{-5pt}
\paragraph{Limitations and Future Directions}


The current theory, while a signficant improvement in complexity compared to prior works which do not average over the random data (such as \citet{bordelon2022self, bordelon2024depthwise}), becomes costly to compute for very deep networks trained for a large number of iterations $T$ and very deep networks $L \gg 1$. We summarize the memory and compute requirements to compute our theory in Table \ref{tab:comp_costs}. Because of the $\sim L^2$ number of response functions required to compute our residual network DMFT, the costs are higher than for a standard non-residual network (which is linear in $L$). Our DMFT solver is more efficient than full batch training in a finite width $N$ linear ResNet provided that $L T^2 \ll N^2 P$. In addition, the function approximation power of this model is quite limited as we are only considering deep linear networks in the present study. Some future directions that could be analyzed within this framework include characterizing the impact of learning rate schedules and other optimizers on the dynamics, studying the effect of normalization layers, and characterizing how nonlinearity changes this theoretical picture. 

%\section*{Impact Statement}

%This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\newpage
\section*{Acknowledgements}

We thank Boris Hanin, Mufan (Bill) Li, Lorenzo Noci, Jacob Zavatone-Veth, Hugo Cui, Francesca Mignacco, Stefano Sarao Manelli, and Clarissa Lauditi for useful and engaging conversations.

B.B. is supported by a Google PhD Fellowship. C.P. is supported by NSF grant DMS-2134157, NSF CAREER Award IIS-2239780, and a Sloan Research Fellowship. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence.

\newpage

\bibliographystyle{icml2025}
\bibliography{example_paper}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Derivation of Mean Field Equations}\label{app:dmft_derivation}

In this section, we provide a mathematical derivation of the DMFT equations that govern the train and test loss dynamics of the deep linear network. We will start by analyzing the non-residual architecture trained with full batch gradient descent and then proceed to study online SGD and the residual network architecture, which involves a larger set of cross-layer response functions in the residual stream. 

\subsection{Non-Residual Deep Linear Network Trained with Full Batch GD}


\subsubsection{Isolating Dependence on The Disorder}
We start by considering the feedforward depth $L$ network with no residual skip connections defined as
\begin{align}
    f(\x) = \frac{1}{N \gamma_0} (\w^L)^\top \prod_{\ell=1}^{L-1} \left( \frac{1}{\sqrt N} \W^\ell \right) \left( \frac{1}{\sqrt D} \W^0 \right) \x  .
\end{align}
In this section, we consider weights are updating these weights with gradient descent on the empirical (training) loss 
\begin{align}
    \hat{ \mathcal L}(t) = \frac{1}{P} \sum_{\mu=1}^P \left[  \Delta_\mu(t) \right]^2 \ , \  \Delta_\mu(t) \equiv y_\mu - f(\x_\mu) = \frac{1}{\sqrt D} \w^\star \cdot \x_\mu + \sigma \epsilon_\mu - f(\x_\mu) .
\end{align}
The update steps are performed using a raw learning rate of $N \eta \gamma_0^2$ following \citet{bordelon2022self}, which gives
\begin{align}
   \w^L(t+1) = \w^L(t) - N \eta \gamma_0^2 \nabla_{\w^L} \mathcal{L}(t) \ , \  \W^\ell(t+1) = \W^\ell(t) - N \eta \gamma_0^2 \nabla_{\W^\ell} \mathcal{L}(t)  \ , \ \ell \in \{0,..., L-1\} .
\end{align}
Using the chain rule at each step $t$, we can decompose the weights at time $t$ into their initial values and all of their updates  
\begin{align}
    &\bm W^0(t) = \bm W^0(0) +  \frac{\eta \gamma_0}{\sqrt D} \sum_{t'<t} \g^1(t') \h^0(t')^\top \nonumber
    \\
    &\bm W^\ell(t) = \bm W^\ell(0) + \frac{\eta \gamma_0}{\sqrt N} \sum_{t' < t} \g^{\ell+1}(t') \h^{\ell}(t')^\top \nonumber
    \\
    &\bm w^L(t) = \bm w^L(0) + \eta \gamma_0 \sum_{t'<t}  \h^L(t') 
\end{align}
where the vectors $\g^\ell(t) , \h^\ell(t) \in \mathbb{R}^N$ that we introduced defined in the following way 
\begin{align}
    &\h^0(t) = \frac{\sqrt D}{P} \bm X^\top \bm\Delta(t)  \quad , \quad \bm\Delta(t) = \frac{1}{\sqrt D} \bm X \v(t) + \sigma \bm\epsilon \quad  , \quad  \bm v(t) = \w_\star - \frac{\sqrt D}{N \gamma_0} \W^0(t)^\top \g^1(t) \nonumber
    \\ \nonumber
    &\bm h^{\ell+1}(t) = \frac{1}{\sqrt N} \bm W^\ell(t) \bm h^\ell(t)  \ , \ \ell \in \{ 0, 1, ... , L-1 \} \nonumber
    \\
    &\bm g^L(t) = \bm w^L(t) \nonumber
    \\
    &\bm g^{\ell}(t) = \frac{1}{\sqrt N} \bm W^{\ell}(t)^\top \bm g^{\ell+1}(t) \ , \ \ell \in \{1,..., L-1\}
\end{align}
In the above definitions, we have introduced the data matrix $\bm X \in \mathbb{R}^{P \times D}$ and the noise in the target values $\bm\epsilon \in \mathbb{R}^D$. We can now isolate the dependence of these vectors on the random initial conditions $\W^\ell(0)$ and the updates from gradient descent. Expanding out the weight dynamics, we can close the $\v(t), \{ \h^\ell(t) , \g^\ell(t) \}_{\ell \in [L] }$ dynamics as
\begin{align}
    &\h^{\ell+1}(t) =  \bm\chi^{\ell+1}(t) + \eta \gamma_0 \sum_{t'<t} C_h^{\ell}(t,t') \g^{\ell+1}(t')  \nonumber
    \\
    &\g^{\ell}(t) = \bm\xi^{\ell}(t) + \eta \gamma_0 \sum_{t'<t} C_g^{\ell+1}(t,t') \h^{\ell}(t')  \nonumber
    \\
    &\bm v(t) = \bm w_\star - \frac{\sqrt{D}}{N \gamma_0} \bm W^0(0)^\top \g^1(t) - \eta \sum_{t'<t}  C_g^1(t,t') \h^0(t')  \nonumber
\end{align}
where we introduced the following correlation functions
\begin{align}
    C^\ell_h(t,t') \equiv \frac{1}{N} \h^\ell(t) \cdot \h^\ell(t') \ , \ C^\ell_g(t,t') \equiv \frac{1}{N} \g^\ell(t) \cdot \g^\ell(t') 
\end{align}
and the following fields that depend on the initial conditions $\bm W^\ell(0)$ 
\begin{align}
    &\bm\xi^0(t) = \frac{\sqrt D}{N \gamma_0} \bm W^0(0)^\top \g^1(t)  \ , \ \bm\chi^1(t) = \frac{1}{\sqrt D} \bm W^0(0) \h^0(t) \nonumber
    \\
    &\bm\chi^{\ell+1}(t) = \frac{1}{\sqrt N} \bm W^\ell(0) \h^\ell(t)  \ , \ \bm\xi^\ell(t) = \frac{1}{\sqrt N} \bm W^\ell(0)^\top \g^{\ell+1}(t) .
\end{align}
These variables isolate the dependence on all of the random initial conditions for the weights. Combined with the defining equations for the variables $\bm\Delta(t), \v(t), \h^0(t)$, we will now aim to characterize the typical case dynamics over draws of the disorder $\mathcal D$
\begin{align}
    \mathcal D = \{ \bm X, \bm\epsilon, \W^0(0), \W^1(0),..., \w^L(0) \} .
\end{align}
Our goal is to characterize the typical case dynamics of the system over random draws of $\mathcal D$. 

\subsubsection{Computing the Dynamical Distribution Over Draws of the Disorder}

\paragraph{Moment Generating Function} We now aim to characterize the joint distribution of $\v(t), \bm\Delta(t) , \h^0(t) , \{\h^\ell(t) , \g^\ell(t)\}_{\ell=1}^L$ over draws of the random disorder $\mathcal D$. To do so, we will start by calculating the moment generating function of these fields with a Martin-Siggia-Rose integral \cite{martin1973statistical} over the trajectories 
\begin{align}
    Z[ \bm j_{v}, \bm j_{\Delta},  \{ \bm j_{h^\ell}, \bm j_{g^\ell} \}_{\ell \in [L]} ] = \left<  \exp\left( \sum_{t} \left[ \bm j_v(t) \cdot \v(t) +   \bm j_\Delta(t) \cdot \bm\Delta(t)  + \sum_{\ell} \bm j_{h^\ell}(t) \cdot \h^\ell(t) + \bm j_{g^\ell}(t) \cdot \g^\ell(t) \right]   \right) \right>_{\mathcal D} . 
\end{align}
This object, once computed, enables computation of arbitrary moments from derivatives with respect to the $\bm j$ variables at $\bm j = 0$. For example, the two-point correlation between the $\v(t)$ variables has the form
\begin{align}
    \left< v_i(t) v_k(t')  \right>_{\mathcal D} = \frac{\partial^2}{\partial j_{v, i}(t) \partial j_{v,k}(t') }  Z[ \bm j_{v}, \bm j_{\Delta},  \{ \bm j_{h^\ell}, \bm j_{g^\ell} \}_{\ell \in [L]} ] .
\end{align}

\paragraph{Why No Replicas?} At this point, we stress that the dynamics for our variables $\v(t), \bm\Delta(t), \h^\ell(t), \g^\ell(t)$ provide us a \textit{complete and normalized joint distribution} unlike in static (replica) mean field calculations where the distribution of the variables of interest are only known up to a normalization constant (eg, where one must perform a quenched average of an observable $\mathcal O(\bm\theta)$ for a Hamiltonian $\mathcal H(\bm\theta,\mathcal D)$ which depends on disorder $\mathcal D$: $\left<  \frac{\int d\bm\theta \exp\left( - \beta \mathcal{H}(\bm\theta,\mathcal D) \right)  \mathcal{O}(\bm\theta) }{\int d\bm\theta \exp\left( - \beta \mathcal{H}(\bm\theta,\mathcal D) \right)} \right>_{\mathcal{D}}$ where we see the disorder appears in both numerator and denominator). This form of average is usually tackled with the replica method. In our case, this is not necessary since $Z$ directly captures the distribution of our trajectories over draws of disorder $\mathcal D$.  

\paragraph{Performing Averages} We now aim to compute $Z$ by averaging the dynamical trajectory over the draws of $\mathcal D$. To make this concrete, we represent the transition probabilities (conditional on $\mathcal D$) with Dirac-delta functions that encode the dynamics. We then represent the Dirac Delta function with a Fourier integral for each timestep $t$ as
\begin{align}
    1 &= \int d\bm\Delta(t) \delta\left( \bm\Delta(t) - \frac{1}{\sqrt D} \bm X \v(t) - \sigma \bm\epsilon  \right) = \int \frac{d \bm\Delta(t) d \hat{\bm\Delta}(t) }{(2\pi)^P} \exp\left( i \hat{\bm\Delta}(t) \cdot\left[ \bm\Delta(t) - \frac{1}{\sqrt D} \bm X \v(t) - \sigma \bm\epsilon \right]  \right) \nonumber
    \\
    1 &= \int d\h^0(t) \delta\left( \h^0(t) - \frac{\sqrt D}{P} \bm X^\top \bm\Delta(t)  \right) = \int \frac{d \h^0(t) d \hat{\h}^0(t) }{(2\pi)^D} \exp\left( i \hat{\h}^0(t) \cdot\left[ \h^0(t) - \frac{\sqrt D}{P} \bm X^\top \bm\Delta(t) \right]  \right)
    \\
    & ... \nonumber
\end{align}
Performing this for all of the relevant fields, we can begin to take averages over the terms involving each of the random matrices in the set of disorder $\mathcal D$. 

\paragraph{Data Average} The average over the data matrix $\bm X$ involves terms of the form
\begin{align}
   &\left< \exp\left( - i \  \text{Tr} \bm X^\top \sum_{t=0}^\infty \left[  \frac{1}{\sqrt D} \hat{\bm \Delta}(t) \v(t)^\top + \frac{\sqrt D}{P} \bm \Delta(t) \hat{\h}^0(t)^\top  \right] \right) \right>_{\bm X}
   \\
   &= \exp\left( - \frac{1}{2} \sum_{t t'} \hat{\bm\Delta}(t) \cdot \hat{\bm\Delta}(t') \underbrace{\frac{1}{D} \v(t) \cdot \v(t')}_{C_v(t,t')} - \frac{D}{2 P} \sum_{t t'}\hat{\h}^0(t) \cdot \hat{\h}^0(t') 
    \underbrace{\frac{1}{P} \bm\Delta(t) \cdot \bm\Delta(t')}_{C_\Delta(t,t')}  \right)
    \\
    &\times \exp\left( - \frac{1}{P} \sum_{t t'} \underbrace{\hat{\bm\Delta}(t) \cdot \bm\Delta(t)}_{i P  R_\Delta(t',t) } \ \underbrace{\v(t) \cdot \hat{\h}^0(t')}_{i D R_{vu}^0(t,t') }  \right) .
\end{align}
The resulting underlined terms reveal the following correlation and response functions induced by averaging over this fixed data matrix
\begin{align}
    &C_v(t,t') \equiv \frac{1}{D} \v(t) \cdot \v(t') \ , \ C_\Delta(t,t') \equiv \frac{1}{P} \bm\Delta(t) \cdot \bm\Delta(t') \nonumber
    \\
    &R_{vu}^0(t,t') \equiv -  \frac{i}{D} \v(t) \cdot \hat{\h}^0(t')  \ , \ R_\Delta(t,t') \equiv - \frac{i}{P} \bm \Delta(t) \cdot \hat{\bm\Delta}(t')
\end{align}
These correlation and response functions will be a subset of the order parameters of the mean field theory theory (more will arise from other disorder averages). We now introduce delta functions to enforce the definitions of these order parameters
\begin{align}
    1 &=  \int \frac{dC_v(t,t') d\hat{C}_v(t,t') }{4\pi D^{-1}} \exp\left( \frac{D}{2} \hat{C}_v(t,t') C_v(t,t')  - \frac{1}{2} \hat{C}_v(t,t') \v(t) \cdot \v(t') \right)
    \\
    1 &=  \int \frac{dC_\Delta(t,t') d\hat{C}_\Delta(t,t') }{4\pi P^{-1}} \exp\left( \frac{P}{2} \hat{C}_\Delta(t,t') C_\Delta(t,t')  - \frac{1}{2} \hat{C}_\Delta(t,t') \bm\Delta(t) \cdot \bm\Delta(t') \right)
\end{align}
For the response function, the conjugate response function to $R^0_{vu}$ is $R_\Delta$ and vice versa so it suffices to just enforce the definition for one of them. For concreteness, we can enforce the definition of $R^0_{vu}(t,t')$
\begin{align}
    1 = \int \frac{d R^0_{vu}(t,t') d R_\Delta(t',t)  }{2\pi D^{-1}} \exp\left( - D R^0_{vu}(t,t') R_\Delta(t',t) - i \v(t) \cdot \hat{\h}^0(t') R_\Delta(t',t)  \right)
\end{align}


\paragraph{First Layer Weight Matrix} We now examine the behavior of the first weight matrix $\W^0(0) \in \mathbb{R}^{N \times D}$ which is rectangular. For this matrix the two relevant dynamical fields are
\begin{align}
    \bm\chi^1(t) = \frac{1}{\sqrt D} \W^0(0) \h^0(t) \ , \ \bm\xi^0(t) = \frac{\sqrt D}{N \gamma_0} \W^0(t) \g^1(t) 
\end{align}
Again, we introduce Dirac delta functions for these fields and need to average 
\begin{align}
    &\left< \exp\left( - i \  \text{Tr} \bm W^0(0)^\top \sum_{t=0}^\infty \left[  \frac{1}{\sqrt D} \hat{\bm \chi}^1(t) \h^0(t)^\top + \frac{\sqrt D}{N \gamma_0} \bm \g^1(t) \hat{\bm\xi}^0(t)^\top  \right] \right) \right>_{\bm W^0(0)}
   \\
   &= \exp\left( - \frac{1}{2} \sum_{t t'} \hat{\bm\chi}^1(t) \cdot \hat{\bm\chi}^1(t') \underbrace{\frac{1}{D} \bm h^0(t) \cdot \h^0(t')}_{C_{h}^0(t,t')} - \frac{D}{2 N \gamma_0^2 } \sum_{t t'} \hat{\bm\xi}^0(t) \cdot \hat{\bm\xi}^0(t') \underbrace{\frac{1}{N} \g^1(t) \cdot \g^1(t')}_{C_g^1(t,t')} \right)
   \\
   &\times\exp\left( - \frac{1}{N \gamma_0} \sum_{t t'} \underbrace{\hat{\bm\chi}^1(t) \cdot \g^1(t')}_{i N R_{gu}^1(t',t)} \  \underbrace{\h^0(t) \cdot \hat{\bm\xi}^0(t')}_{i D R_{hr}^0(t,t')} \right)
\end{align}
where we introduced the order parameters
\begin{align}
    &C_{h}^0(t,t') \equiv \frac{1}{D} \h^0(t) \cdot \h^0(t') \ , \ C_{g}^1(t,t')  \equiv \frac{1}{N} \g^1(t) \cdot \g^1(t') 
    \\
    &R_{gu}^1(t,t') \equiv \frac{1}{N} \g^1(t) \cdot \hat{\bm \chi}^1(t') \ , \ R_{hr}^0(t,t') \equiv \frac{1}{D} \h^0(t) \cdot \hat{\bm \xi}^0(t')
\end{align}
As before, we introduce Dirac delta functions to enforce the definitions of these variables. 

\paragraph{Hidden Layers} Lastly, we examine the disorder averages for the hidden layers $\W^\ell(0) \in \mathbb{R}^{N\times N}$ which are square matrices. The random fields that depend on the random matrix $\W^\ell(0)$ are
\begin{align}
    \bm\chi^{\ell+1}(t) = \frac{1}{\sqrt N} \bm W^\ell(0) \h^\ell(t)  \ , \ \bm\xi^\ell(t) = \frac{1}{\sqrt N} \bm W^\ell(0)^\top \g^{\ell+1}(t) .
\end{align}
Averaging their trajectories over the disorder $\W^\ell(0)$ gives us 
\begin{align}
    &\left< \exp\left( \frac{i}{\sqrt N} \text{Tr} \W^\ell(0)^\top \sum_t \left[ \hat{\bm\chi}^{\ell+1}(t) \h^\ell(t)^\top + \g^{\ell+1}(t) \hat{\bm\xi}^\ell(t)^\top  \right] \right) \right>_{\W^\ell(0)} \nonumber
    \\
    = &\exp\left( - \frac{1}{2} \sum_{tt'} \hat{\bm\chi}^{\ell+1}(t) \cdot \hat{\bm\chi}^{\ell+1}(t') \underbrace{\frac{1}{N} \h^\ell(t) \cdot \h^{\ell}(t')}_{C_h^\ell(t,t')} - \frac{1}{2} \sum_{tt'} \hat{\bm\xi}^{\ell}(t) \cdot \hat{\bm\xi}^{\ell}(t') \underbrace{\frac{1}{N} \g^{\ell+1}(t) \cdot \g^{\ell+1}(t')}_{C_g^{\ell+1}(t,t')}   \right)  \nonumber
    \\
    \times &\exp\left( - \frac{1}{N} \sum_{t t'} \underbrace{\hat{\bm\chi}^{\ell+1}(t) \cdot \g^{\ell+1 }(t')}_{i N R_{gu}^{\ell+1}(t',t)}  
 \underbrace{\h^{\ell}(t) \cdot \hat{\bm\xi}^\ell(t')}_{i N R_{hr}^{\ell}(t,t')} \right)
\end{align}
where the relevant order parameters are
\begin{align}
    &C_h^\ell(t,t') \equiv \frac{1}{N} \h^\ell(t) \cdot \h^\ell(t') \ , \ C_g^{\ell+1}(t,t') \equiv \frac{1}{N} \g^{\ell+1}(t) \cdot \g^{\ell+1}(t') \nonumber
    \\
    &R^{\ell}_{hr}(t,t' ) \equiv -\frac{i}{N} \h^\ell(t) \cdot \hat{\bm\xi}^\ell(t') \ , \ R^{\ell+1}_{gu}(t,t' ) \equiv -\frac{i}{N} \g^{\ell+1}(t) \cdot \hat{\bm\chi}^{\ell+1}(t') 
\end{align}
As before, we introduce these order parameters with dual Fourier variables. 

\paragraph{Proportional Scaling Regime} We notice that many of the terms that arise in the integrals depend on ratios such as $\frac{N}{D}$ and $\frac{P}{D}$. To deal with these terms we introduce definitions for these ratios
\begin{align}
    \alpha \equiv \frac{P}{D} \ , \ \nu \equiv \frac{N}{D}  .
\end{align}
One can interpret, at fixed $D$, as $\alpha$ and $\nu$ representing an increase in the quantity of data or the width of the network respectively. With these constants $\alpha, \nu$ introduced, we can now consider how the moment generating function $Z$ behaves as $D \to \infty$. However, before this, we must organize and simplify the remaining integrals to enable taking the $D \to \infty$ limit.

\paragraph{Decoupling The Single Site Integrals} Now we aim to express the integrals over the variables $\bm\Delta(t), \v(t), \bm\chi^\ell(t), \bm\xi^\ell(t)$ and their conjugates
\begin{align}
    \int \prod_{t=0}^\infty \frac{d\bm\Delta(t) d\hat{\bm\Delta}(t)}{(2\pi)^P} &\exp\left( - \frac{1}{2} \sum_{t t'} \hat{\bm\Delta}(t) \cdot \hat{\bm\Delta}(t') C_v(t,t') + i  \sum_{t t'} \hat{\bm\Delta}(t) \cdot \left( \bm\Delta(t') \delta(t-t') - R^0_{vu}(t,t') \bm \Delta(t')  \right)  \right) \nonumber
    \\
    \times &\exp\left( \sum_{t} \bm j_\Delta(t) \cdot \bm\Delta(t)  - \frac{1}{2}  \sum_{tt'} \hat{C}_{\Delta}(t,t') \bm\Delta(t) \cdot \bm\Delta(t')     \right)  \nonumber
    \\
    = &\prod_{\mu=1}^P \mathcal Z_\Delta[ C_v, \hat{C}_\Delta, R^0_{vu}, j_{\Delta, \mu}(t) ]
\end{align}
where we introduced the \textit{single site moment generating function} for the $\Delta(t)$ fields that has the form of an integral over a single entry (scalar stochastic process) of the original $\bm\Delta(t)$ vector
\begin{align}
    Z_\Delta = \int \prod_{t=0}^\infty \frac{d\Delta(t) d\hat{\Delta}(t)}{2\pi} &\exp\left( - \frac{1}{2} \sum_{t t'} \hat{\Delta}(t) \hat{\Delta}(t') C_v(t,t') + i  \sum_{t t'} \hat{\Delta}(t)  \left( \Delta(t') \delta(t-t') - R^0_{vu}(t,t')  \Delta(t')  \right)  \right) \nonumber
    \\
    &\times \exp\left( \sum_{t}  j_\Delta(t) \Delta(t)  - \frac{1}{2}  \sum_{tt'} \hat{C}_{\Delta}(t,t') \Delta(t) \Delta(t')     \right)
\end{align}

From our disorder average, we also see that there is statistical coupling between $\v(t), \bm\xi^0(t)$ and $\h^0(t)$ variables. We let the joint moment generating function for this set of variables be $\mathcal Z_0$ (for layer $\ell = 0$), and express it as
\begin{align}
    \mathcal Z_0 = \int \prod_{t=0}^\infty &\frac{dv(t)  d\xi^0(t) d\hat{\xi}^0(t) dh^0(t) d\hat{h}^0(t)}{4\pi^2}   \nonumber
    \\
    &\exp\left( - \frac{1}{2 \alpha } \sum_{t t'} \hat{h}^0(t) \hat{h}^0(t') C_\Delta(t,t') - \frac{1}{2 \gamma_0^2 \nu} \sum_{t t'} \hat{\xi}^0(t) \hat{\xi}^0(t')  C_g^1(t,t')    \right)   \nonumber
    \\
    &\exp\left( i \sum_{tt'} \hat{h}^0(t) \left( \delta(t-t') h^0(t')  - R_\Delta(t,t') v(t') \right) + \sum_t j_{h^0}(t) h^0(t) \right) \nonumber
    \\
    &\exp\left( i \sum_{tt'} \hat{\xi}^0(t) \left( \delta(t-t') \xi^0(t')  - \frac{1}{\gamma_0} R_{gu}^1(t,t') h^0(t') \right) + \sum_t j_v(t) v(t)  \right)  \nonumber
    \\
    &\prod_{t} \delta\left( v(t) - w^\star + \xi^0(t) + \eta \sum_{t'<t} C_g^1(t,t') h^0(t') \right) 
\end{align}
For the first hidden layer ($\ell = 1$), we have the following joint single site MGF
\begin{align}
    \mathcal Z_{1} = \int \prod_{t} &\frac{d\chi^1(t) d\hat\chi^1(t) d\xi^1(t) d\hat\xi^1(t) dh^1(t) dg^1(t)  }{4 \pi^2 }  \nonumber
    \\
    &\exp\left( - \frac{1}{2}\sum_{t t'} \hat\chi^1(t) \hat\chi^1(t') C_h^0(t,t') - \frac{1}{2} \sum_{t t'} \hat\xi^1(t) \hat\xi^1(t') C_g^2(t,t') \right) \nonumber
    \\
    &\exp\left( i \sum_{t t'} \hat\chi^1(t) \left( \delta(t-t') \chi^1(t') - \frac{1}{ \nu \gamma_0 } R^0_{hr}(t,t') g^1(t')  \right)  \right) \nonumber
    \\
    &\exp\left( i \sum_{t t'} \hat\xi^1(t) \left( \delta(t-t') \xi^1(t') - R^2_{gu}(t,t') h^1(t')  \right)  \right) \nonumber
    \\
    &\exp\left(  \sum_{t} \left[ j_{h^1}(t) h^1(t) + j_{g^1}(t) g^1(t)  \right] \right) \nonumber
    \\
    &\prod_{t } \delta\left( h^1(t) - \chi^1(t) - \eta \gamma_0 \sum_{t'<t} C_h^{0}(t,t') g^1(t') \right) \delta\left( g^1(t) - \xi^1(t) - \eta \gamma_0 \sum_{t'<t} C_g^{2}(t,t') h^1(t') \right) 
\end{align}
For the hidden layers $\ell \in \{2,...,L-1\}$ we have the following single site MGFs
\begin{align}
    \mathcal Z_{\ell} = \int \prod_{t} &\frac{d\chi^\ell(t) d\hat\chi^\ell(t) d\xi^\ell(t) d\hat\xi^\ell(t) dh^\ell(t) dg^\ell(t)  }{4 \pi^2 }  \nonumber
    \\
    &\exp\left( - \frac{1}{2}\sum_{t t'} \hat\chi^\ell(t) \hat\chi^\ell(t') C_h^{\ell-1}(t,t') - \frac{1}{2} \sum_{t t'} \hat\xi^{\ell}(t) \hat\xi^{\ell}(t') C_g^{\ell+1}(t,t') \right) \nonumber
    \\
    &\exp\left( i \sum_{t t'} \hat\chi^\ell(t) \left( \delta(t-t') \chi^\ell(t') -  R^{\ell-1}_{hr}(t,t') g^\ell(t')  \right)  \right) \nonumber
    \\
    &\exp\left( i \sum_{t t'} \hat\xi^\ell(t) \left( \delta(t-t') \xi^\ell(t') - R^{\ell+1}_{gu}(t,t') h^\ell(t')  \right)  \right) \nonumber
    \\
    &\exp\left(  \sum_{t} \left[ j_{h^\ell}(t) h^\ell(t) + j_{g^\ell}(t) g^\ell(t)  \right] \right) \nonumber
    \\
    &\prod_{t } \delta\left( h^\ell(t) - \chi^\ell(t) - \eta \gamma_0 \sum_{t'<t} C_h^{\ell-1}(t,t') g^\ell(t') \right) \delta\left( g^\ell(t) - \xi^\ell(t) - \eta \gamma_0 \sum_{t'<t} C_g^{\ell+1}(t,t') h^1(t') \right) 
\end{align}
The final layer $\ell = L$ has the following single site MGF
\begin{align}
    \mathcal Z_L = \int &\frac{d h^L(t) d\hat{\chi}^L(t) d\chi^L(t) dg^L(t)}{ 2\pi } \nonumber
    \\
    &\exp\left( - \frac{1}{2}\sum_{t t'} \hat\chi^L(t) \hat\chi^L(t') C^{L-1}_h(t,t')  + i \sum_{tt'} \hat{\chi}^L(t) \left( \chi^L(t') \delta(t-t') - R^{L-1}_{hr}(t,t') g^L(t') \right) \right) \nonumber
    \\
    &\prod_{t} \delta\left( h^L(t) - \chi^L(t) - \eta \gamma_0 \sum_{t'<t} C_h^{L-1}(t,t') g^L(t')  \right)  \nonumber
    \\
    &\left< \prod_t \delta\left( g^L(t) - g^L(0) - \eta \gamma_0 \sum_{t'<t} h^L(t') \right) \right>_{g^L(0) \sim \mathcal{N}(0,1)} \nonumber
    \\
    &\exp\left( \sum_t \left[ j_{h^L}(t) h^L(t) + j_{g^L}(t) g^L(t) \right]  \right)
\end{align}
Now we have reduced all of the field path integrals into single site moment generating functions. 

\paragraph{DMFT Action} We now group all of the correlation and response functions, as well as their conjugate order parameters into a list $\bm q$ and combine all of the source variables into a list $\bm j$. The moment generating function can be written as
\begin{align}
    Z[\bm j] = \int d\bm q \exp\left( - D \mathcal S[\bm q, \bm j] \right)
\end{align}
where $\mathcal S$ is an $\mathcal{O}(1)$ DMFT action which has the form
\begin{align}
    \mathcal S =& - \frac{1}{2} \sum_{t t'} \left[ C_v(t,t') \hat C_v(t,t') + \alpha  C_\Delta(t,t') \hat C_\Delta(t,t')  \right] 
    - \frac{\nu}{2} \sum_\ell \sum_{tt'}\left[ C_{h^\ell}(t,t') \hat C_{h^\ell}(t,t') + C_{g}^\ell(t,t') \hat{C}_g^\ell(t,t')   \right] \nonumber
    \\
    &+ \sum_{t t'} R_{vu}^0(t,t') R_\Delta(t',t) + \gamma_0^{-1} \sum_{tt'} R_{hr}^0(t,t') R_{gu}^1(t',t)   + \nu \sum_{\ell} \sum_{tt'} R_{hr}^{\ell}(t,t') R_{gu}^{\ell+1}(t',t)   \nonumber
    \\
    &-\frac{1}{D} \sum_{\mu=1}^P \ln \mathcal Z_\Delta[ j_{\Delta,\mu} ] - \frac{1}{D} \sum_{k=1}^D \mathcal Z_0[j_{h^0,k}, j_{v, k}] - \frac{1}{D} \sum_{\ell=1}^L \sum_{i=1}^N \ln \mathcal Z_\ell[j_{h^\ell, i} , j_{g^\ell, i}]
\end{align}

\paragraph{Saddle Point Equations} We are now in a position to take the $D \to \infty$ limit. In this limit, the integral over $\bm q$ is dominated by the saddle point of $\mathcal S$. The resulting saddle point equations $\frac{\partial \mathcal  S}{\partial \bm q} = 0$. We start by differentiating with respect to the conjugate order parameters for the correlation functions which give
\begin{align}
    &\frac{\partial \mathcal S}{\partial \hat{C}_v(t,t')} = - \frac{1}{2} C_v(t,t') + \frac{1}{2 D} \sum_{k=1}^D \left< v(t) v(t') \right>_k  = 0 \nonumber
    \\
    &\frac{\partial \mathcal S}{\partial \hat{C}_h^0(t,t')} = - \frac{1}{2} C_h^0(t,t') + \frac{1}{2 D} \sum_{k=1}^D \left< h^0(t) h^0(t') \right>_k  = 0 \nonumber
    \\
    &\frac{\partial \mathcal S}{\partial \hat{C}_\Delta(t,t')} = - \frac{\alpha}{2} C_\Delta(t,t') + \frac{1}{2 D} \sum_{\mu=1}^P \left< \Delta(t) \Delta(t') \right>_\mu  = 0 \nonumber
    \\
    &\frac{\partial \mathcal S}{\partial \hat{C}_h^\ell(t,t')} = - \frac{\nu}{2} C_h^\ell(t,t') + \frac{1}{2D} \sum_{i=1}^N \left< h^\ell(t) h^\ell(t') \right>_i  = 0 \nonumber
    \\
    &\frac{\partial \mathcal S}{\partial \hat C_g^\ell(t,t')} = - \frac{\nu}{2} C_h^\ell(t,t') + \frac{1}{2D} \sum_{i=1}^N \left< g^\ell(t) g^\ell(t') \right>_i  = 0
\end{align}
where $\left< \right>_{\mu}$ represents an average over the single site distribution defined by the moment generating function $\mathcal Z_\Delta[j_{\Delta,\mu}]$. Similarly $\left< \right>_k , \left< \right>_i$ denote averages over the distributions defined by the single site moment generating functions  $\mathcal Z_0[j_k], \{ \mathcal Z_\ell[j_i] \}_{\ell=1}^L$. We note that in the limit of zero source $\bm j \to 0$ these averages all become identical (we will examine this zero source limit shortly). Next, we can work out the saddle point equations for the response functions
\begin{align}
    &\frac{\partial \mathcal S}{\partial R_{vu}^0(t',t) } = R_\Delta(t,t') + \frac{i}{P} \sum_{\mu=1}^P \left< \Delta(t) \hat\Delta(t') \right>_{\mu} =0 \nonumber
    \\
    &\frac{\partial \mathcal S}{\partial R_{\Delta}(t',t) } = R_{vu}(t,t') + \frac{i}{D} \sum_{k=1}^D \left< v(t) \hat h^0(t') \right>_k =0  \nonumber
    \\
    &\frac{\partial \mathcal S}{\partial R_{hr}^0(t',t) } = \gamma_0^{-1} R^1_{gu}(t,t') + \frac{i}{N \gamma_0} \sum_{i=1}^N \left< g^1(t) \hat{\xi}^1(t')  \right>_i = 0 \nonumber
    \\
    &\frac{\partial \mathcal S}{\partial R_{gu}^1(t',t) } =  \gamma_0^{-1} R^0_{hr}(t,t') + \frac{i}{D \gamma_0} \sum_{k=1}^D \left< h^0(t) \hat{\chi}^0(t')  \right>_k = 0 \nonumber
    \\
    &\frac{\partial \mathcal S}{\partial R_{hr}^\ell(t',t) } = \nu R^{\ell+1}_{gu}(t,t') + \frac{i}{D} \sum_{i=1}^N \left< g^{\ell+1}(t) \hat{\xi}^{\ell+1}(t')  \right>_i = 0 \nonumber
    \\    
    &\frac{\partial \mathcal S}{\partial R_{gu}^{\ell+1}(t',t) } =  \nu R^\ell_{hr}(t,t') + \frac{i}{D} \sum_{i=1}^N \left< h^\ell(t) \hat{\chi}^\ell(t')  \right>_i = 0
\end{align}

Lastly, we have a collection of saddle point equations $\frac{\partial \mathcal S}{\partial C } = 0$ that define the conjugate order parameters $\hat C = 0$ which must vanish at the saddle point  \cite{crisanti2018path, mignacco2020dynamical, bordelon2022self}. Thus we have the following remaining set of equations
\begin{align}
    \hat{C}_\Delta(t,t') = 0 \ , \ \hat{C}_v(t,t') = 0 \ , \ \hat{C}_h^\ell(t,t') = 0 \ , \ \hat{C}_g^\ell(t,t') = 0  . 
\end{align}

\paragraph{Hubbard Transformation} Now that we have argued that the correlation and response functions take on deterministic values in the $D \to \infty$ limit, we can represent the quadratic terms in log-density in $\hat \Delta, \hat \chi, \hat\xi$ fields as linear averages Gaussian random variables $u_\Delta(t), u^\ell(t), r^\ell(t)$, giving
\begin{align}
    &\exp\left( - \frac{1}{2} \sum_{tt'} \hat{\Delta}(t) \hat{\Delta}(t') C_v(t,t')  \right) = \left< \exp\left( - i \sum_t \hat{\Delta}(t)  u_\Delta(t)  \right) \right>_{u_\Delta \sim \mathcal{N}(0, C_v)} \nonumber
    \\
    &\exp\left( - \frac{1}{2 \alpha} \sum_{tt'} \hat{h}^0(t)  \hat{h}^0(t') C_\Delta(t,t')  \right) = \left< \exp\left( - i \sum_t \hat{h}^0(t)  u^0(t)  \right) \right>_{u^0 \sim \mathcal{N}\left(0, \alpha^{-1} C_\Delta \right)}  \nonumber
    \\
    &\exp\left( - \frac{1}{2} \sum_{tt'} \hat{\chi}^\ell(t)  \hat{\chi}^\ell(t') C_h^{\ell-1}(t,t')  \right) = \left< \exp\left( - i \sum_t \hat{\chi}^\ell(t)  u^\ell(t)  \right) \right>_{u^\ell \sim \mathcal{N}\left(0,  C_h^{\ell-1} \right)} \nonumber 
    \\
    &\exp\left( - \frac{1}{2 \gamma_0^2 \nu} \sum_{tt'} \hat{\xi}^0(t)  \hat{\xi}^0(t') C_g^{1}(t,t')  \right) = \left< \exp\left( - i \sum_t \hat{\xi}^0(t)  r^0(t)  \right) \right>_{r^0 \sim \mathcal{N}\left(0, \gamma_0^{-2} \nu^{-1}  C_h^{\ell-1} \right)} \nonumber 
    \\
    &\exp\left( - \frac{1}{2} \sum_{tt'} \hat{\xi}^\ell(t)  \hat{\xi}^\ell(t') C_g^{\ell+1}(t,t')  \right) = \left< \exp\left( - i \sum_t \hat{\xi}^\ell(t)  r^\ell(t)  \right) \right>_{r^0 \sim \mathcal{N}\left(0,  C_g^{\ell+1} \right)}  
\end{align}
After introducing these Gaussian random variables, we can now perform integrals over the Fourier variables $\hat \Delta, \hat h^0, \hat\chi^\ell, \hat\xi^\ell$ to obtain defining equations for the random variables of interest. For example, for the $\Delta(t)$ random variable,
\begin{align}
    &\int \prod_t \frac{d\hat\Delta(t)}{2\pi} \left< \exp\left( i \sum_t \hat\Delta(t) \left[ \Delta(t) -  u_\Delta(t) - \sum_{t'} R_{vu}^0(t,t') \Delta(t') \right] \right) \right>_{u_\Delta}  \nonumber
    \\
    &= \left< \delta\left( \Delta(t) - u_\Delta(t) - \sum_{t'} R_{vu}^0(t,t') \Delta(t')   \right) \right>_{u_\Delta(t) \sim \mathcal{N}(0,C_v)} \nonumber
    \\
    \implies& \Delta(t) = u_\Delta(t) + \sum_{t'} R_{vu}^0(t,t') \Delta(t')  \  , \ u_\Delta(t) \sim \mathcal{N}(0, C_v) 
\end{align}
We can repeat this for all of the remaining fields which give defining stochastic processes for them. The general result is that the random fields are driven by a noise component (like $u_\Delta(t)$ above) as well as a memory term involving a response function (like $R_{vu}^0(t,t')$ above). 

\paragraph{Simplifying the Response Functions} We note that the response functions involve averages over the $\hat \Delta, \hat\chi^\ell, \hat\xi^\ell$ variables, which we now argue can be viewed as derivatives with respect to the Hubbard variable. For simplicity, we will now examine the zero source limit where all averages are identical.
\begin{align}
    R_\Delta(t,t') &= - i \left<  \Delta(t) \hat\Delta(t')  \right> \nonumber
    \\
    &= - i \int \left[\prod_t  \frac{d\Delta(t) d\hat\Delta(t) }{2\pi} \right]  \Delta(t) \hat\Delta(t')  \left<
 \exp\left( i \sum_t \hat{\Delta}(t)\left[ \Delta(t) - u_\Delta(t) - \sum_{t'} R_{vu}^0(t,t') \Delta(t') \right] \right) \right>_{u_\Delta} \nonumber
 \\
 &= \int \left[\prod_t  \frac{d\Delta(t) d\hat\Delta(t) }{2\pi} \right]  \Delta(t) 
 \left< \frac{\partial }{\partial u_\Delta(t')}
 \exp\left( i \sum_t \hat{\Delta}(t)\left[ \Delta(t) - u_\Delta(t) - \sum_{t'} R_{vu}^0(t,t') \Delta(t') \right] \right) \right>_{u_\Delta} \nonumber
 \\
 &= \sum_{t''} \left< \Delta(t) [C_v]^{-1}(t',t'') u_\Delta(t'') \right>_{u_\Delta} =  \left< \frac{\partial \Delta(t)}{\partial u_\Delta(t')} \right>_{u_\Delta}
\end{align}
which holds via integration by parts and Stein's lemma. Repeating this for each of the fields, we find the following response function definitions from the saddle point equations
\begin{align}
    &R_\Delta(t,t') = \left< \frac{\partial \Delta(t)}{\partial u_\Delta(t')} \right> \ , \ R_{vu}^0(t,t') = \left<  \frac{\partial v(t)}{\partial u^0(t')} \right> \ , \ R^0_{hr}(t,t') = \left< \frac{\partial h^0(t)}{\partial r^0(t')} \right> \nonumber
    \\
    &R^\ell_{hr}(t,t') = \left< \frac{\partial h^\ell(t)}{\partial r^\ell(t')} \right> \ , \ R^\ell_{gu}(t,t') = \left< \frac{\partial g^\ell(t)}{\partial u^\ell(t')} \right>  .
\end{align}

\subsection{Final DMFT Equations}\label{app:final_DMFT_eqns}

Combining the above equations at zero source $\bm j \to 0$ after our Hubbard transformations, we arrive at the final single site equations for our fields. The DMFT equations that describe the single site stochastic processes are 
\begin{align}
    &v(t) = w^\star - r^0(t) - \sum_{t'<t} \left[ \frac{1}{\gamma_0} R^1_{gu}(t,t') h^0(t') + \eta \gamma_0 C_g^1(t,t') \right] h^0(t') \ , \ r^0(t) \sim \mathcal{GP}\left(0, \frac{1}{\nu \gamma_0^2} G^1 \right)  \nonumber
    \\
    &\Delta(t) = u_\Delta(t) + \frac{1}{\alpha } \sum_{t'<t} R_{vu}^0(t,t') \Delta(t') + \sigma \epsilon \ , \ u_\Delta(t) \sim \mathcal{GP}\left( 0 , C_v \right) \ ,  \ \epsilon \sim \mathcal{N}(0,1) \nonumber
    \\
    &h^0(t) = u^0(t) + \sum_{t'<t} R_{\Delta}(t,t') v(t') \ , \ u^0(t) \sim \mathcal{GP}\left(0, \frac{1}{\alpha} C_\Delta \right) \nonumber
    \\
    &h^1(t) = u^1(t) +  \sum_{t'<t} 
 \left[ \frac{1}{\nu \gamma_0} R_{h r}^0(t,t')   g^1(t') + \eta \gamma_0 C_h^0(t,t') \right] g^1(t') \ , \ u^1(t) \sim \mathcal{GP}(0, C_h^0) \nonumber
    \\
    &h^{\ell}(t) = u^\ell(t) + \sum_{t'<t} \left[ R_{h r}^{\ell-1}(t,t') + \eta \gamma_0 C_h^{\ell-1}(t,t')
    \right] g^\ell(t') \ , \ u^\ell(t) \sim \mathcal{GP}(0, C_h^{\ell-1}) \nonumber
    \\
    &g^{\ell}(t) = r^\ell(t) + \sum_{t'<t} \left[ R_{g u}^{\ell+1}(t,t') + \eta \gamma_0 C_g^{\ell+1}(t,t')  \right] h^\ell(t') \ , \ r^\ell(t) \sim \mathcal{GP}(0, C_g^{\ell+1})
\end{align}
From these equations, all necessary response functions can be computed by differentiating over the noise variables $\{ u, r \}$. 
\begin{align}
    &R_\Delta(t,t') = \left< \frac{\partial \Delta(t)}{\partial u_\Delta(t')} \right> \ , \ R_{vu}^0(t,t') = \left<  \frac{\partial v(t)}{\partial u^0(t')} \right> \ , \ R^0_{hr}(t,t') = \left< \frac{\partial h^0(t)}{\partial r^0(t')} \right> \nonumber
    \\
    &R^\ell_{hr}(t,t') = \left< \frac{\partial h^\ell(t)}{\partial r^\ell(t')} \right> \ , \ R^\ell_{gu}(t,t') = \left< \frac{\partial g^\ell(t)}{\partial u^\ell(t')} \right>  
\end{align}
Similarly, the correlation functions can be computed as averages over these noise variables
\begin{align}
    &C_v(t,t' ) = \left< v(t) v(t') \right> \ , \ C_\Delta(t,t') = \left< \Delta(t) \Delta(t') \right>  \nonumber
    \\ 
    &C_h^\ell(t,t') = \left< h^\ell(t) h^\ell(t') \right> \ , \ C_g^\ell(t,t') = \left< g^\ell(t) g^\ell(t') \right> 
\end{align}

We note again that the test and train losses can be obtained from the time-time diagonal of the correlation functions.
\begin{align}
    \mathcal L(t) = C_v(t,t) + \sigma^2  \ , \ \mathcal{\hat L}(t) = C_\Delta(t,t)
\end{align}

\subsection{Centering: Subtracting the Initial Predictor for Lazy Limit Stability}

At finite values of $\nu$ our theory will diverge if the lazy limit $\gamma_0 \to 0$ is taken. This is due to a divergence of the predictor variance $C_v(t,t)$ which can be eliminated with a \textit{centering} operation. Under centering we define the vector $\bm v(t)$ as
\begin{align}
    \v(t) = \w^\star - \bm\xi^0(t) + \bm\xi^0(0) + \eta \sum_{t'<t} C_g^1(t,t') \h^0(t')
\end{align}


The key adjustment to the DMFT equations from this operation is 
\begin{align}
    &v(t) = w_\star -  r^0(t) + r^0(0) -\frac{1}{\gamma_0} \sum_{t'<t} R^1_{gu}(t,t') h^0(t')   - \eta \sum_{t'<t} G^1(t,t') h^0(t') \nonumber
\end{align}
%
The correlation functions that need to be tracked are $\{ C_v, C_h^0, C_h^1,..., C_h^{L-1}, C_g^1, C_g^2, ... , C_g^{L} \}$ while we need the following response functions $\{ R_\Delta, R_{hr}^0, ... , R^{L-1}_{hr}, R_{gu}^{1}, ..., R_{gu}^{L} \}$. 


\section{Comparing Parameterizations}\label{app:compare_params}

In this section, we briefly comment on how the maximal stable learning rate scales with $\nu$ in NTK and in mean field/$\mu$P parameterization. NTK parameterization is defined as scaling the network so that $\gamma_0 = \Theta\left( \frac{1}{\sqrt \nu} \right)$. However, a consequence of this choice is that the change in the correlation functions $C_g(t,t')$ under a fixed number of steps of optimization will decrease with $\gamma_0$ at fixed learning rate. As a consequence, higher learning rates can be tolerated at larger widths. However, once the network is wide enough to be sufficiently close to the kernel regime, the maximum stable learning rate stabilizes as the dynamics become asymptotically linear. A comparison showing this trend is provided in \ref{fig:compare_params_lrs}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\linewidth]{figures/transfer_MF_vs_NTK_nu_dependence.pdf}
    \caption{Maximal stable learning rate as a function of width $\nu$ for different parameterizations.}
    \label{fig:compare_params_lrs}
\end{figure}

For very rich dynamics $\gamma_0 \gg 1$, the optimal learning rate approximately scales as 
\begin{align}
    \eta_{\text{max}} \sim 
    \begin{cases}
        \mathcal{O}_\nu\left( 1 \right) & \text{MF} \ (\text{Fixed} \ \gamma_0)
        \\
        \mathcal{O}_\nu\left(  \nu^{1 - \frac{1}{L}} \right) & \text{NTK}  \ (\text{Fixed} \ \gamma_0 \sqrt{\nu}) ,
    \end{cases}
\end{align}
based on a simple argument about the scale of the kernel. On the other hand, in the lazy learning regime $\gamma_0 \ll 1$, both MF and NTK parameterizations approach constant optimal learning rate as they approach linear models. 



\section{Residual Networks}

In this section, we consider training residual networks. To streamline the calculation in this section, we will first compute gradient descent on the population risk (incorporating random data can be handled by introducing the $\Delta(t)$ variables as in the previous sections). The key field definitions have the form
\begin{align}
    &\h^0(t) = \w^\star - \frac{\sqrt D}{\gamma_0 N} \W^0(t)^\top \g^1(t)  \nonumber
    \\
    &\h^{\ell+1}(t) = \h^\ell(t) + \frac{\beta}{\sqrt N} \W^\ell(t) \h^\ell(t) \nonumber
    \\
    &\g^L(t) = \g^L(0) + \eta \gamma_0 \sum_{t'<t} \h^L(t') \nonumber
    \\
    &\g^{\ell}(t) = \g^{\ell+1}(t) +  \frac{\beta}{\sqrt N} \W^\ell(t)^\top \g^{\ell+1}(t) .
\end{align}
Introducing $\bm\chi^{\ell+1}(t) = \frac{1}{\sqrt N} \W^{\ell}(0) \h^\ell(t)$ and $\bm\xi^\ell(t) = \frac{1}{\sqrt N} \W^\ell(0)^\top \g^{\ell+1}(t)$, these obey the following DMFT single site dynamics
\begin{align}
    &\chi^{\ell}(t) = u^\ell(t) + \sum_{t' < t} R_{hr}^{\ell-1}(t,t') g^\ell(t') \nonumber
    \\
    &\xi^{\ell}(t) = r^\ell(t) + \sum_{t' < t} R_{gu}^{\ell+1}(t,t') h^\ell(t') .
\end{align}
The full gradient descent dynamics can thus be expanded as 

We can solve simultaneously for correlation and response functions 
\begin{align}
    &h^0(t) = w^\star - r^0(t) - \sum_{t'<t} \left[ \frac{1}{\gamma_0} R_{gu}^1(t,t') + \eta C_g^1(t,t') \right] h^0(t')  \ , \ r^0(t) \sim \mathcal{GP}(0, \gamma_0^{-2} \nu^{-1} C_g^1 )
    \\
    &h^1(t) = u^1(t) + \sum_{t'<t} \left[ \frac{1}{\nu \gamma_0 } R_{hr}^0(t,t') + \eta \gamma_0 C_h^0(t,t') \right] g^1(t')  \ , \ u^1(t) \sim \mathcal{GP}(0,C_h^0  )\nonumber
    \\
    &h^{\ell+1}(t) = h^\ell(t) + \beta u^{\ell+1}(t) + \beta \sum_{t'<t}  \left[  R_{hr}^{\ell}(t,t') + \eta \gamma_0 \beta C^{\ell}_{h}(t,t') \right] g^{\ell+1}(t')   \ , \ u^{\ell+1}(t) \sim \mathcal{GP}(0,C_h^\ell  )   \nonumber
    \\
    &g^L(t) = r^L(t) +  \eta \gamma_0 \sum_{t'<t} h^L(t') \nonumber \ , \  r^L(t) \sim \mathcal{GP}(0, 1 ) 
    \\
    &g^\ell(t) = g^{\ell+1}(t) + \beta r^{\ell}(t) + \beta \sum_{t' < t} \left[ R_{gu}^{\ell+1}(t,t') + \eta \gamma_0 \beta C_g^{\ell+1}(t,t')  \right] h^\ell(t') \ , \  r^\ell(t) \sim \mathcal{GP}(0, C_g^{\ell+1} ) 
\end{align}
To obtain the response functions $R^\ell_{hr}(t,t'), R^\ell_{gu}(t,t')$, we actually must compute all cross-layer response functions which form a closed system of equations as
\begin{align}
    \frac{\partial h^1(t)}{\partial u^k(t')} &= \delta_{k,1} \delta(t-t') +   \sum_{t''<t} \left[ \frac{1}{\nu \gamma_0 } R_{hr}^0(t,t'') + \eta \gamma_0 C_h^0(t,t'') \right] \frac{\partial g^1(t'')}{\partial u^1(t')}  \ , \ \nonumber
    \\
    \frac{\partial h^1(t)}{\partial r^k(t')} &=  \sum_{t''<t} \left[ \frac{1}{\nu \gamma_0 } R_{hr}^0(t,t'') + \eta \gamma_0 C_h^0(t,t'') \right] \frac{\partial g^1(t'')}{\partial r^k(t')}  \ , \ k \in \{ 1, ..., L \} \nonumber
    \\
    \frac{\partial h^{\ell+1}(t)}{\partial u^k(t')} &= \frac{\partial h^{\ell}(t)}{\partial u^k(t')} + \beta \sum_{t''<t} \left[ R^\ell_{hr}(t,t'') + \eta \gamma_0 \beta C_h^\ell(t,t'')  \right] \frac{\partial g^{\ell+1}(t'') }{\partial u^k(t')} \ , \ \ell \in \{1,...,L\} \nonumber
    \\
    \frac{\partial h^{\ell+1}(t)}{\partial r^k(t')} &= \frac{\partial h^{\ell}(t)}{\partial r^k(t')} + \beta \sum_{t''<t} \left[ R^\ell_{hr}(t,t'') + \eta \gamma_0 \beta C_h^\ell(t,t'')  \right] \frac{\partial g^{\ell+1}(t'') }{\partial r^k(t')} \ , \ \ell \in \{1,...,L\} \nonumber
    \\
    \frac{\partial g^L(t)}{\partial u^k(t')} &= \eta \gamma_0 \sum_{t''<t} \frac{\partial h^L(t'')}{\partial u^k(t')} \nonumber
    \\
     \frac{\partial g^L(t)}{\partial r^k(t')} &= \delta_{k,L} \delta(t-t') +  \eta \gamma_0 \sum_{t''<t} \frac{\partial h^L(t'')}{\partial r^k(t')} \nonumber
     \\ 
     \frac{\partial g^\ell(t)}{\partial u^k(t')} &= \frac{\partial g^{\ell+1}(t)}{\partial u^k(t')} + \beta \delta_{\ell,k} \delta(t-t') + \beta \sum_{t'' < t} \left[ R_{gu}^{\ell+1}(t,t'') + \eta \gamma_0 \beta C_g^{\ell+1}(t,t'')  \right] \frac{\partial h^\ell(t'')}{\partial u^k(t')}  \nonumber
     \\
     \frac{\partial g^\ell(t)}{\partial r^k(t')} &= \frac{\partial g^{\ell+1}(t)}{\partial r^k(t')} + \beta \delta_{\ell,k} \delta(t-t') + \beta \sum_{t'' < t} \left[ R_{gu}^{\ell+1}(t,t'') + \eta \gamma_0 \beta C_g^{\ell+1}(t,t'')  \right] \frac{\partial h^\ell(t'')}{\partial r^k(t')}  \nonumber 
\end{align}

After these response functions have been solved for, we must solve for the correlation functions using the single site densities. This gives a closed form prediction for the loss dynamics. 

\subsection{The Large $L$ Limit}\label{app:large_L_limit_resnet} The infinite depth limit with $\beta = \beta_0 / \sqrt{L}$ can be interpreted as a collection of ODEs for the response functions and a collection of ODEs for the correlation functions along the hidden layers. 

First, we note that under the scaling $\beta = \beta_0 / \sqrt{L}$, that for any $k > 1$, the responses scale as
\begin{align}
    \frac{\partial h^\ell(t)}{\partial u^k(t')} = \Theta\left( \frac{\beta_0}{\sqrt L} \right)  \ , \ \frac{\partial g^\ell(t)}{\partial u^k(t')} = \Theta\left( \frac{\beta_0}{\sqrt L} \right)  
\end{align}
and similarly that for all $k < L$, the response functions for the $r^k$ fields scale as
\begin{align}
    \frac{\partial h^\ell(t)}{\partial r^k(t')} = \Theta\left( \frac{\beta_0}{\sqrt L} \right)  \ , \ \frac{\partial g^\ell(t)}{\partial r^k(t')} = \Theta\left( \frac{\beta_0}{\sqrt L} \right)  
\end{align}

To take the large $L$ limit, we introduce a notion of ``layer time"
\begin{align}
    \tau = \frac{\ell}{L}  \in [0,1] 
\end{align}
and we recognize that the finite $L$ ResNet update equations approximate a SDE with a discretized process over $\tau \in [0,1]$ with step size $d\tau \sim 1/L$. We define the limited
\begin{align}
    h(\tau,t) = \lim_{L \to \infty} h^{\ell}(t) |_{\ell = L \tau} \ , \ g(\tau,t) = \lim_{L \to \infty} g^{\ell}(t) |_{\ell = L \tau} 
\end{align}
The functions we aim to characterize in the limit are the limiting correlation functions
\begin{align}
    C^\ell_h(\tau,t,t') \equiv \lim_{L \to \infty} C^{\ell}_h(t,t') |_{\ell = L \tau} \ , \ C^\ell_g(\tau,t,t') \equiv \lim_{L \to \infty} C^{\ell}_g(t,t') |_{\ell = L \tau}
\end{align}
and the following $\Theta(1)$ limiting response functions
\begin{align}
    R_{hr}(\tau,t,t') \equiv \lim_{L \to \infty} \frac{\sqrt{L}}{\beta_0} R^\ell_{hr}(t,t')|_{\ell = L \tau} \ , \ R_{gu}(\tau,t,t') \equiv \lim_{L \to \infty} \frac{\sqrt{L}}{\beta_0} R^\ell_{gu}(t,t')|_{\ell = L \tau}
\end{align}
With these functions introduced, the single site equations in the ``body" of the ResNet take the form of coupled SDEs
\begin{align}
    &dh(\tau,t) = \beta_0 du(\tau,t) + \beta_0^2 d\tau  \sum_{t'<t} \left[ R_{hr}(\tau,t,t')  + \eta \gamma_0 C_h(\tau,t,t') \right] g(\tau,t')  \nonumber
    \\
    &dg(\tau,t) = - \beta_0 dr(\tau,t) - \beta_0^2 d\tau  \sum_{t'<t} \left[ R_{gu}(\tau,t,t')  + \eta \gamma_0 C_g(\tau,t,t') \right] h(\tau,t') 
\end{align}
The noise sources in these SDEs have the correlation structure
\begin{align}
    \left< du(\tau,t) du(\tau',t') \right> = \delta(\tau-\tau')  C_h(\tau,t,t') d\tau d\tau' \ , \  \left< dr(\tau,t) dr(\tau',t') \right> = \delta(\tau-\tau')  C_g(\tau,t,t') d\tau d\tau' 
\end{align}
The $h$ SDE is to be integrated forward in time from $\tau = 0$ to $\tau = 1$ and the $g$ SDE is to be integrated backwards in time from $\tau = 1$ to $\tau = 0$. The necessary response and correlation functions can be computed from this SDE as
\begin{align}
    R_{hr}(\tau,t,t') = \left< \frac{\partial h(\tau,t)}{\partial du(\tau,t')} \right> \ , \ R_{gu}(\tau,t,t') = \left< \frac{\partial g(\tau,t)}{\partial dr(\tau,t')} \right> \nonumber
    \\
     R_{hu}(\tau,t,t') = \left< \frac{\partial g(\tau,t)}{\partial du(\tau,t')} \right> \ , \ R_{gr}(\tau,t,t') = \left< \frac{\partial g(\tau,t)}{\partial dr(\tau,t')} \right> \nonumber
    \\ 
    C_h(\tau,t,t') = \left< h(\tau,t) h(\tau,t') \right> \ , \ C_g(\tau,t,t') = \left< g(\tau,t) g(\tau,t') \right>
\end{align}
However, we note that the single-layer response functions do not actually close, but rather, all possible pairs of layer-times must be considered, ie we must compute the full collection 
\begin{align}
    R_{hr}(\tau,\tau',t,t') = \left< \frac{\partial h(\tau,t)}{\partial du(\tau',t')} \right> \ , \ R_{gu}(\tau,\tau',t,t') = \left< \frac{\partial g(\tau,t)}{\partial dr(\tau',t')} \right>
\end{align}
These four response functions form a set of ordinary differential equations 
\begin{align}
    &\partial_\tau R_{hu}(\tau,t,t') = \delta(t-t') + \beta_0 \sum_{t''<t} \left[ R_{hr}(\tau,t,t') + \eta \gamma_0 C_h(\tau,t,t')   \right] R_{gu}(\tau,t'',t') \nonumber
    \\
    &\partial_\tau R_{hr}(\tau,t,t') =  \beta_0 \sum_{t''<t} \left[ R_{hr}(\tau,t,t') + \eta \gamma_0 C_h(\tau,t,t')   \right] R_{gr}(\tau,t'',t') \nonumber
    \\
    &\partial_\tau R_{gu}(\tau,t,t')=  - \beta_0 \sum_{t''<t} \left[ R_{hr}(\tau,t,t') + \eta \gamma_0 C_h(\tau,t,t')   \right] R_{hu}(\tau,t'',t') \nonumber
    \\
    &\partial_\tau R_{gr}(\tau,t,t')= - \delta(t-t') - \beta_0 \sum_{t''<t} \left[ R_{hr}(\tau,t,t') + \eta \gamma_0 C_h(\tau,t,t')   \right] R_{hr}(\tau,t'',t') . 
\end{align}


\section{Dynamics of Online SGD}

In this section, we characterize discrete time online SGD where at each step $t$, a full batch of data $\bm X(t) \in \mathbb{R}^{B \times D}$ are sampled. This results in the following field definitions
\begin{align}
    \h^0(t) = \frac{\sqrt D}{B} \bm X(t)^\top \bm\Delta(t)  \ , \ \bm\Delta(t) = \frac{1}{\sqrt D} \bm X(t) \v(t) + \sigma \bm\epsilon(t) 
\end{align}
where all other variables are defined as before. Averaging over the samples drawn during SGD induces the following
\begin{align}
    &h^0(t) = u^0(t) + v(t) \ , \ u^0(t)  \sim \mathcal{N}\left(0, \frac{1}{\alpha_B} C_\Delta \  \delta(t-t')  \right)   
    \\
    &\Delta(t) = u_\Delta(t) + \sigma\epsilon(t) \ , \ u_\Delta(t) \sim \mathcal{N}(0, C_v) 
\end{align}
The key differences between online SGD and full batch GD training is
\begin{enumerate}
    \item The noise $u^0$ is local in training steps (decorrelated across time)
    \item There is no nonlocal response function $R_\Delta(t,t')$ that governs the buildup of the gap between test loss and train loss. Instead, the variance of $\left< h^0(t)^2 \right> = C_v(t,t) + \frac{1}{\alpha_B} C_\Delta(t,t)$ is simply shifted by the SGD noise effect. 
\end{enumerate}
The rest of the DMFT equations for $\{ h^\ell(t) , g^\ell(t) \}_{\ell \in \{1,..,L\}}$ are the same as before. 

\section{Online SGD with Structured (Dimension Free) Data}\label{app:structured_data}

In this section we discuss the effect of structured data on the dynamics. In this setting, we allow $\bm x \sim \mathcal{N}(0,\bm\Lambda)$ with $\text{Tr} \bm\Lambda < \infty$ in the $D \to \infty$ limit. Without loss of generality, we let $\bm\Lambda$ be diagonal with entries $\Lambda_{kk} = \lambda_k$ but allow for arbitrary $\w^\star$. In this case, the proper definition of the variables $\h^0$ and $\bm\Delta(t)$ are 
\begin{align}
    &\h^0(t) = \frac{1}{B} \bm X(t)^\top \bm\Delta(t)  \ , \  \bm\Delta(t) = \bm X(t) \v(t) + \sigma \bm\epsilon(t)  \nonumber
    \\
    &\h^1(t) = \W^0(t) \h^0(t) \ , \ \v(t) = \w^\star - \frac{1}{N \gamma_0} \W^0(t)^\top \g^1(t) + \frac{1}{N \gamma_0 } \W^0(0)^\top \g^1(0) 
\end{align}
With centering, this gives the following mean field description
\begin{align}
    h^0_k(t) &= u^0_k(t) + \lambda_k v_k(t)  \ , \ \Delta(t) = u_\Delta(t) + \sigma \epsilon(t) \nonumber \ , \ u^0_k(t) \sim \mathcal{N}\left( 0 , \frac{\lambda_k}{B} C_\Delta \delta(t-t') \right) \ , \ u_\Delta(t) \sim \mathcal{N}(0, C_v)
    \\
    v_k(t) &= w^\star_k - r^0_k(t) + r^0_k(0)  - \sum_{t'<t} \left[ \frac{1}{\gamma_0}  R^1_{gu}(t,t')  + \eta C_g^1(t,t') \right] h^0_k(t') \ , \ r^0_k(t) \sim \mathcal{N}\left(0, \frac{1}{N \gamma_0^2} C_g^1 \right)
\end{align}
The definition of the correlation functions for these fields changes to
\begin{align}
    C_v(t,t') = \sum_{k=1}^\infty \lambda_k \left< v_k(t) v_k(t')  \right> \ , \  C_h^0(t,t') = \sum_{k=1}^\infty  \left< h^0_k(t) h^0_k(t')  \right>
\end{align}
while the response function for the $h^0$ and $v$ variables change to
\begin{align}
    R_{hr}^0(t,t') = \sum_{k=1}^\infty \left< \frac{\partial h^0_k(t)}{\partial r_k^0(t') } \right> \ , \ R_{vu}(t,t') = \sum_{k=1}^\infty \lambda_k \left< \frac{\partial v_k(t)}{\partial u^0_k(t')} \right>  
\end{align}
which are both finite due to the trace class condition $\sum_k \lambda_k < \infty$. To express the test loss, we can introduce the $k$-th mode's transfer function $\mathcal H_k(t,t')$ which has the following definition
\begin{align}
    v_k(t) &= \sum_{t'<t} \mathcal H_k(t,t') \left[ w^\star_k - r^0_k(t') - \sum_{t''<t'} \left[ \gamma_0^{-1} R_{gu}^1(t',t'') + C_g^1(t',t'') \right] u^0_k(t'') \right] 
    \\
    &\mathcal{H}_k(t,t') \equiv \delta(t-t') -  \lambda_k \sum_{t''<t} \left[  \gamma_0^{-1} R_{gu}^1(t,t'') + \eta C_g^1(t,t'') \right] \mathcal{H}_k(t'',t') .
\end{align}
The other hidden layers of the network have the same single site dynamics as before,
\begin{align}
    h^1(t) &= u^1(t) + \sum_{t'<t} \left[ \frac{1}{N \gamma_0 } R_{hr}^0(t,t') + \eta \gamma_0 C_h^0(t,t') \right] g^1(t') \ , \ u^1(t) \sim \mathcal{N}(0, C_h^0) \nonumber
    \\
    h^\ell(t) &= u^\ell(t) + \sum_{t'<t} \left[  R_{hr}^{\ell-1}(t,t') + \eta \gamma_0  C_h^{\ell-1}(t,t') \right] g^\ell(t') \ , \  u^\ell(t) \sim \mathcal{N}(0, C_h^{\ell-1})  \nonumber
    \\
    g^L(t) &= r^L(t) + \eta \gamma_0 \sum_{t'<t} h^L(t')   \nonumber
    \\
    g^{\ell}(t) &= r^\ell(t) + \sum_{t'<t} \left[ R_{gu}^{\ell+1}(t,t') + \eta \gamma_0 C_g^{\ell+1}(t,t')  \right] h^\ell(t')  \ , \ u^\ell(t) \sim \mathcal{N}(0, C_g^{\ell+1})  \nonumber
\end{align}


\subsection{Scaling Laws for Easy and Hard Tasks}

We now examine the scaling law behavior with respect to time $t$ for the $N,B \to \infty$ limit. We also study the small learning rate regime, where we can study a continuous time gradient flow dynamics
\begin{align}
    \partial_t v_k(t) \approx - \lambda_k K(t) v_k(t) 
\end{align}
where $K(t) \approx C_g^1(t,t) + \gamma_0^{-1} R_{gu}^1(t,t)$ is related to the \textit{Neural Tangent Kernel} for the model \cite{jacot2018neural}. We make the following ansatz for the small $\gamma_0$ dynamics for $K(t)$
\begin{align}
    K(t) \sim 1 + \gamma_0^2 t^{\chi-1}  \ , \  \gamma_0 \to 0
\end{align}
where the above is understood to disregard prefactors constants. Following general arguments of \citet{bordelon2024featurelearningimproveneural}, we must try identifying solutions to the self consistent equation
\begin{align}
    \partial_t v_k(t) = - \lambda_k [ 1 +  \gamma_0^2 t^{\chi-1} ] v_k(t)  \implies v_k(t) = \exp\left( - \lambda_k [t + \gamma_0^2 
    t^{\chi}]    \right) w^\star_k
\end{align}
Following the self-consistency argument, we find that the kernel must scale as $K(t) \sim t^{1 - \beta \chi}$. Combining this with the assumption implies that
\begin{align}
    \chi = \frac{2}{1+\beta}   \ , \ \forall \beta < 1 . 
\end{align}
This implies the following loss scalings
\begin{align}
    \mathcal L(t) \sim 
    \begin{cases}   
        t^{-\beta} & \beta > 1
        \\
        t^{-\frac{2\beta}{1+\beta}} & \beta < 1
    \end{cases}
\end{align}
which we show in Figure \ref{fig:loss_scaling_hard}. The case where $\beta < 1$ is known as the ``hard task" regime while $\beta > 1$ is the ``easy task" regime for linear networks trained on powerlaw features.  

\begin{figure}
    \centering
\includegraphics[width=0.5\linewidth]{figures/scaling_laws_vary_beta_hard.pdf}
    \caption{Scaling laws for the hard task regime are well predicted by the adjusted exponent. }
    \label{fig:loss_scaling_hard}
\end{figure}

\section{Different Relative Widths}\label{app:diff_rel_widths}

We can also handle the case where hidden widths $\{  N_\ell \}_{\ell=1}^L$ differ across layers
\begin{align}
    f(\x) = \frac{1}{N_L \gamma_0} \w^L \prod_{\ell=1}^{L-1} \left( \frac{1}{\sqrt{N_{\ell}}} \W^{\ell} \right) \left(  \frac{1}{\sqrt D} \W^0 \right) \x .
\end{align}
In this case, the proportional limit of interest is
\begin{align}
    \alpha = \frac{P}{D} \ , \ \nu_\ell = \frac{N_\ell}{D} .
\end{align}
We can straightforwardly work out the limit using the same techniques as before. 


\section{Multiple Output Channels}\label{app:multiple_outputs} 

The case of multiple (but $\mathcal{O}(1)$) output channels $N_{c}$ can also be handled within our formalism. The cost of this analysis is to track correlation and response functions across all possible pairs of output channels as well as correlations across timesteps. We let the output channel $c$ have readout $\w^L_c$ as before
\begin{align}
    f_c(\x) = \frac{1}{N \gamma_0 } \w_c^L \prod_{\ell=1}^{L-1}\left( \frac{1}{\sqrt N} \W^\ell \right) \left( \frac{1}{\sqrt D} \W^0 \right) \x
\end{align}

In this case, we assume a collection of target vectors $\bm \w^\star_c$ for $c \in \{1 ,..., N_c \}$. In this case, the test error is governed by the following weight discrepancy 
\begin{align}
    \v_c(t) &= \w^\star_c - \frac{1}{N \gamma_0} \W^0(t)^\top \g^1_c(t) \nonumber
    \\
    \g^{\ell}_c(t) &= \frac{1}{\sqrt N} \W^{\ell}(t)^\top \g^{\ell+1}_c(t) \ , \ \g^L_c(t) = \w^L_c(t) 
\end{align}
The dynamics for the weights are
\begin{align}
    &\W^0(t+1) = \W^0(t)  + \frac{\eta \gamma_0}{\sqrt D } \sum_{c=1}^{N_c} \g^1_c(t) \h^0_c(t)^\top  \nonumber
    \\
    &\W^{\ell}(t+1) = \W^{\ell}(t) - \frac{\eta \gamma_0}{\sqrt N} \sum_{c=1}^{N_c} \g_c^{\ell+1}(t) \h_c^\ell(t)^\top \nonumber
    \\
    &\w^L_c(t+1) = \w^L_c(t) + \eta \gamma_0 \h_c(t)  
\end{align}
where the variables $\h^\ell_c(t)$ are defined as
\begin{align}
    \h^{\ell+1}_c(t) = \frac{1}{\sqrt N} \W^\ell(t) \h_c^\ell(t)  \ , \ \h^0_c(t) = \frac{\sqrt D}{P} \sum_{\mu=1}^P \Delta_{\mu,c}(t) \x_\mu
\end{align}
Expanding out the dynamics we find
\begin{align}
    \h^{\ell}_c(t) = \bm\chi^{\ell}_c(t) + \eta \gamma_0 \sum_{t' < t} \sum_{c'} C_{h, cc'}^{\ell-1}(t,t') \g_{c'}^\ell(t')  \nonumber
    \\
    \g^{\ell}_c(t) = \bm\xi^{\ell}_c(t) + \eta \gamma_0 \sum_{t' < t} \sum_{c'} C_{g, cc'}^{\ell+1}(t,t') \h_{c'}^\ell(t') 
\end{align}

As before we can carry out the DMFT computation but the order parameters that we need to track include cross-output channel correlations
\begin{align}
    &C_{v, cc'}^{\ell}(t,t') = \left< v_c(t) v_{c'}(t') \right>  \ , \  C_{\Delta, cc'}^{\ell}(t,t') = \left< \Delta_c(t)  \Delta_{c'}(t') \right> \nonumber
    \\
    &C_{h, cc'}^{\ell}(t,t') = \left< h^\ell_c(t) h^\ell_{c'}(t') \right>  \ , \  C_{g, cc'}^{\ell}(t,t') = \left< g^\ell_c(t)  g^\ell_{c'}(t') \right>
\end{align}
as well as cross channel response functions
\begin{align}
    &R_{vu,cc'}(t,t') =  \left< \frac{\delta v_c(t)}{\delta u_{c'}^0(t')}  \right> \ , \ R_{\Delta,cc'}(t,t') =  \left< \frac{\delta \Delta_c(t)}{\delta u_{\Delta, c'}(t')}  \right> \nonumber
    \\
    &R_{hr,cc'}^\ell(t,t') =  \left< \frac{\delta h_c^\ell(t)}{\delta r_{c'}^\ell(t')}  \right> \ , \ R_{gu,cc'}^\ell(t,t') =  \left< \frac{\delta g_c^\ell(t)}{\delta u_{c'}^\ell(t')}  \right> 
\end{align}
These correlations and response functions close in the obvious generalization of the scalar output case. 

\section{Comparing Computational Costs}

In this section we compare the cost of training a width $N$ and depth $L$ network on $P$ data with full batch GD to integrating the DMFT equations in different settings. 

\begin{table*}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
         &   Finite Network  & Linear DMFT & Res. DMFT & Non-Data Averaged Res. DMFT 
         \\
         \hline
        Memory  & $N^2 L$ & $L T^2$ & $L^2 T^2$ & $L^2 T^2 P^2$
        \\
                 \hline
        Compute  & $N^2 L P T$ & $L T^3$ & $L^2 T^3$ & $L^2 T^3 P^3$
        \\
        \hline
    \end{tabular}    
    \caption{Memory and computational cost to describe full batch gradient descent training of width $N$ depth $L$ network trained on dataset of size $P$ for $T$ steps. The non-data averaged residual DMFT refers to the cost of solving the equations in \cite{bordelon2024depthwise}. }
    \label{tab:comp_costs}
\end{table*}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

