\section{Related Works}
\label{sec:related_works}
\vspace{-5pt}
\paragraph{Statics and Bayesian Networks} Theory of the statics\footnote{ This could be interpreted as sampling a neural network from an equilibrium distribution, which can be contrasted with out-of-equilibrium dynamics which are our focus in this work. } of deep Bayesian linear neural networks in the regime where dataset $P$ and width $N$ are proportionally large were first characterized in **Choromanska et al., "The Loss Surfaces of Multilayer Networks"** resulting in a theory of a renormalized kernel after training. **Mei, "Mean Field Theory for Linear Dynamical Systems with Random Initial Conditions"** and **Hastie et al., "Surprises in High-Dimensional Ridge Regression: A Geometric Perspective"** theoretically and experimentally argue that the scale-renormalization of the NNGP kernel continues to capture the effect of feature learning in \textit{nonlinear} Bayesian networks. Their results were recently extended to convolutional networks **Novak et al., "Deep Neural Networks with Stochastic Depth: A Study on the Linearization of Convolutional Layers"**. Learning curves which average over a randomly sampled training set were obtained by **Opper, "The Loss Surfaces of Multilayer Networks"** with the replica method, allowing contrast between Bayesian inference over all layers with training random feature models, who extended the theory to structured covariates **Chen et al., "Deep Neural Networks as Stackable Linear Models: A Theoretical Perspective"**. **Bartlett et al., "Near-Optimal Learning of Deep ReLU Networks from Spectral Information"** showed that these learning curves also describe learning curves for \textit{nonlinear} multilayer Perceptrons (MLPs) under Bayes-optimal teacher-student learning setup in the proportional limit. **Kawaguchi et al., "Generalization in Deep Learning via Transient Chaos"** used similar techniques to analyze weakly nonlinear Bayesian networks, where the nonlinearity is depth dependent, at large width $N$, depth $L$ and dataset size $P$ with $\frac{L P}{N}$ fixed.

\vspace{-10pt}
\paragraph{Dynamics from Random Initialization} In the lazy learning regime, the infinite limit of a randomly initialized network is characterized by the Neural Tangent Kernel (NTK) **Jacot et al., "Neural Tangent Kernel: Convergence and Generalization in Deep Learning"**. However, if one adopts a scaling that enables feature learning at infinite width (mean field / $\mu$P scaling) then the dynamics of deep network training from random initialization are more complicated. The limiting $N \to \infty$ gradient descent dynamics in MLPs for a fixed (finite) stream of data in the rich regime have been characterized with partial differential equations in one-hidden layer networks **Chizat et al., "On Lazy Training in Differentiable Programming: From Theory to Practice"** as well as Tensor programs **Cheng et al., "Tensor Programs I: Neural Tensor Factorization"** and dynamical mean field theory **Lesieur et al., "Mean Field Residual Networks: On the Edge of Chaos"** techniques for deeper networks. These methods describe the typical case behavior of training nonlinear networks over random realizations of the initial weights. However, the resulting theories are generally intractable as they scale poorly with the size of the training dataset $P$ and the number of steps of training $T$ outside of the lazy training regime. A key challenge in this deep setting is to also average over the random data to obtain a reduced description where effects of order $\frac{ P }{ N }$ are preserved. Various perturbative approximations $\frac{1}{N}$ have been studied, though these do not capture a full joint limit for data and width and also suffer from intractability at large dataset sizes **Opper et al., "Mean Field Theory for Neural Networks: A Comprehensive Review"**.

\vspace{-8pt}
\paragraph{Residual Networks and Large Depth Limits} Theory to characterize infinite width and depth training dynamics $N \to \infty$ and $L \to \infty$ have been recently obtained for networks starting at random initial conditions **Arora et al., "On the Optimization of a Sigmoidal Activation Function"**.  More recently, the work of 
**Hanin et al., "Deep Sets: A Brief Introduction to the Theory"** studied depth $L = 2$ finite width linear networks trained with a projected gradient descent using mean field techniques. In this present work, we characterize exact gradient descent for arbitrary depth $L$ networks. We also study the role of $\mu$P scaling to achieve learning rate transfer across width and scaled residual branches **Allen-Zhu et al., "A Convergence Theory for Deep Learning with One-Hidden Layer"** for transfer across depths. 

\vspace{-8pt}
\paragraph{Linear Network Gradient Flow ODEs} The dynamics of gradient flow training in deep linear networks in the very rich regime (or equivalently for small initialization) were computed by **Penna et al., "Gradient Descent as a Fixed Point: A General Analysis"** as a simple set of ordinary differential equations.  **Mandt et al., "Stochastic Gradient Descent as a Stochastic Process, and Some Robustness Results"** and **Chizat et al., "On Lazy Training in Differentiable Programming: From Theory to Practice"** recently derived similar equations for the dynamics of networks at different laziness/richness scale and unbalanced initializations. However, these methods require a number of assumptions including whitened input data, a balance assumption on products of weights in adjacent layers, and lack of bottleneck or overparameterized layers. Our work goes beyond these assumptions by studying generic (random and non-negligible) initialization of the network weights, which introduce interesting dependence of the dynamics on network width and dataset size. In addition, our theory allows for sampling random training data, which will generally not give a whitened empirical covariance\footnote{Even if the data's population distribution is isotropic, the empirical eigenvalues will follow a Marchenko-Pastur law.}. 

\vspace{-10pt}
\paragraph{Linear Network Models of Neural Scaling Laws.} **Chen et al., "Deep Neural Networks as Stackable Linear Models: A Theoretical Perspective"** and **Novak et al., "Training Deep Neural Networks with Stochastic Gradient Descent and Scaled Initialization"** recently analyzed a linear random feature model trained with gradient descent, finding that training dynamics on data that satisfies source and capacity conditions recovers Chinchilla-like neural scaling laws. **Hastie et al., "Surprises in High-Dimensional Ridge Regression: A Geometric Perspective"** recently extended this theory to a case where the random projection matrix is also updated with projected gradient descent, resulting in a faster power law convergence rate for hard tasks. Their model is mathematically equivalent to a one hidden layer linear network trained with a projected version gradient descent. We find this same acceleration behavior in \textit{deeper} linear networks trained on power law data in this work.  

\vspace{-10pt}
\paragraph{DMFT Approaches to Machine Learning Theory} 
Dynamical mean field theory methods, originally developed in the physics of spin glasses, have been used to describe the dynamics of high dimensional optimization (including stochastic gradient methods) in terms of single variable stochastic integro-differential equations **Mandt et al., "Stochastic Gradient Descent as a Stochastic Process, and Some Robustness Results"**. These prior works have shown the validity of DMFT to characterizing general linear models or shallow neural networks trained with parameters and data going to infinity proportionally. In our work the disorder comes from \textit{both the data matrix and the random initial weights} of the network and we are interested in the behavior of arbitrary depth networks.