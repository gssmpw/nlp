

\section{Special Cases}

\paragraph{Symmetric matrix game.}  One important special class of matrix games is the symmetric matrix game~\citep{cheng2004notes}, with $A=-A^\top$, $\muref=\nuref$, and $m=n$. In this case, we assume the parameter space $\Omega$ preserves anti-symmetry of $A$, i.e., $A_{\omega}  = -A_{\omega}^{\top}$ for any $\omega \in \Omega$. For a symmetric matrix game, it admits a symmetric Nash $(\mu^\star,\mu^\star)$, and Algorithm~\ref{alg:matrix_game}  reduces to a single-player algorithm by only tracking a single policy $\mu_t$, recognizing $\mu_t=\nu_t$ and $\widetilde{\mu}_t= \widetilde{\nu}_t$ due to $f^{\mu,\nu}(A)=-f^{\nu,\mu}(A)$. In addition,  \name only needs to collect one sample from the policy pair $(\widetilde{\mu}_t,\mu_t)$ in each iteration. Altogether, these lead to a simplified algorithm summarized in Algorithm~\ref{alg:symmetric_matrix_game}.

  

 
\begin{algorithm}[th]
    \caption{Value-incentivized Online Symmetric Matrix Game (\name)}
    \label{alg:symmetric_matrix_game}
    \begin{algorithmic}[1]
    \STATE \textbf{Input:}  initial parameter $\omega_0$, regularization coefficient $\alpha>0$, iteration number $T$.  
    \STATE \textbf{Initialization:} dataset $\gD_0\coloneqq \emptyset$.
    \FOR{$t = 1,\cdots,T$}
    \STATE 
    Compute $\mu_t$ by solving the matrix game with the current parameter $\omega_{t-1}$:
    \begin{align}%\label{eq:policy_update}
        \mu_t&=\argmax_{\mu\in\Delta^m}\min_{\nu\in\Delta^n} f^{\mu,\nu}(A_{\omega_{t-1}}) .
    \end{align}
    \STATE Model update: Update the parameter $\omega_t$ by minimizing the following objective:
    \begin{align}%\label{eq:model_update}
        \omega_t=\argmin_{\omega\in\Omega}\sum_{(i,j,\widehat{A}(i,j))\in\gD_{t-1}}\left(A_\omega(i,j)-\widehat{A}(i,j)\right)^2+\alpha f^{\mu_t,\star}(A_{\omega}).
    \end{align}
    \STATE Compute $\widetilde{\mu}_t$  by solving the following optimization problem:
    \begin{align}%\label{eq:policy_update_2}
        \widetilde{\mu}_t& = \argmax_{\mu\in\Delta^m} f^{\mu,\mu_t}(A_{\omega_t}) .
    \end{align}
    \STATE Data collection: Sample $(i_t,j_t)\sim (\widetilde{\mu}_t,\mu_t)$  and get the noisy feedback $\widehat{A}(i_t,j_t)$ following the oracle \eqref{eq:noisy_feedback}. Update the dataset ${\gD}_t=\gD_{t-1}\cup\left\{ (i_t,j_t,\widehat{A}(i_t,j_t) ) \right\}$.
    \ENDFOR 
    \end{algorithmic}
\end{algorithm}


\paragraph{Bandit setting.} By setting $n=1$, we can reduce the matrix game to the bandit setting, where the payoff matrix becomes a reward vector $ A \in \mathbb{R}^{m}$, leading to a simplified algorithm in Algorithm~\ref{alg:bandit}. Here, we let $f^{\mu}(A) = \mu^{\top} A - \beta\KL{\mu}{\muref}$ and 
     $f^{\star}(A)\coloneqq \max_{\mu\in\Delta^m} f^{\mu}(A)$. Interestingly, to encourage exploration, the regularization term favors a reward estimate that maximizes its regret $  f^{\star}(A_{\omega}) -  f^{\mu_t}(A_{\omega}).$ on the current policy $\mu_t$, which is {\em different from} the reward-biasing framework that only regularizes against $  f^{\star}(A_{\omega})$ \citep{cen2024value,liu2020exploration}.   
     
\begin{algorithm}[th]
    \caption{Value-incentivized Online Bandit (\name)}
    \label{alg:bandit}
    \begin{algorithmic}[1]
    \STATE \textbf{Input:}  initial parameter $\omega_0$, regularization coefficient $\alpha>0$, iteration number $T$.  
    \STATE \textbf{Initialization:} dataset $\gD_0\coloneqq \emptyset$.
    \FOR{$t = 1,\cdots,T$}
    \STATE 
    Policy update: Compute $\mu_t$ with the current parameter $\omega_{t-1}$:
    \begin{align}%\label{eq:policy_update}
        \mu_t&=\argmax_{\mu\in\Delta^m}  f^{\mu}(A_{\omega_{t-1}})  \propto \muref \exp\left( \frac{A_{\omega_{t-1}} }{ \beta } \right).
    \end{align}
        \STATE Data collection: Sample $i_t \sim \mu_t $  and get the noisy feedback $\widehat{A}(i_t)$  following the oracle \eqref{eq:noisy_feedback}. Update the dataset ${\gD}_t=\gD_{t-1}\cup\left\{ (i_t,\widehat{A}(i_t) ) \right\}$.
   
    \STATE Model update: Update the parameter $\omega_t$ by minimizing the following objective:  
    \begin{align}%\label{eq:model_update}
        \omega_t=\argmin_{\omega\in\Omega}\sum_{(i,\widehat{A}(i))\in\gD_{t}}\left(A_\omega(i)-\widehat{A}(i)\right)^2 -\alpha f^{\star}(A_{\omega})+\alpha f^{\mu_t}(A_{\omega}).
    \end{align}

    \ENDFOR 
    \end{algorithmic}
\end{algorithm}


\paragraph{MDP setting.}  \name can be reduced to the single-agent setting via either setting the number of players $N=1$  in the multi-player general-sum Markov game, or setting the action space of the min player  to a singleton, i.e., $|\gA_2|=1$, in the two-player zero-sum Markov game. Interestingly, the former (option I) leads to the value regularization $V_{f}^{\star}(\rho)$ studied in MEX (c.f., Algorithm 1 in \citet{liu2024maximize}), while the latter (option II) leads to a new form of regularizer $V_{f}^{\star}(\rho)-V_{f}^{\pi_t}(\rho)$, adding friction from the current policy $\pi_t$. The latter regularizer is also the MDP counterpart of the bandit algorithm in Algorithm~\ref{alg:bandit}. We summarize both variants in Algorithm~\ref{alg:mdp}.


\begin{algorithm}[!ht]
    \caption{Value-incentivized Online Single-agent MDP (\name)}
    \label{alg:mdp}
    \begin{algorithmic}[1]
    \STATE \textbf{Input:}  initial transition kernel estimate $f_0\in\gF$, regularization coefficient $\alpha>0$, iteration number $T$.  
    \STATE \textbf{Initialization:} dataset $\gD_{0,h}\coloneqq \emptyset$ for all $h\in[H]$.
    \FOR{$t = 1,\cdots,T$}
    \STATE 
    Policy update: Compute $\pi_t$ with the current transition kernel estimator $f_{t-1}$:
    \begin{align}%\label{eq:policy_update}
        \pi_t&=\argmax_{\pi\in\Delta(\gA_1)} V^\pi_{f_{t-1}}(\rho).
    \end{align} 
        \STATE Data collection: sample a trajectory with transition tuples $\{(s_{t,h},a_{t,h},s_{t,h+1})\}_{h=1}^H$ following $\pi_t$. Update the dataset $\gD_{t,h}=\gD_{t-1,h}\cup\left\{ (s_{t,h},a_{t,h},s_{t,h+1}) \right\}$ for all $h\in[H]$.
   
 
    \STATE Model update: update the estimator $f_t$ by minimizing the following objective
        \begin{align}%\label{eq:model_update}
    f_t=   \begin{cases} 
    \argmin_{f\in\gF}\sum_{h=1}^H\sum_{(s_h,a_h,s_{h+1})\in\gD_{t,h}}-\log \PP_{f,h}(s_{h+1}|s_h,a_h) -\alpha V_f^\star(\rho)  &  \text{(option I)} \\
    \argmin_{f\in\gF}\sum_{h=1}^H\sum_{(s_h,a_h,s_{h+1})\in\gD_{t,h}}-\log \PP_{f,h}(s_{h+1}|s_h,a_h) -\alpha V_f^\star(\rho)+\alpha V_{f}^{\pi_t}(\rho) & \text{(option II)} 
    \end{cases}
     .
    \end{align}
 
    \ENDFOR 
    \end{algorithmic}
\end{algorithm}

