%\newpage
\section{Proofs of Main Theorems}\label{app:proofs}

\input{appendix_prelim.tex}

\subsection{Proof of Theorem~\ref{thm:regret}}\label{app:proof_regret}
In the proof, for any sequence $\{x_i\}_{i \in \mathbb{Z}}$ and any integers $a,b \in \mathbb{Z}$ where $a > b$, we define $\sum_{i=a}^b x_i \coloneqq 0$.
 
We begin by decomposing the regret as
\begin{align}\label{eq:regret_dec1}
    \regret(T) &= \sum_{t=1}^T f^{\star,\nu_t}(A) - f^{\mu_t,\star}(A)\notag\\
    &= \sum_{t=1}^T \left( f^{\star,\nu_t}(A) - f^{\mu_t,\star}(A) - \left(f^{\star,\nu_t}(A_{\omega_t}) - f^{\mu_t,\star}(A_{\omega_t}) \right)\right) \notag\\
    &\quad + \sum_{t=1}^T \left(f^{\star,\nu_t}(A_{\omega_t})-f^{\widetilde{\mu}_t,\nu_t}(A)\right) + \sum_{t=1}^T \left(f^{\mu_t,\widetilde{\nu}_t}(A)-f^{\mu_t,\star}(A_{\omega_t})\right)\notag\\
    &\quad + \sum_{t=1}^T \left(f^{\widetilde{\mu}_t,\nu_t}(A)-f^{\widetilde{\mu}_t,\nu_t}(A_{\omega_{t-1}})\right) + \sum_{t=1}^T \left(f^{\mu_t,\widetilde{\nu}_t}(A_{\omega_{t-1}})-f^{\mu_t,\widetilde{\nu}_t}(A)\right)\notag\\
    &\quad + \sum_{t=1}^T \left(f^{\widetilde{\mu}_t,\nu_t}(A_{\omega_{t-1}})-f^{\mu_t,\widetilde{\nu}_t}(A_{\omega_{t-1}})\right).
\end{align}

Recall that $(\mu_t,\nu_t)$ is the Nash equilibrium of the matrix game with the pay-off matrix $A_{\omega_{t-1}}$ (see \eqref{eq:policy_update}), we have
\begin{align}\label{eq:last_term_nash}
    \forall t\in[T]:\quad f^{\widetilde{\mu}_t,\nu_t}(A_{\omega_{t-1}}) \leq f^{\mu_t,\nu_t}(A_{\omega_{t-1}})\leq f^{\mu_t,\widetilde{\nu_t}}(A_{\omega_{t-1}}).
\end{align}
This implies the last term in the regret decomposition is non-positive, i.e.,
\begin{align}\label{eq:last_term_nonpos}
    \sum_{t=1}^T \left(f^{\widetilde{\mu}_t,\nu_t}(A_{\omega_{t-1}})-f^{\mu_t,\widetilde{\nu}_t}(A_{\omega_{t-1}})\right)\leq 0.
\end{align}

Moreover, by the definition of $\widetilde{\mu}_t$ and $\widetilde{\nu}_t$ in \eqref{eq:policy_update_2}, we have
\begin{align}\label{eq:dagger_equation}
    f^{\star,\nu_t}(A_{\omega_t})=f^{\widetilde{\mu}_t,\nu_t}(A_{\omega_t})\quad\text{and}\quad f^{\mu_t,\star}(A_{\omega_t})=f^{\mu_t,\widetilde{\nu}_t}(A_{\omega_t}).
\end{align}

Combining \eqref{eq:last_term_nonpos}, \eqref{eq:dagger_equation} with \eqref{eq:regret_dec1}, we have
\begin{align}\label{eq:regret_decomposition}
    \regret(T) &\leq \underbrace{\sum_{t=1}^T \left( f^{\star,\nu_t}(A) - f^{\mu_t,\star}(A) - \left(f^{\star,\nu_t}(A_{\omega_t}) - f^{\mu_t,\star}(A_{\omega_t}) \right)\right) }_{\text{(i)}}\notag\\
    &\quad + \underbrace{\sum_{t=1}^T \left(f^{\widetilde{\mu}_t,\nu_t}(A_{\omega_t})-f^{\widetilde{\mu}_t,\nu_t}(A)\right) + \sum_{t=1}^T \left(f^{\mu_t,\widetilde{\nu}_t}(A)-f^{\mu_t,\widetilde{\nu}_t}(A_{\omega_t})\right)}_{\text{(ii)}}\notag\\
    &\quad + \underbrace{\sum_{t=1}^T \left(f^{\widetilde{\mu}_t,\nu_t}(A)-f^{\widetilde{\mu}_t,\nu_t}(A_{\omega_{t-1}})\right) + \sum_{t=1}^T \left(f^{\mu_t,\widetilde{\nu}_t}(A_{\omega_{t-1}})-f^{\mu_t,\widetilde{\nu}_t}(A)\right)}_{\text{(iii)}}.
\end{align}

We will upper bound the three terms in the right-hand side of \eqref{eq:regret_decomposition} separately.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Step 1: bounding term (i).} Define the squared loss function $L_t(\omega)$ over the dataset $\gD_{t-1}$ as
\begin{align}\label{eq:L}
    L_t(\omega)\coloneqq \sum_{(i,j,\widehat{A}(i,j))\in\gD_{t-1}}\left(A_\omega(i,j)-\widehat{A}(i,j)\right)^2.
\end{align}
Then by the optimality of $\omega_t$ for \eqref{eq:model_update}, we know that
\begin{align*}
    L_t(\omega_t) - \alpha f^{\star,\nu_t}(A_{\omega_t}) + \alpha f^{\mu_t,\star}(A_{\omega_t})\leq L_t(\omega^\star) - \alpha f^{\star,\nu_t}(A) + \alpha f^{\mu_t,\star}(A),
\end{align*}
where we use Assumption~\ref{asmp:expressive}, which implies $A_{\omega^\star}=A$.
Reorganizing the terms, we have
\begin{align}\label{eq:(i)_ub_by_L}
    \text{(i)}\leq \frac{1}{\alpha}\sum_{t=1}^T\left(L_t(\omega^\star)-L_t(\omega_t)\right).
\end{align}
Thus, it is sufficient to bound the term $\sum_{t=1}^T\left(L_t(\omega^\star)-L_t(\omega_t)\right)$. 
For any $t\in[T]$, we denote
$$\widehat{A}(i_t,j_t)=A(i_t,j_t)+\xi_t\quad{and}\quad  \widehat{A}(i'_t,j'_t)=A(i_t,j_t)+\xi'_t.$$
It follows that we can rewrite $L_t(\omega)$ as
\begin{align*}
    L_t(\omega)&=\sum_{s=1}^{t-1} \left(A_\omega(i_s,j_s)-A(i_s,j_s)-\xi_s\right)^2+\sum_{s=1}^{t-1} \left(A_\omega(i'_s,j'_s)-A(i'_s,j'_s)-\xi'_s\right)^2,
\end{align*}
from which we deduce
\begin{align}\label{eq:L_decomposition}
    L_t(\omega^\star)-L_t(\omega) 
    &=-\sum_{s=1}^{t-1} \left[\left(A_\omega(i_s,j_s)-A(i_s,j_s)-\xi_s\right)^2 - \xi_s^2\right]
    -\sum_{s=1}^{t-1} \left[\left(A_\omega(i'_s,j'_s)-A(i'_s,j'_s)-\xi'_s\right)^2 - (\xi'_s)^2\right]\notag\\ 
    &= -\sum_{s=1}^{t-1} \underbrace{\left[\left(A_\omega(i_s,j_s)-A(i_s,j_s)\right)\left(A_\omega(i_s,j_s)-A(i_s,j_s)-2\xi_s\right)\right]}_{\coloneqq X_s^\omega}\notag\\
    &\quad-\sum_{s=1}^{t-1} \underbrace{\left[\left(A_\omega(i'_s,j'_s)-A(i'_s,j'_s)\right)\left(A_\omega(i'_s,j'_s)-A(i'_s,j'_s)-2\xi'_s\right)\right]}_{\coloneqq Y_s^\omega}.
\end{align}

It is then sufficient to bound $-\sum_{s=1}^{t-1} X_s^\omega$ and $-\sum_{s=1}^{t-1} Y_s^\omega$, which is supplied by the following lemma.  
\begin{lm}\label{lm:regret_on_squared_loss} 
When Assumption~\ref{asmp:bounded_payoff}, \ref{asmp:expressive}, \ref{asmp:noise} hold, for any $\delta \in (0,1)$, with probability at least $1-\delta$, it holds  
\begin{align}\label{eq:sum_X_bound}
    \forall t\in[T],\omega\in\Omega:\quad -\sum_{s=1}^{t-1} X_s^\omega &\leq - \frac{1}{2} \sum_{s=1}^{t-1}\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right]\notag\\
    &\quad+ C B_l\left(B_l+\sigma\sqrt{2\log(8T/\delta)}\right)  \left(\log\left(\frac{4T}{\delta}\right)+d\log\left(1+2T\sqrt{d}\right)\right) 
\end{align}
and
\begin{align}\label{eq:sum_Y_bound}
    \forall t\in[T],\omega\in\Omega:\quad -\sum_{s=1}^{t-1} Y_s^\omega &\leq - \frac{1}{2} \sum_{s=1}^{t-1}\E_{i\sim\mu_s,j\sim\widetilde{\nu}_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right]\notag\\
    &\quad+ C B_l\left(B_l+\sigma\sqrt{2\log(8T/\delta)}\right)  \left(\log\left(\frac{4T}{\delta}\right)+d\log\left(1+2T\sqrt{d}\right)\right)
\end{align}
where $C>0$ is some universal constant.
\end{lm}

Combining \eqref{eq:L_decomposition}, \eqref{eq:(i)_ub_by_L} and Lemma~\ref{lm:regret_on_squared_loss} leads to a bound of term (i):
\begin{align}\label{eq:bound_i}
    \text{(i)}\leq \frac{1}{\alpha}\bigg\{-\frac{1}{2} \sum_{t=1}^T\sum_{s=1}^{t-1}\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_{\omega_t}(i,j)-A(i,j)\right)^2\right]
    -  \frac{1}{2} \sum_{t=1}^T\sum_{s=1}^{t-1}\E_{i\sim\mu_s,j\sim\widetilde{\nu}_s}\left[\left(A_{\omega_t}(i,j)-A(i,j)\right)^2\right]\notag\\
    +2T\cdot C B_l\left(B_l+\sigma\sqrt{2\log(8T/\delta)}\right)  \left(\log\left(\frac{4T}{\delta}\right)+d\log\left(1+2T\sqrt{d}\right)\right)\bigg\}.
\end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Step 2: bounding terms (ii) and (iii).} To bound (ii) and (iii), we first prove the following lemma.
\begin{lm}\label{lm:diff_f}
    For any $\{(\widehat{\mu}_t,\widehat{\nu}_t)\}_{t\in[T]}\subset \Delta^m\times\Delta^n$ and any $\{\widehat{\omega}_t\}_{t\in[T]}\subset\Omega$, we have
    \begin{align}\label{eq:diff_f}
        \sum_{t=1}^T \left| f^{\widehat{\mu}_t,\widehat{\nu}_t}(A_{\widehat{\omega}_t}) - f^{\widehat{\mu}_t,\widehat{\nu}_t}(A)\right| 
    &\leq \frac{d(\lambda)}{2\eta} +\frac{\eta}{2}\sum_{t=1}^T \sum_{s=1}^{t-1} \E_{i\sim\widehat{\mu}_s,j\sim\widehat{\nu}_s} [(A_{\widehat{\omega}_t}(i,j)-A(i,j))^2]  \nonumber\\ 
    & \quad + (\sqrt{d}+2B_l) \min\{d(\lambda),T\} +\sqrt{d} \lambda
    \end{align}
    for any $\lambda,\eta>0$, where $d(\lambda)\coloneqq 2d\log\left(1+\frac{T}{d\lambda}\right)$.
\end{lm}

By letting $\widehat{\mu}_t=\widetilde{\mu}_t$, $\widehat{\nu}_t=\nu_t$ and $\widehat{\omega}_t=\omega_t$ in Lemma~\ref{lm:diff_f}, we have
\begin{align}\label{eq:bound_ii_1}
    \sum_{t=1}^T \left(f^{\widetilde{\mu}_t,\nu_t}(A_{\omega_t})-f^{\widetilde{\mu}_t,\nu_t}(A)\right)
    &\leq \frac{d(\lambda)}{2\eta} +\frac{\eta}{2}\sum_{t=1}^T \sum_{s=1}^{t-1} \E_{i\sim\widetilde{\mu}_s,j\sim\nu_s} [(A_{\omega_t}(i,j)-A(i,j))^2] \notag\\
    & \quad+ (\sqrt{d}+2B_l) \min\{d(\lambda),T\} +\sqrt{d} \lambda T. 
\end{align} 


By letting $\widehat{\mu}_t=\mu_t$, $\widehat{\nu}_t=\widetilde{\nu}_t$ and $\widehat{\omega}_t=\omega_t$ in Lemma~\ref{lm:diff_f}, we have 
\begin{align}\label{eq:bound_ii_2}
    \sum_{t=1}^T \left(f^{\mu_t,\widetilde{\nu}_t}(A)-f^{\mu_t,\widetilde{\nu}_t}(A_{\omega_t})\right)     &\leq \frac{d(\lambda)}{2\eta} +\frac{\eta}{2}\sum_{t=1}^T \sum_{s=1}^{t-1} \E_{i\sim\mu_s,j\sim\widetilde{\nu}_s} [(A_{\omega_t}(i,j)-A(i,j))^2] \nonumber \\
    & \quad + (\sqrt{d}+2B_l) \min\{d(\lambda),T\} +\sqrt{d} \lambda T. 
\end{align}

Similarly, we have  
\begin{align}\label{eq:bound_iii_1a}
     \sum_{t=1}^T \left(f^{\widetilde{\mu}_t,\nu_t}(A)-f^{\widetilde{\mu}_t,\nu_t}(A_{\omega_{t-1}})\right)&\leq \frac{d(\lambda)}{2\eta} +\frac{\eta}{2}\sum_{t=1}^T \sum_{s=1}^{t-2} \E_{i\sim\widetilde{\mu}_s,j\sim\nu_s} [(A_{\omega_{t-1}}(i,j)-A(i,j))^2]  \nonumber \\
    &\quad + (\sqrt{d}+2B_l) \min\{d(\lambda),T\} +\sqrt{d} \lambda T + 2B_l^2\eta T,
 \end{align}
 which uses  the fact  
$$\E_{i\sim\widetilde{\mu}_{t-1},j\sim\nu_{t-1}} [(A_{\omega_{t-1}}(i,j)-A(i,j))^2]\leq 4B_l^2.$$
Notice that the second term in \eqref{eq:bound_iii_1a} can be further bounded by
 \begin{align}   
   \sum_{t=1}^T \sum_{s=1}^{t-2} \E_{i\sim\widetilde{\mu}_s,j\sim\nu_s} [(A_{\omega_{t-1}}(i,j)-A(i,j))^2]   &= \sum_{t=0}^{T-1} \sum_{s=1}^{t-1} \E_{i\sim\widetilde{\mu}_s,j\sim\nu_s} [(A_{\omega_{t}}(i,j)-A(i,j))^2]   \notag\\
    &\leq \sum_{t=0}^{T}  \sum_{s=1}^{t-1} \E_{i\sim\widetilde{\mu}_s,j\sim\nu_s} [(A_{\omega_{t}}(i,j)-A(i,j))^2]  \notag\\
    &= \sum_{t=1}^{T} \sum_{s=1}^{t-1} \E_{i\sim\widetilde{\mu}_s,j\sim\nu_s} [(A_{\omega_{t}}(i,j)-A(i,j))^2] ,
\end{align}
where the first line shifts the index of $t$ by 1, and the last equality holds 
because the term is $0$ when $t=0$. Plugging the above inequality back to \eqref{eq:bound_iii_1a} leads to
\begin{align}\label{eq:bound_iii_1}
     \sum_{t=1}^T \left(f^{\widetilde{\mu}_t,\nu_t}(A)-f^{\widetilde{\mu}_t,\nu_t}(A_{\omega_{t-1}})\right) &= \frac{d(\lambda)}{2\eta} +\frac{\eta}{2}\sum_{t=1}^{T} \sum_{s=1}^{t-1} \E_{i\sim\widetilde{\mu}_s,j\sim\nu_s} [(A_{\omega_{t}}(i,j)-A(i,j))^2] \nonumber \\
   &\quad  + (\sqrt{d}+2B_l) \min\{d(\lambda),T\} +\sqrt{d} \lambda T + 2B_l^2\eta T.
\end{align}
Analogously, we have
\begin{align}\label{eq:bound_iii_2}
    \sum_{t=1}^T \left(f^{\mu_t,\widetilde{\nu}_t}(A_{\omega_{t-1}})-f^{\mu_t,\widetilde{\nu}_t}(A)\right) &\leq \frac{d(\lambda)}{2\eta} +\frac{\eta}{2}\sum_{t=1}^{T} \sum_{s=1}^{t-1} \E_{i\sim\mu_s,j\sim\widetilde{\nu}_s} [(A_{\omega_{t}}(i,j)-A(i,j))^2]  \notag\\
    &\quad  + (\sqrt{d}+2B_l) \min\{d(\lambda),T\} +\sqrt{d} \lambda T + 2B_l^2\eta T.
\end{align}


Combining \eqref{eq:bound_ii_1}, \eqref{eq:bound_ii_2}, \eqref{eq:bound_iii_1}, and \eqref{eq:bound_iii_2}, we have \begin{align}\label{eq:bound_ii}
    \text{(ii)}+\text{(iii)}\leq  
    \eta \left\{\sum_{t=1}^T \sum_{s=1}^{t-1} \E_{i\sim\widetilde{\mu}_s,j\sim\nu_s} [(A_{\omega_t}(i,j)-A(i,j))^2] + \sum_{t=1}^T \sum_{s=1}^{t-1} \E_{i\sim\mu_s,j\sim\widetilde{\nu}_s} [(A_{\omega_t}(i,j)-A(i,j))^2]\right\}\notag\\%\tag{II}
    +\frac{2d(\lambda)}{\eta} +4(\sqrt{d}+2B_l) \min\{d(\lambda),T\} +4\sqrt{d} \lambda T + 4B_l^2\eta T.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Step 3: combining the bounds.} 
Letting $\eta=\frac{1}{2\alpha}$ in \eqref{eq:bound_ii}, the first line of \eqref{eq:bound_i} could cancel out the first line of \eqref{eq:bound_ii}, which leads to
\begin{align}
    \regret(T) &= \text{(i)}+\text{(ii)}+\text{(iii)}\notag\\
    &= \frac{T}{\alpha}\cdot2C B_l\left(B_l+\sigma\sqrt{2\log(8T/\delta)}\right)  \left(\log\left(\frac{4T}{\delta}\right)+d\log\left(1+2T\sqrt{d}\right)\right)\notag\\
    &\quad + 4\alpha d(\lambda) +4(\sqrt{d}+2B_l) \min\{d(\lambda),T\} +4\sqrt{d} \lambda T + \frac{2B_l^2T}{\alpha}
\end{align}
with probability at least $1-\delta$.

By choosing
$$\alpha=\sqrt{\frac{T\left(\log(4T/\delta)+d\log\left(1+2\sqrt{d}T\right)\right)}{d\log\left(1+(T/d)^{3/2}\right)}}\quad\text{and}\quad \lambda=\sqrt{\frac{d}{T}},$$
we have with probability at least $1-\delta$,
\begin{align}\label{eq:regret_bd_complete}
    &\regret(T)\notag\\ &\leq  
2\left(CB_l\left(B_l+\sigma\sqrt{2\log(8T/\delta)}\right)+1\right)d\sqrt{T}\cdot\sqrt{\left(\frac{1}{d}\log(4T/\delta)+\log\left(1+2\sqrt{d}T\right)\right)\log\left(1+(T/d)^{3/2}\right)}\notag\\
    &\quad+ 2B_l^2\sqrt{T}\sqrt{\frac{d\log\left(1+(T/d)^{3/2}\right)}{\log(4T/\delta)+d\log\left(1+2\sqrt{d}T\right)}}+4(\sqrt{d}+2B_l) d\log\left(1+(T/d)^{3/2}\right)+4d\sqrt{T}
\end{align}
for some absolute constant $C>0$,
and thus the regret could be bounded by \eqref{eq:regret_bound} by simplifying the logarithmic terms.


\subsubsection{Proof of Lemma~\ref{lm:regret_on_squared_loss}}

To begin, by Assumption~\ref{asmp:noise} together with the sub-Gaussian concentration inequality, we  have that with probability at least $1-\frac{\delta}{2}$, for any $s\in[T]$ and $\omega\in\Omega$,
\begin{align*}
    \PP(|\xi_s|\geq a)\leq 2\exp\left(-\frac{a^2}{2\sigma^2}\right)\quad\text{and}\quad \PP(|\xi'_s|\geq a)\leq 2\exp\left(-\frac{a^2}{2\sigma^2}\right)
    \quad\text{for all }a>0,%\label{eq:subgaussian}
\end{align*}
which implies that with probability at least $1-\frac{\delta}{2}$,
\begin{align}\label{eq:bound_xi}
    |\xi_s|\leq \sigma\sqrt{2\log(8T/\delta)},\quad |\xi'_s|\leq \sigma\sqrt{2\log(8T/\delta)},\quad\forall s\in[T].
\end{align} 
We let  $\mathcal{E}$ be the event that \eqref{eq:bound_xi} holds for all $s\in[T]$, which satisfies $\PP(\mathcal{E})\geq 1-\frac{\delta}{2}$. 


Next, we define filtrations $\gF_t\coloneqq \sigma(\gD_{t})$ for all $t\in[T]$. By Assumption~\ref{asmp:noise}, we have for all $s\in[T]$ and $\omega\in\Omega$,
\begin{align}
    \E[X_s^\omega|  \gF_{s-1}]&=\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right],\label{eq:conditional_expectation_X}\\
    \E[Y_s^\omega|\gF_{s-1}]&=\E_{i\sim\mu_s,j\sim\widetilde{\nu}_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right]\label{eq:conditional_expectation_Y}.
\end{align}

We also have
\begin{align}\label{eq:conditional_var}
    \var\left[X_s^\omega|\gF_{s-1}\right]&\leq \E\left[(X_s^\omega)^2|\gF_{s-1}\right]\notag\\
    &=\E\left[\left(A_\omega(i_s,j_s)-A(i_s,j_s)\right)^2\left(A_\omega(i_s,j_s)-A(i_s,j_s)-2\xi_s\right)^2|\gF_{s-1}\right]\notag\\
    &\leq 4(B_l^2+\sigma^2)\E\left[\left(A_\omega(i_s,j_s)-A(i_s,j_s)\right)^2|\gF_{s-1}\right]\notag\\
    &=4(B_l^2+\sigma^2)\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right],
\end{align}
where we use Assumptions~\ref{asmp:bounded_payoff}, \ref{asmp:expressive} and \ref{asmp:noise} in the last inequality: Assumption~\ref{asmp:expressive} guarantees that $A_{\omega^\star}=A$, and Assumption~\ref{asmp:bounded_payoff} indicates that $\norm{A(i,j)}_\infty\coloneqq\max_{i\in[m],j\in[n]}|A(i,j)|\leq B_l$ and $\norm{A_\omega(i,j)}_\infty\leq B_l$ for all $\omega\in\Omega$; moreover, Assumption \ref{asmp:noise} implies $\E\xi_s^2\leq \sigma^2$. 
Conditioned on event $\mathcal{E}$,
we can bound $|X_s^\omega-\E[X_s^\omega|\gF_{s-1}]|$ using \eqref{eq:L_decomposition} and \eqref{eq:conditional_expectation_X} as follows:
\begin{align}\label{eq:bound_abs_X}
    & \left|X_s^\omega-\E[X_s^\omega|\gF_{s-1}]\right|\notag\\
    &=\left|\left(A_\omega(i_s,j_s)-A(i_s,j_s)\right)\left(A_{\omega}(i_s,j_s)-A(i_s,j_s)-2\xi_s\right)-\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_{\omega}(i,j)-A(i,j)\right)^2\right]\right|\notag\\
    &\leq \left|\left(A_\omega(i_s,j_s)-A(i_s,j_s)\right)^2-\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right]\right|+2|\xi_s|\left|A_\omega(i_s,j_s)-A(i_s,j_s)\right|\notag\\
    &\leq 4B_l\left(B_l+\sigma\sqrt{2\log(8T/\delta)}\right).
\end{align}

In what follows, we apply a standard covering argument together with the Freedman's inequality to prove the desired bound, conditioned on event $\mathcal{E}$. First, for any $\gX\subset\R^d$, let $\gN(\gX,\epsilon,\norm{\cdot})$ be the $\epsilon$-covering number of $\gX$ with respect to the norm $\norm{\cdot}$. 
By Assumption~\ref{asmp:bounded_payoff} we know that $\Omega\subset\mathbb{B}_2^d(\sqrt{d})$. Thus by Lemma~\ref{lm:covering} we have
\begin{align}\label{eq:covering}
    \log\gN(\Omega, \epsilon, \norm{\cdot}_2)\leq \log\gN(\mathbb{B}_2^d(\sqrt{d}), \epsilon, \norm{\cdot}_2)\leq d\log\left(1+\frac{2\sqrt{d}}{\epsilon}\right)
\end{align}
for any $\epsilon>0$.
%
In other words, for any $\epsilon>0$, there exists an $\epsilon$-net $\Omega_\epsilon\subset\Omega$ such that $\log|\Omega_\epsilon|\lesssim d\log\left(1+\frac{2\sqrt{d}}{\epsilon}\right)$.

Applying Freedman's inequality (c.f. Lemma~\ref{lm:Freedman}) to the martingale difference sequence $\{\E[X_s^\omega|\gF_{s-1}]-X_s^\omega\}_{s\in[T]}$ and making use of \eqref{eq:conditional_expectation_X}, \eqref{eq:conditional_var} and \eqref{eq:bound_abs_X} we have under event $\mathcal{E}$, with probability at least $1-\frac{\delta}{4}$,
\begin{align*}%\label{eq:union_bound_X}
    \forall t\in[T],\,\omega\in\Omega_\epsilon: \qquad &  \sum_{s=1}^t\left(\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right] - X_s^\omega\right)\notag\\
    &\leq\frac{1}{2} \sum_{s=1}^t\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right]\notag\\
    &\quad+ 4C B_l\left(B_l+\sigma\sqrt{2\log(8T/\delta)}\right) \left(\log\left(\frac{4T}{\delta}\right)+d\log\left(1+\frac{2\sqrt{d}}{\epsilon}\right)\right),
\end{align*}
where $C>0$ is an absolute constant.

In addition, conditioned on event $\mathcal{E}$, 
for any $\omega,\omega'\in\Omega$, $\norm{\omega-\omega'}_2\leq \epsilon$, we have
\begin{align*}
& \left|\left(\frac{1}{2}\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right] - X_s^\omega\right)-\left(\frac{1}{2}\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_{\omega'}(i,j)-A(i,j)\right)^2\right] - X_s^{\omega'}\right)\right|\notag\\
&\leq \frac{1}{2}\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left|\left(A_\omega(i,j)-A(i,j)\right)^2-\left(A_{\omega'}(i,j)-A(i,j)\right)^2\right|+|X_s^\omega-X_s^{\omega'}|\notag\\
&\leq \left(6B_l+2\sigma\sqrt{2\log(8T/\delta)}\right)\epsilon.
\end{align*}
Thus combining the above two expressions and set $\epsilon=\frac{1}{T}$, we have that under event $\mathcal{E}$, with probability at least $1-\frac{\delta}{4}$,
\begin{align}%\label{eq:sum_X_bound}
    \forall t\in[T],\omega\in\Omega:&\quad \sum_{s=1}^t\left(\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right] - X_s^\omega\right)\notag\\
    &\leq\frac{1}{2} \sum_{s=1}^t\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right]\notag\\
    &\quad+ 4C B_l\left(B_l+\sigma\sqrt{2\log(8T/\delta)}\right)  \left(\log\left(\frac{4T}{\delta}\right)+d\log\left(1+2T\sqrt{d}\right)\right)  
    \end{align}
for sufficiently large constant $C$.
 

Rearanging terms, we have with probability at least $1-\frac{\delta}{4}$,
\begin{align}\label{eq:sum_X_bound_proof}
    \forall t\in[T],\omega\in\Omega:\quad -\sum_{s=1}^{t-1} X_s^\omega &\leq - \frac{1}{2} \sum_{s=1}^{t-1}\E_{i\sim\widetilde{\mu}_s,j\sim\nu_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right]\notag\\
    &\quad+ 4C B_l\left(B_l+\sigma\sqrt{2\log(8T/\delta)}\right)  \left(\log\left(\frac{4T}{\delta}\right)+d\log\left(1+2T\sqrt{d}\right)\right).  
\end{align}

Similar to \eqref{eq:sum_X_bound}, conditioned on event $\mathcal{E}$, we could upper bound $-\sum_{s=1}^{t-1} Y_s^\omega$ as follows with probability at least $1-\frac{\delta}{4}$:
\begin{align}\label{eq:sum_Y_bound_proof}
    \forall t\in[T],\omega\in\Omega:\quad -\sum_{s=1}^{t-1} Y_s^\omega &\leq - \frac{1}{2} \sum_{s=1}^{t-1}\E_{i\sim\mu_s,j\sim\widetilde{\nu}_s}\left[\left(A_\omega(i,j)-A(i,j)\right)^2\right]\notag\\
    &\quad+ 4C B_l\left(B_l+\sigma\sqrt{2\log(8T/\delta)}\right)  \left(\log\left(\frac{4T}{\delta}\right)+d\log\left(1+2T\sqrt{d}\right)\right) .  
\end{align}

Applying union bound completes the proof of Lemma~\ref{lm:regret_on_squared_loss}.



\subsubsection{Proof of Lemma~\ref{lm:diff_f}}

For any $\mu,\nu\in\Delta^m\times\Delta^n$ and any $\omega\in\Omega$, notice that
\begin{align}\label{eq:innerprod}
    f^{\mu,\nu}(A_\omega) - f^{\mu,\nu}(A) = \langle\underbrace{\E_{i\sim\mu,j\sim\nu}[\phi(i,j)]}_{=: x(\mu,\nu)},\omega-\omega^\star\rangle,
\end{align}
where we denote $\E_{i\sim\mu,j\sim\nu}[\phi(i,j)]$ as $x(\mu,\nu)$ for simplicity. By Assumption~\ref{asmp:bounded_payoff}, it guarantees that $\norm{x(\mu,\nu)}_\infty\leq 1$ for all $\mu,\nu$.
For each $t\in[T]$, we define $\Lambda_t\in\R^{d\times d}$ as
\begin{align}\label{eq:Lambda}
    \Lambda_t\coloneqq \lambda I_d + \sum_{s=1}^{t-1} x(\widehat{\mu}_s,\widehat{\nu}_s)x(\widehat{\mu}_s,\widehat{\nu}_s)^\top
\end{align}
for any $\lambda>0$. By Lemma~\ref{lm:potential} and Lemma~\ref{lm:information_gain}, we have
\begin{align}\label{eq:|x|_bound}
    \sum_{s=1}^t \min\left\{ \norm{x(\widehat{\mu}_s,\widehat{\nu}_s)}_{\Lambda_{s}^{-1}},1\right\}\leq 2d\log\left(1+\frac{T}{d\lambda}\right)\coloneqq d(\lambda) ,
\end{align}
which will be used repeatedly in the proof.

We decompose $\sum_{t=1}^T \left| f^{\widehat{\mu}_t,\widehat{\nu}_t}(A_{\widehat{\omega}_t}) - f^{\widehat{\mu}_t,\widehat{\nu}_t}(A)\right|$ into two terms:
\begin{align}\label{eq:dec}
    \sum_{t=1}^T \left| f^{\widehat{\mu}_t,\widehat{\nu}_t}(A_{\widehat{\omega}_t}) - f^{\widehat{\mu}_t,\widehat{\nu}_t}(A)\right|&= \underbrace{\sum_{t=1}^T \left| \langle x(\widehat{\mu}_t,\widehat{\nu}_t),\widehat{\omega}_t-\omega^\star\rangle\right|\mathbbm{1}\left\{\norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}}\leq 1\right\}}_{(a)}\notag\\
    & \quad + \underbrace{\sum_{t=1}^T \left| \langle x(\widehat{\mu}_t,\widehat{\nu}_t),\widehat{\omega}_t-\omega^\star\rangle\right|\mathbbm{1}\left\{\norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}}> 1\right\}}_{(b)}.
\end{align}
To prove Lemma~\ref{lm:diff_f}, below we bound (a) and (b) separately.

\paragraph{Step 1: bounding term (a).} To bound term (a), it follows that
\begin{align}\label{eq:bound_i_1}
    (a) &= \sum_{t=1}^T \left| \langle x(\widehat{\mu}_t,\widehat{\nu}_t),\widehat{\omega}_t-\omega^\star\rangle\right|\mathbbm{1}\left\{\norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}}\leq 1\right\}\notag\\
    &\leq \sum_{t=1}^T \norm{\widehat{\omega}_t-\omega^\star}_{\Lambda_t}\norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_t^{-1}}\mathbbm{1}\left\{\norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}}\leq 1\right\}\notag\\
    &\leq \sum_{t=1}^T \norm{\widehat{\omega}_t-\omega^\star}_{\Lambda_t}\min\left\{ \norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}},1\right\},
\end{align}
where the first inequality uses the Cauchy-Schwarz inequality, and the second inequality uses the fact that
$$\norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_t^{-1}}\mathbbm{1}\left\{\norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}}\leq 1\right\}\leq \min\left\{ \norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}},1\right\}.$$



Also, by Assumption~\ref{asmp:bounded_payoff} and the definition of $\Lambda_t$ in \eqref{eq:Lambda}, we have
\begin{align}\label{eq:|w|_bound}
    \norm{\widehat{\omega}_t-\omega^\star}_{\Lambda_t}\leq 2\sqrt{\lambda d}+\left(\sum_{s=1}^{t-1} |\langle \widehat{\omega}_t-\omega^\star,x(\widehat{\mu}_s,\widehat{\nu}_s)\rangle|^2\right)^{1/2},
\end{align}
which gives
\begin{align}\label{eq:bound_i_2}
    & \sum_{t=1}^T \norm{\widehat{\omega}_t-\omega^\star}_{\Lambda_t}\min\left\{ \norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}},1\right\}\notag\\
    &\leq \sum_{t=1}^T\left(2\sqrt{\lambda d} +\left(\sum_{s=1}^{t-1} |\langle \widehat{\omega}_t-\omega^\star,x(\widehat{\mu}_s,\widehat{\nu}_s)\rangle|^2\right)^{1/2}\right)\cdot\min\left\{ \norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}},1\right\}\notag\\
    &\leq \left(\sum_{t=1}^T 4\lambda d\right)^{1/2}
    \left(\sum_{t=1}^T \min\left\{ \norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}},1\right\}\right)^{1/2}\notag\\
    &\quad + \left(\sum_{t=1}^T \sum_{s=1}^{t-1} |\langle \widehat{\omega}_t-\omega^\star,x(\widehat{\mu}_s,\widehat{\nu}_s)\rangle|^2\right)^{1/2}\left(\sum_{t=1}^T \min\left\{ \norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}},1\right\}\right)^{1/2}\notag\\
    &\leq 2\sqrt{\lambda dT \min\{d(\lambda),T\}} + \left(d(\lambda)\sum_{t=1}^T \sum_{s=1}^{t-1} |\langle \widehat{\omega}_t-\omega^\star,x(\widehat{\mu}_s,\widehat{\nu}_s)\rangle|^2\right)^{1/2},
\end{align}
where the first inequality uses \eqref{eq:|w|_bound} and the second inequality uses the Cauchy-Schwarz inequality and the fact that 
$$\min\left\{ \norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}},1\right\}^2\leq \min\left\{ \norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}},1\right\},$$
and the last inequality uses \eqref{eq:|x|_bound}.

Plugging \eqref{eq:bound_i_2} into \eqref{eq:bound_i_1}, we have
\begin{align}\label{eq:bound_i_lm}
    (a)\leq 2\sqrt{d}\cdot\sqrt{\lambda T \min\{d(\lambda),T\}} + \left(d(\lambda)\sum_{t=1}^T \sum_{s=1}^{t-1} |\langle \widehat{\omega}_t-\omega^\star,x(\widehat{\mu}_s,\widehat{\nu}_s)\rangle|^2\right)^{1/2}.
\end{align}



\paragraph{Step 2: bounding term (b).} It follows that
\begin{align}\label{eq:bound_ii_lm}
    (b) &= \sum_{t=1}^T \left| \langle x(\widehat{\mu}_t,\widehat{\nu}_t),\widehat{\omega}_t-\omega^\star\rangle\right|\mathbbm{1}\left\{\norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}}> 1\right\} \notag\\
    &\leq \sum_{t=1}^T \left| \langle x(\widehat{\mu}_t,\widehat{\nu}_t),\widehat{\omega}_t-\omega^\star\rangle\right|\min\left\{ \norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}},1\right\}  \leq 2B_l \min\{T, d(\lambda)\},
\end{align}
where the first inequality uses the fact that
$$\mathbbm{1}\left\{\norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}}> 1\right\}\leq \min\left\{ \norm{x(\widehat{\mu}_t,\widehat{\nu}_t)}_{\Lambda_{t}^{-1}},1\right\},$$
and the last inequality uses Assumption~\ref{asmp:bounded_payoff} and \eqref{eq:|x|_bound}.

\paragraph{Step 3: combining (a) and (b).} Plugging \eqref{eq:bound_i_lm} and \eqref{eq:bound_ii_lm} into \eqref{eq:dec}, we have
\begin{align}
    & \sum_{t=1}^T \left| f^{\widehat{\mu}_t,\widehat{\nu}_t}(A_{\widehat{\omega}_t}) - f^{\widehat{\mu}_t,\widehat{\nu}_t}(A)\right|\notag\\
    &\leq 2\sqrt{d}\cdot\sqrt{\lambda T \min\{d(\lambda),T\}} + \left(d(\lambda)\sum_{t=1}^T \sum_{s=1}^{t-1} |\langle \widehat{\omega}_t-\omega^\star,x(\widehat{\mu}_s,\widehat{\nu}_s)\rangle|^2\right)^{1/2}+2B_l \min\{T, d(\lambda)\}\notag\\
    &\leq \left(\frac{d(\lambda)}{\eta}\cdot\eta\sum_{t=1}^T \sum_{s=1}^{t-1}  |\langle \widehat{\omega}_t-\omega^\star,x(\widehat{\mu}_s,\widehat{\nu}_s)\rangle|^2\right)^{1/2} + (\sqrt{d}+2B_l) \min\{d(\lambda),T\} +\sqrt{d} \lambda T\notag\\
    &\leq \frac{d(\lambda)}{2\eta} +\frac{\eta}{2}\sum_{t=1}^T \sum_{s=1}^{t-1} |\langle \widehat{\omega}_t-\omega^\star,x(\widehat{\mu}_s,\widehat{\nu}_s)\rangle|^2 + (\sqrt{d}+2B_l) \min\{d(\lambda),T\} +\sqrt{d} \lambda T 
\end{align}
for any $\eta>0$, where the second and third inequalities both use the fact that $\sqrt{ab}\leq \frac{a+b}{2}$ for any $a,b\geq 0$. The proof is completed by plugging in the following fact into the above relation: for any $\mu,\nu\in\Delta^m\times\Delta^n$ and any $\omega\in\Omega$, we have
\begin{align}\label{eq:square_bound}
    |\langle x(\mu,\nu),\omega-\omega^\star\rangle|^2= |\E_{i\sim\mu,j\sim\nu} [A_\omega(i,j)-A(i,j)]|^2\leq \E_{i\sim\mu,j\sim\nu} [(A_\omega(i,j)-A(i,j))^2] .
\end{align} 

