 
\section{Conclusion}
\label{sec:conclusion}
In this paper, we introduced \name, a provably-efficient model-based algorithm for online MARL that balances  exploration and exploitation without requiring explicit uncertainty quantification. The key innovation lies in incentivizing the model estimation to maximize the best-response value functions across all players to implicitly drive exploration. In addition, \name is readily compatible with modern deep reinforcement learning architectures using function approximation, and is demonstrated to achieve a near-optimal regret under linear function approximation of the model class. We believe this work takes an important step toward making MARL more practical and scalable for real-world applications.

Several promising directions remain for future work. For example, designing a model-free counterpart of \name that can be used in conjunction with function approximation could be a valuable extension. Additionally, it will be interesting to develop the performance guarantee of \name under alternative assumptions of function approximation, such as general function approximation and independent function approximation across the players to tame the curse of dimensionality and multi-agency. Last but not least, it will be of interest to study the performance of \name under adversarial environments.
 
