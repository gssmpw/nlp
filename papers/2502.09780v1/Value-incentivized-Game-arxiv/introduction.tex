
\section{Introduction}

Multi-agent reinforcement learning (MARL) is emerging as a crucial paradigm for solving complex decision-making problems in various domains, including robotics, game theory, and machine learning~\citep{busoniu2008comprehensive}. While single-agent reinforcement learning (RL) has been extensively studied and theoretically analyzed, MARL is still in its infancy, and many fundamental questions remain unanswered. Due to the interplay of multiple agents in an unknown environment, one of the key challenges is the design of efficient strategies for exploration that can be seamlessly implemented in the presence of a large number of agents\footnote{In this paper, we use the term agent and player interchangeably.} without the need of complicated coordination among the agents. In addition, due to the large dimensionality of the state and action spaces, which grows exponentially with respect to the number of agents in MARL, it necessitate the adoption of function approximation to enable tractable planning in modern RL regimes.

A de facto approach in exploration in RL is the principle of optimism in the face of uncertainty \citep{lai1987adaptive}, which argues the importance of quantifying the uncertainty, known as the {\em bonus} term, in the pertinent objects, e.g., the value functions, and using their upper confidence bound (UCB) to guide action selection. This principle has been embraced in the MARL literature, leading a flurry of algorithmic developments \citep{liu2021sharp,bai2021sample,song2021can,jin2021v,li2022minimax,ni2022representation,cui2023breaking,wang2023breaking,dai2024refined} that claim provable efficiency in solving Markov games \citep{littman1994markov}, a standard model for MARL. However, a major downside of this approach is that constructing the uncertainty sets quickly becomes intractable as the complexity of function approximation increases, which often requiring a tailored approach. For example, near-optimal techniques for constructing the bonus function in the tabular setting cannot be applied for general function approximation using neural networks.

 
    
Therefore, it is of great interest to explore alternative exploration strategies without resorting to explicit uncertainty quantification, and can be adopted even for general function approximation. Our work is inspired by the pioneering work of \citet{kumar1982new}, which identified the need to regularize the maximum-likelihood estimator of the model parameters using its optimal value function to incentivize exploration, and has been successfully applied to bandits and  single-agent RL problems ~\citep{liu2020exploration,hung2021reward,mete2021reward,liu2024maximize} with matching performance of their UCB counterparts. However, this strategy of {\em value-incentivized} exploration has not yet been fully realized in the Markov game setting; a recent attempt \citep{liu2024maximize} addressed    two-player zero-sum Markov games, however, it requires  asymmetric updates and solving bilevel optimization problems with the lower level problem being a Markov game itself. 
These limitations motivate the development of more efficient algorithms for the general multi-agent setting while enabling symmetric and independent updates of the players. 
 We address the following question: 
\begin{center}
    \emph{Can we develop provably efficient algorithms for online multi-player general-sum Markov games with function approximation using value-incentivized exploration?}
\end{center} 


 

\subsection{Contribution}
 

In this paper, we propose a provably-efficient model-based framework, named \name (\emph{\textbf{V}alue-incentivized \textbf{M}arkov \textbf{G}ame solver}), for solving online multi-player general-sum Markov games with function approximation. 
\name  
incentivizes exploration via biasing the empirical estimate of the model parameters towards those with a higher collective {\em best-response} values of all the players when fixing the other players' policies, thus encouraging the policy to deviate from the equilibrium of the current model estimate for more exploration. This approach is oblivious to different forms of function approximation, bypassing the need of designing tailored bonus functions to quantity the uncertainty in standard approaches. \name also permits simultaneous and uncoupled policy updates of all players, making it more suitable when the number of players scales. Theoretically, we also establish that \name achieves a near-optimal regret for a number of game-theoretic settings under linear function approximation, which are on par to their counterparts requiring explicit uncertainty quantification. Specifically, our main results are as follows.
\begin{itemize} 
    \item For two-player zero-sum matrix games, \name  achieves a near-optimal regret on the order of $\widetilde{O}(d\sqrt{T})$,\footnote{The notation $\widetilde{O}(\cdot)$ hides logarithmic factors in the standard order-wise notation.} where $d$ is the dimension of the feature space and $T$ is the number of iterations for model updates.  This translates to a sample complexity of $\widetilde{O}(d^2/\varepsilon^2)$ for finding an $\varepsilon$-optimal NE in terms of the duality gap.
    \item For finite-horizon multi-player general-sum Markov games, under the linear mixture model of the transition kernel, \name achieves a near-optimal regret on the order of $\widetilde{\gO}(d\sqrt{H^3T})$, where $H$ is the horizon length, and $T$ is the number of iterations for model updates. This translates to a near-optimal  --- up to a factor of $H$ ---  complexity of $\widetilde{O}(Nd^2H^4/\varepsilon^2)$ samples or $\widetilde{O}(Nd^2H^3/\varepsilon^2)$ trajectories for finding an $\varepsilon$-optimal CCE in terms of the optimality gap, which is also applicable to  finding $\varepsilon$-optimal NE for two-player zero-sum Markov games. We also extend \name to the infinite-horizon setting, which achieves a sample complexity of $\widetilde{\gO}(Nd^2/ ((1-\gamma)^4\varepsilon^2 ))$ to achieve $\varepsilon$-optimality.
      
     
     \item The unified framework of \name allows its reduction to important special cases such as symmetric matrix games, linear bandits and single-agent RL, which not only recovers the existing reward-biased MLE framework  but also discovers new formulation that might be of independent interest. 

\end{itemize}

 
\subsection{Related work}
\label{sec:related}


We  discuss a few threads of related work, focusing on those with theoretical guarantees. 
 

\paragraph{Two-player matrix games.} Finding the equilibrium of two-player zero-sum matrix games has been studied extensively in the literature, e.g., \citet{mertikopoulos2018cycles,shapley1953stochastic,daskalakis2018last,wei2020linear}, where faster last-iterate linear convergence is achieved in the presence of KL regularization \citep{cen2021fast,zhan2023policy}. Many of the proposed algorithms focus on the tabular setting with full information, where the expected returns in each iteration can be computed exactly when the payoff matrix is given. More pertinent to our work, \citet{o2021matrix} considered matrix games with bandit feedback under the tabular setting, where only a noisy payoff from the players' actions is observed at each round, and proposed to estimate the payoff matrix using the upper confidence bounds (UCB) in an entry-wise manner~\citep{lai1987adaptive,bouneffouf2016finite}, as well as K-learning~\citep{o2021variational} that is akin to Thompson sampling \citep{russo2018tutorial}. Our work goes beyond the tabular setting, and proposes an alternative to UCB-based exploration that work seamlessly with different forms of function approximation.

 

\paragraph{Multi-player general-sum Markov games.} General-sum Markov games are an important class of multi-agent RL (MARL) problems \citep{littman1994markov}, and a line of recent works~\citep{liu2021sharp,bai2021sample,mao2023provably,song2021can,jin2021v,li2022minimax,sessa2022efficient} studied the non-asymptotic sample complexity for learning various equilibria in general-sum Markov games for the tabular setting under different data generation mechanisms. These works again rely heavily on carefully constructing confidence bounds of  the value estimates to guide data collection and obtain tight sample complexity bounds. In addition, policy optimization algorithms have also been developed assuming full information of the underlying Markov games, e.g., \citet{erez2023regret,zhang2022policy,cen2023faster}. 



\paragraph{MARL with linear function approximation.} Modern MARL problems often involve large state and action spaces, and thus require function approximation to generalize from limited data. Most theoretical results focus on linear function approximation, where the transition kernel, reward or value functions are assumed to be linear functions of some known feature maps. The linear mixture model of the transition kernel considered herein follows a line of existing works in both single-agent and multi-agent settings, e.g., \citet{ayoub2020model,chen2022almost,modi2020sample,jia2020model,chen2022almost,liu2024maximize}, which is subtly different from another popular linear model \citep{jin2020provably,wang2019optimism,yang2019sample,xie2020learning}, and these two models are not mutually exclusive in general~\citep{chen2022almost}. Moreover, \citet{ni2022representation,huang2022towards} considered general function approximation and \citet{cui2023breaking,wang2023breaking,dai2024refined} considered independent function approximation to allow more expressive function classes that lead to stronger statistical guarantees, which usually require solving complicated constrained optimization problems to construct the bonus functions. 

\paragraph{Uncertainty estimation in online RL.} Uncertainty estimation is crucial for efficient exploration in online RL. Common approaches are constructing the confidence set of the model parameters based on the observed data, which have been demonstrated to be provably near-optimal in the tabular and linear function approximation settings~\citep{jin2018q,agarwal2023vo} but have  limited success in the presence of function approximation in practice~\citep{gawlikowski2023survey}. Thompson sampling provides an alternative approach to exploration by maintaining a posterior distribution over model parameters and sampling from this distribution to make decisions, which however becomes generally intractable under complex function approximation schemes~\citep{russo2018tutorial}. Our approach draws inspiration from the reward-biased maximum likelihood estimation framework, originally proposed by \citet{kumar1982new}, which has been recently adopted in the context of bandits \citep{liu2020exploration,hung2021reward,cen2024value} and single-agent RL \citep{mete2021reward,liu2024maximize}. However, to the best of our knowledge, this work is the first to generalize this idea to the multi-player game-theoretic setting, which not only recovers but leads to new formulations for the single-agent setting.

  

\subsection{Paper organization and notation}

The rest of this paper is organized as follows. Section~\ref{sec:matrix_game} studies two-player zero-sum matrix games,  Section~\ref{sec:markov} focuses on episodic multi-player general-sum Markov games, and we conclude in Section~\ref{sec:conclusion}. The proofs as well as the extension to the infinite-horizon setting are deferred to the appendix.

\paragraph{Notation.}
We let $[n]$ denote the index set $\{1, \dots, n\}$. Let $I_n$ denote the $n\times n$ identity matrix, and inner product in Euclidean space \(\mathbb{R}^n\) by $\langle\cdot,\cdot\rangle$. We let $\Delta^n$ denote the $n$-dimensional simplex, i.e., $\Delta^n=\{x\in\mathbb{R}^n: x\geq 0,\sum_{i=1}^n x_i=1\}$. For any $x\in\mathbb{R}^n$, we let $\|x\|_p$ denote the $\ell_p$ norm of $x$, $\forall p\in[1,\infty]$. We let $\mathbb{B}^d_2(R)$ denote the $d$-dimensional $\ell_2$ ball of radius $R$. The Kullback-Leibler (KL) divergence between two distributions $P$ and $Q$ is denoted as $\KL{P}{Q}\coloneqq \sum_{x}P(x)\log\frac{P(x)}{Q(x)}$.  