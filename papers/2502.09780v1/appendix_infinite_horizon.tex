 
\section{Extension to the Infinite-horizon Setting}\label{app:infinite}

In this section, we consider the $N$-player general-sum episodic Markov game with infinite horizon denoted as $\gM_\PP\coloneqq(\gS,\gA,\PP, r,\gamma)$ as a generalization of the finite-horizon case in the main paper, where $\gamma\in[0,1)$ is the discounted factor, and $\PP:\gS\times\gA\rightarrow\Delta(\gS)$ is the homogeneous transition kernel: the probability of transitioning from state $s$ to state $s'$ by the action $\va=(a^1,\cdots,a^n)$ is $\PP(s'|s,\va)$. For the infinite horizon case, the KL-regularized value function is defined as
\begin{align}\label{eq:V_beta_infinite}
    \forall s\in\gS:\quad V_{n}^{\vpi}(s)&\coloneqq \E_{\PP,\vpi}\left[\sum_{h=0}^\infty \gamma^h\left(r^n(s_h,\va_h)-\beta\log\frac{\pi^n(a_h|s_h)}{\piref^n(a_h|s_h)}\right)\bigg|s_0=s\right]\notag\\
    &=\frac{1}{1-\gamma}\E_{(\bar s,\bar\va)\sim d^\vpi(s)}\left[r^n(\bar s,\bar \va)-\beta\log\frac{\pi^n(\bar a^n|\bar s)}{\piref^n(\bar a^n|\bar s)}\right],
\end{align}
where $s_h$ and $\va_h$ are the state and action at timestep $h$, respectively, $d^\vpi(s)\in\Delta(\gS\times\gA)$ is the \textit{discounted state-action visitation distribution} under policy $\vpi$ starting from state $s$:
\begin{align}\label{eq:state_action_visitation_infinite}
    d^\vpi_{\bar s, \bar \va}(s)\coloneqq (1-\gamma)\sum_{h=0}^\infty \gamma^h \PP^{\vpi}(s_h = \bar s, \va_h =  \bar \va|s_0=s).
\end{align}
We assume $\rho\in\Delta(\gS)$ is the initial state distribution, i.e. $s_0\sim\rho$.
We define $d^\vpi(\rho)\coloneqq \E_{s_0\sim\rho}[d^\vpi(s_0)]$ as the discounted state-action visitation distribution under policy $\vpi$ starting from the initial state distribution $\rho$.
The KL-regularized Q-function is defined as
\begin{align}\label{eq:Q_beta_infinite}
    \forall (s,\va)\in\gS\times\gA:&\quad Q_{n}^{\vpi}(s,\va)\coloneqq r^n(s,\va) +  \gamma\E_{s'\sim\PP(\cdot|s,\va)}\left[V_{n}^{\vpi}(s')\right].
\end{align}

We let $\gF$ denote the function class of the estimators of the transition kernel of the Markov game, and we denote $f=\PP_f\in\gF$.
Without otherwise specified, we assume the other notations and settings are the same as the finite-horizon case stated in Section~\ref{sec:markov}.



 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Algorithm development}\label{sec:markov_alg_infinite}

The algorithm for solving the (KL-regularized) Markov game is shown in Algorithm~\ref{alg:markov_game_infinite}, where in \eqref{eq:model_update_mg_infinite} we set the loss function at each iteration $t$ as the negative log-likelihood of the transition kernel estimator $f$:
\begin{align}\label{eq:loss_mg_infinite}
    \gL_t(f)\coloneqq \sum_{(s,\va,s')\in\gD_{t-1}}-\log \PP_f(s'|s,\va).
\end{align}
Except for the loss function, the main change in Algorithm~\ref{alg:markov_game_infinite} is that we need to sample the state-action pair $(s,\va)$ from the discounted state-action visitation distribution $d^\vpi(\rho)$, and sample the next state $s'$ from the transition kernel $\PP(\cdot|s,\va)$, which can be done by calling Algorithm~\ref{alg:sample}. Algorithm~\ref{alg:sample} is adapted from Algorithm~3 in \citet{yuan2023linear}, see also Algorithm~5 in \citet{yang2023federated}. Algorithm~\ref{alg:sample} satisfies $\E[h+1]=\frac{1}{1-\gamma}$, and $\PP(s_h=s,\va_h=\va)=d^\vpi(\rho)$~\citep{yuan2023linear}.

\begin{algorithm}[!thb]
    \caption{Value-incentive Infinite-horizon Markov Game}
    \label{alg:markov_game_infinite}
    \begin{algorithmic}[1]
    \STATE \textbf{Input:} reference policies $\allpiref$, KL coefficient $\beta$, initial state distribution $\rho$, initial transition kernel estimator $f_0\in\gF$, regularization coefficient $\alpha>0$, iteration number $T$.
    \STATE \textbf{Initialization:} dataset $\gD_0^n\coloneqq \emptyset$, $\forall n\in[N]$. $\gD_0 = \cup_{n=1}^N \gD_0^n$.
    \FOR{$t = 1,\cdots,T$}
    \STATE $\vpi_t \leftarrow \gamesolver(\gM_{f_{t-1}})$. \trianglecomment{$\gamesolver(\gM_f)$ returns a CCE or NE of game $\gM_f$.}
    \STATE Model update: Update the estimator $f_t$ by minimizing the following objective:
    \begin{align}\label{eq:model_update_mg_infinite}         
        f_t=\argmin_{f\in\gF}\sum_{(s,\va,s')\in\gD_{t-1}}-\log \PP_f(s'|s,\va) -\alpha \sum_{n=1}^N V_{f,n}^{\star,\vpi_t^{-n}}(\rho).
    \end{align}
    \STATE Compute best-response policies $\{\widetilde{\pi}_t^n\}_{n\in[N]}$:
    \begin{align}\label{eq:policy_update_2_mg_infinite}
        \text{For all }n\in[N]:\quad \widetilde{\pi}_t^n& = \argmax_{\pi^n\in\Delta(\gA_n)} V_{f_t,n}^{\pi^n,\vpi_t^{-n}}(\rho).
    \end{align}
    \STATE Data collection: sample $(s_t,\va_t,s_t')\leftarrow \sample(\vpi_t,\rho)$.
    For all $n\in[N]$, sample 
    $(s_{t}^n,\va_{t}^n,{s_{t}^n}')\leftarrow \sample((\widetilde{\pi}_t^n,\vpi_t^{-n}),\rho)$, and update the dataset $\gD_t^n = \gD_{t-1}^n\cup\{(s_t,\va_t,s_t'),(s_t^n,\va_{t}^n,{s_t^n}')\}$. $\gD_t = \cup_{n=1}^N \gD_t^n$.
    \trianglecomment{$\sample(\vpi,\rho)$ returns $(s,\va)\sim d^\vpi(\rho)$ and $s'\sim \PP(\cdot|s,\va)$, see Algorithm~\ref{alg:sample}.}
    \ENDFOR 
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!thb]
    \caption{$\sample$ for $(s,\va)\sim d^\vpi(\rho)$ and $s'\sim \PP(\cdot|s,\va)$}
    \label{alg:sample}
    \begin{algorithmic}[1]
    \STATE \textbf{Input:} policy $\vpi$, initial state distribution $\rho$, player index $n$.
    \STATE \textbf{Initialization:} $s_0\sim \rho$, $\va_0\sim \vpi(\cdot|s_0)$, time step $h=0$, variable $X\sim$ Bernoulli($\gamma$).
    \WHILE{$X = 1$}
        \STATE Sample $s_{h+1}\sim \PP(\cdot|s_h,\va_h)$
        \STATE Sample $\va_{h+1}\sim \vpi(\cdot|s_{h+1})$
        \STATE $h\leftarrow h+1$
        \STATE $X\sim$ Bernoulli($\gamma$)
    \ENDWHILE
    \STATE Sample $s_{h+1}\sim \PP(\cdot|s_h,\va_h)$
    \RETURN $(s_h,\va_h, s_{h+1})$.
    \end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Theoretical guarantee}\label{sec:markov_analysis_infinite}

We first state our assumptions on the function class for Markov game with infinite horizon.

\begin{asmp}[linear mixture model, infinite horizon]\label{asmp:function_class_infinite}
    The function class $\gF$ is
    $$\gF\coloneqq\left\{f|f(s,\va,s')=\phi(s,\va,s')^\top\theta,\forall (s,\va,s')\in\gS\times\gA\times\gS,\theta\in\Theta\right\},$$
    where $\phi:\gS\times\gA\times\gS\rightarrow\R^d$ is the known feature map, $\norm{\phi(s,\va,s')}_2\leq 1$ for all $(s,\va,s')$, and $\Theta\subseteq\mathbb{B}^d_2(\sqrt{d})$. Moreover,  for each $f\in\gF$ and $(s,\va)\in\gS\times\gA$, $f(\cdot|s,\va)\in\Delta(\gS)$.
\end{asmp}
We also assume the function class $\gF$ is expressive enough to describe the true transition kernel of the Markov game.
\begin{asmp}[realizability]\label{asmp:realizable_infinite}
    There exists $f^\star\in\gF$ such that $\PP_{f^\star}=\PP$.
\end{asmp}

Theorem~\ref{thm:regret_MG_infinite} states our main result for the regret of the infinite-horizon online Markov game, whoes proof is deferred to Appendix~\ref{app:proof_thm_regret_MP_infinite}.

\begin{thm}\label{thm:regret_MG_infinite}
    Under Assumption~\ref{asmp:function_class_infinite} and Assumption~\ref{asmp:realizable_infinite}, if setting the regularization coefficient $\alpha$ as
    $$\alpha = \frac{(1-\gamma)^{3/2}}{\gamma}\sqrt{\frac{\log\left(\frac{N}{\delta}\right) + d\log\left(d|\gS|T\right)}{d\log\left(1+\frac{T^{3/2}}{(1-\gamma)^2\sqrt{d}}\right)}T},$$
    then for any $\beta\geq 0$, with any initial state distribution $\rho$, transition kernel estimator $f_0\in\gF$ and reference policy $\allpiref$, the regret of Algorithm~\ref{alg:markov_game} satisfies the following bound with probability at least $1-\delta$ for any $\delta\in(0,1)$:
    \begin{align}\label{eq:regret_MG_bd_infinite}
        \forall T\in\NN_+:\quad\regret(T)\leq \widetilde{\gO}\left( \frac{\gamma d\sqrt{T}}{(1-\gamma)^{3/2}}\cdot \sqrt{\frac{1}{d}\log\left(\frac{N}{\delta}\right) + \log\left(d|\gS|T\right)}\right).
    \end{align}
\end{thm}

Note that
\begin{align}
    \min_{t\in[T]}\gap{\vpi_t}\leq \frac{\regret(T)}{T},
\end{align}
similar to earlier arguments, Theorem~\ref{thm:regret_MG_infinite} also implies an order of $\widetilde{\gO}\left(\frac{\gamma^2 N d^2}{(1-\gamma)^4 \varepsilon^2}\right)$ sample complexity for Algorithm~\ref{alg:markov_game} to find an $\varepsilon$-NE or $\varepsilon$-CCE of $\gM_\PP$.
 
