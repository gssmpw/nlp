\subsection{Proof of Theorem~\ref{thm:regret_MG}}\label{app:proof_thm_regret_MP}

For notation simplicity, we define
    \begin{align}\label{eq:pi_t_n}
        \widetilde{\vpi}_{t,n}\coloneqq (\widetilde{\pi}_t^n, \vpi_t^{-n}),\quad \forall n\in[N].
    \end{align}

Analogous to \eqref{eq:regret_dec1}, here we decompose the regret as
\begin{align}\label{eq:regret_dec1_mg}
    \regret(T) &= \sum_{t=1}^T\frac{1}{N}\sum_{n=1}^N \left(V_{n}^{\star,\vpi_t^{-n}}(\rho) - V_{n}^{\vpi_t}(\rho)\right),\notag\\
    &= \sum_{t=1}^T \frac{1}{N}\sum_{n=1}^N \left(V_{n}^{\star,\vpi_t^{-n}}(\rho) - V_{f_t,n}^{\star,\vpi_t^{-n}}(\rho)\right) + \sum_{t=1}^T \frac{1}{N}\sum_{n=1}^N \left(V_{f_t,n}^{\star,\vpi_t^{-n}}(\rho) - V_{n}^{\widetilde{\pi}_t^n,\vpi_t^{-n}}(\rho)\right)\notag\\
    &\quad + \sum_{t=1}^T \frac{1}{N}\sum_{n=1}^N \left(V_{n}^{\widetilde{\pi}_t^n,\vpi_t^{-n}}(\rho) - V_{f_{t-1},n}^{\widetilde{\pi}_t^n,\vpi_t^{-n}}(\rho)\right)
    + \sum_{t=1}^T \frac{1}{N}\sum_{n=1}^N \left(V_{f_{t-1},n}^{\widetilde{\pi}_t^n,\vpi_t^{-n}}(\rho) - V_{f_{t-1},n}^{\vpi_t}(\rho)\right)\notag\\
    &\quad + \sum_{t=1}^T \frac{1}{N}\sum_{n=1}^N \left(V_{f_{t-1},n}^{\vpi_t}(\rho) - V_{n}^{\vpi_t}(\rho)\right).
\end{align}

By line 4 in Algorithm~\ref{alg:markov_game} we know that the second term in the third line of \eqref{eq:regret_dec1_mg} is non-positive. 
Besides, \eqref{eq:policy_update_2_mg} indicates
\begin{align}\label{eq:dagger_equation_mg}
    \forall n\in[N]:\quad V_{f_t,n}^{\star,\vpi_t^{-n}}(\rho) =V_{f_t,n}^{\widetilde{\pi}_t^n,\vpi_t^{-n}}(\rho).
\end{align}
Combining these two facts, we have 
\begin{align}\label{eq:regret_dec2_mg}
    \regret(T) &\leq \underbrace{\sum_{t=1}^T \frac{1}{N}\sum_{n=1}^N \left(V_{n}^{\star,\vpi_t^{-n}}(\rho) - V_{f_t,n}^{\star,\vpi_t^{-n}}(\rho)\right)}_{\text{(i)}} 
    + \underbrace{\sum_{t=1}^T \frac{1}{N}\sum_{n=1}^N \left(V_{f_t,n}^{\widetilde{\pi}_t^n,\vpi_t^{-n}}(\rho) - V_{n}^{\widetilde{\pi}_t^n,\vpi_t^{-n}}(\rho)\right)}_{\text{(ii)}}\notag\\
    &\quad + \underbrace{\sum_{t=1}^T \frac{1}{N}\sum_{n=1}^N \left(V_{n}^{\widetilde{\pi}_t^n,\vpi_t^{-n}}(\rho) - V_{f_{t-1},n}^{\widetilde{\pi}_t^n,\vpi_t^{-n}}(\rho)\right)}_{\text{(iii)}}
    + \underbrace{\sum_{t=1}^T \frac{1}{N}\sum_{n=1}^N \left(V_{f_{t-1},n}^{\vpi_t}(\rho) - V_{n}^{\vpi_t}(\rho)\right)}_{\text{(iv)}}.
\end{align}

In the following we upper bound each term in \eqref{eq:regret_dec2_mg} separately.

\paragraph{Step 1: bounding term (i).} 
By Assumption~\ref{asmp:realizable} we know that there exists $f^\star\in\gF$ such that $f^\star\coloneqq \PP = \PP_{f^\star}$.
By the model update rule~\eqref{eq:model_update_mg} in Algorithm~\ref{alg:markov_game} and the definition of the loss function~\eqref{eq:loss_mg}, we have
\begin{align*}
    \gL_t(f_t)-\alpha\sum_{n=1}^N V_{f_t,n}^{\star,\vpi_t^{-n}}(\rho) \leq \gL_t(f^\star)-\alpha\sum_{n=1}^N V_{n}^{\star,\vpi_t^{-n}}(\rho)
\end{align*}
from which we deduce
\begin{align}\label{eq:loss_diff_mg}
   \mathrm{(i)} \leq \frac{1}{N\alpha}\sum_{t=1}^T\left(\gL_t(f^\star)-\gL_t(f_t)\right).
\end{align}
It then boils down to bounding the right-hand side of \eqref{eq:loss_diff_mg}. 

We first define random variables $X_{t,h}^f$ and $Y_{t,h,n}^f$ as
\begin{align}\label{eq:X_t_mg}
    X_{t,h}^f\coloneqq \log \left(\frac{\PP_h(s_{t,h+1}|s_{t,h},\va_{t,h})}{\PP_{f,h}(s_{t,h+1}|s_{t,h},\va_{t,h})}\right)\quad\text{and}\quad Y_{t,h,n}^f\coloneqq \log \left(\frac{\PP_h(s_{t,h+1}^n|s_{t,h}^n,\va_{t,h}^n)}{\PP_{f,h}(s_{t,h+1}^n|s_{t,h}^n,\va_{t,h}^n)}\right),\quad \forall n\in[N].
\end{align}
By the definition of the loss function~\eqref{eq:loss_mg}, we have
\begin{align}\label{eq:loss_diff_decompose_mg}
    \gL_t(f^\star)-\gL_t(f) = -\sum_{i=1}^{t-1}\sum_{h=1}^H\sum_{n=1}^N \left(X_{i,h}^f+Y_{i,h,n}^f\right).
\end{align}

Let $\hellinger{\cdot}{\cdot}$ denote the Hellinger divergence defined as:
\begin{align}\label{eq:Hellinger}
    \hellinger{P}{Q}\coloneqq \frac{1}{2}\int_\gX\left(\sqrt{P(x)}-\sqrt{Q(x)}\right)^2 dx 
\end{align}
for any probability measures $P$ and $Q$ on $\gX$, and define
\begin{align}\label{eq:Hellinger_loss}
    \ell(f_h,s,\va)\coloneqq \hellinger{\PP_{f,h}(\cdot|s,\va)}{\PP_h(\cdot|s,\va)}.
\end{align}
In the following lemma we provide a concentration result for the random variables $X_{t,h}^f$ and $Y_{t,h,n}^f$ in \eqref{eq:loss_diff_decompose_mg} (recall we define $\widetilde{\vpi}_{t,n}\coloneqq (\widetilde{\pi}_t^n, \vpi_t^{-n})$ in \eqref{eq:pi_t_n}), where  the state-action visitation distribution $d_h^{\vpi}(\rho)\in\Delta(\gS\times\gA)$ at step $h$ under the policy $\vpi$ and the initial state distribution $\rho$ is defined as
\begin{align}\label{eq:state_action_visitation_finite}
    d_{h}^{\vpi}(s,a ; \rho)\coloneqq \E_{s\sim\rho}\PP^{\vpi}(s_h=s,\va_h=\va|s_1=s).
\end{align}

\begin{lm}\label{lm:bound_X_Y_mg}
    When Assumptions~\ref{asmp:function_class} and \ref{asmp:realizable} hold, for any $\delta \in (0,1)$, with probability at least $1-\delta$, we have  for all $t\in[T]$, $ f\in\gF$ and $n\in[N]$:
    \begin{align}\label{eq:sum_X_mg}
        -\sum_{i=1}^{t-1}\sum_{h=1}^H X_{i,h}^f& \leq -2\sum_{i=1}^{t-1}\sum_{h=1}^H \E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\ell(f_h,s_{i,h},\va_{i,h})\right] \notag\\
        & \quad + 2\sqrt{2}H + 2H\log\left(\frac{(N+1)H}{\delta}\right) + 2dH\log\left(1+2\sqrt{d}|\gS|^2 T^2\right) .\\ 
     -\sum_{i=1}^{t-1}\sum_{h=1}^H Y_{i,h,n}^f  & \leq -2\sum_{i=1}^{t-1}\sum_{h=1}^H \E_{(s_{i,h}^n,\va_{i,h}^n)\sim d_h^{\widetilde{\vpi}_{i,n}}(\rho)}\left[\ell(f_h,s_{i,h}^n,\va_{i,h}^n)\right] \nonumber \\
        & \quad + 2\sqrt{2}H + 2H\log\left(\frac{(N+1)H}{\delta}\right) + 2dH\log\left(1+2\sqrt{d}|\gS|^2 T^2\right). \label{eq:sum_Y_mg}
    \end{align}
\end{lm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Combining \eqref{eq:loss_diff_mg}, \eqref{eq:loss_diff_decompose_mg}, \eqref{eq:sum_X_mg}, \eqref{eq:sum_Y_mg}, we have with probability at least $1-\delta$:
\begin{align}\label{eq:a_bound}
    \text{(i)} & \leq -\frac{2}{N\alpha}\sum_{n=1}^N\bigg\{\sum_{t=1}^T\sum_{i=1}^{t-1}\sum_{h=1}^H 
    \E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\ell(f_{t,h},s_{i,h},\va_{i,h})\right]\notag\\
  &  \quad +\sum_{t=1}^T\sum_{i=1}^{t-1}\sum_{h=1}^H 
    \E_{(s_{i,h}^n,\va_{i,h}^n)\sim d_h^{\widetilde{\vpi}_{i,n}}(\rho)}\left[\ell(f_{t,h},s_{i,h}^n,\va_{i,h}^n)\right]\bigg\}\notag\\
   & \quad + \frac{4HT}{\alpha}\left(\sqrt{2} + \log\left(\frac{(N+1)H}{\delta}\right) + d\log\left(1+2\sqrt{d}|\gS|^2T^2\right)\right). 
\end{align}

\paragraph{Step 2: bounding terms (ii), (iii) and (iv).} To bound (ii), (iii) and (iv), we introduce the following lemma.  
\begin{lm}\label{lm:V_diff_bound}
    Under Assumptions~\ref{asmp:function_class} and \ref{asmp:realizable}, for any $n\in[N]$, $\beta\geq 0$, $\{\widehat{\vpi}_{t}: \gS\times[H]\rightarrow\Delta(\gA)\}_{t\in[T]}$ and $\{\widehat{f}_{t}\}_{t\in[T]}\subset \gF$,  we have
    \begin{align}\label{eq:V_diff_bound}
       \sum_{t=1}^T \left|V_{\widehat{f}_{t},n}^{\widehat{\vpi}_{t}}(\rho)-V_{n}^{\widehat{\vpi}_{t}}(\rho)\right|&\leq \frac{\eta}{2}\sum_{t=1}^T \sum_{i=1}^{t-1}\sum_{h=1}^H \E_{(s,\va)\sim d_h^{\widehat{\vpi}_i}(\rho)}\ell(\widehat{f}_{t,h},s,\va) \notag\\
       &\quad+H\left(\frac{4d_H(\lambda)H}{\eta}
       + \left(\sqrt{d}+H\right) \min\{d_H(\lambda),T\} +\sqrt{d} \lambda T\right)
    \end{align}
    for any $\eta>0$ and $\lambda>0$, where $d_H(\lambda)$ is defined as
    $$d_H(\lambda)\coloneqq 2d\log\left(1+\frac{H^2T}{\lambda}\right).$$
\end{lm}


Now we are ready to bound (ii), (iii) and (iv). To bound (ii), letting $\widehat{f}_{t}=f_t$ and
$\widehat{\vpi}_{t}=\widetilde{\vpi}_{t,n}$ for each $n \in [N]$ in Lemma~\ref{lm:V_diff_bound} (recall we define $\widetilde{\vpi}_{t,n}\coloneqq (\widetilde{\pi}_t^n,\vpi_t^{-n})$ in \eqref{eq:pi_t_n}), we have for any $\eta>0$:
\begin{align}\label{eq:b_bound} 
    \text{(ii)}&\leq \frac{\eta}{2N}\sum_{h=1}^H\sum_{n=1}^N\sum_{t=1}^T \sum_{i=1}^{t-1} \E_{(s,\va)\sim d_h^{\widetilde{\vpi}_{i,n}}(\rho)}\ell(f_{t,h},s,\va) \notag\\
    &\quad+H\left(\frac{4d_H(\lambda)H}{\eta}
    + \left(\sqrt{d}+H\right) \min\{d_H(\lambda),T\} +\sqrt{d} \lambda T\right).
\end{align}

Letting $\widehat{f}_{t,h}=f_{t-1}$ and
$\widehat{\vpi}_{t,h}=\widetilde{\vpi}_{t,n}$ for each $n \in [N]$ in Lemma~\ref{lm:V_diff_bound}, we can bound (iii) as follows:
\begin{align}\label{eq:c_bound_temp} 
    \text{(iii)}&\leq \frac{\eta}{2N}\sum_{h=1}^H\sum_{n=1}^N\sum_{t=1}^T \sum_{i=1}^{t-1} \E_{(s,\va)\sim d_h^{\widetilde{\vpi}_{i,n}}(\rho)}\ell(f_{t-1,h},s,\va) \notag\\
    &\quad+H\left(\frac{4d_H(\lambda)H}{\eta}
    + \left(\sqrt{d}+H\right) \min\{d_H(\lambda),T\} +\sqrt{d} \lambda T\right).
\end{align}
To continue to bound the first term, note that
\begin{align}
\sum_{h=1}^H \sum_{t=1}^T \sum_{i=1}^{t-1} \E_{(s,\va)\sim d_h^{\widetilde{\vpi}_{i,n}}(\rho)}\ell(f_{t-1,h},s,\va)   & \leq  \sum_{h=1}^H \sum_{t=1}^T \sum_{i=1}^{t-2} \E_{(s,\va)\sim d_h^{\widetilde{\vpi}_{i,n}}(\rho)}\ell(f_{t-1,h},s,\va) + HT  \nonumber \\
    &= \sum_{h=1}^H \sum_{t=0}^{T-1} \sum_{i=1}^{t-1} \E_{(s,\va)\sim d_h^{\widetilde{\vpi}_{i,n}} 
    (\rho)}\ell(f_{t,h},s,\va) +HT \notag\\
    &\leq  \sum_{h=1}^H \sum_{t=1}^{T} \sum_{i=1}^{t-1} \E_{(s,\va)\sim d_h^{\widetilde{\vpi}_{i,n}}(\rho)}\ell(f_{t,h},s,\va) + HT, \nonumber
\end{align}
where the first inequality uses the fact that
\begin{align}
    \ell(f_h,s,\va)=\hellinger{\PP_{f,h}(\cdot|s,\va)}{\PP_h(\cdot|s,\va)}\leq 1,
\end{align}
the second line shifts the index of $t$ by 1, and the last line follows by noticing the first summand is $0$ at $t=0$. Plugging the above relation back to \eqref{eq:c_bound_temp} leads to
\begin{align}\label{eq:c_bound} 
    \text{(iii)}&\leq \frac{\eta}{2N}\sum_{h=1}^H \sum_{n=1}^N \sum_{t=1}^{T} \sum_{i=1}^{t-1} \E_{(s,\va)\sim d_h^{\widetilde{\vpi}_{i,n}}(\rho)}\ell(f_{t,h},s,\va)  \notag\\
    &\quad+H\left(\frac{4d_H(\lambda)H}{\eta}
    + \left(\sqrt{d}+H\right) \min\{d_H(\lambda),T\} +\sqrt{d} \lambda T + \frac{\eta}{2} T \right).
\end{align}

Finally, similar to \eqref{eq:c_bound}, letting $\widehat{f}_{t,h}=f_{t-1}$, $\widehat{\vpi}_{t,h}=\vpi_t$ for each $n \in [N]$ and replace $\eta$ by $2\eta$ in Lemma~\ref{lm:V_diff_bound}, we can bound (iv) as follows:
\begin{align}\label{eq:d_bound} 
    \text{(iv)} &\leq \frac{\eta}{N}\sum_{h=1}^H\sum_{n=1}^N\sum_{t=1}^{T} \sum_{i=1}^{t-1} \E_{(s,\va)\sim d_h^{\vpi_i}(\rho)}\ell(f_{t,h},s,\va) \notag\\
    &\quad+H\left(\frac{2Hd_H(\lambda)}{\eta}
    + \left(\sqrt{d}+H\right) \min\{d_H(\lambda),T\} +\sqrt{d} \lambda T +\eta T\right).
\end{align}

\paragraph{Step 3: combining the bounds.} Letting $\eta = \frac{2}{\alpha}$ in \eqref{eq:b_bound}, \eqref{eq:c_bound} and \eqref{eq:d_bound}, and adding \eqref{eq:a_bound}, \eqref{eq:b_bound}, \eqref{eq:c_bound} and \eqref{eq:d_bound} together, we have with probability at least $1-\delta$:
\begin{align*}
    \regret(T)&\leq \frac{4HT}{\alpha}\left(\sqrt{2} + \log\left(\frac{(N+1)H}{\delta}\right) + d\log\left(1+2\sqrt{d}|\gS|^2T^2\right)\right)\notag\\
    &\quad + H\left(5\alpha d_H(\lambda)H
    + 3\left(\sqrt{d}+H\right) \min\{d_H(\lambda),T\} +3\sqrt{d} \lambda T +\frac{3}{\alpha}T\right).
\end{align*}

By setting
\begin{align}
    \lambda =\sqrt{\frac{d}{T}},\quad \alpha = \sqrt{\frac{\log\left(\frac{(N+1)H}{\delta}\right) + d\log\left(1+2\sqrt{d}|\gS|^2T^2\right)}{Hd\log\left(1+\frac{H^2T^{3/2}}{\sqrt{d}}\right)}T},
\end{align}
in the above expression, we have with probability at least $1-\delta$:
\begin{align}
    \regret(T)&\leq 4(1+\sqrt{2})\sqrt{\frac{d\log\left(1+\frac{H^2T^{3/2}}{\sqrt{d}}\right)}{\log\left(\frac{(N+1)H}{\delta}\right) + d\log\left(1+2\sqrt{d}|\gS|^2T^2\right)}}\cdot \sqrt{HT}\notag\\
    &\quad 14d\sqrt{H^3T}\cdot\sqrt{\left(\frac{1}{d}\log\left(\frac{(N+1)H}{\delta}\right) + \log\left(1+\sqrt{d}|\gS|^2T^2\right)\right)\log\left(1+\frac{H^2T^{3/2}}{\sqrt{d}}\right)}\notag\\
    &\quad + 6H\left(\sqrt{d}+H\right) d\log\left(1+\frac{H^2T^{3/2}}{\sqrt{d}}\right) +3dH\sqrt{T},
\end{align}
which gives the desired result after simplifying the expression.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Proof of Lemma~\ref{lm:bound_X_Y_mg}}\label{app:proof_bound_X_Y_mg}

Same as in \eqref{eq:covering}, for the parameter spaces $\Theta_h$, $h\in[H]$, by Assumption~\ref{asmp:function_class} and Lemma~\ref{lm:covering} we have
\begin{align}\label{eq:covering_mg}
    \forall h\in[H]:\quad\log \gN(\Theta_h,\epsilon,\norm{\cdot}_2)\leq d\log\left(1+\frac{2\sqrt{d}}{\epsilon}\right)
\end{align}
for any $\epsilon>0$. Thus for any $\epsilon>0$, there exists an $\epsilon$-net $\Theta_{h,\epsilon}$ of $\Theta_h$ ($\Theta_{h,\epsilon}\subset\Theta_h$) such that $\log|\Theta_{h,\epsilon}|\leq d\log\left(1+\frac{2\sqrt{d}}{\epsilon}\right)$, $\forall h\in[H]$.
Define
\begin{align*}
    \gF_{h,\epsilon} \coloneqq \left\{f_h\in\gF_h: f_h(s,\va,s_{h+1})=\phi_h(s,\va,s_{h+1})^\top\theta_h,\theta_h\in\Theta_{h,\epsilon}\right\}.
\end{align*}


For any $f\in\gF$, there exists $\theta_h\in\Theta_h$ such that $f_h(s,\va,s_{h+1})=\phi_h(s,\va,s_{h+1})^\top\theta_h$. In addition, there exists $\theta_{h,\epsilon}\in\Theta_{h,\epsilon}$ such that $\norm{\theta_h-\theta_{h,\epsilon}}_2\leq \epsilon$. We let $f_\epsilon(s,\va,s_{h+1})=\phi_h(s,\va,s_{h+1})^\top\theta_{h,\epsilon}$. Then $f_\epsilon\in\gF_{h,\epsilon}$, and we have
\begin{align}\label{eq:diff_eps}
    |\PP_{f,h}(s_{h+1}|s,\va)-\PP_{f_\epsilon,h}(s_{h+1}|s,\va)|=|\phi_h(s,\va,s_{h+1})^\top(\theta_h-\theta_{h,\epsilon})|\leq \epsilon,
\end{align}
from which we deduce
\begin{align}\label{eq:diff_net_approx}
    \forall t\in[T],h\in[H]:\quad -X_{t,h}^f\leq -\log\left(\frac{\PP_h(s_{t,h+1}|s_{t,h},\va_{t,h})}{\PP_{f_\epsilon,h}(s_{t,h+1}|s_{t,h},\va_{t,h})+\epsilon}\right)\coloneqq -X_{t,h}^{f_\epsilon}(\epsilon).
\end{align}

Let $\gF_t\coloneqq \sigma(\gD_t)$ be the $\sigma$-algebra generated by the dataset $\gD_t$. 
By Lemma~\ref{lm:martingale_exp} we have 
with probability at least $1-\frac{\delta}{N+1}$:
\begin{align}\label{eq:exp_concentration_net}
   \forall t\in[T],h\in[H],f_{h,\epsilon}\in\gF_{h,\epsilon}: \quad -\frac{1}{2}\sum_{i=1}^{t-1} X^{f_\epsilon}_{i,h}(\epsilon) &\leq \sum_{i=1}^{t-1} \log\E\left[\exp\left(-\frac{1}{2}X^{f_\epsilon}_{i,h}(\epsilon)\right)\bigg|\gF_{i-1}\right] \notag\\
   &\quad+ \log\left(\frac{(N+1)H}{\delta}\right) + d\log\left(1+\frac{2\sqrt{d}}{\epsilon}\right).
\end{align}


Then we have for all $t\in[T]$, $h\in[H]$ and $f\in\gF$:
\begin{align}\label{eq:concentration_exp_mg_1}
    -\frac{1}{2}\sum_{i=1}^{t-1}\sum_{h=1}^H X_{i,h}^f& \leq  -\frac{1}{2}\sum_{i=1}^{t-1}\sum_{h=1}^H X^{f_\epsilon}_{i,h}(\epsilon)\notag\\
    & \leq  \sum_{i=1}^t \log\E\left[\exp\left(-\frac{1}{2}X^{f_\epsilon}_{i,h}(\epsilon)\right)\bigg|\gF_{i-1}\right] + H\log\left(\frac{(N+1)H}{\delta}\right) + dH\log\left(1+\frac{2\sqrt{d}}{\epsilon}\right),
\end{align}
where the first line follows \eqref{eq:diff_net_approx}, and the second line follows from \eqref{eq:exp_concentration_net}. The first term in the last line of \eqref{eq:concentration_exp_mg_1} can be further bounded as follows:
\begin{align}\label{eq:concentration_exp_mg_part}
    &\sum_{i=1}^t \log\E\left[\exp\left(-\frac{1}{2}X^{f_\epsilon}_{i,h}(\epsilon)\right)\bigg|\gF_{i-1}\right]\notag\\
    &=\sum_{i=1}^{t-1}\sum_{h=1}^H \log\E\left[\sqrt{\frac{\PP_{f_\epsilon,h}(s_{i,h+1}|s_{i,h},\va_{i,h})+\epsilon}{\PP_h(s_{i,h+1}|s_{i,h},\va_{i,h})}}\bigg|\gF_{s-1}\right]\notag\\
    &=\sum_{i=1}^{t-1}\sum_{h=1}^H \log\E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho),\atop s_{i,h+1}\sim\PP_h(\cdot|s_{i,h},\va_{i,h})}\left[\sqrt{\frac{\PP_{f_\epsilon,h}(s_{i,h+1}|s_{i,h},\va_{i,h})+\epsilon}{\PP_h(s_{i,h+1}|s_{i,h},\va_{i,h})}}\right]\notag\\
    &=\sum_{i=1}^{t-1}\sum_{h=1}^H \log\E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\int_\gS\sqrt{\left(\PP_{f_\epsilon,h}(s_{i,h+1}|s_{i,h},\va_{i,h})+\epsilon\right)\PP_h(s_{i,h+1}|s_{i,h},\va_{i,h})} ds_{i,h+1}\right]\notag\\
    & \leq  \sum_{i=1}^{t-1}\sum_{h=1}^H \log\E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\int_\gS\sqrt{\left(\PP_{f,h}(s_{i,h+1}|s_{i,h},\va_{i,h})+2\epsilon\right)\PP_h(s_{i,h+1}|s_{i,h},\va_{i,h})} ds_{i,h+1}\right],
\end{align}
where the last inequality uses \eqref{eq:diff_eps}. Furthermore, we have
\begin{align}\label{eq:construct_Hellinger}
    &\E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\int_\gS\sqrt{\left(\PP_{f,h}(s_{i,h+1}|s_{i,h},\va_{i,h})+2\epsilon\right)\PP_h(s_{i,h+1}|s_{i,h},\va_{i,h})} ds_{i,h+1}\right]\notag\\
    &\leq \E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\int_\gS\sqrt{\PP_{f,h}(s_{i,h+1}|s_{i,h},\va_{i,h})\PP_h(s_{i,h+1}|s_{i,h},\va_{i,h})} ds_{i,h+1}\right] \notag\\
    &\quad + \E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\int_\gS\sqrt{2\epsilon\PP_h(s_{i,h+1}|s_{i,h},\va_{i,h})} ds_{i,h+1}\right]\notag\\
    &\leq 1-\frac{1}{2}\E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\int_\gS\left(\sqrt{\PP_{f,h}(s_{i,h+1}|s_{i,h},\va_{i,h})}-\sqrt{\PP_h(s_{i,h+1}|s_{i,h},\va_{i,h})} \right)^2 ds_{i,h+1}\right] + \sqrt{2\epsilon}|\gS|\notag\\
    &= 1-\E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\hellinger{\PP_{f,h}(\cdot|s_{i,h},\va_{i,h})}{\PP_h(\cdot|s_{i,h},\va_{i,h})}\right] + \sqrt{2\epsilon}|\gS|,
\end{align}
where in the first inequality we use the fact that $\sqrt{a+b}\leq \sqrt{a}+\sqrt{b}$ for any $a,b\geq 0$, and the last line uses the definition of the Hellinger distance in \eqref{eq:Hellinger}.

Plugging \eqref{eq:construct_Hellinger} into \eqref{eq:concentration_exp_mg_part}, we have
\begin{align*}
    & \sum_{i=1}^t \log\E\left[\exp\left(-\frac{1}{2}X^{f_\epsilon}_{i,h}(\epsilon)\right)\bigg|\gF_{i-1}\right]\notag\\
    &\leq \sum_{i=1}^{t-1}\sum_{h=1}^H \log \left(1-\E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\hellinger{\PP_{f,h}(\cdot|s_{i,h},\va_{i,h})}{\PP_h(\cdot|s_{i,h},\va_{i,h})}\right] + \sqrt{2\epsilon}|\gS|\right)\notag\\
    &\leq -\sum_{i=1}^{t-1}\sum_{h=1}^H \E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\hellinger{\PP_{f,h}(\cdot|s_{i,h},\va_{i,h})}{\PP_h(\cdot|s_{i,h},\va_{i,h})}\right] + \sqrt{2\epsilon}|\gS|\notag\\
    & =-\sum_{i=1}^{t-1}\sum_{h=1}^H \E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\ell(f_h,s_{i,h},\va_{i,h})\right]+ TH\sqrt{2\epsilon}|\gS|,
\end{align*}
where the second inequality follows from $\log(x)\leq x-1$ for any $x>0$, and the last line follows the definition \eqref{eq:Hellinger_loss}.

Plugging the above inequality into \eqref{eq:concentration_exp_mg_1}, we have with probability at least $1-\frac{\delta}{N+1}$:
\begin{align}\label{eq:sum_X_mg_proof}
    \forall t\in[T],f\in\gF:\quad -\sum_{i=1}^{t-1}\sum_{h=1}^H X_{i,h}^f &\leq -2\sum_{i=1}^{t-1}\sum_{h=1}^H \E_{(s_{i,h},\va_{i,h})\sim d_h^{\vpi_i}(\rho)}\left[\ell(f_h,s_{i,h},\va_{i,h})\right] \notag\\
    &\quad+ 2TH\sqrt{2\epsilon}|\gS| + 2H\log\left(\frac{(N+1)H}{\delta}\right) + 2dH\log\left(1+\frac{2\sqrt{d}}{\epsilon}\right).
\end{align}

 
Then analogous to \eqref{eq:sum_X_mg_proof}, we can bound $-\sum_{i=1}^{t-1}\sum_{h=1}^H Y_{i,h,n}^f$ for all $n\in[N]$ with probability at least $1-\frac{N\delta}{N+1}$ as follows:
\begin{align}\label{eq:sum_Y_mg_proof}
    \forall t\in[T],f\in\gF, n\in [N] :\quad & -\sum_{i=1}^{t-1}\sum_{h=1}^H Y_{i,h,n}^f  \leq -2\sum_{i=1}^{t-1}\sum_{h=1}^H \E_{(s_{i,h}^n,\va_{i,h}^n)\sim d_h^{\widetilde{\vpi}_{i,n}}(\rho)}\left[\ell(f_h,s_{i,h}^n,\va_{i,h}^n)\right] \notag\\
    &\quad+ 2TH\sqrt{2\epsilon}|\gS| + 2H\log\left(\frac{(N+1)H}{\delta}\right) + 2dH\log\left(1+\frac{2\sqrt{d}}{\epsilon}\right).
\end{align}

Letting $\epsilon=\frac{1}{T^2|\gS|^2}$ in \eqref{eq:sum_X_mg_proof} and \eqref{eq:sum_Y_mg_proof}, we obtain \eqref{eq:sum_X_mg} and \eqref{eq:sum_Y_mg} in Lemma~\ref{lm:bound_X_Y_mg}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Proof Lemma~\ref{lm:V_diff_bound}}\label{app:proof_V_diff_bound}
 

To prove Lemma~\ref{lm:V_diff_bound}, we first express the value difference sum $\sum_{t=1}^T \left|V_{\widehat{f}_{t},n}^{\widehat{\vpi}_{t}}(\rho)-V_{n}^{\widehat{\vpi}_{t}}(\rho)\right|$ on the left hand side of \eqref{eq:V_diff_bound} 
as sum of the expectation of the model estimation errors $\gE_n^{\widehat{\vpi}_{t}}(\widehat{f}_{t,h},s_h,\va_h)$.

\paragraph{Step 1: reformulating the value difference sum.}
    For any $f\in\gF$ and $\vpi=(\pi^1,\cdots,\pi^N):\gS\times[H]\rightarrow\Delta(\gA)$, we have (recall we defined the state-action visitation distribution  $d_h^\vpi(\rho)$ in \eqref{eq:state_action_visitation_finite}) for $n\in[N]$:
    \begin{align}\label{eq:V_f_decompose}
     V_{f,n}^{\vpi}(\rho) &= \E_{\forall h\in[H]:(s_h,\va_h)\sim d_h^\vpi(\rho),\atop s_{h+1}\sim\PP_h(\cdot|s_h,\va_h)}\left[\sum_{h=1}^H\left(V_{f,h,n}^{\vpi}(s_h)-V_{f,h+1,n}^{\vpi}(s_{h+1})\right)\right]\notag\\
        &=\E_{\forall h\in[H]:(s_h,\va_h)\sim d_h^\vpi(\rho),\atop s_{h+1}\sim\PP_h(\cdot|s_h,\va_h)}\left[\sum_{h=1}^H\left(Q_{f,h,n}^{\vpi}(s_h,\va_h)-\beta\log\frac{\pi^n(a_h^n|s_h^n)}{\piref^n(a_h^n|s_h^n)} - V_{f,h+1,n}^\vpi(s_{h+1})\right)\right],
    \end{align}
    where in the second line we use the fact that
    $$V_{f,h,n}^\vpi (s)=\E_{\va\sim\vpi(\cdot|s)}\left[Q_{f,h,n}^\vpi(s,\va)-\beta\log\frac{\pi^n(a^n|s^n)}{\piref^n(a^n|s^n)}\right].$$
    By the definition of $V_{n}^{\vpi}$ we have
    \begin{align}\label{eq:V_by_def}
        \forall n\in[N]:\quad V_n^{\vpi}(\rho) = \E_{\forall h\in[H]:(s_h,\va_h)\sim d_h^\vpi(\rho),\atop s_{h+1}\sim\PP_h(\cdot|s_h,\va_h)}\sum_{h=1}^H\left[r_h^n(s_h,\va_h)-\beta\log\frac{\pi^n(a_h^n|s_h^n)}{\piref^n(a_h^n|s_h^n)}\right].
    \end{align}
    To simplify the notation, we define
    \begin{align}\label{eq:Ppushforward}
        \forall g\in\gF,h\in[H]:\quad\PP_{g,h} V_{f,h+1,n}^{\vpi}(s_h,\va_h)\coloneqq \E_{s_{h+1}\sim\PP_{g,h}(\cdot|s_h,\va_h)}\left[V_{f,h+1,n}^{\vpi}(s_{h+1})\right].
    \end{align}
    Combining \eqref{eq:V_f_decompose} and \eqref{eq:V_by_def}, we have
    \begin{align}
        V_{f,n}^{\vpi}(\rho) - V_{n}^{\vpi}(\rho) &= \E_{\forall h\in[H]:(s_h,\va_h)\sim d_h^\vpi(\rho),\atop s_{h+1}\sim\PP_h(\cdot|s_h,\va_h)}\left[\sum_{h=1}^H \left( Q_{f,n}^{\vpi}(s_h,\va_h) - r_h^n(s_h,\va_h) -  V_{f,h+1,n}^\vpi(s_{h+1})\right)\right]\notag\\
    &= \sum_{h=1}^H\E_{(s_h,\va_h)\sim d_h^\vpi(\rho)}[\underbrace{\PP_{f,h} V_{f,h+1,n}^\vpi(s_h,\va_h) - \PP_h V_{f,h+1,n}^\vpi(s_h,\va_h)}_{ =: \gE_n^\vpi(f_h,s_h,\va_h)}].
    \end{align}

Therefore, we can express the value difference sum $\sum_{t=1}^T \left|V_{\widehat{f}_{t},n}^{\widehat{\vpi}_{t}}(\rho)-V_{n}^{\widehat{\vpi}_{t}}(\rho)\right|$ as sum of the expectation of the model estimation errors $\gE_n^{\widehat{\vpi}_{t}}(\widehat{f}_{t,h},s_h,\va_h)$:
\begin{align}\label{eq:V_to_res}
    \sum_{t=1}^T \left|V_{\widehat{f}_{t},n}^{\widehat{\vpi}_{t}}(\rho)-V_{n}^{\widehat{\vpi}_{t}}(\rho)\right|= \sum_{t=1}^T\sum_{h=1}^H \left| \E_{(s,\va)\sim d_h^{\widehat{\vpi}_{t}}(\rho)}\left[\gE_n^{\widehat{\vpi}_{t}}(\widehat{f}_{t,h},s,\va)\right]\right|.
\end{align}

Thus we only need to bound the right-hand side of \eqref{eq:V_to_res}.

\paragraph{Step 2: bounding the sum of model estimation errors.}

    By Assumption~\ref{asmp:function_class}, there exist $\theta_{f,h}$ and $\theta_h^\star$ in $\Theta_h$ such that $f_h(s_{h+1}|s_h,\va_h)=\phi_h(s_h,\va_h,s_{h+1})^\top\theta_{f,h}$ and $\PP_h(s_{h+1}|s_h,\va_h)=\phi_h(s_h,\va_h,s_{h+1})^\top\theta_h^\star$ for all $h\in[H]$.
    Thus we have
    \begin{align}
        \E_{(s_h,\va_h)\sim d_h^\vpi(\rho)}\left[\gE_n^\vpi(f_h,s_h,\va_h)\right] = (\theta_{f,h}-\theta_h^\star)^\top\underbrace{\E_{(s_h,\va_h)\sim d_h^\vpi(\rho)}\left[\int_{\gS}\phi_h(s_h,\va_h,s_{h+1})V_{f,h+1,n}^\vpi(s_{h+1})ds_{h+1}\right]}_{=: x_{h,n}(f,\vpi)}.
    \end{align}
    
    We let $x_{h,n}^i(f,\vpi)$ denote the $i$-th component of $x_{h,n}(f,\vpi)$, i.e., $$x_{h,n}^i(f,\vpi)=\E_{(s_h,\va_h)\sim d_h^\vpi(\rho)}\left[\int_{\gS}\phi_h^i(s_h,\va_h,s_{h+1})V_{f,h+1,n}^\vpi(s_{h+1})ds_{h+1}\right].$$
    Then we have
    \begin{align}\label{eq:x_{h,n}_i_bound}
        \forall i\in[d]:\quad |x_{h,n}^i(f,\vpi)|\leq H
    \end{align}
    (recall that by the definition of the linear mixture model (c.f. Assumption~\ref{asmp:function_class}), $\phi_h^i(s,\va,\cdot)\in\Delta(\gS)$ for all $i\in[d]$ and $(s,\va)\in\gS\times\gA$), which gives
    \begin{align}\label{eq:x_{h,n}_bound}
        \norm{x_{h,n}(f,\vpi)}_2\leq H\sqrt{d}.
    \end{align}

    
    For each $t\in[T]$, we define $\Lambda_{t,h}\in\R^{d\times d}$ as
\begin{align}\label{eq:Lambda_mg}
    \Lambda_{t,h}\coloneqq \lambda I_d + \sum_{i=1}^{t-1} x_{h,n}(\widehat{f}_i,\widehat{\vpi}_i)x_{h,n}(\widehat{f}_i,\widehat{\vpi}_i)^\top.
\end{align} 

We can decompose the sum of model estimation errors as follows:
\begin{align}\label{eq:dec_mg}
\sum_{t=1}^T \left| \E_{(s,\va)\sim d_h^{\widehat{\vpi}_{t}}(\rho)}\left[\gE_n^{\widehat{\vpi}_{t}}(\widehat{f}_{t,h},s,\va)\right]\right| 
    &= \underbrace{\sum_{t=1}^T \left| \langle x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t}),\widehat{\theta}_{t,h}-\theta_h^\star\rangle\right|\mathbbm{1}\left\{\norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}}\leq 1\right\}}_{(a)}\notag\\
    & \quad+ \underbrace{\sum_{t=1}^T \left| \langle x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t}),\widehat{\theta}_{t,h}-\theta_h^\star\rangle\right|\mathbbm{1}\left\{\norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}}> 1\right\}}_{(b)}.
\end{align}

Below we bound (a) and (b) respectively.

\paragraph{Step 1: bounding term (a).}
By the Cauchy-Schwarz inequality, we have
\begin{align}\label{eq:bound_a_1}
    (a)&\leq \sum_{t=1}^T \norm{\widehat{\theta}_{t,h}-\theta_h^\star}_{\Lambda_{t,h}}\norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}}\mathbbm{1}\left\{\norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}}\leq 1\right\}\notag\\
    &\leq \sum_{t=1}^T \norm{\widehat{\theta}_{t,h}-\theta_h^\star}_{\Lambda_{t,h}}\min\left\{ \norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}},1\right\},
\end{align}
where the last inequality uses the fact that
$$\norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}}\mathbbm{1}\left\{\norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}}\leq 1\right\}\leq \min\left\{ \norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}},1\right\}.$$

By the definition of $\Lambda_{t,h}$ (c.f. \eqref{eq:Lambda_mg}) and Assumption~\ref{asmp:function_class} we have
\begin{align}\label{eq:|w|_bound_mg}
    \norm{\widehat{\theta}_{t,h}-\theta_h^\star}_{\Lambda_{t,h}}\leq 2\sqrt{\lambda d}+\left(\sum_{i=1}^{t-1} |\langle \widehat{\theta}_{t,h}-\theta_h^\star,x_{h,n}(\widehat{f}_i,\widehat{\vpi}_i)\rangle|^2\right)^{1/2},
\end{align}
which gives
\begin{align}\label{eq:intermediate1}
    &\quad\sum_{t=1}^T \norm{\widehat{\theta}_{t,h}-\theta_h^\star}_{\Lambda_{t,h}}\min\left\{ \norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}},1\right\}\notag\\
    &\leq \sum_{t=1}^T\left(2\sqrt{\lambda d} +\left(\sum_{i=1}^{t-1} |\langle \widehat{\theta}_{t,h}-\theta_h^\star,x_{h,n}(\widehat{f}_i,\widehat{\vpi}_i)\rangle|^2\right)^{1/2}\right)\cdot\min\left\{ \norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}},1\right\}\notag\\
    &\leq \left(\sum_{t=1}^T 4\lambda d\right)^{1/2}
    \left(\sum_{t=1}^T \min\left\{ \norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}},1\right\}\right)^{1/2}\notag\\
    &\quad + \left(\sum_{t=1}^T \sum_{i=1}^{t-1} |\langle \widehat{\theta}_{t,h}-\theta_h^\star,x_{h,n}(\widehat{f}_i,\widehat{\vpi}_i)\rangle|^2\right)^{1/2}\left(\sum_{t=1}^T \min\left\{ \norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}},1\right\}\right)^{1/2},
\end{align}
where the first inequality uses \eqref{eq:|w|_bound} and the second inequality uses the Cauchy-Schwarz inequality and the fact that 
$$\min\left\{ \norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}},1\right\}^2\leq \min\left\{ \norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}},1\right\}. $$


By Lemma~\ref{lm:potential}, Lemma~\ref{lm:information_gain} and \eqref{eq:x_{h,n}_bound}, we have
\begin{align}\label{eq:|x|_bound_mg}
    \sum_{i=1}^t \min\left\{ \norm{x_{h,n}(\widehat{f}_i,\widehat{\vpi}_i)}_{\Lambda_{i,h}^{-1}},1\right\}\leq 2d\log\left(1+\frac{H^2T}{\lambda}\right)\coloneqq d_H(\lambda)
\end{align}
holds for any $\lambda>0$ and $t\in[T]$.
By \eqref{eq:|x|_bound_mg}, \eqref{eq:intermediate1} and \eqref{eq:bound_a_1}, we have
\begin{align}\label{eq:intermediate2}
    (a)\leq 2\sqrt{\lambda dT \min\{d_H(\lambda),T\}} + \left(d_H(\lambda)\sum_{t=1}^T \sum_{i=1}^{t-1} |\langle \widehat{\theta}_{t,h}-\theta_h^\star,x_{h,n}(\widehat{f}_i,\widehat{\vpi}_i)\rangle|^2\right)^{1/2}.
\end{align}

To continue, we have
\begin{align}\label{eq:bound_residual}
    |\langle \widehat{\theta}_{t,h}-\theta_h^\star,x_{h,n}(\widehat{f}_i,\widehat{\vpi}_i)\rangle|^2 &= \left|\E_{(s,\va)\sim d_h^{\widehat{\vpi}_i}(\rho)}\left[\int_\gS \left(\PP_{\widehat{f}_{t,h}}(s_{h+1}|s,\va)-\PP_h(s_{h+1}|s,\va) \right)V_{\widehat{f}_i,h+1,n}^{\widehat{\vpi}_i}(s_{h+1})ds_{h+1}\right]\right|^2\notag\\
    &\leq \E_{(s,\va)\sim d_h^{\widehat{\vpi}_i}(\rho)} \left[\left(\int_\gS \left(\PP_{\widehat{f}_{t,h}}(s_{h+1}|s,\va)-\PP_h(s_{h+1}|s,\va) \right)V_{\widehat{f}_i,h+1,n}^{\widehat{\vpi}_i}(s_{h+1})ds_{h+1}\right)^2\right]\notag\\
    &\leq 4\norm{V_{\widehat{f}_i,h+1,n}^{\widehat{\vpi}_i}(\cdot)}_\infty\E_{(s,\va)\sim d_h^{\widehat{\vpi}_{t}}(\rho)}D_{\mathsf{TV}}^2\left(\PP_{\widehat{f}_{t,h}}(\cdot|s,\va)\big\|\PP_h(\cdot|s,\va)\right)\notag\\
    &\leq 8H\E_{(s,\va)\sim d_h^{\widehat{\vpi}_i}(\rho)}\hellinger{\PP_{\widehat{f}_{t,h}}(\cdot|s,\va)}{\PP_h(\cdot|s,\va)}\notag\\
    &=8H \E_{(s,\va)\sim d_h^{\widehat{\vpi}_i}(\rho)}\ell(\widehat{f}_{t,h},s,\va),
\end{align}
where the second line uses the Cauchy-Schwarz inequality, the third line follows from H\"older's inequality, and $D_\mathsf{TV}$ denote the TV distance:
\begin{align}\label{eq:TV}
    D_{\mathsf{TV}}(P\|Q)\coloneqq \frac{1}{2}\int_\gX |P(x)-Q(x)|dx.
\end{align}
The fourth line uses the following inequality:
$$D_{\mathsf{TV}}^2(P\|Q)\leq2\hellinger{P}{Q},$$
and the fact that $\norm{V_{\widehat{f}_{t},h+1,n}^{\widehat{\vpi}_{t,h}}(\cdot)}_\infty\leq H$ (recall we assume $r(s,\va)\in[0,1]$). The last line uses \eqref{eq:Hellinger_loss}.

Plugging \eqref{eq:bound_residual} into \eqref{eq:intermediate2}, we have
\begin{align}\label{eqLbound_a_mg}
    (a)\leq 2\sqrt{d}\cdot\sqrt{\lambda T \min\{d_H(\lambda),T\}} + \left(8Hd_H(\lambda)\sum_{t=1}^T \sum_{i=1}^{t-1} \E_{(s,\va)\sim d_h^{\widehat{\vpi}_i}(\rho)}\ell(\widehat{f}_{t,h},s,\va)\right)^{1/2}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Step 2: bounding term (b).} Now we bound (b) in \eqref{eq:dec_mg}. Note that
$$
\mathbbm{1}\left\{\norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}}> 1\right\}\leq \min\left\{ \norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}},1\right\},
$$
which gives
\begin{align}\label{eq:bound_b_pre}
    (b)\leq \sum_{t=1}^T \left| \langle x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t}),\widehat{\theta}_{t,h}-\theta_h^\star\rangle\right|\min\left\{ \norm{x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t})}_{\Lambda_{t,h}^{-1}},1\right\}.
\end{align}

We also have
\begin{align}\label{eq:H_bound}
    \left| \langle x_{h,n}(\widehat{f}_{t},\widehat{\vpi}_{t}),\widehat{\theta}_{t,h}-\theta_h^\star\rangle\right|=\left|\E_{(s,\va)\sim d_h^{\widehat{\vpi}_{t}}(\rho)}\left[\PP_{\widehat{f}_{t,h}} V_{\widehat{f}_{t},h+1,n}^{\widehat{\vpi}_{t,h}}(s,\va)-\PP_h V_{\widehat{f}_{t},h+1,n}^{\widehat{\vpi}_{t,h}}(s,\va)\right]\right|\leq H.
\end{align}

Combining the \eqref{eq:H_bound},\eqref{eq:|x|_bound_mg} with \eqref{eq:bound_b_pre}, we have
\begin{align}\label{eq:bound_b_mg}
    (b)\leq H\min\{T, d_H(\lambda)\}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Step 3: combining everything together.}
Plugging \eqref{eqLbound_a_mg} and \eqref{eq:bound_b_mg} into \eqref{eq:dec_mg}, we have
\begin{align*}
    &\sum_{t=1}^T \left| \E_{(s,\va)\sim d_h^{\widehat{\vpi}_{t}}(\rho)}\left[\gE_n^{\widehat{\vpi}_{t}}(\widehat{f}_{t,h},s,\va)\right]\right|\notag\\
    &\leq 2\sqrt{d}\cdot\sqrt{\lambda T \min\{d_H(\lambda),T\}} + \left(8Hd_H(\lambda)\sum_{t=1}^T \sum_{i=1}^{t-1} \E_{(s,\va)\sim d_h^{\widehat{\vpi}_i}(\rho)}\ell(\widehat{f}_{t,h},s,\va)\right)^{1/2}+H \min\{T, d_H(\lambda)\}\notag\\
    &\leq \left(\frac{8Hd_H(\lambda)}{\eta}\cdot\eta\sum_{t=1}^T \sum_{i=1}^{t-1}  \E_{(s,\va)\sim d_h^{\widehat{\vpi}_i}(\rho)}\ell(\widehat{f}_{t,h},s,\va)\right)^{1/2} + \left(\sqrt{d}+H\right) \min\{d_H(\lambda),T\} +\sqrt{d} \lambda T\notag\\
    &\leq \frac{4d_H(\lambda)H}{\eta} +\frac{\eta}{2}\sum_{t=1}^T \sum_{i=1}^{t-1} \E_{(s,\va)\sim d_h^{\widehat{\vpi}_i}(\rho)}\ell(\widehat{f}_{t,h},s,\va) + \left(\sqrt{d}+H\right) \min\{d_H(\lambda),T\} +\sqrt{d} \lambda T
\end{align*}
for any $\eta>0$, where the second and third inequalities both use the fact that $\sqrt{ab}\leq \frac{a+b}{2}$ for any $a,b\geq 0$. 

Finally, combining \eqref{eq:V_to_res} with the above inequality, we have \eqref{eq:V_diff_bound}.
