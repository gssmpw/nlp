\section{Related work}
\label{sec:related}


We  discuss a few threads of related work, focusing on those with theoretical guarantees. 
 

\paragraph{Two-player matrix games.} Finding the equilibrium of two-player zero-sum matrix games has been studied extensively in the literature, e.g., \citet{mertikopoulos2018cycles,shapley1953stochastic,daskalakis2018last,wei2020linear}, where faster last-iterate linear convergence is achieved in the presence of KL regularization \citep{cen2021fast,zhan2023policy}. Many of the proposed algorithms focus on the tabular setting with full information, where the expected returns in each iteration can be computed exactly when the payoff matrix is given. More pertinent to our work, \citet{o2021matrix} considered matrix games with bandit feedback under the tabular setting, where only a noisy payoff from the players' actions is observed at each round, and proposed to estimate the payoff matrix using the upper confidence bounds (UCB) in an entry-wise manner~\citep{lai1987adaptive,bouneffouf2016finite}, as well as K-learning~\citep{o2021variational} that is akin to Thompson sampling \citep{russo2018tutorial}. Our work goes beyond the tabular setting, and proposes an alternative to UCB-based exploration that work seamlessly with different forms of function approximation.

 

\paragraph{Multi-player general-sum Markov games.} General-sum Markov games are an important class of multi-agent RL (MARL) problems \citep{littman1994markov}, and a line of recent works~\citep{liu2021sharp,bai2021sample,mao2023provably,song2021can,jin2021v,li2022minimax,sessa2022efficient} studied the non-asymptotic sample complexity for learning various equilibria in general-sum Markov games for the tabular setting under different data generation mechanisms. These works again rely heavily on carefully constructing confidence bounds of  the value estimates to guide data collection and obtain tight sample complexity bounds. In addition, policy optimization algorithms have also been developed assuming full information of the underlying Markov games, e.g., \citet{erez2023regret,zhang2022policy,cen2023faster}. 



\paragraph{MARL with linear function approximation.} Modern MARL problems often involve large state and action spaces, and thus require function approximation to generalize from limited data. Most theoretical results focus on linear function approximation, where the transition kernel, reward or value functions are assumed to be linear functions of some known feature maps. The linear mixture model of the transition kernel considered herein follows a line of existing works in both single-agent and multi-agent settings, e.g., \citet{ayoub2020model,chen2022almost,modi2020sample,jia2020model,chen2022almost,liu2024maximize}, which is subtly different from another popular linear model \citep{jin2020provably,wang2019optimism,yang2019sample,xie2020learning}, and these two models are not mutually exclusive in general~\citep{chen2022almost}. Moreover, \citet{ni2022representation,huang2022towards} considered general function approximation and \citet{cui2023breaking,wang2023breaking,dai2024refined} considered independent function approximation to allow more expressive function classes that lead to stronger statistical guarantees, which usually require solving complicated constrained optimization problems to construct the bonus functions. 

\paragraph{Uncertainty estimation in online RL.} Uncertainty estimation is crucial for efficient exploration in online RL. Common approaches are constructing the confidence set of the model parameters based on the observed data, which have been demonstrated to be provably near-optimal in the tabular and linear function approximation settings~\citep{jin2018q,agarwal2023vo} but have  limited success in the presence of function approximation in practice~\citep{gawlikowski2023survey}. Thompson sampling provides an alternative approach to exploration by maintaining a posterior distribution over model parameters and sampling from this distribution to make decisions, which however becomes generally intractable under complex function approximation schemes~\citep{russo2018tutorial}. Our approach draws inspiration from the reward-biased maximum likelihood estimation framework, originally proposed by \citet{kumar1982new}, which has been recently adopted in the context of bandits \citep{liu2020exploration,hung2021reward,cen2024value} and single-agent RL \citep{mete2021reward,liu2024maximize}. However, to the best of our knowledge, this work is the first to generalize this idea to the multi-player game-theoretic setting, which not only recovers but leads to new formulations for the single-agent setting.