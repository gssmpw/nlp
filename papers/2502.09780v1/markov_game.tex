 

\section{Multi-player General-sum Markov Games}\label{sec:markov}
 
We now turn to the more challenging setting of online multi-player general-sum Markov games, which includes the two-player zero-sum Markov game as a special case. 

\subsection{Problem setting}\label{sec:markov_setting}



\paragraph{Multi-player general-sum Markov game.} We consider an $N$-player general-sum episodic Markov game with a finite horizon denoted as $\gM_\PP\coloneqq(\gS,\gA,\PP, r, H)$, where $\gS$ is the state space, $\gA\coloneqq\gA_1\times\cdots\times\gA_N\coloneqq\prod_{n=1}^{N} \gA_n$ is the joint action space for all players, with $\gA_n$  the action space of player $n$, and
$H\in\NN_+$ is the horizon length. Let $\Delta(\gS)$ and $\Delta(\gA)$ denote the set of probability distributions over $\gS$ and $\gA$, respectively.  
$\PP=\{\PP_h\}_{h\in[H]}$ with $\PP_h:\gS\times\gA\rightarrow\Delta(\gS)$ is the inhomogeneous transition kernel: at step $h$, the probability of transitioning from state $s$ to state $s'$ by the action $\va=(a^1,\cdots,a^n)$ is $\PP_h(s'|s,\va)$. $r =\{r_h^n\}_{h\in[H],n\in[N]}$ stands for the reward function with
$r_h^n:\gS\times\gA\rightarrow[0,1]$ the reward of the $n$-th player at step $h$.

\paragraph{Markov policies.} In this paper, we focus on the class of Markov policies, where the policy of each player depends only on the current state, without dependence on the history. We let $\pi^n:\gS\times[H]\rightarrow \Delta(\gA)$ denote the policy of player $n$, and $\pi^n_h(\cdot|s)\in\Delta(\gA_n)$ denotes the probability distribution of the action of player $n$ at step $h$ given any state $s$. 
We let $\vpi=(\pi^1,\cdots,\pi^N):\gS\times[H]\rightarrow\Delta(\gA)$ denote the joint Markov policy (we assume all policies appear in this paper are Markovian, and we let \textit{joint policy} stands for joint Markov policy), where $\vpi_h(\cdot|s)\coloneqq (\pi_h^1,\cdots,\pi_h^N)(\cdot|s)\in\Delta(\gA)$ for all $s\in\gS$ and $h\in[H]$.  
 For any joint policy $\vpi$, we let $\vpi^{-n}$ denote the joint policy excluding player $n$. With a slight abuse of notation, we write $\vpi=(\pi^n,\vpi^{-n})$. In addition, a joint policy $\vpi$ is called a \textit{product policy} if $\pi^1,\cdots,\pi^N$ are executed independently, i.e., under policy $\vpi$, each player takes actions independently. We denote $\vpi = \pi^1\times \cdots \times \pi^N$ for a product policy.
 



 \paragraph{KL-regularized value function and Q-function.} 
 Given a joint policy $\vpi$, the KL-regularized state-value function (\textit{value function}) $V^{\vpi}_{h,n}: \gS\rightarrow\R$ and the KL-regularized state-action value function (\textit{Q-function}) $Q^{\vpi}_{h,n}:\gS\times\gA\rightarrow\R$ of the $n$-th player under $\vpi$ --- with regularization parameter $\beta\geq 0$ --- are respectively defined as
 \begin{subequations}
 \begin{align}
    \forall s\in\gS, h\in[H]:\quad V_{h,n}^{\vpi}(s) &\coloneqq \E_{\PP,\vpi}\left[\sum_{i=h}^H r_i^n(s_i,\va_i)-\beta\log\frac{\pi^n(a_i^n|s_i)}{\piref^n(a_i^n|s_i)}\bigg|s_h=s\right],\label{eq:V}\\
    \forall (s,\va)\in\gS\times\gA, h\in[H]:\quad Q_{h,n}^{\vpi}(s,\va) &\coloneqq r_h^n(s,\va) +  \E_{s'\sim\PP_h(\cdot|s,\va)}\left[V_{h+1,n}^{\vpi}(s')\right],\label{eq:Q}
 \end{align}
 \end{subequations}
 where $s_i$ and $\va_i$ are the state and action at step $i$, respectively, and $\allpiref:\gS\times[H]\rightarrow\Delta(\gA)$ is the reference policy. When the reference policy is a uniform distribution over the joint action space, the regularization becomes the entropy regularization. 
  In \eqref{eq:V}, $\pi^n(\cdot|s)$ (resp.~$\piref^n(\cdot|s)$) should be understood as the marginal distribution of player $n$ under joint distribution $\vpi(\cdot|s)$ (resp.~$\allpiref(\cdot|s)$),
 and we define $V_{H+1,n}^{\vpi}(s)=0$ for all $s\in\gS$ and $\beta\geq 0$. To simplify the notation, we define $V_n^{\vpi}\coloneqq V_{1,n}^{\vpi}$ and $Q_n^{\vpi}\coloneqq Q_{1,n}^{\vpi}$ for all $n\in[N]$.
We assume $\rho\in\Delta(\gS)$ is the initial state distribution, i.e., $s_1\sim\rho$. Furthermore, we define $V_n^\vpi(\rho)\coloneqq \E_{s\sim\rho}[V_n^\vpi(s)]$.
 

We let $\vpi=\pi^n\times\vpi^{-n}$ denote the policy profile where all players but the $n$-th player execute policy $\vpi^{-n}$, and the $n$-th player executes policy $\pi^n$ independent of the other players. For all $n\in[N]$, we define the best-response value function
\begin{align}\label{eq:V_dagger}
    \forall s\in\gS,h\in[H],n\in[N]:\quad V_{h,n}^{\star,\vpi^{-n}}(s)\coloneqq \max_{\pi^n:\gS\times[H]\rightarrow\Delta(\gA_n)} V_{h,n}^{\pi^n\times\vpi^{-n}}(s),
\end{align}
which is the optimal value of player $n$ when the policies of other agents are fixed by $\vpi^{-n}$.
Importantly, there exists at least one policy $\pi^{n,\star}(\vpi^{-n})$ that achieves the maximum in \eqref{eq:V_dagger} for all $s\in\gS$, and this policy is referred to the \textit{best-response policy} of player $n$ under joint policy $\vpi^{-n}$~\citep{shapley1953stochastic}.
 We also define 
 $$V_n^{\star,\vpi^{-n}}(\rho)\coloneqq \max_{\pi^n:\gS\times[H]\rightarrow\Delta(\gA_n)} V_n^{\pi^n\times\vpi^{-n}}(\rho).$$
One important thing to notice is that the best-response policy $\pi^{n,\star}(\vpi^{-n})$ does not depend on the initial state distribution $\rho$~\citep{mei2020global}.

\paragraph{Equilibria of Markov games.} In a multi-player general-sum Markov game, each agent aims to maximize its own value function, where the Nash equilibrium (NE)~\citep{nash1950non} and the coarse correlated equilibrium (CCE)~\citep{aumann1987correlated} are two widely studied solution concepts, whose definitions are as follows.
\begin{itemize}
    \item \textit{Nash equilibrium (NE):} a product policy $\vpi=\pi^1\times\cdots\times\pi^N$ is a Nash equilibrium of $\gM_\PP$ if
    \begin{align}\label{eq:def_NE}
            \forall s\in\gS, n\in[N]:\quad V_n^{\star,\vpi^{-n}}(s)=V_n^{\pi^n,\vpi^{-n}}(s).
    \end{align}
    \item \textit{Coarse correlated equilibrium (CCE):} a joint policy $\vpi$ is a CCE of $\gM_\PP$ if
    \begin{align}\label{eq:def_CCE}
            \forall s\in\gS, n\in[N]:\quad V_n^{\star,\vpi^{-n}}(s)\leq V_n^{\pi^n,\vpi^{-n}}(s).
    \end{align}
\end{itemize}
It is obvious that every NE is a CCE, but the converse is not true in general. In general, computing the NE in general-sum Markov games is intractable \citep{daskalakis2009complexity}, except for two-player zero-sum Markov games. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Goal: regret minimization.} 
To measure the proximity of a policy $\vpi$ to the equilibrium, we define the (average) sub-optimality gap of policy $\vpi$ w.r.t. the initial distribution $\rho$ as
\begin{align}\label{eq:gap}
    \gap{\vpi}\coloneqq \frac{1}{N}\sum_{n=1}^N \left(V_n^{\star,\vpi^{-n}}(\rho) - V_n^{\vpi}(\rho)\right).
\end{align}
A \textit{product} policy $\vpi$ is said to be an $\varepsilon$-approximate NE (abbreviated as $\varepsilon$-NE) if $\gap{\vpi}\leq \varepsilon$, and a \textit{joint} policy $\vpi$ is said to be an $\varepsilon$-approximate CCE (abbreviated as $\varepsilon$-CCE) if $\gap{\vpi}\leq \varepsilon$.
 

%
We aim to design a model-based framework that find the approximate NE or CCE of the Markov game $\gM_\PP$ in a provably efficient manner. Similar to the matrix game setting, we consider the following regret measure:
\begin{align}\label{eq:regret_MG}
 \regret(T)\coloneqq\sum_{t=1}^T\gap{\vpi_t}=\sum_{t=1}^T\frac{1}{N}\sum_{n=1}^N \left(V_n^{\star,\vpi_t^{-n}}(\rho) - V_n^{\vpi_t}(\rho)\right),
\end{align}
where $\vpi_t$ is the policy profile at time $t$. Our goal is to achieve a sublinear regret with respect to the number of rounds $T$, by carefully balancing the trade-off between exploration and exploitation, even under function approximation of the model class. 


\subsection{Algorithm development}\label{sec:markov_alg}

For simplicity, we will focus on the function approximation over the transition kernel of the Markov game assuming the reward function is fixed and deterministic, while it is straightforwardly to also incorporate the reward function approximation. We let $\gF$ denote the function class of the estimators of the transition kernel of the Markov game, and we denote the parameterized transition kernel as
$$\PP_f=(\PP_{f,1},\cdots,\PP_{f,H})\in\gF=\gF_1\times\cdots\gF_H,$$ 
where $\gF$ is the function class and $f$ is its parameterization.  We define the value function $V_{f,h,n}^{\vpi}$ under the transition kernel $\PP_f$ as
\begin{align}\label{eq:V_f}
    \forall s\in\gS, h\in[H]:\quad V_{f,h,n}^{\vpi}(s)\coloneqq \E_{\PP_f,\vpi}\left[\sum_{i=h}^H \left(r_i^n(s_i,\va_i)-\beta\log\frac{\pi^n(a_i^n|s_i)}{\piref^n(a_i^n|s_i)}\right)\bigg|s_h=s\right],
\end{align}
and the Q-function  $Q_{f,h,n}^{\vpi}$ is defined likewise.

Akin to the matrix game case, \name alternates between updating the model updates based on all the transitions observed so far, and collecting new trajectories using the updated policies.  Suppose that at the $t$-th iteration, the  current estimate of the transition kernel is $\PP_{f_{t-1}}$, and its corresponding NE or CCE is $\vpi_t$. \name alternates between the following two steps.
\begin{itemize}
\item {\em Value-incentivized model updates.} Given all the collected transitions $\gD_{t-1,h}$ at each step $h$ and the equilibrium policy $\vpi_t$, \name updates the model parameter $f_t$  via solving a regularized maximum likelihood estimation (MLE) problem as \eqref{eq:model_update_mg}, favoring models that {\em minimizes} the negative log-likelihood $\gL_t(f)$ of the model, i.e.
\begin{align}\label{eq:loss_mg}
    \gL_t(f)\coloneqq \sum_{h=1}^H\sum_{(s_h,\va_h,s_{h+1})\in\gD_{t-1,h}}-\log \PP_{f,h}(s_{h+1}|s_h,\va_h),
\end{align}
and {\em maximizes} the sum of the {\em best-response} values of each player when the other player's strategy is fixed at $\vpi_t^{-n}$. In words, the regularizer tries to encourage models that incentive the players to deviate from their current policy, resulting in better exploration. 

\item {\em Trajectory collection from best-response policy updates.} Using the updated model  $\PP_{f_{t}}$, \name updates the best-response policy $\widetilde{\pi}_t^n$ of each player while fixing the policy $\vpi_t^{-n}$ of the other player via \eqref{eq:policy_update_2_mg}. \name then collects new trajectories by following policy $\vpi_t$ and $(\widetilde{\pi}_t^n,\vpi_t^{-n})$ for all $n\in [N]$, and update the dataset. 
\end{itemize}
The complete procedure of \name is summarized in Algorithm~\ref{alg:markov_game}, where the function $\gamesolver(\gM_f)$ returns the NE or CCE of the Markov game $\gM_f$ by calling off-the-shelf solvers, e.g., \citet{cai2024near,zhang2022policy}. Note that we are primarily interested in finding the NE for two-player zero-sum Markov games, and the CCE for multi-player general-sum Markov games, due to computational tractability.


 
\paragraph{Comparison with MEX.} \citet[Algorithm~2]{liu2024maximize} proposed the MEX framework, which also considered using value functions as a means to incentive exploration for   two-player zero-sum Markov games. Their algorithm requires asymmetric updates --- and two sets of model parameters as a result --- of the max and min players, where the model update of the max player is regularized by the optimal value $V_f^{\star} = \max_{\pi^1}\min_{\pi^2} V_{1,1}^{\vpi}(\rho)$ of the Markov game, which is an expensive saddle-point optimization problem, and the model update of the min player is regularized by the best-response value function. In contrast, \name only leverages best-response value functions as a regularization, which is much easier to solve. \name also permits simultaneous  updates for all the players, making it amenable to multi-player general-sum Markov games. In contrast, MEX does not apply to this more general setting.

 


\begin{algorithm}[!thb]
    \caption{Value-incentivized Online Markov Game (\name)}
    \label{alg:markov_game}
    \begin{algorithmic}[1]
    \STATE \textbf{Input:} reference policies $\allpiref$, initial transition kernel estimate $f_0\in\gF$, regularization coefficient $\alpha>0$, iteration number $T$.
    \STATE \textbf{Initialization:} dataset $\gD_{0,h}\coloneqq \emptyset$, $\forall  h\in[H]$.  
    \FOR{$t = 1,\cdots,T$}
    \STATE $\vpi_t \leftarrow \gamesolver(\gM_{f_{t-1}})$. \trianglecomment{$\gamesolver(\gM_f)$ returns a CCE or NE of game $\gM_f$.}
    \STATE Model update: Update the estimator $f_t$ by minimizing the following objective:
    \begin{align}\label{eq:model_update_mg}         
        f_t=\argmin_{f\in\gF}\; \gL_t(f)  - \alpha \sum_{n=1}^N V_{f,n}^{\star,\vpi_t^{-n}}(\rho) .
    \end{align}
    \STATE Compute best-response policies $\{\widetilde{\pi}_t^n\}_{n\in[N]}$:
    \begin{align}\label{eq:policy_update_2_mg}
        \forall n\in[N]:\quad \widetilde{\pi}_t^n& = \argmax_{\pi^n:\gS\times[H]\rightarrow\Delta(\gA_n)} V_{f_t,n}^{\pi^n,\vpi_t^{-n}}(\rho).
    \end{align}
    \STATE Data collection: 
    sample a trajectory with transition tuples $\{(s_{t,h},\va_{t,h}, s_{t,h+1})\}_{h=1}^H$ by executing $\vpi_t$, and sample a trajectory with transition tuples $\{(s_{t,h}^n,\va_{t,h}^n, s_{t,h+1}^n)\}_{h=1}^H$ by executing $(\widetilde{\pi}_t^n,\vpi_t^{-n})$
    for each $n\in[N]$.
    Update the dataset  $\gD_{t,h} = \gD_{t-1,h}  \cup_{n=1}^N \{ (s_{t,h},\va_{t,h},s_{t,h+1}), (s_{t,h}^n,\va_{t,h}^n,s_{t,h+1}^n) \}$, $\forall h\in[H]$.  
    \ENDFOR 
    \end{algorithmic}
\end{algorithm}
 

\paragraph{Reduction to the single-agent MDP case.} \name can be reduced to the Markov decision process (MDP) setting via either setting the number of players $N=1$  in the multi-player general-sum Markov game, or setting the action space of the min player  to a singleton in the two-player zero-sum Markov game. Interestingly, the former leads to the value regularization $V_{f}^{\star}(\rho)$ studied in MEX \citep{liu2024maximize}, while the latter leads to a new form of regularizer $V_{f}^{\star}(\rho)-V_{f}^{\pi_t}(\rho)$, adding friction from the current policy $\pi_t$. 

 
 

\subsection{Theoretical guarantee}\label{sec:markov_analysis}

We demonstrate that \name achieves near-optimal regret under the following linear mixture model of the transition kernel for Markov games.


\begin{asmp}[linear mixture model]\label{asmp:function_class}
    The function class $\gF=\gF_1\times\cdots\gF_H$ is
    $$\forall h\in[H]:\quad\gF_h\coloneqq\left\{f_h|f_h(s'|s,\va)=\phi_h(s,\va,s')^\top{\theta_h},\quad \forall (s,\va,s')\in\gS\times\gA\times\gS,\theta_h\in\Theta_h\right\},$$
    where $\phi_h=(\phi_h^1,\cdots,\phi_h^d):\gS\times\gA\times\gS\rightarrow\R^d$ are the known feature maps with $\phi_h^i:\gS\times\gA\rightarrow\Delta(\gS)$ being transition kernels for all $i\in[d]$. $\norm{\phi_h(s,\va,s')}_2\leq 1$ for all $(s,\va,s')$, and $\Theta_h\subseteq\mathbb{B}^d_2(\sqrt{d})$, $\forall h\in[H]$. For each $f_h\in\gF_h$ and $(s,\va)\in\gS\times\gA$, $f_h(\cdot|s,\va)\in\Delta(\gS)$, $\forall h\in[H]$.
\end{asmp}
The linear mixture model is a common assumption in the RL literature, see, for example, \citet{ayoub2020model,modi2020sample,cai2020provably} for  single-agent RL, and \citet{chen2022almost,liu2024maximize} for Markov games.
We also assume the function class $\gF$ is expressive enough to describe the true transition kernel of the Markov game.
\begin{asmp}[realizability]\label{asmp:realizable}
    There exists $f^\star\in\gF$ such that $\PP_{f^\star}=\PP$.
\end{asmp}

\paragraph{Regret guarantee.} We now present our main result for the regret of the online Markov game, whose  proof is deferred to Appendix~\ref{app:proof_thm_regret_MP}. 
\begin{thm}\label{thm:regret_MG}
    Under Assumptions~\ref{asmp:function_class} and \ref{asmp:realizable}, if setting the regularization coefficient $\alpha$ as
    $$\alpha = \sqrt{\frac{ T}{Hd\log\left(1+\frac{T^{3/2}H^2}{\sqrt{d}}\right)} \left( \log\left(\frac{HN}{\delta}\right) + d\log\left(d|\gS|T\right) \right) },$$
    then for any $\beta\geq 0$, with any initial state distribution $\rho$, transition kernel estimator $f_0\in\gF$ and reference policy $\allpiref$, the regret of Algorithm~\ref{alg:markov_game} satisfies the following bound with probability at least $1-\delta$ for any $\delta\in(0,1)$:
    \begin{align}\label{eq:regret_MG_bd}
        \forall T\in\NN_+:\quad\regret(T)\leq \widetilde{\gO}\left(d \sqrt{H^3T}\cdot\sqrt{\frac{1}{d}\log\left(\frac{NH}{\delta}\right) + \log\left(d|\gS|T\right) 
        }\right).
    \end{align}
\end{thm}
Theorem~\ref{thm:regret_MG} establishes that by setting $\alpha$ on the order of $\widetilde{O}(\sqrt{T/H})$, with high probability, the regret of \name is no larger than an order of
$$ \widetilde{\gO}\left(d \sqrt{H^3T}\right)$$
for general-sum Markov games. When reducing to  two-player zero-sum Markov games, our  regret bound --- established for both players --- matches that of MEX~\citep{liu2024maximize}, which only covers  the max player. To the best of our knowledge, this is the first result that establishes a near-optimal sublinear regret for general-sum Markov games without explicit uncertainty quantification via constructing bonus functions or uncertainty sets. 

In addition, since
$    \min_{t\in[T]}\gap{\vpi_t}\leq  \regret(T) / T$ and each iteration collects $N+1$ trajectories, \name is guaranteed to find an $ \varepsilon$-NE ($\varepsilon$-CCE) of $\gM_\PP$ for any $\varepsilon>0$ within
$\widetilde{\gO}\left(\frac{ Nd^2H^3}{\varepsilon^2}\right)$ trajectories
or
$\widetilde{\gO}\left(\frac{ N d^2 H^4}{\varepsilon^2}\right)$ samples. Compared to the minimax sample complexity \citep{chen2022almost}, our sample complexity is near-optimal up to a factor of $H$ when the number of players $N$ is fixed.

  

%\begin{crl}[iteration/sample complexity of the Markov game]\label{crl:complexity_MG}
%    Under the same setting as in Theorem~\ref{thm:regret_MG}, for any $\beta\geq 0$, with any initial state distribution $\rho$, transition kernel estimator $f_0\in\gF$ and reference policy $\allpiref$, if $\gamesolver(\gM_{f_t})$ returns the NE (CCE) of the Markov game $\gM_{f_t}$ in each iteration $t$ of Algorithm~\ref{alg:markov_game}, then with probability at least 1-$\delta$,
%Algorithm~\ref{alg:markov_game} finds an $\varepsilon$-NE ($\varepsilon$-CCE) of $\gM_\PP$ for any $\varepsilon>0$ within
%$T=\widetilde{\gO}\left(\frac{H^3 d^2}{\varepsilon^2}\right)$
%iterations and with a sample complexity 
%$\widetilde{\gO}\left(\frac{H^4 N d^2}{\varepsilon^2}\right)$.
%\end{crl}



 

 
     