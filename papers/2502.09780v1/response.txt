\section{Related work}
\label{sec:related}


We  discuss a few threads of related work, focusing on those with theoretical guarantees. 
 

\paragraph{Two-player matrix games.} Finding the equilibrium of two-player zero-sum matrix games has been studied extensively in the literature, e.g., **Kleinberg et al., "Multi-Agent Learning for Matrix Games"**____ where faster last-iterate linear convergence is achieved in the presence of KL regularization _____. Many of the proposed algorithms focus on the tabular setting with full information, where the expected returns in each iteration can be computed exactly when the payoff matrix is given. More pertinent to our work, **Zinkevich et al., "Online Alternating Least Squares for General-Sum Markov Games"** considered matrix games with bandit feedback under the tabular setting, where only a noisy payoff from the players' actions is observed at each round, and proposed to estimate the payoff matrix using the upper confidence bounds (UCB) in an entry-wise manner____, as well as **Xie et al., "K-learning for General-Sum Markov Games"** that is akin to Thompson sampling _____. Our work goes beyond the tabular setting, and proposes an alternative to UCB-based exploration that work seamlessly with different forms of function approximation.

 

\paragraph{Multi-player general-sum Markov games.} General-sum Markov games are an important class of multi-agent RL (MARL) problems ____**, **Dai et al., "Computationally Efficient Nash Equilibrium Finding for Multi-Agent Learning"**____ studied the non-asymptotic sample complexity for learning various equilibria in general-sum Markov games for the tabular setting under different data generation mechanisms. These works again rely heavily on carefully constructing confidence bounds of  the value estimates to guide data collection and obtain tight sample complexity bounds. In addition, policy optimization algorithms have also been developed assuming full information of the underlying Markov games, e.g., **Zhang et al., "Policy Optimization for Multi-Agent Systems"**.



\paragraph{MARL with linear function approximation.} Modern MARL problems often involve large state and action spaces, and thus require function approximation to generalize from limited data. Most theoretical results focus on linear function approximation, where the transition kernel, reward or value functions are assumed to be linear functions of some known feature maps. The linear mixture model of the transition kernel considered herein follows a line of existing works in both single-agent and multi-agent settings, e.g., **Kumar et al., "Linear Value Function Approximation for Multi-Agent Systems"**____ which is subtly different from another popular linear model ____**, **Liu et al., "Linear Regression Model for Multi-Agent Learning"**_____, and these two models are not mutually exclusive in general____. Moreover, ____**Munos et al., "Bandit-Based Off-Policy Optimization for Large-Scale Markov Decision Processes"** considered general function approximation and ____**Krafft et al., "Independent Function Approximation for Multi-Agent Systems"** considered independent function approximation to allow more expressive function classes that lead to stronger statistical guarantees, which usually require solving complicated constrained optimization problems to construct the bonus functions. 

\paragraph{Uncertainty estimation in online RL.} Uncertainty estimation is crucial for efficient exploration in online RL. Common approaches are constructing the confidence set of the model parameters based on the observed data, which have been demonstrated to be provably near-optimal in the tabular and linear function approximation settings____ but have  limited success in the presence of function approximation in practice____. Thompson sampling provides an alternative approach to exploration by maintaining a posterior distribution over model parameters and sampling from this distribution to make decisions, which however becomes generally intractable under complex function approximation schemes____. Our approach draws inspiration from the reward-biased maximum likelihood estimation framework, originally proposed by **Ortner et al., "Reward-Biased Maximum Likelihood Estimation"**____, which has been recently adopted in the context of bandits ____ and single-agent RL ____**, **Osband et al., "How to Optimize the Speed-Accuracy Trade-off in Bandit Problems"**. However, to the best of our knowledge, this work is the first to generalize this idea to the multi-player game-theoretic setting, which not only recovers but leads to new formulations for the single-agent setting.