
\section{Two-Player Zero-Sum Matrix Games}
\label{sec:matrix_game}

In this section, we start with a simple setting of two-player zero-sum matrix games, to develop our algorithmic framework.

\subsection{Problem setting}\label{sec:matrix_setting}


\paragraph{Two-player zero-sum matrix game.} 
We consider the (possibly KL-regularized) two-player zero-sum matrix games with the following objective:
\begin{align}\label{eq:game_obj}
    \max_{\mu\in\Delta^m} \min_{\nu\in\Delta^{n}} \; f^{\mu,\nu}(A)\coloneqq\mu^\top A \nu - \beta\KL{\mu}{\muref} + \beta\KL{\nu}{\nuref},
\end{align}
where $A\in\mathbb{R}^{m\times n}$ is the payoff matrix, $\mu\in\Delta^m$ and $\muref\in\Delta^m$ (resp. $\nu\in\Delta^n$ and $\nuref\in\Delta^n$) are the policy and reference policy for the max (resp. min) player, and $\beta\geq 0$ is the regularization parameter.\footnote{For simplicity, we set the same regularization parameter for both players; our analysis continues to hold with different regularization parameters $\beta_1$ and $\beta_2$ for each player.} Here, the reference policies can be used to incorporate prior knowledge or preference of the game; when the reference policies are uniform distributions, the KL regularization becomes entropy regularization, which are studied in, e.g., \citet{cen2021fast}.

 
\paragraph{Nash equilibrium.} The policy pair $(\mu^\star,\nu^\star)$ corresponding to the solution to the saddle-point problem \eqref{eq:game_obj} represents a desirable state of the game, where both players perform their (regularized) best-response strategies against  the other player, so that no players will unitarily deviate from its current policy. Specifically,  the policy pair $(\mu^\star,\nu^\star)$ satisfies
$$\forall (\mu,\nu)\in\Delta^m\times\Delta^n:\quad f^{\mu,\nu^\star}(A)\leq f^{\mu^\star,\nu^\star}(A)\leq f^{\mu^\star,\nu}(A), $$
and is called the {\em Nash equilibrium} (NE) of the matrix game \citep{nash1950non}.\footnote{We note that under entropy regularization, the equilibrium is also known as the {\em quantal response equilibrium} (QRE) \citep{mckelvey1995quantal} when $\beta>0$.}

\paragraph{Noisy bandit feedback.}
We are interested in learning the NE when the payoff matrix $A$ is unknown and can only be accessed through a stochastic oracle. Specifically, for any $i\in[m]$ and $j\in[n]$, 
we can query the entry $A(i,j)$, and receive a noisy feedback $\widehat{A}(i,j)$ of $A(i,j)$ from an oracle, i.e.,
\begin{align}\label{eq:noisy_feedback}
    \widehat{A}(i,j) = A(i,j) + \xi,
\end{align}
where the noise $\xi$ is an i.i.d. zero-mean random variable across different queries. Each of the collected data tuple is thus in the form of $(i,j, \widehat{A}(i,j) )$.
 

\paragraph{Goal: regret minimization.} Our goal is to design an easy-to-implement framework that can find the approximate NE of the matrix game \eqref{eq:game_obj} with as few queries as possible to the stochastic oracle in a sequential manner. To begin, we define the following 
\begin{align}\label{eq:f_notation}
    f^{\star,\nu}(A)\coloneqq \max_{\mu\in\Delta^m} f^{\mu,\nu}(A),\quad f^{\mu,\star}(A)\coloneqq \min_{\nu\in\Delta^n} f^{\mu,\nu}(A),\quad\text{and}\quad f^\star(A)\coloneqq \max_{\mu\in\Delta^m}\min_{\nu\in\Delta^n} f^{\mu,\nu}(A)
\end{align}
for any payoff matrix $A$. The duality gap of the matrix game \eqref{eq:game_obj} at a policy pair $(\mu,\nu)$ is defined as
\begin{align}\label{eq:dualgap}
    \dualgap{\mu,\nu}\coloneqq    f^{\star,\nu}(A) - f^{\mu,\star}(A) ,
\end{align}
where it is evident that $ \dualgap{\mu^{\star},\nu^{\star}} = 0$. A policy pair $(\mu,\nu)$ is called an $\varepsilon$-approximate NE (abbreviated as $\epsilon$-NE) of the matrix game \eqref{eq:game_obj} if $\dualgap{\mu,\nu}\leq \varepsilon$.

In an online setting, given a sequence of policy updates $\{(\mu_t, \nu_t)\}_{t=1,\ldots, T}$ over $T$ rounds, a common performance metric is the cumulative regret, defined as 
\begin{align}\label{eq:regret}
    \regret(T)&\coloneqq \sum_{t=1}^T\dualgap{\mu_t,\nu_t}  = \underbrace{ \sum_{t=1}^T \left(  f^{\star,\nu_t}(A)-f^\star(A) \right) }_{\text{regret for min-player}}  + \underbrace{  \sum_{t=1}^T \left( f^\star(A) - f^{\mu_t,\star}(A) \right) }_{\text{regret for max-player}},
\end{align}
which encapsulates the regret from both players. Our goal is to achieve a sublinear, and ideally near-optimal, regret with respect to the number of rounds $T$, by carefully balancing the trade-off between exploration and exploitation, even under function approximation of the model class. 
 

\subsection{Algorithm development}\label{sec:matrix_alg}

We propose a model-based approach, called \name, that enables provably efficient exploration-exploitation trade-off via resorting to a carefully-regularized model (i.e., the payoff matrix) estimator without constructing uncertainty intervals. To enable function approximation, we parameterize the payoff matrix by $A_{\omega} \in \mathbb{R}^{m\times n}$,  where $\omega\in\Omega\subset\mathbb{R}^d$ is some vector in the parameter space $\Omega$. 

The proposed approach, on a high level, alternates between updating the payoff matrix based on all the samples collected so far, and collecting new samples using the updated  policies. Let's elaborate a bit further. At each round $t$, let  the current payoff matrix estimate be $A_{\omega_{t-1}}$, and its corresponding NE be $(\mu_t, \nu_t)$. 
\begin{itemize}
\item {\em Value-incentivized model updates.} Given all the collected data tuples $\gD_{t-1}$ and the policy pair $(\mu_t, \nu_t)$, \name updates the model parameter $\omega_t$  via solving a regularized least-squares estimation problem as \eqref{eq:model_update}, favoring models that {\em minimizes} the squared loss between the model and the noisy feedback stored in $\gD_{t-1}$, and {\em maximizes} the value of each player when the other player's strategy is fixed. In other words, the regularization term aims to maximize the duality gap at $(\mu_t, \nu_t)$, which tries to pull the model away from its current estimate $A_{t-1}$, whose duality gap is $0$ at $(\mu_t, \nu_t)$. The regularized estimator thus strikes a balance of exploitation (via least-squares on $\gD_{t-1}$) and exploration (via regularization against the current model $A_{\omega_{t-1}}$).

\item {\em Data collection from best-response policy updates.} Using the updated payoff matrix $A_{\omega_t}$, \name updates the best-response policy of each player while fixing the policy of the other player via \eqref{eq:policy_update_2}, resulting in policy pairs $(\widetilde{\mu}_t,\nu_t)$ and $(\mu_t,\widetilde{\nu}_t)$. Finally, \name collects one new sample from each of the policy pairs respectively following the oracle \eqref{eq:noisy_feedback}, and add them to the dataset $\gD_{t-1}$ to form $\gD_t$. 

\end{itemize}
The complete procedure of \name is summarized in Algorithm~\ref{alg:matrix_game}. \name invokes the mechanism of regularization as a means for incentivizing exploration, rendering it more amenable to implement in the presence of function approximation. In contrast, prior approach \citep{o2021matrix} heavily relies on 
explicitly adding an exploration bonus to the estimate of the payoff matrix using confidence intervals, which is challenging to construct under general function approximation. In addition, \name allows parallel and independent policy execution from both players.


 
\begin{algorithm}[th]
    \caption{Value-incentivized Online Matrix Game (\name)}
    \label{alg:matrix_game}
    \begin{algorithmic}[1]
    \STATE \textbf{Input:}  initial parameter $\omega_0$, regularization coefficient $\alpha>0$, iteration number $T$.  
    \STATE \textbf{Initialization:} dataset $\gD_0\coloneqq \emptyset$.
    \FOR{$t = 1,\cdots,T$}
    \STATE 
    Compute the Nash equilibrium $(\mu_t,\nu_t)$ of  the matrix game with the current parameter $\omega_{t-1}$:
    \begin{align}\label{eq:policy_update}
        \mu_t&=\argmax_{\mu\in\Delta^m}\min_{\nu\in\Delta^n}\; f^{\mu,\nu}(A_{\omega_{t-1}}),\qquad \nu_t=\argmin_{\nu\in\Delta^n}\max_{\mu\in\Delta^m}\; f^{\mu,\nu}(A_{\omega_{t-1}}).
    \end{align} 
    \STATE Model update: Update the parameter $\omega_t$ by minimizing the following objective:
    \begin{align}\label{eq:model_update}
        \omega_t=\argmin_{\omega\in\Omega}\sum_{(i,j,\widehat{A}(i,j))\in\gD_{t-1}}\left(A_\omega(i,j)-\widehat{A}(i,j)\right)^2  \underbrace{ -\alpha f^{\star, \nu_t}(A_{\omega})+\alpha f^{\mu_t,\star}(A_{\omega})}_{\textrm{value-incentivized~reg.}}.
    \end{align}
    \STATE Compute $\widetilde{\mu}_t$ and $\widetilde{\nu}_t$ by solving the following optimization problems:
    \begin{align}\label{eq:policy_update_2}
        \widetilde{\mu}_t& = \argmax_{\mu\in\Delta^m}\; f^{\mu,\nu_t}(A_{\omega_t}),\qquad \widetilde{\nu}_t = \argmin_{\nu\in\Delta^n}\; f^{\mu_t,\nu}(A_{\omega_t}).
    \end{align}
    \STATE Data collection: Sample $(i_t,j_t)\sim (\widetilde{\mu}_t,\nu_t)$ and $(i'_t,j'_t)\sim (\mu_t,\widetilde{\nu}_t)$ and get the noisy feedback $\widehat{A}(i_t,j_t)$ and $\widehat{A}(i'_t,j'_t)$ following the oracle \eqref{eq:noisy_feedback}. Update the dataset ${\gD}_t=\gD_{t-1}\cup\left\{ (i_t,j_t,\widehat{A}(i_t,j_t) ), \, (i'_t,j'_t,\widehat{A}(i'_t,j'_t) )\right\}$.
    \ENDFOR 
    \end{algorithmic}
\end{algorithm}

\paragraph{The benefit of regularization.} While \name is agnostic to the power of KL regularization in \eqref{eq:game_obj}, the major benefit of regularization comes in terms of computational efficiency. When the KL regularization parameter $\beta>0$, common first-order game solvers such as mirror descent ascent~\citep{sokota2022unified} or policy extragradient~\citep{cen2021fast} achieve a last-iterate  linear convergence rate when solving the matrix game \eqref{eq:policy_update}. Turning to the model update, when $\beta>0$, the regularization term in \eqref{eq:model_update} can be computed in closed form:
\begin{align}\label{eq:regularization_closed_form}
    & -  f^{\star,\nu_t}(A_\omega)+   f^{\mu_t,\star}(A_\omega) \notag\\
    &= - \beta\left[\log\left(\sum_{i=1}^n \mu_{\mathsf{ref},i} \exp\left(\frac{A_{\omega}(i,:)\nu_t}{\beta}\right)\right) +\log\left(\sum_{j=1}^m \nu_{\mathsf{ref},j} \exp\left(-\frac{\mu_t^\top A_{\omega}(:,j)}{\beta}\right)\right)\right]+C,
\end{align}
where $\mu_{\mathsf{ref},i}$ (resp. $\nu_{\mathsf{ref},j}$) is the $i$-th (resp. $j$-th) entry of $\mu_{\mathsf{ref}}$ (resp. $\nu_{\mathsf{ref}}$), $A_{\omega}(i,:)$ (resp. $A_{\omega}(:,j)$) is the $i$-th row (resp. $j$-th column) of $A_{\omega}$, and $C$ is a constant that does not depend on $A_\omega$. Leveraging the closed-form expression, one can bypass solving a bi-level optimization problem \eqref{eq:model_update} on its surface, but resorts to more efficient first-order methods. Last but not least, the policies $\widetilde{\mu}_t$ and $\widetilde{\nu}_t$ in \eqref{eq:policy_update_2} can be computed in closed form as well:
\begin{align}\label{eq:tilde_pi_closed_form}
    \widetilde{\mu}_{t,i} \propto \mu_{\mathsf{ref},i} \exp\left(\frac{A_{\omega_t}(i,:)\nu_t}{\beta}\right) ,\qquad \widetilde{\nu}_{t,j} \propto  \nu_{\mathsf{ref},j} \exp\left(-\frac{\mu_t^\top A_{\omega_t}(:,j)}{\beta}\right) ,\quad \forall i\in[m], j\in[n].
\end{align}


\paragraph{The case of symmetric payoff.} One important special class of matrix games is the symmetric matrix game~\citep{cheng2004notes}, with $A=-A^\top$, $\muref=\nuref$, and $m=n$. Many well-known games are symmetric, from classic games like rock-paper-scissors to the recent example of LLM alignment~\citep{munos2023nash,swamy2024minimaximalist,yang2024faster}. For a symmetric matrix game, it admits a symmetric Nash $(\mu^\star,\mu^\star)$, and Algorithm~\ref{alg:matrix_game}  reduces to a single-player algorithm by only tracking a single policy $\mu_t$, recognizing $\mu_t=\nu_t$ and $\widetilde{\mu}_t= \widetilde{\nu}_t$ due to $f^{\mu,\nu}(A)=-f^{\nu,\mu}(A)$. In addition,  \name only needs to collect one sample from the policy pair $(\widetilde{\mu}_t,\mu_t)$ in each iteration. This is particularly desirable when the policy is expensive to store and update, such as large-scale neural networks or LLMs. 

\paragraph{Reduction to the bandit case.} By setting the action space of the min player to $n=1$, \name seamlessly reduces to the bandit setting, where the payoff matrix becomes a reward vector $A \in \mathbb{R}^m$. Here, we let $f^{\mu}(A) = \mu^{\top} A - \beta\KL{\mu}{\muref}$ and 
     $f^{\star}(A)\coloneqq \max_{\mu\in\Delta^m} f^{\mu}(A)$. Interestingly, to encourage exploration, the regularization term favors a reward estimate that maximizes its regret $  f^{\star}(A_{\omega}) -  f^{\mu_t}(A_{\omega})$ on the current policy $\mu_t$, which is {\em different from} the reward-biasing framework that only regularizes against $  f^{\star}(A_{\omega})$ \citep{cen2024value,liu2020exploration}.   

\subsection{Theoretical guarantee}\label{sec:matrix_analysis}

We demonstrate that \name achieves near-optimal regret, assuming linear function approximation of the payoff matrix. Specifically, we have the following assumption.

\begin{asmp}[Linear function approximation]\label{asmp:bounded_payoff}
The payoff matrix is parameterized as  
\begin{align}\label{eq:A_param}
    A_\omega(i,j)\coloneqq \phi(i,j)^\top \omega, \quad\forall i\in[m], j\in[n],
\end{align}
where $\omega\in\Omega\subset\mathbb{R}^d$ is the parameter vector and $\phi(i,j)\in\mathbb{R}^d$ is the feature vector for the $(i,j)$-th entry. Here, the feature vectors are known and fixed, and satisfy $\norm{\phi(i,j)}_2\leq 1$ for all $i\in[m]$ and $j\in[n]$. For all $ \omega\in\Omega$, we suppose $\norm{\omega}_2\leq \sqrt{d}$ and $\norm{A_\omega}_{\infty}\leq B_l$ for some $B_l>0$. 
\end{asmp}

 

We also assume that the linear function class is expressive enough to describe the true payoff matrix $A$.
\begin{asmp}[realizability]\label{asmp:expressive}
    There exists $\omega^\star\in\Omega$ such that $A_{\omega^\star}=A$.
\end{asmp}

Next, we impose the noise follows standard sub-Gaussian distribution.
\begin{asmp}[i.i.d. sub-Gaussian noise]\label{asmp:noise}
    The noise $\xi$ in \eqref{eq:noisy_feedback} are i.i.d. mean-zero sub-Gaussian random variables with sub-Gaussian parameter $\sigma>0$.
\end{asmp}

\paragraph{Regret guarantee.} The following theorem states the regret bound of \name under appropriate choice of the regularization parameter.  
\begin{thm}\label{thm:regret}
    Suppose Assumptions~\ref{asmp:bounded_payoff}, \ref{asmp:expressive} and \ref{asmp:noise} hold. Let $\delta\in(0,1)$, setting the 
regularization coefficient $\alpha$ as
\begin{align}\label{eq:alpha}
\alpha=\sqrt{\frac{T}{d\log\left(1+(T/d)^{3/2}\right)} \left(\log(4T/\delta)+d\log (dT)\right)},
\end{align}
then for any $\beta\geq 0$, with any initial parameter $\omega_0$ and reference policies $\muref$ and $\nuref$, we have with probability at least $1-\delta$,
    \begin{align}\label{eq:regret_bound}
  \regret(T)=\gO\left(B_l\left(B_l+\sigma\sqrt{2\log(8T/\delta)}\right)d\sqrt{T  \log (dT )}\right)
    \end{align}
    for all $T\in \NN_+$.
\end{thm}

The proof of Theorem~\ref{thm:regret} is deferred to Appendix~\ref{app:proof_regret}. Theorem~\ref{thm:regret} establishes that by setting $\alpha$ on the order of $\widetilde{O}\big(\sqrt{T}\big)$, with high probability, the regret of \name is no larger than an order of
$$\widetilde{\gO}\left(d\sqrt{T}\right),$$
assuming the payoff matrix and the noise $\sigma$ are well-bounded.  In particular, when reduced to the linear bandit setting, this matches with the lower bound $\Omega(d\sqrt{T})$ established in  \citet{dani2008stochastic}, suggesting the near-optimality of our result. 
In addition, since $ \min_{t\in[T]}\dualgap{\mu_t,\nu_t}\leq \regret(T) /T$, \name is guaranteed to find an $\varepsilon$-NE of the matrix game \eqref{eq:game_obj} for any $\varepsilon>0$ within $\widetilde{\gO}\left( d^2 /\varepsilon^2 \right)$ samples.

  

