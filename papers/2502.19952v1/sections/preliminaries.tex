\section{Preliminaries}
This section provides the necessary background and preliminaries for understanding this paper. The frequently used notations
are presented in Table~\ref{tab:notations}.


\subsection{ Scatter-Gather Mining}
\label{ssec: MLSD}
%Money laundering activities usually transfer a bunch of money from one account to another one or a few accounts, during which the money is transferred multiple times as the camouflage for the illegalness of the money. 
% Although the topological structure of a transaction graph for money laundering is complex, what is invariant is that the dirty money is ultimately integrated into an account or a few accounts.
% Given the above insights, a naive method of discovering money laundering activities is to inspect if a large amount of money outflow from one account would be aggregated in other accounts. We name it \textit{Scatter-Gather Discovery} (SGD).

% In money laundering activities, it is common for money to be transferred between accounts in a way that disguises the illegal nature of the money. This involves multiple transfers of money, ultimately integrating the "dirty" money into one or a few accounts. While the transaction graph for money laundering can be complex, one constant is that the illicit money eventually ends up in specific accounts.

\begin{figure}
\begin{center}
\centerline{
    \includegraphics[width=0.8\linewidth]{figures/example4sgd.pdf}}
\caption{Illustration of centralized scatter-gather mining. Consider $a$ as the source and designate all the funds flowing out from $a$ as illicit money, visually represented in \textcolor{red}{red}. The enclosed numbers within boxes indicate the money possessed by the nodes, while numbers above the line represent the money involved in a transaction.
The orange nodes represent the detected money laundering nodes, which have an illicit funds ratio greater than 0.3.}
\label{fig: example4sgd}
\end{center}
\end{figure}

In order to detect money laundering transaction subgraphs of scatter-gather patterns, a simple approach is to examine cases where a significant amount of money flows out of one account and gets aggregated in other accounts~\cite{michalak2011graph}. We refer to this method as \textit{Centralized Scatter-Gather Mining}.
To illustrate, let's consider an example where there's a node $j$ that receives 80\% of the money flowing out from node $i$.  In this case, it's possible that both nodes $i$ and $j$, along with the nodes in-between, are involved in a potential money laundering activity, with $i$ acting as the source within the subgraph, and $j$ serving as the destination.


To determine how much of the money received by node $j$ comes from node $i$, the method utilizes a tracking mechanism based on the transaction graph. This involves marking the outflow money from node $i$ as suspected money and tracing their movement within the graph. When node $i$ sends money to another node $v$, the marked money is transferred to $v$. Similarly, if node $v$ subsequently sends money to node $j$, the marked money is also transferred to node $j$.
In the context of the method, two principles govern the flow of marked money in downstream nodes, considering that money is divisible. Denote $M_{in}^j, M_{out}^j$ as total inflow and outflow of node $j$ separately, and $m_{in}^j, m_{out}^j$ as marked inflow and outflow included in $M_{in}^j, M_{out}^j$ that satisfy $m_{in}^j\leq M_{in}^j,\ m_{out}^j\leq M_{out}^j$.

We have the following principles to calculate $m_{in}^j$:
\begin{enumerate}
\item For a node whose inflow money involves marked money, if $M_{in}^j>M_{out}^j$, then $m_{out}^j = m_{in}^j \frac{ M_{out}^j}{ M_{in}^j}$; if $M_{in}^j\leq M_{out}^j$, then $m_{out}^j = m_{in}^j$.
\item The marked inflow money of a node is the sum of marked money received from other nodes.
\end{enumerate}
After getting the value of $m_{in}^j$, we calculate the ratio of inflow money from $i$ to $j$ as $r_{ij} = \frac{m_{in}^j}{m_{out}^i}$. 


Figure~\ref{fig: example4sgd} illustrates an example of applying the method by considering node $a$ as the source node and discovering the scatter-gather pattern it is involved in. By setting the threshold to $40\%$, we identify three suspected money laundering nodes $b$, $c$, and $e$, which contain $40\%$, $60\%$ and $70\%$ of marked money, respectively.




\subsection{MinHash}
\label{ssec: minhash}
MinHash~\cite{minhash} is a technique to estimate how similar two sets are, where the similarity is defined in terms of the Jaccard similarity coefficient. Specifically, let $A$ and $B$ are two sets. The Jaccard index is defined to be the ratio of the number of elements of their intersection and the number of elements of their union: 
\begin{equation}
    J(A, B)=\frac{|A \cap B|}{|A \cup B|}.
\end{equation}
Let $H$ denote the minhash function that maps a set to a real number; it has the property  
\begin{equation}
    Pr[H(A)=H(B)] = J(A, B).
\end{equation}
That is, the probability that $H(A) = H(B)$ is true is equal to the similarity $J(A, B)$.

The details of the MinHash algorithm is following:
Given a hash function $h$ that maps the members of a set $U$ to real numbers, and \textit{perm} which is a random permutation of the elements of $U$. For any set $S \subset U$, 
% $H$ is defined as the member $x$ of $S$ with the minimum value of $h(perm(x))$, i.e.,
$H$ is defined as the minimum value of $h(perm(x))$, i.e.,

\begin{equation}
    \label{equ: minhash}
    H(S) := min\ h(perm(x)).
\end{equation}
% Meanwhile, in cases where the hash function used is assumed to have pseudo-random properties, the random permutation would be unnecessary.

Let $r$ be a random variable that is 1 when $H(A)=H(B)$ and 0 otherwise, $r$ is the unbiased estimator of $J(A, B)$, i.e., $E(r) = J(A, B)$. 
% However, the variance of $r$ is too high to be a useful estimator for the Jaccard similarity as its values are always 1 or 0. 
The MinHash scheme reduces this variance by averaging together several variables constructed in the same way, such as by applying multiple hash functions.
To estimate $J(A, B)$, let $n$ be the number of hash functions for which $H(A) = H(B)$, $\frac{n}{K}$ is the estimate, where $K$ is the total number of hash functions used. This estimate is the average of $K$ random variables $r$s, each of which is the unbiased estimator of $J(A, B)$. Hence, the average is also unbiased. By standard deviation for sums of the variables, the similarity estimation error is $\mathcal{O}(1/\sqrt{K})$.

% \Paragraph{MinHash and locality-sensitive hashing.}
% % Estimating the similarity between any two sets among the total of $N$ sets requires pairwise comparisons, which is time-consuming as the number of comparisons grows geometrically with the number of sets. Furthermore, most of those comparisons are unnecessary as they do not result in matches.
% The combination of MinHash and locality-sensitive hashing (LSH) provides an alternative approach to finding similar sets without performing pairwise comparisons. 
% % seeks to solve these problems. They make it possible to compute possible matches only once for each set so that the cost of computation grows linearly rather than exponentially. 

% Given $N$ sets, as presented before, we apply $H$ on each set to convert the set into a hash integer and repeat the process with $M$ different hash functions. Now we get a signature matrix of $M$ rows and $N$ columns, where each row represents applying $H$ with the same hash function to all sets, and each column represents applying $H$ with all hash functions with the same set. To apply the LSH algorithm, we divide the matrix into $b$ bands, each having $r$ rows, $M=b\cdot r$. For any sets $A, B$, the probability that the two columns in a band corresponding to $A, B$ are the same is equal to $s^r$, where $s = J(A, B)$ is the Jaccard similarity between $A, B$. For the $b$ bands, the probability of having at least one match can be calculated as $1-(1-s^r)^b$. When $s$ is, for example, 0.7, $r = 5$, and $b = 100$,
% the probability is about 97.48\%, and it increases as $s$ increase. By choosing approximate values for $b$ and $r$, it is possible to identify
% sets with a similarity larger than a predefined level by checking whether there exists a match.




\subsection{Bloom filter}
\label{ssec: bloomfilter}
A Bloom filter~\cite{bloomfilter} is a memory-efficient data structure that is used to test whether an element is present in a set. The price paid for the efficiency is that Bloom filter is a probabilistic data structure: It tells us that the element either \textit{definitely} is not in the set or \textit{may be} in the set. In other words, false positive matches are possible, but false negatives are not.

A Bloom filter is an array of $m$ bits with all positions set to $0$ when it is empty. There are also $k$ hash functions, each of which maps or hashes each element in a set to one of the $m$ positions uniformly. To \textit{add} an element, we simply feed it to each of the $k$ hash functions to get $k$ array positions and set the bits at all these positions to 1.
To \textit{query} an element (test whether it is in the set), hash it using the identical $k$ hash functions to get $k$ array positions. If any of the $k$ positions are $0$, the element is \textit{definitely} not in the set. If all are $1$, then the element is either in the set or the bits were set to $1$ when inserting other elements by chance, resulting in a false positive.
The false positive error $\epsilon$, the size of Bloom filter $m$, and the number of hash functions $k$ are related in the following way:
\begin{equation}
    k = -\log_2\epsilon,\ m = -\frac{n\ln\epsilon}{(\ln2)^2}, \text{and}\ \epsilon=(1-e^{-\frac{kn}{m}})^k
\end{equation}
With $m$ increases, the false positive probability $\epsilon$ decreases.







