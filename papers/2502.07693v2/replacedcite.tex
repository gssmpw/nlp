\section{Background and Related Work}
\label{sec:background}
As a background, this section first introduces legal concepts around privacy, then provides an overview of different types of privacy decisions for which individuals could receive support from \PPAs. 
Lastly, it presents AI and Machine Learning technologies that can provide the technical foundations for \PPAs.

\subsection{Legal Background} 
\label{subsec:legal}
% \todo[inline]{Subsection may need to be moved upward}
% \todo[inline]{Simone for a first pass}
%GDPR, consent requirements, whether a decision should fall under a certain category or another.\\
%How to exercise your rights, explainability, control but not just on pure transparency.\\
%AI act on explainability (important to note the transparency requirements and the exceptions for research, for a short account https://arxiv.org/pdf/2408.01449).

\subsubsection{Roles and Obligations According to the GDPR and AI Act}
\label{legal_background}
\PPAs are using AI techniques for processing data, including personal data, to assist users with making personalized privacy-related decisions. 
For the discussion of any legal requirements regarding the use of personalized privacy assistants, the question of who the data controller is for any personal data processing by the assistants will be of relevance.

If \PPAs are installed and run by users on their own devices or other servers under their control, the users will likely act as data controllers or joint controllers with other service providers. The so-called household exemption (Art. 2(2)(c) GDPR) can take effect, meaning that the GDPR____ will not apply if the user is using the assistant for private purposes on their private devices or servers under their control.
If the \PPA is run not only for purely private purposes on the user's devices or controlled servers, the data controller may be another entity different from the user (e.g., the user's employer). Legal obligations need to be fulfilled by the controllers regarding data protection by design and default (Art. 25) of the assistants, security of data processed (Art. 32), implementing data subject rights, including the data subject's rights to transparency (Art. 13--15), their rights to object to profiling (Art. 21), and the right not to be subject to a decision based solely on automated processing, including profiling (Art. 22).

In addition, legal obligations according to the EU AI Act____ may also have to be considered for the producer and also by the deployers of AI-driven privacy assistants, including requirements for risk management (Art. 9), transparency (Art. 13, 50), robustness, security, accuracy (Art. 15). These obligations however mostly apply if \PPAs could be classified as ``high-risk'' AI systems. This should, however, seldom be the case, especially as \PPAs are typically used for users' own privacy management, which should typically not interfere with the fundamental rights of others.
Exceptions could, however, be \PPAs that are, for example, used for setting permissions for safety-critical applications impacting the safety of the users or others.
%or if they are used to manage privacy decisions related to relational personal data that do not only related to the users but may also include sensitive information about other individuals.

%GDPR: Who is controller / processor? 
%AI act: obligations of producers, "users" (deployers)

%ex ante and ex post transparency, explainability (for automated decision making), and human oversight

\subsubsection{Legal Requirements for Transparency.}
%according to GDPR and the AI act (depending on the risk level).
In cases where the data controllers of the \PPAs are not the data subjects themselves, the controllers should provide the data subjects with privacy policy information \textit{ex-ante} at the time when data are obtained from them according to Art. 13 GDPR, and \textit{ex-post} through the right to access granted in Art. 15 GDPR.
This also should include information about purposes of processing, data categories concerned, but also information about the logic involved and significance, and envisioned consequences of automated decision-making and profiling performed by the \PPAs.

The AI Act also includes obligations for transparency for the producers and deployers of limited-risk and high-risk AI systems (Art. 50). While the providers of limited-risk AI systems have to mainly ensure that humans are informed that AI systems are used, high-risk AI systems require that further clear, comprehensible and adequate information is given to the deployer (Art. 13), traceability of results via logging (Art. 12) and appropriate human oversight (Art. 14).

\subsubsection{Legal Requirements for Consent.} 
Art. 4 (11) of the GDPR defines ‘Consent’ of data subjects as any freely given, specific, informed, and unambiguous indication of the data subject's wishes by which they, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to them.
A valid consent has thus to fulfill several conditions. Namely, it needs to be:
\begin{itemize}
    \item \textit{Freely given}, i.e., the data subject needs to have free choices -- this is usually not the case if there is an imbalance of power in the relation between the data subject and the data controller. Furthermore, there should be no negative consequences if consent is not given. Moreover, consent may not be bundled as a non-negotiable part of terms and conditions.
    \item \textit{Specific}, which means that consent must be given for one or more specific purposes and that the data subject must have a choice in relation to them -- i.e., separate opt-in is needed for each purpose.
    \item \textit{Informed}, which means that the data subject has to be informed about certain elements that are crucial to making a choice.  This includes information about the controller’s identity, the data processing purposes, the type of data, the right to withdraw consent, any use for decisions based solely on automated processing, and risks of data transfers to third countries.
    \item Moreover, a confirming statement or \textit{affirmative action} is needed for a valid consent and requires that the data subject has taken a deliberate action to consent. Therefore, silence, pre-ticked boxes, or inactivity should not constitute consent (Recital 32 GDPR). It also means that consent cannot be fully automated.
\end{itemize}


% Maybe the following shorter text is sufficent (to be decided later):

%According to its definition in Art. 4 (11) GDPR, a valid
%Consent needs to be \textit{informed}, \textit{specific}, \textit{freely given}, and \textit{unambiguous}, which entails a \textit{clear statement} or an \textit{affirmative action} (Article 4 (11) GDPR). 

%Not sure yet, if this should be mentioned as well:
%Moreover, Art. 7(3) of the GDPR explicitly states that users must be able to withdraw consent at any time, and that it shall be as easy to withdraw as to give consent.

%\subsubsection{Automated decision making and explainability}


\subsection{Privacy Decisions}
\label{subsec:privacy_decisions}
Among the most notable definitions, Westin____ has defined privacy as the right to informational self-determination, meaning that individuals should have the \textit{right to decide} for themselves when, how, and what information about them is communicated to others.
As mentioned, in the EU, the GDPR emphasizes that individuals should have control of their personal data (Recital 7), and thus should be empowered to make decisions about their data as one prerequisite for exercising such control.
Delving deeper into this notion of \textit{privacy decisions}, we further elaborate on this concept in the following subsections.

\subsubsection{Individual Privacy Decisions Regulated by Privacy Laws}
Some privacy decisions individuals can make to exercise control over their data are regulated under the GDPR and other privacy laws. 
These decisions notably include, but are not limited to, the \textit{decisions to grant or to withdraw consent} to data collection and processing.

Indeed, the GDPR and most other privacy laws regulate \textit{decisions to exercise data subject rights} granted by the respective laws. 
For instance, according to Art. 15-22 GDPR, data subjects have the rights to access data, request rectification or deletion of data, export data, and object to direct marketing and profiling.
Data subjects can also object in cases where the legal ground for the processing is public interest or legitimate interest, or exercise their right not to be subject to automated decision-making.

%privacy decisions of users for controlling the disclosure and conditions for the processing of their personal data

\subsubsection{Further Types of Privacy Decisions}
\label{subsec:further-decisions}
Further types of privacy decisions concerning users' choices regarding the use of their data by others, which are not directly mentioned or regulated by the GDPR, include \textit{decisions of individuals to publish or share data on their own initiative}, e.g., in social networks. In these cases, data sharing has typically not been formally triggered by a consent request to allow data sharing with another party.
%.Moreover, privacy decisions for controlling the disclosure or conditions for the processing one's personal data include 

Moreover, privacy decisions encompass \textit{privacy permission} (or access control rights) settings, which grant others certain rights for using their data and are, for instance, typically used for permission systems of mobile phone operating systems, such as Android or iOS.
Setting privacy permissions on mobile operating systems often requires consent at installation or during runtime. 
However, instead of consent, other legal grounds -- such as a contract (Art. 6 (1)(b) GDPR) --, can be used, e.g., for a banking app to forward account information when transferring money____.
Let us also note the peculiar case of Global Privacy Control (GPC), a unary signal that permits or prohibits third-party tracking on the browser____.
Due to its enforceability under the California Consumer Privacy Act (CCPA), it is regulated by a privacy law but is technically more akin to a privacy permission.

Additionally, some privacy-enhancing technologies and protocols allow users to decide and set \textit{privacy preferences}, which are simply indications of the users' privacy wishes of how their data should be used without actually granting any rights to others, and thus without legal mandate. 
Privacy preferences have, for instance, been used earlier by the Platform for Privacy Preferences (P3P) ____ or Do Not Track (DNT) as an example for signals that can be set manually in browser settings for allowing users to specify their privacy choices.




%preferences, permissions, data sharing, reject, and what could address consent based on requirements described above (overlap possible, can be hard to distinguish sometimes).





\subsection{AI for Decision-making}
\label{subsec:AI}
%AI used more and more for decision-making, different types of AI tools, and different types of AI techniques
AI is a generic term for various strategies and techniques enabling computers and machines to simulate human intelligence and problem-solving capabilities____.
Machine learning (ML) is a field of AI (we subsume the former under the latter in the rest of the document) that develops and studies statistical algorithms and models, draws inferences from patterns in data, and learns and adapts without following explicit instructions.
% We subsume and include ML in our definition of AI in the rest of the document.
AI-powered tools can particularly lighten the user's cognitive load and thereby improve their decision-making, e.g., by decision support, augmentation, or automation. 

While there are different ways to categorize AI systems, we refer in the present work to the survey paper on eXplainable AI (XAI) by ____.
They distinguish between transparent models and those requiring post-hoc explainability (non-inherently transparent).
We leverage this reference because AI-supported decision-making must be explained under specific circumstances according to the GDPR and the AI Act____.

In their words: ``A model is considered to be transparent if by itself it is understandable.''____
Such models include linear regression, decision trees, k-nearest neighbors, rule-based learning, general additive models, and Bayesian models.
Nonetheless, models that are not deemed intrinsically transparent can be made explainable through the use of \textit{post-hoc} techniques.
Neural networks (especially deep and convoluted) and Support Vector Machines (SVM) typically fall under this category, as well as reinforcement learning____.



% \newpage