\section{Background and Related Work}
\label{sec:background}
As a background, this section first introduces legal concepts around privacy, then provides an overview of different types of privacy decisions for which individuals could receive support from \PPAs. 
Lastly, it presents AI and Machine Learning technologies that can provide the technical foundations for \PPAs.

\subsection{Legal Background} 
\label{subsec:legal}
% \todo[inline]{Subsection may need to be moved upward}
% \todo[inline]{Simone for a first pass}
%GDPR, consent requirements, whether a decision should fall under a certain category or another.\\
%How to exercise your rights, explainability, control but not just on pure transparency.\\
%AI act on explainability (important to note the transparency requirements and the exceptions for research, for a short account https://arxiv.org/pdf/2408.01449).

\subsubsection{Roles and Obligations According to the GDPR and AI Act}
\label{legal_background}
\PPAs are using AI techniques for processing data, including personal data, to assist users with making personalized privacy-related decisions. 
For the discussion of any legal requirements regarding the use of personalized privacy assistants, the question of who the data controller is for any personal data processing by the assistants will be of relevance.

If \PPAs are installed and run by users on their own devices or other servers under their control, the users will likely act as data controllers or joint controllers with other service providers. The so-called household exemption (Art. 2(2)(c) GDPR) can take effect, meaning that the GDPR **Schrems, "The EU-US Privacy Shield is No Shield"** will not apply if the user is using the assistant for private purposes on their private devices or servers under their control.
If the \PPA is run not only for purely private purposes on the user's devices or controlled servers, the data controller may be another entity different from the user (e.g., the user's employer). Legal obligations need to be fulfilled by the controllers regarding data protection by design and default (Art. 25) of the assistants, security of data processed (Art. 32), implementing data subject rights, including the data subject's rights to transparency (Art. 13--15), their rights to object to profiling (Art. 21), and the right not to be subject to a decision based solely on automated processing, including profiling (Art. 22).

In addition, legal obligations according to the EU AI Act **Hindryckx, "EU Artificial Intelligence White Paper"** may also have to be considered for the producer and also by the deployers of AI-driven privacy assistants, including requirements for risk management (Art. 9), transparency (Art. 13, 50), robustness, security, accuracy (Art. 15). These obligations however mostly apply if \PPAs could be classified as ``high-risk'' AI systems. This should, however, seldom be the case, especially as \PPAs are typically used for users' own privacy management, which should typically not interfere with the fundamental rights of others.
Exceptions could, however, be \PPAs that are, for example, used for setting permissions for safety-critical applications impacting the safety of the users or others.
%or if they are used to manage privacy decisions related to relational personal data that do not only related to the users but may also include sensitive information about other individuals.

%GDPR: Who is controller / processor? 
%AI act: obligations of producers, "users" (deployers)

%ex ante and ex post transparency, explainability (for automated decision making), and human oversight

\subsubsection{Legal Requirements for Transparency.}
%according to GDPR and the AI act (depending on the risk level).
In cases where the data controllers of the \PPAs are not the data subjects themselves, the controllers should provide the data subjects with privacy policy information \textit{ex-ante} at the time when data are obtained from them according to Art. 13 GDPR, and \textit{ex-post} through the right to access granted in Art. 15 GDPR.
This also should include information about purposes of processing, data categories concerned, but also information about the logic involved and significance, and envisioned consequences of automated decision-making and profiling performed by the \PPAs.

The AI Act also includes obligations for transparency for the producers and deployers of limited-risk and high-risk AI systems (Art. 50). While the providers of limited-risk AI systems have to mainly ensure that humans are informed that AI systems are used, high-risk AI systems require that further clear, comprehensible and adequate information is given to the deployer (Art. 13), traceability of results via logging (Art. 12) and appropriate human oversight (Art. 14).

\subsubsection{Legal Requirements for Consent.} 
Art. 4 (11) of the GDPR defines ‘Consent’ of data subjects as any freely given, specific, informed, and unambiguous indication of the data subject's wishes by which they, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to them.
A valid consent has thus to fulfill several conditions. Namely, it needs to be:
\begin{itemize}
    \item \textit{Freely given}, i.e., the data subject needs to have free choices -- this is usually not the case if there is an imbalance of power in the relation between the data subject and the data controller. Furthermore, there should be no negative consequences if consent is not given. Moreover, consent may not be bundled as a non-negotiable part of terms and conditions.
    \item \textit{Specific}, i.e., it has to relate to specific processing operations or data categories. This means that the purpose of the processing must be specified, as well as the type and scope of the personal data involved.
    \item \textit{Informed}, i.e., the data subject must have been made aware of all relevant information about the processing, including its purposes, risks, and benefits.
    \item \textit{Unambiguous}, i.e., the consent must be clear and unequivocal. Silence or inaction cannot constitute an unambiguous indication of a data subject’s wishes.
\end{itemize}

\subsection{Privacy Enhancing Technologies}
% AI for Decision-making
% AI-powered tools can particularly lighten the user's cognitive load and thereby improve their decision-making, e.g., by decision support, augmentation, or automation. 

While there are different ways to categorize AI systems, we refer in the present work to the survey paper on **Gunning, "Explainable Artificial Intelligence (XAI): A Survey"**.
They distinguish between transparent models and those requiring post-hoc explainability (non-inherently transparent).
We leverage this reference because AI-supported decision-making must be explained under specific circumstances according to the GDPR and the AI Act **Hindryckx, "EU Artificial Intelligence White Paper"**.

\subsection{AI for Decision-making}
\label{subsec:AI}
%AI used more and more for decision-making, different types of AI tools, and different types of AI techniques
AI is a generic term for various strategies and techniques enabling computers and machines to simulate human intelligence and problem-solving capabilities **Rajamani, "Introduction to Artificial Intelligence"**.
Machine learning (ML) is a field of AI that develops and studies statistical algorithms and models, draws inferences from patterns in data, and learns and adapts without following explicit instructions.
% We subsume and include ML in our definition of AI in the rest of the document.
AI-powered tools can particularly lighten the user's cognitive load and thereby improve their decision-making, e.g., by decision support, augmentation, or automation. 

While there are different ways to categorize AI systems, we refer in the present work to the survey paper on **Gunning, "Explainable Artificial Intelligence (XAI): A Survey"**.
They distinguish between transparent models and those requiring post-hoc explainability (non-inherently transparent).
We leverage this reference because AI-supported decision-making must be explained under specific circumstances according to the GDPR and the AI Act **Hindryckx, "EU Artificial Intelligence White Paper"**.

In their words: ``A model is considered to be transparent if by itself it is understandable.'' **Gunning, "Explainable Artificial Intelligence (XAI): A Survey"**
Such models include linear regression, decision trees, k-nearest neighbors, rule-based learning, general additive models, and Bayesian models.
Nonetheless, models that are not deemed intrinsically transparent can be made explainable through the use of \textit{post-hoc} techniques.
Neural networks (especially deep and convoluted) and Support Vector Machines (SVM) typically fall under this category, as well as reinforcement learning **Russell, "Artificial Intelligence: A Modern Approach"**.