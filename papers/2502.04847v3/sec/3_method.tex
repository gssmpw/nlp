\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/framework_arxiv.pdf}
    \vspace{-0.25in}
    \caption{\textbf{The overview of HumanDiT.} HumanDiT focuses on generate videos from a single image using a pose-guided DiT model. A 3D VAE is employed to encode video segments into latent space. With 3D full attention, the initial frame (green border) serves as a noise-free prefix latent (green cube) for reference. The pose guider extracts body and background pose features, while the DiT-based denoising model renders the final pixel results. During inference, the keypoint-DiT model produces subsequent motions based on the pose of the first frame. With a guided pose sequence, the pose adapter transfers and refines poses via keypoint-DiT to animate the reference image.}
    \label{fig:framework}
    \vspace{-0.15in}
\end{figure*}
\section{Methodology}
\label{sec:method}

% The overall pipeline of our HumanDiT is depicted in Figure~\ref{fig:framework}. This section begins by introducing the fundamentals of diffusion models in Section~\ref{subsec:pre}. Next, in Section~\ref{subsec:data}, we showcase our data collection pipeline. With the large-scale dataset, in Section~\ref{subsec:dit}, we present our proposed pose-guided DiT-based video diffusion model. To generate long video for human motions, we introduce our explicit generation strategy in Section~\ref{subsec:infer}. Due to the pose transfer is required in our model, we propose a novel Pose Adapter and a keypoint generation model to further enhance the stability and vividness of the generated video.  % maybe drop this paragraph

\subsection{Preliminaries}
\label{subsec:pre}

\noindent\textbf{Latent Diffusion Models.} The latent diffusion models~\cite{rombach2022high} learns a denoising process to simulate the probability distribution within the latent space. To reduce the computational load, the image $x$ is transformed from pixel space into a latent space feature $z_0=\mathcal{E}(x)$ with a Variational Autoencoder (VAE) Encoder~\cite{kingma2013auto} $\mathcal{E}$. During the forward diffusion process, Gaussian noise is iteratively added to $z_0$ at various timesteps $t$ to $z_t$ as
$
    q(z_t | z_{t-1}) = \mathbb{N}(z_t;\sqrt{1-\beta_t}z_{t-1}, \beta_tI),
$
where $\beta$ represents a sequence schedule. The denoising process is defined as a iterative Markov Chain that progressively denoises the initial Gaussian noise $z_T \in \mathcal{N}(0, I)$ into the clean latent space $z_0$. The denoising function of LDM is commonly implemented with U-Net~\cite{ronneberger2015u} or Transformers~\cite{vaswani2017attention,peebles2023scalable}, which is trained by minimizing the mean square error loss as
$
    \mathcal{L} = \mathbb{E}_{z_t,c,t,\epsilon \sim \mathcal{N}(0,I)}[||\epsilon - \epsilon_\theta(x_t;c,t)||^2_2],
$ where $\epsilon_\theta$ represents parameterized network for predicting noise and $c$ denotes an optional conditional input. Subsequently, the denoised latent space feature is decoded into image pixels using the VAE Decoder $D$.

\noindent\textbf{Diffusion Transformer (DiT).} 3D U-Net is utilized widely in human video generation~\cite{hu2024animate,zhu2024champ,lin2024cyberhost,xue2024follow} with temporal attention modules to generate continuous video segments, while they always rely on fixed input resolution and reference network to control personality consistency, which cost additional calculations. The DiT~\cite{peebles2023scalable} combines the strengths of diffusion model with transformers~\cite{vaswani2017attention}, which address the limitations of U-Net-based LDM. By leveraging patchify~\cite{peebles2023scalable,yang2024cogvideox} and Rotary Position Encoding (RoPE), denoising model $\epsilon_\theta$ is able to process diverse image resolution and sequence length. 
RoPE is a positional encoding method that incorporates relative positional information into the embedding space by applying rotational transformations. Unlike traditional fixed or absolute encodings, RoPE allows the model to capture relative positional relationships in a resolution-agnostic manner, enabling it to generalize effectively across inputs of varying dimensions.

\subsection{Data Preparation}
\label{subsec:data}

We develop a structured data processing pipeline, which collects millions of videos that capture a wide range of human motions, including speeches, dancing, movies, and daily scenarios. To better understand human motions, a large amount of data is essential.

For each video sequence, the data samples consist of four components: (1) image sequences containing various human motions, (2) corresponding body pose sequences, (3) background keypoint sequences and (4) text areas. First, Yolo~\cite{Jocher_Ultralytics_YOLO_2023} is employed to track and distinguish different individuals in the video and crop them accordingly. To prevent overlapping bodies interfering with the keypoint estimation, cropped videos featuring multiple people are excluded. Next, the body poses sequences are labeled with Sapiens~\cite{khirodkar2024sapiens}, a strong tool for robust and stable pose estimation. The background keypoints are extracted by CoTracker~\cite{karaev2023cotracker}. We distinguish the foreground and background based on the human mask from the first frame. Additionally, since subtitles in the video can lead to undesired text generation during inference, PaddleOCR~\cite{PaddleOCR} is utilized to recognize the text bounding boxes. Finally, to ensure our data quality, those videos are filtered out where wrists or most of the body are not visible. For video clarity assessment, we manually label the clarity of 150K sets of teeth and hand images, each set containing five images with corresponding clarity ranks. Then a lightweight model~\cite{he2016deep} is employed to evaluate the clarity of hands and teeth, filtering high-quality videos based on the clarity score. More details are provided in Appendix.A.

With the data processing pipeline, a large-scale human video dataset is obtained. All videos are split into segments no longer than 20 seconds, and the final dataset consists of 4.5 million video clips, totaling 14,000 hours of duration.

\subsection{Pose-guided Video DiT Model} % HumanDiT
\label{subsec:dit}

Given a reference image $x^{0}$, the primary objective of HumanDiT is to generate high-quality, continuous human motion videos with consistent visual appearance. This task involves synthesizing realistic human motion and camera movements that mimic the pose sequence, while maintaining visual fidelity to the input image. Recent methods have shown promising generation capacity, while they always fix the image resolution and sequence length and require two stage training. To overcome this constraint, as shown in Figure~\ref{fig:framework}, HumanDiT employs scalable diffusion transformer (DiT)~\cite{peebles2023scalable} as its foundational model. With Rotary Position Embedding (RoPE)~\cite{su2024roformer} and patchify, DiT $\epsilon_\theta$ can handle videos of diverse size and length.

Following CogVideoX~\cite{yang2024cogvideox}, HumanDiT adopts a pre-trained 3D VAE with a video compression module. It incorporates 3D convolutions to compress videos both spatially and temporally, which generates longer videos while maintaining the quality and continuity. Specifically, given a video $\mathbf{x}:\{x^0,x^1,...,x^{4f+1}\}$ of size $(4f + 1) \times 8h \times 8w \times 3$, the video latent vector $\mathbf{z}_0$ can be encoded by the 3D VAE $\mathbf{z}_0 = \mathcal{E}(\mathbf{x})$, where $\mathbf{z}_0 \in \mathbb{R}^{(f+1) \times h \times w \times s}$ and $s$ denotes length of latent channel.

\noindent\textbf{Reference Strategy.} Visual consistency has always been a challenge in video generation. Unlike previous works~\cite{hu2024animate,zhang2024mimicmotion}, HumanDiT does not rely on a reference network with the same structure as the denoising model to transfer reference features. Instead, we simply employ a prefix-latent reference strategy. Thanks to the 3D full attention design in DiT, the model can directly capture reference image features from the prefix latent. Specifically, the latent feature vector of a video segment is denoted as $ \mathbf{z}_0 = \{z^0, z^1, z^2, \dots, z^f\} $. During the forward diffusion process, noise is gradually added to $ \{z^1, \dots, z^f\} $, while the prefix latent $ z^0 $ serves as a reference for the model in capturing the input image characteristics. During training, the predicted noise $ z_{t-1}^0 $ of $ z_t^0 $ is excluded from loss computation. Since the first feature $z^0$ of the 3D VAE is obtained from the first frame $x^0$ of the image independently, during inference, the prefix latent is obtained via the encoder $ z^0 = \mathcal{E}(x^0) $.

\noindent\textbf{Pose Guidance.} To extract pose-guided features, inspired by patchify, we introduce a linear pose guider for HumanDiT. Previous approaches often utilize multi-layer convolutional blocks, which effectively capture the contextual information of pose images, while these methods fail to account for temporal features. Unlike these methods, our proposed pose guider converts spatial pose pixels into a sequence of tokens by applying patchify. 

Specifically, the pose images are composed of pixel points representing the positions of body keypoints and background keypoints, where these keypoints are set to 1 and the rest to 0. The pose images corresponding to the video are denoted as  $
    \mathbf{P} \in \mathbb{R}^{(4f+1)\times4h\times4w\times d},
$ where $d$ represents the pose image dimension. To ensure patchify effectively and prevent future information from influencing present or past predictions~\cite{yang2024cogvideox}, padding is applied at the beginning of the pose images for $3$ frames. Subsequently, $ \mathbf{P} $ is patchified across the temporal, height, and width dimensions according to the patch size. To ensure alignment with the latent features $ \mathbf{z} $, the patch size is fixed at 4. Following the patchification process, the resulting pose tokens are represented as $  \mathbf{P}' \in \mathbb{R}^{(f+1) \times h \times w \times 64d} $, which can be projected into the condition $c$ in latent space, effectively capturing both spatial and temporal characteristics. This transformation is implemented through a linear layer and trained jointly with DiT model.

Skeletal representations tend to vary in size across resolutions, which can lead to inconsistencies and overlapping points in pose conditioning. To address this, HumanDiT uses pixel-based representations, which effectively capture pose information while reducing overlap. Heat-map-based conditioning~\cite{wang2021one} typically requires substantial memory due to the large number of body keypoints and extended sequences. To optimize memory usage and minimize pixel overlap, we set the pose image dimension  $d = 8$ , where the first 7 dimensions encode body keypoints and the last dimension is drawn with no more than 20 background keypoints. Each body keypoint is sequentially mapped to one of the first 7 dimensions in a round-robin fashion.

\noindent\textbf{Text Mask.} Subtitles in videos negatively impact the training and inference of HumanDiT. 
To address this issue, we implement a text-masking strategy to control the feature learning scope during training. Specifically, during the training process, text masks are applied to the regions corresponding to text in the latent feature space, preventing the model from calculating loss for those areas. This approach ensures that the model focuses on learning relevant visual and temporal features. Additionally, to enhance the model’s understanding that text regions should not be learned, the text mask is also provided as a conditional input to the model. This allows the model to explicitly recognize and ignore the masked regions during training and concentrate on human motions without being influenced by subtitles.

% In dynamic scenes, subtitles often obscure parts of the human body, potentially misleading the model and resulting in inaccurate or distorted outputs. Additionally, excessive textual signals from subtitles may cause the model to overfit to the subtitle regions during training, affecting the stability of generation and even causing unwanted subtitles to appear in generated videos. The presence of dynamic subtitles can also disrupt the model’s ability to learn temporal features, further hindering its performance and generation quality.

\subsection{Progressive Long Video Generation for Human}
\label{subsec:infer}

The generation of long continuous video sequences has been a challenge due to computational resource constraints. To address this, HumanDiT simply takes the final frame of one segment as the initial frame of the next with the prefix-latent strategy. Generated frames are consistent with the input image, which ensures visual continuity in the generated video. Additionally, with the transformer architecture and our designed pose guider, sequence parallelism~\cite{li2021sequence} is applied along the temporal dimension, allowing computations to be distributed across multiple devices. Longer video sequences is able to be trained and inferred efficiently. 

% previous works have employed strategies that overlap latent features~\cite{bar2023multidiffusion,zhang2024mimicmotion} to ensure consistent generation across frames. A typical way is the progressive latent fusion method introduced by MimicMotion~\cite{zhang2024mimicmotion}, which gradually blends latent features between consecutive frames to maintain visual coherence and smooth transitions throughout the video. This method effectively reduces discontinuities and artifacts, enabling the generation of longer video sequences, while the overlap of latent features increases the computational burden.

\noindent\textbf{Video Continuation from single image.}
To enable portrait animation from a single image, we also design a keypoint generation module that adopts the similar architecture as DiT, which generating keypoint sequences based on the pose in a given image. Specifically, given the initial keypoint $ j_0 $ from an image, the Keypoint-DiT $ \mathcal{K} $ is used to iteratively denoise and predict the keypoints for the subsequent $ m $ frames, expressed as 
$
    \{ j_1, j_2, \dots, j_m \} = \mathcal{K}(j_0).
$ Similar to Video DiT, Keypoint-DiT uses the pose from the first frame as a prefix pose and denoises the subsequent $t-1$ noise frames to generate the following motions.

With this pose generation module, HumanDiT can extend a single human pose into a dynamic and realistic motion. By predicting keypoint sequences, it enables our model to synthesize continuous motion without requiring full pose sequences as reference, making it adaptable to various video generation tasks.

\noindent\textbf{Pose Transfer with pose sequence.}
The U-Net-based LDM utilizes a Reference Network to incorporate reference images into the denoising process through cross attention. These reference images represent diverse scene and pose, making pose alignment with the reference images less critical. However, incorrect body proportions between the reference image and the input pose may lead to inaccurate visual results. In contrast, since HumanDiT generates human motion videos based on continuous poses, pose alignment is crucial. The main idea of our pose adapter is to decouple the motion sequence from the template and apply it to the skeleton of the initial reference image $ x^0 $.

Formally, the human pose action template is represented as a sequence $\mathcal{J} = \{j_k\}^m_{k=1} $, where $\mathcal{J}$ contains the position information of $m$ frames. The human pose in the reference image is denoted as $ j_0 $, which is consist of joints. Given a series of human joint pairs, the Euler distance of each joint $l_k$ and $l_0$ relative to its preceding joint can be calculated as $l^i = \| j^i - j^{i-1} \|_2$, where $j^{i-1}$ and $j^i$ are the neighboring keypoints. Since the human body moves in three-dimensional space, the distances in image space do not accurately reflect the actual joint lengths, which could mislead the transfer of the keypoints. To address this, the pose adapter traverses action sequences to obtain the maximum distance $l^{\prime}$ for each joint pair, using it as the reference joint length in the template pose. Given an initial joint, the remaining joint positions in $j_k$ can be transferred as the following
$
\hat{j}^i_k = \hat{j}^{i-1}_0 + l^i_0 \cdot (j^i_k - j^{i-1}_k)/l_i^{\prime}.
$

This procedure models the transformation of joint orientations and length variations within the template pose sequence, initiating calculations from the neck joint. Subsequent joint positions are derived step by step, ensuring that the directions and lengths of body parts remain consistent with those in the template video and resulting the aligned pose sequence $\hat{\mathcal{J}} = \{\hat{j}_k\}^m_{k=1}$ . 

\begin{table*}[htp!]
\centering\small

\begin{tabular}{c|ccccc|cc|cc}
\Xhline{1pt}
\textbf{Model} & \textbf{FID}  $\downarrow$   & \textbf{SSIM}   $\uparrow$ & \textbf{PSNR}   $\uparrow$  & \textbf{LPIPS}  $\downarrow$  & \textbf{L1}    $\downarrow$  & \textbf{FID-VID}  $\downarrow$  & \textbf{FVD}  $\downarrow$     & \textbf{Resolution} & \textbf{Frame} \\ \hline
DisCo~\cite{wang2024disco}       & 210 & 0.636              &  12.9& 0.439             & 6.29E-05     & 203      & 1808              & $256\times256$ & 1  \\ 
MagicDance~\cite{chang2023magicdance}          & 103 & 0.728              & 17.4& 0.327              &  3.43E-05  & 93.4         & 918   & $512\times512$ & 1 \\ 
AnimateAnyone~\cite{hu2024animate}         & 79.6&  0.737    &  18.1& 0.279              & 2.80E-05        & 68.8   & 730   & $576\times1024$ & 24 \\ 
MimicMotion~\cite{zhang2024mimicmotion}            & 65.4&  0.776              & 20.1& 0.238              &  2.76E-05  & 39.5         & 458   & $576\times1024$ & 72 \\ 
\cellcolor[HTML]{D9D9D9}{Ours}           & \cellcolor[HTML]{D9D9D9}{49.4} & \cellcolor[HTML]{D9D9D9}{0.838}             & \cellcolor[HTML]{D9D9D9}{22.8}  & \cellcolor[HTML]{D9D9D9}{0.186}           & \cellcolor[HTML]{D9D9D9}{1.73E-05}  & \cellcolor[HTML]{D9D9D9}{25.5}         & \cellcolor[HTML]{D9D9D9}{320}   & \cellcolor[HTML]{D9D9D9}$w,h<=1280$ & \cellcolor[HTML]{D9D9D9}25-249 \\ \Xhline{1pt}
\end{tabular}
\vspace{-0.1in}
\caption{Quantitative comparison on the TikTok dataset~\cite{jafarian2021learning} and our self-collected talking and dancing test datasets with existing pose-guided human body animation methods. Further details are provided in the supplemental material (Appendix.D.1).}
\label{tab:metrics}
\vspace{-0.1in}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{fig/compare_arxiv.pdf}
    \vspace{-0.25in}
    \caption{Qualitative comparison. Our approach outperforms others in rendering quality and pose accuracy. }
    \vspace{-0.2in}
    \label{fig:tiktok}
\end{figure*}

\noindent\textbf{Pose Refinement.} Our DiT renderer $ \epsilon_\theta $ performs denoising based on continuous motion sequences. However, directly using $\{j_0, \hat{j_1}, \dots, \hat{j_m}\}$ as conditions for generation leads to a lack of smooth transition between $ j_0 $ and $ \hat{j_1} $. This gap results in visual artifacts such as motion blur and ghosting, particularly affecting the quality of the initial segment of the sequence. Moreover, the hand and face regions pose additional challenges compared to the body skeleton. Due to the complex articulation, as well as the absence of 3D motion information in 2D keypoints, pose transfer struggles to adequately capture facial and hand movements. 

To produce smooth transitional poses and enhance the alignment of facial and hand features with the reference image, we introduce a refinement module based on Keypoint-DiT $ \mathcal{K} $. Specifically, the pose-aligned sequence $ \hat{\mathcal{J}} $ is employed as a condition for Keypoint-DiT $ \mathcal{K} $. To ensure smooth transitions, $ \tau $ frames are padded between $ j_0 $ and $ \hat{j_1} $, forming the final motion sequence $ \mathcal{J}_{\text{fix}} = \mathcal{K}(j_0, \hat{\mathcal{J}},\tau) $. This sequence consists of the initial frame $ j_0 $, $ \tau $ transitional frames, and the aligned pose sequence $\hat{\mathcal{J}}$ of $ m $ frames. The refinement module effectively bridges the gap between $ j_0 $ and $ \hat{j_1} $, maintaining temporal continuity. Additionally, the pose and shape of the face and hands in $ \mathcal{J}_{\text{fix}} $ are refined based on observations from $ j_0 $, resulting in more accurate and realistic movements. % Finally, $ \mathcal{J}_{\text{fix}}$ are employed as the pose guidance for motion transfer.

% training? finetune?
