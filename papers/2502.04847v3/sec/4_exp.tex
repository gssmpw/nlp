

\section{Experiments}
\label{sec:exp}

\begin{figure*}[htp]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/templ_arxiv.pdf}
    \vspace{-0.25in}
    \caption{The template pose-driven human rendering results of HumanDiT on the Flux~\cite{flux} generated images.}
    \vspace{-0.2in}
    \label{fig:templ}
\end{figure*}

\begin{figure}[htp]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/vconti.pdf}
    \vspace{-0.25in}
    \caption{The video continuation with generated motions.}
    \vspace{-0.15in}
    \label{fig:gene}
\end{figure} % one

\subsection{Implementation Details}

Without the constraints of the reference network, the video DiT requires only a single-stage end-to-end training process. We adopt 3D VAE and DiT-based denoising model from CogVideoX-5b~\cite{yang2024cogvideox}. The weights of 3D VAE keep frozen throughout training. The model is trained on our own collected dataset, with max video resolutions capped at $1280$ and a frame rate of $12.5$ fps. To support dynamic resolution, we incorporate Rotary Position Embedding (RoPE)~\cite{su2024roformer} in the model, which allows for flexible adaptation with a relative positional
encoding to variable input sizes without retraining. During training, we expose the model to a range of resolutions, enabling it to learn effectively across diverse video qualities and sizes. To enable training on longer video sequences, we implement sequence parallelism, allowing each sequence to be trained across up to 8 GPUs. The maximum latent feature token size is constrained to ensure $(f+1)\times h\times w<480,000$. For a 720p video at $720\times1280$ resolution with a latent size of $90\times160$, the maximum latent sequence length is $33$, equivalent to $129$ original frames. The model is trained on 128 NVIDIA H100 GPUs (80GB) with a learning rate of $5e-5$. During inference, the classifier-free guidance (CFG) scale for the pose control is set to $1.5$.

\subsection{Results}


For quantitative evaluation, we select 30 video clips outside the training set and adopt sequences 335-340 from the TikTok dataset~\cite{jafarian2021learning}, following~\cite{zhang2024mimicmotion}, as the test set.


We compare our approach against state-of-the-art pose-guided human video generation methods, including Disco~\cite{wang2024disco}, MagicDance~\cite{chang2023magicdance}, AnimateAnyone~\cite{hu2024animate}, and MimicMotion~\cite{zhang2024mimicmotion}. For AnimateAnyone, we utilize the open-source implementation provided by MooreThreads~\cite{moore}. Consistent with \cite{wang2024disco}, we evaluate image quality using L1 error, Peak Signal-to-Noise Ratio (PSNR)\cite{hore2010image}, Structural Similarity Index Measure (SSIM)\cite{wang2004image}, Learned Perceptual Image Patch Similarity (LPIPS)\cite{zhang2018unreasonable}, and Fréchet Inception Distance (FID)\cite{heusel2017gans}. Video-level FID (FID-VID)\cite{balaji2019conditional} and Fréchet Video Distance (FVD)~\cite{unterthiner2018towards} are employed to assess video fidelity.

\noindent\textbf{Quantitative Evaluation.} Table~\ref{tab:metrics} presents a quantitative comparison of various methods evaluated on the TikTok dataset~\cite{jafarian2021learning} and our own collected dancing and talking scene videos. The results demonstrate that our proposed method outperforms all existing approaches, achieving notably lower L1 loss, higher PSNR and SSIM scores, and reduced LPIPS, FID-VID, and FVD values. This performance is attributed to the scale and diversity of our dataset, which enables HumanDiT to generate animated human videos across a wide range of scenes and conditions. 

Leveraging the architecture of DiT and RoPE, our model accommodates variable input resolutions and arbitrary sequence lengths, thereby providing flexibility in handling input images. In contrast, existing methods require center cropping or resizing with padding to fixed dimensions, which can compromise the ability to fully capture the entire image context. Additionally, in terms of video quality, MimicMotion~\cite{zhang2024mimicmotion} relies on overlapping frames between denoised segments to ensure smoother transitions. While our method achieves superior continuity by utilizing only the last frame of the preceding segment, simplifying the process while improving video performance.

\noindent\textbf{Qualitative Analysis.} We conduct qualitative comparisons between the selected baselines and our method across various scenarios. Figure~\ref{fig:tiktok} presents visual results of our test set, where the first frame of the video is used as the reference image. Compared to other methods, our approach demonstrates superior performance in pose simulation and visual consistency. Even when body parts in the reference image, such as hands, are blurry or obscured, our method effectively renders clear and detailed body parts. While MimicMotion also demonstrates strong pose-following capabilities, it struggles with preserving clothing details, often rendering overly smooth and inconsistent appearances. In contrast, our method generates vivid videos with pose guidance while maintaining visual consistency.


% Figure~\ref{fig:ourdata} presents comparison results on our collected test set. In talking head generation, HumanDiT demonstrates accurate facial movement capture with explicit pose control, producing minimal artifacts. %These results suggest that our approach performs well in generating controllable poses and generalizes effectively. 

\noindent\textbf{Human Video Generation.} Figures~\ref{fig:templ} and ~\ref{fig:gene} demonstrate that our method effectively animates the reference human body using either a template or a generated pose sequence, as illustrated in the bottom right corner. Although our DiT-based model requires aligned and continuous poses, the proposed pose alignment strategy allows HumanDiT to animate reference images smoothly in synchronization with motion videos. Our approach produces high-quality, realistic character details while maintaining visual consistency with the reference image and ensuring temporal continuity across frames. The reference images depicted in the figures are generated by Flux~\cite{flux}. % Furthermore, our model accommodates dynamic camera movements, supported by controlled background keypoints, in addition to static background scenarios. Details in Appendix.D.2.



\subsection{Ablation Study}

\begin{figure}[htp]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/abl_sup_arxiv.pdf}
    \vspace{-0.25in}
    \caption{Ablation study of pose adapter and pose refinement.}
    \vspace{-0.15in}
    \label{fig:abl}
\end{figure}

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{p{0.3cm}cc|cccc}
    \Xhline{1pt}
        \textit{Size} & \textit{Tokens} & \textit{Ref.} & SSIM$\uparrow$ & PSNR$\uparrow$  & FID-VID$\downarrow$  & FVD$\downarrow$  \\ \hline
        5B & 80K & concat & 0.702 & 18.71 & 35.5  & 395 \\
        5B & 80K & prefix & 0.719 & 19.50 & 33.5 & 382 \\ \hline
        5B & 400K & prefix & 0.837 & \cellcolor[HTML]{D9D9D9}22.79 & 26.9 & 353 \\
      5B & 480K &
        prefix & \cellcolor[HTML]{D9D9D9}0.838 & 22.75 & \cellcolor[HTML]{D9D9D9}25.5 & \cellcolor[HTML]{D9D9D9}320 \\ \Xhline{1pt}
    \end{tabular}
    \vspace{-0.1in}
    \caption{Quantitative results of ablation study on maximin token size and prefix-latent reference strategy for reference image (\textit{Ref.}). The first two rows are conducted without using sequence parallel. }
    \vspace{-0.2in}
    \label{tab:abl}
\end{table}


To investigate the roles of the proposed conditions, we examine several model variants, altering factors such as maximum token size, prefix latent strategy, text mask during training, and pose refinement during inference.

%\noindent\textbf{Model Size.} We train three DiT models with varying model size (1B, 2B, 5B). Notably, each model is trained from scratch without pretraining for a fair comparison. Figure~\ref{tab:abl} provides an overview of their performance after 400K training iterations. We observe that increasing model size results in substantial improvements in diffusion performance.

\noindent\textbf{Max Token Size.} Table~\ref{tab:abl} demonstrates the importance of generating long video sequences. Reducing the maximum token count from 480K (up to 249 frames) to 80K (about 13-41 frames) significantly decreases the model's temporal visibility, which affects its capacity to fully understand coherent videos. Moreover, when generating longer videos with fewer tokens, the model requires more iterations that rely on the last frame of the previous sequence, which increases the likelihood of accumulated errors from earlier frames. Larger token count is still essential for better video performance while comparing to token size 400K. 

\noindent\textbf{Prefix-Latent Reference Strategy.} In Table~\ref{tab:abl}, we also compare two reference strategies, prefix-latent reference and latent concatenation. The results illustrates that while both methods perform comparably on single image generation, while concatenate latent lacks fidelity when applied to video generation. Additionally, the concatenate latent introduces extra computational cost, making it less efficient.

\noindent\textbf{Pose Refinement.} Relying on the pose adapter may lead to size variations or scaling inconsistencies in the hands and face, as shown in Figure~\ref{fig:abl}. Integrating pose adapter and Keypoint-DiT for pose refinement effectively resolves these inconsistencies, ensuring more accurate alignment with the reference image.

% Furthermore, in Figure~\ref{fig:abl}, the first row illustrates that without the text mask constraint during training, the generated video may include unintended text-like artifacts, as highlighted by the red box. The second row shows a comparison of whether keypoint DiT is used for pose refinement. It is evident that using only the Pose Adapter may lead to size shifts or scaling issues in the hands and face, affecting visual consistency with the reference image. % split two Para

\subsection{User Study}

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{c|cccc}
    \Xhline{1pt}
        Method & \textit{A.A.} & \textit{M.M.} & Ours & GT \\ \hline
        Consistency$\uparrow$ & 42\% / 35\% & 66\% / 79\% & \cellcolor[HTML]{D9D9D9}{98\% / 98\%}  & 99\% / - \\
        Preservation$\uparrow$ & 65\% / 71\% & 69\% / 68\% & \cellcolor[HTML]{D9D9D9}{99\%} / 96\% & 99\% / - \\
        Quality$\uparrow$  & 72\% / 64\% & 79\% / 66\% & \cellcolor[HTML]{D9D9D9}{98\%} / 95\% & 99\% / - \\\Xhline{1pt}
    \end{tabular}
    \vspace{-0.1in}
    \caption{User study. The values represent the win rates of the current method compared to other methods. We ask users to rate the generated videos on our test set (left) and wild pose sequences with pose transfer (right), comparing with Animate Anyone (\textit{A.A.})~\cite{hu2024animate} and MimicMotion (\textit{M.M.})~\cite{zhang2024mimicmotion} in terms of temporal consistency, identify preservation, and visual quality.}
    \vspace{-0.2in}
    \label{tab:user}
\end{table}

To assess the quality of our method and state-of-the-art approaches from a human perspective, we conduct a blind user study with 10 participants. As summarized in Table~\ref{tab:user}, our method significantly outperforms others in terms of temporal consistency, identity preservation, and visual quality, highlighting its effectiveness. Details in Appendix.C.