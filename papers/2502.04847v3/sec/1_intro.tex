\section{Introduction}
\label{sec:intro}

The generation of realistic human motion videos has garnered significant attention in recent years, particularly with the advancements in generative artificial intelligence~\cite{hu2024animate,guo2023animatediff,chang2023magicdance,zhu2024champ,zhang2024mimicmotion,xu2024magicanimate,xue2024follow,lin2024cyberhost}. Compared to image generation, video generation presents greater challenges as it requires not only high-quality visuals but also smooth temporal consistency across frames to maintain visual coherence~\cite{zhang2024mimicmotion}. Applications such as virtual humans, animated films, and immersive experiences drive the need for reliable, high-quality motion video generation methods.

% Despite considerable progress, current approaches to human motion video generation face several limitations. Accurate rendering of body parts, particularly in regions like the face and hands, remains a common challenge in generating long motion sequences. The diversity of human movements often leads to inaccuracies in complex motion scenarios, resulting in noticeable visual distortions, especially when handling diverse motions or significant variations in human poses and appearances. Additionally, to maintain identity consistency, works such as Animate Anyone~\cite{hu2024animate} have introduced reference networks to implicitly control the individuality of generated humans. Nevertheless, this result in  temporal inconsistencies, where the continuity of motion is compromised, causing unnatural transitions between frames. Moreover, many existing techniques struggle to generate high-quality, multi-resolution, long-duration videos due to computational and architectural limitations. % 1) rendering inaccuracies 2) temporal inconsistencies 3) fixed resolution and duration;

Despite notable advances~\cite{hu2024animate,zhu2024champ,xue2024follow,wang2024unianimate,xu2024magicanimate,chang2023magicdance,zhang2024mimicmotion,lin2024cyberhost}, current methods for generating human motion videos still face key limitations. 
First, achieving temporal consistency in long-sequence generation remains challenging due to multiple batches of inference.  Most models~\cite{hu2024animate,xue2024follow,lin2024cyberhost,zhang2024mimicmotion} restrict the number of frames per sequence with U-Net architectural~\cite{hu2024animate} constraints, and continuity methods~\cite{zhang2024mimicmotion,wang2024unianimate} based on overlap fail to prevent error propagation or ensure temporal consistency.
Second, the ability to generalize across varied scenarios is limited, primarily due to the absence of extensive and diverse datasets. Additionally, achieving high-fidelity rendering of facial and hand details poses challenges, often resulting in blurred or inconsistent outputs, especially in extended sequences. 
Third, most of them rely on fixed-resolution inputs, requiring resizing or padding, which impacts both quality and flexibility, as illustrated in Figure~\ref{fig:teaser}(a). 
Finally, most approaches~\cite{wang2024disco,hu2024animate,zhang2024mimicmotion,xue2024follow} are specifically designed for pose transfer with a given poses sequence, and any misalignment in pose can result in visual artifacts.
% 1) temporal consistency 2) Generalization , accurate rendering 3) multi resulution 4) pose transfer

To tackle these challenges, we present HumanDiT, an adaptable pose-guided body animation framework designed for diverse resolutions and long-form video generation (up to 20 seconds). We address the aforementioned challenges with an expert DiT for pose-guided video; a large-scale and high-quality dataset for diverse scenarios; an Keypoint-DiT for pose generation and a pose adapter for pose transfer.

First, to address variable resolution and dynamic sequence length, we replace the traditional U-Net diffusion model with the Diffusion Transformer (DiT)~\cite{peebles2023scalable}. Employing a prefix-latent reference strategy, HumanDiT preserves visual consistency across inputs while accommodating diverse resolutions and durations. The proposed pose guider facilitates the capture of temporal and spatial features via patch-based extraction, ensuring precise pose guidance. This DiT architecture enables sequence parallelism~\cite{li2021sequence}, optimizing training and inference for high-resolution, long-duration video generation.

Second, to enhance generalization and rendering quality, we gather a large-scale, diverse dataset comprising 14,000 hours of in-the-wild videos. This dataset is collected using a novel data processing pipeline, which includes a data extraction sequence and filtering strategies that employ scoring models to assess and select samples based on image clarity, particularly in detail-rich regions such as hands and teeth, thereby improving dataset reliability. We also incorporate text masks into the training process to prevent the model from learning text artifacts present in the wild data.

Third, we leverage an expert Keypoint-DiT for pose generation, thereby supporting a wide range of downstream applications beyond pose transfer~\cite{hu2024animate,zhang2024mimicmotion}. For video continuation, Keypoint-DiT enables the creation of longer, continuous, and natural pose sequences from a single initial pose, facilitating video continuation with free-flowing motion. 
For pose transfer, a pose adapter is introduced to align initial poses, which are subsequently refined by the Keypoint-DiT, thereby improving the accuracy of facial and hand details. In pose transfer, transitional frames are utilized for refinement to bridge the pose gap between the reference image and the guided poses.

In summary, our contributions are as follows: 1) We present HumanDiT, the first DiT-based human body animation framework employing a prefix-latent strategy, which facilitates variable resolutions, dynamic sequence lengths, and high-fidelity video generation. 2) We develop a 14,000-hour in-the-wild video dataset with a structured data pipeline and a lightweight clarity scoring model to filter for high-quality video samples. 3) We introduce a DiT-based pose generation module and pose adapter, enabling HumanDiT to support diverse applications such as long-form video continuation and pose transfer. 4) We achieve superior quantitative and qualitative performance across various scenes, outperforming state-of-the-art methods in comprehensive evaluations.

% \begin{itemize}[]
%     \item To address variable resolution and dynamic sequence length, we replace the traditional U-Net diffusion model with the scalable Diffusion Transformer (DiT)~\cite{peebles2023scalable}. Employing a prefix-latent reference strategy, HumanDiT supports diverse resolutions and durations while preserving visual consistency across inputs. The proposed Pose Guider facilitates the capture of temporal and spatial features via patch-based extraction, ensuring precise pose guidance. This DiT architecture enables sequence parallelism, optimizing training and inference for high-resolution, long-duration video generation. % dit model (temporal consistency, multi reso)
%     \item To enhance generalization and rendering quality, we gather a large-scale, diverse dataset comprising 14,000 hours of in-the-wild videos. This dataset is collected using a novel data processing pipeline, which includes a scoring model to filter samples based on image clarity, particularly in detail-rich regions such as hands and teeth, thereby improving dataset reliability. We also incorporate text masks into the training process to prevent the model from learning text artifacts present in the wild data.  % large scale dataset (Generalization,accurate rendering)
%     \item In comparison to previous studies, HumanDiT not only delivers enhanced rendering quality but also supports a wide range of downstream applications, such as long-form video continuation for pose generation and pose transfer in complex scenarios. For pose extension, a DiT-based pose generation module is employed to explicitly generate human motion from a single pose. For pose transfer, a Pose Adapter is introduced to align initial poses, which are subsequently refined by the pose generation module, thereby improving the accuracy of facial and hand details. Furthermore, transitional frames are utilized for refinement to bridge the pose gap between the reference image and the guided poses. %Transitional frames are used to bridge alignment gaps, ensuring seamless transitions between reference images and target poses.  % pose gene and transfer
% \end{itemize}

 %Scalability is achieved through a Diffusion Transformer (DiT)-based architecture that supports flexible model scaling, complemented by training on a high-quality, large-scale dataset with 14,000 hours of in-the-wild videos. To collect the dataset, we introduce a data processing pipeline and incorporate a scoring model that filters samples based on image clarity, focusing on detail-rich regions like hands and teeth, which significantly enhances the reliability of the dataset. We also incorporate text masks into training to prevent the model from learning text artifacts from wild data. To generate videos with visual consistency, reference networks is replaced with a compact prefix-latent reference strategy. With the proposed Pose Guider, HumanDiT captures both temporal and spatial features through patch-based extraction, enabling effective pose guidance. Moreover, the DiT design and Pose Guider architecture make sequence parallelism possible, optimizing training and inference for longer sequences and producing coherent, long-duration, high-resolution videos.

% Furthermore, to prevent text artifacts from dynamic subtitles, a text mask is applied during training. 
%Furthermore, for pose extension, a DiT-based pose generation module explicitly generates human motion based on a single pose. For pose transfer, we introduce a Pose Adapter that aligns initial poses, which are then refined by the pose generation module to improve the accuracy of facial and hand details. Transitional frames are used to bridge alignment gaps, ensuring seamless transitions between reference images and target poses. By integrating these advancements, HumanDiT consistently outperforms existing methods across tasks such as video reconstruction, continuation, pose transfer for open-set test images. Our key contributions are as follows: 1) We present the first pose-guide, scalable DiT-based human body animation framework, HumanDiT, capable of generating high-fidelity, long-form videos at variable resolutions through a prefix-latent reference strategy. 2) We introduce the Pose Adapter and keypoint generation module to enable robust long-video generation across diverse scenarios, even from a single wild image. 3) We develop a large-scale human video dataset with a structured data pipeline and a lightweight clarity scoring model to filter high-quality video samples, which significantly enhances the modelâ€™s generalization and performance.


% To address the aforementioned challenges in human motion video generation, we develop a scalable body animation framework capable of diverse resolution and long form generation, called HumanDiT. To enhance the accurate rendering of human in diverse situation, we construct a high-quality, large-scale dataset containing 14,000 hours of wild videos and introduce a scoring model specifically designed for image quality of teeth and hand regions, which helps filter low-quality samples. For maintaining visual consistency, we employ Diffusion Transformer (DiT)~\cite{peebles2023scalable} as our backbone, replacing the commonly used reference networks with a prefix-latent reference strategy. The DiT-based diffusion network also has capability of generating diverse size of videos. To capture pose guidance information, the Pose Guider use patch-based extraction to capture both temporal and spatial features of the pose images.  Moreover, the design of DiT and Pose Guider enables to utilize sequence parallelism, which allows for efficient training and inference on longer sequences. This makes it possible to generate more coherent, long-duration, and high-resolution video sequences.  During the training process, to prevent dynamic subtitles from interfering with the generated results, a text mask is introduced. To further reduce the reliance on extensive input conditions, a DiT-based pose generation module is introduced to extend the pose sequence and generate human videos based on the input image explicitly. To address pose alignment issues when driving the reference image with a template pose sequence, we employ a pose alignment strategy where the poses are initially aligned by the Pose Adapter and further refined by the pose generation module, enhancing the accuracy of facial and hand poses. Additionally, transitional frames for refinement are employed to bridge the pose gap between the reference image and the guided pose.

% By integrating these improvements, HumanDiT achieves superior results compared to existing methods. We validate the exceptional performance of HumanDiT across various settings, including video reconstruction, video continuation, template matching, and its one-shot video generation capability for open-set test images. We summarize our technical contributions as follows:1) We propose the first DiT-based human body animation framework that does not rely on a reference net, capable of generating high-quality, diverse-resolution, and variable-length human-driven videos. 2) We collect a large-scale human video dataset, and introduce a lightweight clarity score model to effectively filter high-quality videos. 3) Benefiting from our proposed Pose Adapter and keypoint generation modules, our model can handle data across various scenarios, achieving impressive results even in one-shot settings.
