\section{Related Works}
\label{sec:relate}


\noindent\textbf{Image and Video Generation.} Recent years, image and video generation has made substantial advancements~\cite{vondrick2016generating, ge2022long, bar2023multidiffusion, zeng2024make}, beginning with early approaches based on GANs~\cite{goodfellow2014generative, chan2019everybody, ren2020deep, siarohin2019animating, siarohin2021motion} and VQVAE-based transformers~\cite{ge2022long, xu2016msr, sun2024autoregressive}, which laid the groundwork for synthesizing realistic videos. However, these methods often struggled with issues like temporal inconsistency and the high computational demands associated with modeling continuous motion. More recently, diffusion models (DMs)~\cite{song2020denoising, ho2020denoising, ni2023conditional} have emerged as a powerful alternative, offering greater stability and control. Unlike GANs, diffusion models progressively refine noisy inputs into coherent frames, leading to more robust and photo-realistic outputs. Latent Diffusion Models (LDM)~\cite{rombach2022high}, in particular, have optimized this process by performing it in lower-dimensional latent spaces. %, significantly reducing computational complexity while enhancing efficiency.

For video generation, these diffusion models~\cite{guo2023animatediff, bar2024lumiere} go beyond image synthesis by incorporating temporal layers and attention mechanisms to better model spatial-temporal relationships. To maintain temporal continuity, some approaches~\cite{singer2022make,blattmann2023stable, zhou2022magicvideo, wang2023dreamvideo} directly extend the 2D U-Net~\cite{ronneberger2015u}, pre-trained on text-to-image tasks into 3D. Techniques like Stable Video Diffusion (SVD)~\cite{blattmann2023stable} and AnimateDiff~\cite{guo2023animatediff} represent notable developments, as they extend 2D image generation frameworks to handle the additional complexity of continuous video segments. Recent methodologies~\cite{xu2024easyanimate, yang2024cogvideox, openai2024sora, zhang2024tora} have shown enhanced potential by integrating specialized motion modules and a DiT-based framework~\cite{peebles2023scalable}. Concurrently, 3D Variational Autoencoders (VAEs)~\cite{kingma2013auto} alleviate the computational demands of video data processing through advanced compression techniques. % These advancements have not only improved the efficiency of video generation but also enabled finer control over complex scenes, such as generating realistic human motions or more dynamic environments.

\noindent\textbf{Human Animation.} Pose-guided human image animation has seen significant advancements, particularly with the incorporation of pose estimation methods such as OpenPose~\cite{cao2017realtime}, DWpose~\cite{yang2023effective} and Sapiens~\cite{khirodkar2024sapiens} for guiding motion synthesis. Early approaches~\cite{liao2020speech2video, weng2022humannerf, ginosar2019learning, ye2023geneface}, often based on GANs~\cite{goodfellow2014generative} or NeRFs~\cite{mildenhall2021nerf}, focused on transferring poses between images using explicit skeleton representations, while these struggled with temporal consistency and flexibility in motion transfer. DisCo~\cite{wang2024disco} integrates character features using CLIP and incorporates background features through ControlNet~\cite{zhang2023adding}.

The rise of diffusion models (DMs) has greatly improved the quality of both image and video generation. Recent methods, like MagicAnimate~\cite{xu2024magicanimate} and Animate Anyone~\cite{hu2024animate}, introduce specialized motion modules and lightweight pose guiders to ensure accurate pose-to-motion transfer. Champ~\cite{zhu2024champ} relies on parametric models SMPL~\cite{loper2023smpl}, which offer realistic human representations and serve as ground truth for pose and shape analysis. UniAnimate~\cite{wang2024unianimate} utilizes first frame conditioned input for consistent long video generation. Mimicmotion~\cite{zhang2024mimicmotion} employs cross-frame overlapped diffusion to generate extended animated videos. Xue~\textit{et al.}~\cite{xue2024follow} utilize an optical flow guide to stabilize backgrounds and a depth guide to handle occlusions between body parts. Cyberhost~\cite{lin2024cyberhost} enhances hand and face generation with a Region Codebook Attention mechanism. Moreover,  Tango~\cite{liu2024tango} synthesizes co-speech body-gesture videos by retrieving matching reference video clips and leveraging a diffusion-based interpolation network. Animate-X~\cite{tan2024animate} extends beyond human to anthropomorphic
characters with various body structures by utilizing implicit and explicit pose indicator.

For precise rendering of hands and faces, HandRefiner~\cite{lu2024handrefiner} employs the ControlNet module to correct distorted hands. ShowMaker~\cite{yangshowmaker} utilizes latent features aligned with facial structures for enhancement. RealisDance~\cite{zhou2024realisdance} and TALK-Act~\cite{guan2024talk} leverage 3D priors of hands and faces, providing accurate 3D or depth information as conditioning inputs.
% By decoupling identity and motion through a reference network and pose guide, these approaches achieve visual coherence and enable dynamic human motion synthesis.