\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{section}{0}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thesection}{\Alph{section}}


\section{More Details for Data Collection}
\label{sec:sup_data}
% score model

\subsection{Clarity Scoring Model}

In our video clarity assessment, we manually annotate 150,000 image sets, each comprising five images of teeth and hands extracted from various videos. These images are ranked based on clarity, occlusion level, and detail quality of the target region. The dataset is divided into training and testing subsets with a 9:1 ratio. We develop a lightweight clarity scoring model using ResNet-18~\cite{he2016deep} as the backbone, with output scores derived via a sigmoid function. During training, we randomly select image pairs from each set that differed by at least 2 points in clarity rank and apply MarginRankingLoss~\cite{liu2019multi} to compute the loss between these paired scores. The models for teeth and hand clarity are trained over 100 epochs, achieving test accuracies of 79.8\% and 83.5\%, respectively. These clarity scores are then used to filter high-quality videos, thereby enhancing the dataset's visual quality. Data samples and results are presented in Figure~\ref{fig:scoremodel}.
\begin{figure}[htp]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/score_filter.pdf}
    \vspace{-0.25in}
    \caption{The data samples and model results (score) of hand and teeth dataset. Ranks are the annotated clarity ascending orders. Scores are obtained from our clarity scoring model, respectively. }
    \vspace{-0.1in}
    \label{fig:scoremodel}
\end{figure} 

\subsection{Data Extraction and Preprocessing}
% pipeline
We developed a robust data extraction and filtering pipeline to efficiently isolate and retain high-quality human action videos from extensive in-the-wild datasets. This pipeline ensures the selection of high-clarity, visually consistent video segments appropriate for training and evaluation.

The processing of videos begins with human tracking followed by video cropping. Using YOLO~\cite{Jocher_Ultralytics_YOLO_2023}, we detect and track each human sequence, ensuring each segment contains a single subject. The cropping dimensions for each segment are dynamically adjusted based on the subject’s maximum motion range, preserving full-body visibility and enabling accurate pose analysis. The body masks can also be obtained by YOLO.

\subsection{Feature Extraction}
After identifying and segmenting each individual in the video, various key features were extracted, including human poses, background keypoints, and text bounding boxes.

\noindent1) \textbf{Pose Estimation.} Sapiens~\cite{khirodkar2024sapiens} is employed to obtain pose keypoints, providing robust human pose detection for each frame. Notably, to reduce redundant keypoints and avoid affecting the performance, some keypoints, such as the ears, are excluded from our model.

\noindent2) \textbf{Background Tracking} CoTracker~\cite{karaev2023cotracker} is applied to distinguish foreground movements from background changes, capturing the necessary spatial consistency for high-quality generation.

\noindent3) \textbf{Text Detection.} PaddleOCR~\cite{PaddleOCR} is used to identify and mark text regions in each frame, mitigating the potential interference of text artifacts with the generated data.


To ensure the inclusion of only high-quality frames, we evaluate each video segment at 5-frame intervals using both Q-Align~\cite{wu2023qalign} and custom scoring models. These regions of hand and teeth are obtained from the estimated keypoints. Frames with poor visibility or occlusion of these regions are excluded. This clarity scoring process ensures the final dataset contains samples with clearly visible and well-defined features. Furthermore, image histograms are computed to detect frames with excessive blur, visual transitions, or other quality degradations, further refining the visual consistency across samples.

\subsection{Filtering Criteria}
To ensure the high quality of the extracted videos, we implemented the following measures.

\noindent\textbf{Pose Quality.} A threshold of $0.3$ is set for the relative speed of keypoints to ensure reliable localization of key body parts. The relative speed is determined by measuring the displacement velocity of one end of the human skeleton with respect to the other.

\noindent\textbf{Frame Count and Visibility Constraints.} We filter segments with fewer than $30$ frames and require at least $80\%$ visibility of the upper body and hands across frames.

\noindent\textbf{Visual Quality.} A minimum average IQA score of $60$ (via Q-Align) is required to retain acceptable visual quality, with all frames exceeding $45$ to avoid frames with intermittent blur. We set minimum scores of $0.50$ for average hand clarity and $0.40$ for average teeth clarity to select videos with high detail quality. Cap the histogram variance at $0.3$ to exclude frames that are low-quality, excessively blurred, or contain numerous transitions.

\noindent\textbf{Motion Analysis.} To prevent abrupt scene transitions, the maximum speed threshold across joints is set at $0.3$, and the maximum mask Intersection over Union (IoU) threshold is set at 0.7 between frames to ensure smooth, continuous motion frames.

This meticulously crafted pipeline enables the selection of high-quality video segments distinguished by consistent motion, visual clarity, and pose alignment, thus providing a reliable dataset for robust human motion generation tasks.


\section{More Details for Keypoint-DiT}
\label{sec:sup_posegeneration}

Keypoint-DiT, similar to Video DiT, uses the pose from the first frame as a prefix pose and denoises the subsequent $t-1$ noise frames to generate the following motions. To equip the Keypoint-DiT module with the ability to refine and transition between poses in pose transfer tasks, we incorporate reference keypoints as conditional inputs embedded within the token sequences. This setup allows the model to learn smooth transitions between poses by gradually moving from an initial reference pose to a target reference pose.

To simulate the transition from a reference image pose to a desired target pose, we use keypoints extracted from real human motion data as target references. To encourage effective transition capabilities, the initial $\tau$ frames preceding the target pose are set to zero, allowing the model to progressively refine its generated keypoints and interpolate smoothly between states. During training, there is a 50\% probability that the GT keypoints are provided as conditional inputs in each frame; in the remaining cases, zero-padding is used. This helps the model to generalize by learning to generate realistic transitions both with and without explicit keypoint guidance. Our collected dataset, sampled at 25 fps, is used to train the model with consistent temporal and spatial resolution across frames. All input keypoints are normalized to a square image space (ranging from 0 to 1) to unify the model’s understanding of body proportions and to ensure a consistent spatial reference for each pose. The training process was conducted on 8 H100 GPUs over 1 million steps with a learning rate of 0.0001, allowing the model to achieve high fidelity in both pose transfer and motion generation across variable input conditions.

\section{User Study}

To evaluate the quality of our approach in comparison to state-of-the-art methods, we conducted a blind user study focused on three primary metrics: temporal consistency, identity preservation, and visual quality. Each of these aspects is critical in human motion video generation:

\noindent\textbf{Temporal Consistency.} This metric evaluates the smoothness of transitions between frames, ensuring there are no abrupt changes or flickering artifacts that disrupt motion flow in the video. A high temporal consistency score indicates that the video maintains natural, fluid movement without noticeable jumps.
  
\noindent\textbf{Identity Preservation.} This measures how well the generated video retains the unique characteristics of the subject (such as facial and bodily features) and the background scene across frames. Effective identity preservation ensures that the person and setting remain visually coherent and true to the original throughout the video sequence.

\noindent\textbf{Visual Quality.} This aspect assesses the overall image clarity and absence of blurriness or visual artifacts. High visual quality indicates that the video maintains sharpness, fine details, and avoids pixelation or degradation that could affect the realism of the generated video.

In this study, we curated a set of 30 videos for tasks involving video reconstruction from the first frame and pose transfer. Ten domain experts from diverse industries participated in the evaluation. Each expert was randomly presented with pairs of anonymized videos, generated by our method and existing methods, for direct comparison across three metrics.

\section{More Results}
\label{sec:moreres}

\subsection{Quantitative Evaluation}

\begin{table*}[htb]
\centering\small
\begin{tabular}{c|c|ccccc|cc}
\Xhline{1pt}
\noindent\textbf{Model}        & \noindent\textbf{Dataset} & \noindent\textbf{FID}  $\downarrow$   & \noindent\textbf{SSIM}   $\uparrow$ & \noindent\textbf{PSNR}   $\uparrow$  & \noindent\textbf{LPIPS}  $\downarrow$  & \noindent\textbf{L1}    $\downarrow$  & \noindent\textbf{FID-VID}  $\downarrow$  & \noindent\textbf{FVD}  $\downarrow$   \\ \hline
\multirow{4}{*}{DisCo~\cite{wang2024disco}}  & TikTok              & 131 & 0.664              & 13.9& 0.396              & 5.18E-05   & 125        & 858\\ 
       & Talking & 229 & 0.619              & 12.7& 0.454              & 7.05E-05   & 229        & 1848              \\ 
       & Dancing              & 219 & 0.660              & 12.8& 0.431              & 5.14E-05     & 195      & 2414              \\ 
       & \cellcolor[HTML]{F0F0F0} Total             &\cellcolor[HTML]{F0F0F0} 210 &\cellcolor[HTML]{F0F0F0} 0.636              & \cellcolor[HTML]{F0F0F0} 12.9&\cellcolor[HTML]{F0F0F0} 0.439             &\cellcolor[HTML]{F0F0F0} 6.29E-05     &\cellcolor[HTML]{F0F0F0} 203      &\cellcolor[HTML]{F0F0F0} 1808             \\ \hline
\multirow{4}{*}{MagicDance~\cite{chang2023magicdance}}            & TikTok              & 71.8& 0.748              & 17.4& 0.318              & 3.19E-05     & 65.8     & 437\\ 
       & Talking & 106 & 0.719              & 17.6& 0.325              & 3.78E-05  &  97.7       & 874\\ 
       & Dancing              & 117  & 0.740    & 16.8& 0.340              & 2.68E-05  &  103       & 1394              \\ 
       &\cellcolor[HTML]{F0F0F0} Total &\cellcolor[HTML]{F0F0F0} 103 &\cellcolor[HTML]{F0F0F0} 0.728              &\cellcolor[HTML]{F0F0F0} 17.4&\cellcolor[HTML]{F0F0F0} 0.327              & \cellcolor[HTML]{F0F0F0} 3.43E-05  &\cellcolor[HTML]{F0F0F0} 93.4         &\cellcolor[HTML]{F0F0F0} 918\\ \hline
\multirow{4}{*}{AnimateAnyone~\cite{hu2024animate}}          & TikTok              & 60.7        & 0.734    & 17.4  & 0.287              & 3.07E-05    & 60.0       & 453\\ 
       & Talking & 75.8& 0.755    & 19.0  & 0.254              & 2.79E-05        & 64.9   & 677\\ 
       & Dancing              & 102 & 0.697              & 16.6& 0.328              & 2.63E-05 & 84.3 & 1060              \\ 
       &\cellcolor[HTML]{F0F0F0} Total &\cellcolor[HTML]{F0F0F0} 79.6& \cellcolor[HTML]{F0F0F0} 0.737    &\cellcolor[HTML]{F0F0F0} \cellcolor[HTML]{F0F0F0} 18.1&\cellcolor[HTML]{F0F0F0} 0.279              &\cellcolor[HTML]{F0F0F0} 2.80E-05        &\cellcolor[HTML]{F0F0F0} 68.8   &\cellcolor[HTML]{F0F0F0} 730\\ \hline
\multirow{4}{*}{MimicMotion~\cite{zhang2024mimicmotion}}           & TikTok              & 58.8  & 0.761    & 18.4  & 0.253  & 3.22E-05 & 35.2 & 323   \\ 
       & Talking & 56.3  & 0.794    & 21.5  & 0.215  & 2.27E-05 & 36.1 & 440   \\ 
       & Dancing              & 94.3& 0.737              & 17.8& 0.286              & 3.73E-05   & 51.6        & 607\\ 
       &\cellcolor[HTML]{F0F0F0} Total &\cellcolor[HTML]{F0F0F0} 65.4& \cellcolor[HTML]{F0F0F0} 0.776              &\cellcolor[HTML]{F0F0F0} 20.1&\cellcolor[HTML]{F0F0F0} 0.238              & \cellcolor[HTML]{F0F0F0} 2.76E-05  &\cellcolor[HTML]{F0F0F0} 39.5         &\cellcolor[HTML]{F0F0F0} 458\\ \hline
\multirow{4}{*}{Ours}            & TikTok              & 41.6 & 0.815              & 20.5 & 0.220             & 2.17E-05     & 24.3     & 237\\ 
       & Talking & 41.3 & 0.862              & 24.6 & 0.155              & 1.40E-05  &  18.2       & 257\\ 
       & Dancing              & 76.3  & 0.791    & 19.7 & 0.240              & 2.26E-05  &  45.4       & 548              \\ 
       & \cellcolor[HTML]{D9D9D9} Total& \cellcolor[HTML]{D9D9D9} 49.4 & \cellcolor[HTML]{D9D9D9} 0.838              & \cellcolor[HTML]{D9D9D9} 22.8  & \cellcolor[HTML]{D9D9D9} 0.186            & \cellcolor[HTML]{D9D9D9} 1.73E-05  & \cellcolor[HTML]{D9D9D9} 25.5         & \cellcolor[HTML]{D9D9D9} 320\\ \Xhline{1pt}

\end{tabular}
\vspace{-0.1in}
\caption{Quantitative comparison on the TikTok dataset~\cite{jafarian2021learning} and our self-collected talking and dancing test datasets with existing pose-guided body reenactment methods. }
\vspace{-0.1in}
\label{tab:metrics-total}
\end{table*}

Figure~\ref{tab:metrics-total} illustrates the performance of various methods across different scenarios, including the TikTok~\cite{jafarian2021learning} test set and our collected datasets of speech and dance videos. The dance dataset presents significant challenges due to its extensive motion amplitudes, in contrast to the speech dataset, which generally involves smaller movements and fewer variations, primarily concentrating on facial movements. The TikTok test set is comparatively simpler and fails to adequately assess each model's generalization capability in managing complex dance generation.

\subsection{More Visualization Results}

\begin{figure*}[htp]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/sup_gene.pdf}
    \vspace{-0.25in}
    \caption{More visualization results of video continuation.}
    \vspace{-0.25in}
    \label{fig:sup_gene}
\end{figure*}


\noindent\textbf{Video Continuation.} In Figure~\ref{fig:sup_gene}, we illustrate the video continuation capability of HumanDiT, showcasing its ability to generate realistic and coherent videos by extending motions based on the poses of given images across diverse scenarios. Specifically, our method excels in both full-body motion scenarios, such as dynamic dance sequences, and upper-body-focused tasks, such as speech gestures. These distinct tasks highlight the flexibility of HumanDiT to adapt to varying motion ranges and visual contexts while maintaining temporal consistency, identity preservation, and high rendering quality. 

\section{Limitations and Future Work.}

Despite promising results, HumanDiT has several limitations that can be addressed in future work. While the model excels in pose-guided human motion video generation, challenges remain in handling extreme body proportion variations and intricate hand and facial movements, which may affect visual consistency when the reference image and input pose are misaligned. Additionally, although HumanDiT generates long-form videos using a prefix-latent strategy, multi-batch inference may still result in slight error propagation, making it challenging for subsequent batches to fully preserve the identity characteristics of the first frame. Furthermore, generating videos at higher resolutions or longer durations still incurs high computational costs. Future work could explore more efficient architectures or hardware-accelerated optimizations to mitigate this issue. Lastly, the current pose generation model is limited by its inability to generate poses from images, speech, or textual descriptions. We plan to expand the model’s capabilities to incorporate these input modalities, enabling more dynamic pose generation based on a wider range of input features.