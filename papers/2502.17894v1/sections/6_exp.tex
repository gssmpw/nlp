\section{Experiments}

\subsection{Setup}
\textbf{Simulation Setup.} We train and evaluate our method in IsaacGym and design various scene layouts using our UniVoxGen, which are categorized into three difficulty levels (see appendix~\ref{Generated_scene} for the details):\
\begin{itemize}
    \item \textit{Easy}: In this layout, there are no obstacles in front of the target object, making it straightforward to extract the target.
    \item \textit{Medium}: The target object is partially occluded by obstacles in front, with the occluded area covering less than 50\% of the target's surface.
    \item \textit{Hard}: The occluded area covers more than 50\% of the target's surface, and the nearest obstacle is very close to the target, with a gap of less than 5 mm between them.
\end{itemize}

During the training phase, we abstract both the target object and obstacles as boxes. This simplifies scene generation and policy training while also reducing the Sim2Real gap~\cite{zhang2021sim2real}. It is important to note that this simplification does not affect the generalization ability of our 3D vision policy to different objects. This is because the occupancy prediction focuses on whether each grid point in the 3D voxel space is occupied, rather than the overall geometric shape of the objects. We have verified this approach in real-world experiments. We used approximately 200 different-sized boxes within an 85cm × 28cm area to generate around 1 million scenes, with the number of boxes per scene ranging from 3 to 12.



\textbf{Real-world Setup.} In the real-world experiments, we use the Felxiv Rizon 4S robotic arm. Two cameras, the Intel D435i and Intel D415i, serve as input for our 3D vision policy from different perspectives, with the Intel D435i fixed at the end effector of the robotic arm. We conduct experiments across three difficulty levels—Easy, Medium, and Hard—on a total of about 30 different scenes and 40 different retail items. Detailed experimental results can be found in Table.~\ref{tab:real_world}. The proposed 3D vision policy exhibited real-time operational performance, achieving a consistent inference rate of 10+ frames per second (fps) on an NVIDIA GeForce RTX 4090 GPU.






\subsection{Implementation details}

Our state-based policy is trained on 1 million cluttered scenes, with an equal distribution of easy, medium, and hard scenes (1:1:1 ratio). We then collect approximately 100k expert trajectories for imitation learning.

The proposed 3D vision policy was developed through a two-stage training paradigm utilizing 24 NVIDIA GeForce RTX 4090 GPUs. During the initial stage, the perception module undergoes pretraining on a camera-based 3D semantic occupancy prediction task, consuming approximately 500,000 annotated scenes over 50 training epochs. The architecture incorporates dual visual perspectives: an ego-centric perspective from the robotic wrist-mounted camera and an exocentric observer perspective spatially separated from the robotic arm. During the occupancy prediction, we achieve a spatial resolution of 0.5 cm per voxel. In the subsequent stage, we freeze the pretrained perceptual representations and focus on training the decision module using 100,000 expert demonstration trajectories (comprising 500,000 state-action pairs) across 100 optimization epochs. For systematic evaluation, both baseline methods and our proposed policy undergo rigorous testing on a curated dataset of 3,000 previously unseen scenarios, maintaining an equal distribution across difficulty levels (Easy/Medium/Hard) consistent with training protocols.


\subsection{Baselines and Evaluation Metrics}
\textbf{Baselines.} We use the following methods as comparison baselines for our 3D vision policy:
\begin{itemize}
    \item \textit{Oracle}: This refers to the state-based policy trained using privileged information.
    \item \textit{Heuristic}: As a simple method for object extraction, the heuristic motion first lifts the target object by the height of the front barrier. Then, it directly extracts the object horizontally.
    \item \textit{Collision-based motion planning}: We use CuRobo~\cite{sundaralingam2023curobo} and AIT*~\cite{strub2020adaptively} as the collision-based motion planning methods. The collision-based approach works well when a collision-free path exists. However, in cluttered scenarios, such as when obstacles are in close contact with the target or when only collisions enable extraction, it struggles to find a safe path.
    \item \textit{Learning-based method}: We use Vanilla and DP3~\cite{ze20243d} as learning-based methods baseline. Vanilla is a simplified version of our 3D vision policy. In this approach, the depth map predicted by DepthAnything is directly fed into a ResNet network to extract features, which are then passed to the Diffusion Policy for action prediction. In the single-view setup, only the wrist perspective is used, while in the double-view setup, both the wrist and third-person perspectives are utilized.

\end{itemize}

\textbf{Metrics.} In our evaluation process, we utilize various metrics to gauge the quality of our results. We need to consider both the task completion (i.e., whether the target $O_{target}$ is successfully retrieved to the target goal) and the impact $E$ on obstacles $O_{obstacle}$. Therefore, we design two types of metrics:
\begin{itemize}
    \item \textit{$E_{trans}$ and $E_{rot}$}: $E_{trans}$ and $E_{rot}$ represent the total displacement and rotation of obstacles in the environment, respectively.
    \item \textit{Success Rate}: Success rate (SR) indicates whether the target object can be successfully retrieved to the target goal, while satisfying certain constraints. In this work, we define the constraint as the total displacement of surrounding obstacles not exceeding 3 \textit{cm}.
\end{itemize}

It is important to note that due to the challenges of accurately measuring translation error ($E_{trans}$) and rotation error ($E_{rot}$) in real-world environments, we use only the success rate as the evaluation metric in the real-world experiments.




\begin{table*}[htbp]
\centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccc|ccc|ccc|ccc} 
    \toprule
    Scene Level                          & \multicolumn{3}{c|}{Easy}                             & \multicolumn{3}{c|}{Medium}                           & \multicolumn{3}{c}{Hard}         & \multicolumn{3}{c}{Avg.}                        \\ 
    \midrule
    Method                               & SR (\%)$\uparrow$ & $E_{trans}$ (cm) $\downarrow$ & $E_{rot}$ (rad)$\downarrow$ &SR (\%)$\uparrow$ & $E_{trans}$ (cm)$\downarrow$ & $E_{rot}$ (rad)$\downarrow$ & SR (\%)$\uparrow$ & $E_{trans}$ (cm)$\downarrow$ & $E_{rot}$ (rad)$\downarrow$  & SR (\%)$\uparrow$ & $E_{trans}$ (cm)$\downarrow$ & $E_{rot}$ (rad)$\downarrow$  \\ 
    \midrule
     Oracle                               &        99.72\%           &     0.17             &  0.03              &   \textbf{90.30\%}                &   \textbf{0.17}               &    \textbf{0.25}            &     \textbf{66.79\%}              &   \textbf{4.08}               &      \textbf{0.59}  &     \textbf{85.60\%}              &   \textbf{1.47}               &      \textbf{0.29}           \\ 
    Heuristic                            &        \textbf{100.00\%}           &       \textbf{0.00}           &    \textbf{0.00}            &         51.03\%          &      6.01            &     0.94           &      13.91\%             &     14.13             &    1.77   &      54.98\%          &      6.71            &     0.91           \\
    CuRobo                               &  98.95\%  & 0.31 & 0.05 & 73.55\%  & 4.56 & 0.54 & 32.28\%  & 11.03 & 1.31 & 68.26\%  & 3.96 & 0.63               \\
    AIT*                                &      95.46\%  & 0.44 & 0.07 & 65.39\%  & 5.51 & 0.79 & 26.72\%  & 11.62 & 1.39 & 62.52\%  & 5.86 & 0.75        \\
    \midrule
    DP3                                  &        92.51\%           &      1.35            &   0.08             &   77.80\%                &   4.20               &     0.48           &     46.80\%              &      7.94            &    1.07  &     72.37\%              &      4.49            &    0.54            \\
    % RVT2                                 &    TODO               &                  &                &                   &                  &                &                   &                  &                 \\
    Vanilla (Single View)                   &      87.90\%             &    2.54              &    0.19            &     68.40\%              &      5.76            &     0.73           &               29.34\%       &    11.22           &     1.37     &              61.88\%       &    6.51           &     0.76       \\
    Vanilla (Double View)                   &      89.30\%             &    2.15              &    0.18            &     67.88\%              &     5.49             &    0.67            &              23.83\%     &      12.08            &    1.42     &              60.33\%     &      6.57            &    0.75             \\
    \rowcolor[rgb]{0.8,0.949,0.961} Ours  &  \textbf{96.45\%}            &    \textbf{0.87}           &  \textbf{0.08}           &    \textbf{86.31\%}               &     \textbf{2.35}             &     \textbf{0.33}           &   \textbf{61.63\%}                &    \textbf{5.12}              &    \textbf{0.72}   &   \textbf{81.46\%}                &    \textbf{2.78}              &    \textbf{0.36}           \\
    \bottomrule
    \end{tabular}
}

\caption{Compares the performance of different methods across three difficulty levels (Easy, Medium, Hard) in terms of Success Rate (SR), translation error ($E_{trans}$), and rotation error ($E_{rot}$). Our method outperforms all baselines (except for oracle), achieving the highest success rate and the lowest errors in translation and rotation.}  
\label{tab:main}
\end{table*}

\subsection{Main Results} 
As shown in Table.~\ref{tab:main}, we evaluated the performance of our method against several baselines, including oracle, heuristic, collision-based motion planning, and learning-based approaches such as DP3 and Vanilla. Our method consistently outperforms the baselines (except the oracle, which is our state-based policy) across all difficulty levels (Easy, Medium, and Hard) in terms of success rate (SR), translation error ($E_{trans}$), and rotation error ($E_{rot}$). Specifically, our method achieves the highest average SR of 81.46\%, significantly outperforming both Vanilla and DP3, particularly in harder scenarios. While the oracle achieves near-perfect performance, it relies on privileged information, making it less applicable to real-world scenarios. Heuristic, although effective in simpler environments, shows a sharp decline in performance as scene complexity increases. 
Among the collision-based motion planning methods, CuRobo and AIT* demonstrated commendable performance in Easy and Medium scenarios, but both experienced a sharp decline in Hard scenarios. This is primarily because collision-based motion planning methods tend to fail in the presence of collisions, as they often encounter unsolvable situations leading to planning failures. 
Among the learning-based vision approaches, DP3 performs reasonably well but still falls short compared to our method, especially in handling more complex scenes with occlusion and tight object arrangements. Overall, our approach achieves high success rates and low error metrics, demonstrating superior robustness and generalization across varying levels of difficulty. 

\begin{table}[htbp]
\centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccc}
    \toprule
    \textbf{ROI Size (cm)} & \textbf{Success Rate (\%) $\uparrow$} & \textbf{$E_{trans}$ (cm)$\downarrow$} & \textbf{$E_{rot}$ (rad)$\downarrow$} \\
    \midrule
   \rowcolor[rgb]{0.8,0.949,0.961}  $20\times 20\times 30$                &         \textbf{81.46\%}        &  \textbf{2.78}         &   \textbf{0.36}                        \\
    $40\times 80\times 30$                &       76.93\%           &       3.24                   &        0.43                 \\
    $60\times 150\times 30$               &      74.63\%            &       3.67                     &      0.47                   \\
    \bottomrule

    \end{tabular}
}
\caption{ Ablation study on Region of Interest (ROI) size shows that the smallest ROI ($20 \times 20 \times 30$ cm) achieves the best results, with the highest success rate and lowest errors. Larger ROIs decrease performance due to irrelevant information from distant areas, highlighting the benefit of smaller, localized ROIs for improved accuracy and efficiency.}
\label{tab:roi}

\end{table}


\subsection{Ablation Study} 

\textbf{Ablation Study on Region of Interest (ROI) Size.} Focusing solely on the region of interest is an effective approach that enhances the policy’s generalization ability and aids policy learning. In the ablation study on the Region of Interest (ROI) size (Table.~\ref{tab:roi}), we investigate the impact of varying ROI sizes on policy performance. The results show that smaller ROIs, such as $20 \times 20 \times 30$, lead to the best performance, achieving the highest success rate (81.46\%) and the lowest translation (2.78 cm) and rotation (0.36 rad) errors. As the ROI size increases, both the success rate and accuracy metrics decrease, with larger ROIs introducing more irrelevant information from distant areas of the scene, which in turn reduces the model's ability to focus on the target region and make less precise predictions. These findings highlight the advantage of using small, localized ROIs, which not only improve policy performance but also enhance generalization performance, making the network more robust to scene variations. This approach also accelerates policy inference while maintaining high policy performance, especially in cluttered environments.


\begin{table}[htbp!]
\centering  
\resizebox{\linewidth}{!}{
    
    \begin{tabular}{l|ccc} 
    \toprule
    Dataset Size                        & \textbf{Success Rate (\%) $\uparrow$} & \textbf{$E_{trans}$ (cm)$\downarrow$} & \textbf{$E_{rot}$ (rad)$\downarrow$}  \\ 
    \midrule
    500                                 &       62.33\%       &    6.94         &     0.75      \\
    5,000                                &       70.72\%       &     5.08        &    0.55       \\
    50,000                               &       72.50\%       &     4.57        &    0.51       \\
   \rowcolor[rgb]{0.8,0.949,0.961}  500,000                              &      \textbf{81.46\%}        &  \textbf{2.78}         &   \textbf{0.36}        \\ 
    w/o pre-train                       &       77.90\%       &     3.59        &  0.42         \\
    \bottomrule
    \end{tabular}
}
\caption{ Scaling the training data for occupancy pre-train while maintaining policy training data size.}
\label{tab:occ_scaling}
\end{table}

% \begin{figure}[tb]
% % \vspace{-0.4cm}
%     \centering
%     \includegraphics[trim=0.cm 0cm 0cm 0.0cm, clip,  width=1.0\linewidth]{figures/exp/occ_scaling.pdf}
%     \caption{ Scaling the training data for occupancy pre-train while maintaining policy training data size.}
%     \label{fig:6_1}
%     % \vspace{-0.6cm}
% \end{figure}


% \begin{figure}[tb]
% % \vspace{-0.4cm}
%     \centering
%     \includegraphics[trim=0.cm 0cm 0cm 0.0cm, clip,  width=1.0\linewidth]{figures/exp/policy_scaling.pdf}
%     \caption{ Scaling the training data for policy while maintaining occupancy pre-training data size.}
%     \label{fig:6_2}
%     % \vspace{-0.6cm}
% \end{figure}


\begin{table}
\centering  
\resizebox{\linewidth}{!}{
    
    \begin{tabular}{l|ccc} 
    \toprule
    Dataset Size                        & \textbf{Success Rate (\%) $\uparrow$} & \textbf{$E_{trans}$ (cm)$\downarrow$} & \textbf{$E_{rot}$ (rad)$\downarrow$}  \\ 
    \midrule
    500                                 &   48.23\%           &   9.42          &   1.07        \\
    5,000                                &    54.66\%          &   7.99          &   0.88        \\
    50,000                              &  65.55\%            &    4.67         &   0.62        \\
    \rowcolor[rgb]{0.8,0.949,0.961} 500,000                              &           \textbf{81.46\%}       & \textbf{2.78}         &   \textbf{0.36}          \\ 
    \bottomrule
    \end{tabular}
}
\caption{ Scaling the training data for policy while maintaining occupancy pre-training data size. }
\label{tab:policy_scaling}
\end{table}

\textbf{Ablation Study on Scaling the Training Data.} In our ablation study on scaling the training data, we examine its impact on both the occupancy pre-training (Table.~\ref{tab:occ_scaling}) and the policy training (Table.~\ref{tab:policy_scaling}). For occupancy pre-training, we find a clear correlation between larger datasets and improved performance in policy performance. Starting with 500 scenes, the success rate is 62.33\%, accompanied by relatively high translation (6.94 cm) and rotation (0.75 rad) errors. As the dataset size increases to 5,000 and 50k, the success rate improves to 70.72\% and 72.50\%, respectively, with corresponding reductions in translation and rotation errors. The largest dataset, with 500k scenes, achieves the best performance, reaching a success rate of 81.46\% and reducing translation and rotation errors to 2.78 cm and 0.36 rad, respectively. 
This demonstrates that pre-training on larger datasets significantly enhances the policy performance, providing a more comprehensive understanding of the 3D scene.

Similarly, in the policy training ablation study, we observe a similar trend. With a dataset of 500 state-action pairs, the success rate is 48.23\%, with higher translation (9.42 cm) and rotation (1.07 rad) errors. As the dataset increases to 5,000 and 50k, the success rate rises to 54.66\% and 65.55\%, respectively, with reductions in translation and rotation errors. The largest dataset of 500k state-action pairs yields the best performance, achieving a success rate of 81.46\% and reducing translation and rotation errors to 2.78 cm and 0.36 rad. These results highlight that increasing the training data size for the decision module significantly improves task success and reduces errors, emphasizing the importance of a sufficiently large dataset for accurate and reliable policy performance.

Overall, our findings highlight the critical role of large-scale data in both occupancy pre-training and policy training, underscoring the importance of scaling the dataset for improving the overall performance and accuracy of the 3D vision policy.



\begin{table}
\centering
\resizebox{\linewidth}{!}{
    
    \begin{tabular}{l|ccc} 
    \toprule
    Representation      & \textbf{Success Rate (\%) $\uparrow$} & \textbf{$E_{trans}$ (cm)$\downarrow$} & \textbf{$E_{rot}$ (rad)$\downarrow$}   \\ 
    \midrule
    Oracle       & 85.77\% & 0.95 & 0.02  \\
    \hline
    Point Cloud         & 71.70\% & 4.54  & 0.55  \\
    RGB              & 70.35\% & 4.97 & 0.58  \\
    Depth              & 71.44\% & 4.49 & 0.56  \\
    Pred Depth         & 61.21\% & 6.54 & 0.76  \\ 
    % \hline
    \rowcolor[rgb]{0.8,0.949,0.961} \textbf{Ours} &  \textbf{81.46\%}        &  \textbf{2.78}           &   \textbf{0.36}   \\
    \bottomrule
    \end{tabular}
    }
\caption{ Ablation on 3D representations. We replace the visual observation and the corresponding encoder in our 3D vision policy to evaluate different 3D representations.}
\label{tab:3D-rep}
\end{table}

\textbf{Ablation Study on 3D representations.} In our ablation study on different 3D representations (in Table.~\ref{tab:3D-rep}), we compare the performance of the 3D vision policy using various input types: oracle, point cloud, RGB, depth, predicted depth, and our proposed occupancy prediction. The oracle representation, which utilizes privileged information, achieves the best performance, with a success rate of 85.77\% and the lowest translation (0.95 cm) and rotation (0.02 rad) errors. Among the non-privileged representations, point cloud input yields a success rate of 71.70\%, while RGB and raw depth inputs show similar performance, with success rates of 70.35\% and 71.44\%, respectively. Predicted depth, derived from DepthAnything~\cite{yang2024depth}, performs the worst with a success rate of 61.21\%, suggesting that relying solely on predicted depth data reduces accuracy.

RGB, depth, and predicted depth all use the same multi-view input setup as our occupancy prediction method, which includes both a wrist perspective and a third-person view. However, the feature fusion strategy used for these inputs—simply concatenating features along the channel dimension—fails to fully exploit the rich 3D information available from multiple views. This simplistic fusion approach does not adequately capture the spatial relationships and depth cues crucial for accurate scene understanding. In contrast, our occupancy prediction method leverages a more sophisticated approach to integrate the spatial relationships across views, making full use of the multi-view information. This allows our model to more effectively extract and combine the 3D context, leading to significantly improved performance with a success rate of 81.46\%, and lower translation (2.78 cm) and rotation (0.36 rad) errors.

These results highlight the importance of a more nuanced feature fusion strategy and demonstrate that occupancy prediction provides a more robust and accurate representation. By effectively utilizing all available 3D information, our method significantly outperforms the alternatives in guiding the policy.


\begin{table}
\centering
\resizebox{0.6\linewidth}{!}{

    \begin{tabular}{l|ccc} 
    \toprule
    Method         & Easy & Medium & Hard   \\ 
     \midrule
    
    Heuristic     &      10/10       &    2/10         &     0/10      \\
     \rowcolor[rgb]{0.8,0.949,0.961} \textbf{Ours} &     10/10         &     8/10        &     6/10      \\
    \bottomrule
    \end{tabular}
}
\caption{Success Rate in Real-word }
\label{tab:real_world}
\end{table}


\subsection{Evaluate in Real-world}
As shown in Table.~\ref{tab:real_world}, we compare the performance of our method against a heuristic approach across three difficulty levels: Easy, Medium, and Hard. The heuristic method achieves perfect success in the Easy scenario (10/10), but its performance deteriorates significantly in the Medium (2/10) and Hard (0/10) scenarios, where it struggles with partial and full occlusions. In contrast, our method demonstrates robust performance across all difficulty levels, achieving a success rate of 10/10 in the Easy scenario, 8/10 in the Medium scenario, and 6/10 in the Hard scenario. These results highlight that our 3D vision policy, which leverages occupancy prediction, consistently outperforms the heuristic approach, particularly in more challenging, occluded environments. This underscores the effectiveness and generalization capability of our method, making it a more reliable solution for object extraction tasks in real-world, cluttered scenarios.





