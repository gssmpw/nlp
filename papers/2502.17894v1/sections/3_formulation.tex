\section{Problem Formulation}
\label{formulation}
\subsection{Camera-based 3D Semantic Occupancy Prediction}
Given a set of images captured from multiple viewpoints, camera-based 3D semantic occupancy prediction aims to generate a semantically annotated voxel grid within the robot’s operational workspace. Specifically, we input multi-camera images $I = \{I^1, I^2, \dots, I^N\}$, and the prediction model $\Theta$ will output a semantic voxel volume $\mathbf{Y} \in \{v_0, v_1, \dots, v_C\}^{H \times W \times Z}$. Here, $N$ represents the number of camera viewpoints, $C$ denotes the total number of semantic categories in the scene, $v_0$ signifies an empty voxel and $H$, $W$, $Z$ correspond to the height, width, and depth dimensions of the voxel volume, respectively. In this work, we will use camera-based 3D semantic occupancy prediction as a synergy to pre-train our vision encoder.
\subsection{Problem Definition}
% As mentioned above, both the approach and grasping stages in shelf fetching are already well-solved using a suction cup. Therefore.
Our work primarily focuses on ensuring the safety of the extraction process during object fetching. Specifically, in the shelf environment, there is a target object $O_{target}$, movable obstacles $O_{obstacle}$ that surround the target, and a fixed shelf $O_{shelf}$. Our goal is to use the suction cup to extract the target object $O_{target}$ while minimizing the impact $E$ on surrounding obstacles $O_{obstacle}$ and avoiding contact with the shelf’s barriers $O_{shelf}$. We need to utilize a closed-loop vision policy $\pi_{vision}$ that takes real-world observable inputs $\mathcal{O}$ to complete this task. In this work, the inputs include double-view RGB images $I = \{ I^{1}, I^{2} \}$ and $q^{target}$, which represents the relative pose of the end-effector with respect to the target point, i.e., $\mathcal{O} = \{I, q^{target}\}$. By feeding the observations into the vision policy $\pi_{vision}$, whose vision encoder is trained using a 3D semantic occupancy prediction auxiliary task, the policy outputs an action $a = \{a_{trans}, a_{rot}\}$, where $a_{trans} \in \mathbb{R}^{3}$ denotes the relative translation, and $a_{{rot}} \in SO(3)$ represents the relative rotation, expressed using the axis-angle. Through the iterative execution of the \textit{observation-decision-execution} process in a closed-loop manner, the vision policy $\pi_{vision}$ can enable the target object $O_{target}$ to reach the target point while minimizing the impact $E$ on the surrounding environment as much as possible.



