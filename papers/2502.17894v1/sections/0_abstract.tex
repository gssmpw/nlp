
\begin{abstract}


% In grocery store environments, shelf fetching—the task of approaching, grasping, and retrieving objects from cluttered and upright shelves—presents a compelling application scenario by automating monotonous, repetitive tasks and enhancing daily operational efficiency. Currently, the research community lacks datasets specifically tailored to this task, as creating simulation data that accurately replicates real supermarket environments is extremely challenging. Additionally, existing fetch methods have primarily been developed for Cluttered Stacking Fetch (CSF), where objects are typically extracted from an open, unbounded space, often placed on a tabletop. In contrast, our approach tackles a different and more complex challenge: Cluttered Enclosure Fetch (CEF), where obstacles surround the upright target object in a semi-enclosed setting.In this paper, we introduce ShelfFetch, a novel approach specifically designed to address the shelf fetching task in grocery store environments. Our methodology begins with UniVoxGen, an efficient voxel-based scene generation framework that creates the necessary training scenarios. Subsequently, we employ reinforcement learning (RL) to collect expert data through the RL agent's interaction with the environment, enabling it to learn dynamic-aware behaviors while minimizing its impact. Finally, we implement a vision-based occupancy method that offers a unified view representation, enhancing the integration of multi-view information. This voxel-based 3D data serves as a robust structural prior, facilitating the efficient learning of 6-DoF actions.


% Cluttered scenes are common in our daily lives, and for robots, the ability to fetch objects from these cluttered environments is a vital skill. On the one hand, existing work mainly focuses on object fetching in table-top cluttered scenes; on the other hand, the cluttered scenes in these studies lack realism. To address these limitations, we propose ReShFetch, a novel framework specifically designed to handle object fetching from more complex and realistic cluttered environments within shelf settings. First, we introduce UniVoxGen, an efficient voxel-based scene generator tailored to create realistic cluttered environments in shelf scenarios. Next, we leverage reinforcement learning (RL) to gather expert data through the interaction of the RL agent with these generated scenes. Finally, we introduce OccPolicy, a novel visual imitation learning (IL) method that utilizes a unified view representation derived from occupancy, thereby significantly improving the integration of multi-view information. This 3D representation acts as a robust structural prior, significantly improving the efficiency of learning 6-DoF actions. Our experiments show that OccPolicy achieves a xx\% improvement in performance, approaching or even matching the performance of expert RL agents, highlighting its potential to significantly enhance the practical applicability of robotic fetching in complex, real-world cluttered shelf environments.

% Cluttered scenes are common in our daily lives, and for robots, the ability to fetch objects from these cluttered environments is a vital skill. On the one hand, existing work mainly focuses on object fetching in table-top cluttered scenes; on the other hand, the cluttered scenes in these studies lack realism. To address these limitations, we propose ReShFetch, a novel framework specifically designed to handle object fetching from more complex and realistic cluttered environments within shelf settings. First, we introduce UniVoxGen, an efficient voxel-based scene generator tailored to create realistic cluttered environments in shelf scenarios. Next, we apply reinforcement learning (RL) to cultivate the agent's dynamic awareness through interactions with the generated scenes, thereby collecting expert data in complex environments. Finally, we introduce OccPolicy, a novel visual imitation learning (IL) method that utilizes a unified view representation derived from occupancy, thereby significantly improving the integration of multi-view information. This 3D representation acts as a robust structural prior, significantly improving the efficiency of learning 6-DoF actions. Our experiments show that OccPolicy achieves a xx\% improvement in performance, approaching or even matching the performance of expert RL agents, highlighting its potential to significantly enhance the practical applicability of robotic fetching in complex, real-world cluttered shelf environments.(rewrite)

% A robot capable of fetching products from shelves in retail environments is highly appealing, as it can replace humans in performing repetitive labor tasks. However, replicating human-level fetching capabilities in robots presents many challenges. Unlike bin-picking, where the impact on surrounding items is not a major concern during the lifting process, shelf fetching requires not only retrieving the target item but also minimizing the disturbances to nearby objects in order to ensure safety. As shelf environments in retail stores are often highly cluttered, neglecting safety could lead to destructive consequences, such as fragile items falling. This is critical for real-world applications but has been under-explored in shelf fetching. In light of this, we introduce FetchBot, which aims to fill this gap by leveraging synthetic data in a data-driven manner, considering its potential for generalization, low cost, and efficient data collection inherent in such approaches. To address the Sim2Real challenge of using synthetic data, we have designed a well-structured Sim2Real pipeline that integrates UniVoxGen, an efficient voxel-based scene generation method, dynamic-aware RL for collecting expert trajectories, and the capabilities of foundation models along with unified multi-view 3D representations. By fully leveraging the potential of each component, we can ultimately achieve zero-shot Sim2Real transfer. Both simulation and real-robot experiments demonstrate our remarkable generalizability, which is capable of generalizing to diverse scene layouts, various product textures, and different object geometries.



% A robot capable of fetching objects from shelves in retail environments is highly appealing, as it can assist humans with repetitive labor tasks. However, replicating human-level fetching capabilities in robots is challenging due to the variety of products, cluttered environments, and the critical need for safety during retrieval. The safety aspect is crucial for practical applications but remains under-explored in the field. To address this, we introduce FetchBot, which tackles the issue using a well-structured Sim2Real approach, capitalizing on its potential for generalization, low cost, and efficient data collection. More concretely, the diversity of synthetic scenes serves as the foundation for Sim2Real, motivating us to propose UniVoxGen, an efficient voxel-based scene generation method. With these generated scenes in place, the quality of expert data becomes critical for the subsequent imitation learning. To achieve this, we employ a well-designed dynamic-aware RL that leverages privileged information to collect high-quality expert trajectories. Furthermore, the capabilities of foundation models are harnessed to strengthen the policy's generalization ability, while a unified view representation is integrated to enhance the integration of multi-view information and further bridge the sim2real gap. By fully exploiting the potential of each component, we ultimately achieve zero-shot Sim2Real transfer. Both simulation and real-robot experiments showcase our remarkable generalization ability, particularly in handling a broad range of real-world scenarios, including diverse scene layouts, varying shelf heights, and different object geometries.

% Object fetching from shelves is common in daily life, and enabling robots to learn and master this skill could significantly assist humans, alleviating them from labor-intensive tasks. However, directly training robots for object fetching in real-world environments presents several challenges, including data scarcity, difficulty in data collection, and potential safety risks. Training with synthetic data in simulation offers a promising alternative, as it bypasses these issues. Nonetheless, the significant domain gap between simulation and reality remains a major obstacle to achieving effective Sim2Real transfer for object fetching. To tackle this challenge, we propose FetchBot, a framework designed for Sim2Real object fetching. This framework takes a multi-faceted approach, including but not limited to better scens generation and data collection, unified input for the policy, unified view representation and pretraining, achieving zero-shot Sim2Real transfer in cluttered shelf environments. Specifically, it employs an efficient voxel-based scene generation method to facilitate large-scale scene creation, and a dynamic-aware reinforcement learning approach to collect high-quality expert trajectories. Moreover, we leverage foundation models to bridge the domain gap between simulated and real-world inputs. Lastly, we learn a unified view representation from multi-perspective views by pretraining on 3D panoptic segmentation, enhancing the policy's 3D scene understanding and enabling effective generalization to real-world camera perspectives. Our extensive evaluations demonstrate the framework's remarkable generalization capability and impressive zero-shot Sim2Real performance.

% The ability to fetch objects from cluttered shelves is essential for future robots to assist humans in various scenarios, particularly in retail environments. However, in practical retail settings, fetching objects requires minimizing disturbances to surrounding items, i.e., ensuring safety, which poses significant challenges due to varying shelf layouts, diverse attached objects, limited perception fields, and other complexities. To address this problem, we propose SafeFetching, a novel framework for learning a general closed-loop vision-based fetching policy using synthetic data. Specifically, to overcome data scarcity, we introduce a large-scale dataset featuring diverse scenes generated by UniVoxGen, an efficient voxel-based scene generation method, and expert trajectories collected through a well-designed dynamic-aware RL agent. Drawing inspiration from unsupervised domain adaptation, which leverages intermediate domains to enhance cross-domain generalization, we utilize the intermediate domain of foundation models to mitigate the inherent sim-to-real gap associated with synthetic data. Additionally, to address limited perception fields, we employ a multi-view approach to capture richer scene details, complemented by a unified view representation that integrates information across views for comprehensive 3D scene understanding. Both simulation and real-robot experiments showcase our remarkable gener alization ability, particularly in handling a broad range of real-world scenarios, including diverse scene layouts, varying shelf heights, and different object geometries.

% Addressing the labor shortage, robots capable of safely fetching objects from cluttered lateral-access shelves remain underexplored. Unlike overhead-access settings, lateral-access environments impose unique challenges, such as restricted motion, limited perception, and increased collision risks, which make conventional approaches less effective. \jaylon{may be first two sentecens use simpler words?} To address this gap, we introduce FetchBot, a zero-shot Sim2Real framework designed for generalizable, safety-aware shelf fetching. FetchBot overcomes the challenges of lateral-access scenarios through three key modules: simulation-based data generation, domain style transfer\jaylon{use better description other than sudden strange-concept}, and multi-view policy distillation. Specifically, for synthetic data generation, we employ an efficient voxel-based method, UniVoxGen\jaylon{need to be defined first.}, to create large-scale scene datasets, complemented by a dynamic-aware reinforcement learning (RL) policy for collecting expert trajectory data. To bridge the inherent Sim2Real gap in synthetic data, we leverage a foundation model for domain style transfer\jaylon{same problem}, aligning the simulation and real-world inputs. Lastly, recognizing the limitations of single-view perspectives, we learn a unified view representation from multi-perspective inputs by pretraining on 3D panoptic segmentation. This unified representation enhances the policy’s 3D scene understanding, thereby enabling more effective generalization to real-world camera perspectives. \weiheng{problem in logic} Both simulation and real-robot experiments validate the superior generalization capability of FetchBot, demonstrating its robustness across a wide range of real-world scenarios, including diverse scene layouts and objects with varying geometries and dimensions.

% Both simulation and real-robot experiments demonstrate FetchBot's superior generalization ability, particularly in handling a broad range of real-world scenarios, including diverse scene layouts and objects with varying geometries and dimensions.

% Fetching objects from cluttered shelves is essential for future robots to assist humans in various scenarios. In real-world environments, ensuring safety during this task is critical, yet it presents significant challenges due to restricted motion, limited fields of view, and the risk of collisions with surrounding items. To address these challenges, we propose FetchBot, a zero-shot sim-to-real framework designed for generalizable, safety-aware shelf fetching. It consists of two key modules: synthetic data generation and multi-view policy distillation. Specifically, to overcome data scarcity, we introduce a large-scale synthetic dataset featuring diverse scenes generated by UniVoxGen, an efficient voxel-based scene generation method, along with expert trajectories collected by a dynamics-aware RL agent. Furthermore, to address the limited perspective of a single view, we employ a multi-view policy distillation strategy that integrates RGB information from multiple views into a unified 3D representation, enhancing 3D scene understanding. Given the inherent sim-to-real gap in our synthetic data-driven approach, we leverage the capabilities of foundation models to align simulation and real-world inputs, ensuring robust cross-domain perception. Both simulation and real-robot experiments demonstrate FetchBot's superior generalization ability, particularly in handling a broad range of real-world scenarios, including diverse scene layouts and objects with varying geometries and dimensions.

% Fetching objects from cluttered shelves is essential for future robots to assist humans in various scenarios. In real-world environments, ensuring safety during this task is critical, yet it presents significant challenges due to restricted motion space, limited fields of view, and the risk of collisions with surrounding items. To address these challenges, we propose FetchBot, a zero-shot sim-to-real framework designed for generalizable, safety-aware shelf fetching. It consists of two key modules: synthetic data generation and multi-view policy distillation. Specifically, to overcome data scarcity, we introduce a large-scale synthetic dataset featuring diverse scenes generated by UniVoxGen, an efficient voxel-based scene generation method, along with expert trajectories collected by a dynamics-aware RL agent. To distill a policy that can be deployed in real-world from synthetic data, we leverage the capabilities of foundation models to align simulation and real-world inputs, ensuring robust cross-domain perception. Building on this, we employ a multi-view policy distillation strategy that integrates information from multiple views into a unified 3D representation, overcoming the limited perspective of a single view and enhancing 3D scene understanding. Both simulation and real-robot experiments demonstrate FetchBot's superior generalization ability, particularly in handling a broad range of real-world scenarios, including diverse scene layouts and objects with varying geometries and dimensions.

% Object fetching from cluttered shelves is an important capability for robots to assist humans in real-world scenarios. For this task, safe robotic behaviors that require the minimum contact with surrounding objects during this task are critical but highly challenging due to restricted motion space, limited fields of view, and the risk of collisions with surrounding items. In this paper, we propose FetchBot, a sim-to-real framework designed for achieving zero-shot generalizable safety-aware object fetching from cluttered shelves in the real world. To overcome data scarcity, we propose an efficient voxel-based method to generate diverse scenes with cluttered shelves in simulation at scale and train a dynamics-aware RL policy to generate object fetching in these scenes. We further distill this RL policy that leveraging oracle information to a visual policy network so that it can be applied in the real-world applications. Considering that sim-to-real discrepancies stem from texture variations mostly while from geometric dimensions rarely, we propose to adopt depth information estimated by full-fledged depth foundation models as the input for the vision-based policy network. To tackle the challenge of limited views in this task, we design a novel architecture to learn multi-view representations, enabling comprehensive encoding of cluttered shelf scenes. This enables FetchBot to learn collision avoidance and minimization when fetching objects from various positions and depths on cluttered shelves, ensuring robust and safety-aware operation. Both simulation and real-robot experiments demonstrate FetchBot's superior generalization ability, particularly in handling a broad range of real-world scenarios, including diverse scene layouts and objects with varying geometries and dimensions.

 %
\blfootnote{* Core contributors with equal contributions. $\dagger$ Corresponding authors (e-mail: zhangzz@galbot.com, hewang@pku.edu.cn).}%
Object fetching from cluttered shelves is an important capability for robots to assist humans in real-world scenarios. Achieving this task demands robotic behaviors that prioritize safety by minimizing disturbances to surrounding objects—an essential but highly challenging requirement due to restricted motion space, limited fields of view, and complex object dynamics. In this paper, we introduce FetchBot, a sim-to-real framework designed to enable zero-shot generalizable and safety-aware object fetching from cluttered shelves in real-world settings. To address data scarcity, we propose an efficient voxel-based method for generating diverse simulated cluttered shelf scenes at scale and train a dynamics-aware reinforcement learning (RL) policy to generate object fetching trajectories within these scenes. This RL policy, which leverages oracle information, is subsequently distilled into a vision-based policy for real-world deployment.
Considering that sim-to-real discrepancies stem from texture variations mostly while from geometric dimensions rarely, we propose to adopt depth information estimated by full-fledged depth foundation models as the input for the vision-based policy to mitigate sim-to-real gap. To tackle the challenge of limited views, we design a novel architecture for learning multi-view representations, allowing for comprehensive encoding of cluttered shelf scenes. This enables FetchBot to effectively minimize collisions while fetching objects from varying positions and depths, ensuring robust and safety-aware operation. Both simulation and real-robot experiments demonstrate FetchBot's superior generalization ability, particularly in handling a broad range of real-world scenarios, including diverse scene layouts and objects with varying geometries and dimensions.

% need to talk about the advantage of using suction cup

% Object fetching from cluttered shelves is an important capability for robots to assist humans in real-world scenarios. Achieving this task requires the development of safe robotic behaviors that minimize contact with surrounding objects, a requirement that is both essential and highly challenging due to the constraints of limited motion space, restricted fields of view, and the need for high precision. In this paper, we introduce FetchBot, a sim-to-real framework designed to achieve zero-shot generalizable and safety object fetching from cluttered shelves in real-world settings.
% To address data scarcity, we propose an efficient voxel-based method for generating diverse simulated cluttered shelf scenes at scale, enabling the training of a dynamics-aware reinforcement learning (RL) policy for object fetching. This RL policy, which leverages oracle information during training, is subsequently distilled into a vision-based policy network for real-world deployment. To bridge the sim-to-real gap caused by significant domain differences between RGB inputs in simulation and reality, we utilize depth foundation models to convert both real and simulated RGB inputs into predicted depth maps, providing a unified input representation across domains.
% To overcome the challenge of limited viewpoints, we design a novel architecture for learning multi-view representations, allowing for comprehensive encoding of cluttered shelf scenes. This enables FetchBot to effectively minimize collisions while fetching objects from varying positions and depths, ensuring robust and safe operation. Extensive simulation and real-world experiments demonstrate FetchBot's superior generalization capabilities, successfully handling diverse scenarios with varied shelf layouts and objects of different geometries and dimensions.
\end{abstract}
