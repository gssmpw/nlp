\section{Related Works}


\textbf{Large-scale Cluttered Scene Generation.} Automatic scene generation has been extensively studied in computer vision and graphics \cite{wang2021sceneformer,chang2015text,chang2014learning, ritchie2019fast,paschalidou2021atiss,para2023cofs,feng2024layoutgpt,bautista2022gaudi,fu20213d}. However, the structured nature of the scenes generated by these methods makes them unsuitable for object fetching tasks in clutters. To address this, some studies \cite{wada2022safepicking, li2024broadcasting, xu2023joint, qian2024thinkgrasp,yang2024ground4act,wang2024learning} have employed simple strategies, such as simulating objects dropped from the air to create cluttered layouts. While effective for disorder, these methods often result in unstable and unrealistic scenes. Recent efforts \cite{jia2024cluttergen,zhou2024scenex,dalal2024neural,han2024fetchbench,murali2023cabinet,chamzas2021motionbenchmaker} have aimed to generate more realistic cluttered environments. For example, ClutterGen \cite{jia2024cluttergen} uses reinforcement learning with physics-based rewards to guide scene generation but is limited by its training object set and lacks generalization. Similarly, works \cite{dalal2024neural,han2024fetchbench,murali2023cabinet,chamzas2021motionbenchmaker,fishman2024avoid} such as Neural MP \cite{dalal2024neural}, employ procedural scene generation methods to create cluttered scenarios. These approaches typically rely on complex collision detection mechanisms in simulation to verify scene validity, which significantly limits their efficiency. Moreover, they often support only a single scene generation rule, further constraining the diversity of generated scenes. In response, we propose a novel approach that directly performs scene generation and collision detection in voxel space, eliminating the need for traditional simulations and significantly improving efficiency. By incorporating diverse generation rules, we created a large-scale dataset of 1 million cluttered scenes, offering a valuable resource for studying object fetching in clutters.

\textbf{Robotic Fetching from Cluttered Scenes.} Robotic fetching (grasping) has long been recognized as a fundamental challenge in robotic manipulation, drawing extensive research efforts over the years \cite{newbury2023deep}. A cornerstone in this area is picking objects from cluttered scenes \cite{correll2016analysis,eppner2016lessons,mahler2017learning,mahler2019learning,yu2016summary,li2023sim,murray2024learning,yang2023dynamo,atar2024optigrasp,han2024fetchbench,zhang2024gamma,wang2024quadwbg}, with two primary tasks standing out: bin-picking, which involves vertically lifting objects from cluttered bins \cite{correll2016analysis,eppner2016lessons,mahler2017learning,mahler2019learning,yu2016summary,li2023sim}, and shelf-picking, which entails horizontally extracting items from occluded shelves \cite{murray2024learning,yang2023dynamo,atar2024optigrasp,han2024fetchbench}. Many previous studies on shelf-picking \cite{murray2024learning,yang2023dynamo,atar2024optigrasp} have primarily concentrated on grasp point detection, often overlooking the critical retrieval stage. This stage requires minimizing disturbance to surrounding objects, as even slight collisions can cause nearby items to fall or topple others. Recently, Fetchbench~\cite{han2024fetchbench} has started addressing the challenges of object fetching from shelves, focusing on more complex aspects of shelf-picking. However, the shelf environments in this work do not accurately reflect real-world conditions. Our approach employs a closed-loop vision-based policy to ensure safe retrieval during the fetching process, minimizing the impact on the surrounding environment.

% \textbf{3D Imitation Learning.} Imitation learning enables robots to acquire skills by observing and mimicking expert demonstrations. 3D-based methods have shown promise in visual imitation learning but may face challenges: reliance on keyframe-based approaches, limiting performance in high-dimensional tasks; slow inference due to the high computational cost of 3D feature maps; and difficulty achieving consistent 3D representations through simple multi-view fusion.


% View fusion, which integrates features from different perspectives into a unified view, plays a critical role in robotics. Occupancy-based representations offer an efficient way to achieve this unified view, and have already seen widespread application in the field of autonomous driving. In robot learning, both RVT and VIHE aim to unify different perspectives. However, their viewpoint transformations depend on real depth and remain perspective-based, causing a mismatch between visual features and the action space.In our work, an occupancy-based imitation learning strategy will enable more efficient and effective robot learning.

% \textbf{Sim2Real Transfer for Visuomotor Policies.}
\textbf{Sim2Real Transfer for 3D Visuomotor Policies.} Currently, there are many 3D-based imitation learning policies \cite{shridhar2023perceiver,goyal2023rvt,gervet2023act3d,ze20243d,ke20243d,xian2023chaineddiffuser,liu2024voxact} that utilize 3D observation data to mimic expert actions from demonstrations. However, these methods predominantly rely on real-world data to perform real-robot tasks, failing to fully leverage the potential of simulators. As a result, sim-to-real transfer for 3D visuomotor policies remains an under-explored topic. Most previous works \cite{lyu2024scissorbot,xie2023part,qin2023dexpoint,wang2025mobileh2r} have employed point clouds as representations to achieve sim-to-real, but they still struggle to bridge the sim-to-real gap due to noise and inaccuracies, particularly at object edges and reflective surfaces in real-world point clouds captured by depth sensors. To further advance the field of sim-to-real research, inspired by approaches in autonomous driving \cite{wang2024panoocc,cao2022monoscene,wei2023surroundocc}, we propose using a unified 3D representation to ensure consistency in multi-view image fusion and minimize the sim-to-real gap. Through this unified representation and other sim-to-real strategies, our method can successfully achieve sim-to-real transfer.
