\clearpage
\onecolumn
\setcounter{page}{1}

\section{APPENDIX}

\subsection{Hyper-parameters in State-Based Policy Trainig}
During state-based policy training, our reward function is a combination of $r_{task}, r_{impact}, r_{constr}$. 

The robot behavior constrain reward $r_{constr}$ consists of $r_{vel}, r_{ang}, r_{penetration}, r_{pose}$. The $r_{vel}$ and $r_{ang}$ limit the end-effector's linear velocity and rotational velocity to a threshold. $r_{{penetration}}$ prevents the end-effector and the target object from colliding with the fixed shelf, as such a collision would lead to significant linear and rotational acceleration due to penetration with the shelf. $r_{pose}$ encourages the robot to maintain a kinematically feasible state.

The impact reward $r_{impact}$ consists of ${r_{trans\_step}, r_{rot\_step}, r_{trans\_termi}, r_{rot\_termi}}$. The step rewards $r_{trans\_step}, r_{rot\_step}$  penalize the actions taken in each step based on the total translation $E_{trans}$ and rotation $E_{rot}$ within that step. The termination rewards ${r_{trans\_termi}, r_{rot\_termi}}$ penalize the entire extraction process based on the total translation $E_{trans}$ and rotation $E_{rot}$ over the entire process.

The task reward $r_{task}$ corresponds to successfully fetching the target object, and the total translation $E_{trans}$ and rotation $E_{rot}$ during the entire fetching process must satisfy : $E_{trans} < \sigma_{trans}, E_{rot} < \sigma_{rot}$. We use a $\sigma$ curriculum to to guide the learning process. Once the policy saturates in the current curriculum, we increase the difficulty. Specifically, we set the $\sigma_{trans}$ to $[0.03, 0.015, 0.01, 0.005, 0]$, and we set $\sigma_{rot}$ to $[0.4, 0.2, 0.16, 0.1, 0]$.

We combine the above rewards with weights listed in Table~\ref{tab:rewards}.
\begin{table}
\centering
\resizebox{0.3\linewidth}{!}{

    \begin{tabular}{l|ccc} 
    \toprule
    Hyper-parameters         & Values    \\ 
     \midrule
    
    $\lambda_{task}$     &      1.0        \\
    $\lambda_{trans\_step}$     &      -0.5        \\
    $\lambda_{rot\_step}$     &      -0.5        \\
    $\lambda_{trans\_termi}$     &      -1.0        \\
    $\lambda_{rot\_termi}$     &      -1.0        \\
    $\lambda_{vel}$     &      -0.5        \\
    $\lambda_{ang}$     &      -0.3        \\
    $\lambda_{pose}$     &      -0.1        \\
    $\lambda_{penetration}$     &      -1.0        \\
    \bottomrule
    \end{tabular}
}
\caption{Hyper-parameters for the reward function.}
\label{tab:rewards}
\end{table}

We train our oracle policy with PPO, and the training hyper-parameters are shown in Table~\ref{tab:rl}.

\begin{table}
\centering
\resizebox{0.3\linewidth}{!}{

    \begin{tabular}{l|ccc} 
    \toprule
    Hyper-parameters         & Values    \\ 
     \midrule
    
    Num. envs     &      1024        \\
    Num. steps for per update     &      24        \\
    Num. minibatches     &      4        \\
    Num. learning epochs     &      1500        \\
    learning rate     &      0.0003        \\
    clip range     &      0.2        \\
    entropy coefficient     &      0.0        \\
    kl threshold     &      0.02        \\
    max gradient norm     &     1.0        \\
    $\lambda$   & 0.95\\
    $\gamma$ & 0.99\\
    \bottomrule
    \end{tabular}
}
\caption{Hyper-parameters for the oracle policy learning.}
\label{tab:rl}
\end{table}

\subsection{Generated scenes by UniVoxGen}
\label{Generated_scene}
We use six carefully hand-designed rules to generate realistic shelf layouts using UniVoxGen. The scene is divided into three difficulty levels. The following are the generated scenes displayed both in the simulation and voxel space.

\begin{figure*}[tb]
% \vspace{-0.4cm}
    \centering
    \includegraphics[trim=0.2cm 3cm 14cm 0.0cm, clip,  width=1.0\linewidth]{figures/sup/sup_easy.pdf}
    \caption{Easy scenes, the first and third columns show the scenes in the simulation, while the second and fourth columns display the scenes generated by UniVoxGen.}
    \label{fig:sup_easy}
    % \vspace{-0.6cm}
\end{figure*}


\begin{figure*}[tb]
% \vspace{-0.4cm}
    \centering
    \includegraphics[trim=0.2cm 3cm 14cm 0.0cm, clip,  width=1.0\linewidth]{figures/sup/sup_medium.pdf}
    \caption{Medium scenes, the first and third columns show the scenes in the simulation, while the second and fourth columns display the scenes generated by UniVoxGen.}
    \label{fig:sup_medium}
    % \vspace{-0.6cm}
\end{figure*}


\begin{figure*}[tb]
% \vspace{-0.4cm}
    \centering
    \includegraphics[trim=0.2cm 3cm 14cm 0.0cm, clip,  width=1.0\linewidth]{figures/sup/sup_hard.pdf}
    \caption{Hard scenes, the first and third columns show the scenes in the simulation, while the second and fourth columns display the scenes generated by UniVoxGen.}
    \label{fig:sup_hard}
    % \vspace{-0.6cm}
\end{figure*}


\subsection{Items in Real-World Experiments}
We used approximately 40 items with various shapes for the real-world experiments, as shown in Figure~\ref{fig:sup_obj}. These items encompass a wide range of objects commonly found in retail environments, including boxed items, bottled objects, bagged goods, and fragile items, among others.
\begin{figure*}[tb]
% \vspace{-0.4cm}
    \centering
    \includegraphics[trim=0.2cm 1cm 5cm 0.0cm, clip,  width=1.0\linewidth]{figures/sup/sup_obj.pdf}
    \caption{Items in Real-World Experiments}
    \label{fig:sup_obj}
    % \vspace{-0.6cm}
\end{figure*}


