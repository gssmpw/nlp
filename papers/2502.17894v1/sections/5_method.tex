\section{Method}

\begin{figure*}[tb]
% \vspace{-0.4cm}
    \centering
    \includegraphics[trim=0.2cm 2cm 2cm 0.0cm, clip,  width=1.0\linewidth]{figures/method/5_1.pdf}
    \caption{\textbf{Our Proposed Framework.} In the (a) Sim Data Generation stage, we use UniVoxGen to generate a diverse set of scenes and employ a dynamics-aware RL policy to collect expert trajectories. In the (b) Pre-training stage, we first leverage a foundation model to mitigate the sim-to-real gap, then introduce an occupancy prediction task to learn a voxel-based representation. This task encourages the network to preserve essential geometric information and develop a comprehensive understanding of the scene. In the (c) Policy Training stage, we distill these expert trajectories into a vision-based policy through imitation learning. After training, the vision-based policy can achieve (d) zero-shot sim-to-real.}
    \label{fig:5_1}
    % \vspace{-0.6cm}
\end{figure*}


% Object fetching from cluttered shelves is an important capability for robots to assist humans in real-world scenarios. Achieving this task demands robotic behaviors that prioritize safety by minimizing disturbances to surrounding objects—an essential but highly challenging requirement due to restricted motion space, limited fields of view, and complex object dynamics. In this paper, we introduce FetchBot, a sim-to-real framework designed to enable zero-shot generalizable and safety-aware object fetching from cluttered shelves in real-world settings. To address data scarcity, we propose an efficient voxel-based method for generating diverse simulated cluttered shelf scenes at scale and train a dynamics-aware reinforcement learning (RL) policy to generate object fetching trajectories within these scenes. This RL policy, which leverages oracle information, is subsequently distilled into a vision-based policy for real-world deployment. Considering that sim-to-real discrepancies stem from texture variations mostly while from geometric dimensions rarely, we propose to adopt depth information estimated by full-fledged depth foundation models as the input for the vision-based policy to mitigate sim-to-real gap. To tackle the challenge of limited views, we design a novel architecture for learning multi-view representations, allowing for comprehensive encoding of cluttered shelf scenes. This enables FetchBot to effectively minimize collisions while fetching objects from varying positions and depths, ensuring robust and safety-aware operation. Both simulation and real-robot experiments demonstrate FetchBot's superior generalization ability, particularly in handling a broad range of real-world scenarios, including diverse scene layouts and objects with varying geometries and dimensions.

\label{method}
\subsection{Overview}
% To tackle the object fetching task in cluttered shelf environments, we propose a novel framework named FetchBot. As shown in Fig.~\ref{}. First, in Sec.\ref{UniVoxGen}, we provide a detailed description of the voxel-based scene generator, UniVoxGen, which efficiently produces cluttered shelf scenes with three distinct levels of difficulty. Following this, Sec.\ref{state-based_policy} elaborates on how we leverage RL’s capability to learn dynamic-aware behaviors~\cite{claus1998dynamics, bloembergen2015evolutionary,everett2018motion} and minimize interference with surrounding obstacles, the collected expert data demonstrates significantly higher quality compared to motion planning-based approaches~\cite{sundaralingam2023curobo,schulman2014motion,claussmann2019review,zhao2024survey}. Finally, in Sec. \ref{occ-based_policy}, we discuss the distilled vision-based policy and provide a detailed explanation of how zero-shot Sim2Real transfer is implemented.
% To address the object-fetching task in cluttered shelf environments, we propose a novel sim-to-real framework named FetchBot, as illustrated in Fig. \ref{fig:5_1}.
To enable safe and generalizable object fetching in cluttered shelves, we propose a novel sim-to-real framework named \textbf{FetchBot}, as illustrated in Fig. \ref{fig:5_1}. FetchBot emphasizes training with synthetic data due to its scalability, cost-effectiveness, and reduced risk.
Firstly, to tackle data scarcity, we introduce \textbf{UniVoxGen}, a voxel-based method (Sec. \ref{UniVoxGen}), which accelerates scene generation by efficiently checking collisions among objects in voxel space and generates realistic scene layouts using several hand-designed rules.
Next, to collect trajectories that ensure safety within these generated scenes, we train an RL policy (Sec. \ref{state-based_policy}) that leverages oracle information. This policy is dynamics-aware through extensive feedback from interactions with the environment, enabling it to minimize disturbances to surrounding objects.
To develop a policy suitable for real-world deployment, we distill these trajectories into a vision-based policy (Sec. \ref{3D-vision-policy}) via imitation learning. To bridge the sim-to-real gap, we use a depth foundation model to generate unified inputs for both simulation and the real world by converting RGB inputs into their corresponding predicted depths. Additionally, we employ multi-view voxel-based representations that focus exclusively on the region of interest, enhancing the policy’s generalization ability and addressing the limitations of narrow views in lateral access environments.
By combining the advantages of each component, FetchBot achieves zero-shot sim-to-real transfer and is capable of handling a wide range of real-world scenarios.


% and overcome the challenge of limited views in the shelf object-fetching task, 
% This approach allows for comprehensive encoding of cluttered shelf scenes, enabling FetchBot to minimize collisions while fetching objects from varying positions and depths, thereby ensuring robust and safety-aware operation.




\subsection{Voxel-based Cluttered Scene Generator}
\label{UniVoxGen}

\begin{algorithm}
\caption{Scene Generation Algorithm}
\begin{algorithmic}[1]
\REQUIRE Number of scenes $N$
\REQUIRE Max objects per scene $K$
\REQUIRE A set of objects $\mathcal{O}$
\FOR{scene $1:N$}
    \STATE Initialize scene voxel $V^s = \{\}$
    \STATE Sample a target object $O^{tar}$
    \STATE Sample a pose $P$ in SE(3) for $O^{tar}$
    \STATE Apply $T(V_i, P)$ to transform the target object
    \STATE Apply $V_{O^{tar}} \cup V^s$ to add the target object to the scene
    \STATE Sample number of obstacle objects $k \sim [1, \ldots, K]$
    \FOR{obstacle $O^{obs}$ $1:k$}
        \STATE Sample a pose $P$ in SE(3) for $O^{obs}$
        \STATE Apply $T(V_i, P)$ to transform the obstacle object
        \WHILE{$V_{O^{obs}} \cap V^s$}
            \STATE Sample a new pose $P$ in SE(3) for $O^{obs}$
            \STATE Apply $T(V_i, P)$ to transform the obstacle object
        \ENDWHILE
        \STATE Apply $V_{O^{obs}} \cup V^s$ to add the obstacle object to the scene
    \ENDFOR
    \STATE Save the pose $P$ of each object in the scene
\ENDFOR
\end{algorithmic}
\label{alg:scene_gen}
\end{algorithm}

\begin{figure}[tb]
% \vspace{-0.4cm}
    \centering
    \includegraphics[trim=0.2cm 7cm 25cm 0.0cm, clip,  width=0.8\linewidth]{figures/method/5_2.pdf}
    \caption{A set of operational primitives in voxel space.}
    \label{fig:5_2}
    % \vspace{-0.6cm}
\end{figure}

A diverse and realistic set of scenes is crucial for sim-to-real transfer, requiring an effective scene generation method.
Previous works on generating cluttered scenes~\cite{wada2022safepicking, li2024broadcasting, xu2023joint, qian2024thinkgrasp,yang2024ground4act,wang2024learning} rely heavily on complex collision detection mechanisms within simulations to verify the validity of generated scenes, significantly hindering generation efficiency and scalability. Additionally, many of these methods \cite{wada2022safepicking, li2024broadcasting, xu2023joint, qian2024thinkgrasp,yang2024ground4act,wang2024learning} create cluttered layouts by simulating objects dropped from the air, which may result in unstable and unrealistic scene configurations.
Our method, \textbf{UniVoxGen}, is specifically designed for fast and realistic scene generation in voxel space. It accelerates the generation process by performing efficient collision checks in voxel space and produces realistic scene layouts using a set of carefully crafted hand-designed rules.

We begin by providing formal definitions for key elements in the voxel space. Let $V^o = \{V^o_{1}, V^o_{2}, \dots, V^o_{N}\}$ represent the voxel representation of a set of objects, and $V^s = \{V^s_{1}, V^s_{2}, \dots, V^s_{N}\}$ represent the voxel representation of the scene. As illustrated in Fig.~\ref{fig:5_2}, we define a set of operational primitives in voxel space for manipulating voxels. Specifically, $V_i \cup V_j$ denotes the union operation, which combines two voxel sets and is commonly used to add an object’s voxels into the scene. $V_i \cap V_j$ denotes the intersection operation, which retrieves the intersection of two voxel sets and is used to detect potential collisions when adding a new object. $V_i - V_j$ denotes the difference operation, which removes the overlapping portion of $V_i$ with $V_j$, typically used to remove an object from the scene. 
Finally, $T(V_i, P), P \in {SE}(3)$ represents a transformation of a voxel $V_i$ in ${SE}(3)$ space, commonly used to change the pose of the object. Here, $P$ is a transformation matrix in ${SE(3)}$ that combines rotation and translation.

Based on the previously defined key elements and operation primitives, we further designed a set of generation rules $R = \{R_1, R_2, \dots, R_N\}$. UniVoxGen uses these rules to generate three different levels of cluttered scenes: easy, medium, and hard. Specifically, the easy scene is highly organized, with no obstacles in front of the target object; the medium difficulty scene is partially organized, with a few objects arranged randomly, and there are some obstacles in front of the target object; in the hard difficulty scenes, the obstacles are either highly disordered or tightly attached to the surface of the target object to be fetched. Moreover, these scenes may include unsolvable cases, where it is impossible to retrieve the target object without colliding with any obstacles. It is worth noting that the inclusion of unsolvable cases is intended to better simulate real-world scenarios, as such situations can occur in practice. The procedure for generating these cluttered scenes is outlined in Algorithm~\ref{alg:scene_gen}. It should be noted that, given the complexity of the various scene generation rules, the steps presented here represent a simplified version of our scene generation rules. The detailed generation rules will be made available in the subsequently released source code. Finally, we used UniVoxGen to generate 1 million cluttered scenes, which were then utilized as training scene data for a state-based policy. It takes 12 hours on the workstation equipped with 8 RTX 4090s to generate these 1 million scenes, including easy, medium, and hard levels.
\subsection{State-Based Policy}
\label{state-based_policy}
The quality of the demonstrations determines the quality of the distillation.
Obtaining expert demonstrations for object fetching from cluttered shelves is challenging, as it requires dynamic awareness to minimize scene disruption. Collision-based motion planning is effective for finding collision-free paths but fails to account for environmental dynamics when such paths are unavailable, which is common in cluttered scenes. This limitation can lead to damaging consequences, such as harming fragile items or destabilizing the scene. Additionally, collision-based approaches tend to be time-consuming, thus reducing data collection efficiency. 
To address these challenges and inspired by RLDG \cite{xu2024rldg}, which shows that RL-generated data achieves better distillation performance than human demonstrations, we adopt a RL policy to collect trajectories. Similar to SafePicking \cite{wada2022safepicking}, our policy gains dynamic awareness through extensive interaction with the environment, enabling the collection of expert data that minimizes disturbances.

% To address these challenges, and inspired by RLDG \cite{xu2024rldg}, which shows that data generated by RL can achieve better distillation performance compared to human demonstrations, we adopt a reinforcement learning (RL) policy, similar to SafePicking \cite{wada2022safepicking}, which gains dynamic awareness through extensive interaction with the environment, enabling the collection of expert data that minimizes disturbances.


\begin{figure}[tb]
% \vspace{-0.4cm}
    \centering
    \includegraphics[trim=0.cm 8.6cm 21cm 0.cm, clip,  width=1.0\linewidth]{figures/method/5_3.pdf}
    \caption{\textbf{Scene Encoder Network in State-Based Policy.} The network follows a hierarchical design. First, a local network is used to extract features specific to the individual objects within the scene. Then, a global network processes these features to capture the overall structure and context of the entire scene.}
    \label{fig:3}
    % \vspace{-0.6cm}
\end{figure}
\textbf{Observations Encoding and Outputs.} The observation \( O_t \) of the state-based policy \( \pi_{{state}} \) is defined as:

\[
O_t = [ q_t^{{target}}, a_{t-1}, S_t ],
\]

where \( q_t^{{target}} \) represents the relative pose of the end-effector with respect to the target goal at time step \( t \), consisting of a rotation matrix \( {rot}_t \) and a translation vector \( {trans}_t \). The term \( a_{t-1} \) denotes the previous action, and \( S_t \) is the representation of the scene's geometry, given by:

\[
S_t = f_{{scene}}(K_t, M_t),
\]

where \( K_t \) represents the scene's geometry and \( M_t \) contains the mask information for each object, indicating whether it is the target, an obstacle, the end effector, or the shelf divider. For simplicity, we will omit the subscript  $t$  in the following. As illustrated in Fig.~\ref{fig:3}, and similar to \cite{chen2023predicting}, the scene's geometry \( K \) consists of \( M \) objects, including the target, obstacles, end-effector, and shelf dividers: \( \{ K_{1}, K_{2}, \dots, K_{M} \} \), each characterized by a set of \( N \) keypoints \( \{ k_{i}^1, k_{t}^2, \dots, k_{i}^N \} \), uniformly distributed across the object's surface. This flexible representation allows for adaptation to objects with varying geometries and dimensions.

Inspired by PointNet++ \cite{qi2017pointnet++}, we design a hierarchical network \( f_{{scene}} \) that first extracts each object's local geometric features and then derives the global scene feature. Specifically, using \( f_{{local}} \), we obtain a set of local geometric features \( G^{{local}} = \{ g_{1}^{{local}}, g_{2}^{{local}}, \dots, g_{M}^{{local}} \} \), where \( g_{i}^{{local}} = f_{{local}}(K_{i}) \), and \( f_{{local}} \) is a lightweight MLP network with two layers and a max-pooling function for permutation invariance. Next, using \( f_{{global}} \), which shares the same architecture as \( f_{{local}} \) but includes a projection layer, we obtain the global scene feature:

\[
S = f_{{global}}(G^{{local}}, M),
\]

where \( M = \{ m_{1}, m_{2}, \dots, m_{M} \} \) indicates the object mask information. By concatenating \( S \) and \( M \), and passing them through \( f_{{global}} \), we obtain the final global scene representation. This approach focuses on both the individual objects and the relationships among them, providing a comprehensive scene representation.
After feeding the observation $O_t$ into state-based policy $\pi_{state}$, it outputs the relative action $a_{t} = \{a_{t}^{trans}, a_{t}^{rot}\}$, where $a_{trans} \in \mathbb{R}^{3}$ denotes the relative translation, and $a_{{rot}} \in SO(3)$ represents the relative rotation, expressed using the axis-angle. 

\textbf{Reward Functions.}
The goal of shelf fetching is to minimize the impact on the surrounding environment during both the approach and retrieval processes, thereby avoiding potentially catastrophic consequences. To achieve this, our reward design combines penalties for environmental impact, rewards for task success, and constraints on the actions taken, which can be expressed as:

\[
r = \lambda_{{impact}} r_{{impact}} + \lambda_{{task}} r_{{task}} + \lambda_{{constr}} r_{{constr}},
\]

where  \( r_{{impact}} \) penalizes the impact on surrounding items, in this project, we represent the disturbance to the scene by using the sum of the translations $E_{trans}$ and the sum of the rotations $E_{rot}$ of all obstacle objects. \( r_{{task}} \) indicates whether the extraction process is successfully completed under the given constraints. In this project, it means that the target object needs to be extracted to the target goal while ensuring that the sum of translations $E_{trans}$ and rotations $E_{rot}$ satisfy: $E_{trans} < \sigma_{trans}$ and $E_{rot} < \sigma_{rot}$, where $\sigma$ is scaling term based on the precision requirement. We use a $\sigma$ curriculum during the training, enabling the successful exploration in the early stages and progressively improving precision in the later stages. \( r_{{constr}} \) represents constraints on the end-effector's behavior, such as limits on angular and linear velocities(see appendix for the details).

% \textbf{Domain Randomization.}
% The diversity of data is crucial for a model’s generalization ability. With the diverse scene layouts generated by UniVoxGen~\ref{UniVoxGen}, we also need to generate varied expert trajectories to improve the distillation process. In this project, we randomize the suction point on the visible surface of the target object, as well as the mass and coefficients of the objects, to train the state-based policy $\pi_{state}$, ensuring the generation of diverse expert trajectories.

\subsection{3D Vision Policy}

\begin{figure*}[tb]
% \vspace{-0.4cm}
    \centering
    \includegraphics[trim=0.2cm 10.5cm 4cm 0.0cm, clip,  width=1.0\linewidth]{figures/method/5_4.pdf}
    \caption{ The \textbf{perception module} efficiently integrates features from multiple perspectives into a unified voxel-based representation, focusing solely on the region around the robotic gripper. The\textbf{ decision module} processes the output by the perception module using a transformer and employs a high-capacity Diffusion Policy as the core component for action generation.}
    \label{fig:5_4}
    % \vspace{-0.6cm}
\end{figure*}

\label{3D-vision-policy}
To obtain a policy suitable for the real world, we train a vision-based policy using the generated expert trajectories through imitation learning.
The goal of object fetching from cluttered shelves is to successfully grasp a target object and move it to a desired location while avoiding collisions with surrounding obstacles. This task is fraught with challenges, the most significant of which is the occlusion of the target object from a single perspective, primarily caused by non-target objects and shelf panels blocking the view.
Inspired by advancements in the autonomous driving field~\cite{wang2024panoocc,cao2022monoscene,wei2023surroundocc}, our 3D vision policy leverages multi-view inputs to address the issue of limited view fields, and we introduce a camera-based 3D semantic occupancy prediction task as an auxiliary objective, encouraging the network to retain crucial geometric information in voxel-based representations. As shown in Fig.~\ref{fig:5_4}, our 3D vision policy consists of two key modules: the perception module and the decision module. (a) The perception module efficiently integrates features from multiple perspectives into a unified voxel-based representation, focusing solely on the region around the robotic gripper. This powerful feature representation provides richer information for decision-making in the subsequent decision module and enhances the model’s generalization ability. (b) The decision module processes the output by the perception module using a transformer~\cite{vaswani2017attention} and employs a high-capacity Diffusion Policy~\cite{chi2023diffusion} as the core component for action generation.



\textbf{Perception.} As shown in Fig.~\ref{fig:5_4}, our 3D vision policy first leverages depth foundation models, such as DepthAnything~\cite{yang2024depth}, to convert the raw RGB images $I_{RGB}$ from $N$ camera perspectives into corresponding depth maps $I_{D}$. This conversion reduces the sim-to-real gap caused by texture discrepancies in image distributions between the simulated and real environments. Next, we use a backbone network (e.g., ResNet~\cite{he2016deep}) to extract features $X = \{x_i\}_{i=1}^N$ from these $N$ perspectives, which are then fed into the 3D vision encoder. In the 3D vision encoder, we define a set of learnable local 3D-grid-shaped queries $Q \in \mathbb{R}^{C \times H \times W \times Z}$ centered around the robot’s end-effector. Here, $H$, $W$, and $Z$ represent the number of cells along the $X$, $Y$, and $Z$ axes of the predicted 3D space (in a right-handed coordinate system). For each 3D query, we map it from the 3D space to multiple 2D feature maps $F^{2D}$ using the given camera intrinsics and extrinsics. Here, we only use the views that the 3D reference point hits. We then apply a deformable cross-attention (DCA) mechanism~\cite{zhu2020deformable} to sample 2D features around the projected 2D positions:

$$DCA(q_p,F^{2D})=\frac{1}{|V_{hit}|}\sum_{t\in V_{hit}}DA(q_p,\mathcal{P}(p,t),F_t^{2D}),$$

Here, $q_p$ represents the 3D volume query at point $p = (x, y, z)$, $V_{hit}$ denotes the hit views of the 3D query points, $\mathcal{P}(p,t)$ is the camera projection function, and $DA$ refers to deformable attention. After each DCA operation, we follow the approach from SurroundOcc~\cite{wei2023surroundocc} and apply 3D convolution to further process the sampled 3D features. Finally, in the occupancy prediction stage, we found that predicting occupancy in a smaller local 3D region around the robot’s end-effector yields better results compared to predicting the entire shelf's occupancy. This approach not only accelerates the policy inference speed but also enhances generalization performance, as the shape of the obstacles becomes irregular after cropping, making the network more robust.

\textbf{Decision.} In a manner consistent with prior transformer-based policies~\cite{team2024octo,kim24openvla,wang2024rise}, the decision module leverages a transformer architecture to process the 3D features \( F^{3D} \) derived from the perception module. Initially, we augment the 3D features with learnable 3D position embeddings \( P^{3D} \in \mathbb{R}^{C \times H \times W \times Z} \). The combined 3D feature map is then flattened, transforming it into a set of 3D feature vectors \( V^{3D} \in \mathbb{R}^{C} \). Then, similar to works such as Octo~\cite{team2024octo}, we use a learnable readout token to query the action features \( F_A \). Finally, we utilize a lightweight diffusion head to progressively denoise random Gaussian noise $a^K$ into the noise-free action \( a^0 \), conditioned on the action features \( F_A \).

\textbf{Training.} Our 3D vision policy is a multi-stage training network. In the first stage, we pre-train the perception module on a large-scale scene dataset using the camera-based 3D semantic occupancy prediction task to achieve comprehensive 3D scene understanding. In the second stage, we freeze the perception module and only train the decision module.

During the first stage, for the occupancy prediction task, the dense occupancy ground truth data is generated using UniVoxGen. We use the cross-entropy loss and scene-class affinity loss introduced in~\cite{cao2022monoscene} as supervision signals. In the second stage, we apply the mean squared error loss as the supervision signal to predict the noise added to the original action.
