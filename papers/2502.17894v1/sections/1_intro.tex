\section{Introduction}

% The ability for robots to fetch objects is essential, such as retrieving bowls from cabinets, books from shelves, or, more importantly, goods from retail environments. Robots capable of autonomously fetching goods in retail stores or warehouses are particularly appealing, as they can replace humans in performing repetitive tasks. However, achieving human-level performance in these tasks presents numerous challenges. 

% A vast amount of research has focused on shelf manipulation, including object grasping pose detection in xxx and the development of benchmarks for fetching objects in daily scenarios. However, unlike bin picking, which involves vertically lifting an object from a cluttered bin with little concern on the impact on surrounding objects, shelf picking requires horizontally extracting items from an occluded and cluttered shelf. This process requires great care to minimize disturbances to the surrounding environment, as even minor disruptions could lead to destructive consequences, such as causing nearby objects (e.g., glass cups) to fall off the shelf. Previous work has largely overlooked the challenge of safely extracting objects in a single step, which is critically important but remains under-explored.

% However, solving this safety problem is quite challenging. Compared to rule-based approaches, data-driven methods appear more promising, as rule-based approaches struggle to generalize across different scenarios (e.g., lifting an object to the same height across shelves of varying heights could cause the object to hit the top barrier, making safe retrieval impossible). Data-driven methods, however, depend heavily on the scalability and diversity of the data. One option is to use real-world datasets, which require significant human labor to teleoperate the robot to perform item retrieval. Still, this approach is time-consuming and difficult to generalize due to data scarcity. On the other hand, synthetic data can greatly relieve data scarcity but introduces several challenges: currently, there is no simulation environment tailored for shelf fetching, making it difficult to generate a large number of realistic shelf scenarios that closely resemble real-world setups. Additionally, collecting expert trajectories and achieving effective sim-to-real transfer are critical issues that need to be addressed. 

% Many recent studies leverage depth data as a key input to bridge the sim-to-real gap. However, these approaches face challenges with inaccurate depth information in reflective and transparent objects, object edges, items at close distances, and a significant amount of noise. This is also one of the reasons why these methods have not been widely adopted in practical applications.

% In this work, we introduce FetchBot, making the first endeavor to ensure safety during the fetching process through zero-shot generalization using synthetic data. This generalizability encompasses transitions across various domains, ranging from seen scenes to novel environments, from training objects to different items with varying geometries, and from simulation to real-world scenarios (Sim2Real). To tackle the challenge of data scarcity, we propose UniVoxGen, a voxel-based scene generation method that efficiently creates simulation scenes closely resembling real-world grocery layouts. Leveraging these generated scenes, we apply reinforcement learning (RL) to collect high-quality data. Unlike collision-based motion planning methods, RL enables dynamic awareness through extensive interactions with the environment, allowing for the collection of trajectories that minimize impact on surrounding objects. Then, a vision-based policy is learned through imitation learning to distill the state-based RL expert, incorporating several designs tailored to enhance its generalizability. Specifically, we achieve texture generalization by leveraging a foundation model, DepthAnythingV2, an RGB-based depth estimation method, to obtain relative depth information. Spatial generalization is achieved through local representations, which endow the policy with translation invariance, and object geometry generalization is achieved through a unified view representation—occupancy, similar to those used in autonomous driving. Additionally, scene-level generalization is realized through the vast and diverse set of large-scale scenes we generate ourselves. Through our meticulously designed sim-to-real pipeline, our method achieves zero-shot sim-to-real transfer and demonstrates excellent performance.

% To summarize, our key contributions are:
% \begin{itemize}
%     \item We designed a system for safe object fetching from shelves.
%     \item We proposed an efficient scene generation method, UniVoxGen, which generates a large amount of synthetic scene data and RL-collected expert data for the shelf fetching task.
%     \item We introduced a sim-to-real system framework including several elaborate design choices, that enables the vision-based policy to achieve zero-shot sim-to-real transfer.
%     \item our system was validated in the real world, demonstrating generalizability across different object placements, object geometries, and shelf configurations.
% \end{itemize}
% todo: add multi-view policies

% The labor shortage has posed a growing challenge in recent years, and in this context, robots capable of autonomously assisting humans with repetitive labor tasks have become increasingly appealing as a potential solution to alleviate the issue. For instance, a robot capable of automatically fetching items from shelves in a retail store. However, replicating human-level fetching capabilities in robots presents numerous challenges.

% Cluttered shelves are ubiquitous, especially in warehouses, retail stores, libraries or homes, \etcno, making object retrieval a crucial capability for robots to assist humans in diverse scenarios.
% Cluttered shelves can be found everywhere—from warehouses and retail stores to libraries and homes—making object fetching an essential capability for future robots to assist humans across various scenarios.
% Fetching objects from cluttered shelves falls under grasping in structured clutter \cite{lundell2021ddgc}, which requires to avoid collisions between the robot, the manipulated objects, and surrounding items.
% Unlike bin picking, where objects are stacked in arbitrary poses and the robot can grasp and lift them from above with little concern for the impact on surrounding objects, shelf fetching requires extracting items from the side of shelves while ensuring safety.
% This means that the robot needs to minimize the change of scene structures when it approaches and retrieves the target object from cluttered shelves. Failing to do so could result in undesirable consequences, such as causing nearby objects (especially fragile ones like glass cups) to topple or fall off the shelf. Although robotic fetching has been extensively studied, covering areas such as grasp point detection \cite{yang2023dynamo,atar2024optigrasp,murray2024learning}, benchmark development \cite{han2024fetchbench}, non-prehensile manipulation \cite{wu2024wild}, and mobile manipulation \cite{bajracharya2024demonstrating,spahn2024demonstrating}, these works have largely overlooked safety concerns during the extraction process. Ensuring robust and safe object fetching in various real-world scenarios remains challenging, especially in the presence of restricted motion space, limited fields of view, or complex object dynamics.
% This means that, when retrieving a target object, the robot must minimize disturbances to surrounding items. Neglecting safety could lead to undesirable consequences, such as causing other items—especially fragile ones (e.g., glass cups)—to topple or even fall off the shelf. Despite being long studied in the field of robotic fetching, ranging from grasp point detection \cite{yang2023dynamo,atar2024optigrasp,murray2024learning} and benchmark development \cite{han2024fetchbench} to non-prehensile manipulation \cite{wu2024wild} and mobile manipulation \cite{bajracharya2024demonstrating,spahn2024demonstrating}, these works have paid less attention to safety issues during the extraction process. Ensuring robust and safe object fetching in real-world scenarios with varying scene layouts and diverse objects remains challenging due to several factors, such as restricted motion space, limited fields of view and complex object dynamics.

% In real-world applications, object fetching requires safety, meaning that when retrieving a target object, the robot must minimize disturbances to surrounding items. Neglecting safety could lead to undesirable consequences, such as causing other items-especially fragile ones (e.g., glass cups)-to topple or even fall off the shelf. Despite being long studied in the field of robotic fetching, ensuring robust safe object fetching in real-world scenarios with varying scene layouts and diverse objects remains challenging.

% Unlike bin picking, where objects are stacked in arbitrary poses and the robot can grasp and lift them from above with little concern for the impact on surrounding objects, shelf fetching requires safely extracting items from the side of shelves. This is more challenging for many reasons such as restricted motion space, limited fields of view, complex object dynamics, and varying shelf layouts.

% Unlike bin picking, where objects are stacked in arbitrary poses and the robot can grasp and lift them from above with little concern for the impact on surrounding objects, shelf fetching requires safely extracting items from the side of shelves. This is particularly challenging for many reasons such as restricted motion space, limited fields of view, complex object dynamics, and varying shelf layouts.

% Many prior works have already focused on shelf fetching, ranging from grasping point detection for objects on shelves \cite{yang2023dynamo,atar2024optigrasp,murray2024learning}, the establishment of shelf benchmarks \cite{han2024fetchbench}, the search for objects on shelves \cite{huang2022mechanical}, to using non-prehensile methods to pick ungraspable objects on shelves\cite{wu2024wild}.However, these works have paid less attention to safety issues during the extraction process. The work most closely related to ours is SafePicking \cite{wada2022safepicking}, but it addresses the task in an overhead-access tabletop setting.

% In an overhead-access bin picking setting, where objects can be stacked in arbitrary poses, we need to vertically lift an object from a cluttered bin with little concern for the impact on surrounding objects. However, in a lateral-access shelf fetching setting, we face several challenges, including a restricted action space, a limited perceptual field, and, especially, the need to minimize disturbance to surrounding objects, i.e., ensuring safety. Neglecting safety could lead to undesirable consequences, such as disrupting the existing arrangement of items, which makes the continuation of subsequent tasks more difficult. In some cases, it may even result in destructive outcomes, such as causing items, especially fragile objects (e.g., glass cups), to fall off the shelf. Ensuring safety in a cluttered lateral-access shelf environment is a significant challenge due to the diversity of target objects and the randomness of the item layout. More concretely, the objects we need to fetch can vary greatly, ranging from boxes to bottles, and the diversity of attached objects increases the difficulty of the task. Additionally, other objects may block the target object on the left side, at other times on the right side, or even on both sides simultaneously. We need to perform the extraction of the target object while minimizing the disturbance to surrounding items, all within the unpredictable layout of the shelf.

% Many prior works have already focused on shelf fetching, ranging from grasping point detection for objects on shelves \cite{yang2023dynamo,atar2024optigrasp,murray2024learning}, the establishment of shelf benchmarks \cite{han2024fetchbench}, the search for objects on shelves \cite{huang2022mechanical}, to using non-prehensile methods to pick ungraspable objects \cite{wu2024wild}. However, these works have paid less attention to safety issues during the extraction process in a cluttered shelf environment. The work most closely related to ours is SafePicking \cite{wada2022safepicking}, but it addresses the task in an overhead-access tabletop setting, whereas we focus on a lateral-access shelf environment.

% In this paper, we focus on developing object fetching in cluttered shelves and advancing it to a level of generalization sufficient for real-world applications.
% In this paper, we present the first attempt to achieve safe shelf fetching, with a focus on retail environments. 
% However, directly training robots in the real world poses challenges such as labor-intensive data collection, safety risks, and scalability issues. Due to the scarcity of real-world data, models trained on such data struggle to achieve the level of generalization required for practical applications, especially when objects in shelves are diverse and their arrangements vary significantly.
% Given the wide variety of shelf layouts and the diversity of objects, the method must be sufficiently generalizable to handle a spectrum of real-world scenarios, a capability that data-driven methods can provide. 
% However, directly training robots for shelf fetching in real-world settings presents several challenges, including labor-intensive data collection, potential safety risks, and scalability issues. 
% We highlight that training with synthetic data offers a promising and cost-effective solution, yet it has been underestimated in the past.
% Training with synthetic data provides a promising alternative, as it circumvents these challenges. 
% To fully exploit the potential of synthetic data, we propose \textbf{FetchBot}, a sim-to-real framework designed to enable zero-shot generalizable and safety-aware object fetching from cluttered shelves in real-world settings.
% Nevertheless, the significant domain gap between simulation and reality remains a major obstacle for effective sim-to-real transfer in object fetching. To combine the advantages of data-driven methods and synthetic data, we propose FetchBot, a sim-to-real framework designed to enable zero-shot generalizable and safety-aware object fetching from cluttered shelves in real-world settings.


% To fill the gap in the field of shelf fetching, we introduce FetchBot. We propose a zero-shot sim2real method to learn a generalizable closed-loop vision-based policy to handle the complex and varying real-world cluttered shelf scenarios. Considering its potential for generalization, low cost, and reduced labor intensity, we use synthetic data to achieve this goal and have carefully designed a sim2real pipeline to mitigate the sim2real gap inherent in this approach. 

% The quality and quantity of data dominant the generalization capacity of learned skills, as introduced in \cite{}.
% Although FetchBench \cite{han2024fetchbench} introduces a simulation benchmark for robot fetching, the generated scenes are not realistic and diverse enough to bridge the sim-to-real gaps for real-world deployment.
% To address the data scarcity issue, we propose an efficient voxel-based method, \textbf{UniVoxGen}, for generating diverse and realistic cluttered shelf scenes at scale. UniVoxGen efficiently handles collision checking among objects in voxel space. In contrast to traditional scene generation methods, which involve the time-consuming process of loading assets into the simulation and performing collision checks, UniVoxGen significantly speeds up scene generation. Furthermore, UniVoxGen can generate realistic shelf layouts using several hand-designed rules.
% Once the scenes are generated, we proceed to generate expert trajectories. Unlike collision-based motion planning methods, reinforcement learning (RL) enables dynamic awareness through extensive interactions with the environment. Consequently, we train a dynamics-aware RL policy that utilizes oracle information to collect trajectories that minimize the impact on surrounding objects.
% To obtain a policy suitable for real-world deployment, we distill these expert trajectories into a closed-loop, vision-based policy through imitation learning.

% Although FetchBench \cite{han2024fetchbench} introduces a simulation benchmark for robot fetching, the generated scenes fail to accurately represent real-world shelf layouts. This is primarily due to excessive spacing between objects and low item density in these scenes.
% To address the data scarcity issue, we propose an efficient voxel-based method, UniVoxGen, for generating diverse and releastic cluttered shelf scenes at scale. UniVoxGen efficiently handles collision checking among objects in voxel space. In contrast to traditional scene generation methods, which involve the time-consuming process of loading assets into the simulation and performing collision checks, UniVoxGen significantly speeds up generation.
% With the generated scenes prepared, we need to generate expert trajectories. Compared with collision-based motion planning methods, reinforcement learning (RL) enables dynamic awareness through extensive interactions with the environment. This allows the collection of trajectories that minimize the impact on surrounding objects. 

% Considering that sim-to-real discrepancies stem from texture variations mostly while from geometric dimensions rarely, 

% Inspired by Unsupervised Domain Adaptation (UDA) techniques~\cite{dai2021idm,zhao2023towards,dai2024bridging} that use an intermediate domain to improve cross-domain generalization, we integrate the capabilities of foundation models into our vision-based policy to further enhance its generalization. Specifically, we leverage the predicted depth from the pre-trained DepthAnything model \cite{yang2024depth} as an intermediate domain. This model takes RGB images from either simulation or real-world as input and generates a predicted depth image, focusing exclusively on geometric information.
% Given the limited perceptual view in lateral access environments and the need for comprehensive 3D scene understanding to avoid collisions, we draw inspiration from RVT-2 \cite{goyal2024rvt} and autonomous driving approaches \cite{wang2024panoocc,cao2022monoscene,wei2023surroundocc} to design a novel architecture. This architecture learns unified multi-view representations that focus solely on the region of interest, enabling precise 3D understanding.

% Simulated scenes serve as the foundation for sim2real, much like domain randomization. A diverse set of simulation scenarios can facilitate the transfer of knowledge from simulation to the real world. Although FetchBench \cite{han2024fetchbench} has introduced a new benchmark for fetch tasks, the scene layouts do not accurately reflect real-world shelf arrangements due to the loose spacing between objects and the sparse distribution of items. To address the challenge of data scarcity, we propose UniVoxGen, a voxel-based scene generation method that efficiently creates simulation environments closely resembling real-world grocery layouts. Then, we leverage a well-designed, dynamic-aware RL approach to collect expert data for improved imitation learning training. Compared to collision-based motion planning methods, RL enables dynamic awareness through extensive interactions with the environment, allowing for the collection of trajectories that minimize the impact on surrounding objects. Therefore, we leverage an RL agent to collect expert data for imitation learning, aiming to obtain a better vision-based policy. Inspired by unsupervised domain adaptation techniques that use an intermediate domain to improve cross-domain generalization, we integrate foundation model capabilities into our vision-based policy to further strengthen generalization ability. In this work, we utilize predicted depth from the pre-trained DepthAnything \cite{yang2024depth} model as an intermediate domain, which takes RGB images from either the simulation or real-world domain as input and generates a predicted depth image. Finally, to address the issue of limited perceptual view in lateral access environments, we use RGB images from two perspectives as input and employ a unified 3D representation, similar to the approach in autonomous driving \cite{wang2024panoocc,cao2022monoscene,wei2023surroundocc}. This enhances information fusion across multiple viewpoints while further mitigating the sim2real gap. 

% We use a suction cup as our tool for its simplicity and effectiveness in fetching objects from cluttered shelves. Extensive experiments are conducted to evaluate our proposed FetchBot in both simulated and real-world environments, where it demonstrates strong generalization, particularly in handling diverse real-world scenarios.

% To summarize, our key contributions are:
% \begin{itemize}
%     \item We design a novel architecture to learn a unified multi-view 3D representation, focusing solely on the region of interest. This approach provides comprehensive scene understanding, significantly improving collision avoidance while also enhancing generalization for varying shelf scenarios.
%     \item We generate a diverse set of realistic shelf scenes using our efficient, voxel-based scene generator and produce a substantial number of expert trajectories with our trained dynamics-aware RL agent, thereby creating synthetic data tailored for shelf fetching.
%     \item We propose a system capable of safe object fetching from cluttered shelves through our sim-to-real framework, which leverages synthetic data to learn a generalizable, closed-loop, vision-based policy that can effectively handle varying real-world scenarios.
%     \item Through comprehensive ablation experiments, we ablate the key components of our sim-to-real approach, demonstrating the importance of the foundation model, unified multi-view 3D representation, and focusing solely on the region of interest.
% \end{itemize}
% \begin{itemize}
%     \item We propose a system capable of achieving safe object fetching from cluttered shelves.
%     \item We propose a sim2real framework that leverages synthetic data to learn a generalizable closed-loop vision-based policy for addressing the diverse real-world scenarios.
%     \item Through comprehensive ablation experiments, we discuss the key designs of sim2real, providing insights to facilitate future sim2real transfers.
% \end{itemize}


Cluttered shelves are ubiquitous, especially in warehouses, retail stores, libraries and homes, \etcno, making object retrieval a crucial capability for robots assisting humans in diverse scenarios.
Fetching objects from cluttered shelves falls under grasping in structured clutter \cite{lundell2021ddgc}, which requires avoiding collisions between the robot, the manipulated objects, and surrounding items.
This means that the robot needs to minimize the change of scene structures when it approaches and retrieves the target object from cluttered shelves. Failing to do so could result in undesirable consequences, such as causing nearby objects (especially fragile ones like glass cups) to topple or fall off the shelf. Although robotic fetching has been extensively studied, covering areas such as grasp point detection \cite{yang2023dynamo,atar2024optigrasp,murray2024learning}, benchmark development \cite{han2024fetchbench}, non-prehensile manipulation \cite{wu2024wild}, and mobile manipulation \cite{bajracharya2024demonstrating,spahn2024demonstrating}, these works have largely overlooked safety concerns during the extraction process. Ensuring robust and safe object fetching in various real-world scenarios remains a challenge, especially in the presence of restricted motion space, limited fields of view, or complex object dynamics.

In this paper, we focus on developing object fetching in cluttered shelves and advancing it to a level of generalization sufficient for real-world applications, as shown in Fig.~\ref{fig:teaser}.
However, directly training robots in the real world presents challenges such as labor-intensive data collection, safety risks, and scalability issues. Due to the scarcity of real-world data, models trained on such data struggle to achieve the level of generalization required for practical applications, especially when objects on shelves are diverse and their arrangements vary significantly.
We emphasize that training with synthetic data offers a promising and cost-effective solution, yet it has been underestimated in the past.
To fully leverage the potential of synthetic data, we propose \textbf{FetchBot}, a sim-to-real framework designed to enable zero-shot generalizable and safety-aware object fetching from cluttered shelves in real-world settings.

The quality and quantity of data dominate the generalization capacity of learned skills.
Although FetchBench \cite{han2024fetchbench} introduces a simulation benchmark for robot fetching, the generated scenes are not realistic and diverse enough to bridge the sim-to-real gap for real-world deployment. This is primarily due to excessive spacing between objects, resulting in low item density in their simulated scenes.
To address the data scarcity issue, we propose an efficient voxel-based method, Unified Voxel-Based Scene Generator (\textbf{UniVoxGen)}, for generating diverse and realistic cluttered shelf scenes at scale. UniVoxGen performs collision checking efficiently among objects in the voxel space. In contrast to traditional scene generation methods \cite{jia2024cluttergen,zhou2024scenex,dalal2024neural,han2024fetchbench,murali2023cabinet,chamzas2021motionbenchmaker}, whose asset loading and collision checking are quite time-consuming, UniVoxGen significantly accelerates the generation process via voxel-based scene representation. Additionally, UniVoxGen can generate realistic shelf layouts by applying a set of carefully hand-designed rules.
Beyond efficient large-scale scene generation, we also incorporate a RL-based approach to generate expert trajectories in the generated scenes. Compared to motion planning-based trajectory generation \cite{dalal2024neural}, reinforcement learning enables dynamics awareness through extensive feedback from interactions with the environment. This offers an effective method to minimize the collisions between the robot, the manipulated objects, and surrounding items during large-scale synthetic data generation.

To obtain a policy suitable for real-world deployment, we distill these expert trajectories into a closed-loop, vision-based policy through imitation learning. We then explore how to close the sim-to-real gaps. 
Firstly, we leverage simulation to improve the diversity of synthetic data through extensive randomization of the object to be retrieved, surrounding items, and their layouts. In this way, we enable the training datasets to reach a high diversity efficiently that real-world collected datasets are hard to be comparable. 
In fact, the sim-to-real gaps primarily exist in the texture dimension while rarely lie in geometry and material dimensions. Collision avoidance in object fetching relies more on geometry information than on textural information. 
Thus, secondly, we introduce two novel designs to reduce the reliance on textural information and exploit full-fledged foundation models to reduce the sim-to-real gap of the textural information in our proposed method. 
One is we make full use of multi-view voxel-based representations for the environments as the model inputs. This helps mitigate the limitations of perceptual views in lateral access environments, enables comprehensive 3D scene understanding, and enhances the vision policy’s generalization ability. To facilitate the learning of these representations, we introduce an occupancy prediction task as an auxiliary objective, encouraging the network to preserve essential geometric information in the voxel-based representations. Empirically, we find that predicting local occupancy around the robotic gripper can drive a better trade-off between the efficiency and effectiveness. 
The other is we employ a depth foundation model to convert the original RGB inputs into their corresponding depths. 
This is done during the preprocessing stage of the vision-based policy. In this way, the generalizability of depth foundation models across sim-to-real is fully exploited.



We use a suction cup as our tool for its simplicity and effectiveness in fetching objects from cluttered shelves. Extensive experiments are conducted to evaluate our proposed FetchBot in both simulated and real-world environments, where it demonstrates strong generalization, particularly in handling diverse real-world scenarios.

The core contributions of this work can be summarized in the following four aspects:
\begin{itemize}
    \item We introduce a zero-shot sim-to-real framework for generalizable object fetching from cluttered shelves in real-world scenes. The learned closed-loop policy is dynamics-aware, capable of avoiding collisions, and generalizes to various environments.
    \item We propose a synthetic data generation pipeline for producing diverse object-fetching trajectories in cluttered scenes at scale, serving as the foundation of the aforementioned sim-to-real framework.
    \item We design a novel architecture to learn a unified multi-view 3D representation, focusing solely on the region of interest. This approach enhances scene understanding, significantly improves collision avoidance, and boosts generalization across diverse shelf scenarios.
    \item We study the scaling characteristics of our proposed method and conduct an in-depth analysis of the roles played by different technical components in bridging the sim-to-real gap, providing insights for broader future applications.
\end{itemize}