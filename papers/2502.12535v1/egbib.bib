@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@Article{technologies9010002,
    AUTHOR = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
    TITLE = {A Survey on Contrastive Self-Supervised Learning},
    JOURNAL = {Technologies},
    VOLUME = {9},
    YEAR = {2021},
    NUMBER = {1},
    ARTICLE-NUMBER = {2},
    URL = {https://www.mdpi.com/2227-7080/9/1/2},
    ISSN = {2227-7080},
    ABSTRACT = {Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudolabels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we present a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make meaningful progress.},
    DOI = {10.3390/technologies9010002}
}

@article{HU2024128645,
    title = {A comprehensive survey on contrastive learning},
    journal = {Neurocomputing},
    volume = {610},
    pages = {128645},
    year = {2024},
    issn = {0925-2312},
    doi = {https://doi.org/10.1016/j.neucom.2024.128645},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231224014164},
    author = {Haigen Hu and Xiaoyuan Wang and Yan Zhang and Qi Chen and Qiu Guan},
    keywords = {Contrastive learning, Representation learning, Self-supervised learning, Unsupervised learning},
    abstract = {Contrastive Learning is self-supervised representation learning by training a model to differentiate between similar and dissimilar samples. It has been shown to be effective and has gained significant attention in various computer vision and natural language processing tasks. In this paper, we comprehensively and systematically sort out the main ideas, recent developments and application areas of contrastive learning. Specifically, we firstly provide an overview of the research activity of contrastive learning in recent years. Secondly, we describe the basic principles and summarize a universal framework of contrastive learning. Thirdly, we further introduce and discuss the latest advances of each functional component in detail, including data augmentation, positive/negative samples,network structure, and loss function. Finally, we summarize contrastive learning and discuss the challenges, future research trends and development directions in the area of contrastive learning.}
}

@InProceedings{Varol_2017_CVPR,
    author = {Varol, Gul and Romero, Javier and Martin, Xavier and Mahmood, Naureen and Black, Michael J. and Laptev, Ivan and Schmid, Cordelia},
    title = {Learning From Synthetic Humans},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {July},
    year = {2017}
}

@inproceedings{choi2023rethinking,
    title={Rethinking Self-Supervised Visual Representation Learning in Pre-training for 3D Human Pose and Shape Estimation},
    author={Hongsuk Choi and Hyeongjin Nam and Taeryung Lee and Gyeongsik Moon and Kyoung Mu Lee},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=8U4joMeLRF}
}

@InProceedings{Grauman_2022_CVPR,
    author    = {Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and others},
    title     = {Ego4D: Around the World in 3,000 Hours of Egocentric Video},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {18995-19012}
}

@inproceedings{Deng2009,
    title = {ImageNet: A large-scale hierarchical image database},
    url = {http://dx.doi.org/10.1109/CVPR.2009.5206848},
    DOI = {10.1109/cvpr.2009.5206848},
    booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
    publisher = {IEEE},
    author = {Deng,  Jia and Dong,  Wei and Socher,  Richard and Li,  Li-Jia and Kai Li and Li Fei-Fei},
    year = {2009},
    month = jun 
}

@INPROCEEDINGS{Shan20, 
    author = {Shan, Dandan and Geng, Jiaqi and Shu, Michelle  and Fouhey, David},
    title = {Understanding Human Hands in Contact at Internet Scale},
    booktitle = CVPR, 
    year = {2020} 
}

@InProceedings{Zimmermann_2017_ICCV,
    author = {Zimmermann, Christian and Brox, Thomas},
    title = {Learning to Estimate 3D Hand Pose From Single RGB Images},
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    month = {Oct},
    year = {2017}
}

@InProceedings{Park_2023_ICCV,
    author    = {Park, Joonkyu and Jung, Daniel Sungho and Moon, Gyeongsik and Lee, Kyoung Mu},
    title     = {Extract-and-Adaptation Network for 3D Interacting Hand Mesh Recovery},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    month     = {October},
    year      = {2023},
    pages     = {4200-4209}
}

@InProceedings{Zhou_2024_CVPR,
    author    = {Zhou, Zhishan and Zhou, Shihao and Lv, Zhi and Zou, Minqiang and Tang, Yao and Liang, Jiajun},
    title     = {A Simple Baseline for Efficient Hand Mesh Reconstruction},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {1367-1376}
}

@misc{zhou_simple_2024,
	title = {A {Simple} {Baseline} for {Efficient} {Hand} {Mesh} {Reconstruction}},
	url = {http://arxiv.org/abs/2403.01813},
	abstract = {3D hand pose estimation has found broad application in areas such as gesture recognition and human-machine interaction tasks. As performance improves, the complexity of the systems also increases, which can limit the comparative analysis and practical implementation of these methods. In this paper, we propose a simple yet effective baseline that not only surpasses state-of-the-art (SOTA) methods but also demonstrates computational efficiency. To establish this baseline, we abstract existing work into two components: a token generator and a mesh regressor, and then examine their core structures. A core structure, in this context, is one that fulfills intrinsic functions, brings about significant improvements, and achieves excellent performance without unnecessary complexities. Our proposed approach is decoupled from any modifications to the backbone, making it adaptable to any modern models. Our method outperforms existing solutions, achieving state-of-the-art (SOTA) results across multiple datasets. On the FreiHAND dataset, our approach produced a PA-MPJPE of 5.7mm and a PA-MPVPE of 6.0mm. Similarly, on the Dexycb dataset, we observed a PA-MPJPE of 5.5mm and a PA-MPVPE of 5.0mm. As for performance speed, our method reached up to 33 frames per second (fps) when using HRNet and up to 70 fps when employing FastViT-MA36},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Zhou, Zhishan and zhou, Shihao and Lv, Zhi and Zou, Minqiang and Tang, Yao and Liang, Jiajun},
	month = mar,
	year = {2024},
	note = {arXiv:2403.01813},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@InProceedings{Fu_2023_ICCV,
    author    = {Fu, Qichen and Liu, Xingyu and Xu, Ran and Niebles, Juan Carlos and Kitani, Kris M.},
    title     = {Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {23600-23611}
}

@InProceedings{Chen_2022_CVPR,
    author    = {Chen, Xingyu and Liu, Yufeng and Dong, Yajiao and Zhang, Xiong and Ma, Chongyang and Xiong, Yanmin and Zhang, Yuan and Guo, Xiaoyan},
    title     = {MobRecon: Mobile-Friendly Hand Mesh Reconstruction From Monocular Image},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {20544-20554}
}

@InProceedings{Park_2022_CVPR,
    author    = {Park, JoonKyu and Oh, Yeonguk and Moon, Gyeongsik and Choi, Hongsuk and Lee, Kyoung Mu},
    title     = {HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {1496-1505}
}

@InProceedings{Liu_2021_CVPR,
    author    = {Liu, Shaowei and Jiang, Hanwen and Xu, Jiarui and Liu, Sifei and Wang, Xiaolong},
    title     = {Semi-Supervised 3D Hand-Object Poses Estimation With Interactions in Time},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {14687-14697}
}

@inbook{Spurr2020,
    title = {Weakly Supervised 3D Hand Pose Estimation via Biomechanical Constraints},
    ISBN = {9783030585204},
    ISSN = {1611-3349},
    url = {http://dx.doi.org/10.1007/978-3-030-58520-4_13},
    DOI = {10.1007/978-3-030-58520-4_13},
    booktitle = {Computer Vision – ECCV 2020},
    publisher = {Springer International Publishing},
    author = {Spurr,  Adrian and Iqbal,  Umar and Molchanov,  Pavlo and Hilliges,  Otmar and Kautz,  Jan},
    year = {2020},
    pages = {211–228}
}

@InProceedings{Zhang_2019_ICCV,
    author = {Zhang, Xiong and Li, Qiang and Mo, Hong and Zhang, Wenbo and Zheng, Wen},
    title = {End-to-End Hand Mesh Recovery From a Monocular RGB Image},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month = {October},
    year = {2019}
}

@InProceedings{He_2016_CVPR,
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    title = {Deep Residual Learning for Image Recognition},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2016}
}

@InProceedings{Moon_2020_ECCV_InterHand2.6M,  
    author = {Moon, Gyeongsik and Yu, Shoou-I and Wen, He and Shiratori, Takaaki and Lee, Kyoung Mu},  
    title = {InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image},  
    booktitle = {European Conference on Computer Vision (ECCV)},  
    year = {2020}  
}  

@INPROCEEDINGS{chao:cvpr2021_dexycb,
    author    = {Yu-Wei Chao and Wei Yang and Yu Xiang and Pavlo Molchanov and Ankur Handa and Jonathan Tremblay and Yashraj S. Narang and Karl {Van Wyk} and Umar Iqbal and Stan Birchfield and Jan Kautz and Dieter Fox},
    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    title     = {{DexYCB}: A Benchmark for Capturing Hand Grasping of Objects},
    year      = {2021},
}

@article{Ionescu2014,
    title = {Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments},
    volume = {36},
    ISSN = {2160-9292},
    url = {http://dx.doi.org/10.1109/TPAMI.2013.248},
    DOI = {10.1109/tpami.2013.248},
    number = {7},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
    author = {Ionescu,  Catalin and Papava,  Dragos and Olaru,  Vlad and Sminchisescu,  Cristian},
    year = {2014},
    month = jul,
    pages = {1325–1339}
}

@article{Gower1975,
    title = {Generalized procrustes analysis},
    volume = {40},
    ISSN = {1860-0980},
    url = {http://dx.doi.org/10.1007/BF02291478},
    DOI = {10.1007/bf02291478},
    number = {1},
    journal = {Psychometrika},
    publisher = {Springer Science and Business Media LLC},
    author = {Gower,  J. C.},
    year = {1975},
    month = mar,
    pages = {33–51}
}

@InProceedings{Xu_2023_CVPR,
    author    = {Xu, Hao and Wang, Tianyu and Tang, Xiao and Fu, Chi-Wing},
    title     = {H2ONet: Hand-Occlusion-and-Orientation-Aware Network for Real-Time 3D Hand Mesh Reconstruction},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {17048-17058}
}

@inproceedings{xiao2018simple,
    author={Xiao, Bin and Wu, Haiping and Wei, Yichen},
    title={Simple Baselines for Human Pose Estimation and Tracking},
    booktitle = {European Conference on Computer Vision (ECCV)},
    year = {2018}
}

@InProceedings{Zhang_2021_ICCV,
    author    = {Zhang, Baowen and Wang, Yangang and Deng, Xiaoming and Zhang, Yinda and Tan, Ping and Ma, Cuixia and Wang, Hongan},
    title     = {Interacting Two-Hand 3D Pose and Shape Reconstruction From Single Color Image},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {11354-11363}
}

@InProceedings{Li_2022_CVPR,
    author    = {Li, Mengcheng and An, Liang and Zhang, Hongwen and Wu, Lianpeng and Chen, Feng and Yu, Tao and Liu, Yebin},
    title     = {Interacting Attention Graph for Single Image Two-Hand Reconstruction},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {2761-2770}
}

@InProceedings{Ren_2023_ICCV,
    author    = {Ren, Pengfei and Wen, Chao and Zheng, Xiaozheng and Xue, Zhou and Sun, Haifeng and Qi, Qi and Wang, Jingyu and Liao, Jianxin},
    title     = {Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {8014-8025}
}

@InProceedings{Yu_2023_CVPR,
    author    = {Yu, Zhengdi and Huang, Shaoli and Fang, Chen and Breckon, Toby P. and Wang, Jue},
    title     = {ACR: Attention Collaboration-Based Regressor for Arbitrary Two-Hand Reconstruction},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {12955-12964}
}

@inproceedings{hrnet_sun2019deep,
    title={Deep High-Resolution Representation Learning for Human Pose Estimation},
    author={Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
    booktitle={CVPR},
    year={2019}
}

@misc{mmpose2020,
    title={OpenMMLab Pose Estimation Toolbox and Benchmark},
    author={MMPose Contributors},
    howpublished = {\url{https://github.com/open-mmlab/mmpose}},
    year={2020}
}

@article{Lugaresi2019MediaPipeAF,
    title={MediaPipe: A Framework for Building Perception Pipelines},
    author={Camillo Lugaresi and Jiuqiang Tang and Hadon Nash and Chris McClanahan and Esha Uboweja and Michael Hays and Fan Zhang and Chuo-Ling Chang and Ming Guang Yong and Juhyun Lee and Wan-Teh Chang and Wei Hua and Manfred Georg and Matthias Grundmann},
    journal={ArXiv},
    year={2019},
    volume={abs/1906.08172},
    url={https://api.semanticscholar.org/CorpusID:195069430}
}

@inproceedings{
    xu2022vitpose,
    title={Vi{TP}ose: Simple Vision Transformer Baselines for Human Pose Estimation},
    author={Yufei Xu and Jing Zhang and Qiming Zhang and Dacheng Tao},
    booktitle={Advances in Neural Information Processing Systems},
    year={2022},
}

@article{xu2022vitpose+,
    title={ViTPose+: Vision Transformer Foundation Model for Generic Body Pose Estimation},
    author={Xu, Yufei and Zhang, Jing and Zhang, Qiming and Tao, Dacheng},
    journal={arXiv preprint arXiv:2212.04246},
    year={2022}
}

@InProceedings{10.1007/978-3-031-19772-7_3,
    author="Zhang, Haoyuan
    and Hou, Yonghong
    and Zhang, Wenjing
    and Li, Wanqing",
    editor="Avidan, Shai
    and Brostow, Gabriel
    and Ciss{\'e}, Moustapha
    and Farinella, Giovanni Maria
    and Hassner, Tal",
    title="Contrastive Positive Mining for Unsupervised 3D Action Representation Learning",
    booktitle="Computer Vision -- ECCV 2022",
    year="2022",
    publisher="Springer Nature Switzerland",
    address="Cham",
    pages="36--51",
    abstract="Recent contrastive based 3D action representation learning has made great progress. However, the strict positive/negative constraint is yet to be relaxed and the use of non-self positive is yet to be explored. In this paper, a Contrastive Positive Mining (CPM) framework is proposed for unsupervised skeleton 3D action representation learning. The CPM identifies non-self positives in a contextual queue to boost learning. Specifically, the siamese encoders are adopted and trained to match the similarity distributions of the augmented instances in reference to all instances in the contextual queue. By identifying the non-self positive instances in the queue, a positive-enhanced learning strategy is proposed to leverage the knowledge of mined positives to boost the robustness of the learned latent space against intra-class and inter-class diversity. Experimental results have shown that the proposed CPM is effective and outperforms the existing state-of-the-art unsupervised methods on the challenging NTU and PKU-MMD datasets.",
    isbn="978-3-031-19772-7"
}

@InProceedings{Hampali_2022_CVPR_Kypt_Trans,  
    author = {Shreyas Hampali and Sayan Deb Sarkar and Mahdi Rad and Vincent Lepetit},  
    title = {Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation},  
    booktitle = {IEEE Computer Vision and Pattern Recognition Conference},  
    year = {2022}  
}  

@InProceedings{Zheng_2021_ICCV,
    author    = {Zheng, Mingkai and Wang, Fei and You, Shan and Qian, Chen and Zhang, Changshui and Wang, Xiaogang and Xu, Chang},
    title     = {Weakly Supervised Contrastive Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {10042-10051}
}

@InProceedings{Mo_2023_WACV,
    author    = {Mo, Shentong and Sun, Zhun and Li, Chao},
    title     = {Multi-Level Contrastive Learning for Self-Supervised Vision Transformers},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2023},
    pages     = {2778-2787}
}

@InProceedings{Wang_2023_CVPR,
    author    = {Wang, Haoqing and Tang, Yehui and Wang, Yunhe and Guo, Jianyuan and Deng, Zhi-Hong and Han, Kai},
    title     = {Masked Image Modeling With Local Multi-Scale Reconstruction},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {2122-2131}
}

@InProceedings{Chang_2022_CVPR,
    author    = {Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T.},
    title     = {MaskGIT: Masked Generative Image Transformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {11315-11325}
}

@InProceedings{Xie_2023_CVPR,
    author    = {Xie, Zhenda and Geng, Zigang and Hu, Jingcheng and Zhang, Zheng and Hu, Han and Cao, Yue},
    title     = {Revealing the Dark Secrets of Masked Image Modeling},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {14475-14485}
}

@inproceedings{xie2021simmim,
    title={SimMIM: A Simple Framework for Masked Image Modeling},
    author={Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},
    booktitle={International Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2022}
}

@inproceedings{zheng2023hamuco,
    title={HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning},
    author={Zheng, Xiaozheng and Wen, Chao and Xue, Zhou and Ren, Pengfei and Wang, Jingyu},
    booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
    year={2023}
}

@inproceedings{epipolartransformers,
    title={Epipolar Transformers},
    author={He, Yihui and Yan, Rui and Fragkiadaki, Katerina and Yu, Shoou-I},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={7779--7788},
    year={2020}
}

@article{10.1109/TPAMI.2023.3247907,
    author = {Tu, Zhigang and Huang, Zhisheng and Chen, Yujin and Kang, Di and Bao, Linchao and Yang, Bisheng and Yuan, Junsong},
    title = {Consistent 3D Hand Reconstruction in Video via Self-Supervised Learning},
    year = {2023},
    issue_date = {Aug. 2023},
    publisher = {IEEE Computer Society},
    address = {USA},
    volume = {45},
    number = {8},
    issn = {0162-8828},
    url = {https://doi.org/10.1109/TPAMI.2023.3247907},
    doi = {10.1109/TPAMI.2023.3247907},
    abstract = {We present a method for reconstructing accurate and consistent 3D hands from a monocular video. We observe that the detected 2D hand keypoints and the image texture provide important cues about the geometry and texture of the 3D hand, which can reduce or even eliminate the requirement on 3D hand annotation. Accordingly, in this work, we propose <inline-formula><tex-math notation="LaTeX">$mathrm{{S}^{2}HAND}$</tex-math><alternatives><mml:math><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi> HAND </mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="chen-ieq1-3247907.gif"/></alternatives></inline-formula>, a self-supervised 3D hand reconstruction model, that can jointly estimate pose, shape, texture, and the camera viewpoint from a single RGB input through the supervision of easily accessible 2D detected keypoints. We leverage the continuous hand motion information contained in the unlabeled video data and explore <inline-formula><tex-math notation="LaTeX">$mathrm{{S}^{2}HAND(V)}$</tex-math><alternatives><mml:math><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi> HAND </mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="chen-ieq2-3247907.gif"/></alternatives></inline-formula>, which uses a set of weights shared <inline-formula><tex-math notation="LaTeX">$mathrm{{S}^{2}HAND}$</tex-math><alternatives><mml:math><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi> HAND </mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="chen-ieq3-3247907.gif"/></alternatives></inline-formula> to process each frame and exploits additional motion, texture, and shape consistency constrains to obtain more accurate hand poses, and more consistent shapes and textures. Experiments on benchmark datasets demonstrate that our self-supervised method produces comparable hand reconstruction performance compared with the recent full-supervised methods in single-frame as input setup, and notably improves the reconstruction accuracy and consistency when using the video training data.},
    journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
    month = aug,
    pages = {9469–9485},
    numpages = {17}
}

@INPROCEEDINGS{10655481,
    author={Pavlakos, Georgios and Shan, Dandan and Radosavovic, Ilija and Kanazawa, Angjoo and Fouhey, David and Malik, Jitendra},
    booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
    title={Reconstructing Hands in 3D with Transformers}, 
    year={2024},
    volume={},
    number={},
    pages={9826-9836},
    keywords={Solid modeling;Computer vision;Three-dimensional displays;Annotations;Training data;Benchmark testing;Transformers;3D hand pose estimation;hand mesh recovery;single-image 3D},
    doi={10.1109/CVPR52733.2024.00938}
}

@inproceedings{10.1007/978-3-031-20068-7_22,
    author = {Meng, Hao and Jin, Sheng and Liu, Wentao and Qian, Chen and Lin, Mengxiang and Ouyang, Wanli and Luo, Ping},
    title = {3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal},
    year = {2022},
    isbn = {978-3-031-20067-0},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    url = {https://doi.org/10.1007/978-3-031-20068-7_22},
    doi = {10.1007/978-3-031-20068-7_22},
    abstract = {Estimating 3D interacting hand pose from a single RGB image is essential for understanding human actions. Unlike most previous works that directly predict the 3D poses of two interacting hands simultaneously, we propose to decompose the challenging interacting hand pose estimation task and estimate the pose of each hand separately. In this way, it is straightforward to take advantage of the latest research progress on the single-hand pose estimation system. However, hand pose estimation in interacting scenarios is very challenging, due to (1) severe hand-hand occlusion and (2) ambiguity caused by the homogeneous appearance of hands. To tackle these two challenges, we propose a novel Hand De-occlusion and Removal (HDR) framework to perform hand de-occlusion and distractor removal. We also propose the first large-scale synthetic amodal hand dataset, termed Amodal InterHand Dataset (AIH), to facilitate model training and promote the development of the related research. Experiments show that the proposed method significantly outperforms previous state-of-the-art interacting hand pose estimation approaches. Codes and data are available at .},
    booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part VI},
    pages = {380–397},
    numpages = {18},
    keywords = {3D interacting hand pose estimation, De-occlusion, Removal, Amodal InterHand Dataset},
    location = {Tel Aviv, Israel}
}

@inproceedings{ego_Prakash2024Hands,
    author = {Prakash, Aditya and Tu, Ruisen and Chang, Matthew and Gupta, Saurabh},
    title = {3D Hand Pose Estimation in Everyday Egocentric Images},
    booktitle = {European Conference on Computer Vision (ECCV)},
    year = {2024}
}

@misc{blur_oh2023recovering3dhandmesh,
    title={Recovering 3D Hand Mesh Sequence from a Single Blurry Image: A New Dataset and Temporal Unfolding}, 
    author={Yeonguk Oh and JoonKyu Park and Jaeha Kim and Gyeongsik Moon and Kyoung Mu Lee},
    year={2023},
    eprint={2303.15417},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2303.15417}, 
}

@misc{moon_bringing_2023,
    title = {Bringing {Inputs} to {Shared} {Domains} for {3D} {Interacting} {Hands} {Recovery} in the {Wild}},
    url = {http://arxiv.org/abs/2303.13652},
    abstract = {Despite recent achievements, existing 3D interacting hands recovery methods have shown results mainly on motion capture (MoCap) environments, not on in-the-wild (ITW) ones. This is because collecting 3D interacting hands data in the wild is extremely challenging, even for the 2D data. We present InterWild, which brings MoCap and ITW samples to shared domains for robust 3D interacting hands recovery in the wild with a limited amount of ITW 2D/3D interacting hands data. 3D interacting hands recovery consists of two sub-problems: 1) 3D recovery of each hand and 2) 3D relative translation recovery between two hands. For the first sub-problem, we bring MoCap and ITW samples to a shared 2D scale space. Although ITW datasets provide a limited amount of 2D/3D interacting hands, they contain large-scale 2D single hand data. Motivated by this, we use a single hand image as an input for the first sub-problem regardless of whether two hands are interacting. Hence, interacting hands of MoCap datasets are brought to the 2D scale space of single hands of ITW datasets. For the second sub-problem, we bring MoCap and ITW samples to a shared appearance-invariant space. Unlike the first subproblem, 2D labels of ITW datasets are not helpful for the second sub-problem due to the 3D translation’s ambiguity. Hence, instead of relying on ITW samples, we amplify the generalizability of MoCap samples by taking only a geometric feature without an image as an input for the second sub-problem. As the geometric feature is invariant to appearances, MoCap and ITW samples do not suffer from a huge appearance gap between the two datasets. The code is publicly available1.},
    language = {en},
    urldate = {2024-06-04},
    publisher = {arXiv},
    author = {Moon, Gyeongsik},
    month = oct,
    year = {2023},
    note = {arXiv:2303.13652 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {Moon - 2023 - Bringing Inputs to Shared Domains for 3D Interacti.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\4P4ZPGIG\\Moon - 2023 - Bringing Inputs to Shared Domains for 3D Interacti.pdf:application/pdf},
}

@misc{lin_pre-training_2024,
    title = {Pre-{Training} for {3D} {Hand} {Pose} {Estimation} with {Contrastive} {Learning} on {Large}-{Scale} {Hand} {Images} in the {Wild}},
    url = {http://arxiv.org/abs/2409.09714},
    abstract = {We present a contrastive learning framework based on in-the-wild hand images tailored for pre-training 3D hand pose estimators, dubbed HandCLR. Pre-training on large-scale images achieves promising results in various tasks, but prior 3D hand pose pre-training methods have not fully utilized the potential of diverse hand images accessible from in-the-wild videos. To facilitate scalable pre-training, we first prepare an extensive pool of hand images from in-the-wild videos and design our method with contrastive learning. Specifically, we collected over 2.0M hand images from recent human-centric videos, such as 100DOH and Ego4D. To extract discriminative information from these images, we focus on the similarity of hands; pairs of similar hand poses originating from different samples, and propose a novel contrastive learning method that embeds similar hand pairs closer in the latent space. Our experiments demonstrate that our method outperforms conventional contrastive learning approaches that produce positive pairs sorely from a single image with data augmentation. We achieve significant improvements over the state-of-the-art method in various datasets, with gains of 15\% on FreiHand, 10\% on DexYCB, and 4\% on AssemblyHands.},
    language = {en},
    urldate = {2024-09-24},
    publisher = {arXiv},
    author = {Lin, Nie and Ohkawa, Takehiko and Zhang, Mingfang and Huang, Yifei and Furuta, Ryosuke and Sato, Yoichi},
    month = sep,
    year = {2024},
    note = {arXiv:2409.09714 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {Lin 等 - 2024 - Pre-Training for 3D Hand Pose Estimation with Cont.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\QI7A6FSQ\\Lin 等 - 2024 - Pre-Training for 3D Hand Pose Estimation with Cont.pdf:application/pdf},
}

@misc{he2024diffusionmodelslowlevelvision,
      title={Diffusion Models in Low-Level Vision: A Survey}, 
      author={Chunming He and Yuqi Shen and Chengyu Fang and Fengyang Xiao and Longxiang Tang and Yulun Zhang and Wangmeng Zuo and Zhenhua Guo and Xiu Li},
      year={2024},
      eprint={2406.11138},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.11138}, 
}

@ARTICLE{vrar_9873969,
    author={Shen, Junxiao and Dudley, John and Mo, George and Kristensson, Per Ola},
    journal={IEEE Transactions on Visualization and Computer Graphics}, 
    title={Gesture Spotter: A Rapid Prototyping Tool for Key Gesture Spotting in Virtual and Augmented Reality Applications}, 
    year={2022},
    volume={28},
    number={11},
    pages={3618-3628},
    keywords={Skeleton;Rapid prototyping;Computer architecture;Training;Real-time systems;Graphical user interfaces;Gesture recognition;Gesture interaction;prototyping tools;machine learning;augmented reality;virtual reality},
    doi={10.1109/TVCG.2022.3203004}
}

@article{vrar_DIANATFAR2021407,
    title = {Review on existing VR/AR solutions in human–robot collaboration},
    journal = {Procedia CIRP},
    volume = {97},
    pages = {407-411},
    year = {2021},
    note = {8th CIRP Conference of Assembly Technology and Systems},
    issn = {2212-8271},
    doi = {https://doi.org/10.1016/j.procir.2020.05.259},
    url = {https://www.sciencedirect.com/science/article/pii/S2212827120314815},
    author = {Morteza Dianatfar and Jyrki Latokartano and Minna Lanz},
    keywords = {Human–robot collaboration, Virtual reality, Augmented reality},
    abstract = {During the last decades, there has been wide interest towards creating more agile and reconfigurable automation systems. This includes the research interest towards human–robot collaboration (HRC) solutions, where semi or fully automated process could be combined with human dexterity and flexibility without complexity and inflexibility. However, the communication between the human and the robot is not intuitive, fast or flexible yet. The emerging technologies such as Augmented Reality (AR), Virtual Reality (VR) and Mixed Reality (XR) are seen as good solution candidates for increasing the communication between human and the machine during the design, commission and operation phases. The design of the HRC system includes layout design evaluation, task scenario analysis, robot programming, cycle time calculations, and safety analysis. Virtual Reality and Augmented Reality technologies provide immersive experiences to visualize and analyze these procedures. This paper aims to review the current status of virtual and augmented reality solutions in HRC with meta-analysis and highlight missing elements. Finally, future research directions and requirements are presented.}
}

@Article{face_recog_electronics9081188,
    AUTHOR = {Adjabi, Insaf and Ouahabi, Abdeldjalil and Benzaoui, Amir and Taleb-Ahmed, Abdelmalik},
    TITLE = {Past, Present, and Future of Face Recognition: A Review},
    JOURNAL = {Electronics},
    VOLUME = {9},
    YEAR = {2020},
    NUMBER = {8},
    ARTICLE-NUMBER = {1188},
    URL = {https://www.mdpi.com/2079-9292/9/8/1188},
    ISSN = {2079-9292},
    ABSTRACT = {Face recognition is one of the most active research fields of computer vision and pattern recognition, with many practical and commercial applications including identification, access control, forensics, and human-computer interactions. However, identifying a face in a crowd raises serious questions about individual freedoms and poses ethical issues. Significant methods, algorithms, approaches, and databases have been proposed over recent years to study constrained and unconstrained face recognition. 2D approaches reached some degree of maturity and reported very high rates of recognition. This performance is achieved in controlled environments where the acquisition parameters are controlled, such as lighting, angle of view, and distance between the camera–subject. However, if the ambient conditions (e.g., lighting) or the facial appearance (e.g., pose or facial expression) change, this performance will degrade dramatically. 3D approaches were proposed as an alternative solution to the problems mentioned above. The advantage of 3D data lies in its invariance to pose and lighting conditions, which has enhanced recognition systems efficiency. 3D data, however, is somewhat sensitive to changes in facial expressions. This review presents the history of face recognition technology, the current state-of-the-art methodologies, and future directions. We specifically concentrate on the most recent databases, 2D and 3D face recognition methods. Besides, we pay particular attention to deep learning approach as it presents the actuality in this field. Open issues are examined and potential directions for research in facial recognition are proposed in order to provide the reader with a point of reference for topics that deserve consideration.},
    DOI = {10.3390/electronics9081188}
}

@Article{gesture_jimaging6080073,
    AUTHOR = {Oudah, Munir and Al-Naji, Ali and Chahl, Javaan},
    TITLE = {Hand Gesture Recognition Based on Computer Vision: A Review of Techniques},
    JOURNAL = {Journal of Imaging},
    VOLUME = {6},
    YEAR = {2020},
    NUMBER = {8},
    ARTICLE-NUMBER = {73},
    URL = {https://www.mdpi.com/2313-433X/6/8/73},
    PubMedID = {34460688},
    ISSN = {2313-433X},
    ABSTRACT = {Hand gestures are a form of nonverbal communication that can be used in several fields such as communication between deaf-mute people, robot control, human–computer interaction (HCI), home automation and medical applications. Research papers based on hand gestures have adopted many different techniques, including those based on instrumented sensor technology and computer vision. In other words, the hand sign can be classified under many headings, such as posture and gesture, as well as dynamic and static, or a hybrid of the two. This paper focuses on a review of the literature on hand gesture techniques and introduces their merits and limitations under different circumstances. In addition, it tabulates the performance of these methods, focusing on computer vision techniques that deal with the similarity and difference points, technique of hand segmentation used, classification algorithms and drawbacks, number and types of gestures, dataset used, detection range (distance) and type of camera used. This paper is a thorough general overview of hand gesture methods with a brief discussion of some possible applications.},
    DOI = {10.3390/jimaging6080073}
}

@ARTICLE{
    pose_guided_9200521,
    author={Xu, Chengming and Fu, Yanwei and Wen, Chao and Pan, Ye and Jiang, Yu-Gang and Xue, Xiangyang},
    journal={IEEE Transactions on Image Processing}, 
    title={Pose-Guided Person Image Synthesis in the Non-Iconic Views}, 
    year={2020},
    volume={29},
    number={},
    pages={9060-9072},
    keywords={Task analysis;Image generation;Visualization;Biological system modeling;Computational modeling;Training;Image reconstruction;Image processing;image generation},
    doi={10.1109/TIP.2020.3023853}
}

@InProceedings{
    cnet_Zhang_2023_ICCV,
    author    = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
    title     = {Adding Conditional Control to Text-to-Image Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {3836-3847}
}

@inproceedings{
    vit_dosovitskiy2021an,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{lin_cross-domain_2023,
	address = {Vancouver, BC, Canada},
	title = {Cross-{Domain} {3D} {Hand} {Pose} {Estimation} with {Dual} {Modalities}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350301298},
	url = {https://ieeexplore.ieee.org/document/10203772/},
	doi = {10.1109/CVPR52729.2023.01648},
	abstract = {Recent advances in hand pose estimation have shed light on utilizing synthetic data to train neural networks, which however inevitably hinders generalization to realworld data due to domain gaps. To solve this problem, we present a framework for cross-domain semi-supervised hand pose estimation and target the challenging scenario of learning models from labelled multi-modal synthetic data and unlabelled real-world data. To that end, we propose a dual-modality network that exploits synthetic RGB and synthetic depth images. For pre-training, our network uses multi-modal contrastive learning and attention-fused supervision to learn effective representations of the RGB images. We then integrate a novel self-distillation technique during fine-tuning to reduce pseudo-label noise. Experiments show that the proposed method significantly improves 3D hand pose estimation and 2D keypoint detection on benchmarks.},
	language = {en},
	urldate = {2024-07-09},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lin, Qiuxia and Yang, Linlin and Yao, Angela},
	month = jun,
	year = {2023},
	pages = {17184--17193},
	file = {Lin 等 - 2023 - Cross-Domain 3D Hand Pose Estimation with Dual Mod.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\29BT6NKR\\Lin 等 - 2023 - Cross-Domain 3D Hand Pose Estimation with Dual Mod.pdf:application/pdf},
}

@article{choi_rethinking_2023,
	title = {{RETHINKING} {SELF}-{SUPERVISED} {VISUAL} {REPRESEN}- {TATION} {LEARNING} {IN} {PRE}-{TRAINING} {FOR} {3D} {HUMAN} {POSE} {AND} {SHAPE} {ESTIMATION}},
	abstract = {Recently, a few self-supervised representation learning (SSL) methods have outperformed the ImageNet classiﬁcation pre-training for vision tasks such as object detection. However, its effects on 3D human body pose and shape estimation (3DHPSE) are open to question, whose target is ﬁxed to a unique class, the human, and has an inherent task gap with SSL. We empirically study and analyze the effects of SSL and further compare it with other pre-training alternatives for 3DHPSE. The alternatives are 2D annotation-based pre-training and synthetic data pretraining, which share the motivation of SSL that aims to reduce the labeling cost. They have been widely utilized as a source of weak-supervision or ﬁne-tuning, but have not been remarked as a pre-training source. SSL methods underperform the conventional ImageNet classiﬁcation pre-training on multiple 3DHPSE benchmarks by 7.7\% on average. In contrast, despite a much less amount of pre-training data, the 2D annotation-based pre-training improves accuracy on all benchmarks and shows faster convergence during ﬁne-tuning. Our observations challenge the naive application of the current SSL pre-training to 3DHPSE and relight the value of other data types in the pre-training aspect.},
	language = {en},
	author = {Choi, Hongsuk and Nam, Hyeongjin and Lee, Taeryung and Moon, Gyeongsik and Lee, Kyoung Mu},
	year = {2023},
	file = {Choi 等 - 2023 - RETHINKING SELF-SUPERVISED VISUAL REPRESEN- TATION.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\4WNH97PR\\Choi 等 - 2023 - RETHINKING SELF-SUPERVISED VISUAL REPRESEN- TATION.pdf:application/pdf},
}

@misc{lin_end--end_2021,
	title = {End-to-{End} {Human} {Pose} and {Mesh} {Reconstruction} with {Transformers}},
	url = {http://arxiv.org/abs/2012.09760},
	abstract = {We present a new method, called MEsh TRansfOrmer (METRO), to reconstruct 3D human pose and mesh vertices from a single image. Our method uses a transformer encoder to jointly model vertex-vertex and vertex-joint interactions, and outputs 3D joint coordinates and mesh vertices simultaneously. Compared to existing techniques that regress pose and shape parameters, METRO does not rely on any parametric mesh models like SMPL, thus it can be easily extended to other objects such as hands. We further relax the mesh topology and allow the transformer self-attention mechanism to freely attend between any two vertices, making it possible to learn non-local relationships among mesh vertices and joints. With the proposed masked vertex modeling, our method is more robust and effective in handling challenging situations like partial occlusions. METRO generates new state-of-the-art results for human mesh reconstruction on the public Human3.6M and 3DPW datasets. Moreover, we demonstrate the generalizability of METRO to 3D hand reconstruction in the wild, outperforming existing state-of-the-art methods on FreiHAND dataset. Code and pre-trained models are available at https: //github.com/microsoft/MeshTransformer.},
	language = {en},
	urldate = {2024-07-06},
	publisher = {arXiv},
	author = {Lin, Kevin and Wang, Lijuan and Liu, Zicheng},
	month = jun,
	year = {2021},
	note = {arXiv:2012.09760 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Lin 等 - 2021 - End-to-End Human Pose and Mesh Reconstruction with.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\AIZBSE9G\\Lin 等 - 2021 - End-to-End Human Pose and Mesh Reconstruction with.pdf:application/pdf},
}

@book{avidan_computer_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computer {Vision} – {ECCV} 2022: 17th {European} {Conference}, {Tel} {Aviv}, {Israel}, {October} 23–27, 2022, {Proceedings}, {Part} {V}},
	volume = {13665},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-3-031-20064-9 978-3-031-20065-6},
	shorttitle = {Computer {Vision} – {ECCV} 2022},
	url = {https://link.springer.com/10.1007/978-3-031-20065-6},
	language = {en},
	urldate = {2024-07-06},
	publisher = {Springer Nature Switzerland},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-20065-6},
	file = {Avidan 等 - 2022 - Computer Vision – ECCV 2022 17th European Confere.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\SC9INFU2\\Avidan 等 - 2022 - Computer Vision – ECCV 2022 17th European Confere.pdf:application/pdf},
}

@misc{zhou_ibot_2022,
	title = {{iBOT}: {Image} {BERT} {Pre}-{Training} with {Online} {Tokenizer}},
	shorttitle = {{iBOT}},
	url = {http://arxiv.org/abs/2111.07832},
	abstract = {The success of language Transformers is primarily attributed to the pretext task of masked language modeling (MLM), where texts are first tokenized into semantically meaningful pieces. In this work, we study masked image modeling (MIM) and indicate the advantages and challenges of using a semantically meaningful visual tokenizer. We present a self-supervised framework iBOT that can perform masked prediction with an online tokenizer. Specifically, we perform self-distillation on masked patch tokens and take the teacher network as the online tokenizer, along with self-distillation on the class token to acquire visual semantics. The online tokenizer is jointly learnable with the MIM objective and dispenses with a multi-stage training pipeline where the tokenizer needs to be pre-trained beforehand. We show the prominence of iBOT by achieving an 82.3\% linear probing accuracy and an 87.8\% fine-tuning accuracy evaluated on ImageNet-1K. Beyond the state-of-the-art image classification results, we underline emerging local semantic patterns, which helps the models to obtain strong robustness against common corruptions and achieve leading results on dense downstream tasks, eg., object detection, instance segmentation, and semantic segmentation.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Zhou, Jinghao and Wei, Chen and Wang, Huiyu and Shen, Wei and Xie, Cihang and Yuille, Alan and Kong, Tao},
	month = jan,
	year = {2022},
	note = {arXiv:2111.07832 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\JEVBTQEC\\2111.html:text/html;Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\UHYAMB2F\\Zhou 等 - 2022 - iBOT Image BERT Pre-Training with Online Tokenize.pdf:application/pdf},
}

@inproceedings{zhang_learning_2021,
	address = {Virtual Event China},
	title = {Learning {Positional} {Priors} for {Pretraining} {2D} {Pose} {Estimators}},
	isbn = {978-1-4503-8671-5},
	url = {https://dl.acm.org/doi/10.1145/3475723.3484252},
	doi = {10.1145/3475723.3484252},
	language = {en},
	urldate = {2024-09-10},
	booktitle = {Proceedings of the 2nd {International} {Workshop} on {Human}-centric {Multimedia} {Analysis}},
	publisher = {ACM},
	author = {Zhang, Kun and Yao, Ping and Wu, Rui and Yang, Chuanguang and Li, Ding and Du, Min and Deng, Kai and Liu, Renbiao and Zheng, Tianyao},
	month = nov,
	year = {2021},
	pages = {3--11},
	file = {Zhang 等 - 2021 - Learning Positional Priors for Pretraining 2D Pose.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\GDJTN83F\\Zhang 等 - 2021 - Learning Positional Priors for Pretraining 2D Pose.pdf:application/pdf},
}

@article{xu_vitpose_nodate,
	title = {{ViTPose}: {Simple} {Vision} {Transformer} {Baselines} for {Human} {Pose} {Estimation}},
	abstract = {Although no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for pose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision transformers as backbones to extract features for a given person instance and a lightweight decoder for pose estimation. It can be scaled up from 100M to 1B parameters by taking the advantages of the scalable model capacity and high parallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose tasks. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our basic ViTPose model outperforms representative methods on the challenging MS COCO Keypoint Detection benchmark, while the largest model sets a new state-of-the-art, i.e., 80.9 AP on the MS COCO test-dev set. The code and models are available at https://github.com/ViTAE-Transformer/ViTPose.},
	language = {en},
	author = {Xu, Yufei and Zhang, Jing and Zhang, Qiming and Tao, Dacheng},
	file = {Xu 等 - ViTPose Simple Vision Transformer Baselines for H.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\D72CDLLB\\Xu 等 - ViTPose Simple Vision Transformer Baselines for H.pdf:application/pdf},
}

@inproceedings{chen_simclr_2020,
	title = {{SimCLR}: {A} {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2024-09-10},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\54S45IB7\\Chen 等 - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;Supplementary PDF:C\:\\Users\\Administrator\\Zotero\\storage\\HI436S6L\\Chen 等 - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf},
}

@inproceedings{tang_humanbench_2023,
	address = {Vancouver, BC, Canada},
	title = {{HumanBench}: {Towards} {General} {Human}-{Centric} {Perception} with {Projector} {Assisted} {Pretraining}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350301298},
	shorttitle = {{HumanBench}},
	url = {https://ieeexplore.ieee.org/document/10204833/},
	doi = {10.1109/CVPR52729.2023.02104},
	abstract = {Human-centric perceptions include a variety of vision tasks, which have widespread industrial applications, including surveillance, autonomous driving, and the metaverse. It is desirable to have a general pretrain model for versatile human-centric downstream tasks. This paper forges ahead along this path from the aspects of both benchmark and pretraining methods. Specifically, we propose a HumanBench based on existing datasets to comprehensively evaluate on the common ground the generalization abilities of different pretraining methods on 19 datasets from 6 diverse downstream tasks, including person ReID, pose estimation, human parsing, pedestrian attribute recognition, pedestrian detection, and crowd counting. To learn both coarse-grained and fine-grained knowledge in human bodies, we further propose a Projector AssisTed Hierarchical pretraining method (PATH) to learn diverse knowledge at different granularity levels. Comprehensive evaluations on HumanBench show that our PATH achieves new state-of-the-art results on 17 downstream datasets and on-par results on the other 2 datasets. The code will be publicly at https://github.com/OpenGVLab/HumanBench.},
	language = {en},
	urldate = {2024-09-10},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tang, Shixiang and Chen, Cheng and Xie, Qingsong and Chen, Meilin and Wang, Yizhou and Ci, Yuanzheng and Bai, Lei and Zhu, Feng and Yang, Haiyang and Yi, Li and Zhao, Rui and Ouyang, Wanli},
	month = jun,
	year = {2023},
	pages = {21970--21982},
	file = {Tang 等 - 2023 - HumanBench Towards General Human-Centric Percepti.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\RXMMLFZA\\Tang 等 - 2023 - HumanBench Towards General Human-Centric Percepti.pdf:application/pdf},
}

@incollection{avidan_p-stmo_2022,
	address = {Cham},
	title = {P-{STMO}: {Pre}-trained {Spatial} {Temporal} {Many}-to-{One} {Model} for {3D} {Human} {Pose} {Estimation}},
	volume = {13665},
	isbn = {978-3-031-20064-9 978-3-031-20065-6},
	shorttitle = {P-{STMO}},
	url = {https://link.springer.com/10.1007/978-3-031-20065-6_27},
	abstract = {This paper introduces a novel Pre-trained Spatial Temporal Many-to-One (P-STMO) model for 2D-to-3D human pose estimation task. To reduce the difficulty of capturing spatial and temporal information, we divide this task into two stages: pre-training (Stage I) and fine-tuning (Stage II). In Stage I, a self-supervised pre-training sub-task, termed masked pose modeling, is proposed. The human joints in the input sequence are randomly masked in both spatial and temporal domains. A general form of denoising auto-encoder is exploited to recover the original 2D poses and the encoder is capable of capturing spatial and temporal dependencies in this way. In Stage II, the pre-trained encoder is loaded to STMO model and fine-tuned. The encoder is followed by a many-to-one frame aggregator to predict the 3D pose in the current frame. Especially, an MLP block is utilized as the spatial feature extractor in STMO, which yields better performance than other methods. In addition, a temporal downsampling strategy is proposed to diminish data redundancy. Extensive experiments on two benchmarks show that our method outperforms state-of-the-art methods with fewer parameters and less computational overhead. For example, our P-STMO model achieves 42.1mm MPJPE on Human3.6M dataset when using 2D poses from CPN as inputs. Meanwhile, it brings a 1.5–7.1× speedup to state-of-the-art methods. Code is available at https://github.com/paTRICK-swk/P-STMO.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Shan, Wenkang and Liu, Zhenhua and Zhang, Xinfeng and Wang, Shanshe and Ma, Siwei and Gao, Wen},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-20065-6_27},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {461--478},
	file = {Shan 等 - 2022 - P-STMO Pre-trained Spatial Temporal Many-to-One M.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\DDGHSCPF\\Shan 等 - 2022 - P-STMO Pre-trained Spatial Temporal Many-to-One M.pdf:application/pdf},
}

@misc{grill_bootstrap_2020,
	title = {Bootstrap your own latent: {A} new approach to self-supervised {Learning}},
	shorttitle = {Bootstrap your own latent},
	url = {http://arxiv.org/abs/2006.07733},
	abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3{\textbackslash}\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6{\textbackslash}\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
	language = {en},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
	month = sep,
	year = {2020},
	note = {arXiv:2006.07733 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Grill 等 - 2020 - Bootstrap your own latent A new approach to self-.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\8JE59TPX\\Grill 等 - 2020 - Bootstrap your own latent A new approach to self-.pdf:application/pdf},
}

@article{chen_2d_2023,
	title = {{2D} {Human} pose estimation: a survey},
	volume = {29},
	issn = {0942-4962, 1432-1882},
	shorttitle = {{2D} {Human} pose estimation},
	url = {https://link.springer.com/10.1007/s00530-022-01019-0},
	doi = {10.1007/s00530-022-01019-0},
	abstract = {Human pose estimation aims at localizing human anatomical keypoints or body parts in the input data (e.g., images, videos, or signals). It forms a crucial component in enabling machines to have an insightful understanding of the behaviors of humans, and has become a salient problem in computer vision and related fields. Deep learning techniques allow learning feature representations directly from the data, significantly pushing the performance boundary of human pose estimation. In this paper, we reap the recent achievements of 2D human pose estimation methods and present a comprehensive survey. Briefly, existing approaches put their efforts in three directions, namely network architecture design, network training refinement, and post processing. Network architecture design looks at the architecture of human pose estimation models, extracting more robust features for keypoint recognition and localization. Network training refinement tap into the training of neural networks and aims to improve the representational ability of models. Post processing further incorporates model-agnostic polishing strategies to improve the performance of keypoint detection. More than 200 research contributions are involved in this survey, covering methodological frameworks, common benchmark datasets, evaluation metrics, and performance comparisons. We seek to provide researchers with a more comprehensive and systematic review on human pose estimation, allowing them to acquire a grand panorama and better identify future directions.},
	language = {en},
	number = {5},
	urldate = {2024-09-11},
	journal = {Multimedia Systems},
	author = {Chen, Haoming and Feng, Runyang and Wu, Sifan and Xu, Hao and Zhou, Fengcheng and Liu, Zhenguang},
	month = oct,
	year = {2023},
	pages = {3115--3138},
	file = {Chen 等 - 2023 - 2D Human pose estimation a survey.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\KIYZK8RD\\Chen 等 - 2023 - 2D Human pose estimation a survey.pdf:application/pdf},
}

@misc{deldari_beyond_2022,
	title = {Beyond {Just} {Vision}: {A} {Review} on {Self}-{Supervised} {Representation} {Learning} on {Multimodal} and {Temporal} {Data}},
	shorttitle = {Beyond {Just} {Vision}},
	url = {http://arxiv.org/abs/2206.02353},
	abstract = {Recently, Self-Supervised Representation Learning (SSRL) has attracted much attention in the field of computer vision, speech, natural language processing (NLP), and recently, with other types of modalities, including time series from sensors. The popularity of self-supervised learning is driven by the fact that traditional models typically require a huge amount of well-annotated data for training. Acquiring annotated data can be a difficult and costly process. Self-supervised methods have been introduced to improve the efficiency of training data through discriminative pre-training of models using supervisory signals that have been freely obtained from the raw data. Unlike existing reviews of SSRL that have pre-dominately focused upon methods in the fields of CV or NLP for a single modality, we aim to provide the first comprehensive review of multimodal self-supervised learning methods for temporal data. To this end, we 1) provide a comprehensive categorization of existing SSRL methods, 2) introduce a generic pipeline by defining the key components of a SSRL framework, 3) compare existing models in terms of their objective function, network architecture and potential applications, and 4) review existing multimodal techniques in each category and various modalities. Finally, we present existing weaknesses and future opportunities. We believe our work develops a perspective on the requirements of SSRL in domains that utilise multimodal and/or temporal data},
	language = {en},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {Deldari, Shohreh and Xue, Hao and Saeed, Aaqib and He, Jiayuan and Smith, Daniel V. and Salim, Flora D.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.02353 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Deldari 等 - 2022 - Beyond Just Vision A Review on Self-Supervised Re.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\J5E6SSE3\\Deldari 等 - 2022 - Beyond Just Vision A Review on Self-Supervised Re.pdf:application/pdf},
}

@inproceedings{lin_mpt_2024,
	address = {Waikoloa, HI, USA},
	title = {{MPT}: {Mesh} {Pre}-{Training} with {Transformers} for {Human} {Pose} and {Mesh} {Reconstruction}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350318920},
	shorttitle = {{MPT}},
	url = {https://ieeexplore.ieee.org/document/10483700/},
	doi = {10.1109/WACV57701.2024.00338},
	abstract = {Traditional methods of reconstructing 3D human pose and mesh from single images rely on paired image-mesh datasets, which can be difficult and expensive to obtain. Due to this limitation, model scalability is constrained as well as reconstruction performance. Towards addressing the challenge, we introduce Mesh Pre-Training (MPT), an effective pre-training strategy that leverages large amounts of MoCap data to effectively perform pre-training at scale. We introduce the use of MoCap-generated heatmaps as input representations to the mesh regression transformer and propose a Masked Heatmap Modeling approach for improving pre-training performance. This study demonstrates that pre-training using the proposed MPT allows our models to perform effective inference without requiring finetuning. We further show that fine-tuning the pre-trained MPT model considerably improves the accuracy of human mesh reconstruction from single images. Experimental results show that MPT outperforms previous state-of-the-art methods on Human3.6M and 3DPW datasets. As a further application, we benchmark and study MPT on the task of 3D hand reconstruction, showing that our generic pretraining scheme generalizes well to hand pose estimation and achieves promising reconstruction performance.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {2024 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Lin, Kevin and Lin, Chung-Ching and Liang, Lin and Liu, Zicheng and Wang, Lijuan},
	month = jan,
	year = {2024},
	pages = {3403--3413},
	file = {Lin 等 - 2024 - MPT Mesh Pre-Training with Transformers for Human.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\TE7HERKQ\\Lin 等 - 2024 - MPT Mesh Pre-Training with Transformers for Human.pdf:application/pdf},
}

@misc{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we ﬁnd that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efﬁciently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.},
	language = {en},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = dec,
	year = {2021},
	note = {arXiv:2111.06377 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {He 等 - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\U29BDTWS\\He 等 - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf},
}

@article{li_network_2020,
	title = {Network representation learning: a systematic literature review},
	volume = {32},
	issn = {0941-0643, 1433-3058},
	shorttitle = {Network representation learning},
	url = {https://link.springer.com/10.1007/s00521-020-04908-5},
	doi = {10.1007/s00521-020-04908-5},
	abstract = {Omnipresent network/graph data generally have the characteristics of nonlinearity, sparseness, dynamicity and heterogeneity, which bring numerous challenges to network related analysis problem. Recently, inﬂuenced by the excellent ability of deep learning to learn representation from data, representation learning for network data has gradually become a new research hotspot. Network representation learning aims to learn a project from given network data in the original topological space to low-dimensional vector space, while encoding a variety of structural and semantic information. The vector representation obtained could effectively support extensive tasks such as node classiﬁcation, node clustering, link prediction and graph classiﬁcation. In this survey, we comprehensively present an overview of a large number of network representation learning algorithms from two clear points of view of homogeneous network and heterogeneous network. The corresponding algorithms are deeply analyzed. Extensive applications are introduced in an all-round way, and related experiments are conducted to validate the typical algorithms. Finally, we point out ﬁve future promising directions for next research in terms of theory and application.},
	language = {en},
	number = {21},
	urldate = {2024-09-16},
	journal = {Neural Computing and Applications},
	author = {Li, Bentian and Pi, Dechang},
	month = nov,
	year = {2020},
	pages = {16647--16679},
	file = {Li 和 Pi - 2020 - Network representation learning a systematic lite.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\H9L2WS7C\\Li 和 Pi - 2020 - Network representation learning a systematic lite.pdf:application/pdf},
}

@article{le-khac_contrastive_2020,
	title = {Contrastive {Representation} {Learning}: {A} {Framework} and {Review}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Contrastive {Representation} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9226466/?arnumber=9226466},
	doi = {10.1109/ACCESS.2020.3031549},
	abstract = {Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper, we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.},
	urldate = {2024-09-16},
	journal = {IEEE Access},
	author = {Le-Khac, Phuc H. and Healy, Graham and Smeaton, Alan F.},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Machine learning, Computational modeling, Contrastive learning, Data models, deep learning, Feature extraction, Learning systems, machine learning, Natural language processing, representation learning, self-supervised learning, Task analysis, unsupervised learning},
	pages = {193907--193934},
	file = {全文:C\:\\Users\\Administrator\\Zotero\\storage\\622GXE9H\\Le-Khac 等 - 2020 - Contrastive Representation Learning A Framework a.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Administrator\\Zotero\\storage\\VXVPIRZ8\\9226466.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\7MS8LJIE\\Le-Khac 等 - 2020 - Contrastive Representation Learning A Framework a.pdf:application/pdf},
}

@misc{li_masked_2024,
	title = {Masked {Modeling} for {Self}-supervised {Representation} {Learning} on {Vision} and {Beyond}},
	url = {http://arxiv.org/abs/2401.00897},
	abstract = {As the deep learning revolution marches on, self-supervised learning has garnered increasing attention in recent years thanks to its remarkable representation learning ability and the low dependence on labeled data. Among these varied self-supervised techniques, masked modeling has emerged as a distinctive approach that involves predicting parts of the original data that are proportionally masked during training. This paradigm enables deep models to learn robust representations and has demonstrated exceptional performance in the context of computer vision, natural language processing, and other modalities. In this survey, we present a comprehensive review of the masked modeling framework and its methodology. We elaborate on the details of techniques within masked modeling, including diverse masking strategies, recovering targets, network architectures, and more. Then, we systematically investigate its wide-ranging applications across domains. Furthermore, we also explore the commonalities and differences between masked modeling methods in different fields. Toward the end of this paper, we conclude by discussing the limitations of current techniques and point out several potential avenues for advancing masked modeling research. A paper list project with this survey is available at https://github.com/Lupin1998/Awesome-MIM.},
	language = {en},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {Li, Siyuan and Zhang, Luyuan and Wang, Zedong and Wu, Di and Wu, Lirong and Liu, Zicheng and Xia, Jun and Tan, Cheng and Liu, Yang and Sun, Baigui and Li, Stan Z.},
	month = jan,
	year = {2024},
	note = {arXiv:2401.00897 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Li 等 - 2024 - Masked Modeling for Self-supervised Representation.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\EY8VHIED\\Li 等 - 2024 - Masked Modeling for Self-supervised Representation.pdf:application/pdf},
}

@article{bengio_representation_2013,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	volume = {35},
	issn = {1939-3539},
	shorttitle = {Representation {Learning}},
	url = {https://ieeexplore.ieee.org/document/6472238/?arnumber=6472238},
	doi = {10.1109/TPAMI.2013.50},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	number = {8},
	urldate = {2024-09-16},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = aug,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Machine learning, Feature extraction, Learning systems, representation learning, unsupervised learning, Abstracts, autoencoder, Boltzmann machine, Deep learning, feature learning, Manifolds, neural nets, Neural networks, Speech recognition},
	pages = {1798--1828},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Administrator\\Zotero\\storage\\KKQ8SE4A\\6472238.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\CQUIKMXG\\Bengio 等 - 2013 - Representation Learning A Review and New Perspect.pdf:application/pdf},
}

@inproceedings{feng_self-supervised_2019,
	address = {Long Beach, CA, USA},
	title = {Self-{Supervised} {Representation} {Learning} by {Rotation} {Feature} {Decoupling}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953870/},
	doi = {10.1109/CVPR.2019.01061},
	abstract = {We introduce a self-supervised learning method that focuses on beneﬁcial properties of representation and their abilities in generalizing to real-world tasks. The method incorporates rotation invariance into the feature learning framework, one of many good and well-studied properties of visual representation, which is rarely appreciated or exploited by previous deep convolutional neural network based self-supervised representation learning methods. Speciﬁcally, our model learns a split representation that contains both rotation related and unrelated parts. We train neural networks by jointly predicting image rotations and discriminating individual instances. In particular, our model decouples the rotation discrimination from instance discrimination, which allows us to improve the rotation prediction by mitigating the inﬂuence of rotation label noise, as well as discriminate instances without regard to image rotations. The resulting feature has a better generalization ability for more various tasks. Experimental results show that our model outperforms current state-of-the-art methods on standard self-supervised feature learning benchmarks.},
	language = {en},
	urldate = {2024-09-21},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Feng, Zeyu and Xu, Chang and Tao, Dacheng},
	month = jun,
	year = {2019},
	pages = {10356--10366},
	file = {Feng 等 - 2019 - Self-Supervised Representation Learning by Rotatio.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\7AZVNUM4\\Feng 等 - 2019 - Self-Supervised Representation Learning by Rotatio.pdf:application/pdf},
}

@article{singh_explainable_2024,
	title = {Explainable rotation-invariant self-supervised representation learning},
	volume = {13},
	issn = {22150161},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2215016124004102},
	doi = {10.1016/j.mex.2024.102959},
	language = {en},
	urldate = {2024-09-21},
	journal = {MethodsX},
	author = {Singh, Devansh and Marathe, Aboli and Roy, Sidharth and Walambe, Rahee and Kotecha, Ketan},
	month = dec,
	year = {2024},
	pages = {102959},
	file = {Singh 等 - 2024 - Explainable rotation-invariant self-supervised rep.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\JDLH6KR9\\Singh 等 - 2024 - Explainable rotation-invariant self-supervised rep.pdf:application/pdf},
}

@misc{su_ri-mae_2024,
	title = {{RI}-{MAE}: {Rotation}-{Invariant} {Masked} {AutoEncoders} for {Self}-{Supervised} {Point} {Cloud} {Representation} {Learning}},
	shorttitle = {{RI}-{MAE}},
	url = {http://arxiv.org/abs/2409.00353},
	abstract = {Masked point modeling methods have recently achieved great success in self-supervised learning for point cloud data. However, these methods are sensitive to rotations and often exhibit sharp performance drops when encountering rotational variations. In this paper, we propose a novel RotationInvariant Masked AutoEncoders (RI-MAE) to address two major challenges: 1) achieving rotation-invariant latent representations, and 2) facilitating self-supervised reconstruction in a rotation-invariant manner. For the first challenge, we introduce RI-Transformer, which features disentangled geometry content, rotation-invariant relative orientation and position embedding mechanisms for constructing rotationinvariant point cloud latent space. For the second challenge, a novel dual-branch student-teacher architecture is devised. It enables the self-supervised learning via the reconstruction of masked patches within the learned rotation-invariant latent space. Each branch is based on an RI-Transformer, and they are connected with an additional RI-Transformer predictor. The teacher encodes all point patches, while the student solely encodes unmasked ones. Finally, the predictor predicts the latent features of the masked patches using the output latent embeddings from the student, supervised by the outputs from the teacher. Extensive experiments demonstrate that our method is robust to rotations, achieving the state-of-the-art performance on various downstream tasks.},
	language = {en},
	urldate = {2024-09-21},
	publisher = {arXiv},
	author = {Su, Kunming and Wu, Qiuxia and Cai, Panpan and Zhu, Xiaogang and Lu, Xuequan and Wang, Zhiyong and Hu, Kun},
	month = aug,
	year = {2024},
	note = {arXiv:2409.00353 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Su 等 - 2024 - RI-MAE Rotation-Invariant Masked AutoEncoders for.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\FYLXQKHU\\Su 等 - 2024 - RI-MAE Rotation-Invariant Masked AutoEncoders for.pdf:application/pdf},
}

@inproceedings{wang_dense_2021,
	address = {Nashville, TN, USA},
	title = {Dense {Contrastive} {Learning} for {Self}-{Supervised} {Visual} {Pre}-{Training}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578497/},
	doi = {10.1109/CVPR46437.2021.00304},
	abstract = {To date, most existing self-supervised learning methods are designed and optimized for image classiﬁcation. These pre-trained models can be sub-optimal for dense prediction tasks due to the discrepancy between image-level prediction and pixel-level prediction. To ﬁll this gap, we aim to design an effective, dense self-supervised learning method that directly works at the level of pixels (or local features) by taking into account the correspondence between local features. We present dense contrastive learning (DenseCL), which implements self-supervised learning by optimizing a pairwise contrastive (dis)similarity loss at the pixel level between two views of input images.},
	language = {en},
	urldate = {2024-09-21},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	month = jun,
	year = {2021},
	pages = {3023--3032},
	file = {Wang 等 - 2021 - Dense Contrastive Learning for Self-Supervised Vis.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\8WARPITK\\Wang 等 - 2021 - Dense Contrastive Learning for Self-Supervised Vis.pdf:application/pdf},
}

@misc{spurr_peclr_2022,
	title = {{PeCLR}: {Self}-{Supervised} {3D} {Hand} {Pose} {Estimation} from monocular {RGB} via {Equivariant} {Contrastive} {Learning}},
	shorttitle = {{PeCLR}},
	url = {http://arxiv.org/abs/2106.05953},
	abstract = {Encouraged by the success of contrastive learning on image classification tasks, we propose a new self-supervised method for the structured regression task of 3D hand pose estimation. Contrastive learning makes use of unlabeled data for the purpose of representation learning via a loss formulation that encourages the learned feature representations to be invariant under any image transformation. For 3D hand pose estimation, it too is desirable to have invariance to appearance transformation such as color jitter. However, the task requires equivariance under affine transformations, such as rotation and translation. To address this issue, we propose an equivariant contrastive objective and demonstrate its effectiveness in the context of 3D hand pose estimation. We experimentally investigate the impact of invariant and equivariant contrastive objectives and show that learning equivariant features leads to better representations for the task of 3D hand pose estimation. Furthermore, we show that standard ResNets with sufficient depth, trained on additional unlabeled data, attain improvements of up to 14.5\% in PA-EPE on FreiHAND and thus achieves state-of-the-art performance without any task specific, specialized architectures. Code and models are available at https://ait.ethz.ch/projects/2021/PeCLR/},
	language = {en},
	urldate = {2024-09-21},
	publisher = {arXiv},
	author = {Spurr, Adrian and Dahiya, Aneesh and Wang, Xi and Zhang, Xucong and Hilliges, Otmar},
	month = aug,
	year = {2022},
	note = {arXiv:2106.05953 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Spurr 等 - 2022 - PeCLR Self-Supervised 3D Hand Pose Estimation fro.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\BI4AN7K5\\Spurr 等 - 2022 - PeCLR Self-Supervised 3D Hand Pose Estimation fro.pdf:application/pdf},
}
