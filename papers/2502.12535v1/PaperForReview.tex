% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}


%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{tabularray}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{array}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{appendix}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
% \usepackage{xeCJK}  % remove at official submission


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage{xcolor}
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{0} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2025}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Learning Transformation-Isomorphic Latent Space for \\ Accurate Hand Pose Estimation}  % regression / hpe task

\author{Kaiwen Ren, Lei Hu, Zhiheng Zhang, Yongjing Ye, Shihong Xia\\
University of Chinese Academic of Science, Institute of Computing Technology, CAS\\
Beijing, China\\
{\tt\small \{renkaiwen23s, hulei19z, zhangzhiheng20g, yeyongjing, xsh\}@ict.ac.cn}
% \and
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
% Representation learning has facilitated vision-based hand pose estimation by achieving higher accuracy and faster convergence. However, existing representation learning methods for regression tasks in computer vision often suffer from one of the following cases: (i) The features extracted from images are of high semantic levels, which are inadequate for regressing low-level information, (ii) The extracted features contain task-irrelevant information, making them less compact and interfering with regression tasks. % Hand pose estimation, as a regression task, is affected by these issues.

% To address the existing challenges of representation learning in regression tasks, we propose {\rm\textbf{TI-Net}} (\textbf{T}ransformation \textbf{I}somorphism \textbf{Net}work), a highly versatile visual backbone designed to construct a transformation isomorphic latent space. This ensures that the latent features capture compact, low-level information beneficial for the pose estimation task. Specifically, we model the geometry transformations in the latent space using linear transformations and enforce \text{TI-Net} to align them with the geometry transformations in the image space. We evaluate {\rm TI-Net} on the hand pose estimation task using the popular DexYCB and InterHand2.6M datasets, demonstrating the superiority of our network. On the DexYCB dataset, {\rm TI-Net} achieves an improvement of $10\%$ in the PA-MPJPE metric compared to specialized state-of-the-art (SOTA) hand pose estimation methods. Our code will be released in the future.
Vision-based regression tasks, such as hand pose estimation, have achieved higher accuracy and faster convergence through representation learning. However, existing representation learning methods often encounter the following issues: the high semantic level of features extracted from images is inadequate for regressing low-level information, and the extracted features include task-irrelevant information, reducing their compactness and interfering with regression tasks. To address these challenges, we propose {\rm\textbf{TI-Net}}, a highly versatile visual \textbf{Net}work backbone designed to construct a \textbf{T}ransformation \textbf{I}somorphic latent space. Specifically, we employ linear transformations to model geometric transformations in the latent space and ensure that {\rm TI-Net} aligns them with those in the image space. This ensures that the latent features capture compact, low-level information beneficial for pose estimation tasks. We evaluated {\rm TI-Net} on the hand pose estimation task to demonstrate the network's superiority. On the DexYCB dataset, {\rm TI-Net} achieved a 10\% improvement in the PA-MPJPE metric compared to specialized state-of-the-art (SOTA) hand pose estimation methods. Our code will be released in the future.

% 从RGB视图的手部姿态估计是人机交互的重要功能，在动作捕捉、VR等领域有着重要作用。基于表征学习的方式进行手部姿态估计能够提升模型的泛化性并加快训练收敛。但是在面向回归目标的计算机视觉任务时，现有的表征学习方法训练的网络一般存在以下的问题：(i) 从图像中提取的特征的语义层次太高，无法从中回归低层次的信息；(ii) 从图像中抽取的特征标识不够紧凑，会对回归任务产生干扰。手部姿态估计作为回归任务同样受到这些影响。针对表征学习在回归任务中的现有问题，我们提出了{\rm TI-Net}(\textbf{T}ransformation \textbf{I}somorphism Network)，通过约束让网络产生具有几何变换consistency的隐空间，提升表征学习方法在回归任务上的表现。我们将{\rm TI-Net}在人手姿态估计任务上进行了实验，在主流的DexYCB和InterHand2.6M数据集上进行的实验验证了我们网络的优越性。{\rm TI-Net}在DexYCB数据集上测得的PA-MPJPE指标相比于特化的手部姿态估计SOTA工作提升了XXX。我们代码开源于XXXX。
\end{abstract}

%%%%%%%%% BODY TEXT

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/compare.pdf}
    \caption{In the pretraining phase, (I) Contrastive learning approaches attract positive pairs and repel negative pairs.\cite{chen_simclr_2020,le-khac_contrastive_2020} (II) Masked image modeling approach reconstructs the image from the embedding of the original one.\cite{he_masked_2021,li_masked_2024} (III) TI-Net ensures that the transformation relationships in the image space also hold in the latent space, as does the combined result of transformations. We refer to this property as ``transformation isomorphism.''}
    \label{img:compare}
\end{figure}

\section{Introduction}
\label{sec:intro}

\begin{figure*}[th]
    \centering
    \includegraphics[width=\linewidth]{figs/overview.pdf}
    \caption{Overview of transformation isomorphism. \textbf{Left}: The relationships among three transformations in the image space: horizontal flip, rotation, and the horizontal flip + rotation. Any two of these transformations can be composed to form another transformation, and the rotation inherently includes the identity transformation. \textbf{Right}: In the pose space, there are three transformations that correspond exactly to the three transformations in the image space, and they satisfy the same combination rules. We refer to this perfect correspondence as transformation isomorphism. \textbf{Center}: TI-Net ensures that there exists transformations in latent space that correspond to the ones in image space. Due to the equivalence property of the isomorphism, the transformations in the latent space also correspond to those in the pose space, and satisfying the same combination rules.}
    % Overview of transformation isomorphism. \textbf{左}：图象关于水平翻转、旋转和水平翻转加旋转三种变换的关系，这三种变换的任意两种可以复合形成另外一种变换，同时旋转变换隐含了恒等变换。\textbf{右}：姿态空间中存在与图像上的三种变换完全对应的三种变换，他们也满足与图像空间上的变换完全相同的复合关系。这种完全对应的关系我们称为transformation isomorphism。\textbf{中}：TI-Net使得隐空间上的特征向量也存在与图像空间上的三种变换完全对应的变换，由于同构关系的等价性，隐空间中的变换也与姿态空间中的变换完全对应，且满足完全相同的复合关系。
    \label{img:overview}
\end{figure*}

% 人手姿态估计的任务定义和重要性
Hand pose estimation is a crucial component of human-computer interaction systems, serving as a foundational technology that enables natural and intuitive communication between users and machines through the interpretation of dynamic hand gestures. Vision-based hand pose estimation methods have been widely adopted due to their convenience in AR/VR, image and video understanding, and skeleton-based action recognition. % Recovering hand poses from a single RGB view is currently a primary focus of research in the academic community.
% 人手姿态估计是人机交互系统中的重要环节。基于RGB图象的人手姿态估计方法由于其便捷性得到了广泛的应用，如AR/VR、图象/视频理解、行为理解等。其中从单个RGB视图中恢复人手姿态是目前学术界研究的重心。

% 困难
The primary challenges in estimating hand pose from single-view RGB images include severe self-occlusion and pose diversity. The hand's finer and more complex structure, compared to the human body, increases the likelihood of mutual occlusions among various parts and contributes to a broader range of pose variations. Mainstream hand pose estimation methods can be broadly categorized into two types: task-specific architectures and representation learning approaches. Task-specific architectures involve specially designed networks and branches that directly address the complexities of hand pose estimation, such as creating specialized network structures and utilizing tailored training data to improve accuracy \cite{Yu_2023_CVPR,Ren_2023_ICCV,Li_2022_CVPR,Hampali_2022_CVPR_Kypt_Trans,Zhang_2021_ICCV}. However, these methods typically require a substantial amount of supervised data, which is often costly and labor-intensive to acquire. % Furthermore, capturing accurate 3D hand poses in outdoor environments presents significant challenges, and the pose data collected in laboratory settings tends to have limited variation.
% 单视图的RGB图象的人手姿态估计面临的主要困难是手部的严重自遮挡以及姿态的多样性。相比于人体的结构人手具有更加精细和复杂的结构，更加容易产生部位之间的相互遮挡，也会产生相比于人体更加多样的姿态效果。主流的手部姿态估计方法主要分为两类。第一类直接针对手部姿态估计的难点进行特化的设计，通过设计特殊的网络结构、训练数据，提升手部姿态估计的准确度\cite{Yu_2023_CVPR,Ren_2023_ICCV,Li_2022_CVPR,Hampali_2022_CVPR_Kypt_Trans,Zhang_2021_ICCV}。这一类方法一般需要使用大量的监督数据，但是采集大量的3D手部姿态数据对是复杂繁重的，此外在户外场景中很难采集到准确的3D数据，而在实验室场景中采集的姿态数据存在分布单一的问题。

Representation learning approaches \cite{hrnet_sun2019deep,xiao2018simple,xu2022vitpose,xu2022vitpose+,spurr_peclr_2022,lin_pre-training_2024}  first learn features from a large amount of unlabeled data, enabling faster convergence and preventing overfitting to specific datasets. Subsequently, these methods regress the pose from the learned features during the fine-tuning phase. As illustrated in \cref{img:compare}, representation learning methods can be categorized into Masked Image Modeling(MIM)-based and contrastive learning-based approaches \cite{li_masked_2024}. The features extracted by such models may contain rich information about texture, lighting, and color, focusing on image reconstruction rather than pose estimation. Contrastive learning-based methods extract task-relevant features by ensuring that positive sample pairs yield similar feature representations, achieving notable success in high-level tasks such as classification and understanding \cite{HU2024128645,technologies9010002}.  However, since poses are sensitive to image transformations, the extracted features are less suitable for hand pose estimation.

% The second type of method involves using representation learning \cite{hrnet_sun2019deep,xiao2018simple,xu2022vitpose,xu2022vitpose+,spurr_peclr_2022,lin_pre-training_2024}. In this approach, the network initially learns features that are \emph{beneficial for hand pose estimation} from a large amount of unlabeled data. Subsequently, it regresses the pose from these learned features during the fine-tuning phase. Compared to the first class of methods, the advantage of representation learning lies in the features learned in the first stage, enabling better and faster convergence. Moreover, by pretraining on diverse unlabeled data, the network mitigates the risk of overfitting to specific datasets.  % Compared to the first class of method, the advantage of this approach is that it avoids overfitting the network to specific dataset distributions, thereby enhancing the generalization ability of the network. Additionally, the representation learning in the first stage allows the network to learn the target pose more quickly, improving training efficiency.
% 第二类方法是使用表征学习的方法\cite{hrnet_sun2019deep,xiao2018simple,xu2022vitpose,xu2022vitpose+,spurr_peclr_2022,lin_pre-training_2024}，首先让网络从大量的无标签数据中学习\emph{对手部姿态估计有帮助}的特征，然后在finetuning的阶段中从这样的特征中回归姿态。相比于第一类方法，这样做的优势在于避免了网络过拟合到特定的数据集分布上，提升了网络的泛化性能。此外，第一阶段的表征学习能够使得网络更快地学习到目标姿态，从而提升训练的效率。

% However, representation learning methods also encounter certain challenges. As noted in \cite{li_masked_2024}, representation learning methods can be divided into Masked Image Modeling-based (MIM-based) and contrastive learning-based approaches, as shown in the (I) and (II) parts of \cref{img:compare}. MIM-based methods train the network to reconstruct the images from their features, enabling the model to learn connections between different parts of the image. The features extracted by such models may contain rich information about texture, lighting, and color, focusing on image reconstruction rather than pose estimation. Contrastive learning-based methods, on the other hand, focus on extracting features that are pertinent to the task by ensuring that positive sample pairs yield similar feature representations. Current mainstream contrastive learning frameworks emphasize the extraction of features that remain invariant under image transformations, achieving notable success in high-level tasks such as classification and understanding.\cite{HU2024128645,technologies9010002} However, poses are sensitive to image transformations, which renders features extracted by such contrastive learning frameworks less suitable for hand pose estimation.
% 但是基于表征学习的方法同样存在挑战。根据\cite{li_masked_2024}，表征学习的方法分为MIM-based的方法和contrastive learning-based的方法。MIM-based的方法训练网络从图象特征中恢复图象本身，让模型学习图象不同位置之间的关联信息。这样的模型提取额图像特征中存在材质、光照、颜色等对姿态估计没有帮助的冗余信息，可能导致姿态信息的丢失或者对姿态的估计产生干扰。Contrastive learning-based的方法通过让正样本对产生``相似''的特征来让模型抽取包含任务所关注的信息的特征。目前主流的对比学习框架关注于抽取对于图像变换不变的特征，这类特征在于分类、理解等高层次的任务上取得了瞩目的成果。但是姿态并非是关于图像特征不变的，这种对比学习的框架抽取的特征不能够很好地适用于手部姿态估计。

% 我们的工作……与我们最相近的工作……
We observe a significant relationship, termed \emph{transformation isomorphism}, between image space and pose space. Transformations applied to the image correspond to those in the pose, as illustrated in \cref{img:overview}. For instance, applying a horizontal flip to the image makes the hand pose in the flipped image equivalent to the flipped version of the original pose. Furthermore, the algebraic structure of transformations in pose space mirrors that in image space, establishing an one-to-one correspondence.
% We propose an important relationship between the image space and the pose space: there exists a \emph{transformation isomorphism} between them. As illustrated in \cref{img:overview}, transformations applied to the image correspond to those applied to the pose. For example, when a horizontal flip transformation is applied to the image, the hand pose in the flipped image becomes equivalent to the flipped version of the original pose. Furthermore, the algebraic structure formed by the transformations in the pose space corresponding to the transformations in the image space has the same structure as the algebraic structure formed by the transformations in the image space, establishing a one-to-one correspondence.
% 我们发现了图象空间和姿态空间之间存在的重要联系：图像空间和姿态空间之间存在\emph{transformation isomorphism}。如\cref{img:overview}所示，图象的上的变换和姿态的转变化是对应的，\eg 对图象施加水平翻转变换之后图象中的手部的姿态等于原来的图象中的姿态进行水平翻转，且根据这样的对应关系得到的与图象上的变换对应的姿态空间中的变换构成的代数结构与图象上的变换构成的代数结构具有相同的结构——可交换的群。

Based on the above observation, we propose \textbf{TI-Net} (``Ti'' stands for \textbf{t}ransformation \textbf{i}somorphism) to embed images into a latent space that maintains a transformation isomorphism relationship with both the image and pose spaces, as depicted in \cref{img:compare}. Because geometric transformations cannot be directly defined in the latent space, it is crucial to organize the latent space to establish an isomorphism with the image space, thereby necessitating the definition of geometric transformations within the latent space. To achieve this, we utilize auxiliary linear transformations to model latent transformations aligned with those in the image space, updating them alongside TI-Net during pre-training. In the fine-tuning stage, these auxiliaries are discarded, enabling TI-Net's seamless integration into existing frameworks.  Therefore, TI-Net not only enhances pose estimation accuracy but also leverages vast unlabeled data to improve generalization capabilities. Our main contributions are as follows:
% 基于这样的观察，我们提出了TI-Net(``Ti'' stands for \textbf{t}ransformation \textbf{i}somorphism)，一种能够将图象嵌入到与图像空间和姿态空间之间存在transformation isomorphism关系的隐空间的网络。我们发现在隐空间中无法直接定义几何变换，所以我们提出使用一组低自由度的满秩线性变换。在表征学习的过程的同时更新这些线性变换，一方面通过这组线性变换刻画了隐空间的同构关系结构，另一方面约束TI-Net将图象嵌入到这样的该空间中。借助TI-Net能够显著地提升姿态估计的精度，同时由于使用大量的无标签数据，该网络能够取得更好的泛化能力。我们的主要贡献如下：
\begin{itemize}
    \item We present TI-Net to effectively capture the transformation isomorphism relationships among input space, latent space, and target space, which can enhance pose estimation performance. % 提出了TI-Net，...
    \item TI-Net can be seamlessly integrated into other pose estimation models by replacing the classic vision backbone with ours, requiring minimal modifications. % Improving the accuracy with minimum modification.
    \item We contend that in representation learning for regression tasks, transformation-consistent features are more effective at accurately capturing task-relevant information compared to transformation-invariant features. % 我们认为在进行回归任务的表示学习中，变换一致性的特征比变换不变性的特征更加能够准确捕捉任务所关注的信息。
    % \item We propose to exploit the geometry transformation isomorphism relationship to accurately capture the associations between the input space, latent space, and target space. This relationship is beneficial for pose estimation. % 提出了geometry transformation isomorphism，准确地捕捉了图象空间、隐空间和姿态空间之间对姿态估计有利的关系；
    \item Our proposed method achieves SOTA performance on the pose estimation task. On the DexYCB dataset, it improves PA-MPJPE by approximately $10\%$ compared to the specially designed architecture \cite{Zhou_2024_CVPR}, and achieves an improvement of 1.49mm in MPJPE compared to the SOTA representation learning-based pose estimation method\cite{spurr_peclr_2022}. % 在姿态估计任务上去的了XXX的效果，相比于sota提升了XXX
\end{itemize}

%------------------------------------------------------------------------
\section{Related works}
\label{sec:related}

\subsection{Hand pose estimation}

% TODO：罗列最近的单目手部工作
Mainstream research in hand pose estimation concentrates on estimating hand poses from single-view RGB images or videos \cite{mmpose2020,Lugaresi2019MediaPipeAF,xu2022vitpose,xu2022vitpose+,Hampali_2022_CVPR_Kypt_Trans,10.1109/TPAMI.2023.3247907,10655481,10.1007/978-3-031-20068-7_22,moon_bringing_2023,blur_oh2023recovering3dhandmesh,ego_Prakash2024Hands,lin_pre-training_2024,spurr_peclr_2022}. ViTPose\cite{xu2022vitpose,xu2022vitpose+} successfully integrates Vision Transformer\cite{vit_dosovitskiy2021an} into the pose estimation domain, achieving real-time performance and high precision in 2D pose estimation. Furthermore, the application of advanced architectures such as HRNet\cite{hrnet_sun2019deep,mmpose2020} in 3D hand pose estimation has resulted in significant progress.

A substantial body of work addresses specific challenges in hand pose estimation. For instance, InterWild\cite{moon_bringing_2023} targets the estimation of inter-hand poses in in-the-wild scenarios, \cite{blur_oh2023recovering3dhandmesh} tackles pose estimation in blurred settings, and \cite{ego_Prakash2024Hands} emphasizes first-person hand pose estimation. Concurrently, recent methodologies have emerged that utilize self-supervised learning for hand pose estimation tasks. For example, \cite{lin_pre-training_2024} adopts a 2D pose detector for retrieving similar pose images, allowing the backbone to focus on extracting pose-related features. PeCLR\cite{spurr_peclr_2022} constructs positive samples through geometric transformations, yielding higher accuracy compared to the traditional contrastive learning method for images\cite{chen_simclr_2020}. Other approaches exploit the projection relationship between 3D poses and 2D poses, training networks for pose estimation via self-supervised learning \cite{10.1109/TPAMI.2023.3247907,epipolartransformers,zheng2023hamuco,spurr_peclr_2022}.
% 主流的手部姿态估计工作聚焦于从单视图RGB图象或视频中估计手部姿态\cite{mmpose2020,Lugaresi2019MediaPipeAF,xu2022vitpose,xu2022vitpose+,Hampali_2022_CVPR_Kypt_Trans,10.1109/TPAMI.2023.3247907,10655481,10.1007/978-3-031-20068-7_22,moon_bringing_2023,blur_oh2023recovering3dhandmesh,ego_Prakash2024Hands,lin_pre-training_2024,spurr_peclr_2022}。ViTPose\cite{xu2022vitpose,xu2022vitpose+}将ViT\cite{vit_dosovitskiy2021an}引入到姿态估计领域实现了实时高精度的二维姿态估计。将HRNet\cite{hrnet_sun2019deep,mmpose2020}等改进的网络架构应用到三维手部姿态估计中的工作也取得了令人影响深刻的进展。一方面大量的工作聚焦于解决手部姿态估计容易遇到的问题。例如InterWild\cite{moon_bringing_2023}关注in-the-wild场景下的双手姿态的估计问题，\cite{blur_oh2023recovering3dhandmesh}处理了模糊场景下的姿态估计问题，\cite{ego_Prakash2024Hands}着重于第一人称的手部姿态估计问题。另一方面，近期涌现出了基于自监督方式进行学习的手部姿态估计方案。例如\cite{lin_pre-training_2024}使用2D pose detector进行相似姿态图象检索，使得backbone聚焦于提取姿态特征。PeCLR\cite{spurr_peclr_2022}利用几何变换构造正样本，取得了相比于vanilla的图像对比学习方法SimCLR\cite{chen_simclr_2020}更高的精度。另一类方法利用3D姿态和2D姿态之间的投影关系直接利用自监督学习训练网络获得姿态估计的能力\cite{10.1109/TPAMI.2023.3247907,epipolartransformers,zheng2023hamuco,spurr_peclr_2022}。

Our work, in contrast, adopts a data-driven approach utilizing a purely classical architecture, ResNet\cite{He_2016_CVPR}. By exploiting the isomorphism between input space and latent space, we achieve state-of-the-art (SOTA) performance without employing task-specific pruning designs.

\subsection{Representation learning}

Representation learning seeks to automatically extract valuable latent or representations from raw data. This process allows models to transform complex, high-dimensional data---such as images, text, or audio---into low-dimensional, compact representations that can be effectively utilized for tasks including classification, regression, and clustering. According to \cite{li_masked_2024}, mainstream representation learning methods can be broadly classified into masked image modeling (MIM) methods and contrastive learning methods.
% 表示学习旨在从原始数据中自动提取有效特征或表示。通过表示学习，模型能够将复杂的高维数据（如图像、文本或音频）转换为低维的、紧凑的表示，这些表示可以用于分类、回归、聚类等任务。根据\cite{li_masked_2024}，主流的表示学习方法分为基于Masked Image Modeling的方法和基于Contrastive Learning的方法。

MIM methods enable models to learn the relationships among different regions of an image by reconstructing masked areas based on unmasked regions \cite{he_masked_2021,zhou_ibot_2022,xie2021simmim,Xie_2023_CVPR,Chang_2022_CVPR,Wang_2023_CVPR}. MAE\cite{he_masked_2021} synergizes masked image modeling with ViT \cite{vit_dosovitskiy2021an} to facilitate efficient masked pretraining. Building on the approach established in \cite{he_masked_2021}, numerous representation learning strategies have emerged using masked models. For instance, iBOT \cite{zhou_ibot_2022} incorporates contrastive learning into MIM to capture high-level semantic features, while P-STMO \cite{avidan_p-stmo_2022} extends masked modeling from image space to pose space to learn pose sequence priors.
% 基于MIM的方法使模型通过未被mask的区域恢复mask区域的方式学习图象不同区域之间的关联信息\cite{he_masked_2021,zhou_ibot_2022,xie2021simmim,Xie_2023_CVPR,Chang_2022_CVPR,Wang_2023_CVPR}。例如\cite{he_masked_2021}将图像块mask与ViT\cite{vit_dosovitskiy2021an}结合进行高效率的掩码预训练。基于\cite{he_masked_2021}延伸出了更多利用掩码模型进行表示学习的工作，例如iBOT\cite{zhou_ibot_2022}在MIM学习的基础上引入了对比学习的思想以学习高层语义特征，P-STMO\cite{avidan_p-stmo_2022}将掩码模型从图像空间引入到了姿态空间以学习姿态序列先验。

Contrastive learning methods enable models to learn features beneficial for downstream tasks by bringing closer the representations for positive samples with similar properties \cite{chen_simclr_2020,wang_dense_2021,Mo_2023_WACV,Zheng_2021_ICCV}. \cite{chen_simclr_2020} proposes a foundational framework for contrastive learning within image domain, establishing a baseline for visual contrastive learning. \cite{Zheng_2021_ICCV} treats images belonging to the same class, rather than merely different transformations of the same image, as positive samples for contrastive learning, thus enhancing the model's ability to extract high-level semantic features. Furthermore, \cite{10.1007/978-3-031-19772-7_3} employs 2D poses in contrastive learning, thereby aligning it more effectively with pose estimation tasks. \cite{wang_dense_2021} applies contrastive learning to dense prediction tasks, achieving improvements in object detection, semantic segmentation, and instance segmentation.
% 基于CL的方法使得模型对具有相同性质的正样本产生的特征接近来学习对下游任务有利的特征\cite{chen_simclr_2020,wang_dense_2021,Mo_2023_WACV,Zheng_2021_ICCV}。\cite{chen_simclr_2020}提出了一种在图像空间上进行对比学习的简单框架，奠定了视觉对比学习的baseline。\cite{Zheng_2021_ICCV}将相同类别的图象而不是一张图像的不同变换结果作为正样本进行对比学习，提升网络的高层语义提取能力。\cite{10.1007/978-3-031-19772-7_3}将2D pose用于对比学习，使其更适用于姿态估计领域。\cite{wang_dense_2021}提出将CL应用于稠密预测任务，在对object detection、semantic seg.和instance seg.任务上取得了提升。

% 得益于重建任务对低层信息的抽取能力和CL任务对data-specific特征的学习能力，我们提出将这两点特征结合，在重建任务学习抽取丰富的低层特征的同时利用CL任务构造更加合理的特征空间结构。

% \begin{figure*}[th]
%     \centering
%     \includegraphics[width=\linewidth]{figs/overview.pdf}
%     \caption{Overview of transformation isomorphism. \textbf{Left}: The relationships among three transformations in the image space: horizontal flip, rotation, and the horizontal flip + rotation. Any two of these transformations can be composed to form another transformation, and the rotation inherently includes the identity transformation. \textbf{Right}: In the pose space, there are three transformations that correspond exactly to the three transformations in the image space, and they satisfy the same composite relationships. We refer to this perfect correspondence as transformation isomorphism. \textbf{Center}: TI-Net ensures that the feature embeddings in the latent space have transformations that correspond exactly to the three transformations in the image space. Due to the equivalence property of the isomorphism, the transformations in the latent space also correspond to those in the pose space, satisfying the same composite relationships.}
%     % Overview of transformation isomorphism. \textbf{左}：图象关于水平翻转、旋转和水平翻转加旋转三种变换的关系，这三种变换的任意两种可以复合形成另外一种变换，同时旋转变换隐含了恒等变换。\textbf{右}：姿态空间中存在与图像上的三种变换完全对应的三种变换，他们也满足与图像空间上的变换完全相同的复合关系。这种完全对应的关系我们称为transformation isomorphism。\textbf{中}：TI-Net使得隐空间上的特征向量也存在与图像空间上的三种变换完全对应的变换，由于同构关系的等价性，隐空间中的变换也与姿态空间中的变换完全对应，且满足完全相同的复合关系。
%     \label{img:overview}
% \end{figure*}

\subsection{Learning invariant attributes}

Contrastive learning-based representation learning frameworks generally incorporate a set of transformations applied to the input data, aimed at ensuring that the resulting latent capture information highly relevant to the task. Taking image input as an example, in a contrastive learning framework for high-level tasks (\eg, classification, REID, understanding), this set of transformations often consists of low level transformations like rotations, cropping, scaling, color jittering, noise addition and others. These transformations are crafted to preserve the high-level semantic information contained within the images. Consequently, we expect the backbone to extract \emph{invariant} latent with respect to such transformations under these training frameworks.
% 基于CL的表示学习框架通常包含一组对输入的变换，这一组变换与框架的设计旨在让特征中包含与任务强相关的信息。以图象输入为例，在面向高层次任务（如分类、理解）的对比学习中这一组变换通常由旋转、裁切、缩放、色彩打乱、添加噪点等变换组成，这样的变换对于图象中的高层语义信息不产生影响，相应地在这些框架中也希望backbone对于这样的图象产生\emph{不变invariant}的特征。

Research by \cite{singh_explainable_2024} advocates enabling the network to estimate rotation angles directly from images, while \cite{feng_self-supervised_2019} suggests that the network extract directionally and rotation-invariant features simultaneously. These approaches allow the network to extract high-level semantic information that invariant to rotation, thereby enhancing classification performance. HandCLR \cite{lin_pre-training_2024}, which targeting the pose estimation backbone, introduces the concept of constructing positive samples by retrieving images with analogous poses using a 2D pose detector. This strategy allows the latent extraction network to capture more critical pose information. The aforementioned works concentrating on extracting \textit{transformation invariant} latent, extracting \textit{similar} latent for images subject to different augmentations.

PeCLR\cite{spurr_peclr_2022}, contrarily to \cite{lin_pre-training_2024, singh_explainable_2024, su_ri-mae_2024, feng_self-supervised_2019}, posits that augmentations applied to the image should distinctly affect the corresponding latent representations, resulting in \textit{different} latent for images with varying augmentations. We propose that these different latent should maintain the same structure as images under pre-defined transformations. As shown in (III) of \cref{img:compare}, latent and image compel to same structure with respect to ``flip'', ``rot'' and ``flip+rot''. We refer to this property on latent as \textit{transformation consistency}.

% \cite{singh_explainable_2024} proposes enabling the network to directly estimate rotation angles from images, while \cite{feng_self-supervised_2019} further suggests that the network should extract direction and rotation-invariant features simultaneously. These approaches allow the network to extract high-level semantic information that is invariant to rotation, improving performance on classification tasks. HandCLR \cite{lin_pre-training_2024}, targeting the pose estimation backbone, proposes constructing positive samples by retrieving images with similar poses using a 2D pose detector, enabling the feature extraction network to capture more essential pose information. PeCLR \cite{spurr_peclr_2022} introduces the idea that the augmentations on the image should also affect the corresponding latent. Such innovation enhanced accuracy in hand pose estimation tasks.
% \cite{singh_explainable_2024}提出让网络直接从图像中估计旋转角度，\cite{feng_self-supervised_2019}进一步地提出让网络学习图象的旋转角度和旋转无关的特征，从而使得网络提取旋转无关的高层语义信息，提升了在分类任务上的表现性能。PeCLR\cite{spurr_peclr_2022}提出让特征抽取网络$\mathcal F$对于图象变换$A$是``可交换的''：$\mathcal F\circ A'=A\circ\mathcal F$，提升在手部姿态任务的准确度。HandCLR\cite{lin_pre-training_2024}针对姿态估计backbone，提出了利用2D pose detector检索相似的姿态图象构造正样本，使得更加特征抽取网络能够抽取更加本质的姿态信息。

% \cite{lin_pre-training_2024, singh_explainable_2024, su_ri-mae_2024, feng_self-supervised_2019} focus on extracting \emph{transformation invariant} features, which are beneficial for high-level tasks. However, regression tasks are more closely related to low-level image features, as the orientation and direction of the image are strongly associated with the target of the regression task. For example, a 15° image rotation results in a corresponding 15° rotation of the root joint of hand within the image. We claim this property as ``\textit{transformation consistent}''.% PeCLR\cite{spurr_peclr_2022}, considering this characteristic, combines it with features extracted by backbone to construct \emph{transformation consistent} features, leading to improved performance in pose estimation tasks.
% \cite{lin_pre-training_2024, singh_explainable_2024, su_ri-mae_2024, feng_self-supervised_2019}等工作都是在抽取对于一组\emph{变换不变（transformation invariant）的特征}，这一类特征对于高层次任务是有益的。但是回归任务与图象的低层特征更加密切，图象的朝向、方向与回归任务的目标密切相关，例如图象旋转15°会导致图像中手部跟关节也旋转15°，所以他们的方法可能不适用于回归任务。而PeCLR\cite{spurr_peclr_2022}在工作中考虑到了这一特征，并将之与CNN抽取的特征结合，构造了\emph{变换一致（transformation consistency）的特征}，在姿态估计任务上取得了提升。

Transformation-consistent latent are capable of better capturing the structure of the hand pose space compared to transformation-invariant features, owing to the transformation-consistency relationship between the image and pose space. By enforcing transformation invariance on latent, the vision backbone inadvertently discard pose-relevant information during the pretraining, leading to inferior estimation accuracy. TI-Net addresses this issue by enforcing transformation isomorphism constraints through the integration of masked image modeling (MIM) and contrastive learning, thereby producing features imbued with transformation consistency and achieving improved accuracy.
% trans cons的特征比trans inv的特征能够更好地刻画手部姿态空间的性质，因为目标空间和输入空间之间本身就存在trans cons。强制特征trans inv可能使得姿态相关的信息在extract的过程中丢失，而丢失的信息无法再使用网络恢复，从而降低手部姿态估计的精确度。TI-Net通过结合MIM和对比学习实现trans iso约束，使得提取的特征具有trans cons的性质，以取得更好的手部姿态估计精度。

%Benefiting from the advantage of reconstruction task that enabling the backbone to extract low-level information and the the advantage of contrastive learning task that enabling the backbone to learn task-specific features, TI-Net is able to learn extracting features with rich and compact information about hand pose effectively. While the reconstruction task extracts rich low-level features, the contrastive learning task ensures a more rational feature space structure. Specifically, the key to representation learning is to identify and construct \emph{invariants}. For regression tasks, constructing \emph{transformation-invariant} features neglects low-level information, while \emph{transformation consistency} of the feature space can accurately capture the properties of the feature space desired by regression tasks. By integrating contrastive learning, we construct and learn a latent space based on transformation isomorphism. Compared to PeCLR \cite{spurr_peclr_2022}, the latent space we learn is more rational and offers better interpretability.
% 得益于重建任务对低层信息的抽取能力和CL任务对data-specific特征的学习能力，我们提出将这两点特征结合，在重建任务学习抽取丰富的低层特征的同时利用CL任务构造更加合理的特征空间结构。具体而言，表示学习的关键在于寻找和构造\emph{不变量}。而对于回归任务而言，构造\emph{变换不变的特征}忽略了低层信息，而\emph{特征空间的结构不变/变换一致}则能够准确刻画回归任务所希望的特征空间的性质。结合对比学习的方式，我们了在变换同构的基础上构造和学习隐空间。相比于PeCLR\cite{spurr_peclr_2022}，我们学习的隐空间更加完备且具有更好的可解释性。

%------------------------------------------------------------------------

\section{Method}
\label{sec:method}

In this section, we introduced the architecture and training framework for TI-Net. Our main contributions focus on pretraining TI-Net to obtain transformation isomorphism. While in finetuning stage we introduce 3D annotated hand pose datasets\cite{Moon_2020_ECCV_InterHand2.6M,chao:cvpr2021_dexycb} to condect supervised learning.

In \cref{sec:method-overview} we detail the idea of transformation isomorphism. Then in \cref{sec:learning-ti-latent} we depict the construction and training framework for TI-Net. Finally in \cref{sec:rotation-embedding} we explain how we encode the rotation parameter into latent transformations. % In \cref{sec:simple-baesline} we present the transformation group with four simple transformations. In \cref{sec:incorporating} we incorporate rotation transformation into the previous group to adapt more cases.

\subsection{Modeling}
\label{sec:method-overview}

As shown in \cref{img:overview}, \textit{transformation isomorphism} naturally holds between image space and pose space, with corresponding transformations in image and target space obeying same transformation property with respect to elements in image and target space. It is crucial to notice two important prerequisites of transformation isomorphism:
\begin{enumerate}
    \item Correspondence: there is one-to-one correspondence for transformations in two spaces. The corresponding transformations often share the same semantic.
    \item Consistency: corresponding transformations have follow the same properties, i.e., $f$ and $g$ are corresponding transformation, if $f$ is idempotent then $g$ should also be idempotent.
\end{enumerate}

In our scenario of hand pose estimation, image space and pose space are in transformation isomorphism, with horizontal flipping, rotation, and horizontal flipping+rotation transformation shown in \cref{img:overview} in red, blue and yellow boxes. We denoted transformations in image and pose space as $\mathsf G_{\mathbb I}$ and $\mathsf G_{\mathbb T}$, the isomorphism is denoted as $\mathsf G_{\mathbb I} \cong \mathsf G_{\mathbb T}$.

It is intuitive to hypothesis that will it be possible to organize the latent space $\mathbb L$ and construct transformations for it so that the latent space is transformation isomorphism to image space $\mathsf G_{\mathbb I} \cong \mathsf G_{\mathbb L}$. And the transformation isomorphism among input, latent and target space will follow instantly, termed as $\mathsf G_{\mathbb I} \cong \mathsf G_{\mathbb L} \cong \mathsf G_{\mathbb T}$.

Organizing latent space is the task of TI-Net, the construction of transformations in latent space is a remaining yet important part. In our case, three simple but generic transformations are introduced for image space: $\mathsf G_{\mathbb I}=\{\mathcal H, \mathcal R, \mathcal {HR}\}$, meaning horizontal flipping, rotation, and horizontal flipping+rotation transformation respectively as visualized in \cref{img:overview}. By hypothesis, the corresponding transformations in latent space denoted as $\mathsf G_{\mathbb L}=\{\mathcal T_H, \mathcal T_R, \mathcal T_{HR}\}$ should follow correspondence and consistency prerequisites.

Now, our goal is clear: (1) to find the specific latent transformations $\mathcal T_H, \mathcal T_R, \mathcal T_{HR}$ in latent space $\mathbb L$ that are correspond and consistent to $\mathcal H, \mathcal R, \mathcal {HR}$ in image space $\mathbb I$, (2) to train TI-Net encoding images into latent space where consistency prerequisite of transformations holds.

% We notice that in \emph{low-level regression tasks}, there exists a strong geometric relationship between the target space (pose space, orientation space, position space and etc.) and the input space. For example, in the orientation estimation task, flipping the image upside down leads to an inversion of the object’s orientation, and in the pose estimation task, a diagonal mirroring of the image can cause the root joint to rotate by 180°.
% 我们注意到在\emph{低层次的回归任务}中，回归目标空间（姿态空间、朝向空间、位置空间）与原始的输入图象空间之间存在较强的几何联系。例如在朝向估计任务中图象的上下翻转会导致物体朝向的上下翻转，而在姿态估计任务中图象的中心镜像也会导致根关节旋转180°。
% $\mathbb I = (\mathbb R^2)^{\mathbb R^3}$

% We refer to the corresponding relationship of transformations between image and pose space as \emph{transformation isomorphism}, as shown in \cref{img:overview}. Intuitively, transformation isomorphism between the image space and the pose space means that transformations applied to the image will have equivalent effects on the pose itself. Formally, let the ideal image space be denoted as $\mathbb I $, and the set of transformations on the image space, such as flips and rotations, be denoted as $\mathsf G_{\mathbb I}$. The target space of the task is denoted as $\mathbb T$, and the set of transformations on the target space is denoted as $\mathsf G_{\mathbb T}$. The image space and target space are \emph{transformation isomorphic} if and only if there exists a bijection $\mathscr F: \mathsf G_{\mathbb I} \to \mathsf G_{\mathbb T}$, written as $\mathsf G_{\mathbb I} \cong_{\mathsf G} \mathsf G_{\mathbb T}$. This definition is reasonable because transformations applied to the image itself directly affect the ground truth of the regression task, thus asserting that the transformations of the image and those of the target space are in \emph{one-to-one correspondence}.
% 我们称这样的关系为“\emph{几何变换同构}”。形式化地，记理想图像空间$\mathbb I=(\mathbb R^2)^{\mathbb R^3}$，图像空间上的翻转、旋转等几何变换集合记作$\mathsf G_{\mathbb I}$。回归任务的输出目标空间记作$\mathbb T$，目标空间上的几何变换集合记作$\mathsf G_{\mathbb T}$。图像空间和目标空间是“几何变换同构”的当且仅当存在双射$\mathcal H:\mathsf G_{\mathbb I}\to\mathsf G_{\mathbb T}$，记作$\mathsf G_{\mathbb I}\cong_{\mathsf G}\mathsf G_{\mathbb T}$。这一定义是合理的，因为由于对图象的本身的几何变换会直接对回归任务的真值产生影响，所以可以断言图象的几何变换和目标空间的几何变换是\emph{一一对应}的。
% For low-level regression tasks, since the regression targets should adapt with the transformation of the image, the features extracted by the backbone should not be transformation invariant but reflect the transformations on the image stably. In this case, the task head can more efficiently predict the regression target by utilizing this consistency. Furthermore, we believe that if the latent space also has a \emph{transformation isomorphism} relationship with the image space, it can improve the performance of downstream regression tasks.

% In the classical Encoder-Decoder architecture, an image is first compressed by the encoder into a one-dimensional feature vector or a low-resolution two-dimensional feature map, and then being decoded into task-required output from the features. For high-level classification tasks, the features extracted by the encoder need to contain more semantic information \emph{invariant to transformations}. Contrarily, for low-level regression tasks, since the regression targets should adapt with the transformation of the image, the features learned by the encoder should not be transformation invariant but reflect the geometric transformations on the image stably. In this case, the decoder can more efficiently decode the regression target by utilizing this consistency. Furthermore, we believe that if the latent space also has a \emph{transformation isomorphism} relationship with the image space, it can improve the performance of downstream regression tasks.
% 在经典的Encoder-Decoder架构中，图象首先被Encoder压缩为一维特征向量或低分辨率的二维特征图，然后Decoder从特征中解码出任务需求的数据。对于高层次的分类任务，Encoder提取的特征需要包含更多的语义信息，这一部分语义信息是对\emph{几何变换不变}的，即无论图像本身经过怎样的几何变换，其中的有关高层语义的特征数值不应发生改变（例如分类、身份、年龄等信息）。而对于低层的回归任务，由于回归目标往往随着图象的变换而变换，所以Encoder所学习的特征不能够是几何不变的，而应当能够反映几何变换对图象的影响。如果几何变换对图象地影响反映到特征上时是稳定的、有规律的（这样的性质称为\emph{几何变换一致性}），那么Decoder就能够利用一致性更加高效地解码回归目标。进一步地，我们认为如果能够让Encoder抽取的图象特征构成的隐空间也具有与图像空间“几何变换同构”的关系，那么就够提升下游回归任务的表现。

% Formally, let the latent space be denoted as $\mathbb L = \mathbb R^d$. If the latent space is transformation isomorphic to the image space, i.e., $\mathsf G_{\mathbb L} \cong_{\mathsf G} \mathsf G_{\mathbb I}$, then, based on the equivalence of transformation isomorphism (which is trivial to prove), the latent space is also transformation isomorphic to the target space, i.e., $\mathsf G_{\mathbb L} \cong_{\mathsf G} \mathsf G_{\mathbb T}$.
% 形式化地，记隐空间为$\mathbb L=\mathbb R^d$，如果隐空间与图像空间几何变换同构$\mathsf G_{\mathbb L}\cong_{\mathsf G}\mathsf G_{\mathbb I}$，根据几何变换同构关系的等价性（这一点不难证明），那么隐空间也与目标空间是几何变换同构的$\mathsf G_{\mathbb L}\cong_{\mathsf G}\mathsf G_{\mathbb T}$。
% We carefully select geometric transformations on the images so that they satisfy the definition of group, making the network easier to train.
% When defining the set of geometric transformations $\mathsf G$, we define the transformations to form a closed group. This constraint helps to construct a well-defined transformation structure and also aids in the construction of the loss function.
% 在定义几何变换集合$\mathsf G$时，我们定义其中的变换构成封闭的群。这样的约束有助于构造良好的变换结构，也有助于损失函数的构造。

% -------------------------------------------------------------------

\begin{table}[!t]
    \centering
    \belowrulesep=0pt\aboverulesep=0pt
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l|lll}
    \toprule
    \multicolumn{1}{c|}{$\circ$} & $\mathcal{H}$   & $\mathcal{R}_{\beta_1}$  & $\mathcal{HR}_{\beta_2}$ \\ \midrule
    $\mathcal{H}$             & $\mathcal{R}_0$ & $\mathcal{HR}_{\beta_1}$ & $\mathcal{R}_{\beta_2}$ \\
    $\mathcal{R}_{\alpha_1}$  & $\mathcal{HR}_{-\alpha_1}$ & $\mathcal{R}_{\alpha_1+\beta_1}$ & $\mathcal{HR}_{-\alpha_1+\beta_2}$ \\
    $\mathcal{HR}_{\alpha_2}$ & $\mathcal{R}_{-\alpha_2}$ & $\mathcal{HR}_{\alpha_2+\beta_1}$ & $\mathcal{R}_{-\alpha_2+\beta_2}$ \\
    \bottomrule
    \end{tabular}
    \caption{Combination results for transformation set incorporating rotations. Note that communicative property is no longer held in this case.}
    \label{tab:complete_transform}
\end{table}
\subsection{Learning transformation isomorphism latent}
\label{sec:learning-ti-latent}

% We first here construct a simple group of transformation $\mathsf G_{\mathbb I}$ to illustrate the core concept of our TI-Net. In \cref{sec:incorporating_with_rotations} we will complete the $\mathsf G_{\mathbb I}$ with rotation transformation.

Our objective is to train a vision backbone $\mathcal E_\theta: \mathbb I \to \mathbb L$, such that the transformation set $\mathsf G_{\mathbb I}$ and the corresponding transformation group $\mathsf G_{\mathbb L}$ in the latent space satisfy $\mathsf G_{\mathbb I} \cong_{\mathsf G} \mathsf G_{\mathbb L}$. Such $\mathsf G_{\mathbb I}$ forms a group, the proof can be found in supplement material. To train such vision backbone that extracts transformation isomorphism latent space, we introduce three lightweight latent transformation networks, $\mathcal T_H^\gamma, \mathcal T_R^\eta, \mathcal T_{HR}^\lambda$ to model $\mathcal T_H, \mathcal T_R, \mathcal T_{HR}$ and guide the isomorphism relation between spaces ($\gamma,\eta,\lambda$ are learnable parameters).
% 我们的目的是训练一个上游特征抽取器$\mathcal E_\theta:\mathbb I\to\mathbb L$（其中$\theta$为可训练参数），使得在几何变换集合$\mathsf G_{\mathbb I}=\{\mathcal I,\mathcal H,\mathcal V,\mathcal O\}$以及隐空间中对应的“几何”变换群$\mathsf G_{\mathbb L}$下满足$\mathsf G_{\mathbb I}\cong_{\mathsf G}\mathsf G_{\mathbb L}$，其中$\mathcal I,\mathcal H,\mathcal V,\mathcal O$分别是图象的恒等变换、水平翻转、上下翻转和中心镜像。为了训练这样一个上游特征抽取器，我们需要额外地训练三个轻量的特征变换网络$\mathcal H_\gamma,\mathcal V_\eta,\mathcal O_\lambda:\mathbb L\to\mathbb L$，分别建模隐空间上除了恒等变换之外的三种“几何”变换。

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/train.pdf}
    \caption{Simplified overview of pretraining phase. Weights of latent transformation are updated jointly with TI-Net. We depict only one ordinary and one secondary constraint here for simplicity.}
    \label{fig:train}
\end{figure}

To mitigate overfitting the latent transformations within the latent space, we use low-degree-of-freedom linear transformations $H, R, B \in \mathbb{R}^{d \times d}$ to construct the latent transformation $\mathcal T_H^\gamma, \mathcal T_R^\eta, \mathcal T_{HR}^\lambda$. Taking the $\mathcal T_H^\gamma$ as an example, it is formulated as $H=E + M_HN_H$, where $\gamma = \{M_H, N_H\}$, $M_H, N_H^\top \in \mathbb{R}^{d \times r}$, and $r \ll d$, $E$ being the identity matrix. This construction not only reduces the risk of overfitting the latent space transformations but also ensures that the transformation is likely to be full-rank, aligning with the property that the flip transformation $\mathcal H$ is a full-rank linear transformation.
% 我们避免人工设定隐空间中“几何”变换的形式，从而防止训练不收敛，同时为了避免隐空间的特征变换过拟合，我们利用低自由度的线性变换$H,V,O\in\mathbb R^{d\times d}$构造特征变换网络$\mathcal H_\gamma,\mathcal V_\eta,\mathcal O_\lambda$。以特征空间的“水平翻转”$\mathcal H_\gamma$为例，我们将其利用线性变换$E+A_HB_H,\gamma=\{A_H,B_H\}$构造，其中$A_H,B_H^\top\in\mathbb R ^{d\times r},r\ll d$，$E$为单位方阵。这样的构造在保证了隐空间上的变换不会过拟合的同时确保了变换自身大概率是满秩的，这与翻转变换$\mathcal H$是满秩线性变换的性质对应。

We extend the notation of $\mathcal R$ to $\mathcal R_\alpha$, which means rotating the image around its center for $\alpha$ degree counterclockwise and $\mathcal {HR}_\alpha$ as horizontally flipping then rotating the image, so does $\mathcal T_R^\eta(\cdot;\alpha)$ and $\mathcal T_{HR}^\lambda(\cdot;\alpha)$. The combination relations between every pair of elements in $\mathsf G_{\mathbb I}$ are shown in \cref{tab:complete_transform} (proof in supplement material). By transformation isomorphism $\mathsf G_{\mathbb I}\cong_{\mathsf G}\mathsf G_{\mathbb L}$, we assert members of all parameterized latent transformations $\mathsf G_{\mathbb L}=\{\mathcal T_H^\gamma, \mathcal T_R^\eta, \mathcal T_{HR}^\lambda\}$ should also obey the same combination relations. We utilized such obedience to construct our pretraining framework.
% Given that the transformations $\mathsf G_{\mathbb I}$ on the image satisfy the combination rules are shown in left of \cref{tab:simple_transform}. If $\mathsf G_{\mathbb L} \cong_{\mathsf G} \mathsf G_{\mathbb I}$, then the transformations in the latent space should also satisfy the same rules as shown in right of \cref{tab:simple_transform}.
% 已知图像上的几何变换群$\mathsf G_{\mathbb I}$满足如下\cref{tab:simple_transform}的二元结合运算关系：
% \begin{table}[!t]
%     \centering
%     \belowrulesep=0pt\aboverulesep=0pt
%     \renewcommand{\arraystretch}{1.2}
%     \small

%     \begin{tabular}{c|cccc}
%     \toprule
%     \multicolumn{1}{c|}{$\circ$} & $\mathcal{I}$ & $\mathcal{H}$ & $\mathcal{V}$ & $\mathcal{O}$ \\ \hline
%     $\mathcal{I}$ & $\mathcal{I}$ & $\mathcal{H}$ & $\mathcal{V}$ & $\mathcal{O}$ \\
%     $\mathcal{H}$ & $\mathcal{H}$ & $\mathcal{I}$ & $\mathcal{O}$ & $\mathcal{V}$ \\
%     $\mathcal{V}$ & $\mathcal{V}$ & $\mathcal{O}$ & $\mathcal{I}$ & $\mathcal{H}$ \\
%     $\mathcal{O}$ & $\mathcal{O}$ & $\mathcal{V}$ & $\mathcal{H}$ & $\mathcal{I}$ \\
%     \bottomrule
%     \end{tabular}\hspace{0.2cm}
%     \begin{tabular}{c|cccc}
%     \toprule
%     \multicolumn{1}{c|}{$\circ$} & $\mathcal{I}$ & $\mathcal{H}_{\gamma}$ & $\mathcal{V}_{\eta}$ & $\mathcal{O}_{\lambda}$ \\ \hline
%     $\mathcal{I}$ & $\mathcal{I}$ & $\mathcal{H}_{\gamma}$ & $\mathcal{V}_{\eta}$ & $\mathcal{O}_{\lambda}$ \\
%     $\mathcal{H}_{\gamma}$ & $\mathcal{H}_{\gamma}$ & $\mathcal{I}$ & $\mathcal{O}_{\lambda}$ & $\mathcal{V}_{\eta}$ \\
%     $\mathcal{V}_{\eta}$ & $\mathcal{V}_{\eta}$ & $\mathcal{O}_{\lambda}$ & $\mathcal{I}$ & $\mathcal{H}_{\gamma}$ \\
%     $\mathcal{O}_{\lambda}$ & $\mathcal{O}_{\lambda}$ & $\mathcal{V}_{\eta}$ & $\mathcal{H}_{\gamma}$ & $\mathcal{I}$ \\
%     \bottomrule
%     \end{tabular}

%     \caption{Combination results for simple transformations on image and latent space(left and right). $\mathcal{I,H,V,O}$ on the left stands for identity, horizontal flip, vertical flip and diagonal mirroring transformation of image. Parameterized version on the right stands for the transformation of latent respectively. $\circ$ denotes the combination of two transformations, \eg $\mathcal {H}\circ \mathcal {V} (I) = \mathcal V(\mathcal H(I)) = \mathcal \mathcal O(I)$.}
%     \label{tab:simple_transform}
% \end{table}
% 若$\mathsf G_{\mathbb L}\cong_{\mathsf G}\mathsf G_{\mathbb I}$，则隐空间上的“几何变换”也应该满足同样的关系，如\cref{tab:simple_latent_transform}所示。
% \begin{table}[h!]
%     \centering
%     \caption{Combination results for simple transformations on latent space.}
%     \label{tab:simple_latent_transform}
%     \begin{tabular}{l|llll}
%     \toprule
%     $\circ$ & $\mathcal{I}$ & $\mathcal{H}_\gamma$ & $\mathcal{V}_\eta$ & $\mathcal{O}_\lambda$ \\ \midrule
%     $\mathcal{I}$ & $\mathcal{I}$ & $\mathcal{H}_\gamma$ & $\mathcal{V}_\eta$ & $\mathcal{O}_\lambda$ \\
%     $\mathcal{H}_\gamma$ & $\mathcal{H}_\gamma$ & $\mathcal{I}$ & $\mathcal{O}_\lambda$ & $\mathcal{V}_\eta$ \\
%     $\mathcal{V}_\eta$ & $\mathcal{V}_\eta$ & $\mathcal{O}_\lambda$ & $\mathcal{I}$ & $\mathcal{H}_\gamma$ \\
%     $\mathcal{O}_\lambda$ & $\mathcal{O}_\lambda$ & $\mathcal{V}$ & $\mathcal{H}_\gamma$ & $\mathcal{I}$ \\
%     \bottomrule
%     \end{tabular}
% \end{table}

% Under the premise of transformation isomorphism, the feature extractor $\mathcal E_\theta$ should satisfy the following constraints termed ordinary constraints:
% % 满足几何变换同构的前提下，特征抽取器$\mathcal E_\theta$应该满足如下的一阶约束：
% \begin{equation}
%     \left\{\begin{aligned}
%         \mathbf 0 &=\mathcal E_\theta(\mathcal H(I))-\mathcal H_\gamma(\mathcal E_\theta(I)) \\
%         \mathbf 0 &=\mathcal E_\theta(\mathcal V(I))-\mathcal V_\eta(\mathcal E_\theta(I)) \\
%         \mathbf 0 &=\mathcal E_\theta(\mathcal O(I))-\mathcal O_\lambda(\mathcal E_\theta(I)) \\
%     \end{aligned}\right.
%     \label{eq:simple-ord}
% \end{equation}

% Theoretically, if the network can perfectly satisfy the ordinary constraints in \cref{eq:simple-ord}, it will ensure that the latent space and image space are transformation isomorphic. However, to improve the efficiency of network learning, secondary constraints are introduced during training to further ensure the transformation isomorphism between the latent space and image space:
% % 理论上，如果网络能够良好地学习一阶约束就能够使得隐空间和图像空间在几何变换上同构。但是为了提升网络的学习效率，可以在训练时引入二阶的约束引导隐空间和图像空间同构关系的形成：
% \begin{equation}
%     \left\{\begin{aligned}
%         \mathbf 0 &=\mathcal E_\theta(I)-\mathcal H_\gamma \circ \mathcal H_\gamma(\mathcal E_\theta(I)) \\
%         \mathbf 0 &=\mathcal E_\theta(\mathcal O(I))-\mathcal H_\gamma \circ \mathcal V_\eta(\mathcal E_\theta(I)) \\
%         \mathbf 0 &=\mathcal E_\theta(\mathcal V(I))-\mathcal H_\gamma \circ \mathcal O_\lambda(\mathcal E_\theta(I)) \\
%         &\cdots
%     \end{aligned}\right.
% \end{equation}

The key to learning a transformation isomorphism latent space is the obedience to relations in \cref{tab:complete_transform}. Besides updating vision backbone TI-Net, $\mathcal T_H^\gamma, \mathcal T_R^\eta$ and $\mathcal T_{HR}^\lambda$ should also be updated jointly. We define our loss function as follows:
% The loss function during training is defined as follows:
% 训练时的损失函数定义如下：
\begin{equation}
    \mathcal L_{\rm TI} :=\mathcal L_{\rm classic} + w(\mathcal L_{\rm ord} + \mathcal L_{\rm sec}),
\end{equation}
here, $\mathcal L_{\rm classic}$ represents the traditional reconstruction loss, enforcing image reconstruction from the latent of the original and transformed images using auxiliary network $\mathcal D_\Theta^{\rm aux}$ with learnable parameter $\Theta$. $w$ is a weight coefficient, setting to $w = 0.001$. $\mathcal L_{\rm ord}$ and $\mathcal L_{\rm sec}$ corresponds to the loss for the ordinary and secondary constraints:
% 其中$\mathcal L_{\rm classic}$为传统的重建损失，对原始图象以及三个翻转结果的特征进行图象重建约束。$\alpha$为权重系数，我们取$\alpha=0.001$。$\mathcal L_{\rm ord}$为一阶约束对应的损失：
\begin{equation}
    \left\{
    \begin{aligned}
        &\begin{aligned}
        \mathcal L_{\rm ord} :=\,
            & \Vert \mathcal E_\theta(\mathcal H(I))-\mathcal T_H^\gamma(\mathcal E_\theta(I)) \Vert\, + \\
            & \Vert \mathcal E_\theta(\mathcal R_\alpha(I))-\mathcal T_R^\eta(\mathcal E_\theta(I);\alpha) \Vert\, + \\
            & \Vert \mathcal E_\theta(\mathcal {HR}_\beta(I))-\mathcal T_{HR}^\lambda(\mathcal E_\theta(I);\beta) \Vert ,
        \end{aligned} \\
        &\begin{aligned}
        \mathcal L_{\rm sec} :=\,
            & \Vert \mathcal E_\theta(I)-\mathcal T_H^\gamma (\mathcal T_H^\gamma(\mathcal E_\theta(I))) \Vert\, + \\
            & \Vert \mathcal E_\theta(\mathcal {HR}_\omega(I))-\mathcal T_R^\eta(\mathcal T_H^\gamma(\mathcal E_\theta(I));\omega) \Vert + \cdots,
        \end{aligned}
    \end{aligned}
    \right.
\end{equation}
% and $\mathcal L_{\rm sec}$ corresponds to the loss for the secondary constraints:
% $\mathcal L_{\rm sec}$为二阶约束对应的损失：
$\alpha,\beta,\omega$ are randomly sampled from $[0,2\pi]$. The term ``ordinary'' means only 1 latent transformation is applied to the latent, and the term ``secondary'' means 2 latent transformations are applied to the latent one after the other. Theoretically, we can combine arbitrary $N=2,3,\cdots$ latent transformations to get $N$-\textit{th} loss, but it is practically infeasible.

\cref{fig:train} illustrates the training framework with one ordinary constraints $\Vert \mathcal E_\theta(\mathcal H(I))-\mathcal T_H^\gamma(\mathcal E_\theta(I)) \Vert=\mathbf 0$ and one secondary constraint $\Vert \mathcal E_\theta(\mathcal {HR}_\omega(I))-\mathcal T_R^\eta(\mathcal T_H^\gamma(\mathcal E_\theta(I));\omega) \Vert=\mathbf 0$. Readers can refer to \cref{code:pretraining} for complete pretraining procedure.
% \cref{fig:train} illustrates the training framework after introducing rotation transformations into the transformation isomorphism relationship (see \cref{sec:incorporating_with_rotations} for details). We ensure that the transformed latent is closer to the latent extracted after applying transformations to the images. Additionally, a reconstruction task is introduced to prevent latent collapse.

\begin{algorithm}[!t]
    \small
    \caption{Transformation isomorphism learning}
    \label{code:pretraining}
    \begin{algorithmic}[1]
        \State \textbf{Input:} input image $I$.
        \State \textbf{Output:} $\mathcal L_{\rm TI}$.
        \State $\mathcal L_{\rm ord}=0$ \Comment{Ordinary loss}
        \State Randomly sample $\alpha,\beta\sim\mathcal U(0,2\pi)$
        \State $\mathcal T_R^\eta=\mathcal T_R^\eta(\cdot;\alpha), \mathcal T_{HR}^\lambda=\mathcal T_{HR}^\lambda(\cdot;\beta)$
        \For {$(\mathcal F,\mathcal T^\phi)$ in $\{(\mathcal H,\mathcal T_H^\gamma),(\mathcal R,\mathcal T_R^\eta),(\mathcal {HR},\mathcal T_{HR}^\lambda)\}$}
            \State $\mathcal L_{\rm ord}=\mathcal L_{\rm ord}+
                \Vert
                    \mathcal E_\theta(\mathcal F(I)) - \mathcal T^\phi(\mathcal E_\theta(I))
                \Vert
            $
        \EndFor
        \State $\mathcal L_{\rm sec}=0$ \Comment{Secondary loss}
        \State Randomly sample $\alpha,\beta\sim\mathcal U(0,2\pi)$
        \State $\mathcal T_R^\eta=\mathcal T_R^\eta(\cdot;\alpha), \mathcal T_{HR}^\lambda=\mathcal T_{HR}^\lambda(\cdot;\beta)$
        \For {$(\mathcal F_1,\mathcal T^\phi_1,\mathcal F_2,\mathcal T^\psi_2)$ in $\{(\mathcal H,\mathcal T_H^\gamma),(\mathcal R,\mathcal T_R^\eta),(\mathcal {HR},\mathcal T_{HR}^\lambda)\}^2$}
            \State $\mathcal L_{\rm sec}=\mathcal L_{\rm sec}+
                \Vert
                    \mathcal E_\theta(\mathcal F_1 \circ \mathcal F_2 (I)) -
                    \mathcal T^\phi_2(\mathcal T^\psi_1(\mathcal E_\theta(I)))
                \Vert
            $
        \EndFor
        \State $\mathcal L_{\rm classic}=0$ \Comment{Reconstruction loss}
        \For {$\mathcal F$ in $\{\mathcal H,\mathcal R,\mathcal {HR}\}$}
            \State $\mathcal L_{\rm classic}=\mathcal L_{\rm classic}+\Vert \mathcal D^{\rm aux}_{\Theta}(\mathcal E_\theta(\mathcal F(I))) - \mathcal F(I) \Vert$
        \EndFor
        \State $\mathcal L_{\rm TI}=\mathcal L_{\rm classic}+w(\mathcal L_{\rm ord}+\mathcal L_{\rm sec})$ \Comment{TI loss}
    \end{algorithmic}
\end{algorithm}

% -------------------------------------------------------------------

\subsection{Rotation embedding}
\label{sec:rotation-embedding}

% Horizontal flips, vertical flips, and diagonal mirroring define the structure of the latent space, but they are too simple for common image transformations. To enable the latent space of capturing more general structures of both image space and target space, we need to introduce rotation transformations $\mathcal R_\alpha$ into the geometric transformation group $\mathsf G_{\mathbb I}$, where $\mathcal R_\alpha$ denotes a counterclockwise rotation of the image by $\alpha$ degrees around its center.
% 水平翻转、垂直翻转和中心镜像构造了隐空间的结构，但是对于常见的图象变换而言还是太简单了。为了使得隐空间能够捕捉更加通用的图像空间和目标空间的结构，还需要在几何变换群$\mathsf G_{\mathbb I}$中引入旋转变换$\mathcal R_\alpha$，表示图象绕中心点逆时针旋转$\alpha$度。

% To ensure that the expanded transformation set $\mathsf G'_{\mathbb I}$, which includes $\mathsf G_{\mathbb I}$, still satisfies the group definition, we construct it as $\mathsf G'_{\mathbb I} = \{\mathcal H, \mathcal R_\alpha, \mathcal{HR}_\alpha\}$:
% 为了使得引入旋转之后的变换集合$\mathsf G'_{\mathbb I}$在包含$\mathsf G_{\mathbb I}$的基础上仍然满足群的性质，我们这样构造$\mathsf G'_{\mathbb I}=\{\mathcal H,\mathcal R_\alpha,\mathcal{HR}_\theta\}$：
% \begin{itemize}
%     \item $\mathcal H$: represents the horizontal flip, as before.
%     \item $\mathcal R_\alpha$: represents a counterclockwise rotation by $\alpha$ degrees around the image center.
%     \item $\mathcal{HR}_\alpha$: represents a horizontal flip followed by a counterclockwise rotation by $\alpha$ degrees.
% \end{itemize}

% The architecture incorporating rotation transformation is shown in \cref{img:overview}. The proof of $\mathsf G'_{\mathbb I}$ satisfying group definition, inclusion of $\mathsf G_{\mathbb I}$ and combination rule in \cref{tab:complete_transform} can be found in \cref{sub:proof_for_complete_transform}. %  The proof that this set satisfies group properties and includes the previous four transformations can be found in \cref{sub:proof_for_complete_transform}. The combination relations are shown in \cref{tab:complete_transform}.
% \begin{itemize}
%     \item $\mathcal H$：与之前一样表示水平翻转变换
%     \item $\mathcal R_\alpha$：绕图象中心逆时针旋转$\alpha$度
%     \item $\mathcal{HR}_\alpha$：首先进行水平翻转，然后逆时针旋转$\alpha$度
% \end{itemize}
% 满足群关系且包含之前四种变换的证明见\cref{sub:proof_for_complete_transform}。结合关系如下\cref{tab:complete_transform}所示。

Different from $\mathcal T_H$, $\mathcal T_{H}(\cdot;\alpha)$ and $\mathcal T_{HR}(\cdot;\alpha)$ are parameterized with continuous number $\alpha$. Therefore, when constructing the corresponding transformations in the latent space, \eg $\mathcal T_R^\eta(\cdot;\alpha)$ and $\mathcal T_{HR}^\lambda(\cdot;\alpha)$, the rotation parameter $\alpha$ needs to be treated as input into the transformation. The specific form of the transformation remains a linear one, but the rotation parameter is embedded and concatenated with the latent features as preprocessing. Taking the rotation transformation $\mathcal T_R^\eta(\cdot;\alpha)$ as an example:
% Unlike $\mathsf G_{\mathbb I}$, the transformation set $\mathsf G'_{\mathbb I}$ includes rotations and flip-rotation combinations with continuous parameter $\alpha$. Therefore, when constructing the corresponding transformations in the latent space, \eg $R_\eta(\cdot; \alpha)$ and $HR_\lambda(\cdot; \alpha)$, the rotation parameter needs to be incorporated into the transformation. The specific form of the transformation remains a linear one, but the rotation parameter is embedded and concatenated with the latent features. Taking the rotation transformation $R_\alpha$ as an example:
% 与$\mathsf G_{\mathbb I}$不同的是，$\mathsf G'_{\mathbb I}$中的旋转变换和翻转-旋转变换时含有连续参数$\alpha$的，因此在构造隐空间上对应的变换$R_\eta(\cdot;\alpha)$和$HR_\lambda(\cdot;\alpha)$时，需要将旋转作为也传入到变换中。变换的具体形式仍然沿用线性变换，只是在输入时将旋转参数的嵌入拼接到隐特征上，以旋转变换为例：
\begin{equation}
    \begin{aligned}
        \mathcal T_R^\eta(v;\alpha)&=v+M_RN_R\cdot \verb|cat|(v,e_r) \\
        e_r &= \mathcal K_\xi (r) \\
        r &= [\cos\alpha,\sin\alpha]^\top
    \end{aligned}
\end{equation}
Here, $\mathcal K_\xi$ represents a rotation vector embedding network shared by $\mathcal T^\eta_R(\cdot;\alpha)$ and $\mathcal T^\lambda_{HR}(\cdot;\alpha)$ with learnable parameters $\xi=\eta\cap\lambda$. It is a two-layer MLP for embedding the two-dimensional rotation direction vector into an $n$-dimensional space. $v$ denotes the latent features of the original image.
% 其中$\mathcal K_\omega$表示具有可学习参数$\omega$的旋转向量嵌入网络，是一个两层的全连接神经网络，负责将二维的旋转方向向量嵌入到$n$维的空间中。$v$表示原始图像的隐特征。$A_R\in\mathbb R^{d\times r},B_R\in\mathbb R^{r\times (d+n)}$的含义和作用和前面相同。

% The loss function $\mathcal L_\text{all}$ is defined similarly to $\mathcal L_\text{simple}$. To save memory during training, we only utilize the ordinary loss terms.
% 损失函数$\mathcal L_\text{all}$的定义与$\mathcal L_\text{simple}$相同。为了在训练时节省显存空间，我们只使用了一阶损失项参与训练。

% -------------------------------------------------------------------

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/vis_compare.pdf}
    % \begin{tabular}{>{\raggedleft\arraybackslash}m{1cm} m{1.7cm}<{\centering} m{1.7cm}<{\centering} m{1.7cm}<{\centering} m{1.7cm}<{\centering} m{1.7cm}<{\centering} m{1.7cm}<{\centering} m{1.7cm}<{\centering}}
    %      & \#1 & \#2 & \#3 & \#4 & \#5 & \#6 & \#7 \\

    %     Input
    %      & \includegraphics[width=1.5cm]{figs/vis_compare/1_pic.png}
    %      & \includegraphics[width=1.5cm]{figs/vis_compare/2_pic.png}
    %      & \includegraphics[width=1.5cm]{figs/vis_compare/3_pic.png}
    %      & \includegraphics[width=1.5cm]{figs/vis_compare/4_pic.png}
    %      & \includegraphics[width=1.5cm]{figs/vis_compare/5_pic.png}
    %      & \includegraphics[width=1.5cm]{figs/vis_compare/6_pic.png}
    %      & \includegraphics[width=1.5cm]{figs/vis_compare/7_pic.png} \\

    %     SimCLR\cite{chen_simclr_2020}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/1_sim.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/2_sim.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/3_sim.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/4_sim.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/5_sim.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/6_sim.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/7_sim.png} \\

    %     Ours
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/1_sl4.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/2_sl4.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/3_sl4.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/4_sl4.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/5_sl4.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/6_sl4.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/7_sl4.png} \\

    %     GT
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/1_gt.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/2_gt.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/3_gt.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/4_gt.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/5_gt.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/6_gt.png}
    %      & \includegraphics[width=1.7cm]{figs/vis_compare/7_gt.png} \\
    % \end{tabular}
    \caption{Visualization comparison between our approach and SimCLR\cite{chen_simclr_2020} on DexYCB\cite{chao:cvpr2021_dexycb} Our method exhibits better accuracy under occlusion scenes. TI-Net and SimCLR\cite{chen_simclr_2020} are finetuned on DexYCB\cite{chao:cvpr2021_dexycb} under the same procedure and meta-parameters. GT standards for ground truth annotations. We adjusted the viewing direction for the best comparison.}
    \label{fig:vis-compare}
\end{figure*}

\section{Experiments}

\subsection{Implementation}

We use ResNet \cite{He_2016_CVPR} as the backbone for pretraining. The network takes $224\times 224$ RGB images as input and outputs an image feature map with the size of $7\times 7\times 2048$. Based on our proposed training framework, we train the model for 50 epochs on the ImageNet-1K dataset using the AdamW optimizer. The batch size is set to 68, and the base learning rate is $1.5\times10^{-3}$. For the finetuning stage, we crop the input to $224\times 224$ as the input for TI-Net and add a 3-layer MLP to estimate the MANO pose parameters $\theta \in \mathbb{R}^{16\times 3}$ from the features.
\subsection{Datasets}

\paragraph{DexYCB}
The DexYCB\cite{chao:cvpr2021_dexycb} dataset is a large-scale benchmark designed for 3D hand pose estimation and hand-object interaction tasks. It contains synchronized RGB-D videos of human hands interacting with 20 YCB objects, providing accurate 3D annotations for both hand joints and object poses. The dataset offers diverse scenarios, including various grasp types and hand-object occlusions. It is suitable for training and evaluating models to understand complex hand movements and interactions in real-world settings.

\paragraph{InterHand2.6M}
The InterHand2.6M\cite{Moon_2020_ECCV_InterHand2.6M} dataset is a large-scale dataset specifically created for 3D hand pose estimation, featuring over 2.6 million annotated hand images. It contains both single-hand and interacting-hand scenarios, captured from multiple camera angles with diverse poses. The dataset provides high-quality 3D annotations of hand joints, making it a valuable resource for training and evaluating models aimed at accurately estimating complex hand poses, including hand-to-hand interactions.

\subsection{Evaluation metrics}

\paragraph{MPJPE}
Mean Per Joint Position Error (MPJPE) measures the average Euclidean distance between the predicted and ground truth 3D joint positions after root alignment. It is defined as:
\begin{equation}
    \text{MPJPE} = \frac{1}{N} \sum_{i=1}^{N} \| \mathbf{P}_i - \mathbf{G}_i \|_2,
\end{equation}
where \( N \) denotes the number of joints, \( \mathbf{P}_i \) represents the predicted position of the \( i \)-th joint, and \( \mathbf{G}_i \) is the corresponding ground truth position. MPJPE captures the overall accuracy of the estimated hand pose by providing a direct measure of the discrepancy between predicted and actual joint locations, with lower MPJPE values indicating better performance. It is particularly effective for assessing the precision of 3D hand pose estimation models.

\paragraph{PA-MPJPE}
Procrustes-Aligned Mean Per Joint Position Error (PA-MPJPE)\cite{Ionescu2014} is an evaluation metric for hand pose estimation that measures the average Euclidean distance between the predicted and ground truth 3D joint positions after Procrustes alignment\cite{Gower1975}. This alignment step removes variations in translation, rotation, and scale, focusing solely on the pose similarity. PA-MPJPE is defined as:
\begin{equation}
    \text{PA-MPJPE} = \frac{1}{N} \sum_{i=1}^{N} \| \hat{\mathbf{P}}_i - \mathbf{G}_i \|_2,
\end{equation}
where \( \hat{\mathbf{P}}_i \) denotes the Procrustes-aligned prediction of the \( i \)-th joint. Lower PA-MPJPE values indicate better alignment with the ground truth, making it a robust metric for assessing pose accuracy.

\subsection{Comparison with representation learning methods}

\paragraph{Quantitative comparison}
We compare the performance of our approach with current mainstream image feature networks that use representation learning for pretraining in the hand pose estimation task, as shown in \cref{tab:representation}.

\begin{table}[!t]
    \centering
    \small
    \begin{tabular}{lllc}
        \toprule
        Method & Pretraining & Backbone & MPJPE \\ \midrule
        \multirow{3}{*}{SimCLR\cite{chen_simclr_2020}} & 100DOH-1M\cite{Shan20} & ResNet-50 & 20.13 \\
         & Ego4D-1M\cite{Grauman_2022_CVPR} & ResNet-50 & 20.22 \\
         & ImageNet-1K\cite{Deng2009} & ResNet-50 & 20.32 \\
        \hline
        \multirow{2}{*}{PeCLR\cite{spurr_peclr_2022}} & 100DOH-1M\cite{Shan20} & ResNet-50 & 18.39 \\
         & Ego4D-1M\cite{Grauman_2022_CVPR} & ResNet-50 & 18.99 \\
        \hline
        % \multirow{3}{*}{HandCLR\cite{lin_pre-training_2024}} & 100DOH-1M\cite{Shan20} & - & 17.34 \\
        %  & Ego4D-1M\cite{Grauman_2022_CVPR} & - & 16.99 \\
        %  & 100D\cite{Shan20}+E4D\cite{Grauman_2022_CVPR} & - & \textbf{16.71} \\
        % \hline
        % SimCLR\cite{chen_simclr_2020} & ImageNet-1K\cite{Deng2009} & ResNet-50 & 20.32 \\
        \textbf{Ours} & ImageNet-1K\cite{Deng2009} & ResNet-50 & \textbf{16.79} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of MPJPE results with SOTA representation learning method. The evaluation metrics in the table are referenced from \cite{lin_pre-training_2024}.}
    \label{tab:representation}
\end{table}

Different pretraining tasks and datasets can have varying impacts on pose estimation results. Choi et al. \cite{choi2023rethinking} demonstrated that for pose estimation tasks, using pose images (\eg, SURREAL\cite{Varol_2017_CVPR}) for pretraining yields a backbone that achieves better accuracy compared to ones pretrained on ImageNet-1K\cite{Deng2009}. In \cref{tab:representation}, TI-Net outperforms current SOTA representation methods, even with those pretrained on hand relevant datasets.

\paragraph{Qualitative comparison}
Visualization results are shown in \cref{fig:vis-compare}. Due to transformation isomorphism between image, latent, and pose space, TI-Net achieves a more accurate hand pose as shown in cases \#2, \#6, and \#7, where major parts of the hand are visible in the image. while in occluded cases like \#1, \#3, \#4, and \#5, our approach produces more reasonable poses because the latent TI-Net extracted is more compact and contains more information about the pose, reducing the interference of irrelevant information and alleviating the lack of information in occluded regions.

\paragraph{Training efficiency}
We plot the MPJPE of every five epochs for SimCLR\cite{chen_simclr_2020} and our approach in \cref{fig:epoch-mpjpe}. As shown in the figure, TI-Net presents better training efficiency with faster convergence and more stable decrement. This indicates that SimCLR\cite{chen_simclr_2020} requires more substantial parameter adjustments during finetuning to fit regression tasks effectively. In contrast, TI-Net, by leveraging transformation isomorphism to construct a structure similar to the target space, gains faster training speed.

\subsection{Comparison with other methods}

\paragraph{Evaluation on DexYCB}
We evaluate our model on the large-scale hand-object interaction dataset, DexYCB \cite{chao:cvpr2021_dexycb}. DexYCB contains multi-view hand grasping data, making hand pose estimation more challenging due to the presence of occlusions. We assess both PA-MPJPE and MPJPE metrics, with the results shown in \cref{tab:dexycb}. It is observed that our model achieves the best PA-MPJPE results, even compared to the methods with specifically designed architecture or the ones incorporating temporal information, reaching state-of-the-art performance. This indicates that the transformation isomorphic latent space constructed by our approach has a positive impact, effectively improving the accuracy of hand pose estimation.
% 我们在大规模的手-物品交互数据集DexYCB\cite{chao:cvpr2021_dexycb}上测试我们的模型。DexYCB包含多个视角的手部抓握数据，由于遮挡的存在因此估计手部姿态（pose）更加具有挑战性。我们评估了PA-MPJPE指标和MPJPE指标，结果如\cref{tab:dexycb}所示。观察到我们的模型即使与针对手部姿态任务进行特殊设计的方法相比，仍然在PA-MPJPE上取得了最优的结果，达到了state-of-the-art的水准。这表明我们所构建的变换同构的隐空间对于姿态估计产生了正面效果，有效提升了姿态估计的准确度。

\begin{table}[!h]
    % PA好：同构学的好
    \centering
    \small
    \begin{tabular}{lccc}
        \toprule
        Method & S & PA-MPJPE & MPJPE \\ \midrule
        METRO\cite{Zhang_2019_ICCV} & \checkmark & 7.0 & 15.2   \\
        Spurr et al.\cite{Spurr2020} & \checkmark & 6.8 & 17.3  \\
        Liu et al.\cite{Liu_2021_CVPR} & \checkmark & 6.6 & 15.3  \\
        MobRecon\cite{Chen_2022_CVPR} & \checkmark & 6.4 & 14.2  \\
        HandOccNet\cite{Park_2022_CVPR} & \checkmark & 5.8 & 14.0  \\
        H2ONet\cite{Xu_2023_CVPR}${}^\star$ & \checkmark & 5.7 & 14.0   \\
        Zhou et al.\cite{Zhou_2024_CVPR} & \checkmark & 5.5 & \textbf{12.4}  \\
        Deformer\cite{Fu_2023_ICCV}${}^\star$ & \checkmark & \underline{5.2} & \underline{13.6}  \\
        \hline
        \textbf{Ours (TI-Net + 3$\times$MLP)} & \ding{55} & \textbf{4.91} & 16.8  \\
        \bottomrule
    \end{tabular}
    \caption{Comparison results on the DexYCB\cite{chao:cvpr2021_dexycb} dataset. Our method achieves the best PA-MPJPE results, indicating a more precise estimation of local hand poses. ``S'' indicates whether the work utilized a task-specifically designed network architecture for hand pose estimation. Methods remarked with ``$\star$'' incorporates temporal information.}
    \label{tab:dexycb}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/epoch_mpjpe.pdf}
    \caption{Comparison of MPJPE between our approach and SimCLR\cite{chen_simclr_2020} approach on every epoch on DexYCB\cite{chao:cvpr2021_dexycb}, aligning all training setup the same. Our approach shows faster convergence and more stable training.}
    \label{fig:epoch-mpjpe}
\end{figure}

\paragraph{Evaluation on InterHand2.6M}
InterHand2.6M is a large-scale dataset collected in the lab setting, with challenging cases of interacting hands. We evaluate TI-Net on InterHand2.6M using PA-MPJPE and MPJPE metrics and compare it with related works, as shown in \cref{tab:ih26m}. TI-Net achieves better results than Keypoint Transformer\cite{Hampali_2022_CVPR_Kypt_Trans} and InterWild\cite{moon_bringing_2023}, demonstrating its capability for accurate pose estimation. However, there is a slight inferiority between TI-Net and IntagHand\cite{Li_2022_CVPR}. We believe there are two main reasons for this phenomenon: (1) IntagHand employs a highly specialized model structure, incorporating modules like the Interacting Attention Graph, in addition to the shared ResNet50 backbone, specifically designed for the dual-hand pose estimation scenario; (2) the more severe hand occlusions and greater similarity between left and right hands in InterHand2.6M, which significantly impact the model and cannot be mitigated by the transformation isomorphic relationship.  % We attribute this to the more severe hand occlusions and greater similarity between L/R hands in InterHand2.6M, which significantly impact the model. Such occlusions cannot be mitigated by the transformation isomorphic relationship.

\begin{table}[!t]
    % PA好：同构学的好
    \centering
    \small
    \begin{tabular}{lccc}
        \toprule
        Method & S & PA-MPJPE & MPJPE \\ \midrule
        % Zimmermann et al.\cite{Zimmermann_2017_ICCV} & \checkmark & - & 36.36 \\
        % InterNet\cite{Moon_2020_ECCV_InterHand2.6M} & \checkmark & - & 16.01 \\
        InterShape\cite{Zhang_2021_ICCV} & \checkmark & - & 13.07 \\
        Keypoint Transformer\cite{Hampali_2022_CVPR_Kypt_Trans} & \checkmark & - & 12.78 \\
        InterWild\cite{moon_bringing_2023} & \checkmark & - & 11.67 \\ % 11.2/13.01 \\
        IntagHand\cite{Li_2022_CVPR} & \checkmark & - & \textbf{8.79} \\
        % EANet\cite{Park_2023_ICCV} & \checkmark & - & \textbf{5.45} \\
        \hline
        \textbf{Ours (TI-Net + 3$\times$MLP)} & \ding{55} & 4.47 & \underline{10.34} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison results on the InterHand2.6M \cite{Moon_2020_ECCV_InterHand2.6M} dataset. TI-Net exhibits relatively inferior results compared to SOTA methods. However, it still achieves accurate pose estimation and demonstrates better precision compared to \cite{Zhang_2021_ICCV,Hampali_2022_CVPR_Kypt_Trans,moon_bringing_2023}.}
    \label{tab:ih26m}
\end{table}

\paragraph{Analysis}
We observe that TI-Net achieves excellent PA-MPJPE results, performing better with both the task-specialized methods and the temporal information-enhanced methods on the DexYCB\cite{chao:cvpr2021_dexycb} dataset. We attribute this to the transformation isomorphism relationship, which more accurately describes how joint rotations change in response to geometric transformations of the image. For example, a horizontal flip of the image results in a left-right reversal of the orientation of each hand joint. This implies that the features extracted by TI-Net are more sensitive to local hand features, leading to a more accurate estimation of relative hand poses. Since PA-MPJPE measures the similarity between poses solely, TI-Net achieves better PA-MPJPE scores.

% -------------------------------------------------------------------

\section{Conclusion}

We propose TI-Net, a network that enables accurate hand pose estimation by constructing transformation isomorphism relationships among the input space, latent space, and pose space. This method is based on existing network architectures and enhances task-relevant vision backbone through the introduction of a novel transformation isomorphism, allowing seamless integration into other pose estimation frameworks without significant modifications to the network structure or training pipeline.

In future work, we plan to extend TI-Net from pose estimation tasks to vision regression tasks and enable more backbone architectures to extract transformation isomorphism relationships, improving the performance and effectiveness of regression tasks in computer vision.

% -------------------------------------------------------------------

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib.bib}
}

% -------------------------------------------------------------------
% \appendix
% \section{Proof for \cref{tab:complete_transform}}
% \label{sub:proof_for_complete_transform}

% We need to prove that the transformation set \( \mathsf G_{\mathbb I}=\{\mathcal H, \mathcal R_\theta, \mathcal{HR}_\theta \} \) forms a group and contains the transformation set \( \mathsf G_{\mathbb I}'=\{\mathcal{I,V,H,O}\} \), formally \( \mathsf G_{\mathbb I}\subset\mathsf G_{\mathbb I}' \).

% First, we prove the inclusion \( \mathsf G_{\mathbb I}\subset\mathsf G_{\mathbb I}' \). For each of \( \mathcal{I,V,H,O} \), we observe \( \mathcal I=\mathcal R_0, \mathcal V=\mathcal {HR}_\pi, \mathcal O=\mathcal R_\pi \). This implies that \( \mathsf G_{\mathbb I}\subset\mathsf G_{\mathbb I}' \).

% Next, we prove that the transformation set \( \mathsf{G}_{\mathbb{I}}' \) forms a group.

% \paragraph{Proof of closure}

% Before the formal proof, we first describe the properties of the transformations in \( \mathsf{G}_{\mathbb{I}}' \). The proofs of these properties are straightforward.
% \begin{align}
%     \mathcal {HR}_\theta &= \mathcal H \circ \mathcal R_\theta \\
%     \mathcal H \circ \mathcal R_\theta &= \mathcal R_{-\theta} \circ \mathcal H
% \end{align}

% Enumerate all possible pairs of elements in \( \mathsf{G}_{\mathbb{I}}' \), and prove that their composition remains within \( \mathsf{G}_{\mathbb{I}}' \):
% \begin{itemize}
%     \item Case 1: \( \mathcal H \circ \mathcal H = \mathcal I = \mathcal R_0 \),
%     \item Case 2: \( \mathcal R_\alpha \circ \mathcal R_\beta = \mathcal R_{\alpha + \beta} \),
%     \item Case 3: \( \mathcal {HR}_\alpha \circ \mathcal {HR}_\beta = \mathcal H \circ \mathcal R_\alpha \circ \mathcal H \circ \mathcal R_\beta = \mathcal R_{-\alpha}  \circ \mathcal R_\beta = \mathcal R_{-\alpha + \beta} \),
%     \item Case 4: \( \mathcal H\circ\mathcal R_\theta=\mathcal{HR}_\theta, \mathcal R_{\theta}\circ\mathcal H=\mathcal H\circ\mathcal R_{-\theta}=\mathcal{HR}_{\theta} \),
%     \item Case 5: \( \mathcal H\circ\mathcal {HR}_{\theta}=\mathcal R_{\theta}, \mathcal {HR}_{\theta}\circ\mathcal H=\mathcal R_{-\theta} \),
%     \item Case 6: \( \mathcal R_{\alpha}\circ\mathcal {HR}_{\beta} = \mathcal {HR}_{-\alpha+\beta}, \mathcal {HR}_{\beta}\circ\mathcal R_{\alpha}=\mathcal {HR}_{\alpha+\beta} \),
% \end{itemize}
% in summary, we have completed the proof of closure.

% \paragraph{Existence of identity}
% Proving that \( \mathcal{R}_0 \) is the identity element is trivial.

% \paragraph{Existence of inverse}
% The inverse elements of \( \mathcal{H} \), \( \mathcal{R}_{\theta} \), and \( \mathcal{HR}_{\theta} \) are \( \mathcal{H} \), \( \mathcal{R}_{-\theta} \), and \( \mathcal{HR}_{-\theta} \), respectively.

% In conclusion, we have proven that \( \mathsf{G}_{\mathbb{I}}' \) is a group that contains \( \mathsf{G}_{\mathbb{I}} \).   \(\blacksquare\)

\end{document}
