@InProceedings{10.1007/978-3-031-19772-7_3,
    author="Zhang, Haoyuan
    and Hou, Yonghong
    and Zhang, Wenjing
    and Li, Wanqing",
    editor="Avidan, Shai
    and Brostow, Gabriel
    and Ciss{\'e}, Moustapha
    and Farinella, Giovanni Maria
    and Hassner, Tal",
    title="Contrastive Positive Mining for Unsupervised 3D Action Representation Learning",
    booktitle="Computer Vision -- ECCV 2022",
    year="2022",
    publisher="Springer Nature Switzerland",
    address="Cham",
    pages="36--51",
    abstract="Recent contrastive based 3D action representation learning has made great progress. However, the strict positive/negative constraint is yet to be relaxed and the use of non-self positive is yet to be explored. In this paper, a Contrastive Positive Mining (CPM) framework is proposed for unsupervised skeleton 3D action representation learning. The CPM identifies non-self positives in a contextual queue to boost learning. Specifically, the siamese encoders are adopted and trained to match the similarity distributions of the augmented instances in reference to all instances in the contextual queue. By identifying the non-self positive instances in the queue, a positive-enhanced learning strategy is proposed to leverage the knowledge of mined positives to boost the robustness of the learned latent space against intra-class and inter-class diversity. Experimental results have shown that the proposed CPM is effective and outperforms the existing state-of-the-art unsupervised methods on the challenging NTU and PKU-MMD datasets.",
    isbn="978-3-031-19772-7"
}

@inproceedings{10.1007/978-3-031-20068-7_22,
    author = {Meng, Hao and Jin, Sheng and Liu, Wentao and Qian, Chen and Lin, Mengxiang and Ouyang, Wanli and Luo, Ping},
    title = {3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal},
    year = {2022},
    isbn = {978-3-031-20067-0},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    url = {https://doi.org/10.1007/978-3-031-20068-7_22},
    doi = {10.1007/978-3-031-20068-7_22},
    abstract = {Estimating 3D interacting hand pose from a single RGB image is essential for understanding human actions. Unlike most previous works that directly predict the 3D poses of two interacting hands simultaneously, we propose to decompose the challenging interacting hand pose estimation task and estimate the pose of each hand separately. In this way, it is straightforward to take advantage of the latest research progress on the single-hand pose estimation system. However, hand pose estimation in interacting scenarios is very challenging, due to (1) severe hand-hand occlusion and (2) ambiguity caused by the homogeneous appearance of hands. To tackle these two challenges, we propose a novel Hand De-occlusion and Removal (HDR) framework to perform hand de-occlusion and distractor removal. We also propose the first large-scale synthetic amodal hand dataset, termed Amodal InterHand Dataset (AIH), to facilitate model training and promote the development of the related research. Experiments show that the proposed method significantly outperforms previous state-of-the-art interacting hand pose estimation approaches. Codes and data are available at .},
    booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part VI},
    pages = {380–397},
    numpages = {18},
    keywords = {3D interacting hand pose estimation, De-occlusion, Removal, Amodal InterHand Dataset},
    location = {Tel Aviv, Israel}
}

@article{10.1109/TPAMI.2023.3247907,
    author = {Tu, Zhigang and Huang, Zhisheng and Chen, Yujin and Kang, Di and Bao, Linchao and Yang, Bisheng and Yuan, Junsong},
    title = {Consistent 3D Hand Reconstruction in Video via Self-Supervised Learning},
    year = {2023},
    issue_date = {Aug. 2023},
    publisher = {IEEE Computer Society},
    address = {USA},
    volume = {45},
    number = {8},
    issn = {0162-8828},
    url = {https://doi.org/10.1109/TPAMI.2023.3247907},
    doi = {10.1109/TPAMI.2023.3247907},
    abstract = {We present a method for reconstructing accurate and consistent 3D hands from a monocular video. We observe that the detected 2D hand keypoints and the image texture provide important cues about the geometry and texture of the 3D hand, which can reduce or even eliminate the requirement on 3D hand annotation. Accordingly, in this work, we propose <inline-formula><tex-math notation="LaTeX">$mathrm{{S}^{2}HAND}$</tex-math><alternatives><mml:math><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi> HAND </mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="chen-ieq1-3247907.gif"/></alternatives></inline-formula>, a self-supervised 3D hand reconstruction model, that can jointly estimate pose, shape, texture, and the camera viewpoint from a single RGB input through the supervision of easily accessible 2D detected keypoints. We leverage the continuous hand motion information contained in the unlabeled video data and explore <inline-formula><tex-math notation="LaTeX">$mathrm{{S}^{2}HAND(V)}$</tex-math><alternatives><mml:math><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi> HAND </mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="chen-ieq2-3247907.gif"/></alternatives></inline-formula>, which uses a set of weights shared <inline-formula><tex-math notation="LaTeX">$mathrm{{S}^{2}HAND}$</tex-math><alternatives><mml:math><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi> HAND </mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="chen-ieq3-3247907.gif"/></alternatives></inline-formula> to process each frame and exploits additional motion, texture, and shape consistency constrains to obtain more accurate hand poses, and more consistent shapes and textures. Experiments on benchmark datasets demonstrate that our self-supervised method produces comparable hand reconstruction performance compared with the recent full-supervised methods in single-frame as input setup, and notably improves the reconstruction accuracy and consistency when using the video training data.},
    journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
    month = aug,
    pages = {9469–9485},
    numpages = {17}
}

@INPROCEEDINGS{10655481,
    author={Pavlakos, Georgios and Shan, Dandan and Radosavovic, Ilija and Kanazawa, Angjoo and Fouhey, David and Malik, Jitendra},
    booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
    title={Reconstructing Hands in 3D with Transformers}, 
    year={2024},
    volume={},
    number={},
    pages={9826-9836},
    keywords={Solid modeling;Computer vision;Three-dimensional displays;Annotations;Training data;Benchmark testing;Transformers;3D hand pose estimation;hand mesh recovery;single-image 3D},
    doi={10.1109/CVPR52733.2024.00938}
}

@InProceedings{Chang_2022_CVPR,
    author    = {Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T.},
    title     = {MaskGIT: Masked Generative Image Transformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {11315-11325}
}

@InProceedings{Hampali_2022_CVPR_Kypt_Trans,  
    author = {Shreyas Hampali and Sayan Deb Sarkar and Mahdi Rad and Vincent Lepetit},  
    title = {Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation},  
    booktitle = {IEEE Computer Vision and Pattern Recognition Conference},  
    year = {2022}  
}

@InProceedings{He_2016_CVPR,
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    title = {Deep Residual Learning for Image Recognition},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2016}
}

@article{Lugaresi2019MediaPipeAF,
    title={MediaPipe: A Framework for Building Perception Pipelines},
    author={Camillo Lugaresi and Jiuqiang Tang and Hadon Nash and Chris McClanahan and Esha Uboweja and Michael Hays and Fan Zhang and Chuo-Ling Chang and Ming Guang Yong and Juhyun Lee and Wan-Teh Chang and Wei Hua and Manfred Georg and Matthias Grundmann},
    journal={ArXiv},
    year={2019},
    volume={abs/1906.08172},
    url={https://api.semanticscholar.org/CorpusID:195069430}
}

@InProceedings{Mo_2023_WACV,
    author    = {Mo, Shentong and Sun, Zhun and Li, Chao},
    title     = {Multi-Level Contrastive Learning for Self-Supervised Vision Transformers},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2023},
    pages     = {2778-2787}
}

@InProceedings{Wang_2023_CVPR,
    author    = {Wang, Haoqing and Tang, Yehui and Wang, Yunhe and Guo, Jianyuan and Deng, Zhi-Hong and Han, Kai},
    title     = {Masked Image Modeling With Local Multi-Scale Reconstruction},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {2122-2131}
}

@InProceedings{Xie_2023_CVPR,
    author    = {Xie, Zhenda and Geng, Zigang and Hu, Jingcheng and Zhang, Zheng and Hu, Han and Cao, Yue},
    title     = {Revealing the Dark Secrets of Masked Image Modeling},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {14475-14485}
}

@InProceedings{Zheng_2021_ICCV,
    author    = {Zheng, Mingkai and Wang, Fei and You, Shan and Qian, Chen and Zhang, Changshui and Wang, Xiaogang and Xu, Chang},
    title     = {Weakly Supervised Contrastive Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {10042-10051}
}

@incollection{avidan_p-stmo_2022,
	address = {Cham},
	title = {P-{STMO}: {Pre}-trained {Spatial} {Temporal} {Many}-to-{One} {Model} for {3D} {Human} {Pose} {Estimation}},
	volume = {13665},
	isbn = {978-3-031-20064-9 978-3-031-20065-6},
	shorttitle = {P-{STMO}},
	url = {https://link.springer.com/10.1007/978-3-031-20065-6_27},
	abstract = {This paper introduces a novel Pre-trained Spatial Temporal Many-to-One (P-STMO) model for 2D-to-3D human pose estimation task. To reduce the difficulty of capturing spatial and temporal information, we divide this task into two stages: pre-training (Stage I) and fine-tuning (Stage II). In Stage I, a self-supervised pre-training sub-task, termed masked pose modeling, is proposed. The human joints in the input sequence are randomly masked in both spatial and temporal domains. A general form of denoising auto-encoder is exploited to recover the original 2D poses and the encoder is capable of capturing spatial and temporal dependencies in this way. In Stage II, the pre-trained encoder is loaded to STMO model and fine-tuned. The encoder is followed by a many-to-one frame aggregator to predict the 3D pose in the current frame. Especially, an MLP block is utilized as the spatial feature extractor in STMO, which yields better performance than other methods. In addition, a temporal downsampling strategy is proposed to diminish data redundancy. Extensive experiments on two benchmarks show that our method outperforms state-of-the-art methods with fewer parameters and less computational overhead. For example, our P-STMO model achieves 42.1mm MPJPE on Human3.6M dataset when using 2D poses from CPN as inputs. Meanwhile, it brings a 1.5–7.1× speedup to state-of-the-art methods. Code is available at https://github.com/paTRICK-swk/P-STMO.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Shan, Wenkang and Liu, Zhenhua and Zhang, Xinfeng and Wang, Shanshe and Ma, Siwei and Gao, Wen},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-20065-6_27},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {461--478},
	file = {Shan 等 - 2022 - P-STMO Pre-trained Spatial Temporal Many-to-One M.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\DDGHSCPF\\Shan 等 - 2022 - P-STMO Pre-trained Spatial Temporal Many-to-One M.pdf:application/pdf},
}

@misc{blur_oh2023recovering3dhandmesh,
    title={Recovering 3D Hand Mesh Sequence from a Single Blurry Image: A New Dataset and Temporal Unfolding}, 
    author={Yeonguk Oh and JoonKyu Park and Jaeha Kim and Gyeongsik Moon and Kyoung Mu Lee},
    year={2023},
    eprint={2303.15417},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2303.15417}, 
}

@inproceedings{chen_simclr_2020,
	title = {{SimCLR}: {A} {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2024-09-10},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\54S45IB7\\Chen 等 - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;Supplementary PDF:C\:\\Users\\Administrator\\Zotero\\storage\\HI436S6L\\Chen 等 - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf},
}

@inproceedings{ego_Prakash2024Hands,
    author = {Prakash, Aditya and Tu, Ruisen and Chang, Matthew and Gupta, Saurabh},
    title = {3D Hand Pose Estimation in Everyday Egocentric Images},
    booktitle = {European Conference on Computer Vision (ECCV)},
    year = {2024}
}

@inproceedings{epipolartransformers,
    title={Epipolar Transformers},
    author={He, Yihui and Yan, Rui and Fragkiadaki, Katerina and Yu, Shoou-I},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={7779--7788},
    year={2020}
}

@inproceedings{feng_self-supervised_2019,
	address = {Long Beach, CA, USA},
	title = {Self-{Supervised} {Representation} {Learning} by {Rotation} {Feature} {Decoupling}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953870/},
	doi = {10.1109/CVPR.2019.01061},
	abstract = {We introduce a self-supervised learning method that focuses on beneﬁcial properties of representation and their abilities in generalizing to real-world tasks. The method incorporates rotation invariance into the feature learning framework, one of many good and well-studied properties of visual representation, which is rarely appreciated or exploited by previous deep convolutional neural network based self-supervised representation learning methods. Speciﬁcally, our model learns a split representation that contains both rotation related and unrelated parts. We train neural networks by jointly predicting image rotations and discriminating individual instances. In particular, our model decouples the rotation discrimination from instance discrimination, which allows us to improve the rotation prediction by mitigating the inﬂuence of rotation label noise, as well as discriminate instances without regard to image rotations. The resulting feature has a better generalization ability for more various tasks. Experimental results show that our model outperforms current state-of-the-art methods on standard self-supervised feature learning benchmarks.},
	language = {en},
	urldate = {2024-09-21},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Feng, Zeyu and Xu, Chang and Tao, Dacheng},
	month = jun,
	year = {2019},
	pages = {10356--10366},
	file = {Feng 等 - 2019 - Self-Supervised Representation Learning by Rotatio.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\7AZVNUM4\\Feng 等 - 2019 - Self-Supervised Representation Learning by Rotatio.pdf:application/pdf},
}

@misc{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we ﬁnd that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efﬁciently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.},
	language = {en},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = dec,
	year = {2021},
	note = {arXiv:2111.06377 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {He 等 - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\U29BDTWS\\He 等 - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf},
}

@inproceedings{hrnet_sun2019deep,
    title={Deep High-Resolution Representation Learning for Human Pose Estimation},
    author={Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
    booktitle={CVPR},
    year={2019}
}

@misc{li_masked_2024,
	title = {Masked {Modeling} for {Self}-supervised {Representation} {Learning} on {Vision} and {Beyond}},
	url = {http://arxiv.org/abs/2401.00897},
	abstract = {As the deep learning revolution marches on, self-supervised learning has garnered increasing attention in recent years thanks to its remarkable representation learning ability and the low dependence on labeled data. Among these varied self-supervised techniques, masked modeling has emerged as a distinctive approach that involves predicting parts of the original data that are proportionally masked during training. This paradigm enables deep models to learn robust representations and has demonstrated exceptional performance in the context of computer vision, natural language processing, and other modalities. In this survey, we present a comprehensive review of the masked modeling framework and its methodology. We elaborate on the details of techniques within masked modeling, including diverse masking strategies, recovering targets, network architectures, and more. Then, we systematically investigate its wide-ranging applications across domains. Furthermore, we also explore the commonalities and differences between masked modeling methods in different fields. Toward the end of this paper, we conclude by discussing the limitations of current techniques and point out several potential avenues for advancing masked modeling research. A paper list project with this survey is available at https://github.com/Lupin1998/Awesome-MIM.},
	language = {en},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {Li, Siyuan and Zhang, Luyuan and Wang, Zedong and Wu, Di and Wu, Lirong and Liu, Zicheng and Xia, Jun and Tan, Cheng and Liu, Yang and Sun, Baigui and Li, Stan Z.},
	month = jan,
	year = {2024},
	note = {arXiv:2401.00897 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Li 等 - 2024 - Masked Modeling for Self-supervised Representation.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\EY8VHIED\\Li 等 - 2024 - Masked Modeling for Self-supervised Representation.pdf:application/pdf},
}

@misc{lin_pre-training_2024,
    title = {Pre-{Training} for {3D} {Hand} {Pose} {Estimation} with {Contrastive} {Learning} on {Large}-{Scale} {Hand} {Images} in the {Wild}},
    url = {http://arxiv.org/abs/2409.09714},
    abstract = {We present a contrastive learning framework based on in-the-wild hand images tailored for pre-training 3D hand pose estimators, dubbed HandCLR. Pre-training on large-scale images achieves promising results in various tasks, but prior 3D hand pose pre-training methods have not fully utilized the potential of diverse hand images accessible from in-the-wild videos. To facilitate scalable pre-training, we first prepare an extensive pool of hand images from in-the-wild videos and design our method with contrastive learning. Specifically, we collected over 2.0M hand images from recent human-centric videos, such as 100DOH and Ego4D. To extract discriminative information from these images, we focus on the similarity of hands; pairs of similar hand poses originating from different samples, and propose a novel contrastive learning method that embeds similar hand pairs closer in the latent space. Our experiments demonstrate that our method outperforms conventional contrastive learning approaches that produce positive pairs sorely from a single image with data augmentation. We achieve significant improvements over the state-of-the-art method in various datasets, with gains of 15\% on FreiHand, 10\% on DexYCB, and 4\% on AssemblyHands.},
    language = {en},
    urldate = {2024-09-24},
    publisher = {arXiv},
    author = {Lin, Nie and Ohkawa, Takehiko and Zhang, Mingfang and Huang, Yifei and Furuta, Ryosuke and Sato, Yoichi},
    month = sep,
    year = {2024},
    note = {arXiv:2409.09714 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {Lin 等 - 2024 - Pre-Training for 3D Hand Pose Estimation with Cont.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\QI7A6FSQ\\Lin 等 - 2024 - Pre-Training for 3D Hand Pose Estimation with Cont.pdf:application/pdf},
}

@misc{mmpose2020,
    title={OpenMMLab Pose Estimation Toolbox and Benchmark},
    author={MMPose Contributors},
    howpublished = {\url{https://github.com/open-mmlab/mmpose}},
    year={2020}
}

@misc{moon_bringing_2023,
    title = {Bringing {Inputs} to {Shared} {Domains} for {3D} {Interacting} {Hands} {Recovery} in the {Wild}},
    url = {http://arxiv.org/abs/2303.13652},
    abstract = {Despite recent achievements, existing 3D interacting hands recovery methods have shown results mainly on motion capture (MoCap) environments, not on in-the-wild (ITW) ones. This is because collecting 3D interacting hands data in the wild is extremely challenging, even for the 2D data. We present InterWild, which brings MoCap and ITW samples to shared domains for robust 3D interacting hands recovery in the wild with a limited amount of ITW 2D/3D interacting hands data. 3D interacting hands recovery consists of two sub-problems: 1) 3D recovery of each hand and 2) 3D relative translation recovery between two hands. For the first sub-problem, we bring MoCap and ITW samples to a shared 2D scale space. Although ITW datasets provide a limited amount of 2D/3D interacting hands, they contain large-scale 2D single hand data. Motivated by this, we use a single hand image as an input for the first sub-problem regardless of whether two hands are interacting. Hence, interacting hands of MoCap datasets are brought to the 2D scale space of single hands of ITW datasets. For the second sub-problem, we bring MoCap and ITW samples to a shared appearance-invariant space. Unlike the first subproblem, 2D labels of ITW datasets are not helpful for the second sub-problem due to the 3D translation’s ambiguity. Hence, instead of relying on ITW samples, we amplify the generalizability of MoCap samples by taking only a geometric feature without an image as an input for the second sub-problem. As the geometric feature is invariant to appearances, MoCap and ITW samples do not suffer from a huge appearance gap between the two datasets. The code is publicly available1.},
    language = {en},
    urldate = {2024-06-04},
    publisher = {arXiv},
    author = {Moon, Gyeongsik},
    month = oct,
    year = {2023},
    note = {arXiv:2303.13652 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {Moon - 2023 - Bringing Inputs to Shared Domains for 3D Interacti.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\4P4ZPGIG\\Moon - 2023 - Bringing Inputs to Shared Domains for 3D Interacti.pdf:application/pdf},
}

@article{singh_explainable_2024,
	title = {Explainable rotation-invariant self-supervised representation learning},
	volume = {13},
	issn = {22150161},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2215016124004102},
	doi = {10.1016/j.mex.2024.102959},
	language = {en},
	urldate = {2024-09-21},
	journal = {MethodsX},
	author = {Singh, Devansh and Marathe, Aboli and Roy, Sidharth and Walambe, Rahee and Kotecha, Ketan},
	month = dec,
	year = {2024},
	pages = {102959},
	file = {Singh 等 - 2024 - Explainable rotation-invariant self-supervised rep.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\JDLH6KR9\\Singh 等 - 2024 - Explainable rotation-invariant self-supervised rep.pdf:application/pdf},
}

@misc{spurr_peclr_2022,
	title = {{PeCLR}: {Self}-{Supervised} {3D} {Hand} {Pose} {Estimation} from monocular {RGB} via {Equivariant} {Contrastive} {Learning}},
	shorttitle = {{PeCLR}},
	url = {http://arxiv.org/abs/2106.05953},
	abstract = {Encouraged by the success of contrastive learning on image classification tasks, we propose a new self-supervised method for the structured regression task of 3D hand pose estimation. Contrastive learning makes use of unlabeled data for the purpose of representation learning via a loss formulation that encourages the learned feature representations to be invariant under any image transformation. For 3D hand pose estimation, it too is desirable to have invariance to appearance transformation such as color jitter. However, the task requires equivariance under affine transformations, such as rotation and translation. To address this issue, we propose an equivariant contrastive objective and demonstrate its effectiveness in the context of 3D hand pose estimation. We experimentally investigate the impact of invariant and equivariant contrastive objectives and show that learning equivariant features leads to better representations for the task of 3D hand pose estimation. Furthermore, we show that standard ResNets with sufficient depth, trained on additional unlabeled data, attain improvements of up to 14.5\% in PA-EPE on FreiHAND and thus achieves state-of-the-art performance without any task specific, specialized architectures. Code and models are available at https://ait.ethz.ch/projects/2021/PeCLR/},
	language = {en},
	urldate = {2024-09-21},
	publisher = {arXiv},
	author = {Spurr, Adrian and Dahiya, Aneesh and Wang, Xi and Zhang, Xucong and Hilliges, Otmar},
	month = aug,
	year = {2022},
	note = {arXiv:2106.05953 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Spurr 等 - 2022 - PeCLR Self-Supervised 3D Hand Pose Estimation fro.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\BI4AN7K5\\Spurr 等 - 2022 - PeCLR Self-Supervised 3D Hand Pose Estimation fro.pdf:application/pdf},
}

@misc{su_ri-mae_2024,
	title = {{RI}-{MAE}: {Rotation}-{Invariant} {Masked} {AutoEncoders} for {Self}-{Supervised} {Point} {Cloud} {Representation} {Learning}},
	shorttitle = {{RI}-{MAE}},
	url = {http://arxiv.org/abs/2409.00353},
	abstract = {Masked point modeling methods have recently achieved great success in self-supervised learning for point cloud data. However, these methods are sensitive to rotations and often exhibit sharp performance drops when encountering rotational variations. In this paper, we propose a novel RotationInvariant Masked AutoEncoders (RI-MAE) to address two major challenges: 1) achieving rotation-invariant latent representations, and 2) facilitating self-supervised reconstruction in a rotation-invariant manner. For the first challenge, we introduce RI-Transformer, which features disentangled geometry content, rotation-invariant relative orientation and position embedding mechanisms for constructing rotationinvariant point cloud latent space. For the second challenge, a novel dual-branch student-teacher architecture is devised. It enables the self-supervised learning via the reconstruction of masked patches within the learned rotation-invariant latent space. Each branch is based on an RI-Transformer, and they are connected with an additional RI-Transformer predictor. The teacher encodes all point patches, while the student solely encodes unmasked ones. Finally, the predictor predicts the latent features of the masked patches using the output latent embeddings from the student, supervised by the outputs from the teacher. Extensive experiments demonstrate that our method is robust to rotations, achieving the state-of-the-art performance on various downstream tasks.},
	language = {en},
	urldate = {2024-09-21},
	publisher = {arXiv},
	author = {Su, Kunming and Wu, Qiuxia and Cai, Panpan and Zhu, Xiaogang and Lu, Xuequan and Wang, Zhiyong and Hu, Kun},
	month = aug,
	year = {2024},
	note = {arXiv:2409.00353 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Su 等 - 2024 - RI-MAE Rotation-Invariant Masked AutoEncoders for.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\FYLXQKHU\\Su 等 - 2024 - RI-MAE Rotation-Invariant Masked AutoEncoders for.pdf:application/pdf},
}

@inproceedings{wang_dense_2021,
	address = {Nashville, TN, USA},
	title = {Dense {Contrastive} {Learning} for {Self}-{Supervised} {Visual} {Pre}-{Training}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578497/},
	doi = {10.1109/CVPR46437.2021.00304},
	abstract = {To date, most existing self-supervised learning methods are designed and optimized for image classiﬁcation. These pre-trained models can be sub-optimal for dense prediction tasks due to the discrepancy between image-level prediction and pixel-level prediction. To ﬁll this gap, we aim to design an effective, dense self-supervised learning method that directly works at the level of pixels (or local features) by taking into account the correspondence between local features. We present dense contrastive learning (DenseCL), which implements self-supervised learning by optimizing a pairwise contrastive (dis)similarity loss at the pixel level between two views of input images.},
	language = {en},
	urldate = {2024-09-21},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Xinlong and Zhang, Rufeng and Shen, Chunhua and Kong, Tao and Li, Lei},
	month = jun,
	year = {2021},
	pages = {3023--3032},
	file = {Wang 等 - 2021 - Dense Contrastive Learning for Self-Supervised Vis.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\8WARPITK\\Wang 等 - 2021 - Dense Contrastive Learning for Self-Supervised Vis.pdf:application/pdf},
}

@inproceedings{xie2021simmim,
    title={SimMIM: A Simple Framework for Masked Image Modeling},
    author={Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},
    booktitle={International Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2022}
}

@article{xu2022vitpose+,
    title={ViTPose+: Vision Transformer Foundation Model for Generic Body Pose Estimation},
    author={Xu, Yufei and Zhang, Jing and Zhang, Qiming and Tao, Dacheng},
    journal={arXiv preprint arXiv:2212.04246},
    year={2022}
}

@inproceedings{zheng2023hamuco,
    title={HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning},
    author={Zheng, Xiaozheng and Wen, Chao and Xue, Zhou and Ren, Pengfei and Wang, Jingyu},
    booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
    year={2023}
}

@misc{zhou_ibot_2022,
	title = {{iBOT}: {Image} {BERT} {Pre}-{Training} with {Online} {Tokenizer}},
	shorttitle = {{iBOT}},
	url = {http://arxiv.org/abs/2111.07832},
	abstract = {The success of language Transformers is primarily attributed to the pretext task of masked language modeling (MLM), where texts are first tokenized into semantically meaningful pieces. In this work, we study masked image modeling (MIM) and indicate the advantages and challenges of using a semantically meaningful visual tokenizer. We present a self-supervised framework iBOT that can perform masked prediction with an online tokenizer. Specifically, we perform self-distillation on masked patch tokens and take the teacher network as the online tokenizer, along with self-distillation on the class token to acquire visual semantics. The online tokenizer is jointly learnable with the MIM objective and dispenses with a multi-stage training pipeline where the tokenizer needs to be pre-trained beforehand. We show the prominence of iBOT by achieving an 82.3\% linear probing accuracy and an 87.8\% fine-tuning accuracy evaluated on ImageNet-1K. Beyond the state-of-the-art image classification results, we underline emerging local semantic patterns, which helps the models to obtain strong robustness against common corruptions and achieve leading results on dense downstream tasks, eg., object detection, instance segmentation, and semantic segmentation.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Zhou, Jinghao and Wei, Chen and Wang, Huiyu and Shen, Wei and Xie, Cihang and Yuille, Alan and Kong, Tao},
	month = jan,
	year = {2022},
	note = {arXiv:2111.07832 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\JEVBTQEC\\2111.html:text/html;Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\UHYAMB2F\\Zhou 等 - 2022 - iBOT Image BERT Pre-Training with Online Tokenize.pdf:application/pdf},
}

