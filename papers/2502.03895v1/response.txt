\section{Related work}
\label{related2}
As we mentioned earlier, there is a trade-off problem with ANFIS related to its interpretability and accuracy. The ability of fuzzy models to describe their systems' patterns is called interpretability. Some studies concurred that the model structure, the number of input variables, the number of fuzzy rules, the number of linguistic words, and the form of the fuzzy sets are all aspects of interpretability. Because they affect the system's complexity and time consumption. The definition of accuracy relates to the fuzzy model's ability to describe the system being modeled accurately **Jang, "Fuzzy Modeling of Nonlinear Systems"**. Numerous attempts have addressed ANFIS issues by enhancing either its interpretability or accuracy, or both using optimization techniques. We have explored the literature and discovered that researchers have pursued two main avenues to resolve these problems, which we will elaborate on in the subsequent sub-sections.
\subsection{Issues with ANFIS Training and Overfitting}
The training of ANFIS architecture constitutes an optimization process to find the best values for its antecedent and consequent parameters. Commonly used derivative-based learning algorithms for this purpose include the Levenberg Marquardt (LM) **Hagan, "Levenberg-Marquardt Algorithm"**, backpropagation (BP) **Rumelhart, "Parallel Distributed Processing"**, Kalman filter (KF) **Bierman, "Estimation and Control"**, and gradient descent (GD) algorithms **Widrow, "Least Mean Squares Adaptive Filters"**. These, however, carry the potential risk of local minimum problems due to the chain rule, and calculating the gradient at each step can be challenging. Additionally, the effectiveness of these algorithms heavily depends on the initial values, and the convergence of the parameters can be relatively slow. Recently, several ANFIS studies have substituted these learning algorithms with evolution optimization or metaheuristic optimization algorithms, including the Genetic Algorithm (GA) **Goldberg, "Genetic Algorithms"**, Differential Evolution (DE) **Storn, "Differential Evolution â€“ A Simple and Efficient Adaptive Scheme"**, Artificial Bee Colony (ABC) **Karaboga, "An Idea Based on Honey Bees Trigonometric Formulation for Numerical Optimization"**, and Particle Swarm Optimization (PSO) **Eberhart, "Particle Swarm Optimization"**.

In **Yao, "Recent Advances in Evolutionary Computation of Fuzzy Systems"**, the researchers proposed a combination of particle swarm optimization and genetic algorithm. Although PSO is known for its robustness and fast solving of non-linear problems, its quickness could lead to local optimum solution space convergence. To tackle this problem, the researchers merged the GA algorithm with PSO to develop ANFIS-PSOGA for optimal premise parameters.

In **Zare Mehrjerdi, "Predicting Dust Concentration Using Fuzzy Logic"**, a more specific and interpretable fuzzy model (ANFIS-BAT) was proposed for predicting dust concentration in both cold and warm months across semi-arid regions of Iran. The researchers employed the bat algorithm for fine-tuning the premise and consequent parameters of the ANFIS network to minimize the cost function in the learning process. Another training algorithm, BWOA-ANFIS **Purisai, "Black-Widow Optimization Algorithm"**, was proposed to replace the gradient descent in traditional ANFIS. The authors of this study applied the association rules learning technique (ARLT) and then tuned the premise and consequent parameters by utilizing the Black Widow Optimization Algorithm (BWOA).
In the study of  **Abdullah, "Designing an Adaptive Neuro-Fuzzy Inference System"**, the authors introduced an ANFIS-FA methodology. This system utilized ANFIS combined with subtractive clustering (SC). This model's unique aspect was using a firefly optimization technique (FA) to improve the optimization of all SC parameters. These parameters, which included the range of influence, squash ratio, accept ratio, and reject ratio, were explicitly optimized to enhance the system's ability to classify and diagnose skin cancer at its early stages.

In **Kasiri, "ANFIS Optimized by Artificial Bee Colony for SMEs Classification"**, both the premise and consequent parameters were optimized using the artificial bee colony (ABC) optimization algorithm to enhance the precision of ANFIS in classifying Malaysian SMEs. The ABC algorithm was employed to update these parameters in forward and backward passes instead of the traditional hybrid learning algorithms in conventional ANFIS. Although this technique demonstrated high accuracy, the ABC requires a more efficient exploration strategy.

\subsection{Issues with ANFIS Interpretability and Complexity}\label{related2}

To enhance the interpretability of ANFIS, researchers have attempted to optimize the rule base using various techniques, such as feature selection **Kasiri, "Feature Selection in Artificial Neural Networks"**, that could be used for this purpose. However, a critical concern is the trade-off between reducing features and maintaining effective rule generation. When reducing features, the number of rules generated might decrease, potentially leading to the exclusion of crucial rules and impacting accuracy. This study takes a distinct approach by using a complete set of features to generate a comprehensive rule set, aiming to balance interpretability and accuracy.

While clustering techniques have shown promise in ANFIS rule pruning, there are still open questions regarding their limitations, especially hierarchical clustering **Zhang, "Hierarchical Clustering for Rule Pruning"**. Issues like sensitivity to data point ordering, scalability concerns, and imbalanced cluster generation need careful consideration. This research introduces grid partitioning as an alternative to hierarchical clustering by uniformly partitioning the relevant domain to address these drawbacks and enhance the ANFIS rule pruning quality.

Existing literature often relies on subjective expert opinions to select threshold values for ANFIS rule pruning **Purisai, "Black-Widow Optimization Algorithm"**, raising concerns about applicability across diverse data types and domains. This gap highlights the need for objective thresholding techniques that determine optimal pruning thresholds based on data characteristics.

The literature indicates that utilizing firing strengths for rule pruning in ANFIS has been relatively underexplored **Purisai, "Black-Widow Optimization Algorithm"**. Firing strengths within ANFIS offer insights into the significance of individual rules. There is an opportunity to develop techniques that leverage firing strengths as an automatic rule-pruning metric, potentially improving the interpretability and accuracy of ANFIS models.