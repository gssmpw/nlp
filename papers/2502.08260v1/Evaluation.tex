\section{Implementation and Evaluation}
\label{sec:implementation_evaluation}

%We present our implementation and evaluation of \coolname based on an existing popular ADS and MLLM. 

\subsection{Implementation}
We have implemented \coolname for Apollo 9.0~\cite{apollo90} (the latest version at the time of our experiments) and the widely-used MLLM ChatGPT (version ChatGPT 4 Turbo). The simulator utilised in our experiments is the official \emph{Dreamview Plus~\cite{dreamview_plus}}, provided by Apollo 9.0. 

Our framework, \coolname, comprises three main components:
1) \emph{Trajectory Record Analysis Tool:} this tool identifies the specific time step at which the quantitative semantics of the trace fall below a specified threshold according to a given specification (such as no collisions and adherence to traffic rules in different countries). It enables us to precisely pinpoint the exact moment a violation occurs and to detect near-miss situations where a violation is likely but has not yet occurred.
2) \emph{Prompt Generator:} this component generates both the text prompt and captures a visualisation of the driving conditions at specific time steps. It organises this information into function calls for ChatGPT. Essentially, the the prompt generator automatically creates the input provided to the MLLM based on specified criteria and recorded data.
3) \emph{Translation and Verification:} the response from the MLLM is translated into a domain-specific language, $\mu$Drive, which outlines general driving strategy repairs (e.g.~stopping when a pedestrian is ahead). An additional validity check ensures that the syntax is correct.  These strategy repairs are then verified using the simulator to ensure they enable the autonomous vehicle to successfully navigate the given scenario.

Our implementation leverages some components from previous work. Specifically, from LawBreaker~\cite{Sun-Poskitt-et_al22a}, we utilise its specification language and the corresponding verification algorithm. From $\mu$Drive~\cite{wang2024mudrive}, we employ its DSL and backend support for applying new driving strategies in Apollo.




\subsection{Evaluation}
Our evaluation considers four Research Questions~(RQs):

%We conducted experiments to answer the following Research Questions~(RQs): 

\begin{itemize}
    \item \noindent \textbf{RQ1}: Does \coolname effectively repair the driving behaviour of an ADS?
    \item \noindent \textbf{RQ2}: Are these driving strategy repairs applicable to common driving scenarios?
    \item \noindent \textbf{RQ3}: How much effort is required to compute repairs?
    %\item \noindent \textbf{RQ3}: How much effort is required to utilise \coolname?
    \item \noindent \textbf{RQ4}: What is the impact of using images for critical moments instead of text?
\end{itemize}
RQ1 considers whether \coolname achieves its primary goal of being able to utilise MLLMs to effectively repair the driving behaviour of an ADS following a violation event.
RQ2 investigates the effectiveness of \coolname's driving strategy repairs, based on a small sample of records, in improving AV performance across different scenarios. 
RQ3 examines the computational effort needed to utilise \coolname.
RQ4 validates the effects of using images in the prompt instead of ground-truth values in textual prompts.
Our experiments utilise both Apollo 9.0 and the Apollo Simulation Platform, referred to as Apollo Studio~\cite{Apollostudio}. To account for simulator randomness (e.g.~due to concurrency) each experiment is repeated 20 times, and we present the averages. 
We utilise a Linux machine (Ubuntu 20.04.5 LTS) with 32GB of RAM, an Intel i7-10700k CPU, and an RTX 2080Ti graphics card.


\begin{table*}
 \centering
 \caption{Performance comparison of \coolname, REDriver, and Apollo}
 \vspace{-0.2cm}\scriptsize
    \resizebox{0.9\linewidth}{!}{%
    \begin{tabular}{c|c|c|c|c|c|l}
    \hline
        Law & Scene & Driver & Fix & Pass & Robustness& \multicolumn{1}{c}{Context}\\
    \hline
    \multirow{6}{*}{no collision}  
    & \multirow{3}{*}{S1} & Apollo & -  & 0\% & \textcolor{red}{-0.1} &The AV entered the intersection during a green light vehicles, \\  
    & & REDriver & - & 0\% & \textcolor{red}{-0.1} & but failed to yield to the straight-moving\\  
    & & \coolname & 30\% & \textbf{100\%}  & \textcolor{green!50!black}{1.37}& resulting in an accident.\\
    \cline{2-7}
    & \multirow{3}{*}{S2} & Apollo & - & 0\% & \textcolor{red}{-0.1} &The AV fail to yield to the oncoming straight-through traffic\\  
    & & REDriver & - & 0\% & \textcolor{red}{-0.1} &at the stop sign and proceed to make a left turn at the intersection,\\  
    & & \coolname & 50\% & \textbf{100\%} & \textcolor{green!50!black}{4.41}&  resulting in an accident.\\
    \hline

  
    \multirow{6}{*}{\makecell{Law38}} 
    & \multirow{3}{*}{S3} & Apollo & - & 20\% & 10.7, \textcolor{red}{0.0}, \textcolor{red}{0.0}  &The AV started and entered the intersection when the traffic light\\
    & & REDriver & - & 60\% & 11.55, \textcolor{green!50!black}{0.5}, \textcolor{green!50!black}{0.5} & was yellow.\\ 
    & & \coolname & 45\% & \textbf{100\%} & 12.97, \textcolor{green!50!black}{0.5}, \textcolor{green!50!black}{0.48} &\\
    \cline{2-7}

    & \multirow{3}{*}{S4}  & Apollo & -  & 0\% & 4.7, 0.5, \textcolor{red}{0.0} &\multirow{3}{*}{The AV entered the intersection on a red light.}\\  
    & & REDriver & - & 85\% & 11.52, 0.5, \textcolor{green!50!black}{0.5} & \\  
    & & \coolname & 100\% & \textbf{100\%} & 2.84, 0.5, \textcolor{green!50!black}{0.5} &\\
    \hline
    
    \multirow{3}{*}{\makecell{Law44}}   
    & \multirow{3}{*}{S5} & Apollo & - & 20\% & \textcolor{red}{-19.98} &The AV is traveling in the fast lane and come to a stop due to\\  
    & & REDriver & - & \textbf{100\%} & \textcolor{green!50!black}{4.34} & an static obstacle (failure to change lanes to an available\\  
    & & \coolname & 20\% & \textbf{100\%} & \textcolor{green!50!black}{9.58}& lane on the right), ultimately failing to reach its destination.\\
    \hline

    

    \multirow{3}{*}{Law46}  & \multirow{3}{*}{S6} & Apollo & - & 0\%  &  \textcolor{red}{0.0}, \textcolor{red}{-0.2} & The AV continues to travel at speeds exceeding 30 kilometres \\  
    & & REDriver & - & \textbf{100\%} & \textcolor{green!50!black}{1.00}, \textcolor{green!50!black}{1.00} & per hour despite fog or rain.\\  
    & & \coolname & 100\% & \textbf{100\%}  & \textcolor{green!50!black}{1.23}, \textcolor{green!50!black}{1.23}&\\
    \hline

     \multirow{3}{*}{Law53}  & \multirow{3}{*}{S7} & Apollo & - & 0\%   &   \textcolor{red}{0.0}  &\multirow{3}{*}{The AV is approaching a junction with traffic jam.}\\  
     & & REDriver & - & 0\% & \textcolor{red}{0.0} & \\  
    & & \coolname & 50\% & \textbf{100\%} & \textcolor{green!50!black}{1.0}&\\
    \hline

    \multirow{3}{*}{finish journey}  & \multirow{3}{*}{S8} & Apollo & - & 0\%  &   \textcolor{red}{-0.42}  &The AV failed to overtake a stationary vehicle ahead and became\\  
    & & REDriver & - & 0\% & \textcolor{red}{-0.43} & stuck.\\  
    & & \coolname & 15\% & \textbf{100\%}  & \textcolor{green!50!black}{5.17}&\\
    \hline

    \end{tabular}}
    \label{tab:effectiveness comparison}
    \vspace{-0.5cm}
\end{table*}

\noindent \emph{\textbf {RQ1: Does \coolname effectively repair the driving behaviour of an ADS?}}
To answer this question, we employed a benchmark of scenarios provided by \cite{wang2024mudrive} where Apollo consistently violates specifications. 
Table~\ref{tab:effectiveness comparison} reports the effectiveness of our approach in preventing these violations compared to the original Apollo and the runtime enforcement method REDriver~\cite{sun2024redriver}. Note that the threshold for \coolname is set to 15, determined by an empirical experiment discussed later in this section.
The `Law' column in the table denotes the specific property specification under which the AV is tested. 
We adopted the formalisation of traffic laws reported in~\cite{Sun-Poskitt-et_al22a} as part of our property specifications and evaluated whether \coolname can be applied so that the ADS follows them. 
Specifically, we adopted four rules sourced from the \emph{Regulations for the Implementation of the Road Traffic Safety Law of the People's Republic of China}~\cite{China_traffic_law}: \emph{Law38}, \emph{Law44}, \emph{Law46}, and \emph{Law53}. These rules encompass regulations concerning traffic lights (yellow, green, red), speed limits for the fast lane, speed limits under adverse weather conditions (such as fog, rain, and snow), and managing traffic jam, respectively.
Additionally, we applied the property specifications `no collision' and `finish journey' to evaluate the AV as well. The detailed property specifications of these two rules can be expressed as: 
\vspace{-15pt}\begin{align*}
    no\_collision & \equiv \Box (\lnot NearestNPC(0.1)) \\
     finish\_journey & \equiv  \Box (\Diamond_{[0,200]}(speed > 0.5) \lor dest(5)) 
\end{align*}
Here, the specification `no collision' requires that the distance to other objects always be greater than 0.1 metres (i.e.~$\lnot NearestNPC(0.1)$). The specification `finish journey' requires that the AV must not stop on the road (i.e.~$\Diamond_{[0,200]}(speed > 0.5)$), unless it is close to the destination (i.e.~$dest(5)$). 
We refer the readers to our repository in \cite{source_code}
to see all the detailed specifications.
The `Fix' column in Table~\ref{tab:effectiveness comparison} shows the proportion of successful driving strategy repairs generated by \coolname. A driving strategy repair is deemed successful only if it ensures that the AV causes no violations.
To evaluate this, we repeat the generation process 20 times for each driving record, generating one unique driving suggestion per run. Each suggestion is then applied to the autonomous vehicle and tested for effectiveness in the same scenario through simulation, allowing us to assess whether the repair successfully resolves the failure.
Our empirical analysis indicates that while different suggestions may be generated for the same record, they typically converge into a limited set of outcomes. Consequently, 20 repetitions are empirically sufficient to capture all possible outcomes.
The `Pass' column indicates the proportion of runs that comply with traffic rules.
It signifies the success rate of each effective suggestion, which is always 100\% for \coolname.
Note that the MLLM generates varied suggestions across runs due to inherent uncertainty. 
However, empirical analysis shows that most effective suggestions are consistent across trials. Therefore, we select the most frequently successful suggestion to determine the final values in the `Pass' and `Robustness' column.
The `Robustness' column in the table illustrates the robustness of the AVs regarding current traffic regulations. The robustness value is calculated as the average performance of these effective suggestions. 
For example, the three values for the specification \emph{Law38} indicate the robustness values for green light, yellow light, and red light related traffic laws, respectively.
Specifically, the robustness value measures how closely the vehicle trajectory adheres to these rules. A higher robustness value indicates a lower likelihood of violating traffic regulations, whereas a lower value indicates a higher likelihood of imminent violation. A value less than or equal to $0$ indicates a violation of the corresponding traffic rule.
Furthermore, if a regulation comprises multiple sub-rules, the robustness for each sub-rule is sequentially presented.



\begin{figure}[t]
\begin{minipage}[t]{0.48\linewidth}
\begin{minted}[fontsize=\tiny,frame=single,obeytabs=true,tabsize=4,numbersep=5pt,escapeinside=||]{yaml}
|\textbf{\textcolor{green!50!black}{rule}}| |\textcolor{red!70!black}{"S1 rule1"}|
|\textbf{\textcolor{green!50!black}{trigger}}| 
    always 
|\textbf{\textcolor{green!50!black}{condition}}| 
    front_vehicle_closer_than(10)
|\textbf{\textcolor{green!50!black}{then}}|
    follow_dist(10)
    yield_dist(15)
    overtake_dist(20)
    obstacle_stop_dist(10)
    obstacle_decrease_ratio(1)
|\textbf{\textcolor{green!50!black}{end}}|
\end{minted}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\linewidth}
\begin{minted}[fontsize=\tiny,frame=single,obeytabs=true,tabsize=4,numbersep=5pt,escapeinside=||]{yaml}
|\textbf{\textcolor{green!50!black}{rule}}| |\textcolor{red!70!black}{"S1 rule2"}|
|\textbf{\textcolor{green!50!black}{trigger}}| 
    always 
|\textbf{\textcolor{green!50!black}{condition}}| 
    is_traffic_light(red)
    traffic_light_distance_leq(10)
|\textbf{\textcolor{green!50!black}{then}}|
    traffic_light_stop_dist(5)
|\textbf{\textcolor{green!50!black}{end}}|
\end{minted}
\end{minipage}
\caption{\textsc{$\mu$Drive} driving strategy repair scripts for S1}
\label{lst:s1 commands}
\vspace{-0.6cm}
\end{figure}


As shown in Table~\ref{tab:effectiveness comparison}, Apollo's success rate in these scenarios consistently remains below 50\%, often reaching 0\%. While REDriver can prevent some of the violations sometimes, there are still instances it cannot address.
In contrast, the repairs by \coolname enable the AV to completely avoid accidents and violations.
This is because REDriver focuses on a narrow case-by-case view, making decisions based only on current perception and prediction, and thus heavily depends on the accuracy of the ADS's original predictions, which \emph{may be wrong}.
For example, in scenario \emph{S1} where an AV fails to yield to a straight-moving vehicle, REDriver cannot prevent the violation because it cannot reverse the decision `not to yield', which was based on the prediction that the vehicle would not obstruct its path. 
In contrast, in the driving strategy repairs generated by \coolname, as shown in Figure~\ref{lst:s1 commands}, the first program dynamically adjusts parameters such as follow distance, yield distance, overtake distance, stop distance, and obstacle response rate when a vehicle is within 10 metres ahead. Additionally, the second program ensures safety by adjusting the AV's stop distance when approaching a red light and the distance to the stop line is less than 10 metres. This adaptive approach, based on a global perspective, continuously modifies the vehicle's driving style as conditions change, thereby effectively avoiding accidents before they occur. 
Due to the inherent uncertainty of the generative model, the proportion of successful repairs sometimes falls below $50\%$ (as shown in `Fix' column).  However, our evaluation of \coolname's time and cost efficiency in RQ3 demonstrates that generating each driving strategy repair is both time-efficient and cost-effective, mitigating this issue.

\begin{table}[t]
\centering
\caption{Effectiveness of \coolname across varying $\delta$}
\vspace{-0.2cm}\scriptsize
\label{tab:thresholds}
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{|c|cccccccc|}
\hline
$\delta$ & S1 & S2 & S3 & S4 & S5 & S6 & S7 & S8 \\
\hline
 1 & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\ \hline
 5 & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\ \hline
 10 & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\ \hline
 15 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\ \hline
 20 & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\ \hline
 25 & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\ \hline
 30 & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\
\hline
\end{tabular}
}
%\vspace{-0.8cm}
\end{table}






To further investigate RQ1, we designed a second experiment to examine how varying \coolname's thresholds impact its effectiveness. Recall that the threshold defines the fault tolerance of \coolname, determining what constitutes a near-miss situation, as discussed in Section~\ref{sec:Violation and Near-Miss Moment}. The detailed effectiveness evaluation for all thresholds is shown in Table~\ref{tab:thresholds}. In this table, the first column (`$\delta$') denotes the threshold value, ranging from 1 to 30. The subsequent columns represent scenarios S1 to S8 as mentioned above. If \coolname can provide driving strategy repairs that help the AV resolve the encountered problem, i.e.~satisfy the corresponding specification within 10 queries, we mark it with a \checkmark. Otherwise, we mark it with a $\times$. 
As show in Table~\ref{tab:thresholds}, the threshold value significantly impacts \coolname's effectiveness in certain scenarios. When the threshold $\delta$ is very small, such as 1 or 5, the near-miss moments are too close to the violation moment, providing insufficient information about why the violation occurs. Conversely, if $\delta$ is too large, such as 30, the near-miss moment may not provide any useful information, as there may be no indication of a potential violation. This can lead to \coolname's failure to deliver effective programs.
For example, in scenario S1, where the AV is making a right turn and fails to yield to vehicles going straight, the timing of the threshold is critical. When the threshold is set to values below 10, the AV is already impeding the straight-moving vehicles at the near-miss moment, making it difficult to implement effective changes. Conversely, when the threshold is set to 20, the AV has not started the right turn yet at the near-miss moment, and the potential problem has not yet arisen. In some scenarios, \coolname can be effective for all threshold values if the near-miss moment does not provide critical information. For example, in scenario S6, where the vehicle exceeds 30 km/h in snowy conditions, the critical information is that the AV exceeds 30 km/h at the violation moment. Since snow conditions remain consistent, the choice of near-miss moment does not affect \coolname's effectiveness.
\begin{table}[]
    \centering
    \caption{Performance of \coolname in official scenarios}
    \vspace{-0.2cm}\scriptsize
    \begin{tabular}{c|c|c|c|c}
    \hline
         \multirow{2}{*}{Map} & \multirow{2}{*}{Num} & \multirow{2}{*}{Driver} & \multirow{2}{*}{Finish} & \multirow{2}{*}{Accident} \\
         & & & & \\
         \hline
         \multirow{2}{*}{Sunnyvale} & \multirow{2}{*}{114} & Apollo & 108 & 7   \\
         \cline{3-5}
         & & \coolname & 112 & 6  \\
         \hline
         \multirow{2}{*}{SanMateo} & \multirow{2}{*}{103} & Apollo & 94 & 1 \\
         \cline{3-5}
         & & \coolname  & 95  & 1 \\
         \hline
         \multirow{2}{*}{Apollo Virtual} & \multirow{2}{*}{52} & Apollo & 42 & 1  \\
         \cline{3-5}
         & & \coolname & 46 & 1 \\
         \hline
    \end{tabular}
    \vspace{-0.5cm}
    \label{tab:comparison_with_official_scenarios1}
\end{table}



\begin{table*}[]
    \centering
    \caption{Performance comparison of \coolname and Apollo in official scenarios}
    \vspace{-0.2cm}\scriptsize
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
    \hline
         \multirow{2}{*}{Map} & \multirow{2}{*}{Num} & \multirow{2}{*}{Driver} & \multicolumn{2}{c|}{Speed($m/s$)} & \multicolumn{2}{c|}{Accelerate($m/s^2$)} & \multicolumn{2}{c|}{Obstacle Distance($m$)} & \multirow{2}{*}{Stop Time($s$)} & \multirow{2}{*}{Energy($J$)}\\
         \cline{4-9}
         & & &  Ave & Max & Ave & Max & Ave & Min & &\\
         \hline
         \multirow{2}{*}{Sunnyvale} & \multirow{2}{*}{108} & Apollo & 3.31 & 7.86 & 0.38 & 1.91 & 51.43 & 6.74 & 15.56 & 132840.52  \\
         \cline{3-11}
         & & \coolname & 3.76 & 8.29 & 0.46 & 2.19 & 51.71 &	6.90 & 12.63 & 147434.25 \\
         % & & \coolname & 114 & 3 & 2.93 & 6.43 & 0.32 & 1.87 & 50.49 & 8.34 & 16.25 & 105503.81 \\
         \hline
         \multirow{2}{*}{SanMateo} & \multirow{2}{*}{94} & Apollo & 2.58 & 6.28 & 0.41 & 1.47 & 31.27 & 6.85 & 6.95 & 70570.81 \\
         \cline{3-11}
         & & \coolname  & 2.84 &	6.20 & 0.44 & 1.80 & 30.97 & 7.45 & 4.48 & 69660.75 \\
         % & & \coolname & 95 & 1 & 2.18 & 4.74 & 0.31 & 1.47 & 30.55 & 7.03 & 7.96 & 49617.04\\
         \hline
         \multirow{2}{*}{Apollo Virtual} & \multirow{2}{*}{42} & Apollo & 4.10 & 8.43 & 0.43 & 1.75 & 78.48 & 4.06 & 13.95 & 157633.50 \\
         \cline{3-11}
         & & \coolname & 4.59 & 9.18 & 0.50 & 1.95 & 78.22 & 4.39 & 6.00  & 210024.93\\
         \hline
    \end{tabular}
    \vspace{-0.5cm}
    \label{tab:comparison_with_official_scenarios2}
\end{table*}

\noindent \emph{\textbf {RQ2: Are these driving strategy repairs applicable to common driving scenarios?}}
To answer this question, we applied all the $\mu$Drive driving repair scripts generated by \coolname across the eight scenarios mentioned above. In total, there are 22 different driving strategy repair programs. 
For detailed information on these driving strategy repair programs, we refer readers to \cite{source_code}.
We applied these strategy repairs to all the official scenarios provided by Apollo across three maps: \emph{Sunnyvale}, \emph{San Mateo}, and \emph{Apollo Virtual}.
For the \emph{Sunnyvale} map, there are 114 different scenarios. The \emph{San Mateo} map contains 103 different scenarios, while the \emph{Apollo Virtual} map includes 52 different scenarios. These scenarios cover most situations encountered during daily city driving, such as passing traffic lights, yielding to pedestrians and priority vehicles, cutting in, changing lanes, overtaking, and making U-turns. For detailed descriptions of these official scenarios provided by Apollo, refer to~\cite{Apollostudio}. 

First, we compared Apollo with \coolname (i.e.~Apollo with the driving repairs applied) regarding \emph{Completion Rate} and \emph{Accidents}, as shown in Table~\ref{tab:comparison_with_official_scenarios1}.  Regarding \emph{Completion Rate}, we evaluated whether the AV successfully reached the destination and completed the journey, as indicated in the `Finish' column. All scenarios completed by Apollo were also completed by \coolname. Additionally, \coolname could finish extra scenarios where Apollo got stuck. For example, in some scenarios, Apollo failed to overtake a stationary vehicle ahead because it followed too closely, while \coolname completed these scenarios by maintaining a larger following distance. Regarding \emph{Accidents}, we examined the number of accidents caused by Apollo and \coolname, as shown in the `Accident' column. \coolname not only avoided causing new accidents but also prevented an accident in one scenario. It is important to note that all remaining accidents were not caused by the AV. They were caused by `irrational' vehicles or pedestrians colliding with the driver from behind. Typically, this occurred when the AV had reached its destination and stopped, and another vehicle hit it from the back.
%They were caused by `crazy' background vehicles or pedestrians colliding with the AV from behind. Typically, this occurred when the AV had reached its destination and stopped, and a background vehicle hit it from the back.




To further investigate RQ2, we conducted an in-depth analysis. Specifically, for scenarios that Apollo and \coolname successfully completed, we analysed various aspects of the trajectories, including the speed and acceleration at each point, the distance to the nearest obstacle, the total duration of vehicle stops, and the energy consumption, as shown in Table~\ref{tab:comparison_with_official_scenarios2}.
Regarding \emph{Speed and Acceleration}, we compared the speed and acceleration between Apollo and \coolname, as shown in the `Speed($m/s$)' and `Acceleration($m/s^2$)' columns. Both the average and maximum speeds were examined, with the values in the table representing the averages across all scenarios. Overall, the AV operated at slower speeds under Apollo compared to \coolname. However, this does not imply that \coolname drives more aggressively than Apollo. In fact, \coolname only increased its speed when there were no other vehicles or pedestrians nearby, ensuring safe and considerate driving behaviour.
Regarding \emph{Obstacle Distance}, we examined the average and maximum distances to other objects, as shown in the `Obstacle Distance(m)' column. The results indicated that \coolname maintained a greater distance from other vehicles despite its higher speed, demonstrating both efficiency and safety.
Regarding \emph{Stop Time}, we examined the time that the AV stopped on the road, as shown in the `Stop Time(s)' column. The results indicated that \coolname had less stop time than Apollo, suggesting a smoother driving experience.
Regarding \emph{Energy}, we provided a rough estimate of the average energy consumption for Apollo and \coolname. The energy was calculated using the formula: $\sum_{t=1}^{n-1} \frac{1}{2}m (v_{t+1}^2 - v_t^2)$, where $m$ is the vehicle's mass (1500 kg), $n$ is the length of the trace, and $v_t$ is the speed of the AV at time step $t$. This formula measures the energy consumption based on changes in speed. \coolname consumed more energy than Apollo because it generally travelled at higher speeds, which involved more frequent acceleration and deceleration processes.


Based on these detailed checks, we conclude that the driving strategy repairs provided by \coolname not only promote smoother driving but also contribute to fewer accidents, underscoring its suitability for various common driving scenarios.




\begin{table}[ht]
\centering
\caption{Computational effort required by \coolname}
\vspace{-0.2cm}\scriptsize
\label{tab:effort}
\begin{tabular}{|>{\centering}m{0.05\linewidth}|>{\centering}m{0.058\linewidth}|>{\centering}m{0.05\linewidth}|>{\centering}m{0.05\linewidth}|>{\centering}m{0.05\linewidth}|>{\centering}m{0.05\linewidth}|>{\centering}m{0.05\linewidth}|>{\centering}m{0.05\linewidth}|>{\centering}m{0.05\linewidth}|m{0.05\linewidth}|}
\hline
\multicolumn{2}{|c|}{step} & S1 & S2 & S3 & S4 & S5 & S6 & S7 & S8 \\
\hline
 \multicolumn{2}{|c|}{trace} & 329s & 351s & 281s & 127s & 61s & 63s & 395s & 529s \\ \hline
 \multicolumn{2}{|c|}{localisation} & 187s & 200s & 167s & 156s & 134s & 155s & 205 & 168 \\ \hline
 \multicolumn{2}{|c|}{prompt} & 0.22s & 0.24s & 0.21s & 0.23s & 0.16s & 0.17s & 0.16s & 0.22 \\ \hline
 \multirow{3}{*}{query} & time & 10.6s & 11.4s & 14.1s & 8.9s & 14.9s & 8.6s & 23.3s & 10.8s \\ \cline{2-10}
 & input & 7352 & 7352 & 7436 & 7435 & 7508 & 7498 & 7504 & 7350 \\ \cline{2-10}
 & output & 179 & 163 & 121  & 185 & 97 & 81 & 123 & 82 \\ \hline
  \multicolumn{2}{|c|}{overall time} & 527s & 563s & 462s & 292s & 210s & 227s & 624s & 708s \\ \hline
  \multicolumn{2}{|c|}{cost(\$)} & 0.079 & 0.078 & 0.078 & 0.080 & 0.078 & 0.077 & 0.079 & 0.076 \\ \hline
\end{tabular}
\vspace{-0.1cm}
\end{table}

\noindent \emph{\textbf {RQ3: How much effort is required to compute repairs?}} To answer this question, we present a detailed breakdown of time and token consumption (using model ChatGPT 4 turbo) for \coolname, as shown in Table~\ref{tab:effort}.
The `step' column lists all necessary steps for \coolname to generate driving strategy repairs. The `trace' step involves converting a given record into a trace. The `localisation' refers to identifying \emph{near-miss} and \emph{violation} moments. The `prompt' involves automatically generating prompts for the LLM input, while `query' denotes querying the LLM for a response. 

We detail the time consumption for each step, all measured in seconds. For the whole process, the most time-consuming steps involve two parts: trace generation and moment localisation. Trace generation takes a few minutes due to the thousands of time steps within a trace, typically about one hundred time steps per second. Moment localisation involves calculating the robustness value multiple times, resulting in relatively high time consumption. However, since our method is offline, trace generation and moment localisation need to be performed only once per test case, making the process efficient for practical use. For each test case, the whole process typically takes around 10 minutes to perform, always below 15 minutes, on a desktop with 32GB of RAM, an Intel i7-10700k CPU, and an RTX 2080Ti graphics card, which is a manageable effort.

Additionally, we measure the number of tokens required for querying the LLM. The `input' and `output' rows in the Table~\ref{tab:effort} indicate the average number of input and output response tokens for ChatGPT 4 turbo.  
The number of tokens, including those for images, is calculated using ChatGPT's official tool~\cite{tokenizer_web}.
At the time of experimentation, the direct cost for 1 million input prompt tokens was \$10, while 1 million output response tokens cost \$30. This indicates that each driving suggestion costs less than \$0.08, making it affordable, as shown in the last row of the table.


\begin{table}[h]
\centering
\caption{Comparison of \coolname and a text-only method}
\vspace{-0.2cm}\scriptsize
\label{tab:images_text_transposed}
% \resizebox{0.7\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Scene} & \multicolumn{2}{c|}{performance} & \multicolumn{2}{c|}{input token} & \multicolumn{2}{c|}{output token} & \multicolumn{2}{c|}{cost(\$)} \\ \cline{2-9}
 & ours & text & ours & text & ours & text & ours & text\\ 
\hline
S1 & $30\%$ & $5\%$     & 7352  & 8370 & 179& 308 & 0.079 & 0.092\\ \hline
S2 & $50\%$ & $0\%$     & 7352  & 7294 & 163& 286 & 0.078 & 0.082\\ \hline
S3 & $45\%$ & $5\%$     & 7436  & 8451 & 121& 245 & 0.078 & 0.092\\ \hline
S4 & $100\%$ & $15\%$   & 7435  & 7741 & 185& 247 & 0.080 & 0.085\\ \hline
S5 & $20\%$ & $0\%$     & 7508  & 8442 & 97 & 294 & 0.078 & 0.093\\ \hline
S6 & $100\%$ & $100\%$  & 7498  & 7433 & 81 & 189 & 0.077 & 0.080\\ \hline
S7 & $50\%$ & $20\%$    & 7504  & 10978& 123& 351 & 0.079 & 0.120\\ \hline
S8 & $15\%$ & $0\%$     & 7350  & 7240 & 82 & 241 & 0.076 & 0.080\\ \hline
\end{tabular}%
\vspace{-0.3cm}
\end{table}


\noindent \emph{\textbf {RQ4: What is the impact of using images for critical moments instead of text?}}
\coolname utilises visualisations of violation and near-miss moments as part of its prompt for the MLLM. 
But what would happen if we described these scenarios using only textual prompts instead? To explore this question, we establish a text-only prompt-based method as our baseline. 
To ensure a fair comparison, we keep all other design elements consistent with \coolname, except that descriptions of the violation and near-miss moments are provided solely in text. 
We extract key information from records following the LawBreaker methodology~\cite{Sun-Poskitt-et_al22a}, crafting detailed descriptions for each variable to ensure clarity. These descriptions are formatted and refined using ChatGPT-4 Turbo, optimising them for MLLM interpretation. An example prompt is available in our repository for reference~\cite{source_code}.


Table~\ref{tab:images_text_transposed} compares \coolname and the text-only method in terms of performance, input/output token usage, and cost. The performance threshold, set at 15 based on empirical experimentation (discussed in RQ1), includes 20 repetitions per scenario. The `Performance' column shows the proportion of successful driving strategy repairs generated by \coolname (referred to as `ours') and the text-only method (referred to as `text'). Here, success indicates that the AV correctly follows the traffic rule after the repair. The `Input/Output Tokens' columns display the average input and output token counts per query. As shown, using images significantly enhances performance while reducing costs per query. Images effectively convey spatial details that are challenging to capture in text yet are easily processed by the vision modality. 
Interestingly, image prompts consume fewer input tokens than detailed text descriptions. 
Moreover, text-heavy prompts often result in more output tokens, suggesting that the MLLM is more prone to generating extraneous driving strategy repairs when overloaded with extensive text inputs. 



\noindent \emph{\textbf {Threats to Validity. }}
The inherent randomness of generative models and the limitations of the original ADS introduce threats to validity. First, \coolname cannot guarantee the effectiveness of every generated suggestion. To mitigate this, we generate 20 driving strategy repairs per case and evaluate them in an AV simulator, leveraging fast and cost-effective querying for robustness.

Some scenarios remain beyond \coolname's full control, such as rear-end collisions, where following distance depends on the trailing vehicle. While risk-reduction measures exist, complete prevention is challenging. Moreover, \coolname may propose valid repairs that prove ineffective due to ADS design constraints. For instance, Apollo’s overly cautious behaviour might prevent overtaking, even when \coolname suggests it. Refining text prompts can help address such issues.

Integrating $\mu$Drive into an ADS poses challenges, but once incorporated, it streamlines further modifications, enabling efficient system refinement through various $\mu$Drive scripts.


