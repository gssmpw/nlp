\section{Related Work}
\subsection{Standard Machine Learning Approaches}
Inspired by the success of deep learning in other domains, numerous efforts have sought to apply self-supervised learning to tabular data to develop transfer learning-ready models. These approaches include masked feature prediction **Rasmus et al., "Self-Supervised Learning for Transferable Representations"**____**, feature corruption correction **Vincent et al., "Extracting and Composing Robust Features with Denoising Autoencoders"**____**, and contrastive pre-training **Chen et al., "Improved Baselines with Momentum Contrast for Self-Supervised Learning"**____**. However, comparative studies indicate that gradient-boosted tree ensembles still outperform these methods **Hastie et al., "Gradient Boosting Machine"**. More recently, Nam et al. **Nam et al., "Self-generated Tasks from UNlabeled Tables (STUNT)"** introduced Self-generated Tasks from UNlabeled Tables (STUNT), leveraging self-generated few-shot tasks for tabular learning, though its reliance on large unlabeled datasets may limit practical applicability. Additionally, Hollmann et al. **Hollmann et al., "TabPFN: Tabular Prior-data Fitted Network"** proposed the Tabular Prior-data Fitted Network (TabPFN), a tabular foundation model pre-trained on millions of synthetic datasets.

\subsection{Large Language Models in Tabular Learning}
Most approaches integrating large language models (LLMs) into tabular learning rely on encoding task and feature descriptions in natural language, serializing the data, and leveraging LLMs for inferenceâ€”either through in-context learning **Brown et al., "Language Models are Few-Shot Learners"** or additional fine-tuning **Zhuo et al., "Fine-Tuning Pre-Trained Language Models for Downstream Tasks"**. However, these methods face significant drawbacks, including the high cost of LLM inference for individual samples and the computational demands of fine-tuning.

In sensitive domains such as medicine or finance **Liu et al., "Machine Learning in Medicine"**, where transparency is critical, the opaque decision-making of LLMs is less desirable than traditional, smaller models. Alternative approaches involve using LLMs to generate synthetic examples to augment existing datasets, employing both in-context learning and fine-tuning **Zhuo et al., "Fine-Tuning Pre-Trained Language Models for Downstream Tasks"**. However, these methods inherit the same scalability issues, particularly when dealing with high-dimensional datasets, where generating sufficiently large datasets becomes computationally expensive.

Recently, Han et al. **Han et al., "FeatLLM: Feature Learning via Large Language Models"** introduced FeatLLM, a novel approach that utilizes LLMs as feature engineers. Instead of directly performing inference, FeatLLM employs code-generating LLMs to create preprocessing functions that transform the original dataset into a more suitable representation for few-shot classification. This method implements an ensemble classifier to combine insights from multiple feature transformations, improving robustness and classification accuracy. FeatLLM significantly reduces resource requirements by relying solely on pre-trained LLMs with API-level access. Moreover, FeatLLM outperforms existing fine-tuned and in-context learning approaches while maintaining lower computational costs. However, even though FeatLLM achieves state-of-the-art performance on few-shot tabular classification problems, its many iterations of rule and preprocessing function generation incur significant costs. Furthermore, during preprocessing, FeatLLM produces only binary features, which may limit expressiveness compared to the original data.

\subsection{Explanation Guided Learning}
A growing line of research explores enhancing model behavior through additional supervision derived from explainable artificial intelligence (XAI) techniques. This field can be broadly categorized into local explanation-guided learning and global explanation-guided learning **Adadi et al., "peeking Inside the Black Box: A Survey on Explainable Artificial Intelligence (XAI) Table of Contents"**.

Local explanation guidance applies supervision signals or regularization terms to individual model explanations, steering learning at the sample level. This approach is more prevalent due to the extensive development of local explanation techniques, particularly in the image domain, such as Grad-CAM **Selvaraju et al., "Grad-CAM: Visual Explanations for Convolutional Neural Networks"**____**, Layer-wise Relevance Propagation (LRP) **Bach et al., "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layered Sum of Inputs In Decomposition"**____**, and attention-based attributions **Zhang et al., "Top-down neural document summarization"**. Ross et al. **Ross et al., "Improving the Adversarial Robustness and Interpretability of Deep Neural Networks via Reweighted Maximally Permissive Explanation Regularization (MRER)"** propose regularizing differentiable models by penalizing input gradients, aligning them with expert-defined attribution maps. Dharma et al. **Dharma et al., "Visualizing and Explaining Black Box Models using Object Based Explanations"** use object bounding boxes as explanation supervision signals. In text classification, several studies leverage per-sample human-annotated rationales **Chen et al., "Towards Explaining Convolutional Neural Networks Visually"**. Gao et al. **Gao et al., "Local Explanation of Deep Learning Models for Text Classification Tasks"** demonstrate the effectiveness of local explanation supervision under limited training data. However, a key limitation of this approach is its reliance on per-sample attribution annotations, which are often difficult and costly to obtain, particularly in expert-driven fields like medicine.

Global explanation guidance, in contrast, does not require instance-level attributions, instead offering a broader, more scalable approach to shaping model behavior. Liu et al. **Liu et al., "Fairness and Bias in AI: A Survey"** reduce undesired biases by penalizing nonzero attributions on sensitive tokens. Erion et al. **Erion et al., "Visualizing Black Box Models using Aggregated Attributions"** aggregate local feature attributions via expected gradients to improve interpretability. Weinberger et al. **Weinberger et al., "Meta-Feature Extraction for Deep Attribution Analysis"** extract prior knowledge from multiple gene expression datasets to construct meta-features, training a deep global attribution model alongside a predictive model with a regularization loss. However, this method assumes the availability of additional datasets related to the problem, which may not always be feasible.