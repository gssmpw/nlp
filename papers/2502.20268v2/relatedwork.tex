\section{Related Work}
\subsection{Standard Machine Learning Approaches}
Inspired by the success of deep learning in other domains, numerous efforts have sought to apply self-supervised learning to tabular data to develop transfer learning-ready models. These approaches include masked feature prediction \cite{DBLP:conf/aaai/ArikP21,DBLP:journals/corr/abs-2206-08564}, feature corruption correction \cite{DBLP:conf/iclr/BahriJTM22,DBLP:conf/nips/YoonZJS20}, and contrastive pre-training \cite{DBLP:journals/corr/abs-2106-01342}. However, comparative studies indicate that gradient-boosted tree ensembles still outperform these methods \cite{DBLP:conf/nips/GrinsztajnOV22,DBLP:journals/inffus/Shwartz-ZivA22}. More recently, Nam et al. \cite{DBLP:conf/iclr/NamTLLS23} introduced Self-generated Tasks from UNlabeled Tables (STUNT), leveraging self-generated few-shot tasks for tabular learning, though its reliance on large unlabeled datasets may limit practical applicability. Additionally, Hollmann et al. \cite{DBLP:journals/nature/HollmannMPKKHSH25} proposed the Tabular Prior-data Fitted Network (TabPFN), a tabular foundation model pre-trained on millions of synthetic datasets.

\subsection{Large Language Models in Tabular Learning}
Most approaches integrating large language models (LLMs) into tabular learning rely on encoding task and feature descriptions in natural language, serializing the data, and leveraging LLMs for inferenceâ€”either through in-context learning \cite{DBLP:journals/corr/abs-2304-13188} or additional fine-tuning \cite{DBLP:conf/nips/DinhZZLGRSP022,DBLP:conf/aistats/HegselmannBLA0S23,DBLP:conf/ijcai/0010GX024}. However, these methods face significant drawbacks, including the high cost of LLM inference for individual samples and the computational demands of fine-tuning.

In sensitive domains such as medicine or finance \cite{DBLP:journals/npjdm/ShickWKWDPSD24}, where transparency is critical, the opaque decision-making of LLMs is less desirable than traditional, smaller models. Alternative approaches involve using LLMs to generate synthetic examples to augment existing datasets, employing both in-context learning and fine-tuning \cite{DBLP:conf/icml/SeedatHBS24,DBLP:journals/corr/abs-2302-02041,DBLP:conf/emnlp/ZhangWYJL23}. However, these methods inherit the same scalability issues, particularly when dealing with high-dimensional datasets, where generating sufficiently large datasets becomes computationally expensive.

Recently, Han et al. \cite{DBLP:conf/icml/0001YAP24} introduced FeatLLM, a novel approach that utilizes LLMs as feature engineers. Instead of directly performing inference, FeatLLM employs code-generating LLMs to create preprocessing functions that transform the original dataset into a more suitable representation for few-shot classification. This method implements an ensemble classifier to combine insights from multiple feature transformations, improving robustness and classification accuracy. FeatLLM significantly reduces resource requirements by relying solely on pre-trained LLMs with API-level access. Moreover, FeatLLM outperforms existing fine-tuned and in-context learning approaches while maintaining lower computational costs. However, even though FeatLLM achieves state-of-the-art performance on few-shot tabular classification problems, its many iterations of rule and preprocessing function generation incur significant costs. Furthermore, during preprocessing, FeatLLM produces only binary features, which may limit expressiveness compared to the original data.

\subsection{Explanation Guided Learning}
A growing line of research explores enhancing model behavior through additional supervision derived from explainable artificial intelligence (XAI) techniques. This field can be broadly categorized into local explanation-guided learning and global explanation-guided learning \cite{DBLP:journals/csur/GaoGJHYZ24}.

Local explanation guidance applies supervision signals or regularization terms to individual model explanations, steering learning at the sample level. This approach is more prevalent due to the extensive development of local explanation techniques, particularly in the image domain, such as Grad-CAM \cite{DBLP:journals/ijcv/SelvarajuCDVPB20}, Layer-wise Relevance Propagation (LRP) \cite{Bach2015}, and attention-based attributions \cite{DBLP:conf/acl/AbnarZ20}. Ross et al. \cite{DBLP:conf/ijcai/RossHD17} propose regularizing differentiable models by penalizing input gradients, aligning them with expert-defined attribution maps. Dharma et al. \cite{DBLP:journals/corr/abs-2108-10131} use object bounding boxes as explanation supervision signals. In text classification, several studies leverage per-sample human-annotated rationales \cite{DBLP:conf/emnlp/ChoiPYH20,DBLP:conf/kdd/KanchinadamWYF20,DBLP:journals/corr/abs-1908-06870}. Gao et al. \cite{DBLP:conf/kdd/GaoSBGH022} demonstrate the effectiveness of local explanation supervision under limited training data. However, a key limitation of this approach is its reliance on per-sample attribution annotations, which are often difficult and costly to obtain, particularly in expert-driven fields like medicine.

Global explanation guidance, in contrast, does not require instance-level attributions, instead offering a broader, more scalable approach to shaping model behavior. Liu et al. \cite{DBLP:conf/acl/LiuA19} reduce undesired biases by penalizing nonzero attributions on sensitive tokens. Erion et al. \cite{DBLP:journals/natmi/ErionJSLL21} aggregate local feature attributions via expected gradients to improve interpretability. Weinberger et al. \cite{DBLP:conf/nips/WeinbergerJL20} extract prior knowledge from multiple gene expression datasets to construct meta-features, training a deep global attribution model alongside a predictive model with a regularization loss. However, this method assumes the availability of additional datasets related to the problem, which may not always be feasible.