\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[final]{ACL2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{makecell}
%\usepackage[table]{xcolor} %
\usepackage{tcolorbox}
    \tcbuselibrary{breakable}
\usepackage{verbatim}
\newcommand{\draftcomment}[1]{#1}
\newcommand{\manluo}[1]{\draftcomment{\textcolor{blue}{\small [#1]$_{{ML}}$}}}
\newcommand{\neale}[1]{\draftcomment{\textcolor{purple}{\small [#1]$_{{NR}}$}}}
\newcommand{\mh}[1]{\draftcomment{\textcolor{green}{\small [#1]$_{{MH}}$}}}
\newcommand{\sungduk}[1]{\draftcomment{\textcolor{magenta}{\small [#1]$_{{SY}}$}}}

\title{Semantic Specialization in MoE Appears with Scale: A Study of DeepSeek-R1 Expert Specialization}


\author{
    Matthew Lyle Olson$^{1,\dagger}$ \:
    Neale Ratzlaff$^{1,\dagger}$ \:
    Musashi Hinck$^{1,\dagger}$ \\
    \textbf{Man Luo}$^1$ \:
    \textbf{Sungduk Yu}$^1$ \:
    \textbf{Chendi Xue}$^2$ \:
    \textbf{Vasudev Lal}$^1$ 
    \vspace{0.2cm} \\
    $^1$Intel Labs \: $^2$Intel Corporation \\
}




\begin{document}
\maketitle
\begin{abstract}
DeepSeek-R1, the largest open-source Mixture-of-Experts (MoE) model, has demonstrated reasoning capabilities comparable to proprietary frontier models. Prior research has explored expert routing in MoE models, but findings suggest that expert selection is often token-dependent rather than semantically driven.
Given DeepSeek-R1’s enhanced reasoning abilities, we investigate whether its routing mechanism exhibits greater semantic specialization than previous MoE models. To explore this, we conduct two key experiments: (1) a word sense disambiguation task, where we examine expert activation patterns for words with differing senses, and (2) a cognitive reasoning analysis, where we assess DeepSeek-R1’s structured thought process in an interactive task setting of DiscoveryWorld. We conclude that DeepSeek-R1’s routing mechanism is more semantically aware and it engages in structured cognitive processes. 
\end{abstract}

\input{01_introduction}

\input{02_related_work}

\input{03_method}

\input{04_experiments}

\input{05_conclusion}





\bibliography{custom}
\bibliographystyle{acl_natbib}

\clearpage
\input{appendix}


\end{document}
