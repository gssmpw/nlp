\section{Experiments} %

\subsection{Words-in-Context} 

We leverage polysemy to test for semantic specialization in expert activation. If words that are written the same but have different meanings are routed differently, then this is evidence that routing occurs based on meaning. To test this hypothesis, we use the WiC dataset~\cite{pilehvar2018wic} (CC BY-NC 4.0), which consists of two types of paired sentences: 1) pairs where a target word has the same sense and 2) pairs where the target word has different senses across sentences.
For each target words and sentence, we prompt the model with:
"Please define \{target word\} in this context."
Additionally, we include an internal reasoning step:
``<think> Okay, so I need to figure out the meaning of the word \{target word\}.'' to ensure that the subsequent inference isolates the word in question instead of additional thinking tokens.

\subsection{DiscoveryWorld}
\input{discoveryworld}

\subsubsection*{Sparse Autoencoders}

To get a clearer picture of how these patterns are invoked internally, we 
employ SAEs to learn a mapping between the internal activations of R1 and a set of underlying semantic structures exhibited by the model. 
Briefly, an SAE learns a compressed representation of input vectors $x \in \mathbb{R}^d$. The encoder maps inputs to a higher-dimensional latent space, while the decoder reconstructs the input from the latent representation. Given an encoding dimension $n$, we define the encoder and decoder as:  
$
z = \max(0, W_{\text{enc}} x + b_{\text{enc}})
$
 and
$
\hat{x} = W_{\text{dec}} z
$

where $W_{\text{enc}} \in \mathbb{R}^{n \times d}$ and $W_{\text{dec}} \in \mathbb{R}^{d \times n}$ are the learnable weight matrices of the encoder and decoder respectively, and $b_{\text{enc}} \in \mathbb{R}^{n}$ is a bias term.  
The model is trained using a loss function that balances reconstruction accuracy and sparsity:  
$L = \| x - \hat{x} \|_2^2 + \lambda \| z \|_1$


where the first term is the mean squared error for reconstruction, and the second term is an $L_1$ penalty that encourages sparsity in the latent activations, where we choose $\lambda = 5$ as the trade-off between reconstruction fidelity and sparsity.  











