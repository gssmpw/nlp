\section{Related Work}
Current research on expert specialization in MoE models is sparse, yet available studies reveal little evidence of semantic-level differentiation. For example, \citet{xue2024openmoe} tracked token routing patterns across datasets segmented by different topics, languages, and tasks, but failed to find any coherent pattern at such high-level semantics. Rather, they found indications of token-level specialization, mainly concerning low-level semantic features like special characters or auxiliary verbs. Similar findings have been reported in studies using independently developed MoE models \citep[e.g.,][]{zoph2022stmoe, jiang2024mixtral, fan2024towards}.

While some neuroscience research has provided evidence that the brain functions like a Mixture of Experts \citep{stocco2010conditional,o2021and}---suggesting the possibility of semantic-level specialization---other studies have shown that MoE models with random routing can perform comparably to those using the more common top-k routing approach \citep{roller2021hash, zuo2021taming, ren2023pangu}. One potential explanation for these mixed results is that prior models (using 8 to 32 experts) might not have been sufficiently expressive to capture fine-grained specialization patterns. The recently-released DeepSeek v3, featuring an extensive network of experts (256 routed specialists alongside one shared generalist expert), provides us with a unique opportunity. Hence, in this study, we test whether a more capable MoE architecture exhibits semantic-level expert specialization.