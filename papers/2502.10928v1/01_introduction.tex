\section{Introduction}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/two_sae_heads.png} 
    \caption{Left: identified reasoning tokens of SAE head 15376 (highlights indicate non-zero head activation) on DiscoveryWorld chain of thought generations. This head activates when the model analyzes its hypotheses. Right: tokens from SAE head 12649. This head activates when R1 catches an internal reasoning error.}
    \label{fig:two_sae_heads_text_example}
    \vspace{-1em}
\end{figure*}

\renewcommand\thefootnote{}  % Removes footnote number
\footnotetext{$\dagger$ The first authors contributed equally; order was determined randomly.} 
Since their popularization in \citet{fedus2022switch}, the Mixture-of-Experts (MoE) architecture \citep{ jacobs1991adaptive} has been integrated into many state-of-the-art large language models (LLMs) \citep{lieber2024jamba,jiang2024mixtral,liu2024deepseek}. 
Recently, DeepSeek-R1~\cite{guo2025deepseek} (hereafter abbreviated R1), based on the MoE architecture, was released as the largest open-source (MIT License) LLM by total parameter count.
Notably, it is the first open-source model to achieve performance comparable to OpenAI’s o-series of models \citep{zhong2024evaluation}.
Trained with reinforcement learning (RL)~\cite{shao2024deepseekmath}, R1 demonstrates remarkable reasoning~\cite{wei2022chain}, %
including emergent behaviors such as the ``aha moment'' \citep{guo2025deepseek}.

The release of R1's  model weights opens up several exciting research opportunities, one of which is the study of expert routing behavior in very large scale MoEs. 
Several prior studies have explored expert activation patterns in MoE models, hypothesizing that each expert may specialize in specific domains, tasks, or topics~\cite{zoph2022stmoe,jiang2024mixtral,xue2024openmoe}. 
While it is intuitive to expect some degree of semantic specialization, previous research has struggled to establish a clear semantic role for individual experts, concluding instead that expert activation is primarily token-dependent rather than being driven by deeper semantic relationships.

Given the scale and strong reasoning capabilities %
of R1, we investigate whether its expert routing exhibits greater semantic specialization than previous MoE models.
We design two experiments to analyze R1’s routing mechanisms. First, we employ a word sense disambiguation (WSD) task ~\cite{pilehvar2018wic}, where a target word appears in two sentences, either with the same semantic value (sense) or differing senses. 
Our results show significantly higher expert overlap where the word sense is the same than where it differs.
In contrast, the rate of expert overlap differs less between the two cases in previous MoE models like Mixtral~\cite{jiang2024mixtral}. 
Additionally, we analyze R1's reasoning structure to further understand its cognitive behavior using the agentic \textit{DiscoveryWorld} environment~\cite{jansen2024discoveryworld} (Apache 2.0) as a testbed. We qualitatively probe the model's chain of thought with a Sparse Autoencoder (SAE); a modern tool for interpreting LLMs~\cite{cunningham2023sparse}. Our findings reveal that R1 follows a structured reasoning approach, incorporating self-evaluation, and hypothesis testing (see Figure (\ref{fig:two_sae_heads_text_example}) for an example). 
Finally, R1's thought patterns favor specific experts, indicating that the expert specialization extends to cognitive processes. 

In summary, this study explores the emergent behaviors of DeepSeek-R1, with a particular focus on expert routing and cognitive specialization. Our findings suggest that R1’s routing mechanism demonstrates greater semantic specialization compared to prior MoE models, which may contribute to its remarkable performance.

