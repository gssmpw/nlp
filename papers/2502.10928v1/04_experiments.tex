\section{Results}
\paragraph{Word-in-Context} 
For 1K pairs of sentences in WiC, we collect router activations for DeepSeek-R1, Mixtral-8x7B and Mixtral-8x22B and record the number of overlapping experts at each layer.

We compare the average rate of overlap in sentence pairs where the target word has the same sense versus sentence pairs where it has a different meaning. If sentence pairs where the target word has different senses have higher expert overlap than sentence pairs where the target word has the same sense, then this is evidence that expert routing differentiates on a semantic basis.


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/wic_overlap_by_layer.pdf}
    \caption{\textbf{Average normalized expert overlap by layer}: we show the normalized mean number of overlapping experts by layer for WiC sentence pairs with same and different target word senses. The shading indicates a bootstrapped 95\% confidence interval.%
    }
    \label{fig:layer_overlap}
\end{figure}


Figure (\ref{fig:layer_overlap}) presents the number of overlapping experts by layer, averaged across sentence pairs in each of the two conditions and by model.
We compute selected expert overlap by normalizing using the following formula:

\begin{equation*}
    \text{score} = \frac{\text{observed} - \text{expected}}{\text{maximum} - \text{expected}}
\end{equation*}

where \textit{observed} is the number of overlapping experts, \textit{maximum} is the maximum number of overlapping experts (2 for Mixtral, 8 for DeepSeek) and \textit{expected} is the expected number of overlapping experts from a random baseline. This ensures a more fair comparison with models of differing number of total and selected experts. See \S\ref{apd:stats_formula} for a derivation of the random baseline.

We find strong evidence for semantic specialization in these experiments; expert overlap is \textbf{lower} for sentence pairs where the target word has different senses than when they are the same. This effect is statistically significant ($p<0.001$) for all models considered when averaged across all layers. See appendix \S\ref{apd:stats_wic} for further details. 
For all models the difference in overlap \textit{increases} in intermediary layers. This supports prior findings that semantic features are more salient in the intermediary layers of LLMs ~\citep{niu2022does, kaplan2024tokens}. %
Our results are also suggestive that this pattern emerges at scale; the difference in expert overlap increases with model size. %





\paragraph{DiscoveryWorld}
At a high level, we find that R1's reasoning traces on DiscoveryWorld display many indicators %
reminiscent of System 2 thinking, such as backtracking, self-evaluation, and situational awareness, see Appendix \S\ref{app:discodetails} for an example. %

Given R1's thinking text on DiscoveryWorld, we want to investigate if its experts are specialized on reasoning strategies. Given any reasoning trace, we can find groups of tokens that correspond to a specific reasoning strategy and observe which experts are subsequently activated. If similar experts are used to process all the tokens for a given reasoning strategy, then we have evidence that the experts also specialize by cognitive pattern. 
To this end, we train an SAE to obtain an atlas of different reasoning patterns (see Figure (\ref{fig:two_sae_heads_text_example}) for an example), and show that R1 tends to activate similar experts for all tokens given by single SAE head (neuron), meaning that the experts are not just semantically specialized, but also control the presence of high level reasoning. 

\paragraph{SAE Training Details}

We evaluate DeepSeek-R1 on the DiscoveryWorld environment: "Reactor Lab", collecting 100 steps through the environment. %
For each step we collect all valid output text including the chain of thought and the corresponding pre-router activations: (the embeddings before expert selection). We consider a generation valid if we have a complete set of "<think>", "</think>" tags. In total we collect 200,000 token-activation pairs. We perform all inference using VLLM~\cite{kwon2023efficient} on Intel\textsuperscript{\textregistered} Gaudi~3 AI accelerators in the Intel\textsuperscript{\textregistered} Tiber\texttrademark~AI Cloud.

We train a standard SAE on these activations using the SAELens library \cite{bloom2024saetrainingcodebase} (MIT License). We trained for 30,000 steps with a batch size of $4096$, learning rate of $5e^{-5}$, SAE width of 28,672, and we reset dead SAE weights after 1K steps. We train the SAE on the activations of layer 7 for a trade-off between early layers with clear token-expert mapping and later layers having high expert selection diversity.

\paragraph{Results}
\input{tables/experts_top_tokens}
As an illustrative example, we choose two tokens associated with reasoning: ``hypothesis'' and ``Wait''. As a baseline, Table (\ref{tab:expert_top_tokens}) shows an expert-token analysis without an SAE. We see that the experts that are most often allocated for ``Wait'', are also chosen for tokens like ``microscope'', ``frequency'', and ``crystal''. These ancillary tokens are objects/quantities from the environment i.e. the subject of reasoning, but yield no additional information about the reasoning process itself. %

\input{tables/sae_expert_Wait_tokens_tbl}

The SAE provides further insight by examining sets of tokens that are linked through the maximal activation of a single SAE head. 
Table (\ref{tab:sae_Wait_main}) shows an example where a single head (active on “Wait”) identifies semantically similar tokens. By inspecting the corresponding SAE activations, we observe tokens such as “bet,” “probably,” and “attempt,” which suggest a cognitive pattern of uncertainty regarding the current strategy. Moreover, we find that this reasoning pattern is most commonly routed to a small set of experts. Examining these tokens and activations in context (e.g., see Figure (\ref{fig:two_sae_heads_text_example})) further illustrates how R1 leverages contextual information in its reasoning process.


We also find that the SAE head corresponding to ``hypothesis'', yields a pattern of overlapping experts along semantically similar tokens such as: ``definitely'', ``perform'', ``analyzing'', ``scientific'', and ``information''. See the appendix \S\ref{apd:more_sae_tables} for more detailed examples. In summation, we find that R1 consistently chooses a small set of experts for reasoning patterns identified by the SAE, indicating that the experts also specialize by thought process.  




