\section{Conclusion}

With access to DeepSeek-R1’s model weights and motivated by its significant improvements in reasoning and cognitive behavior, we analyzed its expert selection mechanism. Our study focused on two key settings: semantic context and reasoning strategy analysis.
In the first experiment, we found that DeepSeek-R1 exhibits stronger semantic specialization than previous MoE models, with expert selection aligning more closely with semantic meaning. In the second experiment, we discovered cognitive specialization, where different experts are responsible for distinct reasoning processes.
Overall, our findings provide a deeper interpretation of DeepSeek-R1’s routing behavior, highlighting its semantic awareness and structured cognitive processing. These insights contribute to a better understanding of how MoE models can be optimized for reasoning and efficiency.
\clearpage
\section*{Limitations}
Since DeepSeek-R1 has significantly more experts than Mistral (256 vs. 8), the probability of selecting overlapping experts is inherently lower from a statistical standpoint. To account for this, we normalized the overlap values based on the expected distribution.
Additionally, the larger number of experts in DeepSeek-R1 could itself be a contributing factor to the emergence of semantic specialization. This raises the question of whether the observed semantic alignment is an intrinsic property of DeepSeek-R1’s specific training setup e.g. device-specific expert allocation~\citep{liu2024deepseek}, fine-tuning via GRPO \citep{shao2024deepseekmath}, or a natural consequence of a more fine-grained expert distribution. Future studies should explore this relationship further to disentangle the effects of model architecture and training setup from routing behavior.


\section*{Ethics Statement}
For each artifact used e.g. R1 model weights, WiC dataset, we follow the intended use, and while we do not believe that our analysis of DeepSeek-R1 poses any risks or ethical considerations, we acknowledge the inherent issues with LLMs that are trained on web-scale or biased data. Outputs from LLMs may raise safety concerns due to hallucinations or bias in the training data.  
