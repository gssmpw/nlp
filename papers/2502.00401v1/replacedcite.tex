\section{Related Works}
\vspace{-2mm}
\textbf{Riemannian Geometry in Graph Neural Networks}. Graph Neural Networks (GNNs) have set new benchmarks for tasks like node classification and link prediction. Recently, non-Euclidean (Riemannian) spaces -- particularly hyperbolic ____ and spherical ____ geometries -- have garnered attention for their ability to produce less distorted representations, aligning well with hierarchical and cyclic data structures, respectively. Several approaches have emerged in this context. (a) \textit{Single Manifold GNNs}: GNNs such as HGAT ____, HGCN ____, and HVAE ____ have demonstrated state-of-the-art performance on tree-like or hierarchical graphs by learning representations in hyperbolic space. (b) \textit{Mixed-Curvature GNNs}: To model more complex topologies (for example, a tree branching from a cyclic graph), mixed-curvature GNNs have been proposed. ____ pioneered this direction by embedding graphs in a product manifold combining spherical, hyperbolic, and Euclidean spaces. Building on this, models like $\kappa$-GCN ____ and Q-GCN ____ extended the GCN architecture ____ to constant-curvature spaces using the $\kappa$-stereographic model and pseudo-Riemannian manifolds, respectively. More recently, ____ proposed a mixed-curvature GNN for self-supervised learning, while FPS-T ____ generalized the Graph Transformer ____ to operate across multiple manifolds.

\textbf{Spectral Graph Neural Networks}. Spectral GNNs employ spectral graph filters ____ to process graph data. These models either use fixed filters, as seen in APPNP ____ and GNN-LF/HF ____, or learnable filters, as demonstrated by ChebyNet ____ and GPRGNN ____, which approximate polynomial filters using Chebyshev polynomials and generalized PageRank, respectively. BernNet ____ expresses filtering operations through Bernstein polynomials. However, many of these methods focus primarily on low-frequency components of the eigenspectrum, potentially overlooking important information from other frequency bands -- particularly in heterophilic graphs. Models like GPRGNN and BernNet address this by exploring the entire spectrum, performing well across both homophilic and heterophilic graphs. GPRGNN stands out among them because it can express several polynomial filters and incorporate node features and topological information. Despite these advances, current mixed-curvature and spectral GNNs face significant limitations (\texttt{L1}, \texttt{L2}, \texttt{L3}) that constrain their performance. To the best of our knowledge, this work is the first to unify \textit{geometric} and \textit{spectral} information within a single model. Before presenting the architecture of \modelname, we introduce some key preliminary concepts in the following section.