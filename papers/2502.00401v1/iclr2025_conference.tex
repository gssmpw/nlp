\PassOptionsToPackage{prologue,dvipsnames}{xcolor}
\documentclass{article} % For LaTeX2e
% \hyphenpenalty=1000
% \hbadness=10000
\usepackage{iclr2025_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
% \usepackage[none]{hyphenat}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{minitoc}
\usepackage{url}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{tikz-cd}
\usepackage{amssymb} 
\usepackage{caption}
\usepackage{algorithm}
\usepackage{subcaption}
\usepackage{imakeidx}
\usepackage{tabularx}
\makeindex[name=appendix,title=Index]
\usepackage{enumitem}
\usepackage{titletoc}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\setitemize{leftmargin=5mm}
\setenumerate{leftmargin=5mm}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\newcommand{\tp}[1]{\scriptsize{$\pm$#1}}
\newcommand{\myEmphasis}[1]{{\bf #1}}
\newcommand{\novelty}{\myEmphasis{ Novelty}\xspace}
\newcommand{\effectiveness}{ \myEmphasis{ Effectiveness}\xspace}
\newcommand{\generality}{ \myEmphasis{ Generality}\xspace}
\usepackage[toc,page,header]{appendix}
\title{Spectro-Riemannian Graph Neural Networks}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\newcommand{\daggerthanks}[1]{\footnotemark[1]\footnotetext[1]{#1}}
% \makeatletter
% \def\@fnsymbol#1{\ifcase#1\or \dagger\else\@arabic{#1}\fi}
% \makeatother

\makeatletter
\renewcommand\@fnsymbol[1]{%
  \ensuremath{%
    \ifcase#1\or \dagger\or \ddagger\or \mathsection\or
      \mathparagraph\or \|\or **\or \dagger\dagger\or \ddagger\ddagger
    \else\@ctrerr\fi}}
\makeatother

\author{
  Karish Grover\textsuperscript{\textnormal{1,}}\thanks{The work was done during Karish Grover's internship at Amazon, US.} , 
  Haiyang Yu\textsuperscript{\textnormal{2}}, 
  Xiang Song\textsuperscript{\textnormal{3}}, 
  Qi Zhu\textsuperscript{\textnormal{3}},
  Han Xie\textsuperscript{\textnormal{3}}, \\
  \textbf{
    Vassilis N. Ioannidis\textsuperscript{\textnormal{3}}, 
    Christos Faloutsos\textsuperscript{\textnormal{1}}
  } \\
  \textsuperscript{1}Carnegie Mellon University, 
  \textsuperscript{2}Texas A\&M University,
  \textsuperscript{3}Amazon \\
  \texttt{\{karishg,christos\}@cs.cmu.edu}, 
  \texttt{\{haiyang\}@tamu.edu}, \\
  \texttt{\{xiangsx,qzhuamzn,hanxie,ivasilei\}@amazon.com} \\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\modelname}{\texttt{CUSP}} %or SPRING
\newcommand{\modelshort}{Cusp} %or SPRING
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\modela}{$\mathbb{H}^{24} \times \mathbb{S}^{24}$}
\newcommand{\modelc}{$\mathbb{H}^{8} \times \mathbb{S}^{8} \times \mathbb{E}^{32}$}
\newcommand{\modelb}{$(\mathbb{H}^{8})^2 \times (\mathbb{S}^{8})^2 \times \mathbb{E}^{16}$}
\newcommand{\modeld}{$\mathbb{H}^{16} \times (\mathbb{S}^{16})^2$}
\newcommand{\modele}{$(\mathbb{H}^{16})^2 \times \mathbb{E}^{16}$}
\newcommand{\modelf}{$\mathbb{H}^{24} \times \mathbb{E}^{24}$}
\newcommand{\modelg}{$\mathbb{S}^{24} \times \mathbb{E}^{24}$}
\newcommand{\modelh}{$\mathbb{H}^{16} \times \mathbb{S}^{16} \times \mathbb{E}^{16}$}
\newcommand{\modeli}{$(\mathbb{S}^{8})^2 \times \mathbb{E}^{32}$}
\newcommand{\modelj}{$(\mathbb{H}^{16})^3$}
\newtheorem{theorem}{Theorem}
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}
\newtheorem{rmk}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
% \hyphenpenalty=75000
% \exhyphenpenalty=100
\maketitle

\begin{abstract}
Can integrating \textit{spectral} and \textit{curvature} signals unlock new potential in graph representation learning? Non-Euclidean geometries, particularly Riemannian manifolds such as hyperbolic (negative curvature) and spherical (positive curvature), offer powerful inductive biases for embedding complex graph structures like scale-free, hierarchical, and cyclic patterns. Meanwhile, spectral filtering excels at processing signal variations across graphs, making it effective in homophilic and heterophilic settings. Leveraging both can significantly enhance the learned representations. To this end, we propose \textit{Spectro-Riemannian Graph Neural Networks} (\modelname) -- the \textit{first} graph representation learning paradigm that unifies both \texttt{CU}\text{rvature} (geometric) and \texttt{SP}\text{ectral} insights. \modelname\ is a mixed-curvature spectral GNN that learns spectral filters to optimize node embeddings in \textit{products} of constant-curvature manifolds (hyperbolic, spherical, and Euclidean). Specifically, \modelname\ introduces three novel components: (a) \textit{\modelshort\ Laplacian}, an extension of the traditional graph Laplacian based on Ollivier-Ricci curvature, designed to capture the curvature signals better; (b)  \textit{\modelshort\ Filtering}, which employs multiple Riemannian graph filters to obtain cues from various bands in the eigenspectrum; and (c) \textit{\modelshort\ Pooling}, a hierarchical attention mechanism combined with a curvature-based positional encoding to assess the relative importance of differently \textit{curved} substructures in our graph. Empirical evaluation across eight homophilic and heterophilic datasets demonstrates the superiority of \modelname\ in node classification and link prediction tasks, with a gain of up to $5.3\%$ over state-of-the-art models.

\end{abstract}
\vspace{-2mm}
\section{Introduction}\label{sec:intro}
Graph representation learning has garnered significant research interest in recent years, owing to its fundamental relevance in domains such as natural language processing \citep{mihalcea2011graph}, biology \citep{zhang2021graph}, and social network analysis \citep{grover2022public}. Recent advances have rigorously examined constant \textit{curvature} spaces to learn distortion-free graph representations, as they provide suitable inductive biases for particular structures while avoiding the intrinsic problems of very high dimensionality. For example, hyperbolic space (negative curvature) is optimal for hierarchical tree-like graphs \citep{chami2019hyperbolic}, while spherical geometry (positive curvature) is best suited for cyclic graphs \citep{gu2019learning}.The idea behind all Riemannian GNNs is that optimal embeddings are achieved when the underlying manifold's curvature aligns with the graphs' discrete curvature. To model real-world graphs with complex topologies that cannot be adequately represented within a single constant-curvature manifold, various mixed-curvature GNNs have been proposed, that operate across multiple manifolds \citep{zhu2020graph}. 

Despite their success in managing complex graph topologies, existing mixed-curvature GNNs still have significant limitations (Figure \ref{fig:motivation} (b)). \textbf{(a)} \texttt{L1} (\textit{Low-pass filtering bias}): State-of-the-art mixed-curvature Riemannian GNNs, such as $\kappa$GCN \citep{bachmann2020constant} and $\mathcal{Q}$GCN \citep{xiong2022pseudo}, inherently mimic low-pass spectral filters. These models are adaptations of the Euclidean GCN \citep{kipf2016semi} to Riemannian manifolds, which are known to operate under the homophily assumption (low-pass filters) predominantly. Consequently, their performance degrades on graphs with varying degrees of heterophily. \textbf{(b)} \texttt{L2} (\textit{Task-specific relevance of curvature}): Different datasets and tasks may emphasize more on different curvatures within a graph (Figure \ref{fig:motivation} (a)). For example, the task of community detection focuses on clustered nodes typically associated with a positive curvature \citep{tian2023curvature}. In contrast, fake news detection on social media graphs prioritizes tree-like cascade structures characterized by a negative curvature \citep{grover2022public}. Current models do not adapt curvatures to tasks. \textbf{(c)} \texttt{L3} (\textit{Lack of geometry-equivariance in spectral GNNs}): Spectral GNNs like BernNet \citep{he2021bernnet} and GPRGNN \citep{chien2020adaptive} incorporate flexible graph filters to learn representations based on different parts of the eigenspectrum but assume a flat Euclidean manifold, ignoring the underlying geometry. As the geometry of the graph changes, these filters should adapt to reflect the changing geometric properties, which is currently not the case. 

\begin{figure}[t!]
  \centering
  \includegraphics[width = \textwidth]{imgs/intro.pdf}
  % \vspace{-7mm}
  \caption{\textbf{Motivation behind \modelname}. \textbf{(a)} Diversity of curvatures ($-$ve to $+$ve) and frequencies in homophilic (Cora) and heterophilic (Texas, Chameleon) real-world graphs (Section \ref{sec:experiments}). \textbf{(b)} Spectral GNNs overlook the curvature, whereas Riemannian GNNs are restricted by a cut-off frequency. \modelname\ aims for the best of both -- a rich geometric representation $+$ full access to the spectrum.}
  \vspace{-5mm}
  \label{fig:motivation}
\end{figure}

To bridge these gaps, we introduce \modelname, a mixed-curvature spectral GNN that operates on a product manifold composed of multiple constant-curvature spaces and computes \textit{geometrically} and \textit{spectrally} parameterized graph filters. 
We begin by introducing the \textbf{(a)} \textit{\modelshort\ Laplacian}, a curvature-aware Laplacian operator for graphs, inspired by the equation of heat flow \citep{thanou2017learning} and the discrete Ollivier-Ricci curvature \citep{ollivier2007ricci}. At the core of our approach is \textbf{(b)} \textit{\modelshort\ Filtering}, where we propose a filter bank consisting of multiple mixed-curvature graph filters to ensure that our GNN captures information from multiple bands in the eigenspectrum. Using the \modelshort\ Laplacian, we extend the generalized PageRank (GPR)-based spectral GNN \citep{chien2020adaptive} to mixed-curvature spaces by incorporating operations based on the $\kappa$-stereographic model of Riemannian geometry \citep{bachmann2020constant}. We chose GPRGNN as the spectral backbone of \modelname\ because of its ability to capture node features and topological graph signals simultaneously. Lastly, we introduce the \textbf{(c)} \textit{\modelshort\ Pooling} mechanism, complemented by functional curvature embeddings based on Bochner's Theorem \citep{xu2020inductive}, to weigh differently \textit{curved} substructures, enhancing its ability to model real-world graphs with diverse geometric and spectral properties. We perform an extensive empirical evaluation of \modelname\ for Node Classification (\texttt{NC}) and Link Prediction (\texttt{LP}) tasks, on eight real-world datasets. \modelname\ records state-of-the-art performance, with a gain of up to 5.3\%. We summarize our main contributions as follows:
\begin{itemize}
\vspace{-2mm}
    \item To the best of our knowledge, this is the \textbf{first attempt} towards a graph learning paradigm that seamlessly integrates both \textit{geometry} and \textit{spectral} cues.
    \vspace{-0.5mm} %TODO think of a new bullet header for point 2. For now adding placeholder (MODIFY LATER)
    \item We introduce a curvature-aware \textit{\modelshort\ Laplacian} operator, design a mixed-curvature spectral graph filtering framework, \textit{\modelshort\ Filtering}, and propose a curvature embedding method using classical harmonic analysis and a hierarchical attention mechanism called \textit{\modelshort\ Pooling}.
    \vspace{-0.5mm}
    \item We conduct extensive experimentation on eight real-world benchmarking datasets, featuring homophilic and heterophilic graphs, for node classification and link prediction tasks.
\end{itemize}

\section{Related Works}
\vspace{-2mm}
\textbf{Riemannian Geometry in Graph Neural Networks}. Graph Neural Networks (GNNs) have set new benchmarks for tasks like node classification and link prediction. Recently, non-Euclidean (Riemannian) spaces -- particularly hyperbolic \citep{sala2018representation} and spherical \citep{liu2017sphereface, wilson2014spherical} geometries -- have garnered attention for their ability to produce less distorted representations, aligning well with hierarchical and cyclic data structures, respectively. Several approaches have emerged in this context. (a) \textit{Single Manifold GNNs}: GNNs such as HGAT \citep{zhang2021hyperbolic}, HGCN \citep{chami2019hyperbolic}, and HVAE \citep{sun2021hyperbolic} have demonstrated state-of-the-art performance on tree-like or hierarchical graphs by learning representations in hyperbolic space. (b) \textit{Mixed-Curvature GNNs}: To model more complex topologies (for example, a tree branching from a cyclic graph), mixed-curvature GNNs have been proposed. \cite{gu2019learning} pioneered this direction by embedding graphs in a product manifold combining spherical, hyperbolic, and Euclidean spaces. Building on this, models like $\kappa$-GCN \citep{bachmann2020constant} and Q-GCN \citep{xiong2022pseudo} extended the GCN architecture \citep{kipf2016semi} to constant-curvature spaces using the $\kappa$-stereographic model and pseudo-Riemannian manifolds, respectively. More recently, \cite{sun2022self} proposed a mixed-curvature GNN for self-supervised learning, while FPS-T \citep{cho2023curve} generalized the Graph Transformer \citep{min2022transformer} to operate across multiple manifolds.

\textbf{Spectral Graph Neural Networks}. Spectral GNNs employ spectral graph filters \citep{liao2024benchmarking} to process graph data. These models either use fixed filters, as seen in APPNP \citep{gasteiger2018predict} and GNN-LF/HF \citep{zhu2021interpreting}, or learnable filters, as demonstrated by ChebyNet \citep{defferrard2016convolutional} and GPRGNN \citep{chien2020adaptive}, which approximate polynomial filters using Chebyshev polynomials and generalized PageRank, respectively. BernNet \citep{he2021bernnet} expresses filtering operations through Bernstein polynomials. However, many of these methods focus primarily on low-frequency components of the eigenspectrum, potentially overlooking important information from other frequency bands -- particularly in heterophilic graphs. Models like GPRGNN and BernNet address this by exploring the entire spectrum, performing well across both homophilic and heterophilic graphs. GPRGNN stands out among them because it can express several polynomial filters and incorporate node features and topological information. Despite these advances, current mixed-curvature and spectral GNNs face significant limitations (\texttt{L1}, \texttt{L2}, \texttt{L3}) that constrain their performance. To the best of our knowledge, this work is the first to unify \textit{geometric} and \textit{spectral} information within a single model. Before presenting the architecture of \modelname, we introduce some key preliminary concepts in the following section.
\section{Preliminaries}\label{sec:background}
\vspace{-2mm}
We study graphs $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathbf{A})$, where $\mathcal{V}$ is a finite set of $|\mathcal{V}| = n$ vertices, $\mathcal{E}$ is a set of edges and $\mathbf{A} \in \mathbb{R}^{n \times n}$ is a weighted graph adjacency matrix. The nodes are associated with the node feature matrix $\mathbf{F} \in \mathbb{R}^{n \times d_f}$ ($d_f$ is the feature node dimension). A graph signal $\mathbf{f} : \mathcal{V} \rightarrow \mathbb{R}$ on the nodes of the graph may be regarded as a vector $\mathbf{f} \in \mathbb{R}^n$ where $f_i$ is the value of $\mathbf{f}$ at the $i^{th}$ node. An essential operator in spectral graph analysis is the graph Laplacian $\mathbf{L} = \mathbf{D}- \mathbf{A} \in \mathbb{R}^{n\times n}$ where $\mathbf{D} \in \mathbb{R}^{n \times n}$ is the diagonal degree matrix with $\mathbf{D}_{ii} = \sum_j\mathbf{A}_{ij}$ \citep{kipf2016semi}. The normalized Laplacian is defined as $\mathbf{L}_n = \mathbf{I} - \mathbf{A}_n$ where $\mathbf{A}_n = \mathbf{D}^{-1/2} \mathbf{A}  \mathbf{D}^{-1/2}$ is the normalized adjacency matrix, and $\mathbf{I} \in \mathbb{R}^{n\times n}$ is the identity matrix. As $\mathbf{L}$ is a real symmetric positive semidefinite matrix, it has a complete set of orthonormal eigenvectors $\mathbf{U} = \left[\{\mathbf{u}_l\}^{n - 1}_{l=0}\right] \in \mathbb{R}^{n \times n}$, and their associated ordered real nonnegative eigenvalues $\left[\{\lambda_l\}^{n - 1}_{l=0}\right] \in \mathbb{R}^{n}$, identified as the \textit{frequencies} of the graph. The Laplacian can be diagonalized as $\mathbf{L} = \mathbf{U}\mathbf{\Lambda} \mathbf{U}^{\top}$ where $\mathbf{\Lambda} = \mathbf{diag}(\left[\{\lambda_l\}^{n - 1}_{l=0}\right]) \in \mathbb{R}^{n \times n}$. 


\textbf{Riemannian Geometry} \citep{do1992riemannian}. A smooth \emph{manifold} $\mathcal M$ generalizes the notion of \textit{surface} to higher dimensions. Each point $\mathbf x \in \mathcal M$ is associated with a \emph{tangent space} $\mathcal T_\mathbf x\mathcal M$, which is locally Euclidean.
On tangent space $\mathcal T_\mathbf x\mathcal M$, the \emph{Riemannian metric}, $g_\mathbf x (\cdot, \cdot) : \mathcal T_\mathbf x\mathcal M  \times \mathcal T_\mathbf x\mathcal M \to \mathbb R$, defines an inner product so that geometric notions (like distance, angle, etc.) can be induced.
The pair $(\mathcal M, g)$ is called a \emph{Riemannian manifold}. For $\mathbf x \in \mathcal M$, 
the  \emph{exponential map} at $\mathbf x$, 
$\mathbf{exp}_\mathbf x(\mathbf v): \mathcal T_\mathbf x\mathcal M \to \mathcal M$, 
projects the vector $\mathbf v \in \mathcal T_\mathbf x\mathcal M$ onto the manifold $\mathcal M$, and the \emph{logarithmic map}, 
$\mathbf{log}_\mathbf x(\mathbf y): \mathcal M \to \mathcal T_\mathbf x\mathcal M$, 
projects the vector $\mathbf y \in \mathcal M$ back to the tangent space $\mathcal T_\mathbf x\mathcal M$. The Riemannian metric defines a \textit{curvature} (\(\kappa\)) at each point on the manifold, indicating how the space is curved. There are three canonical types: positively curved \textit{Spherical} ($\mathbb{S}$) space $(\kappa > 0)$, negatively curved \textit{Hyperbolic} ($\mathbb{H}$) space (\(\kappa < 0\)), and flat \textit{Euclidean} ($\mathbb{E}$) space (\(\kappa = 0\)).


\textbf{Product Manifolds} \citep{gu2019learning}. Consider $q$ constant-curvature manifolds $\{\mathcal{M}_{i}^{\kappa_i, d_i}\}_{i=1}^{q}$ with dimension $d_i$ and curvature $\kappa_i$. Then, the product manifold is defined as the Cartesian product $\mathbb{P} =\mathcal{M}_{1}^{\kappa_1, d_1} \times \mathcal{M}_{2}^{\kappa_2, d_2} \dots \times \mathcal{M}_{q}^{\kappa_q, d_q}$, with total dimension $\sum_{i=1}^{q} d_i$. Each $\mathcal{M}_i \in \{\mathbb{H}, \mathbb{S}, \mathbb{E}\}$ is known as a \textit{component} space, and the decomposition $\mathbb{P} = \times_{i=1}^{q} \mathcal{M}_{i}^{\kappa_i, d_i} $ is called the \textit{signature} of $\mathbb{P}$. 

$\kappa-$\textbf{Stereographic Model} \citep{bachmann2020constant}. In this work, we adopt the $\kappa$-\textbf{Stereographic Model} to define Riemannian algebraic operations across both positively and negatively curved spaces within a unified framework. This model eliminates the need for separate mathematical formulations for different geometries.
% Specifically,  
In particular,  $\mathcal M^d_\kappa$ is the stereographic sphere model for spherical manifold ($\kappa > 0$),
while it is the Poincar\'e ball model \citep{ungar2001hyperbolic} for hyperbolic manifold ($\kappa < 0$). More mathematical details and intuitions have been discussed in Appendix \ref{app:kappa_sterographic}.
% \vspace{-3mm}
% TODO: Add table to appendix


\textbf{Ollivier-Ricci Curvature} (\texttt{ORC}). Discrete data like graphs lack manifold structure (hence, no \textit{curvature}). Several discrete analogs of manifold curvature have been defined, which satisfy properties similar to \textit{curvature}. \texttt{ORC} \citep{ollivier2007ricci} is a graph discrete analog of \textit{Ricci curvature} \citep{tanno1988ricci} and is defined by transport along an edge of the network, between neighborhoods of the vertex. In an unweighted graph, for a hyperparameter $\delta \in [0, 1]$, we endow each node's ($x$) neighbourhood ($\mathcal{N}(x)$) with a probability measure, $
		m_{x}^{\delta}(z) := \frac{1-\delta}{|\mathcal{N}(x)|}\; \forall z \in \mathcal{N}(x),$ and $m_{x}^{\delta}(z) = \delta$ when $z = x$, and analogously for $m_y^{\delta}(z)$. \texttt{ORC} for an edge $(x, y)$ is then defined w.r.t. the Wasserstein-1 distance, $\mathbf{W}_1$ \citep{piccoli2016properties}, between these measures, i.e., $
		\widetilde{\kappa} (x, y) := 1 - \frac{\mathbf{W}_1 (m^{\delta}_{x}, m^{\delta}_{y})}{d_\mathcal{G}(x,y)}$. ${d_\mathcal{G}(x,y)}$ is the shortest graph distance between nodes $x$ and $y$. The Ollivier-Ricci curvature $\widetilde{\kappa}(x)$ for a node $x$ is defined as the average curvature of its adjacent edges i.e. $\widetilde{\kappa}(x) = \frac{1}{|\mathcal{N}(x)|}\sum_{z\in \mathcal{N}(x)} \widetilde{\kappa}(x, z)$.

\textbf{Generalized PageRanks} (\texttt{GPR}). Generalized PageRank methods originated in the context of unsupervised graph clustering, where they demonstrated notable improvements over the classical Personalized PageRank~\citep{kloumann2017block,li2019optimizing}. The core idea behind GPRs is as follows: starting with a seed node $s \in \mathcal{V}$ (vertex set) within a graph cluster, an initial feature vector $\mathbf{H}^{(0)} \in \mathbb{R}^{n \times 1}$ is set, where $\mathbf{H}^{(0)}_v = \delta_{vs}$ (i.e., 1 for the seed node and 0 for all others). The \texttt{GPR} score is then defined as $\sum_{k=0}^{\infty} \gamma_k \widetilde{\mathbf{A}}_{\text{n}}^k \mathbf{H}^{(0)} = \sum_{k=0}^{\infty} \gamma_k \mathbf{H}^{(k)}$, where $\gamma_k \in \mathbb{R}$ are the \texttt{GPR} weights that control the importance of higher-order neighbors. This iterative process propagates the feature information throughout the graph. Clustering is performed by locally thresholding the \texttt{GPR} scores.
Refer to the Appendix for more details on \texttt{ORC} (\ref{app:orc}), product manifolds (\ref{app:product_manifolds}), and \texttt{GPR} (\ref{app:GPRGNN}).

\vspace{-3mm}
\section{Proposed Method: \modelname}
\vspace{-2mm}
In this section, we present a comprehensive overview of the architecture of \modelname, as shown in Figure \ref{fig:riccilap}. We start by introducing and deriving the \textit{\modelshort\ Laplacian} (Section \ref{sec:ricci_lap}). Building on this, we propose \textit{\modelshort\ Filtering}, the core component of our approach, which is a $\texttt{GPR}$-based, mixed-curvature spectral graph filtering network (Section \ref{sec:pagerank}). Next, we introduce a curvature embedding technique grounded in classical harmonic analysis (Section \ref{sec:curv_emb}), which acts as the positional encoding in the hierarchical attention-based \textit{\modelshort\ Pooling} mechanism (Section \ref{sec:ricci_pool}). Throughout this paper, we use $\kappa$ to denote the continuous manifold curvature and $\widetilde{\kappa}$ for the Ollivier-Ricci curvature.


% \vspace{-3mm}
\subsection{\modelshort\ Laplacian}\label{sec:ricci_lap}
\vspace{-1mm}
\vspace{-1mm}
\begin{wrapfigure}{r}{0.6\textwidth}
\vspace{-10mm}
    \includegraphics[width=0.6\textwidth]{imgs/heat_flow.pdf}
    \caption{Consider heat diffusion from $y \rightarrow x$. If $\widetilde{\kappa}(x, y)<0$, there is a single path for the heat to diffuse from $y \rightarrow x$. When $\widetilde{\kappa}(x, y) \geq 0$, heat can effectively diffuse through other paths from $y \rightarrow x$ (\color{blue}\textit{dotted}).}
    \vspace{-4mm}
    \label{fig:riccilap}
\end{wrapfigure}
To effectively incorporate geometric insights into spectral graph learning, we introduce the \textit{\modelshort\ Laplacian}, a curvature-aware Laplacian operator. We begin by examining the concept of heat flow on a graph \citep{weber2008analysis}. Suppose $\psi$ describes a temperature distribution across a graph, where $\psi(x)$ is the temperature at vertex $x$. According to Newton's law of cooling \citep{he2024constrained}, the heat transferred from node $x$ to node $y$ is proportional to $\psi(x) - \psi(y)$ if nodes $x$ and $y$ are connected (if they are not connected, no heat is transferred). Consequently, the heat diffusion equation on the graph can be expressed as $\frac{d \psi}{dt} = -\beta \sum_y \mathbf{A}_{xy}(\psi(x) - \psi(y))$, where $\beta$ is a constant of proportionality and $\mathbf{A}$ denotes the adjacency matrix of the graph. Further insight can be gained by considering Fourier’s law of thermal conductance \citep{liu1990fourier}, which states that heat flow is inversely proportional to the resistance to heat transfer. In this context, we leverage the Ollivier-Ricci curvature (\texttt{ORC}) to define the notion of resistance between nodes. Implicitly, \texttt{ORC} measures the transportation cost ($\mathbf{W}_1(:,:)$) between the neighborhoods of two nodes, reflecting the effort required to transport mass between these neighborhoods \citep{bauer2011ollivier}. We interpret this transportation cost as the \textit{resistance} between nodes. 

The vital takeaway here is that $-$ \textit{Heat flow between two nodes in a graph is influenced by the underlying Ollivier-Ricci curvature (\texttt{ORC}) distribution}. The diffusion rate is faster on an edge with positive  curvature (low resistance), and slower on an edge with negative curvature (high resistance). Intuitively, if the neighborhoods of two nodes overlap significantly (Figure \ref{fig:riccilap}(c)), the transportation cost between them is low ($\mathcal{R}^{res}_{xy} = \frac{\mathbf{W}_1(m^{\delta}_x, m^{\delta}_y)}{d_{\mathcal{G}}(x, y)} < 1$), resulting in a positive curvature value for the edge connecting these nodes. In this scenario, messages (analogously, \textit{heat}) can be transmitted efficiently between the neighborhoods. Conversely, if the neighborhoods have little overlap (Figure \ref{fig:riccilap}(a)), the transportation cost is high, leading to a negative curvature value ($\mathcal{R}^{res}_{xy} > 1$), and the edge acts as a bottleneck, impeding effective message-passing. As a result, the heat diffusion process is influenced by the underlying curvature (resistance).


\begin{definition}\label{thm:Laplacian}
The \textit{\modelshort\ Laplacian} operator takes the form $-$
\begin{equation}
\widetilde{\mathbf{L}}\psi(x)= \sum_{y \sim x} \bar{w}_{xy} \left( \psi(x) - \psi(y) \right) = \sum_{y \sim x} e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}} \left( \psi(x) - \psi(y) \right),
\end{equation}
where $x \sim y$ denotes adjacency between nodes x and y, $\bar{w}_{xy} = e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}}$ represents the curvature-based weight between nodes x and y, and $\widetilde{\kappa}(x, y)$ is the discrete Ollivier-Ricci curvature between x and y. The function $\psi : V \to \mathbb{R}$ is defined on the vertex set V of the graph. In the matrix form, we can write, $\widetilde{\mathbf{L}} = \widetilde{\mathbf{D}} - \widetilde{\mathbf{A}}$, where $\widetilde{\mathbf{D}}$ and $\widetilde{\mathbf{A}}$ are the degree and adjacency matrices based on $\bar{w}_{xy}$.
\end{definition}

Refer to Appendix \ref{app:Laplacian} for the derivation of Definition \ref{thm:Laplacian}. 
% The \modelshort\ Laplacian can be seen as a generalization of the Laplace-Beltrami operator \citep{burago2015graph} from the Riemannian geometry, adapted to the discrete setting of the graphs. 
The curvature-based Laplacian operator allow us to incorporate geometric cues into the spectral perspective, which are otherwise not captured by the traditional graph Laplacian.



\subsection{\modelshort\ Filtering}\label{sec:pagerank}
Now, we will talk about how we create a filter bank (i.e. multiple graph filters) to fuse information from different parts of the eigenspectrum and learn node representations on a product manifold $\mathbb{P}$.

\textbf{Product manifold construction}. $\mathbb{P}$ can have multiple hyperbolic or spherical components with distinct \textit{learnable} curvatures (parameters). This enables us to be representative of a wider range of curvatures. However, we only need one Euclidean space, since the Cartesian product of Euclidean space is $\mathbb E^{d_{(e)}}=\times_{i=1}^j \mathbb E^{d_{(i)}}_i$ such that $\sum\nolimits_{i=1}^j d_{(i)}=d_{(e)}$. This is not the case with $\mathbb{H}$ or $\mathbb{S}$ (Eg. \textit{Torus} i.e. $\mathbb{S}^1 \times \mathbb{S}^1 $ is topologically distinct from \textit{Sphere} i.e. $\mathbb{S}^2$). Thus, we can represent the product manifold \textit{signature}, $ \mathbb{P}^{d_{\mathcal{M}}} = \times_{q=1}^{\mathcal{Q}} \mathcal{M}_{q}^{\kappa_{(q)}, d_{(q)}}  = (\times_{h=1}^{\mathcal{H}} \mathbb{H}_{h}^{\kappa_{(h)}, d_{(h)}}) \times (\times_{s=1}^{\mathcal{S}} \mathbb{S}_{s}^{\kappa_{(s)}, d_{(s)}}) \times \mathbb{E}^{d_{(e)}}$ with total dimension $d_{\mathcal{M}} = \sum_{h=1}^{\mathcal{H}} d_{(h)}+ \sum_{s=1}^{\mathcal{S}} d_{(s)} + d_{(e)}$. We use a simple combinatorial construction of the mixed-curvature space, induced by the cartesian product \citep{sun2022self}. 
% For $\mathbf b=||_{i=1}^K \mathbf b_{\mathcal M_i}$ in product space $\mathcal P$, $\mathbf b_{\mathcal M_i}$ denotes the component embedding in  $\mathcal M_i$ and $||$ is the concatenation operator. This means we can first learn representations in each component space and then fuse them. 
In Section \ref{sec:experiments} and Appendix \ref{app:product_manifolds} we describe how we heuristically identify the signature of $\mathbb{P}^{d_{\mathcal{M}}}$.



\begin{figure}[t!]
  \centering
      \includegraphics[width=0.98\textwidth]{imgs/main_cusp.pdf}
      \vspace{-2mm}
  \caption{\textbf{Architecture of \modelname}.  The input graph $\textbf{(a)}$ is used to construct the \textit{\modelshort\ Laplacian} $(\widetilde{\mathbf{L}})$, based on the Ollivier-Ricci curvature \textbf{(b)}, as illustrated in \textbf{(c)}. The computed edge \texttt{ORC} ({\color{red}red}), node \texttt{ORC} ({\color{Green}Green}), and curvature-based weights ({\color{blue}blue}) have been highlighted in the input graph. With $\widetilde{\mathbf{L}}$, \textbf{(d)} \textit{\modelshort\ Filtering} introduces multiple graph filters to capture different parts of the eigenspectrum. Each node receives a curvature positional encoding $\Phi$ in \textbf{(e)} as part of the \textbf{(f)} \textit{\modelshort\ Pooling} mechanism, which computes the relative importance of different filters and manifold components.}
  \label{fig:arch}
  \vspace{-3mm}
\end{figure}


\textbf{Extending PageRank GNN to $\mathbb{P}$}: We choose GPRGNN \citep{chien2020adaptive} as the spectral backbone of \modelname\ (See Appendix \ref{app:GPRGNN} for more background on GPRGNN), becasue it jointly optimizes node feature and topological information extraction, which is different from other spectral GNNs like ChebyNet \citep{defferrard2016convolutional}. The \texttt{GPR} weights automatically adjust to the node label pattern (homophilic or heterophilic). Given the node feature matrix $\mathbf{F} \in \mathbb{R}^{n \times d_f}$ and normalized symmetric \textit{\modelshort\ adjacency} matrix $\widetilde{\mathbf{A}}_n = \widetilde{\mathbf{D}}^{\frac{-1}{2}}\; \widetilde{\mathbf{A}}\;\widetilde{\mathbf{D}}^{\frac{-1}{2}}$, we first extract hidden state features for each node and then use \texttt{GPR} to propagate them. The process can be mathematically described as:
\begin{align}\label{model:eq1}
    & \mathbf{H}^{(0)}_{\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}} = \mathbf{exp}^{\kappa_{(q)}}_{\mathbf{0}}(f_{\theta}(\mathbf{F})) \quad; \mathbf{H}^{(l)}_{\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}} = \widetilde{\mathbf{A}}_{n} \boxtimes_{\kappa_{(q)}} \mathbf{H}^{(l-1)}_{\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}}\\
    & \mathbf{Z}^{(L)}_{\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}} = \bigoplus\limits_{l \in \{0, L\}}^{\kappa_{(q)}} \gamma_l \otimes_{\kappa_{(q)}} \mathbf{H}^{(l)}_{\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}} \quad; \mathbf{Z}^{(L)}_{\mathbb{P}^{d_{\mathcal{M}}}} = ||_{q = 1}^{\mathcal{Q}} \mathbf{Z}^{(L)}_{\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}}
\end{align}
where $f_{\theta}(.): \mathbb{R}^{d_{f}} \rightarrow \mathbb{R}^{d_{\mathcal{M}}}$ represents a neural network with parameter set $\{{\theta\}}$ that generates the hidden state features of dimension $d^{\mathcal{M}}$. Here, $\mathbf{exp}^{\kappa}_{\mathbf{0}} : \mathbb{R}^{d_{\mathcal{M}}} \rightarrow \mathcal{M}^{\kappa, d_{\mathcal{M}}}$ is the exponential map (Section \ref{sec:background}) to transform the euclidean node features $\mathbf{F}$ to the component manifold  $\mathcal{M}$, and $||$ is the attentional concatenation operator (discussed in Section \ref{sec:ricci_pool}). $\oplus_{\kappa}$, $\otimes_{\kappa}$ and $\boxtimes_{\kappa}$ denote \textit{mobius} addition, $\kappa$-right-matrix-multiplication and $\kappa$-left-matrix-multiplication respectively (Appendix \ref{app:kappa_sterographic}). These operations generalize vector addition and multiplication on $\kappa$-stereographic model. The GPR weights $\gamma_l$ are trained together with $\{{\theta\}}$ in an end-to-end fashion. The final mixed-curvature node embeddings after attention can be represented as $\mathbf{Z}^{(L)}_{\mathbb{P}^{d_{\mathcal{M}}}} \in \mathbb{P}^{n \times d_{\mathcal{M}}}$, such that $\sum_{q=1}^{\mathcal{Q}} d_{(q)} = d_{\mathcal{M}}$. 

\textbf{Filter Bank}. The \texttt{GPR} component of the network may be viewed as a polynomial graph filter (See Appendix \ref{app:GPRGNN}). Let $\widetilde{\mathbf{A}}_n = \mathbf{U}\mathbf{\Lambda}\mathbf{U}^T$ be the eigenvalue decomposition of $\widetilde{\mathbf{A}}_n$. Then, the polynomial graph filter equals $\sum_{l=0}^{L}\gamma_l \widetilde{\mathbf{A}}_{n}^l = \mathbf{U}g_{\gamma,L}(\mathbf{\Lambda})\mathbf{U}^T$, where $g_{\gamma,L}(\mathbf{\Lambda})$ is applied element-wise and $g_{\gamma,L}(\lambda) = \sum_{l=0}^L\gamma_l\lambda^l$. If one allows $\gamma_l$ to be negative and learnt adaptively, the graph filter will pass relevant high frequencies. Consequently, \modelname\ performs exceptionally well on heterophilic graphs.

\begin{theorem} [Informal \citep{chien2020adaptive}]\label{thm:LPF} If $\gamma_l\geq 0\;\forall l\in \{0,1,...,L\}$, $\sum_{l=0}^{L}\gamma_l=1$ and $\exists l'>0$ such that $\gamma_{l'} > 0$, then $g_{\gamma,L}(\cdot)$ is a low-pass graph filter. Also, if $\gamma_l = (-\alpha)^l,\alpha\in(0,1)$ and $L$ is large enough, then $g_{\gamma,L}(\cdot)$ is a high-pass graph filter.
    % \pan{You want to use this result to argue against APPNP and SGC. Then, we do not to use a theorem to describe. This result is also too simple to be list as a theorem. }
\end{theorem}
 We encourage the reader to refer to \cite{chien2020adaptive} for a detailed discussion on how different initializations of the \texttt{GPR} weights, as mentioned in the theorem above, assist in designing low-pass and high-pass filters. We present a proof of Theorem \ref{thm:LPF} in Appendix \ref{app:GPRGNN}. Motivated by the limitation $\texttt{L1}$, instead of a single filter, we construct a filter bank to focus on different parts of the eigenspectrum (to assist in both heterophilic and homophilic graphs). In an attempt to be representative of the higher order filters, the proposed filterbank is: $\Omega_{\mathbb{P}^{d_{\mathcal{M}}}} = \big[\mathbf{Z}^{\mathbf{I}}_{\mathbb{P}^{d_{\mathcal{M}}}}, \mathbf{Z}^{(1)}_{\mathbb{P}^{d_{\mathcal{M}}}}, \mathbf{Z}^{(2)}_{\mathbb{P}^{d_{\mathcal{M}}}}, \dots, \mathbf{Z}^{(L)}_{\mathbb{P}^{d_{\mathcal{M}}}}\big]$. Here, $\mathbf{Z}^{\mathbf{I}}_{\mathbb{P}^{d_{\mathcal{M}}}}$ is the unfiltered case, where we pass the identity matrix $\mathbf{I}$ instead of $\widetilde{\mathbf{A}}_n$ for \texttt{GPR} propagation.


\subsection{Functional Curvature Encoding} \label{sec:curv_emb}

Recall our motivation that \modelname\ must be able to pay more attention to differently curved substructures in our model, depending on different tasks and datasets, while learning the final node representations. Specifically, our goal is to obtain a continuous functional mapping $\Phi: \mathbb{K}_{\mathbb{P}} \to \mathbb{P}^{d_{\mathcal{C}}}$ from curvature domain to the $d_{\mathcal{C}}$-dimensional product space to serve as positional encoding in our attention mechanism (Section \ref{sec:ricci_pool}). We approximate this in two steps, by considering the Ollivier-Ricci (\texttt{ORC}) discretization of curvature on our graph and using the Bochner's theorem \citep{moeller2016continuous} from classical harmonic analysis. First, we construct a translation-invariant Euclidean curvature encoding map, and then map it to the product manifold. Without loss of generality, we assume that the curvature domain can be represented by the interval: $\mathbb{K} = [-1, 1]$, where $-1$ to $1$ is the \textit{typical} range\footnote{\texttt{ORC} can theoretically vary from $-\infty$ to 1, but in practice, its values usually lie within $[-1, 1]$ for the majority of graphs. For extreme scenarios, such as very sparse or highly clustered graphs, values may fall outside this range. In such instances, \texttt{ORC} is normalized to $[-1, 1]$.} of \texttt{ORC} in the observed data. Formally, we define the \textbf{Curvature Kernel} $\mathcal{K}_{\mathbb{R}}: \mathbb{K} \times \mathbb{K} \to \mathbb{R}$ with $\mathcal{K}_{\mathbb{R}}(\widetilde{\kappa}_a,\widetilde{\kappa}_b):= \big\langle \Phi_{\mathbb{R}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_a), \Phi_{\mathbb{R}^{d_{\mathcal{C}}}} (\widetilde{\kappa}_b) \big\rangle$ and $\mathcal{K}_{\mathbb{R}}(\widetilde{\kappa}_a,\widetilde{\kappa}_b) = \Psi_{\mathbb{R}}(\widetilde{\kappa}_a-\widetilde{\kappa}_b)$, $\forall \widetilde{\kappa}_a, \widetilde{\kappa}_b \in \mathbb{K}$ for some $\Psi_{\mathbb{R}}: [-2, 2] \to \mathbb{R}$. The intuition behind choosing such a kernel for curvature values, lies in the fact that when comparing $\widetilde{\kappa}_a$ and $\widetilde{\kappa}_b$ we are only concerned with the relative difference between the two values. According to the Bochner's theroem \citep{moeller2016continuous}, a continuous, translation-invariant kernel $\mathcal{K}(\mathbf{x},\mathbf{y})=\Psi(\mathbf{x}-\mathbf{y})$ on $\mathbb{R}^d$ is positive definite if $\Psi$ is the Fourier transform of a non-negative probability measure on $\mathbb{R}$. Our kernel is translation-invariant, since $\mathcal{K}_{\mathbb{R}}(\widetilde{\kappa}_a + \widetilde{c}, \widetilde{\kappa}_b + \widetilde{c}) = \Psi_{\mathbb{R}}((\widetilde{\kappa}_a + \widetilde{c}) - (\widetilde{\kappa}_b + \widetilde{c})) = \mathcal{K}_{\mathbb{R}}(\widetilde{\kappa}_a, \widetilde{\kappa}_b)$ for any constant $\widetilde{c}$. The following theorems defines the Euclidean encoding and it's corresponding mapping to the product space.

\begin{definition} \label{thm:curve_1}
Let $\mathbb{R}^{d_{\mathcal{C}}}$ be the ambient Euclidean space for a node’s curvature encoding. The functional curvature encoding $\Phi_{\mathbb{R}^{d_{\mathcal{C}}}}: \mathbb{K} \to \mathbb{R}^{d_{\mathcal{C}}}$ for curvature $\widetilde{\kappa}(x) \in \mathbb{K}$ of node $x$, is defined as:
\begin{equation}
\Phi_{\mathbb{R}^{d_{\mathcal{C}}}}(\widetilde{\kappa}(x)) = \sqrt{\frac{1}{d_{\mathcal{C}}}} \Big[\cos(\omega_1  \widetilde{\kappa}(x)), \sin(\omega_1 \widetilde{\kappa}(x)), \dots, \cos(\omega_{d_{\mathcal{C}}} \widetilde{\kappa}(x)), \sin(\omega_{d_{\mathcal{C}}}  \widetilde{\kappa}(x)) \Big],
\end{equation}
where $\omega_1,\ldots,\omega_{d_{\mathcal{C}}} \stackrel{\text{i.i.d}}{\sim} p(\omega)$ are sampled from a distribution $p(\omega)$. The corresponding mapping to the product manifold $\mathbb{P}^{d_{\mathcal{C}}}$, is defined as:
\begin{equation}
\Phi_{\mathbb{P}^{d_{\mathcal{C}}}}(\widetilde{\kappa}(x)) = g_{\theta}\Big(\|_{q=1}^{\mathcal{Q}}\mathbf{exp}^{\kappa_{(q)}}_{\mathbf{0}}(\Phi_{\mathbb{R}^{d_{\mathcal{C}}}}(\widetilde{\kappa}(x)))\Big) = g_{\theta}\Big(\|_{q=1}^{\mathcal{Q}} \Phi_{\mathcal{M}_{q}^{\kappa_{(q)}, d_{(q)}}}(\widetilde{\kappa}(x))\Big),
\end{equation}
where $\mathbf{exp}^{\kappa_{(q)}}_{\mathbf{0}}:\mathbb{R}^{d_{\mathcal{C}}}\rightarrow\mathcal{M}_{q}^{\kappa_{(q)}, d_{(q)}}$ denotes the exponential map on the $q^{th}$ component manifold with curvature $\kappa_{(q)}$, $||$ is the concatenation operator and $g_{\theta}: \mathbb{P}^{d_{\mathcal{M}}}\rightarrow \mathbb{P}^{d_{\mathcal{C}}}$ is a Riemannian projector. 
\end{definition}

It is easy to show that $\big\langle \Phi_{d_{\mathcal{C}}}(\widetilde{\kappa}_a), \Phi_{d_{\mathcal{C}}}(\widetilde{\kappa}_b) \big\rangle \approx \mathcal{K} (\widetilde{\kappa}_a,\widetilde{\kappa}_b)$. The unknown distribution $p(\omega)$ is estimated using the inverse cumulative distribution function
(CDF) transformation as in \cite{xu2020inductive}. Next, we prove the translation invariance of the curvature kernel in the product manifold setting.

\begin{theorem}\label{thm:curve_2}
The mixed-curvature kernel $\mathcal{K}_{\mathbb{P}}(\widetilde{\kappa}_a,\widetilde{\kappa}_b):= \big\langle \Phi_{\mathbb{P}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_a), \Phi_{\mathbb{P}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_b) \big\rangle$ is translation invariant, i.e. $\mathcal{K}_{\mathbb{P}}(\widetilde{\kappa}_a,\widetilde{\kappa}_b) = \Psi_{\mathbb{P}}(\widetilde{\kappa}_a-\widetilde{\kappa}_b)$. 
\end{theorem}

Please refer to Appendix \ref{app:functional} for the detailed proofs (derivations) of Definition \ref{thm:curve_1} and Theorem \ref{thm:curve_2}. 
% Furthermore, we establish the stochastic convergence of the kernel approximation \( \big\langle \Phi_{\mathbb{P}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_a), \Phi_{\mathbb{P}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_b) \big\rangle \) to the underlying kernel \( \mathcal{K}_{\mathbb{P}}(\widetilde{\kappa}_a, \widetilde{\kappa}_b) \), and show that only a reasonable number of samples are required for accurate estimation. 
We employ \( \Phi_{\mathbb{P}^{d_{\mathcal{C}}}}(\widetilde{\kappa}(x)) \) as the \texttt{ORC}-based positional encoding for each node $x$ in the final pooling layer, enabling it to incorporate local curvature information and compute the task-specific relevance of substructures with varying curvatures within the graph.



\subsection{\modelshort\ Pooling}\label{sec:ricci_pool}
\vspace{-2mm}
 The importance of constant-curvature component spaces depends on the downstream task. To this end, we propose a hierarchical attention mechanism called \textit{\modelshort\ Pooling}. We perform attentional concatenation to fuse constant-curvature representations across component spaces so as to learn mixed-curvature representations in the product space, weighing them appropriately. Specifically, we lift component encodings to the common tangent space, where we can perform the usual euclidean operations, and compute their centorid by the mean pooling. Then, we model the importance of a component by the position of the embedding relative to the centorid,  parameterized by  $\boldsymbol \theta$.
\begin{align}
\boldsymbol \mu^{(L)}&= \frac{1}{\mathcal{Q}}\sum_{q = 1}^{\mathcal{Q}} \mathbf{log}_{\mathbf 0}^{\kappa_{(q)}} \big(\mathbf W_{q} \otimes_{\kappa_{(q)}} \mathbf{Z}^{(L)}_{\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}}\big) \quad (\text{Centroid using linear transformation }\mathbf{W}_q)\\
\tau_{q}&=\sigma\big({\boldsymbol \theta}^{\top}\big(\mathbf{log}_{\mathbf 0}^{\kappa_{(q)}} \big(\mathbf W_{q} \otimes_{\kappa_{(q)}} \mathbf{Z}^{(L)}_{\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}}\big) - \boldsymbol \mu^{(L)} \big)\big) \quad (\text{Relative importance of }\mathcal{M}_q)
\end{align}
Finally, with the learnable attentional weights $\beta_q = e^{\tau_q}/\sum_{q=0}^{Q} (e^{\tau_q})$, we perform attentional concatenation. Further, as motivated earlier, for every node $x$, we fuse the curvature embedding as positional encoding:\vspace{-2mm}
\begin{align}
\mathbf{Z}^{(L)(x)}_{\mathbb{P}^{d_{\mathcal{M}}}} = \Big\|_{q = 1}^{\mathcal{Q}}
    \left(\beta_q \otimes_{\kappa_{(q)}} \mathbf{Z}^{(L)(x)}_{\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}}\right) \quad;
    \boldsymbol{\zeta}^{(L)}_{(x)} = \mathbf{Z}^{(L)(x)}_{\mathbb{P}^{d_{\mathcal{M}}}}\Big\| \Phi_{\mathbb{P}^{d_{\mathcal{M}}}}(\widetilde{\kappa}(x))
\end{align}
We weigh different filters in our filter bank to yield the final node representation $\boldsymbol{\zeta}_{(x)}$ for $x$ as $\boldsymbol{\zeta}_{(x)} = \sum_{l=1}^{L} \epsilon_l \boldsymbol{\zeta}_{(x)}^{(l)}$. Here, $\boldsymbol{\zeta} \in \mathbb{P}^{n \times (d_{\mathcal{M}}+d_{\mathcal{C}})}$ contains the final node embeddings, and $\epsilon_l$ is a learnable parameter to weigh the importance of different filters in our bank. These node embeddings are then used for the final downstream tasks of node classification and link prediction. In the following section, we lay out the empirical results to validate the efficacy of \modelname.

\section{Experimentation}\label{sec:experiments}
\vspace{-2.5mm}
\textbf{Datasets}. We evaluate \modelname\ on the Node Classification (\texttt{NC}) and Link Prediction (\texttt{LP}) tasks using eight benchmark datasets. These include \textbf{(a) Homophilic} datasets such as \textit{(i) Citation networks} -- Cora, Citeseer and PubMed \citep{sen2008collective, yang2016revisiting}, and \textbf{(b) Heterophilic} datasets, which comprise \textit{(i) Wikipedia graphs} -- Chameleon and Squirrel \citep{rozemberczki2021multi}, \textit{(ii) Actor co-occurrence network} \citep{tang2009social}, and \textit{(iii) Webpage graphs} from WebKB \footnote{\url{http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb}} -- Texas and Cornell. The dataset statistics and their respective homophily ratios are detailed in Table \ref{tab:dataset_stats}. Refer to Appendix \ref{app:graphs} for the discrete curvature and eigenspectrum distributions of these datasets. \vspace{-2mm}
\begin{wraptable}{r}{0.50\textwidth} % 'r' for right, 0.6\textwidth for table width
\setlength{\tabcolsep}{2pt}
\centering
\scriptsize{}
\resizebox{0.50\textwidth}{!}{% Resize the table to fit 0.6\textwidth
\begin{tabular}{c|cccccccc}
\toprule
\textbf{Dataset} & {Cora} & {Citeseer} & {PubMed} & {Chameleon} & {Squirrel} & {Actor} & {Texas} & {Cornell}  \\ 
\midrule
Classes          & 7     & 6       & 5        & 5       & 5      & 5        & 5     & 5     \\
Features         & 1433  & 3703    & 500      & 2325    & 2089   & 932      & 1703  & 1703  \\
Nodes            & 2708  & 3327    & 19717    & 2277    & 5201   & 7600     & 183   & 183   \\
Edges            & 5278  & 4552    & 44324    & 31371   & 198353 & 26659    & 279   & 277   \\
$\mathcal{H}$ & 0.825 & 0.718   & 0.792    & 0.247   & 0.217  & 0.215    & 0.057 & 0.301 \\
\bottomrule
\end{tabular}
}
\caption{Data statistics and homophily ratio ($\mathcal{H}$).}
\vspace{-5mm}\label{tab:dataset_stats}
\end{wraptable}

\textbf{Baselines}. To ensure a fair comparison, we evaluate \modelname\ against three types of baselines: \textbf{(a) Spatial--Euclidean}, including traditional methods such as GCN \citep{kipf2016semi}, GAT \citep{velivckovic2017graph}, and GraphSAGE \citep{hamilton2017inductive}. \textbf{(b) Spatial--Riemannian}, comprising \textit{(i) Constant-curvature models} like HGCN \citep{chami2019hyperbolic} and HGAT \citep{zhang2021hyperbolic}, and \textit{(ii) Mixed-curvature GNNs} such as $\kappa$GCN \citep{bachmann2020constant}, $\mathcal{Q}$GCN \citep{xiong2022pseudo}, and SelfMGNN \citep{sun2022self}. \textbf{(c) Spectral}, including ChebyNet \citep{defferrard2016convolutional}, BernNet \citep{he2021bernnet}, GPRGNN \citep{chien2020adaptive}, and FiGURe \citep{ekbote2024figure} (More details in Appendix \ref{app:baselines}). There are no existing methods that integrate both spectral and geometric signals.

%TODO: Add more experimental details 
\textbf{Experimental Settings}. For the transductive \texttt{LP} task, we randomly split edges into $85\%/5\%/10\%$ for training, validation and test sets, while for transductive \texttt{NC} task, we use the $60\%/20\%/20\%$ split. The results are averaged over 10 random splits and their $95\%$ confidence intervals are reported. We report the AUC-ROC and F1 Score metrics for \texttt{LP} and \texttt{NC} respectively. For computing \texttt{ORC} (in \textit{\modelshort\ Laplacian}), we use $\delta = 0.5$, i.e. equal probability mass is retained by the node and distributed among it's neighbors. We adopt the \texttt{ORC} implementation from \cite{ni2019community}, and use the \textit{Sinkhorn} algorithm \citep{sinkhorn1967concerning} to approximate the $\mathbf{W}_1$ distance. See Appendix \ref{app:orc} for more computational details and complexity analysis. Next, we adopt the implementation of the $\kappa$-stereographic product manifold from \texttt{Geoopt} library\footnote{\url{https://github.com/geoopt/geoopt}}. We heuristically determine the \textit{signature} of our manifold $\mathbb{P}$ (i.e. component manifolds) using the discrete \texttt{ORC} curvature of the input graph. The key idea is that the underlying curvature of the manifold must align with \texttt{ORC}. However, this discussion, along with how we initialise the learnable curvatures for the component manifolds, has been reserved for the Appendix \ref{app:estimate}. For all experiments, we choose the total manifold dimension as $d_{\mathcal{M}}= 48$ and learning rate as \texttt{4e-3}. We use the filter bank  $\Omega_{\mathbb{P}^{d_{\mathcal{M}}}} = \big[\mathbf{Z}^{\mathbf{I}}_{\mathbb{P}^{d_{\mathcal{M}}}}, \mathbf{Z}^{(1)}_{\mathbb{P}^{d_{\mathcal{M}}}}, \mathbf{Z}^{(2)}_{\mathbb{P}^{d_{\mathcal{M}}}}, \dots, \mathbf{Z}^{(L)}_{\mathbb{P}^{d_{\mathcal{M}}}}\big]$, with $L = 10$. For the GPR weights, we experiment with different initializations, $\alpha \in \{0.1, 0.3, 0.5, 0.9\}$. We list the hyperparameter settings in Appendix \ref{app:hyperparam}.

\textbf{Analysis}. Tables \ref{tab:results_node} and \ref{exp:results_edges} present a comparative performance analysis of \modelname\ against baseline models for the \texttt{NC} and \texttt{LP} tasks respectively. \textbf{(a)} \modelname\ consistently outperforms all baseline models across both homophilic and heterophilic datasets, achieving an improvement up to $5.32\%$ in F1-score for \texttt{NC}, up to $5.11\%$ increase in ROC-AUC score for \texttt{LP}. \textbf{(b)} Riemannian baselines perform well for homophilic datasets, while performing poorly in heterophilic cases (Evidence of limitation \texttt{L1}). \textbf{(c)} While spectral baselines like ChebyNet and BernNet perform poorly on heterophilic tasks because they act as \textit{low-pass}, owing to the fixed filters, FiGURe performs well across all tasks because it uses a filter bank to handle different bands in the eigenspectrum. \textbf{(d)} Mixed-curvature baselines ($\kappa$GCN and $\mathcal{Q}$GCN) outperform constant-curvature GNNs (HGCN and HGAT) because they capture the complex graph geometry (Evidence of limitation \texttt{L2}). 
\vspace{2mm}
\begin{wraptable}{r}{0.7\textwidth}
\centering
\setlength{\tabcolsep}{5pt}
\vspace{-2mm}
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\modelname & {Cora} & {Citeseer} & {PubMed} & {Chameleon} & {Actor} & {Squirrel} & {Texas} & {Cornell} \\ \midrule
\modela & 82.50\scriptsize{$\pm$0.18} & 73.20\scriptsize{$\pm$0.16} & 87.20\scriptsize{$\pm$0.26} & 69.42\scriptsize{$\pm$0.01} & 42.70\scriptsize{$\pm$0.22} & 52.00\scriptsize{$\pm$0.20} & \color{blue}81.97\scriptsize{$\pm$0.24} & \color{blue}87.20\scriptsize{$\pm$0.02} \\
\modelb & 82.80\scriptsize{$\pm$0.20} & 73.50\scriptsize{$\pm$0.18} & 85.50\scriptsize{$\pm$0.28} & 69.80\scriptsize{$\pm$0.03} & 43.06\scriptsize{$\pm$0.19} & 50.76\scriptsize{$\pm$0.18} & 91.60\scriptsize{$\pm$0.23} & 91.60\scriptsize{$\pm$0.24} \\
\modelc & 82.70\scriptsize{$\pm$0.32} & 73.40\scriptsize{$\pm$0.17} & 86.40\scriptsize{$\pm$0.29} & 69.60\scriptsize{$\pm$0.07} & 40.73\scriptsize{$\pm$0.20} & 51.20\scriptsize{$\pm$0.19} & \textbf{94.03\scriptsize{$\pm$0.72}} & \textbf{92.31\scriptsize{$\pm$0.09}} \\
\modeld & 81.83\scriptsize{$\pm$0.20} & 72.72\scriptsize{$\pm$0.13} & 85.90\scriptsize{$\pm$0.60} & \textbf{70.23\scriptsize{$\pm$0.61}} & 42.50\scriptsize{$\pm$0.21} & \textbf{52.98\scriptsize{$\pm$0.25}} & 92.50\scriptsize{$\pm$0.22} & 83.11\scriptsize{$\pm$0.36} \\
\modele & 81.90\scriptsize{$\pm$0.17} & 72.50\scriptsize{$\pm$0.15} & \textbf{87.99\scriptsize{$\pm$0.45}} & 65.30\scriptsize{$\pm$0.28} & \textbf{43.91\scriptsize{$\pm$0.11}} & 51.10\scriptsize{$\pm$0.24} & 93.20\scriptsize{$\pm$0.19} & 91.20\scriptsize{$\pm$0.45} \\
\modelf & 81.60\scriptsize{$\pm$0.16} & 72.30\scriptsize{$\pm$0.14} & 87.50\scriptsize{$\pm$0.64} & 67.50\scriptsize{$\pm$0.23} & 43.50\scriptsize{$\pm$0.16} & \color{blue}48.30\scriptsize{$\pm$0.23} & 91.45\scriptsize{$\pm$0.61} & 91.39\scriptsize{$\pm$0.98} \\
\modelg & 80.80\scriptsize{$\pm$0.41} & 71.80\scriptsize{$\pm$0.19} & \color{blue}80.99\scriptsize{$\pm$0.31} & 69.20\scriptsize{$\pm$0.21} & \color{blue}36.44\scriptsize{$\pm$0.23} & 51.83\scriptsize{$\pm$0.21} & 90.99\scriptsize{$\pm$0.21} & 90.00\scriptsize{$\pm$0.10} \\
\modelh & \textbf{83.45\scriptsize{$\pm$0.15}} & \textbf{74.21\scriptsize{$\pm$0.02}} & 85.80\scriptsize{$\pm$0.27} & 70.17\scriptsize{$\pm$0.17} & 43.20\scriptsize{$\pm$0.18} & 52.71\scriptsize{$\pm$0.17} & 93.80\scriptsize{$\pm$0.18} & 91.80\scriptsize{$\pm$0.21} \\
\modeli & 80.50\scriptsize{$\pm$0.30} & 71.50\scriptsize{$\pm$0.18} & \color{blue}79.30\scriptsize{$\pm$0.32} & 68.18\scriptsize{$\pm$0.20} & \color{blue}37.14\scriptsize{$\pm$0.24} & 51.60\scriptsize{$\pm$0.22} & 92.80\scriptsize{$\pm$0.22} & 90.80\scriptsize{$\pm$0.53} \\
\modelj & 81.00\scriptsize{$\pm$0.19} & 72.00\scriptsize{$\pm$0.16} & 87.70\scriptsize{$\pm$0.92} & 68.11\scriptsize{$\pm$0.25} & 43.70\scriptsize{$\pm$0.17} & \color{blue}45.67\scriptsize{$\pm$0.25} & \color{blue}88.15\scriptsize{$\pm$0.25} & \color{blue}85.01\scriptsize{$\pm$0.51} \\
\bottomrule
\end{tabular}}
\caption{Performance comparison of \modelname\ with different manifold signatures for Node Classification (\texttt{NC}). Best performing \textit{signatures} are in \textbf{Bold}, and cases with a large decline in performance because of manifold mismatch are in {\color{blue}Blue}.}\label{tab:signatures}
\vspace{-5mm}
\end{wraptable}Table \ref{tab:filters} presents the learnt filter weights ($\epsilon$), highlighting the distinct preferences of homophilic and heterophilic datasets. Homophilic datasets, such as Citeseer and PubMed, emphasize low-pass filters, with the highest weights assigned to lower-order filters, $\mathbf{Z}^{(2)}$ and $\mathbf{Z}^{(3)}$, at $45.94\%$ and $47.97\%$ respectively. In contrast, heterophilic datasets like Actor and Cornell favor high-pass filters, attributing $40.09\%$ and $28.56\%$ to higher-order filters, $\mathbf{Z}^{(5)}$ and $\mathbf{Z}^{(9)}$. In the next section, we perform extensive ablation studies to evaluate the effectiveness of all components of \modelname.


\begin{table}[t] % 'r' for right, 0.6\textwidth for table width
	\centering
	\setlength{\tabcolsep}{6pt}
	\vspace{0.2cm}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{@{}lccccccccccc@{}}
			\toprule
			 \textbf{Baseline}                & {Cora}                               & {Citeseer}                           & {PubMed}                             & {Chameleon}                          & {Actor}                              & {Squirrel}                           & {Texas}                              & {Cornell}                            & \textbf{Av. $\Delta$Gain} \\ \midrule
			GCN              & 75.21\scriptsize{$\pm$0.28}                & 67.30\scriptsize{$\pm$1.05}                & 83.75\scriptsize{$\pm$0.07}                & 61.16\scriptsize{$\pm$0.23}                & 31.12\scriptsize{$\pm$0.96}                & 43.06\scriptsize{$\pm$0.33}                & 75.61\scriptsize{$\pm$0.07}                & 67.72\scriptsize{$\pm$1.19}                & 11.95                      \\
			GAT              & 76.70\scriptsize{$\pm$0.13}                & 66.23\scriptsize{$\pm$0.85}                & 82.83\scriptsize{$\pm$0.22}                & 63.10\scriptsize{$\pm$0.77}                & 32.65\scriptsize{$\pm$0.23}                & 43.90\scriptsize{$\pm$0.01}                & 76.09\scriptsize{$\pm$0.77}                & 74.01\scriptsize{$\pm$0.01}                & 10.63                      \\
			SAGE             & 71.88\scriptsize{$\pm$0.91}                & 70.01\scriptsize{$\pm$0.64}                & 81.09\scriptsize{$\pm$0.13}                & 59.99\scriptsize{$\pm$0.89}                & 36.73\scriptsize{$\pm$0.01}                & 41.11\scriptsize{$\pm$1.16}                & 77.11\scriptsize{$\pm$0.45}                & 69.91\scriptsize{$\pm$0.24}                & 11.59                      \\ \midrule
			HGCN             & 78.50\scriptsize{$\pm$0.14}                & 69.55\scriptsize{$\pm$0.39}                & 83.72\scriptsize{$\pm$0.21}                & 60.18\scriptsize{$\pm$0.57}                & 35.89\scriptsize{$\pm$0.29}                & 39.93\scriptsize{$\pm$0.35}                & 88.11\scriptsize{$\pm$1.12}                & 72.88\scriptsize{$\pm$1.15}                & 8.97                       \\
			HGAT             & 77.12\scriptsize{$\pm$0.01}                & 70.12\scriptsize{$\pm$0.92}                & 84.02\scriptsize{$\pm$0.19}                & 62.43\scriptsize{$\pm$0.59}                & 35.12\scriptsize{$\pm$0.27}                & 41.78\scriptsize{$\pm$0.37}                & 85.56\scriptsize{$\pm$1.10}                & 73.12\scriptsize{$\pm$0.18}                & 8.91                       \\
			$\kappa$GCN      & 78.71\scriptsize{$\pm$1.37}                & 68.14\scriptsize{$\pm$0.34}                & \color{Green}85.18\scriptsize{$\pm$0.52}   & 62.12\scriptsize{$\pm$0.49}                & 34.57\scriptsize{$\pm$0.26}                & 43.04\scriptsize{$\pm$0.31}                & 85.03\scriptsize{$\pm$0.63}                & \color{magenta}86.36\scriptsize{$\pm$0.64} & 7.06                       \\
			$\mathcal{Q}$GCN & 79.64\scriptsize{$\pm$0.38}                & \color{magenta}71.15\scriptsize{$\pm$1.11} & \color{magenta}84.76\scriptsize{$\pm$0.13} & 61.83\scriptsize{$\pm$1.01}                & 32.24\scriptsize{$\pm$0.65}                & 46.65\scriptsize{$\pm$0.90}                & 82.76\scriptsize{$\pm$0.07}                & 83.90\scriptsize{$\pm$0.71}                & 7.20                       \\
			SelfMGNN         & \color{Green}80.19\scriptsize{$\pm$0.60}   & 70.91\scriptsize{$\pm$0.38}                & 82.81\scriptsize{$\pm$0.34}                & 64.97\scriptsize{$\pm$0.54}                & \color{Green}38.99\scriptsize{$\pm$0.23}   & \color{Green}49.79\scriptsize{$\pm$0.29}   & \color{Green}90.92\scriptsize{$\pm$0.65}   & 85.01\scriptsize{$\pm$0.69}                & 4.62                       \\ \midrule
			ChebyNet         & 71.09\scriptsize{$\pm$0.91}                & 66.67\scriptsize{$\pm$0.38}                & 83.83\scriptsize{$\pm$0.42}                & 59.96\scriptsize{$\pm$1.51}                & 38.02\scriptsize{$\pm$0.01}                & 45.67\scriptsize{$\pm$0.11}                & 79.08\scriptsize{$\pm$0.96}                & 71.33\scriptsize{$\pm$1.04}                & 10.61                      \\
			BernNet          & 73.34\scriptsize{$\pm$0.53}                & 62.12\scriptsize{$\pm$2.09}                & 82.15\scriptsize{$\pm$0.13}                & 62.03\scriptsize{$\pm$0.12}                & 33.55\scriptsize{$\pm$0.24}                & 42.81\scriptsize{$\pm$0.66}                & 75.11\scriptsize{$\pm$0.14}                & 65.56\scriptsize{$\pm$1.02}                & 12.98                      \\
			GPRGNN           & 79.49\scriptsize{$\pm$0.31}                & 67.61\scriptsize{$\pm$0.38}                & 84.07\scriptsize{$\pm$0.09}                & \color{magenta}65.09\scriptsize{$\pm$0.43} & 37.43\scriptsize{$\pm$1.09}                & 47.51\scriptsize{$\pm$0.23}                & \color{magenta}88.34\scriptsize{$\pm$0.09} & \color{Green}87.21\scriptsize{$\pm$0.70}   & 5.58                       \\
			FiGURe           & \color{magenta}80.01\scriptsize{$\pm$0.09} & \color{Green}71.26\scriptsize{$\pm$0.41}   & 83.89\scriptsize{$\pm$0.11}                & \color{Green}67.18\scriptsize{$\pm$0.02}   & \color{magenta}38.31\scriptsize{$\pm$0.36} & \color{magenta}48.71\scriptsize{$\pm$1.02} & 86.66\scriptsize{$\pm$0.62}                & 85.01\scriptsize{$\pm$0.68}                & 4.94                       \\ \midrule 
			\modelname       & \textbf{83.45\scriptsize{$\pm$0.15}}       & \textbf{74.21\scriptsize{$\pm$0.02}}       & \textbf{87.99\scriptsize{$\pm$0.45}}       & \textbf{70.23\scriptsize{$\pm$0.61}}       & \textbf{43.91\scriptsize{$\pm$0.11}}       & \textbf{52.98\scriptsize{$\pm$0.25}}       & \textbf{94.03\scriptsize{$\pm$0.72}}       & \textbf{92.31\scriptsize{$\pm$0.09}}       & 0.0                        \\
			\textbf{$\Delta$Imp.}& 3.26$\%$	&2.95$\%$	&2.81$\%$	&4.05$\%$	&5.32$\%$	&3.19$\%$ &3.11$\%$	&5.10$\%$\\
			\bottomrule
		\end{tabular}
		}
  \caption{Performance comparision of \modelname\ with baselines for \texttt{NC} task (Mean F1 Score $ \pm \; 95\%$ confidence interval). \textbf{First}, {\color{Green}Second} and {\color{Magenta}Third} best performing models are highlighted. \textbf{Av. $\Delta$ Gain} represents the average gain of \modelname\ over the model in that row, averaged across the different datasets. \textbf{$\Delta$Imp.} implies the $\%$ improvement of \modelname\ over the second best performing baseline.}\vspace{-2mm}
  \label{tab:results_node}\end{table}
\begin{table}[t]
\centering
\begin{minipage}{0.61\textwidth}

\scriptsize
\centering
\label{tab:results_nc}
\setlength{\tabcolsep}{5pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\textbf{Dataset}    & $\mathbf{I}$    & $\mathbf{Z}^{(1)}$  & $\mathbf{Z}^{(2)}$  & $\mathbf{Z}^{(3)}$  & $\mathbf{Z}^{(4)}$  & $\mathbf{Z}^{(5)}$  & $\mathbf{Z}^{(6)}$  & $\mathbf{Z}^{(7)}$  &$\mathbf{Z}^{(8)}$ & $\mathbf{Z}^{(9)}$& $\mathbf{Z}^{(10)}$ \\ \midrule
Cora       & \textbf{0.1729} & \color{Green}0.1586 & 0.0695 & 0.0438  & 0.0446 & 0.0221 & \color{magenta}0.1525  & 0.0526 & 0.0711 & 0.0739 & 0.1385 \\
Citeseer   & 0.0478 & 0.0506 & \textbf{0.4594} & 0.0183 & \color{magenta}0.0765 & 0.0187 & \color{Green}0.1093 & 0.0755 & 0.0686 & 0.0232 & 0.0522 \\
PubMed     & \color{Green}0.1666 & 0.0342 & 0.0132 & \textbf{0.4795} &\color{magenta} 0.1116 & 0.0289 & 0.0057 & 0.0153 & 0.0564 & 0.0442 & 0.0444 \\
Chameleon  & 0.0232 & \color{magenta}0.1679 & 0.0536 & \color{Green}0.1696 & 0.0121 & 0.0137 & 0.0392 & \textbf{0.2178} & 0.1356 & 0.0140 & 0.1532 \\
Actor      & 0.0360 & 0.0753 & 0.0332 & 0.0337 & 0.0101 &\textbf{ 0.4009} & \color{Green}0.2419 & 0.0096 & \color{magenta}0.1155 & 0.0089 & 0.0347 \\
Squirrel   & 0.0306 & 0.0796 & 0.0719 & \textbf{0.2562} & 0.0351 & 0.0423 & 0.0415 & \color{Green}0.1322 & 0.1050 & \color{magenta}0.1105 & 0.0951 \\
Texas      & \color{magenta}0.1052 & 0.0308 & 0.0664 & 0.0490 & 0.0766 & 0.0274 & 0.0995 & 0.0402 & \color{Green}0.1741 & 0.0639 & \textbf{0.2667}\\
Cornell    & \color{magenta}0.1159 & 0.0302 & 0.0671 & 0.0434 & 0.0747 & 0.0249 & 0.0966 & 0.0358 & \color{Green}0.1691 & \textbf{0.2856} & 0.0568 \\
\bottomrule
\end{tabular}}
\caption{Learned filter weights ($\texttt{NC}$) for the top-performing split, distinguishing between homophilic (favoring low-pass filters) and heterophilic (favoring high-pass filters). \textbf{First}, {\color{Green}second}, and {\color{magenta}third} highest filter weights are highlighted.}\vspace{-5mm}
\label{tab:filters}
\end{minipage}%
\hfill
\begin{minipage}{0.36\textwidth}
\centering
\setlength{\tabcolsep}{3pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Dataset} & Signature \\ \midrule
Cora &  $\mathbb{H}^{16} ({\color{red}-0.36}, {\color{blue}0.34}) \times \mathbb{S}^{16} ({\color{red}+0.68}, {\color{blue}0.29}) \times \mathbb{E}^{16} ({\color{red}0}, {\color{blue}0.37})$ \\
Citeseer & $\mathbb{H}^{16} ({\color{red}-0.62}, {\color{blue}0.36}) \times \mathbb{S}^{16} ({\color{red}+.55}, {\color{blue}0.49}) \times \mathbb{E}^{16} ({\color{red}0}, {\color{blue}0.15})$ \\
PubMed &  $\mathbb{H}^{16} ({\color{red}-0.91}, {\color{blue}0.40}) \times \mathbb{H}^{16} ({\color{red}-0.37}, {\color{blue}0.36}) \times \mathbb{E}^{16} ({\color{red}0}, {\color{blue}0.24})$ \\
Chameleon &  $\mathbb{H}^{16} ({\color{red}-0.21}, {\color{blue}0.22}) \times \mathbb{S}^{16} ({\color{red}+0.13}, {\color{blue}0.29}) \times \mathbb{S}^{16} ({\color{red}+0.59}, {\color{blue}0.49})$ \\
Actor & $\mathbb{H}^{16} ({\color{red}-0.87}, {\color{blue}0.39}) \times \mathbb{H}^{16} ({\color{red}-0.53}, {\color{blue}0.36}) \times \mathbb{E}^{16} ({\color{red}0}, {\color{blue}0.25})$ \\
Squirrel & $\mathbb{H}^{16} ({\color{red}-0.28}, {\color{blue}0.14}) \times \mathbb{S}^{16} ({\color{red}+0.31}, {\color{blue}0.39}) \times \mathbb{S}^{16} ({\color{red}+0.67}, {\color{blue}0.53})$ \\
Texas   & $\mathbb{H}^{8} ({\color{red}-0.45}, {\color{blue}0.32}) \times \mathbb{S}^{8} ({\color{red}+0.34}, {\color{blue}0.23}) \times \mathbb{E}^{32} ({\color{red}0}, {\color{blue}0.45})$ \\
Cornell & $\mathbb{H}^{8} ({\color{red}-0.50}, {\color{blue}0.14}) \times \mathbb{S}^{8} ({\color{red}+0.23}, {\color{blue}0.19}) \times \mathbb{E}^{32} ({\color{red}0}, {\color{blue}0.67})$ \\
\bottomrule
\end{tabular}}
\caption{Learning results (\texttt{NC}) of \modelname\ for the best performing product signature $-$ \textit{manifold}$^{\text{ (dim)}}$ ({\color{red}curvature}, {\color{blue}weight}).}
\vspace{-5mm}\label{tab:product}
\end{minipage}
\end{table}

\subsection{Ablation Study}\vspace{-2mm}
$\blacksquare$ \textbf{Impact of product manifold \textit{signatures}}. Different datasets leverage substructures with varying curvatures as inductive biases for downstream tasks, leading to differing performance across product manifold signatures. Table \ref{tab:product} shows the learned {\color{red}curvature} and {\color{blue}weight} ($\beta$) for the best-performing configurations. Table \ref{tab:signatures} highlights performance across multiple \textit{signatures} (which are heuristically determined as discussed in Appendix \ref{app:estimate}), with large degradation (marked in {\color{blue}blue}) when there is a mismatch between manifold and graph curvature. For example, Squirrel, which has a predominantly positively curved structure (Figure \ref{app:graphs}), performs poorly (drop of $\sim 6.5\%$) on signatures dominated by negative curvature, such as \modelg\ and \modelj.

$\blacksquare$ \textbf{Impact of mixed-curvature space}. Table \ref{tab:ablation} outlines the comparison of \modelname\ with its Euclidean variant $\modelname_{euc}$, where all representations are learned in Euclidean space, omitting \textit{\modelshort\ pooling}. On average, $\modelname_{euc}$ experiences a $\sim3.2\%$ performance drop across all datasets for the \texttt{NC} task, highlighting the importance of capturing mixed-curvature signals. Notably, the performance degradation is more pronounced on heterophilic datasets ($\sim 4\%$) compared to homophilic datasets ($\sim 2\%$). This suggests that \modelname's integration of spectral and geometric properties is crucial, particularly for heterophilic data where capturing the eigenspectrum is more relevant.\vspace{2mm}
\begin{wraptable}{r}{0.635\textwidth}
 \centering
    \setlength{\tabcolsep}{3pt}
    \resizebox{0.65\textwidth}{!}{
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
Dataset     & $\modelname$                         & $\modelname_{{euc}}$        & $\modelname_{{lap}}$        & $\modelname_{{enc}}$        & $\modelname_{{pool}}$       & $\modelname_{{fil}}$ \\ \midrule 
Cora        & \textbf{83.45\scriptsize{$\pm$0.15}} & 80.78\scriptsize{$\pm$0.13} & 81.95\scriptsize{$\pm$0.34} & 82.61\scriptsize{$\pm$0.25} & 80.11\scriptsize{$\pm$0.22} & 81.20\scriptsize{$\pm$0.29} \\
Citeseer    & \textbf{74.21\scriptsize{$\pm$0.02}} & 72.50\scriptsize{$\pm$0.19} & 72.73\scriptsize{$\pm$0.30} & 71.95\scriptsize{$\pm$0.28} & 73.10\scriptsize{$\pm$0.30} & 72.87\scriptsize{$\pm$0.27} \\
PubMed      & \textbf{87.99\scriptsize{$\pm$0.45}} & 85.23\scriptsize{$\pm$0.25} & 86.67\scriptsize{$\pm$0.10} & 87.11\scriptsize{$\pm$0.12} & 86.23\scriptsize{$\pm$0.10} & 87.29\scriptsize{$\pm$0.09} \\
Chameleon   & \textbf{70.23\scriptsize{$\pm$0.61}} & 66.47\scriptsize{$\pm$0.56} & 68.12\scriptsize{$\pm$0.35} & 67.83\scriptsize{$\pm$0.31} & 68.47\scriptsize{$\pm$0.32} & 66.12\scriptsize{$\pm$0.34} \\
Actor       & \textbf{43.91\scriptsize{$\pm$0.11}} & 39.03\scriptsize{$\pm$0.09} & 43.03\scriptsize{$\pm$0.27} & 41.12\scriptsize{$\pm$0.28} & 40.03\scriptsize{$\pm$0.27} & 38.81\scriptsize{$\pm$0.29} \\
Squirrel    & \textbf{52.98\scriptsize{$\pm$0.25}} & 49.92\scriptsize{$\pm$0.36} & 51.39\scriptsize{$\pm$0.35} & 50.92\scriptsize{$\pm$0.35} & 51.03\scriptsize{$\pm$0.11} & 48.13\scriptsize{$\pm$0.17} \\
Texas       & \textbf{94.03\scriptsize{$\pm$0.72}} & 90.15\scriptsize{$\pm$0.61} & 92.15\scriptsize{$\pm$0.52} & 93.15\scriptsize{$\pm$0.55} & 92.15\scriptsize{$\pm$0.60} & 90.27\scriptsize{$\pm$0.62} \\
Cornell     & \textbf{92.31\scriptsize{$\pm$0.09}} & 89.46\scriptsize{$\pm$0.13} & 90.46\scriptsize{$\pm$0.17} & 91.06\scriptsize{$\pm$0.28} & 90.46\scriptsize{$\pm$0.07} & 89.73\scriptsize{$\pm$0.09} \\
\textbf{Avg. $\Delta$ Gain} & 0 &  \color{blue}3.19	&\color{blue}1.57	&\color{blue}1.67	&\color{blue}2.19	& \color{blue}3.08\\
    \bottomrule
    \end{tabular}}
     \caption{Ablation study (\texttt{NC}) results on benchmark datasets. \textbf{Av. $\Delta$ Gain} represents the average gain of \modelname\ over the model in that column, averaged across the different datasets.}
     \vspace{-3mm}
     \label{tab:ablation}
\end{wraptable}
$\blacksquare$ \textbf{Impact of \modelshort\ Laplacian}. To assess the effectiveness of the \textit{\modelshort\ Laplacian}, we conducted an ablation study by replacing the \modelshort\ Laplacian-based adjacency matrix $\widetilde{\mathbf{A}}$, with the standard graph adjacency matrix (for \modelshort\ filtering) in $\modelname_{{lap}}$. This resulted in a performance drop of $1.57\%$ (avg.) across all datasets. The relatively modest degradation highlights the \modelshort\ Laplacian’s role in capturing curvature information.

$\blacksquare$ \textbf{Impact of the filter bank, curvature encoding and \modelshort\ pooling}. $\modelname_{fil}$ replaces the filter bank with a single learnable filter, as a result the performance degrades largely on heterophilic datasets ($\sim 4\%$), while the degradation in performance is not that large on homophilic datasets ($\sim 2.5\%$). Further, we remove the curvature-based positional encoding in $\modelname_{enc}$ and replace the \modelshort\ pooling mechanism with simple concatenation in $\modelname_{pool}$. We observe a consistent decline in performance across both these ablations. Owing to spatial limitations, we present the experimental results for the \texttt{LP} task in Appendix \ref{app:lp}.


\vspace{-2mm}
\section{Conclusion}
\vspace{-3mm}
In this paper, we propose to unify \textit{Spectral} and \textit{Curvature} signals in a graph for learning optimal graph representations, aiming to inspire further research in this area. \modelname\ introduces a graph learning paradigm parameterized by spectral filters on a mixed-curvature product manifold. We propose a new curvature-informed \textit{\modelshort\ Laplacian} operator to capture the underlying geometry, use it to define a novel GPR-based spectral filter-bank (\textit{\modelshort\ Filtering}) and introduce an attention-based pooling mechanism to fuse representations from multiple mixed-curvature graph filters (\textit{\modelshort\ Pooling}). \modelname\ outperforms the state-of-the-art baselines for node classification and link prediction over homophilic and heterophilic datasets, highlighting the efficacy of combining spectrum and curvature (geometry), in learning graph representations.

\newpage
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage{}
% \tableofcontents
\newpage
\startcontents[subsections]
\addcontentsline{toc}{section}{\Large Table of Contents}  % Ensures it gets listed in the main Table of Contents
\printcontents[subsections]{l}{1}[3]{}
\newpage
\section{Appendix}

\subsection{Notation Table}

\footnotesize{
\begin{tabularx}{\textwidth}{@{} p{3cm} X @{}}
\toprule
\textbf{Notation} & \textbf{Reference} \\
\midrule
% Add your table rows here
$\mathbb{H}$ & Hyperbolic manifold\\
$\mathbb{S}$ & Spherical manifold\\
$\mathbb{E}$ & Euclidean manifold\\
$\mathbb{P}^{d_{\mathcal{M}}}$ & Product manifold of dimension $d_{\mathcal{M}}$\\
$\kappa$ & Continous manifold curvature\\
$\widetilde{\kappa}$ &  Ollivier-Ricci Curvature (\texttt{ORC})\\
$\widetilde{\kappa}(x, y)$ &  \texttt{ORC} of edge $\{x, y\}$\\
$\widetilde{\kappa}(x)$ &  \texttt{ORC} of node $x$\\
$m_x^{\delta}$ & Probability mass assigned to node $x$ for \texttt{ORC} computation\\
$\delta_{ij}$ & Kroneckner delta function\\
$\mathcal{N}(x)$& Neighbourhood set of node $x$\\
$x \sim y$ & This implies that $x$ and $y$ are adjacent nodes\\
$\delta$ & \texttt{ORC} neighbourhood weighting parameter\\
$\mathbf{W}_1(.)$ & Wasserstein-1 distance\\
$d_{\mathcal{G}}(x, y)$ &Shortest path (graph distance) between nodes $x$ and $y$ on graph $\mathcal{G}$\\
$\mathcal{M}_{i}^{\kappa_i, d_i}$ & Constant-curvature manifold with dimension $d_i$ and curvature $\kappa_i$. $\mathcal{M}_{i} \in \{\mathbb{H}, \mathbb{S}, \mathbb{E}\}$\\
$\widetilde{\mathbf{L}}, \widetilde{\mathbf{D}}, \widetilde{\mathbf{A}}$ & \modelshort\ Laplacian operator; $\widetilde{\mathbf{L}} =  \widetilde{\mathbf{D}} - \widetilde{\mathbf{A}}$ \\
$\widetilde{\mathbf{L}}_n, \widetilde{\mathbf{A}}_n$ & Normalized \modelshort\ Laplacian and Adjacency matrices; $\widetilde{\mathbf{A}}_n = \widetilde{\mathbf{D}}^{\frac{-1}{2}}\; \widetilde{\mathbf{A}}\;\widetilde{\mathbf{D}}^{\frac{-1}{2}}$ \\
$\mathbf{L}, \mathbf{D}, \mathbf{A}$ & Traditional graph Laplacian, Adjacency and Degree matrices; $\mathbf{L} = \mathbf{D} - \mathbf{A}$ \\
$d_f$&Input graph feature dimension\\
$d_{\mathcal{M}}$ &Total dimension of the product manifold\\
$d_{\mathcal{C}}$ &Total dimension of the curvature embedding\\
$\mathbf{F} \in \mathbb{R}^{n \times d_f}$ & Input feature matrix\\
$\mathbf{exp}^{\kappa}_{\mathbf{0}} : \mathbb{R}^{d_{\mathcal{M}}} \rightarrow \mathcal{M}^{\kappa}$ & Exponential map, to map from tangent plane (Euclidean) to the product manifold\\
$\oplus_{\kappa}$ & \textit{Mobius} addition\\
$\otimes_{\kappa}$ & $\kappa$-right-matrix-multiplication\\
$\boxtimes_{\kappa}$ & $\kappa$-left-matrix-multiplication\\
$\boldsymbol{\zeta}_{(x)}$ & Final node representation for node $x$\\
$\epsilon_l$ & Learnable weight of $l^{th}$ filter\\
 $\boldsymbol{\zeta} \in \mathbb{P}^{n \times (d_{\mathcal{M}}+d_{\mathcal{C}})}$ & Matrix containing final node embeddings\\
 $\beta_q$ & Learnable weight of $q^{th}$ component manifold\\
  $\tau_q$ & Relative importance of manifold $\mathcal{M}_q$ in \modelshort\ Pooling \\
 $L$& Total number of filters\\
 $\mathcal{Q}$&Total number of components in product manifold\\
 $l$ & Used to denote the $l^{th}$ filter\\
 $\mathbf{Z}^{(L)(x)}_{\mathbb{P}^{d_{\mathcal{M}}}}$ & The \texttt{GPR}-based node representation for filter with $L$ layers, on manifold $\mathbb{P}^{d_{\mathcal{M}}}$\\
 $\kappa_{(q)}$ & Manifold curvature of $q^{th}$ component in product manifold\\
$d_{(q)}$ & Manifold dimension of $q^{th}$ component in product manifold\\ 
 $\mathbf{H}^{(l)}_{\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}}$ & Hidden state representation after $l$ layers of \texttt{GPR}, on $q^{th}$ manifold component with curvature $\kappa_{(q)}$ and dimension $d_{(q)}$\\
 $\gamma_l$ & \texttt{GPR} weight for $l^{th}$ layer in the filter, while propagation \texttt{GPR} score.\\
 $\alpha$& Initialising parameter for \texttt{GPR}\\
 Signature & $ \mathbb{P}^{d_{\mathcal{M}}} = \times_{q=1}^{\mathcal{Q}} \mathcal{M}_{q}^{\kappa_{(q)}, d_{(q)}}  = (\times_{h=1}^{\mathcal{H}} \mathbb{H}_{h}^{\kappa_{(h)}, d_{(h)}}) \times (\times_{s=1}^{\mathcal{S}} \mathbb{S}_{s}^{\kappa_{(s)}, d_{(s)}}) \times \mathbb{E}^{d_{(e)}}$\\
 $\psi : V \to \mathbb{R}$ & A function defined on vertex set $V$\\
 $\Phi_{\mathbb{P}^{d_{\mathcal{C}}}}(\widetilde{\kappa}(x))$ & Curvature encoding on product manifold\\
 $\mathcal{K}_{\mathbb{P}}(\widetilde{\kappa}_a,\widetilde{\kappa}_b)$ & $\mathcal{K}_{\mathbb{P}}(\widetilde{\kappa}_a,\widetilde{\kappa}_b):= \big\langle \Phi_{\mathbb{P}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_a), \Phi_{\mathbb{P}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_b) \big\rangle$ is the curvature kernel\\
 $\lambda_i$ & $i^{th}$ eigenvalue of $\widetilde{\mathbf{A}}_n$\\
 $\widetilde{\lambda}_i$ & $i^{th}$ eigenvalue of $\widetilde{\mathbf{L}}_n$\\
 $f_{\theta}(.): \mathbb{R}^{d_{f}} \rightarrow \mathbb{R}^{d_{\mathcal{M}}}$ & Neural network with parameter set $\{{\theta\}}$ that generates the hidden state features before feeding input to \modelshort\ Filtering.\\
  $g_{\theta}(.): \mathbb{R}^{d_{\mathcal{M}}} \rightarrow \mathbb{R}^{d_{\mathcal{C}}}$ & Neural network projector used in curvature encoding\\

\bottomrule
\end{tabularx}}

\newpage

\newpage
\subsection{More on Preliminaries} \label{app:prelim}
\subsubsection{Analysis of Real-world Graphs}\label{app:graphs}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{imgs/edge_curvatures.png}
  \caption{Distribution of Ollivier-Ricci curvatures $\widetilde{\kappa}(x, y)$ of \textbf{edges} across different datasets. The histograms illustrate the frequencies of edge-based Ollivier-Ricci curvature values  for each dataset, \textit{highlighting} the topological diversity in both homophilic and heterophilic settings, and hence the need of learning representations in manifolds with different curvatures. }
  \label{app_fig:edge_curv}
\end{figure}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{imgs/node_curvatures.png}
  \vspace{-5mm}
  \caption{Distribution of Ollivier-Ricci curvatures $\widetilde{\kappa}(x)$ of \textbf{nodes} across different datasets. }
  \label{app_fig:node_curv}
\end{figure}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{imgs/spectral_distribution.png}
  \vspace{-5mm}
  \caption{Distribution of \textbf{Spectral Energy} with the respective eigenvalues of the graph Laplacian. Spectral Energy, $\mathcal{E}_i = {f_i^2}/{\sum_j{f_j^2}}$, where $f_j$ is the $j^{th}$ Fourier mode of the graph Laplacian. Low frequency implies \textit{homophily} and high frequency components correspond to \textit{heterophily}. These plots highlight the importance of capturing signals from different parts of the eigenspectrum for designing a GNN that works well across multiple tasks.}
  \label{app_fig:spectral_energy}
\end{figure}

\subsubsection{Ollivier-Ricci Curvature (\texttt{ORC})}\label{app:orc}
In an unweighted graph, the neighborhood of each node $x$, denoted as $\mathcal{N}(x)$, is assigned a probability distribution according to a lazy random walk formulation \citep{lin-yau}. Specifically, we define the distribution as follows:
\begin{equation}\label{eq:orc-lazy}
m_z^{\alpha}(x) = \begin{cases}
\alpha, & \text{if } z = x, \\
\frac{1-\alpha}{|\mathcal{N}(x)|}, & \text{if } z \in \mathcal{N}(x), \\
0, & \text{otherwise}.
\end{cases}
\end{equation}
Here, $\alpha$ controls the probability that a random walk will remain at the current node, while the remaining probability mass $(1-\alpha)$ is uniformly distributed across the neighboring nodes. This formulation connects \texttt{ORC} with lazy random walks and influences the balance between local exploration and the likelihood of revisiting a node. In this work, we use $\alpha = 0.5$, meaning that equal probability mass is distributed between the node itself and its neighbors, striking a balance between\textit{ breadth-first} and\textit{ depth-first} search strategies. The choice of $\alpha$ is crucial and depends on the topology of the graph. A smaller $\alpha$ value encourages more local exploration, while a larger $\alpha$ favors revisiting nodes, thereby promoting a “lazy” walk. For our experiments, $\alpha = 0.5$ was chosen to reflect an equal probability mass distribution between the node and its neighbors.

$\blacksquare$\textbf{ Computational Considerations.} Computing \texttt{ORC} can be computationally intensive due to the need to calculate the Wasserstein-1 distance ($W_1$), between the neighborhood distributions of connected nodes. In a discrete setting, this corresponds to solving a linear program. Typically, $W_1(m_x, m_y)$ between two nodes $x$ and $y$ is computed using the \emph{Hungarian algorithm} \citep{kuhn1955hungarian}, which has a cubic time complexity. However, this becomes prohibitively expensive as the graph size increases. Alternatively, the Wasserstein-1 distance can be approximated using the \textit{Sinkhorn algorithm} \citep{sinkhorn1967concerning}, which reduces the complexity to quadratic time. \underline{For this work}, we employ the Sinkhorn approximation to compute \texttt{ORC} efficiently. Below, we provide an \underline{alternative} to approximate \texttt{ORC} of an edge in linear time, in case of very large (million-scale) real-world graphs.

$\blacksquare$\textbf{ Approximating} \texttt{ORC} \textbf{in Linear Time}. Even with the quadratic complexity of the Sinkhorn algorithm, scaling to large networks remains challenging. To address this, a linear-time combinatorial approximation of \texttt{ORC} can be employed, as suggested by \citet{tian2023curvature}. This method approximates the Wasserstein distance by utilizing local structural information, making it much more computationally feasible. The approximation of ORC builds on classical bounds first introduced by \citet{jost2014ollivier}. Let $\#(x, y)$ denote the number of triangles formed by the edge $(x, y)$, and define $a \wedge b = \min(a, b)$, $a \vee b = \max(a, b)$ and $d_x$ is the degree of node $x$. The following bounds on \texttt{ORC} can be derived for an edge $e = {x, y}$:

\begin{theorem}[~\cite{jost2014ollivier}]
\label{the:ollilow}
For an unweighted graph, the Ollivier-Ricci curvature of an edge $e = {x, y}$ satisfies the following bounds:
\begin{enumerate}
\item Lower bound:
\begin{align}\label{eq:ollilow}
\widetilde{\kappa}(x, y) &\geq - \left( 1 - \frac{1}{d_x} - \frac{1}{d_y} - \frac{\#(x, y)}{d_x \wedge d_y} \right)_{+} - \left( 1 - \frac{1}{d_x} - \frac{1}{d_y} - \frac{\#(x, y)}{d_x \vee d_y} \right)_{+} + \frac{\#(x, y)}{d_x \vee d_y}.\nonumber
\end{align}

\item Upper bound:
\begin{equation}\label{eq:olliup}
\widetilde{\kappa}(x, y) \leq \frac{\#(x, y)}{d_x \vee d_y}.
\end{equation}
\end{enumerate}
\end{theorem}

The \texttt{ORC} of an edge, can then be approximated as the arithmetic mean of these bounds:
\begin{equation}\label{eq:orc-approx}
\widehat{\kappa}(x, y) := \frac{1}{2} \left( \kappa^{\text{upper}}(x, y) + \kappa^{\text{lower}}(x, y) \right).
\end{equation}
The proof of these bounds has been detailed in \cite{tian2023curvature}. This approximation is computationally efficient, with linear-time complexity, and can be parallelized easily across edges, making it suitable for large-scale graphs. The computation relies solely on local structural information, such as the degree of the nodes and the number of triangles.



\subsubsection{Product Manifolds} \label{app:product_manifolds}
Let $\mathcal{M}_1, \mathcal{M}_2, \dots, \mathcal{M}_k$ denote a set of smooth manifolds. Their Cartesian product forms a product manifold, denoted by  $\mathbb{P}$ , such that \( \mathbb{P} = \mathcal{M}_1 \times \mathcal{M}_2 \times \dots \times \mathcal{M}_k \). Any point $p \in \mathbb{P}$ is characterized by its coordinates \( p = (p_1, p_2, \dots, p_k) \), where each  $p_i$  corresponds to a point on the individual manifold $\mathcal{M}i$. Similarly, a tangent vector  $v \in  \mathcal{T}_p\mathbb{P}$  can be expressed as \( v = (v_1, v_2, \dots, v_k) \), where each  $v_i \in  \mathcal{T}_{p_i}$ $\mathcal{M}_i$  represents the projection of  $v$  in the tangent space of the respective component manifold  $\mathcal{M}_i$. Optimization over manifolds requires the notion of taking steps along the manifold, which can be achieved by moving in the tangent space and mapping those movements back onto the manifold through the exponential map. The exponential map at a point \( p \in \mathbb{P} \), denoted \( \mathbf{exp}_p : \mathcal{T}_p \mathbb{P} \to \mathbb{P} \), allows for this transfer. For product manifolds, the exponential map decomposes into individual component exponential maps. Specifically, given a tangent vector \( v = (v_1, v_2, \dots, v_k) \) at \( p = (p_1, p_2, \dots, p_k) \in \mathbb{P} \), the exponential map on \( \mathbb{P} \) can be expressed as:
\begin{equation}
    \mathbf{exp}_p(v) = (\mathbf{exp}_{p_1}(v_1), \mathbf{exp}_{p_2}(v_2), \dots, \mathbf{exp}_{p_k}(v_k))
\end{equation}

\subsubsection{Kappa-Stereographic Model}\label{app:kappa_sterographic}
The $\kappa$-stereographic model \citep{bachmann2020constant} unifies Hyperbolic and Spherical geometries under gyrovector formalism. This model leverages the framework of gyrovector spaces to represent all three constant curvature geometries—hyperbolic, Euclidean, and spherical—simultaneously. Additionally, it facilitates smooth transitions between these constant curvature geometries, enabling the joint learning of space curvatures alongside the embeddings. It is a smooth manifold $\mathcal M^{\kappa, d}=\{\boldsymbol z \in \mathbb R^d  | -\kappa||\boldsymbol z ||_2^2 < 1\}$, whose origin is $\mathbf 0 \in \mathbb R^d$, equipped with a Riemannian metric $g_{\boldsymbol z}^\kappa=(\lambda_{\boldsymbol z}^\kappa)^2 \mathbf I$,  where $\lambda_{\boldsymbol z}^\kappa$ is given by $\lambda_{\boldsymbol z}^\kappa=2\left(1+\kappa||\boldsymbol z||_2^2\right)^{-1}.$  The Riemannian operations under this model are elaborated in the table below:f

\begin{table}[ht]
\small
\centering
\begin{tabular}{l|c|c}
\hline
\textbf{Operation} & \textbf{Formalism in $\mathbb E^d$} &\textbf{Unified formalism in $\kappa$-stereographic model ($\mathbb H^d$/ $\mathbb S^d$)}\\
\hline
Distance Metric& 
$d^\kappa_{\mathcal M}(\mathbf{x}, \mathbf{y}) =\left\| \mathbf{x}- \mathbf{y}\right\|_{2}$
&
$
d^\kappa_{\mathcal M}(\mathbf{x}, \mathbf{y})=\frac{2}{\sqrt{|\kappa|}} \tan _{\kappa}^{-1}\left(\sqrt{|\kappa|}\left\|-\mathbf{x} \oplus_{\kappa} \mathbf{y}\right\|_{2}\right)
$
\\
\hline
Exp. Map & 
$\mathbf{exp}_{\mathbf{x}}^{\kappa}(\mathbf{v})=\mathbf{x}+\mathbf{v}$ 
&
$
\mathbf{exp}_{\mathbf{x}}^{\kappa}(\mathbf{v})=\mathbf{x} \oplus_{\kappa}\left(\tan _{\kappa}\left(\sqrt{|\kappa|} \frac{\lambda_{\mathbf{x}}^{\kappa}\|\mathbf{v}\|_{2}}{2}\right) \frac{\mathbf{v}}{\sqrt{|\kappa|}\|\mathbf{v}\|_{2}}\right)
$
\\
Log. Map & 
$\mathbf{log}_{\mathbf{x}}^{\kappa}(\mathbf{y})= \mathbf{x}-\mathbf{y}$
&
$
\mathbf{log}_{\mathbf{x}}^{\kappa}(\mathbf{y})=\frac{2}{\sqrt{|\kappa|} \lambda_{\mathbf{x}}^{\kappa}} \tan _{\kappa}^{-1}\left(\sqrt{|\kappa|}\left\|-\mathbf{x} \oplus_{\kappa} \mathbf{y}\right\|_{2}\right) \frac{-\mathbf{x} \oplus_{\kappa} \mathbf{y}}{\left\|-\mathbf{x} \oplus_{\kappa} \mathbf{y}\right\|_{2}}
$
\\
\hline
Addition & 
$\mathbf{x} \oplus_{\kappa} \mathbf{y}=\mathbf{x} + \mathbf{y}$
&
$
\mathbf{x} \oplus_{\kappa} \mathbf{y}=\frac{\left(1+2 \kappa \mathbf{x}^{T} \mathbf{y}+K\|\mathbf{y}\|^{2}\right) \mathbf{x}+\left(1-\kappa || \mathbf{x}||^{2}\right) \mathbf{y}}{1+2 \kappa \mathbf{x}^{T} \mathbf{y}+\kappa^{2}|| \mathbf{x}||^{2}|| \mathbf{v}||^{2}}
$\\
\hline
\end{tabular}
\caption{Operations in Hyperbolic $\mathbb H^d$, Spherical $\mathbb S^d$ and Euclidean space $\mathbb E^d$.}
\label{tab:kops}
\end{table}

$\blacksquare$ \textbf{$\mathbf{\kappa}$-right-matrix-multiplication}. Given a matrix $\mathbf{X} \in \R^{n \times d}$ holding $\kappa$-stereographic embeddings in its rows and weights $\mathbf{W} \in \R^{d \times e}$, the Euclidean right multiplication can be written row-wise as $(\mathbf{X} \mathbf{W})_{i \bullet} = \mathbf{X}_{i\bullet} \mathbf{W}$. Then the $\mathbf{\kappa}$-right-matrix-multiplication is defined row-wise as
\begin{equation}
    \begin{split}
        (\mathbf{X} \otimes_{\kappa} \mathbf{W})_{i\bullet} &= \mathbf{exp}_{0}^{\kappa}\left((\mathbf{log}_{0}^{\kappa}(\mathbf{X})\mathbf{W})_{i\bullet}\right) = \tan_\kappa \left(\alpha_{i}\tan_\kappa^{-1}(||\mathbf{X}_{\bullet i}||)\right)\frac{(\mathbf{X}\mathbf{W})_{i\bullet}}{||(\mathbf{X}\mathbf{W})_{i\bullet}||}
    \end{split}
\end{equation}

where $\alpha_{i} = \frac{||(\mathbf{X}\mathbf{W})_{i\bullet}||}{||\mathbf{X}_{i\bullet}||}$ and $\mathbf{exp}_{0}^{\kappa}$ and $\mathbf{log}_{0}^{\kappa}$ denote the exponential and logarithmic map in the $\kappa$-stereo. model.

$\blacksquare$ \textbf{$\mathbf{\kappa}$-left-matrix-multiplication}. Given a matrix $\mathbf{X}\in\mathbb{R}^{n\times d}$ holding $\kappa$-stereographic embeddings in its rows and weights $\mathbf{A}\in\mathbb{R}^{n\times n}$, the \textbf{$\kappa$-left-matrix-multiplication} is defined row-wise as 
\begin{equation}
(\mathbf{A}\boxtimes_\kappa \mathbf{X})_{i\bullet} := (\sum_{j}A_{ij})\otimes_\kappa m_\kappa(\mathbf{X}_{1\bullet},\cdots,\mathbf{X}_{n\bullet}; \mathbf{A}_{i\bullet}).
\end{equation}

\subsubsection{Spectral Graph Theory}  \label{app:spectral_theory}
Graph Fourier Transform (\texttt{GFT}) \citep{sandryhaila2013discrete} lays the foundation for Graph Neural Networks (GNNs). A \texttt{GFT} is defined using a \textit{reference} operator $\mathbf{R}$ which admits a spectral decomposition. Traditionally, in the case of GNNs, this reference operator has been the symmetric normalized Laplacian $\mathbf{L}_n = \mathbf{I} - \mathbf{A}_n$ \citep{kipf2016semi}. The graph Fourier transform of a
signal $\mathbf{f} \in \mathbb{R}^n$ is then defined as $\hat{\mathbf{f}} = \mathbf{U}^{\top} \mathbf{f} \in \mathbb{R}^n$, and its inverse as $\mathbf{f} = \mathbf{U}\hat{\mathbf{f}}$. A graph filter is an operator that acts independently on the entire eigenspace of a diagonalisable and symmetric reference operator R, by modulating their corresponding eigenvalues. A graph filter is defined via the graph filter function $g(.)$ operating on the reference operator as $ g(\mathbf{R}) = \mathbf{U} g(\mathbf{\Lambda}) \mathbf{U}^{\top}$.


\subsection{More on \modelshort\ Laplacian}\label{app:Laplacian}
Spectral graph theory has shown significant progress in relating geometric characteristics of graphs to properties of spectrum of graph Laplacians and related matrices. Several variants of the graph Laplacian matrices have been shown to capture specific inductive biases for different tasks \citep{ko2023spectral, belkin2008discrete, jacobson2012cotangent}. 

\begin{proof}[Proof of Definition \ref{thm:Laplacian}] Say the function $\psi : V \to \mathbb{R}$ is defined on the vertex set V of the graph. Suppose $\psi$ describes a temperature distribution across a graph, where $\psi(x)$ is the temperature at vertex $x$. According to Newton's law of cooling \citep{he2024constrained}, the heat transferred from node $x$ to node $y$ is proportional to $\psi(x) - \psi(y)$ if nodes $x$ and $y$ are connected (if they are not connected, no heat is transferred). Consequently, the heat diffusion equation on the graph can be expressed as $\frac{d \psi}{dt} = -\beta \sum_y \mathbf{A}_{xy}(\psi(x) - \psi(y))$, where $\beta$ is a constant of proportionality and $\mathbf{A}$ denotes the adjacency matrix of the graph. Further insight can be gained by considering Fourier’s law of thermal conductance \citep{liu1990fourier}, which states that heat flow is inversely proportional to the resistance to heat transfer. \texttt{ORC} measures the transportation cost ($\mathbf{W}_1(:,:)$) between the neighborhoods of two nodes, reflecting the effort required to transport mass between these neighborhoods \citep{bauer2011ollivier}. We interpret this transportation cost as the \textit{resistance} between nodes. The vital takeaway here is that $-$ \textit{Heat flow between two nodes in a graph is influenced by the underlying Ollivier-Ricci curvature (\texttt{ORC}) distribution}. The diffusion rate is faster on an edge with positive  curvature (low resistance), and slower on an edge with negative curvature (high resistance). Thus, the ratio $\mathcal{R}^{res}_{xy} = \frac{\mathbf{W}_1(m_x, m_y)}{d_{\mathcal{G}}(x, y)}$ represents the \textit{resistance} from node $x$ to node $y$, i.e. $\frac{d \psi_{xy}}{dt} \propto \frac{1}{\mathcal{R}^{res}_{xy}}$. It can be observed that $\frac{1}{\mathcal{R}^{res}_{xy}} = \frac{d_{\mathcal{G}}(x, y)}{\mathbf{W}_1(m_x, m_y)} = \frac{1}{1 - \widetilde{\kappa}(x, y)}$ (From the definition of \texttt{ORC}) would tend to infinity when $\mathbf{W}_1(m_x, m_y) = 0$ (i.e. $\widetilde{\kappa}(x, y) = 1$). Thus, to ensure continuity, we create a new ratio as $\frac{1}{\mathcal{R}^{\bullet}_{xy}} = e^{-\mathcal{R}^{res}_{xy}} = e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}}$. Thus, we can modify the above heat flow equation as:
\vspace{-2mm}
\begin{align*}
\frac{d \psi}{dt} &= -\bar{\beta} \sum_y \frac{\mathbf{A}_{xy}(\psi(x) - \psi(y))}{\mathcal{R}^{\bullet}_{xy}} \quad (\text{Inversely proportional to $\mathcal{R}^{\bullet}_{xy}$}) \\
&= -\bar{\beta} \sum_y \mathbf{A}_{xy}(\psi(x) - \psi(y))\; e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}}
= -\bar{\beta} \left(\psi(x) \sum_{y}\mathbf{A}_{xy} - \sum_{y}\mathbf{A}_{xy}\psi(y))\right)\; e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}} \\
&= -\bar{\beta} \left( \psi(i) \mathbf{D}_{xx} - \sum_y \psi(y)\mathbf{A}_{xy}\right) \; e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}} \quad (\because \mathbf{D}_{xx} = \sum_{y}\mathbf{A}_{xy})\\
&= -\beta \sum_y \left( \delta_{xy} \;e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}} \mathbf{D}_{xx} - e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}}\;\mathbf{A}_{xy}\right)\psi(y) \quad (\delta_{xy} \text{ is the Kronecker delta.})\\
&= -\bar{\beta} \sum_y \left( e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}} \; \mathbf{L}_{xy}\right)\psi(y) \quad (\mathbf{L}, \mathbf{D}, \mathbf{A} \text{ are Laplacian, Degree and Adjacency matrices.})\\
&= -\bar{\beta} \left(\mathbf{L} \odot e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}}\right) \psi = -\bar{\beta} \widetilde{\mathbf{L}}\psi \quad(\odot \text{ is the element-wise product})\\
\vspace{-3mm}
\end{align*}
This gives us the standard heat equation on graphs. Here, $\bar{\beta}$ is the updated constant of proportionality. $\widetilde{\mathbf{L}} = \widetilde{\mathbf{D}}- \widetilde{\mathbf{A}}$ is the \textbf{\modelshort\ Laplacian} operator, where $\widetilde{\mathbf{D}}$ and $\widetilde{\mathbf{A}}$ are the updated Degree and Adjacency matrices, to represent that the graph is transformed under edge weights $w_{xy} = \frac{1}{\mathcal{R}^{\bullet}_{xy}} = e^{\frac{1}{1 - \widetilde{\kappa}(x, y)}}$. Finally, our \modelshort\ Laplacian operator can be written as ($x \sim y$ means $xy$ is an edge in the graph):
\begin{equation}
\widetilde{\mathbf{L}}\psi(x)= \sum_{y \sim x} \bar{w}_{xy} \left( \psi(x) - \psi(y) \right) = \sum_{y \sim x} e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}} \left( \psi(x) - \psi(y) \right)
\end{equation}
$\blacksquare$ \textbf{Why is $e^{-\mathcal{R}_{xy}^{res}} = e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}}$ the right choice?} 
To mathematically justify that  $e^{-\frac{1}{1 - \widetilde{\kappa}(x, y)}}$  is an appropriate choice, we must verify its properties:
\begin{enumerate}
    \item \textbf{Asymptotics}. As  $\widetilde{\kappa}(x, y) \to 1 ,  e^{-\frac{1}{1 - \widetilde{\kappa}(x, y)}} \to 0$ , indicating that nodes with high positive curvature experience very fast heat diffusion (minimal resistance). Conversely, as  $\widetilde{\kappa}(x, y) \to -1$ ,  $e^{-\frac{1}{1 - \widetilde{\kappa}(x, y)}} \to \frac{1}{\sqrt{e}}$ , meaning that nodes with high negative curvature have slow heat diffusion (higher resistance).
    \item \textbf{Continuity}. The exponential function is smooth and continuous, ensuring that even small changes in the curvature result in smooth changes in the heat flow dynamics, which is crucial for stable numerical simulations and theoretical consistency.
    \item \textbf{Monotonicity}. For $\widetilde{\kappa}(x, y) > 0 ,  e^{-\frac{1}{1 - \widetilde{\kappa}(x, y)}}$  is a decreasing function with respect to  $\widetilde{\kappa}(x, y)$. This means as curvature increases, the resistance decreases, aligning with the physical intuition of heat flow.
\end{enumerate}
\end{proof}
\subsubsection{Relevent Theorems for \modelshort\ Laplacian}
\begin{theorem}[Positive Semidefiniteness of \modelshort\ Laplacian]\label{thm:pos_sem} The normalized Laplacian operator $\widetilde{\mathbf{L}}$ is positive semidefinite, i.e., for any real vector $u \in \mathbb{R}^n$, we have $\mathbf{u}^T\widetilde{\mathbf{L}}_{\text{n}}\mathbf{u} \geq 0$.
\end{theorem} 

\begin{proof}

We start by showing that the normalized \textbf{\modelshort\ Laplacian}
\begin{align}\label{sup:eq1}
    \widetilde{\mathbf{L}}_{\text{n}} = \mathbf{I} - \widetilde{\mathbf{D}}^{-1/2}\widetilde{\mathbf{A}}\widetilde{\mathbf{D}}^{-1/2} = \mathbf{I}-\widetilde{\mathbf{A}}_{n}
\end{align}
is positive semi-definite. Let $\mathbf{u}$ be any real vector of unit norm and $\mathbf{f} = \widetilde{\mathbf{D}}^{-1/2}\mathbf{u}$, then we have
\begin{align}
    & \mathbf{u}^T\widetilde{\mathbf{L}}_{\text{n}}\mathbf{u} = \mathbf{u}^T\mathbf{u} - \mathbf{u}^T\widetilde{\mathbf{D}}^{-1/2}\widetilde{\mathbf{A}}\widetilde{\mathbf{D}}^{-1/2}\mathbf{u} = \sum_{x=1}^{n}\mathbf{u}_x^2 - \sum_{x,y=1}^{n}\mathbf{f}_x\mathbf{f}_y\widetilde{\mathbf{A}}_{xy}\\
    & = \sum_{x=1}^{n}\widetilde{\mathbf{D}}_{xx}\mathbf{f}_x^2 - \sum_{x,y=1}^{n}\mathbf{f}_x\mathbf{f}_y\widetilde{\mathbf{A}}_{xy} = \frac{1}{2}(\sum_{x=1}^{n}\widetilde{\mathbf{D}}_{xx}\mathbf{f}_x^2 - 2\sum_{x,y=1}^{n}\mathbf{f}_x\mathbf{f}_y\widetilde{\mathbf{A}}_{xy}+\sum_{y=1}^{n}\widetilde{\mathbf{D}}_{yy}\mathbf{f}_y^2)\\
    & = \frac{1}{2}\sum_{x,y=1}^{n}\widetilde{\mathbf{A}}_{xy}(\mathbf{f}_x-\mathbf{f}_y)^2 = \frac{1}{2}\sum_{x,y=1}^{n} e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}}{\mathbf{A}}_{xy}(\mathbf{f}_x-\mathbf{f}_y)^2,\label{sup:eq2} 
\end{align}
where the last step follows from the definition of the degree. We know that $ e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}}>0 \; \forall \kappa(x, y)$, hence our \modelshort\ Laplacian is positive semidefinite.    
\end{proof}

\begin{theorem}\label{thm:normalized_eigenvalue_bounds}
The eigenvalues $\{\widetilde{\lambda}_i\}_{i=1}^{n} $ of the normalized \modelshort\ Laplacian $\widetilde{\mathbf{L}}_{\text{n}}$ lie in the interval $[0, 2]$.
\end{theorem}

\begin{proof}
We begin by noting that Theorem \ref{thm:pos_sem} shows that the normalized \textbf{\modelshort\ Laplacian} $\widetilde{\mathbf{L}}_{\text{n}}$ has real, non-negative eigenvalues, meaning we need only to prove that the largest eigenvalue, denoted as $\lambda_n$, is less than or equal to 2. Before moving to that, we show that $0$ is indeed an eigenvalue of $\widetilde{\mathbf{L}}$ associated with the unit eigenvector $\boldsymbol{\tau}$ where $\boldsymbol{\tau} = \frac{\sqrt{\widetilde{\mathbf{D}}_{ii}}}{\sqrt{\sum_v\widetilde{\mathbf{D}}_{vv}}}$.

Let $\mathbf{1}$ be the all one vector. Then, a direct calculation reveals that
\begin{align}
    & \widetilde{\mathbf{L}}_{\text{sym}}\boldsymbol{\tau} = \boldsymbol{\tau} - \widetilde{\mathbf{D}}^{-1/2}\widetilde{\mathbf{A}}\widetilde{\mathbf{D}}^{-1/2}\boldsymbol{\tau} = \boldsymbol{\tau} - \widetilde{\mathbf{D}}^{-1/2}\widetilde{\mathbf{A}}\widetilde{\mathbf{D}}^{-1/2}\widetilde{\mathbf{D}}^{1/2}\mathbf{1}\times\frac{1}{\sqrt{\sum_v\widetilde{\mathbf{D}}_{vv}}}\\
    & = \boldsymbol{\tau} - \widetilde{\mathbf{D}}^{-1/2}\widetilde{\mathbf{A}}\mathbf{1}\times\frac{1}{\sqrt{\sum_v\widetilde{\mathbf{D}}_{vv}}} = \boldsymbol{\tau} - \widetilde{\mathbf{D}}^{-1/2}\widetilde{\mathbf{D}}\mathbf{1}\times\frac{1}{\sqrt{\sum_v\widetilde{\mathbf{D}}_{vv}}}\\
    & = \boldsymbol{\tau} - \widetilde{\mathbf{D}}^{1/2}\mathbf{1}\times\frac{1}{\sqrt{\sum_v\widetilde{\mathbf{D}}_{vv}}}= \boldsymbol{\tau} - \boldsymbol{\tau} = 0.
\end{align}
Combining this result with the positive semi-definite property of the Laplacian shows that $0$ is indeed the smallest eigenvalue of $\widetilde{\mathbf{L}}_{\text{sym}}$ associated with the eigenvector $\boldsymbol{\tau}$. For the second part, using the Courant-Fischer theorem, we know that the largest eigenvalue can be expressed as:
\[
    \lambda_n = \max_{\mathbf{u} \neq 0} \frac{\mathbf{u}^\top \widetilde{\mathbf{L}}_{\text{n}} \mathbf{u}}{\mathbf{u}^\top \mathbf{u}}.
\]
Substituting the definition of the normalized \modelshort\ Laplacian $\widetilde{\mathbf{L}}_{\text{n}} = \mathbf{I} - \widetilde{\mathbf{A}}_{\text{n}}$ into this expression, and letting $\mathbf{f} = \widetilde{\mathbf{D}}^{-1/2} \mathbf{u}$, we have:
\[
    \lambda_n = \max_{\mathbf{u} \neq 0} \frac{\mathbf{u}^\top \widetilde{\mathbf{D}}^{-1/2} \widetilde{\mathbf{L}} \widetilde{\mathbf{D}}^{-1/2} \mathbf{u}}{\mathbf{u}^\top \mathbf{u}} = \max_{\mathbf{f} \neq 0} \frac{\mathbf{f}^\top \widetilde{\mathbf{L}} \mathbf{f}}{\mathbf{f}^\top \widetilde{\mathbf{D}} \mathbf{f}}.
\]
The degree matrix, can be expressed in the quadratic form as $\mathbf{f}^\top \widetilde{\mathbf{D}} \mathbf{f} = \sum_{x=1}^{n} \widetilde{\mathbf{D}}_{xx} |\mathbf{f}_x|^2$.

For the numerator involving $\widetilde{\mathbf{L}}$, we expand the quadratic form:
\begin{align}
     \mathbf{f}^\top \widetilde{\mathbf{L}} \mathbf{f} &= \frac{1}{2}\sum_{x,y=1}^{n}\widetilde{\mathbf{A}}_{xy}(\mathbf{f}_x-\mathbf{f}_y)^2 = \frac{1}{2}\sum_{x,y=1}^{n} e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}}{\mathbf{A}}_{xy}(\mathbf{f}_x-\mathbf{f}_y)^2\\
    &\leq \sum_{x,y=1}^{n} e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}}{\mathbf{A}}_{xy}(\mathbf{f}_x + \mathbf{f}_y)^2 \leq 2 \sum_{x=1}^{n} |\mathbf{f}_x|^2 \left( \sum_{y=1}^{n} \mathbf{A}_{xy} \right) = 2 \sum_{x=1}^{n} \mathbf{D}_{xx} |\mathbf{f}(x)|^2.
\end{align}

The last inequality follows from the fact that $e^{\frac{-1}{1-\widetilde{\kappa}(x, y)}}\rightarrow \frac{1}{\sqrt{e}}$ as $\widetilde{\kappa}(x, y) \rightarrow -1$ implies that it is always $< 1$. Thus, we can conclude that, $\mathbf{f}^\top \widetilde{\mathbf{L}} \mathbf{f} \leq 2 \mathbf{f}^\top \widetilde{\mathbf{D}} \mathbf{f}$, and the Rayleigh quotient is bounded $\frac{\mathbf{f}^\top \widetilde{\mathbf{L}} \mathbf{f}}{\mathbf{f}^\top \widetilde{\mathbf{D}} \mathbf{f}} \leq 2$. This shows that the largest eigenvalue of the normalized \modelshort\ Laplacian \(\widetilde{\mathbf{L}}_{\text{n}}\) is bounded by 2, completing the proof that the eigenvalues of \(\widetilde{\mathbf{L}}_{\text{n}}\) are contained within the interval \([0, 2]\).
\end{proof}
    
\subsection{Generalised Pageranks and GPRGNN}\label{app:GPRGNN}



\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{imgs/gprgnn_figure.pdf}
    \caption{Architecture of GPRGNN.
    }
    \label{fig:enter-label}
\end{figure}


$\blacksquare$ \textbf{Equivalence of the GPR method and polynomial graph filtering. } If we truncate the infinite series in the GPR definition at some integer $K$, $\sum_{k=0}^{K}\gamma_k\widetilde{\mathbf{A}}_{\text{n}}^k$ becomes a polynomial graph filter of degree $K$. Consequently, optimizing the GPR weights is tantamount to optimizing the polynomial graph filter. It is important to note that any graph filter can be approximated using a polynomial graph filter, enabling the GPR method to handle a wide variety of node label patterns. Additionally, increasing $K$ enhances the approximation of the optimal graph filter. This again illustrates the advantage of large-step propagation.

$\blacksquare$ \textbf{GPRGNN architecture.} GPR-GNN initially derives hidden state features for each node and subsequently employs GPR to disseminate them. The GPR-GNN procedure can be represented as:
\begin{align}
    & \hat{\mathbf{P}}=\text{softmax}(\mathbf{Z}),\; \mathbf{Z} = \sum_{k=0}^{K}\gamma_k\mathbf{H}^{(k)},\;
    \mathbf{H}^{(k)} = \widetilde{\mathbf{A}}_{\text{sym}}\mathbf{H}^{(k-1)},\;\mathbf{H}^{(0)}_{i:} = f_{\theta}(\mathbf{X}_{i:}),
\end{align}
Here, $f_{\theta}(.)$ denotes a neural network parametrized by $\{{\theta\}}$, which produces the hidden state features $\mathbf{H}^{(0)}$. The GPR weights $\gamma_k$ are optimized alongside $\{{\theta\}}$ in an end-to-end manner. The GPR-GNN model is straightforward to interpret: As previously mentioned, GPR-GNN is capable of adaptively managing the contribution of each propagation step to fit the node label pattern. Analyzing the trained GPR weights also aids in understanding the topological properties of a graph, such as identifying the optimal polynomial graph filter.

\subsubsection{Proof of Theorem~\ref{thm:LPF}}
We first state the formal version of Theorem~\ref{thm:LPF}
\begin{theorem}[Formal version of Theorem~\ref{thm:LPF}]\label{thm:formal_LPF}
 Assume the graph $G$ is connected. Let $\lambda_1\geq \lambda_2\geq ...\geq \lambda_n$ and $\widetilde{\lambda}_1\leq \widetilde{\lambda}_2\leq ...\leq \widetilde{\lambda}_n$ be the eigenvalues of $\mathbf{\widetilde{A}}_{n}$ and $\mathbf{\widetilde{L}}_{n}$, respectively.
 If $\gamma_l\geq 0\;\forall l\in \{0,1,...,L\}$, $\sum_{l=0}^{L}\gamma_l=1$ and $\exists l'>0$ such that $\gamma_{l'} > 0$, then $\left|\frac{g_{\gamma,L}(\lambda_i)}{g_{\gamma,L}(\lambda_1)}\right| < 1\;\forall i\geq2$. Also, if $\gamma_l = (-\alpha)^l,\alpha\in(0,1)$ and $L \rightarrow \infty$, then $\left|\frac{\lim_{L\rightarrow\infty}g_{\gamma,L}(\lambda_i)}{\lim_{L\rightarrow\infty}g_{\gamma,L}(\lambda_1)}\right| > 1\;\forall i\geq2$.
\end{theorem}

\begin{enumerate}
    \item Note that $\left|\frac{g_{\gamma,L}(\lambda_i)}{g_{\gamma,L}(\lambda_1)}\right| < 1\;\forall i\geq2$ implies the \textbf{low-pass case} as after applying the graph filter $g_{\gamma,L}$, the lowest frequency component (correspond to $\lambda_1$) further dominates. 
    \item \textbf{Unfiltered case}. Recall that in the unfiltered case, we do not multiply with $\mathbf{\widetilde{A}}_{\text{n}}$. It can also be viewed as multiplying the identity matrix $I$, where the eigenvalue ratio is $\frac{|\lambda_i|^0}{|\lambda_1|^0}=1$. Hence $g_{\gamma,L}$ acts like a low pass filter in this case.
    \item In contrast, $\left|\frac{\lim_{L\rightarrow\infty}g_{\gamma,L}(\lambda_i)}{\lim_{L\rightarrow\infty}g_{\gamma,L}(\lambda_1)}\right| > 1\;\forall i\geq2$ implies that after applying the graph filter, the lowest frequency component (correspond to $\lambda_1$) no longer dominates. This corresponds to the \textbf{high pass filter} case.
\end{enumerate}





\begin{proof}[\textbf{Proof}]
 We start with the\textbf{ low pass filter result}. From Theorem \ref{thm:normalized_eigenvalue_bounds}, we know that $0 \leq \widetilde{\lambda}_1\leq \widetilde{\lambda}_2\leq ...\leq \widetilde{\lambda}_n \leq 2$. Given the spectrum of $\widetilde{\mathbf{A}}_n$, we know that $-\widetilde{\mathbf{A}}_n$ has
spectrum negatives of $\widetilde{\mathbf{A}}_n$, and $\mathbf{I} -\widetilde{\mathbf{A}}_n$ adds one to each eigenvalue of $-\widetilde{\mathbf{A}}_n$ . Hence, $1 \geq \lambda_1\geq \lambda_2\geq ...\geq \lambda_n \geq -1$ follows directly. Now, we know that $\lambda_1 = 1$ and $|\lambda_i|<1,\forall i\geq 2$. Further, we have assumed that $g_{\gamma,L}(\lambda_1) = \sum_{l=0}^{L} \gamma_l  = 1$. Hence, proving Theorem~\ref{thm:formal_LPF} is equivalent to show
\begin{align*}
    |g_{\gamma,L}(\lambda_i)| < 1\;\forall i\geq2.
\end{align*}
This is obvious since $g_{\gamma,L}(\lambda) = \sum_{l=0}^{L}\gamma_l\lambda^l$ is a polynomial of order $L$ with nonnegative coefficients. It is easy to check that $\forall l\geq 1,\;|\lambda|^l < 1,\forall |\lambda|<1$. Combine with the fact that all $\gamma_k$'s are nonnegative we have
\begin{align*}
    |g_{\gamma,L}(\lambda_i)|\leq \sum_{l=0}^L \gamma_l |\lambda^l| = \sum_{l=0}^L \gamma_l |\lambda|^l \stackrel{\text{(a)}}{\leq} \sum_{l=0}^L \gamma_l = 1.
\end{align*}
Finally, note that the only possibility that the equality holds is $\gamma_l = \delta_{0,l}$ since $\forall l\geq 1,\;|\lambda|^l < 1,\forall |\lambda|<1$. However, by assumption $\sum_{l=0}^{L}\gamma_l=1$ and $\exists l'>0$ such that $\gamma_{l'} > 0$ we know that this is impossible. Hence (a) is a strict inequality $<$. 

For the \textbf{high pass filter result}, it can be observed that:
\begin{align*}
    \lim_{L\rightarrow\infty}g_{\gamma,L}(\lambda) = \lim_{L\rightarrow\infty}\sum_{l=0}^{L}\gamma_l \lambda^l = \lim_{L\rightarrow\infty}\sum_{l=0}^{L} (-\alpha\lambda)^l = \frac{1}{1+\alpha\lambda},
\end{align*}
where the last step is due to the fact that $\alpha\in(0,1)$ and thus $\lim_{L\rightarrow\infty}(-\alpha\lambda)^L = 0,\forall |\lambda|\leq 1$. Thus we have
\begin{align*}
    \left|\frac{\lim_{L\rightarrow\infty}g_{\gamma,L}(\lambda_i)}{\lim_{L\rightarrow\infty}g_{\gamma,L}(\lambda_1)}\right| = \left|\frac{1+\alpha}{1+\alpha\lambda_i}\right| \stackrel{\text{(b)}}{>} 1 \;\forall i\geq2.
\end{align*}
The strict inequalities (b) is from the fact that $|\lambda_i|<1,\forall i\geq 2$. Notably, $\sup_{\lambda\in [1,-1)}\frac{1}{1+\alpha\lambda}$ happens at the boundary $\lambda = -1$, which corresponds the the bipartite graph. It further shows that the graph filter with respect to the choice $\gamma_l = (-\alpha)^l$ emphasizes high frequency components and thus it is indeed acting as a high pass filter.
\end{proof}

\subsubsection{Why GPRGNN as the backbone of \modelname?}

In this section, we elaborate on why GPR is an ideal backbone when compared to other Spectral GNNs.

\begin{enumerate}
    \item \textbf{Adaptive Filter Design}. GPR learns the filter coefficients directly, allowing the spectral response to adapt to the task and dataset. This flexibility is critical for modeling both homophilic and heterophilic graphs.
    \item \textbf{Universality}. Unlike fixed low-pass filters like ChebNet, which excel primarily in homophilic settings, GPR’s learnable filters enable it to balance low-pass and high-pass components, making it suitable for both homophilic and heterophilic graphs. This is one of the main goals of our paper - to achieve superior performance on homophilic and heterophilic tasks. Fixed polynomial filters in ChebNet and Bernstein-based methods approximate spectral responses up to a fixed order, limiting their ability to model complex spectral properties. 
    \item \textbf{GPRGNN escapes oversmoothing}. GPR weights are adaptively learnable, which allows GPR-GNN to avoid over-smoothing and trade node and topology feature informativeness. See Section 4 of \cite{chien2020adaptive} for more theoretical analysis on the same and proofs, which is beyond the scope of this work. GPR not only mitigates feature over-smoothing, but also works on highly diverse node label patterns (See Section 4 and 5 of \cite{chien2020adaptive}).
    \item \textbf{Capturing node features and graph topology}. In many important graph data processing applications, the acquired information includes both node features and observations of the graph topology. GPRGNN jointly optimizes node feature and topological information extraction, regardless of the extent to which the node labels are homophilic or heterophilic.
    \item \textbf{Filter Bank Construction}. Using GPR based spectral filters, helps us to effectively construct a filter bank where each adaptive filter contributes to a specific spectral profile, enabling the model to aggregate information across different spectral bands. This approach captures diverse patterns in node features and topology, unlike ChebNet or Bernstein-based methods, which rely on fixed polynomial approximations and lack such flexibility.
\end{enumerate}	


\subsection{Theorems for Curvature Encoding} \label{app:functional}
\begin{theorem}[Bochner's Theorem]
\citep{moeller2016continuous} A continuous, translation-invariant kernel $\mathcal{K}(\mathbf{x},\mathbf{y})=\Psi(\mathbf{x}-\mathbf{y})$ on $\mathbb{R}^d$ is positive definite if and only if there exists a non-negative measure on $\mathbb{R}$ such that $\Psi$ is the Fourier transform of the measure.
\end{theorem}
\subsubsection{Proof of Definition \ref{thm:curve_1}}
\begin{proof}[\textbf{Proof}]
    
Using the Bochner's theorem stated above, our curvature kernel $\mathcal{K}_{\mathbb{R}}$ has the expression:
\begin{equation}
\label{eqn:bochner}
    \mathcal{K}_{\mathbb{R}}(\widetilde{\kappa}_a, \widetilde{\kappa}_b) = \Psi_{\mathbb{R}}(\widetilde{\kappa}_a, \widetilde{\kappa}_a) = \int_{\mathbb{R}}e^{i\omega(\widetilde{\kappa}_a - \widetilde{\kappa}_b)}p(\omega)d\omega = \E_{\omega}[\xi_{\omega}(\widetilde{\kappa}_a) \xi_{\omega}(\widetilde{\kappa}_b)^*],
\end{equation}
where $\xi_{\omega}(\widetilde{\kappa}) = e^{i\omega \widetilde{\kappa}}$. Since kernel $\mathcal{K}_{\mathbb{R}}$ and measure $p(\omega)$ are real, we extract the real part of (\ref{eqn:bochner}):
\begin{equation}
\label{eqn:bochner-mapping}
    \mathcal{K}_{\mathbb{R}}(\widetilde{\kappa}_a,\widetilde{\kappa}_b) = \E_{\omega}\big[\cos(\omega (\widetilde{\kappa}_a - \widetilde{\kappa}_b))\big]=\E_{\omega}\big[\cos(\omega \widetilde{\kappa}_a)\cos(\omega \widetilde{\kappa}_b) + \sin(\omega \widetilde{\kappa}_a)\sin(\omega \widetilde{\kappa}_b) \big].
\end{equation}

The above formulation suggests approximating the expectation by the Monte Carlo integral, i.e. $\mathcal{K}_{\mathbb{R}}(\widetilde{\kappa}_a,\widetilde{\kappa}_b) \approx \frac{1}{d_{\mathcal{C}}}\sum_{i=1}^{d_{\mathcal{C}}}\cos(\omega_i \widetilde{\kappa}_a)\cos(\omega_i \widetilde{\kappa}_b) + \sin(\omega_i \widetilde{\kappa}_a)\sin(\omega_i \widetilde{\kappa}_b)$, with $\omega_1,\ldots,\omega_{d_{\mathcal{C}}} \stackrel{\text{i.i.d}}{\sim} p(\omega)$.
% \[
% \mathcal{K}(\kappa_a,\kappa_b) \approx \frac{1}{d}\sum_{i=1}^d\cos(\omega_i \kappa_a)\cos(\omega_i \kappa_b) + \sin(\omega_i \kappa_a)\sin(\omega_i \kappa_b), \, \text{with } \; \omega_1,\ldots,\omega_d \stackrel{\text{i.i.d}}{\sim} p(\omega).
% \]
Therefore, we propose the finite dimensional functional mapping to $\mathbb{R}^{d_{\mathcal{C}}}$ as: 
\begin{equation}
\Phi_{\mathbb{R}^{d_{\mathcal{C}}}}(\widetilde{\kappa}) = \sqrt{\frac{1}{d_{\mathcal{C}}}} \Big[\cos(\omega_1 \widetilde{\kappa}), \sin(\omega_1 \widetilde{\kappa}), \dots, \cos(\omega_{d_{\mathcal{C}}} \widetilde{\kappa}), \sin(\omega_{d_{\mathcal{C}}} \widetilde{\kappa}) \Big]
\end{equation} 
The unknown probability distribution $p(\omega)$ is estimated using the inverse cumulative distribution function (CDF) transformation as in \cite{xu2020inductive}. Since our GNN is operating in the mixed-curvature space, we must map our defined curvature kernel based representations to the product manifold. We do so using the exponential map, for a node $x$ with \texttt{ORC} curvature $\widetilde{\kappa}(x)$ as: 
\begin{align}
    \Phi_{\mathbb{P}^{d_{\mathcal{C}}}}(\widetilde{\kappa}(x)) &= g_{\theta}\Big(\|_{q=1}^{\mathcal{Q}}\mathbf{exp}^{\kappa_{(q)}}_{\mathbf{0}}(\Phi_{\mathbb{R}^{d_{\mathcal{C}}}}(\widetilde{\kappa}(x)))\Big)\\
    &= g_{\theta}\Big(\|_{q=1}^{\mathcal{Q}} \Phi_{\mathcal{M}_{q}^{\kappa_{(q)}, d_{(q)}}}(\widetilde{\kappa}(x))\Big)
\end{align}
where $\mathbf{exp}^{\kappa_{(q)}}_{\mathbf{0}}:\mathbb{R}^{d_{\mathcal{C}}}\rightarrow\mathcal{M}_{q}^{\kappa_{(q)}, d_{(q)}}$ denotes the exponential map on the $q^{th}$ component manifold with curvature $\kappa_{(q)}$, $||$ is the concatenation operator and $g_{\theta}: \mathbb{P}^{d_{f}}\rightarrow \mathbb{P}^{d_{\mathcal{C}}}$ is a Riemannian projector. We need $g_{\theta}$ because we maintain a single product manifold for \modelname\, with total dimension $d_f$. So, upon taking the exponential map with respect to this product manifold, we are required to project the curvature embeddings to the required dimension $d_\mathcal{C}$.
\end{proof}

\subsubsection{Proof of Theorem \ref{thm:curve_2}}
\begin{proof}[\textbf{Proof}]
We begin by recalling that in Euclidean space, the curvature kernel $\mathcal{K}_{\mathbb{R}}$ is:
\[
\mathcal{K}_{\mathbb{R}}(\widetilde{\kappa}_a, \widetilde{\kappa}_b) = \big\langle \Phi_{\mathbb{R}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_a), \Phi_{\mathbb{R}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_b) \big\rangle = \Psi_{\mathbb{R}}(\widetilde{\kappa}_a - \widetilde{\kappa}_b).
\]
The key property here is translation invariance:
\[
\mathcal{K}_{\mathbb{R}}(\widetilde{\kappa}_a + \widetilde{c}, \widetilde{\kappa}_b + \widetilde{c}) = \mathcal{K}_{\mathbb{R}}(\widetilde{\kappa}_a, \widetilde{\kappa}_b) = \Psi_{\mathbb{R}}(\widetilde{\kappa}_a - \widetilde{\kappa}_b).
\]
Next, we move to the product manifold $\mathbb{P}^{d_{\mathcal{C}}}$, which consists of multiple components of different curvatures, such as hyperbolic, spherical, and Euclidean spaces. 

For each component manifold $\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}$ with curvature $\kappa_{(q)}$, the stereographic inner product $\langle .,. \rangle_{\mathbf{x}}^{\kappa}:\mathcal{T}_x\mathcal{M}^{n}_{\kappa} \times \mathcal{T}_x\mathcal{M}^{n}_{\kappa} \rightarrow \mathbb{R}$, is defined on the tangent plane of the Riemannian manifold as:
\[
\langle \mathbf{u}, \mathbf{v} \rangle^{\kappa}_{\mathbf{x}} = \mathbf{u}^T \mathbf{g}^{\kappa}_{\mathbf{x}} \mathbf{v} = \left( \lambda^{\kappa}_{\mathbf{x}} \right)^2 \langle \mathbf{u}, \mathbf{v} \rangle,
\]
where the conformal factor $\lambda^\kappa_{\mathbf{x}}$ is defined as:
\[
\lambda^\kappa_{\mathbf{x}} = \frac{2}{1 + \kappa \|\mathbf{x}\|_2^2}.
\]
This conformal factor modulates the stereographic projection in the curved space, and it ensures that distances are mapped correctly in the manifold space. Now, consider the inner product between the curvature embeddings $\Phi_{\mathbb{P}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_a)$ and $\Phi_{\mathbb{P}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_b)$ in the mixed-curvature space.
\[
\mathcal{K}_{\mathbb{P}}(\widetilde{\kappa}_a, \widetilde{\kappa}_b) = \sum_{q=1}^{\mathcal{Q}} \langle \Phi_{\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}}(\widetilde{\kappa}_a), \Phi_{\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}}(\widetilde{\kappa}_b) \rangle_{\kappa_{(q)}},
\]
where each component manifold $\mathcal{M}_q^{\kappa_{(q)}, d_{(q)}}$ contributes to the overall inner product in the product manifold $\mathbb{P}^{d_{\mathcal{C}}}$. Using the stereographic inner product in each component, we can write:
\[
\mathcal{K}_{\mathbb{P}}(\widetilde{\kappa}_a, \widetilde{\kappa}_b) = \sum_{q=1}^{\mathcal{Q}} \left( \lambda^{\kappa_{(q)}}_{\mathbf{x}} \right)^2 \langle \Phi_{\mathbb{R}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_a), \Phi_{\mathbb{R}^{d_{\mathcal{C}}}}(\widetilde{\kappa}_b) \rangle.
\]
We now need to show that translation invariance holds in the mixed-curvature product manifold. Since the conformal factor $\lambda^\kappa_{\mathbf{x}}$ depends only on the norm $\|\mathbf{x}\|_2$, any translation by a constant \(\widetilde{c}\) does not affect the relative difference between curvature embeddings. Specifically, for any constant shift $\widetilde{c}$:
\[
\mathcal{K}_{\mathbb{P}}(\widetilde{\kappa}_a + \widetilde{c}, \widetilde{\kappa}_b + \widetilde{c}) = \sum_{q=1}^{\mathcal{Q}} \left( \lambda^{\kappa_{(q)}}_{\mathbf{x}} \right)^2 \Psi_{\mathbb{R}}((\widetilde{\kappa}_a + \widetilde{c}) - (\widetilde{\kappa}_b + \widetilde{c})) = \Psi_{\mathbb{P}}(\widetilde{\kappa}_a - \widetilde{\kappa}_b).
\]

Thus, the kernel in the mixed-curvature space remains invariant to translation, completing the proof.
\end{proof}


\subsection{Experimentation}
\subsubsection{Datasets}\label{app:datasets}
The performance of \modelname\ is evaluated over eight benchmark datasets for two primary tasks: Node Classification (\texttt{NC}) and Link Prediction (\texttt{LP}). These datasets encompass both homophilic and heterophilic domains. Detailed descriptions of each dataset are provided below.


\begin{enumerate}
    \item \textit{Citation Networks}. \textbf{Cora}, \textbf{PubMed}, and \textbf{Citeseer} \citep{sen2008collective, yang2016revisiting} are citation networks in which nodes symbolize research papers, and edges denote citation links between them. Each node is labeled with its subject category. This dataset is commonly utilized for node classification because of its pronounced \textit{homophilic} properties.
    \item \textit{Wikipedia graphs}. \textbf{Chameleon} and \textbf{Squirrel} \citep{rozemberczki2021multi} are \textit{heterophilic} graphs derived from Wikipedia articles. Nodes represent articles, and edges represent hyperlinks between them. Node labels correspond to website traffic levels.
    \item \textit{Actor Co-occurrence Network}. \textbf{Actor} \citep{tang2009social} is a heterophilic graph dataset where nodes depict actors and edges signify co-occurrences on the same Wikipedia page. The node labels correspond to the professional background of the actors.
    \item \textit{Webpage graphs}. \textbf{Texas} and \textbf{Cornell}\footnote{\url{http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb}} are parts of the WebKB dataset, and are sparsely connected heterophilic graphs. Here, nodes represent web pages, and edges represent hyperlinks between them. Labels reflect different types of webpages.
\end{enumerate}

\subsubsection{Baselines}\label{app:baselines}

This part offers an in-depth discussion of the baseline models against which \modelname\ is compared. We classify the baseline models into three categories: \textbf{Spatial}, \textbf{Riemannian}, and \textbf{Spectral} methods, which each address a distinct facet of graph neural network architectures.


$\blacksquare$ \textbf{Spatial baselines}. The first kind of baselines includes the traditional spatial methods, which directly operate on the node features and their immediate neighborhoods.
    \begin{enumerate}
        \item \textbf{GCN} \citep{kipf2016semi}. Graph Convolutional Networks (GCNs) represent one of the foundational graph neural networks that utilize spectral graph convolution in the spatial domain. They derive node embeddings by combining features from neighboring nodes via a linear combination involving the adjacency matrix and the nodes' features. 
        \vspace{1mm}
        \item \textbf{GAT} \citep{velivckovic2017graph}. Graph Attention Network (GAT) introduces attention mechanisms to graph neural networks. Each node assigns learnable attention weights to its neighbors and aggregates their features based on these weights. 
        \vspace{1mm}
        \item \textbf{GraphSAGE} \citep{hamilton2017inductive}. GraphSAGE is an inductive technique designed to learn node embeddings by sampling and aggregating features from a \textit{fixed} set of neighboring nodes, instead of processing the entire graph. This method enables GraphSAGE to create embeddings for nodes not encountered during training by using efficient neighborhood sampling.
        \vspace{1mm}
        
    \end{enumerate}

$\blacksquare$ \textbf{Riemannian Baselines}. Riemannian models function within non-Euclidean spaces (such as hyperbolic or spherical manifolds) and are tailored for graph data characterized by intricate geometric properties (e.g. hierarchical or cyclic structures).
    \begin{enumerate}
        \item \textbf{HGCN} \citep{chami2019hyperbolic}. Hyperbolic Graph Convolutional Networks utilize \textit{hyperbolic geometry} to represent the hierarchical and tree-like characteristics of graphs. This approach extends GCN to hyperbolic space by introducing a hyperbolic variant of the convolutional operation. It is especially suitable for datasets that exhibit hierarchical or tree-like configurations.
 \vspace{1mm}
 \item \textbf{HGAT} \citep{zhang2021hyperbolic}. Hyperbolic Graph Attention Network (HGAT) extends Graph Attention Networks (GAT) into hyperbolic space by integrating attention mechanisms with hyperbolic geometry, and calculates the attention weights among the nodes in the hyperbolic space to enhance the aggregation of features.
  \vspace{1mm}
  \item \textbf{$\kappa$GCN} \citep{bachmann2020constant}. $\kappa$GCN allows for learning the curvature of each node in a graph and generalizes GCN to operate in mixed-curvature spaces. The curvature parameter $\kappa$ determines whether a node lies in hyperbolic, spherical, or Euclidean space. By learning curvature adaptively, $\kappa$GCN offers flexibility in modeling graphs with regions of different geometries, providing a better fit for graphs with complex structures.
  \vspace{1mm}
  \item \textbf{$\mathcal{Q}$GCN} \citep{xiong2022pseudo}. Pseudo-Riemannian GCN extends a GCN to a pseudo-Riemannian manifold, enabling functionality in mixed-curvature spaces. This network is capable of modeling graph regions with both positive and negative curvature.
  \vspace{1mm}
  \item \textbf{SelfMGNN} \citep{sun2022self}. SelfMGNN generates embeddings within a mixed-curvature space through self-supervision. It dynamically allocates varied curvatures to different regions of the graph by utilizing a mixed-curvature embedding space. This approach incorporates both self-supervision and mixed-curvature learning to improve performance on heterogeneous graphs.
    \end{enumerate}

$\blacksquare$ \textbf{Spectral Baselines}. These techniques utilize the eigenvalues of either the graph Laplacian or adjacency matrix to establish convolutional filters that function in the frequency domain.
\begin{enumerate}
    \item \textbf{ChebyNet} \citep{defferrard2016convolutional}. ChebyNet implements spectral convolutions through a polynomial approximation of the graph Laplacian, sidestepping the expensive process of eigenvalue decomposition. Instead, it approximates the convolution using Chebyshev polynomials. This approach allows ChebyNet to execute localized graph convolutions efficiently, making it well-suited for handling larger graphs.
 \vspace{1mm}
 \item \textbf{BernNet} \citep{he2021bernnet}. BernNet employs Bernstein polynomials to approximate graph filters, providing flexible management over the filter's frequency response. This method extends polynomial-based graph filters and is adaptable to various frequency elements in graphs.
\vspace{1mm}
 \item \textbf{GPRGNN} \citep{chien2020adaptive}. The Generalized PageRank Graph Neural Network (GPRGNN) builds upon the Personalized PageRank (PPR) approach, integrating it into the framework of graph neural networks. It propagates node features through the graph by using a weighted sum of adjacency matrix powers, dynamically adjusting to both homophilic and heterophilic graphs.
 
 \item \textbf{FiGURe} \citep{ekbote2024figure}. FiGURe employs adaptive filters to capture various sections of the graph spectrum, enabling it to learn both high-pass and low-pass filters specific to the task. It dynamically selects the optimal filter bank to accurately represent the graph's architecture.
\end{enumerate}
\subsubsection{Experimental Results for Link Prediction}\label{app:lp}


\begin{table}[ht]
    \label{tab:results_lp}
    \centering
    \vspace{-4mm}
    \setlength{\tabcolsep}{6pt}
    \vspace{0.2cm}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{@{}lccccccccc@{}}
            \toprule
            \textbf{Baseline}        & Cora                      & Citeseer                  & PubMed                    & Chameleon                 & Actor                     & Squirrel                  & Texas                     & Cornell                   & \textbf{Av. $\Delta$ Gain} \\ \midrule
            GCN             & 88.54\scriptsize{$\pm$0.51} & 85.42\scriptsize{$\pm$0.89} & 91.31\scriptsize{$\pm$0.73} & 86.07\scriptsize{$\pm$0.64} & 85.12\scriptsize{$\pm$0.78} & 90.01\scriptsize{$\pm$0.15} & 69.08\scriptsize{$\pm$0.99} & 73.09\scriptsize{$\pm$0.92} & 9.58 \\
            GAT             & 85.45\scriptsize{$\pm$0.66} & 87.23\scriptsize{$\pm$0.11} & 87.65\scriptsize{$\pm$0.04} & 88.99\scriptsize{$\pm$0.13} & 87.33\scriptsize{$\pm$0.08} & 90.23\scriptsize{$\pm$0.14} & 68.79\scriptsize{$\pm$0.72} & 75.12\scriptsize{$\pm$0.77} & 9.31 \\
            SAGE            & 87.12\scriptsize{$\pm$0.82} & 90.71\scriptsize{$\pm$0.65} & 90.09\scriptsize{$\pm$0.90} & 90.01\scriptsize{$\pm$0.58} & 86.06\scriptsize{$\pm$0.73} & 91.02\scriptsize{$\pm$0.61} & 76.54\scriptsize{$\pm$0.69} & 77.98\scriptsize{$\pm$0.88} & 6.97 \\ \midrule
            HGCN            & 91.63\scriptsize{$\pm$0.55} & \color{Magenta}94.13\scriptsize{$\pm$0.67} & 91.04\scriptsize{$\pm$0.79} & 91.45\scriptsize{$\pm$0.62} & 90.01\scriptsize{$\pm$0.80} & 92.34\scriptsize{$\pm$0.01} & 69.99\scriptsize{$\pm$0.84} & 74.03\scriptsize{$\pm$0.57} & 6.34 \\
            HGAT            & 90.43\scriptsize{$\pm$0.03} & 91.02\scriptsize{$\pm$0.16} & 88.99\scriptsize{$\pm$0.89} & 89.77\scriptsize{$\pm$0.02} & 90.99\scriptsize{$\pm$0.01} & 89.22\scriptsize{$\pm$0.04} & 71.58\scriptsize{$\pm$0.89} & 72.03\scriptsize{$\pm$0.22} & 7.66 \\
            $\kappa$GCN     & \color{Magenta}92.04\scriptsize{$\pm$0.70} & 93.33\scriptsize{$\pm$0.57} & \color{Magenta}92.45\scriptsize{$\pm$0.85} & 92.03\scriptsize{$\pm$0.63} & 90.45\scriptsize{$\pm$0.88} & 91.35\scriptsize{$\pm$0.60} & 76.09\scriptsize{$\pm$0.76} & 73.05\scriptsize{$\pm$0.71} & 5.56 \\
            $\mathcal{Q}$GCN& \color{Magenta}92.17\scriptsize{$\pm$0.79} & 92.75\scriptsize{$\pm$0.52} & 92.16\scriptsize{$\pm$0.09} & 91.67\scriptsize{$\pm$0.05} & \color{Magenta}91.07\scriptsize{$\pm$0.06} & 90.98\scriptsize{$\pm$0.92} & 75.44\scriptsize{$\pm$0.10} & 73.89\scriptsize{$\pm$0.26} & 5.65 \\
            SelfMGNN        & \color{Green}93.12\scriptsize{$\pm$0.04} & 92.99\scriptsize{$\pm$0.91} & 90.99\scriptsize{$\pm$0.17} & \color{Green}93.51\scriptsize{$\pm$0.14} & 91.98\scriptsize{$\pm$0.19} & \color{Magenta}95.01\scriptsize{$\pm$0.16} & 74.51\scriptsize{$\pm$0.62} & 78.99\scriptsize{$\pm$0.81} & 4.28 \\ \midrule
            ChebyNet        & 88.23\scriptsize{$\pm$0.85} & 89.22\scriptsize{$\pm$0.06} & 86.54\scriptsize{$\pm$0.29} & 90.01\scriptsize{$\pm$0.23} & 88.09\scriptsize{$\pm$0.44} & 92.13\scriptsize{$\pm$0.57} & 73.45\scriptsize{$\pm$0.01} & 79.01\scriptsize{$\pm$0.18} & 7.33 \\
            BernNet         & 86.34\scriptsize{$\pm$0.13} & 87.09\scriptsize{$\pm$0.60} & 85.34\scriptsize{$\pm$0.82} & 87.15\scriptsize{$\pm$0.37} & 87.22\scriptsize{$\pm$0.15} & 91.22\scriptsize{$\pm$0.55} & \color{Green}77.65\scriptsize{$\pm$0.87} & 78.34\scriptsize{$\pm$0.19} & 8.12 \\
            GPRGNN          & 91.16\scriptsize{$\pm$0.72} & 93.05\scriptsize{$\pm$0.81} & 92.03\scriptsize{$\pm$0.01} & 91.22\scriptsize{$\pm$0.16} & 89.76\scriptsize{$\pm$0.62} & 92.34\scriptsize{$\pm$0.23} & 76.05\scriptsize{$\pm$0.18} & \color{Magenta}80.04\scriptsize{$\pm$0.12} & 4.96 \\
            FiGURe          & \color{Green}91.98\scriptsize{$\pm$0.69} & \color{Green}94.33\scriptsize{$\pm$0.15} & \color{Green}92.67\scriptsize{$\pm$0.83} & \color{Magenta}93.09\scriptsize{$\pm$0.31} & 90.11\scriptsize{$\pm$0.29} & \color{Green}95.43\scriptsize{$\pm$0.65} & \color{Magenta}76.99\scriptsize{$\pm$0.16} & \color{Green}80.12\scriptsize{$\pm$0.58} & 3.82 \\ \midrule
            \modelname          & \textbf{95.08\scriptsize{$\pm$0.13}} & \textbf{96.88\scriptsize{$\pm$0.65}} & \textbf{96.01\scriptsize{$\pm$0.01}} & \textbf{97.66\scriptsize{$\pm$0.33}} & \textbf{96.04\scriptsize{$\pm$0.38}} & \textbf{97.17\scriptsize{$\pm$0.11}} & \textbf{81.23\scriptsize{$\pm$0.14}} & \textbf{85.23\scriptsize{$\pm$0.05}} & 0 \\ 
            \textbf{Imp. $\Delta$} & 1.96&  2.55	&3.34	&4.15	&4.06	&1.74&	3.58&	5.11                      &   \\ \bottomrule
        \end{tabular}
    }
    \caption{Performance comparision of \modelname\ with baselines for \texttt{LP} task (Mean AUC Score $ \pm \; 95\%$ confidence interval). \textbf{First}, {\color{Green}Second} and {\color{Magenta}Third} best performing models are highlighted.}
    \label{exp:results_edges}\vspace{-5mm}
\end{table}


\begin{table}[ht]
 \centering
    \setlength{\tabcolsep}{3pt}
    \resizebox{0.7\textwidth}{!}{
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
Dataset     & $\modelname$                         & $\modelname_{{euc}}$        & $\modelname_{{lap}}$        & $\modelname_{{enc}}$        & $\modelname_{{pool}}$       & $\modelname_{{fil}}$ \\ \midrule 
Cora        & \textbf{95.08\scriptsize{$\pm$0.13}} & 92.45\scriptsize{$\pm$0.25} & 93.21\scriptsize{$\pm$0.42} & 93.78\scriptsize{$\pm$0.33} & 92.13\scriptsize{$\pm$0.40} & 93.02\scriptsize{$\pm$0.27} \\
Citeseer    & \textbf{96.88\scriptsize{$\pm$0.65}} & 94.03\scriptsize{$\pm$0.30} & 95.34\scriptsize{$\pm$0.22} & 94.08\scriptsize{$\pm$0.29} & 94.01\scriptsize{$\pm$0.31} & 94.56\scriptsize{$\pm$0.26} \\
PubMed      & \textbf{96.01\scriptsize{$\pm$0.01}} & 93.52\scriptsize{$\pm$0.60} & 94.81\scriptsize{$\pm$0.40} & 94.92\scriptsize{$\pm$0.38} & 94.11\scriptsize{$\pm$0.35} & 94.16\scriptsize{$\pm$0.39} \\
Chameleon   & \textbf{97.66\scriptsize{$\pm$0.33}} & 95.02\scriptsize{$\pm$0.44} & 96.23\scriptsize{$\pm$0.52} & 96.13\scriptsize{$\pm$0.28} & 95.13\scriptsize{$\pm$0.40} & 95.67\scriptsize{$\pm$0.57} \\
Actor       & \textbf{96.04\scriptsize{$\pm$0.38}} & 91.45\scriptsize{$\pm$0.55} & 93.99\scriptsize{$\pm$0.32} & 92.81\scriptsize{$\pm$0.42} & 91.98\scriptsize{$\pm$0.47} & 92.13\scriptsize{$\pm$0.49} \\
Squirrel    & \textbf{97.17\scriptsize{$\pm$0.11}} & 93.14\scriptsize{$\pm$0.22} & 95.56\scriptsize{$\pm$0.37} & 94.33\scriptsize{$\pm$0.43} & 93.75\scriptsize{$\pm$0.32} & 92.89\scriptsize{$\pm$0.14} \\
Texas       & \textbf{81.23\scriptsize{$\pm$0.14}} & 78.45\scriptsize{$\pm$0.36} & 79.88\scriptsize{$\pm$0.52} & 80.12\scriptsize{$\pm$0.42} & 79.23\scriptsize{$\pm$0.37} & 77.81\scriptsize{$\pm$0.54} \\
Cornell     & \textbf{85.23\scriptsize{$\pm$0.05}} & 82.89\scriptsize{$\pm$0.32} & 83.91\scriptsize{$\pm$0.39} & 84.23\scriptsize{$\pm$0.37} & 82.31\scriptsize{$\pm$0.48} & 82.45\scriptsize{$\pm$0.42} \\
\textbf{Avg. $\Delta$ Gain} & 0 &  \color{blue}3.0437&	\color{blue}1.5462&	\color{blue}1.8625&	\color{blue}2.8312& \color{blue}2.8262\\
    \bottomrule
    \end{tabular}}
     \caption{Ablation study (\texttt{LP}) results. $\modelname_{euc}$ is the Euclidean variant, $\modelname_{lap}$ uses the traditional Laplacian, $\modelname_{enc}$ gets rid of curvature encoding, $\modelname_{pool}$ replaces \modelshort\ pooling with concatenation, and $\modelname_{fil}$ uses a single filter instead of a filter bank. \textbf{Av. $\Delta$ Gain} represents the average gain of \modelname\ over the ablation model in that column, averaged across the different datasets.}
     \label{tab:lp_ablation_link_prediction}
\end{table}
\begin{table}[ht]
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\modelname & {Cora} & {Citeseer} & {PubMed} & {Chameleon} & {Actor} & {Squirrel} & {Texas} & {Cornell} \\ \midrule
\modela & 93.10\scriptsize{$\pm$0.22} & 94.33\scriptsize{$\pm$0.35} & 95.41\scriptsize{$\pm$0.25} & 97.28\scriptsize{$\pm$0.45} & 95.25\scriptsize{$\pm$0.44} & 96.05\scriptsize{$\pm$0.51} & \color{blue}75.43\scriptsize{$\pm$0.31} & \color{blue}76.95\scriptsize{$\pm$0.38} \\
\modelb & 93.00\scriptsize{$\pm$0.27} & 95.11\scriptsize{$\pm$0.28} & 94.52\scriptsize{$\pm$0.34} & 96.66\scriptsize{$\pm$0.39} & 95.34\scriptsize{$\pm$0.38} & 95.40\scriptsize{$\pm$0.57} & 80.95\scriptsize{$\pm$0.42} & 84.80\scriptsize{$\pm$0.43} \\
\modelc & 93.20\scriptsize{$\pm$0.24} & 93.25\scriptsize{$\pm$0.30} & 95.65\scriptsize{$\pm$0.30} & 97.11\scriptsize{$\pm$0.37} & 94.92\scriptsize{$\pm$0.40} & 95.51\scriptsize{$\pm$0.45} & \textbf{81.23\scriptsize{$\pm$0.14}} & \textbf{85.23\scriptsize{$\pm$0.05}} \\
\modeld & 93.50\scriptsize{$\pm$0.25} & 94.12\scriptsize{$\pm$0.33} & 95.32\scriptsize{$\pm$0.45} & \textbf{97.66\scriptsize{$\pm$0.33}} & 95.20\scriptsize{$\pm$0.34} & \textbf{97.17\scriptsize{$\pm$0.11}} & 80.30\scriptsize{$\pm$0.45} & \color{blue}78.02\scriptsize{$\pm$0.47} \\
\modele & 93.25\scriptsize{$\pm$0.21} & 93.05\scriptsize{$\pm$0.32} & \textbf{96.01\scriptsize{$\pm$0.01}} & 96.80\scriptsize{$\pm$0.40} & \textbf{96.04\scriptsize{$\pm$0.38}} & 96.15\scriptsize{$\pm$0.55} & 79.50\scriptsize{$\pm$0.38} & 84.60\scriptsize{$\pm$0.40} \\
\modelf & 92.70\scriptsize{$\pm$0.30} & 94.71\scriptsize{$\pm$0.35} & 95.45\scriptsize{$\pm$0.42} & 96.51\scriptsize{$\pm$0.47} & 95.00\scriptsize{$\pm$0.37} & \color{blue}88.95\scriptsize{$\pm$0.53} & 79.70\scriptsize{$\pm$0.54} & 81.20\scriptsize{$\pm$0.60} \\
\modelg & 88.50\scriptsize{$\pm$0.39} & 91.23\scriptsize{$\pm$0.35} & 89.99\scriptsize{$\pm$0.44} & 91.12\scriptsize{$\pm$0.43} & \color{blue}79.90\scriptsize{$\pm$0.52} & 91.20\scriptsize{$\pm$0.51} & 79.08\scriptsize{$\pm$0.52} & 83.75\scriptsize{$\pm$0.48} \\
\modelh & \textbf{95.08\scriptsize{$\pm$0.13}} & \textbf{96.88\scriptsize{$\pm$0.65}} & 95.55\scriptsize{$\pm$0.41} & 94.44\scriptsize{$\pm$0.34} & 95.55\scriptsize{$\pm$0.35} & 96.25\scriptsize{$\pm$0.38} & 81.07\scriptsize{$\pm$0.29} & 84.60\scriptsize{$\pm$0.30} \\
\modeli & 88.05\scriptsize{$\pm$0.40} & 90.35\scriptsize{$\pm$0.42} & \color{blue}86.30\scriptsize{$\pm$0.48} & 89.34\scriptsize{$\pm$0.45} & 93.33\scriptsize{$\pm$0.54} & 90.25\scriptsize{$\pm$0.48} & 80.10\scriptsize{$\pm$0.43} & 82.01\scriptsize{$\pm$0.55} \\
\modelj & 89.10\scriptsize{$\pm$0.44} & 91.01\scriptsize{$\pm$0.50} & 93.93\scriptsize{$\pm$0.55} & 96.19\scriptsize{$\pm$0.50} & 95.25\scriptsize{$\pm$0.49} & 92.30\scriptsize{$\pm$0.52} & \color{blue}77.77\scriptsize{$\pm$0.53} & 79.44\scriptsize{$\pm$0.58} \\
\bottomrule
\end{tabular}}
\caption{Performance comparison of \modelname\ with different manifold signatures for Link Prediction (\texttt{LP}). Best performing \textit{signatures} are in \textbf{Bold}, and cases with a large decline in performance because of manifold mismatch are in {\color{blue}Blue}.}\label{tab:lp_signatures}
\end{table}
\begin{table}[ht]
\centering
\setlength{\tabcolsep}{3pt}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Dataset} & Signature \\ \midrule
Cora &  $\mathbb{H}^{16} ({\color{red}-0.21}, {\color{blue}0.31}) \times \mathbb{S}^{16} ({\color{red}+0.49}, {\color{blue}0.38}) \times \mathbb{E}^{16} ({\color{red}0}, {\color{blue}0.31})$ \\
Citeseer & $\mathbb{H}^{16} ({\color{red}-0.78}, {\color{blue}0.29}) \times \mathbb{S}^{16} ({\color{red}+0.55}, {\color{blue}0.39}) \times \mathbb{E}^{16} ({\color{red}0}, {\color{blue}0.32})$ \\
PubMed &  $\mathbb{H}^{16} ({\color{red}-0.76}, {\color{blue}0.56}) \times \mathbb{H}^{16} ({\color{red}-0.28}, {\color{blue}0.41}) \times \mathbb{E}^{16} ({\color{red}0}, {\color{blue}0.03})$ \\
Chameleon &  $\mathbb{H}^{16} ({\color{red}-0.34}, {\color{blue}0.09}) \times \mathbb{S}^{16} ({\color{red}+0.71}, {\color{blue}0.25}) \times \mathbb{S}^{16} ({\color{red}+0.55}, {\color{blue}0.34})$ \\
Actor & $\mathbb{H}^{16} ({\color{red}-0.77}, {\color{blue}0.17}) \times \mathbb{H}^{16} ({\color{red}-0.39}, {\color{blue}0.42}) \times \mathbb{E}^{16} ({\color{red}0}, {\color{blue}0.41})$ \\
Squirrel & $\mathbb{H}^{16} ({\color{red}-0.17}, {\color{blue}0.23}) \times \mathbb{S}^{16} ({\color{red}+0.54}, {\color{blue}0.38}) \times \mathbb{S}^{16} ({\color{red}+0.38}, {\color{blue}0.39})$ \\
Texas   & $\mathbb{H}^{8} ({\color{red}-0.38}, {\color{blue}0.31}) \times \mathbb{S}^{8} ({\color{red}+0.18}, {\color{blue}0.19}) \times \mathbb{E}^{32} ({\color{red}0}, {\color{blue}0.50})$ \\
Cornell & $\mathbb{H}^{8} ({\color{red}-0.41}, {\color{blue}0.19}) \times \mathbb{S}^{8} ({\color{red}+0.09}, {\color{blue}0.26}) \times \mathbb{E}^{32} ({\color{red}0}, {\color{blue}0.55})$ \\
\bottomrule
\end{tabular}}
\caption{Learning results of \modelname\ on Link Prediction (\texttt{LP}) task for the best performing product signature. Format of entries $-$ \textit{manifold type}$^{\text{ (dim)}}$ ({\color{red}learnt curvature}, {\color{blue}learnt manifold weight}).}
    \label{tab:lp_product}
\end{table}

\begin{table}[!htbp]
\centering
\scriptsize
\setlength{\tabcolsep}{5pt}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\textbf{Dataset}    & $\mathbf{I}$    & $\mathbf{Z}^{(1)}$  & $\mathbf{Z}^{(2)}$  & $\mathbf{Z}^{(3)}$  & $\mathbf{Z}^{(4)}$  & $\mathbf{Z}^{(5)}$  & $\mathbf{Z}^{(6)}$  & $\mathbf{Z}^{(7)}$  & $\mathbf{Z}^{(8)}$  & $\mathbf{Z}^{(9)}$  &$\mathbf{Z}^{(10)}$ \\ \midrule
Cora       & \textbf{0.1125}& \color{Green}0.2393 & 0.0520 & 0.0504  & 0.0676 & 0.0272 & \color{magenta}0.1531  & 0.1251 & 0.0939 & 0.0095 & 0.0694 \\
Citeseer   & \textbf{0.2131}  & 0.0150 & \color{Green}0.2195 & 0.0283 & \color{magenta}0.0922 & 0.1530 & 0.0350 & 0.0320 & 0.0282 & 0.1254 & 0.0582  \\
PubMed     & 0.0279 & \color{Green}0.1793 & 0.0285 & 0.0296 & 0.0563 & \textbf{0.3870} & 0.0603 & 0.0324 & 0.0612 & 0.0271 & \color{magenta}0.1104 \\
Chameleon  & 0.0601 & 0.1136 & \color{Green}0.1694 & \color{magenta}0.0924 & \textbf{0.1329} & 0.1605 & 0.1026 & 0.0157 & 0.0475 & 0.0344 & 0.0709 \\
Actor      & \color{magenta}0.1230 & 0.0321 & 0.0324 & 0.1147 & 0.1287 & \textbf{0.3700} & 0.0136 & 0.0389 & 0.0604 & \color{Green}0.0691 & 0.0172 \\
Squirrel   & 0.0182 & 0.0289 & \color{Green}0.1056 & \textbf{0.2099} & 0.0209 & 0.0355 & 0.0854 & \color{magenta}0.2159 & 0.0475 & 0.1042 & 0.1279 \\
Texas      & 0.0890 & \color{magenta}0.1983 & 0.0179 & \textbf{0.4570} & 0.0035 & \color{Green}0.1429 & 0.0177 & 0.0087 & 0.0474 & 0.0131 & 0.0046 \\
Cornell    & 0.0886 & \color{magenta}0.2026 & 0.0181 & \textbf{0.4460} & 0.0034 & \color{Green}0.1472 & 0.0183 & 0.0089 & 0.0487 & 0.0134 & 0.0048 \\
\bottomrule
\end{tabular}}
\caption{Learned filter weights (Link Prediction) for the top-performing split, distinguishing between homophilic (favoring low-pass filters) and heterophilic (favoring high-pass filters). \textbf{First}, {\color{Green}second}, and {\color{magenta}third} highest filter weights are highlighted.}
\label{tab:lp_filters}
\end{table}

\subsubsection{Estimating Product Manifold Signature}\label{app:estimate}
In our model, the mixed-curvature product manifold $\mathbb{P}^{d_{\mathcal{C}}}$ is essential for representing the geometric structure of the data. The curvature configuration needed for each dataset depends on the intrinsic geometry of its graph. To generalize across various datasets, we aim to determine the optimal signature of the product manifold, specifically the proportions of hyperbolic, spherical, and Euclidean components. This estimation is based on analyzing the Ollivier-Ricci curvature (\texttt{ORC}) distribution as a heuristic. See Figures \ref{app_fig:edge_curv} and \ref{app_fig:node_curv} in Appendix \ref{app:graphs} for the \texttt{ORC} distribution of multiple datasets. For datasets with many positively curved edges, we select a Spherical component, and for those with negatively curved edges, we choose a Hyperbolic component. For example, the curvature distribution of PubMed's edges in Figure \ref{app_fig:edge_curv} shows two significant peaks around $-0.45$ and $+0.25$. Given this distribution, we opt for Spherical and Hyperbolic components when evaluating \modelname\ on PubMed. Empirically, the best performance for PubMed is achieved with the signature \modele. We initialize the curvatures of PubMed's component manifolds with these prominent values: $\mathbb{H}$ with $-0.45$ and $\mathbb{S}$ with $+0.25$. We select two hyperbolic manifolds to capture different curvature ranges. 

An overview of this simple idea is provided in Algorithm \ref{alg:signature_identification}. By systematically analyzing the curvature distribution, our heuristic-based algorithm identifies the manifold signature that best represents the dataset's underlying geometric structure. We heuristically cluster the curvature distribution and identify the centroid curvatures without altering their order or frequencies. The use of predefined dimensions allows for flexibility based on experimental settings. Since optimal dimension allocations can vary and are complex to analyze, we manually set the dimensions of the component manifolds as a hyperparameter. This ensures fair and uniform comparison across multiple datasets, as different datasets may perform best with different configurations. We do not claim that this algorithm finds the best possible, optimal combination of component manifolds, rather, it \textit{estimates} a potential signature that might be a good fit for a particular dataset.

\begin{algorithm}[ht]
\caption{Product manifold signature estimation and curvature initialisation}
\label{alg:signature_identification}
\footnotesize
\begin{algorithmic}[1]
\Require
\begin{itemize}
    \item Edge curvature histogram \( \mathcal{C} = \{ (\kappa_i, f_i) \}_{i=1}^{N} \)
    \item Threshold \( \epsilon \) to distinguish between curved and flat regions
    \item Maximum number of Hyperbolic $(\mathcal{H}_{\max})$ and Spherical $( \mathcal{S}_{\max})$ components.
    \item Total product manifold dimension \( d_{\mathcal{M}} \)
    \item (Optional) Preferred component manifold dimensions \( d_{(h)}^{\text{pre}} ,  d_{(s)}^{\text{pre}}, d_{(e)}^{\text{pre}} \)
\end{itemize}
\Ensure Product manifold signature \( \mathbb{P}^{d_{\mathcal{M}}} = \times_{q=1}^{\mathcal{Q}} \mathcal{M}_q^{\kappa_{(q)}, d_{(q)}} \)
\Statex

\State Normalize frequencies: \( f'_i = \frac{f_i}{\sum_{j=1}^{N} f_j} \)
\State Construct weighted curvature set: \( \mathcal{C}' = \{ (\kappa_i, f'_i) \}_{i=1}^{N} \)
\State Determine optimal number of clusters \( K \) using methods like the elbow method, constrained by \( K \leq \mathcal{H}_{\max} + \mathcal{S}_{\max} + 1\) \Comment{There can be only 1 Euclidean component}
\State Cluster \( \mathcal{C}' \) into \( K \) clusters using weighted clustering (e.g., weighted K-means) 
\State Initialize empty lists \( \mathcal{H}, \mathcal{S}, \mathcal{E} \)
\For{each cluster \( c \) in clusters}
    \State Compute cluster centroid curvature \( \kappa_c = \frac{\sum_{(\kappa_i, f'_i) \in c} \kappa_i }{|c|}\)
    \State Compute total frequency weight \( w_c = \sum_{(\kappa_i, f'_i) \in c} f'_i \)
    \If{\( \kappa_c < -\epsilon \) \textbf{and} \( |\mathcal{H}| \leq \mathcal{H}_{\max} \)} \Comment{Negative curvature}
        \State Assign manifold component:  $\mathcal{M}_q = \mathbb{H}^{\kappa_c}$  \Comment{Curvature initialization}
        \State Add  $(\mathcal{M}q, w_c)$  to  $\mathcal{H}$
    \ElsIf{\( \kappa_c > \epsilon \) \textbf{and} \( |\mathcal{S}| \leq \mathcal{S}_{\max} \)} \Comment{Positive curvature}
        \State Assign manifold component:  $\mathcal{M}_q = \mathbb{S}^{\kappa_c}$  \Comment{Curvature initialization}
        \State Add  $(\mathcal{M}q, w_c)$  to  $\mathcal{S}$
    \Else
        \State Assign manifold component:  $\mathcal{M}_q = \mathbb{E}$ \Comment{Approximate zero curvature, i.e. $\kappa_c \in [-\epsilon, \epsilon]$}
        \State Add  $(\mathcal{M}q, w_c)$  to  $\mathcal{E}$ 
    \EndIf
\EndFor

\If{Predefined dimensions \( d_{(h)}^{\text{pre}} ,  d_{(s)}^{\text{pre}}, d_{(e)}^{\text{pre}} \) are provided}
    \State Assign dimensions \( d_{(q)} \) to each component \( q \) as per predefined values \Comment{Dimension assignment}
\Else
    \State Set total number of components \( \mathcal{Q} = |\mathcal{H}| + |\mathcal{S}| + |\mathcal{E}| \) \Comment{Dimension assignment}
    \State Allocate dimensions \( d_{(q)} \) to each component \( q \):  $ d_{(q)} = \left\lfloor d_{\mathcal{M}} \times \frac{w_q}{\sum_{p=1}^{\mathcal{Q}} w_p} \right\rfloor$ \Comment{Proportional to weights}
    \State Adjust \( d_{(q)} \) to ensure \( \sum_{q=1}^{\mathcal{Q}} d_{(q)} = d_{\mathcal{M}} \)
\EndIf
\State Formulate manifold signature:
\[
\mathbb{P}^{d_{\mathcal{M}}} = \left( \times_{h=1}^{|\mathcal{H}|} \mathbb{H}_{h}^{\kappa_{(h)}, d_{(h)}} \right) \times \left( \times_{s=1}^{|\mathcal{S}|} \mathbb{S}_{s}^{\kappa_{(s)}, d_{(s)}} \right) \times \mathbb{E}^{d_{(e)}}
\]
\end{algorithmic}
\end{algorithm}

\subsubsection{More Experimental Settings} \label{app:hyperparam}

\begin{table}[ht]
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{l|c|l}
\toprule
\textbf{Hyperparameter}       & \textbf{Tuning Configurations}                                              & \textbf{Description}                                             \\ \midrule

$L$                    & $\{5, {\color{red}10}, 15, 20, 25\}$                                                   & Total number of graph filters.                             \\
$\delta$ & $\{0.2, {\color{red}0.5}, 0.7\}$ & Neighbourhood weight distribution parameter for \texttt{ORC}\\
$\alpha$                & $\{0.1, {\color{red}0.3}, 0.5, 0.9\}$                                             & Alpha (initialization) parameter for \texttt{GPR} propagation                              \\ 
$d^{\mathcal{C}}$                 & $\{8, {\color{red}16}, 32, 64\}$                                                       & Total dimension of curvature embeddings.                           \\
$d^{\mathcal{M}}$                 & $\{32, {\color{red}48}, 64, 128, 256\}$                                                       & Total dimension of the product manifold.                           \\
\texttt{dropout}              & $\{0.2, {\color{red}0.3}, 0.5\} $                                                    & Dropout rate                                                                                 \\ 
\texttt{epochs}               & $\{50, {\color{red}100}, 300\}$                                                     & Number of training epochs            \\
\texttt{lr}                   & $\{1e-4, {\color{red}{4e-3}}, 0.001, 0.01\}$                                               & Learning rate                             \\
\texttt{weight\_decay}        & $\{0, 1e-4, {\color{red}5e-4}\}$                                                  & Weight decay              \\                  
\bottomrule
\end{tabular}
}
\caption{Hyperparameter configurations used in the experiments for all baselines. Some of the hyperparameters are specific to \modelname. We highlight the final configuration of \modelname\ for \texttt{NC} in \color{Red}{Red}.}
\vspace{-5mm}
\label{tab:hyperparams_full}
\end{table}


\stopcontents[subsections]
% \newpage
% \subsection{Experiments}\label{app:experiments}
% \subsubsection{Baselines} \label{app:baseline}
\end{document}
