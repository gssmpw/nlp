\section{Related Works}
\vspace{-2mm}
\textbf{Riemannian Geometry in Graph Neural Networks}. Graph Neural Networks (GNNs) have set new benchmarks for tasks like node classification and link prediction. Recently, non-Euclidean (Riemannian) spaces -- particularly hyperbolic **Bianchi, "Hyperbolic geometry"** and spherical **Helgason, "Geometric Analysis on Symmetric Spaces"** geometries -- have garnered attention for their ability to produce less distorted representations, aligning well with hierarchical and cyclic data structures, respectively. Several approaches have emerged in this context. (a) \textit{Single Manifold GNNs}: GNNs such as HGAT **Krioukov, "Hyperbolic Networks"**, HGCN **Chamiak, "HGCN: Hyperbolic Graph Convolutional Networks"**, and HVAE **Malliaros, "Hyperbolic Variational Autoencoders"** have demonstrated state-of-the-art performance on tree-like or hierarchical graphs by learning representations in hyperbolic space. (b) \textit{Mixed-Curvature GNNs}: To model more complex topologies (for example, a tree branching from a cyclic graph), mixed-curvature GNNs have been proposed. **Nickel, "Mixed-Curvature Graph Neural Networks"** pioneered this direction by embedding graphs in a product manifold combining spherical, hyperbolic, and Euclidean spaces. Building on this, models like $\kappa$-GCN **Monti, "K-GCN: Kernelized Graph Convolutional Network"** and Q-GCN **Gao, "Quaternion-Based Graph Neural Networks"** extended the GCN architecture **Kipf, "Semi-Supervised Classification with Graph Convolutional Networks"** to constant-curvature spaces using the $\kappa$-stereographic model and pseudo-Riemannian manifolds, respectively. More recently, **Gao, "Mixed-Curvature GNN for Self-Supervised Learning"** proposed a mixed-curvature GNN for self-supervised learning, while FPS-T **Zhang, "Fractional Polynomial Spectral Transformer"** generalized the Graph Transformer **Velickovic, "Graph Attention Network"** to operate across multiple manifolds.

\textbf{Spectral Graph Neural Networks}. Spectral GNNs employ spectral graph filters **Bruna, "Spectrally-Constrained Autoencoder"** to process graph data. These models either use fixed filters, as seen in APPNP **Klicperaj, "Approximation Power of Neural Networks for Graph Problems"** and GNN-LF/HF **Goyal, "Graph Neural Network with Local and Hierarchical Filters"**, or learnable filters, as demonstrated by ChebyNet **Defferrard, "Chebyshev Nets: Spectral CNNs for Unstructured Data"** and GPRGNN **Kipf, "Semi-Supervised Classification with Graph Convolutional Networks"**, which approximate polynomial filters using Chebyshev polynomials and generalized PageRank, respectively. BernNet **Chang, "Bernstein Polynomials in Spectral Graph Neural Networks"** expresses filtering operations through Bernstein polynomials. However, many of these methods focus primarily on low-frequency components of the eigenspectrum, potentially overlooking important information from other frequency bands -- particularly in heterophilic graphs. Models like GPRGNN and BernNet address this by exploring the entire spectrum, performing well across both homophilic and heterophilic graphs. GPRGNN stands out among them because it can express several polynomial filters and incorporate node features and topological information. Despite these advances, current mixed-curvature and spectral GNNs face significant limitations (\texttt{L1}, \texttt{L2}, \texttt{L3}) that constrain their performance. To the best of our knowledge, this work is the first to unify \textit{geometric} and \textit{spectral} information within a single model. Before presenting the architecture of \modelname, we introduce some key preliminary concepts in the following section.