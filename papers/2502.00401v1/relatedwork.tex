\section{Related Works}
\vspace{-2mm}
\textbf{Riemannian Geometry in Graph Neural Networks}. Graph Neural Networks (GNNs) have set new benchmarks for tasks like node classification and link prediction. Recently, non-Euclidean (Riemannian) spaces -- particularly hyperbolic \citep{sala2018representation} and spherical \citep{liu2017sphereface, wilson2014spherical} geometries -- have garnered attention for their ability to produce less distorted representations, aligning well with hierarchical and cyclic data structures, respectively. Several approaches have emerged in this context. (a) \textit{Single Manifold GNNs}: GNNs such as HGAT \citep{zhang2021hyperbolic}, HGCN \citep{chami2019hyperbolic}, and HVAE \citep{sun2021hyperbolic} have demonstrated state-of-the-art performance on tree-like or hierarchical graphs by learning representations in hyperbolic space. (b) \textit{Mixed-Curvature GNNs}: To model more complex topologies (for example, a tree branching from a cyclic graph), mixed-curvature GNNs have been proposed. \cite{gu2019learning} pioneered this direction by embedding graphs in a product manifold combining spherical, hyperbolic, and Euclidean spaces. Building on this, models like $\kappa$-GCN \citep{bachmann2020constant} and Q-GCN \citep{xiong2022pseudo} extended the GCN architecture \citep{kipf2016semi} to constant-curvature spaces using the $\kappa$-stereographic model and pseudo-Riemannian manifolds, respectively. More recently, \cite{sun2022self} proposed a mixed-curvature GNN for self-supervised learning, while FPS-T \citep{cho2023curve} generalized the Graph Transformer \citep{min2022transformer} to operate across multiple manifolds.

\textbf{Spectral Graph Neural Networks}. Spectral GNNs employ spectral graph filters \citep{liao2024benchmarking} to process graph data. These models either use fixed filters, as seen in APPNP \citep{gasteiger2018predict} and GNN-LF/HF \citep{zhu2021interpreting}, or learnable filters, as demonstrated by ChebyNet \citep{defferrard2016convolutional} and GPRGNN \citep{chien2020adaptive}, which approximate polynomial filters using Chebyshev polynomials and generalized PageRank, respectively. BernNet \citep{he2021bernnet} expresses filtering operations through Bernstein polynomials. However, many of these methods focus primarily on low-frequency components of the eigenspectrum, potentially overlooking important information from other frequency bands -- particularly in heterophilic graphs. Models like GPRGNN and BernNet address this by exploring the entire spectrum, performing well across both homophilic and heterophilic graphs. GPRGNN stands out among them because it can express several polynomial filters and incorporate node features and topological information. Despite these advances, current mixed-curvature and spectral GNNs face significant limitations (\texttt{L1}, \texttt{L2}, \texttt{L3}) that constrain their performance. To the best of our knowledge, this work is the first to unify \textit{geometric} and \textit{spectral} information within a single model. Before presenting the architecture of \modelname, we introduce some key preliminary concepts in the following section.