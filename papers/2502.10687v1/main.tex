
% \documentclass[10pt, conference, letterpaper]{IEEEtran} 
\documentclass[journal]{IEEEtran}

\usepackage{amsmath,amsfonts,bm}
% \usepackage{algorithmic}
% \usepackage{algorithm}
\usepackage{array}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
% \usepackage{limits}
\usepackage{amssymb}
% \usepackage{caption}
\usepackage[ruled,linesnumbered, noend]{algorithm2e}
\usepackage{booktabs}
\ifCLASSOPTIONcompsoc
\usepackage[caption=false, font=footnotesize, labelfont=sf, textfont=sf,subrefformat=parens]{subfig}
\else
\usepackage[caption=false, font=footnotesize,,subrefformat=parens]{subfig}
% \usepackage[font=small]{caption} 
\usepackage{makecell}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{hhline}


% \usepackage{subcaption}
\bibliographystyle{IEEEtran}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021
\begin{document}

% \title{IRS-assisted UAV for Power Transfer and Data Collection: A Deep Reinforcemence Learning Method}

\title{Multi-objective Aerial IRS-assisted ISAC Optimization via Generative AI-enhanced Deep Reinforcement Learning}


\author{
        Wenwen Xie,
        Geng Sun,~\IEEEmembership{Senior Member,~IEEE},    
        Jiacheng Wang,
        Hongyang Du,
        Jiawen Kang,~\IEEEmembership{Senior Member,~IEEE}, 
        Kaibin Huang,~\IEEEmembership{Fellow,~IEEE},
        Victor C. M. Leung,~\IEEEmembership{Life Fellow,~IEEE}
        \IEEEcompsocitemizethanks
        {
        \IEEEcompsocthanksitem Wenwen~Xie is with the College of Computer Science and Technology, Jilin University, Changchun 130012, China~(e-mail: xieww22@mails.jlu.edu.cn).
        \IEEEcompsocthanksitem Geng~Sun is with the College of Computer Science and Technology, Jilin University, Changchun 130012, China,  and also with the College of Computing and Data Science, Nanyang Technological University, Singapore 639798 (e-mail: sungeng@jlu.edu.cn).
        \IEEEcompsocthanksitem Jiacheng~Wang is with the College of Computing and Data Science, Nanyang Technological University, Singapore 639798 (e-mail: jiacheng.wang@ntu.edu.sg).
        \IEEEcompsocthanksitem Hongyang Du and Kainbin Huang are with the Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong 999077, China~(emails: duhy@eee.hku.hk, huangkb@hku.hk).
        \IEEEcompsocthanksitem Jiawen~Kang is with the School of Automation, Guangdong University of Technology, Guangzhou 510006, China (e-mail: kavinkang@gdut.edu.cn).
        \IEEEcompsocthanksitem Victor C.M. Leung is with the Artificial Intelligence Research Institute, Shenzhen MSU-BIT University, Shenzhen 518115, China, with the College of Computer Science and Software Engineering, Shenzhen University, Shenzhen 518060, China, and also with the Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver V6T 1Z4, Canada (E-mail: leung@ieee.org).
       \IEEEcompsocthanksitem (\textit{Corresponding author: Geng Sun.})
        }
 }

 \maketitle


 
\begin{abstract}
\par Integrated sensing and communication (ISAC) has garnered substantial research interest owing to its pivotal role in advancing the development of next-generation (6G) wireless networks. However, achieving a performance balance between communication and sensing in the dual-function radar communication (DFRC)-based ISAC system remains a significant challenge. In this paper, an aerial intelligent reflecting surface (IRS)-assisted ISAC system is explored, where a base station (BS) supports dual-functional operations, enabling both data transmission for multiple users and sensing for a blocked target, with the channel quality enhanced by an IRS mounted on the unmanned aerial vehicle (UAV). Moreover, we formulate an integrated communication, sensing, and energy efficiency multi-objective optimization problem (CSEMOP), which aims to maximize the communication rate of the users and the echo rate of the target, while minimizing UAV propulsion energy consumption by jointly optimizing the BS beamforming matrix, IRS phase shifts, the flight velocity and angle of the UAV. Considering the non-convexity, trade-off, and dynamic nature of the formulated CSEMOP, we propose a generative diffusion model-based deep deterministic policy gradient (GDMDDPG) method to solve the problem. Specifically, the diffusion model is incorporated into the actor network of DDPG to improve the action quality, with noise perturbation mechanism for better exploration and recent prioritized experience replay (RPER) sampling mechanism for enhanced training efficiency. Simulation results indicate that the GDMDDPG method delivers superior performance compared to the existing methods.

\end{abstract}

\begin{IEEEkeywords}

ISAC, IRS, multi-objective optimization, generative AI, deep reinforcement learning

\end{IEEEkeywords}

%section
%Introduction
%
\section{Introduction}
\label{Introduction}

\par As fifth-generation (5G) networks commercially deploys and research on sixth-generation (6G) progresses, conventional communication systems have revealed notable shortcomings in meeting the increasingly diverse and growing demands~\cite{Su2024,Lyu2023}. In particularly, as the low-altitude economy continues to evolve~\cite{10759668}, leveraging airspace for activities such as logistics and rescue operations, the need for seamless communication and real-time sensing becomes even more critical. Therefore, it is necessary to adopt a new approach that moves beyond traditional communication-centric network designs and embraces integrated sensing-communication networks. In this case, integrated sensing and communication (ISAC) technology stands out as a practical and efficient solution.~\cite{Dong2023}, and it has been recognized by the International Telecommunication Union (ITU) as a critical application scenario among the six primary use cases envisioned for the 6G era~\cite{Kaushik2024}.


\par In the practical implement of ISAC systems, non-ideal propagation environments caused by multipath fading or obstacle occlusion can significantly affect system performance~\cite{Geng2025}. In this context, intelligent reflecting surface (IRS) emerges as a promising solution~\cite{Wu2021,Gong2020}. Specifically, IRS consists of a planar array with densely packed sub-wavelength reflective elements, each of which can individually modify the phase and amplitude of incoming signals.~\cite{Sun2024a}. Because each reflective element is small in size, an IRS with practical size can incorporate numerous elements to enable substantial beamforming gains via co-modulation, effectively compensating for path loss or constructing virtual line-of sight (LoS) links~\cite{Hua2024}. Thus, the unique capability for remodeling propagation environment provides an innovative solution to enhance the robustness of the ISAC system in occlusion scenarios~\cite{Sankar2024}. Moreover, forming an aerial IRS by integrating the IRS on a maneuverable unmanned aerial vehicle (UAV) can break through the spatial coverage limitation of the traditional fixed IRS, so that further enhancing the effectiveness of the ISAC system~\cite{Xu2024}.

\par Currently, the ISAC system can mainly be classified into two major architectures, \textit{i.e.}, radar-communication coexistence (RCC) architecture and dual-functional radar-communication (DFRC) architecture~\cite{Yang2024}. In the former, radar transceivers and communication transmitters are physically distributed across distinct locations~\cite{He2022}, which typically leads to severe co-channel interference issues. Moreover, to achieve effective coordination among the radar and communication functionalities, the RCC architecture requires the complex information feedback mechanisms, inevitably introducing substantial communication overhead. In contrast, the DFRC architecture enables simultaneous communication and sensing functionalities by sharing the same hardware resources, thereby improving the network resource utilization~\cite{Liu2022}.

\par However, designing an effective beamforming and resource allocation strategy address the competing demands between the communication and sensing functionalities in DRFC architecture remains challenging.~\cite{Zuo2023}. In particular, in the aerial IRS-assisted ISAC system, dynamic characteristics are introduced to both application scenario and optimization problem due to the time-varying nature of channels and the mobility of aerial IRS. However, obtaining accurate prior knowledge in advance in the dynamic scenario is difficult, which makes traditional optimization methods (\textit{e.g.}, convex optimization) show limitations in solving such dynamic optimization problem~\cite{Guo2023}. In this context, deep reinforcement learning (DRL) provides a feasible solution. Specifically, DRL enables the agent to learn by interacting with the environment, removing the necessity for prior information and attaining performance close to optimal. However, traditional DRL algorithms employ Multi-Layer Perceptron (MLP)-based actor networks, which struggle to capture the complex environment features of the aerial IRS-assisted ISAC system~\cite{10759093}. Moreover, DRL algorithms typically necessitate a substantial number of agent-environment interactions to achieve meaningful learning progress.

\par To address the above challenges, we first formulate a multi-objective optimization problem, and propose a generative artificial intelligence (GenAI)-enabled DRL method. To the best of our knowledge, no existing work simultaneously optimizes communication, sensing, and energy efficiency performance in the movable IRS-assisted ISAC system. The primary contributions of this paper are outlined as follows:

\begin{itemize}
    \item \textbf{Aerial IRS-assisted ISAC System}: An aerial IRS-assisted ISAC system is explored, where a dual-functional base station (BS) equipped with multiple antennas simultaneously communicates with multiple users and provides sensing services to an occluded target via beamforming. Moreover, to mitigate the impact of obstacles on the sensing performance of the BS, the IRS is mounted on a UAV to form an aerial IRS. Compared with fixed IRS installations, aerial IRS offers greater spatial flexibility, enabling the dynamic establishment of virtual LoS links in response to environment changes, thereby significantly improving the channel quality of the system.
    
    \item \textbf{Formulation of Multi-objective Optimization Problem}: An integrated communication, sensing, and energy efficiency multi-objective optimization problem (CSEMOP) is formulated, which aims at maximizing the communication rate of users and the echo rate of the target while minimizing the UAV propulsion energy consumption by jointly optimizing the BS active beamforming matrix, IRS phase shifts, the flight velocity and angle of the UAV. Notably, the formulated CSEMOP is a non-convex dynamic optimization problem with trade-offs, which makes traditional optimization algorithms limited in solving the problem.
    \item \textbf{GenAI-enabled DRL Method}: We first transform the formulated CSEMOP into a Markov decision process (MDP), and then propose a diffusion model-based deep deterministic policy gradient (GDMDDPG) method, which is a GenAI-enabled DRL method to solve the problem. Specifically, the diffusion model-based actor network integrates the analytical and generative capabilities of the diffusion model, enhancing both environment state analysis and action generation of the method. Moreover, the noise perturbation mechanism introduces stochasticity to overcome policy stagnation and foster exploration within the action space. Meanwhile, the RPER mechanism combines recent experience emphasis (ERE) and prioritized experience replay (PER) to dynamically select high-value recent experiences during network updates, which accelerates learning process of the method.
    \item \textbf{Simulation and Analysis}: Simulation results validate the effectiveness of the proposed GenAI-enabled DRL method. Specifically, the proposed method exhibits superior performance across various simulation settings compared to the three deployment methods and two other DRL benchmark algorithms, particularly achieving enhanced communication and sensing performance even under lower UAV energy consumption. Moreover, we perform a simulation analysis of the proposed method across various UAV initial locations to further validate its robustness.
\end{itemize}
 % Specifically, the actor network of the DDPG algorithm incorporates the diffusion model to generate actions of higher quality. Moreover, a noise perturbation mechanism is applied to improve the exploration capabilities of the agent within the action space. In addition, a RPER sampling mechanism based on recent experience emphasis (ERE) and prioritized experience replay (PER) is employed to select experiences from the replay buffer during the network update phase, thereby accelerating the learning process.
% Specifically, given the powerful analysis and modeling capabilities of the diffusion model, it is integrated into the actor network of DDPG algorithm to enhance the analysis of complex environment and generate high-quality action.
    

\par The organization of this paper is as follows: Section~\ref{sec: Related Work} introduces related work. Section~\ref{sec: System Model and Problem Formulation} presents the system model. Section~\ref{sec: Problem Analyses} formulates and analyzes the optimization problem.  Section~\ref{section: Proposed Algorithm} provides the proposed GenAI-enabled DRL method. Section~\ref{sec: Simulations} conducts the simulations. Finally, the paper is summarized in Section~\ref{sec: Conclusion}.


%section
%Related Work
%
\section{Related Work}
\label{sec: Related Work}

\par In this section, we present the existing work related to ISAC from three perspectives, \textit{i.e.}, scenario, optimization objectives, and optimization methods.

\subsection{IRS-assisted ISAC System}
\par For IRS-assisted DFRC-based system, the authors in~\cite{Hu2023} utilized semi-passive IRS to assist single-antenna users in achieving single-input multiple-output (SIMO) millimeter-wave (mmWave) communication with a BS while simultaneously performing location-aware services. The authors in~\cite{Li2023} deployed active IRS equipped with amplifiers to assist multiple-input multiple-output (MIMO) DFRC-based ISAC scenario. Moreover, multi-IRS-assisted ISAC systems are explored in~\cite{Fang2024}, addressing two distinct sensing scenarios involving point targets and extended targets. In addition, the authors in~\cite{Liao2023} explored the scenario where sensing signals enable communication with multiple users, taking into account the influence of environmental clutter on ISAC systems supported by IRS. However, the aforementioned existing works primarily employ fixed IRS to assist ISAC system. In contrast, mobile IRS can provide more flexible support for ISAC system, which can further enhances the system performance.

\subsection{Optimization Variables in IRS-assisted ISAC System}
\par In terms of communication metrics, the authors in~\cite{Wang2023,Xu2024} optimized resource allocation for the IRS-supported DFRC-based ISAC system to improve the communication rate for users. Moreover, the authors in~\cite{Li2024,Jiang2024} tackled eavesdropping threats in the ISAC scenario by refining BS beamforming matrix and adjusting IRS phase shifts to enhance the secure communication rate. In terms of sensing metrics, the authors in~\cite{Zuo2023} explored serving users and targets simultaneously by transmitting superimposed NOMA signals, aiming to maximize the beampattern gain, which measures energy concentration on the target. Based on this, the authors in~\cite{Zhang2025} further explored maximizing the beampattern gain for targets in a STAR-RIS-supported ISAC system with environmental clutter. Moreover, the authors in~\cite{Long2024} explored ISAC technology to tackle precise vehicle positioning and high-quality communication challenges by optimizing IRS phase shifts to minimize the position error bound. However, the aforementioned existing works optimized either communication or sensing metrics individually, without addressing the simultaneous optimization of both metrics.

\subsection{Optimization Methods in IRS-assisted ISAC System}
\par Currently, various optimization methods have been widely used to solve the optimization problems in IRS-assisted ISAC systems. For example, the authors in~\cite{Hu2024} adopted a multi-strategy alternating optimization algorithm based on quadratically constrained quadratic programming and semidefinite relaxation to enhance both the transmission rate and probing power. Moreover, the authors in~\cite{Cao2023} designed an improved particle swarm optimization algorithm to jointly optimize active and passive beamforming for maximizing the echo signal power. In addition, the authors in~\cite{10594249} employed the proximal policy optimization (PPO) algorithm to optimize beamforming matrix and the UAV trajectory to maximize the transmission rate in the aerial IRS-assisted ISAC system. Furthermore, the authors in~\cite{10257639} explored a STAR-RIS-supported secure ISAC framework and used the soft actor-critic (SAC) and DDPG algorithms to improve the long-term secrecy rate.
\par However, both convex optimization and swarm intelligence methods face limitations in dynamic scenarios where the prior knowledge is difficult to acquire. Moreover, while DRL algorithms show inherent advantages over traditional optimization methods, limitations still persist in analyzing complex environmental features and state-action relationships, which may affect the quality of decision.

% However, in practical scenarios, obtaining prior knowledge about the environment in advance is often challenging. 

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{System_Model_1.pdf}
    \caption{Aerial IRS-assisted ISAC system.}
    \label{fig: system model}
\end{figure}

%section
%System Model and Problem Formulation
%
\section{System Model}
\label{sec: System Model and Problem Formulation}

\par In this section, we provide detailed models of communication, sensing and UAV mobility in the aerial IRS-assisted ISAC system.

\subsection{System Overview}
\par As shown in Fig.~\ref{fig: system model}, an aerial IRS-assisted ISAC system is considered, which consists of a BS equipped with a uniform antenna array of $ M $ antennas, a ground user set denoted as $ \mathcal{N} = \{1, \cdots, N\} $, and a ground target. Specifically, the dual-functional BS communicates with the $N$ users while performing sensing tasks for the target. We assume that the presence of obstacles prevents direct sensing link between the BS and the target, which significantly affects the sensing performance of the BS~\cite{Xu2024}. In this case, we deploy a UAV-carried IRS, consisting of $L$ reflective elements denoted as $\mathcal{L} = \{1, \cdots, L\}$, to establish virtual line-of-sight (LoS) links between the BS and the target, which can mitigate the blocking effect of existing obstacles. Therefore, the introduction of the aerial IRS enhances both sensing and communication performance by improving signal quality, which boosts a more flexible and robust system.

\par We discretize the operation time of the system, dividing the total time $T_{\text{d}}$ into $T$ equal-length time slots, denoted as $\mathcal{T}=\{1, \cdots, T\}$, with the length of each time slot $t_{\text{d}}=\frac{T_{\text{d}}}{T}$. Moreover, we adopt the widely used Cartesian coordinate system to describe the locations of elements in our scenario. Specifically, due to the maneuverability of the UAV, the location of the aerial IRS is variable, and it can be represented as $q^{\text{r}}[t]=(x^{\text{r}}[t], y^{\text{r}}[t], z^{\text{r}})$ in time slot $t$. Similarly, the locations of the BS, $n$-th user and the target are denoted as $q^{\text{b}}=(x^{\text{b}}, y^{\text{b}}, z^{b})$, $q_{n}^{\text{u}}=(x_{n}^{\text{u}}, y_{n}^{\text{u}}, 0)$, $q^{\text{g}}=(x^{\text{g}}, y^{\text{g}}, 0 )$, respectively.

\par Notably, multiple sensing and communication links change as the trajectory of the aerial IRS varies, which introduces highly dynamic to the considered system. In the following, we provide a detailed model for sensing, communication, and UAV mobility to capture the key optimization variables and objectives.



% \par It is worth noting that the channel varies with the aerial IRS trajectory, which makes the system we consider highly dynamic.

\subsection{Channel Model}
\par The channel between the BS and the user is comprised of two main components, \textit{i.e.}, the direct link and the reflected link assisted by the aerial IRS. Next, a detailed description for the channel model is provided.

\subsubsection{Direct Link}
\par Due to the intricate nature of the signal propagation in the considered scenario, we utilize the Rician model to represent the direct links, effectively capturing both the LoS component and scattering effects from multipath propagation.~\cite{Xu2024,AlHourani2014}. Then, the channel vector $h_{n}^{bu}[t] \in \mathbb{C}^{M \times 1}$ from the BS to $n$-th user in time slot $t$ can be represented as
\begin{equation}
    \label{equ_1}
    h_{n}^{\text{bu}}[t] = \sqrt{\frac{L_{0}}{(d_{n}^{\text{bu}})^{\alpha^{\text{bu}}}}}  \big(\sqrt{\frac{\eta^\text{{bu}}}{\eta^{\text{bu}}+1}}g_{n,\text{L}}^{\text{bu}}[t] + \sqrt{\frac{1}{\eta^{\text{bu}}+1}}g_{n,\text{N}}^{\text{bu}}[t] \big),
\end{equation}

\noindent where $L_{0}$ represents the channel power gain at a reference distance of 1 m, $\alpha^{\text{bu}}$ denotes the path loss, and $\eta^{\text{bu}}$ is the Rician factor. Moreover, $d_{n}^{\text{bu}}=\sqrt{\|q^{\text{b}} - q_{n}^{\text{u}} \|^{2}}$ represents the distance between the BS and the $n$-th user. In addition, $g_{n,\text{L}}^{\text{bu}}[t]$ denotes the LoS component, and $g_{n, \text{N}}^{\text{bu}}[t] \sim \mathcal{N}(0, \mathbf{I})$ is the NLoS component that follows the complex Gaussian distribution.

\subsubsection{Reflected Link}
\par Note that the reflected link consists of the BS-aerial IRS link and the aerial IRS-users link, respectively.
\par When the aerial IRS is deployed at a sufficiently high altitude, the associated air-ground communication links are primarily dominated by the LoS component~\cite{AlHourani2014}. Then, the channel vector $h^{\text{br}}[t] \in \mathbb{C}^{L \times M}$ between the BS and the aerial IRS in time slot $t$ is given by
\begin{equation}
    \label{equ_2}
    h^{\text{br}}[t] = \sqrt{\frac{L_{0}}{(d^{\text{br}}[t])^{\alpha^{\text{br}}}}} g_{\text{L}}^{\text{br}}[t],
\end{equation}
\noindent where $\alpha^{\text{br}}$ in the BS-aerial IRS link, and $d^{\text{br}}[t]=\sqrt{\|q^{\text{b}}-q^{\text{r}}[t]\|^{2}}$ represents the distance from the BS to the aerial IRS at time slot $t$. Moreover, the LoS component $g_{\text{L}}^{\text{br}}[t]$ can be denoted as
\begin{eqnarray}
    \label{equ_3}
    g_{\text{L}}^{\text{br}}[t] &=& \left[1, e^{-j\frac{2\pi d_{r}\cos({\zeta})}{\lambda}}, \cdots, e^{-j\frac{2\pi d_{r}(L-1)\cos({\zeta})}{\lambda}}\right]^{H} \nonumber \\
    & \otimes & \left[1, e^{-j\frac{2\pi d_{s}\sin({\iota})}{\lambda}}, \cdots, e^{-j\frac{2\pi d_{s}(M-1)\sin({\iota})}{\lambda}}\right],
\end{eqnarray}

\noindent where $d_{r}$ and $d_{s}$ represent the spacing between IRS elements and the separation between neighboring antennas at the BS, respectively. Moreover, $\lambda$ is the wavelength. In addition, $\zeta$ and $\iota$ denote the angles of arrival (AoA) in the vertical and horizontal directions at the aerial IRS, respectively.

\par Likewise, the channel vector $h_{n}^{\text{ru}}[t] \in \mathbb{C}^{L \times 1}$ represents the communication link between the aerial IRS and the $n$-th user in time slot $t$, which is given by
\begin{equation}
    \label{equ_4}
    h_{n}^{\text{ru}}[t] = \sqrt{\frac{L_{0}}{(d_{n}^{\text{ru}}[t])^{\alpha^{\text{ru}}}}} g_{n,\text{L}}^{\text{ru}}[t],
\end{equation}
% \begin{eqnarray}
%     \label{equ_4}
%     h_{n}^{ru}[t] &=& \sqrt{\frac{L_{0}}{(d_{n}^{ru}[t])^{\alpha^{ru}}}} \left( \sqrt{\frac{\eta^{ru}}{\eta^{ru}+1}}g_{n,L}^{ru}[t] \right. \nonumber \\
%     && \left. + \sqrt{\frac{1}{\eta^{ru}+1}}g_{n,N}^{ru}[t] \right),
% \end{eqnarray}

\noindent where $\alpha^{\text{ru}}$ indicates the path loss. Moreover, $d_{n}^{\text{ru}}[t]=\sqrt{\|q^{\text{r}}[t]-q_{n}^{\text{u}}\|^{2}}$ is used to represent the distance from the aerial IRS and the $n$-th user. In addition, the LoS component $g_{n,\text{L}}^{\text{ru}}[t]$ is given by
\begin{equation}
    \label{equ_5}
    g_{n,\text{L}}^{\text{ru}}[t] = \left[1, e^{-j\frac{2\pi d_{r}\cos({\zeta_{n}})}{\lambda}}, \cdots, e^{-j\frac{2\pi d_{r}(L-1)\cos({\zeta_{n}})}{\lambda}}\right],
\end{equation}

\noindent where $\zeta_{n}$ is the angle of departure (AoD) from the aerial IRS to the $n$-th user.

\subsubsection{Composite Link}
\par In time slot $t$, the diagonal phase-shift matrix of the IRS can be represented as
\begin{equation}
    \label{equ_6}
    \Phi[t] = \rm{diag}\left(\mathbf{v}[t]\right),
\end{equation}

\noindent where $\mathbf{v}[t]=\left[e^{j\theta_{1}[t]}, e^{j\theta_{2}[t]}, \cdots, e^{j\theta_{L}[t]}\right]$ is the passive beamforming vector of the IRS, and $\theta_{l}[t] \in [0, 2\pi)$ is the phase shift of the $l$-th IRS element, wherein $l=\{1, 2, \cdots, L\}$.

\par As such, the composite channel between the BS and the $n$-th user can be denoted as
\begin{equation}
    \label{equ_7}
    \widetilde{H}_{n}^{\text{bu}}[t]=\left(h_{n}^{\text{bu}}[t]\right)^{\text{H}} + \left(h_{n}^{\text{ru}}[t]\right)^{\text{H}}\Phi[t]h^{\text{br}}[t].
\end{equation}

\noindent where $(\cdot)^{\rm H}$ represents Hermitian transpose.



\subsection{Communication Model}
\par Let $s_{n}[t]$ and $\omega_{n}[t] \in \mathbb{C}^{M \times 1}$ denote the symbol transmitted by the BS to the $n$-th user and the corresponding active beamforming vector in time slot $t$, respectively. We assume the symbol are independent and identically distributed with zero mean and unit variance, expressed as $s_{n} \sim \mathcal{N}(0, 1)$. Therefore, the BS transmission signal can be represented by
\begin{equation}
    \label{equ_8}
    \mathbf{x}[t] = \sum_{n=1}^{N}\omega_{n}[t]s_{n}[t].
\end{equation}

\par Then, the received signal of the $n$-th user in time slot $t$ is given by
\begin{eqnarray}
    \label{equ_9}
    y_{n}[t] = \underbrace{\widetilde{H}_{n}^{\text{bu}}[t]\omega_{n}[t]s_{n}[t]}_{\text{desired \ signal}} + \underbrace{\sum\nolimits_{i \neq n}^{N} \widetilde{H}_{n}^{\text{bu}}[t]\omega_{i}[t]s_{i}[t]}_{\text{interference}} + \underbrace{\varrho_{n}[t]}_{\text{noise}},
\end{eqnarray}

\noindent where $\varrho_{n}[t] \sim \mathcal{N}(0, \sigma_{n}^{2})$ represents the additive white Gaussian noise characterized by a variance $\sigma_{n}^{2}$ in time slot $t$.

\par Consequently, the transmission rate for the $n$-th user in time slot $t$ is given by
\begin{equation}
    \label{equ_10}
    R_{n}[t] = \log_{2}(1+\gamma_{n}[t]),
\end{equation}

\noindent where $\gamma_{n}[t]$ represents the signal-to-interference-plus-noise
 ratio (SINR) of the $n$-th user in time slot $t$, \textit{i.e.},
\begin{equation}
    \label{equ_11}
    \gamma_{n}[t] = \frac{\left|\widetilde{H}_{n}^{\text{bu}}[t]\omega_{n}[t]\right|^{2}}{\sum_{i=1, i \neq n}^{N}\left|\widetilde{H}_{n}^{\text{bu}}[t]\omega_{i}[t]\right|^{2}+\sigma_{n}^{2}}.
\end{equation}


% \subsection{Sensing Model}
% \par Notably, by sharing the spectrum resources, ISAC can use the transmitted communication signals for sensing operations~\cite{Xu2024}, thereby achieving efficient resource utilization. Given that the presence of obstacles impacts the sensing performance of the BS, an aerial IRS is deployed to reconfigure the sensing link by adjusting the phase of its reflective elements, enabling the direct target sensing services~\cite{Liao2023}. Therefore, the recieved signal at the IRS in time slot $t$ can be expressed as
% % \footnote{Like the previous work~\cite{Zuo2023}, sensors are deployed on the IRS to form a sensing-at-IRS structure, enabling the analysis of the reflected echoes of the targets.}
% \begin{equation}
%     \label{equ_12}
%     \widetilde{\mathbf{x}}_{r}[t] = \Phi[t]h^{br}[t]\left(\sum_{n=1}^{N}\omega_{n}[t]s_{n}[t]\right).
% \end{equation}

% \par Then, the target response matrix can be represented as
% % \begin{eqnarray}
% %     \label{equ_13}
% %     \mathbf{R}_{\widetilde{\mathbf{x}}}[t] &=& \mathbb{E}\left[\widetilde{\mathbf{x}}_{r}[t]\widetilde{\mathbf{x}}_{r}^{H}[t]\right]\nonumber \\ &=& \Phi[t]h^{br}[t]\left(\sum_{n=1}^{N}\omega_{n}[t]s_{n}[t]\right)(\Phi[t])^{H}(h^{br}[t])^{H}.
% % \end{eqnarray}
% \begin{equation}
% \label{equ_13}
% \begin{aligned}
%     \mathbf{R}_{\widetilde{\mathbf{x}}}[t] &= \mathbb{E}\left(\widetilde{\mathbf{x}}_{r}[t]\widetilde{\mathbf{x}}_{r}^{H}[t]\right) \\
%     &= \Phi[t]h^{br}[t]\left(\sum_{n=1}^{N}\omega_{n}[t]\omega_{n}^{H}[t]\right)(\Phi[t])^{H}(h^{br}[t])^{H}.
% \end{aligned}
% \end{equation}



% \par Moreover, we adopt the beampattern gain metric to measure the sensing performance. The beampattern gain of the IRS for the $k$-th angle of interest in time slot $t$ is defined as
% \begin{equation}
%     \label{equ_14}
%     G_{\vartheta_{k}^{ST}}[t] = a^{H}(\vartheta_{k}^{ST}[t])\mathbf{R}_{\widetilde{\mathbf{x}}}[t]a(\vartheta_{k}^{ST}[t]),
% \end{equation}

% \noindent where $a(\vartheta)=\left[1, e^{j\frac{2\pi \sin({\vartheta})}{\lambda}}, \cdots, e^{j\frac{2\pi (L-1) \sin({\vartheta})}{\lambda}}\right]^{T}$ denote the steering vector of the IRS with respect to the angle $\vartheta$, and $\mathcal{L}_{\vartheta}^{ST}[t]=\{\vartheta_{1}^{ST}[t], \vartheta_{2}^{ST}[t], \cdots, \vartheta_{L}^{ST}[t]\}$ is the set of the angles of interest in time slot $t$.

\subsection{Sensing Model}
\par Notably, ISAC can use the transmitted communication signals for sensing operations by sharing the spectrum resources~\cite{Xu2024}, thereby achieving efficient resource utilization. Given that the presence of obstacles impacts the sensing performance of the BS, an aerial IRS is deployed to reconfigure the sensing link by adjusting the phase of its reflective elements, enabling the direct target sensing services~\cite{Liao2023}. Thus, the steering vector from the IRS to the target can be expressed as~\cite{Hua2024}
\begin{equation}
    \label{equ_13}
    a[t]=\alpha[t]\left[1, e^{j\frac{2\pi}{\lambda}\sin({\vartheta^{\text{ST}}}[t])}, \cdots, e^{j\frac{2\pi (L-1)}{\lambda}\sin({\vartheta^{\text{ST}}}[t])}\right]^{T},
\end{equation}

\noindent where $\vartheta^{\text{ST}}[t]$ is the angle of interest in time slot $t$, and $\alpha[t]$ represents the large-scale fading coefficient.

% \footnote{Like the previous work~\cite{Zuo2023}, sensors are deployed on the IRS to form a sensing-at-IRS structure, enabling the analysis of the reflected echoes of the targets.}
% \begin{equation}
%     \label{equ_12}
%     \widetilde{\mathbf{x}}_{k}[t] = a_{k}^{H}[t]\Phi[t]h^{br}[t]\left(\sum_{n=1}^{N}\omega_{n}[t]s_{n}[t]\right) + \varrho_{k}^{ST}[t],
% \end{equation}

% \noindent where $\varrho_{k}^{ST}[t]$ represents the noise at the $k$-th target in time slot $t$. Moreover, $a_{k}[t]$ is the steering vector of the IRS, which is given by

% \begin{equation}
% \label{equ_13}
% \begin{aligned}
%     \mathbf{R}_{\widetilde{\mathbf{x}}}[t] &= \mathbb{E}\left(\widetilde{\mathbf{x}}_{r}[t]\widetilde{\mathbf{x}}_{r}^{H}[t]\right) \\
%     &= \Phi[t]h^{br}[t]\left(\sum_{n=1}^{N}\omega_{n}[t]\omega_{n}^{H}[t]\right)(\Phi[t])^{H}(h^{br}[t])^{H}.
% \end{aligned}
% \end{equation}

\par Then, the received signal of the target can be denoted as~\cite{Hua2024}
\begin{equation}
    \label{equ_12}
    \widetilde{\mathbf{x}}[t] = a^{\text{H}}[t]\Phi[t]h^{\text{br}}[t]\left(\sum_{n=1}^{N}\omega_{n}[t]s_{n}[t]\right) + \varrho^{\text{ST}}[t],
\end{equation}
\noindent where $\varrho^{\text{ST}}[t] \sim \mathcal{N}(0,\sigma_{\text{g}}^{2})$ is the noise received at the target. The beampattern gain of the IRS for the target in time slot $t$ is defined as

\begin{equation}
    \label{equ_14}
    \begin{aligned}
    \mathbf{G}[t] &= \mathbb{E}\left\{\left| \mathbf{g}^{\text{H}}[t]\left(\sum_{n=1}^{N}\omega_{n}[t]s_{n}[t]\right) \right|^{2}
 \right\} \\
    & = \mathbf{g}^{\text{H}}[t]\left(\sum_{n=1}^{N}\omega_{n}[t]\omega_{n}^{\text{H}}[t]\right)\mathbf{g}[t],
    \end{aligned}
\end{equation}

\noindent where $\mathbf{g}^{\text{H}}[t]=a^{\text{H}}[t]\Phi[t]h^{br}[t]$.

\par In this case, the echo rate of the target can be represented as~\cite{Xu2024}
\begin{equation}
    \label{equ_addition}
    R^{\text{ST}}[t] = \log_{2}(1+\frac{\mathbf{G}[t]}{\sigma_{\text{g}}^{2}}).
\end{equation}


\subsection{UAV Mobility and Energy Model}
\par We assume that the UAV flies at a fixed altitude $z^{\text{r}}$ within the target area for task execution. The horizontal location $\left(x^{\text{r}}[t], y^{\text{r}}[t]\right)$ of the UAV (\textit{i.e.}, aerial IRS) in time slot $t$ depends on the flight speed $v_{\text{u}}[t]$ and yaw angle $\theta_{\text{u}}[t] \in [-\pi, \pi]$, which can be denoted as
\begin{eqnarray}
    \label{equ_15}
    \begin{aligned}
    & x^{\text{r}}[t] = x^{\text{r}}[t-1] + v_{\text{u}}[t]t_{\text{d}}\cos(\theta_{\text{u}}[t]), \\
    & y^{\text{r}}[t] = y^{\text{r}}[t-1] + v_{\text{u}}[t]t_\text{d}\sin(\theta_{\text{u}}[t]).
    \end{aligned}
\end{eqnarray}

\par Due to the limited energy resources of the UAV, optimizing propulsion energy consumption can enhance endurance, thereby extending mission execution time and improving the overall operational efficiency of the system. The propulsion energy of the UAV in time slot $t$ can be expressed as
\begin{align}
    \label{equ_16}
    E_{\text{u}}[t]&=\left(P_{\text{a}}\left(1+\frac{3v_{\text{u}}^2[t]}{V_{\text{tip}}^{2}}\right) + P_{\text{b}}\left(\sqrt{1+\frac{v_{\text{u}}^{4}[t]}{4v_{\text{a}}^{4}}} -\frac{v_{\text{u}}^{2}}{2v_{\text{a}}^{2}}\right)^\frac{1}{2}\right. \nonumber \\
    &\left. +\frac{1}{2}d_{\text{a}}\rho s A v_{\text{u}}^{3}[t]\right)t_{\text{d}},
\end{align}
\noindent where $P_{\text{a}}$ and $P_{\text{b}}$ are blade profile power in hovering and induced power, respectively. Moreover, $V_{\text{tip}}$ and $v_{\text{a}}$ represent the tip speed of the rotor blade and mean rotor induced velocity during hovering. In addition, $d_{\text{a}}$, $\rho$, $s$, and $A$ denote the drag ratio, air density, rotor solidity, and disc area, respectively.


% \subsection{Problem Formulation and Analyses}

% \par First, we define the optimization variables as follows: \textit{(i)} $\mathbf{\Omega}=\{\omega_{n}[t]| n\in\mathcal{N}, t\in \mathcal{T}\}$ represents the active beamforming vectors of the BS for all users in all time slots. \textit{(ii)} $\mathbf{\Psi}=\{\Phi[t]| t \in \mathbf{T}\}$ denotes the phase shift matrix of the IRS in all time slots. \textit{(iii)} $\mathbf{\mathcal{V}}=\{v_{\text{u}}[t]|t \in \mathcal{T}\}$ represents the flight velocity of the UAV in all time slots. \textit{(iv)} $\mathbf{\Theta}=\{\theta_{\text{u}}[t]|t \in \mathcal{T}\}$ denotes the yaw angle of the UAV in all time slots.

% % \par Second, the optimization objectives are presented as follows:

% % \par \textit{Objective 1}: To enhance the communication performance of the considered system, maximizing the sum transmission rate between the BS and the users is the primary objective. Thus, the first optimization objective can be expressed as $f_{1}(\mathbf{\Omega}, \mathbf{\Psi},\mathbf{\mathcal{V}},\mathbf{\Theta}) = \sum_{t \in \mathcal{T}} \sum_{n \in \mathcal{N}} R_{n}[t]$.

% % \par \textit{Objective 2}: The echo rate of target is a widely used metric to measure the sensing performance of the ISAC system. Thus, our second optimization objective is to maximize the echo rate of the target, which is represented as $f_{2}(\mathbf{\Omega}, \mathbf{\Psi},\mathbf{\mathcal{V}},\mathbf{\Theta}) = \sum_{t \in \mathcal{T}}R^{ST}[t]$.

% % \par \textit{Objective 3}: UAV is an energy-constrained airborne platform, and reducing the energy consumption of the UAV can extend the operating time of the system. Thus, the third optimization objectives is to minimize the propulsion energy of the UAV, which is represented as $f_{3}(\mathbf{\mathcal{V}}) = \sum_{t \in \mathcal{T}} E_{u}[t]$.

% \par Then, according to the optimization variables above, we formulate the CSEMOP aiming to maximize the communication rate of users, maximize the echo rate of the target and minimize the energy consumption of the UAV, which can be represented as
% \begin{subequations}
% \label{equ_20}
% \begin{align}
%     \mathcal{P}: \ &\min_{\mathbf{\Omega}, \mathbf{\Psi}, \mathbf{\mathcal{V}}, \mathbf{\Theta}}(\sum_{t \in \mathcal{T}} \sum_{n \in \mathcal{N}} R_{n}[t], \sum_{t \in \mathcal{T}}R^{\text{ST}}[t], \sum_{t \in \mathcal{T}} E_{\text{u}}[t]) \label{Za}\\
%     \text{s.t.}: \
%     & X_{ \rm min} \leq x^{\rm r}[t] \leq X_{\rm max}, \quad \forall t \in \mathcal{T}, \label{Zb}\\
%     &Y_{\rm min} \leq y^{\rm r}[t] \leq Y_{\rm max}, \quad \forall t \in \mathcal{T},  \label{Zc}\\
%     &V_{\rm min} \leq v_{\rm u}[t] \leq V_{\rm max}, \quad \forall t \in \mathcal{T},  \label{Zd}\\
%     &-\pi \leq \theta_{\rm u}[t] \leq \pi, \quad \forall t \in \mathcal{T}, \label{Ze}\\
%     &\sum_{n=1}^{N}|\omega_{n}[t]|^{2} \leq P_{\rm max}, \quad \forall t \in \mathcal{T}, \label{Zf} \\
%     &\theta_{l}[t] \in [0, 2\pi), \quad \forall l \in \mathcal{L}, \ t \in \mathcal{T}, \label{Zg}
% \end{align}
% \end{subequations}

% \noindent where the constraints (\ref{Zb}) and (\ref{Zc}) represent the boundaries of the area within which the UAV performs the mission. Moreover, constraints (\ref{Zd}) and (\ref{Ze}) define the range of the flight speed and yaw angle of the UAV in each time slot. In addition, constraint (\ref{Zf}) indicates that the transmission power of the BS is less than $P_{\rm max}$. Furthermore, constraint (\ref{Zg}) specifies the phase shift constraint of the IRS.

% \par We can find that the formulated CSEMOP has the following natures:

% \par \textit{Non-convexity}: According to the three optimization objectives presented in Eq.~(\ref{equ_20}), it can be observed that the optimization variables $\mathbf{\Omega}$, $\mathbf{\Psi}$, $\mathbf{\mathcal{V}}$, and $\mathbf{\Theta}$ are coupled. Moreover, the phase shift $\theta_{l}$[t] appears in an exponential form in the passive beamforming vector of the IRS~\cite{Zuo2023}. Therefore, the CSEMOP is highly non-convex and challenging to solve.

% \par \textit{Trade-off}: It can be observed that there are conflicts among the three optimization objectives. For instance, under given channel conditions, focusing on the transmission rate between the BS and the users will impact the sensing capability of the BS for the target. Moreover, improving both sensing and communication performance of the system requires rapid deployment of the aerial IRS to suitable regions, which leads to increased energy consumption of the UAV. Therefore, there is a trade-off among the three optimization objectives.

\section{Problem Formulation and Analyses}
\label{sec: Problem Analyses}

\par In this section, we aim to formulate CSEMOP that focuses on the communication, sensing, and energy efficiency performance of the considered aerial IRS-assisted ISAC system. We first introduce the optimization variables and objectives. Based on this, the optimization problem is formulated and analyzed.

\subsection{Problem Formulation}
\par First, we define the optimization variables as follows: \textit{(i)} $\mathbf{\Omega}=\{\omega_{n}[t]| n\in\mathcal{N}, t\in \mathcal{T}\}$ represents the active beamforming vectors of the BS for all users in all time slots. \textit{(ii)} $\mathbf{\Psi}=\{\Phi[t]| t \in \mathbf{T}\}$ denotes the phase shift matrix of the IRS in all time slots. \textit{(iii)} $\mathbf{\mathcal{V}}=\{v_{u}[t]|t \in \mathcal{T}\}$ represents the flight velocity of the UAV in all time slots. \textit{(iv)} $\mathbf{\Theta}=\{\theta_{u}[t]|t \in \mathcal{T}\}$ denotes the yaw angle of the UAV in all time slots.

\par Then, based on the optimization variables above, the optimization objectives are presented as follows:

\par \textit{Objective 1}: Improving communication quality requires maximizing the total transmission rate between the BS and users, which serves as the primary optimization objective. Consequently, our first objective can be expressed as
\begin{equation}
    \label{equ_17}
    f_{1}(\mathbf{\Omega}, \mathbf{\Psi},\mathbf{\mathcal{V}},\mathbf{\Theta}) = \sum_{t \in \mathcal{T}} \sum_{n \in \mathcal{N}} R_{n}[t].
\end{equation}

\par \textit{Objective 2}: Echo rate is a common metric for evaluating the sensing performance in an ISAC system. Thus, the second objective aims to maximize the echo rate of the target, which is given by
\begin{equation}
    \label{equ_18}
    f_{2}(\mathbf{\Omega}, \mathbf{\Psi},\mathbf{\mathcal{V}},\mathbf{\Theta}) = \sum_{t \in \mathcal{T}}R^{\rm ST}[t].
\end{equation}

\par \textit{Objective 3}: UAV is an energy-constrained airborne platform, and reducing its energy consumption can extend operating time of the considered system. Therefore, the third objective focuses on minimizing propulsion energy, expressed as follows:
\begin{equation}
    \label{equ_19}
    f_{3}(\mathbf{\mathcal{V}}) = \sum_{t \in \mathcal{T}} E_{\rm u}[t].
\end{equation}

\par According to the optimization variables and optimization objectives above, the considered CSEMOP can be formulated as follows:
\begin{subequations}
\label{equ_20}
\begin{align}
    \mathcal{P}: \ &\max_{\mathbf{\Omega}, \mathbf{\Psi}, \mathbf{\mathcal{V}}, \mathbf{\Theta}}(f_{1}, f_{2}, -f_{3}) \label{Za}\\
    \text{s.t.}: \
    & X_{ \rm min} \leq x^{\rm r}[t] \leq X_{\rm max}, \quad \forall t \in \mathcal{T}, \label{Zb}\\
    &Y_{\rm min} \leq y^{\rm r}[t] \leq Y_{\rm max}, \quad \forall t \in \mathcal{T},  \label{Zc}\\
    &V_{\rm min} \leq v_{\rm u}[t] \leq V_{\rm max}, \quad \forall t \in \mathcal{T},  \label{Zd}\\
    &-\pi \leq \theta_{\rm u}[t] \leq \pi, \quad \forall t \in \mathcal{T}, \label{Ze}\\
    &\sum_{n=1}^{N}|\omega_{n}[t]|^{2} \leq P_{\rm max}, \quad \forall t \in \mathcal{T}, \label{Zf} \\
    &\theta_{l}[t] \in [0, 2\pi), \quad \forall l \in \mathcal{L}, \ t \in \mathcal{T}, \label{Zg}
\end{align}
\end{subequations}

\noindent where the constraints (\ref{Zb}) and (\ref{Zc}) represent the boundaries of the area within which the UAV performs the mission. Moreover, the flight speed and yaw angle in each time slot are bounded by constraints (\ref{Zd}) and (\ref{Ze}). In addition, constraint (\ref{Zf}) indicates that the transmission power of the BS is less than $P_{\rm max}$. Furthermore, constraint (\ref{Zg}) specifies the phase shift constraint of the IRS.

\subsection{Problem Properties}
\par We can find that the formulated CSEMOP has the following natures:

\par \textit{Non-convexity}: According to the three optimization objectives presented in Eqs.~(\ref{equ_17}-\ref{equ_19}), it can be observed that the optimization variables $\mathbf{\Omega}$, $\mathbf{\Psi}$, $\mathbf{\mathcal{V}}$, and $\mathbf{\Theta}$ are coupled. Moreover, IRS phase shift $\theta_{l}$[t] appears exponentially in the IRS beamforming vector~\cite{Zuo2023}. As a result, CSEMOP is highly non-convex, making it difficult to solve.


\par \textit{NP-hard}: When only the first optimization objective is considered, optimizing the IRS phase shifts while keeping $\Omega$, $\mathcal{V}$, and $\Theta$ fixed leads to a simplified problem that exhibits the characteristics of a non-convex quadratically constrained quadratic programming (QCQP) problem~\cite{Sun2024a}, which has been proven to be NP-hard~\cite{Wu2021tutorial}. Therefore, CSEMOP also belongs to the NP-hard problem.

\par \textit{Trade-off}: It can be observed that there are conflicts among the three optimization objectives. For example, focusing on the communication performance between the BS and the users will impact the sensing capability of the BS for the target under given channel conditions. Moreover, improving both sensing and communication performance of the system requires rapid deployment of the aerial IRS to suitable regions, which increases UAV propulsion energy consumption. Therefore, there is a trade-off among the three optimization objectives.



\section{Proposed Algorithm}
\label{section: Proposed Algorithm}

\par In this section, we first clarify the motivation for using DRL to solve CSEMOP. Subsequently, we reformulate the CSEMOP as a MDP. Next, we present the core principles of the conventional DDPG algorithm and the improved factors. Finally, we show the proposed GDMDDPG method and analyze the computation complexity.

\subsection{Motivation of Using DRL}
\par For traditional optimization algorithms, such as convex optimization, the optimization problem are typically decomposed into several subproblems and solved using alternating optimization algorithms, which may influence the accuracy of the final result. Moreover, aerial IRS makes the system highly dynamic, and the prior knowledge required by traditional optimization algorithms is difficult to obtain in such a dynamic environment. In addition, the considered system evolves over time, making it a significant challenge to balance short-term and long-term gains of the system during task execution. However, traditional optimization algorithms have limitations in achieving such balance. 

\par In contrast, DRL algorithms use the trial-and-error learning mechanism through interactions between the agent and the environment, and possess superior capabilities for balancing short-term and long-term rewards in the dynamic environment, providing a feasible solution to the aforementioned challenges. Therefore, we propose a GDMDDPG method to solve the considered CSEMOP.

\subsection{MDP Construction}
\par DRL is based on two fundamental components, \textit{i.e.}, the agent and environment. To model the decision-making process of the agent within its environment, the MDP serves as a mathematical foundation. MDP is represented as a five-element structure $\langle \mathcal{S}, \mathcal{A}, \mathcal{R}, \gamma, \mathcal{P} \rangle$, where each element defines a distinct aspect of the agent-environment interaction. specifically, $\mathcal{S}$ corresponds to the state space, while $\mathcal{A}$ refers to the possible actions the agent can take. The reward function $\mathcal{R}$ quantifies the immediate benefit the agent receives after performing an action in a certain state. Moreover, $\gamma$ represents the discount factor, controlling how much future rewards are weighed relative to immediate ones. In addition, $\mathcal{P}$ captures the probabilities governing state transitions after an action is performed. Within the MDP framework, $\mathcal{S}$, $\mathcal{A}$, and $\mathcal{R}$ play pivotal roles, which are explicitly detailed in the context of our formulated CSEMOP as follows:

\subsubsection{State Space $\mathcal{S}$}
\par The environment state $s[t] \in \mathcal{S}$ observed by the agent in time slot $t$ is defined as $s[t]=\{t, q^{r}[t]\}$, where $t$ represents the index of current time slot in the pre-defined time slot set $\mathcal{T}$, and $q^{r}[t]$ is the location of the aerial IRS in time slot $t$.

\subsubsection{Action Space $\mathcal{A}$} 
\par According to the observed environment state $s[t]$ in time slot $t$, the agent cantake the action $a[t]\in \mathcal{A}$, which is denoted as $a[t]=\{\omega[t], \phi[t], v_{u}[t], \theta_{u}[t]\}$. Specifically, $\omega[t]=\{\omega_{n}[t]|n\in\mathcal{N}\}$ represents the beamforming matrix of the BS for all users, and $\phi[t]$ represents the phase shifts of the aerial IRS in time slot $t$. Moreover, $v_{u}[t]$ and $\theta_{u}[t]$ denote the flight velocity and yaw angle of the UAV in time slot $t$.

\subsubsection{Reward Function $\mathcal{R}$}
\par The reward function is essential in DRL training because it quantifies the quality of the actions taken by the agent, directly influencing the learning direction and strategy selection of the agent. As such, the reward function typically aligns with CSEMOP optimization objectives, facilitating problem-solving through DRL. Accordingly, the reward function in our considered scenario is given by
\begin{equation}
    \label{equ_reward}
    r[t] = \xi_{1}\frac{R^{\rm U}[t]\times R^{\rm ST}[t]}{E_{\rm u}[t]} - PT,
\end{equation}
\noindent where $R^{\rm U}[t]=\sum_{n=1}^{N}R_{n}[t]$. Moreover, $\xi_{1}$ is a constant that adjusts the reward value to affect the convergence performance of the method. In addition, $PV$ is the penalty applied when the UAV flies out of the pre-defined target area, aiming to regulate the decisions of the agent. As such, $PV$ is defined as
\begin{equation}
    \label{equ_10}
    PV = \left \{
    \begin{array}{ll}
        p_{\rm o}, &  \text{if \ (\ref{Za}) \ and \ (\ref{Zb}) \ are \ not \ satisfied},\\
        0, & \text{Otherwise}
    \end{array}
    \right.
\end{equation}
\noindent where $p_{\rm o}$ is a positive value.

\subsection{Principles of DDPG Algorithm}
\par DDPG is a widely used DRL algorithm based on the Actor-Critic architecture and oriented to continuous action space~\cite{Lillicrap2016}, which can be regarded as an extended version of deep Q-network (DQN). In the following, we will provide a detailed explanation of the principles of DDPG algorithm.

% \par The TD3 algorithm is a widely used DRL algorithm based on the Actor-Critic architecture and oriented to continuous action space, which can be regarded as an improved version of DDPG. To address the limitations of DDPG in terms of Q-value overestimation and poor training stability, TD3 effectively mitigates these issues by introducing three key improvements, \textit{i.e.}, double-critic networks, delayed update, and target policy smoothing regularization. With these improvements, TD3 demonstrates excellent performance in solving optimization problems in different domains. In the following, we will provide a detailed explanation of the principles of TD3.

\subsubsection{Actor-Critic Framework}

\par In the DDPG algorithm, neural networks are leveraged to approximate both the policy and the Q-function. These are represented by the actor network $\mu(s|\eta)$ and the critic network $Q(s,a|\zeta)$, where $\eta$ and $\zeta$ denote the respective parameters of each network. Specifically, the actor network is responsible for generating actions based on the current state, while the critic network estimates the quality of these actions in terms of their expected long-term return. To ensure stability during training, DDPG incorporates target networks. These include the target actor network $\mu(s|\eta')$ and the target critic network $Q(s,a|\zeta')$, where $\eta'$ and $\zeta'$ represent the parameters of the target networks.


\subsubsection{Experience Replay Buffer}
\par The experience replay buffer is set up to improve the sample efficiency of DRL. Specifically, as the agent interacts with the environment, experience tuples $(s[t], a[t], r[t], s'[t])$ are generated and then retained in the buffer, which can be sampled and reused multiple times during subsequent network training, thereby enhancing data utilization. Note that consecutive experiences in the buffer tend to be strongly correlated because they are derived from successive interactions with the environment. However, training the neural network on such highly correlated data may result in unstable convergence. In this case, randomly sampling experiences from the buffer and performing network update can increase the diversity and reduce correlations of the training data, which improves the training stability.

\subsubsection{Network Update}
\par The training objective of the critic network in DDPG is to minimize the gap between the target Q-value and current Q-value, defined as the temporal difference (TD) error, which is given by
\begin{equation}
    \label{equ_25}
    \mathcal{L}(\zeta) = \mathbb{E}\left[\left(Q\left(s[t],a[t]|\zeta\right)-y[t]\right)^{2}\right], 
\end{equation}
\noindent where $y[t]$ is the target value.

\par We can approximate the loss function in Eq.~(\ref{equ_25}) by randomly sampling $B$ experience from the replay buffer for updates, which can be represented as
\begin{equation}
    \label{equ_26}
    \mathcal{L}(\zeta) = \frac{1}{B}\sum_{b=1}^{B}\left(Q\left(s_{b},a_{b}|\zeta\right)-y_{b}\right)^{2}, 
\end{equation}
\noindent where $y_{b}=r_{b}+\gamma Q(s_{b}', \mu(s_{b}'|\eta')|\zeta')$ is target Q-value
% , which can be denoted as
% \begin{equation}
%     \label{equ_27}
%     y_{b} = r_{b}+\gamma Q(s_{b}', \mu(s_{b}'|\eta')|\zeta').
% \end{equation}

\par Note that the actor network adjusts its actions based on the evaluation of the critic network. As such, the actor network can be updated as follows:
\begin{equation}
    \label{equ_28}
    \mathcal{L}(\eta) = -\frac{1}{B}\sum_{b=1}^{B}\left(Q(s_{b}, \mu(s|\eta)|\zeta)\right).
\end{equation}

\par To avoid significant changes in the target network during training, which could lead to instability in the agent learning process, the target networks are updated using a soft-update method as follows:
\begin{eqnarray}
    \label{equ_29}
    \eta' \leftarrow \varepsilon\eta + (1 - \varepsilon)\eta', \quad \zeta' \leftarrow \varepsilon \zeta + (1-\varepsilon) \zeta',
\end{eqnarray}
\noindent where $\varepsilon$ is a small constant that controls the soft-update rate.


%section
%Proposed Solution
\subsection{GDMDDPG}
\label{Sec: Proposed Improved Solution}
\par In this part, we first analyze the limitations of conventional DDPG algorithm, and then introduce the principles of the corresponding improved factors.

\subsubsection{The Limitations of Conventional DDPG}

\par First, in conventional DDPG algorithm, the actor network typically employs MLP as the core architecture due to the simplicity and ease of implementation. However, in more complex environments, the fixed MLP architecture may fails to fully capture and analyze the intricate information present in the environment states, which in turn limits the decision-making capabilities of the agent. Notably, diffusion model provides a viable solution to address the above limitations due to its strong ability in modeling complex data distributions and generating high-quality samples~\cite{10839238}. By replacing the traditional MLP with the diffusion model, the actor network can more effectively analyze the environment states, thereby generating superior actions and further enhancing the overall system performance.

\par Second, DDPG is based on deterministic policy, where the actor network directly outputs a specific action value rather than a probability distribution over actions. Such a deterministic policy limits the agent ability to explore the action space effectively. To address this issue, noise can be added to the actions generated by the actor network, introducing randomness to promote the agent exploration of the action space. Moreover, this noise perturbation mechanism helps alleviate the tendency of the DDPG to output action boundary values during early training phase, thereby enhancing the convergence speed of the training process.

\par Third, the experience replay buffer in conventional DDPG employs a random sampling strategy, which might fail to prioritize experiences that hold significant learning value, resulting in slower convergence. To address this issue, the RPER sampling mechanism, which integrates ERE and PER techniques, can be incorporated into the network update process of the DDPG. Specifically, ERE technique ensures that more recent experiences are sampled more actively. Based on this, PER technique assigns a priority to each experience sample, allowing those that contribute more to learning to be sampled with higher probability. As such, combining PER and ERE can effectively mitigate the limitations of the random sampling strategy.


% \par In conventional DDPG algorithm, the actor network typically employs MLP as the core architecture due to the simplicity and ease of implementation. However, in more complex environments, the fixed MLP architecture may fails to fully capture and analyze the intricate information present in the environment states, which in turn limits the decision-making capabilities of the agent. Moreover, DDPG is based on deterministic policy, where the actor network directly outputs a specific action value rather than a probability distribution over actions. Such a deterministic policy limits the agent ability to explore the action space effectively. In addition, the random experience sampling strategy used in conventional DDPG may overlook the important experiences with high learning value, resulting in the slower convergence. 

\par In the following, we present an in-depth explanation of the diffusion model-based actor network, noise perturbation mechanism, and RPER sampling mechanism.

% Notably, diffusion Model provides a viable solution to address the above limitations due to its strong ability in modeling complex data distributions and generating high-quality samples~\cite{10839238}. By replacing the traditional MLP with the diffusion model, the actor network can more effectively analyze the environment states, thereby generating superior actions and further enhancing the overall system performance.
% Second, DDPG is based on deterministic policy, where the actor network directly outputs a specific action value rather than a probability distribution over actions. Such a deterministic policy limits the agent ability to explore the action space effectively. 
% To address this issue, noise can be added to the actions output by the actor network, introducing randomness to promote the agent exploration of the action space. Moreover, this noise perturbation mechanism helps alleviate the tendency of the DDPG algorithm to output action boundary values during training, thereby enhancing the convergence of the training process.
% Third, the random sampling strategy used in the experience replay buffer of the DDPG algorithm may overlook the important experiences with high learning value, resulting in slower convergence. 
% To address this issue, the RPER sampling mechanism, which integrates ERE and PER techniques, can be incorporated into the network update process of the DDPG algorithm. Specifically, ERE technique ensures that more recent experiences are sampled more actively. Based on this, PER technique assigns a priority to each experience sample, allowing those that contribute more to learning to be sampled with higher probability. As such, Combining PER and ERE can effectively mitigate the limitations of the random sampling strategy.

% \par In the following, we will provide a detailed principle description of the diffusion model-based actor network, noise perturbation mechanism, RPER sampling mechanism.

\subsubsection{Diffusion Model-based Actor Network}
\par The diffusion model consists of two key processes, \textit{i.e.}, the forward process and the reverse process. In the forward process, the model gradually perturbs the original data by adding noise step by step, transforming the data into a nearly pure noise state. In the reverse process, the model learns the inverse of the noise-adding process, attempting to progressively denoising the noisy data and ultimately recovering the original data. In particular, this progressive denoising process enables the model to capture complex nonlinear relationships and dependencies in the data, thus enabling deep modeling of the data distribution. As such, diffusion model-based DDPG are better able to generate progressively  more refined and optimized decisions based on the current environment state during the exploration and exploitation, and enhance its adaptability in complex environments. Next, the detailed mathematical representation of the diffusion model is presented.
% \par Diffusion model has strong abilities in modeling complex data distributions and generating samples~\cite{10839238}. Therefore, by replacing the traditional MLP with the diffusion model, the actor network can more effectively analyze the environment state and generate high-quality actions. Specifically, the diffusion model consists of two key processes, \textit{i.e.}, the forward process and the reverse process. In the forward process, the model gradually perturbs the original data by adding noise step by step, transforming the data into an approximately pure noise state. In the reverse process, the model learns the inverse of the noise-adding process, attempting to denoise the noisy data progressively and ultimately recover the distribution of the original data. Next, the detailed mathematical representation of the diffusion model is presented.

\par \textbf{\textit{Forward Process}}: Let $x_0$ denote the original data, and assume that the forward process consists of $G$ steps, denoted as $\mathcal{G} = \{1, \dots, g, \dots, G\}$. Note that the data at the $g$-th step is expressed as $x_g$, which shares the same dimensionality as $x_0$. By gradually adding Gaussian noise to $x_0$ over $G$ steps, the sequence of data $x_1, x_2, \dots, x_G$ is obtained. As such, the distribution $q(x_{g} | x_{g-1})$ from $x_{g-1}$ to $x_{g}$ can be expressed as
\begin{equation}
    \label{equ_31}
    q(x_{g} | x_{g-1}) = \mathcal{N}(x_{g}; \sqrt{1 - \beta_{g}} \, x_{g-1}, \beta_{g} \mathbf{I}),
\end{equation}
\noindent where $\mathbf{I}$ is the identity matrix, and $\beta_{g}$ represents the noise variance at $g$-th step, which can be calculated according to Variational Posterior (VP) scheduler as follows:
\begin{equation}
    \label{equ_32}
    \beta_{g}=1-e^{-\frac{{c_1}}{G}-\frac{2g-1}{2G^{2}}({c_2}-{c_1})},
\end{equation}
\noindent where ${c_1}$ and ${c_2}$ are constant parameters.

\par From the Eq.~(\ref{equ_31}), it can be observed that $x_{g} $ depends only on $x_{g-1}$, which allows the forward process to be modeled as a Markov chain. In this case, the transition distribution $q(x_{G}|x_{0})$ from the original data $x_{0}$ to $x_{G}$ can be expressed as
\begin{equation}
    \label{equ_33}
    q(x_{G}|x_{0}) = \prod_{g=1}^{G}q(x_{g}|x_{g-1}).
\end{equation}

\par However, when the value of $g$ is large, sampling $x_{g}$ directly according to Eq.~(\ref{equ_33}) becomes computationally intensive and time-consuming. In this case, based on the mathematical relationship between $x_{0}$ and $x_{g}$, $x_{g}$ can be expressed as:
\begin{equation}
    \label{equ_34}
    x_{g} = \sqrt{{\hat{\alpha}_{g}}}x_{0} + (\sqrt{1-{\hat{\alpha}_{g}}})\epsilon,
\end{equation}
\noindent where $\alpha_{g}=1-\beta_{g}$, and $\hat{\alpha}_{g}=\prod_{i=1}^{g}\alpha_{i}$ is the product of all $\alpha_i$ ($i\leq g$). Moreover, $\epsilon \sim \mathcal{N}(0, \mathbf{I})$ represents the standard normal noise.

\par Note that the forward process is based on the original data $x_{0}$, which can be regarded as the optimal solution in the optimization problem. However, in our considered dynamic ISAC scenario, it is impractical to obtain the optimal solution in advance~\cite{Du2024a}. Therefore, we only utilize the subsequent reverse process to enhance the actor network of DDPG.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth,]{Algorithm_Framework.pdf}
    \caption{The architecture of the proposed GDMDDPG for solving CSEMOP.}
    \label{fig: structure of the proposed algorithm}
\end{figure*}

\par \textit{\textbf{Reverse Process}}: The reverse process is a step-by-step denoising procedure aimed at recovering the original data $x_{0}$ from the noisy data $x_{G}$. If the distribution $q(x_{g-1}|x_{g})$ from $x_{g}$ to $x_{g-1}$ is known, $x_{0}$ can be sampled directly. However, statistical estimates of $q(x_{g-1}|x_{g})$ involves complex data distribution calculations, which are difficult to implement in practice. To address this, a parameterized model $p_{\theta}$ is introduced to approximate $q(x_{g-1}|x_{g})$, which can be expressed as
\begin{equation}
    \label{equ_35}
    p_{\eta}(x_{g-1}|x_{g}) = \mathcal{N}(x_{g-1}; \kappa_{\eta}(x_{g}, g, d), \tilde{\beta}_{g}\mathbf{I}),
\end{equation}
\noindent where $\tilde{\beta}_{g}\mathbf{I}$ is the variance of the distribution, and $\tilde{\beta}_{g}$ can be represented as
\begin{equation}
    \label{equ_36}
    \tilde{\beta}_{g} = \frac{1-\hat{\alpha}_{-1}}{1-\hat{\alpha}_{g}}\beta_{g}.
\end{equation}
\par The mean $\kappa_{\eta}(x_{g}, g, d)$ is a deep model with parameter $\eta$, and $d$ represents the condition information. Note that $\kappa_{\eta}(x_{g}, g, d)$ can be expressed as
\begin{equation}
    \label{equ_37}
    \kappa_{\eta}(x_{g}, g, d) = \frac{\sqrt{\alpha_{g}}(1-\hat{\alpha}_{g-1})}{1-\hat{\alpha_{g}}}x_{t} + \frac{\sqrt{\hat{\alpha}_{g-1}}\beta_{g}}{1-\hat{\alpha}_{g}}x_{0}.
\end{equation}

\par Moreover, it can be observed that $x_{0}$ can be obtained according to Eq.~(\ref{equ_34}), which can be represented as
\begin{equation}
    \label{equ_38}
    x_{0} = \frac{x_{g}}{\sqrt{\hat{\alpha}_{g}}}-\frac{{\sqrt{1-\hat{\alpha}_{g}}}\text{tanh}(\epsilon_{\eta}(x_{g}, g, d))}{\sqrt{\hat{\alpha}_{g}}},
\end{equation}
\noindent where $\epsilon_{\eta}(x_{g}, g, d)$ is a deep neural network with parameter $\eta$, and its role is to generate the denoising noise at the $g$-th step based on the condition information $s$. Moreover, to prevent the denoising noise generated by $\epsilon_{\eta}$ from becoming too large and affecting the actions, the tanh function is used to scale it. However, Eq.~(\ref{equ_38}) cannot be directly used to generate $x_0$, because $\epsilon_{\eta}$ is not the same as $\epsilon$ in Eq.~(\ref{equ_34}). Instead, Eq.~(\ref{equ_38}) can be applied into Eq.~(\ref{equ_37}) to estimate the mean of the distribution, which can be denoted as
\begin{equation}
    \label{equ_39}
    \kappa_{\eta}(x_{g}, g, d) = \frac{1}{\sqrt{\alpha_{g}}}\left(x_{g}-\frac{\beta_{g}}{\sqrt{1-\bar{\alpha}_{g}}}\text{tanh}\left(\epsilon_{\eta}\left(x_{g}, g, d\right)\right)\right).
\end{equation}

\par As such, according to Eq.~(\ref{equ_39}) and Eq.~(\ref{equ_36}), the transition distribution $p_{\eta}$ is obtained. Then, the original data $x_{g-1}$ can be sampled from the distribution $p(x_{g})p_{\eta}(x_{g-1}|x_{g})$. Similar to the forward process, the original data $x_{0}$ can be obtained from the distribution as follows:
\begin{equation}
    \label{equ_40}
    p_{\eta}(x_{0}) = p(x_{G})\prod_{g=1}^{G}(p_{\eta}(x_{g-1}|x_{g})),
\end{equation}
\noindent where $p(x_{G})$ is a Gaussian distribution.

\par Therefore, the reverse process of the diffusion model can be integrated into the actor network of DDPG algorithm, sampling from the distribution $p_{\theta}(x_{0})$ to generate high-quality action. However, during the network training process, the inability to backpropagate gradients through the random variables presents a challenging issue. In this case, the reparameterization method can be used to decouple the random variable from the distribution~\cite{Du2024a}, which can be represented as
\begin{equation}
    \label{equ_41}
    x_{g-1} = \kappa_{\eta}(x_{g}, g, s[t]) + (\tilde{\beta}_{g})^{2} \odot \epsilon,
\end{equation}
\noindent where $s[t]$ represents the environment state in time slot $t$, and $\epsilon \sim \mathcal{N}(0, \mathbf{I})$. As such, by iteratively applying Eq.~(\ref{equ_41}), all $x_{g} \ (1 \leq g \leq G)$, and in particular $x_{0}$, \textit{i.e.}, the action $\widetilde{a}[t]$, based on the observed environment state $s[t]$.

\subsubsection{Noise Perturbation Mechanism}
\par Drawing from the principles of the twin delayed deep deterministic policy gradient (TD3) algorithm~\cite{Fujimoto2018}, we integrate a noise perturbation mechanism to improve the exploration ability of the agent. In this mechanism, the decision $\widetilde{a}[t]$ made by the diffusion model-based actor network at time step $t$ is modified by adding noise to obtain the actual action $a[t]$, \textit{i.e.},
\begin{equation}
    \label{equ_42}
    a[t] = \widetilde{a}[t] + \iota_{a}, \quad \iota_{a} \sim \text{clip}\left(\mathcal{N}\left(0,\hat{\sigma}\right), -b, b\right),
\end{equation}
\noindent where $\iota_{a}$ represents the noise added to the action, which is sampled from a normal distribution with a mean of 0 and a standard deviation of $\hat{\sigma}$. The clip operation restricts the noise to a range of $[-b, b]$, which prevents instability in the policy due to over-exploration.

\begin{algorithm}[t]
    \SetAlgoLined %end
	\caption{GDMDDPG}%
        \label{Algorithm1}
    \LinesNumbered
    Initialize the weights of the actor and critic networks $\eta$ and $\zeta$. Moreover, Initialize the target networks, \textit{i.e.}, $\eta' \leftarrow \eta$, $\zeta' \leftarrow \zeta$. \\
    Initialize experience replay buffer $\mathcal{B}$ and batch size $B$.\\
    Initialize the maximum number of episodes $N_{e}$ and time slot length $T$.\\
    \For{episode = \rm{1 to} $N_{e}$}{
        \For{t = \rm{1 to} $T$}{
            Observe the state $s[t]$ and initialize a random distribution $x_{G} \sim \mathcal{N}(0,1)$. \\
            \For{g = \rm{$G$ to} 1}{
                Deploy a denoising network $\epsilon_{\eta}(x_{g},g,s[t])$. \\
                Calculate the mean $\kappa_{\eta}$ and variance $\tilde{\beta}_{g}$ based on Eq.~(\ref{equ_39}) and Eq.~(\ref{equ_36}).\\
                Calculate $x_{g}$ according to Eq.~(\ref{equ_41}). \\
            }
            Obtain the action $\tilde{a}[t]$ based on the reverse process above, and obtain the action $a[t]$ according to Eq.~(\ref{equ_42}). \\
            Take the action $a[t]$, obtain reward $r[t]$ and next state $s'[t]$.\\
            Store the transition $\{s[t],a[t],r[t],s'[t]\}$ in to the replay buffer $\mathcal{B}$.\\
            Sample $B$ transitions from $\mathcal{B}$ according to the probability in Eq.~(\ref{equ_45}), and then update $\eta$ and $\zeta$ according to Eq.~(\ref{equ_28}) and Eq.~(\ref{equ_47}). \\
            Update the target networks according to Eq.~(\ref{equ_29}). \\
        }
    }
    Return the actor network.
\end{algorithm}

\subsubsection{RPER Sampling Mechanism}
\par To enhance the convergence speed and accuracy of the DDPG algorithm in the training process, we adopt a RPER sampling mechanism to sample the experiences from the replay buffer for network update~\cite{Wang2019}. Note that RPER sampling mechanism consists of two phase, \textit{i.e.}, defining sampling range and priority-based sampling.

\par \textbf{\textit{Defining Sampling Range}}: Considering that more recent experiences in the experience replay buffer are typically more informative than earlier ones, the sampling range can be gradually reduced during the update process and increase the sampling frequency of these recent experiences. Specifically, assume that during the current network update phase, a total of $U$ mini-batch updates are required. At the $u$-th update, $1 \leq u \leq U$, the most recent $F_{u}$ experiences are selected from the experience buffer for subsequent sampling, which can be represented as
\begin{equation}
    \label{equ_43}
    F_{u} = \max\{B_{\rm max} \cdot \rho^{(u\cdot\frac{1000}{U})}, F_{\rm min} \},
\end{equation}
\noindent where $B_{\rm max}$ is the capacity of the replay buffer, and $\rho \in (0, 1]$ is a parameter that represents the importance given to recent experiences. Moreover, $F_{\rm min}$ refers to the minimum value within the range of $F_{u}$. It can be observed that this technique allows for frequent sampling of recent experiences while also taking past experiences into account.

\par \textbf{\textit{Priority-based Sampling}}: Given that different experiences contribute variably to the training process, priority-based sampling can enhance training efficiency more effectively than simple random uniform sampling. In the RPER sampling mechanism, the priority of each experience sample is quantified by TD error, and experiences are sample from the sampling range defined by Eq.~(\ref{equ_43}). In this case, the priority of the $i$-th experience can be expressed as
\begin{equation}
    \label{equ_44}
    p_{i} = |\delta_{i}| + \epsilon_{p}, \quad i \in D_{F_{u}},
\end{equation}
\noindent where $\delta_{i}$ represents the TD error, and $\epsilon_{p}$ is a small positive constant used to prevent the priority of certain experiences from being zero. Moreover, $D_{F_{u}}$ represents the experience set obtained according to Eq.~(\ref{equ_43}).

\par As such, the probability of sampling $i$-th experience can be denoted as
\begin{equation}
    \label{equ_45}
    P_{i} = \frac{p_{i}^{\beta_{1}}}{\sum_{j}p_{j}^{\beta_{1}}}, \quad i, j \in D_{F_{u}},
\end{equation}
\noindent where $\beta_{1}$ is a hyperparameter that controls the influence of priority on the sampling probability.

\par However, frequent sampling of experiences with high TD errors may lead to instability in the training process. To address this issue, a sampling-importance weight is introduced, which can be expressed as
\begin{equation}
    \label{equ_46}
    \omega_{i} = (\frac{1}{B_{\rm max}}\cdot\frac{1}{P_{i}})^{\beta_{2}},
\end{equation}
\noindent where $\beta_{2}$ is a hyperparameter that controls the correction extend.

\par Therefore, the loss function of the critic network of the DDPG algorithm in Eq.~(\ref{equ_26}) can be rewritten as follows:
\begin{equation}
    \label{equ_47}
    \mathcal{L}(\zeta) = \frac{1}{B}\sum_{b=1}^{B}\omega_{b}\left(Q\left(s_{b},a_{b}|\zeta\right)-y_{b}\right)^{2}.
\end{equation}


% \begin{table}[t]
% \caption{Simulation Parameters.}
% \label{tab:parameter}
% \centering
% % \begin{tabular}{@{}llll@{}}
% \setlength{\tabcolsep}{6.3mm}{
% \begin{tabular}{@{}@{\extracolsep{\fill}}cccc@{}}
% \hline
% % \toprule
%         Parameter & Value & Parameter & Value\\
%         \hline
%         $X_{min}$ & 300 m & $Y_{max}$ & 300 m \\
%         $V_{max}$ & 20 m/s & $L_{0}$ & 0.001 \\
%         $M$ & 4 & $\eta^{bu}$ & 1 \\
%         $\alpha^{br}$ & 2.2 & $\alpha^{ru}$ & 2.2\\
%         $\alpha^{bu}$ & 4.6 & $\sigma_{n}^{2}$ & -90 dbm \\
%         $\sigma_{g}^{2}$ & -90 dbm & $L$ & 16 \\
%         $P_{s}$          & 79.85               & $P_{m}$          & 88.63               \\
% $U_{r}$          & 120 m/s             &$V_{h}$           & 4.03                \\
% $d_{0}$          & 0.6                 & $\rho_{a}$       & 1.225 kg/$\rm{m^{3}}$     \\
% $z$                & 0.05              & $G$              & 0.503 $\rm{m^{2}}$       \\
%         \hline
%         $N_{e}$ & 4500 & $\gamma$ & 0.99 \\
%         $B_{max}$ & 100000 & $B$ & 128 \\
%         $F_{min}$ & 3000 & $\varepsilon$ & 0.005 \\
%         \hline
% % \bottomrule
% \end{tabular}}
% \end{table}

\begin{table}[t]
\caption{Simulation Parameters}
\label{tab:parameter}
\centering
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4.5mm}{
% \begin{tabular}{@{}@{\extracolsep{\fill}}cccc@{}}
\begin{tabular}{@{}@{\extracolsep{\fill}}p{3mm}@{}|llll@{}}
% \setlength{\tabcolsep}{6.3mm}{
\hline
 & Parameter & Value & Parameter & Value \\
\hline
\multirow{10}{*}{\rotatebox[origin=c]{90}{Scenario}} & $X_{\min}$ & 300 m & $Y_{\max}$ & 300 m \\
& $V_{\max}$ & 20 m/s & $L_0$ & 0.001 \\
& $M$ & 4 & $\eta^{bu}$ & 1 \\
& $\alpha^{br}$ & 2.2 & $\alpha^{ru}$ & 2.2 \\
& $\alpha^{bu}$ & 4.6 & $\sigma_n^2$ & -90 dBm \\
& $\sigma_g^2$ & -90 dBm & $L$ & 16 \\
& $P_s$ & 79.85 & $P_m$ & 88.63 \\
& $U_r$ & 120 m/s & $V_h$ & 4.03 \\
& $d_0$ & 0.6 & $\rho_a$ & 1.225 kg/m$^3$ \\
& $z$ & 0.05 & $G$ & 0.503 m$^2$ \\ \hline

\multirow{3}{*}{\rotatebox[origin=c]{90}{DRL}}
& $N_e$ & 4500 & $\gamma$ & 0.99 \\
& $B_{\max}$ & 100000 & $B$ & 128 \\
& $F_{\min}$ & 3000 & $\varepsilon$ & 0.005 \\ 
\hline
\end{tabular}}
\end{table}

\subsection{Main Flow of GDMDDPG Method}

\par Fig.~\ref{fig: structure of the proposed algorithm} illustrates the architecture of the proposed GDMDDPG method. In this method, the conventional actor network of DDPG is improved by the diffusion model to facilitate the generation of diverse and high-quality actions. Moreover, the noise perturbation mechanism is introduced to further strengthen the exploration of the agent on the action space. Meanwhile, the RPER mechanism improves the algorithm training efficiency by giving higher priority to recent experiences with significant TD-error. The comprehensive steps for implementing the GDMDDPG method are provided in Algorithm~\ref{Algorithm1}.

\subsubsection{Training and Execution}
\par In our considered aerial IRS-assisted ISAC system, the BS possesses sufficient computational capability to undertake the training and deployment of the algorithm. Therefore, the BS is designated as the agent, which implements our proposed GDMDDPG method to make decisions based on the environment states. Moreover, throughout the training process, all generated experiences are retained in a buffer, which is used to update the network parameters. After undergoing sufficient comprehensive training, the BS can adaptively perform real-time operations according to the environment states through the well-trained actor network.


\subsubsection{Computation Complexity Analysis}
\par The computational complexity of training the GDMDDPG method consists of the following components~\cite{Zhang2025}:

\par \textit{Network Initialization}: GDMDDPG method contains an actor network, a critic network, a target actor network, and a target critic network. Assume that the number of parameters for the actor network and the critic network are $|\eta|$ and $|\zeta|$. Moreover, the target network and the main network have the same number of parameters. Therefore, the computation complexity of the network initialization process is $\mathcal{O}(2|\eta|+2|\zeta|)$.

\par \textit{Action Sampling}: Assume that the number of training episodes, diffusion steps, and time slots are $N_{e}$, $G$, and $T$, respectively. Then, the computation complexity of the generated action process is $\mathcal{O}(N_{e}GT|\eta|)$.

\par \textit{Experience Collection}: Assume that the complexity of the agent interacting with the environment is $V$, the complexity of the experience collection process is $\mathcal{O}(N_{e}TV)$.

\par \textit{Network Update}: The network update process consists of three parts, \textit{i.e.}, sampled experience priority calculation, actor and critic network updates, and soft updates of the target networks. Therefore, the computation complexity of the network update process is $\mathcal{O}(N_{e}T(B+2|\eta|+2|\zeta|))$.

\par As such, the computation complexity of the proposed GDMDDPG method is $\mathcal{O}(2|\eta|+2|\zeta|+N_{e}GT|\eta|+N_{e}TV+N_{e}T(B+2|\eta|+2|\zeta|)$.

% \begin{figure}[t]   
    
%   \centering
%   \subfloat[]   
%   {
%     \label{subfig:Convergence LR}\includegraphics[width=3.3in]{Pic/LR_4.pdf}
%   }
  
%   \subfloat[]   
%   {
%     \label{subfig:noise schedule function}\includegraphics[width=3.3in]{Pic/beta_2.pdf}
%   }
%   \captionsetup{font=footnotesize}
%   \caption{Convergence of the proposed GDMDDPG method with (a) different learning rates (b) different noise schedule functions. }    
%   \label{fig: LR and noise schedule functions}            
% \end{figure}

%section
%Simulation Results and Analysis
%
\section{Simulation Results}
\label{sec: Simulations}

\par In this section, we construct simulations to validate the performance of the proposed GDMDDPG method. First, the parameter settings and the comparison methods are introduced. Subsequently, the experimental results are presented and analyzed in detail.

\subsection{Simulation Setup}
\subsubsection{Environment Details}
\par We consider a target area measuring $300 \ \text{m} \times 300 \ \text{m}$, which includes one BS with four antennas that simultaneously serves three communication users and one sensing target, as well as one aerial IRS to enhance channel quality. The total runtime of the system is $100 \ \text{s}$, and each time slot is of length $1 \ \text{s}$. The initial position of the aerial IRS is $[0 \ \text{m}, 300 \ \text{m}, 40 \ \text{m}]$, and the UAV maintains a constant altitude of $40 \ \text{m}$ during the flight. Other parameters related to the considered scenario are shown in Table~\ref{tab:parameter}.

\subsubsection{Network Structure}
\par The proposed GDMDDPG method consists of a critic network and a diffusion model-based actor network. The critic network includes two hidden layers, each comprising 256 neurons and utilizing the ReLU activation function. The input layer of the diffusion model-based actor network handles three types of data, \textit{i.e.}, the reverse step index, the action from the previous step, and the current environment state. To capture temporal dynamics in the diffusion process, sinusoidal positional embeddings are applied. Moreover, the denoising network consists of two hidden layers, each containing 256 neurons with ReLU activation. In addition, more parameters about the proposed GDMDDPG are shown in Table~\ref{tab:parameter}.

% The critic network contains two hidden layers, each with 256 neurons and using the ReLU activation function. Moreover, the input layer of the diffusion model-based actor network processes three types of data, \textit{i.e.}, the reverse step index, the action sampled in the previous step, and the current environmental state, where sinusoidal positional embedding is employed to capture the temporal dynamics within the diffusion process. In addition, the actor network contains two hidden layers, each consisting of 256 neurons and activated by the ReLU function. In addition, more parameters about the proposed GDMDDPG are shown in Table~\ref{tab:parameter}.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\textwidth]{Pic/Ablation.pdf}
%     \caption{Convergence curves of different methods. (a) Average Reward. (b) Average communication rate. (c) Average echo rate. (d) Average UAV energy consumption.}
%     \label{fig:comparison results}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.95\textwidth]{Pic/Power.pdf}
%     \caption{The impact of transmit power on different optimization objectives. (a) Average Reward. (b) Average echo rate. (c) Average UAV energy consumption.}
%     \label{fig:Power}
% \end{figure*}

\begin{figure*}[t]   
    
  \centering
  \subfloat[]   
  {
    \label{subfig:Convergence LR}\includegraphics[width=3.3in, height=2in]{LR_4.pdf}
  }
  \subfloat[]   
  {
    \label{subfig:noise schedule function}\includegraphics[width=3.3in, height=2in]{beta_2.pdf}
  }
  \captionsetup{font=footnotesize}
  \caption{Convergence of the proposed GDMDDPG method with different parameters. (a) Learning rates. (b) Noise schedule functions. }    
  \label{fig: LR and noise schedule functions}            
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Ablation.pdf}
    \caption{Convergence curves of different methods. (a) Average Reward. (b) Average communication rate. (c) Average echo rate. (d) Average UAV energy consumption.}
    \label{fig:comparison results}
\end{figure*}

\subsection{Comparison Methods}

\par We evaluate the effectiveness of the proposed method by comparing it with the following methods:
\begin{itemize}
    \item \textit{Random Method}: In this method, the active beamforming matrix of the BS, the IRS phase shifts, and the UAV trajectory are set randomly.
    \item \textit{Communication-only Method}: In this method, the GDMDDPG method focuses only on the communication performance of the system, \textit{i.e.}, it only maximizes the communication rate between the BS and the users.
    \item \textit{Sensing-only Method}: In this method, the GDMDDPG method focuses only on the sensing performance \textit{i.e.}, it only maximizes the echo rate of the target.
\end{itemize}

\par Moreover, we also compare our proposed GDMDDPG method with the following State of the art DRL algorithms:
\begin{itemize}
    \item \textit{DDPG}: DDPG algorithm is used to solve the CSEMOP to verify the effectiveness of the proposed improvement factors. Note that DDPG is applied with the noise perturbation mechanism to encourage exploration.
    \item \textit{TD3}: TD3 algorithm can be considered as an enhanced version of DDPG, which employs improvements such as dual Q-network, delayed update, and noise regularization to address the limitations of DDPG.
\end{itemize}



\subsection{Simulation Results}
\par In this part, we analyze the performance of the GDMDDPG method under different setup conditions versus the comparison methods.

\subsubsection{Impact of Algorithm Parameters}
\par Fig.~\subref*{subfig:Convergence LR} shows the convergence performance of the GDMDDPG method under different learning rates. It is evident that the method performs better in terms of both accuracy and stability when the learning rate is set to $5e^{-4}$. This is because, with a smaller learning rate, the learning process of the agent becomes too slow, often leading to premature convergence to a local optimal. On the contrary, a larger learning rate makes the parameter update step of the neural network become larger, resulting in the algorithm oscillating or even not converging during the training process.

\par Fig.~\subref*{subfig:noise schedule function} shows the convergence performance of the GDMDDPG method under different noise scheduler functions. The effectiveness of the diffusion model is highly dependent on the noise schedule function, which determines how the noise level changes with the diffusion steps. Specifically, this function influences the ability of the model to learn and generate high-quality samples by controlling the noise variation in the diffusion process. To systematically assess the effect of different noise scheduler functions on the proposed GDMDDPG method, we conducted comparative experiments using three representative scheduling functions., \textit{i.e.}, VP, linear, and cosine. As can be seen, the GDMDDPG method performs better when VP is used as a noise scheduling function, which suggests that it is more helpful in solving the formulated CSEMOP.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{Power.pdf}
    \caption{The impact of transmit power on different optimization objectives. (a) Average Reward. (b) Average echo rate. (c) Average UAV energy consumption.}
    \label{fig:Power}
\end{figure*}

\subsubsection{Comparison with Other Methods}
\par Fig.~\ref{fig:comparison results} shows the convergence curves of all the methods in terms of reward and the three optimization objectives. The results show that the performance of the diffusion model-based DDPG algorithm without the noise perturbation mechanism is significantly degraded, particularly during the initial phase of training, the UAV frequently flies out of the target area. This is due to the fact that DDPG is a deterministic policy algorithm, and the noise perturbation mechanism not only enhances the exploration ability of the DDPG algorithm on the solution space, but also effectively alleviate the issue of outputting boundary actions during the early stage of training. Moreover, it can be found that the TD3 algorithm outperforms the DDPG algorithm. This improvement is attributed to the fact that TD3 mitigates the overestimation issue of Q-values in DDPG by introducing dual Q-networks and enhances training stability through target policy smoothing regularization. 

\par Compared to both DDPG and TD3, the proposed GDMDDPG method achieves superior performance across all three optimization metrics. This performance enhancement stems from three improvement factors proposed in this paper. First, the diffusion model strengthens the agent ability to analyze environment state, thereby improving decision-making quality. Second, noise perturbation mechanism enhances the exploratory capability of the agent within the action space. Finally, the RPER sampling mechanism, which integrates ERE and PER techniques, enables more efficient utilization of experience data. Specifically, it avoids forgetting past experiences while increasing the sampling frequency of recent experiences with high learning value. This experience sampling mechanism further boosts the learning efficiency of the algorithm. 

 % Moreover, the proposed GDMDDPG method shows significant advantages in optimizing all three optimization objectives. First, the introduction of diffusion model enhances the  ability of the agent to analyze the environment state, thus improving the decision quality. Meanwhile, the EPER sampling mechanism further improves the learning efficiency and overall performance of the algorithm by prioritizing high-value historical experience data for network updates. These improvements enable the GDMDDPG method to significantly enhance the decision-making ability in the complex environment.


 \subsubsection{Impact of Maximum Power $P_{max}$}
 \par Fig.~\ref{fig:Power} shows the trends of the three optimization objectives in CSEMOP with the variation of the BS maximum transmit power $P_{max}$. Experimental results demonstrate that as $P_{max}$ increases, the communication rate and target echo rate of DDPG, TD3, and the proposed GDMDDPG methods all show an upward trend. This is because the increase in $P_{max}$ enhances the signal strength, thereby improving both communication and sensing performance. Moreover, the GDMDDPG method achieves the lowest UAV energy consumption, demonstrating its effectiveness in UAV trajectory design.
 
 \par Notably, individually optimized objectives under communication-only and sensing-only methods outperform the corresponding objectives in the jointly optimized situation, which confirms that a significant trade-off relationship exists in the optimization objectives in the formulated CSEMOP. Moreover, in the communication-only method, the target echo rate approaches zero because the aerial ISR is deployed closer to the user cluster area and the passive beamforming of the IRS is fully aligned with the user, resulting in a significant attenuation of the sensing performance. In contrast, the user communication rate in the sensing-only method does not approach zero, likely due to the direct communication link between the BS and the users, which maintains a certain level of communication capability even under the sensing-priority condition. In addition, the random method performs poorly in all performance metrics due to the lack of systematic and theoretical basis for setting optimization variables by using a random strategy.

 \begin{figure}[t]
    \centering
    \includegraphics[width=3.2in]{Trajectory.pdf}
    \caption{The trajectories of the UAV. With the same spatial distribution of BS, users and target, the UAVs start from different initial locations.}
    \label{fig:Trajectory}
\end{figure}


 \subsubsection{UAV Trajectory Analysis}
 \par The schematic of the UAV trajectory under the spatial distribution of the BS, users, and target is shown in the Fig.~\ref{fig:Trajectory}. Initially, the ground location of aerial IRS is $(0 \ \text{m}, 300 \ \text{m})$. To efficiently perform the communication and sensing tasks, the aerial IRS flies toward the clustered region of the users, target and BS, and then hovers at its final location. Moreover, to validate the robustness of the proposed method, we conducted a repeated experiment with the initial location of the UAV set to $(300 
\ \text{m}, 0 \ \text{m})$. The final hovering location is similar to that of the first simulation, further confirming the consistency and stability of the GDMDDPG method.

 


%section
%Conclusion
%
\section{Conclusion}
\label{sec: Conclusion}
\par In this paper, we investigated an aerial IRS-assisted ISAC system, where a BS simultaneously provides communication and sensing services to multiple users and a target. Moreover, we formulated a CSEMOP to maximize the communication rate, echo rate while minimizing the UAV propulsion energy consumption by jointly optimizing BS beamforming matrix, IRS phase shifts, the flight velocity and angle of the UAV. Moreover, we proposed the GDMDDPG method, which improves the decision quality and learning efficiency by integrating the diffusion model, the noise perturbation mechanism, and the RPER sampling mechanism. Simulation results confirm that the proposed GDMDDPG method effectively accomplishes simultaneous optimization of communication, sensing, and energy efficiency performance, outperforming other comparison methods. In addition, GDMDDPG exhibits significant advantages in terms of robustness.




\bibliography{main}

\end{document}




