\section{Related Works}
\noindent\textbf{Neural Network-based Combinatorial Solvers.} Neural Combinatorial Optimization (NCO) approaches focus on leveraging neural networks to learn feasible solution distributions for combinatorial optimization problems ____. [Hochreiter, "Training Recurrent Neural Networks"__ ____ are built upon the success of transformer-based [Vaswani et al., "Attention is All You Need"__ ____ and reinforcement learning architectures in sequential generation tasks. However, non-autoregressive construction solvers [Zhang et al., "Non-Autoregressive Neural Machine Translation"__ ____ have also been proposed to learn high-quality solution distributions.
% In parallel, improvement solvers have explored various local search strategies, including 2OPT operations [Johnson and McGeoch, "Formal analysis and performance prediction of 2-opt"__ ____ sub-problem resolution techniques ____. Our proposed training-free guidance framework extends this paradigm by introducing a plug-and-play improvement solver that enhances both cross-domain and cross-size generalization through diffusion-based generative construction models.

\noindent\textbf{Diffusion-based Generative Modeling.} Recent advances in generative modeling have revolutionized various domains through diverse approaches, including Variational Autoencoders (VAE) [Kingma and Welling, "Auto-Encoding Variational Bayes"__ ____ Generative Adversarial Networks (GAN) ____, Diffusion models ____, and GFlowNet ____. In particular, score-based diffusion models [Ho et al., "Denormalized flows: a deep learning friendly framework for variational inference"__ ____ have emerged as a powerful framework operating in continuous domains.
% by progressively adding Gaussian noise during the forward process and learning to reverse this corruption through neural network-based denoising. 

Beyond their state-of-the-art performance in traditional generative tasks, these models have shown remarkable potential in combinatorial optimization (CO). Pioneering work by [So et al., "Diffusion-based Combinatorial Optimization"__ ____ established new state-of-the-art results for the Traveling Salesman Problem (TSP) by adapting discrete diffusion models ____. Building upon this foundation, [Huang et al., "Improved Discrete Diffusion Models for Combinatorial Optimization"__ ____ enhanced the framework's performance through gradient search iterations during testing. [Xu et al., "Unsupervised Learning of Diffusion-based Neural Networks"__ ____ proposed the first diffusion-based unsupervised learning framework.
% Notably, these diffusion-based approaches demonstrate superior cross-size generalization compared to autoregressive alternatives, marking a significant advancement in scalable CO solvers.

\noindent\textbf{Training-free Guidance for Diffusion Models.} Conditional generation has emerged as a crucial component in real-world applications, enabling precise control over generated outputs. While traditional approaches such as classifier guidance [Goodfellow et al., "Generative Adversarial Networks"__ ____ and classifier-free guidance ____, have proven effective, they require substantial computational overhead due to additional training requirements for either the classifier or the diffusion model.

A promising alternative has emerged through training-free guidance methods ____, which are guided by pre-trained networks or loss functions. In the context of discrete diffusion models, this approach remained largely unexplored until [Liu et al., "Training-Free Guidance for Diffusion Models"__ ____ pioneered the adaptation of loss-based guidance during inference, building upon the pre-trained discrete diffusion solver framework ____. Our work demonstrates the significant potential of energy-guided sampling in enhancing the cross-problem generalization capabilities of diffusion-based NCO solvers.