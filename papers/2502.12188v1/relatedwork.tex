\section{Related Works}
\noindent\textbf{Neural Network-based Combinatorial Solvers.} Neural Combinatorial Optimization (NCO) approaches focus on leveraging neural networks to learn feasible solution distributions for combinatorial optimization problems \cite{bengio2021machine, zhang2023survey}. Autoregressive construction solvers \cite{khalil2017learning, kool2018attention, kwon2020pomo, kim2022sym, hottung2021learning} are built upon the success of transformer-based \cite{vaswani2017attention} and reinforcement learning architectures in sequential generation tasks. However, non-autoregressive construction solvers \cite{joshi2019efficient, fu2021generalize, qiu2022dimes, wang2024asp, sun2023difusco, sanokowski2024diffusion} have also been proposed to learn high-quality solution distributions.
% In parallel, improvement solvers have explored various local search strategies, including 2OPT operations \cite{da2021learning, wu2021learning}, sub-problem resolution techniques \cite{li2021learning, hou2023generalize}. Our proposed training-free guidance framework extends this paradigm by introducing a plug-and-play improvement solver that enhances both cross-domain and cross-size generalization through diffusion-based generative construction models.

\noindent\textbf{Diffusion-based Generative Modeling.} Recent advances in generative modeling have revolutionized various domains through diverse approaches, including Variational Autoencoders (VAE) \cite{kingma2013auto}, Generative Adversarial Networks (GAN) \cite{goodfellow2020generative}, Diffusion models \cite{ho2020denoising}, and GFlowNet \cite{bengio2023gflownet}. In particular, score-based diffusion models \cite{ho2020denoising, song2020score, sohl2015deep, song2019generative, dhariwal2021diffusion} have emerged as a powerful framework operating in continuous domains.
% by progressively adding Gaussian noise during the forward process and learning to reverse this corruption through neural network-based denoising. 

Beyond their state-of-the-art performance in traditional generative tasks, these models have shown remarkable potential in combinatorial optimization (CO). Pioneering work by \cite{sun2023difusco} established new state-of-the-art results for the Traveling Salesman Problem (TSP) by adapting discrete diffusion models \cite{austin2021structured} to graph structures. Building upon this foundation, \cite{li2024distribution} enhanced the framework's performance through gradient search iterations during testing. \cite{sanokowski2024diffusion} proposed the first diffusion-based unsupervised learning framework.
% Notably, these diffusion-based approaches demonstrate superior cross-size generalization compared to autoregressive alternatives, marking a significant advancement in scalable CO solvers.

\noindent\textbf{Training-free Guidance for Diffusion Models.} Conditional generation has emerged as a crucial component in real-world applications, enabling precise control over generated outputs. While traditional approaches such as classifier guidance \cite{dhariwal2021diffusion} and classifier-free guidance \cite{ho2022classifier} have proven effective, they require substantial computational overhead due to additional training requirements for either the classifier or the diffusion model.

A promising alternative has emerged through training-free guidance methods \cite{bansal2023universal, chung2022diffusion, yu2023freedom, shen2024understanding}, which are guided by pre-trained networks or loss functions. In the context of discrete diffusion models, this approach remained largely unexplored until \cite{li2024distribution} pioneered the adaptation of loss-based guidance during inference, building upon the pre-trained discrete diffusion solver framework \cite{sun2023difusco}. Our work demonstrates the significant potential of energy-guided sampling in enhancing the cross-problem generalization capabilities of diffusion-based NCO solvers.