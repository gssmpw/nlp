%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{subcaption}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
%\usepackage{icml2025} 
%\usepackage[nohyperref]{icml2025}
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:


%\icmltitlerunning{Position: A Call to Rethink LLM for Mental Health—The SAFE-I Implementation and HAAS-E Evaluation Frameworks}

\begin{document}

\twocolumn[
\icmltitle{Position: Beyond Assistance – Reimagining LLMs as Ethical and Adaptive Co-Creators in Mental Health Care}
%\icmltitle{Position: A Call to Rethink LLM for Mental Health — The SAFE-I Implementation Guidelines and HAAS-E Evaluation Framework}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Abeer Badawi}{y,vec}
\icmlauthor{Md Tahmid Rahman Laskar}{x}
\icmlauthor{Jimmy Xiangji Huang}{x}
\icmlauthor{Shaina Raza}{vec}
\icmlauthor{Elham Dolatabadi}{y,vec}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{y}{Faculty of Health, York University, Toronto, Canada}
\icmlaffiliation{x}{Information Retrieval and Knowledge Management Research Lab, York University, Toronto, Canada}
\icmlaffiliation{vec}{Vector Institute, Toronto, Canada}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Abeer Badawi}{abeerbadawi@yorku.ca}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
 ]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\printAffiliationsAndNotice{}
\begin{abstract}

This position paper argues for a fundamental shift in how Large Language Models (LLMs) are integrated into the mental health care domain. We advocate for their role as co-creators rather than mere assistive tools. While LLMs have the potential to enhance accessibility, personalization, and crisis intervention, their adoption remains limited due to concerns about bias, evaluation, over-reliance, dehumanization, and regulatory uncertainties. To address these challenges, we propose two structured pathways: SAFE-\textit{i} (Supportive, Adaptive, Fair, and Ethical Implementation) Guidelines for ethical and responsible deployment, and HAAS-\textit{e} (Human-AI Alignment and Safety Evaluation) Framework for multidimensional, human-centered assessment. SAFE-\textit{i} provides a blueprint for data governance, adaptive model engineering, and real-world integration, ensuring LLMs align with clinical and ethical standards. HAAS-\textit{e} introduces evaluation metrics that go beyond technical accuracy to measure trustworthiness, empathy, cultural sensitivity, and actionability. We call for the adoption of these structured approaches to establish a responsible and scalable model for LLM-driven mental health support, ensuring that AI complements—rather than replaces—human expertise.

%This position paper argues that the responsible integration of Large Language Models (LLMs) in mental health care requires a shift from performance-driven assessments to a human-centered, ethically grounded framework using complementary AI. While LLMs have the potential to enhance accessibility and personalization in mental health support, their deployment poses risks related to bias, regulatory gaps, and misalignment with clinical standards. Through our collaboration with an e-mental health organization, where we evaluated LLMs on anonymized crisis support conversations, we found that the lack of robust developing, evaluating, and deploying frameworks with human-in-the-loop hinders their safe and effective deployment in mental health care. Thus, in this paper, we propose the ``SAFE" (Safety, Accountability, Fairness, and Ethics) Framework, a structured approach to developing, evaluating, and deploying LLMs for mental health applications. This framework extends beyond evaluations to encompass the entire lifecycle of LLM-based mental health systems, ensuring reliability, fairness, and security with AI complementarity. By advocating real-world data, open-source models, and equity-focused evaluation, we call for ethical guidelines that ensure LLMs augment, rather than replace, human counselors in mental health care. This paper calls on researchers, policymakers, and practitioners to prioritize transparency, accountability, and inclusivity in the development and deployment of AI-driven mental health solutions.
\end{abstract}
\vspace{-4mm}
\section{Introduction}
The rapid integration of Large Language Models (LLMs) into mental health presents an unprecedented opportunity to enhance the accessibility, personalization, and scalability of mental health support \cite{Bedi2024Evaluation}. Yet, the global shortage of mental health professionals poses a significant barrier to care. According to the World Health Organization's mental health atlas \cite{WHO2021MentalHealthAI}, the global median number of mental health workers is 13 per 100,000 people - equivalent to a stadium filled with 8,000 individuals, yet only one professional available to provide support. This disparity highlights the urgent need for innovative solutions to bridge the gap in mental health care delivery.

Despite the rapid advancements of AI in healthcare and the urgent demand for mental health solutions \cite{DAlfonso2020AI}, recent reports \cite{MIT_GE_Healthcare_2024} highlight that mental health analytics remains one of the least deployed AI products and services. A survey of over 900 healthcare professionals found that while AI adoption is prevalent in electronic health records automation (63\%), medical imaging (64\%), and patient analytics (62\%), its integration into mental health analytics is significantly lower (48\%). Additionally, only 21\% of healthcare institutions have adopted AI for mental health, with another 27\% considering adoption, making it one of the least prioritized areas of AI implementation \cite{MIT_GE_Healthcare_2024}.

The under-utilization of AI in mental health is not merely a technological issue but a reflection of deeper concerns surrounding trust, ethical considerations, and the preservation of human expertise \cite{Hamdoun2023AI}. As LLMs become increasingly sophisticated, the mental health community faces a critical challenge: how to leverage their transformative potential while upholding the human-centered principles that define effective care \cite{obradovich2024opportunities}. This tension is further exacerbated by the ability of LLMs to mimic human interaction and generate seemingly personalized responses, which may lead individuals to overestimate the depth of understanding these models possess \cite{sharma2020computational}. Such dynamics can result in undue trust in LLM outputs, potentially neglecting other forms of support or treatment \cite{hua2024llms_mental_health}. These concerns are shared by both individuals seeking mental health support and the professionals providing it, creating resistance and uncertainty around AI integration \cite{sobaih2025unlocking}. Without a clear framework to ensure complementarity between AI and human-led interventions, these technologies risk being underutilized or misapplied, undermining their potential to augment mental health. Despite these challenges, early applications of human-AI collaboration demonstrate promising results. For instance, HAILEY \cite{sharma2023human}, a system designed to enhance empathy in peer-to-peer mental health support, has shown that conversations co-authored by AI are consistently rated as more empathic and supportive than human-only interactions. %This highlights the practical potential of LLMs to enhance mental health support networks when integrated thoughtfully and ethically. However, the path to effective integration is not without obstacles.

However, the deployment of LLMs in mental health care remains fraught with technical and ethical challenges. Studies reveal that these models often exhibit demographic biases, producing less empathetic or even harmful responses when interacting with underrepresented groups \cite{zack2024gpt4_biases_healthcare, raza2024exploring}. Furthermore, proprietary models, such as ChatGPT 3.5, have demonstrated unsafe triage rates, misclassify urgent mental health crises, and potentially delay critical care—raising serious concerns about their reliability in high-stakes scenarios \cite{Fraser2023}. The absence of robust frameworks for development, evaluation, and deployment makes it difficult to ensure the effectiveness and safety of these tools. Accordingly, this paper proposes a path forward, redefining the role of LLMs in this sensitive domain through collaborative, ethical, and adaptive AI–human partnerships.
 % These limitations underscore the need for rigorous evaluation frameworks and ethical safeguards to ensure that LLMs enhance, rather than compromise, the quality and safety of mental health support."

%Nonetheless, the FAIIR model, developed and evaluated using over 700,000 anonymized crisis support conversations in collaboration with an e-mental health organization, further underscores these challenges \cite{faiir2024}. While the model demonstrates the potential of LLMs to provide patient-centered support, the findings reveal that our ecosystem remains unprepared for the widespread adoption of such technologies. The absence of robust frameworks for development, evaluation, and deployment makes it difficult to ensure the true effectiveness and safety of these tools. Accordingly, this paper proposes a path forward, redefining the role of LLMs in this sensitive domain through collaborative, ethical, and adaptive AI–human partnerships.

\paragraph{Our position} This paper argues that LLMs have reached a pivotal stage where their implementation and evaluation of mental care is both viable and necessary. We advocate for reimagining LLMs as \textbf{active co-creators rather than passive assistants, emphasizing supportive, collaborative, ethical, and adaptive AI-human partnerships that enhance - rather than replace - human-led mental health support.}

In our view, LLMs should evolve as dynamic and adaptive tools to enhance healthcare providers' experience through iterative learning, personalization, and interpretability. This paradigm shift recognizes the deeply personal, emotional, and high-risk nature of mental health care, ensuring that LLMs complement human expertise while addressing the unique challenges of this domain. To achieve this, we argue the need for ethical data practices, open-source models, and human-AI collaboration to ensure safety and accountability. We propose reframing the role of LLMs as \textit{augmentative} rather than \textit{autonomous}, with implementation and evaluation frameworks that move beyond narrow technical metrics to encompass trustworthiness, empathy, cultural sensitivity, and the ability to drive meaningful, actionable outcomes.

This position paper makes the following key contributions:
\begin{itemize}
    \item \textbf{Comprehensive Analysis of Prior Work and Alternative Viewpoints} We offer a critical examination of existing LLM applications in mental health by identifying their strengths, limitations, and alternative perspectives.
    % examining existing LLM applications in mental health, identifying their strengths, limitations, and alternative viewpoints.
    \item \textbf{Identification of Key Challenges and Gaps} that hinder the responsible deployment of LLMs in mental health, including: (1) the necessity of ethical and diverse data foundations, (2) the need for robust model engineering with adaptive optimization, and (3) the absence of %multidimensional 
    human-centered evaluation frameworks.
    \item \textbf{Proposing the SAFE-\textit{i} (Supportive, Adaptive, Fair, and
Ethical Implementation) Guidelines} to ensure LLMs function as supportive, adaptive, fair, and ethical implementation co-creators in mental healthcare. The structured approach is built on three core pillars: Ethical Data Foundations, Model Engineering, and Real-World Integration as shown in Figure \ref{fig:system2}.
    \item \textbf{Introducing the HAAS-\textit{e} ((Human-AI Alignment and Safety Evaluation) Framework} to rigorously assess LLMs in mental health using a multidimensional approach. It defines four core evaluation criteria—trustworthiness, fairness, empathy, and helpfulness—operationalized through four novel quantitative metrics that measure alignment with human expertise, cultural sensitivity, personalization effectiveness, and actionability. Additionally, it integrates four validation methods—randomized trials, multi-method evaluations, red teaming, and adversarial testing—to ensure safety, accountability, and real-world applicability as shown in Figure \ref{fig:system2}.
\end{itemize}

%TO-ADD-THIS: While model evaluation is inherently a part of model implementation, we have dedicated a separate section to it (Section X) to provide a comprehensive and detailed exploration of evaluation principles.

%Through these contributions, we establish a structured pathway for ethical and effective LLM adoption in mental health, ensuring AI complements rather than replaces human expertise, preventing potential harm, and ensuring the well-being of individuals in vulnerable mental states.

%We provide to the healthcare providers, organizations, and stakeholders the
%SAFE-I implementation framework, which prioritizes ethical data practices, open-source models, and human-AI collaboration to ensure safety and accountability
%Furthermore, the HAAS-E evaluation framework should be used to assess LLMs beyond mere accuracy, focusing on trustworthiness, empathy, cultural sensitivity, and actionability


%The under-utilization of AI in mental health care necessitates \textcolor{red}{ED: to revise at the end so it is aligned with our position: a structured, ethical, and accountable framework to ensure AI adoption is safe, fair, and transparent}. As crisis response organizations increasingly explore LLM-based conversational AI tools, early evidence demonstrates both their transformative potential and significant risks. Nonetheless, evaluating LLMs for their applicability in a critical domain like mental health is not trivial. This is because contrary to traditional natural language processing (NLP) tasks, mental health interactions are deeply contextual and emotionally nuanced, which may vary significantly across individuals. Therefore, it is important to ensure multidimensional evaluation of LLMs based on diverse criteria (e.g., empathy, helpfulness, bias, ethics, etc.) to ensure their reliable utilization in a critical domain like mental healthcare. % could be useful to address these gaps, human evaluation remains inconsistent—influenced by subjective interpretations and contextual variations. Thus, a comprehensive evaluation framework is also needed to ensure the development and deployment of a reliable and robust model for mental healthcare.  %These challenges are further compounded by gaps in regulatory frameworks, leaving questions of responsibility—data sources, model developers, or governing bodies—unanswered~\cite{lawrence2024llm_mental_health}
%Moreover, %assessing
%evaluating the performance of LLMs in mental health contexts, especially in conversational data, is uniquely challenging and subjective, as there is no universal ground truth for evaluating conversational AI. 
%Moreover, evaluating the performance of LLMs in mental health contexts is uniquely challenging and subjective. This is because 
%Contrary to traditional natural language processing (NLP) tasks, mental health interactions are deeply contextual and emotionally nuanced, which may vary significantly across individuals. This complexity increases the risk of hallucinations, biases, and unreliable decision-making. %, making standardized evaluation difficult. % However, %human evaluation remains inconsistent—influenced by subjective interpretations and contextual variations. Moreover, 


%The ability of LLMs to mimic human interaction and provide seemingly personalized responses could lead individuals to believe that these models have a deeper understanding of their needs and experiences than they actually do. This could result in users placing undue trust in LLM outputs and potentially neglecting other forms of support or treatment \cite{hua2024llms_mental_health}. 
%This creates concerns over AI replacing human roles. These apprehensions are shared by both individuals seeking mental health support and counselors providing it, creating resistance and uncertainty around AI integration. %Without a clear pathway to achieve complementarity between AI and human-led interventions, these technologies risk being underutilized or misapplied, despite their potential to augment mental health care. 

%Therefore, it is recommended to use AI as a complement to humans instead of solely relying on AI in a critical domain like mental health. One such prominent application is HAILEY~\cite{sharma2023human}, a system enabling human-AI collaboration to enhance empathy in peer-to-peer mental health support. The study demonstrated that conversations co-authored by HAILEY were consistently rated as more empathic and supportive than human-only interactions, showcasing the practical impact of such models in mental health support networks~\cite{sharma2023human}.


%While these models can facilitate real-time triage, intent classification, and empathetic dialogue generation, their deployment still remains fraught with challenges \cite{Pfohl2024toolbox}. For instance, \citet{zack2024gpt4_biases_healthcare} show that LLMs have demographic biases when utilized within the mental health domain, showing that these models often produce less empathetic or even harmful responses when interacting with underrepresented groups, which also draws ethical concerns. Additionally, it has been observed that models like ChatGPT 3.5 exhibit unsafe triage rates, misclassify urgent mental health crises, and potentially delay critical care—posing serious risks to patients and raising concerns about their reliability in high-stakes scenarios \cite{Fraser2023}. 


%\textcolor{red}{Nonetheless, through a collaboration with an e-mental health organization, where we developed and evaluated various LLMs—including supervised encoders and autoregressive decoders—on over 700,000 anonymized crisis support conversations \cite{faiir2024} via leveraging open-source models, it became evident that our ecosystem is still not ready for the adoption of these technologies. While these models exhibit remarkable capabilities for being patient-centered, the absence of robust frameworks for the development, evaluation, and deployment of these models makes it difficult to determine their true effectiveness and safety.}



%This position paper advocates for a human-centered, ethical framework—the "SAFE" framework (Safety, Accountability, Fairness, and Ethics)—for integrating Large Language Models (LLMs) into mental healthcare. We highlight the potential benefits of LLMs in increasing accessibility and personalization but emphasize the significant risks of bias, inadequate regulation, and insufficient real-world evaluation. We argue against simply focusing on performance metrics and instead propose a structured approach encompassing the entire lifecycle of LLM development and deployment, prioritizing human-AI collaboration and open-source models to ensure responsible and equitable use. This paper addresses counterarguments questioning the suitability of LLMs in mental health, ultimately advocating for their use as tools to augment, not replace, human counselors.

%To address the above concerns, this paper argues that the field of machine learning must urgently address the broader implications of LLM adoption, particularly in sensitive domains like mental health. We propose that effective regulation and evaluation frameworks are as critical as technical advancements in data and modeling. This perspective challenges the prevailing focus on performance benchmarks and calls for a shift toward more robust, ethical, and human-centered approaches to AI system design and deployment. We consider risk identification and mitigation strategies by identifying specific risks associated with utilizing LLMs in mental health, including the provision of inappropriate advice and the exacerbation of mental health issues, and provide actionable strategies to mitigate them. We also advocate for interdisciplinary collaboration between AI researchers, clinicians, and policymakers to create evaluation standards that align with clinical and ethical priorities.

 
%LLMs operate at an unprecedented scale, yet 
%Therefore, it is important to ensure safe and responsible development of LLMs for mental health applications. 
%Furthermore, these ChatGPT-like closed-source LLMs are only accessible using their respective APIs. This creates privacy and security concerns due to the risk of sharing sensitive data with these LLM providers, making it unable to further fine-tune these models to mitigate their existing limitations.  % performance. %These challenges are further compounded by gaps in regulatory frameworks, leaving questions of responsibility—data sources, model developers, or governing bodies—unanswered~\cite{lawrence2024llm_mental_health} 

%Contrary to traditional natural language processing (NLP) tasks, mental health interactions are deeply contextual and emotionally nuanced, which may vary significantly across individuals. 
%In a critical domain like healthcare, it is important to leverage AI technologies that are reliable and ensure robustness in diverse scenarios, alongside demonstrating fairness and equity across various demographics, while preserving privacy concerns. Since closed-source LLMs have many of these limitations,  developing AI technologies for a critical domain like healthcare via leveraging open-source models could address many of these existing concerns related to lack of fairness and reliability, as well as privacy concerns. Therefore, the utilization of open-source models would help ensure transparency and community scrutiny, enabling robust model development. % and performance evaluation, to ensure reliability. 


% \textcolor{red}{please mention as early after 2-3 paras, do not detrack reviewers}

%Despite the system’s ability to classify issues and generate insights with high-performance metrics, challenges such as biases, unaddressed emerging topics, and the need for human oversight highlighted critical gaps. These findings underscore the urgent need for an evaluation framework that can systematically assess the reliability, ethical considerations, and contextual applicability of LLMs in mental health contexts. This conclusion forms the basis for our proposed framework, aimed at addressing these pressing challenges while ensuring the responsible integration of LLMs into sensitive domains like mental health.

%Current approaches to integrating LLMs in mental health predominantly focus on optimizing technical performance metrics, such as accuracy and response time. However, these methods often fall short of addressing critical ethical and equity challenges:

%Many studies evaluate LLMs using simulated datasets, which fail to capture the complexities of real-world mental health interactions. In contrast, our framework emphasizes the use of diverse, real-world data to ensure ecological validity~\cite{Bedi2024Evaluation}. Existing evaluation methods rarely include robust mechanisms for detecting and mitigating biases. Our framework incorporates demographic-aware prompting and multifactorial bias audits to address this gap~\cite{Pfohl2024toolbox}. Furthermore, few approaches involve clinicians or policymakers in the development process. Our framework advocates for a human-centered design, fostering collaboration to align AI outputs with clinical and ethical priorities~\cite{lawrence2024llm_mental_health}.

% \textcolor{blue}{\textbf{This paper addresses the research question:} How can LLMs be responsibly integrated into mental health care while addressing their potential harms, ensuring equity, and maximizing their complementary role alongside human-led interventions and healthcare regulations? This question includes consideration of who is accountable for potential harms: the data, the models, or the regulations.}


%\textcolor{red}{this is not right way of contributon for a position paper, firs ttell we take position ---- and we present alternate view in section ----. then second cotnribution is ur safe framework , combine below points into 1 , third one : say some contribution how ur work can be used bu ehealth community}
%\textbf{(i) SAFE Development Phase:} The acquisition of real-world data from diverse sources, ensuring stringent data privacy policy, and focusing on leveraging open-source LLMs. % a model development pipeline that prioritizes open-source LLMs with a focus on reliability and robustness via %on empathy 
%training %, fine-tuning
%on mental health data, and using chain-of-thought reasoning for accuracy and interoperability; 

%\textbf{(ii) SAFE Evaluation Phase:}
%Implementing a multi-dimensional approach to assess the performance of the model, in terms of diverse criteria, such as accuracy, empathy, and ethical considerations, with bias detection methods and expert oversight, etc. 

%\textbf{(iii) SAFE Deployment Phase:} Prioritizing human-centeredness, promoting human-AI collaboration where LLMs augment, not replace, human counselors, and using red teaming to simulate risks; and continuous monitoring and improvement, including real-time performance tracking, user feedback, and regular updates, while adhering to ethical standards. 

\begin{figure*}[!h]
    \centering
    \includegraphics[width=17cm, height=9cm]{systemf.png}
  %\vspace{-2mm}
    \caption{The proposed SAFE-\textit{i} Implementation Guidelines and HAAS-\textit{e} Evaluation Framework}
 \label{fig:system2}
\end{figure*}

\section{Alternative Views on LLMs in Mental Health}

\paragraph{AI Cannot Replicate Human Emotional Intelligence.} Some researchers argue that LLMs, despite advances in empathetic response generation, lack the depth of understanding, lived experience, and contextual sensitivity required for mental health support. Unlike trained professionals, AI models may misinterpret complex emotional cues, potentially leading to %misaligned or 
harmful advice  \cite{Montemayor2022EmpathicAI}.\\
\textbf{Response}: LLMs can be designed to operate within well-defined boundaries, providing initial support, triage, or supplemental resources while flagging complex cases for human intervention. If we leverage domain-specific models and expert-guided annotations, LLMs can be tuned to recognize nuanced emotional cues \cite{Yang_2024} and defer high-risk or ambiguous situations to human responders \cite{sharma2023human}. Moreover, continuous evaluation of an LLM's ability to align with human emotional understanding ensures that AI tools remain supportive and safe, complementing rather than competing with human emotional intelligence \cite{stade2024_behavioral_healthcare}.

\paragraph{The Risk of Over-Reliance and Dehumanization.} LLMs also create a false sense of human-like understanding, leading users to overestimate their reliability. There is concern that increased reliance on AI-driven mental health solutions may reduce the role of human therapists and crisis responders, leading to depersonalization of care \cite{DeChoudhury2023LLMHealth}. For instance, vulnerable individuals might develop trust in AI-based therapeutic tools, potentially substituting them for human therapists, increasing the risk of social isolation.
If organizations prioritize AI over human-led interventions due to cost or scalability, the quality of support may decline, especially for individuals who need deeper, long-term engagement. \\
\textbf{Response}: To mitigate over-reliance, it is essential to implement LLMs as complementary tools rather than replacements for human therapists  \cite{sharma2023human}. %User education on the limitations combined with implementing personalized care strategies and feedback mechanisms ensure that they adapt to individual needs while encouraging users to seek human support when necessary 
Educating users on limitations, personalizing care strategies, and integrating feedback mechanisms ensure adaptation to individual needs while also encouraging users to seek human support when necessary \cite{strong2024towards}. LLM systems can provide initial support when we integrate safety nets and escalation protocols while ensuring high-risk cases are promptly addressed by qualified professionals.
 
\paragraph{Regulatory and Safety Uncertainties.} Some experts advocate against LLM integration in mental health until robust regulatory frameworks are in place. The lack of standardized safety measures %and accountability mechanisms 
raises ethical concerns, particularly regarding potential harm 
if AI-generated responses are inaccurate or inappropriate \cite{Tavory2024}.\\
\textbf{Response}: A comprehensive regulatory framework is crucial for the safe deployment and reliable evaluation of LLMs in mental health \cite{stade2024_behavioral_healthcare}. This includes establishing standardized safety protocols for data, including real-time monitoring and adversarial testing, which can help identify and address potential risks \cite{de2025artificial}. Furthermore, accountability mechanisms, such as continuous performance evaluation and stakeholder feedback loops, ensure that LLMs adhere to ethical guidelines and remain aligned with the needs of users and professionals \cite{ferrara2023fairness,hogg2023stakeholder,kaye2024moving}.

%Despite these concerns, this paper argues that LLMs should not be dismissed outright. Instead, their role must be carefully defined to augment rather than replace human counselors. %The proposed Frameworks in this work directly address these challenges by prioritizing human-AI collaboration, ethical oversight, and evaluation standards, ensuring responsible and equitable deployment in mental health care.



%As a position paper, this work advocates for a collective rethinking of how accountability, trust, and equity are built into large language model systems. By examining mental health support as a case study, we highlight the need for interdisciplinary collaboration and standardized evaluation practices to guide responsible LLM integration. This contribution aims to stimulate debate on AI adoption's ethical, regulatory, and societal dimensions, offering a roadmap for the machine learning community to proactively shape the future of high-impact applications. We emphasize the need for a holistic approach to designing and evaluating LLMs that goes beyond traditional performance metrics, integrating ethical considerations and user-centered perspectives into the core of these technologies. We propose that effective regulation and evaluation frameworks are as critical as technical advancements in data and modeling. This perspective challenges the prevailing focus on performance benchmarks and calls for a shift toward more robust, ethical, and human-centered approaches to AI system design and deployment.


%\textcolor{blue}{\textbf{Our Position:} We take the position that the responsible integration of LLMs into mental health care requires a paradigm shift toward equity-focused evaluation frameworks that prioritize accountability, transparency, and inclusivity.}

%\textcolor{red}{We take the position for equity-focused evaluation frameworks that ensure accountability, transparency, and inclusivity as the cornerstone of responsibly integrating LLMs into mental health care}

%This paper proposes an evaluation framework for LLMs in mental health that integrates ethical considerations into traditional performance metrics. 


%By introducing a practical system that aligns AI with clinical workflows and multidisciplinary standards, this work offers actionable strategies for integrating LLMs into mental health care. We aim to foster a human-centric, equitable approach that enhances mental health support while safeguarding against harm. The paper's novel contribution lies in its holistic approach, addressing technical, ethical, and practical challenges to guide the development of safer and more effective LLM-based mental health support systems. The framework ensures LLMs are used responsibly, ethically, and equitably while prioritizing user autonomy and well-being.



\vspace{-2mm}
\section{Prior Efforts in LLM-Powered Applications for Mental Health: A Landscape}
%\section{The Status Quo of Transforming Mental Health Support with Large Language Models}

The growing demand for mental health services, exacerbated by the COVID-19 pandemic~\cite{Hamdoun2023AI}, has led to the exploration of generative AI technologies in various mental health applications \cite{Zhang2023GenAI,C2023ChatGPT}. 
One of the core technologies used in the Generative AI domain is LLMs, such as ChatGPT and GPT-4 \cite{openai2023gpt4}, which utilize billions of parameters to generate coherent, contextually appropriate responses in mental health dialogues~\cite{guo2024large,J2024Generative}. %leveraging datasets such as clinical notes and mental health forums 
 LLMs have been effectively applied in various application areas of mental health, such as crisis intervention \cite{faiir2024,sharma2024selfguided}, therapy recommendations~\cite{Wilhelm2023,M2024overview,berrezueta2024adhd}, etc. %and conversational support .
 Other applications of LLMs in mental healthcare include the work of \citet{perlis2024bipolar}, where they showed GPT-4 aligns with expert bipolar depression management, while ~\citet{lee2024gpt4} found GPT-4 had comparable sensitivity to clinicians in predicting suicidal ideation from intake data.
%For instance, \citet{liu2024positive} found ChatGPT to be efficient in positive psychology interventions, demonstrating that personalized, adaptive dialogues and real-time feedback significantly improved user outcomes. 
%~\citet{sharma2024selfguided} evaluated a large-scale language model-driven cognitive restructuring tool, finding it effective for reducing emotional intensity and reframing thoughts, while emphasizing the need for simplified interventions to support adolescents effectively. 
% ~\cite{DAlfonso2020AI,C2023ChatGPT}
%subsection{Recent advances}
%\section{Large Language Models for Mental Health Support}
%Mental health issues are highly prevalent globally, with conditions such as depression, anxiety, and suicidal ideation affecting millions of people. The COVID-19 pandemic has exacerbated these issues, leading to a further decline in mental well-being across populations~\cite{Hamdoun2023AI}. The growing demand for mental health services has outpaced available resources, necessitating scalable, efficient, and accessible solutions~\cite{Thieme2023FoundationMI}. In recent years, advancements in generative AI opened up many opportunities for mental health applications, where models are being developed to support a range of systems, from crisis intervention to personalized therapy recommendations~\cite{M2024overview, Zhang2023GenAI}. 
%The state-of-the-art applications in this area focus on AI responses to classify and understand mental health problems, which are critical elements in mental health interactions. Multiple studies employ different datasets such as clinical notes and mental health forums to improve the model's recognition of mental health conditions patterns, which in turn contribute to both diagnostic and therapeutic performance~\cite{DAlfonso2020AI}. Such extensive training allows these models to develop a broad understanding of language, making them well-suited for applications like conversational support, classifying mental health problems, and empathy simulation in mental health contexts~\cite{C2023ChatGPT}.
%Van Heerden et al. (2023)\cite{vanHeerden2023} also highlight the potential of LLMs to address global mental health service gaps by aiding diagnosis, treatment, and training non-specialists. They emphasize the need for oversight and equitable AI deployment to ensure responsible integration into mental health systems.
%A study conducted empirical evaluations of LLM responses in mental health support contexts, finding that while GPT-4 showed promise in providing empathetic responses, it exhibited concerning demographic biases ~\cite{gabriel2024airelatetestinglarge}.
%For ADHD therapy, a study explored ChatGPT's integration into ADHD, highlighting its potential to enhance nonpharmacological treatments through empathy and adaptability, though improvements in privacy and cultural sensitivity are needed~\cite{berrezueta2024adhd}. 
%Perlis et al. demonstrated that GPT-4, when prompted with treatment guidelines, could align with expert recommendations for bipolar depression management, emphasizing the need to mitigate risks of overreliance and bias~\cite{perlis2024bipolar}. Other applications of LLMs in mental health include the work of ~\citet{lee2024gpt4}, where they compared the ability of GPT-4 and expert clinicians to predict suicidal ideation with a plan among telemental health patients based on intake data. While clinicians exhibited higher precision, GPT-4 demonstrated comparable sensitivity, particularly when additional patient history was included. %For cognitive reframing, a study validated a language model-based system in a randomized field study, showing that empathic and specific reframes are most effective for reducing emotional intensity and overcoming negative thoughts~\cite{sharma2023cognitive}.
Moreover, domain-specific LLMs have also gained a lot of attention recently in the mental healthcare domain \cite{Yang_2024}. For instance, the Serena model, \cite{L2023Deep} is developed as an effective counselor and 
demonstrates enhanced relevance and sensitivity toward therapeutic approaches~\cite{L2023Deep} with just 2.7 billion-parameters. 
More recently, \citet{Guo2024} introduced SouLLMate, an adaptive LLM system integrating Retrieval-Augmented Generation (RAG), suicide risk detection, and proactive dialogues to enhance accessibility in mental health support. 

In mental health applications, conversational AI tasks represent a major application area, with chatbots designed to engage users in text-based therapeutic conversations or monitor mental well-being \cite{liu2024positive}. For example, the chatbot Woebot, which uses cognitive-behavioral techniques, has shown efficacy in alleviating symptoms of depression and anxiety by delivering timely interventions~\cite{Fitzpatrick2017Woebot}. The SuDoSys chatbot \cite{Chen2024}, which is based on WHO’s PM+ framework, ensures structured multi-turn psychological counseling with coherent stage tracking. The Coral framework proposed by \citet{Harsh2021Coral} is designed to integrate conversational agents in mental health applications. 
For the evaluation of LLMs in clinical conversations, \citet{Johri2025} present CRAFT-MD, an evaluation framework assessing diagnostic reasoning in clinical LLMs, highlighting limitations of LLMs in conversational accuracy and the need for multimodal integration before deployment.


%Some research also focuses on creating AI-powered tools to support crisis responders (CRs) in mental health services for youth. For instance, the FAIIR tool is designed to assist CRs by categorizing issues during conversations, thus improving the efficiency and accuracy of youth mental health support recommendations ~\cite{faiir2024}. Similarly, systems using generative AI in focused therapeutic interventions, such as cognitive-behavioral therapy,show promising results in helping users build coping skills and improve their mental resilience~\cite{undefined2021Novel}. 


  %Another practical application in crisis intervention is developing a natural language processing system significantly reducing response times to mental health crisis messages~\cite{Swaminathan2023NaturalLP}. %The authors highlight the potential of conversational agents (CAs) in providing scalable mental health support and offering personalized responses to individuals in need. 
%Additionally, recommender systems are being incorporated into mental health applications to suggest personalized self-care activities or connect users with relevant mental health resources based on their interaction history and reported symptoms~\cite{Valentine2022Recommender}. %Such systems are crucial for maintaining user engagement and offering targeted support, but they raise ethical concerns such as privacy, fairness, and transparency that must be carefully managed. 
%Another example is XAIA, a VR-based AI assistant combining spatial computing with GPT-4, which participants found helpful and safe for addressing mild anxiety and depression, albeit less preferred compared to human therapists~\cite{spiegel2024vr}. 

%Despite the progress, LLM evaluations remain fragmented, often relying on simulated data rather than real-world patient interactions. The rapid integration of LLMs into mental health care exposes critical gaps in ethical data sourcing, safe deployment, and comprehensive evaluation, forming the basis for the three major challenges discussed in the next section

\vspace{-2mm}

\section{Key Challenges in Utilizing LLMs for Mental Health}

%\textcolor{red}{PLEASE REMOVE this paragraph and add reference somewhere ele in the paper, e.g., introduction: }LLMs hold immense promise for addressing global mental health challenges through scalable and personalized solutions. However, their deployment faces barriers, including a lack of concrete examples of their application using real-world counseling services or hospitals, integration with clinical workflows, and ethical concerns ~\cite{DAlfonso2020AI}. Studies caution about the risks of using generative AI for mental health, including inappropriate responses to crises and potential negative user reactions~\cite{De2023Chatbots, Sackett2024Do}. Other challenges include ensuring clinical safety and privacy and addressing potential biases~\cite{Peng2023Generative}. 

This section outlines three key challenges from previous work and alternative views in this field. %These challenges must be addressed to ensure that LLMs can be responsibly integrated into mental health field.


\paragraph{Challenge 1: The Need for Ethical Data Foundations} The lack of real-world, diverse, and privacy-compliant data limits model reliability and applicability. \citet{Bedi2024Evaluation} recently conducted a systematic review to examine how LLMs are evaluated in the healthcare domain. They find that existing studies predominantly rely on simulated or social media-based data like Twitter and Reddit, with only 5\% of studies utilizing real patient care data for evaluation. Nonetheless, data from these sources may fail to capture the nuances and complexities of real-world mental health interactions (e.g.,  counseling services or hospitals) ~\cite{Eichstaedt2018,Tadesse2019,Coppersmith2018}. 
 This suggests a significant gap between the theoretical capabilities of LLMs and their actual implementation in mental health settings. As an example, \citet{Fraser2023} compared the diagnostic and triage accuracy of ChatGPT with human physicians in an emergency department. However, this study didn't involve actual patient interactions. %relying instead on presenting the same clinical information to both the LLM and the physicians. 
%%%Thus, prior work stressed the importance of using real patient care data for evaluating LLMs since benchmarking LLMs with hypothetical scenarios or simulated datasets will fail to adequately reflect the %complexities and nuances of  real-world clinical practice ~\cite{Liu2024, Wachter2024, Karabacak2023}. %They argue that  Real patient data provides a more ecologically valid assessment of how LLMs would perform in actual healthcare settings.

Moreover, LLMs trained on large datasets of publicly available text may inadvertently absorb and amplify existing societal biases surrounding mental health. If this biased information is then presented to users seeking mental health support, it could reinforce negative perceptions of mental illness, discourage help-seeking behaviors, and exacerbate existing inequalities in access to care ~\cite{lawrence2024llm_mental_health}.  Without robust data collection strategies, LLMs risk biases, misinformation, and ethical concerns. 
 %The majority of the research relied on simulated data like medical exam questions or clinician-created scenarios. 
%Many studies investigating large language models for mental health rely on social media data like Twitter and Reddit. This data is readily available and provides insights into language use and mental health, but may not reflect the nuances and complexities of interactions within counseling services or hospitals ~\cite{Eichstaedt2018,Tadesse2019,Coppersmith2018}. 
%Also, we should be cautious against relying solely on simulated data for LLM evaluation, as it can lead to inflated expectations and potentially unsafe implementations ~\cite{Karabacak2023, Landi2024, Webster2023}.
Recent research highlights the importance of data diversity and representation in training and evaluating LLMs for mental health. Counseling and hospital data often underrepresent diverse populations, especially marginalized communities \cite{hua2024llms_mental_health, omiye2023race_based_medicine}. Consequently, LLMs trained on data from limited demographics may underperform for other groups, risking misdiagnosis and ineffective treatments \cite{hua2024llms_mental_health, omiye2023race_based_medicine}. While GPT-4 showed promise in providing empathetic responses in mental health support contexts, it also exhibited concerning demographic biases \cite{gabriel2024airelatetestinglarge}. %Furthermore, LLMs may exhibit varying performance across different languages. While some mental health chatbots support multiple languages, research suggests that the accuracy, consistency, and verifiability of these chatbots may not be as robust in non-English languages ~\cite{jin2024crosslingual}. 

%Similarly, FAIIR—a categorization tool for youth mental health crises—has been shown to enhance efficiency and relevance in crisis response. This system organizes user concerns into actionable categories, significantly reducing response times while improving intervention effectiveness~\cite{faiir2024}. These examples highlight the critical need to embed LLMs within real-world clinical workflows to optimize their impact and validity. Expanding collaborations with mental health institutions could enhance these models’ contextual understanding and applicability in diverse settings.


\paragraph{Challenge 2: The Need for Robust Model Engineering and Adaptive Model Optimization}
LLMs in mental health applications face significant risks related to correctness, safety, and therapeutic reliability. Issues such as hallucinations, misinformation, and inappropriate responses \cite{zhao2023survey} necessitate more structured engineering processes (e.g., construction of optimized prompts) and real-world testing to ensure reliability and alignment with mental health best practices. Researchers emphasize the need for careful implementation, collaboration with stakeholders, and integration into existing healthcare systems~\cite{J2024Generative}. As the field evolves, there is a focus on developing empathetic, context-aware conversational agents~\cite{Harsh2021Coral} and exploring diverse applications of  AI in healthcare~\cite{Gozalo2023ChatGPT}. % generative AI

Moreover, ChatGPT-like closed-source proprietary LLMs are only accessible via their APIs \cite{laskar2024systematic}, which restricts users from fine-tuning the models locally or accessing their internal layers and weights \cite{Pfohl2024toolbox}. Moreover, relying too much on APIs raises privacy and security concerns, as sensitive data must be shared with third-party providers, increasing risks of data exposure. The lack of transparency in these models further complicates efforts to thoroughly evaluate their reliability and safety, a critical issue when handling sensitive mental health information \cite{lawrence2024llm_mental_health}. %The lack of transparency in these models makes it difficult to assess their reliability and safety thoroughly, which is a critical concern when dealing with sensitive mental health information \cite{lawrence2024llm_mental_health}. %%%This opacity could also hinder efforts to adapt these models to specific cultural contexts or linguistic nuances, further limiting their effectiveness and equitable application. % The reliance on proprietary models may limit the ability to 

%Furthermore, these ChatGPT-like closed-source LLMs are only accessible using their respective APIs. This creates privacy and security concerns due to the risk of sharing sensitive data with these LLM providers, making it unable to further fine-tune these models to mitigate their existing limitations.  % performance. %These challenges are further compounded by gaps in regulatory frameworks, leaving questions of responsibility—data sources, model developers, or governing bodies—unanswered~\cite{lawrence2024llm_mental_health} 

%Choudhury et al. ~\cite{Choudhury2023benefits} explored the dual nature of LLMs in digital mental health, highlighting their potential to enhance therapeutic care while warning about risks to the therapeutic alliance and patient safety. Their work emphasized the need to carefully consider implementation strategies and appropriate safeguards. 

Prior works underscore the absence of a widely accepted framework for healthcare tasks and their evaluation dimensions in mental health~\cite{Goldberg2024}. This inconsistency severely hinders the ability to compare results across studies or effectively gauge progress in LLM development for healthcare applications ~\cite{Stafie2023, Kohane2024}, ultimately stalling advancements in this critical field. A recent comprehensive review of 519 studies on healthcare applications of LLMs by ~\citet{Bedi2024Evaluation} also highlights the need for standardized implementation methods.
 
There is also a growing imbalance in AI accessibility across different demographics and healthcare systems.  For instance, the cost of fine-tuning models for specific populations remains prohibitively high, leading to disparities in how well these tools serve different groups \cite{obradovich2024opportunities}. While a recent study by ~\citet{stade2024_behavioral_healthcare} proposed a framework for the responsible development of LLMs in behavioral healthcare that could potentially augment or even replace certain aspects of human-led psychotherapy, the authors also acknowledge significant ethical and practical challenges associated with implementing this framework.  Additionally, the over-alignment of models to safety constraints can result in over-cautious responses, where LLMs refuse to engage with critical mental health queries, limiting their usefulness in real therapeutic settings \cite{obradovich2024opportunities}. %s44277-024-00010-z. 
%Conversely, unsafe jailbreaks can allow models to bypass ethical constraints, making them vulnerable to misuse.

 % revealing a fragmented evaluation landscape with insufficient use of real patient data and limited attention to fairness and bias assessment. Their findings 

%A study proposed a framework for responsible development and evaluation of LLMs in behavioral healthcare, suggesting that these models could potentially augment or even replace certain aspects of human-led psychotherapy while acknowledging significant ethical and practical challenges~\cite{stade2024_behavioral_healthcare}. 

%The study underscores GPT-4's potential to augment crisis identification but highlights the need for thorough bias mitigation and careful integration into clinical workflows.



%Another concern related to the potential for LLMs to foster a sense of trust that may not be warranted given the limitations of these models. LLMs can be trained to express empathy and build rapport with users, which can be beneficial in mental health support \cite{hua2024llms_mental_health, lawrence2024llm_mental_health}. However, this risk of inappropriate trust is particularly concerning in this context because vulnerable individuals might over-rely on LLM-generated advice or disclose personal information without fully understanding the potential risks involved. 

 %This inequity is exacerbated by the dominance of proprietary LLMs, where companies control access and limit transparency, restricting independent audits and external validation

\paragraph{Challenge 3: The Need for Multidimensional and Human-Centered Evaluation}
Proper evaluation frameworks are critical to ensure that LLMs in mental health deliver accurate, safe, and ethical outcomes. %Proper evaluation frameworks and thoughtful implementation strategies are essential 
This is essential to maximize their potential benefits while minimizing risks to patient safety and therapeutic trust ~\cite{Bedi2024Evaluation}. Nonetheless, 
traditional AI evaluation methods focus primarily on accuracy, neglecting critical aspects such as empathy, cultural sensitivity, and bias detection. %Some studies focused only on accuracy in evaluating performance. 
For instance, ~\citet{Fraser2023} only compared the diagnostic accuracy of ChatGPT with human physicians using data analysis,  %This study aimed to evaluate how well ChatGPT could diagnose patients in an emergency department setting, again emphasizing accuracy as a key evaluation metric.
Similarly, ~\citet{Pagano2023} investigated only the use of GPT-4 for diagnosing arthrosis and providing treatment recommendations. 

However, without human-centered evaluation frameworks, LLMs may fail to capture the nuances of real-world mental health support, where human-centered factors like trust, emotional validation, and cultural sensitivity are essential for success. For instance, ~\citet{Pfohl2024toolbox}  revealed that traditional evaluation approaches often miss subtle but important biases that could impact healthcare equity. % To address this, they developed a comprehensive framework for identifying biases in medical LLMs, introducing novel methodologies and resources that emphasize the importance of involving diverse annotators in bias detection. 
 Similarly, ~\citet{zack2024gpt4_biases_healthcare} conducted a detailed analysis of GPT-4's potential to perpetuate racial and gender biases in healthcare settings, finding concerning patterns in the model's differential diagnoses and treatment recommendations across demographic groups. Recently, ~\citet{Babonnaud2024TheBT} proposed a qualitative protocol for uncovering implicit biases in LLMs, focusing on stereotypes related to gender, sexual orientation, nationality, ethnicity, and religion. Their methodology revealed both explicit and subtle biases in model outputs, particularly in descriptions of minority groups. Furthermore, ~\citet{adam2022biased_ai_decision_making} demonstrated that the way AI recommendations are framed significantly impacts decision-making bias, with prescriptive recommendations more likely to induce biased outcomes compared to descriptive flags.

%Recent research has highlighted significant concerns regarding bias in healthcare AI applications. There is a growing interest in developing AI models for underserved and marginalized groups, such as those of a specific gender, race, or nationality of the less privileged group, to build specialized applications with ethical considerations ~\cite{Pfohl2024toolbox}. the integration of LLMs into clinical workflows remains a major hurdle. Unlike pharmaceuticals that undergo rigorous multi-phase clinical trials, AI-driven mental health tools lack structured evaluation frameworks

  %This study compared the model's performance to that of clinicians, with a focus on evaluating the accuracy of both diagnoses and treatment suggestions. %Another example is a chatbot designed for the 2SLGBTQIAP+ community that has been developed to provide mental health interventions through a culturally appropriate lens that recognizes the unique experiences of discrimination faced by these communities~\cite{Bragazzi2023Queering}. 

%Some studies focused only on accuracy in evaluating performance. Fraser et al. (2023) ~\cite{Fraser2023} compared the diagnostic accuracy of ChatGPT with human physicians using clinical data analysis. This study aimed to evaluate how well ChatGPT could diagnose patients in an emergency department setting, again emphasizing accuracy as a key evaluation metric. Pagano et al. (2023) ~\cite{Pagano2023} investigated the use of GPT-4 for diagnosing arthrosis and providing treatment recommendations. %This study compared the model's performance to that of clinicians, with a focus on evaluating the accuracy of both diagnoses and treatment suggestions.

%We need robust ethical guidelines to ensure that LLMs are used responsibly and that users are adequately informed about the capabilities and limitations of these technologies. These guidelines should address issues such as data privacy, informed consent, and transparency in model development and deployment. It is essential to ensure that the use of LLMs in mental health prioritizes the well-being and autonomy of users and does not inadvertently exacerbate existing disparities or create new ethical challenges \cite{lawrence2024llm_mental_health}.

%\textbf{Summary of Challenges}
%    The integration of LLMs into mental health faces three key challenges: (1) The Need for Ethical Data Foundations, as existing datasets lack real-world diversity and privacy compliance, leading to biases and ethical concerns; (2) The Need for Robust Model Engineering and Adaptive Model Optimization, given risks such as failure to ensure equitable access, as well as the presence of hallucinations and misinformation that necessitate structured engineering and real-world validation; and (3) The Need for Multidimensional Human-Centered Evaluation, where traditional AI evaluation metrics fail to capture empathy, actionability, and fairness. To overcome these challenges, we propose two structured frameworks. The SAFE Implementation Framework is a comprehensive approach to data acquisition, model engineering, and real-world integration to ensure LLMs are developed, deployed, and maintained responsibly in mental health settings. The second one is The HAAS-E Evaluation Framework which is a multidimensional, human-centered assessment system that evaluates LLMs beyond standard performance metrics. Together, these frameworks provide a roadmap for ethical, reliable, and impactful AI adoption in mental health. %The following sections detail how SAFE structures the development process, while HAAS-E ensures continuous monitoring and responsible AI evaluation.


\vspace{-2mm}
\section{SAFE-\textit{i}: \underline{S}upportive, \underline{A}daptive, \underline{F}air, and \underline{E}thical \underline{I}mplementation Guidelines}
Building on our position and an extensive review of existing literature and alternative views, we propose a structured approach to implementing LLMs, which we term SAFE-\textit{i} (Supportive, Adaptive, Fair, and Ethical Implementation). This approach, detailed below and illustrated in Figure \ref{fig:system2}, ensures that LLMs function as supportive, collaborative, ethical, and adaptive co-creators in mental health care, enhancing rather than replacing human-led support.

%\textcolor{ED}{this paragraph is redundant and lots of repetitives}This section introduces a comprehensive implementation framework for integrating Large Language Models (LLMs) into mental health support systems. Unlike general-purpose LLMs, models designed for mental health interactions must be adapted to the sensitive and high-stakes nature of mental health support. The SAFE Implementation Framework ensures the responsible, ethical, and effective integration of LLMs in mental health applications. It consists of three primary components: (1) Ethical Data Foundations, (2) Model Engineering, and (3) Real-World Integration as shown in 

%\textcolor{red}{In our view, we reviewed the previous work and alternative views; we propose the SAFE-I framework to implement mental health support applications using LLM. The SAFE-I framework is structured  Figure \ref{fig:system2}.}
\vspace{-2mm}
\subsection{The Data Foundation: Preparing Reliable and Diverse Mental Health Data}
%\begin{itemize}
\paragraph{Real-World Data Harvesting} LLMs for mental health must be trained on real-world data from naturalistic sources like text messages, counselor notes, and conversations. However, only 5\% of reviewed studies utilize real patient care data~\cite{Bedi2024Evaluation}, limiting model robustness and generalizability. Synthetic datasets often fail to capture the complexity, variability, and contextual nuances of real-world interactions \cite{pratap2022real,bond2023digital,koch2024real}. Future implementations must prioritize ethically sourced real-world data while ensuring transparency, informed consent, and opt-out mechanisms for participants \cite{bhatt2024ethical}.

\paragraph{Demographic Mosaic Construction} A core component is population variability, where the source data should reflect different demographics, cultural backgrounds, languages, and mental health conditions \cite{obermeyer2019dissecting}. Regular audits must be conducted to identify the over-representation or under-representation of specific groups \cite{mienye2024survey}. Adoptive sampling strategies \cite{lum2016statistical,chawla2002smote} or synthetic data augmentation \cite{shahul2024bias,juwara2024evaluation} should be employed where necessary to correct disparities, ensuring mitigating the risk of biases and fairness across wide audience \cite{abramoff2023considerations,10.5555/3692070.3694580}. 

\paragraph{Compliance with Regulatory Standards} Sensitive mental health data must be collected, stored, and processed in strict compliance with regulatory standards, including HIPAA \cite{HIPAA1996} and GDPR \cite{GDPR2016}. In addition, implementing robust technical safeguards is critical \cite{paul2020safeguards}. This includes encrypting data at rest and in transit, utilizing secure storage solutions, and conducting periodic security audits to identify vulnerabilities \cite{shojaei2024security}. Staff training on privacy and security protocols will also ensure both regulatory adherence and data protection \cite{arain2019assessing}.

\paragraph{Expert-guided Annotation and Quality Assurance} In unsupervised and self-supervised learning scenarios, the emphasis shifts to the quality and comprehensiveness of the dataset \cite{yu2024makes}. LLM models must be trained and evaluated on well-annotated datasets where domain experts label data with relevant markers such as emotional tone, urgency, and risk levels \cite{lao2022analyzing}. In high-risk cases—such as expressions of self-harm or psychosis, annotations should include severity scores, urgency indicators, and clinical insights to improve targeted interventions. Annotation protocols must be continuously refined.

%Mental health experts play a crucial role in annotating data with labels that capture nuances such as emotional tone, urgency, and risk levels \cite{lao2022analyzing}. High-quality labeling enables the model to learn critical features effectively. %Furthermore, to maintain high data quality, erroneous or incomplete entries should be removed, and text formats normalized to address variations such as spelling differences or slang usage. Therefore, where feasible, mental health professionals should be engaged to annotate high-risk samples, such as those indicating self-harm or psychosis. These annotations should include severity levels, urgency indicators, and emotional tone, providing valuable insights for targeted interventions and enhancing the dataset's overall quality.
%\end{itemize}

\vspace{-2mm}
\subsection{Model Engineering: Designing Adaptive and Effective LLMs}

\paragraph{Model Selection with Open-Source Prioritization}
Mental health LLMs should prioritize open-sourcing to foster transparency, community-driven scrutiny, and long-term reliability \cite{hua2024largelanguagemodelsmental,Yang_2024}. Unlike closed-source LLMs (e.g., GPT-4), open-source LLMs enable consistent evaluation and ensure reproducibility \cite{laskar2024systematic}. %, and bias auditing. 
The ability to %inspect and 
refine the model architecture ensures that AI-driven mental health solutions remain stable, accountable, and adaptable to evolving healthcare needs.%When selecting open-source models, it is essential to balance performance with compute limitations, keeping the target environment in mind—whether it’s a resource-constrained clinic or a cloud-based telehealth service.

\paragraph{Domain Adaptive Model Tuning} LLMs designed for mental health must be continuously specialized and refined to maintain therapeutic relevance, ethical integrity, and cultural competence \cite{guo2024large,thakkar2024artificial}. Adopting (e.g., fine-tuning or instruction-tuning) high-quality and domain-specific datasets is essential to embed empathy, rapport-building, and risk assessment into model behavior \cite{Yang_2024}. Expert-in-the-loop mechanisms must be integrated to ensure sustained alignment with real-world therapeutic practices, allowing for iterative refinement based on feedback and emerging patient needs \cite{Guo2024}. Furthermore, models must dynamically adapt to linguistic evolution, cultural shifts, and emerging mental health concerns, ensuring that LLM remains an inclusive, context-aware, and reliable support tool \cite{stade2024_behavioral_healthcare,thakkar2024artificial}. %\textcolor{red}{REF}

\paragraph{Empathy and Action-Oriented Prompt Design} Effective mental health AI requires carefully designed prompts for model adoptations and tuning that shape interactions in a supportive and actionable manner \cite{li2024optimizing,yu2024experimental,priyadarshana2024prompt}. Empathy-driven prompts position the LLM as a compassionate ally, encouraging users to share their feelings safely. Scenario-specific templates address diverse mental health contexts, from anxiety management to crisis support. Prompts also include calls to action, encouraging users to take steps (e.g., contacting a helpline), making the system both informative and actionable \cite{mesko2023prompt,patil2024prompt}.

\paragraph{Neural Augmentation via Structured Reasoning and Thought-Based Processing} Tree of Thoughts (ToT) \cite{yao2024tree} and Chain of Thought (CoT) \cite{,wei2022chain} reasoning enhance AI ability to break down complex mental health queries into structured, transparent decision paths, improving logical coherence and reducing hallucinations \cite{Yao2023}. By guiding the model to think through problems systematically rather than relying on direct pattern matching, these techniques help reduce hallucinations and enhance interpretability. Moreover, research on self-reflective AI suggests that LLMs can improve their accuracy by critically evaluating their own outputs before finalizing responses \cite{ji2023towards,shinn2024reflexion}. Furthermore, integrating uncertainty-aware architectures further enhances safety by enabling models to quantify their confidence levels in sensitive conversations \cite{yin2024reasoning}. When faced with high-risk inputs, these architectures allow AI systems to flag uncertain responses for human review, reducing the likelihood of misleading or inadequate crisis interventions.
%\begin{itemize}
%\item \textbf{Model Selection with Open-Source Prioritization}

%Unlike sequential generation, ToT enables AI systems to decompose complex mental health queries, evaluate alternative solutions, and iteratively refine responses. Integrating tree-of-thought reasoning in LLMs enables structured, step-by-step "thinking aloud," providing transparency into the model’s intermediate reasoning. This approach allows users and clinicians to better understand how recommendations are generated, fostering trust and interoperability. The structured exploration of reasoning paths reduces hallucinations and biases, fostering responsible AI-driven mental health interventions. 
%\end{itemize}
\vspace{-2mm}
\subsection{Real-World Integration: Human-Centered Continuous Monitoring of LLMs}

\paragraph{Human AI Complementarity Integration} This involves designing systems that specialize tasks based on strengths—AI for data processing and pattern recognition, and humans for empathy and complex decision-making—while ensuring high-risk cases are escalated to human experts \cite{sharma2023human,higgins2023artificial}. Additionally, AI should reduce cognitive burden through intuitive interfaces and automated workflows \cite{fragiadakis2024evaluating}.
    
\paragraph{Personalized Care Orchestration} The system should be adapted to individual needs, providing tailored recommendations, insights, or support \cite{kim2024mindfuldiary}. The system should also prioritize user trust by being explainable \cite{kerz2023toward,joyce2023explainable}. Transparency is critical \cite{stade2024_behavioral_healthcare}; users must be clearly informed about which parts of their care or support are AI-generated, how the LLMs were developed, fine-tuned, and evaluated. It is important to clarify whether LLMs used are general-purpose models or explicitly optimized for mental health applications.

\paragraph{Pilot Program and Safety Net Deployment} Before deploying the system, pilot programs must be conducted to assess safety, ethical considerations, and real-world usability \cite{sallam2023pilot,callahan2024standing,esmaeilzadeh2024challenges}. Safeguards such as toxicity detection tools (e.g., LLama Guard \cite{inan2023llamaguard}) and automated high-risk content monitoring should be integrated. AI models must be equipped with automated triggers to detect harmful, coercive, or crisis-related content (e.g., suicidal ideation) and escalate cases to human professionals when necessary \cite{sharma2023human,higgins2023artificial,strong2024towards}. Without these safety nets, AI-driven mental health support risks unintended harm.
    
\paragraph{Feedback Loop Optimization} Systems must incorporate structured feedback loops that allow users, mental health professionals, and stakeholders to report errors, suggest improvements, and refine system performance over time \cite{de2025artificial}. These mechanisms should include: real-time issue reporting to capture model failures and biases \cite{ferrara2023fairness,cabrera2021discovering}, stakeholder-driven evaluations to assess the performance from multiple perspectives \cite{hogg2023stakeholder,kaye2024moving}, and the “Learning from Incidents” framework \cite{lukic2012framework} that continuously monitors operational failures and systematically addresses them to improve reliability and accountability.

With the key implementation guidelines established, we now explore core evaluation criteria, metrics, and assessment methods for LLMs in mental healthcare.
\vspace{-1mm}
\section{HAAS-\textit{e}:  \underline{H}uman- \underline{A}I  \underline{A}lignment and  \underline{S}afety  \underline{E}valuation Framework}

Traditional AI evaluation metrics, focused on accuracy and efficiency, fail to capture the ethical, emotional, and safety complexities of mental health applications. We advocate for a human-centered approach, we term it Human-AI Alignment and Safety Evaluation (HAAS-\textit{e}), that defines the key dimensions for LLMs evaluations in mental health as shown in Figure \ref{fig:system2}.
\vspace{-2mm}
\subsection{HAAS-\textit{e} Multidimensional Evaluation Criteria} 
To complement our position we define four core dimensions that delineate the key aspects of LLM performance essential for assessing its alignment with human needs and ethical considerations.

\paragraph{Trustworthiness and Correctness} %(Correctness and Hallucination Detection)
The model's reliability should be assessed through correctness and factual accuracy. In mental health contexts, intent classification can be measured using precision, recall, and F1-score, while AlignScore \cite{zha2023alignscore} evaluates response accuracy. To prevent misinformation, hallucination detection techniques—such as chain-of-thought prompting \cite{wei2022chain}, fact-checking with knowledge graphs, and retrieval-augmented generation \cite{gao2023retrieval}—should be employed. Sentiment analysis can further help filter toxic responses \cite{huang2023survey}.
% Meanwhile, The quality of the model-generated response can be evaluated using metrics like ROUGE \cite{lin-2004-rouge} and BERTScore \cite{zhang2019bertscore} by comparing it against the gold reference. If no gold annotation data is available, then different LLMs can be used as the judge \cite{li2024llms,gu2024survey} for reference-free evaluation.


\paragraph{Bias and Ethical Auditing} This step includes the evaluation of biases and ethical concerns in the model’s outputs to ensure fair and equitable LLM responses. These considerations are integral to ensuring fairness and equity. For this purpose, different splits in the test set can be constructed depending on the demographic information to evaluate whether the model has any biases in data constructed from certain demographics \cite{Pfohl2024toolbox}. Moreover, specific prompts can be constructed to evaluate the potential biases and ethical concerns in certain scenarios. For instance, demographic-aware prompting may include demographic information about the patient, when appropriate and available, to evaluate the biases in model-generated responses in certain demographics \cite{Babonnaud2024TheBT}. %In addition, models should be evaluated based on their alignment with mental health guidelines (e.g., ethical, cultural, and clinical standards).

\paragraph{Empathy and Therapeutic Alliance Assessment} Beyond technical accuracy, the models must demonstrate empathy and provide constructive support. This is an important metric to ensure a human-centered evaluation of the models. While these can be achieved automatically via leveraging various neural models \cite{wankhade2022survey} or by using LLMs-as-the-judge \cite{li2024llms,gu2024survey}, evaluating the model responses by human experts, at least on some sampled responses is required to ensure a high-quality evaluation. Moreover, using a standardized framework like the EPITOME \cite{sharma2020computational} that measures empathy based on emotional reactions, interpretations, and perspective-taking could also be used.

 
\paragraph{Helpfulness and Actionability Analysis} Another criteria for human-centered evaluation is to measure the helpfulness of the model-generated responses \cite{tuan2024towardshelpfulness}. This can be achieved by giving a helpfulness rating to the model response (e.g., via leveraging LLM judges \cite{li2024llms,gu2024survey} or human experts). In addition, response generation latency (i.e., model's inference speed), computational requirements, escalation rates for high-risk cases, etc. should also be measured to ensure that the system can be useful for real users. 


\vspace{-2mm}
\subsection{The HAAS-\textit{e} Evaluation Metrics}
Building on the four core evaluation dimensions, the HAAS-\textit{e} metrics operationalize these principles, offering quantitative and qualitative tools to rigorously assess LLM performance in mental health contexts:

\paragraph{Contextual Empathy \& Emotional Score (CES)} measures an LLM's ability to understand and respond empathetically to user emotions within mental health contexts. Unlike basic sentiment analysis, CES evaluates the alignment between the LLM responses and the user's emotional state, situational context, and therapeutic goals. Mathematically, CES can be formulated as a linear combination of two key components: Semantic Coherence which is the alignment, $Align(R_{\text{llm}}, C_{\text{user}})$, between the LLM's response, $R_{\text{llm}}$, and the user's expressed concerns, $C_{\text{user}}$, and Emotional Alignment, which is the alignment, $Align(R_{\text{llm}}, C_{\text{user}}, E_{\text{human}})$, with both the user's emotions and expert human counselor evaluations $E_{\text{human}}$. %For example, a high CES would reflect the LLM's ability to detect subtle cues of distress and provide supportive, contextually appropriate responses. 
This metric can be quantified by comparing LLM outputs to expert human counselor responses or through user feedback in double-blind studies. Research supports the feasibility of quantifying empathy\cite{sharma2020computational}, and recent studies have also demonstrated its applicability in mental health AI evaluation \cite{gabriel2024airelatetestinglarge}, underscoring the need for nuanced metrics like CES.

\paragraph{Cultural Sensitivity Index (CSI)} evaluates an LLM’s ability to adapt its language, tone, and advice to align with diverse cultural backgrounds, ensuring responses are culturally appropriate and free from biases. Mathematically, CSI can be formulated as a cultural appropriateness alignment score, $Align(R_{\text{llm}}, C_{\text{culture}})$, where LLM response,  $R_{\text{llm}}$, is assessed against the user's cultural context, $C_{\text{culture}}$, by experts who assign a cultural appropriateness score. The metric goes beyond simple language translation to analyze whether the model avoids cultural stereotypes, understands nuanced cultural norms, and provides relevant advice. For example, a high CSI would reflect the LLM ability to offer culturally sensitive guidance to a user from a specific community without resorting to stereotypes. Research highlights the risks of cultural biases in LLMs \cite{zack2024gpt4_biases_healthcare}, emphasizing the need for CSI metric to mitigate these risks \cite{Pfohl2024toolbox, Babonnaud2024TheBT}.

\paragraph{Personalization Appropriateness Score (PAS)} evaluates how well an LLM tailors its responses to individual users, moving beyond generic advice to incorporate user-specific context. Mathematically, PAS can be formulated as a personalization alignment score, $Align(R_{\text{llm}}$, $U_{\text{history}})$, where $U_{\text{history}}$ captures the user's interaction history. This metric assesses the model's ability to recall prior interactions, recognize individual preferences, and adapt its guidance to meet the user's unique needs. For example, a high PAS would reflect the LLM's ability to provide contextually relevant and personalized support, ensuring responses are aligned with the user's specific circumstances rather than being generic. Research demonstrates that personalized models outperform generic ones \cite{Liu2024}, and tailored recommendations significantly enhance mental health care effectiveness\cite{Valentine2022Recommender}.

\paragraph{Actionability and Safety Assessment (ASA)} evaluates the likelihood that a user will take a specific, beneficial action based on an LLM-generated response. Mathematically, ASA can be formulated as the conditional probability $P(Action_\text{Taken} \mid R_{\text{llm}})$, where $Action_\text{Taken}$ denotes the user's adherence to the recommended action. This metric ensures that LLM responses not only provide empathetic support but also drive real-world help-seeking behavior, such as contacting a helpline or scheduling an appointment. For example, a high ASA score would reflect the LLM's ability to deliver practical, actionable guidance that users are likely to follow. Research demonstrates that effective prompt design enhances the actionability of AI-generated responses \cite{priyadarshana2024prompt}, and can significantly improve outcomes in mental health interventions \cite{ Fitzpatrick2017Woebot, Swaminathan2023NaturalLP}.


\vspace{-2mm}
\subsection{Empirical Validation Methods in HAAS-\textit{e}}
To ensure HAAS-\textit{e}'s effectiveness and reliability, we propose a multi-method validation strategy that combines quantitative and qualitative measures.

%\begin{itemize}

   % \item 
    \textbf{Randomized Controlled Trials (RCTs) with Real-World Data} RCTs remain the gold standard in collaboration with mental health organizations using real patient data. This approach would compare the outcomes of groups receiving support from LLM-enhanced tools against control groups receiving standard care. %Collected data should include conversation outcomes, user satisfaction, and cost-effectiveness measures.

    %\item
    \textbf{Multi-Method Evaluation} To capture a comprehensive view of model performance, where technical accuracy is complemented by human-centered validation, we propose a combination of quantitative and qualitative measures. Quantitative metrics include the HAAS-\textit{e} evaluation metrics (CES, CSI, PAS, and ASA). Qualitative data is gathered through interviews with users and professionals to evaluate perceived helpfulness and ethical alignment. Additionally, expert reviews examine the content of responses for safety, quality, and relevance.

    %\item
    \textbf{Red Teaming and Adversarial Testing}  Red teaming To proactively identify vulnerabilities and ethical risks, red teaming should be conducted by internal and external domain experts \cite{lin2024againstredteam} simulating adversarial conditions. These tests should include: (i) emotionally intense queries, (ii) ethical dilemmas (e.g., conflicting cultural advice), and (iii) high-risk situations (e.g., suicidal ideation). %By systematically stress-testing LLMs in controlled environments, red teaming helps refine safeguards against biases, hallucinations, and unsafe recommendations before deployment in real-world settings.
    
    %\item 
    \textbf{A/B Testing with Different Models} To continuously refine LLM performance, A/B testing should be conducted across different LLM architectures (open-source vs. proprietary models), prompting strategies, and fine-tuning techniques. By systematically comparing performance using HAAS-\textit{e} metrics, A/B testing identifies optimal configurations that maximize fairness, actionability, and user trust.
%\end{itemize}



%The HAAS-E Framework introduces a novel, structured evaluation process that goes beyond accuracy metrics by integrating empathy, personalization, cultural sensitivity, and actionability. It also balances quantitative + qualitative assessments to ensure real-world effectiveness. It aligns with human expertise, improving AI’s ability to support mental health responsibly. This framework ensures that mental health AI tools are safe, accountable, and effective—driving ethical AI adoption in real-world clinical and support settings.

\vspace{-3mm}
\section{Conclusion}

This position paper calls for a fundamental shift in how LLMs are integrated into mental care, advocating for their role as co-creators rather than mere assistants. While LLMs offer scalability, personalization, and crisis intervention potential, they also pose unintended harms, including bias, over-reliance, dehumanization, and regulatory uncertainties. To address these, we propose the following call of action: 

\textbf{(1) Cross-Disciplinary Governance:} Foster interdisciplinary collaboration between AI researchers, health providers, ethicists, and policymakers to create standardized evaluation practices that align with healthcare provider's priorities.\textbf{ (2) Open-Source Frameworks and Tools:} Advocate for the prioritization of open-source and transparent LLM development, enabling scrutiny, fairness, and adaptability in mental health applications. \textbf{(3) Human-Centredness}: Promote responsible AI-human collaboration by adopting the SAFE-\textit{i} implementation guidelines, ensuring LLMs augment rather than replace human-led care. \textbf{ (4) Evaluations Beyond Accuracy:} Implement structured evaluation frameworks, such as the HAAS-\textit{e}, to assess LLMs beyond accuracy, focusing on trustworthiness, empathy, cultural sensitivity, and actionability. 

The proposed frameworks serve as a starting point for rethinking accountability and fostering trust in LLM-driven mental health systems. The paper emphasizes that the machine learning community, healthcare providers, organizations, and stakeholders must proactively adopt these measures to ensure that AI technologies are not only effective but also ethical and equitable in %their 
real-world scenarios. % applications.

\paragraph{Impact Statement}
This paper advocates for reimagining LLMs as ethical co-creators in mental health care rather than passive assistants. We introduce the SAFE-\textit{i} implementation guidelines and the HAAS-\textit{e} evaluation framework as a structured approach to ensure LLMs enhance, rather than replace, human-led mental health support. Our work lays the foundation for responsible AI integration, emphasizing trust, empathy, and collaboration to bridge critical gaps in mental health accessibility and safety.
%While it is true that LLMs currently cannot fully replicate the nuanced emotional intelligence of human counselors, this paper proposes the SAFE and HAAS-E frameworks to address these limitations. Specifically, the SAFE framework emphasizes ethical data collection and model training, focusing on incorporating empathy and cultural sensitivity, while the HAAS-E framework introduces metrics for evaluating empathy and emotional alignment to ensure that LLMs provide appropriate and supportive responses. By leveraging real-world data and expert oversight, these frameworks are designed to ensure that LLMs augment, not replace, the human element in mental health care. The concern that AI could lead to a dehumanization of care is valid, and this paper recognizes the importance of maintaining human-led interventions. The SAFE framework prioritizes human-AI collaboration to ensure that AI tools are used to augment rather than replace human counselors. This includes using AI for tasks like initial triage, data analysis, and personalized support while maintaining human involvement for complex and long-term care needs. Furthermore, the emphasis on explainable AI in our proposed frameworks ensures that users understand the role and limitations of AI in mental health care. It is crucial to acknowledge that current regulatory and safety measures for LLMs in mental health are indeed insufficient. This paper addresses this by calling for the development of robust ethical guidelines and evaluation frameworks. The HAAS-E framework introduces multidimensional evaluations that go beyond accuracy to include measures of trustworthiness, cultural sensitivity, and safety, addressing the ethical and safety concerns by ensuring rigorous evaluation and monitoring. This approach aims to make AI a reliable tool in mental health, ensuring that it is safe for those who rely on it.

%The alternative viewpoints highlight the importance of approaching LLM integration with caution and responsibility. The paper’s proposed SAFE and HAAS-E frameworks are specifically designed to address these concerns by providing structure for ethical data sourcing, model development, and evaluation. LLMs should not be dismissed outright but instead, should be carefully integrated to augment the existing mental health care infrastructure with human oversight to ensure patient safety and well-being






 \def\hide#1{\section{EXTRA}

\begin{itemize}
  
    \item \textbf{EXTRA [Already discussed in the previous section] Complementary AI:} The system should be designed to complement human counselors rather than replace them. This hybrid approach ensures that critical decisions remain in human hands. Before deployment, the following should be considered:
    \begin{itemize}
        \item \textbf{Human-AI Collaboration:} AI is a tool or assistant, which should help humans make better-informed decisions or complete tasks more efficiently.
        \item \textbf{Focus on Augmentation:} Instead of automation that replaces human effort, complementary AI empowers humans to perform tasks with greater precision and insight.
        \item \textbf{Personalization:} The systems should be adapted to individual needs, providing tailored recommendations, insights, or support.
        \item \textbf{Transparency and Interpretability:} Complementary AI systems prioritize user trust by being explainable and intuitive.
    \end{itemize}


    
    \item \textbf{EXTRA: User-Centered Evaluation:} Collect user feedback to assess the usability, acceptability, and perceived helpfulness of the LLM. [Discuseed in 5.2]
     
    \item \textbf{EXTRA: Agentic LLM:} This component focuses on creating autonomous systems that adapt and learn from interactions, continuously improving their contextual understanding and responsiveness.
    
    \item \textbf{EXTRA: Reporting and Regulatory Compliance:}
    \begin{itemize}
        \item \textbf{Ethical and Legal Standards:} Ensure adherence to ethical and legal standards. [Discussed in 5.3]
        \item \textbf{Regular Reporting:} Generate regular reports on system performance, including metrics on accuracy, bias, and safety.
        \item \textbf{Regulatory Compliance:} Maintain compliance with relevant mental health regulations (e.g., HIPAA, GDPR) and ethical guidelines.
        \item \textbf{Transparency:} Ensure it is clear when information is generated using LLMs, how the LLMs were developed and tested, and whether they are general-purpose or fine-tuned for mental health. [Discussed in 5.3]
    \end{itemize}
\end{itemize}
}

%\nocite{langley00}

\bibliography{icml}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%\section{Appendix.}
%\subsection{Case Study: Applying HAAS-E in Mental Health Support Platform}
%To illustrate the HAAS-E framework in practice, we examine its application in a digital mental health platform designed to support crisis intervention. We assume that we have a dataset from conversations related to mental health problems, and we analyze how to use the evaluation framework.
%\begin{itemize}
%    \item CES: Evaluated using a dataset of anonymized crisis support conversations, comparing LLM replies to human counselor responses, measuring semantic alignment and emotional sentiment. 
%    \item CSI: Tested across diverse demographic groups, with responses rated by culturally diverse mental health professionals for language and advice appropriateness. 
%    \item PAS: Deployed over multiple user sessions, tracking the LLM’s ability to recall past interactions and adapt responses with user feedback via the Likert scale. 
%    \item ASA: Monitored whether users followed recommendations, such as calling a crisis hotline, using engagement metrics and user surveys.
%\end{itemize}

%You can have as much text here as you want. The main body must be at most $8$ pages long.
%For the final version, one more page can be added.
%If you want, you can use an appendix like this one.  

%The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
