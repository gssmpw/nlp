
% These findings extend a long line of research on counterfactual reasoning and knowledge conflicts to the multimodal setting. Concretely, the \segsub framework is the mulitmodal extension of the Entity Replacement Framework \citep{longpre_entity-based_2022} wherein perturbations are applied to text to change named entities in the source passages of natural language QA datasets. Similarly, we segment relevant entities and objects from image sources. The types of perturbations we have applied are also inspired by previous works on knowledge conflicts \citep{chen_rich_2022,longpre_entity-based_2022} and counterfactual reasoning \citep{neeman_disentqa_2022,hong_why_2024} in text---once entities have been segmented from our images, we remove them and/or infill the masked segment with a modified object to generate counterfactual examples and knowledge conflicts respectively. What's more, the fact that the \segsub framework is successful in generating samples on which existing VLMs fail to reason (and frequently hallucinate, as in \autoref{fig:counterfactual_examples}) underscores the multimodal applicability of Entity Replacement frameworks for assessing the reasoning abilities of language models. 

% \segsub demonstrates the potential of Entity Replacement frameworks for evaluating multimodal reasoning in VLMs, by successfully generating samples that current VLMs fail to reason about, and frequently hallucinate on (\autoref{fig:counterfactual_examples}).


The \segsub framework extends research on reasoning with knowledge conflicts to the multimodal domain. The framework builds on the unimodal text-based Entity Replacement Framework \citep{longpre_entity-based_2022} and extends it to VQA by segmenting and modifying relevant entities and objects in images. Our perturbations are inspired from prior work on knowledge conflicts \citep{chen_rich_2022,longpre_entity-based_2022} and counterfactual reasoning \citep{neeman_disentqa_2022,hong_why_2024} in LLMs. 

VLMs, like LLMs, may internalize statistical and factual knowledge from large-scale training data. This includes details such as the typical colors of specific bird and flower species, (\autoref{fig:perturbed_examples}), or even historical facts such as the color of the horse that Eli Bremer rode in the 2008 Summer Olympics (\autoref{fig:seg_sub_pipeline}). We measure the degree to which VQA models prioritize these parametric facts over the information contained in input sources. Whereas LLMs have been shown to exhibit strong parametric tendencies, we find that this is not the case for VLMs. As seen, parametric response rates are low, $\sim$20\% across all models tested (\autoref{fig:parametric_effect}). 


%The approach taken here to address the sensitivity of VLMs with respect to counterfactual samples and knowledge conflicts are as follows.
% First, we identify how this gap in reasoning manifests as a missing component in the labeled data, in this case VQA dataset. Then, we apply generative models such as LaMa \citep{suvorov_resolution-robust_2022} and Stable Diffusion \citep{rombach_high-resolution_2022} in a constrained manner to augment existing VQA datasets. 

Our core contributions lie in our analysis of model robustness to different types of knowledge conflict (\autoref{fig:finetuning_results}). Without finetuning, models such as GPT-4o ignore the counterfactual sources and instead hallucinate (\autoref{fig:context_scores}). While the counterfactual reasoning task may seem unreasonable as hallucinations could represent the correct answer for common-sense questions, we highlight that the utility of counterfactual samples is that they reveal a significant gap in understanding between human and machine performance. For instance, it is immediately obvious to a human that examples in \autoref{fig:counterfactual_examples} are unusual. The fact that this is not obvious to VLMs motivates our framework and dataset.


% Our analysis highlights that existing VLMs perform poorly on counterfactual sources and fail to detect source conflicts. Baseline models hallucinate responses to counterfactual sources and disregard source conflicts altogether by choosing an answer from one of the possible sources at random. These shortcomings can be addressed by finetuning the models on knowledge conflicts (\autoref{fig:finetuning_results}). 

% \todo{add something about parametric conflicts}


% For instance, the WebQA dataset is composed of examples drawn from Wikipedia, and the answers for questions based on the images can often be found within the text of the Wikipedia corpus. There is reason to believe then, that the parameteric effect often seen in unimodal LLMs will translate to the multimodal setting, in particular where VLMs are also trained on Wikipedia data. 
% Importantly, the ability of baseline models to correctly determine the correct answer for modified sources shows that baseline models are not relying on parametric memory when answering the question.



The ease of construction and availability of paired image-caption data has made it vital for image summarization tasks. As such, our framework is also motivated by a broader challenge: an over reliance on paired image-caption data and contrastive loss functions for training VLMs. While these image-caption helps models learn to reason about what is in the image, we find that models struggle with reasoning about what \textit{is not} in the image. Our work aims to correct the counterfactual reasoning gap by paving the way for counterfactual samples to be integrated into the training process.

% Furthermore, models tend to hallucinate when tasked with VQA-style questions on such cases.

We demonstrate that counterfactual reasoning in VLMs is conditional on the sources presented. Reasoning over `randomized negatively sampled counterfactuals' (i.e. a question and an unrelated image) appears trivial for both base and finetuned models (\autoref{tab:natural_counterfactual_results}). However, cases with high image-question contextualization present interesting insights as they trigger hallucinations in even the most advanced VLMs. This link between hallucinations and highly contextualized counterfactual samples underlines the value of our framework and dataset for multimodal reasoning.

Without our framework, such samples are difficult and costly to collect\footnote{Alternatives approaches that aim to identify counterfactual image sources instead of using a generative approach would entail image retrieval systems capable of advanced multimodal reasoning, which is not the task they are typically trained for.}. Our methodology provides a systematic way for future work to build on counterfactual reasoning, source conflicts, and hallucinations in the multimodal setting. Future work may center around developing more sophisticated sets of generative constraints, extending the \segsub framework and dataset to tackle aspects of visual reasoning that continue to be underrepresented in VQA datasets. 





% Future work may also incorporate the \segsub framework with multimodal retrieval systems. Our experimental methodology is derived from \cite{labruna2024retrieve}, who propose a method of learning to retrieve when the combination of input context and parametric knowledge is insufficient to answer the given question. They demonstrate empirically that integrating retrieval modules into LLMs in this fashion leads to improved accuracy on popular QA benchmarks. We stop short of doing this. While we find that models can be trained to predict when their image sources are insufficient, we stop short of connecting the \retlabel to a retrieval system. Multimodal retrieval mechanisms such as UniVLDR \citep{liu_universal_2023} could be integrated into future experiments.


% Retrieval stuff

% Particularly relevant for the retrieval setting, another long line of work focuses on detecting generated or perturbed images \citep{dong_think_2022,luo_lare2_2024,ojha_towards_2023}, for which the \segsub framework can be adopted. Instead of distinguishing between \retlabel and \updatedlabel, all \segsub generations would be positive samples. Of course, the distribution of generated images is of most concern in this field of work, and classifying counterfactual images as generated or not generated is likely not of interest. Rather, the \segsub framework could be applied to create a more relevant distribution of images based on a different set of constraints for the generative models.

% In practical applications, a high-quality retrieval system might ignore perturbed, counterfactual, or conflicting samples altogether, bypassing the challenges these samples introduce. For instance, an advanced multimodal model could prioritize information from a primary, unaltered source image over conflicting or counterfactual images, reducing the real-world relevance of the generated samples for some applications. However, this limitation underscores the importance of designing retrieval systems that handle ambiguity, especially in domains requiring robust reasoning across multiple image sources. Future iterations on the Segmentation Substitution Framework could include tests for how various retrieval systems handle perturbed samples.

% However, as we are artificially generating the conflicting images that cause the retrieval tokens to be predicted, including such a retrieval system in the experiments in this paper would be unlikely to lead to increased performance in comparison with original evaluation sets of the tasks we study, as they do not contain the perturbed examples. As such, we leave this to future work.



% All Peters Bad Ideas should go below

% The impact of this research is not limited to improving the robustness of multimodal LLM's to these specific kinds of perturbation. Given the proliferation of generative AI images online \footnote{\url{https://petapixel.com/2024/10/10/ai-is-muddying-google-image-search-results-people-arent-happy/}}, the ability of multimodal LLM's to reason over perturbed image sources will become increasingly important. 

% With regards dead internet theory, we find that rather than deteriorating model performance, incorporating artificially generated images into the training of LLMs can improve model performance. This is particularly the case where generations are drawn from part of the distribution that is difficult to sample from. For instance, when we generate images that cause knowledge conflicts when taken together as a set, we are sample from part of the overall distribution of images that is expensive to sample from naturally. In this way, strategic image perturbations fill a gap in existing multimodal datasets and tasks. Without these perturbations, multimodal models that are trained and evaluated on these datasets assume that the sources they have been presented with contain an answer to their question.

% The field of natural language processing has seen dramatic advances with the success of LLMs across all kinds of reasoning tasks. Notwithstanding the impressive scaling ability of transformer architectures with parameter size, this success has also been enabled by ability of these models to train in a self-supervised fashion. Objectives such as Masked Language Modelling, Next Token Prediction, and Next Sentence Prediction require little more than large corpora of text. Unfortunately, VLMs cannot be trained in this fashion as the visual modality does not afford self supervised objectives. As a result, 
