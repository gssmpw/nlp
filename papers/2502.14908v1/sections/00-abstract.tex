%Visual Question Answering (VQA) requires models to reason about image sources and the intent of the question (text based), to provide the appropriate response. Prior work has found that question answering (QA) models are not robust to knowledge conflicts that arise between parametric knowledge (encoded in the model weights during training) and contextual knowledge (external knowledge sources given to the model). This has led to work in instruction tuning models with augmented datasets, which primarily focus on addition of QA pairs to improve robustness against the same image sources. While this is less expensive, it only focuses on the models ability to generalize to different questions, and does little to diversify the image sources the questions are based on.

% Visual Question Answering (VQA) requires models to reason about image sources and understand the intent of text-based questions. However, models often struggle when noisy environments give rise to conflicting information between pretrained (parametric) knowledge and external contextual knowledge. 
% %Several works have explored augmenting VQA datasets to increase question variety, but none have explored approaches to augment the image sources on which the questions are based. 
% This has led to work in improving robustness by instruction tuning models with augmented datasets, which primarily increases question variety but does little to diversify the image sources on which questions are based.


% v0
%While much is known about how large language models (LLMs) reason over counterfactual samples and knowledge conflicts in textual data, there is a gap in understanding how vision language models (VLMs) reason over such conflicts in image data. In this work, we propose \segsub, a Segmentation Substitution framework to improve the robustness of VLMs by introducing augmentations that modify object features (shape or color), add counterfactual images, and conflicts between image sources. While prior work has shown that LLMs are sensitive to textual perturbations, we find VLMs are largely robust to image perturbation. However, we find that VLMs perform poorly on counterfactual examples ($<30\%$ accuracy) and fail to reason over source conflicts ($<1\%$ accuracy). We also find a link between hallucinations and image context, with GPT-4o prone to hallucination when presented with highly contextualized counterfactual examples.We mitigate this by finetuning models on \segsub, which significantly improves reasoning over counterfactual samples. However, challenges persist with source conflicts, and our findings highlight the need for VLM training methodologies that enhance their reasoning capabilities, particularly in addressing complex knowledge conflicts between multimodal sources.




%Finally, we demonstrate that finetuning models on \segsub data results in dramatic improvements in accuracy over counterfactual samples, while knowledge conflicts remain a challenge for future research. 
%Overall, our work highlights the importance of training VQA systems to be robust to feature modifications and counterfactual samples. 
%Overall, our work highlights the importance of training VQA systems to be robust to feature modifications and counterfactual samples. 

%We release \segsubdata, the augmented datasets that we generated, to further encourage research in this direction. 


%\TODO{insert link to dataset/artifacts?}.  


% new abstract
The robustness of large language models (LLMs) against knowledge conflicts in unimodal question answering systems has been well studied. However, the effect of conflicts in information sources on vision language models (VLMs) in multimodal settings has not yet been explored. In this work, we propose \segsub, a framework that applies targeted perturbations to image sources to study and improve the robustness of VLMs against three different types of knowledge conflicts, namely parametric, source, and counterfactual conflicts.
Contrary to prior findings that showed that LLMs are sensitive to parametric conflicts arising from textual perturbations, we find VLMs are largely robust to image perturbation. On the other hand, VLMs perform poorly on counterfactual examples ($<30\%$ accuracy) and fail to reason over source conflicts ($<1\%$ accuracy). We also find a link between hallucinations and image context, with GPT-4o prone to hallucination when presented with highly contextualized counterfactual examples.
While challenges persist with source conflicts, finetuning models  significantly improves reasoning over counterfactual samples. Our findings highlight the need for VLM training methodologies that enhance their reasoning capabilities, particularly in addressing complex knowledge conflicts between multimodal sources.