\clearpage
\setcounter{page}{1}
\appendix



% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/results/negative_eval.pdf}
%     \caption{Full results for the randomized negatively sampled robustness check. Models finetuned on \segsub data (-ft) outperform baseline models in identifying images irrelevant to the given query.}
%     \label{fig:negatively_sampled_counterfactual_eval}
% \end{figure}



\section{Model Finetuning}

Hyperparameters for the finetuned models are given in \autoref{tab:hyperparameters}. Note: Clip-vit refers to openai/clip-vit-large-patch14-336. Convergence of training loss within one epoch for Qwen2 and Phi3 is shown in \autoref{fig:training_loss}.

\begin{table}[h!]
\caption{Important hyperparameters for the models.}
\label{tab:hyperparameters}
\centering
\begin{tabular}{lccc}
\toprule
Hyperparameter & Phi3V & Qwen2VL & Llava \\
\midrule
hidden size & 3072 & 3584 & 4096 \\
hidden act & silu & silu & gelu \\
intermediate size & 8192 & 18944 & 4096 \\
\# attention heads & 32 & 28 & 16 \\
\# hidden layers & 32 & 28 & 24 \\
vision model & clip & qwen2 & clip \\
$|$image embedding$|$ & 1024 & N/A & 768 \\
vocab size & 32k & 152k & 32k \\
$|$pos. embedding$|$ & 131k & 32k & 4096 \\
torch dtype & bf16 & bf16 & f16 \\
initializer range & 0.02 & 0.02 & N/A \\
sliding window & 131k & 32k & N/A \\
temperature & 0.01 & 0.01 & 0.01 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}
    \centering
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=0.98\linewidth]{figures/results/qwen2_train_loss.png}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=0.98\linewidth]{figures/results/phi3_train_loss.png}
    \end{subfigure}    
    \caption{Top: Qwen2 training loss. Bottom: Phi3 training loss.}
    \label{fig:training_loss}
\end{figure}

\section{Prompts}
Prompts for QA checks and image-question context evaluation are listed here---namely the counterfactual QA check, the feature modification QA check, and the image-question contextualization prompt.

\begin{framed}
\label{frame:quality_check_prompt_webqa_yesno}
\textbf{human}: 

$\langle$image-placeholder$\rangle$

Caption: $\langle$Original Image$\rangle$

$\langle$image-placeholder$\rangle$

Caption: $\langle$Perturbed Image$\rangle$

Question (for object removal): is the $\langle$object$\rangle$ present in both the original image and the perturbed image?

Question (for color and shape change): what is the $\langle$category$\rangle$ of the $\langle$object$\rangle$ in the image?

\textbf{ai}: 
\end{framed}


\begin{framed}
% /Inpaint-Anything/tasks/webqa/gpt_qa_check.py
\label{frame:quality_check_prompt_webqa_color_shape}
\textbf{system}: You must use the provided image sources to answer the question. If the answer is not in the image, respond 'unknown'.

\textbf{human}: 

Image: $\langle$image-placeholder$\rangle$

Caption: $\langle$caption$\rangle$

Question: $\langle$query$\rangle$

\textbf{ai}: 
\end{framed}

\begin{framed}
\label{frame:image_question_contextualization}
\textbf{system}: Give a contextualization score for each image question pair. The score, between 1 and 10, should reflect the degree to which the image contextualizes the question. That is, how likely is it that you might come up with the question while looking at the image. Focus on the range of possible questions that might be asked about the image; that is, how likely is the given question, in this entire set. Give just the score, no explanation.

\textbf{human}: 

$\langle$counterfactual-image$\rangle$

Question: $\langle$question$\rangle$

\textbf{ai}: 
\end{framed}

\section{Larger VLMs}
Finally, we include the accuracy of two additional baseline models, Llava-1.5-13b and GPT-4o-mini, on both the original VQA tasks and the various tasks in the \segsub dataset (\autoref{fig:baseline_model_performance}). As previously discussed, performance improvements from larger baseline VLMs are limited (Llava-7b vs Llava-13b). None of the baseline models are capable of matching the performance of \segsub finetuned models.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/results/baseline_results.pdf}
    \caption{Baseline model performance on original and perturbed labels for the various datasets and tasks.}
    \label{fig:baseline_model_performance}
\end{figure*}


\section{Robustness Checks}
As models are not trained on irrelevant images, randomly sampling negative image query pairs from across our three datasets is an out-of-distribution task. This evaluates the robustness of our finetuning process on the more trivial cases where the image and query are irrelevant. \autoref{tab:natural_counterfactual_results} shows the full set of results, which as previously discussed reveal that finetuned models have improved performance compared with baseline models. The list of `acknowledgment' terms we consider as admissions of failure to reason over an image query pair due to incomplete information are given in \autoref{tab:ret_acknowledged}. 

Accuracy on OK-VQA negatively sampled counterfactuals is lower, which we attribute to the fact that the task itself is designed in such a way as to require knowledge external to the sources presented to the model. Future work on incorporating retrieval systems that are robust to counterfactual noise is warranted, particularly for open-domain, outside-knowledge tasks such as OK-VQA.


\begin{table}
    \caption{Full results for the randomized negatively sampled robustness check. Models finetuned on \segsub data (-ft) outperform baseline models in identifying images irrelevant to the given query.}
    \label{tab:natural_counterfactual_results}
    \centering
    \begin{tabular}{@{}llll@{}}
\toprule
Model & WebQA              & VQA                 & OK-VQA               \\ \midrule
qwen2-ft   & 0.80  & 0.62 & 0.28  \\
qwen2      & 0.11  & 0.28 & 0.07  \\
phi3-ft    & 0.36  & 0.60 & 0.27  \\
phi3       & 0.30  & 0.34 & 0.37  \\
llava15-ft & 0.24  & 0.59 & 0.28  \\
llava15    & 0.00  & 0.07 & 0.06  \\ \bottomrule
\end{tabular}
\end{table}


\begin{table}
    \caption{A list of terms that baseline models may use to express a failure to answer the given question based on insufficient information.}
    \label{tab:ret_acknowledged}
    \centering
    \begin{tabular}{l}
    $\langle$RET$\rangle$ (i.e. \retlabel) \\
    Sorry \\
    I cannot \\
    I do not \\
    image does not \\
    information \\
    not enough \\
    not clear \\
    not visible \\
    not sure \\
    not able \\
    determine \\
    blurry   \\
    blurred   \\
    no existence \\
    context \\
    apologize \\
    % white background \\
    \end{tabular}
\end{table}

\section{WebQA Accuracy}
\label{sec:accuracy}
Accuracy on the WebQA task is determined by comparing a restricted bag of words (bow) vector between the expected (E) and generated (G) answers;
\begin{equation}
    \label{eq:ACC}
    \text{Acc} = \frac{1}{n}\Sigma [\frac{|\text{bow}_{E} \cap \text{bow}_{G}|}{|\text{bow}_{E}|} == 1]
\end{equation}

The vectorsâ€™ vocabulary is limited to a domain determined by the question type. Questions are classified into domains such as yes/no, color, shape, or number, and each domain uses a predefined vocabulary list (see \autoref{fig:categories}).

\begin{figure}
\begin{lstlisting}
yesno_set = {'yes', 'no'}
color_set = {
    'orangebrown', 'spot', 'yellow', 'blue', 'rainbow', 'ivory', 
    'brown', 'gray', 'teal', 'bluewhite', 'orangepurple', 'black', 
    'white', 'gold', 'redorange', 'pink', 'blonde', 'tan', 'turquoise', 
    'grey', 'beige', 'golden', 'orange', 'bronze', 'maroon', 'purple', 
    'bluere', 'red', 'rust', 'violet', 'transparent', 'yes', 'silver', 
    'chrome', 'green', 'aqua'
}
shape_set = {
    'globular', 'octogon', 'ring', 'hoop', 'octagon', 'concave', 'flat', 
    'wavy', 'shamrock', 'cross', 'cylinder', 'cylindrical', 'pentagon', 
    'point', 'pyramidal', 'crescent', 'rectangular', 'hook', 'tube', 
    'cone', 'bell', 'spiral', 'ball', 'convex', 'square', 'arch', 'h', 
    'cuboid', 'step', 'rectangle', 'dot', 'oval', 'circle', 'star', 
    'crosse', 'crest', 'octagonal', 'cube', 'triangle', 'semicircle', 
    'domeshape', 'obelisk', 'corkscrew', 'curve', 'circular', 'xs', 
    'slope', 'pyramid', 'round', 'bow', 'straight', 'triangular', 
    'heart', 'fork', 'teardrop', 'fold', 'curl', 'spherical', 
    'diamond', 'keyhole', 'conical', 'dome', 'sphere', 'bellshaped', 
    'rounded', 'hexagon', 'flower', 'globe', 'torus'
}   
\end{lstlisting}
\caption{Keywords for WebQA question categories.}
\label{fig:categories}
\end{figure}
% \break


% \section{All results}


% Evaluation on perturbations (\autoref{tab:webqa_perturbed}), conflicts (\autoref{tab:webqa_conflicts}), and counterfactuals (WebQA: \autoref{tab:webqa_counterfactuals}, VQA: \autoref{vqa_counterfactuals}) is given. 

% \begin{table*}[]
% \caption{Evaluation on WebQA perturbed image sources ($n=$) image state-label state}
% \label{tab:webqa_perturbed}
% \begin{tabular}{@{}lllll@{}}
% \toprule
%  & \oldimg-\oldlabel & \oldimg-\updatedlabel & \newimg-\oldlabel & \newimg-\updatedlabel \\ \midrule
% llava-1.5-7b\_v3 & 0.581                     & 0.053                      & 0.194                      & 0.315                       \\
% llava-1.5-7b\_v2          & 0.585                     & 0.051                      & 0.191                      & 0.317                       \\
% llava-hf/llava-1.5-7b-hf                                                                             & 0.542                     & 0.071                      & 0.191                      & 0.552                       \\
% llava-hf/llava-1.5-13b-hf                                                                            & 0.587                     & 0.062                      & 0.192                      & 0.582                       \\
% microsoft/Phi-3-vision-128k-instruct                                                                 & 0.525                     & 0.063                      & 0.199                      & 0.425                       \\
% Qwen/Qwen2-VL-7B-Instruct                                                                            & 0.641                     & 0.065                      & 0.212                      & 0.581                       \\ \bottomrule
% \end{tabular}
% \end{table*}


% \begin{table*}[]
% \caption{Evaluation on WebQA conflicting image sources ($n=$)}
% \label{tab:webqa_conflicts}
% \resizebox{\textwidth}{!}{\begin{tabular}{@{}lllllll@{}}
% \toprule
% & \oldimg-\oldlabel & \oldimg-\retlabel & \oldimg-\updatedlabel & \newimg-\oldlabel & \newimg-\updatedlabel & \newimg-\retlabel \\ \midrule
% llava-1.5-7b\_v3 & 0.606              & 0.000         & 0.028               & 0.387               & 0.239                & 0.173          \\
% llava-1.5-7b\_v2          & 0.631              & 0.000         & 0.029               & 0.390               & 0.244                & 0.159          \\
% llava-hf/llava-1.5-7b-hf                           & 0.567              & 0.000         & 0.026               & 0.377               & 0.379                & 0.000          \\
% llava-hf/llava-1.5-13b-hf                          & 0.628              & 0.000         & 0.021               & 0.432               & 0.352                & 0.000          \\
% microsoft/Phi-3-vision-128k-instruct               & 0.479              & 0.000         & 0.021               & 0.328               & 0.222                & 0.000          \\
% Qwen/Qwen2-VL-7B-Instruct                          & 0.702              & 0.000         & 0.022               & 0.483               & 0.303                & 0.000          \\ \bottomrule
% \end{tabular}}
% \end{table*}



% \begin{table*}[]
% \caption{Evaluation on WebQA counterfactual image sources ($n=$)}
% \label{tab:webqa_counterfactuals}
% \resizebox{\textwidth}{!}{\begin{tabular}{@{}llllllllll@{}}
% \toprule
% & \oldimg-\oldlabel & \oldimg-\anylabel & \oldimg-\retlabel & \blankimg-\oldlabel & \blankimg-\anylabel & \blankimg-\retlabel & \newimg-\oldlabel & \newimg-\anylabel & \newimg-\retlabel \\ \midrule
% llava-1.5-7b\_v3 & 0.498             & 0.895         & 0.008         & 0.327          & 0.782      & 0.008      & 0.467              & 0.875          & 0.008          \\
% llava-1.5-7b\_v2           & 0.494             & 0.895         & 0.008         & 0.333          & 0.782      & 0.008      & 0.471              & 0.872          & 0.008          \\
% llava-hf/llava-1.5-7b-hf                           & 0.315             & 0.805         & 0.000         & 0.311          & 0.790      & 0.175      & 0.307              & 0.732          & 0.008          \\
% llava-hf/llava-1.5-13b-hf                          & 0.381             & 0.984         & 0.000         & 0.342          & 0.973      & 0.101      & 0.354              & 0.930          & 0.012          \\
% microsoft/Phi-3-vision-128k-instruct               & 0.477             & 0.922         & 0.058         & 0.354          & 0.957      & 0.233      & 0.342              & 0.887          & 0.117          \\
% Qwen/Qwen2-VL-7B-Instruct                          & 0.506             & 0.934         & 0.035         & 0.346          & 0.930      & 0.039      & 0.383              & 0.938          & 0.163          \\ \bottomrule
% \end{tabular}}
% \end{table*}

% \begin{table*}[]
% \caption{Evaluation on VQA counterfactual image sources ($n=$)}
% \label{vqa_counterfactuals}
% \resizebox{\textwidth}{!}{\begin{tabular}{@{}lllllll@{}}
% \toprule
%   & \oldimg-\oldlabel & \oldimg-\retlabel & \blankimg-\oldlabel & \blankimg-\retlabel & \newimg-\oldlabel & \newimg-\retlabel \\ \midrule
% llava-1.5-7b\_v3                     & 0.719             & 0.003         & 0.490          & 0.036      & 0.511              & 0.007          \\
% llava-1.5-7b\_v2                     & 0.721             & 0.003         & 0.491          & 0.035      & 0.509              & 0.007          \\
% llava-hf/llava-1.5-7b-hf             & 0.738             & 0.021         & 0.481          & 0.246      & 0.520              & 0.037          \\
% llava-hf/llava-1.5-13b-hf            & 0.737             & 0.022         & 0.451          & 0.287      & 0.530              & 0.054          \\
% microsoft/Phi-3-vision-128k-instruct & 0.716             & 0.110         & 0.423          & 0.477      & 0.458              & 0.392          \\
% Qwen/Qwen2-VL-7B-Instruct            & 0.786             & 0.103         & 0.456          & 0.233      & 0.459              & 0.233          \\ \bottomrule
% \end{tabular}}
% \end{table*}



% \begin{figure*}[htbp]
%     \centering
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \caption{Image 1}
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/purple_bird_1.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \caption{Perturbed Image 1}
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/purple_bird_1_gen.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \caption{Image 2}
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/purple_bird_2.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \caption{Perturbed Image 2}
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/purple_bird_2_gen.png}  
%     \end{minipage}
%     \vspace{0.25em}
%     \caption*{What color do the Agapornis fischeri and Bay-headed Tanager both have on their face?}
%     \par\vspace{0.75em} 

%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[height=\imageheight]{figures/segmentation/perturbed/pink_flower_1.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[height=\imageheight]{figures/segmentation/perturbed/pink_flower_1_gen.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/pink_flower_2.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/pink_flower_2_gen.png}  
%     \end{minipage}
%     \vspace{0.25em}
%     \caption*{What color is at the center of both the Plumeria and the Herbaceous peony?}
%     \par\vspace{0.75em} 

%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/gold_car_1.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/gold_car_1_gen.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/gold_car_2.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/gold_car_2_gen.png}  
%     \end{minipage}
%     \vspace{0.25em}
%     \caption*{What color is both the primary body paint color off the 1954 Maserati A6 GCS and the Lamborghini Veneno?}
%     \par\vspace{0.75em} 

%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/red_ceiling_1.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/red_ceiling_1_gen.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/red_ceiling_2.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/perturbed/red_ceiling_2_gen.png}  
%     \end{minipage}
%     \vspace{0.25em}
%     \caption*{What color are the ceilings in the Grand Bazaar in Istanbul?}
%     \par\vspace{0.75em}

%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[height=\imageheight]{figures/segmentation/perturbed/cone_building_1.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[height=\imageheight]{figures/segmentation/perturbed/cone_building_1_gen.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[height=\imageheight]{figures/segmentation/perturbed/cone_building_2.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.24\textwidth}
%         \centering
%         \includegraphics[height=\imageheight]{figures/segmentation/perturbed/cone_building_2_gen.png}  
%     \end{minipage}
%     \vspace{0.25em}
%     \caption*{What shape is at the top of both the Baiyoke Tower and the Turning Torso skyscraper?}
%     \par\vspace{0.75em}  
    
%     \caption{Examples of original and perturbed images in the \segsub validation set. Baseline samples are comprised of image 1 and 2. Perturbed examples are comprised of perturbed image 1 and 2. Conflicting samples are comprised of (image 1, perturbed image 2) and (perturbed image 1, image 2).}
%     \label{fig:perturbed_examples}
% \end{figure*}










% \section{Rationale}
% \label{sec:rationale}
% % 
% Having the supplementary compiled together with the main paper means that:
% % 
% \begin{itemize}
% \item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
% \item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
% \item When submitted to arXiv, the supplementary will already included at the end of the paper.
% \end{itemize}
% % 
% To split the supplementary pages from the main paper, you can use \href{https://support.apple.com/en-ca/guide/preview/prvw11793/mac#:~:text=Delete%20a%20page%20from%20a,or%20choose%20Edit%20%3E%20Delete).}{Preview (on macOS)}, \href{https://www.adobe.com/acrobat/how-to/delete-pages-from-pdf.html#:~:text=Choose%20%E2%80%9CTools%E2%80%9D%20%3E%20%E2%80%9COrganize,or%20pages%20from%20the%20file.}{Adobe Acrobat} (on all OSs), as well as \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.



% During review, several generative artifacts were noted.

% \begin{figure}
%     \centering
%     % \includegraphics[width=0.9\linewidth]{figures/segmentation/counterfactual/counterfactual_30003391_d5c0f39c0dba11ecb1e81171463288e9_masked.png}
%     \includegraphics[width=0.97\linewidth]{figures/segmentation/counterfactual/counterfactual_30283286_d5ca118e0dba11ecb1e81171463288e9_masked.png}
%     \includegraphics[width=0.9\linewidth]{figures/segmentation/counterfactual/counterfactual_30015968_d5c745d00dba11ecb1e81171463288e9_masked.png}
%     \includegraphics[width=0.9\linewidth]{figures/segmentation/counterfactual/counterfactual_30265978_d5c944fc0dba11ecb1e81171463288e9_masked.png}
%     \includegraphics[width=0.9\linewidth]{figures/segmentation/counterfactual/counterfactual_30272063_d5c2f8e00dba11ecb1e81171463288e9_masked.png}
%     \caption{Counterfactual perturbations remove the object of the question from the image, thus making the question unanswerable.\nr{use larger fonts in figures}}
%     \label{fig:counterfactual_examples}
% \end{figure}

% \begin{comment}
% \begin{figure}[htbp]
%     \centering
%     \begin{minipage}{0.22\textwidth}
%         \centering
%         \caption{Object Segmented}
%         \includegraphics[width=\textwidth]{figures/segmentation/counterfactual/bat_original.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.22\textwidth}
%         \centering
%         \caption{Object Removed}
%         \includegraphics[width=\textwidth]{figures/segmentation/counterfactual/bat_removed.png}  
%     \end{minipage}    
%     \vspace{0.25em}
%     \caption*{Is he holding a baseball BAT?}
%     \par\vspace{0.75em} 

%     \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/counterfactual/Ears_original.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/counterfactual/Ears_removed.png}  
%     \end{minipage}    
%     \vspace{0.25em}
%     \caption*{Are the EARS of the Persian Leopard wider than its paws?}
%     \par\vspace{0.75em} 

%     \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/counterfactual/tie_original.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/counterfactual/tie_removed.png}  
%     \end{minipage}    
%     \vspace{0.25em}
%     \caption*{What type of knot is used on this man's TIE?}
%     \par\vspace{0.75em} 

%     \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/counterfactual/bundles_original.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/segmentation/counterfactual/bundles_removed.png}  
%     \end{minipage}    
%     \vspace{0.25em}
%     \caption*{How many BUNDLES of bananas are there?
% }
%     \par\vspace{0.75em} 

%     \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[trim={0 0 0 6cm},clip, width=\textwidth]{figures/segmentation/counterfactual/bird_original.png}  
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[trim={0 0 0 6cm},clip, width=\textwidth]{figures/segmentation/counterfactual/bird_removed.png}  
%     \end{minipage}    
%     \vspace{0.25em}
%     \caption*{What type of BIRD is sitting on the buoy?}
%     \par\vspace{0.75em}  
    
%     \caption{Examples of original and counterfactual images from the \segsub generations.}
%     \label{fig:counterfactual_examples}
% \end{figure}

% \end{comment}


% \begin{figure}
%     \centering
%     % \includegraphics[width=0.9\linewidth]{figures/segmentation/counterfactual/counterfactual_30003391_d5c0f39c0dba11ecb1e81171463288e9_masked.png}
%     \includegraphics[width=0.9\linewidth]{figures/segmentation/conflict/conflict_d5be1f3c0dba11ecb1e81171463288e9_masked.png}
%     \includegraphics[width=0.9\linewidth]{figures/segmentation/conflict/conflict_d5c159040dba11ecb1e81171463288e9_masked.png}
%     % \includegraphics[width=0.9\linewidth]{figures/segmentation/conflict/conflict_d5c36ba40dba11ecb1e81171463288e9_masked.png}
%     \includegraphics[width=0.9\linewidth]{figures/segmentation/conflict/conflict_d5cdde720dba11ecb1e81171463288e9_masked.png}
%     \includegraphics[width=0.9\linewidth]{figures/segmentation/conflict/conflict_d5cd6d200dba11ecb1e81171463288e9_masked.png}
%     \caption{Perturbations to an object in an image can introduce conflicts when compared with other images of the same object. We call these visual knowledge conflicts.\nr{1. use larger fonts in figures. 2. regenerate images to be the same size so they look more consistent}}
%     \label{fig:conflict_examples}
% \end{figure}