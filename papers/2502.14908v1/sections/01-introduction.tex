
Recent advancements in vision language models (VLMs) have led to AI assistants capable of Visual Question Answering (VQA). Given few image sources and a text-based question, a VQA system generates a relevant response by interpreting the content in the images, and understanding the intent of the question.
% They have a wide range of real world applications from improving accessibility to robotics, by answering questions about the environment.
Prior work has found that unimodal question answering (QA) models are not robust to knowledge conflicts that arise between parametric knowledge (encoded in the model weights during training) and contextual knowledge (external knowledge sources given to the model)~\citep{neeman_disentqa_2022}. While a body of research improves the robustness of unimodal LLMs to conflicts \citep{longpre_entity-based_2022}, multimodal robustness studies \citep{liu2024visual} have not addressed multimodal conflicts \citep{conflicts-main-survey}.

We aim to address this gap and investigate three different types of multimodal knowledge conflict in the VQA setting, namely, parametric conflicts (arising between the encoded knowledge and external input information source), source conflicts (between two input information sources) and counterfactual conflicts (such that a query cannot be answered with the given input information source), see Section~\ref{sec:tasks}. We propose \segsub\footnote{\codebaseurl},
a framework to enhance the reasoning abilities of vision-language models (VLMs) over knowledge conflicts through constrained dataset augmentation.

\segsub extends existing VQA datasets by introducing augmentations for each type of knowledge conflict. First, we generate parametric conflicts, where image perturbations alter attributes like the shape or color of the object in question, therefore changing the expected response (for example, replacing the color of the horse, as demonstrated in ~\autoref{fig:seg_sub_pipeline}). Next, we generate counterfactual conflicts where image perturbations remove the object in question therein making it impossible to answer the question using the new image (for example, removing the bat from the child's hand and asking what the child is holding as demonstrated in ~\autoref{fig:bat_boy}). Lastly, we generate source conflicts where one of two image sources is modified to create a conflict that makes the image source inconclusive (for example, presenting the model with 2 images of the same room, where one of them was altered and asking the model for the color of the ceiling, as shown in~\autoref{fig:perturbed_examples}).  

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/segmentation/webqa_segment_infill.png}
    \vspace{-2mm}
    \caption{The \segsub framework: given a VQA task, we perturb existing (image, question, answer) triples with new images and answers to augment the dataset.}
     \vspace{-4mm}
    % d5c76d760dba11ecb1e81171463288e9
    \label{fig:seg_sub_pipeline}
\end{figure}


We apply \segsub on three datasets, WebQA~\cite{chang_webqa_2021}, VQAv2~\cite{goyal2017making} and OKVQA~\cite{marino_ok-vqa_2019}. The resulting knowledge conflict dataset includes over 35,000 perturbed samples\footnote{\dataseturl}. 
We then use \segsub data to evaluate model performance on three types of knowledge conflict. We find that VLMs are largely robust to parametric conflicts, with models generating the original label for perturbed samples $\sim$20\% of the time (\autoref{fig:parametric_effect}). In contrast, VLMs by and large fail to recognize source conflicts and often hallucinate responses to counterfactual conflicts. Even the best-performing VLM identifies generated counterfactuals only 30\% of the time, while none of the baseline VLMs can resolve source conflicts (accuracy $<1\%$). Instead, they attend to a single image source (at random) and ignore conflicting sources. We attribute this shortcoming in reasoning over multiple image sources to a lack of multimodal, multihop training data. 

Finally, we find that counterfactual samples where the image question pair is highly contextualized provoke VLMs to hallucinate. Moreover, finetuning consistently improves VLM robustness to counterfactual conflicts. Our framework thereby enables future research to strengthen model resilience against conflicting multimodal information sources in complex visual reasoning tasks.


%  ------------ INTRO GRAVEYARD -----------------
% (entity substitution~\citep{longpre_entity-based_2022}, knowledge conflicts \citep{chen_rich_2022}, counterfactual noise \citep{hong_why_2024}, and the parametric effect \citep{neeman_disentqa_2022}), 
% While instruction tuning has been shown to improve robustness of VLMS \citep{liu2024visual}, reasoning under conflicting sources of information in VLMs
% Recent research has underscored this limitation, highlighting the need for understanding how multimodal models behave when presented with conflicting information sources \citep{conflicts-main-survey}. 

%The recent development of vision language models (VLMs) has paved the way for Visual Question Answering (VQA) systems which enhances the means of interactions with AI assistants. Given few image sources and a text-based question, a VQA system generates a relevant response by interpreting the content in the images, and understanding the intent of the question. They have a wide range of real world applications from improving accessibility to robotics, by answering questions about the environment. 

%VQA systems typically have three main components, which includes, (i) the visual component: extracts and analysis features from the image sources that are relevant to the subject of the question, (ii) the natural language component: analysis the intent of the text-based question to determine the type of information needed from the image sources, and (iii) the response generator, which generates the appropriate answer to the question based on the image sources.

% The recent development of  has paved the way for Visual Question Answering (VQA) systems, which enhance interactions with AI assistants. Given a set of image sources and a text-based question about those images, a VQA system generates a relevant response by interpreting image content and understanding the questionâ€™s intent. These systems have a wide range of real-world applications, from improving accessibility to aiding robots.

%\todo{Insert proper intro paragraph here.}

% This is the first work aimed at closing the gap for the vision-language setting. 



% , it only focuses on the models ability to generalize to different questions, and does little to diversify the image sources the questions are based on. 


% VLMs are generative multimodal models that learn from both images and text to produce text outputs, typically using cross-modal attention. Unlike unimodal LLMs trained on large text corpora, VLMs rely on image-text pairs from general caption and task-specific VQA datasets. These datasets are expensive to create and limited in size, hindering the development of visual reasoning beyond image summarization. Recent work on instruction tuning VQA models with augmented datasets enhances robustness against question variation but does not address image variation, or reasoning under conflicting sources of information in VLMs \citep{liu2024visual}.
% In contrast, gathering image-question-answer triples for training VQA models using crowd source workers is much more expensive.
% , compared to the scale of textual data unimodal LLMs can be trained on. 

% VLMs are generative multimodal models that learn from both images and text, using cross-modal attention mechanisms to generate text outputs. Unlike unimodal large language models (LLMs), which are trained on vast amounts of textual data from the internet, VLMs are trained on paired image and text data from image caption datasets. These datasets are expensive to create and limited in size compared to the extensive textual data available for training unimodal LLMs.




% Next we evaluate the robustness of existing VLMs against the augmented \segsub data, and compute the performance against each of the conditions. This allows us to evaluate whether the models are capable of (i) adapting their responses to feature modifications that changed the expected answer, (ii) recognizing when a counterfactual image source does not contain the answer to the given question, (iii) recognizing when conflicts exist between the image sources and the given question. 


% This is particularly important for models that have been already trained the images in COCO image caption data \citep{lin2014microsoft} or Wikipedia, from which VQAv2, OK-VQA, and WebQA are derived.



% We examine how VQA models rely on parametric memory when given image captions, as overly descriptive captions may elicit a response based on learned textual data. Evaluating responses to images with feature modifications, we find that VLMs are generally robust to these perturbations, adapting their answers to the modified images. While LLMs are sensitive to textual perturbations in QA tasks, our results suggest that the diversity introduced by image modifications is still within the distribution of training samples, allowing models to generalize effectively.

% The degree to which these VQA models rely on parametric memory when presented with image captions is also of interest, as overly descriptive captions may trigger a parametric response learnt from textual data. To investigate this, we evaluate model responses when presented with images that have feature modifications. While prior work has shown that LLMs are sensitive to textual perturbations in QA tasks, we find that VLMs are generally robust to image perturbations with feature modifications and adapt their responses to the modified images. While the perturbations introduce diversity in the image samples, the questions being asked on these images are still well within the distribution of the training samples, which may explain why the models are able to generalize to these samples. 



% \vspace{-3mm}

% \paragraph{Summary of contributions:}
% \begin{enumerate}
%     \item We publicly release code for the \segsub  framework\footnote{\codebaseurl}, which applys targeted perturbations on images with the goal of enhancing robustness of VLMs against counterfactual images and knowledge conflicts.
%     \item We publicly release \segsub  data\footnote{\dataseturl}, by augmenting existing three datasets, namely, WebQA, VQA and OKVQA, which we use for both evaluation and finetuning.
%     \item We present the first study on evaluating the robustness of current VLM models on multimodal knowledge conflicts and counterfactual examples using \segsub data. 
%     \item We finetune various VLM models using \segsub data and demonstrate improved robustness against counterfactuals, with minimal regression in performance on the original datasets.
%     \item We contribute analyses to better understand the rate of \update{contextual} hallucination in these models using \segsub data.
% \end{enumerate}

