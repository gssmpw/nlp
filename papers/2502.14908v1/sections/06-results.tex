\subsection{Qualitative Results}
After generating a large number of samples (\textgreater$200,000$), we apply quality checks to remove noisy generations, resulting in approximately $35,225$ samples. See \autoref{fig:counterfactual_examples}  for examples of counterfactual image generations and \autoref{fig:perturbed_examples} for parametric and source conflicts.

Two raters independently labeled a subset of $100$ quality-checked generations for each category of conflicts to determine if the new label (\retlabel or \updatedlabel) matches the perturbed image---see label quality ratings in \autoref{tab:dataset_description}. Counterfactuals have a higher quality rating (\textgreater90\%). Parametric (76\%) and source conflicts (82\%) produce more noisy generations which we attribute to the increased difficulty in replacing an object versus removing it. Raters only disagreed on a small fraction of samples (30/300), while a Cohen's Kappa of 0.45 reflects that disagreements happened only on lower quality generations \cite{delgado2019cohen}.
% However, since these disagreements are on the  ~\cite{delgado2019cohen} this results in a Cohen's Kappa of 0.45. 

%The low Kappa score is a consequence of class imbalance ~\cite{delgado2019cohen} which, for instance, can be seen from the high quality rating score (87-93\%) for counterfactual generations.


% \begin{figure*}
%     \centering
%     \vspace{-4mm}
%     \includegraphics[width=0.8\linewidth]{figures/results/finetuning_eval.pdf}
%     \vspace{-2mm}
%     \caption{Evaluation of baseline (-Base) and \segsub finetuned (-Ft) model accuracy on counterfactual and source conflicts (higher is better). Evaluation on original samples from VQAv2, OK-VQA, and WebQA datasets shows that finetuning does not result in performance regression on these tasks (except on WebQA two-image samples). Finetuned models outperform baselines across all types of knowledge conflict.}
%     \vspace{-4mm}
%     \label{fig:finetuning_results}
    
% \end{figure*}


\vspace{-2mm}
\paragraph{Parametric Conflicts}
While Phi3 model does benefit somewhat from finetuning (4\% drop in parametric response rate), Qwen2 and Llava are unaffected. Parametric response rates are low across the board ($\sim$20\%, \autoref{fig:parametric_effect}), showing that baseline models are already robust to conflicts between input sources and parametric memory.

\begin{figure}
    \centering
   
    \includegraphics[width=0.8\linewidth]{figures/results/parametric_plot.pdf}
    % \includegraphics[width=0.9\linewidth]{figures/results/perturbed_1_vs_2_image.pdf}
    \vspace{-4mm}
    \caption{Parametric effect analysis: how often does the model predict the original label for perturbed images? Lower is better, implying a reduced parametric effect.}
    \label{fig:parametric_effect}
    \vspace{-2mm}
\end{figure}


\subsection{Quantitative Results}
In \autoref{fig:finetuning_results} we find that baseline VLMs fail to acknowledge counterfactual conflicts (Counter) and source conflicts (Source). Finetuning mitigates this across every dataset. The resulting finetuned models (-Ft) outperform the baseline models (-Base) on perturbed samples. Finetuning has some benefit on the original samples (Original) for VQA and WebQA counterfactual sources, but a large performance regression is apparent for samples with source conflicts in WebQA.

%\begin{figure}[!ht]
%    \centering
%     \includegraphics[width=0.9\linewidth]{figures/results/perturbed_1_vs_2_image.pdf}
%     \caption{Finetuning on samples generated to induce parametric conflicts improves performance on original datasets for one image questions, but regresses it for the two image multihop case. \todo{update image or caption, currently inconsistent}}
%     \label{fig:one_two_image}
% \end{figure}


%Baseline models are capable of predicting the generated labels for samples generated to induce parametric conflicts (\autoref{fig:one_two_image}). Furthermore, finetuning on these augmented samples does not lead to a substantial improvement in model performance on the original labels. 

%As this perturbation category can have either one or two images per sample, we partition our evaluation into a single image set and a two image set. We find that for finetuned models, there is a gulf in performance between one and two image samples (\autoref{fig:one_two_image}). We attribute this performance gap to the increased difficulty in multihop reasoning. Rather than learning to determine if a two image sample has a valid answer, finetuned models over-predict the \retlabel, indicating a source conflict. Thus, performance is deteriorated for original two image samples from the WebQA dataset.



% As such, training models with images that differ only in the concept in question does not seem to improve performance, at least for the color and shape categories. We qualify this by the observation that questions concerning color and shape attributes are well represented in popular VQA datasets, including all three datasets in this study. As such, adding perturbed samples in these categories will have diminishing returns at best. 

% It is worth noting that these samples are still necessary to scaffold the model in learning the concept of knowledge conflicts so that the model cannot shortcut to predicting that every set of generated samples with 2 images is a conflict.

\vspace{-2mm}
\paragraph{Source Conflicts}
For WebQA samples with source conflicts, the finetuned models have extremely low accuracy on original samples. This is a result of the finetuned models failing to predict the old label and instead overpredicting the \retlabel when presented with two images. %We attribute this to the high amount of noise with knowledge conflict generations achieving a quality rating of only 82\% (\autoref{tab:dataset_description}), and the multihop nature of the knowledge conflict task. 
Interestingly, instead of generating an `acknowledgement' response, baseline models tend to predict one of the two incorrect answers---either the original label (for the unperturbed image) or \updatedlabel (for the perturbed image)---uniformly at random. 
% See \autoref{tab:webqa_conflicts} in the supplementary.


\begin{figure*}
    \centering
    \vspace{-4mm}
    \includegraphics[width=0.8\linewidth]{figures/results/finetuning_eval.pdf}
    \vspace{-2mm}
    \caption{Evaluation of baseline (-Base) and \segsub finetuned (-Ft) model accuracy on counterfactual and source conflicts (higher is better). Evaluation on original samples from VQAv2, OK-VQA, and WebQA datasets shows that finetuning does not result in performance regression on these tasks (except on WebQA two-image samples). Finetuned models outperform baselines across all types of knowledge conflict.}
    \vspace{-4mm}
    \label{fig:finetuning_results}
    
\end{figure*}



\vspace{-2mm}
\paragraph{Counterfactual Conflicts}
Baseline models perform poorly on counterfactual conflicts, with no model achieving more than 30\% accuracy. Since these models are not trained to return the \retlabel, we consider any admission of failure by the model as a \retlabel. These baseline models are sometimes able to determine when an image lacks the information required to answer a question, they are not robust to these samples. Finetuning on enables these models to identify counterfactual conflicts with high accuracy, without degrading performance on the original datasets. Additionally, finetuning provides a 5-10\% performance gain on the original samples from WebQA and VQA datasets.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/results/context_scores.pdf}
    \vspace{-3mm}
    \caption{Decreased Accuracy on Counterfactual Conflicts in finetuned VLMs (and GPT4-o-mini) with Increasing Image Contextualization Scores. Baseline unsmoothed data is in the background.}
    \label{fig:context_scores}
    \vspace{-4mm}
\end{figure}


\vspace{-2mm}
\paragraph{Robustness of Counterfactual Conflicts}
We find that finetuned models are robust in detecting randomized counterfactual samples. They are not simply detecting images that have been modified by LaMa to remove objects. The finetuned Qwen2 model predicts \retlabel for 80\% of the randomized counterfactuals sampled from the WebQA dataset. \autoref{tab:natural_counterfactual_results} in the supplementary has further details.

\vspace{-2mm}
\paragraph{Parameter Size}
We find that performance improvements on the evaluation metrics derived from increasing model size have diminishing returns. There exists a gap in performance between SoTA models (i.e. GPT-4o-mini) and the finetuned models (see \autoref{fig:baseline_model_performance} in the supplementary).

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/segmentation/context/baseball_boy.jpeg}  % Replace with your figure file
        \caption{ChatGPT: "There doesnâ€™t appear to be an object clearly visible in his hands."}
        \label{fig:bat_boy}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/segmentation/context/bat_removed.png}  % Replace with your figure file
        \caption{ChatGPT: "The batsman in the image is holding a baseball bat as he prepares to swing."}
        \label{fig:bat_batsman}
    \end{subfigure}
    \caption{These counterfactual examples were generated by removing a baseball bat from two different VQA images. When asked 'what is he holding?', ChatGPT only hallucinates in the highly contextualized case (right).}
    \label{fig:baseball_example}
\end{figure}


\paragraph{Image-Question Contextualization}
Intuitively, image-question contextualization relates to contextual cues within an image that provides the models with clues to answer the question, as in \autoref{fig:baseball_example}. We find evidence for a link between image-question contextualization, as approximated by GPT-4o-mini, and accuracy on counterfactual samples. \autoref{fig:context_scores} reveals that models perform poorly in identifying a sample as counterfactual (i.e. lower accuracy of predicting \retlabel) and is more likely to hallucinate on heavily contextualized image question pairs. Interestingly, GPT-4o-mini hallucinates for all of the counterfactual examples given in \autoref{fig:counterfactual_examples}.

For a concrete example, see \autoref{fig:baseball_example}, where both counterfactual examples were generated by removing a baseball bat. Here, a poorly contextualized image question pair features a child standing in a field with the question "what is he holding?" (\autoref{fig:bat_boy}). The only contextual cues as to what the child might have been holding are the generic outdoor setting, and the child's body positioning. Contrasting this in the adjoining sample is a baseball player, adorned in a jersey with his player number printed on the back, in a stadium filled with sporting fans (\autoref{fig:bat_batsman}). ChatGPT recognizes that the child is holding nothing, but hallucinates a bat in the hands of the batsman. Alongside previous works that show a relationship between image context and object detection \citep{beery2018recognition}, these results indicate that contextual cues have a priming effect that induces hallucinations in VLMs for highly contextualized counterfactuals.
% \citep{beer}. 



% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c}
%          &  \\
%          & 
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:qualatitive_examples}
% \end{table}
