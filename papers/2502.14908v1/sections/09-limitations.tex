Our framework effectively generates and evaluates parametric, source, and counterfactual conflicts across VQA datasets. However, three key limitations may affect its generalizability: reliance on VLMs for quality checks, residual and generative artifacts, and image-question contextualization.

First, we rely on smaller quantized VLMs for quality assurance which may introduce an additional source of error. A fine-grained visual and semantic understanding in the VLM could lead to overlooked errors in perturbation or segmentation that affect the datasetâ€™s overall quality. Although we manually review a subset of outputs from each perturbation type to gauge quality, the effectiveness of quality control could be enhanced by leveraging more powerful models or ensemble-based methods. We also note the possibility of the quality-check ruling out high quality generations. However, this is less of a concern as we wish to minimize false positives in the dataset, and we can compensate simply by generating more samples.

Second, handling residual artifacts left after object removal, like shadows or reflections, is challenging. These artifacts can indicate the previous presence of objects, introducing noise and inconsistencies that may mislead models that are sensitive to visual details. While we mitigate this partially through manual evaluation and quality checks, future work could explore advanced inpainting or shadow removal for cleaner counterfactuals.

Current generative methods suffer from quality issues, with artifacts like blurred infilled regions and excessive noise in segmented areas, despite high quality ratings across perturbation categories. Emerging text-to-image editing models \citep{hui_hq-edit_2024,bodur_iedit_2023,zhang_magicbrush_2024} may help address these issues. While we employ a rule-based segmentation approach, these models dynamically infer infill regions from input prompts. Given the lower quality ratings for knowledge conflict perturbations, future work should explore new generative methods to improve this aspect.

Finally, our analysis of these hallucinations follows a naive approach where image-question contextualization is determined by GPT-4o-mini. Alternatively, generating question sets for each image and computing text similarity with dataset questions could enhance contextualization. Informed by our findings on VLM hallucinations, future work is needed to refine this approach (\autoref{fig:context_scores}). 


 % Finally, we find that performance on multihop reasoning tasks is poor (\autoref{fig:one_two_image}). Future work can consider new architectures that build fusion not only across inputs of multiple modalities, but also across a variable number of these sources.




% while many of the samples that possess such generative artifacts are filtered out by VLM quality checks, many positive samples of counterfactual images are also discarded. 
% For example, all the models we tested hallucinate on the batsman example (\autoref{fig:bat_batsman}). As such, despite being an example of a high-quality generation that induces hallucinations, it fails the quality check, as the quality check VLM believes that the baseball bat has not been removed in this image. Ironically, it is therefore not included in the \segsub dataset. This represents a limitation for identifying hallucinations, as many high-quality generations will be discarded if the quality check VLM hallucinates.



% As such, these methods may be more useful for the knowledge conflict category, where the goal is not to induce hallucinations, but to improve the multihop reasoning ability of VLMs over multiple image sources. Manual evaluation found that \segsub knowledge conflicts were lower quality than the other perturbations, and so 
% \footnote{Models capable of removing objects and their shadows are already available, i.e. \url{https://huggingface.co/spaces/finegrain/finegrain-object-eraser}}








% \begin{itemize}
%     \item Noise in generated samples
%         \subitem the Shadows of objects are not removed during counterfactual generations
%     \item Use of small models for quality checks
%     \item In real world applications, a good enough retriever may ignore conflicting / counterfactual / perturbed samples
% \end{itemize}
