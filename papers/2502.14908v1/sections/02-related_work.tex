% Our work stands at the intersection of several well-established pillars of research---knowledge conflicts in text sources \citep{conflicts-main-survey,conflicts-survey-1}, dataset augmentation for QA \citep{neeman_disentqa_2022,dataset-gen-attack,dataset-gen-contradoc,dataset-gen-contraqa}, VQA tasks \citep{marino_ok-vqa_2019,goyal2017making}, and diffusion models for text-to-image generation \citep{ravi_sam_2024,liu_grounding_2024,rombach_high-resolution_2022,yu_inpaint_2023,rombach_high-resolution_2022}.
% Our work stands at the intersection of several well-established pillars of research---knowledge conflicts in text sources, dataset augmentation for VQA tasks, and diffusion models for text-to-image generation.

Prior work on addressing parametric conflicts falls into two broad categories; the construction of evaluation datasets to quantify where and when conflicts occur, and method-based contributions to train QA models to overcome their reasoning limitations. Along these lines, our work extends diffusion models for conditional image generation to investigate knowledge conflicts in the multimodal setting.


% \vspace{-2mm}
%\subsection{Augmenting Reasoning Tasks for LLM Evaluation \nr{rephrase text to avoid overflow}}
% \paragraph{Robustness of LLMs against reasoning tasks} 
% Furthermore, the reasoning required by VQA datasets is likely quite different than that required for arithmetic, logical reasoning, and algorithmic problems, and so the findings on these domains may not translate to the vision language setting. We aim to address this gap in the literature by applying image perturbations to augment VQA datasets for the purpose of VLM evaluation.
% \paragraph{Knowledge conflicts in QA tasks}
\paragraph{Knowledge Conflict Evaluation}
Recent work on evaluation has shown that LLMs are not robust to perturbations in text-based reasoning tasks \citep{zhang_darg_2024,mirzadeh_gsm-symbolic_2024,zhu2023dyval,wang2024benchmark} and that LLM performance degrades when conflicts exist in the source data for QA tasks \cite{conflicts-main-survey,conflicts-survey-1}. Longpre et al. \citep{longpre_entity-based_2022} introduced an entity-based knowledge conflict framework for evaluating how models handle conflicting information between learned parametric knowledge and contextual (non-parametric) data. Chen et al. \citep{chen_rich_2022} evaluate QA model on source conflicts. Hong et al. \citep{hong_why_2024} induce hallucinations in retrieval-augmented models by introducing counterfactual noise, which they define as conflicting but contextually relevant information. They also find that retrieval-augmented models ignore conflicting sources. 
% This bias can lead to reduced confidence and less accurate answers. Our work seeks to explore these issues in a multimodal context. 

\paragraph{Knowledge Conflict Fine-tuning}
Attempts to address this reasoning gap in LLMs include fine-tuning on both human annotated \cite{dataset-hum-contradict-wiki,dataset-hum-contradict-claim} and LLM generated \cite{dataset-gen-attack,dataset-gen-contradoc,dataset-gen-contraqa} datasets. Generative approaches involve extending a base dataset like SQuAD \cite{dataset-squad} to include sources with conflicting information \cite{controllable-memory}. Neeman et al. adopt a combination of prompting and entity-substitution techniques for data augmentation on textual QA datasets, producing the DisentQA\citep{neeman_disentqa_2022}. Recent work demonstrates that LLMs can be trained to retrieve more relevant context when the parametric information and provided sources are insufficient \citep{labruna2024retrieve,wang2024learningretrieveincontextexamples}. However, these methods do not focus on multimodal QA tasks~\cite{conflicts-main-survey} and our work builds on these foundations by fine-tuning VLMs with knowledge conflicts to recognize when visual evidence is insufficient to complete the VQA task.

% Our work builds on this idea by applying similar principles to VQA datasets, allowing us to observe how Vision Language Models (VLMs) navigate visual knowledge conflicts.

% \paragraph{Datasets for evaluating knowledge conflicts} 


% , diminishes performance in scenarios requiring nuanced reasoning.  By fine-tuning models on datasets that include structured conflicts, our research aims to evaluate how effectively VLMs can disentangle and adapt to competing visual sources in VQA tasks.

% Existing work primarily focuses on only text-based knowledge conflicts whereas in this work, we propose a curated knowledge conflict dataset consisting of images. Following the approach in DyVal \cite{wang2024benchmark} we run a post-generation validation check for each our of generations to filter out noisy samples.

\paragraph{Conditional Image Generation}
Along with discriminative models that can segment images \citep{ravi_sam_2024,liu_grounding_2024}, advancements in Computer Vision have resulted in diffusion models that can generate images \citep{rombach_high-resolution_2022} based on textual prompts. Generative Adversarial Networks have proven successful in conditional generation \citep{lu_cigli_2021}, such as modifying the color of specific objects in an image \citep{khodadadeh2021automatic}. While naive approaches to counterfactual robustness include image masking \citep{chen2020counterfactual} and noising \citep{ishmam2024visual}, these recent advances enable a generative approach.

Counterfactual image generation has been used for several distinct tasks, from human AI teaching \citep{goyal_counterfactual_nodate} and object classification \citep{sauer_counterfactual_2021}, to model explainability \citep{vermeire_explainable_2022,chang_explaining_2019}. Overall, the focus is on image classifiers, how they are susceptible to noise, and how counterfactuals can help interpret the inner workings of these classifiers. As of yet, counterfactual image generation has not been used for inducing knowledge conflicts. In this work, we apply image segmentation \citep{yu_inpaint_2023,rombach_high-resolution_2022,suvorov_resolution-robust_2022} and conditional image generation to create counterfactual images by segmenting and then infilling or inpainting objects in an image. This method allows us to augment existing VQA datasets and finetune VLMs to enhance robustness against knowledge conflicts and counterfactual samples.

% \paragraph{\todo{Counterfactual Image Generation}}

% \paragraph{Instruction-based Image Editing}

% \begin{enumerate}
%     \item \citep{goyal_counterfactual_nodate}: CF visual explanations. very different task from us
%     \item \citep{sauer_counterfactual_2021}: CF generative networks. "CFs improve out of distribution robustness, with marginal drop in original (image classification) task". We can use this to back up our findings, but our tasks/goal is different.
%     \item \citep{vermeire_explainable_2022}: Explaining image classifiers by CF generations: the infill generation is similar, can use it to back up our work
%     \item \citep{chang_explaining_2019}: similar to \citep{vermeire_explainable_2022}. 
% \end{enumerate}



% datasets and models for Instruction-based Image Editing, wherein models are evaluated on how faithfully they can follow text-based image editing prompts \citep{hui_hq-edit_2024,bodur_iedit_2023,zhang_magicbrush_2024}. The segmentation substitution method proposed in this work is parallel to these efforts and may be used to generate synthetic instruction-based image editing datasets similar to \cite{bodur_iedit_2023}. However, the focus of this paper will be on the application of these methods to the augmentation of visual reasoning tasks.
% % Lynnette's GAN \citep{lu_cigli_2021}