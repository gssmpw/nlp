\begin{table}[t]
    \caption{Distribution of the VQA datasets.}
    \vspace{-2mm}
    \label{tab:org_dataset}
    \centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{lrr}
    \toprule
        \textbf{Dataset} & \textbf{\# Training samples} & \textbf{\# Validation samples} \\
        \midrule
        
        WebQA & 8634 & 1081 \\
        VQAv2 & 7765 & 1830 \\
        OK-VQA & 0 & 474 \\
        \midrule
        \segsub & 30155 & 5070 \\
        \midrule
        Total & 46554 & 8455\\
        \bottomrule
    \end{tabular}}
    \vspace{-4mm}
\end{table}

\segsub is a framework designed to enhance the robustness of VLMs by augmenting existing VQA datasets with the intention of introducing knowledge conflicts using perturbed images. Quality checks ensure that noisy perturbations are filtered out before we finetune models on the generations. Model performance is then evaluated on both the original and perturbed datasets. Finally, we analyze the effect of image-question contextualization on hallucination rate for counterfactual conflicts.

%Inspired by the Entity Replacement Framework \citep{longpre_entity-based_2022}, we term our framework for generating perturbed images Segmentation Substitution (SegSub). 

\subsection{The \segsub~Framework}
\label{sec:framework}
%Through targeted perturbations following the process outlined in \autoref{fig:seg_sub_pipeline}, the \segsub framework generates image samples on which existing VLMs struggle to reason. 
\autoref{fig:seg_sub_pipeline} gives an overview of the framework. First, given a QA pair with image sources $i_1, ..., i_n$, we prompt Gemini-1.5-flash to extract the noun that functions as the object of the question. We then prompt the Segment Anything Model v2 (SAMv2) \citep{ravi_sam_2024, liu_grounding_2024} to segment the object of the question in each of the images $i_1, ..., i_n$. Finally, we apply a perturbation to the segmented regions by either removing the object from the image using Large Mask Inpainting (LaMa) \citep{suvorov_resolution-robust_2022} or changing the color or shape of the object using Stable Diffusion \citep{rombach_high-resolution_2022}. These perturbations are used to generate different kinds of augmentations that enable us to study the reasoning ability of the models on the three types of knowledge conflict.

\begin{figure}
    \centering
    %  [trim={left bottom right top},clip]
    \includegraphics[width=\linewidth,trim={0cm 4cm 10cm 0cm}]{figures/results/counterfactual_examples_grid.pdf}
    \caption{Examples of original images and counterfactual image generations. At the time of writing, ChatGPT hallucinates on these examples.}
    \label{fig:counterfactual_examples}
\end{figure}

\subsection{Knowledge Conflict Types}
\label{sec:tasks}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth,trim={0cm 4cm 0cm 0cm}]{figures/results/perturbed_examples_grid.pdf}
    
    \caption{Examples of original and perturbed images in the \segsub validation set. Baseline samples are comprised of image 1 and 2. Perturbed examples are comprised of perturbed image 1 and 2. Conflicting samples are comprised of (image 1, perturbed image 2) and (perturbed image 1, image 2).}
    \label{fig:perturbed_examples}
\end{figure*}

We look at three main types of conflicts between different sources of information, and study the reasoning abilities of different models on them.

\noindent(i) \textit{Counterfactual conflicts}: We introduce conflicts between the query and image source. We do so by removing the object in question from the image source to invalidate the premise of the question. As a result, any answer except for requests for more information, or statements about lacking information (\retlabel) are incorrect (\autoref{fig:counterfactual_examples}). 


\noindent(ii) \textit{Parametric conflicts}: Here we introduce conflicts between the encoded knowledge (embedded in the learned weights) and an input information source, in this case the perturbed image. To study this effect, we alter attributes like the shape or color of the object under consideration in the image, therefore changing the expected response to the new label, \updatedlabel. This requires the model to rely on the new image and ignore any learned knowledge it may have about the image to answer the question correctly (for example, \autoref{fig:perturbed_examples}).
    
\noindent(iii) \textit{Source conflicts}: We introduce conflicts between the sources of information, in this case between multiple image sources, such that the question becomes unanswerable. For multihop questions (i.e. questions with two image sources), we augment that dataset by combining the perturbed variant of one of the two images with the original version of the other i.e. (image 1, perturbed image 2) and vice versa, therein introducing a conflict that makes the question unanswerable and therefore making retrieval token \retlabel the only correct response (see \autoref{fig:perturbed_examples}). 

Note, we adopt the concept of the retrieval token \retlabel from Labruna et. al.\citep{labruna2024retrieve}.

\subsection{The Knowledge Conflicts Dataset}
Existing VQA datasets do not include examples with conflicting sources of information. To address this gap, we take three popular VQA datasets, WebQA \citep{chang_webqa_2021}, VQAv2 \citep{goyal2017making}, and OK-VQA \citep{marino_ok-vqa_2019} (see \autoref{tab:org_dataset}), and augment them with knowledge conflicts by perturbing the image sources and updating the expected answers using the \segsub framework.

Unlike WebQA, where questions fall into specific categories (color, shape, yesno, number), VQAv2 on OK-VQA are open-domain tasks. As a result, we can use feature modifications to generate parametric conflicts only for the WebQA dataset (as in \autoref{fig:seg_sub_pipeline}, \autoref{fig:perturbed_examples}). In addition, since source conflicts require two images, we only generate them for the multihop portion of the WebQA dataset. We cannot generate source conflicts for VQAv2 and OK-VQA as they are single-image VQA tasks. Lastly, we generate samples with counterfactual conflicts for all three datasets. 

\autoref{tab:dataset_description} gives a breakdown of the samples generated for each dataset along with the method used. Note that for every perturbed sample, we also keep the corresponding original, unperturbed samples from each of the constituent datasets. This ensures that models finetuned on the generated knowledge conflicts dataset learn to discriminate between conflicting and counterfactual sources, while also learning to answer questions on the original image samples. 38\% of the resulting generations have the answer \retlabel.

% perturbations that modify object attributes such as color and shape would not result in a new label. As a result, we focus on generating counterfactual samples on VQAv2 and OK-VQA. 

\begin{table*}[t]
    \caption{A breakdown of the generated knowledge conflicts dataset by the constituent datasets, the total number of generations, and the number of generations that pass the quality checks along with label quality rating from manual evaluation.}
    \label{tab:dataset_description}
    \centering
    \resizebox{\textwidth}{!}{\begin{tabular}{llllrrr}
    \toprule
        
        \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Conflict Type}} & \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\parbox{1.5 cm}{\textbf{New \\ Answer}}} & \multicolumn{2}{r}{\textbf{\# Generations: train (validation)}} & \multirow{2}{*}{\parbox{2.3 cm}{\textbf{Label Quality\\ Rating}}}  \\\cline{5-6}
        & & & & \textbf{Pre Quality} & \textbf{Post Quality} &  \\
        \midrule
        WebQA(Color, Shape) & Parametric & object infill & \updatedlabel & 141003 & 12537~(1459) & 76\% \\ 
        WebQA(Color, Shape) & Source & object infill & \retlabel & 141003 & 8038~(1050) & 82\% \\ 
        WebQA(Yes/No) & Counterfactual & object removal & \retlabel & 11077 & 1815~~~(257) & 87\% \\  
        VQAv2 & Counterfactual & object removal & \retlabel & 49742 & 7765~(1830) & 92\% \\
        OK-VQA & Counterfactual & object removal & \retlabel & 4648 & 0~~~(474) & 93\% \\
        \midrule
        Total Generations & -- & -- & -- & 201822 & 30155 (5070) & --\\
        %\midrule
        %WebQA & None & None & -- & -- & 8634 (1081) & -- \\
        %VQAv2 & None & None & -- & -- & 7765 (1830) & -- \\
        %OKVQA & None & None & -- & -- & 0 (474) & -- \\
        %\midrule
        %Total Dataset & -- & -- & -- & -- & 46554 (8455) & -- \\
        \bottomrule
    \end{tabular}}
\end{table*}

\paragraph{Quality Checks}
The generative methods used for perturbing images are imperfect. We therefore apply quality checks to filter out the noisy generations before finetuning VQA models. We present each generated sample to a quantized Qwen2-VL-7b-Instruct VLM and ask whether the modified feature is the same (or for object removal, whether the object exists), in both the original and perturbed images.
Framing the question in this way eliminates bias towards affirmative responses. Manual evaluation of the quality-checked images finds that they are indeed high quality (\autoref{tab:dataset_description}). Quality checks prompts are listed in the supplementary (\autoref{frame:quality_check_prompt_webqa_color_shape}).

\subsection{Finetuning on knowledge conflicts data}
To evaluate the \segsub frameworks efficacy in developing VLM robustness, we finetune three VLMs on the generated knowledge conflicts data---Llava-1.5-7b \citep{liu2024improved}, Phi3-vision-128k-instruct \citep{abdin2024phi}, and Qwen2-VL-7B-Instruct \citep{wang2024qwen2}. All models are finetuned on the training set (\autoref{tab:dataset_description}) for 1 epoch on 2x NVIDIA RTX A6000 GPUs using SWIFT \cite{zhao2024swiftascalablelightweightinfrastructure}, with convergence shown in the appendix (\autoref{fig:training_loss}). Subject to resource limitations, we apply LoRA \citep{hu2021lora} to reduce GPU memory requirements and use Distributed Data Parallel methods DeepSpeed \citep{rasley2020deepspeed} and ZeRO \citep{rajbhandari2020zero} to train across multiple GPUs. Refer to \autoref{tab:hyperparameters} for hyperparameters.

\subsection{Evaluation}
We compare performance of the finetuned versions of the VLMs against their base versions on the \segsub validation set (\autoref{tab:dataset_description}). We also evaluate on---Llava-1.5-13b \citep{liu2024improved} and GPT-4o-mini \citep{achiam2023gpt}.

\paragraph{Evaluation on \segsub Generations}
We measure the VLM's reasoning ability over conflicting sources of information with the following accuracy scores (see \autoref{sec:accuracy} for details)---

\textit{Parametric response rate}: \% of model responses that incorrectly predict the original label when a color or shape attribute has been changed. Therefore, highlighting the effect of parametric conflicts on model performance by showcasing the model's over reliance on the encoded parametric knowledge instead of adapting to the modified image source.

\textit{Accuracy for counterfactual conflicts}: \% of model responses that correctly generate \retlabel~~or any response which acknowledges the models failure to answer on the set of counterfactual samples\footnote{We consider VLM responses that make a reference to not having enough information or context, being unable to make a determination, or the image source being obscured in some way as `acknowledgement' responses, equivalent to \retlabel (i.e. \autoref{tab:ret_acknowledged} in the appendix).}.

\textit{Accuracy for source conflicts}: \% of model responses that correctly generate \retlabel or any response which acknowledges the models failure to answer on the set of source conflicts.  See \autoref{tab:ret_acknowledged} in the supplementary for the `acknowledgment' phrases we parse from model responses.

% a blank, all-white image. 
% which have been fine-tuned on the datasets we're looking at?

\paragraph{Evaluation on Original Samples}
We evaluate model accuracy on original samples to check for performance regressions on the original VQAv2, OK-VQA, and WebQA validation sets that may occur as a result of finetuning. Accuracy scores on the original samples are simply the \% of model responses that generate the original labels in each dataset when presented with the original, unperturbed images. These results are reported alongside accuracy scores for the knowledge conflict tasks. 
%and That is, for each generated knowledge conflict, we include the original unperturbed sample.
% \todo{WHAT? Note, for each combination of perturbation and dataset, we compare set of original samples that correspond to the set of perturbed samples.}


\paragraph{Robustness on Counterfactuals}
Counterfactual conflicts are generated using LaMa. To ensure that our finetuned models do not learn to predict \retlabel~ based on whether or not the image was modified by LaMa, we include an additional robustness check. For each perturbed counterfactual image and question pair in the WebQA dataset, we create randomized counterfactual samples by pairing a question with an unaltered image sampled at random from the WebQA dataset. We call these randomized, negatively sampled counterfactuals.

\paragraph{Image-Question Contextualization}
Finally, we analyze the effect of contextualization between images and questions. The motivation behind investigating contextualization is to understand why VLMs hallucinate responses for some counterfactual sources, but not for others. As such, we prompt GPT-4o-mini to assign a `contextualization score' to each counterfactual image and question pair in the \segsub validation set (see \autoref{frame:image_question_contextualization} in the supplementary).  Intuitively, this concept should relate to the amount of contextual cues that an image has for a given question, i.e. the more the number of contextual cues an image has, the more hints the model has to answer the given question. For highly contextualized image question pairs, visual reasoning is reinforced by various elements within the image that prime the model to hallucinate. In poorly contextualized pairs, image sources lack the context cues that exhibit this priming effect, and therefore do not provoke hallucinations.




