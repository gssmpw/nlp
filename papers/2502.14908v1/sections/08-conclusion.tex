% In this work, we introduced \segsub, a Segmentation Substitution framework designed to improve the robustness of visual reasoning in VLMs. Through the application of image segmentation and inpainting techniques, we augment VQA datasets with counterfactual samples and knowledge conflicts. These samples test LLMs' abilities to recognize and respond to various types of image-based reasoning challenges. Our experiments demonstrate that while VLMs show resilience to certain perturbations such as feature modifications that lie within their training distribution, they struggle with counterfactual cases and inconsistencies across multiple image sources, especially in multi-hop scenarios.

% Our findings highlight the need for robust VQA models that can navigate diverse visual contexts. We hope our contribution to advancing visual reasoning and model resilience against counterfactual noise will encourage future research in this area. 

% The \segsub framework serves as a tool for strengthening VQA tasks, advancing the study of multimodal reasoning in real-world applications.

We introduce \segsub, a framework designed to improve the robustness of visual reasoning in VLMs. Through the application of image segmentation and inpainting techniques, we augment VQA datasets with parametric, source and counterfactual conflicts. These samples test LLMs' abilities to recognize and respond to various types of image-based reasoning challenges. While our experiments demonstrate VLM resilience to perturbations that lie within their training distribution (i.e. feature modifications that induce parametric conflicts), they struggle with counterfactual cases and conflicts across multiple image sources, especially in multi-hop scenarios. Our findings highlight the need for VQA models that are robust to knowledge conflicts and we hope that our contribution will inspire future research in advancing visual reasoning. 

% Shorter
% We introduced \segsub, a framework that enhances visual reasoning in VLMs by augmenting VQA datasets with counterfactual samples and knowledge conflicts through segmentation and inpainting. Our experiments show that while VLMs are resilient to perturbations within their training distribution, they struggle with counterfactuals and inconsistencies in multi-image contexts, particularly in multi-hop tasks. These results emphasize the need for more robust VQA models capable of handling diverse visual challenges, encouraging further research in counterfactual noise and visual reasoning


% The \segsub framework serves as a tool 