\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
  \centering
   \includegraphics[width=0.9\linewidth]{fig/relate.pdf}
   \caption{This figure presents three different tasks related to human skill learning. Given an initial image and a text prompt, procedure planning generates a series of steps in textual form. Video generation models can produce a single action video based on detailed text prompts. In contrast, KS-Gen generates multiple key-step videos that complete the skill, using only a simple skill description and image as input.}
   \label{fig:intro}
\end{figure}

Human knowledge can be divided into declarative and procedural knowledge~\cite{Knowledge}. Generative models can now produce diverse and high-quality images~\cite{dalle2,sd} and videos~\cite{SVD,aa,AnimateDiff,latte,dynamicrafter,cogvideox} based on the text description by capturing the declarative knowledge of the real world (e.g., fine-grained visual concepts and temporal dynamics of physical law). 
In addition to capture this descriptive knowledge, we argue that another important and challenging task for generative model is to generate procedural knowledge (i.e., human skill). Human skills~\cite{howto100m,COIN,Survery21} refer to the ability of planning a procedure composed of several key-step actions to accomplish a complex goal. Human skill generator aims to learn the underline distribution of complex human skills, such as how to sow seeds and how to replace batteries. With such a skill generator, humans can learn to perform skills following generated videos, and robots can acquire skill knowledge from synthetic experiences~\cite{gen2act}. 

Given the status of current video generation methods~\cite{SVD,aa,AnimateDiff,latte,dynamicrafter,cogvideox}, human skill generation is extremely challenging because a skill involves multiple steps in the correct order, rather than a simple and atomic action. Additionally, We find that a complete skill video contains many redundant segments, such as repeated actions and numerous scene transitions. We believe these redundant segments have limited generative value and significantly increase the difficulty of generation. Therefore, we propose the Key-step Skill Generation (KS-Gen), a goal-driven and multi-step video generation task of producing a sequence of clips corresponding to the key actions in procedure planning. 


As shown in Figure~\ref{fig:intro}, we present some existing tasks related to human skill learning. Procedure planning~\cite{DDN} is among the first to introduce procedural knowledge into video understanding, but regardless of whether the input is an image or text, the output is merely a description of the steps, which is less intuitive than a video. In video generation, some studies~\cite{SVD,aa,dynamicrafter,cogvideox} have attempted to generate human action videos, but most are limited to single-step video generation with highly detailed textual descriptions. Other work~\cite{pandora} has employed auto-regressive methods to generate multi-step, long-duration videos, but these videos typically involve continuous steps without significant changes in object states or scene transitions. Our task, by contrast, aims to generate multiple key-step video segments of a skill process, given an image representing the current state and a skill description. This task is more challenging than previous tasks, as it not only requires step planning but also demands the generation of multiple consistent video segments that capture significant state and scene changes. In real-world applications like instructional cooking videos or product assembly tutorials, videos are rarely filmed in a single continuous take without transitions. Instead, they typically consist of multiple segments to better emphasize key steps. Our task aligns with this practical approach, making it highly relevant to these applications.

One challenge in building the key-step generator is the lack of high-quality skill datasets and suitable evaluation metrics. To generate human activities in real-world environments, we build on existing instructional video datasets, such as COIN~\cite{COIN}, CrossTask~\cite{crosstask} and HT-Step~\cite{htstep}, which cover hundreds of real-world skills, along with Kinetics-400 (K400)~\cite{k400} to enhance action quality. However, the dataset quality varies and presents numerous issues. To address this, we develop a data curation pipeline to improve clip quality and propose diverse evaluation metrics to assess KS-Gen performance. Meanwhile, we propose a new framework for KS-Gen, which consists of the following three main components: (i) \textbf{MLLMs planning}: Based on the provided image and skill description, key-step descriptions are generated utilizing multimodal large language models (MLLMs). To ensure better control over the output, a retrieval-based approach is implemented to optimize the accuracy and sequence of the steps. (ii) \textbf{Key-step image generation}: Since only the initial image is available, subsequent clips lack a first-frame image to serve as a reference. Furthermore, due to the discontinuities and state transitions between the key-step clips, autoregressive generation methods are not well-suited for this task. To address the challenge of generating a consistent sequence of images for each key step, we introduce a novel Key-step Image Generator (KIG). Leveraging the initial image and the step descriptions, KIG generates the first frame for each step, ensuring visual consistency and coherence throughout the key-step images. (iii) \textbf{Video generation}: We generate each key-step clip using our fine-tuned video generation models~\cite{aa,SVD,dynamicrafter,cogvideox}, based on the first frame of each key step and its corresponding text description as prompts.

Our contributions are summarized in three aspects: (i) We take the first step towards human skill generation in the wild at key-step levels (KS-Gen), and build a new benchmark for KS-Gen, including a well-curated dataset and various metrics to assess the performance of skill generators. (ii) We propose a novel framework for KS-Gen that includes three main components. We comprehensively investigate this framework by building several baseline methods and conducting detailed analysis, hoping to provide more insights on human skill generation. (iii) We introduce the Key-step Image Generator (KIG) to address the challenge of generating multiple, non-continuous key-step clips. Experimental results demonstrate that incorporating KIG improves both the quality and consistency of generated images.

