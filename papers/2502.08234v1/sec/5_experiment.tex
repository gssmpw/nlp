\section{Experiment}
\label{sec:exp}
Our experiment section first analyze the results of different generators under a standard setting, leveraging the planning capabilities of MLLMs combined with key-step image generator and video generation models to produce key-step clips. We demonstrate the effectiveness of our novel framework across different video models, in comparison to baseline. Subsequently, we conduct detailed ablation studies, including performing multiple ablations on the image and video generation models and planning with various MLLMs. Finally, we display various visualization results of the skill generator under both close-set and open-set settings.

\subsection{Evaluation on key-step skill generators}

\begin{table}[t]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lcccccc}
\hline
\textbf{Model} & \textbf{Action$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{DINO$\uparrow$} & \textbf{Motion$\downarrow$} & \textbf{FVD$\downarrow$} & \textbf{FID$\downarrow$} \\ \hline
DC~\cite{dynamicrafter}+Gen~\cite{genhowto} & 40.8 & 74.9 & 56.3 & 2.17 & 144 & 22.7 \\
DC~\cite{dynamicrafter}+SDXL~\cite{sdxl} & 40.2	& 77.5 & 58.6 & 2.17 & 127 & 21.4 \\
DC~\cite{dynamicrafter}+Ours & 40.8 & 78.1 & 58.9 & \textbf{2.12} & \textbf{119} & \textbf{20.5} \\ \hline
Cog~\cite{cogvideox}+Gen~\cite{genhowto} & 42.9 & 75.3 & 58.7 & 2.87 & 188 & 26.5 \\
Cog~\cite{cogvideox}+SDXL~\cite{sdxl} & 42.5 & 77.6 & 60.4 & 3.06 & 138 & 24.4 \\
Cog~\cite{cogvideox}+Ours & \textbf{43.2} & \textbf{78.4} & \textbf{61.1} & 3.11 & 127 & 22.2 \\ \hline
\end{tabular}}
\caption{Evaluation on baseline methods. We evaluate different video models under different settings based on step descriptions generated by a multimodal LLM. ``Gen" refers to using the GenHowTo model, ``SDXL" refers to using SDXL model with IP-Adapter plus, and ``Ours" denotes using the KIG model to generate key-step images.}
\label{tab:overview}
\vspace{-15pt}
\end{table}

% We constructed a baseline method using GenHowTo~\cite{genhowto} as the image generation model. Specifically, GenHowTo is a generative model designed to produce object state changes in instructional video, conditioned on both images and text. In this work, we directly use GenHowTo to generate key-step images $\{I_1,I_2,...I{n-1}\}$ without fine-tuning. However, Genhowto is not entirely zero-shot, as it is trained on instructional videos~\cite{crosstask,changeit}. We evaluated the generation results using DynamiCrafter (DC)~\cite{dynamicrafter} and CogVideoX-5b-I2V (Cog)~\cite{cogvideox} as video models. While Dynamicrafter was fine-tuned on the training data, CogVideoX was not fine-tuned due to resource and time constraints. As shown in Table~\ref{tab:overview}, our method outperforms GenHowTo across nearly all metrics, with significant improvements observed in FVD and FID. Specifically, when using CogVideoX as the video model, FVD is reduced by approximately 50, and there are notable gains in both CLIP and DINO metrics. These results indicate that our approach leads to substantial improvements in video generation, enhancing both foreground and background quality. Furthermore, these results are obtained under a standard setting, where only the initial image and skill description are provided during the inference process. Ablation experiments on both the video and image models will be presented in the following sections.

We constructed a baseline method using GenHowTo~\cite{genhowto} and SDXL~\cite{sdxl} model with IP-Adapter~\cite{ip} plus as the image generation model. Specifically, GenHowTo is a generative model designed to produce object state changes in instructional video, conditioned on both images and text. In this work, we directly use GenHowTo to generate key-step images $\{I_1,I_2,...I_{n-1}\}$ without fine-tuning. However, GenHowTo is not entirely zero-shot, as it is trained on instructional videos~\cite{crosstask,changeit}. We evaluated the generation results using DynamiCrafter (DC)~\cite{dynamicrafter} and CogVideoX (Cog)~\cite{cogvideox} as video models. While DynamiCrafter was fine-tuned on the training data, CogVideoX was not fine-tuned due to resource and time constraints. As shown in Table~\ref{tab:overview}, our method outperforms GenHowTo and SDXL across nearly all metrics, with significant improvements observed in FVD and FID. Specifically, when using CogVideoX as the video model, FVD is reduced by approximately 50 than GenHowTo, and there are notable improvements in both CLIP and DINO metrics. These results indicate that our approach enhances both foreground and background quality. Furthermore, these results are obtained under a standard setting, where only the initial image and skill description are provided during the inference process.

\subsection{Ablations}

\begin{table}[t]
\centering
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{lccc}
\hline
\textbf{Model} & \textbf{CLIP$\uparrow$}               & \textbf{DINO$\uparrow$}               & \textbf{FID$\downarrow$}                \\ \hline
GenHowTo~\cite{genhowto}       & 66.8                        & 46.4                        & 67.7 \\ \hline
Kolors~\cite{kolors}         & 68.1                        & 46.5                        & 67.6 \\
Kolors ft.~\cite{kolors}     & 69.0   & 46.5  &  67.2                            \\
Kolors~\cite{kolors}+ST.  & 69.9                        & 47.5                        & 67.1 \\ \hline
SDXL~\cite{sdxl}           & 72.5 & 49.5 & 64.8 \\
SDXL ft.\cite{sdxl}       &72.8 & 49.4 & 64.3 \\
\rowcolor[rgb]{0.9,0.9,0.9}SDXL~\cite{sdxl}+ST.    & \textbf{74.1}                        & \textbf{50.4}                        & \textbf{61.1}                        \\ \hline
\end{tabular}}
\caption{Ablations on image generation models. Since Kolors and SDXL were originally designed for text-to-image generation, we use the IP-Adapter to inject the initial image as an image condition into both models. ``ft.” denotes the image model is fine-tuned, while ``+ST.” indicates that image features are passed through our trained Skill Transformer before being injected into the image model. The default choice is colored in \colorbox[rgb]{0.9,0.9,0.9}{gray}.}
\label{tab:ablation image}
\end{table}

\textbf{Image generation model.} To validate the effectiveness of our Key-step Image Generator (KIG), we evaluated its impact on Kolors~\cite{kolors} and SDXL~\cite{sdxl}, using CLIP, DINO, and FID metrics to assess image generation quality. We also choose the GenHowTo model, which is pretrained on instructional videos, as a baseline method. The Kolors and SDXL we used are modified with the IP-Adapter~\cite{ip} plus, as these models were initially designed purely for text-to-image generation. As shown in Table~\ref{tab:ablation image}, SDXL outperforms Kolors and GenHowTo in baseline generation quality. Integrating our Skill Transformer with both Kolors and SDXL yield notable improvements. Additionally, we find that combining the Skill Transformer with non-fine-tuned image models yields superior results compared to using fine-tuned image models alone. This demonstrates that our approach is both highly effective and efficient. To show the robustness of our model, we employ step descriptions generated by an our MLLM, which may contain inaccuracies. Our novel approach effectively enhances model robustness, even with imperfect input descriptions.

\begin{table}[t]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lccccccc}
\hline
\textbf{Model} & \textbf{Act.$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{DINO$\uparrow$} & \textbf{Mot.$\downarrow$} & \textbf{FVD$\downarrow$}  & \textbf{FID$\downarrow$} & \textbf{Time} \\ \hline
DC$_{1024}$ zs.~\cite{dynamicrafter}      & 49.6            & 90.0          & 82.9          & 4.26            & 108.5         & 8.14         & 1.56          \\
Cog zs.~\cite{cogvideox}         & \textbf{54.9}   & 90.1          & \textbf{84.6} & 2.36            & \textbf{63.7} & 8.73         & 3.29          \\ \hline
AA ft. ~\cite{aa}           & 45.7            & 85.4          & 80.3          & 2.67            & 152.6         & 15.8         & 0.14          \\
SVD ft. ~\cite{SVD}          & 45.3            & 86.1          & 78.1          & 2.07            & 162.6         & 15.2         & \textbf{0.12} \\
DC$_{512}$ ft.~\cite{dynamicrafter}         &50.5                &87.7               &79.0               &1.75                 &  75.2             & 8.85             & 0.41          \\
DC$_{1024}$ ft. ~\cite{dynamicrafter}        & 52.9            & \textbf{90.6} & 83.5          & \textbf{1.55}   & 66.3          & \textbf{7.82} & 1.55          \\ \hline
\end{tabular}}
\caption{Ablations on video generation models. During inference, the model is provided with ground truth initial frames and LLM-fused step descriptions. ``zs." stands for zero-shot models, ``ft." indicates fine-tuned models. DC$_{512}$ refers to the generated videos have a resolution of $512\times$320. DC$_{1024}$ refers to the generated videos have a resolution of $1024\times$576.}
\label{tab:ablation video}
\vspace{-10pt}
\end{table}

\textbf{Video generation model.} We evaluated the performance of four different video generation models. To isolate the effect of the video models, we provided each model with the ground-truth first frame of each clip and the step description during generation. As shown in Figure~\ref{tab:ablation video}, we fine-tuned models AA~\cite{aa}, SVD~\cite{SVD}, and DC~\cite{dynamicrafter}, with DC tested at two different resolutions with two model variants. Due to resource limitations and the slower training and inference speeds of CogVideoX~\cite{cogvideox}, we evaluated it in a zero-shot setting only. Results indicate that DC achieves comparable performance to CogVideoX with half the generation time, showing particularly strong improvements in motion metrics, and DC has a significantly smaller model size than CogVideoX. Additionally, we tested DC in a zero-shot setting and observed that fine-tuning the video models led to a notable enhancement in generation quality.

\begin{table}[t]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lccccccc}
\hline
\textbf{Filter} & \textbf{Action$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{DINO$\uparrow$} & \textbf{Motion$\downarrow$} & \textbf{FVD$\downarrow$}  & \textbf{FID$\downarrow$}   \\ \hline
None            & 50.4            & 86.2          & 75.8          & 2.84            & 98.3          & 9.60               \\
+Scene         & 50.8            & 87.6          & 78.3          & 2.34            & 90.0          & 9.21             \\
+Motion        & \textbf{52.2}   & 87.8          & 78.8          & \textbf{1.92}   & \textbf{82.0} & 8.93        \\
+Semtantic     & 51.9            & 88.6          & 80.4          & 1.98            & 86.1          & \textbf{8.86}  \\
\rowcolor[rgb]{0.9,0.9,0.9}+Text          & 52.0            & \textbf{88.6} & \textbf{80.5} & 1.95            & 87.1          & 8.94           \\ \hline
\end{tabular}}
\caption{Ablations on data curation pipeline. We evaluate the performance of the Dynamictafter model under different data curation pipeline. During inference, the model is provided with ground truth initial frames and LLM-fused step descriptions. The generated videos have a resolution of 256$\times$384, consisted of 16 frames, and are generated at 8 fps. In the table, ``+" indicates the addition of the corresponding setting to the previous row. The default choice is colored in \colorbox[rgb]{0.9,0.9,0.9}{gray}.}
\label{tab:ablation data curation}
\vspace{-10pt}
\end{table}

\textbf{Data curation.} The quality of training data is crucial for video generation models, and one of our main contributions is proposing a data curation pipeline that enhances the quality of video clips. Our pipeline mainly consists of four aspects, and we compare the most basic training results, which do not include data processing, with results progressively incorporating different data curation methods. As shown in the Table~\ref{tab:ablation data curation}, removing scene transitions in clips makes the generated videos smoother, leading to improvements in all metrics. The newly proposed method of histogram template matching for optical flow magnitude effectively enhances the motion score, reducing it from 2.34 to 1.92, which convincingly demonstrates the efficacy of this approach. Additionally, handling motion also improves other metrics. Considering that clips and text aligned with semantics are more beneficial for training generative models, we further incorporated semantic processing, which achieves the best results on the FID metric. Training with video segments that contain excessive text can lead to the sudden appearance of text in generated videos, so we removed samples that contained a lot of text. As shown in the last line, by incorporating text processing, we achieved the best results in both CLIP and DINO metrics. Overall, our data curation efforts lead to significant improvements across all metrics.

\begin{table}[t]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lcccccc}
\hline
\textbf{Prompt} & \textbf{Action$\uparrow$} & \textbf{CLIP$\uparrow$} & \textbf{DINO$\uparrow$} & \textbf{Motion$\downarrow$} & \textbf{FVD$\downarrow$}  & \textbf{FID$\downarrow$}  \\ \hline
Verb-noun       & 51.9            & 88.3          & 79.9          & 1.95            & 85.3          & 9.04          \\
Video           & 51.5            & 88.4          & 80.2          & \textbf{1.88}   & 87.3          & 9.00          \\
Image           & 51.1            & 87.9          & 79.3          & 1.94            & \textbf{83.9} & 9.07          \\
\rowcolor[rgb]{0.9,0.9,0.9} Fusion          & \textbf{52.0}   & \textbf{88.6} & \textbf{80.5} & 1.95            & 87.1          & \textbf{8.94} \\ \hline
\end{tabular}}
\caption{Ablations on step description. We also evaluate the performance of DC model with different step description. The generated videos have a resolution of 256$\times$384. ``Fusion” indicates using LLM to merge multiple descriptions, yielding the best training results. The default choice is colored in \colorbox[rgb]{0.9,0.9,0.9}{gray}. }
\label{tab:ablation prompt}
\end{table}

\textbf{Step Description.} We evaluate video generation models trained with different text prompts. During inference, we uniformly use the ground truth image as the first frame for each clip and the ground truth LLM-fusion description as the prompt. As shown in the Table~\ref{tab:ablation prompt}, model training with LLM-fusion descriptions achieves the best results in four metrics. However, model trained with video captions slightly lead in motion score, and model train with image captions achieve the best FVD. Considering all metircs, we select the fusion description.

\begin{table}[]
\centering
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{lcc}
\hline
\textbf{Method}            & \textbf{SR$\uparrow$} & \textbf{Acc$\uparrow$} \\ \hline
GPT-4o          & 11.3        & 28.5         \\
GPT-4o-mini     & 1.80         & 15.4        \\
Claude 3.5 Sonnet& 15.6       & 33.2        \\
ChatGPT-4o-latest  & 17.1       & 37.7        \\
\rowcolor[rgb]{0.9,0.9,0.9} ChatGPT-4o-latest +RAG                       & \textbf{50.5}       & \textbf{65.0}        \\ \hline
\end{tabular}}
\caption{Ablations on MLLMs planning. We experiment with different MLLMs, where “+RAG” indicates the addition of retrieval-augmented generation. The default choice is colored in \colorbox[rgb]{0.9,0.9,0.9}{gray}.}
\label{tab:ablation llm}
\vspace{-15pt}
\end{table}

\textbf{Multimodal LLM planning.} To verify the accuracy of multimodal LLMs planning sequence, we follow the metrics of procedure planning. As shown in Section~\ref{sec: planning}, we evaluate the step sequences under a close-set setting, along with Success rate (SR) and Accuracy (Acc). SR is the strictest criterion, requiring the sequence to match the ground truth exactly, whereas Acc only requires individual steps to be correct. As shown in Table~\ref{tab:ablation llm}, we test the metrics for GPT-4o~\cite{gpt-4o}, GPT-4o-mini~\cite{gpt-4o-mini}, Claude 3.5 Sonnet~\cite{claude} and ChatGPT-4o-latest~\cite{gpt-4o} without retrieval argument. The lightweight GPT-4o-mini model provides fast inference speed but performs less accurately than other models. ChatGPT-4o-latest yields the best planning results, achieving the highest success and accuracy rates, with a success rate of 17\%. However, a success rate below 20\% highlights the limitations of current multimodal LLMs in directly handling procedure planning tasks, even when selectable steps are provided. With the addition of retrieval argument, there is a significant improvement in both SR and Acc, indicating that the skill database effectively guides the LLM in sequencing planning. We apply retrieval-augmented generation to the ChatGPT-4o-latest model, which serves as our final result.


\subsection{Visualizations}

\begin{figure}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{fig/demos.pdf}
   \caption{The visualization of the skill generator.}
   \label{fig:vis}
   \vspace{-20pt}
\end{figure}


We showcase the visualization results of skill generation. Figure~\ref{fig:vis} displays the outcomes of four different skills generated by CogVideoX~\cite{cogvideox} with our KIG assistance. Additionally, in Figure~\ref{fig:ablaimage}, we present visualization results using different image generation models. The images generated by GenHowTo~\cite{genhowto} exhibit poor consistency and contain errors, such as the soda cup, which should have disappeared in Step 2, remaining in the images. The results using the SDXL~\cite{sdxl} model alone are also suboptimal, with the hand appearing to float in the air in Steps 3 and 4, which is inconsistent with realistic actions. In contrast, our model demonstrates superior consistency and image quality compared to both GenHowTo and SDXL. We present the qualitative analysis results of our data curation, as shown in the Figure~\ref{fig:abladata}. For the first line generated with the raw dataset, the faces of the characters in the video have undergone significant changes, and the action is wrong. By incorporating scene detection, inconsistencies within the clip can be effectively avoided. After we implemented motion control, it is clear that both the consistency of the video and the extent of motion have been greatly improved. Finally, we display the generation results of a skill that have not appeared in the training set as shown in Figure~\ref{fig:zeroshot}. The figures show the simplified step descriptions. More visualizations and detailed descriptions are provided in Appendix~\ref{appendix vis}.
\begin{figure}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{fig/image_abla.pdf}
   \caption{The visulization with different image generation models.}
   \label{fig:ablaimage}
   \vspace{-5pt}
\end{figure}  

\begin{figure}[t]
  \centering
   \includegraphics[width=0.97\linewidth]{fig/abla_filter.pdf}
   \caption{The visualization with different data curation.}
   \label{fig:abladata}
   \vspace{-5pt}
\end{figure}


\begin{figure}[t]
  \centering
   \includegraphics[width=0.97\linewidth]{fig/openset.pdf}
   \caption{The visualization of unseen skill during training.}
   \label{fig:zeroshot}
   \vspace{-10pt}
\end{figure}
