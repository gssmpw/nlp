%\clearpage
%\setcounter{page}{1}
% \maketitlesupplementary

\newtheorem{sketch_definition}{Definition}[section]
\section*{Appendix}
\appendix


Our appendix includes the following sections: Section~\ref{appendix stat} presents detailed statistics of our dataset. Section~\ref{appendix data} provides an in-depth introduction to our data curation pipeline. Section~\ref{appendix metrics} thoroughly describes the evaluation metrics we employ. Section~\ref{user study} provides the user study we conducted. Section~\ref{appendix llm} explains how we utilize a MLLM to generate step sequences. Section~\ref{appendix kig} details the implementation of the Skill Transformer and the hyperparameters used. Section~\ref{appendix kcg} provides the hyperparameters of the video models we used. Finally, Section~\ref{appendix vis} displays additional visualization results.

\section{Dataset statistics}
\label{appendix stat}
As shown in Table~\ref{tab:statistics}, we display the number of skills, videos and clips include in our dataset. Since K400~\cite{k400} is not an instructional video dataset, it does not contain skill statistics. HT-Step~\cite{htstep} is used exclusively for training the video generation model.

\begin{table}[h]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lccccc}
\hline
Split & Skills  & Videos & Clips  & Clip Duration & Subclips\\ \hline
Train (COIN+CrossTask) & 182    & 12326       &  58510  & 214 h  & 23997 \\
Train (K400) & -    & -      &  56915 & 154 h  & 22499 \\
Train (HT-Step) & 433  & 16233 &  93997 & 399 h  & 63906 \\
Test  & 133    & 557       &  1672     & 0.89 h & 1672 \\
Total & 615    & 29116   &   133330   &768 h & 112074 \\ \hline
\end{tabular}}
\caption{The statistics of our dataset including COIN, CrossTask, HT-Step and K400.}
\label{tab:statistics}
\end{table}

\section{Dataset curation pipeline}

\label{appendix data}
\textbf{Scene Transition Detection.} For each clip, we compute the 256-dimensional color histogram features for each frame. To detect gradual transitions, we calculate the L1 distance between the histogram features of frames at three intervals: zero frames (adjacent frames), two frames, and four frames. A frame is detected as a scene transition if the L1 distance at any of these intervals exceeds the respective threshold.

\begin{figure}[h]
  \centering
   \includegraphics[width=0.9\linewidth]{fig/motion_template.pdf}
   \caption{The template of optical flow histogram}
   \label{fig:motion}
\end{figure}

\textbf{Optical Flow Amplitude.} We utilize the RAFT-Large~\cite{raft} model to compute the optical flow between each frame and its adjacent frame for every clip. The optical flow magnitude histograms are then generated, with the magnitude range on a log scale from \(2^{-7}\) to \(2^5\), divided into 256 bins. For each optical flow histogram, we compute the KL divergence from a predefined histogram template. This template, shown in Figure~\ref{fig:motion}, represents a mixture of two Gaussian distributions: the lower mean Gaussian represents static parts of the video (e.g., background), while the higher mean Gaussian represents the motion parts. The ideal video should predominantly contain static parts with some moderate motion parts. After calculating the KL divergence between the template and each frame's histogram, we use a sliding window and a set threshold to identify the most appropriate subclips within each clip while filtering out less suitable subclips.

\textbf{Aligned clip and description.} We employ the EVA01-CLIP-g~\cite{evaclip} model to compute the clip score between each frame and the action descriptions provided in the dataset. Using a sliding window and a set threshold, we identify the most appropriate subclips within each clip, filtering out less suitable subclips.

\textbf{Reducing Text-Heavy Segments.} The CRAFT~\cite{craft} model is used to calculate the proportion of each frame covered by text. Frames with a high proportion of text are excluded based on a predefined threshold.

\textbf{Step Descriptions Enhancement.}
To enhance the quality of step descriptions, we employ the image captioning model BLIP-2-opt-6.4b~\cite{blip2} to generate an image caption every 2 seconds and the video captioning model VAST~\cite{VAST} to produce 10 video captions for each clip. We randomly select three image captions and three video captions, then combine these with the provided step descriptions using the Llama3-8B-instruct model. This fusion process generates finely detailed and accurate captions, further improving the quality of video generation.

\section{Metrics}
\label{appendix metrics}
To measure the quality and similarity of generated videos in comparison to real videos, we employ a set of semantic similarity and video quality metrics. All metrics are computed on 16-frame clips at 8 frames per second (fps) from the test set. For reproducibility, the associated code will be made available. Below are the detailed implementations of each metric:

\textbf{Action Score:} We utilize the VideoMAE-base~\cite{videomae} model fine-tuned on the Something-Something v2~\cite{ssv2} dataset to calculate the cosine similarity between the logits of generated and real video clips. Each video is resized such that its shorter side is 224 pixels, followed by a center crop to 224$\times$224 pixels. Cosine similarity is computed between the 174-dimensional logits vectors of the generated and real videos. For comparison convenience, we scale the similarity by a factor of 100, resulting in an action score ranging from 0 to 100, where higher values indicate better performance.

\textbf{CLIP:} To measure frame-by-frame semantic similarity, we employ the OpenCLIP-ViT-bigG~\cite{openclip} model. Videos are resized to have a shorter side of 224 pixels and then center-cropped to 224$\times$224 pixels. The similarity score between frames of the generated and real videos is scaled by 100 for easier comparison, yielding an image score that ranges from 0 to 100, with higher scores indicating better visual content correspondence.

\textbf{DINO:} Following VBench~\cite{vbench}, we calculate the DINO feature similarity with DINOv2-large~\cite{dinov2} model to evaluate the semantic consistency of the main objects between the generated and real videos. Similar to the CLIP metric, the videos are also cropped to 224 and the similarity score is scaled by a factor of 100.

\textbf{Motion Distance:} Motion distance is assessed using the RAFT-Large~\cite{raft} model to calculate optical flow. We then compute the histogram of the optical flow magnitude on a log scale ranging from \(2^{-7}\) to \(2^5\) with 256 bins. The histograms are normalized, and the Kullback-Leibler (KL) divergence between the histograms of the generated and real videos is computed. The motion distance score ranges from 0 to infinity, with lower values indicating closer similarity. The shorter side of the video is resized to 256 pixels.

\textbf{Fréchet Inception Distance (FID):} We employ the FID implementation from `torchmetrics` to compute the FID score between the frames of generated and real videos. Lower FID scores indicate higher quality and greater similarity to the visual properties of the original dataset.

\textbf{Fréchet Video Distance (FVD):} The FVD score is calculated using the torchscript provided by~\cite{StyleGAN}, comparing the distributions of generated and real video clips. Similar to FID, lower FVD scores signify better performance in terms of video quality and resemblance to real videos.

\section{User study}
\label{user study}
To evaluate the alignment between automatic metrics and human preferences, we conducted a comprehensive user study involving 38 participants. The study was divided into two sections, each focusing on a specific aspect of video generation: action accuracy (whether the video accurately and realistically depicts the described actions) and object consistency (whether the state changes of objects in the video are logical and free from irrelevant artifacts). Each section contained 10 examples, with participants required to rank 4 videos per example based on the given criteria. This structured approach generated approximately 4,500 pairwise comparisons, which were used to compute the average win ratio for each model across the two aspects. 

We then analyzed the correlation between human evaluations and several automatic metrics, including Action, FID, FVD, and Motion for action accuracy, as well as CLIP and DINO for object consistency. The results, illustrated in Figure~\ref{fig:user}, demonstrate a strong correlation between these automatic metrics and human evaluations, indicating that the metrics effectively capture human preferences in video generation tasks. This alignment suggests that the evaluated metrics can serve as reliable proxies for human evaluations in assessing video quality.

\begin{figure}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{fig/user_study.pdf}
   \caption{Correlation between automatic metrics and human evaluations for action accuracy and object consistency.}
   \label{fig:user}
\end{figure}


\section{Using MLLMs as a step planner}
\label{appendix llm}
As described in the main paper, we use different MLLMs to generate a sequence of steps, including detailed descriptions of each step. We present the versions of the MLLMs we used in Table~\ref{tab:llm version}.

\begin{table}[h]
\centering
\resizebox{1.\linewidth}{!}{
\begin{tabular}{lccc}
\hline
\textbf{Method}            & \textbf{SR$\uparrow$} & \textbf{Acc$\uparrow$} & \textbf{Version}\\ \hline
GPT-4o          & 11.3        & 28.5      &gpt-4o-2024-05-13   \\
GPT-4o-mini     & 1.80         & 15.4    &gpt-4o-mini-2024-07-18    \\
Claude 3.5 Sonnet& 15.6       & 33.2     &claude-3-5-sonnet-2024062   \\
ChatGPT-4o-latest  & 17.1       & 37.7 &gpt-4o-latest       \\
ChatGPT-4o-latest +RAG                    &\textbf{50.5}       & \textbf{65.0}  &-      \\ \hline
\end{tabular}}
\caption{Ablations on MLLMs planning. We present the versions of the MLLMs used in our study.}
\label{tab:llm version}
\end{table}

The detailed prompt is as follows. The [step pool] and [T] are only applicable in a close-set setting and will be removed in an open-set setting. The [examples] are the examples we obtained through retrieval argument.


\begin{quote}
\textbf{System:}

You are a helpful assistant, an expert in answering questions about action planning in instructional videos. Based on the given goal and picture, you need to infer the steps to accomplish this goal based on the current picture.

You have a pool of steps to choose from, and you should select the appropriate ones to describe the process. The step pool is [step\_pool].

Output the name of the steps and describe each step based on the given image. The description must be limited to one sentence.

Example: 1. pump up the fuel: A person is cutting a piece of wood, pumping up the fuel.

Example: 2. open the car key cover: A person holds a small black object, likely the car key, in their hand as they open the car key cover.

Example: 3. unscrew the screw and take out the needle: A person is carefully unscrewing the screw and taking out the needle from the sewing machine.

\textbf{User:}

Describe the steps to accomplish the goal based on the input image. The steps must be steps from the given step pool.

The goal is [goal];The number of steps is [T]; The initial image is [Image encoded with base64]; The example step sequences: [examples].



\textbf{Assistant:}

1. open the car key cover: A person is holding a car key and using a small tool to pry open the car key cover.

2. take out the car key battery: The car key cover is open, and the person is removing the old battery from its compartment.

3. put in the battery: The person places a new battery into the car key, ensuring it fits correctly in the compartment.

4. close the car key cover: The person closes the car key cover, securing it back into place.
\end{quote}

\section{Key-step image generation}
\label{appendix kig}

\textbf{Baselines.} We selected three open-source image generation models as baselines for comparison. The hyperparameters used for evaluating each model are provided in the Table~\ref{tab:imagesett}. For the Kolors~\cite{kolors} and SDXL~\cite{sdxl} models, we also explored fine-tuning, and the configurations used for fine-tuning are detailed accordingly.

\begin{table}[h]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lccc}
\hline
\textbf{Hyperparameter} & \textbf{GenHowTo}~\cite{genhowto} & \textbf{Kolors}~\cite{kolors} & \textbf{SDXL}~\cite{sdxl} \\ 
\hline
Weight & GenHowTo-ACTIONS & Kolors-IP-Adapter-Plus & SDXL-base-IP-Adapter-Plus \\
Resolution & $512 \times 512$ & $768 \times 1344$ & $768 \times 1344$ \\
Sampler & DDIM & DPM & EDM \\
Steps & 50 & 50 & 50 \\
Guidance scale & 9 & 4 & 4 \\
IP-Adapter scale & - & 1 & 1 \\
Learnable blocks & - & \multicolumn{2}{c}{All Attention blocks in UNet (1.3B)} \\
Learning rate & - & $1 \times 10^{-6}$ & $1 \times 10^{-6}$ \\ 
Training steps & - & 5K & 5K \\
\hline
\end{tabular}}
\caption{Hyperparameters for baseline image models.}
\label{tab:imagesett}
\end{table}

\textbf{Skill Transformer.} During the training of the Skill Transformer, we employ Teacher Forcing to encourage faster convergence and stable learning. To enhance the consistency between the predicted features and the initial image features, we utilize a two-part loss function, both based on Mean Squared Error (MSE). The first loss measures the discrepancy between the predicted image features $f'_n$ and the ground-truth image features $f_n$. The second loss ensures consistency by assessing the difference between the predicted features $f'_n$ and the initial image features $f_0$. To balance the influence of these two losses, we multiply the second loss by a consistency weight.

During inference, we similarly combine the predicted features $f'_n$ and the initial image features $f_0$ using a fusion weight $w$. Specifically, we compute the final feature $\hat{f_n}$ as:  

\[
\hat{f_n} = w \cdot f'_n + (1-w) \cdot f_0
\]

The hyperparameters used, including the consistency weight and fusion weight, are detailed in Table~\ref{tab:stsett}. The fused image features are then injected into the image generation model through the IP-Adapter~\cite{ip} to synthesize the corresponding step images. The image generation process adopts the same hyperparameter configuration as the Baseline models.

\begin{table}[h]
\centering
\resizebox{.9\linewidth}{!}{
\begin{tabular}{lcc}
\hline
\textbf{Hyperparameter} & \textbf{ST for SDXL~\cite{sdxl}} & \textbf{ST for Kolors~\cite{kolors}} \\
\hline
Text encoder & CLIP-ViT-H-14 & CLIP-ViT-L-14-336 \\
Transfomer depth & \multicolumn{2}{c}{4} \\
Channels & \multicolumn{2}{c}{2048} \\ 
Head channels & \multicolumn{2}{c}{128} \\ 
Training steps & \multicolumn{2}{c}{10K} \\ 
Learning rate & \multicolumn{2}{c}{$1 \times 10^{-4}$} \\ 
Overall batch size & \multicolumn{2}{c}{32} \\ 
Learnable param. & \multicolumn{2}{c}{270M} \\ 
Consistency weight & \multicolumn{2}{c}{0.5} \\ 
Fusion weight & \multicolumn{2}{c}{0.5} \\
\hline
\end{tabular}}
\caption{Hyperparameters for Skill Transformer.}
\label{tab:stsett}
\end{table}

\section{Key-step clips generation}
\label{appendix kcg}

We fine-tune the AnimateAnything (AA)~\cite{aa}, Stable Video Diffusion (SVD)~\cite{SVD}, and DynamiCrafter (DC)~\cite{dynamicrafter} models. For the DC model, we test two variants: DC$_{512}$ and DC$_{1024}$. Additionally, we evaluate the zero-shot performance of the CogVideoX-5b-I2V~\cite{cogvideox} model. The hyperparameters used for evaluating each model are provided in the Table~\ref{tab:kcgsett}.

\begin{table}[h]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lccccc}
\hline
\textbf{Hyperparameter} & \textbf{AA}~\cite{aa} & \textbf{SVD}~\cite{SVD} & \textbf{DC$_{512}$}~\cite{dynamicrafter} & \textbf{DC$_{1024}$}~\cite{dynamicrafter} & \textbf{CogVideoX}~\cite{cogvideox} \\
\hline
Resolution & $256 \times 384$ & $256 \times 384$ & $320 \times 512$ & $576 \times 1024$ & $480 \times 720$ \\
Inference Time & 0.14 & 0.12 & 0.41 & 1.55 & 3.29 \\
Sampler & DPM & EDM & \multicolumn{2}{c}{DDIM} & DPM \\
Steps & 25 & 25 & \multicolumn{2}{c}{50} & 25 \\
Guidance scale & 9 & 1.0-3.0 & \multicolumn{2}{c}{2} & 1.5 \\
Motion/FPS & 4 & 127 & \multicolumn{2}{c}{15} & - \\
Learnable param. & 641M & 669M & \multicolumn{2}{c}{315M} & - \\
Training steps & \multicolumn{4}{c}{5K} & - \\
Learning rate & \multicolumn{4}{c}{$5 \times 10^{-5}$} & - \\
Overall batch size & \multicolumn{4}{c}{16} & - \\
\hline
\end{tabular}}
\caption{Hyperparameters for Video Generator.}
\label{tab:kcgsett}
\end{table}


\section{More visualizations}
\label{appendix vis}

We present additional visualizations with three skills in Figure~\ref{fig:vis sup}. In Figure~\ref{fig:rebuttal}, we present a complex skill example under the open-set setting and provide our complete step descriptions generated by the MLLM.

\begin{figure}[h]
  \centering
   \includegraphics[width=1.0\linewidth]{fig/vis_sup.pdf}
   \caption{The visualization of the skill generator.}
   \label{fig:vis sup}
\end{figure}


\begin{figure}[t]  
  \centering
   \includegraphics[width=1.0\linewidth]{fig/rebuttal.pdf}
   \caption{An example of an open-set complex skill.}
   \label{fig:rebuttal}
\end{figure}
