

\section{Method}


Considering the complexity of human skill generation in the wild, our framework consists of three main components. As shown in Figure~\ref{fig:overview}, we first generate detailed descriptions of key steps using multimodal large language models (MLLMs), denoted as $\{D_0, D_1,...,D_{n-1}\}$, which is a training-free stage. Additionally, we use retrieval argument to optimize the output of the MLLM. Then we use the initial image $I_0$ and the corresponding descriptions $\{D_0, D_1,...,D_{n-1}\}$ to generate key step clips $\{V_0, V_1,...,V_{n-1}\}$. Since only the initial image is available, subsequent clips lack a first-frame image to serve as a reference. We use a novel Key-step Image Generation~(KIG) model to generate images $\{I_1, I_2,...,I_{n-1}\}$. Finally, we utilize a video generation model to generate key-step clips based on the generated key-step images and descriptions. Below, we provide a detailed explanation of how to utilize MLLMs as planners, along with the process for generating key-step images and videos. 

\subsection{Using MLLMs as a step planner}
\label{sec: planning}
We use the multimodal processing ability of MLLMs~\cite{gpt-4o,gpt-4o-mini,claude} generate step-by-step descriptions $\{D_0, D_1,...,D_{n-1}\}$ for executing the given skills $G$ based on the initial image $I_0$. Having been exposed to massive datasets, MLLMs naturally acquires a variety of skills. Providing only images and skills as inputs to MLLMs for planning may lead to outputs that vary widely. The complexity of planned steps can vary, ranging from highly intricate to overly simplistic. To address this, we propose a retrieval-augmented approach to effectively control the output of MLLMs. Specifically, we build a skill database based on COIN and CrossTask, which includes the names of each skill and corresponding reference step sequences. During the inference process with MLLMs, we first compute the textual similarity between the current skill and all skills in the database, selecting the top three detailed skills and their reference step sequences as examples for MLLMs. We provide detailed prompts in Appendix~\ref{appendix llm}.

We evaluate the quantitative results under a close-set setting, where the skills during inference are all previously seen in the skill database, and we pre-define a set of possible steps and the number of steps need to generate. Acknowledging that a close-set setting may limit the planning capabilities of MLLMs, we also demonstrate qualitative results under an open-set condition. For skills not previously encountered, we do not pre-define a possible set of steps or the number of steps, retaining only the retrieval enhancement, allowing MLLMs to still infer reasonable sequences. We try four different MLLMs~\cite{gpt-4o,gpt-4o-mini,claude} as our planner, and considering the accuracy of generation and the speed of inference, we ultimately choose ChatGPT-4o-latest model as our step planner.

\subsection{Key-step image generation}
\label{image generation}


\begin{figure}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{fig/kig.pdf}
   \caption{\textbf{Key-step Image Generation.} The input consists of an initial image and step descriptions, from which features are extracted using the IP-Adapter image encoder and CLIP text encoder, respectively. These image and text features are fed into a multi-layer Transformer decoder to autoregressively generate the image features for subsequent clips. The predicted features are then injected into Stable Diffusion XL with IP-Adapter to produce the images.}
   \label{fig:kig}
   \vspace{-10pt}
\end{figure}

To address the challenge of missing $\{I_1, I_2,...,I_{n-1}\}$ and and the inability to use autoregressive methods due to the lack of continuity between key-step clips, we propose a novel Key-step Image Generation (KIG) model. As depicted in Figure~\ref{fig:kig}, KIG consists of two main components: a Skill Transformer and an image generator equipped with IP-Adapter~\cite{ip}. KIG accepts the initial image $I_0$ along with step-wise descriptions $\{D_0, D_1, ..., D_{n-1}\}$ as input. Firstly, a pretrained IP-Adapter~\cite{ip} plus encoder is employed to extract fine-grained image features, denoted as $f_0$, from the initial image $I_0$. Simultaneously, the corresponding textual descriptions of each step are embedded using a CLIP~\cite{clip} Text Encoder, translating these descriptions into skill-specific textual features. Both the image and textual features are integrated and processed through a causal Transformer decoder, which we utilize to predict the IP-Adapter image features for each of the subsequent steps in this skill.
The predicted image features are then passed through the Stable Diffusion XL~\cite{sdxl} model with IP-Adapter to synthesize the corresponding high-quality image for each key-step, ensuring semantic consistency across all key-steps.

During training, we leverage Teacher Forcing by providing the ground-truth features as input to the Transformer. To optimize the model, we employ two MSE losses, one to minimizes the discrepancy between the predicted features and the ground truth feature, and another to ensure the predicted tokens remain consistent with the initial image features $f_0$.
During inference, we autoregressively generate the image features of the remaining key-steps, $\{f_1,f_2,...f_{n-1}\}$. The predicted features are fused with $f_0$ and injected into the existing image model to obtain $\{I_1, I_2,...,I_{n-1}\}$. Notably, our approach does not require fine-tuning of the image generation model, greatly improving training efficiency without sacrificing performance. A complete training process demands only 5 GPU hours and approximately 11GB of VRAM. We use the Stable Diffusion XL (SDXL) as our image generation model and also present results from other image models in the experimental section. 

\subsection{Key-step clips generation}
\label{video generation}
We fine-tune the AnimateAnything (AA)~\cite{aa}, Stable Video Diffusion (SVD)~\cite{SVD}, and DynamiCrafter (DC)~\cite{dynamicrafter} models. Additionally, we evaluate the zero-shot performance of the CogVideoX-5b-I2V~\cite{cogvideox} model. For the AnimateAnything model, which is a text-conditional image-to-video diffusion model, we adhere to the methodology described in the original paper and specifically fine-tune certain layers. In the case of the latent video diffusion model, SVD, since only the image-to-video model parameters are available, we replace the UNetâ€™s conditioning from CLIP image features to CLIP text features and fine-tune only the down blocks and middle block of the UNet. For the DC model, we follow the methodology outlined in the paper and fine-tune all spatial blocks. Furthermore, we report the results for both the DC-512 and DC-1024 model variants. Due to computational constraints, we only evaluate the zero-shot performance of the CogVideoX model.
