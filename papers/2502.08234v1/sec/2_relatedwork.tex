\section{Related work}
\label{sec:related work}

\textbf{Learning skill knowledge in instructional videos.} Instructional videos provide intuitive visual examples for learners to acquire skill knowledge. In recent years, a number of datasets for instructional video analysis~\cite{COIN,crosstask,MPII,Youcook,Youcook2,50salads,breakfast,EPIC-KITCHENS,assembly101,howto100m} have been collected in the community. There are various work for instructional video understanding, among which DDN~\cite{DDN} first proposed procedure planning in instructional videos, requiring the model to plan an action sequence from the start state to the end state to simulate different human skill processes. Recently, many work~\cite{plate,p3iv,PDPP,E3P} have attempted this task using transformer or diffusion models. However, the outputs of previous tasks were only action labels or textual descriptions, which cannot intuitively display the entire process. Therefore, we propose learning human skill generators at key-step levels in instructional videos, which intuitively present skills in video form.

\noindent\textbf{Learning real-world generators.} UniSim~\cite{unisim} has shown it is possible to learn a simulator of the real world in response to various action inputs ranging from texts to robot controls. UniPi~\cite{unipi} has showcased the effectiveness of utilizing text-conditioned video generation to represent policies. The setting of VLP~\cite{VLP} is similar to ours but focuses only on robotic scenarios. Although previous tasks cover various scenarios, we consider human operation scenarios to be the most valuable and challenging. Thus, our task focuses on generating human operation videos, encompassing a wider range of actions and skills. Compared to prior tasks, our approach is more challenging due to the abstract nature of skill descriptions, which demand advanced planning method. Additionally, skill videos often involve changes in object states, which makes this task even more difficult.


\noindent\textbf{Video generation model.} With recent advance in diffusion models~\cite{ddpm,ldm}, video generation models~\cite{SVD,aa,AnimateDiff,latte,dynamicrafter} can now synthesize diverse and realistic videos. However, generating long-duration videos remains a significant challenge. Sora~\cite{sora} attempted to generate high-quality, long-duration videos by extending Diffusion Transformers~\cite{dit}. Additionally, Pandora~\cite{pandora}, a hybrid autoregressive-diffusion model, simulates world states by generating videos and supports real-time control with free-text actions. However, since the key-steps in skills are not fully continuous, the autoregressive approach is unsuitable for our task. To address the discontinuity between steps, we introduce an innovative Key-step Image Generation method that autoregressively generates the initial image for each key step, followed by video clip generation. Our method effectively addresses the discontinuity between steps while ensuring consistency across step clips within the same skill.
