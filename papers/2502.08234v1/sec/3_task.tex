\section{Key-step skill generation}
Given an initial image and a description of the skill, our key-step skill generation aims to generate a sequence of short clips that depict all key steps involved in performing this skill from the current state. In this section, we introduce our task definition in Sec~\ref{task definition}, and the benchmark for our task, which includes a well-curated dataset in Sec~\ref{bench dataset}, and metrics in Sec~\ref{bench metrics}.

\subsection{Task definition}
\label{task definition}
We define an initial image $I_0$, depicting the starting condition of the skill environment, and a textual description of the skill goal $G$ to achieve, which corresponds to the final status. The output is a series of video clips $\{V_0, V_1,...,V_{n-1}\}$ where each video $V_i$ corresponds to one of the key steps necessary for accomplishing the skill as detailed in $G$. The primary objective is to generate a coherent and logically ordered sequence of video segments that collectively synthesize the skill from initiation to completion, accurately reflecting the transitions and outcomes described in $G$. 

\subsection{Dataset construction}
\label{bench dataset}

\textbf{Data source.} We use COIN~\cite{COIN}, CrossTask~\cite{crosstask} and Ht-step~\cite{htstep} as our raw data, which include annotations of step categories and the corresponding segments. COIN involves 180 skills and 778 steps, including 10,250 videos and 40,000 clips. CrossTask involves 18 skills and 133 steps, including 2,325 videos and 19,079 clips. HT-step involves 433 skills and 4,958 steps, including 16,233 videos and 93,997 segments. To better generate human action videos, we also selected 56,915 videos from 88 categories in Kinetics-400~\cite{k400} for auxiliary training. Since these datasets are curated by different communities, it is necessary to overcome their divergence in granularity and annotation quality, even though they all consist of real-world videos. Previous work~\cite{SVD} has demonstrated that data curation is an essential ingredient for video generation. So we design a comprehensive data curation pipeline.

In the aforementioned datasets, the duration of clips ranges from a minimum of 1 second to a maximum of 300 seconds. Considering that longer videos may include repetitive actions and redundant segments, we divide the original clips into several subclips that can accurately describe the corresponding key steps. Each subclip is kept to not exceed two seconds in length.

We filter subclips based on the following points. (i) \textbf{Scene transition removal:} For each clip, we calculate color histogram features and compute the L1 distance between adjacent frames. Frames where the L1 distance exceeds a set threshold are marked as scene transitions, dividing each clip into subclips without transitions. (ii) \textbf{Ensuring appropriate motion amplitude:} Previous methods~\cite{SVD} removed videos with low optical flow, however, they overlooked camera panning segments. To address this, we propose a template matching method utilizing optical flow magnitude histograms. Using RAFT~\cite{raft}, we compute the histogram of optical flow magnitudes for each subclip, and subclips exhibiting high KL divergence from a predefined template are excluded to ensure appropriate motion amplitude. (iii) \textbf{Clip-description alignment:} To align each subclip with its corresponding step description, we use EVA-CLIP~\cite{evaclip} to extract feature vectors from both video frames and text descriptions. We calculate cosine similarity to select subclips with the highest alignment scores. (iv) \textbf{Reducing text-heavy segments:} We identify and remove segments with high text coverage using CRAFT~\cite{craft}, discarding subclips where the average text area in frames exceeds a set threshold. (v) \textbf{Step description optimization:} Initial step descriptions consist of simple verb-noun phrases, which we enhance using image captioner BLIP-2~\cite{blip2} and video captioner VAST~\cite{VAST}. The descriptions are refined through merging with Llama3-8B~\cite{llama} to produce detailed captions, improving video generation quality.




\begin{figure*}[t]
  \centering
   \includegraphics[width=0.7\linewidth]{fig/main.pdf}
   \caption{\textbf{Overview of key-step skill generator.} Taking the skill ``make matcha" as an example, this skill includes three key steps. First, based on the given initial image and skill description, we generate detailed descriptions of the three steps through a MLLM using retrieval argument (RAG). (The figure shows the simplified step descriptions.) Then, We input the initial image and step description into the Key-step Image Generation model to generate the first frame of each step. Finally, we use the generated step descriptions as prompts for the video generation model and create video clips corresponding to each of the three key steps, based on the the corresponding key-step images.}
   \label{fig:overview}
\end{figure*}


\textbf{Dataset split.} Through our pipeline, we efficiently curate the above three datasets, culminating in a collection of about about 110,000 subclips for training. Our training set record over 28,500 operational videos from 615 different skills, involving multiple domains such as Housework, Vehicle, Nursing and Care, and more. For testing, we select 557 videos, 1,672 key-steps from the COIN dataset as our test set. Detailed statistics refer to Appendix~\ref{appendix stat}.

\subsection{Metrics}
\label{bench metrics}
To evaluate the performance of our KS-Gen task, we employ a comprehensive set of metrics that collectively assess various aspects of the generated content. These metrics include action similarity, motion dynamics, and overall visual quality. To evaluate the alignment between automatic metrics and human evaluations, we also conducted a user study. For implementation details, please refer to Appendix~\ref{appendix metrics} and Appendix~\ref{user study}.

(i) \textbf{Action score:} We utilize a VideoMAE~\cite{videomae} model that has been fine-tuned on the Something-Something v2~\cite{ssv2} dataset to calculate the cosine similarity of actions between the generated and real videos. This metric helps in understanding how closely the generated actions mirror the intended real activities. (ii) \textbf{CLIP:} We measure the frame-by-frame semantic similarity between generated and real videos by computing the CLIP~\cite{clip} feature similarity. (iii) \textbf{DINO:} Following VBench~\cite{vbench}, we calculate the DINO~\cite{dino} feature similarity to evaluate the semantic consistency of the main objects between the generated and real videos. (iv) \textbf{Motion distance:} The same method previously described for motion score calculation is used here to measure the motion distance between generated and real clips. This ensures that the generated videos exhibit similar motion dynamics as the real actions. (v) \textbf{Fréchet Inception Distance (FID):} FID~\cite{FID} evaluates the quality of generated images by comparing the distribution of generated single frames to real frames from the dataset. Lower FID scores indicate better quality and greater similarity to the original dataset's visual properties. (vi) \textbf{Fréchet Video Distance (FVD):} FVD~\cite{FVD} measures the distance between the distribution of generated videos and real video clips. It is akin to the FID but adapted for videos.