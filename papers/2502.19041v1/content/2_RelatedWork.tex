
\section{Related Work}

\subsection{Jailbreak Attacks}
Extensive research has been devoted to exploring attack methods for eliciting harmful outputs from LLMs \citep{huang2025trustgen}.
These attacks share a common essence in that they both aim to conceal the true intention and bypass the safety alignment of LLMs in various strategies, such as role-playing, story-telling, ethical dilemmas,rule-breaking, and format-transforming. For example, some works instruct the model to disregard defense mechanisms \citep{perez2022ignore,schulhoff2023ignore}, while others induce the model to generate harmful responses by providing it with examples of unsafe question-and-answer pairs \citep{wei2023jailbreak}. Besides, other approaches utilize implicit templates to redirect original instructions to domains where LLMs lack enough safeguarding. For example, Cipher \citep{yuan2023gpt} converts the original harmful input into alternative encoding formats, Multilingual \citep{deng2023multilingual} translates them into multilingual contexts and FlipAttack \citep{liu2024flipattack} reconstructs through flipping, or embedding them into scenarios like storytelling \citep{ding2023wolf,li2023deepinception}, role-playing \citep{li2023multi} and code completion \citep{lv2024codechameleon}. Similarly, PAIR \citep{chao2023jailbreaking} iteratively optimizes prompts using feedback and scores from LLMs, GPTFuzzer \citep{yu2023gptfuzzer} selects high-quality seed templates by mutating templates and embedding harmful questions in the collected template seed library to attack LLMs.
Although the above attacks attempt to craft jailbreak prompts through a variety of methods, they all share a common essence which is to conceal their true intention to jailbreak LLMs' defenses in various strategies. The motivation of our work is based on this observation: despite the variety of attacks, their essence is limited. As long as we can grasp the common essence of different attacks, we can propose a more universal defense method.

\subsection{Jailbreak Defenses}
Current defense strategies against jailbreak attacks primarily focus on three key perspectives \citep{dong2024attacks}: (1) Safety Alignment, which strengthens the safety capabilities of LLMs through safety alignment. (2) Inference-guidance defenses, which enhance the safety capabilities of LLMs by employing inference guidance techniques, such as prompt engineering (3) Input or output filter defenses, which detect and filter malicious inputs or outputs using predefined filter models or some fixed rules.

Although existing methods have achieved some success in defending against jailbreak attacks, they still exhibit certain limitations. While Safety Alignment enhances the security of LLMs through fine-tuning and alignment data, its effectiveness is constrained by its reliance on high-quality alignment datasets \citep{ji2024aligner} and its inability to address novel, unknown attack patterns. Additionally, due to the vast number of parameters in LLMs, safety fine-tuning is both resource-intensive and impractical. Defense methods applied during the inference phase, such as Intention Analysis \citep{zhang2024intention}, face challenges in detecting complex prompts, particularly those involving encoded inputs, multilingual transformations, or role-playing scenarios designed to conceal harmful intent. Moreover, these methods often compromise the utility of the model. In the realm of input and output filtering, rule-based approaches like the PPL filter \citep{alon2023detecting} tend to be rigid, leading to high false-positive rates \citep{wei2023jailbreak}. Methods such as Autodefense \citep{zeng2024autodefense}, which focus on filtering outputs, introduce additional computational overhead due to real-time detection and may result in over-defensiveness due to overly complex defense frameworks \citep{varshney2023art}. DATDP \citep{armstrong2025defense} utilizes an evaluation agent to conduct iterative assessments of user inputs. It employs a weighted scoring system to classify user inputs as safe or unsafe. However, DATAP will iterate quite a few times, which may cause a waste of inference resources.

Based on the limitations of the above existing defense methods, we propose the Essence-Driven Defense Framework (EDDF), which identifies and detects harmful intentions implied in jailbreak attacks by deeply analyzing the nature of the prompts, rather than solely on surface-level analysis. The EDDF framework is designed to balance security and utility through essences extracted from diverse attacks.




