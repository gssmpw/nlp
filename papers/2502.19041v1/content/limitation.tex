\newpage

\section*{Limitations}
Despite the success of our Essence-Driven Defense Framework in defending against diverse jailbreak prompts and ensuring the acceptance of benign prompts. There remain a few limitations in this work. First, our framework cannot defend against attacks that have not been previously extracted in the offline essence database. To address this, we need to dynamically monitor emerging attack essences and update our offline database in real-time. Furthermore, our approach requires further validation on more advanced models. However, the essence extraction capability of our defense mechanism relies on the fundamental text comprehension capabilities of LLMs. We believe this approach could be effectively generalized to different models as a safety mechanism. Our research highlights the importance of "attack essence" in enhancing LLM safety, and this concept may provide a path forward for securely deploying high-performance language models in the face of ongoing, adversarial, and ever-evolving jailbreak attempts.

\section*{Ethics Consideration}
We prioritize ethical considerations throughout our research. This paper focuses on enhancing the safety of large language models (LLMs) by mitigating jailbreak attacks through an essence-driven defense framework. Our work aims to significantly reduce unsafe responses from LLMs. All experiments are conducted using publicly available datasets, and the findings and conclusions are reported with accuracy and objectivity. Consequently, we believe this research does not raise ethical concerns.
