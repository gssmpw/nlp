\section{Introduction}
Large Language Models (LLMs) have garnered significant attention for their impressive performance across a broad spectrum of natural language tasks\citep{minaee2024large}. Since LLMs are pre-trained on vast amounts of unannotated text data sourced from the Internet \citep{alizadeh2025open}, the content generated by LLMs inevitably includes undesirable elements\citep{crothers2023machine}. Basically, LLMs are fine-tuned through alignment data to refuse answering malicious queries \citep{liu2023trustworthy}, however, they still remain vulnerable to jailbreak attacks \citep{yi2024jailbreak}. Jailbreak refers to attacks aiming to circumvent the constraints to unlock or misuse the full potential of LLMs \citep{yu2024don}. 

Therefore, how to defend LLMs against jailbreak attacks has become critical research. At the model level, safety alignment is conducted before the model is released \citep{zhou2024emulated}. The most apparent drawback is the high time cost and high resource cost for training. Another disadvantage is the slow update of safety alignment, which often becomes outdated as new jailbreak attacks emerge.

Beyond safety alignment, there are two main defense methods during the inference phase. One is inference-guidance defenses, which mainly utilize prompt engineering approaches. Another one is input or output filter defenses, which detect and filter malicious inputs or outputs using predefined filters. Prompt-based defense fundamentally relies on the model's safety capabilities and instruction understanding, which are inherently limited. For example, the method of defending against jailbreak attacks through in-context learning \citep{zhou2024defending} exhibits significant variability in various models. Output filter defenses usually depend on the output of LLMs. Compared to input filter defenses, the target model has already generated harmful content. Traditional input-filter defenses, such as perplexity filtering \citep{alon2023detecting}, paraphrasing \citep{jain2023baseline} and re-tokenization \citep{cao2023defending}, fail to grasp the essence of the difference between jailbreak attacks and benign queries.

Attackers can easily generate new variants of jailbreak attacks based on existing data. However, current methods focus mainly on surface-level patterns. These approaches, trained on outdated data, and prompt-based defenses, which only recognize shallow attack techniques, fail to capture the deeper essence of jailbreak attacks. In this paper, we propose the Essence-Driven Defense Framework (EDDF), which can generalize and match unknown attacks based on limited known data.

To summarize, our contributions are as follows:
\begin{itemize}
      \item We introduce EDDF, a novel method that significantly enhances LLM safety against diverse and evolving jailbreak attacks that share a common underlying essence, through an Essence-Driven Defense Framework.
      \item EDDF is a plug-and-play input-filtering method that eliminates the need for costly safe training. It extracts the core "attack essence" from a wide range of known attack instances and stores these essences in an offline vector database. When a new user query is received, the framework retrieves relevant essences and applies them to defend against attacks. 
      \item Experimental results demonstrate that our work achieves state-of-the-art performance. EDDF significantly outperforms existing methods by reducing the Attack Success Rate by at least 20\%. Additionally, in benign query identification, EDDF achieves a False Positive Rate (FPR) of just 2.18\%.
\end{itemize}

