%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
\documentclass[sigconf,screen,natbib=true]{acmart}

% \documentclass[sigconf,review,screen,natbib=true,anonymous=true]{acmart}


%%
%% \BibTeX command to typeset BibTeX logo in the docs
% \AtBeginDocument{%
%   \providecommand\BibTeX{{%
%     Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2025}
% \acmYear{2025}
% \acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\usepackage{color}
\usepackage{amsmath}
%\usepackage{amssymb}
\usepackage{xspace}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{CJKutf8} %% 新增
\usepackage{multicol} %% 新增
\usepackage{multirow} %% 新增
\usepackage{subcaption}
\usepackage{graphicx}



\begin{document}
\begin{CJK}{UTF8}{gbsn} %% 新增
%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \title{The Ghost in the Machine: Uncovering Source Bias in AI-Generated Videos}
\title{Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated Videos}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
% }
% \thanks{\ \ Corresponding author}

\author{Haowen Gao\quad
Liang Pang*\quad
Shicheng Xu}
\affiliation{%
  \institution{CAS Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences}
  \country{China}}
\email{gaohaowen23s@ict.ac.cn}
\email{pangliang@ict.ac.cn}
\email{xushicheng21s@ict.ac.cn}



\author{Leigang	Qu\quad
Tat-Seng Chua}
\affiliation{%
  \institution{Sea-NExT Joint Lab, National University of Singapore}
  \country{Singapore}}
\email{leigangqu@gmail.com}
\email{dcscts@nus.edu.sg}

\author{Huawei Shen, Xueqi Cheng}
\affiliation{%
  \institution{CAS Key Laboratory of AI Security, Institute of Computing Technology, Chinese Academy of Sciences}
  \country{China}}
\email{shenhuawei@ict.ac.cn}
\email{cxq@ict.ac.cn}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

With the rapid development of AI-generated content (AIGC), the creation of high-quality AI-generated videos has become faster and easier, resulting in the Internet being flooded with all kinds of video content.
However, the impact of these videos on the content ecosystem remains largely unexplored. 
Video information retrieval remains a fundamental approach for accessing video content.
Building on the observation that retrieval models often favor AI-generated content in ad-hoc and image retrieval tasks, we investigate whether similar biases emerge in the context of challenging video retrieval, where temporal and visual factors may further influence model behavior.
To explore this, we first construct a comprehensive benchmark dataset containing both real and AI-generated videos, along with a set of fair and rigorous metrics to assess bias.
This benchmark consists of 13,000 videos generated by two state-of-the-art open-source video generation models. 
We meticulously design a suite of rigorous metrics to accurately measure this preference, accounting for potential biases arising from the limited frame rate and suboptimal quality of AIGC videos.
We then applied three off-the-shelf video retrieval models to perform retrieval tasks on this hybrid dataset. 
Our findings reveal a clear preference for AI-generated videos in retrieval.
Further investigation shows that incorporating AI-generated videos into the training set of retrieval models exacerbates this bias. 
Unlike the preference observed in image modalities, we find that video retrieval bias arises from both unseen visual and temporal information, making the root causes of video bias a complex interplay of these two factors. 
To mitigate this bias, we fine-tune the retrieval models using a contrastive learning approach. 
The results of this study highlight the potential implications of AI-generated videos on retrieval systems and offer valuable insights for future research in this area. 
Our dataset and code are publicly available at \url{https://github.com/Siaaaaaa1/video-source-bias}.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317</concept_id>
<concept_desc>Information systems~Information retrieval</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Information retrieval}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Text-Video Retrieval, AIGC, Bias and Fairness}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.

%\newpage
\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{\ Corresponding author}
\section{Introduction}

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{picture/sigir-graph-main.pdf}
% \caption{The Visual-Temporal Induced Source Bias identified in this paper occurs when AI-generated videos, produced by video generation models~\cite{yang2024cogvideox,opensora}, are mixed with real videos on the Internet. When performing text-video retrieval, the retrieval model~\cite{bain2021frozen,li2022align,wang2022internvideo} tends to prioritize AI-generated videos in the search results. This bias arises from the additional visual and temporal information embedded in the videos by the generation model.}
\caption{The Visual-Temporal Induced Source Bias occurs when AI-generated videos, created by video generation models~\cite{yang2024cogvideox,opensora}, are mixed with real videos on the Internet. During text-video retrieval, the retrieval model~\cite{bain2021frozen,li2022align,wang2022internvideo} tends to prioritize AI-generated videos due to the extra visual and temporal information embedded by the generation model.}
\label{fig:overall}
\end{figure}



In the contemporary digital era, video content stands out among various media formats due to its unique dynamism and vividness, emerging as the preferred medium for information dissemination and entertainment~\cite{goggin2010global,tarchi2021learning}.
Information retrieval, particularly in the video domain, acts as the vital entry point for users navigating this vast content ecosystem.
As artificial intelligence (AI) rapidly evolves, the production of AI-generated videos has become significantly easier and faster~\cite{wu2023ai,foo2023ai,xing2024survey,opensora}, leading to a surge of this content type online. 
This influx of AI-generated videos raises a critical question: How will video retrieval models handle these AI-generated videos?

Similar questions have also been proposed in the textual and image content ecosystem. 
Previous studies have found that in both text and image domains, retrieval models prioritize AI-generated content, a phenomenon known as "source bias"~\cite{dai2023llms,xu2024invisible}, the aim of our study is to explore the source bias in video modality.
However, the video modality presents unique challenges, making bias assessment more complex.
First, generating AI-generated videos that are semantically similar to real ones is particularly difficult due to the resource-intensive and time-consuming nature of video creation. This issue is further compounded by the limitations of open-source models, which often fail to produce satisfactory results.
Second, assessing bias in video retrieval models requires a more nuanced approach. It is necessary to incorporate multidimensional metrics across the retrieval list to capture various biases.
Additionally, the impact of semantic discrepancies between videos must be minimized to avoid skewing evaluation results. 
Specifically, it is crucial to ensure that the retrieval model does not favor AI-generated or real videos simply due to semantic proximity to the textual description.
Lastly, pinpointing the sources of bias is more challenging in video than in text or image domains. 
Videos contain not only rich visual information but also unique temporal elements, adding complexity to bias analysis.

The first two challenges have led us to focus on creating a standardized benchmark for video retrieval that includes both real and AI-generated videos, with relevance annotations for these videos also being developed. 
To the best of our knowledge, no such dataset currently exists.
To ensure that the benchmark is suitable for assessing video retrieval bias, we must address challenges related to video generation alignment (the similarity between queries and generated videos), video generation quality (the similarity between real and generated videos), and unbiased bias assessment metrics (which account for varying relevance levels) (See~\textbf{\color{red}{\S\ref{sec:2}}}).
Our dataset consists of 13,000 videos, including 9,000 training videos and four test sets, each containing 1,000 videos. It leverages two state-of-the-art video generation models: CogVideoX~\cite{yang2024cogvideox} and OpenSora V1.2~\cite{opensora}. These models collaboratively generate videos by integrating text, real video frames, or clips, all based on the MSR-VTT dataset~\cite{xu2016msr}.
To further minimize the impact of semantic similarity between videos and their corresponding queries on retrieval ranking, we introduce a novel evaluation metric, $Normalized\Delta$.
Additionally, to offer a more comprehensive assessment of multi-dimensional retrieval performance, we propose the MixR metric. 
Unlike previous evaluation measures that focus solely on top-ranked retrieval (R@1), MixR combines MeanR and MedR with R@1 to capture the broader impact of the entire retrieval list.

Experimental results from our constructed benchmark reveal an intriguing phenomenon: text-video retrieval models tend to prioritize AI-generated videos over real videos. 
Specifically, these models often rank AI-generated videos higher than real videos, even when both have the same relevance level (see \textbf{\color{red}{\S\ref{sec:3.2}}}). 
As AI-generated videos become more prevalent on the internet, they are likely to be incorporated into the training datasets of future retrieval models. 
Our further findings suggest that as the proportion of AI-generated videos in the training set increases, retrieval models progressively favor AI-generated content, exhibiting a growing bias toward prioritizing it (see \textbf{\color{red}{\S\ref{sec:3.3}}}).

Videos differ significantly from other modalities, as they contain several times more information than text and images.
Source bias in videos not only encompasses the visual bias found in image modalities but also integrates temporal information.
Compared to the singular causes of source bias in text and image modalities, the root causes of video source bias are far more complex and challenging to analyze.
In response to this challenge, we investigate the underlying causes of video source bias by disrupting temporal information through randomized frame order (see \textbf{\color{red}{\S\ref{sec:4.1}}}) and isolating visual information by extracting single-frame images (see \textbf{\color{red}{\S\ref{sec:4.2}}}).
Experiments demonstrate that the additional information embedded in both the visual and temporal components of generated videos plays a key role in generating bias, which we term \textbf{Visual-Temporal Induced Source Bias}. 
Specifically, real videos contain richer temporal information, whereas generated videos, lacking sufficient temporal depth, primarily rely on single-frame changes. 
This lack of depth contributes to the formation of source bias. 
Moreover, compared to retrieval tasks in other modalities, video retrieval exhibits a stronger preference for the first retrieved video. It shows a general tendency to favor AI-generated videos in the retrieval results.

To mitigate Visual-Temporal Induced Source Bias in retrieval models, we apply a contrastive learning approach~\cite{chen2020simple} to fine-tune the models, incorporating AI-generated videos into the training set (see \textbf{\color{red}{\S\ref{sec:5.1}}}).
Through fine-tuning, we train the model to prioritize real videos over AI-generated ones, placing real videos at the top of the retrieval list and ensuring they appear before AI-generated videos in the overall ranking, which effectively reduces Visual-Temporal Induced Source Bias.
By quantifying the differences in vector representations between the debiased and original models, we use t-SNE~\cite{van2008visualizing} to visualize the Visual-Temporal Induced Source Bias in the videos (See \textbf{\color{red}{\S\ref{sec:5.2}}}).

Our contributions include:

(1) We construct a benchmark that includes both real and AI-generated videos to investigate the impact of AI-generated content on video retrieval models. 

(2) We reveal that AI-generated videos introduce Visual-Temporal Induced Source Bias, which stems from the additional visual and temporal information embedded by video generation encoders, leading retrieval models to rank them higher.

(3) We propose a debiasing method for video retrieval models that effectively reduces Visual-Temporal Induced Source Bias towards AI-generated videos.

\section{Benchmark Construction}
\label{sec:2}
In this section, we pioneeringly construct a benchmark to evaluate the impact of AI-generated videos on text-video retrieval models. The construction of this benchmark involves four stages: real retrieval dataset selection, semantic-equivalent video generation, dataset quality evaluation, and construction of bias evaluation metrics. Additionally, the benchmark should meet three key requirements to ensure the reliability of the research outcomes:

\textbf{(1) Identical Semantics:} Ensuring generated videos have the same semantics as original ones, so they share the same relevant labels as the query, which helps prevent abnormal retrieval rankings due to excessive video-query similarity.


\textbf{(2) Realistic Generation:} Video generation methods should align with real-world applications (e.g., generating videos from texts or combining texts with images) and ensure video quality, especially the similarity between real and generated videos.


\textbf{(3) Unbiased Assessment:} Ensuring identical semantics (the first requirement) is hard to precisely attain because of generation model capabilities. Therefore, we need robust metrics accounting for varying relevance levels to measure source bias impact and prevent the influence of different levels.

\begin{figure*}[h]
\centering

\begin{subfigure}{0.17\textwidth}
    \centering
    \includegraphics[width=\linewidth]{picture/cogvideox_sim_list.pdf}
    \caption*{\scriptsize(a) CogVideoX TextCond}
    \label{fig:REALR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.17\textwidth}
    \centering
    \includegraphics[width=\linewidth]{picture/opensora_fusion_sim_list.pdf}
    \caption*{\scriptsize(b) OpenSora TextCond}
    \label{fig:AIR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.17\textwidth}
    \centering
    \includegraphics[width=\linewidth]{picture/opensora_image_condition_sim_list.pdf}
    \caption*{\scriptsize(c) OpenSora ImageCond}
    \label{fig:mixRealR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.17\textwidth}
    \centering
    \includegraphics[width=\linewidth]{picture/opensora_video_extending_sim_list.pdf}
    \caption*{\scriptsize(d) OpenSora VideoExt}
    \label{fig:mixAIR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.17\textwidth}
    \centering
    \includegraphics[width=\linewidth]{picture/opensora_fusion_train_sim_list.pdf}
    \caption*{\scriptsize(e) OpenSora TextCond(train)}
    \label{fig:relativedeltaR@k}
\end{subfigure}
\vspace{-1em}
\caption{Using the CLIP model to compute the similarity between AI-generated video datasets and real video datasets (X-axis), and analyzing the distribution of similarities frequency (Y-axis).}
\vspace{-1em}
\label{fig:benchmark}
\end{figure*}


\begin{table}
\belowrulesep=0pt
  \aboverulesep=0pt
  \caption{AI-generated video Datasets Overview.}
    \vspace{-0.8em}
  \label{tab:benchmark}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{1.25}
  \large
  % \rowcolors{2}{white}{gray!15}
  \resizebox{0.45\textwidth}{!}{%
  \begin{tabular}{lccccc}
  \toprule
    \makecell{Dataset} & \makecell{Number} & \makecell{FPS} & \makecell{Duration} & \makecell{Resolution} & \makecell{Similarity} \\
    \midrule
    \small CogVideoX TextCond & 1000 & 8 & 6.12 & 720×480 & 0.7232 \\
    \small OpenSora TextCond & 1000 & 24 & 4.25 & 640×360 & 0.7304 \\
    \small OpenSora ImageCond & 1000 & 24 & 4.25 & 640×360 & 0.8725 \\ 
    \small OpenSora VideoExt & 1000 & 12* & 8.50 & 424×240* & 0.7745 \\
    \small OpenSora TextCond (Train) & 9000 & 24 & 4.25 & 640×360 & 0.7332
   \\ \bottomrule 
  \end{tabular}
  }
  \footnotesize
  \raggedright
  \textit{\\ * Due to the limitation of resources.}
  \vspace{-2em}
\end{table}


\subsection{Real Retrieval Dataset Selection}
\label{sec:2.1}

%To select an appropriate real retrieval dataset, we analyze 

To select an appropriate real retrieval dataset, we analyze five well-known text-video retrieval datasets, including MSR-VTT~\cite{xu2016msr}, MSVD~\cite{chen2011collecting}, DiDeMo~\cite{anne2017localizing}, ActivityNet~\cite{caba2015activitynet}, and LSMDC~\cite{rohrbach2015dataset}. The selected datasets should meet the following two core criteria. 
(1)~\textbf{Scenario Diversity}: they should include a diverse range of real videos, enriching the variety of video types encountered by users. This ensures reliability and broad applicability in evaluating source bias. For example, the LSMDC dataset does not meet this requirement.
(2)~\textbf{Annotation Completeness}: the captions for videos should cover the entire video, not just segments, and as many captions as possible should be provided. 
Without comprehensive annotations, accurate video generation cannot be achieved to minimize semantic biases between AI-generated and real videos. 
For instance, the MSVD, DiDeMo, and ActivityNet datasets do not meet this requirement.
Consequently, we select MSR-VTT, a widely recognized, large-scale dataset comprising 10,000 videos across 20 categories, with each video annotated by 20 English captions. 
The training set contains 9,000 videos, while the test set includes 1,000 videos. The dataset split follows the same partitioning as in Bain \textit{et al.}~\cite{bain2021frozen}.

\subsection{Semantic-equivalent Video Generation}
\label{sec:2.2}
Selecting appropriate video generation models and strategies is essential for generating semantically identical AI-generated videos for the MSR-VTT dataset. CogVideoX~\cite{yang2024cogvideox} and OpenSora V1.2~\cite{opensora} are two publicly available and widely used video generation models. They have distinct technical advantages, which makes them suitable for comprehensively assessing the presence of source bias.

CogVideoX can generate high-quality content. However, it is time-consuming and has a single text-input interface. In contrast, OpenSora consumes fewer resources and has a more diverse interface, allowing the combination of images or videos for video generation. 
Given these state-of-the-art video generation models, generating semantically identical videos to the original ones remains a substantial challenge. Therefore, we employ multiple strategies to ensure comprehensive and robust experimental results.

For the videos in the test set, taking advantage of the diverse interfaces of OpenSora, we can adopt three strategies: text-only, text-image integration, and text-video integration for video generation. CogVideoX, on the other hand, can only use the text-only interface to generate videos. These four settings in the test set can more precisely verify the source bias.
For the videos in the training set, we simply use the highly efficient OpenSora with only text as the prompt input. This is to further explore the influence of AIGC in the training of the retrieval model. 
The detail settings of these datasets are listed below:

\textbf{(1) Text-condition (TextCond):} To ensure that the generated videos are semantically similar to the original ones and encapsulate all relevant information, we integrate multiple captions into a single prompt.
For each video, we use GPT-4 to integrate its 20 captions by inputting them with the prompt: "I will provide 20 captions of the same video. Please assist in merging them into a comprehensive description." The fusion caption summarizes the multiple descriptions of the video and represents the full content of the real video. Next, we input the fusion caption into CogVideoX and OpenSora, and obtain the CogVideoX TextCond and OpenSora TextCond (test and train) datasets.

\textbf{(2) Image-condition (ImageCond):} To generate video content that better aligns with the visual semantics of video, we use a text-image integration approach. 
Specifically, we pick a keyframe from the real video at the 20\% timestamp and its fusion caption, then input them into OpenSora to create the OpenSora ImageCond dataset. Given the high generation cost, this dataset contains only the test set.

\textbf{(3) Video-extending (VideoExt):} To capture the full content of a real video, we employ a text-video integration approach for video generation. 
We input the first half of a real video and its fusion caption into OpenSora to create the OpenSora VideoExt dataset. This method improves the model's real video understanding, increasing the chance of generating visually and semantically similar videos. Due to high generation costs, the dataset only has a test set.

\subsection{Dataset Quality Evaluations}
\label{sec:2.3}
To validate the reliability of the constructed benchmark, we compile key properties of the videos and assess their similarity to real videos. Key parameters of the dataset are shown in Table~\ref{tab:benchmark}. For similarity assessment, we uniformly select 10 frames from each generated video and its corresponding real video. Utilizing the CLIP model~\cite{radford2021learning}, we compute the average video representation and subsequently calculate the cosine similarity between the real and generated videos.

% We found that the AI-generated videos in the test set exhibit high semantic similarity to the real videos, with an average similarity exceeding 0.72, while the OpenSora ImageCond dataset achieves a similarity of 0.87. Additionally, the consistency between the training and corresponding test sets is well preserved. In the OpenSora TextCond dataset, the similarity between the train and test sets is nearly identical, with an average difference of a mere 0.0028.

We find that AI-generated videos in the test set exhibit a high semantic similarity to the real videos. Specifically, the average similarity exceeds 0.72, while the OpenSora ImageCond dataset achieves a similarity of 0.87. Additionally, the consistency between the training and corresponding test sets is well preserved. In the OpenSora TextCond dataset, the similarity between the training and test sets is nearly identical, with an average difference of only 0.0028.


\subsection{Bias Evalueation Metrics}
\label{sec:2.4}

% To offer a more fair and comprehensive evaluation of source bias, we propose two metrics: MixR and $Normalized\Delta$. Specifically, the MixR metric integrates the assessment of both the top-ranked retrieval and the entire retrieval list, while $Normalized\Delta$ mitigates the influence of semantic discrepancies in videos.
To provide a more fair and comprehensive evaluation of source bias, we propose two metrics: MixR and $Normalized\Delta$. Specifically, the MixR metric combines the evaluation of both the top-ranked retrieval and the entire retrieval list, while $Normalized\Delta$ mitigates the influence of semantic discrepancies between videos.

\textbf{Notation:} Formally, for a single dataset, $REAL$ and $AI$ denote retrieval from the real and AI-generated video datasets, respectively. For a mixed dataset, $mixed-REAL$ and $mixed-AI$ correspond to retrieval tasks for real and AI-generated videos. $Metric$ encompasses R@k, MeanR, and MedR. Specifically, R@k measures the proportion of relevant items retrieved within the top-k results, MeanR calculates the average rank of relevant items, and MedR represents the median rank of relevant items across all queries. $Rank$ denotes the retrieval rank of a relevant item in a single query.

\textbf{Evaluation Metrics Calculation:} 
To quantify source bias during retrieval, we introduce the $Relative\Delta$ metric, as proposed by Xu \textit{et al}.~\cite{xu2024invisible}, which assesses the impact of AI-generated videos on the ranking of real videos in mixed retrieval scenarios.
When $Metric$ is R@k, the formula is as follows:
$$
Relative\Delta=\frac{2(Metric_{mixed-REAL}-Metric_{mixed-AI})}{(Metric_{mixed-REAL}+Metric_{mixed-AI})}\times 100\%.
$$
When $Metric$ is MeanR or MedR, the formula becomes:
$$
Relative\Delta=\frac{2(Metric_{mixed-AI}-Metric_{mixed-REAL})}{(Metric_{mixed-REAL}+Metric_{mixed-AI})}\times 100\%.
$$

Given the high cost and instability of current video generation models, the quality of generated videos is often inconsistent, which may cause retrieval models to mistakenly favor certain videos during source bias evaluation. 
Methods are needed to mitigate the differences between $Metric_{REAL}$ and $Metric_{AI}$, reducing the impact of measurement bias. 
To address this, we propose a novel metric, $Normalized\Delta$ and $Localtion\Delta$. 
$Location\Delta$ estimates the expected retrieval score by analyzing the ranking positions of real and AI-generated videos when retrieved separately, while disregarding semantic differences between the datasets. 
$Normalized\Delta$ combines $Relative\Delta$ and $Location\Delta$. 
Compared to directly using $Relative\Delta$, $Normalized\Delta$ more effectively mitigates the influence of semantic discrepancies, providing a more accurate bias assessment. 

To calculate $Location\Delta$, we first record the ranking of each video when retrieved independently from both the real and AI-generated datasets. 
Since the datasets are not yet mixed, the rankings do not account for cross-dataset visual-semantic differences. 
Next, we interpolate the rankings by combining the rank lists of $REAL$ and $AI$. 
The mixed rankings are calculated as follows, $c$ is a random number of 0 or 1:
\vspace{-0.2em}
\begin{align*}
Rank_{mixed-REAL}&=\ 2*Rank_{REAL}-c,\\
Rank_{mixed-AI}&=2\ *\ Rank_{AI}-(1-c).
\vspace{-0.2em}
\end{align*}

We calculate $Metric_{mixed-REAL}^L$ and $Metric_{mixed-AI}^L$, which represent $Metric$ computed through $Rank$, and then compute:
$$
Location\Delta=\frac{2(Metric_{mixed-REAL}^L-Metric_{mixed-AI}^L)}{(Metric_{mixed-REAL}^L+Metric_{mixed-AI}^L)} \times 100\%.
$$ 
$Relative\Delta$ reflects the actual retrieval model performance, and $Location\Delta$ accounts for performance without considering semantic differences between real and AI-generated videos. 
The difference between these metrics represents the Visual-Temporal Induced Source Bias in the mixed retrieval model, termed $Normalized\Delta$:
\vspace{-0.2em}
$$
Normalized\Delta = Relative\Delta - Location\Delta.
$$

When $Normalized\Delta > 0$, the model favors real videos over AI-generated ones. 
When $Normalized\Delta < 0$, the model favors AI-generated videos over real ones. 
Compared to directly using $Relative\Delta$, $Normalized\Delta$ better mitigates the influence of semantic discrepancies, offering a more accurate bias assessment.

Additionally, MixR combines R@1, MedR, and MeanR as follows. 
With $\Delta$ representing $Relative\Delta$, $Location\Delta$ or $Normalized\Delta$, the calculation method is as follows:

$$
\Delta MixR = (\Delta R@1+\Delta MedR+\Delta MeanR) / 3.
\vspace{-0.2em}
$$

\begin{table*}[ht]
    \belowrulesep=0pt
    \aboverulesep=0pt
    \caption{The retrieval performance of different models is evaluated on CogVideoX TextCond and OpenSora TextCond. When $Relative\Delta>0$ or $Normalized\Delta>0$, it indicates that the retrieval model tends to rank real videos higher. Conversely, \textcolor{red}{$Relative\Delta<0$} or \textcolor{red}{$Normalized\Delta<0$} suggests that the model tends to rank AI-generated videos higher. The absolute values of these metrics reflect the magnitude of the bias. $Normalized\Delta$ incorporates a penalty term to the original $Relative\Delta$, providing a more precise measurement of the bias.}
  \label{tab:main-exp-text}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \small
  \resizebox{0.78\textwidth}{!}{%
  \begin{tabular}{ll|cccccc|cccccc}
  \toprule
    \multicolumn{2}{c|}{\makecell[l]{\hspace{1.5em}\textbf{Dataset}}}  & 
    \multicolumn{6}{c|}{\makecell[l]{\hspace{3em}\textbf{CogVideoX TextCond}}} &
    \multicolumn{6}{c}{\makecell[l]{\hspace{3em}\textbf{OpenSora TextCond}}}
    \\\midrule
     Model &   Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
     \multirow{6}{*}{\makecell[c]{Alpro}}& REAL & 24.10  & 45.10  & 55.50  & 8.00  & 49.61 & - & 24.10  & 45.10  & 55.50  & 8.00  & 49.61 & -  \\
      & AI & 30.50  & 51.70  & 61.90  & 5.00  & 40.14 & - & 37.00  & 59.30  & 68.90  & 3.00  & 27.72 & -  \\
     & mixed-REAL & 10.10  & 34.60  & 45.50  & 14.00  & 82.94  & -  & 10.80  & 35.40  & 46.80  & 13.50  & 83.72  & -  \\ 
     & mixed-AI & 22.60  & 42.70  & 50.70  & 10.00  & 101.16  & -  & 24.50  & 49.50  & 56.10  & 6.00  & 69.39  & -  \\ 
     & $Relative\Delta$ & \textcolor{red}{-76.45}  & \textcolor{red}{-20.96}  & \textcolor{red}{-10.81}  & \textcolor{red}{-33.33}  & 19.80  & \textcolor{red}{-29.99}  & \textcolor{red}{-77.62}  & \textcolor{red}{-33.22}  & \textcolor{red}{-18.08}  & \textcolor{red}{-76.92}  & \textcolor{red}{-18.71}  & \textcolor{red}{-57.75}  \\ 
    & $Normalized\Delta$ & \textcolor{red}{-53.01}  & \textcolor{red}{-2.59}  & 2.83  & 14.67  & 41.02  & 0.89  & \textcolor{red}{-35.39}  & 3.05  & 9.12  & 18.32  & 38.26  & 7.06  \\ \hline
     \multirow{6}{*}{\makecell[c]{Frozen}} & REAL & 22.90  & 43.20  & 53.60  & 8.00  & 49.81 & - & 22.90  & 43.20  & 53.60  & 8.00  & 49.81 & -  \\
     & AI & 29.80  & 50.60  & 60.80  & 5.00  & 39.98 & - & 31.50  & 54.70  & 64.30  & 4.00  & 31.56 & -  \\
     & mixed-REAL &
    6.90  & 28.20  & 39.10  & 20.00  & 92.25  & -  & 8.90  & 31.40  & 41.40  & 17.00  & 90.35  & -  \\ 
     & mixed-AI &
    23.80  & 45.20  & 53.00  & 8.00  & 90.98  & -  & 25.50  & 46.80  & 55.40  & 7.00  & 72.41  & -  \\ 
     & $Relative\Delta$ &
    \textcolor{red}{-110.10}  & \textcolor{red}{-46.32}  & \textcolor{red}{-30.18}  & \textcolor{red}{-85.71}  & \textcolor{red}{-1.39}  & \textcolor{red}{-65.73}  & \textcolor{red}{-96.51}  & \textcolor{red}{-39.39}  & \textcolor{red}{-28.93}  & \textcolor{red}{-83.33}  & \textcolor{red}{-22.05}  & \textcolor{red}{-67.30}  \\ \multirow{-6}{*} & $Normalized\Delta$ &
    \textcolor{red}{-83.91}  & \textcolor{red}{-23.51}  & \textcolor{red}{-14.40}  & \textcolor{red}{-37.71}  & 20.63  & \textcolor{red}{-33.66}  & \textcolor{red}{-64.89}  & \textcolor{red}{-10.89}  & \textcolor{red}{-5.44}  & \textcolor{red}{-13.76}  & 23.08  & \textcolor{red}{-18.52}  \\ \hline
     \multirow{6}{*}{\makecell[c]{Intern\\Video}} & REAL & 40.60  & 66.70  & 75.20  & 2.00  & 22.27 & - & 40.60  & 66.70  & 75.20  & 2.00  & 22.27 & -  \\
     & AI & 40.20  & 64.00  & 73.40  & 2.00  & 25.30 & - & 47.20  & 71.50  & 78.40  & 2.00  & 17.85 & -  \\
     & mixed-REAL &
    19.60  & 52.30  & 63.50  & 5.00  & 43.39  & -  & 27.40  & 53.10  & 62.20  & 5.00  & 74.16  & -  \\ 
     & mixed-AI &
    27.60  & 56.10  & 64.90  & 4.00  & 56.31  & -  & 22.50  & 58.20  & 68.90  & 4.00  & 26.87  & -  \\ 
     & $Relative\Delta$ &
    \textcolor{red}{-33.90}  & \textcolor{red}{-7.01}  & \textcolor{red}{-2.18}  & \textcolor{red}{-22.22}  & 25.92  & \textcolor{red}{-10.07}  & 19.64  & \textcolor{red}{-9.16}  & \textcolor{red}{-10.22}  & \textcolor{red}{-22.22}  & \textcolor{red}{-93.61}  & \textcolor{red}{-32.06}  \\ 
     & $Normalized\Delta$ &
    \textcolor{red}{-34.89}  & \textcolor{red}{-11.17}  & \textcolor{red}{-6.31}  & \textcolor{red}{-22.22}  & 13.06  & \textcolor{red}{-14.68}  & 34.67  & \textcolor{red}{-1.66}  & \textcolor{red}{-3.27}  & \textcolor{red}{-22.22}  & \textcolor{red}{-71.32}  & \textcolor{red}{-19.62} 
    \\ \bottomrule 
  \end{tabular}
  }
   % \vspace{-0em}
\end{table*}


\section{Video Source Bias Assessment}
\label{sec:3}

In this section, we select three retrieval models and use the benchmark we have constructed to evaluate the impact of incorporating AI-generated videos into the video library on retrieval performance. Moreover, through experiments involving AI-generated videos into both the test and training sets, we observe that the retrieval models tend to rank AI-generated videos higher in the retrieval results.

\subsection{Text-Video Retrieval Models}
To assess the source bias of retrieval models, it is first necessary to select appropriate models. We choose three distinct open-source video-text retrieval models with robust zero-shot retrieval capabilities to examine the source bias of AI-generated videos, including:

\textbf{(1) Frozen in Time~\cite{bain2021frozen}}: This model uses joint video and image encoders for end-to-end retrieval. 
It employs a Space-Time Transformer Encoder, which processes both image and video data flexibly, treating images as "frozen" snapshots of videos during training.


\textbf{(2) ALPRO~\cite{li2022align}}: By sparsely sampling video frames, ALPRO achieves effective cross-modal alignment without explicit object detectors. 
It introduces a Video-Text Contrastive Loss for aligning video and text features, simplifying cross-modal interaction modeling. 
Additionally, ALPRO features a Prompting Entity Modeling task for fine-grained alignment of visual regions and textual entities via self-supervision.


\textbf{(3) InternVideo~\cite{wang2022internvideo}}: This model combines generative and discriminative self-supervised learning strategies to optimize video representations. 
It uses Masked Video Modeling and Video-Language Contrastive Learning as pretraining tasks, with a learnable coordination mechanism to integrate both types of video representations.



\begin{figure*}[ht]
\centering
\begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\linewidth]{picture/Real_Rk.pdf}
        \caption*{\footnotesize(a) R@k of Independent retrieval on real videos}
        \label{fig:REALR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.16\textwidth}
    \includegraphics[width=\linewidth]{picture/AI_Rk.pdf}
    \caption*{\footnotesize(b) R@k of Independent retrieval on AI videos}
    \label{fig:AIR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.16\textwidth}
    \includegraphics[width=\linewidth]{picture/mixReal_Rk.pdf}
    \caption*{\footnotesize(c) R@k of real videos retrieval on mixed dataset}
    \label{fig:mixRealR@k}
    
\end{subfigure}
\hfill
\begin{subfigure}{0.16\textwidth}
    \includegraphics[width=\linewidth]{picture/mixAI_Rk.pdf}
    \caption*{\footnotesize(d) R@k of AI videos retrieval on mixed dataset}
    \label{fig:mixAIR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.16\textwidth}
    \includegraphics[width=\linewidth]{picture/RelativeDelta_Rk.pdf}
    \caption*{\footnotesize(e) R@k of RelativeΔ on mixed dataset}
    \label{fig:relativedeltaR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.16\textwidth}
    \includegraphics[width=\linewidth]{picture/DebiasDelta_Rk.pdf}
    \caption*{\footnotesize(f) R@k of NormalizedΔ on mixed dataset}
    \label{fig:debiasdeltaR@k}
\end{subfigure}
\vspace{-0.6em}
\caption{Evaluation results on a training set containing a mix of AI-generated videos. We vary the proportion of AI-generated videos in the dataset (X-axis), while keeping the total number of training samples constant. The model is then tested on the OpenSora TextCond test set.}
\label{fig:mix-metric}
\vspace{-1.5em}
\end{figure*}



\subsection{Visual-Temporal Induced Source Bias}
\label{sec:3.2}
Experiments shown in Table~\ref{tab:main-exp-text} and Table~\ref{tab:main-exp-picture}, demonstrate that source bias is prevalent across various video retrieval models. Specifically, retrieval models tend to rank AI-generated videos higher than real videos, showing a clear preference for AI-generated content. Our specific key findings are:

(1) \textbf{Source bias is a widespread phenomenon, not specific to any particular video generation or retrieval model.} As shown in Table \ref{tab:main-exp-text}, for different video generation models, both $Relative\Delta$ and $Normalized\Delta$ values are generally negative. This suggests that vision-language models pre-trained on large video-text and image-text datasets tend to rank AI-generated videos higher.

(2) \textbf{Incorporating video or image segment information into the video generation process amplifies the Visual-Temporal Induced Source Bias.} As shown in Table~\ref{tab:main-exp-picture}, we repeat the experiment on the OpenSora ImageCond and VideoExt datasets, where real video content is integrated into the generated videos. Our findings show that the retrieval metric gap narrows during individual retrieval, while the $Relative\Delta$ and $Normalized\Delta$ values increase.

(3) \textbf{When AI-generated videos are included in a video library, source bias significantly influences both users' initial impressions and their overall satisfaction with the search results.} Specifically, the presence of source bias affects multiple retrieval metrics, including R@1, MeanR, and MedR. R@1 indicates the content users encounter first, MeanR measures overall retrieval performance, and MedR offers a more robust metric by minimizing the influence of outliers, though it is less sensitive to minor ranking changes. MixR provides a comprehensive assessment of the impact of source bias on retrieval performance.


\begin{table*}
    \belowrulesep=0pt
    \aboverulesep=0pt
    \vspace{-1.2em}
  \caption{The retrieval performance of different models is evaluated on the OpenSora ImageCond and OpenSora VideoExt datasets. For the definitions of $Relative\Delta$ and $Normalized\Delta$, please refer to Table \ref{tab:main-exp-text}.}
    \vspace{-0.8em}
  \label{tab:main-exp-picture}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \small
  % \rowcolors{2}{white}{gray!15}
  \resizebox{0.78\textwidth}{!}{%
  \begin{tabular}{ll|cccccc|cccccc}
  \toprule
    \multicolumn{2}{c|}{\makecell[l]{\hspace{1.5em}\textbf{Dataset}}}  & 
    \multicolumn{6}{c|}{\makecell[l]{\hspace{3em}\textbf{OpenSora ImageCond}}} &
    \multicolumn{6}{c}{\makecell[l]{\hspace{3em}\textbf{OpenSora VideoExt}}} 
    
    \\\midrule
     Model & Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
     \multirow{6}{*}{\makecell[c]{Alpro}} & REAL & 24.1 & 45.1 & 55.5 & 8 & 49.61 & - & 24.1 & 45.1 & 55.5 & 8 & 49.61 & -  \\
      & AI & 29.6 & 54.5 & 63.2 & 4 & 33.59 & - & 32.1 & 54.5 & 65.5 & 4 & 36.42 & -  \\
     & mixed-REAL & 8 & 33.7 & 43.5 & 15.5 & 94.31 & - & 8.7 & 33.2 & 41.8 & 17 & 95.90 & -  \\
     & mixed-AI & 22.4 & 45.4 & 55.5 & 7 & 70.33 & - & 23.7 & 47.2 & 57.6 & 7 & 75.38 & -  \\
     & $Relative\Delta$ & \textcolor{red}{-94.74} & \textcolor{red}{-29.58} & \textcolor{red}{-24.24} & \textcolor{red}{-75.56} & \textcolor{red}{-29.13} & \textcolor{red}{-66.48} & \textcolor{red}{-92.59} & \textcolor{red}{-34.83} & \textcolor{red}{-31.79} & \textcolor{red}{-83.33} & \textcolor{red}{-23.97} & \textcolor{red}{-66.63}  \\
     & $Normalized\Delta$ &
    \textcolor{red}{-74.26} & \textcolor{red}{-9.35} & \textcolor{red}{-5.36} & \textcolor{red}{-5.99} & 9.61 & \textcolor{red}{-23.55} & \textcolor{red}{-64.12} & \textcolor{red}{-9.13} & \textcolor{red}{-12.91} & \textcolor{red}{-13.76} & 6.87 & \textcolor{red}{-23.67}  \\\hline
    \multirow{6}{*}{\makecell[c]{Frozen}} & REAL & 22.9 & 43.2 & 53.6 & 8 & 49.81 & - & 22.9 & 43.2 & 53.6 & 8 & 49.811 & -  \\
     & AI & 25.7 & 50.6 & 62.6 & 5 & 37.93 & - & 28.3 & 51 & 60.1 & 5 & 37.34 & -  \\
     & mixed-REAL &
    9.1 & 31 & 42.3 & 18 & 94.78 & - & 8.3 & 29.3 & 39.2 & 21 & 104.51 & -  \\
     & mixed-AI &
    18.9 & 40.3 & 51.7 & 9 & 80.01 & - & 21.6 & 45.5 & 53.6 & 8 & 71.89 & -  \\
     & $Relative\Delta$ &
   \textcolor{red}{-70} & \textcolor{red}{-26.09} & \textcolor{red}{-20} & \textcolor{red}{-66.67} & \textcolor{red}{-16.9} & \textcolor{red}{-51.19} & \textcolor{red}{-88.96} & \textcolor{red}{-43.32} & \textcolor{red}{-31.03} & \textcolor{red}{-89.66} & \textcolor{red}{-36.99} & \textcolor{red}{-71.87}  \\ 
    & $Normalized\Delta$ &
    \textcolor{red}{-58.48} & \textcolor{red}{-13.34} & \textcolor{red}{-4.22} & \textcolor{red}{-18.67} & 10.34 & \textcolor{red}{-22.27} & \textcolor{red}{-67.87} & \textcolor{red}{-22.65} & \textcolor{red}{-14.47} & \textcolor{red}{-41.66} & \textcolor{red}{-8.21} & \textcolor{red}{-39.25}  \\ \hline
    \multirow{6}{*}{\makecell[c]{Intern\\Video}} & REAL & 40.6 & 66.7 & 75.2 & 2 & 22.27 & - & 40.6 & 66.7 & 75.2 & 2 & 22.27 & -  \\
     & AI & 42.7 & 70.2 & 78.9 & 2 & 18.62 & - & 46.6 & 71 & 78.6 & 2 & 17.62 & -  \\
     & mixed-REAL &
    29.1 & 52.5 & 61.9 & 4 & 83.65 & - & 28.2 & 53.6 & 62.8 & 4 & 75.72 & -  \\
     & mixed-AI &
    16.2 & 56.3 & 70.3 & 4 & 26.31 & - & 20.4 & 56.6 & 68.8 & 4 & 26.57 & -  \\
     & $Relative\Delta$ &
    56.95 & \textcolor{red}{-6.99} & \textcolor{red}{-12.71} & 0.00 & \textcolor{red}{-104.29} & \textcolor{red}{-15.78} & 32.1 & \textcolor{red}{-5.44} & \textcolor{red}{-9.12} & 0.00 & \textcolor{red}{-96.08} & \textcolor{red}{-21.33}  \\
     & $Normalized\Delta$ &
    61.99 & \textcolor{red}{-3.59} & \textcolor{red}{-7.6} & 0.00 & \textcolor{red}{-86.22} & \textcolor{red}{-8.08} & 45.86 & 2.8 & \textcolor{red}{-2.87} & 0.00 & \textcolor{red}{-72.5} & \textcolor{red}{-8.88}  \\ \bottomrule 
  \end{tabular}
  }
\vspace{-1em}
\end{table*}

\subsection{More Serious Bias Caused by Training}
\label{sec:3.3}
As AI-generated videos become more widely available on the Internet, they inevitably integrate into the training datasets of video retrieval models. Our findings show that when AI-generated videos are included in the training set, retrieval models tend to rank them higher, thereby amplifying source bias.

Specifically, to examine the impact of AI-generated videos on model training, we replace 20\% of the real videos in the MSR-VTT training set with their corresponding AI-generated counterparts, creating a mixed training set. This new set contains 1,800 AI-generated videos and 7,200 real videos, compared to the original training set, which consists entirely of real videos.

Experimental results shown in Figure~\ref{fig:mix-metric} indicate that fine-tuning retrieval models on the real video training set improves retrieval performance compared to the original model, while significantly reducing source bias. In terms of the $Normalized\Delta$ metrics, R@1 increases by 49.29, MeanR by 106.45, and MixR by 76.53. These improvements suggest that incorporating real data into the fine-tuning set effectively mitigates source bias.


% Fine-tuning with a mixed training set containing 20\% AI-generated videos also improves the model’s retrieval performance. However, compared to fine-tuning with only real videos, source bias increases significantly. The $Normalized\Delta$ metrics show a decrease of 89.52 in R@1, 54.47 in MeanR, and 79.74 in MixR. This suggests that while retrieval performance may not differ drastically, the model is more likely to retrieve AI-generated videos when they are included in the training set. Even a 20\% mix of AI-generated videos significantly impacts source bias.
Fine-tuning with a mixed training set comprising 20\% AI-generated videos also enhances the model’s retrieval performance. However, when compared to fine-tuning with only real videos, source bias increases substantially. The $Normalized\Delta$ metrics indicate a decrease of 89.52 in R@1, 54.47 in MeanR, and 79.74 in MixR. This suggests that, while retrieval performance may not exhibit substantial differences, the model becomes more likely to retrieve AI-generated videos as their proportion in the training set increases. Even a 20\% inclusion of AI-generated videos has a notable impact on source bias.

As depicted in Figure~\ref{fig:mix-metric}, the model's bias toward AI-generated videos becomes increasingly evident as the proportion of such videos in the training set rises from 20\% to 40\%, 60\%, and 80\%. Additionally, the Visual-Temporal Induced source bias intensifies with the growing proportion of AI-generated content.

\section{Causes of Video Souce Bias}

\label{sec:4}
In this section, we explore the causes of source bias in AI-generated videos. We identify that source bias stems from two key sources of information embedded in these videos: visual and temporal data. We investigate the origins of these biases using methods such as frame shuffling and single-frame retrieval.


\subsection{Visual Information Induces Source Bias}
\label{sec:4.1}
We find that both visual and temporal information play significant roles in generating source bias. This chapter focuses primarily on the source bias caused by visual information. Additionally, we observe that temporal information in real videos is more diverse and richer.
To separately assess the impact of each on source bias, we designed an experiment to modify the temporal information of the videos. 
In this experiment, we modify the temporal information of the videos by rearranging the frame order, effectively altering the temporal sequence without changing the visual content.
This approach allows us to examine the impact of temporal sequence on video retrieval bias while preserving visual integrity. We shuffle the frame order of both OpenSora TextCond videos and real videos, maintaining the original frame rates. Two datasets are created for experimentation using random and reverse frame orders. The experimental results are shown in Table \ref{tab:main-exp-random}. Results for the reverse dataset are similar to those of the random dataset but are not presented here due to space constraints.


We investigate two scenarios to evaluate the influnce of temporal information on retrieval bias: first, by shuffling only the frames of AI-generated videos, and second, by shuffling the frames of both real and AI-generated videos. The first scenario examines whether temporal information in AI-generated videos contributes to source bias while maintaining the visual content of real videos. The second scenario compares the impact of temporal sequence on retrieval performance while preserving the visual content of both video types. Our experiments indicate that shuffling frames of AI-generated videos alone leads to a reduction in retrieval accuracy, with $Normalized\Delta$ either decreasing or fluctuating. However, when frames from both real and AI-generated videos are shuffled, $Normalized\Delta$ decreases significantly. For instance, in the InternVideo model, $Normalized\Delta$ decreases by 29.63.

\begin{table*}
  \belowrulesep=0pt
  \aboverulesep=0pt
   \vspace{-1.2em}
   \caption{The retrieval performance of Text-Video Retrieval Models on the OpenSora TextCond dataset is assessed following the shuffling of video frame order. 'Random' denotes the shuffling of both real and AI-generated videos, while 'Random-only-AI' refers to shuffling only the AI-generated videos. For the definitions of $Relative\Delta$ and $Normalized\Delta$, please refer to Table~\ref{tab:main-exp-text}.}
    \vspace{-0.8em}
  \label{tab:main-exp-random}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \small
  % \rowcolors{2}{white}{gray!15}
  \resizebox{0.78\textwidth}{!}{%
  \begin{tabular}{ll|cccccc|cccccc}
  \toprule
    \multicolumn{2}{c|}{\makecell[l]{\hspace{1.5em}\textbf{Dataset}}}  & 
    \multicolumn{6}{c|}{\makecell[l]{\hspace{3em}\textbf{Random}}} &
    \multicolumn{6}{c}{\makecell[l]{\hspace{3em}\textbf{Random-only-AI}}} 
    
    \\\midrule
     Model &   Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
     \multirow{6}{*}{\makecell[c]{Alpro}} & REAL & 23.5 & 43.6 & 52.6 & 9 & 53.71 & - & 24.1 & 45.1 & 55.5 & 8 & 49.61 & - \\
      & AI & 37.0 & 59.5 & 68.5 & 3 & 28.56 & - & 37.0 & 59.5 & 68.5 & 3 & 28.56 & -  \\
     & mixed-REAL & 9.7 & 34.1 & 43.6 & 16 & 90.49 & - & 10.3 & 35.3 & 46.3 & 13 & 84.72 & - \\
     & mixed-AI & 25.8 & 49.2 & 56.9 & 6 & 71.15 & - & 25.9 & 48.9 & 57.4 & 6 & 69.77 & - \\
     & $Relative\Delta$ & \textcolor{red}{-90.7} & \textcolor{red}{-36.25} & \textcolor{red}{-26.47} & \textcolor{red}{-90.91} & \textcolor{red}{-23.93} & \textcolor{red}{-68.51} & \textcolor{red}{-86.19} & \textcolor{red}{-32.3} & \textcolor{red}{-21.41} & \textcolor{red}{-73.68} & \textcolor{red}{-19.35} & \textcolor{red}{-59.74} \\
     & $Normalized\Delta$ & \textcolor{red}{-46.07} & 2.1 & 4.37 & 13.44 & 37.58 & 1.65 & \textcolor{red}{-43.96} & 3.68 & 6.12 & 21.56 & 34.85 & 4.15 \\ \hline
     \multirow{6}{*}{\makecell[c]{Frozen}} & REAL & 20.5 & 41.9 & 51.8 & 9 & 56.19 & - & 22.9 & 43.2 & 53.6 & 8 & 49.81 & -  \\
     & AI & 30.7 & 54.9 & 65.4 & 4 & 32.57 & - & 30.7 & 54.9 & 65.4 & 4 & 32.57 & - \\
     & mixed-REAL &
    7.2 & 29.6 & 40.6 & 19 & 102.92 & - & 9.3 & 32.9 & 42.8 & 16 & 90.58 & - \\
     & mixed-AI &
    24.3 & 46 & 54.2 & 7 & 74.01 & - & 24 & 45.1 & 54.2 & 8 & 74.48 & -  \\
     & $Relative\Delta$ &
   \textcolor{red}{-108.57} & \textcolor{red}{-43.39} & \textcolor{red}{-28.69} & \textcolor{red}{-92.31} & \textcolor{red}{-32.69} & \textcolor{red}{-77.86} & \textcolor{red}{-88.29} & \textcolor{red}{-31.28} & \textcolor{red}{-23.51} & \textcolor{red}{-66.67} & \textcolor{red}{-19.51} & \textcolor{red}{-58.16}  \\ 
    & $Normalized\Delta$ &
    \textcolor{red}{-68.73} & \textcolor{red}{-11.52} & \textcolor{red}{-1.83} & \textcolor{red}{-12.31} & 20.83 & \textcolor{red}{-20.07} & \textcolor{red}{-59.19} & \textcolor{red}{-4.67} & 0.34 & 2.9 & 22.6 & \textcolor{red}{-11.23}  \\ \hline
    \multirow{6}{*}{\makecell[c]{Intern\\Video}} & REAL &
    40.6 & 66.7 & 75.2 & 2 & 22.27 & - & 40.6 & 66.7 & 75.2 & 2 & 22.27 & -  \\
     & AI & 47 & 69.9 & 77.9 & 2 & 18.04 & - & 47 & 69.9 & 77.9 & 2 & 18.04 & -  \\
     & mixed-REAL & 22.9 & 48.4 & 57.1 & 6 & 86.24 & - & 28.3 & 54.1 & 63 & 4 & 75.43 & -  \\
     & mixed-AI &
    28 & 61.1 & 71 & 3 & 25.37 & - & 20.8 & 56 & 68.4 & 4 & 27.36 & -  \\
     & $Relative\Delta$ &
    \textcolor{red}{-20.04} & \textcolor{red}{-23.2} & \textcolor{red}{-21.7} & \textcolor{red}{-66.67} & \textcolor{red}{-109.07} & \textcolor{red}{-65.26} & 30.55 & \textcolor{red}{-3.45} & \textcolor{red}{-8.22} & 0 & \textcolor{red}{-93.53} & \textcolor{red}{-20.99}  \\
     & $Normalized\Delta$ &
    \textcolor{red}{-2.95} & \textcolor{red}{-12.29} & \textcolor{red}{-14.59} & \textcolor{red}{-66.67} & \textcolor{red}{-78.14} & \textcolor{red}{-49.25} & 45.16 & 3.31 & \textcolor{red}{-3.53} & 0 & \textcolor{red}{-72.32} & \textcolor{red}{-9.05}  \\ \bottomrule 
  \end{tabular}
  }
\end{table*}


\begin{table}
  \belowrulesep=0pt
  \aboverulesep=0pt
  \caption{Model performance on single-frame retrieval using the OpenSora TextCond dataset.}
  \label{tab:singleframe}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \resizebox{0.42\textwidth}{!}{
  \begin{tabular}{ll|cccccc}
    \toprule
     Model & Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
     \multirow{6}{*}{\makecell[c]{Alpro}} & REAL & 32.2 & 52.2 & 61.5 & 5 & 49.74 & -\\
     & AI & 33.3 & 56.9 & 66.6 & 3 & 37.69 & -\\
     & mixed-REAL & 16.3 & 42.5 & 52.4 & 9 & 83.66 & -\\ 
     & mixed-AI & 22.4 & 47.4 & 57.1 & 6 & 96.41 & -\\ 
     & $Relative\Delta$ & \textcolor{red}{-31.52} & \textcolor{red}{-10.9} & \textcolor{red}{-8.58} & \textcolor{red}{-40} & 14.16 & \textcolor{red}{-19.12} \\
     & $Normalized\Delta$ &
    \textcolor{red}{-28.16} & \textcolor{red}{-3.35} & 0.04 & 13.33 & 41.89 & 9.02  \\ 
    \hline
    \multirow{6}{*}{\makecell[c]{Frozen}} & REAL & 34.7 & 56.4 & 65.4 & 4 & 42.16 & - \\
     & AI & 37.7 & 61.6 & 69.3 & 3 & 30.00 & - \\
     & mixed-REAL &
    16.8 & 45.6 & 55.4 & 7 & 70.95 & - \\
     & mixed-AI &
    26.3 & 51.6 & 60.6 & 5 & 77.42 & - \\
     & $Relative\Delta$ &
   \textcolor{red}{-44.08} & \textcolor{red}{-12.35} & \textcolor{red}{-8.97} & \textcolor{red}{-33.33} & 8.72 & \textcolor{red}{-22.9}  \\ 
    & $Normalized\Delta$ &
    \textcolor{red}{-35.79} & \textcolor{red}{-3.91} & \textcolor{red}{-0.16} & \textcolor{red}{-2.56} & 42.66 & 1.44 \\ 
    \hline
    \multirow{6}{*}{\makecell[c]{Intern\\Video}} & REAL & 32.2 & 52.2 & 61.5 & 5 & 49.74  & - \\
     & AI & 41.6 & 66 & 75.3 & 2 & 23.33 & - \\
     & mixed-REAL &
    15.5 & 44 & 53.2 & 9 & 83.14 & - \\
     & mixed-AI &
    28.1 & 54.3 & 63.8 & 4 & 60.12  & - \\
     & $Relative\Delta$ &
    \textcolor{red}{-57.8} & \textcolor{red}{-20.96} & \textcolor{red}{-18.12} & \textcolor{red}{-76.92} & \textcolor{red}{-32.13} & \textcolor{red}{-55.62}  \\
     & $Normalized\Delta$ &
    \textcolor{red}{-32.33} & 3.13 & 5.23 & 15.39 & 40.66 & 7.91   \\ 
    \bottomrule 
  \end{tabular}
  }
\end{table}

\subsection{Temporal Information Induces Source Bias}

\label{sec:4.2}


We find that the presence of temporal information causes AI-generated videos to appear at the top of the retrieval list, not just in the very first position. When video retrieval is performed using only a single frame, the video modality degenerates into a static image modality. In this case, the retrieval system relies solely on the visual information from that single frame, ignoring temporal sequence data. This setup allows us to analyze how the visual content of a video impacts retrieval results and potential biases in the absence of temporal and multi-frame information.


The experimental results, as shown in Table~\ref{tab:singleframe}, indicate that Visual-Temporal Induced Source Bias still exists, but is primarily observed in the R@1 metric. In the retrieval results of the three models, source bias remains evident in R@1, while it is almost nonexistent in MeanR and MedR. This suggests that when retrieving a single frame, the model’s bias is most pronounced in its preference for the first retrieved image. Overall, the combination of visual and temporal information contributes to the source bias phenomenon in text-video retrieval.


\section{Mitigating and Visualizing Bias}
\label{sec:5}
In this section, we propose a method using contrastive learning to mitigate this bias. By incorporating AI-generated videos into the contrastive learning training set, we fine-tune the model to increase the likelihood of retrieving real videos while reducing the likelihood of retrieving AI-generated ones. Additionally, we extract a debiasing vector from the model, which can be applied to other video encoding vectors to further reduce the Visual-Temporal Induced Source Bias in the retrieval system. This vector can also be used to visualize the bias.

\subsection{Debiased Model Training}
\label{sec:5.1}

We use the OpenSora TextCond (Train) dataset from Section~\ref{sec:2} to train the debiasing model with a contrastive learning approach.

\textbf{Notation:} A real video corresponds to both an AI-generated video and a caption. 
For a video-text pair, if the video is AI-generated, we represent it as $(V_G, C)$, and if it is a real video, we represent it as $(V_R, C)$. 
In retrieval, the model first samples $f$ frames from the video, represented as $I_j$ ($j \in [0, f-1]$). 
It then uses the pre-trained image encoder $E_I$ and text encoder $E_C$ to encode the images and text into vectors $h_{Ij}$ and $h_C$. 
These vectors are subsequently input into the video retrieval model $E_V$, which computes the final video-text similarity $r_{VC}$, expressed as:

\vspace{-1.2em}
\begin{align*}
    r_{VC}=E_V([h_{I0}&...h_{If-1}],h_{C },\theta_{VC}),\\
    h_{Ij} = E_I(Ij,\theta_I&)\ \ \ h_{C} = E_C(C,\theta_C).
\vspace{-0.4em}
\end{align*}

\textbf{Loss Construction:} The optimization objective is as follows. Let $y$ represent the label: when $y = 1$, the video corresponds to the text, and when $y = 0$, they do not correspond. $\mathcal{L}$ denotes the loss function, which minimizes the distance between the image and its corresponding text embedding vectors:


\vspace{-1.2em}
\begin{align*}
\theta_{VC} = arg\ min_\theta\ \mathcal{L}(r_{VC},y,\theta_{VC}).
\vspace{-0.4em}
\end{align*}

During the training of the debiased model, we aim to enable the retrieval model to more easily retrieve the corresponding real videos based on the text, while avoiding AI-generated videos. For each text, we have a real video-AI-generated video-text triplet $(V_R, V_G, C_i)$. After image sampling, we use the image encoder and text encoder to encode them into vectors $h_{Rf}$, $h_{Gf}$, and $h_{Ci}$, respectively. A contrastive loss function is introduced in the debiased model:


\vspace{-1.2em}
\begin{align*}
    \Delta r(G,R,C)=& E_V([h_{G0}...h_{Gf-1}],h,\theta_{VC})
                    \\&-E_V([h_{R0}...h_{Rf-1}],h,\theta_{VC}),
\vspace{-0.4em}
\end{align*}
where $\Delta r$ measures the difference in scores assigned by the model between two videos, providing a comprehensive way to assess the consistent invisible bias of the model toward real and AI-generated videos. This helps guide the model in reducing bias against generated videos. Additionally, when $\Delta r < 0$, we do not apply this loss function, ensuring that during contrastive learning, the model still favors generated videos while increasing the likelihood of retrieving real videos. The overall training objective is:

\vspace{-1.2em}
\begin{align*}
     \theta_{VC} = arg\ min_\theta\ \mathcal{L}(r_{VC},y,\theta_{VC}) + \Delta r(G,R,C).
\end{align*}
The model training results, as shown in Table~\ref{tab:debias}, indicate that the debiased model is more likely to rank real videos at the top of the list, significantly reducing the model’s source bias.

\begin{table}
  \belowrulesep=0pt
  \aboverulesep=0pt
  \vspace{-1.2em}
  \caption{The performance of the InternVideo model after debiasing fine-tuning using contrastive learning. Fine-tuning is performed on the OpenSora TextCond dataset, and testing is conducted on three different datasets.}
    \vspace{-0.8em}
  \label{tab:debias}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \small
  \resizebox{0.45\textwidth}{!}{
  \begin{tabular}{ll|cccccc}
  \toprule
    \multicolumn{8}{c}{\cellcolor{white}\makecell[l]{\hspace{5em}\textbf{InternVideo Contrastive-Debias}}}
    \\\midrule
     Dataset &   Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
    \multirow{6}{*}{\makecell[c]{OpenSora\\TextCond}} & REAL & 41.2 & 66.5 & 76 & 2 & 20.436 & - \\
     & AI & 41.5 & 64.5 & 73.9 & 2 & 23.662 & - \\
     & mixed-REAL & 41.2 & 66.5 & 76 & 2 & 23.534 & - \\ 
     & mixed-AI & 0 & 0.2 & 0.5 & 224 & 293.688 & - \\ 
     & $Relative\Delta$ & 200 & 198.8 & 197.39 & 196.46 & 170.32 & 188.93 \\
     & $Normalized\Delta$ & 200.73 & 195.28 & 194.34 & 196.46 & 155.52 & 184.24 \\
    \hline
    \multirow{6}{*}{\makecell[c]{OpenSora\\ImageCond}} & REAL & 41.2 & 66.5 & 76 & 2 & 20.436 & - \\
     & AI & 35.3 & 63.1 & 74.2 & 3 & 22 & - \\
     & mixed-REAL & 41.2 & 66.5 & 76 & 2 & 23.727 & - \\ 
     & mixed-AI & 0 & 0.3 & 0.4 & 247 & 304.562 & - \\ 
     & $Relative\Delta$ & 200 & 198.2 & 197.91 & 196.79 & 171.09 & 189.29 \\
     & $Normalized\Delta$ & 184.58 & 189.5 & 192.66 & 152.35 & 163.63 & 166.85 \\
    \hline
    \multirow{6}{*}{\makecell[c]{CogVideoX\\TextCond}} & REAL & 41.2 & 66.5 & 76 & 2 & 20.436 & - \\
     & AI & 37.1 & 61.4 & 70.9 & 3 & 24.279 & - \\
     & mixed-REAL & 41.2 & 66.5 & 76 & 2 & 24.13 & - \\ 
     & mixed-AI & 0 & 2.9 & 7.5 & 99.5 & 183.991 & - \\ 
     & $Relative\Delta$ & 200 & 183.29 & 164.07 & 192.12 & 153.62 & 181.91 \\
     & $Normalized\Delta$ & 189.53 & 173.32 & 156.1 & 147.68 & 136.24 & 157.82\\ 
    \bottomrule 
  \end{tabular}
  }
\vspace{-1em}
\end{table}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{picture/pvector.pdf}
\vspace{-1em}
\caption{Changes in retrieval metrics of the model ($\Delta$) after altering the vector representations of videos using p-vectors and random p-vectors.}
\label{fig:p}
\vspace{-1em}
\end{figure}


\subsection{Visual and Temporal Vectors Visualization}
\label{sec:5.2}
In this section, we use the debiased model from the previous section to analyze the source of bias and visualize the invisible source bias. Given that the retrieval model exhibits a general preference for AI-generated videos, which is reversed by the debiased model, we analyze the video embeddings after debiasing to explore this reversal.

\textbf{Notation:} After obtaining the debiased model, the debiased video encoder is denoted as $E_v^d$. We can obtain the original video embedding $h_v = [h_1, h_2, \dots, h_n]$, the debiased video embedding $h_v^d = [h_1^d, h_2^d, \dots, h_n^d]$, and the text embedding $C = [c_1, c_2, \dots, c_n]$.

\textbf{Visualization Bias:} We define a vector $p$ to represent the difference between the debiased video embedding and the original video embedding, capturing the shift in the video representation after debiasing:
\begin{align*}
     p =  [p_1,p_2,...,p_n]
       =  [h_{1}^{d}-h_1,h_{2}^{d}-h_2,...,h_{n}^{d}-h_n].
\end{align*}

After performing t-SNE visualization on the vectors $p$, $h_v$, and $h_v^d$, as shown in Figure~\ref{fig:TSNE}, we observe that the vector $p$ forms a distinct clustering pattern. 
This indicates that the AI video generation model embeds additional, consistent information across generated videos. 
We identify these extra details as the direct cause of the Visual-Temporal Induced Source Bias. 
Three key observations emerged: 
(1) These extra details are generalizable, and incorporating them into real videos can increase their retrieval probability and reduce bias.
(2) Some of this additional information encodes temporal aspects, with certain details integrated into the generated videos through temporal sequences. 
(3) The extra information exhibits a high degree of consistency, as all AI-generated videos share a concentrated embedding of this additional information.


\begin{table}
\belowrulesep=0pt
  \aboverulesep=0pt
    \vspace{-1.2em}
    \caption{The effect of adding the extracted $p$ vector to the original video representation on retrieval performance: $\Delta > 0$ indicates that adding the $p$ vector makes the retrieval model more likely to rank real videos higher, while $\Delta < 0$ suggests that the model tends to prioritize AI-generated videos.}
    \vspace{-0.8em}
  \label{tab:p-debias}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \small
  % \rowcolors{2}{white}{gray!15}
  \resizebox{0.45\textwidth}{!}{%
  \begin{tabular}{l|cccccc}
  \toprule
    \multicolumn{7}{c}{\cellcolor{white}\makecell[l]{\hspace{5em}\textbf{InternVideo p-Debias}}}
    \\\midrule
     Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
    $REAL$ & -20.48 & -17.32 & -15.04 & -22.23 & -39.9 & -23.49 \\
    $REAL_{p-debias}$ & 17.12 & 7.54 & 1.21 & 39.4 & -26.0 & 10.17 \\
    $\Delta$ & 37.60 & 24.86 & 16.25 & 61.63 & 13.90 & 33.66 \\\bottomrule 
  \end{tabular}
  }
\end{table}

\begin{table}
\belowrulesep=0pt
  \aboverulesep=0pt
    \vspace{-1.2em}
    \caption{The effect of adding extracted $p_{random}$ vector to the original video representation on retrieval performance: $\Delta > 0$ indicates that adding  $p_{random}$ vector makes the retrieval model more likely to rank real videos higher, while $\Delta < 0$ suggests that model tends to prioritize AI-generated videos.}
    \vspace{-0.8em}
  \label{tab:visual-debias}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \small
  % \rowcolors{2}{white}{gray!15}
  \resizebox{0.45\textwidth}{!}{%
  \begin{tabular}{l|cccccc}
  \toprule
    \multicolumn{7}{c}{\cellcolor{white}\makecell[l]{\hspace{5em}\textbf{InternVideo p-random-Debias}}}
    \\\midrule
    Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
    $REAL$ & -51.36 & -12.0 & -7.06 & -35.56 & 0.42 & -28.83 \\
    $REAL_{p-debias}$ & -41.14 & -1.9 & -5.29 & -4.19 & 2.04 & -14.43 \\
    $\Delta$ & 10.22 & 10.1 & 1.77 &  35.56 & -1.62 & 14.40 \\\bottomrule 
  \end{tabular}
  }
\vspace{-1em}
\end{table}

\begin{figure}
\centering
\begin{subfigure}{0.23\textwidth}
        \includegraphics[width=\linewidth]{picture/TSNE.pdf}
        \caption*{(a) Original Temporal Information t-SNE Visualization}
        \label{fig:TSNE1}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\linewidth]{picture/TSNE_time.pdf}
    \caption*{(b) Scramble Temporal Information t-SNE Visualization}
    \label{fig:TSNE2}
\end{subfigure}
\hfill
\caption{t-SNE visualization of image representations and
transformations vector.}
\label{fig:TSNE}
\vspace{-1em}
\end{figure}

For the first point, we design an experiment to investigate the effect of the debiasing model. Since the model adds a vector $p_i = h_{R}^d - h_R$ to each generated video $V_{Gi}$, we simplify the analysis by using the average of all $p_i$ vectors, denoted as $p_{avg}$, to represent the additional information introduced. Using the original video encoder, we obtain the raw representation of the real video $h_{R} = [h_{R1}, h_{R2}, \dots, h_{Rn}]$, and then add the computed $p_{avg}$ to each video representation, resulting in the biased real video representation $h_{R}^{pd} = [h_{R1} + p_{avg}, h_{R2} + p_{avg}, \dots, h_{Rn} + p_{avg}]$. The experimental results, as shown in Table~\ref{tab:p-debias}, demonstrate that inputting the new real video representation into the retrieval model leads to a reduction in bias across all evaluation metrics, particularly in the R@1 metric, where the bias is even reversed. This finding suggests that the additional information is universal; it can be applied not only to AI-generated videos but also directly to real videos, thereby enhancing their retrieval performance. This confirms that the vector $p$ plays a crucial role in introducing Visual-Temporal Induced Source Bias.


For the second point, we investigate whether the additional information includes temporal data. We input the Random OpenSora TextCond dataset into the model, generating and extracting the $p_{random}$ vector, which contains scrambled temporal information. This $p_{random}$ vector preserves the original visual content while introducing disrupted temporal sequences. When this vector is added to the real video representation, the experimental results, as shown in Table~\ref{tab:visual-debias}, indicate a reduction in bias across the evaluation metrics. However, the effect is weaker compared to when real temporal information is included. Figure~\ref{fig:p} suggests that temporal information in AI-generated videos plays a significant role in Visual-Temporal Induced Source Bias, with additional information encoded within the temporal sequences.


For the third point, we visualize the vectors $p$, $h_G$, $h_R$, $h_{G}^d$, and $h_{R}^d$, along with the corresponding vectors from the random dataset, in two dimensions. The results are presented in Figure~\ref{fig:TSNE}. From the t-SNE plot, we observe that, compared to the other vectors, both the $p$ and $p_{random}$ vectors display clustering patterns. This suggests that the $p$ and $p_{random}$ vectors extracted from different videos are highly similar, and the visual and temporal additional information in AI-generated videos shows significant clustering.


\section{Related Work}
\label{sec:6}
In this section, we summarize related work on bias in information retrieval and AI-generated content detection.
\subsection{Bias in Information Retrieval}
\label{sec:6.1}

Bias in information retrieval has attracted significant attention. \cite{mowshowitz2002assessing} first introduced the concept of bias, defining it, analyzing its sources, and proposing methods for evaluating bias, such as comparing search engine performance. Later studies focused on real-time methods for measuring bias in web search engines~\cite{mowshowitz2005measuring}. Research has since progressed in three main areas: analyzing bias sources in specific domains, exploring methods for assessing and mitigating bias, and investigating retrieval bias induced by AIGC-generated content.

% Research on assessing and mitigating bias has revealed a significant negative correlation between retrieval bias and performance~\cite{wilkie2014retrievability}. Subsequent studies addressed fairness across various systems and proposed mitigation strategies~\cite{yao2017new, yang2017measuring, geyik2019fairness}. Efforts to quantify bias in search engines, social networks, and recommendation services, along with fairness in collaborative filtering systems, were also explored~\cite{pitoura2018measuring}. Additional work focused on tackling fairness challenges in search and information retrieval systems~\cite{gao2021addressing, gao2021toward}.

Regarding methods for assessing and mitigating bias, research has explored the relationship between retrieval bias and performance, revealing a significant negative correlation~\cite{wilkie2014retrievability}. Later studies expanded on fairness in various systems and proposed mitigation strategies~\cite{yao2017new, yang2017measuring, geyik2019fairness}. Fairness in collaborative filtering recommendation systems and ranked outputs are examined, with efforts to quantify bias in search engines, social networks, and recommendation services~\cite{pitoura2018measuring}. Other studies concentrated on addressing fairness challenges in search and retrieval systems~\cite{gao2021addressing, gao2021toward}.

The rise of AIGC has introduced new challenges in retrieval bias. Research has explored the bias introduced by large language models (LLMs) in retrieval systems, revealing that neural retrieval models tend to prioritize AIGC-generated documents, a phenomenon known as source bias~\cite{dai2023llms}. Studies also show that objects in images generated by large vision-language models (LVLMs) exhibit more hallucination features compared to natural images~\cite{gao2024aigcs}. Additionally, it has been highlighted that synthetic images can introduce biases, with strategies proposed to mitigate these effects~\cite{xu2024invisible}. While prior research has not addressed source bias in video generation, this work validates its existence, identifies its origins in visual and temporal factors, and proposes solutions to mitigate the bias.

\subsection{AI-generated Content Detection}

% In recent years, researchers have extensively studied detecting AI-generated content. Existing methods for detecting AI-generated text can be broadly categorized into three approaches: Watermarking methods, which embed identifiable markers into AI-generated content to verify its origin~\cite{topkara2006hiding,ueoka2021frustratingly,gu2022watermarking,liu2023private,liu2024adaptive}; statistical methods, which use metrics like entropy to distinguish AI-generated text from human-written text~\cite{vasilatos2023howkgpt,mitchell2023detectgpt,su2023detectllm}; and supervised learning methods, which train deep classifiers to differentiate between AI-generated and human-written content.

Current methods for detecting AI-generated images are primarily classified into two categories: GAN-based detection methods, which focus on identifying artifacts unique to GAN-generated images~\cite{wang2020cnn,liu2020global}, and generalizable detection methods for diffusion models, which aim to identify a broader range of AI-generated images~\cite{ma2023exposing,luo2024lare,wu2023generalizable,epstein2023online,wang2023dire,corvi2023detection}. In contrast, the detection of AI-generated videos remains relatively underexplored. Existing methods include a motion discrepancy-based approach to distinguish AI-generated fake videos from real ones~\cite{fei2021exposing}, the use of a 3D convolutional network to analyze appearance, motion, and geometry for video differentiation~\cite{chang2024matters}, and H.264 re-compression to detect synthetic videos~\cite{Vahdati_2024_CVPR}. This work demonstrates that AI-generated videos contain additional visual and temporal information embedded by video generation models, which can be exploited to detect such videos. Furthermore, these studies indirectly support the existence of this additional information, as identified in our research.

\section{Conclusion}

This study investigates the impact of AI-generated videos on text-video retrieval. We construct a comprehensive retrieval scenario that includes both real and AI-generated videos and conduct experiments based on this benchmark. The results show that AI-generated videos are preferentially retrieved by the model, appearing at the top of the retrieval list. As the proportion of AI-generated videos in the training set increases, the source bias becomes more pronounced. 
We analyze the reasons for the source bias. It not only mainly originates from the visual information in AI-generated videos but also from temporal information.
%We also analyze the sources of this bias, identifying that it primarily arises from the visual information in AI-generated videos, with contribution from temporal information. This temporal aspect causes AI-generated videos to rank higher overall. 
Finally, we employ a contrastive learning-based debiasing approach to alleviate the source bias and find that the additional information encoded by the generative model contributes to this bias.
The findings highlight the potential impact of AI-generated videos on text-video retrieval and offer valuable insights for future research.

% \newpage

\balance
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\end{CJK}
\end{document} %% 新增
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
