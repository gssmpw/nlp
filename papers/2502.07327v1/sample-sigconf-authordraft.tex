%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
\documentclass[sigconf,review,screen,natbib=true,anonymous=true]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\usepackage{color}
\usepackage{xspace}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{CJKutf8} %% 新增
\usepackage{multicol} %% 新增
\usepackage{multirow} %% 新增
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amssymb}

\begin{document}
\begin{CJK}{UTF8}{gbsn} %% 新增
%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{The Ghost in the Machine: Uncovering Source Bias in AI-Generated Videos}
\title{Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated Videos}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

With the rapid development of AI-generated content (AIGC), the creation of high-quality AI-generated videos has become faster and easier, resulting in the Internet being flooded with all kinds of video content. However, the impact of these videos on the content ecosystem remains largely unexplored. Video information retrieval remains a fundamental approach for accessing video content. Building on the observation that retrieval models often favor AI-generated content in ad-hoc and image retrieval tasks, we investigate whether similar biases emerge in the context of challenging video retrieval, where temporal and visual factors may further influence model behavior. To explore this, we first construct a comprehensive benchmark dataset containing both real and AI-generated videos, along with a set of fair and rigorous metrics to assess bias. This benchmark consists of 13,000 videos generated by two state-of-the-art open-source video generation models. We meticulously design a suite of rigorous metrics to accurately measure this preference, accounting for potential biases arising from the limited frame rate and suboptimal quality of AIGC videos. We then applied three off-the-shelf video retrieval models to perform retrieval tasks on this hybrid dataset. Our findings reveal a clear preference for AI-generated videos in retrieval. Further investigation shows that incorporating AI-generated videos into the training set of retrieval models exacerbates this bias. Unlike the preference observed in image modalities, we find that video retrieval bias arises from both unseen visual and temporal information, making the root causes of video bias a complex interplay of these two factors. To mitigate this bias, we fine-tune the retrieval models using a contrastive learning approach. The results of this study highlight the potential implications of AI-generated videos on retrieval systems and offer valuable insights for future research in this area. Our dataset and code are publicly available at \url{https://xxx.github.com}.


% With the rapid development of AI-generated content (AIGC), the creation of high-quality AI-generated videos has become faster and easier, resulting in the Internet being flooded with all kinds of video content.
% However, the impact of these videos on the content ecosystem remains largely unexplored. 
% Information retrieval remains a fundamental method for accessing information. 
% Drawing inspiration from the observation that retrieval models may favor AI-generated content in ad-hoc and image retrieval, we explore whether this also applies to video retrieval.
% To explore this, we first construct a comprehensive benchmark dataset containing both real and AI-generated videos, along with a set of fair and rigorous metrics to assess bias.
% This benchmark consists of 13,000 videos generated by two state-of-the-art open-source video generation models. 
% We meticulously design a suite of rigorous metrics to accurately assess source bias, accounting for potential biases arising from the limited frame rate and suboptimal quality of AIGC videos.
% We then applied three off-the-shelf video retrieval models to perform retrieval tasks on this hybrid dataset. 
% Our findings reveal a clear source bias in video retrieval.
% Further investigation shows that incorporating AI-generated videos into the training set of retrieval models exacerbates this bias. 
% Unlike the source bias observed in image modalities, we find that video source bias arises from both unseen visual and temporal information, making the root causes of video bias a complex interplay of these two factors. 
% To mitigate video source bias, we fine-tune the retrieval models using a contrastive learning approach. 
% The results of this study highlight the potential implications of AI-generated videos on retrieval systems and offer valuable insights for future research in this area. 
% Our dataset and code are publicly available at https://xxx.github.com.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317</concept_id>
<concept_desc>Information systems~Information retrieval</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Information retrieval}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Text-Video Retrieval, AIGC, Bias and Fairness}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
%\newpage
\section{Introduction}

\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{picture/sigir-video-bias.pdf}
\caption{The Visual-Temporal Relevance Bias identified in this paper. AI-generated videos are produced by video generation models \cite{yang2024cogvideox,opensora} and mixed with real videos on the Internet. When performing text-video retrieval, the retrieval model \cite{bain2021frozen,li2022align,wang2022internvideo} tends to prioritize AI-generated videos in the search results. This bias arises from the additional visual and temporal information embedded in the videos by the generation model.}
\label{fig:overall}
\end{figure}

In the contemporary digital era, video content stands out among various media formats due to its unique dynamism and vividness, emerging as the preferred medium for information dissemination and entertainment~\cite{goggin2010global,tarchi2021learning}. 
Information retrieval, particularly in the video domain, acts as the vital entry point for users navigating this vast content ecosystem.
As artificial intelligence (AI) rapidly evolves, the production of AI-generated videos has become significantly easier and faster~\cite{wu2023ai,foo2023ai,xing2024survey,opensora}, leading to a surge of this content type online. 
This influx of AI-generated videos raises a critical question: How will video retrieval models handle these AI-generated videos?

%As artificial intelligence (AI) technologies rapidly evolve in the video domain, they significantly lower production costs and expedite the pace of video creation~\cite{wu2023ai,foo2023ai,xing2024survey,opensora}.
%AI-generated video content is anticipated to surge on the Internet, profoundly impacting the video content ecosystem. 
Similar questions have also been proposed in the textual and image content ecosystem. 
Previous studies have found that in both text and image domains, retrieval models prioritize AI-generated content, a phenomenon known as "source bias"~\cite{dai2023llms,xu2024invisible}.
However, the video modality presents unique challenges, making bias assessment more complex.
First, generating AI-generated videos that are semantically similar to real ones is particularly difficult due to the resource-intensive and time-consuming nature of video creation. This issue is further compounded by the limitations of open-source models, which often fail to produce satisfactory results.
Second, assessing bias in video retrieval models requires a more nuanced approach. It is necessary to incorporate multidimensional metrics across the retrieval list to capture various biases. Additionally, the impact of semantic discrepancies between videos must be minimized to avoid skewing evaluation results. Specifically, it is crucial to ensure that the retrieval model does not favor AI-generated or real videos simply due to semantic proximity to the textual description.
Lastly, pinpointing the sources of bias is more challenging in video than in text or image domains. Videos contain not only rich visual information but also unique temporal elements, adding complexity to bias analysis.
% In the textual domain, Dai \textit{et al}. \cite{dai2023llms} find that neural retrieval models tend to prioritize texts generated by large models in search results, a phenomenon known as "source bias". 
% Similarly, in the image domain, Xu \textit{et al}. \cite{xu2024invisible} identified a similar invisible relevance bias, where image retrieval models rank images generated by large models at the top of search results. 
%Based on these trends, we speculate that text-video retrieval models, which play a significant role in the video content ecosystem, will also be significantly affected. 
%As video libraries increasingly contain AI-generated content, a crucial question emerges: How will video retrieval models handle these AI-generated videos?
%Compared to other modalities, video modality presents its unique challenges. 
%First, generating AI-generated videos that are semantically similar to real videos is particularly difficult. 
%This is due to the resource-intensive and time-consuming nature of video creation, compounded by the fact that open-source models often fail to deliver satisfactory results. 
%Second, a more nuanced approach is required when assessing bias in video retrieval models. This involves incorporating multidimensional metrics across the retrieval list to capture various biases. 
%It is equally important to minimize the impact of semantic discrepancies between videos on evaluation results. 
%It is essential to ensure that the retrieval model is not biased toward AI-generated (or real) videos simply because their semantics are closer to the textual description.
% ensuring that the results reflect true model performance rather than semantic differences. 
%Lastly, pinpointing the sources of bias is more challenging in video than in text or image. 
% Videos not only contain rich visual information but also carry unique temporal elements, adding layers of complexity that make bias analysis more difficult.
%Videos not only contain rich visual information but also carry unique temporal elements, introducing additional complexity that complicates bias analysis.

To address the three challenges mentioned above and evaluate the impact of introducing AI-generated videos on retrieval models, we focus on creating an unbiased benchmark. 
In the absence of a standardized benchmark for video retrieval, our primary goal is to develop a framework specifically designed to examine potential biases in video retrieval models when dealing with both real and AI-generated videos.
% In the absence of a standardized benchmark for video retrieval, our primary focus is on developing an unbiased benchmark designed to investigate potential biases in video retrieval models when handling both real and AI-generated videos.
Such a benchmark requires that video generation techniques reflect real-world scenarios and that retrieval models do not exhibit preferences for either AI-generated or real videos based solely on semantic similarity to the query (See \textbf{\color{red}{\S\ref{sec:2}}}). 
Our dataset consists of 13,000 videos, including 9,000 training videos and four test sets, each containing 1,000 videos. 
To construct this dataset, we utilize two state-of-the-art video generation models: CogVideoX \cite{yang2024cogvideox} and OpenSora V1.2 \cite{opensora}. 
These models generate videos collaboratively by integrating text, real video frames, or clips, all generated based on the MSR-VTT dataset \cite{xu2016msr}.

% To achieve this, we employ two state-of-the-art video generation models: CogVideoX \cite{yang2024cogvideox} and OpenSora V1.2 \cite{opensora}, and integrate text, real video frames, or clips for collaborative video generation.
% Our dataset comprises 13,000 videos, including 9,000 training videos and four test sets of 1,000 videos each, all of which are generated based on the MSR-VTT dataset \cite{xu2016msr}.

After a statistical analysis of video parameters and similarities, we find this setup well-suited for studying biases in video retrieval models.
To further minimize the impact of semantic similarity between videos and their corresponding queries on retrieval ranking, we introduce a novel evaluation metric, $Normalized\Delta$.
Additionally, to offer a more comprehensive assessment of multi-dimensional retrieval performance, we propose the MixR metric. 
Unlike previous evaluation measures that focus solely on top-ranked retrieval (R@1), MixR combines MeanR and MedR with R@1 to capture the broader impact of the entire retrieval list.

% In the absence of a standardized benchmark for video retrieval, our primary focus is on developing an unbiased benchmark designed to investigate potential biases in video retrieval models when handling both real and AI-generated videos.
% This benchmark ensures that video generation techniques reflect real-world scenarios and that retrieval models do not exhibit preferences for either AI-generated or real videos based solely on semantic similarity to the query.
% Furthermore, it mitigates the risk of excluding videos with lower semantic similarity, thus ensuring a more accurate retrieval process. (\textbf{\color{red}{\S\ref{sec:2}}})
% To achieve this, we use two state-of-the-art video generation models: CogVideoX \cite{yang2024cogvideox} and OpenSora V1.2 \cite{opensora}.
% These models leverage text-video generation techniques while incorporating real video frames or clips.
% Our dataset comprises 9,000 videos, divided into four test sets, each containing 1,000 videos. All videos are generated based on the MSR-VTT dataset \cite{xu2016msr}.
% After conducting a comprehensive statistical analysis of video parameters and similarities, we found this experimental setup to be well-suited for studying biases in video retrieval models.
% To further minimize the influence of semantic similarity between videos and their corresponding queries on retrieval ranking, we introduce a novel evaluation metric, $Normalized\Delta$. 
% Additionally, to provide a more comprehensive assessment of multi-dimensional retrieval performance, we propose the MixR metric. 
% Unlike previous evaluation measures that focus solely on the top-ranked retrieval (R@1), MixR integrates Mean Rank and Median Rank with R@1 to capture the broader impact of the entire retrieval list.

Experimental results from our constructed benchmark reveal an intriguing phenomenon: text-video retrieval models tend to prioritize AI-generated videos over real videos. 
Specifically, these models exhibit a tendency to rank AI-generated videos at or near the top of the retrieval list. (See \textbf{\color{red}{\S\ref{sec:3.2}}}) 
As AI-generated videos become more prevalent on the Internet, they are likely to be integrated into the training datasets of future retrieval models. 
Our findings indicate that as the proportion of AI-generated videos in the training set increases, retrieval models progressively favor AI-generated content, exhibiting a growing bias toward prioritizing it (See \textbf{\color{red}{\S\ref{sec:3.3}}}).

Videos differ significantly from other modalities. 
In text and image modalities, source bias typically arises from the additional information provided by a single type of text or visual data in generation models.
However, the amount of information in video modalities is several times greater than in text and images, encompassing not only visual information but also temporal information. 
Therefore, we explore the root cause of video source bias by disrupting the temporal information through randomizing the frame order (See \textbf{\color{red}{\S\ref{sec:4.1}}}) and capturing the visual information by extracting single-frame images (See \textbf{\color{red}{\S\ref{sec:4.2}}}). 
Experiments domonstrate that the additional information embedded in both the visual and temporal components of generated videos plays a key role in generating bias, which we term \textbf{Visual-Temporal Relevance Bias}. 
Specifically, real videos contain richer temporal information, whereas generated videos, lacking sufficient temporal depth, primarily rely on single-frame changes. 
This lack of depth contributes to the formation of source bias. 
Moreover, compared to retrieval tasks in other modalities, video retrieval exhibits a stronger preference for the first retrieved video and shows a general tendency to favor AI-generated videos in the retrieval results.
% Moreover, compared to other modalities, video retrieval not only shows a stronger preference for the first retrieved video but also demonstrates a general inclination toward AI-generated videos in the retrieval list.

% Experimental results from our constructed benchmark reveal an intriguing phenomenon: text-video retrieval models tend to prioritize AI-generated videos over real videos. 
% Specifically, these models exhibit a tendency to rank AI-generated videos at the top or near the top of the retrieval list. (\textbf{\color{red}{\S\ref{sec:3.3}}}) 
% As AI-generated videos become more prevalent on the Internet, they are likely to be integrated into the training datasets of future retrieval models. 
% Our findings indicate that as the proportion of AI-generated videos in the training set increases, retrieval models progressively favor AI-generated content, exhibiting a growing bias toward prioritizing it (\textbf{\color{red}{\S\ref{sec:3.4}}}).

% Videos differ significantly from other modalities. In text and image modalities, source bias typically arises from the additional information present in generation models. 
% However, the amount of information in video modalities is several times greater than in text and images, encompassing not only visual information but also temporal information. 
% Therefore, we explored the root cause of video source bias by disrupting the temporal information through randomizing the frame order (\textbf{\color{red}{\S\ref{sec:4.1}}}) and capturing the visual information by extracting single-frame images (\textbf{\color{red}{\S\ref{sec:4.2}}}). 
% Through experiments, we found that the additional information embedded in both the visual and temporal components of generated videos plays a key role in generating bias. 
% We term this bias Visual-Temporal Relevance Bias. Specifically, our findings are as follows:

% \begin{enumerate}

%     \item Real videos contain richer temporal information, while generated videos lack sufficient temporal depth, relying primarily on single-frame changes. When the frame order of both real and AI-generated videos was disrupted, the bias phenomenon intensified significantly, and the retrieval model showed a stronger preference for AI-generated videos.
    
%     \item Temporal information in generated videos also contributes to the formation of source bias. When only the frame order of generated videos was disrupted, while the frame order of real videos remained intact, the bias phenomenon was somewhat mitigated and the retrieval model no longer showed such a strong preference for AI-generated videos.
    
%     \item Compared to other modalities, video retrieval exhibits not only a stronger preference for the first retrieved video but also an overall inclination toward AI-generated videos in the retrieval list. When videos were reduced to single-frame images, the bias was not reflected in the MeanR and MedR metrics but was apparent in the R@1 metric.

% \end{enumerate}

To mitigate Visual-Temporal Relevance Bias in retrieval models, we applied a contrastive learning approach \cite{chen2020simple} to fine-tune the models, incorporating AI-generated videos into the training set (see \textbf{\color{red}{\S\ref{sec:5.1}}}).
Through fine-tuning, we train the model to prioritize real videos over AI-generated ones, placing real videos at the top of the retrieval list and ensuring they appear before AI-generated videos in the overall ranking, which effectively reduces Visual-Temporal Relevance Bias.
By quantifying the differences in vector representations between the debiased and original models, we use t-SNE \cite{van2008visualizing} to visualize the Visual-Temporal Relevance Bias in the videos (See \textbf{\color{red}{\S\ref{sec:5.2}}}).

% To mitigate Visual-Temporal Relevance Bias in retrieval models, we employed a contrastive learning approach \cite{chen2020simple}, incorporating generated videos into the training set. (\textbf{\color{red}{\S\ref{sec:5.1}}}) Through fine-tuning, we trained the model to prioritize real videos over AI-generated ones, placing real videos at the top of the retrieval list and ensuring they appear before AI-generated videos in the overall ranking. This approach effectively reduces Visual-Temporal Relevance Bias. By quantifying the differences in vector representations between the debiased and original models, we identified the root cause of the bias and further illustrated this with T-SNE visualization. (\textbf{\color{red}{\S\ref{sec:5.2}}})

Our contributions include:
\begin{enumerate}
    \item We construct a benchmark that includes both real and AI-generated videos to investigate the impact of AI-generated content on video retrieval models. 
    % \item We investigated the impact of AI-generated content on video retrieval models, constructing a reasonable benchmark that includes both real and AI-generated videos for research. We found that AI-generated videos introduce Visual-Temporal Relevance Bias, causing retrieval models to rank AI-generated videos higher, even though they do not contain more information than real videos. This was observed across multiple structurally different video-text retrieval models and various video generation methods.
    \item We reveal that AI-generated videos introduce Visual-Temporal Relevance Bias, which stems from the additional visual and temporal information embedded by video generation encoders, leading retrieval models to rank them higher.
    % \item We explored the source of Visual-Temporal Relevance Bias in video modalities and discovered that it originates from the additional information embedded in videos by video generation encoders. This extra information is present not only in the visual content but also in the temporal aspects. Visual information is the primary factor contributing to this phenomenon, while temporal information mainly causes an overall bias in the retrieval sequence.
    % \item We proposed a debiasing method for video retrieval models that can effectively reduce the Visual-Temporal Relevance Bias towards AI-generated videos. This method does not alter the original structure of the retrieval model, thereby improving retrieval effectiveness.
    \item We propose a debiasing method for video retrieval models that effectively reduces Visual-Temporal Relevance Bias towards AI-generated videos.
\end{enumerate}

\section{Benchmark Construction}
\label{sec:2}
In this section, we pioneeringly construct a benchmark to evaluate the impact of AI-generated videos on text-video retrieval models. The construction of this benchmark involves three stages: real retrieval dataset selection, semantically-equivalent video generation, and quality evaluation. Additionally, the benchmark should meet three key requirements to ensure the reliability of the research outcomes:
% In this section, we pioneeringly construct a benchmark to evaluate the impact of AI-generated videos on text-video retrieval models. The construction of this benchmark involves three stages: selection of a real retrieval dataset, generation of semantically-equivalent videos, and evaluation of the benchmark dataset's quality. Additionally, the benchmark must meet three key requirements to ensure the reliability of the research outcomes:

\textbf{(1) Semantic Similarity:} Ensuring that generated videos possess similar semantic features to real videos. This helps prevent abnormal retrieval rankings caused by excessive similarity between real or AI-generated videos and the query.

% Bias in retrieval evaluation is mitigated by ensuring that generated videos possess similar semantic features to real videos. This helps prevent abnormal retrieval rankings caused by excessive similarity between real or AI-generated videos and the query.

\textbf{(2) Realistic Generation:} Video generation methods should align with real-world applications, such as generating videos directly from text or combining text with images.

\textbf{(3) One-to-one Mapping:} The generated and real videos in the dataset should have a one-to-one mapping, enabling direct comparison during evaluation. This ensures fairness and eliminates potential biases arising from discrepancies in quantity.
%（1） The dataset selection reflects the diverse range of video types within the real content ecosystem. This enhances the practical value of the evaluation by more comprehensively including the content that users encounter.

%（4） Video generation methods align with those commonly used in real-world applications. For example, generating videos directly from text enhances the similarity between the videos in benchmark datasets and the AI-generated videos commonly encountered in real-world scenarios.


% The number of generated videos matches that of the real videos, enabling direct comparison during performance evaluation. This ensures fairness and eliminates potential biases arising from quantity discrepancies.

\begin{figure*}[h]
\centering

\begin{subfigure}{0.17\textwidth}
    \centering
    \includegraphics[width=\linewidth]{picture/cogvideox_sim_list.pdf}
    \caption*{\scriptsize(a) CogVideoX TextCond}
    \label{fig:REALR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.17\textwidth}
    \centering
    \includegraphics[width=\linewidth]{picture/opensora_fusion_sim_list.pdf}
    \caption*{\scriptsize(b) OpenSora TextCond}
    \label{fig:AIR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.17\textwidth}
    \centering
    \includegraphics[width=\linewidth]{picture/opensora_image_condition_sim_list.pdf}
    \caption*{\scriptsize(c) OpenSora ImageCond}
    \label{fig:mixRealR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.17\textwidth}
    \centering
    \includegraphics[width=\linewidth]{picture/opensora_video_extending_sim_list.pdf}
    \caption*{\scriptsize(d) OpenSora VideoExt}
    \label{fig:mixAIR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.17\textwidth}
    \centering
    \includegraphics[width=\linewidth]{picture/opensora_fusion_train_sim_list.pdf}
    \caption*{\scriptsize(e) OpenSora TextCond(train)}
    \label{fig:relativedeltaR@k}
\end{subfigure}
\vspace{-1em}
\caption{Using the CLIP model to compute the similarity between AI-generated video datasets and real video datasets (X-axis), and analyzing the distribution of similarities frequency (Y-axis).}
\vspace{-1em}
\label{fig:benchmark}
\end{figure*}





\subsection{Real Retrival Dataset Selection}
\label{sec:2.1}



To select an appropriate real retrieval dataset, we analyze six well-known text-video retrieval datasets, including MSR-VTT \cite{xu2016msr}, MSVD \cite{chen2011collecting}, DiDeMo \cite{anne2017localizing}, ActivityNet \cite{caba2015activitynet}, WebVid \cite{jin2021webvid}, and LSMDC \cite{rohrbach2015dataset}. The selected datasets should meet the following three core criteria. 
Firstly, they should encompass a diverse range of real videos, enriching the variety of video types encountered by users. Otherwise, the reliability of the experiment cannot be guaranteed, e.g., LSMDC.
Secondly, the captions for videos should cover the entire video, not just segments, and as many captions as possible should be provided. Without comprehensive annotations, accurate video generation cannot be achieved to minimize semantic biases between AI-generated and real videos, e.g., MSVD, DiDeMo, and ActivityNet.
% Secondly, accurate video generation requires comprehensive annotations.  to avoid semantic biases between AI-generated and real videos due to incomplete annotations. 
Last but not least, the videos in the dataset should be video raw data, rather than pre-extracted feature vectors. If violated, it will not be possible to generate videos for experiments, e.g., WebVid.
Consequently, we select MSR-VTT, a widely recognized, large-scale dataset comprising 10,000 videos across 20 categories, with each video annotated by 20 English captions. The training set contains 9,000 videos, while the test set includes 1,000 videos. The dataset split follows the same partitioning as in Bain \textit{et al.} \cite{bain2021frozen}.
% To select an appropriate real retrieval dataset, we analyze six well-known text-video retrieval datasets, including MSR-VTT, MSVD \cite{chen2011collecting}, DiDeMo \cite{anne2017localizing}, ActivityNet \cite{caba2015activitynet}, WebVid \cite{jin2021webvid}, and LSMDC \cite{rohrbach2015dataset}.  The selection of datasets must meet the following four core criteria. Firstly, the selection of datasets contains a diverse range of real videos, which enriches types of videos that users encounter, thus enhancing the practical value of the evaluation. Secondly, accurate video generation requires comprehensive annotations. Each video’s description must thoroughly reflect its core content and key information to avoid semantic biases between AI-generated and real videos caused by incomplete annotations. Furthermore, the videos in the dataset are in a playable video format, rather than pre-extracted feature vectors. Lastly, generated and real videos in datasets have one-to-one mapping, which enables direct comparison during evaluation. This ensures fairness and eliminates potential biases arising from quantity discrepancies.

% \begin{table}
% \belowrulesep=0pt
%   \aboverulesep=0pt
%   \caption{The reason for selecting the MSR-VTT dataset: $\textcolor{red}{\times}$ indicates non-compliance with the respective criteria. We ultimately chose MSR-VTT, as the other five datasets failed to meet certain standards.}
%     \vspace{-0.8em}
%   \label{tab:dataset_selection}
%   \centering
%   \setlength{\tabcolsep}{0.8mm}
%   \renewcommand{\arraystretch}{1.3}
%   % \small
%   % \rowcolors{2}{white}{gray!15}
%   \resizebox{0.45\textwidth}{!}{%
%   \begin{tabular}{lcccccc}
%   \toprule
%      \makecell{Dataset} & \makecell{MSR-VTT} & \makecell{MSVD} & \makecell{DiDeMo} & \makecell{ActivityNet} & \makecell{WebVid} & \makecell{LSMDC} \\
%     \midrule
%     Scenario Diversity & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & $\textcolor{red}{\times}$\\
%     Annotation Completeness & \checkmark & $\textcolor{red}{\times}$ & $\textcolor{red}{\times}$ & $\textcolor{red}{\times}$ & \checkmark & \checkmark \\
%     Playable format & \checkmark & \checkmark & \checkmark & \checkmark & $\textcolor{red}{\times}$ & \checkmark \\
%     % One-to-one Mapping & \checkmark & \checkmark & \checkmark& \checkmark & \checkmark & $\textcolor{red}{\times}$ \\
%     \bottomrule
%   \end{tabular}
%   }
%   \vspace{-1em}
% \end{table}

% Overall, MSR-VTT is the optimal choice for video-text retrieval research, thanks to its diversity, accuracy, comprehensive content, and clear evaluation standards.

\subsection{Semantic-similar Video Generation}
\label{sec:2.2}
To generate semantically similar AI-generated videos for the MSR-VTT dataset, we need to select appropriate video generation models and strategies. 
We select CogVideoX \cite{yang2024cogvideox} and OpenSora V1.2 \cite{opensora} as the video generation models, as they are widely recognized for their effectiveness and popularity among open-source options, making them suitable for broadly assessing the presence of source bias.
% We choose CogVideoX \cite{yang2024cogvideox} and OpenSora V1.2 \cite{opensora} as the video generation models. These two models are the most popular, effective, and widely used among open-source video generation models, capable of broadly measuring the presence of source bias. 
Given the current difficulty of generating AI videos that are semantically highly similar to real videos, we employ multiple methods to ensure comprehensive experimental results. CogVideoX produces high-quality generated content, while OpenSora is characterized by fast generation and a variety of generation methods, such as combining images and videos. Therefore, we applied different generation strategies to the two models, resulting in distinct datasets. For video generation strategies, we use the following three methods:

% CogVideoX is selected for its exceptional performance in metrics such as Dynamic Degree, Multiple Objects, and Scene. OpenSora V1.2 is favored for its diverse video generation methods, rapid generation speed, and low computational resource requirements. 

\textbf{(1) Text-condition (TextCond):} To ensure that the generated video semantics are similar to real videos and align with video generation methods used in the real world, we employ a text-video generation approach. This requires obtaining a comprehensive textual description for each real video. For every real video, we use GPT-4 to integrate its 20 captions by inputting them with the prompt: "I will provide 20 captions of the same video. Please assist in merging them into a comprehensive description." The resulting response is referred to as the fusion caption, which summarizes the multiple descriptions of the video and represents the full content of the real video. Next, we input the fusion caption into the generation models, CogVideoX and OpenSora, and obtain the \textcolor{red}{CogVideoX TextCond} and \textcolor{red}{OpenSora TextCond} (test and train) datasets.

\textbf{(2) Image-condition (ImageCond):} To generate video content that more accurately aligns with the visual semantics of the real video, we adopt a text-image integration approach that incorporates multimodal information. Specifically, we select a keyframe from the real video at the 20\% timestamp, along with the fusion caption, and input them into OpenSora, resulting in the creation of the \textcolor{red}{OpenSora ImageCond} dataset. Due to the high generation cost, this dataset includes only the test set.

\textbf{(3) Video-extending (VideoExt):} To capture the full content of a real video, we employ a text-video integration approach for video generation. This method enhances the model's understanding, thereby improving the likelihood of generating videos that visually and semantically resemble the real video. We select the first half of the real video and input it, along with the fusion caption, into OpenSora, creating the \textcolor{red}{OpenSora VideoExt} dataset. Due to the high generation cost, this dataset includes only the test set.

\subsection{Quality Evaluation}
\label{sec:2.3}
% After constructing a dataset of AI-generated videos, it is crucial to validate the reliability of the benchmark. We compile key parameters
To validate the reliability of the constructed benchmark, we compile key properties of the videos and assess their similarity. The dataset’s key parameters are shown in Table \ref{tab:benchmark}. For the video similarity assessment, we uniformly select ten frames from each generated video and its corresponding real video. Using the CLIP model \cite{radford2021learning}, we compute the average video representation and calculate the cosine similarity between the real and generated videos.

This ensures that the AI-generated videos in the test set exhibit high semantic similarity to the real videos, with an average similarity exceeding 0.72, while the OpenSora ImageCond dataset achieves a similarity of 0.87. 
Additionally, the consistency between the training and corresponding test sets is maintained. 
In the OpenSora TextCond dataset, the similarity between the training and test sets is nearly identical, with an average difference of only 0.0028.


\begin{table}
\belowrulesep=0pt
  \aboverulesep=0pt
    \vspace{-1.2em}
  \caption{AI-generated video Datasets Overview.}
    \vspace{-0.8em}
  \label{tab:benchmark}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{1.25}
  \large
  % \rowcolors{2}{white}{gray!15}
  \resizebox{0.45\textwidth}{!}{%
  \begin{tabular}{lccccc}
  \toprule
    \makecell{Dataset} & \makecell{Number} & \makecell{FPS} & \makecell{Duration} & \makecell{Resolution} & \makecell{Similarity} \\
    \midrule
    \small CogVideoX TextCond & 1000 & 8 & 6.12 & 720×480 & 0.7232 \\
    \small OpenSora TextCond & 1000 & 24 & 4.25 & 640×360 & 0.7304 \\
    \small OpenSora ImageCond & 1000 & 24 & 4.25 & 640×360 & 0.8725 \\ 
    \small OpenSora VideoExt & 1000 & 12 & 8.50 & 424×240 & 0.7745 \\
    \small OpenSora TextCond(train) & 9000 & 24 & 4.25 & 640×360 & 0.7332
   \\ \bottomrule 
  \end{tabular}
  }
  \vspace{-1em}
\end{table}



\section{Video Source Bias Assessment}
\label{sec:3}

In this section, we use the benchmark we have constructed to evaluate the impact of incorporating AI-generated videos into the video library on retrieval performance. We select three retrieval models and proposed new evaluation metrics. Through experiments that mixed AI-generated videos into both the test and training sets, we find that the retrieval models tend to rank AI-generated videos higher in the retrieval results.



\subsection{Retrieval Models and Evaluation Metrics}
\label{sec:3.1}
In this section, we build the retrieval models and evaluation metrics required for the experiments. We select Frozen in Time \cite{bain2021frozen}, ALPRO \cite{li2022align}, and InternVideo \cite{wang2022internvideo} as video retrieval models, and propose suitable metrics for evaluating source bias.

% $Normalized\Delta$ and MixR 
\noindent \textbf{Video Retrieval Models:} We select three distinct open-source video-text retrieval models with strong zero-shot retrieval capabilities to explore the source bias of AI-generated videos, including:
% All these models exhibit strong zero-shot retrival capabilities, including:

\textbf{(1) Frozen in Time}: This model uses joint video and image encoders for end-to-end retrieval. 
It employs a Space-Time Transformer Encoder, which processes both image and video data flexibly, treating images as "frozen" snapshots of videos during training.

\textbf{(2) ALPRO}: By sparsely sampling video frames, ALPRO achieves effective cross-modal alignment without explicit object detectors. 
It introduces a Video-Text Contrastive Loss for aligning video and text features, simplifying cross-modal interaction modeling. 
Additionally, ALPRO features a Prompting Entity Modeling task for fine-grained alignment of visual regions and textual entities via self-supervision.

\textbf{(3) InternVideo}: This model combines generative and discriminative self-supervised learning strategies to optimize video representations. 
It uses Masked Video Modeling and Video-Language Contrastive Learning as pretraining tasks, with a learnable coordination mechanism to integrate both types of video representations.

\noindent \textbf{Notation:} Formally, for a single dataset, $REAL$ and $AI$ refer to retrieval from the real and AI-generated video datasets, respectively. For a mixed dataset, $mixed-REAL$ and $mixed-AI$ represent retrieval tasks for real and AI-generated videos. $Metric$ includes $R@k$, MeanR, and MedR. Specifically, $R@k$ measures the proportion of relevant items retrieved within the top-k results, MeanR calculates the average rank of relevant items, and MedR represents the median rank of relevant items across all queries. $Rank$ denotes the retrieval rank of a relevant item in a single query.

\noindent \textbf{Evaluation Metrics Calculation:} To provide a more fair and comprehensive evaluation of source bias, we propose MixR metric, which integrates the assessment of both the top-ranked retrieval and the entire retrieval list. 
Additionally, we introduce $Normalized\Delta$ to mitigate the influence of video semantic discrepancies.

To quantify source bias during retrieval, we introduce the $Relative\Delta$ metric, based on \cite{xu2024invisible}, which evaluates the impact of AI-generated videos on the ranking of real videos in mixed retrieval scenarios. 
When the metric is $R@k$, the calculation formula is as follows:
$$
Relative\Delta=\frac{2(Metric_{mixed-REAL}-Metric_{mixed-AI})}{(Metric_{mixed-REAL}+Metric_{mixed-AI})}\times 100\%
$$
When the $Metric$ is MeanR or $Med$, the calculation formula becomes:
$$
Relative\Delta=\frac{2(Metric_{mixed-AI}-Metric_{mixed-REAL})}{(Metric_{mixed-REAL}+Metric_{mixed-AI})}\times 100\%
$$

Given the high cost and instability of current video generation models, the generated video quality is often inconsistent, which may lead to retrieval models mistakenly favoring certain videos during Source Bias evaluation. 
Methods are needed to mitigate the differences between $Metric_{REAL}$ and $Metric_{AI}$, reducing the impact of measurement bias. 
To address this, we propose novel metric, $Normalized\Delta$ and $Localtion\Delta$. 
$Location\Delta$ estimates the expected retrieval score by analyzing the ranking positions of real and AI-generated videos when retrieved separately, while disregarding semantic differences between the datasets. 
$Normalized\Delta$ combines $Relative\Delta$ and $Location\Delta$. 
Compared to directly using $Relative\Delta$, $Normalized\Delta$ better mitigates the influence of semantic discrepancies, offering a more accurate bias assessment. 

\begin{table*}[ht]
    \belowrulesep=0pt
    \aboverulesep=0pt
    \caption{The retrieval performance of different models is evaluated on CogVideoX TextCond and OpenSora TextCond. When $Relative\Delta>0$ or $Normalized\Delta>0$, it indicates that the retrieval model tends to rank real videos higher. Conversely, \textcolor{red}{$Relative\Delta<0$} or \textcolor{red}{$Normalized\Delta<0$} suggests that the model tends to rank AI-generated videos higher. The absolute values of these metrics reflect the magnitude of the bias. $Normalized\Delta$ incorporates a penalty term to the original $Relative\Delta$, offering a more accurate measure of the bias.}
  \label{tab:main-exp-text}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \small
  % \rowcolors{2}{white}{gray!15}
  \resizebox{0.8\textwidth}{!}{%
  \begin{tabular}{ll|cccccc|cccccc}
  \toprule
    \multicolumn{2}{c|}{\makecell[l]{\hspace{1.5em}\textbf{Dataset}}}  & 
    \multicolumn{6}{c|}{\makecell[l]{\hspace{3em}\textbf{CogVideoX TextCond}}} &
    \multicolumn{6}{c}{\makecell[l]{\hspace{3em}\textbf{OpenSora TextCond}}}
    \\\midrule
     Model &   Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
     \multirow{6}{*}{\makecell[c]{Alpro}}& REAL & 24.10  & 45.10  & 55.50  & 8.00  & 49.61 & - & 24.10  & 45.10  & 55.50  & 8.00  & 49.61 & -  \\
      & AI & 30.50  & 51.70  & 61.90  & 5.00  & 40.14 & - & 37.00  & 59.30  & 68.90  & 3.00  & 27.72 & -  \\
     & mixed-REAL & 10.10  & 34.60  & 45.50  & 14.00  & 82.94  & -  & 10.80  & 35.40  & 46.80  & 13.50  & 83.72  & -  \\ 
     & mixed-AI & 22.60  & 42.70  & 50.70  & 10.00  & 101.16  & -  & 24.50  & 49.50  & 56.10  & 6.00  & 69.39  & -  \\ 
     & $Relative\Delta$ & \textcolor{red}{-76.45}  & \textcolor{red}{-20.96}  & \textcolor{red}{-10.81}  & \textcolor{red}{-33.33}  & 19.80  & \textcolor{red}{-29.99}  & \textcolor{red}{-77.62}  & \textcolor{red}{-33.22}  & \textcolor{red}{-18.08}  & \textcolor{red}{-76.92}  & \textcolor{red}{-18.71}  & \textcolor{red}{-57.75}  \\ 
    & $Normalized\Delta$ & \textcolor{red}{-53.01}  & \textcolor{red}{-2.59}  & 2.83  & 14.67  & 41.02  & 0.89  & \textcolor{red}{-35.39}  & 3.05  & 9.12  & 18.32  & 38.26  & 7.06  \\ \hline
     \multirow{6}{*}{\makecell[c]{Frozen}} & REAL & 22.90  & 43.20  & 53.60  & 8.00  & 49.81 & - & 22.90  & 43.20  & 53.60  & 8.00  & 49.81 & -  \\
     & AI & 29.80  & 50.60  & 60.80  & 5.00  & 39.98 & - & 31.50  & 54.70  & 64.30  & 4.00  & 31.56 & -  \\
     & mixed-REAL &
    6.90  & 28.20  & 39.10  & 20.00  & 92.25  & -  & 8.90  & 31.40  & 41.40  & 17.00  & 90.35  & -  \\ 
     & mixed-AI &
    23.80  & 45.20  & 53.00  & 8.00  & 90.98  & -  & 25.50  & 46.80  & 55.40  & 7.00  & 72.41  & -  \\ 
     & $Relative\Delta$ &
    \textcolor{red}{-110.10}  & \textcolor{red}{-46.32}  & \textcolor{red}{-30.18}  & \textcolor{red}{-85.71}  & \textcolor{red}{-1.39}  & \textcolor{red}{-65.73}  & \textcolor{red}{-96.51}  & \textcolor{red}{-39.39}  & \textcolor{red}{-28.93}  & \textcolor{red}{-83.33}  & \textcolor{red}{-22.05}  & \textcolor{red}{-67.30}  \\ \multirow{-6}{*} & $Normalized\Delta$ &
    \textcolor{red}{-83.91}  & \textcolor{red}{-23.51}  & \textcolor{red}{-14.40}  & \textcolor{red}{-37.71}  & 20.63  & \textcolor{red}{-33.66}  & \textcolor{red}{-64.89}  & \textcolor{red}{-10.89}  & \textcolor{red}{-5.44}  & \textcolor{red}{-13.76}  & 23.08  & \textcolor{red}{-18.52}  \\ \hline
     \multirow{6}{*}{\makecell[c]{Intern\\Video}} & REAL & 40.60  & 66.70  & 75.20  & 2.00  & 22.27 & - & 40.60  & 66.70  & 75.20  & 2.00  & 22.27 & -  \\
     & AI & 40.20  & 64.00  & 73.40  & 2.00  & 25.30 & - & 47.20  & 71.50  & 78.40  & 2.00  & 17.85 & -  \\
     & mixed-REAL &
    19.60  & 52.30  & 63.50  & 5.00  & 43.39  & -  & 27.40  & 53.10  & 62.20  & 5.00  & 74.16  & -  \\ 
     & mixed-AI &
    27.60  & 56.10  & 64.90  & 4.00  & 56.31  & -  & 22.50  & 58.20  & 68.90  & 4.00  & 26.87  & -  \\ 
     & $Relative\Delta$ &
    \textcolor{red}{-33.90}  & \textcolor{red}{-7.01}  & \textcolor{red}{-2.18}  & \textcolor{red}{-22.22}  & 25.92  & \textcolor{red}{-10.07}  & 19.64  & \textcolor{red}{-9.16}  & \textcolor{red}{-10.22}  & \textcolor{red}{-22.22}  & \textcolor{red}{-93.61}  & \textcolor{red}{-32.06}  \\ 
     & $Normalized\Delta$ &
    \textcolor{red}{-34.89}  & \textcolor{red}{-11.17}  & \textcolor{red}{-6.31}  & \textcolor{red}{-22.22}  & 13.06  & \textcolor{red}{-14.68}  & 34.67  & \textcolor{red}{-1.66}  & \textcolor{red}{-3.27}  & \textcolor{red}{-22.22}  & \textcolor{red}{-71.32}  & \textcolor{red}{-19.62} 
    \\ \bottomrule 
  \end{tabular}
  }
   % \vspace{-0em}
\end{table*}


First, we record the ranking of each video when retrieved independently from the real and AI-generated datasets. 
Since the datasets are not yet mixed, the rankings do not account for cross-dataset visual-semantic differences. 
Next, we interpolate the rankings by combining the rank lists of $REAL$ and $AI$. 
The mixed rankings are calculated as follows, $c$ is a random number of 0 or 1:
\vspace{-0.5em}
\begin{align*}
Rank_{mixed-REAL}=&\ 2*Rank_{REAL}-c\\
Rank_{mixed-AI}=2\ *\ &Rank_{AI}-(1-c)
\vspace{-0.55em}
\end{align*}

We use the $Rank$ to calculate $Metric_{mixed-REAL}^L$ and $Metric_{mixed-AI}^L$ ,and then compute $Locational\Delta$:
$$
Locational\Delta=\frac{2(Metric_{mixed-REAL}^L-Metric_{mixed-AI}^L)}{(Metric_{mixed-REAL}^L+Metric_{mixed-AI}^L)} \times 100\%
$$
While $Relative\Delta$ reflects the actual retrieval model performance, $Locational\Delta$ accounts for performance without considering semantic differences between real and AI-generated videos. 
The difference between these metrics represents the Visual-Temporal Relevance Bias in the mixed retrieval model, termed $Normalized\Delta$:
\vspace{-0.45em}
$$
Normalized\Delta = Relative\Delta - Locational\Delta
\vspace{-0.45em}
$$

When $Normalized\Delta > 0$, the model favors real videos over AI-generated ones. 
When $Normalized\Delta < 0$, the model favors AI-generated videos. 
Compared to directly using $Relative\Delta$, $Normalized\Delta$ better mitigates the influence of semantic discrepancies, offering a more accurate bias assessment.

Additionally,MixR combines R@1, MedR, and MeanR as follows. 
With $\Delta$ representing $Relative\Delta$, $Locational\Delta$ or $Normalized\Delta$, the calculation method is as follows:
\vspace{-0.45em}
% $$
% \Delta MixR = \frac{\Delta R@1+\Delta MedR+\Delta MeanR}{3}
% \vspace{-0.45em}
% $$
$$
\Delta MixR = (\Delta R@1+\Delta MedR+\Delta MeanR) \div 3
\vspace{-0.45em}
$$





\subsection{Visual-Temporal Relevance Bias}
\label{sec:3.2}
Experiments shown in Table \ref{tab:main-exp-text} and \ref{tab:main-exp-picture} demonstrate that source bias is prevalent across various video retrieval models. Specifically, retrieval models tend to rank AI-generated videos higher than real videos, showing a clear preference for AI-generated content. Our key findings are as follows:

(1) \textbf{Source bias is not specific to a particular video generation or retrieval model, but rather a widespread phenomenon.} As shown in Table \ref{tab:main-exp-text}, espite differences in the pretraining datasets and video generation models used, both $Relative\Delta$ and $Normalized\Delta$ values are generally negative. 
This suggests that vision-language models, pre-trained on large video-text and image-text datasets, tend to rank AI-generated videos higher.

(2)\textbf{Incorporating real video or image segments into generated videos amplifies the visual-temporal relevance bias.} As shown in Table \ref{tab:main-exp-picture}, we repeat the experiment on the OpenSora ImageCond and OpenSora VideoExt datasets, where more real video content is integrated into the generated videos. We find that the gap in retrieval metrics decreases further during individual retrieval, while $Relative\Delta$ and $Normalized\Delta$ values increase.

(3) \textbf{When AI-generated videos are included in a video library, source bias significantly influences both users' initial impressions and overall satisfaction with the search results.} Specifically, the presence of source bias affects multiple retrieval metrics, including R@1, MeanR, and MedR.
R@1 reflects the content users see first, MeanR measures overall retrieval performance, and MedR provides a more robust metric by reducing the influence of outliers, though it is less sensitive to minor ranking changes. 
MixR provides a comprehensive reflection of the impact of source bias on retrieval performance.
\begin{table*}
    \belowrulesep=0pt
    \aboverulesep=0pt
    \vspace{-1.2em}
  \caption{The retrieval performance of different models is evaluated on OpenSora ImageCond and OpenSora VideoExt. For the definitions of $Relative\Delta$ and $Normalized\Delta$, please refer to Table \ref{tab:main-exp-text}.}
  % When $Relative\Delta>0$ or $Normalized\Delta>0$, it indicates that the retrieval model tends to rank real videos higher. Conversely, \textcolor{red}{$Relative\Delta<0$} or \textcolor{red}{$Normalized\Delta<0$} suggests that the model tends to rank AI-generated videos higher. The absolute values of these metrics reflect the magnitude of the bias. $Normalized\Delta$ incorporates a penalty term to the original $Relative\Delta$, offering a more accurate measure of the bias.
    \vspace{-0.8em}
  \label{tab:main-exp-picture}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \small
  % \rowcolors{2}{white}{gray!15}
  \resizebox{0.8\textwidth}{!}{%
  \begin{tabular}{ll|cccccc|cccccc}
  \toprule
    \multicolumn{2}{c|}{\makecell[l]{\hspace{1.5em}\textbf{Dataset}}}  & 
    \multicolumn{6}{c|}{\makecell[l]{\hspace{3em}\textbf{OpenSora ImageCond}}} &
    \multicolumn{6}{c}{\makecell[l]{\hspace{3em}\textbf{OpenSora VideoExt}}} 
    
    \\\midrule
     Model & Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
     \multirow{6}{*}{\makecell[c]{Alpro}} & REAL & 24.1 & 45.1 & 55.5 & 8 & 49.61 & - & 24.1 & 45.1 & 55.5 & 8 & 49.61 & -  \\
      & AI & 29.6 & 54.5 & 63.2 & 4 & 33.59 & - & 32.1 & 54.5 & 65.5 & 4 & 36.42 & -  \\
     & mixed-REAL & 8 & 33.7 & 43.5 & 15.5 & 94.31 & - & 8.7 & 33.2 & 41.8 & 17 & 95.90 & -  \\
     & mixed-AI & 22.4 & 45.4 & 55.5 & 7 & 70.33 & - & 23.7 & 47.2 & 57.6 & 7 & 75.38 & -  \\
     & $Relative\Delta$ & \textcolor{red}{-94.74} & \textcolor{red}{-29.58} & \textcolor{red}{-24.24} & \textcolor{red}{-75.56} & \textcolor{red}{-29.13} & \textcolor{red}{-66.48} & \textcolor{red}{-92.59} & \textcolor{red}{-34.83} & \textcolor{red}{-31.79} & \textcolor{red}{-83.33} & \textcolor{red}{-23.97} & \textcolor{red}{-66.63}  \\
     & $Normalized\Delta$ &
    \textcolor{red}{-74.26} & \textcolor{red}{-9.35} & \textcolor{red}{-5.36} & \textcolor{red}{-5.99} & 9.61 & \textcolor{red}{-23.55} & \textcolor{red}{-64.12} & \textcolor{red}{-9.13} & \textcolor{red}{-12.91} & \textcolor{red}{-13.76} & 6.87 & \textcolor{red}{-23.67}  \\\hline
    \multirow{6}{*}{\makecell[c]{Frozen}} & REAL & 22.9 & 43.2 & 53.6 & 8 & 49.81 & - & 22.9 & 43.2 & 53.6 & 8 & 49.811 & -  \\
     & AI & 25.7 & 50.6 & 62.6 & 5 & 37.93 & - & 28.3 & 51 & 60.1 & 5 & 37.34 & -  \\
     & mixed-REAL &
    9.1 & 31 & 42.3 & 18 & 94.78 & - & 8.3 & 29.3 & 39.2 & 21 & 104.51 & -  \\
     & mixed-AI &
    18.9 & 40.3 & 51.7 & 9 & 80.01 & - & 21.6 & 45.5 & 53.6 & 8 & 71.89 & -  \\
     & $Relative\Delta$ &
   \textcolor{red}{-70} & \textcolor{red}{-26.09} & \textcolor{red}{-20} & \textcolor{red}{-66.67} & \textcolor{red}{-16.9} & \textcolor{red}{-51.19} & \textcolor{red}{-88.96} & \textcolor{red}{-43.32} & \textcolor{red}{-31.03} & \textcolor{red}{-89.66} & \textcolor{red}{-36.99} & \textcolor{red}{-71.87}  \\ 
    & $Normalized\Delta$ &
    \textcolor{red}{-58.48} & \textcolor{red}{-13.34} & \textcolor{red}{-4.22} & \textcolor{red}{-18.67} & 10.34 & \textcolor{red}{-22.27} & \textcolor{red}{-67.87} & \textcolor{red}{-22.65} & \textcolor{red}{-14.47} & \textcolor{red}{-41.66} & \textcolor{red}{-8.21} & \textcolor{red}{-39.25}  \\ \hline
    \multirow{6}{*}{\makecell[c]{Intern\\Video}} & REAL & 40.6 & 66.7 & 75.2 & 2 & 22.27 & - & 40.6 & 66.7 & 75.2 & 2 & 22.27 & -  \\
     & AI & 42.7 & 70.2 & 78.9 & 2 & 18.62 & - & 46.6 & 71 & 78.6 & 2 & 17.62 & -  \\
     & mixed-REAL &
    29.1 & 52.5 & 61.9 & 4 & 83.65 & - & 28.2 & 53.6 & 62.8 & 4 & 75.72 & -  \\
     & mixed-AI &
    16.2 & 56.3 & 70.3 & 4 & 26.31 & - & 20.4 & 56.6 & 68.8 & 4 & 26.57 & -  \\
     & $Relative\Delta$ &
    56.95 & \textcolor{red}{-6.99} & \textcolor{red}{-12.71} & 0.00 & \textcolor{red}{-104.29} & \textcolor{red}{-15.78} & 32.1 & \textcolor{red}{-5.44} & \textcolor{red}{-9.12} & 0.00 & \textcolor{red}{-96.08} & \textcolor{red}{-21.33}  \\
     & $Normalized\Delta$ &
    61.99 & \textcolor{red}{-3.59} & \textcolor{red}{-7.6} & 0.00 & \textcolor{red}{-86.22} & \textcolor{red}{-8.08} & 45.86 & 2.8 & \textcolor{red}{-2.87} & 0.00 & \textcolor{red}{-72.5} & \textcolor{red}{-8.88}  \\ \bottomrule 
  \end{tabular}
  }
\end{table*}

\begin{figure*}[ht]
\centering
\begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\linewidth]{picture/Real_R@k.pdf}
        \caption*{\footnotesize(a) R@k of Independent retrieval on real videos}
        \label{fig:REALR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.16\textwidth}
    \includegraphics[width=\linewidth]{picture/AI_R@k.pdf}
    \caption*{\footnotesize(b) R@k of Independent retrieval on AI videos}
    \label{fig:AIR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.16\textwidth}
    \includegraphics[width=\linewidth]{picture/mixReal_R@k.pdf}
    \caption*{\footnotesize(c) R@k of real videos retrieval on mixed dataset}
    \label{fig:mixRealR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.16\textwidth}
    \includegraphics[width=\linewidth]{picture/mixAI_R@k.pdf}
    \caption*{\footnotesize(d) R@k of AI videos retrieval on mixed dataset}
    \label{fig:mixAIR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.16\textwidth}
    \includegraphics[width=\linewidth]{picture/RelativeDelta_R@k.pdf}
    \caption*{\footnotesize(e) R@k of RelativeΔ on mixed dataset}
    \label{fig:relativedeltaR@k}
\end{subfigure}
\hfill
\begin{subfigure}{0.16\textwidth}
    \includegraphics[width=\linewidth]{picture/DebiasDelta_R@k.pdf}
    \caption*{\footnotesize(f) R@k of NormalizedΔ on mixed dataset}
    \label{fig:debiasdeltaR@k}
\end{subfigure}
\vspace{-0.6em}
\caption{Evaluation results on a training set containing a mix of AI-generated videos. We vary the proportion of AI-generated videos in the dataset (X-axis), while keeping the total number of training samples constant. The model is then tested on the OpenSora TextCond test set.}
\label{fig:mix-metric}
\end{figure*}






\subsection{Impact of AI-Generated Videos in Training Datasets}
\label{sec:3.3}
As AI-generated videos become widely available on the Internet, they inevitably blend into the training datasets of video retrieval models. 
We find that when AI-generated videos are introduced into the training set, retrieval models are more likely to rank AI-generated videos higher in the retrieval list, making source bias more pronounced.

To examine the impact of AI-generated videos on model training, we replace 20\% of the real videos in the MSR-VTT training set with their corresponding AI-generated counterparts, creating a mixed training set. 
This new set contains 1,800 AI-generated videos and 7,200 real videos, compared to the original training set, which consists all of real videos. 

The experimental results, as shown in Table \ref{fig:mix-metric}, indicate that fine-tuning retrieval models on the real video training set enhances the retrieval performance compared to the original model, while significantly reducing Source Bias. 
In terms of the $Normalized\Delta$ metrics: R@1 increases by 49.29, MeanR increases by 106.45, and MixR increases by 76.53. 
These improvements suggest that incorporating real data into the fine-tuning set effectively mitigates Source Bias.

However, when fine-tuning with the mixed training set containing 20\% AI-generated videos, the model's retrieval performance also improves. 
Yet, compared to fine-tuning with only real videos, Source Bias increases significantly. 
In terms of the $Normalized\Delta$ metrics, R@1 decreases by 89.52, MeanR drops by 54.47, and MixR declines by 79.74. 
This indicates that while retrieval performance may not differ greatly, the model is significantly more likely to retrieve AI-generated videos when they are included in the training set. 
Even a 20\% mix of AI-generated videos has a large impact on Source Bias.

Furthermore, as shown in Figure \ref{fig:mix-metric}, the model's bias towards AI-generated videos becomes more pronounced as the proportion of such videos in the training set increases—from 20\% to 40\%, 60\%, and 80\%. 
The Visual-Temporal Relevance Bias intensifies with the growing proportion of AI-generated content.



\section{Causes of Video Souce Bias}

\label{sec:4}
In this section, we explore the causes of the inherent, and subtle biases in AI-generated videos. We identify that source bias arises from two key sources of information embedded in AI-generated videos: visual and temporal information. We investigate the origins of these biases through methods including frame shuffling and single-frame retrieval.

\subsection{Visual Information Induces Source Bias}
\label{sec:4.1}
We find that visual information contributes more significantly to the generation of source bias, while temporal information also plays an indispensable role. 
However, temporal information in real videos is more diverse and rich. 
In the experiment, by rearranging the frame order of a video, we can effectively adjust its temporal sequence without altering its visual content. 
This approach allows us to examine the impact of temporal sequence on video retrieval bias while maintaining the integrity of the visual elements. 
We shuffle the frame order of both OpenSora TextCond videos and real videos, preserving the frame rates. 
Two datasets are created for experimentation using random and reverse frames. 
The experimental results are shown in Table \ref{tab:main-exp-random}. 
The results for the reverse dataset are similar to those of the random dataset, but are not presented here due to space limitations.

We explore two scenarios to assess how temporal information influences retrieval bias: first, by shuffling only the AI-generated video frames, and second, by shuffling the frames of both real and AI-generated videos. 
The first scenario tests whether temporal information in AI-generated videos contributes to source bias while maintaining the visual content of real videos. 
The second scenario compares the impact of temporal sequence on retrieval performance while preserving the visual content of both video types. 
Our experiments show that shuffling frames of AI-generated videos alone reduces retrieval accuracy, with $Normalized\Delta$ either decreasing or fluctuating. 
However, when frames from both real and AI-generated videos are shuffled, $Normalized\Delta$ decreases significantly. 
For example, in the InternVideo model, $Normalized\Delta$ decreases by 29.63. 

% Indicating that real videos contain richer temporal information. This suggests that damage to the temporal sequence of AI-generated videos degrades their quality and contributes to Visual-Temporal Relevance Bias in retrieval models. When this information is disrupted, it severely impacts the quality of real videos, while AI-generated videos, with their limited temporal information, are less affected.

\begin{table*}
  \belowrulesep=0pt
  \aboverulesep=0pt
   \vspace{-1.2em}
  \caption{The retrieval performance of Text-Video Retrieval Models on the OpenSora TextCond, after shuffling the video frame order, is evaluated. Random refers to shuffling both real and AI-generated videos, while Random-only-AI refers to shuffling only the AI-generated videos. 
  For the definitions of $Relative\Delta$ and $Normalized\Delta$, please refer to Table \ref{tab:main-exp-text}.}
  % When $Relative\Delta>0$ or $Normalized\Delta>0$, it indicates that the retrieval model tends to rank real videos higher. Conversely, \textcolor{red}{$Relative\Delta<0$} or \textcolor{red}{$Normalized\Delta<0$} suggests that the model tends to rank AI-generated videos higher. The absolute values of these metrics reflect the magnitude of the bias. $Normalized\Delta$ incorporates a penalty term to the original $Relative\Delta$, offering a more accurate measure of the bias.}
    \vspace{-0.8em}
  \label{tab:main-exp-random}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \small
  % \rowcolors{2}{white}{gray!15}
  \resizebox{0.8\textwidth}{!}{%
  \begin{tabular}{ll|cccccc|cccccc}
  \toprule
    \multicolumn{2}{c|}{\makecell[l]{\hspace{1.5em}\textbf{Dataset}}}  & 
    \multicolumn{6}{c|}{\makecell[l]{\hspace{3em}\textbf{Random}}} &
    \multicolumn{6}{c}{\makecell[l]{\hspace{3em}\textbf{Random-only-AI}}} 
    
    \\\midrule
     Model &   Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
     \multirow{6}{*}{\makecell[c]{Alpro}} & REAL & 23.5 & 43.6 & 52.6 & 9 & 53.71 & - & 24.1 & 45.1 & 55.5 & 8 & 49.61 & - \\
      & AI & 37.0 & 59.5 & 68.5 & 3 & 28.56 & - & 37.0 & 59.5 & 68.5 & 3 & 28.56 & -  \\
     & mixed-REAL & 9.7 & 34.1 & 43.6 & 16 & 90.49 & - & 10.3 & 35.3 & 46.3 & 13 & 84.72 & - \\
     & mixed-AI & 25.8 & 49.2 & 56.9 & 6 & 71.15 & - & 25.9 & 48.9 & 57.4 & 6 & 69.77 & - \\
     & $Relative\Delta$ & \textcolor{red}{-90.7} & \textcolor{red}{-36.25} & \textcolor{red}{-26.47} & \textcolor{red}{-90.91} & \textcolor{red}{-23.93} & \textcolor{red}{-68.51} & \textcolor{red}{-86.19} & \textcolor{red}{-32.3} & \textcolor{red}{-21.41} & \textcolor{red}{-73.68} & \textcolor{red}{-19.35} & \textcolor{red}{-59.74} \\
     & $Normalized\Delta$ & \textcolor{red}{-46.07} & 2.1 & 4.37 & 13.44 & 37.58 & 1.65 & \textcolor{red}{-43.96} & 3.68 & 6.12 & 21.56 & 34.85 & 4.15 \\ \hline
     \multirow{6}{*}{\makecell[c]{Frozen}} & REAL & 20.5 & 41.9 & 51.8 & 9 & 56.19 & - & 22.9 & 43.2 & 53.6 & 8 & 49.81 & -  \\
     & AI & 30.7 & 54.9 & 65.4 & 4 & 32.57 & - & 30.7 & 54.9 & 65.4 & 4 & 32.57 & - \\
     & mixed-REAL &
    7.2 & 29.6 & 40.6 & 19 & 102.92 & - & 9.3 & 32.9 & 42.8 & 16 & 90.58 & - \\
     & mixed-AI &
    24.3 & 46 & 54.2 & 7 & 74.01 & - & 24 & 45.1 & 54.2 & 8 & 74.48 & -  \\
     & $Relative\Delta$ &
   \textcolor{red}{-108.57} & \textcolor{red}{-43.39} & \textcolor{red}{-28.69} & \textcolor{red}{-92.31} & \textcolor{red}{-32.69} & \textcolor{red}{-77.86} & \textcolor{red}{-88.29} & \textcolor{red}{-31.28} & \textcolor{red}{-23.51} & \textcolor{red}{-66.67} & \textcolor{red}{-19.51} & \textcolor{red}{-58.16}  \\ 
    & $Normalized\Delta$ &
    \textcolor{red}{-68.73} & \textcolor{red}{-11.52} & \textcolor{red}{-1.83} & \textcolor{red}{-12.31} & 20.83 & \textcolor{red}{-20.07} & \textcolor{red}{-59.19} & \textcolor{red}{-4.67} & 0.34 & 2.9 & 22.6 & \textcolor{red}{-11.23}  \\ \hline
    \multirow{6}{*}{\makecell[c]{Intern\\Video}} & REAL &
    40.6 & 66.7 & 75.2 & 2 & 22.27 & - & 40.6 & 66.7 & 75.2 & 2 & 22.27 & -  \\
     & AI & 47 & 69.9 & 77.9 & 2 & 18.04 & - & 47 & 69.9 & 77.9 & 2 & 18.04 & -  \\
     & mixed-REAL & 22.9 & 48.4 & 57.1 & 6 & 86.24 & - & 28.3 & 54.1 & 63 & 4 & 75.43 & -  \\
     & mixed-AI &
    28 & 61.1 & 71 & 3 & 25.37 & - & 20.8 & 56 & 68.4 & 4 & 27.36 & -  \\
     & $Relative\Delta$ &
    \textcolor{red}{-20.04} & \textcolor{red}{-23.2} & \textcolor{red}{-21.7} & \textcolor{red}{-66.67} & \textcolor{red}{-109.07} & \textcolor{red}{-65.26} & 30.55 & \textcolor{red}{-3.45} & \textcolor{red}{-8.22} & 0 & \textcolor{red}{-93.53} & \textcolor{red}{-20.99}  \\
     & $Normalized\Delta$ &
    \textcolor{red}{-2.95} & \textcolor{red}{-12.29} & \textcolor{red}{-14.59} & \textcolor{red}{-66.67} & \textcolor{red}{-78.14} & \textcolor{red}{-49.25} & 45.16 & 3.31 & \textcolor{red}{-3.53} & 0 & \textcolor{red}{-72.32} & \textcolor{red}{-9.05}  \\ \bottomrule 
  \end{tabular}
  }
\end{table*}


\begin{table}
  \belowrulesep=0pt
  \aboverulesep=0pt
    %\vspace{-1.2em}
  \caption{Model performance on single-frame retrieval using the OpenSora TextCond dataset.}
  %OpenSora TextCond数据集进行单帧检索的模型表现
    %\vspace{-0.8em}
  \label{tab:singleframe}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  %\small
  % \rowcolors{2}{white}{gray!15}
  \resizebox{0.45\textwidth}{!}{
  \begin{tabular}{ll|cccccc}
    \toprule
    %\multicolumn{8}{c}{\cellcolor{white}\makecell[l]%{\hspace{5em}\textbf{Single-Frame}}}
    %\\\midrule
     Model & Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
     \multirow{6}{*}{\makecell[c]{Alpro}} & REAL & 32.2 & 52.2 & 61.5 & 5 & 49.74 & -\\
     & AI & 33.3 & 56.9 & 66.6 & 3 & 37.69 & -\\
     & mixed-REAL & 16.3 & 42.5 & 52.4 & 9 & 83.66 & -\\ 
     & mixed-AI & 22.4 & 47.4 & 57.1 & 6 & 96.41 & -\\ 
     & $Relative\Delta$ & \textcolor{red}{-31.52} & \textcolor{red}{-10.9} & \textcolor{red}{-8.58} & \textcolor{red}{-40} & 14.16 & \textcolor{red}{-19.12} \\
     & $Normalized\Delta$ &
    \textcolor{red}{-28.16} & \textcolor{red}{-3.35} & 0.04 & 13.33 & 41.89 & 9.02  \\ 
    \hline
    \multirow{6}{*}{\makecell[c]{Frozen}} & REAL & 34.7 & 56.4 & 65.4 & 4 & 42.16 & - \\
     & AI & 37.7 & 61.6 & 69.3 & 3 & 30.00 & - \\
     & mixed-REAL &
    16.8 & 45.6 & 55.4 & 7 & 70.95 & - \\
     & mixed-AI &
    26.3 & 51.6 & 60.6 & 5 & 77.42 & - \\
     & $Relative\Delta$ &
   \textcolor{red}{-44.08} & \textcolor{red}{-12.35} & \textcolor{red}{-8.97} & \textcolor{red}{-33.33} & 8.72 & \textcolor{red}{-22.9}  \\ 
    & $Normalized\Delta$ &
    \textcolor{red}{-35.79} & \textcolor{red}{-3.91} & \textcolor{red}{-0.16} & \textcolor{red}{-2.56} & 42.66 & 1.44 \\ 
    \hline
    \multirow{6}{*}{\makecell[c]{Intern\\Video}} & REAL & 32.2 & 52.2 & 61.5 & 5 & 49.74  & - \\
     & AI & 41.6 & 66 & 75.3 & 2 & 23.33 & - \\
     & mixed-REAL &
    15.5 & 44 & 53.2 & 9 & 83.14 & - \\
     & mixed-AI &
    28.1 & 54.3 & 63.8 & 4 & 60.12  & - \\
     & $Relative\Delta$ &
    \textcolor{red}{-57.8} & \textcolor{red}{-20.96} & \textcolor{red}{-18.12} & \textcolor{red}{-76.92} & \textcolor{red}{-32.13} & \textcolor{red}{-55.62}  \\
     & $Normalized\Delta$ &
    \textcolor{red}{-32.33} & 3.13 & 5.23 & 15.39 & 40.66 & 7.91   \\ 
    \bottomrule 
  \end{tabular}
  }
\end{table}

\subsection{Temporal Information Induces Source Bias}

\label{sec:4.2}

We find that the presence of temporal information causes AI-generated videos to appear at the top of the retrieval list, not just at the very first position in the results. 
When video retrieval is performed using only a single frame, the video modality degenerates into a static image modality. 
In this case, the retrieval system relies solely on the visual information from a single frame, ignoring temporal sequence data. 
This setup helps us analyze how the visual content of a video impacts retrieval results and potential biases in the absence of temporal and multi-frame information.

The experimental results, as shown in Table \ref{tab:singleframe}, indicate that Visual-Temporal Relevance Bias still exists, but is primarily observed in the R@1 metric. 
In the retrieval results of the three models, source bias remains evident in the R@1 metric, while it is almost nonexistent in MeanR and MedR. 
This suggests that when retrieving a single frame, the model’s bias is most pronounced in its preference for the first retrieved image. 
Overall,The combination of visual and temporal information contributes to the source bias phenomenon in text-video retrieval.

\section{Mitigating and Visualizing Bias}
\label{sec:5}
We have demonstrated that the presence of AI-generated videos causes Visual-Temporal Relevance Bias in retrieval models. 
In this section, we propose a method using contrastive learning to mitigate this bias. 
By incorporating AI-generated videos into the contrastive learning training set, we fine-tune the model to increase the likelihood of retrieving real videos while reducing the likelihood of retrieving AI-generated ones. 
We also extract a debiasing vector from the model, which can be applied to other video encoding vectors to further reduce the Visual-Temporal Relevance Bias in the retrieval system. 
This vector can also be used to visualize the bias.

\subsection{Debiased Model Training}
\label{sec:5.1}

We use the OpenSora TextCond (train) dataset construct in Section \ref{sec:2} and train the debiasing model using a contrastive learning approach.

\noindent \textbf{Notation:}For a real video, it corresponds to both a AI-generated video and a caption. For a video-text pair, if the video is AI-generated, we represent it as $(V_G, C)$, and if it is a real video, we represent it as $(V_R, C)$.
In retrieval, the model first samples $f$ frames from the video, represented as $I_j$($j \in [0, f-1]$). 
It then uses the pre-trained image encoder $E_I$ and text encoder $E_C$ to encode the images and text into vectors $h_{Ij}$ and $h_C$. 
These vectors are subsequently input into the video retrieval model $E_V$ , which computes the final video-text similarity $r_{VC}$, expressed as:
\vspace{-0.2em}
\begin{align*}
    r_{VC}=E_V([h_{I0}&...h_{If-1}],h_{C },\theta_{VC})\\
    h_{Ij} = E_I(Ij,\theta_I&)\ \ \ h_{C} = E_C(C,\theta_C)
\vspace{-0.4em}
\end{align*}

\noindent  \textbf{Loss Construction:}The optimization objective is as follows. 
Let $y$ represent the label: when $y = 1$, the video corresponds to the text; when $y = 0$, they do not correspond. 
$\mathcal{L}$ denotes the loss function, which minimizes the distance between the image and its corresponding text embedding vectors:

\vspace{-1.2em}
\begin{align*}
\theta_{VC} = arg\ min_\theta\ \mathcal{L}(r_{VC},y,\theta_{VC})
\vspace{-0.4em}
\end{align*}

During the training of the debiased model, we aim to enable the retrieval model to more easily retrieve the corresponding real videos based on the text, while avoiding AI-generated videos. 
For each text, we have a real video-AI-generated video-text triplet $(V_R, V_G, C_i)$. 
After image sampling, we use the image encoder and text encoder to encode them into vectors $h_{Rf}$, $h_{Gf}$, and $h_{Ci}$, respectively. 
A contrastive loss function is introduced in the debiased model:

\vspace{-1.2em}
\begin{align*}
    \Delta r(G,R,C)=& E_V([h_{G0}...h_{Gf-1}],h,\theta_{VC})
                    \\&-E_V([h_{R0}...h_{Rf-1}],h,\theta_{VC})
\vspace{-0.4em}
\end{align*}


$\Delta r$ measures the difference in the model's scores between two videos, providing a comprehensive way to assess the model's consistent invisible bias toward real and AI-generated videos. 
This helps guide the model to reduce bias against generated videos. 
Additionally, when $\Delta r < 0$, we do not apply this loss function, ensuring that during contrastive learning, the model still favors generated videos while increasing the probability of retrieving real videos. 
The overall training objective is:

\vspace{-1.2em}
\begin{align*}
     \theta_{VC} = arg\ min_\theta\ \mathcal{L}(r_{VC},y,\theta_{VC}) + \Delta r(G,R,C)
\vspace{-0.4em}
\end{align*}

The model training results, as shown in Table \ref{tab:debias}, reveal that the debiased model is more likely to rank real videos at the top of the list, significantly reducing the model’s source bias.

\begin{table}
  \belowrulesep=0pt
  \aboverulesep=0pt
  \vspace{-1.2em}
  \caption{The performance of the InternVideo model after debiasing fine-tuning using contrastive learning is evaluated. Fine-tuning is performed with the OpenSora TextCond dataset, and testing is conducted on three different datasets.}
    \vspace{-0.8em}
  \label{tab:debias}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \small
  \resizebox{0.45\textwidth}{!}{
  \begin{tabular}{ll|cccccc}
  \toprule
    \multicolumn{8}{c}{\cellcolor{white}\makecell[l]{\hspace{5em}\textbf{InternVideo Contrastive-Debias}}}
    \\\midrule
     Dataset &   Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
    \multirow{6}{*}{\makecell[c]{OpenSora\\TextCond}} & REAL & 41.2 & 66.5 & 76 & 2 & 20.436 & - \\
     & AI & 41.5 & 64.5 & 73.9 & 2 & 23.662 & - \\
     & mixed-REAL & 41.2 & 66.5 & 76 & 2 & 23.534 & - \\ 
     & mixed-AI & 0 & 0.2 & 0.5 & 224 & 293.688 & - \\ 
     & $Relative\Delta$ & 200 & 198.8 & 197.39 & 196.46 & 170.32 & 188.93 \\
     & $Normalized\Delta$ & 200.73 & 195.28 & 194.34 & 196.46 & 155.52 & 184.24 \\
    \hline
    \multirow{6}{*}{\makecell[c]{OpenSora\\ImageCond}} & REAL & 41.2 & 66.5 & 76 & 2 & 20.436 & - \\
     & AI & 35.3 & 63.1 & 74.2 & 3 & 22 & - \\
     & mixed-REAL & 41.2 & 66.5 & 76 & 2 & 23.727 & - \\ 
     & mixed-AI & 0 & 0.3 & 0.4 & 247 & 304.562 & - \\ 
     & $Relative\Delta$ & 200 & 198.2 & 197.91 & 196.79 & 171.09 & 189.29 \\
     & $Normalized\Delta$ & 184.58 & 189.5 & 192.66 & 152.35 & 163.63 & 166.85 \\
    \hline
    \multirow{6}{*}{\makecell[c]{CogVideoX\\TextCond}} & REAL & 41.2 & 66.5 & 76 & 2 & 20.436 & - \\
     & AI & 37.1 & 61.4 & 70.9 & 3 & 24.279 & - \\
     & mixed-REAL & 41.2 & 66.5 & 76 & 2 & 24.13 & - \\ 
     & mixed-AI & 0 & 2.9 & 7.5 & 99.5 & 183.991 & - \\ 
     & $Relative\Delta$ & 200 & 183.29 & 164.07 & 192.12 & 153.62 & 181.91 \\
     & $Normalized\Delta$ & 189.53 & 173.32 & 156.1 & 147.68 & 136.24 & 157.82\\ 
    \bottomrule 
  \end{tabular}
  }
\end{table}

\subsection{Visualization of Invisible and Temporal}
\label{sec:5.2}

In this section, we use the debiased model from the previous section to analyze the source of bias. 
Since the retrieval model tends to show an overall preference for AI-generated videos, and the debiased model reverses this preference, we explore this by analyzing the video embeddings after debiasing.

\noindent \textbf{Notation:}After obtaining the debiased model,  the debiased video encoder is denoted as $E_v^d$. 
We can obtain the original video embedding $h_v = [h_1, h_2, \dots, h_n]$, the debiased video embedding $h_v^d = [h_1^d, h_2^d, \dots, h_n^d]$, and the text embedding $C = [c_1, c_2, \dots, c_n]$.

\noindent \textbf{Visualization Bias：}We define a vector $p$ that represents the difference between the debiased video embedding and the original video embedding, capturing the shift in the video representation after debiasing:
\vspace{-0.8em}
\begin{align*}
     p = & [p_1,p_2,...,p_n]\\
       = & [h_{1}^{d}-h_1,h_{2}^{d}-h_2,...,h_{n}^{d}-h_n]
\vspace{-0.4em}
\end{align*}

After performing t-SNE visualization on the vectors $p$, $h_v$, and $h_v^d$, as shown in Figure \ref{fig:TSNE},we observe that the vector $p$ shows a distinct clustering pattern. 
This indicates that the AI video generation model embeds additional, consistent information across generated videos. 
We identify these extra details as the direct cause of the Visual-Temporal Relevance Bias. 
Three key observations emerged: 
(1) These extra details are generalizable; incorporating them into real videos can increase their retrieval probability and reduce bias. 
(2) Some of this additional information encodes temporal aspects, with certain details being integrated into the generated videos via temporal sequences. 
(3) The extra information exhibits a high degree of consistency, as all AI-generated videos share a concentrated embedding of this additional information.


\begin{table}
\belowrulesep=0pt
  \aboverulesep=0pt
    \vspace{-1.2em}
  \caption{The effect of adding the extracted $p$ vector to the original video representation on retrieval performance is as follows: $\Delta > 0$ indicates that after adding the $p$ vector, the retrieval model is more likely to rank real videos higher. Conversely, $\Delta < 0$ suggests that the model tends to prioritize AI-generated videos.}
    \vspace{-0.8em}
  \label{tab:p-debias}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \small
  % \rowcolors{2}{white}{gray!15}
  \resizebox{0.45\textwidth}{!}{%
  \begin{tabular}{l|cccccc}
  \toprule
    \multicolumn{7}{c}{\cellcolor{white}\makecell[l]{\hspace{5em}\textbf{InternVideo p-Debias}}}
    \\\midrule
     Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
    $REAL$ & -20.48 & -17.32 & -15.04 & -22.23 & -39.9 & -23.49 \\
    $REAL_{p-debias}$ & 17.12 & 7.54 & 1.21 & 39.4 & -26.0 & 10.17 \\
    $\Delta$ & 37.60 & 24.86 & 16.25 & 61.63 & 13.90 & 33.66 \\\bottomrule 
  \end{tabular}
  }
\end{table}

\begin{table}
\belowrulesep=0pt
  \aboverulesep=0pt
    \vspace{-1.2em}
  \caption{The effect of adding the extracted $p_{random}$ vector to the original video representation on retrieval performance is as follows: A $\Delta > 0$ indicates that after adding the $p_{random}$ vector, the retrieval model is more likely to rank real videos higher. Conversely, a $\Delta < 0$ suggests that the model tends to prioritize AI-generated videos.}
    \vspace{-0.8em}
  \label{tab:visual-debias}
  \centering
  \setlength{\tabcolsep}{0.8mm}
  \renewcommand{\arraystretch}{0.9}
  \small
  % \rowcolors{2}{white}{gray!15}
  \resizebox{0.45\textwidth}{!}{%
  \begin{tabular}{l|cccccc}
  \toprule
    \multicolumn{7}{c}{\cellcolor{white}\makecell[l]{\hspace{5em}\textbf{InternVideo p-random-Debias}}}
    \\\midrule
    Metric & R@1 & R@5 & R@10 & MedR & MeanR & MixR \\
    \midrule
    $REAL$ & -51.36 & -12.0 & -7.06 & -35.56 & 0.42 & -28.83 \\
    $REAL_{p-debias}$ & -41.14 & -1.9 & -5.29 & -4.19 & 2.04 & -14.43 \\
    $\Delta$ & 10.22 & 10.1 & 1.77 &  35.56 & -1.62 & 14.40 \\\bottomrule 
  \end{tabular}
  }
\end{table}

For the first point, we design an experiment. 
Since the debiasing model adds a vector $p_i = h_{R}^d - h_R$ to each generated video $V_{Gi}$, we simplify the analysis by using the average of all $p_i$ vectors, denoted as $p_{avg}$, to represent the additional information introduced. 
Using the original video encoder, we obtain the raw representation of the real video $h_{R} = [h_{R1}, h_{R2}, \dots, h_{Rn}]$, and then add the computed $p_{avg}$ to each video representation, yielding the biased real video representation $h_{R}^{pd} = [h_{R1} + p_{avg}, h_{R2} + p_{avg}, \dots, h_{Rn} + p_{avg}]$. 
The experimental results, as shown in Table \ref{tab:p-debias}, indicate that when we input the new real video representation into the retrieval model, we observe a reduction in bias across all evaluation metrics, particularly in the R@1 metric, where the bias was even reversed. 
This finding suggests that the additional information is universal; it can be applied not only to AI-generated videos but also directly to real videos, improving their retrieval performance. 
This confirms that the vector $p$ is indeed a key factor in introducing Visual-Temporal Relevance Bias.


\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{picture/pvector.pdf}
\caption{Changes in the retrieval model's metrics ($\Delta$) after altering the vector representations of videos using p-vectors and random p-vectors.}
\label{fig:p}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{0.23\textwidth}
        \includegraphics[width=\linewidth]{picture/TSNE.pdf}
        \caption*{(a) Original Temporal Information t-SNE Visualization}
        \label{fig:TSNE1}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\linewidth]{picture/TSNE_time.pdf}
    \caption*{(b) Scramble Temporal Information t-SNE Visualization}
    \label{fig:TSNE2}
\end{subfigure}
\hfill
\caption{t-SNE visualization of image representations and
transformations vector.}
\label{fig:TSNE}
\end{figure}

For the second point, we examine whether the additional information contains temporal data. 
We input the Random OpenSora TextCond dataset into the model, generating and extracting the $p_{random}$ vector with scrambled temporal information. 
This $p_{random}$ vector preserves the original visual content while incorporating disrupted temporal sequences. 
When this $p_{random}$ vector is added to the real video representation, the experimental results, as shown in Table \ref{tab:visual-debias}, reveal a reduction in bias across the evaluation metrics, although the effect is notably weaker compared to when real temporal information is included. 
The results, as shown in Figure \ref{fig:p}, suggest that temporal information in AI-generated videos plays a significant role in Visual-Temporal Relevance Bias, with some additional information encoded in the temporal sequences.

For the third point, we visualize the vectors $p$, $h_G$, $h_R$, $h_{G}^d$, and $h_{R}^d$, along with the corresponding vectors from the random dataset, in two dimensions. 
The results are shown in Figure \ref{fig:TSNE}. 
From the t-SNE plot, we observe that compared to the other vectors, both $p$ and $p_{random}$ vectors show clustering patterns. 
This indicates that the $p$ and $p_{random}$ extracted from different videos are highly similar, and the visual and temporal additional information in AI-generated videos exhibits significant clustering.

\section{Related Work}
\label{sec:6}
\subsection{Bias of Information Retrieval}
\label{sec:6.1}
Since the early 21st century, bias in information retrieval has garnered significant attention from researchers. Mowshowitz \textit{et al}. \cite{mowshowitz2002assessing} initially introduced the concept of bias, providing a definition and analyzing its potential sources. They also proposed methods for evaluating bias, such as comparing search engine performance. Later studies focused on real-time methods for measuring bias in web search engines \cite{mowshowitz2005measuring}. Research has since evolved in three primary directions: analyzing the sources of bias in specific domains, exploring methods for assessing and mitigating bias, and examining retrieval bias induced by AIGC-generated content.

Regarding domain-specific biases, Zuccon \textit{et al}. \cite{zuccon2016understandability} introduced the concept of "Understandability Bias" in retrieval systems and suggested using this bias to evaluate IR systems. Azzopardi \textit{et al}. \cite{azzopardi2021cognitive} confirmed the presence of cognitive biases in information retrieval and analyzed their impact. Bigdeli \textit{et al}. \cite{bigdeli2021exploring} explored potential gender biases in IR systems by analyzing various psychological characteristics of query-relevant documents. Kulshrestha \textit{et al}. \cite{kulshrestha2017quantifying} proposed a framework for quantifying social media bias, discussing its consequences, and extending the approach to decouple different sources of bias \cite{kulshrestha2019search}. Gerritse \textit{et al}. \cite{gerritse2020bias} focused on biases in conversational AI systems, particularly in personalized knowledge graphs.

Concerning methods for assessing and mitigating bias, Wilkie \textit{et al}. \cite{wilkie2014retrievability} examined the relationship between retrieval bias and performance, finding a significant negative correlation. Subsequent studies addressed fairness issues across various systems and proposed mitigation strategies. Yao \textit{et al}. \cite{yao2017new}, Yang \textit{et al}. \cite{yang2017measuring}, and Geyik \textit{et al}. \cite{geyik2019fairness} explored fairness in collaborative filtering recommendation systems and ranked outputs. Pitoura \textit{et al}. \cite{pitoura2018measuring} discussed methods for quantifying bias in search engines, social networks, and recommendation services. Gao \textit{et al}. \cite{gao2021addressing}, Shah \textit{et al}. \cite{gao2021toward} focused on addressing fairness challenges in search and information retrieval systems.

The rise of AIGC has introduced new challenges in retrieval bias. Dai \textit{et al}. \cite{dai2023llms} investigated the bias that large language models (LLMs) introduce into retrieval systems, finding that neural retrieval models tend to rank AIGC-generated documents higher, a phenomenon known as source bias. Gao \textit{et al}. \cite{gao2024aigcs} discovered that, compared to natural images, objects in images generated by large vision-language models (LVLMs) exhibit more hallucination features. Xu \textit{et al}. \cite{xu2024invisible} revealed that synthetic images also introduce biases and proposed methods to mitigate these effects. Previously, no one had addressed the source bias introduced by video generation. Our work validates the existence of source bias, identifies its origins in both visual and temporal factors, and mitigates the bias.

\subsection{Detection of AI-generated content}
In recent years, researchers have conducted extensive studies on detecting AI-generated content. Existing methods for detecting AI-generated text can be categorized into three main approaches: Watermarking methods embed identifiable markers into AI-generated content to verify its origin \cite{topkara2006hiding,ueoka2021frustratingly,gu2022watermarking,liu2023private,liu2024adaptive}. Statistical methods leverage metrics such as entropy to distinguish AI-generated text from human-written text \cite{vasilatos2023howkgpt,mitchell2023detectgpt,su2023detectllm}. Supervised learning methods train deep classifiers to differentiate between AI-generated and human-written content \cite{chen2023gpt,soto2024few,liu2023coco}.

For detecting AI-generated images, current methods are primarily divided into two categories: GAN-based detection methods focus on identifying artifacts unique to GAN-generated images \cite{wang2020cnn,liu2020global}. Generalizable detection methods for diffusion models aim to identify a broader range of AI-generated images \cite{ma2023exposing,luo2024lare,wu2023generalizable,epstein2023online,wang2023dire,corvi2023detection}.

In contrast, the detection of AI-generated videos remains relatively underexplored. Existing methods include: Fei et al. \cite{fei2021exposing}, who proposed a motion discrepancy-based approach to effectively distinguish AI-generated fake videos from real ones. Chang et al. \cite{chang2024matters}, who utilized a 3D convolutional network to analyze appearance, motion, and geometry for video differentiation. Vahdati et al. \cite{Vahdati_2024_CVPR}, who employed H.264 re-compression to detect synthetic videos. We discover that AI-generated videos contain additional visual and temporal information embedded by video generation models, which can aid in detecting AI-generated videos. Moreover, these studies indirectly validate the existence of the additional information we proposed.


\section{Conclusion}

This study investigates the impact of AI-generated videos on text-video retrieval. We constructed a comprehensive retrieval scenario that includes both real and AI-generated videos, and conducted experiments based on this benchmark. The results demonstrate that AI-generated videos are preferentially retrieved by the model and appear at the top of the retrieval list. As the proportion of AI-generated videos in the training set increases, the bias in retrieval results becomes more pronounced. Furthermore, we analyzed the sources of this bias, which primarily stem from the visual information in AI-generated videos, with some contribution from temporal information. This temporal information causes AI-generated videos to be ranked higher overall. Finally, we employed a debiasing approach based on contrastive learning to alleviate the Visual-Temporal Relevance Bias and identified that the additional information encoded by the generative model contributes to this bias. The findings of this study elucidate the potential influence of AI-generated videos on text-video retrieval and provide insights for future research.

% \subsection{Generative AI for Video Content Generation}
% \label{sec:6.2}
% With the widespread recognition of videos generated by models like Sora \cite{liu2024sora}, video generation technology has gained public attention. Four core models have played a key role in the evolution of video generation: Generative Adversarial Networks (GANs) \cite{goodfellow2020generative}, which optimize generators to produce realistic video samples (e.g., VideoGAN \cite{vondrick2016generating}); Variational Autoencoders (VAEs) \cite{kingma2013auto}, such as SVG \cite{denton2018stochastic}, which capture temporal structure and generate coherent video sequences; Transformer-based autoregressive models \cite{vaswani2017attention}, which generate smoother and more natural video content than traditional GANs; and diffusion models \cite{ho2020denoising}, which convert the video generation task into a denoising process and have made breakthroughs in video synthesis. These models have demonstrated excellent performance in different aspects of video generation, driving technological advancements in the field. In the past two years, Runway introduced the Gen-1 model \cite{esser2023structure} and the Gen-2 model \cite{gen2}, both of which propose a controllable structure and content-aware video diffusion mechanism. Meta released Emu Video \cite{sheynin2024emu, girdhar2023emu}, which allows for free-form editing based on instructions, significantly improving video quality. PixelDance, a video generation method based on latent diffusion models, better aligns with text prompts \cite{zeng2024make}. Additionally, Pika 2.0 supports uploading multiple items as references, allowing these elements to be seamlessly integrated into AI-generated videos. In this paper, we use two state-of-the-art open-source models, CogVideoX \cite{yang2024cogvideox} and OpenSora V1.2 \cite{opensora}, to generate videos.


% 过去的几年里，研究者们对AI生成内容检测已经进行了广泛的研究。现有的AI生成文本检测方法主要分为三类：水印方法，通过在AI生成的内容中嵌入标记，验证其来源\cite{topkara2006hiding,ueoka2021frustratingly,gu2022watermarking,liu2023private,liu2024adaptive}。统计方法，应用熵等统计指标区分AI生成文本和人类撰写的文本 \cite{vasilatos2023howkgpt,mitchell2023detectgpt,su2023detectllm}。最后，监督学习方法，通过训练深度分类器区分人工智能来区分 \cite{chen2023gpt,soto2024few,liu2023coco}。现有的AI生成图像检测方法主要分为两类：基于GAN模型的检测方法\cite{wang2020cnn,liu2020global}和基于扩散模型的可泛化的检测方法\cite{ma2023exposing,luo2024lare,wu2023generalizable,epstein2023online,wang2023dire,corvi2023detection}。而现有的对AI生成视频的检测方法仍然较少，Fei 儿童al. \cite{fei2021exposing} 提出了 a motion discrepancy based method that can effectively differentiate AI-generated fake videos from real ones. Chang et al\cite{chang2024matters} 利用3D convolutional network 分别检测视频的 appearance, motion, and geometry 用来区分AI生成视频和真实视频. Vahdati et al.\cite{Vahdati_2024_CVPR} 利用H.264 re-compression检测synthetic video.

\newpage

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\end{CJK}
\end{document} %% 新增
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
