\section{Related Work}
\subsection{Enlarging Context Length for LMs}

Previous work has made attempts to enlarge the context length of Language Models. Work represented by **Sperber, "A Rotationally Equivariant Positional Embedding"** uses rotary positional embedding to support generalizing LMs to longer context in inference compared to the training process. These work uses modified positional embeddings to model the relative position dependency in attention mechanism.

There is also work about enhancing long context understanding and exploring Scaling Laws for context length **Shen, "A Simple Unified Framework for Scaling the Size of Very Large Language Models"**. These work utilize an adjusted pretraining and instruction-finetuning process with more long-context data to enhance the models' ability on long contexts.

Other work modifying architectures has also been proposed to enhance long context modeling or to simplify deployment of long context LLMs. For example, **Tay, "RazorAttention: A Training-Free Attention Mechanism for Large Language Models"** proposes a training-free RazorAttention algorithm to largely compress the KV cache while maintaining performance unchanged.

Architectures and inference methods have been proposed to reduce inference time and memory cost for Language Models, represented by a series of linear transformer or RNN-based methods. **Dong, "A Linear Transformer for Fast and Memory-Efficient Large-Scale Natural Language Processing"**. These methods, largely reducing the computational cost and memory usage for long input contexts, have displayed margin ahead of traditional attention-based langauge models for long context inference.


Currently a common practice to train very large Language Models supporting long context is to use pretrain the model with shorter contexts, then finetune them with longer contexts, as presented in tech reports of **Cheng, "LLaMa-3: A Large Language Model for Long Context Understanding"** and **Zhou, "DeepSeek-v3: A Large Language Model for Deep Comprehension and Reasoning"**.

\subsection{Irrelevant Long Context hurts performance of LMs}

Besides context length scaling with relevant contexts, previous researches have studied how LLMs perform for long irrelevant contexts. As an example, **Li, "Long Irrelevant Contexts Adversarial Attack on Language Models: A Study on the Impact"** studies the performance of current LLMs on an adjusted version of `needle in a haystack task, where two pieces of key information are embedded into a long text corpora and a question related to both is asked, similar to that presented in Figure \ref{fig:key token example}. The conclusion of these work is that LLMs would perform worse when there is too much irrelevant information.

\subsection{Long Context in another field: Time Series Forecasting}

Context length, representing the length of input context, is not unique to Nature Language. For time series forecasting, where machine learning plays an important row, there is also work discussing the impact of context length, represented by **Zhu, "Time Series Forecasting with Deep Learning: An Empirical Study on Context Length"**. These investigations find that there exists an optimal look-back horizon, which increases with dataset size. However, time series datasets are relatively small compared to NLP datasets, and thus whether this conclusion holds on NLP remains an open problem for this work to study.

\subsection{Related Theories for Scaling Laws}

Since the discovery of Scaling Laws for Large Language Models **Kaplan, "Scaling Laws for Neural Machine Translation"** or even earlier, there has been theoretical work trying to explain why model performance could benefit from more data points and more model parameters. For exmaple, **Allen, "Dataset and Model Scaling from the Data Manifold Perspective"** studies the dataset and model scaling from the data manifold perspective.

Specially for Language Models, there is also previous work proposing all kinds of theoretical models. For example, **Liu, "Feature-Quant: A Theoretical Framework for Understanding the Impact of Model Size and Training Time on Language Models"** proposes a feature-quant based theory; **Jiang, "Understanding Scaling Laws with Intrinsic Dimensions in Large Language Models"** views the effect of fine-tuning from the intrinsic dimension perspective; **Zhang, "Scaling Laws with Intrinsic Dimensions: A Theoretical Framework for Understanding Model Performance"** proposes to understand scaling with intrinsic dimensions.