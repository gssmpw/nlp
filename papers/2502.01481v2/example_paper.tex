%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[preprint]{icml2025}
% select from [] (submission format), [preprint], [accepted]. [preprint] is modified by Jingzhe Shi, not native for ICML 2025 template.

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[symbol]{footmisc}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Explaining Context Length Scaling and Bounds for Language Models}

\begin{document}

\twocolumn[
\icmltitle{Explaining Context Length Scaling and Bounds for Language Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jingzhe Shi}{IIIS,CPHOS}
\icmlauthor{Qinwei Ma}{IIIS}
\icmlauthor{Hongyi Liu}{Zhili College}
\icmlauthor{Hang Zhao}{IIIS}
\icmlauthor{Jenq-Neng Hwang}{UW}
\icmlauthor{Serge Belongie}{UCPH}
\icmlauthor{Lei Li}{UW,UCPH}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \icmlaffiliation{THU}{Tsinghua University}
\icmlaffiliation{IIIS}{Institute for Interdisciplinary Information Sciences, Tsinghua University}
\icmlaffiliation{Zhili College}{Zhili College, Tsinghua University}
\icmlaffiliation{UW}{University of Washington}
\icmlaffiliation{UCPH}{University of Copenhagen}
\icmlaffiliation{CPHOS}{CPHOS Research$^*$}


\icmlcorrespondingauthor{Hang Zhao}{hangzhao@mail.tsinghua.edu.cn}
\icmlcorrespondingauthor{Lei Li}{lilei@di.ku.dk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Deep Learning, Context Length, Language Modeling}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.
\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% Previous work attempting to extend effective context length for Language Models has shown model performance could improve with relevant context length, summarized as the Scaling Laws for context length. However, other work also shows that irrelevant long context could worsen performance, and in other domains like time series, even long relevant context could harm performance. This calls for a more thorough understanding of the impact of context length to model performance. In this work, we (1) propose a framework rethinking the impact of context length from a loss decomposition perspective, considering the impact of adding context tokens to both Bayesian and Approximation loss starting \textbf{from First-Principles} and simple assumptions on Entropy and Intrinsic Space; and (2) experimentally validate our theory deduction on real and synthetic data, finding that even long relevant context could harm performance: optimal context length exists and it increases with dataset size. As LLMs grow in their context length (but the amount of data is limited), there could be potentially a wall bounded by dataset size. We hope our work may inspire new long context Language Models, as well as future work studying Physics for Language Models.

Long Context Language Models have drawn great attention in the past few years.
%Besides work on extending effective context length of Language Models, 
There has been work discussing the impact of long context on Language Model performance: some find that long irrelevant context could harm performance, while some experimentally summarize loss reduction by relevant long context as Scaling Laws. This calls for a more thorough understanding on how long context impact Language Modeling. In this work, we (1) propose a clean and effective theoretical framework on explaining the impact of context length to Language Modeling, from an Intrinsic Space perspective; and (2) conduct experiments on natural language and synthetic data, validating our proposed theoretical assumptions and deductions. Our theoretical framework can provide practical insights such as establishing that training dataset size dictates an optimal context length and bounds context length scaling for certain case.
We hope our work may inspire new long context Language Models, as well as future work studying Physics for Language Models.
\ifdefined\isvisible %provide public github url for preprint version
Code for our experiments is available at this url: \url{https://github.com/JingzheShi/NLPCtlScalingAndBounds}.
\else %provide anonymous git url for submission
Code for our experiments is available at this anonymous url: \url{https://anonymous.4open.science/r/xxxxxx}. % hide for arxiv version
\fi
%Our theory can derive useful deductions, for example, we show that there exists an optimal context length for training Language Models with certain training dataset size, beyond which validation Loss would increase. 
\end{abstract}

\section{Introduction}
\label{intro}

Because of the rapid development of capacity of Language Models and the importance of a long context length in tasks like reasoning, retrieval, etc, past years people have been attempting to extend the context length of Language Models. There have been a variety of methods on supporting long context Language Models\cite{rope,linearattention,mamba,rwkv,tttllm}. A wide variety of work is proposed to discuss the impact of context length: some shows long irrelevant context would worsen performance for LMs\cite{retrievalmeetslongcontext, sametaskmoretoken}; some shows long context would improve performance in a way summarized as Scaling Laws\cite{longllamascaling}; while work in other domains like time series shows long relevant context would hurt performance \cite{scalingtimeseries}. \textbf{This calls for a more thorough understanding of how context length affects Language Models' performance.}.

Previously, theories have been proposed to explain the Scaling Laws with respect to the data set and the size of the model\cite{explainingneuralscalinglaws,sharma2020neuralscalinglawdimension}. However, these theories do not study how context length impact Language Modeling, thus they cannot contribute directly to the problem.

In this work, we propose a theory framework to discuss the impact of context length from an Intrinsic Space perspective. Starting with simple assumptions w.r.t. Intrinsic Space and Intrinsic Dimension, we come up with a clean and effective theoretical explanation of relationship between \textbf{Cross Entropy Loss, Intrinsic Dimension and Context Length}. We further use real language and synthetic data to validate our assumptions and deductions. \textbf{Our main contributions include},
\begin{itemize}
    \item 1. We propose a theoretical framework on understanding Language Modeling for different context length in Language Models from the perspective of Intrinsic Space.
    \item 2. We conduct experiments on real and synthetic data, validating our theoretical assumptions and deductions.
\end{itemize}

Our theoretical framework can naturally derive phenomena that we have observed. For example, our theory derives that, for a certain amount of training data, as the context length increases, the neural network first would first behave like the Bayes Model (thus the loss decreases); then beyond a certain optimal context length, the gap between the trained model and the Bayes Model would increase, hence the validation loss would increase: this is experimentally verified, as shown in Figure \ref{fig: intro fig}, 

We hope our work may inspire future work when it comes to explaining context impact and/or designing new long context Language Models. For example, we observe that longer contexts have less impact (or there is a ceiling on the information about the next token as context length approaches infinity), discouraging models with excessively large context lengths. On the other hand, these findings have the potential to provide guarantees for RNNs with finite hidden states in long-sequence modeling.
\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/introfig.pdf}
  \caption{Upper figure: Validation Loss vs. Context Length, measured on subsets of OpenWebText dataset. We see for each training dataset size, there exists an optimal context length that minimizes pretraining validation loss, which increases with the dataset size (More details can be found in \textbf{Section \ref{sec: Optimal Context Length on LMs}}). Lower figure: similar results obtained on our \textbf{Synthetic Dataset} in \textbf{Section \ref{subsection: synthetic dataset point 3}}. This proves the deduction of our theory.}
  \label{fig: intro fig}
\end{figure}

Our phenomenological theory in the main paper (mainly presented in Section \ref{sec: theory}) based on Intrinsic Dimension is simpler and more intuitive, while it can also be explained by a more fundamental theory based on Entropy, as presented in \textbf{Appendix \ref{app: Entropy discussions}}.

% elucidate the relationship between \textbf{CE Loss, Intrinsic Dimension and Context Length} with experiments on Language and synthetic data, and propose a theoretical framework to explain such relation. We view the problem from an Intrinsic Space perspective, coming up with clean and effective theoretical explanation.


% Because of the rapid development of capacity of Language Models and the importance of a long context length in tasks like reasoning, retrieval, etc, past years people have been attempting to extend the context length of Language Models. Previously there is work to support context extrapolating represented by RoPE\cite{rope}, work about enhancing long context understanding and exploring Scaling Laws for context length\cite{longllamascaling}, and work about redesigning or adding retrieval-based architectures to enhance models' performance on related tasks\cite{razorattention}. Architectures and inference methods have been proposed to lower inference time and memory cost for Language Models with long contexts\cite{linearattention, mamba, rwkv, tttllm}.

% However, there is also previous work showing irrelevant context could worsen performance for Language Models\cite{retrievalmeetslongcontext, sametaskmoretoken}. For other domains in Machine Learning where data is insufficient like Time Series Forecasting, previous work has found even relevant context could worsen performance\cite{scalingtimeseries}. These previous work leads to such questions:
% \begin{itemize}
%     \item 1. How to model the impact of (relevant) long context?
%     Would (relevant) long context be worse in Language Modeling, as seen in Time Series Forecasting?
%     \item 2. How to analyze and explain the underlying law for the impact of longer contexts?
% \end{itemize}

% These questions call for more thorough understanding to the impact of context length, and a more general method to model the impact of each token to the prediction task: a framework is needed to analyze: (1) the reliance of tokens for a wide variety of different tasks from retrieval to complex reasoning, and (2) given the impact of such dependency, how context length impact minimum-possible loss (or Bayes Risk) and how our trained models approximate this loss.

% In this work, we introduce a comprehensive Scaling Laws theory for context length in Nature Language Processing tasks, with special focus on token dependency. We further conduct experiments to validate our assumptions and find that (relevant) long context could indeed harm performance for Language Models.



% \textbf{Our main contributions include},
% \begin{itemize}
%     \item 1. We elucidate the relationship between \textbf{CE Loss, Intrinsic Dimension and Context Length} with experiments on Language and synthetic data, and propose a theoretical framework to explain such relation. We view the problem from an Intrinsic Space perspective, coming up with clean and effective theoretical explanation.
    
%     \item 2. We derive that, given a certain number of training data, as long as farther tokens have less impact on next-token-prediction, there would be an optimal context length which increases with dataset size. We verify that there does exist an optimal context length for certain pretraining scenarios that increase with the amount of training data, represented by results in Figure \ref{fig: intro fig}.
% \end{itemize}



% We hope our work may inspire future work when it comes to explaining context impact and/or designing new long context Language Models. For example, we observe longer context has less impact (or there is a ceil on information when context length goes to infinity), discouraging models with too large context length; but on the other hand, these findings have the potential to provide guarantees for RNNs with finite hidden state for long sequence modeling.



\section{Assumptions, Deductions and Observations for Language Modeling}
\label{sec: theory}
\subsection{Preliminary: Loss Decomposition and Intrinsic Space}

It is common in ML studies to decompose the loss into \textbf{Bayes Risk} (the minimum loss possible, achieved by the theoretically optimal Bayesian Model), and \textbf{Approximation Loss} (the loss measuring the ability of a trained model actually to approximate the Bayesian Model). Specifically for Cross-Entropy loss, we have (please refer to Appendix \ref{app: CE} for more details):
\begin{equation}
\begin{aligned}
    H(P,Q_l)=&R_{Bayes}+L_{Approximate}\\
            =&H(P,P_l)+L_{Approximate}(P_l,Q_l)
\end{aligned}
\end{equation}

Where $P=p(x_0|x_{-\infty:0})$ is the distribution of Natural Language (or our experimented dataset), $P_l=p(x_0|x_{-l:0})$ is the Bayesian Model for context length $l$ and $Q=q(x_0|x_{-l:0})$ is the learned Language Model. $H(P,P_l)$ is the \textbf{Bayes Risk} of optimal model (the assumed `limit' when we have infinite data points and model parameters) and $L_{Approximate}(D,N,l,\ldots)$ is the \textbf{Approximation Loss}.

Bayesian Models are typically related to the Intrinsic Space of data manifold in such a way: the middle-layer or last-layer data representation of a well-trained Bayesian Neural Network is often used to approximate the Intrinsic Space\cite{bridginginformationtheorymeasureintrinsicdimensionlanguagemodels}. In \textbf{Section \ref{sec:Bayesian Theory}} we discuss more how this bridges \textbf{Bayes Risk} with Intrinsic Space, and how context length impacts Bayes Risk.

Approximation Loss, or how well the trained model learns Bayesian Model, is also related to Intrinsic Dimension in the perspective of Scaling Laws. In \textbf{Section \ref{sec:Approx Theory}} we discuss more about how the context length impacts \textbf{Approximation Loss} from this perspective.

We use a \textbf{ synthetic data set} to prove concepts in \textbf{Section \ref{sec: synthetic dataset}}.

We further derive that the balance between \textbf{Bayes Risk} and \textbf{Approximation Loss} would lead to an optimal context length which increases with the size of the training dataset. Our theoretical deduction and experiments on language and synthetic data are presented in \textbf{Section \ref{sec: Optimal Context Length on LMs}} and \textbf{Section \ref{subsection: synthetic dataset point 3}}.


\subsection{Bayes Risk with context length: an intrinsic space perspective}
\label{sec:Bayesian Theory}
In this section we discuss the impact of context length on the Bayes Risk, from an Intrinsic Space and Intrinsic Dimension perspective; we also conduct experiments measuring the dependency of context length to Bayes Risk.

\subsubsection{Bayes Risk and Entropy in Intrinsic Space: derived from First Principles}
\label{sec:experiment approximation of Bayesion Loss}

We propose a simple theory model to relate $H(P, P_l)$ with the intrinsic dimension $dim(l)$ of the intrinsic space $\text{space}_l$ of the text corpora of length $l$ (for the next-token prediction task).

From \textbf{first-principles} we assume that,

\begin{itemize}
    \item Assumption 1. Intrinsic Dimension of the Bayes Model $\lim_{l\rightarrow\infty}dim(l)=dim(\infty)$ is finite, which is the Intrinsic Dimension of next token prediction of language itself.
    \item Assumption 2. $\forall l_1,l_2\text{ such that } l_1<l_2$, $dim(l_1)<dim(l_2)$. This is because a longer context contains more information about the next possible token.
\end{itemize}

To simplify deduction, we further assume that,
\begin{itemize}
    \item Assumption 3. Each intrinsic dimension would add $s$ bits of information to the next-token prediction task, so there are $dim(l)*s$ bits of information that can be represented in $\text{space}_l$ for the next-token prediction. \textbf{Note this does not mean these are the only information in the Intrinsic Space, hence $s$ can be small, or even smaller than $1$}. 
    \item Assumption 4. KL-divergence for the Bayes <+Model of context length $l$, $P_l$, with Bayes Model of infinite context length, $P=P_{\infty}$, equal to $s*(dim(\infty)-dim(l))$.
\end{itemize}



We further assume that in intrinsic space the states are with equal probability. With these assumptions, we can derive $H(P,P_l)$ with $dim(l)$ (please refer to \textbf{Appendix \ref{app: CE}} for more details on the definition of Cross-Entropy loss used in this work, and \textbf{Appendix \ref{app:derive for KL distance for P and P_l}} for a detailed derivation):

\begin{equation}
    \begin{aligned}
R_{Bayes}(l)&=H(P,P_l)\\
               &=H(P)+D_{KL}(P,P_l)\\
               &\approx H(P)+s*(dim(\infty)-dim(l))\\
               &=-s*dim(l)+H(P)+s*dim(\infty).
    \end{aligned}\label{eq: LB vs. ID}
\end{equation}

This \textbf{ linear relationship} is observed in experiments for LMs in \textbf{Section \ref{subsubsection: linear of Bayesian Risk and ID for LMs}}, and for synthetic data in \textbf{Section \ref{subsection: synthetic dataset point 1}}.

Note that by Assumptions 1 and 2 we derive that: 

\begin{equation}
\frac{\partial R_{Bayes}}{\partial l}<0\text{, and }\lim_{l\rightarrow\infty} \frac{\partial R_{Bayes}}{\partial l} = 0.
\end{equation}

\subsubsection{Bayes Risk and Intrinsic Dimension: Experiment Measurement}
\label{subsubsection: linear of Bayesian Risk and ID for LMs}


We use well-trained Large Language Models to conduct experiments for approximating the Bayes Risk $H(P_l)$ on certain text corpora.

\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/CE_vs_Ctl.pdf}
  \caption{\textbf{Bayes Risk vs. Context Length}: Bayes Risk is approximated by Cross Entropy loss measured with LLaMa-3.1 series on OpenWebText, for different context length.}
  \label{fig:8b_1}
\end{figure}

We find that:

\begin{equation}\label{eq:approximate Bayes Risk}
H(P_l)\approx C_0+C/l^\gamma
\end{equation}

approximates the experimented behavior on OpenWebText well. Please see Figure \ref{fig:8b_1} for the result. Moreover, we further conduct experiments on a dataset that is sured not to be included in LLaMa 3.1 8B's pretraining dataset. Please see further information in Appendix \ref{app: anotherdatasetllama3}.

We further use PCA as a metric to measure the Intrinsic Dimension of Dataset with respect to context length. Figure \ref{fig:mid feature, opwbtxt 8B} provides the relative degradation of the eigenvalue in the feature space of LLaMa-3.1-8B, for the last token. We see that larger input length would indeed provide feature with lower degradation in the intrinsic space. Notably, when $5<idx<1500$ the curves is similar to Zip-f distribution ($\log eig = C_0-C*\log idx$), and for $500<idx<4000$ it resembles exponential degradation ($\log eig = C_0-C*idx$). Naturally, w `Intrinsic Dimension'. \textbf{However, these formulas are very similar around $idx\approx 1000$, hence cannot provide accurate estimations to the transformation index}.

Instead, following previous practice, here we use some \textbf{threshold} to decide the Ine would like to use the transformation index of these two states as Itrinsic Dimension: $\max_{idx} rela\_eig(idx)\geq \text{threshold}$ is used as the measured \textbf{Intrinsic Dimension}z. \textbf{We further evaluate the validity of this method in \textbf{Point 2} in Section \ref{subsection: points to achieve in synthetic dataset}, on a synthetic Dataset with synthetic data; and explain the linear measurement from an Entropy perspective in Appendix \ref{app: Entropy discussions}}. Notably, the threshold here is a hyperparameter which is set to constants like $1/20$ in some previous work\cite{aghajanyan2020intrinsicdimensionalityexplainseffectiveness}, but we observe here that many thresholds would validate the linear correspondence of Cross Entropy vs. Intrinsic Dimension, which further enhance the robustness of our result. In detail, as shown in Figure \ref{fig:mid feature, opwbtxt 8B} and Figure \ref{fig:CE vs ctl opwbtxt 8B}, we use thresholds from $0.002$ to $0.25$ to make our conclusion more robust.

For a certain threshold, we conduct experiments on several context lengths, and measure CE Loss on certain text corpora with these context lengths. We observe a fairly linear relationship between CE Loss and ID measured (supporting our theory), as shown in Figure \ref{fig:CE vs ctl opwbtxt 8B}:

\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/MidFeatures.pdf}
  \caption{\textbf{Relative Eigen Value} for LLaMa-3.1-8B on a subset of OpenWebText, presented in different x-axis scales, with different context length visible to Language Model. Gray lines represent different \textbf{threshold}s we take to measure the intrinsic dimension of the current model. }
  \label{fig:mid feature, opwbtxt 8B}
\end{figure}

\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/CEvsIDmeasured.pdf}
  \caption{\textbf{Cross Entropy Loss} vs. \textbf{Measured Intrinsic Dimension}, for LLaMa-3.1-8B on a subset of OpenWebText. Each line represents a certain threshold used to measure ID in the intrinsic space of the used LLM. Different Measurements would give ID values that are linear w.r.t. each other, and they are all linear w.r.t. CE loss.}
  \label{fig:CE vs ctl opwbtxt 8B}
\end{figure}

We see from Figure \ref{fig:CE vs ctl opwbtxt 8B} that, no matter what threshold we use, the Cross Entropy Loss usually follows a linear relationship with the Intrinsic Dimension we measured, showing the robustness of the PCA evaluation method, and validating our theoretical assumptions:

$$
dim(l) \approx -s*dim(l)+Const
$$

Which aligns well with \textbf{Equation \ref{eq: LB vs. ID}}, thus validating our previous deduction.

We use the concept of Intrinsic Dimension and Measured Intrinsic Dimension to simplify the deduction. Moreover, by using the perspective of $S=\log\Omega$ where $\Omega$ is the number of states in Intrinsic Space, \textbf{similar results can be derived}, and \textbf{why the measured IDs are linear w.r.t. each other and CE loss} can be (partially) explained. Please refer to \textbf{Appendix \ref{app: Entropy discussions}} for further discussions.

\subsection{Approximation loss with Context Length: an intrinsic dimension perspective}
\label{sec:Approx Theory}

In previous work people have experimentally summarize the Scaling Laws \cite{openaiscaling, chinchilla} as: $L_{Approx}(D)=C_0+A/D^\alpha$ for different context length. Previous work has succeeded in explaining this from an intrinsic space perspective, represented by \cite{neuralscalingdatamanifold} as: $\alpha \approx c/d_{intrinsic}$ where $c=2\text{ or }4$ based on model property, and $d_{intrinsic}$ is the dimension of the intrinsic space of the dataset.

As assumed in Section \ref{sec:experiment approximation of Bayesion Loss}, the Intrinsic Dimension should increase with $l$. Combined with previous results on $\alpha = c/dim(l)$, we have,

\begin{equation}
    \begin{aligned}
        L_{Approx} &= C_0+A(l)/D^{\alpha(l)},\\
        \frac{\partial \alpha}{\partial l} & < 0.
    \end{aligned}   
\end{equation}


This shows that longer context length would make it harder for the model to learn to approximate the Bayesian Model.

\section{Deduction: Optimal Context Length and Training Dataset Size}
\label{sec: Optimal Context Length on LMs}

In this section, we show a deduction of our theory presented in Section \ref{sec: theory}. We study the problem about a certain model trained on certain amount of training dataset $D$ with context length $l$, and validated on the validation set with the same context length $l$, where we want to know the impact of $l$ on validation loss.

As shown in Section \ref{sec:Bayesian Theory}, we can write Loss as:
\begin{equation}
\begin{aligned}
    L_{CE} &= C_0 + \frac C {l^\gamma} + \frac {A(l)}{D^{\alpha(l)}},
\end{aligned}
\end{equation}

In previous sections, we did not specifically discuss the relationship between $A$ and $l$. We consider $l$ where $\partial_l L_{CE}=0$ would give us an optimal $l$ with respect to $D$:

\begin{equation}
\partial_l A = - A \ln D(-\partial_l\alpha)+\gamma C\frac{D^\alpha}{l^{\gamma + 1}}=f(D,l).
\end{equation}

We find $\lim_{D\rightarrow 0} f=-\infty$ and $\lim_{D\rightarrow\infty} f = \infty$. This shows that for fixed $l$, no matter what $\partial_l A$ is, there exists some $D$ s.t. $\partial_l L_{CE}=0$.

We see that Bayes Risk decreases with $l$, Approximation Loss increases with $l$ but decreases with $D$; the balance between these two losses results in an optimal $l$ that increases with the optimal $D$.

We conduct experiments on a subset of OpenWebText with a sufficiently long context length. We trained GPT-2 on different context lengths with different amounts of training data, until the validation loss increases. We show our results in Figure \ref{fig: intro fig} and Figure \ref{fig: exp on optimal context and dataset size}. Details for our experiment settings are presented in the Appendix \ref{app: experiment settings}.

As shown both theoretically and experimentally, when training until overfitting on the training dataset and considering the perplexity on validation dataset, there does exist an optimal context length, \textbf{beyond which even relevant long context would increase validation loss} of pretraining Language Models. Such optimal context length would increase with training dataset size.

% When trained on small context lengths, the model approximates the Bayes Model well (or Bayes Risk dominates final loss), hence validation loss decreases with context length; but beyond it, the model no longer approximates the Bayes Model well and the loss is dominated by Approximation Loss, where validation loss would increase with context length.

\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/val_loss_comparison.pdf}
  \caption{\textbf{Openwebtext subset}, Validation Loss vs. Context Length, for different dataset sizes. Different curves represent different amount of training data used. A more readable figure can be found in Figure \ref{fig: intro fig}, where the minimum validation loss reachable for each training dataset size is subtracted.}
  \label{fig: exp on optimal context and dataset size}
\end{figure}

\section{Proof of Concept with Synthetic Data}
\label{sec: synthetic dataset}
\subsection{List of Points to prove}
\label{subsection: points to achieve in synthetic dataset}
In this section, we conduct experiments on a synthetic dataset, explaining the Bayes Risk and related theories we proposed in Section \ref{sec:Bayesian Theory}. With this synthetic dataset, we would like to prove the following,

\begin{itemize}
    \item \textbf{Point 1}. \textbf{Cross Entropy} Loss is approximately linear with \textbf{Intrinsic Dimension}. Shown in \textbf{Section} \ref{subsection: synthetic dataset point 1}.
    % \item \textbf{Point 2}. We would like to construct a synthetic model to approximate the relationship between \textbf{Intrinsic Dimension} (and \textbf{Cross Entropy} loss as well) and \textbf{context length} that are measured in the previous sections: $CE\approx C_0+C/l^\gamma$.
    \item \textbf{Point 2}. By measuring \textbf{Eigen-value} degradation in \textbf{Intrinsic Space} of well-trained models, one could obtain \textbf{a valid measurement of the Intrinsic Dimension}. Shown in \textbf{Section} \ref{subsection: synthetic dataset point 2}.
    \item \textbf{Point 3}. There exists an \textbf{optimal context length} for each \textbf{training dataset size} used, and such optimal context length \textbf{increases} with the amount of training dataset. Shown in \textbf{Section} \ref{subsection: synthetic dataset point 3}.
\end{itemize}

\subsection{Construction of Synthetic Data: the `position weighted multitask sparse parity' dataset}
\label{subsection: synthetic dataset definition}

In previous work, a common practice is to mask the leftmost tokens and leave $l$ tokens before the token-to-predict visible to Language Models, as shown in Figure \ref{fig:key token example}. Although this may not show the impact of important tokens to final answer perplexity (e.g., it fails to show the importance of the second key info in Figure \ref{fig:key token example}), this method aligns well with our setting of increasing context length.

\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/Example_1.pdf}
  \caption{An example of the `two needles in a haystack' task, similar to those in \cite{sametaskmoretoken}. Left: the input to the Language Model, with key information and question visualized in blue. Right: perplexity of the answer token $\langle 8 \rangle$ of LLaMa-3.1-8B (horizontal axis) vs. number of masked leftmost tokens (vertical axis). Although seeing both pieces of information are necessary to answer the question, perplexity rises dramatically only when the first piece of information is masked.}
  \label{fig:key token example}
\end{figure}

Although the next token to predict might depend on several pieces of key information, we see from Figure \ref{fig:key token example} that the first key token would raise model perplexity.

Inspired by this concept in Figure \ref{fig:key token example} and the `multitask sparse parity dataset previously studied in \cite{michaud2024quantizationmodelneuralscaling, syntheticdatasetorigin}, we propose the `position-weighted multitask sparse parity dataset. In detail, each input consists of $T$ `control bits and $L$ `context bits, each bit lies in $\{0,1\}$. Each subtask takes xor on two certain bits in the context bits, and the answer to some sample is the answer of the only activated subtask, as shown in Figure \ref{fig:synthetic dataset sample}. We \textbf{ set tasks to have the same probability} in the training and testing datasets. To emphasize the importance of context in different positions, for task $t$, we ensure $l(t)=max(context\_bit_1(t), context\_bit_2(t))$ follows $l(t)\approx20/(1-t/50)^{1/1.2}$. This ensures,

$$
t(l)\approx 50-\frac{50}{(l/20)^{1.2}},
$$
which is the number of solvable tasks given a model of context length $l$.

\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/syntheticdataset.pdf}
  \caption{An example of our synthetic data. The answer for Subtask 1,2,3 is $0\oplus0=0,0\oplus1=1$ and $1\oplus1=0$ respectively, but since the thrid bit is $1$ for control bits, only Subtask 3 is activated and the final answer is $0$. However, for a model of context length $7$, it cannot see the $9th$ bit required by subtask 3, making it unable to predict the answer correctly.}
  \label{fig:synthetic dataset sample}
\end{figure}
In usual cases, Intrinsic Dimension is equal or smaller than the number of subtasks given its definition to be the least dimension needed to capture useful information required to carry out all the subtasks in the system. We carefully design $min(context\_bit_1(t), context\_bit_2(t))$ (please refer to \textbf{Appendix \ref{app: special case in synthetic dataset}} for more details) to make the theoretical intrinsic dimension equaling to the number of solvable tasks.



Thus we have, for some model who presents $s$ bits of information (about the output logit) in one Intrinsic Dimension:

$$
ID(l,s)=t(l)/(s*T)\approx ID_0-C'/l^\gamma,
$$

in which, if we assume that the $1$ dimension in Intrinsic Space represents $1$ subtask (which has $1$ bits of information about the subtask and hence $1/T$ bits of information to the output logit), then $s=1/T$, thus $ID(l,1/T)=t(l)$.

\subsection{Cross Entropy vs. Intrinsic Dimension vs. Context Length}
\label{subsection: synthetic dataset point 1}
We train a large enough MLP on data generated on the previous tasks, and evaluate our model on the validation dataset. During synthetic data generation, we make sure the training dataset and validation dataset do not overlap on any sample. We train until overfitting the training dataset. We assume $1$ dimension in Intrinsic Space can store information about $1$ subtask, hence we take $ID(l)=t(l)$ as its theoretical value here. After training, we obtain such results:

% \begin{figure*}[h]
%   \includegraphics[width=2\columnwidth]{figures/syntheticdataset_results.pdf}
%   \caption{Left: ID vs. Context Length of the synthetic dataset generated (this is a property of the dataset which is irrelevant to the model trained). Middle: BCE Loss vs. Context Length, the fitted curve is very similar in shape (the index is the same as the Left figure). Right: BCE Loss vs. Intrinsic Dimension; this shows a very good linear relationship.}
%   \label{fig:synthetic dataset results}
% \end{figure*}

Let $f(x,C,C_0,\gamma)=C_0-C/x^\gamma$ and $g(x,k,b)=k*x+b$.

The fitted results are:
\begin{itemize}
    \item ID vs. CL: $ID\approx f(CL,C,C_0,\gamma)$,$C_0=-51.1\pm1.0$, $C=-1.7*10^3\pm0.3*10^3$,  $\gamma=1.18\pm 0.06$, $R^2=0.9997$.
    \item CE vs. CL: $ID\approx f(CL,C,C_0,\gamma)$,$C_0=-0.015\pm0.013$, $C=23.8\pm4.3$, $\gamma=1.18\pm 0.06$, $R^2=0.9997$.
    \item CE vs. ID: $CE\approx g(ID,k,b)$, $k=0.693\pm 1*10^{-5}$, $b\approx0.0139\pm4*10^{-7}$, $R^2=1-7*10^{-9}$.
\end{itemize}

As shown, we construct synthetic data such that $ID(l)=ID_0-C'/l^\gamma$, and our measurements show $CE=C-C'/l^\gamma$. More importantly, \textbf{for the synthetic data example, Cross Entropy loss is almost perfectly linear with the Intrinsic Dimension as we defined previously.} This validates \textbf{Point 1}: Cross Entropy Loss is approximately linear with Intrinsic Dimension; and we have also provided a construction to match the measured relationship $CE(l)\approx C_0+C/l^\gamma$.
% as mentioned in \textbf{Point 2} in Section \ref{subsection: points to achieve in synthetic dataset}.



\subsection{Eigen Values in Intrinsic Space vs. Intrinsic Dimension}
\label{subsection: synthetic dataset point 2} 
In this subsection, we train a model with a specialized architecture, allowing us to use the feature representation of a middle layer as a feature vector for input context bits, as shown in Figure \ref{fig:synthetic dataset model}. 
\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/syntheticDatasetmodel.pdf}
  \caption{Model trained on the proposed synthetic dataset; $\oplus$ represents feature concatenation. Only the first $l$ bits are used as input to context MLP when the context length is set to $l$. We conduct PCA on the Context Feature to analyze the intrinsic dimension of input context bits for various context lengths.}
  \label{fig:synthetic dataset model}
\end{figure}

After training the model on data with different context length, we obtain the context feature representation and conduct PCA on it to study the Intrinsic Space of this model, presented in Figure \ref{fig:synthetic dataset, eig-val vs. idx}.

From Figure \ref{fig:synthetic dataset, eig-val vs. idx} we find that: (1) the neural network would indeed learn the key information in the context bits. For models with different input context lengths, although their inner dimensions are the same ($80$), the representation of inputs in this inner space mainly lies in the first $ID$ dimensions, and the eigen values corresponding to other dimensions are very small; and (2) there exists such threshold $y^*$ that would estimate $ID$ for all context lengths accurately. In detail, using $y^*$ as the threshold to estimate $ID$ means that we find $idx\ s.t.\ rela\_eig(idx)>y^*$ and $rela\_eig(idx+1)<y^*$. We label a rectangle to show the possible $y^*$ for each context length and show that there exists $y^*$ to provide accurate estimations in Figure \ref{fig:synthetic dataset, eig-val vs. idx}.

\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/syntheticdataset_IDmeasure.pdf}
  \caption{Illustration of relative eigen value vs. index, for models trained on different context length. Vertical lines: $x_l=ID(l)$ and $x_l'=ID(l+1)$. For example, a context length $27$ has $16$ subtasks visible, corresponding to $16$ bits in Intrinsic Space. Assuming $1$ dimension in Intrinsic Space represents $1$ bit, the leftmost purple rectangle drawn means a range of $y_{threshold}$ that would provide an accurate estimation of $ID(27)=16$ for context length $27$. Obviously, there exists $y^*$ that would provide an estimation of $ID$ for all context lengths, as shown in the figure.}
  \label{fig:synthetic dataset, eig-val vs. idx}
\end{figure}

This provides evidence for \textbf{Point 2} in Section \ref{subsection: points to achieve in synthetic dataset}: we can take some threshold $y^*$ to estimate the intrinsic dimension, by obtaining the maximum index of the relative eigen value such that the relative eigen value is larger than $y^*$, which would give accurate and consistent estimates.

The linear relationship between CE Loss and Intrinsic Dimension can also be explained from an Entropy in Intrinsic Space perspective, which is discussed in \textbf{Appendix \ref{app: Entropy discussions}}.

\subsection{Optimal Context Length and Training Dataset Size: Synthetic Dataset}
\label{subsection: synthetic dataset point 3}

To validate the idea that longer context lengths are suitable for larger datasets, we conduct experiments on our proposed synthetic dataset with a fixed size MLP. During our experiments, we change the dataset size and context length used, and obtain validation Binary Cross Entropy loss vs. Context Length, as shown in the lower figure in \textbf{Figure \ref{fig: intro fig}}. (We also plot $y=-\log (1/2)$ as the loss of random guess in \textbf{Figure \ref{fig: intro fig}}.)

% \begin{figure}[h]
%   \includegraphics[width=\columnwidth]{figures/syntheticdataset_ValLoss_Ctl.pdf}
%   \caption{\textbf{synthetic Dataset}: Validation Loss vs. Context Length, for different training dataset sizes. Each curve represent the result obtained with a fixed training dataset of certain size. We also show $y=-\log(1/2)$ as the BCE loss of random guess.}
%   \label{fig:synthetic dataset: exp on optimal context and dataset size}
% \end{figure}

From Figure \ref{fig: intro fig}, we see that models with longer context length need larger datasets to train, and training long-context models on small datasets could lead to worse performance or even failure to improve from the first step (performance equal to random guess). Moreover, for each curve that represents a certain size of the training dataset used, we observe an optimal horizon beyond which validation loss would increase. This critical point moves right (optimal context length increases) with the increase of the training dataset size. This proves \textbf{ point 3} in Section \ref{subsection: points to achieve in synthetic dataset}.





% \subsection{Case 2: Training given fixed Compute}

% This subsection is deprecated.

% Consider Linear Transformers or RNN models where:

% \begin{equation}
% \text{Compute} \propto lND.
% \end{equation}

% To simplify the deduction we view $A(l)$ and $\alpha(l)$ as constant:

% \begin{equation}
% L_{CE} = C_0 + \frac C {l^\gamma} + \frac {A}{D^{\alpha}}+\frac{B}{N^\beta}.
% \end{equation}

% We derive that:

% \begin{equation}
% \begin{aligned}
%     N_{opt} &\propto \text{Compute}\verb|^| \{1/(1+\alpha/\beta+\alpha/\gamma)\},\\
%     D_{opt} &\propto \text{Compute}\verb|^| \{1/(1+\beta/\gamma+\beta/\alpha)\},\\
%     l_{opt} &\propto \text{Compute}\verb|^| \{1/(1+\gamma/\alpha+\gamma/\beta)\}.
% \end{aligned}
% \end{equation}

% (When $\gamma\rightarrow\infty$ these go to the Chinchilla Scaling Laws\cite{chinchilla}.)

% We derive that extra compute could be used to enlarge the context length, and optimal $D$ and $N$ would grow slowlier than the Chinchilla Scaling Laws.

% For typical Transformers models where $\text{Compute}\propto l^2ND$, simply substituting $\gamma$ with $\gamma/2$ and $l_{opt}$ with $l_{opt}^{2}$ would work.


\section{Related Work}

\subsection{Enlarging Context Length for LMs}

Previous work has made attempts to enlarge the context length of Language Models. Work represented by RoPE\cite{rope} uses rotary positional embedding to support generalizing LMs to longer context in inference compared to the training process. These work uses modified positional embeddings to model the relative position dependency in attention mechanism.

There is also work about enhancing long context understanding and exploring Scaling Laws for context length\cite{longllamascaling}. These work utilize an adjusted pretraining and instruction-finetuning process with more long-context data to enhance the models' ability on long contexts.

Other work modifying architectures has also been proposed to enhance long context modeling or to simplify deployment of long context LLMs. For example, \cite{razorattention} proposes a training-free RazorAttention algorithm to largely compress the KV cache while maintaining performance unchanged.

Architectures and inference methods have been proposed to reduce inference time and memory cost for Language Models, represented by a series of linear transformer or RNN-based methods.\cite{linearattention, mamba, tttllm}. These methods, largely reducing the computational cost and memory usage for long input contexts, have displayed margin ahead of traditional attention-based langauge models for long context inference.


Currently a common practice to train very large Language Models supporting long context is to use pretrain the model with shorter contexts, then finetune them with longer contexts, as presented in tech reports of LLaMa-3\cite{llama3} and DeepSeek-v3\cite{deepseekv3}.

\subsection{Irrelevant Long Context hurts performance of LMs}

Besides context length scaling with relevant contexts, previous researches have studied how LLMs perform for long irrelevant contexts. As an example, \cite{sametaskmoretoken} studies the performance of current LLMs on an adjusted version of `needle in a haystack task, where two pieces of key information are embedded into a long text corpora and a question related to both is asked, similar to that presented in Figure \ref{fig:key token example}. The conclusion of these work is that LLMs would perform worse when there is too much irrelevant information.

\subsection{Long Context in another field: Time Series Forecasting}

Context length, representing the length of input context, is not unique to Nature Language. For time series forecasting, where machine learning plays an important row, there is also work discussing the impact of context length, represented by \cite{scalingtimeseries}. These investigations find that there exists an optimal look-back horizon, which increases with dataset size. However, time series datasets are relatively small compared to NLP datasets, and thus whether this conclusion holds on NLP remains an open problem for this work to study.

\subsection{Related Theories for Scaling Laws}

Since the discovery of Scaling Laws for Large Language Models \cite{openaiscaling} or even earlier, there has been theoretical work trying to explain why model performance could benefit from more data points and more model parameters. For exmaple, \cite{neuralscalingdatamanifold} studies the dataset and model scaling from the data manifold perspective.

Specially for Language Models, there is also previous work proposing all kinds of theoretical models. For example, \cite{michaud2024quantizationmodelneuralscaling} proposes a feature-quant based theory; \cite{aghajanyan2020intrinsicdimensionalityexplainseffectiveness} views the effect of fine-tuning from the intrinsic dimension perspective; \cite{havrilla2024understandintrinsicdim} proposes to understand scaling with intrinsic dimensions. 



\section{Conclusion and Discussions}

\subsection{Conclusion}

In this work, we discuss the impact of context length on language modeling, especially Bayes risk and approximate loss, from both a theoretical and experimental perspective.

In Section \ref{sec: theory}, we propose assumptions related to the relationship between \textbf{CE Loss}, \textbf{intrinsic dimension} and \textbf{context length}. We derive a linear relation between CE loss and Intrinsic Dimension, and study the impact of context length to intrinsic dimension. We further investigate the relationship between intrinsic dimension, context length, and Entropy in intrinsic space in the appendix \ref{app: Entropy discussions} from an Entropy perspective.

We also conduct experiments on real data (Section \ref{sec: theory}, Section \ref{sec: Optimal Context Length on LMs}) and synthetic data (Section \ref{sec: synthetic dataset}), on measuring Intrinsic Dimension and on the relationship between Cross Entropy Loss (Bayes Risk and Approximation Loss), Context Length and Intrinsic Dimension.

As a correlation of our theory, for the training-till-overfitting setting, there exists an optimal context length that increases with dataset size in the pretraining process. This is also validated in Section \ref{sec: Optimal Context Length on LMs} and Section \ref{subsection: synthetic dataset point 3}.

We hope our work may provide insight for future work about long context Language Models, or about Physics for Language Models.


\subsection{Limitations and Future Work}

In Section \ref{sec: Optimal Context Length on LMs} we mainly discuss the case for pretraining with long context, but common practice is to train with shorter contexts and then do long-context-training with longer contexts \cite{deepseekv3}. We leave these to future work.

Our theory starting from Intrinsic Dimension only holds with assumptions in Section \ref{sec: theory}; and in Appendix \ref{app: Entropy discussions} we use the perspective of Entropy and possible states number in Intrinsic Space to (partially) explain our assumptions and measurements w.r.t. Intrinsic Dimension. We hope future work may try to propose even more fundamental theories to explain our Intrinsic Dimension measurements and assumptions.

Though not discussed in detail in this paper, there are problems that could be potentially explained by our theoretical framework. For example, in the measurement conducted in this work, the Intrinsic Dimension with increasing context length converges to a finite point, which could potentially explain why RNN models\cite{mamba,rwkv,tttllm} with limited hidden state can be good Language Models.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Statement for Possible Impact}

This paper presents work whose goal is to advance the field of Machine Learning (or more specifically, about understanding the impact of long contexts in Language Modeling). There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

% \textbf{This is special for ICML 25'. Not seen in 24' template.}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.



% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Definition of Cross-Entropy loss used in this work}
\label{app: CE}

It is well-known that

\begin{equation}
    \begin{aligned}
        H_{org}(P,Q)=\sum_x& -P(x)\log Q(x)\\
              =\sum_{x}& -P(x_{-\infty:0})P(x_0|x_{-\infty:0})\\
              &*\log \{Q(x_0|x_{-\infty:0})Q(x_{-\infty:0})\},
    \end{aligned}
\end{equation}

where $x_{a:b}$ denotes $x_{a},x_{a+1},\ldots,x_{b-1}$.
Although this should be the definition of $H(P,Q)$, it is common practice to calculate perplexity of Language Models with its input as GT lables (e.g. in technical report of LLaMa-3\cite{llama3}), in other words,

\begin{equation}
    \begin{aligned}
                &H_{exp}(P,Q)\\
                =&\sum_x -P(x)\log Q(x)\\
                =&\sum_{x} -P(x_{-\infty:1})\log \mathbf{\{}Q(x_0|x_{-\infty:0})\mathbf{P(x_{-\infty:0})}\mathbf{\}}\\
                =&\text{Const}+E_{x_{-\infty:0}}\sum_{x_0}-P(x_0|x_{-\infty:0})\log Q(x_0|x_{-\infty:0}).
    \end{aligned}
\end{equation}

Therefore, in this work we use:
\begin{equation}
    \begin{aligned}
                &H(P,Q)\\
                =&\text{E}_{x_{-\infty:0}}[\sum_{x_0}-P(x_0|x_{-\infty:0})\log Q(x_0|x_{-\infty:0})]
    \end{aligned}
\end{equation}
as the definition of Cross-Entropy loss, and $P(x_0|x_{-\infty:0})$, $Q(x_0|x_{-\infty:0})$ as the definition of Nature Language distribution and Language Model distribution, respectively.
\section{Detailed Derivation for KL distance in Section \ref{sec:Bayesian Theory}}
\label{app:derive for KL distance for P and P_l}

Based on the definition in Appendix \ref{app: CE}, we derive $D_{KL}=(dim(\infty)-dim(l))* S$ here.

\begin{equation}
    \begin{aligned}
        &D_{KL}(P,P_l)\\
        &=E_{x_{-\infty:0}} \sum_{x_0} D_{KL,x_0}(P(x_0|x_{-\infty,0}), P_l(x_0|x_{-\infty,0}))\\
        &=(dim(\infty)-dim(l))*S
    \end{aligned}
\end{equation}

This is because we assume that each dimension in the intrinsic space can store $S$ bits of information.

Another intuitive example is: assuming that the vocab is an integer from $0$ to $2^{dim(\infty)*S}-1$. assuming $P(x_0|x_{-\infty:0})=\delta_{x_0,y}$, that is, the next token given $x_{-\infty:0}$ is sure to be $y$. $y$. For $P_l(x_0|x_{-\infty:0})$, the first $dim(l)S$ digits of the integer (in binary representation) are known, but the remaining $(dim(\infty)-dim(l)) S$ digits are unknown, making a guess in these numbers yield $P_l(x_0|x_{-\infty:0})=1/2^{S*(dim(\infty)-dim(l))}$. Thus, $D_{KL,x_0}(P(x_0|x_{-\infty,0}), P_l(x_0|x_{-\infty,0}))=1*\log 1/(1/2^{S*(dim(\infty)-dim(l))})=S*(dim(\infty)-dim(l))$.
\section{Entropy perspective: hyper-volume in Intrinsic Space}
\label{app: Entropy discussions}
\subsection{Bayes Risk from an Entropy-in-Intrinsic-Space perspective}
Here we derive similar results as in Section \ref{sec:Bayesian Theory}, but not from an Intrinsic-Dimension perspective, rather we derive it from an Entropy perspective.

\begin{itemize}
    \item Assumption 1. Entropy of the Bayes Model $\lim_{l\rightarrow\infty}H_{ntp}(l)=H_{ntp}(\infty)$ is finite, which is the Entropy of next token prediction of language itself.
    \item Assumption 2. $\forall l_1,l_2\text{ such that } l_1<l_2$, $H_{ntp}(l_1)<H_{ntp}(l_2)$. This is because a longer context contains more information about the next possible token.
\end{itemize}

To simplify deduction, we further assume that,
\begin{itemize}
    \item Assumption 3. ${D_{KL}}_{,ntp}(P,P_l)\approx H_{ntp}(P)-H_{ntp}(P_l)+Const$. This means that the KL divergence between the Bayes LM of infinite context length and the Bayes LM of context length $l$ can be written approximately in the form of entropy difference.
    \item Assumption 4. The Entropy w.r.t. Next Token Prediction, i.e., $H_{ntp}(P_l)$, is linear with the Entropy in the Intrinsic Space of the Bayes Model, i.e., $H_{IS}$.
    
    
    % Each intrinsic dimension can store $s$ bits of information, so in all there are $dim(l)*s$ bits of information that can be represented in $\text{space}_l$.
    % \item Assumption 4. KL-divergence for Bayes Model of context length $l$, $P_l$, with Bayes Model of infinite context length, $P=P_{\infty}$, equals to $s*(dim(\infty)-dim(l))$.
\end{itemize}


To establish a relationship between Cross Entropy and Intrinsic Space, we run LLaMa-3-8b on a subset of the Openwebtext dataset and obtain the feature of the last token as the feature representation, or Intrinsic Space of the approximated Bayes Model, as shown in the left part of Figure \ref{fig:mid feature, opwbtxt 8B in app}. We see that the \textbf{ model with larger context length tends to have larger relative eigenvalues in intrinsic space, thus containing more information}.

\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/eig_val_and_increment.pdf}
  \caption{\textbf{Left}: \textbf{Relative Eigen Value} Measured for the last token, for LLaMa-3.1-8B on a subset of OpenWebText. \textbf{Right}: relative increment of relative eigenvalues (for different context lengths measured). We can see that the relative eigenvalues approximately increase at a same scale.}
  \label{fig:mid feature, opwbtxt 8B in app}
\end{figure}

Entropy of a system can be defined as $H=\log \Omega$ where $\Omega$ is the possible number of states of the system. Similar to the calculation process of Entropy in Statistical Physics\cite{landau1980statistical}, we define Entropy of Intrinsic Space as:
$$
\begin{aligned}
        &H_{IS}\\
        =& \log \Omega\\
           =& \log V/h^{dim(V)} \text{ where $V$ is the volume in intrinsic space}\\
           =& \sum_{i} \log rel\_eigval_i/h\\
           =& \sum_i \log rel\_eigval_i + Const
\end{aligned}
$$

Here $h$ is the `plank constant', meaning that one state corresponds to a unit hyper-volume of $h^{dim}$ in the Intrinsic Space. A different value of $h$ would only add a constant to $H_{IS}$ and would not affect change in Entropy. Thus, we use $\sum_i \log rel\_eigval_i$ as Entropy in Intrinsic Space. Moreover, experiments in Figure \ref{fig:mid feature, opwbtxt 8B in app} show that relative eigenvalues increase in the same ratio as the context length increases. This means \textbf{the number of states in any subspace would scale proportionally with respect to the number of states in the whole space}, that is: $H_{subspace} = H_{IS}*dim(subspace)/dim(IS)$, hence \textbf{Entropy of subspaces are linear with respect to each other.}

Though related, Entropy in Intrinsic Space does not equal to Entropy in the next token prediction task. From the probability perspective, let $dec(x)$ be the next decoded token for some point $x$ in the intrinsic space, we have: $H_{IS}=\sum_{x\in IS}-P(x)\log P(x)$, while $H_{ntp}=-\sum_{v\in vocab}P(v)\log P(v)$ where $P(v) = \sum_{x\in IS, dec(x)=v} P(x)$. $H_{ntp}$ is a coarse-grained Entropy compared to $H_{IS}$. $H_{IS}$ contains important information on previous tokens that are important for the prediction of future tokens, while $H_{ntp}$ is related only to the next token.

Experiments in Figure \ref{fig:CE vs ctl opwbtxt 8B in app} show that, no matter what subspace we use, the Cross Entropy Loss usually follows a linear relationship with the Entropy we measured in the subspace, \textbf{supporting the claim that the next token prediction task likely lies in some subspace of the Intrinsic Space, or (statistically) its Entropy should be some weighted average of Entropy of several subspaces of similar dimension.}. This also suggests that $H_{ntp}$ is approximately linear with $H_{IS}$, which validates our previous assumptions and claims.


\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/CEvsEntropy_and_corr.pdf}
  \caption{Upper-left, Upper-right, Bottom-left: \textbf{Cross Entropy Loss} vs. $\sum_{i\leq N}\log$ rel\_eig\_val; Bottom-right: correlation between minus CE loss and $\sum_{i\leq N} \log$ rel\_eig\_val. All experiments are for LLaMa-3.1-8B on a subset of OpenWebText. From the first three figure, we see CE loss is linear with the Entropy of certain subspaces. From the bottom-right figure, we see that Entropy measured in different subspaces are highly correlated ($corr > 0.97$), which are also highly correlated with the CE loss for Next Token Prediction.}
  \label{fig:CE vs ctl opwbtxt 8B in app}
\end{figure}

\subsection{Bridging the gap between Intrinsic Dimension explanation and Entropy explanation}

Here, starting from previous assumptions and measurements w.r.t. Entropy in Intrinsic Space, we explain why CE is linear w.r.t. Intrinsic Dimension measured in Section \ref{sec:Bayesian Theory}, for $idx>500$. We see in Figure \ref{fig:mid feature, opwbtxt 8B} that for $idx>500$, the eigenvalues mainly follow an exponential decay:

$$
releigval_{ctl,\ idx} = releigval_{ctl,\ 0}*\exp\{-\alpha_{ctl}*idx \}\text{, for certain context length}
$$

We also now from the previous result that for different context lengths, the relative eigenvalues increase almost in the same proportion, especially for $idx>1000$. That is, $\alpha_{ctl}=\alpha$, and $releigval_{ctl,\ idx}=\gamma(ctl)*releigval_{\infty,idx}$.

For the subspace for the next token prediction task, the entropy should be proportional to log of volume in the subspace; hence it should be proportional to $m*\log\gamma(ctl)$ where $m$ is the dimension of this exact subspace. That is: 
\begin{equation}
H_{subspace}(ctl)=m\log \gamma(ctl)+Const\label{eq: H=mloggamma}
\end{equation}

For some certain threshold $thres$, we use, for some context length, the measured intrinsic dimension is:

$$
releigval_{\infty,0}*\gamma*\exp\{-\alpha*dim(ctl,thres)\} = thres,
$$

hence $\gamma(ctl)=thres/releigval_{\infty,0}*\exp\{\alpha*dim(ctl,thres)\}$. Plugging this into Equation (\ref{eq: H=mloggamma}) we have:

\begin{equation}
CE=-H_{subspace}(ctl)+Const = -m\alpha*dim(ctl,thres)+Const(thres).\label{eq:CE=-malphadim+const}
\end{equation}

Thus, we derive our assumptions in Section \ref{sec: theory}, where $s=m\alpha$. Equation (\ref{eq:CE=-malphadim+const}) can also be validated in the fourth part of Figure \ref{fig:CE vs ctl opwbtxt 8B}, where the measured Intrinsic Dimensions (for $idx\geq500$) are measured in the exponential decay area, and share similar slopes w.r.t. CE Loss. 

\subsection{Synthetic dataset: Entropy in Intrinsic Space, and Entropy for output layer}

For our synthetic dataset, if we view the \textbf{Context Feature Vector} shown in \textbf{Figure \ref{fig:synthetic dataset model}} as the feature in the Intrinsic Space, then the best strategy for the context encoder is to generate the answer for all subtasks (it can see) in the Intrinsic Space (since it cannot see the task bits). This would lead to an Entropy of $H_{IS}=T\log 2$ in the Intrinsic Space.

The entropy of the output layer is, however, $H_{output}=\log 2$ since the answer bits $0,1$ have the same probability. In this way, the answer of the output layer actually corresponds to one dimension in the Intrinsic Space, which should be the exact dimension at which the answer of the current task is stored. Therefore, $H_{output} = 1/T*H_{IS}$, which explains why the Entropy for output logits is linear to the Entropy for Intrinsic Space.

\subsection{New Synthetic dataset: further experiments for Entropy}

In the main paper, we use $T<ctl$, where the task number is smaller than the context bits. Here, we further study the case where $T>ctl$. Moreover, instead of letting tasks show at the same frequency, we use weighted frequency proportional to $max(bit_1,bit_2)^{-1.2}$ as the weights of tasks.


Here we train a single MLP (instead of multiple MLPs) as Bayes Model for different context lengths. The architecture is like:

\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/toymodel_entropy.pdf}
  \caption{New MLP trained with mask bits. For the example shown, the context length is $7$ and we mask the other task bits to be $0.5$.}
  \label{fig: mlp trained with mask bits}
\end{figure}

We use $100$ tasks and $60$ task bits. From $11th$ to the $60th$ bit, each bit corresponds to the max bit of two tasks: that is, $\#Task|_{max(bit_1,bit_2)=i}=2,\forall i\in\{11,12,\ldots,60\}$.

During training, $50\%$ of the samples are unmasked, while for the other $50\%$ samples, we mask the last $X$ task bits to be $0.5$, where $X$ is a random int from $60-10$ to $60-60$. This ensures our model to be able to handle mask bits, and also ensures it can learn uncommon tasks (relying on context bits that are at the end of the context bits) well.

The context bits MLP has hidden size $400$, and the prediction MLP has hidden size $300$. The linear embedding and the context feature have the same dimension $200$, and are added before being sent to the predict MLP. LeakyRELU with leaky hyperparameter $1e-3$ is used as an activation function for both MLPs. We train the model on a training set of $10000000$ and a validation set of size $1000000$, for $125$ epochs (and an early stopping setting of $25$ epochs, though the training process did not trigger early stopping).


To make sure that the trained model can be used to approximate the Bayes Model, we compare the model's loss on validation set with context $ctl$ with the calculated minimum possible CE Loss for the task:

$$
MinCELoss(ctl) = \frac{\sum_{\text{task s.t. $max(bit_1,bit_2)>ctl$}}P(task)*\log 2}{\sum_{task}P(task)}
$$

And we obtain such a result:

\begin{table}[h]
\begin{center}
\begin{tabular}{c|cc}
\toprule
Context Length &Model CE Loss & Minimum CE Loss Calculated \\ % & \#char_{in}^{avg}& \#char_{out}^{avg}\\
\midrule
15             &0.531&0.516\\%&-&- \\
17             &0.478&0.464\\%&-&- \\
20             &0.408&0.399\\%&-&- \\
23             &0.353&0.344\\%&-&- \\
25             &0.320&0.312\\%&-&- \\
28             &0.276&0.269\\%&-&- \\
30             &0.250&0.243\\%&-&- \\
35             &0.193&0.186\\%&-&- \\
40             &0.154&0.139\\%&-&- \\
50             &0.082&0.061\\%&-&- \\
60             &0.051&0.0\\%&-&- \\
\bottomrule
\end{tabular}
\end{center}
\caption{Comparison between trained model and Bayes Model for Synthetic Data}
\label{table: comparison between trained model and Bayes Model in theory}
\end{table}

We can see from Table \ref{table: comparison between trained model and Bayes Model in theory} that the model is not too different from the Bayes Model: the BCE Loss only differs by around $0.02$. \textbf{Thus, we can use the middle-layer-representation (shown as context feature in Figure \ref{fig: mlp trained with mask bits}) as the feature in Intrinsic Space to approximate the Bayes Model for $17 \leq ctl \leq 50$.}
\newpage
\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/syntheticdataeigvalueresult_entropy.pdf}
  \caption{Eigen value and CE results measured on trained model for Synthetic Dataset in this section. Left: relative eigen value vs. Index. Middle: Model CE Loss vs. $\sum_{i}\log rel\_eigval_i$ and Theoretical Min CE Loss vs. $\sum_{i} \log rel\_eigval_i$. Right: Correlation between minus Experimentally measured model CE Loss, minus Theoretically Minimum CE Loss, and $\sum_{i\leq N} \log rel\_eigval_i$ for different $N$s. All correlation factors are larger than $0.97$.}
  \label{fig: eigen value results for synthetic dataset for Entropy results}
\end{figure}
We show our results in Figure \ref{fig: eigen value results for synthetic dataset for Entropy results}. We see from Figure \ref{fig: eigen value results for synthetic dataset for Entropy results} that: (1) Larger context length contains more information, hence eigen values in Intrinsic Space degrades slowlier (left figure); (2) the model approximates the theoretical Bayes Model well (as the green points in the middle figure is very close to the orange ones) (middle figure); (3) CE Loss follows a very good linear relationship with sum of log eigenvalues of the first $N$ dimensions for $N\geq 70$ in the Intrinsic Space (right figure), where the case $N=200$ (all eigen values) are also shown in the middle figure.


\section{More experiments of LLaMa on another dataset}
\label{app: anotherdatasetllama3}

According to the technical report of LLaMa 3\cite{llama3}, the text corpora with number of `dirty words' beyond certain threshold would be filtered out, as proposed in \cite{T5}.
% @misc{nsfwdataset,
%   author = {bluuwhale},
%   title = {nsfwstory},
%   year = {2024},
%   publisher = {Hugging Face},
%   howpublished = {\url{https://huggingface.co/datasets/bluuwhale/nsfwstory}}
% }
We collect some text corpora online which include forbidden words defined in \cite{T5}, as text corpora unseen by LLaMa 3. By conducting experiments on it we obtain results similar to Openwebtext subset.

\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/llamansfwstory_2.pdf}
  \caption{Cross Entropy Loss vs. Context Length, with log scale. We see that $y=C_0+C/x^\gamma$ fits this curve well.}
  \label{fig: llama 3.1 on nsfwStory dataset}
\end{figure}

According to Figure \ref{fig: llama 3.1 on nsfwStory dataset}, we see that $CE=C_0+C/l^\gamma$ approximates well for text corpora that are sure not to be seen by the model.

\section{Construction of the Synthetic dataset}
\label{app: special case in synthetic dataset}

If the subtasks defined in Section \ref{subsection: synthetic dataset definition} is independent with each other, and number of subtasks visible is smaller than number of context bits visible, then the intrinsic dimension should equal to the number of subtasks: we need $T$ bits to store the required information of these subtasks. However, in special cases when different tasks are too dependent with each other, then the number of bits needed to represent the answer is $4$ instead of $T=5$; for example: if we have $4$ context bits, and $T=5$ tasks dependent of context bits $\{(1st,2nd),(1st,3rd),(2nd,3rd),(3rd,4th),(1st,4th)\}$ respectively, then the Intrinsic Dimension should be $4$ instead of $5$. This would make Intrinsic Dimension less than the number of tasks defined.

To avoid this we carefully tune the bits dependency of our tasks defined, and list them as follows:
$$
\begin{aligned}
    \{&(21,20),(21,1),(21,2),(22,3),(22,4),(22,5),\\
    &(23,6),(23,7),(24,8),(24,9),(25,10),(25,11),\\
    &(26,12),(26,13),(27,14),(27,15),(28,16),\\
    &(29,17),(30,18),(30,19),(31,20),(32,1),(33,3),\\
    &(34,6),(35,8),(36,10),(37,12),(39,14),(40,16),\\
    &(42,38),(43,41),(45,44),(47,46),(50,48),(52,51),\\
    &(55,54),(58,57),(62,61),(66,65),(71,70),(77,76),\\
    &(84,83),(93,92),(103,102),(118,87),(137,25),\\
    &(165,20),(209,34),(293,128),(522,353)\}
\end{aligned}
$$

One can check that the task defined here has an intrinsic dimension for any model of context length $l\geq 23$ equal to the number of subtasks visible for that specific context length.
\section{Experiment Settings}
\label{app: experiment settings}
\subsection{Natural Language Data}
\subsubsection{Optimal Context Length Experiments}
We use aanogpt\cite{nanogpt} and train a model with GPT-2\cite{gpt-2} architecture on a subset of OpenwebText dataset. Our model is the same with GPT-2-124M ($12$-head transformers, $768$-dim feature vector) except that it uses half the transformers layer size ($12\rightarrow6$) to reduce GPU memory for long contexts. For training, we use the AdamW\cite{adamw} optimizer, an equivalent batch size of $480$ learning rate of $6e-4$, weight decay of $1e-1$, $2000$ warm-up iterations and $600000$ lr-decaying iterations. We train until overfitting, that is, we validate the validation loss of our model on the validation set every $45$ steps, and choose the best iteration as the min validation loss measured for a single run.

We train the model on a subset of OpenWebText. To be specific, we first select text corpora with context length beyond specific limits larger than the maximum training context length from OpenWebText, then split into Training set and Validation Set. The training set we used to train the models have $12M,25M,50M$ tokens respectively, and the validation set has $134M$ tokens.

Each point in the corresponding figure in Figure \ref{fig: intro fig} and Figure \ref{fig: exp on optimal context and dataset size} is produced with $8$ AMD MI-250X GPUs (which are similar in performance to Nvidia A100 gpus) trained for $2$ days.
\subsubsection{Intrinsic Dimension Experiments}

We select long enough text corpora from the Openwebtext dataset. Then, following previous practice\cite{bridginginformationtheorymeasureintrinsicdimensionlanguagemodels}, we conduct experiments with LLaMa-3.1-8b on $10000$ samples of this subset. We extract the feature representation of the last token in the last layer, as the Intrinsic Representation of samples.

Conducting all intrinsic dimension measurements cost up to around $100$ gpu hours for MI-250X gpus.
\subsection{Synthetic Dataset}
\subsubsection{Settings in Section \ref{subsection: synthetic dataset point 1} and \ref{subsection: synthetic dataset point 3}}

We use a validation dataset size of $2000000$. Training dataset size is $2000000$ in Section \ref{subsection: synthetic dataset point 1}, and varies as shown in Figure \ref{fig: intro fig} in Section \ref{subsection: synthetic dataset point 3}.

We use a large enough MLP on large enough datasets. To be specific, our MLP has four linear layers and two leaky-relu activation layers. The input has shape $context\_length+taskbits\_length$, and output has shape $1$. The hidden layer is of dimension $400$ and middle layer is of dimension $200$.
During training, we use the Adam\cite{kingma2017adammethodstochasticoptimization} Optimizer, batch size is $10000$, learning rate is set to $1e-3$, weight decay is $1e-4$ and we train until validation loss increases for $25$ epochs, with a maximum epoch number of $200$.

For Section \ref{subsection: synthetic dataset point 1}, these settings enable the model to be fully trained on enough data, to be viewed as a Bayes Model.
\subsubsection{Settings in Section \ref{subsection: synthetic dataset point 2}}

For models shown in Figure \ref{fig:synthetic dataset model}, we use two-layer mlp for both context feature encoding and logit decoding. The hidden sizes are $400$ and $200$, respectively, and the context feature dimension is set to $80$. The training rests are similar to the setting in Section \ref{subsection: synthetic dataset point 1}.

Similar training settings are used for training Synthetic Data for Appendix \ref{app: Entropy discussions}.

The experiments for Synthetic Data take around $10$ hours on an Nvidia RTX-3090 gpu.

\end{document}



% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
