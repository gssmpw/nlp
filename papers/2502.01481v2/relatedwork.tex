\section{Related Work}
\subsection{Enlarging Context Length for LMs}

Previous work has made attempts to enlarge the context length of Language Models. Work represented by RoPE\cite{rope} uses rotary positional embedding to support generalizing LMs to longer context in inference compared to the training process. These work uses modified positional embeddings to model the relative position dependency in attention mechanism.

There is also work about enhancing long context understanding and exploring Scaling Laws for context length\cite{longllamascaling}. These work utilize an adjusted pretraining and instruction-finetuning process with more long-context data to enhance the models' ability on long contexts.

Other work modifying architectures has also been proposed to enhance long context modeling or to simplify deployment of long context LLMs. For example, \cite{razorattention} proposes a training-free RazorAttention algorithm to largely compress the KV cache while maintaining performance unchanged.

Architectures and inference methods have been proposed to reduce inference time and memory cost for Language Models, represented by a series of linear transformer or RNN-based methods.\cite{linearattention, mamba, tttllm}. These methods, largely reducing the computational cost and memory usage for long input contexts, have displayed margin ahead of traditional attention-based langauge models for long context inference.


Currently a common practice to train very large Language Models supporting long context is to use pretrain the model with shorter contexts, then finetune them with longer contexts, as presented in tech reports of LLaMa-3\cite{llama3} and DeepSeek-v3\cite{deepseekv3}.

\subsection{Irrelevant Long Context hurts performance of LMs}

Besides context length scaling with relevant contexts, previous researches have studied how LLMs perform for long irrelevant contexts. As an example, \cite{sametaskmoretoken} studies the performance of current LLMs on an adjusted version of `needle in a haystack task, where two pieces of key information are embedded into a long text corpora and a question related to both is asked, similar to that presented in Figure \ref{fig:key token example}. The conclusion of these work is that LLMs would perform worse when there is too much irrelevant information.

\subsection{Long Context in another field: Time Series Forecasting}

Context length, representing the length of input context, is not unique to Nature Language. For time series forecasting, where machine learning plays an important row, there is also work discussing the impact of context length, represented by \cite{scalingtimeseries}. These investigations find that there exists an optimal look-back horizon, which increases with dataset size. However, time series datasets are relatively small compared to NLP datasets, and thus whether this conclusion holds on NLP remains an open problem for this work to study.

\subsection{Related Theories for Scaling Laws}

Since the discovery of Scaling Laws for Large Language Models \cite{openaiscaling} or even earlier, there has been theoretical work trying to explain why model performance could benefit from more data points and more model parameters. For exmaple, \cite{neuralscalingdatamanifold} studies the dataset and model scaling from the data manifold perspective.

Specially for Language Models, there is also previous work proposing all kinds of theoretical models. For example, \cite{michaud2024quantizationmodelneuralscaling} proposes a feature-quant based theory; \cite{aghajanyan2020intrinsicdimensionalityexplainseffectiveness} views the effect of fine-tuning from the intrinsic dimension perspective; \cite{havrilla2024understandintrinsicdim} proposes to understand scaling with intrinsic dimensions.