@article{10.1162/neco.1992.4.4.590,
    author = {MacKay, David J. C.},
    title = "{Information-Based Objective Functions for Active Data Selection}",
    journal = {Neural Computation},
    volume = {4},
    number = {4},
    pages = {590-604},
    year = {1992},
    month = {07},
    abstract = "{Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed that measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1992.4.4.590},
    url = {https://doi.org/10.1162/neco.1992.4.4.590},
    eprint = {https://direct.mit.edu/neco/article-pdf/4/4/590/812354/neco.1992.4.4.590.pdf},
}

@article{10.1214/23-STS915,
author = {Tom Rainforth and Adam Foster and Desi R. Ivanova and Freddie Bickford Smith},
title = {{Modern Bayesian Experimental Design}},
volume = {39},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {100 -- 114},
keywords = {Active learning, Bayesian adaptive design, Bayesian optimal design, information maximization},
year = {2024},
doi = {10.1214/23-STS915},
URL = {https://doi.org/10.1214/23-STS915}
}

@article{Andreopoulos_2013,
author = {Andreopoulos, Alexander and Tsotsos, John K.},
title = {A Computational Learning Theory of Active Object Recognition Under Uncertainty},
year = {2013},
issue_date = {January 2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {101},
number = {1},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-012-0551-6},
doi = {10.1007/s11263-012-0551-6},
abstract = {We present some theoretical results related to the problem of actively searching a 3D scene to determine the positions of one or more pre-specified objects. We investigate the effects that input noise, occlusion, and the VC-dimensions of the related representation classes have in terms of localizing all objects present in the search region, under finite computational resources and a search cost constraint. We present a number of bounds relating the noise-rate of low level feature detection to the VC-dimension of an object representable by an architecture satisfying the given computational constraints. We prove that under certain conditions, the corresponding classes of object localization and recognition problems are efficiently learnable in the presence of noise and under a purposive learning strategy, as there exists a polynomial upper bound on the minimum number of examples necessary to correctly localize the targets under the given models of uncertainty. We also use these arguments to show that passive approaches to the same problem do not necessarily guarantee that the problem is efficiently learnable. Under this formulation, we prove the existence of a number of emergent relations between the object detection noise-rate, the scene representation length, the object class complexity, and the representation class complexity, which demonstrate that selective attention is not only necessary due to computational complexity constraints, but it is also necessary as a noise-suppression mechanism and as a mechanism for efficient object class learning. These results concretely demonstrate the advantages of active, purposive and attentive approaches for solving complex vision problems.},
journal = {Int. J. Comput. Vision},
month = jan,
pages = {95–142},
numpages = {48},
keywords = {Visual search, Object recognition, Computational complexity of vision, Attention, Active vision}
}

@article{Arridge_Maass_Oktem_Schonlieb_2019, 
title={Solving inverse problems using data-driven models}, 
volume={28}, 
DOI={10.1017/S0962492919000059}, 
journal={Acta Numerica}, 
author={Arridge, Simon and Maass, Peter and Öktem, Ozan and Schönlieb, Carola-Bibiane}, 
year={2019}, 
pages={1–174}}

@article{Bajcsy_2018, 
title={Revisiting active perception}, 
volume={42}, 
journal={Autonomous Robots}, 
author={Bajcsy, R. and Aloimonos, Y. and Tsotsos, J.K.}, 
year={2018}, 
pages={177–196}}

@inproceedings{Bohm:2019hpu,
    author = {B\"ohm, Vanessa and Lanusse, Fran\c{c}ois and Seljak, Uro\v{s}},
    title = "{Uncertainty Quantification with Generative Models}",
    booktitle = "{33rd Annual Conference on Neural Information Processing Systems}",
    eprint = "1910.10046",
    archivePrefix = "arXiv",
    primaryClass = "stat.ML",
    month = "10",
    year = "2019"
}

@ARTICLE{Mackay_1992,
  author={MacKay, David J. C.},
  journal={Neural Computation}, 
  title={Information-Based Objective Functions for Active Data Selection}, 
  year={1992},
  volume={4},
  number={4},
  pages={590-604},
  keywords={},
  doi={10.1162/neco.1992.4.4.590}}

@book{Mockus_1989,
  author    = {Mockus, Jonas},
  title     = {{Bayesian Approach to Global Optimization}},
  year      = {1989},
  publisher = {Springer Dordrecht: Kluwer Academic}
}

@article{Saar_Tsechansky_2009,
author = {Saar-Tsechansky, Maytal and Melville, Prem and Provost, Foster},
title = {Active Feature-Value Acquisition},
year = {2009},
issue_date = {April 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {55},
number = {4},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.1080.0952},
doi = {10.1287/mnsc.1080.0952},
abstract = {Most induction algorithms for building predictive models take as input training data in the form of feature vectors. Acquiring the values of features may be costly, and simply acquiring all values may be wasteful or prohibitively expensive. Active feature-value acquisition (AFA) selects features incrementally in an attempt to improve the predictive model most cost-effectively. This paper presents a framework for AFA based on estimating information value. Although straightforward in principle, estimations and approximations must be made to apply the framework in practice. We present an acquisition policy, sampled expected utility (SEU), that employs particular estimations to enable effective ranking of potential acquisitions in settings where relatively little information is available about the underlying domain. We then present experimental results showing that, compared with the policy of using representative sampling for feature acquisition, SEU reduces the cost of producing a model of a desired accuracy and exhibits consistent performance across domains. We also extend the framework to a more general modeling setting in which feature values as well as class labels are missing and are costly to acquire.},
journal = {Manage. Sci.},
month = apr,
pages = {664–684},
numpages = {21},
keywords = {active feature acquisition, active learning, business intelligence, data mining, imputation, information acquistion, machine learning, predictive modeling, utility-based data mining}
}

@article{Whitehead_1990, 
title={Active perception and reinforcement learning}, 
  volume={2},
  number={4},
journal={Neural Computation}, 
author={Whitehead, Steven D. and Ballard, Dana H.}, 
year={1990}, 
pages={409–419}}

@article{doi:10.1137/21M1414978,
author = {Habring, Andreas and Holler, Martin},
title = {A Generative Variational Model for Inverse Problems in Imaging},
journal = {SIAM Journal on Mathematics of Data Science},
volume = {4},
number = {1},
pages = {306-335},
year = {2022},
doi = {10.1137/21M1414978},

URL = { 
    
        https://doi.org/10.1137/21M1414978
    
    

},
eprint = { 
    
        https://doi.org/10.1137/21M1414978

}
,
    abstract = { This paper is concerned with the development, analysis, and numerical realization of a novel variational model for the regularization of inverse problems in imaging. The proposed model is inspired by the architecture of generative convolutional neural networks; it aims to generate the unknown from variables in a latent space via multilayer convolutions and nonlinear penalties, and penalizes an associated cost. In contrast to conventional neural-network-based approaches, however, the convolution kernels are learned directly from the measured data such that no training is required. The present work provides a mathematical analysis of the proposed model in a function space setting, including proofs for regularity and existence/stability of solutions, and convergence for vanishing noise. Moreover, in a discretized setting, a numerical algorithm for solving various types of inverse problems with the proposed model is derived. Numerical results are provided for applications in inpainting, denoising, deblurring under noise, superresolution, and JPEG decompression with multiple test images. }
}

@book{garnett_bayesoptbook_2023,
  author    = {Garnett, Roman},
  title     = {{Bayesian Optimization}},
  year      = {2023},
  publisher = {Cambridge University Press}
}

@article{kossen2023active,
title={Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task},
author={Jannik Kossen and C{\u{a}}t{\u{a}}lina Cangea and Eszter V{\'e}rtes and Andrew Jaegle and Viorica Patraucean and Ira Ktena and Nenad Tomasev and Danielle Belgrave},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=Gbu1bHQhEL},
note={}
}

@inproceedings{lewis_2021_accurate,
title={Accurate Imputation and Efficient Data Acquisition with Transformer-based {VAE}s},
author={Sarah Lewis and Tatiana Matejovicova and Yingzhen Li and Angus Lamb and Yordan Zaykov and Miltiadis Allamanis and Cheng Zhang},
booktitle={NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},
year={2021},
url={https://openreview.net/forum?id=N_OwBEYTcKK}
}

@article{ma2019bayesian,
  title={Bayesian {EDDI}: Sequential Variable Selection with {B}ayesian Partial {VAE}},
  author={Ma, Chao and Gong, Wenbo and Tschiatschek, Sebastian and Nowozin, Sebastian and José Miguel Hernández-Lobato and Zhang, Cheng},
  journal={Workshop on Real-World Sequential Decision Making: Reinforcement Learning and Beyond at NeurIPS},
  year={2019},
  tag={PMI,SDM}
}

@InProceedings{pmlr-v139-foster21a,
  title = 	 {Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design},
  author =       {Foster, Adam and Ivanova, Desi R and Malik, Ilyas and Rainforth, Tom},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3384--3395},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/foster21a/foster21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/foster21a.html},
  abstract = 	 {We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of adaptive Bayesian experimental design that allows experiments to be run in real-time. Traditional sequential Bayesian optimal experimental design approaches require substantial computation at each stage of the experiment. This makes them unsuitable for most real-world applications, where decisions must typically be made quickly. DAD addresses this restriction by learning an amortized design network upfront and then using this to rapidly run (multiple) adaptive experiments at deployment time. This network represents a design policy which takes as input the data from previous steps, and outputs the next design using a single forward pass; these design decisions can be made in milliseconds during the live experiment. To train the network, we introduce contrastive information bounds that are suitable objectives for the sequential setting, and propose a customized network architecture that exploits key symmetries. We demonstrate that DAD successfully amortizes the process of experimental design, outperforming alternative strategies on a number of problems.}
}

@InProceedings{pmlr-v97-ma19c,
  title = 	 {{EDDI}: Efficient Dynamic Discovery of High-Value Information with Partial {VAE}},
  author =       {Ma, Chao and Tschiatschek, Sebastian and Palla, Konstantina and Hernandez-Lobato, Jose Miguel and Nowozin, Sebastian and Zhang, Cheng},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4234--4243},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR}}%,
  pdf = 	 {http://proceedings.mlr.press/v97/ma19c/ma19c.pdf},
  url = 	 {https://proceedings.mlr.press/v97/ma19c.html},
  abstract = 	 {Many real-life decision making situations allow further relevant information to be acquired at a specific cost, for example, in assessing the health status of a patient we may decide to take additional measurements such as diagnostic tests or imaging scans before making a final assessment. Acquiring more relevant information enables better decision making, but may be costly. How can we trade off the desire to make good decisions by acquiring further information with the cost of performing that acquisition? To this end, we propose a principled framework, named <em>EDDI</em> (Efficient Dynamic Discovery of high-value Information), based on the theory of Bayesian experimental design. In EDDI, we propose a novel <em>partial variational autoencoder</em> (Partial VAE) to predict missing data entries problematically given any subset of the observed ones, and combine it with an acquisition function that maximizes expected information gain on a set of target variables. We show cost reduction at the same decision quality and improved decision quality at the same cost in multiple machine learning benchmarks and two real-world health-care applications.}
}

