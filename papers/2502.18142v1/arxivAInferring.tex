
\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

%added CFH
\usepackage{graphicx}
\usepackage{amsmath}

% Useful packages
\usepackage{mathtools}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D_{KL}\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

%https://www.overleaf.com/learn/latex/How_to_Write_a_Thesis_in_LaTeX_(Part_3)%3A_Figures%2C_Subfigures_and_Tables#Subfigures
\usepackage{caption}
\usepackage{subcaption}

%https://www.overleaf.com/learn/how-to/Is_there_a_shortcut_for_commenting_out_several_lines_of_code_at_once%3F
\usepackage{comment}


%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmax}{\arg\!\max}
%\DeclareMathOperator*{\argmin}{arg\,min}
%\DeclareMathOperator*{\argmin}{\arg\!\min}
\usepackage{amssymb}
%$$ \mathbb{E}[X] = \sum_{i=1}^{n} x_i \cdot p_i $$

%https://www.overleaf.com/learn/latex/Algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{natbib}
\title{Bibliography management: \texttt{natbib} package}

\title{Actively Inferring Optimal Measurement Sequences}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

%\author{{Catherine~F.~Higham}^{[1,*]}\\
%\AND {Paul Henderson}^{[1]}\\
%\AND {Roderick Murray-Smith}^{[1]}\\
%\address{[1]}{[~School of Computing Science] \\ University of Glasgow, Glasgow, G12 8QQ, UK} \\
%\address{[*]}{catherine.higham@glasgow.ac.uk}}




%\address{$^{1}$~School of Computing Science University of Glasgow, Glasgow, G12 8QQ, UK, \\
%E-mail: Catherine.Higham@glasgow.ac.uk 
%}



\author{\name Catherine F. Higham \email Catherine.Higham@glasgow.ac.uk \\
      \addr School of Computing Science\\
      University of Glasgow, Glasgow, G12 8QQ, UK
      \AND
      \name Paul Henderson \email Paul.Henderson@glasgow.ac.uk \\
      \addr School of Computing Science\\
      University of Glasgow, Glasgow, G12 8QQ, UK
      \AND
      \name Roderick Murray-Smith \email Roderick.Murray-Smith@glasgow.ac.uk\\
      \addr School of Computing Science\\
      University of Glasgow, Glasgow, G12 8QQ, UK}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version

\newcommand{\pmh}[1]{\textcolor{violet}{[\textit{#1}]}}
%\newcommand{\cfh}[1]{\textcolor{blue}{\textit{#1}}}
\newcommand{\cfh}[1]{\textcolor{black}{{#1}}}
\begin{document}


\maketitle

\begin{abstract}

Measurement of a physical quantity such as light intensity is an integral part of many reconstruction and decision scenarios but can be costly in terms of acquisition time, invasion of or damage to the environment and storage. Data minimisation and compliance with data protection laws is also an important consideration. Where there are a range of measurements that can be made, some may be more informative and compliant with the overall measurement objective than others. We develop an active sequential inference algorithm that uses the low dimensional representational latent space from a variational autoencoder (VAE) to choose which measurement to make next. Our aim is to recover high dimensional data by making as few measurements as possible. We adapt the VAE encoder to map partial data measurements on to the latent space of the complete data. The algorithm draws samples from this latent space and uses the VAE decoder to generate data conditional on the partial measurements. Estimated measurements are made on the generated data and fed back through the partial VAE encoder to the latent space where they can be evaluated prior to making a measurement. Starting from no measurements and a normal prior on the latent space, we consider alternative strategies for choosing the next measurement and updating the predictive posterior prior for the next step. The algorithm is illustrated using the Fashion MNIST dataset and a novel convolutional Hadamard pattern measurement basis. We see that useful patterns are chosen within 10 steps, leading to the convergence of the guiding generative images. Compared with using stochastic variational inference to infer the parameters of the posterior distribution for each generated data point individually, the partial VAE framework can efficiently process batches of generated data and obtains superior results with minimal measurements.
\end{abstract}


\section{Introduction and overview}

In many circumstances, data collection incurs a cost. This cost may be in terms of acquisition time, invasion of or damage to the environment and storage. Identifying which, out of a range of data measurements, to collect next is potentially a valuable cost saving activity. Importantly, it also facilitates fast decision making \citep{10.5555/2074158.2074191}. Data minimisation is an important consideration for compliance with data protection laws worldwide \footnote{https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/}. 
%The data minimisation principle is expressed in Article 5(1)(c) of the GDPR and Article 4(1)(c) of Regulation (EU) 2018/1725, which provide that personal data must be "adequate, relevant and limited to what is necessary in relation to the purposes for which they are processed".
In defense situations, the requirement for covert human intelligence means that the act of taking measurements can also pose a security risk.  \footnote{https://www.gov.uk/government/publications/covert-human-intelligence-sources-code-of-practice-2022/covert-human-intelligence-sources-revised-code-of-practice-accessible}

%\subsection{Footnotes}

%Indicate footnotes with a number\footnote{Sample of the first footnote} in the
%text. Place the footnotes at the bottom of the page on which they appear.
%Precede the footnote with a horizontal rule of 2~inches.\footnote{Sample of the second footnote}


%https://secureprivacy.ai/blog/data-minimization-principles-in-privacy-laws-eu-us-global-perspectives

%Data minimization is one of the core privacy protection principles throughout the data protection laws worldwide. It requires businesses to collect data they truly need for processing, and nothing more.

%Businesses usually think that minimizing data collection and data retention means shooting themselves in the leg. Yet, what they do is create data exposure risks.

%Collecting and storing data that you don't need is a risk to your company. It leads to non-compliance, doesn't protect individual privacy, and entails unnecessary risks.

%Data minimization promotes the opposite.

%Data Minimization According to the EU GDPR
%Article 5(1)(3) of the General Data Protection Regulation (GDPR) of the EU states that personal data shall be "adequate, relevant, and limited to what is necessary in relation to the purposes for which they are processed (‘data minimisation’)".

%Recital 39 of the GDPR stipulates that the processing of personal data must be adequate, relevant, and limited to what is necessary for the intended purposes. The GDPR further explains that we should only process personal data if other means could reasonably fulfill the processing purpose.

In this work, our overall aim is to identify an optimal measurement sequence.
%In this work, 
We develop an active sequential inference algorithm that uses a low dimensional data representation to infer its high dimensional state conditional on partial measurement of that state. Reducing dimension allows for more efficient exploration of the space of state possibilities. The state and types of tasks we are primarily interested in are image/scene reconstruction and related classification tasks using photon based imaging and sensing technology such as a single pixel camera \citep{Higham_DLRTSPV_2018}. However, the method is applicable to a broader range of activities. %Our aim is a reasoning approach that can be easily adapted to different imaging and sensing systems. 

The overall aim is to customise a generative probabilistic model to provide the agent or user with different reconstruction scenarios conditional on partial measurements.
Given a suitably large dataset of task relevant data, an appropriate generative model is the variational autoencoder (VAE) \citep{DBLP:journals/corr/KingmaW13}. A VAE learns to encode data in the convenient form of a low dimensional multivariate Gaussian and to decode this representation back to data space. This provides a means to obtain different reconstructions through manipulation of a low dimensional latent space.
This neural network model is trained on relevant data to learn the underlying distribution of the training data and used to generate new data. A VAE comprises %In summary, the VAE provides 
an encoder (a map from data space to a low dimensional latent representative space) and a decoder (a map from the low dimensional space back to data space). To achieve this the VAE introduces latent variables and the objective of a VAE is to understand the true posterior distribution of the latent variables. A VAE accomplishes this by employing an encoder network to approximate the genuine posterior distribution with a learned approximation. Once trained, samples from the prior on the latent space can be pushed through the decoder in order to obtain new generated data. Similarly, samples from the posterior can be pushed through the decoder in order to obtain data conditional on the input data. Here we adapt the encoder to map partially measured data to the latent space. This necessitates creating a training set of partially measured data. %We show how this can be done efficiently using a custom random mask during training \pmh{this sounds dangerously trivial}. 
Once trained, the partial encoder and the original decoder are used by the algorithm to sequentially reason about the full state and choose the next measurement.  
The probabilistic model we are inferring over is capturing both the (approximate) data-generating process and the noisy measurement model.

The main idea of variational methods is to cast inference as an optimization problem \citep{Blei_2017}. Stochastic variational inference (SVI) \citep{JMLR:v14:hoffman13a} can be used to infer the posterior probability distribution for specific data and a given set of measurements but requires multiple computation steps and thus is prohibitively slow when required to estimate many possible measurement sets.
%The application of an inference network to predict local variational parameters which are shared across the dataset is referred to as amortized variational inference (AVI) \pmh{strange wording; in my mind the local parameters are the random variables (whose stats are pred by the NN), and the shared network weights are something else?}.
We propose a hybrid approach, in a similar spirit to \cite{pmlr-v80-kim18e} but novel in our context, to combine the strengths of VAE and SVI. We use the partial encoder to choose between patterns and integrate robust SVI when a pattern has been selected for inferring the posterior distribution parameters based on actual measurements. The algorithm is developed to actively select the next best measurement. It starts by pushing samples from the prior distribution on the latent space through the decoder to obtain candidate images. Possible measurements on these candidate images are estimated, forming an indexed set. The problem can be thought of as choosing the next measurement index from a set of possible measurement indexes. The partial encoder or SVI is used to characterise the posterior distribution and hence provide a score under that distribution for each possible measurement index for each candidate image. There are different ways to define this score. We consider two approaches. First, we choose the index with the highest average (over the candidate images) likelihood score. Second, we choose the index with the least uncertainty score. The aim is to develop a method that is flexible to the task context and measurement basis. In Section \ref{s:methodVAE} we describe our method for posterior inference given measurements. The active sequential measurement algorithm is outlined in Section \ref{s:methodalg}. An overview of the algorithm is provided in Figure \ref{fig:active}.

\begin{figure}[h]
\begin{center}
%\framebox[5.0in]{\includegraphics[width=1.0\textwidth]{figures/Slide1.PNG}}
\framebox[6.5in]{\includegraphics[width=1.0\textwidth]{figures/Algorithm0801.png}}
\end{center}
\caption{\label{fig:active} Active Learning. At step $k=0$, for each of $N$ starting points, sample $\mathbf{z}_i$ from the prior for step $k=0$, push $\mathbf{z}_i$ through the decoder to obtain a generated image $\hat{\mathbf{x}}_i$ and estimate possible measurements $\hat{\mathbf{y}}_i$ for each pattern not yet measured (e.g. $\hat{\mathbf{y}}_1$, $\hat{\mathbf{y}}_2$ and $\hat{\mathbf{y}}_3$ illustrated above). Use the partial encoder to approximate the posterior with probability distributions and estimate probability densities $q^1(\mathbf{z}_1|\hat{\mathbf{y}}_1)$, $q^2(\mathbf{z}_1|\hat{\mathbf{y}}_2)$ and $q^3(\mathbf{z}_1|\hat{\mathbf{y}}_3)$ (illustrated above). Select the pattern which maximises the chosen expression and take the measurement associated with this pattern. Update the measurement set and update the predictive prior for the next step. By repeating these steps, we move towards the target distribution $p(\mathbf{z}|\mathbf{x})$. }
\end{figure}

\section{Related work}

%Extra references \citep{Saar_Tsechansky_2009}, \citep{kossen2023active}, \citep{Bajcsy_2018}, \citep{Mackay_1992}, \citep{Andreopoulos_2013}, \citep{Whitehead_1990} and \citep{lewis_2021_accurate}.
%Related work could be strengthened. There are many related topics, including active feature acquisition [1], active multimodal acquisition [2], active perception [3], bayesian active learning [4], active-RL [5], active vision [6], and more. [7] is very relevant and the reviewer encourages the authors to discuss it. The reviewer only listed one paper for each topic, but encourages the authors to discuss more.
%We will be happy to add the suggested related works. The workshop paper [7] is relevant and extends work by Ma et al. 2019, referenced in the paper, by extending the partial VAE architecture with transformer components. One advantage of our partial encoder, over these works, is that given a full VAE on a domain, the partial encoder can be trained on different measurement basis.
%\pmh{there is an enormous literature on VAEs/GANs/DDMs for (visual) inverse problems; I'd expect like 50 cites here not 3!}
There is a large literature on data driven models for solving inverse problems \citep{Arridge_Maass_Oktem_Schonlieb_2019}.
In particular, generative variational models have been developed for a range of inverse problems in imaging \citep{doi:10.1137/21M1414978}. These include inpainting, denoising, deblurring, super resolution and JPEG decompression. Here, we focus on acquiring data rather than how to utilise data once collected. 
Quantitative methods for optimizing data acquisition have been explored in statistics and machine learning literature \citep{10.1214/23-STS915}. Bayesian experimental design is a powerful model-based framework for choosing designs optimally using information-theoretic principles. This includes Bayesian active learning \citep{10.1162/neco.1992.4.4.590}, sequential \citep{pmlr-v139-foster21a} and Bayesian optimization \citep{Mockus_1989, garnett_bayesoptbook_2023}. Other related topics include active feature acquisition \citep{Saar_Tsechansky_2009}, active multi-modal acquisition \citep{kossen2023active}, active perception \citep{Bajcsy_2018}, Bayesian active learning \citep{Mackay_1992}, active reinforcement learning \citep{Andreopoulos_2013} and active vision \citep{Whitehead_1990}.

A generative model-based approach to Bayesian inverse problems,
such as image reconstruction from noisy and incomplete images, is developed in \cite{Bohm:2019hpu}. Their inference framework makes use of a VAE to provide complex, data-driven priors that comprise all available information about the uncorrupted data distribution and enables computationally tractable uncertainty
quantification in the form of posterior analysis in latent and data space. Our approach differs in how we extend the VAE to the data. Our focus is identifying high value data points so we propose a different VAE to provide the prior and facilitate posterior analysis in latent and data space. Similarly our approach can be adapted to different data systems without retraining the core VAE. Our extended VAE can be used repeatedly for the sequence of measurements whereas the method in \cite{Bohm:2019hpu} has as its focus corrupted or missing data and is designed to tackle recovery of data for a given instance rather than exploring uncertainty in a sequential manner.

A partial variational autoencoder (Partial VAE) is introduced in \cite{pmlr-v97-ma19c} to predict problem specific missing data entries given a subset of of the observed ones. The model is combined with an acquisition function that maximises expected information gain on a set of target variables. The VAE based framework is extended to a Bayesian treatment of the weights in \cite{ma2019bayesian}. Our work overlaps in that we develop a VAE and use it to identify high value data points. However it differs in that we are concerned with measurements of data taken with respect to a basis. We adapt our encoder to the measurement basis rather than the problem and hence extend the active learning application to image reconstruction using sensing technology. The workshop paper by \cite{Saar_Tsechansky_2009} is also relevant to our work and extends the work \citep{pmlr-v97-ma19c} by adding transformer components to the partial VAE architecture. One advantage of our partial encoder, over these works, is that given a full VAE on a domain, the partial encoder can be trained on different measurement basis.

%In Section \ref{s:methodVAE} we describe our method for posterior inference given measurements. The active sequential measurement algorithm is outlined in Section \ref{s:methodalg}. An overview of the algorithm is provided in Figure \ref{fig:active}.

\section{Method for posterior inference given measurements}\label{s:methodVAE}

We describe the underlying VAE, Section \ref{ss:VAE}, the extension of this framework to sensor measurements, Section \ref{ss:extVAE}, and the SVI method, Section \ref{ss:altSVI}. We then introduce the partial encoder, Section \ref{ss:partEnc}, and training of the partial encoder, Section \ref{ss:partTrain}. 

\subsection{Underlying VAE model}\label{ss:VAE}

%\begin{figure}[h]
%\begin{center}
%\framebox[4.0in]{\includegraphics[width=0.5\textwidth]{figures/thumbnail_Presentation9.jpg}}
%\end{center}
%\caption{\label{fig:model} Models and variational inference. \pmh{make wider; remove outer frame; make caption informative} \pmh{is (c) identical to showing the data-generative bit combined with the measurement model? it's a bit strange to call it `partial' here if so}}
%\end{figure}

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{figures/AInferringF2crop.png}
\caption{\label{fig:model} Graphical representation of the VAE model. An image $\mathbf{x}$ is generated by a random variable $\mathbf{z}$ parameterized by a deep neural network with parameters $\theta$, a). A variational distribution parameterized by a deep neural network with parameters $\phi$ is introduced to infer $\mathbf{z}$ given $\mathbf{x}$, b). The encoder and decoder are trained together using $N$ images. To train the partial encoder we simulate $N \times E$ measurements $\mathbf{y}^{[b]}$ where $b$ is a subset of patterns, c). The partial encoder parameterized by $\phi^p$ is trained with the original decoder, d). The SVI method involves inferring the mean $\mu$ and variance $\Sigma$ for each image individually, e).  }
\end{figure}

We assume that an image, $\mathbf{x}$, is generated by a latent random variable, $\mathbf{z}$, in a complex non-linear manner, parameterized by a deep neural network decoder, $D_{\theta}$, with parameters $\mathbf{\theta}$.  The structure of this model is represented graphically in column (a) of Figure \ref{fig:model}. 
To do inference in this model we introduce a variational distribution to approximate the posterior distribution of $\mathbf{z}$ given $\mathbf{x}$.
Through training the VAE learns a function that maps each $\mathbf{x}$ to the parameters of posterior densities. Typically this mapping encodes the mean, $\mathbf{\mu}$, and variance, $\mathbf{\Sigma}$, of a Gaussian distribution, $\mathcal{N}(\mathbf{\mu}, \mathbf{\Sigma})$, in latent space. This function is also parameterized by a deep neural network with parameters $\mathbf{\phi}$ (encoder network $E_{\phi}$) and the variational distribution, %$q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x};\mathbf{\mu},\mathbf{\Sigma})$
$q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$, 
is represented graphically in column (b) of Figure \ref{fig:model}.
The goal of training is to find optimal values for $\mathbf{\theta}$ and $\mathbf{\phi}$ so that the model, $p_{\mathbf{\theta}}(\mathbf{x}|\mathbf{z})$, is a good fit to the data (log evidence is large) and the variational distribution is a good approximation to the posterior, $p(\mathbf{z}|\mathbf{x})$.

Having learnt $\theta$ and $\phi$ we can generate images by sampling $\mathbf{z}$ from the latent space according to the prior $p(\mathbf{z})$ 
and pushing $\mathbf{z}$ through the decoder map, $D_{\mathbf{\theta}} (\mathbf{z}): \mathbf{z} \rightarrow \mathbf{x}$. We can also generate images, $\hat{\mathbf{x}}$, conditioned on test $\mathbf{x}$, in three steps. First by using the encoder map, $E_{\mathbf{\phi}}(\mathbf{x}): \mathbf{x} \rightarrow \mathbf{\mu}, \mathbf{\Sigma}$, to characterise the posterior, $q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}; \mathbf{\mu}, \mathbf{\Sigma})$. Second, by drawing samples from this distribution. And third, by using the decoder map as before, $D_{\mathbf{\theta}} (\mathbf{z}|\mathbf{x}): \mathbf{z}|\mathbf{x} \rightarrow \hat{\mathbf{x}}$.
The full VAE is trained by maximising an evidence lower bound (ELBO), $\mathcal{L}(\theta,\phi; \mathbf{x})$, which is equivalent to minimizing the KL divergence between $p(\mathbf{z}|\mathbf{x})$ and $q_{\mathbf{\phi}}(\mathbf{z}|\mathcal\mathbf{x})$ \citep{DBLP:journals/corr/KingmaW13}, given by 
\cfh{\begin{align}
    p(\mathbf{x}) &= \infdiv {q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})} {p(\mathbf{z}|\mathbf{x})}  + \mathcal{L}(\theta,\phi; \mathbf{x}) \nonumber \\
     p(\mathbf{x}) & \ge \mathcal{L} (\theta,\phi; \mathbf{x})
%    \infdiv {q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})} {p(\mathbf{z}|\mathbf{x})} &= \mathbb{E}_{z \sim q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})} [\log q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) - \log p(\mathbf{z}|\mathbf{x})] \nonumber \\
     \equiv \mathbb{E}_{z \sim q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})} [\log q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) - \log p_{\theta}(\mathbf{x}|\mathbf{z}) - \log p(\mathbf{z})].
\end{align}}

\subsection{Extension of VAE framework to sensor measurements}\label{ss:extVAE}

We now consider the situation where we wish to determine a new $\mathbf{x}$ by taking optimal sequential measurements on $\mathbf{x}$ with respect to a measurement basis with $N$ indexes. 
At sequential step $k$ the measurement basis indexes chosen so far are 
denoted $B^k = \{b_1, b_2, \ldots, b_k \}$. %We also define a linear measurement function %, $\mathbf{y}^{\[B\]}=meas(x,b)$ 
We obtain $\mathbf{y}^{[B^k]}$ by taking a measurement using these basis indices; for convenience we write
\begin{equation}\label{eqn:meas}
\mathbf{y}^{[B^k]}=f(\mathbf{x},B^k).   
\end{equation}
%We assume Gaussian noise, $\sigma^2$, in our measurement and $y$ is a random variable from a Gaussian probability distribution with mean $f(x,B^k)$.
We model these measurements using isotropic Gaussians with variance $\sigma^2$.

\subsection{SVI method}\label{ss:altSVI}

%Applying Bayes' Rule to our model we can marginalise out $\mathbf{x}$ and obtain an expression for $\mathbf{z}$ and $\mathbf{y}^{[B^k]}$ 
%\begin{equation}
%   p(\mathbf{z}) = p(\mathbf{z}|\mathbf{y}^{[B^k]},\mathbf{x}) p(\mathbf{y}^{[B^k]}) p(\mathbf{x}) = p(\mathbf{z}|\mathbf{y}^{[B^k]}) p(\mathbf{y}^{[B^k]}).
%\end{equation}
Here inference is performed for one measurement instance $\mathbf{y}^{[B^k]}$.
\cfh{Marginalising $\mathbf{x}$ and applying Bayes' rule, we have
\begin{equation}
    p (\mathbf{z}|\mathbf{y}) = \frac{p(\mathbf{z})p(\mathbf{y}|\mathbf{z})}{p(\mathbf{y})}.
\end{equation}}
The log of the posterior distribution of the latent variables for a given partial observation $\mathbf{y}^{[B^k]}$ is therefore
\begin{equation}
    \log p(\mathbf{z}|\mathbf{y}^{[B^k]}) = \log p(\mathbf{z}) + \log p (\mathbf{y}^{[B^k]}|\mathbf{z}) - \log p(\mathbf{y}^{[B^k]}).
\end{equation}
This equation, as Equation (\ref{eqn:meas}), is intractable motivating the use of SVI.
The aim of SVI is to approximate the posterior with a multivariate Gaussian and infer the mean $\mu^{k}$ and variance $\Sigma^{k}$ of this distribution.
As the posterior, $q^k=q(\mathbf{z}|\mathbf{y}^{[B^k]};\mu^{k},\Sigma^{k})$, evolves with the number and index of basis measurements, the mean and variance are indexed by $k$. SVI is an iterative method. The variational parameters for each data input are randomly initialized and then optimized to minimise the KL divergence

%\begin{align}\label{eqn:DKLalt}
%    \infdiv {q^k (\mathbf{z}|\mathbf{y}^{[B^k]})} {p(\mathbf{z}|\mathbf{y}^{[B^k]})} &= \mathbb{E}_{\mathbf{z} \sim q^k (\mathbf{z}|\mathbf{y}^{[B^k]})} [ \log q^k (z|\mathbf{y}^{[B^k]}) - \log p(\mathbf{z}|\mathbf{y}^{[B^k]}) ] \nonumber \\
%     & \le \mathbb{E}_{\mathbf{z} \sim q^k(\mathbf{z}|\mathbf{y}^{[B^k]})} [ \log q^k(\mathbf{z}|\mathbf{y}^{[B^k]}) - \log p(\mathbf{y}^{[B^k]}|\mathbf{z}) - \log p(\mathbf{z}) ] \equiv \mathcal{L}^k.
%\end{align}
\cfh{\begin{align}
%    p(\mathbf{x}) &= \infdiv {q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})} {p(\mathbf{z}|\mathbf{x})}  + \mathcal{L}(\theta,\phi; \mathbf{x}) \nonumber \\
     %p(\mathbf{x}) & \ge 
     \mathcal{L}^{k}
%    \infdiv {q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})} {p(\mathbf{z}|\mathbf{x})} &= \mathbb{E}_{z \sim q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})} [\log q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) - \log p(\mathbf{z}|\mathbf{x})] \nonumber \\
     & \equiv \mathbb{E}_{\mathbf{z} \sim q^k(\mathbf{z}|\mathbf{y}^{[B^k]})} [ \log q^k(\mathbf{z}|\mathbf{y}^{[B^k]}) - \log p(\mathbf{y}^{[B^k]}|\mathbf{z}) - \log p(\mathbf{z}) ].
\end{align}}

\subsection{Partial encoder}\label{ss:partEnc}

The aim of the partial encoder is to encode incomplete measurements $\mathbf{y}^{[B^k]}$ as defined in Equation (\ref{eqn:meas}). We introduce a partial variational distribution, $q_{\phi^p}$, to approximate the posterior distribution and train a partial encoder, $E_{\phi^p}$, to infer $\mathbf{\mu}^k$ and $\mathbf{\Sigma}^k$ for specific $\mathbf{y}^{[B^k]}$, see Figure 1d).
The partial encoder VAE is trained by minimizing the KL divergence between $p(\mathbf{z}|\mathbf{x})$, established by the full VAE, and the partial posterior distribution, $q_{\phi^p}(\mathbf{z}|\mathcal{\mathbf{y}}^{[B^k]}; \mathbf{\mu}^k, \mathbf{\Sigma}^k)$, parameterised also by a neural network with parameters $\phi^p$.
%\cfh{
%\begin{align}\label{eqn:DKLpartial}
%    \infdiv {q_{\phi^p} (\mathbf{z}|\mathbf{y}^{[B^k]})} {p(\mathbf{z}|\mathbf{x})} &= \mathbb{E}_{z \sim q_{\phi^p}(z|\mathbf{y}^{[B^k]})} [ \log q_{\phi^p}(\mathbf{z}|\mathbf{y}^{[B^k]}) - \log p(\mathbf{z}|\mathbf{x}) ] \nonumber \\
%     & \le \mathbb{E}_{z \sim q_{\phi ^p}(\mathbf{z}|\mathbf{y}^{[B^k]})} [ \log q_{\phi^p}(\mathbf{z}|\mathbf{y}^{[B^k]}) - \log p(\mathbf{x}|\mathbf{z}) - \log p(\mathbf{z}) ] \equiv \mathcal{L}_{partial}.
%\end{align}
%}
\cfh{\begin{align}
%    p(\mathbf{x}) &= \infdiv {q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})} {p(\mathbf{z}|\mathbf{x})}  + \mathcal{L}(\theta,\phi; \mathbf{x}) \nonumber \\
     %p(\mathbf{x}) & \ge 
     \mathcal{L}_{partial}
%    \infdiv {q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})} {p(\mathbf{z}|\mathbf{x})} &= \mathbb{E}_{z \sim q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})} [\log q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}) - \log p(\mathbf{z}|\mathbf{x})] \nonumber \\
     & \equiv \mathbb{E}_{z \sim q_{\phi ^p}(\mathbf{z}|\mathbf{y}^{[B^k]})} [ \log q_{\phi^p}(\mathbf{z}|\mathbf{y}^{[B^k]}) - \log p(\mathbf{x}|\mathbf{z}) - \log p(\mathbf{z})].
\end{align}}


As with the full VAE, once we have learnt $\mathbf{\phi}^p$ we can generate images from a set of measurements by sampling from the approximate posterior distribution $q_{\phi ^p}(\mathbf{z}|\mathbf{y}^{[b]})$ and then pushing these samples through the decoder, $D_{\theta} (\mathbf{z}|\mathbf{y}^{[B^k]}): \mathbf{z}|\mathbf{y}^{[B^k]} \rightarrow \hat{\mathbf{x}}$ to generate the reconstructed image $\hat{\mathbf{x}}$.

In summary, we have two approaches to approximate the posterior $p(\mathbf{z}|\mathbf{y}^{[B^k]})$. First using a partial encoder, $q_{\phi^p}(\mathbf{z}|\mathbf{y}^{[B^k]})$, and second using SVI, $q^k (\mathbf{z}|\mathbf{y}^{[B^k]})$.
We compare these approaches in Section \ref{ss:comp}.

\subsection{Training the partial encoder}\label{ss:partTrain}

To train the partial encoder %(illustrated in Figure \ref{fig:partialvae}) 
we assume that our measurement sensor can provide a series of $J=B^k$ observations, $\{y_1,y_2, \ldots, y_J\}$, 
each associated with an action (experiment or basis measurement resulting in an observation) for an image $\mathbf{x}$, see Equation (\ref{eqn:meas}). We now %create a large training set 
simulate training data by randomly sampling $N$ experiments for every image $\mathbf{x}$, simulating observations $\mathbf{y}^{[J]}$ for each of them. 
This is repeated $E$ times for each $\mathbf{x}$ giving $N\times E$ experiments in total where the measurement vector, $\mathbf{y}^{[J]}$, varies in terms of both the number of measurements and the measurement index. This is achieved in an efficient manner by introducing a mask layer into the encoder network that randomly masks a different number and index of measurements with each training batch. Using this training set we learn a %conditional 
variational autoencoder which generates the required mapping, $E_{\phi^p} (\mathbf{y}^{[J]}, j) : \rightarrow \mu^{J}, \Sigma^{J}$.


\section{Method for active sequential inference}\label{s:methodalg}

\subsection{The algorithm}
We now present our sequential algorithm to actively choose the next best measurement. Our proposed active inference algorithm is illustrated in Figure \ref{fig:active} and pseudocode provided in Algorithm \ref{alg:cap}. 
The aim of the algorithm is to identify, under some criteria (details in Section \ref{ss:crit}), the next best measurement to take.
The algorithm is designed to leverage the encoding and decoding properties of the VAE. The encoder provides a means to map incomplete measurements onto a low dimensional space for exploration. The decoder provides a means to project back to image space to assess future measurements.

At the first step, $k=0$, $N$ samples are drawn from the prior on the latent space, $p_0 = \mathcal{N}(0,1)$. These samples, $\mathbf{z}_i$, are pushed through the decoder, $D_{\theta}(\mathbf{z}_i): \mathbf{z}_i \rightarrow \hat{\mathbf{x}}_i$, to obtain $N$ generated images, $\hat{\mathbf{x}}_i$. %Examples of these generated images, for the Fashion MNIST experiment, can be found in Figure \ref{fig:steps} 
Simulated measurements, $\hat{\mathbf{y}}_i=f(\hat{\mathbf{x}}_i,j)$, are made 
under the chosen measurement basis for each basis element indexed $j$.  
The simulated measurements are pushed through the encoder, $E_{\phi^p}: \hat{\mathbf{y}}_i \rightarrow : \mu^j, \Sigma^j$, and the output used to characterise the conditional posterior, $q_i^j$. 
The algorithm evaluates each measurement indicator and chooses the measurement indicator which best satisfies the decision criteria. This indicator is added to the indicator set, $B^{k+1}$. Actual measurements are taken on the test image, $\mathbf{y}^{[B^{k+1}]}=f(\mathbf{x},B^{k+1})$. SVI is used to infer the posterior and predictive prior for the next step, $p^{k+1}=q^{k+1}$. The algorithm continues for $K-1$ steps.

%\pmh{I think these paras need to motivate more clearly why each step of the algo occurs, and made to sound like these are natural (or clever and insightful); currently it's rather cookbook-ish and will perhaps raise questions of why / why-not-this-instead}

At step $k+1$, $N$ samples, $\mathbf{z}=\{z_1, \dots z_N\}$, are drawn from the predictive prior, $p_k=\mathcal{N}(\mu^{B^k},\Sigma^{B^k})$, where $\mu^{B^k}$ and $\Sigma^{B^k}$ are the mean and variance predicted by the partial encoder conditional on the measurements, $\mathbf{y}^{[B^k]}$, made so far 
\begin{equation}
    \mathrm{enc}_{\phi^p} (\mathbf{y}^{[B^k]}) : \mathbf{y}^{[B^k]} \rightarrow  \mu^{B^k}, \Sigma^{B^k}. 
\end{equation}
These samples are then mapped by the decoder to generate images
\begin{equation}
    \mathrm{dec}_{\theta} (\mathbf{z}) : \mathbf{z} \rightarrow  \hat{\mathbf{x}}=\{\hat{x}_1 \ldots \hat{x}_N \}. 
\end{equation}
For each remaining pattern indicator, $j \in B \backslash B^k$, we create a set of pattern indicators $J=\{b_1, \ldots b_k, b_j\}$ and simulate measurements made on each generated image, $\{ \hat{\mathbf{y}}_1^{[J]}, \ldots, \hat{\mathbf{y}}_N^{[J]} \}$.
We now use the partial encoder to approximate the posterior distribution 
conditional on these simulated measurements for each generated image 
denoted $q_i^J=\mathcal{N} (\mu_i^J, \Sigma_i^J)$. 

\subsection{Criteria for choosing next measurement}\label{ss:crit}

We consider three criteria for choosing the next pattern to measure. A good choice of measurement is one that provides useful information to the agent.

\subsubsection{Likelihood (QP)}

Here, the criterion for choosing the next pattern, $b_{k+1}$, corresponds to choosing the pattern, $b_j$, with the highest log likelihood with respect to $q_i^J$ over all $N$ images;   
%\begin{equation}\label{eqn:critlogq}
%  b_{k+1} =  \argmax_j{\sum_{i=1}^N \log(q_i^J(z_i)) - %\log(p_k(z_i))}.  
%\end{equation}
\begin{equation}\label{eqn:critlogq}
  b_{k+1} =  \argmax_j{\sum_{i=1}^N \log(q_i^J(z_i))}. 
\end{equation}

By using the likelihood we establish which measurements provide information that is consistent with the predictive prior for that step. We investigate the ability of the algorithm to move towards the target distribution. 

\subsubsection{Mutual Information (MI)}
An alternative criterion based on the concept of conditional mutual information \citep{10.5555/1162264} is to choose the measurement which most reduces the posterior entropy or uncertainty. The mutual information, $MI$, between $\mathbf{z}_i$ and $\hat{\mathbf{y}}_i^{[J]}$ is defined in terms of entropy $H^e$ as 
\begin{equation}\label{eqn:MI}
 MI(\mathbf{z}_i;\hat{\mathbf{y}}_i^{[J]}) =  H^e(\mathbf{z}_i) - H^e(\mathbf{z}|\hat{\mathbf{y}}_i^{[J]}).   
\end{equation}
The entropy of random variable $\mathbf{x}$ from a multivariate Gaussian of dimension $D$ and variance $\Sigma$ is
\begin{equation}\label{eqn:He}
H^e(\mathbf{x}) = \frac{D}{2} \left(1+ \log \left( 2 \pi \right) \right) + \frac{1}{2} \log |\Sigma |. 
\end{equation}
Our criteria for choosing the next pattern is then
\cfh{\begin{equation}\label{eqn:critlogMI}
  b_{k+1} =  \argmax_j{\sum_{i=1}^N \frac{1}{2} ( \log|\Sigma^J_i| - \log|\Sigma^{B^k}| )}.  
\end{equation}}
High $MI$ between measurement and posterior means that we learn as much as possible about the posterior from the new measurement given what we already know.

\subsubsection{Inference-free Hadamard optimisation (HO)}

Some measurement bases, for example the Hadamard transform \citep{Ahmed1975}, permit patterns to be prioritised according to the absolute value of the measurement instead of involving inference. High absolute values (of the eigenvalues associated with each basis eigenvector) contribute more to the reconstructed image which motivates the choice
%
\begin{equation}\label{eqn:critH}
  b_{k+1} =  \argmax_j \sum_{i=1}^N \lvert  \{ \hat{{\mathbf{y}}}_i^{[J]} \} \rvert.  
\end{equation}


\begin{algorithm}[t]
\caption{Active Sequential Inference}\label{alg:cap}
\begin{algorithmic}
\State $B^0 \gets \emptyset$ \Comment{pattern index set}
\For{$k = 0 \ldots K-1$} \Comment{for each step}
\For{$i = 1 \ldots N$} \Comment{for each generated image}
\If{$k=0$}
\State $p_0=\mathcal{N}(0,1)$ \Comment{set prior for step 0}
\ElsIf{$k > 0$}
\State $p_{k} \gets q^{k}$ \Comment{set predictive posterior prior for step $k$% based on posterior made at step $k-1$.
}
\EndIf
\State $\mathbf{z}_i \sim p_k$ \Comment{sample latent vector from current pdf}
\State $\hat{\mathbf{x}_i} \gets \mathrm{dec}_{\theta} (\mathbf{z}_i)$ \Comment{\textbf{A.} input to decoder to obtain generated image $\hat{\mathbf{x}}_i$}
\For{ each element $j \in B \backslash B^k$} \Comment{for each remaining pattern}
\State $J \gets  \{b_1, \ldots, b_k, j \}$ \Comment{add pattern index $j$ to measurement set $J$}
\State $\hat{\mathbf{y}_i} \gets f(\hat{\mathbf{x}_i}, J)$ \Comment{\textbf{B.} estimate possible measurements}
\State $q_i^j \gets \mathrm{enc}_{\phi^p}(\hat{\mathbf{y}}_i)$ \Comment{\textbf{C.} use partial encoder to approximate pdf $q^j$}
\EndFor
\EndFor
\State $\mathbf{M}_{ij} \gets \log(q_i^{j}(\mathbf{z}_i)) - \log(p_k(\mathbf{z}_i))$ \Comment{\textbf{D.} store results in matrix $\mathbf{M}$}
\State  $\{b_{k+1}\} \gets \{\argmax_j \sum_i \mathbf{M}_{ij} \} $ \Comment{\textbf{E.} find pattern index which maximises expression}
\State  $\mathcal{B}^{k+1} \gets \mathcal{B}^k \cup \{b_{k+1} \} $ \Comment{add pattern index to pattern index set } %$\argmax_j \sum_i \log(q^{j}(z_i)) $}
\State $\mathbf{y}^{[B^{k+1}]} \gets f(\mathbf{x},B^{k+1})$ \Comment{take actual measurement on $\mathbf{x}$}
\State $q^{k+1} \gets SVI(\mathbf{y}^{[B^{k+1}]})$ \Comment{\textbf{F.} set predictive posterior prior for next step}
\EndFor


\end{algorithmic}
\end{algorithm}

%\begin{figure}
%\centering
%\includegraphics[width=1\textwidth]{figures/Slide1.PNG}
%\caption{\label{fig:active} Active Learning. At step $k=0$, for each of $N$ starting points, sample $\mathbf{z}_i$ from the prior for step $k=0$, push $\mathbf{z}_i$ through the decoder to obtain a generated image $\hat{\mathbf{x}}_i$ and estimate possible measurements $\hat{\mathbf{y}}_i$ for each pattern not yet measured (e.g. $\hat{y}_1$,$\hat{y}_2$ and $\hat{y}_3$ illustrated above). Use the partial encoder to approximate the posterior with probability distributions and estimate probability densities $q^1(\mathbf{z}_1|\hat{y}_1)$,$q^2(\mathbf{z}_1|\hat{y}_2)$ and $q^3(\mathbf{z}_1|\hat{y}_3)$ (illustrated above). Choose the pattern which maximises the chosen expression and take the measurement associated with this pattern. Update the measurement set and update the predictive prior for the next step. By repeating these steps, we move towards the target distribution $p(\mathbf{z}|\mathbf{x})$.}
%\end{figure}

%\begin{figure}[h]
%\begin{center}
%\framebox[5.0in]{\includegraphics[width=0.7\textwidth]{figures/Slide1.PNG}}
%\end{center}
%\caption{\label{fig:active} Active Learning. At step $k=0$, for each of $N$ starting points, sample $\mathbf{z}_i$ from the prior for step $k=0$, push $\mathbf{z}_i$ through the decoder to obtain a generated image $\hat{\mathbf{x}}_i$ and estimate possible measurements $\hat{\mathbf{y}}_i$ for each pattern not yet measured (e.g. $\hat{y}_1$, $\hat{y}_2$ and $\hat{y}_3$ illustrated above). Use the partial encoder to approximate the posterior with probability distributions and estimate probability densities $q^1(\mathbf{z}_1|\hat{y}_1)$, $q^2(\mathbf{z}_1|\hat{y}_2)$ and $q^3(\mathbf{z}_1|\hat{y}_3)$ (illustrated above). Choose the pattern which maximises the chosen expression and take the measurement associated with this pattern. Update the measurement set and update the predictive prior for the next step. By repeating these steps, we move towards the target distribution $p(\mathbf{z}|\mathbf{x})$. \pmh{remove frame; make wider; reduce whitespace; if possible don't use the powerpoint-style arrows, but something a bit tidier (e.g.~a line instead of 'solid')}}
%\end{figure}

\section{Experimental Results}

We illustrate the algorithm on Fashion MNIST \citep{xiao2017fashionmnist}. 
The basic VAE is trained using the 60,000 training images from Fashion MNIST \cite{xiao2017fashionmnist}.  The encoder/decoder architectures and training details are provided in Appendix \ref{apx:arch}. The partial encoder is trained on simulated measurements from a novel 4$\times$4 convolutional Hadamard measurement basis, details given in Section \ref{ss:convH}. The partial encoder architecture is adapted and fitted with a random measurement layer so that experiments involving different numbers and types of patterns can be efficiently simulated, in terms of computation and memory, during training. 

\subsection{Convolutional Hadamard basis}\label{ss:convH}

In the context of single pixel imaging, the measurements, $\mathbf{y}$, are made by projecting a series of spatial patterns on to a scene and capturing the reflected light with a single pixel detector sensor \cite{Higham_DLRTSPV_2018}. Mathematically, the measurement is the inner product between the patterns and the scene and defines function $f$ from equation (\ref{eqn:meas}). Expressing an image in vector form, $\mathbf{x}$, and the pattern basis in matrix form, $\mathbf{H}$, where $\mathbf{H} \in \mathbb{R}^{D \times D}$, we have $\mathbf{y}=\mathbf{Hx}$. Of particular interest is the Hadamard basis, an orthogonal binary 
 ${-1,1}$ basis \citep{Ahmed1975}. A Hadamard basis is suitable for experimental realization due to the binary nature of patterns that can be projected using DMD (Digital Micromirror Device) technology as spatial light modulator \citep{edgar_principles_2019}. We take $\mathbf{H}$ to be this basis though we emphasise our method is not specific to the Hadamard basis and the approach could be generalised to another appropriate basis. %\pmh{maybe strength this, e.g. `though we emphasise our method is not specific to the Hadamard basis, unlike some prior works XYZ'}. 
 For an image with $2^n \times 2^n$ pixels where $n$ is a positive integer, the complete Hadamard basis, required for perfect image reconstruction, comprises $N=2^{2n}$ patterns.

In this work we develop a novel convolutional Hadamard basis inspired by convolutional layers. The convolutional approach provides local spatial and resolution rather than global frequency information. %to enable efficient simulation of measurements and training of the partial encoder %\pmh{is this really why? isn't the spatial-locality of the convolutional version nicer? is it in fact more efficient to simulate/train than `full' non-local hadamard (which is what the wording seems to imply)}. 

A Hadamard basis matrix with $2^4$ rows and columns is rearranged as a $4 \times 4 \times 16$ tensor which replaces the filter of a standard convolutional mapping layer, $f_{conv}$. The Hadamard basis has the property of being its own inverse so the tensor can be used with transpose convolutional mapping layer, $f_{tpconv}$, to recover the input image from the feature image, $f_{conv}:\mathbf{x} \rightarrow \mathbf{y}$ and $f_{tconv}:\mathbf{y} \rightarrow \mathbf{x}$.

An advantage of this convolutional Hadamard basis %\pmh{layer or basis?} 
is that the resulting feature $f \in \mathbb{R}^{N/4 \times N/4 \times 16}$ has both spatial (vertical and horizontal) and frequency or resolution dimension associated to each element $f_j$ where $j = 1 \ldots N^2$. We exploit this in our illustration to give further insights into the decision making process.

\subsection{Comparison of pVAE and SVI}\label{ss:comp}

At each step of the algorithm, estimating the parameters of the approximate posterior, $q^J$, requires one pass through the partial encoder but several iterations with the SVI method. We compare the performance of pVAE (1 iteration) with SVI and a variable number of iterations $\{10,20,30,40,50,60,70\}$ in terms of reconstruction indexes: mean square error (MSE) and similarity structure (SSIM) \cite{1284395}. The performance scores are averaged over 100 samples from ten images (each from a different class) and the algorithm is run for 100 steps, see Figure \ref{fig:compSVI}. The SVI method improves as the number of iterations increases but only matches the pVAE method after 60 iterations. This makes the SVI method an order of magnitude slower than pVAE. %\pmh{I'd quote an overall wall-clock time for each too}. 
For our final algorithm, we therefore use pVAE to approximate $q_i^J$ based on simulated measurements (C in Algorithm 1) and SVI with 100 iterations to approximate $q^{k+1}$ based on actual measurements (F in Algorithm 1). This way we achieve a balance between performance and computation time.

%\begin{figure}
%\centering
%\includegraphics[width=0.9\textwidth]{figures/comp_0522.jpg}
%\caption{\label{fig:compSVI} Comparison of pVAE and SVI.}
%\end{figure}

\begin{figure}[h]
\begin{center}
\framebox[5.0in]{\includegraphics[width=0.7\textwidth]{figures/comp_0522.jpg}}
\end{center}
\caption{\label{fig:compSVI} Comparison of pVAE and SVI. The performance scores (mse \emph{left column} and ssim \emph{right column}) are averaged over 100 samples from ten images (each from a different class) and the algorithm is run for 100 steps. Here lower is better for mse \emph{left} and higher is better for ssim \emph{right}. The performance of pVAE (1 iteration) \emph{bold line} with SVI and a variable number of iterations $\{10,20,30,40,50,60,70\}$ \emph{mixed light lines}.
\cfh{The SVI method improves as the number of iterations increases. At 100 steps the results for pVAE lie between the results for SVI with 60 (SVI60) and 70 (SVI70) iterations. In terms of timings, the pVAE takes $9\times{10}^{-4}$ seconds per step and the SVI method takes $7.2\times{10}^{-3}$ seconds per iteration. With many iterations required for SVI, this makes the SVI method at least an order of magnitude slower than pVAE. Time measurements were taken using a NVIDIA GeForce RTX 3090 GPU.}}
\end{figure}

\subsection{Comparison of choice criteria}

The different criteria for choosing the next pattern (\textbf{QP}, \textbf{MI} and \textbf{HO}), equations
(\ref{eqn:critlogq}), (\ref{eqn:critlogMI}) and (\ref{eqn:critH}) respectively, were evaluated using 10 test images, one from each class, and $Ns=\{1,10,100,200\}$ latent vector samples over 100 steps. Performance measures, SSIM and MSE, were evaluated at each step and averaged over the test images, see Figure \ref{fig:compSVISSIM} and, in Appendix B, Figure \ref{fig:compSVImse}.  

%\begin{figure}[h]
%\begin{center}
%\framebox[6.0in]{\includegraphics[width=0.8\textwidth]{figures/compmmse_0301.jpg}}
%\end{center}
%\caption{\label{fig:compSVImse} mean squared error}
%\end{figure}

\begin{figure}[h]
\begin{center}
\framebox[5.0in]{\includegraphics[width=0.7\textwidth]{figures/compmssim_0301.jpg}}
\end{center}
\caption{\label{fig:compSVISSIM} Comparison of choice criteria in terms of the structural similarity index (SSIM). The different criteria for choosing the next pattern (\textbf{QP}, \textbf{MI} and \textbf{HO}), equations
(\ref{eqn:critlogq}), (\ref{eqn:critlogMI}) and (\ref{eqn:critH}) respectively, were evaluated (mean SSIM) using 10 test images, one from each class, and 1 (\emph{top left}), 10 (\emph{top right}), 100 (\emph{bottom left}), 200 (\emph{bottom right}) latent vector samples over 100 steps. Our method (\textbf{QP}) outperforms Hadamard optimisation (\textbf{HO}) for the first 25 steps and both show superior performance to \textbf{MI} across all the experiments. \cfh{In terms of timings, \textbf{HO} took 0.7778 seconds, \textbf{QP} took 1.3583 seconds and \textbf{MI} took 1.4219 seconds to complete 50 steps. Time measurements were taken using a NVIDIA GeForce RTX 3090 GPU.}}
\end{figure}

Our method (\textbf{QP}) outperforms Hadamard optimisation (\textbf{HO}) for the first 25 steps with 1, 10, 100 and 200 starting images in terms of MSE and SSIM. %, see Figures \ref{fig:compSVImse} and \ref{fig:compSVISSIM}. 
The methods \textbf{QP} and \textbf{HO} differ in both the criteria for choosing the next pattern and reconstruction whereas the methods \textbf{QP} and \textbf{MI} differ only in the criteria for choosing the next pattern. After these first steps, the performance of both methods plateaus with a slightly superior performance from the Hadamard reconstruction. Comparing \textbf{QP} with \textbf{MI}, \textbf{QP} shows superior performance across all the experiments. Investigation of the patterns chosen suggest that using \textbf{MI} early in the process leads to the algorithm getting stuck in the latent space. This method relies on choosing the next pattern to reduce uncertainty across several generated images whereas \textbf{QP} chooses the next pattern to increase likelihood. The \textbf{MI} strategy promotes larger initial steps in the latent space and consequently a tendency to get stuck and miss patterns that are useful for reconstruction. The \textbf{QP} strategy encourages smaller steps in the latent space and moves more steadily to a convergence of generated images. The \textbf{QP} method is therefore the one that we recommend.


\subsection{Visualisation of the active learning algorithm using UMAP}

We now illustrate the active learning algorithm by exploring low dimensional image space using a two dimensional representation of the latent space of 10,000 test images from Fashion MNIST created with UMAP \cite{Meehan_2022}. UMAP is applied to the mean of the latent representations. The classes are colour coded and the UMAP clusters within class and between similar classes (i.e.~ankle boot, sneaker and shoe) indicate that class structure has been retained by the latent representation and further dimension reduction, see Figure \ref{fig:steps}. 

%The algorithm proceeds as follows. 
%\pmh{the algorithm doesn't differ from what's discussed in earlier parts, does it? this wording makes it sound like the algo itself is different, rather than merely the visualisation} 
A number, $N$, of latent variables %, $\mathbf{z}_{i}, i=1, \ldots, N$, 
are drawn from the prior. %, $p_0=\mathcal{N}(0,1)$. 
These are shown as black crosses projected on to the two dimensional space along with the projected class colour coded test images. These variables are then passed to the generator, $p_{\theta}(\mathbf{x}_{i}|\mathbf{z}_{i})$, to produce the generated images shown on the right. Also shown is the image reconstruction from actual measurements made so far (\emph{top right box}), none at step 0, 10 at step 10 and 30 at step 30, and the target image (\emph{bottom left box}). 

At steps 1 to $K$, the generated images from the previous step are used to estimate possible measurements corresponding to the set of pattern indexes not yet taken. 
These possible measurements are individually added to the actual measurements %$y=\{\hat{y}, y_1, \ldots, y_{k-1}\}$
and passed to the encoder to obtain the posterior probability distribution. % $q_{ij}$.  
The measurement index, $j$, which is considered most informative satisfies the following expression:
\begin{equation}\label{eqn:exp}
 \argmax_j \sum_i \log(q_{ij}(z_i) - \log(p_k(z_i)).   
\end{equation}

At step $k$ our certainty, quantified by a probability distribution about our location in latent space, having taken $k-1$ measurements, is denoted by $p_{k-1}(z_i)$. If we were to take another measurement, our certainty becomes $q_{kij}(z_i)$. In the interest of increasing our certainty we choose the value which maximises the above expression. This simple procedure allows us to move through latent space converging on a reconstruction close to the target.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/newstep0.jpg}
         %\caption{step 0}
         %\label{fig:step0}
     \end{subfigure}
     %\hfill
         %\begin{subfigure}[b]{0.9\textwidth}
         %\centering
         %\includegraphics[width=\textwidth]{figures/step1_0929.jpg}
         %\caption{step 1}
         %\label{fig:step1}
     %\end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/newstep3.jpg}
         %\caption{step 3}
         %\label{fig:step3}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.9\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/newstep10.jpg}
        % \caption{step 10}
         %\label{fig:step10}
     \end{subfigure}
     %\hfill
     %\begin{subfigure}[b]{0.50\textwidth}
         %\centering
         %\includegraphics[width=\textwidth]{figures/step30_0822.jpg}
         %\caption{step 30}
         %\label{fig:step30}
     %\end{subfigure}
        \caption{UMAP is used to reduce the mean of the latent representations of 10,000 test images to 2 dimensions (\emph{left hand column}). The classes are colour coded and the UMAP clusters within class and between similar classes (i.e. ankle boot, sneaker and shoe) indicate that the class structure has been retained by the latent representation and further dimension reduction. The mean of 100 samples is similarly projected (black crosses) on to the map at each step of the algorithm. The measurements made, the samples projected back into image space and the image to be recovered are shown in the top left corner, centre and bottom right corner of the (\emph{right hand column}) respectively. We see the diversity of possible images in the \emph{right hand column}, and this decreases as uncertainty is reduced by more measurements. } 
        \label{fig:steps}
\end{figure}



\section{Discussion and Conclusion}

We have shown that given a large dataset and a measurement basis the encoder of a VAE, trained on this large dataset, can be adapted to map partial measurements on to a representative latent space. An sequential measurement algorithm is developed to explore this latent space in order to optimise the next best measurement. The algorithm is illustrated using the Fashion MNIST dataset and a novel convolutional Hadamard measurement basis. We see  
that useful patterns are chosen within 10 steps leading to the convergence of the guiding generative images. In situations where there is a cost attached to measurement, the ability to reduce the number of measurements is a significant benefit. We believe that this algorithm is the first to address the task of active sequential inference in this context, and we note that it has the potential to increase efficiency dramatically in many high profile applications.

%\subsection{Challenges in our work}
%he decoder or generator has to recover the scene. The reconstruction ability of the generative model is limited by the range of its generator (a small subset of the signal space of interest). We improve improve the ability of the generator to recover the scene by increasing the relative weight given to reconstruction error (over the KL term) in the loss function and introducing resnet skip layers in the decoder. We consider VAE \cite{DBLP:journals/corr/KingmaW13}, VQVAE2 \cite{NEURIPS2019_Razavi} and NVAE \cite{NEURIPS2020_Vahdat}. We also consider beta-vae \cite{higgins2017betavae} and VAE with an autoregressive decoder\cite{NIPS2016_ddeebdee}. 

%Some tools for checking the sufficiency and minimality properties of VAE representation are presented in \cite{zhang2022improving}.

%\section*{Data Statement}
%Code for the experiments presented here will be made available upon publication.

\begin{comment}

\section{Submission of papers to TMLR}

TMLR requires electronic submissions, processed by
\url{https://openreview.net/}. See TMLR's website for more instructions.

If your paper is ultimately accepted, use option {\tt accepted} with the {\tt tmlr} 
package to adjust the format to the camera ready requirements, as follows:
\begin{center}  
  {\tt {\textbackslash}usepackage[accepted]\{tmlr\}}. 
\end{center}
You also need to specify the month and year
by defining variables {\tt month} and {\tt year}, which respectively
should be a 2-digit and 4-digit number. To de-anonymize and remove mentions
to TMLR (for example for posting to preprint servers), use the preprint option,
as in {\tt {\textbackslash}usepackage[preprint]\{tmlr\}}. 

Please read carefully the instructions below, and follow them
faithfully.


%\bibliographystyle{alpha}
%\bibliography{sample}


    


\subsection{Style}

Papers to be submitted to TMLR must be prepared according to the
instructions presented here.

Authors are required to use the TMLR \LaTeX{} style files obtainable at the
TMLR website. Please make sure you use the current files and
not previous versions. Tweaking the style files may be grounds for rejection.

\subsection{Retrieval of style files}

The style files for TMLR and other journal information are available online on
the TMLR website.
The file \verb+tmlr.pdf+ contains these
instructions and illustrates the
various formatting requirements your TMLR paper must satisfy.
Submissions must be made using \LaTeX{} and the style files
\verb+tmlr.sty+ and \verb+tmlr.bst+ (to be used with \LaTeX{}2e). The file
\verb+tmlr.tex+ may be used as a ``shell'' for writing your paper. All you
have to do is replace the author, title, abstract, and text of the paper with
your own.

The formatting instructions contained in these style files are summarized in
sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

\section{General formatting instructions}
\label{gen_inst}

The text must be confined within a rectangle 6.5~inches wide and
9~inches long. The left margin is 1~inch.
Use 10~point type with a vertical spacing of 11~points. Computer Modern Bright is the
preferred typeface throughout. Paragraphs are separated by 1/2~line space,
with no indentation.

Paper title is 17~point, in bold and left-aligned.
All pages should start at 1~inch from the top of the page.

Authors' names are
set in boldface. Each name is placed above its corresponding
address and has its corresponding email contact on the same line, in italic 
and right aligned. The lead author's name is to be listed first, and
the co-authors' names are set to follow vertically.

Please pay special attention to the instructions in section \ref{others}
regarding figures, tables, acknowledgments, and references.

\section{Headings: first level}
\label{headings}

First level headings are in bold,
flush left and in point size 12. One line space before the first level
heading and 1/2~line space after the first level heading.

\subsection{Headings: second level}

Second level headings are in bold,
flush left and in point size 10. One line space before the second level
heading and 1/2~line space after the second level heading.

\subsubsection{Headings: third level}

Third level headings are in bold,
flush left and in point size 10. One line space before the third level
heading and 1/2~line space after the third level heading.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone, regardless of the formatter being used.

\subsection{Citations within the text}

Citations within the text should be based on the \texttt{natbib} package
and include the authors' last names and year (with the ``et~al.'' construct
for more than two authors). When the authors or the publication are
included in the sentence, the citation should not be in parenthesis, using \verb|\citet{}| (as
in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
towards AI~\citep{Bengio+chapter2007}.'').

The corresponding references are to be listed in alphabetical order of
authors, in the \textbf{References} section. As to the format of the
references themselves, any style is acceptable as long as it is used
consistently.

\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches.\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures.
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.
\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}. Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{Default Notation}

In an attempt to encourage standardized notation, we have included the
notation file from the textbook, \textit{Deep Learning}
\cite{goodfellow2016deep} available at
\url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
is not required and can be disabled by commenting out
\texttt{math\_commands.tex}.


\centerline{\bf Numbers and Arrays}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1in}p{3.25in}}
$\displaystyle a$ & A scalar (integer or real)\\
$\displaystyle \va$ & A vector\\
$\displaystyle \mA$ & A matrix\\
$\displaystyle \tA$ & A tensor\\
$\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
$\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
$\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
$\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
$\displaystyle \ra$ & A scalar random variable\\
$\displaystyle \rva$ & A vector-valued random variable\\
$\displaystyle \rmA$ & A matrix-valued random variable\\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Sets and Graphs}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \sA$ & A set\\
$\displaystyle \R$ & The set of real numbers \\
$\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
$\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
$\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
$\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
$\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
$\displaystyle \gG$ & A graph\\
$\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
\end{tabular}
\vspace{0.25cm}


\centerline{\bf Indexing}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
$\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
$\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
$\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
$\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
$\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
$\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
$\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
\end{tabular}
\egroup
\vspace{0.25cm}


\centerline{\bf Calculus}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
% NOTE: the [2ex] on the next line adds extra height to that row of the table.
% Without that command, the fraction on the first line is too tall and collides
% with the fraction on the second line.
$\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
$\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
$\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
$\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
$\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
$\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
$\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
$\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
$\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Probability and Information Theory}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
$\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
a variable whose type has not been specified\\
$\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
$\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
$\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
$\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
$\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
$\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
$\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Functions}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
$\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
  $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
  (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
$\displaystyle \log x$ & Natural logarithm of $x$ \\
$\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
$\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
$\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
$\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
$\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
$\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
\end{tabular}
\egroup
\vspace{0.25cm}



\section{Final instructions}
Do not change any aspects of the formatting parameters in the style files.
In particular, do not modify the width or length of the rectangle the text
should fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.

\section{Preparing PostScript or PDF files}

Please prepare PostScript or PDF files with paper size ``US Letter'', and
not, for example, ``A4''. The -t
letter option on dvips will produce US Letter files.

Consider directly generating PDF files using \verb+pdflatex+
(especially if you are a MiKTeX user).
PDF figures must be substituted for EPS figures, however.

Otherwise, please generate your PostScript and PDF files with the following commands:
\begin{verbatim}
dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
ps2pdf mypaper.ps mypaper.pdf
\end{verbatim}

\subsection{Margins in LaTeX}

Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+
from the graphicx package. Always specify the figure width as a multiple of
the line width as in the example below using .eps graphics
\begin{verbatim}
   \usepackage[dvips]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.eps}
\end{verbatim}
or % Apr 2009 addition
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
for .pdf graphics.
See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

A number of width problems arise when LaTeX cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

\subsubsection*{Broader Impact Statement}
In this optional section, TMLR encourages authors to discuss possible repercussions of their work,
notably any potential negative impact that a user of this research should be aware of. 
Authors should consult the TMLR Ethics Guidelines available on the TMLR website
for guidance on how to approach this subject.

\subsubsection*{Author Contributions}
If you'd like to, you may include a section for author contributions as is done
in many journals. This is optional and at the discretion of the authors. Only add
this information once your submission is accepted and deanonymized. 

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.
Only add this information once your submission is accepted and deanonymized. 

\end{comment}

\section*{Acknowledgments}
%The authors received funding from the Designing Interaction Freedom via Active Inference (DIFAI) ERC Advanced Grant (proposal 101097708, funded by the UK Horizon guarantee scheme as EPSRC project EP/Y029178/1). R.M-S. also received funding from EPSRC projects EP/T00097X/1, EP/R018634/1, and EP/T021020/1.
C.F.H and R.M-S. received funding from EP/T00097X/1, 
EP/R018634/1, and EP/T021020/1. R.M-S. also 
received funding from the Designing Interaction Freedom via Active Inference (DIFAI) ERC Advanced Grant
(proposal 101097708, 
funded by the UK Horizon guarantee scheme as EPSRC project EP/Y029178/1). 

\bibliography{sample.bib}
\bibliographystyle{tmlr}

\newpage
\appendix
\section{VAE architecture and training}\label{apx:arch}
%For more information about deep learning architectures and training see \cite{bengio_dl_book} 

The encoder comprised an image input layer, two encoding blocks and a fully connected layer.  Each encoding block contained a convolutional layer, a batchnorm layer and a ReLU activation layer. The input image ($28 \times 28$) was downsized to ($14 \times 14 \times 32$) and ($7 \times 7 \times 64$) by the encoding blocks respectively. The output of the fully connected layer, $[\mu,\log \Sigma]$, is twice the size of the latent space ($32 \times 1$).

The partial encoder was formed by replacing the first encoding block with a convolutional layer, modified to use the convolutional Hadamard basis as fixed weights, resulting in a feature ($7 \times 7 \times 64$). This block was followed by a random mask layer that randomly selects a number of patterns and pattern indexes. The subsequent encoding block was adjusted to downsize to ($4 \times 4 \times 64$).

The decoder comprised an latent sample input layer ($16 \times 1$), a project and reshape layer ($7 \times 7 \times 64$) and three decoding blocks. The decoding blocks each contained a transposed convolutional layer and an activation layer RELU for the first two blocks and sigmoid for the last block). The input feature was up sampled to ($14 \times 14 \times 64$), ($28 \times 28 \times 32$) and ($28 \times 28 \times 1$) by the decoding blocks respectively.

The encoder and decoder were trained together using a custom training loop and 60,000 fashion MNIST images \cite{xiao2017fashionmnist} in mini-batches of 128 for 100 epochs. The parameters were updated using the adaptive moment estimation (ADAM) algorithm \cfh{\citep{KingBa15}} with settings: learning rate = 0.001, gradient decay = 0.9, squared gradient decay = 0.999 and epsilon = 1e-8) chosen using validation set performance. 

The partial encoder was trained using the previously trained decoder with fixed settings for 200 epochs. The random mask layer was reset for each mini-batch iteration simulating $200 \times 450$ different experiments.

We modify the KL divergence term in the loss functions to include an additional scaling factor, $\beta$, in front of the KL divergence term. This $\beta VAE$ approach was introduced in \cite{higgins2017betavae} to encourage a more flexible latent space representation, while still ensuring that the learned distribution is close to the prior distribution. The value of $\beta$ is set to 0.1 for the VAE and the partial VAE.




\section{More results}\label{apx:results}

The different criteria for choosing the next pattern (\textbf{QP}, \textbf{MI} and \textbf{HO}), equations
(\ref{eqn:critlogq}), (\ref{eqn:critlogMI}) and (\ref{eqn:critH}) respectively, were evaluated using 10 test images, one from each class, and $Ns=\{1,10,100,200\}$ latent vector samples over 100 steps. Performance measures, SSIM and MSE, were evaluated at each step and averaged over the test images, see Figure \ref{fig:compSVISSIM} and, in Appendix B, Figure \ref{fig:compSVImse}.  

\begin{figure}[h]
\begin{center}
\framebox[5.0in]{\includegraphics[width=0.7\textwidth]{figures/compmmse_0301.jpg}}
\end{center}
\caption{\label{fig:compSVImse} Comparison of choice criteria in terms of mean squared error (MSE). The different criteria for choosing the next pattern (\textbf{QP}, \textbf{MI} and \textbf{HO}), equations
(\ref{eqn:critlogq}), (\ref{eqn:critlogMI}) and (\ref{eqn:critH}) respectively, were evaluated (mean MSE) using 10 test images, one from each class, and 1 (\emph{top left}), 10 (\emph{top right}), 100 (\emph{bottom left}), 200 (\emph{bottom right}) latent vector samples over 100 steps. (\textbf{QP}) outperforms Hadamard optimisation (\textbf{HO}) for the first 25 steps.}
\end{figure}

\section{Interpreting results}
The patterns belonging to the convolutional Hadamard basis have two spatial and one resolution component. A log likelihood map for each generated image can be formed by averaging the row $M_{i,:}$ over the resolution component
\begin{equation}
    M_{i,xy} = \sum_r M_{i,xyr}.
\end{equation}
Figure \ref{fig:imgs} shows resized $M_{xy}$ overlaid on the generated image $\hat{x}$ at step 0. This visualisation highlights regions of interest. Namely the tops of the sleeves for T-shirt (a), the back and toe of the boot (b) and the waist and lower legs for the trousers (c). 

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/YNew_1.jpg}
         \caption{}
         \label{fig:img1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/YNew_14.jpg}
         \caption{}
         \label{fig:img2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/YNew_81.jpg}
         \caption{}
         \label{fig:img3}
     \end{subfigure}
        \caption{After step 0, we overlay $M_i$ over $\hat{x_i}$ to indicate regions of high information (white) and low information (black) within the active learning frame at this step. For the long sleeved top, regions of interest are the tops of the sleeves (a). The back and toe of the boot are regions of interest (b). The waist and lower legs are regions of interest for the trousers (c). Using expression in equation \ref{eqn:exp} the next measurement taken is determined by averaging the information over $N$ images. }
        \label{fig:imgs}
\end{figure}


\end{document}
