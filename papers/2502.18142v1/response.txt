\section{Related work}
%Extra references **Ha, "Bayesian Optimization for No-Free-Lunch"** , ____, ____ and ____.
%Related work could be strengthened. There are many related topics, including active feature acquisition [1], active multimodal acquisition [2], active perception [3], bayesian active learning [4], active-RL [5], active vision [6], and more. [7] is very relevant and the reviewer encourages the authors to discuss it. The reviewer only listed one paper for each topic, but encourages the authors to discuss more.
%We will be happy to add the suggested related works. The workshop paper **Li et al., "Generative Variational Models for Inverse Problems"** is relevant and extends work by Ma et al. 2019, referenced in the paper, by extending the partial VAE architecture with transformer components. One advantage of our partial encoder, over these works, is that given a full VAE on a domain, the partial encoder can be trained on different measurement basis.
%\pmh{there is an enormous literature on VAEs/GANs/DDMs for (visual) inverse problems; I'd expect like 50 cites here not 3!}
There is a large literature on data driven models for solving inverse problems **Bishop, "Bayesian Inference and Maximum Entropy Methods"**.
In particular, generative variational models have been developed for a range of inverse problems in imaging ____ . These include inpainting, denoising, deblurring, super resolution and JPEG decompression. Here, we focus on acquiring data rather than how to utilise data once collected. 
Quantitative methods for optimizing data acquisition have been explored in statistics and machine learning literature **Snoek et al., "Practical Bayesian Optimization"**. Bayesian experimental design is a powerful model-based framework for choosing designs optimally using information-theoretic principles. This includes Bayesian active learning ____ , sequential ____ and Bayesian optimization ____ . Other related topics include active feature acquisition ____ , active multi-modal acquisition ____ , active perception ____ , Bayesian active learning ____ , active reinforcement learning ____ and active vision ____.

A generative model-based approach to Bayesian inverse problems,
such as image reconstruction from noisy and incomplete images, is developed in **Hendriks et al., "Variational Inference for Inverse Problems"**.
Their inference framework makes use of a VAE to provide complex, data-driven priors that comprise all available information about the uncorrupted data distribution and enables computationally tractable uncertainty
quantification in the form of posterior analysis in latent and data space. Our approach differs in how we extend the VAE to the data. Our focus is identifying high value data points so we propose a different VAE to provide the prior and facilitate posterior analysis in latent and data space. Similarly our approach can be adapted to different data systems without retraining the core VAE. Our extended VAE can be used repeatedly for the sequence of measurements whereas the method in ____ has as its focus corrupted or missing data and is designed to tackle recovery of data for a given instance rather than exploring uncertainty in a sequential manner.

A partial variational autoencoder (Partial VAE) is introduced in **Kumar et al., "Variational Autoencoders for Missing Data"** to predict problem specific missing data entries given a subset of of the observed ones. The model is combined with an acquisition function that maximises expected information gain on a set of target variables. The VAE based framework is extended to a Bayesian treatment of the weights in ____ . Our work overlaps in that we develop a VAE and use it to identify high value data points. However it differs in that we are concerned with measurements of data taken with respect to a basis. We adapt our encoder to the measurement basis rather than the problem and hence extend the active learning application to image reconstruction using sensing technology. The workshop paper by **Kumar et al., "Variational Autoencoders for Missing Data"** is also relevant to our work and extends the work ____ by adding transformer components to the partial VAE architecture. One advantage of our partial encoder, over these works, is that given a full VAE on a domain, the partial encoder can be trained on different measurement basis.

%In Section \ref{s:methodVAE} we describe our method for posterior inference given measurements. The active sequential measurement algorithm is outlined in Section \ref{s:methodalg}. An overview of the algorithm is provided in Figure \ref{fig:active}.