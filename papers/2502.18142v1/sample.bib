%The main idea of variational methods is to cast inference as an optimization problem.
@article{Blei_2017,
   title={Variational Inference: A Review for Statisticians},
   volume={112},
   ISSN={1537-274X},
   url={http://dx.doi.org/10.1080/01621459.2017.1285773},
   DOI={10.1080/01621459.2017.1285773},
   number={518},
   journal={Journal of the American Statistical Association},
   publisher={Informa UK Limited},
   author={Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
   year={2017},
   month=apr, pages={859–877} }



%[1] Saar-Tsechansky, Maytal, Prem Melville, and Foster Provost. "Active feature-%value acquisition." Management Science 55.4 (2009): 664-684.

@article{Saar_Tsechansky_2009,
author = {Saar-Tsechansky, Maytal and Melville, Prem and Provost, Foster},
title = {Active Feature-Value Acquisition},
year = {2009},
issue_date = {April 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {55},
number = {4},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.1080.0952},
doi = {10.1287/mnsc.1080.0952},
abstract = {Most induction algorithms for building predictive models take as input training data in the form of feature vectors. Acquiring the values of features may be costly, and simply acquiring all values may be wasteful or prohibitively expensive. Active feature-value acquisition (AFA) selects features incrementally in an attempt to improve the predictive model most cost-effectively. This paper presents a framework for AFA based on estimating information value. Although straightforward in principle, estimations and approximations must be made to apply the framework in practice. We present an acquisition policy, sampled expected utility (SEU), that employs particular estimations to enable effective ranking of potential acquisitions in settings where relatively little information is available about the underlying domain. We then present experimental results showing that, compared with the policy of using representative sampling for feature acquisition, SEU reduces the cost of producing a model of a desired accuracy and exhibits consistent performance across domains. We also extend the framework to a more general modeling setting in which feature values as well as class labels are missing and are costly to acquire.},
journal = {Manage. Sci.},
month = apr,
pages = {664–684},
numpages = {21},
keywords = {active feature acquisition, active learning, business intelligence, data mining, imputation, information acquistion, machine learning, predictive modeling, utility-based data mining}
}

%[2] Kossen, Jannik, et al. "Active acquisition for multimodal temporal data: A %challenging decision-making task." arXiv preprint arXiv:2211.05039 (2022).

@article{kossen2023active,
title={Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task},
author={Jannik Kossen and C{\u{a}}t{\u{a}}lina Cangea and Eszter V{\'e}rtes and Andrew Jaegle and Viorica Patraucean and Ira Ktena and Nenad Tomasev and Danielle Belgrave},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=Gbu1bHQhEL},
note={}
}

%[3] Bajcsy, Ruzena, Yiannis Aloimonos, and John K. Tsotsos. "Revisiting active %perception." Autonomous Robots 42 (2018): 177-196.
%Bajcsy, R., Aloimonos, Y. & Tsotsos, J.K. Revisiting active perception. Auton %Robot 42, 177–196 (2018). https://doi.org/10.1007/s10514-017-9615-3

@article{Bajcsy_2018, 
title={Revisiting active perception}, 
volume={42}, 
journal={Autonomous Robots}, 
author={Bajcsy, R. and Aloimonos, Y. and Tsotsos, J.K.}, 
year={2018}, 
pages={177–196}} 

%[4] MacKay, David JC. "Information-based objective functions for active data %selection." Neural computation 4.4 (1992): 590-604.

@ARTICLE{Mackay_1992,
  author={MacKay, David J. C.},
  journal={Neural Computation}, 
  title={Information-Based Objective Functions for Active Data Selection}, 
  year={1992},
  volume={4},
  number={4},
  pages={590-604},
  keywords={},
  doi={10.1162/neco.1992.4.4.590}}

%[5] Andreopoulos, Alexander, and John K. Tsotsos. "A theory of active object %localization." 2009 IEEE 12th International Conference on Computer Vision. IEEE, 2009.
@article{Andreopoulos_2013,
author = {Andreopoulos, Alexander and Tsotsos, John K.},
title = {A Computational Learning Theory of Active Object Recognition Under Uncertainty},
year = {2013},
issue_date = {January 2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {101},
number = {1},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-012-0551-6},
doi = {10.1007/s11263-012-0551-6},
abstract = {We present some theoretical results related to the problem of actively searching a 3D scene to determine the positions of one or more pre-specified objects. We investigate the effects that input noise, occlusion, and the VC-dimensions of the related representation classes have in terms of localizing all objects present in the search region, under finite computational resources and a search cost constraint. We present a number of bounds relating the noise-rate of low level feature detection to the VC-dimension of an object representable by an architecture satisfying the given computational constraints. We prove that under certain conditions, the corresponding classes of object localization and recognition problems are efficiently learnable in the presence of noise and under a purposive learning strategy, as there exists a polynomial upper bound on the minimum number of examples necessary to correctly localize the targets under the given models of uncertainty. We also use these arguments to show that passive approaches to the same problem do not necessarily guarantee that the problem is efficiently learnable. Under this formulation, we prove the existence of a number of emergent relations between the object detection noise-rate, the scene representation length, the object class complexity, and the representation class complexity, which demonstrate that selective attention is not only necessary due to computational complexity constraints, but it is also necessary as a noise-suppression mechanism and as a mechanism for efficient object class learning. These results concretely demonstrate the advantages of active, purposive and attentive approaches for solving complex vision problems.},
journal = {Int. J. Comput. Vision},
month = jan,
pages = {95–142},
numpages = {48},
keywords = {Visual search, Object recognition, Computational complexity of vision, Attention, Active vision}
}


%[6] Whitehead, Steven D., and Dana H. Ballard. "Active perception and %reinforcement learning." Machine Learning Proceedings 1990. Morgan Kaufmann, 1990. %179-188.
%Whitehead, S. D., & Ballard, D. H. (1990). Active perception and reinforcement %learning. Neural Computation, 2(4), 409–419. %https://doi.org/10.1162/neco.1990.2.4.409
@article{Whitehead_1990, 
title={Active perception and reinforcement learning}, 
  volume={2},
  number={4},
journal={Neural Computation}, 
author={Whitehead, Steven D. and Ballard, Dana H.}, 
year={1990}, 
pages={409–419}} 


%[7] Lewis, Sarah, et al. "Accurate imputation and efficient data acquisitionwith %transformer-based vaes." NeurIPS 2021 Workshop on Deep Generative Models and %Downstream Applications. 2021.
@inproceedings{lewis_2021_accurate,
title={Accurate Imputation and Efficient Data Acquisition with Transformer-based {VAE}s},
author={Sarah Lewis and Tatiana Matejovicova and Yingzhen Li and Angus Lamb and Yordan Zaykov and Miltiadis Allamanis and Cheng Zhang},
booktitle={NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},
year={2021},
url={https://openreview.net/forum?id=N_OwBEYTcKK}
}


@article{Arridge_Maass_Oktem_Schonlieb_2019, 
title={Solving inverse problems using data-driven models}, 
volume={28}, 
DOI={10.1017/S0962492919000059}, 
journal={Acta Numerica}, 
author={Arridge, Simon and Maass, Peter and Öktem, Ozan and Schönlieb, Carola-Bibiane}, 
year={2019}, 
pages={1–174}} 

@InProceedings{pmlr-v139-foster21a,
  title = 	 {Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design},
  author =       {Foster, Adam and Ivanova, Desi R and Malik, Ilyas and Rainforth, Tom},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3384--3395},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/foster21a/foster21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/foster21a.html},
  abstract = 	 {We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of adaptive Bayesian experimental design that allows experiments to be run in real-time. Traditional sequential Bayesian optimal experimental design approaches require substantial computation at each stage of the experiment. This makes them unsuitable for most real-world applications, where decisions must typically be made quickly. DAD addresses this restriction by learning an amortized design network upfront and then using this to rapidly run (multiple) adaptive experiments at deployment time. This network represents a design policy which takes as input the data from previous steps, and outputs the next design using a single forward pass; these design decisions can be made in milliseconds during the live experiment. To train the network, we introduce contrastive information bounds that are suitable objectives for the sequential setting, and propose a customized network architecture that exploits key symmetries. We demonstrate that DAD successfully amortizes the process of experimental design, outperforming alternative strategies on a number of problems.}
}


@inproceedings{10.5555/2074158.2074191,
author = {Horvitz, Eric and Barry, Matthew},
title = {Display of information for time-critical decision making},
year = {1995},
isbn = {1558603859},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We describe methods for managing the complexity of information displayed to people responsible for making high-stakes, time-critical decisions. The techniques provide tools for real-time control of the configuration and quantity of information displayed to a user, and a methodology for designing flexible human-computer interfaces for monitoring applications. After defining a prototypical set of display decision problems, we introduce the expected value of revealed information (EVRI) and the related measure of expected value of displayed information (EVDI). We describe how these measures can be used to enhance computer displays used for monitoring complex systems. We motivate the presentation by discussing our efforts to employ decision-theoretic control of displays for a time-critical monitoring application at, the NASA Mission Control Center in Houston},
booktitle = {Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence},
pages = {296–305},
numpages = {10},
location = {Montr\'{e}al, Qu\'{e}, Canada},
series = {UAI'95}
}

@book{garnett_bayesoptbook_2023,
  author    = {Garnett, Roman},
  title     = {{Bayesian Optimization}},
  year      = {2023},
  publisher = {Cambridge University Press}
}

@book{Mockus_1989,
  author    = {Mockus, Jonas},
  title     = {{Bayesian Approach to Global Optimization}},
  year      = {1989},
  publisher = {Springer Dordrecht: Kluwer Academic}
}


@article{10.1162/neco.1992.4.4.590,
    author = {MacKay, David J. C.},
    title = "{Information-Based Objective Functions for Active Data Selection}",
    journal = {Neural Computation},
    volume = {4},
    number = {4},
    pages = {590-604},
    year = {1992},
    month = {07},
    abstract = "{Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed that measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1992.4.4.590},
    url = {https://doi.org/10.1162/neco.1992.4.4.590},
    eprint = {https://direct.mit.edu/neco/article-pdf/4/4/590/812354/neco.1992.4.4.590.pdf},
}

@article{10.1214/23-STS915,
author = {Tom Rainforth and Adam Foster and Desi R. Ivanova and Freddie Bickford Smith},
title = {{Modern Bayesian Experimental Design}},
volume = {39},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {100 -- 114},
keywords = {Active learning, Bayesian adaptive design, Bayesian optimal design, information maximization},
year = {2024},
doi = {10.1214/23-STS915},
URL = {https://doi.org/10.1214/23-STS915}
}

@article{doi:10.1137/21M1414978,
author = {Habring, Andreas and Holler, Martin},
title = {A Generative Variational Model for Inverse Problems in Imaging},
journal = {SIAM Journal on Mathematics of Data Science},
volume = {4},
number = {1},
pages = {306-335},
year = {2022},
doi = {10.1137/21M1414978},

URL = { 
    
        https://doi.org/10.1137/21M1414978
    
    

},
eprint = { 
    
        https://doi.org/10.1137/21M1414978

}
,
    abstract = { This paper is concerned with the development, analysis, and numerical realization of a novel variational model for the regularization of inverse problems in imaging. The proposed model is inspired by the architecture of generative convolutional neural networks; it aims to generate the unknown from variables in a latent space via multilayer convolutions and nonlinear penalties, and penalizes an associated cost. In contrast to conventional neural-network-based approaches, however, the convolution kernels are learned directly from the measured data such that no training is required. The present work provides a mathematical analysis of the proposed model in a function space setting, including proofs for regularity and existence/stability of solutions, and convergence for vanishing noise. Moreover, in a discretized setting, a numerical algorithm for solving various types of inverse problems with the proposed model is derived. Numerical results are provided for applications in inpainting, denoising, deblurring under noise, superresolution, and JPEG decompression with multiple test images. }
}

@InProceedings{KingBa15,
  author    = {Kingma, Diederik and Ba, Jimmy},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title     = {Adam: A Method for Stochastic Optimization},
  year      = {2015},
  address   = {San Diega, CA, USA},
  optmonth  = {12},
}

%M. Edgar, G. Gibson, and M. Padgett, “Principles and prospects for single-pixel imaging,” Nat. Photonics 13(1), 13–20 (2019). 
%Edgar, M.P., Gibson, G.M. & Padgett, M.J. Principles and prospects for single-pixel imaging. Nature Photon 13, 13–20 (2019). https://doi.org/10.1038/s41566-018-0300-7

@article{edgar_principles_2019,
	title = {Principles and prospects for single-pixel imaging},
	volume = {13},
	journal = {Nature Photon},
	author = {Edgar, M.P. and Gibson, G.M. and Padgett, M.J.},
		year = {2019},
		pages = {13--20},
}

@Inbook{Ahmed1975,
author="Ahmed, Nasir
and Rao, Kamisetty Ramamohan",
title="Walsh-Hadamard Transform",
bookTitle="Orthogonal Transforms for Digital Signal Processing",
year="1975",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="99--152",
abstract="This chapter is devoted to the study of the Walsh-Hadamard transform (WHT), which is perhaps the most well-known of the nonsinusoidal orthogonal transforms. The WHT has gained prominence in various digital signal processing applications, since it can essentially be computed using additions and subtractions only. Consequently its hardware implementation is also simpler.",
isbn="978-3-642-45450-9",
doi="10.1007/978-3-642-45450-9_6",
url="https://doi.org/10.1007/978-3-642-45450-9_6"
}

@book{bengio_dl_book, 
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron}, title = {Deep Learning}, year = {2016}, isbn = {0262035618}, publisher = {The MIT Press}, abstract = {"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject." -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.} }

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
}

@ARTICLE{1284395,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612}}%,
  keywords={Image quality;Humans;Transform coding;Visual system;Visual perception;Data mining;Layout;Quality assessment;Degradation;Indexes},
  doi={10.1109/TIP.2003.819861}}

%Higham, C.F., Murray-Smith, R., Padgett, M.J. et al. Deep learning for real-time single-pixel video. Sci Rep 8, 2369 (2018). https://doi.org/10.1038/s41598-018-20521-y

@article{Higham_DLRTSPV_2018,
  author  = {C.F. Higham and R. Murray-Smith and M.J. Padgett, M.J. and M.P. Edgar},
  title   = {Deep learning for real-time single-pixel video},
  journal = {Sci Rep},
  year    = {2018},
  volume  = {8},
  pages   = {2018}}
 % url     = {https://doi.org/10.1038/s41598-018-20521-y}
}



@book{10.5555/1162264,
author = {Bishop, Christopher M.},
title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
year = {2006},
isbn = {0387310738},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

@article{JMLR:v14:hoffman13a,
  author  = {Matthew D. Hoffman and David M. Blei and Chong Wang and John Paisley},
  title   = {Stochastic Variational Inference},
  journal = {Journal of Machine Learning Research},
  year    = {2013},
  volume  = {14},
  number  = {40},
  pages   = {1303--1347}}%,
  url     = {http://jmlr.org/papers/v14/hoffman13a.html}
}

@InProceedings{pmlr-v80-kim18e,
  title = 	 {Semi-Amortized Variational Autoencoders},
  author =       {Kim, Yoon and Wiseman, Sam and Miller, Andrew and Sontag, David and Rush, Alexander},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2678--2687},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kim18e/kim18e.pdf}}%,
  url = 	 {https://proceedings.mlr.press/v80/kim18e.html},
  abstract = 	 {Amortized variational inference (AVI) replaces instance-specific local inference with a global inference network. While AVI has enabled efficient training of deep generative models such as variational autoencoders (VAE), recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. This semi-amortized approach enables the use of rich generative models without experiencing the posterior-collapse phenomenon common in training VAEs for problems like text generation. Experiments show this approach outperforms strong autoregressive and variational baselines on standard text and image datasets.}
}

@misc{xiao2017fashionmnist,
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images
of 70,000 fashion products from 10 categories, with 7,000 images per category.
The training set has 60,000 images and the test set has 10,000 images.
Fashion-MNIST is intended to serve as a direct drop-in replacement for the
original MNIST dataset for benchmarking machine learning algorithms, as it
shares the same image size, data format and the structure of training and
testing splits. The dataset is freely available at
https://github.com/zalandoresearch/fashion-mnist},
  added-at = {2021-10-12T06:50:19.000+0200},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  biburl = {https://www.bibsonomy.org/bibtex/2de51af2f6c7d8b0f4cd84a428bb17967/andolab},
  description = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  interhash = {0c81f9a6170118f14703b6796101ce40},
  intrahash = {de51af2f6c7d8b0f4cd84a428bb17967},
  keywords = {Fashion-MNIST Image_Classification_Benchmark},
  timestamp = {2023-01-31T20:34:07.000+0100},
  title = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning
  Algorithms},
  url = {http://arxiv.org/abs/1708.07747},
  year = 2017
}




@inproceedings{Bohm:2019hpu,
    author = {B\"ohm, Vanessa and Lanusse, Fran\c{c}ois and Seljak, Uro\v{s}},
    title = "{Uncertainty Quantification with Generative Models}",
    booktitle = "{33rd Annual Conference on Neural Information Processing Systems}",
    eprint = "1910.10046",
    archivePrefix = "arXiv",
    primaryClass = "stat.ML",
    month = "10",
    year = "2019"
}



@article{10.1214/aoms/1177728796,
author = {Eugene Lukacs and Edgar P. King},
title = {{A Property of the Normal Distribution}},
volume = {25},
journal = {The Annals of Mathematical Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {389 -- 394},
year = {1954},
doi = {10.1214/aoms/1177728796},
URL = {https://doi.org/10.1214/aoms/1177728796}
}



%Connor Meehan, Jonathan Ebrahimian, Wayne Moore, and Stephen Meehan (2022). Uniform Manifold Approximation and Projection (UMAP) (https://www.mathworks.com/matlabcentral/fileexchange/71902), MATLAB Central File Exchange.

@misc
{  Meehan_2022,
   author={Connor Meehan and Jonathan Ebrahimian and Wayne Moore and Stephen Meehan}, 
   title={Uniform Manifold Approximation and Projection ({UMAP})}, 
   year=2022,
   howpublished = "\url{https://www.mathworks.com/matlabcentral/fileexchange/71902}"
}

@inproceedings{higgins2017betavae,
title={beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational Framework},
author={Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
booktitle={International Conference on Learning Representations},
year={2017}}
%url={https://openreview.net/forum?id=Sy2fzU9gl}
}


@InProceedings{Hoffman_2016,
  title = 	 {{ELBO} surgery: yet another way to carve up the variational evidence lower bound.},
  author =       {Hoffman, M. D. and Johnson M. J.},
  booktitle = 	 {Workshop in Advances in Approximate Bayesian Inference},
  pages = 	 {},
  year = 	 {2016},
}


@InProceedings{pmlr-v89-dieng19a,
  title = 	 {Avoiding Latent Variable Collapse with Generative Skip Models},
  author =       {Dieng, Adji B. and Kim, Yoon and Rush, Alexander M. and Blei, David M.},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2397--2405},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/dieng19a/dieng19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/dieng19a.html},
  abstract = 	 {Variational autoencoders (VAEs) learn distributions of high-dimensional data. They model data with a deep latent-variable model and then fit the model by maximizing a lower bound of the log marginal likelihood. VAEs can capture complex distributions, but they can also suffer from an issue known as "latent variable collapse," especially if the likelihood model is powerful. Specifically, the lower bound involves an approximate posterior of the latent variables; this posterior "collapses" when it is set equal to the prior, i.e., when the approximate posterior is independent of the data. While VAEs learn good generative models, latent variable collapse prevents them from learning useful representations. In this paper, we propose a simple new way to avoid latent variable collapse by including skip connections in our generative model; these connections enforce strong links between the latent variables and the likelihood function. We study generative skip models both theoretically and empirically. Theoretically, we prove that skip models increase the mutual information between the observations and the inferred latent variables. Empirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to existing VAE architectures, we show that generative skip models maintain similar predictive performance but lead to less collapse and provide more meaningful representations of the data.}
}


@misc{zhang2022improving,
      title={Improving {VAE}-based Representation Learning}, 
      author={Mingtian Zhang and Tim Z. Xiao and Brooks Paige and David Barber},
      year={2022},
      eprint={2205.14539},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2205.14539}, 
}

@inproceedings{NIPS2016_ddeebdee,
 author = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Improved Variational Inference with Inverse Autoregressive Flow},
 volume = {29},
 year = {2016}
}

@inproceedings{NEURIPS2020_Vahdat,
 author = {Vahdat, Arash and Kautz, Jan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {19667--19679},
 publisher = {Curran Associates, Inc.},
 title = {NVAE: A Deep Hierarchical Variational Autoencoder},
 volume = {33},
 year = {2020}
}

@inproceedings{
karras2021aliasfree,
title={Alias-Free Generative Adversarial Networks},
author={Tero Karras and Miika Aittala and Samuli Laine and Erik H{\"a}rk{\"o}nen and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=Owggnutk6lE}
}

@inproceedings{NEURIPS2019_Razavi,
 author = {Razavi, Ali and van den Oord, Aaron and Vinyals, Oriol},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generating Diverse High-Fidelity Images with {VQ-VAE-2}},
 volume = {32},
 year = {2019}
}

@inproceedings{ICASSP_Catal_2020,
  abstract     = {{Active inference is a process theory of the brain that states that all living organisms infer actions in order to minimize their (expected) free energy. However, current experiments are limited to predefined, often discrete, state spaces. In this paper we use recent advances in deep learning to learn the state space and approximate the necessary probability distributions to engage in active inference.}},
  author       = {{Catal, Ozan and Verbelen, Tim and Nauta, Johannes and De Boom, Cedric and Dhoedt, Bart}},
  booktitle    = {{ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}},
  isbn         = {{9781509066315}},
  issn         = {{2379-190X}},
  keywords     = {{FREE-ENERGY PRINCIPLE,active inference,deep learning,perception,planning}},
  language     = {{eng}},
  location     = {{Barcelona, Spain}},
  pages        = {{3952--3956}},
  publisher    = {{IEEE}},
  title        = {{Learning perception and planning with deep active inference}},
  url          = {{http://dx.doi.org/10.1109/ICASSP40776.2020.9054364}},
  year         = {{2020}},
}

@inproceedings{NEURIPS2020_Fountas,
 author = {Fountas, Zafeirios and Sajid, Noor and Mediano, Pedro and Friston, Karl},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11662--11675},
 publisher = {Curran Associates, Inc.},
 title = {Deep active inference agents using Monte-Carlo methods},
 url = {https://proceedings.neurips.cc/paper/2020/file/865dfbde8a344b44095495f3591f7407-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{Ueltzh_ffer_2018,
	doi = {10.1007/s00422-018-0785-7},
  
	url = {https://doi.org/10.1007%2Fs00422-018-0785-7},
  
	year = 2018,
	month = {oct},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {112},
  
	number = {6},
  
	pages = {547--573},
  
	author = {Kai Ueltzhöffer},
  
	title = {Deep active inference},
  
	journal = {Biological Cybernetics}
}

@article{Blei_VI_2017,
author = {David M. Blei and Alp Kucukelbir and Jon D. McAuliffe},
title = {Variational Inference: A Review for Statisticians},
journal = {Journal of the American Statistical Association},
volume = {112},
number = {518},
pages = {859-877},
year  = {2017},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2017.1285773},
URL = {  https://doi.org/10.1080/01621459.2017.1285773},
eprint = { https://doi.org/10.1080/01621459.2017.1285773 }
}

@ARTICLE{Mirza_2016,
  
AUTHOR={Mirza, M. Berk and Adams, Rick A. and Mathys, Christoph D. and Friston, Karl J.},   
	 
TITLE={Scene Construction, Visual Foraging, and Active Inference},      
	
JOURNAL={Frontiers in Computational Neuroscience},      
	
VOLUME={10},           
	
YEAR={2016},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fncom.2016.00056},       
	
DOI={10.3389/fncom.2016.00056},      
	
ISSN={1662-5188},   
   
ABSTRACT={This paper describes an active inference scheme for visual searches and the perceptual synthesis entailed by scene construction. Active inference assumes that perception and action minimize variational free energy, where actions are selected to minimize the free energy expected in the future. This assumption generalizes risk-sensitive control and expected utility theory to include epistemic value; namely, the value (or salience) of information inherent in resolving uncertainty about the causes of ambiguous cues or outcomes. Here, we apply active inference to saccadic searches of a visual scene. We consider the (difficult) problem of categorizing a scene, based on the spatial relationship among visual objects where, crucially, visual cues are sampled myopically through a sequence of saccadic eye movements. This means that evidence for competing hypotheses about the scene has to be accumulated sequentially, calling upon both prediction (planning) and postdiction (memory). Our aim is to highlight some simple but fundamental aspects of the requisite functional anatomy; namely, the link between approximate Bayesian inference under mean field assumptions and functional segregation in the visual cortex. This link rests upon the (neurobiologically plausible) process theory that accompanies the normative formulation of active inference for Markov decision processes. In future work, we hope to use this scheme to model empirical saccadic searches and identify the prior beliefs that underwrite intersubject variability in the way people forage for information in visual scenes (e.g., in schizophrenia).}
}

@ARTICLE{Heins_2020,
  
AUTHOR={Heins, R. Conor and Mirza, M. Berk and Parr, Thomas and Friston, Karl and Kagan, Igor and Pooresmaeili, Arezoo},   
	 
TITLE={Deep Active Inference and Scene Construction},      
	
JOURNAL={Frontiers in Artificial Intelligence},      
	
VOLUME={3},           
	
YEAR={2020},      
	  
URL={https://www.frontiersin.org/articles/10.3389/frai.2020.509354},       
	
DOI={10.3389/frai.2020.509354},      
	
ISSN={2624-8212},   
   
ABSTRACT={Adaptive agents must act in intrinsically uncertain environments with complex latent structure. Here, we elaborate a model of visual foraging—in a hierarchical context—wherein agents infer a higher-order visual pattern (a “scene”) by sequentially sampling ambiguous cues. Inspired by previous models of scene construction—that cast perception and action as consequences of approximate Bayesian inference—we use active inference to simulate decisions of agents categorizing a scene in a hierarchically-structured setting. Under active inference, agents develop probabilistic beliefs about their environment, while actively sampling it to maximize the evidence for their internal generative model. This approximate evidence maximization (i.e., self-evidencing) comprises drives to both maximize rewards and resolve uncertainty about hidden states. This is realized via minimization of a free energy functional of posterior beliefs about both the world as well as the actions used to sample or perturb it, corresponding to perception and action, respectively. We show that active inference, in the context of hierarchical scene construction, gives rise to many empirical evidence accumulation phenomena, such as noise-sensitive reaction times and epistemic saccades. We explain these behaviors in terms of the principled drives that constitute the expected free energy, the key quantity for evaluating policies under active inference. In addition, we report novel behaviors exhibited by these active inference agents that furnish new predictions for research on evidence accumulation and perceptual decision-making. We discuss the implications of this hierarchical active inference scheme for tasks that require planned sequences of information-gathering actions to infer compositional latent structure (such as visual scene construction and sentence comprehension). This work sets the stage for future experiments to investigate active inference in relation to other formulations of evidence accumulation (e.g., drift-diffusion models) in tasks that require planning in uncertain environments with higher-order structure.}
}

%Citation: Friston KJ, Daunizeau J, Kiebel SJ (2009) Reinforcement Learning or Active Inference? PLoS ONE 4(7): e6421. https://doi.org/10.1371/journal.pone.0006421

@article{Friston2009_Reinforce,
  title={Reinforcement Learning or Active Inference?},
  author={K.J. Friston and J. Daunizeau and S.J. Kiebel},
  journal={PLoS ONE},
  year={2009},
  volume={4},
  number = {7},
  pages={e6421}
}

%Friston KJ, Parr T, de Vries B. The graphical brain: Belief propagation and active inference. Netw Neurosci. 2017;1(4):381-414. doi: 10.1162/NETN_a_00018. Epub 2017 Dec 31. PMID: 29417960; PMCID: PMC5798592.

@article{Friston2017_Graphical,
  title={The graphical brain: Belief propagation and active inference},
  author={K.J. Friston and T. Parr and B. de Vries},
  journal={Netw Neurosci.},
  year={2017},
  volume={1},
  number = {4},
  pages={381-414}
}


@article{Ramstead_TwoDen_2020,
author = {Maxwell JD Ramstead and Michael D Kirchhoff and Karl J Friston},
title ={A tale of two densities: active inference is enactive inference},
journal = {Adaptive Behavior},
volume = {28},
number = {4},
pages = {225-239},
year = {2020},
doi = {10.1177/1059712319862774},
URL = { https://doi.org/10.1177/1059712319862774},
eprint = { https://doi.org/10.1177/1059712319862774},
    abstract = { The aim of this article is to clarify how best to interpret some of the central constructs that underwrite the free-energy principle (FEP) – and its corollary, active inference – in theoretical neuroscience and biology: namely, the role that generative models and variational densities play in this theory. We argue that these constructs have been systematically misrepresented in the literature, because of the conflation between the FEP and active inference, on the one hand, and distinct (albeit closely related) Bayesian formulations, centred on the brain – variously known as predictive processing, predictive coding or the prediction error minimisation framework. More specifically, we examine two contrasting interpretations of these models: a structural representationalist interpretation and an enactive interpretation. We argue that the structural representationalist interpretation of generative and recognition models does not do justice to the role that these constructs play in active inference under the FEP. We propose an enactive interpretation of active inference – what might be called enactive inference. In active inference under the FEP, the generative and recognition models are best cast as realising inference and control – the self-organising, belief-guided selection of action policies – and do not have the properties ascribed by structural representationalists. }
}


@book{vonHelmholtz1867-VONTOP,
	author = {Heinrich von Helmholtz},
	publisher = {Dover Publications},
	title = {Treatise on Physiological Optics Vol. Iii},
	year = {1867}
}

%Friston, K. (2009). “The free-energy principle: a rough guide to the brain?” Trends in
%Cognitive Sciences 13(7): 293–301.

@article{Friston2007FreeenergyAT,
  title={Free-energy and the brain},
  author={Karl J. Friston and Klaas Enno Stephan},
  journal={Synthese},
  year={2007},
  volume={159},
  pages={417-458}
}

@article{FRISTON2016862,
title = {Active inference and learning},
journal = {Neurosci. Biobehav. Rev.},
volume = {68},
pages = {862-879},
year = {2016},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2016.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0149763416301336},
author = {Karl Friston and Thomas FitzGerald and Francesco Rigoli and Philipp Schwartenbeck and John O. Doherty and Giovanni Pezzulo},
keywords = {Active inference, Habit learning, Bayesian inference, Goal-directed, Free energy, Information gain, Bayesian surprise, Epistemic value, Exploration, Exploitation},
abstract = {This paper offers an active inference account of choice behaviour and learning. It focuses on the distinction between goal-directed and habitual behaviour and how they contextualise each other. We show that habits emerge naturally (and autodidactically) from sequential policy optimisation when agents are equipped with state-action policies. In active inference, behaviour has explorative (epistemic) and exploitative (pragmatic) aspects that are sensitive to ambiguity and risk respectively, where epistemic (ambiguity-resolving) behaviour enables pragmatic (reward-seeking) behaviour and the subsequent emergence of habits. Although goal-directed and habitual policies are usually associated with model-based and model-free schemes, we find the more important distinction is between belief-free and belief-based schemes. The underlying (variational) belief updating provides a comprehensive (if metaphorical) process theory for several phenomena, including the transfer of dopamine responses, reversal learning, habit formation and devaluation. Finally, we show that active inference reduces to a classical (Bellman) scheme, in the absence of ambiguity.}
}


@article{Friston_2009,
	year = 2009,
	volume = {13},
 	number = {7},
 	pages = {293–301},
 	author = {K. Friston},
 	title = {The free-energy principle: a rough guide to the brain?},
  	journal = {Trends in Cognitive Sciences}
}

%Doya, K., Ishii, S., Pouget, A., & Rao, R. P. N. (Eds.). (2007). Bayesian brain: Probabilistic approaches to neural coding. MIT Press. 
@book{doya_bb_book,
  editor = {Doya, K. and Ishii, S. and Pouget, A. and Rao, R. P. N.},
  title = {Bayesian Brain: Probabilistic Approaches to Neural Coding},
  year = 2007,
  publisher = {MIT Press},
}

@ARTICLE{Catal_2020,
AUTHOR={{\c{C}}atal, Ozan and Wauthier, Samuel and De Boom, Cedric and Verbelen, Tim and Dhoedt, Bart},   
TITLE={Learning Generative State Space Models for Active Inference},      
JOURNAL={Frontiers in Computational Neuroscience},      
VOLUME={14},           
YEAR={2020},      
URL={https://www.frontiersin.org/articles/10.3389/fncom.2020.574372},       
DOI={10.3389/fncom.2020.574372},      
ISSN={1662-5188},   
ABSTRACT={In this paper we investigate the active inference framework as a means to enable autonomous behavior in artificial agents. Active inference is a theoretical framework underpinning the way organisms act and observe in the real world. In active inference, agents act in order to minimize their so called free energy, or prediction error. Besides being biologically plausible, active inference has been shown to solve hard exploration problems in various simulated environments. However, these simulations typically require handcrafting a generative model for the agent. Therefore we propose to use recent advances in deep artificial neural networks to learn generative state space models from scratch, using only observation-action sequences. This way we are able to scale active inference to new and challenging problem domains, whilst still building on the theoretical backing of the free energy principle. We validate our approach on the mountain car problem to illustrate that our learnt models can indeed trade-off instrumental value and ambiguity. Furthermore, we show that generative models can also be learnt using high-dimensional pixel observations, both in the OpenAI Gym car racing environment and a real-world robotic navigation task. Finally we show that active inference based policies are an order of magnitude more sample efficient than Deep Q Networks on RL tasks.}
}


@article{Mazzaglia_2022,
	doi = {10.3390/e24020301},
	url = {https://doi.org/10.3390%2Fe24020301},
	year = 2022,
	month = {feb},
	publisher = {{MDPI} {AG}},
	volume = {24},
 	number = {2},
 	pages = {301},
 	author = {Pietro Mazzaglia and Tim Verbelen and Ozan {\c{C}}atal and Bart Dhoedt},
 	title = {The Free Energy Principle for Perception and Action: A Deep Learning Perspective},
  	journal = {Entropy}
}


@book{parr_ai_book, 
author = { Parr, Thomas and Pezzulo, Giovanni and Friston, Karl J.}, title = {Active Inference: The Free Energy Principle in Mind, Brain, and Behavior}, year = {2022}, isbn = {9780262369978}, publisher = {The MIT Press}}




@InProceedings{pmlr-v97-ma19c,
  title = 	 {{EDDI}: Efficient Dynamic Discovery of High-Value Information with Partial {VAE}},
  author =       {Ma, Chao and Tschiatschek, Sebastian and Palla, Konstantina and Hernandez-Lobato, Jose Miguel and Nowozin, Sebastian and Zhang, Cheng},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4234--4243},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR}}%,
  pdf = 	 {http://proceedings.mlr.press/v97/ma19c/ma19c.pdf},
  url = 	 {https://proceedings.mlr.press/v97/ma19c.html},
  abstract = 	 {Many real-life decision making situations allow further relevant information to be acquired at a specific cost, for example, in assessing the health status of a patient we may decide to take additional measurements such as diagnostic tests or imaging scans before making a final assessment. Acquiring more relevant information enables better decision making, but may be costly. How can we trade off the desire to make good decisions by acquiring further information with the cost of performing that acquisition? To this end, we propose a principled framework, named <em>EDDI</em> (Efficient Dynamic Discovery of high-value Information), based on the theory of Bayesian experimental design. In EDDI, we propose a novel <em>partial variational autoencoder</em> (Partial VAE) to predict missing data entries problematically given any subset of the observed ones, and combine it with an acquisition function that maximizes expected information gain on a set of target variables. We show cost reduction at the same decision quality and improved decision quality at the same cost in multiple machine learning benchmarks and two real-world health-care applications.}
}

@inproceedings{10.1145/3219819.3220084,
author = {Huang, Sheng-Jun and Xu, Miao and Xie, Ming-Kun and Sugiyama, Masashi and Niu, Gang and Chen, Songcan},
title = {Active Feature Acquisition with Supervised Matrix Completion},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220084},
doi = {10.1145/3219819.3220084},
abstract = {Feature missing is a serious problem in many applications, which may lead to low quality of training data and further significantly degrade the learning performance. While feature acquisition usually involves special devices or complex process, it is expensive to acquire all feature values for the whole dataset. On the other hand, features may be correlated with each other, and some values may be recovered from the others. It is thus important to decide which features are most informative for recovering the other features as well as improving the learning performance. In this paper, we try to train an effective classification model with least acquisition cost by jointly performing active feature querying and supervised matrix completion. When completing the feature matrix, a novel objective function is proposed to simultaneously minimize the reconstruction error on observed entries and the supervised loss on training data. When querying the feature value, the most uncertain entry is actively selected based on the variance of previous iterations. In addition, a bi-objective optimization method is presented for cost-aware active selection when features bear different acquisition costs. The effectiveness of the proposed approach is well validated by both theoretical analysis and experimental study.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1571–1579},
numpages = {9},
keywords = {matrix completion, feature acquisition, active learning},
location = {London, United Kingdom},
series = {KDD '18}
}

@TechReport{melville:tech04,
title={Active Feature-Value Acquisition for Classifier Induction},
author={Prem Melville and Maytal Saar-Tsechansky and Foster Provost and Raymond J. Mooney},
number={UT-AI-TR-04-311},
month={February},
institution={Artificial Intelligence Lab, University of Texas at Austin},
url="http://www.cs.utexas.edu/users/ai-lab?melville:tech04",
year={2004}
}

@article{10.1214/aoms/1177728069,
author = {D. V. Lindley},
title = {{On a Measure of the Information Provided by an Experiment}},
volume = {27},
journal = {The Annals of Mathematical Statistics},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {986 -- 1005},
year = {1956},
doi = {10.1214/aoms/1177728069},
URL = {https://doi.org/10.1214/aoms/1177728069}
}

@ARTICLE{6795562,
  author={MacKay, David J. C.},
  journal={Neural Computation}, 
  title={Information-Based Objective Functions for Active Data Selection}, 
  year={1992},
  volume={4},
  number={4},
  pages={590-604},
  doi={10.1162/neco.1992.4.4.590}}

@InProceedings{pmlr-v80-garnelo18a,
  title = 	 {Conditional Neural Processes},
  author =       {Garnelo, Marta and Rosenbaum, Dan and Maddison, Christopher and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo and Eslami, S. M. Ali},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1704--1713},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/garnelo18a/garnelo18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/garnelo18a.html},
  abstract = 	 {Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet, GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification and image completion.}
}


@article{NAZABAL2020107501,
title = {Handling incomplete heterogeneous data using VAEs},
journal = {Pattern Recognition},
volume = {107},
pages = {107501},
year = {2020},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107501},
url = {https://www.sciencedirect.com/science/article/pii/S0031320320303046},
author = {Alfredo Nazábal and Pablo M. Olmos and Zoubin Ghahramani and Isabel Valera},
keywords = {Generative models, Variational autoencoders, Incomplete heterogenous data},
abstract = {Variational autoencoders (VAEs), as well as other generative models, have been shown to be efficient and accurate for capturing the latent structure of vast amounts of complex high-dimensional data. However, existing VAEs can still not directly handle data that are heterogenous (mixed continuous and discrete) or incomplete (with missing data at random), which is indeed common in real-world applications. In this paper, we propose a general framework to design VAEs suitable for fitting incomplete heterogenous data. The proposed HI-VAE includes likelihood models for real-valued, positive real valued, interval, categorical, ordinal and count data, and allows accurate estimation (and potentially imputation) of missing data. Furthermore, HI-VAE presents competitive predictive performance in supervised tasks, outperforming supervised models when trained on incomplete data.}
}

@misc{wu2018conditional,
      title={Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding}, 
      author={Ga Wu and Justin Domke and Scott Sanner},
      year={2018},
      eprint={1805.07785},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@misc{zhang2018advances,
      title={Advances in Variational Inference}, 
      author={Cheng Zhang and Judith Butepage and Hedvig Kjellstrom and Stephan Mandt},
      year={2018},
      eprint={1711.05597},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{pmlr-v32-rezende14,
  title = 	 {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author = 	 {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1278--1286},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/rezende14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/rezende14.html},
  abstract = 	 {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}
}


@inproceedings{DBLP:journals/corr/KingmaW13,
  author    = {Diederik P. Kingma and
               Max Welling},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Auto-Encoding Variational {B}ayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014}}%,
  url       = {http://arxiv.org/abs/1312.6114},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Lewenberg2017KnowingWT,
  title={Knowing What to Ask: A Bayesian Active Learning Approach to the Surveying Problem},
  author={Yoad Lewenberg and Yoram Bachrach and Ulrich Paquet and Jeffrey S. Rosenschein},
  booktitle={AAAI},
  year={2017}
}

@inproceedings{NIPS2014_97d01458,
 author = {Gopalan, Prem K and Charlin, Laurent and Blei, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Content-based recommendations with Poisson factorization},
 url = {https://proceedings.neurips.cc/paper/2014/file/97d0145823aeb8ed80617be62e08bdcc-Paper.pdf},
 volume = {27},
 year = {2014}
}

@InProceedings{pmlr-v106-hamesse19a,
  title = 	 {Simultaneous Measurement Imputation and Outcome Prediction for Achilles Tendon Rupture Rehabilitation},
  author =       {Hamesse, Charles and Tu, Ruibo and Ackermann, Paul and Kjellstr{\"{o}}m, Hedvig and Zhang, Cheng},
  booktitle = 	 {Proceedings of the 4th Machine Learning for Healthcare Conference},
  pages = 	 {614--640},
  year = 	 {2019},
  editor = 	 {Doshi-Velez, Finale and Fackler, Jim and Jung, Ken and Kale, David and Ranganath, Rajesh and Wallace, Byron and Wiens, Jenna},
  volume = 	 {106},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--10 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v106/hamesse19a/hamesse19a.pdf},
  url = 	 {https://proceedings.mlr.press/v106/hamesse19a.html},
  abstract = 	 {Achilles Tendon Rupture (ATR) is one of the typical soft tissue injuries. Rehabilitation after such a musculoskeletal injury remains a prolonged process with a very variable outcome. Accurately predicting rehabilitation outcome is crucial for treatment decision support. However, it is challenging to train an automatic method for predicting the ATR rehabilitation outcome from treatment data, due to a massive amount of missing entries in the data recorded from ATR patients, as well as complex nonlinear relations between measurements and outcomes. In this work, we design an end-to-end probabilistic framework to impute missing data entries and predict rehabilitation outcomes simultaneously. We evaluate our model on a real-life ATR clinical cohort, comparing with various baselines. The proposed method demonstrates its clear superiority over traditional methods which typically perform imputation and prediction in two separate stages.}
}

@inproceedings{NIPS2016_85422afb,
 author = {Yu, Hsiang-Fu and Rao, Nikhil and Dhillon, Inderjit S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Temporal Regularized Matrix Factorization for High-dimensional Time Series Prediction},
 url = {https://proceedings.neurips.cc/paper/2016/file/85422afb467e9456013a2a51d4dff702-Paper.pdf},
 volume = {29},
 year = {2016}
}


@inproceedings{NIPS2010_08d98638,
 author = {Jain, Prateek and Meka, Raghu and Dhillon, Inderjit},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Guaranteed Rank Minimization via Singular Value Projection},
 url = {https://proceedings.neurips.cc/paper/2010/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf},
 volume = {23},
 year = {2010}
}

@inproceedings{NIPS2009_aa942ab2,
 author = {Keshavan, Raghunandan and Montanari, Andrea and Oh, Sewoong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Matrix Completion from Noisy Entries},
 url = {https://proceedings.neurips.cc/paper/2009/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Paper.pdf},
 volume = {22},
 year = {2009}
}

@inproceedings{10.1145/1390156.1390267,
author = {Salakhutdinov, Ruslan and Mnih, Andriy},
title = {Bayesian Probabilistic Matrix Factorization Using Markov Chain Monte Carlo},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390267},
doi = {10.1145/1390156.1390267},
abstract = {Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be performed efficiently even on very large datasets. However, unless the regularization parameters are tuned carefully, this approach is prone to overfitting because it finds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be efficiently trained using Markov chain Monte Carlo methods by applying them to the Netflix dataset, which consists of over 100 million movie ratings. The resulting models achieve significantly higher prediction accuracy than PMF models trained using MAP estimation.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {880–887},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}


@inproceedings{NIPS2017_f22e4747,
 author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Sets},
 url = {https://proceedings.neurips.cc/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf},
 volume = {30},
 year = {2017}
}

@INPROCEEDINGS{8099499,
  author={Charles, R. Qi and Su, Hao and Kaichun, Mo and Guibas, Leonidas J.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation}, 
  year={2017},
  volume={},
  number={},
  pages={77-85},
  doi={10.1109/CVPR.2017.16}}


@article{ma2019bayesian,
  title={Bayesian {EDDI}: Sequential Variable Selection with {B}ayesian Partial {VAE}},
  author={Ma, Chao and Gong, Wenbo and Tschiatschek, Sebastian and Nowozin, Sebastian and José Miguel Hernández-Lobato and Zhang, Cheng},
  journal={Workshop on Real-World Sequential Decision Making: Reinforcement Learning and Beyond at NeurIPS},
  year={2019},
  tag={PMI,SDM}
}

@inproceedings{
bora2018ambientgan,
title={Ambient{GAN}: Generative models from lossy measurements},
author={Ashish Bora and Eric Price and Alexandros G. Dimakis},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hy7fDog0b},
}

@inproceedings{10.5555/3305381.3305437,
author = {Bora, Ashish and Jalal, Ajil and Price, Eric and Dimakis, Alexandros G.},
title = {Compressed Sensing Using Generative Models},
year = {2017},
publisher = {JMLR.org},
abstract = {The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is represented by sparsity in a well-chosen basis. We show how to achieve guarantees similar to standard compressed sensing but without employing sparsity at all. Instead, we suppose that vectors lie near the range of a generative model G : ℝk → ℝn. Our main theorem is that, if G is L-Lipschitz, then roughly O(k log L) random Gaussian measurements suffice for an ℓ2/ℓ2 recovery guarantee. We demonstrate our results using generative models from published variational autoencoder and generative adversarial networks. Our method can use 5-10x fewer measurements than Lasso for the same accuracy.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {537–546},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}


@ARTICLE{6081866,
  author={Hospedales, Timothy M. and Gong, Shaogang and Xiang, Tao},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Finding Rare Classes: Active Learning with Generative and Discriminative Models}, 
  year={2013},
  volume={25},
  number={2},
  pages={374-386},
  doi={10.1109/TKDE.2011.231}}


@InProceedings{pmlr-v162-zhang22ai,
  title = 	 {Uncertainty Modeling in Generative Compressed Sensing},
  author =       {Zhang, Yilang and Xu, Mengchu and Mao, Xiaojun and Wang, Jian},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {26655--26668},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zhang22ai/zhang22ai.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zhang22ai.html},
  abstract = 	 {Compressed sensing (CS) aims to recover a high-dimensional signal with structural priors from its low-dimensional linear measurements. Inspired by the huge success of deep neural networks in modeling the priors of natural signals, generative neural networks have been recently used to replace the hand-crafted structural priors in CS. However, the reconstruction capability of the generative model is fundamentally limited by the range of its generator, typically a small subset of the signal space of interest. To break this bottleneck and thus reconstruct those out-of-range signals, this paper presents a novel method called CS-BGM that can effectively expands the range of generator. Specifically, CS-BGM introduces uncertainties to the latent variable and parameters of the generator, while adopting the variational inference (VI) and maximum a posteriori (MAP) to infer them. Theoretical analysis demonstrates that expanding the range of generators is necessary for reducing the reconstruction error in generative CS. Extensive experiments show a consistent improvement of CS-BGM over the baselines.}
}

@misc{https://doi.org/10.48550/arxiv.1910.10046,
  doi = {10.48550/ARXIV.1910.10046},
  url = {https://arxiv.org/abs/1910.10046},
  author = {Böhm, Vanessa and Lanusse, François and Seljak, Uroš},
  keywords = {Machine Learning (stat.ML), Cosmology and Nongalactic Astrophysics (astro-ph.CO), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
  title = {Uncertainty Quantification with Generative Models},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{
loo2021generalized,
title={Generalized Variational Continual Learning},
author={Noel Loo and Siddharth Swaroop and Richard E Turner},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=_IM-AfFhna9}
}


@InProceedings{pmlr-v162-mitchell22a,
  title = 	 {Memory-Based Model Editing at Scale},
  author =       {Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {15817--15831},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/mitchell22a/mitchell22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/mitchell22a.html},
  abstract = 	 {Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit’s intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely related to the edit, and they often fail altogether after many edits. As a higher-capacity alternative, we propose Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory and learns to reason over them to modulate the base model’s predictions as needed. To enable more rigorous evaluation of model editors, we introduce three challenging language model editing problems based on question answering, fact-checking, and dialogue generation. We find that only SERAC achieves high performance on all three problems, consistently outperforming existing approaches to model editing by a significant margin. Code, data, and additional project information will be made available at https://sites.google.com/view/serac-editing.}
}

@inproceedings{NEURIPS2021_c46489a2,
 author = {Santurkar, Shibani and Tsipras, Dimitris and Elango, Mahalaxmi and Bau, David and Torralba, Antonio and Madry, Aleksander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {23359--23373},
 publisher = {Curran Associates, Inc.},
 title = {Editing a classifier by rewriting its prediction rules},
 url = {https://proceedings.neurips.cc/paper/2021/file/c46489a2d5a9a9ecfc53b17610926ddd-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{
Sinitsin2020Editable,
title={Editable Neural Networks},
author={Anton Sinitsin and Vsevolod Plokhotnyuk and Dmitry Pyrkin and Sergei Popov and Artem Babenko},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJedXaEtvS}
}


@inproceedings{NEURIPS2020_b8a65506,
 author = {Nguyen, Quoc Phong and Low, Bryan Kian Hsiang and Jaillet, Patrick},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {16025--16036},
 publisher = {Curran Associates, Inc.},
 title = {Variational Bayesian Unlearning},
 url = {https://proceedings.neurips.cc/paper/2020/file/b8a6550662b363eb34145965d64d0cfb-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{NEURIPS2019_5f146156,
 author = {Hara, Satoshi and Nitanda, Atsushi and Maehara, Takanori},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Data Cleansing for Models Trained with SGD},
 url = {https://proceedings.neurips.cc/paper/2019/file/5f14615696649541a025d3d0f8e0447f-Paper.pdf},
 volume = {32},
 year = {2019}
}

@misc{Yingzhen2022,
  doi = {10.48550/ARXIV.2207.04806},
  url = {https://arxiv.org/abs/2207.04806},
  author = {Tanno, Ryutaro and Pradier, Melanie F. and Nori, Aditya and Li, Yingzhen},
   keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Repairing Neural Networks by Leaving the Right Past Behind},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{PARISI201954,
title = {Continual lifelong learning with neural networks: A review},
journal = {Neural Networks},
volume = {113},
pages = {54-71},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300231},
author = {German I. Parisi and Ronald Kemker and Jose L. Part and Christopher Kanan and Stefan Wermter},
keywords = {Continual learning, Lifelong learning, Catastrophic forgetting, Developmental systems, Memory consolidation},
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.}
}

@InProceedings{pmlr-v70-koh17a,
  title = 	 {Understanding Black-box Predictions via Influence Functions},
  author =       {Pang Wei Koh and Percy Liang},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1885--1894},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/koh17a/koh17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/koh17a.html},
  abstract = 	 {How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.}
}

