\section{Related Work}
\textbf{Deep Learning in Survival analysis.} Machine learning and deep learning techniques for survival analysis are not limited to LVSM. Goodfellow et al., "Maxout Networks" introduced the first neural-network-based Cox regression model, allowing nonlinear relationships between covariates. A modern yet similar one is Cui et al., "DeepSurv". Deep Cox Mixture Chen et al., "Deep Cox Mixture Model for Survival Analysis" extends this idea to finite mixture models, but all these Coxian models rely on the proportional hazards (PH) assumption, which results in separated survival functions Chen et al. and may be unrealistic. A famous nonparametric tree ensemble approach, Random Survival Forest Ishwaran et al., "Graphical Models via Generalized Linear Representations" ____ builds multiple decision trees to model the cumulative hazard function, leveraging Nelson-Aalen estimator Borgan et al.. That said, hazard function estimation for discrete time-to-event can also be framed as a series of binary classification problems, which can be solved by black-box methods via various network architectures. DeepHit Lee et al., "DeepHit: A Neural Network for Estimating Competing Risks" uses a simple shared network to model competing risks, while RNN- Zhang et al., "Learning Fine-Grained Representations for Survival Analysis" and Transformer-based Liu et al., "Transformer-based Survival Prediction" structures capture sequential relationships in time-specific predictions. These methods often require additional techniques to mitigate overfitting.

\textbf{Inference Optimality in Survival Analysis.} Improving VI of latent variable models has been extensively discussed in general learning tasks. For instance, Hoffman et al., "Variational Inference for Latent Variables" suggests utilizing a more expressive variational family than the commonly used factorized Gaussians. Maddison et al., "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning" suggests that an annealed training dynamic for model parameters can enhance the estimated $q_\phi$ and $\theta$. We subjectively summarize the types of strategies to facilitate a better estimate $\theta^*$: (1) maximize the availability of optimal VI for more $\theta$, i.e., increasing the support of $\theta$, where $\min_{\phi} B(\theta,\phi)=0$;  (2) propose a tighter lower bound than ELBO to reduce $\min_{\phi} B(\theta,\phi)$ for general $\theta$; (3) training strategies to avoid some notorious suboptimal inference like posterior collapse. %However, most of them do not apply to LVSM and account for censored data. 
As contributions, our criticism on vanilla VI, the extensions of IS and DVI on CDVI, and the discussion on decoder variance fall under each type, respectively.

\textbf{Variational method for other tasks.} Variational methods in survival analysis are not limited to time-to-event modeling. One unsupervised task is identifying potential sub-populations, providing valuable insights for treatment recommendations and clinical decision-making King et al., "Deep Clustering: Discriminative Representations for Clustering via Deep Autoencoders". These clustering models, if used as an intermediate step of time-to-event modeling, can be seen as a restricted LVSM, often in a D-separation latent structure. While a restrictive approach can help prevent overfitting, our criticism remains valid: the objective of VI in unsupervised tasks can be misaligned with M-estimation of the time-to-event distribution, undermining the performance of survival time prediction.