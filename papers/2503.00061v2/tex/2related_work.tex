
\section{Related Work}
\subsection{LLM Agent Safety}
With the increasing deployment of LLM agents in high-stakes domains such as finance~\cite{li2023tradinggpt, DBLP:conf/aaaiss/YuLCJLZLSK24,hu2025leap}, laboratory research~\cite{DBLP:journals/natmi/BranCSBWS24, DBLP:journals/corr/abs-2304-05332}, healthcare~\cite{DBLP:journals/corr/abs-2310-02374,DBLP:journals/corr/abs-2405-02957,DBLP:journals/corr/abs-2401-05654}, and autonomous driving~\cite{DBLP:journals/itsm/CuiMCYW24,DBLP:journals/corr/abs-2309-13193,DBLP:journals/corr/abs-2311-10813}, it has become imperative to address their safety concerns.
Recent studies have focused on agent security, particularly the potential for harmful behaviors, as outlined in general surveys~\cite{DBLP:journals/corr/abs-2407-19354,DBLP:journals/corr/abs-2406-02630} and benchmark analyses~\cite{DBLP:journals/corr/abs-2401-10019,zhang2024agent}, evaluating various risks and attacks across different domains~\cite{DBLP:journals/corr/abs-2402-04247}.
We categorize the risks into two main types: unintentional risks and intentional attacks.

Unintentional risks occur without a malicious attacker. However, LLM agents can still pose risks~\cite{DBLP:conf/iclr/RuanDWPZBDMH24,DBLP:journals/corr/abs-2408-02544} by potentially executing harmful behaviors during interactions with benign users, necessitating improvements in agent robustness.


Intentional attacks involve malicious actors. There are several known methods to attack LLM agents:
(1) IPI attacks: injecting malicious instructions into the agent's tool responses~\cite{DBLP:conf/acl/ZhanLYK24,DBLP:journals/corr/abs-2406-13352}.
(2) Retrieval-augmented generation (RAG) poinsoning: poisoning the knowledge base of RAG-based LLM agents~\cite{DBLP:journals/corr/abs-2407-12784}.
(3) Backdoor attacks: finetuning LLMs to embed triggers that cause the agent to generate harmful behaviors\cite{DBLP:journals/corr/abs-2402-11208,DBLP:conf/acl/WangXZQ24,DBLP:journals/corr/abs-2401-05566}.
Researchers have also proposed other attack methods~\cite{DBLP:journals/corr/abs-2407-20859,DBLP:conf/icml/GuZPDL00L24}.

This paper presents the first study of defenses and adaptive attacks against LLM agents in the context of IPI attacks. Unlike prompt injection attacks in LLMs~\cite{liu2024automatic}, targeting agents with tool usage poses extra challenges: (1) the attack must compel harmful actions rather than just generating a target output, and (2) the greater complexity of inputs and outputs complicates optimization. Moreover, attack success can be directly measured by the execution of the attacker tool, in contrast to the uncertain evaluations based on keyword mapping and LLM judgments in previous work.

\subsection{LLM Safety and Adversarial Attacks}
Research on LLM safety predates that on LLM agent safety. 
Base LLMs, trained on large web corpora, often generate harmful content such as toxicity and bias. 
One common approach to mitigate these risks is finetuning models to align with human preferences, using methods like reinforcement learning from human feedback (RLHF)~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22} and direct preference pptimization (DPO)~\cite{DBLP:conf/nips/RafailovSMMEF23}.
However, malicious actors can still bypass these defenses using carefully crafted prompts, leading to so-called jailbreak attacks~\cite{DBLP:conf/nips/0001HS23,DBLP:conf/iclr/YuanJW0H0T24},  which have become a common red-teaming method for LLMs.
To automate and strengthen these attacks, researchers have developed methods like GCG~\cite{DBLP:journals/corr/abs-2307-15043} and AutoDAN attacks~\cite{DBLP:conf/iclr/LiuXCX24,zhu2023autodan}, which automatically find prompts to jailbreak LLMs. In this paper, we adapt these strategies to the LLM agent setting and evaluate them in the context of IPI attacks under more challenging defense scenarios.



\subsection{Adaptive Attacks}
New attacks that bypass existing defenses frequently arise, a phenomenon well-documented in computer vision~\cite{DBLP:conf/icml/AthalyeC018, DBLP:conf/nips/TramerCBM20,DBLP:conf/cvpr/YuG021,DBLP:conf/ccs/Carlini017}.
Defenses must withstand adaptive attacks to demonstrate their robustness.
The study of adaptive attacks has also expanded into LLMs, particularly in the context of jailbreak~\cite{DBLP:journals/corr/abs-2309-00614,DBLP:journals/corr/abs-2404-02151}.
To the best of our knowledge, this paper is the first to explore adaptive attacks specifically targeting LLM agent safety.
