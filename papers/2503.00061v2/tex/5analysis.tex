\section{Analysis}

In this section, we present detailed results to further analyze various defenses and adaptive attacks.

\subsection{Detailed Results and Analysis}

\input{tex/tables/detection_rate.tex}

\minihead{Detection-based Defense}
We define the \textit{detection rate} as the ratio of test cases classified as attacks by the detectors. 
Table~\ref{tab:detection_rate} shows the detection rates for the original IPI attacks, which contain only malicious instructions in the tool response, and for attacks where an adversarial string is concatenated with the malicious instructions.
We observe that the fine-tuned detector achieves the highest detection rate on the original IPI attacks for the Vicuna-7B based agent and LLM-based detetor for the Llama3-8B based agent.
This is due to that we use the same model as the agent backbone model in LLM-based detector, and Llama3-8B is strong and better safety aligned, showing better ability in detecting IPI attacks.
Notably, for perplexity filtering, the detection rates for the original attacks are zero, as it is designed to detect adversarial strings with nonsensical meaning. When tested on cases with strings generated by GCG, the detection rate rises to 65\%, indicating its effectiveness.

However, after applying adaptive attacks, the detection rates drop to nearly zero in most cases, demonstrating the effectiveness of adversarial strings in bypassing the detectors. This confirms that the stealth objective in multi-objective training is well fulfilled, indicating that further improvements in attack strategies should focus on enhancing the attack objective.


\minihead{Input-level Defense}
From Figure~\ref{fig:results}, we observe that most prompt modification defenses offer weak protection against the attacks for the Vicuna-7B based agent, except for sandwich prevention. After analyzing the results, we note that this is partly because sandwich prevention generates more invalid cases than other defenses. However, it still shows the best defense performance among all prompt designs when considering only valid results.
\input{tex/tables/defense_of_adversarial_string.tex}

\minihead{Model-level Defense}
As mentioned earlier, the GCG attack over the adversarial training defense achieves a high target rate but a relatively low ASR for the Vicuna-7B based agent. 
Upon closer examination, we found that this is primarily due to many unsuccessful cases in the second step of the data stealing attack. 
This highlights one of the limitations of our designed attacks: the adversarial string is only trained to control the agent's response in the first step after receiving external content.
We show more detailed analysis of this limitation in section~\ref{sec:breakdown_asr}.

\input{tex/tables/breakdown_asr.tex}

\minihead{Defenses of Adversarial Attacks}
\label{sec:advstring}
Among the eight defenses, perplexity filtering and paraphrasing are specifically designed to address adversarial strings. 
Figure~\ref{fig:results} shows that our adaptively trained adversarial string achieves a high ASR over these two defenses.
To further analyze this, we compare the results of the adaptive adversarial string with the adversarial string trained using GCG targeting no defense without the adaptive strategy. 
We present the results in Table~\ref{tab:defense_adv_string}, showing that our adaptive adversarial strings are more effective than the non-adaptive strings.

\subsection{Impact on Valid Rate}  
We analyze how defenses and attacks affect the valid rate for the Vicuna-7B based agent, where the valid rate is the ratio of valid outputs.  
For the Llama3-8B based agent, the backbone model’s stronger capabilities result in mostly valid outputs.  
Figure~\ref{fig:valid_rate} presents valid rates across different defenses and attacks.  
The results show that most defenses increase the number of invalid outputs, except for the fine-tuned detector, LLM-based detector, and adversarial finetuning.  
The first two exceptions arise because they directly block certain tool responses that would otherwise lead to invalid outputs.  
Adversarial finetuning increases the valid rate by using finetuning data consisting of valid outputs.  
The adaptive attacks impact valid rates in two ways. Introducing adversarial strings sometimes creates more chaos in the context, resulting in invalid outputs like repetitive content. 
However, the adversarial strings are also designed to force the model to generate the target string starting with ``Thought:'' which follows the ReAct format, leading to more valid outputs.

\subsection{Breakdown of ASRs}
\label{sec:breakdown_asr}

Table~\ref{tab:breakdown_results} provides detailed results for the direct harm attack and the two stages of the data stealing attack. 
The results indicate that our adaptive attacks yield minimal improvement in ASRs for the second stage of the data stealing attack. 
This occurs because all adversarial strings are trained to let the model generate the target output, which only has direct influence for the first step, limiting their impact on the later steps.

\input{tex/figures/cross_results.tex}

\subsection{Cross Evaluation}
Figure~\ref{fig:cross_asr} presents the cross evaluation of different adaptive attacks and defenses.
We observe that for both agents, the adversarial strings trained for specific defenses achieve the highest attack effectiveness against the defense they were trained on in most cases.
For the top three strongest defenses shown in Figure~\ref{fig:results} for the Vicuna-7B based agent—the fine-tuned detector, sandwich prevention, and adversarial finetuning—as well as defenses specifically targeting adversarial strings, i.e., paraphrasing and perplexity filtering, we observe that the adaptive attack for each defense significantly outperforms other attacks.
This highlights the necessity of conducting adaptive attacks to evaluate the robustness of these defenses.





