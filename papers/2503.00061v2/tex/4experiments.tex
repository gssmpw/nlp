\section{Experiments}

In this section, we present the experimental setup and results for the defenses and adaptive attacks.


\subsection{Experiment Settings}
\label{sec:experiment_settings}
\minihead{Dataset}
We use the InjecAgent~\cite{DBLP:conf/acl/ZhanLYK24},
a benchmark designed to evaluate LLM agents' resilience against IPI attacks, to assess the effectiveness of various defenses and attack strategies.
The benchmark contains 1,054 test cases, covering two attack types:
(1) Direct harm attacks: where the attacker instruction  cause the agent to execute actions that can directly harm the user, such as initiating financial transactions or controlling devices like robots.
(2) Data stealing attacks: where the attacker directs the agent to extract sensitive user information, such as health or payment data, and then execute a tool to transmit this information to the attacker.
We randomly select 50 test cases from each attack type, creating a subset of 100 test cases for evaluation. 
This subset includes 25 unique direct harm attacks, 27 distinct data stealing attacks, and 16 user scenarios.

\minihead{Agent Details}
We evaluate two types of LLM agents following InjecAgent: the \textit{prompted agent}, which leverages prompt like ReAct~\cite{DBLP:conf/iclr/YaoZYDSN023} to enable LLMs to reason, perform actions, and receive observations; and the \textit{finetuned agent}, where the backbone LLMs are fine-tuned on function-calling cases, allowing them to be directly deployed as agents.
For prompted agent, we use Vicuna-7B\footnote{\url{https://huggingface.co/lmsys/vicuna-7b-v1.5}}~\cite{NEURIPS2023_91f18a12} as the backbone model, with the detailed prompt shown in Appendix~\ref{appendix:prompt_react}.
For finetuned agent, we use Llama3-8B\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}}.
Both of the models are commonly employed for testing automated adversarial attacks on LLMs~\cite{DBLP:journals/corr/abs-2307-15043,DBLP:conf/iclr/LiuXCX24}. 

We design the training targets for the adversarial string for each type of agent.
For ReAct prompt based agent, we let target $=$ ``Thought: I will use the <$T_a$> tool to'', where <$T_a$> is replaced with the name of the attacker tool.
This target follows the ReAct format, aiming to influence the model's subsequent action by shaping its thought process.
For Llama3-8B based agent, we let target $=$ ``\{"name": "<$T_a$>"''.
This target aligns with the finetuned structure of Llama3-8B for tool using and can directly lead to the execution of the attacker tool.

For the Vicuna-7B-based prompted agent, we evaluate all defenses. 
However, for the finetuned agent, due to its predefined conversational structure and clear separation between tool responses and context, we exclude the defenses for data prompt isolation and sandwich prevention when using Llama3-8B. 


\minihead{Evaluation Process}
During evaluation, following the InjecAgent procedures, we assume the agent successfully executes the user tool and retrieves its response, evaluating only its next one or two outputs.  
In other words, we evaluate a single turn of interaction between the user and the agent, though the agent may interact with tools multiple times within this turn to complete the attacker instruction.  
For direct harm attacks, we consider the attack successful if the model’s output includes the execution of the harmful tool. 
For data stealing attacks, evaluation occurs in two steps:
(1) We consider the first step successful if the model’s output includes executing the data extraction tool.
(2) If the first step succeeds, we use \texttt{gpt-4-0613} to simulate the tool response and ask the agent to generate a second output. We consider the attack fully successful if this second output includes the execution of the data transmission tool.

In addition to categorizing outputs as successful or unsuccessful, the benchmark also define an ``invalid output'' category, which neither lead to success nor failure.
This includes cases where, for example, the model fails to follow the ReAct format or produces repetitive content, highlighting the limited capabilities of the backbone LLM.

\minihead{Evaluation Metric}
Following InjecAgent, we use the attack success rate (ASR) as the primary metric, which is the ratio of successful attacks to the total number of test cases. For each defense and its corresponding adaptive attack, we report two key metrics:
(1) \textit{ASR-defense}: the ASR after deploying the defense strategy.
(2) \textit{ASR-adaptive attack}: the ASR after applying the adaptive attack against the defense.
The original InjecAgent benchmark includes two ASRs: ASR-all and ASR-valid. 
ASR-valid represents the ratio of successful attacks out of the valid outputs. 
By default, we report ASR-all, as we do not credit an attack for producing invalid output.

Notably, the IPI setting offers a more precise evaluation of defenses and adversarial attacks compared to the jailbreak setting, where success is measured by whether the agent performs specific harmful actions. In contrast, jailbreak settings often rely on keyword detection or LLM evaluations to assess maliciousness, which may be subjective and inconsistent~\cite{DBLP:conf/iclr/LiuXCX24}.

Additionally, for the prompted agent, generating the target does not guarantee the success of an attack.
Therefore, to directly evaluate the effectiveness of the designed attacks, we report the \textit{target rate}, which measures the ratio of outputs that start with the training target, better reflecting optimization quality than the ASRs.


\minihead{Implementation Details}
For each agent, we use the agent backbone model in the LLM-based detector, perplexity filtering, and paraphrasing. 
Appendix~\ref{appendix:im_detail} provides further implementation details of the defenses and attacks.





\input{tex/figures/target_rate.tex}

\subsection{Experiment Results}

We present the overall results in Figure~\ref{fig:results}. 
The results show that all \textit{ASR-adaptive attack} across different defenses and agents exceed 50\%, demonstrating that the defenses can be circumvented.

For the Vicuna-7B based agent, most defenses reduce the original ASR from 56\% to lower values, such as 12\% with adversarial finetuning.  
The Llama3-based agent shows greater resilience to IPI attacks, with an original ASR of 9\% and even lower ASRs under defenses.  
This aligns with the conclusion in InjecAgent~\cite{DBLP:conf/acl/ZhanLYK24} that fine-tuned agents are more resilient to IPI attacks.  
For the Vicuna-7B based agent, perplexity filtering, instructional prevention, paraphrasing, and data prompt isolation maintain higher ASRs than other defenses.  
Perplexity filtering and paraphrasing specifically target adversarial strings, leading to their reduced effectiveness.  
However, adaptively trained adversarial strings still bypass these defenses, achieving high ASRs for both agents.  


It is important to note that we cannot directly compare which defense is better based purely on \textit{ASR-defense}, as it only measures how well a defense prevents attacks. A good defense should also minimize the impact on normal cases. However, since this paper focuses on evaluating robustness against adaptive attacks, we do not assess the impact on normal cases here.


\input{tex/figures/valid_rate.tex}


Figure~\ref{fig:target_rate} presents the target rates of generated outputs for the Vicuna-7B based agent, with \textit{ASR-adaptive attack} provided for reference.  
Most GCG models achieve high target rates, reflecting their training effectiveness. However, a high target rate does not always correspond to a high ASR, as seen in GCG over adversarial finetuning.  
Despite its relatively high target rate, it shows a lower ASR than other GCG attacks.  
Additionally, AutoDAN achieves the lowest target rate among the methods but still attains a high ASR.  
Further investigation reveals that many outputs in this setting begin with sequences similar to the target, such as ``Thought: I need to use the <$T_a$> tool to.''  
If we consider these semantically similar outputs as hitting the target, the target rate increases to 62\%.  
We speculate that this occurs because AutoDAN optimizes for the semantic coherence of the adversarial string, leading to outputs that resemble the target when optimization is suboptimal.  



