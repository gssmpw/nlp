
\section{Preliminaries}
Following the notation for IPI attacks introduced by~\citet{DBLP:conf/acl/ZhanLYK24}, let $L$ represent an LLM agent, which consists of an LLM $M$ and a set of tools $\mathcal{T}$.
We denote the benign user instruction as $I_u$, which directs the LLM agent to execute a specifc tool $T_u \in \mathcal{T}$ and receive the corresponding tool response $R_{T_u}$.

In an IPI attack, the tool response $R_{T_u}$ incorporates external content $E_{T_u}$---such as an email or a review---that attackers can manipulate.
Attackers embed a malicious instruction $I_a$ into the external content, which directs the agent to execute an attacker tool $T_a$. 
Upon receiving this response, the LLM agent processes it through a structured prompt, combining both the benign and malicious instructions into the input: $\text{input} = Prompt(I_u, I_a)$.
Then the LLM $M$ generates an output: $\text{output} = M(Prompt(I_u, I_a))$. If the output includes a command to execute the malicious instruction $I_a$, such as ``Action: <$T_a$>'' and ``\{"name": "<$T_a$>"'', the attack is considered successful.



\section{Defense Techniques}
In this paper, we aim to include a representative and practical subset of defenses from prior work that address IPI attacks in the agent setting or can be adapted to it.
We classify the defenses we implemented against IPI attacks into three primary categories. We summarize all defenses and their adaptive attacks in Table~\ref{tab:defense}.

\subsection{Detection-based Defense}
To counter malicious instructions embedded in external content, a straightforward method is to employ a detector $D$ that analyzes the tool response $R_{T_u}$ and flags potential IPI attacks. 
This detection can be based on various approaches:

\minihead{Fine-tuned Detector (FD)} 
We employ a fine-tuned version of DeBERTaV3~\cite{he2021deberta}, specifically designed to detect and classify prompt injection attacks~\cite{deberta-v3-base-prompt-injection-v2}. 
By feeding the tool response $R_{T_u}$ into the model, we obtain a probability score indicating whether the response contains an IPI attack, $P(D(R_{T_u}) = 1)$. 
If the probability exceeds 0.5, we flag it as a successful detection and consider the attack failed.

\minihead{LLM-based Detector (LD)} 
Alternatively, we can use an LLM for detection~\cite{llmdetector}. We design a prompt to instruct the LLM to respond with a simple ``Yes'' or ``No'' based on whether the tool response $R_{T_u}$ contain an IPI attack. The detailed prompt is provided in Appendix~\ref{appendix:llm_based_detector}.

\minihead{Perplexity Filtering (PF)} 
This is an effective strategy for identifying adversarial inputs lacking coherent meaning~\cite{DBLP:journals/corr/abs-2308-14132,DBLP:journals/corr/abs-2309-00614}. 
If the perplexity of a tool response $R_{T_u}$ exceeds a predefined threshold $\theta_\text{ppl}$, we flag it as an attack. Specifically, we set the perplexity threshold to the maximum perplexity of the tool response in the original attack, ensuring none of the original responses are filtered out, following previous work~\cite{DBLP:journals/corr/abs-2309-00614}.


\subsection{Input-level Defense} 
Another approach is to modify the input of the LLM. One method involves altering the agent’s prompt. We include three techniques for designing $Prompt$ to defend against IPI attacks:

\minihead{Instructional Prevention (IP)} 
This technique involves explicitly instructing the model to be wary of  IPI attacks and ignore commands from external content~\cite{Instructiondefense,learnpromptingLearnPrompting}. We show the prompt in Appendix~\ref{appendix:ip_prompt}.

\minihead{Data Prompt Isolation (DPI)} This method introduces delimiters around the tool response to create clear boundaries between the tool's output and the rest of the context, reducing the chance of an IPI attack~\cite{SimonWillison,AlexandraMendes,learnpromptingLearnPrompting}. We use \texttt{\textquotesingle\textquotesingle\textquotesingle} to wrap the tool response.

\minihead{Sandwich Prevention (SP)} By attaching an additional user instruction following the tool response, we ensure that the LLM follows the legitimate user command~\cite{Sandwitchdefense,learnpromptingLearnPrompting}. We show the prompt in Appendix~\ref{appendix:sp_prompt}.

\minihead{Paraphrasing (P)} 
Another method is to paraphrase the external content $E_{T_u}$ to disrupt token-level optimized adversarial strings~\cite{DBLP:journals/corr/abs-2309-00614}, thereby reducing their effectiveness. We provide the paraphrasing prompt in Appendix~\ref{appendix:paraphrasing}


\subsection{Model-level Defense} 
\minihead{Adversarial Finetuning (AF)}
This defense involves modifying the model itself by finetuning $M$ to create a more robust version $M'$. The processing aims to improve the model’s resistance to IPI attacks by finetuning over adversarial examples~\cite{DBLP:conf/esorics/PietASCWSAW24,DBLP:journals/corr/abs-2312-14197}. 
To create the finetuning dataset, we first evaluate all the test cases and filter out successful attacks and invalid outputs, keeping only the unsuccessful attacks. 
The goal is to use inputs with IPI attacks and the corresponding resilient outputs to train the LLM to ignore malicious instructions embedded in the tool response.




\section{Adaptive Attack Techniques}
Since attackers can only manipulate external content $E_{T_u}$ in an IPI setting, the most direct method of an adaptive attack involves inserting an adversarial string $S$ into the external content before or after the attacker instruction. That is, $E_{T_u} = I_a \oplus S$ (adversarial suffix) or $S \oplus I_a$ (adversarial prefix), aiming to cause the model to execute the malicious command and invoke the attacker tool $T_a$.  In this section, we use the adversarial suffix as an example.  
In the adaptive attack setting, we assume the attacker has knowledge of and white-box access to the agent and defenses~\cite{DBLP:conf/icml/AthalyeC018}.

\minihead{Greedy Coordinate Gradient (GCG)~\cite{DBLP:journals/corr/abs-2307-15043}}  
We adapt the GCG algorithm to the IPI scenario, leveraging its effectiveness in crafting adversarial strings for LLMs.  
Originally introduced for jailbreak attacks, GCG aims to train adversarial strings to let the LLM generate affirmative prefixes that induce malicious content following the malicious instruction.  
To adapt it for IPI attacks, we modify the target response  based on the agent's behavior, ensuring it leads to the execution of the attacker tool.  
We show the detailed targets for different agents in Section~\ref{sec:experiment_settings}.

Formally, for each test case, we aim to find an adversarial string $S$ that maximizes the probability 
$$P_M(\text{target}|Prompt(I_u, I_a \oplus S)),$$ where $P_M(y|x)$ is the probability of the model $M$ generating the output $y$ given input $x$. 
The loss function is defined as: 
$$\mathcal{L}_\text{attack} = -\log P_M(\text{target}|Prompt(I_u, I_a \oplus S)).$$ 
We follow the GCG algorithm to optimize over the discrete token space, generating the optimized adversarial string $S$.


\minihead{Multi-objective GCG (M-GCG)}  
To bypass detection-based defenses, we design the adversarial string $S$ to satisfy an additional stealth objective—causing the detection model to misclassify the tool response $R_{T_u}$ as benign.  
For the fine-tuned detector, this means maximizing the probability $P(D(R_{T_u}) = 0)$.  
For the LLM-based detector, the goal is to maximize the likelihood of a ``No'' response.  
When the agent and detector models share the same tokenizer, we jointly optimize the loss:  
\[
\mathcal{L}_\text{joint} = \alpha\mathcal{L}_\text{attack} + (1- \alpha)\mathcal{L}_\text{detect}
\]  
using the GCG algorithm, where $\mathcal{L}_\text{detect}$ represents the log-likelihood of the target probability.  
If the models use different tokenizers, such as in the fine-tuned detector with DeBERTaV3, we apply iterative optimization, alternating between objectives and optimizing each for a single step at a time.  


\minihead{Two-stage GCG (T-GCG)}
This method specifically targets the paraphrasing defense, which is effective against token-level optimized adversarial strings. 
To overcome this defense, we adapt the two-step generation strategy for adversarial strings introduced by~\citet{DBLP:journals/corr/abs-2309-00614}.
In this approach, we first train an adversarial string $S_1$, which, when appended as a prefix to the attack instruction (i.e. $E_{T_u} = S_1 \oplus I_a$), prompts the model to output the desired target.
Next, we train a second adversarial string $S_2$ and set $E_{T_u}=S_1 \oplus I_a \oplus S_2$, so that it can let the paraphraser paraphrase $E_{T_u}$ into $S_1 \oplus I_a$, which will let the agent generate the target output.


\input{tex/figures/asr_defense_adaptive_attack}

\minihead{AutoDAN~\cite{zhu2023autodan}}
The adversarial string $S$ produced by the GCG algorithm is optimized at the token level, often resulting in gibberish strings. These strings tend to have high perplexity, making them easy targets for perplexity filtering. 
To address this, researchers introduced methods to generate semantically meaningful adversarial strings for the jailbreak setting, such as the genetic-algorithm-based AutoDAN~\cite{DBLP:conf/iclr/LiuXCX24} and the GCG-based AutoDAN~\cite{zhu2023autodan}. 
However, we found that the former struggles to adapt to the IPI setting. 
We speculate the difficulty arises because the IPI context is much longer than a simple malicious instruction in the jailbreak setting, requiring a much longer adversarial string for sufficient mutation to escape local optima. 
In contrast, the latter AutoDAN, which selects adversarial string tokens through a left-to-right process, performs well in the IPI setting.
We recommend referring to its original paper for further details.

