\section{Introduction}
The rapid advancement of Large Language Models (LLMs) agents has enabled their widespread deployment in various applications, including high-stakes domains such as finance~\cite{DBLP:conf/aaaiss/YuLCJLZLSK24}, healthcare~\cite{DBLP:journals/corr/abs-2401-05654}, autonomous driving~\cite{DBLP:journals/itsm/CuiMCYW24}, and chemical laboratories handling hazardous materials~\cite{DBLP:journals/natmi/BranCSBWS24}. 
These agents use LLMs for processing and external tools for executing actions.

While the external tools expand the capabilities of LLMs, they also introduce risks, such as the threat of indirect prompt injection (IPI) attacks~\cite{DBLP:conf/acl/ZhanLYK24,DBLP:conf/ccs/AbdelnabiGMEHF23}. 
In an IPI attack, adversaries embed malicious instructions into external data sources accessed by the agent, aiming to manipulate its behavior. 
Such attacks are especially dangerous due to their ease of execution and potential to cause significant harm, including unauthorized transactions, data leaks, or even physical damage.




Given the severity of these risks,  it is crucial to develop effective defense mechanisms against IPI attacks.
A robust defense must not only withstand current threats but also anticipate future adaptive attacks—those specifically designed after the defense is fully disclosed~\cite{DBLP:conf/icml/AthalyeC018, DBLP:conf/icml/MazeikaPYZ0MSLB24}.
In standard computer security and ML security, adaptive attacks serve as a standard approach to test the reliability of defenses~\cite{katz2007introduction,DBLP:conf/nips/TramerCBM20}.

Prior studies have shown that non-adaptive attacks can greatly underestimate a system's vulnerabilities, as defenses that seem robust under these attacks may be entirely compromised by adaptive ones, drastically reducing accuracy and exposing a false sense of security~\cite{DBLP:conf/icml/AthalyeC018}.
While defenses against IPI attacks have been proposed~\cite{DBLP:journals/corr/abs-2312-14197}, no studies have yet explored their effectiveness against adaptive attacks, leaving their robustness in question.

\input{tex/figures/system.tex}
\input{tex/tables/defense}
To address this gap, we conduct a comprehensive evaluation of existing IPI defenses by testing their resilience to adaptive attacks.
Our goal is to find adversarial attack methods to compromise these defenses. 
In the context of IPI attacks, the attacker can only manipulate the content of external sources, such as reviews or emails~\cite{DBLP:conf/acl/ZhanLYK24,DBLP:conf/ccs/AbdelnabiGMEHF23}. 
Therefore, adaptive attacks in this setting involve crafting adversarial examples in the external content to manipulate the LLM agent. 
This is similar to adversarial attacks in jailbreak attacks of LLMs, where the goal is to bypass models' safety alignment and trigger harmful outputs
~\cite{DBLP:journals/corr/abs-2307-15043, DBLP:conf/iclr/LiuXCX24, zhu2023autodan}.
Building on this, we leverage strong attack strategies from jailbreak settings to design adaptive attacks in the IPI context. 


We implement eight different defenses against IPI attacks on two types of LLM agents and design adaptive attacks to expose their vulnerabilities.  
Our results show that adaptive attacks consistently achieve success rates above 50\% across the targeted defenses and LLM agents—exceeding the ASR before deploying any defense and significantly outperforming non-adaptive attacks.  
These findings reveal weaknesses in current defense strategies and emphasize the need to test defenses against adaptive attacks to ensure robustness and reliability.






