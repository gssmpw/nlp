\section{Related work}
%
\label{sec:related_work}

For denoising (and related contractive) autoencoders, \citet{alain_what_2014} derive a formula that shows that, for each \textit{fixed data point}, \textit{asymptotically} as the noise approaches zero, the difference between the reconstructed data vector and the original will tend to the score of the data:
\footnote{Adapted from Theorem 1 in \citet{alain_what_2014}.}
\begin{equation}
    d^*(e_\sigma^*(x)) = x+\sigma^2 \frac{\partial \log p(x)}{\partial x}+o\left(\sigma^2\right) \quad \text{ as } \sigma \rightarrow 0, \forall x \text{ fixed}.
    \label{eq:bengio_matching}
\end{equation}

In \citet{bengio_implicit_2012}, the authors further derive a connection between the score and the first two local moments of the data distribution, that is, the moments restricted to a $\delta$-ball in the ambient space centered on a given data point. Using the relation between the reconstruction for a given data point and the score, they propose estimators for these moments, which hold (additionally to the noise approaching zero) asymptotically as the radius $\delta$ approaches zero.


This point forms a natural connection to our results in Section~\ref{sec:score}. In contrast, the first two momenta appear directly in the relation given by Eq.~\ref{eq:thm1}, and are the \textit{level-set} momenta. The relation gives a \textit{geometric constraint} that describes how the entire space of the encoding adapts to the score of the data distribution and holds \textit{a.s.} for all data points. Perhaps, it is interesting to note that the results presented here are also not connected to any sort of (whether denoising, contractive, or other) regularization, as they arise directly from the distributional regression obtained from using the energy score \citep{gneiting_strictly_2007}.


Level sets themselves have been investigated for, e.g. Variational Autoencoders, for which the optimal decoder outputs the expected value over the level set: $d^*(\boldsymbol{e(x)})=\mathbb{E}[\boldsymbol{x}: \boldsymbol{x} \in \mathcal{L}_e(x)]$, resulting in e.g. blurry images and spurring the development of improvements such as the $\beta$-VAE \citep{murphy_probabilistic_2023}.



% NOTE: leave out sampling for later
% To address the locality of the score approximation, \citep{bengio_generalized_2013} and \citep{alain_regularized_2012} propose sampling approaches that converge to samples from the data distribution. The former   \todo{refresh generalized}
% \todol{Sampling with diffusion - connect to De Bortoli - not as strong} 
% \todol{Reverse Markov learning: is it connected to the sampling scheme? mention \textit{subsequent to this work ...}}
% \citep{shen_reverse_2025}

An interesting parallel that illustrates the relative power of using the energy score over score matching was recently illustrated in \citet{bortoli_distributional_2025}. By replacing the usual diffusion loss (which amounts to score matching across different noise levels \citep{song_score-based_2021}) with the energy score (as used by the DPA), the authors demonstrate improved generation performance. This mirrors the contrast between the results derived in \citep{alain_what_2014} (naturally connected to "ordinary" diffusion models via \citep{vincent_connection_2011}) and the ones presented in this work.
The score matching property of diffusion models has recently been used in \citep{arts_two_2023} to obtain the force field from data following the Boltzmann distribution, which is paralleled in this work for the MÃ¼ller-Brown potential.





In terms of the informativeness of extraneous dimensions, \citet{liu_deep_2023} show that for a model architecture called the Chart Autoencoder \citep{schonsheck_chart_2020} that builds on the denoising autoencoder approach, both for exactly and approximately parameterizable manifolds, the reconstruction loss of an optimal model decreases exponentially with the manifold intrinsic dimension (and not the ambient dimension). They show that these results also hold for the classical autoencoder in the case where the manifold is globally parameterizable. The results require that the data is noised with components normal to the manifold, which requires some knowledge of the manifold geometry. In contrast, our results prove that for the DPA, the extra dimensions are conditionally independent in both manifold scenarios, and carry \textit{no} additional information, without requiring any knowledge about the data manifold. Given the results of Section~\ref{sec:score}, there might also exist a possible connection between the requirement of noising in the normal direction of the data manifold, and the DPA discovering the manifold structure by aligning the level sets with the score in the normal direction, which is left for future work.

For VAEs with a learnable decoder variance, \citet{zheng_learning_2022} show that, for simple Riemannian manifolds, the learnable variance of a globally optimal VAE decoder will tend to zero, inducing the VAE loss to tend to zero in proportion to a product of the extra dimensionality ($p-k$ in our notation) and the log of the learnable variance. In this sense, the manifold dimension can be learned. If one considers the conditional VAE, the authors show that with a careful design of the conditioning function, one can increase the above rate of convergence on subsets of the simple manifold. In our case, the results hold for any manifold that can be approximated (in the described way) by the model, and provide a concrete and precise (i.e., not limiting-behavior) characterization --- conditional independence --- of the extraneous dimensions.

Prior work on disentangled representation learning has investigated the phenomenon of extraneous dimensions (depending on the formulation) being "turned-off" \citep{higgins_beta-vae_2016, kim_disentangling_2019}; much like with the score alignment, the DPA again achieves this without any explicit regularization terms.