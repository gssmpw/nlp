@article{alain_regularized_2012,
	title = {Regularized {Auto}-{Encoders} {Estimate} {Local} {Statistics}},
	url = {https://www.semanticscholar.org/paper/Regularized-Auto-Encoders-Estimate-Local-Statistics-Alain-Bengio/57e17ce6e9a06aa8137ea355ba53073e3ffc7de6},
	abstract = {What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of the unknown data generating density. This paper clarifies some of these previous intuitive observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. More precisely, we show that the auto-encoder captures the score (derivative of the logdensity with respect to the input) or the local mean associated with the unknown data-generating density. This is the second result linking denoising auto-encoders and score matching, but in way that is different from previous work, and can be applied to the case when the auto-encoder reconstruction function does not necessarily correspond to the derivative of an energy function. The theorems provided here are completely generic and do not depend on the parametrization of the autoencoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood, i.e., one not involving a partition function. Finally, we make the connection to existing sampling algorithms for such autoencoders, based on an MCMC walking near the high-density manifold.},
	urldate = {2025-02-22},
	journal = {CoRR},
	author = {Alain, Guillaume and Bengio, Yoshua and Rifai, Salah},
	month = nov,
	year = {2012},
}

@article{alain_what_2014,
	title = {What Regularized Auto-Encoders Learn from the Data-Generating Distribution},
	volume = {15},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v15/alain14a.html},
	abstract = {What do auto-encoders learn about the underlying data-generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data- generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parameterization of the auto-encoder: they show what the auto- encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings {MCMC} can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments.},
	pages = {3743--3773},
	number = {110},
	journaltitle = {Journal of Machine Learning Research},
	author = {Alain, Guillaume and Bengio, Yoshua},
	date = {2014},
	langid = {english},
	keywords = {Computer Science - Machine Learning, {STAR}, Statistics - Machine Learning},
	file = {Alain and Bengio - What Regularized Auto-Encoders Learn from the Data.pdf:/Users/leban/myStuff/Zotero_Library/storage/C7F9SMT8/Alain and Bengio - What Regularized Auto-Encoders Learn from the Data.pdf:application/pdf;Preprint PDF:/Users/leban/myStuff/Zotero_Library/storage/53NLYFZ9/Alain and Bengio - 2014 - What Regularized Auto-Encoders Learn from the Data.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/T5LMFPW9/1211.html:text/html},
}

@misc{arts_two_2023,
	title = {Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics},
	url = {http://arxiv.org/abs/2302.00600},
	shorttitle = {Two for One},
	abstract = {Coarse-grained ({CG}) molecular dynamics enables the study of biological processes at temporal and spatial scales that would be intractable at an atomistic resolution. However, accurately learning a {CG} force field remains a challenge. In this work, we leverage connections between score-based generative models, force fields and molecular dynamics to learn a {CG} force field without requiring any force inputs during training. Specifically, we train a diffusion generative model on protein structures from molecular dynamics simulations, and we show that its score function approximates a force field that can directly be used to simulate {CG} molecular dynamics. While having a vastly simplified training setup compared to previous work, we demonstrate that our approach leads to improved performance across several protein simulations for systems up to 56 amino acids, reproducing the {CG} equilibrium distribution, and preserving dynamics of all-atom simulations such as protein folding events.},
	number = {{arXiv}:2302.00600},
	publisher = {{arXiv}},
	author = {Arts, Marloes and Satorras, Victor Garcia and Huang, Chin-Wei and Zuegner, Daniel and Federici, Marco and Clementi, Cecilia and Noé, Frank and Pinsler, Robert and Berg, Rianne van den},
	urldate = {2023-11-21},
	date = {2023-09-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2302.00600 [cs]},
	keywords = {\_tablet, Computer Science - Machine Learning, {STAR}},
	file = {Arts et al. - 2023 - Two for One Diffusion Models and Force Fields for.pdf:/Users/leban/myStuff/Zotero_Library/storage/6EP3MXQD/Arts et al. - 2023 - Two for One Diffusion Models and Force Fields for.pdf:application/pdf;arXiv.org Snapshot:/Users/leban/myStuff/Zotero_Library/storage/TPELMGIJ/2302.html:text/html},
}

@article{bengio_generalized_2013,
	title = {Generalized {Denoising} {Auto}-{Encoders} as {Generative} {Models}},
	url = {http://arxiv.org/abs/1305.6663},
	doi = {10.48550/arXiv.1305.6663},
	abstract = {Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying datagenerating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justiﬁcation which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-inﬁnitesimal corruption noise (or non-inﬁnitesimal contractive penalty).},
	language = {en},
	author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
	month = nov,
	year = {2013},
	note = {arXiv:1305.6663 [cs]},
	keywords = {\_tablet, Computer Science - Machine Learning, STAR},
	annote = {Contents1 Introduction2 Generalizing Denoising Auto-Encoders2.1 Definition and Training2.2 Sampling2.3 Consistency2.4 Locality of the Corruption and Energy Function3 Reducing the Spurious Modes with Walkback Training4 Experimental Validation5 Conclusion and Future Work},
	file = {Bengio et al. - Generalized Denoising Auto-Encoders as Generative .pdf:/Users/leban/myStuff/Zotero_Library/storage/MFHEUJ4Z/Bengio et al. - Generalized Denoising Auto-Encoders as Generative .pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/XCVYKTPE/1305.html:text/html},
}

@misc{bengio_implicit_2012,
	title = {Implicit Density Estimation by Local Moment Matching to Sample from Auto-Encoders},
	url = {http://arxiv.org/abs/1207.0057},
	doi = {10.48550/arXiv.1207.0057},
	abstract = {Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of the unknown data generating density. This paper contributes to the mathematical understanding of this phenomenon and helps define better justified sampling algorithms for deep learning based on auto-encoder variants. We consider an {MCMC} where each step samples from a Gaussian whose mean and covariance matrix depend on the previous state, defines through its asymptotic distribution a target density. First, we show that good choices (in the sense of consistency) for these mean and covariance functions are the local expected value and local covariance under that target density. Then we show that an auto-encoder with a contractive penalty captures estimators of these local moments in its reconstruction function and its Jacobian. A contribution of this work is thus a novel alternative to maximum-likelihood density estimation, which we call local moment matching. It also justifies a recently proposed sampling algorithm for the Contractive Auto-Encoder and extends it to the Denoising Auto-Encoder.},
	number = {{arXiv}:1207.0057},
	publisher = {{arXiv}},
	author = {Bengio, Yoshua and Alain, Guillaume and Rifai, Salah},
	urldate = {2025-02-22},
	date = {2012-06-30},
	eprinttype = {arxiv},
	eprint = {1207.0057 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, {STAR}},
	file = {Bengio et al. - 2012 - Implicit Density Estimation by Local Moment Matchi.pdf:/Users/leban/myStuff/Zotero_Library/storage/9CFTWC74/Bengio et al. - 2012 - Implicit Density Estimation by Local Moment Matchi.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/4P85DJLZ/1207.html:text/html},
}

@misc{bortoli_distributional_2025,
	title = {Distributional Diffusion Models with Scoring Rules},
	url = {http://arxiv.org/abs/2502.02483},
	doi = {10.48550/arXiv.2502.02483},
	abstract = {Diffusion models generate high-quality synthetic data. They operate by defining a continuous-time forward process which gradually adds Gaussian noise to data until fully corrupted. The corresponding reverse process progressively "denoises" a Gaussian sample into a sample from the data distribution. However, generating high-quality outputs requires many discretization steps to obtain a faithful approximation of the reverse process. This is expensive and has motivated the development of many acceleration methods. We propose to accomplish sample generation by learning the posterior \{{\textbackslash}em distribution\} of clean data samples given their noisy versions, instead of only the mean of this distribution. This allows us to sample from the probability transitions of the reverse process on a coarse time scale, significantly accelerating inference with minimal degradation of the quality of the output. This is accomplished by replacing the standard regression loss used to estimate conditional means with a scoring rule. We validate our method on image and robot trajectory generation, where we consistently outperform standard diffusion models at few discretization steps.},
	number = {{arXiv}:2502.02483},
	publisher = {{arXiv}},
	author = {Bortoli, Valentin De and Galashov, Alexandre and Guntupalli, J. Swaroop and Zhou, Guangyao and Murphy, Kevin and Gretton, Arthur and Doucet, Arnaud},
	urldate = {2025-02-06},
	date = {2025-02-04},
	eprinttype = {arxiv},
	eprint = {2502.02483 [cs]},
	keywords = {Computer Science - Machine Learning, {STAR}, Statistics - Machine Learning},
	file = {Bortoli et al. - 2025 - Distributional Diffusion Models with Scoring Rules.pdf:/Users/leban/myStuff/Zotero_Library/storage/AK3HWA26/Bortoli et al. - 2025 - Distributional Diffusion Models with Scoring Rules.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/7NJDUFIP/2502.html:text/html},
}

@article{gneiting_strictly_2007,
	title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
	volume = {102},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	pages = {359--378},
	number = {477},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	urldate = {2023-10-02},
	date = {2007-03},
	keywords = {\_tablet, {STAR}},
	file = {Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:/Users/leban/myStuff/Zotero_Library/storage/RAPSJYE4/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:application/pdf},
}

@inproceedings{higgins_beta-vae_2016,
	title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
	shorttitle = {beta-{VAE}},
	url = {https://www.semanticscholar.org/paper/beta-VAE%3A-Learning-Basic-Visual-Concepts-with-a-Higgins-Matthey/a90226c41b79f8b06007609f39f82757073641e2},
	abstract = {an},
	urldate = {2023-09-28},
	author = {Higgins, I. and Matthey, L. and Pal, Arka and Burgess, Christopher P. and Glorot, Xavier and Botvinick, M. and Mohamed, S. and Lerchner, Alexander},
	month = nov,
	year = {2016},
	file = {Higgins et al. - 2016 - beta-VAE Learning Basic Visual Concepts with a Co.pdf:/Users/leban/myStuff/Zotero_Library/storage/C8AT2GF9/Higgins et al. - 2016 - beta-VAE Learning Basic Visual Concepts with a Co.pdf:application/pdf},
}

@misc{kim_disentangling_2019,
	title = {Disentangling by {Factorising}},
	url = {http://arxiv.org/abs/1802.05983},
	doi = {10.48550/arXiv.1802.05983},
	abstract = {We deﬁne and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon β-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
	language = {en},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Kim, Hyunjik and Mnih, Andriy},
	month = jul,
	year = {2019},
	note = {arXiv:1802.05983 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {FactorVAE

},
	file = {Kim and Mnih - 2019 - Disentangling by Factorising.pdf:/Users/leban/myStuff/Zotero_Library/storage/L3BY5C4E/Kim and Mnih - 2019 - Disentangling by Factorising.pdf:application/pdf},
}

@misc{liu_deep_2023,
	title = {Deep {Nonparametric} {Estimation} of {Intrinsic} {Data} {Structures} by {Chart} {Autoencoders}: {Generalization} {Error} and {Robustness}},
	shorttitle = {Deep {Nonparametric} {Estimation} of {Intrinsic} {Data} {Structures} by {Chart} {Autoencoders}},
	url = {http://arxiv.org/abs/2303.09863},
	doi = {10.48550/arXiv.2303.09863},
	abstract = {Autoencoders have demonstrated remarkable success in learning low-dimensional latent features of high-dimensional data across various applications. Assuming that data are sampled near a low-dimensional manifold, we employ chart autoencoders, which encode data into low-dimensional latent features on a collection of charts, preserving the topology and geometry of the data manifold. Our paper establishes statistical guarantees on the generalization error of chart autoencoders, and we demonstrate their denoising capabilities by considering \$n\$ noisy training samples, along with their noise-free counterparts, on a \$d\$-dimensional manifold. By training autoencoders, we show that chart autoencoders can effectively denoise the input data with normal noise. We prove that, under proper network architectures, chart autoencoders achieve a squared generalization error in the order of \${\textbackslash}displaystyle n{\textasciicircum}\{-{\textbackslash}frac\{2\}\{d+2\}\}{\textbackslash}log{\textasciicircum}4 n\$, which depends on the intrinsic dimension of the manifold and only weakly depends on the ambient dimension and noise level. We further extend our theory on data with noise containing both normal and tangential components, where chart autoencoders still exhibit a denoising effect for the normal component. As a special case, our theory also applies to classical autoencoders, as long as the data manifold has a global parametrization. Our results provide a solid theoretical foundation for the effectiveness of autoencoders, which is further validated through several numerical experiments.},
	urldate = {2025-04-09},
	publisher = {arXiv},
	author = {Liu, Hao and Havrilla, Alex and Lai, Rongjie and Liao, Wenjing},
	month = oct,
	year = {2023},
	note = {arXiv:2303.09863 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Liu et al. - 2023 - Deep Nonparametric Estimation of Intrinsic Data St.pdf:/Users/leban/myStuff/Zotero_Library/storage/MUA9L88U/Liu et al. - 2023 - Deep Nonparametric Estimation of Intrinsic Data St.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/T2TA82SX/2303.html:text/html},
}

@book{murphy_probabilistic_2023,
	address = {Cambridge, Massachusetts London, England},
	series = {Adaptive computation and machine learning},
	title = {Probabilistic machine learning: advanced topics},
	isbn = {978-0-262-37600-6 978-0-262-04843-9},
	shorttitle = {Probabilistic machine learning},
	language = {en},
	publisher = {The MIT Press},
	author = {Murphy, Kevin P.},
	year = {2023},
	file = {Murphy - 2023 - Probabilistic machine learning advanced topics.pdf:/Users/leban/University of Michigan Dropbox/Andrej Leban/Literatura/ml/deep_learning/Murphy - 2023 - Probabilistic machine learning advanced topics.pdf:application/pdf},
}

@misc{schonsheck_chart_2020,
	title = {Chart {Auto}-{Encoders} for {Manifold} {Structured} {Data}},
	url = {http://arxiv.org/abs/1912.10094},
	doi = {10.48550/arXiv.1912.10094},
	abstract = {Deep generative models have made tremendous advances in image and signal representation learning and generation. These models employ the full Euclidean space or a bounded subset as the latent space, whose flat geometry, however, is often too simplistic to meaningfully reflect the manifold structure of the data. In this work, we advocate the use of a multi-chart latent space for better data representation. Inspired by differential geometry, we propose a {\textbackslash}textbf\{Chart Auto-Encoder (CAE)\} and prove a universal approximation theorem on its representation capability. We show that the training data size and the network size scale exponentially in approximation error with an exponent depending on the intrinsic dimension of the data manifold. CAE admits desirable manifold properties that auto-encoders with a flat latent space fail to obey, predominantly proximity of data. We conduct extensive experimentation with synthetic and real-life examples to demonstrate that CAE provides reconstruction with high fidelity, preserves proximity in the latent space, and generates new data remaining near the manifold. These experiments show that CAE is advantageous over existing auto-encoders and variants by preserving the topology of the data manifold as well as its geometry.},
	urldate = {2025-04-09},
	publisher = {arXiv},
	author = {Schonsheck, Stefan and Chen, Jie and Lai, Rongjie},
	month = aug,
	year = {2020},
	note = {arXiv:1912.10094 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 22 pages, 12 figures},
	file = {Schonsheck et al. - 2020 - Chart Auto-Encoders for Manifold Structured Data.pdf:/Users/leban/myStuff/Zotero_Library/storage/SDQAIVRH/Schonsheck et al. - 2020 - Chart Auto-Encoders for Manifold Structured Data.pdf:application/pdf},
}

@misc{shen_reverse_2025,
	title = {Reverse Markov Learning: Multi-Step Generative Models for Complex Distributions},
	url = {http://arxiv.org/abs/2502.13747},
	doi = {10.48550/arXiv.2502.13747},
	shorttitle = {Reverse Markov Learning},
	abstract = {Learning complex distributions is a fundamental challenge in contemporary applications. Generative models, such as diffusion models, have demonstrated remarkable success in overcoming many limitations of traditional statistical methods. Shen and Meinshausen (2024) introduced engression, a generative approach based on scoring rules that maps noise (and covariates, if available) directly to data. While effective, engression struggles with highly complex distributions, such as those encountered in image data. In this work, we extend engression to improve its capability in learning complex distributions. We propose a framework that defines a general forward process transitioning from the target distribution to a known distribution (e.g., Gaussian) and then learns a reverse Markov process using multiple engression models. This reverse process reconstructs the target distribution step by step. Our approach supports general forward processes, allows for dimension reduction, and naturally discretizes the generative process. As a special case, when using a diffusion-based forward process, our framework offers a method to discretize the training and inference of diffusion models efficiently. Empirical evaluations on simulated and climate data validate our theoretical insights, demonstrating the effectiveness of our approach in capturing complex distributions.},
	number = {{arXiv}:2502.13747},
	publisher = {{arXiv}},
	author = {Shen, Xinwei and Meinshausen, Nicolai and Zhang, Tong},
	urldate = {2025-02-23},
	date = {2025-02-19},
	eprinttype = {arxiv},
	eprint = {2502.13747 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, {STAR}},
	file = {Shen et al. - 2025 - Reverse Markov Learning Multi-Step Generative Mod.pdf:/Users/leban/myStuff/Zotero_Library/storage/WQV8P8TB/Shen et al. - 2025 - Reverse Markov Learning Multi-Step Generative Mod.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/N562TRIA/2502.html:text/html},
}

@misc{song_score-based_2021,
	title = {Score-Based Generative Modeling through Stochastic Differential Equations},
	url = {http://arxiv.org/abs/2011.13456},
	doi = {10.48550/arXiv.2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation ({SDE}) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time {SDE} that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time {SDE} depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical {SDE} solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time {SDE}. We also derive an equivalent neural {ODE} that samples from the same distribution as the {SDE}, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on {CIFAR}-10 with an Inception score of 9.89 and {FID} of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
	number = {{arXiv}:2011.13456},
	publisher = {{arXiv}},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	urldate = {2023-09-20},
	date = {2021-02-10},
	eprinttype = {arxiv},
	eprint = {2011.13456 [cs, stat]},
	keywords = {\_tablet, Computer Science - Machine Learning, {STAR}, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/leban/myStuff/Zotero_Library/storage/IXCLAMGC/2011.html:text/html;Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf:/Users/leban/myStuff/Zotero_Library/storage/Z6Z2NAI4/Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf:application/pdf},
}

@article{vincent_connection_2011,
	title = {A {Connection} {Between} {Score} {Matching} and {Denoising} {Autoencoders}},
	volume = {23},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/NECO_a_00142},
	doi = {10.1162/NECO_a_00142},
	abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.},
	number = {7},
	urldate = {2024-02-02},
	journal = {Neural Computation},
	author = {Vincent, Pascal},
	month = jul,
	year = {2011},
	keywords = {STAR, \_tablet},
	pages = {1661--1674},
	annote = {(hidden zotfile data)},
	annote = {(hidden zotfile data)},
	
}

@inproceedings{zheng_learning_2022,
	title = {Learning {Manifold} {Dimensions} with {Conditional} {Variational} {Autoencoders}},
	url = {https://openreview.net/forum?id=Lvlxq_H96lI#:~:text=as%20is%20likely%20the%20case,world%20datasets},
	abstract = {Although the variational autoencoder (VAE) and its conditional extension (CVAE) are capable of state-of-the-art results across multiple domains, their precise behavior is still not fully understood, particularly in the context of data (like images) that lie on or near a low-dimensional manifold. For example, while prior work has suggested that the globally optimal VAE solution can learn the correct manifold dimension, a necessary (but not sufficient) condition for producing samples from the true data distribution, this has never been rigorously proven. Moreover, it remains unclear how such considerations would change when various types of conditioning variables are introduced, or when the data support is extended to a union of manifolds (e.g., as is likely the case for MNIST digits and related). In this work, we address these points by first proving that VAE global minima are indeed capable of recovering the correct manifold dimension. We then extend this result to more general CVAEs, demonstrating practical scenarios whereby the conditioning variables allow the model to adaptively learn manifolds of varying dimension across samples. Our analyses, which have practical implications for various CVAE design choices, are also supported by numerical results on both synthetic and real-world datasets.},
	language = {en},
	urldate = {2025-04-09},
	author = {Zheng, Yijia and He, Tong and Qiu, Yixuan and Wipf, David},
	month = oct,
	year = {2022},
	keywords = {\_tablet, STAR},
	file = {Zheng et al. - 2022 - Learning Manifold Dimensions with Conditional Vari.pdf:/Users/leban/myStuff/Zotero_Library/storage/MXVS74P2/Zheng et al. - 2022 - Learning Manifold Dimensions with Conditional Vari.pdf:application/pdf},
}

