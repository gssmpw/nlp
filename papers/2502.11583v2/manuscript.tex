\documentclass[a4paper]{article}
\usepackage{template, times}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023

\PassOptionsToPackage{square, semicolon, sort&compress}{natbib}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}
\setcitestyle{square,semicolon,sort&compress}

% ready for submission
% \usepackage{neurips_2023}
% NOTE: to check if author formatting etc OK, disable before submission
% to compile a preprint version, e.g., for submission to arXiv, add the
% [preprint] option:


% TODO: At submission time, please omit the final and preprint options. This will anonymize your

% submission and add line numbers to aid review. Please do not refer to these line numbers in your
% paper as they will be removed during generation of camera-ready copies.
% \usepackage[]{neurips_2023}

% to compile a camera-ready version, add the [final] option, e.g.:
    % \usepackage[final]{neurips_2023}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}



\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage[colorlinks=true,
% linkcolor=blue,
% citecolor=blue
]{hyperref}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%% EXTRA PACKAGES
% \usepackage{amsmath}
% \usepackage{amsfonts}
% \usepackage{amssymb}
% % \usepackage{amsthm}

\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{booktabs}    

\usepackage{caption}
\usepackage{subcaption}

% \usepackage{layouts}

%% ORCID integration
\usepackage{academicons}
% \definecolor{orcidlogocol}{HTML}{A6CE39}
% \newcommand{\orcid}[1]{\href{https://orcid.org/#1}{\textcolor[HTML]{A6CE39}{\aiOrcid}}}
\usepackage{orcidlink}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%NOTE: for todos not to complain, comment out for final
\setlength{\marginparwidth}{2cm}
% \usepackage[textsize=small]{todonotes}
%NOTE: disable todos by uncommenting below 
\usepackage[disable,textsize=small]{todonotes}


%NOTE: User-defined commands, merge if needed:
% \input{macros.tex}
% \input{math_commands.tex}
\usepackage{macros}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
% \newcommand*\cetithanks[2][\value{footnote}]{\footnotemark[#2]}
\bibliographystyle{abbrvnat}
% \bibliographystyle{unsrtnat}

% \setcitestyle{authoryear,open={(},close={)}}
\setcitestyle{numbers}
% open={(},close={)}
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{Distributional Autoencoders Know the Score}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Andrej Leban$~$ \orcidlink{0000-0003-0617-6843}\\
  Department of Statistics,\\
  University of Michigan,\\
  Ann Arbor, MI, United States\\
  \texttt{leban@umich.edu} \\
}
% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\date{}

%NOTE: Decrease the margins from NeurIPS - update if needed
\newgeometry{
    textheight=9in,
    textwidth=6.5in,  % 8.5 inches - 2 inches (1 inch on each side)
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}


\maketitle


\begin{abstract}
    This work presents novel and desirable properties of a recently introduced class of autoencoders --- the Distributional Principal Autoencoder (DPA) ---  which combines distributionally correct reconstruction with principal components-like interpretability of the encodings. 
    
    First, we show formally that the level sets of the encoder orient themselves exactly with regard to the score of the data distribution. This both explains the method's often remarkable performance in disentangling the factors of variation of the data, as well as opens up possibilities of recovering its distribution while having access to samples only. In settings where the score itself has physical meaning --- such as when the data obeys the Boltzmann distribution --- we demonstrate that the method can recover scientifically important quantities such as the \textit{minimum free energy path}.
    
    Second, we prove that if the data lies on a manifold that can be approximated by the encoder, the optimal encoder's components beyond the dimension of the manifold will carry absolutely no additional information about the data distribution. This promises potentially new ways of determining the number of relevant dimensions of the data.

    The results thus demonstrate that the DPA elegantly combines two often disparate goals of unsupervised learning: the learning of the data distribution and the learning of the intrinsic data dimensionality.
\end{abstract}

\section{Introduction}



The Distributional Principal Autoencoder (DPA) is an autoencoder variant recently introduced in  \citet{shen_distributional_2024}, connecting to other recent approaches \citep{shen_engression_2024, bortoli_distributional_2025} that utilize scoring rules such as the \textit{energy score} \citep{gneiting_strictly_2007}. 

Due to the distributional matching obtained by the use of the energy score, it guarantees correct distributional reconstruction of all the data mapped to a single value by the encoder, which, in turn, aims to minimize the unexplained variance given this encoding. Furthermore, various dimensions of the encoding can be optimized simultaneously, leading to principal-components-like interpretability while retaining an expressive, nonlinear encoding.

In this work, we further prove that not only is the reconstruction of the data --- conditional on the encoding value --- distributionally correct, but that the level set induced by this encoding value is itself oriented exactly according to the \textit{score}, that is, the gradient of the log-density of the data. These results are presented in Section~\ref{sec:score}, and significantly improve on prior work, discussed in Section~\ref{sec:related_work}.

Furthermore, we prove in Section~\ref{sec:independence} that if the data lie on a lower-dimensional manifold that can (in a sense made more precise therein) be described by the encoder, the dimensions of the encoding beyond the manifold one will carry no further information about the data. In other words, we show their conditional independence from the data given the first $K$ dimensions of the encoding, where $K$ is the manifold's dimension. This is, again an improvement in many regards on prior work, as is discussed in Section~\ref{sec:related_work}.

The theoretical results are followed up on in Section~\ref{sec:experiments} by examples that both help build intuition about the findings, as well as demonstrate a novel application of the method to a molecular dynamics. As a consequence of the results presented in Section~\ref{sec:score}, we demonstrate that the method approximates the \textit{minimum free energy path} between different stable states of the Müller-Brown potential, a common benchmark in molecular simulations. 







\section{Optimal encoder's level sets orient themselves according to the score}%%
\label{sec:score}

\subsection{Background and preliminaries}

We will denote the data as $X \in \R^p \sim P_{data} (x)$, supported on $\X$. The encoder will be denoted as $e(\cdot): \mathbb{R}^p \rightarrow \mathbb{R}^k$, and the corresponding decoder as $d(\cdot): \mathbb{R}^k \rightarrow \mathbb{R}^p$, with $p$ and $k$ being the dimensionalities of the data and latent space, respectively (and typically assuming $k \leq p$). Optimal encoder/decoder(s) will typically be denoted by ${}^*$. The following definitions from \citet{shen_distributional_2024} will be used throughout the work.

% \todol{More on DPA, engression, scoring rule, etc? See De Bortoli for how much time they spend introducing}

\begin{definition}[Oracle reconstructed distribution -- ORD, Definition 1 in \citep{shen_distributional_2024} ]

    For a given encoder $e(\cdot): \mathbb{R}^p \rightarrow \mathbb{R}^k$ and a given sample $x \in \mathbb{R}^p \sim P_{data}$, the oracle reconstructed distribution, denoted by $P_{e, x}^*$, is defined as the conditional distribution of $X$, given that its embedding $e(X)$ matches the embedding $e(x)$ of $x$:
    \begin{equation}
    (X \mid e(X)=e(x)) \sim P_{e, x}^*
    \end{equation}%%
\label{def:ORD}
\end{definition}

In other words, the ORD is the distribution of the data on the \textit{level set} of the encoder.

\begin{definition}[DPA encoder optimization objective, Eq. 4 in \citep{shen_distributional_2024}]

The optimal DPA encoder seeks to minimize the expected variability in the induced oracle reconstructed distribution:
\begin{equation}
e^* \in \underset{e \in \gC^1}{\operatorname{argmin}} ~
\mathbb{E}_{X \sim P_{data}} \left[ \mathbb{E}_{Y, Y^{\prime} \iid P_{e, X}^* \subseteq \X} \left[ \left\| Y-Y^{\prime} \right\|^\beta \right] \right],%
\label{eq:encoder_opt_objective}
\end{equation}
with $\beta$ a hyperparameter, and the norm taken to be the Euclidean norm in $\R^p$.%
\label{def:opt_objective}
\end{definition}


\subsection{Results}

 We generally assume $e$ to be at least differentiable (i.e., $e \in \mathcal{C}^1$) and Lipschitz, since it's parameterized by a neural network throughout this work. With this in mind, we present the main result of this section:

% \vspace{\baselineskip}
\begin{theorem}[Optimal encoder's level sets orient themselves according to the score]
    Set $\beta=2$ and denote the \textbf{level set} of an optimal encoder as:
    \begin{equation}
        L_{e^*(X)} \Def \{y: e^*(y) = e^*(X)\},%
\label{eq:level_set}
    \end{equation}
    for $X \sim P_{data}$.
    
    Denote the optimal encoder's Jacobian matrix evaluated at $y$ as $D_e^* (y)$, and assume it has full row rank $k$ \textit{a.e.} on $L_{e^*(X)}.$
    
    Then, excepting the points with rank deficiency, the following relation holds for $y$ on the level sets of an optimal encoder $L_{e^*(X)}$; that is, \textit{a.s.} in $y$ and $X$:
    
    \begin{equation}
    \frac{2 \, (y - c(X))}{\frac{V(X)}{Z(X)} - \|y-c(X)\|^2} ~ D_{e^*}^{\top}(y) = \frac{\nabla_y P_{\text {data }}(y)}{P_{\text {data }}(y)} ~ D_{e^*}^{\top}(y) = \nabla_y \log P_{data}(y) ~ D_{e^*}^{\top}(y) = s_{data} (y) ~ D_{e^*}^{\top}(y),%
\label{eq:thm1}
    \end{equation}

    where $s_{data}(y)$ denotes the (Stein) score, assuming the following quantities are well-defined:
    
    the \textbf{level-set mass}:
    \begin{equation}
        Z(X) = \int P_{data}(y) ~\delta(e(y) - e(X))~dy,%
\label{eq:Z(X)}
    \end{equation}
    the level-set \textbf{center-of-mass}:
    \begin{equation}
        c(X) = \frac{1}{Z(X)} \int y \, P_{data}(y)\, \delta(e(y) - e(X)) ~dy,%
\label{eq:c(X)}
    \end{equation}
    and the level-set \textbf{variance}:
    \begin{equation}
        V(X) = \int \|y-c(X)\|^2 \, P_{data}(y)~ \delta(e(y) - e(X)) ~dy.%
\label{eq:V(X)}
    \end{equation}%
\label{thm:grad_level_set}    
\end{theorem}

The proof is based on a calculus of variations approach and is presented in Appendix~\ref{sec:proof_score}.

We observe that the level sets attempt to, first, balance a \textit{global factor} $\frac{V(X)}{Z(X)}$ against a \textit{local} one $\|y - c(X)\|^2$, and, second, both against how the data is distributed locally, i.e., $\nabla_y \log P_{data}(y)$. Due to the variance minimization objective (Eq.~\ref{eq:encoder_opt_objective}), the level sets will be ``pulled'' close to $c(X)$, however, this must be considered against the data distribution.

The projection to the normal space $D_e^\top$ is understandable, since the derived relation is a type of a balance condition; as the encoding only changes in the normal directions, the optimal encoding is such that balances the above factors. Furthermore, the dimensionality of the normal space is $k$; thus, we have a natural trade-off between dimensionality reduction (from $p$ to $k$) and the strictness of the need to align with the score of the data (which will only be enforced in $k$ of the $p$ dimensions).

Similarly to the results presented for the encoder in \citet{shen_distributional_2024}, $\beta = 2$ is a legitimate hyperparameter choice when talking about an optimal \textit{encoder}. 
An issue arises when talking about the \textit{uniqueness} of the optimal \textit{decoder}; the latter is trained via the energy score loss \citep{gneiting_strictly_2007}, and \textit{could} be outputting another conditional distribution that is not the ORD, yet achieves \textit{exactly} the same loss. As far as it can be judged from the experiments presented --- Section~\ref{sec:experiments} --- this does not seem to be a pressing issue for non-pathological data.


The requirement that the Jacobian has full row rank almost everywhere on the level set can be violated in practice in a couple of ways. For instance, if the encoder is suffering from ``mode collapse'', that is, mapping large regions of the input to exactly the same constant, then its Jacobian will be exactly zero there. A similar scenario is if, for a sufficiently deep network, all the activations are constant on a considerable region. This means, e.g., for ReLUs all the neurons are set to zero; for sigmoids, all the neurons are ``saturated'' at 0 or 1. Another cause might be that the neural network is not sufficiently expressive to approximate the manifold, which is assumed \textit{not} to be the case throughout this work (e.g., in Section~\ref{sec:independence}).

We also note the following consequence of Theorem~\ref{thm:grad_level_set} when dealing with the extrema of the data distribution:
\begin{corollary}[Consequence of Theorem~\ref{thm:grad_level_set} for extrema of the data distribution]
    Assume locally full row-rank of the Jacobian $D_e^*$ at the extrema of the data distribution.
    
    Not considering possible minima at locations approaching infinity from the level-set center-of-mass, the optimal encoder will orient the level sets that cross the extrema so that the latter \textit{either} coincide with the level-set center-of-mass $c(X)$, \textit{or} that the vector from the center-of-mass to the extremum $y - c(X)$  lies purely in the tangent space of the level set at the extremal point.%%
\label{corr:extrema}
\end{corollary}

The proof is located in Appendix~\ref{sec:proof_extrema}.

% \todol{compare to Bengio's claim on p3753. Basically what we get for the mixture}

As noted, due to the variance minimization objective, it is typically suboptimal for the same level set to cross multiple maxima of the data distribution. If it does occur, it will often mean that the level set between them is approximately a line segment, as $c(X)$ will lie close to the connecting line (due to the density weighting) and $y-c(X)$ must be tangent in both.








\subsection{Examples}

\subsubsection{General exponential family distributions}

The exponential family is given by:
$$
p_\theta(x) = \exp \left[\eta(\theta) \cdot T(x) - A(\eta(\theta))\right] h(x), 
$$
where $T(x): \R^p \rightarrow \R^d$ is the sufficient statistic, $\eta \in \R^d$ the natural parameter, $h(x)$ the base measure, and $A$ the log-partition function.

We have 
$$ \nabla_x \log p_\theta(x) = \sum_{i=1}^d \eta(\theta)_i \nabla_x T(x) + \nabla_x \log h(x).$$

Thus, for $y$ on the level set of $e(X)$:
\begin{equation}
\frac{2 \, (y - c(X))}{\frac{V(X)}{Z(X)} - \|y - c(X)\|^2} ~  D_{e^*}^{\top}(y) = \left[ \sum_{i=1}^d \eta(\theta)_i \nabla_y T(y) + \nabla_y \log h(y) \right] ~ D_{e^*}^{\top}(y).%
\label{eq:exp_families}
\end{equation}



\subsubsection{The Boltzmann distribution}%
\label{sec:boltzmann}

A member of the exponential family of significant interest for scientific applications is the Boltzmann distribution:
$$P_{data}(x; T) = \frac{1}{Z}e^{-U(x) / k_B T},$$
where $T$ is the temperature and $k_B$ the Boltzmann constant.

In the context of exponential families, the potential $U(x)$ is the sufficient statistic, $\frac{-1}{k_B T}$ the natural parameter, and $\log{Z}$ the log-partition function with $h(y)=1$.

We have
$$\frac{\nabla_x P_{data}(x)}{P_{data}(x)} = - \frac{1}{k_B T}\nabla_x U(x).$$

After re-arranging the original equation~\ref{eq:thm1}, we obtain for each level set defined by $X$:
\begin{equation}
    \vec{F}(y)~ D_{e^*}^\top = - \nabla_y \, U(y) ~D_{e^*}^\top (y) = 2~ k_B T ~ \frac{y - c(X)} {\frac{V(X)}{Z(X)} - \|y-c(X)\|^2}~ D_{e^*}^\top (y),%
\label{eq:boltzmann_result}
\end{equation}
where $\vec{F}(y)$ is the force (field) at position $y$. This implies that the level set geometry is determined by the normal components of the force field to the level set and could be recoverable from the latter. The alignment between the level sets and the potential field $U(y)$ is illustrated by an example in Section~\ref{sec:MB}.




% \newpage



\section{For (approximately) parameterizable manifolds, the extraneous dimensions are uninformative}%
\label{sec:independence}

In this section, we wish to consider the relationships between the data manifold's dimension (i.e., the intrinsic dimension), denoted as $K$, and the dimensions of the encoding greater than $K$. 

\subsection{Background and preliminaries}

The optimization objective for the encoder and decoder is \citep{shen_distributional_2024}:

\begin{equation}
    \left(e^*, d^*\right) \in \underset{e, d}{\operatorname{argmin}}\left\{\mathbb{E}_X \mathbb{E}_{Y \sim P_{d, e(X)}}\left[\|X-Y\|^\beta\right] - \frac{1}{2} \mathbb{E}_X \mathbb{E}_{Y, Y \iid P_{d, e(X)}} \left[\left\|Y-Y^{\prime}\right\|^\beta\right]\right\}
\end{equation}

Note that the decoder is taken to be a stochastic network that takes noise $\varepsilon \sim  \gN(0, I_{(p - k)})$ (by default) as input, and, by Theorem 1 in \citet{shen_distributional_2024}:
$$d^*\left(e^*(x), \varepsilon\right) \sim P_{e^*, x}^*.$$

Given this, one can consider multiple possible encoder output dimensions $k$, with the remaining input dimensions of $d$ being padded by adding dimensions to $\varepsilon$:
$d\left([e_{1: k}(x), \varepsilon_{(k+1): p}]\right) \sim P_{d, e_{1: k}}(X)$.
Then, the joint optimization objective across all components becomes (Eq. 12 in \citep{shen_distributional_2024}):
\begin{equation}
    (e^*, d^*) \in \underset{e, d}{\operatorname{argmin}} \sum_{k=0}^{p} \omega_k\left[\mathbb{E}_X \mathbb{E}_{Y \sim P_{d, e_{1: k}(X)}}\left[\|X-Y\|^\beta\right]-\frac{1}{2} \mathbb{E}_X \mathbb{E}_{Y, Y \iid P_{d, e_{1: k}(X)}}\left[\left\|Y-Y^{\prime}\right\|^\beta\right]\right],%
\label{eq::opt_joint_obj}
\end{equation}

where $\omega_k \in [0, 1]$ are (optional) weights such that $\sum_{k=0}^p \omega_k = 1$.

In this section, we will generally be assuming uniform weights: $\frac{1}{p+1}$, and $\beta \in (0, 2)$, which makes the objective a \textit{strictly proper scoring rule} \citep{gneiting_strictly_2007}, and thus the global optimum unique.

We denote the $k$-th term of the optimization objective~\ref{eq::opt_joint_obj} as:
\begin{equation}
    L_k[e, d] = \mathbb{E}_X \mathbb{E}_{Y \sim P_{d, e_{1: k}(X)}}\left[\|X-Y\|^\beta\right]-\frac{1}{2} \mathbb{E}_X \mathbb{E}_{Y, Y^{\prime} \iid P_{d, e_{1:k}(X)}}\left[\left\|Y-Y^{\prime}\right\|^\beta\right].%
\label{eq:L_k}
\end{equation}
 
% \vspace{\baselineskip}
\subsection{Results}

We will assume the data $X \sim P_{data}$ to lie on a $K$-dimensional manifold, with $K<p$.
We first focus on a case where the manifold cannot be exactly parameterized via an encoder, but can be approximated in the \textit{energy-score} sense. One such case might be when the data lies on an union of manifolds, for example.

\begin{definition}[$K'$-parameterizable manifold]
    A $K$-dimensional manifold is \textit{$K'$-parameterizable} if it can be approximated in the minimum energy score sense in $K'$-dimensions, that is, for an optimal encoder/decoder pair, the $K'$-term in the loss~\ref{eq::opt_joint_obj} is globally the smallest among all terms \textit{and} among all encoder/decoder pairs:
        \begin{equation}    
        L_{K'}[(e^*, d^*)] = \min_{e, d, k} L_k[e, d]%
\label{eq:global_min_term}
        \end{equation}%
\label{def:K_prime}
\end{definition}

Naturally, we have $K' \geq K$. As $K$ dimensions cannot be fully ``captured'' (in the exact or approximate sense) by a lower-dimensional mapping, the reconstruction term in the loss could always be reduced by considering $K$ dimensions, leading to a contradiction since, at optimum, the second term in Eq.~\ref{eq:L_k} equals the reconstruction term (without the $\frac{1}{2}$ factor, cf. Prop. 1 in \citep{shen_distributional_2024}), hence that is not minimal, either, implying that $K' \geq K$.


Next, we show that this definition is compatible with the usual notion of exact parameterizability:

\begin{proposition}[An exactly parameterizable manifold is $K'$-parameterizable]
    Consider the case where the data is located on a $K$-dimensional manifold $\subset \R^p$, which is exactly parameterizable by some function $\R^p \rightarrow \R^K$, and our encoder function class is expressive enough to realize such a parameterization in its first $K$ components.

    If an encoder realizing the manifold parameterization $e^*$ is optimal (together with a suitable optimal decoder), then the manifold is $K'$-parameterizable with $K' = K$, and $L_{K'}[(e^*, d^*)] = 0$.%
\label{prop:parameterizable_is_K_prime}
\end{proposition}

The proof can be found in Appendix~\ref{sec:proof_K_prime}.

With this, we can state our main result regarding the ``extraneous'' dimensions:

\begin{theorem}[Extraneous dimensions of an optimal encoder of a $K'$-parameterizable manifolds are uninformative] 
    Suppose the data is supported on a $K$-dimensional manifold, which is $K'$-parameterizable.
    
    If a solution $(e^*, d^*)$ satisfying Eq.~\ref{eq:global_min_term} is also optimal among all dimension-$K'$ encoders:
    $$
        (e^*, d^*) \in \underset{e, d}{\operatorname{argmin}} \sum_{k=0}^{K'} L_k [e,d],
    $$
    we denote it as the \textbf{$K'$-best-approximating encoder} (with an an accompanying optimal decoder).%
    \footnote{We assume WLOG $e^*: \R^p \rightarrow \R^p$, but the results hold for any output dimension $ \geq K'$.}
    
    Then this solution is also \textit{the} optimal solution when optimizing across all $p$, with the dimensions $(K' + 1, \cdots, p)$ obeying:
    \begin{equation}
        P_{d^*, e^*_{1: k}(X)} = P_{d^*, e^*_{1: K'}(X)}, ~ \text{ for }  k > K'.%
\label{eq:extra_dim_dist}
    \end{equation}

    Furthermore, the dimensions $(K' + 1, \ldots, p)$ of the encoder will be conditionally independent of $X$, given the relevant components $(e_1, \cdots, e_{K'})$:
    \begin{equation}
    X \ind e_{K' + i}(X) \mid e_{1: K'}(X), \qquad \forall i \in [1, \ldots, p - K'].%
\label{eq:cond_indep}
    \end{equation}
    
    In other words, they will carry no additional information about the data distribution:
    $$
    I\left(X ; e_{K' + i}(X) \mid e_{1: K'}(X)\right) = 0, \qquad \forall i \in [1, \ldots, p-K'],
    $$
    where $I(\cdot; \cdot)$ is the mutual information.%
\label{thm:independence_relaxed}
\end{theorem}
The proof is located in Appendix~\ref{sec:proof_ind_relaxed}.


The $K'$-best-approximating encoder will simply replicate the $K'$-dimensional manifold distributional approximation in the extraneous dimensions. Thus, the ``extra'' dimensions might either be deterministic functions of the preceding dimensions (or of the data), our even stochastic but conditionally uncorrelated with $X$. This result naturally extends the property of PCA (or linear autoencoders), which find the principal linear subspace of the data, if the latter truly lies in a linear subspace.


\begin{proposition}[Optimal encoders that exactly parameterize the manifold output the data distribution]
    We consider the case of an exactly parameterizable manifold with the parameterization $e_{1:K}$.
    
    If this encoder is optimal among $K$-dimensional encoders, then it is a $K$-best-approximating encoder and satisfies the requirements of Theorem~\ref{thm:independence_relaxed} with $K' = K$.
    
    Furthermore, together with an accompanying optimal decoder, it will output exactly the data distribution using the first $K$ dimensions:%
    \footnote{We would like to note that we are not using Prop. 5 from \citep{shen_distributional_2024}, 
    hence we do not need to satisfy $e(X) \stackrel{d}{=} \varepsilon$.}
    \begin{equation}
    d^*\left(e_{1: K}(X), \varepsilon_{K+1:p}\right) \stackrel{\text { a.s. }}{=} X \sim P_{data}.%
\label{eq:reconstr}
    \end{equation}%
\label{prop:exact_data_dist}
\end{proposition}

The proof can be found in Appendix~\ref{sec:proof_K_prime}. The capacity of an autoencoder architecture to parameterize sufficiently ``nice'' manifolds was recently proven in \citep{braunsmann_convergent_2024}; while the same remains as a future work for the DPA, it is certainly plausible.


Finally, we consider the case where we cannot precisely guarantee the global optimality of the parameterizing encoder:
\begin{remark}
    Consider the case of an exactly parameterizable $K$-dimensional manifold with parameterization $e_{1:K}$.
    
    Then, \textit{typically} when $p \gg K$, such an encoder will be \textit{an} optimal encoder (for $K$ \textit{and} $p$ dimensions), with the results~\ref{thm:independence_relaxed} and~\ref{prop:exact_data_dist} holding.%
\label{remark:independence_exact}
\end{remark}

A more precise statement is given in Appendix~\ref{sec:proof_ind_exact}.

Note that unlike Theorem~\ref{thm:independence_relaxed} we do not assume that the partial loss $\sum_{k=0}^K L_l [e,d]$ is the global minimum, but instead remark that in a ``typical'' case --- i.e., for ``nice'' enough data manifolds --- the parameterizing encoder is typically the best $K$-dimensional encoder. 




% \newpage
\section{Experiments}%%
\label{sec:experiments}

Below, we present results for scenarios in which the data density $P_{data}$ is known. In order of increasing complexity, we consider a standard multivariate Normal distribution, a Gaussian mixture, and the Müller-Brown potential, which is a standard benchmark in computational chemistry.

\subsection{Multivariate normal}

The first example shows the results when DPA is trained on a sample of $10000$ points from a standard Normal distribution, shown in Fig.~\ref{fig:2d_gauss_data}. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.3\linewidth]{figs/2d_gauss_data.png}
    \caption{The training data for the multivariate Normal.}%
\label{fig:2d_gauss_data}
\end{figure}
% \todo{increase ticksize}

In Fig.~\ref{fig:2d_gauss}, we present the two components (since $\max K=p=2$) of the encoder's output. The figure presents a heatmap of the encoder's latent values, together with some of the level sets as contour lines.  The data density is displayed in red contours, and the negative gradient with black arrows.\footnote{The level set plots were produced by a modified version of the code available in the \texttt{mlcolvar} package \citep{bonati_unified_2023}.}

The reason for the negative comes from the $(-1)$ factor when either the gradient or $y-c(X)$ is projected into the normal space of the encoder (determined by $\nabla_y \,e(y)$ here), depending on whether we consider the left- or the right-hand normal to the manifold as ``the'' normal here.
Note that for the level sets obtained, $c(X)$ should roughly coincide with the mean of the data distribution due to their geometry and the exponential decay of the density. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.75\linewidth]{figs/2D_gauss_grad700.png}
    \caption{The encoding and level sets for the multivariate standard normal. \textcolor{red}{Red:} the data density contours, with the (negative) gradient shown with black arrows. \textit{Left}: the first component of the encoder's output, \textit{right}, the second.}%
\label{fig:2d_gauss}
\end{figure}

First, we observe that the method roughly recovers the polar coordinate system, which is due to the perfect isotropy of the standard Normal. The first component roughly encodes $\varphi$ and the second $r$, both parameterized with an increasing value of the latent $z$ from $\varphi=-\frac{1}{2}\pi$ to $\varphi=\frac{3}{2}\pi$ for the first  and the usual direction for the latter.

Thus, the level sets of the two components are roughly locally orthogonal to each other. The first solution attempts to \textit{minimize} both sides of Eq.~\ref{eq:thm1}, that is, have $y - c(X)$ and $\nabla_y \log P_{data}(y)$ lie in the tangent space of the level set. Thus, we get (approximately) an alignment between the level sets and $- \nabla_y \log P_{data}(y)$; the approximate part comes from the need to consider the normal components only, $c(X)$ not exactly coinciding with the mean of the distribution, and, naturally, from training the model on a finite sample for a finite number of epochs, with points far away from the mean being very rare (cf. Fig.~\ref{fig:2d_gauss_data}).


The second solution, on the other hand, attempts to \textit{maximize} both sides of Eq.~\ref{eq:thm1} by having  $y - c(X)$ and $\nabla_y \log P_{data}(y)$ almost entirely normal to the level set. Thus, we get (approximately) level sets that are orthogonal to the score gradient. We will observe this dichotomy of solutions in other examples, as well. 

The encoding solution also exhibits other desirable properties, such as all the level sets being connected, as well as a smooth, directed variation of the latent. We observe what seems to be a \textit{foliation}, which implies that the Jacobian is full ``row'' (as it is a vector) rank almost everywhere on the level set. Note that the (usual) discontinuity for the angle in $\varphi=\frac{3}{2}\pi$ and $\varphi=-\frac{1}{2}\pi$ does not necessarily imply a rank defect of the Jacobian.
As the encoder is a smooth neural network, the ``jump'' is still smooth; furthermore, the level sets of the second ($r$) component are not exactly collinear with the first ($\varphi$) component at that point (they are nested and slanted ``hyperbolas'', which might be harder to spot in the figure).

% \todol{comment on the extrema here?}


\subsection{A Gaussian mixture}

The next example we present is a mixture of multivariate Gaussians where the centers are chosen to be slightly non-equidistant:
$$P_{data}(x) = \sum_{i=1}^3 \pi_i~ \gN(x; \mu_i, \sigma I),$$
where we set $\mu_1 = [-1.1, -1.1], \mu_2=[1.1, -.9]$, $\mu_3=[-0.33, 1]$, $\sigma = 0.66$, and $\pi_1 = \pi_2 = \pi_3 = \frac{1}{3}$.
The training sample is shown in Fig.~\ref{fig:trimodal_data}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.3\linewidth]{figs/trimodal_data.png}
    \caption{The training data for the mixture of Gaussians.}%
\label{fig:trimodal_data}
\end{figure}


\begin{figure}[ht!]
    \centering
    \includegraphics[width=.75\linewidth]{figs/trimodal_grad_1900.png}
    \caption{The encoding and level sets for the mixture of Gaussians. \textcolor{red}{Red:} the data density contours, with the (negative) gradient shown with black arrows. \textit{Left}: the first component of the encoder's output, \textit{right}, the second.}%
\label{fig:trimodal}
\end{figure}

The resulting DPA encoding for this problem is shown in Figure~\ref{fig:trimodal}. We again observe a similar ``polar'' decomposition, with the region of high density of the level sets coinciding with the near-zero gradient region of the data distribution (i.e., where the contributions of the three components cancel out). 

To illustrate the claims of Eq.~\ref{eq:thm1}, consider the second component. As the level sets typically have contributions from all three mixture components, the level set center $c(X)$ is likely to lie ``within'' the hyperbolas and roughly coincide with the overall center of mass for the mixture which is located in the near-zero gradient region.
Then the vectors to the level set $y - c(X)$ have a nonzero normal component almost everywhere (as we are dealing with the ``radial'' component of the encoding), which coincides with the normal component of the local density gradient emanating from the three mixture centers. In the region of ``discontinuity'' itself, the gradients of the data distribution are very small; the normal component matching between them and $c(X)$ (which likely lies in the vicinity of the ``discontinuity'') explains the slight curvature of the discontinuity.

% \todol{remove $\downarrow$}
% NOTE: Probably some slight local curvature at the exact bottom left maximum that curves left 
% For bottom right, c(X) is a bit up the line-like level set from the maximum, the level set is straight there THEN takes a turn after the maximum
%
% To illustrate Corollary~\ref{corr:extrema}, consider now the first (``polar'') component. We observe that the level sets that cross the maxima cross it in a locally approximately straight line, then join the rest at the ``discontinuity'', without picking up contributions from other regions of high density. Due to the exponential nature of the density, the center of mass $c(X)$ is likely to be very close to the local extremum. For the bottom left maximum, $c(X)$  should be slightly above and to the left; for the bottom right one, it should be relatively further to the left as the part of the level set within the ``discontinuity'' is longer. Thus we get the ``slopes'' of the approximately straight lines for those level sets, with $y - c(X)$ being tangent to them in the maxima.   







% \newpage






\subsection{The Müller-Brown Potential}%
\label{sec:MB}

The Müller-Brown potential \citep{muller_location_1979} is a commonly used benchmark two-dimensional example in computational chemistry.
It consists of three minima and multiple saddle points; the former can be observed in Fig.~\ref{fig:MB} where the contours of the potential are overlaid in red. From a physical perspective, the minima are treated as meta-stable states, and the potential is designed to simulate chemical processes where a molecule undergoes (potentially rare) transitions between different configurations (states).

The data is then generated by running Brownian dynamics simulations, typically starting from the minima. For this example, the data consists of points from different trajectories (that is, just their $x$ and $y$ coordinates) and is presented in Figure~\ref{fig:MB_data}. Furthermore, we discard any trajectory information and treat it as \textit{i.i.d.} samples from an unknown distribution; the true distribution is naturally, the Boltzmann distribution for the Müller-Brown Potential $U(x)$ (c.f. Sec.~\ref{sec:boltzmann}). This is in contrast to a popular line of deep learning methods for this problem, which explicitly takes the time-dependent nature of the trajectories into consideration by assuming a Markovian structure of the process \citep{mardt_vampnets_2018}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.3\linewidth]{figs/MB_data.png}
    \caption{Müller-Brown Potential: the training data.}%
\label{fig:MB_data}
\end{figure}


A property of considerable interest is the \textit{minimum free energy path} (MFEP), which runs through the minima and saddle points; namely, from the top left to the bottom right minimum through the middle one in Fig.~\ref{fig:MB}. %
\footnote{For the exact path, please see, for example, Figures \textit{5 g)} and \textit{h)} in \citet{bonati_unified_2023}.}%
It represents the ``least-energy-costly'' transition between the stable states. In existing examples of the use of autoencoders that treat the data as \textit{i.i.d.}, such as \citep{chen_molecular_2018, bonati_unified_2023}, the authors commonly use an autoencoder-type network to find an approximation to it by an \textit{iterative procedure}: first, encode the data produced by an unbiased simulation (such as the data used here), then use the resulting encoding to add bias to the potential and run another simulation, then encode the data again, and so on until convergence. Both steps of this procedure are potentially computationally expensive for larger problems. 

The two principal components of the DPA encoding are presented in Fig.~\ref{fig:MB}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=.75\linewidth]{figs/MB_grad_1800_red2.png}
    \caption{The encoding and level sets for the Müller-Brown potential. \textcolor{red}{Red:} the Müller-Brown potential contours, with the gradient shown with black arrows. \textit{Left}: the first component of the encoder's output, \textit{right}, the second.}%
\label{fig:MB}
\end{figure}



In contrast to the above-mentioned existing approaches, we are able to approximate the minimum free energy path  with a \textit{single} encoding of the unbiased data, despite the very limited data for the regions outside the minima (as shown in Fig.~\ref{fig:MB_data}).

Formally, the MFEP is defined as a path that connects (two) minima, along which the gradient of the potential is tangent to it everywhere.
We have shown that DPA aligns the level sets to the force (or equivalently, the negative gradient of the potential, cf. Example~\ref{sec:boltzmann}) in the normal space of the level sets. In two-dimensional examples presented so far, we typically observe  solutions where the level sets are either (mostly) tangential or orthogonal to the gradient.

In the results presented in Figure~\ref{fig:MB}, the \textit{first} component thus approximately parameterizes the MFEP in its latent $z$, tracing the path from the top-left minimum to the bottom-right one with increasing $z$.
Its level sets align themselves tangentially to the gradient almost everywhere between the minima, and cross the high-density regions orthogonally, meaning that their $c(X)$ is roughly on the MFEP.
Thus, the path can be traced by following $\nabla_x~ e(x)$, that is, moving to the next level set by increasing the value of $z$, having started from the top-left minimum.
In the second component, the level sets are typically orthogonal to the gradient in the regions between the minima, and the level set  $\{e(x) \approx 0\}$ essentially traces out the MFEP.

% NOTE: removing for now
% TODO:  revisit later
% Finally, as discussed in Example~\ref{sec:boltzmann}, the Boltzmann distribution's sufficient statistic and natural parameter are scalars (fields); hence the fact that the second component's level sets are approximately (locally) orthogonal to the first component's ones almost everywhere is relatively unsurprising; the first dimension aligns itself with the gradient of $U(y)$, which is a scalar field that explains most of the ``essential variation'' of the data; due to the encoder objective (Eq.~\ref{eq:encoder_opt_objective}), the second dimension thus attempts to explain the remaining variance and is thus approximately orthogonal to the first.\todo{rephrase?}


% \newpage

\section{Related work}%
\label{sec:related_work}

For denoising (and related contractive) autoencoders, \citet{alain_what_2014} derive a formula that shows that, for each \textit{fixed data point}, \textit{asymptotically} as the noise approaches zero, the difference between the reconstructed data vector and the original will tend to the score of the data:
\footnote{Adapted from Theorem 1 in \citet{alain_what_2014}.}
\begin{equation}
    d^*(e_\sigma^*(x)) = x+\sigma^2 \frac{\partial \log p(x)}{\partial x}+o\left(\sigma^2\right) \quad \text{ as } \sigma \rightarrow 0, \forall x \text{ fixed}.
    \label{eq:bengio_matching}
\end{equation}

In \citet{bengio_implicit_2012}, the authors further derive a connection between the score and the first two local moments of the data distribution, that is, the moments restricted to a $\delta$-ball in the ambient space centered on a given data point. Using the relation between the reconstruction for a given data point and the score, they propose estimators for these moments, which hold (additionally to the noise approaching zero) asymptotically as the radius $\delta$ approaches zero.


This point forms a natural connection to our results in Section~\ref{sec:score}. In contrast, the first two momenta appear directly in the relation given by Eq.~\ref{eq:thm1}, and are the \textit{level-set} momenta. The relation gives a \textit{geometric constraint} that describes how the entire space of the encoding adapts to the score of the data distribution and holds \textit{a.s.} for all data points. Perhaps, it is interesting to note that the results presented here are also not connected to any sort of (whether denoising, contractive, or other) regularization, as they arise directly from the distributional regression obtained from using the energy score \citep{gneiting_strictly_2007}.


Level sets themselves have been investigated for, e.g. Variational Autoencoders, for which the optimal decoder outputs the expected value over the level set: $d^*(\boldsymbol{e(x)})=\mathbb{E}[\boldsymbol{x}: \boldsymbol{x} \in \mathcal{L}_e(x)]$, resulting in e.g. blurry images and spurring the development of improvements such as the $\beta$-VAE \citep{murphy_probabilistic_2023}.



% NOTE: leave out sampling for later
% To address the locality of the score approximation, \citep{bengio_generalized_2013} and \citep{alain_regularized_2012} propose sampling approaches that converge to samples from the data distribution. The former   \todo{refresh generalized}
% \todol{Sampling with diffusion - connect to De Bortoli - not as strong} 
% \todol{Reverse Markov learning: is it connected to the sampling scheme? mention \textit{subsequent to this work ...}}
% \citep{shen_reverse_2025}

An interesting parallel that illustrates the relative power of using the energy score over score matching was recently illustrated in \citet{bortoli_distributional_2025}. By replacing the usual diffusion loss (which amounts to score matching across different noise levels \citep{song_score-based_2021}) with the energy score (as used by the DPA), the authors demonstrate improved generation performance. This mirrors the contrast between the results derived in \citep{alain_what_2014} (naturally connected to "ordinary" diffusion models via \citep{vincent_connection_2011}) and the ones presented in this work.
The score matching property of diffusion models has recently been used in \citep{arts_two_2023} to obtain the force field from data following the Boltzmann distribution, which is paralleled in this work for the Müller-Brown potential.





In terms of the informativeness of extraneous dimensions, \citet{liu_deep_2023} show that for a model architecture called the Chart Autoencoder \citep{schonsheck_chart_2020} that builds on the denoising autoencoder approach, both for exactly and approximately parameterizable manifolds, the reconstruction loss of an optimal model decreases exponentially with the manifold intrinsic dimension (and not the ambient dimension). They show that these results also hold for the classical autoencoder in the case where the manifold is globally parameterizable. The results require that the data is noised with components normal to the manifold, which requires some knowledge of the manifold geometry. In contrast, our results prove that for the DPA, the extra dimensions are conditionally independent in both manifold scenarios, and carry \textit{no} additional information, without requiring any knowledge about the data manifold. Given the results of Section~\ref{sec:score}, there might also exist a possible connection between the requirement of noising in the normal direction of the data manifold, and the DPA discovering the manifold structure by aligning the level sets with the score in the normal direction, which is left for future work.

For VAEs with a learnable decoder variance, \citet{zheng_learning_2022} show that, for simple Riemannian manifolds, the learnable variance of a globally optimal VAE decoder will tend to zero, inducing the VAE loss to tend to zero in proportion to a product of the extra dimensionality ($p-k$ in our notation) and the log of the learnable variance. In this sense, the manifold dimension can be learned. If one considers the conditional VAE, the authors show that with a careful design of the conditioning function, one can increase the above rate of convergence on subsets of the simple manifold. In our case, the results hold for any manifold that can be approximated (in the described way) by the model, and provide a concrete and precise (i.e., not limiting-behavior) characterization --- conditional independence --- of the extraneous dimensions.

Prior work on disentangled representation learning has investigated the phenomenon of extraneous dimensions (depending on the formulation) being "turned-off" \citep{higgins_beta-vae_2016, kim_disentangling_2019}; much like with the score alignment, the DPA again achieves this without any explicit regularization terms.




\section{Discussion and Conclusion}%
\label{sec:conclusion}

In this work, we present novel and desirable properties of the Distributional Principal Autoencoder (DPA), chiefly: that the level sets of the encoder orient themselves \textit{exactly} with regard to the score of the data distribution in the normal directions, \textit{and} that if the data lie on a manifold that can be approximated by the encoder, the encoder's components beyond the dimension of the manifold (or its best approximation) will be completely uninformative, that is, they will carry absolutely no additional information about the data distribution.

The first finding is crucial to explaining the method's remarkable performance in disentangling the factors that explain the data, for example, approximating the polar coordinate system for spherically-symmetric distributions. In settings where the score itself has physical meaning, such as when the data obeys the Boltzmann distribution, this can lead to recovering (in this case) the force field stemming from the potential in a single encoding. The derived relationship between the score of the data distribution and the overall geometry of the encoding significantly improves on results that hold for other autoencoder architectures, where the score approximation property is local and only holds asymptotically. 

Such strong relation also opens up possibilities of recovering the data distribution from the score by evaluating the level sets of an encoder that was trained from samples only. On this point, given the fact that DPA is ``learning'' the score, its performance as a generative model also merits investigation. 
% \todo{update in light of Markov Engression}

% \todol{move this $\downarrow$ to related work?}
% \todo{recent markov paper}
% In contrast to flow matching \citep{albergo_stochastic_2023, lipman_flow_2023}, which (can) perform optimal transport between distributions in the original data space, the authors of DPA have already suggested performing optimal transport between distributions in the latent space of DPA. Here, we additionally draw parallels to diffusion models (which flow matching generalizes), which in essence also learn the score of the data distribution \citep{song_score-based_2021}, albeit via a different mechanism.

% \todol{TODO: need to mention that this method combines both score learning and manifold detection; stress novelty}
The conditional independence of the encoded dimensions above the data manifold likewise combines and significantly improves on results shown previously for related architectures. It could promise new ways of determining the number of relevant dimensions of the data beyond heuristics such as the scree plot commonly used with methods such as PCA. Given the results presented, directly testing for conditional independence of the encoding could be an option to more precisely determine the true dimension of the data manifold.

Thus, the DPA is shown to combine two disparate and very desirable goals of unsupervised learning: the exact learning of the data distribution, as well as providing an exact way of determining the data's intrinsic dimensionality.





\newpage
\bibliography{bibliography.bib}

% \newpage
% \section*{Acknowledgments}

\newpage
\appendix
\section{Appendix}


\subsection{Proof of Theorem~\ref{thm:grad_level_set}}%
\label{sec:proof_score}
Writing out the first variation condition, we have:
\begin{equation}
    \left.\frac{\delta}{\delta e}\right|_{e^*} \mathbb{E}_{X \sim P_{data}}\left[\mathbb{E}_{Y,Y' \iid P_{e,X}^*}\left[\|Y-Y'\|^\beta\right]\right] = 0.%
\label{eq:var_condition}
\end{equation}


Equivalently, $\forall \eta$, we have for $Y \sim P^*_{e + \varepsilon \eta, X}$:

\begin{equation}
(e^* + \varepsilon \eta)(Y) \equivd (e^* + \varepsilon \eta)(X),%
\label{eq:dist_equivalence}
\end{equation}

by the definition of the ORD. We adopt the same assumptions on $\eta$ as we did on $e$ as $e + \varepsilon \eta$ needs to be a valid encoder.
Furthermore, we will be dropping the $*$ in $e^*$ --- we will be assuming $e$ to be optimal in the arguments to follow to clear up the notation.

Now, as is the norm in calculus of variations, assume the following form of the variation for small $\varepsilon$:
\begin{equation}
    e(Y) + \varepsilon \eta (Y) \stackrel{d}{\approx} e(X) + \varepsilon \eta(X),%
\label{eq:dist_approx}
\end{equation}

for $Y \sim P^*_{e + \varepsilon \eta, X}$, i.e. $\{y: (e + \varepsilon \eta)(y) \equivd (e + \varepsilon \eta)(X)\} = L_{(e^* + \varepsilon \eta)(X)}$, that is, from the \textit{perturbed} level set.


More precisely, to the first order in $\varepsilon$, we have:
$$e(Y) + \varepsilon \eta(Y) = e(X) + \varepsilon \eta(X) + \mathcal{O}(\varepsilon^2)$$

Which leads to the following property of the perturbation $\eta$:
\begin{equation}
    e(Y) - e(X) = -\varepsilon \big( \eta(Y) - \eta(X) \big) + \mathcal{O}(\varepsilon^2).%
\label{eq:variation_of_eta}
\end{equation}

for $Y \sim P^*_{e + \varepsilon \eta, X}$.

Furthermore, due to $e, \eta \in \gC^1$ and both being Lipschitz, we have $\eta(Y)-\eta(X)=\mathcal{O}(1)$.

$\forall \eta$, the stationarity condition~\ref{eq:var_condition} thus becomes:

\begin{equation}
    \left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0} \mathbb{E}_{X \sim P_{data}}\left[\mathbb{E}_{Y,Y' \iid P_{e+\varepsilon \eta, X}^*}\left[\|Y-Y'\|^\beta\right]\right] = 0%
\label{eq:var_condition2}
\end{equation}


Using the Dirac delta functions, we can express the ORD as:
\begin{equation}
    P_{e,X}^*(y) = \frac{P_{data}(y) \, \delta(e(y) - e(X))}{\int P_{data}(z) \, \delta(e(z) - e(X))~dz},%
\label{eq:ORD_delta}
\end{equation}

and, equivalently, the ``perturbed'' ORD as:
\begin{equation}
    P_{e+\varepsilon \eta, X}^* = \frac{P_{data}(y) \, \delta((e+\varepsilon\eta)(y) - (e+\varepsilon\eta)(X))}{\int P_{data}(z) \, \delta((e+\varepsilon\eta)(z) - (e+\varepsilon\eta)(X))~dz}%
\label{eq:ORD_delta_pert}
\end{equation}
Using the definition of the \textit{level-set mass}~\ref{eq:Z(X)}:
\begin{equation*}
     Z(X) = \int P_{data}(z) \, \delta(e(z)-e(X))~dz,
\end{equation*}
we have for the ``perturbed'' mass:
$$Z_\varepsilon(X) = \int P_{data}(z) ~\delta((e+\varepsilon\eta)(z) - (e+\varepsilon\eta)(X))~dz$$

Writing out Eq.~\ref{eq:var_condition2} explicitly:
\begin{align*}
0 = \E_{X \sim P_{data}} \left[ \iint \|y-y'\|^\beta
\frac{d}{d \varepsilon}\Big|_{\varepsilon=0} \left[ P_{e + \varepsilon \eta,X}^*(y) ~P_{e + \varepsilon \eta, X}^*(y')\right]  \, dy\,dy'
\right]
\end{align*}



We note:
\begin{equation}
    \left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0}P_{e+\varepsilon\eta,X}^*(y) = P_{e,X}^*(y)\left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0}\log(P_{e+\varepsilon\eta,X}^*(y)).
\end{equation}

Using the product rule on the product of the two ORDs:
\begin{align*}
&\left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0} \left[P_{e+\varepsilon\eta,X}^*(y) \, P_{e+\varepsilon\eta,X}^*(y')\right] \\
&= \left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0}P_{e+\varepsilon\eta,X}^*(y) \cdot P_{e,X}^*(y') + P_{e,X}^*(y) \cdot \left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0}P_{e+\varepsilon\eta,X}^*(y')
\end{align*}

and substituting the above, we obtain
$$\left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0} \left[P_{e+\varepsilon\eta,X}^*(y) \, P_{e+\varepsilon\eta,X}^*(y')\right] = P_{e,X}^*(y)P_{e,X}^*(y')\left[\left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0}\log(P_{e+\varepsilon\eta,X}^*(y)) + \left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0}\log(P_{e+\varepsilon\eta,X}^*(y'))\right]$$


Thus, Eq.~\ref{eq:var_condition2} is equivalent to:
\begin{equation}
    0 = \E_{X \sim P_{data}} \left[
    \iint \|y-y'\|^\beta P_{e,X}^*(y)P_{e,X}^*(y') \left[\left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0}\log(P_{e+\varepsilon\eta,X}^*(y)) + \left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0}\log(P_{e+\varepsilon\eta,X}^*(y'))\right] \, dy\,dy'
    \right]%
\label{eq:var_cond_log}
\end{equation}

Now, let's examine the derivatives of the \textit{perturbed} log-ORDs explicitly using~\ref{eq:ORD_delta_pert} and the linear variation approximation~\ref{eq:dist_approx}:
\begin{align*}
\log(P_{e+\varepsilon\eta,X}^*(y)) = \log(P_{data}(y)) + \log( \delta( e(y) + \varepsilon\eta(y) - e(X) - \varepsilon \eta(X))) \\
- \log\left(\int P_{data}(z) \, \delta(e (z) + \varepsilon\eta(z) - e(X) - \varepsilon\eta(X))dz\right)
\end{align*}

After taking the derivative evaluated at $\varepsilon=0$, we get for the second term:
$$(\eta(y) - \eta(X))\frac{\delta'(e(y) - e(X))}{\delta(e(y) - e(X))}$$

and equivalently for the second log-ORD for $y'$.  

We have used $\left.\frac{d}{d\varepsilon}\right|_{\varepsilon=0} \delta(f(x) + \varepsilon g(x)) = g(x)~\delta'(f(x))$, where we leave the derivative of the Dirac delta undefined, for now.

For the third term we get:
\begin{align*}
\frac{d}{d\varepsilon}\Big|_{\varepsilon=0} -  \log Z_\varepsilon (X) = \frac{-1}{Z(X)} \int P_{data}(z) \frac{d}{d\varepsilon}\Big|_{\varepsilon=0}\delta(e (z) + \varepsilon\eta(z) - e(X) - \varepsilon\eta(X))~dz  \\
=\frac{-1}{Z(X)} \int P_{data}(z)~(\eta(z)-\eta(X))~\delta'(e(z)-e(X))~dz,
\end{align*}

where we used our definition of the level-set mass~\ref{eq:Z(X)}.

The first-order variation condition Eq.~\ref{eq:var_condition2} thus becomes:

\begin{align}
& 0 = \mathbb{E}_{X \sim P_{data}} \left[ \iint \|y-y'\|^\beta \, \frac{P_{data}(y)P_{data}(y')}{Z(X)^2}  \, \delta(e(y) - e(X))  \, \delta(e(y') - e(X)) \right. \nonumber\\
&\quad \cdot \left\{(\eta(y)-\eta(X))  \, \frac{\delta'(e(y)-e(X))}{\delta(e(y)-e(X))} + (\eta(y')-\eta(X)) \, \frac{\delta'(e(y')-e(X))}{\delta(e(y')-e(X))} \right. \nonumber\\
& \left. \quad\quad \left. - \frac{2}{Z(X)} \int P_{data}(z) \, (\eta(z)-\eta(X)) ~ \delta'(e(z)-e(X)) \, dz \right\} \, dy\,dy' \right]%
\label{eq:var_simpl}
\end{align}

We want to use integration by parts to get rid of the pesky derivatives of the Dirac delta functions.

Now $\delta(e(z) - c)$ maps $\R^k \rightarrow \R$, and $e$: $\R^p \rightarrow \R^k$. By the higher-dimensional chain rule:
$$
D ~(\delta(e(z)-c))= D_{\delta(u)} \big|_{u = e(z) - c} ~ D_{e(z) - c} (z) = \nabla_u \delta(u) \big|_{u = e(z) - c} ~ D_e (z),
$$
where $D$ is the total derivative, and $D_{\bullet}$ the Jacobian.

We have slightly abused notation previously, using $\delta^{\prime}(e(z)-c)$ as a shorthand for the $k$-dimensional gradient $\nabla_{u} \delta(u)$ evaluated at $u=e(z)-c$, and the product of the delta with the other terms is a dot product from the higher-dimensional chain rule:

$$
\left.\frac{d}{d \varepsilon} \delta(\mathbf{u})\right|_{\varepsilon=0}=\left.\nabla_{\mathbf{u}} \delta(\mathbf{u})\right|_{\mathbf{u}=e(z)-e(X)} \cdot[\eta(z)-\eta(X)].
$$

Thus,  we have terms of the following form:

$$
-\left.\frac{1}{Z(X)} \int P_{d a t a}(z)[\eta(z)-\eta(X)] \cdot \nabla_{\mathbf{u}} \delta(\mathbf{u})\right|_{\mathbf{u}=e(z)-e(X)} d z
$$

Next, the gradients of the $\delta$ function are to be interpreted distributionally: for any (smooth, compactly supported) test function $\psi(z): \R^p \rightarrow \R$ that vanishes at the boundary, we have:
$$
\int \psi(z) \nabla_z \delta(e(z)-c)  \, dz=-\int \delta(e(z)-c) \nabla_z \psi(z) \, dz
$$
or, for the dot product with a vector-valued function $\phi$ that, likewise vanishes:
$$
\int\phi(z) \cdot \nabla_z \delta(\ldots) ~dz=-\int \nabla_z \cdot \phi(z) \, \delta(\ldots)~dz
$$
by a ``distributional'' integration by parts (via the divergence theorem).

Thus, the above chain rule is to be interpreted as the following distributional equality:
$$
\int \psi(z) \nabla_z \delta(e(z)-c)~d z=\left.\int \psi(z)\left[\nabla_{\mathbf{u}} \delta(\mathbf{u})\right]\right|_{\mathbf{u}=e(z)-c} D_e (z)~d z,
$$

% \vspace{3\baselineskip}
% \todol{NOTE: this is the approach with the explicit inverse. still we need invertibility, so might as well use $\uparrow$ and the Implicit function theorem.}

% Now, multiply both sides with $D_e^\top(z) [D_e D_e^\top(z)]^{-1}$, and obtain:
% $$
% \nabla_u \delta(u) \big|_{u = e(z) - c} = \nabla_z \delta(e(z) - e(X)) D_e^\top(z) [D_e D_e^\top(z)]^{-1}
% $$
% \todo{will need to assume full row rank of $D_e$ in $\R^k$}

% For the matrix inverse, we need $[D_e D_e^\top(z)]$ to be of rank $k$

% \todol{}
% \vspace{3\baselineskip}




% \vspace{1\baselineskip}
% \todo{equality in distributions argument} 

Now, let's apply this to Eq.~\ref{eq:var_simpl}, starting with the third term inside the square brackets:
$$\int P_{data}(z) \, (\eta(z)-\eta(X)) ~ \delta'(e(z)-e(X)) \, dz$$

Denote $\phi(z) =  P_{data}(z) ~ (\eta(z)-\eta(X)) \in \mathbb{R}^k$.

Introduce a test function $f$, i.e., examine the following integral:
$$
\int \phi(z) \cdot\left[\nabla_{\mathbf{u}} \delta(\mathbf{u})\right] \big|_{u = e(z) - c} ~ f(z) d z
$$
Now define: $G(z):=\phi(z) f(z) \in \mathbb{R}^k$. Note that $G$ vanishes at the boundary due to the $P_{data}$ factor, hence we don't need that requirement on $f$. Thus, using $u = e(z) - e(X)$:
$$
\int \phi(z) \cdot\left[\nabla_{\mathrm{u}} \delta(\mathbf{u})\right] \, f(z) \, dz=\int G(z) \cdot\left[\nabla_{\mathbf{u}} \delta(\mathbf{u})\right] \, dz
$$

Now examine the integral where we multiply the integrand with $D_e^\top (z)$ from the left, and $D_e(z)$ from the right:
$$
\int D_e^\top (z) ~ G(z) \cdot\left[\nabla_{\mathbf{u}} \delta(\mathbf{u})\right] ~ D_e(z)~ d z,
$$
\textit{assuming} the Jacobian $D_e(z)$ has \textit{full row rank} $k$ a.e. on the level set $L_{e(X)}$, as that is the integration domain induced by the $\delta$ function.

We will not be simplifying the linear algebra, but instead note that by the distributional equality of the chain rule, this must equal:
$$
\int [D_e^\top (z) ~ G(z)] \cdot (\left[\nabla_{\mathbf{u}} \delta(\mathbf{u})\right] ~ D_e(z)) ~ dz = \int (D_e^\top (z) ~ G(z)) \cdot \left[\nabla_{z} \delta(e(z) - e(X))\right] ~ dz
$$

Since this holds for any $f$, we get for $f=1$:
\begin{equation}
    \int \phi(z) \cdot \left[\nabla_{\mathbf{u}} \delta(\mathbf{u})\right] d z = \int (D_e^\top (z) ~ \phi(z)) \cdot \left[\nabla_{z} \delta(e(z) - e(X))\right] d z%
\label{eq:dist_chain}
\end{equation}

We can now integrate by parts and obtain: 
\begin{equation}
    \int (D_e^\top (z) ~ \phi(z)) \cdot \left[\nabla_{z} \delta(e(z) - e(X))\right] d z  = - \int \nabla_z \cdot (D_e^\top (z) ~ \phi(z)) ~\delta(e(z) - e(X)) ~d z%
\label{eq:per_partes}
\end{equation}

We have discarded the boundary terms since $\varphi \rightarrow 0$ as $\|z\| \rightarrow \infty$ due to $P_{data}$ being a valid (integrable) density, and $\eta$ being Lipschitz.

Thus, we arrive at the following expression for the third term:
$$\int P_{data}(z)(\eta(z)-\eta(X)) ~ \delta'(e(z)-e(X)) \, dz  = - \int \nabla_z \cdot [D_e^\top (z) ~  P_{data}(z) ~ (\eta(z)-\eta(X))] ~\delta(e(z) - e(X)) ~d z$$


% \vspace{3\baselineskip}
% \todol{TODO: go to the level set - manifold integration option}

% Thus, we arrive at the expression
% $$- \int_{\X} \nabla_z \cdot [D_e^\top (z) ~  P_{data}(z) ~ (\eta(z)-\eta(X))] ~\delta(e(z) - e(X)) ~d z  = \ldots$$ \todo{need the coarea formula}



% \todol{}
% \vspace{3\baselineskip}

% \todol{keeping the $\delta$ functions approach:}


Let's now apply the same to the other two, ``symmetric'' terms in Eq.~\ref{eq:var_simpl}:
Focus on the first one which depends on $y$. Ignoring the outer expectation over $X$, we have:

$$
\iint\left\|y-y^{\prime}\right\|^\beta \frac{P_{\text {data }}(y) P_{\text {data }}\left(y^{\prime}\right)}{Z(X)^2} \, \delta(e(y)-e(X)) \, \delta(e(y^{\prime})-e(X)) \left[(\eta(y)-\eta(X)) \frac{\delta^{\prime}(e(y)-e(X))}{\delta(e(y)-e(X))}\right]\, dy\, dy^{\prime}
$$

$$ = \frac{1}{Z(X)^2} \int P_{\text {data }} (y^{\prime})  ~ \delta\left(e\left(y^{\prime}\right)-e(X)\right) ~  d y^{\prime} \int \left\|y-y^{\prime}\right\|^\beta P_{\text {data }}(y) (\eta(y)-\eta(X)) ~ \delta^{\prime}(e(y)-e(X)) ~ dy
$$

Denoting now $\phi(y) = \left\|y-y^{\prime}\right\|^\beta P_{\text {data }}(y) ~ (\eta(y)-\eta(X))$, we get by the same integration by parts (cf. Eq.~\ref{eq:per_partes}):

$$ = \frac{-1}{Z(X)^2} \int P_{\text {data }} (y^{\prime})  ~ \delta(e(y^{\prime})-e(X)) ~  d y^{\prime} 
\int \nabla_y \cdot [D^\top_e (y)\left\|y-y^{\prime}\right\|^\beta P_{\text {data }}(y) (\eta(y)-\eta(X))] ~ \delta(e(y)-e(X)) ~ dy
$$

And, due to symmetry, for the other term:
$$ = \frac{-1}{Z(X)^2} \int P_{\text {data }} (y) ~ \delta(e(y)-e(X)) ~  d y
\int \nabla_{y^\prime} \cdot [D^\top_e (y^{\prime})\left\|y-y^{\prime}\right\|^\beta P_{\text {data }}(y^{\prime}) (\eta(y^{\prime})-\eta(X))] ~ \delta(e(y^{\prime})-e(X)) ~ dy^{\prime}
$$

Putting it all together, we have the following equivalent of Eq.~\ref{eq:var_simpl}:
\begin{align}
& 0 = \mathbb{E}_{X \sim P_{data}}\Bigg[ \frac{1}{Z(X)^2}  \int \int  \delta(e(y)-e(X))~ \delta(e(y^{\prime})-e(X)) \nonumber\\
& \Big\{ - P_{\text {data }} (y^{\prime})  ~ \nabla_y \cdot [D^\top_e (y)\left\|y-y^{\prime}\right\|^\beta P_{\text {data }}(y) (\eta(y)-\eta(X))] 
- P_{\text {data }} (y) \nabla_{y^\prime} \cdot [D^\top_e (y^{\prime})\left\|y-y^{\prime}\right\|^\beta P_{\text {data }}(y^{\prime}) (\eta(y^{\prime})-\eta(X))] \nonumber \\
& + \frac{2}{Z(X)} \|y-y'\|^\beta P_{data}(y) P_{data}(y') \int \nabla_z \cdot [D_e^\top (z) ~  P_{data}(z) ~ (\eta(z)-\eta(X))] ~\delta(e(z) - e(X)) ~d z \Big\} dy ~dy' \Bigg]
\end{align}


We can now spot that the first two terms inside the curly brackets are identical since $Y, Y' \iid P_{e, X}^*$, hence we can simplify. Thus, $\forall \eta$:

\begin{align}
& 0 = \mathbb{E}_{X \sim P_{data}}\Bigg[ \frac{2}{Z(X)^2}  \int P_{\text {data }} (y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy' \int  \delta(e(y)-e(X)) \nonumber\\ 
&\Big[ -  \nabla_y \cdot [D^\top_e (y)\left\|y-y^{\prime}\right\|^\beta P_{\text {data }}(y) (\eta(y)-\eta(X))] 
\nonumber \\
& + \frac{1}{Z(X)} \|y-y'\|^\beta P_{data}(y)  \int \nabla_z \cdot [D_e^\top (z) ~  P_{data}(z) ~ (\eta(z)-\eta(X))] ~\delta(e(z) - e(X)) ~d z \Big] dy \Bigg]%
\label{eq:var_simpl2}
\end{align}

The optimality condition will be zero in full generality --- $\forall \eta$ --- if the terms inside the brackets are zero, implying $\forall X$ \textit{almost surely}:

\begin{align}
    &\int P_{\text {data }} (y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy' \int  \nabla_y \cdot [D^\top_e (y)\left\|y-y^{\prime}\right\|^\beta P_{\text {data }}(y) (\eta(y)-\eta(X))]~  \delta(e(y)-e(X))   ~dy \nonumber \\
    & = \frac{1}{Z(X)}  \int P_{\text {data }} (y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy'  \int \|y-y'\|^\beta P_{data}(y)~  \delta(e(y)-e(X))  ~dy  \nonumber \\
    &\int \nabla_z \cdot [D_e^\top (z) ~  P_{data}(z) ~ (\eta(z)-\eta(X))] ~\delta(e(z) - e(X)) ~d z%
\label{eq:var_general_final}
\end{align}




\subsubsection{Picking \texorpdfstring{$\beta$}{beta} \texorpdfstring{$= 2$}{equal to two}}
While Eq.~\ref{eq:var_general_final} holds in full generality, we now focus on the case of the squared Euclidean norm: $\beta=2$.

First, let's restate the definition of the \textit{level-set center of mass} (Eq.~\ref{eq:c(X)}):
\begin{equation*}
    c(X) = \frac{1}{Z(X)}\int y ~ P_{data}(y) ~ \delta(e(y) - e(X)) ~  dy
\end{equation*}

Likewise, for the \textit{level-set variance} (Eq.~\ref{eq:V(X)}).
\begin{equation*}   
    V(X) = \int\|y-c(X)\|^2 P_{data}(y) ~ \delta(e(y) - e(X)) ~dy
\end{equation*}

Expanding the norm, we get:
\begin{align*}
& \|y-y'\|^2  = \|y - c(X) + c(X) - y'\|^2  = \underbrace{\|y-c(X)\|^2}_{A} + \underbrace{\|y'-c(X)\|^2}_{B} \underbrace{-2 \langle y-c(X), y'-c(X)\rangle}_{C}
\end{align*}

On both sides of Eq.~\ref{eq:var_general_final}, the C term vanishes:
First, the RHS:
\begin{align*}
   & \frac{-2}{Z(X)}  \int P_{\text {data }} (y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy'  \int  \langle y- c(X), y' - c(X) \rangle 
 ~ P_{data}(y) ~ \delta(e(y)-e(X)) ~dy \int \ldots d z \\
      & =  \frac{-2}{Z(X)}  \int  \langle y- c(X), \smallint  (y' - c(X)) P_{\text {data }} (y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy'   \rangle  ~ P_{data}(y) ~ \delta(e(y)-e(X))  ~ dy \int \ldots d z \\
      & = 0
\end{align*}
where we used the bilinearity property of inner product (and Fubini's theorem), and:
$$\int  (y' - c(X)) P_{\text {data }}(y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy' = Z(X) c(X) - c(X) Z(X)  = 0$$
by the definition of the center-of mass.

On the LHS, we get a term like:
$$\int P_{\text {data }} (y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy' \int  \delta(e(y)-e(X))   \nabla_y \cdot [ \langle y - c(X), y' - c(X) \rangle 
P_{\text {data }}(y) ~ D^\top_e (y)(\eta(y)-\eta(X))] ~dy 
$$
Since the divergence operator is again linear, we can bring the integral over $y'$ inside the inner product, getting this term to vanish, again. 
% \todo{add the derivation by writing out the divergence component-wise + Fubini?}

The B term gives us on the RHS:
\begin{align*}
    & \frac{1}{Z(X)}  \int  \|y' - c(X)\|^2  P_{\text {data }} (y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy'  \int  \delta(e(y)-e(X)) P_{data}(y) ~dy \\
    & \int \nabla_z \cdot [D_e^\top (z) ~  P_{data}(z) ~ (\eta(z)-\eta(X))] ~\delta(e(z) - e(X)) ~d z  = \frac{1}{Z(X)} V(X) Z(X) \int \ldots d z \\
    & = V(X) \int \nabla_z \cdot [D_e^\top (z) ~  P_{data}(z) ~ (\eta(z)-\eta(X))] ~\delta(e(z) - e(X)) ~d z
\end{align*}

On the LHS:

\begin{align*}
    &\int P_{\text {data }} (y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy' \int  \delta(e(y)-e(X))   \nabla_y \cdot [D^\top_e (y)\left\|y^{\prime} - c(X) \right\|^2 P_{\text {data }}(y) (\eta(y)-\eta(X))] ~dy\\
    & = V(X) \int    \nabla_y \cdot [D^\top_e (y) P_{\text {data }}(y) (\eta(y)-\eta(X))] ~ \delta(e(y)-e(X))  ~dy
\end{align*}

Thus, we have for the optimality condition~\ref{eq:var_general_final} in terms of $c(X)$ and $V(X)$ on the RHS:

\begin{align*}
&\frac{1}{Z(X)}  \int P_{\text {data }} (y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy'  \int  \delta(e(y)-e(X)) \|y-y'\|^2 P_{data}(y) ~dy  \nonumber \\
&\int \nabla_z \cdot [D_e^\top (z) ~  P_{data}(z) ~ (\eta(z)-\eta(X))] ~\delta(e(z) - e(X)) ~d z \nonumber \\
& = \frac{1}{Z(X)}  \int P_{\text {data }} (y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy'  \int  \|y - c(X)\|^2  P_{data}(y) ~ \delta(e(y)-e(X)) ~dy \int \ldots ~d z \nonumber \\
& +  V(X) \int \nabla_z \cdot [D_e^\top (z) ~  P_{data}(z) ~ (\eta(z)-\eta(X))] ~\delta(e(z) - e(X)) ~d z \nonumber \\
&= 2 ~V(X) \int \nabla_z \cdot [D_e^\top (z) ~  P_{data}(z) ~ (\eta(z)-\eta(X))] ~\delta(e(z) - e(X)) ~d z \nonumber 
\end{align*}

On the LHS:
\begin{align*}
&\int P_{\text {data }} (y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy' \int  \nabla_y \cdot [D^\top_e (y)\left\|y-y^{\prime}\right\|^2 P_{\text {data }}(y) (\eta(y)-\eta(X))] ~ \delta(e(y)-e(X))   ~dy \nonumber \\
& = \int P_{\text {data }} (y^{\prime}) ~ \delta(e(y^{\prime})-e(X)) ~dy' \int  \delta(e(y)-e(X))   \nabla_y \cdot [ \left\|y - c(X) \right\|^2 ~ P_{\text {data }}(y) D^\top_e (y)(\eta(y)-\eta(X))] ~dy \nonumber \\
& + V(X) \int    \nabla_y \cdot [D^\top_e (y) P_{\text {data }}(y) (\eta(y)-\eta(X))] ~ \delta(e(y)-e(X))~dy \\
& = Z(X) \int     \nabla_y \cdot [ \left\|y - c(X) \right\|^2 ~ P_{\text {data }}(y) D^\top_e (y)(\eta(y)-\eta(X))]~ \delta(e(y)-e(X)) ~dy\\
& + V(X) \int    \nabla_y \cdot [ P_{\text {data }}(y) D^\top_e (y)  (\eta(y)-\eta(X))] ~ \delta(e(y)-e(X))~dy 
\end{align*}


Thus:
\begin{align*}
& Z(X) \int     \nabla_y \cdot [ \left\|y - c(X) \right\|^2 ~ P_{\text {data }}(y) D^\top_e (y)(\eta(y)-\eta(X))]~ \delta(e(y)-e(X)) ~dy\\
& + V(X) \int    \nabla_y \cdot [ P_{\text {data }}(y) D^\top_e (y)  (\eta(y)-\eta(X))] ~ \delta(e(y)-e(X))~dy \\
=
& 2 ~V(X) \int \nabla_z \cdot [~  P_{data}(z) ~ D_e^\top (z)  (\eta(z)-\eta(X))] ~\delta(e(z) - e(X)) ~d z \nonumber 
\end{align*}


Since $y$ and $z$ are just integration variables, we can rename them and we bring over the second term on the LHS to the RHS.
Thus we get, $\forall \eta$, $\forall X$ almost surely:
\begin{align}
& \int \left(\nabla_y \cdot [ \left\|y - c(X) \right\|^2 ~ P_{\text {data }}(y) D^\top_e (y)(\eta(y)-\eta(X))] \right)~ \delta(e(y)-e(X)) ~dy \nonumber \\
& = \frac{V(X)}{Z(X)} \int \left(\nabla_y \cdot [~  P_{data}(y) ~ D_e^\top (y)  (\eta(y)-\eta(X))] \right) ~\delta(e(y) - e(X)) ~dy%
\label{eq:integral_balance}
\end{align}


In the above, we are dealing with divergences of the following form: $\nabla_y \cdot [f(y) M(y) v(y)]$,
where  $f_1(y)=\|y-c(X)\|^2 P_{\text {data }}(y)$ is a scalar (with $f_2(y)=\frac{V(X)}{Z(X)} P_{\text {data }}(y)$ on the other side), $M(y)=D_e^{\top}(y)$ a $p \times k$ matrix, and $v(y)=\eta(y)-\eta(X)$ a $k$-vector. We will only expand the first term, namely:
$$
\nabla_y \cdot[f(y) M(y) v(y)] = \nabla_y f(y) \cdot [M(y) v(y)] + f(y) \nabla_y \cdot[M(y) v(y)]
$$

Eq.~\ref{eq:integral_balance} thus decomposes into:
\begin{align}
& \int   \nabla_y f_1(y) \cdot [M(y) v(y)] ~ \delta(e(y)-e(X)) ~dy + \int f_1(y) \nabla_y \cdot[M(y) v(y)] ~ \delta(e(y)-e(X)) ~dy \nonumber ~ = \\
& \int   \nabla_y f_2(y) \cdot [M(y) v(y)] ~ \delta(e(y)-e(X)) ~dy + \int f_2(y) \nabla_y \cdot[M(y) v(y)] ~ \delta(e(y)-e(X)) ~dy%
\label{eq:integral_balance_expanded}
\end{align}


We wish to go from an integral equality to a statement about the \textit{integrands}. The argument will be made as follows:
\begin{enumerate}
    \item We will argue that the second, divergence terms become negligible in the first-order stationarity conditions (Eq.~\ref{eq:var_condition2}).
    \item \textit{At this order} (i.e., at order $\varepsilon$), we will show that the \textit{integrands} of the first terms must coincide \textit{almost surely}.
\end{enumerate}


From matching the second terms (with the perturbation inside the divergence), one might expect that on the level sets, we would have 
$$f_1(y) \stackrel{a.e.}{\equiv} f_2(y), $$ which would lead to spherical level sets:
$$\|y-c(X)\|^2=\frac{V(X)}{Z(X)}.$$

Another option is for the divergence $\nabla_y \cdot[M(y) v(y)]$ to vanish. The ``trivial'' solution $M(y) v(y) = 0 \Leftrightarrow D_e^{\top}(y)(\eta(y)-\eta(X)) = 0$ would imply that all perturbations are \textit{tangential} to the level set; this would fly in the face of the variational argument where we allow $\eta$ to vary freely under a mild assumption (i.e., that it is differentiable and smooth).

Thus, we aim to show that the divergence  $\nabla_y \cdot[M(y) v(y)]$  vanishes when integrated when considering the first-order optimality conditions. As a refresher, we have denoted the unperturbed level set (manifold) as:
$$
L_{e(X)} = \left\{y \in \mathbb{R}^p: e(y) = e(X) \right\}
$$
We have already assumed that $D_e$ has full row rank $k$ \textit{a.e.} on $L_{e(X)}$. Thus, $\operatorname{dim}\left(L_{e(X)}\right)=p-k$.
The normal space at $y \in L_{e(X)}$ is spanned by the rows of $D_e(y)$ (or, equivalently, the columns of $D_e^{\top}(y)$). 
Thus, any small displacement $\delta y \in \mathbb{R}^p$ of $y$ can be uniquely decomposed into the normal and tangential component:
$$
\delta y=\delta y_{\|}+\delta y_{\perp}, \quad \text { with } \quad D_e(y) \, \delta y_{\|}=0, \quad D_e(y) \, \delta y_{\perp} \neq 0
$$

Let's consider now the perturbed level set for a small $\varepsilon$:
$$
L_{(e + \varepsilon \eta)(X)} =\{y:(e+\varepsilon \eta)(y)=(e+\varepsilon \eta)(X)\}
$$

As we've already shown: to the first order in $\varepsilon$, if $y$ is on $L_{(e + \varepsilon \eta)(X)}$, then we have Eq.~\ref{eq:variation_of_eta}:
$$
e(y)-e(X) = -\varepsilon[\eta(y)-\eta(X)]
$$

Now, let's expand $e(y+\delta y) - e(y)$ locally around $y \in L_{e(X)}$:
$$e(y+\delta y) - e(y) \approx D_e(y) \delta y.$$

Or, equivalently, by setting $\delta y$ to be the difference from some reference point $y_0 \in  L_{e(X)}$, we get
$$
e(y) - e\left(y_0\right) \approx D_e \left(y_0\right) \left(y-y_0\right)
$$

Again, for $y$ on the (close-by) perturbed manifold, that difference must equal $-\varepsilon\left[\eta(y)-\eta\left(y_0\right)\right]$. So

$$
D_e\left(y_0\right)\left[y-y_0\right] \approx-\varepsilon\left[\eta(y)-\eta\left(y_0\right)\right]
$$


We have assumed that $\eta$ is Lipschitz, meaning that $\eta(y)-\eta\left(y_0\right)$ is $\mathcal{O}\left(\left\|y-y_0\right\|\right)$.

We now wish to show that the \textit{normal displacement} $y-y_0$ is forced to be $\mathcal{O}(\varepsilon)$.
Again, use $\delta y = y  - y_0$, with $y \in L_{(e + \varepsilon \eta)(X)}$ and $y_0 \in L_{e(X)}$, as we can WLOG pick the displacement that lands on a nearby level set of \textit{some} perturbed encoder.

Denote the projection onto the row space of $D_e\left(y_0\right)$ as $\Pi_{\perp}$ (i.e., the projection into the normal space of the level set at $y_0$).
We have:
$$
D_e\left(y_0\right) \delta y  = D_e\left(y_0\right) (\delta y_{\|} + \delta y_{\perp})  = D_e\left(y_0\right) \Pi_{\perp} \delta y
$$

Thus:
$$
D_e(y_0) \Pi_{\perp} \delta y=\Pi_{\perp} \left[-\varepsilon\left(\eta(y)-\eta\left(y_0\right)\right)\right] + \OO(\varepsilon^2) = \mathcal{O}(\varepsilon)
$$

Since $D_e(y_0)$ has full row rank by assumption, its pseudo-inverse $\exists$, hence we can state:
$$
\Pi_{\perp} \delta y=\Pi_{\perp} \left[ D_e(y_0)^{\dagger} \left( -\varepsilon (\eta(y) - \eta (y_0) ) \right)\right] + \OO(\varepsilon^2)
$$

Taking a norm on both sides:

\begin{align*}
&\|\Pi_{\perp} \delta y \| = \| \Pi_{\perp} \left[ D_e(y_0)^{\dagger} \left( -\varepsilon (\eta(y) - \eta (y_0) ) \right)\right] \|
\leq |\varepsilon|  ~ \| \Pi_{\perp} D_e(y_0)^{\dagger} \| ~ \|(\eta(y) - \eta (y_0) )\|\\
& \leq |\varepsilon| ~ \| D_e(y_0)^{\dagger} \| ~ \|(\eta(y) - \eta (y_0) )\|,
\end{align*}
where we repeatedly applied the Cauchy-Schwarz inequality and used the fact that the norm of a projection operator is one.

We will use the fact that both $e$ and $\eta$  are smooth and Lipschitz with constants $L_1$ and $L_2$. Furthermore, besides the existing assumption that 
$D_e(y_0)$ has full row rank, assume that this \textit{also} holds in the small (open) neighborhood that includes $y + \delta y$.
Thus, the norm of the pseudo-inverse is also (locally) bounded by a finite constant $C$. Hence, we have
$$
\|\delta y_{\perp} \| \leq |\varepsilon| ~ C ~ L_2 \|\delta y \| + \OO(\varepsilon^2) = \OO(\varepsilon)
$$


Next, we note that the divergence operator is a linear operator and can be decomposed into the tangential and normal component (relative to the current level set):
$$
\nabla_y \cdot = (\nabla_{y, \|} \cdot ) ~ + ~  (\nabla_{y, \perp} \cdot)
$$

We have, $\forall \delta y$: $\nabla_{y, \|} \cdot \delta y_\perp = 0$ and  $\nabla_{y, \perp} \cdot \delta y_\| = 0$

Since the terms we are taking  the divergence over are of the form:
 $$\nabla_y \cdot\left[D_e^{\top} (y) (\eta(y)-\eta(X))\right],$$

 we thus have
$$\nabla_y \cdot\left[D_e^{\top} (y) (\eta(y)-\eta(X))\right] = \nabla_{y, \perp} \cdot\left[D_e^{\top} (y) \left(\eta(y)-\eta(X)\right) \right]_{\perp}  $$

(since the column space of $D_e^\top$ spans exactly the normal space).
Thus, only the normal components of the divergence will play a part. 

Denote the vector field that the divergence is taken over as $$
v(y) = \left[ D_e^{\top}(y)[\eta(y)-\eta(X)] \right]_\perp
$$

Namely, since $\|\delta y_{\perp} \|  = \OO(\varepsilon)$ (as shown above), we have again by Lipschitz-ness of the perturbation:
$\|(\eta(y)-\eta(y_0))_\perp\| = \mathcal{O}(\|y-y_0\|) = \mathcal{O}(\|\delta y_\perp\|) = \mathcal{O}(\varepsilon)$. So $\|v(y)\|=\mathcal{O}(\varepsilon)$, since the norm of the Jacobian is bounded, as well, due to its regularity.

Next, let's define a ``cylindrical'' region $\mathcal{R}_\varepsilon$ around the old level set $L_{e(X)}$:
$$
\mathcal{R}_\varepsilon:=\left\{y :  \operatorname{d}\left( y, L_{e(X)}\right) \leq c~\varepsilon\right\}
$$
for some small constant $c$ so that the perturbed level set is contained within $\gR_\varepsilon$. The ``thickness'' of this region in the normal direction is at most $\mathcal{O}(\varepsilon)$. Its measure thus  cannot exceed $\mathcal{O}(\varepsilon)$ times the measure of the manifold $L_{e(X)}$, meaning it is again $\OO(\varepsilon)$.

Finally, let's apply the divergence theorem:
$$
\int_{\mathcal{R}_\varepsilon} \nabla_y \cdot v(y) d y=\int_{\partial \mathcal{R}_\varepsilon} v(y) \cdot \hat{n}(y) d S
$$

$\|v(y)\|$ is  $\mathcal{O}(\varepsilon)$ for all $y$ in the region, and the measure of $\partial \mathcal{R}_\varepsilon$ is at most $\mathcal{O}(\varepsilon)$. Thus the (flux) integral is at most $\mathcal{O}(\varepsilon) \times \mathcal{O}(\varepsilon)=\mathcal{O}\left(\varepsilon^2\right)$. 


This means, that when considering the first-order optimality condition~\ref{eq:var_condition2}, for which we have obtained and expression of the form:
$$
0=\lim _{\varepsilon \rightarrow 0} \frac{1}{\varepsilon}(F(\varepsilon)-F(0))= (\int \nabla_y f(y) \cdot \ldots dy \text { - like terms}) +  (\int f(y) \nabla_y  \cdot \ldots dy \text { - like terms}),
$$
where we combine the terms from both sides of Eq.~\ref{eq:integral_balance_expanded}.
We have shown that the second group of terms is $\OO(\varepsilon^2)$, thus it vanishes in the limit and cannot play a role in the first-order optimality conditions. 


Now to the second point. That is, \textit{assume} that the second terms, where the variation $\eta$ appears inside the divergence, vanish in the first-order stationarity condition.

Thus we have the following integral equality, $\forall X, \forall \eta$:
\begin{align*}
& \int \nabla_y \left[\|y-c(X)\|^2 P_{\text {data }}(y)\right] ~ D_e^{\top}(y) \, \delta(e(y)-e(X)) ~ (\eta(y)-\eta(X)) \, dy\\
& = \int \frac{V(X)}{Z(X)} \nabla_y \left[ P_{\text {data }}(y)\right] ~ D_e^{\top}(y) \, \delta(e(y)-e(X)) ~ (\eta(y)-\eta(X))~  d y
\end{align*}


Now, denote:
$$
F_1(y, X ) = \nabla_y \left[\|y-c(X)\|^2 P_{\text {data }}(y)\right] ~ D_e^{\top}(y) ~\delta(e(y)-e(X))
$$

and

$$
F_2(y, X)= \frac{V(X)}{Z(X)} \nabla_y \left[ P_{\text {data }}(y)\right] ~ D_e^{\top}(y) ~\delta(e(y)-e(X))
$$

So, the identity becomes:
\begin{align*}
& \int F_1(y, X) ~ (\eta(y)-\eta(X)) ~  d y  = \int F_2(y, X) ~ (\eta(y)-\eta(X))~  d y
\end{align*}

While we have put the $\delta$ functions inside the integrands to be compared as to leave the (what are to be) test functions $\eta$ clearly separated, one needs to keep in mind that they will induce the integrals to be over the level set manifold surface measure. Additionally, $F_1$ and $F_2$ are now distributions. Also note that we have assumed no specific constraints on $\eta$ besides them being smooth and Lipschitz.

We will proceed via a proof by contradiction. Assume $\exists A \subseteq \{(y, X)\}$ with nonzero measure w.r.t. $dy ~dP_{data}(x)$ such that:
$$F_1(y, X) \indic{(y, X) \in A} \neq F_2(y, X) \indic{(y, X) \in A}$$

Furthermore, assume that $\pi_y(A) \subset$ $\{y: e(y)=e(X)\}$, where $\pi_y$ is the projection to $y$. In other words, assume that the integrands differ on a ``subsection'' of the (unperturbed) level set (the delta functions inside $F$ would make the opposite --- $A \not\subset L_{e(X)}$ --- impossible, anyway).

Now, let's pick ($\forall X$) an $\eta$ to be a smooth function such that:
$$
\eta(y) - \eta(X) = 
\begin{cases}0, & \text {outside } \pi_y(A) \\
\text { nonzero and positive in all components} & \text { inside } \pi_y(A)
\end{cases},
$$

that is, we're using the ``bump'' or \textit{partition of unity} approach common in calculus of variations \citep{giaquinta_calculus_2004}.

Then, we have clearly for such $\eta$:
\begin{align*}
& \int F_1(y, X) ~ (\eta(y)-\eta(X)) ~  d y  \neq \int F_2(y, X) ~ (\eta(y)-\eta(X))~  d y
\end{align*}

Since the integral equality must hold for any $\eta$, $\eta$-s form a rich function class, and our picked $\eta$ satisfies the requirements of the class, we arrive at a contradiction.

% \vspace{\baselineskip}

Thus we have, in the first order of $\varepsilon$:
$$
\nabla_y \left[\|y-c(X)\|^2 P_{\text {data }}(y)\right] ~ D_e^{\top}(y) \stackrel{\text{a.s. in } y}{=}
\frac{V(X)}{Z(X)} \nabla_y \left[ P_{\text {data }}(y)\right] ~ D_e^{\top}(y),
$$
almost surely in $X$ on the level set of $e$.

Taking the gradients, we obtain:

$$
\left[ 2(y - c(X)) P_{\text{data}}(y) +  \|y-c(X)\|^2 ~ \nabla_y P_{\text {data }}(y)\right] ~ D_e^{\top}(y) \, \stackrel{\text{a.s. in } y}{=} \, \frac{V(X)}{Z(X)} \nabla_y P_{\text {data }}(y) ~ D_e^{\top}(y),
$$


and finally:

$$
\frac{2(y - c(X))}{\frac{V(X)}{Z(X)} - \|y-c(X)\|^2} ~  D_e^{\top}(y) \stackrel{\text{a.s. in } y}{=} \frac{\nabla_y P_{\text {data }}(y)}{P_{\text {data }}(y)} ~ D_e^{\top}(y).
$$

\qed

\subsubsection{Proof of Corollary~\ref{corr:extrema}}%
\label{sec:proof_extrema}
By the fact that at the extrema, we have $\nabla_y \, P_{data}(y) = 0$. This gives us the following relation for Eq.~\ref{eq:thm1}:

\begin{equation*}
    \frac{2 (y - c(X))}{\frac{V(X)}{Z(X)} - \|y-c(X)\|^2} ~ D_{e^*}^{\top}(y) = 0.    
\end{equation*}

As $V(X)$ is finite by assumption and actually minimized on the level set due to the encoder's optimization objective~\ref{eq:encoder_opt_objective}, and $0 < Z(X) \leq 1$, the only way the denominator could go to ($-$) infinity is if $\|y-c(X)\|$ was itself approaching infinity. 

As stated, we will not be considering ``trivial'' minima at distance approaching infinity; thus if $y$ is a maximum at ``infinity'', the data density is not integrable, and we get a contradiction.

Hence we have:
$$(y - c(X)) ~  D_{e^*}^{\top}(y) = 0.$$

This satisfied by either:
\begin{enumerate}
    \item $(y - c(X)) = 0$, that is, the encoder aligns the level set so its center of mass coincides with the local extremum (most likely maximum), \textit{or}
    \item $(y - c(X)) ~  D_{e^*}^{\top}(y) = 0$, that is, the normal projection of $(y - c(X))$ is exactly zero, meaning that $(y - c(X))$ lies in the tangent space of the level set at the extremal point.
\end{enumerate}

\qed



\newpage

\subsection{Proofs of Section~\ref{sec:independence}}

\subsubsection{Comment: DPA recovers the data distribution}

To refresh, for an optimal encoder/decoder pair, we have:
$$d^*\left(e^*(X), \varepsilon\right) \sim P_{e^*, X}^* $$
That is:
$$d^*(z, \varepsilon) \equivd (X | e^*(X) = z), ~~ \forall z$$

Treating $Z = e^*(X)$ as a random variable, we have:
$$d^*(Z = z, \varepsilon) \equivd (X | Z = z)$$
Thus, for any measurable set $A$:
$$
\P(X \in A)=\int P(X \in A \mid Z=z) ~P_Z(z) ~d z
$$
and
$$
\P(d^*(Z, \varepsilon) \in A) = \int P(d^*(Z, \varepsilon) \in A \mid Z=z) ~P_Z(z)~ d z
$$
Since $d^*(Z = z, \varepsilon) \equivd (X | Z = z)$, the two integrals match, and by the law of total probability:

\begin{equation}
    d^*(e(X), \varepsilon) \stackrel{d}{=} X%
\label{eq:data_dist_recon}
\end{equation}



\subsubsection{Proof of Propositions~\ref{prop:exact_data_dist} and~\ref{prop:parameterizable_is_K_prime}}%
\label{sec:proof_K_prime}

We assume that the manifold $\mathcal{M} $ can be exactly parameterized by a smooth, injective $e$ in the first $K$ components $e_{1: K}: \mathcal{M} \rightarrow \mathbb{R}^K$, 
and that our encoder function class is expressive enough to achieve this.

If we assume this $e_{1:K}$ is optimal among $K$-dimensional encoders, this must necessarily imply by the injectivity of the parameterization and the definition of the ORD:
\begin{equation}
    d^*\left([e_{1: K}(x), \varepsilon_{(K+1): p}]\right) \sim P_{e_{1: K, x}}^* (x)= \delta(x), ~ \forall x,%
\label{eq:k_ord}
\end{equation}
where $d^*$ is the accompanying optimal decoder.%
\footnote{This has been given as an example on page 6 of \citet{shen_distributional_2024} for a general invertible $e^*$.}

Or in other words, Eq.~\ref{eq:data_dist_recon} becomes an almost-sure equality:
$$
d^*\left(e_{1: K}(X), \varepsilon\right) \stackrel{\text { a.s. }}{=} X
$$

This implies for the two terms in $L_K[e_{1:K}, d^*]$:
$$\mathbb{E}_X \mathbb{E}_{Y \sim P_{d, e_{1: K}(X)}}\left[\|X-Y\|^\beta\right] = \mathbb{E}_X \mathbb{E}_{Y \sim \delta(X)}\left[\|X-Y\|^\beta\right] \equiv 0$$
and
$$\mathbb{E}_X \mathbb{E}_{Y, Y \iid P_{d, e_{1: K}(X)}}\left[\left\|Y-Y^{\prime}\right\|^\beta\right] =  \mathbb{E}_X \mathbb{E}_{Y, Y \iid \delta(X)}\left[\left\|Y-Y^{\prime}\right\|^\beta\right] \equiv 0 $$

Since $L_k[e, d] \geq 0$, this is indeed the global minimum. Thus, the encoder is an $K$-best-approximating encoder, and the manifold is $K'$-parameterizable by $(e_{1:K}, d^*)$ with $K'=K$.

\qed%


\subsubsection{Proof of Theorem~\ref{thm:independence_relaxed}}%%
\label{sec:proof_ind_relaxed}
We consider the setting where the data $X$ are still supported on a $K$-dimensional manifold, which, however, might not be globally parameterizable by a single, smooth encoder $e$ with $K$ output dimensions.

% In general, we denote: $d\left([e_{1: k}(x), \varepsilon_{(k+1): p}]\right) \sim P_{d, e_{1: k}}(X)$.

We have defined the $K'$-best-approximating encoder as the encoder that minimizes the loss the when considering terms up to the $K'$-th term only:
$$
(e^*, d^*) \in \underset{e, d}{\operatorname{argmin}} \sum_{k=0}^{K'} L_k[e, d]
$$
again taking the weights to be uniform, \textit{and} which achieves the globally best energy score in its $K'$-th term:
$$L_{K'}[(e^*, d^*)] = \min_{e, d, k} L_k[e, d].$$


This also automatically makes the manifold $K'$-parameterizable, with any remaining manifold variance being non-explainable with a DPA.

By the fact that for $\beta \in (0, 2)$, the energy score is strictly proper, this encoder/decoder pair is indeed \textit{the unique} global optimum for $K'$-dimensional encoders, with $P_{d^*, e^*_{1: K'}(X)}$ being the \textit{unique} distribution minimizing $L_{K'}[(e^*, d^*)]$.

Now consider the overall $p$-dimensional problem given by Eq.~\ref{eq::opt_joint_obj}.
The terms for $K'+1 \ldots p$ cannot do better than $L_{K'}[e^*,d^*]$ by the definition of our $K'$-best-approximating encoder, thus the best an encoder can do is to output the same distribution $P_{d^*, e^*_{1: K'}(X)}$.

Next we observe that the $K'$-dimensional optimization is a nested subproblem of the $p$-dimensional one.
Thus, when optimizing over $p$ dimensions, the optimal encoder must coincide with the above $K'$-best-approximating encoder in terms up to $K'$, otherwise it would contradict the latter being the global optimum among $K'$-dimensional encoders.

Hence, the $K'$-best-approximating encoder is the global optimum for all $p$ dimensions and we obtain for the optimal solution:
$$P_{d^*, e^*_{1: k}(X)} = P_{d^*, e^*_{1: K}(X)}, ~ \text{ for }  k > K'$$ 


Next, we wish to show that the ``extra'' dimensions are independent of the data conditioned on the relevant components $1:K'$. Consider the $(K'+1)$-th component.
Note that $e^*_{1: K'}(X)$ and $e^*_{1: K' + 1}(X)$, when viewed as joint distributions (over dimensions of $e^*$), form a filtration; denote $\gF_K = \sigma((e^*_{1}(X), \ldots, e^*_{{K'}}(X)))  \subseteq \gF_{{K'}+1} = \sigma((e^*_{1}(X), \ldots, e^*_{{K'}+1}(X)))$.

By Eq.~\ref{eq:extra_dim_dist}, we have
$$X | \gF_{{K'}+1} \equivd X | \gF_{{K'}}, $$
hence
$$\E[f(X) | \gF_{{K'}+1}] = \E[f(X) | \gF_{{K'}}]$$ 
for any (Borel-measurable) function $f$.

Let $Z$ be $\gF_{{K'}+1}$ --- but not $\gF_{{K'}}$ --- measurable. The claim from the theorem is then equivalent to the following conditional independence:
$$ Z \ind X \vert \gF_{{K'}}$$
or
$$\E[g(Z) f(X) \vert \gF_{K'}] = \E[g(Z) \vert \gF_{K'}] ~\E[f(X) \vert \gF_{K'}]$$
for any pair of integrable functions $f, g$.

We can prove this claim in the following way:
\begin{align*}
    & \E[g(Z) f(X) \vert \gF_{K'}] \underbrace{=}_{\text{tower}} \E[\E[g(Z) f(X) | \gF_{K' + 1}] | \gF_{K'}] \underbrace{=}_{Z \text{ is } \gF_{K'+1}~\text{ meas.}} \E[g(Z) ~\E[ f(X) | \gF_{K' + 1}] | \gF_{K'}]\\
    &  \underbrace{=}_{\text{assumption}}  \E[g(Z) ~\E[ f(X) | \gF_{K' }] | \gF_{K'}]  \underbrace{=}_{\E[ f(X) | \gF_{K' }]  ~ \gF_{K'}~\text{ meas.}} \E[g(Z) | \gF_{K'}]~\E[ f(X) | \gF_{K' }]
\end{align*}

The other dimensions $K'+2, \ldots, p$ follow by the fact that the filtrations are nested, meaning that the above derivation holds when $\mathcal{F}_{K'+1}$ is replaced by $\mathcal{F}_{K'+2}$, etc.
Thus, we have shown that the ``extra'' dimensions are independent of the data, given the relevant first $K'$ components.

\qed

\subsubsection{Discussion on Remark~\ref{remark:independence_exact}}%
\label{sec:proof_ind_exact}
We are considering the ``exactly-parameterizable'' manifold scenario.
We have shown in Proof~\ref{sec:proof_K_prime} that the $K$-th terms $L_K[e,d]$ are identically zero and thus the global minimum.


As in Proof~\ref{sec:proof_ind_relaxed}, we observe for the higher terms (e.g.,  $K+1$-th):
$$\mathbb{E}_X \mathbb{E}_{Y \sim P_{d^*, e_{1: K+1}(X)}}\left[\|X-Y\|^\beta\right]  > 0$$
and
$$\mathbb{E}_X \mathbb{E}_{Y, Y' \iid P_{d^*, e_{1: K+1}(X)}}\left[\|Y'-Y\|^\beta\right]  > 0$$
\textit{unless}
$$P_{d^*, e_{1: K+1}(X)} = \delta(X) = P_{d^*, e_{1: K}(X)}$$


In other words, the only zero variance distribution is the delta distribution, which is (by assumption) the distribution induced by $P_{d^*, e_{1: K}}$. Thus, the encoder that parameterizes the manifold in the $e_{1:K}$ dimensions and outputs the same distribution for terms $K+1:p$ is \textit{the} optimal encoder for the terms $K:p$. Denote this encoder as $e^*$ in the following argument.

Next, we argue that \textit{typically} for $p \gg K$, this is also the optimal encoder for the first $K-1$ dimensions, thus the $K$-best-approximating one, or equivalently, the global optimum.

Again, assume that all the weights $\omega_k \in [0,1]$ are uniform, i.e. $\frac{1}{p+1}$. As discussed in \citet{shen_distributional_2024}, it remains an open question whether an optimal encoder is necessarily the one that minimizes all the terms in the loss \textit{simultaneously} (which is the case for the terms $K:p$ when the encoder is the $K$-best-approximating one), so the following argument will examine what is \textit{likely} to happen for parameterizable manifolds as $p \gg K$.

Suppose that there is another, different (in the sense that it reconstructs different ORDs) globally optimal $(\tilde{e}, \tilde{d})$ pair that, by ``sacrificing'' perfect reconstruction at dimensions $K:p$, improves $L_k(e^*, d^*)$ for terms $k = 1, \ldots, K-1$ to such an extent as to strictly beat the manifold-parameterizing encoder $e^*$ in the aggregate loss. Note that for the latter, the ``partial'' manifold parameterization should \textit{typically} (i.e., for non-pathological manifolds) already be a reasonably good encoding for dimensions $k<K$ (in the energy-score sense), so $L_k[e^*, d^*]$ is \textit{unlikely} to be very far from the minimum for each of these $k$.

We will be using Proposition 1 (together with Theorem 1) of \citep{shen_distributional_2024}, which states that at the optimum, the two terms in the loss are equal and thus focusing on the reconstruction one (as we are conjecturing the existence of another global optimum).

Since the data manifold is $K$ dimensional, the encodings for $k < K$ cannot describe the manifold, leading to imperfect reconstruction:
$\| X - Y\|^\beta > 0$. 
This is true for any encoder/decoder pair, and we can denote the \textit{global} minimal loss when considering $k$ dimensions (i.e.. a single term in the optimization objective) as:
$$
R_{k, \min }:=\min_{e_{1: k}, d} L_k[e, d] > 0
$$

Furthermore, \textit{typically} for $k < K$ each encoding  has to ``describe'' $K-k$ additional ``directions'' of the manifold, meaning that  $P^*_{e_{1:k}, X} = (  X | \left.e_{1: k}(X)=z\right)$ becomes more spread out on the manifold as $k$ decreases; hence, \textit{typically}:
$$R_{k, \min } \geq R_{k', \min } ~ \text {for } k < k',$$
which implies
$$L_k [\tilde{e}, \tilde{d}] \geq  L_{k'} [\tilde{e}, \tilde{d}] ~ \text {for } k < k',$$


Suppose now that for some $S \subseteq\{1, \ldots, K-1\}$ this pair reduces the loss $L_k[\tilde{e}, \tilde{d}]$ below $L_k\left[e^*, d^*\right]$, 
which implies (by ``breaking'' the manifold parameterization):
$$L_k[\tilde{e}, \tilde{d}] > L_k[e^*, d^*] = 0, ~\forall k \geq K$$

Denote the hypothesized improvements (in absolute value) on the $k<K$ terms as $\Delta_k$, and the latter costs for $k \geq K$ as $\varepsilon_k$.
Thus we have the following best-case trade-off that needs to hold in order to $(\tilde{e}, \tilde{d})$ to be optimal:
$$(K-1) \cdot \max_k \Delta_k > (p - K) \min_k \varepsilon_k$$

$\varepsilon_k$ are nonzero, and the improvements $\Delta_k$ are bounded by $L_k[e^*, d^*] - R_{k, \min }$ for $k < K$. As discussed, \textit{typically}, in the latter term $L_k[e^*, d^*]$ are unlikely to be too large, and $R_{k, \min } > 0$. Thus, this trade-off is unlikely to be beneficial as $p \gg K$.


Thus, \textit{typically}, an encoder that parameterizes the manifold in the first $K$ dimensions is an optimal encoder. Then, due to Prop.~\ref{prop:exact_data_dist}, the results in Theorem~\ref{thm:independence_relaxed} follow.


\end{document}