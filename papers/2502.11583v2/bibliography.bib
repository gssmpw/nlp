
@misc{shen_distributional_2024,
	title = {Distributional {Principal} {Autoencoders}},
	url = {http://arxiv.org/abs/2404.13649},
	abstract = {Dimension reduction techniques usually lose information in the sense that reconstructed data are not identical to the original data. However, we argue that it is possible to have reconstructed data identically distributed as the original data, irrespective of the retained dimension or the specific mapping. This can be achieved by learning a distributional model that matches the conditional distribution of data given its low-dimensional latent variables. Motivated by this, we propose Distributional Principal Autoencoder (DPA) that consists of an encoder that maps high-dimensional data to low-dimensional latent variables and a decoder that maps the latent variables back to the data space. For reducing the dimension, the DPA encoder aims to minimise the unexplained variability of the data with an adaptive choice of the latent dimension. For reconstructing data, the DPA decoder aims to match the conditional distribution of all data that are mapped to a certain latent value, thus ensuring that the reconstructed data retains the original data distribution. Our numerical results on climate data, single-cell data, and image benchmarks demonstrate the practical feasibility and success of the approach in reconstructing the original distribution of the data. DPA embeddings are shown to preserve meaningful structures of data such as the seasonal cycle for precipitations and cell types for gene expression.},
	language = {en},
	urldate = {2024-06-22},
	publisher = {arXiv},
	author = {Shen, Xinwei and Meinshausen, Nicolai},
	month = apr,
	year = {2024},
	note = {arXiv:2404.13649 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, \_tablet, STAR},
	annote = {Extracted Annotations (6/28/2024, 11:33:37 AM)
"lose information in" (Shen and Meinshausen 2024:1)"irrespective of the retained dimension" (Shen and Meinshausen 2024:1)"adaptive choice of the latent dimension." (Shen and Meinshausen 2024:1)"of all data that are mapped to a certain latent value," (Shen and Meinshausen 2024:1)"WAE (Tolstikhin et al., 2018)" (Shen and Meinshausen 2024:1)"the conditional mean of all samples" (Shen and Meinshausen 2024:1)"stochastic component," (Shen and Meinshausen 2024:2)"reconstructed data in general follow a different distribution than the original data." (Shen and Meinshausen 2024:2)"vector" (Shen and Meinshausen 2024:2)"temporal mean precipitation per location." (Shen and Meinshausen 2024:2)"k ≤ p." (Shen and Meinshausen 2024:3)"distributional reconstruction" (Shen and Meinshausen 2024:3)"Rk ×Rl →Rp" (Shen and Meinshausen 2024:3)"an additional noise variable ε ∈Rl" (Shen and Meinshausen 2024:3)"second aim is to learn the 'principal' components," (Shen and Meinshausen 2024:3)"distributional criterion for lossless compression" (Shen and Meinshausen 2024:3)"variability among different DPA samples decreases as k grows larger as more information is retained in the embedding." (Shen and Meinshausen 2024:4)"mean squared reconstruction error regularised by the KL divergence between an isotropic Gaussian prior pz (z) and the variational posterior qe (z{\textbar}x) induced by the encoder:" (Shen and Meinshausen 2024:4)"aggregated posterior qe (z) :=E[qe (z{\textbar}X)]" (Shen and Meinshausen 2024:5)"generation," (Shen and Meinshausen 2024:5)"both of which require the latent space to be of the same dimension as the data space" (Shen and Meinshausen 2024:5)"proper scoring rules" (Shen and Meinshausen 2024:5)"(Shen and Meinshausen, 2023)." (Shen and Meinshausen 2024:5)"ordered latent representations" (Shen and Meinshausen 2024:5)"Nested Dropout" (Shen and Meinshausen 2024:5)"essential goal is still mean reconstruction," (Shen and Meinshausen 2024:5)"sample" (Shen and Meinshausen 2024:6)"retains as much information as possible" (Shen and Meinshausen 2024:6)"oracle reconstructed distribution," (Shen and Meinshausen 2024:6)"sense of minimising the variability in the oracle reconstructed distribution:" (Shen and Meinshausen 2024:6)"for an invertible function e," (Shen and Meinshausen 2024:6)"for a constant encoder e(x) ≡ c, P ∗,x is the same as the original data" (Shen and Meinshausen 2024:6)"is essentially the same ob jective as for the encoder in AE or PCA," (Shen and Meinshausen 2024:6)"its equivalence to minimising the reconstruction error," (Shen and Meinshausen 2024:6)"whereE is a pre-specified family of possible encoder functions," (Shen and Meinshausen 2024:6)"(1)" (Shen and Meinshausen 2024:7)"(Gaussian data and linear encoders)." (Shen and Meinshausen 2024:7)"(x) = ΠQ⊤k x, where Π ∈Rk×k" (Shen and Meinshausen 2024:7)"β = 2, (" (Shen and Meinshausen 2024:7)"distributional reconstruction" (Shen and Meinshausen 2024:7)"d(z, ε) ∼ P ∗,x for z = e(x)" (Shen and Meinshausen 2024:7)"lower dimensional manifolds" (Shen and Meinshausen 2024:7)"distributional decoder also enables optimisation for the encoder." (Shen and Meinshausen 2024:7)"oracle reconstructed distribution P ∗,X ." (Shen and Meinshausen 2024:8)"Theorem 1." (Shen and Meinshausen 2024:8)"fixed encoder e," (Shen and Meinshausen 2024:8)"Pd,z denotes the distribution of d(z, ε) for any z ∈Rk ." (Shen and Meinshausen 2024:8)"Then when taking λ = 1/2" (Shen and Meinshausen 2024:8)"we do not have access to the" (Shen and Meinshausen 2024:8)"enable sampling from P ∗,X ." (Shen and Meinshausen 2024:8)"distribution induced by a decoder." (Shen and Meinshausen 2024:8)"is the expected negative energy score between X and the distributional fit Pd,e(X)." (Shen and Meinshausen 2024:8)"optimal decoder d∗ such that d∗" (Shen and Meinshausen 2024:9)"unconditional generation problem where the decoder," (Shen and Meinshausen 2024:9)"in a way that induces an ordering." (Shen and Meinshausen 2024:9)"are left unconstrained for now." (Shen and Meinshausen 2024:9)"there exists an" (Shen and Meinshausen 2024:9)"argmin pX k=0 ωkEXEY,Y ′ iidP ∗1:k ,X ∥Y − Y ′∥" (Shen and Meinshausen 2024:10)"∗ (x) = Q⊤x, independent of the choice of weights." (Shen and Meinshausen 2024:10)"the ordering of the principal directions." (Shen and Meinshausen 2024:10)"can be minimised simultaneously." (Shen and Meinshausen 2024:10)"DPA for a varying latent dimension could also be helpful to guide a choice of the most appropriate latent dimension (" (Shen and Meinshausen 2024:10)"mini-batch of ε of" (Shen and Meinshausen 2024:11)"d by descending its gradients" (Shen and Meinshausen 2024:11)},
	file = {Shen and Meinshausen - 2024 - Distributional Principal Autoencoders.pdf:/Users/leban/myStuff/Zotero_Library/storage/PDTHG7AN/Shen and Meinshausen - 2024 - Distributional Principal Autoencoders.pdf:application/pdf},
}



@article{glielmo_unsupervised_2021,
	title = {Unsupervised {Learning} {Methods} for {Molecular} {Simulation} {Data}},
	volume = {121},
	issn = {0009-2665},
	url = {https://doi.org/10.1021/acs.chemrev.0c01195},
	doi = {10.1021/acs.chemrev.0c01195},
	abstract = {Unsupervised learning is becoming an essential tool to analyze the increasingly large amounts of data produced by atomistic and molecular simulations, in material science, solid state physics, biophysics, and biochemistry. In this Review, we provide a comprehensive overview of the methods of unsupervised learning that have been most commonly used to investigate simulation data and indicate likely directions for further developments in the field. In particular, we discuss feature representation of molecular systems and present state-of-the-art algorithms of dimensionality reduction, density estimation, and clustering, and kinetic models. We divide our discussion into self-contained sections, each discussing a specific method. In each section, we briefly touch upon the mathematical and algorithmic foundations of the method, highlight its strengths and limitations, and describe the specific ways in which it has been used-or can be used-to analyze molecular simulation data.},
	number = {16},
	urldate = {2024-05-11},
	journal = {Chemical Reviews},
	author = {Glielmo, Aldo and Husic, Brooke E. and Rodriguez, Alex and Clementi, Cecilia and Noé, Frank and Laio, Alessandro},
	month = aug,
	year = {2021},
	note = {Publisher: American Chemical Society},
	keywords = {\_tablet, Review, STAR},
	pages = {9722--9758},
	annote = {Annotations(5/12/2024, 1:42:21 PM)

},
	file = {Glielmo et al. - 2021 - Unsupervised Learning Methods for Molecular Simula.pdf:/Users/leban/myStuff/Zotero_Library/storage/ADX5C52J/Glielmo et al. - 2021 - Unsupervised Learning Methods for Molecular Simula.pdf:application/pdf},
}



@article{muller_location_1979,
	title = {Location of saddle points and minimum energy paths by a constrained simplex optimization procedure},
	volume = {53},
	issn = {1432-2234},
	url = {https://doi.org/10.1007/BF00547608},
	doi = {10.1007/BF00547608},
	abstract = {Two methods are proposed, one for the location of saddle points and one for the calculation of steepest-descent paths on multidimensional surfaces. Both methods are based on a constrained simplex optimization technique that avoids the evaluation of gradients or second derivative matrices. Three chemical reactions of increasing structural complexity are studied within the PRDDO SCF approximation. Predicted properties of reaction hypersurfaces are in good overall agreement with those determined by gradient minimization and gradient following algorithms in connection with various ab initio SCF methods. Computational efforts required by the new procedures are discussed.},
	language = {en},
	number = {1},
	urldate = {2024-09-24},
	journal = {Theoretica chimica acta},
	author = {Müller, Klaus and Brown, Leo D.},
	month = mar,
	year = {1979},
	keywords = {Constrained simplex optimization method for the calculation of steepest-descent paths on multidimensional surfaces, Constrained simplex optimization method for the location of saddle points},
	pages = {75--93},
	file = {Full Text PDF:/Users/leban/myStuff/Zotero_Library/storage/G6W6GA5K/Müller and Brown - 1979 - Location of saddle points and minimum energy paths.pdf:application/pdf},
}


@book{gelfand_calculus_2012,
	series = {Dover {Books} on {Mathematics}},
	title = {Calculus of {Variations}},
	isbn = {978-0-486-13501-4},
	url = {https://books.google.com/books?id=CeC7AQAAQBAJ},
	publisher = {Dover Publications},
	author = {Gelfand, I.M. and Fomin, S.V.},
	year = {2012},
	file = {Gelfand & Fomin - Calculus of Variations.pdf:/Users/leban/myStuff/Zotero_Library/storage/RCF8M8MM/Gelfand & Fomin - Calculus of Variations.pdf:application/pdf},
}




@book{murphy_probabilistic_2023,
	address = {Cambridge, Massachusetts London, England},
	series = {Adaptive computation and machine learning},
	title = {Probabilistic machine learning: advanced topics},
	isbn = {978-0-262-37600-6 978-0-262-04843-9},
	shorttitle = {Probabilistic machine learning},
	language = {en},
	publisher = {The MIT Press},
	author = {Murphy, Kevin P.},
	year = {2023},
	file = {Murphy - 2023 - Probabilistic machine learning advanced topics.pdf:/Users/leban/University of Michigan Dropbox/Andrej Leban/Literatura/ml/deep_learning/Murphy - 2023 - Probabilistic machine learning advanced topics.pdf:application/pdf},
}



@misc{shen_engression_2023,
	title = {Engression: Extrapolation for Nonlinear Regression?},
	url = {http://arxiv.org/abs/2307.00835},
	doi = {10.48550/arXiv.2307.00835},
        year = {2023},
	shorttitle = {Engression},
	abstract = {Extrapolation is crucial in many statistical and machine learning applications, as it is common to encounter test data outside the training support. However, extrapolation is a considerable challenge for nonlinear models. Conventional models typically struggle in this regard: while tree ensembles provide a constant prediction beyond the support, neural network predictions tend to become uncontrollable. This work aims at providing a nonlinear regression methodology whose reliability does not break down immediately at the boundary of the training support. Our primary contribution is a new method called `engression' which, at its core, is a distributional regression technique for pre-additive noise models, where the noise is added to the covariates before applying a nonlinear transformation. Our experimental results indicate that this model is typically suitable for many real data sets. We show that engression can successfully perform extrapolation under some assumptions such as a strictly monotone function class, whereas traditional regression approaches such as least-squares regression and quantile regression fall short under the same assumptions. We establish the advantages of engression over existing approaches in terms of extrapolation, showing that engression consistently provides a meaningful improvement. Our empirical results, from both simulated and real data, validate these findings, highlighting the effectiveness of the engression method. The software implementations of engression are available in both R and Python.},
	number = {{arXiv}:2307.00835},
	publisher = {{arXiv}},
	author = {Shen, Xinwei and Meinshausen, Nicolai},
	urldate = {2024-03-08},
	date = {2023-09-15},
	eprinttype = {arxiv},
	eprint = {2307.00835 [cs, stat]},
	keywords = {\_tablet, Computer Science - Machine Learning, {STAR}, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/leban/myStuff/Zotero_Library/storage/TRV3TH27/Shen and Meinshausen - 2023 - Engression Extrapolation for Nonlinear Regression.pdf:application/pdf;arXiv.org Snapshot:/Users/leban/myStuff/Zotero_Library/storage/CPP64A47/2307.html:text/html},
}

@misc{tolstikhin_wasserstein_2019,
	title = {Wasserstein Auto-Encoders},
	url = {http://arxiv.org/abs/1711.01558},
	abstract = {We propose the Wasserstein Auto-Encoder ({WAE})—a new algorithm for building a generative model of the data distribution. {WAE} minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a diﬀerent regularizer than the one used by the Variational Auto-Encoder ({VAE}) [1]. This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders ({AAE}) [2]. Our experiments show that {WAE} shares many of the properties of {VAEs} (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the {FID} score.},
	number = {{arXiv}:1711.01558},
	publisher = {{arXiv}},
	author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
	urldate = {2024-04-18},
	date = {2019-12-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1711.01558 [cs, stat]},
	keywords = {\_tablet, Computer Science - Machine Learning, {STAR}, Statistics - Machine Learning},
	file = {Tolstikhin et al. - 2019 - Wasserstein Auto-Encoders.pdf:/Users/leban/myStuff/Zotero_Library/storage/WEP63KMN/Tolstikhin et al. - 2019 - Wasserstein Auto-Encoders.pdf:application/pdf},
}


@book{baez_what_2024,
	title = {What is Entropy?},
	url = {https://arxiv.org/abs/2409.09232v1},
	abstract = {This short book is an elementary course on entropy, leading up to a calculation of the entropy of hydrogen gas at standard temperature and pressure. Topics covered include information, Shannon entropy and Gibbs entropy, the principle of maximum entropy, the Boltzmann distribution, temperature and coolness, the relation between entropy, expected energy and temperature, the equipartition theorem, the partition function, the relation between expected energy, free energy and entropy, the entropy of a classical harmonic oscillator, the entropy of a classical particle in a box, and the entropy of a classical ideal gas.},
	author = {Baez, John C.},
	urldate = {2024-09-18},
	date = {2024-09-13},
	langid = {english},
	keywords = {\_tablet},
	file = {Full Text PDF:/Users/leban/myStuff/Zotero_Library/storage/ASCHMAK9/Baez - 2024 - What is Entropy.pdf:application/pdf},
}

@article{nielsen_onicescus_2022,
	title = {Onicescu’s Informational Energy and Correlation Coefficient in Exponential Families},
	volume = {2},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2673-9321},
	url = {https://www.mdpi.com/2673-9321/2/2/25},
	doi = {10.3390/foundations2020025},
	abstract = {The informational energy of Onicescu is a positive quantity that measures the amount of uncertainty of a random variable. However, contrary to Shannon’s entropy, the informational energy is strictly convex and increases when randomness decreases. We report a closed-form formula for Onicescu’s informational energy and its associated correlation coefficient when the probability distributions belong to an exponential family. We show how to instantiate the generic formula for several common exponential families. Finally, we discuss the characterization of valid thermodynamic process trajectories on a statistical manifold by enforcing that the entropy and the informational energy shall vary in opposite directions.},
	pages = {362--376},
	number = {2},
	journaltitle = {Foundations},
	author = {Nielsen, Frank},
	urldate = {2024-10-08},
	date = {2022-06},
	langid = {english},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {\_tablet, cauchy-schwarz divergence, exponential families, location-scale families, Onicescu’s correlation coefficient, Onicescu’s informational energy, Rényi’s quadratic entropy, Shannon’s entropy, Simpson’s diversity index, thermodynamics process},
	file = {Full Text PDF:/Users/leban/myStuff/Zotero_Library/storage/M9KYYUSY/Nielsen - 2022 - Onicescu’s Informational Energy and Correlation Co.pdf:application/pdf},
}

@article{nielsen_many_2022,
	title = {The Many Faces of Information Geometry},
	volume = {69},
	issn = {0002-9920, 1088-9477},
	url = {https://www.ams.org/notices/202201/rnoti-p36.pdf},
	doi = {10.1090/noti2403},
	pages = {1},
	number = {1},
	journaltitle = {Notices of the American Mathematical Society},
	shortjournal = {Notices Amer. Math. Soc.},
	author = {Nielsen, Frank},
	urldate = {2024-10-08},
	date = {2022-01-01},
	langid = {english},
	keywords = {\_tablet, Review},
	file = {Nielsen - 2022 - The Many Faces of Information Geometry.pdf:/Users/leban/myStuff/Zotero_Library/storage/LV2YP9VB/Nielsen - 2022 - The Many Faces of Information Geometry.pdf:application/pdf},
}

@book{calin_geometric_2014,
	location = {Cham},
	title = {Geometric Modeling in Probability and Statistics},
	rights = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-319-07778-9 978-3-319-07779-6},
	url = {https://link.springer.com/10.1007/978-3-319-07779-6},
	publisher = {Springer International Publishing},
	author = {Calin, Ovidiu and Udrişte, Constantin},
	urldate = {2024-10-10},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-319-07779-6},
	keywords = {\_tablet},
	file = {Calin and Udrişte - 2014 - Geometric Modeling in Probability and Statistics.pdf:/Users/leban/myStuff/Zotero_Library/storage/PCFDD7CK/Calin and Udrişte - 2014 - Geometric Modeling in Probability and Statistics.pdf:application/pdf},
}

@book{santambrogio_course_2023,
	location = {Cham},
	title = {A Course in the Calculus of Variations: Optimization, Regularity, and Modeling},
	rights = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-45035-8 978-3-031-45036-5},
	url = {https://link.springer.com/10.1007/978-3-031-45036-5},
	series = {Universitext},
	shorttitle = {A Course in the Calculus of Variations},
	publisher = {Springer International Publishing},
	author = {Santambrogio, Filippo},
	urldate = {2024-10-10},
	date = {2023},
	langid = {english},
	doi = {10.1007/978-3-031-45036-5},
	keywords = {\_tablet, {STAR}},
	file = {Santambrogio - 2023 - A Course in the Calculus of Variations Optimizati.pdf:/Users/leban/myStuff/Zotero_Library/storage/D8KQTYMV/Santambrogio - 2023 - A Course in the Calculus of Variations Optimizati.pdf:application/pdf},
}

@incollection{diehl_optimization_2010,
	location = {Berlin, Heidelberg},
	title = {Optimization On Manifolds: Methods and Applications},
	isbn = {978-3-642-12597-3 978-3-642-12598-0},
	url = {https://link.springer.com/10.1007/978-3-642-12598-0_12},
	shorttitle = {Optimization On Manifolds},
	pages = {125--144},
	booktitle = {Recent Advances in Optimization and its Applications in Engineering},
	publisher = {Springer Berlin Heidelberg},
	author = {Absil, P.-A. and Mahony, R. and Sepulchre, R.},
	editor = {Diehl, Moritz and Glineur, Francois and Jarlebring, Elias and Michiels, Wim},
	urldate = {2024-10-30},
	date = {2010},
	langid = {english},
	doi = {10.1007/978-3-642-12598-0_12},
	keywords = {\_tablet},
	file = {Absil et al. - 2010 - Optimization On Manifolds Methods and Application.pdf:/Users/leban/myStuff/Zotero_Library/storage/RFPFUW9Z/Absil et al. - 2010 - Optimization On Manifolds Methods and Application.pdf:application/pdf},
}

@book{rockafellar_variational_1998,
	location = {Berlin, Heidelberg},
	title = {Variational Analysis},
	volume = {317},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-540-62772-2 978-3-642-02431-3},
	url = {http://link.springer.com/10.1007/978-3-642-02431-3},
	series = {Grundlehren der mathematischen Wissenschaften},
	publisher = {Springer Berlin Heidelberg},
	author = {Rockafellar, R. Tyrrell and Wets, Roger J. B.},
	editorb = {Berger, M. and De La Harpe, P. and Hirzebruch, F. and Hitchin, N. J. and Hörmander, L. and Kupiainen, A. and Lebeau, G. and Ratner, M. and Serre, D. and Sinai, Y. G. and Sloane, N. J. A. and Vershik, A. M. and Waldschmidt, M.},
	editorbtype = {redactor},
	urldate = {2024-10-30},
	date = {1998},
	langid = {english},
	doi = {10.1007/978-3-642-02431-3},
	keywords = {\_tablet, {STAR}},
	file = {Rockafellar and Wets - 1998 - Variational Analysis.pdf:/Users/leban/myStuff/Zotero_Library/storage/8B9HVKSC/Rockafellar and Wets - 1998 - Variational Analysis.pdf:application/pdf},
}

@article{alazar_variational_nodate,
	title = {{VARIATIONAL} {METHODS} {IN} {OPTIMIZATION}},
	abstract = {After a review of some well-known optimization problems, properties of vector spaces, and a close examination of functionals, a familiar approach to solving max and min problems is generalized from elementary calculus in order to ﬁnd solutions to more diﬃcult extremum problems. Using the Gateaux variation, a fundamental necessary condition for an extremum is established and applied. Optimization problems with one constraint are explored along with weak continuity of variations. Following a statement of the Euler-Lagrange Multiplier Theorem, more extremum problems are solved and then applications of the Euler-Lagrange Multiplier Theorem in the Calculus of Variations end the work.},
	author = {Alazar, Henok},
	langid = {english},
	keywords = {\_tablet},
	file = {Alazar - VARIATIONAL METHODS IN OPTIMIZATION.pdf:/Users/leban/myStuff/Zotero_Library/storage/G7G2BX7F/Alazar - VARIATIONAL METHODS IN OPTIMIZATION.pdf:application/pdf},
}

@online{noauthor_variational_2023,
	title = {Variational Optimization, and How It Simplifies Manifold Optimization (Part I: the continuous side of the story)},
	url = {https://itsdynamical.github.io/article/2023/06/01/variational-optimization-1.html},
	shorttitle = {Variational Optimization, and How It Simplifies Manifold Optimization (Part I},
	abstract = {{TL}; {DR} Gradient Descent ({GD}) is one of the most popular optimization algorithms for machine learning, and momentum is often used to accelerate its convergence. In this blog, we will start with a variational formulation of momentum {GD}, explore its rich connection to mechanics, and demonstrate how it allows natural generalizations of momentum {GD} to optimizing functions defined on manifolds.},
	titleaddon = {It’s dynamical},
	urldate = {2024-10-30},
	date = {2023-06-01},
	langid = {english},
	file = {Snapshot:/Users/leban/myStuff/Zotero_Library/storage/9HBD3M8V/variational-optimization-1.html:text/html},
}

@book{smith_variational_1974,
	title = {Variational methods in optimization},
	isbn = {978-0-13-940627-0},
	url = {http://archive.org/details/variationalmetho0000smit},
	abstract = {xv, 378 p. 25 cm; Includes bibliographical references},
	pagetotal = {410},
	publisher = {Englewood Cliffs, N.J., Prentice-Hall},
	author = {Smith, Donald R. (Donald Ray)},
	editora = {{Internet Archive}},
	editoratype = {collaborator},
	urldate = {2024-10-30},
	date = {1974},
	keywords = {\_tablet, Mathematical optimization, Calculus of variations},
	file = {Smith - 1974 - Variational methods in optimization.pdf:/Users/leban/myStuff/Zotero_Library/storage/TI9NPI72/Smith - 1974 - Variational methods in optimization.pdf:application/pdf},
}

@book{amari_information_2016,
	location = {Tokyo},
	title = {Information Geometry and Its Applications},
	volume = {194},
	rights = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-4-431-55977-1 978-4-431-55978-8},
	url = {https://link.springer.com/10.1007/978-4-431-55978-8},
	series = {Applied Mathematical Sciences},
	publisher = {Springer Japan},
	author = {Amari, Shun-ichi},
	urldate = {2024-10-30},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-4-431-55978-8},
	keywords = {\_tablet},
	file = {Amari - 2016 - Information Geometry and Its Applications.pdf:/Users/leban/myStuff/Zotero_Library/storage/IL4DQPEV/Amari - 2016 - Information Geometry and Its Applications.pdf:application/pdf},
}

@book{amari_methods_2007,
	location = {Providence, Rhode                     Island},
	title = {Methods of Information Geometry},
	volume = {191},
	isbn = {978-0-8218-4302-4 978-1-4704-4605-5},
	url = {http://www.ams.org/mmono/191},
	series = {Translations of Mathematical                         Monographs},
	publisher = {American Mathematical                     Society},
	author = {Amari, Shun-ichi and Nagaoka, Hiroshi},
	translator = {Harada, Daishi},
	urldate = {2024-10-30},
	date = {2007-04-13},
	langid = {english},
	doi = {10.1090/mmono/191},
	keywords = {\_tablet},
	file = {Amari and Nagaoka - 2007 - Methods of Information Geometry.pdf:/Users/leban/myStuff/Zotero_Library/storage/NLRFVY64/Amari and Nagaoka - 2007 - Methods of Information Geometry.pdf:application/pdf},
}

@book{evans_mathematical_nodate,
	title = {{MATHEMATICAL} {METHODS} {FOR} {OPTIMIZATION} Dynamic Optimization},
	author = {Evans, Lawrence C},
	langid = {english},
	keywords = {\_tablet},
	file = {Evans - MATHEMATICAL METHODS FOR OPTIMIZATION Dynamic Opti.pdf:/Users/leban/myStuff/Zotero_Library/storage/ELKQGZDY/Evans - MATHEMATICAL METHODS FOR OPTIMIZATION Dynamic Opti.pdf:application/pdf},
}

@misc{li_geometry_2024,
	title = {The Geometry of Concepts: Sparse Autoencoder Feature Structure},
	url = {http://arxiv.org/abs/2410.19750},
	shorttitle = {The Geometry of Concepts},
	abstract = {Sparse autoencoders have recently produced dictionaries of high-dimensional vectors corresponding to the universe of concepts represented by large language models. We find that this concept universe has interesting structure at three levels: 1) The "atomic" small-scale structure contains "crystals" whose faces are parallelograms or trapezoids, generalizing well-known examples such as (man-woman-king-queen). We find that the quality of such parallelograms and associated function vectors improves greatly when projecting out global distractor directions such as word length, which is efficiently done with linear discriminant analysis. 2) The "brain" intermediate-scale structure has significant spatial modularity; for example, math and code features form a "lobe" akin to functional lobes seen in neural {fMRI} images. We quantify the spatial locality of these lobes with multiple metrics and find that clusters of co-occurring features, at coarse enough scale, also cluster together spatially far more than one would expect if feature geometry were random. 3) The "galaxy" scale large-scale structure of the feature point cloud is not isotropic, but instead has a power law of eigenvalues with steepest slope in middle layers. We also quantify how the clustering entropy depends on the layer.},
	number = {{arXiv}:2410.19750},
	publisher = {{arXiv}},
	author = {Li, Yuxiao and Michaud, Eric J. and Baek, David D. and Engels, Joshua and Sun, Xiaoqing and Tegmark, Max},
	urldate = {2024-10-30},
	date = {2024-10-10},
	eprinttype = {arxiv},
	eprint = {2410.19750},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
	file = {Li et al. - 2024 - The Geometry of Concepts Sparse Autoencoder Featu.pdf:/Users/leban/myStuff/Zotero_Library/storage/QENVZVVW/Li et al. - 2024 - The Geometry of Concepts Sparse Autoencoder Featu.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/EXEUJCP9/2410.html:text/html},
}

@misc{su_disentangling_2024,
	title = {Disentangling Interpretable Factors with Supervised Independent Subspace Principal Component Analysis},
	url = {http://arxiv.org/abs/2410.23595},
	abstract = {The success of machine learning models relies heavily on effectively representing high-dimensional data. However, ensuring data representations capture humanunderstandable concepts remains difficult, often requiring the incorporation of prior knowledge and decomposition of data into multiple subspaces. Traditional linear methods fall short in modeling more than one space, while more expressive deep learning approaches lack interpretability. Here, we introduce Supervised Independent Subspace Principal Component Analysis ({sisPCA}), a {PCA} extension designed for multi-subspace learning. Leveraging the Hilbert-Schmidt Independence Criterion ({HSIC}), {sisPCA} incorporates supervision and simultaneously ensures subspace disentanglement. We demonstrate {sisPCA}’s connections with autoencoders and regularized linear regression and showcase its ability to identify and separate hidden data structures through extensive applications, including breast cancer diagnosis from image features, learning aging-associated {DNA} methylation changes, and single-cell analysis of malaria infection. Our results reveal distinct functional pathways associated with malaria colonization, underscoring the essentiality of explainable representation in high-dimensional data analysis.},
	number = {{arXiv}:2410.23595},
	publisher = {{arXiv}},
	author = {Su, Jiayu and Knowles, David A. and Rabadan, Raul},
	urldate = {2024-11-03},
	date = {2024-10-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2410.23595 [stat]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Genomics, Statistics - Machine Learning},
	file = {Su et al. - 2024 - Disentangling Interpretable Factors with Supervise.pdf:/Users/leban/myStuff/Zotero_Library/storage/MNFYCDM2/Su et al. - 2024 - Disentangling Interpretable Factors with Supervise.pdf:application/pdf},
}

@book{giaquinta_calculus_2004,
	location = {Berlin, Heidelberg},
	title = {Calculus of Variations I},
	volume = {310},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-642-08074-6 978-3-662-03278-7},
	url = {http://link.springer.com/10.1007/978-3-662-03278-7},
	series = {Grundlehren der mathematischen Wissenschaften},
	publisher = {Springer Berlin Heidelberg},
	author = {Giaquinta, Mariano and Hildebrandt, Stefan},
	editorb = {Chenciner, A. and Chern, S. S. and Eckmann, B. and De La Harpe, P. and Hirzebruch, F. and Hitchin, N. and Hörmander, L. and Knus, M.-A. and Kupiainen, A. and Lebeau, G. and Ratner, M. and Serre, D. and Sinai, Y. G. and Sloane, N. J. A. and Tits, J. and Totaro, B. and Vershik, A. and Waldschmidt, M. and Berger, M. and Coates, J. and Varadhan, S. R. S.},
	editorbtype = {redactor},
	urldate = {2024-11-21},
	date = {2004},
	langid = {english},
	doi = {10.1007/978-3-662-03278-7},
	keywords = {\_tablet, {STAR}},
	file = {Giaquinta and Hildebrandt - 2004 - Calculus of Variations I.pdf:/Users/leban/myStuff/Zotero_Library/storage/6LF9KWBK/Giaquinta and Hildebrandt - 2004 - Calculus of Variations I.pdf:application/pdf},
}

@book{giaquinta_calculus_2004-1,
	location = {Berlin, Heidelberg},
	title = {Calculus of Variations {II}},
	volume = {311},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-642-08192-7 978-3-662-06201-2},
	url = {http://link.springer.com/10.1007/978-3-662-06201-2},
	series = {Grundlehren der mathematischen Wissenschaften},
	publisher = {Springer Berlin Heidelberg},
	author = {Giaquinta, Mariano and Hildebrandt, Stefan},
	editorb = {Chenciner, A. and Chern, S. S. and Eckmann, B. and De La Harpe, P. and Hirzebruch, F. and Hitchin, N. and Hörmander, L. and Knus, M.-A. and Kupiainen, A. and Lebeau, G. and Ratner, M. and Serre, D. and Sinai, Y. G. and Sloane, N. J. A. and Tits, J. and Totaro, B. and Vershik, A. and Waldschmidt, M. and Berger, M. and Coates, J. and Varadhan, S. R. S.},
	editorbtype = {redactor},
	urldate = {2024-11-21},
	date = {2004},
	langid = {english},
	doi = {10.1007/978-3-662-06201-2},
	keywords = {\_tablet, {STAR}},
	file = {Giaquinta and Hildebrandt - 2004 - Calculus of Variations II.pdf:/Users/leban/myStuff/Zotero_Library/storage/L6NYFK8S/Giaquinta and Hildebrandt - 2004 - Calculus of Variations II.pdf:application/pdf},
}

@book{goldman_geometric_nodate,
	title = {Geometric structures on manifolds},
	abstract = {The study of locally homogeneous geometric structures on manifolds was initiated by Charles Ehresmann in 1936, who ﬁrst proposed the classiﬁcation of putting a “classical geometry” on a topological manifold. In the late 1970’s, locally homogeneous Riemannian structures on 3-manifolds formed the context for Bill Thurston’s Geometrization Conjecture, later proved by Perelman. This book develops the theory of geometric structures modeled on a homogeneous space of a Lie group, which are not necessarily Riemannian. Drawing on a diverse collection of techniques, we hope to invite researchers at all levels to this fascinating and currently extremely active area of mathematics.},
	author = {Goldman, William M},
	langid = {english},
	keywords = {\_tablet},
	file = {Goldman - Geometric structures on manifolds.pdf:/Users/leban/myStuff/Zotero_Library/storage/ISGNISP2/Goldman - Geometric structures on manifolds.pdf:application/pdf},
}

@book{spivak_calculus_2018,
	location = {Boca Raton London New York},
	title = {Calculus on manifolds: a modern approach to classical theorems of advanced calculus},
	isbn = {978-0-8053-9021-6 978-0-429-50190-6},
	series = {Mathematics monograph series},
	shorttitle = {Calculus on manifolds},
	pagetotal = {1},
	publisher = {{CRC} Press, Taylor \& Francis Group},
	author = {Spivak, Michael},
	date = {2018},
	langid = {english},
	keywords = {\_tablet},
	file = {Spivak - 2018 - Calculus on manifolds a modern approach to classi.pdf:/Users/leban/myStuff/Zotero_Library/storage/UJGBJZ2K/Spivak - 2018 - Calculus on manifolds a modern approach to classi.pdf:application/pdf},
}

@book{troutman_variational_1996,
	location = {New York, {NY}},
	title = {Variational Calculus and Optimal Control},
	rights = {http://www.springer.com/tdm},
	isbn = {978-1-4612-6887-1 978-1-4612-0737-5},
	url = {http://link.springer.com/10.1007/978-1-4612-0737-5},
	series = {Undergraduate Texts in Mathematics},
	publisher = {Springer New York},
	author = {Troutman, John L.},
	editorb = {Axler, Sheldon and Gehring, F. W. and Halmos, Paul R.},
	editorbtype = {redactor},
	urldate = {2024-11-21},
	date = {1996},
	langid = {english},
	doi = {10.1007/978-1-4612-0737-5},
	keywords = {\_tablet},
	file = {Troutman - 1996 - Variational Calculus and Optimal Control.pdf:/Users/leban/myStuff/Zotero_Library/storage/FYTBYMJ4/Troutman - 1996 - Variational Calculus and Optimal Control.pdf:application/pdf},
}

@book{luenberger_optimization_1997,
	title = {Optimization by Vector Space Methods},
	isbn = {978-0-471-18117-0},
	url = {https://books.google.com/books?id=M5n9DwAAQBAJ},
	series = {Professional Series},
	publisher = {Wiley},
	author = {Luenberger, D.G.},
	date = {1997},
	lccn = {68008716},
	keywords = {\_tablet},
	file = {Luenberger - 1997 - Optimization by Vector Space Methods.pdf:/Users/leban/myStuff/Zotero_Library/storage/H99XE7TH/Luenberger - 1997 - Optimization by Vector Space Methods.pdf:application/pdf},
}

@misc{alemi_fixing_2018,
	title = {Fixing a Broken {ELBO}},
	url = {http://arxiv.org/abs/1711.00464},
	doi = {10.48550/arXiv.1711.00464},
	abstract = {Recent work in unsupervised representation learning has focused on learning deep directed latent-variable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound ({ELBO}) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical {ELBO}, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.},
	number = {{arXiv}:1711.00464},
	publisher = {{arXiv}},
	author = {Alemi, Alexander A. and Poole, Ben and Fischer, Ian and Dillon, Joshua V. and Saurous, Rif A. and Murphy, Kevin},
	urldate = {2025-01-14},
	date = {2018-02-13},
	eprinttype = {arxiv},
	eprint = {1711.00464 [cs]},
	keywords = {\_tablet, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/leban/myStuff/Zotero_Library/storage/46S48PMC/Alemi et al. - 2018 - Fixing a Broken ELBO.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/BK74H9HT/1711.html:text/html},
}

@misc{blau_rethinking_2019,
	title = {Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff},
	url = {http://arxiv.org/abs/1901.07821},
	doi = {10.48550/arXiv.1901.07821},
	shorttitle = {Rethinking Lossy Compression},
	abstract = {Lossy compression algorithms are typically designed and analyzed through the lens of Shannon's rate-distortion theory, where the goal is to achieve the lowest possible distortion (e.g., low {MSE} or high {SSIM}) at any given bit rate. However, in recent years, it has become increasingly accepted that "low distortion" is not a synonym for "high perceptual quality", and in fact optimization of one often comes at the expense of the other. In light of this understanding, it is natural to seek for a generalization of rate-distortion theory which takes perceptual quality into account. In this paper, we adopt the mathematical definition of perceptual quality recently proposed by Blau \& Michaeli (2018), and use it to study the three-way tradeoff between rate, distortion, and perception. We show that restricting the perceptual quality to be high, generally leads to an elevation of the rate-distortion curve, thus necessitating a sacrifice in either rate or distortion. We prove several fundamental properties of this triple-tradeoff, calculate it in closed form for a Bernoulli source, and illustrate it visually on a toy {MNIST} example.},
	number = {{arXiv}:1901.07821},
	publisher = {{arXiv}},
	author = {Blau, Yochai and Michaeli, Tomer},
	urldate = {2025-01-14},
	date = {2019-07-30},
	eprinttype = {arxiv},
	eprint = {1901.07821 [cs]},
	keywords = {\_tablet, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Information Theory, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/leban/myStuff/Zotero_Library/storage/3J93BJBR/Blau and Michaeli - 2019 - Rethinking Lossy Compression The Rate-Distortion-.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/B3UYZK8K/1901.html:text/html},
}

@inproceedings{huang_evaluating_2020,
	title = {Evaluating Lossy Compression Rates of Deep Generative Models},
	url = {https://proceedings.mlr.press/v119/huang20c.html},
	abstract = {The field of deep generative modeling has succeeded in producing astonishingly realistic-seeming images and audio, but quantitative evaluation remains a challenge. Log-likelihood is an appealing metric due to its grounding in statistics and information theory, but it can be challenging to estimate for implicit generative models, and scalar-valued metrics give an incomplete picture of a model’s quality. In this work, we propose to use rate distortion ({RD}) curves to evaluate and compare deep generative models. While estimating {RD} curves is seemingly even more computationally demanding than log-likelihood estimation, we show that we can approximate the entire {RD} curve using nearly the same computations as were previously used to achieve a single log-likelihood estimate. We evaluate lossy compression rates of {VAEs}, {GANs}, and adversarial autoencoders ({AAEs}) on the {MNIST} and {CIFAR}10 datasets. Measuring the entire {RD} curve gives a more complete picture than scalar-valued metrics, and we arrive at a number of insights not obtainable from log-likelihoods alone.},
	eventtitle = {International Conference on Machine Learning},
	pages = {4444--4454},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Huang, Sicong and Makhzani, Alireza and Cao, Yanshuai and Grosse, Roger},
	urldate = {2025-01-14},
	date = {2020-11-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	keywords = {\_tablet},
	file = {Full Text PDF:/Users/leban/myStuff/Zotero_Library/storage/XLSCD5F4/Huang et al. - 2020 - Evaluating Lossy Compression Rates of Deep Generat.pdf:application/pdf;Supplementary PDF:/Users/leban/myStuff/Zotero_Library/storage/EJGMJ9P3/Huang et al. - 2020 - Evaluating Lossy Compression Rates of Deep Generat.pdf:application/pdf},
}

@article{shen_engression_2024,
	title = {Engression: extrapolation through the lens of distributional regression},
	issn = {1369-7412},
        year = {2024},
	url = {https://doi.org/10.1093/jrsssb/qkae108},
	doi = {10.1093/jrsssb/qkae108},
	shorttitle = {Engression},
	abstract = {Distributional regression aims to estimate the full conditional distribution of a target variable, given covariates. Popular methods include linear and tree ensemble based quantile regression. We propose a neural network-based distributional regression methodology called ‘engression’. An engression model is generative in the sense that we can sample from the fitted conditional distribution and is also suitable for high-dimensional outcomes. Furthermore, we find that modelling the conditional distribution on training data can constrain the fitted function outside of the training support, which offers a new perspective to the challenging extrapolation problem in nonlinear regression. In particular, for ‘preadditive noise’ models, where noise is added to the covariates before applying a nonlinear transformation, we show that engression can successfully perform extrapolation under some assumptions such as monotonicity, whereas traditional regression approaches such as least-squares or quantile regression fall short under the same assumptions. Our empirical results, from both simulated and real data, validate the effectiveness of the engression method. The software implementations of engression are available in both R and Python.},
	pages = {qkae108},
	journaltitle = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	shortjournal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Shen, Xinwei and Meinshausen, Nicolai},
	urldate = {2025-02-04},
	date = {2024-11-26},
	keywords = {\_tablet},
	file = {Full Text PDF:/Users/leban/myStuff/Zotero_Library/storage/6HSPG5UJ/Shen and Meinshausen - 2024 - Engression extrapolation through the lens of dist.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/PB7PQCK4/7909013.html:text/html},
}

@misc{bortoli_distributional_2025,
	title = {Distributional Diffusion Models with Scoring Rules},
	url = {http://arxiv.org/abs/2502.02483},
	doi = {10.48550/arXiv.2502.02483},
	abstract = {Diffusion models generate high-quality synthetic data. They operate by defining a continuous-time forward process which gradually adds Gaussian noise to data until fully corrupted. The corresponding reverse process progressively "denoises" a Gaussian sample into a sample from the data distribution. However, generating high-quality outputs requires many discretization steps to obtain a faithful approximation of the reverse process. This is expensive and has motivated the development of many acceleration methods. We propose to accomplish sample generation by learning the posterior \{{\textbackslash}em distribution\} of clean data samples given their noisy versions, instead of only the mean of this distribution. This allows us to sample from the probability transitions of the reverse process on a coarse time scale, significantly accelerating inference with minimal degradation of the quality of the output. This is accomplished by replacing the standard regression loss used to estimate conditional means with a scoring rule. We validate our method on image and robot trajectory generation, where we consistently outperform standard diffusion models at few discretization steps.},
	number = {{arXiv}:2502.02483},
	publisher = {{arXiv}},
	author = {Bortoli, Valentin De and Galashov, Alexandre and Guntupalli, J. Swaroop and Zhou, Guangyao and Murphy, Kevin and Gretton, Arthur and Doucet, Arnaud},
	urldate = {2025-02-06},
	date = {2025-02-04},
	eprinttype = {arxiv},
	eprint = {2502.02483 [cs]},
	keywords = {Computer Science - Machine Learning, {STAR}, Statistics - Machine Learning},
	file = {Bortoli et al. - 2025 - Distributional Diffusion Models with Scoring Rules.pdf:/Users/leban/myStuff/Zotero_Library/storage/AK3HWA26/Bortoli et al. - 2025 - Distributional Diffusion Models with Scoring Rules.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/7NJDUFIP/2502.html:text/html},
}

@article{gneiting_strictly_2007,
	title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
	volume = {102},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	pages = {359--378},
	number = {477},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	urldate = {2023-10-02},
	date = {2007-03},
	keywords = {\_tablet, {STAR}},
	file = {Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:/Users/leban/myStuff/Zotero_Library/storage/RAPSJYE4/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:application/pdf},
}




@article{bonati_unified_2023,
	title = {A unified framework for machine learning collective variables for enhanced sampling simulations: mlcolvar},
	volume = {159},
	issn = {0021-9606},
	url = {https://doi.org/10.1063/5.0156343},
	doi = {10.1063/5.0156343},
	shorttitle = {A unified framework for machine learning collective variables for enhanced sampling simulations},
	abstract = {Identifying a reduced set of collective variables is critical for understanding atomistic simulations and accelerating them through enhanced sampling techniques. Recently, several methods have been proposed to learn these variables directly from atomistic data. Depending on the type of data available, the learning process can be framed as dimensionality reduction, classification of metastable states, or identification of slow modes. Here, we present mlcolvar, a Python library that simplifies the construction of these variables and their use in the context of enhanced sampling through a contributed interface to the {PLUMED} software. The library is organized modularly to facilitate the extension and cross-contamination of these methodologies. In this spirit, we developed a general multi-task learning framework in which multiple objective functions and data from different simulations can be combined to improve the collective variables. The library’s versatility is demonstrated through simple examples that are prototypical of realistic scenarios.},
	pages = {014801},
	number = {1},
	journaltitle = {The Journal of Chemical Physics},
	shortjournal = {The Journal of Chemical Physics},
	author = {Bonati, Luigi and Trizio, Enrico and Rizzi, Andrea and Parrinello, Michele},
	urldate = {2024-07-09},
	date = {2023-07-06},
	keywords = {\_tablet, Review, {STAR}},
	file = {Bonati et al. - 2023 - A unified framework for machine learning collectiv.pdf:/Users/leban/myStuff/Zotero_Library/storage/EL9DUPG5/Bonati et al. - 2023 - A unified framework for machine learning collectiv.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/LSYA252S/A-unified-framework-for-machine-learning.html:text/html},
}

@article{mardt_vampnets_2018,
	title = {{VAMPnets} for deep learning of molecular kinetics},
	volume = {9},
	rights = {2018 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-017-02388-1},
	doi = {10.1038/s41467-017-02388-1},
	abstract = {There is an increasing demand for computing the relevant structures, equilibria, and long-timescale kinetics of biomolecular processes, such as protein-drug binding, from high-throughput molecular dynamics simulations. Current methods employ transformation of simulated coordinates into structural features, dimension reduction, clustering the dimension-reduced data, and estimation of a Markov state model or related model of the interconversion rates between molecular structures. This handcrafted approach demands a substantial amount of modeling expertise, as poor decisions at any step will lead to large modeling errors. Here we employ the variational approach for Markov processes ({VAMP}) to develop a deep learning framework for molecular kinetics using neural networks, dubbed {VAMPnets}. A {VAMPnet} encodes the entire mapping from molecular coordinates to Markov states, thus combining the whole data processing pipeline in a single end-to-end framework. Our method performs equally or better than state-of-the-art Markov modeling methods and provides easily interpretable few-state kinetic models.},
	pages = {5},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Mardt, Andreas and Pasquali, Luca and Wu, Hao and Noé, Frank},
	urldate = {2024-05-13},
	date = {2018-01-02},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {\_tablet, Machine learning, Molecular modelling, Theoretical chemistry, {STAR}},
	file = {Mardt et al. - 2018 - VAMPnets for deep learning of molecular kinetics.pdf:/Users/leban/myStuff/Zotero_Library/storage/JYCYF7K5/Mardt et al. - 2018 - VAMPnets for deep learning of molecular kinetics.pdf:application/pdf},
}



@article{chen_molecular_2018,
	title = {Molecular enhanced sampling with autoencoders: On-the-fly collective variable discovery and accelerated free energy landscape exploration},
	volume = {39},
	rights = {© 2018 Wiley Periodicals, Inc.},
	issn = {1096-987X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.25520},
	doi = {10.1002/jcc.25520},
	shorttitle = {Molecular enhanced sampling with autoencoders},
	abstract = {Macromolecular and biomolecular folding landscapes typically contain high free energy barriers that impede efficient sampling of configurational space by standard molecular dynamics simulation. Biased sampling can artificially drive the simulation along prespecified collective variables ({CVs}), but success depends critically on the availability of good {CVs} associated with the important collective dynamical motions. Nonlinear machine learning techniques can identify such {CVs} but typically do not furnish an explicit relationship with the atomic coordinates necessary to perform biased sampling. In this work, we employ auto-associative artificial neural networks (“autoencoders”) to learn nonlinear {CVs} that are explicit and differentiable functions of the atomic coordinates. Our approach offers substantial speedups in exploration of configurational space, and is distinguished from existing approaches by its capacity to simultaneously discover and directly accelerate along data-driven {CVs}. We demonstrate the approach in simulations of alanine dipeptide and Trp-cage, and have developed an open-source and freely available implementation within {OpenMM}. © 2018 Wiley Periodicals, Inc.},
	pages = {2079--2102},
	number = {25},
	journaltitle = {Journal of Computational Chemistry},
	author = {Chen, Wei and Ferguson, Andrew L.},
	urldate = {2024-05-13},
	date = {2018},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jcc.25520},
	keywords = {accelerated sampling, artificial neural networks, molecular dynamics simulation, nonlinear dimensionality reduction, protein folding, {STAR}},
	file = {Full Text PDF:/Users/leban/myStuff/Zotero_Library/storage/YVWY9QNT/Chen and Ferguson - 2018 - Molecular enhanced sampling with autoencoders On-.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/MG9AYYXH/jcc.html:text/html},
}

@misc{arts_two_2023,
	title = {Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics},
	url = {http://arxiv.org/abs/2302.00600},
	shorttitle = {Two for One},
	abstract = {Coarse-grained ({CG}) molecular dynamics enables the study of biological processes at temporal and spatial scales that would be intractable at an atomistic resolution. However, accurately learning a {CG} force field remains a challenge. In this work, we leverage connections between score-based generative models, force fields and molecular dynamics to learn a {CG} force field without requiring any force inputs during training. Specifically, we train a diffusion generative model on protein structures from molecular dynamics simulations, and we show that its score function approximates a force field that can directly be used to simulate {CG} molecular dynamics. While having a vastly simplified training setup compared to previous work, we demonstrate that our approach leads to improved performance across several protein simulations for systems up to 56 amino acids, reproducing the {CG} equilibrium distribution, and preserving dynamics of all-atom simulations such as protein folding events.},
	number = {{arXiv}:2302.00600},
	publisher = {{arXiv}},
	author = {Arts, Marloes and Satorras, Victor Garcia and Huang, Chin-Wei and Zuegner, Daniel and Federici, Marco and Clementi, Cecilia and Noé, Frank and Pinsler, Robert and Berg, Rianne van den},
	urldate = {2023-11-21},
	date = {2023-09-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2302.00600 [cs]},
	keywords = {\_tablet, Computer Science - Machine Learning, {STAR}},
	file = {Arts et al. - 2023 - Two for One Diffusion Models and Force Fields for.pdf:/Users/leban/myStuff/Zotero_Library/storage/6EP3MXQD/Arts et al. - 2023 - Two for One Diffusion Models and Force Fields for.pdf:application/pdf;arXiv.org Snapshot:/Users/leban/myStuff/Zotero_Library/storage/TPELMGIJ/2302.html:text/html},
}



@misc{song_score-based_2021,
	title = {Score-Based Generative Modeling through Stochastic Differential Equations},
	url = {http://arxiv.org/abs/2011.13456},
	doi = {10.48550/arXiv.2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation ({SDE}) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time {SDE} that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time {SDE} depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical {SDE} solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time {SDE}. We also derive an equivalent neural {ODE} that samples from the same distribution as the {SDE}, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on {CIFAR}-10 with an Inception score of 9.89 and {FID} of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
	number = {{arXiv}:2011.13456},
	publisher = {{arXiv}},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	urldate = {2023-09-20},
	date = {2021-02-10},
	eprinttype = {arxiv},
	eprint = {2011.13456 [cs, stat]},
	keywords = {\_tablet, Computer Science - Machine Learning, {STAR}, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/leban/myStuff/Zotero_Library/storage/IXCLAMGC/2011.html:text/html;Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf:/Users/leban/myStuff/Zotero_Library/storage/Z6Z2NAI4/Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf:application/pdf},
}



@article{bas_sampling_2011,
	title = {Sampling on locally defined principal manifolds},
	url = {http://ieeexplore.ieee.org/document/5946936/},
	doi = {10.1109/ICASSP.2011.5946936},
	abstract = {We start with a locally defined principal curve definition for a given probability density function (pdf) and define a pairwise manifold score based on local derivatives of the pdf. Proposed manifold score can be used to check if data pairs lie on the same manifold. We use this score to i) cluster nonlinear manifolds having irregular shapes, and ii) (down)sample a selected principal curve with sufficient accuracy sparsely. Our goal is to provide a heuristic-free formulation for principal graph generation and curve parametrization in order to form a basis for a principled principal manifold unwrapping method.},
	pages = {2276--2279},
	journaltitle = {2011 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Bas, Erhan and Erdogmus, Deniz},
	urldate = {2025-02-22},
	date = {2011-05},
	note = {Conference Name: {ICASSP} 2011 - 2011 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})
{ISBN}: 9781457705380
Place: Prague, Czech Republic
Publisher: {IEEE}},
}

@article{alain_regularized_2012,
	title = {Regularized Auto-Encoders Estimate Local Statistics},
	url = {https://www.semanticscholar.org/paper/Regularized-Auto-Encoders-Estimate-Local-Statistics-Alain-Bengio/57e17ce6e9a06aa8137ea355ba53073e3ffc7de6},
	abstract = {What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of the unknown data generating density. This paper clarifies some of these previous intuitive observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. More precisely, we show that the auto-encoder captures the score (derivative of the logdensity with respect to the input) or the local mean associated with the unknown data-generating density. This is the second result linking denoising auto-encoders and score matching, but in way that is different from previous work, and can be applied to the case when the auto-encoder reconstruction function does not necessarily correspond to the derivative of an energy function. The theorems provided here are completely generic and do not depend on the parametrization of the autoencoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood, i.e., one not involving a partition function. Finally, we make the connection to existing sampling algorithms for such autoencoders, based on an {MCMC} walking near the high-density manifold.},
	journaltitle = {{CoRR}},
	author = {Alain, Guillaume and Bengio, Yoshua and Rifai, Salah},
	urldate = {2025-02-22},
	date = {2012-11-18},
	file = {Alain et al. - 2012 - Regularized Auto-Encoders Estimate Local Statistic.pdf:/Users/leban/myStuff/Zotero_Library/storage/9NQU3JU7/Alain et al. - 2012 - Regularized Auto-Encoders Estimate Local Statistic.pdf:application/pdf},
}

@article{atzmon_isometric_2020,
	title = {Isometric Autoencoders},
	url = {https://www.semanticscholar.org/paper/Isometric-Autoencoders-Atzmon-Gropp/18ed38b4e0a95d9ad81ee9b827ee99ddcfe9e41c},
	abstract = {High dimensional data is often assumed to be concentrated on or near a low-dimensional manifold. Autoencoders ({AE}) is a popular technique to learn representations of such data by pushing it through a neural network with a low dimension bottleneck while minimizing a reconstruction error. Using high capacity {AE} often leads to a large collection of minimizers, many of which represent a low dimensional manifold that fits the data well but generalizes poorly. Two sources of bad generalization are: extrinsic, where the learned manifold possesses extraneous parts that are far from the data; and intrinsic, where the encoder and decoder introduce arbitrary distortion in the low dimensional parameterization. An approach taken to alleviate these issues is to add a regularizer that favors a particular solution; common regularizers promote sparsity, small derivatives, or robustness to noise. In this paper, we advocate an isometry (i.e., local distance preserving) regularizer. Specifically, our regularizer encourages: (i) the decoder to be an isometry; and (ii) the encoder to be the decoder's pseudo-inverse, that is, the encoder extends the inverse of the decoder to the ambient space by orthogonal projection. In a nutshell, (i) and (ii) fix both intrinsic and extrinsic degrees of freedom and provide a non-linear generalization to principal component analysis ({PCA}). Experimenting with the isometry regularizer on dimensionality reduction tasks produces useful low-dimensional data representations.},
	journaltitle = {{ArXiv}},
	author = {Atzmon, Matan and Gropp, Amos and Lipman, Y.},
	urldate = {2025-02-22},
	date = {2020-06-16},
	file = {Atzmon et al. - 2020 - Isometric Autoencoders.pdf:/Users/leban/myStuff/Zotero_Library/storage/8UVMER7R/Atzmon et al. - 2020 - Isometric Autoencoders.pdf:application/pdf},
}


@article{alain_what_2014,
	title = {What Regularized Auto-Encoders Learn from the Data-Generating Distribution},
	volume = {15},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v15/alain14a.html},
	abstract = {What do auto-encoders learn about the underlying data-generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data- generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parameterization of the auto-encoder: they show what the auto- encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings {MCMC} can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments.},
	pages = {3743--3773},
	number = {110},
	journaltitle = {Journal of Machine Learning Research},
	author = {Alain, Guillaume and Bengio, Yoshua},
	date = {2014},
	langid = {english},
	keywords = {Computer Science - Machine Learning, {STAR}, Statistics - Machine Learning},
	file = {Alain and Bengio - What Regularized Auto-Encoders Learn from the Data.pdf:/Users/leban/myStuff/Zotero_Library/storage/C7F9SMT8/Alain and Bengio - What Regularized Auto-Encoders Learn from the Data.pdf:application/pdf;Preprint PDF:/Users/leban/myStuff/Zotero_Library/storage/53NLYFZ9/Alain and Bengio - 2014 - What Regularized Auto-Encoders Learn from the Data.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/T5LMFPW9/1211.html:text/html},
}


@misc{bengio_implicit_2012,
	title = {Implicit Density Estimation by Local Moment Matching to Sample from Auto-Encoders},
	url = {http://arxiv.org/abs/1207.0057},
	doi = {10.48550/arXiv.1207.0057},
	abstract = {Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of the unknown data generating density. This paper contributes to the mathematical understanding of this phenomenon and helps define better justified sampling algorithms for deep learning based on auto-encoder variants. We consider an {MCMC} where each step samples from a Gaussian whose mean and covariance matrix depend on the previous state, defines through its asymptotic distribution a target density. First, we show that good choices (in the sense of consistency) for these mean and covariance functions are the local expected value and local covariance under that target density. Then we show that an auto-encoder with a contractive penalty captures estimators of these local moments in its reconstruction function and its Jacobian. A contribution of this work is thus a novel alternative to maximum-likelihood density estimation, which we call local moment matching. It also justifies a recently proposed sampling algorithm for the Contractive Auto-Encoder and extends it to the Denoising Auto-Encoder.},
	number = {{arXiv}:1207.0057},
	publisher = {{arXiv}},
	author = {Bengio, Yoshua and Alain, Guillaume and Rifai, Salah},
	urldate = {2025-02-22},
	date = {2012-06-30},
	eprinttype = {arxiv},
	eprint = {1207.0057 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, {STAR}},
	file = {Bengio et al. - 2012 - Implicit Density Estimation by Local Moment Matchi.pdf:/Users/leban/myStuff/Zotero_Library/storage/9CFTWC74/Bengio et al. - 2012 - Implicit Density Estimation by Local Moment Matchi.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/4P85DJLZ/1207.html:text/html},
}

@misc{shen_reverse_2025,
	title = {Reverse Markov Learning: Multi-Step Generative Models for Complex Distributions},
	url = {http://arxiv.org/abs/2502.13747},
	doi = {10.48550/arXiv.2502.13747},
	shorttitle = {Reverse Markov Learning},
	abstract = {Learning complex distributions is a fundamental challenge in contemporary applications. Generative models, such as diffusion models, have demonstrated remarkable success in overcoming many limitations of traditional statistical methods. Shen and Meinshausen (2024) introduced engression, a generative approach based on scoring rules that maps noise (and covariates, if available) directly to data. While effective, engression struggles with highly complex distributions, such as those encountered in image data. In this work, we extend engression to improve its capability in learning complex distributions. We propose a framework that defines a general forward process transitioning from the target distribution to a known distribution (e.g., Gaussian) and then learns a reverse Markov process using multiple engression models. This reverse process reconstructs the target distribution step by step. Our approach supports general forward processes, allows for dimension reduction, and naturally discretizes the generative process. As a special case, when using a diffusion-based forward process, our framework offers a method to discretize the training and inference of diffusion models efficiently. Empirical evaluations on simulated and climate data validate our theoretical insights, demonstrating the effectiveness of our approach in capturing complex distributions.},
	number = {{arXiv}:2502.13747},
	publisher = {{arXiv}},
	author = {Shen, Xinwei and Meinshausen, Nicolai and Zhang, Tong},
	urldate = {2025-02-23},
	date = {2025-02-19},
	eprinttype = {arxiv},
	eprint = {2502.13747 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, {STAR}},
	file = {Shen et al. - 2025 - Reverse Markov Learning Multi-Step Generative Mod.pdf:/Users/leban/myStuff/Zotero_Library/storage/WQV8P8TB/Shen et al. - 2025 - Reverse Markov Learning Multi-Step Generative Mod.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/N562TRIA/2502.html:text/html},
}

@misc{ozair_deep_2014,
	title = {Deep Directed Generative Autoencoders},
	url = {http://arxiv.org/abs/1410.0630},
	doi = {10.48550/arXiv.1410.0630},
	abstract = {For discrete data, the likelihood \$P(x)\$ can be rewritten exactly and parametrized into \$P(X = x) = P(X = x {\textbar} H = f(x)) P(H = f(x))\$ if \$P(X {\textbar} H)\$ has enough capacity to put no probability mass on any \$x'\$ for which \$f(x'){\textbackslash}neq f(x)\$, where \$f({\textbackslash}cdot)\$ is a deterministic discrete function. The log of the first factor gives rise to the log-likelihood reconstruction error of an autoencoder with \$f({\textbackslash}cdot)\$ as the encoder and \$P(X{\textbar}H)\$ as the (probabilistic) decoder. The log of the second term can be seen as a regularizer on the encoded activations \$h=f(x)\$, e.g., as in sparse autoencoders. Both encoder and decoder can be represented by a deep neural network and trained to maximize the average of the optimal log-likelihood \${\textbackslash}log p(x)\$. The objective is to learn an encoder \$f({\textbackslash}cdot)\$ that maps \$X\$ to \$f(X)\$ that has a much simpler distribution than \$X\$ itself, estimated by \$P(H)\$. This "flattens the manifold" or concentrates probability mass in a smaller number of (relevant) dimensions over which the distribution factorizes. Generating samples from the model is straightforward using ancestral sampling. One challenge is that regular back-propagation cannot be used to obtain the gradient on the parameters of the encoder, but we find that using the straight-through estimator works well here. We also find that although optimizing a single level of such architecture may be difficult, much better results can be obtained by pre-training and stacking them, gradually transforming the data distribution into one that is more easily captured by a simple parametric model.},
	number = {{arXiv}:1410.0630},
	publisher = {{arXiv}},
	author = {Ozair, Sherjil and Bengio, Yoshua},
	urldate = {2025-02-24},
	date = {2014-10-02},
	eprinttype = {arxiv},
	eprint = {1410.0630 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:/Users/leban/myStuff/Zotero_Library/storage/7VMXATY5/Ozair and Bengio - 2014 - Deep Directed Generative Autoencoders.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/Z8F98CIT/1410.html:text/html},
}

@misc{martino_are_2023,
	title = {Are We Using Autoencoders in a Wrong Way?},
	url = {http://arxiv.org/abs/2309.01532},
	doi = {10.48550/arXiv.2309.01532},
	abstract = {Autoencoders are certainly among the most studied and used Deep Learning models: the idea behind them is to train a model in order to reconstruct the same input data. The peculiarity of these models is to compress the information through a bottleneck, creating what is called Latent Space. Autoencoders are generally used for dimensionality reduction, anomaly detection and feature extraction. These models have been extensively studied and updated, given their high simplicity and power. Examples are (i) the Denoising Autoencoder, where the model is trained to reconstruct an image from a noisy one; (ii) Sparse Autoencoder, where the bottleneck is created by a regularization term in the loss function; (iii) Variational Autoencoder, where the latent space is used to generate new consistent data. In this article, we revisited the standard training for the undercomplete Autoencoder modifying the shape of the latent space without using any explicit regularization term in the loss function. We forced the model to reconstruct not the same observation in input, but another one sampled from the same class distribution. We also explored the behaviour of the latent space in the case of reconstruction of a random sample from the whole dataset.},
	number = {{arXiv}:2309.01532},
	publisher = {{arXiv}},
	author = {Martino, Gabriele and Moroni, Davide and Martinelli, Massimo},
	urldate = {2025-02-24},
	date = {2023-09-04},
	eprinttype = {arxiv},
	eprint = {2309.01532 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Martino et al. - 2023 - Are We Using Autoencoders in a Wrong Way.pdf:/Users/leban/myStuff/Zotero_Library/storage/2LXSSI4E/Martino et al. - 2023 - Are We Using Autoencoders in a Wrong Way.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/RVYMJZDC/2309.html:text/html},
}

@misc{giraldo_rate-distortion_2014,
	title = {Rate-Distortion Auto-Encoders},
	url = {http://arxiv.org/abs/1312.7381},
	doi = {10.48550/arXiv.1312.7381},
	abstract = {A rekindled the interest in auto-encoder algorithms has been spurred by recent work on deep learning. Current efforts have been directed towards effective training of auto-encoder architectures with a large number of coding units. Here, we propose a learning algorithm for auto-encoders based on a rate-distortion objective that minimizes the mutual information between the inputs and the outputs of the auto-encoder subject to a fidelity constraint. The goal is to learn a representation that is minimally committed to the input data, but that is rich enough to reconstruct the inputs up to certain level of distortion. Minimizing the mutual information acts as a regularization term whereas the fidelity constraint can be understood as a risk functional in the conventional statistical learning setting. The proposed algorithm uses a recently introduced measure of entropy based on infinitely divisible matrices that avoids the plug in estimation of densities. Experiments using over-complete bases show that the rate-distortion auto-encoders can learn a regularized input-output mapping in an implicit manner.},
	number = {{arXiv}:1312.7381},
	publisher = {{arXiv}},
	author = {Giraldo, Luis G. Sanchez and Principe, Jose C.},
	urldate = {2025-02-24},
	date = {2014-04-17},
	eprinttype = {arxiv},
	eprint = {1312.7381 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/leban/myStuff/Zotero_Library/storage/TT6QMWRD/Giraldo and Principe - 2014 - Rate-Distortion Auto-Encoders.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/U3DZA67A/1312.html:text/html},
}

@misc{creswell_improving_2017,
	title = {Improving Sampling from Generative Autoencoders with Markov Chains},
	url = {http://arxiv.org/abs/1610.09296},
	doi = {10.48550/arXiv.1610.09296},
	abstract = {We focus on generative autoencoders, such as variational or adversarial autoencoders, which jointly learn a generative model alongside an inference model. Generative autoencoders are those which are trained to softly enforce a prior on the latent distribution learned by the inference model. We call the distribution to which the inference model maps observed samples, the learned latent distribution, which may not be consistent with the prior. We formulate a Markov chain Monte Carlo ({MCMC}) sampling process, equivalent to iteratively decoding and encoding, which allows us to sample from the learned latent distribution. Since, the generative model learns to map from the learned latent distribution, rather than the prior, we may use {MCMC} to improve the quality of samples drawn from the generative model, especially when the learned latent distribution is far from the prior. Using {MCMC} sampling, we are able to reveal previously unseen differences between generative autoencoders trained either with or without a denoising criterion.},
	number = {{arXiv}:1610.09296},
	publisher = {{arXiv}},
	author = {Creswell, Antonia and Arulkumaran, Kai and Bharath, Anil Anthony},
	urldate = {2025-02-24},
	date = {2017-01-12},
	eprinttype = {arxiv},
	eprint = {1610.09296 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Creswell et al. - 2017 - Improving Sampling from Generative Autoencoders wi.pdf:/Users/leban/myStuff/Zotero_Library/storage/8L7VX3ED/Creswell et al. - 2017 - Improving Sampling from Generative Autoencoders wi.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/7H4T5L2K/1610.html:text/html},
}



@misc{albergo_stochastic_2023,
	title = {Stochastic {Interpolants}: {A} {Unifying} {Framework} for {Flows} and {Diffusions}},
	shorttitle = {Stochastic {Interpolants}},
	url = {http://arxiv.org/abs/2303.08797},
	doi = {10.48550/arXiv.2303.08797},
	abstract = {A class of generative models that unifies flow-based and diffusion-based methods is introduced. These models extend the framework proposed in Albergo \& Vanden-Eijnden (2023), enabling the use of a broad class of continuous-time stochastic processes called `stochastic interpolants' to bridge any two arbitrary probability density functions exactly in finite time. These interpolants are built by combining data from the two prescribed densities with an additional latent variable that shapes the bridge in a flexible way. The time-dependent probability density function of the stochastic interpolant is shown to satisfy a first-order transport equation as well as a family of forward and backward Fokker-Planck equations with tunable diffusion. Upon consideration of the time evolution of an individual sample, this viewpoint immediately leads to both deterministic and stochastic generative models based on probability flow equations or stochastic differential equations with an adjustable level of noise. The drift coefficients entering these models are time-dependent velocity fields characterized as the unique minimizers of simple quadratic objective functions, one of which is a new objective for the score of the interpolant density. Remarkably, we show that minimization of these quadratic objectives leads to control of the likelihood for any of our generative models built upon stochastic dynamics. By contrast, we establish that generative models based upon a deterministic dynamics must, in addition, control the Fisher divergence between the target and the model. We also construct estimators for the likelihood and the cross-entropy of interpolant-based generative models, discuss connections with other stochastic bridges, and demonstrate that such models recover the Schr{\textbackslash}"odinger bridge between the two target densities when explicitly optimizing over the interpolant.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Albergo, Michael S. and Boffi, Nicholas M. and Vanden-Eijnden, Eric},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08797 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, \_tablet, Mathematics - Probability, STAR},
	file = {Albergo et al. - 2023 - Stochastic Interpolants A Unifying Framework for .pdf:/Users/leban/myStuff/Zotero_Library/storage/TMRTNCPR/Albergo et al. - 2023 - Stochastic Interpolants A Unifying Framework for .pdf:application/pdf;arXiv.org Snapshot:/Users/leban/myStuff/Zotero_Library/storage/WI8P8A68/2303.html:text/html},
}

@misc{lipman_flow_2023,
	title = {Flow {Matching} for {Generative} {Modeling}},
	url = {http://arxiv.org/abs/2210.02747},
	doi = {10.48550/arXiv.2210.02747},
	abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
	urldate = {2024-03-01},
	publisher = {arXiv},
	author = {Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
	month = feb,
	year = {2023},
	note = {arXiv:2210.02747 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, \_tablet, STAR},
	file = {arXiv.org Snapshot:/Users/leban/myStuff/Zotero_Library/storage/MM9DLN99/2210.html:text/html;Lipman et al. - 2023 - Flow Matching for Generative Modeling.pdf:/Users/leban/myStuff/Zotero_Library/storage/T45MJB2R/Lipman et al. - 2023 - Flow Matching for Generative Modeling.pdf:application/pdf},
}



@misc{ozair_deep_2014,
	title = {Deep {Directed} {Generative} {Autoencoders}},
	url = {http://arxiv.org/abs/1410.0630},
	doi = {10.48550/arXiv.1410.0630},
	abstract = {For discrete data, the likelihood \$P(x)\$ can be rewritten exactly and parametrized into \$P(X = x) = P(X = x {\textbar} H = f(x)) P(H = f(x))\$ if \$P(X {\textbar} H)\$ has enough capacity to put no probability mass on any \$x'\$ for which \$f(x'){\textbackslash}neq f(x)\$, where \$f({\textbackslash}cdot)\$ is a deterministic discrete function. The log of the first factor gives rise to the log-likelihood reconstruction error of an autoencoder with \$f({\textbackslash}cdot)\$ as the encoder and \$P(X{\textbar}H)\$ as the (probabilistic) decoder. The log of the second term can be seen as a regularizer on the encoded activations \$h=f(x)\$, e.g., as in sparse autoencoders. Both encoder and decoder can be represented by a deep neural network and trained to maximize the average of the optimal log-likelihood \${\textbackslash}log p(x)\$. The objective is to learn an encoder \$f({\textbackslash}cdot)\$ that maps \$X\$ to \$f(X)\$ that has a much simpler distribution than \$X\$ itself, estimated by \$P(H)\$. This "flattens the manifold" or concentrates probability mass in a smaller number of (relevant) dimensions over which the distribution factorizes. Generating samples from the model is straightforward using ancestral sampling. One challenge is that regular back-propagation cannot be used to obtain the gradient on the parameters of the encoder, but we find that using the straight-through estimator works well here. We also find that although optimizing a single level of such architecture may be difficult, much better results can be obtained by pre-training and stacking them, gradually transforming the data distribution into one that is more easily captured by a simple parametric model.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Ozair, Sherjil and Bengio, Yoshua},
	month = oct,
	year = {2014},
	note = {arXiv:1410.0630 [stat]},
	keywords = {\_tablet, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/leban/myStuff/Zotero_Library/storage/7VMXATY5/Ozair and Bengio - 2014 - Deep Directed Generative Autoencoders.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/Z8F98CIT/1410.html:text/html},
}

@misc{martino_are_2023,
	title = {Are {We} {Using} {Autoencoders} in a {Wrong} {Way}?},
	url = {http://arxiv.org/abs/2309.01532},
	doi = {10.48550/arXiv.2309.01532},
	abstract = {Autoencoders are certainly among the most studied and used Deep Learning models: the idea behind them is to train a model in order to reconstruct the same input data. The peculiarity of these models is to compress the information through a bottleneck, creating what is called Latent Space. Autoencoders are generally used for dimensionality reduction, anomaly detection and feature extraction. These models have been extensively studied and updated, given their high simplicity and power. Examples are (i) the Denoising Autoencoder, where the model is trained to reconstruct an image from a noisy one; (ii) Sparse Autoencoder, where the bottleneck is created by a regularization term in the loss function; (iii) Variational Autoencoder, where the latent space is used to generate new consistent data. In this article, we revisited the standard training for the undercomplete Autoencoder modifying the shape of the latent space without using any explicit regularization term in the loss function. We forced the model to reconstruct not the same observation in input, but another one sampled from the same class distribution. We also explored the behaviour of the latent space in the case of reconstruction of a random sample from the whole dataset.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Martino, Gabriele and Moroni, Davide and Martinelli, Massimo},
	month = sep,
	year = {2023},
	note = {arXiv:2309.01532 [cs]},
	keywords = {\_tablet, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Martino et al. - 2023 - Are We Using Autoencoders in a Wrong Way.pdf:/Users/leban/myStuff/Zotero_Library/storage/2LXSSI4E/Martino et al. - 2023 - Are We Using Autoencoders in a Wrong Way.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/RVYMJZDC/2309.html:text/html},
}

@misc{giraldo_rate-distortion_2014,
	title = {Rate-{Distortion} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1312.7381},
	doi = {10.48550/arXiv.1312.7381},
	abstract = {A rekindled the interest in auto-encoder algorithms has been spurred by recent work on deep learning. Current efforts have been directed towards effective training of auto-encoder architectures with a large number of coding units. Here, we propose a learning algorithm for auto-encoders based on a rate-distortion objective that minimizes the mutual information between the inputs and the outputs of the auto-encoder subject to a fidelity constraint. The goal is to learn a representation that is minimally committed to the input data, but that is rich enough to reconstruct the inputs up to certain level of distortion. Minimizing the mutual information acts as a regularization term whereas the fidelity constraint can be understood as a risk functional in the conventional statistical learning setting. The proposed algorithm uses a recently introduced measure of entropy based on infinitely divisible matrices that avoids the plug in estimation of densities. Experiments using over-complete bases show that the rate-distortion auto-encoders can learn a regularized input-output mapping in an implicit manner.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Giraldo, Luis G. Sanchez and Principe, Jose C.},
	month = apr,
	year = {2014},
	note = {arXiv:1312.7381 [cs]},
	keywords = {\_tablet, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/leban/myStuff/Zotero_Library/storage/TT6QMWRD/Giraldo and Principe - 2014 - Rate-Distortion Auto-Encoders.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/U3DZA67A/1312.html:text/html},
}

@misc{creswell_improving_2017,
	title = {Improving {Sampling} from {Generative} {Autoencoders} with {Markov} {Chains}},
	url = {http://arxiv.org/abs/1610.09296},
	doi = {10.48550/arXiv.1610.09296},
	abstract = {We focus on generative autoencoders, such as variational or adversarial autoencoders, which jointly learn a generative model alongside an inference model. Generative autoencoders are those which are trained to softly enforce a prior on the latent distribution learned by the inference model. We call the distribution to which the inference model maps observed samples, the learned latent distribution, which may not be consistent with the prior. We formulate a Markov chain Monte Carlo (MCMC) sampling process, equivalent to iteratively decoding and encoding, which allows us to sample from the learned latent distribution. Since, the generative model learns to map from the learned latent distribution, rather than the prior, we may use MCMC to improve the quality of samples drawn from the generative model, especially when the learned latent distribution is far from the prior. Using MCMC sampling, we are able to reveal previously unseen differences between generative autoencoders trained either with or without a denoising criterion.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Creswell, Antonia and Arulkumaran, Kai and Bharath, Anil Anthony},
	month = jan,
	year = {2017},
	note = {arXiv:1610.09296 [cs]},
	keywords = {\_tablet, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Creswell et al. - 2017 - Improving Sampling from Generative Autoencoders wi.pdf:/Users/leban/myStuff/Zotero_Library/storage/8L7VX3ED/Creswell et al. - 2017 - Improving Sampling from Generative Autoencoders wi.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/7H4T5L2K/1610.html:text/html},
}

@article{meila_manifold_2024,
	title = {Manifold {Learning}: {What}, {How}, and {Why}},
	volume = {11},
	issn = {2326-8298, 2326-831X},
	shorttitle = {Manifold {Learning}},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-040522-115238},
	doi = {10.1146/annurev-statistics-040522-115238},
	abstract = {Manifold learning (ML), also known as nonlinear dimension reduction, is a set of methods to find the low-dimensional structure of data. Dimension reduction for large, high-dimensional data is not merely a way to reduce the data; the new representations and descriptors obtained by ML reveal the geometric shape of high-dimensional point clouds and allow one to visualize, denoise, and interpret them. This review presents the underlying principles of ML, its representative methods, and their statistical foundations, all from a practicing statistician\&apos;s perspective. It describes the trade-offs and what theory tells us about the parameter and algorithmic choices we make in order to obtain reliable conclusions.},
	language = {en},
	number = {Volume 11, 2024},
	urldate = {2025-03-10},
	journal = {Annual Review of Statistics and Its Application},
	author = {Meilă, Marina and Zhang, Hanyu},
	month = apr,
	year = {2024},
	note = {Publisher: Annual Reviews},
	keywords = {\_tablet, Review, STAR},
	pages = {393--417},
	file = {Snapshot:/Users/leban/myStuff/Zotero_Library/storage/F2RE5NAI/annurev-statistics-040522-115238.html:text/html;Submitted Version:/Users/leban/myStuff/Zotero_Library/storage/DR865HNY/Meilă and Zhang - 2024 - Manifold Learning What, How, and Why.pdf:application/pdf},
}


@article{swersky_autoencoders_2011,
	title = {On {Autoencoders} and {Score} {Matching} for {Energy} {Based} {Models}},
	abstract = {We consider estimation methods for the class of continuous-data energy based models (EBMs). Our main result shows that estimating the parameters of an EBM using score matching when the conditional distribution over the visible units is Gaussian corresponds to training a particular form of regularized autoencoder. We show how diﬀerent Gaussian EBMs lead to diﬀerent autoencoder architectures, providing deep links between these two families of models. We compare the score matching estimator for the mPoT model, a particular Gaussian EBM, to several other training methods on a variety of tasks including image denoising and unsupervised feature extraction. We show that the regularization function induced by score matching leads to superior classiﬁcation performance relative to a standard autoencoder. We also show that score matching yields classiﬁcation results that are indistinguishable from better-known stochastic approximation maximum likelihood estimators.},
	language = {en},
	author = {Swersky, Kevin and Ranzato, Marc’Aurelio and Buchman, David and Marlin, Benjamin M and de Freitas, Nando},
	year = {2011},

}

@article{kamyshanska_autoencoder_nodate,
	title = {On autoencoder scoring},
	abstract = {Autoencoders are popular feature learning models because they are conceptually simple, easy to train and allow for eﬃcient inference and training. Recent work has shown how certain autoencoders can assign an unnormalized “score” to data which measures how well the autoencoder can represent the data. Scores are commonly computed by using training criteria that relate the autoencoder to a probabilistic model, such as the Restricted Boltzmann Machine. In this paper we show how an autoencoder can assign meaningful scores to data independently of training procedure and without reference to any probabilistic model, by interpreting it as a dynamical system. We discuss how, and under which conditions, running the dynamical system can be viewed as performing gradient descent in an energy function, which in turn allows us to derive a score via integration. We also show how one can combine multiple, unnormalized scores into a generative classiﬁer.},
	language = {en},
	author = {Kamyshanska, Hanna and Memisevic, Roland},
	keywords = {\_tablet, STAR},
	file = {Kamyshanska and Memisevic - On autoencoder scoring.pdf:/Users/leban/myStuff/Zotero_Library/storage/9KBMN2JB/Kamyshanska and Memisevic - On autoencoder scoring.pdf:application/pdf},
}

@article{lim_ar-dae_nodate,
	title = {{AR}-{DAE}: {Towards} {Unbiased} {Neural} {Entropy} {Gradient} {Estimation}},
	abstract = {Entropy is ubiquitous in machine learning, but it is in general intractable to compute the entropy of the distribution of an arbitrary continuous random variable. In this paper, we propose the amortized residual denoising autoencoder (AR-DAE) to approximate the gradient of the log density function, which can be used to estimate the gradient of entropy. Amortization allows us to signiﬁcantly reduce the error of the gradient approximator by approaching asymptotic optimality of a regular DAE, in which case the estimation is in theory unbiased. We conduct theoretical and experimental analyses on the approximation error of the proposed method, as well as extensive studies on heuristics to ensure its robustness. Finally, using the proposed gradient approximator to estimate the gradient of entropy, we demonstrate state-ofthe-art performance on density estimation with variational autoencoders and continuous control with soft actor-critic.},
	language = {en},
	author = {Lim, Jae Hyun and Courville, Aaron and Pal, Christopher and Huang, Chin-Wei},
	keywords = {\_tablet},
	file = {Lim et al. - AR-DAE Towards Unbiased Neural Entropy Gradient E.pdf:/Users/leban/myStuff/Zotero_Library/storage/MUVDUU2C/Lim et al. - AR-DAE Towards Unbiased Neural Entropy Gradient E.pdf:application/pdf},
}

@article{im_conservativeness_2016,
	title = {Conservativeness of {Untied} {Auto}-{Encoders}},
	volume = {30},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10268},
	doi = {10.1609/aaai.v30i1.10268},
	abstract = {We discuss necessary and sufﬁcient conditions for an autoencoder to deﬁne a conservative vector ﬁeld, in which case it is associated with an energy function akin to the unnormalized log-probability of the data. We show that the conditions for conservativeness are more general than for encoder and decoder weights to be the same (“tied weights”), and that they also depend on the form of the hidden unit activation functions. Moreover, we show that contractive training criteria, such as denoising, enforces these conditions locally. Based on these observations, we show how we can use auto-encoders to extract the conservative component of a vector ﬁeld.},
	language = {en},
	number = {1},
	urldate = {2025-03-15},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Im, Daniel and Belghazi, Mohamed and Memisevic, Roland},
	month = feb,
	year = {2016},
	keywords = {\_tablet},
	file = {Im et al. - 2016 - Conservativeness of Untied Auto-Encoders.pdf:/Users/leban/myStuff/Zotero_Library/storage/ARHLB4CA/Im et al. - 2016 - Conservativeness of Untied Auto-Encoders.pdf:application/pdf},
}

@article{lee_convergence_nodate,
	title = {Convergence of score-based generative modeling for general data distributions},
	abstract = {Score-based generative modeling (SGM) has grown to be a hugely successful method for learning to generate samples from complex data distributions such as that of images and audio. It is based on evolving an SDE that transforms white noise into a sample from the learned distribution, using estimates of the score function, or gradient log-pdf. Previous convergence analyses for these methods have suffered either from strong assumptions on the data distribution or exponential dependencies, and hence fail to give efficient guarantees for the multimodal and non-smooth distributions that arise in practice and for which good empirical performance is observed. We consider a popular kind of SGM—denoising diffusion models—and give polynomial convergence guarantees for general data distributions, with no assumptions related to functional inequalities or smoothness. Assuming L2-accurate score estimates, we obtain Wasserstein distance guarantees for any distribution of bounded support or sufficiently decaying tails, as well as TV guarantees for distributions with further smoothness assumptions.},
	language = {en},
	author = {Lee, Holden and Lu, Jianfeng and Tan, Yixin},
	keywords = {\_tablet},
	file = {Lee et al. - Convergence of score-based generative modeling for.pdf:/Users/leban/myStuff/Zotero_Library/storage/69PDUSDE/Lee et al. - Convergence of score-based generative modeling for.pdf:application/pdf},
}

@inproceedings{cole_score-based_2023,
	title = {Score-based generative models break the curse of dimensionality in learning a family of sub-{Gaussian} distributions},
	url = {https://openreview.net/forum?id=wG12xUSqrI#:~:text=introduce%20a%20measure%20of%20complexity,which%20is%20interesting%20in%20its},
	abstract = {While score-based generative models (SGMs) have achieved remarkable successes in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a measure of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep network approximation rate for the true score function associated to the forward process, which is interesting in its own right.},
	language = {en},
	urldate = {2025-03-15},
	author = {Cole, Frank and Lu, Yulong},
	month = oct,
	year = {2023},
	keywords = {\_tablet},
	file = {Cole and Lu - 2023 - Score-based generative models break the curse of d.pdf:/Users/leban/myStuff/Zotero_Library/storage/8QNC6B77/Cole and Lu - 2023 - Score-based generative models break the curse of d.pdf:application/pdf},
}

@misc{zhu_sample_2023,
	title = {Sample {Complexity} {Bounds} for {Score}-{Matching}: {Causal} {Discovery} and {Generative} {Modeling}},
	shorttitle = {Sample {Complexity} {Bounds} for {Score}-{Matching}},
	url = {http://arxiv.org/abs/2310.18123},
	doi = {10.48550/arXiv.2310.18123},
	abstract = {This paper provides statistical sample complexity bounds for score-matching and its applications in causal discovery. We demonstrate that accurate estimation of the score function is achievable by training a standard deep ReLU neural network using stochastic gradient descent. We establish bounds on the error rate of recovering causal relationships using the score-matching-based causal discovery method of Rolland et al. [2022], assuming a sufﬁciently good estimation of the score function. Finally, we analyze the upper bound of score-matching estimation within the score-based generative modeling, which has been applied for causal discovery but is also of independent interest within the domain of generative models.},
	language = {en},
	urldate = {2025-03-15},
	publisher = {arXiv},
	author = {Zhu, Zhenyu and Locatello, Francesco and Cevher, Volkan},
	month = oct,
	year = {2023},
	note = {arXiv:2310.18123 [cs]},
	keywords = {\_tablet, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zhu et al. - 2023 - Sample Complexity Bounds for Score-Matching Causa.pdf:/Users/leban/myStuff/Zotero_Library/storage/CBXVI6NE/Zhu et al. - 2023 - Sample Complexity Bounds for Score-Matching Causa.pdf:application/pdf},
}


@article{bengio_generalized_2013,
	title = {Generalized {Denoising} {Auto}-{Encoders} as {Generative} {Models}},
	url = {http://arxiv.org/abs/1305.6663},
	doi = {10.48550/arXiv.1305.6663},
	abstract = {Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying datagenerating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justiﬁcation which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-inﬁnitesimal corruption noise (or non-inﬁnitesimal contractive penalty).},
	language = {en},
	author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
	month = nov,
	year = {2013},
	note = {arXiv:1305.6663 [cs]},
	keywords = {\_tablet, Computer Science - Machine Learning, STAR},
	annote = {Contents1 Introduction2 Generalizing Denoising Auto-Encoders2.1 Definition and Training2.2 Sampling2.3 Consistency2.4 Locality of the Corruption and Energy Function3 Reducing the Spurious Modes with Walkback Training4 Experimental Validation5 Conclusion and Future Work},
	file = {Bengio et al. - Generalized Denoising Auto-Encoders as Generative .pdf:/Users/leban/myStuff/Zotero_Library/storage/MFHEUJ4Z/Bengio et al. - Generalized Denoising Auto-Encoders as Generative .pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/XCVYKTPE/1305.html:text/html},
}


@article{alain_regularized_2012,
	title = {Regularized {Auto}-{Encoders} {Estimate} {Local} {Statistics}},
	url = {https://www.semanticscholar.org/paper/Regularized-Auto-Encoders-Estimate-Local-Statistics-Alain-Bengio/57e17ce6e9a06aa8137ea355ba53073e3ffc7de6},
	abstract = {What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of the unknown data generating density. This paper clarifies some of these previous intuitive observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. More precisely, we show that the auto-encoder captures the score (derivative of the logdensity with respect to the input) or the local mean associated with the unknown data-generating density. This is the second result linking denoising auto-encoders and score matching, but in way that is different from previous work, and can be applied to the case when the auto-encoder reconstruction function does not necessarily correspond to the derivative of an energy function. The theorems provided here are completely generic and do not depend on the parametrization of the autoencoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood, i.e., one not involving a partition function. Finally, we make the connection to existing sampling algorithms for such autoencoders, based on an MCMC walking near the high-density manifold.},
	urldate = {2025-02-22},
	journal = {CoRR},
	author = {Alain, Guillaume and Bengio, Yoshua and Rifai, Salah},
	month = nov,
	year = {2012},
}



@article{vincent_connection_2011,
	title = {A {Connection} {Between} {Score} {Matching} and {Denoising} {Autoencoders}},
	volume = {23},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/NECO_a_00142},
	doi = {10.1162/NECO_a_00142},
	abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.},
	number = {7},
	urldate = {2024-02-02},
	journal = {Neural Computation},
	author = {Vincent, Pascal},
	month = jul,
	year = {2011},
	keywords = {STAR, \_tablet},
	pages = {1661--1674},
	annote = {(hidden zotfile data)},
	annote = {(hidden zotfile data)},
	
}




@article{braunsmann_convergent_2024,
	title = {Convergent autoencoder approximation of low bending and low distortion manifold embeddings},
	volume = {58},
	copyright = {© The authors. Published by EDP Sciences, SMAI 2024},
	issn = {2822-7840, 2804-7214},
	url = {https://www.esaim-m2an.org/articles/m2an/abs/2024/01/m2an220261/m2an220261.html},
	doi = {10.1051/m2an/2023088},
	abstract = {Autoencoders are widely used in machine learning for dimension reduction of high-dimensional data. The encoder embeds the input data manifold into a lower-dimensional latent space, while the decoder represents the inverse map, providing a parametrization of the data manifold by the manifold in latent space. We propose and analyze a novel regularization for learning the encoder component of an autoencoder: a loss functional that prefers isometric, extrinsically flat embeddings and allows to train the encoder on its own. To perform the training, it is assumed that the local Riemannian distance and the local Riemannian average can be evaluated for pairs of nearby points on the input manifold. The loss functional is computed {\textless}i{\textgreater}via{\textless}i/{\textgreater} Monte Carlo integration. Our main theorem identifies a geometric loss functional of the embedding map as the Γ-limit of the sampling-dependent loss functionals. Numerical tests, using image data that encodes different explicitly given data manifolds, show that smooth manifold embeddings into latent space are obtained. Furthermore, due to the promotion of extrinsic flatness, interpolation between not too distant points on the manifold is well approximated by linear interpolation in latent space.},
	language = {en},
	number = {1},
	urldate = {2025-04-09},
	journal = {ESAIM: Mathematical Modelling and Numerical Analysis},
	author = {Braunsmann, Juliane and Rajković, Marko and Rumpf, Martin and Wirth, Benedikt},
	month = jan,
	year = {2024},
	note = {Number: 1
Publisher: EDP Sciences},
	pages = {335--361},
	file = {Braunsmann et al. - 2024 - Convergent autoencoder approximation of low bendin.pdf:/Users/leban/myStuff/Zotero_Library/storage/63ATWWQW/Braunsmann et al. - 2024 - Convergent autoencoder approximation of low bendin.pdf:application/pdf},
}



@misc{liu_deep_2023,
	title = {Deep {Nonparametric} {Estimation} of {Intrinsic} {Data} {Structures} by {Chart} {Autoencoders}: {Generalization} {Error} and {Robustness}},
	shorttitle = {Deep {Nonparametric} {Estimation} of {Intrinsic} {Data} {Structures} by {Chart} {Autoencoders}},
	url = {http://arxiv.org/abs/2303.09863},
	doi = {10.48550/arXiv.2303.09863},
	abstract = {Autoencoders have demonstrated remarkable success in learning low-dimensional latent features of high-dimensional data across various applications. Assuming that data are sampled near a low-dimensional manifold, we employ chart autoencoders, which encode data into low-dimensional latent features on a collection of charts, preserving the topology and geometry of the data manifold. Our paper establishes statistical guarantees on the generalization error of chart autoencoders, and we demonstrate their denoising capabilities by considering \$n\$ noisy training samples, along with their noise-free counterparts, on a \$d\$-dimensional manifold. By training autoencoders, we show that chart autoencoders can effectively denoise the input data with normal noise. We prove that, under proper network architectures, chart autoencoders achieve a squared generalization error in the order of \${\textbackslash}displaystyle n{\textasciicircum}\{-{\textbackslash}frac\{2\}\{d+2\}\}{\textbackslash}log{\textasciicircum}4 n\$, which depends on the intrinsic dimension of the manifold and only weakly depends on the ambient dimension and noise level. We further extend our theory on data with noise containing both normal and tangential components, where chart autoencoders still exhibit a denoising effect for the normal component. As a special case, our theory also applies to classical autoencoders, as long as the data manifold has a global parametrization. Our results provide a solid theoretical foundation for the effectiveness of autoencoders, which is further validated through several numerical experiments.},
	urldate = {2025-04-09},
	publisher = {arXiv},
	author = {Liu, Hao and Havrilla, Alex and Lai, Rongjie and Liao, Wenjing},
	month = oct,
	year = {2023},
	note = {arXiv:2303.09863 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Liu et al. - 2023 - Deep Nonparametric Estimation of Intrinsic Data St.pdf:/Users/leban/myStuff/Zotero_Library/storage/MUA9L88U/Liu et al. - 2023 - Deep Nonparametric Estimation of Intrinsic Data St.pdf:application/pdf;Snapshot:/Users/leban/myStuff/Zotero_Library/storage/T2TA82SX/2303.html:text/html},
}


@inproceedings{zheng_learning_2022,
	title = {Learning {Manifold} {Dimensions} with {Conditional} {Variational} {Autoencoders}},
	url = {https://openreview.net/forum?id=Lvlxq_H96lI#:~:text=as%20is%20likely%20the%20case,world%20datasets},
	abstract = {Although the variational autoencoder (VAE) and its conditional extension (CVAE) are capable of state-of-the-art results across multiple domains, their precise behavior is still not fully understood, particularly in the context of data (like images) that lie on or near a low-dimensional manifold. For example, while prior work has suggested that the globally optimal VAE solution can learn the correct manifold dimension, a necessary (but not sufficient) condition for producing samples from the true data distribution, this has never been rigorously proven. Moreover, it remains unclear how such considerations would change when various types of conditioning variables are introduced, or when the data support is extended to a union of manifolds (e.g., as is likely the case for MNIST digits and related). In this work, we address these points by first proving that VAE global minima are indeed capable of recovering the correct manifold dimension. We then extend this result to more general CVAEs, demonstrating practical scenarios whereby the conditioning variables allow the model to adaptively learn manifolds of varying dimension across samples. Our analyses, which have practical implications for various CVAE design choices, are also supported by numerical results on both synthetic and real-world datasets.},
	language = {en},
	urldate = {2025-04-09},
	author = {Zheng, Yijia and He, Tong and Qiu, Yixuan and Wipf, David},
	month = oct,
	year = {2022},
	keywords = {\_tablet, STAR},
	file = {Zheng et al. - 2022 - Learning Manifold Dimensions with Conditional Vari.pdf:/Users/leban/myStuff/Zotero_Library/storage/MXVS74P2/Zheng et al. - 2022 - Learning Manifold Dimensions with Conditional Vari.pdf:application/pdf},
}

@misc{schonsheck_chart_2020,
	title = {Chart {Auto}-{Encoders} for {Manifold} {Structured} {Data}},
	url = {http://arxiv.org/abs/1912.10094},
	doi = {10.48550/arXiv.1912.10094},
	abstract = {Deep generative models have made tremendous advances in image and signal representation learning and generation. These models employ the full Euclidean space or a bounded subset as the latent space, whose flat geometry, however, is often too simplistic to meaningfully reflect the manifold structure of the data. In this work, we advocate the use of a multi-chart latent space for better data representation. Inspired by differential geometry, we propose a {\textbackslash}textbf\{Chart Auto-Encoder (CAE)\} and prove a universal approximation theorem on its representation capability. We show that the training data size and the network size scale exponentially in approximation error with an exponent depending on the intrinsic dimension of the data manifold. CAE admits desirable manifold properties that auto-encoders with a flat latent space fail to obey, predominantly proximity of data. We conduct extensive experimentation with synthetic and real-life examples to demonstrate that CAE provides reconstruction with high fidelity, preserves proximity in the latent space, and generates new data remaining near the manifold. These experiments show that CAE is advantageous over existing auto-encoders and variants by preserving the topology of the data manifold as well as its geometry.},
	urldate = {2025-04-09},
	publisher = {arXiv},
	author = {Schonsheck, Stefan and Chen, Jie and Lai, Rongjie},
	month = aug,
	year = {2020},
	note = {arXiv:1912.10094 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 22 pages, 12 figures},
	file = {Schonsheck et al. - 2020 - Chart Auto-Encoders for Manifold Structured Data.pdf:/Users/leban/myStuff/Zotero_Library/storage/SDQAIVRH/Schonsheck et al. - 2020 - Chart Auto-Encoders for Manifold Structured Data.pdf:application/pdf},
}


@misc{kim_disentangling_2019,
	title = {Disentangling by {Factorising}},
	url = {http://arxiv.org/abs/1802.05983},
	doi = {10.48550/arXiv.1802.05983},
	abstract = {We deﬁne and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon β-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
	language = {en},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Kim, Hyunjik and Mnih, Andriy},
	month = jul,
	year = {2019},
	note = {arXiv:1802.05983 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {FactorVAE

},
	file = {Kim and Mnih - 2019 - Disentangling by Factorising.pdf:/Users/leban/myStuff/Zotero_Library/storage/L3BY5C4E/Kim and Mnih - 2019 - Disentangling by Factorising.pdf:application/pdf},
}

@inproceedings{higgins_beta-vae_2016,
	title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
	shorttitle = {beta-{VAE}},
	url = {https://www.semanticscholar.org/paper/beta-VAE%3A-Learning-Basic-Visual-Concepts-with-a-Higgins-Matthey/a90226c41b79f8b06007609f39f82757073641e2},
	abstract = {an},
	urldate = {2023-09-28},
	author = {Higgins, I. and Matthey, L. and Pal, Arka and Burgess, Christopher P. and Glorot, Xavier and Botvinick, M. and Mohamed, S. and Lerchner, Alexander},
	month = nov,
	year = {2016},
	file = {Higgins et al. - 2016 - beta-VAE Learning Basic Visual Concepts with a Co.pdf:/Users/leban/myStuff/Zotero_Library/storage/C8AT2GF9/Higgins et al. - 2016 - beta-VAE Learning Basic Visual Concepts with a Co.pdf:application/pdf},
}
