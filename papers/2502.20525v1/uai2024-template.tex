% \documentclass{uai2024} % for initial submission
\documentclass[accepted]{uai2024} % after acceptance, for a revised version; 
% also before submission to see how the non-anonymous paper would look like 
                        
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2024} % ptmx math instead of Computer

                                         % Modern (has noticeable issues)
% \documentclass[mathfont=newtx]{uai2024} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams

\usepackage{hyperref}       % hyperlinks

\hypersetup{
colorlinks   = true, %Colours links instead of ugly boxes
urlcolor     = blue, %Colour for external hyperlinks
linkcolor    = blue, %Colour of internal links
citecolor    = blue %Colour of citations
}

% \hypersetup{
% colorlinks   = true, %Colours links instead of ugly boxes
% urlcolor     = blue, %Colour for external hyperlinks
% linkcolor    = blue, %Colour of internal links
% citecolor    = blue %Colour of citations
% }

\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath}  
\usepackage{multirow}

\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}     
\usepackage{amsthm}     
% blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{tabularx}

\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

\usepackage{titletoc}

\newcommand\DoToC{%
  \startcontents
  \printcontents{}{1}{\textbf{Table of Contents}\vskip3pt\hrule\vskip5pt}
  \vskip3pt\hrule\vskip5pt
}

% For math	
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}

% customized commands
\DeclareMathOperator{\argmax}{\arg\max}
\DeclareMathOperator{\argmin}{\arg\min}
\DeclareMathOperator*{\minimize}{\text{minimize}}
\DeclareMathOperator*{\maximize}{\text{maximize}}
\DeclareMathOperator*{\st}{\text{subject to}}

% customized theorem environment
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

% \usepackage[labelsep=period]{caption}
% \captionsetup[table]{name=TABLE}
\renewcommand{\thetable}{\Roman{table}}
\renewcommand{\thefigure}{\Roman{figure}}
\usepackage{caption}

\usepackage{wrapfig}

\usepackage{color}
\definecolor{lightgray}{gray}{0.75}
\usepackage{tcolorbox}

%\usepackage{algorithm}
%\usepackage{algpseudocode}

\usepackage{caption}

\usepackage{titlesec}
\titlespacing\section{0pt}{0pt plus 0pt minus 2pt}{0pt plus 0pt minus 2pt}
\titlespacing\subsection{0pt}{0pt plus 0pt minus 2pt}{0pt plus 0pt minus 2pt}
\titlespacing\subsubsection{0pt}{0pt plus 0pt minus 2pt}{0pt plus 0pt minus 2pt}
% \usepackage[nodisplayskipstretch]{setspace}
% \setstretch{1.5}

% \AtBeginDocument{%
%   \addtolength\abovedisplayskip{-0.3\baselineskip}%
%   \addtolength\belowdisplayskip{-0.3\baselineskip}%

\AtBeginDocument{%
  \addtolength\abovedisplayskip{-0.3\baselineskip}%
  \addtolength\belowdisplayskip{-0.3\baselineskip}%
 \addtolength\abovedisplayshortskip{-0.3\baselineskip}%
 \addtolength\belowdisplayshortskip{-0.3\baselineskip}%
}


%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example

\title{Revisiting Kernel Attention with Correlated Gaussian Process Representation}

% The standard author block has changed for UAI 2024 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors
\author[1,3]{\href{mailto:<minhlongbui2000@gmail.com>?Subject=Your UAI 2024 paper}{Long Minh Bui}{}}
\author[1,2]{Tho Tran Huu}
\author[1]{Duy Dinh}
\author[2,*]{Tan Minh Nguyen}
\author[3,*]{Trong Nghia Hoang}
% Add affiliations after the authors
\affil[1]{%
    FPT Software AI Center
}
\affil[2]{%
    Department of Mathematics,
    National University of Singapore    
}
\affil[3]{%
    School of Electrical Engineering and Computer Science, Washington State University
}
\affil[*]{Co-last author}
% \usepackage{fancyhdr}
% \pagestyle{fancy}
% \fancyfoot[C]{40th Conference on Uncertainty in Artificial Intelligence (UAI 2024).}

\begin{document}
\maketitle
\begin{abstract}
    Transformers have increasingly become the de facto method to model sequential data with state-of-the-art performance. Due to its widespread use, being able to estimate and calibrate its modeling uncertainty is important to understand and design robust transformer models. To achieve this, previous works have used Gaussian processes (GPs) to perform uncertainty calibration for the attention units of transformers and attained notable successes. However, such approaches have to confine the transformers to the space of symmetric attention to ensure the necessary symmetric requirement of their GP's kernel specification, which reduces the representation capacity of the model. To mitigate this restriction, we propose the Correlated Gaussian Process Transformer (CGPT), a new class of transformers whose self-attention units are modeled as cross-covariance between two correlated GPs (CGPs). This allows asymmetries in attention and can enhance the representation capacity of GP-based transformers. We also derive a sparse approximation for CGP to make it scale better. Our empirical studies show that both CGP-based and sparse CGP-based transformers achieve better performance than state-of-the-art GP-based transformers on a variety of benchmark tasks. The code for our experiments is available at \url{https://github.com/MinhLong210/CGP-Transformers}.
\end{abstract}

\section{Introduction}
\label{sec:intro}
\input{intro}

\section{Preliminaries}
\label{sec:background}
\input{background}

\section{Revisiting Kernel Attention}
\label{sec:method}
\input{method}

\section{Experimental Results}
\label{sec:experiments}
\input{experiments}

% \section{Empirical Analysis}
% \label{sec:analysis}
% \input{gptransformers/sections/analysis}

\section{Related Work}
\label{sec:related_work}
\input{related_work}

\section{Concluding Remarks}
\label{sec:conclusion}
\input{conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \bibliography{gptransformers/references}
%\bibliographystyle{plain}

\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Al-Rfou et~al.(2019)Al-Rfou, Choe, Constant, Guo, and
  Jones]{al2019character}
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones.
\newblock Character-level language modeling with deeper self-attention.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3159--3166, 2019.

\bibitem[Arnab et~al.(2021)Arnab, Dehghani, Heigold, Sun, Lučić, and
  Schmid]{9710415}
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and
  Cordelia Schmid.
\newblock Vivit: A video vision transformer.
\newblock In \emph{2021 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 6816--6826, 2021.
\newblock \doi{10.1109/ICCV48922.2021.00676}.

\bibitem[Baevski and Auli(2019)]{baevski2018adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ByxZX20qFQ}.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Bradshaw et~al.(2017)Bradshaw, Matthews, and
  Ghahramani]{bradshaw2017adversarial}
John Bradshaw, Alexander G de~G Matthews, and Zoubin Ghahramani.
\newblock Adversarial examples, uncertainty, and transfer testing robustness in
  gaussian process hybrid deep networks.
\newblock \emph{arXiv preprint arXiv:1707.02476}, 2017.

\bibitem[Brown and et~al.(2020)]{NEURIPS2020_1457c0d6}
Tom Brown and et~al.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 1877--1901, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chan et~al.(2021)Chan, Rottmann, and Gottschalk]{chan2021entropy}
Robin Chan, Matthias Rottmann, and Hanno Gottschalk.
\newblock Entropy maximization and meta classification for out-of-distribution
  detection in semantic segmentation.
\newblock In \emph{Proceedings of the ieee/cvf international conference on
  computer vision}, pages 5128--5137, 2021.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 15084--15097, 2021.

\bibitem[Chen and Li(2023)]{chen2023calibrating}
Wenlong Chen and Yingzhen Li.
\newblock Calibrating transformers via sparse gaussian processes.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=jPVAFXHlbL}.

\bibitem[Chen et~al.(2024)Chen, Tao, Tonin, and Suykens]{chen2024primal}
Yingyi Chen, Qinghua Tao, Francesco Tonin, and Johan Suykens.
\newblock Primal-attention: Self-attention through asymmetric kernel svd in
  primal representation.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Chien and Ku(2015)]{chien2015bayesian}
Jen-Tzung Chien and Yuan-Chu Ku.
\newblock Bayesian recurrent neural network for language modeling.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  27\penalty0 (2):\penalty0 361--374, 2015.

\bibitem[Cho et~al.(2014)Cho, van Merri{\"e}nboer, Gulcehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{cho-etal-2014-learning}
Kyunghyun Cho, Bart van Merri{\"e}nboer, Caglar Gulcehre, Dzmitry Bahdanau,
  Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using {RNN} encoder{--}decoder for
  statistical machine translation.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing ({EMNLP})}, pages 1724--1734, Doha, Qatar,
  October 2014. Association for Computational Linguistics.
\newblock \doi{10.3115/v1/D14-1179}.
\newblock URL \url{https://www.aclweb.org/anthology/D14-1179}.

\bibitem[Cinquin et~al.(2021)Cinquin, Immer, Horn, and
  Fortuin]{cinquin2021pathologies}
Tristan Cinquin, Alexander Immer, Max Horn, and Vincent Fortuin.
\newblock Pathologies in priors and inference for bayesian transformers.
\newblock \emph{arXiv preprint arXiv:2110.04020}, 2021.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, 2019.

\bibitem[Dehghani et~al.(2019)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz
  Kaiser.
\newblock Universal transformers.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HyzdRiR9Y7}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Fan et~al.(2020)Fan, Zhang, Chen, and Zhou]{fan2020bayesian}
Xinjie Fan, Shujian Zhang, Bo~Chen, and Mingyuan Zhou.
\newblock Bayesian attention modules.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 16362--16376, 2020.

\bibitem[Guo et~al.(2021)Guo, Cai, Liu, Mu, Martin, and Hu]{guo2021pct}
Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph~R Martin, and
  Shi-Min Hu.
\newblock Pct: Point cloud transformer.
\newblock \emph{Computational Visual Media}, 7\penalty0 (2):\penalty0 187--199,
  2021.

\bibitem[Gustafsson et~al.(2020)Gustafsson, Danelljan, and
  Schon]{gustafsson2020evaluating}
Fredrik~K Gustafsson, Martin Danelljan, and Thomas~B Schon.
\newblock Evaluating scalable bayesian deep learning methods for robust
  computer vision.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition workshops}, pages 318--319, 2020.

\bibitem[Hamelijnck et~al.(2021)Hamelijnck, Wilkinson, Loppi, Solin, and
  Damoulas]{hamelijnck2021spatio}
Oliver Hamelijnck, William Wilkinson, Niki Loppi, Arno Solin, and Theodoros
  Damoulas.
\newblock Spatio-temporal variational gaussian processes.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 23621--23633, 2021.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{arXiv preprint arXiv:1903.12261}, 2019.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016baseline}
Dan Hendrycks and Kevin Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock \emph{arXiv preprint arXiv:1610.02136}, 2016.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Basart, Mazeika, Zou, Kwon,
  Mostajabi, Steinhardt, and Song]{hendrycks2019scaling}
Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza
  Mostajabi, Jacob Steinhardt, and Dawn Song.
\newblock Scaling out-of-distribution detection for real-world settings.
\newblock \emph{arXiv preprint arXiv:1911.11132}, 2019.

\bibitem[Hensman et~al.(2013)Hensman, Fusi, and Lawrence]{Hensman13}
J.~Hensman, N.~Fusi, and N.~D. Lawrence.
\newblock Gaussian processes for big data.
\newblock In \emph{Proc. UAI}, pages 282--290, 2013.

\bibitem[Hoang et~al.(2017)Hoang, Hoang, and Low]{NghiaAAAI17}
Q.~M. Hoang, T.~N. Hoang, and K.~H. Low.
\newblock A generalized stochastic variational {B}ayesian hyperparameter
  learning framework for sparse spectrum {G}aussian process regression.
\newblock In \emph{Proc. {AAAI}}, pages 2007--2014, 2017.

\bibitem[Hoang et~al.(2015)Hoang, Hoang, and Low]{NghiaICML15}
T.~N. Hoang, Q.~M. Hoang, and K.~H. Low.
\newblock A unifying framework of anytime sparse {Gaussian} process regression
  models with stochastic variational inference for big data.
\newblock In \emph{Proc. {ICML}}, pages 569--578, 2015.

\bibitem[Hoang et~al.(2016)Hoang, Hoang, and Low]{NghiaICML16}
T.~N. Hoang, Q.~M. Hoang, and K.~H. Low.
\newblock A distributed variational inference framework for unifying parallel
  sparse {G}aussian process regression models.
\newblock In \emph{Proc. {ICML}}, pages 382--391, 2016.

\bibitem[Hoang et~al.(2019)Hoang, Hoang, Low, and How]{NghiaAAAI19}
T.~N. Hoang, Q.~M. Hoang, K.~H. Low, and J.~P. How.
\newblock Collective online learning of {G}aussian processes in massive
  multi-agent systems.
\newblock In \emph{Proc. {AAAI}}, 2019.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{janner2021offline}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 1273--1286, 2021.

\bibitem[Kendall and Gal(2017)]{kendall2017uncertainties}
Alex Kendall and Yarin Gal.
\newblock What uncertainties do we need in bayesian deep learning for computer
  vision?
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Kim et~al.(2017)Kim, Denton, Hoang, and Rush]{kim2017structured}
Yoon Kim, Carl Denton, Luong Hoang, and Alexander~M Rush.
\newblock Structured attention networks.
\newblock \emph{arXiv preprint arXiv:1702.00887}, 2017.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Nair, and
  Hinton]{krizhevsky2009cifar}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock Cifar-10 and cifar-100 datasets.
\newblock \emph{URl: https://www. cs. toronto. edu/kriz/cifar. html},
  6\penalty0 (1):\penalty0 1, 2009.

\bibitem[{L\'{a}zaro}-Gredilla et~al.(2010){L\'{a}zaro}-Gredilla,
  {Qui\~{n}onero}-Candela, Rasmussen, and Figueiras-Vidal]{Miguel10}
M.~{L\'{a}zaro}-Gredilla, J.~{Qui\~{n}onero}-Candela, C.~E. Rasmussen, and
  A.~R. Figueiras-Vidal.
\newblock Sparse spectrum {G}aussian process regression.
\newblock \emph{Journal of Machine Learning Research}, pages 1865--1881, 2010.

\bibitem[Lin et~al.(2022)Lin, Wang, Liu, and Qiu]{lin2022survey}
Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.
\newblock A survey of transformers.
\newblock \emph{AI Open}, 2022.

\bibitem[Lin et~al.(2017)Lin, Feng, dos Santos, Yu, Xiang, Zhou, and
  Bengio]{DBLP:journals/corr/LinFSYXZB17}
Zhouhan Lin, Minwei Feng, C{\'{\i}}cero~Nogueira dos Santos, Mo~Yu, Bing Xiang,
  Bowen Zhou, and Yoshua Bengio.
\newblock A structured self-attentive sentence embedding.
\newblock \emph{CoRR}, abs/1703.03130, 2017.
\newblock URL \url{http://arxiv.org/abs/1703.03130}.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Lin, Padhy, Tran, Bedrax~Weiss, and
  Lakshminarayanan]{liu2020simple}
Jeremiah Liu, Zi~Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax~Weiss, and
  Balaji Lakshminarayanan.
\newblock Simple and principled uncertainty estimation with deterministic deep
  learning via distance awareness.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7498--7512, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Wang, Owens, and Li]{liu2020energy}
Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li.
\newblock Energy-based out-of-distribution detection.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 21464--21475, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Liu et~al.(2022)Liu, Ning, Cao, Wei, Zhang, Lin, and Hu]{liu2021video}
Ze~Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu.
\newblock Video swin transformer.
\newblock In \emph{I{EEE} {C}onference on {C}omputer {V}ision and {P}attern
  {R}ecognition (CVPR)}, 2022.

\bibitem[Luttinen and Ilin(2012)]{luttinen2012efficient}
Jaakko Luttinen and Alexander Ilin.
\newblock Efficient gaussian process inference for short-scale spatio-temporal
  modeling.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 741--750.
  PMLR, 2012.

\bibitem[Medsker and Jain(2001)]{medsker2001recurrent}
Larry~R Medsker and LC~Jain.
\newblock Recurrent neural networks.
\newblock \emph{Design and Applications}, 5:\penalty0 64--67, 2001.

\bibitem[Mukhoti and Gal(2018)]{mukhoti2018evaluating}
Jishnu Mukhoti and Yarin Gal.
\newblock Evaluating bayesian deep learning methods for semantic segmentation.
\newblock \emph{arXiv preprint arXiv:1811.12709}, 2018.

\bibitem[M{\"u}ller et~al.(2021)M{\"u}ller, Hollmann, Arango, Grabocka, and
  Hutter]{muller2021transformers}
Samuel M{\"u}ller, Noah Hollmann, Sebastian~Pineda Arango, Josif Grabocka, and
  Frank Hutter.
\newblock Transformers can do bayesian inference.
\newblock \emph{arXiv preprint arXiv:2112.10510}, 2021.

\bibitem[Parikh et~al.(2016)Parikh, T{\"a}ckstr{\"o}m, Das, and
  Uszkoreit]{parikh-etal-2016-decomposable}
Ankur Parikh, Oscar T{\"a}ckstr{\"o}m, Dipanjan Das, and Jakob Uszkoreit.
\newblock A decomposable attention model for natural language inference.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pages 2249--2255, Austin, Texas, November 2016.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D16-1244}.
\newblock URL \url{https://www.aclweb.org/anthology/D16-1244}.

\bibitem[Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer]{DBLP:conf/naacl/PetersNIGCLZ18}
Matthew~E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer.
\newblock Deep contextualized word representations.
\newblock In Marilyn~A. Walker, Heng Ji, and Amanda Stent, editors,
  \emph{Proceedings of the 2018 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies,
  {NAACL-HLT} 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long
  Papers)}, pages 2227--2237. Association for Computational Linguistics, 2018.
\newblock \doi{10.18653/v1/n18-1202}.
\newblock URL \url{https://doi.org/10.18653/v1/n18-1202}.

\bibitem[{Qui\~{n}onero}-Candela and Rasmussen(2005)]{Candela05}
J.~{Qui\~{n}onero}-Candela and C.~E. Rasmussen.
\newblock A unifying view of sparse approximate {Gaussian} process regression.
\newblock \emph{Journal of Machine Learning Research}, 6:\penalty0 1939--1959,
  2005.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{OpenAI report}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning}, pages
  8821--8831. PMLR, 2021.

\bibitem[Rasmussen and Williams(2006)]{Rasmussen06}
C.~E. Rasmussen and C.~K.~I. Williams.
\newblock \emph{Gaussian Processes for Machine Learning}.
\newblock MIT Press, 2006.

\bibitem[Ritter et~al.(2021)Ritter, Kukla, Zhang, and Li]{ritter2021sparse}
Hippolyt Ritter, Martin Kukla, Cheng Zhang, and Yingzhen Li.
\newblock Sparse uncertainty representation in deep learning with inducing
  weights.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 6515--6528, 2021.

\bibitem[Schwaighofer and Tresp(2003)]{Tresp03}
A.~Schwaighofer and V.~Tresp.
\newblock Transductive and inductive methods for approximate {Gaussian} process
  regression.
\newblock In \emph{Proc. NIPS}, pages 953--960, 2003.

\bibitem[Seeger et~al.(2003)Seeger, Williams, and Lawrence]{seeger2003fast}
Matthias~W Seeger, Christopher~KI Williams, and Neil~D Lawrence.
\newblock Fast forward selection to speed up sparse gaussian process
  regression.
\newblock In \emph{International Workshop on Artificial Intelligence and
  Statistics}, pages 254--261. PMLR, 2003.

\bibitem[Shi et~al.(2022)Shi, Gao, Xu, Liang, Li, Kong, Lee, and
  Kwok]{shi2022revisiting}
Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen
  Lee, and James~T Kwok.
\newblock Revisiting over-smoothing in bert from the perspective of graph.
\newblock \emph{arXiv preprint arXiv:2202.08625}, 2022.

\bibitem[Smola and Bartlett(2001)]{Smola01}
A.~J. Smola and P.~L. Bartlett.
\newblock Sparse greedy {G}aussian process regression.
\newblock In \emph{Proc. NIPS}, pages 619--625, 2001.

\bibitem[Snelson and Gharahmani(2005)]{Snelson06}
E.~Snelson and Z.~Gharahmani.
\newblock Sparse {G}aussian processes using pseudo-inputs.
\newblock In \emph{Proc. NIPS}, pages 1259--1266, 2005.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Bahri, and Metzler]{tay2022efficient}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (6):\penalty0 1--28, 2022.

\bibitem[Titsias(2009)]{Titsias09}
M.~K. Titsias.
\newblock Variational learning of inducing variables in sparse {G}aussian
  processes.
\newblock In \emph{Proc. {AISTATS}}, 2009.

\bibitem[Titsias et~al.(2013)Titsias, L{\'a}zaro-Gredilla,
  et~al.]{aueb2013variational}
Michalis Titsias, Miguel L{\'a}zaro-Gredilla, et~al.
\newblock Variational inference for mahalanobis distance metrics in gaussian
  process regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Tran et~al.(2019)Tran, Dusenberry, Van Der~Wilk, and
  Hafner]{tran2019bayesian}
Dustin Tran, Mike Dusenberry, Mark Van Der~Wilk, and Danijar Hafner.
\newblock Bayesian layers: A module for neural network uncertainty.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Tresp(2000)]{Tresp00}
V.~Tresp.
\newblock A {B}ayesian committee machine.
\newblock \emph{Neural Computation}, 12:\penalty0 2719--2741, 2000.

\bibitem[Tsai et~al.(2019)Tsai, Bai, Yamada, Morency, and
  Salakhutdinov]{tsai2019transformer}
Yao-Hung~Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and
  Ruslan Salakhutdinov.
\newblock Transformer dissection: A unified understanding of transformer's
  attention via the lens of kernel.
\newblock \emph{arXiv preprint arXiv:1908.11775}, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{warstadt2019neural}
Alex Warstadt, Amanpreet Singh, and Samuel~R Bowman.
\newblock Neural network acceptability judgments.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 625--641, 2019.

\bibitem[Xue et~al.(2021)Xue, Yu, Xu, Liu, Hu, Ye, Geng, Liu, and
  Meng]{xue2021bayesian}
Boyang Xue, Jianwei Yu, Junhao Xu, Shansong Liu, Shoukang Hu, Zi~Ye, Mengzhe
  Geng, Xunying Liu, and Helen Meng.
\newblock Bayesian transformer language models for speech recognition.
\newblock In \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 7378--7382. IEEE, 2021.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1906.08237}, 2019.

\bibitem[Zhao et~al.(2021)Zhao, Jiang, Jia, Torr, and Koltun]{zhao2021point}
Hengshuang Zhao, Li~Jiang, Jiaya Jia, Philip~HS Torr, and Vladlen Koltun.
\newblock Point transformer.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 16259--16268, 2021.

\end{thebibliography}


\newpage
\appendix
\onecolumn
\begin{center}
{\Large \bf Supplementary Materials for}
\end{center}
\vspace{-0.4cm}
\begin{center}
\large \bf \textit{Revisiting Kernel Attention with Correlated Gaussian Process Representation}
\end{center}

\DoToC

\newpage

\input{appendix}
\end{document}
