\section{Algorithms}
\label{sec:algorithms}

%Graph-based algorithms have been one of the most popular algorithms for solving ANN problem. Among those popular algorithms DiskANN, HNSW, NSG, 
Before introducing our in-place deletion algorithm,
we recap basic ideas from the DiskANN and FreshDiskANN algorithms which we use as our baseline.
We describe ideas behind searching, incremental indexing, and consolidation of changes to the index.


\subsection{DiskANN}

It is easier to describe the  construction of the DiskANN index once we understand how
how to answers queries, since the index must be built to support queries efficiently.
In fact, the following Greedy Search query algorithm is a subroutine in index construction.

\textsf{{\bf GreedySearch} (Algorithm~\ref{alg:search})} 
To find the nearest neighbors of the query vector $q$, the algorithm performs 
a greedy beam search  on the graph starting from a fixed starting point $s$.
At each step, the algorithm \emph{expands} the closest unexplored vertex,
computes the distance from $x_q$ to each out-neighbor, and adds the neighbors
to a priority queue sorting on increasing distance from $x_q$.
The search iteratively does this operation until all the top-$l_s$
closest vertices in the priority queue have been explored. 
At the end, the algorithm returns the top-k closest vertices seen as the answers.

%\vspace{-7pt}
\begin{algorithm}[h]
\caption{GreedySearch($G$, $x_q$, $k$, $l_s$)}
\label{alg:search}
\begin{algorithmic}[1]
\State \textbf{Input}: Current index graph $G(P,E)$, query $x_q$, search parameter $l_s$.
\State \textbf{Output}: Visited list $V$ and top-$k$ nearest neighbors

\State Initialize $L=\{s\}$ and $V=\{\}$ // $s$ is the starting point of $G$.

\While{$L\setminus V\neq \varnothing$}
\State $v\gets\argmin\limits_{v\in L\setminus V} D(x_v,q)$
\State $L\gets L\cup N_{out}(v)$
\State $V\gets V\cup v$
\If{$|L|>l_s$}
\State $L\gets$ top $l_s$ closest vertices to $q$ in $L$
\EndIf
\EndWhile
\State \textbf{Return} $V$ and $k$ closest vertices to $q$ in $V$
\end{algorithmic}
\end{algorithm}
%\vspace{-7pt}

\textsf{{\bf Insert} (Algorithm~\ref{alg:insert}):}
The DiskANN graph is built incrementally by inserting the vectors into the graph.
\footnote{Although the original paper~\cite{DiskANN19} describes a 2-pass construction,
in practice one pass suffices~\cite{diskann-github} and also lowers index construction
time normalized for index quality.}
To insert a new vector $x_p$, it searches for $x_p$ using \textsf{GreedySearch} 
and captures the list of expanded vertices, $V$.
It then \emph{prunes} $V$ down to a list $V'$ of cardinality $\leq R$,
and attempts connections between every $v\in V'$ and $p$ in both directions. 
If this causes the degree of vertex $v$ to exceed $R$, it prunes $N_{out}(v)$
to reduce the out-neighbor list to below the degree limit. 

\begin{algorithm}
\caption{Insert($G$, $x_p$,  $l_b$, $R$, $\alpha$)}
\label{alg:insert}
\begin{algorithmic}[1]
\State \textbf{Input}: Current index graph $G(P,E)$, point to be inserted $x_p$, build parameter $l_b$, graph degree $R$, pruning parameter $\alpha$
\State \textbf{Output}: Graph $G(P',E')$ where $P'=P\cup \{p\}$

\State $[V,A]\gets$ \textsf{GreedySearch}($G$, $x_p$, $1$, $l_b$)
\State $N_{out}(p)\gets$ \textsf{RobustPrune}($G$, $p$, $V$, $R$, $\alpha$)
\For{$v\in N_{out}(p)$}
\State $N_{out}(v)\gets N_{out}(v)\cup {p}$
\If{$|N_{out}(v)|>R$}
\State $N_{out}(v)\gets$ \textsf{RobustPrune}($G$, $v$, $N_{out}(v)$, $R$, $\alpha$)
\EndIf
\EndFor

\end{algorithmic}
\end{algorithm}

\textsf{{\bf RobustPrune} (Algorithm~\ref{alg:pruning}): }
When a vertex $p$ exceeds the degree limit $R$, \textsf{RobustPrune} selects the edges to retain. 
Let $V$ be the  list of $p$'s out-neighbors, sorted by their distance to vector $x_p$. 
At each step, remove the point $v\in V$ that is currently closest to $x_p$,
add $v$ to the new neighbor list $V'$, and remove those points still in $V$  that
are much closer to $v$ than to $p$ from $V$. 
The algorithm repeatedly adds edges to the new neighbor list until it reaches the degree limit.
The prune procedure determines the structure of the graph.
The parameter $\alpha$
is carefully set so that prune retains a mixture of nearby nodes and directionally diverse distant nodes.
It is set to constant slightly greater than $1$, typically $1.2$,
to reduce the number of hops seen by greedy search~\cite{DiskANN19}.

\begin{algorithm}
\caption{RobustPrune($G$, $p$, $V$, $R$, $\alpha$)}
\label{alg:pruning}
\begin{algorithmic}[1]
\State \textbf{Input}: Current index graph $G(P,E)$, point $p$, 
candidate out neighbors $V$, graph degree $R$, pruning parameter $\alpha$.
\State \textbf{Output}: A subset $V'\subset V$ of cardinality $\leq R$.
\State $V'=\varnothing$
\While{$V\neq \varnothing$}
\State $v\gets \argmin\limits_{v\in V}D(x_v,x_p)$
\State $V'\gets V'\cup v$
\State $V\gets V \setminus v$
\State $V\gets \{u\in V: \alpha\cdot D(x_u,x_v) > D(x_u,x_p)\}$
\EndWhile

\end{algorithmic}
\end{algorithm}

\subsection{FreshDiskANN}

While DiskANN naturally supports insertion of new vertices, there is no trivial way to delete vectors in place.
The \emph{lazy} deletion method introduced by FreshDiskANN \cite{freshdiskann} marks deleted
vertices online. The \textsf{GreedySearch} procedure still navigates vertices marked as deleted,
but does not return them as part of the visited list, or as nearest neighbors in the final answer. 
Periodically, the graph is \emph{consolidated} in the background to clean up the deleted vertices,
while preserving the index quality, as follows:

\textsf{{\bf Consolidation} (Algorithm~\ref{alg:consolidation_baseline}):}
Let $D$ be the set of deleted vertices. The background consolidation eliminates the vertices $D$
from the graph. It also cleans up the  neighborhood of each $p\in D$ to preserve the graph's \emph{navigability}
by (a) adding an edge from each up-stream neighbor of $p$ (i.e., $N_{in}(p)$) to each downstream neighbor (in $N_{out}(p)$),
and (b) RobustPruning each $v\in N_{in}(p)$ that has exceeded their degree limit.
Since  $N_{in}(p)$ is not explicitly maintained, the same effect can be had by
iterating through each $p\in P$, finding $p$'s deleted out-neighbors
$C = \{v\in N_{out}(p) \cap D\}$, and concatenating their out-neighbors  to $N_{out}(p)$, i.e.,
$N_{out}(p) \gets N_{out}(p) \cup_{v\in C} N_{out}(v) \setminus D$.
Pruning is done on $N_{out}(v)$ if degree exceeds bounds,
and $\alpha>1$ (e.g, $1.2$) is critical for maintaining recall.

\begin{algorithm}
\caption{Consolidation($G$, $D$, $R$, $\alpha$) (baseline)}
\label{alg:consolidation_baseline}
\begin{algorithmic}[1]
\State \textbf{Input}: Current index graph $G(P,E)$, deleted point set $D$.
\State \textbf{Output}: Updated index graph $G$ on nodes $P'=P\setminus D$.

\For{$p$ in \textsf{$P \setminus D$}}
    \State $C\gets N_{out}(p)\cap D$
    \For{$v$ in $C$}
        \State $N_{out}(p)\gets N_{out}(p) \cup (N_{out}(v)\setminus D)$ 
    \EndFor
    \State $N_{out}(p)\gets$ RobustPrune($G$, $p$, $N_{out}(p)$, $R$, $\alpha$)
\EndFor

\end{algorithmic}
\end{algorithm}



\subsection{In-place deletion algorithm}

In this section, we present the main algorithmic ideas for handling deletions in place.
First, let us consider the motivation behind the method used in FreshDiskANN and its limitations.
To delete a vertex in a graph without affecting connectivity, a simple approach is to
connect all its in-neighbors to all its out-neighbors. However, this impacts graph quality in two ways:

\begin{itemize}
\item  A deleted vertex with $R$ in-neighbors and $R$ out-neighbors
will result in $R^2$ edges added to the graph. This can be slow since it 
could trigger too many invocations of the robust pruning procedure to select edges. 
% \xnote{measure time taken to do robust pruning}
\item Some unnecessary short range edges will be added forcing out a few useful long range ones
due to the degree limit, degrading the overall connectivity.
%\todo{harsha: note sure I follow the second point. Any data on this?}\todo{haike: because we are adding more than enough short edges which occupies the degree limit. We could have measure average degree length, but I don't think we have time now. I am happy to remove this if you think we can't support this conclusion}
% \xnote{measure average edge length?}
\end{itemize}

In Algorithm~\ref{alg:deletion}, we propose a solution that remedies these two issues.
%
Instead of connecting all vertices in $N_{in}(p)$ to all vertices in $N_{out}(p)$, we could augment the out-neighborhood of each $z\in N_{in}(p)$ with at most $c$ points near $x_p$ and $x_z$.
%
Similarly, for each $w\in N_{out}(p)$, we can find at most $c$ existing vertices near $x_p$ and $x_w$,
and add $w$ to their out-neighborhood.
%
The constant $c$ has to be small so that the number of added edges $O(cR)$ is much smaller than $O(R^2)$.
Yet, too small a value would impact the navigability of the graph.
We found $c=3$ a reasonable trade-off (more in Sec.~\ref{sec:ablation}).
% \todo{harsha: @haike, I changed notation and reworded the paragraph. please check for accuracy}

Another issue is that we cannot easily enumerate the in-neighbors of the deleted point
as they are not explicitly stored in practical implementations.
The FreshDiskANN algorithm bypasses this issue by adding connections only
during an offline consolidation phase, where it performs a scan over the index,
avoiding the need for in-neighbors. However, this approach requires
periodic heavy consolidations to maintain satisfactory graph quality,
which our in-place algorithm aims to avoid.

\begin{algorithm}[b]
\caption{In-place Deletion($G$, $p$, $l_{d}$, $k$, $c$, $\alpha$, $R$)}
\label{alg:deletion}
\begin{algorithmic}[1]
\State \textbf{Input}: Current index graph $G(P,E)$, the point to be deleted $p$,
deletion parameter $l_d=128$, candidate list size $k$, number of edge copies $c$, pruning parameter $\alpha > 1$, graph degree $R$.
Constants typically set to $k=50$,  $c=3$, $\alpha=1.2$.
\State \textbf{Output}: Updated index graph $G$ on nodes $P'=P\setminus \{p\}$.
\State [\textsf{Visited}, \textsf{Candidates}] $\gets$ GreedySearch($G$, $x_p$, $k$, $l_{d}$)
%\State \textsf{candidates} $\gets$ closest top-k points from $v$ in \textsf{vis}
\State $N'_{in}(p)\gets \{z \in \textsf{Visited}: p \in N_{out}(z) \} $
\For{$z \in N'_{in}(p)$}
    \State $C_z \gets$ closest-$c$ points to $x_z$ in \textsf{Candidates}
    \State $N_{out}(z) \gets N_{out}(z) \cup C_z \setminus \{p\}$
\EndFor

\For{$w \in N_{out}(p)$}
    \State $C_w \gets$ closest-$c$ points to $x_w$ in \textsf{Candidates}
    \For{$y \in C_w$}
        \State $N_{out}(y) \gets N_{out}(y) \cup \{w\} $
    \EndFor
\EndFor

\State Remove vertex $p$ from $G$ (immediately)
\State For any updated vertex $v$ that exceeds degree bound $R$, $N_{out}(v)\gets$ \textsf{RobustPrune}($G, v, N_{out}(v), R, \alpha$)
%\textbf{\textcolor{red}{Is prune called at the end or as soon as one node exceeds degree bound???}}\xnote{I think it happens at the end as some nodes may get multiple edges added}
\end{algorithmic}
\end{algorithm}

Instead, we find an approximation of $N_{in}(p)$ as follows.
During the insertion of point $x_p$, the visited list $V$ served as the source of in-neighbors of $p$.
Therefore, it is reasonable to expect that if we search for $x_p$ once more (when we aim to delete $p$),
the new visited list approximates the old one. 
Therefore, our in-place algorithm (Algorithm~\ref{alg:deletion}) runs \textsf{GreedySearch} to
obtain the visited list $V$. Those nodes in $V$ with an out-edge to $p$ are our approximation of $N_{in}(p)$.

We now consider how to find $c$ appropriate edges to add to $z\in N_{in}(p)$.
A good replacement for edge $(z,p)$ is one that is close to both endpoints. 
Therefore, among the \textsf{candidates}, which contains the closest $k$ points to $x_p$,
we select $C_z$, the top-$c$ closest points to $x_z$, and add edges from $z$ to $C_z$.
Similarly, for each $w \in N_{out}(p)$, we find a replacement for $(p,w)$ as follows:
search for the $C_w$, the closest $c$ nodes to $x_w$ in the visited set. 
Add edges from each member of $C_w$ to $x_w$.
These modifications end up adding more edges than those removed, and could
cause a few vertices to exceed the degree bound. We address this problem the same
way as in previous algorithms -- invoke \textsf{RobustPrune} on its out-neighbor list.
It is important to use a parameter $\alpha>1$ to preserve the navigability of the graph
(here we use $1.2$ as in prior work).

After deleting $p$ from $G$, there may still be dangling edges in $G$ pointing to $p$
since Algorithm~\ref{alg:deletion} does not guarantee all incoming edges to $p$ are deleted.
To address this problem, we periodically scan the index to remove such dangling edges using Algorithm~\ref{alg:consolidation_ours}
usually after the number of deletions exceed a certain fraction of the index size.
Although our algorithm for handling deletion is not perfectly online, this offline
consolidation procedure is extremely lightweight and does not do any distance calculations.
As a result,  it does not impose a significant burden on our underlying system.

% Note this requires an $O(cR)$ number of distance comparison to find what are the top-c closest points to those neighbors of the deleted point.

\begin{algorithm}
\caption{Consolidation($G$, $D$) (ours)}
\label{alg:consolidation_ours}
\begin{algorithmic}[1]
\State \textbf{Input}: Current index graph $G(P,E)$, deleted point set $D$.
\State \textbf{Output}: Updated index graph $G$ on nodes $P'=P\setminus D$.

\For{$p$ in \textsf{$P \setminus D$}}
    \State $N_{out}(p)\gets N_{out}(p)\setminus D$ 
\EndFor

\end{algorithmic}
\end{algorithm}
%\todo{harsha: are we planning to study replace algorithm?}\todo{haike: what do you mean replace?}

% \begin{itemize}
% \item \textbf{Hard Deletion}: Remove $p$ and its edges from the graph
% \item \textbf{Soft Deletion}: Only mark $p$ as deleted and remove it from any future return lists. Perform consolidation (scan the whole index, and connect all the in-neighbors and out-neighbors of a deleted point in a fully connected way) every certain time period.
% \item \textbf{Eager Deletion}: Connect all the in-neighbors and out-neighbors of $p$ in a fully connected way. Remove $p$ and its edges from the graph.
% \end{itemize}

% Regarding efficiency, hard deletion is the fastest way to handle deletion, but it inevitably degrades the graph connectivity and results in lower recall over time.

% Eager deletion maintains the graph quality by adding approximately $deg^2$ number of new edges to replace the deleted point. By intuition, this should give the highest graph quality, but it is too time-consuming and not implementable as we are unable to track the in-neighbors in practice.

% Soft deletion is the amortized version of eager deletion trying to maintain similar quality index graphs but save much time.

% In our algorithm, the deletion strategy we invent is called replace deletion.

% \begin{itemize}
% \item \textbf{Replace Deletion}: Find a set $C$ of nearest neighbors of $p$, replace the out edges $(p,v)$ of $p$ with $(c,v)$ for some $c\in C$, and replace the in edges $(u,p)$ of $p$ with $(u,c)$ for some $c\in C$.
% \end{itemize}

% The idea behind our replace deletion algorithm is to add only a concise set of edges to keep the connectivity while ensuring the process remains fast. In practice, we add edges from multiple $c\in C$ to enhance graph quality, and we use the returned visited list to approximate the in-neighbors of the deleted point $p$. See Algorithm~\ref{alg:deletion} for details.


