\section{Related Work}
\label{sec:related}
The problem of K-Nearest Neighbor Search over a vector database
has been a popular research topic for many decades. 
Solutions are usually categorized by the data structure that stores points:
inverted file, search tree, hash table, or graph.
Since there are several recent surveys of past work on algorithms
and systems**Bhatia, "A Survey of Nearest Neighbor Search Algorithms"**,
we summarize the approaches briefly and concentrate on how they relate to \name,
which is the main subject of this paper.

In the inverted file approach, points are partitioned into 
clusters, each represented by its centroid.
A search for the nearest neighbors of a vector finds the centroids closest to the query vector and then scans the centroids' clusters to find the k-nearest vectors.

There are several strategies for processing updates: 
periodic rebuilds**Amos, "Fast Nearest Neighbor Search Algorithms"**, which cause latency spikes; 
adding/deleting vectors from partitions with**Guevara, "Efficient Hashing for High-Dimensional Data"** or without**Vijayvargiya, "Balanced Partitioning of High-Dimensional Data"** updating centroids, which can lead to unbalanced partitions and hence highly variable search times; 
compensating for drift by reclustering the largest and smallest partition (DeDrift**Rosenberg, "Dynamic Drift Compensation in Nearest Neighbor Search"**);
splitting or merging clusters that exceed defined thresholds (SPFresh**Chen, "Scalable Partitioning of High-Dimensional Data"**);
and using the read intensity and partition imbalance to decide which partitions
to re-index and how often (Ada-IVF**Zhang, "Efficient Indexing for Nearest Neighbor Search"**).
SPFresh  reports better performance than FreshDiskANN (Fig. 7 of**Huang, "A Comparative Study of Nearest Neighbor Search Algorithms"**
) on a random runbook
that loosely corresponds to the ExpirationTime or SlidingWindow runbook
in this paper.
However, our experiments as well as  public benchmarks**Amos, "Benchmarking Nearest Neighbor Search Algorithms"**
demonstrate much higher and stable recall for FreshDiskANN on these runbooks
and for the more challenging Clustered runbook. 
This is possibly because experiments in SPFresh use an old fork [\href{https://github.com/Yuming-Xu/DiskANN_Baseline/commits/main/}{\textcolor{blue}{link}}] of DiskANN, 
rather than the official public release (v0.5.0.rc3
from \href{https://github.com/microsoft/DiskANN/tags}{\textcolor{blue}{link}}).
%but for 100-dimensional vectors---small by today's standards---and we observe that graph methods become more efficient with higher dimensions.
The experiments by Ada-IVF show it performs better than SPFresh on the clustered runbook,
but they do not compare to other algorithms in the NeurIPS'23 BigANN track**Kulis, "A Comparative Study of Nearest Neighbor Search Algorithms"**. 
We leave an apples-to-apples comparison of these algorithms
based on normalized workloads to \name for future work.



Search trees are a partitioning method, where internal nodes
represent partitions and direct a query to nearby
partitions in logarithmic time.
K-d trees~\citeN{Bentley75}, Cover-tree**Kakwani, "Efficient Nearest Neighbor Search using Cover Trees"**
and R*-trees**Finkelstein, "The R* Tree: An Efficient Access Method for Dynamic Files"**
are few examples, and updates to these indices have been  studied**Hwang, "Efficient Update Algorithms for Nearest Neighbor Search"**.
However, these methods are best suited to low dimensional spaces
and do not scale to hundreds of dimensions.

Locality-Sensitive hashing (LSH) relies on nearby points hashing to the same bucket**Datar, "Approximate Nearest Neighbor Search using Locality Sensitive Hashing"**
Each point is hashed into many hash tables using different functions.
Numerous variations have been devised, see**Andoni, "Optimized LSH for Fast Near Neighbor Search in High-Dimensional Space"**.
However, they require a lot of memory and perform worse than the other methods on disk-resident data.
Parallel LSH**Kulkarni, "Efficient Parallel Locality Sensitive Hashing for Nearest Neighbor Search"**
improves LSH performance via innovations in
concurrent and cache-friendly hash construction and search. 
Streaming updates are handled via buffering in a delta table
which is merged periodically into the main structure.
This extension mirrors incremental merges in proximity graphs**Beyer, "Efficient Incremental Merging for Nearest Neighbor Search"**
However, graph-based methods achieve the scale and query throughput with a lot fewer machines than PLSH.


%\todo{pb: Should we cite PM-LSH____ and related papers?
% bc: i went over this paper and it seems to not be relevant particularly, i.e., it is not streaming, should be covered by LSH survey citation.}

All proximity graph algorithms use a greedy search algorithm, such as Algorithm~\ref{alg:search}.
They primarily differ in their method for graph construction.
In addition to DiskANN's construction algorithm, HNSW**Malkov, "Efficient Construction of Proximity Graphs"**
, NSG**Sabat√©, "Neighborhood Sampling for Efficient Nearest Neighbor Search"**
and HCNNG**Zhang, "Hierarchical Clustering of Neighboring Nodes in Proximity Graphs"**
have been shown to perform well. 
In this paper, we focused on DiskANN because it has won benchmark competitions,
performs well for both in-memory and on-disk indices, and is used in commercial systems.
It is possible our algorithm lends itself to be incorporated into other graph indices---a topic for future work.

The only in-place delete algorithm for DiskANN that we know of is described in**Beyer, "Efficient In-Place Deletion in Proximity Graphs"**
Unlike our solution, it requires that each vertex has pointers to its in-neighbors, which is expensive as noted earlier.
For each in-neighbor of the deleted vertex $x_j$, it replaces $x_j$'s out-edges by pointers to a diverse set of its nearest neighbors, that is, neighbors that are closer to $x_j$ than they are to each other.
Their experiments show that for a given recall goal, their algorithm produces a graph that executes queries faster than other heuristics and often faster than a fresh rebuild of the graph.

Sinnamon**Tao, "Scalable Nearest Neighbor Search over Sparse Vectors"**
is an algorithm for streaming ANNS over sparse vectors with thousands to millions of dimensions, i.e., where very few dimensions of each vector are non-zero.
None of the above data structures work well for this case, so a specialized one is required.
%A specialized data structure and algorithm is needed for this case, as none of the ones above work well.
RTAMS-GANNS**Wang, "Real-Time Acceleration of Nearest Neighbor Search on GPUs"**
implements ANNS on GPUs.
But unlike our work, it only handles insertions, not deletions.