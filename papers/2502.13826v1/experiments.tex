
\section{Experiments}
\label{sec:experiments}
In this section, we demonstrate that our \name algorithm can preserve index quality
over a range of streaming scenarios that we emulate with the help of \emph{runbooks}.
A \emph{\textbf{Runbook}} is a collection of vectors and a sequence of updates---insertions, deletions, and 
replacements---that an index must perform using vectors from this collection. 
We measure the quality of the index by querying the index after every step in this sequence.
Runbooks can differ in:
\begin{itemize}[leftmargin=*]
\item The number of steps between inserting and deleting a vector.
\item Spatial correlation of vectors deleted in one step.
\end{itemize}

Taking these two factors into consideration, we designed the following three categories of runbooks:

\begin{itemize}[leftmargin=*]
    \item{SlidingWindow:} Randomly divide the dataset into $T_{max}=200$ parts of equal size.
    Insert one part per step from $T=1$ to $T_{max}$.
    Starting from $T=T_{max}/2+1$, delete points inserted $T_{max}/2$ steps ago.
    Each point is in the index for the same number of steps.
    The index size becomes stable starting from time $T_{max}/2+1$. 
    We measure the algorithm's query performance starting from step $T_{max}/2+1$.
    This runbook simulates  ``news'' indices that index recent data that expires after a certain time window.
    \item{ExpirationTime:} Points differ on how long they reside in the index:
    forever, for a long time, or for short period.  If the runbook is $T_{max}=100$ steps long,
    these could correspond to a lifespan of 100, 50, and 10 steps respectively. We set
    their proportion to 1:2:10, so we have roughly equal number
    of points of each type at any time.
    At each step, we insert a $1/T_{max}$ fraction of the dataset
    in random order, assign them a random class upon arrival,
    and delete previous points when they expire.
    This runbook tests how well an index can handle points with different lifetimes.
    \item{Clustered \cite{simhadri2024resultsbigannneurips23}:} We partition the dataset into 64 clusters using k-means clustering.
    The runbook consists of 5 rounds. In each round, we iterate all the clusters and insert
    a random proportion of the cluster to the index. Next, we iterate over all the clusters
    and delete a random proportion of the active points in the cluster from the index.
    In this runbook, nearby points are always inserted or deleted at the same time,
    and we measure the average query recall after handling each cluster. This runbook
    tests the index quality against extreme changes in the distribution of the active point set. This is the most challenging of the three types.
\end{itemize}
 

We can instantiate each type of runbook with a different dataset to cover
various embedding models and distance functions.
We instantiate with the two datasets below.
The runbook name ``MSTuring-10M-SlidingWindow''
indicates that it uses the Sliding Window template
with 10 million vectors from the MSTuring dataset.
We will release links to the runbooks' source after the anonymous review.
\begin{tabular}{c|c|c|c}
%\footnotesize
\toprule
  Dataset    & Dimensions  & Distance      & Embedding \\ \hline
  MSTuring   & 100         & Euclidean     & MS Turing~\cite{bigann21}  \\ \hline
  Wikipedia  & 768         & Inner Product & \href{https://cohere.com/embed}{Cohere} \\ \hline
\end{tabular}



\paragraph{Parameters}
We use the same parameters for the \name algorithm,
and the FreshDiskANN baseline we test it against.
Typically this is $R=64$, $l_b=l_s=128, \alpha=1.2$ for the high-recall regime.
and $R=32, l_b=l_s=64, \alpha=1.2$ for the low-recall regime. 
Consolidation is triggered for both algorithms when $20\%$ of the points in the graph are marked deleted.
For those parameters unique to our algorithm, we present ablation studies in Section~\ref{sec:ablation}.
In this section, we set them to $l_d=128$, $k=50$, $c=3$.

We also compare with the canonical implementation of
another popular graph-based ANN algorithm -- HNSW~\cite{HNSW-git}, using commit \texttt{c1b9b79}.
We use \textsf{M}=48, and \textsf{ef\_construction}=\textsf{ef\_search}=128.
These parameters are chosen so that they consume roughly the same space as our algorithm.


\paragraph{HNSW update logic}
We note that the above version of HNSW handles deletions by marking them deleted
and avoiding returning them for queries (see Github issues
\href{https://github.com/nmslib/hnswlib/issues/249}{249} and
\href{https://github.com/nmslib/hnswlib/issues/275}{275}).
Furthermore, incoming inserts may be used to ``replace" deleted
nodes to keep the size of the graph from overflowing some pre-set cutoff.
First, it updates all of the deleted point $p$'s one-hop neighbors
by adding all of $p$'s two-hop neighbors to each of them,
and then trimming them back down to respect the degree limit, first by retaining only the closest $ef\_construction$ to each node, then calling their prune process to retain at most $M$ neighbors.
Then, it proceeds like a standard insert, calculating new
edges for the replaced points and overwriting the old edges.
In our experiments, we called the replace operation whenever
20\% or more of the nodes in the graph were marked deleted,
consistent with how often we called the consolidate function in FreshDiskANN.
%with replacements of deleted nodes once more than $20\%$ of the points are deleted.
% \xnote{we can remove "once more than 20\%" because I didn't find how HNSW support this function, though Magdalen told me she was doing this.}


\paragraph{Machines and Code}
All our experiments were conducted using 16 threads on a 32 vCPU Azure Standard\_L32s\_v3 VM,
with an Intel® Xeon® Platinum 8370C (Ice Lake) processor with hyper-threading
and 256 GB DRAM. We used a closed-source Rust implementation that
is consistent with the open-source implementation of FreshDiskANN 
used in the 2023 NeurIPS Big ANN Benchmarks (v0.5.0.rc3.post1 of ~\cite{diskann-github}).
Our implementation is faster, includes the new \name algorithm,
and has an improved implementation of the inner product distance function.

\begin{table}[t]
\footnotesize
\begin{tabular}{p{1.5cm}|c|c|c|c|c}
\toprule
                                       &                 & Deletion & Insertion & Search & Recall@10 \\ 
                                       
\midrule
\multirow{3}{*}{\shortstack{MSTuring-10M \\ SlidingWindow}} 
                                    & FreshDiskANN         & 1710     & 1452      & 143    & 94.4   \\  
                                    & \name & 1110     & 1461      & 136    & 94.8   \\
                                    & HNSW & N/A & 4016     & 66    & 91.8   \\
                                      
\midrule
\multirow{3}{*}{\shortstack{MSTuring-30M \\ Clustered}} 
                                    & FreshDiskANN         & 3611     & 3187      & 845    & 91.8   \\
                                    & \name & 3146     & 3133      & 807    & 92.5   \\
                                    & HNSW & N/A & 15296     & 859    & 89.5   \\

\midrule
\multirow{3}{*}{\shortstack{Wiki-1M \\ ExpirationTime}}     
                                    & FreshDiskANN         & 38      & 88       & 63    & 94.8      \\  
                                    & \name & 92       & 100       & 71     & 97.1   \\ 
                                    & HNSW & N/A & 240     & 42    & 95.9   \\


\midrule
\multirow{2}{*}{\shortstack{Wiki-10M \\ ExpirationTime}}  & FreshDiskANN         & 3222      & 2633       & 148    & 97.9      \\  
                                       & \name & 2170       & 2592       & 141     & 97.9  \\ 

\midrule
\multirow{2}{*}{\shortstack{Wiki-35M \\ ExpirationTime}}  & FreshDiskANN         & 14535      & 12347       & 619    & 97.3      \\ 
                                       & \name & 10027       & 11886       & 584     & 97.5   \\ 
                                       
\bottomrule
\end{tabular}
\caption{Comparison of running time (in seconds) between \name,
FreshDiskANN and HNSW in the high-recall regime. 
We report recall@10, the total time spent
on deletion (including consolidation), insertion, and executing queries across different runbooks.
The insertion time for HNSW reflects all updates including deletions.
\vspace{-10pt}
}
\label{tab:running_time_highrecall}
\end{table}

\subsection{Observations}
We report the recall, number of distance computations per query, and query throughput
in the high-recall regime for five runbooks, at each step,
in Figure~\ref{fig:highrecallplots}. 
The \name algorithm consistently outperforms the FreshDiskANN baseline by an
average recall improvement of 0.4, 0.7, 2.3, 0.03 and 0.2 percentages over these runbooks,
while using fewer distance computations and providing higher query throughput.
This phenomenon is interesting as the same $l_s$ parameter is used in both algorithms
and one might expect queries to have the same complexity in both algorithms.
One possible explanation is that since \name adds $cR$ rather than $O(R^2)$
edges per deletion, we have fewer unnecessary edges. This results in a slightly sparser
graph which is faster to query.

We report the time each algorithm spends on handling insertion and deletion in Table~\ref{tab:running_time_highrecall}. We observe that though the average
insertion time and search time is roughly the same, \name spends less time
on handling deletions on large datasets
(e.g., for dataset with more than 10 million points) compared with FreshDiskANN and HNSW.
Here the deletion time for \name includes Algorithm~\ref{alg:deletion} 
as well as periodically cleaning up of dangling edges using Algorithm~\ref{alg:consolidation_ours}.
For FreshDiskANN, the deletion time is dominated by the background
consolidation phase. For HNSW, deletion time is included in insertion time, since deletion is implemented as a replacement of a deleted node with a newly inserted node. 

\include{highrecallplots}

We observe that HNSW's overall insertion and deletion cost tends to be significantly
higher than that of \name and FreshDiskANN. We hypothesize that this is due to
several mutually reinforcing factors.
First, since HNSW's deletion adds all the two-hop neighbors of the deleted point
to all its one-hop neighbors, it adds $O(R^3)$ edges per deletion,
compared with $O(cR)$ for \name and $O(R^2)$ for FreshDiskANN.
Second, since deletion can only be processed as replacement of
deleted points by newly inserted nodes, the steady size of the
index will be larger on average than that of FreshDiskANN and \name,
meaning that each operation takes somewhat longer.
This interacts especially poorly with the clustered runbook,
in which long runs of deletions are followed by long runs of insertions.
Finally, the lack of amortization in the consolidation process means that
non-deleted nodes might be pruned up to tens of times for a set of
deletions of 20\% of the index size, as opposed to only once in the amortized context of FreshDiskANN.
All these factors contribute to a significant lower performance for updates.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/msturing-30M_highparams_knn_withstatic.png}
    \vspace{-5pt}
    \caption{Recall comparison between FreshDiskANN, \name, HNSW, and ``rebuild the index from scratch'' approach (Static DiskANN) at every 64th step on the MSTuring-30M-clustered runbook.
    }
    \label{fig:build_scratch}
\end{figure}


To test the algorithm's scaling properties, we use the same type of runbook and dataset
and scale the number of datapoints from 1 to 10 to 35 million, yielding the runbooks
Wiki-1M-ExpirationTime, Wiki-10M-ExpirationTime, and Wiki-35M-ExpirationTime
(the last 3 rows in Figure~\ref{fig:highrecallplots} and Table~\ref{tab:running_time_highrecall}).
It is interesting to see that \name spends more time on deletion
for the 1 million scale dataset but becomes considerably faster as the scale of data
grows to 10 million and beyond. The reason is that most of 
the deletion time is spent searching for alternative edge connections,
the complexity of which grows sublinearly
in terms of the data size. On the other hand, FreshDiskANN scans through the
entire index trying to clean up each adjacency list, the complexity of which scales linearly.
% \todo{replot Fig 1 with same colors for all figures. remove redundant text in legend. IS it timesteps or just steps?}\todo{just steps, we test query every 2 time steps}


We also compare the streaming approach with the approach of rebuilding the
index from scratch with the active point set at a given step.
This represents the default static approach to index construction
without a bound on computational resources.
For each selected time step,
we scan the runbook to identify all active points at the current step.
We then build a standard DiskANN index from scatch with the active points and measure 
the recall with the given parameters.
Due to limited resources, we report snapshots
from the MSTuring-30M-Clustered runbook every 64 steps in Figure~\ref{fig:build_scratch}.
It is interesting to observe that ``building from scratch'' does not yield
the optimal recall even compared to the FreshDiskANN baseline.
We hypothesize that deleting and adding new edges over time gradually
improves the graph connectivity which makes the graph easier to traverse than the initial graph.




We also report the performance the algorithms in a resource constrained environment,
where we accept lower recall for faster index construction.
%From Figure~\ref{fig:lowrecallplots} and Table~\ref{tab:running_time_lowrecall}, our in-place deletion algorithm consistently 
Measurements in Figure~\ref{fig:lowrecallplots} and Table~\ref{tab:running_time_lowrecall}
show that even in this lower recall regime, \name outperforms the FreshDiskANN baseline, 
achieving an average recall improvement of 1.2, 1.8, and 5.2 across these runbooks while using fewer distance computations 
and providing higher query throughput. 
Also, our algorithm requires less time to handle deletions on the two larger runbooks. 
Although it takes more time on the Wiki-1m-ExpirationTime runbook, we believe this is due to the data size, as we have demonstrated that it scales well on larger datasets. (Cf. Table~\ref{tab:running_time_highrecall}).

\begin{table}[t]
\footnotesize
\begin{tabular}{p{2cm}|c|c|c|c|c}
\toprule
                                       &                 & Deletion & Insertion & Search & Recall@10 \\ 
\midrule
\multirow{2}{*}{\shortstack{MSTuring-10m \\ SlidingWindow}}           & FreshDiskANN         & 720     & 466      & 47    & 76.7   \\  
                                       & \name & 394     & 473      & 46    & 77.9   \\
\midrule
\multirow{2}{*}{\shortstack{MSTuring-30m \\ Clustered}} & FreshDiskANN         & 1376     & 1027      & 288    & 71.3   \\
                                       & \name & 1170     & 1031      & 288    & 73.1   \\

\midrule

\multirow{2}{*}{\shortstack{Wiki-1m \\ ExpirationTime}}                & FreshDiskANN         & 11      & 31       & 25    & 83.1      \\  
                                       & \name & 53       & 33       & 26     & 88.3   \\ 

\bottomrule
\end{tabular}
\caption{Comparison of running time between \name
and FreshDiskANN in the low-recall regime with $R=32, l_b=l_s=64$.
We report Recall@10, the total amount of time spent (in seconds)
on deletion (including consolidation), insertion, and executing queries across different runbooks.
\vspace{-10pt}
}
\label{tab:running_time_lowrecall}
\end{table}


%\todo{bc: can you remove rust from the legend of Figure 1?}

\afterpage

% 1. reimplement the algorithm and test it in under the big-ann-benchmark environment

% 2. design more runbooks to test different strategies under different datasets

% 3. ablation study on different components of the algorithm

% Main results shall be shown on three different datasets, three scales, three types of runbooks.

% datasets: MSTuring, Wikipedia, MSMARCO

% three scales: 10m, 30m, 100m

% three runbooks: clustered (the MSTuring30m-clustered), sliding window, random expiration time

% Measure recall/time step, QPS/time step, average recall, insert time, delete time, consolidation time, max memory.
