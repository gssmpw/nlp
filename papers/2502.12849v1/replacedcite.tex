\section{Related Work}
\label{sec:related}
In this paper, we concentrate on two main families of Out-of-Distribution methods: \textit{post-hoc} and \textit{training-based}____.

\minisection{Post-hoc methods} are applied after the model has been trained and typically involve analyzing its predictions or intermediate representations to identify whether an input is OoD____. These methods often focus on computational efficiency and adaptability to pre-trained models, as they avoid retraining____.

\minisection{Training-based methods} modify the training process, sometimes completely restructuring the model to accommodate OoD detection____. These methods often come at the cost of higher training complexity, and might dilute the efforts to obtain an optimal ID training accuracy____. Additionally, exposure to outliers (real or generated) can be done to improve generalization____.

\minisection{Baselines.}
A classic baseline for OoD is considered to be Maximum Softmax Probability (MSP)____, a simple approach that relies on the logit scores to identify OoD samples. However, a major limitation of this approach is the tendency of models to produce overconfident predictions on anomalous data, leading to poor performance____. Temperature scaling____ is a simple post-hoc way of tackling the overconfidence issue, where logits are scaled by a temperature $T$, but its results are not optimal____.

\minisection{OoD and intermediate layers.}
Some methods leverage intermediate embeddings within the network. However, most do it to refine the head's detection capabilities, rather than for direct OoD detection. ASH____ enhances the network's OoD detection capabilities through activation masking of hidden layers. Similarly, ReAct____ proposes to rectify the embeddings of the penultimate layer to reduce overconfidence. However, despite leveraging intermediate embeddings to an extent, the final detection decisions in both methods rely solely on the output logits. Mahalanobis distance-based method (MDSEns)____ uses features from hidden layers to compute distances from the known distribution. However, this approach relies on the assumption that the class-conditional distributions of hidden layer features are Gaussian, which may not hold true for complex datasets and deep network architectures____. Head2Toe____ leverages intermediate representations by training a classifier head on concatenated embeddings from multiple hidden layers to improve generalization during \textit{transfer learning}. This enables the refinement of existing OoD detection techniques through the utilization of hidden layer structures.