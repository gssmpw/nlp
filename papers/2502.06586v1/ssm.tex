\section{Strong spatial mixing for edge colorings on trees when $q>(3+o(1))\Delta$}\label{sec:ssm}

In this section, we prove our theorem for strong spatial mixing.

\begin{theorem}\label{thm:SSM}
    Given a $\beta$-extra edge coloring instance $(G,\+L)$ where $G$ is a tree of maximum degree $\Delta$, the uniform distribution on such instance exhibits strong spatial mixing with exponential decay rate $1-\delta$ and constant $C = \max\{32q^{\frac{\Delta + 2}2}\Delta^2(1-\delta)^{-3},(1-\delta)^{-4}\}$ if $\beta > \max\{\Delta + 50, (1 + \eta_\Delta)\Delta + 1\}$, where
    \[
         \delta = \frac{(1 + (\beta - 1 -\Delta)/\Delta)^2 - (1+\eta_{\Delta})^2}{2(1 + (\beta - 1 -\Delta)/\Delta)^2}, \eta_{\Delta} = \+\+O\tp{\frac{\log^2 \Delta}{\Delta}}.
    \]
    Specifically, if $\beta = \tp{1 + \frac{\log^3 \Delta}{\Delta}}\Delta + 50$, then $\delta \approx \frac{\log^3 \Delta}{\Delta}$.
 \end{theorem}
 
 %\ctodo{Define broom}

We already introduced the recursion for marginal probabilities of edge colorings on trees and derived certain marginal bounds in \Cref{sec:marginals}.  We will then analyze its Jacobian matrix in \Cref{sec:jacobian}. Using the bounds on the norm of the Jacobian matrix, we prove \Cref{thm:SSM} in \Cref{sec:contraction}. Finally, we discuss the limit of our approach and possible further improvement in \Cref{sec:limit}. A key ingredient in our bounds for the norm of Jacobian matrix is a bound for certain covariance matrices, which is addressed in \Cref{sec:covariance}. 
%\ctodo{update this}

\subsection{Upper bound the 2-norm of Jacobian}\label{sec:jacobian}

\subsubsection{The Jacobian}
Recall the recursion $f$ for marginals introduced in \Cref{sec:recursion}. We regard $f=(f_\pi)_{\pi \in C_r}: \bb{R}_{\geq 0}^{C_{v_1}}\times \bb{R}_{\geq 0}^{C_{v_2}}\times \cdots \times \bb{R}_{\geq 0}^{C_{v_d}} \rightarrow \+D(C_{r})$
as a function taking inputs $\*p = (\*p_1, \*p_2, \dots, \*p_d)$ where $\*p_i\in \bb R^{C_{v_i}}$ for $i\in [d]$. The Jacobian of $f$ is a matrix $(\+J f)(\*p) \in \bb{R}^{C_r\times \bigcup_{i\in [d]} C_{v_i}}$. Since $C_{v_i}$'s are disjoint, for every $\tau \in \bigcup_{i\in [d]} C_{v_i}$, we will denote it by $(i,\tau)$ if $\tau\in C_{v_i}$ for clarity. Therefore, 
\begin{equation*}
    (\+J f)_{\pi,(i,\tau)}(\*p)= \frac{\partial f_\pi}{\partial \*p_{i}(\tau)}.
\end{equation*}
For each $i\in [d]$, define the matrix $\+J_i \in \bb{R}^{C_r\times C_{v_i}}$ with entries
\begin{equation*}
    (\+J_i f)_{\pi,\tau}(\*p) = (\+J f)_{\pi,(i,\tau)}(\*p)
\end{equation*}
for every $\pi\in C_r$ and $\tau\in C_{v_i}$. 

\bigskip
We can write $\+J_if$ in a compact way. 
\begin{proposition} 
Let $\*p_r = f(\*p)$. Then
\begin{align*}
    (\+J_i f)(\*p)=\sum_{c\in \+L(e_i)}\*a_{i,c} \*b_{i,c}^{\top},
\end{align*}
where $\*a_{i,c^*}=f(\*p) \odot \Big[\1{\pi(e_i)=c^*}-\sum_{\pi' \in C_{r}: \pi'(e_i)=c^*}\*p_r(\pi')\Big]_{\pi\in C_r}$
and $\*b_{i,c^*}=\Big[\frac{\1{c^*\notin \tau}}{\sum_{\tau' \in C_{v_i}: c^*\notin \tau'}\*p_i(\tau')} \Big]_{\tau\in C_{v_i}}$. \footnote{We write $\*u \odot \*v$ for their Hadamard product (entry-wise product).}
\end{proposition}

%\ctodo{The notation $[f(i)]_{i\in I}$}
\begin{proof}
    Let $q_{i,\rho}=\sum_{\tau
    \in C_{v_i}: \rho(e_i)\notin \tau} \*p_{i}(\tau)$.
    For any $\pi \in C_r$, we write $f_\pi$ as a function of $q_{i,\rho}$ 
    \begin{equation*}
        f_\pi = \frac{\prod_i q_{i,\pi}}{\sum_{\rho \in C_r} \prod_i q_{i,\rho}}.
    \end{equation*}
    Then we can compute
    \begin{equation*}
        \frac{\partial f_\pi}{\partial q_{i,\rho}} = \frac{1}{q_{i, \rho}}\tp{\1{\rho=\pi}-f_\rho}f_\pi.
    \end{equation*}
    Therefore,
    \begin{equation*}
          (\+J f)_{\pi,(i,\tau)}(\*p)=\frac{\partial f_\pi}{\partial \*p_i(\tau)}=\sum_{\rho\in C_r} \frac{\partial f_\pi}{\partial q_{i,\rho}} \frac{\partial q_{i,\rho}}{\partial \*p_i(\tau)}
         =\sum_{\rho\in C_r}\frac{1}{q_{i, \rho}}(\1{\rho=\pi}-f_\rho)f_\pi\cdot \1{\rho(e_i)\notin \tau}
    \end{equation*}
We write $\+J_i f$ explicitly:
\begin{align*}
(\+J_i f)(\*p)%&=\sum_{\rho \in C_r}\frac{1}{q_{i,\rho}} f(\*p)\cdot (\operatorname{diag}(\1{\pi=\rho})_{\pi \in C_r}-f_\rho(\*p)\cdot \-I)\cdot (\1{\rho(e_i)\notin \tau})_{\tau\in C_{v_i}}^{\top}\\
=\operatorname{diag}(f(\*p))\cdot\sum_{\rho \in C_r}\frac{1}{q_{i,\rho}}\cdot\Big[\1{\pi=\rho}-f_\rho(\*p)\Big]_{\pi \in C_r} \Big[\1{\rho(e_i)\notin \tau}\Big]_{\tau\in C_{v_i}}^\top
%\sum_{\rho \in C_r}\frac{1}{q_{i,\rho}} \operatorname{diag}(f(\*p))\left(\operatorname{diag}(\1{\pi=\rho})_{\pi \in C_r} - f_\rho(\*p)\cdot \-I\right)\cdot \*1 \*1^{\top} \cdot\operatorname{diag}\left(\1{\rho(e_i)\notin \tau}\right)_{\tau\in C_{v_i}}
\end{align*}
Noting that $q_{i,\rho}=\Pr[T_i]{\rho(e_i)\notin c(E_{T_i}(v_i))}$ only relies on the color $\rho(e_i)$, we have
\begin{align*}
    (\+J_i f)(\*p) = \operatorname{diag}(f(\*p))\sum_{c^*\in L({e_i})} \Big[\1{\pi(e_i)=c^*}-\sum_{\pi' \in C_{r} :\pi'(e_i)=c^*}\*p_r(\pi)\Big]_{\pi \in C_r}\Big[\frac{\1{c^*\notin \tau}}{\sum_{\tau' \in C_{v_i} :c^*\notin \tau'}\*p_i(\tau')}\Big]_{\tau\in C_{v_i}}^\top.
\end{align*}
\end{proof}

A well-known trick in the analysis of decay of correlation is to apply a potential function on the marginal recursion to amortize the contraction rate. Given an increasing  potential function $\phi:[0,1]\rightarrow \bb{R}$, we define $f^\phi$ such that for any $\pi \in C_r$ and $\*m \in \bb{R}^{C_{v_1}\times C_{v_2}\times \dots \times C_{v_d}}$,
\begin{equation*}
f^{\phi}_\pi(\*m)=\phi\left(f_\pi\left(\left(\phi^{-1}(\*m_{1}),\phi^{-1}(\*m_{2}),\dots, \phi^{-1}(\*m_{d})\right)\right)\right).
\end{equation*}

As a result, the Jacobian of $f^\phi$ can be obtained by the chain rule and the inverse function theorem as follows.
\begin{proposition}\label{prop:eq_of_Ji}
Given a smooth increasing function $\phi:[0,1]\rightarrow \bb{R}$ with derivative $\Phi = \phi'$, let $\*p = \phi^{-1}(\*m)$. Then we have
\begin{align*}
    (\+J_i f^{\phi})(\*m)=\sum_c (\Phi(f(\*p))\odot \*a_{i,c})(\*b_{i,c} \odot \Phi^{-1}(\*p_i))^{\top}.
\end{align*}
\end{proposition}
Taking $\Phi(x)=\frac{1}{\sqrt{x}}$, we have
\begin{align*}
    (\+J_i f^{\phi})(\*m)=\sum_c \*a^{\phi}_{i,c} (\*b^{\phi}_{i,c})^{\top},
\end{align*}
where $\*a^{\phi}_{i,c^*}=\sqrt{f(\*p)}\odot \Big[\1{\pi(e_i)=c^*}-\sum_{\pi' \in C_{r} : \pi'(e_i)=c^*}\*p_r(\pi')\Big]_{\pi\in C_r}$ and $\*b^{\phi}_{i,c^*}=\Big[\frac{\1{c^*\notin \tau}\sqrt{\*p_i(\tau)}}{\sum_{\tau' \in C_{v_i}: c^*\notin \tau'}\*p_i(\tau')}\Big]_{\tau\in C_{v_i}}$.

\subsubsection{Bounding $\norm{(\+J f^{\phi})(\*p)}_2$}

In this section, we aim to derive an upper bound for the 2-norm of the Jacobian of the tree recursion.
For brevity, we follow some notations which is defined in \Cref{sec:marginal_bounds_on_trees} of marginal probabilities w.r.t $\*p_i$ and $\*p_r$.
% Let $\*p_i(c)=\sum_{\tau \in C_{v_i}:c \in \tau} \*p_i(\tau)$, $\*p_i(\bar{c})=\sum_{\tau \in C_{v_i}:a \notin \tau} \*p_i(\tau)$ and $\*p_r(i,c)=\sum_{\pi \in C_r: \pi(e_i)=c} \*p_r(\pi)$. Moreover, let $\*p_i(\bar{c_1},\bar{c_2})=\sum_{\tau \in C_{v_i}:c_1,c_2 \notin \tau} \*p_i(\tau)$ and $\*p_r(i,c_1,j,c_2)=\sum_{\pi \in C_r: \pi(e_i)=c_1, \pi(e_j)=c_2} \*p_r(\pi)$.

Also we introduce some notations of matrices used in later proof.
\begin{definition}\label{def:covariance}
    For a distribution $\*p$ over proper colorings on a broom $E_{T_v}(v)=\set{e_1,\dots,e_m}$, we define $X_v=\set{(i,c):i\in [m], c\in \+L(e_i)}$ and its (local) covariance matrix $ \!{Cov}(\*p)\in \bb{R}^{X_v\times X_v}$ with entries:
    $$ \!{Cov}(\*p)((i,c_1),(j,c_2)) = \sum_{\tau \in C_v:\tau(e_i)=c_1 \& \tau(e_j)=c_2} \*p(\tau)-\tp{\sum_{\tau \in C_v:\tau(e_i)=c_1}\*p(\tau)}\tp{\sum_{\tau \in C_v:\tau(e_j)=c_2}\*p(\tau)}.$$
\end{definition}


\begin{definition}\label{def:diag_mean}
    For a distribution over colorings $\*p$ on a broom $E_{T_v}(v)=\set{e_1,\dots,e_m}$, we define $X_v=\set{(i,c):i\in [m], c\in \+L(e_i)}$ and  its diagonal matrix of mean vector $\Pi(\*p)\in \bb{R}^{X_v\times X_v}$ as follows,
    $$
        \Pi(\*p) = \-{diag}\set{\sum_{\tau\in C_v: \tau(e_i) = c}\*p(\tau)}_{(i,c)\in X_v}.
    $$
\end{definition}
Now we define the notion of spectral independence on a broom.
\begin{definition}\label{def:sepctral_independence}
    For any distribution over colorings $\*p$ on a broom $E(v) = \{e_1,...,e_m\}$, we say $\*p$ is $C$-spectrally independent if it holds that
    $$
         \!{Cov}(\*p) \preceq C\cdot \Pi(\*p).
    $$
\end{definition}
The following is the main result in this section.
\begin{condition}[marginal bound]\label{cond:marginal}
    For any $i\in [d]$, $\*p_i$ is a distribution on $C_{v_i}$ such that for any color $a$
\[
\*p_i(a)\leq \frac{\abs{E_{T_i}(v_i)}}{\beta -1 +\abs{E_{T_i}(v_i)}},
\]
and for $\*p_r = (f_\pi(\*p_1,\*p_2, \dots, \*p_d))_{\pi \in C_v}$,
\[
\frac{\*p_r(i,a)}{\*p_i(\bar{a})}\leq \frac{1}{\beta -1},
\]
where $\beta \geq (1+o(1))\Delta$ .
\end{condition}

\begin{theorem}\label{thm:bound_Jacobian}
    % \zjtodo{$o(1)$ and $\eta$ should be determined according to~\Cref{lem:SI_mu}}
    For any $i\in [d]$, $\*p_i = \phi^{-1}(\*m_i)$ and $\*p_r = f((\*p_i)_{i\in[d]})$ satisfy \Cref{cond:marginal} and $(1 + \eta)$-spectrally independent. Then $\beta \geq 1 + \frac{(1 + \eta)\Delta}{\sqrt{1 - 2\delta}}$ implies that 
    $$
        \norm{\+J f^\phi(\*m)}_2 \leq \frac{1-\delta}{\sqrt \Delta}
    $$
    where $\*m = \phi(\*p)$ and $\phi(x) = 2\sqrt{x}$.
\end{theorem}

We will prove the theorem in \Cref{sec:bound_Jac} after introducing our key reduction in \Cref{sec:dim}.

\subsubsection{Dimension reduction}\label{sec:dim}
By the definition of 2-norm, we have that $\norm{(\+J f^\phi)(\*p)}_2 = \sqrt{\lambda_{\max}( (\+J f^\phi)(\*p)(\+J f^\phi)(\*p)^\top )}$. Let $\*A := (\+J f^\phi)(\*p)(\+J f^\phi)(\*p)^\top$. We have that
\begin{align*}\label{eq:2norm_J}                 
    \*A
    = \sum_{i=1}^d  (\+J_i f^\phi)(\*p)(\+J_i f^\phi)(\*p)^\top
    = \sum_{i=1}^d \sum_{c_1,c_2\in \+L({e_i})} \inner{\*b_{i,c_1}^\phi}{\*b_{i,c_2}^\phi} \*a_{i,c_1}^\phi (\*a_{i,c_2}^\phi)^\top.
\end{align*}
The last equation simply follows from~\Cref{prop:eq_of_Ji}. The above calculation suggests that although the dimension of $\*A$ is exponential in $d$, its rank is polynomial in $d$. In the following, we will find a much smaller matrix which can be used to upper bound $\*A$. The idea is to use the trace method, namely to study $\Tr\tp{\*A^k}$. We have the following lemma.

% \begin{align}\label{eq:2norm_J}                 
%     \nonumber \norm{(\+J f^\phi)(\*p)}_2 &= \sqrt{\lambda_{\max}( (\+J f^\phi)(\*p)(\+J f^\phi)(\*p)^\top )} 
%     \\\nonumber &= \sqrt{\lambda_{\max} (\sum_{i=1}^d  (\+J_i f^\phi)(\*p)(\+J_i f^\phi)(\*p)^\top )}
%     \\&= \sqrt{\lambda_{\max} (\sum_{i=1}^d \sum_{c_1,c_2\in \+L({e_i})} \inner{\*b_{i,c_1}^\phi}{\*b_{i,c_2}^\phi} \*a_{i,c_1}^\phi (\*a_{i,c_2}^\phi)^\top )  }
% \end{align}
% The last equation simply follows from~\Cref{prop:eq_of_Ji}.
% For simplicity, let $\*A := (\+J f^\phi)(\*p)(\+J f^\phi)(\*p)^\top$.
% ~\Cref{eq:2norm_J} implies that the matrix which we concern is of low rank.
% This inspires us to convert the matrix into a smaller one which is technically easy to bound the 2norm.

\begin{lemma}\label{lem:clac_lammax}
    For any positive semi-definite matrix $\*M \in \mathbb R^{n\times n}$, we have that
    \[
        \lambda_{\max} (\*M) = \lim_{k\to \infty}
        \tp{\Tr(\*M^k)}^{\frac 1k}.
    \]
\end{lemma}
\begin{proof}[Proof of~\Cref{lem:clac_lammax}]
    Assume that $\lambda_1,...,\lambda_n$ are eigenvalues of $\*M$ and $0\leq \lambda_1 \leq \lambda_2 \leq ... \leq \lambda_n$.
    $\*M$ can be factored as $\*Q\*\Lambda\*Q^{-1}$ where $\*\Lambda$ is a diagonal matrix satisfying $\*\Lambda(i,i) = \lambda_i$.
    Therefore,
    $$
        \lim_{k\to \infty}\Tr\tp{\*M^k}^{\frac 1k}
        = \lim_{k\to \infty}\Tr\tp{\*Q\*\Lambda^k\*Q^{-1}}^{\frac 1k}
        = \lim_{k\to \infty}\Tr\tp{\*\Lambda^k\*Q^{-1}\*Q}^{\frac 1k}
        = \lim_{k\to \infty} \tp{\sum_{i=1}^n \lambda_i^k}^{\frac 1k}
        =\lambda_n.
    $$
\end{proof}
To simplify notations, we let
\[
V(i,z_1,c_1,z_2,c_2) \defeq \inner{\*b_{i,c_1}^\phi}{\*b_{i,c_2}^\phi}
    \tp{\1{z_1=c_1} - \*p_r(i,c_1)}\tp{
    \1{z_2=c_2} - \*p_r(i,c_2)}.
\]
Then we can write $\*A$ explicitly.
\begin{equation}\label{eq:equation_A}
    \*A(\pi,\tau) = \sqrt{f(\*p)(\pi)f(\*p)(\tau)} \sum_{i=1}^d \sum_{c_1,c_2\in \+L(e_i)} V(i,\pi(e_i),c_1,\tau(e_i),c_2).
\end{equation}
Let $g_k^\pi(i,c)$ denote
\begin{align*}
    &\sum_{\substack{\tau_1,...,\tau_{k-1}\in C_r\\ \tau_0=\pi}} \prod_{j=1}^{k-1}f(\*p)(\tau_j)
    \sum_{i_1,...,i_{k-1}\in[d],i_k=i}
    \sum_{\substack{c_{1,1}\in \+L(e_{i_1})\\...\\c_{1,k}\in \+L(e_{i_k})}}
    \sum_{\substack{c_{2,1}\in \+L(e_{i_1})\\...\\c_{2,k-1}\in \+L(e_{i_{k-1})}\\c_{2,k}=c}}
    \prod_{j=1}^{k-1} V(i_j,\tau_{j-1}(i_j),c_{1,j},\tau_j(i_j),c_{2,j})
    \\&\times \inner{\*b_{i,c_{1,k}}^\phi}{\*b_{i,c}^\phi}(\1{\tau_{k-1}(i) = c_{1,k}} -  \*p_r(i,c_{1,k})).
\end{align*}
We omit $\pi$ in $g_k^\pi$ for brevity.
Then we have that for any $\pi \in C_r$
\begin{align}\label{eq:trace_of_A}
    \*A^k(\pi,\pi) = f(\*p)(\pi)\sum_{i=1}^d
    \sum_{c\in \+L(e_i)} g_k(i,c) (\1{\pi(i) = c} -  \*p_r(i,c)).
\end{align}
Fix $\pi$, then we will show that $\{g_k\}_{k\geq 1}$ can be computed recursively, which gives a simple representation of $\*A^k(\pi,\pi)$. Let $X=\set{(i,c)|i\in [d], c\in \+L(e_i)}$ be the set of all feasible edge-color pairs. 
\begin{lemma}\label{lem:recursion_for_g}
    If $\*B(\*p)\in \mathbb R^{X\times X}$ satisfies that
    \begin{align*}
        \*B(\*p)((i,c_2),(j,c_4)) &= 
        \sum_{c_3 \in \+L(e_j)}
        \frac{\*p_j(\bar{c_3},\bar{c_4})}{\*p_j(\bar{c_3})\*p_j(\bar{c_4})}\times \tp{\*p_r(j,c_3,i,c_2) - \*p_r(j,c_3)\*p_r(i,c_2)}.
    \end{align*}
    Then we have that $g_k^\top = \alpha^\top_\pi \*B^{k-1}$ where
    \begin{align*}
        \alpha_\pi(i,c_2) &= \sum_{c_1 \in \+L(e_i)}
         \frac{\*p_i(\bar{c_1},\bar{c_2})}{\*p_i(\bar{c_1})\*p_i(\bar{c_2})}
        \times (\1{\pi(i) = c_1} - \*p_r(i,c_1)).
    \end{align*}
\end{lemma}
\begin{proof}[Proof of \Cref{lem:recursion_for_g}]
    For any $k > 1$, we expand one layer of summation and get
    \begin{align*}
        g_k(i,c) &= \sum_{\tau_{k-1}\in C_r} f(\*p)(\tau_{k-1})\sum_{i_{k-1}\in [d]} \sum_{c_{1,k}\in \+L(e_i)} \sum_{c_{2,k-1}\in \+L(e_{i_{k-1}})}
        g_{k-1}(i_{k-1},c_{2,k-1}) 
        \\&\quad\times \inner{\*b_{i,c_{1,k}}^\phi}{\*b_{i,c}^\phi}(\1{\tau_{k-1}(i) = c_{1,k}} - \*p_r(i,c_{1,k}))
        (\1{\tau_{k-1}(i_{k-1}) = c_{2,k-1}} - \*p_r(i_{k-1}, c_{2,k-1}))
        \\&= \sum_{c_1\in \+L(e_i)}\inner{\*b_{i,c_1}^\phi}{\*b_{i,c}^\phi}\sum_{j\in[d]}\sum_{c_{2,k-1}\in \+L(e_{j})}(\*p_r(i,c_1,j,c_{2,k-1})-\*p_r(i,c_1)\*p_r(j,c_{2,k-1})) g_{k-1}(j,c_{2,k-1}).
    \end{align*}
    Recall that
    $$ 
        \inner{\*b_{i,c_1}^\phi}{\*b_{i,c}^\phi} = 
        \frac{\*p_i(\bar{c_1},\bar{c})}{\*p_i(\bar{c_1})\*p_i(\bar{c})},
    $$
    which indicates that $g_k^\top = g_{k-1}^\top \*B(\*p)$.
    Now It is sufficient to prove that $g_1 = \alpha_{\pi}$.
    Straight calculation shows that
    $$
        g_1(i,c) = \sum_{c_1\in \+L(e_i)} \inner{\*b_{i,c_1}^\phi}{\*b_{i,c}^\phi}(\1{\pi(i) = c_1} - \*p_r(i,c_1)) = \alpha_\pi(i,c_2).
    $$
\end{proof}
In the following, we omit $(\*p)$ in $\*B(\*p)$ for brevity if there is no ambiguity. \Cref{lem:recursion_for_g} directly indicates that we can use the 2-norm of $\*B$ to upper bound that of $\*A$.
\begin{lemma}\label{lem:convert_A_to_B}
    Let $\*B \in \mathbb R^{X\times X}$ and $\alpha_\pi$ denote the matrix and the vector defined in ~\Cref{lem:recursion_for_g}.
    Then we have that for any $k\geq 1$,
    \begin{align}\label{eq:lem8}
        \sum_{\pi\in C_r}\*A^k(\pi,\pi)
        = \sum_{\pi\in C_r}f(\*p)(\pi)\alpha_\pi^\top \*B^{k-1}\beta_\pi
    \end{align}
    where $\beta_\pi(j,c_4) = \1{\pi(j) = c_4} - \*p_r(j,c_4)$, implying that $\lambda_{\max}(\*A) \leq \|\*B\|_2$.
\end{lemma}
% \begin{remark}
% \color{red}
%     In fact, in most cases, including the worst case, we have that $\lambda_{\max}(\*A) = \*B$.
%     This indicates that the dimension reduction is almost without any loss, though it is hard to prove (?), and it is verified by programming experiments.
% \end{remark}
\begin{proof}[Proof of~\Cref{lem:convert_A_to_B}]
    ~\Cref{eq:lem8} immediately follows from ~\Cref{eq:trace_of_A} and ~\Cref{lem:recursion_for_g}.
    Therefore, the maximum eigenvalue of $\*A$ can be expressed as follows.
    \begin{align*}
        \lambda_{\max}(\*A) &= \lim_{k\to \infty}\tp{\sum_{\pi\in C_r}f(\*p)(\pi)\alpha_\pi^\top \*B^{k-1}\beta_\pi}^{\frac 1k}
        \\&\leq \lim_{k\to \infty}\tp{\sum_{\pi\in C_r}f(\*p)(\pi)\norm{\alpha_\pi}_2 \norm{\*B^{k-1}\beta_\pi}_2}^{\frac 1k}
        \\&\leq \lim_{k\to \infty}\tp{\sum_{\pi\in C_r}f(\*p)(\pi)\norm{\alpha_\pi}_2 \norm{\beta_\pi}_2}^{\frac 1k} \norm{\*B}_2^{\frac{k-1}{k}}
        \\&= \|\*B\|_2.
    \end{align*}
\end{proof}
\subsubsection{Bound the transition matrix} \label{sec:bound_Jac}
Let $D_T\in \mathbb{R}^{X\times X}$ be the diagonal matrix where $D_T((i,c),(i,c)) = \*p_i(\bar{c})$.
In this section, we give an upper bound for $\norm{\*B}_2$ as $\*B$ can be represented as the product of covariance matrices of $\*p_r$, $\*p_i$ and some auxiliary diagonal matrices.

\begin{proposition}\label{prop:convert_B_to_Cov}
    Let $C_i\in \mathbb{R}^{X_{v_i}\times |\+L(e_i)|}$ denote the matrix satisfying that $C_i((j,c_1),c_2) = \1{c_1 = c_2}$ for any $i\in [d]$.
    Let $\*R \in \mathbb{R}^{X\times X}$ denote $$
    \operatorname{diag}\{C^\top_i \!{Cov}(\*p_i)C_i\}_{i\in [d]}.
    $$
    Then we have that
    $$
        \*B =  \!{Cov}(\*p_r)D_T^{-1}\*RD_T^{-1}.
    $$
\end{proposition}
\begin{proof}[Proof of~\Cref{prop:convert_B_to_Cov}]
Note that for any $i\in [d]$ and $c_1,c_2\in \+L(e_i)$,
    \begin{align*}
        \*R((i,c_1),(i,c_2)) &= C_i^\top  \!{Cov}(\*p_i)C_i(c_1,c_2)
        \\&= \*p_i(\bar{c_1},\bar{c_2})-\*p_i(\bar{c_1})\*p_i(\bar{c_2}).
    \end{align*}
    Therefore, for any $c_2\in \+L(e_i)$ and $c_4\in \+L(e_j)$, the following always holds.
    \begin{align*}
        \*B((i,c_2),(j,c_4)) &= 
        \sum_{c_3 \in \+L(e_j)}
        (\*p_r(j,c_3,i,c_2) - \*p_r(j,c_3)\*p_r(i,c_2))
        \times \frac{\*p_i(\bar{c_3},\bar{c_4})}{\*p_i(\bar{c_3})\*p_i(\bar{c_4})}
        \\&= \sum_{c_3\in \+L(e_j)}  \!{Cov}(\*p_r)((i,c_2),(j,c_3)) \tp{\frac{\*p_i(\bar{c_3},\bar{c_4})}{\*p_i(\bar{c_3})\*p_i(\bar{c_4})} - 1}
        \\&= \sum_{(k,c_3): c_3\in \+L(e_k)}  \!{Cov}(\*p_r)((i,c_2),(k,c_3))
        \frac{1}{\*p_k(\bar{c_3})}
        \*R((k,c_3),(j,c_4))
        \frac{1}{\*p_k(\bar{c_4})}
    \end{align*}
    where the second equality comes from $\sum_{c_3 \in \+L(e_j)}
        (\*p_r(j,c_3,i,c_2) - \*p_r(j,c_3)\*p_r(i,c_2))=0$.
\end{proof}
Then we can establish~\Cref{thm:bound_Jacobian} through the above conclusions.
\begin{proof}[Proof of~\Cref{thm:bound_Jacobian}]
The Loewner Order still holds under the Congruent transformation. 
Therefore, by spectral independence of $\*p_i$, we have that
\begin{align}\label{eq:bound_R}
    \*R \preceq (1+\eta)\operatorname{diag}\{C^\top_i\Pi(\*p_i)C_i\}_{i\in [d]}
    = (1+\eta)(I - D_T).
\end{align}
Plugging in $\-{Cov}(\*p_r)\preceq (1+\eta)\Pi(\*p_r)$ and~\Cref{eq:bound_R}, we get
\begin{align}
    \nonumber \|\*B\|_2 &\leq (1+\eta)^2\lambda_{\max}(\Pi(\*p_r)D_T^{-1}(I - D_T)D_T^{-1})
    \\ \label{eq:marginal_equation} &= (1+\eta)^2\max_{(i,c): c\in \+L(e_i)} \frac{\*p_r(i,c)\*p_i(c)}{\*p_i(\bar{c})^2}.
\end{align}
Applying marginal bounds for~\Cref{cond:marginal}, we have that
\begin{align*}
    \|\*B\|_2  &\leq (1+\eta)^2\max_{(i,c): c\in \+L(e_i)} \frac{\*p_i(c)}{\*p_i(\bar{c})(\beta - 1)}
    \\&\leq (1+\eta)^2\max_{i\in[d]} \frac{\deg(v_i)-1}{(\beta - 1)^2} 
    \\&\leq \frac {1 - 2\delta}{\Delta}
\end{align*}
for some $\delta > 0$.
The last inequality follows from $\beta \geq 1 + \frac{(1 + \eta)\Delta}{\sqrt{1 - 2\delta}}$.
By~\Cref{lem:convert_A_to_B}, $\norm{(\+J f^\phi)(\*p)}^2 \leq \|\*B\|_2$.
Therefore, 
$$
    \norm{(\+J f^\phi)(\*p)}_2 \leq \sqrt{\|\*B\|_2} \leq \frac{1-\delta}{\sqrt \Delta}.
$$
\end{proof}
\subsection{Strong spatial mixing via contraction}\label{sec:contraction}
\newcommand{\ra}{\rightarrow}
As \Cref{thm:bound_Jacobian} gives the upper bounds on the 2-norm of the Jacobian matrix, we now proceed to demonstrate how these bounds can be used to prove strong spatial mixing via contraction. Specifically, we will quantify the decay of correlations using the derived bounds.
% Let $B(e,d)$ denote $\set{e'\in E \mid \-{dist}(e,e') = d}$ where $\-{dist}(e'e).$
Let $B_{G}(u,d)$ denote $\set{u'\in V\mid \dist_G(u,u') = d}$.

\begin{proof}[Proof of~\Cref{thm:SSM}]
Fix $e_r = (u, r)\in E$ and $r$ is the root of the tree.
Let $\tau_1$ and $\tau_2$ be two different feasible pinnings on $\Lambda \subseteq E\setminus \set{e_r}$.
We use $(T',\+L')$ to denote the edge coloring instance which is obtained by removing every $e\in \Lambda \setminus \partial_{\tau_1,\tau_2}$ from $T$ and removing $\tau_1(e)$ from the lists of the neighbours of $e$. It is easy to verify that $(T',\+L')$ is still a $\beta$-extra edge coloring instance.

Let $\ell := \min_{e\in \partial_{\tau_1,\tau_2}} \-{dist}_{T'}(e_r,e) - 1$.
It is trivial if $\ell = \infty$.
Without loss of generality, assume $\ell \geq 3$.
Let $\*p_\ell$ and $\*p_\ell'$ denote the marginal distribution over $\bigcup_{u\in B_{T'}(r,\ell)} E_{T_u'}(u)$ under the pinning $\tau_1$ and $\tau_2$ respectively.
Let $f^{\phi,i\ra i-1}$ and $f^{i\ra i-1}$ denote the concatenation of recursive function $f^\phi$ and $f$ of subtrees which is rooted at $B_{T'}(r, i-1)$ respectively.
For simplicity, let $f^{\phi,i\ra j} := f^{\phi,j+1\ra j} \circ f^{\phi,j+2\ra j+1} \circ \dots \circ f^{\phi,i\ra i-1}$ for any $i > j$ and it is the same for $f^{i\ra j}$.
We use $\*m_\ell(t)$ denote the linear combination of  $\phi(\*p_\ell)$ and $\phi(\*p_\ell')$, that is,
$\*m_\ell(t) = t\phi(\*p_\ell) + (1-t) \phi(\*p_\ell')$.
Then we have that
\begin{align}
    \nonumber \norm{f^{\phi,\ell\ra 0}(\*m_\ell(0)) - f^{\phi,\ell\ra 0}(\*m_\ell(1))}_2
    &= \norm{\int_0^1 (\+J f^{\phi,\ell\ra 0}(\*m_\ell(t))) \cdot (\phi(\*p_\ell) - \phi(\*p_\ell')) \d t}_2
    \\\nonumber &\leq \int_0^1 \norm{ (\+J f^{\phi,\ell\ra 0}(\*m_\ell(t))) \cdot (\phi(\*p_\ell) - \phi(\*p_\ell')) }_2\d t
    \\\nonumber &\leq \max_{t\in[0,1]} \norm{ (\+J f^{\phi,\ell\ra 0}(\*m_\ell(t)))}_2\norm{(\phi(\*p_\ell) - \phi(\*p_\ell')) }_2
    \\ \label{eq:contraction} &\leq \max_{t\in[0,1]} \norm{ (\+J f^{\phi,\ell\ra 0}(\*m_\ell(t)))}_2
    \max_{u\in B_{T'}(r,\ell-1)}\Delta^{\frac{\ell}{2}}\norm{\phi(\*p_u) - \phi(\*p_u')}_2
\end{align}
where $\*p_u$ and $\*p_u'$ are the marginal distributions over $E_{T_u'}(u)$ on $T_u'$ under the pinning $\tau_1$ and $\tau_2$ respectively.
As $\phi(x) = 2\sqrt{x}$, $\norm{\phi(\*p_u) - \phi(\*p_u')}_2 \leq \norm{\phi(\*p_u')}_2 + \norm{\phi(\*p_u)}_2 = 4$.
And we have that for any $t\in [0,1]$,
\begin{align}
    \nonumber \norm{ (\+J f^{\phi,\ell\ra 0}(\*m_\ell(t)))}_2 &= \norm{(\+J f^{\phi,1\ra 0}(\*m_1(t)))\dots (\+J f^{\phi,\ell\ra \ell-1}(\*m_\ell(t)))  }_2
    \\\nonumber &\leq \prod_{i=1}^{\ell}\norm{\+J f^{\phi,i\ra i-1}(\*m_i(t))}_2
    % \\&= \prod_{i=1}^{\ell}\norm{\operatorname{diag}\{\+J f^{\phi}(\*m_u(t))\}_{u\in B_{T'}(r,i-1)}}_2
    \\\nonumber &= \prod_{i=1}^{\ell}\sup_{u\in B_{T'}(r,i-1)}\norm{\+J f^{\phi}(\*m_u(t))}_2
    \\\label{eq:apply_trivial_bound} &\leq 16\Delta q \tp{\sup_{u\in B_{T'}(r,d), d\leq \ell-3}\norm{\+J f^{\phi}(\*m_u(t))}_2}^{\ell-2}
\end{align}
where $\*m_i(t) := f^{\phi,\ell\ra i}(\*m_\ell(t)) = \phi(f^{\ell\ra i}(\phi^{-1}(\*m_\ell(t))))$ and $\*m_i(t) = (\*m_u(t))_{u\in B_{T'}(r,i-1)}$.
Here \Cref{eq:apply_trivial_bound} follows from the following claim.
\begin{claim}\label{claim:trivial_bound}
    For any $t\in [0,1]$, we have that
    \begin{enumerate}
    \item $\norm{\+J f^\phi(\*m_u(t))}_2 \leq 4\sqrt{2\Delta q}$ holds for $u\in B_{T'}(r,\ell-1)$.
    \item $\norm{\+J f^\phi(\*m_u(t))}_2 \leq 2\sqrt{2\Delta q}$ holds for $u\in B_{T'}(r,\ell-2)$.
    \end{enumerate}
\end{claim}
\begin{proof}[Proof of \Cref{claim:trivial_bound}]
    Fix $u\in B_{T'}(r,\ell-1)$.
    By the concavity of $\phi$, we have that for any $\tau \in C_u$ and $t\in [0,1]$,
    $$
        \phi^{-1}(\*m_u(t))(\tau)
        = \phi^{-1}(t \phi(\*p_u(\tau)) + (1-t)\phi(\*p_u'(\tau)))
        \leq t \*p_u(\tau) + (1-t)\*p_u'(\tau);
    $$
    $$
        \phi^{-1}(\*m_u(t))(\tau)
        \geq t^2\*p_u(\tau) + (1-t)^2\*p_u'(\tau).
    $$
    Since $\ell = \min_{e\in \partial_{\tau_1,\tau_2}} \-{dist}_{T'}(e_r,e) - 1$, by \Cref{lem:marginal_bound_1} and \Cref{lem:marginal_bound_2}, $\*p_\ell$ and $\*p_\ell'$ satisfy \Cref{cond:marginal}, thus we have that
    $$
        \norm{\+J f^\phi(\*m_u(t))}_2^2 \leq \norm{\*B(\phi^{-1}(\*m_u(t)))}_1
        \leq \Delta q\times \frac{1}{\tp{\frac 12 \frac{\beta -1}{\beta -1 + \Delta}}^2}\times 2
        = 32 \Delta q.
    $$
    For any $u\in B_{T'}(r,\ell-2)$, by \Cref{lem:marginal_bound_1} and \Cref{lem:marginal_bound_2}, $\phi^{-1}(\*m_u(t))$ satisfies \Cref{cond:marginal}.
    Therefore, 
    $$
        \norm{\+J f^\phi(\*m_u(t))}_2^2 \leq \norm{\*B(\phi^{-1}(\*m_u(t)))}_1
        \leq \Delta q\times \frac{1}{\tp{\frac{\beta -1}{\beta -1 + \Delta}}^2}\times 2
        = 8 \Delta q.
    $$
\end{proof}
The second equation follows from $\+J f^{\phi,i\ra i-1}(\*m_i(t)) = \operatorname{diag}\{\+J f^{\phi}(\*m_u(t))\}_{u\in B_{T'}(r,i-1)}$.
By \Cref{lem:marginal_bound_1}, \Cref{lem:marginal_bound_2} and \Cref{lem:SI-broom}, for any $u\in B_{T'}(r,d)$ and $d\leq \ell-3$, $\*m_u(t)$ satisfies~\Cref{cond:marginal} and is $(1+\eta_\Delta)$-spectrally independent.
Plugging in~\Cref{thm:bound_Jacobian} and~\Cref{eq:contraction}, we have that 
\begin{align*}
     \norm{\mu_{e_r}^{\tau_1}- \mu_{e_r}^{\tau_2}}_{\-{TV}}
    &\leq \norm{\mu_{E_T(r)}^{\tau_1}- \mu_{E_T(r)}^{\tau_2}}_{\-{TV}}
    \\&= \frac 12\norm{f^{\phi,\ell\ra 0}(\*m_\ell(0)) - f^{\phi,\ell\ra 0}(\*m_\ell(1))}_1
    \\&\leq \frac{\sqrt{q^\Delta}}2\norm{f^{\phi,\ell\ra 0}(\*m_\ell(0)) - f^{\phi,\ell\ra 0}(\*m_\ell(1))}_2
    \\&\leq 32q^{\frac{\Delta + 2}2}\Delta^2(1-\delta)^{\ell-2}.
\end{align*}

We pick $C = \max\{32q^{\frac{\Delta + 2}2}\Delta^2(1-\delta)^{-3},(1-\delta)^{-4}\}$ to finish the proof.
\end{proof}

\subsection{Worst-case scenario}\label{sec:limit}

Though \Cref{thm:bound_Jacobian} establishes an upper bound on the 2-norm of the Jacobian matrix under certain conditions, in this section, we will introduce the ``worst'' pinning of $q$-edge coloring, which is the bottleneck of our analysis. 
In fact, with potential function $\phi(x) = 2\sqrt x$ and applying 2-norm for the correlation decay step, the best bound we expected to prove is $q > \tp{\frac{3+\sqrt 5}{2}+ o(1)}\Delta\approx 2.618\Delta$. However what we can only prove strong spatial mixing for instances of $(1+o(1))\Delta$-extra edge colorings, as we currently lack a better upper bound for $\*R$.
Note that the pinning that maximizes~\cref{eq:marginal_equation} is the same as the pinning in~\Cref{thm:worst_pinning} and~\cref{eq:marginal_equation} can indeed achieve the upper bound $\frac{\Delta-1}{(q-2\Delta+2)^2}$ under this worst pinning.

Before showing the worst-case scenario, we introduce the following technical lemma to calculate the eigenvalues of some simple matrices.
\begin{lemma}\label{lem:calc_eigenvalues}
    Given constant $k_1,k_2\neq 0$, 
    the eigenvalues of $k_1\*1\*1^\top +k_2 \!{Id}_n$ are either $k_2$ or $nk_1 + k_2$ where $\!{Id}_n$ is the identity matrix in $\mathbb{R}^{n\times n}$.
\end{lemma}
\begin{proof}[Proof of~\Cref{lem:calc_eigenvalues}]
    The eigenvalues $\lambda$ and the corresponding eigenvectors $\*v$ satisfies that 
    \begin{align}\label{eq:eigen_equation}
        (k_1 \*1\*1^\top + k_2\!{Id})\*v = \lambda \*v
        \implies k_1\langle\*1,\*v\rangle \*1 = (\lambda - k_2)\*v.
    \end{align}
    Therefore, there are only two cases for~\Cref{eq:eigen_equation}.
    $$
    \begin{cases}
        \lambda = k_2 \land \*1\perp \*v
        \\\*v \parallel \*1
    \end{cases}.
    $$
    Plugging in $\*v = \*1$ and~\Cref{eq:eigen_equation}, we get
    $$
        k_1\langle\*1,\*1\rangle = \lambda - k_2 \implies \lambda = k_2 + nk_1.
    $$
\end{proof}
Now we show the worst pinning and corresponding norm value of $\*B$ and $(\+J f^\phi)(\*p)$.
\begin{theorem}\label{thm:worst_pinning}
    Under some specific pinning, the norm of $\*B$ satisfies lower bound that
    $$
    \|(\+J f^\phi)(\*p)\|_2^2 = \|\*B\|_2 = \frac{\Delta-1}{(q-\Delta)(q-2\Delta+2)}.
    $$
    Then $\|\*B\|_2 < \frac 1\Delta$ implies that $q > \tp{\frac{3+\sqrt 5}{2}+o(1)}\Delta$.
\end{theorem}
\begin{proof}[Proof of~\Cref{thm:worst_pinning}]
    Consider the following instance of edge coloring $(G,\+L)$ generated from a $q$-coloring instance by pinning:
    \begin{enumerate}
        \item $\deg(r) = 1$, $\+L(e_1) = \{\Delta,...,q\}$, that is, in original instance $r$ has $\Delta$ children and we pin $e_2,...,e_{\Delta}$ with $1,2,...,\Delta-1$.
        \item $\deg(v_1) = \Delta$ and for any $u\in N(v_1)$ and $u\neq r$, $\+L(\{u,v_1\}) = \{\Delta,...,q\}$, that is, we assume $u$ has $\Delta-1$ children and we pin the children of $u$ with color $1,2,...,\Delta-1$.
    \end{enumerate}
    Then by~\Cref{prop:convert_B_to_Cov}$, \*B$ is equivalent to a matrix in $\mathbb{R}^{(q-\Delta+1)\times (q-\Delta+1)}$ and holds for the following equation
    $$
        \*B = -\frac{\Delta-1}{(q-\Delta+1)(q-\Delta)(q-2\Delta+2)}\tp{\*1\*1^\top - \!{Id}} + \frac{\Delta-1}{(q-\Delta+1)(q-2\Delta+1)}\!{Id}.
    $$
    $\*B$ is a symmetrical matrix, thus the singular values equals to the eigenvalues. 
    Applying~\Cref{lem:calc_eigenvalues}, we have that
    $$
        \|\*B\|_2 = \lambda_{\max}(\*B) = \frac{\Delta-1}{(q-\Delta)(q-2\Delta+2)}.
    $$
    For $(\+J f^\phi)(\*p)$, since $\abs{C_r} = |\+L(e_1)| = q-\Delta+1$, $(\+J f^\phi)(\*p)(\+J f^\phi)^\top(\*p) = \*A \in \mathbb{R}^{(q-\Delta+1)\times (q-\Delta+1)}$.
    Now it is sufficient to show that $\*A = \*B$.
    By~\Cref{eq:equation_A}, we have that
    \begin{align*}
        \*A(c_1,c_2) &= \frac 1{q-\Delta+1}\sum_{c_3,c_4\in \+L(e_1)}\langle \*b^\phi_{1,c_3}, \*b^\phi_{1,c_4}\rangle\tp{\1{c_1=c_3} - \frac 1{q-\Delta+1}}\tp{\1{c_2 = c_4} - \frac 1{q-\Delta+1}}
        \\&= \begin{cases}
            \frac{\Delta-1}{(q-\Delta+1)(q-2\Delta+2)} &, c_1 = c_2
            \\\frac{1-\Delta}{(q-\Delta+1)(q-\Delta)(q-2\Delta+2)} &, c_1\neq c_2
        \end{cases}
        \\&= \*B(c_1,c_2).
    \end{align*}
\end{proof}
\Cref{thm:worst_pinning} indicates the limitations of our current analysis by providing a lower bound on the norm of the Jacobian matrix. This indicates the best bound one can expect to use $2$-norm and the potential function $\phi(x) = 2\sqrt{x}$ is $q\approx 2.618\Delta$.


%This insight is crucial for understanding the challenges to improving our results.
