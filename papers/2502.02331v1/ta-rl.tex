\section{Details of RL Simulations}
\label{sec:add-rl}

This section gives additional details about RL-like simulations in Sections
\ref{sec:one-rl} and \ref{sec:inf-rl}.

\subsection{One-period Episodic Simulations}

We consider episodic exploration of $T=1$ slow model, where we additionally
assume that $\lambda = 0$ and $\lambda$ is known to the provider. In this
setting, we assume that each episode has the following structure.
\begin{enumerate}
    \item Nature samples $q_0$.
    \item The provider observes $\{z^i_0\}_{i=0}^{m-1} \sim D_0^m$.
    \item The provider deploys $\prm_0$.
    \item The provider observes $\{z^i_1\}_{i=0}^{m-1} \sim D_1^m$.
\end{enumerate}

We implement \cref{alg:rl-episodic}, adopted version of Algorithm 1 of
\citet{l22w}, where we denote the episode number by $\tau$, with hyperparameter
$\beta=2^{-8}$ to find the optimal prediction. (Notice that the second period
observations are non-informative for the log-likelihood maximization because
$\lambda = 0$.)

\begin{algorithm}[ht]
    \caption{Optimistic Maximum Likelihood Estimation}
    \label{alg:rl-episodic}
    \begin{algorithmic}
        \STATE {\bfseries Initialize:} $B^0 = \{(\alpha, \pi) : \alpha \in [-1,
        1], \pi \in [-1/2, 1/2]\}$, $\mathcal{D} = \{\}$
        \FOR{$\tau=0$ {\bfseries to} $T$}
            \STATE Deploy $\prm^\tau_0 = \argmin_{\prm \in [-\nicefrac{1}{2},
            \nicefrac{1}{2}]} \min_{(\alpha, \pi) \in
            B^\tau} \loss\Par*{\prm \given \alpha, \pi}$
            \STATE Observe $S^\tau_1 \sim (D^\tau_1)^m$
            \STATE Add $(\prm^\tau_0, S^\tau_1)$ to $\mathcal{D}$
            \STATE Update $
                B^{\tau+1} = \Bc[\bigg]{(\alpha, \pi) \in B^0 : \sum_{(\prm, S)
                \in D} \log \Pr\Par*{S \given \alpha, \pi, \prm} \ge
                \max_{(\alpha, \pi) \in B^0} \sum_{(\prm, S) \in D} \log
                \Pr\Par*{S \given \alpha, \pi, \prm} - \beta}
                $
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\subsection{Infinite Horizon Simulations}

We consider episodic exploration of $T=\infty$ slow model, where the provider
know the value of $\gamma$. In this setting, we assume that each step has the
following structure.
\begin{enumerate}
    \item The provider observes $\{z^i_t\}_{i=0}^{m-1} \sim D_t^m$.
    \item The provider deploys $\prm_t$.
\end{enumerate}
For this case, we implement heuristic \cref{alg:rl-heuristic}, where we denoted
the value function as
\[
    V(p_0, \alpha, \pi, \lambda, \gamma) \defeq
    \min_{(\prm_t)_{t=0}^\infty} \sum_{t=0}^\infty \gamma^t \loss\Par*{\prm_t
    \given \prm_{t-1}, \dots, \prm_0, p_0, \alpha, \pi, \lambda}.
\]
This algorithm learns the performative response by deploying extreme
predictions $\{-1/2, 1/2\}$ at random for the first $4$ steps. Then the model
provider learns the parameters of the performative response by likelihood
maximization and deploys the optimal policy under their estimates.
\begin{algorithm}[ht]
    \caption{Greedy Exploration}
    \label{alg:rl-heuristic}
    \begin{algorithmic}
        \STATE Observe $S_0 \sim D_0^m$
        \FOR{$t=0$ {\bfseries to} $3$}
            \STATE Deploy $\prm_t$ at random from $\{-1/2, 1/2\}$
            \STATE Observe $S_{t+1} \sim D_{t+1}^m$
        \ENDFOR
        \FOR{$t=4$ {\bfseries to} $T$}
            \STATE Estimate $(\alpha, \pi, \lambda, p_0) = \argmax_{\alpha,
            \pi, \lambda, p_0} \sum_{\tau=0}^t \log \Pr\Par*{S_t \given
            \prm_{t-1}, \dots, \prm_0, p_0, \alpha, \pi, \lambda}$
            \STATE Deploy $\prm_t = \argmin_{\prm_t}
            \loss\Par*{\prm_t \given p_{t+1}(\prm_t, \dots, \prm_0, p_0,
            \alpha, \pi, \lambda)} + \gamma V(p_{t+1}(\prm_t, \dots, \prm_0,
            p_0, \alpha, \pi, \lambda), \alpha, \pi, \lambda, \gamma)$
            \STATE Observe $S_{t+1} \sim D_{t+1}^m$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\clearpage
