\section{Infinite Horizon Model}
\label{sec:infinite_horizon}

\begin{figure*}[ht]
    \input{fig5-inf.pgf}
    \caption{The plots depict the dependence of $\prm^*_0$ (blue), $p^*_1$
    (orange), $s_1$ (green), and $p^*_\infty$ (red) on $p_0$ for $\lambda =
    0.8$, $\pi = 0.2$, and $\gamma=0.5$ in $T=\infty$ case. Columns correspond
    to the different values of $\alpha$, the top row corresponds to the slow
    case, the bottom row corresponds to the slow case.}
    \label{fig:inf-sol}
\end{figure*}

Now, we study the long-term effects of performativity by analyzing our model
for $T=\infty$. We first theoretically study the perfect information case and then use simulations to analyze the case of unknown problem parameters.

\subsection{Perfect Information}
\label{sec:inf-slow}
\subsubsection{Slow Deployment}

\begin{theorem}[Proof in \cref{sec:proof-inf-slow-sol}]
    \label{thm:inf-slow-sol}
    Assume that the PRM path does not take extreme values $\forall t \:
    \abs{\prm^*_t} \neq 1/2$ and $1 - 2 \alpha \ge \sqrt{\gamma} \beta$. Then,
    the solution to the problem (\ref{eq:opt-cont-prob}) in the $T=\infty$ slow deployment
    case satisfies
    \[
        \frac{\prm^*_t - \prm^*_\infty}{p_0 - p^*_\infty} = \frac{2 (1 -
        \abs{\alpha}) \lambda}{1 - 2 \alpha + \xi} \omega^t, \: \frac{p^*_t -
        p^*_\infty}{p_0 - p^*_\infty} = \omega^t, 
    \]
    where
    \[
        \begin{split}
            \prm^*_\infty & \defeq \frac{(1 - \gamma \beta) (1 - \abs{\alpha} -
            \beta) \pi}{1 - 2 \alpha - \beta + \alpha \beta - \gamma
            \beta (1 - \alpha - \beta)},\\
            p^*_\infty &\defeq \frac{(1 - \alpha - \gamma \beta) (1 -
            \abs{\alpha} - \beta) \pi}{1 - 2 \alpha - \beta + \alpha
            \beta - \gamma \beta (1 - \alpha - \beta)},\\
            \omega & \defeq \beta + \frac{2 \alpha \beta}{1 - 2 \alpha + \xi},
            \xi \defeq \sqrt{1 - \frac{4 \alpha (1 - \alpha)}{1 - \gamma
            \beta^2}}.
        \end{split}
    \]
\end{theorem}

Notice that the restriction $\forall t \: \abs{\prm^*_t} \neq 1/2$ could hold
only if $\omega \le 1$. There is an upper bound on $\alpha$ beyond which the
model provider is incentivized to choose the extreme values of $\prm_t$. So, if
this bound does not hold, after some time, the model provider always benefits
from setting $\abs{\prm^*_t} = 1/2$, even though this prediction is necessarily
biased. Additionally, if $\omega < 1$, the solution converges $\prm^*_t \to
\prm^*_\infty, s^*_t \to s^*_\infty, p^*_t \to p^*_\infty$ in the limit $t \to
\infty$, allowing us to study the long-term effects of PRM.

We visualize the solution for all cases in \cref{fig:inf-sol} (top row). We
see that the restriction $\forall t \: \abs{\prm^*_t} \neq 1/2$ does not cover
the cases of big positive values of $\alpha$. In such scenarios, the PRM
prediction depends on $p_0$ discontinuously because the model provider has a
strong incentive to shift the mean to extreme values.

For the rest of this section, we assume that $\pi > 0$.

\myparagraph{Long-Term Bias} The long-term bias follows
\[
    \prm^*_\infty - p^*_\infty = \frac{\alpha (1 - \abs{\alpha} - \beta) \pi}{1
    - 2 \alpha - \beta + \alpha \beta - \gamma \beta (1 - \alpha - \beta)}.
\]
Even in the limit $t \to \infty$, the PRM solution has a non-vanishing
bias. If $\alpha > 0$ and $\alpha$ is small, the long-term bias is positive.
Even though the bias increases the error term in \cref{eq:mse}, the model
provider benefits in terms of uncertainty because the biased prediction shifts the
mean to more extreme values. On the other hand, if $\alpha < 0$ and
$\abs{\alpha}$ is small, the bias is negative. In the negative feedback case,
the negative bias again shifts the mean to more extreme values than the
unbiased prediction, reducing uncertainty.

\myparagraph{Long-Term Shift} The long-term shift of $\prm^*_t$ is non-zero:
\[
    p^*_\infty - \pi = \frac{(\alpha - \abs{\alpha} + \alpha \abs{\alpha} +
    \gamma \beta (\abs{\alpha} - \alpha)) \pi}{1 - 2 \alpha - \beta + \alpha
    \beta - \gamma \beta (1 - \alpha - \beta)}.
\]

\myparagraph{Comparison with Naive Path} We have that $$\prm^n_\infty = p^n_\infty = \frac{1 - \abs{\alpha} - \beta}{1 - \alpha -
    \beta} \pi$$. The bias of the naive path tends to zero as $t \to \infty$. The long-term shift
is also zero if $\alpha > 0$. If $\alpha < 0$,
\[
    \frac{p^*_\infty - \pi}{p^n_\infty - \pi} = \frac{1 - \gamma \beta +
    \frac{\abs{\alpha}}{2}}{1 - \gamma \beta + \frac{\abs{\alpha}}{1 +
    \abs{\alpha} / (1 - \beta)}} < 1.
\]
The long-term shift of the naive path is bigger than that of the PRM path
in the negative feedback case.

Similarly to $T=1$, the naive path has a smaller bias and shift than the PRM
path in the positive feedback case, while the PRM path has a smaller shift
in the negative feedback case. However, the long-term bias of the naive path is
$0$, even in the negative feedback case.

\subsubsection{Rapid Deployment}
\label{sec:inf-rapid}

\begin{theorem}[Proof in \cref{sec:proof-inf-rapid-sol}]
    \label{thm:inf-rapid-sol}
    Assume that the PRM path does not take extreme values $\forall
    t \: \abs{\prm^*_t} \neq 1/2$. Then, the solution to the problem
    (\ref{eq:opt-cont-prob}) in $T=\infty$ rapid case satisfies
    \[
        \prm^*_t = \frac{2}{1 + \chi} (p_0 - p^*_\infty) \kappa^t +
        \prm^*_\infty,\:
        p^*_t = (p_0 - p^*_\infty) \kappa^t + p^*_\infty,
    \]
    where $\kappa \defeq \beta + \frac{2 \alpha}{1 + \chi}$, $\chi \defeq \sqrt{1 - \frac{4 \gamma \alpha (\alpha + \beta)}{1 -
            \gamma \beta^2}}$ and
    \[
        \begin{split}
            \prm^*_\infty & \defeq \frac{(1 - \gamma \beta) (1 - \abs{\alpha} -
            \beta) \pi}{1 - \alpha - \beta - \gamma (\alpha + \beta -
            \beta (2 \alpha + \beta))},\\
            p^*_\infty & \defeq \frac{(1 - \gamma (\alpha + \beta)) (1 -
            \abs{\alpha} - \beta) \pi}{1 - \alpha - \beta - \gamma
            (\alpha + \beta - \beta (2 \alpha + \beta))}.
        \end{split}
    \]
\end{theorem}

Similarly to the slow case, the restriction $\forall t \: \abs{\prm^*_t} \neq
1/2$ could hold only if $\abs{\kappa} \le 1$. If $\abs{\kappa} < 1$,
$\prm^*_\infty$ and $p^*_\infty$ represent the long-term values of $\prm^*_t$
and $p^*_t$, respectively.

We visualize the solution for all cases in \cref{fig:inf-sol} (bottom row).
Again, the assumption $\forall t \: \abs{\prm^*_t} \neq 1/2$ does not cover
large values of $\abs{\alpha}$. If $\abs{\alpha}$ is large, the PRM
prediction depends discontinuously on $p_0$. If $\alpha > 0$, the mean,
depending on $p_0$, converges to one of two equilibrium values. If $\alpha <
0$, the mean oscillates between two values that correspond to extreme
predictions.

For the rest of this section, we assume that $\pi > 0$.

\myparagraph{Long-Term Bias} We get
\[
    \prm^*_\infty - p^*_\infty = \frac{\gamma \alpha (1 - \abs{\alpha} - \beta)
    \pi}{1 - \alpha - \beta - \gamma (\alpha + \beta - \beta (2 \alpha +
    \beta))}.
\]
The bias is again not zero and behaves similarly to the slow case for small
$\abs{\alpha}$.

\myparagraph{Long-Term Shift} We get a non-zero long-term shift:
\[
    p^*_\infty - \pi = \frac{(\alpha - \abs{\alpha} + \gamma (\alpha
    \abs{\alpha} + (\abs{\alpha} - \alpha) \beta) \pi}{1 - \alpha - \beta -
    \gamma (\alpha + \beta - \beta (2 \alpha + \beta))}.
\]

\myparagraph{Comparison with Naive Path} Notice that the mean in the naive path case satisfies $p^n_{t+1} = \alpha
p^n_{t-1} + \beta p^n_t + (1 - \abs{\alpha} - \beta) \pi$. Since $\alpha +
\beta < 1$, the mean converges to an equilibrium, which satisfies $\prm^n_\infty = p^n_\infty = \frac{1 - \abs{\alpha} - \beta}{1 - \alpha -
    \beta} \pi$.
Again, the long-term bias of the naive path is zero. The shift is zero if
$\alpha > 0$. If $\alpha < 0$,
\[
    \frac{p^*_\infty - \pi}{p^n_\infty - \pi}
    = \frac{1 + \gamma \abs{\alpha} / 2 - \gamma \beta}{1 + \gamma \abs{\alpha}
    / 2 - \gamma \beta + \frac{\gamma \abs{\alpha} (1 - \abs{\alpha} -
    \beta)}{2 (1 + \abs{\alpha} - \beta)}} < 1.
\]
Similarly to the slow case, the shift is smaller for $\prm^*_t$.

\subsection{RL Simulations}
\label{sec:inf-rl}

Finally, we check whether our results in the perfect information case
transfer to the general performative prediction problem with information
restrictions. We consider a usual sequential RL problem. We implement a simple heuristic algorithm, which learns the
performative response by deploying extreme predictions $\{-1/2, 1/2\}$ at
random for the first $4$ steps. Then the model provider learns the parameters of
the performative response by likelihood maximization and deploys the optimal
policy under the resulting estimates. We visualize the prediction path of the algorithm
in \cref{fig:rl} (right). After some exploration, the predictions and the means of the distribution quickly converge to the
theoretically-predicted equilibrium values, which validates our theoretical
analysis of the perfect information case.
