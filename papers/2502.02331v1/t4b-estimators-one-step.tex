\subsection{Imperfect Information}
\label{sec:imperf-one-slow}

This section analyzes how uncertainty affects our full information results in
the slow deployment case. 

\subsubsection{Unknown Mean}

First we analyze the case where $\alpha$ and $\lambda$ are known to the
model provider, but $p_0$ is unknown. Therefore, the model provider needs to simultaneously learn $p_0$ and adjust for performativity. For simplicity, we focus on the equilibrium
case, where $p_0 = \pi$. To learn $p_0$, the model provider observes $m$ i.i.d.
samples $S_0 = \{p_{0, i} \}_{i=1}^m \sim D_0^m$. The learner uses an estimator
$\theta_0\colon \mathbb{R}^m \to [-1/2, 1/2]$ to produce an estimate
$\theta_0(S_0)$.

%
%The main difficulty here is that the expectation is with respect to both the randomness of the generation of $z$, together with the random variable $\theta_0$, which is the arguments of the minimisation problem. This arises from the lack of information on the true value of the parameter $p_0$ and thus we seek solution which is a function of the i.i.d. observations in each step. 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BIAS - VARIANCE%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{lemma}[Bias-Variance Tradeoff]\label{lemma: bias-variance}
%	Let $\theta_0$ be any estimator and let $z$ follow a general distribution of mean $p_1(\theta_0)$, denoted by $D_1$. In the one-period model, the performative risk minimisation objective is given by
%	\begin{equation*}
%		\mathbb{E}[(\theta_0 - z)^2] = \mathbb{E}[(\theta_0 - p_1(\theta_0))^2] + \mathbb{E}[\mathbb{V}[z | \theta_0]]
%	\end{equation*}
%	where the expectation is only with respect to the the randomness of the observations $\{p_{0, i} \}_{i=1}^m$.
%%	Furthermore, if the distribution $D_1$ and $p_1(\theta_0)$ are as described above, then 
%%	\begin{align*}
%%		&\mathbb{E}[(\theta_0 - z)^2] = (1-2\alpha)\mathbb{E}[\theta_0^2] - 2 (1 - |\alpha|)p_0 \mathbb{E}[\theta_0] + \frac14,
%%	\end{align*}
%%	where the expectation is only in terms of the randomness of the observations $\{p_{0, i} \}_{i=1}^m$.
%\end{lemma}
%
%\myparagraph{Discussion} In the first result of the lemma, we decompose the objective function into two parts: a mean squared error term and a variance term. In the classical framework, the distribution remains unaffected by the deployed model, so the objective function simplifies to
%\begin{equation*}
%\mathbb{E}[(\theta_0 - z)^2] = \mathbb{E}[(\theta_0 - p)^2] + \mathbb{V}[z],
%\end{equation*} 
%where the second term becomes an irreducible error which does not take part in the minimisation problem. Furthermore, the mean squared error term is classically decomposed into the squared bias and the variance terms, both with respect to the estimator $\theta_0$. The bias reflects how close the estimator is to the true value of the mean, while the variance measures how much the estimator fluctuates around its mean. The performative framework extends this decomposition by introducing an additional variance term, corresponding to the variance of the shifted distribution, which is no longer an irreducible error in the optimization problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ESTIMATORS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\myparagraph{Estimators} To study the extend to which the results of the previous section transfer, we
introduce the analogues of the optimal and naive predictions. For the naive
case, we use the empirical mean $\hat{\theta}_0^n := \frac{1}{m} \sum_{i=1}^m
p_{0, i} \eqdef \bar{p}_0$. For the optimal case, we use the result from
\cref{thm:one-slow-sol}, in which we replace $s_1$ with $\bar{p}_0$, which
results in estimator $\hat{\prm}^*_0$.

\myparagraph{Bias and Shift} \cref{fig: bias_shift} illustrates the bias and shift of $\hat{\theta}^*_0$
with one standard deviation confidence intervals.
%, for positive and negative values of $\alpha$, and two values of $m$.
Notice that for $\alpha > 0$, the confidence intervals shrink very fast with
$m$ for big values of $p_0$ due to the shrinking introduced by $\clip$
function.

%\nikita{Do we need such a detailed discussion here?}
%Specifically, when clipping does not occur, there are two sources of
%randomness affecting the bias and the shift: (1) the sample mean $\bar{p}_0$
%coming from the value of the estimator $\hat{\theta}_0^\ast$ and (2) the
%probability that this sample mean is in a region where clipping is not
%necessary. In contrast, when clipping does occur, only the second source of
%randomness is present.

%\begin{equation}
%    \hat{\theta}_0^\ast =
%    \begin{cases}
%        \clip\Par*{\frac{(1 - \abs{\alpha}) \overline{p_0}}{1 - 2 \alpha},
%        -\frac{1}{2}, \frac{1}{2}}, & 1 - 2 \alpha > 0,\\
%        \frac{\sign(\overline{p_0})}{2}, & 1 - 2 \alpha \le 0,
%    \end{cases}
%\end{equation}
%which is obtained by the optimal solution for the performative risk minimisation problem in the case of full information (Theorem \ref{thm:one-slow-sol}), by estimating $p_0$ with $\overline{p_0}$. 
%
%This naive estimator corresponds to the case where the learner is unaware of the performative effect of the deployment of models and estimates the unknown probability $p_0$ using the standard sample mean. However, if the learner recognizes the performative structure of the framework, we propose a more appropriate estimator. In this approach, we use the optimal solution to the full-information optimization problem, $\theta_0^\ast(p_0)$, and we substitute the sample mean $\overline{p_0}$ for the unknown parameter $p_0$.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% EXPECTATION%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{lemma}
%For the estimator $\hat{\theta}_0^\ast$, we have that
%\begin{equation*}
%\mathbb{E}[\hat{\theta}_0^\ast] = 
%\begin{cases}
%\frac{(1 - |\alpha|) p_0}{1-2\alpha} & \alpha \in (-1, 0]\\
%\frac12 - F_{m, p_0 + \frac12} (\frac{m}{2}) & \alpha \in [0.5, 1)\\
%\sum_{x \in I} \big(\frac{1 - \alpha}{1-2\alpha}\big) \big(\frac{x}{m} - \frac12 \big) p(x) & \\
%+ \frac12  - \frac12 F_{m, p_0 + \frac12}\big( \frac{2 - 3\alpha}{2-2\alpha}m \big) & \\
% - \frac12 F_{m, p_0 + \frac12}\big( \frac{\alpha }{2-2\alpha}m\big)  & \alpha \in (0, 0.5)
%\end{cases}
%\end{equation*}
%and 
%\begin{equation*}
%\mathbb{E}[(\hat{\theta}_0^\ast)^2] = 
%\begin{cases}
%\big( \frac{1-|\alpha|}{1-2\alpha}\big)^2 \big( \frac{0.25 - p_0^2}{m} + p_0^2 \big) & \alpha \in (-1, 0]\\
%\frac14 & \alpha \in [0.5, 1)\\
%\sum_{x \in I} \big(\frac{1 - \alpha}{1-2\alpha}\big)^2 \big(\frac{x}{m} - \frac12 \big)^2 p(x)  & \\
%+ \frac14  - \frac14 F_{m, p_0 + \frac12}\big( \frac{2 - 3\alpha}{2-2\alpha}m \big) & \\
%+ \frac14 F_{m, p_0 + \frac12}\big( \frac{\alpha }{2-2\alpha}m\big)  & \alpha \in (0, 0.5) 
%\end{cases}
%\end{equation*}
%where $I$ is the set of integers in $\bigg[\frac{\alpha m}{2-2\alpha}, \frac{(2-3\alpha)m}{2-2\alpha} \bigg]$, $F_{m, p_0 + %\frac12} (x) := \sum_{k=0}^{\lfloor x \rfloor} p(x),$ and 
%\begin{align*}
%p(x) := \binom{m}{x} \bigg(\frac12 + p_0\bigg)^x \bigg(\frac12 - p_0\bigg)^{m-x}
%\end{align*}
%\end{lemma}
%
%\myparagraph{Discussion} The value of $\mathbb{E}[\hat{\theta}_0^\ast \mid \alpha \in (0, 0.5)]$, as presented in the lemma above, can be expressed as follows:
%\begin{align*}
%	\mathbb{E}\bigg[ \frac{(1-|\alpha|)\overline{p_0}}{1-2\alpha}  \chi_{\{\overline{p_0} \in A\}} \bigg] + \frac12 \mathbb{P}[\overline{p_0} \in B  ] - \frac{1}{2} \mathbb{P}[\overline{p_0} \in C ]
%\end{align*}
%where $A$ denotes the event that $\hat{\theta}_0^\ast$ has not been clipped, $B$ represents the event that it has been clipped from above, and $C$ represents the event that it has been clipped from below. As $m \to \infty$, this expression converges to $\mathbb{E}[\hat{\theta}_0^\ast \mid \alpha \in (0, \frac{1}{2})]$. This result follows from: (1) the law of large numbers, which ensures that $\overline{p_0} \to p_0$ almost surely as $m \to \infty$, and (2) the dominated convergence theorem. A similar argument can be applied to the remaining terms of the equations for $\mathbb{E}[\hat{\theta}_0^\ast]$ and $\mathbb{E}[(\hat{\theta}_0^\ast)^2]$, with further details available in Appendix \ref{}. Therefore, we arrive at the following asymptotic results:
%\begin{align*}
%\lim_{m \to \infty} \mathbb{E}[\hat{\theta}_0^\ast] = \theta_0^\ast, \quad \lim_{m \to \infty} \mathbb{E}[(\hat{\theta}_0^\ast)^2] = (\theta_0^\ast)^2.
%\end{align*} 

\myparagraph{Loss} Now, we analyze the loss of $\hat{\prm}^*_0$.

\begin{theorem}
    \label{theorem: expected_loss}
    The expected loss of $\hat{\theta}_0^\ast$ for $\alpha \le 0$ is
    %\[
    %    \begin{split}
    %        \E&{}_{z \sim D^*_1}(\hat{\theta}^*_0 - z)^2) =\\
    %        & [\alpha \in (0, \nicefrac{1}{2})] \Bigl\{ \sum_{x \in I} ((1 -
    %        2\alpha) g(x) - 2(1 - \abs{\alpha})) g(x) p(x)\\
    %        & + \frac{2\alpha - 1 + (1 - \abs{\alpha})p_0}{2} \Par[\Big]{F
    %        \Par[\Big]{\frac{2 - 3\alpha}{2 - 2\alpha}m} - 1}\\
    %        & - \frac{1 - 2\alpha + (1 - \abs{\alpha})p_0}{2}
    %        F\Par[\Big]{\frac{\alpha m}{2 - 2\alpha}} \Bigr\}\\
    %        & + [\alpha \le 0] \Par[\Big]{\frac{(1 -
    %        \abs{\alpha})^2}{1-2\alpha} \Par[\Big]{\frac{1 - 4(m+1)p_0}{4m}} +
    %        \frac{1}{4}}\\
    %        & + [\alpha \ge \nicefrac{1}{2}] (1 - \alpha)
    %        \Par[\Big]{p_0 \Par[\Big]{2 F \Par[\Big]{\frac{m}{2}} - 1} +
    %        \frac{1}{2}},
    %    \end{split}
    %\]
    \[
        \E_{z \sim D^*_1}((\hat{\theta}^*_0 - z)^2) = \frac{(1 -
        \abs{\alpha})^2}{1-2\alpha} \Par[\Big]{\frac{1 - 4(m+1)p_0}{4m}} +
        \frac{1}{4}.
    \]
    %where $I$ is the set of integers in $[\frac{\alpha m}{2-2\alpha},
    %\frac{(2-3\alpha)m}{2-2\alpha}]$, $F(x)$ and $p(x)$ are the CDF and PMF of
    %$\Bin(m, p_0 + 1/2)$, $g(x) \defeq (\frac{1-\alpha}{1-2\alpha})
    %(\frac{x}{m} - \frac{1}{2})$, and $[\cdot]$ is the indicator function.
%\[
%\mathbb{E}\left[\left(\hat{\theta}_0^\ast - z\right)^2\right] =
%\]
%\[
%\begin{cases}
%\frac{(1 - |\alpha|)^2}{1-2\alpha} \left( \frac{1 - 4(m+1)p_0}{4m} \right) + \frac{1}{4}, & \alpha \leq 0, \\[10pt]
%(1 - \alpha) \left( p_0 \left( 2 F_{m, p_0 + \frac{1}{2}}\left( \frac{m}{2} \right) - 1 \right) + \frac{1}{2} \right), & \alpha %\geq \frac{1}{2}, \\[10pt]
%\begin{aligned}
%& \sum_{x \in I} \left( (1 - 2\alpha)g(x)^2 - 2(1 - |\alpha|)g(x) \right) p(x) \\
%& + \frac{2\alpha - 1 + (1 - |\alpha|)p_0}{2} \left( F_{m, p_0 + \frac{1}{2}}\left( \frac{2 - 3\alpha}{2 - 2\alpha}m \right) - 1 %\right) \\
%& - \frac{1 - 2\alpha + (1 - |\alpha|)p_0}{2} F_{m, p_0 + \frac{1}{2}}\left( \frac{\alpha m}{2 - 2\alpha} \right),
%\end{aligned} & \alpha \in (0, \frac{1}{2}).
%\end{cases}
%\]
    For all values of $\alpha$, the expected loss converges to the
    optimal expected loss: $\lim_{m \to \infty} \E((\hat{\theta}_0^\ast - z)^2) =
        \E(({\theta}_0^\ast - z)^2).$
\end{theorem}

We discuss the loss for all values of $\alpha$ in \cref{sec:proofs}. To visualize the results of \cref{theorem: expected_loss}, we plot the
difference in expected losses between $\hat{\theta}_0^\ast$ and
$\hat{\theta}_0^n$ in \cref{fig: loss}. Due to random sampling, we observe a region where $\hat{\theta}_0^n$ outperforms
$\hat{\theta}_0^\ast$. However,
for larger values of $m$, this region diminishes.

%\nikita{I highly doubt that it happens because of non-smooth changes. If this
%hypothesis was true than $\hat{\prm}^*_0$ would underperform in the clipping
%region. I think that $\hat{\prm}^*_0$ underperforms simply because it has
%higher Lipschitz constant in the region of small $p_0$.}
%This phenomenon again occurs due
%to the non-smoothness in $\hat{\prm}^*_0$, which lead to worse performance for
%the performative estimator in this region.


\begin{figure}[ht]
    \includegraphics[width=\linewidth]{figures/heatmap.png}
    \caption{The dependence of the differences in expected
    losses, $\E(\loss(\hat{\prm}^*_0) - \loss(\hat{\prm}^n_0))$, on $p_0$ and
    $\alpha$, for different $m$.}
    \label{fig: loss}
\end{figure}
