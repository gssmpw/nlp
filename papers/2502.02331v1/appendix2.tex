\section{Additional Results for $T=1$, imperfect information case}
\label{sec: appendix2}
\subsection{Bias and Mean Shift Theoretical Results}
The bias for the naive estimator $\hat{\theta}_0^n$ is given by
\begin{equation*}
	\bias_0^n = p_0(|\alpha| - \alpha).
\end{equation*}
For the performative estimator $\hat{\theta}_0^\ast$, the bias is 
\begin{equation*}
	\bias_0^\ast= (1 - \alpha) \mathbb{E}[\hat{\theta}_0^\ast] - (1 - |\alpha|) p_0.
\end{equation*}

For the naive estimator $\hat{\theta}_0^n$ the mean shift is  
\begin{equation*}
	\shift_1^n = p_0(\alpha - |\alpha|) = - \bias_0^n,
\end{equation*}
and for the performative estimator $\hat{\theta}_0^\ast$, we have
\begin{equation*}
\shift_1^\ast = \alpha \E[\hat{\theta}_0^\ast] - |\alpha|p_0.
\end{equation*}

\subsection{Proof of Lemma \ref{lemma: bias-variance}}
\begin{proof}
Using conditional expectation we have
\begin{align*}
\E[(\theta_t - z)^2 \mid \theta_t, p_t^{test}] &= \E[\theta_t^2 - 2 \theta_t  z + z^2 \mid  \theta_t, p_t^{test}]\\
&= \theta_t^2 - 2 \theta_t \E[z \mid \theta_t, p_t^{test}] + \E[z^2 \mid \theta_t, p_t^{test}]\\	
&= \theta_t^2 - 2 \theta_t p_t^{test} + \frac14\\
&= (\theta_t^2 - p_t^{test})^2 + \frac14 - (p_t^{test})^2
\end{align*}
%\begin{align*}
%\E[(\theta_0 - z)^2 ] &= \E[\E[ \theta_0^2 - 2 \theta_0 z_0 + z_0^2 | \theta_0]]\\
%&= \E[\theta_0^2 - 2\theta_0 \E[z_0 | \theta_0] + \E[z_0^2 | \theta_0]]\\
%&= \E[\theta_0^2 - 2 \theta_0 \E[z_0 | \theta_0] + (\E[z_0 | \theta_0])^2 - (\E[z_0 | \theta_0])^2 + \E[z_0^2 | \theta_0]]\\
%&= \E[ (\theta_0 - \E[z_0 | \theta_0])^2] + \E[\E[z_0^2 | \theta_0] - (\E[z_0 | \theta_0])^2]\\
%&= \E[(\theta_0 - p_1(\theta_0))^2] + \E[\mathbb{V}[z_0 | \theta_0]]
%\end{align*}
\end{proof}

\subsection{Expected Loss Theoretical Results} \label{sec: expected_loss_additional}
Before computing the expected loss, we first show the following result regarding the first two moments of the performative estimator: 

\begin{lemma}[Moments of the Performative Estimator] \label{lemma: moments}
	For the performative estimator $\hat{\theta}_0^\ast$, we have that the first two moments are given by
\begin{equation*}
\E[\hat{\theta}_0^\ast] = 
\begin{cases}
\frac{(1 - |\alpha|) p_0}{1-2\alpha} & \alpha \in (-1, 0]\\
\frac12 - F_{m, p_0 + \frac12} (\frac{m}{2}) & \alpha \in [0.5, 1)\\
\sum_{x \in I} \big(\frac{1 - \alpha}{1-2\alpha}\big) \big(\frac{x}{m} - \frac12 \big) p(x) & \\
+ \frac12  - \frac12 F_{m, p_0 + \frac12}\big( \frac{2 - 3\alpha}{2-2\alpha}m \big) & \\
 - \frac12 F_{m, p_0 + \frac12}\big( \frac{\alpha }{2-2\alpha}m\big)  & \alpha \in (0, 0.5)
\end{cases}
\end{equation*}
and 
\begin{equation*}
\E[(\hat{\theta}_0^\ast)^2] = 
\begin{cases}
\big( \frac{1-|\alpha|}{1-2\alpha}\big)^2 \big( \frac{0.25 - p_0^2}{m} + p_0^2 \big) & \alpha \in (-1, 0]\\
\frac14 & \alpha \in [0.5, 1)\\
\sum_{x \in I} \big(\frac{1 - \alpha}{1-2\alpha}\big)^2 \big(\frac{x}{m} - \frac12 \big)^2 p(x)  & \\
+ \frac14  - \frac14 F_{m, p_0 + \frac12}\big( \frac{2 - 3\alpha}{2-2\alpha}m \big) & \\
+ \frac14 F_{m, p_0 + \frac12}\big( \frac{\alpha }{2-2\alpha}m\big)  & \alpha \in (0, 0.5) 
\end{cases}
\end{equation*}
where $I$ is the set of integers in $\big(\frac{\alpha m}{2-2\alpha}, \frac{(2-3\alpha)m}{2-2\alpha} \big]$, $F_{m, p_0 + \frac12} (x) := \sum_{k=0}^{\lfloor x \rfloor} p(x),$ and 
\begin{equation*}
p(x) := \binom{m}{x} \bigg(\frac12 + p_0\bigg)^x \bigg(\frac12 - p_0\bigg)^{m-x}
\end{equation*}
\end{lemma}
\begin{proof}[Proof of lemma \ref{lemma: moments}]
Recall that $\hat{\theta}_0^\ast$ is given by
\[
        \prm^*_0 =
        \begin{cases}
            \clip\Par[\big]{\frac{(1 - \abs{\alpha}) \overline{p_0}}{1 - 2 \alpha},
            -\frac{1}{2}, \frac{1}{2}}, & 1 - 2 \alpha > 0,\\
            \sign(\overline{p_0}) / 2, & 1 - 2 \alpha \le 0.
        \end{cases}
\]
We consider three cases for the value of $\alpha$: 
\begin{enumerate}[(i)]
\item $\alpha \in (-1, 0]$

In this case we have 
\begin{equation*}
	\prm^\ast_0 = \frac{1 - |\alpha|}{1-2\alpha} \overline{p_0}
\end{equation*}
and therefore 
\begin{equation*}
	\E[\prm^\ast_0] = \frac{1 - |\alpha|}{1-2\alpha} \overline{p_0}, \quad \E[(\prm^\ast_0)^2] = \bigg( \frac{1 - |\alpha|}{1-2\alpha}\bigg)^2\E[\overline{p_0}^2] = \bigg( \frac{1 - |\alpha|}{1-2\alpha}\bigg)^2\bigg( p_0^2 + \frac{\frac14 - p_0^2}{m}\bigg),
\end{equation*}
where we have used that $p_{0, i} \sim D_0$ for $i=1, \dots, m$, and thus $p_{0,i} + \frac12$ follows a Bernoulli distribution with parameter $p_0 + \frac12$. 

\item $\alpha \in [0.5, 1)$

In this case, we have that 
\begin{equation*}
\prm^\ast_0 = 
\begin{cases}
\frac12 & \overline{p_0} \ge 0\\
-\frac12 & \overline{p_0} <  0.
\end{cases}
\end{equation*}
Since $\overline{p_0} = \overline{q} - \frac12$, where $\overline{q} := \frac{1}{m}\sum_{i=1}^m q_i$ and $q_i := p_{0, i}$, so that $q_i \sim Bern(p_0 + \frac12)$, we know that the events can be written as
\begin{align*}
	\{ \overline{p_0} \ge 0 \} = \{ \overline{q} \ge 0.5 \}, \quad \{ \overline{p_0} < 0 \} = \{ \overline{q} < 0.5 \}.
\end{align*}
Therefore 
\begin{align*}
\prm^\ast_0 = \frac12 \chi_{\{ \overline{q} \ge 0.5 \}} - \frac12 \chi_{\{ \overline{q} < 0.5 \}}.
\end{align*}
Finally, using the law of total expectation, we get that
\begin{align*}
\E[\prm^\ast_0] &= \E[\prm^\ast_0 | \overline{q} \ge 0.5] \mathbb{P}[\overline{q} \ge 0.5] + \E[\prm^\ast_0 |\overline{q} < 0.5] \mathbb{P}[\overline{q} < 0.5]\\
&= \frac12 \mathbb{P}[\overline{q} \ge 0.5] - \frac12 \mathbb{P}[\overline{q} < 0.5]\\
&= \frac12 - F_{m, p_0 + \frac12}(0.5m),
\end{align*}
where we have used that $m \overline{q} \sim Bin(m ,p_0 + 0.5)$. Similarly for the second moment 
\begin{align*}
\E[(\prm^\ast_0)^2] &= \E[(\prm^\ast_0)^2 | \overline{q} \ge 0.5] \Pr[\overline{q} \ge 0.5] + \E[(\prm^\ast_0)^2 |\overline{q} < 0.5] \Pr[\overline{q} < 0.5]\\
&= \frac14 \Pr[\overline{q} \ge 0.5] + \frac14 \Pr[\overline{q} < 0.5]\\
&= \frac14.
\end{align*}
\item $\alpha \in (0, 0.5)$

In this case we have 
\begin{align*}
\prm^\ast_0 &= 
\begin{cases}
\frac{1 - \alpha}{1-2\alpha} \overline{p_0}, & \text{if } \overline{p_0} \in \big(- \frac{1-2\alpha}{2 - 2\alpha}, \frac{1-2\alpha}{2 - 2\alpha} \big] =: A\\
\frac12 , & \text{if } \overline{p_0} > \frac{1-2\alpha}{2 - 2\alpha} =: B\\
-\frac12 , & \text{if } \overline{p_0} \le - \frac{1-2\alpha}{2 - 2\alpha} =: C
\end{cases}
\end{align*}
where we have denoted by $A,B,C$ the random events that we have not clipped the value of the performative estimator, that we have clipped it from above or that we have clipped in from below. Using the law of total expectation, we have 
\begin{align*}
\E[\prm^\ast_0] &= \E[\prm^\ast_0 | A] \Pr[A] + \E[\prm^\ast_0 | B] \Pr[B] + \E[\prm^\ast_0 | C] \Pr[C]\\
&= \E[\prm^\ast_0 \chi_{A}] + \frac12 \Pr[B] - \frac12 \Pr[C]\\
&= \E[\prm^\ast_0 \chi_{A}] + \frac12 \Pr\bigg[\overline{q} > \frac{2-3\alpha}{2-2\alpha}\bigg] - \frac12 \Pr\bigg[\overline{q} \le \frac{\alpha}{2-2\alpha}\bigg]
\end{align*}
The first term can be computed as follows
\begin{align*}
\E[\prm^\ast_0 \chi_{A}] &= \sum_{x \in I} \frac{1 - \alpha}{1-2\alpha} \bigg( \frac{x}{m} - \frac12 \bigg) p(x),
\end{align*}
where we have used that $m\overline{p_0} + m/2 \sim Bin(m, p_0 + \frac12)$ and have denoted by $p(x)$ the PMF of $Bin(m, p_0 + \frac12$. The last two terms are easily expressed via the CDF of the same distribution, giving us that 
\begin{align*}
\E[\prm^\ast_0] = \sum_{x \in I} \bigg(\frac{1 - \alpha}{1-2\alpha}\bigg) \bigg(\frac{x}{m} - \frac12 \bigg) p(x) 
+ \frac12  - \frac12 F_{m, p_0 + \frac12}\bigg( \frac{2 - 3\alpha}{2-2\alpha}m \bigg)
 - \frac12 F_{m, p_0 + \frac12}\bigg( \frac{\alpha }{2-2\alpha}m\bigg).
\end{align*}
where $I$ is the set of integers in the interval $( \frac{\alpha }{2 - 2\alpha}m, \frac{2-3\alpha}{2 - 2\alpha} m]$. Similarly, for the second moment we have that 
\begin{align*}
\E[(\prm^\ast_0)^2] &= \E[(\prm^\ast_0)^2 | A] \Pr[A] + \E[(\prm^\ast_0)^2 | B] \Pr[B] + \E[(\prm^\ast_0)^2 | C] \Pr[C]\\
&= \E[(\prm^\ast_0)^2 \chi_{A}] + \frac14 \Pr[B] + \frac14 \Pr[C]\\
&= \E[(\prm^\ast_0)^2 \chi_{A}] + \frac14 \Pr\bigg[\overline{q} > \frac{2-3\alpha}{2-2\alpha}\bigg] - \frac12 \Pr\bigg[\overline{q} \le \frac{\alpha}{2-2\alpha}\bigg]\\
&= \sum_{x \in I} \bigg(\frac{1 - \alpha}{1-2\alpha}\bigg)^2 \bigg(\frac{x}{m} - \frac12 \bigg)^2 p(x)  
+ \frac14  - \frac14 F_{m, p_0 + \frac12}\bigg( \frac{2 - 3\alpha}{2-2\alpha}m \bigg) 
+ \frac14 F_{m, p_0 + \frac12}\bigg( \frac{\alpha }{2-2\alpha}m\bigg),
\end{align*}
which finishes the proof.
\end{enumerate}
\end{proof}

We now present a result that generalizes Theorem \ref{theorem: expected_loss}, offering theoretical insights for all possible values of $\alpha \in (-1,1)$.
\begin{theorem} \label{theorem: expected_loss_full}
For the naive estimator $\hat{\theta}_0^n$ the expected loss is 
\begin{equation*}
\E_{z \sim D_1^{test}}[(\hat{\theta}_0^n - z)^2] = p_0^2 (2 |\alpha| - 2\alpha - 1) + \frac{(\frac12 - p_0)(\frac12 + p_0)}{m} + \frac14,
\end{equation*}
and for the performative estimator $\hat{\theta}_0^\ast$, we have 
\begin{align*}
\E[(\hat{\theta}_0^\ast - z)^2] = 
\begin{cases}
\frac{(1 - |\alpha|)^2}{1-2\alpha} \bigg( \frac{\frac14 - p_0^2}{m} - p_0^2 \bigg) + \frac14 & \alpha \in (-1, 0]\\
p_0 (1 - |\alpha|) \big(2F_{m, p_0 + \frac12}(\frac{m}{2}) - 1\big) + \frac{1-\alpha}{2} & \alpha \in [0.5, 1)\\
\sum_{x \in I}\left( (1 - 2\alpha)g(x)^2 - 2(1 - |\alpha|)g(x) \right) p(x) + (p_0(1-|\alpha|) - \frac{1-2\alpha}{4})F_{m, p_0 + \frac{1}{2}}\left( \frac{2 - 3\alpha}{2 - 2\alpha}m \right) & \\
 + (p_0(1-|\alpha|) + \frac{1-2\alpha}{4})F_{m, p_0 + \frac{1}{2}}\left( \frac{\alpha m}{2 - 2\alpha} \right) - p_0 (1-|\alpha|) - \frac{1-\alpha}{2}& \alpha \in (0, 0.5)
\end{cases}
\end{align*}
Asymptotically, we have that as $m \to \infty$
\begin{equation*}
\E[(\hat{\theta}_0^\ast - z)^2] \to \verb|loss|_0^\ast
\end{equation*}
i.e. as $m$ goes to infinity, $\hat{\theta}_0^\ast$ approaches the optimal estimator for the risk minimisation problem. 
\end{theorem}
%\subsection{Proof of Lemma \ref{lemma: optimal_theta_1}}
%\begin{lemma}\label{lemma: optimal_theta_1}
%	Suppose that $p_0, \alpha \ge 0$ are known. Then the solution to the one-step performative risk minimisation problem 
%	\begin{equation*}
%	\min_{\theta \in [0, 1]} \E_{z \sim Bern(p_1(\theta))}[(\theta - z)^2]
%	\end{equation*}
%	is given by
%	\begin{equation*}
%	\theta^\ast(p_0) = 
%	\begin{cases}
%        \frac{-\alpha + (2 - 2\alpha)p_0}{2-4\alpha}, &\text{\normalfont if } A\\
%        1, &\text{\normalfont if }  B\\
%        0, &\text{\normalfont if } C
%    \end{cases}
%	\end{equation*}
%	where the events $A, B, C$ are defined as
%	\begin{align*}
%		A &= \{ \alpha < \frac12, p_0 \in[\frac{\alpha}{2(1-\alpha)}, \frac{2-3\alpha}{2-2\alpha}]  \}\\
%		B &= \{ \alpha < \frac12, p_0 \ge \frac{2-3\alpha}{2-2\alpha} \}  \cup \{ \alpha \in [\frac12, 1), p_0 \ge \frac12 \} \\
%		C &= \{ \alpha < \frac12, p_0 \le \frac{\alpha}{2(1-\alpha)} \} \cup  \{ \alpha \in [\frac12, 1), p_0 < \frac12 \} 
%	\end{align*}
%\end{lemma}
%\begin{proof}
%	Since the value of $p_0$ is known, any data we observe should not affect the value of $\theta$. Thus we can consider $\theta$ as a constant here and the optimisation problem from lemma \ref{lemma: bias-variance} becomes
%	\begin{equation*}
%		\min_{\theta \in [0, 1]} (1-2\alpha)\theta^2 + (\alpha - 2(1-\alpha)p_0) \theta + (1-\alpha)p_0.
%	\end{equation*}
%
%    We consider three possible cases: $(i)$ $\alpha = \frac12$, $(ii)$ $\alpha < \frac12$, and $(iii)$ $\alpha > \frac12$. 
%    \begin{enumerate}[(i)]
%        \item If $\alpha = \frac12$, then the minimisation problem becomes
%        \begin{equation}
%            \min_{\theta \in [0,1]} \bigg(\frac12 - p_0\bigg) \theta + \frac{p_0}{2}
%        \end{equation}
%        The solution to this is 
%        \begin{equation}
%            \theta^\ast = 
%            \begin{cases}
%                0, & \text{if } p_0 \le \frac12 \\
%                1, & \text{if } p_0 \ge \frac12.
%            \end{cases}
%        \end{equation}
%        \item If $\alpha < \frac12$, then the objective function is convex, and since $\theta$ is a constant in $[0, 1]$, we have the following optimisation problem:
%        \begin{align*}
%            \min_{\theta} \quad & (1-2\alpha)\theta^2 + (\alpha - 2(1-\alpha) p_0) \theta + (1-\alpha)p_0\\
%            s.t. \quad & \theta \le 1 \quad \text{and} \quad -\theta \le 0
%        \end{align*}
%        The Lagrangian is given by 
%        \begin{equation}
%            \mathcal{L} := (1-2\alpha)\theta^2 + (\alpha - 2(1-\alpha) p_0) \theta + (1-\alpha)p_0 + \lambda_1 (\theta - 1) - \lambda_2 \theta
%        \end{equation}
%        Using the KKT conditions, we know that $\theta^\ast$ is an optimal solution to the minimisation problem if the following system is satisfied
%        \begin{align*}
%            \frac{\partial \mathcal{L}}{\partial \theta} &= 0\\
%            \lambda_1 (\theta - 1) &= 0\\
%            \lambda_2 \theta &= 0
%        \end{align*}
%        where $\lambda_1 \ge 0, \lambda_2 \ge 0$. We divide this into three cases:
%    
%    \begin{enumerate}
%        \item $\theta^\ast = 1, \lambda_2 = 0$. In this case we have 
%        \begin{align*}
%            \frac{\partial \mathcal{L}}{\partial \theta} &= 2(1-2\alpha) + (\alpha - 2(1-\alpha)p_0) + \lambda_1 = 0 \implies \\
%            \lambda_1 &= 2(1-\alpha)p_0 - \alpha) - 2(1-2\alpha)
%        \end{align*}
%        This is non-negative if and only if 
%        \begin{equation*}
%            p_0 \ge \frac{2-3\alpha}{2-2\alpha}.
%        \end{equation*}
%        \item $\theta^\ast = 0, \lambda_1 = 0$. In this case we have
%        \begin{align*}
%            \frac{\partial \mathcal{L}}{\partial \theta} &= (\alpha - 2(1-\alpha)p_0) - \lambda_2 = 0 \implies \\
%            \lambda_2 &= (\alpha - 2(1-\alpha)p_0)
%        \end{align*}
%        We have that $\lambda_2$ is non-negative if and only if 
%        \begin{equation*}
%            p_0 \le \frac{\alpha}{2-2\alpha}.
%        \end{equation*}
%        \item $\lambda_1 = 0, \lambda_2 = 0$. In this case we have 
%        \begin{align*}
%            \frac{\partial \mathcal{L}}{\partial \theta} &= 2(1-2\alpha)\theta + (\alpha - 2(1-\alpha)p_0) = 0 \implies \\
%            \theta^\ast &= \frac{2(1-\alpha)p_0 - \alpha}{2 - 4\alpha}
%        \end{align*}
%        Since this needs to be in $[0, 1]$, this solution is feasible only when 
%        \begin{equation*}
%            \frac{\alpha}{2 - 2\alpha} \le p_0 \le \frac{2-3\alpha}{2-2\alpha}
%        \end{equation*}
%    \end{enumerate}
%    Thus we have the following solution in the $\alpha < \frac12$ case:
%    \begin{equation}
%    \theta^\ast = 
%        \begin{cases}
%            1, & p_0  \ge \frac{2-3\alpha}{2-2\alpha}, \alpha < \frac12\\
%            0, & p_0 \le \frac{\alpha}{2-2\alpha}, \alpha < \frac12\\
%            \frac{2(1-\alpha)p_0 - \alpha}{2 - 4\alpha}, & \frac{\alpha}{2 - 2\alpha} \le p_0 \le \frac{2-3\alpha}{2-2\alpha}, \alpha < \frac12
%        \end{cases}
%    \end{equation}
%    \item When $\alpha > \frac12$, the objective function is concave and thus the minimum is obtained at one of the boundary points, i.e. we have 
%    \begin{equation}
%        \theta^\ast = 
%        \begin{cases}
%            0, & \text{if }p_0 \le \frac12, \alpha \ne 1\\
%            1, & \text{if }p_0 \ge \frac12, \alpha \ne 1\\
%        \end{cases}
%    \end{equation}
%    where if $p_0 = \frac12$ both values for $\theta$ achieve the same optimal solution for the objective. Finally, if $\alpha = 1$, then both values $0$ and $1$ are again optimal for $\theta$. 
%    \end{enumerate}
%\end{proof}
\subsection{Proof of Theorem \ref{theorem: expected_loss_full}}
\begin{proof}[Proof of Theorem \ref{theorem: expected_loss}]
We begin by rewriting the expected loss as follows 
%\begin{lemma}[Expected loss] \label{lemma: expected loss}
%For a deployed model $\theta_0$, the expected loss is given by
%\begin{align*}
%		&\E[(\theta_0 - z)^2] = (1-2\alpha)\E[\theta_0^2] - 2 (1 - |\alpha|)p_0 \E[\theta_0] + \frac14,
%	\end{align*}
%	where the expectation is only in terms of the randomness of the observations $\{p_{0, i} \}_{i=1}^m$.
%\end{lemma}
%\begin{proof}[Proof of lemma \ref{lemma: expected loss}]
\begin{align*}
\E[(\theta_0 - z_0)^2] &= \E[\E[\theta_0^2 - 2\theta_0 z_0 + z_0^2 | \theta_0]]\\
&= \E[\theta_0^2 - 2 \theta_0 \E[z_0 | \theta_0] + \E[z_0^2 | \theta_0]]\\
&= \E\bigg[\theta_0^2 - 2 \theta_0 p_1(\theta_0) + \frac14\bigg]\\
&=(1-2\alpha)\E[\theta_0^2] - 2 (1 - |\alpha|)p_0 \E[\theta_0] + \frac14
\end{align*}
where the expectation is only in terms of the randomness of the observations $\{p_{0, i} \}_{i=1}^m$.
%\end{proof}
For the naive estimator, $\hat{\theta}_0^n$, we have that the first two moments are 
\begin{align*}
\E[\hat{\theta}_0^n] & = p_0\\
\E[(\hat{\theta}_0^n)^2] &= p_0^2 + \frac{(\frac12 - p_0)(\frac12 + p_0)}{m},
\end{align*} 
which follows since $p_{0,i} \sim D_0$ for $i = 1, \dots, m$. Therefore, we get
\begin{align*}
\E[(\hat{\theta}_0^n - z)^2] &=  
(1-2\alpha)\E[\theta_0^2] - 2 (1 - |\alpha|)p_0 \E[\theta_0] + \frac14\\
&= p_0^2 (2 |\alpha| - 2\alpha - 1) + \frac14 + \frac{(2\alpha - 1)(4 p_0^2 - 1)}{4m}
\end{align*}

For the performative estimator, we use the first and second moments of $\hat{\theta}_0^\ast$ from lemma \ref{lemma: moments} to obtain
\begin{align*}
\E[(\hat{\theta}_0^\ast - z)^2] = 
\begin{cases}
\frac{(1 - |\alpha|)^2}{1-2\alpha} \bigg( \frac{\frac14 - p_0^2}{m} - p_0^2 \bigg) + \frac14 & \alpha \in (-1, 0]\\
p_0 (1 - |\alpha|) \big(2F_{m, p_0 + \frac12}(\frac{m}{2}) - 1\big) + \frac{1-\alpha}{2} & \alpha \in [0.5, 1)\\
\sum_{x \in I}\left( (1 - 2\alpha)g(x)^2 - 2(1 - |\alpha|)g(x) \right) p(x) + (p_0(1-|\alpha|) - \frac{1-2\alpha}{4})F_{m, p_0 + \frac{1}{2}}\left( \frac{2 - 3\alpha}{2 - 2\alpha}m \right) & \\
 + (p_0(1-|\alpha|) + \frac{1-2\alpha}{4})F_{m, p_0 + \frac{1}{2}}\left( \frac{\alpha m}{2 - 2\alpha} \right) - p_0 (1-|\alpha|) - \frac{1-\alpha}{2}& \alpha \in (0, 0.5)
\end{cases}
\end{align*}
where $g(x) \defeq (\frac{1-\alpha}{1-2\alpha})(\frac{x}{m} - \frac{1}{2})$.

Asymptotically, as $m \to \infty$, we have that the moments of $\hat{\theta}_0^\ast$ for $\alpha \in (-1, 0]$ are given by:
\begin{align*}
\E[\hat{\theta}_0^\ast] &= \frac{(1-|\alpha|)}{1-2\alpha} p_0 \to \frac{(1-|\alpha|)}{1-2\alpha} p_0\\
\E[(\hat{\theta}_0^\ast)^2] &= \frac{(1-|\alpha|)^2}{(1-2\alpha)^2} \bigg( \frac{0.25 - p_0^2}{m} +  p_0^2\bigg) \to \frac{(1-|\alpha|)^2}{(1-2\alpha)^2} p_0^2
\end{align*}
Similarly, for $\alpha \in [0,5, 1)$, we have 
\begin{align*}
\E[\hat{\theta}_0^\ast] &= \frac12 - F_{m, p_0 + \frac12}\bigg(\frac{m}{2}\bigg) \to \frac{sign(p_0)}{2}\\
\E[(\hat{\theta}_0^\ast)^2] &= \frac14 \to \frac14
\end{align*}
where we have used that the CDF function $F_{m, p_0 + \frac12}\bigg(\frac{m}{2}\bigg)$ converges to $1$ for non-negative $p_0$ and to $0$ for negative $p_0$ as $m\to \infty$. 

Finally, for $\alpha \in (0, 0.5)$, we have that 
\begin{align*}
	\E[\hat{\theta}_0^\ast] &= \E\bigg[\clip{\bigg( \frac{1 - |\alpha| p_0}{1-2\alpha}, -\frac12, \frac12 \bigg)}\bigg]\\
	&= \E\bigg[ \frac{(1-|\alpha|)\overline{p_0}}{1-2\alpha}  \chi_{\{\overline{p_0} \in A\}} \bigg] + \frac12 \Pr[\overline{p_0} \in B  ] - \frac{1}{2} \Pr[\overline{p_0} \in C ]\\
	&\to \frac{(1-|\alpha|){p_0}}{1-2\alpha}  \chi_{\{{p_0} \in A\}} + \frac12 \chi_{[{p_0} \in B  ]} - \frac{1}{2} \chi_{[{p_0} \in C ]}\\
	&= \E[{\theta}_0^\ast \mid \alpha \in (0, 0.5)].
\end{align*}
where $A$ denotes the region (a function of $\alpha$), where $\hat{\theta}_0^\ast$ has not been clipped, $B$ represents the region where it has been clipped from above, and $C$ is the region where it has been clipped from below. The third line follows from: (1) the law of large numbers, which ensures that $\overline{p_0} \to p_0$ almost surely as $m \to \infty$, and (2) the dominated convergence theorem. The same argument applies for $\E[(\hat{\theta}_0^\ast)^2]$. Thus, combining this with the other two cases for $\alpha$, we get the following asymptotic results 
\begin{align*}
\lim_{m \to \infty} \E[\hat{\theta}_0^\ast] = \theta_0^\ast, \quad \lim_{m \to \infty} \E[(\hat{\theta}_0^\ast)^2] = (\theta_0^\ast)^2.
\end{align*}
Therefore, we can conclude that as $m\to \infty$,
\begin{equation*}
\E[(\hat{\theta}_2^\ast - z)^2] \to \verb|loss|_0^\ast.
\end{equation*}
\end{proof}
