\section{Experiments}

\subsection{Compared Methods}
To evaluate the effectiveness of our proposed DRGG framework on the GraphDoc dataset, we conducted experiments comparing it with several state-of-the-art methods in document layout analysis (DLA) and graphical structure analysis (GSA), including \textbf{DETR}~\citep{DETR}, \textbf{Deformable DETR}~\citep{zhu2020deformable}, \textbf{DINO}~\citep{zhang2022dino}, and \textbf{RoDLA}~\citep{chen2024rodla}. These methods represent a broad range of approaches in object detection and relation extraction. We further explore the impact of various backbone architectures, including \textbf{InternImage}~\citep{wang2022internimage}, \textbf{ResNet}~\citep{he2016deepresnet}, \textbf{ResNeXt}~\citep{ResNext}, and \textbf{Swin Transformer}~\citep{liu2021swintransformerhierarchicalvision}, across these models. This allows us to understand the influence of different combination of feature extraction backbones and detector on the overall performance of the models.

\subsection{Implementation Details}
\label{implementation}
For a fair comparison, we train and evaluate all methods in the MMDetection~\citep{mmdetection} framework. All experiments were conducted using the GraphDoc dataset for both training and validation. To evaluate the performance of our proposed end-to-end model, we jointly trained and evaluated the DLA and gDSA tasks without separating them. For the object detector component, we employed the model's original configuration. More details are in Appendix~\ref{appendix:implementation}.

\subsection{Evaluation metrics}
\label{eval_all}
To assess the performance of the models on the DLA and gDSA tasks, we employ a set of evaluation metrics tailored to capture both the layout elements' detection accuracy and the correctness of the predicted relations. For the DLA task, we utilize the mean Average Precision (mAP) at multiple Intersections over Union (IoU) thresholds,i.e., mAP@$50$:$5$:$95$. This metric computes the average precision across IoU thresholds ranging from 0.50 to 0.95 in increments of 0.05. It accounts for both the localization accuracy of the bounding boxes and the classification accuracy of the layout element categories. In the gDSA task, we report mR$_g$ at confidence thresholds of 0.5 and mAP$_g$ at confidence thresholds of 0.5, 0.75, and 0.95. By employing these metrics, we ensure a comprehensive evaluation of both the detection of document layout elements and their complex relational structures, reflecting the real-world challenges of document structure analysis tasks.

\subsection{Results}
In this section, we evaluate our proposed DRGG with several models on the GraphDoc dataset to benchmark DLA and gDSA tasks. More detailed  results of DRGG design are in Appendix~\ref{ablation}.

\noindent\textbf{Document Layout Analysis.} Table~\ref{tab:single_result} presents the results of the DLA task, where we report the mean Average Precision (mAP@$50$:$5$:$95$) for different combinations of backbones and object detectors. Our proposed DRGG framework, integrated with the InternImage backbone and the RoDLA detector, achieves mAP of $81.5\%$, surpassing all other combinations, including the original setup without DRGG. This result highlights the effectiveness of integrating a powerful backbone with a detector specifically optimized for document layout analysis. Among the other detectors evaluated, DINO achieves mAP of $79.5\%$ with the InternImage backbone, showing competitive performance. Deformable DETR and DETR obtain lower mAP scores of $73.4\%$ and $68.2\%$, respectively, indicating challenges in capturing complex document layouts with these models. When analyzing the impact of different backbone networks using the RoDLA in combination with DRGG, the InternImage backbone consistently outperforms others. Specifically, InternImage achieves mAP of $81.5\%$, compared to $77.9\%$ with ResNeXt, $73.7\%$ with Swin Transformer, and $71.0\%$ with ResNet. These results suggest that the advanced feature extraction capabilities of InternImage are crucial for accurately detecting and classifying diverse layout elements in complex documents.

\input{table/result_single}

\noindent\textbf{Graph based Document Structure Analysis.} For the gDSA task, we evaluate the models using mean Recall (mR$_g$@$0.5$) and mean Average Precision at different relation confidence thresholds (mAP$_g$@$0.5$, mAP$_g$@$0.75$, and mAP$_g$@$0.95$). As shown in Table~\ref{tab:single_result}, the combination of InternImage, RoDLA and DRGG achieves superior performance across all metrics. Specifically, it attains a mean recall of $30.7\%$ and the highest mean average precision scores of \textbf{57.6\%} at a 0.5 threshold, $56.3\%$ at 0.75, and 46.5\% at 0.95. Comparatively, other models exhibit significantly lower performance on the gDSA task. DINO, despite performing well on the DLA task, achieves a mean recall of $19.2\%$ and a mean average precision of $25.2\%$ at a 0.5 threshold. Deformable DETR and DETR perform even worse, with mean recalls of $11.5\%$ and $7.1\%$, respectively. These results emphasize the difficulty of accurately predicting relational structures in documents and demonstrate the effectiveness of our proposed DRGG framework in addressing this challenge. Examining different backbones with the RoDLA and DRGG further highlights the importance of the backbone network in gDSA performance. The InternImage backbone consistently yields the best results, with significant margins over ResNeXt, Swin Transformer, and ResNet. This suggests that capturing complex relational information in documents requires not only specialized detectors but also powerful feature extraction capabilities provided by advanced backbone networks.

\noindent \textbf{Relation prediction analysis per category} To gain deeper insights into the model's performance on different types of relations, we present per-category relation detection results in Table~\ref{tab:result_rel_cat}. Our DRGG model with InternImage and RoDLA achieves the highest Average Precision (AP$_g$@$0.5$) across almost all relation categories. For spatial relations, \textit{left} and \textit{right}, the model achieves near-perfect scores of $99.0\%$, indicating exceptional ability to capture spatial positioning between layout elements. In \textit{up} and \textit{down} relations, it attains impressive scores of $49.0\%$ each, outperforming other models by substantial margins. In logical relations, \textit{parent} and \textit{child}, the model achieves scores of $45.5\%$ for both, demonstrating effectiveness in identifying hierarchical structures within documents. For the \textit{sequence} relation, critical for understanding reading order, the model attains an AP of $56.4\%$, significantly higher than other configurations. The \textit{reference} relation remains challenging, with the highest AP being $18.8\%$ achieved by ResNeXt with RoDLA. Our model achieves an AP of $16.8\%$ in this category. The lower performance in \textit{reference} relations suggests that further work is needed to improve the detection of less frequent and more complex relations, possibly by incorporating textual content understanding or additional context. 

\input{table/result_per_cat}