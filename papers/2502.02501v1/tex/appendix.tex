\section{Details of GraphDoc Dataset}
\subsection{Rule-based relation annotation system}
\label{detailed pipeline}
In this subsection, we provide an in-depth explanation of the methodologies employed in our rule-based relation extraction system. This detailed account covers the technical aspects of each step, which were briefly outlined in the main text.

\noindent \textbf{Content Extraction}: We extract textual content from all bounding boxes except those labeled as \textit{Table} and \textit{Picture} by combining Optical Character Recognition (OCR) and direct text extraction from PDF files. Initially, we utilize pdfplumber~\footnote{\url{https://github.com/jsvine/pdfplumber}} to extract text and positional information directly from PDFs, enabling accurate mapping of text snippets to their corresponding bounding boxes. For regions where direct extraction is ineffective—such as scanned documents or encrypted PDFs—we apply Tesseract OCR~\footnote{\url{https://tesseract-ocr.github.io/}} configured with appropriate language settings. By selectively employing OCR only when necessary, we enhance both the efficiency and accuracy of the content extraction process. Integrating both methods ensures comprehensive and reliable retrieval of textual information across various document types and qualities.

\noindent \textbf{Spatial Relation Extraction}: To determine spatial relations in the four cardinal directions, we leverage the non-overlapping property of bounding boxes ensured by the DocLayNet annotation rules. For each bounding box, we calculate its center point and identify the nearest neighboring bounding box in each direction by checking for horizontal and vertical overlaps. If two bounding boxes overlap horizontally, we consider them for \textit{left} or \textit{right} relations; if they overlap vertically, we consider them for \textit{up} or \textit{down} relations. We compute edge distances only when the bounding boxes do not overlap in the respective direction, ensuring accurate neighbor identification. Recording only the nearest neighbor in each direction maintains simplicity and avoids redundancy. This approach efficiently constructs a spatial map of document elements, which is crucial for understanding the layout and for subsequent processes like determining the reading order and building hierarchical structures.

\noindent \textbf{Basic Reading Order}: We establish a basic reading order that mirrors natural human reading patterns. First, we analyze the document layout to determine if it follows a Manhattan (grid-like) or non-Manhattan structure by assessing alignment consistency and spacing uniformity. We then apply the Recursive X-Y Cut algorithm~\citep{xycut} to segment the page hierarchically based on whitespace gaps. This algorithm recursively divides the page into smaller regions, creating a tree structure where leaf nodes correspond to individual bounding boxes. We traverse this tree in a depth-first manner, ordering the content from left to right and top to bottom, adjusted for the document's language and layout specifics. For multi-column layouts, we modify the traversal to process content column by column, respecting the intended flow. This method provides a logical reading sequence that aligns with human expectations and supports tasks like text extraction and summarization.

\noindent \textbf{Hierarchical Structure}: We organize the document elements into a hierarchical structure that reflects their logical relations. Annotations are grouped into four categories: 
\begin{itemize}
    \item Elements with direct structural relations (\textit{Section-Header}, \textit{Text}, \textit{Formula}, \textit{List-Item});
    \item Non-textual content within the logical structure (\textit{Table}, \textit{Picture}, \textit{Caption});
    \item Elements lacking direct associations (\textit{Page-Header}, \textit{Page-Footer}, \textit{Title}); 
    \item References only (\textit{Footnotes})
\end{itemize}

For the first group, we construct the hierarchy by linking each \textit{Section-Header} to the subsequent content elements (\textit{Text}, \textit{Formula}, \textit{List-Item}) that belong to that section, based on the established reading order. Subsections are nested under their respective higher-level sections, creating a tree structure that mirrors the document's outline. For the second group, we associate each \textit{Caption} with its corresponding \textit{Table} or \textit{Picture} based on their proximity in the document. The combined \textit{Table}/\textit{Picture} and \textit{Caption} units are then placed into the hierarchy at positions determined by the reading order, linking them to the relevant sections or subsections. This hierarchical arrangement effectively captures the logical structure of the document, facilitating tasks such as information retrieval and semantic analysis by reflecting the inherent relations among the document elements.

\noindent \textbf{Relation Completion}: Building on the hierarchical structure, we establish \textit{Parent}, \textit{Child}, \textit{Sequence}, and \textit{Reference} relations among the elements. Child nodes under the same parent are connected via sequence relations that reflect the established reading order, with attributes indicating their positional sequence. Reference relations are identified by scanning the text for markers such as citations and footnote indicators, linking them to corresponding elements:
\begin{itemize}
    \item \textit{Footnotes}: Superscript numbers or symbols in the text are linked to \textit{Footnote} elements.
    \item \textit{Tables and Figures}: Mentions, for example, 'see Table 1' are linked to the respective \textit{Table} or \textit{Picture} elements.
\end{itemize}

We exclude \textit{Caption} elements from being directly referenced to avoid redundancy but maintain references within captions to other elements. Consistency and integrity checks are performed to ensure all relations are correctly established, resolving any conflicts based on predefined rules. 

{While documents from various domains may have unique characteristics, adopting a consistent and general rule of relations allows for a unified approach to structure analysis. To address domain-specific nuances and ensure accuracy, we incorporate human verification, which helps adapt our method to diverse document domains while maintaining relation type definition principles. The extensive human verification and refinement cover approximately 58.5\% of the dataset. We reviewed 4,852 pages of Government Tenders, 12,000 pages of Financial Reports, 6,469 pages of Patents, and 8,000 pages from other domains. The refinement rates for relation labels varied across domains: approximately 23\% for Financial Reports, 8\% for Scientific Articles, 26\% for Government Tenders, and 17\% for Patents. Based on our comprehensive cross-validation evaluation, we believe that our dataset is high-quality for the proposed gDSA task. We hope our new dataset and benchmark can provide an innovative advancement in DSA and document an understanding research field.} 


\subsection{Detailed statistics of GraphDoc Dataset}
In this section, we provide detailed statistics on the GraphDoc dataset. Building upon DocLayNet~\citep{doclaynet2022}, GraphDoc extends it with rich relational annotations while maintaining coherence in instance categories and bounding boxes for a comprehensive analysis of document structures. As shown in Figure~\ref{fig:sub9}, spatial relations constitute a significant portion of the relational data, representing more than half of all annotated relations. Of the remaining logical relations, \textit{parent} and \textit{child} and \textit{sequence} relations dominate, while \textit{reference} relations form a comparatively smaller subset. This distribution
highlights the relation dataset appears to be imbalanced, which could easily lead to long-tail problems during model training.

\noindent \textbf{Spatial Relations} in the dataset are dominated by four types: \textit{down}, \textit{up}, \textit{left}, and \textit{right}, each representing the relative positioning of document components. {Spatial relations are essential in document structure analysis because they provide contextual information beyond the raw bounding boxes of document layout elements. Simply knowing the positions of elements is insufficient for understanding the document's relational structure, especially when real-world perturbations occur, e.g., document image rotation and translation. By defining four fundamental spatial relation types, we aim to capture how document elements interact within a document fundamentally, facilitating a more robust and generalized understanding across different domains. }As represented in Figures~\ref{fig:sub1} and~\ref{fig:sub2}, document elements \textit{Section-header} and \textit{Text} commonly follow a vertical arrangement, positioned above \textit{Text}, reflecting a conventional reading order. This vertical structuring is consistent across most document types and contributes to an intuitive user experience when processing document layouts. In addition, as illustrated in Figures~\ref{fig:sub3} and~\ref{fig:sub4}, \textit{left} and \textit{right} relations account for another significant portion of spatial proximity relations. Understanding these left-right positional relations is critical when reconstructing the visual layout during document parsing tasks, as they often indicate the intended grouping of related elements. 

\noindent \textbf{Logical Relations} are essential for understanding both the hierarchical and contextual connections between document layouts. These include \textit{parent}, \textit{child}, \textit{sequence}, and \textit{reference} relations, each contributing to the logical structure within documents. \textit{Parent} and \textit{child} relations define the hierarchical structure of document elements.As observed in Figures~\ref{fig:sub5} and~\ref{fig:sub6}, logical relations provide a clearer horizon compared to spatial relations. \textit{Captions} are primarily the children of \textit{Picture} and \textit{Table}, while \textit{Section-header} often serves as the parent of \textit{Text}, \textit{Formula}, and \textit{List-item}. These relations are fundamental to defining the document's logical structure, as they guide the flow of information and the progression from one element to another. Additionally, \textit{sequence} relations are important for capturing the order in which document components should be read or interpreted. Figure~\ref{fig:sub7} indicates that \textit{sequence} relations mainly occur among \textit{Text}, \textit{List-item}, and \textit{Formula} categories. Figure~\ref{fig:sub8} demonstrates that \textit{reference} relations, while limited in number, are critical for linking different parts of the document. These relations typically appear among \textit{List-item}, \textit{Text}, \textit{Table}, and \textit{Picture} elements, forming cross-references that provide additional context or clarification. While reference relations constitute a smaller fraction of the overall relational data, their significance cannot be overlooked, as they are key to understanding interdependencies between document elements.
\input{supply_figure/interaction_detail}

\section{Evaluation Metrics}
\label{detail_eval}
This section details the evaluation metrics for assessing model performance on the document layout analysis (DLA) and graph-based document structure analysis (gDSA) tasks. Specifically, we discuss the Mean Average Precision (mAP) for the DLA task, and the Mean Recall (mR) and Mean Average Precision for relations (mAP$_g$) in the gDSA task.

\noindent \textbf{Mean Average Precision for DLA (mAP)}. For the DLA task, we employ the mAP over multiple Intersection over Union (IoU) thresholds, denoted as mAP@[$50$:$5$:$95$]. To compute the mAP, we first calculate the Average Precision (AP) for each class $c$ at each IoU threshold $t \in {0.50, 0.55, \dots, 0.95}$ by integrating the area under the precision-recall curve. Then, we average the APs over all classes and IoU thresholds:

\begin{equation} \text{mAP} = \frac{1}{|T|} \sum_{t \in T} \left( \frac{1}{C} \sum_{c=1}^{C} \text{AP}_{c}(t) \right), \end{equation}

where $T$ is the set of IoU thresholds and $C$ is the number of classes. A prediction is considered correct if the predicted class matches the ground truth and the IoU exceeds threshold $t$.

\noindent \textbf{Mean Recall for gDSA (mR$_g$)}. In the gDSA task, we employ Mean Recall (mR) to evaluate the model's ability to detect relations, especially given multiple coexisting relations and class imbalance. To compute the mR, we first match predicted instances to ground truth based on class labels and Intersection over Union (IoU) with a threshold commonly set at 0.5. Next, we extract relations from the matched instances, defined as subject-object-prediction triplets. We then apply a relation confidence threshold $T_{R}$ and consider only relations with confidence scores above $T_{R}$. For each relation category $r$, the recall is computed as:

\begin{equation} \text{Recall}{r} = \frac{\text{TP}{r}}{\text{TP}{r} + \text{FN}{r}}, \end{equation}

where $\text{TP}{r}$ is the number of true positives and $\text{FN}{r}$ is the number of false negatives for relation $r$. The Mean Recall is then calculated by averaging the recalls over all relation categories:

\begin{equation} \text{mR} = \frac{1}{R} \sum_{r=1}^{R} \text{Recall}_{r}, \end{equation}

where $R$ is the total number of relation categories.

\noindent \textbf{Mean Average Precision for gDSA (mAP$_g$)}. To further comprehensively assess model performance in document relational graph prediction, we use the Mean Average Precision for gDSA (mAP$_g$). We begin by performing instance matching and relation extraction as described in the computation of mR. We then evaluate the relations at confidence thresholds $ T_{R} \in \{0.5, 0.75, 0.95\} $. For each relation category, we compute precision and recall, and calculate the Average Precision (AP) by integrating the precision-recall curve. The  mAP$_g $ is then obtained by averaging the APs over all relation categories:

\begin{equation}
    \text{mAP}_g = \frac{1}{R} \sum_{r=1}^{R} \text{AP}_{r},
\end{equation}

where $ \text{AP}_{r} $ is the Average Precision for relation category $ r $, and $ R $ is the total number of relation categories. This metric balances precision and recall, rewarding models that predict correct relations with high confidence.

Since elements can have multiple relations, we treat relation prediction as a multi-label classification problem for each pair of instances. By evaluating performance per relation category and averaging, we ensure that rare but important relations are appropriately weighted, effectively addressing class imbalance. Additionally, relation evaluation depends on correctly detected instances, linking the quality of relation prediction to the performance on the DLA task. By employing mAP for the DLA task and $mR_g$ and $mAP_g$ for the gDSA task, we provide a comprehensive evaluation framework that addresses the challenges of document structure analysis, including multiple relations and class imbalance. This approach encourages the development of models capable of effectively interpreting complex document structures.

\section{DRGG}
\label{appendix_drgg}
In this subsection, we provide a detailed structural analysis of the Document Relation Graph Generator (DRGG) {and a detailed structural illustration as shown in Figure~\ref{detail_DRGG}}.

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{figures/DRGG_revised.pdf}
    \caption{{The overall architecture and the work flow of the proposed DRGG model. Given an image of document as input, the backbone will extract the feature from the document image and forward to the Encoder-Decoder architecture. The output of Decoder will be forwarded to the object heads and the relation heads for the prediction of document layouts and relations. }}
    \label{detail_DRGG}
\end{figure*}

\subsection{Analysis of Weighted Token Aggregation strategy}

The Weighted Token Aggregation strategy in DRGG is a crucial mechanism that fine-tunes the importance of relational features extracted from different decoder layers, resulting in more accurate and refined predictions. In the DETR framework, object queries at various layers capture feature information at different scales and abstraction levels, which leads to inherent variations in the corresponding relational features. These differences are key to understanding how document elements relate to each other. Different types of relations in documents require attention to distinct aspects of the layout. For instance, \textit{reference} relations requires a deeper focus on the content within the document elements. On the other hand, spatial relations demand more emphasis on the geometric properties and boundaries of the document elements. This nuanced understanding of relational features is what enables DRGG to employ a single relation head to effectively capture and classify multiple types of relations simultaneously. By adjusting the contribution of relational information from different decoder layers, DRGG can adapt to the varying scopes and demands of each type of relation, ensuring a comprehensive and precise representation of document structure.

\subsection{Relation Predictor with auxiliary relation head}

To enhance the stability and accuracy of DRGG's relational predictions, we introduce an auxiliary relation prediction head. This auxiliary relation head focuses solely on determining whether a relation exists between two document elements, without classifying the type of relation. By decoupling the existence of a relation from its categorization, the auxiliary relation head acts as a stabilizer, ensuring that false positives are minimized during inference.

During training, both the main relation predictor and the auxiliary relation head are trained simultaneously using Binary Cross Entropy (BCE) loss. At test time, the predictions from the auxiliary relation head are combined with the main relation predictor's output by multiplying their respective results. This multiplicative correction reduces uncertainty and enhances the robustness of the relational predictions.

Let the output from the main relation predictor, responsible for classifying specific relations, be denoted as $ G_{\text{pred}} \in \mathbb{R}^{N \times N \times k} $, where $ N $ represents the number of document elements and $ k $ is the number of relation categories. Similarly, let the auxiliary relation head output, which predicts the existence of any relation between elements, be denoted as $ A_{\text{pred}} \in \mathbb{R}^{N \times N} $, where each entry in $ A_{\text{pred}} $ represents a binary prediction (relation exists or not) for a pair of document elements.

During inference, the final relational prediction $ G_{\text{final}} $ is computed by multiplying the two outputs element-wise:

\begin{equation}
G_{\text{final}} = G_{\text{pred}} \odot A_{\text{pred}}^{\otimes k},
\end{equation}

where $ \odot $ denotes the element-wise product, and $ A_{\text{pred}}^{\otimes k} $ represents the auxiliary relation head’s predictions expanded along the third dimension to match the number of relation categories $ k $. This operation ensures that only relations that are confidently predicted to exist by the auxiliary relation head are retained in the final output.

\subsection{Loss Function with Hungarian Matching}

For training, the loss computation in DRGG leverages the results of the \textbf{Hungarian matching algorithm}~\citep{HungarianAssignment} from the object detection head in the final decoder layer. This algorithm ensures instance-level matching between predicted document elements and the ground truth elements, providing a one-to-one mapping between predictions and annotations. Once this matching is established, the predicted relation graph can be filtered and adjusted according to the matched pairs, which is critical for accurately training the relation predictor.

The Hungarian matching algorithm aims to minimize the total matching cost by finding the optimal permutation \( \sigma^* \) that maps the set of predicted elements \( \mathcal{P} = \{p_1, p_2, \dots, p_N\} \) to the ground truth elements \( \mathcal{T} = \{t_1, t_2, \dots, t_N\} \). The cost function is defined as:

\begin{equation}
\text{Cost}(\sigma) = \sum_{i=1}^{N} \mathcal{L}(p_i, t_{\sigma(i)}),
\end{equation}

where \( \mathcal{L}(p_i, t_{\sigma(i)}) \) is the loss between the predicted element \( p_i \) and its matched ground truth element \( t_{\sigma(i)} \). The optimal matching is obtained by minimizing this cost:

\begin{equation}
\sigma^* = \arg\min_{\sigma \in \mathfrak{S}_N} \sum_{i=1}^{N} \mathcal{L}(p_i, t_{\sigma(i)}),
\end{equation}

where \( \mathfrak{S}_N \) is the set of all possible permutations of \( N \) elements. This matching is critical for aligning predicted relations with the ground truth during training, ensuring that predictions are corrected for each element's actual match.

The loss function for both the relation predictor and the auxiliary relation head is based on Binary Cross Entropy (BCE), computed independently for each of the predictions. Specifically, let $ G_{\text{gt}} \in \mathbb{R}^{N \times N \times k} $ denote the ground truth relational graph, and let $ A_{\text{gt}} \in \mathbb{R}^{N \times N} $ denote the ground truth existence of relations (i.e., whether a relation exists between pairs of elements). The total loss $ \mathcal{L}_{\text{total}} $ is the sum of the losses for objects heads and relation predictor and the auxiliary relation head:

\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{cls}} + \mathcal{L}_{\text{bbox}} + \lambda \mathcal{L}_{\text{rel}} + \sigma \mathcal{L}_{\text{rel}_\text{aux}},
\end{equation}

where $ \lambda $ is a hyperparameter that controls the weight of the prediction head loss and $\sigma$ is another hyperparameter that controls the weight of the auxiliary relation head loss.

The relation prediction loss $ \mathcal{L}_{\text{rel}} $ is defined as:

\begin{equation}
\mathcal{L}_{\text{rel}} = - \sum_{i,j=1}^{N} \sum_{c=1}^{K} \left( G_{\text{gt}}^{(i,j,k)} \log G_{\text{pred}}^{(i,j,k)} + (1 - G_{\text{gt}}^{(i,j,k)}) \log (1 - G_{\text{pred}}^{(i,j,k)}) \right),
\end{equation}


where $ G_{\text{gt}}^{(i,j,k)} $ and $ G_{\text{rel}}^{(i,j,k)} $ denote the ground truth and predicted probabilities for the $ k $-th relation category between elements $ i $ and $ j $.

Similarly, the auxiliary relation existence loss $ \mathcal{L}_{\text{aux}} $ is given by:

\begin{equation}
\mathcal{L}_{\text{rel}_\text{aux}} = - \sum_{i,j=1}^{N} \left( A_{\text{gt}}^{(i,j)} \log A_{\text{pred}}^{(i,j)} + (1 - A_{\text{gt}}^{(i,j)}) \log (1 - A_{\text{pred}}^{(i,j)}) \right).
\end{equation}


By incorporating both losses, DRGG is trained to accurately predict both the existence and the type of relations between document elements. The auxiliary relation head plays a crucial role in stabilizing the predictions, while the Hungarian matching ensures precise, instance-level alignment between predictions and ground truth, thus improving the overall quality of the relational graph.


{\section{Additional Results of DRGG}}
\label{addition_results}
{In this section, we provide detailed supplementary results from our additional DRGG experiments to offer deeper insights into the gDSA task and the structural design of DRGG.}

{\noindent \textbf{Results on Different Document Domains of GraphDoc Dataset.}}

{To comprehensively evaluate the performance of DRGG, we conducted experiments across multiple document domains separately in GraphDoc dataset, reflecting diverse layouts and structural complexities. These experiments aim to demonstrate the adaptability of our method to varying document types. The detailed results of six different document domains (i.e., Financial Reports, Scientific Articles, Laws and Regulations, Government Tenders, Manuals, and Patents) are presented in Table~\ref{tab:mRg_results} and Table~\ref{tab:mAPg_results} below. We used InternImage as the backbone, RoDLA as the detector, and DRGG for relationship extraction. The tables below summarize the performance in terms of mRg and mAPg under relation confidence thresholds of 0.5, 0.75, and 0.95 under the IoU threshold of 0.5. }
\begin{table}[ht]
\centering
\caption{{mR$_g$ Results on different document domains of GraphDoc Dataset.}}
\renewcommand{\arraystretch}{1.2}
\label{tab:mRg_results}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c}
\toprule[1.5pt]
\textbf{Relation Confidence Thresholds} & \textbf{Financial Reports} & \textbf{Scientific Articles} & \textbf{Laws and Regulations} & \textbf{Government Tenders} & \textbf{Manuals} & \textbf{Patents} \\ \hline
0.5                                      & 15.0                       & 46.3                          & 38.7                           & 40.6                        & 40.6            & 22.7            \\ \hline
0.75                                     & 12.3                       & 42.0                          & 36.5                           & 38.7                        & 35.6            & 20.5            \\ \hline
0.95                                     & 9.0                        & 35.6                          & 33.5                           & 34.1                        & 27.1            & 17.5            \\ 
\bottomrule[1.2pt]
\end{tabular}
}
\end{table}

\begin{table}[ht]
\centering
\caption{{mAP$_g$ Results on different document domains of GraphDoc dataset.}}
\renewcommand{\arraystretch}{1.2}
\label{tab:mAPg_results}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c}
\toprule[1.5pt]
\textbf{Relation Confidence Thresholds} & \textbf{Financial Reports} & \textbf{Scientific Articles} & \textbf{Laws and Regulations} & \textbf{Government Tenders} & \textbf{Manuals} & \textbf{Patents} \\ \hline
0.5                                      & 52.6                       & 54.5                          & 63.2                           & 55.9                        & 46.8            & 31.8            \\ \hline
0.75                                     & 50.9                       & 52.9                          & 58.7                           & 51.4                        & 44.4            & 30.7            \\ \hline
0.95                                     & 20.2                       & 47.5                          & 54.6                           & 48.1                        & 32.5            & 29.3            \\ 
\bottomrule[1.2pt]
\end{tabular}
}
\end{table}

{The results demonstrate clear domain-specific trends. Laws and Regulations achieve the highest mAP$_g$@0.5 with 63.2, benefiting from their structured and consistent layouts, while Patents perform worst, with mR$_g$@0.95 at 17.5, due to their dense and complex layouts. Both mR$_g$ and mAP$_g$ decline as the relation confidence threshold increases, reflecting the challenges of capturing precise relationships under stricter criteria. These findings highlight the varying complexities across domains and the need for robustness in handling diverse document structures.}

{\noindent \textbf{Results on Spatial and Logical Relations of GraphDoc Dataset.}}

{To investigate the impact of different relationship types, we analyzed DRGG’s performance on documents containing only spatial relations compared to those containing both spatial and logical relations. We used InternImage as the backbone, RoDLA as the detector, and DRGG for relationship prediction. We used InternImage as the backbone, RoDLA as the detector, and DRGG for relationship extraction. mR$_g$ and mAP$_g$ metrics were computed under relation confidence thresholds of 0.5, 0.75, and 0.95 with an IoU threshold of 0.5, as shown in Table~\ref{tab:spatial_logical_results}.}

\begin{table}[ht]
\centering
\caption{{Results for relation prediction performance under different relation types.}}
\renewcommand{\arraystretch}{1.2}
\label{tab:spatial_logical_results}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c}
\toprule[1.5pt]
\textbf{Spatial Relation} & \textbf{Logical Relation} & \textbf{mR$_g$@0.5} & \textbf{mR$_g$@0.75} & \textbf{mR$_g$@0.95} & \textbf{mAP$_g$@0.5} & \textbf{mAP$_g$@0.75} & \textbf{mAP$_g$@0.95} \\ \hline
${\surd}$                          &                           & 32.1              & 27.7              & 22.1              & 49.5              & 49.5              & 41.3              \\ \hline
${\surd}$                          & ${\surd}$                         & 26.7              & 23.9              & 20.1              & 57.5              & 56.2              & 37.6              \\ 
\bottomrule[1.2pt]
\end{tabular}
}
\end{table}

{The results show that capturing spatial and logical relations is challenging, as indicated by the lower metrics. Spatial relations alone achieve an mR$_g$@0.5 of 32.1 and a mAP$_g$@0.5 of 49.5. When logical relations are included, mR$_g$@0.5 drops to 26.7, while mAP$_g$@0.5 slightly improves to 57.5. Nevertheless, performance declines significantly at stricter thresholds, i.e., mR$_g$@0.95 and mA$_g$@0.95.}


\section{Ablation Study Result of DRGG}
\label{ablation}
In this section, we present {the} ablation study of the DRGG design to validate the effectiveness of the DRGG model. {The analysis evaluates four key aspects: the impact of using DRGG as a relational graph prediction head, the effectiveness of the relation feature extractor module, the influence of IoU thresholds, and the effect of relation confidence thresholds on different relation types.}

\noindent \textbf{Ablation of DRGG Model.} Table~\ref{tab:ablation_drgg} {highlight the effectiveness of integrating the DRGG relation prediction head into the document layout analysis task. InternImage combined with DINO sees an improvement from 80.5 to 81.5, the highest among all configurations, illustrating the harmony between the DRGG head and advanced backbones. This improvement mark the DRGG module's utility in capturing complex document structures, as it effectively augments the detector's ability to model relationships between document elements. These findings validate the design of DRGG and its critical role in advancing the accuracy and reliability of document structure analysis.}
\input{supply_table/ablation_drgg}

\noindent \textbf{Ablation of Relation Feature Extractor.} Table~\ref{tab:ablation_feature} illustrates {the importance of the relation feature extractor in the DRGG model. When paired with InternImage and RoDLA, the feature extractor significantly outperforms a linear layer replacement across all metrics. For DLA, it achieves a higher mAP result of 81.5. In gDSA, the extractor shows clear advantages in mR$_g$ and mAP$_g$.}
\input{supply_table/ablation_extractor}

{\noindent \textbf{Ablation of IoU Thresholds.}}
{We understand the importance of evaluating model performance under high IoU thresholds to assess alignment between predicted and actual bounding boxes. To evaluate the impact of high IoU thresholds on model performance, we conducted experiments using InternImage as the backbone, RoDLA as the detector, and DRGG for relationship extraction. The results of Table~\ref{tab:iou_thresholds} below present mR$_g$ and mAP$_g$ values under IoU thresholds of 0.5, 0.75, and 0.95:}

\begin{table}[ht]
\centering
\caption{{Impact of IoU thresholds on mR$_g$ and mAP$_g$.}}
\renewcommand{\arraystretch}{1.2}
\label{tab:iou_thresholds}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c}
\toprule[1.5pt]
\textbf{IoU Threshold} & \textbf{mR$_g$@0.5} & \textbf{mR$_g$@0.75} & \textbf{mR$_g$@0.95} & \textbf{mAP$_g$@0.5} & \textbf{mAP$_g$@0.75} & \textbf{mAP$_g$@0.95} \\ \hline
0.5                    & 30.7             & 28.2              & 24.5              & 57.6              & 56.3               & 46.5               \\ \hline
0.75                   & 28.8             & 26.5              & 23.0              & 56.7              & 54.8               & 36.8               \\ \hline
0.95                   & 22.1             & 20.7              & 18.4              & 55.5              & 54.3               & 36.5               \\ 
\bottomrule[1.2pt]
\end{tabular}
}
\end{table}
{As shown in the results, at the highest IoU threshold of 0.95, the model achieves 18.4 mR$_g$@0.95 and 36.5 mAP$_g$@0.95, demonstrating the significant challenges in capturing precise alignments, particularly in complex or densely packed layouts where bounding box prediction errors have a greater impact. While lower IoU thresholds allow the model to achieve higher recall and precision, stricter thresholds demand fine-grained alignment, which may not always be feasible due to the inherent limitations of bounding box prediction accuracy. These findings emphasize the need to balance strict alignment metrics with practical utility based on specific application requirements. Higher IoU thresholds, while providing stricter metrics, may not fully capture the model's overall effectiveness in scenarios where moderate overlap suffices.}

{\noindent \textbf{Ablation of Relation Confidence Thresholds among Relation Categories.}}
{Table~\ref{tab:mRg_results_r_threshold} and Table~\ref{tab:mAPg_results_r_threshold} shows the influence of different relationship confidence thresholds in the context of imbalanced sample sizes among relation categories. We used InternImage as the backbone, RoDLA as the detector, and DRGG for relationship extraction. mRg @0.5, mRg @0.75, and mRg @0.95 denote the mean Recall in the gDSA Task for relation confidence threshold 0.5, 0.75, and 0.95 under IoU threshold 0.5, respectively. mAPg @0.5, mAPg @0.75, and mAPg @0.95 denote the mean Average Precision in the gDSA Task for relation confidence threshold 0.5, 0.75, and 0.95 under IoU threshold 0.5, respectively.}:
\begin{table}[ht]
\centering
\caption{{mR$_g$ Results at different relation confidence thresholds.}}
\renewcommand{\arraystretch}{1.2}
\label{tab:mRg_results_r_threshold}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\toprule[1.5pt]
\textbf{Confidence Threshold} & \textbf{Up} & \textbf{Down} & \textbf{Left} & \textbf{Right} & \textbf{Parent} & \textbf{Child} & \textbf{Sequence} & \textbf{Reference} \\ \hline
0.5                           & 41.7        & 50.0          & 71.4          & 71.4           & 12.5            & 25.0           & 0.0               & 0.0                \\ \hline
0.75                          & 41.7        & 33.3          & 42.9          & 57.1           & 12.5            & 12.5           & 0.0               & 0.0                \\ \hline
0.95                          & 8.3         & 8.3           & 28.6          & 28.6           & 12.5            & 0.0            & 0.0               & 0.0                \\ 
\bottomrule[1.2pt]
\end{tabular}
}
\end{table}

\begin{table}[ht]
\centering
\caption{{mAP$_g$ Results at different relation confidence thresholds.}}
\renewcommand{\arraystretch}{1.2}
\label{tab:mAPg_results_r_threshold}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\toprule[1.5pt]
\textbf{Confidence Threshold} & \textbf{Up} & \textbf{Down} & \textbf{Left} & \textbf{Right} & \textbf{Parent} & \textbf{Child} & \textbf{Sequence} & \textbf{Reference} \\ \hline
0.5                           & 49.0        & 49.0          & 99.0          & 99.0           & 45.5            & 45.5           & 56.4              & 16.8               \\ \hline
0.75                          & 47.4        & 45.1          & 99.0          & 99.0           & 45.5            & 45.5           & 51.2              & 16.8               \\ \hline
0.95                          & 40.4        & 40.4          & 49.5          & 49.5           & 37.6            & 36.6           & 46.5              & 0.0                \\ 
\bottomrule[1.2pt]
\end{tabular}
}
\end{table}

{From the experiment result, we could find that, spatial relations, i.e.,  \textit{Left}, \textit{Right}, \textit{Up}, and \textit{Down} achieve consistently higher mR and mAP values compared to logical relations, i.e., \textit{Parent}, \textit{Child}, \textit{Sequence}, and \textit{Reference}, reflecting their prevalence in the dataset and larger training sample sizes. As the confidence threshold increases, both mR and mAP values decline across all relation types, with logical relations showing the steepest drop; for instance, Reference achieves 16.8 mAP at a 0.5 threshold but drops to 0.0 at 0.95, highlighting the challenges of capturing infrequent or ambiguous relationships. A confidence threshold of 0.5 strikes a balance between precision and recall, but addressing dataset imbalance through weighted training could further enhance performance.}

\section{Implementation Details}
\label{appendix:implementation}

\noindent \textbf{Hardware Setup.}  In this work, all experiments were conducted on a computing cluster node equipped with four Nvidia A100 GPUs, each with 40 GB of memory. Each node would also with $300$ GB of CPU memory.

\noindent \textbf{Training Settings.} We implemented our method using PyTorch v1.10 and trained the model with the AdamW optimizer using a batch size of 4. The initial learning rate was set to $1 \times 10^{-4}$, with a weight decay of $5 \times 10^{-3}$. The AdamW hyperparameters, betas and epsilon, were configured to $(0.9, 0.999)$ and $1 \times 10^{-8}$, respectively. To enhance the model's robustness and accuracy, we employed a multi-scale training strategy. Specifically, the shorter side of each input image was randomly resized to one of the following lengths: {480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800}, while ensuring that the longer side did not exceed 1333 pixels. This approach helps the model generalize better to varying document sizes and layouts, reflecting the diverse nature of real-world document data.

\section{Qualitative Results of DRGG}
In this section, we present several qualitative results predicted by DRGG on GraphDoc validation dataset, alongside their corresponding ground truth annotations for comparison. 

\input{supply_figure/qualitative_result}

As illustrated in Figure~\ref{qualiresult}, {errors in relation prediction arise primarily from two sources. First is the ambiguity in densely populated layouts, where elements, e.g., captions and figures, lack clear alignment. Secondly, misclassification and inaccurate bounding boxes, from the DLA stage, propagate errors to the relation prediction process. Despite these challenges, DRGG demonstrates promising capabilities in capturing key spatial and logical relationships, such as parent-child links between \textit{Picture} and \textit{Caption}. Nonetheless, the DRGG performance is hindered in DLA accuracy, as seen in cases of misclassified tables leading to missing relationships. To address these issues, we suggest incorporating multimodal embeddings that combine visual and textual features, improving the DLA backbone for enhanced detection performance, and integrating post-processing methods to refine predictions using contextual cues. Additionally, extending DRGG to multi-page relational understanding will enhance its applicability for comprehensive document structure analysis and relation predictions. }




