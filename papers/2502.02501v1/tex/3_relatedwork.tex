\section{Related Work}
\noindent\textbf{Document Layout Analysis.} 
To analyze the document layout is a fundamental task of the document understanding. 
Recent advancements in deep learning~\citep{schreiber2017deepdesrt,prasad2020cascadetabnet} treat Document Layout Analysis (DLA) as a traditional visual object detection or segmentation challenge, employing convolutional neural networks (CNNs) to address this task.
Drawing inspiration from BEiT~\citep{bao2021beit}, compared to the CNN-based methods, DiT~\citep{li2022dit} trains a document image transformer specifically for DLA, achieving promising results, albeit overlooking the textual information within documents.
Beyond the single modality, UniDoc~\citep{gu2021unidoc} and LayoutLMv3~\citep{huang2022layoutlmv3} integrate text, vision, and layout modalities within a unified architecture.
Not only methods and architectures, but also benchmark datasets have achieved promising evolution. 
While PubLayNet~\citep{zhong2019publaynet} and DocLayNet~\citep{doclaynet2022} have only two modalities, \ie, visual and layout, FUNSD~\citep{jaume2019funsd}, XFUNSD~\citep{xu-etal-2022-xfund}, ReadingBank~\citep{wang2021layoutreader} and Form-NLU~\citep{ding2023form} have textual, visual, layout and order modalities. It is regrettable that the aforementioned datasets, despite considering other modalities, are designed solely for textual information without non-textual information consideration. HRDoc~\citep{Ma_Du_Hu_Zhang_Zhang_Zhu_Liu_2023} and its improved version, the Comp-HRDoc dataset~\citep{wang2024detect}, both take into account multimodal processing of both textual and non-textual information. Additionally, they introduce a hierarchical structure as a new modality for document analysis.
However, all publicly available datasets do not consider the graphical structure of document, which is crucial for both spatial and logical structure analysis of documents.
In this work, we propose GraphDoc dataset, which contains six modalities, \ie, textual, visual, layout, order, hierarchy and graph, targeting complex Document Structure Analysis (DSA) tasks. 


\noindent\textbf{Graphical Representation and Generation.}
To construct a graph-based structured representation is a foundational step toward higher-level visual understanding. 
Graph-based representation 
Scene Graph Generation (SGG) is versatile tool for various vision-language tasks, such as image captioning~\citep{gao2018image,yang2019auto}, visual question answering~\citep{li2019relation,zhang2019empirical}, content-based image retrieval~\citep{johnson2015image,schuster2015generating}, image generation~\citep{johnson2018image,mittal2019interactive}, and referring expression comprehension~\citep{yang2019cross}. On the other hand, in the field of natural language processing knowledge graph generation is also well-explored.
Instead of building the entire global graph structures, some methods~\citep{li2016commonsense,yao2019kg,malaviya2020commonsense} look into a simpler problem of graph completion.
Alternatively, other works~\citep{roberts2020much,jiang2020can,shin2020autoprompt,li2021prefix} propose to query the pre-trained models to extract the learned factual and commonsense knowledge.
CycleGT~\citep{guo2020cyclegt} is an unsupervised approach for both text-to-graph and graph-to-text generation. In this method, the graph generation process utilizes a pre-existing entity extractor, followed by a classifier for relations. Inspired by the graph generation from computer vision and natural language processing, we propose a graph-based task for document analysis called graph-based Document Structure Analysis (gDSA). gDSA refers to the task of mapping document images into a comprehensive structural graph that contains the understanding of document structure.

\noindent \textbf{Document Relation Extraction.}
Document relation extraction is a crucial task in understanding the complex interactions within documents by identifying relations between document elements. ReadingBank~\citep{wang2021layoutreader} is designed for the task of reading order detection, which aims to capture the sequence of words as naturally understood by human readers. FUNSD~\citep{jaume2019funsd}, Form-NLU~\citep{ding2023form} and XFUND~\citep{xu-etal-2022-xfund} focuses on extracting relations in semi-structured documents, particularly text-only forms. Addresses the challenges in scanned documents by identifying key-value pairs and relations between textual elements. PDF-VQA~\citep{pdfvqa} extends document relation extraction to multimodal documents by incorporating visual question answering techniques. This dataset requires the identification of relations between document elements within PDFs. HRDoc~\citep{Ma_Du_Hu_Zhang_Zhang_Zhu_Liu_2023} constructs a dataset for document reconstruction but overlooks the spatial structure and the interaction between textual and non-textual elements. Our proposed GraphDoc dataset includes both spatial and logical relations between textual and non-textual elements, resulting in a comprehensive analysis of document structure.  
