\section{Methods}
% \section{Document Graph Generation}
\label{sec:method}

\subsection{GraphDoc Dataset}
In this section, we introduce the GraphDoc dataset, specifically developed for document layout and structure analysis. Additionally, we define the corresponding tasks and describe the annotation pipeline employed for constructing such datasets.

\subsubsection{Task Definition}
The goals of the GraphDoc Dataset can be represented into two tasks: Document Layout Analysis (DLA) and graph-based Document Structure Analysis (gDSA). We detail definitions respectively.

\textbf{Document Layout Analysis (DLA)}. This task focuses on extracting layout information with labeled bounding box, representing layout elements within the document. For the DLA task, the setup is similar to that of the DocLayNet~\citep{doclaynet2022} dataset, with the layout element size being at the paragraph level except \textit{Table} and \textit{Picture}. The labels are categorized into 11 distinct classes: \textit{Caption}, \textit{Footnote}, \textit{Formula}, \textit{List-item}, \textit{Page-footer}, \textit{Page-header}, \textit{Picture}, \textit{Section-header}, \textit{Table}, \textit{Text}, and \textit{Title}. DLA task can be represented by the following objective function:
\vspace{ -5px}
\begin{equation}
    \mathcal{L}_{\text{DLA}} = \sum_{i=1}^{n} \mathcal{L}_{\text{bbox}}(b_i, \hat{b}_i) + \mathcal{L}_{\text{cls}}(c_i, \hat{c}_i)
\vspace{ -10px}
\end{equation}

where $ b_i $ and $ \hat{b}_i $ are the ground truth and predicted bounding boxes for the $i$-th layout element, respectively, and $ c_i $ and $ \hat{c}_i $ are the corresponding class labels. The loss function $\mathcal{L}_{\text{DLA}}$ thus encapsulates both the bounding box regression loss $\mathcal{L}_{\text{bbox}}$ and the classification loss $\mathcal{L}_{\text{cls}}$.

\input{figures/task_venn}

\textbf{Graph-based Document Structure Analysis (gDSA)}. gDSA aims to extract the relational graph among layout elements within the document, which could be formed as $G=(V, E)$. For gDSA, nodes $V$ correspond to the layout elements, edges $E$ represent the relations between these layout elements, e.g., \textit{reference}. The objective for gDSA could be expressed as:
\vspace{ -3px}
\begin{equation}
    \mathcal{L}_{\text{gDSA}} = \sum_{(v_i, v_j) \in E} \left( \mathcal{L}_{\text{cls}}(v_i, \hat{v}_i) + \mathcal{L}_{\text{rel}}(r_{ij}, \hat{r}_{ij}) \right)
\vspace{ -10px}
\end{equation}

Here, $ v_i $ and $ \hat{v}_i $ are the ground truth and predicted labels for the layout element $ i $, and $ r_{ij} $ and $ \hat{r}_{ij} $ represent the ground truth and predicted relations between the layout elements $ i $ and $ j $. The classification loss $\mathcal{L}_{\text{cls}}$ for the nodes ensures that the layout elements are accurately identified, while the relation loss $\mathcal{L}_{\text{rel}}$ for the edges captures the accuracy of the predicted relations within the document's structure. Additionally, the specific functions of all the above-mentioned losses depend on the requirements of the model and the task.

Two sub-tasks can derive further from the gDSA task: Reading Order Prediction (ROP) and Hierarchical Structure Analysis (HSA). The ROP task involves determining the correct sequence in which the layout elements should be arranged. The HSA task focuses on identifying the hierarchical relations among the layout elements and establishing a structural organization within the document. In addition to the tasks described above, the gDSA task further leverages reference relations to establish connections between textual and non-textual layout elements within the document. This integration ensures that these two types of layout element are not analyzed in isolation, but rather as interconnected components. As shown in Figure~\ref{fig:venn}, the gDSA tasks in the GraphDoc dataset achieves a novel and comprehensive visual analysis of document task, paving the way for novel document visual content analysis of modern complex documents.


\subsubsection{Dataset Collection}
Our GraphDoc Dataset is primarily derived from the DocLayNet~\citep{doclaynet2022} dataset, which contains over 80,000 document page images spanning a diverse array of content types, including financial reports, user manuals, scientific papers, and legal regulations. We leveraged the existing detailed annotations and the PDF files offered through DocLayNet Dataset, to create new annotations that focus specifically on the relations between various layout elements within the documents. Additionally, in accordance with the License CDLA 1.0, users are permitted to modify and redistribute enhanced versions of datasets based on the DocLayNet dataset. Due to page limitations of DocLayNet, we will only consider relations within the same page and not those across pages.

\subsubsection{Document Relational Graphs}
\label{sec:relationgraph}
For visually rich documents, the spatial layout and relations between various layout elements carry significant meaning. These relations include hierarchical relations between section headers and text, sequential relations between text blocks, and references to tables or figures. Understanding these structural and relational details aids in better extraction of document information and in gaining a deeper comprehension of the document as a whole. Moreover, graphs themselves are an effective modality for enhancing the performance of scene understanding tasks.

Consequently, in our GraphDoc dataset, we have defined two types of relational graphs. The first type is the spatial relational graph, which primarily categorizes spatial relations into four types: \textit{up}, \textit{down}, \textit{left}, and \textit{right}. In scientific literature, the spatial structure is typically more standardized, often formatted as either two-column or single-column documents in Manhattan-Layout, which refers to a grid-like layout where content is arranged in straight, non-overlapping rectangular regions. Thus, these four spatial relations can effectively cover most of the spatial relations between layout elements within scientific documents.

The second type is the logical relation graph, which is independent of layout position and focuses on capturing the relations between layout elements from a logical structure perspective. In this logical relation graph, we categorize all relations between document layout elements into four types of relations: \textit{parent}, \textit{child}, \textit{sequence}, and \textit{reference}. All logical relations are illustrated in Figure~\ref{relationship_def} for better understanding. The detailed definitions of relation are as follows:
\begin{itemize}
\item \textbf{Parent}: Indicates the parent part of a parent-child. For example, a section header can be the parent of the subsection header, as in Fig.~\ref{relationship_def}(b).
\item \textbf{Child}: Represents the child part of a parent-child. For instance, paragraphs that belong to a section are considered children of that section header, as in Fig.~\ref{relationship_def}(c).
\item \textbf{Sequence}: Denotes the sequential order of layout elements. For example, the natural reading order of paragraphs in a section or the steps in a procedure, as in Fig.~\ref{relationship_def}(d).
\item \textbf{Reference}: Captures citation or references. For example, a figure or table being cited within the text or references to external documents, as in Fig.~\ref{relationship_def}(e).
\end{itemize}
\input{figures/logical_relationship}

\subsubsection{Dataset Annotation Pipeline}
\label{pipeline}
In order to create high-quality annotations for the GraphDoc-Dataset, we invested significant effort in enhancing the relational annotations while maintaining the foundational document layout annotations (DLA) from the original DocLayNet~\citep{doclaynet2022}. One of the primary challenges we encountered was the complexity of accurately capturing and annotating the intricate relations between document components, particularly for tasks involving spatial and logical structures. For these challenges, we designed a heuristic rule-based relation annotation system. This system is based on the DLA task annotations and the provided PDF files from the DocLayNet dataset. The steps for relation annotating with a rule-based system are as follows:
\begin{itemize}
    \item \textbf{Content Extraction}: We apply the Tesseract OCR~\footnote{https://tesseract-ocr.github.io/} and PDF parser to extract the text content contained within the bounding boxes of all categories except for Table and Picture.
    
    \item \textbf{Spatial relation Extraction}: To extract spatial relations in the four directions, we heed DocLayNet annotation rules, which ensure that there is no overlap between bounding boxes. This allows us to determine spatial relations by scanning pixel by pixel along the x-axis and y-axis for spatial relations in \textit{up}, \textit{down}, \textit{left}, and \textit{right}. We record only the nearest adjacent bounding box in each direction to avoid redundant definitions.
    
    \item \textbf{Basic Reading Order}: We designed an algorithm to detect Manhattan or non-Manhattan layouts according to the spatial relation among all annotations. Additionally, we employ the Recursive X-Y Cut algorithm~\citep{xycut} to roughly establish a basic reading order based on the general left-to-right, top-to-bottom reading rule.
    
    \item \textbf{Hierarchical Structure}: Annotations were categorized into four groups based on their roles: (1) elements with direct structural relations; (2) non-textual content within the logical structure; (3) elements lacking direct associations; and (4) references. We establish an internal tree structure for the first two groups based on the text annotation, category, and basic reading order. Within the non-textual content group, \textit{Caption} is designated as the child of the corresponding \textit{Table} and \textit{Picture}, to provide textual representations.
    
    \item \textbf{Relation Completion}: Using the extracted hierarchical structure, we establish \textit{parent} and \textit{child} relations within each group. \textit{child} nodes under the same \textit{parent} are sequentially ordered via \textit{sequence} relations based on basic reading order. We match annotation texts to construct \textit{reference} relation. The \textit{reference} relations among \textit{Table} and \textit{Picture} are established, excluding \textit{Caption}. However, \textit{references} within \textit{Caption} to others are maintained.

\end{itemize}

In summary, we developed a rule-based relation annotation system that efficiently constructs instance-level relational graph annotations, aligned with document elements bounding box and category annotations for the gDSA task. Moreover, the most of the results have been manually verified and refined. Our annotation system captures the inherent spatial and logical relations of document layouts, resulting a robust foundation for training and evaluating models on complex DSA tasks.

\subsubsection{Dataset Statistics}
In total, the GraphDoc dataset extends DocLayNet~\citep{doclaynet2022} by enriching it with detailed relational annotations while maintaining consistency in instance categories and bounding boxes. It comprises $80,000$ single-page document images, each selected from an individual document, resulting in $1.10$ million instances across $11$ categories: \textit{Caption}, \textit{Footnote}, \textit{Formula}, \textit{List-item}, \textit{Page-footer}, \textit{Page-header}, \textit{Picture}, \textit{Section-header}, \textit{Table}, \textit{Text}, and \textit{Title}. We have expanded the relational data into eight categories as defined in Sec.~\ref{sec:relationgraph}, yielding $4.13$ million relation pairs. Spatial relations constitute $64.06\%$ of these pairs, while logical relations make up the remaining $36.94\%$. It shows that spatial relations dominate the dataset, reflecting the structured nature of document layouts, where components such as \textit{Section-header}, \textit{Page-footer}, and \textit{Text} are frequently positioned in spatial proximity. Logical relations, although comprising a smaller portion, play a critical role in linking elements, e.g., \textit{Table} and \textit{Picture} to the corresponding \textit{Text}. 

\input{figures/relation_distribution}

The detailed distribution of these relation pairs is illustrated in Figure~\ref{relation_dist}, which provides a comprehensive overview of the relational statistics within the dataset. The left side of Figure~\ref{relation_dist} presents an aggregate view of the total relation flow between different object categories, disregarding the specific types of relations (e.g., spatial or logical). This visualization highlights how various document elements, such as \textit{Text}, \textit{Picture}, and \textit{Section-header}, interact within the dataset. The intensity of relation flow between categories such as \textit{Text} and \textit{Picture} underscores the typical structure of documents, where these elements frequently co-occur or are positioned in proximity to one another.

On the right-hand side of Figure~\ref{relation_dist}, the figure delves deeper into a specific relation type \textit{reference}. The top section presents a heatmap that captures the frequency and distribution of reference relations between different object categories. This heatmap highlights that category \textit{Table} and \textit{Picture} have significantly intensive interactions observed with other document layout elements, e.g., \textit{Text} and \textit{List-item}. The lower section provides concrete examples of these reference relations, illustrating the detailed reference situation of \textbf{Picture} in a real-world document context. Together, these visualizations offer a holistic view of both the overall relational patterns and the specific behaviors of \textit{reference} relations, providing deeper insights into the structural complexity of document layouts.

\subsection{Document Relation Graph Generator}
\label{DRGG}
In this section, we introduce the Document Relation Graph Generator (DRGG), an architecture designed to generate instance-level relational graphs. DRGG provides an end-to-end solution to construct graphs that capture both spatial and logical relations between document layout elements. By leveraging visual features, DRGG aims to detect and analyze the structure of document layout elements accurately.

\input{figures/DocGraphGenerator}

As depicted in Figure~\ref{DGG_structure}, the proposed model is based on Encoder-Decoder architecture with backbone for feature extraction. The backbone extracts low-level features from the document {image}, which are refined through the Encoder-Decoder framework. These refined features are processed through two main heads: the object detection head, responsible for {document layout analysis task}, and the relation head {(DRGG)}, which predicts relations. DRGG is designed as a plug-and-play component, enabling seamless integration with existing models without requiring any modification. DRGG consists of two parts: relation feature extractor and relation feature aggregation. 

\textbf{Relation Feature Extractor}. The object queries ($X^0$) and object feature representations ($X^l$) calculated at each decoder layer $l$ are fed into independent relation feature extractors {in DRGG respectively}. These are then processed separately through two independent pooling layers ($P$) and Multi-Layer Perceptrons ($\text{MLP}_p$) in extractors as follows:
\vspace{-2px}
\begin{equation}
D^l_1 = \text{MLP}^1_p(P_1(X^l)), \quad D^l_2 = \text{MLP}^2_p(P_2(X^l)),
\vspace{-2px}
\end{equation}
% ,
where $X^l \in \mathbb{R}^{N \times d_{\text{embed}}}, D^l_{1,2} \in \mathbb{R}^{N \times d_{\text{pool}}}$. Pooling aggregates information across channels, reducing redundancy and improving robustness.  The extracted one-dimensional relational features are then through upsampling layer ($U$) and further refined through MLP layers ($\text{MLP}_u$), then concatenated with the original object features to form a unified representation of the relational feature ($D$). The two representations are subsequently expanded into two dimensions along different axes and concatenated to derive the final relational features:
\begin{equation}
F^l = \text{Concat}(\sigma(\text{MLP}^1_u(U_1(D^l_1)) + X^l) \otimes \mathbf{1}_{d_{\text{embed}}}, \sigma(\text{MLP}^1_u(U_1(D^l_2)) + X^l)^T \otimes \mathbf{1}_{d_{\text{embed}}}),
\end{equation}
% , 
where $F^l \in \mathbb{R}^{N \times N \times 2d_{\text{embed}}}$. This approach captures both direct relations, e.g., spatial proximity, and indirect relations, e.g., \textit{reference}, between elements.

\textbf{Relational Feature Aggregation}. The extracted relation features from each decoder layer are combined using a weighted aggregation method to form a unified representation of the relations between all object queries. This unified representation is subsequently incorporated into the relation predictor (MLP$_{g}$) to generate the relational graph prediction:
\vspace{-5px}
\begin{equation} 
G = MLP_{g} \left( \sum_{l=1}^{L} \alpha^{(l)} F^l\right),
\label{eqï¼šDRGG}
\vspace{-5px}
\end{equation}

where $G \in \mathbb{R}^{N \times N \times k}$, $k$ is number of relation category. \( \alpha^{(l)} \) are learnable weights for token aggregation. This query-based mechanism ensures that the final document relation graph, which represents a combination of image features, spatial layout, and semantic relations, would also be able to improve the accuracy of both document layout analysis and relational prediction. 

The output of DRGG is a well-structured graph where nodes represent document elements, and edges represent the relations between these elements. By combining {DLA result from detection head}, DRGG ensures a more detailed and accurate representation for document structure analysis. {More details about the DRGG architecture are presented in the supplementary Sec.~\ref{appendix_drgg}.} 

\subsection{Evaluation Metrics for gDSA}
\label{gdsa_eval}
In traditional Scene Graph Generation (SGG) evaluations, metrics such as Mean Recall@$k$ and Pair-Recall@$k$ assess the top-$k$ subject-predicate-object triplets ranked by predicted confidence scores~\citep{lorenz2024sgbench}. However documents often contain a variable number of relations, and limiting the evaluation to a fixed top-$k$ can result in important relations being overlooked if they are not among the top predictions. Furthermore, there is a significant class imbalance in the relations within documents: spatial relations are prevalent, whereas logical relations such as \textit{reference} are relatively rare. This imbalance poses challenges for evaluation metrics that rely on top-$k$ filtering. 
Threshold-based filtering, in contrast, allows for the inclusion of all relations that exceed a certain threshold, regardless of their frequency or ranking. This approach ensures that rare but critical relations are adequately considered during evaluation. Moreover, unlike in traditional SGG, where typically only one relation exists between subject-object pairs, layout elements in the gDSA task can have multiple coexisting relations (e.g., spatial and logical relations), both of which are essential for understanding the document structure. Therefore, the proposed evaluation metrics, $\text{mR}_g$ %@$T_{R}$ 
and $\text{mAP}_g$, should be capable of measuring the performance in both aspects: detecting layout elements and identifying multiple relations between them, including less frequent but significant relations.

To address these challenges, we first perform an exact matching of predicted instances to ground-truth instances based on both bounding box overlap and object category correspondence. Once this mapping is established, we evaluate the predicted relations within this matched set. Similar to the Intersection over Union (IoU) threshold used in object detection, we introduce a relation confidence threshold $T_{R}$. All relations with confidence scores exceeding this threshold are considered positive relation predictions. The remaining settings align with standard SGG evaluation metrics. This method ensures that the relation evaluation depends on both the performance of document layout analysis and the relation predictions. By explicitly considering the impact of bounding box detection and label prediction on the quality of relation predictions, our evaluation provides a comprehensive assessment of the gDSA task. The detailed algorithmic process is presented in Algorithm~\ref{alg:metric}.

\input{table/metric}






