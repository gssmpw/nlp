\section{Related work}
Neural architecture adaptation algorithms can be broadly classified into two categories: i) neural architecture search algorithms and ii) principled adaptive strategies. 
Neural architecture search (NAS) algorithms rely on metaheuristic optimization,  reinforcement learning strategy, or Bayesian hyperparameter optimization to arrive at a reasonable architecture \cite{zoph2016neural, stanley2002evolving, suganuma2017genetic, elsken2018efficient, real2019regularized, balaprakash2018deephyper, miikkulainen2019evolving, liu2021survey,li2020random,kandasamy2018neural}. However, these strategies involve training and evaluating many candidate architectures (possibly deep) in the process and are thus computationally expensive. One key issue with NAS algorithms is that most of the methods report the performance of the best-found architecture, presumably resulting from a single run of the search process \cite{li2020random}. However,  random initializations of each candidate architecture could drastically influence the choice of best architecture prompting the use of multiple runs to select the best architecture, and hence prohibitively expensive. 
On the other hand, sensible adaptive strategies  are algorithms for growing neural networks where one
starts by training a small network and progressively increasing the size of the network (width/depth) \cite{wu2019splitting, wynne1993node,wu2020firefly,evci2022gradmax,krishnanunni2022layerwise,chen2015net2net}.

In particular, existing works have considered growing the width gradually by adding neurons for a fixed depth neural network \cite{wu2019splitting, wynne1993node,maile2022and,evci2022gradmax}. Liu et al. \cite{wu2019splitting} developed a simple criterion for
deciding the best subset of neurons to split and a splitting gradient for optimally updating the off-springs.  Wynne-Jones \cite{wynne1993node}  considered splitting the neurons (adding neurons) based on a principal component analysis on the oscillating weight vector.  Chen et al. \cite{chen2015net2net} showed that replacing a network with an equivalent network that is wider (has more neurons in
each hidden layer) allows the equivalent network to inherit the knowledge from the existing one and can be trained to further improve the performance. Firefly algorithm by Wu et al. \cite{wu2020firefly} generates candidate neurons that either split existing
neurons with noise or are completely new and selects those with the highest gradient norm.



On the other hand, many efforts have been proposed for growing neural architecture along the depth \cite{hettinger2017forward,kulkarni2017layer,wen2020autogrow}. Layerwise training of neural networks is an approach that addresses the issue of the choice of depth of a neural network and the computational complexity involved with training \cite{xu1999training}.   Hettinger et al. \cite{hettinger2017forward} showed that layers can be trained one at a time and the resulting DNN can generalize better. Bengio et al.  \cite{bengio2007greedy} proposed a greedy layerwise unsupervised learning algorithm where the initial layers of a network are supposed to represent more abstract concepts that explain the input observation, whereas subsequent layers extract low-level features. Recently, we devised a manifold regularized greedy layerwise training approach for adapting a neural architecture along its depth \cite{krishnanunni2022layerwise}. Net2Net algorithm \cite{chen2015net2net} introduced the concept of function-preserving transformations for rapidly transferring the information stored in one
neural network into another neural network. Wei et al. \cite{wen2020autogrow} proposed the AutoGrow algorithm to automate depth discovery in deep neural networks where new layers are inserted progressively
if the accuracy improves; otherwise, stops growing and
thus discovers the depth.






It is important to note that any algorithm for growing neural networks (width and/or depth) should adequately address the following questions in a mathematically principled way rather than a heuristic approach \cite{evci2022gradmax}: {\bf{When}} to add new capacity (neurons/layers)?; {\bf{Where}} to add new capacity?; {\bf{How}} to initialize the new capacity? 
Note that some efforts for growing neural networks along the width (adding neurons) have attempted to answer the above questions satisfactorily to some extent \cite{wu2019splitting,wu2020firefly,wynne1993node,evci2022gradmax,maile2022and}. However,  when it comes to growing neural architecture along the depth, to our best knowledge, the works cited above do not address the above questions adequately. Therefore, it is imperative to develop a mathematically principled depth-adaptation strategy that answers the above questions while also yielding superior performance in comparison to an ad-hoc baseline network. Further, note that since we know how to initialize the new capacity, the only source of randomness in the algorithm is in the initialization of the initial small network.