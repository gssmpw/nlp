
% SIAM Article Template
\documentclass[onefignum,onetabnum]{siamonline171218}
%\documentclass[onefignum,onetabnum,review]{siamonline190516}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.
\input{Macros}
\input{ex_shared}
\usepackage{algorithm} %for writing pseudoalgorithms
\usepackage{algpseudocode} %for writing pseudoalgorithms
\usepackage{ntheorem}
\usepackage{tablefootnote}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{patterns}
\usetikzlibrary{arrows,shapes}
\usetikzlibrary{decorations.pathreplacing,calligraphy}
%\newsiamthm{prop}{Proposition}
%\crefname{prop}{Proposition}{Propositin}
\usepackage{comment}
% Optional PDF information
%\ifpdf
%\hypersetup{
 % pdftitle={An Example Article},
%  pdfauthor={C G Krishnanunni, and Tan Bui-Thanh}
%}
%\fi



\newenvironment{proof}{\paragraph{\textit{Proof:}}}{}

%% Use \myexternaldocument on Overleaf
\myexternaldocument{ex_supplement}

% FundRef data to be entered by SIAM
%<funding-group>
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>
\newcounter{inlineenum}
\renewcommand{\theinlineenum}{\alph{inlineenum}}
\newenvironment{inlineenum}
  {\unskip\ignorespaces\setcounter{inlineenum}{0}%
   \renewcommand{\item}{\refstepcounter{inlineenum}{\textit{\theinlineenum})~}}}
  {\ignorespacesafterend}

\begin{document}
\maketitle
\begin{abstract}
 This work presents a novel algorithm for progressively adapting neural network architecture along the depth. In particular, we attempt to address the following questions in a mathematically principled way: i)  Where to add a new capacity (layer) during the training process? ii) How to initialize the new capacity? At the heart of our approach are two key ingredients: i) the introduction of a ``shape functional" to be minimized, which depends on neural network topology, and ii) the introduction of %The algorithm defining the 
a topological derivative of the shape functional %It is conceptually the derivative of a shape functional 
with respect to %infinitesimal changes in 
the neural network topology. Using an optimal control viewpoint, we show that the network topological derivative exists under certain conditions, and its closed-form expression is derived. In particular,  we explore, for the first time, the connection between the topological derivative from a topology optimization framework with the Hamiltonian from optimal control theory. Further, we show that 
%the derived expression for the topological derivative 
 the optimality condition for the shape functional
 leads to an eigenvalue problem for deep neural architecture adaptation. %The algorithm we derived 
 Our approach thus determines the most sensitive location along the depth where a new layer needs to be inserted during the training phase and the associated parametric initialization for the newly added layer.  We also demonstrate that our layer insertion strategy can be derived from an optimal transport viewpoint as a solution to maximizing a topological derivative in $p$-Wasserstein space, where $p\geq 1$. Numerical investigations with fully connected network, convolutional neural network, and vision transformer on various regression and classification problems demonstrate that our proposed approach can outperform an ad-hoc baseline network and other architecture adaptation strategies. Further, we also demonstrate other applications of topological derivative in fields such as transfer learning.
\end{abstract}

% REQUIRED
\begin{keywords}
  Neural architecture adaptation, Topological derivative, Optimal control theory, Hamiltonian, Optimal transport theory.
\end{keywords}

% REQUIRED
\begin{AMS}
  68T07,  68T05
\end{AMS}

\section{Introduction}
It has been observed that deep neural networks (DNNs) create increasingly
simpler but more useful  representations  of the learning problem layer by layer \cite{hinton2007learning, montavon2010layer,
  montavon2011kernel, zeiler2014visualizing}. Furthermore, 
empirical evidence supports the paradigm that  depth of a network is of crucial
importance \cite{russakovsky2015imagenet, simonyan2014very,
  szegedy2015going,krishnanunni2022layerwise}.
 Some of the problems associated with training such deep networks include: %\cite{low2019stacking}:  
i) a possible large training set is needed to overcome the over-fitting issue; ii) the architecture adaptability problem, e.g., any amendments to a pre-trained DNN, requires retraining even with transfer learning; iii) GPU employment is almost  mandatory due to %the %huge size of the
massive network and data sizes. 
Most importantly, it is often unclear on the number of layers and number of neurons to be used in each layer while training
a neural network for a specific task.  Therefore, there is a critical need for rigorous adaptive principles to guide the architecture design of a neural network.

\subsection{Related work}

Neural architecture adaptation algorithms can be broadly classified into two categories: i) neural architecture search algorithms and ii) principled adaptive strategies. 
Neural architecture search (NAS) algorithms rely on metaheuristic optimization,  reinforcement learning strategy, or Bayesian hyperparameter optimization to arrive at a reasonable architecture \cite{zoph2016neural, stanley2002evolving, suganuma2017genetic, elsken2018efficient, real2019regularized, balaprakash2018deephyper, miikkulainen2019evolving, liu2021survey,li2020random,kandasamy2018neural}. However, these strategies involve training and evaluating many candidate architectures (possibly deep) in the process and are thus computationally expensive. One key issue with NAS algorithms is that most of the methods report the performance of the best-found architecture, presumably resulting from a single run of the search process \cite{li2020random}. However,  random initializations of each candidate architecture could drastically influence the choice of best architecture prompting the use of multiple runs to select the best architecture, and hence prohibitively expensive. 
On the other hand, sensible adaptive strategies  are algorithms for growing neural networks where one
starts by training a small network and progressively increasing the size of the network (width/depth) \cite{wu2019splitting, wynne1993node,wu2020firefly,evci2022gradmax,krishnanunni2022layerwise,chen2015net2net}.

In particular, existing works have considered growing the width gradually by adding neurons for a fixed depth neural network \cite{wu2019splitting, wynne1993node,maile2022and,evci2022gradmax}. Liu et al. \cite{wu2019splitting} developed a simple criterion for
deciding the best subset of neurons to split and a splitting gradient for optimally updating the off-springs.  Wynne-Jones \cite{wynne1993node}  considered splitting the neurons (adding neurons) based on a principal component analysis on the oscillating weight vector.  Chen et al. \cite{chen2015net2net} showed that replacing a network with an equivalent network that is wider (has more neurons in
each hidden layer) allows the equivalent network to inherit the knowledge from the existing one and can be trained to further improve the performance. Firefly algorithm by Wu et al. \cite{wu2020firefly} generates candidate neurons that either split existing
neurons with noise or are completely new and selects those with the highest gradient norm.



On the other hand, many efforts have been proposed for growing neural architecture along the depth \cite{hettinger2017forward,kulkarni2017layer,wen2020autogrow}. Layerwise training of neural networks is an approach that addresses the issue of the choice of depth of a neural network and the computational complexity involved with training \cite{xu1999training}.   Hettinger et al. \cite{hettinger2017forward} showed that layers can be trained one at a time and the resulting DNN can generalize better. Bengio et al.  \cite{bengio2007greedy} proposed a greedy layerwise unsupervised learning algorithm where the initial layers of a network are supposed to represent more abstract concepts that explain the input observation, whereas subsequent layers extract low-level features. Recently, we devised a manifold regularized greedy layerwise training approach for adapting a neural architecture along its depth \cite{krishnanunni2022layerwise}. Net2Net algorithm \cite{chen2015net2net} introduced the concept of function-preserving transformations for rapidly transferring the information stored in one
neural network into another neural network. Wei et al. \cite{wen2020autogrow} proposed the AutoGrow algorithm to automate depth discovery in deep neural networks where new layers are inserted progressively
if the accuracy improves; otherwise, stops growing and
thus discovers the depth.






It is important to note that any algorithm for growing neural networks (width and/or depth) should adequately address the following questions in a mathematically principled way rather than a heuristic approach \cite{evci2022gradmax}: {\bf{When}} to add new capacity (neurons/layers)?; {\bf{Where}} to add new capacity?; {\bf{How}} to initialize the new capacity? 
Note that some efforts for growing neural networks along the width (adding neurons) have attempted to answer the above questions satisfactorily to some extent \cite{wu2019splitting,wu2020firefly,wynne1993node,evci2022gradmax,maile2022and}. However,  when it comes to growing neural architecture along the depth, to our best knowledge, the works cited above do not address the above questions adequately. Therefore, it is imperative to develop a mathematically principled depth-adaptation strategy that answers the above questions while also yielding superior performance in comparison to an ad-hoc baseline network. Further, note that since we know how to initialize the new capacity, the only source of randomness in the algorithm is in the initialization of the initial small network.

\subsection{Our contributions}

In this work, we derive an algorithm for progressively increasing a
neural networkâ€™s depth inspired by topology optimization.
%in a mathematically principled fashion. 
In particular, we provide solutions to the following questions: i) Where to add a new layer; ii)  When to add a new layer; and iii) How to initialize the new layer? 
%\tanbui{I thought our approach also answered when to add a new layer? BTW, when means time or something else?} \krish{Yes, when means time, at what point of training phase should we add a new layer} 
We shall present two versions of the algorithm: i) a semi-automated version where a predefined scheduler is used to decide when a new layer needs to be added during the training phase  \cite{evci2022gradmax}; ii) a fully automated growth process in which a validation metric is employed to automatically detect when a new layer needs to be added. Our method does not have the limitation that the loss function needs to plateau before growing such as those in \cite{wu2019splitting,kilcher2018escaping}. Our algorithm is based on deriving a closed-form expression for the topological derivative for a shape functional with respect to a neural network (\cref{exist_th}). To that end, in \cref{admissible_pert} and \cref{prop_admissible} we introduce the concept of admissible perturbation and suggest ways for constructing an admissible perturbation. In \cref{deriv_algo}, we show that the first-order optimality condition leads to an eigenvalue problem whose solution determines where a new layer needs to be inserted along with the associated parametric initialization for the added layer. 
The efficiency of our proposed algorithm is explored in \cref{experim} by carrying out extensive numerical investigations with different architectures on prototype regression and classification problems.  

\section{Mathematical framework}



In this work, all the matrices and vectors are represented in boldface. 
%\tanbui{need consistency: many vector/matrix quantities in the following are not boldface. You can boldface Greek letters with boldsymbol command}. 
Consider a regression/classification task where one is provided with $S$ training data points, input data dimension $n_0$, and label dimension $n_T$. Let the inputs $\bx_i \in \mathbb{R}^{n_0}$ %\tanbui{need to define $n_0$, $S$ and $n_T$} 
for $i \in \{1,2,...S\}$ be organized
row-wise into a matrix $\bX \in \mathbb{R}^{S\times {n_0}}$ and let the corresponding true labels be denoted as $\bc_i\in \mathbb{R}^{n_T}$ and  stacked row-wise as $\bC \in \mathbb{R}^{S\times {n_T}}$.
We start by considering the following empirical risk minimization problem with some loss function\footnote{More precisely the notation $\Phi_s$ may be used. } $\Phi$  for a typical feed-forward neural network:
%\krish{replace with semi-colon throughout}
\begin{equation}
    \begin{aligned}
        \hspace{0 cm} \min_{\boldsymbol{\theta}\in \Theta} \sJ(\Btheta)= \frac{1}{S}\sum_{s=1}^S \Phi\LRp{\bx_{s,T}},\quad \text{subject to:} \ \ \bx_{s,t+1}=\bff_{t+1}(\bx_{s,t};\ \Btheta_{t+1}), \ \ \ \bx_{s,0}=\bx_s,\\
        %  & \text{subject to:} \ \ \bx_{s,t+1}=f_{t+1}(\bx_{s,t},\ \Btheta_{t+1}),  \quad  t=0,\dots T-1, \quad s\in \{1,\dots S\},
    \end{aligned}
    \label{training_problem}
\end{equation}
where  $t=0,\dots T-1$, $ s\in \{1,\dots S\}$,  $\bff_{t+1}: \real^{n(t)}\times \Theta_{t+1}\mapsto \real^{n(t+1)}$ 
%\tanbui{Need to define the relationship between $\Theta$ and $\Btheta$} 
denotes the forward propagation function, $n(t)$ denotes the number of neurons in the $t$-th  layer, the parameter set $\Theta_{t+1}$ is a subset of a Euclidean space and  $\Theta=\Theta_1\times \dots \times \Theta_T$,
$\Btheta_{t+1}$ represents the network parameters corresponding to $(t+1)^{th}$ layer, $\boldsymbol{\theta}=\LRp{\Btheta_1,\dots,\Btheta_T}$ such that $\Btheta \in \Theta$ implies  $\Btheta_{t+1}\in \Theta_{t+1}$, $\bx_{s,t}$ represents the hidden states of the network,  $T-1$ is the total number of hidden layers in the network.  In the case of a fully connected network (FNN), one  has:
\[\bff_{t+1}(\bx_{s,t};\ \Btheta_{t+1})=\sigma_{t+1} \LRp{\bW_{t+1}\bx_{s,t}+\bb_{t+1}},\]
where $\bW_{t+1} \in \real^{n(t+1)\times n(t)}$, $\bb_{t+1} \in \real^{n(t+1)}$, and $\sigma_{t+1}$ is a nonlinear activation function acting component-wise on its arguments. In the following sections, for clarity, let us limit our discussion to the case of a fully-connected neural network.


%depends on the type of network employed such as fully connected neural network (FNN) or Convolutional neural network (CNN)  etc. 

\subsection{Optimal control viewpoint for neural network training}
\label{opt_top_full}

%\krish{Tan: Can you please checkpage 4 and 5 since I changed a few things, made things more clear, kept things simpler, removed distance function and all!}
Recently, there has been an interest in formulating deep learning as a  discrete-time
optimal control problem \cite{li2018optimal, benning2019deep}. Note that in the context of popular architectures such as residual neural networks (ResNet), equation \eqref{training_problem} can be interpreted as discretizations of an optimal control problem subject to an ordinary differential equation constraint \cite{benning2019deep}.
The Hamiltonian  for the $t$-th layer, $H_t: \real^{n(t)}\times \real^{n(t+1)}\times \Theta_{t+1} \mapsto \real$ corresponding to the loss in \eqref{training_problem} is defined as \cite{todorov2006optimal}: %\tanbui{need to define what $p$ is. Also, $x, p, \theta$ should have indices as they are for the $t$th hidden layer.}
\begin{equation}
   % H_t\LRp{\bx_{s,t},\bp_{s,t+1},\Btheta_{t+1}}=\bp_{s,t+1}\cdot \bff_{t+1}(\bx_{s,t},\Btheta_{t+1}),
    H_t\LRp{\bx_{s,t};\ \bp_{s,t+1};\ \Btheta_{t+1}}=\bp_{s,t+1} \cdot \bff_{t+1}(\bx_{s,t};\ \Btheta_{t+1}).
    \label{hamiltonian}
\end{equation}
%\tanbui{$\bff_{t+1}$ and other vector(ized) quantities need to be in boldface to be consistent with your convention.}
where, $\{ \bp_{s,1},\ \bp_{s,2},\dots \bp_{s,T} \}$ denote the adjoint variables computed during a particular gradient descent iteration (backpropagation) as follows \cite{todorov2006optimal}:% \krish{below equation (last equation) modified to denote gradient descent iteration which is important to connect with def 2.2.}%If the sequences $\{\btheta_1,\ \btheta_2, \dots \btheta_T\}$ and  $\{\bx_{s,1},\ \bx_{s,2}, \dots \bx_{s,T}\}$ are optimal solutions to \eqref{training_problem}, then the adjoints $\{\bp_{s,1},\ \bp_{s,2}, \dots \bp_{s,T}\}$ are given by \cite{todorov2006optimal}:
\begin{equation}
    \begin{aligned}
       & \bp_{s,T}=-\frac{1}{S}\nabla \Phi\LRp{\bx_{s,T}},\\
         & \bp_{s,t}=\nabla_{\bx} H_t\LRp{\bx_{s,t};\ \bp_{s,t+1};\ \Btheta_{t+1}}= \LRs{\nabla_{\bx}\bff_{t+1}(\bx_{s,t};\Btheta_{t+1})}^T\ \bp_{s,t+1},\\
         & \bx_{s,t+1}=\nabla_\bp H_t\LRp{\bx_{s,t};\bp_{s,t+1};\Btheta_{t+1}},\\
      %   & \nabla_{\btheta} H_t\LRp{\bx_{s,t};\ \bp_{s,t+1};\ \Btheta_{t+1}}=\LRs{\nabla_{\btheta}\bff_{t+1}(\bx_{s,t};\Btheta_{t+1})}^T\bp_{s,t+1}=0,\\
& \Btheta_{t+1}=\Btheta_{t+1}-\ell\ \nabla_{\btheta} H_t\LRp{\bx_{s,t};\ \bp_{s,t+1};\ \Btheta_{t+1}}=\Btheta_{t+1}-\ell\ \LRs{\nabla_{\btheta}\bff_{t+1}(\bx_{s,t};\Btheta_{t+1})}^T\bp_{s,t+1}.
\end{aligned}
    \label{set_of}
\end{equation}
where, $\nabla_{\bx}\bff_{t+1}$ denotes the gradient of $\bff_{t+1}$ with respect to the first argument (i.e. the state $\bx$) and $\nabla_{\btheta}\bff_{t+1}$ denotes the gradient of $\bff_{t+1}$ with respect to the second argument (i.e. the parameter $\btheta$).
%\tanbui{This really invites confusion and causes difficulties for the readers in that they have to keep track of which comma is for differentiation and which one is for derivative. You have already used $\nabla_{x}$ for the derivative w.r.t $x$, why not using $\nabla_{x}$ here?}. 
From a  Lagrangian viewpoint, the Hamiltonian can be understood as a device to generate the first-order necessary conditions for optimality \cite{todorov2006optimal}. 

%\krish{below statement added }
In the topological derivative approach, one often studies how the presence of a circular hole (or inclusion of new material) of radius $\epsilon$  at a specific location $\bz$ in the domain $\Omega_0\subset \real^2$ affects
the solution of a given partial differential equation (PDE) with prescribed boundary conditions \cite{amstutz2022introduction}.
%through a certain ``shape functional" $\sJ$ \cite{amstutz2022introduction}. %Here $\epsilon$ is the magnitude of perturbation and let us denote the perturbed domain as $\Omega_\epsilon$.
%if one denotes the perturbed domain (with the circular hole) denoted as $\Omega_{\epsilon}$, then it is clear that $\Omega_{\epsilon}\Big |_{\epsilon=0}=\Omega_0$.
In this work, we will first look at how the addition of a new layer at a specific location $l\in \{1,2,\dots T-1\}$ in the neural network $\Omega_0$ affects the solutions given by \eqref{set_of}. To that end, we will develop key concepts such as ``perturbed network" and ``admissible perturbation"  leading to the definition of a discrete topological derivative which we call as the ``network topological derivative" in our work.
\begin{definition}[Network perturbation]
\label{def_pert}
Consider $\sA$ as the set of all feed-forward neural networks with constant width `n' ($n \in \mathbb{N}$) in each hidden layer. %across  hidden layers  and arbitrary depth. 
Let $\Omega_0 \in \sA$ be a given neural network with $(T-1)$ hidden layers, and 
%$\LRp{l,\ \epsilon\boldsymbol{\phi},\ \sigma}$ be a 3-tuple  with $l\in \mathbb{N}$ and $1\leq l\leq T-1$, $ \boldsymbol{\phi}\in \real^{n_p}$, $\epsilon \in \real^{+}$, and 
$\sigma\LRp{.}:\real\mapsto \real$ be a non-linear activation function. A perturbation $\Omega_\epsilon$  from  $\Omega_0$ at $l\in \{1,2,\dots T-1\}$ along the ``direction"   $\boldsymbol{\phi}$ and with magnitude $\epsilon$ is defined as:
\[ \Omega_{\epsilon}={\Omega_0\oplus (l,\ \epsilon \boldsymbol{\phi}, \ \sigma)}, \quad s.t.\ \ \Omega_\epsilon \in \sA,\]
where $\oplus$ represents the operation of adding a layer between  the $l^{th}$ and $(l+1)^{th}$ layer of network $\Omega_0$ initialized with vectorized parameters (weights/biases) $\epsilon \boldsymbol{\phi}$  and activation $\sigma$ (refer \cref{fig:topo}). The added layer is denoted as $\LRp{l+\frac{1}{2}}$ in \cref{fig:topo}. Note that $\Omega_{\epsilon}$ is a also a function of $
l, \boldsymbol{\phi}$ and $\sigma$, but for the simplicity of the notation, we omit them.
\label{perturbedNetworkDef}
\end{definition}
\begin{comment}
\begin{definition}[Admissible perturbation]
\label{admissible_pert}
We say that the 3-tuple $\LRp{l,\ \epsilon\boldsymbol{\phi},\ \sigma}$ represents an admissible perturbation if:
    \[\lim_{\epsilon \rightarrow 0} \Omega_{\epsilon}=\Omega_\epsilon\Large |_{\epsilon=0}=\Omega_0,\]
%    \krish{Hi Tan, since $\Omega_\epsilon$ is a function, here again $\lim_{\epsilon \rightarrow 0}\Omega_{\epsilon}(\bx)=\Omega_\epsilon \Large |_{\epsilon=0}(\bx)$ might be a correct usage to discuss the limiting operation for the perturbed network? That is for any $\bx$, $\Omega_\epsilon$ is continuous  at $\epsilon=0$, with the limit network behaving the same as $\Omega_0$ at any hidden layer for any $\bx$ which is the second equality. Thus, introducing a new layer is a continuous operation in this sense. I was thinking about this more now and do you think to discuss the limiting operation we should work with $d(\Omega_{\epsilon},\ \Omega_{\epsilon}\Large |_{\epsilon=0})=\sup_{\bx \in \sC \subset \real^{n(0)}}\norm{\Omega_\epsilon(\bx)-\Omega_\epsilon \Large |_{\epsilon=0}(\bx)}_2, \ \sC$ compact space on which the data lies? }
 %   \tanbui{See my latest email to you. You mix things together as it is not clear/consistent in your thinking, or at least your expression is ambiguous. You tend to try to fix a thing locally without a global view (e.g. does it conflict/duplicate the other sections/variables/etc?). A paper is a coherent and integrated entity: it is not a patch of several duplicated/conflicted ideas/sections/definitions. Definition 2.1 $\Omega_\epsilon$ is simply a neural network architecture. If you like it to be a function, it is ok. But then it is the same as $x_{s,T}$ which is the output of the neural network. Why not using $x_{s,T}$ in the first place?}
 %   \krish{Hi Tan, $\Omega_\epsilon$ is always considered as a function throughout the paper even in definition 2.1 and wherever it appears later such as equation 2.8. If you look at definition 2.1, you will see that $\Omega_0: \real^{n(0)}\rightarrow \real^{n(T)}$ is a function and it belongs to the set A of feed forward networks of constant width. And if you see the definition of $\Omega_\epsilon$ it also lies in set A and thereby $\Omega_\epsilon$ is also a function by definition. If we use $\bx_{s,T}$ then I do not see a way to define perturbation in terms of adding a new layer and hence perturbed network function. In def 2.1, everytime network means network function. Ultimately the key thing is we are talking about function perturbation.}
 So by definition, any member of A is a neural network function. And in defining
    where the first equality denotes the limit of the perturbed network $\Omega_{\epsilon}$ under an appropriate metric.\footnote{For a set of neural networks, say $\mathbb{S}$, that have the same architecture (same number of layers, number of neurons on each layer, and activation functions), we can endow a natural metric for them. In particular, for any two neural networks  $\Omega^1$ and $\Omega^2$ in  $\mathbb{S}$, we define their distance as
    \[
    d_2\LRp{\Omega^1,\Omega^2} := \norm{\bs{W}^1 - \bs{W}^2}_2,
    \]
    where $\bs{W}^1 $ and $ \bs{W}^2$ are vectors containing all weights and biases for $\Omega^1$ and $\Omega^2$, respectively. Here, $\norm{\cdot}_2$ denotes the standard Euclidean norm.} The second equality identifies 
    %is used to denote the fact that  the network 
    %
    $\Omega_\epsilon\Large |_{\epsilon=0}$  
    %behaves exactly the same  as 
    with $\Omega_0$ as they behave the same under gradient-based training process. In particular,
   % \tanbui{you need to define what the standard is. I do not think there is a standard out there}
     the added layer is redundant and only acts as a ``message-passing layer".\footnote{More precisely, at $\epsilon=0$, the adjoints $\bp_{s,t}$ and states $\bx_{s,t}$ in \eqref{set_of}, $\forall t\in \{1,2,\dots l,\ l+1,\dots T\}$ of the limit network $\Omega_\epsilon \Large |_{\epsilon=0}$ matches with that of $\Omega_0$ and the gradients of the added layer $\nabla_{\btheta}\bff_{l+\frac{1}{2}}(\bx_{s,l+\frac{1}{2}};\ {\bf{0}} )={\bf{0}}$.} 

% \krish{ $\Omega_\epsilon$ is a neural network function that acts on $\bx \in \real^{n(0)}$. So the above limiting process has the definition below:
%      \[   \forall \epsilon_1>0, \forall \bx \in \real^{n(0)}, \exists \delta=\delta\LRp{\bx,\epsilon_1} >0\colon \forall \epsilon \in \real \LRp{|\epsilon|< \delta \implies \norm{\Omega_\epsilon(\bx)-\Omega_\epsilon \Large |_{\epsilon=0}(\bx)}_2<  \epsilon_1}.\]}
    %\tanbui{I am not sure if we over-state/over-commit here. First, note that only in the limit that they are the same. In the proof of the Proposition 1, you do not provide any evidence that the training process would be the same: note that the training process requires derivative, gradient descents, etc. All you showed there is that the evaluation of the two neural networks are the same in the limit.}
\end{definition}
\end{comment}
%\begin{definition}[Network action]
%\label{network_action}
 %   We equip any network  $\Omega \in \sA$ in \cref{def_pert} with the following action $\Omega(\bx):\real^{n(0)}\rightarrow \real^{n(T)}$, where the action of network $\Omega$ on some $\bx_s\in \real^{n(0)}$ is defined by the forward propagation equation (constraint) given in \eqref{training_problem}.
%\end{definition}
In the rest of the paper (and in \cref{def_pert}), the notations $\Omega_0,\ \Omega_{\epsilon}$ denote the neural network with the list of all parameters and activation functions.
 In addition, we adopt the notation $\Omega_0(\bx_s):\real^{n_0}\mapsto \real^{n_T}$, and  $\Omega_\epsilon(\bx_s; \  \epsilon \bphi):\real^{n_0}\times \real^{n_p}\mapsto \real^{n_T}$ to denote the neural network functions (forward propagation constraint given in \eqref{training_problem}) where the second argument $ \epsilon\bphi$ denotes the parameters of the added layer for $\Omega_\epsilon$. Now it is necessary that by setting the magnitude of perturbation $\epsilon=0$ in \cref{def_pert}, 
the network $\Omega_\epsilon \Big |_{\epsilon=0}$ should behave exactly the same as
$\Omega_0$ under the gradient-based training process. We formalize this notion  in \cref{admissible_pert} below.  
%To \krish{quantify the discrepancy in behavior of $\Omega-\epsilon$}that end, let us define a distance function $d_2^K\LRp{.;\Omega_0}:\mathbb{S}_{\Omega_0}\mapsto \real$ as follows:

\begin{definition}[Admissible perturbation]
\label{admissible_pert}
We say that $\Omega_\epsilon$ in \cref{def_pert}  is an admissible perturbation  if:
\begin{equation}
\Omega_{\epsilon}\Big |_{\epsilon=0}=\Omega_0,
\label{admit_per}
\end{equation}
 where `=' in \eqref{admit_per} is used to denote the fact that  the network $\Omega_\epsilon\Large |_{\epsilon=0}$  behaves exactly the same  as $\Omega_0$ under gradient based training process. In particular,
   % \tanbui{you need to define what the standard is. I do not think there is a standard out there}
     the added layer in $\Omega_{\epsilon}\Big |_{\epsilon=0}$ is redundant and only acts as a message-passing layer. Furthermore,  the solutions  given by \eqref{set_of} for $t=\{0,1,\dots l,(l+1),\dots T\}$ and the loss $\sJ$ in \eqref{training_problem} together with its gradient  coincide for both $\Omega_0$ and $\Omega_\epsilon \Big |_{\epsilon=0}$ at every gradient descent iteration.
     \end{definition}  
%\begin{remark}
%Condition \eqref{admit_per}    ensures that in the limit $\epsilon \rightarrow 0$ (magnitude of perturbation),
 %    the network $\Omega_\epsilon$ behaves the same as
%$\Omega_0$ under the gradient-based training process.  In particular, 
 % the added layer in $\Omega_\epsilon$ is redundant and only acts as a ``message-passing layer". 
  %Note that the last term in \eqref{distance_metric} ensures that the parameters of the added layer never get updated during training.
  %      \end{remark}    
\begin{proposition}[Construction of an admissible perturbation]
\label{prop_admissible}
Consider \cref{admissible_pert}. The following two steps produce an admissible perturbation:
%An admissible perturbation can be constructed by making the following choices: 
\begin{enumerate}
   \item Choose $\sA$ as the set of all residual neural networks where the hidden layer propagation equation in \eqref{training_problem} 
   %\tanbui{please use the  eqref command instead of ref for all equations so that we automatically have the parentheses} 
   is written as:
   \begin{equation}
       \bx_{s,t+1} =\bff_{t+1}(\bx_{s,t};\ \Btheta_{t+1})=\bx_{s,t}+\bg_{t+1}(\bx_{s,t};\ \Btheta_{t+1}),\quad t=1,\dots T-2.
       \label{res_o}
   \end{equation}
   \label{one}
   \item Choose the activation function $\sigma$ such that $\bg_{t+1}\LRp{.;\ .}$ is continuously differentiable  w.r.t both the arguments and:
   \[ \bg_{t+1}(\bx_{s,t};\ {\bf{0}})={\bf{0}}, \ \ \nabla_{\bx}\bg_{t+1}(\bx_{s,t};\ {\bf{0}})={\bf{0}}, \ \ \nabla_{\btheta}\bg_{t+1}(\bx_{s,t};\ {\bf{0}})={\bf{0}}.\]
    \label{two}
 %   \item Choose $\LRp{l,\ \epsilon\boldsymbol{\phi},\ \sigma}$ such that $\sigma(0)=0,\ \sigma'(0)=0$.
\end{enumerate}
\end{proposition}

\begin{proof}{}{}
 %Let us denote the newly added hidden layer as the  $(l+\frac{1}{2})^{th}$ hidden layer and 
 Let us compute the states and adjoints in \eqref{set_of} for network $\Omega_{\epsilon}$ corresponding to the first gradient descent iteration. The forward propagation of residual neural network $\Omega_\epsilon$ from $l^{th}$ layer to $(l+1)^{th}$ layer can be  written as:
\begin{subequations}
\begin{gather}
\label{forr_a}
\bx_{s,l+\frac{1}{2}} =\bff_{l+\frac{1}{2}}(\bx_{s,l};\ \epsilon \bphi)= \bx_{s,l} + \bg_{l+\frac{1}{2}}(\bx_{s,l};\ \epsilon \bphi),\\
\bx_{s,l+1} = \bff_{l+1}\LRp{ \bx_{s,l+\frac{1}{2}};\ \Btheta_{l+1}}= \bx_{s,l+\frac{1}{2}} + \bg_{l+1}\LRp{ \bx_{s,l+\frac{1}{2}};\ \Btheta_{l+1}}.
\end{gather}
\label{forr}
\end{subequations}
Now, for $\epsilon=0$ \eqref{forr_a}  gives
%and assuming $\bg_{l+\frac{1}{2}}\LRp{.,\ .}$ to be continuous everywhere w.r.t second argument yields:
\[   \bx_{s,l+\frac{1}{2}} =\bx_{s,l} , \implies \bx_{s,l+1} = \bx_{s,l} +\bg_{l+1}\LRp{ \bx_{s,l} ;\ \Btheta_{l+1}}=\bff_{l+1}\LRp{ \bx_{s,l};\ \Btheta_{l+1}} . \]
where we have used the fact that $\bg_{l+\frac{1}{2}}(\bx_{s,l};\ {\bf{0}})={\bf{0}}$.
%, since $\sigma_{(l+\frac{1}{2})}(0)=0$.
Therefore, adding a new layer recovers the forward propagation equations of the original network $\Omega_0$ when $\epsilon = 0$ for the first gradient descent iteration. Now, the backward propagation of the adjoint using \eqref{set_of}  can be written as:
%\tanbui{You haven't introduced adjoint anywhere.}
\begin{equation}
\begin{aligned}
   \bp_{s,l+\frac{1}{2}}&= \LRs{\nabla_{\bx}\bff_{l+1}(\bx_{s,l+\frac{1}{2}};\Btheta_{l+1})}^T\bp_{s,l+1}=\bp_{s,l+1} + \LRs{\nabla_{\bx}\bg_{l+1}\LRp{ \bx_{s,l+\frac{1}{2}};\ \Btheta_{l+1}}}^T\bp_{s,l+1} ,\\
   \bp_{s,l} & = \LRs{\nabla_{\bx}\bff_{l+\frac{1}{2}}\LRp{ \bx_{s,l};\ \epsilon\bphi}}^T\bp_{s,l+\frac{1}{2}} =\bp_{s,l+\frac{1}{2}} + \LRs{\nabla_{\bx}\bg_{l+\frac{1}{2}}\LRp{ \bx_{s,l};\ \epsilon\bphi}}^T\bp_{s,l+\frac{1}{2}}.
\end{aligned}
\label{for}
\end{equation}
Again using $ \nabla_{\bx}\bg_{l+\frac{1}{2}}\LRp{ \bx_{s,l};\ \bf{0}}={\bf{0}}$, we conclude
 %by %assumption \ref{two}.
% Therefore we have:
\[\bp_{s,l}=\bp_{s,l+\frac{1}{2}}.\]
Therefore, when $\epsilon = 0$, the adjoint of the perturbed network and the original one are the same for the first gradient descent iteration. 
Finally, the third condition in \cref{two} for the added layer, i.e. $\nabla_{\btheta}\bg_{l+\frac{1}{2}}(\bx_{s,l};\ {\bf{0}})={\bf{0}}$ implies  that $\nabla_{\btheta}\bff_{l+\frac{1}{2}}(\bx_{s,l};\ {\bf{0}})={\bf{0}}$. Therefore,
for $\epsilon= 0$, the gradients for updating the parameters of the added layer given by the last equation in \eqref{set_of} is ${\bf{0}}$, leading to the parameters of the added layer not getting updated at the end of the first gradient descent iteration. Applying the above arguments recursively for subsequent gradient descent iterations concludes the proof.
%\[\lim_{\epsilon\rightarrow 0}d_2^{K}\LRp{\Omega_{\epsilon};\Omega_0} =0,\quad \forall K\in \mathbb{N}.\]
%one also recovers the adjoint propagation equation of the original network $\Omega_0$  as $\epsilon \rightarrow 0$. Further, the added layer cannot be trained since the gradient of the loss $\sJ$ w.r.t the parameters $\epsilon \bphi$ is {\bf{0}} as  $\epsilon \rightarrow 0$ which is again due to the assumption that  $\bg_{l+\frac{1}{2},2}\LRp{ \bx_{s,l},\ {\bf{0}}}={\bf{0}}$. Therefore, 
%the added layer acts as a message passing layer  and hence the network $\Omega_\epsilon$ behaves exactly the same way as $\Omega_0$ when $\epsilon \rightarrow 0$.
\end{proof}

\cref{fig:topo} shows our layer insertion strategy where a new layer with parameters $\epsilon \bphi$ is inserted at the location between layers 1 and 2 of the new network $\Omega_0$. %\tanbui{the figure is shown way ahead of this statement. It should be right after the argument.}

\def\layersep{1.4cm}
\def\nodeinlayersep{0.7cm}
\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=\layersep,
    edge/.style={-stealth,shorten >=1pt, draw=black!50,thin},
    neuron/.style={circle,fill=black!25,minimum size=10pt,inner sep=0pt},
    operator/.style={rectangle,fill=green!,minimum height= \nodeinlayersep, minimum width= 0.8 * \layersep, inner sep=0pt, rounded corners},
    input neuron/.style={neuron, fill=green!50,minimum size=12pt},
    output neuron/.style={neuron, fill=green!50,minimum size=12pt},
    hidden neuron/.style={neuron, fill=blue!50},
    Forward map/.style={operator, fill=red!50},
    annot/.style={text width=4em, text centered},
    every node/.style={scale=1.0},
    node1/.style={scale=2.0}
]
    % Draw the input layer nodes
    \foreach \name / \y in {1,...,4}
        {\ifnum \y=3
            \node (I-\name) at (0,-\nodeinlayersep * \y - \nodeinlayersep * 0.5) {$\vdots$};
            % \node[input neuron] (I-\name) at (0,-\y-0.4) {$y_{\y}$};
        \else
            \ifnum \y=4
                \node[input neuron] (I-\name) at (0,-\nodeinlayersep * \y - \nodeinlayersep * 0.5) {$x_{n}$};
            \else
                \node[input neuron] (I-\name) at (0,-\nodeinlayersep *\y - \nodeinlayersep * 0.5 ) {$x_{\y}$};
            \fi
        \fi}
        
    % Draw the output layer node
    \foreach \name / \y in {1,...,4}
        {\ifnum \y=3
            \node (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.5) {$\vdots$};
            % \node[input neuron] (I-\name) at (0,-\y-0.4) {$y_{\y}$};
        \else
            \ifnum \y=4
                \node[input neuron] (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.5) {$y_{p}$};
            \else
                \node[input neuron] (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.5) {$y_{\y}$};
            \fi
        \fi}
        

    
    
        
    
    % set number of hidden layers
    \newcommand \Nhidden{2}
    % Draw the hidden layer nodes
    \foreach \N in {1,...,\Nhidden} {
        \foreach \y in {1,...,5} { %%% MODIFIED (1,...,12 -> 1,...,5, and the next five lines)
            \ifnum \y=4
                \node at (\N*\layersep,-\y*\nodeinlayersep) {$\vdots$};
            \else
                \node[hidden neuron] (H\N-\y) at (\N*\layersep,-\y*\nodeinlayersep ) {$\sigma$};
        \fi
      }
    }

    %%% <-- MODIFIED (from H\Nhidden-6 to H\Nhidden-3) 
    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,2,4}
        \foreach \dest in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
            \draw[edge] (I-\source) -- (H1-\dest);
    
    % connect all hidden stuff
    \foreach [remember=\N as \lastN (initially 1)] \N in {2,...,\Nhidden}
      \foreach \source in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
          \foreach \dest in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
              \draw[edge] (H\lastN-\source) -- (H\N-\dest);
              
    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
        \foreach \dest in {1,2,4}
            \draw[edge] (H\Nhidden-\source) -- (O-\dest);
    

    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1*\layersep,-.7*\nodeinlayersep) -- (1*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=1$};
  

    
    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2*\layersep,-.7*\nodeinlayersep) -- (2*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=2$};
       \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1.5*\layersep,-.7*\nodeinlayersep) -- (1.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=4cm,black]{Original network $\Omega_0$};

      % \draw [dashed] (5.5,-4.8) -- (5.5,0.5);
      

\end{tikzpicture}
\hspace{2 cm}
\begin{tikzpicture}[
    node distance=\layersep,
    edge/.style={-stealth,shorten >=1pt, draw=black!50,thin},
    neuron/.style={circle,fill=black!25,minimum size=10pt,inner sep=0pt},
    operator/.style={rectangle,fill=green!,minimum height= \nodeinlayersep, minimum width= 0.8 * \layersep, inner sep=0pt, rounded corners},
    input neuron/.style={neuron, fill=green!50,minimum size=12pt},
    output neuron/.style={neuron, fill=green!50,minimum size=12pt},
    hidden neuron/.style={neuron, fill=blue!50},
    Forward map/.style={operator, fill=red!50},
    annot/.style={text width=4em, text centered},
    every node/.style={scale=1.0},
    node1/.style={scale=2.0}
]
    % Draw the input layer nodes
    \foreach \name / \y in {1,...,4}
        {\ifnum \y=3
            \node (I-\name) at (0,-\nodeinlayersep * \y - \nodeinlayersep * 0.5) {$\vdots$};
            % \node[input neuron] (I-\name) at (0,-\y-0.4) {$y_{\y}$};
        \else
            \ifnum \y=4
                \node[input neuron] (I-\name) at (0,-\nodeinlayersep * \y - \nodeinlayersep * 0.5) {$x_{n}$};
            \else
                \node[input neuron] (I-\name) at (0,-\nodeinlayersep *\y - \nodeinlayersep * 0.5 ) {$x_{\y}$};
            \fi
        \fi}
        
    % Draw the output layer node
    \foreach \name / \y in {1,...,4}
        {\ifnum \y=3
            \node (O-\name) at (4*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.5) {$\vdots$};
            % \node[input neuron] (I-\name) at (0,-\y-0.4) {$y_{\y}$};
        \else
            \ifnum \y=4
                \node[input neuron] (O-\name) at (4*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.5) {$y_{p}$};
            \else
                \node[input neuron] (O-\name) at (4*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.5) {$y_{\y}$};
            \fi
        \fi}
        

    
    
        
    
    % set number of hidden layers
    \newcommand \Nhidden{3}
    % Draw the hidden layer nodes
    \foreach \N in {1,...,\Nhidden} {
        \foreach \y in {1,...,5} { %%% MODIFIED (1,...,12 -> 1,...,5, and the next five lines)
            \ifnum \y=4
                \node at (\N*\layersep,-\y*\nodeinlayersep) {$\vdots$};
            \else
                \node[hidden neuron] (H\N-\y) at (\N*\layersep,-\y*\nodeinlayersep ) {$\sigma$};
        \fi
      }
    }

    %%% <-- MODIFIED (from H\Nhidden-6 to H\Nhidden-3) 
    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,2,4}
        \foreach \dest in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
            \draw[edge] (I-\source) -- (H1-\dest);
    
    % connect all hidden stuff
    \foreach [remember=\N as \lastN (initially 1)] \N in {2,...,\Nhidden}
      \foreach \source in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
          \foreach \dest in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
              \draw[edge] (H\lastN-\source) -- (H\N-\dest);
              
    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
        \foreach \dest in {1,2,4}
            \draw[edge] (H\Nhidden-\source) -- (O-\dest);
    

    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1*\layersep,-.7*\nodeinlayersep) -- (1*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=1$};
  

    
    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2*\layersep,-.7*\nodeinlayersep) -- (2*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=1.5$};

        \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (3*\layersep,-.7*\nodeinlayersep) -- (3*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=2$};
        
    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1.5*\layersep,-.7*\nodeinlayersep) -- (1.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,above=0.5cm,black]{Added layer with };

    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1.5*\layersep,-.7*\nodeinlayersep) -- (1.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,above=0.1cm,black]{ parameters $\epsilon \boldmath\phi$};

    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2.0*\layersep,-.7*\nodeinlayersep) -- (2.0*\layersep,-.7*\nodeinlayersep) node[pos=1.8,below=4cm,black]{ Perturbed network $\Omega_{\epsilon}$};

%\draw [decorate,decoration={brace,amplitude=5pt,mirror,raise=4ex}]  (2.7,-1.1) -- (1.5,-1.1);
\draw[->, thick] (2,-0.3) -- (2,-0.65);
%\draw [|<->|] (1.5,.4) -- node[above=1mm] {5mm} (2,.4);
\end{tikzpicture}
\caption{Schematic view of the topological derivative approach: A new layer with parameters $\epsilon \bphi$ is inserted between the $1^{st}$ and $2^{nd}$ layer. When $\epsilon=0$, the network $\Omega_\epsilon$ behaves exactly the same way as $\Omega_0$ under the standard training process (Residual connections are not shown in the figure).} %\tanbui{the upperbrace in the right figure is confusing. An arrow pointing to the middle hidden layer is a better notation for the insertion of a middle layer.}}
\label{fig:topo}
\end{figure}

\begin{remark}{}{}$ $
\label{act_const}
\begin{enumerate}
    \item Note that for typical neural networks such as FNN's and CNN's,  condition \ref{two} can be easily realized by choosing continuously differentiable activation function $\sigma$ with the property that $\sigma(0)=0,$ and $\sigma'(0)=0$.
    \item   The activation $\sigma(x)$  for the added layer can be constructed as a linear combination of some existing activation functions, i.e $\sigma(x)=\alpha_1\sigma_1(x)+\sigma_2(x)$, where $\sigma_1(0)=\sigma_2(0)=0$ and $\alpha_1=-\frac{\sigma_2'(0)}{\sigma'_1(0)}$, $\sigma_1'(0)\neq 0$. For instance, the set $\sF=\{\sigma_1,\ \sigma_2 \}$ can be chosen as follows:
\begin{equation}
    \sF=\{ Swish,\ tanh\},\ \{ Mish,\ tanh\},\  \{ Swish,\ Mish\},\ \ etc.
    \label{univer}
\end{equation}
Note that it can be proved that the constructed function $\sigma(x)$ based on the choices in \eqref{univer}
%\tanbui{you meant from the second remark? If yes, then this should belong to the second remark.} 
is universal\footnote{Let $\sN\sN^{\sigma}_{n,m,k}$ represent the class of functions $\real^n \rightarrow \real^m$ described by feedforward neural networks of arbitrary number of hidden layers, each with $k$ neurons and activation
function  $\sigma$. Let $K\subseteq \real^n$ be compact. We say that the activation function $\sigma(x)$ is universal if $\sN\sN^{\sigma}_{n,m,k}$ is dense in $C(K;\real^m)$ with respect to the uniform norm.} by verifying the conditions in \cite{kidger2020universal} or \cite{bui2024unified}.
\end{enumerate}
 
\end{remark}

\subsection{Topological derivative}
\label{full_topo}

The concept of topological derivative
was formally introduced  as the "bubble method" for the
optimal design of structures \cite{eschenauer1994bubble}, and further studied in detail by Sokolowski and Zochowski \cite{sokolowski1999topological}. The topological derivative is, conceptually, a derivative of a shape functional with respect to infinitesimal changes in its topology, such as adding an infinitesimal hole or crack.
The concept finds enormous application in the field of structural mechanics, image processing and inverse problems \cite{amstutz2022introduction}. In the field of structural mechanics, the concept has been used to detect and locate cracks for a simple model problem: the steady-state heat equation with the heat flux imposed and the temperature measured on the boundary \cite{amstutz2005crack}. In the field of image processing, the topological derivative has been used to derive a non-iterative algorithm to perform edge detection and image restoration by studying the impact of an insulating crack in the domain \cite{belaid2006image}. In the field of inverse problems, the topological derivative approach has been applied to tomographic reconstruction problem \cite{auroux2010application}. 
%A key challenge in the field is to derive a closed-form expression for the same. 

Inspired by the topological derivative approach in mechanics, we define the ``network topological derivative'' for a feed-forward neural network and derive its explicit expression. Note that the admissible perturbation (adding a layer) defined in  \cref{admissible_pert} can be viewed as an infinitesimal change in the neural network topology. 
In order to formally define the ``network topological derivative'' let us first rewrite the loss function \eqref{training_problem}  as follows:
\begin{equation}
     \sJ(\Omega_\epsilon)= \frac{1}{S}\sum_{s=1}^S\Phi\LRp{\Omega_\epsilon\LRp{\bx_{s,0};\ \epsilon\bphi}},
     \label{for_later}
\end{equation}
where $\epsilon \bphi$ is the initialization of the added layer in  $\Omega_\epsilon$ (see \cref{def_pert}).
%\krish{Hi Tan, could you please check the below definition. Looks much better without using the 3-tuple! }
\begin{definition}[Network topological derivative]
%Let $\boldsymbol{\phi} \in \real^n$, where $n$ is the total number of parameters in the added  layer. 
Consider an admissible perturbation $\Omega_\epsilon$ in \cref{admissible_pert}. We say the loss functional $\sJ$ in \eqref{for_later} admits a network topological derivative\textemdash denoted as $d\sJ(\Omega_0;\ (l,\  \boldsymbol{\phi},\ \sigma))$\textemdash at $\Omega_0\in \sA$ and at the location $l\in \{1,2\dots T-1\}$ along the direction $\boldsymbol{\phi}$  if there exists a  function $q: \real^{+}\mapsto \real^+$ with $\lim\limits_{\epsilon \downarrow  0}q(\epsilon)=0$ such that the following ``topological asymptotic expansion" holds:
\begin{equation}
    \sJ(\Omega_\epsilon)=\sJ(\Omega_0)-q(\epsilon)d\sJ(\Omega_0;\ (l,\  \boldsymbol{\phi},\ \sigma))+o(q(\epsilon)),
    \label{asymptotic_exp}
\end{equation}
where $o(q(\epsilon))$ is the remainder\footnote{The notation $p(\epsilon)=o(q(\epsilon))$ means that $\lim_{\epsilon \rightarrow 0}\frac{p(\epsilon)}{q(\epsilon)}=0$, where $q(\epsilon)\neq 0$ for all $\epsilon \neq 0$.}. \eqref{asymptotic_exp} is equivalent to the existence of the following limit:
\begin{equation}
    d\sJ(\Omega_0;\ (l,\  \boldsymbol{\phi},\ \sigma))=-\lim_{\epsilon \downarrow 0}\frac{\sJ(\Omega_\epsilon)-\sJ(\Omega_0)}{q(\epsilon)},
    \label{topo_der}
\end{equation}
%\tanbui{You need to define what "small-oh" notation here.}
\end{definition}
%\krish{modified below remark}
\begin{remark}
Note that the `-' sign is used in \eqref{topo_der} to ensure that 
    a positive derivative favors layer addition and vice versa.  
    %Further, note that \eqref{topo_der} is a discrete derivative that can be evaluated only at $l\in \{1,2,\dots T-1\}$. 
    Since for an  admissible perturbation $\Omega_\epsilon$ in \cref{admissible_pert}, we have $\sJ\LRp{\Omega_{\epsilon}\Big |_{\epsilon=0}}=\sJ\LRp{\Omega_0}$, 
the ``topological asymptotic expansion" in \eqref{asymptotic_exp},  can  be understood as the following Taylor series expansion:
    \begin{equation}
            \sJ(\Omega_\epsilon)=\sJ\LRp{\Omega_\epsilon\Big |_{\epsilon=0}}+ \epsilon\LRs{\frac{d}{d\epsilon}\LRp{\sJ\LRp{\Omega_\epsilon}}}_{\epsilon=0}+ \frac{\epsilon^2}{2}\LRs{\frac{d^2}{d\epsilon^2}\LRp{\sJ\LRp{\Omega_\epsilon}}}_{\epsilon=0}+\dots
    \label{eq:Taylor}
    \end{equation}
where $\sJ\LRp{\Omega_\epsilon}$ is given by \eqref{for_later}. In \cref{exist_th}  we will unveil a non-trivial connection between 
the  Hamiltonian $H_l$ introduced in \eqref{hamiltonian} with the network topological derivative  in \eqref{topo_der}.%Note that if $q(\epsilon) = \epsilon$, then $d\sJ(\Omega_0;\ (l,\ \boldsymbol{\phi},\ \sigma))$ is a directional derivative of $\sJ$ along the direction $\boldsymbol{\phi}$. \krish{discrete derivative}
\end{remark}


 



\begin{theorem}[Existence of network topological derivative]
\label{exist_th}
Assume the conditions in \cref{prop_admissible} and the loss $\sJ$ in \eqref{training_problem}. Further, let $\mathcal{X}_t$ be the set of all possible states $\bx_{s,t}$ reachable\footnote{We say that a state $\bx_{s,t+1}$ is reachable at layer $(t+1)$ if there exists $\btheta \in \Theta$ and  sample $\bx_{s,0}$ such that the constraint in \eqref{training_problem} holds.}  at layer $t$ from all possible  initial sample $\bx_{s,0}$ 
and trainable parameters in $\Theta$.  Assume that
\begin{enumerate}
    \item $\Phi$ has bounded second and third order derivatives on $\mathcal{X}_T$; 
   % \tanbui{You meant $t$ or $T$?}\krish{It is $T$ to denote the set of states at output layer}
   \label{cond_one}
    \item $\bff_{t+1}(.;\ .)$ has bounded second and third order derivatives on $\mathcal{X}_t \times \Theta_{t+1}$.
    \label{cond_two}
\end{enumerate}
Then, the network topological derivative given by \eqref{topo_der} exists with $q(\epsilon)=\epsilon^2$ and is given by:
\begin{equation}
     d\sJ(\Omega_0;\ (l,\  \boldsymbol{\phi},\ \sigma))=\frac{1}{2}\sum_{s=1}^S \boldsymbol{\phi}^T \ \nabla^2_{\boldsymbol{\theta}} H_l\LRp{\bx_{s,l};\bp_{s,l};\Btheta}\Big |_{\boldsymbol{\theta}={\bf{0}}}\ \boldsymbol{\phi}
     \label{topo_de}
\end{equation}
where $H_l$ is the Hamiltonian defined in \eqref{hamiltonian}\footnote{Note the change in subscript  for the adjoint variable $\bp$ while evaluating the Hamiltonian.}.






\end{theorem}


\begin{proof}
%Let us denote the newly added hidden layer with parameters $\epsilon \bphi$ as the  $\LRp{l+\frac{1}{2}}^{th}$ hidden layer.  
Let us represent the parameters of the perturbed network $\Omega_\epsilon$ as $\btheta^\epsilon$ and the parameters of the original network $\Omega_0$ as  $\btheta^0$. For an admissible perturbation $\Omega_\epsilon$ in \cref{admissible_pert},   $\Omega_0$ is equivalent to  $\Omega_\epsilon\Large |_{\epsilon=0}$. We thus identify $\Omega_0$ with $\Omega_\epsilon\Large |_{\epsilon=0}$. 
By  \cref{perturbedNetworkDef} $\btheta^\epsilon_i=\btheta^0_i,\ \forall i\neq l+\frac{1}{2}$.  Define
\[ \delta \bx_{s,t}=\bx_{s,t}^{{\btheta}^\epsilon}-\bx_{s,t}^{\btheta^0},\quad \delta \bp_{s,t}=\bp_{s,t}^{\btheta^\epsilon}-\bp_{s,t}^{\btheta^0},
\]
and we have
\[  \delta \bx_{s,t}=0,\ \ \forall t\leq l.\]
Applying definition \eqref{hamiltonian} for both $\Omega_{\epsilon}$ and $\Omega_0$ gives
\begin{subequations}
\begin{gather}
\label{hamil_cases_a}
\sum_{t=0}^{T-1} \LRp{H_t\LRp{\bx_{s,t}^{\theta^0}+\delta \bx_{s,t};\ \bp_{s,t+1}^{\btheta^0}+\delta \bp_{s,t+1};\ \btheta_{t+1}^\epsilon}-(\bp_{s,t+1}^{\btheta^0}+\delta \bp_{s,t+1})^T(\bx_{s,t+1}^{\theta^0}+\delta \bx_{s,t+1})}=0,\\
\label{hamil_cases_b}
\sum_{t=0}^{T-1} \LRp{H_t\LRp{\bx_{s,t}^{\theta^0};\ \bp_{s,t+1}^{\btheta^0};\ \btheta_{t+1}^0}-(\bp_{s,t+1}^{\btheta^0})^T(\bx_{s,t+1}^{\theta^0})}=0.
\end{gather}
\label{hamil_cases}
\end{subequations}
Note that the  summation from $t=0$ to $t=T-1$ also includes $t=\LRp{l+\frac{1}{2}}$. Subtracting   \eqref{hamil_cases_b} from \eqref{hamil_cases_a} yields:
\begin{equation}
    \begin{aligned}
  &  \sum_{t=0}^{T-1} H_t\LRp{\bx_{s,t}^{\theta^0}+\delta \bx_{s,t};\ \bp_{s,t+1}^{\btheta^0}+\delta \bp_{s,t+1};\ \btheta_{t+1}^\epsilon}- H_t\LRp{\bx_{s,t}^{\theta^0};\ \bp_{s,t+1}^{\btheta^0};\ \btheta_{t+1}^0}=\\
  &  \LRp{\delta \bp_{s,l+\frac{1}{2}}\cdot \delta \bx_{s,l+\frac{1}{2}}}+\LRp{\bp_{s,l+\frac{1}{2}}^{\btheta^0}\cdot \delta \bx_{s,l+\frac{1}{2}}}\\
   &+\underbrace{\sum_{t=l}^{T-1} \LRp{\delta \bp_{s,t+1}\cdot \delta \bx_{s,t+1}}}_{I}+\underbrace{\sum_{t=l}^{T-1} \LRp{\bp_{s,t+1}^{\btheta^0}\cdot \delta \bx_{s,t+1}}}_{II}+\underbrace{\sum_{t=0}^{T-1} \LRp{\delta \bp_{s,t+1}\cdot \bx_{s,t+1}^{\btheta_0}}}_{III}.
\end{aligned}
\label{decomposition}
\end{equation}
Now let's analyze each term on the right-hand side of \eqref{decomposition}. 

\hspace{-0.8 cm} {\bf{\underline{Analyzing Term I in \eqref{decomposition}}}}


    \[  \delta \bx_{s,l+\frac{1}{2}}=\bff_{l+\frac{1}{2}}\LRp{\bx_{s,l}^{\btheta_\epsilon};\ \epsilon \bphi}-\bff_{l+\frac{1}{2}}\LRp{\bx_{s,l}^{\btheta_0};\ \boldsymbol{0}}.\]
  Assuming that $\bff_{l+\frac{1}{2}}(.;.)$ has bounded second order derivative w.r.t the second argument and noting that $\bx_{s,l}^{\btheta_\epsilon}=\bx_{s,l}^{\btheta_0}$,  Taylor series approximation about $\epsilon=0$ gives:
    \[ \delta \bx_{s,l+\frac{1}{2}}=\epsilon \ \nabla_{\btheta}\bff_{l+\frac{1}{2}}\LRp{\bx_{s,l}^{\btheta_0};\ \boldsymbol{0}}\cdot \bphi+\mathcal{O}(\epsilon^2)=0+\mathcal{O}(\epsilon^2),\]
where, $\nabla_{\btheta}\bff_{l+\frac{1}{2}}$ denotes the derivative of $\bff_{l+\frac{1}{2}}$ w.r.t the second argument. Note that one has $\nabla_{\btheta}\bff_{l+\frac{1}{2}}(\bx_{s,l};\ \boldsymbol{0})={\bf{0}}$ by condition \ref{two} of \cref{prop_admissible}. Therefore, we have:

\[  \delta \bx_{s,l+1}= \bff_{l+1}\LRp{\bx_{s,l+\frac{1}{2}}^{\btheta^0}+\mathcal{O}(\epsilon^2);\ \btheta_{l+1}^\epsilon}-\bff_{l+1}\LRp{\bx_{s,l+\frac{1}{2}}^{\btheta^0};\ \btheta_{l+1}^0}.\]
Now, considering the Taylor expansion about $\bx_{s,l+\frac{1}{2}}^{\btheta^0}$ and assuming that $\bff_{l+1}(.;.)$ has bounded second order derivative w.r.t the first argument, it is easy to see that by Taylor's theorem:
%there exists a function $h\LRp{\bx,\ \btheta}: \real^{n(l+\frac{1}{2})}\times {\Theta_{l+1}}\mapsto \real^{n(l+\frac{1}{2})\times n(l+\frac{1}{2})}$ such that:
\begin{equation}
    \begin{aligned}
       % \delta \bx_{s,l+1}&=\bff_{l+1,1}\LRp{\bx_{s,l+\frac{1}{2}}^{\btheta^0},\ \btheta_{l+1}^0}\cdot \mathcal{O}(\epsilon^2)+h\LRp{\bx_{s,l+\frac{1}{2}}^{\btheta^0}+\mathcal{O}(\epsilon^2),\ \btheta^0_{l+1}}\cdot \mathcal{O}(\epsilon^2)\\
       \delta \bx_{s,l+1}&=\nabla_{\bx}\bff_{l+1}\LRp{\bx_{s,l+\frac{1}{2}}^{\btheta^0};\ \btheta_{l+1}^0}\cdot \mathcal{O}(\epsilon^2)+ \mathcal{O}(\epsilon^4)\\
        &=\LRs{\mathcal{I}+\nabla_{\bx}\bg_{l+1}\LRp{\bx_{s,l+\frac{1}{2}}^{\btheta^0};\ \btheta_{l+1}^0}}\cdot \mathcal{O}(\epsilon^2)+\mathcal{O}(\epsilon^4),
       % &=\LRs{\mathcal{I}+\bg_{l+1,1}\LRp{\bx_{s,l+\frac{1}{2}}^{\btheta^0},\ \btheta_{l+1}^0}}\cdot \mathcal{O}(\epsilon^2)+h\LRp{\bx_{s,l+\frac{1}{2}}^{\btheta^0}+\mathcal{O}(\epsilon^2),\ \btheta^0_{l+1}}\cdot \mathcal{O}(\epsilon^2),
    \end{aligned}
\end{equation}
where we have used the fact that $\btheta_{l+1}^\epsilon=\btheta_{l+1}^0$ and used the ResNet propagation equation in \eqref{forr}. Therefore, we have:
\[ \delta \bx_{s,l+1}=\mathcal{O}(\epsilon^2), \]
Applying the above arguments recursively, one can show that:

\begin{equation}
    \delta \bx_{s,i}=\mathcal{O}(\epsilon^2),\quad \ i=\LRp{l+\frac{1}{2}},(l+1),\dots T.
    \label{taylor_x}
\end{equation}
Now from  \eqref{set_of} we have:
\[\bp_{s,T}^{\btheta^\epsilon}=-\frac{1}{S}\nabla \Phi\LRp{\bx_{s,T}^{\btheta^\epsilon}}, \quad \bp_{s,T}^{\btheta^0}=-\frac{1}{S}\nabla \Phi\LRp{\bx_{s,T}^{\btheta^0}},\]
for networks $\Omega_\epsilon$ and $\Omega_0$ respectively. Therefore,
\begin{equation}
    \begin{aligned}
     \delta \bp_{s,T}&= \bp_{s,T}^{\btheta^\epsilon}-\bp_{s,T}^{\btheta^0}\\
        &= \frac{1}{S}\LRs{\nabla \Phi\LRp{\bx_{s,T}^{\btheta^0}}-\nabla \Phi\LRp{\bx_{s,T}^{\btheta^\epsilon}}}=\frac{1}{S}\LRs{\nabla \Phi\LRp{\bx_{s,T}^{\btheta^0}}-\nabla\Phi\LRp{\bx_{s,T}^{\btheta^0}+\delta \bx_{s,T}}}\\
        &=\frac{1}{S}\LRs{\nabla \Phi\LRp{\bx_{s,T}^{\btheta^0}}-\nabla \Phi\LRp{\bx_{s,T}^{\btheta^0}+\mathcal{O}(\epsilon^2)}}.
    \end{aligned}
\end{equation}
Now assuming that $\Phi$ has bounded third order derivative,  Taylor series approximation about $\bx_{s,T}^{\btheta^0}$ gives:
\[ \delta \bp_{s,T}= \mathcal{O}(\epsilon^2). \]
Further, from  \eqref{set_of} we also have:
\begin{equation}
    \begin{aligned}
        \bp_{s,T-1}^{\btheta^\epsilon}&=\LRs{\nabla_{\bx}\bff_{T}\LRp{\bx_{s,T-1}^{\btheta^\epsilon};\btheta_{T}^\epsilon}}^T\bp_{s,T}^{\btheta^\epsilon}=\LRs{\nabla_{\bx}\bff_{T}\LRp{\bx_{s,T-1}^{\btheta^0}+\mathcal{O}(\epsilon^2);\ \btheta_{T}^0}}^T\LRp{\bp_{s,T}^{\btheta^0}+\mathcal{O}(\epsilon^2)},\\
        \bp_{s,T-1}^{\btheta^0}&=\LRs{\nabla_{\bx}\bff_{T}\LRp{\bx_{s,T-1}^{\btheta^0};\btheta_{T}^0}}^T\bp_{s,T}^{\btheta^0}.
    \end{aligned}
    \label{t_one}
\end{equation}
Again, under the assumption that $\bff_T(.;\ .)$ has bounded third order derivative  w.r.t the first argument, it is easy to show that:
\[ \delta  \bp_{s,T-1}= \bp_{s,T-1}^{\btheta^\epsilon}-\bp_{s,T-1}^{\btheta^0}=\mathcal{O}(\epsilon^2),\]
where we have considered the Taylor expansion of $\nabla_{\bx}\bff_{T}$ about $\bx_{s,T-1}^{\btheta^0}$. Now using the above arguments recursively, one can show that:
\begin{equation}
    \delta \bp_{s,i}=\mathcal{O}(\epsilon^2),\quad \ i=1,\dots T.
    \label{taylor_p}
\end{equation}
Therefore, from \eqref{taylor_x} and \eqref{taylor_p} we have:
\begin{equation}
    {\delta \bp_{s,t}\cdot \delta \bx_{s,t}}=\mathcal{O}(\epsilon^4),\quad \ t=\LRp{l+\frac{1}{2}},(l+1),\dots T.
    \label{five_t}
\end{equation}
\end{proof}

\hspace{-0.8 cm} {\bf{\underline{Analyzing Term II and III in \eqref{decomposition}}}}
%\tanbui{for equations that spill over the second line (even with the number), it is cleaner to use the multiline environment.}
\begin{equation}
    \begin{aligned}
       &  \sum_{t=l}^{T-1} \LRp{\bp_{s,t+1}^{\btheta^0}\cdot \delta \bx_{s,t+1}}+\sum_{t=0}^{T-1} \LRp{\delta \bp_{s,t+1}\cdot \bx_{s,t+1}^{\btheta_0}}=\\ 
       & \sum_{t=l+1}^{T-1} \LRp{\bp_{s,t}^{\btheta^0}\cdot \delta \bx_{s,t}}+\bp_{s,T}^{\btheta^0} \cdot \delta \bx_{s,T}+ \sum_{t=0}^{T-1} \LRp{\delta \bp_{s,t+1}\cdot \bx_{s,t+1}^{\btheta_0}}. 
    \end{aligned}
     \label{new_t}
\end{equation}
Now from \eqref{set_of} note that:
\[ \bp_{s,T}^{\btheta^0} \cdot \delta \bx_{s,T}=-\frac{1}{S}\nabla \Phi\LRp{\bx_{s,T}^{\btheta^0}}\cdot \delta \bx_{s,T}= -\frac{1}{S}\LRs{\Phi\LRp{\bx_{s,T}^{\btheta^\epsilon}}-\Phi\LRp{\bx_{s,T}^{\btheta^0}}}+\mathcal{O}\LRp{\norm{\delta \bx_{s,T}}_2^2}.\]
where we have assumed $\Phi$ has bounded second order derivative. Therefore,
\begin{equation}
\sum_{s=1}^S \bp_{s,T}^{\btheta^0} \cdot \delta \bx_{s,T}=\sJ(\Omega_0)-\sJ(\Omega_\epsilon)+\mathcal{O}(\epsilon^4).
\label{two_t}
\end{equation}
Further, the remaining terms in \eqref{new_t} can be written in terms of the Hamiltonian as, 
\begin{equation}
  \begin{aligned}
    &\sum_{t=l+1}^{T-1} \LRp{\bp_{s,t}^{\btheta^0}\cdot \delta \bx_{s,t}}+\sum_{t=0}^{T-1} \LRp{\delta \bp_{s,t+1}\cdot \bx_{s,t+1}^{\btheta_0}}=\\
   & \sum_{t=l+1}^{T-1}\nabla_{\bx} H_t\LRp{\bx_{s,t}^{\btheta^0}; \bp_{s,t+1}^{\btheta^0};\btheta_{t+1}^0}\cdot \delta \bx_{s,t}+\sum_{t=0}^{T-1} \nabla_{\bp} H_t\LRp{\bx_{s,t}^{\btheta^0}; \bp_{s,t+1}^{\btheta^0};\btheta_{t+1}^0}\cdot \delta \bp_{s,t+1}
\end{aligned}  
\label{three_t}
\end{equation}
Finally, substituting \eqref{two_t}, \eqref{three_t}, \eqref{five_t} in \eqref{decomposition} and summing over all the training data points we have:
\begin{equation}
   \begin{aligned}
   & \sJ(\Omega_0)-\sJ(\Omega_\epsilon) +\sum_{s=1}^S\sum_{t=l+1}^{T-1}\nabla_{\bx} H_t\LRp{\bx_{s,t}^{\btheta^0}; \bp_{s,t+1}^{\btheta^0};\btheta_{t+1}^0}\cdot \delta \bx_{s,t}\\
    &+\sum_{s=1}^S\sum_{t=0}^{T-1}\nabla_{\bp} H_t\LRp{\bx_{s,t}^{\btheta^0}; \bp_{s,t+1}^{\btheta^0};\btheta_{t+1}^0}\cdot \delta \bp_{s,t+1}+\sum_{s=1}^S\nabla_{\bx} H_{l+\frac{1}{2}}\LRp{\bx_{s,{l+\frac{1}{2}}}^{\btheta^0}, \bp_{s,l+1}^{\btheta^0},\btheta_{l+1}^0}\cdot \delta \bx_{s,l+\frac{1}{2}}+\mathcal{O}(\epsilon^4)\\
    &=\sum_{s=1}^S\sum_{t=0}^{T-1} \LRs{H_t\LRp{\bx_{s,t}^{\theta^0}+\delta \bx_{s,t};\ \bp_{s,t+1}^{\btheta^0}+\delta \bp_{s,t+1};\ \btheta_{t+1}^\epsilon}- H_t\LRp{\bx_{s,t}^{\theta^0};\ \bp_{s,t+1}^{\btheta^0};\ \btheta_{t+1}^0}}
\end{aligned} 
\label{final_one}
\end{equation}
The R.H.S of \eqref{final_one} can be simplified by considering the Taylor series expansion about $\LRp{\bx_{s,t}^{\btheta^0},\ \bp_{s,t}^{\btheta^0},\ \btheta_{t+1}^\epsilon}$as:
\begin{align*}
    & H_t\LRp{\bx_{s,t}^{\btheta^0}+\delta \bx_{s,t};\ \bp_{s,t+1}^{\btheta^0}+\delta \bp_{s,t+1};\ \btheta_{t+1}^\epsilon}- H_t\LRp{\bx_{s,t}^{\theta^0};\ \bp_{s,t+1}^{\btheta^0};\ \btheta_{t+1}^0}=\\
     & H_t\LRp{\bx_{s,t}^{\btheta^0};\ \bp_{s,t+1}^{\btheta^0};\ \btheta_{t+1}^\epsilon}-H_t\LRp{\bx_{s,t}^{\theta^0};\ \bp_{s,t+1}^{\btheta^0};\ \btheta_{t+1}^0}+\nabla_{\bx} H_t(\bx_{s,t}^{\btheta^0}; \bp_{s,t+1}^{\btheta^0};\btheta_{t+1}^\epsilon)\cdot \delta \bx_{s,t}+\\
     &\nabla_{\bp} H_t\LRp{\bx_{s,t}^{\btheta^0}; \bp_{s,t+1}^{\btheta^0};\btheta_{t+1}^\epsilon}\cdot \delta \bp_{s,t+1}+\mathcal{O}(\epsilon^4).
\end{align*}
Substituting the above in \eqref{final_one} and simplifying  (note that $\btheta_{t}^0=\btheta_{t}^\epsilon, \ \forall t\neq l+\frac{1}{2}$) we have:
\begin{equation}
    \begin{aligned}
        \sJ(\Omega_0)-\sJ(\Omega_\epsilon)&=\sum_{s=1}^S H_l\LRp{\bx_{s,l}^{\btheta^0};\ \bp_{s,l+\frac{1}{2}}^{\btheta^0};\ \epsilon \bphi}-H_l\LRp{\bx_{s,l}^{\theta^0};\ \bp_{s,l+\frac{1}{2}}^{\btheta^0};\ {\bf{0}}}+\mathcal{O}(\epsilon^4)\\
        &+ \LRs{\nabla_{\bx} H_{l}\LRp{\bx_{s,l}^{\btheta^0}, \bp_{s,l+\frac{1}{2}}^{\btheta^0},\epsilon \bphi}-\nabla_{\bx} H_{l}\LRp{\bx_{s,l}^{\btheta^0}, \bp_{s,l+\frac{1}{2}}^{\btheta^0},{\bf{0}}}}\cdot \delta \bx_{s,l}\\
       & +\LRs{\nabla_{\bp} H_{l}\LRp{\bx_{s,l}^{\btheta^0}, \bp_{s,l+\frac{1}{2}}^{\btheta^0},\epsilon \bphi}-\nabla_{\bp} H_{l}\LRp{\bx_{s,l}^{\btheta^0}, \bp_{s,l+\frac{1}{2}}^{\btheta^0},{\bf{0}}}}\cdot \delta \bp_{s,l}.
    \end{aligned}
    \label{tay}
\end{equation}
Now considering the Taylor expansion about $\epsilon=0$ for the last two terms in \eqref{tay} and noting that $ \nabla_{\bx\btheta}H_{l}\LRp{\bx_{s,l}^{\btheta^0}, \bp_{s,l+\frac{1}{2}}^{\btheta^0},{\bf{0}}}={\bf{0}}$ and $\nabla_{\bp\btheta}H_{l}\LRp{\bx_{s,l}^{\btheta^0}, \bp_{s,l+\frac{1}{2}}^{\btheta^0},{\bf{0}}}={\bf{0}}$ due to assumption \ref{two},
it follows that the last two terms in \eqref{tay} are of order  $\mathcal{O}(\epsilon^4)$. Therefore,  we finally have:
\[\sJ(\Omega_0)-\sJ(\Omega_\epsilon)=\sum_{s=1}^S H_l\LRp{\bx_{s,l}^{\btheta^0};\ \bp_{s,l}^{\btheta^0};\ \epsilon \bphi}-H_l\LRp{\bx_{s,l}^{\theta^0};\ \bp_{s,l}^{\btheta^0};\ {\bf{0}}}+\mathcal{O}(\epsilon^4). 
\]
where we have also used the fact $\bp_{s,l+\frac{1}{2}}^{\btheta_0}=\bp_{s,l}^{\btheta_0}$ since the added layer with zero weights and biases acts as a message-passing layer due to \cref{prop_admissible}. Finally, applying Taylor series expansion about $\epsilon=0$ and assuming that $\bff_t(.;\ .)$  has bounded third order derivative w.r.t second argument we have:
\begin{equation}
    \sJ(\Omega_0)-\sJ(\Omega_\epsilon)=\sum_{s=1}^S \LRp{\epsilon \nabla_{\btheta} H_l\Big |_{\boldsymbol{\theta}=0} \cdot \bphi+\frac{\epsilon^2}{2}\boldsymbol{\phi}^T \ \nabla^2_{\boldsymbol{\theta}} H_{l}\Big |_{\boldsymbol{\theta}=0}\ \boldsymbol{\phi}  +\mathcal{O}(\epsilon^3)}.
    \label{connect}
    \end{equation}
Note that $\nabla_{\btheta} H_l\Big |_{\boldsymbol{\theta}=0}=\bf{0}$  by condition \ref{two} of  \cref{prop_admissible}. Therefore, the topological derivative is computed as:
\[   d\sJ(\Omega_0;\ (l,\ \boldsymbol{\phi},\ \sigma))=-\lim_{\epsilon \downarrow 0}\frac{\sJ(\Omega_\epsilon)-\sJ(\Omega_0)}{\epsilon^2}=\frac{1}{2}\sum_{s=1}^S \boldsymbol{\phi}^T \ \nabla^2_{\boldsymbol{\theta}} H_{l}\Big |_{\boldsymbol{\theta}=0}\ \boldsymbol{\phi}.\]
thereby concluding the proof.

\begin{corollary}
Assume the conditions in  \cref{exist_th}. For the expansion \cref{eq:Taylor}  there hold:
    \begin{itemize}
        \item $\LRs{\frac{d}{d\epsilon}\LRp{\sJ\LRp{\Omega_\epsilon}}}_{\epsilon=0}= \nabla_{\btheta}\sJ\LRp{\Omega_\epsilon}\Big |_{\epsilon=0} \cdot \boldsymbol{\phi} = -\sum\limits_{s=1}^S\nabla_{\btheta} H_l\Big |_{\boldsymbol{\theta}=0} \cdot \bphi=        
        0$, 
        \item  $\frac{1}{2} \LRs{\frac{d^2}{d\epsilon^2}\LRp{\sJ\LRp{\Omega_\epsilon}}}_{\epsilon=0}        = 
        \frac{1}{2}\boldsymbol{\phi}^T\nabla^2_{\boldsymbol{\theta}}\sJ\LRp{\Omega_\epsilon}\Big|_{\epsilon=0}\boldsymbol{\phi}=-\frac{1}{2}\sum \limits_{s=1}^S \boldsymbol{\phi}^T \ \nabla^2_{\boldsymbol{\theta}} H_{l}\Big |_{\boldsymbol{\theta}=0}\ \boldsymbol{\phi}$, and        \item $d\sJ(\Omega_0;\ (l,\ \boldsymbol{\phi},\ \sigma))       = 
        -\frac{1}{2}\boldsymbol{\phi}^T\nabla^2_{\boldsymbol{\theta}}\sJ\LRp{\Omega_\epsilon}\Big|_{\epsilon=0}\boldsymbol{\phi}=\frac{1}{2}\sum \limits_{s=1}^S \boldsymbol{\phi}^T \ \nabla^2_{\boldsymbol{\theta}} H_{l}\Big |_{\boldsymbol{\theta}=0}\ \boldsymbol{\phi}$.
    \end{itemize}
    \label{coro:Hessian}
\end{corollary}
\begin{proof}
    Comparing equations \cref{connect} and \cref{eq:Taylor} and noting that $\sJ\LRp{\Omega_{\epsilon}\Big |_{\epsilon=0}}=\sJ\LRp{\Omega_0}$ concludes the proof.
    \end{proof}

% \begin{remark}
% In terms of Taylor expansion in \cref{eq:Taylor}, we see that the network derivative for our problem is nothing more than than 
% \end{remark}
As can be seen from \cref{coro:Hessian}, the directional derivative along the direction $\boldsymbol{\phi}$ of the loss function $\sJ$ at $\boldsymbol{0}$ vanishes. That is, when the newly added layer is a message passing layer, it does not affect the loss function, and this is consistent with \cref{prop_admissible}. Ignoring the factor $1/2$, we can also see that that network derivative is the negative of the Hessian of the loss function at $\boldsymbol{0}$ acting on $\boldsymbol{\phi}$ both on the left and the right. \cref{coro:Hessian} was also the reason we  introduced the generalized notion of network derivative in \cref{topo_der} that allows us to deploy the Hessian as the measure of sensitivity when the gradient vanishes.

\subsection{Derivation of our adaptation algorithm}
\label{deriv_algo}
%\krish{This section modified based on global perspective.}
We now exploit the derived closed-form expression for the network topological derivative \eqref{topo_de} to devise a greedy algorithm for architecture adaptation. Given the network derivative in \cref{topo_de}, the criterion is obvious: {\bf we add a new layer at the depth $l^*$ if the network derivative there is the largest} as that will incur the greatest decrease of the loss function according to \cref{asymptotic_exp}. In fact, \eqref{topo_de} helps us determine not only the best location $l^*$ to add a new layer but also an appropriate initialization for the training process when the new layer is added. %\subsubsection{Deriving our adaptation algorithm via a global perturbation viewpoint}
\begin{comment}
Consider a network $\Omega_0\in \sA$ in \cref{def_pert} with $(T-1)$ hidden layers and define a global perturbation  $\Omega_{\boldsymbol{\epsilon}}^g$ from $\Omega_0$  as:
\[ \Omega_{\boldsymbol{\epsilon}}^g={\Omega_0\oplus (1,\ \epsilon_1 {\bphi}_1, \ \sigma)\oplus (2,\ \epsilon_2 {\bphi}_2, \ \sigma)\dots \oplus (T-1,\ \epsilon_{(T-1)} {\bphi}_{(T-1)}, \ \sigma)}, \quad s.t.\ \ \Omega_{\boldsymbol{\epsilon}}^g \in \sA,\]
where we  added a new layer with parameters $\epsilon_i \bphi_i$ is added in front of each hidden layer of $\Omega_0$. %Let us adopt the notation  $\Omega_{\boldsymbol{\epsilon}}^g\LRp{\bx_s; \ \epsilon \bphi_1;\dots \epsilon \bphi_{(T-1)}}$ to denote the neural network function. 
The following two observations are in order:
\begin{enumerate}
    \item Assuming that the conditions in \cref{prop_admissible} are satisfied, one has:
    \[ d_2^{K}\LRp{\Omega_{\boldsymbol{\epsilon}}^g \Big |_{\boldsymbol{\epsilon}=\bf{0}};\ \Omega_0} =0,\quad \forall K\in \mathbb{N},\]
     where $d_2^K\LRp{\Omega_\epsilon;\Omega_0}$ is the distance function in \eqref{distance_metric}. That is all the added layer in $\Omega_{\boldsymbol{\epsilon}}^g$ is redundant and only acts as a ``message-passing layer". 
     \item From this global perspective, the Topological derivative \eqref{topo_de} can be reinterpreted as:
     \[ d\sJ(\Omega_0;\ (l,\ \boldsymbol{\phi},\ \sigma))= d\sJ\LRp{\Omega_\epsilon^g\Big |_{\boldsymbol{\epsilon} =0};\ (l,\ {\bphi_l},\ \sigma)}, \]
     where we consider activating the added layer at $l^{th}$ location along the depth with non-zero initialization $\bphi_l$ while maintaining zero weights/biases (i.e. $\bphi_i={\bf{0}},\ \forall i\neq l$) for  other added layers. 
\end{enumerate}
{\it{Based on the above global perspective, we seek to determine which added layer to activate (non-zero initialization)  along with the initialization $\bphi_l$ while maintaining zero weights/biases (i.e. $\bphi_i={\bf{0}},\ \forall i\neq l$) for other added layers. }}
\end{comment}
To begin, we note that the maximal network derivative at a location $l$ and the corresponding direction $\Phi_l$ (and hence the most appropriate initialization of the newly added layer, has it been added) are determined by
%The effect of adding a layer at the $l^{th}$ location is determined by computing the following quantities:
%In particular, we use \eqref{topo_de} to determine the best location (along the depth) to add a new layer along with its initialization for the training process.  To that end, let us define:
\begin{equation}
    \begin{aligned}
       & \Lambda_l= \max_{\bphi} \LRp{d\sJ(\Omega_0;\ (l,\ \bphi,\ \sigma))},\quad s.t. \norm{\bphi}_2^2=1,\\
       & \Phi_l=\underset{\bphi}{\operatorname{argmax}}  \LRp{d\sJ(\Omega_0;\ (l,\ \bphi,\ \sigma))},\quad s.t. \norm{\bphi}_2^2=1,
    \end{aligned}
    \label{quant}
\end{equation}
where we have considered initializing the new layer with unit direction $\bphi$ %. Normalizing $\boldsymbol{\phi}$ 
to avoid an arbitrary scaling in \eqref{quant}.  %The optimization problem \eqref{quant} therefore seeks to find a direction ${\bphi}_l$ along which the topological derivative varies the most for the insertion of a new layer between layer $l$ and layer $l+1$.  
{\it{ A new layer is added at the location $l^*=\argmax\limits_l \{\Lambda_l\}_{l=1}^{T-1}$.}}
%The optimization problem \eqref{quant} therefore seeks to find a direction $\boldsymbol{\phi}$ along which the topological derivative varies the most for the insertion of a new layer between layer $l$ and layer $l+1$.  
It is clear that $\Lambda_l$ is the maximum eigenvalue and $\Phi_l$ is the corresponding eigenvector to the following eigenvalue problem:
\begin{equation}
    Q_l\bphi=\lambda \bphi,\quad Q_l=\frac{1}{2}\sum_{s=1}^S \nabla^2_{\btheta} H_l\LRp{\bx_{s,l};\bp_{s,l};\Btheta}\Big |_{\btheta=0} = -\frac{1}{2}\nabla^2_{\boldsymbol{\theta}}\sJ\LRp{\Omega_\epsilon}\Big|_{\epsilon=0}.
    \label{eigen_matrixx}
\end{equation}

\begin{theorem}
    Consider adding a new layer with weights/biases $\epsilon{\bphi}$ at depth $l$. Suppose $\Lambda_l$ in \cref{quant} is positive,
    and the Hessian $\nabla^2_{\boldsymbol{\theta}}\sJ\LRp{\Omega_\epsilon}$ is locally Lipschitz at $\epsilon = {0}$ in the neighborhood of radius $\frac{2\Lambda_l}{L}$ with Lipschitz constant $L > 0$. If $0 < \epsilon < \frac{2\Lambda_l}{L}$, then initializing the newly added layer with $\epsilon \Phi_l$ decreases the loss function, i.e., $\sJ\LRp{\Omega_\epsilon} < \sJ\LRp{\Omega_0}$.
\end{theorem}
\begin{proof}
    Terminating the Taylor expansion \cref{eq:Taylor} at the second order term, we have
    \[
    \sJ(\Omega_\epsilon)=\sJ\LRp{\Omega_0}+ \frac{\epsilon^2}{2}{\Phi_l}^T\nabla^2_{\boldsymbol{\theta}}\sJ\LRp{\Omega_\gamma}{\Phi_l},
    \]
    for some $0 \le \gamma \le \epsilon$. It follows that
    \begin{multline*}
        \sJ(\Omega_\epsilon)=\sJ\LRp{\Omega_0} - \Lambda_l\epsilon^2  + \frac{\epsilon^2
        }{2}{\Phi_l}^T\LRs{\nabla^2_{\boldsymbol{\theta}}\sJ\LRp{\Omega_\gamma}-\nabla^2_{\boldsymbol{\theta}}\sJ\LRp{\Omega_0}}{\Phi_l} \\ \le 
        \sJ\LRp{\Omega_0} - \Lambda_l\epsilon^2  + \frac{\gamma L\epsilon^2
        }{2} <  \sJ\LRp{\Omega_0}.
    \end{multline*}
\end{proof}

\begin{remark}{}{}
   We have shown that if $\Lambda_l$, given by \eqref{quant}, is  greater than $0$, then adding a new layer at depth $l$  guarantees a decrease in the loss for sufficiently small $\epsilon$. The question is when  $\Lambda_l > 0$?  \cref{coro:Hessian} shows that $\Lambda_l$ is half of the negative of the smallest eigenvalue value of the  Hessian $\nabla^2_{\boldsymbol{\theta}}\sJ\LRp{\Omega_\epsilon}\Big|_{\epsilon=0}$. Thus, the smallest eigenvalue needs to be negative. In that case, initializing the newly added layer with $\epsilon \Phi_l$ corresponds to moving along a negative curvature direction, and thus decreasing the loss. If the smallest eigenvalue is non-negative, and thus $\Lambda_L$ is non-positive, we do not add a new layer at the depth $l$, as it would not improve the loss function.
   %Therefore, for successful deployment of our approach, it is necessary that the matrix $Q_l$ must be non-negative-definite for at least some $l$.
   %\footnote{Even though there could exist scenarios where $Q_l$ is negative definite $\forall l$, we have never encountered such a scenario in any of our numerical experiments.} %\tanbui{the bad scenario corresponding to negative definite Hamiltonian: could that happen? Under which condition the Hamiltonian is not negative definite, i.e. having negative curvature in any direction?}\krish{This could happen theoretically. If we look at \eqref{block_diagonal}, it really depends on the sign of $\bp_{s,l+1}$. If $\bp_{s,l+1}$ is negative (all components) for all $s$ and $\sigma''(0)>0$, then $Q_l$ is negative definite. If this scenario happens for all layers $l$ as well, then method fails.   }}
\end{remark}
\begin{remark}
Note that if the geometric multiplicity of $\Lambda_l$ (the maximum eigenvalue in \eqref{quant}) is greater than 1, then one may choose  $\Phi_l$ (eigenvector in \eqref{quant}) corresponding to any eigenvalue as initialization for the new layer since all these initialization gives the same topological derivative.
    %\tanbui{For the rest of the discussion in this section, the geometric multiplicity for each of the eigenvalues must be 1. Otherwise, picking all the basic vectors of the eigenspaces of the largest eigenvalues gives the largest topological derivative.}
\end{remark}

Our algorithm starts by training a small network for some $E_e$ epochs. Then, we compute the quantities in \eqref{quant}  for each layer $l$. At the $l^*$ location where the topological derivative is highest and positive, we insert a new layer there. That is, we add a new layer at the depth at which the loss function is most topologically sensitive. Our formulation \eqref{eigen_matrixx} also suggests that the initial value of the parameters of the newly added layer should be $\epsilon \Phi_l$. %\tanbui{Should we initialize with $ \Phi_l$ instead of $\epsilon \Phi_l$?} \krish{After solving the eigen value problem for $\Phi_l$, we will need to initialize with $\epsilon \Phi_l$, since that's how we define a perturbed network (Figure 1) and that leads to the expression for the derivative we have. Numerically we do a backtracking algorithm on $\epsilon$ to get maximum decrease in loss at each stage of adaptation. The details on backtracking is provided later in the paper.}
%A new layer is then inserted at the $l^{th}$ location with the highest topological derivative, i.e highest eigenvalue $\Lambda_l$ and initialized with parameter $\epsilon \Phi_l$. 
The new network $\Omega_\epsilon$ is trained again for $E_e$ epochs and the procedure is repeated until a suitable termination criteria is satisfied (see \cref{Algo_full}). The procedure in the case of a fully-connected network is provided in  \cref{Algo_full}. 

\subsection{Application to fully-connected neural network}

In this section, we derive the expression for the topological derivative $ d\sJ(\Omega_0;\ (l,\  \boldsymbol{\phi},\ \sigma))$ for a fully connected network. The Hamiltonian $H_l$ for the $l^{th}$  layer is written as (refer equation \eqref{hamiltonian}):
\begin{equation}
    H_l\LRp{\bx_{s,l};\bp_{s,l};\bW_{l+1};\bb_{l+1} }=\bp_{s,l}\cdot \LRs{\bx_{s,l}+\sigma(\bW_{l+1}^T \bx_{s,l}+\bb_{l+1})},
    \label{hamiltonian_full}
\end{equation}
where,  $\bW_{l+1}$ and $\bb_{l+1}$ denote the weights and biases. Further, without loss of generality let's assume $\bb_{l+1}={\bf{0}}$ %\tanbui{what if $\bb_{t+1} \ne {\bf{0}}$: in particular, does this change any discussion/conclusion in the following? If not, then say that without loss of generality, we assume that $\bb_{t+1}={\bf{0}}$} 
for the hidden layers and vectorized parameters (weights) as $\btheta_{l+1}=\LRs{\bW_{l+1}^{11},\dots \bW_{l+1}^{n1}, \bW_{l+1}^{12},\dots \bW_{l+1}^{n2},\ \bW_{l+1}^{1N_h},\dots \bW_{l+1}^{n n}}$. Then,  the matrix $Q_l$ in \eqref{eigen_matrixx} can be explicitly written as
\begin{equation}
    Q_l=\frac{1}{2}\begin{bmatrix}
   \sum_{s=1}^S\bp_{s,l}^{(1)}\bx_{s,l}\bx_{s,l}^T\sigma''(0) & {\bf{0}}&{\bf{0}}  & \dots \\ 
  {\bf{0}} &  \sum_{s=1}^S\bp_{s,l}^{(2)}\bx_{s,l}\bx_{s,l}^T\sigma''(0)  & {\bf{0}} & \dots \\ 
  \vdots &  &  \ddots &  \\ 
  {\bf{0}} & \dots &  {\bf{0}} &  \sum_{s=1}^S\bp_{s,l}^{(n)}\bx_{s,l}\bx_{s,l}^T\sigma''(0)  
 \end{bmatrix},
 \label{block_diagonal}
\end{equation}
where $\bp_{s,l+1}^{(r)}$ denotes the $r^{th}$ component of the vector. Therefore, in the case of a fully connected network, $Q_l$ turns out to be a block-diagonal matrix. This structure can be further exploited to devise a strategy to choose the best subset of neurons in the new layer to activate in  \cref{select_neuron}. 

%On the other hand, we can choose not to add a new layer, but to split the most sensitive neuron to grow the network in the width, and this is the subject of Section \ref{width_growing}.  The combination of both depth and width growing will be discussed in Section \ref{depth_width_growing}.

\subsubsection{Activating the most sensitive neurons in a new layer}
\label{select_neuron}
Let $Q_l^{(i)}$ be the $i^{th}$ block (along the diagonal) of $Q_l$ given by \eqref{block_diagonal}. Note that the eigenvalues of $Q_l$ are just the list of eigenvalues of each $Q_l^{(i)}$. \footnote{This follows from the fact that the characteristic polynomial of $Q_l$ is the product of the characteristic polynomials of all  $Q_l^{(i)}$ \cite{garcia2020block}} Therefore, one only needs to solve the eigenvalue problem for each block $Q_l^{(i)}$ separately thereby significantly reducing the computational cost. 
Let $Q_l^{(r)} := \sum_{s=1}^S\bp_{s,l}^{(r)}\bx_{s,l}\bx_{s,l}^T\sigma''(0) \in \real^{n \times n}$ be the $r$th diagonal block in $Q_l$ corresponding to the $r$th neuron. 
The following result is obvious.

\begin{proposition}
Let $\vb$ be a vector in  $\real^{n}$. Define
\begin{equation}
\Phi_r :=[\underbrace{{{0}},\ \dots }_{{(r-1)\times n.}}, \vb, \ {{0}} \dots {{0}}]^T.
\label{new_rep}
\end{equation}
Then $\vb$ is an eigenvector of $Q_l^{(r)}$ if and only if $\Phi_r$ is an eigenvector of $Q_l$. Furthermore, the eigenvalues corresponding to $\vb$ and $\Phi_r$ are the same.
\label{subBlockQl}
\end{proposition}


As a direct consequence of \cref{subBlockQl}, if $\vb$ is an eigenvector corresponding to the largest eigenvalue (among all eigenvalues of all diagonal blocks), then the corresponding $\Phi_r$ is associated with the largest eigenvalue of $Q_l$
.
% that has the largest eigenvalue (among other blocks) \tanbui{How do we know that there is no more one block containing the largest eigenvalue? In other words, how do we rule out the case in which there are two (or more) blocks having the same highest eigenvalue?}. Let the eigenvector associated with the largest eigenvalue of $Q_l^{(r)}$ \tanbui{How do we know that the eigenspace is one-dimensional: i.e. geometric multiplicity 1? do you mean ``an eigenvector"?} be 
% $\hat{\Phi}^{(r)}$ \tanbui{We do not need the subscript $l$ here to avoid notational cumbersome, as we are in the $l$ layer and we do not talk about other layers.}
% %$\hat{\Phi}_l^{(r)}$. 
% Define

% \[
% \Phi_l :=[\underbrace{{{0}},\ \dots }_{{(r-1)\times n.}}, \hat{\Phi}^{(r)}, \ {{0}} \dots {{0}}]^T
% \]
% then, the  eigenvector of $Q_l$ associated with the largest can be written in the form
% \tanbui{Needs a citation on this or a proof}. 
As a result, the shape functional is most sensitive to the $r$th neuron, and the $r$th neuron should be activated (i.e. initialized with non-zero weights/biases). Alternatively, given an integer $m$, we can identify ``the best" $m$ neurons such that when they are activated, the induced change in the shape functional is largest.


\begin{proposition}
\label{prop_fi}
    Consider the following constrained optimization problem:
\begin{equation}
%\begin{gather}
\label{quant_full_a}
 \max_{\bphi_i, \bphi_j \in \LRc{\Phi_k}_{k=1}^n,
 \bphi_i \ne \bphi_j}  \LRp{d\sJ(\Omega_0;\ (l,\  (\bphi_1+\dots \bphi_{m}),\ \sigma))}, \quad s.t \norm{\bphi_i}_2^2=1, \, i=1,\hdots,m. 
 %\ and,
% \label{quant_full_b}
%\text{pair $\bphi_i$, $\bphi_j$ is such that}  \ \ \bphi_i=[\underbrace{0,\dots }_{{(r-1)\times n.}}, \hat{\bphi}^{(r)}, \ 0 \dots],\ \bphi_j=[\underbrace{0,\dots }_{{(k-1)\times n.}}, \hat{\bphi}^{(k)}, \ 0 \dots] \ \ \text{with}\ \ r\neq k.
%\end{gather}
%\label{quant_full}
\end{equation}
Then, the optimal value is the sum of the first $m$ largest eigenvalues of $m$ diagonal sub-blocks of $Q_l$. The optimal solutions $\LRc{\bphi_i}_{i=1}^m$ are the corresponding eigenvectors of these diagonal sub-blocks written in the form given by \eqref{new_rep}.
\end{proposition}
\begin{proof}
    The first-order optimality condition can be obtained by first formulating the 
Lagrangian as:
\begin{equation}
\begin{aligned}
     L\LRp{\bphi_1;\dots, \bphi_m;\lambda_1;\dots \lambda_m}&=d\sJ(\Omega_0;\ (l,\  \varphi,\ \sigma))+\sum_{i=1}^{m}\lambda_i (1-\norm{\bphi_i}_2^2)\\
     &=\varphi^T Q_l \varphi+\sum_{i=1}^{m}\lambda_i (1-\norm{\bphi_i}_2^2),
\end{aligned} 
\label{lag}
\end{equation}
where $\varphi=\bphi_1+\dots \bphi_m$. Now notice that using the constraint $\bphi_i\neq \bphi_j$ in \eqref{quant_full_a} and $\bphi_i, \bphi_j \in \LRc{\Phi_k}_{k=1}^n$,  the Lagrangian can be rewritten in terms of the new variables $\{\hat{\bphi}_1,\dots \hat{\bphi}_m\}$ as:
%\tanbui{$\Phi_l$ conflicts with the optimal $\Phi_l$ above. We need to be straight with notations here: if you use capital $\Phi$ for the optimal solution as in \eqref{quant}, you need to stick with it. I have changed it to $\varphi$} 
%and we have ignored constraint \ref{quant_full_b}.  However, note that constraint \ref{quant_full_b} implies any $\Phi_l$ has the following representation:
 % \[\Phi_l=[\underbrace{\underbrace{0,\dots }_{{(r-1)\times n.}}, \hat{\bphi}^{(r)}, \ 0 \dots,}_{(k-1)\times n} \hat{\bphi}^{(k)}\dots ]. \]
%Therefore, incorporating  constraint \ref{quant_full_b}, \ref{lag} can be rewritten as:

%\tanbui{why suddenly switch to $\hat{}$ instead the original without hats?}\krish{$\bphi_i\in \real^{n^2}$ and $\hat{\bphi}_i\in \real^n$. This is the difference.}
\[ L\LRp{\hat{\bphi}_1;\dots; \hat{\bphi}_m;\lambda_1;\dots \lambda_m}=\sum_{i=1}^m \LRs{\hat{\bphi}_{i}}^T Q_l^{\iota(i)} \hat{\bphi}_{i} + \sum_{i=1}^{m}\lambda_i \LRp{ 1-\norm{\hat{\bphi}_{i}}_2^2},
\]
where $\iota\LRp{\cdot}: \LRc{1,\hdots,m} \ni i \mapsto \iota(i) \in \LRc{1,\hdots, n}$ is the mapping from the subscript of $\hat{\bphi}_i$ to the diagonal block number $\iota(i)$ in $Q_l$, and $\lambda_i$, $i=1,\hdots,m$ are the Lagrangian multiplier. Using the Lagrangian,  the first-order optimality condition of \eqref{quant_full_a} reads

\[ 
Q_l^{\iota(i)} \hat{\bphi}_{i} =\lambda_i  \hat{\bphi}_{i},\quad \forall i,
\]
that is, the pair $\LRc{\lambda_i ,\hat{\bphi}_{i}}$ is an eigenpair of the diagonal block $\iota(i)$ of $Q_l$. 

% As a result, the solution to the optimization problem \eqref{quant_full_a} is given as

% \begin{equation}
%     \Lambda_l= \sum_{i=1}^{m}\lambda_i, \quad  \Phi_l=\bphi_1+\dots \bphi_{m},
%     \label{compute_full}
% \end{equation}
% where, $\lambda_i$ denote the largest eigenvalue of  $Q_l^{(i)}$ and the corresponding eigenvector $\hat{\bphi}^{(i)}$  is used to construct $\bphi_i$ based on \ref{quant_full_b}. 

As a result, the optimal objective function in \eqref{quant_full_a} is the sum of the first $m$ largest eigenvalues of $m$ diagonal sub-blocks of $Q_l$.  The optimal solutions $\LRc{\bphi_i}_{i=1}^m$ are the corresponding eigenvectors of these diagonal sub-blocks written in the form \eqref{new_rep}. If we denote the $m$ largest eigenvalues (from highest to lowest) as $\{\lambda_i \}_{i=1}^m$, the solution is written as:
\begin{equation}
\Lambda_l=\sum_{i=1}^m \lambda_i, \quad \Phi_l=\bphi_i+\dots \bphi_m.
\label{compute_full}
\end{equation}

\end{proof}
Based on the above-computed quantities, our algorithm for adapting the depth of a fully connected feed-forward architecture is given in \cref{Algo_full}. 

\begin{remark}{Additional results:}
\label{additional_rem}
    \begin{enumerate}
        \item Note that our framework can also be applied to a convolutional neural network (CNN) architecture and the details are provided in \cref{CNN_application}.
        \item  We also consider an extension of \cref{Algo_full} called ``Fully automated network growing algorithm" to automatically select the parameter $m$ in \cref{Algo_full}, i.e automatically determine the number of neurons (width) of the added hidden layer. A detailed explanation of ``Fully automated network growing" is provided in \cref{automated}.
        \item In addition, in \cref{optimal_trans} we show that our topological derivative approach can also be viewed through the lens of an optimal transport problem. In particular, we show that our layer insertion strategy can be derived as a
solution to maximizing a topological derivative in $p-$Wasserstein space, where $p\geq 1$.
        \end{enumerate}
\end{remark}
%\textcolor{blue}{Here, there might be a way to innovate for width adaptivity, where $a_m$ is also a variable treated as an unknown variable as we have both width and depth adaptivity together. Let's look at the following optimization problem for instance:}


%\[ r_l^1=\max_{\phi_1}d_T\sJ(\Omega_0,\ l,\ \epsilon \bphi_1,\ \sigma),\quad s.t \norm{\bphi_1}_2=1.\]

%Choose $a_m^l$ such that $\forall i\in [2,\ a_m^l]$:

%\[ r_l^1-\underbrace{\max_{\phi_{i}} d_T\sJ(\Omega_0,\ l,\ \epsilon \bphi_{i},\ \sigma)}_{r_l^i}\leq \epsilon,\quad s.t \norm{\bphi_i}_2=1\]

%\[ \text{pair $\bphi_i$, $\bphi_j$ is such that}  \ \ \bphi_i=[\underbrace{0,\dots }_{{(r-1)\times n.}}, \hat{\bphi_r}, \ 0 \dots],\ \bphi_j=[\underbrace{0,\dots }_{{(k-1)\times n.}}, \hat{\bphi_k}, \ 0 \dots] \ \ \text{with}\ \ r\neq k\]

%Then store:


%\[  \Lambda_l=r_l^1+\dots r_l^{a_m^l}\]

%\[  \Phi_l=\bphi_1^*+\dots \bphi_{a_m^l}^*.\]



%\textcolor{blue}{Write what the above constraint means. }

 

\begin{algorithm} 
	\caption{Fully connected architecture adaptation algorithm for a regression task}
	\hspace*{\algorithmicindent} \textbf{Input}: Training data $\bX$, labels $\bC$, validation data $\bX_1$, validation labels $\bC_1$, number of neurons in each hidden layer $n$, loss function $\Phi$, number of neurons to activate in each hidden layer ${m}$, number of iterations $N_n$, parameter $\epsilon$, $\epsilon^t$, parameter $T_b$, hyperparameters and predefined scheduler for optimizer (\cref{hyper_parameter_n}).\\
	\hspace*{\algorithmicindent} \textbf{Initialize}:   Initialize  network $\sQ_1$ with $T_b$ hidden layers.\\
	\begin{algorithmic}[1] 
  \State Train network $\sQ_1$ and store the validation loss $(\epsilon_v)^{1}$.
  		\State set $i =1$,  $(\epsilon_v)^{0}>>(\epsilon_v)^{1}$, $\Lambda_l^m\geq \epsilon^t$
		\While{$i \le N_n$ \textbf{and} $\LRs{(\epsilon_v)^{i}\leq (\epsilon_v)^{i-1}}$ \textbf{and} $\Lambda_l^m\geq \epsilon^t$} 
  %\tanbui{or the largest topological derivative at any layer is less than some tolerance. This criteria ensures that adding a new layer at any location induces a negligible change in the loss function. That is the training loss saturates.}
		%\State Train network $\sQ_i$ for $E_e$ epochs using standard optimizers.
		%\State Compute the topological derivative for each layer $l$ as $\Lambda_l=\sum_{j=1}^{a_m}\lambda_j^l$, where $\{\lambda_1^l\dots \lambda_{a_m}^l \}$ represents the top 
 % \hspace*{\algorithmicindent}   $a_m$ eigen values of $\sQ_l$ given by \eqref{eigen_matrix}. 
 \State Compute the topological derivative for each layer $l$  using \eqref{compute_full} and store as $\{ \Lambda_l\}$, also store $\Lambda_l^m=\max_l\{ \Lambda_l\}$
   \State Store the corresponding eigenvectors for each layer as $\Phi_l$ given by \eqref{compute_full}.
        \State Obtain the new network $\sQ_{i+1}$ by adding a new layer at position $l^*=\argmax\limits_l \{\Lambda_l\}$ with initialization  $\epsilon\Phi_{l^*}$.
        \State Perform a backtracking line search to update $\epsilon$ as outlined in \cref{back_track}.
        \State Update the parameters for optimizer if required (refer  \cref{hyper_parameter_n} for scheduler details).
%        \State Perform a backtracking algorithm for finding the best $\epsilon\in [0,\ 1]$ that gives the maximum decrease in training 
      %    \hspace*{\algorithmicindent}  loss.
        \State Train network $\sQ_{i+1}$  and store the best validation loss $(\epsilon_v)^{i+1}$ and the best network $\sQ_{i+1}$.
		\State $i = i+1$
		\EndWhile
	\end{algorithmic} \label{Algo_full}
\hspace*{\algorithmicindent} \textbf{Output}: Network $\sQ_{i-1}$
\end{algorithm}






\section{Numerical experiments}
\label{experim}
In this section, we numerically demonstrate the proposed approach using different types of architecture such as a) Radial basis function neural network; b) Fully connected neural network; and c) vision transformer (ViT). Our supplementary file contains additional numerical examples where we considered both simulated and real-world data sets and also demonstrated the approach for a convolutional neural network architecture (CNN). General experimental settings for all the problems and descriptions of methods adopted for
comparison are detailed in \cref{hyper_parameter}. Note that in our numerical results, Proposed (I) refers to \cref{Algo_full} and proposed (II) refers to ``fully automated growing" algorithm mentioned in \cref{additional_rem}.


\subsection{Proof of concept example: Radial basis function (RBF) neural network}

As a proof of concept of our proposed approach, we first consider the problem of learning a 1-dimensional function using a radial basis function (RBF) neural network. In particular, we consider the multi-layered RBF neural network \cite{craddock1996multi,bodyanskiy2020multilayer} with residual connections where $\bg_{t+1}(.;\ .)$ in \eqref{res_o} is given by:
\begin{equation}
   \bg_{i+1}(x_{s,i};\ \btheta_{i+1})=\btheta_{{i+1}}^{(3)}\times \exp\LRp{-\frac{1}{2}\LRp{\btheta_{{i+1}}^{(1)}x_{s,i}+\btheta_{{i+1}}^{(2)}-c}^2}-\btheta_{{i+1}}^{(3)}\times \exp\LRp{-\frac{1}{2}\LRp{c}^2},
    \label{resnet_RBF}
\end{equation}
where  $\btheta_i=\LRs{\theta_{i}^{(1)},\ \theta_{i}^{(2)},\ \theta_{i}^{(3)}}^T\in \real^3$ for $i=0,\dots T-1$, and $c$ is a non-zero parameter of the activation function.  Note that, the second term in \eqref{resnet_RBF} is introduced to satisfy condition \ref{two} of   \cref{prop_admissible} and therefore in this case we have a modified radial basis function. Further, note that \eqref{resnet_RBF} corresponds to using a single neuron in each hidden layer. 

%\tanbui{Krish: it does not make much sense to write $\frac{1}{2}(c)^2$. Also, note that we have the function $\exp$ in latex, which looks much nicer. Again, you need to explain all new notations you introduce to avoid confusing the readers: not sure what $c$ is, is it the same $c$ in the optimal transport section?}




\subsubsection{Topological derivative for the modified RBF network \eqref{resnet_RBF}}

The topological derivative is computed based on solving the eigenvalue problem \eqref{eigen_matrixx} where the matrix $Q_l$ (equation \eqref{eigen_matrixx}) is given as:
\begin{equation}
Q_l= \frac{1}{2}\sum_{s=1}^S   \LRp{\begin{bmatrix}
  0  & 0&  c_1 \ p_{s,l}\ x_{s,l}  \\ 
  0 & 0 &     c_1 \ p_{s,l}\\ 
   c_1 \ p_{s,l}\ x_{s,l} &   c_1 \ p_{s,l} &      0
 \end{bmatrix}},
\end{equation}
where $c_1=c\ \exp\LRp{-\frac{1}{2}\LRp{c}^2}$ and $c$ is a parameter in \eqref{resnet_RBF}.
\subsubsection{Data generation and numerical results}

For generating the training data set, we set $T=15$ in \eqref{res_o} and sample a true set of parameters $\{\btheta_i\}_{i=1}^T$ from 
normal distribution $\sN\LRp{{\bf{0}},3 {\bf{I}}}$, where ${\bf{I}}$ is the $3\times 3$ identity matrix.
%\tanbui{forgot if you have introduced the notation for normal distribution $\sN$ anywhere yet. Also if $\btheta_i$ is a vector, then the normal distribution must have a covariance matrix.}
Further, we set $c=0.1$ in \eqref{resnet_RBF}.  The training data set $D=\{x_i,\ c_i\}_{i=1}^{5000}$ is then generated by drawing $x_i$ uniformly from $[-2,\ 2]$ and computing the corresponding labels as  $c_i=\Omega^*(x_i)$, where $\Omega^*$ denote the true map to be learnt. In addition, we consider an additional $500$ data points for generating the validation data set and $1000$ data points for the testing data set. The generated true function is shown in \cref{learnt_curve_for_different_cases} (rightmost figure).
\begin{figure}[h!]      

\hspace{-0.7 cm}  
  \begin{tabular}{c}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.38]{Fig/theory_vs_numerical.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

\hspace{-1 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.38]{Fig/angle_variation.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

\hspace{-1 cm}

 \begin{tabular}{c}
        %  \includegraphics[scale=0.5]{Figures/RBF/learnt_curves_new.pdf}
           \includegraphics[scale=0.38]{Fig/learnt_curves_new.pdf}          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}
  
  \end{tabular}
  % \end{subfigure} \\
    \caption{Validating \cref{exist_th}. {\bf Left subfigure}: comparison of the theoretically computed topological derivative (equation  \eqref{topo_de}) with the numerically computed derivative for layer `l' with the largest eigenvalue and initialization  $\Phi_l$ given by \eqref{quant}; {\bf Middle subfigure}: effect of initialization $\bphi$ on the numerically computed derivative $ d\sJ(\Omega_0;\ (l,\  \boldsymbol{\phi},\ \sigma))$ in \eqref{topo_der} for $l=1$ and at the end of $1^{st}$ iteration; {\bf Right subfigure}: learned function using the proposed approach at different iterations of the algorithm. %\tanbui{the caption is really ambiguous: please explicitly say what you want to show for the left figure and the right ones. I am clueless on the right subfigure, especially all these blue circles.}
    }
  \label{learnt_curve_for_different_cases}
\end{figure} 

\subsubsection{Discussion of results}
As outlined in  \cref{deriv_algo}, we begin the adaptive training process by starting with a one-hidden layer network and progressively adding new layers. Each layer is trained to a (approximately) local minimum before adding a new layer. To validate our proposed theory, we compare the topological derivative predicted by our  \cref{exist_th}  with that computed numerically at each iteration of the algorithm and the results are shown in   \cref{learnt_curve_for_different_cases} (left). %\tanbui{which one? when you have subfigures, in your discussion you need to point out which subfigure you are referring to. You cannot let the readers solve an inverse problem to figure out which one you are talking about.} 
Note that the derivatives are computed for the layer $l$ with the largest eigenvalue at each iteration and initialization $\bphi=\Phi_l$ given by \eqref{quant}. We use a step size of $\epsilon=1\times 10^{-4}$ to compute  the numerical derivative $ d\sJ(\Omega_0;\ (l,\  \boldsymbol{\phi},\ \sigma))$ in \eqref{topo_der}. \cref{learnt_curve_for_different_cases} (left) shows that the numerical derivative is in close agreement with the theoretical derivative.  \cref{learnt_curve_for_different_cases} (middle) investigates how the choice of initialization $\bphi$ influences the derivative $ d\sJ(\Omega_0;\ (l,\ \boldsymbol{\phi},\ \sigma))$ in \eqref{topo_der}. For this, we set $l=1$ (at the end of training the initial one hidden layer network)  and generated different unit vectors (each sampled vector is represented as a blue circle in \cref{learnt_curve_for_different_cases} (middle)). The x-axis denotes the angle that each sampled vector makes with the optimal direction $\Phi_l$ given by \eqref{quant} and the y-axis denotes the numerically computed derivative for each sampled $\bphi$.  It is clear from \cref{learnt_curve_for_different_cases} (middle) that the maximum 
topological derivative is indeed observed for the optimal eigenvector $\Phi_l$ predicted by \eqref{quant}. \cref{learnt_curve_for_different_cases} (rightmost figure) shows the learned function (by our proposed approach) at each iteration of the algorithm.

\begin{figure}[h!]      
  \centering
  \begin{tabular}{c}
\begin{tabular}{c}
       %   \includegraphics[scale=0.5]{Figures/RBF/training_curves_new.pdf}
          \includegraphics[scale=0.4]{Fig/training_curves_new.pdf}          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}
 \end{tabular}


      \begin{tabular}{|c | c | c | c|}
        \hline
    Method &  Test loss  ($\mu \pm \sigma$)  \\ \hline
          {\bf{  Proposed }} & $0.051 \pm 0.027$\\ \hline
   Random layer insertion (I)   & $0.099 \pm 0.0514$ \\ \hline
   Net2DeeperNet (II) \cite{chen2015net2net}  & $0.069 \pm 0.0335$  \\ \hline
     Baseline  network  & $0.0897 \pm 0.038$ \\ \hline
Forward Thinking \cite{hettinger2017forward} &  $0.23 \pm 0.150$   \\ \hline
	\end{tabular} 
 \end{tabular}
  % \end{subfigure} \\
    \caption{{\bf Left subfigure}:  typical training loss curves for different approaches. {\bf Right subfigure}: summary of results.
    %\tanbui{The curves in the left figure can be more visible with thicker lines.}
    }
  \label{training}
\end{figure} 
Typical training curves for different adaptation strategies are shown in  \cref{training} (left). The sharp dip in the loss in  \cref{training} (left) corresponds to a decrease in loss on adding a new layer.  It is clear from  \cref{training} (left)
%\tanbui{which one? when you have subfigures, in your discussion you need to point out which subfigure you are referring to. You cannot let the readers solve an inverse problem to figure out which one you are talking about.} 
that our proposed framework outperforms all other adaptation strategies while also exhibiting superior performance in comparison to a baseline network trained from scratch. It is also interesting to note that while Net2DeeperNet (II) seems to exhibit superior performance in the beginning stages in \cref{training} (left), it is clear that there is no guarantee that  Net2DeeperNet (II)  will escape saddle points as evident from the later stages of adaptation. In contrast, our layer initialization strategy ensures a decrease in loss for sufficiently small $\epsilon$  thereby escaping saddle points.  The decision-making process at each iteration on where to add a new layer is shown in  \cref{rel_top_po_rbf} where the blocks represent the relative magnitude of the derivative for different layers. Note that a new layer is inserted at the location with maximum derivative.
\begin{comment}
\begin{figure}[h!]      
    \centering
  \begin{tabular}{c}
  
      \begin{tabular}{c}

          \hspace{0.1 cm}
            \includegraphics[scale=0.45]{Figures/RBF/adaptation_summary_new.pdf}
        
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}
 \begin{tabular}{c}

          \hspace{0.8 cm}
           \includegraphics[scale=0.45]{Figures/RBF/Adaptation_results.pdf}
         
      \end{tabular}


  
  \end{tabular}
  % \end{subfigure} \\
    \caption{ Left to Right: Summary of the adaptation results: Bar chart showing the test loss achieved at each iteration of the algorithm; Relative magnitude of the topological derivative for different hidden layers during the adaptation procedure (a new layer is inserted at the position of maximum topological derivative).\textcolor{blue}{ Draw in MATLAB.} }
  \label{adaptation_results_sum}
\end{figure} 
\end{comment}
\begin{figure}[h!]      
\centering
  
  \begin{tabular}{c}
\hspace{-1 cm}
      \begin{tabular}{c}

          \centering
            \includegraphics[scale=0.31,trim={0 0 0 5cm},clip]{Fig/iter_1_rbf.pdf}
        
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

\hspace{-1.5 cm}


      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.31,trim={0cm 0 0 5cm},clip]{Fig/iter_2_rbf.pdf}
      \end{tabular}
\hspace{-1.5 cm}

 \begin{tabular}{c}

          \centering
            \includegraphics[scale=0.31,trim={0 0 0 5cm},clip]{Fig/iter_3_rbf.pdf}
        
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}
\hspace{-1.5 cm}
         \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.31,trim={0 0 0 5cm},clip]{Fig/iter_4_rbf.pdf}
      \end{tabular}

  
  \end{tabular}
  
   
  % \end{subfigure} \\
    \caption{Relative magnitude of the topological derivative for different hidden layers during the adaptation procedure. A new layer is inserted at the position of the maximal topological derivative (yellow block), and the red dots denote the location of layers.
    %\tanbui{Not the best explanation: what are the red dots? You also need to tell what each block means.}
    }
    \label{rel_top_po_rbf}
\end{figure} 
Further, the effectiveness of the approach is quantified by performing an uncertainty analysis where the algorithm is run considering different random initializations for the initial single hidden layer network and the results are tabulated in  \cref{training} (right).  \cref{training} (right) shows that on average our proposed method outperformed all other adaptation strategies while also exhibiting lower uncertainty. Note that in contrast to other approaches, the only source of uncertainty in our approach is the random initialization of initial network $\sQ_1$ in  \cref{Algo_full}. Further, the results in  \cref{training} (right) can be further improved (lower testing loss) if one considers an RBF network with multiple neurons in each hidden layer. It is also important to note that in practice it is not essential to train the network to a local minima at each iteration of the algorithm as demonstrated in \cref{training} (left) (note that the topological derivative in Theorem \eqref{exist_th} is valid even when the current network $\Omega_0$ is not in a local minima).  This is a significant advantage over some other methods that require the training loss to plateau before growing the network \cite{wu2019splitting,kilcher2018escaping}.
One may consider more efficient training approaches where the network is trained for a fixed user-defined number of epochs (scheduler) at each iteration as suggested in \cite{evci2022gradmax}. Our \cref{Algo_full} considers the use of a predefined scheduler to decide when a new layer needs to be added during the training phase, whereas  \cref{Algo_full_auto}
employs the use of a validation metric to automatically detect when a new layer needs to be added. 

In the following sections, we consider experiments with more complex datasets and assess the performance of both  \cref{Algo_full} and \cref{Algo_full_auto}.

\subsection{Experiments with fully connected neural network}

In this section, we consider experiments with fully connected neural networks (see  \cref{Algo_full}). 



\subsubsection{Learning the observable to parameter map for 2D heat equation}
\label{poisson_sec}

In this section we consider solving the 2D heat conductivity inversion problem \cite{nguyen2024tnet}. 
The heat equation we consider is the following:
\begin{equation}
\begin{aligned}
     -\nabla \cdot \LRp{e^u \nabla y} & = 20  \quad \text{in } \Omega = \LRp{0,1}^2,\\
    y & = 0 \quad \text{ on } \Gamma^{\text{ext}}, \\
    \textbf{n} \cdot \LRp{e^u \nabla y} & = 0 \quad \text{ on } \Gamma^{\text{root}},
\end{aligned}
\label{heat_equation_o}
\end{equation}
where the conductivity $u$ is the parameter of interest (PoI), $y$ is the temperature field, and $\textbf{n}$ is the unit outward normal vector on Neumann boundary part $\Gamma^{\text{root}}$. The objective is to infer the parameter field $u(\bx)$ given the observation field $y(\bx)$. We construct the input vector for the neural network as $\by=[y(\bx_1),\ \dots y(\bx_{n_0})]$, where $\bx_i$ are fixed locations on $\Omega$. The domain $\Omega$ along with the fixed locations is provided in \cref{mesh_details}. For the present experiment, we set the input dimension $n_0=10$.

\vspace{0.1 cm}

\hspace{-0.6 cm}{\bf{\underline{Data generation and numerical results}}}

\vspace{0.1 cm}


Note that in order to generate the observation vector $\by$, one needs to assume a parameter field $u(\bx)$ and solve \eqref{heat_equation_o}. The  parameter field $u(\bx)$ is generated as follows:
\begin{equation}
   u(\mb{x}) = \sum_{i =1 }^{n_T} \sqrt{\lambda_i}\  \mb{\phi}_i(\bx)\  c_i,\quad \bx \in [0,\ 1]^2,
   \label{twopo}
\end{equation}
where $\LRp{\lambda_i, \ \mb{\phi}_i}$ is the eigenpair of an exponential two-point correlation function from \cite{constantine2016accelerating} and $\bc = \LRp{c_1,\hdots, c_{n_T}}\sim \sN({\bf{0}}, {\bf{I}})$ is a standard Gaussian random vector. Note that we consider $\bc$ as the network output with dimension $n_T = 12$. In addition, $5 \%$ additive Gaussian noise is added to the observations $\by$ to represent the actual field condition. Therefore, the training data points can be denoted as $ \{ \by_i,\ \bc_i\}_{i=1}^S$. Given a new observation data $\by$, the network outputs the vector $\bc$ which can then be used to reconstruct $u(\bx)$ using \eqref{twopo}.  To quantify the performance of our algorithm, we compute the average
relative errors on the test dataset  as follows:

\[ \text{Err}=\frac{1}{M}\sum_{i=1}^M \frac{\norm{\bu^{pred}_i-\bu^{true}_i}^2}{\norm{\bu^{true}_i}^2},\]
where $M$ denotes the number of test data samples, $\bu^{pred}_i$ denotes the neural network prediction for the parameter field $u(\bx)$ for the $i^{th}$ sample and $\bu^{true}_i$ denotes the synthetic ground truth
parameters. Here, $\bu$ is a vector containing the solutions on a $16\times 16$ grid shown in \cref{mesh_details} (right).


\begin{figure}[h!]      
  % \begin{subfigure}[b]{\textwidth}
\centering
  \begin{tabular}{c}


      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/Poisson/proposed_0.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

  % \hspace{0.5 cm}

    %  \begin{tabular}{c}

      %    \centering
      %    \includegraphics[scale=0.3]{Figures/Poisson/proposed_1.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

     % \end{tabular}

 
 \hspace{-0.35 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/Poisson/proposed_3.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

 \hspace{-0.35 cm}
 
\begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/Poisson/proposed_4.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

 \hspace{-0.35 cm}
 
       \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/Poisson/proposed_6.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

 \hspace{-0.35 cm}
 
       \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/Poisson/solution_true.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}
      
  \end{tabular}\\



  \caption{Evolution of parameter field $u(\bx)$ for a particular test observation upon adding new layers for $S=1000$ (Left to Right): inverse solution after the $1^{st}$ iteration;  inverse solution after the $3^{rd}$ iteration; inverse solution after the $5^{th}$ iteration;  inverse solution after the $7^{th}$ iteration; and the groundtruth parameter distribution.}
\label{param_evolution}
\end{figure}     

Further, we consider experiments with training data set of size $S=1000$ and $S=1500$. We consider an additional $200$ data points for generating the validation data set and $500$ data points for the testing data set. Other details on the hyperparameter settings are provided in \cref{Input_values_different_problems}. Note that for each adaptation approach, the best network (in terms of least average relative error on the validation dataset) out of $100$ random initialization is retained for comparison purposes (such as for generating  \cref{param_evolution} and \cref{comparis_po}). 

 


 \cref{param_evolution} shows the parameter field (for a particular observational data input) predicted by our proposed approach at different iterations of our algorithm. It is interesting to see that while the initial networks with a smaller number of parameters learn the low-frequency components of the solution, later iterations of the algorithm capture the more complex features in the solution (as more parameters are added). In other words, our algorithm identifies missing high-level (high-frequency) features and inserts new layers to capture them.  Further, the final parameter field is visibly close to the ground truth solution as evident from  \cref{param_evolution}.
%Figure \ref{sum_po} shows the average relative error achieved on the test dataset at the end of each iteration.
\begin{comment}
    
\begin{figure}[h!]      
\centering
  
  \begin{tabular}{c}

      \begin{tabular}{c}

          \centering
      \includegraphics[scale=0.4]{Figures/Poisson/adaptation_summary_poisson.pdf}      

          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}



      \begin{tabular}{c}

          \centering
        \includegraphics[scale=0.4]{Figures/Poisson/adaptation_summary_poisson.pdf}      
      \end{tabular}
  \end{tabular}
  % \end{subfigure} \\
        \caption{ Summary of the adaptation results ($S=1000$). (Left to right): Bar chart showing the average relative error on training dataset achieved at the end of each iteration;  Bar chart showing the average relative error on test dataset achieved at the end of each iteration.}
  \label{sum_po}
\end{figure} 
\end{comment}
\begin{comment}
\begin{figure}[h!]      
\centering
  
  \begin{tabular}{c}
  \hspace{-1.0 cm}
      \begin{tabular}{c}

          \centering
            \includegraphics[scale=0.41,trim={0 0 0 5cm},clip]{Fig/iter_1_pois.pdf}
        
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

\hspace{-1.7 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.41,trim={0cm 0 0 5cm},clip]{Fig/iter_2_pois.pdf}
      \end{tabular}

\hspace{-1.7 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.41,trim={0cm 0 0 5cm},clip]{Fig/iter_3_pois.pdf}
      \end{tabular}

  
  \end{tabular}
  \\
  
   \begin{tabular}{c}
 \hspace{-1.0 cm}
      \begin{tabular}{c}

          \centering
            \includegraphics[scale=0.41,trim={0 0 0 5cm},clip]{Fig/iter_4_pois.pdf}
        
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

\hspace{-1.7 cm}
      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.41,trim={0 0 0 5cm},clip]{Fig/iter_5_pois.pdf}
      \end{tabular}

\hspace{-1.7 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.41,trim={0 0 0 5cm},clip]{Fig/iter_6_pois.pdf}
      \end{tabular}
  
  \end{tabular}
  % \end{subfigure} \\
    \caption{Relative magnitude of the topological derivative for hidden layers at different iterations for $S=1000$. A new layer is inserted at the position of maximal  derivative (yellow block), and the red dots denote the location of layers.}
    \label{rel_top_po}
\end{figure} 
\end{comment}
%Further, the relative magnitude of  topological
%derivative across the hidden layers at each iteration is also shown in  \cref{rel_top_po}. Note that a new layer is inserted at the position of maximum topological derivative.
Further, in an attempt to estimate the accuracy of each adaptation strategy (for a graphical representation of the error by each approach), we define the pointwise average relative error as:
\begin{equation}
    \text{Err}_j=\frac{1}{M}\sum_{i=1}^M \frac{\LRp{\bu^{pred}_{i,j}-\bu^{true}_{i,j}}^2}{\norm{\bu^{true}_i}^2/|\bu_i|},
    \label{error_metric}
\end{equation}
where subscript $j$ denotes the $j^{th}$ component of $\bu_i$ and $|\bu_i|$ denotes the number of elements in the vector. \cref{comparis_po} shows the error $\text{Err}_j$ plotted for  the main adaptation strategies. It is quite clear from   \cref{comparis_po} that our approach outperforms all other adaptation strategies by a good margin. 



\begin{comment}
\begin{table}[h!]
\caption{Statistics of the average relative error when using training data set of size $S=1000$}
\centering
\begin{tabular}{||c c c  c c c||} 
 \hline
&{\bf{Proposed}}  & Random layer insertion (I)  & Random layer insertion (II)  &   Baseline & Hettinger et al.\\ [0.5ex] 
 \hline \hline
 Mean&{\bf{ $0.439 $}}& $0.456$  & $0.456$ & $0.50$ & $0.66 $\\ 
 Std&{\bf{ $0.030 $}}& $0.030$  & $0.027$ & $0.028$ & $0.032 $\\ 
{\bf{Min}}&{\bf{ 0.366 }}& {\bf{0.392}}  & {\bf{ 0.40}} & {\bf{0.446}} & {\bf{0.57 }}\\ 
Max&{\bf{ $0.524 $}}& $0.535$  & $0.51$ & $0.584$ & $0.738 $\\ 
 \hline
\end{tabular}
\end{table}

\begin{table}[h!]
\caption{Statistics of the average relative error when using training data set of size $S=2000$ \textcolor{blue}{Revise this table }}
\centering
\begin{tabular}{||c c c  c c c||} 
 \hline
&{\bf{Proposed}}  & Random layer insertion (I)  & Random layer insertion (II)  &   Baseline & Hettinger et al.\\ [0.5ex] 
 \hline \hline
 Mean&{\bf{ $0.439 $}}& $0.456$  & $0.456$ & $0.50$ & $0.66 $\\ 
 Std&{\bf{ $0.030 $}}& $0.030$  & $0.027$ & $0.028$ & $0.032 $\\ 
{\bf{Min}}&{\bf{ 0.366 }}& {\bf{0.392}}  & {\bf{ 0.40}} & {\bf{0.446}} & {\bf{0.57 }}\\ 
Max&{\bf{ $0.524 $}}& $0.535$  & $0.51$ & $0.584$ & $0.738 $\\ 
 \hline
\end{tabular}
\end{table}
\end{comment}

\begin{comment}
\begin{table}[h!]
\caption{Statistics of the average relative error for different methods (2D heat equation)}
\centering
\begin{tabular}{|c | c | c | c|c|}
        \hline
    Method &  Avg. relative error ($\mu \pm \sigma$) & Avg. relative error ($\mu \pm \sigma$) & $\%$ improvement of \\ 
     &  ($S=1000$) & ($S=1500$) & proposed over others \\ \hline
          {\bf{  Proposed (I)}} & $0.444 \pm 0.031$ 
&$0.439 \pm 0.025$& $S=1000 \ \ S=1500$ \\ \hline
{\bf{  Proposed (II)}}   & $0.446 \pm 0.049$ & $0.434 \pm 0.029$& 0.67$\%$   \vline \ \ 1.78 $\%$\\ \hline
   Random layer insertion (I)   & $0.457 \pm 0.031$ & $0.447 \pm 0.029$& 2.84 $\%$   \vline \ \ 1.78 $\%$\\ \hline
   Net2DeeperNet (II) \cite{chen2015net2net}  & $0.456 \pm 0.027$ & $0.451 \pm 0.022$& 2.63$\%$   \vline \ \ 2.66 $\%$  \\ \hline
     Baseline  network  & $0.50 \pm 0.028$ & $0.489 \pm 0.029$&  11.2 $\%$   \vline \ \ 10.2 $\%$\\ \hline
Forward Thinking \cite{hettinger2017forward} &  $0.66 \pm 0.033$ &   $0.65 \pm 0.034$&  32.7 $\%$   \vline \ \ 32.4 $\%$\\ \hline
	\end{tabular} 
 \label{stat_po}
\end{table}
\end{comment}

\begin{table}[h!]
\caption{Statistics ($\mu \pm \sigma$) of the relative error (rel. error) for different methods (2D heat equation)}
\centering
\begin{tabular}{|c | c | c | c|c|}
        \hline
    Method &  rel. error  & rel. error & Best relative error\\ 
     &  ($S=1000$) & ($S=1500$) & $S=1000 \ \vline  \ S=1500$\\ \hline
     {\bf{  Proposed (II)}}   & ${\bf{0.400}}  \pm 0.032$ & ${\bf{0.391}} \pm 0.033$& ${\bf{0.327}}$   \vline \ ${\bf{0.321}}$  \\ \hline
          {\bf{  Proposed (I)}} & $0.434 \pm 0.022$ 
&$0.429 \pm 0.023$& 0.351  \vline \  0.364 \\ \hline
   Random layer insertion (I)   & $0.457 \pm 0.031$ & $0.447 \pm 0.029$& 0.392   \vline \  0.360 \\ \hline
   Net2DeeperNet (II) \cite{chen2015net2net}  & $0.456 \pm 0.027$ & $0.451 \pm 0.022$& 0.400   \vline \ 0.362   \\ \hline
     Baseline  network  & $0.50 \pm 0.028$ & $0.489 \pm 0.029$&  0.446   \vline \  0.420 \\ \hline
Forward Thinking \cite{hettinger2017forward} &  $0.66 \pm 0.033$ &   $0.65 \pm 0.034$&  0.570   \vline \  0.555 \\ \hline
NAS \cite{li2020system,li2020random}&  $-- \pm --$ &   $-- \pm --$&  0.349   \vline \  0.344 \\ \hline
	\end{tabular} 
 \label{stat_po}
\end{table}

\begin{comment}
\begin{table}[h!]
\caption{Best relative error achieved by different methods (2D heat equation)}
\centering
\begin{tabular}{|c | c | c | c|c|c|c|c|}
        \hline
&   {\bf{  P(I)}} &  {\bf{  P(II)}} & R(I) & R(II) & (B) & (H) & (S)\\ \hline
Best relative error (S=1000)&  $0.366$ &   $0.368$&   $0.392$ & $0.400$ & $0.446$& $0.57$& $0.383$\\ \hline
Best relative error (S=1500)&  $--$ &   $--$ &  $--$   & $--$  & $--$ & $--$ & $--$ \\ \hline
	\end{tabular} 
 \label{stat_po_max}
\end{table}
\end{comment}
\begin{figure}[h!]      
  % \begin{subfigure}[b]{\textwidth}
\centering
  \begin{tabular}{c}


      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/Poisson/proposed_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

   \hspace{-0.35 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/Poisson/unit_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

 
\hspace{-0.35 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/Poisson/random_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

 \hspace{-0.35 cm}
 
 \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/Poisson/hettinger_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

 \hspace{-0.35 cm}
 
           \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/Poisson/baseline_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

      
  \end{tabular}





 
  \caption{Average error in the predicted parameter field over the spatial domain (for the full test data set,  $S=1000$). Left to right: Proposed method (II); Random layer insertion (I);  Net2DeeperNet (II) \cite{chen2015net2net}; Forward Thinking \cite{hettinger2017forward}; Baseline. }
  \label{comparis_po}
\end{figure}     
The uncertainty associated with each method is also investigated and the results are provided in \cref{stat_po} where it is clear that our proposed method outperformed all other strategies (details of other methods are provided in \cref{hyper_parameter}). Note that the relative error reported is with respect to the test dataset. Further, we have also conducted experiments with a larger data set ($S=1500$) and the results are shown in  \cref{stat_po}.  When considering the best relative error achieved, we observe from  \cref{stat_po} that the effect of our proposed strategy (I) over other methods decreases as the number of data points increases. However, the fully-automated network growing presented in  \cref{automated} (Proposed (II)) seems to outperform all the other methods both in the low and high training data regime. More theoretical analysis is necessary to characterize our algorithm's performance in low and high data regimes (see the discussion in  \cref{conclude} for more details).

\begin{comment}
\begin{figure}[h!]      
\centering
  
  \begin{tabular}{c}
  
      \begin{tabular}{c}

          \centering
            \includegraphics[scale=0.4]{Figures/Poisson/adaptation_summary_poisson.pdf}
        
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}


      \begin{tabular}{c}

          \centering
           \includegraphics[scale=0.4]{Figures/Poisson/bar_chart_poisson.pdf}
      \end{tabular}


  
  \end{tabular}
  % \end{subfigure} \\
    \caption{ Left to Right: Summary of the adaptation results: Bar chart shows the test loss achieved at each stage of the procedure; Error bar 
 with one standard deviation of uncertainty for different adaptation strategies: 100 different random initialization are considered to investigate the uncertainty in each method.}
  \label{adaptation_results_sum_poisson}
\end{figure} 
\end{comment}


%\textcolor{blue}{Error bar plot }

%error bar


%proposed: 0.445 $\pm$ 0.033

%baseline: 0.50 $\pm$ 0.028

%unit vec=0.456 $\pm$ 0.031

%random vec=0.456 $\pm$ 0.027

%hettinger= 0.66 $\pm$ 0.032


\subsubsection{Learning the observable to parameter map for 2D Navier-Stokes equation}
\label{nav_main}
In this section, we consider another inverse problem concerning the 2D Navier-Stokes equation for
viscous and incompressible fluid \cite{li2020fourier} written as:
\begin{equation}
\begin{aligned}
    \partial_t u(\bx,\ t)+{\bv(\bx,\ t)\cdot \nabla} u(\bx,\ t)&=\nu \Delta u(\bx,\ t)+f(\bx),\quad \bx \in \LRp{0,\ 1}^2,\ t\in (0,\ T] \\
    \nabla \cdot \bv(\bx,\ t)&=0,\quad \quad \quad \quad \quad \quad \quad \quad \ \bx \in \LRp{0,\ 1}^2,\ t\in (0,T]\\
    u(\bx,\ 0)&=u_0(\bx),\quad \quad \quad \quad \quad \quad \ \bx \in \LRp{0,\ 1}^2,
\end{aligned}
\label{navier_equation_o}
\end{equation}
where $\bv(\bx,\ t)$ is the velocity field, $u(\bx,\ t)$ is the vorticity, $u_0(\bx)$ is the initial vorticity, $f(\bx)=0.1\LRp{\sin{(2\pi(x_1+x_2))}+\cos{(2\pi (x_1+x_2))}}$ is the forcing function, and $\nu=10^{-3}$ is the viscosity coefficient. The spatial domain is discretized with $32 \times 32$ uniform mesh, while the
time horizon $t \in (0,10)$ is subdivided into $1000$ time steps with $\Delta t=10^{-2}$. Our objective is to reconstruct the initial vorticity $u_0(\bx)$ from the measurements of vorticity at $20$ observed points at the final time $T=10$ (denoted as $\by$), i.e, in this case, we have input dimension $n_0=20$.  Additional details on dataset generation is provided in \cref{nav_addi}.


  
We consider experiments with training data set of size $S=250$ and $S=500$. 
We consider an additional $50$ data points for validation data set and $200$ data points for testing data set. Other details on the hyperparameter settings are provided in \cref{Input_values_different_problems}. 
\begin{figure}[h!]      
  % \begin{subfigure}[b]{\textwidth}
\centering
  \begin{tabular}{c}



      \begin{tabular}{c}

          \centering
          \includegraphics[trim={0 0 2cm 0},clip,scale=0.42]{Figures/Navier_Stokes/proposed_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

  \hspace{-0.5 cm}

      \begin{tabular}{c}

          \centering
           \includegraphics[trim={0 0 2cm 0},clip,scale=0.42]{Figures/Navier_Stokes/unit_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

 
\hspace{-0.5 cm}

      \begin{tabular}{c}

          \centering
         \includegraphics[trim={0 0 2cm 0},clip,scale=0.42]{Figures/Navier_Stokes/random_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

\hspace{-0.5 cm}

 \begin{tabular}{c}

          \centering
        \includegraphics[trim={0 0 2cm 0},clip,scale=0.42]{Figures/Navier_Stokes/hettinger_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

\hspace{-0.5 cm}

  \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.42]{Figures/Navier_Stokes/baseline_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}
      
  \end{tabular}
  \caption{Average error in the predicted parameter field over the spatial domain (for the full test data set, $S=250$). Left to right: Proposed method (II) (best rel. error: 0.295); Random layer insertion (I) (best rel. error: 0.299); Net2DeeperNet (II) \cite{chen2015net2net} (best rel. error: 0.309); Forward Thinking \cite{hettinger2017forward} (best rel. error: 0.362); Baseline (best rel. error: 0.305). }
  \label{rel_error_nav}
\end{figure} 
 \cref{rel_error_nav} shows the error $\text{Err}_j$ (equation \eqref{error_metric}) plotted for each approach (for S=250) where it is clear that our proposed approach provided the best results. Additional results on the performance (statistics of relative error) for each method is also provided in  \cref{stat_na}, where we observed 
that in the low data regime, when one considers the average relative error, our proposed approach outperformed all other strategies, performing on par with a neural architecture search algorithm. However, as the dataset size increased, we observed that random layer insertion (I) strategy performed equally well to our approach (see \cref{stat_na}). We hypothesize that our initialization strategy based on local sensitivity analysis matters more (in terms of generalization) in the low-data regime. Similar observations have been made in \cref{poisson_sec}.




\subsection{Topological derivative informed transfer learning approach}
Transfer learning is a machine learning technique in which knowledge gained through one task or dataset is used to improve the model's performance on a new dataset by fine-tuning the model on the new dataset. In this section, we demonstrate how the results in \cref{exist_th} can be used for adapting a pre-trained model in the context of transfer learning.
\subsubsection{Improving performance of pre-trained state-of-the-art machine learning models}
\label{vision_t}
 Using models pre-trained on large datasets and transferring to tasks with fewer data points is a commonly adopted strategy in machine learning literature \cite{dosovitskiy2020image}. In this section, we consider a tiny vision transformer model (ViT) pre-trained on ImageNet dataset (more details on the specific architecture employed is provided in \cref{vit_tran}). Our objective is to fine-tune this model to achieve the best performance on the CIFAR-10 dataset. The traditional approach
is to modify the output layer, i.e the MLP (multilayer perception) head of the ViT to have an output dimension of $10$ and retrain the whole network to achieve the best performance on CIFAR-10 dataset. Let's call the best-performing model  obtained this way as ``ViT baseline" (as denoted in \cref{ViT})
Our objective in this section is to use the topological derivative approach as a post-processing stage to improve the performance of ``ViT baseline" by further adapting the MLP Head using  \cref{Algo_full}.  
 




 
\begin{table}[h!]
\caption{Accuracy achieved by different adaptation strategies}
\centering
\begin{tabular}{|c | c | c | c|c|c|c|}
        \hline
    ViT  &  Proposed (II) & Proposed (I)  & Random layer & Net2DeepNet \cite{chen2015net2net}& Forward  \\ 
    baseline  &  &  &insertion (I) & & Thinking \cite{hettinger2017forward} \\ \hline
         $90.9\%$ & $91.37 \%$ &  ${\bf{91.52}}\%$ &  $91.11 \pm 0.027$ & $91.11\pm 0.0137$ & $90.9 \pm 0.0$\\ \hline
	\end{tabular} 
 \label{ViT}
\end{table}
The best accuracy achieved by different adaptation strategies is shown in  \cref{ViT} and it is clear that our approach produces the best results. Note that for this task there is no source of randomness in our proposed approach and hence the statistics is not reported. However, other adaptation strategies has an inherent randomness due to random initialization of an added layer.
%In addition, Figure \ref{} also shows how the performance improves with the addition of each layer on the MLP head, where it is again clear how other strategies  fails to generalize better in comparison to our approach.  
Further, Forward Thinking \cite{hettinger2017forward} was unable to improve the performance over ViT baseline due to the freezing of previous layers at each adaptation step.  Other details of hyperparameters used for the problem are provided in \cref{hyper_parameter_n}. Numerical experiments on transfer learning with bigger ViT models are under investigation.

 




\subsubsection{Application in parameter-efficient fine tuning}
\label{sci_tran}
%Transfer learning is a machine learning technique in which knowledge gained through one task or dataset is used to improve model's performance on a new dataset by fine-tuning the model on the new dataset. 
%However, new learning may interfere catastrophically  with old learning (catastrophic forgetting) when networks are trained sequentially \cite{kirkpatrick2017overcoming}. 
Parameter-efficient fine tuning is a transfer learning approach to address catastrophic forgetting \cite{kirkpatrick2017overcoming} by freezing many parameters/layers in the pre-trained network thereby preventing overfitting on the new data \cite{poth2023adapters}. %Parameter-efficient  transfer learning finds important application in scientific machine learning when it is often desirable to build machine learning surrogates combining sparse experimental data with huge simulation data \cite{olson2024transformer}. 
The procedure adopted in parameter-efficient transfer learning may be summarized as follows: a) Train a neural network from scratch on the large dataset  $\sD_s$; b) Fine tune a small part of the network (by freezing parameters in rest of the network) to fit the sparse data-set $\sD_e$. 
However, in this procedure, it is often unclear on which hidden layers of the network needs retraining to fit the new data $\sD_e$. Subel et al. \cite{subel2022explaining} pointed out that common wisdom guided transfer learning wherein one trains the last few layers is not necessarily the best option.  Often in literature one finds that the layers that need retraining are treated as a hyperparameter and a random search is conducted to arrive at the best decision \cite{olson2024transformer}.  

\begin{figure}[h!]      
  % \begin{subfigure}[b]{\textwidth}
 \centering
  \begin{tabular}{c}



      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/transfer_learning/au_1.png}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}


      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/transfer_learning/au_2.png}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}




      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/transfer_learning/bi_1.png}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}



            \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.23]{Figures/transfer_learning/bi_2.png}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}


  \end{tabular}
  \caption{Samples of conductivity field drawn from different distributions. Left to right: First two are samples from autocorrelation  prior; Last two are samples from a BiLaplacian prior.  }
\label{param_efficient_tr}
\end{figure} 
In this section, we demonstrate how the topological derivative in \cref{exist_th} could be used to inform which layers need retraining in parameter-efficient transfer learning. In particular, we seek to determine the most sensitive part of the network with respect to 
the new data set $\sD_e$. This is accomplished by defining the loss functional in \eqref{topo_der} based on the new data set $\sD_e$ and computing the topological derivative which informs where to add a new layer as described in  \cref{deriv_algo}. 
For demonstration, we revisit the heat equation \eqref{heat_equation_o} where the objective now is to learn a surrogate for the parameter to observable map (PtO). In particular, the network takes in the conductivity field $\bu\in \real^{256}$ as input and predicts the corresponding temperature $\by\in \real^{50}$ at fixed locations on the domain.
To train the initial network, the data-set $\sD_s$ consists of $10,000$ data points where the conductivity field $\bu$ is drawn from a BiLaplacian prior \cite{wittmer2023unifying} with mean ${\bf{\mu}}=2$ and covariance $\sC$ given in \eqref{priors} with parameters $(\gamma,\ \delta)=(0.1,\ 0.5)$. It is expected that with the passage of time the distribution of conductivity field $\bu$ changes due to changes in field conditions. To simulate this shift in distribution, the new data-set $\sD_e$ consists of $50$ samples where $\bu$ is drawn from a Gaussian autocorrelation smoothness prior \cite{kaipio2006statistical} with mean $\bf{\mu}=2$ and covariance  ${\bf{\Gamma}}$ given in \eqref{priors} with with $\sigma^2=2,\ \rho=0.5$.
\begin{equation}
   \sC=\LRp{\delta I+\gamma \nabla \cdot ( \nabla)}^{-2},\quad {\bf{\Gamma}}_{ij}=\sigma^2\exp \LRp{-\frac{\norm{\bx_i-\bx_j}_2^2}{2\rho^2}}, 
   \label{priors}
\end{equation}


\begin{table}[h!]
\caption{Mean squared error achieved by different transfer learning approaches}
\centering
\begin{tabular}{|c | c | c | c|c|}
        \hline
    Complete &  Traditional transfer learning& Proposed  approach &  Exhaustive search \\ 
   retraining   &  &  & Best case \vline \ Worst case \\ \hline
         $3.572$ & $5.040$ &  ${\bf{2.576}}$ &  2.604 \vline \ \ \  3.240  \\ \hline
	\end{tabular} 
 \label{transfer}
\end{table}
Samples of generated conductivity fields from the two distributions are shown in  \cref{param_efficient_tr}.  Performance of different transfer learning approaches on a test data set of size $1000$ is shown in \cref{transfer} where it is clear that our proposed approach provides the best results. In  \cref{transfer}, ``Complete retraining" refers to the case when the entire pre-trained network is trained to fit the new data; ``Traditional transfer learning" refers to retraining only the last layer in the network; ``Proposed approach" refers to the topological derivative approach where a new layer is added and retrained along with the first and last layers; and ``Exhaustive search" refers to the case of retraining the last layer, first layer and a randomly chosen hidden layer. Additional justification (with numerical results) on why topological derivative serves as a good indicator for
determining where to add a new layer is provided in \cref{param_eff}.






\section{Conclusion}
\label{conclude}

In this work, we derived an expression for the  ``topological derivative" for a neural network and demonstrated how the concept can be used for progressively adapting a neural network along the depth. In \cref{optimal_trans}, we also showed that our layer insertion strategy can be viewed through the lens of an optimal transport problem. 
Numerically, we observed that our proposed method outperforms other adaptation strategies for most datasets considered in the study. In particular, our method  (especially Proposed (I)) exhibits superior performance compared to other approaches in the low training data regime. Numerical results further suggest that employing a scheduler (Proposed I) is in general effective leading to superior performance in comparison to other strategies although occasionally our Proposed (II) (fully automated growing in \cref{additional_rem}) exhibited superior performance. %Motivated by this observation, to extract the best performing architecture  we recommend allotting half the computational resources for Proposed (I) strategy, and half for Proposed (II) strategy. 

Note that the topological derivative approach is a greedy approach where one chooses the best decision (locally) on where to add a layer along with the initialization at each step to reach at a globally optimal architecture. Here we define globally optimal architecture as the one that gives the lowest generalization error out of all possible architecture sampled from a predefined search space (see  \cref{hyper_parameter} for the search space we consider) and considering all random initializations from the normal distribution $\sN\LRp{{\bf{0}},\sigma_n^2 {\bf{I}}}$. In spite of our approach being greedy, we observe that our approach achieves comparable performance to that of a neural architecture search algorithm \cite{li2020random} for the datasets considered in this study. Deriving bounds on  the distance between the best architecture from our algorithm and the globally optimal architecture is beyond the scope of present work and will be investigated in the future (previous works have developed distance metric in the space of neural network
architectures \cite{kandasamy2018neural}). We anticipate that this analysis would provide insight on why our method (especially Proposed (I)) work particularly well in the low training data regime in comparison to other methods. 

%\krish{The conclusions should also summarize your theoretical findings (including connection with optimal transport) and numerical findings, and how your approach is better than others in which regimes. When your methods are not good, and point to ongoing research to address these issues (or point out why such issues are intrinsic to your methods)

%\krish{other hyperparameters need tuning}

\section*{Acknowledgment}
The authors would like to thank  Hai Van Nguyen for fruitful
discussions and helping to generate some of the data sets for computational experiments. The authors also acknowledge the Texas Advanced Computing Center (TACC)
at The University of Texas at Austin for providing HPC, visualization, database, or grid
resources that have contributed to some of the results reported within this paper. \href{http://www.overleaf.com}{URL:}  \url{http://www.tacc.utexas.edu.}

\bibliographystyle{siamplain}
\begin{thebibliography}{10}

\bibitem{amstutz2022introduction}
{\sc S.~Amstutz}, {\em An introduction to the topological derivative},
  Engineering Computations, 39 (2022), pp.~3--33.

\bibitem{amstutz2005crack}
{\sc S.~Amstutz, I.~Horchani, and M.~Masmoudi}, {\em Crack detection by the
  topological gradient method}, Control and cybernetics, 34 (2005),
  pp.~81--101.

\bibitem{auroux2010application}
{\sc D.~Auroux, L.~Jaafar-Belaid, and B.~Rjaibi}, {\em Application of the
  topological gradient method to tomography}, Revue Africaine de Recherche en
  Informatique et Math{\'e}matiques Appliqu{\'e}es, 13 (2010).

\bibitem{balaprakash2018deephyper}
{\sc P.~Balaprakash, M.~Salim, T.~D. Uram, V.~Vishwanath, and S.~M. Wild}, {\em
  Deephyper: Asynchronous hyperparameter search for deep neural networks}, in
  2018 IEEE 25th international conference on high performance computing (HiPC),
  IEEE, 2018, pp.~42--51.

\bibitem{belaid2006image}
{\sc L.~J. Belaid, M.~Jaoua, M.~Masmoudi, and L.~Siala}, {\em Image restoration
  and edge detection by topological asymptotic expansion}, Comptes Rendus
  Mathematique, 342 (2006), pp.~313--318.

\bibitem{bengio2007greedy}
{\sc Y.~Bengio, P.~Lamblin, D.~Popovici, H.~Larochelle, et~al.}, {\em Greedy
  layer-wise training of deep networks}, Advances in neural information
  processing systems, 19 (2007), p.~153.

\bibitem{benning2019deep}
{\sc M.~Benning, E.~Celledoni, M.~J. Ehrhardt, B.~Owren, and C.-B.
  Sch{\"o}nlieb}, {\em Deep learning as optimal control problems: Models and
  numerical methods}, arXiv preprint arXiv:1904.05657,  (2019).

\bibitem{bodyanskiy2020multilayer}
{\sc Y.~Bodyanskiy, A.~Pirus, and A.~Deineko}, {\em Multilayer radial-basis
  function network and its learning}, in 2020 IEEE 15th International
  Conference on Computer Sciences and Information Technologies (CSIT), vol.~1,
  IEEE, 2020, pp.~92--95.

\bibitem{bui2024unified}
{\sc T.~Bui-Thanh}, {\em A unified and constructive framework for the
  universality of neural networks}, IMA Journal of Applied Mathematics, 89
  (2024), pp.~197--230.

\bibitem{carmona2018probabilistic}
{\sc R.~Carmona, F.~Delarue, et~al.}, {\em Probabilistic theory of mean field
  games with applications I-II}, Springer, 2018.

\bibitem{chen2015net2net}
{\sc T.~Chen, I.~Goodfellow, and J.~Shlens}, {\em Net2net: Accelerating
  learning via knowledge transfer}, arXiv preprint arXiv:1511.05641,  (2015).

\bibitem{constantine2016accelerating}
{\sc P.~G. Constantine, C.~Kent, and T.~Bui-Thanh}, {\em Accelerating markov
  chain monte carlo with active subspaces}, SIAM Journal on Scientific
  Computing, 38 (2016), pp.~A2779--A2805.

\bibitem{craddock1996multi}
{\sc R.~Craddock and K.~Warwick}, {\em Multi-layer radial basis function
  networks. an extension to the radial basis function}, in Proceedings of
  International Conference on Neural Networks (ICNN'96), vol.~2, IEEE, 1996,
  pp.~700--705.

\bibitem{dosovitskiy2020image}
{\sc A.~Dosovitskiy}, {\em An image is worth 16x16 words: Transformers for
  image recognition at scale}, arXiv preprint arXiv:2010.11929,  (2020).

\bibitem{elsken2018efficient}
{\sc T.~Elsken, J.~H. Metzen, and F.~Hutter}, {\em Efficient multi-objective
  neural architecture search via lamarckian evolution}, arXiv preprint
  arXiv:1804.09081,  (2018).

\bibitem{eschenauer1994bubble}
{\sc H.~A. Eschenauer, V.~V. Kobelev, and A.~Schumacher}, {\em Bubble method
  for topology and shape optimization of structures}, Structural optimization,
  8 (1994), pp.~42--51.

\bibitem{evci2022gradmax}
{\sc U.~Evci, B.~van Merrienboer, T.~Unterthiner, M.~Vladymyrov, and
  F.~Pedregosa}, {\em Gradmax: Growing neural networks using gradient
  information}, arXiv preprint arXiv:2201.05125,  (2022).

\bibitem{garcia2020block}
{\sc S.~R. Garcia and R.~A. Horn}, {\em Block matrices in linear algebra},
  PRIMUS, 30 (2020), pp.~285--306.

\bibitem{hettinger2017forward}
{\sc C.~Hettinger, T.~Christensen, B.~Ehlert, J.~Humpherys, T.~Jarvis, and
  S.~Wade}, {\em Forward thinking: Building and training neural networks one
  layer at a time}, arXiv preprint arXiv:1706.02480,  (2017).

\bibitem{hinton2007learning}
{\sc G.~E. Hinton}, {\em Learning multiple layers of representation}, Trends in
  cognitive sciences, 11 (2007), pp.~428--434.

\bibitem{kaipio2006statistical}
{\sc J.~Kaipio and E.~Somersalo}, {\em Statistical and computational inverse
  problems}, vol.~160, Springer Science \& Business Media, 2006.

\bibitem{kandasamy2018neural}
{\sc K.~Kandasamy, W.~Neiswanger, J.~Schneider, B.~Poczos, and E.~P. Xing},
  {\em Neural architecture search with bayesian optimisation and optimal
  transport}, Advances in neural information processing systems, 31 (2018).

\bibitem{kidger2020universal}
{\sc P.~Kidger and T.~Lyons}, {\em Universal approximation with deep narrow
  networks}, in Conference on learning theory, PMLR, 2020, pp.~2306--2327.

\bibitem{kilcher2018escaping}
{\sc Y.~Kilcher, G.~B{\'e}cigneul, and T.~Hofmann}, {\em Escaping flat areas
  via function-preserving structural network modifications},  (2018).

\bibitem{kingma2014adam}
{\sc D.~P. Kingma and J.~Ba}, {\em Adam: A method for stochastic optimization},
  arXiv preprint arXiv:1412.6980,  (2014).

\bibitem{kirkpatrick2017overcoming}
{\sc J.~Kirkpatrick, R.~Pascanu, N.~Rabinowitz, J.~Veness, G.~Desjardins, A.~A.
  Rusu, K.~Milan, J.~Quan, T.~Ramalho, A.~Grabska-Barwinska, et~al.}, {\em
  Overcoming catastrophic forgetting in neural networks}, Proceedings of the
  national academy of sciences, 114 (2017), pp.~3521--3526.

\bibitem{krishnanunni2022layerwise}
{\sc C.~Krishnanunni and T.~Bui-Thanh}, {\em Layerwise sparsifying training and
  sequential learning strategy for neural architecture adaptation}, arXiv
  preprint arXiv:2211.06860,  (2022).

\bibitem{kulkarni2017layer}
{\sc M.~Kulkarni and S.~Karande}, {\em Layer-wise training of deep networks
  using kernel similarity}, arXiv preprint arXiv:1703.07115,  (2017).

\bibitem{li2020system}
{\sc L.~Li, K.~Jamieson, A.~Rostamizadeh, E.~Gonina, J.~Ben-Tzur, M.~Hardt,
  B.~Recht, and A.~Talwalkar}, {\em A system for massively parallel
  hyperparameter tuning}, Proceedings of Machine Learning and Systems, 2
  (2020), pp.~230--246.

\bibitem{li2020random}
{\sc L.~Li and A.~Talwalkar}, {\em Random search and reproducibility for neural
  architecture search}, in Uncertainty in artificial intelligence, PMLR, 2020,
  pp.~367--377.

\bibitem{li2018optimal}
{\sc Q.~Li and S.~Hao}, {\em An optimal control approach to deep learning and
  applications to discrete-weight neural networks}, in International Conference
  on Machine Learning, PMLR, 2018, pp.~2985--2994.

\bibitem{li2020fourier}
{\sc Z.~Li, N.~Kovachki, K.~Azizzadenesheli, B.~Liu, K.~Bhattacharya,
  A.~Stuart, and A.~Anandkumar}, {\em Fourier neural operator for parametric
  partial differential equations}, arXiv preprint arXiv:2010.08895,  (2020).

\bibitem{liu2021survey}
{\sc Y.~Liu, Y.~Sun, B.~Xue, M.~Zhang, G.~G. Yen, and K.~C. Tan}, {\em A survey
  on evolutionary neural architecture search}, IEEE transactions on neural
  networks and learning systems,  (2021).

\bibitem{maile2022and}
{\sc K.~Maile, E.~Rachelson, H.~Luga, and D.~G. Wilson}, {\em When, where, and
  how to add new neurons to anns}, in International Conference on Automated
  Machine Learning, PMLR, 2022, pp.~18--1.

\bibitem{miikkulainen2019evolving}
{\sc R.~Miikkulainen, J.~Liang, E.~Meyerson, A.~Rawal, D.~Fink, O.~Francon,
  B.~Raju, H.~Shahrzad, A.~Navruzyan, N.~Duffy, et~al.}, {\em Evolving deep
  neural networks}, in Artificial intelligence in the age of neural networks
  and brain computing, Elsevier, 2019, pp.~293--312.

\bibitem{montavon2011kernel}
{\sc G.~Montavon, M.~L. Braun, and K.-R. MÃ¼ller}, {\em Kernel analysis of deep
  networks}, Journal of Machine Learning Research, 12 (2011), pp.~2563--2581.

\bibitem{montavon2010layer}
{\sc G.~Montavon, K.-R. Muller, and M.~Braun}, {\em Layer-wise analysis of deep
  networks with gaussian kernels}, Advances in neural information processing
  systems, 23 (2010), pp.~1678--1686.

\bibitem{nguyen2024tnet}
{\sc H.~V. Nguyen and T.~Bui-Thanh}, {\em Tnet: A model-constrained tikhonov
  network approach for inverse problems}, SIAM Journal on Scientific Computing,
  46 (2024), pp.~C77--C100.

\bibitem{olson2024transformer}
{\sc M.~L. Olson, S.~Liu, J.~J. Thiagarajan, B.~Kustowski, W.-K. Wong, and
  R.~Anirudh}, {\em Transformer-powered surrogates close the icf
  simulation-experiment gap with extremely limited data}, Machine Learning:
  Science and Technology, 5 (2024), p.~025054.

\bibitem{scikit-learn}
{\sc F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay}, {\em Scikit-learn:
  Machine learning in {P}ython}, Journal of Machine Learning Research, 12
  (2011), pp.~2825--2830.

\bibitem{poth2023adapters}
{\sc C.~Poth, H.~Sterz, I.~Paul, S.~Purkayastha, L.~Engl{\"a}nder, T.~Imhof,
  I.~Vuli{\'c}, S.~Ruder, I.~Gurevych, and J.~Pfeiffer}, {\em Adapters: A
  unified library for parameter-efficient and modular transfer learning}, arXiv
  preprint arXiv:2311.11077,  (2023).

\bibitem{real2019regularized}
{\sc E.~Real, A.~Aggarwal, Y.~Huang, and Q.~V. Le}, {\em Regularized evolution
  for image classifier architecture search}, in Proceedings of the aaai
  conference on artificial intelligence, vol.~33, 2019, pp.~4780--4789.

\bibitem{russakovsky2015imagenet}
{\sc O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, et~al.}, {\em Imagenet large scale
  visual recognition challenge}, International journal of computer vision, 115
  (2015), pp.~211--252.

\bibitem{santambrogio2015optimal}
{\sc F.~Santambrogio}, {\em Optimal transport for applied mathematicians},
  Birk{\"a}user, NY, 55 (2015), p.~94.

\bibitem{simonyan2014very}
{\sc K.~Simonyan and A.~Zisserman}, {\em Very deep convolutional networks for
  large-scale image recognition}, arXiv preprint arXiv:1409.1556,  (2014).

\bibitem{sokolowski1999topological}
{\sc J.~Sokolowski and A.~Zochowski}, {\em On the topological derivative in
  shape optimization}, SIAM journal on control and optimization, 37 (1999),
  pp.~1251--1272.

\bibitem{stanley2002evolving}
{\sc K.~O. Stanley and R.~Miikkulainen}, {\em Evolving neural networks through
  augmenting topologies}, Evolutionary computation, 10 (2002), pp.~99--127.

\bibitem{subel2022explaining}
{\sc A.~Subel, Y.~Guan, A.~Chattopadhyay, and P.~Hassanzadeh}, {\em Explaining
  the physics of transfer learning a data-driven subgrid-scale closure to a
  different turbulent flow}, arXiv preprint arXiv:2206.03198,  (2022).

\bibitem{suganuma2017genetic}
{\sc M.~Suganuma, S.~Shirakawa, and T.~Nagao}, {\em A genetic programming
  approach to designing convolutional neural network architectures}, in
  Proceedings of the genetic and evolutionary computation conference, 2017,
  pp.~497--504.

\bibitem{szegedy2015going}
{\sc C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich}, {\em Going deeper with convolutions}, in
  Proceedings of the IEEE conference on computer vision and pattern
  recognition, 2015, pp.~1--9.

\bibitem{todorov2006optimal}
{\sc E.~Todorov}, {\em Optimal control theory},  (2006).

\bibitem{wen2020autogrow}
{\sc W.~Wen, F.~Yan, Y.~Chen, and H.~Li}, {\em Autogrow: Automatic layer
  growing in deep convolutional networks}, in Proceedings of the 26th ACM
  SIGKDD International Conference on Knowledge Discovery \& Data Mining, 2020,
  pp.~833--841.

\bibitem{wittmer2023unifying}
{\sc J.~Wittmer, C.~Krishnanunni, H.~V. Nguyen, and T.~Bui-Thanh}, {\em On
  unifying randomized methods for inverse problems}, Inverse Problems, 39
  (2023), p.~075010.

\bibitem{wu2020firefly}
{\sc L.~Wu, B.~Liu, P.~Stone, and Q.~Liu}, {\em Firefly neural architecture
  descent: a general approach for growing neural networks}, Advances in neural
  information processing systems, 33 (2020), pp.~22373--22383.

\bibitem{wu2019splitting}
{\sc L.~Wu, D.~Wang, and Q.~Liu}, {\em Splitting steepest descent for growing
  neural architectures}, Advances in Neural Information Processing Systems, 32
  (2019).

\bibitem{wynne1993node}
{\sc M.~Wynne-Jones}, {\em Node splitting: A constructive algorithm for
  feed-forward neural networks}, Neural Computing \& Applications, 1 (1993),
  pp.~17--22.

\bibitem{xu1999training}
{\sc D.~Xu and J.~C. Principe}, {\em Training mlps layer-by-layer with the
  information potential}, in IJCNN'99. International Joint Conference on Neural
  Networks. Proceedings (Cat. No. 99CH36339), vol.~3, IEEE, 1999,
  pp.~1716--1720.

\bibitem{zeiler2014visualizing}
{\sc M.~D. Zeiler and R.~Fergus}, {\em Visualizing and understanding
  convolutional networks}, in European conference on computer vision, Springer,
  2014, pp.~818--833.

\bibitem{zoph2016neural}
{\sc B.~Zoph and Q.~V. Le}, {\em Neural architecture search with reinforcement
  learning}, arXiv preprint arXiv:1611.01578,  (2016).

\end{thebibliography}

%\bibliography{references}

\section*{SUPPLEMENTARY MATERIAL: TOPOLOGICAL DERIVATIVE APPROACH FOR DEEP NEURAL
NETWORK ARCHITECTURE ADAPTATION}

 \phantom{.}

\appendix



\begin{section}{Architecture adaptation for convolutional neural network (CNN)}
\label{CNN_application}
Note that though  \cref{opt_top_full} and \cref{full_topo} present the approach in the context of a fully connected network, the framework can also be applied to a CNN architecture where the input is a tensor of shape: (input height) $\times$ (input width) $\times$ (input channels). Note that in this case the function $\bff_{t+1}(\bx_{s,t};\ \Btheta_{t+1})$ in \eqref{training_problem} simply represents the 2D convolutional layer where $\bx_{s,t}$ represents the vectorized input and $\btheta_{t+1}$ represents the vectorized filter parameters. Note that $\bff_t(.;\ .)$ satisfies the assumptions in \cref{exist_th} when employing differentiable activation functions.

\begin{algorithm}
	\caption{CNN architecture adaptation algorithm}
	\hspace*{\algorithmicindent} \textbf{Input}: Training data $\bX$, labels $\bC$, validation data $\bX_1$, validation labels $\bC_1$, number of channels in each hidden layer $n$, loss function $\Phi$, filter size $f\times f \times n$, stride $s$, number of channels to activate in each hidden layer ${m}$, number of iterations $N_n$, parameter $\epsilon,\ \epsilon^t$, parameter $T_b$, hyperparameters and predefined scheduler for optimizer (\cref{hyper_parameter_n}).\\
	\hspace*{\algorithmicindent} \textbf{Initialize}:   Initialize  network $\sQ_1$ with $T_b$ hidden layers.\\
	\begin{algorithmic}[1] 
  \State Train network $\sQ_1$  and store the validation loss $(\epsilon_v)^{1}$.
  		\State set $i =1$,  $(\epsilon_v)^{0}>>(\epsilon_v)^{1}$, $\Lambda_l^m\geq \epsilon^t$
		\While{$i \le N_n$ \textbf{and} $\LRs{(\epsilon_v)^{i}\leq (\epsilon_v)^{i-1}}$\textbf{and} $\Lambda_l^m\geq \epsilon^t$} 
		%\State Train network $\sQ_i$ for $E_e$ epochs using standard optimizers.
		%\State Compute the topological derivative for each layer $l$ as $\Lambda_l=\sum_{j=1}^{a_m}\lambda_j^l$, where $\{\lambda_1^l\dots \lambda_{a_m}^l \}$ represents the top 
 % \hspace*{\algorithmicindent}   $a_m$ eigen values of $\sQ_l$ given by \eqref{eigen_matrix}. 
 \State Compute the topological derivative for each layer $l$  using \eqref{compute_full} and store as $\{ \Lambda_l\}$, also store $\Lambda_l^m=\max_l\{ \Lambda_l\}$.
   \State Store the corresponding eigenvectors for each layer as $\Phi_l$ representing $n$ filters of size $f\times f \times n$.
        \State Obtain the new network $\sQ_{i+1}$ by adding a new layer at position $l^*=\argmax\limits_l \{\Lambda_l\}$ with filter parameters  $\epsilon\Phi_{l^*}$.
        \State Perform a backtracking line search to update $\epsilon$ as outlined in \cref{back_track}.
        \State Update the parameters for optimizer if required (refer  \cref{hyper_parameter_n} for scheduler details).
%        \State Perform a backtracking algorithm for finding the best $\epsilon\in [0,\ 1]$ that gives the maximum decrease in training 
      %    \hspace*{\algorithmicindent}  loss.
        \State Train network $\sQ_{i+1}$  and store the best validation loss $(\epsilon_v)^{i+1}$ and the best network $\sQ_{i+1}$.
		\State $i = i+1$
		\EndWhile
	\end{algorithmic} \label{Algo_full_CNN}
\hspace*{\algorithmicindent} \textbf{Output}: Network $\sQ_{i-1}$
\end{algorithm}



In order to satisfy condition \ref{one} in  \cref{prop_admissible} (ResNet propagation), zero-padding is employed to preserve the original input size. Note that in this case $\Phi_l$ in \eqref{compute_full} denotes the filter parameters for the added layer (vectorized). Our algorithm for a CNN architecture is given in  \cref{Algo_full_CNN} and is self-explanatory.
Note that our proposed approach can be applied to any architecture as long as conditions in  \cref{prop_admissible} and \cref{exist_th} are satisfied.


    
\end{section}



\begin{section}{Fully automated network growing \cref{Algo_full_auto}}
\label{automated}
Note that \cref{Algo_full} activates the ``the best" $m$ neurons while inserting a new layer as described in  \cref{select_neuron}. Note that $m$ is a user-defined hyperparameter in \cref{Algo_full}. In this section, we extend our algorithm to automatically select the parameter $m$, i.e. automatically determine the number of neurons (width) of the added hidden layer. To that end let us revisit
 \cref{prop_fi} where  $m$ is now unknown. The idea is to look at how the optimal loss in \eqref{quant_full_a} changes as one increases $m$. In particular, for each layer $l$ we choose the largest value of $m$ such that:
\begin{equation}
\frac{\lambda_{1}-\lambda_{m}}{\lambda_1}\leq \epsilon_s,\quad s.t \ \ \lambda_1,\ \lambda_m>0,
\label{additional_constraint}
\end{equation}
where $\lambda_i$ is  defined in \eqref{compute_full} and $\epsilon_s$ is a user-defined hyper parameter. Since now $m$ could be different for each layer $l$, let us denote it as $m(l)$. Criterion \eqref{additional_constraint} simply compares the sensitivity of each neuron relative to the most sensitive neuron in the added layer. 
If $\epsilon_s=0$, then it is clear that only the most sensitive neuron gets activated. On the other hand, when $\epsilon_s=1$ all the neurons (with $\lambda_m>0$) in the hidden layer gets activated. In practice, we choose $\epsilon_s=0.5$ such that all neurons which are at least $50\ \%$ as sensitive as the most sensitive neuron gets activated in the added layer. Further, note that the derivative $\Lambda_l$ in \eqref{compute_full} is computed with respect to the operation of activating exactly $m$ neurons in an added layer. However, since in this case $m=m(l)$ can be different for each layer $l$, one needs to normalize the derivative in \eqref{compute_full} as $\Lambda_l=\frac{\Lambda_l}{m(l)}$  while making decision on where to add a new layer. The full algorithm is provided in  \cref{Algo_full_auto}.

Further, in the fully automated network growing (Proposed II), we do not use a predefined scheduler (such as in \cref{Algo_full}) to decide when a new layer needs to be added during the training process. Instead, a validation data set is employed to decide when a new layer needs to be added.  The key difference of fully-automated growing from \cref{Algo_full} is the training loop without using a predefined scheduler (lines 9-17 in  \cref{Algo_full_auto}). Note that one stops training the current network $\sQ_{i+1}$ if the validation loss does not decrease for $N_k$ consecutive training epochs. Then, a new layer is added. 
The extension of \cref{Algo_full_auto} to the case of a CNN architecture is trivial (in this case the parameter $m$  corresponds to the number of channels in the hidden layer).
    \begin{algorithm} 
	\caption{Fully automated network growing algorithm for fully connected architecture}
	\hspace*{\algorithmicindent} \textbf{Input}: Training data $\bX$, labels $\bC$, validation data $\bX_1$, validation labels $\bC_1$, number of neurons in each hidden layer $n$, loss function $\Phi$, number of iterations $N_n$,  parameter $\epsilon$, $\epsilon^t$, parameter $T_b,\ N_k$, hyperparameters for optimizer (\cref{hyper_parameter_n}).\\
	\hspace*{\algorithmicindent} \textbf{Initialize}:   Initialize  network $\sQ_1$ with $T_b$ hidden layers.\\
	\begin{algorithmic}[1] 
  \State Train network $\sQ_1$  and store the best validation loss $(\epsilon_v)^{1}$.
  		\State set $i =1$,  $(\epsilon_v)^{0}>>(\epsilon_v)^{1}$, $\Lambda_l^m\geq \epsilon^t$
		\While{$i \le N_n$ \textbf{and} $\LRs{(\epsilon_v)^{i}\leq (\epsilon_v)^{i-1}}$ \textbf{and} $\Lambda_l^m\geq \epsilon^t$} 
  %\tanbui{or the largest topological derivative at any layer is less than some tolerance. This criteria ensures that adding a new layer at any location induces a negligible change in the loss function. That is the training loss saturates.}
		%\State Train network $\sQ_i$ for $E_e$ epochs using standard optimizers.
		%\State Compute the topological derivative for each layer $l$ as $\Lambda_l=\sum_{j=1}^{a_m}\lambda_j^l$, where $\{\lambda_1^l\dots \lambda_{a_m}^l \}$ represents the top 
 % \hspace*{\algorithmicindent}   $a_m$ eigen values of $\sQ_l$ given by \eqref{eigen_matrix}. 
 \State Compute $m(l)$ for each layer $l$ using   \eqref{additional_constraint} as outlined in  \cref{automated}.
 \State Compute the derivative for each layer $l$  using \eqref{compute_full} and store as $\{ \frac{\Lambda_l}{m(l)}\}$, also store $\Lambda_l^m=\max_l\{ \frac{\Lambda_l}{m(l)}\}$
   \State Store the corresponding eigenvectors for each layer as $\Phi_l$ given by \eqref{compute_full}.
        \State Obtain the new network $\sQ_{i+1}$ by adding a new layer at position $l^*=\argmax\limits_l \{ \frac{\Lambda_l}{m(l)}\}$ with initialization  $\epsilon\Phi_{l^*}$.
        \State Perform a backtracking line search to update $\epsilon$ as outlined in \cref{back_track}.
%        \State Perform a backtracking algorithm for finding the best $\epsilon\in [0,\ 1]$ that gives the maximum decrease in training 
      %    \hspace*{\algorithmicindent}  loss.
\State Set epoch $j=1$, $(\epsilon_v)^{i+1}_j$ as the validation loss for $\sQ_{i+1}$, and set $k=1$.
      \While{$k \leq N_k$}\Comment{Training loop without using a scheduler}
        \State Update parameters of network $\sQ_{i+1}$   and store the current best validation loss $(\epsilon_v)^{i+1}_{j+1}$. 
        \If{$(\epsilon_v)^{i+1}_{j+1}\geq (\epsilon_v)^{i+1}_{j}$}
        \State k=k+1
                \Else
        \State set $k=1$
        \EndIf
		\State $j = j+1$
		\EndWhile
  \State Store the best validation loss for $\sQ_{i+1}$ as $(\epsilon_v)^{i+1}$=$(\epsilon_v)^{i+1}_{j+1}$ and the corresponding  best network $\sQ_{i+1}$.
  \State $i = i+1$
  \EndWhile
	\end{algorithmic} \label{Algo_full_auto}
\hspace*{\algorithmicindent} \textbf{Output}: Network $\sQ_{i-1}$
\end{algorithm}



\end{section}

\begin{comment}
\begin{section}{Topological derivative approach through the lens of an  optimal transport problem}
\label{optimal_trans}
In this section, we show that our layer adaptation strategy can also be derived as a solution to 
%a constrained 
an optimization problem in $p-$Wasserstein space, where $p\geq 1$. Optimization in $\infty-$Wasserstein space was earlier considered by Liu et al. \cite{wu2019splitting} to explain their neuron splitting strategy for growing neural networks along the width. The standard optimal transport (Kantorovich formulation \cite{santambrogio2015optimal}) aims at computing a minimal cost transportation plan $\gamma$  between a source probability measure $\mu$, and target probability measure $\nu$.
%Note that the plan $\gamma$ is a joint probability measure whose marginals are $\mu$ and $\nu$ on the first and second factors, respectively.
A common formulation of the cost function is based on the Wasserstein distance  $W_p (\mu;\ \nu)$.
\begin{definition}[$p$-Wasserstein metric]
The  $p$-Wasserstein distance between two probability measures $\mu$ and $\nu$ is given as:
\begin{equation}
    W_p \LRp{\mu;\ \nu}={\inf_{\gamma \in \Gamma (\mu;\ \nu)} \LRp{\underset{(\bphi,\ \bphi') \sim \gamma}{\expect}  \norm{\bphi-\bphi'}_2^p}^{1/p}},
    \label{wasr}
\end{equation}
    where  $\Gamma (\mu;\ \nu)$ is the set of couplings of $\mu$ and $\nu$. A coupling $\gamma$ is a joint probability measure whose marginals are $\mu$ and $\nu$ on the first and second factors, respectively. 
    %$esssup$ denotes the essential supremum \footnote{essential supremum denotes the smallest number $c$ such that the set $\{ (\bphi,\ \bphi'): \norm{\bphi-\bphi'}_2 >c\}$ has zero probability under $\gamma$. } of $\norm{\bphi-\bphi'}_2$ with respect to measure $\gamma$.
    

\end{definition}




In this work, {\em we consider an inverse problem where only the measure $\mu$ is known and the candidate optimal map is given,  but the target measure $\nu$ is unknown to be determined. }
%As a result 
% we need an additional constraint/ reformulate the cost that allows us to compute the optimal plan $\gamma^*$ along with the optimal $\nu^*$. 
%  To that end, let us consider the following constrained minimization problem:
% \begin{equation}
%     \min_{\nu\in \mathcal{Y}_p} {\mathcal{T}(\mu;\ \nu)}\ \ s.t\ \ \ W_p (\mu;\ \nu)\leq  \epsilon,
%     \label{optimal_transport}
% \end{equation}
% where $\mathcal{Y}_p$ is the p-Wasserstein space,\footnote{Let $(X,\ d)$ be a compact metric space. For $p\geq 1$, the Wasserstein space $\mathcal{Y}_p$ is defined as the set $\mathcal{P}(X)$  of probability measures on $X$ equipped with the p-Wasserstein distance.}   and $T$ is the loss functional, $\epsilon \in \real$ is given. 
We still consider the scenario depicted in \cref{fig:topo} in which we add a message-passing layer $\Omega_\epsilon\Large |_{\epsilon=0}$  with zero weights/biases, and then ask the question:  what is the initialization values $\boldsymbol{\phi}$ for the weights/biases of the newly added layer such that the lost function is most sensitive too? In this section, we answer this question from an optimal transport point of view. For the clarity of the exposition (and without loss of generality), we assume that there is only one data, i.e. $S = 1$, and thus we will remove the subscripts corresponding to the data. 




%Without loss of generality, we illustrate the idea for the case of a one-hidden layer fully connected network $\Omega_0$ with the output layer having the same dimension as the hidden layer.  Let $\Omega_\epsilon$ be the new network obtained by adding a new layer (this is the new output layer as shown in  \cref{fig:TRANSPORT}).  We wish to optimally transport the particles (parameters $\bphi^a$) of the fictitious output layer of network $\Omega_0$    to the parameters $\overline{T}\LRp{\bphi^a}$ of new network $\Omega_\epsilon$ in some sense, thereby leading to a natural connection with optimal transport theory. 
%The scenario is depicted in \cref{fig:TRANSPORT}. 
%The fictitious output layer initialized with parameters $\bphi^a$ should have the property that the output of network $\Omega_0$ remains unchanged after adding this new layer as depicted in  \cref{fig:TRANSPORT}.  \cref{prop_admissible} gives details on how to construct one such transformation.    In this section, we look at more generic conditions for constructing a fictitious layer.
% \def\layersep{1.4cm}
% \def\nodeinlayersep{0.7cm}
% \begin{figure}[H]
% \centering
% \begin{tikzpicture}[
%     node distance=\layersep,
%     edge/.style={-stealth,shorten >=1pt, draw=black!50,thin},
%     neuron/.style={circle,fill=black!25,minimum size=10pt,inner sep=0pt},
%     operator/.style={rectangle,fill=green!,minimum height= \nodeinlayersep, minimum width= 0.8 * \layersep, inner sep=0pt, rounded corners},
%     input neuron/.style={neuron, fill=green!50,minimum size=12pt},
%     output neuron/.style={neuron, fill=green!50,minimum size=12pt},
%     hidden neuron/.style={neuron, fill=blue!50},
%     Forward map/.style={operator, fill=red!50},
%     annot/.style={text width=4em, text centered},
%     every node/.style={scale=1.0},
%     node1/.style={scale=2.0}
% ]
%     % Draw the input layer nodes
%     \foreach \name / \y in {1,...,4}
%         {\ifnum \y=3
%             \node (I-\name) at (0,-\nodeinlayersep * \y - \nodeinlayersep * 0.5) {$\vdots$};
%             % \node[input neuron] (I-\name) at (0,-\y-0.4) {$y_{\y}$};
%         \else
%             \ifnum \y=4
%                 \node[input neuron] (I-\name) at (0,-\nodeinlayersep * \y - \nodeinlayersep * 0.5) {$x_{n}$};
%             \else
%                 \node[input neuron] (I-\name) at (0,-\nodeinlayersep *\y - \nodeinlayersep * 0.5 ) {$x_{\y}$};
%             \fi
%         \fi}
        
%     % Draw the output layer node
%     \foreach \name / \y in {1,...,5}
%         {\ifnum \y=4
%             \node (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.0) {$\vdots$};
      
%             % \node[input neuron] (I-\name) at (0,-\y-0.4) {$y_{\y}$};
%         \else
%             \ifnum \y=5
%                 \node[input neuron] (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.0) {$y_{p}$};
%             \else
%                 \node[input neuron] (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.0) {$y_{\y}$};
%             \fi
%         \fi}
        

    
    
        
    
%     % set number of hidden layers
%     \newcommand \Nhidden{2}
%     % Draw the hidden layer nodes
%     \foreach \N in {1,...,\Nhidden} {
%         \foreach \y in {1,...,5} { %%% MODIFIED (1,...,12 -> 1,...,5, and the next five lines)
%             \ifnum \y=4
%                 \node at (\N*\layersep,-\y*\nodeinlayersep) {$\vdots$};
%             \else
%                 \node[hidden neuron] (H\N-\y) at (\N*\layersep,-\y*\nodeinlayersep ) {$\sigma$};
%         \fi
%       }
%     }

%     %%% <-- MODIFIED (from H\Nhidden-6 to H\Nhidden-3) 
%     % Connect every node in the input layer with every node in the
%     % hidden layer.
%     \foreach \source in {1,2,4}
%         \foreach \dest in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
%             \draw[edge] (I-\source) -- (H1-\dest);
    
%     % connect all hidden stuff
%     \foreach [remember=\N as \lastN (initially 1)] \N in {2,...,\Nhidden}
%       \foreach \source in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
%           \foreach \dest in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
%               \draw[edge] (H\lastN-\source) -- (H\N-\dest);
              
%     % Connect every node in the hidden layer with the output layer
%     \foreach \source in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
%         \foreach \dest in {1,2,4,5}
%             \draw[dashed] (H\Nhidden-\source) -- (O-\dest);
    

%     \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1*\layersep,-.7*\nodeinlayersep) -- (1*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=1$};
  
%     \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2.5*\layersep,-.7*\nodeinlayersep) -- (2.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,above=0.5cm,black]{Fictitious output layer};

%     \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2.5*\layersep,-.7*\nodeinlayersep) -- (2.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,above=0.1cm,black]{ with parameters $\bphi^a$};
    
%     \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2*\layersep,-.7*\nodeinlayersep) -- (2*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=2$};
%        \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1.5*\layersep,-.7*\nodeinlayersep) -- (1.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=4cm,black]{Original network $\Omega_0$};

%       % \draw [dashed] (5.5,-4.8) -- (5.5,0.5);
%           \draw[->, thick] (3.5,-0.3) -- (3.5,-0.65);
%   \draw[->, thick] (5.8,-0.1) -- (9.5,-0.1) node[pos=0.5,above=0cm,black]{Transport of parameters};
% \end{tikzpicture}
% \hspace{-2 cm}
% \begin{tikzpicture}[
%     node distance=\layersep,
%     edge/.style={-stealth,shorten >=1pt, draw=black!50,thin},
%     neuron/.style={circle,fill=black!25,minimum size=10pt,inner sep=0pt},
%     operator/.style={rectangle,fill=green!,minimum height= \nodeinlayersep, minimum width= 0.8 * \layersep, inner sep=0pt, rounded corners},
%     input neuron/.style={neuron, fill=green!50,minimum size=12pt},
%     output neuron/.style={neuron, fill=green!50,minimum size=12pt},
%     hidden neuron/.style={neuron, fill=blue!50},
%     Forward map/.style={operator, fill=red!50},
%     annot/.style={text width=4em, text centered},
%     every node/.style={scale=1.0},
%     node1/.style={scale=2.0}
% ]
%     % Draw the input layer nodes
%     \foreach \name / \y in {1,...,4}
%         {\ifnum \y=3
%             \node (I-\name) at (0,-\nodeinlayersep * \y - \nodeinlayersep * 0.5) {$\vdots$};
%             % \node[input neuron] (I-\name) at (0,-\y-0.4) {$y_{\y}$};
%         \else
%             \ifnum \y=4
%                 \node[input neuron] (I-\name) at (0,-\nodeinlayersep * \y - \nodeinlayersep * 0.5) {$x_{n}$};
%             \else
%                 \node[input neuron] (I-\name) at (0,-\nodeinlayersep *\y - \nodeinlayersep * 0.5 ) {$x_{\y}$};
%             \fi
%         \fi}
        
%     % Draw the output layer node
%     \foreach \name / \y in {1,...,5}
%         {\ifnum \y=4
%             \node (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.0) {$\vdots$};
      
%             % \node[input neuron] (I-\name) at (0,-\y-0.4) {$y_{\y}$};
%         \else
%             \ifnum \y=5
%                 \node[input neuron] (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.0) {$y_{p}$};
%             \else
%                 \node[input neuron] (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.0) {$y_{\y}$};
%             \fi
%         \fi}
        

    
    
        
    
%     % set number of hidden layers
%     \newcommand \Nhidden{2}
%     % Draw the hidden layer nodes
%     \foreach \N in {1,...,\Nhidden} {
%         \foreach \y in {1,...,5} { %%% MODIFIED (1,...,12 -> 1,...,5, and the next five lines)
%             \ifnum \y=4
%                 \node at (\N*\layersep,-\y*\nodeinlayersep) {$\vdots$};
%             \else
%                 \node[hidden neuron] (H\N-\y) at (\N*\layersep,-\y*\nodeinlayersep ) {$\sigma$};
%         \fi
%       }
%     }

%     %%% <-- MODIFIED (from H\Nhidden-6 to H\Nhidden-3) 
%     % Connect every node in the input layer with every node in the
%     % hidden layer.
%     \foreach \source in {1,2,4}
%         \foreach \dest in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
%             \draw[edge] (I-\source) -- (H1-\dest);
    
%     % connect all hidden stuff
%     \foreach [remember=\N as \lastN (initially 1)] \N in {2,...,\Nhidden}
%       \foreach \source in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
%           \foreach \dest in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
%               \draw[edge] (H\lastN-\source) -- (H\N-\dest);
              
%     % Connect every node in the hidden layer with the output layer
%     \foreach \source in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
%         \foreach \dest in {1,2,4,5}
%             \draw[edge] (H\Nhidden-\source) -- (O-\dest);
    

%     \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1*\layersep,-.7*\nodeinlayersep) -- (1*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=1$};
  
%     \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2.5*\layersep,-.7*\nodeinlayersep) -- (2.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,above=0.5cm,black]{New output layer with };

%     \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2.5*\layersep,-.7*\nodeinlayersep) -- (2.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,above=0.1cm,black]{ parameters $\overline{T}\LRp{\bphi^a}$};
    
%     \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2*\layersep,-.7*\nodeinlayersep) -- (2*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=2$};
%        \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1.5*\layersep,-.7*\nodeinlayersep) -- (1.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=4cm,black]{New network $\Omega_\epsilon$};

%       % \draw [dashed] (5.5,-4.8) -- (5.5,0.5);
%       \draw[->, thick] (3.5,-0.3) -- (3.5,-0.65);

% \end{tikzpicture}
% \caption{Optimal transport interpretation of our proposed approach:  We wish to optimally transport (in some sense) the parameters from network $\Omega_0$ (left figure) to the new network $\Omega_\epsilon$ (right figure).  }
% \label{fig:TRANSPORT}
% \end{figure}





Note that using the Dirac measure,  the functional representation of the loss \eqref{for_later} can be written as:
\begin{equation}
    \sJ(\mu)=\frac{1}{S}\sum_{s=1}^S \Phi \LRp{ \underset{\bphi \sim \mu }{\expect}    \LRs{\Omega_\epsilon (\bx_{s,0};\ \bphi)}}, \quad \quad %\mu=\frac{1}{r}\sum_{i=1}^{r} \delta_{\bphi_i}, 
    \mu=\delta_{\bphi_1^a} \times \delta_{\bphi_2^a}\times \dots \delta_{\bphi_r^a},
    \label{delta_measure}
\end{equation}
where we have introduced the dependence of\krish{$\Omega_\epsilon$ is a function} $\Omega_\epsilon$ (perturbed network) on the input $\bx_{s,0}$ and the parameters $\bphi$ of the added layer, $\delta_{\bphi_i^a}$ denotes the Dirac measure on $\real$ with $\bphi_i^a$ denoting the actual prescribed values on the parameters, $r=$ represents the total number of parameters in the added layer, $\mu$ is the product measure. The most general condition for the added layer to act as a fictitious layer in \cref{fig:TRANSPORT} is as follows:
\begin{equation}
\begin{aligned}
\Omega_\epsilon (\bx_{s,0};\ \bphi^a)&=\Omega_0\LRp{\bx_{s,0}},\quad \forall s \in \{1,\dots S\},\\
\LRp{\bp_{s,l}}_{\Omega_0}&=\LRp{\bp_{s,l}}_{\Omega_\epsilon},\quad \ \ \forall s \in \{1,\dots S\}, \  \mathrm{and} \ l=\{1,2\},\\
\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ \bphi^a)&={\bf{0}},\quad \quad \quad \quad \forall s\in \{1,\dots S\},
\end{aligned}
\label{general_c}
\end{equation}
where $\Omega_0$ is the network with one hidden layer in   \cref{fig:TRANSPORT}) ($l=2$ is it's output layer), $\LRp{\bp_{s,l}}_{\Omega_0}$ denotes the adjoints of  $\Omega_0$ as computed from \eqref{hamiltonian}, and $\LRp{\bp_{s,l}}_{\Omega_\epsilon}$ denotes the adjoints of  $\Omega_\epsilon$ with $\overline{T}=I$ in \cref{fig:TRANSPORT}. 
Note that conditions \eqref{general_c} align with the concept of admissible perturbation introduced in \cref{admissible_pert} where we looked at the case $\bphi^a={\bf{0}}$. Now in order to define the optimization problem \eqref{optimal_transport} associated with our strategy, let us consider the concept of $p-$ Wasserstein metric as follows: 







\begin{definition}[Topological derivative in $p-$ Wasserstein space]
\label{top_was_def}
%Let $\boldsymbol{\phi} \in \real^n$, where $n$ is the total number of parameters in the added  layer. 
Let $\mu$ be a given probability measure  \eqref{delta_measure} such that \eqref{general_c} is satisfied and $\nu={\overline{T}}_{\#}\mu$, be another perturbed probability measure satisfying $ W_p \LRp{\mu;\ \nu}\leq \epsilon$.
We say the loss functional $\sJ$ in \eqref{delta_measure} admits a topological derivative at $\mu$  along $\nu$ if there exists a  function $f: \real^{+}\mapsto \real^+$ with $\lim\limits_{\epsilon \downarrow  0}q(\epsilon)=0$ such that the following limit exists:
\begin{equation}
    d\sJ(\mu; \ \nu)=-\lim_{\epsilon \downarrow 0}\frac{\sJ(\nu)-\sJ(\mu)}{q(\epsilon)},
    \label{topo_der_w}
\end{equation}
%\tanbui{You need to define what "small-oh" notation here.}
\end{definition}
\krish{I tried to make sense of a topological derivative in Wasserstein space. $\nu$ should depend on $\epsilon$ via the constraint and the LHS should then only depend on $\mu$. Adopting this notation hit me with some confusions later on in the proof. I am thinking on this now Tan. But right now I am skeptical if this type of definition makes sense in this context!}

\begin{lemma}[Topological derivative in $p-$ Wasserstein space]
\label{lem_opti}
Let $\mu$ be a given probability measure  \eqref{delta_measure} such that \eqref{general_c} is satisfied. Let $\nu={\overline{T}}_{\#}\mu$, be another (unknown) push forward probability measure  such that $ W_p \LRp{\mu;\ \nu}\leq \epsilon$. Assume that the conditions \ref{cond_one}, \ref{cond_two} of  \cref{exist_th} are satisfied. 
Then, with $q(\epsilon)=\epsilon^2$ the topological derivative in \cref{top_was_def} is given by:
%be any probability measure absolutely continuous with respect to Lebesgue measure such that $ W_p \LRp{\mu,\ \nu}\leq \epsilon$. Assume that the conditions of Theorem \ref{exist_th} are satisfied. Then, for any admissible perturbation given by proposition \ref{prop_admissible} we have:
\begin{equation}
  d\sJ(\mu;\nu)= - \lim_{\epsilon \downarrow  0} \frac{\sJ(\nu)-\sJ(\mu)}{q(\epsilon)} = -\frac{1}{2}\lim_{\epsilon \downarrow  0}\frac{\mathbf{D}^2J(\mu)[\nu]}{q(\epsilon)},
   \label{taylor_1}
\end{equation}
 %\tanbui{what does $[\nu]$ mean in the second variation? Would the second variation require two directions to act on, just like the Hessian requires to act on two vectors to return a number. Maybe you meant something else, which I do not understand. See chapter 23 in my unfinished book that I sent you.}
% \krish{I understand what you mean by Hessian in the context of infinite dimensional newton's method which requires two direction to act. The Hessian in that context depends on the three functions $\mu$, $\nu_1$, $\nu_2$, where $\nu_1,\ \nu_2$ are arbitrary directions (in the specified function space) and $\mu$ is the evaluation point. In the above we have the special choice of direction $\nu_1=\nu_2=\nu$ where we choose the same direction. The main think is that given measures  $\mu,\ \nu$ the Taylor series equation (39) holds and second variation simply depends on evaluation point $\mu$ and direction $\nu$. }
  where, $\mathbf{D}^2J(\mu)[\nu]$ denotes the second variation of the functional $J(\mu)$ computed based on the   displacement interpolation \cite{santambrogio2015optimal, carmona2018probabilistic,wu2019splitting} given by:
\[  \mathbf{D}^2J(\mu)[\nu]=\LRs{\frac{d^2}{d\eta^2}\sJ(\mu_\eta)}_{\eta=0},\ \ \mu_\eta=(\pi_\eta)_{\#}\mu,\]
where $\pi_\eta=(1-\eta)\bphi+\eta\overline{T}(\bphi)$ with $\eta=[0,\ 1]$.
\end{lemma}



\begin{proof}
First, from the Taylor series expansion of the functional $\sJ$ about the measure $\mu$ we have:
\begin{equation}
    \sJ(\nu)-\sJ(\mu)= \mathbf{D}J(\mu)[\nu]+ \frac{1}{2}\mathbf{D}^2J(\mu)[\nu]+\frac{1}{6}\LRs{\frac{d^3}{d\eta^3}\sJ(\mu_\eta)}_{\eta=\zeta},
    \label{variation_taylor}
\end{equation}
where $\zeta$ is a number between $0$ and $1$. Analyzing the first-order term in \eqref{variation_taylor} yields:
\begin{equation}
    \begin{aligned}
  \mathbf{D}J(\mu)[\nu]&=\frac{d}{d\eta} \LRp{\frac{1}{S}\sum_{s=1}^S \Phi \LRp{\underset{\bphi_{\eta} \sim \mu_{\eta} }{\expect}  
  \LRs{\Omega_\epsilon \LRp{\bx_{s,0};\ \bphi_\eta}}}}\Bigg  |_{\eta=0} \\
    &= \frac{d}{d\eta} \LRp{\frac{1}{S}\sum_{s=1}^S \Phi \LRp{\underset{\bphi \sim \mu }{\expect}    
    \LRs{\Omega_\epsilon \LRp{\bx_{s,0};\ \eta \overline{T}(\bphi)+(1-\eta)\bphi}}}}\Bigg  |_{\eta=0}\\
    &= \frac{1}{S}\sum_{s=1}^S \LRs{\nabla \Phi \LRp{\underset{\bphi \sim \mu }{\expect}    
    \LRs{\Omega_\epsilon (\bx_{s,0};\ \bphi)}} \cdot \underset{\bphi \sim \mu }{\expect}    
    \LRs{\LRp{\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ \bphi)} \LRp{\overline{T}(\bphi)-\bphi} } },
\end{aligned}
\label{last_line}
\end{equation}
where $\nabla_{\bphi}$ denotes the gradient w.r.t the second argument of $\Omega_\epsilon$. 
Further  note that the second term in the last line of \eqref{last_line}  can be simplified as:
\[\underset{\bphi \sim \mu }{\expect}\LRs{\LRp{\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ \bphi)} \LRp{\overline{T}(\bphi)-\bphi} } =\underset{\bphi \sim \mu }{\expect}\LRs{(\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ \bphi)) \overline{T}(\bphi)} -\underset{\bphi \sim \mu }{\expect}\LRs{(\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ \bphi)) \bphi }\]

\[=\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ \bphi^a) \overline{T}(\bphi^a) -  {(\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ \bphi^a)) \bphi^a}={\bf{0}},\]
where we have used the property of the Dirac-measure as defined in \eqref{delta_measure}  and also used the assumption
$\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ \bphi^a)={\bf{0}}$ in \cref{lem_opti}.

Proceeding similarly for computing the third-order term in \eqref{variation_taylor}, it can be shown after some algebraic manipulations that \cite{wu2019splitting}:
\[ \Bigg | {\LRs{\frac{d^3}{d\eta^3}\sJ(\mu_\eta)}_{\eta=\zeta}} \Bigg | \leq  c \times  \underset{\bphi \sim \mu }{\expect}\LRs{\norm{\bphi-\overline{T}(\bphi)}_2^3},  \]
%\[ {\LRs{\frac{d^3}{d\eta^3}\sJ[\mu_\eta]}_{\eta=\zeta}}= \mathcal{O} \LRp{\LRp{\underset{\bphi \sim \mu}{\text{esssup}} {\norm{(\bphi- \bar{T}(\bphi))}_2}}^3}=\mathcal{O} \LRp{\LRp{\inf_{\overline{T}_{\#}\mu=\nu}\underset{\bphi \sim \mu}{\text{esssup}} {\norm{(\bphi- \bar{T}(\bphi))}_2}}^3},\]
where we have used  the assumptions in  \cref{exist_th} while simplifying and $c$ is a positive constant. Further, with the constraint $W_p (\mu,\ \nu)\leq  \epsilon$ and the property  that for deterministic distributions, $W_p (\mu,\ \nu)\leq  \epsilon  \implies \norm{\overline{T}(\bphi^a)-\bphi^a}_2\leq \epsilon$, we have:
\begin{equation}
\Bigg | {\LRs{\frac{d^3}{d\eta^3}\sJ(\mu_\eta)}_{\eta=\zeta}} \Bigg | \leq c \times \underset{\bphi \sim \mu }{\expect}
\LRs{\norm{\bphi-\overline{T}(\bphi)}_2^3} \leq c\times \norm{\bphi^a-\overline{T}(\bphi^a)}_2^3 \leq c \epsilon^3,   
\label{third_deriva}
\end{equation}
Now using the result \eqref{third_deriva}, \eqref{last_line}  and choosing $q(\epsilon)=\epsilon^2$, we have from \eqref{variation_taylor}:


\begin{equation}
\begin{aligned}
   \lim_{\epsilon \downarrow  0} \frac{\sJ(\nu)-\sJ(\mu)}{f(\epsilon)}&=  \frac{1}{2} \lim_{\epsilon \downarrow  0}\frac{\mathbf{D}^2J(\mu)[\nu]}{\epsilon^2}+\frac{1}{6} \lim_{\epsilon \downarrow  0} \frac{\LRs{\frac{d^3}{d\eta^3}\sJ(\mu_\eta)}_{\eta=\zeta}}{\epsilon^2},\\
   & =\frac{1}{2} \lim_{\epsilon \downarrow  0}\frac{\mathbf{D}^2J(\mu)[\nu]}{\epsilon^2},
    \end{aligned}
       \label{variation_taylor_deriv}
\end{equation}
thereby concluding the proof.
%used the fact that the transport map $\bar{T}$ is unique. Further, note that
%The unique transport map leads to the optimal transport plan $\gamma=\mu \times \overline{T}_{\#}\mu$ such that:
%since the measures $\mu$ and $\nu$ are absolutely continuous w.r.t the Lebesgue measure and  $p>1$ (in our case $p$). 
%\[\mathcal{O} \LRp{ \LRp{\inf_{\gamma \in \Gamma (\mu,\ \nu)}\underset{(\bphi,\ \bphi') \sim \gamma}{\text{esssup}} {\norm{(\bphi-\bphi')}_2}}^3} = \mathcal{O} \LRp{W_p \LRp{\mu,\ \nu}^3}= \mathcal{O} \LRp{\epsilon^3}\leq \mathcal{O} \LRp{\LRp{\inf_{\overline{T}_{\#}\mu=\nu}\underset{\bphi \sim \mu}{\text{esssup}} {\norm{(\bphi- \bar{T}(\bphi))}_2}}^3}\]
%\[\LRp{\inf_{\overline{T}_{\#}\mu=\nu}\underset{\bphi \sim \mu}{\text{esssup}} {\norm{(\bphi- \bar{T}(\bphi))}_2}}^3= \LRp{\inf_{\gamma \in \Gamma (\mu;\ \nu)}\underset{(\bphi,\ \bphi') \sim \gamma}{\text{esssup}} {\norm{(\bphi-\bphi')}_2}}^3= W_p \LRp{\mu;\ \nu}^3 \leq \epsilon^3, \]
%where  we have applied the constraint $W_p \LRp{\mu;\ \nu} \leq \epsilon$.
\end{proof}


%\begin{remark}{}{}$ $
%\label{rem}
%\begin{enumerate}
  %  \item  In order to construct the loss functional $T(\mu;\ \nu)$ in \eqref{optimal_transport},  we make use of the fact that we need to maximize the decrease in loss $\LRp{\sJ(\mu)-\sJ(\nu)}$ due to insertion of a new layer. Therefore, we look at maximizing the lower bound in \eqref{taylor_1}.
%    \item We will show in  Theorem \ref{steepst_descnt} that any solution pair $(\gamma,\ \nu)$ satisfying the constraint \ref{const}
 %must satisfy $T(\mu,\ \nu)= \sJ(\nu)-\sJ(\mu)+\mathcal{O}(\epsilon^3)$. 
  %  \item For sufficiently small $\epsilon$, one may neglect the higher order terms in \eqref{taylor_1} and set $T(\mu;\ \nu)=\frac{1}{2}\mathbf{D}^2J(\mu)[\nu]$ in \eqref{optimal_transport}.
  %  \end{enumerate}
 
    
   % \[   \sL (\nu)=\sL (\mu)+ \delta \sJ(\mu,\ \nu)+\frac{1}{2} \delta^2 \sJ(\mu,\ \nu)+\frac{1}{6} \nabla^3_{\eta\eta\eta} \sL(\mu_\eta) \Big |_{\eta=\zeta},\]
%\end{remark}

We can now use the results of \cref{lem_opti} to formulate our optimization problem in $p-$ Wasserstein space.





 
\begin{theorem}[Optimization problem in $p-$ Wasserstein space]
\label{steepst_descnt}
Let $\bphi \sim \mu$ where $\mu$ is given by \eqref{delta_measure} and  $\bphi' \sim \nu$, where $\nu=\overline{T}_{\#}\mu$ is an unknown probability measure as defined in  \cref{lem_opti}.  Consider the optimization problem:
\begin{align}
\label{fun} 
&  \max_{\nu \in \mathcal{Y}_p} \mathcal{T}(\mu,\ \nu)=\max_{\nu \in \mathcal{Y}_p }  \LRp{d\sJ(\mu;\nu)},\\  
\label{const}
 & s.t. \quad  W_p \LRp{\mu;\ \nu}\leq \epsilon
\end{align}
Define the matrix $Q(\bphi)$ as:
  \[ Q(\bphi)=\sum_{s=1}^S \nabla^2_{\bphi} H_2\LRp{\bx_{s,2};\ \bp_{s,3}; \ \bphi},\]
where $H_2$ is the Hamiltonian  given by \eqref{hamiltonian} corresponding to the output layer. Let $\Lambda (\bphi)$ be the maximum eigenvalue of $Q(\bphi)$ and $v_{max}(\bphi)$ is the eigenvector of $Q(\bphi)$ corresponding to  $\Lambda (\bphi)$. Assume that the conditions of  \cref{lem_opti} are satisfied and $Q(\bphi^a)$ is positive definite.    Then, the optimal transport map $\overline{T}$ for the problem \eqref{const} satisfies the condition: 
\[ \overline{T}(\bphi^a)= \bphi^a+\epsilon \ v_{max}(\bphi^a),\]
such that:
\[ \max_{\nu \in \mathcal{Y}_p} \mathcal{T}(\mu,\ \nu)= \frac{\Lambda (\bphi^a)}{2} .\]
\end{theorem}



\begin{proof}
Analyzing the second-order term $\mathbf{D}^2J(\mu)[\nu]$ in \eqref{taylor_1}, we have:
\begin{equation}
    \begin{aligned}
 &  \mathbf{D}^2J(\mu)[\nu]= \frac{d}{d\eta} \LRs{\frac{1}{S}\sum_{i=1}^S \LRs{\nabla \Phi \LRp{\underset{\bphi \sim \mu }{\expect}  
 \LRs{\Omega_\epsilon \LRp{\bx_{s,0};\ \pi_\eta}}} \cdot \underset{\bphi \sim \mu }{\expect}  
 \LRs{\LRp{\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ {\pi_\eta)}} g(\bphi) } }}\Bigg  |_{\eta=0}=\\
  & \frac{1}{S}\sum_{s=1}^S \LRs{\LRp{\nabla^2\Phi   
  \LRs{     \underset{\bphi \sim \mu }{\expect}   \LRs{\Omega_\epsilon (\bx_{s,0};\ \bphi)}}\hspace{-0.1 cm}\underset{\bphi \sim \mu }{\expect}   \LRs{\LRp{\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ {\bphi})} (g(\bphi))}} \cdot \hspace{-0.2 cm} \underset{\bphi \sim \mu }{\expect}   
  \LRs{\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ {\bphi}) g(\bphi)} }\\
  &+\sum_{s=1}^S \LRs{  \underset{\bphi \sim \mu }{\expect}   
  \LRs{g(\bphi)^T \ \nabla^2_{\bphi} \ (-\bp_{s,T} \cdot \Omega_\epsilon (\bx_{s,0};\ {\bphi}))\ g(\bphi) }},
  \end{aligned}
  \label{second_order}
\end{equation}
where $g(\bphi)=\overline{T}(\bphi)-\bphi$ and we have used \eqref{set_of} in the second term of \eqref{second_order} while applying the chain rule, i.e,
\[\bp_{s,T}=-\frac{1}{S}\nabla\Phi\LRp{\bx_{s,T}}=
-\frac{1}{S} \nabla\Phi \LRp{\underset{\bphi \sim \mu }{\expect} 
\LRs{\Omega_\epsilon \LRp{\bx_{s,0};\ \bphi}}}.\]
Now note that the first term in \eqref{second_order} vanishes due to the property of the Dirac-measure as defined in \eqref{delta_measure}  and also used the assumption
$\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ \bphi^a)={\bf{0}}$ in  \cref{lem_opti}.
Further, note that from \eqref{hamiltonian} we have: 
  \[ H_2\LRp{\bx_{s,2};\ \bp_{s,3};\ \bphi}=\bp_{s,T}  \cdot \Omega_\epsilon (\bx_{s,0};\ {\bphi})=\bp_{s,T}  \cdot \bff_3\LRp{\bx_{s, 2};\ \bphi}.\]
Using the above result and the definition of $Q(\bphi)$, \eqref{second_order} can be simplified as:
  \[   \mathbf{D}^2J(\mu)[\nu]= -\sum_{s=1}^S \underset{\bphi \sim \mu }{\expect}  
  \LRs{\LRp{\overline{T}(\bphi)-\bphi}^T \ \nabla^2_{\bphi} H_2\LRp{\bx_{s,2};\bp_{s,3};\bphi}\LRp{\overline{T}(\bphi)-\bphi} }\]
  \[=-\underset{\bphi \sim \mu }{\expect}\LRs{\LRp{\overline{T}(\bphi)-\bphi}^T\  Q(\bphi) \ \LRp{\overline{T}(\bphi)-\bphi}}.\]
Therefore, the topological derivative in \cref{taylor_1} can be written as:
\[  d\sJ(\mu;\ \nu)=-\frac{1}{2}\lim_{\epsilon \downarrow  0}\frac{\mathbf{D}^2J(\mu)[\nu]}{\epsilon^2}= \lim_{\epsilon \downarrow  0} \frac{\underset{\bphi \sim \mu }{\expect}\LRs{\LRp{\overline{T}(\bphi)-\bphi}^T\  Q(\bphi) \ \LRp{\overline{T}(\bphi)-\bphi}}}{2\epsilon^2}. \]
Therefore, we have:
\begin{equation}
  d\sJ(\mu;\ \nu)= \lim_{\epsilon \downarrow  0} \frac{\LRp{\overline{T}(\bphi^a)-\bphi^a}^T\  Q(\bphi^a) \ \LRp{\overline{T}(\bphi^a)-\bphi^a}}{2\epsilon^2},   
  \label{top_wass}
\end{equation}
where we have used the property of the Dirac-measure as defined in \eqref{delta_measure} in computing the expectation.
%\[ \max d\sJ(\mu)= \lim_{\epsilon \downarrow  0} \max \frac{\LRp{\overline{T}(\bphi^a)-\bphi^a}^T\  Q(\bphi^a) \ \LRp{\overline{T}(\bphi^a)-\bphi^a}}{2\epsilon^2} \]
Now note that by Cauchy-Schwartz inequality and the consistency of  spectral norm, we have:
\begin{equation}
\frac{(\overline{T}(\bphi^a)-\bphi^a)^T\  Q(\bphi^a) \ (\overline{T}(\bphi^a)-\bphi^a)}{2\epsilon^2}\leq \frac{\norm{\overline{T}(\bphi^a)-\bphi^a}^2_2 \norm{Q(\bphi^a)}_2}{2\epsilon^2} \leq \frac{\epsilon^2\norm{Q(\bphi^a)}_2}{2\epsilon^2} 
\label{mx_att}
 \end{equation}
where we have used the constraint $W_p (\mu,\ \nu)\leq  \epsilon  \implies \norm{\overline{T}(\bphi^a)-\bphi^a}_2\leq \epsilon$. Now from \eqref{top_wass} and \eqref{mx_att} we have:
\begin{equation}
     d\sJ(\mu;\ \nu)= \lim_{\epsilon \downarrow  0} \frac{\LRp{\overline{T}(\bphi^a)-\bphi^a}^T\  Q(\bphi^a) \ \LRp{\overline{T}(\bphi^a)-\bphi^a}}{2\epsilon^2} \leq \frac{\norm{Q(\bphi^a)}_2}{2}.
     \label{top_was_1}
\end{equation}
Now finding $\nu \in \mathcal{Y}_p$ that maximizes $d\sJ(\mu;\ \nu)$ in \eqref{top_was_1} is equivalent to finding the transport map $\overline{T}$ that maximizes \eqref{top_was_1}. Now from \eqref{top_was_1}, the  maximum value of $d\sJ(\mu;\ \nu)$ can be attained when:
\[\frac{(\overline{T}(\bphi^a)-\bphi^a)^T\  Q(\bphi^a) \ (\overline{T}(\bphi^a)-\bphi^a)}{2\epsilon^2}= \frac{\Lambda(\bphi^a)}{2}\]
where $\Lambda (\bphi^a)$ is the maximum eigenvalue of $Q(\bphi^a)$ and we have assumed that $Q(\bphi^a)$ is positive definite. Therefore, the optimal $\overline{T}$ satisfies:
\[ \overline{T}(\bphi^a)= \bphi^a+\epsilon \ v_{max}(\bphi^a),\]
and,
\[ \max_{\nu \in \mathcal{Y}_p }  \LRp{d\sJ(\mu;\nu)}= \frac{\Lambda(\bphi^a)}{2}.\]
This concludes the proof.
\end{proof}
\begin{remark}
\label{comp_sens}
Note that choosing residual connections and constructing activation function with property $\sigma(0)=0,$ and $\sigma'(0)=0$ (\cref{act_const}) is a particular case that satisfies the conditions in  \cref{lem_opti} and \cref{steepst_descnt}. For this case, we have chosen $\bphi^a={\bf{0}}$.
%Note that the topological derivative (\cref{exist_th}) provides us with information on the sensitivity of the loss functional $\sJ$ due to an admissible perturbation. Based on \cref{exist_th}, \cref{deriv_algo} identifies the most sensitive adaptation strategy (admissible perturbation). On the other hand, the optimal transport viewpoint presented in \cref{optimal_trans} sets up a constrained optimization problem in $p-$ Wasserstein space to recover our layer adaptation strategy.
\end{remark}  
\end{section}
\end{comment}


\begin{section}{Topological derivative approach through the lens of an  optimal transport problem}
\label{optimal_trans}
%\krish{The below section completely revised to derive the topological derivative in $p-$Wasserstein space and the optimization problem (Theorem C.2).}
In this section, we show that our layer insertion strategy can be derived  as a solution to maximizing a topological derivative in $p$-Wasserstein space, where $p\geq 1$. Optimization in $\infty-$Wasserstein space was earlier considered by Liu et al. \cite{wu2019splitting} to explain their neuron splitting strategy for growing neural networks along the width. The standard optimal transport (Kantorovich formulation \cite{santambrogio2015optimal}) aims at computing a minimal cost transportation plan $\gamma$  between a source probability measure $\mu$, and target probability measure $\nu$.
%Note that the plan $\gamma$ is a joint probability measure whose marginals are $\mu$ and $\nu$ on the first and second factors, respectively.
A common formulation of the cost function is based on the Wasserstein distance  $W_p (\mu;\ \nu)$.
\begin{definition}[$p$-Wasserstein metric]
The  $p$-Wasserstein distance between two probability measures $\mu$ and $\nu$ is given as:
\begin{equation}
    W_p \LRp{\mu;\ \nu}={\inf_{\gamma \in \Gamma (\mu;\ \nu)} \LRp{\underset{(\bphi,\ \bphi') \sim \gamma}{\expect}  \norm{\bphi-\bphi'}_2^p}^{1/p}},
    \label{wasr}
\end{equation}
    where  $\Gamma (\mu;\ \nu)$ is the set of couplings of $\mu$ and $\nu$. A coupling $\gamma$ is a joint probability measure whose marginals are $\mu$ and $\nu$ on the first and second factors, respectively. 
    %$esssup$ denotes the essential supremum \footnote{essential supremum denotes the smallest number $c$ such that the set $\{ (\bphi,\ \bphi'): \norm{\bphi-\bphi'}_2 >c\}$ has zero probability under $\gamma$. } of $\norm{\bphi-\bphi'}_2$ with respect to measure $\gamma$.
    

\end{definition}
In this work, {\em we set up an optimization problem  to determine the unknown target measure $\nu$ given the source measure $\mu$.} Let us consider the scenario depicted in \cref{fig:TRANSPORT} in which we add a message-passing layer  with zero weights/biases, and then ask the question:  what is the initialization values $\overline{T}({\bf{0}})$ (where $\overline{T}$ is the transport map) for the weights/biases of the newly added layer such that the loss function is most sensitive too? In this section, we answer this question from an optimal transport point of view.  For the clarity of the exposition (and without loss of generality), let us assume that the message-passing layer (new layer) is added in front of the output layer of $\Omega_0$ as depicted in 
\cref{fig:TRANSPORT}.
\def\layersep{1.4cm}
\def\nodeinlayersep{0.7cm}
\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=\layersep,
    edge/.style={-stealth,shorten >=1pt, draw=black!50,thin},
    neuron/.style={circle,fill=black!25,minimum size=10pt,inner sep=0pt},
    operator/.style={rectangle,fill=green!,minimum height= \nodeinlayersep, minimum width= 0.8 * \layersep, inner sep=0pt, rounded corners},
    input neuron/.style={neuron, fill=green!50,minimum size=12pt},
    output neuron/.style={neuron, fill=green!50,minimum size=12pt},
    hidden neuron/.style={neuron, fill=blue!50},
    Forward map/.style={operator, fill=red!50},
    annot/.style={text width=4em, text centered},
    every node/.style={scale=1.0},
    node1/.style={scale=2.0}
]
    % Draw the input layer nodes
    \foreach \name / \y in {1,...,4}
        {\ifnum \y=3
            \node (I-\name) at (0,-\nodeinlayersep * \y - \nodeinlayersep * 0.5) {$\vdots$};
            % \node[input neuron] (I-\name) at (0,-\y-0.4) {$y_{\y}$};
        \else
            \ifnum \y=4
                \node[input neuron] (I-\name) at (0,-\nodeinlayersep * \y - \nodeinlayersep * 0.5) {$x_{n}$};
            \else
                \node[input neuron] (I-\name) at (0,-\nodeinlayersep *\y - \nodeinlayersep * 0.5 ) {$x_{\y}$};
            \fi
        \fi}
        
    % Draw the output layer node
    \foreach \name / \y in {1,...,5}
        {\ifnum \y=4
            \node (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.0) {$\vdots$};
      
            % \node[input neuron] (I-\name) at (0,-\y-0.4) {$y_{\y}$};
        \else
            \ifnum \y=5
                \node[input neuron] (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.0) {$y_{p}$};
            \else
                \node[input neuron] (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.0) {$y_{\y}$};
            \fi
        \fi}
        

    
    
        
    
    % set number of hidden layers
    \newcommand \Nhidden{2}
    % Draw the hidden layer nodes
    \foreach \N in {1,...,\Nhidden} {
        \foreach \y in {1,...,5} { %%% MODIFIED (1,...,12 -> 1,...,5, and the next five lines)
            \ifnum \y=4
                \node at (\N*\layersep,-\y*\nodeinlayersep) {$\vdots$};
            \else
                \node[hidden neuron] (H\N-\y) at (\N*\layersep,-\y*\nodeinlayersep ) {$\sigma$};
        \fi
      }
    }

    %%% <-- MODIFIED (from H\Nhidden-6 to H\Nhidden-3) 
    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,2,4}
        \foreach \dest in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
            \draw[edge] (I-\source) -- (H1-\dest);
    
    % connect all hidden stuff
    \foreach [remember=\N as \lastN (initially 1)] \N in {2,...,\Nhidden}
      \foreach \source in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
          \foreach \dest in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
              \draw[edge] (H\lastN-\source) -- (H\N-\dest);
              
    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
        \foreach \dest in {1,2,4,5}
            \draw[dashed] (H\Nhidden-\source) -- (O-\dest);
    

    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1*\layersep,-.7*\nodeinlayersep) -- (1*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=1$};
  
    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2.5*\layersep,-.7*\nodeinlayersep) -- (2.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,above=0.5cm,black]{message-pasing layer};

    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2.5*\layersep,-.7*\nodeinlayersep) -- (2.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,above=0.1cm,black]{ with parameters ${\bf{0}}$};
    
    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2*\layersep,-.7*\nodeinlayersep) -- (2*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=2$};
       \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1.5*\layersep,-.7*\nodeinlayersep) -- (1.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=4cm,black]{Original network $\Omega_0$};

      % \draw [dashed] (5.5,-4.8) -- (5.5,0.5);
          \draw[->, thick] (3.5,-0.3) -- (3.5,-0.65);
  \draw[->, thick] (5.8,-0.1) -- (9.5,-0.1) node[pos=0.5,above=0cm,black]{Transport of parameters};
\end{tikzpicture}
\hspace{-2 cm}
\begin{tikzpicture}[
    node distance=\layersep,
    edge/.style={-stealth,shorten >=1pt, draw=black!50,thin},
    neuron/.style={circle,fill=black!25,minimum size=10pt,inner sep=0pt},
    operator/.style={rectangle,fill=green!,minimum height= \nodeinlayersep, minimum width= 0.8 * \layersep, inner sep=0pt, rounded corners},
    input neuron/.style={neuron, fill=green!50,minimum size=12pt},
    output neuron/.style={neuron, fill=green!50,minimum size=12pt},
    hidden neuron/.style={neuron, fill=blue!50},
    Forward map/.style={operator, fill=red!50},
    annot/.style={text width=4em, text centered},
    every node/.style={scale=1.0},
    node1/.style={scale=2.0}
]
    % Draw the input layer nodes
    \foreach \name / \y in {1,...,4}
        {\ifnum \y=3
            \node (I-\name) at (0,-\nodeinlayersep * \y - \nodeinlayersep * 0.5) {$\vdots$};
            % \node[input neuron] (I-\name) at (0,-\y-0.4) {$y_{\y}$};
        \else
            \ifnum \y=4
                \node[input neuron] (I-\name) at (0,-\nodeinlayersep * \y - \nodeinlayersep * 0.5) {$x_{n}$};
            \else
                \node[input neuron] (I-\name) at (0,-\nodeinlayersep *\y - \nodeinlayersep * 0.5 ) {$x_{\y}$};
            \fi
        \fi}
        
    % Draw the output layer node
    \foreach \name / \y in {1,...,5}
        {\ifnum \y=4
            \node (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.0) {$\vdots$};
      
            % \node[input neuron] (I-\name) at (0,-\y-0.4) {$y_{\y}$};
        \else
            \ifnum \y=5
                \node[input neuron] (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.0) {$y_{p}$};
            \else
                \node[input neuron] (O-\name) at (3*\layersep,-\nodeinlayersep *\y - \nodeinlayersep * 0.0) {$y_{\y}$};
            \fi
        \fi}
        

    
    
        
    
    % set number of hidden layers
    \newcommand \Nhidden{2}
    % Draw the hidden layer nodes
    \foreach \N in {1,...,\Nhidden} {
        \foreach \y in {1,...,5} { %%% MODIFIED (1,...,12 -> 1,...,5, and the next five lines)
            \ifnum \y=4
                \node at (\N*\layersep,-\y*\nodeinlayersep) {$\vdots$};
            \else
                \node[hidden neuron] (H\N-\y) at (\N*\layersep,-\y*\nodeinlayersep ) {$\sigma$};
        \fi
      }
    }

    %%% <-- MODIFIED (from H\Nhidden-6 to H\Nhidden-3) 
    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,2,4}
        \foreach \dest in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
            \draw[edge] (I-\source) -- (H1-\dest);
    
    % connect all hidden stuff
    \foreach [remember=\N as \lastN (initially 1)] \N in {2,...,\Nhidden}
      \foreach \source in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
          \foreach \dest in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
              \draw[edge] (H\lastN-\source) -- (H\N-\dest);
              
    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,3,5} %%% <-- MODIFIED (1,...,12 -> 1...,3,5)
        \foreach \dest in {1,2,4,5}
            \draw[edge] (H\Nhidden-\source) -- (O-\dest);
    

    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1*\layersep,-.7*\nodeinlayersep) -- (1*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=1$};
  
    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2.5*\layersep,-.7*\nodeinlayersep) -- (2.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,above=0.5cm,black]{New output layer with };

    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2.5*\layersep,-.7*\nodeinlayersep) -- (2.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,above=0.1cm,black]{ parameters $\overline{T}\LRp{{\bf{0}}}$};
    
    \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (2*\layersep,-.7*\nodeinlayersep) -- (2*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=3.5cm,black]{$l=2$};
       \draw [decorate, decoration = {calligraphic brace, mirror}, thick]  (1.5*\layersep,-.7*\nodeinlayersep) -- (1.5*\layersep,-.7*\nodeinlayersep) node[pos=1.5,below=4cm,black]{New network $\Omega_\epsilon$};

      % \draw [dashed] (5.5,-4.8) -- (5.5,0.5);
      \draw[->, thick] (3.5,-0.3) -- (3.5,-0.65);

\end{tikzpicture}
\caption{Optimal transport interpretation of our proposed approach:  We wish to optimally transport (in some sense) the parameters from network $\Omega_0$ (left figure) to the new network $\Omega_\epsilon$ (right figure).  }
\label{fig:TRANSPORT}
\end{figure}
For the message-passing layer with zero weights/biases, the functional representation of the loss \eqref{for_later} can be written as: 
\begin{equation}
\begin{aligned}
    \sJ\LRp{\mu_{\bf{0}}}=\frac{1}{S}\sum_{s=1}^S \Phi \LRp{ \underset{\bphi \sim \mu_{\bf{0}} }{\expect}    \LRs{\Omega_\epsilon \LRp{\bx_{s,0};\ \bphi}}}, \quad \quad %\mu=\frac{1}{r}\sum_{i=1}^{r} \delta_{\bphi_i}, 
    \mu_{\bf{0}}=\delta_{0} \times \delta_{0}\times \dots \mathrm(n_p\ times)\dots \times \delta_{0},
    \end{aligned}
    \label{delta_measure}
\end{equation}
where the second argument in  $\Omega_\epsilon \LRp{\bx_{s,0};\ \bphi}$ denotes the parameters $\bphi$ of the added layer, $\delta_{0}$ denotes the Dirac measure on $\real$, $n_p$ denotes the total number of parameters in the added layer, $\mu_{\bf{0}}$ is the product measure. Similarly, for some non-zero initialization values $\epsilon\boldsymbol{\phi}$ (which we seek to determine) for the weights/biases of the newly added layer, the functional representation of the loss \eqref{for_later} can be written as: 
\begin{equation}
\begin{aligned}
    \sJ(\nu_{\epsilon\bphi})=\frac{1}{S}\sum_{s=1}^S \Phi \LRp{ \underset{\bphi \sim \nu_{\epsilon\bphi} }{\expect}    \LRs{\Omega_\epsilon \LRp{\bx_{s,0};\ \bphi}}}, \quad \quad %\mu=\frac{1}{r}\sum_{i=1}^{r} \delta_{\bphi_i}, 
    \nu_{\epsilon\bphi}=\delta_{\epsilon\bphi_1} \times \delta_{\epsilon\bphi_2}\times \dots \times \delta_{\epsilon\bphi_{n_p}},
    \end{aligned}
    \label{delta_measure_v}
\end{equation}







\begin{theorem}[Topological derivative in $p-$Wasserstein space]
\label{lem_opti}
Let $\mu_{\bf{0}}$ be a given probability measure  \eqref{delta_measure} and assume that conditions \cref{one} and, \cref{two} in \cref{prop_admissible} are satisfied for the neural network. Let $\nu_{\epsilon\bphi}$, be another (unknown) perturbed measure \eqref{delta_measure_v} such that $ W_p \LRp{\mu_{\bf{0}};\ \nu_{\epsilon\bphi}}\leq \epsilon$. Further, assume that the conditions \ref{cond_one}, \ref{cond_two} of  \cref{exist_th} are satisfied. 
Then, the following results holds:
\begin{enumerate}
\item The probability measure $\nu_{\epsilon\bphi}$  has the form $\nu_{\epsilon\bphi}=\overline{T}_{\#}\mu$ where the transport map $\overline{T}$ satisfies:
\begin{equation}
    \overline{T}({\bf{0}})=\epsilon\bphi, \quad \mathrm{where}\ \norm{\bphi}_2\leq 1,
    \label{transp}
\end{equation}
    \item With $ W_p \LRp{\mu_{\bf{0}};\ \nu_{\epsilon\bphi}}\leq \epsilon$, the Topological derivative in $p-$Wasserstein space at $\mu_{\bf{0}}$ along the direction $\bphi$ is given by: 
    \begin{equation}
    d\sJ(\mu_{\bf{0}};\ \bphi)=-\lim_{\epsilon \downarrow 0}\frac{\sJ(\nu_{\epsilon\bphi})-\sJ(\mu_{\bf{0}})}{\epsilon^2}= \frac{\bphi^TQ(\bf{0})\bphi}{2},\quad \norm{\bphi}_2\leq 1,
    \label{topo_der_w}
\end{equation}
where  \[ Q(\bphi)=\sum_{s=1}^S \nabla^2_{\bphi} H_2\LRp{\bx_{s,2};\ \bp_{s,2}; \ \bphi},\]
and $H_2$ is the Hamiltonian  given by \eqref{hamiltonian} corresponding to the output layer. 
 %where,  $\mathbf{D}^2J(\mu)[\nu]$ denotes the second variation of the functional $J(\mu)$ computed based on the   displacement interpolation \cite{santambrogio2015optimal, carmona2018probabilistic,wu2019splitting} given by:
%\[  \mathbf{D}^2J(\mu)[\nu]=\LRs{\frac{d^2}{d\eta^2}\sJ(\mu_\eta)}_{\eta=0},\ \ \mu_\eta=(\pi_\eta)_{\#}\mu,\]
%where $\pi_\eta=(1-\eta)\bphi+\eta\overline{T}(\bphi)$ with $\eta=[0,\ 1]$.
\item Consider the problem of maximizing the Topological derivative in $p-$Wasserstein space:
\[ \max_{\bphi} \LRp{d\sJ(\mu_{\bf{0}};\ \bphi)},\quad \text{subject to:} \norm{\bphi}_2\leq 1.\]
Then the optimal transport map in \eqref{transp} satisfies the following:
\[ \overline{T}({\bf{0}})= \epsilon \ v_{max}({\bf{0}}),\]
where $v_{max}({\bf{0}})$ is the eigenvector of $Q({\bf{0}})$ corresponding to  the maximum eigenvalue. 
\end{enumerate}


\end{theorem}



\begin{proof}
\begin{inlineenum}
    \item The first part of the proof follows from the fact that for Dirac measures $\mu_{\bf{0}},\ \nu_{\epsilon\bphi}$, $p-$Wasserstein distance  \cref{wasr} simplifies as:
\[W_p \LRp{\mu_{\bf{0}};\ \nu_{\epsilon\bphi}}=\norm{\overline{T}({\bf{0}})-{\bf{0}}}_2.\]
Therefore,
\[W_p \LRp{\mu_{\bf{0}};\ \nu_{\epsilon\bphi}}\leq  \epsilon  \implies \norm{\overline{T}({\bf{0}})-{\bf{0}}}_2\leq \epsilon\implies \overline{T}({\bf{0}})=\epsilon\bphi, \quad \norm{\bphi}_2\leq 1. \]
\item To derive the Topological derivative in $p-$Wasserstein space, we look at  the Taylor series expansion of the functional $\sJ$ about the measure $\mu_{\bf{0}}$ as follows:
\begin{equation}
    \sJ(\nu_{\epsilon\bphi})-\sJ(\mu_{\bf{0}})= \mathbf{D}J(\mu_{\bf{0}})[\nu_{\epsilon\bphi}]+ \frac{1}{2}\mathbf{D}^2J(\mu_{\bf{0}})[\nu_{\epsilon\bphi}]+\frac{1}{6}\LRs{\frac{d^3}{d\eta^3}\sJ(\mu_\eta)}_{\eta=\zeta},
    \label{variation_taylor}
\end{equation}
where $\zeta$ is a number between $0$ and $1$, $\mathbf{D}^kJ(\mu_{\bf{0}})[\nu_{\epsilon\bphi}]$ denotes the $k^{th}$ variation of the functional $J(\mu_{\bf{0}})$ computed based on the   displacement interpolation \cite{santambrogio2015optimal, carmona2018probabilistic,wu2019splitting} given by:
\[  \mathbf{D}^kJ(\mu_{\bf{0}})[\nu_{\epsilon\bphi}]=\LRs{\frac{d^k}{d\eta^k}\sJ(\mu_\eta)}_{\eta=0},\ \ \mu_\eta=(\pi_\eta)_{\#}\mu_{\bf{0}},\]
where $\pi_\eta=(1-\eta)\bphi+\eta\overline{T}(\bphi)$ with $\eta=[0,\ 1]$ (note that $\mu_0=\mu_{{\bf{0}}}$ and $\mu_1=\nu_{\epsilon\bphi}$). Analyzing the first-order term in \eqref{variation_taylor} yields:
\begin{equation}
    \begin{aligned}
  \mathbf{D}J(\mu_{\bf{0}})[\nu_{\epsilon\bphi}]&=\frac{d}{d\eta} \LRp{\frac{1}{S}\sum_{s=1}^S \Phi \LRp{\underset{\bphi_{\eta} \sim \mu_{\eta} }{\expect}  
  \LRs{\Omega_\epsilon \LRp{\bx_{s,0};\ \bphi_\eta}}}}\Bigg  |_{\eta=0} \\
    &= \frac{d}{d\eta} \LRp{\frac{1}{S}\sum_{s=1}^S \Phi \LRp{\underset{\bphi \sim \mu_{\bf{0}} }{\expect}    
    \LRs{\Omega_\epsilon \LRp{\bx_{s,0};\ \eta \overline{T}(\bphi)+(1-\eta)\bphi}}}}\Bigg  |_{\eta=0}\\
    &= \frac{1}{S}\sum_{s=1}^S \LRs{\nabla \Phi \LRp{\underset{\bphi \sim \mu_{\bf{0}} }{\expect}    
    \LRs{\Omega_\epsilon (\bx_{s,0};\ \bphi)}} \cdot \underset{\bphi \sim \mu_{\bf{0}} }{\expect}    
    \LRs{\LRp{\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ \bphi)} \LRp{\overline{T}(\bphi)-\bphi} } },
\end{aligned}
\label{last_line}
\end{equation}
where $\nabla_{\bphi}$ denotes the gradient w.r.t the second argument of $\Omega_{\epsilon}$. 
Further  note that the second term in the last line of \eqref{last_line}  can be simplified as:
\[\underset{\bphi \sim \mu_{\bf{0}} }{\expect}\LRs{\LRp{\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ \bphi)} \LRp{\overline{T}(\bphi)-\bphi} } =\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ {\bf{0}}) \overline{T}({\bf{0}}) -  {(\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ {\bf{0}})) {\bf{0}}}={\bf{0}},\]
%\[=\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ {\bf{0}}) \overline{T}({\bf{0}}) -  {(\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ {\bf{0}})) {\bf{0}}}={\bf{0}},\]
where we have used the property of the Dirac-measure as defined in \eqref{delta_measure}  and also used 
$\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ {\bf{0}})={\bf{0}}$ due to
to assumption (\cref{two}) in  \cref{lem_opti}. Proceeding similarly for computing the second-order term $\mathbf{D}^2J(\mu_{\bf{0}})[\nu_{\epsilon\bphi}]$ in \eqref{variation_taylor}, we have:
\begin{equation}
    \begin{aligned}
 &  \mathbf{D}^2J(\mu_{\bf{0}})[\nu_{\epsilon\bphi}]= \frac{d}{d\eta} \LRs{\frac{1}{S}\sum_{i=1}^S \LRs{\nabla \Phi \LRp{\underset{\bphi \sim \mu_{\bf{0}} }{\expect}  
 \LRs{\Omega_\epsilon \LRp{\bx_{s,0};\ \pi_\eta}}} \cdot \underset{\bphi \sim \mu_{\bf{0}} }{\expect}  
 \LRs{\LRp{\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ {\pi_\eta)}} g(\bphi) } }}\Bigg  |_{\eta=0}=\\
  & \frac{1}{S}\sum_{s=1}^S \LRs{\LRp{\nabla^2\Phi   
  \LRs{     \underset{\bphi \sim \mu_{\bf{0}} }{\expect}   \LRs{\Omega_\epsilon (\bx_{s,0};\ \bphi)}}\hspace{-0.1 cm}\underset{\bphi \sim \mu_{\bf{0}} }{\expect}   \LRs{\LRp{\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ {\bphi})} (g(\bphi))}} \cdot \hspace{-0.2 cm} \underset{\bphi \sim \mu_{\bf{0}} }{\expect}   
  \LRs{\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ {\bphi}) g(\bphi)} }\\
  &+\sum_{s=1}^S \LRs{  \underset{\bphi \sim \mu_{\bf{0}} }{\expect}   
  \LRs{g(\bphi)^T \ \nabla^2_{\bphi} \ (-\bp_{s,T} \cdot \Omega_\epsilon (\bx_{s,0};\ {\bphi}))\ g(\bphi) }},
  \end{aligned}
  \label{second_order}
\end{equation}
where $g(\bphi)=\overline{T}(\bphi)-\bphi$ and we have used \eqref{set_of} in the second term of \eqref{second_order} while applying the chain rule, i.e,
\[\bp_{s,T}=-\frac{1}{S}\nabla\Phi\LRp{\bx_{s,T}}=
-\frac{1}{S} \nabla\Phi \LRp{\underset{\bphi \sim \mu_{\bf{0}} }{\expect} 
\LRs{\Omega_\epsilon \LRp{\bx_{s,0};\ \bphi}}}.\]
Now note that the first term in \eqref{second_order} vanishes due to the property of the Dirac-measure as defined in \eqref{delta_measure}  and also used 
$\nabla_{\bphi} \ \Omega_\epsilon (\bx_{s,0};\ {\bf{0}})={\bf{0}}$ due to assumptions (\cref{two}) in  \cref{lem_opti}.
Further, note that from \eqref{hamiltonian} we have: 
  \[ H_2\LRp{\bx_{s,2};\ \bp_{s,2};\ \bphi}=\bp_{s,T}  \cdot \Omega_\epsilon (\bx_{s,0};\ {\bphi})=\bp_{s,T}  \cdot \bff_3\LRp{\bx_{s, 2};\ \bphi}.\]
where we have used $\bp_{s,2}=\bp_{s,3}=\bp_{s,T}$ since the added layer with zero weights and biases acts as a message-passing layer due to \cref{prop_admissible}.  Using the above result and the definition of $Q(\bphi)$, \eqref{second_order} can be simplified as:
  \[   \mathbf{D}^2J(\mu_{\bf{0}})[\nu_{\epsilon\bphi}]= -\sum_{s=1}^S \underset{\bphi \sim \mu_{\bf{0}} }{\expect}  
  \LRs{\LRp{\overline{T}(\bphi)-\bphi}^T \ \nabla^2_{\bphi} H_2\LRp{\bx_{s,2};\bp_{s,2};\bphi}\LRp{\overline{T}(\bphi)-\bphi} }\]
  \[=-\underset{\bphi \sim \mu_{\bf{0}} }{\expect}\LRs{\LRp{\overline{T}(\bphi)-\bphi}^T\  Q(\bphi) \ \LRp{\overline{T}(\bphi)-\bphi}}=-{\overline{T}({\bf{0}})}^T\  Q({\bf{0}}) \ {\overline{T}({\bf{0}})},\]
where we used the property of Dirac measure in the last line. Now using \eqref{transp}, the expression for the second variation above can be further simplified as:
\begin{equation}
    \mathbf{D}^2J(\mu_{\bf{0}})[\nu_{\epsilon\bphi}]=-\epsilon^2\bphi^T  Q({\bf{0}})\bphi,\quad \mathrm{where}\ \norm{\bphi}_2\leq 1.
    \label{second_var_simplif}
    \end{equation}
Proceeding similarly for computing the third-order term in \eqref{variation_taylor}, it can be shown after some algebraic manipulations that \cite{wu2019splitting}:
\begin{equation}
{\LRs{\frac{d^3}{d\eta^3}\sJ(\mu_\eta)}_{\eta=\zeta}}= \mathcal{O} \LRp{\underset{\bphi \sim \mu_{\bf{0}} }{\expect}\LRs{\norm{\bphi-\overline{T}(\bphi)}_2^3}}=\mathcal{O} \LRp{\norm{{\bf{0}}-\overline{T}({\bf{0}})}_2^3} = \mathcal{O} \LRp{\epsilon^3},
\label{c_10}
\end{equation}
where we have used  the assumptions in  \cref{exist_th} while simplifying and used the form of transport map \eqref{transp}.
Substituting results \eqref{last_line}, \eqref{second_var_simplif}, \eqref{c_10} in \eqref{variation_taylor} we have:
\[  \sJ(\nu_{\epsilon\bphi})-\sJ(\mu_{\bf{0}})= -\frac{\epsilon^2\bphi^T  Q({\bf{0}})\bphi}{2}  +\frac{1}{6} \mathcal{O} \LRp{\epsilon^3}, \quad \mathrm{where}\ \norm{\bphi}_2\leq 1.\]
Therefore, the Topological derivative in $p-$Wasserstein space at $\mu_{\bf{0}}$ along the direction $\bphi$ is given by: 
   \[ d\sJ(\mu_{\bf{0}};\ \bphi)=-\lim_{\epsilon \downarrow 0}\frac{\sJ(\nu_{\epsilon\bphi})-\sJ(\mu_{\bf{0}})}{\epsilon^2}= \frac{\bphi^TQ(\bf{0})\bphi}{2},\quad \mathrm{where}\ \norm{\bphi}_2\leq 1.\]
\item Finally maximizing the Topological derivative \eqref{topo_der_w}  yields:
\begin{equation}
     \max_{\bphi} \LRp{d\sJ(\mu_{\bf{0}};\ \bphi)}= \max_{\bphi}\LRp{\frac{\bphi^TQ(\bf{0})\bphi}{2}},\quad \mathrm{subject \ to}\ \norm{\bphi}_2\leq 1.
     \label{optim_was}
     \end{equation}
The optimal solution $\bphi^*$ to \eqref{optim_was} is clearly given by:
\[ \bphi^*=v_{max}({\bf{0}}),\]
where $v_{max}({\bf{0}})$ is the eigenvector of $Q({\bf{0}})$ corresponding to  the maximum eigenvalue. Therefore, from \eqref{transp} the optimal transport map satisfies:
\[  \overline{T}({\bf{0}})=\epsilon\bphi^*=\epsilon v_{max}({\bf{0}}),\]
thereby concluding the proof.
\end{inlineenum}




\end{proof}







 




\end{section}


\begin{section}{General setting for numerical experiments}{}
   \label{hyper_parameter}
All codes were written in PyTorch. Throughout the study, we have employed the Adam optimizer \cite{kingma2014adam} for minimizing the loss function.      Our proposed approach is compared with a number of different approaches as given below: 
\begin{equation*}
 \begin{aligned}
\text{Proposed  (I)}&: \text{Semi-automated architecture adaptation based on the }\\
& \text{\ \ \ topological  derivative approach as described in  }\\
& \text{\ \ \ \cref{Algo_full}, \cref{Algo_full_CNN}}.\\
\text{Proposed  (II)}&: \text{Automated architecture adaptation based on the }\\
& \text{\ \ \ topological derivative approach as described in }\\
& \text{\ \ \ \cref{automated} and \cref{Algo_full_auto}}.\\
\text{Random layer insertion (I)}&: \text{Adaptation strategy based on inserting a new layer at }\\
& \text{\ \ \ random position initialized with $\epsilon \Phi$ where $\Phi$ is a random }\\
& \text{\ \ \ unit vector}.\\
\text{ Net2DeeperNet  (II) }&: \text{Increasing depth of network based on  based on function }\\
& \text{\ \ \  preserving transformations. A layer is inserted at random  }\\
& \text{\ \ \ position with a small Gaussian noise added to the }\\
& \text{\ \ \ parameters to break symmetry \cite{chen2015net2net}}.\\
\text{ Baseline network (B)}&: \text{Training a randomly initialized network with the same }\\
& \text{\ \ \ final architecture as obtained by our proposed approach}.\\
\text{Forward Thinking (H)}&: \text{Algorithm for layerwise adaptation proposed by}\\
& \text{\ \ \  Hettinger et al. \cite{hettinger2017forward}}.\\
\text{Neural Architecture Search (NAS)}&: \text{Random search with early stopping proposed }\\
& \text{\ \ \  in \cite{li2020random,li2020system}}.\\
\end{aligned}  
\end{equation*}
We maintain the same activation functions and hyperparameters for all the adaptation strategies in order to make a fair comparison (except Proposed (II) which does not have a predefined scheduler (see  \cref{automated})). Note that the strategies Proposed (I), Random layer insertion (I), and Net2DeeperNet (II) differ only in the way a newly added layer is initialized and where a new layer is inserted. Note that the uncertainty in each approach is quantified (presented in the numerical results) by running the algorithm for different random initializations (100 in this case). For all problems we consider $\sF=\{ Swish,\ tanh\}$ (see  \cref{act_const}) for constructing the activation function $\sigma(x)$. Further, for neural architecture search \cite{li2020random,li2020system}, the search space is defined by considering architectures with a maximum of $N_n$ layers with each layer having a maximum of $n$ neurons (refer to \cref{hyper_parameter_n} for the values of $N_n$ and $n$ for each problem). We randomly sample a total of $10,000$ architectures in the beginning and follow the procedure in \cite{li2020random,li2020system} to arrive at the best architecture.
\end{section}


\begin{section}{Backtracking algorithm}
\label{back_tr}
The backtracking line search for choosing parameter $\epsilon$ is provided in \cref{back_track}. Note that lines 1-4 in \cref{back_track} try to find the $\epsilon$ that leads to a maximum decrease in loss $\sJ$. 
%However,  for an initially chosen $\epsilon$,  if $\sJ(\epsilon \Phi_l)> \sJ(\bf{0})$, then one need to decrease $\epsilon$ until the loss decreases. \krish{remove! }This is achieved in Lines 6-9 which shows the backtracking line search to find $\epsilon$ that fulfills the Armijo-Goldstein condition \cite{armijo1966minimization}. 
The values of $\tau_1$ and $\epsilon$ used is provided in   \cref{hyper_parameter_n}.
    \begin{algorithm} 
	\caption{Backtracking line search}
	\hspace*{\algorithmicindent} \textbf{Input}: Loss function $\sJ$, initialization of the added layer $\Phi_l$,  parameter $\tau_1$, $\epsilon$.\\
	%\hspace*{\algorithmicindent} \textbf{Initialize}:   Initialize  $\epsilon=\epsilon_0$.\\
	\begin{algorithmic}[1] 
   \While{$\sJ((\epsilon+\tau_1) \Phi_l)\leq \sJ(\epsilon \Phi_l)$} 
   \State $\epsilon=\epsilon+\tau_1$
   \EndWhile
	\end{algorithmic} \label{back_track}
\hspace*{\algorithmicindent} \textbf{Output}: Return $\epsilon$ as the solution.
\end{algorithm}

\end{section}


\begin{section}{Additional numerical results}
\label{additional_numerical_res}


\subsection{Learning the observable to parameter map for 2D heat equation}
\label{poisson_supply}

\cref{mesh_details} describes the domain $\Omega$ along with the fixed locations $\bx_i$ in \cref{poisson_sec}.

\begin{figure}[h!]      
    \centering
  \begin{tabular}{c}
  
      \begin{tabular}{c}

          \hspace{0.1 cm}
              \includegraphics[scale=0.18]{Figures/Poisson/domain.pdf}
        
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}
 \begin{tabular}{c}

          \hspace{0.8 cm}
           \includegraphics[scale=0.7]{Figures/Poisson/mesh_new.pdf}
         
      \end{tabular}


  
  \end{tabular}
  % \end{subfigure} \\
       \caption{ 2D heat conductivity inversion problem (Left to Right): The domain and the boundaries; A $16\times 16$  finite element mesh and $10$ observational locations.}
  \label{mesh_details}
\end{figure} 


\subsection{Learning the observable to parameter map for 2D Navier-Stokes equation}
\label{nav_addi}

\paragraph{Data generation and numerical results}





For dataset generation in \cref{nav_main}, we draw
samples of $u(\bx,\ 0)$ based on the truncated Karhunen-Lo\`eve expansion as:
\begin{equation}
    u(x,\ 0)=\sum_{i=1}^{n_T}\sqrt{\lambda_i}\omega_i(x)c_i,
    \label{kl_nav}
\end{equation}
 where $\bc = \LRp{c_1,\hdots, c_{n_T}}\sim \sN({\bf{0}},I)$ is a standard Gaussian random vector, $\LRp{\lambda_i, \ \mb{\omega}_i}$  are eigenpairs obtained by the eigendecomposition of the covariance operator $7^{\frac{3}{2}}\LRp{-\Delta+49 {\bf{I}}}^{-2.5}$ with periodic boundary conditions. For demonstration, we choose $n_T=15$. For a given $u_0(\bx)$, we solve the Navier-Stokes equation by the stream-function formulation with a pseudospectral method \cite{li2020fourier} to compute $u(\bx,\ T)$ on the grid, which is then used to generate the observation vector $\by$. Similar to  \cref{poisson_sec} for a new observation data $\by$, the network outputs the vector $\bc$ which can then be used to reconstruct $u_0(\bx)$ using \eqref{kl_nav}.
\begin{figure}[h!]      
  % \begin{subfigure}[b]{\textwidth}
 \centering
 
  \begin{tabular}{c}


      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.5]{Figures/Navier_Stokes/solution_0_n.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

   \hspace{0.2 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.5]{Figures/Navier_Stokes/solution_1_n.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

 
\hspace{0.2 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.5]{Figures/Navier_Stokes/solution_2_n.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

  \end{tabular}\\


\vspace{-0.2 cm}


  \begin{tabular}{c}


      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.5]{Figures/Navier_Stokes/solution_3_n.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

   \hspace{0.2 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.5]{Figures/Navier_Stokes/solution_5_n.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

 
\hspace{0.2 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.5]{Figures/Navier_Stokes/solution_true_n.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

  \end{tabular}
  \caption{Evolution of parameter field  $u(\bx,\ T)$ for a particular test observation upon adding new layers for $S=250$ (Left to Right): inverse solution after the $1^{st}$ iteration;  inverse solution after the $2^{nd}$ iteration;  inverse solution after the $3^{rd}$ iteration; inverse solution after the $4^{th}$ iteration; inverse solution after the $6^{th}$ iteration; and the groundtruth parameter distribution.}
  \label{param_evol_nav}
\end{figure}   
 \cref{param_evol_nav} shows the parameter field (for a particular observational data input) predicted by our proposed approach at
different iterations of our algorithm where we see that the predicted vorticity field improves as one adds more parameters (weights/biases). 
\begin{comment}
\begin{figure}[h!]      
\centering
  
  \begin{tabular}{c}

      \begin{tabular}{c}

          \centering
           \includegraphics[scale=0.4]{Figures/Navier_Stokes/adaptation_summary_navier.pdf}
        
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}



      \begin{tabular}{c}

          \centering
         \includegraphics[scale=0.4]{Figures/Navier_Stokes/adaptation_summary_navier.pdf}
      \end{tabular}
  \end{tabular}
  % \end{subfigure} \\
       \caption{ Summary of the adaptation results ($S=250$). (Left to right):  Bar chart showing the average relative error on training dataset achieved at the end of each iteration; Bar chart showing the average relative error on test dataset achieved at the end of each iteration.}
  \label{sum_nav}
\end{figure} 
\end{comment}

\begin{comment}
\begin{figure}[h!]      
\centering
  
  \begin{tabular}{c}
  \hspace{-1.5 cm}
      \begin{tabular}{c}

          \centering
            \includegraphics[scale=0.33,trim={0 0 0 8cm},clip]{Figures/Poisson/iter_1.jpg}
        
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

\hspace{-1.5 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.33,trim={0cm 0 0 8cm},clip]{Figures/Poisson/iter_2.jpg}
      \end{tabular}

\hspace{-1.5 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.33,trim={0cm 0 0 8cm},clip]{Figures/Poisson/iter_3.jpg}
      \end{tabular}

  
  \end{tabular}
  \\
  
   \begin{tabular}{c}
 \hspace{-1.5 cm}
      \begin{tabular}{c}

          \centering
            \includegraphics[scale=0.33,trim={0 0 0 8cm},clip]{Figures/Poisson/iter_4.jpg}
        
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

\hspace{-1.5 cm}
      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.33,trim={0 0 0 8cm},clip]{Figures/Poisson/iter_5.jpg}
      \end{tabular}

\hspace{-1.5 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.33,trim={0 0 0 8cm},clip]{Figures/Poisson/iter_6.jpg}
      \end{tabular}
  
  \end{tabular}
  % \end{subfigure} \\
    \caption{Relative magnitude of the topological derivative for different hidden layers during the adaptation procedure for $S=250$. A new layer is inserted at the position of maximum topological derivative (shown in yellow).\textcolor{blue}{ Change this figure }}
    \label{rel_nav}
\end{figure} 
\end{comment}

Further, the performance (statistics of relative error) for each method is also provided in  \cref{stat_na}. Results in \cref{stat_na} are explained in \cref{nav_main}.
%From  \cref{stat_na} it is clear that in the low data regime, when one considers the average relative error, our proposed approach outperformed all other strategies, performing on par with a neural architecture search algorithm. However, as the dataset size increases, we see that random layer insertion (I) strategy performed equally well to our approach. We hypothesize that our initialization strategy based on local sensitivity analysis matters more (in terms of generalization) in the low-data regime. Similar observations have been made in \cref{poisson_sec}.
\begin{table}[h!]
\caption{Statistics ($\mu \pm \sigma$) of the relative error (rel. error) different methods (Navier-Stokes equation)}
\centering
\begin{tabular}{|c | c | c | c|c|}
        \hline
    Method &  rel. error & rel. error & Best  error\\ 
     &  ($S=250$) & ($S=500$) & $S=250 \ \ \ \vline  \ S=500$\\ \hline
     {\bf{  Proposed (II)}}   &  $0.328 \pm 0.0227$ & $0.283\pm 0.0039$& ${\bf{0.295}}$  \vline \  0.274\\ \hline
          {\bf{  Proposed (I)}} & ${\bf{0.320}} \pm 0.0198$ 
&${\bf{0.277}} \pm 0.0034$& $0.301$ \vline \  $0.271$\\ \hline
   Random layer insertion (I)   & $0.326 \pm 0.0219$ & ${\bf{0.277}} \pm 0.0036$ &$0.299$  \vline \ ${\bf{0.267}}$\\ \hline
   Net2DeeperNet (II) \cite{chen2015net2net}  & $0.347 \pm 0.050$ & $0.285 \pm 0.006$& $0.309$   \vline \ $0.275$  \\ \hline
     Baseline  network  &$0.350 \pm 0.038$ & $0.280 \pm 0.0037$&  $0.305$   \vline \  $0.273$\\ \hline
Forward Thinking \cite{hettinger2017forward} &  $0.429 \pm 0.0286$ &   $0.313 \pm 0.017$& $0.362$   \vline \  $0.284$ \\ \hline
NAS \cite{li2020system,li2020random} &  $-- \pm --$ &   $-- \pm --$&  ${\bf{0.295}}$  \vline \  0.273\\ \hline
	\end{tabular} 
 \label{stat_na}
\end{table}
\begin{comment}
\begin{table}[h!]
\caption{Statistics of the average relative error for different methods (2D Navier-Stokes equation)}
\centering
\begin{tabular}{|c | c | c | c|c|}
        \hline
    Method &  Avg. relative error ($\mu \pm \sigma$) & Avg. relative error ($\mu \pm \sigma$)  & $\%$ improvement of \\ 
     &  ($S=250$) & ($S=500$) & proposed over others\\ \hline
          {\bf{  Proposed }} & $0.320 \pm 0.0198$ & 
$0.277 \pm 0.0034$ &$S=250\ \ S=500$\\ \hline
   Random layer insertion (I)   & $0.326 \pm 0.0219$ & $0.277 \pm 0.0036$ & 1.84 $\%$   \vline \ \ 0.036 $\%$\\ \hline
   Net2DeeperNet (II) \cite{chen2015net2net}  & $0.347 \pm 0.050$ & $0.285 \pm 0.006$& 7.78 $\%$   \vline \ \ 2.807 $\%$\\ \hline
     Baseline  network  & $0.350 \pm 0.038$ & $0.280 \pm 0.0037$ & 8.57 $\%$   \vline \ \ 1.071 $\%$\\ \hline
Forward Thinking \cite{hettinger2017forward} &  $0.429 \pm 0.0286$ &   $0.313 \pm 0.017$& 25.4 $\%$   \vline \ \ 11.50 $\%$\\ \hline
	\end{tabular} 
 \label{stat_na}
\end{table}
\end{comment}



%\begin{table}[h!]
%\caption{Statistics of the average relative error based on $100$ random initialization for each method }
%\centering
%\begin{tabular}{||c c c  c c c||} 
 %\hline
%&{\bf{Proposed}}  & Random layer insertion (I)  & Random layer insertion (II)  &   Baseline & Hettinger et al.\\ [0.5ex] 
 %\hline \hline
 %Mean&{\bf{ $0.324 $}}& $0.326$  & $0.347$ & $0.35$ & $0.429 $\\ 
 %Std&{\bf{ $0.0198 $}}& $0.0219$  & $0.05$ & $0.038$ & $0.0286 $\\ 
%{\bf{Min}}&{\bf{ 0.300 }}& {\bf{0.300}}  & {\bf{ 0.309}} & {\bf{0.305}} & {\bf{0.36 }}\\ 
%Max&{\bf{ $0.373 $}}& $0.385$  & $0.51$ & $0.66$ & $0.498$\\ 
 %\hline
%\end{tabular}
%\end{table}

\begin{comment}
\begin{figure}[h!]      
\centering
  
  \begin{tabular}{c}
  
      \begin{tabular}{c}

          \centering
            \includegraphics[scale=0.4]{Figures/Navier_Stokes/adaptation_summary_navier.pdf}
        
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}


      \begin{tabular}{c}

          \centering
           \includegraphics[scale=0.4]{Figures/Navier_Stokes/bar_chart_navier.pdf}
      \end{tabular}


  
  \end{tabular}
  % \end{subfigure} \\
    \caption{ Left to Right: Summary of the adaptation results: Bar chart shows the test loss achieved at each stage of the procedure; Error bar 
 with one standard deviation of uncertainty for different adaptation strategies: 100 different random initialization are considered to investigate the uncertainty in each method.}
  \label{adaptation_results_sum_poisson}
\end{figure} 

\end{comment}

\subsection{Performance on a real-world regression data set: The California housing dataset}

In this section, we consider experiments with a real-world data set, i.e the California housing dataset (ML data set in scikit-learn \cite{scikit-learn}) where the task is to predict the housing prices given a set of $8$ input features, i.e we have $n_0=8$ and $n_T=1$ for this problem. We consider experiments with training data sets of sizes $S=500$ and $S=1000$,  and considered a validation data set of size $100$  and a testing data set of size $6192$. Other details on the
hyperparameter settings are provided in \cref{Input_values_different_problems}. \cref{stat_cal} provides the summary of the result (we use mean squared error (mse) to quantify the performance) where it is clear that our proposed method (I) outperformed other strategies. However, the fully-automated network growing \cref{Algo_full_auto} (i.e proposed (II)) does not seem to be very effective for this dataset in comparison to other adaptation strategies which could be due to overfitting in the early iterations of the algorithm.
%Figure \ref{rel_calif} and \ref{sum_cal} shows the summary of our adaptation strategy and the mean squared error achieved on the test dataset at the end of each iteration (the results are for the best network out of $100$ random initialization).



\begin{comment}
\begin{figure}[h!]      
\centering
  
  \begin{tabular}{c}

      \begin{tabular}{c}

          \centering
           \includegraphics[scale=0.4]{Figures/Navier_Stokes/adaptation_summary_navier.pdf}
        
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}



      \begin{tabular}{c}

          \centering
         \includegraphics[scale=0.4]{Figures/Navier_Stokes/adaptation_summary_navier.pdf}
      \end{tabular}
  \end{tabular}
  % \end{subfigure} \\
       \caption{ Summary of the adaptation results ($S=1000$). (Left to right): Bar chart showing the mean squared error on the training dataset achieved at the end of each iteration; Bar chart showing mean squared error on test dataset achieved at the end of each iteration. \textcolor{blue}{ Change this figure }}
     \label{sum_cal}
\end{figure} 
\end{comment}


\begin{table}[h!]
\caption{Statistics ($\mu \pm \sigma$) of the mean squared error (MSE) for different methods (California housing dataset)}
\centering
\begin{tabular}{|c | c | c | c|c|}
        \hline
    Method & MSE  &MSE  & Best  error\\ 
     &  ($S=500$) & ($S=1000$) & $S=500 \ \ \ \vline  \ S=1000$\\ \hline
     {\bf{  Proposed (II)}}   &  $0.455 \pm 0.015$ & $0.407 \pm 0.020$& $0.398$  \vline \  0.350\\ \hline
          {\bf{  Proposed (I)}} & ${\bf{0.448}} \pm 0.020$ 
&${\bf{0.384}} \pm 0.0192$& ${\bf{0.392}}$ \vline \  ${\bf{0.346}}$\\ \hline
   Random layer insertion (I)   & $0.453 \pm 0.016$ & $0.386 \pm 0.0185$&$0.403$  \vline \ $0.350$\\ \hline
   Net2DeeperNet (II) \cite{chen2015net2net}  & $0.455 \pm 0.017$ & $0.391 \pm 0.0197$& $0.407$   \vline \ $0.346$  \\ \hline
     Baseline  network  & $0.481 \pm 0.032$ & $0.391 \pm 0.0207$&  $0.405$   \vline \  $0.351$\\ \hline
Forward Thinking \cite{hettinger2017forward} &  $0.541 \pm 0.028$ &   $0.430 \pm 0.033$& $0.440$   \vline \  $0.380$ \\ \hline
NAS \cite{li2020system,li2020random} &  $-- \pm --$ &   $-- \pm --$&  $0.395$  \vline \  0.347\\ \hline
	\end{tabular} 
 \label{stat_cal}
\end{table}



    


\subsection{Wind velocity reconstruction problem}
This example considers the wind velocity reconstruction problem where the objective is to predict the magnitude of wind velocity on a uniform 3D grid based on sparse measurement data. The observational dataset consists of $(x,y,z)$ position components and the corresponding velocity $\LRp{u(x,y,z),\ v(x,y,z),\ w(x,y,z)}$  over North America. We train a network that takes in the $\LRp{x,\ y,\ z}$ components as inputs and predicts the magnitude of wind velocity $\sqrt{u^2+v^2+z^2}$ as the output. We consider a training data set of sizes $S=1000$ and $S=5000$,   a validation data set of size $2000$  and a testing data set of size $21525$. The performance of each method is provided in  \cref{stat_wind}. It is clear that both our proposed method (I) and (II) outperformed all other strategies. Further, the improvement in solution (3D air current profile) on adding new layers is shown in  \cref{wind_evolution} where one clearly sees that the algorithm progressively picks up complex features in the solution as evident from more contour lines appearing in later stages of the algorithm. Further,  \cref{rel_error_wind} shows the relative error in predictions w.r.t truth (equation \eqref{error_metric}) over the 3D domain for different methods. It is clear from  \cref{rel_error_wind} that our proposed approach provides the most accurate estimates.

\begin{figure}[h!]      
  % \begin{subfigure}[b]{\textwidth}
\centering
  \begin{tabular}{c}

\hspace{-1.3 cm}
      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.4]{Fig/l7.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

 \hspace{-1.1 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.4]{Fig/l8.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

 
\hspace{-1.1 cm}
      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.4]{Fig/l9.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}


  \end{tabular}\\

  



  \begin{tabular}{c}

\hspace{-1.3 cm}
      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.4]{Fig/l11.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

\hspace{-1.1 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.4]{Fig/l13.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

 
\hspace{-1.1 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.4]{Fig/true.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

  \end{tabular}
  \caption{Evolution of solution (3D air current profile) upon adding new hidden layers for $S=1000$ (Left to right): Solution after the $4^{th}$ iteration; Solution after the $5^{th}$ iteration; Solution after the $6^{th}$ iteration; Solution after the $8^{th}$ iteration;  Solution after the $10^{th}$ iteration; Ground truth solution.  }
\label{wind_evolution}
\end{figure} 

\begin{table}[h!]
\caption{Statistics ($\mu \pm \sigma$) of the mean squared error (MSE) for different methods (Wind velocity reconstruction)}
\centering
\begin{tabular}{|c | c | c | c|c|}
        \hline
    Method &  MSE  & MSE & Best  error\\ 
     &  ($S=1000$) & ($S=5000$) & $S=1000 \ \ \ \vline \ \ S=5000$\\ \hline
     {\bf{  Proposed (II)}}   &  $16.57 \pm 2.26$ & $5.32 \pm 0.97$& 13.01  \vline \  \ 3.97\\ \hline
          {\bf{  Proposed (I)}} & ${\bf{16.01}} \pm 2.90$ & 
${\bf{4.89}} \pm 1.23$&{\bf{12.40}}\ \vline \ \ {\bf{3.77}}\\ \hline
   Random layer insertion (I)   &  $17.85 \pm 4.17$ & $5.84 \pm 1.41$& 13.29   \vline \ \ 4.68 \\ \hline
   Net2DeeperNet (II) \cite{chen2015net2net}  & $19.10 \pm 4.15$ & $5.70 \pm 1.86$& 13.28  \vline \ \ 4.60  \\ \hline
     Baseline  network  & $18.96\pm 4.62$ & $5.53 \pm 2.02$&  13.24  \vline \ \ 4.53\\ \hline
Forward Thinking \cite{hettinger2017forward} &  $87.90 \pm 17.64$ &   $20.59 \pm 10.23$&48.15  \ \vline \  15.23 \\ \hline
NAS \cite{li2020system,li2020random} &  $-- \pm --$ &   $-- \pm --$&  14.12  \vline \ \ 4.35\\ \hline
	\end{tabular} 
 \label{stat_wind}
\end{table}




\begin{comment}
\begin{table}[h!]
\caption{Statistics of the mean squared error for different methods (Wind velocity reconstruction)}
\centering
\begin{tabular}{|c | c | c | c|c|}
        \hline
    Method &  Avg. mse ($\mu \pm \sigma$) & Average mse ($\mu \pm \sigma$)  & $\%$ improvement of\\ 
     &  ($S=1000$) & ($S=5000$) & proposed over others\\ \hline
          {\bf{  Proposed }} & $16.01 \pm 2.90$ & 
$5.20 \pm 1.36$&$S=1000\ \ S=5000$\\ \hline
   Random layer insertion (I)   & $17.85 \pm 4.17$ & $5.42 \pm 1.24$& 10.3 $\%$   \vline \ \ 4.05 $\%$\\ \hline
   Net2DeeperNet (II) \cite{chen2015net2net}  & $19.10 \pm 4.15$ & $5.70 \pm 1.86$& 16.2 $\%$   \vline \ \ 8.77 $\%$\\ \hline
     Baseline  network  & $18.96\pm 4.62$ & $5.53 \pm 2.02$&  15.5 $\%$   \vline \ \ 5.96 $\%$\\ \hline
Forward Thinking \cite{hettinger2017forward} &  $87.90 \pm 17.64$ &   $20.59 \pm 10.23$&81.8 $\%$   \vline \ \ 74.7 $\%$\\ \hline
	\end{tabular} 
 \label{stat_wind}
\end{table}

\end{comment}

 
\begin{figure}[h!]      
  % \begin{subfigure}[b]{\textwidth}
\centering
  \begin{tabular}{c}

 \hspace{-0.5 cm}
      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.37]{Fig/proposed_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

   \hspace{-1 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.37]{Fig/unit_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}

 
\hspace{-1 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.37]{Fig/random_error.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}


  \end{tabular}\\
    \begin{tabular}{c}


      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.37]{Fig/hettinger.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_other_good_name}

      \end{tabular}

   \hspace{0.2 cm}

      \begin{tabular}{c}

          \centering
          \includegraphics[scale=0.37]{Fig/base_line.pdf}
          % \caption{Effect of noise on DI and Tikhonov solutions}
          % \figlab{some_good_name}

      \end{tabular}
 \end{tabular}
  \caption{ Relative error in the predicted 3D air current profile over the spatial domain ($S=1000$). Left to right: Proposed method (I); Random layer insertion (I); Net2DeeperNet (II) \cite{chen2015net2net}; Forward Thinking \cite{hettinger2017forward}; Baseline. }
  \label{rel_error_wind}

\end{figure} 

\subsection{Image classification with convolutional neural network}

Finally, we test the performance of our adaptation scheme on a CNN architecture for an image classification task. In particular, we consider experiments on 
the MNIST handwritten dataset. Our architecture consists of an upsampling layer that maps the input image to $m$ channels, a sequence of residual layers that we adapt, a downsampling layer that maps $m$ channels to one channel, and a fully connected layer that outputs the probability vector to classify digits.  Other details on the
architecture are provided in  \cref{Input_values_different_problems}.   A summary of results for MNIST dataset (for two different sizes) is provided in \cref{stat_mnist}. Even in this case, we observe that our algorithm significantly outperforms other methods in the low data regime as evident from the maximum accuracy achieved in  \cref{stat_mnist}. However, when looking at the performance achieved by training on the full dataset we do not see considerable advantages over other approaches.
\begin{table}[h!]
\caption{Statistics ($\mu \pm \sigma$)  of the classification accuracy for different methods (MNIST Classification)}
\centering
\begin{tabular}{|c | c | c | c|c|}
        \hline
    Method &  Accuracy in $\%$ & Accuracy in $\%$  & Best  accuracy\\ 
     &  ($S=600$) & ($S=60000$) & $S=600 \ \ \ \vline  \ S=60000$\\ \hline
     {\bf{  Proposed (II)}}   &  $86.52  \pm 0.53$ & $98.90 \pm 0.07$& $87.40$  \vline \  98.95\\ \hline
          {\bf{  Proposed (I)}} & ${\bf{86.98}} \pm 0.73$ 
&$98.92 \pm 0.06$& ${\bf{88.44}}$ \vline \  $98.99$\\ \hline
   Random layer insertion (I)   & $86.78 \pm 0.62$ & $98.90 \pm 0.06$&$88.00$  \vline \ $98.95$\\ \hline
   Net2DeeperNet (II) \cite{chen2015net2net}  & $86.76 \pm 0.71$ & $98.92 \pm 0.07$& $87.64$   \vline \ $98.99$  \\ \hline
     Baseline  network  & $86.78 \pm 0.64$ & ${\bf{98.98}} \pm 0.06$&  $87.81$   \vline \  $99.05$\\ \hline
Forward Thinking \cite{hettinger2017forward} &  $86.60 \pm 0.85$ &   $98.90 \pm 0.08$ & $87.66$   \vline \  $98.95$ \\ \hline
NAS \cite{li2020system,li2020random} &  $-- \pm --$ &   $-- \pm --$&  $88.00$  \vline \  ${\bf{99.07}}$\\ \hline
	\end{tabular} 
\label{stat_mnist}
\end{table}
\begin{comment}
\begin{table}[h!]
\caption{Statistics of achieved accuracy by different methods (MNIST Classification)}
\centering
\begin{tabular}{|c | c | c | c|c|}
        \hline
    Method &  Avg. accuracy in $\%$ ($\mu \pm \sigma$) & Avg. accuracy in $\%$ ($\mu \pm \sigma$)  & Maximum \\ 
     &  ($S=600$) & ($S=60000$) & accuracy achieved\\ 
     &  &  &S=600 \ \ \ \ \vline \ \ S=60000 \\\hline
          {\bf{  Proposed }} & $86.98 \pm 0.73$ & 
$98.92 \pm 0.06$ &  88.44 \vline \ \ 98.99\\ \hline
   Random layer insertion (I)   & $86.78 \pm 0.62$ & $98.90 \pm 0.06$& 88.00 \vline \ \ 98.95\\ \hline
   Net2DeeperNet (II) \cite{chen2015net2net}  & $86.76 \pm 0.71$ & $98.92 \pm 0.07$& 87.64 \vline \ \ 98.99\\ \hline
     Baseline  network  & $86.78 \pm 0.64$ & $98.98 \pm 0.06$& 87.81 \vline \ \ 99.05\\ \hline
Forward Thinking \cite{hettinger2017forward} &  $86.60 \pm 0.85$ &   $98.90 \pm 0.08$&87.66 \vline \ \ 98.95\\ \hline
	\end{tabular} 
 \label{stat_mnist}
\end{table}
\end{comment}
\begin{comment}
\section{Other applications of Theorem \ref{exist_th}}
In the previous sections, we have successfully demonstrated the effectiveness of topological derivative approach for neural network architecture adaptation. The method works particularly well in the low data regime. In this section we show yet another application of topological derivative \eqref{topo_de} given by Theorem \ref{exist_th}. We look at the following questions:

\begin{enumerate}
    \item Can we further improve the performance of a pre-trained state-of-the-art models using the concept of topological derivative?
    \item How to use Theorem \ref{exist_th} for transfer learning?
\end{enumerate}

\subsection{Improving accuracy of state-of-the-art models with topological derivative approach}



\subsection{Transfer learning with  topological derivative approach}
\end{comment}

\subsection{Improving performance of pre-trained state-of-the-art machine learning models}
\label{vit_tran}
For discussion in \cref{vision_t}, we consider a ViT model by Google Brain Team with $5.8$ million parameters (vit\textunderscore tiny\textunderscore patch16\textunderscore 384), a schematic of which is shown in  \cref{vit}. The model is trained on ImageNet-21k dataset and fine-tuned on ImageNet-1k.
\begin{figure}[h!]      
  \centering
          \includegraphics[scale=1.2]{Figures/transfer_learning/vision_transformer.pdf}
    \caption{Schematic of a vision transformer (ViT) image classification model (Dosovitskiy et al. \cite{dosovitskiy2020image})}
  \label{vit}
\end{figure}
Note that in \cref{vision_t}, we have only considered adapting the MLP head in  \cref{vit} since it is reasonable to assume that the transformer Encoder block in \cref{vit} has learnt a good representation of the CIFAR-10 input data and it is only necessary to increase the capacity of the classifier (MLP Head) to further improve the classification accuracy. However, it is noteworthy that our approach may be extended to adapting multiple blocks in an iterative fashion, i.e make decisions on whether one should increase the capacity (add more parameters) for the transformer encoder block or MLP Head in \cref{vit}. This is beyond the scope of the present work and will be investigated in the future.

\subsection{Topological derivative informed transfer learning approach: Application in parameter efficient fine tuning}
\label{param_eff}

In order to further back our claim that topological derivative is indeed a good indicator for determining where to add a new layer (refer \cref{sci_tran}), we consider adding a new layer at different locations and the results are tabulated in \cref{correlation}. We found that the computed topological derivative correlates well with the observed mean squared error
and the best performance is achieved by adding a layer at the location of the highest topological derivative. Interestingly, we observe that retraining the first hidden layer yields the best results, contrary to traditional transfer learning, where the last few layers are typically retrained. Other details of hyperparameters used for the problem are provided in \cref{hyper_parameter_n}.

\begin{table}[h!]
\caption{Correlation of topological derivative with mean squared error achieved by proposed method}
\centering
\begin{tabular}{|c | c | c | c}
        \hline
   Hidden layer number&  Topological derivative& Mean squared error  \\ 
    (Added layer)  &  &   \\ \hline
         1 & ${\bf{0.974}}$ &  ${\bf{2.576}}$  \\ \hline
          3 & $0.941$ &  $2.716$  \\ \hline
           5 & $0.907$ &  $3.120$  \\ \hline
           7 & $0.851$ &  $3.280$  \\ \hline
           8 & \textcolor{red}{\textbf{0.821}} &  \textcolor{red}{\textbf{3.524}}  \\ \hline
	\end{tabular} 
 \label{correlation}
\end{table}

\end{section}

\begin{section}{Details of hyperparameter values for different problems}{}
   \label{hyper_parameter_n}
   Details of hyperparameters used in  \cref{Algo_full} and \cref{Algo_full_CNN} is provided in  \cref{Input_values_different_problems}.   \cref{Input_values_different_problems} additionally provides details on the hyperparameters used for the optimizer. 
The description of each problem is also provided below.
\begin{table}[h!]
	\caption{Details of hyperparameters  for \cref{Algo_full}} \label{Input_values_different_problems}
	\renewcommand{\arraystretch}{1.3}
	\centering	
	\begin{tabular}{|c | c | c | c | c | c | c | c | c |c|c|c|c|c|}
 \hline
		\centering
		 Problem &$n$ &$m$ &$N_n$ &   $ T_b$ & $i_s$ ($\%$)& $E_e$ & $\kappa_e$ & $b_s$ & $\ell_r$&  $\epsilon^t$& $\epsilon$& $\tau_1$\\
		\hline
  I (S=1000)& 10 &5 & 7& 2 & $30$ $\%$&2000 & 1000 &1000& 0.001& 0.01& 0.001 & 0.001\\
   I (S=1500)& 10 &5 & 7& 2 & $30$ $\%$ &2000 &1000 &1500& 0.001&  0.01 & 0.001 & 0.001\\ \hline
  II (S=250)& 20 &10 & 6& 2 & $25$ $\%$&2000 & 1000 &125& 0.001& 0.01& 0.001 & 0.001\\
   II (S=500)& 20 &10 & 6& 2 & $25$ $\%$ &2000 &1000 &250& 0.001& 0.01&  0.001& 0.001\\ \hline
  III (S=500)& 20 &10 & 6& 2 & $25$ $\%$&500 & 500 &500& 0.01& 0.01 &0.001& 0.001\\
   III (S=1000)& 20 &10 & 6& 2 & $25$ $\%$ &500 &500 &1000& 0.01&  0.01 & 0.001 & 0.001\\ \hline
  IV (S=1000)& 20 &10 & 6& 2 & $10$ $\%$&500 & 500 &1000& 0.001& 0.01 & 0.001 & 0.001\\ 
   IV (S=5000)& 20 &10 & 6& 2 & $10$ $\%$ &500 &500 &5000& 0.001&  0.01&  0.001 & 0.001\\ \hline
     VI & 10 &5&20& $0$  & $0$ $\%$ &1 &1&512& 0.001&  0.01&0.001 & 0.1\\ \hline
      VII & 50 &50& 1  & 8 &0 $\%$ &2000&0& 20& 0.001& 0.01& 0.001 & 0.001\\ 
  \hline
	\end{tabular} 
\end{table}
In \cref{Input_values_different_problems}, $b_s$ denotes the batch size, $\ell_r$ denotes the learning rate,  and $\sigma_n=0.01$ is chosen as the standard deviation of Gaussian noise for initializing the weights/biases. In order to reduce the number of parameters in the first layer, we introduce the sparsity parameter denoted as $i_s$. In our experiments, $i_s\%$ parameters in the first layer are initialized as zeros and the corresponding connections are removed (or those parameters are untrainable/frozen throughout the procedure). Further, as more layers are added to capture more complex features in the solution, it is also natural to consider training the bigger network for a larger number of epochs. We consider the following scheduler for choosing the training epochs at each iteration,
$E(i)=E_e+\kappa_e \times (i-1)$, where $i$ denotes the $i^{th}$ iteration of the algorithm. For  \cref{Algo_full_auto}, we use the same hyperparameters in  \cref{Input_values_different_problems}, \cref{Input_values_CNN} except that no scheduler is employed (i.e $E_e,\ \kappa_e$ is not employed) and we not fix the parameter $m$. Further, the parameter $N_k$ in  \cref{Algo_full_auto} is chosen as $100$ for all problems. For MNIST classification problem we fix $N_k$ as 20.
\begin{table}[h!]
	\caption{Details of hyperparameters  for \cref{Algo_full_CNN}} \label{Input_values_CNN}
	\renewcommand{\arraystretch}{1.3}
	\centering	
	\begin{tabular}{|c | c | c | c | c | c | c | c | c |c|c|c|c|c|c|}
 \hline
		\centering
		 Problem &$n$ &$m$ &$N_n$ & $T_b$ &  $f$ & $ s$& $E_e$ & $\kappa_e$& $b_s$& $\ell_r$&  $\epsilon^t$ & $\epsilon$& $\tau_1$ \\
		\hline
  V  (S=600)& $10$ &$5$ & $6$& $2$ & $5$ &$1$ & $15$ & 0& $600$&$0.001$& 0.01& 0.0001& 0.00001\\
  V  (S=60000)& $10$ &$10$ & $7$& $6$ & $5$ &$1$ & $5$ &0 & $600$&$0.001$&  0.01& 0.0001& 0.00001\\ \hline
	\end{tabular} 
\end{table}

\begin{equation*}
 \begin{aligned}
% \text{I}&: \text{1D regression with RBF neural network}.\\
\text{I}&: \text{Learning the observable to parameter map for 2D heat equation}.\\
\text{II }&: \text{Learning the observable to parameter map for Navier Stokes equation}.\\
\text{III }&: \text{Regression on  California housing dataset.}\\
\text{IV }&: \text{Wind velocity reconstruction problem.}\\
\text{V }&: \text{Image classification on MNIST dataset with CNN architecture.}\\
\text{VI }&: \text{Transfer learning for vision transformer (\cref{vision_t}).}\\
\text{VII }&: \text{Transfer learning involving scientific data-set ( \cref{sci_tran}).}
%\text{VI }&: \text{Image classification on CIFAR10 dataset with CNN architecture.}\\
\end{aligned}  
\end{equation*}


\end{section}



\end{document}
