[
  {
    "index": 0,
    "papers": [
      {
        "key": "wendler-etal-2024-llamas",
        "author": "Wendler, Chris and Veselovsky, Veniamin and Monea, Giovanni and West, Robert",
        "title": "Do Llamas Work in {E}nglish? On the Latent Language of Multilingual Transformers"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhong2024beyond",
        "author": "Zhong, Chengzhi and Cheng, Fei and Liu, Qianying and Jiang, Junfeng and Wan, Zhen and Chu, Chenhui and Murawaki, Yugo and Kurohashi, Sadao",
        "title": "Beyond English-Centric LLMs: What Language Do Multilingual Language Models Think in?"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhao2024large",
        "author": "Yiran Zhao and\nWenxuan Zhang and\nGuizhen Chen and\nKenji Kawaguchi and\nLidong Bing",
        "title": "How do Large Language Models Handle Multilingualism?"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "logit-lens",
        "author": "Nostalgebraist",
        "title": "{Interpreting GPT: The Logit Lens}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "belrose2023eliciting",
        "author": "Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob",
        "title": "Eliciting latent predictions from transformers with the tuned lens"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "elhage2021mathematical",
        "author": "Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others",
        "title": "A mathematical framework for transformer circuits"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "meng2022locating",
        "author": "Kevin Meng and\nDavid Bau and\nAlex Andonian and\nYonatan Belinkov",
        "title": "Locating and Editing Factual Associations in {GPT}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "variengien2024look",
        "author": "Alexandre Variengien and Eric Winsor",
        "title": "Look Before You Leap: A Universal Emergent Decomposition of Retrieval Tasks in Language Models"
      },
      {
        "key": "geiger2022inducing",
        "author": "Geiger, Atticus and Wu, Zhengxuan and Lu, Hanson and Rozner, Josh and Kreiss, Elisa and Icard, Thomas and Goodman, Noah and Potts, Christopher",
        "title": "Inducing causal structure for interpretable neural networks"
      },
      {
        "key": "kramar2024atp",
        "author": "Kram{\\'a}r, J{\\'a}nos and Lieberum, Tom and Shah, Rohin and Nanda, Neel",
        "title": "AtP*: An efficient and scalable method for localizing LLM behaviour to components"
      },
      {
        "key": "ghandeharioun2024patchscope",
        "author": "Asma Ghandeharioun and\nAvi Caciularu and\nAdam Pearce and\nLucas Dixon and\nMor Geva",
        "title": "Patchscopes: {A} Unifying Framework for Inspecting Hidden Representations\nof Language Models"
      },
      {
        "key": "chen2024selfie",
        "author": "Haozhe Chen and\nCarl Vondrick and\nChengzhi Mao",
        "title": "SelfIE: Self-Interpretation of Large Language Model Embeddings"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "husain2024romansetu",
        "author": "Jaavid, J and Dabre, Raj and Aswanth, M and Gala, Jay and Jayakumar, Thanmay and Puduppully, Ratish and Kunchukuttan, Anoop",
        "title": "Romansetu: Efficiently unlocking multilingual capabilities of large language models via romanization"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu-etal-2024-translico",
        "author": "Yihong Liu and\nChunlan Ma and\nHaotian Ye and\nHinrich Sch{\\\"{u}}tze",
        "title": "TransliCo: {A} Contrastive Learning Framework to Address the Script\nBarrier in Multilingual Pretrained Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "xhelili-etal-2024-breaking",
        "author": "Xhelili, Orgest and Liu, Yihong and Schuetze, Hinrich",
        "title": "Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment"
      }
    ]
  }
]