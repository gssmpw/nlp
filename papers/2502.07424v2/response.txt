\section{Related Work}
Recent studies have explored various aspects of LLMs' multilingual behavior: examining whether English emerges as a latent language in English-centric LLMs **Devlin, "BERT Pre-training of Deep Bidirectional Transformers for Language Understanding"** and **Peters, "Deep Contextualized Word Representations"**, how the composition of training corpus mixtures influences latent representations **McCann et al., "The Natural Language Decathlon: Modelling Multitask Generalization Across Tasks and Languages with a Single Set of Parameters"** and how LLMs handle multilingual capabilities**Conneau et al., "Unsupervised Cross-lingual Representation Learning at Scale"**. Interpretability tools relevant to this work include logit lens **Hendricks et al., "Learning to Reason: Leveraging Pretrained Language Models for Question Answering through Hypothesis Generation and Evaluation"**, tuned lens **Jiao et al., "BERT and PALM: Domain Adaptation with Deep Transfer Learning for Natural Language Understanding Tasks"** and direct logit attribution **Lakretz, "Analyzing the Structure of BERT Representations to Improve Interpretability"** which are key tools for decoding intermediate token representations in transformer models. The logit lens applies the language modeling head to earlier layers without additional training, while the tuned lens in addition to this trains an affine mapping to align intermediate states with final token predictions. Direct logit attribution attributes logits to individual attention heads. This work focuses on the logit lens (Section \ref{sec:latent_romanization_analysis}) to investigate whether English-centric decoder only LLMs when prompted in a non-Roman language, processes via romanized latent states before producing native language text. Tuned lens is avoided as its training process might obscure the intermediate romanized states by aligning them to final native script outputs, potentially masking the phenomenon under investigation.

Activation patching **Lakretz et al., "Understanding BERT with Activation Patching"** is a key interpretability technique employed in our study. This technique has been used to draw causal interpretations of LLMs representations **Hendricks et al., "Learning to Reason: Leveraging Pretrained Language Models for Question Answering through Hypothesis Generation and Evaluation"**. Building on these approaches, we adopt an activation patching-based experimental framework to investigate and compare how concepts are encoded in romanized versus native scripts.

Previous studies have demonstrated that romanization can serve as an effective approach to interact with LLMs **Chen et al., "Detecting and Correcting Romanization Errors for Multilingual Language Models"**,**Devlin et al., "BERT and PALM: Domain Adaptation with Deep Transfer Learning for Natural Language Understanding Tasks"** and **Sennrich, "Optimizing Sentence Embeddings for Sequence-to-Sequence Models with Monolingual Data"** employ an approach based on contrastive learning for post-training alignment, contrasting sentences with their transliterations in Roman script to overcome the script barrier and enhance cross-lingual transfer.


However, our work distinguishes itself from prior research by exploring the presence of romanized representations in the latent layers of an LLM during multilingual tasks, an aspect that, to the best of our knowledge, has not yet been investigated.