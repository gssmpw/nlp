\section{Related Work}
Recent studies have explored various aspects of LLMs' multilingual behavior: examining whether English emerges as a latent language in English-centric LLMs \cite{wendler-etal-2024-llamas}, how the composition of training corpus mixtures influences latent representations \cite{zhong2024beyond} and how LLMs handle multilingual capabilities\cite{zhao2024large}. Interpretability tools relevant to this work include logit lens \cite{logit-lens}, tuned lens \cite{belrose2023eliciting} and direct logit attribution \cite{elhage2021mathematical} which are key tools for decoding intermediate token representations in transformer models. The logit lens applies the language modeling head to earlier layers without additional training, while the tuned lens in addition to this trains an affine mapping to align intermediate states with final token predictions. Direct logit attribution attributes logits to individual attention heads. This work focuses on the logit lens (Section \ref{sec:latent_romanization_analysis}) to investigate whether English-centric decoder only LLMs when prompted in a non-Roman language, processes via romanized latent states before producing native language text. Tuned lens is avoided as its training process might obscure the intermediate romanized states by aligning them to final native script outputs, potentially masking the phenomenon under investigation.

Activation patching \citep{meng2022locating} is a key interpretability technique employed in our study. This technique has been used to draw causal interpretations of LLMs representations \cite{variengien2024look,geiger2022inducing,kramar2024atp,ghandeharioun2024patchscope,chen2024selfie}. Building on these approaches, we adopt an activation patching-based experimental framework to investigate and compare how concepts are encoded in romanized versus native scripts.

Previous studies have demonstrated that romanization can serve as an effective approach to interact with LLMs \cite{husain2024romansetu}. \citet{liu-etal-2024-translico} and \citet{xhelili-etal-2024-breaking} employ an approach based on contrastive learning for post-training alignment, contrasting sentences with their transliterations in Roman script to overcome the script barrier and enhance cross-lingual transfer.


However, our work distinguishes itself from prior research by exploring the presence of romanized representations in the latent layers of an LLM during multilingual tasks, an aspect that, to the best of our knowledge, has not yet been investigated.