\begin{figure*}
    \centering
    \includegraphics[width=17.6cm]{picture/RAViT_Backbone.png}
    \caption{Overall RAViT backbone with 4-stage pyramid architecture with structural reparameterization to increase the inference speed. Stages 1 and 2 use the same RepMSDW as the spatial token mixer. (a) RAViT Block that can mix between RepMSDW and RepSA (b) Overview the reparameterization of multi-scale depth-wise convolution (RepMSDW) (c) Illustration of Reparameterize Self-Attention (d) Architecture of Stem (e) Architecture of Downsample }
    \label{fig:RAViT}
\end{figure*} 
\section{Related Works}
% \subsection{FCOS Object Detector}
\subsection{Efficient Vision Transformer}
Recent advances in efficient vision transformers began with MobileViTs \cite{mehta2022mobilevit}, which combined MobileNets' efficiency with Vision Transformers' (ViTs) global modeling abilities. EfficientFormers \cite{li2022efficientformer, li2023rethinking} demonstrated a feature dimension-aware design that employs hardware-optimized 4D modules and effective 3D Multi-Head Self-Attention (MHSA) blocks. FastViT \cite{vasu2023fastvit} enhanced model capacity and efficiency by integrating 7 x 7 depthwise convolutions with Structural Re-Parameterization. EdgeNext \cite{maaz2022edgenext}applied local-global blocks to better combine MHSA and convolution. SHViT \cite{yun2024shvit} addressed computational redundancy through a Single-Head Self-Attention (SHSA) mechanism applied to a subset of channels. EMO \cite{zhang2023rethinking} addressed a simplicity that merged window self-attention with inverted bottleneck convolution into a single block.

\subsection{Large Kernel Convolution}
Initially, traditional CNNs like AlexNet and GoogLeNet favored large kernels in their early architecture, but with VGG \cite{ding2021repvgg}, the focus shifted to using stacked 3 × 3 kernels. InceptionNets \cite{szegedy2016rethinking, szegedy2017inception} improved computational efficiency by breaking down n×n convolutions into sequential $1 \times n$ and $n \times 1$ operations. SegNeXt \cite{guo2022segnext} extended the effective kernel size with combined $1 \times k + k \times 1$ and $k \times 1 + 1 \times k$ convolutions for semantic segmentation tasks. MogaNet \cite{li2023moganet} employed multi-scale spatial aggregation blocks that utilize dilated convolutions to capture discriminative features. ConvNeXt \cite{liu2022convnet} experimented with modern CNN designs using 7 × 7 depthwise convolutions, reflecting the Swin Transformer \cite{liu2021swin} architectural strategies. InceptionNeXt \cite{yu2024inceptionnext} enhanced throughput and performance by splitting large-kernel depthwise convolutions into four parallel branches. SKNet \cite{cui2021sknet} and LSKNet \cite{li2024lsknet} employed multi-branch convolutions in both channel and spatial dimensions. Moreover, RepLKNet \cite{ding2022scaling} expanded kernel sizes to 31 $\times$ 31 using SRP, achieving performance comparable to Swin Transformers. 

\subsection{Structural Re-Parameterization}
Recent studies, RepVGG \cite{ding2021repvgg} have shown that reparameterizing skip connections can lessen memory access costs. To improve efficiency, previous works such as MobileOne \cite{vasu2023mobileone} have utilized factorized k×k convolutions combined with depthwise or grouped convolutions followed by 1×1 pointwise convolutions. This approach significantly boosts overall model efficiency, although the reduced number of parameters could lead to lower capacity. Recently, reparameterized MLP like the token mixer that is proposed in \cite{ding2022repmlpnet} called RepMLPNet. To the best of our knowledge, using structural reparameterization to remove skip connections with multiple-scale convolution has not been explored in hybrid transformer architectures before. Moreover, the combination of reparameterized convolution combined with the self-attention mechanism has also not been explored yet.
