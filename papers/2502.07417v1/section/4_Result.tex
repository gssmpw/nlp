\begin{table*}[!ht]
\centering
\caption{Comparison of RAViT variant with State-Of-The-Art on ImageNet-1K Dataset. GPU inference throughput is measured using RTX3090. NPU and Edge are the latency that was measured on the iPhone 15 Pro using CoreML format and the NVIDIA Jetson Orin Nano Edge Device using ONNX format.}
\begin{tabular}{ m{3.8cm}|>{\centering}m{0.8cm}|>{\centering}m{1.1cm}|>{\centering}m{1.0cm}|>{\centering}m{1.1cm}|>{\centering}m{1.0cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|c }
\hline
\multirow{2}{*}{Model} & \multirow{2}{*}{Type} & Img  & Param & FLOPs & GPU & NPU & Edge & \multirow{2}{*}{Top-1} \\ 
                          & & Size  &   (M)  & (G)  & (Img/s) & (ms) &  (ms) &   \\ \hline
% EfficientFormerV2-S0              &Hybrid & 224 & 3.5  & 0.4 & 1274 & 0.64 & 11.8 & 73.7 \\
% EdgeNeXt-XS                       &Hybrid & 256 & 2.3  & 0.6 & 3126 & 26.4 & 10.4 & 75.0 \\
% EMO-5M                            &Hybrid & 224 & 2.3  & 0.5 & 2037 & 2.44 & 16.5 & 75.1 \\
% FastViT-T8\cite{vasu2023fastvit}  &Hybrid & 256 & 3.6  & 0.7 & 3909 & 0.67 & 8.2  & 75.6 \\
% % \rowcolor{gray!30}
% % \textbf{RAViT-XT}               &Hybrid & 224 & 6.8  & 0.7 & 4358 & 0.89 & 10.6 & 76.9? \\
% % \hline 
% PoolFormer-S12                    &Hybrid & 224 & 11.9 & 1.8 & 2750 & -    & 8.2  & 77.2 \\
% RepViT-M0.9                       &Conv   & 224 & 5.1  & 0.8 & 4817 & 0.60 & 12.1 & 77.4 \\
% MobileOne-S2                      &Conv   & 224 & 7.8  & 1.3 & 2793 & 0.77 & 11.2 & 77.4 \\
EfficientFormerV2-S1\cite{li2023rethinking} &Hybrid & 224 & 16.1 & 0.7 & 1153 & 0.76 & 14.3 & 77.9 \\
% RepViT-M1.0                     &Conv   & 224 & 6.8  & 1.1 & 3910 & 0.69 & 12.3 & 78.6 \\
MobileViTV2-1.0\cite{mehta2023separable}    &Hybrid & 224 & 4.9  & 1.8 & 1858 & 1.94 & 14.4 & 78.1 \\
MobileOne-S3\cite{vasu2023mobileone}        &Conv   & 224 & 10.1 & 1.8 & 2793 & 0.93 & 15.2 & 78.1 \\
\rowcolor{gray!30}
\textbf{RAViT-T26}                    &Hybrid & 224 & 8.2  & 0.9 & 3755 & 0.71 & 12.9 & 78.4 \\
\hline 
EMO-6M\cite{zhang2023rethinking}      &Hybrid & 224 & 5.1  & 1.0 & 1916 & 2.49 & 16.9 & 79.0 \\
FastViT-T12\cite{vasu2023fastvit}     &Hybrid & 256 & 6.8  & 1.4 & 3182 & 0.99 & 10.9 & 79.1 \\
EdgeNeXt-S \cite{maaz2022edgenext}    &Hybrid & 256 & 5.6  & 1.3 & 1207 & 32.5 & 13.1 & 79.4 \\
MobileOne-S4 \cite{vasu2023mobileone} &Conv   & 224 & 14.8 & 2.9 & 1892 & 1.21 & 19.0 & 79.4 \\
RepViT-M1.1\cite{wang2024repvit}      &Conv   & 224 & 8.2  & 1.3 & 3604 & 0.74 & 11.4 & 79.4 \\
\rowcolor{gray!30}
\textbf{RAViT-S22}                &Hybrid & 224 & 10.2 & 1.2 & 3491 & 0.80 & 11.9 & 79.6 \\
\hline
FastViT-S12\cite{vasu2023fastvit} &Hybrid & 256 & 8.8  & 1.8 & 2313 & 1.13 & 11.8 & 79.8 \\
PoolFormer-S24\cite{yu2022metaformer} &Hybrid & 224 & 21.4 & 3.4 & 1424 & 1.68 & 18.3 & 80.3 \\
MobileViTV2-1.5 \cite{mehta2023separable} &Hybrid & 256 & 10.6 & 4.2 & 1116 & 2.95 & 20.9 & 80.4 \\
EfficientFormerV2-S2\cite{li2023rethinking} &Hybrid & 224 & 12.6 & 1.3 & 611  & 1.23 & 21.5 & 80.4 \\
\rowcolor{gray!30}
\textbf{RAViT-S26}                &Hybrid & 224 & 11.5 & 1.4 & 3079 & 0.83 & 13.0 & 80.2 \\
\rowcolor{gray!30}
\textbf{RAViT-SA22}               &Hybrid & 224 & 10.9 & 1.3 & 2878 & 0.94 & 17.3 & 80.4 \\

\hline
FastViT-SA12\cite{vasu2023fastvit}     &Hybrid & 256 & 10.9 & 1.9 & 2181 & 1.22 & 12.8 & 80.6 \\
RepViT-M1.5 \cite{wang2024repvit}      &Conv   & 224 & 14.0 & 2.3 & 2151 & 1.04 & 19.4 & 81.2 \\
Swin-T \cite{liu2021swin}              &ViT    & 224 & 29.0 & 4.5 & 886  & 6.51 & 24.5 & 81.3 \\
PoolFormer-S36\cite{yu2022metaformer} &Hybrid & 224 & 30.9 & 5.0 & 967 & 2.35 & 26.8 & 81.4 \\
% \rowcolor{gray!30}
% \textbf{RAViT-M22}              &Hybrid & 224 & 16.2 & 2.1 & 2470 & 1.09 & 12.7 & 81.2 \\
\rowcolor{gray!30}
\textbf{RAViT-M26}                &Hybrid & 224 & 18.5 & 2.4 & 2193 & 1.17 & 14.4 & 81.4 \\
\hline
% FastViT-SA24\cite{vasu2023fastvit} &Hybrid & 256 & 20.6 & 3.8 & 1128 & 1.17 & 22.0 & 82.6 \\
% RAViT-LA                           &Hybrid & 224 &      &     &      &     &      &  \\
\end{tabular}
\label{tab:imagenet}
\end{table*}

\begin{table*}[!ht]
\centering
\caption{Instance segmentation test of RAViT as a backbone on COCO val2017 with Mask R-CNN. $AP^b$ and $AP^m$ denote bounding box average precision and mask average precision, respectively. The FLOPs and latency are measured at resolution 1280 $\times$ 800.
}
\begin{tabular}{ m{2.8cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|c} \hline
\multirow{2}{*}{Backbone} & \multirow{2}{*}{$AP^{b}$}  & \multirow{2}{*}{$AP^{b}_{50}$} & \multirow{2}{*}{$AP^{b}_{75}$} & \multirow{2}{*}{$AP^{m}$}  & \multirow{2}{*}{$AP^{m}_{50}$} & \multirow{2}{*}{$AP^{m}_{75}$} & Param & FLOPs & GPU & EDGE \\
    &  &  &  &  &  &  & (M) & (G) & (img/s) & (ms)   \\ \hline

PVT-T\cite{wang2021pyramid} & 36.7 & 59.2 & 39.3 & 35.1 & 56.7 & 37.3 & 32.8 & 239.8  & 24.7 &  517 \\ 
% PVTv2-B1\cite{wang2022pvt} & 41.8 & 64.3  & 45.9 &  38.8     & 61.2  & 41.6 & 33.7 & 243.7G   \\ 
PoolFormer-S12\cite{yu2022metaformer} & 37.3 & 59.0 & 40.1 & 34.6 & 55.8 & 36.9 & 31.6  & 207.3  & 20.0 & 353 \\
ResNet-50\cite{he2016deep}   & 38.0 & 58.6 & 41.4 & 34.4 & 55.1 & 36.7 & 44.2 & 260.1 & 31.5 & 333 \\
FastViT-SA12\cite{vasu2023fastvit} & 38.9 & 60.5 & 42.2 & 35.9 & 57.6 & 38.1 & 30.5 & 200.4 & 37.5 & 413 \\
RepViT-M1.1\cite{wang2024repvit} & 39.8 & 61.9 & 43.5 & 37.2 & 58.8 & 40.1 & 27.9 & 197.6 & 42.2 & 370 \\ 
\rowcolor{gray!30}
\textbf{REViT-S26}           & 40.4 & 62.5 & 44.2 & 37.8 & 59.8 & 40.2 & 29.6 & 198.8 & 40.0 & 284 \\ \hline
PoolFormer-S24\cite{yu2022metaformer} & 40.1 & 62.2 & 43.4 & 37.0 & 59.1 & 39.6 & 41.0 & 213.8 & 12.2 & 558 \\ 
PVT-S\cite{wang2021pyramid}  & 40.4 & 62.9 & 43.8 & 37.8 & 60.1 & 40.3 & 44.1 & 304.5 & 16.9 & 731 \\ 
RepViT-M1.1\cite{wang2024repvit} & 41.6 & 63.2 & 45.3 & 38.6 & 60.5 & 41.4 & 33.8 & 217.1 & 35.8 & 427 \\ 
\rowcolor{gray!30}
\textbf{REViT-M26}           & 41.6 & 64.0 & 45.3 & 38.9 & 60.9 & 41.9 & 34.4 & 212.3 & 36.7 & 334 \\ \hline

\end{tabular}
\label{tab:obj}
\end{table*}
\section{Result}
To evaluate the effectiveness of the RAViT backbone and Fast-COS, we perform several tests. The ImageNet-1K dataset with 1000 categories is selected as the image classification benchmarking for the backbone test. We also perform a backbone evaluation test for instance segmentation on the COCO dataset. In a specific task, we advance Fast-COS with RepFPN to perform driving scene object detection on the BDD100K and TJU-DHD-traffic datasets. Since the FLOPs do not directly affect the computational complexity, we compare RAViT and Fast-COS with other models in the state-of-the-art using throughput and latency metrics that were tested on three different wide-range application processing devices, including the GPU RTX3090, the Neural Processing Unit (NPU) on the iPhone 15 Pro as a mobile processing unit, and the Jetson Orin Nano as the edge device processing unit. This extensive benchmark will represent real-time performance on diverse hardware platforms. 
\subsection{Evaluation Result of RAViT Backbone}
\subsubsection{Setup}
We evaluate RAViT using ImageNet-1K \cite{deng2009imagenet} as the most popular image classification benchmarking dataset. ImageNet-1K has 1000 categories with 1.2 million images for training and 50000 images for validation. We follow a training recipe in \cite{liu2021swin} with a total of 300 epochs in each RAViT model variant, with a resolution of 224$\times$224. Data augmentation and regularization approaches encompass several methods such as RandAugment \cite{cubuk2020randaugment}, Mixup, CutMix \cite{yun2019cutmix}, Random Erasing \cite{zhong2020random}, weight decay, label smoothing, and Stochastic Depth. We employ the AdamW optimizer with a 0.004 base learning rate and a total batch size of 2046 on 4$\times$A6000 GPUs for most RAViT models.

We also conducted an experiment on the COCO dataset, a benchmark widely used for object detection and instance segmentation using Mask R-CNN. The backbone of Mask R-CNN was replaced with RAViT, which leverages multi-scale convolution and self-attention to improve feature extraction. The COCO dataset's training split was used for model training, while the validation split was used for evaluation, adhering to the standard COCO metrics, including mean Average Precision (mAP) for both bounding box detection and segmentation masks.

We assess performance through inference latency tests conducted on two types of constrained-resource device hardware and a single desktop GPU. For mobile device performance assessment, the iPhone 15 Pro is utilized. All models are transformed into the CoreML format, and each undergoes 50 $\times$ inference loops following a prior 20-loop warm-up. The mean inference duration serves as the evaluation metric. The Jetson Orin Nano is chosen to represent Edge device hardware for evaluation purposes. Models are adapted to the ONNX format for latency measurement on Edge devices. In Edge device assessment, a warm-up duration of 20 seconds precedes 1000 $\times$ inference loops.

\subsubsection{Benchmarking on ImageNet-1K}
The comparative analysis presented in Table \ref{tab:imagenet} highlights the performance of RAViT variants against state-of-the-art models on the ImageNet-1K dataset. We evaluate across various hardware platforms including GPU, Mobile NPU, and Edge devices to give a wide application illustration as a backbone. The RAViT models demonstrate competitive accuracy and computational efficiency trade-offs when compared to recent state-of-the-art models. For instance, RAViT-M26 reaches 81.4\% Top-1 accuracy while achieving $2.27\times$ faster GPU throughput, $2\times$ faster NPU latency, and $1.8\times$ faster Edge device latency compared to PoolFormer-S36 and Swin-T. RAViT-M26 achieves higher 0.2\% Top-1 accuracy while maintaining similar NPU and Edge device latency compared to the recent mobile vision transformer such as RepViT, showcasing its architectural efficiency. Similarly, RAViT-M26 achieves higher 0.8\% Top-1 accuracy compared to FastViT-SA12, while maintaining 4\% faster mobile NPU inference speed. 

The RAViT models consistently achieve high accuracy across different configurations. Although MobileOne also uses the reparameterization technique, the hybrid transformer architecture of RAViT indicates a trade-off between accuracy and speed. For example, RAViT-S22 can outperform MobileOne-S4 with $1.8\times$ faster GPU throughput, $1.5\times$ faster Mobile NPU latency, and $1.6\times$ faster Edge device latency. RAViT models achieve faster GPU, Mobile NPU, Edge device inference while providing superior accuracy, showcasing their versatility and effectiveness for real-world deployment scenarios.
\subsection{Benchmarking with SOTA Models in COCO Instance Segmentation}
Table III presents the evaluation of the proposed RAViT backbones on the COCO val2017 dataset with Mask R-CNN, compared to other state-of-the-art methods. The evaluation metrics include bounding box average precision $(AP_b)$, mask average precision $(AP_m)$, computational complexity (FLOPs), parameter size, and latency on GPU and EDGE devices. The results highlight the ability of RAViT to achieve a favorable balance between segmentation accuracy and computational efficiency.

The RAViT backbones demonstrate competitive or superior performance in both bounding box and mask precision. RAViT-S26 achieves an $AP_b$ of 40.4 and an $AP_m$ of 37.2, outperforming PVT-S and PoolFormer-S24. RAViT-M26 achieves the highest scores among all methods, with 41.6\% of $AP_b$   and 38.9\% of $AP_m$, surpassing RepViT-M1.1 while only using the convolution mixer in accuracy, while maintaining comparable efficiency. These results validate the effectiveness of the RAViT architecture in improving instance segmentation performance.

Latency and inference speed further highlight the efficiency of the RAViT architecture. RAViT-S26 achieves the highest GPU inference speed of 40.0 images per second, significantly outperforming PVT-S $2.4\times$ and 8.25\% faster than RepViT-M1.1. On EDGE devices, RAViT-S26 achieves the lowest latency at 284 ms, making it highly suitable for real-time applications. RAViT-M26 also delivers competitive EDGE latency at 334 ms, outperforming several other methods in the comparison.

Compared to PoolFormer, PVT, and RepViT, the proposed RAViT architecture strikes an excellent balance between accuracy and efficiency. RAViT-M26 achieves the highest accuracy across the evaluated models, while RAViT-S26 stands out for its minimal latency and high inference speed. These results demonstrate the scalability of RAViT, offering lightweight and high-performance variants that cater to diverse deployment scenarios. In conclusion, the proposed RAViT backbones are well-suited for both performance-driven and latency-critical applications, solidifying their value in instance segmentation tasks.

\begin{table*}[!ht]
\centering
\caption{Object detection on BDD100K and TJU-DHD-Traffic dataset with  FCOS-RAViT. $AP^b$ denote bounding box average precision. The FLOPs, GPU and EDGE inference throughput are measured at resolution 1280 $\times$ 720.
}
\begin{tabular}{ c|m{1.9cm}|m{1.9cm}|>{\centering}m{0.7cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|>{\centering}m{0.7cm}|>{\centering}m{0.7cm}|>{\centering}m{0.7cm}|>{\centering}m{0.7cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|c} \hline
\multirow{2}{*}{Dataset}&\multirow{2}{*}{Network} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{$AP$} & \multirow{2}{*}{$AP_{50}$}& \multirow{2}{*}{$AP_{75}$} & \multirow{2}{*}{$AP_{s}$}  & \multirow{2}{*}{$AP_{m}$} & \multirow{2}{*}{$AP_{l}$} & Param & FLOPs & GPU & EDGE \\
           & &  & & &  &  &  &  & (M) & (G) & (img/s) & (img/s)   \\ \hline
\multirow{6}{*}{BDD100K}&YOLOF\cite{mboutayeb2024fcosh}&ResNet-50\cite{he2016deep}& 24.5 & 45.1 & 22.9 & 6.6 & 30.3 & 51.0 & 42.5 & 94.5& 60.7 & 9.9  \\ 
& FCOS\cite{mboutayeb2024fcosh}&ResNet-50\cite{he2016deep}
                        &29.0 &52.9 &26.8 &12.3 &34.7 &50.6 &31.9 &181.6 &39.3 & 6.8  \\
&FCOS\cite{mboutayeb2024fcosh}&ResNet-101\cite{he2016deep} 
                        &30.0 &54.2 &27.9 &12.6 &35.8 &52.2 &50.8 &251.3 &28.2 &-\\ \cline{2-13}
& \multirow{2}{*}{FCOS\cite{9010746}}&RAViT-S26 &30.2 &54.6 &28.3 &12.7 &35.2 &52.3 &17.4 &132.6 &51.8 & 8.6 \\
                    &  &RAViT-M26 &30.5 &55.3 &28.5 &13.2 &35.1 &53.2 &24.6 &152.8 &45.7 & 7.6 \\ \cline{2-13}
&\multirow{2}{*}{Fast-COS} &RAViT-S26 &31.1 &56.6 &29.3 &14.0 &36.5 &51.7 &14.7 &120.9 &57.0 & 9.3 \\
                    &  & RAViT-M26 &31.8 &57.2 &30.1 &14.4 &37.2 &53.6 &21.9 &141.2 &49.6 & 8.1 \\
\hline
\multirow{6}{*}{TJU-DHD}&$^\dagger$RetinaNet\cite{ross2017focal}&ResNet-50\cite{he2016deep} & 53.5 & 80.9 & 60.0 & 24.0 & 50.5 & 68.0 & 36.4 & 216.1 & 33.7 & 6.9 \\ 
&$^\dagger$FCOS\cite{9010746}&ResNet-50\cite{he2016deep}  
                        &53.8 &80.0 &60.1 &24.6 &50.6 &50.6 &32.1 &211.6 &33.0 &  6.9  \\ \cline{2-13}
&\multirow{2}{*}{$^\dagger$FCOS\cite{9010746}}& RAViT-S26 &53.9 &79.3 &61.0 &26.2 &50.4 &69.1 &17.4 &143.9 &48.5 &8.6 \\
                      & & RAViT-M26 &54.5 &80.0 &61.4 &27.2 &50.7 &70.0 &24.6 &165.9 &47.7 &7.6 \\
\cline{2-13} 
&\multirow{2}{*}{Fast-COS} &RAViT-S26 &53.4 &79.2 &60.3 &26.1 &49.7 &68.3 &17.4 &120.9 &56.8 & 9.3 \\
                    & & RAViT-M26 &54.5 &80.0 &61.4 &26.9 &50.5 &69.4 &21.9 &141.2 &49.4 &8.1 \\
                      
\hline
\end{tabular}
\label{tab:driver-scene}
\end{table*}

\subsection{Evaluation Result of Fast-COS on driving scene Object Detection Task}
\subsubsection{Setup} The proposed model was tested on two high-resolution, large-scale datasets: BDD100K and TJU-DHD-traffic, both suited for assessing detection networks from a driver's viewpoint. These datasets include diverse scenes such as urban streets and residential areas, with 1.84 million and 239,980 annotated bounding boxes, respectively. BDD100K features 10 categories, including Bus and Car, while TJU-DHD covers 5 categories like Pedestrian and Cyclist, available under different lighting conditions. Both datasets offer scenarios in various weather conditions, providing a valuable resource for real-world model testing. Organization comprises 45,266 training images and 5,000 validation images for TJU-DHD Traffic, and 70,000 training images and 10,000 validation images for BDD100K.

In the driving scene object detection experiment, we use an input size of 1280 Ã— 720 pixels in both training and evaluation. Training utilizes 4 NVIDIA GPUs, each handling a mini-batch of 8 images. The AdamW optimizer governs training, starting with a learning rate of 0.0001, which is reduced by a factor of 10 at the 8th and 11th epochs. The model undergoes end-to-end training with RAViT, initialized from pre-trained weights for efficient learning. Data augmentation, including random flipping and resizing, is applied for robustness. During inference, the top 100 detection bounding boxes per image are recorded for performance assessment.
\subsubsection{Benchmarking on BDD100K and TJU-DHD Traffic}
A comparative study illustrated in Table \ref{tab:driver-scene} showcases the performance of Fast-COS utilizing RAViT backbone variants, in comparison with original FCOS models, evaluated on the BDD100K and TJU-DHD Traffic datasets. The findings demonstrate that the RAViT backbone can enhance the inference speed of FCOS by 62\% over the original FCOS-ResNet-101, when juxtaposed with FCOS-RAViT-M26. Additionally, the RAViT backbone contributes to an increase in prediction accuracy, indicated by a 2\% improvement in $AP_{50}$ in the BDD100K dataset test. By integrating the RAViT-M26 model with the RepFPN, which includes the use of RepMSDW, the Fast-COS can achieve $AP_{50}$ accuracy 5.5\% higher along with a 75.9\% improvement in GPU inference speed when compared to the FCOS-ResNet-101. 

In the TJU-DHD Traffic dataset, employing the RAViT-M26 as a backbone enhances the overall $AP$ by 1.3\%, with a 38\% enhancement in predicting large objects ($AP_l$). Utilizing the same input size configuration ($1333\times800$), the proposed RAViT backbone also increases detection speed by 44.5\% compared to the original FCOS. To achieve a detection speed comparable to the BDD100K test, the Fast-COS model variant was evaluated with an input size of $1280\times720$ pixels. In this configuration, Fast-COS boosts detection speed by 49.6\% relative to the original FCOS, while maintaining similar prediction accuracy to the FCOS-RAViT variant trained with a larger input size.

In the evaluation conducted on the Edge device utilizing Jetson Orin Nano with the ONNX framework, the peak performance of the Fast-COS variant is observed when employing RAViT-S26, achieving a prediction speed of 9.3 FPS across both driving scene dataset tests. Despite being 6.5\% slower than YOLOF-ResNet50, Fast-COS with RAViT-S26 attains a 26.9\% improvement in overall average precision $AP$ in BDD100K test. The comprehensive hardware assessment indicates that while several models can exceed 30 FPS for GPU inference speed, optimization on the EDGE device is essential to attain at least 15 FPS for it to be viable as a real-time driving scene object detection hardware option.

\begin{figure*}
\centering
         \includegraphics[width=5.8cm]{picture/obj_det_result/fcos_1.jpg}
         \vspace{0.25mm}
         \includegraphics[width=5.8cm]{picture/obj_det_result/fast-cos_1.jpg}
         \vspace{0.25mm}
         \includegraphics[width=5.8cm]{picture/obj_det_result/gt_1.jpg}
         \vspace{0.25mm}
         \includegraphics[width=5.8cm]{picture/obj_det_result/fcos_fe1d9184-d144106a.jpg}
         \vspace{0.25mm}
         \includegraphics[width=5.8cm]{picture/obj_det_result/fast-cos_fe1d9184-d144106a.jpg}
         \vspace{0.25mm}
         \includegraphics[width=5.8cm]{picture/obj_det_result/fe1d9184-d144106a.jpg}
         \vspace{0.25mm}
         \includegraphics[width=5.8cm]{picture/obj_det_result/fcos_b1ca2e5d.jpg}
         \vspace{0.25mm}
         \includegraphics[width=5.8cm]{picture/obj_det_result/fast-cos_b1ca2e5d.jpg}
         \vspace{0.25mm}
         \includegraphics[width=5.8cm]{picture/obj_det_result/b1ca2e5d.jpg}
         \vspace{0.5mm}
         \includegraphics[width=5.8cm]{picture/obj_det_result/fcos_2.jpg}
         \includegraphics[width=5.8cm]{picture/obj_det_result/fast-cos_2.jpg}
         \includegraphics[width=5.8cm]{picture/obj_det_result/gt_2.jpg}
         \\(1)\hspace{5.8cm}(2)\hspace{5.8cm}(3)
\caption{The detection results using (1). FCOS+ResNet-50 and (2). Fast-COS+RAViT-M26 compare to (3). ground truth}
\label{fig:qualitativeobj}
\end{figure*}

\subsection{Ablation Experiment}
\subsubsection{Multi-Scale in RepMSDW and Combination with Self-Attention}
We conducted an ablation study on Multi-Scale convolution kernel sizes and compared them against sole square kernel reparameterization and configurations without kernel reparameterization. This analysis was specifically performed using the RAViT-S22 variant. As depicted in Table \ref{tab:ablation_token_mixer}, adopting Multi-Scale kernel sizes during reparameterization results in enhanced classification accuracy on the ImageNet-1K dataset. Utilizing multi-scale reparameterization offers a 0.12\% improvement over single square reparameterization, as referenced in \cite{wang2024repvit, vasu2023fastvit, ding2021repvgg}, and a 0.22\% gain over configurations without reparameterization. 

Given that RepMSDW is constrained in capturing long dependencies, we evaluated its integration with self-attention, a technique employed in Transformer models \cite{li2022efficientformer, liu2023efficientvit, liu2021swin}. Initially, we incorporated RepMSDW in Multi-Head Self-Attention (MHSA) to substitute the $3\times3$ DWConv, as described in \cite{li2023rethinking}, with a $7\times7$ RepMSDW. This modification increased accuracy to 79.2\%. However, MHSA requires numerous array transformations, impacting GPU throughput and NPU latency. Subsequently, we adopted single-head attention (SA), as proposed in \cite{yun2024shvit}, to address the computational redundancies in MHSA. Combining RepMSDW with SA elevated accuracy to 79.6\% while maintaining efficiency in both GPU throughput and NPU latency.
\begin{table}[!ht]
\centering
\caption{Ablation of RepMSDW and RepSA on ImageNet-1K with 224$\times$224 image size. GPU denotes the throughput in image/s and NPU denotes the latency in iPhone 15 Pro.}
\begin{tabular}{ m{1.2cm}|>{\centering}m{1.4cm}|>{\centering}m{0.5cm}|>{\centering}m{0.8cm}|>{\centering}m{0.6cm}|>{\centering}m{0.6cm}|c } \hline
\multirow{2}{*}{Ablation} & \multirow{2}{*}{Variant} & Par & FLOPs & GPU   & NPU & \multirow{2}{*}{Top-1}  \\ 
                         &                           & (M) &  (G)  & img/s & ms  &        \\ \hline
\multirow{2}{*}{RepMSDW}& Multi-Scale & 9.3 & 1.12 & 3632 & 0.77 & 79.12 \\ \cline{2-7}
                        & Square      & 9.3 & 1.12 & 3632 & 0.77 & 79.0 \\ \cline{2-7}
                        & w/o Rep     & 9.3 & 1.12 & 3632 & 0.77 & 78.9\\ \hline
\multirow{3}{*}{RepSA} & RepMSDW$+$ MHSA & 12.7 & 1.31 & 3255 & 0.99 & 79.2  \\ \cline{2-7}
                       & RepMSDW$+$ SA  & 10.2 & 1.17 & 3491 & 0.80 & 79.6  \\ %\cline{2-6}
\hline

\end{tabular}
\label{tab:ablation_token_mixer}
\end{table}
\subsubsection{Combination in RAViT Backbone architecture}
Following the ablation of RepMSDW and RepSA, we also conducted an ablation study on the macro architecture. The investigation commenced with a comparison between the 3-stage (V1) and the 4-stage (V2) architectures. The findings from this analysis are detailed in Section \ref{subsec:abl-macro}, where V2 was adopted as the baseline configuration. Subsequently, we experimented with increasing the RepMSDW kernel size in stages three and four from $K=3$ to $K=7$ (V3), resulting in a 0.5\% enhancement in Top-1 accuracy, accompanied by a reduction in inference speed of 1\%, 2.5\%, and 10\% for NPU, EDGE, and GPU respectively. Incorporating RepSA in stage four (V4) yields a 0.9\% improvement in accuracy, but inference speed decreases by 3.8\%, 19\%, and 22\% on NPU, EDGE, and GPU, respectively. In the final ablation, we applied RepSA in both stages three and four, achieving a 1.7\% boost in accuracy, though inference speed slowed by 22.1\%, 44.9\%, and 78.6\% on NPU, GPU, and EDGE respectively. Ultimately, V4 was selected as the primary configuration for the RAViT backbone to balance between speed and accuracy.
\begin{table}[!ht]
\centering
\caption{Ablation of RAViT on ImageNet-1K with 224$\times$224 image size. M and A denote RepMSDW and  RepSA then followed by kernel size and number of blocks. C denotes a convolution followed by kernel size, stride, and number of blocks.}
\begin{tabular}{c|c|>{\centering}m{1.0cm}|>{\centering}m{0.9cm}|>{\centering}m{0.9cm}|>{\centering}m{0.9cm}|c} \hline
\multicolumn{2}{c|}{Config} & V1      & V2      & V3      & V4 & V5 \\ \hline
\multicolumn{2}{c|}{Stem} & C3S2$\times4$      & \multicolumn{4}{c}{C3S2$\times2$}\\ \hline
\multirow{4}{*}{Stage}  & 1  & M3$\times4$ & M3$\times2$ & M3$\times2$ & M3$\times2$ & M3 $\times2$  \\
                        & 2  & M3$\times16$ & M3$\times4$ & M3$\times4$ & M3$\times4$ & M3 $\times4$  \\
                        & 3  & M3$\times4$ & M3$\times12$ & M7$\times12$ & M7$\times12$& A7$\times12$ \\
                        & 4  & - & M3$\times4$ & M7 $\times 4$ & A7 $\times4$  & A7 $\times4$  \\ \hline
\multicolumn{2}{l|}{Param (M)}     & 10.0  & 9.18    & 9.3     & 10.2    & 10.9 \\ \hline
\multicolumn{2}{l|}{FLOPs (G)}     & 0.32  & 1.10    & 1.12    & 1.17    & 1.33  \\ \hline
\multicolumn{2}{l|}{GPU (img/s)}   & 13484 & 4059   & 3632    & 3316    & 2801 \\ \hline
\multicolumn{2}{l|}{NPU (ms)}      & 0.73  & 0.77    & 0.79    & 0.80    & 0.94 \\ \hline
\multicolumn{2}{l|}{EDGE (ms)}     & 10.0  & 9.8     & 9.9     & 11.7    & 17.5    \\ \hline
\multicolumn{2}{l|}{Top-1}         & 75.8  & 78.7    & 79.1    & 79.6    & 80.4 \\ \hline

\end{tabular}
\label{tab:ablation_macro}
\end{table}
\subsubsection{Fast-COS ablation}
Table \ref{tab:driver-scene} presents the ablation study of Fast-COS utilizing the RAViT variant as the backbone and RepFPN as the intermediary component of the original FCOS head detector. When compared to the standard FCOS employing ResNet-101 on the BDD100K dataset, incorporating RAViT-M26 enhances the detection accuracy for small $(AP_s)$ and large $(AP_l)$ objects by 4.8\% and 1.9\%, respectively. The system employing a reparameterizable RAViT backbone, particularly through the RepMSDW residual connection reparameterization, boosts the GPU inference speed by 62.1\%. By integrating RepMSDW into the FPN, notable improvements in $(AP_l)$, $(AP_m)$, and $(AP_s)$ are recorded at 2.7\%, 3.9\%, and 14.2\%, respectively. Moreover, the RepFPN version, functioning with merely three levels of feature extraction, enhances the GPU inference speed by 75.9\%. 

\subsection{Visualization Results}
Fig. \ref{fig:qualitativeobj} illustrates the comparison of detection results achieved by Fast-COS and the baseline algorithms. The images originate from the BDD validation set, which encompasses urban and suburban traffic scenes as well as traffic scenes in adverse weather conditions. Observing the detection results, it is evident that Fast-COS, through reparameterized multi-scale kernel size convolutions and reparameterized self-attention mechanisms, effectively resolves occlusion issues (such as occlusion between vehicles) and enhances detection performance relative to the baseline. This is particularly noticeable when detecting small objects like a signal light, traffic sign, or distant pedestrian. 

Through an expanded explanation, it is demonstrated within the first and second group of examples that, in urban traffic scenarios during daylight conditions, Fast-COS exhibits the ability to identify several traffic signs located at a considerable distance, in addition to an obstructed bus and automobile positioned in the background. In contrast, the baseline method is inadequate in its predictive capabilities. Moreover, in the third and fourth groups, Fast-COS effectively discerns a diminutive and partially hidden vehicle situated between two larger entities, even under nighttime conditions.



