\section{Introduction}
\label{sec:intro}
Autonomous driving systems represent a major breakthrough in transportation by allowing vehicles to operate without human involvement. These systems usually integrate various sensors as perception systems to collect real-time traffic data, enabling independent navigation \cite{betz2022autonomous}. Cameras supply essential high-resolution visual data needed for driving scene image processing tasks like object detection \cite{wang2024occludedinst}. Robust perception of the driving environment is crucial for autonomous vehicles in complex scenarios. This requires high accuracy, real-time processing, and resilience. It is crucial to accurately recognize and predict the movements of objects, performing efficient real-time processing to avoid decision-making delays that might lead to traffic congestion or accidents, and to ensure operation in poor weather or low-light conditions. An effective object detection algorithm is vital to the safety and effectiveness of camera-based perception systems in autonomous vehicles \cite{chib2023recent}.

In contemporary object detection architectures within deep learning, the structure typically comprises backbone, neck, and head detector elements \cite{chen2022mixed, wang2025yolov9, farhadi2018yolov3}. Detectors are mainly divided into two categories: two-stage and one-stage. Two-stage models, such as those in the R-CNN family \cite{ yang9847011, li10474576}, emphasize region proposal and feature extraction. These models are known for their precise object localization but face increased computational costs due to the numerous region proposals. Conversely, single-stage detectors like You Only Look Once (YOLO) \cite{Tian9298480} or RetinaNet \cite{ross2017focal} conduct object detection and localization regression in a single network pass. A variant of the single-stage detector is the Fully Convolutional One-Stage detector (FCOS) \cite{9010746,tian2020fcos}, which employs an anchor-free strategy by predicting on a per-pixel basis, thus removing the need for predefined anchor boxes and enhancing computational efficiency. Nevertheless, FCOS is criticized for having an inefficient model architecture due to a substantial backbone and neck. 

Throughout the decade, Convolutional Neural Networks (CNNs), notably ResNet \cite{he2016deep}, have been frequently employed as backbone networks due to their outstanding performance in numerous downstream tasks \cite{Djenouri9757754}, including biometric recognition \cite{hsu2024inpainting, li2021identity}, medical segmentation \cite{tan2023multi}, and image dehazing \cite{li2024ma}. Nevertheless, owing to the constraints of the receptive field and short-range dependencies, they encounter issues with occlusion, especially prevalent in driving scene object detection \cite{wang2023centernet}. Recently, vision models based on Transformers have demonstrated exceptional success as backbone networks \cite{liu2021swin} in various computer vision applications, or as segmentation encoder-decoder architectures \cite{chen2023transattunet}, leveraging their global receptive field and long-range dependencies, surpassing CNN performance. However, these models are generally computationally intensive due to their quadratic computational complexity. For instance, the original Vision Transformer (ViT) \cite{dosovitskiy2020image} necessitates 85 million to 632 million parameters for ImageNet classification. This complexity poses challenges for deployment on resource-constrained devices, such as mobile and edge devices, and may not be suitable for some applications, such as driving scene object detection \cite{wang2023centernet} and resource-limited platform deployment. 

Several efficient design approaches have been developed to make transformers efficient or can be implemented in mobile or edge devices \cite{maaz2022edgenext, li2022efficientformer, mehta2022mobilevit, mehta2023separable, li2023rethinking}. The most groundbreaking methods integrate transformers with convolutional neural networks (CNNs) \cite{maaz2022edgenext, mehta2022mobilevit}. Some strategies introduce a novel self-attention model with linear complexity \cite{mehta2023separable} and a dimension-sensitive architecture \cite{li2022efficientformer, li2023rethinking}. These methods demonstrate that CNNs play vital roles in deploying transformers on resource-constrained devices. Alternatively, there's a focus on designing a transformer-oriented vision model at the architectural level for rapid inference \cite{graham2021levit, liu2023efficientvit, yun2024shvit}. Many achieve expedited inference with very low resolution from the initial architectural phase using a 16 $\times$ 16 stem. Moreover, innovative self-attention mechanisms have been introduced to minimize computational redundancy \cite{liu2023efficientvit, yun2024shvit}. While these methods perform well in fast GPU inference, they are less effective for inference on resource-limited hardware with fewer core processors. 

In contrast, the sophisticated architecture of ViTs has inspired recent CNN development \cite{liu2022convnet, vasu2023fastvit, wang2024repvit}. Instead of using combined spatial and channel feature extraction, such as traditional CNN \cite{howard2019searching, sandler2018mobilenetv2}, they design it separately following ViT's architecture. To capture global spatial context, several CNNs try to increase the kernel size to $7 \times 7$ \cite{liu2022convnet, vasu2023fastvit}  instead of using the common $3 \times 3$ \cite{wang2024repvit}. Furthermore, in \cite{ding2022scaling, liu2022more}, they scaled the kernel to $51 \times 51$ to attain a larger receptive field. However, extremely large kernels significantly increase memory access and parameters, making optimization challenging.

To address the trade-off between performance and speed, we introduce RAViT, which denotes Reparameterized Attention Vision Transformer. This hybrid transformer incorporates a reconfigurable token mixer blending attention with multiscale large kernel CNN. On a macro scale, RAViT merges the general framework of lightweight ViT \cite{graham2021levit, liu2023efficientvit, yun2024shvit} with the latest CNN-based design \cite{wang2024repvit}. At the micro level, the Re-parameterized Multi Scale Depth Wise Convolution (RepMSDW) and Re-parameterized Self-Attention (RepSA) are introduced to maintain global and local dependencies. Utilizing the RAViT as the backbone, we optimize FCOS to be more efficient and fast, hence we call it Fast Convolutional One-Stage object detector or Fast-COS. Not only the backbone, in the neck level, the Feature Pyramid Network (FPN) of FCOS is enhanced with RepFPN, which takes advantage of RepMSDW. Extensive evaluations confirm its efficiency across various vision benchmarks, including ImageNet-1K for backbone image classification, and BDD100K and TJU-DHD Traffic for object detection in driving scenes. In summary, our key contributions include:
\begin{itemize}
    \item We propose RAViT, a hybrid vision transformer with multiscale large kernel reparameterization, integrating partial self-attention.
    \item We demonstrate the effectiveness of RAViT as a backbone network for extracting multi-scale features and highlight its potential to enhance the FCOS object detector, achieving high accuracy on the BDD100K and TJU-DHD datasets.
    \item By leveraging RepMSDW, we refined the original FCOS FPN neck for multi-scale feature extraction, showcasing RepFPN's role as the neck along with the RAViT backbone, forming the Fast-FCOS object detector to achieve superior accuracy in the driving scene datasets.
    \item We shows that our models exhibit low latency on various platforms, including mobile, edge devices, and desktop GPUs, which will significantly benefit driving scene object detection systems on diverse hardware.
\end{itemize}

%-------------------------------------------------------------------------

