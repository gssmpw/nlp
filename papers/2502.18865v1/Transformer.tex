\subsection{Settings of Transformer in In-context Learning}\label{subsection_tra1}

\textbf{In-Context Learning Setting}. ICL involves a transformer model processing a sequence of input-output examples to perform inference without parameter updates. Unlike traditional supervised learning, where a model is trained on a fixed dataset and then makes predictions, ICL allows the model to adapt on-the-fly to new queries based on the provided examples. We denote a prompt, containing $n$ in-context examples followed by the ($n+1$)-th query input, as
$
S_{0}=\left(\boldsymbol{z}_1, \boldsymbol{z}_2, \ldots, \boldsymbol{z}_{n}, \boldsymbol{x}_{n+1}\right),
$
where $\left(\boldsymbol{z}_i\right)_{i=1}^n=\left(\boldsymbol{x}_i, \boldsymbol{y}_i\right)_{i=1}^n \in \mathcal{Z}=\mathcal{X} \times \mathcal{Y}$ represents i.i.d. in-context samples, and $\boldsymbol{x}_{n+1} \in \mathcal{X}$ is the query input whose label we want to predict. The transformer model, denoted as $\mathrm{TF}(\cdot)$, takes the prompt $S_0$ as input and outputs the predicted label $\hat{\boldsymbol{y}}_{n+1}$ for the query $\boldsymbol{x}_{n+1}$: $\hat{\boldsymbol{y}}_{n+1}=\mathrm{TF}(S_0)$.

\textbf{Recursive Data Generation in STLs with ICL}. We extend the traditional ICL setting to an STL, where the transformer recursively generates new data using its own ICL predictions. Starting with an initial real dataset $S_0$, this serves as the initial real in-context examples for the transformer. The process begins by sampling the first generation queries $\left\{\boldsymbol{x}_{1, j}\right\}_{j=1}^n$ i.i.d. from the input distribution $\mathcal{X}$. Each query $\boldsymbol{x}_{1, j}$ is incorporated into the in-context examples from $S_0$ as a new query $\boldsymbol{x}_{0,n+1}$, and the transformer predicts the corresponding label $\hat{\boldsymbol{y}}_{1, j}$. This produces a synthetic dataset $S_1$, consisting of inputs $\left\{\boldsymbol{x}_{1, j}\right\}_{j=1}^n$ and their predicted labels $\left\{\hat{\boldsymbol{y}}_{1, j}\right\}_{j=1}^n$. A mixed dataset $\widetilde{S}_j$ is then formed and used as the in-context examples for the next generation. This process continues, with each generation producing a new synthetic dataset $S_{j+1}$ based on the updated mixed dataset $\widetilde{S}_j$.


\subsection{Recursive Stability of In-Context Learning with Transformers}\label{subsection_tra2}
In this section, we demonstrate that transformers exhibit recursive stability within the ICL framework. Following the ICL setting from \cite{li2023transformers}, we show that the model effectively controls error propagation from perturbations in the initial real dataset, ensuring stability across the STLs.
\begin{theorem}\label{therorem_stability of transformer} Let $S_{0}, S_0^{\prime}$ be two initial real datasets that only differ at the inputs $\boldsymbol{z}_j=\left(\boldsymbol{x}_j, \boldsymbol{y}_j\right)$ and $\boldsymbol{z}_j^{\prime}=$ $\left(\boldsymbol{x}_j^{\prime}, \boldsymbol{y}_j^{\prime}\right)$ where $1\leq j\leq n$. Assume the inputs and labels lie within the unit Euclidean ball in $\mathbb{R}^d$. Represent the prompts $S_{0}$ and $S_0^{\prime}$ as matrices $\boldsymbol{Z}_0, \boldsymbol{Z}_0^{\prime} \in \mathbb{R}^{(2n+1) \times d}$. Let $\mathrm{TF}(\cdot)$ be an $L$-layer transformer. Given $\boldsymbol{Z}_{0}$ as the initial input, the $k$-th layer applies MLPs and self-attention, producing the output:
$$
\left.\boldsymbol{Z}_{k}=\operatorname{Parallel\_\operatorname {MLPs}(ATTN}\left(\boldsymbol{Z}_{k-1}\right)\right) \text { where } \operatorname{ATTN}(\boldsymbol{Z}):=\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W} \boldsymbol{Z}^{\top}\right) \boldsymbol{Z} \boldsymbol{V}.
$$
Assume $\mathrm{TF}$ is normalized as $\|\boldsymbol{V}\| \leq 1,\|\boldsymbol{W}\| \leq B_W $ and $\operatorname{MLPs}$ obey $\operatorname {MLP}(\boldsymbol{z})=\operatorname{ReLU}(\boldsymbol{M} \boldsymbol{z})$ with $\|\boldsymbol{M}\| \leq 1$. Let $\mathrm{TF}$ output the last token of the final layer $\boldsymbol{Z}_{L}$ that corresponds to the query $\boldsymbol{x}_{j, n+1}$. Let $n$ represent the sample size of the mixed dataset $\widetilde{S}_j$, where $\widetilde{S}_j=\alpha S_0+(1-\alpha) S_j$ for $1 \leq j \leq i$. Then, we obtain:
\begin{align}
     \left\|\operatorname{TF}(\widetilde{S}_i)-\operatorname{TF}(\widetilde{S}_i^{\prime})\right\|_{\ell_2}\lesssim 
   (1-\alpha)^i \frac{\widetilde{B}_W^{(i+1)L}}{2n+1},\notag 
\end{align}
where $\widetilde{B}_W=\left(1+2 B_W\right) e^{2 B_W}$ and $\widetilde{S}_i^{\prime}$ denotes the mixed dataset at the $i$-th generation in the STL when the initial real dataset is $S_0'$. Additionally, if the measure $d$ for the recursive stability parameter in Definition \ref{iterative stability} is taken as the $\ell_2$ norm, then the recursive stability $\gamma_n^i \lesssim  (1-\alpha)^i \frac{\widetilde{B}_W^{(i+1)L}}{2n+1}$.    
\end{theorem}

\begin{remark}\textbf{Controlling Exponential Growth with Real Data Proportion}.\label{remark_stability of transformer} In this remark, we further investigate the influence of the proportion of real data $\alpha$ on the recursive stability of transformers. As outlined in Theorem \ref{therorem_stability of transformer}, the upper bound of the recursive stability parameter includes a term that grows exponentially with the number of generations $i$ in the STL, specifically $\widetilde{B}_W^{(i+1) L}$. However, we show that even a constant proportion of real data, $\alpha$, is sufficient to control this growth.

Specifically, setting $\alpha=\Omega(1-\widetilde{B}_W^{-((i+1) L)/i})$, we establish that the recursive stability parameter in Theorem \ref{therorem_stability of transformer} satisfies $\gamma_n^i \lesssim \frac{1}{2 n+1}$. Additionally, as the number of generations $i$ in the STL approaches infinity, the proportion $\alpha$ asymptotically converges to $1-\widetilde{B}_W^{-L}$. Notably,  the depth \(L\) is typically small in practical settings. For example, studies on LLM performance in STLs, such as \cite{briesch2023large}, often employ models with \(L = 6\). Furthermore, techniques like layer normalization effectively constrain the norm of weights \(B_W\), ensuring numerical stability during training. Thus, with a constant real data proportion $\alpha$ independent of the STL generation number $i$, the exponential growth term $\widetilde{B}_W^{(i+1) L}$ can be effectively controlled, ensuring that $\gamma_n^i=\mathcal{O}(1 / n)$.
    
\end{remark}




\subsection{Generalization Bound for Transformers in In-Context Learning}\label{subsection_tra3}
In this section, we investigate the behavior of transformers under the ICL framework in STLs. We select SGD as the learning algorithm $\mathcal{A}$ and consider a binary task with $\mathcal{Y} = \{0,1\}$. Applying our general theoretical framework from Theorem \ref{theorem_generalization}, we derive the generalization error bound by addressing the terms $\beta_n$ and $d_{\mathrm{TV}}(n)$ using recent results on SGD \citep{zhang2022stability} and ICL \citep{zhang2023and}. The recursive stability parameter $\gamma_n^i$ is obtained from Theorem \ref{therorem_stability of transformer}. We assume that the loss function $\ell(\cdot; z)$ is $\kappa$-smooth and $\rho$-Lipschitz, which are standard assumptions in related works \citep{hardt2016train,lei2020fine}, with formal definitions provided in Appendix \ref{appedix_definitu}. Examples include logistic and Huber losses. We now present the generalization error bound:


\begin{theorem}\label{theo_transformer_generalization}
Consider an $L$-layer transformer under the setting described in Theorem \ref{therorem_stability of transformer}. Let $n$ represent the sample size of the mixed dataset $\widetilde{S}_j$, where $\widetilde{S}_j=\alpha S_0+(1-\alpha) S_j$ for $1 \leq j \leq i$. Suppose that the loss function $\ell(\cdot ; \boldsymbol{z})$ is $\kappa$-smooth, $\rho$-Lipschitz and bounded by $M>0$ for every $\boldsymbol{z}$. Let $\mathcal{A}(\widetilde{S}_i)$ denote the output after running SGD for $T\gtrsim n$ iterations with a step size $\eta_t=\mathcal{O}(\frac{1}{\kappa t})$ on the mixed dataset $\widetilde{S}_i$. Then, for any $\delta \in(0,1)$, with probability at least $1-\delta$, the following holds:
\begin{align}
    &\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right|\lesssim n^{-1/2}\log (n) M\rho^2 \alpha \sqrt{1-\alpha}\log \frac{1}{\delta}\notag\\
    &\quad+n^{-1}\log^2(n)\rho^2((1-\alpha)\widetilde{B}_W^L)^i \alpha  \log (\frac{1}{\delta}) +n^{-1/4}\alpha^{-1} M\left(1-(1-\alpha)^i\right) \log (\frac{1}{\delta}).
\end{align}
\end{theorem}
\begin{remark}
    In this remark, we provide a detailed explanation of the theoretical results of Theorem \ref{theo_transformer_generalization}. As discussed earlier in Remark \ref{remark_stability of transformer}, $\alpha$ is set to $1-\widetilde{B}_{W}^{-L}$. To enhance clarity and focus on the primary results, we omit constant terms and the $\log (1 / \delta)$ factor. Consequently, the bound in Theorem \ref{theo_transformer_generalization} can be expressed as follows:
 \begin{align}
\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right| \lesssim n^{-1 / 2} \log (n)+n^{-1} \log ^2(n)+n^{-1 / 4}. \notag
\end{align}
In this bound, the terms $n^{-1 / 2} \log (n)+n^{-1} \log ^2(n)$ correspond to the generalization error on the mixed dataset, while the term $n^{-1 / 4}$ represents the cumulative distribution shift across generations, which is primarily governed by the learnability of the generative model.

It is evident from this result that the generative model's capacity plays a crucial role in the performance within the STLs. The ability of the generative model to maintain distributional fidelity over multiple generations directly impacts the generalization error and determines how well the model can control the propagation of errors across generations.
\end{remark}








\subsection{Synthetic Data Augmentation}\label{subsection_tra4}
%The previous theorem considers the scenario where the training dataset is inadvertently contaminated by synthetic data, resulting in a self-consuming loop. In addition to this, many researchers deliberately incorporate synthetic data into the real dataset to augment the training set, which also creates a self-consuming loop. Next, we explore this Expanding Data Cycle scenario, as outlined in paper [a]. Formally, we consider an initial real dataset $S_0$ containing $n$ data points. The transformer, during in-context learning inference, generates a first-generation synthetic dataset $S_1$ containing $\lambda n$ data points. Thus, the first-generation mixed dataset is $\widetilde{S}_1=S_0+S_1$. By feeding the mixed dataset $\widetilde{S}_1$ into the model, it produces a second-generation synthetic dataset with $\lambda n$ data points. Consequently, for the $i$-th generation, the mixed dataset is given by $\widetilde{S}_i=\sum_{j=0}^i S_j$. 
The previous theorem addresses the scenario where the training dataset is unintentionally contaminated by synthetic data, leading to STLs. In contrast, many researchers intentionally incorporate synthetic data to augment the real dataset, also creating STLs. Next, we explore this synthetic data augmentation scenario, where each generation's synthetic data is added to the mixed dataset, i.e., $\widetilde{S}_i = \sum_{j=0}^i S_j$.

\begin{theorem}\label{theorem_expanding cylce}
    Consider an $L$-layer transformer under the setting described in Theorem \ref{therorem_stability of transformer}. Let $n$ and $\lambda n$ represent the sample size of the real dataset $S_0$ and the synthetic dataset $S_j$, respectively, where $1 \leq j \leq i$. The mixed dataset $\widetilde{S}_i$ is denoted as $\sum_{j=0}^i S_j$. Suppose that the loss function $\ell(\cdot ; \boldsymbol{z})$ is $\kappa$-smooth, $\rho$-Lipschitz and bounded by $M>0$ for every $\boldsymbol{z}$. Let $\mathcal{A}(\widetilde{S}_i)$ denote the output after running SGD for $T\gtrsim n$ iterations with a step size $\eta_t=\mathcal{O}(\frac{1}{\kappa t})$ on the mixed dataset $S_i$. Then, for any $\delta \in(0,1)$, with probability at least $1-\delta$, the following holds:
\begin{align}
    &\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right| \lesssim n^{-\frac{1}{4}}\log ((1+i\lambda)n)M\log\frac{1}{\delta}\notag \\
    &\quad+ n^{-1}\frac{\rho^2}{(1+i\lambda)^2} \log ((1+i\lambda) n)i!\widetilde{B}_W^{(i+1) L}\log\frac{1}{\delta}+n^{-\frac{1}{2}}\frac{Mi}{1+i\lambda} \sqrt{\log\frac{1}{\delta}}. \notag
\end{align}
\begin{remark}\textbf{Analyzing the Trade-off in Synthetic Data Augmentation for STLs}. In this remark, we examine the trade-off between generalization and distribution shifts from increased synthetic data, providing insights into optimal synthetic data size. At each generation, $\lambda n$ synthetic data points are added to the mixed dataset. We analyze how the coefficient $\lambda$, representing the scale of synthetic data augmentation, affects the generalization error in STLs. From the bound in Theorem \ref{theorem_expanding cylce}, we observe that the \textbf{Cumulative Distribution Shift Across Generations} term is expressed as:
$$
n^{-\frac{1}{4}} \log ((1+i \lambda) n) M \log (1/\delta).
$$
As the coefficient $\lambda$ increases, the cumulative distribution shift correspondingly grows, thereby amplifying the associated error. This behavior aligns with intuition, as an increase in $\lambda$ reduces the proportion of real data within the mixed dataset at each generation. Consequently, this reduction in real data leads to a greater divergence between the mixed distribution and the true underlying distribution, exacerbating the deviation and compounding the error across successive generations. In contrast, for the \textbf{Generalization Error on Mixed Distributions} term:
$$
n^{-1} \frac{\rho^2}{(1+i \lambda)^2} \log ((1+i \lambda) n) i!\widetilde{B}_W^{(i+1) L} \log \frac{1}{\delta}+n^{-\frac{1}{2}} \frac{M i}{1+i \lambda} \sqrt{ \log \frac{1}{\delta}}.
$$
We observe that as $\lambda$ increases, the corresponding error decreases. This outcome is consistent with theoretical intuition, as augmenting the dataset with synthetic data effectively enlarges the mixed dataset. A larger dataset provides a more comprehensive representation of the mixed distribution, which in turn reduces the generalization error associated with this distribution. By incorporating more synthetic data, the mixed dataset better approximates the underlying mixed distribution, leading to improved generalization performance.

From the above discussion, we can conclude that the inclusion of synthetic data introduces a trade-off: on one hand, it increases the error from the cumulative distribution shift, while on the other, it reduces the generalization error on the mixed distribution. This trade-off has been explored theoretically in \cite{futowards}, though they primarily provided theoretical intuition. In contrast, our work explicitly decomposes the error into two terms, offering a deeper understanding of this trade-off and its implications for model performance in STLs. As for the optimal augmentation coefficient $\lambda^*$, it must satisfy the following condition:
\begin{align}
    \lambda^*=\inf_{\lambda} &\Big\{n^{-\frac{1}{4}} \log ((1+i \lambda) n) M \log (1 / \delta) \notag \\
    &\lesssim n^{-1} \frac{\rho^2}{(1+i \lambda)^2} \log ((1+i \lambda) n) i!\widetilde{B}_W^{(i+1) L} \log \frac{1}{\delta}+n^{-\frac{1}{2}} \frac{M i}{1+i \lambda} \sqrt{\log \frac{1}{\delta}}\Big\}. \notag
\end{align}
Unfortunately, obtaining a closed-form solution for $\lambda^*$ from this equation proves to be analytically intractable. However, we can derive the relationship between $\lambda^*$, the size of the real dataset $n$ from the above equation. Specifically, by
omitting irrelevant constants and the $\log(1/\delta)$ term, we obtain that $\lambda^*$ should satisfy the
following expression:
$$
\frac{i!\widetilde{B}_W^{(i+1) L}}{n^{3/4}(1+i\lambda^*)^2}+\frac{i}{n^{1/4}(1+i\lambda^*)\log((1+i\lambda^*)n)}=\mathcal{O}(1).$$
We observe an important trend: the value of $\lambda^*$ increases as the size of the real dataset $n$ decreases. This aligns with theoretical intuition, as a smaller real dataset struggles to adequately represent the underlying distribution, leading to higher generalization error. Consequently, more synthetic data is required to control the generalization error of each generation on the mixed distribution. Conversely, when the real dataset is sufficiently large, the need for synthetic data augmentation diminishes.






%However, we can observe an important trend: as the number of iterations in the self-consuming loop, $i$, increases, the value of $\lambda^*$ tends to decrease. This observation aligns with theoretical intuition, as a greater number of recursive training iterations exacerbates the cumulative distribution shift. To counterbalance this growing shift, the optimal coefficient $\lambda^*$ diminishes, effectively increasing the proportion of real data in the mixed dataset, thereby mitigating the error introduced by the cumulative distribution shift.
    
\end{remark}
\end{theorem}



