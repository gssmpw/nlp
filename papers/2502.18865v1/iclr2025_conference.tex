
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
          % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{bm}
\usepackage{amsthm}
%\usepackage{authblk}
\usepackage{comment}
\usepackage{afterpage}
\usepackage{indentfirst}
\usepackage{enumerate}
\usepackage[titlenumbered,ruled,lined,linesnumbered]{algorithm2e}
%\usepackage[numbers,sort&compress]{natbib}
%\usepackage[backend=bibtex,firstinits=true]{biblatex}$
\usepackage[resetlabels]{multibib} %https://www.overleaf.com/learn/latex/Multibib resetlabels,labeled
% https://tex.stackexchange.com/questions/168749/bibliography-style-with-only-the-initials-of-the-first-names

%\usepackage{algorithm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{exercise}{Exercise}
\newtheorem{claim}{Claim}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}
\theoremstyle{definition}
\newtheorem{property}{Property}
\newtheorem{remark}{Remark}


%\title{A Theoretical Perspective: When Do Self-Consuming Training Loops Generalize}

\title{A Theoretical Perspective: How to Prevent Model Collapse in Self-consuming Training Loops}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{Shi Fu$^{1}$\quad Yingjie Wang$^{1}$\footnotemark[1]\quad Yuzhu Chen$^{2}$\quad Xinmei Tian$^{2}$\quad Dacheng Tao$^{1}$\footnotemark[1]\\[1.2pt]
  $^1$Generative AI Lab, College of Computing and Data Science, \\
 \ \ Nanyang Technological University, Singapore 639798,\\
  $^2$University of Science and Technology of China, Hefei, China \\
   \texttt{fs311@mail.ustc.edu.cn}\textbf{,}\ \texttt{yingjiewang@upc.edu.cn}\textbf{,}\\ \texttt{cyzkrau@mail.ustc.edu.cn}\textbf{,}\ \texttt{xinmei@ustc.edu.cn}\textbf{,}
   \texttt{dacheng.tao@gmail.com}
}

%\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
%about author (webpage, alternative address)---\emph{not} for acknowledging
%funding agencies.  Funding acknowledgements go at the end of the paper.} \\
%Department of Computer Science\\
%Cranberry-Lemon University\\
%Pittsburgh, PA 15213, USA \\
%\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
%\And
%Ji Q. Ren \& Yevgeny LeNet \\
%Department of Computational Neuroscience \\
%University of the Witwatersrand \\
%Joburg, South Africa \\
%\texttt{\{robot,net\}@wits.ac.za} \\
%\AND
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email}
%}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}} 
\footnotetext[1]{Corresponding authors}

\begin{abstract}
%This paper presents the first theoretical generalization analysis of recursive training for generative models on mixed real and synthetic data, known as self-consuming loops. To achieve this, we formally define the novel concept of \textit{recursive stability}, capturing how perturbations in the initial real dataset propagate through these loops, and further derive a generalization bound addressing the complexities of recursive structures and non-i.i.d. data. Our results demonstrate that, when (1) the model satisfies recursive stability and (2) the proportion of real data is kept at a constant level, the generalization error converges, preventing model collapse. We extend this analysis to transformers in in-context learning, exploring the trade-off between generalization and distribution shifts induced by increased synthetic data, offering insights for optimal synthetic data size.

%The quest for high-quality data is paramount in the training of large generative models, yet the vast reservoir of language and visual data available online has become nearly depleted. In response to this challenge, Self-consuming Training Loops (STLs) have emerged, harnessing the power of models to generate their own data for further training. However, the results are strikingly inconsistent: some achieve continuous performance enhancements, while others find their models faltering or even collapsing, with a significant lack of theoretical insight. This paper introduces the intriguing notion of \textit{recursive stability} and reveals how both model architecture and the proportion between real and synthetic data impact the success of STLs under the umbrella of theoretical generalization analysis. The discoveries pave the way for deeper understanding and future advancements of STLs in model training.

%High-quality data is essential for training large generative models, yet the vast reservoir of real data available online has become nearly depleted. Consequently, models increasingly generate their own data for further training, forming Self-Consuming Training Loops (STLs). However, the empirical results have been strikingly inconsistent: some models exhibit continuous performance improvements, while others stagnate or collapse, with a notable lack of theoretical explanations. This paper introduces the intriguing notion of \textit{recursive stability} and presents the first theoretical generalization analysis, revealing how both model architecture and the proportion between real and synthetic data influence the success of STLs. We further extend this analysis to transformers in in-context learning, offering insights into optimal synthetic data sizing and advancing theoretical understanding of STLs.

High-quality data is essential for training large generative models, yet the vast reservoir of real data available online has become nearly depleted. Consequently, models increasingly generate their own data for further training, forming Self-consuming Training Loops (STLs). However, the empirical results have been strikingly inconsistent: some models degrade or even collapse, while others successfully avoid these failures, leaving a significant gap in theoretical understanding to explain this discrepancy. This paper introduces the intriguing notion of \textit{recursive stability} and presents the first theoretical generalization analysis, revealing how both model architecture and the proportion between real and synthetic data influence the success of STLs. We further extend this analysis to transformers in in-context learning, showing that even a constant-sized proportion of real data ensures convergence, while also providing insights into optimal synthetic data sizing.









%This paper addresses the theoretical challenges associated with recursive training of generative models on mixtures of real and synthetic data, termed self-consuming loops. We derive a general generalization bound that captures the complexities arising from the recursive structure and the non-i.i.d. nature of the data. Our key technical contribution is the introduction of recursive stability, a novel concept that quantifies error propagation across generations. Furthermore, we apply our general generalization analysis to transformers within in-context learning under recursive training. Finally, we explore the trade-off between the benefits of synthetic data augmentation for enhancing generalization and its potential to amplify distributional shifts, establishing a rigorous theoretical foundation for advancing generative model training.

%This paper tackles the theoretical challenges of recursive training for generative models on mixed real and synthetic data, known as self-consuming loops. We present the first generalization bound by addressing the complexities posed by recursive structures and non-i.i.d. data. Our key technical contribution is the introduction of \textit{recursive stability}, a novel concept that quantifies error propagation across generations. We prove that when (1) the generative model satisfies recursive stability, (2) the mixed dataset maintains a constant proportion of real data, and (3) a learning algorithm with uniform stability is used, the generalization error converges, thereby preventing model collapse. Furthermore, we apply our generalization analysis to transformers within in-context learning and explore the trade-off between synthetic data augmentation and distribution shifts, providing conditions for the optimal synthetic data size.

%This paper addresses the lack of theoretical generalization analysis in recursive training for generative models on mixed real and synthetic data, known as self-consuming loops. Despite progress in generalization error theory, reliance on i.i.d. assumptions and static training limits applicability to these loops. We introduce \textit{recursive stability}, a novel concept designed to quantify error propagation across generations. Building on this, we derive a generalization bound that accommodates the intricacies of recursive structure and non-i.i.d. data. Our results show that when (1) the generative model satisfies recursive stability, and (2) the proportion of real data is maintained at a non-negligible constant level, the generalization error converges, thereby preventing model collapse. Furthermore, we apply our generalization analysis to transformers within in-context learning, exploring the trade-off between synthetic data augmentation and distribution shifts, and providing conditions for determining the optimal size of synthetic data.

%This paper addresses the lack of theoretical generalization analysis in recursive training for generative models on mixed real and synthetic data, known as self-consuming loops. Despite progress in generalization theory, reliance on i.i.d. assumptions and static training limits applicability to these loops. We introduce \textit{recursive stability}, a novel concept that quantifies error propagation across generations, and derive a generalization bound that addresses the complexities of recursive structures and non-i.i.d. data. Our results show that when (1) the generative model satisfies recursive stability, and (2) the proportion of real data is kept at a constant level, the generalization error converges, preventing model collapse. We extend this analysis to transformers in in-context learning, exploring trade-offs in synthetic data augmentation and offering conditions for optimizing synthetic data size.
\end{abstract}


\section{Introduction}
\input{intro2}

\begin{figure}[t] \label{figure_selfconsuming}
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{ICLR22025.png}}
\caption{Self-consuming Training Loops: The initial model $\mathcal{G}_0$ is trained on the real dataset $S_0$. For generation $1 \leq j \leq i$, the model $\mathcal{G}_j$ is trained on the mixed dataset $\widetilde{S}_j$. 
}
\label{icml-historical}
\end{center}
\vskip -0.2in
\end{figure}

\section{Related Work}
\input{Related_Work}

\section{Preliminaries}
In this section, we begin by formally describing the training process of generative models in STLs, then introduce algorithmic stability with a focus on uniform stability, and finally define recursive stability to address the challenges specific to STLs.



\subsection{Generative Models within Self-consuming Training Loops}
\input{Self-Consuming_Training_of_Generative_Models}

\subsection{Algorithmic Stability}
\input{Stability}


\section{General Theoretical Results}
%n this section, we present our main theoretical results. Specifically, we provide a generalization error bound in Section \ref{section_genral} and extend the theoretical framework to transformers in in-context learning in Section \ref{section_transformer}.

%\subsection{General Generalization Bound}\label{section_genral}
\input{General_Generalization_Bound}

\section{Theoretical Analysis of Transformers in In-Context Learning}\label{section_transformer}
In this section, we first present the transformer in in-context learning (ICL) and its settings within SLTs in Section \ref{subsection_tra1}. In Section \ref{subsection_tra2}, we prove that it satisfies recursive stability, followed by the derivation of the generalization error bound for transformers in ICL in Section \ref{subsection_tra3}. Finally, in Section \ref{subsection_tra4}, we explore the scenario of synthetic data augmentation and investigate the associated trade-offs.
\input{Transformer}

\section{Conclusion}
As real-world data becomes increasingly scarce and existing datasets are progressively contaminated with synthetic content, STLs have emerged as a necessary strategy. STLs enable generative models to recursively train on a mix of real and synthetic data. However, empirical outcomes have varied significantly, revealing the need for a theoretical foundation to guide their successful application.

In this work, we introduced recursive stability as a key technical innovation and established the first generalization error bounds for STLs, which consider the impact of different model architectures. Our analysis demonstrated that preventing model collapse requires two critical conditions: maintaining a non-negligible proportion of real data and ensuring that models satisfy recursive stability. Furthermore, we were the first to extend this framework to transformers in in-context learning, showing that they also satisfy recursive stability and establish their generalization error bounds. Finally, we explored the trade-off introduced by synthetic data augmentation, balancing generalization improvement with potential distributional shifts. These contributions provide new insights into enhancing the stability and performance of generative models in STLs.


\section*{Acknowledgement}
This project is supported by the National Research Foundation, Singapore, under its NRF Professorship Award No. NRF-P2024-001.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Appendix}
\input{Appendix}


\end{document}
