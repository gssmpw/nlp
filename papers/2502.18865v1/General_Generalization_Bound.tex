In this section, we present a general framework for analyzing generalization error. Moving beyond traditional analyses of parameter changes \citep{bertrandstability} and distributional discrepancies \citep{futowards}, we focus on evaluating the utility of synthetic data after recursive training \citep{hittmeir2019utility,xu2023utility}. Specifically, we examine the behavior of a uniformly stable learning algorithm $\mathcal{A}$ trained on the mixed dataset $\widetilde{S}_i$ in the $i$-th generation. Our goal is to study the generalization error of the hypothesis $\mathcal{A}(\widetilde{S}_i)$. Formally, we aim to bound $|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i)) - \widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))|$, where $R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i)) = \mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0}[\ell(\mathcal{A}(\widetilde{S}_i), \boldsymbol{z})]$ represents the population risk of $\mathcal{A}(\widetilde{S}_{i})$ under the real distribution $\mathcal{D}_0$, and $\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i)) = \frac{1}{n} \sum_{\boldsymbol{z}_i \in \widetilde{S}_i} \ell(\mathcal{A}(\widetilde{S}_i), \boldsymbol{z}_i)$ denotes the empirical risk on the mixed dataset. To derive this bound, we first decompose the generalization error as follows.
\begin{align}
\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right| \leq \underbrace{\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))\right|}_{\text {Cumulative distribution shift across generations}}+\underbrace{\left| R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i)) \right|}_{\text {Generalization error on mixed distributions}}. \notag
\end{align}
The first term captures the accumulation of error and distribution divergence over multiple generations within the STLs. This heavily depends on the capacity of the generative model to preserve distributional fidelity across generations, requiring recursive techniques to manage error propagation. The second term reflects the generalization performance of the learning algorithm on the non-i.i.d. mixed dataset, where synthetic data points are influenced by the initial real dataset. Drawing on \cite{zheng2023toward}, we observe that while $S_0$ satisfies the i.i.d. assumption, the synthetic datasets $S_i$ follow a conditional i.i.d. assumption given $S_0$. Leveraging this, along with moment bounds and concentration inequalities, we address the challenge of bounding the second term and managing dependencies within the STLs. We now present the following result.
\begin{theorem}[General Generalization Bound]\label{theorem_generalization}Assume that $\mathcal{A}$ is a $\beta_n$-uniformly stable learning algorithm and the loss function $\ell$ is bounded by $M$. Let $n$ represent the sample size of the mixed dataset $\widetilde{S}_j$, defined as $\widetilde{S}_j=\alpha S_0+(1-\alpha) S_j$ for $1 \leq j \leq i$, where $0<\alpha\leq 1$ denotes the proportion of real data. Assume further that the generative model $\mathcal{G}$ is recursively $\gamma_n^i$-stable, and the TV distance for each generation $T V(\widetilde{\mathcal{D}}_j, \mathcal{D}_{j+1})$ is of the same order, denoted by $d_{\mathrm{TV}}(n)$. Then, for any $\delta \in(0,1)$, with probability at least $1-\delta$, the following holds:
\begin{align}
&\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right| \lesssim \gamma_n^i \alpha M\log (n\alpha)\log(1/\delta)+ n^{-1 / 2}M \sqrt{\log 1/\delta} \notag\\
&\quad+\beta_n\left(\log n \log (1/\delta)+\alpha\sqrt{(1-\alpha)n\log (1/\delta)}\right)+d_{\mathrm{TV}}(n)M\left(1-(1-\alpha)^i\right) \alpha^{-1}, \label{mainthero_1}
\end{align}
where $\gamma_n^i= \sup_{j}TV(\mathcal{D}_{i}^{n(1-\alpha)}(S_{0}'),\mathcal{D}_{i}^{n(1-\alpha)}(S_{0}))$, with $S_0$ and $S_0'$ representing two real datasets of size $n$, differing by only a single data point.
\end{theorem}
\begin{remark}\textbf{Recursive Stability in STLs}. 
In Theorem \ref{theorem_generalization}, the recursive stability parameter is quantified using the TV distance to measure the divergence between the distributions of the $n(1-\alpha)$ synthetic data points generated by the model $\mathcal{G}_i$ at the $i$-th generation. Notably, the concept of recursive stability, introduced in Definition \ref{iterative stability}, is adaptable to various metrics, making it applicable across different types of generative models. In Theorem \ref{therorem_stability of transformer}, the recursive stability parameter for transformers is instead defined using the $\ell_2$ norm between tokens, allowing this concept to be generalized to a broader range of model architectures.

Moreover, Theorem \ref{theorem_generalization} demonstrates that generative models with higher recursive stability exhibit better performance after undergoing the STL. Specifically, the results indicate that the convergence rate of recursive stability parameter is at least faster than $\mathcal{O}(1 / \log n)$, which is a relatively mild condition. Furthermore, Theorem \ref{therorem_stability of transformer} shows that, under mild assumptions, the recursive stability parameter for transformers in in-context learning settings achieves a convergence rate of $\gamma_n^i = \mathcal{O}(1 / n)$ when measured by the $\ell_2$ norm between tokens.



%To better understand the behavior of generative models within self-consuming loops, we introduce the concept of Recursive Stability, a novel measure designed to quantify the differences in a generative model's outputs after multiple rounds of recursive training when small perturbations are applied to the initial real dataset. Notably, the concept of recursive stability, introduced in Definition \ref{iterative stability}, is flexible and adaptable to various contexts. Instead of prescribing a fixed measure for output differences, it accommodates a range of metrics suited to different types of generative models.

%In Theorem \ref{theorem_generalization}, the recursive stability parameter is quantified using the TV distance to measure output discrepancies. However, in Theorem \ref{therorem_stability of transformer}, introduced later, the recursive stability parameter for transformers is instead defined based on the $\ell_2$ norm between tokens. This variation allows the concept to be applied broadly across different model architectures.

%Moreover, Theorem \ref{theorem_generalization} shows that the more recursively stable a generative model is, the better its performance remains after undergoing the self-consuming loop. Specifically, the results of Theorem \ref{theorem_generalization} indicate that the recursive stability parameter's convergence rate is at least faster than $\mathcal{O}(1 / \log n)$, which is a rather relaxed condition. In Theorem \ref{therorem_stability of transformer}, we further demonstrate that, under mild assumptions, the recursive stability parameter for transformers in in-context learning settings achieves a convergence rate of $\gamma_n^i=\mathcal{O}(1 / n)$ when evaluating the $\ell_2$ norm between tokens.



\end{remark}
\begin{remark}\textbf{Effect of Real Data Proportion on Error Control}.\label{remark_real} Previous experimental results \citep{shumailov2024ai,alemohammadself} have demonstrated that incorporating real data can mitigate model collapse and help control errors. This remark focuses on exploring the effect of the real data proportion $\alpha$ on the generalization error within the STLs. As shown in Theorem \ref{theorem_generalization}, the real data proportion $\alpha$ plays a significant role in the cumulative distribution shift across generations, specifically in the term $2 M\left(1-(1-\alpha)^i\right) \alpha^{-1} d_{\mathrm{TV}}(n)$.

When \(\alpha \to 0\), we observe that \(\frac{(1 - (1 - \alpha)^i)}{\alpha} \to i\), leading to a linear accumulation of errors due to the Distribution Shift, making it increasingly challenging to control the overall error. This observation aligns with the theoretical results reported in \cite{shumailov2024ai,dohmatob2024model, futowards}. However, it is important to note that the conditions on $\alpha$ for controlling this term are not strict. In fact, as long as $\alpha$ remains at a non-negligible constant level, the expression $\left(1-(1-\alpha)^i\right) \alpha^{-1}$ remains bounded, effectively controlling the error. This aligns with theoretical intuition: when $\alpha$ is too small, the mixed dataset contains insufficient real data, resulting in a more severe distribution shift.

Moreover, the proportion of real data $\alpha$ also impacts the generalization error on mixed distributions, primarily through its effect on the recursive stability parameter $\gamma_n^i$. As $\alpha$ increases, the generative model becomes more recursively stable. We will further explore the influence of $\alpha$ on the recursive stability parameter $\gamma_n^i$ for specific generative models, such as transformers, in Theorem \ref{theo_transformer_generalization}, particularly in Remark \ref{remark_stability of transformer}.
    
\end{remark}

\begin{remark}\textbf{Convergence Rate of Uniform Stability Parameter}. With respect to the uniform stability parameter $\beta_n$, we observe from the third term on the right-hand side of inequality \ref{mainthero_1} that the convergence rate of $\beta_n$ must be at least $\mathcal{O}(1 / \sqrt{n})$ to adequately control the error. This is a relatively mild requirement.

For example, in the case of widely used algorithms such as SGD, it has been shown that the uniform stability parameter $\beta_n$ converges at a rate of $\mathcal{O}(\log (n) / n)$ under the assumptions of Lipschitz continuity and smoothness of the loss function \citep{zhang2022stability}. Additionally, for regularization-based algorithms, such as kernel regularization schemes and the Minimum Relative Entropy (MRE) algorithm, it has been demonstrated that $\beta_n$ can achieve a convergence rate of $\mathcal{O}(1 / n)$ under certain conditions \citep{bousquet2002stability}.

%These results suggest that the requirement on $\beta_n$ for controlling the generalization error in our setting is relatively mild, as many commonly used algorithms meet or exceed the necessary rate. This makes the control of uniform stability practical and achievable across a wide range of optimization methods.
\end{remark}

\begin{remark}\textbf{Convergence of the Distribution Shift Term $d_{\mathrm{TV}}(n)$}.\label{remark_distrubbutionshift}
Regarding the convergence of the term $2 M\left(1-(1-\alpha)^i\right) \alpha^{-1} d_{\mathrm{TV}}(n)$, as discussed in Remark \ref{remark_real}, when $\alpha$ remains a non-negligible constant, attention turns to the distribution shift term $d_{\mathrm{TV}}(n)$. This term critically depends on the generative model's capacity and quantifies the divergence between the learned distribution and the input distribution in each generation. 

Theoretical studies have provided various convergence rates for $d_{\mathrm{TV}}(n)$ across different generative models. For instance, in diffusion models, $d_{\mathrm{TV}}(n)$ has been shown to converge at a rate of $\mathcal{O}\left(1 / n^{1 / 4}\right)$ \citep{futowards}. Similarly, for GANs, the convergence rate is also $\mathcal{O}\left(1 / n^{1 / 4}\right)$ \citep{liang2021well}. More generally, by applying Pinsker's inequality to relate KL divergence and TV distance, the convergence rates for other models, such as Bias potential models and Normalizing flows, have been explored in previous works \citep{yang2022mathematical}. Additionally, we will further examine the behavior of transformer models in Theorem \ref{theo_transformer_generalization}, demonstrating the flexibility of our theoretical framework across a wide range of generative models.

%For different generative models, theoretical studies have provided various convergence rates for $d_{\mathrm{TV}}(n)$. For instance, in the case of diffusion models, it has been proven that $d_{\mathrm{TV}}(n)$ converges at a rate of $\mathcal{O}\left(1 / n^{1 / 4}\right)$ \citep{futowards}. Similarly, for GANs, research has shown the same convergence rate of $\mathcal{O}\left(1 / n^{1 / 4}\right)$ \citep{liang2021well}. More broadly, using Pinsker's inequality to relate KL divergence and TV distance, the convergence rates for other models, such as Bias potential models and Normalizing flows, have been explored in previous works \citep{yang2022mathematical}.

%Moreover, we will further investigate the behavior of transformer models in Theorem \ref{theo_transformer_generalization}. This highlights the versatility of our theoretical framework, demonstrating its applicability across various types of generative models.
\end{remark}

\begin{remark} \textbf{Comparision with Previous Works}.
In the realm of theoretical research on the STL, where models are recursively trained on the synthetic data they generate, the foundational work was introduced by \cite{shumailov2024ai} and \cite{alemohammadself}. They provided the initial theoretical definitions and analyzed the behavior of a simplistic multivariate Gaussian toy model in such loops. However, their analyses were limited to basic theoretical insights and lacked in-depth exploration of more complex generative models.

Recent advancements in this field have primarily come from \cite{bertrandstability} and \cite{futowards}. \cite{bertrandstability} established an upper bound on the deviation of likelihood-based model output parameters from the optimal ones, denoted as $\left\|\theta_{i}-\theta^*\right\|$. This was achieved by making direct assumptions on the upper bounds of both statistical and optimization errors in generative models, as outlined in their Assumption 3. In contrast, \cite{futowards} derived bounds on the TV distance, addressing the distribution divergence between the synthetic data distributions produced by future models and the original real data distribution, with a specific focus on diffusion models. Our work makes significant theoretical advancements over both \cite{bertrandstability} and  \cite{futowards} in several key aspects:

1. \textbf{Innovative Concept of Recursive Stability.} A central technical contribution of our work is the extension of the traditional notion of algorithmic stability. We define recursive stability, a crucial factor for controlling error propagation across generations. This novel concept tackles the theoretical challenges posed by non-i.i.d. data and recursive structures within STLs, while also incorporating the influence of model architectures into the generalization error. Moreover, recursive stability serves as a new measure for assessing the stability of generative models within STLs. In Theorem \ref{therorem_stability of transformer}, we further establish an upper bound on the recursive stability parameter for transformers under mild conditions, underscoring the broad applicability and robustness of our framework.

2. \textbf{Establishing the First Generalization Error Bound for STLs.} While \cite{bertrandstability} primarily focused on parameter deviations in generative models and \cite{futowards} concentrated on distribution divergence, our work emphasizes the utility of the generated data produced by STLs. Specifically, by utilizing recursive stability, we present the first generalization error bound that quantifies the gap between the population risk on the initial real data distribution $\mathcal{D}_0$ and the empirical risk of the hypothesis $\mathcal{A}(\widetilde{S}_i)$, generated by applying learning algorithms to the synthetic data produced after multiple generations of STLs. This introduces a new layer of complexity compared to prior work, as it necessitates handling not only the distribution shifts within STLs but also the challenges arising from the non-i.i.d. nature of the mixed datasets, where each generationâ€™s data is influenced by all preceding generations.


3. \textbf{A More General Framework Accounting for Model Structure.} Our proposed theoretical framework is more comprehensive than previous studies. \cite{bertrandstability} restricted their analysis to simplified likelihood-based generative models, while \cite{futowards} focused specifically on diffusion models. Importantly, neither of their theoretical results accounted for the impact of different model architectures. In contrast, as discussed in Remark \ref{remark_distrubbutionshift}, our framework explicitly incorporates the effects of varying model structures, thereby extending its applicability to a broader range of generative models. Notably, we are the first to extend the theory of SLTs to transformer models, further broadening the scope of our approach across diverse generative model architectures.

4. \textbf{Comprehensive Collapse Prevention Through Recursive Stability}. In addition to the existing theoretical work, which primarily analyzes conditions to avoid model collapse based on the proportion of real data (e.g., \cite{bertrandstability,futowards}), our work extends these analyses by considering the impact of model architecture. Specifically, Theorem \ref{theorem_generalization} demonstrates that under a recursive stability condition and a non-negligible constant level of real data, model collapse can be avoided across a variety of model architectures. This analysis offers broader conditions for preventing collapse by incorporating recursive stability, deepening the understanding of how model architecture affects training robustness.

%In summary, our work advances the theoretical understanding of STLs by introducing recursive stability, establishing the first generalization error bounds, and incorporating model architecture into the analysis, providing a more comprehensive framework for preventing model collapse across diverse generative models.
\end{remark}



\begin{remark}\textbf{Proof Sketch of Theorem \ref{theorem_generalization}}. We first decompose the generalization error of STLs into two distinct terms: (1) the cumulative distribution shift across generations, and (2) the generalization error on the mixed dataset.

\textbf{Cumulative Distribution Shift:} This term measures the shift between the real dataset $\mathcal{D}_{0}$ and the mixed distribution $\mathcal{D}_i$ after the $i$-th generation. Using the TV distance to quantify the shift introduced by the generative model, we bound the difference as:
$$\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))\right|\leq(1-\alpha)\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{i-1}}(\mathcal{A}(\widetilde{S}_i))\right|+2(1-\alpha)MTV(\widetilde{\mathcal{D}}_{i-1},\mathcal{D}_i).$$
By leveraging the recursive structure of the generative process, this cumulative distribution shift can be bounded across all generations as:
$$|R_{\mathcal{D}_0}(A(S_i))-R_{\mathcal{D}_i}(A(S_i))|\leq2M\left(1-(1-\alpha)^i\right)\alpha^{-1}d_{\mathrm{TV}}(n).$$
\textbf{Generalization Error on the Mixed Dataset:} The second term quantifies the generalization error when training on the mixed dataset $\widetilde{S}_{i}$, which consists of both real and synthetic data. Our goal is to establish a moment bound on the generalization error, which can be decomposed as follows:
$$\|\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_i\in S_{0,\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_i)\|_p+\|(1-\alpha)R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_i\in S_{i,1-\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_i)\|_p.$$
In this context, \(S_{0,\alpha}\) represents 
a proportion \(\alpha\) of the \(n\) data points in \(S_0\), leading to a total 
of \(n \times \alpha\) data points. Similarly, \(S_{i,1-\alpha}\) denotes a 
subset of the synthetic dataset \(S_i\), where \(S_{i,1-\alpha} \subseteq S_i\) 
and its size is \((1 - \alpha) \times |S_i|\).  For each term, we leverage the uniform stability $\beta_n$ of the learning algorithm $\mathcal{A}$ and the
recursive stability $\gamma_n^i$ of the generative model to address the non-i.i.d. nature of the mixed dataset. The mixed dataset exhibits conditional independence \citep{zheng2023toward}, with synthetic data conditioned on the initial real dataset $S_0$, allowing the application of recursive techniques to derive the moment bound. Subsequently, Lemma \ref{theorem_moment} and Lemma \ref{lemma_highprobability} are utilized to derive the high-probability bound for the final result.
\end{remark}
