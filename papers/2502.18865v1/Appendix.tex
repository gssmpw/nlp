\subsection{Auxiliary Definitions}\label{appedix_definitu}
Below, we present some essential definitions.
\begin{definition}(Lipschitz and Smoothness). Let constants $\kappa, \rho > 0$. Consider the function $\ell: \mathcal{W} \times \mathcal{Z} \rightarrow \mathbb{R}$. We define the following properties:
\begin{itemize}
    \item \textbf{Lipschitz Continuity}: The loss $\ell$ is said to be $\rho$-Lipschitz continuous if  $\|\ell(\boldsymbol{w}_1, \boldsymbol{z}) - \ell(\boldsymbol{w}_2, \boldsymbol{z})\| \leq \rho \|\boldsymbol{w}_1 - \boldsymbol{w}_2\|$ for any $\boldsymbol{w}_1, \boldsymbol{w}_2, \boldsymbol{z}$.
    \item \textbf{Smoothness}: The loss $\ell$ is said to be $\kappa$-Smooth  if $\|\nabla_{\boldsymbol{w}}\ell(\boldsymbol{w}_1, \boldsymbol{z})-\nabla_{\boldsymbol{w}}\ell(\boldsymbol{w}_2, \boldsymbol{z})\|\leq \kappa\|\boldsymbol{w}_1-\boldsymbol{w}_2\|$ for any $\boldsymbol{w}_1, \boldsymbol{w}_2, \boldsymbol{z}$.
\end{itemize}
\end{definition}


\subsection{Expansion to Gaussian Mixture Models}


We adopt the setup from prior works \cite{zheng2023toward} and consider a binary classification task where \(Y = \{-1, 1\}\). Given a vector \(\mu \in \mathbb{R}^d\) with \(\|\mu\|_2 = 1\) and noise variance \(\sigma^2 > 0\), the data distribution is specified as follows: \(y \sim \text{uniform}\{-1, 1\}\) and \(x \mid y \sim \mathcal{N}(y \mu, \sigma^2 I_d)\). We define the conditional generative model using parameters \(\mu_y\) and \(\sigma_k^2\), where \(y \in \{-1, 1\}\) and \(k \in [d]\). For \(n\) data points, let \(n_y\) represent the number of samples in class \(y\). The parameters of the Gaussian mixture model are then learned as:
\[
\hat{\mu}_y = \frac{\sum_{y_i = y} x_i}{n_y}, \quad
\hat{\sigma}_k^2 = \sum_y \frac{n_y}{n} \frac{\sum_{y_i = y} (x_{ik} - \hat{\mu}_{yk})^2}{n_y - 1}.
\]
Then we can generate new samples from the distribution: \(y \sim \text{uniform}\{-1, 1\}\) and \(x \mid y \sim \mathcal{N}(\hat{\mu}_y, \Sigma)\), where \(\Sigma = \text{diag}(\sigma_1^2, \dots, \sigma_d^2)\). Additionally, the learning algorithm functions as a linear classifier, parameterized by \(\theta \in \mathbb{R}^d\), with predictions given by: $\hat{y} = \text{sign}(\theta^\top \mathbf{x})$. The loss function is defined as:  
\[
\ell(\theta, (x, y)) = \frac{1}{2\sigma^2} (x - y\theta)^\top (x - y\theta).
\]
Thus, the output is  
\(
\hat{\theta} = \frac{1}{m} \sum_{i=1}^m y_i x_i.
\)

In this setting, we demonstrate recursive stability for the Gaussian mixture model as follows:

\begin{theorem}
Let \(S_0, S_0'\) denote two initial real datasets differing by a single example. Let \(n\) represent the sample size of the mixed dataset \(\tilde{S}_j\), where \(\tilde{S}_j = \alpha S_0 + (1 - \alpha) S_j\) for \(1 \leq j \leq i\). Choose \(m = \mathcal{O}(\sqrt{n})\). Consider the previously described sampling and learning steps, where real data samples are drawn from the Gaussian Mixture Model distribution \(\mathcal{D}\), and the synthetic data for the \(i\)-th generation is generated from the learned Gaussian Mixture distribution of the \(i\)-th generation. Then with probability at least \(1-\delta\), we have:
\begin{align}
    \gamma_n^i \lesssim n^{-1/2} \alpha^{-1}(1-(1-\alpha)^i)\log (nd/\delta),
\end{align}
where the measure for the recursive stability parameter is taken as the KL divergence.
\end{theorem}

As \(\alpha\) approaches 0, indicating that no real data is incorporated during each generation of training, we observe  
\[
\gamma_n^i \lesssim i n^{-1/2} \log \frac{nd}{\delta},
\]
which suggests a linear accumulation of errors. This finding aligns closely with the theoretical insights presented in \cite{shumailov2024ai,alemohammadself}, where a Gaussian model trained without real data demonstrated a linear divergence in variance. Thus, this underscores the validity of our theoretical results, confirming that the derived bound is meaningful and not vacuous.

Moreover, by leveraging the generalization error bound established in Theorem 1, we derive the following:

\begin{theorem}
 Consider the Gaussian Mixture Model in the setting outlined above. Let \(n\) represent the sample size of the mixed dataset \(\tilde{S}_j\), where \(\tilde{S}_j = \alpha S_0 + (1 - \alpha) S_j\) for \(1 \leq j \leq i\). Suppose the loss function is defined as \(\ell(\theta, (\mathbf{x}, y)) = \frac{1}{2\sigma^2} (\mathbf{x} - y\theta)^\top (\mathbf{x} - y\theta)\). Let \(\mathcal{A}(\tilde{S}_i)\) denote the output of applying the linear classifier described above to the mixed dataset \(\tilde{S}_i\). Then, for any \(\delta \in (0,1)\), with probability at least \(1-\delta\), the following holds:
\begin{align}
    &\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right|\lesssim n^{-1/2}(d+\log(n/\delta))\log n\log (1/\delta) \notag \\
    &\quad+n^{-1/4}(1-(1-\alpha)^i)\alpha^{-1}(d+\log (n/\delta))\sqrt{d\log(nd/\delta)}.
\end{align}
\end{theorem}

We observe that when \(\alpha\) is set to a constant (e.g., \(\alpha = 0.1\)), the generalization error can be effectively controlled, preventing model collapse. This result aligns with the experimental findings in \cite{alemohammadself} for Gaussian models.




\subsection{Additional Comparison with Related Work on Theorem 1}

\cite{dohmatob2024model} examined a linear regression setting, focusing solely on statistical approximation error without addressing the functional approximation error described in \cite{shumailov2024ai}. They did not consider incorporating real data to prevent collapse and demonstrated a linear dependency of degradation on the generation number in the case of fully synthetic data. Similarly, \cite{alemohammadself} and \cite{shumailov2024ai} provided theoretical insights using simple Gaussian models without incorporating real data, proving that the variance diverges linearly with the generation number. \cite{seddik2024bad} explored a linear softmax classifier and, while also neglecting functional approximation error, demonstrated that adding real data can mitigate model collapse. \cite{marchi2024heat} used asymptotic analysis to study parameter variance, assuming an infinite number of training generations and considering scenarios where the generative model is controlled via a ``temperature'' parameter. They proved that parameter variance is bounded under these conditions.  



In contrast, our work addresses a much more complex and realistic scenario by introducing the novel concept of \textbf{recursive stability} and providing the \textbf{first} generalization analysis for STLs. Our analysis accounts for \textbf{statistical approximation error, functional approximation error, and optimization error} during the training of generative models. Unlike the settings explored in prior theoretical works, such as linear regression \citep{dohmatob2024model,gerstgrasser2024is}, Gaussian models \citep{alemohammadself, shumailov2024ai}, or asymptotic assumptions \citep{marchi2024heat}, our framework accommodates more complex generative model architectures, such as transformers. Specifically, we reveal how both \textbf{model architecture} and \textbf{the ratio} of real to synthetic data influence the success of STLs. For example, in Theorem 3, we demonstrate how our general generalization bound applies to transformer-based generative models, providing a theoretical framework that aligns with practical and more sophisticated use cases. 



Additionally, while \cite{marchi2024heat} assumed an \textbf{infinite number} of training generations for their asymptotic analysis, we consider \textbf{finite generations}, which is more practical since most experimental setups limit generations to fewer than 10 (as noted in \cite{shumailov2024ai}). Moreover, our results confirm that when \(\alpha = 0\) (i.e., no real data is used), the last term in our bound, representing the Cumulative Distribution Shift (\(d_{\text{TV}}(n) M (1 - (1 - \alpha)^i) \alpha^{-1}\)), grows linearly. This finding aligns with the theoretical results of \cite{dohmatob2024model,alemohammadself,shumailov2024ai,futowards}. Furthermore, we show that introducing even a constant proportion of real data significantly mitigates model collapse, aligning with experimental findings by \cite{alemohammadself} and \cite{bertrandstability}.



\subsection{Additional Comparison with Related Work on Theorem 4}

\cite{gerstgrasser2024is} also explored the use of accumulating data to prevent model collapse. They considered a simple linear regression setting without accounting for the dynamic process of training generative models, focusing solely on statistical approximation error. They demonstrated that under the assumption of fixed synthetic data quality matching the original real data, statistical approximation error can be controlled.  


 By contrast, our work addresses a much more complex and realistic scenario, incorporating the dynamic behavior of transformer-based generative models, learning algorithms, and both statistical and functional approximation errors. Additionally, we allow for dynamic regulation of synthetic data size via a \(\lambda\) coefficient, enabling us to identify the optimal synthetic dataset size for avoiding model collapse in these more challenging settings.



\subsection{Auxiliary Lemmas}
In this section, we begin by introducing a set of auxiliary theorems that will be utilized in the subsequent proofs.
\begin{lemma}[McDiarmid's Inequality]\label{Mcdiarmid}
 Consider independent random variables $Z_1, \cdots, Z_n \in \mathcal{Z}$ and a mapping $\phi: \mathcal{Z}^n \rightarrow \mathbb{R}$. If, for all $i \in\{1, \cdots, n\}$, and for all $z_1, \cdots, z_n, z_i^{\prime} \in \mathcal{Z}$, the function $\phi$ satisfies
$$
\left|\phi\left(z_1, \cdots, z_{i-1}, z_i, z_{i+1}, \cdots, z_n\right)-\phi\left(z_1, \cdots, z_{i-1}, z_i^{\prime}, z_{i+1}, \cdots, z_n\right)\right| \leq c,
$$
then,
$$
P\left(|\phi\left(Z_1, \cdots, Z_n\right)-\mathbb{E} \phi\left(Z_1, \ldots, Z_n\right) \geq t|\right) \leq 2\exp \left(\frac{-2 t^2}{n c^2}\right).
$$
Furthermore, for any $p \geq 2$,
$$
\left\|\phi\left(Z_1, \ldots, Z_n\right)-\mathbb{E}\left[\phi\left(Z_1, \ldots, Z_n\right)\right]\right\|_p \leq 2\sqrt{np}c.
$$
\end{lemma}
\begin{lemma}(\citep{bousquet2020sharper}).\label{theorem_moment} Let $\boldsymbol{z}=\left(Z_1, \ldots, Z_n\right)$ be a vector of independent random variables each taking values in $\mathcal{Z}$, and let $g_1, \ldots, g_n$ be some functions $g_i: \mathcal{Z}^n \rightarrow \mathbb{R}$ such that the following holds for any $i \in[n]$ :
\begin{itemize} 
    \item $\left|\mathbb{E}\left[g_i(\boldsymbol{z}) \mid Z_i\right]\right| \leq M$,
    \item  $\mathbb{E}\left[g_i(\boldsymbol{z}) \mid \boldsymbol{z}^{\backslash i}\right]=0$,
    \item  $g_i$ has a bounded difference $\beta$ with respect to all variables except the $i$-th variable, that is, for all $j \neq i, \boldsymbol{z}=\left(Z_1, \ldots, Z_n\right)$ and $\boldsymbol{z}^j=\left(Z_1, \ldots, Z_j^{\prime}, \ldots, Z_n\right) \in \mathbb{R}^n$, we have $\left|g_i(\boldsymbol{z})-g_i\left(\boldsymbol{z}^j\right)\right| \leq \beta$.
\end{itemize}
Then, for any $p \geq 2$,

$$
\left\|\sum_{i=1}^n g_i(\boldsymbol{z})\right\|_p \leq 12 \sqrt{2} p n \beta \log n+4 M \sqrt{p n}.
$$
\end{lemma} 

\begin{lemma}\label{lemma_highprobability} If $\|Y\|_p \leq \sqrt{p} a+p b$ for any $p \geq 1$, then for any $\delta \in(0,1)$, with probability at least $1-\delta$,

$$
|Y| \leq e\left(a \sqrt{\log \left(\frac{e}{\delta}\right)}+b \log \left(\frac{e}{\delta}\right)\right).
$$

\end{lemma}

In addition, we introduce the definition of the Total Variation (TV) distance as follows:
\begin{definition}[Total Variation Distance] Given two probability distributions \( p \) and \( q \) over a multidimensional space \( \mathbb{R}^d \), the Total Variation Distance between \( p \) and \( q \) is:
\[ TV(p, q) = \frac{1}{2} \int_{\mathbb{R}^d} |p(\boldsymbol{z}) - q(\boldsymbol{z})| \, d\boldsymbol{z}. \]
\end{definition}














\subsection{Proof of Theorem \ref{theorem_generalization}}
In this Section, we prove Theorem \ref{theorem_generalization} by first decomposing the generalization error into two components: the \textit{Cumulative Distribution Shift Across Generations} and the \textit{Generalization Error on Mixed Distributions}. We then proceed to bound the \textit{Cumulative Distribution Shift Across Generations} by leveraging the properties of the generative model and recursive techniques. For the \textit{Generalization Error on Mixed Distributions}, we follow the framework of \cite{zheng2023toward}, leveraging the fact that within the mixed dataset $\widetilde{S}_i$, the set $S_i$ satisfies the conditional i.i.d. assumption when $S_0$ is fixed. Combined with moment bounds, this allows us to effectively bound the generalization error.


The main proof is as follows:

\begin{proof}[Proof of Theorem \ref{theorem_generalization}]
We begin by decomposing the generalization error as follows:
\begin{align}
\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right| \leq \underbrace{\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))\right|}_{\text {Cumulative distribution shift across generations}}+\underbrace{\left| R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i)) \right|}_{\text {Generalization error on mixed distributions}}. \notag
\end{align}


\textbf{Upper Bounding Cumulative Distribution Shift Term}


For the term $\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))\right|$, we first note that $\widetilde{\mathcal{D}}_i=\alpha \mathcal{D}_0+(1-\alpha) \mathcal{D}_i$. Therefore, we obtain:
\begin{align}
    &\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))\right|\notag \\
    &=\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i)-(1-\alpha)R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))\right| \notag \\
    &=(1-\alpha)\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i)-R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))\right| .\label{proof1_term1}
\end{align}
Furthermore, we can further decompose it as follows:
\begin{align}
   \left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i)-R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))\right| \leq \left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{i-1}}(\mathcal{A}(\widetilde{S}_i))\right|+\left|R_{\widetilde{\mathcal{D}}_{i-1}}(\mathcal{A}(\widetilde{S}_i))-R_{\mathcal{D}_{i}}(\mathcal{A}(\widetilde{S}_i))\right| \label{proof1_term2}.
\end{align}
By substituting inequality \ref{proof1_term2} into inequality \ref{proof1_term1}, we obtain:
\begin{align}
    &\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))\right| \notag \\
    &\leq  (1-\alpha)\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{i-1}}(\mathcal{A}(\widetilde{S}_i))\right|+(1-\alpha)\left|R_{\widetilde{\mathcal{D}}_{i-1}}(\mathcal{A}(\widetilde{S}_i))-R_{\mathcal{D}_{i}}(\mathcal{A}(\widetilde{S}_i))\right| \label{proof1_term3}.
\end{align}
Then, for the term $|R_{\widetilde{\mathcal{D}}_{i-1}}(\mathcal{A}(\widetilde{S}_i))-R_{\mathcal{D}_{i}}(\mathcal{A}(\widetilde{S}_i))|$, we have:
\begin{align}
\left|R_{\widetilde{\mathcal{D}}_{i-1}}(\mathcal{A}(\widetilde{S}_i))-R_{\mathcal{D}_{i}}(\mathcal{A}(\widetilde{S}_i))\right|
&=\Bigg|\int_{\boldsymbol{z}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z})\left(\mathbb{P}_{\widetilde{\mathcal{D}}_{i-1}}(\boldsymbol{z})-\mathbb{P}_{\mathcal{D}_i}(\boldsymbol{z})\right)d\boldsymbol{z}\Bigg| \notag \\
&\leq\int_{\boldsymbol{z}}\biggl|\ell(\mathcal{A}(\widetilde{S}),\boldsymbol{z})\left(\mathbb{P}_{\widetilde{\mathcal{D}}_{i-1}}(\boldsymbol{z})-\mathbb{P}_{\mathcal{D}_i}(\boldsymbol{z})\right)\biggr| d\boldsymbol{z} \notag\\
&\leq M\int_{\boldsymbol{z}}\Bigl|\mathbb{P}_{\widetilde{\mathcal{D}}_{i-1}}(\boldsymbol{z})-\mathbb{P}_{\mathcal{D}_i}(\boldsymbol{z})\Bigr| d\boldsymbol{z} \notag\\
&= 2M TV\left(\widetilde{\mathcal{D}}_{i-1},\mathcal{D}_{i}\right).  \label{proof1_term4}
\end{align}
Incorporating inequality \ref{proof1_term4} into inequality \ref{proof1_term3}, we arrive at:
\begin{align}
    &|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))|\notag \\
    &\leq (1-\alpha)|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{i-1}}(\mathcal{A}(\widetilde{S}_i))|+2(1-\alpha)M TV\left(\widetilde{\mathcal{D}}_{i-1},\mathcal{D}_{i}\right)\label{proof1_term5}.
\end{align}
Next, we apply recursive techniques to address the problem further. First, we obtain
\begin{align}
    &|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{i-1}}(\mathcal{A}(\widetilde{S}_i))|\notag \\
    &\leq (1-\alpha)|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{i-2}}(\mathcal{A}(\widetilde{S}_i))|+2(1-\alpha)M TV\left(\widetilde{\mathcal{D}}_{i-2},\mathcal{D}_{i-1}\right)\label{proof1_term6}.
\end{align}
Plugging inequality \ref{proof1_term6} into inequality \ref{proof1_term5}into the inequality, we obtain that:
\begin{align}
    &|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))|\notag \\
    &\leq (1-\alpha)^2|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{i-2}}(\mathcal{A}(\widetilde{S}_i))|+2(1-\alpha)^2MTV\left(\widetilde{\mathcal{D}}_{i-2},\mathcal{D}_{i-1}\right)+2(1-\alpha)MTV\left(\widetilde{\mathcal{D}}_{i-1},\mathcal{D}_{i}\right) \notag.
\end{align}
By recursion, we obtain:
\begin{align}
    &|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))|\notag \\
    &\leq (1-\alpha)^{i-1}|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{1}}(\mathcal{A}(\widetilde{S}_i))|+2(1-\alpha)^{i-1}MTV\left(\widetilde{\mathcal{D}}_{1},\mathcal{D}_{2}\right)+...+2(1-\alpha)MTV\left(\widetilde{\mathcal{D}}_{i-1},\mathcal{D}_{i}\right) \notag \\
    &\leq 2(1-\alpha)^iMTV\left(\mathcal{D}_{0},\mathcal{D}_{1}\right)+2(1-\alpha)^{i-1}MTV\left(\widetilde{\mathcal{D}}_{1},\mathcal{D}_{2}\right)+...+2(1-\alpha)MTV\left(\widetilde{\mathcal{D}}_{i-1},\mathcal{D}_{i}\right)\notag .
\end{align}
Let $n_0$ represent the sample size of the real dataset $S_0$, and let $n_i$ denote the sample size of the mixed dataset $\widetilde{S}_i$ in the $i$-th generation. Thus, $T V\left(\widetilde{\mathcal{D}}_j, \mathcal{D}_{j+1}\right)$ can be written as a function of $n_j$. Assuming that the sample size for each generation's dataset is identical, i.e., $n_0=n_1=\cdots=n_i=n$, and that the TV distance for each generation is of the same order, denoted by $d_{\mathrm{TV}}(n)$, we can derive the following result:
\begin{align}
    |R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))|&\leq 2Md_{\mathrm{TV}}(n)\left[(1-\alpha)^i+(1-\alpha)^{i-1}+...+(1-\alpha)\right]\notag \\
    &=2M\left(1-(1-\alpha)^i\right)\alpha^{-1}d_{\mathrm{TV}}(n).
\end{align}
Then we obtain:
\begin{align}
&| R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))|\leq| R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))|+|R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))|\notag\\
&\leq 2M\left(1-(1-\alpha)^i\right)\alpha^{-1}d_{\mathrm{TV}}(n)+|R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))|.
\end{align}








\textbf{Upper Bounding Generalization Error on Mixed Distributions Term}

Next, we turn our attention to the term $|R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))|$. Our primary objective is to establish a moment bound for this expression.
%注意这个地方考虑的是noniid 的情况，也就是说\widetilde{S}_i里面包含的是S_0与S_i,但是S_i depend on S_0，所以这个地方是个non-iid的情况。

\begin{align}
&\left\|R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right\|_{p} \notag \\
&=\left\|\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))+(1-\alpha)R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{i,1-\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\right\|_{p} \notag\\
&\leq \underbrace{\left\|\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\right\|_{p}}_{\text{Term 1}}+\underbrace{\left\|(1-\alpha)R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{i,1-\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\right\|_{p}}_{\text{Term 2}}. \label{proof-generalizati-decomp}
\end{align}
%这个地方思考的是S_0和S_i都是n个数据，然后取比例混合。
The newly sampled dataset, denoted as $S_{0, \alpha}$, is a subset of the original dataset $S_0$, where $S_{0, \alpha} \subseteq S_0$ and its size is $\alpha \times\left|S_0\right|$. Specifically, $S_{0, \alpha}$ contains a proportion $\alpha$ of the $n$ data points in $S_0$, resulting in a total of $n \times \alpha$ data points. Similarly, $S_{i, 1-\alpha}$ is a subset of the synthetic dataset $S_i$, where $S_{i, 1-\alpha} \subseteq S_i$, and its size is $(1-\alpha) \times\left|S_i\right|$. Specifically, $S_{i, 1-\alpha}$ contains a proportion $1-\alpha$ of the $n$ data points in $S_i$, resulting in $n \times(1-\alpha)$ data points.

We observe that for any function $f(S)$, if there exists a bound $\|f\|_p\left(S_j\right) \leq C$ for some subset $S_j \subseteq S$, then we have the following:

$$
\|f\|_p=\left(\mathbb{E} \mathbb{E}\left[|f|^p \mid S_j\right]\right)^{1 / p} \leq\left(\mathbb{E}\left[C^p\right]\right)^{1 / p} \leq C.
$$



Fix $S_0$, then data in $S_i$ are independent. We use this property and Lemma \ref{theorem_moment} to bound the Term 2.
We introduce functions $f_j(S_{i,1-\alpha})$ which play the same role as $g_j$'s in Lemma \ref{theorem_moment} as
$$
f_j(S_{i,1-\alpha})=\mathbb{E}_{\boldsymbol{z}_{i,j}'\sim\mathcal{D}_i}\left[\mathbb{E}_{\boldsymbol{z}\sim\mathcal{D}_i}\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z})-\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z}_{i,j})\right],
$$
where $\boldsymbol{z}_{i,j}$ is the $j$-th data in $S_{i,1-\alpha}$, and $S_{i,1-\alpha}^j$ obtained by replacing $\boldsymbol{z}_{i,j}$ by $\boldsymbol{z}_{i,j}^{\prime}.$ Next, we prove that $f_j$ satisfies the three conditions outlined in Lemma \ref{theorem_moment}. First, we demonstrate condition $|f_j|\leq M$.
\begin{align}
|f_{j}| &=\left|\mathbb{E}_{\boldsymbol{z}_{i,j}'\sim\mathcal{D}_i}\left[\mathbb{E}_{\boldsymbol{z}\sim\mathcal{D}_i}\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z})-\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z}_{i,j})\right]\right| \notag\\
&\leq  \mathbb{E}_{\boldsymbol{z}_{i,j}'\sim\mathcal{D}_i}\mathbb{E}_{\boldsymbol{z}\sim\mathcal{D}_i}\left|\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z})-\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z}_{i,j})\right|\notag. \\
&\leq M \notag
\end{align}
We then continue by proving conditions $\mathbb{E}[f_j|S_{i,1-\alpha}^{\setminus j}]=0$:
\begin{align}
&\mathbb{E}\left[f_j \mid S_{i,1-\alpha}^{\backslash j}\right] \notag \\
& =\mathbb{E}_{\boldsymbol{z}_{i,j} \sim \mathcal{D}_i}\left[\mathbb{E}_{\boldsymbol{z}_{i,j}'\sim\mathcal{D}_i}\left[\mathbb{E}_{\boldsymbol{z}\sim\mathcal{D}_i}\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z})-\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z}_{i,j})\right] \mid S_{i,1-\alpha}^{\backslash j}\right] \notag \\
& =\mathbb{E}_{\boldsymbol{z}_{i,j}'\sim\mathcal{D}_i}\left[\left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_i} \ell\left(\mathcal{A}\left(S_{0,\alpha} \cup S_{i,1-\alpha}^j\right), \boldsymbol{z}\right)-\mathbb{E}_{\boldsymbol{z}_{i,j} \sim \mathcal{D}_i} \ell\left(\mathcal{A}\left(S_{0,\alpha} \cup S_{i,1-\alpha}^j\right), \boldsymbol{z}_{i,j}\right)\right] \mid S_{i,1-\alpha}^{\backslash j}\right] \notag\\
& =\mathbb{E}_{\boldsymbol{z}_{i,j}'\sim\mathcal{D}_i}\left[0 \mid S_{i,1-\alpha}^{\backslash j}\right]=0. \notag
\end{align}


Finally, we prove that $f_j$ has a bounded difference $2\beta_{n}$ with respect to all variables except the $j$-th variable. Let $t\neq j$, then we obtain:
\begin{align}
|f_j\left(S_{i,1-\alpha}\right)-&f_j\left(S_{i,1-\alpha}^t\right)|\notag\\
=&|\mathbb{E}_{\boldsymbol{z}_{i,j}'\sim\mathcal{D}_i}\left[\mathbb{E}_{\boldsymbol{z}\sim\mathcal{D}_i}\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z})-\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z}_{i,j})\right] \notag\\
& -\mathbb{E}_{\boldsymbol{z}_{i,j}'\sim\mathcal{D}_i}\left[\mathbb{E}_{\boldsymbol{z}\sim\mathcal{D}_i}\ell(\mathcal{A}(S_{0,\alpha}\cup (S_{i,1-\alpha}^t)^j),\boldsymbol{z})-\ell(\mathcal{A}(S_{0,\alpha}\cup (S_{i,1-\alpha}^t)^j),\boldsymbol{z}_{i,j})\right]\mid \notag \\
\leq & \left|\mathbb{E}_{\boldsymbol{z}_{i,j}^{\prime} \sim \mathcal{D}_i} \mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_i}\left[\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z})-\ell(\mathcal{A}(S_{0,\alpha}\cup (S_{i,1-\alpha}^t)^j),\boldsymbol{z})\right]\right|\notag \\
& +\left|\mathbb{E}_{\boldsymbol{z}_{i,j}^{\prime} \sim \mathcal{D}_i}\left[\ell(\mathcal{A}(S_{0,\alpha} \cup S_{i, 1-\alpha}^j), \boldsymbol{z}_{i,j})-\ell(\mathcal{A}(S_{0,\alpha} \cup(S_{i,1-\alpha}^t)^j), \boldsymbol{z}_{i,j})\right]\right|\notag \\
\leq & \beta_{n}+\beta_{n}=2 \beta_{n} . \notag
\end{align}

Therefore, for any fixed $S_0$, by Lemma \ref{theorem_moment}, for any $p \geq 2$, we have

\begin{equation}\label{proof_term7}
\left\|\sum_{j=1}^{n(1-\alpha)} f_j\left(S_{i,1-\alpha}\right)\right\|_p \lesssim p n(1-\alpha) \beta_{n} \log (n(1-\alpha))+M \sqrt{p n(1-\alpha)}.
\end{equation}
We note that the difference between Term 2 and $\sum_{j=1}^{n(1-\alpha)} f_j$ is minimal. Consequently, for any fixed $S_0$, we can bound Term 2 using inequality \ref{proof_term7} as follows.
\begin{align}
&\left\|(1-\alpha)R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{i,1-\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\right\|_{p} \notag \\
&=\left\|(1-\alpha)R_{\mathcal{D}_i}(\mathcal{A}(S_{0,\alpha} \cup S_{i,1-\alpha}))-\frac{1}{n}\sum_{j=1}^{n(1-\alpha)}\ell(\mathcal{A}(S_{0,\alpha} \cup S_{i,1-\alpha}),\boldsymbol{z}_{i,j})\right\|_{p}  \qquad \qquad \text{Fix}\ S_{0,\alpha}\notag \\
& =\left\|\frac{1}{n}\sum_{j=1}^{n(1-\alpha)}\left(\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_i} \ell\left(\mathcal{A}\left(S_{0,\alpha} \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left(S_{0,\alpha} \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{i,j}\right)\right)\right\|_p \notag\\
& \leq \frac{1}{n}\left\|\sum_{j=1}^{n(1-\alpha)}\left(\mathbb{E}_{\boldsymbol{z}_{i,j}'\sim\mathcal{D}_i}\left[\mathbb{E}_{\boldsymbol{z}\sim\mathcal{D}_i}\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z})-\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j),\boldsymbol{z}_{i,j})\right]\right)\right\|_p+(1-\alpha)\left\|2\beta_{n}\right\|_p \notag \\
& =\frac{1}{n}\left\|\sum_{j=1}^{n(1-\alpha)} f_j\left(S_{i,1-\alpha}\right)\right\|_p+(1-\alpha)\left\|2  \beta_{n}\right\|_p \notag\\
& \lesssim p (1-\alpha) \beta_{n} \log (n(1-\alpha))+M \sqrt{\frac{p(1-\alpha)}{n}}+2 (1-\alpha) \beta_{n} \notag\\
& \lesssim p (1-\alpha) \beta_{n} \log (n(1-\alpha))+M \sqrt{\frac{p(1-\alpha)}{n}}.\notag
\end{align}
Next, for Term 2, we derive the following result:
\begin{align}
    \left\|(1-\alpha)R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{i,1-\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\right\|_{p}\lesssim p (1-\alpha) \beta_{n} \log (n(1-\alpha))+M \sqrt{\frac{p(1-\alpha)}{n}}. \label{proof-Term 2}
\end{align}
Now, we use a similar idea to bound Term 1 $\left\|\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\right\|_{p}$. We decompose it as the following.


\begin{align}
&\left\|\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\right\|_p \notag \\
& \leq \underbrace{\left\|(\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i}))-\mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}} (\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i}))\right\|_p}_{\text{Term 3}}\notag\\
&+\underbrace{\left\|\mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}} (\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i}))\right\|}_{\text{Term 4}}. \label{proof_34dec}
\end{align}
We proceed by bounding each term. Specifically, Term 3 can be bounded using McDiarmid's inequality, as outlined in Lemma \ref{Mcdiarmid}, and Term 4 can be bounded by applying Lemma \ref{theorem_moment}.

To bound Term 3, we begin by fixing $S_{0, \alpha}$ and utilizing the conditional independence property of $S_i$ once again. In order to apply Lemma \ref{theorem_moment}, we must show that $\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})$ exhibits a bounded difference with respect to $S_{i, 1-\alpha}$ when $S_{0, \alpha}$ is fixed. This expression can be formulated as follows.
\begin{align}
&\left|\alpha R_{\mathcal{D}_0}(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}}\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}),\boldsymbol{z}_{i})\right.\notag\\
&\left.-\alpha R_{\mathcal{D}_0}(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j))+\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}} \ell(\mathcal{A}(S_{0,\alpha} \cup S_{i,1-\alpha}^j), \boldsymbol{z}_i)\right| \notag\\
& \leq \alpha\left|R_{\mathcal{D}_0}(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}))-R_{\mathcal{D}_0}(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}^j))\right| \notag \\
& +\frac{1}{n}\left|\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}}\ell(\mathcal{A}(S_{0,\alpha}\cup S_{i,1-\alpha}),\boldsymbol{z}_{i})-\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}} \ell\left(\mathcal{A}\left(S_{0,\alpha} \cup S_{i,1-\alpha}^j\right), \boldsymbol{z}_i\right)\right| \notag \\
&\leq \alpha \beta_{n}+\alpha \beta_{n}=2 \alpha \beta_{n} . \notag
\end{align}

Thus, by Mcdiarmid Inequality, we have

\begin{align}
&\left\|(\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i}))-\mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}} (\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n}\sum_{\boldsymbol{z}_{i}\in S_{0,\alpha}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i}))\right\|_p \notag \\
&\leq 4 \sqrt{n(1-\alpha) p} \alpha \beta_{n} \lesssim \sqrt{n(1-\alpha) p} \alpha \beta_{n}. \label{proof1-delta1}
\end{align}

We now introduce a set of functions and apply Lemma \ref{theorem_moment} once more to bound Term 4. Specifically, we define $h_j(S)$, which serves a similar role to the $g_i$ 's in Lemma \ref{theorem_moment}, as follows:

\begin{align}
&h_j(S_{0,\alpha})\notag \\
=&\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left(S_{0,\alpha}^j\right)}\left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left(S_{0,\alpha}^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left(S_{0,\alpha}^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)\right]
\end{align}
where $\boldsymbol{z}_{0, j}$ denote the $j$-th data point in $S_{0, \alpha}$, and $S_{0, \alpha}^j$ represent the dataset obtained by replacing $\boldsymbol{z}_{0, j}$ with $\boldsymbol{z}_{0, j}^{\prime}$. Additionally, it is important to note that $S_{i, 1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left(S_{0, \alpha}^j\right)$ indicates that $S_{i, 1-\alpha}$ is the synthetic dataset generated after the self-consuming loop, following $i$-generations, and obtained by modifying a single data point from the initial real dataset $S_0$. This complex scenario can be addressed using the recursive stability we have defined for the self-consuming loop in Definition \ref{iterative stability}. Moreover, similar to the process above, we observe that $\left|h_j\right| \leq M$ and $\mathbb{E}\left[h_j \mid S_{0, \alpha}^{\backslash j}\right]=0$. More intricately, we will now prove that $h_j$ exhibits a bounded difference. This will be demonstrated as follows.

%这个地方注意了，我们需要做的是定义一个全新的stability，也即想定义刚开始的训练集稍微不同，或者说刚开始的real data distribution的TV distance稍微不同，经过self consuming loop之后，到第i轮的TV distance 会相差多少。

\begin{align}
&|h_j(S_{0,\alpha})-h_j\left(S_{0,\alpha}^t\right)|\notag \\
= & \mid \mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left(S_{0,\alpha}^j\right)}\left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left(S_{0,\alpha}^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left(S_{0,\alpha}^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)\right] \notag \\
-&  \mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left((S_{0,\alpha}^t)^j\right)}\left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)\right] \notag\\
\leq & \mid \mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left(S_{0,\alpha}^j\right)}\left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left(S_{0,\alpha}^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left(S_{0,\alpha}^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)\right] \notag \\
-&\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left(S_{0,\alpha}^j\right)} \left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)\right] | \label{proof-8} \\
+&|\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left(S_{0,\alpha}^j\right)} \left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)\right] \notag\\
-&\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left((S_{0,\alpha}^t)^j\right)}\left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)\right]|\label{proof-9}.
\end{align}

We can bound equation \ref{proof-8} by applying the concept of uniform stability, resulting in an upper bound of $2\beta_n$. Regarding equation \ref{proof-9}, for ease of notation, let us represent $\ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)$ as $Q$. Consequently, we obtain the following:
\begin{align}
&|\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left(S_{0,\alpha}^j\right)} \left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)\right] \notag\\
&-\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left((S_{0,\alpha}^t)^j\right)}\left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)\right]| \notag \\
&=\left|\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0}\left[\mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left(S_{0,\alpha}^j\right)} Q-\mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left((S_{0,\alpha}^t)^j\right)} Q\right]\right|\notag \\
& =\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0}\left|\int_{S_{i,1-\alpha}}\left(\mathbb{P}\left(S_{i,1-\alpha} \mid S^j_{0,\alpha}\right)-\mathbb{P}\left(S_{i,1-\alpha} \mid\left(S^t_{0,\alpha}\right)^j\right)\right) Q d S_{i,1-\alpha}\right| \notag\\
& \leq \mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0}\left[\int_{S_{i,1-\alpha}}\left|\left(\mathbb{P}\left(S_{i,1-\alpha} \mid S^j_{0,\alpha}\right)-\mathbb{P}\left(S_{i,1-\alpha} \mid\left(S^t_{0,\alpha}\right)^j\right)\right) Q\right| d S_{i,1-\alpha}\right] \notag\\
& \leq M \mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0}\left[\int_{S_{i,1-\alpha}}\left|\left(\mathbb{P}\left(S_{i,1-\alpha} \mid S^j_{0,\alpha}\right)-\mathbb{P}\left(S_{i,1-\alpha} \mid\left(S^t_{0,\alpha}\right)^j\right)\right) \right| d S_{i,1-\alpha}\right] \notag\\
& \leq 2 M \sup _j TV\left(\mathcal{D}_i^{n(1-\alpha)}(S_0^j), \mathcal{D}_i^{n(1-\alpha)}(S_0)\right)\notag \\
&= 2M \gamma_n^i \label{proof_19}.
\end{align}

Thus, $h_j$ exhibits a bounded difference of $2 \beta_n+2 M \gamma_n^i$ with respect to all variables except the $j$-th variable. By applying Lemma \ref{theorem_moment}, we obtain the following:

$$
\begin{aligned}
\left\|\sum_{j=1}^{n\alpha} h_j(S_{0,\alpha})\right\|_p & \leq 12 \sqrt{2} p n\alpha\left(2 \beta_{n}+2 M \gamma_n^i\right) \log (n\alpha)+4 M \sqrt{p n\alpha} \\
& \lesssim  p n\alpha\left( \beta_{n}+ M \gamma_n^i\right) \log (n\alpha)+ M \sqrt{p n\alpha}.
\end{aligned}
$$
We observe that the difference between Term 4 and $\left\|\sum_{j=1}^{n \alpha} h_j\left(S_{0, \alpha}\right)\right\|_p$ is negligible. Thus, we can bound Term 4 as follows:

$$
\begin{aligned}
& \left\|\mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}}\left[\alpha R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{n} \sum_{\boldsymbol{z}_i \in S_{0, \alpha}} \ell\left(\mathcal{A}\left(\widetilde{S}_i\right), \boldsymbol{z}_i\right)\right]\right\|_p \\
 &= \left\|\frac{1}{n} \sum_{\boldsymbol{z}_i \in S_{0, \alpha}} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}}\left[ R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\ell\left(\mathcal{A}(\widetilde{S}_i), \boldsymbol{z}_i\right)\right]\right\|_p \\
 & \leq\left\|\frac{1}{n}\sum_{j=1}^{n\alpha}\left(\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left(S_{0,\alpha}^j\right)}\left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left(S_{0,\alpha}^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left(S_{0,\alpha}^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)\right]\right)\right\|_p \\
& +\left\|2 \alpha \beta_{n}+2 \alpha M  \gamma_n^i\right\|_p \\
 &= \left\|\frac{1}{n}\sum_{j=1}^{n\alpha} h_j(S_{0,\alpha})\right\|_p+\left\|2 \alpha \beta_{n}+2 \alpha M  \gamma_n^i\right\|_p\\
&\lesssim  p \alpha\left( \beta_{n}+ M \gamma_n^i\right) \log (n\alpha)+ M \sqrt{p\alpha n^{-1}}+\alpha\beta_n+\alpha M \gamma_n^i \\
&\lesssim  p \alpha\left( \beta_{n}+ M \gamma_n^i\right) \log (n\alpha)+ M \sqrt{p \alpha n^{-1}}.
\end{aligned}
$$
By substituting the above inequality and inequality \ref{proof1-delta1} into the decomposition \ref{proof_34dec}, we obtain:
\begin{align}
    &\left\|\alpha R_{\mathcal{D}_0}\left(\mathcal{A}\left(\widetilde{S}_i\right)\right)-\frac{1}{n} \sum_{\boldsymbol{z}_i \in S_{0, \alpha}} \ell\left(\mathcal{A}\left(\widetilde{S}_i\right), \boldsymbol{z}_i\right)\right\|_p \notag \\
    &\lesssim \sqrt{n(1-\alpha) p} \alpha \beta_n+p  \alpha\left(\beta_n+M \gamma_n^i\right) \log (n \alpha)+M \sqrt{p  \alpha n^{-1}} \notag \\
    &\lesssim \sqrt{p}(\sqrt{(1-\alpha)n}\alpha\beta_n+M\sqrt{\alpha n^{-1}})+p  \alpha\left(\beta_n+M \gamma_n^i\right) \log (n \alpha)\label{proof-phi}.
\end{align}
Plug inequalities \ref{proof-phi} and \ref{proof-Term 2} into the inequality \ref{proof-generalizati-decomp}, then we obtain:
\begin{align}
\|R_{\widetilde{\mathcal{D}}_i}&(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\|_p \notag \\
\lesssim &p(1-\alpha) \beta_n \log (n(1-\alpha))+M \sqrt{p(1-\alpha)n^{-1}}+\sqrt{p}(\sqrt{(1-\alpha)n}\alpha\beta_n+M\sqrt{\alpha n^{-1}})\notag \\
&+p  \alpha\left(\beta_n+M \gamma_n^i\right) \log (n \alpha)\notag \\
=& \sqrt{p}\left(\sqrt{(1-\alpha)n}\alpha\beta_n+Mn^{-1/2}(\sqrt{1-\alpha}+\sqrt{\alpha})\right)\notag \\
&+p\left((1-\alpha) \beta_n \log (n(1-\alpha))+\alpha\left(\beta_n+M \gamma_n^i\right) \log (n \alpha)\right).
\end{align}
By applying Lemma \ref{theorem_moment}, we can derive a bound on the generalization error with respect to the mixed distribution.$\left|R_{\widetilde{\mathcal{D}}_i}\left(\mathcal{A}\left(\widetilde{S}_i\right)\right)-\widehat{R}_{\widetilde{S}_i}\left(\mathcal{A}\left(\widetilde{S}_i\right)\right)\right|$ as follows.
$$
\begin{aligned}
& \left|R_{\widetilde{\mathcal{D}}_i}\left(\mathcal{A}\left(\widetilde{S}_i\right)\right)-\widehat{R}_{\widetilde{S}_i}\left(\mathcal{A}\left(\widetilde{S}_i\right)\right)\right| \\
& \lesssim \left(\sqrt{(1-\alpha) n} \alpha \beta_n+M n^{-1 / 2}(\sqrt{1-\alpha}+\sqrt{\alpha})\right) \sqrt{\log \left(\frac{1}{\delta}\right)} \\
& +\left((1-\alpha) \beta_n \log (n(1-\alpha))+\alpha\left(\beta_n+M \gamma_n^i\right) \log (n \alpha)\right) \log \left(\frac{1}{\delta}\right).
\end{aligned}
$$
Finally, we conclude that:
\begin{align}
&\left|R_{\mathcal{D}_0}\left(A\left(\widetilde{S}_i\right)\right)-\widehat{R}_{\widetilde{S}_i}\left(A\left(\widetilde{S}_i\right)\right)\right|\notag\\
&\leq\left|R_{\mathcal{D}_0}\left(A\left(\widetilde{S}_i\right)\right)-R_{\widetilde{\mathcal{D}}_i}\left(A\left(\widetilde{S}_i\right)\right)\right|+\left|R_{\widetilde{\mathcal{D}}_i}\left(A\left(\widetilde{S}_i\right)\right)-\widehat{R}_{\widetilde{S}_i}\left(A\left(\widetilde{S}_i\right)\right)\right| \notag \\
&\leq2 M\left(1-(1-\alpha)^i\right) \alpha^{-1} d_{\mathrm{TV}}(n)\notag \\
&+\left((1-\alpha) \beta_n \log (n(1-\alpha))+\alpha\left(\beta_n+M \gamma_n^i\right) \log (n \alpha)\right) \log \left(\frac{1}{\delta}\right)\notag \\
&+\left(\sqrt{(1-\alpha) n} \alpha \beta_n+M n^{-1 / 2}(\sqrt{1-\alpha}+\sqrt{\alpha})\right) \sqrt{\log \left(\frac{1}{\delta}\right)}.
\end{align}
\end{proof}






























\subsection{Proof of Theorem \ref{therorem_stability of transformer}}
In this section, we prove that transformers in in-context learning exhibit recursive stability. Specifically, we utilize the framework and lemmas from \cite{li2023transformers}, combined with recursive techniques, to establish the proof.

\begin{lemma}[\cite{li2023transformers}]\label{lemma_lisoftmax} Let $\boldsymbol{z}, \boldsymbol{\varepsilon} \in \mathbb{R}^n$ be vectors obeying $\|\boldsymbol{z}\|_{\ell_{\infty}},\|\boldsymbol{z}+\boldsymbol{\varepsilon}\|_{\ell_{\infty}} \leq c$. Then, there exists a constant $C=C(c)$, such that

$$
\|\operatorname{softmax}(\boldsymbol{z})\|_{\ell_{\infty}} \leq e^{2 c} / n \quad \text { and } \quad\|\operatorname{softmax}(\boldsymbol{z})-\operatorname{softmax}(\boldsymbol{z}+\boldsymbol{\varepsilon})\|_{\ell_1} \leq e^{2 c}\|\boldsymbol{\varepsilon}\|_{\ell_1} / n.
$$

\end{lemma}



\begin{proof}[Proof of Theorem \ref{therorem_stability of transformer}]. Let $\boldsymbol{Z}=\left[\boldsymbol{z}_1,  \ldots  ,\boldsymbol{z}_n\right]^{\top}$ and $\boldsymbol{E}=\left[\boldsymbol{\varepsilon}_1, \ldots , \boldsymbol{\varepsilon}_n\right]^{\top}$ be the input and perturbation matrices respectively. Given that the tokens $\boldsymbol{z}_i$ lie in the unit ball, and assuming $\boldsymbol{z}_i+\boldsymbol{\varepsilon}_i$ also lies in the unit ball, we can proceed with the following. For a matrix, let $\|\cdot\|_{2, p}$ denote the $\ell_p$-norm of the vector formed by the $\ell_2$-norms of its rows. Therefore, we obtain $\|\boldsymbol{Z}\|_{2, \infty} \leq 1$ and $\|\boldsymbol{\bar{Z}}\|_{2, \infty}=\|\boldsymbol{Z}+\boldsymbol{E}\|_{2, \infty} \leq$ 1. Let the attention outputs be defined as $\boldsymbol{A}=\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W} \boldsymbol{Z}^{\top}\right) \boldsymbol{Z} \boldsymbol{V}$ and $\bar{\boldsymbol{A}}=\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W} \bar{\boldsymbol{Z}}^{\top}\right) \bar{\boldsymbol{Z}} \boldsymbol{V}$. Define the perturbation as $\bar{\boldsymbol{E}}=\bar{\boldsymbol{A}}-\boldsymbol{A}:=\left[\bar{\boldsymbol{\varepsilon}}_1, \ldots  \bar{\boldsymbol{\varepsilon}}_n\right]^{\top}$.

Let us examine the attention output difference $\bar{\boldsymbol{E}}=\bar{\boldsymbol{A}}-\boldsymbol{A}$, which can be further decomposed as follows:
\begin{align}
\boldsymbol{\bar{E}}&=\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W} \bar{\boldsymbol{Z}}^{\top}\right) \bar{\boldsymbol{Z}} \boldsymbol{V}-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W} \boldsymbol{Z}^{\top}\right) \boldsymbol{Z} \boldsymbol{V} \notag \\
&=\underbrace{\left[\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W} \bar{\boldsymbol{Z}}^{\top}\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W} \boldsymbol{Z}^{\top}\right)\right] \boldsymbol{Z} \boldsymbol{V}}_{\boldsymbol{\bar{E}_1}}+\underbrace{\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W} \bar{\boldsymbol{Z}}^{\top}\right) \boldsymbol{E} \boldsymbol{V}}_{\boldsymbol{\bar{E}_2}} .\label{proof_edecomp}
\end{align}
We first observe that $\boldsymbol{V}$ preserves the norm, meaning that $\boldsymbol{Z} \boldsymbol{V}$ satisfies $\|\boldsymbol{Z} \boldsymbol{V}\|_{2, \infty} \leq$ $\|\boldsymbol{Z}\|_{2, \infty} \leq 1$ and $\|\boldsymbol{E} \boldsymbol{V}\|_{2,1} \leq\|\boldsymbol{E}\|_{2,1}$. Moreover, for any pair of tokens, it holds that $\left|\boldsymbol{z}_i^{\top} \boldsymbol{W} \boldsymbol{z}_j\right| \leq B_W$. Applying Lemma \ref{lemma_lisoftmax}, we can therefore derive the following:
\begin{align}
\left\|\boldsymbol{\bar{E}}_2\right\|_{2,1}=\left\|\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W} \bar{\boldsymbol{Z}}^{\top}\right) \boldsymbol{E} \boldsymbol{V}\right\|_{2,1} \leq e^{2 B_W}\|\boldsymbol{E}\|_{2,1} .\label{proof_e1}
\end{align}
Subsequently, for $\boldsymbol{\bar{E}}_1$, we establish the following expression
\begin{align}
\left\|\boldsymbol{\bar{E}}_1\right\|_{2,1} &= \left\|[\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W} \bar{\boldsymbol{Z}}^{\top}\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W} \boldsymbol{Z}^{\top}\right)]\boldsymbol{Z} \boldsymbol{V}\right\|_{2,1} \notag \\
& \leq\left\|\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W} \bar{\boldsymbol{Z}}^{\top}\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W} \boldsymbol{Z}^{\top}\right)\right\|_{\ell_1}\|\boldsymbol{Z} \boldsymbol{V}\|_{2, \infty}\notag \\
& \leq\left\|\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W} \bar{\boldsymbol{Z}}^{\top}\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W} \boldsymbol{Z}^{\top}\right)\right\|_{\ell_1} .\notag
\end{align}
To advance the analysis, we introduce the $\delta$-scaled perturbation $\boldsymbol{E}^{\prime}=\delta \boldsymbol{E}=\bar{\boldsymbol{Z}}^{\prime}-\boldsymbol{Z}$, where $\delta$ is constrained within $0 \leq \delta \leq 1$. Our approach involves first bounding the derivative as $\delta \rightarrow 0$, and then integrating this bound along the path of $\boldsymbol{E}$, effectively covering the interval from $\delta=0$ to $\delta=1$. Notably, as $\delta \rightarrow 0$, the quadratic terms proportional to $\delta^2 \boldsymbol{E}$ diminish, simplifying the analysis at this limit. 
\begin{align}
&\left\|\operatorname{softmax}\left(\bar{\boldsymbol{Z}}^{\prime} \boldsymbol{W} \bar{\boldsymbol{Z}}^{\prime \top}\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W} \boldsymbol{Z}^{\top}\right)\right\|_{\ell_1} \notag \\
&\leq\left\|\operatorname{softmax}(\bar{\boldsymbol{Z}}^{\prime} \boldsymbol{W} \boldsymbol{Z}^{\top})-\operatorname{softmax}(\boldsymbol{Z} \boldsymbol{W} \boldsymbol{Z}^{\top})\right\|_{\ell_1}+\left\|\operatorname{softmax}(\boldsymbol{Z} \boldsymbol{W} \bar{\boldsymbol{Z}}^{\prime \top})-\operatorname{softmax}(\boldsymbol{Z} \boldsymbol{W} \boldsymbol{Z}^{\top})\right\|_{\ell_1} . \notag
\end{align}
To bound the latter, we focus on each row separately. Consider a row from $\boldsymbol{Z}$ and its perturbed version $\boldsymbol{Z}+\boldsymbol{E}^{\prime}$, represented by the pair $\left(\boldsymbol{z}, \boldsymbol{z}+\boldsymbol{\varepsilon^{\prime}}\right)$. It follows that for any cross product, we have the guarantees $\left|\left(\boldsymbol{z}+\boldsymbol{\varepsilon^{\prime}}\right)^{\top} \boldsymbol{W} \boldsymbol{z}_i\right| \leq B_W$ and $\left|\boldsymbol{z}^{\top} \boldsymbol{W} \boldsymbol{z}_i\right| \leq B_W$. Additionally, the bounds $\left\|\boldsymbol{\varepsilon}^{\prime \top} \boldsymbol{W} \boldsymbol{Z}\right\|_{\ell_1} \leq B_W n\left\|\boldsymbol{\varepsilon}^{\prime}\right\|_{\ell_2}$ and $\left\|\boldsymbol{z}^{\top} \boldsymbol{W} \boldsymbol{E}^{\prime \top}\right\|_{\ell_1} \leq B_W\left\|\boldsymbol{E}^{\prime}\right\|_{2,1}$ hold. Applying the perturbation bounds provided by Lemma \ref{lemma_lisoftmax}, we obtain the desired result
\begin{align} 
&\left\|\operatorname{softmax}\left(\left(\boldsymbol{z}+\boldsymbol{\varepsilon}^{\prime}\right)^{\top} \boldsymbol{W} \boldsymbol{Z}^{\top}\right)-\operatorname{softmax}\left(\boldsymbol{z}^{\top} \boldsymbol{W} \boldsymbol{Z}^{\top}\right)\right\|_{\ell_1} \leq B_W e^{2 B_W}\left\|\boldsymbol{\varepsilon}^{\prime}\right\|_{\ell_2} \notag\\ & \left\|\operatorname{softmax}\left(\boldsymbol{z}^{\top} \boldsymbol{W}\left(\boldsymbol{Z}+\boldsymbol{E}^{\prime}\right)^{\top}\right)-\operatorname{softmax}\left(\boldsymbol{z}^{\top} \boldsymbol{W} \boldsymbol{Z}^{\top}\right)\right\|_{\ell_1} \leq B_W e^{2 B_W}\left\|\boldsymbol{E}^{\prime}\right\|_{2,1} / n.\notag
\end{align}
Summing across all $n$ rows, we obtain the following:
\begin{align}
    \lim _{\delta \rightarrow 0}\left\|\operatorname{softmax}\left((\boldsymbol{Z}+\delta \boldsymbol{E}) \boldsymbol{W} \bar{\boldsymbol{Z}}^{\top}\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W} \boldsymbol{Z}^{\top}\right)\right\|_{\ell_1} / \delta \leq 2 B_W e^{2 B_W}\|\boldsymbol{E}\|_{2,1} \notag.
\end{align}
By integrating the derivative over the interval $\delta=0$ to $\delta=1$, we obtain the final expression,
\begin{align}
    \left\|\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W} \bar{\boldsymbol{Z}}^{\top}\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W} \boldsymbol{Z}^{\top}\right)\right\|_{\ell_1} \leq 2 B_W e^{2 B_W}\|\boldsymbol{E}\|_{2,1} \label{proof_e2}.
\end{align}
By substituting inequality \ref{proof_e2} and inequality \ref{proof_e1} into the decomposition \ref{proof_edecomp}, we derive the following result:
\begin{align}
   \|\boldsymbol{\bar{A}}-\boldsymbol{A}\|_{2,1}= \|\bar{\boldsymbol{E}}\|_{2,1}\leq (2B_W+1)e^{2B_W}\|\boldsymbol{E}\|_{2,1} \label{proof_attenweightdifference}.
\end{align}
To continue, we aim to control the output for a specific index $j$ where the input perturbation remains small, specifically $\left\|\boldsymbol{\varepsilon}_j\right\|_{\ell_2} \leq \frac{\|\boldsymbol{E}\|_{2,1}}{n}$. To address this, we will apply the same argument, focusing on the $j$-th token. For the $j$-th token (omitting subscripts for clarity), let the inputs be denoted as $\boldsymbol{z}, \boldsymbol{\bar{z}}, \boldsymbol{\varepsilon}=\boldsymbol{\bar{z}}-\boldsymbol{z}$, and the corresponding outputs as $\boldsymbol{a}, \bar{\boldsymbol{a}}, \bar{\boldsymbol{\varepsilon}}=\bar{\boldsymbol{a}}-\boldsymbol{a}$. Similar to the previous decomposition, we can derive the following:
\begin{align}
\boldsymbol{\bar{\varepsilon}}=\underbrace{\boldsymbol{V}^{\top} \boldsymbol{Z}^{\top}\left[\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W}^{\top} \bar{\boldsymbol{Z}}\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W}^{\top} \boldsymbol{Z}\right)\right]}_{\boldsymbol{\bar{\varepsilon}}_1}+\underbrace{\boldsymbol{V}^{\top} \boldsymbol{E}^{\top} \operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W}^{\top} \bar{\boldsymbol{Z}}\right)}_{\boldsymbol{\bar{\varepsilon}}_2}.
\end{align}
By leveraging the fact that $\left|\boldsymbol{z}_i^{\top} \boldsymbol{W} \boldsymbol{z}_j\right| \leq B_W$ for all $i, j$, and applying Lemma \ref{lemma_lisoftmax}, we can establish a bound similar to that in equation \ref{proof_e1}. Specifically, we can constrain the terms involved as follows:
\begin{align}
    \left\|\boldsymbol{\bar{\varepsilon}}_2\right\|_{\ell_2} \leq\left\|\boldsymbol{E}^{\top} \operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W}^{\top} \bar{\boldsymbol{z}}\right)\right\|_{\ell_2} \leq \frac{e^{2 B_W}}{n}\|\boldsymbol{E}\|_{2,1} .
\end{align}
Similarly, for $\bar{\boldsymbol{\varepsilon}}_1$, we can derive the following:
\begin{align}
\left\|\bar{\boldsymbol{\varepsilon}}_1\right\|_{\ell_2} & \leq\left\|\boldsymbol{Z}^{\top}\left[\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W}^{\top} \bar{\boldsymbol{z}}\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W}^{\top} \boldsymbol{z}\right)\right]\right\|_{\ell_2} \notag\\
& \leq\|\boldsymbol{Z}\|_{2, \infty}\left\|\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W}^{\top} \bar{\boldsymbol{z}}\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W}^{\top} \boldsymbol{z}\right)\right\|_{\ell_1} \notag\\
& \leq\left\|\operatorname{softmax}\left(\bar{\boldsymbol{Z}} \boldsymbol{W}^{\top} \bar{\boldsymbol{z}}\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W}^{\top} \boldsymbol{z}\right)\right\|_{\ell_1} .\notag
\end{align}
Now, considering the perturbation $\boldsymbol{E}^{\prime}=\delta \boldsymbol{E}$, and letting $\delta \rightarrow 0$, we apply the triangle inequality to obtain the following result:
\begin{align}
 \lim _{\delta \rightarrow 0} &\delta^{-1}\left\|\operatorname{softmax}\left((\boldsymbol{Z}+\delta \boldsymbol{E}) \boldsymbol{W}^{\top}(\boldsymbol{z}+\delta \boldsymbol{\varepsilon})\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W}^{\top} \boldsymbol{z}\right)\right\|_{\ell_1} \notag \\
 \leq &\lim _{\delta \rightarrow 0} \delta^{-1}\left\|\operatorname{softmax}\left((\boldsymbol{Z}+\delta \boldsymbol{E}) \boldsymbol{W}^{\top} \boldsymbol{z}\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W}^{\top} \boldsymbol{z}\right)\right\|_{\ell_1}\notag \\
&\quad +\delta^{-1}\left\|\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W}^{\top}(\boldsymbol{z}+\delta \boldsymbol{\varepsilon})\right)-\operatorname{softmax}\left(\boldsymbol{Z} \boldsymbol{W}^{\top} \boldsymbol{z}\right)\right\|_{\ell_1} \notag\\
\leq& B_W e^{2 B_W}\|\boldsymbol{E}\|_{2,1} / n+B_W e^{2 B_W}\|\boldsymbol{\varepsilon}\|_{\ell_2}\notag \\
 \leq& 2 B_W e^{2 B_W} \|\boldsymbol{E}\|_{2,1}/ n .
\end{align}
In a similar manner to the previous steps, we can derive the following:
\begin{align}
   \left \|\bar{\boldsymbol{\varepsilon}}\right\|_{\ell_2}\leq  \frac{1}{n}(2 B_W+1) e^{2 B_W} \|\boldsymbol{E}\|_{2,1}.
\end{align}
Next, we examine the effect of the MLP layer on the model's behavior. Let $\left(\boldsymbol{M}_i\right)_{i=1}^n \in \mathbb{R}^{d \times d}$ represent the weights of the parallel MLPs that follow the self-attention mechanism. Given that $\left\|\boldsymbol{M}_i\right\| \leq 1$, we denote the MLP outputs corresponding to the self-attention results $\boldsymbol{A}$ and $\boldsymbol{\bar{A}}$ as $\boldsymbol{U}$ and $\boldsymbol{\bar{U}}$, respectively. From this, we can derive the following relationship.

Let $\phi$ denote the ReLU function, which is a 1-Lipschitz continuous activation function with $\phi(0)=0$. First, observe that each row of $\boldsymbol{U}$ is given by $\boldsymbol{u}_i=\phi\left(\boldsymbol{M}_i \boldsymbol{a}_i\right)$, where $\boldsymbol{M}_i \in \mathbb{R}^{d \times d}$ represents the weights of the MLPs. Given the properties of the ReLU function, we can derive the following bound: 
$$\left\|\boldsymbol{u}_i\right\|_{\ell_2} \leq\left\|\phi\left(\boldsymbol{M}_i \boldsymbol{a}_i\right)\right\|_{\ell_2} \leq\left\|\boldsymbol{M}_i \boldsymbol{a}_i\right\|_{\ell_2} \leq\left\|\boldsymbol{a}_i\right\|_{\ell_2} \leq 1.
$$
Next, we consider the difference between the perturbed and original outputs. We can express the difference as $\left\|\boldsymbol{u}_i-\boldsymbol{\bar{u}}_i\right\|_{\ell_2} \leq\left\|\phi\left(\boldsymbol{M_i} \boldsymbol{a_i}\right)-\phi\left(\boldsymbol{M_i} \boldsymbol{\bar{a}_i}\right)\right\|_{\ell_2}$, which, due to the 1-Lipschitz property of $\phi$, is further bounded by $\left\|\boldsymbol{M}_i\left(\boldsymbol{a}_i-\boldsymbol{\bar{a}}_i\right)\right\|_{\ell_2} \leq\left\|\boldsymbol{a}_i-\boldsymbol{\bar{a}}_i\right\|_{\ell_2}$. Finally, we obtain:
\begin{align}
    \left\|\boldsymbol{u}_i-\boldsymbol{\bar{u}}_i\right\|_{\ell_2} \leq \left\|\boldsymbol{a}_i-\boldsymbol{\bar{a}}_i\right\|_{\ell_2}.
\end{align}
Thus, we conclude that the perturbations in the rows of $\boldsymbol{U}$ are controlled by the corresponding perturbations in $\boldsymbol{A}$. Consequently, we establish the bound 
$$\|\boldsymbol{U}-\boldsymbol{\bar{U}}\|_{2,1} \leq\|\boldsymbol{A}-\bar{\boldsymbol{A}}\|_{2,1}.
$$
Thus, from inequality \ref{proof_attenweightdifference}, we derive the following result:
\begin{align}
    \|\boldsymbol{U}-\boldsymbol{\bar{U}}\|_{2,1} \leq \left(2 B_W+1\right) e^{2 B_W}\|\boldsymbol{E}\|_{2,1}.
\end{align}
Furthermore, for any $i \in[n]$ such that $\left\|\boldsymbol{\varepsilon}_i\right\|_{\ell_2} \leq \frac{\|\boldsymbol{E}\|_{2,1}}{n}$, it holds that 
$$
\left\|\boldsymbol{u}_i-\bar{\boldsymbol{u}}_i\right\|_{\ell_2} \leq \frac{1}{n}(2 B_W+1) e^{2 B_W} \|\boldsymbol{E}\|_{2,1},
$$
where $\boldsymbol{u}_i$ represents the $i$-th row of $\boldsymbol{U}$. With this, we have addressed the stability of the single-layer transformer. Moving forward, we will extend our analysis and focus on the stability of $L$-layer transformer. First, we can derive the following:
\begin{align}
    \left\|\boldsymbol{Z}_{(k)}-\bar{\boldsymbol{Z}}_{(k)}\right\|_{2,1} \leq(1+2B_W) e^{2B_W}\left\|\boldsymbol{Z}_{(k-1)}-\bar{\boldsymbol{Z}}_{(k-1)}\right\|_{2,1}, \notag
\end{align}
where $1\leq k \leq L$ represents the number of layers in the transformer. Then, for $L$-layer transformer, we have the following:
\begin{align}
    \left\|\boldsymbol{Z}_{(L)}-\bar{\boldsymbol{Z}}_{(L)}\right\|_{2,1} \leq((1+2B_W) e^{2B_W})^L\left\|\boldsymbol{Z}_{(0)}-\bar{\boldsymbol{Z}}_{(0)}\right\|_{2,1} \notag
\end{align}
What remains is to perform induction on the difference between the last tokens $\boldsymbol{z}_n^{(i)}-\boldsymbol{z}_n^{\prime(i)}$. We claim that, for all layers, 
$$
\left\|\boldsymbol{z}_n^{(i)}-\boldsymbol{z}_n^{\prime(i)}\right\|_{\ell_2} \leq \frac{1}{n}((1+2B_W) e^{2B_W})^i\left\|\boldsymbol{Z}_{(0)}-\bar{\boldsymbol{Z}}_{(0)}\right\|_{2,1} .
$$
This claim holds at $i=0$ because the change in the last token is at most $\left\|\boldsymbol{Z}_{(0)}-\bar{\boldsymbol{Z}}_{(0)}\right\|_{2,1} / n$. By induction, the claim holds for all layers, and we conclude the proof by setting $i=L$, covering the entire depth of the $L$-layer transformer. Finally, we obtain:
\begin{align}
    \left\|\boldsymbol{z}_n^{(L)}-\boldsymbol{z}_n^{\prime(L)}\right\|_{\ell_2} \leq \frac{1}{n}((1+2B_W) e^{2B_W})^L\left\|\boldsymbol{Z}_{(0)}-\bar{\boldsymbol{Z}}_{(0)}\right\|_{2,1} .
\end{align}                                                       
Next, we further analyze the self-consuming process. Let $S_0=[\boldsymbol{z}_{0,1},...,\boldsymbol{z}_{0,j},...,\boldsymbol{z}_{0,n}]^{\top}$ and $S_0^{\prime}=[\boldsymbol{z}_{0,1},...,\boldsymbol{z}_{0,j}',...,\boldsymbol{z}_{0,n}]^{\top}$ represent two initial real datasets that differ only in their inputs, specifically $\boldsymbol{z}_{0,j}=\left(\boldsymbol{x}_{0,j}, \boldsymbol{y}_{0,j}\right)$ and $\boldsymbol{z}_{0,j}^{\prime}=\left(\boldsymbol{x}_{0,j}^{\prime}, \boldsymbol{y}_{0,j}^{\prime}\right)$, where $j \leq n$. Since $\|S_0-S_0'\|_{2,1}\leq 2$, then, we have the following:
\begin{align}
    \left\|\mathrm{TF}\left(S_0\right)-\mathrm{TF}\left(S_{0}^{\prime}\right)\right\|_{\ell_2} &\leq 
    \frac{1}{2 n+1}\left((1+2B_W) e^{2B_W}\right)^L\|S_0-S_0'\|_{2,1} \label{proof_tramsfor_output}\\
    &\leq  \frac{2}{2 n+1}\left((1+2B_W) e^{2B_W}\right)^L \notag .
\end{align}
Then, $S_0$ and $S_0^{\prime}$ are used as in-context examples, and i.i.d. queries $\{\boldsymbol{x}_{1,j}\}_{j=1}^n$ are sampled from $\mathcal{X}$. These queries, along with the in-context examples $S_0$ and $S_0^{\prime}$, are processed through the transformer model to predict their respective labels. As a result, the first generation of synthetic datasets, $S_1=[\boldsymbol{z}_{1,1},...,\boldsymbol{z}_{1,j},...,\boldsymbol{z}_{1,n}]^{\top}$ and $S_1^{\prime}=[\boldsymbol{z}_{1,1}',...,\boldsymbol{z}_{1,j}',...,\boldsymbol{z}_{1,n}']^{\top}$, is produced. Then we obtain:
\begin{align}
    \|S_1-S_1'\|_{2,1}\leq \frac{2n}{2 n+1}\left((1+2B_W) e^{2B_W}\right)^L.
\end{align}
Given the mixed dataset $\widetilde{S}_j$, where $\widetilde{S}_j=\alpha S_0+(1-\alpha) S_j$ for $1 \leq j \leq i$, we can proceed with further analysis based on the specified combination of the original dataset $S_0$ and the synthetic dataset $S_j$.
\begin{align}
    \|\widetilde{S}_1-\widetilde{S}_1'\|_{2,1}&\leq \alpha\|S_0-S_0'\|_{2,1}+(1-\alpha) \|S_1-S_1'\|_{2,1} \notag \\
    &\leq 2\alpha+(1-\alpha)\frac{2n}{2 n+1}\left((1+2B_W) e^{2B_W}\right)^L.
\end{align}
By reintroducing the mixed datasets $\widetilde{S}_1$ and $\widetilde{S}_1'$ as in-context examples into the transformer model, and considering the query set $\left\{\boldsymbol{x}_{2, j}\right\}_{j=1}^n$ as i.i.d. samples from the distribution $\mathcal{X}$, we can derive the transformer's output according to Equation \ref{proof_tramsfor_output}:
\begin{align}
    &\left\|\mathrm{TF}\left(\widetilde{S}_1\right)-\mathrm{TF}\left(\widetilde{S}_1'\right)\right\|_{\ell_2} \notag\\
    &\leq \frac{1}{2 n+1}\left((1+2B_W) e^{2B_W}\right)^L \|\widetilde{S}_1-\widetilde{S}_1'\|_{2,1} \notag \\
    &\leq  \frac{1}{2 n+1}\left((1+2B_W) e^{2B_W}\right)^L \left(2\alpha+(1-\alpha)\frac{2n}{2 n+1}\left((1+2B_W) e^{2B_W}\right)^L\right)\notag \\
    &\leq (1-\alpha)\frac{2n}{(2 n+1)^2}\left((1+2B_W) e^{2B_W}\right)^{2L}+\alpha \frac{2}{2 n+1}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^L.
\end{align}
From the above expression, we can further derive that
\begin{align}
    \left\|S_2-S_2^{\prime}\right\|_{2,1} \leq (1-\alpha)\frac{2n^2}{(2 n+1)^2}\left((1+2B_W) e^{2B_W}\right)^{2L}+\alpha \frac{2n}{2 n+1}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^L. \notag
\end{align}
Thus,
\begin{align}
    &\|\widetilde{S}_2-\widetilde{S}_2'\|_{2,1}\notag \\
    &\leq \alpha\|S_0-S_0'\|_{2,1}+(1-\alpha) \|S_2-S_2'\|_{2,1} \notag \\
    &\leq 2\alpha+(1-\alpha)^2\frac{2n^2}{(2 n+1)^2}\left((1+2B_W) e^{2B_W}\right)^{2L}+\alpha(1-\alpha) \frac{2n}{2 n+1}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^L \notag.
\end{align}
Similarly, for the 2-th generation, following analogous steps, we can derive that
\begin{align}
    &\left\|\mathrm{TF}\left(\widetilde{S}_2\right)-\mathrm{TF}\left(\widetilde{S}_2'\right)\right\|_{\ell_2} \notag\\
    &\leq \frac{1}{2 n+1}\left((1+2B_W) e^{2B_W}\right)^L \|\widetilde{S}_2-\widetilde{S}_2'\|_{2,1} \notag \\
    &\leq (1-\alpha)^2 \frac{2 n^2}{(2 n+1)^3}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{3 L}+\alpha(1-\alpha) \frac{2 n}{(2 n+1)^2}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{2L} \notag\\
    &\quad +\alpha \frac{2}{2 n+1}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^L.
\end{align}
Building on the above expression, we can further deduce that
\begin{align}
    &\left\|S_3-S_3^{\prime}\right\|_{2,1}\notag \\
    &\leq (1-\alpha)^2 \frac{2 n^3}{(2 n+1)^3}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{3 L}+\alpha(1-\alpha) \frac{2 n^2}{(2 n+1)^2}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{2L} \notag\\
    &\quad +\alpha \frac{2n}{2 n+1}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^L.
\end{align}
The discrepancy between the mixed datasets is as follows:
\begin{align}
    &\|\widetilde{S}_3-\widetilde{S}_3'\|_{2,1}\notag \\
    &\leq \alpha\|S_0-S_0'\|_{2,1}+(1-\alpha) \|S_3-S_3'\|_{2,1} \notag \\
     &\leq (1-\alpha)^3 \frac{2 n^3}{(2 n+1)^3}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{3 L}+\alpha(1-\alpha)^2 \frac{2 n^2}{(2 n+1)^2}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{2L} \notag\\
    &\quad +\alpha (1-\alpha)\frac{2n}{2 n+1}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^L+2\alpha.
\end{align}
Utilizing recursive techniques, we can obtain the following:
\begin{align}
    &\|\widetilde{S}_i-\widetilde{S}_i'\|_{2,1}\notag \\
     &\leq (1-\alpha)^i \frac{2 n^i}{(2 n+1)^i}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{i L}+\alpha(1-\alpha)^{i-1} \frac{2 n^{i-1}}{(2 n+1)^{i-1}}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{(i-1)L} \notag\\
    &\quad+...+\alpha(1-\alpha)^{2} \frac{2 n^{2}}{(2 n+1)^{2}}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{2L} +\alpha (1-\alpha)\frac{2n}{2 n+1}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^L\notag \\
    &\quad+2\alpha \notag \\
    &\leq 2(1-\alpha)^i \frac{ n^i}{(2 n+1)^i}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{i L}\notag \\
    &\quad+2\alpha\left[1-(1-\alpha) \frac{ n}{2 n+1}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^L\right]^{-1}\left[1-(1-\alpha)^i \frac{ n^i}{(2 n+1)^i}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{i L}\right].
\end{align}
Ultimately, the discrepancy between the transformer outputs after $i$ generations of the self-consuming loop for $S_0$ and $S_0^{\prime}$ can be obtained as follows:
\begin{align}
    &\left\|\mathrm{TF}\left(\widetilde{S}_i\right)-\mathrm{TF}\left(\widetilde{S}_i'\right)\right\|_{\ell_2} \notag\\
    &\leq \frac{1}{2 n+1}\left((1+2B_W) e^{2B_W}\right)^L \|\widetilde{S}_i-\widetilde{S}_i'\|_{2,1} \notag \\
   &\leq (1-\alpha)^i \frac{2 n^i}{(2 n+1)^{i+1}}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{(i+1) L}+\alpha(1-\alpha)^{i-1} \frac{2 n^{i-1}}{(2 n+1)^{i}}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{iL} \notag\\
    &\quad+...+\alpha(1-\alpha)^{2} \frac{2 n^{2}}{(2 n+1)^{3}}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{3L} +\alpha (1-\alpha)\frac{2n}{(2 n+1)^2}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{2L}\notag \\
    &\quad+2\alpha \frac{1}{2 n+1}\left((1+2B_W) e^{2B_W}\right)^L\notag \\
    &\leq 2(1-\alpha)^i \frac{ n^i}{(2 n+1)^{i+1}}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{(i+1) L}+2\alpha\left[\frac{1}{2 n+1}\left((1+2B_W) e^{2B_W}\right)^L\right] \notag \\
    &\times \left[1-(1-\alpha) \frac{ n}{2 n+1}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^L\right]^{-1}\left[1-(1-\alpha)^i \frac{ n^i}{(2 n+1)^i}\left(\left(1+2 B_W\right) e^{2 B_W}\right)^{i L}\right] \notag.
\end{align}
Subsequently, given that $\widetilde{B}_W=(1+2B_W) e^{2B_W}$, we define the measure $d$ as the $\ell_2$-norm to quantify the output discrepancy of the generative transformer model after $i$ iterations of the self-consuming loop, starting from the initial real datasets $S_0$ and $S_0^{\prime}$. In this context, the recursive stability parameter $\gamma_n^i$, as described in Definition \ref{iterative stability}, can be bounded by the following expression, providing a formal measure of the model's stability across iterations:
\begin{align}
    \left\|\operatorname{TF}(\widetilde{S}_i)-\operatorname{TF}(\widetilde{S}_i^{\prime})\right\|_{\ell_2}\lesssim 
   (1-\alpha)^i \frac{\widetilde{B}_W^{(i+1)L}}{2n+1}\notag .
\end{align}
The proof is complete.

\end{proof}






























\subsection{Proof of Theorem \ref{theo_transformer_generalization}}
In this section, building on the general theoretical framework established in Theorem \ref{theorem_generalization}, we provide the proof of Theorem \ref{theo_transformer_generalization} by analyzing the terms $\beta_n$ and $d_{\mathrm{TV}}(n)$, leveraging recent advancements in SGD \citep{zhang2022stability} and ICL \citep{zhang2023and}. The recursive stability parameter $\gamma_n^i$ is derived from Theorem \ref{therorem_stability of transformer}.

\begin{lemma}(Uniform stability of SGD in the non-convex case \citep{zhang2022stability})\label{lemma_sgd}. Assume $f$ is $\kappa$-smooth and $\rho$-Lipschitz. Running $T \gtrsim n$ iterations of $S G D$ with step size $\eta_t=\frac{1}{\beta t}$. Choose the stability of SGD satisfies
$$
\beta_n \lesssim \frac{16 \rho^2 \log n}{n}.
$$
\end{lemma}


 \begin{lemma}\citep{zhang2023and}\label{lemma_TV_trans} Let $\mathbb{P}_\theta$ represent the probability distribution induced by the transformer with parameter $\theta$. Additionally, the model $\mathbb{P}_{\hat{\theta}}$ is pretrained by the algorithm:
$$
\hat{\theta}=\underset{\theta \in \Theta}{\operatorname{argmin}}-\frac{1}{n} \sum_{t=1}^{n-1} \log \mathbb{P}_\theta\left(\boldsymbol{x}_{t+1}^n \mid S_t^n\right),
$$
where $S_t^n=\left(\boldsymbol{x}_1, \boldsymbol{y}_1, \ldots \boldsymbol{x}_{t}, \boldsymbol{y}_{t}\right)$. Furthermore, we consider the realizable setting, where ground truth probability distribution $\mathbb{P}(\cdot \mid S)$ and $\mathbb{P}_{\theta^*}(\cdot \mid S)$ are consistent for some $\theta^* \in \Theta$. Then, with probability at least $1-\delta$, the following inequality holds:
\begin{align}
\operatorname{TV}\left(\mathbb{P}(\cdot \mid S), \mathbb{P}_{\hat{\theta}}(\cdot \mid S)\right) \lesssim \frac{1}{n^{1/2}}\log (1+n)+\frac{1}{n^{1/4}}\log (1/\delta),
\end{align}
where $\lesssim$ denotes that we omit constants that are independent of $n$ and $\delta$.
 \end{lemma}


 \begin{proof}[Proof of Theorem \ref{theo_transformer_generalization}] First, we note that in the setting where the transformer generates data through in-context learning, the generalization error of the self-consuming loop is given by:
 \begin{align}
\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right|=\left|\mathbb{E}_{\boldsymbol{z}\sim\mathbb{P}(\cdot \mid S_0)} \ell(\mathcal{A}(\widetilde{S}_i), \boldsymbol{z})-\frac{1}{n} \sum_{\boldsymbol{z}_i \in \widetilde{S}_{i}}\ell(\mathcal{A}(\widetilde{S}_i), \boldsymbol{z}_i)\right|.
 \end{align}
 Now, we are ready to prove Theorem \ref{theo_transformer_generalization}. The main idea is to bound the uniform stability parameter $\beta_n$, the recursive stability parameter $\gamma_n^i$, and the learnability of the generative model through the total variation distance $d_{\mathrm{TV}}(n)$ as stated in Theorem \ref{theorem_generalization}. First, as for the bound for the total variation distance $d_{\text {TV }}(n)$ in Theorem \ref{theorem_generalization}. For Equation \ref{proof1_term4} in the proof of Theorem \ref{theorem_generalization}, we can rewrite it in the setting of in-context learning as follows:
\begin{align}
\left|R_{\widetilde{\mathcal{D}}_{i-1}}(\mathcal{A}(\widetilde{S}_i))-R_{\mathcal{D}_{i}}(\mathcal{A}(\widetilde{S}_i))\right|
&=\left|\mathbb{E}_{\boldsymbol{z}\sim\mathbb{P}(\cdot \mid \widetilde{S}_{i-1})} \ell(\mathcal{A}(\widetilde{S}_i), \boldsymbol{z}) -\mathbb{E}_{\boldsymbol{z}\sim\mathbb{P}(\cdot \mid S_{i})} \ell(\mathcal{A}(\widetilde{S}_i), \boldsymbol{z})\right| \notag \\
&=\left|\mathbb{E}_{\boldsymbol{z}\sim\mathbb{P}(\cdot \mid \widetilde{S}_{i-1})} \ell(\mathcal{A}(\widetilde{S}_i), \boldsymbol{z}) -\mathbb{E}_{\boldsymbol{z}\sim\mathbb{P}_{\hat{\theta}}(\cdot \mid \widetilde{S}_{i-1})} \ell(\mathcal{A}(\widetilde{S}_i), \boldsymbol{z})\right| \notag \\
&=\Bigg|\int_{\boldsymbol{z}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z})\left(\mathbb{P}\left(\boldsymbol{z} \mid \widetilde{S}_{i-1}\right)-\mathbb{P}_{\hat{\theta}}\left(\boldsymbol{z} \mid \widetilde{S}_{i-1}\right)\right)d\boldsymbol{z}\Bigg| \notag \\
&\leq\int_{\boldsymbol{z}}\biggl|\ell(\mathcal{A}(\widetilde{S}),\boldsymbol{z})\left(\mathbb{P}\left(\boldsymbol{z} \mid \widetilde{S}_{i-1}\right)-\mathbb{P}_{\hat{\theta}}\left(\boldsymbol{z} \mid \widetilde{S}_{i-1}\right)\right)\biggr| d\boldsymbol{z} \notag\\
&\leq M\int_{\boldsymbol{z}}\Bigl|\mathbb{P}\left(\boldsymbol{z} \mid \widetilde{S}_{i-1}\right)-\mathbb{P}_{\hat{\theta}}\left(\boldsymbol{z} \mid \widetilde{S}_{i-1}\right)\Bigr| d\boldsymbol{z} \notag\\
&= 2M TV\left(\mathbb{P}(\cdot \mid \widetilde{S}_{i-1}), \mathbb{P}_{\hat{\theta}}(\cdot \mid \widetilde{S}_{i-1})\right). \label{proof3-1}
\end{align}
Where, the second equality holds because, in the $(i-1)$-th generation of the self-consuming loop, the mixed data distribution from the $(i-1)$-th generation is reintroduced as the ground truth distribution to train the transformer. As a result, the transformer outputs the synthetic data distribution for the $i$-th generation. Thus,  $T V\left(\mathbb{P}(\cdot \mid \widetilde{S}_{j}), \mathbb{P}_{\hat{\theta}}(\cdot \mid \widetilde{S}_{j})\right)$ corresponds to $d_{\mathrm{TV}}(n)$ in Theorem \ref{theorem_generalization}. Finally, the bound for the total variation distance $d_{\text {TV }}(n)$ follows from Lemma \ref{lemma_TV_trans}.
\begin{align}
    d_{\text {TV }}(n) \lesssim \frac{1}{n^{1 / 2}} \log (1+n)+\frac{1}{n^{1 / 4}} \log (1 / \delta).
\end{align}
Similarly, for the recursive stability parameter in the self-consuming loop, we rederive Equation \ref{proof_19} from the proof of Theorem \ref{theorem_generalization} under the in-context learning setting:
\begin{align}
&|\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left(S_{0,\alpha}^j\right)} \left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)\right] \notag\\
&-\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left((S_{0,\alpha}^t)^j\right)}\left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left((S_{0,\alpha}^t)^j \cup S_{i,1-\alpha}\right), \boldsymbol{z}_{0,j}\right)\right]| \notag \\
&=\left|\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0}\left[\mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left(S_{0,\alpha}^j\right)} \ell\left(\mathcal{A}\left(\left(S_{0, \alpha}^t\right)^j \cup S_{i, 1-\alpha}\right), \boldsymbol{z}\right)\right. \right.\notag \\
&\left.\left. \quad-\mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left((S_{0,\alpha}^t)^j\right)} \ell\left(\mathcal{A}\left(\left(S_{0, \alpha}^t\right)^j \cup S_{i, 1-\alpha}\right), \boldsymbol{z}\right)\right]\right|  \notag\\
&\quad+\left|\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0}\left[\mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left(S_{0,\alpha}^j\right)} \ell\left(\mathcal{A}\left(\left(S_{0, \alpha}^t\right)^j \cup S_{i, 1-\alpha}\right), \boldsymbol{z}_{0, j}\right)\right. \right.\notag \\
&\left.\left. \quad-\mathbb{E}_{S_{i,1-\alpha} \sim \mathcal{D}_i^{n(1-\alpha)}\left((S_{0,\alpha}^t)^j\right)} \ell\left(\mathcal{A}\left(\left(S_{0, \alpha}^t\right)^j \cup S_{i, 1-\alpha}\right), \boldsymbol{z}_{0, j}\right)\right]\right|  \notag\\
&\leq 2n(1-\alpha)\beta_n \left\|\mathrm{TF}\left(\left(S_{0, \alpha}^t\right)^j \cup S_{i-1, 1-\alpha}\right)-\mathrm{TF}\left(\left(S_{0, \alpha}^t\right)^j \cup S_{i-1, 1-\alpha}^{\prime}\right)\right\|_{\ell_2}\notag \\
&\lesssim 2n(1-\alpha)\beta_n \frac{2\widetilde{B}_W^L}{2n+1}\left[((1-\alpha)\widetilde{B}_W^L)^{i-1}+\alpha\frac{1-((1-\alpha)\widetilde{B}_W^L)^{i-1}}{1-(1-\alpha)\widetilde{B}_W^L}\right]\notag\\ 
&=2n(1-\alpha)\beta_n \gamma_n^{i-1}.
\end{align} 
 For the uniform stability parameter $\beta_n$ of SGD algorithm, we can derive the bound from Lemma \ref{lemma_sgd}. Substituting above results into Theorem \ref{theo_transformer_generalization}, we obtain the following conclusion:
 \begin{align}
&\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right|\notag \\
&\leq \left((1-\alpha) \beta_n \log (n(1-\alpha))+\alpha\left(\beta_n+(1-\alpha)\rho^2\gamma_n^{i-1}\right) \log (n \alpha)\right) \log (\frac{1}{\delta})\notag \\
&\quad +\left(\sqrt{(1-\alpha) n} \alpha \beta_n+M n^{-1 / 2}(\sqrt{1-\alpha}+\sqrt{\alpha})\right) \sqrt{\log (\frac{1}{\delta})}+2 M\left(1-(1-\alpha)^i\right) \alpha^{-1} d_{\mathrm{TV}}(n)\notag \\
&\leq \beta_n\left[ (1-\alpha)\log(n(1-\alpha))\log (\frac{1}{\delta})+\alpha \log (n\alpha)\log (\frac{1}{\delta})+\alpha\sqrt{(1-\alpha)n \log \frac{1}{\delta}}\right]\notag \\
&\quad +\gamma_n^{i-1}\alpha(1-\alpha)\rho^2 \log (n\alpha)\log (\frac{1}{\delta})+n^{-1/2}M(\sqrt{1-\alpha}+\sqrt{\alpha})\sqrt{\log (\frac{1}{\delta})}+2d_{\mathrm{TV}}(n) M\left(1-(1-\alpha)^i\right) \alpha^{-1}  \notag \\
&\lesssim n^{-1/2}\log (n) M\rho^2 \alpha \sqrt{1-\alpha}\log \frac{1}{\delta}+n^{-1}\rho^2((1-\alpha)\widetilde{B}_W^L)^i \alpha \log^2(n) \log \left(\frac{1}{\delta}\right) \notag \\
&\quad +n^{-1/4}\alpha^{-1} M\left(1-(1-\alpha)^i\right) \log (\frac{1}{\delta}).
\end{align}
 \end{proof}



















 \subsection{Proof of Theorem \ref{theorem_expanding cylce}}
 In this section, we prove Theorem \ref{theorem_expanding cylce}. The proof follows a similar approach to that of Theorem \ref{theo_transformer_generalization}; however, it is more intricate due to the fact that the mixed dataset in Theorem \ref{theorem_expanding cylce} contains synthetic data from all previous generations. Each generation's synthetic dataset depends on the synthetic datasets of previous generations, leading to a more complex non-i.i.d. setting. Similar to Theorem \ref{theo_transformer_generalization}, we begin by decomposing the generalization error into two components: the \textit{Cumulative Distribution Shift Across Generations} and the \textit{Generalization Error on Mixed Distributions}.
 
 The main proof is as follows:

\begin{proof}[Proof of Theorem \ref{theorem_expanding cylce}]
We begin by decomposing the generalization error as follows:
\begin{align}
\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right| \leq \underbrace{\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))\right|}_{\text {Cumulative distribution shift across generations}}+\underbrace{\left| R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i)) \right|}_{\text {Generalization error on mixed distributions}}. \notag
\end{align}


\textbf{Upper Bounding Cumulative Distribution Shift Term}


For the term $\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))\right|$, we first note that $\widetilde{\mathcal{D}}_i=\frac{1}{1+i\lambda}\mathcal{D}_0+\frac{\lambda}{1+i\lambda} \mathcal{D}_1+\frac{\lambda}{1+i\lambda} \mathcal{D}_2+...+\frac{\lambda}{1+i\lambda} \mathcal{D}_i$. Therefore, we obtain:
\begin{align}
    &\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))\right|\notag \\
    &=\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{1+i\lambda} R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i)-\frac{\lambda}{1+i\lambda}R_{\mathcal{D}_1}(\mathcal{A}(\widetilde{S}_1))-...-\frac{\lambda}{1+i\lambda}R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))\right| \notag \\
    &=\left|\frac{i\lambda}{1+i\lambda} R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i)-\frac{\lambda}{1+i\lambda}R_{\mathcal{D}_1}(\mathcal{A}(\widetilde{S}_1))-...-\frac{\lambda}{1+i\lambda}R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))\right| \notag \\
    &\leq \frac{\lambda}{1+i\lambda}\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i)-R_{\mathcal{D}_1}(\mathcal{A}(\widetilde{S}_i))\right|+...+\frac{\lambda}{1+i\lambda}\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i)-R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))\right| \notag\\
    &\leq \frac{\lambda}{1+i\lambda}\sum_{j=1}^i \left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i)-R_{\mathcal{D}_j}(\mathcal{A}(\widetilde{S}_i))\right|.\label{proof4_term1}
\end{align}

Furthermore, we can further decompose it as follows:
\begin{align}
   \left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i)-R_{\mathcal{D}_j}(\mathcal{A}(\widetilde{S}_i))\right| \leq \left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{j-1}}(\mathcal{A}(\widetilde{S}_i))\right|+\left|R_{\widetilde{\mathcal{D}}_{j-1}}(\mathcal{A}(\widetilde{S}_i))-R_{\mathcal{D}_{j}}(\mathcal{A}(\widetilde{S}_i))\right|. \label{proof4_term2}
\end{align}
By substituting inequality \ref{proof4_term2} into inequality \ref{proof4_term1}, we obtain:
\begin{align}
    &\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))\right| \notag \\
    &\leq  \frac{\lambda}{1+i \lambda} \sum_{j=1}^i \left(\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{j-1}}(\mathcal{A}(\widetilde{S}_i))\right|+\left|R_{\widetilde{\mathcal{D}}_{j-1}}(\mathcal{A}(\widetilde{S}_i))-R_{\mathcal{D}_{j}}(\mathcal{A}(\widetilde{S}_i))\right| \right)\label{proof4_term3}.
\end{align}
Thus, from equation \ref{proof3-1} in the proof of Theorem \ref{theo_transformer_generalization} and lemma \ref{lemma_TV_trans}, we obtain:
\begin{align}
    \left|R_{\widetilde{\mathcal{D}}_{j-1}}(\mathcal{A}(\widetilde{S}_i))-R_{\mathcal{D}_{j}}(\mathcal{A}(\widetilde{S}_i))\right| &\leq  2 M T V\left(\mathbb{P}\left(\cdot \mid \widetilde{S}_{j-1}\right), \mathbb{P}_{\hat{\theta}}\left(\cdot \mid \widetilde{S}_{j-1}\right)\right)\notag \\
   & \lesssim Mn_{j-1}^{-1/4}\log n_{j-1} \log (1/\delta)\label{proof4_term4}.
\end{align}
Incorporating inequality \ref{proof4_term4} into inequality \ref{proof4_term3}, we arrive at:
\begin{align}
    &|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))|\notag \\
    &\lesssim\frac{\lambda}{1+i \lambda} \sum_{j=1}^i \left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{j-1}}(\mathcal{A}(\widetilde{S}_i))\right|+\frac{\lambda}{1+i \lambda}\sum_{j=0}^{i-1}Mn_j^{-1/4}\log n_j \log (1/\delta)\label{proof4_term5}.
\end{align}
Let $f(i)=\sum_{j=0}^{i-1}Mn_j^{-1/4}\log n_j \log (1/\delta)$, Then, we obtain:
\begin{align}
    &|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))|\notag \\
    &\lesssim\frac{\lambda}{1+i \lambda}\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{i-1}}(\mathcal{A}(\widetilde{S}_i))\right|+...+\frac{\lambda}{1+i \lambda}\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{1}}(\mathcal{A}(\widetilde{S}_i))\right|+\frac{\lambda}{1+i \lambda}f(i)\notag.
\end{align}
Similarly, we get:
\begin{align}
    &|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{i-1}}(\mathcal{A}(\widetilde{S}_i))|\notag \\
    &\lesssim\frac{\lambda}{1+(i-1) \lambda}\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{i-2}}(\mathcal{A}(\widetilde{S}_i))\right|+...+\frac{\lambda}{1+(i-1) \lambda}\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{1}}(\mathcal{A}(\widetilde{S}_i))\right|\notag\\
    & \quad+\frac{\lambda}{1+(i-1) \lambda}f(i-1) \notag.
\end{align}
Then, we have
\begin{align}
 &|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))|\lesssim \frac{\lambda}{1+i \lambda} f(i)+\frac{\lambda}{1+i \lambda}\frac{\lambda}{1+(i-1) \lambda} f(i-1)+\notag \\
 &(\frac{\lambda}{1+i \lambda}+\frac{\lambda}{1+i \lambda} \frac{\lambda}{1+(i-1) \lambda})(\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{i-2}}(\mathcal{A}(\widetilde{S}_i))\right|+...+\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_{1}}(\mathcal{A}(\widetilde{S}_i))\right|).
\end{align}
Thus, by applying recursive techniques, we obtain the following result:
\begin{align}
 &|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))|\notag \\
 &\lesssim \frac{\lambda}{1+i \lambda} f(i)+\frac{\lambda}{1+i \lambda}\frac{\lambda}{1+(i-1) \lambda} f(i-1)+
 (\frac{\lambda}{1+i \lambda}\frac{\lambda}{1+(i-2) \lambda}+\mathcal{O}(\frac{1}{(1+i\lambda)^2}))f(i-2)\notag \\
 &+...+(\frac{\lambda}{1+i \lambda}\frac{\lambda}{1+\lambda}+\mathcal{O}(\frac{1}{(1+i\lambda)}))f(1) \notag \\
 &\lesssim \frac{\lambda}{1+i \lambda} \left[f(i)+\frac{\lambda}{1+(i-1) \lambda} f(i-1)+\frac{\lambda}{1+(i-2) \lambda}f\left(i-2\right)+...+\frac{\lambda}{1+\lambda}f(1)\right]\notag \\
 &\lesssim M\log \frac{1}{\delta}\frac{\lambda}{1+i \lambda} \Big[n_{i-1}^{-\frac{1}{4}}\log (n_{i-1})+(1+\frac{\lambda}{1+(i-1) \lambda})n_{i-2}^{-\frac{1}{4}}\log (n_{i-2})+\notag \\
 &(1+\frac{\lambda}{1+(i-1) \lambda}+\frac{\lambda}{1+(i-2) \lambda})n_{i-3}^{-\frac{1}{4}}\log (n_{i-3})+...+(1+...+\frac{\lambda}{1+\lambda})n_{0}^{-\frac{1}{4}}\log (n_{0})\Big] \notag \\
 &\lesssim n^{-\frac{1}{4}}\log ((1+i\lambda)n)M\log\frac{1}{\delta}\label{proof4_hhhh}.
\end{align}





\textbf{Upper Bounding Generalization Error on Mixed Distributions Term}


Next, we turn our attention to the term $|R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))|$. Our primary objective is to establish a moment bound for this expression.
%注意这个地方考虑的是noniid 的情况，也就是说\widetilde{S}_i里面包含的是S_0与S_i,但是S_i depend on S_0，所以这个地方是个non-iid的情况。

\begin{align}
&\left\|R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right\|_{p} \notag \\
&=\Big\|\frac{1}{1+i\lambda} R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))+\frac{\lambda}{1+i\lambda}R_{\mathcal{D}_1}(\mathcal{A}(\widetilde{S}_i))+\frac{\lambda}{1+i\lambda}R_{\mathcal{D}_2}(\mathcal{A}(\widetilde{S}_i))...+\frac{\lambda}{1+i\lambda}R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))\notag \\
&\quad -\frac{1}{(1+i\lambda)n}\sum_{\boldsymbol{z}_{i}\in S_{0}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})-\frac{1}{(1+i\lambda) n}\sum_{\boldsymbol{z}_{i}\in S_{1}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})-...-\frac{1}{(1+i\lambda)n}\sum_{\boldsymbol{z}_{i}\in S_{i}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\Big\|_{p} \notag\\
&\leq \underbrace{\left\|\frac{1}{1+i\lambda} R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{(1+i\lambda)n}\sum_{\boldsymbol{z}_{i}\in S_{0}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\right\|_{p}}_{\text{Term 0}}+\underbrace{\left\|\frac{\lambda}{1+i\lambda}R_{\mathcal{D}_1}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{(1+i\lambda)n}\sum_{\boldsymbol{z}_{i}\in S_{1}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\right\|_{p}}_{\text{Term 1}} \notag \\
&\quad+..+\underbrace{\left\|\frac{\lambda}{1+i\lambda} R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{(1+i\lambda)n}\sum_{\boldsymbol{z}_{i}\in S_{i}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\right\|_{p}}_{\text{Term i}}\label{proof-generalizati-decomp}.
\end{align}
%这个地方思考的是S_0和S_i都是n个数据，然后取比例混合。
Fixing $S_0, S_1, \dots, S_{i-1}$, the data in $S_i$ are independent. Following a similar approach to the proof of Theorem \ref{theorem_generalization}, we utilize this property along with Lemma \ref{theorem_moment} to bound Term i. Consequently, from Equation \ref{proof-Term 2} in the proof of Theorem \ref{theorem_generalization}, we obtain:
\begin{align}
    \left\|\frac{\lambda}{1+i\lambda}R_{\mathcal{D}_i}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{(1+i\lambda )n}\sum_{\boldsymbol{z}_{i}\in S_{i}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\right\|_{p}\lesssim p \frac{\lambda}{1+i\lambda} \beta_{(1+i\lambda) n} \log (\lambda n)+\frac{M}{1+i\lambda} \sqrt{\frac{p\lambda}{n}} \label{proof4-Term i}.
\end{align}
%Next, we consider Term 0. Similar to equation \ref{proof1-delta1} in Proof of Theorem \ref{theo_transformer_generalization}, we first apply McDiarmid's inequality to establish the result:
%\begin{align}
%&\Big\|\frac{1}{1+i\lambda} R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{(1+i\lambda)n}\sum_{\boldsymbol{z}_{i}\in S_{0}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\notag \\
%&\quad -\mathbb{E}_{S_{i} \sim \mathcal{D}_i^{\lambda n}} (\frac{1}{1+i\lambda} R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{(1+i\lambda)n}\sum_{\boldsymbol{z}_{i}\in S_{0}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i}))\Big\|_p \notag \\
%&\leq \frac{4}{1+i\lambda} \sqrt{\lambda n p} \beta_{(1+i\lambda)n} \lesssim \frac{1}{1+i\lambda} \sqrt{\lambda n p} \beta_{(1+i\lambda)n} \label{proof4-delta1}
%\end{align}
Next, we consider Term 0. Similar to Proof of Theorem \ref{theo_transformer_generalization}, we first introduce a set of functions and apply Lemma \ref{theorem_moment} to bound Term 0. Specifically, we define $h_j(S)$, which serves a similar role to the $g_i$ 's in Lemma \ref{theorem_moment}, as follows:

\begin{align}
&h_j(S_{0})\notag\\
&=\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left(S_{0}^j \cup  S_1\cup ...\cup S_{i}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left(S_{0}^j \cup S_1\cup ...\cup S_{i}\right), \boldsymbol{z}_{0,j}\right)\right],
\end{align}
where $\boldsymbol{z}_{0, j}$ denote the $j$-th data point in $S_{0}$, and $S_{0}^j$ represent the dataset obtained by replacing $\boldsymbol{z}_{0, j}$ with $\boldsymbol{z}_{0, j}^{\prime}$. Moreover, following the procedure above, we observe that $\left|h_j\right| \leq M$ and $\mathbb{E}\left[h_j \mid S_{0, \alpha}^{\backslash j}\right]=0$
. More intricately, we will now prove that $h_j$ exhibits a bounded difference. However, it is important to note that $S_1, \ldots, S_i$ all depend on $S_0$, so when a single data point in $S_0$ is changed, the corresponding datasets will also change. We denote these modified datasets as $S_1^{\prime}, \ldots, S_i^{\prime}$ and consequently, we have the following:
\begin{align}
  & | h_j(S_0)-h_j(S_0^t)|\notag \\
 &= |\mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left(S_{0}^j \cup  S_1\cup ...\cup S_{i}\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left(S_{0}^j \cup S_1\cup ...\cup S_{i}\right), \boldsymbol{z}_{0,j}\right)\right] |\notag\\
&-  \mathbb{E}_{\boldsymbol{z}_{0,j}^{\prime} \sim \mathcal{D}_0} \left[\mathbb{E}_{\boldsymbol{z} \sim \mathcal{D}_0} \ell\left(\mathcal{A}\left((S_{0}^t)^j \cup  S_1'\cup ...\cup S_{i}'\right), \boldsymbol{z}\right)-\ell\left(\mathcal{A}\left((S_{0}^t)^j \cup S_1'\cup ...\cup S_{i}'\right), \boldsymbol{z}_{0,j}\right)\right]\notag\\ 
&\leq 2\beta_{(1+i\lambda)n}\Big(\|S_{0}^j-(S_{0}^t)^j\|_{\ell_2} +\|S_1-S_1'\|_{\ell_2}+...+\|S_i-S_i'\|_{\ell_2}\Big).
\end{align}
Thus, by applying the recursive stability established in Theorem \ref{therorem_stability of transformer}, it is important to first note that in Theorem \ref{therorem_stability of transformer}, the mixed dataset is defined as $\widetilde{S}_j=\alpha S_0+(1-\alpha) S_j$, whereas in this theorem, the mixed dataset is defined as $\widetilde{S}_i=\sum_{j=0}^i S_j$. Therefore, by following the proof steps outlined in Theorem \ref{therorem_stability of transformer}, we can derive the following:
\begin{align}
| h_j(S_0)-h_j(S_0^t)|\lesssim 2\beta_{(1+i\lambda)n}\Big(i !\widetilde{B}_W^{iL}\Big)\notag. 
\end{align}
Thus, we apply lemma \ref{theorem_moment}:
$$
\begin{aligned}
\left\|\sum_{j=1}^{n} h_j(S_{0})\right\|_p & \leq 12 \sqrt{2} p n 2\beta_{(1+i\lambda)n}\left(i !\widetilde{B}_W^{iL}\right) \log (n)+4 M \sqrt{p n} \notag \\
&\lesssim  p \frac{\rho^2}{1+i\lambda}\Big(i !\widetilde{B}_W^{iL}\Big) \log (n(1+i\lambda))+ M \sqrt{p n}.\\
\end{aligned}
$$
We observe that the difference between Term 0 and $\frac{1}{(1+i\lambda)n}\left\|\sum_{j=1}^{n} h_j(S_{0})\right\|_p$ is negligible. Thus, we can bound Term 0 as follows:
\begin{align}
   & \left\|\frac{1}{1+i\lambda} R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{(1+i\lambda)n}\sum_{\boldsymbol{z}_{i}\in S_{0}}\ell(\mathcal{A}(\widetilde{S}_i),\boldsymbol{z}_{i})\right\|_{p}\notag \\
   & \lesssim p \frac{\rho^2}{(1+i\lambda)^2n}\Big(i !\widetilde{B}_W^{iL}\Big) \log (n(1+i\lambda))+ \frac{1}{1+i\lambda}M \sqrt{p/n}.
\end{align}
Using the same method, for Term $j$, where $1 \leq j \leq i-1$, we can derive the following:
\begin{align}
  &  \left\|\frac{\lambda}{1+i\lambda}R_{\mathcal{D}_j}(\mathcal{A}(\widetilde{S}_i))-\frac{1}{(1+i\lambda)n}\sum_{\boldsymbol{z}_{i}\in S_{1}}\ell(\mathcal{A}(\widetilde{S}_j),\boldsymbol{z}_{i})\right\|_{p}\notag \\
  &\lesssim p \frac{\rho^2}{(1+i\lambda)^2n}\Big(j !\widetilde{B}_W^{jL}\Big) \log (n(1+i\lambda))+\frac{1}{1+i\lambda}M \sqrt{p/n}.
\end{align}
In summary, we can finally bound the Generalization Error on the Mixed Distributions term as follows:
\begin{align}
 &\left\|R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right\|_{p} \notag \\
 &\lesssim p \frac{\rho^2}{(1+i\lambda)^2n} \log ((1+i\lambda) n)i!\widetilde{B}_W^{(i+1) L}+\frac{Mi}{1+i\lambda} \sqrt{\frac{p}{n}}. \notag
\end{align}
Then, according to Lemma \ref{lemma_highprobability}, we obtain, with probability at least $1-\delta$:
\begin{align}
 &\left\|R_{\widetilde{\mathcal{D}}_i}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right\|_{p} \notag \\
 &\lesssim  \frac{\rho^2}{(1+i\lambda)^2n} \log ((1+i\lambda) n)i!\widetilde{B}_W^{(i+1) L}\log\frac{1}{\delta}+\frac{Mi}{1+i\lambda} \sqrt{\frac{1}{n}\log\frac{1}{\delta}}. \notag
\end{align}
Then, combine the above inequality with inequality \ref{proof4_hhhh}, we obtain:
\begin{align}
    &\left|R_{\mathcal{D}_0}(\mathcal{A}(\widetilde{S}_i))-\widehat{R}_{\widetilde{S}_i}(\mathcal{A}(\widetilde{S}_i))\right|\notag \\
    &\lesssim n^{-\frac{1}{4}}\log ((1+i\lambda)n)M\log\frac{1}{\delta}+ \frac{\rho^2}{(1+i\lambda)^2n} \log ((1+i\lambda) n)i!\widetilde{B}_W^{(i+1) L}\log\frac{1}{\delta}+\frac{Mi}{1+i\lambda} \sqrt{\frac{1}{n}\log\frac{1}{\delta}}  \notag\\
    &\lesssim n^{-\frac{1}{2}} \frac{M i}{1+i \lambda} \sqrt{\log \frac{1}{\delta}}+n^{-1}\frac{\rho^2}{(1+i\lambda)^2} \log ((1+i\lambda) n)i!\widetilde{B}_W^{(i+1) L}\log\frac{1}{\delta}\notag \\
    &\quad+n^{-\frac{1}{4}}\log ((1+i\lambda)n)M\log\frac{1}{\delta}\notag.
\end{align}
The proof is complete.



\end{proof}


\section{Experiments}
In this section, we present some experimental results. Specifically, we trained transformer models to in-context learn linear functions within STLs.

In these experiments, we considered the class of linear functions:
\[
\mathcal{F} = \left\{ f \mid f(\boldsymbol{x}) = \boldsymbol{w}^\top \boldsymbol{x}, \boldsymbol{w} \in \mathbb{R}^d \right\},  
\]
in \(d = 5\) dimensions. We sampled \(\boldsymbol{x}_1, \ldots, \boldsymbol{x}_k, \boldsymbol{x}_{\text{query}}\), and \(\boldsymbol{w}\) independently from the isotropic Gaussian distribution \(\mathcal{N}(0, I_d)\). For each \(x_i\), we computed \(y_i = \boldsymbol{w}^\top \boldsymbol{x}_i\) and constructed the prompt as:
\[
P = (\boldsymbol{x}_1, y_1, \boldsymbol{x}_2, y_2, \ldots, \boldsymbol{x}_k, y_k, \boldsymbol{x}_{\text{query}}).  
\]

We employed a 12-layer, 8-head GPT-2 model with a hidden size of 256, trained on an \(\mathbb{R}^5\) linear regression task with 40 in-context examples. Two cases were considered:
\begin{itemize}
    \item \textbf{Mixed Case:} Fresh data and generated data were mixed in a 0.5 ratio.
    \item \textbf{Full Synthetic Case:} No fresh data was used.
\end{itemize}

The results of these experiments are summarized below:
\[
\begin{array}{|c|c|c|c|c|c|c|}
\hline
\text{Loop} & 1 & 2 & 3 & 4 & 5 & 6\\
\hline
\text{Full Synthetic} & 0.3817 & 1.4975 & 1.5396 & 2.0836 & 2.3912 & 2.8764\\
\hline
\text{Mixed} & 0.3817 & 0.4208 & 0.4391 & 0.4503 & 0.4641 & 0.4702\\
\hline
\end{array}
\]

As observed, the error accumulates progressively with more self-consuming loops, particularly in the full synthetic case, where the error grows rapidly. In contrast, maintaining a constant-sized proportion of real data effectively reduces the loss, which is consistent with our theoretical findings.