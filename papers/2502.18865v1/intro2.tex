The quest of high-quality data is paramount in the training of generative artificial intelligence (AI), such as large language models (LLMs). However, the vast reservoir of publicly available data on the internet has nearly been exhausted \citep{villalobos2022will}, pushing the research community to seek innovative yet plausible solutions. One promising approach is to train the next generation of LLMs using synthetic data generated by earlier generations of the models themselves \citep{briesch2023large}. Additionally, reliance on synthetic data has become almost unavoidable, as many existing datasets are already polluted with synthetic content \citep{schuhmann2022laion}, which proves difficult to detect reliably \citep{sadasivan2023can}. This has led to the development of Self-consuming Training Loops (STLs), as illustrated in Figure \ref{figure_selfconsuming}, where generative models are recursively trained on a mix of real and synthetic data generated by the models themselves. In theory, these STLs of data creation and refinement could propel models to new levels of capability, reducing reliance on external datasets. 

However, despite their potential, the empirical results of STLs have been highly inconsistent across studies \citep{shumailov2024ai,alemohammadself,xing2025caveats,dohmatob2024strong}. Some studies \citep{shumailov2024ai} have encountered significant setbacks—certain models have shown signs of stagnation, failing to improve or adapt, while others have even regressed, leading to sharp declines in performance. Conversely, other works \citep{gerstgrasser2024is,gillmanself,alemohammad2024self,ferbach2024self} have successfully avoided model collapse by incorporating sufficient real data, augmenting with synthetic data, or introducing guidance during the generation process. However, these observed phenomena lack thorough theoretical explanations. 

%Research on deep generative models within STLs has garnered significant scholarly interest due to their immense potential \citep{shumailov2024ai}. Much of this research has primarily focused on empirical analyses \citep{martinez2023towards}. Notably, \citep{shumailov2024ai, briesch2023large} have observed a considerable decline in output diversity after successive training generations. In a related vein, \citep{alemohammadself} suggest that incorporating a sufficient proportion of real data can effectively mitigate the risk of model collapse within STLs. Furthermore, other studies \citep{gerstgrasser2024is} indicate that the gradual accumulation of synthetic data across multiple generations can also serve as a safeguard against collapse.

When and how do STLs generalize effectively, thereby preventing model collapse from a theoretical perspective? Even among “refined” LLMs drawing from similar pools of model-generated data, the results vary significantly \citep{briesch2023large,fu2024championing}. These inconsistencies highlight the urgency of establishing theoretical guarantees for STLs by exploring the underlying mechanisms that determine when synthetic data generation either facilitates or impedes model development. Initial theoretical explorations have started to address these gaps. For instance, \cite{shumailov2024ai} and \cite{alemohammadself} demonstrated model collapse when models were trained exclusively on synthetic data, using simplified Gaussian models to illustrate this issue. In a more detailed theoretical study, \cite{bertrandstability} derived upper bounds on parameter deviations for likelihood-based models in STLs, establishing convergence under strict statistical and optimization assumptions. Meanwhile, \cite{futowards} relaxed these assumptions and provided bounds on the divergence between synthetic and real data distributions for a simplified diffusion model.

%Despite these empirical advancements, there remains a notable absence of theoretical guarantees and insights regarding the prevention of model collapse in STLs. Initial theoretical efforts have begun to address these gaps. \cite{shumailov2024ai} and \cite{alemohammadself} demonstrated model collapse in settings where models were trained exclusively on synthetic data, using simplified Gaussian models to illustrate this issue. In a more detailed theoretical exploration, \cite{bertrandstability} derived upper bounds on parameter deviations within STLs, establishing convergence under strict assumptions concerning statistical and optimization errors. \cite{futowards}, on the other hand, relaxed these assumptions and provided bounds on the divergence between synthetic and real data distributions.


However, existing theoretical research lacks a unified framework and has yet to thoroughly investigate the generalization error of STLs. Additionally, current studies often overlook the role of model architectures in preventing model collapse. Moreover, the behavior of transformers within STLs remains largely unexamined, leaving significant theoretical gaps in the literature. Notably, there is limited exploration of the theoretical trade-offs introduced by synthetic data augmentation. This paper aims to address these gaps with the following contributions:

1. \textbf{Theoretical Generalization Framework:} This paper fills a gap in prior research by being the first to establish generalization error bounds. The key innovation, recursive stability, is introduced to address the core theoretical challenges, specifically the complex recursive structures and the non-i.i.d. nature of the data. Moreover, we demonstrate that the generalization error converges under the following conditions: (1) the generative model satisfies recursive stability, and (2) the proportion of real data is maintained at a non-negligible constant level, thus preventing model collapse.


2. \textbf{Application to Transformers in In-Context Learning:} This paper is the first to extend the theoretical framework to transformer models in in-context learning. We prove that transformers in this setting satisfy recursive stability with a constant-level proportion of real data, controlling output differences in STLs under small perturbations to the initial dataset. Consequently, we show that the generalization error is bounded by $\mathcal{O}(n^{-1}\log^2(n) + n^{-1/2}\log(n) + n^{-1/4})$.

3. \textbf{Trade-off Analysis of Synthetic Data Augmentation:}  We investigate the trade-off in synthetic data augmentation. By employing decomposition techniques, we demonstrate that while synthetic data improves the generalization performance of each generation on mixed datasets, it concurrently exacerbates distribution divergence across successive generations. Our theoretical findings further show that the optimal size of synthetic augmentation increases as the size of real dataset decreases.
