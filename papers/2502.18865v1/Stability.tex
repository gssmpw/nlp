Algorithmic stability measures the impact of modifying or removing a small number of examples from the training set on the resulting model, a key concept in statistical learning theory \citep{bousquet2002stability}. Its primary advantage lies in providing generalization bounds independent of model capacity. Among various stability notions \citep{shalev2010learnability}, we focus on uniform stability, the most widely studied form. Let $S$ and $S'$ be two datasets differing by one point. Then, we formally define uniform stability as follows:

\begin{definition}(Uniform Stability \citep{bousquet2002stability}). Algorithm $\mathcal{A}$ is uniformly $\beta_n$-stable with respect to the loss function $\ell$ if the following holds
$$
\forall S,\ S' \in \mathcal{Z}^n,\ \forall \boldsymbol{z} \in \mathcal{Z},\ \sup _{\boldsymbol{z}}\left|\ell(\mathcal{A}(S), \boldsymbol{z})-\ell\left(\mathcal{A}\left(S'\right), \boldsymbol{z}\right)\right| \leq \beta_n.
$$
\end{definition}
Traditional notions of stability have predominantly been studied in the context of learning algorithms, such as SGD \citep{lei2020fine}. More recently, there has been significant progress in extending the concept of stability to generative models \citep{farnia2021train,zheng2023toward,li2023transformers}. Building on these advancements, we propose \textit{recursive stability} to specifically address generative models within STLs. This new stability measure is designed to quantify the differences in a generative modelâ€™s outputs after multiple
generations of recursive training when small perturbations are applied to the initial real dataset. The formal definition of recursive stability is presented below.

\begin{definition}(Recursive Stability)\label{iterative stability}
Let \(S_0\) represent the original real dataset, and \(S'_0\) denote a dataset differing from \(S_0\) by a single example. A generative model \(\mathcal{G}\) is said to be recursively \(\gamma_n^{i,\alpha}\)-stable with respect to the distance measure \(d\) after the \(i\)-th generation of STLs, where the ratio of real to synthetic data is set to \(\alpha\), if the following condition holds:  
\[
\forall S_0, S'_0 \in \mathbb{Z}^n, \quad d\left(\mathcal{G}^{(i)}(S_0), \mathcal{G}^{(i)}(S'_0)\right) \leq \gamma_n^{i,\alpha}.
\]  
where $\mathcal{G}^{(i)}$ denotes the output of the generative model at the $i$-th generation in the STLs. The distance measure $d$ quantifies the deviation between the outputs generated from inputs $S_0$ and $S_0'$ across STLs. Specifically, $d$ can be defined using Total Variation (TV) distance, Kullback-Leibler (KL) divergence, or various norms (e.g., $\ell_2$ norm), allowing flexibility in assessing the differences in generated outputs.
\end{definition}
