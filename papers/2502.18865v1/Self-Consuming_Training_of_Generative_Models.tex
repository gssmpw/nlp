Generative models have made significant strides in producing highly realistic data, such as images and text, which are frequently shared online and often indistinguishable from real content. Meanwhile, the supply of real data has nearly been exhausted. Consequently, deep generative models increasingly rely on synthetic data, either unintentionally \citep{schuhmann2022laion} or intentionally \citep{huang2022large}. This reliance creates a recursive cycle where successive generations are trained on mixed datasets of real and synthetic data, a process known as an STL, as shown in Figure \ref{figure_selfconsuming}.

More concretely, we explore a stochastic process that evolves through sequential generations. In an STL, we start with an initial dataset $S_0$, consisting of real data points $\boldsymbol{z} \in \mathcal{Z}$, sampled from the original real distribution $\mathcal{D}_0$. The initial generative model $\mathcal{G}_0$ is trained on this real dataset $S_0$, producing the first generation synthetic dataset $S_1$, whose distribution is denoted as $\mathcal{D}_1$. Next, the real dataset $S_0$ is combined with the synthetic dataset $S_1$ in a certain proportion to form a new mixed dataset $\widetilde{S}_1$, with distribution $\widetilde{\mathcal{D}}_1$. The next generation generative model $\mathcal{G}_1$ is then trained on this mixed dataset $\widetilde{S}_1$. Moving forward, for each subsequent generation $1\leq j \leq i$, the mixed dataset $\widetilde{S}_j$ is composed of real data and synthetic data from previous generations. The generative model $\mathcal{G}_j$ is trained on $\widetilde{S}_j$, producing the synthetic dataset $S_{j+1}$. This STL proceeds iteratively until the maximum generation, denoted as $i$, is reached.

%This self-consuming loop continues iteratively until the maximum generation is reached, as described in Algorithm 1.