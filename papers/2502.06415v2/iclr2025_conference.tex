
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[font=normalsize,labelfont=bf,tableposition=top]{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{array}

\usepackage{float}      % To control figure placement
\usepackage[symbol]{footmisc}

\definecolor{Gray}{gray}{0.85}

\pdfobjcompresslevel=1

% \usepackage{floatrow}
\usepackage{graphicx,wrapfig,lipsum}

% \def\shownotes{1} 
%  \ifnum\shownotes=1
% \newcommand{\authnote}[2]{{[#1: #2]}}
% \else 
% \newcommand{\authnote}[2]{{}}
% \fi
% \newcommand{\CS}[1]{{\color{blue}\authnote{CS}{#1}}}
% \newcommand{\JM}[1]{{\color{red}\authnote{JM}{#1}}}
% \newcommand{\SH}[1]{{\color{magenta}\authnote{SH}{#1}}}
% \newcommand{\YC}[1]{{\color{brown}\authnote{YC}{#1}}}
% \newcommand{\FX}[1]{{\color{cyan}{#1}}}
% \newcommand{\SC}[1]{{\color{orange}\authnote{SC}{#1}}}

% \newcommand{\itso}{}

\title{Systematic Outliers in Large Language Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yongqi An$^{1,2}$, Xu Zhao$^{1,4}$, Tao Yu$^{1,2}$, Ming Tang$^{1,2}$, Jinqiao Wang$^{1,2,3,4,}$\thanks{Corresponding Author} \\
$^{1}$Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences, Beijing, China \\
$^{2}$School of artificial intelligence, University of Chinese Academy of Sciences, Beijing, China \\
$^{3}$Wuhan AI Research, Wuhan, China, $^{4}$Objecteye Inc., Beijing, China \\
\texttt{\{yongqi.an, xu.zhao, tangm, jqwang\}@nlpr.ia.ac.cn} \\
\texttt{yutao2022@ia.ac.cn} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
\vspace{-1.5mm}
Outliers have been widely observed in Large Language Models (LLMs), significantly impacting model performance and posing challenges for model compression. Understanding the functionality and formation mechanisms of these outliers is critically important. Existing works, however, largely focus on reducing the impact of outliers from an algorithmic perspective, lacking an in-depth investigation into their causes and roles. In this work, we provide a detailed analysis of the formation process, underlying causes, and functions of outliers in LLMs. We define and categorize three types of outliers—activation outliers, weight outliers, and attention outliers—and analyze their distributions across different dimensions, uncovering inherent connections between their occurrences and their ultimate influence on the attention mechanism. Based on these observations, we hypothesize and explore the mechanisms by which these outliers arise and function, demonstrating through theoretical derivations and experiments that they emerge due to the self-attention mechanism's softmax operation. These outliers act as implicit context-aware scaling factors within the attention mechanism. As these outliers stem from systematic influences, we term them systematic outliers. Our study not only enhances the understanding of Transformer-based LLMs but also shows that structurally eliminating outliers can accelerate convergence and improve model compression. The code is avilable at \url{https://github.com/an-yongqi/systematic-outliers}.
\end{abstract}

\vspace{-2.5mm}
\section{Introduction}\label{sec:intr}
\vspace{-1.5mm}

Large Language Models (LLMs) have recently demonstrated remarkable capabilities~\citep{brown2020language,achiam2023gpt,touvron2023llama}, making them a central topic of research across various domains. Numerous studies have uncovered intriguing phenomena within these models~\citep{dettmers2022gpt3,xiao2023efficient,sun2024massive}, which are crucial for advancing the understanding and application of LLMs. Among these phenomena, the presence of outliers, which are values that deviate significantly from the average of their distribution, has garnered considerable attention~\citep{zhang2024unveiling,kovaleva2021bert,paglieri2024outliers}. 

However, research on outliers in LLMs predominantly emphasizes mitigating their impact through algorithmic techniques, often neglecting a thorough exploration of their underlying causes and functional roles. This narrow focus results in two key shortcomings: first, it limits our understanding of why outliers occur and how they influence model behavior~\citep{yin2023outlier,xiao2023smoothquant,hooper2024kvquant}; and second, it overlooks the interrelationships between different types of outliers, treating them in isolation rather than as part of a comprehensive, systematic framework~\citep{zhang2024unveiling,sun2024massive,liao2024free}. These gaps hinder a deeper understanding of the mechanisms underlying LLMs and constrain opportunities for more effective optimization and broader applications.

To address these gaps, we systematically analyze outliers in LLMs, focusing on their formation, distribution, and roles within the models. We begin by defining and categorizing three types of outliers: \emph{activation outliers, weight outliers, and attention outliers}. By examining their distributions across various dimensions, we uncover inherent connections between their occurrences and demonstrate how these outliers collectively influence the attention mechanism. Building on these findings, we propose a hypothesis regarding the formation mechanisms and functions of these outliers, supported by theoretical derivations and experiments. Specifically, we show that these outliers emerge as a result of the self-attention mechanism's softmax operation and act as implicit, context-aware scaling factors in the attention mechanism. Furthermore, our experiments show that structurally eliminating these outliers can accelerate convergence and enhance model compression, providing new insights for future model optimization and design.

Our main contributions are:
\begin{itemize}[leftmargin=20pt]
    \item We define and categorize outliers in LLMs into three types—\emph{activation, weight, and attention outliers}—and uncover their systematic relationships.
    \item We reveal that these outliers emerge from the self-attention mechanism’s softmax operation and function as implicit, context-aware scaling factors.
    \item We demonstrate that eliminating outliers accelerates convergence and enhances model compression, offering insights for optimizing LLMs.
\end{itemize}

\vspace{-0.5mm}
\section{Related Work}\label{sec:related}
\vspace{-0.5mm}
\paragraph{Outliers in Large Language Models.}\label{subsec:related-outlier}
Outliers in LLMs refer to values that deviate significantly from the average of their distribution~\citep{dettmers2022gpt3}. Studies have documented various types of outliers in weights, activations, and attention scores, highlighting their presence and impact. For instance, \citet{dettmers2022gpt3} identified activation outliers and proposed quantization techniques to mitigate their effects. \citet{sun2024massive} explored the role of large activations as biases in the attention mechanism. Similarly, \citet{zhang2024unveiling} analyzed weight outliers in LayerNorm layers, demonstrating their importance for maintaining language modeling capabilities in models like GPT-2 and LLaMA2-13B. Additionally, \citet{xiao2023efficient} introduced the concept of the “Attention Sink,” which occurs when a few keys dominate attention scores.

While previous studies recognize the presence of outliers, they typically focus on specific cases or task-specific solutions like quantization and pruning. In contrast, our work provides a systematic categorization of outliers—\emph{activation, weight, and attention outliers}—and reveals their interconnections and collective influence on the attention mechanism in LLMs.

\vspace{-0.5mm}
\paragraph{The Impact of Outliers on Model Performance and Compression.}\label{subsec:related-influence}
Outliers significantly affect both the performance and efficiency of LLMs. Previous research has shown that removing outliers without proper handling can severely degrade performance~\citep{puccetti2021bert, kovaleva2021bert, zhang2024unveiling}. In quantization, outliers amplify rounding and clipping errors, leading to substantial quantization losses~\citep{wei2022outlier, nrusimha2024mitigating, lin2024rotation}. Similarly, magnitude-based pruning strategies face challenges in maintaining model performance when outliers are present~\citep{sun2023simple}. \citet{yin2023outlier} observed that outliers correlate strongly with layer sparsity, further complicating pruning approaches. Moreover, in KV cache compression, \citet{xiao2023efficient} found that attention score outliers associated with specific tokens play a critical role in preserving context.

Despite extensive research on the adverse effects of outliers, their formation mechanisms and functional roles remain largely unexplored. Existing studies focus on mitigating their impact but lack a systematic investigation of their origins. In contrast, our work explores the formation of outliers within the self-attention mechanism, revealing their role as implicit, context-aware scaling factors and proposing structural solutions to enhance model convergence and compression efficiency.

\vspace{-0.5mm}
\section{Systematic Outliers: Definition, Existence, and Localization}\label{sec:loc}
\vspace{-0.5mm}

% Insert the figure before the abstract
\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/existence/ao_down_llama2_7b.pdf}
        \caption{activation: $\mathbf{x}_{\ell}^{\text{down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/existence/wo_llama2_7b.pdf}
        \caption{weight: $\mathbf{W}_{\ell}^{\text {down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/existence/ao_out_llama2_7b.pdf}
        \caption{activation: $\mathbf{h}_{\ell}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/existence/attno_llama2_7b.pdf}
        \caption{attention: $\mathbf{A}^i_{\ell}$}
    \end{subfigure}
    \vspace{-1mm}
    \caption{Systematic outliers in LLaMA2-7B. Outliers are identified in four locations: activations (layer outputs $\mathbf{h}_{\ell}$ and down-projection inputs $\mathbf{x}_{\ell}^{\text{down}}$), weights (down-projection matrices $\mathbf{W}_{\ell}^{\text {down}}$), and attention (attention weights $\mathbf{A}^i_{\ell}$).}
    \vspace{-1mm}
    \label{fig:so_llama2_7b}
\end{figure}

\paragraph{Definition of Outliers in LLMs.}\label{subsec:loc-def}
Outliers in LLMs are values that deviate significantly from the average of their distribution, often surpassing a threshold $\tau$. From Figure~\ref{fig:so_llama2_7b}, we observe that three distinct types of outliers—\emph{activation outliers, weight outliers, and attention outliers}—appear systematically in four key locations within LLaMA2-7B. The specific positions of these outliers are further summarized in Figure~\ref{fig:so_pos}. To formalize this, we define outliers mathematically for each type as follows:

\begin{itemize}[leftmargin=20pt]
    \item \textbf{Activation Outliers:}  
    For layer outputs $\mathbf{h}_{\ell} \in \mathbb{R}^{B \times H}$ (batch size $B$ and hidden dimension $H$), the set of activation outliers $\mathcal{O}_{\text{activation}}$ is:  
    \[
    \mathcal{O}_{\text{activation}} = \{(i, j) \mid |h_{i,j}| > \tau \cdot \mu_{h}\},
    \]  
    where $\mu_{h} = \frac{1}{B \cdot H} \sum_{i,j} |h_{i,j}|$ is the mean absolute value of $\mathbf{h}_{\ell}$.  
    Additionally, for down-projection inputs $\mathbf{x}_{\ell}^{\text{down}} \in \mathbb{R}^{B \times H}$, activation outliers are defined as:  
    \[
    \mathcal{O}_{\text{activation-down}} = \{(i, j) \mid |x_{i,j}^{\text{down}}| > \tau \cdot \mu_{x^{\text{down}}}\},
    \]  
    where $\mu_{x^{\text{down}}} = \frac{1}{B \cdot H} \sum_{i,j} |x_{i,j}^{\text{down}}|$.  

    \item \textbf{Weight Outliers:}  
    For projection weights $\mathbf{W} \in \mathbb{R}^{O \times I}$ (output dimension $O$, input dimension $I$), weight outliers $\mathcal{O}_{\text{weight}}$ are defined as:  
    \[
    \mathcal{O}_{\text{weight}} = \{(i, j) \mid |w_{i,j}| > \tau \cdot \mu_{w_i}\},
    \]  
    where $\mu_{w_i} = \frac{1}{I} \sum_{j} |w_{i,j}|$ is the row-wise mean absolute value of $\mathbf{W}$.  

    \item \textbf{Attention Outliers:}  
    For cumulative attention scores $\mathbf{A} \in \mathbb{R}^{L \times L}$ (sequence length $L$), attention outliers $\mathcal{O}_{\text{attention}}$ are:  
    \[
    \mathcal{O}_{\text{attention}} = \{j \mid \hat{A}_{j} > \tau \cdot \mu_{A}\},
    \]  
    where $\hat{A}_{j} = \sum_{i=1}^{L} A_{i,j}$ is the cumulative attention contribution for token $j$, and $\mu_{A} = \frac{1}{L} \sum_{j} \hat{A}_{j}$.  
\end{itemize}

\begin{wrapfigure}{r}{0.28\textwidth} % Adjust the optional argument [9] for number of lines to wrap
    \vspace{-5mm} % Reduce the vertical space above the figure
    \centering
    \includegraphics[width=\linewidth]{figures/so_pos.pdf}
    \caption{Illustration of systematic outliers locations in LLMs.}
    \vspace{-3mm} % Reduce the vertical space above the figure
    \label{fig:so_pos}
\end{wrapfigure}

These definitions provide a unified framework to identify and analyze outliers across LLM components. We empirically set $\tau = 1000$ in our experiments to isolate extreme deviations.

\paragraph{Existence of Outliers in LLMs.}\label{subsec:loc-exist}
Systematic outliers consistently emerge in LLMs across various components and architectures. Figure~\ref{fig:so_llama2_7b} highlights their presence in LLaMA2-7B~\citep{touvron2023llama2}, where we observe abnormally large values in activations, weights, and attention scores at specific indices. These patterns indicate the regular appearance of systematic outliers at critical positions within the model.

This phenomenon extends beyond LLaMA2-7B to a wide range of LLMs, spanning diverse model sizes and families. Our analysis confirms that systematic outliers are a common feature across pretrained and fine-tuned LLMs. For additional examples from other architectures, refer to Appendix~\ref{app:existence}.To deepen our understanding, we next analyze their distributions across various dimensions, providing insights into their underlying causes and functional roles, which are crucial for unraveling their systemic impact.

\vspace{-1.5mm}
\subsection{Where are Activation Outliers located?}\label{subsec:loc-ao_loc}
\vspace{-1.5mm}

Activation outliers manifest as abnormally large values in specific sequence and feature dimensions, as shown in Figures~\ref{fig:so_llama2_7b}(a) and~\ref{fig:so_llama2_7b}(c). These outliers appear in two distinct activation types: \textbf{layer outputs $\mathbf{h}_{\ell}$} and \textbf{down-projection inputs $\mathbf{x}_{\ell}^{\text{down}}$}. We analyze their distributions across layers, sequences, and feature dimensions to understand their patterns. Detailed experimental settings are provided in Appendix~\ref{app:setting-position}.

For layer outputs, Figure~\ref{fig:pos_ao_out}(a) reveals that activation outliers are concentrated in shallow layers, persist through middle layers, and diminish in the final layers. Additionally, Figure~\ref{fig:pos_ao_out}(b) shows that these outliers are associated with start tokens and weak semantic tokens, such as "." and "$\_$", while Figure~\ref{fig:pos_ao_out}(c) highlights their confinement to specific feature dimensions.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.35\textwidth}  
        \includegraphics[width=\textwidth]{figures/pos_ao_out/llama2_7b_layers.pdf}
        \caption{Which layers?}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.26\textwidth}
        \includegraphics[width=\textwidth]{figures/pos_ao_out/llama2_7b_seqs.pdf}
        \caption{Which sequences?}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.37\textwidth}
        \includegraphics[width=\textwidth]{figures/pos_ao_out/llama2_7b_features.pdf}
        \caption{Which features?}
    \end{subfigure}
    \hfill
    \vspace{-1mm}
    \caption{Distribution of activation outliers in $\mathbf{h}_{\ell}$ across layers, sequences, and feature dimensions.}
    \label{fig:pos_ao_out}
\end{figure}

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.40\textwidth}  
        \includegraphics[width=\textwidth]{figures/pos_ao_down/llama2_7b_layers.pdf}
        \caption{Which layers?}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.29\textwidth}
        \includegraphics[width=\textwidth]{figures/pos_ao_down/llama2_7b_layer2_seqs.pdf}
        \caption{Layer 2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.29\textwidth}
        \includegraphics[width=\textwidth]{figures/pos_ao_down/llama2_7b_layer31_seqs.pdf}
        \caption{Layer 31}
    \end{subfigure}
    \hfill
    \vspace{-1mm}
    \caption{Distribution of activation outliers in $\mathbf{x}_{\ell}^{\text{down}}$ across layers, sequences, and feature dimensions.}
    \vspace{-3mm}
    \label{fig:pos_ao_down}
\end{figure}

For down-projection inputs, Figure~\ref{fig:pos_ao_down} indicates that activation outliers are confined to a few shallow and deep layers. Similar to layer outputs, these outliers are linked to start tokens and weak semantic tokens and are concentrated in a limited set of feature dimensions.

In summary, activation outliers in $\mathbf{x}_{\ell}^{\text{down}}$ are restricted to shallow and deep layers, while those in $\mathbf{h}_{\ell}$ persist from shallow to middle layers. Both types predominantly affect fixed feature dimensions and tokens with weak semantic content.

\vspace{-1.5mm}
\subsection{Where are Weight Outliers located?}\label{subsec:pos_relation-wo_loc}
\vspace{-1.5mm}

As shown in Figure~\ref{fig:so_llama2_7b}(b), weight outliers are characterized by extreme values concentrated in specific columns. To quantify this, we compute the extremal ratio, defined as the ratio of the maximum value to the mean value within each column, since large column values directly influence the corresponding output activations.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.44\textwidth}  
        \includegraphics[width=\textwidth]{figures/pos_wo/llama2_7b_modules_layers.pdf}
        \caption{Which layers?}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/pos_wo/wo_llama2_7b_layer_31.pdf}
        \caption{Layer 31}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/pos_wo/wo_llama2_7b_layer_32.pdf}
        \caption{Layer 32}
    \end{subfigure}
    \hfill
    \vspace{-1mm}
    \caption{Distribution of weight outliers in $\mathbf{W}_{\ell}^{\text {down}}$ across layers, modules, and feature dimensions.}
    \label{fig:pos_wo}
\end{figure}

Figure~\ref{fig:pos_wo}(a) illustrates the extremal ratio across layers and modules in LLaMA2-7B, highlighting that weight outliers are concentrated in the down-projection matrices $\mathbf{W}_{\ell}^{\text{down}}$ of the second layer and the last two layers. Figures~\ref{fig:pos_wo}(b) and~\ref{fig:pos_wo}(c) provide detailed visualizations of these outliers in the last two layers.

In summary, weight outliers in LLaMA2-7B are primarily located in the MLP’s down-projection matrices, concentrated in specific shallow and deep layers.

\vspace{-1.5mm}
\subsection{Where are Attention Outliers located?}\label{subsec:pos_relation-attn_loc}
\vspace{-1.5mm}

To understand the distribution of attention outliers, we analyze cumulative attention scores on the attention weights $\mathbf{A}^i_{\ell}$ across layers, keys, and heads.

Figure~\ref{fig:pos_attno} highlights key patterns in the distribution of attention outliers. In Figure~\ref{fig:pos_attno}(a), the two largest cumulative attention scores per layer show that attention outliers persist across all layers. Figure~\ref{fig:pos_attno}(b) categorizes keys with outliers, indicating a strong association with start tokens and weak semantic tokens. Figure~\ref{fig:pos_attno}(c) visualizes score fluctuations across heads, revealing significant variation between heads and layers.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.34\textwidth}  
        \includegraphics[width=\textwidth]{figures/pos_attno/llama2_7b_layers.pdf}
        \caption{Which layers?}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{figures/pos_attno/llama2_7b_keys.pdf}
        \caption{Which keys?}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.39\textwidth}
        \includegraphics[width=\textwidth]{figures/pos_attno/llama2_7b_heads.pdf}
        \caption{Which heads?}
    \end{subfigure}
    \hfill
    \vspace{-1mm}
    \caption{Distribution of attention outliers in $\mathbf{A}^i_{\ell}$ across layers, keys and heads.}
    \vspace{-3mm}
    \label{fig:pos_attno}
\end{figure}

\vspace{-1.5mm}
\section{Systematic Outliers Are Simultaneous and Interconnected}\label{sec:relate}
\vspace{-1.5mm}

Systematic outliers are not isolated phenomena; instead, they exhibit strong correlations across feature and sequence dimensions as well as layers. Understanding these interconnections and their lifecycle is crucial for uncovering how outliers propagate through the model and affect computations. This section analyzes these relationships in detail, laying the groundwork for the next section, where we hypothesize and validate their functional roles.

\vspace{-1.5mm}
\subsection{How Are These Outliers Related?}\label{subsec:relate-consist}
\vspace{-1.5mm}

\begin{table}[ht]
    \vspace{-2mm}
    \caption{Consistency of different types of outliers across dimensions.}
    \vspace{-1.5mm}
    \centering
    \begin{tabular}{ccc|c}
        \toprule
        \textbf{Outlier Type 1} & \textbf{Outlier Type 2} & \textbf{Dimension} & \textbf{Consistency} \\
        \midrule
        weight outliers in $\mathbf{W}_{\ell}^{\text{down}}$ & activation outliers in $\mathbf{x}_{\ell}^{\text{down}}$ & feature & 100\% \\
        weight outliers in $\mathbf{W}_{\ell}^{\text{down}}$ & activation outliers in $\mathbf{h}_{\ell}$ & feature & 100\% \\
        activation outliers in $\mathbf{x}_{\ell}^{\text{down}}$ & activation outliers in $\mathbf{h}_{\ell}$ & sequence & 100\% \\
        activation outliers in $\mathbf{h}_{\ell}$ & attention outliers in $\mathbf{A}^i_{\ell}$ & sequence & 95\% \\ 
        \bottomrule
    \end{tabular}
    \vspace{-3mm}
    \label{tab:outliers_overlap}
\end{table}

Table~\ref{tab:outliers_overlap} provides a quantitative analysis of the alignment among the three types of outliers. Detailed experimental settings can be found in Appendix~\ref{app:setting-consistency}. We find that the three types of outliers are not isolated but occur simultaneously and are interconnected across multiple dimensions within the model. Specifically, weight outliers align perfectly with activation outliers in feature dimensions, while over 95\% of activation outliers overlap with attention outliers in sequence dimensions, primarily concentrated in start and weak semantic tokens.

\vspace{-1.5mm}
\subsection{The Lifecycle of Systematic Outliers}\label{subsec:relate-lifecycle}
\vspace{-1.5mm}

The correlations between different types of outliers suggest a deeper connection underlying their occurrences. By visualizing their lifecycle, we observe a chain of interactions: weight outliers lead to activation outliers, which then influence attention outliers, with this influence extending to non-outlier tokens.

\begin{figure*}[t!]
    \vspace{-1.5mm}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/lifecycle/emergence.pdf}
    \caption{The emergence of activation outliers from weight outliers.} 
    \vspace{-1.5mm}
    \label{fig:emergence}
\end{figure*}

\vspace{-1.5mm}
\paragraph{The Emergence of Activation Outliers from Weight Outliers.}
In the second layer of LLaMA2-7B's MLP, weight outliers in the up- and gate-projection matrices cause extreme neuron responses. These are amplified by the SiLU activation function~\citep{elfwing2018sigmoid} and GLU operation~\citep{shazeer2020glu}, resulting in activation outliers that are up to a thousand times the average magnitude. Additionally, weight outliers in the down-projection matrix amplify activations in specific feature dimensions (e.g., 1415th and 2533rd), dominating the residual connection and influencing the final output (see Figure~\ref{fig:emergence}).

\begin{figure*}[b!]
    \vspace{-1.5mm}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/lifecycle/spread.pdf}
    \caption{The spread of attention outliers from activation outliers. Activation outliers influence the self-attention mechanism, extending their impact to other sequence dimensions.}
    \vspace{-2mm}
    \label{fig:spread}
\end{figure*}

\begin{figure*}[t!]
    \vspace{-1.5mm}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/lifecycle/disappearance.pdf}
    \caption{The disappearance of outliers in the final layers.} 
    \vspace{-2mm}
    \label{fig:disappearance}
\end{figure*}

\vspace{-1.5mm}
\paragraph{The Spread of Attention Outliers from Activation Outliers.}
Activation outliers propagate into the attention mechanism through their influence on query, key, and value vectors. In the third layer’s Multi-Head Attention (MHA), we observe that tokens with activation outliers exhibit alignment in the 58th and 122nd dimensions of both \textbf{query} and \textbf{key vectors}. This alignment significantly increases the dot product between these vectors, leading to disproportionately high attention weights assigned to the outlier tokens (see Figure~\ref{fig:spread}). As a result, the attention mechanism focuses heavily on these tokens, amplifying their influence across layers.

Interestingly, despite receiving significant attention, the \textbf{value vectors} corresponding to these tokens show comparatively smaller magnitudes. This indicates that while outlier tokens attract attention, they may contribute less directly to the final output. This phenomenon could imply a mechanism where the model leverages these tokens as "anchors" to affect attention outputs, we explore it further in Section~\ref{sec:role}.

Additionally, we find that this pattern—alignment in query and key dimensions and reduced magnitude in value dimensions—is consistent across most heads and layers. This consistency highlights the systematic nature of how activation outliers propagate their influence through the attention mechanism.

\vspace{-1.5mm}
\paragraph{The Disappearance of Outliers in the Final Layers.}
Outliers gradually vanish in the final layers due to cancellation by values of opposite signs. This neutralization occurs progressively rather than abruptly (see Figure~\ref{fig:disappearance}). As activation outliers diminish, attention outliers are similarly reduced, with some heads in the final layer showing no outliers at all.

\vspace{-1.5mm}
\paragraph{Summary.} In the lifecycle of systematic outliers, weight outliers drive the emergence of activation outliers, which propagate anomalies into the attention mechanism. This interdependence extends their influence to non-outlier tokens. These findings reveal that systematic outliers are intrinsically linked to the attention mechanism, setting the stage for the next section, where we hypothesize and validate their functional roles.

\vspace{-1.5mm}
\section{Systematic Outliers as Context-Aware Scaling Factors in Attention Mechanisms}\label{sec:role}
\vspace{-1.5mm}

\subsection{Hypotheses on the Role of Systematic Outliers}\label{subsec:role-hypo}
\vspace{-1.5mm}

Based on the observations in Section~\ref{sec:relate}, we propose three hypotheses on the potential role of systematic outliers in LLMs, drawing from prior research and our findings:

\begin{enumerate}
    \item \textbf{Fixed but Important Biases:} Inspired by the concept of Massive Activations~\citep{sun2024massive}, systematic outliers may act as fixed biases that consistently influence model behavior. These outliers could serve as stable values that help the model emphasize certain tokens or features, regardless of the context.

    \item \textbf{Context-Aware Biases:} As seen in Figure~\ref{fig:pos_attno}(c), the attention outliers vary significantly across heads and tokens (20\% to 95\%). This suggests that these outliers may dynamically adjust their influence based on the input sequence, acting as context-aware signals that adapt to specific content and guide attention allocation.

    \item \textbf{Context-Aware Scaling Factors:} Figure~\ref{fig:spread} shows that the value vectors corresponding to outlier tokens have significantly smaller magnitudes, suggesting that these outliers may act as implicit scaling factors. By reducing the impact of contextual information on certain tokens, these scaling factors help minimize unnecessary updates.
\end{enumerate}

\vspace{-1.5mm}
\subsection{Empirical Validation of Systematic Outliers Hypotheses}\label{subsec:role-exp}

\paragraph{Formulation.}
\vspace{-1.5mm}

In this part, we introduce five different attention formulations to explore the role of systematic outliers. Each formulation represents a specific variant of the attention mechanism, designed to isolate different aspects of bias and scaling effects within the model. The formulations are listed in Table~\ref{tab:attn}, followed by an explanation of their roles.

\begin{table}[h!]
\centering
\vspace{-2mm}
\caption{Variants of the attention mechanism for systematic outliers analysis.}
\vspace{-1.5mm}
\resizebox{\textwidth}{!}{ % 调整表格到页面宽度
\begin{tabular}{c|c|c}
\toprule
\textbf{ID} & \textbf{Attention Variant} & \textbf{Formulation} \\
\midrule
(a) & Default Attention~\citep{vaswani2017attention} & 
\(\small \operatorname{Attn}(Q, K, V) = \operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d}}\right)V\) \\
(b) & Explicit Fixed Bias & 
\(\small \operatorname{Attn}(Q, K, V; \mathbf{v}^{\prime}) = \operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d}}\right)V + \mathbf{v}^{\prime}\) \\
(c) & Explicit Context-Aware Bias & 
\(\small \operatorname{Attn}(Q, K, V; \mathbf{k}^{\prime}, \mathbf{v}^{\prime}) = \operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d}}\right)V + \operatorname{softmax}\left(\frac{Q [K^T \ \mathbf{k}^{\prime}]}{\sqrt{d}}\right)\begin{bmatrix}0^T \\ {\mathbf{v}^{\prime}}^T \end{bmatrix}\) \\
(d) & Attention Bias & 
\(\small \operatorname{Attn}(Q, K, V; \mathbf{k}^{\prime}, \mathbf{v}^{\prime}) = \operatorname{softmax}\left(\frac{Q [K^T \ \mathbf{k}^{\prime}]}{\sqrt{d}}\right) \begin{bmatrix}V \\ {\mathbf{v}^{\prime}}^T \end{bmatrix}\) \\
(e) & Explicit Context-Aware Scaling Factor & 
\(\small \operatorname{Attn}(Q, K, V) = S_c(x) \cdot \operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d}}\right)V\) \\
\bottomrule
\end{tabular}
}
\vspace{-1mm}
\label{tab:attn}
\end{table}

%\noindent

In Table~\ref{tab:attn}, \( Q, K, V \in \mathbb{R}^{T \times d} \) are the query, key, and value matrices, and \( d \) is the dimensionality of the hidden space. For variants with bias or scaling modifications, \( \mathbf{k}^{\prime}, \mathbf{v}^{\prime} \in \mathbb{R}^{d} \) represent the additional learnable bias terms, and \( S_c(x) \) is a learnable scaling factor dependent on the input context.

\begin{itemize}
    \item \textbf{Default Attention (a):} The standard attention mechanism serves as the baseline, without bias or scaling modifications.
    \item \textbf{Explicit Fixed Bias (b):} Adds a fixed, learnable bias \( \mathbf{v}^{\prime} \) only to the value matrix to isolate the impact of fixed biases on systematic outliers.
    \item \textbf{Explicit Context-Aware Bias (c):} Introduces context-aware bias terms \( \mathbf{k}^{\prime} \) and \( \mathbf{v}^{\prime} \), which vary based on the input sequence to isolate the impact of context-aware bias on systematic outliers.
    \item \textbf{Attention Bias (d):} Incorporates learnable bias terms \( \mathbf{k}^{\prime} \) and \( \mathbf{v}^{\prime} \) into the key and value matrices. It provides both context-aware bias and a scaling factor.
    \item \textbf{Explicit Context-Aware Scaling Factor (e):} Utilizes a learnable scaling factor \( S_c(x) \) that dynamically adjusts the attention weights, helping investigate the scaling effect on reducing systematic outliers.
\end{itemize}

\vspace{-1.5mm}
\paragraph{Results.}

We train five GPT-2~\citep{radford2019language} models with these attention variants. Detailed experimental settings can be found in Appendix~\ref{app:setting-attention-variants}. We visualize the presence of activation outliers across different attention formulations in Figure~\ref{fig:exp_3d} and plotted the top-3 largest activation magnitudes for each layer in Figure~\ref{fig:exp_layerwise}. The results reveal distinct patterns, where only attention bias (d) and explicit context-aware scaling factor (e) effectively prevent the formation of systematic outliers.

These results strongly suggest that scaling factor plays a crucial role in managing outlier behavior in LLMs. Fixed or context-aware bias alone is insufficient to mitigate outliers, whereas explicit context-aware scaling factor provides the necessary dynamic adjustment to prevent their occurrence.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/exp/default_3d.pdf}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/exp/fixed_bias_3d.pdf}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/exp/cabias_3d.pdf}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/exp/attn_bias_3d.pdf}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/exp/ca_scaling_3d.pdf}
        \caption{}
    \end{subfigure}
    \caption{Activation outliers across different attention formulations.}
    \label{fig:exp_3d}
\end{figure}

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/exp/default_layerwise.pdf}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/exp/fixed_bias_layerwise.pdf}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/exp/cabias_layerwise.pdf}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/exp/attn_bias_layerwise.pdf}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figures/exp/ca_scaling_layerwise.pdf}
        \caption{}
    \end{subfigure}
    \vspace{-2mm}
    \caption{Top-3 largest activation outliers for each layer.}
    \vspace{-3mm}
    \label{fig:exp_layerwise}
\end{figure}

The main conclusions about the role of systematic outliers from the experiments are as follows:

\begin{itemize} 
    \item \textbf{It is not the fixed bias:} Experiment (b) clearly shows that fixed bias alone does not prevent outliers, proving that fixed bias mechanisms are ineffective in this regard. 
    \item \textbf{It is not the context-aware bias:} Experiment (c) shows that context-aware bias alone is insufficient. A comparison with Experiment (d) highlights that context-aware biases do not eliminate outliers. 
    \item \textbf{It is the context-aware scaling factor:} Both Experiment (d) and Experiment (e) explicitly use a context-aware scaling factor. While Experiment (d) includes additional context-aware bias, the comparison between (c) and (d) confirms that the primary factor in preventing systematic outliers is the context-aware scaling factor. 
    % Detailed analysis of how outliers function as context-aware scaling factors can be found in Appendix~\ref{app:role}.
\end{itemize}

\subsection{Further Analysis of Systematic Outliers}\label{subsec:role-analysis}

\vspace{-1.5mm}
\paragraph{Softmax Attention is the root cause of systematic outliers.}

In Transformers, multi-head attention (MHA) aims to augment token embeddings with contextual information to improve prediction accuracy. The difficulty of this task varies across tokens: some require complex contextual updates (e.g., for ambiguous or semantically rich tokens), while others (e.g., delimiter or padding tokens) require minimal updates. However, the softmax operation enforces that attention scores always sum to one, even for simpler tasks where little contextual information is needed. To satisfy this constraint, the model must produce a large dynamic range in the input to softmax, amplifying the disparity between tokens. This dynamic range is further exaggerated as the model learns to allocate most attention to low-information tokens in some cases, ensuring minimal updates for those tokens.

This demand for a large dynamic range propagates through the network and affects earlier layers. Layer Normalization, applied before softmax, standardizes input distributions, inadvertently compressing the required dynamic range. To compensate, multi-layer perceptron (MLPs) in preceding layers generate activations of significantly higher magnitude, resulting in the emergence of activation outliers. These high-magnitude activations propagate through the residual connections, amplifying gradients during backpropagation and encouraging the formation of weight outliers in the projection matrices.

Furthermore, because softmax outputs are strictly positive and sum to one, all tokens maintain non-zero probabilities, leading to persistent gradients for all queries and keys. This forces the model to continuously adjust activations and weights to accommodate large dynamic ranges, further amplifying systematic outliers over successive layers. As a result, softmax normalization, coupled with architectural constraints like Layer Normalization and residual connections, becomes the fundamental driver of systematic outliers. Detailed derivations and analysis are provided in Appendix~\ref{app:cause}.

\vspace{-1.5mm}
\paragraph{Potential Applications for Model Compression.}

Systematic outliers complicate compression techniques such as quantization and pruning by increasing memory usage and degrading performance. Our experiments demonstrate that context-aware scaling factors effectively mitigate these issues. Table~\ref{tab:model_compression} compares GPT-2 Default and GPT-2 with Context-aware Scaling Factor under common compression methods.

\begin{table}[h!] 
    \centering 
    \vspace{-1.5mm}
    \caption{Comparison of GPT-2 Default and GPT-2 with Context-aware Scaling Factor under pruning and quantization methods.} 
    \vspace{-1.5mm}
    \label{tab:model_compression} 
    \begin{tabular}{l|c|c|c} 
        \toprule 
        \textbf{Model} & \textbf{PPL (FP16)} & \textbf{PPL (AbsMax W8)} & \textbf{PPL (50\% Sparse)} \\
        \midrule 
        GPT-2 Default & 27.24 & 93.44 & 7235.68 \\ 
        GPT-2 + Context-aware Scaling & 26.95 & 29.22 & 39.47 \\ 
        \bottomrule 
    \end{tabular} 
\end{table}

The results, tested on WikiText2, show that context-aware scaling significantly improves robustness to compression. For quantization, it reduces PPL from 93.44 to 29.22, and for pruning, it stabilizes PPL at 39.47 compared to 7235.68 for GPT-2 Default. Importantly, the addition of context-aware scaling factors incurs minimal memory overhead. For example, in GPT-2, the parameter count increases from 123.59M to 123.70M—less than a 0.1\% increase. This negligible overhead ensures that the method remains practical for large-scale deployment. These findings validate that context-aware scaling enhances model robustness to compression, enabling efficient model deployment without compromising performance.

\vspace{-3mm}
\paragraph{Influence on Convergence and Training Stability.}

Incorporating context-aware scaling factors significantly accelerates convergence and enhances training stability in Transformer models. By dynamically adjusting attention scores, these factors reduce reliance on extreme outliers, leading to a smoother optimization landscape. As shown in Figure~\ref{fig:train_loss}, models with context-aware scaling converge faster during early training steps, but their final validation losses are comparable to those of default attention mechanisms, indicating improved convergence speed without necessarily better final performance.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/train_loss_comparison.pdf}
    \vspace{-1mm}
    \caption{Training loss comparison showing improved convergence by eliminating outliers.} 
    \vspace{-2mm}
    \label{fig:train_loss}
\end{figure*}
% \paragraph{Variability in Head-Level Attention Patterns.}

% \paragraph{Attention Output Decomposition.}

% \paragraph{Absence of Outliers in Sigmoid Attention.}


% To summarize our findings in this section:

% \textit{}
% \vspace{-1.5mm}
% \section{Discussion}\label{sec:disc}
% \vspace{-1.5mm}

% \paragraph{Qualitative Explanation of Context-Aware Scaling Factor.}

% In Transformers, each layer's multi-head attention (MHA) mechanism is designed to augment the embedding of the "current token" with contextual information from other tokens to help predict the "next token". The difficulty of each prediction task varies, but due to softmax normalization and the constraint of attention scores summing to 1, the model tends to ensure that even the most challenging tasks are predicted correctly. This, however, leads to attention redundancy in simpler tasks where not as much contextual information is needed. The context-aware scaling factor helps adjust the amount of attention dynamically, reducing redundancy for easier predictions while still allowing sufficient attention for more complex tasks.

% \vspace{-1.5mm}
% \paragraph{Potential Applications for Model Compression.}

% Systematic outliers complicate model compression techniques such as quantization, pruning, and KV cache compression. Design measures taken to protect these outliers often result in increased memory usage and hardware inefficiencies. Our research clarifies the origins of these outliers and shows that an explicit context-aware scaling factor can effectively mitigate their impact. This approach paves the way for model compression methods that are not only more resource-efficient but also capable of maintaining the high accuracy levels required by modern applications.

% \vspace{-1.5mm}
% \paragraph{Softmax Attention is the root cause of systematic outliers.}

% In some cases, attention heads may allocate most of their probability mass to tokens with low information content (e.g., delimiter tokens) to avoid updating certain token representations. To achieve this, the input to the softmax function needs to have a large dynamic range. As the input to softmax approaches zero, an infinitely large dynamic range is required.

% Layer Normalization~\citep{lei2016layer}, applied before the softmax in many Transformer architectures, standardizes the inputs, which can inadvertently compress the dynamic range required for the desired attention distribution. To compensate, the outputs of the preceding feed-forward networks (FFNs) must produce activations of significantly higher magnitude, leading to the emergence of activation outliers. This need for high-magnitude activations propagates backward, encouraging the growth of weight outliers during training.

% \begin{wrapfigure}[13]{r}{0.3\textwidth} % Adjust the width to fit the content better
%     \vspace{-3mm}
%     \centering
%     \includegraphics[width=0.3\textwidth]{figures/exp/sigmoid_3d.pdf}
%     \caption{Absence of outliers in sigmoid attention.}
%     \label{fig:exp_sigmoid}
% \end{wrapfigure}

% Moreover, since the softmax outputs are always positive and sum to one, no token is assigned zero probability, ensuring a non-zero gradient during backpropagation. This persistent gradient drives the model to continually adjust its weights and activations, often amplifying outliers, which can lead to numerical instability and hinder generalization. \citet{ramapuram2024theory} proposed sigmoid self-attention, as shown in Equation~\ref{eq:sigmoid_attn}.

% This function outputs values between 0 and 1 independently for each token, it is possible for some outputs to approach zero, which can result in the gradient effectively vanishing for certain inputs. We train a GPT-2 model with sigmoid Attention, and as shown in Figure~\ref{fig:exp_sigmoid}, sigmoid Attention eliminated systemic outliers, further supporting our analysis.
% \begin{equation}
%     \begin{array}{r}
%     \text { SigmoidAttn }(\boldsymbol{X})=\sigma\left(\boldsymbol{Q} \boldsymbol{K}^T / \sqrt{d_{q k}}\right) \boldsymbol{V}, \\
%     \text { with } \sigma: u \mapsto \operatorname{sigmoid}(u+b):=\left(1+e^{-(u+b)}\right)^{-1}
%     \end{array}
%     \label{eq:sigmoid_attn}
% \end{equation}

% \vspace{-3mm}
% \paragraph{Influence on Convergence and Training Stability.}

% Incorporating context-aware scaling factor significantly accelerates convergence and enhances training stability in Transformer models. These factors dynamically adjust attention scores, reducing the reliance on extreme outliers for attention distribution. This leads to a smoother optimization landscape, promoting faster and more stable convergence. As shown in Figure~\ref{fig:train_loss}, models with context-aware scaling exhibit a more gradual decline in training loss compared to those using default attention mechanisms, indicating the potential for more stable training dynamics.

% \begin{figure*}[h!]
%     \vspace{-2mm}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/train_loss_comparison.pdf}
%     \vspace{-1mm}
%     \caption{Training loss comparison showing improved convergence by eliminating outliers.} 
%     \vspace{-2mm}
%     \label{fig:train_loss}
% \end{figure*}


% \begin{table}[h!]
%     \centering
%     \begin{tabular}{ccc|c}
%         \toprule
%         \textbf{Methods} & \textbf{Setup} & \textbf{Default GPT-2} & \textbf{GPT-2 with Context-aware Scaling Factor} \\
%         \midrule
%         Baseline & / & 39.3678 & 37.9458 \\
%         Magnitude Pruning & 50\% weights & 809.2368 & 1019.9517 \\
%         Weight Quantization & int4 & 3 & 3 \\
%         KV Cache Compression & local & 3 & 3 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Consistency of different types of outliers across dimensions}
%     \label{tab:compression_friendly}
% \end{table}


\vspace{-1.5mm}
\section{Conclusion}\label{sec:conclusion}
\vspace{-1.5mm}

In this work, we systematically analyzed the distribution, formation, and roles of outliers in large language models (LLMs), categorizing them into activation, weight, and attention outliers. Our findings reveal that these outliers are interconnected across layers and dimensions, stemming from the softmax operation in the self-attention mechanism. Acting as implicit, context-aware scaling factors, these outliers dynamically adjust attention distributions, enabling the model to balance diverse contextual demands. By eliminating these outliers through explicit context-aware scaling, we showed improvements in model convergence and compression efficiency. Our approach helps reduce unnecessary attention allocation, making models more efficient and stable. This finding provides new insights into the internal workings of Transformer-based LLMs and opens up avenues for refining attention mechanisms to improve performance and efficiency. We believe that our study not only deepens the theoretical understanding of outliers in LLMs but also has practical implications for the development of more efficient and robust language models.

\vspace{-1.5mm}
\section{Acknowledgements}\label{sec:acknow}
\vspace{-1.5mm}

This work was supported by the National Key R$\&$D Program of China under Grant No.2023ZD0120400, Beijing Municipal Science and Technology Project (Z231100007423004), Zhejiang Lab (No. 2021KH0AB07), and National Natural Science Foundation of China (Grant No. 62206290, 62276260, 62176254, 61976210, 62076235).

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix

\section{Additional Results on Systematic Outliers in LLMs}\label{app:existence}

This section extends the analysis of systematic outliers in LLMs, complementing the findings presented in the main paper. We provide additional results on both pretrained LLMs (Appendix~\ref{app:existence-more-llms}) and fine-tuned variants (Appendix~\ref{app:existence-ft-llms}), highlighting the consistency and variation in outlier behavior across different model families and training paradigms.

\subsection{Pretrained LLMs}\label{app:existence-more-llms}

In Section~\ref{sec:loc}, we identified systematic outliers in models such as LLaMA2-7B. To further validate these findings, we extend our evaluation to a broader set of pretrained LLMs, including Phi-2~\citep{javaheripi2023phi2}, Mistral-7B~\citep{jiang2023mistral}, LLaMA2-13B, LLaMA3~\citep{dubey2024llama}, OPT-6.7B~\citep{zhang2022opt}, MPT-7B~\citep{databricks_mpt7b}, and Falcon-7B~\citep{almazrouei2023falcon}. The corresponding results are depicted in Figure~\ref{fig:app-llama2-13b}, \ref{fig:app-llama3-8b}, \ref{fig:app-opt-7b}, \ref{fig:app-mpt-7b} and \ref{fig:app-falcon-7b}. 

Our analysis reveals several key insights:

\begin{itemize}[leftmargin=20pt]
    \item \textbf{Consistency of Outliers:} Systematic outliers consistently appear across all evaluated models, with patterns similar to those observed in Section~\ref{sec:loc}.  
    \item \textbf{Influence of Model Architecture:} The design of the MLP architecture and the choice of activation function significantly impact the distribution of weight outliers and activation outliers. For example, OPT-6.7B, which employs a two-layer linear structure with ReLU activation, exhibits a more dispersed pattern of outliers compared to the compact clustering seen in models like LLaMA2-13B and Mistral-7B.  
    \item \textbf{Layer-Specific Trends:} In deeper layers, particularly in LLaMA3 and Falcon-7B, outliers in down-projection activations are more pronounced, suggesting that architectural modifications in newer models can amplify outlier formation in specific layers.  
\end{itemize}  

These results demonstrate that systematic outliers are a pervasive phenomenon across diverse model families and architectures.  

\subsection{Fine-tuned LLMs}\label{app:existence-ft-llms}

Beyond pretrained models, fine-tuning plays a crucial role in adapting LLMs for specific tasks such as instruction following or conversational applications~\citep{ouyang2022training}. To assess the impact of fine-tuning on systematic outliers, we analyze fine-tuned variants of LLaMA2 and Mistral, with results shown in Figures~\ref{fig:app-llama2-7b-chat}, \ref{fig:app-llama2-13b-chat}, and \ref{fig:app-mistral-7b-instruct}.  

\begin{itemize}[leftmargin=20pt]  
    \item \textbf{Persistence of Outliers:} Systematic outliers persist after fine-tuning, with their magnitudes and distributions largely unchanged from the corresponding pretrained models. For instance, Figures~\ref{fig:app-mistral-7b} and \ref{fig:app-mistral-7b-instruct} demonstrate that outlier patterns in Mistral-7B are consistent with its fine-tuned counterpart, Mistral-7B Instruct. Similarly, Figures~\ref{fig:so_llama2_7b} and \ref{fig:app-llama2-7b-chat} show that fine-tuning LLaMA2-7B into LLaMA2-7B-Chat introduces minimal changes to outlier locations and distributions.  
    \item \textbf{Fine-Tuning Effects:} Instruction fine-tuning does not significantly alter the structural patterns of systematic outliers. Although minor variations in magnitudes or attention scores are observed in some layers, such as reduced attention outlier intensities in Mistral-7B Instruct, these adjustments do not affect the overall spatial or dimensional concentration of outliers.  
    \item \textbf{Generalizability:} The consistent presence of systematic outliers across both pretrained and fine-tuned models indicates a structural origin rooted in the Transformer architecture, rather than task-specific artifacts introduced during fine-tuning. This highlights their intrinsic nature and robustness to different training paradigms.  
\end{itemize}  

These results highlight the robustness of systematic outliers across both pretrained and fine-tuned models, emphasizing their structural roots within Transformer architectures.  

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/existence/ao_down_phi2.pdf}
        \caption{activation: $\mathbf{x}_{\ell}^{\text{down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/existence/wo_phi2.pdf}
        \caption{weight: $\mathbf{W}_{\ell}^{\text {down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/existence/ao_out_phi2.pdf}
        \caption{activation: $\mathbf{h}_{\ell}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/existence/attno_phi2.pdf}
        \caption{attention: $\mathbf{A}^i_{\ell}$}
    \end{subfigure}
    \vspace{-1mm}
    \caption{Systematic outliers in Phi-2.}
    \vspace{-1mm}
    \label{fig:app-phi2}
\end{figure}

\begin{figure}[ht!] 
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/existence/ao_down_mistral_7b.pdf}
        \caption{activation: $\mathbf{x}_{\ell}^{\text{down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/existence/wo_mistral_7b.pdf}
        \caption{weight: $\mathbf{W}_{\ell}^{\text {down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/existence/ao_out_mistral_7b.pdf}
        \caption{activation: $\mathbf{h}_{\ell}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/existence/attno_mistral_7b.pdf}
        \caption{attention: $\mathbf{A}^i_{\ell}$}
    \end{subfigure}
    \vspace{-1mm}
    \caption{Systematic outliers in Mistral-7B.}
    \vspace{-1mm}
    \label{fig:app-mistral-7b}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/ao_down_llama2_13b.pdf}
        \caption{activation: $\mathbf{x}_{\ell}^{\text{down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/wo_llama2_13b.pdf}
        \caption{weight: $\mathbf{W}_{\ell}^{\text {down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/ao_out_llama2_13b.pdf}
        \caption{activation: $\mathbf{h}_{\ell}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/attno_llama2_13b.pdf}
        \caption{attention: $\mathbf{A}^i_{\ell}$}
    \end{subfigure}
    \vspace{-1mm}
    \caption{Systematic outliers in LLaMA2-13B.}
    \vspace{-1mm}
    \label{fig:app-llama2-13b}
\end{figure}


\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/ao_down_llama3_8b.pdf}
        \caption{activation: $\mathbf{x}_{\ell}^{\text{down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/wo_llama3_8b.pdf}
        \caption{weight: $\mathbf{W}_{\ell}^{\text {down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/ao_out_llama3_8b.pdf}
        \caption{activation: $\mathbf{h}_{\ell}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/attno_llama3_8b.pdf}
        \caption{attention: $\mathbf{A}^i_{\ell}$}
    \end{subfigure}
    \vspace{-1mm}
    \caption{Systematic outliers in LLaMA3-8B.}
    \vspace{-1mm}
    \label{fig:app-llama3-8b}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/ao_down_opt_7b.pdf}
        \caption{activation: $\mathbf{x}_{\ell}^{\text{down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/wo_opt_7b.pdf}
        \caption{weight: $\mathbf{W}_{\ell}^{\text {down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/ao_out_opt_7b.pdf}
        \caption{activation: $\mathbf{h}_{\ell}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/attno_opt_7b.pdf}
        \caption{attention: $\mathbf{A}^i_{\ell}$}
    \end{subfigure}
    \vspace{-1mm}
    \caption{Systematic outliers in OPT-6.7B.}
    \vspace{-1mm}
    \label{fig:app-opt-7b}
\end{figure}


\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/ao_down_mpt_7b.pdf}
        \caption{activation: $\mathbf{x}_{\ell}^{\text{down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/wo_mpt_7b.pdf}
        \caption{weight: $\mathbf{W}_{\ell}^{\text {down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/ao_out_mpt_7b.pdf}
        \caption{activation: $\mathbf{h}_{\ell}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/attno_mpt_7b.pdf}
        \caption{attention: $\mathbf{A}^i_{\ell}$}
    \end{subfigure}
    \vspace{-1mm}
    \caption{Systematic outliers in MPT-7B.}
    \vspace{-1mm}
    \label{fig:app-mpt-7b}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/ao_down_falcon_7b.pdf}
        \caption{activation: $\mathbf{x}_{\ell}^{\text{down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/wo_falcon_7b.pdf}
        \caption{weight: $\mathbf{W}_{\ell}^{\text {down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/ao_out_falcon_7b.pdf}
        \caption{activation: $\mathbf{h}_{\ell}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_pretrained_models/attno_falcon_7b.pdf}
        \caption{attention: $\mathbf{A}^i_{\ell}$}
    \end{subfigure}
    \vspace{-1mm}
    \caption{Systematic outliers in Falcon-7B.}
    \vspace{-1mm}
    \label{fig:app-falcon-7b}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_ft_models/ao_down_llama2_7b_chat.pdf}
        \caption{activation: $\mathbf{x}_{\ell}^{\text{down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_ft_models/wo_llama2_7b_chat.pdf}
        \caption{weight: $\mathbf{W}_{\ell}^{\text {down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_ft_models/ao_out_llama2_7b_chat.pdf}
        \caption{activation: $\mathbf{h}_{\ell}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_ft_models/attno_llama2_7b_chat.pdf}
        \caption{attention: $\mathbf{A}^i_{\ell}$}
    \end{subfigure}
    \vspace{-1mm}
    \caption{Systematic outliers in LLaMA2-7B-Chat.}
    \vspace{-1mm}
    \label{fig:app-llama2-7b-chat}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_ft_models/ao_down_llama2_13b_chat.pdf}
        \caption{activation: $\mathbf{x}_{\ell}^{\text{down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_ft_models/wo_llama2_13b_chat.pdf}
        \caption{weight: $\mathbf{W}_{\ell}^{\text {down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_ft_models/ao_out_llama2_13b_chat.pdf}
        \caption{activation: $\mathbf{h}_{\ell}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_ft_models/attno_llama2_13b_chat.pdf}
        \caption{attention: $\mathbf{A}^i_{\ell}$}
    \end{subfigure}
    \vspace{-1mm}
    \caption{Systematic outliers in LLaMA2-13B-Chat.}
    \vspace{-1mm}
    \label{fig:app-llama2-13b-chat}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_ft_models/ao_down_mistral_7b_instruct.pdf}
        \caption{activation: $\mathbf{x}_{\ell}^{\text{down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_ft_models/wo_mistral_7b_instruct.pdf}
        \caption{weight: $\mathbf{W}_{\ell}^{\text {down}}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_ft_models/ao_out_mistral_7b_instruct.pdf}
        \caption{activation: $\mathbf{h}_{\ell}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{figures/appendix_ft_models/attno_mistral_7b_instruct.pdf}
        \caption{attention: $\mathbf{A}^i_{\ell}$}
    \end{subfigure}
    \vspace{-1mm}
    \caption{Systematic outliers in Mistral-7B-Instruct.}
    \vspace{-1mm}
    \label{fig:app-mistral-7b-instruct}
\end{figure}

\section{Detailed Experimental Settings}\label{app:setting}  

This section provides comprehensive details on the experimental settings used to analyze systematic outliers in LLMs. It covers the methodologies and configurations employed in different aspects of the study, ensuring reproducibility and clarity. The following subsections provide detailed descriptions of the experimental setups for each analysis.

\subsection{Position Analysis Settings}\label{app:setting-position}

This subsection describes the experimental settings used to analyze the distribution of systematic outliers in LLaMA2-7B across layers, sequences, and feature dimensions. The focus is on three types of outliers—\textbf{activation outliers}, \textbf{weight outliers}, and \textbf{attention outliers}, with results presented in Figures~\ref{fig:pos_ao_out}, \ref{fig:pos_ao_down}, \ref{fig:pos_wo}, and \ref{fig:pos_attno}. 

For activation outliers in $\mathbf{h}_{\ell}$ (layer outputs) and $\mathbf{x}_{\ell}^{\text{down}}$ (down-projection inputs), we analyze the top-3 largest activation values and the median activation value for each layer to examine how outliers are distributed across shallow, middle, and deep layers. Using 100 sequences of length 2,048 from the RedPajama dataset~\citep{together2023redpajama}, the positions where activation outliers first appear are identified. Additionally, scatter plots visualize the channel indices where activation outliers occur, highlighting fixed feature dimensions associated with these outliers.

For weight outliers in $\mathbf{W}_{\ell}^{\text{down}}$ (down-projection matrices), we compute the \emph{extremal ratio} for each layer and module to measure how concentrated the largest weights are in specific columns.

For attention outliers in $\mathbf{A}^i_{\ell}$ (attention weights), we analyze the top-2 cumulative attention scores and the median score for each layer to track how attention outliers persist across layers. Sequence positions where attention outliers first appear are identified using the same set of 100 sequences. Finally, the mean, upper limit, and lower limit of attention outlier scores are computed for different heads across layers to reveal head-specific patterns.

These settings provide a systematic approach to understanding the spatial and dimensional patterns of systematic outliers in LLaMA2-7B.

\subsection{Consistency Analysis Settings}\label{app:setting-consistency}

To analyze the consistency between different types of outliers, we calculate the alignment of activation and attention outliers across 100 randomly selected samples from the RedPajama dataset~\citep{together2023redpajama}. Each sample has a sequence length of 2,048, and attention outliers are analyzed separately for each attention head.

The overlap between activation and attention outliers is defined as the percentage of sequence indices where activation outliers in $\mathbf{h}_{\ell} \in \mathbb{R}^{\text{seqlen} \times d_{\text{hidden}}}$ align with attention outliers in $\mathbf{A}^i_{\ell} \in \mathbb{R}^{\text{seqlen} \times \text{seqlen}}$, where $\mathbf{A}^i_{\ell}$ is the cumulative attention score matrix for head $i$ at layer $\ell$. For each sample and attention head, overlaps are computed as:

$$
\text{Overlap}^i = \frac{\lvert \mathcal{O}_{\text{activation}} \cap \mathcal{O}_{\text{attention}}^i \rvert}{\lvert \mathcal{O}_{\text{activation}} \rvert},
$$

where $\mathcal{O}_{\text{activation}}$ and $\mathcal{O}_{\text{attention}}^i$ are the sets of sequence indices corresponding to activation and attention outliers, respectively. The overall overlap across $n_{\text{samples}}$ samples and $n_{\text{heads}}$ attention heads is then averaged as:

$$
\text{Overall Overlap} = \frac{1}{n_{\text{samples}} \cdot n_{\text{heads}}} \sum_{k=1}^{n_{\text{samples}}} \sum_{i=1}^{n_{\text{heads}}} \text{Overlap}^i.
$$

These settings systematically quantify the alignment between activation and attention outliers, offering detailed insights into their interconnected nature, as summarized in Table~\ref{tab:outliers_overlap}.

\subsection{GPT-2 Attention Variant Training Settings}\label{app:setting-attention-variants}

We utilize the open-source GPT-2 implementation from the NanoGPT repository~\citep{karpathy2023nanogpt}, following the default recommended training setup and optimizer settings. Each of the five GPT-2 variants was trained for 50,000 iterations, processing approximately 2 billion tokens in total. For the attention bias variant, we followed the initialization method proposed by~\citep{sun2024massive}, setting $k'$ and $v'$ to $\mathcal{N}(0, 0.02\mathbf{I})$.

\subsection{Model Compression Experiment Settings}\label{app:setting-compression}

To evaluate the robustness of context-aware scaling factors under compression, we conducted experiments on GPT-2 models using two common methods: quantization and pruning. For quantization, 8-bit weight quantization was applied using the AbsMax scaling method, which normalizes weights by their maximum absolute value. For pruning, we performed unstructured magnitude pruning, removing 50\% of the smallest-magnitude weights across all layers. 

Both models were evaluated on the WikiText2 dataset using perplexity (PPL) as the primary metric, comparing GPT-2 Default and GPT-2 with context-aware scaling factors. These settings were chosen to test the models' ability to maintain performance under aggressive compression techniques.

% \section{Systematic Outliers as Implicit Context-Aware Scaling Factors}\label{app:role}

\section{How Softmax Causes Systematic Outliers in Transformer Models}\label{app:cause}

The formation of systematic outliers in transformer models stems from the inherent characteristics of the softmax operation within the self-attention mechanism. While capturing the complete dynamics of outlier emergence requires a complex understanding of training processes, we provide a mathematical analysis that outlines the logical connection between softmax and the appearance of systematic outliers. This analysis reveals how the interaction between softmax and model architecture propagates and localizes these anomalies. The following sequence summarizes the key steps:

\begin{enumerate}[leftmargin=15pt]
    \item \textbf{Necessity of Zero-Update in MHA:} Certain tokens, such as initial tokens or weakly semantic tokens, often require minimal contextual updates. The Multi-Head Attention (MHA) mechanism addresses this by dynamically suppressing updates for these tokens, placing strict constraints on gradients and weights.
    
    \item \textbf{Softmax-Induced Dynamic Range Expansion:} Achieving the zero-update behavior requires softmax to focus attention weights on a limited number of keys. This necessitates substantial differences in the dot products of query and key vectors, leading to extreme dynamic ranges in the attention scores.

    \item \textbf{Propagation of Systematic Outliers:} The extreme attention scores propagate anomalies through transformer computations:
    \begin{itemize}[leftmargin=10pt]
        \item In MHA, the shared projection weights for keys ($W_K$) and values ($W_V$) accumulate steep gradients, introducing activation anomalies.
        \item In the Multi-Layer Perceptron (MLP), these activation anomalies are amplified by the projection layers, resulting in systematic outliers.
    \end{itemize}

    \item \textbf{Localization of Outliers:} Outliers concentrate at specific tokens and feature dimensions:
    \begin{itemize}[leftmargin=10pt]
        \item \textbf{Token-Level:} Initial tokens (e.g., [CLS]) and weakly semantic tokens exhibit pronounced updates due to their unique roles in aggregation and suppression.
        \item \textbf{Channel-Level:} Outliers appear in a limited set of feature dimensions, reducing their impact on other parts of the model but exacerbating localized anomalies.
    \end{itemize}

    \item \textbf{Why Earlier Layers Avoid This:} Early transformer layers focus on distributing token information uniformly, maintaining balanced attention distributions. Systematic outliers predominantly emerge in deeper layers, where higher-level semantic differentiation sharpens the attention mechanism.
\end{enumerate}

This systematic analysis reveals that the softmax operation, in combination with architectural constraints, is a central factor driving the emergence and propagation of systematic outliers. The subsequent sections delve into the specific mechanisms behind these phenomena, starting with the necessity of zero-update behavior in self-attention.


\subsection{Necessity of Zero-Update in Multi-Head Attention (MHA)}\label{app:cause-zero-update}

In transformer models, the Multi-Head Attention (MHA) mechanism dynamically adjusts token representations based on contextual information. However, certain tokens—such as initial tokens (e.g., [CLS]) or weakly semantic tokens (e.g., punctuation marks)—often require minimal updates during training. For these tokens, the desired behavior is a near-zero update:

\[
\Delta x = \text{MHA}(Q_x, K, V) \approx 0.
\]

Achieving this condition imposes constraints on the gradients and attention weights. The softmax normalization within MHA must allocate most of the attention probability to a few keys with negligible values, effectively canceling the contributions of the remaining keys. The output of the attention mechanism is given by:

\[
\text{MHA}(Q, K, V) = \text{Softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V,
\]

where \( Q, K, V \) are the query, key, and value matrices, and \( d_k \) is the dimensionality of the keys. For a given query \( Q_x \), the attention weights must satisfy:

\[
\sum_{j=1}^n \text{Softmax}\left(\frac{Q_x K_j^\top}{\sqrt{d_k}}\right) = 1.
\]

To approximate zero-update, the weighted sum of \( V_j \) values must effectively cancel out. This necessitates a highly concentrated attention weight distribution, which in turn creates large disparities in the query-key dot products (\( Q_x K_j^\top \)).

This zero-update requirement, while essential for certain tokens, introduces challenges:

\begin{itemize}[leftmargin=15pt]
    \item \textbf{Dynamic Weight Adjustment:} The softmax must focus the attention weights on specific keys, requiring the associated dot products (\( Q_x K_j^\top \)) to dominate.
    \item \textbf{Gradient Amplification:} The backpropagation process must enforce selective updates to queries (\( Q_x \)) and keys (\( K_j \)), leading to steep gradients.
\end{itemize}

This forms the foundation for the emergence of extreme values in both the attention weights and the gradients, as discussed in the following section.

\subsection{Softmax-Induced Dynamic Range Expansion}\label{app:cause-dynamic-range}

The softmax operation, central to the self-attention mechanism, inherently expands the dynamic range of attention scores. This behavior is critical for satisfying the zero-update condition for certain tokens but simultaneously leads to the emergence of extreme values.

The softmax operation is defined as:

\[
A_{ij} = \text{Softmax}\left(\frac{Q_i K_j^\top}{\sqrt{d_k}}\right) = \frac{\exp\left(\frac{Q_i K_j^\top}{\sqrt{d_k}}\right)}{\sum_{k=1}^n \exp\left(\frac{Q_i K_k^\top}{\sqrt{d_k}}\right)},
\]

where \( A_{ij} \) represents the attention weight assigned to key \( j \) by query \( i \), and \( n \) is the sequence length.

To achieve zero-update for a token \( x \), the softmax must concentrate the attention weight \( A_{xj} \) on specific keys \( j^* \) while suppressing the weights for others. This requires the dot product \( \frac{Q_x K_{j^*}^\top}{\sqrt{d_k}} \) to significantly dominate over other terms \( \frac{Q_x K_k^\top}{\sqrt{d_k}} \). Mathematically, this results in:

\[
\frac{Q_x K_{j^*}^\top}{\sqrt{d_k}} \gg \frac{Q_x K_k^\top}{\sqrt{d_k}}, \quad \forall k \neq j^*.
\]

As a consequence, the ratio of exponential terms grows exponentially with the difference in dot products. For example, if \( \frac{Q_x K_{j^*}^\top}{\sqrt{d_k}} = M \) and \( \frac{Q_x K_k^\top}{\sqrt{d_k}} = 0 \) for \( k \neq j^* \), the resulting attention weight is:

\[
A_{xj^*} = \frac{\exp(M)}{\exp(M) + (n-1)} \approx 1, \quad \text{as } M \to \infty.
\]

In contrast, the weights for other keys approach zero:

\[
A_{xk} = \frac{1}{\exp(M) + (n-1)} \quad \text{for } k \neq j^*.
\]

This dynamic range expansion, driven by the softmax operation, imposes two key challenges:

\begin{itemize}[leftmargin=15pt]
    \item \textbf{Extreme Values in Attention Scores:} The attention weights for dominant keys grow exponentially, while others become negligible. This leads to highly imbalanced attention distributions.
    \item \textbf{Gradient Amplification:} Backpropagation through the softmax induces steep gradients for the dominant keys, amplifying updates to the query (\( Q \)) and key (\( K \)) vectors. This can destabilize the training process.
\end{itemize}

These extreme values propagate through subsequent layers, contributing to the formation of systematic outliers in both activations and weights, as detailed in the next subsection.

\subsection{Propagation of Systematic Outliers}\label{app:cause-propagation}

Systematic outliers, once introduced by the softmax mechanism, propagate through the transformer layers due to shared weights and non-linear transformations in the Multi-Head Attention (MHA) and Multi-Layer Perceptron (MLP) components.

\paragraph{Impact in MHA.}
The shared projection weights in MHA—specifically, the key and value matrices \( W_K \) and \( W_V \)—magnify the effect of extreme attention scores. The query (\( Q \)), key (\( K \)), and value (\( V \)) matrices are computed as:

\[
Q = \text{LN}(h_\ell) W_Q, \quad K = \text{LN}(h_\ell) W_K, \quad V = \text{LN}(h_\ell) W_V,
\]

where \( h_\ell \) represents the layer's input, and \( \text{LN} \) is layer normalization. When the attention scores \( A \) are dominated by a few keys, the output of MHA focuses heavily on the corresponding values \( V_j \). The MHA output for token \( x \) is given by:

\[
\text{MHA}(x) = \sum_{j=1}^n A_{xj} V_j,
\]

where \( A_{xj} \) concentrates on a few dominant keys. This imbalance introduces large updates to the projection weights \( W_K \) and \( W_V \) during backpropagation, as the gradients for these weights are computed from \( \nabla_{W_K} L \) and \( \nabla_{W_V} L \), respectively.

\paragraph{Amplification in MLP.}
Following MHA, the output passes through the MLP block, which consists of up-projection (\( W_\text{up} \)), a non-linear activation (\( \sigma \)), and down-projection (\( W_\text{down} \)):

\[
z_\text{down} = W_\text{down} \sigma(W_\text{up} \text{LN}(h_{\ell+1/2})),
\]

where \( h_{\ell+1/2} = \text{LN}(h_\ell) + \text{MHA}(\text{LN}(h_\ell)) \). If \( \text{MHA}(x) \) produces extreme values due to attention outliers, these anomalies are further amplified by the non-linear activation (\( \sigma \)) and concentrated in the down-projection weights (\( W_\text{down} \)).

\paragraph{Emergence of Outliers.}
The combination of steep gradients and non-linear transformations results in the emergence of outliers in both activations and weights. Key observations include:

\begin{itemize}[leftmargin=15pt]
    \item \textbf{Activation Outliers:} These appear in the intermediate representations \( h_{\ell+1} \), primarily concentrated in specific sequence positions and feature dimensions.
    \item \textbf{Weight Outliers:} The steep gradients for \( W_K \), \( W_V \), and \( W_\text{down} \) lead to concentrated large weights in these matrices, reinforcing the cycle of extreme values.
\end{itemize}

This propagation mechanism underscores the role of softmax-induced dynamic range expansion in perpetuating systematic outliers throughout the transformer layers.

\subsection{Localization of Systematic Outliers}\label{app:cause-localization}

Systematic outliers are not randomly distributed but exhibit specific patterns of localization across tokens, layers, and feature dimensions. This section explores how these outliers are concentrated at particular positions and channels, minimizing their overall disruption while fulfilling the model's dynamic range requirements.

\paragraph{Token-Level Localization.}
Outliers are predominantly associated with specific tokens, such as initial tokens (e.g., \texttt{[CLS]}) or tokens with weak semantic content (e.g., punctuation marks). These tokens are particularly susceptible to outliers due to their roles in aggregating sequence information or carrying minimal intrinsic meaning. For instance:
\begin{itemize}[leftmargin=15pt]
    \item \textbf{Initial Tokens:} Initial tokens aggregate global information, receiving disproportionately high attention scores, which amplifies their values in the MHA output.
    \item \textbf{Weak-Semantics Tokens:} Tokens like \texttt{.} or \texttt{-} often have low intrinsic information, leading the model to assign high attention scores to stabilize their contextual representation. This results in exaggerated updates during training.
\end{itemize}

\paragraph{Channel-Level Localization.}
Outliers in feature dimensions are typically confined to a small subset of channels. This sparsity arises because the model prioritizes containing the impact of extreme values to a few dimensions rather than spreading them across the entire representation. Key characteristics include:
\begin{itemize}[leftmargin=15pt]
    \item \textbf{Fixed Dimensions:} Outliers are observed in specific rows or columns of weight matrices (e.g., \( W_\text{down} \)) across layers, suggesting a structural origin.
    \item \textbf{Robustness Preservation:} By localizing extreme values to a few channels, the model ensures that most dimensions remain stable, preserving the robustness of the overall representation.
\end{itemize}

\subsection{Conclusion}\label{app:cause-conclusion}

The mathematical analysis presented in this section demonstrates how the softmax operation in the self-attention mechanism is a key driver of systematic outliers in transformer models. By fulfilling the zero-update requirement for certain tokens, softmax induces extreme disparities in attention scores, leading to steep gradients and the emergence of outliers. These anomalies propagate through transformer layers, being amplified by shared projection weights and non-linear activations in the MLP, ultimately manifesting as systematic outliers in both activations and weights. 

Moreover, these outliers exhibit distinct localization patterns, being concentrated at specific tokens and feature dimensions. This localization minimizes their overall disruption to the model while fulfilling the dynamic range demands imposed by the softmax mechanism. Understanding these dynamics offers valuable insights into the structural origins of systematic outliers, paving the way for mitigation strategies such as explicit context-aware scaling factors to prevent their formation and improve model robustness.

\section{More analysis fo Systematic Outliers}\label{app:more}  

\subsection{Absence of Outliers in Sigmoid Attention}\label{app:more-sigmoid}

In \citet{ramapuram2024theory}, sigmoid self-attention was proposed as an alternative to traditional softmax-based attention, formulated in Equation~\ref{eq:sigmoid_attn}. Unlike softmax, sigmoid attention independently maps each attention score to a value between 0 and 1, introducing a fixed bias term $b$ to adjust the sigmoid function's activation.

\begin{equation}
    \begin{array}{r}
    \text { SigmoidAttn }(\boldsymbol{X})=\sigma\left(\boldsymbol{Q} \boldsymbol{K}^T / \sqrt{d_{q k}}\right) \boldsymbol{V}, \\
    \text { with } \sigma: u \mapsto \operatorname{sigmoid}(u+b):=\left(1+e^{-(u+b)}\right)^{-1}
    \end{array}
    \label{eq:sigmoid_attn}
\end{equation}

A key distinction of sigmoid attention is its ability to output near-zero values for certain tokens. While this can lead to vanishing gradients for some inputs, it also eliminates the extreme dynamic range caused by softmax normalization, thereby mitigating the formation of systematic outliers.

To evaluate its impact, we trained a GPT-2 model with sigmoid attention using the same experimental setup described in Appendix~\ref{app:setting-attention-variants}. Figure~\ref{fig:app-sigmoid} illustrates that sigmoid attention successfully eliminates systematic outliers. This finding reinforces our hypothesis that softmax normalization is a primary cause of outliers in self-attention mechanisms.

\begin{figure}[ht!]
    \vspace{-3mm}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/exp/sigmoid_3d.pdf}
    \caption{Absence of outliers in sigmoid attention.}
    \label{fig:app-sigmoid}
\end{figure}

\subsection{Explicit Context-Aware Scaling Factor in Tiny-LLaMA}\label{app:more-llama}

To verify the generalizability of our findings beyond GPT-2, we conducted additional experiments using a TinyLLaMA-120M model. The training setup and code were adapted from the open-source implementation of TinyLLaMA~\citep{tinyllama2024}, which provides an efficient framework for pretraining Transformer models.

In this experiment, we replaced the standard self-attention mechanism in TinyLLaMA with the "explicit context-aware scaling factor" variant. As shown in Figure~\ref{fig:app-tinyllama}, the results were consistent with those observed for GPT-2: systematic outliers were completely eliminated, confirming the effectiveness of the explicit context-aware scaling factor.

This finding further corroborates the analysis presented in the main paper. Standard attention mechanisms often require $\text{MHA}(x) \approx 0$ updates for certain tokens during training. Achieving this with softmax-based attention induces extremely large gradients, which lead to the formation of outliers in activations and weights. By contrast, the explicit context-aware scaling factor achieves the same zero-update objective without generating large gradients, thus avoiding outlier formation. These results demonstrate the robustness of the proposed approach across different Transformer architectures.

\begin{figure}[ht!]
    \vspace{-3mm}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/exp/tinyllama_3d.pdf}
    \caption{Visualization of TinyLLaMA-120M with explicit context-aware scaling factor. The absence of systematic outliers illustrates the effectiveness of this approach in preventing outlier formation.}
    \label{fig:app-tinyllama}
\end{figure}

\subsection{Impact of Sequence Length on Outliers}\label{app:more-seqlen}

Our analysis shows that sequence length does not affect the existence of attention outliers, but it does influence their specific positions within the sequence. Outliers consistently appear at the first tokens and semantically weak tokens, with their relative positioning shifting as the sequence length changes. The observations are summarized as follows:

\begin{itemize}[leftmargin=20pt]
    \item \textbf{Short Sequence:} In the sequence "Summer is warm!", attention outliers occur at "Summer".
    \item \textbf{Moderately Extended Sequence:} Extending the sequence to "Summer is warm! Winter is cold." introduces an additional outlier at "." (the first period), alongside "Summer".
    \item \textbf{Further Extended Sequence:} In the sequence "Summer is warm! Winter is cold. Spring is good.", outliers remain at "Summer" and the first period, demonstrating that outlier diversity does not increase with sequence length.
    \item \textbf{Modified Sequence:} Modifying the sequence to "Summer is warm! Winter is cold! Spring is good." shifts the outlier from the first period to the last period, while "Summer" and weak semantic tokens remain consistent outlier locations.
\end{itemize}

These examples suggest that while sequence length can shift the positions of outliers, their presence is robust across varying lengths. Notably, there is no evidence that specific sequence lengths are more prone to generating outliers. Instead, outliers are influenced by token-level semantics, consistently favoring the first tokens and semantically weak tokens, regardless of sequence length.


\end{document}
