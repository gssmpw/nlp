@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{kovaleva2021bert,
  title={BERT busters: Outlier dimensions that disrupt transformers},
  author={Kovaleva, Olga and Kulshreshtha, Saurabh and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2105.06990},
  year={2021}
}

@article{lin2024rotation,
  title={Rotation and Permutation for Advanced Outlier Management and Efficient Quantization of LLMs},
  author={Lin, Haokun and Xu, Haobo and Wu, Yichen and Cui, Jingzhi and Zhang, Yingtao and Mou, Linzhan and Song, Linqi and Sun, Zhenan and Wei, Ying},
  journal={arXiv preprint arXiv:2406.01721},
  year={2024}
}

@article{nrusimha2024mitigating,
  title={Mitigating the impact of outlier channels for language model quantization with activation regularization},
  author={Nrusimha, Aniruddha and Mishra, Mayank and Wang, Naigang and Alistarh, Dan and Panda, Rameswar and Kim, Yoon},
  journal={arXiv preprint arXiv:2404.03605},
  year={2024}
}

@inproceedings{puccetti2021bert,
  title={How Do BERT Embeddings Organize Linguistic Knowledge?},
  author={Puccetti, Giovanni and Miaschi, Alessio and Dellâ€™Orletta, Felice},
  booktitle={Proceedings of deep learning inside out (DeeLIO): the 2nd workshop on knowledge extraction and integration for deep learning architectures},
  pages={48--57},
  year={2021}
}

@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@article{sun2024massive,
  title={Massive activations in large language models},
  author={Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang},
  journal={arXiv preprint arXiv:2402.17762},
  year={2024}
}

@article{wei2022outlier,
  title={Outlier suppression: Pushing the limit of low-bit transformer language models},
  author={Wei, Xiuying and Zhang, Yunchen and Zhang, Xiangguo and Gong, Ruihao and Zhang, Shanghang and Zhang, Qi and Yu, Fengwei and Liu, Xianglong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17402--17414},
  year={2022}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@article{yin2023outlier,
  title={Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity},
  author={Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Pechenizkiy, Mykola and Liang, Yi and Wang, Zhangyang and Liu, Shiwei},
  journal={arXiv preprint arXiv:2310.05175},
  year={2023}
}

@article{zhang2024unveiling,
  title={Unveiling linguistic regions in large language models},
  author={Zhang, Zhihao and Zhao, Jun and Zhang, Qi and Gui, Tao and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2402.14700},
  year={2024}
}

