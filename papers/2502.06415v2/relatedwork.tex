\section{Related Work}
\label{sec:related}
\vspace{-0.5mm}
\paragraph{Outliers in Large Language Models.}\label{subsec:related-outlier}
Outliers in LLMs refer to values that deviate significantly from the average of their distribution~\citep{dettmers2022gpt3}. Studies have documented various types of outliers in weights, activations, and attention scores, highlighting their presence and impact. For instance, \citet{dettmers2022gpt3} identified activation outliers and proposed quantization techniques to mitigate their effects. \citet{sun2024massive} explored the role of large activations as biases in the attention mechanism. Similarly, \citet{zhang2024unveiling} analyzed weight outliers in LayerNorm layers, demonstrating their importance for maintaining language modeling capabilities in models like GPT-2 and LLaMA2-13B. Additionally, \citet{xiao2023efficient} introduced the concept of the “Attention Sink,” which occurs when a few keys dominate attention scores.

While previous studies recognize the presence of outliers, they typically focus on specific cases or task-specific solutions like quantization and pruning. In contrast, our work provides a systematic categorization of outliers—\emph{activation, weight, and attention outliers}—and reveals their interconnections and collective influence on the attention mechanism in LLMs.

\vspace{-0.5mm}
\paragraph{The Impact of Outliers on Model Performance and Compression.}\label{subsec:related-influence}
Outliers significantly affect both the performance and efficiency of LLMs. Previous research has shown that removing outliers without proper handling can severely degrade performance~\citep{puccetti2021bert, kovaleva2021bert, zhang2024unveiling}. In quantization, outliers amplify rounding and clipping errors, leading to substantial quantization losses~\citep{wei2022outlier, nrusimha2024mitigating, lin2024rotation}. Similarly, magnitude-based pruning strategies face challenges in maintaining model performance when outliers are present~\citep{sun2023simple}. \citet{yin2023outlier} observed that outliers correlate strongly with layer sparsity, further complicating pruning approaches. Moreover, in KV cache compression, \citet{xiao2023efficient} found that attention score outliers associated with specific tokens play a critical role in preserving context.

Despite extensive research on the adverse effects of outliers, their formation mechanisms and functional roles remain largely unexplored. Existing studies focus on mitigating their impact but lack a systematic investigation of their origins. In contrast, our work explores the formation of outliers within the self-attention mechanism, revealing their role as implicit, context-aware scaling factors and proposing structural solutions to enhance model convergence and compression efficiency.

\vspace{-0.5mm}