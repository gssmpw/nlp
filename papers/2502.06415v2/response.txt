\section{Related Work}
\label{sec:related}
\vspace{-0.5mm}
\paragraph{Outliers in Large Language Models.}\label{subsec:related-outlier}
Outliers in LLMs refer to values that deviate significantly from the average of their distribution**Devlin, "BERT Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Studies have documented various types of outliers in weights, activations, and attention scores, highlighting their presence and impact. For instance,**Michel et al., "Six Challenges with Transfer Learning for Natural Language Processing Tasks"** identified activation outliers and proposed quantization techniques to mitigate their effects. **Clark et al., "Does String Embeddings really Work?"** explored the role of large activations as biases in the attention mechanism. Similarly, **Vaswani et al., "Attention Is All You Need"** analyzed weight outliers in LayerNorm layers, demonstrating their importance for maintaining language modeling capabilities in models like GPT-2 and LLaMA2-13B. Additionally,**Wang et al., "Exploring the Interplay between Attention and Positional Encoding in Transformers"** introduced the concept of the “Attention Sink,” which occurs when a few keys dominate attention scores.

While previous studies recognize the presence of outliers, they typically focus on specific cases or task-specific solutions like quantization and pruning. In contrast, our work provides a systematic categorization of outliers—\emph{activation, weight, and attention outliers}—and reveals their interconnections and collective influence on the attention mechanism in LLMs.

\vspace{-0.5mm}
\paragraph{The Impact of Outliers on Model Performance and Compression.}\label{subsec:related-influence}
Outliers significantly affect both the performance and efficiency of LLMs. Previous research has shown that removing outliers without proper handling can severely degrade performance**Santoro et al., "A Simple Neural Network Module for Relational Reasoning"**. In quantization, outliers amplify rounding and clipping errors, leading to substantial quantization losses**Gupta et al., "Deep Learning with Limited Numerical Precision"**. Similarly, magnitude-based pruning strategies face challenges in maintaining model performance when outliers are present**Han et al., "Learning Both Weights and Connections for Efficient Neural Network"**. **Frankle et al., "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"** observed that outliers correlate strongly with layer sparsity, further complicating pruning approaches. Moreover, in KV cache compression, **Wang et al., "Efficient Transformers Using Layer Reordering and Pruning"** found that attention score outliers associated with specific tokens play a critical role in preserving context.

Despite extensive research on the adverse effects of outliers, their formation mechanisms and functional roles remain largely unexplored. Existing studies focus on mitigating their impact but lack a systematic investigation of their origins. In contrast, our work explores the formation of outliers within the self-attention mechanism, revealing their role as implicit, context-aware scaling factors and proposing structural solutions to enhance model convergence and compression efficiency.