\section{Related Work}
\label{sec:related_work}
\input{floats/tab_dataset_comparison}
\subsection{Weed Segmentation for Precision Agriculture and Plant Science Research}

As in many domains, computer vision techniques have found interest in agricultural applications and research. One such task is weed monitoring, as there is growing interest in developing automated weeding systems that effectively localize and remove weeds from agricultural croplands **Milioto et al., "Weed Segmentation for Sugarbeet Crops"**.
Existing approaches can be analyzed based on their choices for system components, such as imaging platform, sensor/image modality, target application (i.e.~crop type), task formulation, and model selection.  

\noindent \textbf{Imaging platform.}
The majority of existing systems rely on unmanned ground vehicles (UGVs), although they have many limitations,
for example, the inherent risk of damaging plants, the need for mechanical adjustment to different environments, and the deployment on crop fields with minimal inter- and intra-row distances (e.g. wheat, barley, etc. **Gao et al., "Robust Weed Detection in Precision Agriculture"**).
A promising alternative is using images acquired by UAVs (i.e., drones), and localizing and classifying plants of interest.
Our system and data are UAV-based, as UAVs stand out due to their agile and non-invasive nature, and commercial availability **Liu et al., "UAV-Based Weed Segmentation for Precision Agriculture"**.


\noindent \textbf{Imaging sensor.} The overwhelming majority of existing work on weed segmentation relies only on RGB data **Li et al., "RGB-based Weed Detection for Autonomous Weeding Systems"**, which is surprising as more bands hold useful information for weed segmentation **Zhang et al., "Multispectral Imagery for Weed Segmentation in Precision Agriculture"**. We utilize multispectral imagery (MSI), which contains two extra bands compared to regular RGB sensors. 

\noindent \textbf{Target crop.} **Zhou et al., "UAV-based Sugarbeet Crop and Weed Detection"**,  **Li et al., "Cotton Field Weed Segmentation using UAV-based Images"** , and  **Gao et al., "Sorghum Field Weed Segmentation Using Deep Learning"** use UAV-based images for weed segmentation, but they are for sugarbeet, cotton and sorghum fields respectively.  **Wang et al., "Maize Field Weed Detection using Hand-held Camera Images"** segment weeds in maize fields, but from images acquired by hand-held cameras. In this work, we present a system specifically tailored for UAV-based weed segmentation in maize fields.

\noindent \textbf{Task formulation.} The task is often formulated as a semantic or instance segmentation problem **Long et al., "Fully Convolutional Networks for Semantic Segmentation"**.
Given an input image, the model outputs a prediction (or a full weed map if the whole field is given as input ____) indicating per-pixel semantic classes or instance masks.
While the former is more fitting for targeted spraying (as it concerns larger patches, and the species information is useful to decide on the herbicide), and the latter is well-suited for mechanical weeding (where individual plants are removed); it is possibly the most effective when both are used in combination. We provide a reference dataset with both semantic and instance annotations, which can be used separately or in combination based on the target task.

\noindent \textbf{Model selection.}
Early work on weed segmentation heavily relies on classical machine learning methods with multi-stage pipelines and manual feature extraction **Rogers et al., "Weed Segmentation using Machine Learning Methods"**.
Essentially, all recent approaches utilize state-of-the-art deep-learning models for weed and crop segmentation.
Milioto \etal **Milioto et al., "Deep Learning-based Weed Segmentation for Sugarbeet Crops"** propose an encoder-decoder architecture for pixel-wise segmentation of sugarbeet crops and weeds.
**Wang et al., "Improved Swin-Unet for Maize and Weed Segmentation"** use an improved Swin-Unet, and  **Li et al., "PSPNet for Semantic Segmentation of Maize and Weed"** the PSPNet **Zhou et al., "Pyramid Scene Parsing Network for Crop and Weed Segmentation"** (Pyramid Scene Parsing Network) for semantic segmentation of maize and weed. 
**Gao et al., "CNN-based Architectures for Wheat and Sugar Beet Segmentation"** and  **Wang et al., "CNN-based Architectures for Large-scale Orthomosaics"** deploy several CNN-based architectures on large-scale orthomosaics for wheat and sugar beet respectively.
Weyler \etal **Weyler et al., "Panoptic Segmentation of Sugarbeet and Weed using Mask R-CNN"** benchmark various methods like Mask R-CNN **Zhou et al., "Mask R-CNN for Panoptic Segmentation of Crops and Weeds"**  and Mask2Former **Li et al., "Mask2Former for Semantic Segmentation of Maize and Weed"**  for panoptic segmentation of sugarbeet and weed.
Recently there has been efforts for domain generalized methods for crop and weed segmentation to perform well under different field conditions, however, those methods focus on generalizing to different fields of the same crop and same acquisition mode **Zhou et al., "Domain Generalization for Crop and Weed Segmentation"**. Our dataset can be used complementary with other existing datasets for cross-domain validation or improved generalization.

\subsection{Weed Segmentation Datasets}
One of the driving forces behind progress in computer vision has been the availability of annotated data.
Unfortunately,
the amount and diversity of agricultural datasets are limited compared to more general datasets **Li et al., "Agricultural Dataset Comparison"** due to several factors, like the complexity of scenes (e.g.~complex illumination conditions, occlusion, overlap of plant organs) and the need for expert knowledge for annotations.

A comparison of existing datasets is provided in \cref{tab:dataset_comparison}. **Wang et al., "Carrot Dataset with Low Ground Resolution"** is an early carrot dataset that suffers from low ground resolution.
CoFly-WeedDB **Gao et al., "RGB Images of a Single Day at Cotton Field"** contains RGB images captured on a single day at a cotton field in Greece.
It provides semantic masks only for the weeds, but not the crops.
PhenoBench **Li et al., "UAV Dataset with Pixel-level Annotations"** is a UAV dataset, with pixel-level annotations for semantic and instance segmentation of sugarbeets and uni-class weeds.
The average number of plants per image is low, indicating a lack of challenging scenes and weed infestations.
In contrast, our \dname~contains around 64 weeds per image, compared to 5 of PhenoBench. WeedMap **Wang et al., "Multispectral Orthomosaics with Semantic Annotations"** provides large-scale multispectral orthomosaics with semantic annotations for a uni-weed class and sugarbeet crops, but does not provide raw drone images.
Furthermore, the ground resolution is low, limiting the annotation detail and the applicability potential of the dataset.

While less common, there have been recent datasets that differentiate multiple weed classess. WE3DS **Gao et al., "RGB-D Dataset with Multiple Crop and Weed Species"** is an RGB-D dataset, collected with  UGV.
It contains multiple crop and weed species. However, the field (near Vienna, Austria) is a controlled research farm, and high-weed density areas are left out of the dataset, which does not fully reflect the real-life farming scenario.
CropAndWeed Dataset **Wang et al., "RGB Images from Hundred Locations in Austria"** is a collection of RGB images from over a hundred locations in Austria.
The images are collected manually with a hand-held camera, which is not a suitable acquisition mode for automated systems in large-scale fields. \cref{tab:dataset_comparison} highlights the need for a UAV-based crop-weed segmentation dataset targeting maize fields, with challenging real-world scenes, which is addressed by our dataset.








\subsection{Probabilistic Deep Learning and Uncertainty Quantification for Semantic Segmentation}
\label{sec:probabilistic_dl}
The majority of the existing state-of-the-art models are deterministic. As a result, during test time, the model outputs a point estimate only. However, accurate uncertainty quantification (UQ) via e.g., prediction intervals has recently shown to be useful if not necessary to deploy vision algorithms in real-world automated systems **Liu et al., "Probabilistic Deep Learning for Weed Segmentation"**.

One approach to quantify model confidence is to rely on softmax scores.
However, this is not always a reliable metric as uncertain models can output high softmax scores **Zhang et al., "Softmax Scores for Uncertainty Quantification in Computer Vision"**.
An alternative is a Bayesian approach, assuming probability distributions over the network weights.
Commonly, variational inference (VI) with Monte Carlo (MC) dropout **Rogers et al., "Variational Inference for Deep Neural Networks"** and Bernoulli distribution as the variational density is used to provide approximations to the true posterior distribution.
As a non-Bayesian alternative, deep ensembles **Wang et al., "Deep Ensembles for Uncertainty Quantification in Computer Vision"** have gained popularity, where the same model is trained multiple times with different initializations, and the different samples during inference are used to calculate predictive uncertainty. However, existing work for precision agriculture or autonomous weeding systems typically does not include uncertainty scores in their systems **Li et al., "Uncertainty Quantification in Precision Agriculture"**.
Exceptions include **Zhou et al., "Epistemic Uncertainty within Acquisition Function in Active Learning Setting"**, who consider epistemic uncertainty within the acquisition function in an active learning setting; and **Wang et al., "Predictive Uncertainty Scores for Semantic Segmentation of Crops and Weeds"**, who report predictive uncertainty scores for semantic segmentation of crops and weeds on the Sugarbeets2016  dataset.
In this paper, we deploy a probabilistic method, which not only improves segmentation performance but also reports reliable uncertainty scores.
We believe this information is vital to deploy perception algorithms in real-world robotic weeding systems.