\section{Related Work}
\label{sec:related_work}
\input{floats/tab_dataset_comparison}
\subsection{Weed Segmentation for Precision Agriculture and Plant Science Research}

As in many domains, computer vision techniques have found interest in agricultural applications and research. One such task is weed monitoring, as there is growing interest in developing automated weeding systems that effectively localize and remove weeds from agricultural croplands \cite{Champ20aps, https://doi.org/10.1002/rob.21938, DENG2024102546}.
Existing approaches can be analyzed based on their choices for system components, such as imaging platform, sensor/image modality, target application (i.e.~crop type), task formulation, and model selection.  

\noindent \textbf{Imaging platform.}
The majority of existing systems rely on unmanned ground vehicles (UGVs), although they have many limitations,
for example, the inherent risk of damaging plants, the need for mechanical adjustment to different environments, and the deployment on crop fields with minimal inter- and intra-row distances (e.g. wheat, barley, etc. \cite{DEVITA201769, BOSTROM2012144}).
A promising alternative is using images acquired by UAVs (i.e., drones), and localizing and classifying plants of interest.
Our system and data are UAV-based, as UAVs stand out due to their agile and non-invasive nature, and commercial availability \cite{9705188, 8239328}.


\noindent \textbf{Imaging sensor.} The overwhelming majority of existing work on weed segmentation relies only on RGB data \cite{GENZE2022107388, weyler2023phenobench, Steininger_2023_WACV, app10207132}, which is surprising as more bands hold useful information for weed segmentation \cite{9081915, milioto2018real}. We utilize multispectral imagery (MSI), which contains two extra bands compared to regular RGB sensors. 

\noindent \textbf{Target crop.} \cite{weyler2023phenobench, Sa2018}, \cite{KRESTENITIS2022108575} and \cite{Genze2023} use UAV-based images for weed segmentation, but they are for sugarbeet, cotton and sorghum fields respectively. \cite{agronomy13071846, Steininger_2023_WACV} segment weeds in maize fields, but from images acquired by hand-held cameras. In this work, we present a system specifically tailored for UAV-based weed segmentation in maize fields.

\noindent \textbf{Task formulation.} The task is often formulated as a semantic or instance segmentation problem \cite{Ahmadi22iros, Champ20aps, milioto2018real, rs10091423}.
Given an input image, the model outputs a prediction (or a full weed map if the whole field is given as input \cite{rs10091423}) indicating per-pixel semantic classes or instance masks.
While the former is more fitting for targeted spraying (as it concerns larger patches, and the species information is useful to decide on the herbicide), and the latter is well-suited for mechanical weeding (where individual plants are removed); it is possibly the most effective when both are used in combination. We provide a reference dataset with both semantic and instance annotations, which can be used separately or in combination based on the target task.

\noindent \textbf{Model selection.}
Early work on weed segmentation heavily relies on classical machine learning methods with multi-stage pipelines and manual feature extraction~\cite{7989347, GUERRERO201211149, 6835733}.
Essentially, all recent approaches utilize state-of-the-art deep-learning models for weed and crop segmentation.
Milioto \etal \cite{milioto2018real} propose an encoder-decoder architecture for pixel-wise segmentation of sugarbeet crops and weeds.
\cite{agronomy13071846} use an improved Swin-Unet, and \cite{PICON2022106719} the PSPNet \cite{Zhao_2017_CVPR} (Pyramid Scene Parsing Network) for semantic segmentation of maize and weed. 
\cite{Wang_2023_ICCV} and \cite{rs10091423} deploy several CNN-based architectures on large-scale orthomosaics for wheat and sugar beet respectively.
Weyler \etal \cite{weyler2023phenobench} benchmark various methods like Mask R-CNN \cite{he2017mask} and Mask2Former \cite{cheng2021mask2former} for panoptic segmentation of sugarbeet and weed.
Recently there has been efforts for domain generalized methods for crop and weed segmentation to perform well under different field conditions, however, those methods focus on generalizing to different fields of the same crop and same acquisition mode \cite{Weyler2023TowardsDG, GAO2024122980}. Our dataset can be used complementary with other existing datasets for cross-domain validation or improved generalization.

\subsection{Weed Segmentation Datasets}
One of the driving forces behind progress in computer vision has been the availability of annotated data.
Unfortunately,
the amount and diversity of agricultural datasets are limited compared to more general datasets~\cite{Lin14eccv,Cordts16cvpr}
due to several factors, like the complexity of scenes (e.g.~complex illumination conditions, occlusion, overlap of plant organs) and the need for expert knowledge for annotations.

A comparison of existing datasets is provided in \cref{tab:dataset_comparison}. \cite{haug15} is an early carrot dataset that suffers from low ground resolution.
CoFly-WeedDB \cite{KRESTENITIS2022108575} contains RGB images captured on a single day at a cotton field in Greece.
It provides semantic masks only for the weeds, but not the crops.
PhenoBench \cite{weyler2023phenobench} is a UAV dataset, with pixel-level annotations for semantic and instance segmentation of sugarbeets and uni-class weeds.
The average number of plants per image is low, indicating a lack of challenging scenes and weed infestations.
In contrast, our \dname~contains around 64 weeds per image, compared to 5 of PhenoBench. WeedMap \cite{rs10091423} provides large-scale multispectral orthomosaics with semantic annotations for a uni-weed class and sugarbeet crops, but does not provide raw drone images.
Furthermore, the ground resolution is low, limiting the annotation detail and the applicability potential of the dataset.

While less common, there have been recent datasets that differentiate multiple weed classess. WE3DS \cite{s23052713} is an RGB-D dataset, collected with  UGV.
It contains multiple crop and weed species. However, the field (near Vienna, Austria) is a controlled research farm, and high-weed density areas are left out of the dataset, which does not fully reflect the real-life farming scenario.
CropAndWeed Dataset \cite{Steininger_2023_WACV} is a collection of RGB images from over a hundred locations in Austria.
The images are collected manually with a hand-held camera, which is not a suitable acquisition mode for automated systems in large-scale fields. \cref{tab:dataset_comparison} highlights the need for a UAV-based crop-weed segmentation dataset targeting maize fields, with challenging real-world scenes, which is addressed by our dataset.








\subsection{Probabilistic Deep Learning and Uncertainty Quantification for Semantic Segmentation}
\label{sec:probabilistic_dl}
The majority of the existing state-of-the-art models are deterministic. As a result, during test time, the model outputs a point estimate only. However, accurate uncertainty quantification (UQ) via e.g., prediction intervals has recently shown to be useful if not necessary to deploy vision algorithms in real-world automated systems \cite{9506719, Sagar_2022_WACV, HERNANDEZ2020106597, Fang_2020}.

One approach to quantify model confidence is to rely on softmax scores.
However, this is not always a reliable metric as uncertain models can output high softmax scores \cite{pmlr-v48-gal16, pmlr-v162-postels22a}.
An alternative is a Bayesian approach, assuming probability distributions over the network weights.
Commonly, variational inference (VI) with Monte Carlo (MC) dropout \cite{pmlr-v48-gal16} and Bernoulli distribution as the variational density is used to provide approximations to the true posterior distribution.
As a non-Bayesian alternative, deep ensembles \cite{NIPS2017_9ef2ed4b} have gained popularity, where the same model is trained multiple times with different initializations, and the different samples during inference are used to calculate predictive uncertainty. However, existing work for precision agriculture or autonomous weeding systems typically does not include uncertainty scores in their systems \cite{Champ20aps}.
Exceptions include \cite{vanmarrewijk2024active}, who consider epistemic uncertainty within the acquisition function in an active learning setting; and \cite{Celikkan_2023_ICCV}, who report predictive uncertainty scores for semantic segmentation of crops and weeds on the Sugarbeets2016 \cite{chebrolu2017ijrr} dataset.
In this paper, we deploy a probabilistic method, which not only improves segmentation performance but also reports reliable uncertainty scores.
We believe this information is vital to deploy perception algorithms in real-world robotic weeding systems.