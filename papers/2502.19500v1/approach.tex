\begin{figure}[!t]
    \centering
    \includegraphics[scale=0.13]{images/language_hierarchical_planning.png}
    \caption{Overview of hierarchical planning via natural language.}
    \label{fig:overview}
\end{figure}

As described in Section \ref{sec:intro}, the goals and tasks users pursue can last for multiple days, months, or even years \cite{christakopoulou2023large}. Developing AI agents assisting towards these can be framed as a \emph{long-horizon} planning problem \cite{sutton1999reinforcement}. Traditionally, this would be framed in a hierarchical reinforcement learning (RL) framework, where the underlying hierarchy of options/ skills would need to be given or learned \cite{bacon2017option}. To enable \emph{conversational} AI agents that can help along the user's journey towards these goals/ tasks in a similar way a human would, and that can adapt based on user feedback, we need to address the following research question:
%\begin{quote}
\emph{How might we do \textbf{hierarchical planning via natural language} to assist with the users' \textbf{plans} serving their real-life goals / tasks?}
%\end{quote}

\subsection{Overview: Language-based Interactive Hierarchical Planning}
\noindent \textbf{RL Framing.} We frame the conversational AI system as a language-based agent that can interact with the user $u$ who is part of the world/environment $\mathcal{E}$. The user starts by using language to specify the goal they want to pursue $g \in \mathcal{L}$ --- this can be viewed as the initial observation $o_1$.  The agent can interact with the user only via natural language feedback $o \in \mathcal{L}$. The feedback can contain new observations about the user, allowing the user to provide more information about themselves and their needs/goals/preferences/how their journey towards this goal is evolving (e.g., \emph{"I've already chosen my vacation destination"}/skill level (e.g., \emph{"I have prior drawing experience"} for a learning journey), and rewards about the provided plan (e.g., \emph{"I don't like this plan"}, or \emph{"Can you add more steps on relaxation techniques before going to bed?"} for a health journey). Given the observations/feedback $o$ from the user $u$, the agent decides the next action $a$ to take, which changes the state from $s_t$ to $s_{t+1}$. The agent can only see partial observations about the user's state based on the natural language feedback. This can be framed as a POMDP \cite{sutton1999reinforcement}, where the agent takes actions according to a $\theta$-parameterized policy, i.e., $a \sim \pi( a | s; \theta)$. We consider language-based agents with policies instantiated via CoT-prompted LLMs.

\noindent \textbf{Hierarchical RL Framing.} The agent can take various actions, such as choosing content resources, asking a question, and providing a plan for the user to follow. Analogously to human reasoning, the agent can make these decisions in a hierarchical fashion, starting from a high level and proceeding to more granular actions. This allows for dealing with long horizons. At every user-agent interaction, rather than providing a single (primitive) action, the agent needs to choose a macro-action / goal / option / skill at every $c$ steps. Then given that macro-action, more granular primitive actions can be selected. This corresponds to a hierarchical policy, consisting of a \emph{high-level policy} or \emph{meta-controller} deciding the macro-action (Section \ref{subsec:meta-controller}), and the \emph{low-level policy}, a sub-policy over primitive actions (Sections \ref{subsec:low-level-policy}, \ref{subsec:tool-use}), both updated over time based on user feedback and with potentially different temporal granularity. This paradigm has been long considered in the RL literature \cite{sutton1999reinforcement, bacon2017option}, but has received little attention in the context of LLMs assisting long-term user goals/tasks. Next, we detail the language-based meta-controller and sub-policies. The overall framework is shown in Figure \ref{fig:overview}.
  
%\subsection{Language-based Meta-Controller: Deciding to ask questions, add or alter plan steps}
\subsection{Proposed Framework Components}

\noindent \textbf{Language-based Meta-Controller: Deciding to ask questions, add or alter plan steps}
\label{subsec:meta-controller}
The meta-policy is instantiated via a CoT-prompted LLM \cite{wei2022chain} capable of generating structured language objects such as JSON \cite{zhou2024self}. The space of actions $\mathcal{Z}$ is discrete, i.e., \texttt{add-steps}, \texttt{alter-steps}, and \texttt{ask-question}, but the underlying LLM can also generate natural language arguments for each macro-action $x_z$, e.g., the specific step names to add or alter, or more context about the question asked. 
The LLM can generate an action $z$, generate arguments for that action, and reason about its actions through thoughts $\tau \in \mathcal{L}$. The thoughts, actions, and arguments for the actions become the structured language meta-actions the controller can take, denoted as $\hat{z}$, with $\hat{\mathcal{Z}} = \mathcal{Z} \cup \mathcal{T} \cup \mathcal{X} \subseteq \mathcal{L}$. Thus, if $\Phi$ denotes all the parameters of the LLM implementing the policy, then the meta-controller selects the macro-action $\hat{z}$ given the context $c_t$, which includes the user's initial goal $g$, user feedback $o_t$, the history of user-system interactions $H$, and previous macro-actions the meta-controller has taken $\hat{z}_0, \ldots, \hat{z}_{t-1}$: 
%\begin{equation}
$\text{Meta-Controller: ~~}\pi(\hat{z} | c_t; \Phi), \text{ where context } c_t = \text{Concat}(o_t, \mathcal{H}, \hat{z}_0, \ldots, \hat{z}_{t-1}, g).\\
$%\end{equation}

%\subsection{Language-based sub-policies: Generating the added/altered steps or question}
\noindent \textbf{Language-based sub-policies: Generating the added/altered steps or question}
\label{subsec:low-level-policy}
Once the meta-controller selects the macro-action $\hat{z}$, the corresponding language-based sub-policy comes into play. The sub-policies are implemented also via (potentially the same) LLM, but CoT-prompted with different examples. We have three different language sub-policies, one per discrete action $z$ of the meta-controller: $\pi_{\texttt{add-steps}}(\cdot | ; \Omega_1)$, $\pi_{\texttt{alter-steps}} (\cdot | ; \Omega_2)$ and $\pi_{\texttt{ask-question}} (\cdot | ; \Omega_3)$, each with parameters $\Omega_1, \Omega_2$, and $\Omega_3$, respectively. The $\pi_{\texttt{add-steps}}$ generates sets of actions in the structured language space. After instantiating the specific schema each plan step should follow (name, description, follow-up per step, search keywords used by tool-use policies), the LLM-based policy for adding steps generates a tentative plan of new steps along with the reasoning/thought behind it, as well as a summarization of what it has learned about the user so far. Similarly, the $\pi_{\texttt{alter-steps}}$, CoT-prompted with different examples illustrating how a step should be altered, maps the existing plan along with the name of the step to be altered as generated by the argument of the meta-controller, to an altered step, again living in the same structured language schema described above. The $\pi_{\texttt{ask-question}}$ policy maps the meta-controller macro-action to the specific question to ask the user. Note that when the sub-policy asks a question, the plan remains unchanged. These questions are different from the questions the $\pi_\texttt{add-steps}$ and $\pi_{\texttt{alter-step}}$ generate per plan step, which the system collects across the steps of the plan in order to enable the user to choose one as a way to continue the planning process.\\

\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.14]{images/inventors_flow.png}
    \caption{Demonstration of "explaining to my kids about inventors".}
    \label{fig:inventors_flow}
\end{figure*}
%\subsection{Language-based Low-Level Policies: Connecting the user with content per plan step}
\noindent \textbf{Language-based Low-Level Policies: Connecting the user with content per plan step}
\label{subsec:tool-use}
Framing recommendation as an RL problem, the action the system takes at every step is the content item $c$ to show to the user \cite{chen2019top}. This becomes the primitive action space of our language-based hierarchical agent. The language-based low-level policies decide what content to show to the user per plan step via a combination of \emph{tool use} and \emph{CoT-prompted LLMs}. Specifically, for each step in the updated plan, after the execution of the \texttt{add-steps} or \texttt{alter-step} policy, the LLM can decide which tool to call (e.g., \texttt{SEARCH}, \texttt{RECOMMEND-ENGINE}) and fetch $n$ content items per step. Then, given these fetched content items, the same, but different CoT-prompted LLM can decide which of those items should be in the top-$k$ that are shown to the user. In other words, we approximate the retrieval-ranking two-stage recommendation approach \cite{covington2016deep} by tool use and CoT prompting, respectively.\\
%\subsection{Interactive Planning based on User's Natural Language Feedback}

\noindent \textbf{Interactive Planning based on User's Natural Language Feedback}
Using LLMs as the meta-controller, the sub-policies, and the low-level policies, the AI agent can create or update a plan, and ask a question to assist with a user's goal $g \in \mathcal{G} \subset \mathcal{L}$ expressed in natural language. As the user's journey towards this goal evolves over time, the user can provide natural language feedback $o$ which is then used to update both the meta-controller in terms of the next macro-action to take, the sub-policies in terms of the specific steps to add / alter or specific questions to ask, and the specific content items to include per plan step. The policies are updated by including user feedback in the context window, as is done with natural language-based RL agents \cite{shinn2024reflexion, zhao2024empowering}. This mechanism allows for the plan to be interactively adapted and personalized to different users.