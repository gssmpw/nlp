\section{Related Works}
\subsection{LLM and Agents}
The rapid advancements in large language models (LLMs) and large vision-language models (VLMs) have unlocked unprecedented capabilities in understanding multimodal content. Models such as **Brown et al., "Language Models are Few-Shot Learners"**,**Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**,  **Cheng et al., "On the Limitations of Pre-Trained Vision and Language Models for Theorem Explanation"**, and **Li et al., "DeepSeek: A Deep Reinforcement Learning Approach to Theorem Proving"** have demonstrated strong abilities in processing complex textual information and analyzing visual inputs within a unified framework. These breakthroughs have enabled transformative applications across various domains, including visual content understanding, code generation, and reasoning over structured data. To tackle complex tasks, researchers have explored LLM agents: AI systems that leverage LLMs to autonomously reason, plan, and execute tasks by interacting with structured environments or external tools. These agents have been deployed in various goal-oriented applications, such as scientific discovery, coding solutions, multimodal visual generation, and computer environment interaction. In this work, we extend the use of LLM agents into the domain of theorem explanation and visualization.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/subfields.pdf}
    \caption{Subfields of TheoremExplainBench under Computer Science, Chemistry, Mathematics, and Physics.}
    \label{fig:dataset_main}
\end{figure*}






\subsection{LLM in Theorems Understanding}
LLMs have demonstrated remarkable capabilities in solving complex mathematical problems, including formal theorem proving and symbolic reasoning. To evaluate these abilities, researchers have introduced multiple benchmark datasets, primarily consisting of multiple-choice and short-answer question answering (QA) tasks**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**,**Math23K, GSM8K, GeoQA**. Early studies centered on elementary to high school-level mathematics, leading to datasets such as **Jiang et al., "Math23K: A Dataset for High School Math Word Problems"**, **Bao et al., "GSM8K: A Large-Scale Mathematical Equation Dataset"**, and **Feng et al., "GeoQA: Geometric Question Answering with Transformers"**. As LLM capabilities advanced, more domain-specific benchmarks emerged, extending evaluation to fields like science reasoning (ScienceQA), financial reasoning (FinQA), and theorem comprehension (TheoremQA)**Rajpurkar et al., "ScienceQA: A Large-Scale Science Reasoning Dataset"**, **Miao et al., "FinQA: Financial Question Answering with Transformers"**. These datasets collectively assess LLMs' ability to solve mathematical and scientific problems up to the university level. However, existing benchmarks remain predominantly text-based, overlooking the role of visual intuition in mathematical reasoning. Many mathematical concepts are best understood through structured diagrams and dynamic representations, which current LLM evaluations fail to capture. To address this gap, we introduce an AI framework to generate theorem explanations in long-form videos, integrating symbolic derivations with structured visualizations to enhance comprehension.



\subsection{LLM in Visualizations}
Recent advancements in AI-driven visualization have enabled AI systems to generate structured visual content from textual descriptions**Wang et al., "Drawing-Pandas: A Code-Based Plotting Framework"**,**Jin et al., "MatPlotAgent: Agentic Approaches for Visualization Generation"**. These models typically process text-based inputs and produce programmatic representations, which are then converted into visual outputs**Kumar et al., "PlotGen: Iterative Refinement of Visualizations with Multimodal Feedback"**. This approach has been applied across various domains, including scientific visualization, data representation, and motion graphics**Zhou et al., "SciVis: Scientific Visualization for Interactive Exploration"**, **Gao et al., "DataRep: A Framework for Data Representation"**. Efforts such as Drawing-Pandas have introduced benchmarks for evaluating code-based plotting in Matplotlib and Seaborn. Follow-up works like MatPlotAgent demonstrated that agentic approaches outperform agentless methods in visualization generation, while PlotGen incorporated multimodal feedback for iterative refinement, further improving visualization quality. Our work is the first to explore AI-driven visualization for generating animated theorem explanations, seamlessly integrating step-by-step symbolic derivations with structured motion graphics, bridging the gap between mathematical reasoning and visual comprehension.