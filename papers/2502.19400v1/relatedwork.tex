\section{Related Works}
\subsection{LLM and Agents}
The rapid advancements in large language models (LLMs) and large vision-language models (VLMs) have unlocked unprecedented capabilities in understanding multimodal content. Models such as GPT-4~\cite{openai2023gpt4}, Gemini~\cite{geminiteam2024gemini}, Claude-3.5 Sonnet v1~\cite{anthropic2025claude3}, and DeepSeek~\cite{deepseekai2024deepseekv3technicalreport} have demonstrated strong abilities in processing complex textual information and analyzing visual inputs within a unified framework~\cite{zhang2023multicot}. These breakthroughs have enabled transformative applications across various domains, including visual content understanding~\cite{hu2023tifa, ku2023viescore}, code generation~\cite{nijkamp2023codegenopenlargelanguage, jimenez2024swebench, yang2024swebenchmultimodalaisystems}, and reasoning over structured data. To tackle complex tasks, researchers have explored LLM agents: AI systems that leverage LLMs to autonomously reason, plan, and execute tasks by interacting with structured environments or external tools. These agents have been deployed in various goal-oriented applications, such as scientific discovery~\cite{lu2024aiscientist, si2024llmsgeneratenovelresearch, schmidgall2025agentlaboratoryusingllm}, coding solutions~\cite{abramovich2024enigmaenhancedinteractivegenerative}, multimodal visual generation~\cite{he2024kubrickmultimodalagentcollaborations}, and computer environment interaction~\cite{OSWorld}. In this work, we extend the use of LLM agents into the domain of theorem explanation and visualization.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/subfields.pdf}
    \caption{Subfields of TheoremExplainBench under Computer Science, Chemistry, Mathematics, and Physics.}
    \label{fig:dataset_main}
\end{figure*}






\subsection{LLM in Theorems Understanding}
LLMs have demonstrated remarkable capabilities in solving complex mathematical problems, including formal theorem proving and symbolic reasoning. To evaluate these abilities, researchers have introduced multiple benchmark datasets, primarily consisting of multiple-choice and short-answer question answering (QA) tasks~\cite{zhang2024multiplechoicequestionsefficientrobust, amini2019mathqainterpretablemathword, hendrycksmath2021}. Early studies centered on elementary to high school-level mathematics, leading to datasets such as Math23K~\cite{zhou2023learninganalogydiversequestions}, GSM8K~\cite{cobbe2021trainingverifierssolvemath}, and GeoQA~\cite{chen2022geoqageometricquestionanswering}. As LLM capabilities advanced, more domain-specific benchmarks emerged, extending evaluation to fields like science reasoning (ScienceQA)~\cite{lu2022learn}, financial reasoning (FinQA)~\cite{chen2022finqadatasetnumericalreasoning}, and theorem comprehension (TheoremQA)~\cite{chen2023theoremqa}. These datasets collectively assess LLMs' ability to solve mathematical and scientific problems up to the university level. However, existing benchmarks remain predominantly text-based, overlooking the role of visual intuition in mathematical reasoning. Many mathematical concepts are best understood through structured diagrams and dynamic representations, which current LLM evaluations fail to capture. To address this gap, we introduce an AI framework to generate theorem explanations in long-form videos, integrating symbolic derivations with structured visualizations to enhance comprehension.



\subsection{LLM in Visualizations}
Recent advancements in AI-driven visualization have enabled AI systems to generate structured visual content from textual descriptions~\cite{li2024visualizationgenerationlargelanguage}. These models typically process text-based inputs and produce programmatic representations, which are then converted into visual outputs~\cite{ritchie2023neurosymbolicmodelscomputergraphics, goswami2025plotgenmultiagentllmbasedscientific}. This approach has been applied across various domains, including scientific visualization~\cite{yang2024matplotagent}, data representation~\cite{galimzyanov2024drawingpandasbenchmarkllms}, and motion graphics~\cite{zhang2023motiongraphics}. Efforts such as Drawing-Pandas~\cite{galimzyanov2024drawingpandasbenchmarkllms} have introduced benchmarks for evaluating code-based plotting in Matplotlib and Seaborn. Follow-up works like MatPlotAgent\cite{yang2024matplotagent} demonstrated that agentic approaches outperform agentless methods in visualization generation, while PlotGen~\cite{goswami2025plotgenmultiagentllmbasedscientific} incorporated multimodal feedback for iterative refinement, further improving visualization quality. Our work is the first to explore AI-driven visualization for generating animated theorem explanations, seamlessly integrating step-by-step symbolic derivations with structured motion graphics, bridging the gap between mathematical reasoning and visual comprehension.