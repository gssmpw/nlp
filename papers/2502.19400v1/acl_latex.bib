% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@misc{3b1bvideo,
  type         = {Video},
  author       = {Sanderson, {\relax G. [3Blue1Brown]}},
  year         = {2020},
  month        = {August},
  day          = {19},
  title        = {Group theory, abstraction, and the 196,883-dimensional monster},
  howpublished = {YouTube},
  url          = {https://www.youtube.com/watch?v=mH0oCDa74tE}
}

@software{The_Manim_Community_Developers_Manim_Mathematical_2024,
author = {{The Manim Community Developers}},
license = {MIT},
month = apr,
title = {{Manim – Mathematical Animation Framework}},
url = {https://www.manim.community/},
version = {v0.18.1},
year = {2024}
}

@article{hu2023tifa,
title={TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering},
author={Hu, Yushi and Liu, Benlin and Kasai, Jungo and Wang, Yizhong and Ostendorf, Mari and Krishna, Ranjay and Smith,
Noah A},
journal={arXiv preprint arXiv:2303.11897},
year={2023}
}

@misc{ku2023viescore,
                title={VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation}, 
                author={Max Ku and Dongfu Jiang and Cong Wei and Xiang Yue and Wenhu Chen},
                year={2023},
                eprint={2312.14867},
                archivePrefix={arXiv},
                primaryClass={cs.CV}
            }

@inproceedings{
    jimenez2024swebench,
    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},
    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=VTF8yNQM66}
}

@article{Zhang2023GPT4VisionAA,
  title={GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks},
  author={Xinlu Zhang and Yujie Lu and Weizhi Wang and An Yan and Jun Yan and Lianke Qin and Heng Wang and Xifeng Yan and William Yang Wang and Linda Ruth Petzold},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.01361},
  url={https://api.semanticscholar.org/CorpusID:264935635}
}

@misc{geminiteam2024gemini,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Gemini-Team and Machel Reid and Nikolay Savinov and Denis Teplyashin and Dmitry and Lepikhin and Timothy Lillicrap and Jean-baptiste Alayrac and Radu Soricut and Angeliki Lazaridou and Orhan Firat and Julian Schrittwieser and Ioannis Antonoglou and Rohan Anil and Sebastian Borgeaud and Andrew Dai and Katie Millican and Ethan Dyer and Mia Glaese and Thibault Sottiaux and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and James Molloy and Jilin Chen and Michael Isard and Paul Barham and Tom Hennigan and Ross McIlroy and Melvin Johnson and Johan Schalkwyk and Eli Collins and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and Clemens Meyer and Gregory Thornton and Zhen Yang and Henryk Michalewski and Zaheer Abbas and Nathan Schucher and Ankesh Anand and Richard Ives and James Keeling and Karel Lenc and Salem Haykal and Siamak Shakeri and Pranav Shyam and Aakanksha Chowdhery and Roman Ring and Stephen Spencer and Eren Sezener and Luke Vilnis and Oscar Chang and Nobuyuki Morioka and George Tucker and Ce Zheng and Oliver Woodman and Nithya Attaluri and Tomas Kocisky and Evgenii Eltyshev and Xi Chen and Timothy Chung and Vittorio Selo and Siddhartha Brahma and Petko Georgiev and Ambrose Slone and Zhenkai Zhu and James Lottes and Siyuan Qiao and Ben Caine and Sebastian Riedel and Alex Tomala and Martin Chadwick and Juliette Love and Peter Choy and Sid Mittal and Neil Houlsby and Yunhao Tang and Matthew Lamm and Libin Bai and Qiao Zhang and Luheng He and Yong Cheng and Peter Humphreys and Yujia Li and Sergey Brin and Albin Cassirer and Yingjie Miao and Lukas Zilka and Taylor Tobin and Kelvin Xu and Lev Proleev and Daniel Sohn and Alberto Magni and Lisa Anne Hendricks and Isabel Gao and Santiago Ontanon and Oskar Bunyan and Nathan Byrd and Abhanshu Sharma and Biao Zhang and Mario Pinto and Rishika Sinha and Harsh Mehta and Dawei Jia and Sergi Caelles and Albert Webson and Alex Morris and Becca Roelofs and Yifan Ding and Robin Strudel and Xuehan Xiong and Marvin Ritter and Mostafa Dehghani and Rahma Chaabouni and Abhijit Karmarkar and Guangda Lai and Fabian Mentzer and Bibo Xu and YaGuang Li and Yujing Zhang and Tom Le Paine and Alex Goldin and Behnam Neyshabur and Kate Baumli and Anselm Levskaya and Michael Laskin and Wenhao Jia and Jack W. Rae and Kefan Xiao and Antoine He and Skye Giordano and Lakshman Yagati and Jean-Baptiste Lespiau and Paul Natsev and Sanjay Ganapathy and Fangyu Liu and Danilo Martins and Nanxin Chen and Yunhan Xu and Megan Barnes and Rhys May and Arpi Vezer and Junhyuk Oh and Ken Franko and Sophie Bridgers and Ruizhe Zhao and Boxi Wu and Basil Mustafa and Sean Sechrist and Emilio Parisotto and Thanumalayan Sankaranarayana Pillai and Chris Larkin and Chenjie Gu and Christina Sorokin and Maxim Krikun and Alexey Guseynov and Jessica Landon and Romina Datta and Alexander Pritzel and Phoebe Thacker and Fan Yang and Kevin Hui and Anja Hauth and Chih-Kuan Yeh and David Barker and Justin Mao-Jones and Sophia Austin and Hannah Sheahan and Parker Schuh and James Svensson and Rohan Jain and Vinay Ramasesh and Anton Briukhov and Da-Woon Chung and Tamara von Glehn and Christina Butterfield and Priya Jhakra and Matthew Wiethoff and Justin Frye and Jordan Grimstad and Beer Changpinyo and Charline Le Lan and Anna Bortsova and Yonghui Wu and Paul Voigtlaender and Tara Sainath and Shane Gu and Charlotte Smith and Will Hawkins and Kris Cao and James Besley and Srivatsan Srinivasan and Mark Omernick and Colin Gaffney and Gabriela Surita and Ryan Burnell and Bogdan Damoc and Junwhan Ahn and Andrew Brock and Mantas Pajarskas and Anastasia Petrushkina and Seb Noury and Lorenzo Blanco and Kevin Swersky and Arun Ahuja and Thi Avrahami and Vedant Misra and Raoul de Liedekerke and Mariko Iinuma and Alex Polozov and Sarah York and George van den Driessche and Paul Michel and Justin Chiu and Rory Blevins and Zach Gleicher and Adrià Recasens and Alban Rrustemi and Elena Gribovskaya and Aurko Roy and Wiktor Gworek and Sébastien M. R. Arnold and Lisa Lee and James Lee-Thorp and Marcello Maggioni and Enrique Piqueras and Kartikeya Badola and Sharad Vikram and Lucas Gonzalez and Anirudh Baddepudi and Evan Senter and Jacob Devlin and James Qin and Michael Azzam and Maja Trebacz and Martin Polacek and Kashyap Krishnakumar and Shuo-yiin Chang and Matthew Tung and Ivo Penchev and Rishabh Joshi and Kate Olszewska and Carrie Muir and Mateo Wirth and Ale Jakse Hartman and Josh Newlan and Sheleem Kashem and Vijay Bolina and Elahe Dabir and Joost van Amersfoort and Zafarali Ahmed and James Cobon-Kerr and Aishwarya Kamath and Arnar Mar Hrafnkelsson and Le Hou and Ian Mackinnon and Alexandre Frechette and Eric Noland and Xiance Si and Emanuel Taropa and Dong Li and Phil Crone and Anmol Gulati and Sébastien Cevey and Jonas Adler and Ada Ma and David Silver and Simon Tokumine and Richard Powell and Stephan Lee and Kiran Vodrahalli and Samer Hassan and Diana Mincu and Antoine Yang and Nir Levine and Jenny Brennan and Mingqiu Wang and Sarah Hodkinson and Jeffrey Zhao and Josh Lipschultz and Aedan Pope and Michael B. Chang and Cheng Li and Laurent El Shafey and Michela Paganini and Sholto Douglas and Bernd Bohnet and Fabio Pardo and Seth Odoom and Mihaela Rosca and Cicero Nogueira dos Santos and Kedar Soparkar and Arthur Guez and Tom Hudson and Steven Hansen and Chulayuth Asawaroengchai and Ravi Addanki and Tianhe Yu and Wojciech Stokowiec and Mina Khan and Justin Gilmer and Jaehoon Lee and Carrie Grimes Bostock and Keran Rong and Jonathan Caton and Pedram Pejman and Filip Pavetic and Geoff Brown and Vivek Sharma and Mario Lučić and Rajkumar Samuel and Josip Djolonga and Amol Mandhane and Lars Lowe Sjösund and Elena Buchatskaya and Elspeth White and Natalie Clay and Jiepu Jiang and Hyeontaek Lim and Ross Hemsley and Zeyncep Cankara and Jane Labanowski and Nicola De Cao and David Steiner and Sayed Hadi Hashemi and Jacob Austin and Anita Gergely and Tim Blyth and Joe Stanton and Kaushik Shivakumar and Aditya Siddhant and Anders Andreassen and Carlos Araya and Nikhil Sethi and Rakesh Shivanna and Steven Hand and Ankur Bapna and Ali Khodaei and Antoine Miech and Garrett Tanzer and Andy Swing and Shantanu Thakoor and Lora Aroyo and Zhufeng Pan and Zachary Nado and Jakub Sygnowski and Stephanie Winkler and Dian Yu and Mohammad Saleh and Loren Maggiore and Yamini Bansal and Xavier Garcia and Mehran Kazemi and Piyush Patil and Ishita Dasgupta and Iain Barr and Minh Giang and Thais Kagohara and Ivo Danihelka and Amit Marathe and Vladimir Feinberg and Mohamed Elhawaty and Nimesh Ghelani and Dan Horgan and Helen Miller and Lexi Walker and Richard Tanburn and Mukarram Tariq and Disha Shrivastava and Fei Xia and Qingze Wang and Chung-Cheng Chiu and Zoe Ashwood and Khuslen Baatarsukh and Sina Samangooei and Raphaël Lopez Kaufman and Fred Alcober and Axel Stjerngren and Paul Komarek and Katerina Tsihlas and Anudhyan Boral and Ramona Comanescu and Jeremy Chen and Ruibo Liu and Chris Welty and Dawn Bloxwich and Charlie Chen and Yanhua Sun and Fangxiaoyu Feng and Matthew Mauger and Xerxes Dotiwalla and Vincent Hellendoorn and Michael Sharman and Ivy Zheng and Krishna Haridasan and Gabe Barth-Maron and Craig Swanson and Dominika Rogozińska and Alek Andreev and Paul Kishan Rubenstein and Ruoxin Sang and Dan Hurt and Gamaleldin Elsayed and Renshen Wang and Dave Lacey and Anastasija Ilić and Yao Zhao and Adam Iwanicki and Alejandro Lince and Alexander Chen and Christina Lyu and Carl Lebsack and Jordan Griffith and Meenu Gaba and Paramjit Sandhu and Phil Chen and Anna Koop and Ravi Rajwar and Soheil Hassas Yeganeh and Solomon Chang and Rui Zhu and Soroush Radpour and Elnaz Davoodi and Ving Ian Lei and Yang Xu and Daniel Toyama and Constant Segal and Martin Wicke and Hanzhao Lin and Anna Bulanova and Adrià Puigdomènech Badia and Nemanja Rakićević and Pablo Sprechmann and Angelos Filos and Shaobo Hou and Víctor Campos and Nora Kassner and Devendra Sachan and Meire Fortunato and Chimezie Iwuanyanwu and Vitaly Nikolaev and Balaji Lakshminarayanan and Sadegh Jazayeri and Mani Varadarajan and Chetan Tekur and Doug Fritz and Misha Khalman and David Reitter and Kingshuk Dasgupta and Shourya Sarcar and Tina Ornduff and Javier Snaider and Fantine Huot and Johnson Jia and Rupert Kemp and Nejc Trdin and Anitha Vijayakumar and Lucy Kim and Christof Angermueller and Li Lao and Tianqi Liu and Haibin Zhang and David Engel and Somer Greene and Anaïs White and Jessica Austin and Lilly Taylor and Shereen Ashraf and Dangyi Liu and Maria Georgaki and Irene Cai and Yana Kulizhskaya and Sonam Goenka and Brennan Saeta and Ying Xu and Christian Frank and Dario de Cesare and Brona Robenek and Harry Richardson and Mahmoud Alnahlawi and Christopher Yew and Priya Ponnapalli and Marco Tagliasacchi and Alex Korchemniy and Yelin Kim and Dinghua Li and Bill Rosgen and Kyle Levin and Jeremy Wiesner and Praseem Banzal and Praveen Srinivasan and Hongkun Yu and Çağlar Ünlü and David Reid and Zora Tung and Daniel Finchelstein and Ravin Kumar and Andre Elisseeff and Jin Huang and Ming Zhang and Ricardo Aguilar and Mai Giménez and Jiawei Xia and Olivier Dousse and Willi Gierke and Damion Yates and Komal Jalan and Lu Li and Eri Latorre-Chimoto and Duc Dung Nguyen and Ken Durden and Praveen Kallakuri and Yaxin Liu and Matthew Johnson and Tomy Tsai and Alice Talbert and Jasmine Liu and Alexander Neitz and Chen Elkind and Marco Selvi and Mimi Jasarevic and Livio Baldini Soares and Albert Cui and Pidong Wang and Alek Wenjiao Wang and Xinyu Ye and Krystal Kallarackal and Lucia Loher and Hoi Lam and Josef Broder and Dan Holtmann-Rice and Nina Martin and Bramandia Ramadhana and Mrinal Shukla and Sujoy Basu and Abhi Mohan and Nick Fernando and Noah Fiedel and Kim Paterson and Hui Li and Ankush Garg and Jane Park and DongHyun Choi and Diane Wu and Sankalp Singh and Zhishuai Zhang and Amir Globerson and Lily Yu and John Carpenter and Félix de Chaumont Quitry and Carey Radebaugh and Chu-Cheng Lin and Alex Tudor and Prakash Shroff and Drew Garmon and Dayou Du and Neera Vats and Han Lu and Shariq Iqbal and Alex Yakubovich and Nilesh Tripuraneni and James Manyika and Haroon Qureshi and Nan Hua and Christel Ngani and Maria Abi Raad and Hannah Forbes and Jeff Stanway and Mukund Sundararajan and Victor Ungureanu and Colton Bishop and Yunjie Li and Balaji Venkatraman and Bo Li and Chloe Thornton and Salvatore Scellato and Nishesh Gupta and Yicheng Wang and Ian Tenney and Xihui Wu and Ashish Shenoy and Gabriel Carvajal and Diana Gage Wright and Ben Bariach and Zhuyun Xiao and Peter Hawkins and Sid Dalmia and Clement Farabet and Pedro Valenzuela and Quan Yuan and Ananth Agarwal and Mia Chen and Wooyeol Kim and Brice Hulse and Nandita Dukkipati and Adam Paszke and Andrew Bolt and Kiam Choo and Jennifer Beattie and Jennifer Prendki and Harsha Vashisht and Rebeca Santamaria-Fernandez and Luis C. Cobo and Jarek Wilkiewicz and David Madras and Ali Elqursh and Grant Uy and Kevin Ramirez and Matt Harvey and Tyler Liechty and Heiga Zen and Jeff Seibert and Clara Huiyi Hu and Andrey Khorlin and Maigo Le and Asaf Aharoni and Megan Li and Lily Wang and Sandeep Kumar and Norman Casagrande and Jay Hoover and Dalia El Badawy and David Soergel and Denis Vnukov and Matt Miecnikowski and Jiri Simsa and Praveen Kumar and Thibault Sellam and Daniel Vlasic and Samira Daruki and Nir Shabat and John Zhang and Guolong Su and Jiageng Zhang and Jeremiah Liu and Yi Sun and Evan Palmer and Alireza Ghaffarkhah and Xi Xiong and Victor Cotruta and Michael Fink and Lucas Dixon and Ashwin Sreevatsa and Adrian Goedeckemeyer and Alek Dimitriev and Mohsen Jafari and Remi Crocker and Nicholas FitzGerald and Aviral Kumar and Sanjay Ghemawat and Ivan Philips and Frederick Liu and Yannie Liang and Rachel Sterneck and Alena Repina and Marcus Wu and Laura Knight and Marin Georgiev and Hyo Lee and Harry Askham and Abhishek Chakladar and Annie Louis and Carl Crous and Hardie Cate and Dessie Petrova and Michael Quinn and Denese Owusu-Afriyie and Achintya Singhal and Nan Wei and Solomon Kim and Damien Vincent and Milad Nasr and Christopher A. Choquette-Choo and Reiko Tojo and Shawn Lu and Diego de Las Casas and Yuchung Cheng and Tolga Bolukbasi and Katherine Lee and Saaber Fatehi and Rajagopal Ananthanarayanan and Miteyan Patel and Charbel Kaed and Jing Li and Shreyas Rammohan Belle and Zhe Chen and Jaclyn Konzelmann and Siim Põder and Roopal Garg and Vinod Koverkathu and Adam Brown and Chris Dyer and Rosanne Liu and Azade Nova and Jun Xu and Alanna Walton and Alicia Parrish and Mark Epstein and Sara McCarthy and Slav Petrov and Demis Hassabis and Koray Kavukcuoglu and Jeffrey Dean and Oriol Vinyals},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}

@article{Yin2023ASO,
  title={A Survey on Multimodal Large Language Models},
  author={Shukang Yin and Chaoyou Fu and Sirui Zhao and Ke Li and Xing Sun and Tong Xu and Enhong Chen},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.13549},
  url={https://api.semanticscholar.org/CorpusID:259243718}
}

@article{Touvron2023LLaMAOA,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13971},
  url={https://api.semanticscholar.org/CorpusID:257219404}
}

@misc{yang2023dawn,
      title={The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)}, 
      author={Zhengyuan Yang and Linjie Li and Kevin Lin and Jianfeng Wang and Chung-Ching Lin and Zicheng Liu and Lijuan Wang},
      year={2023},
      eprint={2309.17421},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{chen2023theoremqa,
  title={Theoremqa: A theorem-driven question answering dataset},
  author={Chen, Wenhu and Yin, Ming and Ku, Max and Lu, Pan and Wan, Yixin and Ma, Xueguang and Xu, Jianyu and Wang, Xinyi and Xia, Tony},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@article{lu2024aiscientist,
  title={The {AI} {S}cientist: Towards Fully Automated Open-Ended Scientific Discovery},
  author={Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
  journal={arXiv preprint arXiv:2408.06292},
  year={2024}
}

@misc{schmidgall2025agentlaboratoryusingllm,
      title={Agent Laboratory: Using LLM Agents as Research Assistants}, 
      author={Samuel Schmidgall and Yusheng Su and Ze Wang and Ximeng Sun and Jialian Wu and Xiaodong Yu and Jiang Liu and Zicheng Liu and Emad Barsoum},
      year={2025},
      eprint={2501.04227},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2501.04227},
}

@misc{si2024llmsgeneratenovelresearch,
      title={Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers}, 
      author={Chenglei Si and Diyi Yang and Tatsunori Hashimoto},
      year={2024},
      eprint={2409.04109},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.04109}, 
}

@misc{he2024kubrickmultimodalagentcollaborations,
      title={Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation}, 
      author={Liu He and Yizhi Song and Hejun Huang and Daniel Aliaga and Xin Zhou},
      year={2024},
      eprint={2408.10453},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.10453}, 
}

@misc{OSWorld,
      title={OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments},
      author={Tianbao Xie and Danyang Zhang and Jixuan Chen and Xiaochuan Li and Siheng Zhao and Ruisheng Cao and Toh Jing Hua and Zhoujun Cheng and Dongchan Shin and Fangyu Lei and Yitao Liu and Yiheng Xu and Shuyan Zhou and Silvio Savarese and Caiming Xiong and Victor Zhong and Tao Yu},
      year={2024},
      eprint={2404.07972},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{abramovich2024enigmaenhancedinteractivegenerative,
      title={EnIGMA: Enhanced Interactive Generative Model Agent for CTF Challenges},
      author={Talor Abramovich and Meet Udeshi and Minghao Shao and Kilian Lieret and Haoran Xi and Kimberly Milner and Sofija Jancheska and John Yang and Carlos E. Jimenez and Farshad Khorrami and Prashanth Krishnamurthy and Brendan Dolan-Gavitt and Muhammad Shafique and Karthik Narasimhan and Ramesh Karri and Ofir Press},
      year={2024},
      eprint={2409.16165},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.16165},
}

@misc{nijkamp2023codegenopenlargelanguage,
      title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis}, 
      author={Erik Nijkamp and Bo Pang and Hiroaki Hayashi and Lifu Tu and Huan Wang and Yingbo Zhou and Silvio Savarese and Caiming Xiong},
      year={2023},
      eprint={2203.13474},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.13474}, 
}

@misc{helbling2023manimmlcommunicatingmachinelearning,
      title={ManimML: Communicating Machine Learning Architectures with Animation}, 
      author={Alec Helbling and Duen Horng Chau},
      year={2023},
      eprint={2306.17108},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.17108}, 
}

@article{liu2024cmm,
  title={CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models},
  author={Liu, Wentao and Pan, Qianjun and Zhang, Yi and Liu, Zhuo and Wu, Ji and Zhou, Jie and Zhou, Aimin and Chen, Qin and Jiang, Bo and He, Liang},
  journal={arXiv preprint arXiv:2409.02834},
  year={2024}
}

@misc{zhu2025vcevalrethinkinggoodeducational,
      title={VCEval: Rethinking What is a Good Educational Video and How to Automatically Evaluate It}, 
      author={Xiaoxuan Zhu and Zhouhong Gu and Sihang Jiang and Zhixu Li and Hongwei Feng and Yanghua Xiao},
      year={2025},
      eprint={2407.12005},
      archivePrefix={arXiv},
      primaryClass={cs.MM},
      url={https://arxiv.org/abs/2407.12005}, 
}

@misc{leiker2023generativeailearninginvestigating,
      title={Generative AI for learning: Investigating the potential of synthetic learning videos}, 
      author={Daniel Leiker and Ashley Ricker Gyllen and Ismail Eldesouky and Mutlu Cukurova},
      year={2023},
      eprint={2304.03784},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.03784}, 
}

@misc{dan2023educhatlargescalelanguagemodelbased,
      title={EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education}, 
      author={Yuhao Dan and Zhikai Lei and Yiyang Gu and Yong Li and Jianghao Yin and Jiaju Lin and Linhao Ye and Zhiyan Tie and Yougen Zhou and Yilei Wang and Aimin Zhou and Ze Zhou and Qin Chen and Jie Zhou and Liang He and Xipeng Qiu},
      year={2023},
      eprint={2308.02773},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.02773}, 
}

@article{baidoo2023education,
  title={Education in the era of generative artificial intelligence (AI): Understanding the potential benefits of ChatGPT in promoting teaching and learning},
  author={Baidoo-Anu, David and Ansah, Leticia Owusu},
  journal={Journal of AI},
  volume={7},
  number={1},
  pages={52--62},
  year={2023},
  publisher={{\.I}zmir Academy Association}
}

@inproceedings{yang2024sweagent,
  title={{SWE}-agent: Agent-Computer Interfaces Enable Automated Software Engineering},
  author={John Yang and Carlos E Jimenez and Alexander Wettig and Kilian Lieret and Shunyu Yao and Karthik R Narasimhan and Ofir Press},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://arxiv.org/abs/2405.15793}
}

@misc{ghanem2022questiongenerationreadingcomprehension,
      title={Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask}, 
      author={Bilal Ghanem and Lauren Lutz Coleman and Julia Rivard Dexter and Spencer McIntosh von der Ohe and Alona Fyshe},
      year={2022},
      eprint={2204.02908},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.02908}, 
}

@misc{zhang2024simulatingclassroomeducationllmempowered,
      title={Simulating Classroom Education with LLM-Empowered Agents}, 
      author={Zheyuan Zhang and Daniel Zhang-Li and Jifan Yu and Linlu Gong and Jinchang Zhou and Zhanxin Hao and Jianxiao Jiang and Jie Cao and Huiqin Liu and Zhiyuan Liu and Lei Hou and Juanzi Li},
      year={2024},
      eprint={2406.19226},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.19226}, 
}

@book{1901aristotle,
  title={Aristotle's Posterior Analytics},
  author={Aristotle},
  url={https://books.google.ca/books?id=NCeXTBJVl_EC},
  year={1901},
  publisher={B.H. Blackwell}
}


@article{fu2024blink,
  title={BLINK: Multimodal Large Language Models Can See but Not Perceive},
  author={Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2404.12390},
  year={2024}
}

@misc{yang2024matplotagent,
      title={MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization}, 
      author={Zhiyu Yang and Zihan Zhou and Shuo Wang and Xin Cong and Xu Han and Yukun Yan and Zhenghao Liu and Zhixing Tan and Pengyuan Liu and Dong Yu and Zhiyuan Liu and Xiaodong Shi and Maosong Sun},
      year={2024},
      eprint={2402.11453},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vázquez2024llmsreadyvisualization,
      title={Are LLMs ready for Visualization?}, 
      author={Pere-Pau Vázquez},
      year={2024},
      eprint={2403.06158},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2403.06158}, 
}

@article{zhang2023motiongraphics,
author = {Zhang, Sharon and Ma, Jiaju and Wu, Jiajun and Ritchie, Daniel and Agrawala, Maneesh},
title = {Editing Motion Graphics Video via Motion Vectorization and Transformation},
year = {2023},
doi = {10.1145/3618316},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {229},
numpages = {13}
}

@misc{ritchie2023neurosymbolicmodelscomputergraphics,
      title={Neurosymbolic Models for Computer Graphics}, 
      author={Daniel Ritchie and Paul Guerrero and R. Kenny Jones and Niloy J. Mitra and Adriana Schulz and Karl D. D. Willis and Jiajun Wu},
      year={2023},
      eprint={2304.10320},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2304.10320}, 
}

@article{zhang2023multicot,
  title={Multimodal Chain-of-Thought Reasoning in Language Models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  journal={arXiv preprint arXiv:2302.00923},
  year={2023}
}

@misc{pezeshkpour2023largelanguagemodelssensitivity,
      title={Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions}, 
      author={Pouya Pezeshkpour and Estevam Hruschka},
      year={2023},
      eprint={2308.11483},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.11483}, 
}

@misc{keluskar2024llmsunderstandambiguitytext,
      title={Do LLMs Understand Ambiguity in Text? A Case Study in Open-world Question Answering}, 
      author={Aryan Keluskar and Amrita Bhattacharjee and Huan Liu},
      year={2024},
      eprint={2411.12395},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.12395}, 
}

@misc{yang2024swebenchmultimodalaisystems,
      title={SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?}, 
      author={John Yang and Carlos E. Jimenez and Alex L. Zhang and Kilian Lieret and Joyce Yang and Xindi Wu and Ori Press and Niklas Muennighoff and Gabriel Synnaeve and Karthik R. Narasimhan and Diyi Yang and Sida I. Wang and Ofir Press},
      year={2024},
      eprint={2410.03859},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.03859},
}

@misc{zhang2024multiplechoicequestionsefficientrobust,
      title={Multiple-Choice Questions are Efficient and Robust LLM Evaluators}, 
      author={Ziyin Zhang and Zhaokun Jiang and Lizhen Xu and Hongkun Hao and Rui Wang},
      year={2024},
      eprint={2405.11966},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.11966}, 
}

@inproceedings{lu2022learn,
    title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
    author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Ashwin Kalyan},
    booktitle={The 36th Conference on Neural Information Processing Systems (NeurIPS)},
    year={2022}
}

@misc{chen2022geoqageometricquestionanswering,
      title={GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning}, 
      author={Jiaqi Chen and Jianheng Tang and Jinghui Qin and Xiaodan Liang and Lingbo Liu and Eric P. Xing and Liang Lin},
      year={2022},
      eprint={2105.14517},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2105.14517}, 
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}

@misc{chen2023programthoughtspromptingdisentangling,
      title={Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks}, 
      author={Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},
      year={2023},
      eprint={2211.12588},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.12588}, 
}

@misc{openstax, 
author={Richard Baraniuk},
title= {OpenStax: Free Textbooks Online with No Catch},
url={https://openstax.org/}, 
year={2025}, journal={OpenStax}} 

@misc{libretexts, 
author={Delmar Larsen},
title = {LibreTexts: The Future is Open},
url={https://libretexts.org/}, 
year={2025}, journal={LibreTexts}} 

@misc{galimzyanov2024drawingpandasbenchmarkllms,
      title={Drawing Pandas: A Benchmark for LLMs in Generating Plotting Code}, 
      author={Timur Galimzyanov and Sergey Titov and Yaroslav Golubev and Egor Bogomolov},
      year={2024},
      eprint={2412.02764},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2412.02764}, 
}

@misc{goswami2025plotgenmultiagentllmbasedscientific,
      title={PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback}, 
      author={Kanika Goswami and Puneet Mathur and Ryan Rossi and Franck Dernoncourt},
      year={2025},
      eprint={2502.00988},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.00988}, 
}

@misc{li2024visualizationgenerationlargelanguage,
      title={Visualization Generation with Large Language Models: An Evaluation}, 
      author={Guozheng Li and Xinyu Wang and Gerile Aodeng and Shunyuan Zheng and Yu Zhang and Chuangxin Ou and Song Wang and Chi Harold Liu},
      year={2024},
      eprint={2401.11255},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2401.11255}, 
}

@misc{anthropic2025claude3,
  author       = {Anthropic},
  title        = {The Claude 3 model family: Opus, Sonnet, Haiku},
  year         = {2024},
  month        = {February},
  note         = {Accessed: 2025-02-11},
  howpublished = {Online},
  url          = {https://www.anthropic.com}
}


@misc{openai2025o3mini,
  author       = {OpenAI},
  title        = {OpenAI O3 Mini},
  year         = {2025},
  month        = {February},
  note         = {Accessed: 2025-02-11},
  howpublished = {Online},
  url          = {https://openai.com/index/openai-o3-mini/}
}

@misc{deepmind2025gemini2flash,
  author       = {DeepMind},
  title        = {Gemini 2.0 Flash},
  year         = {2025},
  month        = {February},
  note         = {Accessed: 2025-02-11},
  howpublished = {Online},
  url          = {https://deepmind.google/technologies/gemini/flash/}
}

@misc{tjandra2025metaaudioboxaestheticsunified,
      title={Meta Audiobox Aesthetics: Unified Automatic Quality Assessment for Speech, Music, and Sound}, 
      author={Andros Tjandra and Yi-Chiao Wu and Baishan Guo and John Hoffman and Brian Ellis and Apoorv Vyas and Bowen Shi and Sanyuan Chen and Matt Le and Nick Zacharov and Carleigh Wood and Ann Lee and Wei-Ning Hsu},
      year={2025},
      eprint={2502.05139},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2502.05139}, 
}


@inproceedings{zou-etal-2024-vgbench,
  title = "{VGB}ench: Evaluating Large Language Models on Vector Graphics Understanding and Generation",
  author = "Zou, Bocheng  and
    Cai, Mu  and
    Zhang, Jianrui  and
    Lee, Yong Jae",
  editor = "Al-Onaizan, Yaser  and
    Bansal, Mohit  and
    Chen, Yun-Nung",
  booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2024",
  address = "Miami, Florida, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.emnlp-main.213",
  pages = "3647--3659",
  abstract = "In the realm of vision models, the primary mode of representation is using pixels to rasterize the visual world. Yet this is not always the best or unique way to represent visual content, especially for designers and artists who depict the world using geometry primitives such as polygons. Vector graphics (VG), on the other hand, offer a textual representation of visual content, which can be more concise and powerful for content like cartoons, sketches and scientific figures. Recent studies have shown promising results on processing vector graphics with capable Large Language Models (LLMs). However, such works focus solely on qualitative results, understanding, or a specific type of vector graphics. We propose VGBench, a comprehensive benchmark for LLMs on handling vector graphics through diverse aspects, including (a) both visual understanding and generation, (b) evaluation of various vector graphics formats, (c) diverse question types, (d) wide range of prompting techniques, (e) under multiple LLMs and (f) comparison with VLMs on rasterized representations. Evaluating on our collected 4279 understanding and 5845 generation samples, we find that LLMs show strong capability on both aspects while exhibiting less desirable performance on low-level formats (SVG). Both data and evaluation pipeline will be open-sourced.",
}

@inproceedings{
deng2023markuptoimage,
title={Markup-to-Image Diffusion Models with Scheduled Sampling},
author={Yuntian Deng and Noriyuki Kojima and Alexander M Rush},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=81VJDmOE2ol}
}

@misc{zhou2023learninganalogydiversequestions,
      title={Learning by Analogy: Diverse Questions Generation in Math Word Problem}, 
      author={Zihao Zhou and Maizhen Ning and Qiufeng Wang and Jie Yao and Wei Wang and Xiaowei Huang and Kaizhu Huang},
      year={2023},
      eprint={2306.09064},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.09064}, 
}

@misc{cobbe2021trainingverifierssolvemath,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.14168}, 
}

@misc{amini2019mathqainterpretablemathword,
      title={MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms}, 
      author={Aida Amini and Saadia Gabriel and Peter Lin and Rik Koncel-Kedziorski and Yejin Choi and Hannaneh Hajishirzi},
      year={2019},
      eprint={1905.13319},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.13319}, 
}

@misc{chen2022finqadatasetnumericalreasoning,
      title={FinQA: A Dataset of Numerical Reasoning over Financial Data}, 
      author={Zhiyu Chen and Wenhu Chen and Charese Smiley and Sameena Shah and Iana Borova and Dylan Langdon and Reema Moussa and Matt Beane and Ting-Hao Huang and Bryan Routledge and William Yang Wang},
      year={2022},
      eprint={2109.00122},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.00122}, 
}

@InProceedings{huang2023vbench,
      title={{VBench}: Comprehensive Benchmark Suite for Video Generative Models},
      author={Huang, Ziqi and He, Yinan and Yu, Jiashuo and Zhang, Fan and Si, Chenyang and Jiang, Yuming and Zhang, Yuanhan and Wu, Tianxing and Jin, Qingyang and Chanpaisit, Nattapol and Wang, Yaohui and Chen, Xinyuan and Wang, Limin and Lin, Dahua and Qiao, Yu and Liu, Ziwei},
      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
      year={2024}
}

@misc{Krippendorff2011ComputingKA,
  title={Computing Krippendorff's Alpha-Reliability},
  author={Klaus Krippendorff},
  year={2011},
  url={https://api.semanticscholar.org/CorpusID:59901023}
}


@article{fleiss1973equivalence,
  title={The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability},
  author={Fleiss, Joseph L and Cohen, Jacob},
  journal={Educational and psychological measurement},
  volume={33},
  number={3},
  pages={613--619},
  year={1973},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@misc{AIManimVideoGenerator,
author = {Shah, Krish and Abey, Chris and Mujral, Hargun},
title = {3Brown1Blue: AI-Generated Educational Videos with Manim},
year = {2024},
publisher = {GitHub},
url = {https://github.com/KrishKrosh/manim-video-gen}
}

@misc{GatekeepAI,
  author    = {Gatekeep},
  title     = {Gatekeep AI: Start learning faster with personalized videos},
  year      = {2024},
  publisher = {Gatekeep AI},
  url       = {https://gatekeep.ai/}
}

@misc{GenerativeManim,
  author    = {GenerativeManim},
  title     = {Generative Manim: AI-Driven Animations for Mathematics and Education},
  year      = {2024},
  publisher = {Vercel},
  url       = {https://generative-manim.vercel.app/}
}

@misc{soman2024observationsbuildingragsystems,
      title={Observations on Building RAG Systems for Technical Documents}, 
      author={Sumit Soman and Sujoy Roychowdhury},
      year={2024},
      eprint={2404.00657},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.00657}, 
}