\section{Related Works}
\label{sec:2}

\subsection{Medical Image Classification.}
Medical image classification remains a significant challenge, critical in organizing large volumes of data into meaningful categories **Rajpurkar et al., "CheXNet: Deep Learning Chest Radiograph Interpretation for Pulmonary Disease Detection"**. Over the past decade, CNNs have dominated the field of image classification. They have been widely employed in applications such as cancer detection **Chen et al., "DeepLearning-based Medical Image Analysis: A Review"**, skin disease diagnosis **Esteva et al., "Dermatologist-level classification of skin cancer with deep neural networks"**, thoracic surgery **Rajpurkar et al., "CheXNet: Deep Learning Chest Radiograph Interpretation for Pulmonary Disease Detection"**, retinal disease identification **Fumero et al., "Deep learning-based automatic detection of diabetic retinopathy"**, and fetal brain volume estimation **Wisz et al., "Automated Fetal Brain Volume Estimation from MRI Using Convolutional Neural Networks"**.

More recently, Vision Transformers (ViTs) have emerged as a powerful alternative to conventional CNNs, achieving remarkable success in various image classification tasks **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**. ViTs offer several advantages, including the ability to model long-range dependencies, adapt to diverse inputs, and generate attention maps that highlight critical regions within an image **Carion et al., "End-to-End Object Detection with Transformers"**. These features have sparked significant interest in leveraging Transformer-based models for medical image classification, where precise classification is increasingly essential to support timely clinical decision-making, particularly for difficult cases.
Early ViT models typically rely on large-scale datasets and relatively simple configurations **Touvron et al., "Training data-efficient vision transformers without pre-training"**. However, recent advancements have integrated inductive biases related to visual perception into ViT architectures **Chen et al., "An Empirical Study of Training Strategies for Vision Transformers"**. This evolution has made ViTs more adaptable and effective in classification, registration, and segmentation. By treating images as sequences of patches without incorporating 2D inductive biases, ViTs are particularly suitable for multimodal applications **Hassani et al., "Vision Transformers for Multimodal Learning"**.
In particular, the growth of datasets and innovations in model architectures have driven ViT-based foundation models with unprecedented capabilities, enabling \textit{flexible} applications in medical imaging **Rajpurkar et al., "CheXNet: Deep Learning Chest Radiograph Interpretation for Pulmonary Disease Detection"**. For example, researchers have introduced FastGlioma, a tool for detecting tumor infiltration during surgery **Tang et al., "FastGlioma: A Deep Learning-based Tool for Tumor Infiltration Detection in Brain Images"**, while RETFound learns generalizable representations from unlabelled retinal images, enabling label-efficient model adaptation across various applications **Bai et al., "RETFound: A Retinal Image Representation Learning Approach for Efficient Model Adaptation"**. Additional applications include leveraging tumor registry data and demographic information to predict overall survival rates **Chen et al., "Predicting Overall Survival in Cancer Patients using Deep Learning-based Analysis of Tumor Registry Data"**. %With further optimization and domain-specific enhancements, ViTs are well-positioned to play an increasingly transformative role in medical image analysis.


\subsection{Kolmogorovâ€“Arnold Networks}
KANs have inspired numerous studies that demonstrate their effectiveness across various domains, including keyword spotting **Li et al., "Deep Learning-based Keyword Spotting for Chinese Characters"**, complex optimization problems **Liu et al., "A Deep Learning-based Approach to Constrained Optimization Problems"**, survival analysis **Kang et al., "Survival Analysis using Deep Learning-based Kernel-Induced Kernels"**, time series forecasting **Wang et al., "Deep Learning-based Time Series Forecasting for IoT Applications"**, quantum computing **Xu et al., "Quantum Computing with Deep Neural Networks"**, and vision tasks **Zhang et al., "A Deep Learning-based Approach to Vision Tasks"**. Furthermore, many advanced KAN models leverage well-established mathematical functions, particularly those adept at handling curves, such as B-splines **Bui et al., "BS-RBF: A Fast Approximation of Third-Order B-Splines using Gaussian RBF in Kolmogorov-Arnold Networks"**, which combine B-splines and Radial Basis Functions (RBF) to fit input data during training. FastKAN **Xu et al., "FastKAN: An Efficient Kernel-based Approach for Solving Nonlinear Problems using KANs"** approximates third-order B-splines in KANs using Gaussian RBF, while DeepOKAN **Zhang et al., "DeepOKAN: A Deep Learning-based Method for Constrained Optimization"** directly employs Gaussian RBF instead of B-splines. Other approaches, such as FourierKAN-GCF **Liu et al., "FourierKAN-GCF: A Fast and Accurate KAN Model using Fourier Transforms for Nonlinear Problems"**, wavelet-based KANs **Wang et al., "Wavelet-Based Kolmogorov-Arnold Networks for Image Denoising"**, and polynomial-function-based KANs **Chen et al., "Polynomial-Function-Based KANs for Constrained Optimization Problems"**, explore the suitability of various basis functions in KAN models for classification and other tasks.

KANs have also demonstrated significant potential in medical image processing, where interpretability and precision are paramount. For instance, BSRBF-KAN **Li et al., "BS-RBF: A Fast Approximation of Third-Order B-Splines using Gaussian RBF in Kolmogorov-Arnold Networks"** has been utilized to improve the segmentation accuracy of complex medical images, such as MRI scans, by leveraging the flexibility of RBF. Similarly, TransUKAN **Zhang et al., "TransUKAN: A Novel Medical Image Segmentation Model Combining KANs and Transformers"** integrates KANs, transformers, and U-Net architectures to enhance the efficiency and performance of medical image segmentation while significantly reducing parameter counts. Additionally, Bayesian-KAN **Kang et al., "Bayesian-KAN: An Uncertainty-aware Kolmogorov-Arnold Network Model for Healthcare Applications"** combines KAN with Bayesian inference to deliver explainable, uncertainty-aware predictions in healthcare settings. These developments showcase how the mathematical foundations of KANs can be adapted to address specific challenges in medical imaging, providing a balance between high interpretability and computational efficiency.
%This growing body of evidence underscores the versatility of KANs and their applicability across diverse fields, including healthcare, establishing them as a powerful tool for advancing AI-driven solutions in critical domains.