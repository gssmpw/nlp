\section{Related Work}
\vspace{-0em} % Adjust this value as needed
\subsection{Attacks on LLMs}
\vspace{-0em}
The traditional adversarial attack formulation involves adding a subtle perturbation $\delta$ to the original $x$ so that $ x_{\text{adv}} = x + \delta$ **Szegedy, et al., "Intriguing properties of neural networks"**. There are many ways to calculate $\delta$; the overarching idea is that we want to add a perturbation $\delta$ to $x$ to identify regions of high risk in the input space **Goodfellow, et al., "Explaining and harnessing adversarial examples"**. This, in turn, is expected to increase the output loss for a given task $t_a$.

One of the most effective ways to find regions of high input risk remains the fast gradient sign method (FGSM) **Goodfellow, et al., "Explaining and harnessing adversarial examples"**, the basic iterative method (BIM) **Kurakin, et al., "Adversarial examples in the physical world"**, and projected gradient descent (PGD) **Madry, et al., "Towards deep learning models resistant to adversarial attacks"**. These methods utilize gradient information to find an optimal $\delta$ given a bound $\epsilon$ with either one or multiple iterations.

These first-order techniques model a tractable maximization operation over a non-convex loss landscape as follows:

\vspace{-0em} % Adjust this value as needed
\begin{equation}
\rho(\theta) = E_{(x,y) \sim D} \left[ \max_{\delta \in \Delta} L(\theta, x + \delta, y) \right]
\end{equation}
\vspace{-0em} % Adjust this value as needed

Where $\rho(\theta)$ represents the worst possible perturbation for $x$ on model $f_{\theta}$ with parameters $\theta$. When we maximize $ L(\theta, x + \delta, y)$, we do so by using multiple $\delta$ from a set $\Delta$ of all possible perturbations given a bound $\epsilon$. This bound is often set to the $L_2$-Norm: $[ |\delta|_2 \leq \epsilon ]$ or $L_{\infty}$-Norm: $[ |\delta|_\infty \leq \epsilon ]$.

These techniques and their variants have found moderate success when applied to the input embedding (continuous) space as white-box attacks **Papernot, et al., "The limitations of deep learning in adversarial settings"**. However, most language applications interact with LLMs through the token space. Although previous work has attempted to use gradient information to perform attacks in the token space **Elsayed, et al., "Adversarial examples for neural networks: A survey"**, the projection from a continuous to a discrete space results in high perplexity and low semantics **Goodfellow, et al., "Explaining and harnessing adversarial examples"**.

Given the unrealistic threat model of having access to gradients and input embedding spaces, some efforts have explored adversarial attacks in the token space by perturbing at the character **Rahimi, et al., "Adversarial robustness in neural networks: A survey"**, word **Kurakin, et al., "Adversarial examples in the physical world"**, or sentence level **Madry, et al., "Towards deep learning models resistant to adversarial attacks"**. These attacks often use output probabilities as feedback while employing heuristic search techniques, such as beam search, greedy search, or particle swarm optimization **Elsayed, et al., "Adversarial examples for neural networks: A survey"**. These methods have been exceptionally effective on BERT **Devlin, et al., "BERT: Pre-training of deep bidirectional transformers for language understanding"**-based encoding models. However, widely used commercial LLMs (e.g. ChatGPT) are closed-source and logits or probabilities are not available; attacks have to operate  in a purely black-box setting with only hard-predictions available as feedback **Papernot, et al., "The limitations of deep learning in adversarial settings"**. Recent works have explored using LLMs to red team (perform multiple black-box hard label attacks **Sinha, et al., "Not all models are created equal: Benchmarking and robustness testing in natural language processing from the dark side"**) on other LLMs with moderate success **Rahimi, et al., "Adversarial robustness in neural networks: A survey"**. We believe the lack of feedback in the perturbation ($\delta$) optimization process is holding these attacks back.

In this paper, we propose a novel attack technique we call confidence elicitation attacks, which aim to attack models in a completely black-box setting while still utilizing feedback from the model in the form of elicitation. Our work shows promising state-of-the-art results on word substitution attacks on LLMs.

\subsection{Confidence Elicitation}
\vspace{-0em}
% Previeous work explored how well LLMs can express their confidence levels accurately. Previes work notices some basic prompting techniques do not express their confidence accurately **Sinha, et al., "Not all models are created equal: Benchmarking and robustness testing in natural language processing from the dark side"**, but did have some success on more sophisitcated strategies such as elicting k guesses **Hendrycks, et al., "A baseline for detecting adversarial inputs to deep neural networks"**. 

% LLMs often do not express their confidence levels accurately **Guo, et al., "On calibration of modern neural networks"**, which implies they are poorly calibrated. A common method, which has been thoroughly explored in previous work **Kumar, et al., "Confidence-calibrated adversarial training for black-box attacks on deep learning models"** involves using output logits as a proxy for confidence. This could be implemented by focusing on the first generated vector for a specific token or by adding a binary classification prediction head that utilizes the last generated token **Hendrycks, et al., "A baseline for detecting adversarial inputs to deep neural networks"**. While these approaches could be effective, several challenges arise. Firstly output logits may not be accessible, particularly with proprietary models. Secondly the likelihood of the next token primarily signifies lexical confidence and not epistemic uncertainty **Kumar, et al., "Confidence-calibrated adversarial training for black-box attacks on deep learning models"**. This uncertainty underscores the model's difficulty in precisely determining a single method of response, reflecting a broader uncertainty pertinent to \textit{the same question} **Guo, et al., "On calibration of modern neural networks"**.

% LLMs often do not express their confidence levels accurately **Guo, et al., "On calibration of modern neural networks"**, which implies they are poorly calibrated. 

Multiple studies have explored calibration in language models. A common method, which has been thoroughly explored in previous work **Kumar, et al., "Confidence-calibrated adversarial training for black-box attacks on deep learning models"** involves using output probabilities as a proxy for confidence. This could be implemented by focusing on the first generated vector for a specific token, by adding a binary classification prediction head that utilizes the last generated token **Hendrycks, et al., "A baseline for detecting adversarial inputs to deep neural networks"**, focusing on the answer specific token or take the average of the probabilities across the whole sequence, these techniques have been classified as white-box confidence estimation. While these approaches could be effective, several challenges arise. Firstly output logits or probabilities may not be accessible, particularly with proprietary models. Secondly the likelihood of the next token primarily signifies lexical confidence and not epistemic uncertainty **Kumar, et al., "Confidence-calibrated adversarial training for black-box attacks on deep learning models"**, and therefore, struggles to capture the semantic uncertainty in the entire text **Guo, et al., "On calibration of modern neural networks"**. 
% Thirdly although confidence can be reasonably approximated for classification tasks, it is more nuanced in generative tasks, where no single token's logit might reflect the overall response's confidence.   

% Consequently, there remains a need for models capable of expressing uncertainty in natural language more directly.

As a result, previous work highlighted the need for models capable of directly expressing uncertainty in natural language in a black-box setting. Some research has explored enhancing calibration by empirically deriving confidence through repetitive model querying **Kumar, et al., "Confidence-calibrated adversarial training for black-box attacks on deep learning models"**. Alternatively, models can be prompted to express their confidence verbally, either through verbalized numerical confidence elicitation **Sinha, et al., "Not all models are created equal: Benchmarking and robustness testing in natural language processing from the dark side"** or verbal confidence elicitation **Hendrycks, et al., "A baseline for detecting adversarial inputs to deep neural networks"**. It has been found that some prompts can achieve reasonable uncertainty quantification, especially by querying the model twice, first for the prediction, and the second time for the uncertainty estimates **Kumar, et al., "Confidence-calibrated adversarial training for black-box attacks on deep learning models"** (Example of a prompt for confidence elicitation is in Table \ref{appendix:tab:2Sprompt} in the Appendix).

 

% Models might also be instruction-finetuned to improve calibration **Kumar, et al., "Confidence-calibrated adversarial training for black-box attacks on deep learning models"**. However, these strategies pose drawbacks including the additional computational costs and resources required for further training, which may not be feasible for all users or the requirement of task specific labels. There are also out-of-the-box solutions that do not necessitate further modifications, like semantic entropy. This technique employs an entailment model to cluster sequences generated by the model before estimating the generated uncertainty **Elsayed, et al., "Adversarial examples for neural networks: A survey"**. This technique employs an entailment model to cluster sequences generated by the model before estimating the generated uncertainty **Sinha, et al., "Not all models are created equal: Benchmarking and robustness testing in natural language processing from the dark side"**, other than an entailment model, a similar approach could be using semantic similarity encoders, which could give more fine grained similarity/meaning information for generated samples **Hendrycks, et al., "A baseline for detecting adversarial inputs to deep neural networks"**.