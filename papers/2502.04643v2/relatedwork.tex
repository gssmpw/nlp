\section{Related Work}
\vspace{-0em} % Adjust this value as needed
\subsection{Attacks on LLMs}
\vspace{-0em}
The traditional adversarial attack formulation involves adding a subtle perturbation $\delta$ to the original $x$ so that $ x_{\text{adv}} = x + \delta$ \citep{Adversarial_examples}. There are many ways to calculate $\delta$; the overarching idea is that we want to add a perturbation $\delta$ to $x$ to identify regions of high risk in the input space \citep{FreeLB}. This, in turn, is expected to increase the output loss for a given task $t_a$.

One of the most effective ways to find regions of high input risk remains the fast gradient sign method (FGSM) \citep{AT}, the basic iterative method (BIM) \citep{BIM}, and projected gradient descent (PGD) \citep{PGD}. These methods utilize gradient information to find an optimal $\delta$ given a bound $\epsilon$ with either one or multiple iterations.

These first-order techniques model a tractable maximization operation over a non-convex loss landscape as follows:

\vspace{-0em} % Adjust this value as needed
\begin{equation}
\rho(\theta) = E_{(x,y) \sim D} \left[ \max_{\delta \in \Delta} L(\theta, x + \delta, y) \right]
\end{equation}
\vspace{-0em} % Adjust this value as needed

Where $\rho(\theta)$ represents the worst possible perturbation for $x$ on model $f_{\theta}$ with parameters $\theta$. When we maximize $ L(\theta, x + \delta, y)$, we do so by using multiple $\delta$ from a set $\Delta$ of all possible perturbations given a bound $\epsilon$. This bound is often set to the $L_2$-Norm: $[ |\delta|_2 \leq \epsilon ]$ or $L_{\infty}$-Norm: $[ |\delta|_\infty \leq \epsilon ]$.

These techniques and their variants have found moderate success when applied to the input embedding (continuous) space as white-box attacks \citep{ASCC, FreeLB, Convexhull}. However, most language applications interact with LLMs through the token space. Although previous work has attempted to use gradient information to perform attacks in the token space \citep{Hotflip, LLM_Attack_1, FGPM}, the projection from a continuous to a discrete space results in high perplexity and low semantics \citep{AutoDAN_Add_New_Tokens_Algo, AutoDan_Genetic_Algo}.

Given the unrealistic threat model of having access to gradients and input embedding spaces, some efforts have explored adversarial attacks in the token space by perturbing at the character \citep{TextBugger, Viper, SSTA,Empirical_Punctuation}, word \citep{Textfooler, PWWS, BERTAttack, morphin}, or sentence level \citep{Sentance_Level_Attack, Paraphrase_Attack, Paraphrase_Attack_2,Multi-Hop-Attacks}. These attacks often use output probabilities as feedback while employing heuristic search techniques, such as beam search, greedy search, or particle swarm optimization \citep{Sememe_PSO}. These methods have been exceptionally effective on BERT \citep{BERT}-based encoding models. However, widely used commercial LLMs (e.g. ChatGPT) are closed-source and logits or probabilities are not available; attacks have to operate  in a purely black-box setting with only hard-predictions available as feedback \citep{sspattack}. Recent works have explored using LLMs to red team (perform multiple black-box hard label attacks \citep{HardLabel}) on other LLMs with moderate success \citep{PAIR, LLM_Fool_Itself}. We believe the lack of feedback in the perturbation ($\delta$) optimization process is holding these attacks back.

In this paper, we propose a novel attack technique we call confidence elicitation attacks, which aim to attack models in a completely black-box setting while still utilizing feedback from the model in the form of elicitation. Our work shows promising state-of-the-art results on word substitution attacks on LLMs.


\subsection{Confidence Elicitation}
\vspace{-0em}
% Previeous work explored how well LLMs can express their confidence levels accurately. Previes work notices some basic prompting techniques do not express their confidence accurately \citep{Confidence_Elicitation}, but did have some success on more sophisitcated strategies such as elicting k guesses \citep{Just_Ask_For_Calibration}. 

% LLMs often do not express their confidence levels accurately \citep{Confidence_Elicitation}, which implies they are poorly calibrated. A common method, which has been thoroughly explored in previous work \citep{Calibration_Logits,Calibration_Logits_LLMs} involves using output logits as a proxy for confidence. This could be implemented by focusing on the first generated vector for a specific token or by adding a binary classification prediction head that utilizes the last generated token \citep{Language_Models_Mostly_Know}. While these approaches could be effective, several challenges arise. Firstly output logits may not be accessible, particularly with proprietary models. Secondly the likelihood of the next token primarily signifies lexical confidence and not epistemic uncertainty \citep{Epistetic_Uncertenty}. In fact, the confidence measure often depends on the application. For instance, when a model answers the query, "What is the capital of Italy?" the initial token could be highly uncertain among "Rome", "The" and "Italy" due to multiple valid response structures such as "Rome is the capital," "The capital is Rome," or "Italy's capital is Rome." This uncertainty underscores the model's difficulty in precisely determining a single method of response, reflecting a broader uncertainty pertinent to \textit{the same question} \citep{Semantic_Uncertainty}. Thirdly Although confidence can be reasonably approximated for classification tasks, it is more nuanced in generative tasks, where no single token's logit might reflect the overall response's confidence.



% LLMs often do not express their confidence levels accurately \citep{Confidence_Elicitation}, which implies they are poorly calibrated. 

Multiple studies have explored calibration in language models. A common method, which has been thoroughly explored in previous work \citep{Calibration_Logits,Calibration_Logits_LLMs} involves using output probabilities as a proxy for confidence. This could be implemented by focusing on the first generated vector for a specific token, by adding a binary classification prediction head that utilizes the last generated token \citep{Language_Models_Mostly_Know}, focusing on the answer specific token or take the average of the probabilities across the whole sequence, these techniques have been classified as white-box confidence estimation. While these approaches could be effective, several challenges arise. Firstly output logits or probabilities may not be accessible, particularly with proprietary models. Secondly the likelihood of the next token primarily signifies lexical confidence and not epistemic uncertainty \citep{Epistetic_Uncertenty}, and therefore, struggles to capture the semantic uncertainty in the entire text \citep{Confidence_Elicitation}. 
% Thirdly although confidence can be reasonably approximated for classification tasks, it is more nuanced in generative tasks, where no single token's logit might reflect the overall response's confidence.   

% Consequently, there remains a need for models capable of expressing uncertainty in natural language more directly.

As a result, previous work highlighted the need for models capable of directly expressing uncertainty in natural language in a black-box setting. Some research has explored enhancing calibration by empirically deriving confidence through repetitive model querying \citep{Strenght_In_Numbers_Elicitation}. Alternatively, models can be prompted to express their confidence verbally, either through verbalized numerical confidence elicitation \citep{Confidence_Elicitation} or verbal confidence elicitation \citep{Epistetic_Uncertenty}. It has been found that some prompts can achieve reasonable uncertainty quantification, especially by querying the model twice, first for the prediction, and the second time for the uncertainty estimates \citep{Just_Ask_For_Calibration} (Example of a prompt for confidence elicitation is in Table \ref{appendix:tab:2Sprompt} in the Appendix).

 

% Models might also be instruction-finetuned to improve calibration \citep{Elicitation_Instruction_Finetuning}. However, these strategies pose drawbacks including the additional computational costs and resources required for further training, which may not be feasible for all users or the requirement of task specific labels. There are also out-of-the-box solutions that do not necessitate further modifications, like semantic entropy. This technique employs an entailment model to cluster sequences generated by the model before estimating the generated uncertainty \citep{Semantic_Uncertainty}, other than an entailment model, a similar approach could be using semantic similarity encoders, which could give more fine grained similarity/meaning information for generated samples \citep{Universalsentenceencoder}\citep{Bertsentence}.