\documentclass[11pt,twocolumn]{article}


% Make sure arxiv knows what to do:
\pdfoutput=1
% Reset layout:
\usepackage[a4paper,top=1.2in,bottom=1in,left=0.7in,right=0.7in]{geometry}
\setlength{\columnsep}{1.5em}
%% \usepackage{fancyhdr}
%% \renewcommand{\headrulewidth}{0pt}
%% \fancyhead{}%
%% \fancyfoot{}%
%% %\fancyhead[L]{Sample selection}
%% \fancyhead[R]{\rule[-0.5em]{0pt}{.5em}\footnotesize\thepage}
%% \fancypagestyle{frontpage}{\fancyhead{}\renewcommand{\headrulewidth}{0pt}}
%% \pagestyle{fancy}


\usepackage{caption}
\captionsetup[figure]{font=footnotesize,labelfont=footnotesize}

\usepackage{subcaption}

\usepackage{titlesec}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\Alph{subsubsection}}
\titleformat{\section}[hang]{\normalfont\bfseries}{\thesection}{.5em}{}
\titlelabel{\subsubsection}{\empty}
\titleformat{\subsection}{\normalfont\bfseries}{\thesubsection.}{.5em}{}[]
\titleformat{\subsubsection}[runin]{\normalfont}{\thesubsubsection.}{0em}{}[]
\newcommand{\sub}{\subsection{}}
\newcommand{\subsub}{\subsubsection{}}
\newcommand{\sectionbreak}{\vspace{1em}}%{\clearpage}
\renewcommand{\abstract}[1]{{\gdef\thepoabstract{#1}}}

\usepackage{amsthm}
\usepackage{thmtools}

% Set up title page
\newcommand{\keywords}[1]{{\gdef\thepokeywords{#1}}}
\newcommand{\runtitle}[1]{{\gdef\theporuntitle{#1}}}
\newcommand{\affiliation}[1]{{\gdef\thepoaffiliation{#1}}}

\makeatletter
\renewcommand\maketitle
{
\thispagestyle{empty}%{frontpage}
\rmfamily\selectfont
\ifdefined\@title{\noindent\Large\bfseries\centering \@title \par}\else\fi
\vspace{2em}
\ifdefined\@author{\centering\normalfont \@author \par}
\vspace{.5em}
\ifdefined\thepoaffiliation{\noindent\small\thepoaffiliation\par}\fi\vspace{3em}\else\fi
\ifdefined\thepoabstract{\small\noindent{{\bfseries Abstract}.\;\;}\thepoabstract\par\vspace{2em}}\else\fi
\ifdefined\thepokeywords{{\noindent\bfseries Keywords\;\;}\thepokeywords\par\vspace{5em}}\else\fi
\ifdefined\theporuntitle{\fancyhead[L]{\footnotesize\theporuntitle}}\fi
}
\makeatother



%\usepackage[paperheight=297mm,paperwidth=210mm]{geometry}
%\renewenvironment{abstract}{\noindent{\bfseries Abstract}.}{\par}
%\newenvironment{keywords}{\noindent{\bfseries Keywords}\quad}{\par}
% Set up hyperlinks:
\usepackage[usenames,dvipsnames]{color}
\definecolor{RefColor}{rgb}{0,0,.85}
\definecolor{UrlColor}{rgb}{.5,.5,.5}%
\RequirePackage[colorlinks,linkcolor=RefColor,citecolor=RefColor,urlcolor=UrlColor,linktoc=page]{hyperref}
% Remove journal name in PDF info metadata
\hypersetup{pdfinfo={Subject={ }}}
% Minimal typesetting toolkit:
\RequirePackage[OT1]{fontenc}
% Natbib: Choose \bibliographystyle{} from {abbrvnat, plainnat, unsrtnat, natdin}
\usepackage[numbers,sort]{natbib}
\renewcommand\bibfont{\footnotesize}
\renewcommand\bibsep{.1em}
\bibliographystyle{abbrvnat}

\usepackage[english]{babel}

\usepackage{enumitem}
\setlist[itemize]{leftmargin=1.5em}

\usepackage{amsmath,amssymb,amscd,amsfonts,amsthm,mathtools}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage{dsfont}
\usepackage{thmtools}
\usepackage{nccmath}
\usepackage{scalerel}

\renewcommand\labelitemi{\raisebox{0.35ex}{\tiny$\bullet$}}



\usepackage{bibentry}

%%% Local definitions %%%
% \usepackage{booktabs}
% \usepackage{tikz}
% \usepackage[low-sup]{subdepth}
% \usetikzlibrary{calc}
% \usetikzlibrary{decorations.markings,decorations.pathreplacing}
% \tikzstyle{mybraces}=[mirrorbrace/.style={
%           decoration={brace, mirror},
%           decorate},brace/.style={
%           decoration={brace},
%           decorate}]
% \usepackage{pst-node}
%\usepackage{auto-pst-pdf}
% \usepackage{tikz-cd}

\usepackage{tocloft}
\renewcommand{\cftsecfont}{\normalfont}
\renewcommand{\cftsecpagefont}{\normalfont}
\cftsetpnumwidth{5cm}
\cftsetrmarg{6cm}

\usepackage{wrapfig}
% \usepackage{subcaption}
% Make References heading show in pdf table of content:
\usepackage[nottoc]{tocbibind}
\settocbibname{References}
 

\theoremstyle{plain}
\declaretheoremstyle[postheadspace=.4em,headfont=\bfseries,bodyfont=\itshape,spaceabove=8pt,
spacebelow=10pt]{basic}
\theoremstyle{basic}
\declaretheorem[style=basic,name={Theorem}]{theorem}
\declaretheorem[style=basic,sibling=theorem,name={Lemma}]{lemma}
\declaretheorem[style=basic,sibling=theorem,name={Fact}]{fact}
\declaretheorem[style=basic,sibling=theorem,name={Proposition}]{proposition}
\declaretheorem[style=basic,sibling=theorem,name={Corollary}]{corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\declaretheorem[style=definition,name={Remark}]{remark}
\declaretheorem[style=definition,name={Remark},numbered=no]{remark*}
\declaretheorem[style=definition,name={Example}]{example}
\declaretheorem[style=definition,name={Assumption}]{assumption}
%\declaretheoremstyle[postheadspace=.4em,headfont=\scshape]{examplestyle}
%\declaretheorem[style=examplestyle,numbered=no,name=Examples]{examples} 


\usepackage{times}


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
% \usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\input{khh_math_commands}
\def\elif#1{\textcolor[rgb]{1,0,1}{#1}} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{fact}[theorem]{Fact}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}





% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{cuted}

\titlespacing\section{0pt}{8pt plus 4pt minus 2pt}{6pt plus 2pt minus 2pt}


\title{Diagonal Symmetrization of Neural Network Solvers \\ for the Many-Electron Schr\"odinger Equation}

\begin{document}

\begin{abstract}{
    Incorporating group symmetries into neural networks has been a cornerstone of success in many AI-for-science applications. Diagonal groups of isometries, which describe the invariance under a simultaneous movement of multiple objects, arise naturally in many-body quantum problems. Despite their importance, diagonal groups have received relatively little attention, as they lack a natural choice of invariant maps except in special cases. We study different ways of incorporating diagonal invariance in neural network ans\"atze trained via variational Monte Carlo methods, and consider specifically data augmentation, group averaging and canonicalization. We show that, contrary to standard ML setups, in-training symmetrization destabilizes training and can lead to worse performance. Our theoretical and numerical results indicate that this unexpected behavior may arise from a unique computational-statistical tradeoff not found in standard ML analyses of symmetrization. Meanwhile, we demonstrate that post hoc averaging is less sensitive to such tradeoffs and emerges as a simple, flexible and effective method for improving neural network solvers.}
\end{abstract}

\author{
    Kevin Han Huang\footnotemark[1], \; Ni Zhan\footnotemark[2], \; Elif Ertekin\footnotemark[3], \; Peter Orbanz\footnotemark[1], \; Ryan P. Adams\footnotemark[2]
    \\[1em]
    \footnotemark[1]Gatsby Unit, University College London
    \\[.2em]
    \footnotemark[2]Department of Computer Science, Princeton University 
    \\[.2em]
    \footnotemark[3]Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign
    \vspace{-1.5em}
}

\twocolumn[
    \begin{@twocolumnfalse}
    \maketitle
    \end{@twocolumnfalse}
]

\begin{figure*}[t]
    \vspace{-.5em}
    \centering    
    \begin{tikzpicture}
        
        \node[inner sep=0pt] at (-4,0) {\includegraphics[trim={3.8cm 1.1cm 3.5cm 1.2cm},clip,width=.65\linewidth]{figs/wavefunction-scan-OG-PA.png}}; 
        \node[inner sep=0pt,rotate=90] at (-9.75,-0.25){\scriptsize $y$-displacement};

        \node[inner sep=0pt] at (4.45,-0.03) {\includegraphics[trim={1.8cm 1.1cm 1.8cm 1.2cm},clip,width=.325\linewidth]{figs/wavefunction-scan-OG-PA-diff.png}}; 

        \node[inner sep=0pt] at (-6.55,-3.35){\scriptsize $x$-displacement}; 
        \node[inner sep=0pt] at (-1.1,-3.35){\scriptsize $x$-displacement}; 
        \node[inner sep=0pt] at (4.55,-3.35){\scriptsize $x$-displacement}; 

        \node[inner sep=0pt] at (-6.7,-3.8){\scriptsize \textbf{(a) $\log | \psi^{({\rm OG})}_{\theta} |^2$, original DeepSolid}};
        \node[inner sep=0pt] at (-1.3,-3.8){\scriptsize \textbf{(b) $\log | \psi^{({\rm PA};\G)}_{\theta}|^2$, post hoc averaging}};
        \node[inner sep=0pt] at (4.4,-3.8){\scriptsize \textbf{(c) Improvement of $\log | \psi^{({\rm OG})}_{\theta} |^2$ over $\log | \psi^{({\rm PA};\G)}_{\theta}|^2$}};


        \node[inner sep=0pt] at (-6.7,-6) {\includegraphics[trim={4.5cm 5.5cm 5.2cm 6cm},clip,width=.28\linewidth]{figs/graphene_1_symmetry.pdf}};


        \node[inner sep=0pt] at (-1.3,-6.2) {\includegraphics[trim={2.5cm 4cm 4.2cm 2cm},clip,width=.33\linewidth]{figs/graphene_1_symmscan.pdf}};

        \node[minimum size=.2em] at (-1.1, -.2) {\footnotesize (e)};

        \node[minimum size=.2em] at (0.2, -1.) {\footnotesize (f)};

        % \node[inner sep=0pt] at (4.1,-6.1) {\includegraphics[trim={3.3cm .92cm 3.7cm 0.95cm},clip,width=.36\linewidth]{figs/wavefunction-scan-OG-PA-zoomed-in.png}};        

        \node[inner sep=0pt] at (4.5,-6.2) {\includegraphics[trim={2.5cm 4cm 4.2cm 2cm},clip,width=.33\linewidth]{figs/graphene_1_symmscan_shifted.pdf}};        

        \node[inner sep=0pt,rotate=90] at (-9.25,-6){\scriptsize $y$ (Bohr)};
        \node[inner sep=0pt] at (-6.7,-7.65){\scriptsize $x$ (Bohr)};

        \node[inner sep=0pt] at (-1.2,-8.1){\scriptsize $x$ (Bohr)};
        \node[inner sep=0pt] at (-4.1,-5.9){\scriptsize $y$};
        \node[rotate=70,inner sep=0pt] at (-3.6,-7.8){\scriptsize $z$};

        \node[inner sep=0pt] at (4.6,-8.1){\scriptsize $x$ (Bohr)};
        \node[inner sep=0pt] at (1.7,-5.9){\scriptsize $y$};
        \node[rotate=70,inner sep=0pt] at (2.2,-7.8){\scriptsize $z$};

        

        \node[inner sep=0pt, align=left] at (-6.7,-8.3){\scriptsize \textbf{(d) graphene $1 \times 1$ system,} \\[-.2em] \scriptsize \hspace{1.2em} \textbf{\texttt{P6mm} point group}};

        \node[inner sep=0pt] at (-1,-8.5){\scriptsize \textbf{(e) a symmetric configuration of $12$ electrons}};

        \node[inner sep=0pt] at (4.4,-8.5){\scriptsize \textbf{(f) a shifted version of (e)}};
    \end{tikzpicture}
    \vspace{-1.8em}
    \caption[]{Visualizations of the (partial) diagonal invariance of an unsymmetrized wavefunction versus a symmetrized wavefunction in a graphene $1 \times 1$ system. (a) and (b) are generated by evaluating $\log | \psi(x_1+t, \ldots, x_{12}+t) |^2$ under a simultaneous 2d translation $t$ of the configuration $(x_1, \ldots, x_{12})$ given by the $12$ blue spheres in (e). The red overlay indicates the unit cell in (e) such that the \textcolor{red}{ref.~line} is exactly at the origin when $t=0$. (d) shows $2$ atoms (orange spheres) in an $1 \times 1$ planar supercell, with $\G$ illustrated by the 
    \tikz[baseline=-0.3em]{\draw[line width=.15em] (0,-0.1) -- (0,0.1) -- (0.1,0.1); \draw[line width=.15em] (-0.1,0) -- (0,0);}
    objects. (e) and (f) are shifted copies of the same configuration, with their positions marked in (b). This method only visualizes the partial \texttt{P3m1} symmetry; see \cref{appendix:diag:inv:illustration} for a method that shows the full \texttt{P6mm} symmetry of $\psi_{\theta}^{(\rm PA;\G)}$. Details setups are in Sec.~\ref{sec:eval}, \ref{sec:experiments} and \cref{appendix:experiments}.
    }
    \vspace{-1em}
    \label{fig:OG:PA:wf:scan}
\end{figure*}

\section{Introduction}

\noindent
We study the effect of symmetrizing neural network solutions to the Schr\"{o}dinger equation. Solving the many-body Schr\"{o}dinger equation is of fundamental importance in science, because it provides the key to understanding and predicting the behavior of quantum systems and thereby many physical phenomena. Ab initio computational methods seek to solve the non-relativistic electronic Schr\"{o}dinger equation from first principles. There, the computation is performed directly from physical constraints and without relying on empirical approximations or training data, with the promise of producing high-accuracy electronic wavefunctions. However, the strict requirement on physical constraints also makes it challenging to incorporate neural networks into these methods. PauliNet \citep{hermann2020deep} and FermiNet \citep{pfau2020ab} are two of the first successful ab initio neural network methods, which learn the ground state wavefunction of atoms and molecules via a variational Monte Carlo (VMC) approach. Many neural network methods have emerged since then \citep{li2022ab,glehn2023a,cassella2023discovering}, each seeking to improve how physical constraints are modelled in different systems. While these approaches have proved to produce state-of-the-arts results in terms of ground state energy and other physical properties, one notable drawback is their exorbitant training cost compared to classical VMC methods. This issue becomes dire for modelling large systems, as the Hilbert space of wavefunctions grows exponentially with the number of electrons.

In other AI-for-science approaches, symmetrization has proved to be successful both for modelling physical constraints and for improving neural network performance \cite{batatia2022mace,du2022se,batzner20223,duval2023faenet}. One VMC example is DeepSolid \citep{li2022ab}, a FermiNet-type wavefunction that employs translationally invariant features to model periodic solids.


Motivated by these successes, we examine the effectiveness of symmetrizing ab initio neural network solvers under the natural symmetry groups of a many-body problem, which are the \emph{diagonal groups of isometries}. These groups, roughly speaking, describe the invariance of the system under a simultaneous movement of multiple objects; see Sec.~\ref{sec:setup:diagonal:symmetry} for a detailed review. Common symmetrization approaches in machine learning (ML) roughly fall under three categories:
\vspace{.2em}
\begin{itemize}[topsep=0em, parsep=0em, partopsep=0em, itemsep=0.2em, leftmargin=1em]
    \item Randomly transforming data by symmetry operations (\emph{data augmentation});
    \item Averaging over group operations (\emph{group averaging});
    \item Invariant maps (\emph{invariant features} and \emph{canonicalization}).
\end{itemize}
\vspace{.2em}
Investigating these approaches, we find that, perhaps surprisingly, the effects of diagonal symmetrization for VMC problems are mixed and nuanced. This arises due to the unique combination of challenges posed by VMC and diagonal invariance, as well as the joint consideration of computational cost, statistical behaviors and physical constraints. In particular, our analyses indicate:

\vspace{.2em}

\emph{In-training symmetrization can hurt.} VMC training operates in an ``infinite-data'' regime, and every symmetry operation comes at the cost of forgoing one new data point. Holding the computational budget constant, symmetrization can destabilize training and lead to worse performance. This is demonstrated by theoretical and numerical results in Sec.~\ref{sec:symm:training}.

\emph{Post hoc symmetrization helps.} At inference time, VMC solvers are less sensitive to computational costs, and allow for averaging over a moderate number of group operations (Sec.~\ref{sec:symm:inference}). We show that post hoc averaging (PA) leads to improved energy, variance and symmetry properties of the learned wavefunction (Fig.~\ref{fig:OG:PA:wf:scan}, Table \ref{table:stats}). In one case, post hoc averaged DeepSolid achieves performance close to DeepSolid trained with $10 \times$ more computational budget (Sec.~\ref{sec:experiments}).

The remainder of the paper provides mathematical discussions and computational tools for understanding diagonal symmetries in VMC. Sec.~\ref{sec:setup:diagonal:symmetry} reviews the concept of diagonal invariance and discusses why, except for simple cases e.g., translations or $E(3)$, finding a natural smooth invariant map is difficult. This is further corroborated by mathematical and empirical results in the special case of a \emph{smoothed canonicalization} (Sec.~\ref{sec:canon} and \cref{appendix:canon}). Sec.~\ref{sec:setup:vmc} briefly reviews the VMC setup and how different computational costs arise. Sec.~\ref{sec:symm:training} and \ref{sec:symm:inference} respectively examine in-training and post hoc symmetrization. Sec.~\ref{sec:eval} discusses our evaluation metrics and develops a method for visualizing diagonal symmetries (Fig.~\ref{fig:OG:PA:wf:scan}). Sec.~\ref{sec:experiments} discusses experiment details. Additional results and proofs are included in the appendix. 

\vspace{-.5em}

\subsection{Many-body Schr\"{o}dinger equation} 

\noindent
Throughout, we shall consider finding the $n$-electron ground state wavefunction $\psi: \R^{3n} \rightarrow \C$ and the corresponding minimal energy $E \in \R$ of the Schr\"{o}dinger equation \vspace{-.5em}
\begin{align}
        H \psi(\bx) = E \psi(\bx) ,
        \quad 
        \bx \coloneqq (x_1, \ldots, x_n) \in \R^{3n}
        \;.
   \label{eq:schrodinger}
\end{align}  \\[-1.4em]
The Hamiltonian $H$ is given by $H \psi(\bx) \coloneqq - \frac{1}{2} \Delta \psi(\bx) + V(\bx) \psi(\bx)$, $\Delta$ is the Laplacian representing the kinetic energy and $V: \R^{3n} \rightarrow \R$ is the potential energy of the physical system. We also denote neural network ans\"atze by $\psi_\theta$, parametrized by some network weights $\theta \in \R^q$. Note that in general, the wavefunction depends on each electron via $(x_i, \sigma_i)$, where $\sigma_i \in \{ \uparrow, \downarrow\}$ is the spin, and $\psi$ is required to be anti-symmetric with respect to permutations of $(x_i, \sigma_i)$. We focus on the case with fixed spins for simplicity, as is done in FermiNet and DeepSolid.


\section{Diagonal invariance} \label{sec:setup:diagonal:symmetry}



\begin{figure}[t]
    \vspace{-.5em}
    \centering    
    \begin{tikzpicture}
        \node[inner sep=0pt] at (-2,0) {\includegraphics[trim={6.6cm 5.9cm 6.0cm 5.9cm},clip,width=.51\linewidth]{figs/graphene_1_2-elec.pdf}};
        
        \node[inner sep=0pt] at (2.1,0) {\includegraphics[trim={6.6cm 5.9cm 6.0cm 5.9cm},clip,width=.51\linewidth]{figs/graphene_1_2-elec.pdf}};
        
        \draw[line width=.2em] (-1.4,-0.78) arc[start angle=-110, end angle=15, radius=.8em];
        \draw[->, shift={(-1.02,-0.45)}, rotate around={-90:(0,0)}, line width=.2em] (0:.1em) -- (180:.1em);

        \draw[->, line width=.2em] (-2.,0.49) -- (-1.56, 0.21);

        \draw[->, line width=.2em] (2.1,0.49) -- (2.54, 0.21);
        \draw[->, line width=.2em] (2.35,-0.65) -- (1.24, 0.01);
    \end{tikzpicture}
    \vspace{-2em}
    \caption{Different invariances for two electrons in a graphene system. \emph{Left.} Separate invariance under a reflection and a rotation. \emph{Right.} Diagonal invariance under a simultaneous reflection.
    }
    \vspace{-1em}
    \label{fig:sep:inv:diag:inv}
\end{figure}

\noindent
Many physical systems of interest exhibit symmetry under some group $\G$ of isometries. $\G$~consists of maps of the form \\[-1.5em]
\begin{align*}
    x \;\mapsto\; A x + b \;, \qquad x \in \R^3\;,
    \tagaligneq \label{eq:G:iso}
\end{align*}  \\[-1.4em]
for some orthogonal $A \in \R^{3 \times 3}$ and some translation $b \in \R^3$. We focus on groups that are countable. For systems with $n > 1$ electrons, $\G$ typically causes the potential $V$ in \eqref{eq:schrodinger} to be invariant under a \emph{diagonal group} $\Gdiag$ acting on $\R^{3n}$:  \vspace{-.2em}
\begin{align*}
    \Gdiag \;\coloneqq\; \{ (g, \ldots, g) \,|\, g \in \G \}\;.
\end{align*} \\[-1.4em]
To see how $\Gdiag$ may arise, consider the Coulomb potential 
\begin{align*} 
    V_{\rm Coul}(\bx) = \msum_{i < j} \mfrac{1}{\|x_i - x_j\|} + \msum_{i, I} \mfrac{1}{\|x_i - r_I\|} + \ldots\,.
\end{align*} 
Each $r_I$ is the fixed, known position of the $I$-th atom (under the Born-Oppenheimer approximation), $\|\argdot\|$ is the Euclidean norm, and $\ldots$ are the omitted electron-independent terms. If the set of atom positions $\{r_I\}$ is invariant under some $\G$, $V_{\rm Coul}$ is invariant under a \emph{simultaneous} transformation of all $x_i$'s by the same $g \in \G$. Note however that $V_{\rm Coul}$ does \emph{not} satisfy \emph{separate invariance}, i.e.~invariance does not hold if $x_1$ and $x_2$ are transformed by different group elements, since  $\frac{1}{\|g_1(x_1) - g_2(x_2)\|} \neq \frac{1}{\|x_1-x_2\|}$ in general.
Mathematically, separate invariance can be modelled by the product group $\G^n$ acting on $\R^{3n}$, and diagonal invariance arises as a specific subgroup of $\G^n$, i.e.~$\Gdiag \;\subseteq\; \G^n$. Fig.~\ref{fig:sep:inv:diag:inv} illustrates the difference between $\Gdiag$ and $\G^n$ in a 2-electron system: Under the given symmetry, a potential function is left unchanged when both electrons are reflected, but not when one is reflected and the other is rotated. 

\vspace{.5em}

\noindent
\textbf{$\Gdiag$-invariant wavefunction.} Throughout this paper, we use the shorthand $g(\bx) = (g(x_1), \ldots, g(x_n))$ for $g \in \G$, and focus on $\Gdiag$-invariant potentials $V$, i.e.,
\begin{align*}
    V( \bx ) \;=\; V( g(\bx) )
    \qquad\text{ for all } 
    g \in \G \;.
    \tagaligneq \label{eq:diag:inv}
\end{align*}
Invariance of $V$ does not imply the invariance of $\psi$: For a translation-invariant $V$ with one electron, \citet{bloch1929quantenmechanik} proves that $\psi$ is only invariant up to a phase factor, and non-invariant solutions can occur when the ground state is degenerate \cite{tinkham2003group}. Nonetheless, an invariant solution can always be constructed from a linear combination of these states. The next result confirms this for the general case of $n \geq 1$ electrons and a diagonal group of isometries.

\begin{fact} \label{fact:inv:soln:exists} Suppose $(\psi, E)$ solves \eqref{eq:schrodinger} and $V$ is invariant under some $\Gdiag$ induced by a group $\G$ of isometries. Then for any finite subset $\cG$ of $\G$, 
\begin{align*}
    \psi^\cG( \bx ) \;\coloneqq\; \mfrac{1}{|\cG| } \msum_{g \in \cG} \psi(g(\bx))
    \tagaligneq \label{eq:GA:network}
\end{align*}
also solves \eqref{eq:schrodinger} with respect to same energy $E$. In particular, if $\cG$ is a subgroup, $\psi^\cG$ is invariant under $\cG$.
\end{fact}

\cref{fact:inv:soln:exists} motivates us to seek wavefunctions that respect the $\Gdiag$-invariance of the system. Fig.~\ref{fig:OG:PA:wf:scan}(a) also shows that an unsymmetrized, well-trained wavefunction already attempts to achieve some extent of approximate invariance.

\vspace{.2em}

For more references on how $\Gdiag$-invariance arises and plays an important role in modelling $\psi$, see \citet{rajagopal1995variational,zicovich1998use}.


\vspace{.5em}

\noindent
\textbf{Challenges in modelling $\Gdiag$-invariance.} Despite successes in modelling simple $n$-electron symmetries such as the Euclidean group $E(3)$ \cite{batzner20223} and translations \cite{whitehead2016jastrow}, generalizing the approaches in those settings to general isometries can be challenging:

\vspace{.5em}

\begin{itemize}[topsep=0em, parsep=0em, partopsep=0em, itemsep=0.5em, leftmargin=1em]
    \item Translation groups possess simple and well-understood symmetries. Common symmetrizations include periodic Fourier bases \cite{rajagopal1995variational} and projection via taking a modulus \cite{dym2024equivariant}. The former trades off representation power against computational cost as the number of bases used varies, whereas the latter suffers from discontinuity and requires smoothing (Sec.~\ref{sec:canon}). Neither has been extended to a general $\Gdiag$.
    \item $E(3)$ consists of all isometries in $\R^3$, and also admits simple invariant features \cite{batzner20223}.  
    However, the additional symmetries under $E(3)$ are undesirable when $\G$ is only a subgroup of $E(3)$, since the missing asymmetries represent a loss of information and therefore limit the representative power of the feature. Indeed, building an invariant map without losing information necessitates the availability of a maximal invariant \cite{lehmann1986testing}, whereas building a maximal invariant for $\Gdiag$ while respecting continuity constraints requires extending the mathematical theory of orbifolds \cite{ratcliffe1994foundations,adams2023representing} to $\Gdiag$ of isometries. Such extensions are not known to the best of our knowledge.
\end{itemize}

\vspace{.2em}

In short, invariant maps are well-understood for groups with simple isometries and with many isometries, and the difficult cases are often found in the ones with restricted symmetries. This problem is particularly pronounced in the case of a diagonal group. Mathematically, the symmetries of $\Gdiag$ are described by a fixed group $\G$ in $\R^3$, but act on a larger and larger space $\R^{3n}$ as the number of electrons~$n$ grows. This means that $\Gdiag$ admits substantially less structure than that of e.g.~the product group $\G^n$. 

\vspace{.2em}

\noindent
\textbf{Space group $\G_{\rm sp}$ for a crystal system.} A particular difficult case of $\Gdiag$-invariance, in view of the above discussion, is the one induced by a space group $\G_{\rm sp}$ in a crystal lattice. These groups are described by \eqref{eq:G:iso} with a finite number of orthogonal matrices $A$ and a countable number of translations $b$, which tile the $\R^3$ space with a finite unit volume called the \emph{fundamental region}. Fig.~\ref{fig:OG:PA:wf:scan}(d) illustrates the \texttt{P6mm} point group in a graphene system, where the space is tiled by  a triangular fundamental region. The study of space group is a fundamental subject of solid state physics, and we refer interested readers to \citet{ashcroft1976solid}. 

Since $\G_{\rm sp}$ is infinite, \cref{fact:inv:soln:exists} does not apply directly to ${\cG=\G_{\rm sp}}$. In practice however, to avoid computing an infinite crystal lattice, a common VMC practice is to restrict the system to a finite volume called the \emph{supercell} \citep{esler2010fundamental,kittel2018introduction}. This effectively reduces the set of translations $b$ in \eqref{eq:G:iso} to a finite set and hence $\G_{\rm sp}$ to a finite group, which allows \cref{fact:inv:soln:exists} to apply.

While our theoretical results apply to all finite groups of isometries, our numerical experiments focus on $\G_{\rm sp}$. Each $\G_{\rm sp}$ is denoted by standard shorthands e.g.~\texttt{P6mm}, and an exhaustive list of space groups $\G_{\rm sp}$ can be found in \citet{brock2016international}.

\vspace{.2em}

\noindent
\textbf{Symmetrization of many-body wavefunctions.} We shall focus on generic symmetrization techniques from ML that can be applied directly to many-electron wavefunctions. This is to be distinguished with classical techniques, e.g., \citet{zicovich1998use}: There, symmetrization is performed by 
building
invariance into 
single-electron features, 
which
does not generalize well to state-of-the-art neural network solvers that are many-body by design.

\section{Neural network VMC solver} \label{sec:setup:vmc}

\noindent
We briefly review the VMC approach for training our ab initio neural network solvers. VMC seeks to solve the minimum eigenvalue problem of \eqref{eq:schrodinger} via the optimization  
\begin{align}
    \underset{\theta \in \R^q}{\argmin} \, \mfrac{\langle \psi_\theta, H \psi_\theta \rangle}{\langle \psi_\theta, \psi_\theta \rangle} 
    \;=\;
    \underset{\theta \in \R^q}{\argmin} \, \mean[E_{\rm local; \psi_\theta}(\bX)]
    \;,
    \label{eq:VMC}
\end{align}
where ${\bX \sim p_{\psi_\theta}}$ and $E_{\rm local; \psi_\theta}(\bx) \coloneqq H \psi_\theta(\bX) / \psi_\theta(\bX)$ is the local energy. $\langle f, g \rangle \coloneqq \int f(\bx)^* g(\bx) \, d\bx$ is the complex inner product, and $p_\psi(\bx) = \frac{|\psi(\bx)|^2}{\langle \psi, \psi\rangle}$ is the probability distribution obtained by normalizing a wavefunction $\psi$. The optimization may be performed by first or second-order methods, which can be represented by the generic functions $F_{\bx;\psi_\theta} \equiv F(\psi_\theta(\bx), \Delta \psi_\theta(\bx)) \in \R^q$ and $Q_{\bx;\psi_\theta} \equiv Q( \psi_\theta(\bx), \Delta \psi_\theta(\bx) ) \in \R^{q \times q}$ as
\begin{align*}
    \theta 
    \mapsto&\,
    \theta \,-\, \mean[ F_{\bX;\psi_\theta} ]
    ,
    \;
    \theta 
    \mapsto
    \theta \,-\, \mean[ Q_{\bX;\psi_\theta} ]^{-1} 
    \mean[ F_{\bX;\psi_\theta}]
    .
\end{align*}
Examples of $F_{\bX;\psi_\theta}$ and $Q_{\bX;\psi_\theta}$ can be found in \citet{pfau2020ab}. Notably, the expectation formulation above converts the expensive integral over the entire space into an expectation, which can then be approximated by Monte Carlo averages computed on finitely many samples from $p_{\psi_\theta}$.

\vspace{.5em}

\noindent
\textbf{Training (optimization) phase.} Every training step consists of two sub-steps: (i) \emph{Sampling.} Samples are obtained from running $N$ independent MCMC chains with $p_{\psi_\theta}$ as the target distribution; (ii) \emph{Gradient computation. } $F_{\bX;\psi_\theta}$ (or $Q_{\bX;\psi_\theta}$) is computed on the $N$ samples. Since (i) typically requires computing only the derivative $\partial_x \psi_\theta$, whereas (ii) involves at least $\partial_\theta \psi_\theta$ and $\partial_\theta \partial^2_x \psi_\theta$, the one-step computational costs of (i) and (ii) typically compare as 
$
    C_{\rm samp} \,\ll\, C_{\rm grad} 
$.
This is particularly true for neural network solvers, where short chains ($20-100$ steps) are typically used due to the expensive gradient evaluation and that any small increase in per-step cost is amplified by the large number of training steps. We verify this cost comparison in \cref{table:computational:cost}. 

\noindent
\textbf{Inference phase.} Having obtained a trained wavefunction $\psi_{\hat \theta}$  parametrized by $\hat \theta$, we draw samples from \emph{long} chains that target $p_{\psi_{\hat \theta}}$. These samples are used to compute various physical properties of $\psi_{\hat \theta}$ that can be expressed as expectations of $p_{\psi_{\hat \theta}}$; see Sec.~\ref{sec:eval} for details. Note that the only computational cost occurred here is in terms of $C_{\rm samp}$.

\vspace{.5em}


\begin{figure}[t]
    \centering
    \vspace{-.5em}
    \begin{tikzpicture}
        \node[inner sep=0pt] at (0,0) {\includegraphics[trim={1cm .2cm .8cm .5cm},clip,width=.9\columnwidth]{figs/gradient_stab_std_gpuhrs.pdf}};
        \node[inner sep=0pt,rotate=90] at (-4,0){\scriptsize $\frac{1}{\sqrt{q}} \| \Var[\delta \theta] \|$};
        \node[inner sep=0pt] at (0,-2.6){\scriptsize GPU hours};
    \end{tikzpicture} 
    \caption{ Normalized variance of differently symmetrized gradient updates against GPU hours. Experiment details in Sec.~\ref{sec:experiments}.
    }
    \vspace{-1em}
    \label{fig:grad:stab}
\end{figure}

Symmetrization can be performed during training, inference or both. We analyze symmetrization techniques in the two phases separately in Sec.~\ref{sec:symm:training} and Sec.~\ref{sec:symm:inference}, as they involve different computational tradeoffs. Our theoretical analysis considers first-order methods for simplicity. Our numerical results focus on the symmetrization of DeepSolid \cite{li2022ab}, a state-of-the-arts neural network solver for solid systems with $\G_{\rm sp}$-symmetry, trained with the second-order method  \texttt{KFAC} \cite{martens2015optimizing}; see \cref{appendix:DeepSolid}.


\section{Symmetrization during training} \label{sec:symm:training}

\noindent
We first present a theoretical analysis of the behavior of gradient updates under different in-training symmetrization techniques. Let $\bX_1, \ldots, \bX_N$ be i.i.d.~samples from $p^{(m)}_{\psi_\theta}$, the distribution of an $m$-th step MCMC chain with $p_{\psi_\theta}$ as the target distribution. Our benchmark for comparison is the update rule with the original (OG) unsymmetrized $\psi_\theta$,
\begin{align*}
    \theta \mapsto \theta - \delta \theta^{(\rm OG)}\;,
    \;\;
    \delta
    \theta^{(\rm OG)} 
    \coloneqq
    \mfrac{1}{N} \msum_{i \leq N} F_{\bX_i;\psi_\theta}
    \;.
    \tagaligneq \label{eq:OG}
\end{align*}

\noindent
\textbf{Sample size bottlenecked by computational cost.} A key element of our analysis is that the VMC methods are in an \emph{infinite data regime}. More precisely, unlike setups where sample size is constrained by the number of data points --- commonly found in many theoretical analyses of symmetrization techniques \citep{chen2020group,lyle2020benefits,huang2022quantifying} --- we are theoretically allowed to draw infinitely many samples from the sampling step during training. The practical limitation comes from $C_{\rm samp}$ and $C_{\rm grad}$, both of which are affected by the batch size $N$ as well as the symmetrization techniques used. By taking the computational effects into account, we shall see that symmetrization  techniques exhibit substantially different statistical behaviors from those observed in the existing literature.


\subsection{Pitfalls of data augmentation (DA)}


Since ``training data'' correspond to samples drawn from $p^{(m)}_{\psi_\theta}$, a $k$-fold data augmentation is performed as follows:
\begin{enumerate}[topsep=-0.5em, parsep=0em, partopsep=0em, itemsep=0.2em, leftmargin=2em]
    \item[(i)] Sample $N/k$  $\bX_1, \ldots, \bX_{N/k}  \overset{\rm i.i.d.}{\sim} p^{(m)}_{\psi_\theta}$;
    \item[(ii)]Sample $\bg_{1,1}, \ldots, \bg_{N/k,k}$ i.i.d.~from some distribution~on~$\G$;
    \item[(iii)] Compute the DA update as $\theta \mapsto \theta - \delta \theta^{(\rm DA)}$, where  
    \begin{align*}
        \delta \theta^{(\rm DA)} 
        \;\coloneqq\; 
        \mfrac{1}{N} \msum_{i \leq N/k} \msum_{j \leq k}
         F_{\bg_{i,j}(\bX_i); \psi_\theta}
        \;.
    \end{align*}  \\[-1.5em]
\end{enumerate}
Notice that the sample size in (i) is reduced to $N/k$, since a set of $k$-times augmented $N/k$ samples and a set of unaugmented $N$ samples incur 
 the same gradient evaluation cost $C_{\rm grad}$, which dominates the overall computational cost.
 

 While the sampling cost, $C_{\rm samp} /k$, enjoys a minor speed-up, the next result shows that this comes at the cost of increased instability of the gradient estimate. Below, we denote the distribution of $\bX^\bg_1 \coloneqq \bg_{1,1}(\bX_1)$ by 
$p^{(m)}_{\psi_\theta;{\rm DA}}$, and define a distance between two distributions $p, p'$ on $\R^{3n}$ as 
\begin{align*}
    d_\cF(p, p')
    \;=\;
    \msup_{f \in \cF} 
    \| \mean_p[f(\bX)] - \mean_{p'}[f(\bX)] \|\;.
\end{align*}
$\cF$ is a class of $\R^{3n} \rightarrow \R^{p+p^2}$ test functions such that 
\begin{align*}
    \big\{ 
    \bx \mapsto 
    \big( 
        F_{\bx;\psi_\theta}
        \,,\,
        F_{\bx;\psi_\theta}^{\otimes 2}
    \big)
    \,\big|\, 
    \theta \in \R^q 
    \big\} 
    \,\subseteq\, \cF \;.
\end{align*} 
$d_\cF$ is called an integral probability metric \cite{muller1997integral}.


\begin{table}[t]
    \vspace{-.2em}
    \centering 
    \captionsetup{font=footnotesize}
    \footnotesize
    \begin{tabular}{||c | c | c ||c|c||c||}
        \hline 
        Method & $N$ & $k$ & $C_{\rm samp}$ (s) & $C_{\rm grad}$ (s) & Total (s) \\
        \hline 
        OG & $1000$ & - & 0.16(3)   & 2.4(3) & 2.5(3) \\
        DA & $90$ & $12$ & 0.041(5) & 2.4(2) &  2.5(2) \\
        GA & $90$ & $12$ & 0.16(2)  & 2.6(1) & 2.7(1) \\
        GA & $1000$ & $12$ & 1.50(1) & 24(1) & 25(1) \\
        GAs & $1000$ & $1$ & 0.16(1) & 2.4(1) & 2.5(1) \\
        \hline 
    \end{tabular}
    \caption{Computational cost per training step. Details in Sec.~\ref{sec:experiments}.}
    \vspace{-1em}
    \label{table:computational:cost}
\end{table}


\begin{proposition} \label{prop:DA} Fix $\theta \in \R^p$. Then 
\begin{align*}
    &\,\| \mean[ \delta \theta^{(\rm DA)}] - \mean[\delta \theta^{(\rm OG)}] \|
    \;\leq\;
    d_\cF\big( 
        p^{(m)}_{\psi_\theta;{\rm DA}}
        \,,\,
        p^{(m)}_{\psi_\theta} 
    \big)
    \;,
    \\
    &
    \Big\| 
    \Var[ \delta \theta^{(\rm DA)}]
    -
    \Var[  \delta \theta^{(\rm OG)}]
    - 
    \mfrac{(k-1)  \Var \mean[ F_{\bX^\bg_1; \psi_\theta} | \bX_1 ] }{N} 
    \Big\|
    \\
    &
    \hspace{-.1em}\leq \hspace{-.1em}
    \mfrac{ 1 +  2 \| \mean[ \delta \theta^{(\rm OG)} ] \| + d_\cF (  p^{(m)}_{\psi_\theta; {\rm DA}},  p^{(m)}_{\psi_\theta} )   }{N}  d_\cF \big(  p^{(m)}_{\psi_\theta; {\rm DA}},  p^{(m)}_{\psi_\theta} \big).
\end{align*}
In particular, if the distribution $p^{(m)}_{\psi_\theta}$ is invariant under $\Gdiag$, i.e.~$\bg(\bX_1) \overset{d}{=} \bX_1$ for all $\bg \in \Gdiag$,  we have 
\begin{align*}
    \mean[ \delta \theta^{(\rm DA)}] = \mean[ \delta \theta^{(\rm OG)}]
    \;,\;
    \Var[ \delta \theta^{(\rm DA)}] \gtrsim \Var[  \delta \theta^{(\rm OG)}]\;,
\end{align*}
where $\gtrsim$ is the Loewner order of non-negative matrices.
\end{proposition}

The error $d_\cF (  p^{(m)}_{\psi_\theta; {\rm DA}},  p^{(m)}_{\psi_\theta} )$ describes how much an augmented sample from $m$-step chain deviates from an unaugmented sample on average, as measured through the gradient $F_{x;\psi_\theta}$ and the squared gradient $F_{x;\psi_\theta}^{\otimes 2}$. For early training, we expect this error to have small contributions to the overall optimization compared to other sources of noise, e.g., the error incurred by running short chains instead of long chains and by stochastic gradients. At the final steps of training, we expect this error to be small as $p^{(m)}_{\psi_\theta}$ becomes approximately invariant; Fig.~\ref{fig:OG:PA:wf:scan}(a) shows that this is the case for an unsymmetrized, well-trained neural network.

\vspace{.5em}

Several messages follow from \cref{prop:DA}:  

\vspace{.5em}

\noindent 
\textbf{DA leads to similar gradients in expectation but a possibly worse variance}. This is in stark contrast to known analyses of DA in the ML literature, where DA for empirical averages is expected to improve the variance \cite{chen2020group,huang2022quantifying}. This surprising difference arises because those analyses focus on statistical errors arising from augmenting a size-$N$ real-life dataset to a size $Nk$ dataset, whereas our analysis pays attention to both statistical errors and computational errors in a setup that compares a size $N$ dataset versus a size $N/k \times k$ augmented dataset. Indeed, since the only computational saving of augmentation is the sampling cost $C_{\rm samp} \ll C_{\rm optim}$, every augmentation comes at the cost of one i.i.d.~sample unused. 

\vspace{.5em}

\noindent 
\textbf{Instability of DA is not specific to mean and variance}. While \cref{prop:DA} only controls the mean and the variance, they do describe the distributions of $\delta \theta^{(\rm DA)}$ and $\delta \theta^{(\rm OG)}$ well, even \emph{in the high-dimensional regime} where the number of parameters $q$ is large compared to the batch size $N$. This is due to recent results on high-dimensional Central Limit Theorem (CLT): In \cref{thm:DA:CLT} in the appendix, we adapt results from \citet{chernozhukov2017central} to show that $\delta \theta^{(\rm DA)}$ and $\delta \theta^{(\rm OG)}$ are approximately normal in an appropriate sense. This shows that the instability of DA parameter update is a general feature of the distribution, and not just specific to the mean and the variance.

\vspace{.5em}

\noindent 
\textbf{Applicability to multi-step updates}. \cref{prop:DA} concerns one-step gradients, but the analysis applies to multi-step updates: VMC methods draws fresh samples at every step conditionally on the parameter $\theta$ from the previous step, so the same bounds hold with $\mean[\argdot]$ and $\Var[\argdot]$ replaced by the conditional counterparts $\mean[ \argdot | \theta ]$ and $\Var[ \argdot | \theta ]$. 

\vspace{.5em}

One limitation of the above analysis is that it is restricted to first-order updates. Extending similar analyses to second-order updates is a known challenge in the literature: Those methods typically pre-multiply the empirical average $\frac{1}{N} \sum_{i \leq N/k} \sum_{j \leq k}
F_{\bg_{i,j}(\bX_i); \psi_\theta}$ by the inverse of an empirical Fisher information matrix, which is also affected by augmentation, and there exist pathological examples where augmentation may increase or decrease the variance depending on problem-specific parameters (see e.g., ridge regression analysis in \citet{huang2022quantifying}). Nevertheless, we confirm numerically in Fig.~\ref{fig:grad:stab} that the gradient variance under \texttt{KFAC}, a second-order method used by DeepSolid, is also inflated under DA. \cref{table:computational:cost} also verifies that the computational speedup from DA is negligible. Fig.~\ref{fig:graphene:stats} and Table \ref{table:stats} show that training with DA leads to a worse performance. 



\subsection{Group-averaging (GA)}

Fix $\cG$ with $|\cG| = k$ and let $\psi^\cG_\theta$ be the averaged network obtained from taking $\psi=\psi_\theta$ in \eqref{eq:GA:network}. As with DA, since $k$-times more derivatives are computed, we need to use a batch size of $N/k$ to maintain the same computational cost. Draw $\bX^\cG_1, \ldots, \bX^\cG_{N/k} \overset{\rm i.i.d.}{\sim} p^{(m)}_{\psi^\cG_\theta}$. The GA update rule is  \\[-1.2em] 
\begin{align*}
    \theta \mapsto \theta - \delta \theta^{(\rm GA)}
    \;,
    \;
    \delta \theta^{(\rm GA)}
    \coloneqq
    \mfrac{1}{N/k} 
    \msum_{i \leq N/k}
    F_{\bX^\cG_i; \psi^\cG_\theta} 
    \;.
\end{align*} \\[-1em] 
The mean and variance of $\delta \theta^{(\rm GA)}$ is straightforward:

\begin{lemma} \label{lem:GA} Fix $\theta \in \R^p$. Then   \\[-2em] 
\begin{align*}
    \mean[ \delta \theta^{(\rm GA)} ]
    =
    \mean[  F_{\bX^\cG_1; \psi^\cG_\theta}  ]
    \;,
    \;\; 
    \Var[\delta \theta^{(\rm GA)}]
    =
    \mfrac{\Var[  F_{\bX^\cG_1; \psi^\cG_\theta}  ]}{N/k}
    \;.
\end{align*} 
\end{lemma}

\begin{remark} As with DA, we can mean and variance as proxies to understand the distribution of $\delta \theta^{(\rm GA)}$ since a high-dimensional CLT does apply here; see \cref{appendix:CLT}.
\end{remark}

 
\begin{figure}[t]
    \vspace{-.1em}
    \centering
    \begin{tikzpicture}
        \node[inner sep=0pt] at (0,0) {\includegraphics[trim={.2cm .5cm .2cm .3cm},clip,width=.95\columnwidth]{figs/canon_1d.pdf}};
        \node[inner sep=0pt] at (-4.1,0.15){\scriptsize $f(x)$};
        \node[inner sep=0pt] at (0.18,-0.8){\scriptsize $x$};
    \end{tikzpicture}
    \vspace{-1.2em}
    \centering
    \caption{
        Canonicalization functions for 1d unit translations. 
    }
    \label{fig:canon:1d}
    \vspace{-1.1em}
\end{figure}

\noindent
\textbf{GA may also destabilize gradients.} The decrease in sample size is again visible in the variance in \cref{lem:GA}, which suffers from a $\sqrt{k}$ blowup. Notice that the reference mean and variance are stated in terms of the gradient $F_{\bX^\cG_1; \psi^\cG_\theta}$, which depends on the GA wavefunction both through the samples $\bX^\cG_1$ and through the gradient evaluation at $\psi^\cG_\theta$. Unlike the discussion in \cref{prop:DA}, we no longer expect that the mean and variance of $F_{\bX^\cG_1; \psi^\cG_\theta}$ are close to the unsymmetrized analogue $F_{\bX_1; \psi_\theta}$, since it does not suffice for $\bX^\cG_1$ and $\bX_1$ to have similar distributions. In general, $\Var[\delta \theta^{(\rm GA)}]$ increases if and only if the ratio  \\[-1.3em]
\begin{align*}
    \mfrac{\Var[F_{\bX^\cG_1; \psi^\cG_\theta} ]}{\Var[F_{\bX_1; \psi_\theta} ]} 
    \;>\;
    \mfrac{1}{k}
    \;. 
\end{align*}  \\[-1.em]
Empirically, we see that a variance increase for $\delta \theta^{(\rm GA)}$ is visible for \texttt{KFAC} in Fig.~\ref{fig:grad:stab} compared to $\delta \theta^{(\rm OG)}$ with similar computational costs (Table \ref{table:computational:cost}).

\vspace{.5em}

We also include two further comparisons: \\[.5em]
\textbf{GA with subsampling (GAs).} One way to circumvent this computational hurdle is to average over a size-$k$ uniform subsample of $\cG$ at every training step, and use the full $\cG$ at inference time. We numerically investigate the effects of keeping $N$ constant and uniformly sample $k=1$ element: While GAs further destabilizes the gradient (Fig.~\ref{fig:grad:stab}), \cref{table:stats} shows that its performance improves from OG. Yet, it falls short of the performance obtained by post hoc averaging directly on the original wavefunction.  \\[.5em]
\textbf{GA with same $N$.} Say the batch size $N$ is kept the same. While the per-step cost of a $k$-fold averaging increases by $\approx k$ times (Table \ref{table:computational:cost}), one may ask if the number of steps until convergence may be reduced under the symmetrized wavefunction, such that the overall training cost is constant.  Fig.~\ref{fig:grad:stab} shows that while such a reduction does appear, it is not enough to offset the increase in per-step cost.

\subsection{Smoothed canonicalization (SC)} \label{sec:canon}



Another symmetrization method that gained traction in the theoretical ML community is canonicalization, defined as the projection to the fundamental region of a given group \citep{kaba2023equivariance,dym2024equivariant}. For 1d unit translations, an example of  canonicalization is the map $x \mapsto x \textrm{ mod } 1$, illustrated in blue in Fig.~\ref{fig:canon:1d}. 
Canonicalization for space groups $\G_{\rm sp}$ is possible but suffers from non-smoothness at the boundary, as visible at $0$ and $1$ in Fig.~\ref{fig:canon:1d} and as we show in \cref{appendix:canon}. Jastrow factors \cite{whitehead2016jastrow} from VMC methods can be viewed as a way to smooth 1d canonicalization along each lattice vector, but the construction is specific to translations. \citet{dym2024equivariant} proposes a smoothed canonicalization (SC) for large permutation and rotation groups by taking weighted averages at the boundary. In \cref{appendix:canon}, we adapt their idea to develop an SC for diagonal invariance under $\G_{\rm sp}$ that also respects the anti-symmetry constraint of \eqref{eq:schrodinger}. The orange curve in Fig.~\ref{fig:canon:1d} illustrates our method for 1d translations. However, we demonstrate in \cref{appendix:canon} that SC via weighted averaging requires averaging over $n \times |\cG_\epsilon|$ elements, where $\cG_\epsilon$ is some carefully chosen subset of $\G_{\rm sp}$ and $n$ is the number of electrons; the additional cost of $n$ arises from an anti-symmetry requirement. Therefore, SC suffers from similar computational bottlenecks as DA and GA and typically to a worse extent. This renders SC unsuitable for training. Nonetheless, the availability of an SC for $\Gdiag$ may be of independent interest, and we provide theoretical guarantees and discussions on its weaknesses in \cref{appendix:canon}.



\section{Post hoc symmetrization} \label{sec:symm:inference}

\noindent
An alternative to in-training symmetrization is post hoc symmetrization: We may first train $\hat \theta$ with unsymmetrized updates (e.g.~\eqref{eq:OG}), and seek to symmetrize $\psi_{\hat \theta}$ during inference. In contrast to Sec.~\ref{sec:symm:training}, post hoc symmetrization no longer incurs the cost of $C_{\rm grad}$. While computing properties based on Monte Carlo estimates still incurs $C_{\rm sample}$, samples are typically obtained in large batches from long chains only once, before being used for multiple downstream computations. This allows us to perform a moderate amount of averaging without compromising on sample size.  Meanwhile, a direct evaluation of the wavefunction, e.g.~for producing the visualization in Fig.~\ref{fig:OG:PA:wf:scan}, also does not incur $C_{\rm sample}$.

\vspace{.5em}

A wavefunction $\psi_\theta$, as a physical object, is deterministic and should not involve exogeneous randomness. As such, we do not consider DA for post-processing, and discuss only group averaging and canonicalization in this setup.

\vspace{.5em}

\noindent
\textbf{Post hoc averaging (PA). } Since the cost of averaging still scales linearly with $|\G|$, taking an average over the entire $\G$ can still be prohibitive when $\G$ is large. Fortunately, \cref{fact:inv:soln:exists} ensures the validity of averaging over any finite subset $\cG \subseteq \G$ of our choice. One may choose $\cG$ to be a subgroup of interest, or a generating set of $\G$. Given a \emph{trained} wavefunction $\psi_{\hat \theta}$, the corresponding PA wavefunction reads  \\[-1em]
\begin{align*}
    \psi^{({\rm PA};\cG)}_{\hat \theta} 
    (\bx)
    \;\coloneqq\; 
    \mfrac{1}{|\cG|} \msum_{g \in \cG} \psi_{\hat \theta}(g(\bx))
    \;.
\end{align*}  \\[-1em]
Estimates of physical quantities are obtained by drawing $N$ samples from $p_{\psi^{({\rm PA};\cG)}_{\hat \theta} }$, incurring a computational cost of $N\, |\cG| \, C_{\rm samp}$ per MCMC step. For $\cG$ of a moderate size, we can compare $N$ samples from $p_{\psi^{({\rm PA};\cG)}_{\hat \theta}}$ directly with $N$ samples from $p_{\psi_{\hat \theta}}$, without incurring a larger statistical error.  $\psi^{({\rm PA};\cG)}_{\hat \theta}$ offers two clear advantages over $\psi_{\hat \theta}$: 

\noindent
(i) \emph{More symmetry}. By  construction, $\psi^{({\rm PA};\cG)}_{\hat \theta}$ exhibits a higher degree of symmetry than $\psi_{\hat \theta}$, the extent of which depends on the subset $\cG$ chosen and the degree of symmetry already present in $\psi_{\hat \theta}$. See Fig.~\ref{fig:OG:PA:wf:scan} for the improved symmetry.

\begin{figure}[t]
    \centering
    \vspace{-.2em}
    \hspace{-.2em}
    \begin{tikzpicture}
        \node[inner sep=0pt] at (-3.4,0) {\includegraphics[trim={.51cm .4cm .3cm .39cm},clip,height=15em]{figs/graphene1_3e4_energy_gpuhrs.pdf}};
        \node[inner sep=0pt] at (-0.1,0) {\includegraphics[trim={.4cm .4cm .2cm .34cm},clip,height=15em]{figs/graphene1_3e4_variance_gpuhrs.pdf}};
        \node[inner sep=0pt] at (1.85,0) {\includegraphics[trim={.4cm .4cm .2cm .4cm},clip,height=15em]{figs/graphene1_3e4_symm_ratio_var_gpuhrs.pdf}};

        \node[inner sep=0pt] at (-3.05, -3.1){\scriptsize \textbf{(a)} Energy (Ha)};

        \node[inner sep=0pt] at (-0.1, -3.1){\scriptsize \textbf{(b)} $\Var[E_{\rm local}]$ ($\text{Ha}^2$)};

        \node[inner sep=0pt] at (2.0, -3.1){\scriptsize \textbf{(c)} Symmetry};
    \end{tikzpicture}
    \vspace{-1.5em}
    \caption{ Performance of wavefunctions against GPU hours, obtained with different symmetrization methods. Metrics are defined in Sec.~\ref{sec:eval} and experiment details in Sec.~\ref{sec:experiments}. (c) is computed via $\Var[{\rm PA}/ {\rm OG}]$ in Sec.~\ref{sec:eval} but with $\psi_{\theta}^{(\rm OG)}$ replaced by different $\psi_{\theta}$'s. 
    }
    \vspace{-1.6em}

    \label{fig:graphene:stats}
\end{figure}

\vspace{.5em}

\noindent
(ii) \emph{Robustness to outliers}. The ground-truth wavefunctions are required to be non-smooth whenever an electron coincides with an atom or an electron. At those regions of $\bx$, the Hamiltonian $H \psi_\theta(\bx)$ diverges, but for ground-truth wavefunctions as well as carefully constrained classical wavefunction ansatz, the local energy $E_{\rm local; \psi_\theta}(\bx) = H \psi_\theta(\bx) / \psi_\theta(\bx)$ in \eqref{eq:VMC} stays finite due to the renormalization by $\psi_\theta(\bx)$. The same is not necessarily true for neural network solvers, which are much more flexible by design: The Laplacian term in $H \psi_\theta(\bx)$ may become large even when the magnitude of $\psi_\theta(\bx)$ stays put. This issue also manifests in the evaluation of other empirical quantities, e.g., the gradient terms computed in MCMC. The averaging in $\psi^{({\rm PA};\cG)}_{\hat \theta}$ improves robustness in the sense that, if $\psi_{\hat \theta}(g(\bx))$ takes a large numerical value for one particular $g \in \cG$ and not for the rest, the numerical outlier is rescaled by a factor $\frac{1}{|\cG|}$. The level of robustness also increases with $|\cG|$. 


\begin{table*}[t]
    \vspace{-.2em}
    \centering
    \captionsetup{font=footnotesize}
    \scriptsize
    \begin{tabular}{||c|c|c|c|c|c|c||c|c||c||}
        \hline 
        System & $\cG$ & Method  & $N$ & $k$ & 
         Steps & $\text{GPU hours}^*$ & Energy (Ha) & $\Var[E_{\rm local}]$ ($\text{Ha}^2$) & $\Var[{\rm PA}/ {\rm OG}]$ \\
        \hline
        \multirow{7}{*}{
            \parbox{1.5cm}{
                \centering 
                Graphene \\ 
                $1\times1$
            }
        }
        &
        -
        & OG & $1000$ & - & $80,000$ &
        $281$
        & $-76.039(6)$ & $2.02(3)$ &
        \\
        &
        \multirow{6}{*}{
            \parbox{1cm}{
                \centering 
                \texttt{P6mm}
            }
        }
        & DA & $90$ & $12$ & $80,000$  & $277$ & $-76.039(3)$ & $2.48(5)$ & 
        \\
        & & GA & $90$ & $12$ & $80,000$ &  $304$ & $\mathbf{-76.049(3)}$ & $0.58(2)$ & 
        \\
        & & GA & $1000$ & $12$ & $10,000$ & $351$ & $-76.034(5)$ & $1.2(2)$ & 
        \\
        & & GAs & $1000$ & $1$ & $80,000$ & $281$  & 
        $\mathbf{-76.049(3)}$ &
        $0.48(2)$ &  
        \\[.1em]
        & & PA & $1000$ & $12$ & $80,000$ & $281$ & $\mathbf{-76.050(3)}$ & $\mathbf{0.33(1)}$ & $0.00180(4) $ 
        \\
         & & PC & $1000$ & $12$ & $80,000$ & $281$ & $-70.1(9)$ & $8(1) \times 10^2$ & 
        \\
        \hline
        \multirow{6}{*}{
            \parbox{1.5cm}{
                \centering 
                Lithium 
                Hydride
                \\ (LiH) \\ 
                $2\times2 \times 2$ 
            }
        }
        & - & OG & 
        \multirow{6}{*}{
             \parbox{2em}{
                 \centering 
                 $4000$
             }
         }  & - & 
         \multirow{6}{*}{
              \parbox{3.2em}{
                  \centering 
                  $30,000$
              }
          }  & 
          \multirow{6}{*}{
              \parbox{2em}{
                  \centering 
                  $571$
              }
          }
          & $-8.138(2)$ & $0.06(1)$ & 
        \\
        & \texttt{P\={1}}  & PA &  & $2$ & & & $-8.144(1)$  & $0.0344(9)$  & $0.0183(4)$
        \\
        & \texttt{P2/m} & PA & & $4$ & & & $-8.148(1)$ & $0.0197(6)$ & $0.0249(5)$
        \\
        & \texttt{F222} & PA & & $16$ & & & $\mathbf{-8.1495(9)}$ & $0.0162(7)$ & $0.0265(5)$
        \\
        & \texttt{Pm\={3}m} & PA & & $48$ & & &  $\mathbf{-8.1502(7)}$  & $\mathbf{0.0122(7)}$  & $0.0289(8)$
        \\
        & \texttt{Fm\={3}m} & PA & & $192$ & & &  $\mathbf{-8.1507(8)}$  & $\mathbf{0.0118(7)}$  & $0.0289(8)$
        \\ 
        % \cline{1-9}
        \hline
        \multirow{4}{*}{
            \parbox{1.5cm}{
                \centering 
                Metallic Lithium 
                (bcc-Li)
                \\ 
                $2\times2 \times 2$ 
            }
        }
        & - & OG & 
        \multirow{4}{*}{
             \parbox{2em}{
                 \centering 
                 $3000$
             }
         }  & - & 
         \multirow{4}{*}{
              \parbox{3.2em}{
                  \centering 
                  $20,000$
              }
          }  & 
          \multirow{4}{*}{
              \parbox{2em}{
                  \centering 
                  $462$
              }
          }
          & $-15.011(1)$ & $0.059(2)$ &  
        \\
        & \texttt{P4/mmm} & PA & & $16$ & & & $\mathbf{-15.021(2)}$ & $\mathbf{0.033(2)}$ &  $0.092(5)$
        \\
        & \texttt{Fmmm} & PA & & $32$ & & & $\mathbf{-15.020(1)}$ & $0.036(2)$ &  $0.0101(4)$
        \\
        & \texttt{Im\={3}m} & PA & & $96$ & & & $\mathbf{-15.022(3)}$ & $\mathbf{0.031(3)}$ & $0.0139(6)$ 
        \\
        \hline
    \end{tabular}
    \caption{Performance of symmetrization methods with similar computational budgets.
     Energy and variance are both reported at the per unit cell level. *See \cref{appendix:experiment:parameters} for specifications of the GPUs used for training.
    } \vspace{-1em}
    \label{table:stats}
\end{table*}


Fig.~\ref{fig:graphene:stats} and \cref{table:stats} show that $\psi_{\hat \theta}^{({\rm PA}; \cG)}$ outperforms in-training symmetrization with the same computational costs in all metrics considered. Compare, for example, PA with 40k steps of training with $N=1000$ versus GA with 10k steps of training, $N=1000$ and $k=12$: PA achieves a lower energy with lower variance as well as perfect symmetry, with only $1/4$ of the training budget. Among methods with similar end-of-training energies, PA also attain a lower energy and variance with fewer training steps (Fig.~\ref{fig:graphene:stats}). We also remark that GA and GAs by default implement PA at inference, so their only difference with PA is from training.

\vspace{.5em}

\noindent
\textbf{Issues with post hoc canonicalization (PC).} One may also use smooth canonicalization post hoc to symmetrize a trained wavefunction $\psi_{\hat \theta}$. For completeness, we record the performance of PC in Table \ref{table:stats}. The results are significantly worse than other methods, despite being applied to a well-trained wavefunction. The issue might arise from the fact that a weighted averaging near the boundary leads to a blowup in second derivatives, and we examine it in detail in \cref{appendix:canon}. This makes PC unsuitable specifically for our problem, since $E_{\rm local}$ involves the Hamiltonian. 



\section{Evaluation and visualization methods} \label{sec:eval}

\noindent
VMC wavefunctions are typically assessed via \vspace{-.3em}
\begin{align*}
    \mean[ 
        E_{\rm local; \psi_{ \theta}}(\bX)
    ]
    \;,\;
    \Var[ 
        E_{\rm local; \psi_{ \theta}}(\bX)
    ]\;,
    {\bX \sim p_{\psi_{ \theta}}}\;.
    \tagaligneq \label{eq:energy:var}
\end{align*} \\[-1.5em]
The energy is our optimization objective \eqref{eq:VMC}. The variance is another measure of fit for \eqref{eq:schrodinger}: It admits a lower bound $0$ that is attained by any true solution $\psi_*$ to \eqref{eq:schrodinger}, since $E_{\rm local; \psi_*}$ is everywhere constant. See \citet{kent1999monte}.

\vspace{.5em}

To show the amount of approximate symmetry already present in the OG wavefunction $\psi_{\theta}^{(\rm OG)}$, we compare $\psi_{\theta}^{(\rm OG)}$ against PA wavefunctions averaged over different $\cG$'s. This is reported as $\Var[{\rm PA}/ {\rm OG}]$ in Table \ref{table:stats}, which stands for 
\begin{align*}
    \Var \Big[ \mfrac{1}{|\cG|} \msum_{g \in \cG} \psi_{\theta}^{(\rm OG)}(g(\bX))  \,\big/ \, \psi_{\theta}^{(\rm OG)}(\bX) \Big]
\end{align*}
for ${\bX \sim p_{\psi_{\theta}^{(\rm OG)}}}$.
We also seek to visualize $\psi_{\theta}$ for its symmetry under $\Gdiag$.
Visualizing diagonal symmetry can be challenging, as $\Gdiag$ acts on a high-dimensional space $\R^{3n}$. We propose a visualization method that exploits the fact that our $\Gdiag$ is completely described by a group $\G$ of isometries in $\R^3$:

\vspace{.5em}

\begin{enumerate}[topsep=0em, parsep=0em, partopsep=0em, itemsep=0.2em, leftmargin=2em]
    \item[(i)] Let $\tilde \G$ be a group acting on $\R^3$ defined as \qquad\quad\,\textcolor{white}{:} \\[.4em]
    $
    \big\{ A \in \R^{3 \times 3} \,\big|\, A(\argdot) + b \in \G \text{ for some } b \in \R^3 \big\}\;
    $;
    \item[(ii)] Fix $\tilde \bx_{\rm symm} \in \R^{3n}$, a configuration of $n$ electrons such that for every $g \in \tilde \G$,
    $
        g(\tilde \bx_{\rm symm} ) = \tilde \bx_{\rm symm} \,
    $;
    \item[(iii)] Given a function $f: \R^{3n} \rightarrow \R$ to visualize, we plot the function $\tilde f(t)\coloneqq f( \tilde \bx_{\rm symm} + t )$ with $t \in \R^3$, i.e.~all electrons are translated by $t$ simultaneously.
\end{enumerate}

\vspace{.5em}

The next result confirms the validity of this method:

\begin{lemma} \label{lem:visual} Let $f: (\R^3)^n \rightarrow \R$ be a function invariant under permutations of its $n$ arguments. Then for any $g \in \G$ and $t \in \R^3$,
$
    \tilde f( g(t) ) - \tilde f( t )
    \;=\;
    f(g(\bx + t)) - f(\bx + t)
    \;.
$
\end{lemma}

\vspace{-.5em}

The permutation invariance assumption holds for $| \psi_\theta|$ and $E_{\rm local;\psi_\theta}$ since $\psi_\theta$ is anti-symmetric. Fig.~\ref{fig:OG:PA:wf:scan}(a), (b) and (f) are plotted with this method, with $f(\bx)=\log | \psi_\theta(\bx) |^2$, $\tilde \bx_{\rm symm}$ given in Fig.~\ref{fig:OG:PA:wf:scan}(e) and $\G = \tilde \G = \texttt{P3m1}$, which illustrates the \emph{partial} symmetries of the \texttt{P6mm} group of graphene. To see the $\G=\texttt{P6mm}$ symmetry, a different $\tilde \bx_{\rm symm}$ is required since $\tilde \G \neq \G$ in this case; see \cref{appendix:diag:inv:illustration}.

\section{Experimental details} \label{sec:experiments}

\noindent
Experiments are performed with DeepSolid \cite{li2022ab} on crystalline solids. Each network is evaluated by sampling from MCMC chains with 30k length, and the model from the last training step is used unless otherwise specified. Supercell size is included in the first column of Table \ref{table:stats}. \cref{appendix:experiments} includes further specifications, experiments and a remark about why energy improvements in the decimals are considered substantial. A few remarks about each system: 

\vspace{.5em}

\noindent
\textbf{Graphene.} This is the setup considered in Fig.~\ref{fig:OG:PA:wf:scan}, \ref{fig:grad:stab}, \ref{fig:graphene:stats} and Table \ref{table:computational:cost}. PA outperforms other methods both in terms of the metrics in Table \ref{table:stats} and speed of convergence in Fig.~\ref{fig:graphene:stats}. 
\vspace{.5em}

\noindent
\textbf{LiH.} We examine nested subgroups of \texttt{Fm\={3}m} and observe that the performance improves with $k$. For comparison, \citet{li2022ab} report the energy $-8.15096(1)$ for DeepSolid trained with 3e5 steps and batch 4096. PA attains comparable performance in similar systems\footnote{The energy by \citet{li2022ab} is for lattice vector $4.0$ \r{A}. We followed the Materials Project \cite{jain2013commentary} to use $4.02$ \r{A}.
}~with 3e4 steps and batch 4000. 
\vspace{.5em}

\noindent
\textbf{bcc-Li.} This is a known difficult case for ab initio methods including DeepSolid \citep{yao1996pseudopotential,li2022ab} and PA again helps. \texttt{Fmmm} and \texttt{P4/mmm} are subgroups of \texttt{Im\={3}m} containing different symmetries, and each offers similar improvements. For both LiH and bcc-Li, we also observe a saturation effect: The improvement saturates once sufficiently many symmetries are incorporated. 

\vspace{-.5em}

\section*{Acknowledgement}

\noindent
This work was partially supported by NSF OAC 2118201. KHH and PO are supported by the Gatsby Charitable Foundation (GAT3850). NZ acknowledges support from the Princeton AI$^2$ initiative. This work used Princeton ionic cluster and Delta GPU at the National Center for Supercomputing Applications through allocation MAT220011 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services \& Support (ACCESS) program, which is supported by National Science Foundation grants \#2138259, \#2138286, \#2138307, \#2137603, and \#2138296.

\clearpage

\bibliography{ref}
\clearpage 
\appendix
\onecolumn

\input{appendix}

\end{document}