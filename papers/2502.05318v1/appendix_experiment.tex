
\section{Experimental details and additional results}  \label{appendix:experiments}

\subsection{DeepSolid architecture} \label{appendix:DeepSolid}

All our experiments use DeepSolid \cite{li2022ab} as $\psi_\theta$, the baseline unsymmetrized network. DeepSolid adapts FermiNet, a molecular neural network ansatz \citep{pfau2020ab}, to infinite periodic solids. We briefly review their architectures. 

\vspace{.5em}

Architecture-wise, FermiNet takes as inputs $n$ electron positions and split them into two groups according to spins. For some distance function $d$, the electrons are encoded through feature maps of the form $d(x_j-x_k)$ and $d(x_j-r_K)$, i.e.~electron-electron interactions and electron-nucleus interactions, which are passed into a fully-connected network with $\tanh$ activations. The outputs are orbital information at the single-electron level that depends on electron-electron interactions, which are combined by a weighted sum of Slater determinants to form antisymmetric many-electron wavefunctions. For details, see Fig.~1 and Algorithm 1 of \cite{pfau2020ab}.

\vspace{.5em}

DeepSolid makes several adaptations of FermiNet that are crucial for modelling periodic solids. Of those, the most relevant one to our discussion are their choice of periodic features $d(x_j-x_k)$ and $d(x_j-x_K)$, stated in (2)~of \cite{pfau2020ab}. Their periodic features introduce separate invariance with respect to supercell translations, such that the $n$ electrons are effectively restricted within the supercell. The construction is based on Jastrow correlation factors \cite{whitehead2016jastrow} and can be viewed as a version of smoothed canonicalization for 1d translations (\cref{sec:canon}).


\subsection{Physical systems}

\begin{table}[h]
    \centering
    \captionsetup{font=footnotesize}
    \begin{tabular}{|c||c|c|c|}
        \hline
         Physical systems & Graphene $1 \times 1$ & LiH $2 \times 2 \times 2$  & bcc-Li $2 \times 2 \times 2$ \\
         \hline
         Number of electrons & $12$ & $32$ & $48$ \\
         \hline 
         Point group symmetry of the system & \texttt{P6mm} & \texttt{Fm\={3}m}  & \texttt{Im\={3}m} \\
         \hline 
         Number of point group elements & $12$ & $192$ & $96$ \\
         \hline
         Lattice vector length (conventional) 
         & 
         \parbox{4em}{ \vspace{.1em} \centering $2.4612$ \r{A} \vspace{.2em} }
         &
         \parbox{4em}{ \vspace{.1em} \centering $4.02$ \r{A}  \vspace{.2em} }
         &
         \parbox{4em}{ \vspace{.1em} \centering $3.4268$ \r{A}  \vspace{.2em}  }
         \\
         \hline
    \end{tabular}
    \caption{List of physical systems considered in the experiments, where the number $a \times b$ indicates the size of the supercell for a 2d system and $a \times b \times c$ is that for a 3d system. The primitive cell (corresponding to $1 \times 1 \times 1$ supercell) of graphene is visualized in Fig.~\ref{fig:OG:PA:wf:scan}(d), and those of LiH and bcc-Li are in Fig.~\ref{fig:LiH:bccLi:cells}(a) and Fig.~\ref{fig:LiH:bccLi:cells}(e). }
    \label{tab:physical:systems}
    \vspace{-.1em}
\end{table}

\begin{figure}[h]
    \centering
    \vspace{-.5em}
    \begin{tikzpicture}
        \node[inner sep=0pt] at (-5.5,0) {\includegraphics[trim={5.5cm .7cm 9cm 3.5cm},clip,width=.3\linewidth]{figs/LiH-1.pdf}};   
        \node[inner sep=0pt] at (0,0) {\includegraphics[trim={5.5cm .7cm 9cm 3.5cm},clip,width=.3\linewidth]{figs/LiH-1-extra-atoms.pdf}};   
        \node[inner sep=0pt] at (5.5,0) {\includegraphics[trim={5.5cm .7cm 9cm 3.5cm},clip,width=.3\linewidth]{figs/LiH-1-g2.pdf}};   

        \node[inner sep=0pt] at (-5.5,-3.2){\scriptsize \textbf{(a) LiH $1 \times 1 \times 1$ primitive cell}};
        \node[inner sep=0pt, align=center] at (0, -3.2){\scriptsize \textbf{(b) LiH $1 \times 1 \times 1$ with atoms from} \\ \scriptsize \textbf{adjacent cells to show the cubic structure}};
        \node[inner sep=0pt] at (5.5,-3.2){\scriptsize \textbf{(c) \texttt{P\={1}} symmetry of LiH $1 \times 1 \times 1$}};

        \node[inner sep=0pt] at (-5.5,-6.6) {\includegraphics[trim={5.5cm .7cm 9cm 3.5cm},clip,width=.3\linewidth]{figs/LiH-1-g10.pdf}};  
        \node[inner sep=0pt] at (0,-6.6) {\includegraphics[trim={5.5cm .7cm 9cm 3.5cm},clip,width=.3\linewidth]{figs/bcc-Li-1.pdf}};   
        \node[inner sep=0pt] at (5.5,-6.6) {\includegraphics[trim={5.5cm .7cm 9cm 3.5cm},clip,width=.3\linewidth]{figs/bcc-Li-1-extra-atoms.pdf}};   

        \node[inner sep=0pt] at (-5.5,-9.8){\scriptsize \textbf{(d) \texttt{P2/m} symmetry of LiH $1 \times 1 \times 1$}};

        \node[inner sep=0pt] at (0, -9.8){\scriptsize \textbf{(e) bcc-Li $1 \times 1 \times 1$  primitive cell}};
        \node[inner sep=0pt, align=center] at (5.5,-9.8){\scriptsize \textbf{(f) bcc-Li $1 \times 1 \times 1$ with atoms from} \\ \scriptsize \textbf{adjacent cells to show the cubic structure} };
    \end{tikzpicture}
    \caption{Visualization of the primitive cells of LiH and bcc-Li.}
    \label{fig:LiH:bccLi:cells}
\end{figure}

\noindent
The experimental benchmark used for comparing all symmetrization strategies is graphene with an $1 \times 1$ supercell for computational convenience. Effects of post-hoc averaging over different choices of $\cG$ are illustrated through LiH and bcc-Li, which possess substantially more symmetries. Two types of simple symmetries in LiH, \texttt{P\={1}} and \texttt{P2/m}, are illustrated in Fig.~\ref{fig:LiH:bccLi:cells}(c) and (d); we refer readers to \cite{brock2016international} for an exhaustive visualization of all 2d and 3d space groups.

\subsection{Parameter settings} \label{appendix:experiment:parameters}

The symmetrization strategies we consider, i.e.~DA, GA, SC, PA and PC, are all implemented as wrapper functions around the original DeepSolid model. Architecturally this can be viewed as inserting one layer each before and after the DeepSolid processing pipeline. Notably, this allows us to leave most of the training parameters from DeepSolid unchanged and retain the performance from the original DeepSolid.

\vspace{.5em}

To ensure a fair comparison, we keep the default network and training parameter settings from DeepSolid across all our experiments, except that we vary the training batch size according to $k$, the number of symmetry operations used. Graphene training uses NVIDIA GeForce RTX 2080 Ti (12GB) and LiH and bcc-Li use NVIDIA A100 SXM4 (40GB).

\vspace{.5em}

At the inference stage, we collect samples from independent MCMC chains with length $30,000$. The numbers of samples collected are respectively $50,000$ for graphene and $20,000$ for LiH and bcc-Li. The choice of smoothing in post-hoc canonicalization is $s_\infty$ defined in \cref{appendix:lambda:d}; see \cref{appendix:canon} for details.

\vspace{-.5em}

\subsection{Additional results} \label{appendix:add:fig}

\textbf{A remark about the scale of energy improvements.} Notice that the energy improvements reported in \cref{table:stats} are in the third decimals in Hartree. Improvements at this scale are crucial in %VMC 
physics and chemistry. In the physical systems we consider, core electron binding energies are on the order of keV ($1$ keV $\approx 37$ Hartree), while valence electron binding energies are in the range of $1-10$ eV ($\approx 0.037$ Hartree). To obtain wavefunctions with accurate ground state energy, it is therefore crucial to obtain energy improvements on the order of $10^{-3}$ Hartree or smaller. In fact, the term ``chemical accuracy" -- used in computational chemistry to describe the level of precision needed for calculated energies to provide meaningful predictions of chemical phenomena, is given by $1$ kcal / mol ($\approx 0.00159$ Hartree). For more references, see \citet{williams2020direct,perdew1996generalized}.

\vspace{.2em}

\textbf{Varying number of subsamples in GAs.} In \cref{table:stats} and Fig.~\ref{fig:graphene:stats}, group-averaging with subsampling (GAs) achieves the closest performance to post hoc averaging on graphene $1 \times 1$. In Fig.~\ref{fig:subsample:additional} below, we verify that the performance does not improve if one varies $k$, the number of subsampled group elements, and that a larger $k$ also leads to gradient destabilization.

\begin{figure}[h]
    \centering
    \vspace{-.6em}
    \begin{tikzpicture}
        % \node[inner sep=0pt,rotate=90] at (-4.2,0){\scriptsize $y$ (Bohr)};
        \node[inner sep=0pt] at (0,0) {\includegraphics[trim={1cm .2cm .8cm .5cm},clip,height=16em]{figs/gradient_stab_std_gpuhrs_additional.pdf}};
        % \node[inner sep=0pt,rotate=90] at (-4,0){\scriptsize $\frac{1}{\sqrt{q}} \| \Var[\delta \theta] \|$};
        % \node[inner sep=0pt] at (0,-4) {\includegraphics[trim={0 .2cm 0 .9cm},clip,width=.95\columnwidth]{figs/gradient_stab_mean.pdf}};
        % \node[inner sep=0pt,rotate=90] at (-4.3,-4.5){\scriptsize param mean diff from OG};
        \node[inner sep=0pt] at (0,-3.6){\scriptsize \textbf{(a) $\frac{1}{\sqrt{q}} \| \Var[\delta \theta] \|$, normalized variance of gradient updates} };
        \node[inner sep=0pt] at (7,0) {\includegraphics[trim={.51cm .4cm .3cm .39cm},clip,height=16em]{figs/graphene1_3e4_energy_gpuhrs_additional.pdf}};
        \node[inner sep=0pt] at (7.5,-3.6){\scriptsize \textbf{(b) Energy (Ha)} };
        \node[inner sep=0pt] at (10.5,0) {\includegraphics[trim={.4cm .4cm .2cm .34cm},clip,height=16em]{figs/graphene1_3e4_variance_gpuhrs_additional.pdf}};
        \node[inner sep=0pt] at (10.,-3.6){\scriptsize \textbf{(c)  $\Var[E_{\rm local}]$ ($\text{Ha}^2$)} };
    \end{tikzpicture}
    % \vspace{-.6em}
    \caption{Performance of GAs plotted against GPU hours and across different subsample size $k$.
    }
    \vspace{.5em}
    \label{fig:subsample:additional}
\end{figure}

\vspace{.2em}

\textbf{Averaging over diagonal translations.} The results in \cref{table:stats} focus on averaging over point groups, and one may ask whether averaging additionally over translations helps. We perform a preliminary investigation in LiH and bcc-Li, and find that incorporating translations does not lead to significant performance improvement. The results are indicated in \cref{table:stats:translate} by $\times \T$, and we include the corresponding results for point groups without translations for comparison. Note that as averaging over translations incurs $8 \times$ of the computational cost, we have computed the statistics only on $10,000$ samples.


\begin{table*}[h]
    % \vspace{-.2em}
    \centering 
    \scriptsize
    \captionsetup{font=footnotesize}
    \begin{tabular}{||c|c|c|c|c|c|c||c|c||}
        \hline 
        System & $\cG$ & Method  & $N$ & $k$ & 
         Steps & GPU hours & Energy (Ha) & $\Var[E_{\rm local}]$ ($\text{Ha}^2$) \\
        \hline
        \multirow{3}{*}{
            \parbox{1.5cm}{
                \centering 
                LiH \\
                $2\times2 \times 2$ 
            }
        }
        & - & OG & 
        \multirow{3}{*}{
             \parbox{2em}{
                 \centering 
                 $4000$
             }
         }  & - & 
         \multirow{3}{*}{
              \parbox{3.2em}{
                  \centering 
                  $30,000$
              }
          }  & 
          \multirow{3}{*}{
              \parbox{2em}{
                  \centering 
                  $571$
              }
          }
          & $-8.138(2)$ & $0.06(1)$ 
        \\
        & \texttt{Pm\={3}m} & PA & & $48$ & & &  $\mathbf{-8.1502(7)}$  & $\mathbf{0.0122(7)}$  
        \\
        & \texttt{Pm\={3}m} $\times \T$ & PA & & $384$ & & &  
        $-8.1488(7)$
        & 
        $0.012(1)$
        \\ 
        \cline{1-9}
        \multirow{3}{*}{
            \parbox{1.5cm}{
                \centering 
                bcc-Li
                \\ 
                $2\times2 \times 2$ 
            }
        }
        & - & OG & 
        \multirow{3}{*}{
             \parbox{2em}{
                 \centering 
                 $3000$
             }
         }  & - & 
         \multirow{3}{*}{
              \parbox{3.2em}{
                  \centering 
                  $20,000$
              }
          }  & 
          \multirow{3}{*}{
              \parbox{2em}{
                  \centering 
                  $462$
              }
          }
          & $-15.011(1)$ & $0.059(2)$ 
        \\
        & \texttt{P4/mmm} & PA & & $16$ & & & $\mathbf{-15.021(2)}$ & $\mathbf{0.033(2)}$ 
        \\
        & \texttt{P4/mmm}$\times \T$ & PA & & $128$ & & & $-15.017(6)$ & $0.05(2)$  
        \\
        \hline
    \end{tabular}
    \caption{Performance of post hoc averaging with translations.}
    \vspace{-1em}
    \label{table:stats:translate}
\end{table*}

\vspace{.5em}



\textbf{Averaging over subsets of group elements.} \cref{fact:inv:soln:exists} ensures the validity of averaging over any \emph{subset} of group elements and not just subgroups. The only caveat is that for a general finite subset $\cG$, the average $\frac{1}{|\cG|} \sum_{g \in \G} \psi(g(x))$ is no longer guaranteed to be invariant under $\cG$. Nevertheless, one may still ask if averaging over subsets of a group $\G$ that are ``sufficiently representative'', e.g.~the generators of the group $\G$, can be helpful. We perform a preliminary investigation on graphene and LiH for PA with $\textrm{Gen}(\G)$, a fixed set of generators of $\G$ plus the identity element. The statistics are computed on $40,000$ samples and reported in \cref{table:stats:subset}. The results are inconclusive: We find that for graphene, PA with $\textrm{Gen}(\G)$ improves energy but significantly inflates variance, whereas for LiH, PA with $\textrm{Gen}(\G)$ has worse energy and variance. As a sanity check, we also compute $\Var[ {\rm PA}^{\G} / \psi_\theta ]$ for each wavefunction to verify that the PA with $\textrm{Gen}(\G)$ is closer to PA computed on $\G$ compared to the original wavefunction. 

\vspace{.5em}

\begin{table*}[h]
    \centering 
    \scriptsize
    \captionsetup{font=footnotesize}
    \begin{tabular}{||c|c|c|c|c|c|c||c|c|| c ||}
        \hline 
        System & $\cG$ & Method  & $N$ & $k$ & 
         Steps & GPU hours & Energy (Ha) & $\Var[E_{\rm local}]$ ($\text{Ha}^2$) & $\Var[ {\rm PA} / \psi_\theta ]$  \\ 
        \hline
        \multirow{3}{*}{
            \parbox{1.5cm}{
                \centering 
                Graphene \\ 
                $1\times1$
            }
        }
        &
        -
        & OG &
        \multirow{3}{*}{
            \parbox{2em}{
                \centering 
               $1000$
            }
        }
         & - & 
        \multirow{3}{*}{
            \parbox{3.2em}{
                \centering 
               $80,000$
            }
        }
         &
         \multirow{3}{*}{
            \parbox{2em}{
                \centering 
               $281$
            }
        }
        & $-76.039(6)$ & $2.02(3)$ & $1.80(4) \times 10^{-3}$
        \\
        &
        \texttt{P6mm}
        & PA & & $12$ & & & $-76.050(3)$ & $\mathbf{0.33(1)}$  & $\mathbf{0.0}$
        \\
        &
        Gen(\texttt{P6mm})
        & PA &  & $4$ & & & $\mathbf{-76.064(5)}$ & $1.04(2)$ & $3.3(1) \times 10^{-4}$
        \\
        \hline
        \multirow{3}{*}{
            \parbox{1.5cm}{
                \centering 
                LiH \\
                $2\times2 \times 2$ 
            }
        }
        & - & OG & 
        \multirow{3}{*}{
             \parbox{2em}{
                 \centering 
                 $4000$
             }
         }  & - & 
         \multirow{3}{*}{
              \parbox{3.2em}{
                  \centering 
                  $30,000$
              }
          }  & 
          \multirow{3}{*}{
              \parbox{2em}{
                  \centering 
                  $571$
              }
          }
          & $-8.138(2)$ & $0.06(1)$ & $2.65(5) \times 10^{-2}$
        \\
        & \texttt{F222} & PA & & $16$ & & &  $\mathbf{-8.1495(9)}$ & $\mathbf{0.0162(7)}$ & $\mathbf{0.0}$
        \\
        & Gen(\texttt{F222}) & PA & & $5$ & & & $-8.1456(7)$ & $0.0235(5)$ & $6.6(1) \times 10^{-3}$
        \\ 
        \hline
    \end{tabular}
    \caption{Performance of post hoc averaging with subsets of group elements.} \vspace{-2em}
    \label{table:stats:subset}
\end{table*}


% \clearpage
