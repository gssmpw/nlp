
\section{Proof of Fact~\ref{fact:inv:soln:exists}} \label{appendix:proof:inv:soln}

\noindent
Fix $g \in \G$ with its action on $x \in \R^3$ represented by $g(x) = A x + b$. Given that $(\psi, E)$ solves \eqref{eq:schrodinger}, we seek to show that under the stated conditions, $\psi_g(\bx) \coloneqq \psi(g(\bx))$ is also an anti-symmetric solution to \eqref{eq:schrodinger} with respect to the same energy $E$. For $\bx \in \R^{3n}$ and $g \in \G$, denote $\bx_g = g(\bx)$. Then for every $\bx \in \R^{3n}$, 
\begin{align*}
    \Big( - \mfrac{1}{2} \nabla^2 + V(\bx) \Big) \, \psi_{g}( \bx )
    \;=&\; 
    \Big( - \mfrac{1}{2} \nabla^2 + V(\bx) \Big) \, \psi(  A x_1 + b, \ldots, A x_n + b )
    \\
    \;=&\;  
    - \mfrac{1}{2} (A^\top A) \nabla^2  \psi( \bx_{g} )
    + V( \bx ) \psi( \bx_{g} )
    \\
    \;\overset{(a)}{=}&\;
    - \mfrac{1}{2} \nabla^2  \psi( \bx_{g})
    + V( \bx_{g} ) \psi( \bx_{g})
    \;\overset{(b)}{=}\; 
    E \psi( \bx_{g} )
    \;=\; 
    E \psi_{g}( \bx )
    \;.
\end{align*}
In $(a)$, we have used the $\G_\diag$-invariance of $V$;  in $(b)$, we used that $(\psi, E)$ solve \eqref{eq:schrodinger}. Since the above holds for all $\bx \in \R^{3n} $, we get that $(\psi_{g},E)$ also solves the Schr\"odinger's equation. To verify the anti-symmetric requirement, we write the wavefunction $\psi$ as $\psi(\tilde \bx) = \psi(\tilde x_1, \ldots, \tilde x_n)$, where each $\tilde x_i = (x_i, \sigma_i)$ now additionally depends on the spin $\sigma_i \{ \uparrow, \downarrow\}$. Since $\G$ only acts on the spatial position in $\R^3$ and leaves the spins invariant, we can WLOG express the $g$-transformed version of $\tilde x_i$ as $g(\tilde x_i) = (g(x_i), \sigma_i)$. Moreover, since $\sigma \in P_n$ commutes with the diagonal action of $g \in \G$ on $(\R^3 \times \{\uparrow, \downarrow\})^n$, 
\begin{align*}
    \psi_g( \sigma(\tilde \bx) )
    \;=\; 
    \psi( g \circ \sigma (\tilde \bx) )
    \;=\;
    \psi( \sigma \circ g(\tilde \bx) ) 
    \;\overset{(c)}{=}\; 
    \textrm{sgn}(\sigma) \, \psi( g(\tilde \bx) ) 
    \;=\; 
    \textrm{sgn}(\sigma) \, \psi_g( \tilde \bx ) \;.
\end{align*}
In $(c)$, we have used the anti-symmetry of $\psi$. This proves that $(\psi_{g},E)$ solves \eqref{eq:schrodinger} with the correct anti-symmetric requirement. To prove the theorem statement, we see that $\psi^\G$ is a linear combination of finitely many eigenfunctions $(\psi_{g})_{g \in \cG}$ with the same eigenvalue $E$ and therefore yields an anti-symmetric solution with the same energy $E$. 
\qed 


\section{Proofs for data augmentation and group-averaging}  \label{appendix:proof:DA:GA}

\subsection{Proof of Proposition~\ref{prop:DA}}
We first compute the difference in expectation as
\begin{align*}
    \| \mean[ \delta \theta^{(\rm DA)}] - \mean[\delta \theta^{(\rm OG)}] \|
    \;=&\;
    \Big\| 
        \mean\Big[ \mfrac{1}{N} \msum_{i \leq N/k} \msum_{j \leq k}
    F_{\bg_{i,j}(\bX_i); \psi_\theta} \Big] 
        - 
        \mean\Big[ \mfrac{1}{N} \msum_{i \leq N} F_{\bX_i; \psi_\theta} 
        \Big] 
    \Big\|
    \\
    \;=&\;
    \| \mean[  F_{\bg_{1,1}(\bX_1); \psi_\theta} ] - \mean[  F_{\bX_1; \psi_\theta} ] \|
    \;=\; 
    \Big\| \mean_{\bX \sim p^{(m)}_{\psi_\theta; {\rm DA}}}[  F_{\bX; \psi_\theta} ] - \mean_{\bY \sim p^{(m)}_{\psi_\theta}}[  F_{\bY; \psi_\theta} ] \Big\|
    \;.
\end{align*}
In the last line, we used linearity of expectation, the fact that $\bg_{i,j}(\bX_i)$'s are identically distributed and that $\bX_i$'s are identically distributed. Recall that $ \big\{ 
    \bx \mapsto 
    \big( 
        F_{\bx;\psi_\theta}
        \,,\,
        F_{\bx;\psi_\theta}^{\otimes 2}
    \big)
    \,\big|\, 
    \theta \in \R^q
    \big\} 
    \,\subseteq\, \cF$
and $d_\cF(p,q) = \sup_{f \in \cF} \| \mean_{\bX \sim p}[f(\bX)] -  \mean_{\bY \sim q}[f(\bY)] \|$. This implies that
\begin{align*}
    \| \mean[ \delta \theta^{(\rm DA)}] - \mean[ \delta \theta^{(\rm OG)}] \|
    \;=\;
    \Big\| \mean_{\bX \sim p^{(m)}_{\psi_\theta; {\rm DA}}}[  F_{\bX; \psi_\theta} ] - \mean_{\bY \sim p^{(m)}_{\psi_\theta}}[  F_{\bY; \psi_\theta} ] \Big\|
    \;\leq\; 
    d_\cF \big(  p^{(m)}_{\psi_\theta; {\rm DA}},  p^{(m)}_{\psi_\theta} \big),
\end{align*}
which proves the first bound. For the second bound, notice that 
\begin{align*}
    \Var[ \delta \theta^{(\rm DA)} ]
    \;=&\; 
    \Var\Big[ 
        \mfrac{1}{N} \msum_{i \leq N/k} \msum_{j \leq k}
        F_{\bg_{i,j}(\bX_i); \psi_\theta}
    \Big]
    \\
    \;\overset{(a)}{=}&\;
    \mfrac{1}{N/k}
    \Var\Big[ 
        \mfrac{1}{k} \msum_{j \leq k}
        F_{\bg_{1,j}(\bX_1); \psi_\theta}
    \Big]
    \\
    \;\overset{(b)}{=}&\;
    \mfrac{1}{N}
    \Var[ F_{\bg_{1,1}(\bX_1); \psi_\theta}]
    +
    \mfrac{k-1}{N} 
    \Cov[  F_{\bg_{1,1}(\bX_1); \psi_\theta},  F_{\bg_{1,2}(\bX_1); \psi_\theta} ]
    \\
    \;\overset{(c)}{=}&\;
    \mfrac{1}{N}
    \Var[ F_{\bg_{1,1}(\bX_1); \psi_\theta}]
    +
    \mfrac{k-1}{N} 
    \Var \, \mean\big[ F_{\bg_{1,1}(\bX_1); \psi_\theta} \big| \bX_1 \big]
    \;.
\end{align*}
In $(a)$, we have noted that the summands are i.i.d.~across $i \leq N/k$; in $(b)$, we have computed the variance of the sum explicitly by expanding the expectation of a double-sum; in $(c)$, we have applied the law of total covariance to obtain that 
\begin{align*}
    &\;\Cov[  F_{\bg_{1,1}(\bX_1); \psi_\theta},  F_{\bg_{1,2}(\bX_1); \psi_\theta} ]
    \\
    &\hspace{5em}
    \;=\;
    \underbrace{\Cov\big[ 
        \mean[ F_{\bg_{1,1}(\bX_1); \psi_\theta} | \bX_1 ]
        \,,\,  
        \mean[ F_{\bg_{1,2}(\bX_1); \psi_\theta} | \bX_1 ] 
    \big]}_{ =  \, \Var \, \mean[ F_{\bg_{1,1}(\bX_1); \psi_\theta} | \bX_1 ] }
    +
        \mean \, 
        \underbrace{\Cov\big[ F_{\bg_{1,1}(\bX_1); \psi_\theta} \,,\, F_{\bg_{1,2}(\bX_1); \psi_\theta} \,\big|\, \bX_1 \big]
    }_{ = 0}
    \;.
\end{align*}
Meanwhile, the same calculation with $k=1$ and $\bg_{1,1}$ replaced by identity gives 
\begin{align*}
    \Var[ \delta \theta^{(\rm OG)} ]
    \;=\;
    \mfrac{1}{N}
    \Var[ F_{\bX_1; \psi_\theta}]
    \;.
\end{align*}
Taking a difference and applying the triangle inequality twice, we have 
\begin{align*}
    &\; 
    \Big\| \Var[ \delta \theta^{(\rm DA)} ] - \Var[ \delta \theta^{(\rm OG)} ] - \mfrac{k-1}{N} 
    \Var \, \mean\big[ F_{\bg_{1,1}(\bX_1); \psi_\theta} \big| \bX_1 \big] 
    \Big\| 
    \;=\;
    \mfrac{1}{N} \big\|  \Var[ F_{\bg_{1,1}(\bX_1); \psi_\theta}] - \Var[ F_{\bX_1; \psi_\theta}] \big\|
    \\
    &\;\leq\;
    \mfrac{1}{N} \big\|  \mean\big[ F_{\bg_{1,1}(\bX_1); \psi_\theta}^{\otimes 2}\big] - \mean\big[ F_{\bX_1; \psi_\theta}^{\otimes 2} \big] \big\|
    +
    \mfrac{1}{N} \big\|  \mean\big[ F_{\bg_{1,1}(\bX_1); \psi_\theta}\big]^{\otimes 2} - \mean\big[ F_{\bX_1; \psi_\theta} \big]^{\otimes 2} \big\|
    \\
    &\;\leq\;
    \mfrac{1}{N} \big\|  \mean\big[ F_{\bg_{1,1}(\bX_1); \psi_\theta}^{\otimes 2}\big] - \mean\big[ F_{\bX_1; \psi_\theta}^{\otimes 2} \big] \big\|
    +
    \mfrac{1}{N} \big\|  \mean\big[ F_{\bg_{1,1}(\bX_1); \psi_\theta}\big]+  \mean\big[ F_{\bX_1; \psi_\theta} \big] \big\| \, \big\|  \mean\big[ F_{\bg_{1,1}(\bX_1); \psi_\theta}\big] -  \mean\big[ F_{\bX_1; \psi_\theta} \big] \big\|
    \\
    &\;\leq\;
    \mfrac{d_\cF \big(  p^{(m)}_{\psi_\theta; {\rm DA}},  p^{(m)}_{\psi_\theta} \big)}{N}
    +
    \mfrac{1}{N} \big( 2 \big\| \mean\big[ F_{\bX_1; \psi_\theta} \big] \big\| + d_\cF \big(  p^{(m)}_{\psi_\theta; {\rm DA}},  p^{(m)}_{\psi_\theta} \big) \big) \, \times \, d_\cF \big(  p^{(m)}_{\psi_\theta; {\rm DA}},  p^{(m)}_{\psi_\theta} \big)
    \\
    &\;=\;
    \mfrac{ 1 + 2 \| \mean[ \delta \theta^{(\rm OG)} ] \| + d_\cF (  p^{(m)}_{\psi_\theta; {\rm DA}},  p^{(m)}_{\psi_\theta} )   }{N} \, \times \,  d_\cF \big(  p^{(m)}_{\psi_\theta; {\rm DA}},  p^{(m)}_{\psi_\theta} \big)
    \;. \tagaligneq \label{eq:var:analysis:DA}
\end{align*} 
This proves the second bound. In the case when $\bg(\bX_1) \overset{d}{=} \bX_1$ for all $\bg \in \Gdiag$, we have $p^{(m)}_{\psi_\theta; {\rm DA}} = p^{(m)}_{\psi_\theta}$ and therefore the bounds above all evaluate to zero. In this case we have $\mean[ \delta \theta^{(\rm DA)}] = \mean[\delta \theta^{(\rm OG)}]$ and 
\begin{align*}
    \Var[ \delta  \theta^{(\rm DA)}] - \Var[ 
        \delta \theta^{(\rm OG)}] \;=\;  \mfrac{k-1}{N} 
    \Var \, \mean\big[ F_{\bg_{1,1}(\bX_1); \psi_\theta} \big| \bX_1 \big]\;,
\end{align*}
which is positive semi-definite. \qed

\subsection{Proof of Lemma~\ref{lem:GA}}

By construction, $\delta \theta^{(\rm GA)} = \frac{1}{N / k} \sum_{i \leq N/k} F_{\bX^\cG_i; \psi^\cG_\theta}$ is a size-$N/k$ empirical average of i.i.d.~quantities. The mean and variance formulas thus follows directly from a standard computation:
\begin{align*}
    \mean[ \delta \theta^{(\rm GA)} ]
    \;=&\;
    \mean[  F_{\bX^\cG_1; \psi^\cG_\theta}  ]
    &\text{ and }&&
    \Var[\delta \theta^{(\rm GA)}]
    \;=&\;
    \mfrac{\Var[  F_{\bX^\cG_1; \psi^\cG_\theta}  ]}{N/k}
    \;. \qedhere
\end{align*}

\subsection{Proof of Theorem~\ref{thm:DA:CLT}} 
To prove the coordinate-wise bound, fix $l \leq p$. Note that if $\sigma^{(\rm DA)}_l = 0$, then $ \delta \theta^{(\rm DA)}_{1l} = \mean[ \delta \theta^{(\rm DA)}_{1l}]$ with probability $1$, implying that the distribution difference is zero and hence the bound is satisfied. In the case $\sigma^{(\rm DA)}_l > 0$, $\delta \theta^{(\rm DA)}_{1l} = \frac{1}{N / k} \msum_{i \leq N / k} F^{(\rm DA)}_{il}$ is an average of i.i.d.~univariate random variables with positive variance. By renormalizing $t$ and applying the Berry-Ess\'een theorem (see Theorem 3.7 of \citet{chen2011normal} or \citet{shevtsova2013optimization} for the version with a tight constant $C_1 = 0.469$) applied to $\frac{1}{N / k} \msum_{i \leq N / k} F_{il}$, we get that
\begin{align*}
    &\;
    \sup_{t \in \R} 
    \,
    \Big|
    \,
        \P\big( \, 
        \delta \theta^{(\rm DA)}_{1l}
        \,\leq\, t 
        \big)
        -
        \P\Big( \, \mean\big[ \delta \theta^{(\rm DA)}_{1l} \big]  
        + (\Var[\delta \theta^{(\rm DA)}_{1l}  ])^{1/2} \, Z_l \, 
        \,\leq\, t 
        \Big)
    \,
    \Big|
    \\
    &=
    \sup_{t \in \R} 
    \,
    \Big|
    \,
        \P\Big( \, 
        \mfrac{1}{N / k} \msum_{i \leq N / k} \big( F^{(\rm DA)}_{il}  - \mean\big[F^{(\rm DA)}_{il}\big] \big)
        \,\leq\, t 
        \Big)
        -
        \P\Big( \, \mfrac{\sigma^{(\rm DA)}_l}{\sqrt{N/k}} \, Z_l \, 
        \,\leq\, t 
        \Big)
    \,
    \Big|
    \leq
    \mfrac{C_1 \, \mean | F^{(\rm DA)}_{1l} |^3}{ \sqrt{N/k} \, \big(\sigma^{(\rm DA)}_l\big)^3}
    \;.
\end{align*}
This proves the first set of bounds. To prove the second set of bounds, we first denote the mean-zero variable $\bar F_{il} \coloneqq - F^{(\rm DA)}_{il} + \mean[F^{(\rm DA)}_{il}]$, and let $(\bar Z_{11}, \ldots, \bar Z_{np})$ be an $\R^{np}$-valued Gaussian vector with the same mean and variance as $(\bar F_{11}, \ldots, \bar F_{np})$. The difference in distribution function can be re-expressed as 
\begin{align*}
    &\;
    \sup_{t \in \R} 
    \,
    \Big|
    \,
        \P\Big( \, 
            \max_{l \leq p} \Big|\delta  \theta^{(\rm DA)}_{1l} -  \mean\big[\delta  \theta^{(\rm DA)}_{1l} \big]   \Big|
        \,\leq\, t 
        \big)
        -
        \P\Big( \, 
        \max_{l \leq p} \Big|
            (\Var[ \delta \theta^{(\rm DA)}_{1l}  ])^{1/2} \, Z_l \,
        \Big| 
        \,\leq\, t 
        \Big)
    \,
    \Big|
    \\
    &
    \;=\;
    \sup_{t \in \R} 
    \,
    \Big|
    \,
        \P\Big( \, 
            \max_{l \leq p} \Big| \mfrac{1}{\sqrt{ N / k }} \msum_{i \leq N/k} \bar F_{il} \Big|
        \,\leq\, t 
        \big)
        -
        \P\Big( \, 
        \max_{l \leq p} \Big|
        \mfrac{1}{\sqrt{ N / k}} \msum_{i \leq N/k} \bar Z_{il} 
        \Big| 
        \,\leq\, t 
        \Big)
    \,
    \Big|
    \;,
    \tagaligneq \label{eq:DA:diff:intermediate}
\end{align*}
where we have used the definition of $\theta^{(\rm DA)}_1$ and also replaced $t$ by $t  / \sqrt{N k}$. Note that $\bar F_{il}$'s are i.i.d.~mean-zero across $1 \leq i \leq N/k$ and $\bar Z_{il}$'s are i.i.d.~mean-zero across $1 \leq i \leq N/k$. As before, if $\sigma^{(\rm DA)}_l = 0$ for all $1 \leq l \leq p$, the two random variables to be compared are both $0$ with probability $1$ and the distributional difference above evaluates to zero. If there is at least one $l$ such that $\sigma^{(\rm DA)}_l = 0$, we can restrict both maxima above to be over $l \leq p$ such that $\sigma^{(\rm DA)}_l = 0$ and ignore the coordinates with zero variance. As such, we can WLOG assume that $ \sigma^{(\rm DA)}_l > 0$ for all $l \leq p$. Now write 
\begin{align*}
    \tilde F_i \;\coloneqq\; ( \sigma_1^{-1} \bar F_{i1}, \ldots, \sigma_p^{-1} \bar F_{ip})\;,
    \qquad 
    \tilde Z_i \;\coloneqq\; ( \sigma_1^{-1} \bar Z_{i1}, \ldots,  \sigma_p^{-1} \bar Z_{ip})\;,
\end{align*}
and denote the hyper-rectangular set $\cA(t) \coloneqq [ - \sigma_1^{-1} t, + \sigma_1^{-1} t ]
\times  \cdots  \times 
[ - \sigma_p^{-1} t, + \sigma_p^{-1} t ] \subseteq \R^q$. We can now express the difference above further as 
\begin{align*}
    \eqref{eq:DA:diff:intermediate}
    \;=&\;
    \sup_{t \in \R} 
    \,
    \Big|
    \,
        \P\Big( \, 
        \mfrac{1}{\sqrt{ N / k }} \msum_{i \leq N/k} \tilde F_i 
        \in 
        \cA(t) 
        \Big)
        -
        \P\Big( \, 
        \mfrac{1}{\sqrt{ N / k }} \msum_{i \leq N/k} \tilde Z_i 
        \in 
        \cA(t) 
        \Big)
    \,
    \Big|\;.
\end{align*}
This is a difference in distribution functions between a normalized empirical average of $p$-dimensional vectors with zero mean and identity covariance and a standard Gaussian in $\R^q$, measured through a subset of hyperrectangles. In particular, this is the quantity controlled by \citet{chernozhukov2017central}: Under the stated moment conditions and applying their Proposition 2.1, we have that for some absolute constant $C_2 > 0$,
\begin{align*}
    \eqref{eq:DA:diff:intermediate}
    \;\leq\;
    C_2
    \Big( 
    \mfrac{  (\tilde F^{(\rm DA)})^2 \, (\log (p N / k) )^7}
    { N / k }  
    \Big)^{1/6}\;.
    \tag*{\qed}
\end{align*}

\section{Proofs for results on canonicalization}  \label{appendix:proof:canon}

\subsection{Proof of Theorem~\ref{thm:SC}}

To prove (i), we fix any $\tilde g \in \G$, and WLOG let $k=1$. Then by the definition of $\psiSCone_{\theta;\epsilon}$,
\begin{align*}
    &\; \psiSCone_{\theta;\epsilon}
    (\tilde g(x_1), \ldots, \tilde g(x_n)) 
    \\
    & \hspace{4em} 
    \;=\;  
    \sum_{\substack{g \in \G \text{ s.t. } \\ d(\tilde g(x_1), g(\Pi_0)) \leq \epsilon}} 
    \Big(
    \, 
    \mfrac{ \lambda_\epsilon\big( d(\tilde g(x_1), g(\Pi_0)) \big) }{\sum_{\substack{g' \in \G \text{ s.t. } \\ d(\tilde g(x_1), g'(\Pi_0)) \leq \epsilon }}  \lambda_\epsilon\big( d(\tilde g(x_1), g'(\Pi_0)) \big)  }
    \times 
        \psi_\theta\big( g^{-1} \tilde g(x_1), \ldots, g^{-1} \tilde g(x_n) \big)
    \Big)\;.
\end{align*}
Relabelling $g$ by $\tilde g g$ and $g'$ by $\tilde g g'$ in the sums above, and noting that $d(\tilde g(x_1), \tilde g g(\Pi_0)) = d(x_1, g(\Pi_0))$, we obtain that 
\begin{align*}
    \psiSCone_{\theta;\epsilon}
    (\tilde g(x_1), \ldots, \tilde g(x_n)) 
    \;=&\;
    \sum_{\substack{g \in \G \text{ s.t. } \\ d(\tilde g(x_1), \tilde g g(\Pi_0)) \leq \epsilon}} 
    \Big(
    \, 
    \mfrac{ \lambda_\epsilon\big( d(\tilde g(x_1), \tilde g g(\Pi_0)) \big) }{\sum_{\substack{g' \in \G \text{ s.t. } \\ d(\tilde g(x_1),  \tilde g g'(\Pi_0)) \leq \epsilon}}  \lambda_\epsilon\big( d(\tilde g(x_1), \tilde g g'(\Pi_0)) \big)  }
    \,
    \\ 
    &\hspace{8em}
    \times 
        \psi_\theta\big( g^{-1} \tilde g^{-1} \tilde g(x_1), \ldots, g^{-1} \tilde g^{-1} \tilde g(x_n) \big)
    \Big)
    \\
    \;=&\;
    \sum_{\substack{g \in \G \text{ s.t. } \\ d(x_1, g(\Pi_0)) \leq \epsilon}} 
    \, 
    \mfrac{ \lambda_\epsilon\big(  d(x_1, g(\Pi_0)) \big) }{\sum_{\substack{g' \in \G \text{ s.t. } \\ d(x_1,  g'(\Pi_0)) \leq \epsilon}}  \lambda\big(  d(x_1,  g'(\Pi_0))\big)  }
    \,\;
    \psi_\theta\big( g^{-1} (x_1), \ldots, g^{-1} (x_n) \big)
    \\
    \;=&\;
    \psiSCone_{\theta;\epsilon}
    (x_1, \ldots, x_n)
    \;.
\end{align*}
This proves diagonal $\G$-invariance. 

\vspace{.5em}

To prove (ii), we shall make the spin-dependence explicit and write $\tilde x_i = (x_i, \sigma_i)$ and $g(\tilde x_i) = (g(x_i), \sigma_i)$. First consider $\pi$, a transposition that swaps the indices $1$ and $2$. Then
\begin{align*}
    \psiSC_{\theta;\epsilon}(\tilde x_{\pi(1)}, \ldots, \tilde  x_{\pi(n)} )
    \;=&\;
    \mfrac{1}{n} \msum_{k=1}^n
    \psiSCk_{\theta;\epsilon}(\tilde  x_2, \tilde x_1, \tilde  x_3 \ldots, \tilde x_n )
    \;.
\end{align*}
By the anti-symmetry of $\psi_\theta$, we see that for $k \geq 3$,
\begin{align*}
    \psiSCk_{\theta;\epsilon}(\tilde x_2, \tilde x_1, \tilde x_3 \ldots, \tilde x_n )
    \;=&\;
    \msum_{g \in \cG_\epsilon(x_k)} 
    \, 
    w^g_\epsilon ( x_k)
    \, 
    \psi_\theta \big( g^{-1}(\tilde x_2), g^{-1}(\tilde x_1), g^{-1}(\tilde x_3), \ldots, g^{-1}(\tilde x_n) \big)
    \\
    \;=&\;
    -
    \msum_{g \in \cG_\epsilon(x_k)} 
    \, 
    w^g_\epsilon (x_k)
    \, 
    \psi_\theta \big( g^{-1}(\tilde x_1), g^{-1}(\tilde x_2), g^{-1}(\tilde x_3), \ldots, g^{-1}(\tilde x_n) \big)
    \\
    \;=&\;
    -
    \psiSCk_{\theta;\epsilon}(\tilde x_1, \ldots, \tilde x_n )
    \;.
\end{align*}
For the case $k=1,2$, we can apply a similar calculation while noting that the weights remain unchanged, and obtain
\begin{align*}
    \psiSCone_{\theta;\epsilon}(\tilde x_2, \tilde x_1, \tilde x_3 \ldots, \tilde x_n )
    \;=&\;
    - \psiSCtwo_\theta(\tilde x_1, \tilde x_2, \tilde x_3 \ldots, \tilde  x_n ) 
    \;,
    \\
    \psiSCtwo_\theta(\tilde x_2, \tilde x_1, \tilde x_3 \ldots, \tilde x_n ) 
    \;=&\;
    - \psiSCone_{\theta;\epsilon}(\tilde x_1, \tilde x_2, \tilde x_3 \ldots, \tilde x_n ) 
    \;.
\end{align*}
This implies that 
\begin{align*}
    \psiSC_{\theta;\epsilon}(\tilde x_{\pi(1)}, \ldots, \tilde x_{\pi(n)} )
    \;=&\;
    -
    \mfrac{1}{n} \msum_{k=1}^n
    \psiSCk_{\theta;\epsilon}(\tilde x_1, \tilde x_2, \tilde x_3 \ldots, \tilde  x_n )
    \;=\;
    {\rm sgn}(\pi)
    \,
    \psiSC_{\theta;\epsilon}(\tilde x_1,  \ldots, \tilde x_n )
    \;.
\end{align*}
Since the choice of indices $1$ and $2$ are arbitrary, the above in fact holds for all transpositions $\pi$, which implies 
\begin{align*}
    \psiSC_{\theta;\epsilon}(\tilde x_{\tau(1)}, \ldots,\tilde  x_{\tau(n)} )
    \;=\;
    {\rm sgn}(\tau)
    \,
    \psiSC_{\theta;\epsilon}(\tilde x_1, \ldots, \tilde x_n )
\end{align*}
for all permutations $\tau$ of the $n$ electrons. This proves (ii).

\vspace{.5em}

To prove (iii), it suffices to show that for every $k \leq n$, the function $\psiSCk_{\theta;\epsilon}$ defined above is $p$-times continuously differentiable at $\bx$, i.e.~$\nabla^p \psiSCk_{\theta;\epsilon}$ exists and is continuous at $\bx$. Again it suffices to show this for the case $k=1$. Let $\tilde \bx \coloneqq (\tilde x_1, \ldots, \tilde x_n)$ be a vector in a sufficiently small neighborhood of a fixed $\bx \coloneqq (x_1, \ldots, x_n)$, and recall that 
\begin{align*}
    \cG_\epsilon( \tilde x_1 )
    \;=\; 
    \big\{ g \in \G \,\big|\, d( \tilde x_1, g(\Pi_0)) \leq \epsilon \big\}
    \;.
\end{align*}
For $\tilde \bx$ in a sufficiently small neighborhood of $\bx$, $\cG_\epsilon( \tilde x_1 )$ takes value in $\{ \cG_l \}_{0 \leq l \leq M}$, where 
\begin{align*}
    \cG_l
    \;\coloneqq\;
    \cG_\epsilon(x_1)
    \,\setminus\, 
    \{ g^\epsilon_1, \ldots, g^\epsilon_l \} 
    \;,
\end{align*}
and $g^\epsilon_1, \ldots, g^\epsilon_M \in \G$ is an enumeration of all group elements such that 
\begin{align*}
    d\big( x_1 \,,\, g^\epsilon_l(\Pi_0) \big)
    \;=\; 
    \epsilon
    \qquad 
    \text{ for } 0 \leq l \leq M\;.
\end{align*}
Therefore $\psiSCone_{\theta;\epsilon}(\tilde \bx)$ takes values in $\{\psi_{\cG_l}(\tilde \bx) \}_{0 \leq l \leq M}$, where 
\begin{align*}
    \psi_{\cG_l}(\tilde \bx)
    \coloneqq
    \msum_{g \in \cG_l} 
    \mfrac{ \lambda_\epsilon\big( \frac{\epsilon - d(  \tilde x_1, g(\Pi_0))}{\epsilon} \big) }{\sum_{g' \in \cG_l} \lambda_\epsilon\big( \frac{\epsilon - d(\tilde x_1, g'(\Pi_0))}{\epsilon} \big)  }
    \, \psi_\theta\big(
        g^{-1}  (\tilde x_1)
        \,,\,
        \ldots 
        \,,\,
        g^{-1} (\tilde x_n)
    \big) 
    \;.
\end{align*}
Notice that at $\tilde \bx=\bx$, by the definition of $g^\epsilon_l$, we have
\begin{align*}
    \psi_{\cG_l}(\bx)
    \;=&\;
    \msum_{g \in \cG_l} 
    \mfrac{ \lambda_\epsilon \big( d(  x_1, g(\Pi_0)) \big) }{\sum_{g' \in \cG_l}  \lambda_\epsilon \big( d(  x_1, g'(\Pi_0)) \big)  }
    \times
    \, \psi_\theta\big(
        g^{-1}  ( x_1)
        \,,\,
        \ldots 
        \,,\,
        g^{-1}  ( x_n)
    \big) 
    \\ 
    \;\overset{(a)}{=}&\;
    \msum_{g \in \cG_\epsilon(x_1)} 
    \mfrac{ \lambda_\epsilon \big(  d(   x_1, g(\Pi_0))  \big) }{\sum_{g' \in \cG_\epsilon(x_1)} \lambda_\epsilon\big( d( x_1, g'(\Pi_0)) \big)  }
    \times
    \, \psi_\theta\big(
        g^{-1} ( x_1)
        \,,\,
        \ldots 
        \,,\,
        g^{-1}  ( x_n)
    \big) 
    \;=\; \psiSCone_{\theta;\epsilon}(\bx)
    \;.
\end{align*}
Since the above argument works with $\bx$ replaced by $\tilde \bx$, we also have 
\begin{align*}
    \psiSCone_{\theta;\epsilon}(\tilde \bx)
    \;=\;
    \msum_{g \in \tilde \cG_l} 
    \mfrac{ \lambda_\epsilon \big(  d(  x_1, g(\Pi_0)) \big) }{\sum_{g' \in \tilde \cG_l} \lambda_\epsilon\big( d( x_1, g'(\Pi_0)) \big)  }
    \times
    \, \psi_\theta\big(
        g^{-1}  ( x_1)
        \,,\,
        \ldots 
        \,,\,
        g^{-1}  ( x_n)
    \big) 
    \tagaligneq \label{eq:continuity:F1:eps}
\end{align*}
for $0 \leq l \leq \tilde M$, where we have defined 
\begin{align*}
    \tilde \cG_l \;\coloneqq\; \cG_\epsilon(\tilde x_1) \,\setminus\, 
    \{ \tilde g^\epsilon_1, \ldots, \tilde g^\epsilon_l \} 
    \;,
\end{align*}
and $\tilde g^\epsilon_1, \ldots, \tilde g^\epsilon_M \in \G$ is an enumeration of all group elements such that 
\begin{align*}
    d\big( \tilde x_1 \,,\, \tilde g^\epsilon_l(\Pi_0) \big)
    \;=\; 
    \epsilon
    \qquad 
    \text{ for } 0 \leq l \leq M\;.
\end{align*}
Notice that for $\tilde \bx$ in a sufficiently small neighborhood of $\bx$, we have $\{ \tilde \cG_l \}_{l \leq \tilde M} = \{ \cG_l \}_{l \leq M}$, in which case \eqref{eq:continuity:F1:eps} implies 
\begin{align*}
    \psiSCone_{\theta;\epsilon}(\tilde \bx) 
    \;=\; 
    \psi_{\cG_l}(\tilde \bx)
    \qquad 
    \text{ for all } 0 \leq l \leq M\;.
\end{align*}
In other words, we have shown that in a sufficiently small neighborhood of $\bx$, $F_1$ equals $\psi_{\cG_l}$ for all $1 \leq l \leq M$. Recall that $\lambda$ and $d(\argdot, g(\Pi_0))$ are $p$-times continuously differentiable for all $g \in \G$, and $\psi_\theta$ is $p$-times continuously differentiable at $g(\bx)$ for all $g \in \G$ by assumption. This implies that $\psi_{\cG_l}$ is also $p$-times continuously differentiable at $\bx$ and so is $\psiSCone_{\theta;\epsilon}$. Moreover, for $0 \leq q \leq p$, the derivative can be computed as
\begin{align*}
    \nabla^q \psiSCone_{\theta;\epsilon}(\bx)
    \;=\;
    \nabla^q \psi_{\cG_l}(\bx)
    \;=\;
    \msum_{g \in \cG_\epsilon(x_1)} 
    \nabla^q 
    \psiSCk_{\theta; g, x_1}
    (\bx)
    \;,
\end{align*}
where we recall that for $1 \leq k \leq n$, $g \in \G$ and $x, y_1, \ldots, y_n \in \R^3$, 
\begin{align*}
    \psiSCk_{\theta,\epsilon;g,x}
    (y_1, \ldots, y_n)
    \;\coloneqq&\;
    \mfrac{
        \lambda_\epsilon
        \big(  d( y_k, g(\Pi_0)) \big) 
    }{
        \sum_{g' \in \cG_\epsilon(x)}
        \lambda_\epsilon
        \big(  d( y_k, g'(\Pi_0))  \big) 
    }
    \,
    \psi_\theta( g^{-1}(y_1) \,,\, \ldots \,,\, g^{-1}(y_n) )
    \;.
\end{align*}
The same argument applies to all $\psiSCk_{\theta;\epsilon}$'s with $k \leq n$ and therefore for $0 \leq q \leq p$,
\begin{align*}
    \nabla^q 
    \psiSC_{\theta;\epsilon}(x_1, \ldots, x_n)
    \;=&\;
    \mfrac{1}{n} \msum_{k=1}^n 
    \nabla^q 
    \psiSCk_{\theta;\epsilon}(x_1, \ldots, x_n)
    \;=\;
    \mfrac{1}{n}
    \msum_{k=1}^n
    \msum_{g \in \cG_\epsilon(x_k)}
    \,
    \nabla^q 
    \psiSCk_{\theta,\epsilon;g,x_k}
    (\bx)\;,
\end{align*}
which proves (iii). \qed

\subsection{Proof of Lemma~\ref{lem:eps:lamb:blowup}} 

Since $\lambda_\epsilon(0) = 1$, $\lambda_\epsilon(\epsilon) = 0$ and $\lambda_\epsilon$ is continuously differentiable, by the mean value theorem, there is $y_1 \in (0, \epsilon)$ such that 
\begin{align*}
    \partial\lambda_\epsilon(y_1) \;=\; (1-0)/ \epsilon \;=\; \epsilon^{-1}   \;.
\end{align*}    
Meanwhile, since $\lambda_\epsilon(w) = 1$ for  all $w \leq 0$, $\partial \lambda_\epsilon(w) = 0$ for  all $w < 0$, and since $\partial \lambda_\epsilon$ is continuous, $\partial \lambda_\epsilon(0) = 0$. By the twice continuous differentiability of $\lambda_\epsilon$ and the mean value theorem again, there exists $y'_1 \in (0, y_1) \subset (0, \epsilon)$ such that 
\begin{align*}
    \partial^2 \lambda_\epsilon(y'_1) \;=\; (\epsilon^{-1}-0)/ y_1 \;=\; \mfrac{1}{\epsilon y'_1} \;\geq\; \mfrac{1}{\epsilon^2}  \;.
\end{align*}    
Since $\partial^2 \lambda_\epsilon(w) = 0$ for all $w < 0$ and $\partial^2 \lambda_\epsilon$ is continuous, $\partial^2 \lambda_\epsilon(0) = 0$. As $\partial^2 \lambda_\epsilon(y'_1) \geq  \mfrac{1}{\epsilon^2}$, by the intermediate value theorem, there exists some $y_2 \in [0, y_1] \subseteq [0, \epsilon]$ such that  $\partial^2 \lambda_\epsilon(y_2) = \epsilon^{-2}$. \qed