\paragraph{IL Theory} Our theoretical guarantees are in the same setting of previous works like \cite{Shani:2021} and \cite{xu2023provably} with the difference that their work focuses on the easier finite horizon setting. Additionally, their exploration techniques only applies to the tabular setting. Indeed, in the MuJoCo experiments in  \cite{Shani:2021}, the authors do not attempt to implement the exploration mechanism required for their theoretical guarantees. 
In a similar way, also \cite{xu2023provably} can not be implemented beyond the tabular setting because it relies on a reward free procedure requiring bonuses proportional to the number of visits to each state action pair.
\cite{ren2024hybrid} suggest an algorithm which does not require exploration but it can not improve upon behavioural cloning in terms of expert trajectories. 
\cite{rajaraman2020toward, rajaraman2021provablybreaking,foster2024behavior} analyze instead offline imitation learning (behavioural cloning) where no additional interaction with the environment is allowed. This setting is more general but it comes at the cost of additional assumptions such as policy realizability or worst depedence on the horizon on the required number of trajectories.
\cite{foster2024behavior} presents an analysis for general policy classes but they require a maximum likelihood oracle which can not be implemented exactly when using neural network function approximation.

There has been also a variety of studies tackling the problem of computationally efficient algorithm with linear function approximation such as \cite{Kamoutsi:2021, viano2022proximal,viano2024imitation,rajaraman2021value,swamy2022minimax}.
However, their proof techniques are strictly depending on the linearity of the dynamics therefore the experiments in continuous control tasks require changes in the algorithmic design. Albeit our guarantees are restricted to the tabular setting, the algorithm can be implemented with no modifications with neural networks.

Several works focus on the setting where expert queries are allowed at any state visited during the MDP interaction \cite{Ross:2010, Ross:2011,swamy2021moments} or that require a generative model for the algorithm updates  \cite{swamy2022minimax}.
Another recent work requires a generative model to sample the initial state of the trajectory from the expert occupancy measure \cite{swamy2023inverse}.
Our algorithm requires sampling only trajectories in the MDP therefore it does not leverage the aforementioned generative model
assumption.
In contrast, the setting of this work matches the most practical one adopted for example in \cite{Ho:2016, Ho:2016b, Fu:2018, Reddy:2020, Dadashi:2021, watson2023coherent, Garg:2021, ni2021f}. In this case, the expert policy can  not be queried and the learner access only a precollected dataset of expert demonstrations.
\paragraph{Theory for IL from States Only}This setting has been firstly studied in \cite{sun2019provably} in the finite horizon setting and with general function approximation their work does not use exploration mechanism. However their work requires an additional realizability assumption of the expert value function, it can only learn a difficult to store and deploy non stationary policy and provides suboptimal guarantees on $\tau_E$ in terms of the horizon dependence.

The follow up from \cite{arora2020provable}, still requires the realizability of the state value function which is not needed in our work.
The work of \citet{kidambi2021mobile}  uses the idea of exploration in state only finite horizon imitation learning. Their analysis for tabular MDP gives a bound on $K$ which has a worst horizon dependence and it requires the design of exploration bonuses tight to the structural properties of the MDP. Therefore, their NN experiments requires an empirical approximation of such bonuses while the SOAR framework applies naturally.

\citet{wu2024diffusing} imposes expert score function realizability and that the expected state norm remains bounded during learning. The algorithm has provable guarantees but it requires an expensive \emph{RL in the loop} routine that we avoid in our work.
\paragraph{Exploration Techniques in Deep RL}
Ensemble of $Q$ networks has also been used for training stabilization \cite{anschel2017averaged}. \cite{zhang2025beta} introduces exploration technique based on multiple actors.
\citet{ciosek2019better} does not have theoretical guarantees but it uses the idea of constructing an optimistic critic using mean plus standard deviation but only to define an exploratory policy with which collecting data. Our approach instead maintains only one actor policy which is updated with the optimistic $Q$ estimate. \cite{parker2020effective,lyu2022efficient} exploration with ensemble of actors rather than critics. \cite{kurutach2018model, chua2018deep} uses an ensemble of networks trained to learn the transition model to improve the sample complexity in model based RL. 
\cite{henaff2022exploration} learns instead an inverse dynamics model and via an encoder and decoder model and uses the features output by the encoder to compute elliptical potential bonuses which are standard in linear bandits \cite{Abbasi-Yadkori:2011}.
%\cite{depeweg2018decomposition} used an ensemble to estimate the standard deviation of the cost function. These found application in safe RL.
\citet{moskovitz2021tactical} improved TD-3 \cite{fujimoto2018addressing} using an ensemble of critics and a bandit algorithm to find an aggregation rule balancing well the amount of optimism required by online exploration and pessimism required by off policy algorithms such as TD-3.  

In addition, there are several deep RL work that takes a bayesian point of view to the problem, these algorithms often achieve remarkable performance but the algorithm implemented with deep networks requires usually adjustments creating a mismatch compared to the provable algorithms in the tabular case. 
Among those \cite{luis2023model,zhou2020deep,o2018uncertainty} use the Bellman equation for the state value function variance to train a network (dubbed $U$ network) that models the uncertainty of the network predicting the $Q$ values. They respectively prove that this trick improves the performances of SAC, PPO \cite{Schulman:2017} and DQN\cite{Mnih:2015}. \cite{curi2020efficient} uses the model uncertainty estimate in the update of the actor.  

Moreover, building on the theoretical analysis of PSRL \cite{osband2014near} and RLSVI \cite{osband2016generalization} that show sublinear bayesian regret bound. At any step, these algorithms sample from a posterior distribution either an MDP where to plan or a value function to follow greedly at each step. Between one step and the other the posterior is updated given the new data.
While the theorical analysis in the above works prescribe a randomization at the value function parameters level, in the deep RL version, dubbed Boostrapped DQN \cite{osband2016deep},  the perturbation is performed implicitly maintaining a set of $Q$ networks and sampling uniformly at each round according to which network the agent chooses the greedy action. 
\cite{chen2017ucb}
improved upon Bootstrapped DQN using an aggregation rule. That is acting greedy with respect to the mean plus standard deviation of the $q$ ensemble.
\citet{osband2018randomized} further builds on this idea adding a differ prior to each network in the ensemble to increase diversity. Finally, \citet{osband2023approximate} replaces the uniform sampling in \cite{osband2016deep} with a learned distribution with an epistemic network \cite{osband2023epistemic}.


Furthermore, motivated by the bayesian regret bound proven in \cite{o2021variational} in the tabular case and the one in  \cite{o2023efficient}, \cite{tarbouriech2024probabilistic} proves a regret bound in the function approximation setting and showcased convincing performance in the Atari benchmark.
Their algorithm requires to know the variance of the cost posterior distribution which is not available in the neural network experiments. Therefore, it is estimated using the standard deviation of an ensemble of cost network. In our work, we use an ensemble of $Q$ networks and not cost networks.

Additionally,
\cite{ishfaq2021randomized} analyzed ensemble exploration techniques in the general function approximation setting. Their ensemble consists of different critics trained on the same state actions dataset but with rewards perturbed with a gaussian random vector. \cite{ishfaq2023provable,ishfaq2024more} looked at efficient implementation of Thompson sampling in Deep RL and obtained convincing results in Atari and providing guarantees for linear MDPs and general function approximation respectively.
Moreover, \cite{ishfaq2025langevin} extended the above results for continuous action spaces.
Unfortunately, these methods do not apply directly to imitation learning because they require a fixed reward function.





\paragraph{Exploration techniques in Deep IL}
As mentioned only few works investigated exploration techniques in Deep IL. Apart from the 
previously mentioned works, \cite{yu2020intrinsic} adopts a model based approach and used exploration bonuses based on prediction error of the next observed state (a.k.a. curiosity driven exploration \cite{pathak2017curiosity,burda2018large}).

\paragraph{State-only imitation learning}
\citet{torabi2018generative} tackled the problem of imitation learning from states only modifying the discriminator of GAIL \cite{Ho:2016b} to take as input state next state pairs instead of state action pairs. Further practical improvements have been proposed in \cite{zhu2020off} that allows for the use of off-policy data. The works \cite{yang2019imitation,nair2017combining,pathak2018zero,radosavovic2021state} use the idea of an inverse dynamic model while \cite{edwards2019imitating,ganai2023learning} develops a practical algorithm aiming at estimating the forward dynamic model. Furthermore, \cite{torabi2018behavioral} introduces a twist in behavioral cloning using inverse dynamic modelling to make it applicable to state only expert datasets.  A comprehensive literature review can be found in \cite{torabi2019recent}. 
More recently, features/state only imitation learning has found application in non markovian decision making problems \cite{qin2024learning}. \citet{sikchi2022ranking} introduce an algorithm that takes advantage of an offline ranker between trajectories  to get strong empirical results in LfO setting.
 Another line of works \cite{gupta2017learning,sermanet2018time,liu2019state,viano2021robust,viano2022robust,gangwani2020state,cao2021learning,gangwani2022imitation} motivate imitation learning from observation alone arguing that the expert providing the demonstrations and the learner acts in slightly different environments. In \cite{kim2022lobsdice,sikchi2024dual}, the authors proposed convex programming based methods to imitate an expert policy from  expert state only demonstration and auxiliary arbitrary state action pairs. Several works \cite{ni2021f,kim2022demodice,ma2022versatile,yu2023offline} introduce empirical methods to minimize an $f$-divergence between expert and learner state occupancy measure. Complementary, \cite{chang2023imitation} minimizes the Wasserstein distance between expert and learner state occupancy measure. Their numerical results are convincing but no sample complexity bounds are provided. Convincing results have been obtained also in \cite{chang2024adversarial} that uses the idea of boosting and in \cite{wu2024diffusing} which uses a diffusion models inspired loss to update the cost.
