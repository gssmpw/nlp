We begin by analyzing the rate of normal approximation for the Polyak-Ruppert-averaged SGD iterates \eqref{eq:PR_estimate}. We focus on smooth and strongly convex minimization problems, following the framework established in \cite{moulines2011non}, \cite{pmlr-v99-anastasiou19a} and \cite{shao2022berry}. In particular, we impose the following regularity conditions for the objective function $f$:
\begin{assum}
\label{ass:L-smooth}
The function $f$ is two times continuously differentiable and $L_{1}$-smooth on $\rset^{d}$, i.e., there is a constant $L_{1} > 0$, such that for any $\theta,\theta' \in \rset^{d}$,
\begin{equation}
\label{eq:L-smooth}
\norm{\nabla f(\theta) - \nabla f(\theta')} \leq L_{1} \norm{\theta - \theta'}\eqsp.
\end{equation}
Moreover, we assume that $f$ is $\mu$-strongly convex on $\rset^{d}$, that is, there exists a constant $\mu > 0$, such that for any $\theta,\theta' \in \rset^{d}$, it holds that
\begin{equation}
\label{eq:strong_convex}
(\mu/2) \norm{\theta-\theta'}^2 \leq f(\theta) - f(\theta') - \langle \nabla f(\theta'), \theta - \theta' \rangle \eqsp.
\end{equation}
\end{assum}
\Cref{ass:L-smooth} implies  the following two-sided bound on the Hessian $\nabla^2 f(\theta)$, $\mu \Id_d \preceq \nabla^2 f(\theta) \preceq L_1 \Id_d$ for all $\theta \in \rset^{d}$. We now formalize the assumptions on $F(\theta, \xi)$. Namely, we rewrite $F(\theta, \xi)$ as  
\begin{equation}
\label{eq: gradF decompositon}
\textstyle 
F(\theta_{k-1}, \xi_k) = \nabla f(\theta_{k-1}) + \zeta_k,  
\end{equation}
where $\{ \zeta_k\}_{k \in \nset}$ is a sequence of $d$-dimensional random vectors. Under this representation, the SGD recursion takes the form  
\begin{equation}
\label{eq:sgd_recursion_main_new}
\textstyle
\theta_{k} = \theta_{k-1} - \alpha_{k} (\nabla f(\theta_{k-1}) + \zeta_k) \eqsp, \quad \theta_0 \in \rset^{d}\eqsp.
\end{equation}  
We impose a family of assumptions, denoted as \Cref{ass:noise_decomposition}($p$) with $p \geq 2$, on the noise sequence $\zeta_k$:

\begin{assum}[p]
\label{ass:noise_decomposition}
For each $ k \geq 1$, $\zeta_k$ admits the decomposition $\zeta_k = \eta(\xi_k) + g(\theta_{k-1}, \xi_k)$, where
\begin{enumerate}[(i),noitemsep,nolistsep]
    \item  $\{\xi_k\}_{k=1}^{n-1}$ is a sequence of i.i.d. random variables on $(\Zset,\Zsigma)$ with distribution $\PP_{\xi}$, $\eta: \Zset \rightarrow \rset^d$ is a function such that $\PE[\eta(\xi_1)]= 0$ and $\PE[\eta(\xi_1)\eta(\xi_1)^\top]= \Sigma_{\xi}$. Moreover, $\lambda_{\min}(\Sigma_{\xi}) > 0$.
    \item The function $g: \rset^d \times \Zset \rightarrow \rset^d$ satisfies $\PE[g(\theta,\xi_1)]=0$ for any $\theta \in \rset^{d}$. Moreover, there exists  $L_2 > 0$ such that for any $\theta, \theta' \in \rset^{d}$, it holds that 
    \begin{equation}
        \label{eq: g_bound_norm}
        \textstyle 
        \norm{g(\theta, \xi)-g(\theta', \xi)}\leq L_2\norm{\theta - \theta'} \quad \text{and} \quad g(\thetas, z) = 0\eqsp \text{ for all } z \in \Zset\eqsp. 
    \end{equation}
    \item There exists $\sigma_p > 0$ such that $\PE^{1/p}[\|\eta(\xi_1)\|^p]\leq \sigma_p\eqsp.$ 
\end{enumerate}
\end{assum}
The above assumption implies that, given $\F_k = \sigma(\xi_1, \ldots, \xi_k)$, we have $\PE[g(\theta_k, \xi_{k+1}) | \F_k] = 0$ a.s. However, we prefer to state this condition as in (ii) above, as this particular formulation will be instrumental to establish the validity of the bootstrap procedure in \Cref{sec:bootstrap}. As an example of a sequence $\zeta_k$ satisfying \Cref{ass:noise_decomposition}($2$), consider the case where $\{\xi_k\}_{k=1}^{n-1}$ are i.i.d. random variables and the oracle function $F(\theta, \xi)$ satisfies:
\begin{enumerate}[noitemsep, nolistsep]
    \item $\PE[F(\theta, \xi)] = \nabla f(\theta)$ for all $\theta \in \rset^{d}$;
    \item $\norm{F(\theta, \xi) - F(\theta', \xi)} \leq L \norm{\theta - \theta'}$ for all $\xi \in \Zset$;
    \item $\PE[\norm{F(\thetas, \xi)}^p] < \infty$.
\end{enumerate}
In this case, \Cref{ass:noise_decomposition}($p$) holds with $\eta(\xi) = F(\thetas, \xi)$ and $g(\theta, \xi) = F(\theta, \xi) - F(\thetas, \xi)$. Additionally, note that the identity \eqref{eq: g_bound_norm} can be relaxed when one considers only last iterate bounds, such as $\PE[\norm{\theta_k - \thetas}^2]$. In particular, an almost sure bound can be substituted with a bound in expectation, e.g. \cite[Assumption H2]{moulines2011non}. At the same time, studying average iterates $\bar{\theta}_n$ requires stronger assumptions, which are similar to \eqref{eq: g_bound_norm}, see \cite[Assumption H2']{moulines2011non},  \cite{durmus2020biassgd}, \cite{shao2022berry}. To proceed with our analysis, we further impose the following condition on the Hessian matrix $\nabla^2f(\theta)$ at $\thetas$:
\begin{assum}
\label{ass:hessian_Lipschitz_ball}
There exist  $L_3, \beta > 0$ such that for all $\theta$ with $\norm{\theta-\thetas} \leq \beta$, it holds 
\begin{equation}
\textstyle 
\norm{\nabla^2f(\theta) -\nabla^2f(\thetas)}\leq L_3\norm{\theta-\thetas}\eqsp.
\end{equation}
\end{assum} 
The assumption \Cref{ass:hessian_Lipschitz_ball} ensures that the Hessian matrix of \(f\) is locally Lipschitz continuous in a neighborhood of \(\theta^*\). 
Similar assumptions have been previously considered in \cite{shao2022berry} and \cite{pmlr-v99-anastasiou19a}, as well as in other works on first-order optimization methods, see, e.g., \cite{li2022root}. In contrast, several studies on the non-asymptotic analysis of SGD impose even stronger smoothness assumptions, such as bounded derivatives of \(f\) up to order four, see \cite{durmus2020biassgd}. We also impose the following assumption on the step sizes \( \alpha_k\):

\begin{assum}
\label{ass:step_size}
Suppose that $\alpha_k = c_0 / (k_0+k)^{\gamma}$, where $\gamma \in (1/2, 1)$, $k_0\geq 1$, and $c_0$ satisfies $2c_0L_1\leq 1$.
\end{assum}

While it is more common in the literature (see, e.g., \cite{polyak1992acceleration}) to consider step sizes of the form $\alpha_k = c_0 / k^{\gamma}$, we emphasize that the results in this section do not require fine-tuning of the constant $k_0$. The choice of $k_0$ affects only constant factors, but does not alter the convergence rates. Some restrictions on $k_0$ will be required later in \Cref{sec:bootstrap}. 


\subsection{Central limit theorem for Polyak-Ruppert averaged SGD iterates}
\label{subsec:CLT_PR}
It is well established (see, e.g., \cite{polyak1992acceleration}) that under \Cref{ass:L-smooth}-\Cref{ass:step_size}, the following central limit theorem holds:
\begin{equation}
\label{eq:Sigma_infty_def}
\textstyle 
\sqrt{n}(\bar{\theta}_{n} - \thetas) \dto \gauss(0,\Sigma_\infty)\eqsp, \quad \text{where} \quad \Sigma_\infty = G^{-1} \Sigma_{\xi} G^{-\top}\eqsp, \quad \text{and} \quad G = \nabla^2f(\thetas)\eqsp.
\end{equation}
To derive Gaussian approximation rates, we follow the approach of \cite{shao2022berry}, expanding $\sqrt{n}(\bar{\theta}_{n} - \thetas)$ into a weighted sum of independent random vectors, along with the remaining terms of smaller order. By the Newton-Leibniz formula, we obtain
\begin{align}
\label{eq:H_theta_def}
\textstyle
\nabla f(\theta) = G(\theta-\thetas) + H(\theta), \quad \text{where} \quad H(\theta) = \int_{0}^1 (\nabla^2f(\thetas + t(\theta-\thetas)) - G)(\theta-\thetas)\, \rmd t.
\end{align}
Here $H(\theta)$ is of the order $\|\theta - \thetas\|^2$ (see \Cref{lem:H_theta_bound}).  
Using this notation, the recursion for the SGD algorithm error  \eqref{eq:sgd_recursion_main_new} can be expressed as
\begin{equation}
\label{eq: sgd_reccurence_with_remainder_main}
\textstyle
\theta_k - \thetas = (\Id_d - \alpha_k G) (\theta_{k-1} - \thetas) - \alpha_k ( \eta(\xi_k) + g(\theta_{k-1}, \xi_k) + H(\theta_{k-1}))\eqsp.
\end{equation}
For $i \in \{0,\ldots,n-1\}$, we define the matrices
\begin{equation}
\label{eq:Q_i_def}
\textstyle
Q_i = \alpha_i\sum_{j=i}^{n-1}\prod_{k=i+1}^{j}(\Id_d-\alpha_k G)\eqsp,
\end{equation}
where empty products are defined to be equal to $\Id$ by convention.
Then taking average of \eqref{eq: sgd_reccurence_with_remainder_main} and rearranging the terms, we obtain the following expansion: 
\begin{equation}
\label{eq:linear and nonlinear terms}
    \sqrt{n}(\bar\theta_n -\thetas) = W + D,
\end{equation}
where we set 
\begin{equation}
\label{eq: linear part}
\begin{split}
W &= - \frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}Q_i\eta(\xi_i), \\ D &= \frac{1}{\sqrt{n}\alpha_0}Q_0(\theta_0-\thetas)-\frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}Q_ig(\theta_{i-1}, \xi_i) - \frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}Q_iH(\theta_{i-1})\eqsp.
\end{split}
\end{equation}
Note that $W$ is a weighted sum of i.i.d. random vectors with mean zero and covariance matrix
\begin{equation}
\label{eq:sigma_n_def} 
\Sigma_n = \PE[W W^\top] = \frac{1}{n} \sum_{k=1}^{n-1} Q_k \noisecov Q_k^\top \eqsp.
\end{equation}
The decomposition \eqref{eq:linear and nonlinear terms} - \eqref{eq: linear part} represents a specific instance of the general problem of Gaussian approximation for nonlinear statistics of the form $\sqrt{n}(\bar\theta_n -\thetas)$, where the estimator is expressed as the sum of linear and nonlinear components. It is important to note that this decomposition is not unique, as the choice of the linear term \( W \) can be different. Specifically, the following representations have been considered in the literature:

\begin{itemize}[noitemsep, nolistsep]
    \item \cite{shao2022berry} adopted the form of \( W \) and \( D \) given in \eqref{eq: linear part}, a decomposition that was also employed in \cite{wu2024statistical} for temporal difference (TD) learning with linear function approximation.   
    \item \cite{samsonov2024gaussian} and \cite{srikant2024rates} proposed an alternative representation, taking advantage of the fact that \( Q_i \) is close to \( G^{-1} \). In their formulation, $\sqrt{n}(\bar{\theta}_n - \thetas)$ is decomposed as \( W^{\prime} + D^{\prime} \), where \begin{equation}
    W^{\prime} = - \frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}G^{-1} \eta(\xi_i).
    \end{equation}
    A notable property of this representation is that it satisfies \( \PE[W^{\prime}\{W^{\prime}\}^{\top}] = \Sigma_{\infty} \), making it particularly attractive for theoretical analysis.
\end{itemize}
The Gaussian approximation rates established in \cite{wu2024statistical} improve on the results of \cite{samsonov2024gaussian} and \cite{srikant2024rates}, although in a slightly different problem of temporal difference learning \cite{sutton:book:2018}, an algorithm which does not correspond to gradient dynamics. For this reason, we adopt the decomposition of \( W \) and \( D \) as given in \eqref{eq: linear part}. The result of \cite[Theorem 3.4]{shao2022berry} establishes convergence rates for  
\begin{equation}
\label{eq:conex_distance_shao_bound}
\kolmogorov(\sqrt{n}\Sigma_{n}^{-1/2}(\bar{\theta}_{n} - \thetas), \gauss(0,\Id_{d}))\eqsp,
\end{equation}
where \(\Sigma_n\) denotes the covariance matrix of the linear statistic \( W \) defined above.  
In \Cref{sec:shao_self-normalized}, we derive a version of the bound in \eqref{eq:conex_distance_shao_bound}, explicitly characterizing the constants and tracing the dependence of our results on the initial condition \(\norm{\theta_0 - \thetas}\). We then complete the proof by establishing a bound on \(\norm{\Sigma_n - \Sigma_{\infty}}\) under \Cref{ass:step_size}.

\subsection{Gaussian approximation result with $\Sigma_n$}
\label{sec:shao_self-normalized}
To establish the Gaussian approximation result, we adapt the arguments from \cite{shao2022berry}, which can be stated as follows. Let \( X_1, \ldots, X_n \) be independent random variables taking values in some space \( \mathcal{X} \), and let \( T = T(X_1, \ldots, X_n) \) be a general \( d \)-dimensional statistic that can be decomposed as  
\begin{equation}
\label{eq:W-D-decomposition} 
\textstyle 
W := W(X_1,\ldots,X_n) = \sum_{\ell = 1}^n Z_\ell, \quad D := D(X_1, \ldots, X_n) = T - W\eqsp.
\end{equation}
Here, we define \( Z_\ell = r_\ell(X_\ell) \), where \( r_\ell: \mathcal{X} \to \rset^d \) is a Borel-measurable function. The term \( D \) represents the nonlinear component and is treated as an error term, assumed to be "small" relative to \( W \) in an appropriate sense.  
Suppose that \( \PE[Z_\ell] = 0 \) and that the $Z_{\ell}$ is normalized in such a way that \( \sum_{\ell=1}^n \PE[Z_\ell Z_\ell^\top] = \Id_d \) holds. Let   $\Upsilon_n = \sum_{\ell=1}^n \PE[\|Z_\ell\|^3]$. Then, for \( Y \sim \mathcal{N}(0,\Id_d) \), the following bound holds:
\begin{equation}
\label{eq:shao_zhang_bound_main}
\kolmogorov(T, Y) \leq 259 d^{1/2} \Upsilon_{n} + 2 \PE[\|W\| \|D\|] + 2 \sum_{\ell=1}^n \PE[\|Z_\ell\| \|D - D^{(\ell)}\|],
\end{equation}
where \( D^{(\ell)} = D(X_1, \ldots, X_{\ell-1}, X_{\ell}^{\prime}, X_{\ell+1}, \ldots, X_n) \) and \( X_\ell^{\prime} \) is an independent copy of \( X_\ell \). This result follows from \cite[Theorem~2.1]{shao2022berry}. Furthermore, this bound can be extended to the case where \( \sum_{\ell=1}^n \PE[Z_\ell Z_\ell^\top] = \Sigma \succ 0 \), as detailed in \cite[Corollary~2.3]{shao2022berry}. In order to apply \eqref{eq:shao_zhang_bound_main}, we let $X_i = \xi_i$, $Z_{\ell} = h(X_{\ell})$, $\xi_{i}^{\prime}$ be an i.i.d. copy of $\xi_i$ and we need to upper bound $\PE^{1/2}[\norm{D(\xi_1,\ldots,\xi_{n-1})}^2]$ and $\PE^{1/2}[\norm{D - D_{i}^{\prime}}^2]$, respectively. We obtain the following result, which improves the previous bound obtained in \cite[Theorem~3.4] {shao2022berry} providing explicit problem-specific constants.
\begin{theorem}    
\label{th:bound_kolmogorov_dist_pr_sigma_n}
Assume \Cref{ass:L-smooth} - \Cref{ass:noise_decomposition}($4$)- \Cref{ass:hessian_Lipschitz_ball}-\Cref{ass:step_size}. Then, with $Y \sim \mathcal{N}(0,\Id_d)$, it holds that 
\begin{equation}
\label{eq:kolm-dist}
\kolmogorov(\sqrt{n} \Sigma_{n}^{-1/2}(\bar\theta_n -\thetas), Y) \leq \frac{\ConstC_1}{\sqrt{n}}   +  \frac{\ConstC_2}{n^{\gamma - 1/2}}+ \frac{\ConstC_3}{n^{\gamma/2}}\eqsp,
\end{equation}
where $\ConstC_1, \ConstC_2, \ConstC_3$ are given in \eqref{eq:def_Const_M_3_i} in the appendix.
\end{theorem}
\begin{proof}
The proof is given in section \ref{sec Gar in real worlkd proof} in the appendix.
\end{proof}
When \( \gamma \to 1 \),the correction terms above scale as \(\mathcal{O}(1/\sqrt{n}) \), yielding the overall approximation rate that approaches \( 1/\sqrt{n} \). Expressions for $\ConstC_1, \ConstC_2, \ConstC_3$ from \Cref{th:bound_kolmogorov_dist_pr_sigma_n} depend upon the problem dimension $d$, parameters specified in \Cref{ass:L-smooth} - \Cref{ass:noise_decomposition}($4$)- \Cref{ass:hessian_Lipschitz_ball}-\Cref{ass:step_size}. Moreover, $\ConstC_2$ depends upon $\norm{\theta_0-\thetas}$. When $\gamma \in (0,1)$, we have that $1/n^{\gamma/2} < 1/n^{\gamma-1/2}$, thus, the term $\ConstC_2/n^{\gamma-1/2}$ dominates. We prefer to keep both terms in \eqref{eq:kolm-dist}, since they are responsible for the moments of statistics $\frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}Q_iH(\theta_{i-1})$ and $\frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}Q_ig(\theta_{i-1}, \xi_i)$, respectively. The first of them has non-zero mean, since $H(\theta_{i-1})$ is quadratic in $\norm{\theta_i-\thetas}^2$. When using constant step size SGD, one can correct this term using the Richardson-Romberg technique \cite{durmus2020biassgd,SheshukovaICLR}, however, it is unclear if this type of ideas can be generalized for diminishing step size.

\begin{remark}
\label{rem:remark_1}
Since the matrix $\Sigma_n$ is non-degenerate, and an image of a convex set under non-degenerate linear mapping is a convex set, we have 
\begin{align}
\kolmogorov(\sqrt{n}\Sigma_n^{-1/2}(\bar\theta_n -\thetas), Y) = \kolmogorov(\sqrt{n}(\bar \theta_n -\thetas), \Sigma_n^{1/2} Y)\eqsp.
\end{align}
\end{remark}

\subsection{Convergence rate in CLT for averaged SGD iterates \eqref{eq:Sigma_infty_def}}
Next, we demonstrate how the result of \Cref{th:bound_kolmogorov_dist_pr_sigma_n} can be utilized to quantify the convergence rate in \eqref{eq:Sigma_infty_def}. The key step in establishing this result is the following lemma:

\begin{lemma}
\label{lem:bound_kolmogorov_dist_sigma_n_sigma_infty}
Assume that \cref{ass:L-smooth} and \Cref{ass:step_size} holds. Let \( Y, Y^\prime \sim \mathcal{N}(0, \Id_d) \). Then, the Kolmogorov distance between the distributions of \( \Sigma_n^{1/2} Y \) and \( \Sigma_\infty^{1/2} Y^\prime \) is bounded by  
\begin{equation}
    \kolmogorov (\Sigma_n^{1/2} Y, \Sigma_\infty^{1/2} Y^\prime) \leq  C_{\infty} n^{\gamma-1} \eqsp,
\end{equation}
where the constant \( C_{\infty} \) is defined in \eqref{eq:def_C_infty}.
\end{lemma}
\begin{proof}
The closedness of Gaussian measures in total variation distance, and consequently in convex distance, can be controlled via the Frobenius norm of the covariance perturbation. Specifically, we have  
\begin{equation}
\label{eq:kolmogorov_}
\kolmogorov (\Sigma_n^{1/2} Y, \Sigma_\infty^{1/2} Y^\prime) \leq (3/2) \frobnorm{\Sigma_n^{-1/2} \Sigma_\infty \Sigma_n^{-1/2} - \Id_d} \eqsp.
\end{equation}
This result follows directly from \Cref{Pinsker}. To complete the proof, it remains to establish an upper bound on \( \|\Sigma_n - \Sigma_\infty\| \), which is provided in \Cref{sec: proof of difference between cov}.
\end{proof}
\Cref{lem:bound_kolmogorov_dist_sigma_n_sigma_infty} and triangular inequality imply the following result on closeness to $\mathcal{N}(0,\Sigma_{\infty})$. 

\begin{theorem}
\label{cor:berry-esseen}
Assume \Cref{ass:L-smooth} - \Cref{ass:noise_decomposition}($4$)- \Cref{ass:hessian_Lipschitz_ball}-\Cref{ass:step_size}. Then, with $Y \sim \mathcal{N}(0,\Id_d)$ it holds that 
    \begin{equation}
    \label{eq:Berry-Esseen_Bound_Sigma_infty}
        \kolmogorov(\sqrt{n}(\bar\theta_n -\thetas), \Sigma_\infty^{1/2} Y)  \leq 
        \frac{\ConstC_1}{\sqrt{n}}   +  \frac{\ConstC_2}{n^{\gamma - 1/2}}+ \frac{\ConstC_{3}}{n^{\gamma/2}} + \frac{C_\infty}{ n^{1 - \gamma}} \eqsp,
   \end{equation}
   where $\ConstC_1, \ConstC_2$ and $\ConstC_3$ are given in Theorem \ref{th:bound_kolmogorov_dist_pr_sigma_n}. 
\end{theorem}
\begin{proof}
    The proof follows directly from \Cref{th:bound_kolmogorov_dist_pr_sigma_n}, \Cref{lem:bound_kolmogorov_dist_sigma_n_sigma_infty} and triangle inequality.
\end{proof}

\paragraph{Discussion.} Theorem~\ref{th:bound_kolmogorov_dist_pr_sigma_n} reveals that the normal approximation through \(\mathcal{N}(0,\Sigma_n)\) improves when the step sizes \(\alpha_k\) are less aggressive, that is, as \(\gamma \to 1\). However, \Cref{cor:berry-esseen} shows that there is a trade-off, since the rate at which \(\Sigma_n\) converges to \(\Sigma_{\infty}\) also affects the overall quality of the approximation. Optimizing the bound in \eqref{eq:Berry-Esseen_Bound_Sigma_infty} for \(\gamma\) yields an optimal value of \(\gamma = 3/4\), leading to the following approximation rate:
\begin{align}
\label{eq:optimal_Berry-Esseen_Bound_Sigma_infty} 
\kolmogorov(\sqrt{n}(\bar\theta_n -\thetas), \Sigma_\infty^{1/2} Y)  \leq \frac{\ConstC_1^{\prime}}{n^{1/4}} + \frac{\ConstC_2^{\prime}}{\sqrt{n}}(\norm{\theta_0-\thetas}+\norm{\theta_0-\thetas}^2)\eqsp,
\end{align}
where $\ConstC_1^{\prime}$ and $\ConstC_2^{\prime}$ are instance-dependent quantities (but not depending on $\norm{\theta_0-\thetas}$), that can be inferred from \Cref{cor:berry-esseen}. Given the result of \Cref{cor:berry-esseen} one can proceed with a non-asymptotic evaluation of the methods for constructing confidence intervals based on direct estimation of $\Sigma_{\infty}$, such as \cite{chen2020aos,zhu2023online_cov_matr}.  