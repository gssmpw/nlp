\subsection{Example of distribution satisfying \Cref{ass:bound_bootstap_weights}}
\label{sec:example_distribution}
To construct examples of distributions satisfying the above assumption, one can use the beta distribution, which is defined on \([0, 1]\), and then shift and scale it. Set \( W = a + bX \)  where \( X \sim \text{Beta}(\alpha, \beta) \) and \(a,b>0.\) We have \( \mathbb{E}[X] = \frac{\alpha}{\alpha + \beta}, \)  \( \text{Var}(X) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)} \) and \(a\leq W\leq a+b \) a.s.  By solving (for \( a \) and \( b \)) the equations \( \mathbb{E}[W] = a + b\mathbb{E}[X] = 1 \) and \( \text{Var}(W) = b^2\text{Var}(X) = 1, \) we derive \(b=1/\sqrt{\text{Var}(X)}\) and \(a=1-\mathbb{E}[X]/\sqrt{\text{Var}(X)}.\) Note that $a>0$ provided $\alpha+\beta+1<\beta/\alpha$.


\subsection{From non-linear to linear statistics}
\label{sec: from nonlinear to linear boot}
In this section we prove \eqref{crude bound for nonlinear stat}. We start from the definition of an isoperimetric constant.  
Define 
\[
A^{\varepsilon} = \{x \in \rset^d : \rho_A(x) \leq \varepsilon\}
\quad \text{and} \quad
A^{-\varepsilon} = \{x \in A : B_{\varepsilon}(x) \subset A\},
\]
where $\rho_A(x) = \inf\limits_{y \in A} \|x - y\|$ is the distance between $A \subset \rset^d$ and $x \in \rset^d$, and 
\[
B_{\varepsilon}(x) = \{y \in \rset^d : \|x - y\| \leq \varepsilon\}.
\]
For some class $\mathcal A$ of subsets of $\rset^d$ we define its isoperimetric constant $a_d(\mathscr{A})$ (depending only on $d$ and $\mathscr{A}$)  as follows: for all $A \in \mathscr{A}$  and $\varepsilon > 0$,
\[
\mathbb{P} \{ Y \in A^{\varepsilon} \setminus A \} \leq a_d \varepsilon, \quad
\mathbb{P} \{ Y \in A \setminus A^{-\varepsilon} \} \leq a_d \varepsilon
\]
where  $Y$ follows the standard Gaussian distribution on $\rset^d$. \citep{ball_reverse_1993} has proved that
\begin{equation}
\label{ball}
    e^{-1} \sqrt{\ln d} \leq \sup_{A \in \mathscr{C}} \int_{\partial A} p(x) \, \mathrm{d}s \leq 4 d^{1/4},
\end{equation}
where $p(x)$ is the standard normal $d$-dimensional density and $ds$ is the surfac emeasure on the boundary $\partial A$ of $A$. Using \eqref{ball} one can show that for the class of convex sets 
\begin{equation}
\label{eq: isoperimetric const convex}
    e^{-1} \sqrt{\ln d} \leq a_d(\Conv(\rset^d)) \le   4 d^{1/4} \eqsp. 
\end{equation}
We denote $c_d = a_d(\Conv(\rset^d))$. 
\begin{proposition}
\label{nonlinearapprox}
Let $\nu$ be a standard Gaussian measure in $\rset^d$. Then for any random vectors $X, Y$ taking values in $\rset^d$, and any $p \geq 1$,
  \begin{equation}
\label{eq:shao_zhang_bound}
\sup_{B \in \Conv(\rset^d)}|\PP(X + Y \in B) - \nu(B)| \le \sup_{B \in \Conv(\rset^d)}|\PP(X
\in B) - \nu(B)| + 2 c_d^{p/(p+1)} \PE^{1/(p+1)}[\|Y\|^p]\eqsp, 
\end{equation}
where $c_d$ is the isoperimetric constant of class $\Conv(\rset^d)$.
\end{proposition}
\begin{proof}
Let $\varepsilon \geq 0$. Define
$\rho(B) = \PP(X + Y \in B) - \nu(B)$. 
Let $B$ be such that $\rho(B) \geq 0$.  By Markov's inequality
\begin{multline}
\rho(B) \le \PP(X + Y \in B, |Y| \le \varepsilon) + \frac{1}{\varepsilon^{p}} \PE [\|Y\|^p] - \nu(B) \\
    \le \sup_{B}|\PP(X \in B) - \nu(B)| + \PP(Y \in A^\varepsilon \setminus A) + \frac{1}{\varepsilon^{p}} \PE [\|Y\|^p].  
\end{multline}   
Choosing 
\begin{equation}
\label{eps choice}
    \varepsilon = \frac{1}{c_d^{1/(p+1)}}\PE^{1/(p+1)}[\|Y\|^p]
\end{equation}
we obtain
\begin{equation}
\sup_{B}|\PP(X + Y \in B) - \nu(B)| \le \sup_{B }|\PP(X \in B) - \nu(B)| + 2 c_d^{p/(p+1)} \PE^{1/(p+1)}[\|Y\|^p] \eqsp. 
\end{equation}
Assume now that $\rho(A) < 0$. We distinguish between $A^{-\varepsilon} = \emptyset$ or $A^{-\varepsilon} \neq \emptyset$. In the first case, $\PP(Y \in A^{-\varepsilon}) = 0$ and
$$
-\rho(A) \le \gamma(A) = \PP(Y \in A) - \PP(Y \in A^{-\varepsilon}) = \PP(Y \in A \setminus A^{-\varepsilon}) \le c_d \varepsilon.  
$$
Finally, in the case $A^{-\varepsilon} \neq \emptyset$,
$$
-\rho(A) \le \sup_{B}|\PP(X \in B) - \nu(B)| + \PP(Y \in A \setminus A^{-\varepsilon}) + \frac{1}{\varepsilon^{p}} \PE [\|Y\|^p]\eqsp. 
$$
Taking $\varepsilon$ as in \eqref{eps choice}
we conclude the proof.
\end{proof}



% \begin{lemma}
%     Assum ...
%     \begin{equation}
%         \norm{\theta_k^{\boot} - \theta_k}^2 \lesssim \alpha_k \eqsp. 
%     \end{equation}
% \end{lemma}
% \begin{proof}
% Note that 
% \begin{multline}
%  \theta_k^{\boot} - \theta_k =  \theta_{k-1}^{\boot} -\theta_{k-1}- \alpha_{k} w_k\{\nabla f(\theta_{k-1}^{\boot})+g(\theta_{k-1}^{\boot}, \xi_k) + \eta(\xi_k)\} + \alpha_{k}\{\nabla f(\theta_{k-1})+g(\theta_{k-1}, \xi_k) + \eta(\xi_k)\}\\=
%  \theta_{k-1}^{\boot} -\theta_{k-1}- \alpha_{k}(w_k-1)\{\nabla f(\theta_{k-1}^{\boot})+g(\theta_{k-1}^{\boot}, \xi_k) + \eta(\xi_k)\}\\- \alpha_{k}\{\nabla f(\theta_{k-1}^{\boot})+g(\theta_{k-1}^{\boot}, \xi_k)-\nabla f(\theta_{k-1})-g(\theta_{k-1}, \xi_k) \}\eqsp.
% \end{multline}
% Using \cref{ass:L-co-coercivity-assum}, \cref{ass:L-smooth}, \Cref{ass:noise_decomposition} and \cref{ass:bound_noise}, we get 
% \begin{align}
%     &\PE_{w_k}[\norm{\theta_k^{\boot} - \theta_k}^2] \leq \norm{\theta_{k-1}^{\boot} -\theta_{k-1}}^2  +   \alpha_{k}^2\norm{\nabla f(\theta_{k-1}^{\boot})+g(\theta_{k-1}^{\boot}, \xi_k) + \eta(\xi_k)}^2
%     \\& \qquad \qquad-(2\alpha_k-\alpha_k^2L_3)\langle\nabla f(\theta_{k-1}^{\boot})
%   +g(\theta_{k-1}^{\boot}, \xi_k)-\nabla f(\theta_{k-1})-g(\theta_{k-1}, \xi_k), \theta_{k-1}^{\boot} -\theta_{k-1}\rangle \\  
%     &\qquad \leq
%     (1-(2\alpha_k-\alpha_k^2L_3)\mu)\norm{\theta_{k-1}^{\boot} -\theta_{k-1}}^2 + 2\alpha_k^2(L_1+L_2)^2\norm{\theta_{k-1}^{\boot}-\thetas}^2 + 2\alpha_k^2\norm{\eta(\xi_k)}^2
%     \\& \qquad \qquad -(2\alpha_k-\alpha_k^2L_3)\langle
%   g(\theta_{k-1}^{\boot}, \xi_k)-g(\theta_{k-1}, \xi_k), \theta_{k-1}^{\boot} -\theta_{k-1}\rangle\\ 
%     &\qquad \leq ?
%     % (1-\alpha_k(2/L_3 - \alpha_k)(L_1+L_2)^2 + 4\alpha_k^2(L_1+L_2)^2)\norm{\theta_{k-1}^{\boot} -\theta_{k-1}}^2 \\
%     % &\qquad \qquad \qquad \qquad \qquad + 4\alpha_k^2(L_1+L_2)^2\norm{\theta_{k-1}-\thetas}^2 + 2\alpha_k^2 \norm{\eta(\xi_k)}^2\eqsp.
% \end{align}
% \end{proof}





\subsection{High probability bounds on the last iterate}

\begin{lemma}
\label{lem: high_prob_last_iter_boot}
Assume \Cref{ass:L-smooth}, \Cref{ass:L-co-coercivity-assum}, \Cref{ass:bound_noise}, \Cref{ass:bound_bootstap_weights} \Cref{ass:step_size_new_boot}. Then for any $\delta \in (0,1)$ with probability at least $1-\delta$ for any $k\in\{1,\ldots n\}$ it holds 
\begin{equation}
    \norm{\theta_k^b - \thetas}^2 \leq \alpha_k K_1\log\biggl(\frac{\rme n}{\delta}\biggr)\eqsp,
\end{equation}
where 
\begin{equation}
    \label{eq:def_K_1}
    K_1 = \max\biggl(\frac{8W_{\max}^2(C_{1,\xi}+2C_{2,\xi})^2}{W_{\min}\mu},\frac{k_0^\gamma\norm{\theta_0-\thetas}^2}{c_0} \biggr)
\end{equation}
\end{lemma}
\begin{proof}
Using \eqref{eq:sgd_bootstrap}, we have
\begin{align}
    \norm{\theta_k^b-\thetas}^2 &= \norm{\theta_{k-1}^b-\thetas}^2 - 2\alpha_kw_k\langle  F(\theta_{k-1}^b, \xi_k), \theta_{k-1}^b-\thetas \rangle + \alpha_k^2w_k^2\norm{\nabla F(\theta_{k-1}^b, \xi_k)}^2 \\& \leq
    \norm{\theta_{k-1}^b-\thetas}^2 - 2\alpha_kw_k\langle   F(\theta_{k-1}^b, \xi_k)-  F(\thetas, \xi_k), \theta_{k-1}^b-\thetas \rangle \\& \qquad \qquad- 2\alpha_kw_k\langle \eta(\xi_k), \theta_{k-1}^b-\thetas \rangle+  2\alpha_k^2w_k^2\norm{ F(\theta_{k-1}^b, \xi_k)- F(\thetas, \xi_k)}^2 + 2\alpha_k^2w_k^2\norm{\eta(\xi_k)}^2 \eqsp.
\end{align}
Using \Cref{ass:L-co-coercivity-assum} and \Cref{ass:step_size_new_boot}, we obtain 
\begin{align}
     \norm{\theta_k^b-\thetas}^2 &\leq \norm{\theta_{k-1}^b-\thetas}^2 - 2\alpha_kw_k(1-\alpha_kw_kL_4)\langle   F(\theta_{k-1}^b, \xi_k)-  F(\thetas, \xi_k), \theta_{k-1}^b-\thetas \rangle \\& \qquad \qquad \qquad - 2\alpha_kw_k\langle \eta(\xi_k), \theta_{k-1}^b-\thetas \rangle + 2\alpha_k^2w_k^2\norm{\eta(\xi_k)}^2 \\&\leq  \norm{\theta_{k-1}^b-\thetas}^2 - \alpha_kw_k\langle   F(\theta_{k-1}^b, \xi_k)-  F(\thetas, \xi_k), \theta_{k-1}^b-\thetas \rangle \\& \qquad \qquad \qquad - 2\alpha_kw_k\langle \eta(\xi_k), \theta_{k-1}^b-\thetas \rangle + 2\alpha_k^2w_k^2\norm{\eta(\xi_k)}^2
\end{align}
 Using  \Cref{ass:L-smooth}, we have
\begin{equation}
    \norm{\theta_k^b-\thetas}^2 \leq
    (1-\mu\alpha_kw_k)\norm{\theta_{k-1}^b-\thetas}^2 - \alpha_kw_k\langle g(\theta_{k-1}^b,\xi_k)+2\eta(\xi_k), \theta_{k-1}^b-\thetas \rangle + 2\alpha_k^2w_k^2\norm{\eta(\xi_k)}^2 \eqsp.
\end{equation}
Using \Cref{ass:bound_bootstap_weights}, we get 
\begin{equation}
\label{eq:last_iter_reccurrence_high_prob}
\begin{split}
    \norm{\theta_k^b-\thetas}^2 &\leq
    (1-\mu\alpha_kW_{\min})\norm{\theta_{k-1}^b-\thetas}^2 - \alpha_kw_k\langle g(\theta_{k-1}^b,\xi_k)+2\eta(\xi_k), \theta_{k-1}^b-\thetas \rangle \\
    & + 2\alpha_k^2W_{\max}^2\norm{\eta(\xi_k)}^2 \eqsp.
    \end{split}
\end{equation}
Define $Y_k = \alpha_k^{-1}\norm{\theta_k^b-\thetas}^2$, and $\hat X_{k-1}  = \frac{w_k\langle g(\theta_{k-1}^b, \xi_k)+2\eta(\xi_k), \theta_{k-1}^b-\thetas \rangle}{W_{\max}(C_{2,\xi} + 2C_{1,\xi})\norm{\theta_{k-1}^b-\thetas}} $, then using \eqref{eq:last_iter_reccurrence_high_prob}, we obtain 
\begin{equation}
    Y_k \leq\alpha_k^{-1} \alpha_{k-1}(1-\mu W_{\min}\alpha_k)Y_{k-1} - \sqrt{\alpha_{k-1}}W_{\max}(C_{2,\xi} + 2C_{1,\xi})\hat X_{k-1}\sqrt{Y_{k-1}}+ 2W_{\max}^2\alpha_kC_{1,\xi}^2\eqsp.
\end{equation}
Note that 
\begin{align}
\label{eq:bound_contracting_multiplier_last_iter}
     \frac{\alpha_{k-1}}{\alpha_k}(1-\mu W_{\min}\alpha_k) &= \biggl(\frac{k_0+k}{k_0+k-1}\biggr)^{\gamma} -\frac{\mu W_{\min}c_0}{(k_0+k-1)^{\gamma}}\\&\leq 1 + \frac{c_0(\gamma/c_0)}{k_0+k-1}-\frac{\mu W_{\min}c_0}{(k_0+k-1)^{\gamma}}\\& = 1 - \alpha_{k-1}\biggr( \mu W_{\min} -\frac{(\gamma/c_0)}{(k_0+k-1)^{1-\gamma}}\biggl)\eqsp.
\end{align}
 Since $k_0\geq \biggl(\frac{2\gamma}{c_0\mu W_{\min}}\biggr)^{1/(1-\gamma)}$, we have 
\begin{equation}
    Y_k \leq(1-\frac{\mu W_{\min}}{2}\alpha_{k-1})Y_{k-1} - \sqrt{\alpha_{k-1}}W_{\max}(C_{2,\xi} + 2C_{1,\xi})\hat X_{k-1}\sqrt{Y_{k-1}}+ 2W_{\max}^2\alpha_kC_{1,\xi}^2\eqsp.
\end{equation}

Note that using \Cref{ass:noise_decomposition} and \Cref{ass:bound_noise}, we have
\begin{equation}
\begin{split}
    &\PE[\hat X_{k-1}|\widetilde{\F}_{k-1}] = 0\\
    &\norm{\hat X_{k-1}} \leq \frac{\norm{w_i}(\norm{g(\theta_{k-1}^b, \xi_k)}+2\norm{\eta(\xi_k)})\norm{\theta_{k-1}^b-\thetas \rangle}}{W_{\max}(C_{2,\xi} + 2C_{1,\xi})\norm{\theta_{k-1}^b-\thetas}} \leq 1 \eqsp,
\end{split}
\end{equation}
where $\widetilde{\F}_{k-1}$ is defined in \eqref{eq: extended filtration}. Then using \cite[Theorem 4.1]{harvey2019tight}, with probability at least $1-\delta$ for $\forall k\in\{1,\ldots n\}$
\begin{equation}
     Y_k \leq K_1 \log\biggl(\frac{\rme n}{\delta}\biggr), 
\end{equation}
where $K_1 = \underset{k\in\{1,\ldots n\}}{\max}\biggl(\frac{8\alpha_kW_{\max}^2 C_{1,\xi}^2}{\mu W_{\min}\alpha_{k-1}}, \frac{4W_{\max}^2(C_{1,\xi}+2C_{2,\xi})^2}{\mu W_{\min}}, \frac{k_0^\gamma\norm{\theta_0-\thetas}^2}{c_0}\biggr) \leq  \max\biggl(\frac{8W_{\max}^2(C_{1,\xi}+2C_{2,\xi})^2}{W_{\min}\mu},\frac{k_0^\gamma\norm{\theta_0-\thetas}^2}{c_0} \biggr)$.
\end{proof}

\begin{corollary}
\label{cor: last_iter_boot_p_moment}
Under the assumptions of \Cref{lem: high_prob_last_iter_boot}
for any $ k\in\{1,\ldots n\}$ and any $p\geq 2$ it holds 
\begin{equation}
     \PE^{2/p}[\norm{\theta_k^b - \thetas}^{p}] \leq p \alpha_k(\rme n)^{2/p} K_1/2 \eqsp,
\end{equation}
where $K_1$ is defined in \eqref{eq:def_K_1}.
\end{corollary}
\begin{proof}
Note that from \Cref{lem: high_prob_last_iter_boot} for $\forall k\in\{1,\ldots n\}$ and for any $t\geq 0$ it holds 
    \begin{equation}
    \PP\bigl[\norm{\theta_k^b - \thetas}^2 \geq t\bigr]\leq f(t)\eqsp,
\end{equation}
where 
\[
f(t) = \rme n\exp\biggl\{-\frac{t}{K_1\alpha_k}\biggr\}\eqsp.
\]
Then, we have 
\begin{align}
    \PE[\norm{\theta_k^b - \thetas}^{p}] &= \int_{0}^{+\infty}\PP[\norm{\theta_k^b - \thetas}^{p} > u]\rmd u \leq\int_{0}^{+\infty}\rme n\exp\biggl\{-\frac{u^{2/p}}{K_1\alpha_k}\biggr\}\rmd u \\&=\rme n(p/2)\biggl(K_1\alpha_k\biggr)^{p/2}\int_{0}^{+\infty}\rme^{-x}x^{p/2-1}\rmd x \leq\rme n\biggl((p/2)K_1\alpha_k\biggr)^{p/2}\eqsp,
\end{align}
where in the last inequality we use that $\Gamma(p/2) \leq (p/2)^{p/2 -1}$ (see \cite[Theorem 1.5]{AndersonGammaFunc}).
% For arbitrary $\theta_0^\boot = \theta_0$ we consider synchronous coupling contraction. Applying the similar arguments as in \Cref{prop:2p-moment-bound}, we obtain 
% \begin{equation}
%     \PE[\norm{\theta_k^\boot-\theta_k'^\boot}^p] \leq \PE^{1/2}[\norm{\theta_k^\boot-\theta_k'^\boot}^{2p}] \leq (C_{p}^\boot)^p\exp\biggl\{-\frac{p\mu c_0W_{\min}}{1-\gamma}\biggr\}\norm{\theta_0-\thetas}^p\eqsp,
% \end{equation}
% where 
% \begin{equation}
%       C_{p}^\boot = \exp\biggl\{\exp\biggl\{ 3pc_0W_{\max}(L_1+L_2)\biggr\}\frac{2p^2W_{\max}^2(L_1+L_2)^2)}{2\gamma-1} + \frac{p\mu c_0}{1-\gamma}k_0^{1-\gamma}\biggr\}\eqsp.
% \end{equation}
\end{proof}

\begin{lemma}
\label{lem: high_prob_last_iter}
Assume \Cref{ass:L-smooth}, \Cref{ass:L-co-coercivity-assum}, \Cref{ass:bound_noise}, \Cref{ass:step_size_new_boot}. Then for any $\delta \in (0,1)$ with probability at least $1-\delta$  for any $k\in\{1,\ldots n\}$ it holds 
\begin{equation}
    \norm{\theta_k - \thetas}^2 \leq \alpha_k K_2 log\biggl(\frac{\rme n}{\delta}\biggr)\eqsp, 
\end{equation}
where 
\begin{equation}
    \label{eq:def_K_2}
    K_2 = \max\biggl(\frac{8(C_{1,\xi}+2C_{2,\xi})^2}{\mu}, \frac{k_0^{\gamma}\norm{\theta_0-\thetas}^2}{\alpha_0}\biggr)
\end{equation}
Moreover, it holds 
for any $k\in\{1,\ldots n\}$ and any $p\geq 2$ that 
\begin{equation}
     \PE^{2/p}[\norm{\theta_k - \thetas}^{p}]\leq p\alpha_k (\rme n)^{2/p}K_2/2\eqsp.
\end{equation}
\end{lemma}
\begin{proof}
    The proof is similar to the proof of \cref{lem: high_prob_last_iter_boot} and \Cref{cor: last_iter_boot_p_moment}. 
\end{proof}
% \begin{proof}
% Note that 
% \begin{align}
%     &\norm{\theta_k-\thetas}^2 = \norm{\theta_{k-1}-\thetas}^2 - 2\alpha_k\langle F(\theta_{k-1}, \xi_k), \theta_{k-1}-\thetas \rangle + \alpha_k^2\norm{ F(\theta_{k-1}, \xi_k)}^2 \\&\leq
%     \norm{\theta_{k-1}-\thetas}^2 - 2\alpha_k\langle F(\theta_{k-1}, \xi_k), \theta_{k-1}-\thetas \rangle + 2\alpha_k^2\norm{F(\theta_{k-1}, \xi_k)-F(\thetas, \xi_k)}^2 + 2\alpha_k^2\norm{\eta(\xi_k)}^2 \eqsp.
% \end{align}
% Using \Cref{ass:L-co-coercivity-assum}, \Cref{ass:mu-convex} and \Cref{ass:step_size_new_boot}, we have
% \begin{align}
%     \norm{\theta_{k}-\thetas}^2 &\leq
%     \norm{\theta_{k-1}-\thetas}^2 - (2\alpha_k- 2\alpha_k^2L_3)\langle  F(\theta_{k-1}, \xi_k)-F(\thetas, \xi_k), \theta_{k-1}-\thetas \rangle \\
%     & \qquad \qquad \qquad \qquad \qquad - 2\alpha_k\langle \eta(\xi_k), \theta_{k-1}-\thetas \rangle + 2\alpha_k^2\norm{\eta(\xi_k)}^2 \\
%     &\leq (1-\mu\alpha_k)\norm{\theta_{k-1}-\thetas}^2 - \alpha_k\langle g(\theta_{k-1}, \xi_k)+2\eta(\xi_k), \theta_{k-1}-\thetas \rangle +  2\alpha_k^2\norm{\eta(\xi_k)}^2\eqsp.
% \end{align}
% Finally, using \Cref{ass:bound_noise}, we get 
% \begin{equation}
% \label{eq:last_iter_reccurrence_high_prob}
%      \norm{\theta_k-\thetas}^2 \leq (1-\mu\alpha_k)\norm{\theta_{k-1}-\thetas}^2 - \alpha_k\langle g(\theta_{k-1}, \xi_k)+2\eta(\xi_k), \theta_{k-1}-\thetas \rangle 
%     + 2\alpha_k^2C_{1,\xi}^2\eqsp.
% \end{equation}
% Set 
% \[
% Y_k = \alpha_k^{-1}\norm{\theta_k-\thetas}^2 \quad \text{ and } \quad \hat X_{k-1}  = \frac{\langle g(\theta_{k-1}, \xi_k)+2\eta(\xi_k), \theta_{k-1}-\thetas \rangle}{(C_{2,\xi} + 2C_{1,\xi})\norm{\theta_{k-1}-\thetas}}\eqsp.
% \]
% Then using \eqref{eq:last_iter_reccurrence_high_prob}, we obtain 
% \begin{equation}
%     Y_k \leq\alpha_k^{-1} \alpha_{k-1}(1-\mu\alpha_k)Y_{k-1} - (C_{2,\xi} + 2C_{1,\xi})\sqrt{\alpha_{k-1}} \hat{X}_{k-1}\sqrt{Y_{k-1}}+ 2\alpha_kC_{1,\xi}^2\eqsp.
% \end{equation}
% Note that 
% \begin{align}
% \label{eq:bound_contracting_multiplier_last_iter}
%      \frac{\alpha_{k-1}}{\alpha_k}(1-\mu\alpha_k) &= \biggl(\frac{k_0+k}{k_0+k-1}\biggr)^{\gamma} -\mu\frac{\alpha_0}{(k_0+k-1)^{\gamma}}\\&\leq 1 + \frac{\alpha_0(\gamma/\alpha_0)}{k_0+k-1}-\mu\frac{\alpha_0}{(k_0+k-1)^{\gamma}}\\& = 1 - \alpha_{k-1}\biggr( \mu -\frac{(\gamma/\alpha_0)}{(k_0+k-1)^{1-\gamma}}\biggl)
% \end{align}
% Then, since $k_0 \geq (\frac{2\gamma}{\mu\alpha_0})^{1/(1-\gamma)}$, we have  
% \begin{equation}
%     Y_k \leq (1-\frac{\mu}{2}\alpha_{k-1})Y_{k-1} - (C_{2,\xi} + 2C_{1,\xi})\sqrt{\alpha_{k-1}}\hat{X}_{k-1}\sqrt{Y_{k-1}}+ 2\alpha_kC_{1,\xi}^2\eqsp.
% \end{equation}
% Using \Cref{ass:noise_decomposition} and \Cref{ass:bound_noise}, we get that $X_{k-1}$ is $\F_k$-measurable, and 
% \begin{equation}
% \begin{split}
%     &\PE[\hat X_{k-1}|\F_{k-1}] = 0\\
%     &\norm{\hat X_{k-1}} \leq \frac{(\norm{g(\theta_{k-1}, \xi_k)}+2\norm{\eta(\xi_k)})\norm{\theta_{k-1}-\thetas}}{(C_{2,\xi} + 2C_{1,\xi})\norm{\theta_{k-1}-\thetas}} \leq 1\eqsp.
% \end{split}
% \end{equation}
% Then using \cite[Theorem 4.1]{harvey2019tight}, with probability at least $1-\delta$ it holds for any $k\in\{1,\ldots n\}$ that
% \begin{equation}
%      Y_k \leq K \log\biggl(\frac{\rme n}{\delta}\biggr)\eqsp, 
% \end{equation}
% where $K = \underset{k\in\{1,\ldots n\}}{\max}\biggl(\frac{8\alpha_k C_{1,\xi}^2}{\mu\alpha_{k-1}}, \frac{4 (C_{1,\xi}+2C_{2,\xi})^2}{\mu}\biggr) \leq  \frac{8(C_{1,\xi}+2C_{2,\xi})^2}{\mu}$.
% \end{proof}

% \begin{corollary}
% \label{cor: last_iter_p_moment}
% Under assumptions of \Cref{lem: high_prob_last_iter}, it holds 
% for any $k\in\{1,\ldots n\}$ and any $p\geq 2$ that 
% \begin{equation}
%      \PE^{2/p}[\norm{\theta_k - \thetas}^{p}]\leq p (\rme n)^{2/p} \frac{4(C_{1,\xi}+2C_{2,\xi})^2\alpha_k}{\mu}
% \end{equation}
% \end{corollary}
% \begin{proof}
% Note that from \Cref{lem: high_prob_last_iter_boot} for $\forall k\in\{1,\ldots n\}$ and for any $t\geq 0$ it holds 
%     \begin{equation}
%     \PP\biggl[\norm{\theta_k - \thetas}^2 \geq t\biggr]\leq f(t)\eqsp,
% \end{equation}
% where 
% \[
% f(t) = \rme n\exp\biggl\{-\frac{t\mu}{8(C_{1,\xi}+2C_{2,\xi})^2\alpha_k}\biggr\}\eqsp.
% \]
% Then, we have 
% \begin{align}
%     \PE[\norm{\theta_k - \thetas}^{p}] &= \int_{0}^{+\infty}\PP[\norm{\theta_k - \thetas}^{p} > u]\rmd u  \\ &\leq\int_{0}^{+\infty}\rme n\exp\biggl\{-\frac{u^{2/p}\mu}{8(C_{1,\xi}+2C_{2,\xi})^2\alpha_k}\biggr\}\rmd u \\&=\rme n(p/2)\biggl(\frac{8(C_{1,\xi}+2C_{2,\xi})^2\alpha_k}{\mu}\biggr)^{p/2}\int_{0}^{+\infty}\rme^{-x}x^{p/2-1}\rmd x \\&\leq\rme n\biggl(\frac{4p (C_{1,\xi}+2C_{2,\xi})^2\alpha_k}{\mu}\biggr)^{p/2}\eqsp,
% \end{align}
% where in the last inequality we use that $\Gamma(p/2) \leq (p/2)^{p/2 -1}$ (see \cite[Theorem 1.5]{AndersonGammaFunc}).
% \end{proof}

\subsection{Bounds for $D^\boot$}
Recall that the term $D^\boot$ defined in \eqref{eq:D_boot-def}, has a form: 
\begin{equation}
\label{eq:D_boot-def-appendix}
\begin{split}
        D^\boot &= -\frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}(w_i-1) Q_{i}\biggl( G(\theta_{i-1}^{\boot}-\thetas) + g(\theta_{i-1}^{\boot}, \xi_i) + H(\theta_{i-1}^{\boot})\biggr) \\&- \frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}Q_i\biggl(H(\theta_{i-1}^{\boot}) +g(\theta_{i-1}^{\boot}, \xi_i) -  H(\theta_{i-1}) -g(\theta_{i-1}, \xi_i)\biggr)\eqsp. 
\end{split}
\end{equation}
The following proposition estimates the moments of $D^\boot$. 
\begin{proposition} \label{prop:prob-D-boot-bound}
Assume \Cref{ass:L-smooth}, \Cref{ass:hessian_Lipschitz_ball}, \Cref{ass:L-co-coercivity-assum}, \Cref{ass:bound_noise}, \Cref{ass:bound_bootstap_weights},  \Cref{ass:step_size_new_boot}. Then it holds for any $p \geq 2$ that 
\begin{equation}
\label{eq:D-boot-joint-bound}
\PE^{1/p}[\norm{D^\boot}^p] \leq M_{1,1}^{\boot}\rme^{1/p}p^{3/2}n^{1/p-\gamma/2} +  M_{2,1}^{\boot}\rme^{2/p}p n^{1/2+2/p-\gamma}\eqsp,
\end{equation}
where the constants are given by 
\begin{equation}
\label{eq:def_M_11_M_21_boot}
\begin{split} 
&M_{1,1}^{\boot} =4C_Q\max(L_1,L_2)\frac{\max(\sqrt{K_2},\sqrt{K_1})\sqrt{c_0k_0^{1-\gamma}}(W_{\max}+1)}{\sqrt{2}(1-\gamma)}\eqsp,\\
& M_{2,1}^{\boot}=  3C_QL_H\frac{c_0k_0^{1-\gamma}\max(K_2,K_1)(W_{\max}+1)}{2(1-\gamma)} \eqsp,
\end{split}
\end{equation}
 and $K_1, K_2$ are defined in \eqref{eq:def_K_1}, \eqref{eq:def_K_2}, respectively.
Moreover, there is a set $\Omega_0 \in \F_{n-1} = \sigma(\xi_1,\ldots,\xi_{n-1})$, such that $\PP(\Omega_0) \geq 1-1/n$, and on $\Omega_0$ it holds that
\begin{equation}
\label{eq:D-boot-high-prob-bound}
\{\PEb[\norm{D^\boot}^{p}]\}^{1/p} \leq M_{1,1}^{\boot}\rme^{1/p}p^{3/2}n^{2/p-\gamma/2} +  M_{2,1}^{\boot}\rme^{2/p}p n^{1/2+3/p-\gamma}\eqsp.
\end{equation}
\end{proposition}
\begin{proof}
We first show \eqref{eq:D-boot-joint-bound}. We split 
\[
D^\boot = D^\boot_{1} + D^\boot_{2}\eqsp,
\]
where 
\begin{equation}
\begin{split}
D^\boot_{1} &= -\frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}(w_i-1) Q_{i}\bigl( G(\theta_{i-1}^{\boot}-\thetas) + g(\theta_{i-1}^{\boot}, \xi_i)\bigr) - \frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}Q_i\bigl(g(\theta_{i-1}^{\boot}, \xi_i) -g(\theta_{i-1}, \xi_i)\bigr)\eqsp, \\
D^\boot_{2} &= -\frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}(w_i-1) Q_{i} H(\theta_{i-1}^{\boot}) - \frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}Q_i\bigl(H(\theta_{i-1}^{\boot})-  H(\theta_{i-1})\bigr)\eqsp.
\end{split}
\end{equation}
Applying Minkowski's inequality together with \Cref{lem:bound_p_moment_martingale_components} and \Cref{lem:bound_p_moment_for_H_component} we get \eqref{eq:D-boot-joint-bound}.

To proof \eqref{eq:D-boot-high-prob-bound} we consider 
\begin{equation}
    \Omega_0 =\{\{\PEb[\norm{D^\boot}^{p}]\}^{1/p} \leq M_{1,1}^{\boot}\rme^{1/p}p^{3/2}n^{2/p-\gamma/2} +  M_{2,1}^{\boot}\rme^{2/p}p n^{1/2+3/p-\gamma}\}\eqsp.
\end{equation}
Note that by Markov's inequality 
\begin{align}
    &\PP(\Omega_0^c) \leq \frac{\PE[\{\PEb[\norm{D^\boot}^{p}]\}]}{n(M_{1,1}^{\boot}\rme^{2/p}p^{3/2}n^{2/p-\gamma/2} +  M_{2,1}^{\boot}\rme^{1/p}p n^{1/2+1/p-\gamma})^p}\\& = 
    \frac{\PE[\norm{D^\boot}^{p}]}{n(M_{1,1}^{\boot}\rme^{1/p}p^{3/2}n^{1/p-\gamma/2} +  M_{2,1}^{\boot}\rme^{2/p}p n^{1/2+2/p-\gamma})^p} \leq \frac{1}{n}\eqsp.
\end{align}

\end{proof}





\label{sec: Dboot bounds}
\begin{lemma}
\label{lem:bound_p_moment_martingale_components}
Assume \Cref{ass:L-smooth}, \Cref{ass:hessian_Lipschitz_ball}, \Cref{ass:L-co-coercivity-assum}, \Cref{ass:bound_noise}, \Cref{ass:bound_bootstap_weights}, \Cref{ass:step_size_new_boot}. Then for any $p\geq 2$ it holds
\begin{equation}
    \PE^{1/p}[\norm{D^\boot_{1}}^p]\leq M_{1,1}^{\boot}\rme^{1/p}p^{3/2}n^{1/p-\gamma/2} \eqsp, 
\end{equation}
where 
\begin{equation}
    M_{1,1}^{\boot} = 4C_Q\max(L_1,L_2)\frac{\max(\sqrt{K_2},\sqrt{K_1})\sqrt{c_0k_0^{1-\gamma}}(W_{\max}+1)}{\sqrt{2}(1-\gamma)}\eqsp,
\end{equation}
and $K_1, K_2$ are defined in \eqref{eq:def_K_1}, \eqref{eq:def_K_2}, respectively.
% \begin{enumerate}[(a)]
%  \item \label{eq: bound_p_moment_sum_Q_g}\begin{align}
%     &\frac{1}{\sqrt{n}}\PE^{1/p}\biggl[\norm{\sum_{i=1}^{n-1}Q_i\biggl(g(\theta_{i-1},\xi_i)-g(\thetas,\xi_i) \biggr)}^{p}\biggr] \leq M_{1,1}^{\boot}\rme^{2/p}p^{3/2}n^{2/p-\gamma/2}\eqsp,\\
%     &M_{1,1}^{\boot} = \frac{C_QL_2\sqrt{8\alpha_0}(C_{1,\xi}+2C_{2,\xi})k_0^{1/2-\gamma/2}}{\sqrt{\mu(1-\gamma)}}
%  \end{align}
%  \item \label{eq: bound_p_moment_sum_Q_g_boot}\begin{align}
%     &\frac{1}{\sqrt{n}}\PE^{1/p}\biggl[\norm{\sum_{i=1}^{n-1}Q_i\biggl(g(\theta_{i-1}^{\boot},\xi_i)-g(\thetas,\xi_i) \biggr)}^{p}\biggr] \leq M_{1,2}^{\boot}\rme^{2/p}p^{3/2}n^{2/p-\gamma/2}\eqsp, \\
%     &M_{1,2}^{\boot}=\frac{C_QL_2\sqrt{8\alpha_0}(C_{1,\xi}+2C_{2,\xi})W_{\max}k_0^{1/2-\gamma/2}}{\sqrt{\mu(1-\gamma)W_{\min}}}
% \end{align}
%  \item \label{eq: bound_p_moment_sum_Q_boot_g_boot} \begin{align}
%     &\frac{1}{\sqrt{n}}\PE^{1/p}\biggl[\norm{\sum_{i=1}^{n-1}(w_i-1)Q_i\biggl(g(\theta_{i-1}^{\boot},\xi_i)-g(\thetas,\xi_i) \biggr)}^{p}\biggr] \leq M_{1,3}^{\boot}\rme^{2/p}p^{3/2}n^{2/p-\gamma/2}\eqsp,\\
%     &M_{1,3}^{\boot}=\frac{C_QL_2\sqrt{8\alpha_0}(C_{1,\xi}+2C_{2,\xi})(W_{\max}+1)W_{\max}k_0^{1/2-\gamma/2}}{\sqrt{\mu(1-\gamma)W_{\min}}}
%  \end{align}
%   \item \label{eq: bound_p_moment_sum_Q_boot_theta_boot} \begin{align}
%     &\frac{1}{\sqrt{n}}\PE^{1/p}\biggl[\norm{\sum_{i=1}^{n-1}(w_i-1)Q_iG(\theta_{i-1}^{\boot}-\thetas)}^{p}\biggr] \leq M_{1,4}^{\boot}\rme^{2/p}p^{3/2}n^{2/p-\gamma/2}\eqsp,\\
%     &M_{1,4}^{\boot}=\frac{C_QL_1\sqrt{8\alpha_0}(C_{1,\xi}+2C_{2,\xi})(W_{\max}+1)W_{\max}k_0^{1/2-\gamma/2}}{\sqrt{\mu(1-\gamma)W_{\min}}}
%  \end{align}
% \end{enumerate}
\end{lemma}
\begin{proof}
    We split \( D^\boot_{1} \) into four parts, where each part is a sum of martingale differences.
    Note that $\{Q_i(g(\theta_{i-1},\xi_i)-g(\thetas,\xi_i))\}_{i=1}^{n}$ is a martingale difference with respect to $\F_{i-1}$.
    Then applying Burholder's inequality \cite[Theorem 8.6]{oskekowski2012sharp} together with Minkowski's inequality and \Cref{lem:bound_Q_i_and_Sigma_n}, we obtain that 
    \begin{align}   
    &\PE^{1/p}\bigl[\norm{\sum_{i=1}^{n-1}Q_i\bigl(g(\theta_{i-1},\xi_i)-g(\thetas,\xi_i) \bigr)}^{p}\bigr] \\ 
    &\qquad \qquad \leq p\bigl(\PE^{2/p}\bigl[\bigl( \sum_{i=1}^{n-1}\norm{Q_i\bigl(g(\theta_{i-1},\xi_i)-g(\thetas,\xi_i) \bigr)}^2\bigr)^{p/2}\bigr]\bigr)^{1/2}
    \\ 
    &\qquad \qquad \leq C_Qp\bigl(\PE^{2/p}\bigl[\bigl( \sum_{i=1}^{n-1}\norm{g(\theta_{i-1},\xi_i)-g(\thetas,\xi_i)}^2\bigr)^{p/2}\bigr]\bigr)^{1/2}\\ 
    &\qquad \qquad \leq C_Qp\bigl(\sum_{i=1}^{n-1}\PE^{2/p}[\norm{g(\theta_{i-1},\xi_i)-g(\thetas,\xi_i) }^{p}]\bigr)^{1/2}\eqsp.
    \end{align}
Finally, using \Cref{ass:noise_decomposition} and \Cref{lem: high_prob_last_iter}, we obtain 
\begin{align}
&\PE^{1/p}\bigl[\norm{\sum_{i=1}^{n-1}Q_i\bigl(g(\theta_{i-1},\xi_i)-g(\thetas,\xi_i) \bigr)}^{p}\bigr] \leq pC_QL_2\bigl(\sum_{i=1}^{n-1}\PE^{2/p}[\norm{\theta_{i-1} -\thetas}^{p}]\bigr)^{1/2}\\ 
& \qquad \qquad \leq  C_QL_2(\rme n)^{1/p}p^{3/2}\frac{\sqrt{K_2}}{\sqrt{2}}\bigl(\sum_{i=0}^{n-2}\alpha_i\bigr)^{1/2} \\
& \qquad \qquad \leq  C_QL_2(\rme n)^{1/p}p^{3/2}\frac{\sqrt{K_2}}{\sqrt{2}}\bigl(c_0\frac{(k_0+n-2)^{1-\gamma}-(k_0-1)^{1-\gamma}}{1-\gamma}\bigr)^{1/2}\eqsp.
\end{align}
Since $k_0 \geq 1$ and $(k_0+n-2)^{1-\gamma}-(k_0-1)^{1-\gamma}\leq k_0^{1-\gamma}n^{1-\gamma}$ we complete the proof for $$
\PE^{1/p}\bigl[\norm{\sum_{i=1}^{n-1}Q_i\bigl(g(\theta_{i-1},\xi_i)-g(\thetas,\xi_i) \bigr)}^{p}\bigr] \eqsp. 
$$
The proof for other three terms is analogous, since each of the terms 
\begin{align}
\{Q_i\bigl(g(\theta_{i-1}^{\boot},\xi_i)-g(\thetas,\xi_i) \bigr)\}_{i=1}^{n-1}\eqsp,\{(w_i-1)Q_i\bigl(g(\theta_{i-1}^{\boot},\xi_i)-g(\thetas,\xi_i) \bigr)\}_{i=1}^{n-1}\eqsp,
\{(w_i-1)Q_iG(\theta_{i-1}^{\boot}-\thetas)\}_{i=1}^{n-1} \eqsp,
\end{align}
are martingale differences with respect to $\widetilde{\F}_{i-1}$ (see definition in \eqref{eq: extended filtration}). 
We finish the proof applying Minkowski's inequality.
\end{proof}

\begin{lemma}
\label{lem:bound_p_moment_for_H_component}
Assume \Cref{ass:L-smooth}, \Cref{ass:hessian_Lipschitz_ball}, \Cref{ass:L-co-coercivity-assum},\Cref{ass:bound_noise}, \Cref{ass:bound_bootstap_weights},  \Cref{ass:step_size_new_boot}.  Then for any $p\geq 2$ it holds
\begin{align}
    &\PE^{1/p}[\norm{D_2^{\boot}}^p]\leq M_{2,1}^{\boot}\rme^{2/p}p n^{1/2+2/p-\gamma}\eqsp,\\
    & M_{2,1}^{\boot}= 3C_QL_H\frac{c_0k_0^{1-\gamma}\max(K_2,K_1)(W_{\max}+1)}{2(1-\gamma)}\eqsp,
\end{align}
and $K_1, K_2$ are defined in \eqref{eq:def_K_1}, \eqref{eq:def_K_2}, respectively.
% \begin{enumerate}[(a)]
% \item \label{eq: bound_p_moment_Q_i_H}\begin{align}
%          &\frac{1}{\sqrt{n}}\PE^{1/p}\biggl[\norm{\sum_{i=1}^{n-1}Q_iH(\theta_k)}^{p}\biggr] \leq M_{2,1}^{\boot}\rme^{1/p}p n^{1/2+1/p-\gamma}\eqsp,\\
%          & M_{2,1}^{\boot}= \frac{8C_QL_H(C_{1,\xi}+2C_{2,\xi})^{2}\alpha_0 k_0^{1-\gamma}}{\mu(1-\gamma)}
%     \end{align}
% \item \label{eq: bound_p_moment_Q_i_H_boot}\begin{align}
%          &\frac{1}{\sqrt{n}}\PE^{1/p}\biggl[\norm{\sum_{i=1}^{n-1}Q_iH(\theta_k^{\boot})}^{p}\biggr] \leq M_{2,2}^{\boot}\rme^{1/p}p n^{1/2+1/p-\gamma}\eqsp,\\
%          & M_{2,2}^{\boot}= \frac{8C_QL_H(C_{1,\xi}+2C_{2,\xi})^{2}W_{\max}^2\alpha_0 k_0^{1-\gamma}}{\mu(1-\gamma)W_{\min}}
%     \end{align}
% \item \label{eq: bound_p_moment_Q_i_boot_H_boot}\begin{align}
%          &\frac{1}{\sqrt{n}}\PE^{1/p}\biggl[\norm{\sum_{i=1}^{n-1}(w_i-1)Q_iH(\theta_k^{\boot})}^{p}\biggr] \leq M_{2,3}^{\boot}\rme^{1/p}p n^{1/2+1/p-\gamma}\eqsp,\\
%          & M_{2,3}^{\boot}= \frac{8C_QL_H(C_{1,\xi}+2C_{2,\xi})^{2}W_{\max}^2(W_{\max}+1)\alpha_0 k_0^{1-\gamma}}{\mu(1-\gamma)W_{\min}}
%     \end{align}
% \end{enumerate}
\end{lemma}
\begin{proof}
 Using Minkowski's inequality, we get 
 \begin{equation}
     \label{eq: D_2 boot bound}
 \begin{split}
          \PE^{1/p}[\norm{D_2^{\boot}}^p] &\leq \frac{1}{\sqrt{n}}\PE^{1/p}[\norm{\sum_{i=1}^{n-1}Q_i H(\theta_{i-1})}^p]\\& +\frac{1}{\sqrt{n}}\PE^{1/p}[\norm{\sum_{i=1}^{n-1}(w_i-1) Q_{i}\biggl(H(\theta_{i-1}^{\boot})\biggr)}^p]\\
     &+ \frac{1}{\sqrt{n}}\PE^{1/p}[\norm{\sum_{i=1}^{n-1}Q_iH(\theta_{i-1}^{\boot})}^p]\eqsp. 
     \end{split}
\end{equation}
We will now consider each term separately.
   Using Minkowski's inequality together with \Cref{lem:H_theta_bound}, we obtain 
   \begin{equation}
   \begin{split}
       &\frac{1}{\sqrt{n}}\PE^{1/p}\biggl[\norm{\sum_{i=1}^{n-1}Q_iH(\theta_{i-1})}^{p}\biggr] \leq \frac{C_QL_H}{\sqrt{n}}\sum_{i=0}^{n-2}\PE^{1/p}\biggl[\norm{\theta_{i}-\thetas}^{2p}\biggr] \\&\qquad \qquad\leq \frac{C_QL_Hp}{\sqrt{n}}(\rme n)^{2/p}(K_2/2)\sum_{i=0}^{n-1}\alpha_i \\&\qquad \qquad \leq \frac{C_QL_Hp}{\sqrt{n}}(\rme n)^{2/p}(K_2/2)\biggl(c_0\frac{(k_0+n-2)^{1-\gamma}-(k_0-1)^{1-\gamma}}{1-\gamma} \biggr)\eqsp.
       \end{split}
   \end{equation}
   Since $k_0 \geq 1$ and $(k_0+n-2)^{1-\gamma}-(k_0-1)^{1-\gamma}\leq k_0^{1-\gamma}n^{1-\gamma}$ we complete the proof for the first term in the r.h.s. of \eqref{eq: D_2 boot bound}. The proof for other two terms is analogous.
\end{proof}


\subsection{Matrix Bernstein inequality for $
\Sigma_n^\boot$ and Gaussian comparison}

\begin{lemma}
\label{lem:matrix_bernstein}
Under assumptions \Cref{ass:L-smooth}, \Cref{ass:bound_noise}, \Cref{ass:step_size_new_boot}, \Cref{ass:n_lower_bound},  there is a set $\Omega_1 \in \F_{n-1}$, such that $\PP(\Omega_1) \geq 1 - 1/n$ and on $\Omega_1$ it holds that
\begin{equation}
\norm{\Sigma_n^\boot - \Sigma_n} \leq \frac{10C_{Q,\xi}\sqrt{\log(2d n)}}{3\sqrt{n}}
% \frac{4C_{Q,\xi}\log(2d n)}{3n} + \frac{2C_{Q,\xi}\sqrt{\log(2d n)}}{\sqrt{n}} \eqsp, 
\end{equation}
where the constant $C_{Q,\xi}$ is given by
\begin{equation}
\label{eq:const_C_Q_xi_def}
C_{Q,\xi} := C_{Q}^2(C_{1,\xi}^2 + \lambda_{\max}(\Sigma_{\xi}))\eqsp,
\end{equation}
and $C_{1,\xi}$, $C_{Q}$ are defined in \Cref{ass:bound_noise} and Lemma \ref{lem:bound_Q_i_and_Sigma_n}, respectively.

\end{lemma}
\begin{proof}
Note that 
\[
\Sigma_n^\boot - \Sigma_n = \frac{1}{n} \sum_{i=1}^{n-1} Q_i(\eta(\xi_i)\eta(\xi_i)^\top-\Sigma_{\xi})Q_i^\top\eqsp.
\]
For simplicity we denote $A_i = Q_i(\eta(\xi_i)\eta(\xi_i)^\top-\Sigma_{\xi})Q_i^\top$. Note that for any $i \in \{1, \ldots n-1\}$ it holds that 
\begin{equation}
\PE[A_i] = 0 \eqsp, \quad \norm{A_i} \leq C_{Q,\xi}\eqsp, \quad \norm{\sum_{i=1}^{n-1}\PE[A_iA_i^\top]}\leq nC_{Q,\xi}^2\eqsp.
\end{equation}
Then, using matrix Bernstein inequality \cite[Chapter 6]{tropp2015introduction}, we obtain 
\begin{equation}
\PP\left(\frac{1}{n}\norm{\sum_{i=1}^{n-1}A_i}\geq t\right)\leq 2d\exp\biggl\{\frac{-t^2n^2/2}{nC_{Q,\xi}^2 + nC_{Q,\xi}t/3}\biggr\} \eqsp.
    \end{equation}
    Taking $t_\delta = \frac{4C_{Q,\xi}\log(2d/\delta)}{3n} + \frac{2C_{Q,\xi}\sqrt{\log(2d/\delta)}}{\sqrt{n}}$, we obtain that with probability at least  $1-\delta$, it holds 
    \begin{equation}
    \frac{1}{n}\norm{\sum_{i=1}^{n-1}A_i}\leq t_\delta\eqsp.
    \end{equation}
Setting $\delta = 1/n$ and applying \Cref{ass:n_lower_bound} completes the proof.
\end{proof}

\begin{corollary}
\label{lambda min boot}
Under assumptions \Cref{ass:L-smooth}, \Cref{ass:bound_noise}, \Cref{ass:step_size_new_boot}, \Cref{ass:n_lower_bound}, on $\Omega_1$ it holds that 
\begin{equation}
    \lambda_{\min}(\Sigma_n^\boot) \geq \frac{1}{2 C_\Sigma^2} \eqsp. 
\end{equation}
\end{corollary}
\begin{proof}
Using eigenvalue stability (Lidski's) inequality, we obtain 
\begin{equation}
    \lambda_{\min}(\Sigma_n^\boot) \geq \lambda_{\min}(\Sigma_n)-\norm{\Sigma_n-\Sigma_n^\boot}\eqsp.
\end{equation}
Note that on $\Omega_1$, we have 
\begin{equation}
    \norm{\Sigma_n-\Sigma_n^\boot} \leq  \frac{10C_{Q,\xi}\sqrt{\log(2d n)}}{3\sqrt{n}} \leq \frac{1}{2C_\Sigma^2}\eqsp,
\end{equation}
where in the last inequality we use \Cref{ass:n_lower_bound}.
\end{proof}

\begin{lemma}
\label{lem: gaussion comparison boot and real} 
Assume Under assumptions \Cref{ass:L-smooth}, \Cref{ass:bound_noise}, \Cref{ass:step_size_new_boot}, \Cref{ass:n_lower_bound}. Then on $\Omega_1$, it holds that
$$
\kolmogorov(\{\Sigma_n^\boot\}^{-1/2} \eta^\boot, \Sigma_n^{1/2}\eta) \le \frac{5C_{Q,\xi} C_\Sigma^2 \sqrt{d \log(2d n)}}{\sqrt{n}} \eqsp.
$$
\end{lemma}
\begin{proof}
By Lemma \ref{lem:bound_Q_i_and_Sigma_n}, $\|\Sigma_n^{-1/2}\| \le C_{\Sigma}$. Hence,  due to Lemma \ref{lem:matrix_bernstein}, we have 
\begin{equation}
\trace \{ (\Sigma_m^{-1/2} \Sigma_n^\boot \Sigma_n^{-1/2} - I_p)^2 \}\leq d \norm{(\Sigma_n^{-1/2} \Sigma_n^\boot \Sigma_n^{-1/2} - I_p)^2}^2 \leq dC_\Sigma^2\norm{\Sigma_n^\boot-\Sigma_n}^2 \leq \delta^2 \eqsp.
\end{equation}
 where we have  set 
$$
\delta = \frac{10C_{Q,\xi} C_\Sigma^2 \sqrt{d \log(2d n)}}{3\sqrt{n}}
$$
We finish the proof applying Lemma \ref{Pinsker}.
\end{proof}


\subsection{GAR in the bootstrap world}

\begin{theorem}
\label{GAR bootstrap}
Assume \Cref{ass:L-smooth}, \Cref{ass:hessian_Lipschitz_ball}, and \Cref{ass:L-co-coercivity-assum} - \Cref{ass:n_lower_bound}. Then with $\PP$ - probability at least $1 - 2/n$, it holds
\begin{equation}
\begin{split}
    &\sup_{B \in \Conv(\rset^d)}|\PPb( \sqrt n\{\Sigma_n^\boot\}^{-1/2}   (\bar{\theta}_{n}^\boot - \bar{\theta}_n) \in B) - \PPb(Y^\boot \in B)|  \\ &\qquad\qquad\le \frac{M_{3,1}^\boot}{n^{1/2}}  +  \frac{M_{3,2}^\boot  \log n}{n^{\gamma-1/2}}  + \frac{M_{3,3}^\boot \log^{3/2} n}{n^{\gamma/2}} \eqsp,
    \end{split}
\end{equation}
where
\begin{equation}
\label{M_3i boot def}
\begin{split}
    &M_{3,1}^\boot = 259  (\sqrt{2} C_\Sigma C_Q C_{1, \xi})^3W_{\max} \sqrt{d} \eqsp,\\ 
    &M_{3,2}^\boot =2^{3/2}c_dC_{\Sigma}M_{2,1}^{\boot}  e^{3/2+\gamma}\eqsp, \\
    &M_{3,2}^\boot =  2^{3/2}c_d C_{\Sigma}M_{1,1}^{\boot} e^{3/2+\gamma/2}\eqsp, 
\end{split}   
\end{equation}
and $M_{1,1}^{\boot}, M_{2,1}^{\boot}$ are defined in \eqref{eq:def_M_11_M_21_boot}.
\end{theorem}

\begin{proof}
Since the matrix $\Sigma_n^\boot$ concentrates around $\Sigma_n$ due to \Cref{lem:matrix_bernstein} , there is a set $\Omega_1$ such that $\PP(\Omega_1) \geq 1-1/n$ and  $\lambda_{\min}(\Sigma_n^\boot) > 0$ on $\Omega_1$. Moreover, on this set 
    Applying Lemma \ref{nonlinearapprox} with 
    $$
    X = \{\Sigma_n^\boot\}^{-1/2} W^\boot, \quad Y = \{\Sigma_n^\boot\}^{-1/2} D^\boot,
    $$
    we get
    \begin{equation}
\begin{split}
    &\sup_{B \in \Conv(\rset^d)}|\PPb( \sqrt n  \{\Sigma_n^\boot\}^{-1/2} (\bar{\theta}_{n}^\boot - \bar{\theta}_n) \in B) - \PPb(Y^\boot \in B)| \\
    & \qquad\qquad\le \sup_{B \in \Conv(\rset^d)}|\PPb(\{\Sigma_n^\boot\}^{-1/2} W^\boot \in B) - \PPb(Y^\boot \in B)|  + 2 c_d (\PEb[\|\{\Sigma_n^\boot\}^{-1/2} D^\boot\|^p])^{1/(1+p)} \eqsp. 
    \end{split}
\end{equation}
By \citep{shao2022berry} (with $D = 0$) we may estimate 
\begin{equation}
    \label{CLT boot lin stat proof}
    \begin{split}
    &\sup_{B \in \Conv(\rset^d)}|\PPb(\{\Sigma_n^\boot\}^{-1/2} W^\boot \in B) - \PPb(Y^\boot \in B)| \\
    &\qquad\qquad\qquad\qquad\le \frac{259 d^{1/2}}{n^{3/2}} \sum_{i = 1}^n \PEb[|w_i-1|^3] \| (\{\Sigma_n^\boot\}^{-1/2} Q_i 
    \eta(\xi_i)\|^3 \eqsp.
    \end{split}
\end{equation}
Applying Lemma \ref{lem:bound_Q_i_and_Sigma_n} and Corollary \ref{lambda min boot} we get 
\begin{equation}
    \label{CLT boot lin stat proof 2}
    \sup_{B \in \Conv(\rset^d)}|\PPb(\{\Sigma_n^\boot\}^{-1/2} W^\boot \in B) - \PPb(Y^\boot \in B)| \le \frac{259 d^{1/2} (\sqrt{2} C_\Sigma  C_Q C_{1, \xi})^3 W_{\max}}{n^{1/2}} \eqsp.
\end{equation}
From Proposition \ref{prop:prob-D-boot-bound} and Corollary \ref{lambda min boot} it follows that on the set we $\Omega_0 \cap \Omega_1$ the following bound is satisfied 
\begin{equation}
(\PEb[\|\{\Sigma_n^\boot\}^{-1/2} D^\boot\|^p])^{1/(p+1)} \leq \sqrt{2}C_{\Sigma}(M_{1,1}^{\boot}\rme^{1/p}p^{3/2}n^{2/p-\gamma/2} +  M_{2,1}^{\boot}\rme^{2/p}p n^{1/2+3/p-\gamma})^{p/(p+1)} \eqsp.
\end{equation}
Since $p\geq 2, M_{1,1}^{\boot}, M_{2,1}^{\boot} \geq 1$, we obtain 
\begin{equation}
    (\PEb[\|\{\Sigma_n^\boot\}^{-1/2} D^\boot\|^p])^{1/(p+1)} \leq 
    \sqrt{2} C_{\Sigma}(\rme^{1/2} M_{1,1}^{\boot}p^{3/2} n^{\frac{2}{p+1}}n^{-\gamma/2}n^{\frac{\gamma/2}{(p+1)}} +  \rme M_{2,1}^{\boot} p n^{\frac{3}{p+1}}n^{1/2-\gamma}n^{-\frac{1/2-\gamma}{p+1}}) \eqsp.
\end{equation}
Setting $p = \log n - 1$, we get 

\begin{equation}
    (\PEb[\|\{\Sigma_n^\boot\}^{-1/2} D^\boot\|^p])^{1/(p+1)} \leq 
    \sqrt{2}C_{\Sigma}(M_{1,1}^{\boot}(\log n)^{3/2} e^{3/2+\gamma/2}n^{-\gamma/2}+  M_{2,1}^{\boot} (\log n) e^{3/2+\gamma}n^{1/2-\gamma})\eqsp.
\end{equation}
By combining the above inequalities, we complete the proof.
\end{proof}
\begin{remark}
We use \citep{shao2022berry} with $D = 0$ to prove \eqref{CLT boot lin stat proof} since we are not aware of Berry-Esseen results for non i.i.d. random vectors in dimension $d$ with precise constants and dependence on $d$. The result \cite{BENTKUS2003385} may be applied for i.i.d. vectors only. 
\end{remark}

\subsection{Proof of Theorem \ref{th:bootstrap_validity}}
Collecting bounds of Theorem \ref{th:bound_kolmogorov_dist_pr_sigma_n}, Theorem \ref{GAR bootstrap} and Lemma we get that with $\PP$ - probability at least $1 - 2/n$, it holds:
\begin{multline}
  \sup_{B \in \Conv(\rset^{d})} |\PPb(\sqrt n (\bar{\theta}_{n}^\boot - \bar{\theta}_n) \in B ) - \PP(\sqrt n (\bar{\theta}_n - \thetas) \in B)| \le   \frac{\ConstC_4 \sqrt{\log n}}{n^{1/2}} + \frac{\ConstC_5 \log n}{n^{\gamma - 1/2} } +  \frac{\ConstC_6 \log^{3/2} n}{n^{\gamma/2}} \eqsp, 
\end{multline}
where 
\begin{align}
\label{eq: bootsrap constants def}
    \ConstC_{4} = \ConstC_1 + M_{3,1}^\boot + 5C_{Q,\xi} C_\Sigma^2 \sqrt{d \log(2d)}, \quad \ConstC_{5} = \ConstC_2 + M_{3,2}^\boot, \quad \ConstC_{6} =  \ConstC_{3} + M_{3,3}^\boot. 
\end{align}




 


