\subsection{Proof of Lemma \ref{lem:bound_kolmogorov_dist_sigma_n_sigma_infty}}
\label{sec: proof of difference between cov}
By definition of $\Sigma_n$ and $\Sigma_\infty$ we may write
\begin{align}
    \label{repr:Lstar_minus_Ln}
    \Sigma_n - \Sigma_{\infty} = \underbrace{\frac{1}{n}\sum_{t=1}^{n-1} (Q_t - G^{-1})\noisecov G^{-\top} + \frac{1}{n} \sum_{t=1}^{n-1} G^{-1} \noisecov (Q_t - G^{-1})^\top}_{D_1} + \\ + \underbrace{\frac{1}{n} \sum_{t=1}^{n-1} (Q_t - G^{-1}) \noisecov (Q_t - G^{-1})^{\top}}_{D_2} - \frac{1}{n} \Sigma_{\infty} \eqsp. 
\end{align}
The following lemma is an analogue of \citep[pp. 26-30]{wu2024statistical}. 

\begin{lemma} The following identities hold
\begin{equation}
    \label{repr:qtminusa}
    Q_i - \bA^{-1} = S_i - G^{-1} G_{i:n-1}^{(\alpha)} ,~ S_i = \sum_{j=i+1}^{n-1} (\alpha_i - \alpha_j) G_{i+1:j-1}^{(\alpha)} \eqsp,  
\end{equation}
and
\begin{equation}
    \label{repr:sumqtminusa}
    \sum_{i=1}^{n-1} (Q_i - G^{-1}) = -G^{-1} \sum_{j=1}^{n-1} G_{1:j}^{(\alpha)} \eqsp,
\end{equation}
where 
\begin{equation}
     G_{i:j}^{(\alpha)} = \prod_{k=i}^{j}(I-\alpha_kG)
\end{equation}
\end{lemma}

 \begin{proof}
To prove \eqref{repr:sumqtminusa} we first change the order of summation and then use the  properties of the telescopic sums we get
\begin{align}
\sum_{i=1}^{n-1} Q_i &= \sum_{i=1}^{n-1} \alpha_i \sum_{j=i}^{n-1}\prod_{k=i+1}^{j}(I-\alpha_k G) = \sum_{j=1}^{n-1} \sum_{i = 1}^j \alpha_i \prod_{k=i+1}^{j}(I-\alpha_k G) \\
& = \sum_{j=1}^{n-1} \sum_{i = 1}^j G^{-1} (\prod_{k = i+1}^{j} -  \prod_{k = i}^{j}) (\Id - \alpha_k G)  = G^{-1} \sum_{j=1}^{n-1} (\Id - \prod_{k = 1}^j (\Id - \alpha_k G)) \eqsp.
\end{align}
The proof of \eqref{repr:qtminusa} could be obtained by the following arguments. Note that
\begin{align}
\alpha_i G Q_i & = Q_i - (\Id - \alpha_i G) Q_i = \\
& = \alpha_i \Id + \alpha_i \sum_{j=i+1}^{n-1}\prod_{k=i+1}^{j}(I-\alpha_k G) - \alpha_i \sum_{j=i+1}^{n-1} \prod_{k=i}^{j-1}(\Id - \alpha_k G) - \alpha_i \prod_{k=i}^{n-1} (\Id - \alpha_k G)\eqsp. 
\end{align}  
It remains to note that
$$
\prod_{k=i+1}^{j}(I-\alpha_k G) - \prod_{k=i}^{j-1}(\Id - \alpha_k G) = (\alpha_i - \alpha_j) G \prod_{k=i+1}^{j-1} (\Id - \alpha_k G) \eqsp.
$$
The last two equations imply \eqref{repr:qtminusa}.
\end{proof}

\begin{lemma}
\label{bound_G_S}
It holds that 
\begin{enumerate}[(a)]
\item 
\begin{equation}
    \norm{S_i}\leq C_{S}(i + k_0)^{\gamma-1}\eqsp,
\end{equation}
where 
$$
C_{S} = 2\alpha_0\exp\biggl\{\frac{\mu c_0}{k_0^{\gamma}}\biggr\}\biggl(2^{\gamma/(1-\gamma)}\frac{1}{\mu c_0} + (\frac{1}{\mu c_0})^{1/(1-\gamma)}\Gamma(\frac{1}{1-\gamma})\biggr)\eqsp.
$$
\item 
\begin{equation}
    \sum_{i=1}^{n-1}\norm{G_{i:n-1}^{(\alpha)}}^2\leq \frac{1}{1 - (1 - c_0\mu(n+k_0)^{-\gamma})^2}
\end{equation}
\item 
\begin{equation}
    \norm{\sum_{i=1}^{n-1}G_{i:n-1}^{(\alpha)}}\leq \frac{k_0^\gamma n^\gamma}{c_0\mu}
\end{equation}
\end{enumerate}
\end{lemma}
\begin{proof}
For simplicity we define $m_i^j = \sum_{k=i}^j (k+k_0)^{-\gamma}$.
Note that 
\begin{equation}
    \norm{\sum_{j=i+1}^{n-1} (\alpha_i - \alpha_j) G_{i+1:j-1}^{(\alpha)}} \leq \sum_{j=i}^{n-2} \frac{c_0}{(j+k_0+1)^\gamma}\biggl(\biggl(\frac{j+k_0+1}{i+k_0}\biggr)^\gamma - 1\biggr) \exp\{-\mu c_0m_{i+1}^{j}\}
\end{equation}
    Following the proof of \cite[Lemma A.5]{wu2024statistical}, we have 
    \begin{equation}
        \biggl(\frac{j+k_0+1}{i+k_0}\biggr)^\gamma - 1 \leq (i+k_0)^{\gamma-1}\biggl(1+ (1-\gamma)m_{i}^{j}\biggr)^{\gamma/(1-\gamma)}
    \end{equation}
    Hence, we obtain 
    \begin{align}
        \norm{S_i} &\leq c_0(i+k_0)^{\gamma-1}\sum_{j=i}^{n-2} \frac{1}{(j+k_0+1)^\gamma}\biggl(1+ (1-\gamma)m_{i}^{j}\biggr)^{\gamma/(1-\gamma)} \exp\{-\mu c_0m_{i+1}^{j}\}\\ &\leq
        c_0(i+k_0)^{\gamma-1}\sum_{j=i}^{n-2} \frac{1}{(j+k_0)^\gamma}\biggl(1+ (1-\gamma)m_{i}^{j}\biggr)^{\gamma/(1-\gamma)}\exp\{\mu c_0(k_0+i)^{-\gamma}\} \exp\{-\mu c_0m_{i}^{j}\}\\&\leq 
         c_0\exp\{\frac{\mu c_0}{k_0^{\gamma}}\}(i+k_0)^{\gamma-1}\sum_{j=i}^{n-2} (m_i^j-m_i^{j-1})\biggl(1+ (1-\gamma)m_{i}^{j}\biggr)^{\gamma/(1-\gamma)} \exp\{-\mu c_0m_{i}^{j}\}\\ &\leq 
         2c_0\exp\{\frac{\mu c_0}{k_0^{\gamma}}\}(i+k_0)^{\gamma-1}\int_{0}^{+\infty}\biggl(1+ (1-\gamma)m\biggr)^{\gamma/(1-\gamma)} \exp\{-\mu c_0m\}\rmd m \\ &\leq 
         2c_0\exp\{\frac{\mu c_0}{k_0^{\gamma}}\}(i+k_0)^{\gamma-1}\biggl(2^{\gamma/(1-\gamma)}\frac{1}{\mu c_0} + (\frac{1}{\mu c_0})^{1/(1-\gamma)}\Gamma(\frac{1}{1-\gamma})\biggr)\eqsp.
    \end{align}
    Note that 
\begin{align}
    \norm{\sum_{i=1}^{n-1}G_{i:n-1}^{(\alpha)}}&\leq \sum_{i=1}^{n-1}\prod_{k=i}^{n-1}(1-\alpha_k\mu) = \sum_{i=1}^{n-1}\prod_{k=i}^{n-1}\alpha_{i-1}^{-1}\alpha_{i-1}(1-\alpha_k\mu) \\&\leq\frac{(k_0+n-2)^\gamma}{c_0\mu}\sum_{i=1}^{n-1}\biggl(\prod_{k=i}^{n-1}(1-\alpha_k\mu)-\prod_{k=i-1}^{n-1}(1-\alpha_k\mu)\biggr)\leq \frac{k_0^{\gamma}n^{\gamma}}{\mu c_0}
\end{align}
    Bound for $\sum_{i=1}^{n-1}\norm{G_{i:n-1}^{(\alpha)}}^2$ is obtained similarly to $ \norm{\sum_{i=1}^{n-1}G_{i:n-1}^{(\alpha)}}$.
\end{proof}

To finish the proof of Lemma \ref{lem:bound_kolmogorov_dist_sigma_n_sigma_infty} we need to bound $D_1, D_2$. By \eqref{repr:sumqtminusa} we obtain
\begin{align}
\normop{\frac{1}{n}\sum_{i=1}^{n-1} (Q_i - G^{-1})\noisecov G^{-\top}} &= \normop{-\frac{1}{n} G^{-1} \sum_{j=1}^{n-1} G_{1:j}^{(\alpha)}\noisecov G^{-\top}} \\ &= \normop{n^{-1} \Sigma_{\infty}  \sum_{j=1}^{n-1} G_{1:j}^{(\alpha)}} \leq n^{-1} \normop{\Sigma_{\infty}} \cdot \normop{\sum_{j=1}^{n-1} G_{1:j}^{(\alpha)}} \eqsp. 
    \end{align}
    It remains to apply \Cref{bound_G_S} which gives
    \begin{align}
        \normop{\frac{1}{n}\sum_{i=1}^{n-1} (Q_i - G^{-1})\noisecov G^{-\top}}  \leq \normopadapt{\Sigma_{\infty}} \frac{k_0^\gamma n^{\gamma-1}}{c_0\mu}
    \end{align}
    Hence, 
    \begin{align}
        \label{lemma:D1_bound}
        \normop{D_1} \leq 2 \normopadapt{\Sigma_{\infty}} \frac{k_0^\gamma n^{\gamma-1}}{c_0\mu}
    \end{align}
To bound $D_2$ we use \eqref{repr:qtminusa} which gives
    \begin{align}
        \label{lemma:D2_expanded_repr}
        &n^{-1}\sum_{i=1}^{n-1}(Q_i - G^{-1}) \noisecov (Q_i - G^{-1})^\top \\
        &\qquad = n^{-1}\sum_{i=1}^{n-1}\bigl(S_i - G^{-1} \prod_{k=i}^{n-1} (\Id - \alpha_k G)\bigr) \noisecov \bigl(S_i - G^{-1} \prod_{k=i}^{n-1} (\Id - \alpha_k G)\bigr)^\top \\
        &\qquad = \underbrace{n^{-1} \sum_{i=1}^{n-1} S_i \noisecov S_i^\top}_{D_{21}} + \underbrace{n^{-1}\sum_{i=1}^{n-1}G^{-1} \prod_{k=i}^{n-1} (\Id - \alpha_k G) \noisecov G^{-\top} \prod_{k=i}^{n-1} (\Id - \alpha_k G)^\top}_{D_{22}} \\ 
        &\qquad\qquad \qquad  -\underbrace{n^{-1} \sum_{i=1}^{n-1} G^{-1} \prod_{k=i}^{n-1} (\Id - \alpha_k G) \cdot \noisecov S_i^\top}_{D_{23}} - \underbrace{n^{-1}\sum_{i=1}^{n-1} S_i \noisecov G^{-\top} \prod_{k=i}^{n-1} (\Id - \alpha_k G)^{\top}}_{D_{24}}\eqsp.
    \end{align}
    To bound $D_{21}$ we use  \Cref{bound_G_S}, and obtain 
    \begin{align}
        \label{lemma:D21_bound}
        \normop{D_{21}} = \normop{n^{-1} \sum_{i=1}^{n-1} S_i \noisecov S_i^\top} &\leq n^{-1} \sum_{i=1}^{n-1} \normop{\noisecov} \normop{S_i}^2 \\  &\leq
        n^{-1}\normop{\noisecov} C_{S}^2\sum_{i=1}^{n-1} (i + k_0)^{2(\gamma-1)}\\ &\leq n^{-1}\normop{\noisecov} C_{S}^2 \frac{(n + k_0-1)^{2\gamma-1}-k_0^{2\gamma-1}}{2\gamma-1}
        \\ &\leq \normop{\noisecov} C_{S}^2k_0^{2\gamma-1} \frac{n^{2(\gamma-1)}}{2\gamma-1}
    \end{align}
    The bound for $D_{22}$ follows from  \Cref{bound_G_S}
    \begin{align} 
        \label{lemma:D22_bound}
        \normop{D_{22}} &= \normop{n^{-1}\sum_{i=1}^{n-1} \prod_{k=i}^{n-1} (\Id - \alpha_k G) G^{-1} \noisecov G^{-\top} \prod_{k=i}^{n-1} (\Id - \alpha_k G)^\top} \leq n^{-1} \normop{\Sigma_\infty} \sum_{i=1}^{n-1} \normop{G_{i:n-1}^{(\alpha)}}^2 \\ 
        &\leq 
        n^{-1} \frac{\normop{\Sigma_\infty}}{ 2c_0\mu(n+k_0)^{-\gamma} - c_0^2\mu^2(n+k_0)^{-2\gamma}} \leq 
         \normop{\Sigma_\infty} k_0^{\gamma}\frac{n^{\gamma-1}}{c_0\mu}\eqsp.
    \end{align}
    Since $D_{23} = D_{24}^\top$, we concentrate on $\normop{D_{24}}$. \Cref{bound_G_S} immediately imply 
    \begin{align}
        \normop{D_{24}} & \leq n^{-1} \normop{\noisecov G^{-\top}} \sum_{i=1}^{n-1} \normop{S_i} \normop{\prod_{k=i}^{n-1} (\Id - \alpha_k G)^{\top}} \\ &\leq n^{-1} \normop{\noisecov}\frac{1}{\mu}C_{S}\sum_{i=1}^{n-1}(i+k_0)^{\gamma-1}\prod_{k=i}^{n-1}(1-\mu\frac{c_0}{(k+k_0)^\gamma}) \\ &\leq n^{-1} \normop{\noisecov}\frac{1}{\mu}C_{S}\sum_{i=1}^{n-1}(i+k_0)^{2\gamma-1}(i+k_0)^{-\gamma}\prod_{k=i+1}^{n-1}(1-\mu\frac{c_0}{(k+k_0)^\gamma})\\ &\leq \normop{\noisecov}C_{S}k_0^{2\gamma-1}\frac{n^{2(\gamma-1)}}{\mu^2c_0}
    \end{align}
    Combining all inequalities above, we obtain
    \begin{equation}
        \norm{\Sigma_n-\Sigma_{\infty}} \leq C_{\infty}'n^{\gamma-1}\eqsp,
    \end{equation}
    where
    \begin{equation}
    \label{eq:def_C_infty_prime}
        C_{\infty}' = (3\frac{k_0^{\gamma}}{c_0\mu} +1)\normopadapt{\Sigma_{\infty}} +  (C_{S}^2 \frac{1}{2\gamma-1}+
        C_{S}\frac{n^{2(\gamma-1)}}{\mu^2c_0})k_0^{2\gamma-1}\normop{\noisecov}\eqsp.
    \end{equation}
To finish the proof it remains to apply Lemma \ref{Pinsker}, since
\begin{equation}
\label{eq:def_C_infty}
3/2\|\Sigma_n^{-1/2}\Sigma_{\infty}\Sigma_n^{-1/2}-I\|_{\mathsf{F}}\leq C_{\infty}n^{\gamma-1}\eqsp, \text{ where } C_{\infty}=3/2 \sqrt{d} C_\Sigma^2 C_{\infty}'\eqsp.
\end{equation}

\subsection{Gaussian comparison lemma}
There are quite a lot of works devoted to the comparison of Gaussian measures with different covariance matrices and means. Among others we note the works \cite{BarUly86}, \cite{Bernolli2019}, \cite{Devroye2018}. In this work we will use the result from \cite{Devroye2018}[Theorem 1.1]. Note that, this inequality can be significantly improved if instead of the set of all convex sets we take the set of rectangles and the set of all balls.

\begin{lemma}
\label{Pinsker}
Let $\Sigma_1$ and $\Sigma_2$ be positive definite covariance matrices in $\rset^{p \times p}$. Let $X \sim \mathcal{N}(0, \Sigma_1)$ and $Y \sim \mathcal{N}(0, \Sigma_2)$. Then
\begin{equation}
\mathsf{d}_{\mathsf{TV}}(X, Y) \le \frac{3}{2} \|\Sigma_2^{-1/2} \Sigma_1 \Sigma_2^{-1/2} - I_p\|_{\mathsf{F}} \eqsp.
\end{equation}
\end{lemma}

%Here we present an inequality in which the closeness of Gaussian measures in the total variation distance is measured using the Frobenius norm of the covariance matrix relation. We also give an elementary proof based on Pinsker's inequality. 




