Among contributions to the analysis of the LSA algorithm, we should mention the papers \cite{polyak1992acceleration, kushner2003stochastic, borkar:sa:2008, benveniste2012adaptive}. These works investigate the asymptotic properties of the LSA estimates (such as asymptotic normality and almost sure convergence) under i.i.d. and Markov noise. Non-asymptotic results for the LSA and PR-averaged LSA estimates were obtained in \cite{rakhlin2012making, nemirovski2009robust, bhandari2018finite, lakshminarayanan2018linear, mou2021optimal}, where MSE bounds were established, and in \cite{mou2020linear, durmus2021tight, durmus2022finite}, which provided high-probability error bounds. The latter results enable the construction of Bernstein-type confidence intervals for the error $\prtheta_{n}-\thetas$. Unfortunately, the corresponding bounds typically depend on unknown problem properties of \eqref{eq:lsa}, related to the design matrix $\bA$ and the noise variables $\funcA{Z_k}$, $\funcb{Z_k}$. For this reason, applying these error bounds in practice is complicated. Furthermore, concentration bounds for the LSA error \cite{mou2020linear, durmus2021tight, durmus2022finite} do not imply convergence rates of the rescaled error $\sqrt{n}(\prtheta_{n} - \thetas)$ to the normal distribution in Wasserstein or Kolmogorov distance. Non-asymptotic convergence rates were previously studied in \cite{pmlr-v99-anastasiou19a} using the Stein method, but the resulting rate corresponds to a smoothed Wasserstein distance. Recent work \cite{srikant2024rates} investigates convergence rates to the normal distribution in Wasserstein distance for LSA with Markovian observations. Both papers yield bounds that are less tight with respect to their dependence on trajectory length $n$ than those presented in the present work, see a detailed comparison after \Cref{th:shao2022_berry}. 
\par 
A popular method for constructing confidence intervals in the context of parametric estimation is based on the bootstrap approach (\cite{efron1992bootstrap}). Its analysis has attracted many contributions, in particular a series of papers \cite{Chernozhukov2013} and \cite{Chernozhukov2015} that validate a bootstrap procedure for a test based on the maximum of a large number of statistics. Their study shows a close relationship between bootstrap validity results, Gaussian comparison and anticoncentration bounds for rectangular sets. The papers \cite{spokoiny2015} and \cite{Bernolli2019} investigate the applicability of likelihood-based statistics for finite samples and large parameter dimensions under possible model misspecification. The important step in proving bootstrap validity is again based on Gaussian comparison and anticoncentration bounds, but now for spherical sets. The bootstrap procedure for spectral projectors of covariance matrices is discussed in \cite{PTRF2019} and \cite{jirak2022quantitative}. The authors follow the same steps to prove the validity of the bootstrap. 
\par 
Extending the classical bootstrap approach to online learning algorithms is a challenge. For example, the iterates $\{\theta_k\}_{k \in \nset}$ determined by \eqref{eq:lsa} are not necessarily stored in memory, which makes the classical bootstrap inapplicable. This problem can be solved by performing randomly perturbed updates of the online procedure, as proposed in \cite{JMLR:v19:17-370} for the iterates of the Stochastic Gradient Descent (SGD) algorithm. The authors in \cite{JASA2023} used the same procedure for the case of Markov noise and policy evaluation algorithms in reinforcement learning, but in both papers the authors only consider the asymptotic validity. In our paper we use the same multiplier bootstrap approach (see \Cref{sec:bootstrap}), but we provide an explicit error bound for the bootstrap approximation of the distribution of the statistics $\sqrt{n}(\prtheta_{n} - \thetas)$. 
\par 
In addition to the bootstrap approach, one can also use the pivotal statistics \cite{LEE2024105673,li2023online,li2023statistical} or various estimates of the asymptotic covariance matrix \cite{zhu2023online_cov_matr} to construct the confidence intervals for $\thetas$. The latter approach can be based on the plug-in estimators \cite{pmlr-v178-li22b}, batch mean estimators \cite{chen2020aos} or in combination with the multiplier bootstrap approach \cite{zhong2023online}. However, the theoretical guarantees for mentioned methods remain purely asymptotic.
