\paragraph{Outline of the multiplier bootstrap procedure.}
This section establishes the nonasymptotic validity of the bootstrap method proposed in \cite{JMLR:v19:17-370}. We restate the procedure for the sake of clarity. Let 
\(\mathcal{W}^{n-1} = \{w_{\ell}\}_{1 \leq \ell \leq n-1}\)
be  i.i.d.\ random variables with distribution $\PP_w$, each with mean \(\PE[W_1] = 1\) and variance \(\var[W_1] = 1\). Assume \(\mathcal{W}^{n-1}\) is independent of
\(\Xi^{n-1} = \{\xi_{\ell}\}_{1 \leq \ell \leq n-1}\). Denote by \(\PPb = \PP(\cdot \mid \Xi^{n-1})\) and \(\PEb = \PE(\cdot \mid \Xi^{n-1})\) the corresponding conditional probability and expectation operators. We do not discuss the construction of the underlying probability space and refer the reader to \cite{bucher2019note}.
\par 
In parallel with the updates \eqref{eq:stoch_minimization}, that generate 
\(\{\theta_{k}\}_{0 \leq k \leq n-1}\) 
and \(\prtheta_{n}\),
we draw \(M\) independent samples 
\((w_{1}^{\ell}, \ldots, w_{n-1}^{\ell})\), for 
\(1 \leq \ell \leq M\),
each distributed as \(\mathcal{W}^{n-1}\). We then use these samples to construct \(M\) randomly perturbed SGD trajectories, following the same recursive structure as the primary sequence 
\(\{\theta_k\}_{0 \leq k \leq n-1}\), that is: 
\begin{equation}
\label{eq:sgd_bootstrap}
\begin{split}
 \theta_{k}^{\boot,\ell} 
&= \theta_{k-1}^{\boot,\ell} - \alpha_{k} w_k^{\ell}\{\nabla f(\theta_{k-1}^{\boot,\ell})+g(\theta_{k-1}^{\boot,\ell}, \xi_k) + \eta(\xi_k)\} \eqsp,~~ k \geq 1 \eqsp, ~~ \theta_{0}^{\boot,\ell} = \theta_{0} \eqsp, \\
\prtheta_{n}^{\boot,\ell} 
&=  \frac{1}{n} \sum_{k=0}^{n-1} \theta_k^{\boot,\ell} \eqsp, ~~n \geq 1 \eqsp.
\end{split}
\end{equation}
We use a short notation $\bar{\theta}_{n}^\boot$ for $\bar{\theta}_{n}^{\boot,1}$. Note that, when generating different weights $w_{k}^{\ell}$, we can draw samples from the conditional distribution of $\bar{\theta}_{n}^\boot$ given the data $\Xi^{n-1}$. The core principle behind the bootstrap procedure \eqref{eq:sgd_bootstrap} is that the "bootstrap world" probabilities $\PPb\big(\sqrt{n} (\bar{\theta}_{n}^\boot - \bar{\theta}_n) \in B \big)$ are close to $\PP\big(\sqrt{n} (\bar{\theta}_n - \thetas) \in B\big)$ for $B \in \Conv(\rset^{d})$.

\begin{remark}
While an analytical expression for \(\PPb(\sqrt{n} (\bar{\theta}_{n}^\boot - \bar{\theta}_n) \in B)\) is unavailable, it can be approximated via Monte Carlo simulations by generating \(M\) perturbed trajectories according to \eqref{eq:sgd_bootstrap}. Standard arguments (see, e.g., \cite[Section~5.1]{shao2003mathematical}) suggest that the accuracy of this Monte Carlo approximation scales as \(\mathcal{O}(M^{-1/2})\).
\end{remark}
\noindent More formally, we say that the procedure \eqref{eq:sgd_bootstrap} is asymptotically valid if 
\begin{equation}
\label{eq:boot_validity_supremum}
\sup_{B \in \Conv(\rset^{d})} \Big| \PPb\big(\sqrt{n} (\bar{\theta}_{n}^\boot - \bar{\theta}_n) \in B \big) - \PP\big(\sqrt{n} (\bar{\theta}_n - \thetas) \in B\big) \Big|
\end{equation}
converges to $0$ in $\PP$-probability as $n \to \infty$. This result was studied in \cite{JMLR:v19:17-370} under assumptions close to the original paper \cite{polyak1992acceleration}.

\paragraph{Outline of the quantitative bounds for \eqref{eq:boot_validity_supremum}.} In order to proceed with the non-asymptotic analysis, we perform the following steps. We note that \eqref{eq: sgd_reccurence_with_remainder_main} and \eqref{eq:sgd_bootstrap} imply the following recurrence:
\begin{equation}
\label{eq:bootstrap_reccurence_with_remainder}
    \begin{split}
    \theta_{k}^{\boot}-\theta_{k}  &= (I-\alpha_{k}G)(\theta_{k-1}^{\boot}-\theta_{k-1}) \\
     &- \alpha_k\bigl(H(\theta_{k-1}^{\boot}) +g(\theta_{k-1}^{\boot}, \xi_k) -  H(\theta_{k-1}) -g(\theta_{k-1}, \xi_k)\bigr)\\&- \alpha_k(w_k-1)\bigl( G(\theta_{k-1}^{\boot}-\thetas)+\eta(\xi_k) + g(\theta_{k-1}^{\boot}, \xi_k) + H(\theta_{k-1}^{\boot})\bigr)\eqsp.   
    \end{split}
\end{equation}
Taking an average of \eqref{eq:bootstrap_reccurence_with_remainder} and rearranging the terms, we obtain a counterpart of \eqref{eq:linear and nonlinear terms} - \eqref{eq: linear part}:

\begin{align}
\label{eq:D_boot-def}
    \sqrt{n}(\prtheta_n^{\boot}-\prtheta_{n}) &= W^\boot + D^\boot \eqsp,  \text{ where } W^\boot = -\frac{1}{\sqrt{n}} \sum_{i=1}^{n-1}(w_i-1) Q_{i} \eta(\xi_i) \eqsp, \\
    D^\boot &= -\frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}(w_i-1) Q_{i}\bigl( G(\theta_{i-1}^{\boot}-\thetas) + g(\theta_{i-1}^{\boot}, \xi_i) + H(\theta_{i-1}^{\boot})\bigr) \\&- \frac{1}{\sqrt{n}}\sum_{i=1}^{n-1}Q_i\bigl(H(\theta_{i-1}^{\boot}) +g(\theta_{i-1}^{\boot}, \xi_i) -  H(\theta_{i-1}) -g(\theta_{i-1}, \xi_i)\bigr)\eqsp. 
\end{align}
Here $W^\boot$ is a weighted sum of i.i.d. random variables $\Xi^{n-1}$, such that  $\PEb[W^\boot] = 0$ and 
\begin{equation}
\label{Sigman boot def}
\textstyle 
\PEb[W^\boot \{W^\boot\}^{\top}] := \Sigma_{n}^\boot = n^{-1} \sum_{i=1}^{n-1} Q_i \eta (\xi_i) \eta(\xi_i)^\top Q_i^\top\eqsp.
\end{equation}
Furthermore, $D^\boot$ is a non-linear statistic of $\Xi^{n-1}$. The common approach to prove bootstrap validity is based on the Gaussian approximation performed both in the "real" world and bootstrap world together with an appropriate Gaussian comparison inequality:

\begin{tikzcd}[column sep = 130pt]
    \text{Real world: \quad\quad} \sqrt n  (\bar{\theta}_{n} - \thetas) \arrow[<->]{r}{\text{Gaussian approx.,  Th.}~\ref{th:bound_kolmogorov_dist_pr_sigma_n}}  &
  \Sigma_{n}^{1/2}Y \sim \mathcal N(0,  \Sigma_{n})  \arrow[<->]{d}{\text{Gaussian comparison, Lem. }\ref{lem: gaussion comparison boot and real} } 
\\
\text{Bootstrap world: } \sqrt n  (\bar{\theta}_{n}^\boot - \bar{\theta}_n) \arrow[<->]{r}{\text{Gaussian approx., Bootstrap world, Th.}~\ref{GAR bootstrap}} &
\{\Sigma_{n}^\boot\}^{1/2} Y^\boot \sim \mathcal N(0,   \Sigma_{n}^\boot ) 
\end{tikzcd}
An important question related to the above scheme is the choice of the approximating Gaussian distribution. This choice is purely instrumental in the sense that it does not change the procedure \eqref{eq:sgd_bootstrap}, but only affects the rates in \eqref{eq:boot_validity_supremum}. The authors of \cite{JMLR:v19:17-370} choose the approximation with $\mathcal N(0,  \Sigma_{\infty})$ for their asymptotic analysis. A similar approach was considered in \cite[Theorem 3]{samsonov2024gaussian} for the LSA algorithm setting. However, with this choice, the result of \Cref{cor:berry-esseen} does not allow us to obtain a normal approximation rate in \eqref{eq:boot_validity_supremum} faster than $n^{-1/4}$. At the same time, as we demonstrate later, we can achieve approximation rates of up to $n^{-1/2}$ by selecting $\mathcal N(0,  \Sigma_{n})$ and its bootstrap-world counterpart in the Gaussian approximation. This effect highlights the fundamental difference between the multiplier bootstrap approach and the plug-in approach \cite{chen2020aos}. The latter approach aims to estimate $\Sigma_{\infty}$, which is a challenging problem from a statistical perspective, especially when stochastic Hessian estimates are not available. Moreover, the Gaussian approximation rates for $\sqrt n  (\bar{\theta}_{n} - \thetas)$ are slower with $\mathcal{N}(0,\Sigma_{\infty})$ compared to $\mathcal{N}(0,\Sigma_{n})$, as shown in \Cref{sec:gaus_approximation}.
\par 
The second principal difficulty that arises when considering the conditional distribution of $\sqrt n  (\bar{\theta}_{n}^\boot - \bar{\theta}_n)$ given the data $\Xi^{n-1}$. In fact, the approach of \cite{shao2022berry} would require one to control the second moments of $D^\boot$ and $D^\boot - \{D^\boot\}^{(i)}$ with respect to a bootstrap measure $\PPb$, on the high-probability event with respect to a measure $\PP$. At the same time, we loose a martingale structure of the summands in $D^\boot$, unless we condition on the extended filtration 
\begin{equation}
\label{eq: extended filtration}
  \widetilde{\F}_i = \sigma(w_1,\ldots w_i, \xi_1,\ldots \xi_i) \eqsp, 1 \le i \le n-1 \eqsp.
\end{equation} 
Therefore, it is not clear if we can directly apply the approach of \cite{shao2022berry} discussed in \Cref{subsec:CLT_PR}. Instead, we can control $\bar{\PE}[\norm{D^\boot}^p]$ by Burkholder's inequality, where $\bar{\PE}$ denotes the expectation w.r.t. the product measure $\PP_\xi^{\otimes n} \otimes \PP_w^{\otimes n}$. Then we proceed with Markov's inequality to obtain $\PP$ -- high-probability bounds on the behavior of $\PEb[\norm{D^\boot}^p]$, and then proceed with a more crude result on Gaussian approximation for $\sqrt n  (\bar{\theta}_{n}^\boot - \bar{\theta}_n)$ detailed in \Cref{nonlinearapprox} in \Cref{sec: bootstrap validity section}. This result requires us to provide bounds for
\begin{equation}
\label{eq:hpd_bounds_last_iterate}
\textstyle 
\bar{\PE}^{1/p}[\norm{\theta_k - \thetas}^{p}] \quad \text{  and  } \quad \bar{\PE}^{1/p}[\norm{\theta_k^{\boot} - \thetas}^{p}]\eqsp, \quad k \in \{1,\ldots,n-1\}\eqsp,
\end{equation}
with $p \simeq \log{n}$ and polynomial dependence on $p$. Bounds \eqref{eq:hpd_bounds_last_iterate} will require additional assumptions on the stochastic gradient $F(\theta,\xi)$ discussed below.

\paragraph{Assumptions.} We additionally assume an almost sure co-coercivity of the stochastic gradient and strengthen \Cref{ass:noise_decomposition}($p$) to the bounded noise assumption:
\begin{assum}
\label{ass:L-co-coercivity-assum}
The stochastic gradient $F(\theta, \xi):= \nabla f(\theta) + g(\theta, \xi) + \eta(\xi)$ is almost surely $L_4$-co-coercive, that is, for any
$\theta, \theta'\in\rset^d$, it holds $\PP_{\xi}$-almost surely that 
\begin{equation}
\label{eq:L-co-coercivity-assum}
 L_4\langle  F(\theta, \xi) - F(\theta', \xi),\theta-\theta'\rangle \geq \norm{ F(\theta, \xi)- F(\theta', \xi)}^2\eqsp.
\end{equation}
\end{assum}
\begin{assum}
\label{ass:bound_noise}
Conditions (i) and (ii) from \Cref{ass:noise_decomposition} holds. Moreover, there exist $C_{1,\xi}, C_{2,\xi} > 0$ such that $\PP_{\xi}$-almost surely that $\norm{ \eta(\xi)} \leq C_{1,\xi}$ and $\sup_{\theta} \norm{g(\theta, \xi)}\leq  C_{2,\xi}$. 
\end{assum}
%The co‐coercivity condition  is a standard way of ensuring that  \(F(\theta,\xi)\) behaves ``nicely'' with respect to the parameter \(\theta\) as it ensures a tight coupling between changes in the parameter \(\theta\) and changes in the operator \(F(\theta,\xi)\).  In fact, the Baillon–Haddad theorem says that if a function \(f\) is convex and has an \(L\)-Lipschitz gradient, then \(\nabla f\) is \(1/L\)-co‐coercive.  
In particular, \Cref{ass:L-co-coercivity-assum} holds (see e.g. \cite{zhu2018coercive}), when there is a function $(\theta,\xi) \to v(\theta,\xi)$, such that $F(\theta, \xi) = \nabla_{\theta} v(\theta,\xi)$, where $v(\theta,\xi)$ is convex $\PP_\xi$-a.s. and $L_4$-smooth. Note that co‐coercivity is stronger than just requiring \(F(\theta,\xi)\) to be monotone. The assumption \Cref{ass:bound_noise} is crucial to prove high-order moment bounds \eqref{eq:hpd_bounds_last_iterate}, see \Cref{lem: high_prob_last_iter}. In our proof, we closely follow the argument presented in \cite[Theorem 4.1]{harvey2019tight}, which requires that the noise variables $\zeta_k$ be almost sure to be bounded. This setting can be generalized to the case where $\zeta_k$ is sub-Gaussian conditioned on $\F_{k-1}$ with variance proxy which is uniformly bounded by a constant factor, that is, there is a constant $M$, such that $\PE[\exp\{\norm{F(\theta,\xi_1)}^2/M^2\}] \leq 2$ for any $\theta \in \rset^{d}$. This assumption is widely considered in the literature; see \cite{nemirovski2009robust,hazan2014beyond}, and the remarks in \cite{harvey2019tight}. However, when $\zeta_k = g(\theta_{k-1},\xi_k) + \eta(\xi_k)$ and $g$ is only Lipschitz w.r.t. $\theta$, its moments will naturally scale with $\norm{\theta_{k-1} - \thetas}$, thus the sub-Gaussian bound with $M$ not depending upon $\theta$ is unlikely to hold.
\par 
Other authors who considered bounds of type \eqref{eq:hpd_bounds_last_iterate}, e.g. \cite{rakhlin2012making}, made stronger assumption that $\sup_{\theta \in \rset^{d}}\norm{F(\theta,\xi)}$ is a.s. bounded. Another popular direction is to consider schemes for gradient clipping; see \cite{sadiev2023high}. Such schemes change the decomposition \eqref{eq: linear part} and make it more complicated to identify the linear part $W$ as was done in \eqref{eq: linear part}. We leave further studies of clipped gradient schemes for future work. We also impose an assumption on the bootstrap weights $W_i$ used in the algorithm:
\begin{assum}
\label{ass:bound_bootstap_weights}
There exist constants $0 < W_{\min} < W_{\max} < +\infty$, such that $W_{\min}\leq W_1\leq  W_{\max}$ a.s.
\end{assum}
The original paper \cite{JMLR:v19:17-370} also considered positive bootstrap weights $W_i$. We have to impose boundedness of $W_i$ due to our high-probability bound on \Cref{lem: high_prob_last_iter}. A particular example of a distribution satisfying \Cref{ass:bound_bootstap_weights} is provided in \Cref{sec:example_distribution}. We also consider the following bound for step sizes $\alpha_k$ and sample size $n$:
\begin{assum}
\label{ass:step_size_new_boot} Let $\alpha_k = c_0 \{k_0 +k\}^{-\gamma}$, where $\gamma \in (1/2, 1)$, an $c_0$ satisfies $c_0 W_{\max} \max(2L_4, \mu) \leq 1$ and $k_0 \geq(\frac{2\gamma}{\mu c_0 W_{\min}})^{1/(1-\gamma)}$.
\end{assum} 
\begin{assum}
\label{ass:n_lower_bound}
Number of observations $n$ satisfies $n \geq \rme^3$ and $\frac{n}{\log(2dn)}\geq \max(1, \frac{(20 C_{Q, \xi}C_{\Sigma}^2)^2}{9})$. The constants $C_{Q, \xi}$ and $C_\Sigma$ are defined in \eqref{eq:const_C_Q_xi_def} and \eqref{eq:def_C_Sigma}, respectively.
\end{assum}
The particular bound on $k_0$ in \Cref{ass:step_size_new_boot} appears due to the high-order moment bounds from \Cref{lem: high_prob_last_iter}. Note that it is possible to remove the co-coercivity assumption \Cref{ass:L-co-coercivity-assum}, but at the price of slightly stronger constraints on $c_0$ above. We discuss the bound on the number of observations imposed in \Cref{ass:n_lower_bound} later in the proof of \Cref{th:bootstrap_validity}. 

\begin{theorem}
\label{th:bootstrap_validity}
Assume \Cref{ass:L-smooth}, \Cref{ass:hessian_Lipschitz_ball}, and \Cref{ass:L-co-coercivity-assum} - \Cref{ass:n_lower_bound}. Then with $\PP$ - probability at least $1 - 2/n$, it holds
\begin{equation}
\label{eq:gaussian_approximation_bootstrap}
\sup_{B \in \Conv(\rset^{d})} |\PPb(\sqrt n (\bar{\theta}_{n}^\boot - \bar{\theta}_n) \in B ) - \PP(\sqrt n (\bar{\theta}_n - \thetas) \in B)| \le   \frac{\ConstC_4 \sqrt{\log n}}{n^{1/2}} + \frac{\ConstC_5 \log n}{n^{\gamma - 1/2} } +  \frac{\ConstC_6 (\log n)^{3/2}}{n^{\gamma/2}} \eqsp,
\end{equation}
where $\ConstC_4, \ConstC_5$ and $\ConstC_6$ are given in \eqref{eq: bootsrap constants def}. 
\end{theorem}

\begin{proof} We provide the sketch of the proof, with the detailed arguments postponed to \Cref{sec: bootstrap validity section}. Here we combine the result of \Cref{th:bound_kolmogorov_dist_pr_sigma_n} together with the following result: 
\begin{equation}
\label{crude bound for nonlinear stat}
\begin{split}
&\sup_{B \in \Conv(\rset^d)}|\PPb(\{\Sigma_n^\boot\}^{-\tfrac{1}{2}} (W^\boot + D^\boot) \in B) - \PPb(Y^\boot \in B)|  \\ 
&\quad \leq \sup_{B \in \Conv(\rset^d)}|\PPb(\{\Sigma_n^\boot\}^{-\tfrac{1}{2}} W^\boot \in B) - \PPb(Y^\boot \in B)| +  2 c_d (\PEb[\norm{\{\Sigma_n^\boot\}^{-\tfrac{1}{2}} D^\boot}^p])^{\tfrac{1}{1+p}}\eqsp, 
\end{split}
\end{equation}
where $c_d \le 4 d^{1/4}$ is the isoperimetric constant of the class of convex sets. The proof of \eqref{crude bound for nonlinear stat} is provided in \Cref{nonlinearapprox} in \Cref{sec: bootstrap validity section}. Since the matrix $\Sigma_n^\boot$ concentrates around $\Sigma_n$ due to the matrix Bernstein inequality (see \Cref{lem:matrix_bernstein} for details), there is a set $\Omega_1$ such that $\PP(\Omega_1) \geq 1-1/n$ and $\lambda_{\min}(\Sigma_n^\boot) > 0$ on $\Omega_1$. Moreover, on this set 
\begin{equation}
\textstyle 
    \sup_{B \in \Conv(\rset^d)}|\PPb(\{\Sigma_n^\boot\}^{-1/2} W^\boot \in B) - \PPb(Y^\boot \in B)| \leq M_{3,1}^\boot n^{-1/2}\eqsp,
\end{equation}
where $M_{3,1}^\boot$ is defined in \eqref{M_3i boot def}. $\PP$ -- high-probability bounds on the behavior of $\PEb[\norm{D^\boot}^p]$ can be obtained using Markov's inequality and the bounds for $\bar{\PE}[\norm{D^\boot}^p]$, see \Cref{prop:prob-D-boot-bound} in the Appendix. Applying the latter proposition, there is a set $\Omega_0$ such that $\PP(\Omega_0) \geq 1-1/n$ and on $\Omega_0$
\begin{equation}
    2 c_d (\PEb[\norm{\{\Sigma_n^\boot\}^{-1/2} D^\boot}^p])^{1/(1+p)} \leq M_{3,2}^\boot n^{1/2-\gamma} \log n  + M_{3,3}^\boot n^{-\gamma/2} \log^{3/2} n \eqsp,
\end{equation}
with $M_{3,2}^\boot, M_{3,3}^\boot$ are defined in \Cref{sec: bootstrap validity section}, \eqref{M_3i boot def}. Combining the above arguments, it holds that 
\begin{equation}
\begin{split} 
    &\sup_{B \in \Conv(\rset^d)}|\PPb( \sqrt n   (\bar{\theta}_{n}^\boot - \bar{\theta}_n) \in B) - \PPb((\Sigma_n^\boot)^{1/2} Y^\boot \in B)| \leq  \frac{M_{3,1}^\boot}{n^{1/2}}  +  \frac{M_{3,2}^\boot  \log n}{n^{\gamma-1/2}}  + \frac{M_{3,3}^\boot \log^{3/2} n}{n^{\gamma/2}}\eqsp,
    \end{split}
\end{equation}
see details in \Cref{GAR bootstrap} in appendix. 
To finish the proof, it remains to combine the results of \Cref{th:bound_kolmogorov_dist_pr_sigma_n}, \Cref{GAR bootstrap}, and the Gaussian comparison result of \Cref{lem: gaussion comparison boot and real}. 
\end{proof}

\begin{remark}
\label{rem:step_size_remark}
It is possible to prove the result of \Cref{th:bootstrap_validity} for the step size $\alpha_k = c_0/(k+k_0)$. The required Gaussian approximation result with the covariance matrix $\Sigma_n$ is proved in \cite{shao2022berry}, and we expect that the only difference with \Cref{th:bootstrap_validity} will occur in extra $\log{n}$ factors in the corresponding bound and slightly different conditions on $c_0$ and $k_0$ in \Cref{ass:step_size_new_boot}.
\end{remark}

\paragraph{Discussion} In \cite{samsonov2024gaussian} a counterpart of \Cref{th:bootstrap_validity} was established with an approximation rate of the order $n^{-1/4}$ up to logarithmic factors for the setting of the LSA algorithm. The obtained rate is suboptimal, since the authors have chosen $\mathcal{N}(0,\Sigma_{\infty})$ for Gaussian approximation when showing bootstrap validity. A recent paper \cite{wu2024statistical} improved this rate to $n^{-1/3}$ in a setting of the TD learning procedure. Since the algorithm they considered is based on the direct estimate of $\Sigma_{\infty}$, yielding a more pessimistic bound (see \cite[Theorem 3.4 and 3.5]{wu2024statistical}). The authors in \cite{chen2020aos} constructed a plug-in estimator $\hat{\Sigma}_n$ of $\Sigma_{\infty}$ and showed guarantees of the form $\PE[\norm{\hat{\Sigma}_n - \Sigma_{\infty}}] \lesssim C n^{-\gamma/2}$, $\gamma \in (1/2,1)$ under weaker assumptions than those considered in the current section. The result in this particular form is not sufficient to prove the analogue of the Gaussian comparison result \Cref{lem:bound_kolmogorov_dist_sigma_n_sigma_infty} for $\mathcal{N}(0,\hat{\Sigma}_n)$ and $\mathcal{N}(0,\Sigma_{\infty})$ on the set with large $\PP$-probability. At the same time, approximating quantiles of $\sqrt{n}(\bar{\theta}_n - \thetas)$ with the method of \cite{chen2020aos} would require one more step - a Berry-Esseen type bound presented in  \Cref{cor:berry-esseen} - where the rates of convergence seem to be slower.




