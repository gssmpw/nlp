Stochastic Gradient Descent (SGD) is a widely used first-order optimization method that is well suited for large data sets and online learning. The algorithm has attracted much attention; see \cite{robbins1951stochastic,polyak1992acceleration,nemirovski2009robust,moulines2011non,bubeck2015convex}. SGD aims at solving the optimization problem:
\begin{equation}
\label{eq:stoch_minimization}
f(\theta) \to \min_{\theta \in \rset^{d}}\eqsp, \qquad \nabla f(\theta) = \PE_{\xi \sim \PP_{\xi}}[F(\theta,\xi)] \eqsp, 
\end{equation}
where $\xi$ is a random variable defined on a measurable space $(\Zset,\Zsigma)$. Instead of the exact gradient $\nabla f(\theta)$, the algorithm can only access unbiased stochastic estimates $F(\theta,\xi)$. 
\par 
Throughout this work, we focus on the case of strongly convex objective functions and denote by $\thetas$ the unique minimizer of \eqref{eq:stoch_minimization}. The iterates $\theta_k$, $k \in \nset$, generated by SGD follow the recursive update:
\begin{equation}
\label{eq:sgd_recursion_main}
\theta_{k+1} = \theta_{k} - \alpha_{k+1} F(\theta_k,\xi_{k+1})\eqsp, \quad \theta_0 \in \rset^{d}\eqsp, 
\end{equation}
where $\{\alpha_k\}_{k \in \nset}$ is a sequence of step sizes (or learning rates), which may be diminishing or constant, and $\{\xi_k\}_{k \in \nset}$ is an \iid\, sequence sampled from $\PP_{\xi}$. Theoretical properties of SGD, particularly in the convex and strongly convex settings, have been extensively studied; see, e.g., \cite{nesterov:2004, moulines2011non, bubeck2015convex, lan_2020}. Many optimization algorithms build upon the recurrence \eqref{eq:sgd_recursion_main} to accelerate the convergence of the sequence $\theta_k$ to $\thetas$. Notable examples include momentum acceleration \cite{qian1999momentum}, variance reduction techniques \cite{defazio2014saga, schmidt2017minimizing}, and averaging methods. In this work, we focus on Polyak-Ruppert averaging, originally proposed in \cite{ruppert1988efficient} and \cite{polyak1992acceleration}, which improves convergence by averaging the SGD iterates \eqref{eq:sgd_recursion_main}. Specifically, the estimator is defined as  
\begin{equation}
\label{eq:PR_estimate}
\bar \theta_n = \frac{1}{n}\sum_{i=0}^{n-1}\theta_i\eqsp, \quad n \in \nset\eqsp.
\end{equation}

It has been established (see \cite[Theorem 3]{polyak1992acceleration}) that under appropriate conditions on the objective function $f$, the noisy gradient estimates $F$, and the step sizes $\alpha_k$, the sequence of averaged iterates $\{\bar{\theta}_{n}\}_{n \in \nset}$ satisfies the central limit theorem:
\begin{equation}
\label{eq:CLT_fort}
\textstyle 
\sqrt{n}(\bar{\theta}_{n} - \thetas) \dto \gauss(0,\Sigma_\infty)\eqsp,
\end{equation}
where $\dto$ denotes convergence in distribution, and $\gauss(0,\Sigma_\infty)$ is a zero-mean Gaussian distribution with covariance matrix $\Sigma_\infty$, defined later in \Cref{sec:gaus_approximation}.  

This result raises two key questions: (i) what is the rate of convergence in \eqref{eq:CLT_fort}, and (ii) how can \eqref{eq:CLT_fort} be leveraged to construct confidence sets for $\thetas$, given that $\Sigma_\infty$ is typically unknown in practice? To quantify convergence, we employ convex distance as a measure of discrepancy. The convex distance is defined for random vectors $X, Y \in \rset^d$ as  
\begin{equation}
\label{eq:convex-distance-definiton}
\textstyle 
\kolmogorov(X, Y) = \sup_{B \in \Conv(\rset^{d})}\left|\P\bigl(X \in B\bigr) - \P(Y \in B)\right|\eqsp,
\end{equation}
where $\Conv(\rset^{d})$ denotes the collection of convex subsets of $\rset^{d}$. The authors of \cite{shao2022berry} derive Berry-Esseen-type bounds for $\kolmogorov(\sqrt{n}\Sigma_{n}^{-1/2}(\bar{\theta}_{n} - \thetas), \gauss(0,\Id_d))$, where $\Sigma_n$ is the covariance matrix of the linearized counterpart of \eqref{eq:sgd_recursion_main}, see precise definitions in \Cref{subsec:CLT_PR}. We complement this result with the rates of convergence in \eqref{eq:CLT_fort}. We also provide a non-asymptotic analysis of the multiplier bootstrap procedure for constructing confidence sets for $\thetas$ based on perturbing the trajectory \eqref{eq:sgd_recursion_main}, as proposed in \cite{JMLR:v19:17-370}.  

%The convergence rate of $\sqrt{n}(\bar{\theta}_{n} - \thetas)$ to a normal distribution, measured in terms of $\kolmogorov(\cdot,\cdot)$, was recently analyzed in \cite{shao2022berry}. We complement their result with the direct analysis of 
%When constructing confidence intervals, two main classes of methods can be identified. The first class consists of approaches that estimate the covariance matrix $\Sigma_{\infty}$ directly, as studied in \cite{chen2020aos, zhu2023online_cov_matr}. The second class includes bootstrap-based techniques \cite{efron1992bootstrap}, widely used for independent observations. For SGD-type methods they typically involve perturbing the trajectory \eqref{eq:sgd_recursion_main}, as proposed in \cite{JMLR:v19:17-370}. Most existing works focus on the asymptotic validity of these confidence intervals, 

\paragraph{Main contributions.} Our key contributions are as follows:
\begin{itemize}[noitemsep, nolistsep]
    \item We analyze the Polyak-Ruppert averaged SGD iterates \eqref{eq:PR_estimate} for strongly convex minimization problems and establish Gaussian approximation rates in \eqref{eq:CLT_fort} in terms of the convex distance. Specifically, we show that the approximation rate $\kolmogorov(\sqrt{n}(\bar{\theta}_{n} - \thetas), \gauss(0,\Sigma_\infty))$ is of order $n^{-1/4}$ when using the step size $\alpha_k = c_0/(k+k_0)^{3/4}$ with a suitably chosen $\alpha_0$. Our result builds on techniques from \cite{shao2022berry} and \cite{wu2024statistical}. 
    \item We establish the non-asymptotic validity of the multiplier bootstrap procedure introduced in \cite{JMLR:v19:17-370}. Under appropriate regularity conditions, our bounds imply that the quantiles of the exact distribution of $\sqrt{n}(\bar{\theta}_{n} - \thetas)$ can be approximated, up to logarithmic factors, at a rate of $n^{-\gamma/2}$ for step sizes of the form $\alpha_k = c_0/(k+k_0)^{\gamma}, \gamma \in (1/2,1)$. To our knowledge, this provides the first fully non-asymptotic bound on the accuracy of bootstrap approximations in SGD algorithms. Notably, this rate can be faster than the one that we can prove in \eqref{eq:CLT_fort}. Our results improve upon recent works \cite{samsonov2024gaussian, wu2024statistical}, which addressed the convergence rate in similar procedures for the LSA algorithm.
    \item Our analysis of the multiplier bootstrap procedure reveals an interesting property: unlike plug-in estimators, the validity of the bootstrap method does not directly depend  on approximating $\sqrt{n}(\bar{\theta}_{n} - \thetas)$ by $\gauss(0,\Sigma_\infty)$. Instead, it requires approximating $\gauss(0,\Sigma_n)$ for some matrix $\Sigma_n$. The structure of $\Sigma_n$ and its associated convergence rates play a central role in our present analysis, both for convergence rate in \eqref{eq:CLT_fort} and non-asymptotic bootstrap validity. Precise definitions are provided in \Cref{sec:gaus_approximation}.
\end{itemize}


\paragraph{Notations.} Throughout this paper, we use the following notations. For a matrix $A \in \rset^{d \times d}$ and a vector $x \in \rset^{d}$, we denote by $\norm{A}$ and $\norm{x}$ their spectral norm and Euclidean norm, respectively. We also write $\frobnorm{A}$ for Frobenius norm of matrix $A$. Given a function $f: \rset^{d} \to \rset$, we write $\nabla f(\theta)$ and $\nabla^2 f(\theta)$ for its gradient and Hessian at a point $\theta$.  Additionally, we use the standard abbreviations "i.i.d." for "independent and identically distributed" and "w.r.t." for "with respect to".

\paragraph{Literature review}
Asymptotic properties of the SGD algorithm, including the asymptotic normality of the estimator $\bar{\theta}_n$ and its almost sure convergence, have been extensively studied for smooth and strongly convex minimization problems \cite{polyak1992acceleration, kushner2003stochastic, benveniste2012adaptive}. Optimal mean-squared error (MSE) bounds for $\theta_n - \thetas$ and $\bar{\theta}_n - \thetas$ were established in \cite{nemirovski2009robust} for smooth and strongly convex objectives, and later refined in \cite{moulines2011non}. The case of constant-step size SGD for strongly convex problems has been analyzed in depth in \cite{durmus2020biassgd}.   High-probability bounds for SGD iterates were obtained in \cite{rakhlin2012making} and later extended in \cite{harvey2019tight}. Both works address non-smooth and strongly convex minimization problems. %The high-probability bounds established in this paper build heavily on the arguments developed in \cite{harvey2019tight}.
\par 
It is important to note that the results discussed above do not directly imply convergence rates for $\sqrt{n}(\bar{\theta}_{n} - \thetas)$ to $\gauss(0,\Sigma_\infty)$ in terms of $\kolmogorov(\cdot, \cdot)$ or the Kantorovich--Wasserstein distance. Among the relevant contributions in this direction, we highlight recent works \cite{srikant2024rates, samsonov2024gaussian, wu2024statistical}, which provide quantitative bounds on the convergence rate in \eqref{eq:CLT_fort} for iterates of the temporal difference learning algorithm and general linear stochastic approximation (LSA) schemes. However, these algorithms do not necessarily correspond to SGD with a quadratic objective $f$, as the system matrix in LSA is not necessarily symmetric. Non-asymptotic convergence rates of order $1/\sqrt{n}$ in a smooth Wasserstein distance were established in \cite{pmlr-v99-anastasiou19a}. Recent paper \cite{agrawalla2023high} provide Berry-Essen bounds for last iterate of SGD for high-dimensional linear regression of order up to $n^{-1/4}$.
\par 
Bootstrap methods for i.i.d. observations were first introduced in \cite{efron1992bootstrap}. In the context of SGD methods, \cite{JMLR:v19:17-370} proposed the multiplier bootstrap approach for constructing confidence intervals for $\thetas$ and established its asymptotic validity. The same algorithm, with non-asymptotic guarantees, was analyzed in \cite{samsonov2024gaussian} for the LSA algorithm, obtaining rate $n^{-1/4}$ when approximating quantiles of the exact distribution of $\sqrt{n}(\bar{\theta}_n - \thetas)$.
\par 
Popular group of methods for constructing confidence sets for $\thetas$ is based on estimating the asymptotic covariance matrix $\Sigma_{\infty}$. Plug-in estimators for $\Sigma_{\infty}$ attracted lot of attention, see  \cite{chen2020aos,chen2021statistical,chen2022online}, especially in the setting when the stochastic estimates of Hessian are available. The latter two papers focused on learning with contextual bandits. Estimates for $\Sigma_{\infty}$ based on batch-mean method and its online modification were considered in \cite{chen2020aos} and \cite{zhu2023online_cov_matr}. The authors in \cite{pmlr-v178-li22b} considered the asymptotic validity of the plug-in estimator for $\Sigma_{\infty}$ in the local SGD setting. \cite{zhong2023online} refined the validity guarantees for both the multiplier bootstrap and batch-mean estimates of $\Sigma_{\infty}$ for nonconvex problems. However, these papers typically provide recovery rates $\Sigma_{\infty}$, but only show asymptotic validity of the proposed confidence intervals. A notable exception is the recent paper \cite{wu2024statistical}, where the temporal difference (TD) learning algorithm was studied. The authors of \cite{wu2024statistical} provided purely non-asymptotic analysis of their procedure, obtaining rate $n^{-1/3}$ when approximating quantiles of $\sqrt{n}(\bar{\theta}_n - \thetas)$. 





